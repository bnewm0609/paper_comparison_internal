{"id": 220935624, "updated": "2023-10-06 13:03:29.634", "metadata": {"title": "Uncertainty-based Traffic Accident Anticipation with Spatio-Temporal Relational Learning", "authors": "[{\"first\":\"Wentao\",\"last\":\"Bao\",\"middle\":[]},{\"first\":\"Qi\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Yu\",\"last\":\"Kong\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 28th ACM International Conference on Multimedia", "publication_date": {"year": 2020, "month": 8, "day": 1}, "abstract": "Traffic accident anticipation aims to predict accidents from dashcam videos as early as possible, which is critical to safety-guaranteed self-driving systems. With cluttered traffic scenes and limited visual cues, it is of great challenge to predict how long there will be an accident from early observed frames. Most existing approaches are developed to learn features of accident-relevant agents for accident anticipation, while ignoring the features of their spatial and temporal relations. Besides, current deterministic deep neural networks could be overconfident in false predictions, leading to high risk of traffic accidents caused by self-driving systems. In this paper, we propose an uncertainty-based accident anticipation model with spatio-temporal relational learning. It sequentially predicts the probability of traffic accident occurrence with dashcam videos. Specifically, we propose to take advantage of graph convolution and recurrent networks for relational feature learning, and leverage Bayesian neural networks to address the intrinsic variability of latent relational representations. The derived uncertainty-based ranking loss is found to significantly boost model performance by improving the quality of relational features. In addition, we collect a new Car Crash Dataset (CCD) for traffic accident anticipation which contains environmental attributes and accident reasons annotations. Experimental results on both public and the newly-compiled datasets show state-of-the-art performance of our model. Our code and CCD dataset are available at https://github.com/Cogito2012/UString.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2008.00334", "mag": "3101279734", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/mm/BaoYK20", "doi": "10.1145/3394171.3413827"}}, "content": {"source": {"pdf_hash": "72797f6986121d17032894f350ffd6b3c3f7072f", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2008.00334v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2008.00334", "status": "GREEN"}}, "grobid": {"id": "12246eaccc4a139bac4d6dc34f9ff32097e5ced7", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/72797f6986121d17032894f350ffd6b3c3f7072f.txt", "contents": "\nUncertainty-based Traffic Accident Anticipation with Spatio-Temporal Relational Learning\nOctober 12-16, 2020. October 12-16, 2020\n\nWentao Bao \nQi Yu qi.yu@rit.edu \nYu Kong yu.kong@rit.edu \nWentao Bao \nQi Yu \nYu Kong \nUncertainty-based Traffic Accident Anticipation with Spatio-Temporal Relational Learning\n\nProceedings of the 28th ACM International Conference on Multimedia (MM '20)\nthe 28th ACM International Conference on Multimedia (MM '20)Seattle, WA, USA 2020; Seattle, WA, USAOctober 12-16, 2020. October 12-16, 202010.1145/3394171.3413827Rochester Institute of Technology Rochester, New York, USA Rochester Institute of Technology Rochester, New York, USA Rochester Institute of Technology Rochester, New York, USA ACM ISBN 978-1-4503-7988-5/20/10. . . $15.00 ACM Reference Format:. ACM, New York, NY, USA, 9 pages. https: //CCS CONCEPTS \u2022 Computing methodologies \u2192 Computer vision tasksBayesian network modelsNeural networks KEYWORDS Accident anticipationgraph convolutionbayesian neural networks\nFigure 1: Illustration of Uncertainty-based Accident Anticipation. This paper presents a novel model to predict the probabilities (black curve) of a future accident (ranges from 90-th to 100-th frame). Our goal is to achieve early anticipation (large Time-to-Accident) giving a threshold probability (horizontal dashed line), while estimating two kinds of predictive uncertainties, i,e., aleatoric uncertainty (wheat color region) and epistemic uncertainty (blue region).ABSTRACTTraffic accident anticipation aims to predict accidents from dashcam videos as early as possible, which is critical to safety-guaranteed self-driving systems. With cluttered traffic scenes and limited visual cues, it is of great challenge to predict how long there will be an accident from early observed frames. Most existing approaches are developed to learn features of accident-relevant agents for accident anticipation, while ignoring the features of their spatial and temporal relations. Besides, current deterministic deep neural networks could be overconfident in false predictions, leading to high risk of traffic accidents caused by self-driving systems. In this paper, we propose an uncertainty-based accident anticipation model with spatio-temporal relational learning. It sequentially predicts the probability of traffic accident occurrence with dashcam videos. Specifically, we propose to take advantage of graph convolution and recurrent networks for relational feature learning, and leverage Bayesian neural networks to address the intrinsic variability of latent relational representations. The derived uncertainty-based ranking loss is found to significantly boost model performance by improving Permission to make digital the quality of relational features. In addition, we collect a new Car Crash Dataset (CCD) for traffic accident anticipation which contains environmental attributes and accident reasons annotations. Experimental results on both public and the newly-compiled datasets show state-of-the-art performance of our model. Our code and CCD dataset are available at: https://github.com/Cogito2012/UString.\n\nINTRODUCTION\n\nAccident anticipation aims to predict an accident from dashcam video before it happens. It is one of the most important tasks for safety-guaranteed autonomous driving applications and has been receiving increasing attentions in recent years [4,7,10,32]. Thanks to accident anticipation, the safety level of intelligent systems on vehicles could be significantly enhanced. For example, even a successful anticipation made with only a few seconds earlier before the accident happens can help a self-driving system to make urgent safety control, avoiding a possible car crash accident.\n\nHowever, accident anticipation is still an extremely challenging task due to noisy and limited visual cues in an observed dashcam video. Take Fig. 1 as an example, a traffic scene captured in egocentric view is typically crowded with multiple cars, pedestrians, motorcyclists, and so on. In this scenario, accident-relevant visual cues could be overwhelmed by objects that are not relevant to the accident, making an intelligent system insensible to a car crash accident happened at the road intersection. Nevertheless, traffic accidents are foreseeable by training a powerful uncertainty-based model to distinguish the accident-relevant cues from noisy video data. For example, the inconsistent motions of multiple vehicles may indicate high risk of possible future accidents.\n\nIn this paper, we propose a novel uncertainty-based accident anticipation model with spatio-temporal relational learning. The model aims to learn accident-relevant cues for accident anticipation by considering both spatial and temporal relations among candidate agents. The candidate agents are a group of moving objects like vehicles and their relational features are indicative of future unobserved accidents. The spatial relations of candidate agents are learned from their spatial distance, visual appearance features, as well as historical visual memory. The temporal relations of agents provide learnable patterns to indicate how the agents evolve and end with an accident in temporal context. It can be recurrently learned by updating historical memory with agent-specific features and the spatial relational representation. To address the variability of the spatio-temporal relational representations, a probabilistic module is incorporated to simultaneously predict accident scores and estimate how much uncertainty when making the prediction.\n\nAs shown in Fig. 2, on one hand, we propose to learn spatial relations with graph convolutional networks (GCN) [8,19] by considering the hidden states from recurrent neural network (RNN)) [22,29] cell. On the other hand, we propose to build temporal relations with RNNs by considering both spatial relational and agent-specific features. The cyclic process of the coupled GCNs and RNNs could generate representative latent spatio-temporal relational features. Besides, we propose to incorporate Bayesian deep neural networks (BNNs) [9,24] into our model to address the predictive uncertainty. With the Bayesian formulation, our derived epistemic uncertaintybased ranking loss is effective to improve the quality of the learned relational features and significantly leads to performance gain. At last, to further consider the global guidance of all hidden states in training stage, we propose a self-attention aggregation layer as shown in Fig. 4, from which an auxiliary video-level loss is obtained and demonstrated beneficial to our model. Compared with existing RNN-based methods [4,32], our model captures not only agent-specific features but also relational features for accident anticipation. Compared with the recent approach [25] which is developed with 3D CNNs, our model is developed with GCNs and RNNs so that both spatial and temporal relations can be learned. Moreover, our method is capable of estimating the predictive uncertainty while all existing methods are deterministic.\n\nThe proposed model is evaluated on two public dashcam video datasets, i.e., DAD [4] and A3D [32], and our collected Car Crash Dataset (CCD). Experimental results show that our model can outperform existing methods on all datasets. For DAD datasets, our method can anticipate traffic accident 3.53 seconds on average earlier before an accident happens. With best precision setting, our model can achieve 72.22% average precision. Compared with DAD and A3D datasets, our CCD dataset includes diversified environmental annotations and accident reason descriptions, which could promote research on traffic accident reasoning.\n\nThe main contributions of this paper are summarized below:\n\n\u2022 We propose a traffic accident anticipation model by considering both agent-specific features and their spatio-temporal relations, as well as the predictive uncertainty. \u2022 With Bayesian formulation, the spatio-temporal relational representations can be learned with high quality by a novel uncertainty-based ranking loss. \u2022 We propose a self-attention aggregation layer to generate video-level prediction in the training stage, which serves as global guidance and is demonstrated beneficial to our model. \u2022 We release a new dataset containing real traffic accidents, in which diversified environmental annotations and accident reasons are provided.\n\n\nRELATED WORK 2.1 Traffic Accident Anticipation\n\nTo anticipate traffic accidents that happened in future frames, an intuitive solution is to iteratively predict accident confidence score for each time step. Chan et al. [4] recently proposed DSA framework to leverage candidate objects appeared in each frame to represent the traffic status. They applied spatial-attention on these objects to get weighted feature representation for each LSTM cell. Based on this work, Suzuki et al. [32] proposed an adaptive loss for early anticipation with quasi-recurrent neural networks [2]. Similar to DSA that implements dynamic-spatial attention to focus on accident-relevant objects, Corcoran and James [7] proposed a two-stream approach to traffic risk assessment. They utilized features of candidate objects as spatial stream and optical flow as temporal stream, and the two-stream features are fused for risk level classification. Instead of using dashcam videos, Shah et al. [30] proposed to use surveillance videos to anticipate traffic accidents by using the framework DSA. Different from previous works, recently Neumann and Zisserman [25] used 3D convolutional networks to predict the sufficient statistics of a mixture of 1D Gaussian distributions. In addition to using only dashcam video data, Takimoto et al. [33] proposed to incorporate physical location data to predict the occurrence of traffic accidents. Closely related to traffic accident anticipation, the traffic accident detection is recently studied by Yao et al. [36]. They proposed to detect traffic anomalies by predicting the future locations on video frames using ego-motion information. To anticipate both spatial risky regions and temporal accident occurrence, Zeng et al. [38] proposed a soft-attention RNN by considering event agent such as human that triggers the event. However, existing work typically ignores the relations between accident-relevant agents which capture important cues to anticipate accidents in future frames. Besides, none of them considers the uncertainty estimation in developing their models, which is critical to safety-guaranteed systems.  Figure 2: Framework of the proposed model. With graph embedded representations G(X t , A t ) at time step t, our model learns the latent relational representations Z t by the cyclic process of graph convolutional networks (GCNs) and recurrent neural network (RNN) cell, and predicts the accident score a t by Bayesian neural networks (BNNs).\n\n\nUncertainty in Sequential Modeling\n\nUncertainty estimation is crucial to sequential relational modeling. One way is to directly formulate the latent representations of relational observations at each time step as random variables, which follow posterior distributions that can be approximated by deep neural networks. This is similar to variational auto-encoder (VAE) [17,28]. Inspired by VAE, Chung et al. [6] proposed variational recurrent neural network (VRNN) which formulates the hidden states of RNN as random variables and uses neural networks to approximate the posterior distributions of the variables.\n\nTo further consider the relational representation of sequential data, Hajiramezanali et al. [14] proposed variational graph recurrent neural networks (VGRNN) for dynamic link prediction problem by combining the graph RNN and variational graph auto-encoder (VGAE) [18]. Another way to address uncertainty estimation is to formulate the weights of neural network as random variables such as Bayesian neural networks (BNNs) [9,24]. Recently, Zhao et al. [39] proposed a Bayesian graph convolution LSTM model for skeleton-based action recognition. In this paper, we also use graph convolution and BNNs but the difference is that their method uses stochastic gradient Hamiltonian Monte Carlo (SGHMC) sampling for posterior approximation, while we use Bayes-by-backprop [1] as our approximation method. Compared with SGHMC, Bayes-by-backprop can be seamlessly integrated into deep learning optimization process so that it is more flexible to handle the learning tasks with large-scale dataset, i.e., dashcam videos used in traffic accident anticipation.\n\n\nPROPOSED METHOD\n\nProblem Setup. In this paper, the goal of accident anticipation is to predict an accident from dashcam videos before it happens. Formally, given a video with current time step t, the model is expected to predict the probability a t that an accident event will happen in the future. Furthermore, suppose an accident will happen at time step y where t < y, the Time-to-Accident (TTA) is defined as \u03c4 = y \u2212 t when t is the first time that a t is larger than given threshold (see Fig. 1). For any t \u2265 y with a positive video that contains an accident, we define \u03c4 = 0 which means the model fails to anticipate the accident. In this paper, our goal is to predict a t and expect \u03c4 to be as large as possible for dashcam videos that contain accidents. Similar to [4], the ground truth of a t is expressed with 2-dimensional one-hot encoding so that prediction target is\na t = (a (p) t , a (n) t ) T , where a (p)\nt and a (n) t represent the positive and negative predictions, respectively, meaning an accident will happen or not happen in the given video.\n\nFramework Overview. The framework of our model is depicted in Fig. 2. With a dashcam video as input, a graph is constructed with detected objects and corresponding features at each time step. To learn the spatio-temporal relations of these objects, we use graph convolutional networks (GCNs) to learn the spatial relations and leverage the hidden state h t of recurrent neural network (RNN) cell to enhance the input of the last GCN layer. Besides, the latent relational features are fused with corresponding object features as input of an RNN cell to update the hidden state at next time step. The cyclic process encourages our model to learn the latent relational features Z t from both spatial and temporal aspects. Furthermore, we propose to use Bayesian neural network (BNN) to predict accident scores a t so that predictive uncertainties are naturally formulated. During the training stage, we propose a self-attention aggregation (SAA, in Fig. 4) layer to predict video-level score, which can globally guide the learning of the proposed model.\n\nIn the following sections, each part of our model will be introduced in detail.\n\n\nSpatio-Temporal Relational Learning\n\nThe spatio-temporal relations of traffic accident-relevant agents are informative to predict future accidents. In our model, we propose to use graph structured data to represent the observation at each time step. Then, the feature learning of spatial and temporal relations are coupled into a cyclic process.\n\nGraph Representation. Graph representation for traffic scene has the advantages over full-frame feature embedding in that the impact of cluttered traffic background can be reduced and informative relations of traffic agents can be discovered for accident anticipation. Similar to [4,30], we exploit object detectors [3,27] to obtain a fixed number of candidate objects. These objects are treated as graph nodes so that a complete graph can be formed. However, the computational cost of graph convolution could be tremendous if the node features are with high dimensions.\n\nIn this paper, to construct low-dimensional but representative features for graph nodes X t , we introduce fully-connected (FC) layers to embed both the features of full-frame and candidate objects into the same low-dimensional space. Then, the frame-level and all object-level features are concatenated to enhance the feature representation capability:\nX (i) t = \u03a6 O (i) t , \u03a6 (F t ) ,(1)\nwhere \u03a6 denotes FC layer, O (i) t and F t are high-dimensional features of the i-th object and corresponding frame at time t, respectively. The operator [, ] represents concatenation in feature dimension and is used throughout this paper for simplicity.\n\nThe graph edge at time t is expressed as an adjacent matrix A t of a complete graph since we do not have information on which candidate object will be involved in an accident. Typically, an object with closer distance to others has higher possibility to be involved in an future accident. Therefore, the spatial distance between objects should be considered in edge weights such that we define A t as\nA (i j) t = exp{\u2212d(r i , r j )} i j exp{\u2212d(r i , r j )} ,(2)\nwhere d(r i , r j ) measures the Euclidean distance between two candidate object regions r i and r j . By this formulation, closer distance\nleads to larger A (i j)\nt . This means the two objects i and j will be applied with larger weight when we use graph convolution to learn their relational features for accident anticipation. Note that due to object occlusions, small distance defined in pixel space does not necessarily indicate close distance in physical world. It is possible to use 3D real-world distance if camera intrinsics are known. Nevertheless, the adjacency matrix defined in Eq. 2 has advantage to suppress the impact of irrelevant objects with significant large pixel distance to relevant objects.\n\nTemporal Relational Learning. To build temporal relations at different time steps, RNN methods such as LSTM [15] and GRU [5] are widely adopted in existing works. However, traffic objects may not always be remained in each frame, the node features of the statically structured graph will be dynamically changing over time. Thanks to the recent graph convolutional recurrent network (GCRN) [29], it can handle the node dynamics defined over a static graph structure [14]. Therefore, we propose to adapt GCRN for temporal relational feature learning. Specifically, the hidden states h t of RNN cell at each time step are recurrently updated by\nh t +1 = GCRN ([Z t , X t ] , h t ) ,(3)\nwhere Z t is the relational feature generated by the last GCN layer. The feature fusion between Z t and X t ensures our model to make fully use of both agent-specific and relational features. Spatial Relational Learning. To capture spatial relations of detected objects, we follow the graph convolution defined by [8,19] for each GCN layer. In this paper, we use two stacked GCN layers and consider the hidden state h t learned by RNNs to learn the spatial relational features:\nZ t = GCN ([GCN (X t , A t ) , h t ] , A t ) .(4)\nThe fusion with h t enables the latent relational representation aware of temporal contextual information. This fusion method is demonstrated to be effective to boost the performance of accident anticipation in our experiments.\n\n\nBNNs for Accident Anticipation\n\nTo predict traffic accident score a t , a straightforward way is to utilize neural networks (NNs) as shown in Fig. 3(a). However, the output of NNs is a point estimate which cannot address the intrinsic variability of the input relational features at each time step. Moreover, NNs could be overconfident in false model predictions when the model suffers from over-fitting problem.\n\nTo this end, we incorporate Bayesian neural networks (BNNs) [9,24] into our framework for accident score prediction. The architecture is shown in Fig. 3(b). The BNNs module consists of two BNN layers with latent representation Z t given by Eq. 4 as input to predict accident score a t . To best of our knowledge, we are the first to incorporate BNNs into video-based traffic accident anticipation such that predictive uncertainty can be achieved.  (Fig.. 3(a)), network parameters of BNNs ( Fig. 3(b)) are sampled from Gaussian distributions so that both a t and its uncertainty can be obtained.\n\nuncertainty could be utilized to not only guide the relational features learning (see Section 3.3), but also provide tools to interpret the model performance.\n\nAs we formulate the accident anticipation part as BNNs, the network parameters of BNNs such as weights and biases are all random variables, denoted as \u03b8 . Each entry of \u03b8 is drawn from a Gaussian distribution determined by a mean and variance, i.e., \u03b8 (j) \u223c N (\u00b5, \u03c3 ), in which \u03b1 (j) = (\u00b5, \u03c3 ) need to be learned with dataset D = (Z t , a t ). Therefore, the likelihood of prediction can be expressed as p(a t |Z t , \u03b8 ) = N (f (Z t ; \u03b8 ), \u03b2), where \u03b2 is the predictive variance. However, according to Bayesian rule, to obtain the true posterior of model parameters, i.e., p(\u03b8 |D), in addition to the likelihood and prior of \u03b8 , the marginal distribution \u222b p(a t |Z t , \u03b8 )d\u03b8 is required, which is intractable since a t = f (Z t , \u03b8 ) is modeled by a complex neural network. To estimate p(\u03b8 |D), existing variational inference methods (VI) [1,11,13] could be used.\n\nIn this paper, we adopt the VI method Bayes-by-Backprop [1] to approximate p(\u03b8 |D) since it can be seamlessly incorporated in standard gradient-based optimization to learn from large-scale video dataset. According to [1], the variational approximation aims to minimize the following objective:\narg min \u03b1 J i=1 log q (\u03b8 i |\u03b1 ) \u2212 log p (\u03b8 i ) \u2212 log (p (D |\u03b8 i )) ,(5)\nwhere J is the number of Monte Carlo samplings for \u03b8 . The first term q (\u03b8 i |\u03b1 ) is the variational posterior distribution parameterized by \u03b1 . The distribution parameters \u03b1 can be efficiently learned by using reparameterization trick and standard gradient descent methods [1]. We denote this loss term as L V POS . The second term p(\u03b8 i ) is the prior distribution of \u03b8 . It is typically modeled with a spike-and-slab distribution, i.e., a mixture of two Gaussian density functions with zero means but different variances. We denote this loss term as L P RI . The third term in Eq. 5 is the negative log-likelihood of model predictions. Since minimizing this term is equivalent to minimizing the mean squared error (MSE), in this paper, we propose to use exponential binary cross entropy to achieve this objective: where f is the constant frame rate for the given video, and y is the beginning time of an accident provided by training set. The exponential weighted factor applies larger penalty to the time step that is closer to the beginning time of an accident.\nL EX P = T t =1 \u2212e \u2212 max 0, y\u2212t f log a (p) t + T t =1 \u2212 log 1 \u2212 a (n) t ,(6)\n\nUncertainty-guided Ranking Loss\n\nWith the Bayesian formulation for accident anticipation, we can perform multiple forward passes at each time step such that an assembled prediction could be obtained by taking the average of these multiple outputs. Furthermore, as suggested by [16], the predictive uncertainty (variance) can be decomposed as aleatoric uncertainty and epistemic uncertainty [20,31]:\nU t = 1 M M i=1 diag (\u00e2 i ) \u2212\u00e2 i\u00e2 T i Aleatoric Uncertainty(U al t t ) + 1 M M i=1 (\u00e2 i \u2212\u0101) (\u00e2 i \u2212\u0101) T Epistemic Uncertainty(U ept t ) ,(7)where\u0101 = 1 M M i=1\u00e2 i and\u00e2 i = (\u00e2 (n) t ,\u00e2 (p) t ) T i .\nThey are the predictions of the i-th forward pass at time step t with total M forward passes. The first term in Eq. 7 is the aleatoric uncertainty, which measures the input variability (noise) of BNNs. In our model, the aleatoric uncertainty serves as an indicator to the quality of the learned relational features from GCNs and RNNs.\n\nThe second term in Eq. 7 is epistemic uncertainty which is determined by the BNNs model itself. Inspired by Ma et al. [23], ideally the epistemic uncertainties of sequential predictions should be monotonically decreasing, since as more frames the model observes, the more confident of the learned model (smaller epistemic uncertainty) will be. Therefore, we propose a novel ranking loss:\nL RAN K = max 0, trace U ept t \u2212 U ept t \u22121 ,(8)\nwhere U ept t \u22121 and U ept t are epistemic uncertainties of successive frames t \u2212 1 and t defined in Eq. 7. Note that U t as well as the two terms in Eq. 7 are matrices with size 2 \u00d7 2, therefore in practice we propose to use matrix trace to quantify the uncertainties, which is similar to the method adopted in [31]. Our proposed ranking loss aims to apply penalty to the predictions that do not follow the epistemic uncertainty ranking rule.\n\nFor aleatoric uncertainty U al t t , it is not necessary to satisfy the monotonic ranking requirement since the noise ratio of accumulated data in video sequence is intrinsically not monotonic.\n\n\nTemporal Self-Attention Aggregation\n\nRecurrent network can naturally build temporal relations of observations. However, the drawback of RNNs is that inaccurate hidden states in early temporal stages could be accumulated in iterative procedure and mislead the model to give false predictions in latter temporal stages. Besides, the hidden states in different time steps should be adaptive to anticipate the occurrence of a future accident.\n\nTo this end, motivated by recent self-attention design [34], we propose a self-attention aggregation (SAA) layer in the training stage by adaptively aggregating hidden states of all time steps. Then, we use the aggregated representation to predict video-level accident score. The architecture of SAA layer is shown in Fig. 4.\n\nSpecifically, we first aggregate hidden states of N individual objects at each time step by applying the concatenation between mean-and max-pooling results. Then, the self-attention [34] is adapted to weigh the representation of all T time steps. In this module, the embedding layers are not used. Lastly, instead of using simple average pooling, we introduce an FC layer with T learnable parameters to adaptively aggregate the T temporal hidden states. The aggregated video-level representation is used to predict the video-level accident score a by two FC layers. This network is trained with binary cross-entropy (BCE) loss:\nL BC E = \u2212 log a (p) \u2212 log 1 \u2212 a (n) ,(9)\nwhere a = (a (n) , a (p) ) T normalized by softmax function. This auxiliary learning objective encourages the model to learn better hidden states even though SAA layer is not used in testing stage. Finally, the complete learning objective of our model is to minimize the following weighted loss: L = L EX P +w 1 \u00b7(L V POS \u2212 L P RI )+w 2 \u00b7L RAN K +w 3 \u00b7L BC E (10) where the L V POS and L P RI are loss functions of variational posterior and prior. The constants w 1 , w 2 and w 3 are set to 0.001, 10 and 10, respectively, to balance the magnitudes of these loss terms. The second penalty term (L V POS \u2212 L P RI ) is also termed as complexity loss and has similar effect to overcome over-fitting problem. The third penalty term L BC E introduces video-level classification guidance while the fourth term L RAN K brings uncertainty ranking guidance to train our model.\n\n\nEXPERIMENTAL RESULTS\n\nIn this section, we evaluate our model on three real-world datasets, including our collected Car Crash Dataset (CCD) and two public datasets, i.e., Dashcam Accident Dataset (DAD) [4] and AnAn Accident Detection (A3D) dataset [36]. State-of-the art methods are compared and ablation studies are performed to validate our model.\n\n\nDatasets\n\nCCD dataset 1 . In this paper, we collect a challenging Car Crash Dataset (CCD) for accident anticipation. We ask annotators to label YouTube accident videos with temporal annotations, diversified environmental attributes (day/night, snowy/rainy/good weather conditions), whether ego-vehicles involved, accident participants, and accident reason descriptions. For temporal annotations, the   Table 1: Comparison between CCD dataset and existing datasets. Information about DAD and A3D is obtained from their released sources. Temporal Annos. means the temporal accident time annotations. Random ABT. means accidents beginning times are randomly placed. Ego-involved means the ego-vehicles are involved in accidents. Day/Night indicates the data is collected in day or night. Weather includes rainy, snowy, and sunny conditions. Participants Bbox means the bounding boxes tracklets for accident participants. Accident Reasons contains multiple possible reasons for each accident participant. accident beginning time is labeled at the time when a car crash actually happens. To get trimmed videos with 5 seconds long, the accident beginning times are further randomly placed in last 2 seconds, generating 1,500 traffic accident video clips. We also collected 3,000 normal dashcam videos from BDD100K [37] as negative samples. The dataset is divided into 3,600 training videos and 900 testing videos. Examples are shown in Fig. 5 and comparison details with existing datasets are reported in Table 1. Compared with DAD [4] and A3D [36], our CCD is larger with diversified annotations. DAD dataset. DAD [4] contains dashcam videos collected in six cities in Taiwan. It provides 620 accident videos and 1,130 normal videos. Each video is trimmed and sampled into 100 frames with totally 5 seconds long. For accident videos, accidents are placed in the last 10 frames. The dataset has been divided into 1,284 training videos (455 positives and 829 negatives) and 466 testing videos (165 positives and 301 negatives).\n\nA3D dataset. A3D [36] is also a dashcam accident video dataset. It contains 1,500 positive traffic accident videos. In this paper, we only keep the 587 videos in which ego-vehicles are not involved in accidents. We sampled each A3D video with 20 fps to get 100 frames in total and placed the beginning time of each accident at the last 20 frames similar to DAD. The dataset is divided into 80% training set and 20% testing set.\n\n\nEvaluation Metrics\n\nAverage Precision. This metric evaluates the correctness of identifying an accident from a video. Following the same definition as [4], at time step t, if a (p) t is larger than a threshold, then the prediction at frame t is positive to contain an accident, otherwise it is negative. For accident videos, all frames are labeled with ones (positive), otherwise the labels are zeros (negative). By this way, the precision, recall, as well as the derived Average Precision (AP) can be adopted to evaluate models.\n\nTime-to-Accident. This metric evaluates the earliness of accident anticipation based on positive predictions. For a range of threshold values, multiple TTA results as well as corresponding recall rates can be obtained. Then, we use mTTA and TTA@0.8 to evaluate the earliness, where mTTA is the average of all TTA values and TTA@0.8 is the TTA value when recall rate is 80%. Note that if a large portion of predictions are false positives, very high TTA results can still be achieved while corresponding AP would be low. That means the model is overfitting on accident video and may give positive predictions for arbitrary input. Therefore, except for fair comparison with existing methods, we mainly report TTA metrics when the highest AP is achieved, because it is meaningless to obtain high TTA if high AP cannot be guaranteed.\n\nPredictive Uncertainty. Based on Eq. 7, we introduce to use the mean aleatoric uncertainty (mAU) and mean epistemic uncertainty (mEU) to evaluate the predictive uncertainties.\n\n\nImplementation Details\n\nWe implement our model with PyTorch [26]. For DAD dataset, we use the candidate objects and corresponding features provided Table 2: Evaluation results on DAD, A3D, and CCD datasets. Results of baselines on DAD are obtained from [38] and [32]. The notation \"-\" means the metric is not applicable.  [21] neck as our object detector on KITTI 2D detection dataset [12]. The trained detector is used to detect candidate objects and then extract VGG-16 features of full-frame and all objects. As suggested by Bayes-by-backprop [1], we set the number of forward passes M to 2 in training stage and 10 for testing stage. For the hyper-parameters of prior distribution, we set the mixture ratio \u03c0 to 0.5 and the variances of the two Gaussian distributions \u03c3 1 to 1 and \u03c3 2 to exp(\u22126). The dimensions of both hidden state of RNN and output of GCNs are set to 256. In the training stage, we set batch size to 10 and initial learning rate to 0.0005 with ReduceLROnPlateau as learning rate scheduler. The model is trained by Adam optimizer for totally 70 training epochs.\n\n\nPerformance Evaluation\n\nCompare with State-of-the-art Methods. Existing methods [4,32,38] are compared and results are reported in Table 2. For fair comparison, we use the model at the last training epoch for evaluation on DAD datasets. Nevertheless, the trained model with best AP is kept for evaluation on other two datasets since high AP is important to suppress impact of false positives on TTA evaluation. Note that these two metrics currently are only applicable to our model, since we are the first to introduce uncertainty formulation for accident anticipation. From Table 2, our model on DAD dataset achieves the best mTTA which means the model anticipates on average 3.53 seconds earlier before an accident happens, while keeping competitive AP performance at 53.7% compared with L-RAI and adaLEA. Note that the video lengths of the three datasets are all 5 seconds, our high performance on A3D and CCD demonstrate that our model is easier to be trained on different datasets. This can be explained by the mAU results due to their consistence with TTA evaluation results in Table 2. The low mAU values on A3D and DAD datasets reveal that our model has learned relational representations with high quality on these datasets.\n\nWe further report TTA results with different recall rates from 10% to 90% in Table 3. It shows that our model outperform DSA in most of recall rate requirements. For recall rates larger than 80%,  our method performs poorly compared with DSA. However, high recall rate may also lead to too much false alarm so that AP cannot be guaranteed to be high. This finding also supports our motivation to use the trained model with best AP for evaluation. Visualization We visualized accident anticipation results with samples in DAD dataset (see Fig. 6). The uncertainty regions indicate that in both early and late stages, the model is quite confident on prediction (low uncertainties), while in the middle stage when accident scores start are increasing, the model is uncertain to give predictions. Note that the predicted epistemic uncertainty (blue region) is not necessary to be monotonically decreasing since we only use Eq. 8 as training regularizer rather than strict guarantee on predictions. The results are with good interpretability, in that driving system is typically quite sure about the accident risk level when the self-driving car is far from or almost being involved in an accident, while it is uncertain about it when accumulated accident cues are insufficient to make decision.\n\n\nAblation Study\n\nIn this section, to validate the effectiveness of the several main components, the following components are replaced or removed, and compared with our model based on best AP setting. (1) BNNs: The BNNs are replaced with vanilla FC layers. Note that in this case, L V POS \u2212 L P RI and our proposed ranking loss L RAN K in Eq. 10, as well as mAU are not applicable. (2) SAA: The SAA layer is removed so that L BC E in Eq. 10 is not used. (3) GCN: We replace GCNs with vanilla FC layers in Eq. 3 and Eq. 4. (4) Fusion: For this variant, the fusion in Eq. 3 and Eq. 4 are removed such that only Z t and GCN(X t , A t ) are used, respectively. (5) RankLoss: The epistemic uncertainty-based ranking loss is removed so that L RAN K in Eq. 10 is not applicable. Results are shown in Table 4.\n\nWe can clearly see that the uncertainty-based ranking loss contributes most to our model by comparing variant (2)(6) with (1), with about 7.6% performance gain. Though the BNNs module leads to small performance gain, we attribute the benefit of BNNs to its derived uncertainty ranking loss as well as the interpretable results. Furthermore, the lowest mAU and highest AP for variant (1) demonstrate that the learned relational features are of highest quality (smallest uncertainty) compared with other variants.  Figure 6: Examples of our predictions on DAD datasets. The red curves indicate smoothed accident scores as observed frames increase. The ground truth (beginning time of accident) are labeled at 90-th frame. We plot one time of squared epistemic (blue region) and aleatoric uncertainties (wheat color region). The horizontal line indicates probability threshold 0.5. The results of variants (3) validate the effectiveness of our selfattention aggregation (SAA) layer, while the results of variant (4) validate the superiority GCN over naive FC layers. The results of variant (5) show that the feature fusion between GCN outputs and hidden states, and the fusion between relational features and agentspecific features are important to accident anticipation, leading to approximately 7% performance gain.\n\nModel Size Comparison. The number of network parameters are counted and reported in Table 5. It shows that the proposed model is much light-weighted than DSA, and only slightly increases the model size when compared with other variants of our model.\n\n\nCONCLUSION\n\nIn this paper, we propose an uncertainty-based traffic accident anticipation with spatio-temporal relational learning. Our model can handle the challenges of relational feature learning and anticipation uncertainty from video data. Moreover, the introduced Bayesian formulation not only significantly boosts anticipation performance by using the uncertainty-based ranking loss, but also provides interpretation on predictive uncertainty. In addition, we release a Car Crash Dataset (CCD) for accident anticipation which contains rich environmental attributes and accident reason annotations.\n\nFigure 3 :\n3Compared with NNs\n\nFigure 4 :\n4SAA Layer. First, all N \u00d7T hidden states are gathered and pooled by max-avg concatenation. Then, the simplified self-attention and adaptive aggregation are proposed to predict video-level accident score a.\n\n\u22f0Figure 5 :\n5Annotation samples of our Car Crash Dataset (CCD). The gray box on top-left contains video-level annotations, while the other three white boxes provide instance-level annotations.\n\n\nDatasets # Videos # Positives Total Hours Temporal Annos. Random ABT. Ego-Involved Day/Night Weather Participants Bbox Accident ReasonsDAD [4] \n1,750 \n620 \n2.43 h \n\u2713 \nA3D [36] \n1,500 \n1,500 \n3.56 h \n\u2713 \n\u2713 \n\u2713 \nOurs (CCD) 4,500 \n1,500 \n6.25 h \n\u2713 \n\u2713 \n\u2713 \n\u2713 \n\u2713 \n\u2713 \n\u2713 \n\n\n\nTable 3 :\n3TTA with different recall rates on DAD dataset. DSA [4] 0.28 0.50 0.73 0.87 0.92 1.02 1.24 1.35 2.28 Ours 0.59 0.75 0.84 0.96 1.07 1.16 1.33 1.56 1.99Recall \n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 \n\n\nTable 4 :\n4Ablation studies results on DAD dataset.Variants BNNs SAA GCN Fusion RankLoss AP(%) mAU \n(1) \n\u2713 \n\u2713 \n\u2713 \n\u2713 \n\u2713 \n72.22 0.0731 \n(2) \n\u2713 \n\u2713 \n\u2713 \n70.38 \n-\n(3) \n\u2713 \n\u2713 \n\u2713 \n\u2713 \n67.34 0.1150 \n(4) \n\u2713 \n\u2713 \n\u2713 \n\u2713 \n67.10 0.1250 \n(5) \n\u2713 \n\u2713 \n\u2713 \n\u2713 \n65.50 0.1172 \n(6) \n\u2713 \n\u2713 \n\u2713 \n\u2713 \n64.60 0.0950 \n\n\n\nTable 5 :\n5Model size comparison. Our model variants (2), (4), and (5) are included for comparison. Unit M means a million.Methods \nDSA \nOurs \nv(2) \nv(4) \nv(5) \n# Params. (M) \n4.40 \n1.97 \n1.66 \n1.97 \n1.90 \n\n\nCCD dataset is available at: https://github.com/Cogito2012/CarCrashDataset\nDSA: https://github.com/smallcorgi/Anticipating-Accidents 3 MMDetection: https://github.com/open-mmlab/mmdetection\nACKNOWLEDGMENTSWe thank NVIDIA for GPU donation and Haiting Hao for organizing dataset collection. This research is supported by an ONR Award N00014-18-1-2875. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agency.\nWeight Uncertainty in Neural Networks. Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, Daan Wierstra, International Conference on Machine Learning. Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. 2015. Weight Uncertainty in Neural Networks. In International Conference on Machine Learning.\n\nQuasi-Recurrent Neural Networks. James Bradbury, Stephen Merity, Caiming Xiong, Richard Socher, International Conference on Learning Representations. James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher. 2017. Quasi-Recurrent Neural Networks. In International Conference on Learning Rep- resentations.\n\nCascade R-CNN: Delving into High Quality Object Detection. Zhaowei Cai, Nuno Vasconcelos, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. IEEE Conference on Computer Vision and Pattern RecognitionZhaowei Cai and Nuno Vasconcelos. 2018. Cascade R-CNN: Delving into High Quality Object Detection. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.\n\nAnticipating Accidents in Dashcam Videos. Yu-Ting Fu-Hsiang Chan, Yu Chen, Min Xiang, Sun, Asian Conference on Computer Vision. Fu-Hsiang Chan, Yu-Ting Chen, Yu Xiang, and Min Sun. 2016. Anticipating Accidents in Dashcam Videos. In Asian Conference on Computer Vision.\n\nLearning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation. Kyunghyun Cho, Caglar Bart Van Merri\u00ebnboer, Dzmitry Gulcehre, Fethi Bahdanau, Holger Bougares, Yoshua Schwenk, Bengio, Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language ProcessingKyunghyun Cho, Bart van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Transla- tion. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.\n\nA Recurrent Latent Variable Model for Sequential Data. Junyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, C Aaron, Yoshua Courville, Bengio, Proceedings of Neural Information Processing Systems. Neural Information Processing SystemsJunyoung Chung, Kyle Kastner, Laurent Dinh, Kratarth Goel, Aaron C Courville, and Yoshua Bengio. 2015. A Recurrent Latent Variable Model for Sequential Data. In Proceedings of Neural Information Processing Systems.\n\nTraffic Risk Assessment: A Two-Stream Approach Using Dynamic Attention. G Corcoran, J Clark, Conference on Computer and Robot Vision. G. Corcoran and J. Clark. 2019. Traffic Risk Assessment: A Two-Stream Approach Using Dynamic Attention. In Conference on Computer and Robot Vision.\n\nConvolutional Neural Networks on Graphs with Fast Localized Spectral Filtering. Micha\u00ebl Defferrard, Xavier Bresson, Pierre Vandergheynst, Proceedings of Neural Information Processing Systems. Neural Information Processing SystemsMicha\u00ebl Defferrard, Xavier Bresson, and Pierre Vandergheynst. 2016. Convo- lutional Neural Networks on Graphs with Fast Localized Spectral Filtering. In Proceedings of Neural Information Processing Systems.\n\nTransforming Neural-Net Output Levels to Probability Distributions. John S Denker, Yann Lecun, Proceedings of Neural Information Processing Systems. Neural Information Processing SystemsJohn S. Denker and Yann LeCun. 1990. Transforming Neural-Net Output Levels to Probability Distributions. In Proceedings of Neural Information Processing Systems.\n\nDADA-2000: Can Driving Accident be Predicted by Driver Attention? Analyzed by A Benchmark. J Fang, D Yan, J Qiao, J Xue, H Wang, S Li, IEEE Intelligent Transportation Systems Conference. J. Fang, D. Yan, J. Qiao, J. Xue, H. Wang, and S. Li. 2019. DADA-2000: Can Driving Accident be Predicted by Driver Attention? Analyzed by A Benchmark. In IEEE Intelligent Transportation Systems Conference.\n\nBayesian Convolutional Neural Networks with Bernoulli Approximate Variational Inference. Yarin Gal, Zoubin Ghahramani, International Conference on Learning Representations (Workshop). Yarin Gal and Zoubin Ghahramani. 2016. Bayesian Convolutional Neural Net- works with Bernoulli Approximate Variational Inference. In International Confer- ence on Learning Representations (Workshop).\n\nAre We Ready for Autonomous Driving? The KITTI Vision Benchmark Suite. Andreas Geiger, Philip Lenz, Raquel Urtasun, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. IEEE Conference on Computer Vision and Pattern RecognitionAndreas Geiger, Philip Lenz, and Raquel Urtasun. 2012. Are We Ready for Autonomous Driving? The KITTI Vision Benchmark Suite. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.\n\nPractical Variational Inference for Neural Networks. Alex Graves, Proceedings of Neural Information Processing Systems. Neural Information Processing SystemsAlex Graves. 2011. Practical Variational Inference for Neural Networks. In Proceedings of Neural Information Processing Systems.\n\nVariational Graph Recurrent Neural Networks. Ehsan Hajiramezanali, Arman Hasanzadeh, Krishna Narayanan, Nick Duffield, Mingyuan Zhou, Xiaoning Qian, Proceedings of Neural Information Processing Systems. Neural Information Processing SystemsEhsan Hajiramezanali, Arman Hasanzadeh, Krishna Narayanan, Nick Duffield, Mingyuan Zhou, and Xiaoning Qian. 2019. Variational Graph Recurrent Neural Networks. In Proceedings of Neural Information Processing Systems.\n\nLong Short-Term Memory. Sepp Hochreiter, J\u00fcrgen Schmidhuber, Neural Computation. 9Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long Short-Term Memory. Neural Computation 9, 8 (1997), 1735-1780.\n\nWhat Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision. Alex Kendall, Yarin Gal, Proceedings of Neural Information Processing Systems. Neural Information Processing SystemsAlex Kendall and Yarin Gal. 2017. What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?. In Proceedings of Neural Information Pro- cessing Systems.\n\nAuto-Encoding Variational Bayes. P Diederik, Max Kingma, Welling, International Conference on Learning Representations. Diederik P Kingma and Max Welling. 2013. Auto-Encoding Variational Bayes. In International Conference on Learning Representations.\n\nVariational Graph Auto-Encoders. N Thomas, Max Kipf, Welling, Proceedings of Neural Information Processing Systems (Workshop). Neural Information Processing Systems (Workshop)Thomas N Kipf and Max Welling. 2016. Variational Graph Auto-Encoders. In Proceedings of Neural Information Processing Systems (Workshop).\n\nSemi-supervised Classification with Graph Convolutional Networks. N Thomas, Max Kipf, Welling, International Conference on Learning Representations. Thomas N Kipf and Max Welling. 2017. Semi-supervised Classification with Graph Convolutional Networks. In International Conference on Learning Representations.\n\nUncertainty Quantification Using Bayesian Neural Networks in Classification: Application to Ischemic Stroke Lesion Segmentation. Yongchan Kwon, Joong-Ho Won, Beom Joon Kim, Myunghee Cho Paik, Medical Imaging with Deep Learning. Yongchan Kwon, Joong-Ho Won, Beom Joon Kim, and Myunghee Cho Paik. 2018. Uncertainty Quantification Using Bayesian Neural Networks in Classification: Application to Ischemic Stroke Lesion Segmentation. In Medical Imaging with Deep Learning.\n\nKaiming He, Bharath Hariharan, and Serge Belongie. Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. IEEE Conference on Computer Vision and Pattern RecognitionFeature Pyramid Networks for Object DetectionTsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. 2017. Feature Pyramid Networks for Object Detection. In Pro- ceedings of IEEE Conference on Computer Vision and Pattern Recognition.\n\nJohn Zachary C Lipton, Charles Berkowitz, Elkan, arXiv:1506.00019A Critical Review of Recurrent Neural Networks for Sequence Learning. Zachary C Lipton, John Berkowitz, and Charles Elkan. 2015. A Critical Review of Recurrent Neural Networks for Sequence Learning. arXiv:1506.00019 (2015).\n\nLearning Activity Progression in LSTMs for Activity Detection and Early Detection. Shugao Ma, Leonid Sigal, Stan Sclaroff, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. IEEE Conference on Computer Vision and Pattern RecognitionShugao Ma, Leonid Sigal, and Stan Sclaroff. 2016. Learning Activity Progression in LSTMs for Activity Detection and Early Detection. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.\n\nBayesian learning for neural networks. M Radford, Neal, Springer Science & Business Media118Radford M Neal. 2012. Bayesian learning for neural networks. Vol. 118. Springer Science & Business Media.\n\nFuture Event Prediction: If and When. Lukas Neumann, Andrew Zisserman, Andrea Vedaldi, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (Workshop). IEEE Conference on Computer Vision and Pattern Recognition (Workshop)Lukas Neumann, Andrew Zisserman, and Andrea Vedaldi. 2019. Future Event Prediction: If and When. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (Workshop).\n\nPyTorch: An Imperative Style, High-Performance Deep Learning Library. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary Devito, Proceedings of Neural Information Processing Systems. Junjie Bai, and Soumith ChintalaNeural Information Processing SystemsMartin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu FangAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Des- maison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Proceedings of Neural Information Processing Systems.\n\nFaster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. Kaiming Shaoqing Ren, Ross He, Jian Girshick, Sun, Proceedings of Neural Information Processing Systems. Neural Information Processing SystemsShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster R-CNN: To- wards Real-Time Object Detection with Region Proposal Networks. In Proceedings of Neural Information Processing Systems.\n\nStochastic Backpropagation and Approximate Inference in Deep Generative Models. Danilo Jimenez Rezende, Shakir Mohamed, Daan Wierstra, International Conference on Machine Learning. Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. 2014. Stochastic Backpropagation and Approximate Inference in Deep Generative Models. In International Conference on Machine Learning.\n\nStructured Sequence Modeling with Graph Convolutional Recurrent Networks. Youngjoo Seo, Micha\u00ebl Defferrard, Pierre Vandergheynst, Xavier Bresson, Proceedings of Neural Information Processing Systems. Neural Information Processing SystemsYoungjoo Seo, Micha\u00ebl Defferrard, Pierre Vandergheynst, and Xavier Bresson. 2018. Structured Sequence Modeling with Graph Convolutional Recurrent Net- works. In Proceedings of Neural Information Processing Systems.\n\nCADP: A Novel Dataset for CCTV Traffic Camera based Accident Analysis. Ankit Shah, Jean Baptiste Lamare, Tuan Nguyen, Anh , Alexander Hauptmann, International Workshop on Traffic and Street Surveillance for Safety and Security. Ankit Shah, Jean Baptiste Lamare, Tuan Nguyen Anh, and Alexander Hauptmann. 2018. CADP: A Novel Dataset for CCTV Traffic Camera based Accident Analysis. In International Workshop on Traffic and Street Surveillance for Safety and Security.\n\nKumar Shridhar, Felix Laumann, Marcus Liwicki, arXiv:1806.05978Uncertainty Estimations by Softplus Normalization in Bayesian Convolutional Neural Networks with Variational Inference. Kumar Shridhar, Felix Laumann, and Marcus Liwicki. 2018. Uncertainty Esti- mations by Softplus Normalization in Bayesian Convolutional Neural Networks with Variational Inference. arXiv:1806.05978 (2018).\n\nAnticipating Traffic Accidents with Adaptive Loss and Large-scale Incident DB. Tomoyuki Suzuki, Hirokatsu Kataoka, Yoshimitsu Aoki, Yutaka Satoh, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. IEEE Conference on Computer Vision and Pattern RecognitionTomoyuki Suzuki, Hirokatsu Kataoka, Yoshimitsu Aoki, and Yutaka Satoh. 2018. Anticipating Traffic Accidents with Adaptive Loss and Large-scale Incident DB. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.\n\nPredicting Traffic Accidents with Event Recorder Data. Yoshiaki Takimoto, Yusuke Tanaka, Takeshi Kurashima, Shuhei Yamamoto, Maya Okawa, Hiroyuki Toda, Proceedings of ACM SIGSPATIAL International Workshop on Prediction of Human Mobility. ACM SIGSPATIAL International Workshop on Prediction of Human MobilityYoshiaki Takimoto, Yusuke Tanaka, Takeshi Kurashima, Shuhei Yamamoto, Maya Okawa, and Hiroyuki Toda. 2019. Predicting Traffic Accidents with Event Recorder Data. In Proceedings of ACM SIGSPATIAL International Workshop on Prediction of Human Mobility.\n\nAttention is All You Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, Illia Polosukhin, Proceedings of Neural Information Processing Systems. Neural Information Processing SystemsAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All You Need. In Proceedings of Neural Information Processing Systems.\n\nAggregated Residual Transformations for Deep Neural Networks. Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, Kaiming He, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. IEEE Conference on Computer Vision and Pattern RecognitionSaining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. 2017. Aggregated Residual Transformations for Deep Neural Networks. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.\n\nUnsupervised Traffic Accident Detection in First-person Videos. Yu Yao, Mingze Xu, Yuchen Wang, J David, Ella M Crandall, Atkins, International Conference on Intelligent Robots and Systems. Yu Yao, Mingze Xu, Yuchen Wang, David J Crandall, and Ella M Atkins. 2019. Unsupervised Traffic Accident Detection in First-person Videos. In International Conference on Intelligent Robots and Systems.\n\nBDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning. Fisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, Trevor Darrell, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. IEEE Conference on Computer Vision and Pattern RecognitionFisher Yu, Haofeng Chen, Xin Wang, Wenqi Xian, Yingying Chen, Fangchen Liu, Vashisht Madhavan, and Trevor Darrell. 2020. BDD100K: A Diverse Driving Dataset for Heterogeneous Multitask Learning. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.\n\nAgent-Centric Risk Assessment: Accident Anticipation and Risky Region Localization. Kuo-Hao Zeng, Shih-Han Chou, Fu-Hsiang Chan, Juan Carlos Niebles, Min Sun, Proceedings of IEEE Conference on Computer Vision and Pattern Recognition. IEEE Conference on Computer Vision and Pattern RecognitionKuo-Hao Zeng, Shih-Han Chou, Fu-Hsiang Chan, Juan Carlos Niebles, and Min Sun. 2017. Agent-Centric Risk Assessment: Accident Anticipation and Risky Region Localization. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition.\n\nBayesian Graph Convolution LSTM for Skeleton Based Action Recognition. Rui Zhao, Kang Wang, Hui Su, Qiang Ji, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionRui Zhao, Kang Wang, Hui Su, and Qiang Ji. 2019. Bayesian Graph Convolu- tion LSTM for Skeleton Based Action Recognition. In Proceedings of the IEEE International Conference on Computer Vision.\n", "annotations": {"author": "[{\"end\":143,\"start\":132},{\"end\":164,\"start\":144},{\"end\":189,\"start\":165},{\"end\":201,\"start\":190},{\"end\":208,\"start\":202},{\"end\":217,\"start\":209}]", "publisher": null, "author_last_name": "[{\"end\":142,\"start\":139},{\"end\":149,\"start\":147},{\"end\":172,\"start\":168},{\"end\":200,\"start\":197},{\"end\":207,\"start\":205},{\"end\":216,\"start\":212}]", "author_first_name": "[{\"end\":138,\"start\":132},{\"end\":146,\"start\":144},{\"end\":167,\"start\":165},{\"end\":196,\"start\":190},{\"end\":204,\"start\":202},{\"end\":211,\"start\":209}]", "author_affiliation": null, "title": "[{\"end\":89,\"start\":1},{\"end\":306,\"start\":218}]", "venue": "[{\"end\":383,\"start\":308}]", "abstract": "[{\"end\":3121,\"start\":1006}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3381,\"start\":3378},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3383,\"start\":3381},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3386,\"start\":3383},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3389,\"start\":3386},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5668,\"start\":5665},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5671,\"start\":5668},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5746,\"start\":5742},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5749,\"start\":5746},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6089,\"start\":6086},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6092,\"start\":6089},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6640,\"start\":6637},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6643,\"start\":6640},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6791,\"start\":6787},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7130,\"start\":7127},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7143,\"start\":7139},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8603,\"start\":8600},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8867,\"start\":8863},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8957,\"start\":8954},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9077,\"start\":9074},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9354,\"start\":9350},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9517,\"start\":9513},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9695,\"start\":9691},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9910,\"start\":9906},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10126,\"start\":10122},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11234,\"start\":11230},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11237,\"start\":11234},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11272,\"start\":11269},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11571,\"start\":11567},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11742,\"start\":11738},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11899,\"start\":11896},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11902,\"start\":11899},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":11930,\"start\":11926},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12242,\"start\":12239},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13301,\"start\":13298},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15356,\"start\":15353},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":15359,\"start\":15356},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15392,\"start\":15389},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":15395,\"start\":15392},{\"end\":16192,\"start\":16188},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17580,\"start\":17576},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17592,\"start\":17589},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":17861,\"start\":17857},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17937,\"start\":17933},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18468,\"start\":18465},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18471,\"start\":18468},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":19386,\"start\":19383},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19389,\"start\":19386},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20923,\"start\":20920},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20926,\"start\":20923},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":20929,\"start\":20926},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21166,\"start\":21163},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":21589,\"start\":21586},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":22739,\"start\":22735},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22852,\"start\":22848},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":22855,\"start\":22852},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23511,\"start\":23507},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":24142,\"start\":24138},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24966,\"start\":24962},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":25420,\"start\":25416},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26267,\"start\":26263},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26978,\"start\":26975},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":27025,\"start\":27021},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":28437,\"start\":28433},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":28654,\"start\":28651},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":28667,\"start\":28663},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":28737,\"start\":28734},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":29168,\"start\":29164},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":29731,\"start\":29728},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":31181,\"start\":31177},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":31374,\"start\":31370},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":31383,\"start\":31379},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":31443,\"start\":31439},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":31506,\"start\":31502},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":31666,\"start\":31663},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":32286,\"start\":32283},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":32289,\"start\":32286},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":32292,\"start\":32289},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":35918,\"start\":35915},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":36622,\"start\":36619}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":37734,\"start\":37704},{\"attributes\":{\"id\":\"fig_2\"},\"end\":37953,\"start\":37735},{\"attributes\":{\"id\":\"fig_3\"},\"end\":38147,\"start\":37954},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":38413,\"start\":38148},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":38621,\"start\":38414},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":38905,\"start\":38622},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":39114,\"start\":38906}]", "paragraph": "[{\"end\":3719,\"start\":3137},{\"end\":4498,\"start\":3721},{\"end\":5552,\"start\":4500},{\"end\":7045,\"start\":5554},{\"end\":7668,\"start\":7047},{\"end\":7728,\"start\":7670},{\"end\":8379,\"start\":7730},{\"end\":10859,\"start\":8430},{\"end\":11473,\"start\":10898},{\"end\":12522,\"start\":11475},{\"end\":13404,\"start\":12542},{\"end\":13590,\"start\":13448},{\"end\":14642,\"start\":13592},{\"end\":14723,\"start\":14644},{\"end\":15071,\"start\":14763},{\"end\":15643,\"start\":15073},{\"end\":15998,\"start\":15645},{\"end\":16288,\"start\":16035},{\"end\":16690,\"start\":16290},{\"end\":16891,\"start\":16752},{\"end\":17466,\"start\":16916},{\"end\":18109,\"start\":17468},{\"end\":18628,\"start\":18151},{\"end\":18906,\"start\":18679},{\"end\":19321,\"start\":18941},{\"end\":19918,\"start\":19323},{\"end\":20078,\"start\":19920},{\"end\":20944,\"start\":20080},{\"end\":21239,\"start\":20946},{\"end\":22378,\"start\":21312},{\"end\":22856,\"start\":22491},{\"end\":23387,\"start\":23053},{\"end\":23776,\"start\":23389},{\"end\":24269,\"start\":23826},{\"end\":24464,\"start\":24271},{\"end\":24905,\"start\":24504},{\"end\":25232,\"start\":24907},{\"end\":25861,\"start\":25234},{\"end\":26771,\"start\":25904},{\"end\":27122,\"start\":26796},{\"end\":29145,\"start\":27135},{\"end\":29574,\"start\":29147},{\"end\":30106,\"start\":29597},{\"end\":30937,\"start\":30108},{\"end\":31114,\"start\":30939},{\"end\":32200,\"start\":31141},{\"end\":33436,\"start\":32227},{\"end\":34728,\"start\":33438},{\"end\":35530,\"start\":34747},{\"end\":36846,\"start\":35532},{\"end\":37097,\"start\":36848},{\"end\":37703,\"start\":37112}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13447,\"start\":13405},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16034,\"start\":15999},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16751,\"start\":16691},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16915,\"start\":16892},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18150,\"start\":18110},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18678,\"start\":18629},{\"attributes\":{\"id\":\"formula_6\"},\"end\":21311,\"start\":21240},{\"attributes\":{\"id\":\"formula_7\"},\"end\":22456,\"start\":22379},{\"attributes\":{\"id\":\"formula_8\"},\"end\":22996,\"start\":22857},{\"attributes\":{\"id\":\"formula_9\"},\"end\":23052,\"start\":22996},{\"attributes\":{\"id\":\"formula_10\"},\"end\":23825,\"start\":23777},{\"attributes\":{\"id\":\"formula_11\"},\"end\":25903,\"start\":25862}]", "table_ref": "[{\"end\":27534,\"start\":27527},{\"end\":28631,\"start\":28624},{\"end\":31272,\"start\":31265},{\"end\":32341,\"start\":32334},{\"end\":32785,\"start\":32778},{\"end\":33294,\"start\":33287},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":33522,\"start\":33515},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":35529,\"start\":35522},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":36939,\"start\":36932}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3135,\"start\":3123},{\"attributes\":{\"n\":\"2\"},\"end\":8428,\"start\":8382},{\"attributes\":{\"n\":\"2.2\"},\"end\":10896,\"start\":10862},{\"attributes\":{\"n\":\"3\"},\"end\":12540,\"start\":12525},{\"attributes\":{\"n\":\"3.1\"},\"end\":14761,\"start\":14726},{\"attributes\":{\"n\":\"3.2\"},\"end\":18939,\"start\":18909},{\"attributes\":{\"n\":\"3.3\"},\"end\":22489,\"start\":22458},{\"attributes\":{\"n\":\"3.4\"},\"end\":24502,\"start\":24467},{\"attributes\":{\"n\":\"4\"},\"end\":26794,\"start\":26774},{\"attributes\":{\"n\":\"4.1\"},\"end\":27133,\"start\":27125},{\"attributes\":{\"n\":\"4.2\"},\"end\":29595,\"start\":29577},{\"attributes\":{\"n\":\"4.3\"},\"end\":31139,\"start\":31117},{\"attributes\":{\"n\":\"4.4\"},\"end\":32225,\"start\":32203},{\"attributes\":{\"n\":\"4.5\"},\"end\":34745,\"start\":34731},{\"attributes\":{\"n\":\"5\"},\"end\":37110,\"start\":37100},{\"end\":37715,\"start\":37705},{\"end\":37746,\"start\":37736},{\"end\":37966,\"start\":37955},{\"end\":38424,\"start\":38415},{\"end\":38632,\"start\":38623},{\"end\":38916,\"start\":38907}]", "table": "[{\"end\":38413,\"start\":38285},{\"end\":38621,\"start\":38576},{\"end\":38905,\"start\":38674},{\"end\":39114,\"start\":39030}]", "figure_caption": "[{\"end\":37734,\"start\":37717},{\"end\":37953,\"start\":37748},{\"end\":38147,\"start\":37968},{\"end\":38285,\"start\":38150},{\"end\":38576,\"start\":38426},{\"end\":38674,\"start\":38634},{\"end\":39030,\"start\":38918}]", "figure_ref": "[{\"end\":3869,\"start\":3863},{\"end\":5572,\"start\":5566},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":6499,\"start\":6493},{\"end\":10526,\"start\":10518},{\"end\":13024,\"start\":13018},{\"end\":13660,\"start\":13654},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14544,\"start\":14538},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19060,\"start\":19051},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19478,\"start\":19469},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19782,\"start\":19771},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19823,\"start\":19814},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25231,\"start\":25225},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28561,\"start\":28555},{\"end\":33982,\"start\":33976},{\"end\":36053,\"start\":36045}]", "bib_author_first_name": "[{\"end\":39652,\"start\":39645},{\"end\":39669,\"start\":39663},{\"end\":39686,\"start\":39681},{\"end\":39704,\"start\":39700},{\"end\":39968,\"start\":39963},{\"end\":39986,\"start\":39979},{\"end\":40002,\"start\":39995},{\"end\":40017,\"start\":40010},{\"end\":40312,\"start\":40305},{\"end\":40322,\"start\":40318},{\"end\":40696,\"start\":40689},{\"end\":40715,\"start\":40713},{\"end\":40725,\"start\":40722},{\"end\":41021,\"start\":41012},{\"end\":41033,\"start\":41027},{\"end\":41063,\"start\":41056},{\"end\":41079,\"start\":41074},{\"end\":41096,\"start\":41090},{\"end\":41113,\"start\":41107},{\"end\":41656,\"start\":41648},{\"end\":41668,\"start\":41664},{\"end\":41685,\"start\":41678},{\"end\":41700,\"start\":41692},{\"end\":41708,\"start\":41707},{\"end\":41722,\"start\":41716},{\"end\":42122,\"start\":42121},{\"end\":42134,\"start\":42133},{\"end\":42419,\"start\":42412},{\"end\":42438,\"start\":42432},{\"end\":42454,\"start\":42448},{\"end\":42841,\"start\":42837},{\"end\":42843,\"start\":42842},{\"end\":42856,\"start\":42852},{\"end\":43210,\"start\":43209},{\"end\":43218,\"start\":43217},{\"end\":43225,\"start\":43224},{\"end\":43233,\"start\":43232},{\"end\":43240,\"start\":43239},{\"end\":43248,\"start\":43247},{\"end\":43606,\"start\":43601},{\"end\":43618,\"start\":43612},{\"end\":43975,\"start\":43968},{\"end\":43990,\"start\":43984},{\"end\":44003,\"start\":43997},{\"end\":44408,\"start\":44404},{\"end\":44688,\"start\":44683},{\"end\":44710,\"start\":44705},{\"end\":44730,\"start\":44723},{\"end\":44746,\"start\":44742},{\"end\":44765,\"start\":44757},{\"end\":44780,\"start\":44772},{\"end\":45123,\"start\":45119},{\"end\":45142,\"start\":45136},{\"end\":45372,\"start\":45368},{\"end\":45387,\"start\":45382},{\"end\":45690,\"start\":45689},{\"end\":45704,\"start\":45701},{\"end\":45942,\"start\":45941},{\"end\":45954,\"start\":45951},{\"end\":46289,\"start\":46288},{\"end\":46301,\"start\":46298},{\"end\":46669,\"start\":46661},{\"end\":46684,\"start\":46676},{\"end\":46699,\"start\":46690},{\"end\":46717,\"start\":46705},{\"end\":47061,\"start\":47053},{\"end\":47072,\"start\":47067},{\"end\":47085,\"start\":47081},{\"end\":47506,\"start\":47502},{\"end\":47532,\"start\":47525},{\"end\":47881,\"start\":47875},{\"end\":47892,\"start\":47886},{\"end\":47904,\"start\":47900},{\"end\":48300,\"start\":48299},{\"end\":48502,\"start\":48497},{\"end\":48518,\"start\":48512},{\"end\":48536,\"start\":48530},{\"end\":48962,\"start\":48958},{\"end\":48974,\"start\":48971},{\"end\":48991,\"start\":48982},{\"end\":49003,\"start\":48999},{\"end\":49016,\"start\":49011},{\"end\":49034,\"start\":49027},{\"end\":49049,\"start\":49043},{\"end\":49065,\"start\":49059},{\"end\":49078,\"start\":49071},{\"end\":49095,\"start\":49091},{\"end\":49109,\"start\":49104},{\"end\":49128,\"start\":49121},{\"end\":49141,\"start\":49135},{\"end\":49155,\"start\":49148},{\"end\":49902,\"start\":49895},{\"end\":49921,\"start\":49917},{\"end\":49930,\"start\":49926},{\"end\":50324,\"start\":50318},{\"end\":50348,\"start\":50342},{\"end\":50362,\"start\":50358},{\"end\":50696,\"start\":50688},{\"end\":50709,\"start\":50702},{\"end\":50728,\"start\":50722},{\"end\":50750,\"start\":50744},{\"end\":51143,\"start\":51138},{\"end\":51154,\"start\":51150},{\"end\":51176,\"start\":51172},{\"end\":51188,\"start\":51185},{\"end\":51200,\"start\":51191},{\"end\":51540,\"start\":51535},{\"end\":51556,\"start\":51551},{\"end\":51572,\"start\":51566},{\"end\":52010,\"start\":52002},{\"end\":52028,\"start\":52019},{\"end\":52048,\"start\":52038},{\"end\":52061,\"start\":52055},{\"end\":52500,\"start\":52492},{\"end\":52517,\"start\":52511},{\"end\":52533,\"start\":52526},{\"end\":52551,\"start\":52545},{\"end\":52566,\"start\":52562},{\"end\":52582,\"start\":52574},{\"end\":53029,\"start\":53023},{\"end\":53043,\"start\":53039},{\"end\":53057,\"start\":53053},{\"end\":53071,\"start\":53066},{\"end\":53088,\"start\":53083},{\"end\":53101,\"start\":53096},{\"end\":53103,\"start\":53102},{\"end\":53117,\"start\":53111},{\"end\":53131,\"start\":53126},{\"end\":53521,\"start\":53514},{\"end\":53531,\"start\":53527},{\"end\":53547,\"start\":53542},{\"end\":53563,\"start\":53556},{\"end\":53575,\"start\":53568},{\"end\":53996,\"start\":53994},{\"end\":54008,\"start\":54002},{\"end\":54019,\"start\":54013},{\"end\":54027,\"start\":54026},{\"end\":54039,\"start\":54035},{\"end\":54041,\"start\":54040},{\"end\":54402,\"start\":54396},{\"end\":54414,\"start\":54407},{\"end\":54424,\"start\":54421},{\"end\":54436,\"start\":54431},{\"end\":54451,\"start\":54443},{\"end\":54466,\"start\":54458},{\"end\":54480,\"start\":54472},{\"end\":54497,\"start\":54491},{\"end\":55004,\"start\":54997},{\"end\":55019,\"start\":55011},{\"end\":55035,\"start\":55026},{\"end\":55046,\"start\":55042},{\"end\":55053,\"start\":55047},{\"end\":55066,\"start\":55063},{\"end\":55527,\"start\":55524},{\"end\":55538,\"start\":55534},{\"end\":55548,\"start\":55545},{\"end\":55558,\"start\":55553}]", "bib_author_last_name": "[{\"end\":39661,\"start\":39653},{\"end\":39679,\"start\":39670},{\"end\":39698,\"start\":39687},{\"end\":39713,\"start\":39705},{\"end\":39977,\"start\":39969},{\"end\":39993,\"start\":39987},{\"end\":40008,\"start\":40003},{\"end\":40024,\"start\":40018},{\"end\":40316,\"start\":40313},{\"end\":40334,\"start\":40323},{\"end\":40711,\"start\":40697},{\"end\":40720,\"start\":40716},{\"end\":40731,\"start\":40726},{\"end\":40736,\"start\":40733},{\"end\":41025,\"start\":41022},{\"end\":41054,\"start\":41034},{\"end\":41072,\"start\":41064},{\"end\":41088,\"start\":41080},{\"end\":41105,\"start\":41097},{\"end\":41121,\"start\":41114},{\"end\":41129,\"start\":41123},{\"end\":41662,\"start\":41657},{\"end\":41676,\"start\":41669},{\"end\":41690,\"start\":41686},{\"end\":41705,\"start\":41701},{\"end\":41714,\"start\":41709},{\"end\":41732,\"start\":41723},{\"end\":41740,\"start\":41734},{\"end\":42131,\"start\":42123},{\"end\":42140,\"start\":42135},{\"end\":42430,\"start\":42420},{\"end\":42446,\"start\":42439},{\"end\":42468,\"start\":42455},{\"end\":42850,\"start\":42844},{\"end\":42862,\"start\":42857},{\"end\":43215,\"start\":43211},{\"end\":43222,\"start\":43219},{\"end\":43230,\"start\":43226},{\"end\":43237,\"start\":43234},{\"end\":43245,\"start\":43241},{\"end\":43251,\"start\":43249},{\"end\":43610,\"start\":43607},{\"end\":43629,\"start\":43619},{\"end\":43982,\"start\":43976},{\"end\":43995,\"start\":43991},{\"end\":44011,\"start\":44004},{\"end\":44415,\"start\":44409},{\"end\":44703,\"start\":44689},{\"end\":44721,\"start\":44711},{\"end\":44740,\"start\":44731},{\"end\":44755,\"start\":44747},{\"end\":44770,\"start\":44766},{\"end\":44785,\"start\":44781},{\"end\":45134,\"start\":45124},{\"end\":45154,\"start\":45143},{\"end\":45380,\"start\":45373},{\"end\":45391,\"start\":45388},{\"end\":45699,\"start\":45691},{\"end\":45711,\"start\":45705},{\"end\":45720,\"start\":45713},{\"end\":45949,\"start\":45943},{\"end\":45959,\"start\":45955},{\"end\":45968,\"start\":45961},{\"end\":46296,\"start\":46290},{\"end\":46306,\"start\":46302},{\"end\":46315,\"start\":46308},{\"end\":46674,\"start\":46670},{\"end\":46688,\"start\":46685},{\"end\":46703,\"start\":46700},{\"end\":46722,\"start\":46718},{\"end\":47065,\"start\":47062},{\"end\":47079,\"start\":47073},{\"end\":47094,\"start\":47086},{\"end\":47523,\"start\":47507},{\"end\":47542,\"start\":47533},{\"end\":47549,\"start\":47544},{\"end\":47884,\"start\":47882},{\"end\":47898,\"start\":47893},{\"end\":47913,\"start\":47905},{\"end\":48308,\"start\":48301},{\"end\":48314,\"start\":48310},{\"end\":48510,\"start\":48503},{\"end\":48528,\"start\":48519},{\"end\":48544,\"start\":48537},{\"end\":48969,\"start\":48963},{\"end\":48980,\"start\":48975},{\"end\":48997,\"start\":48992},{\"end\":49009,\"start\":49004},{\"end\":49025,\"start\":49017},{\"end\":49041,\"start\":49035},{\"end\":49057,\"start\":49050},{\"end\":49069,\"start\":49066},{\"end\":49089,\"start\":49079},{\"end\":49102,\"start\":49096},{\"end\":49119,\"start\":49110},{\"end\":49133,\"start\":49129},{\"end\":49146,\"start\":49142},{\"end\":49162,\"start\":49156},{\"end\":49915,\"start\":49903},{\"end\":49924,\"start\":49922},{\"end\":49939,\"start\":49931},{\"end\":49944,\"start\":49941},{\"end\":50340,\"start\":50325},{\"end\":50356,\"start\":50349},{\"end\":50371,\"start\":50363},{\"end\":50700,\"start\":50697},{\"end\":50720,\"start\":50710},{\"end\":50742,\"start\":50729},{\"end\":50758,\"start\":50751},{\"end\":51148,\"start\":51144},{\"end\":51170,\"start\":51155},{\"end\":51183,\"start\":51177},{\"end\":51210,\"start\":51201},{\"end\":51549,\"start\":51541},{\"end\":51564,\"start\":51557},{\"end\":51580,\"start\":51573},{\"end\":52017,\"start\":52011},{\"end\":52036,\"start\":52029},{\"end\":52053,\"start\":52049},{\"end\":52067,\"start\":52062},{\"end\":52509,\"start\":52501},{\"end\":52524,\"start\":52518},{\"end\":52543,\"start\":52534},{\"end\":52560,\"start\":52552},{\"end\":52572,\"start\":52567},{\"end\":52587,\"start\":52583},{\"end\":53037,\"start\":53030},{\"end\":53051,\"start\":53044},{\"end\":53064,\"start\":53058},{\"end\":53081,\"start\":53072},{\"end\":53094,\"start\":53089},{\"end\":53109,\"start\":53104},{\"end\":53124,\"start\":53118},{\"end\":53142,\"start\":53132},{\"end\":53525,\"start\":53522},{\"end\":53540,\"start\":53532},{\"end\":53554,\"start\":53548},{\"end\":53566,\"start\":53564},{\"end\":53578,\"start\":53576},{\"end\":54000,\"start\":53997},{\"end\":54011,\"start\":54009},{\"end\":54024,\"start\":54020},{\"end\":54033,\"start\":54028},{\"end\":54050,\"start\":54042},{\"end\":54058,\"start\":54052},{\"end\":54405,\"start\":54403},{\"end\":54419,\"start\":54415},{\"end\":54429,\"start\":54425},{\"end\":54441,\"start\":54437},{\"end\":54456,\"start\":54452},{\"end\":54470,\"start\":54467},{\"end\":54489,\"start\":54481},{\"end\":54505,\"start\":54498},{\"end\":55009,\"start\":55005},{\"end\":55024,\"start\":55020},{\"end\":55040,\"start\":55036},{\"end\":55061,\"start\":55054},{\"end\":55070,\"start\":55067},{\"end\":55532,\"start\":55528},{\"end\":55543,\"start\":55539},{\"end\":55551,\"start\":55549},{\"end\":55561,\"start\":55559}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":1806222},\"end\":39928,\"start\":39606},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":51559},\"end\":40244,\"start\":39930},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":206596979},\"end\":40645,\"start\":40246},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":45520437},\"end\":40915,\"start\":40647},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":5590763},\"end\":41591,\"start\":40917},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1594370},\"end\":42047,\"start\":41593},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":92988418},\"end\":42330,\"start\":42049},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":3016223},\"end\":42767,\"start\":42332},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1258314},\"end\":43116,\"start\":42769},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":139107866},\"end\":43510,\"start\":43118},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2682206},\"end\":43895,\"start\":43512},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6724907},\"end\":44349,\"start\":43897},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":14885866},\"end\":44636,\"start\":44351},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":201668367},\"end\":45093,\"start\":44638},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1915014},\"end\":45289,\"start\":45095},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":71134},\"end\":45654,\"start\":45291},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":216078090},\"end\":45906,\"start\":45656},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":14249137},\"end\":46220,\"start\":45908},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3144218},\"end\":46530,\"start\":46222},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":51950520},\"end\":47000,\"start\":46532},{\"attributes\":{\"id\":\"b20\"},\"end\":47500,\"start\":47002},{\"attributes\":{\"doi\":\"arXiv:1506.00019\",\"id\":\"b21\"},\"end\":47790,\"start\":47502},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":16133383},\"end\":48258,\"start\":47792},{\"attributes\":{\"id\":\"b23\"},\"end\":48457,\"start\":48260},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":197632462},\"end\":48886,\"start\":48459},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":202786778},\"end\":49813,\"start\":48888},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":10328909},\"end\":50236,\"start\":49815},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":16895865},\"end\":50612,\"start\":50238},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":2687749},\"end\":51065,\"start\":50614},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":55779657},\"end\":51533,\"start\":51067},{\"attributes\":{\"doi\":\"arXiv:1806.05978\",\"id\":\"b30\"},\"end\":51921,\"start\":51535},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":4713643},\"end\":52435,\"start\":51923},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":207811019},\"end\":52994,\"start\":52437},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":13756489},\"end\":53450,\"start\":52996},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":8485068},\"end\":53928,\"start\":53452},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":67855477},\"end\":54321,\"start\":53930},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":215415900},\"end\":54911,\"start\":54323},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":5658191},\"end\":55451,\"start\":54913},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":207978618},\"end\":55877,\"start\":55453}]", "bib_title": "[{\"end\":39643,\"start\":39606},{\"end\":39961,\"start\":39930},{\"end\":40303,\"start\":40246},{\"end\":40687,\"start\":40647},{\"end\":41010,\"start\":40917},{\"end\":41646,\"start\":41593},{\"end\":42119,\"start\":42049},{\"end\":42410,\"start\":42332},{\"end\":42835,\"start\":42769},{\"end\":43207,\"start\":43118},{\"end\":43599,\"start\":43512},{\"end\":43966,\"start\":43897},{\"end\":44402,\"start\":44351},{\"end\":44681,\"start\":44638},{\"end\":45117,\"start\":45095},{\"end\":45366,\"start\":45291},{\"end\":45687,\"start\":45656},{\"end\":45939,\"start\":45908},{\"end\":46286,\"start\":46222},{\"end\":46659,\"start\":46532},{\"end\":47051,\"start\":47002},{\"end\":47873,\"start\":47792},{\"end\":48495,\"start\":48459},{\"end\":48956,\"start\":48888},{\"end\":49893,\"start\":49815},{\"end\":50316,\"start\":50238},{\"end\":50686,\"start\":50614},{\"end\":51136,\"start\":51067},{\"end\":52000,\"start\":51923},{\"end\":52490,\"start\":52437},{\"end\":53021,\"start\":52996},{\"end\":53512,\"start\":53452},{\"end\":53992,\"start\":53930},{\"end\":54394,\"start\":54323},{\"end\":54995,\"start\":54913},{\"end\":55522,\"start\":55453}]", "bib_author": "[{\"end\":39663,\"start\":39645},{\"end\":39681,\"start\":39663},{\"end\":39700,\"start\":39681},{\"end\":39715,\"start\":39700},{\"end\":39979,\"start\":39963},{\"end\":39995,\"start\":39979},{\"end\":40010,\"start\":39995},{\"end\":40026,\"start\":40010},{\"end\":40318,\"start\":40305},{\"end\":40336,\"start\":40318},{\"end\":40713,\"start\":40689},{\"end\":40722,\"start\":40713},{\"end\":40733,\"start\":40722},{\"end\":40738,\"start\":40733},{\"end\":41027,\"start\":41012},{\"end\":41056,\"start\":41027},{\"end\":41074,\"start\":41056},{\"end\":41090,\"start\":41074},{\"end\":41107,\"start\":41090},{\"end\":41123,\"start\":41107},{\"end\":41131,\"start\":41123},{\"end\":41664,\"start\":41648},{\"end\":41678,\"start\":41664},{\"end\":41692,\"start\":41678},{\"end\":41707,\"start\":41692},{\"end\":41716,\"start\":41707},{\"end\":41734,\"start\":41716},{\"end\":41742,\"start\":41734},{\"end\":42133,\"start\":42121},{\"end\":42142,\"start\":42133},{\"end\":42432,\"start\":42412},{\"end\":42448,\"start\":42432},{\"end\":42470,\"start\":42448},{\"end\":42852,\"start\":42837},{\"end\":42864,\"start\":42852},{\"end\":43217,\"start\":43209},{\"end\":43224,\"start\":43217},{\"end\":43232,\"start\":43224},{\"end\":43239,\"start\":43232},{\"end\":43247,\"start\":43239},{\"end\":43253,\"start\":43247},{\"end\":43612,\"start\":43601},{\"end\":43631,\"start\":43612},{\"end\":43984,\"start\":43968},{\"end\":43997,\"start\":43984},{\"end\":44013,\"start\":43997},{\"end\":44417,\"start\":44404},{\"end\":44705,\"start\":44683},{\"end\":44723,\"start\":44705},{\"end\":44742,\"start\":44723},{\"end\":44757,\"start\":44742},{\"end\":44772,\"start\":44757},{\"end\":44787,\"start\":44772},{\"end\":45136,\"start\":45119},{\"end\":45156,\"start\":45136},{\"end\":45382,\"start\":45368},{\"end\":45393,\"start\":45382},{\"end\":45701,\"start\":45689},{\"end\":45713,\"start\":45701},{\"end\":45722,\"start\":45713},{\"end\":45951,\"start\":45941},{\"end\":45961,\"start\":45951},{\"end\":45970,\"start\":45961},{\"end\":46298,\"start\":46288},{\"end\":46308,\"start\":46298},{\"end\":46317,\"start\":46308},{\"end\":46676,\"start\":46661},{\"end\":46690,\"start\":46676},{\"end\":46705,\"start\":46690},{\"end\":46724,\"start\":46705},{\"end\":47067,\"start\":47053},{\"end\":47081,\"start\":47067},{\"end\":47096,\"start\":47081},{\"end\":47525,\"start\":47502},{\"end\":47544,\"start\":47525},{\"end\":47551,\"start\":47544},{\"end\":47886,\"start\":47875},{\"end\":47900,\"start\":47886},{\"end\":47915,\"start\":47900},{\"end\":48310,\"start\":48299},{\"end\":48316,\"start\":48310},{\"end\":48512,\"start\":48497},{\"end\":48530,\"start\":48512},{\"end\":48546,\"start\":48530},{\"end\":48971,\"start\":48958},{\"end\":48982,\"start\":48971},{\"end\":48999,\"start\":48982},{\"end\":49011,\"start\":48999},{\"end\":49027,\"start\":49011},{\"end\":49043,\"start\":49027},{\"end\":49059,\"start\":49043},{\"end\":49071,\"start\":49059},{\"end\":49091,\"start\":49071},{\"end\":49104,\"start\":49091},{\"end\":49121,\"start\":49104},{\"end\":49135,\"start\":49121},{\"end\":49148,\"start\":49135},{\"end\":49164,\"start\":49148},{\"end\":49917,\"start\":49895},{\"end\":49926,\"start\":49917},{\"end\":49941,\"start\":49926},{\"end\":49946,\"start\":49941},{\"end\":50342,\"start\":50318},{\"end\":50358,\"start\":50342},{\"end\":50373,\"start\":50358},{\"end\":50702,\"start\":50688},{\"end\":50722,\"start\":50702},{\"end\":50744,\"start\":50722},{\"end\":50760,\"start\":50744},{\"end\":51150,\"start\":51138},{\"end\":51172,\"start\":51150},{\"end\":51185,\"start\":51172},{\"end\":51191,\"start\":51185},{\"end\":51212,\"start\":51191},{\"end\":51551,\"start\":51535},{\"end\":51566,\"start\":51551},{\"end\":51582,\"start\":51566},{\"end\":52019,\"start\":52002},{\"end\":52038,\"start\":52019},{\"end\":52055,\"start\":52038},{\"end\":52069,\"start\":52055},{\"end\":52511,\"start\":52492},{\"end\":52526,\"start\":52511},{\"end\":52545,\"start\":52526},{\"end\":52562,\"start\":52545},{\"end\":52574,\"start\":52562},{\"end\":52589,\"start\":52574},{\"end\":53039,\"start\":53023},{\"end\":53053,\"start\":53039},{\"end\":53066,\"start\":53053},{\"end\":53083,\"start\":53066},{\"end\":53096,\"start\":53083},{\"end\":53111,\"start\":53096},{\"end\":53126,\"start\":53111},{\"end\":53144,\"start\":53126},{\"end\":53527,\"start\":53514},{\"end\":53542,\"start\":53527},{\"end\":53556,\"start\":53542},{\"end\":53568,\"start\":53556},{\"end\":53580,\"start\":53568},{\"end\":54002,\"start\":53994},{\"end\":54013,\"start\":54002},{\"end\":54026,\"start\":54013},{\"end\":54035,\"start\":54026},{\"end\":54052,\"start\":54035},{\"end\":54060,\"start\":54052},{\"end\":54407,\"start\":54396},{\"end\":54421,\"start\":54407},{\"end\":54431,\"start\":54421},{\"end\":54443,\"start\":54431},{\"end\":54458,\"start\":54443},{\"end\":54472,\"start\":54458},{\"end\":54491,\"start\":54472},{\"end\":54507,\"start\":54491},{\"end\":55011,\"start\":54997},{\"end\":55026,\"start\":55011},{\"end\":55042,\"start\":55026},{\"end\":55063,\"start\":55042},{\"end\":55072,\"start\":55063},{\"end\":55534,\"start\":55524},{\"end\":55545,\"start\":55534},{\"end\":55553,\"start\":55545},{\"end\":55563,\"start\":55553}]", "bib_venue": "[{\"end\":40469,\"start\":40411},{\"end\":41280,\"start\":41214},{\"end\":41833,\"start\":41796},{\"end\":42561,\"start\":42524},{\"end\":42955,\"start\":42918},{\"end\":44146,\"start\":44088},{\"end\":44508,\"start\":44471},{\"end\":44878,\"start\":44841},{\"end\":45484,\"start\":45447},{\"end\":46083,\"start\":46035},{\"end\":47229,\"start\":47171},{\"end\":48048,\"start\":47990},{\"end\":48701,\"start\":48632},{\"end\":49362,\"start\":49250},{\"end\":50037,\"start\":50000},{\"end\":50851,\"start\":50814},{\"end\":52202,\"start\":52144},{\"end\":52744,\"start\":52675},{\"end\":53235,\"start\":53198},{\"end\":53713,\"start\":53655},{\"end\":54640,\"start\":54582},{\"end\":55205,\"start\":55147},{\"end\":55684,\"start\":55632},{\"end\":39759,\"start\":39715},{\"end\":40078,\"start\":40026},{\"end\":40409,\"start\":40336},{\"end\":40773,\"start\":40738},{\"end\":41212,\"start\":41131},{\"end\":41794,\"start\":41742},{\"end\":42181,\"start\":42142},{\"end\":42522,\"start\":42470},{\"end\":42916,\"start\":42864},{\"end\":43303,\"start\":43253},{\"end\":43694,\"start\":43631},{\"end\":44086,\"start\":44013},{\"end\":44469,\"start\":44417},{\"end\":44839,\"start\":44787},{\"end\":45174,\"start\":45156},{\"end\":45445,\"start\":45393},{\"end\":45774,\"start\":45722},{\"end\":46033,\"start\":45970},{\"end\":46369,\"start\":46317},{\"end\":46758,\"start\":46724},{\"end\":47169,\"start\":47096},{\"end\":47635,\"start\":47567},{\"end\":47988,\"start\":47915},{\"end\":48297,\"start\":48260},{\"end\":48630,\"start\":48546},{\"end\":49216,\"start\":49164},{\"end\":49998,\"start\":49946},{\"end\":50417,\"start\":50373},{\"end\":50812,\"start\":50760},{\"end\":51293,\"start\":51212},{\"end\":51716,\"start\":51598},{\"end\":52142,\"start\":52069},{\"end\":52673,\"start\":52589},{\"end\":53196,\"start\":53144},{\"end\":53653,\"start\":53580},{\"end\":54118,\"start\":54060},{\"end\":54580,\"start\":54507},{\"end\":55145,\"start\":55072},{\"end\":55630,\"start\":55563}]"}}}, "year": 2023, "month": 12, "day": 17}