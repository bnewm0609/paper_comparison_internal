{"id": 259978778, "updated": "2023-12-14 03:50:32.042", "metadata": {"title": "Lossy Scientific Data Compression With SPERR", "authors": "[{\"first\":\"Shaomeng\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Peter\",\"last\":\"Lindstrom\",\"middle\":[]},{\"first\":\"John\",\"last\":\"Clyne\",\"middle\":[]}]", "venue": "2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS)", "journal": "2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS)", "publication_date": {"year": 2023, "month": 5, "day": 1}, "abstract": "As the need for data reduction in high-performance computing (HPC) continues to grow, we introduce a new and highly effective tool to help achieve this goal\u2014SPERR. SPERR is a versatile lossy compressor for structured scientific data; it is built on top of an advanced wavelet compression algorithm, SPECK, and provides additional capabilities valued in HPC environments. These capabilities include parallel execution for large volumes and a compression mode that satisfies a maximum point-wise error tolerance. Evaluation shows that in most settings SPERR achieves the best rate-distortion trade-off among current popular lossy scientific data compressors.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/ipps/LiLC23", "doi": "10.1109/ipdps54959.2023.00104"}}, "content": {"source": {"pdf_hash": "d17bbb67c7bc31f336333ecfa34132bf51126aa0", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "6b48677b2140822766270f6b7ce91ff7da6f606e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d17bbb67c7bc31f336333ecfa34132bf51126aa0.txt", "contents": "\nLossy Scientific Data Compression With SPERR\n\n\nShaomeng Li \nLawrence Livermore Nat'l Lab\nNat'l Center for Atmospheric Research\nNat'l Center for Atmospheric Research\n\n\nPeter Lindstrom \nLawrence Livermore Nat'l Lab\nNat'l Center for Atmospheric Research\nNat'l Center for Atmospheric Research\n\n\nJohn Clyne \nLawrence Livermore Nat'l Lab\nNat'l Center for Atmospheric Research\nNat'l Center for Atmospheric Research\n\n\nLossy Scientific Data Compression With SPERR\n10.1109/IPDPS54959.2023.00104\nAs the need for data reduction in high-performance computing (HPC) continues to grow, we introduce a new and highly effective tool to help achieve this goal-SPERR. SPERR is a versatile lossy compressor for structured scientific data; it is built on top of an advanced wavelet compression algorithm, SPECK, and provides additional capabilities valued in HPC environments. These capabilities include parallel execution for large volumes and a compression mode that satisfies a maximum point-wise error tolerance. Evaluation shows that in most settings SPERR achieves the best rate-distortion trade-off among current popular lossy scientific data compressors.\n\nI. INTRODUCTION\n\nNumerical simulations running on high-performance computers (HPCs) face a growing gap between data generation and storage-the ability to generate data is very much outpacing the ability to store data. As a result, simulation scientists are forced to take mitigation measures, including outputting fewer variables, outputting less frequently, and with an increasing popularity, adopting a data compression strategy. Between the two types of compression, lossless and lossy compression, the former is of limited use in scientific data compression, because it only achieves very modest data reduction. The latter, however, offers the potential for significant data reduction, and is viable as long as the information loss does not lead to misinterpretation of scientific results. In this paper, we focus on lossy compression of floating-point data.\n\nAmongst the current leading lossy scientific data compression methods [1]- [10] there exists a good degree of variation in performance and capabilities, such as compression and decompression speed, support for an error guarantee, and ratedistortion trade-offs. Better rate-distortion trade-offs, for example using less storage for the same compression quality, may be prioritized in many applications. Consider large community data sets that can be analyzed by hundreds or even thousands of researchers, and have a lifetime lasting years. One example is the NCAR CESM LENS climate simulation data set [11], [12], which occupies 500 TB of on-premise storage. Another example is the Johns Hopkins Turbulence Database [13], [14], which hosts hundreds of terabytes of publicly accessible turbulence simulation outputs. In both cases the data sets may be written once, but accessed and/or transmitted repeatedly by researchers worldwide. Thus, achieved compression rates may trump other considerations such as computational speeds. Such scenarios serve as a motivation for our work.\n\nWhen performing lossy compression, a criterion is needed to decide when to terminate the coding process. Most termination criteria are expressed as either a size bound or an error bound. With a size-bounding criterion, the compression algorithm terminates when its output reaches a specified size, which is often expressed as a bitrate measured by bit-per-point, or BPP. With an error-bounding criterion, however, the algorithm strives to achieve the maximum data reduction without the resulting reconstructed data exceeding an error bound. Note that no compressor can generally satisfy size and error bounds simultaneously. For the lossy compression of scientific data, a preferred termination criterion is often an error bound, particularly a point-wise error (PWE) tolerance. A PWE tolerance t (t > 0) means that any data point cannot deviate from its true value by more than t as a result of lossy compression; it often serves as an intuitive quality control in practice.\n\nOne of the most efficient lossy compressors from a ratedistortion perspective is SPECK [15], [16], which encodes coefficients produced by wavelet transforms. SPECK only operates as a size-bounded compressor. The development of SPECK, like most other transform-based schemes targetting multi-media (e.g., imagery and video) compression, was driven by the desire to minimize average error, not the maximum PWE, under a size constraint. However, it is also our observation that SPECK often proportionately reduces the maximum PWE while minimizing average error; this observation motivates our approach to augment SPECK to support a PWE guarantee while keeping its low average error.\n\nWe have implemented an improved version of the SPECK algorithm. After compressing input data with our SPECK implementation, we find all reconstructed data points that exceed a specified PWE tolerance, which are referred to as outliers. We then record their positions and correction values needed to restore the outlier values to be within the PWE tolerance, also using a SPECK-inspired algorithm. Our software taking on this approach is named SPERR, which stands for SPEck with ERRor-bounding. SPERR offers the best performance of any of the leading compression technologies we have evaluated from a rate-distortion perspective. The rest of this paper describes in detail our major contributions:\n\n\u2022 an open-source scientific data compressor, SPERR, that features the ability to bound a maximum point-wise error (GitHub: https://github.com/NCAR/SPERR); \u2022 thorough discussion and evaluation of design choices we have made when developing SPERR; and \u2022 comparisons with leading scientific data compressors that provide a full profile of SPERR's characteristics.\n\nII. BACKGROUND The random nature of the least significant bits in floating point numbers, and specialized requirements-such as the de-sire to control point-wise errors-have led to the investigation and development of purpose-built scientific data compression technologies [1]- [10], [17]- [19]. Here, we briefly review only the current and most widely reported ones in the literature, which generally fall into two broad categories: predictionbased and transform-based.\n\nThe most widely reported prediction-based compressor is the SZ family of compressors [4]- [7], which have explored a variety of mathematical predictors, the most recent being splines [5]. The most widely reported transform-based compressor is ZFP [8], which employs a custom decorrelating transform, similar to the discrete cosine transform. SZ and ZFP were both developed in the last decade and have received great amounts of attention in both literature and practice.\n\nIn addition to these two established compression technologies, we reference two more recent additions: MGARD [2], [3] and TTHRESH [18]. The former is inspired by wavelet decompositions [20]- [24] and multi-grid methods used by linear solvers. TTHRESH is also transform-based, however, unlike most transform-based approaches that use predefined bases, TTHRESH uses the Tucker tensor decomposition [25] to generate data-dependent bases. The target application for TTHRESH is visualization, where bigger compression error is an acceptable trade-off for greater compression.\n\nOur compressor, SPERR, is based on wavelet transforms. The result of applying wavelet transforms to an input is a set of wavelet coefficients having the property of information compaction: most information is stored in a small percentage of coefficients, whose information content is proportional to their magnitude. Wavelet transforms are reversible and achieve no data reduction themselves; it is during the coefficient coding process that data reduction occurs and information loss may be introduced. A wide range of sophisticated algorithms have been developed to efficiently code the addresses and values of the small amount of most information-rich coefficients [15], [16], [26]- [28]. Among them, SPECK [15], [16] serves as the basis of our own compressor (more discussion in Section III).\n\nSPECK does not support a PWE guarantee though. Our approach to supporting such a guarantee is to explicitly encode positions and correction values of outlier residuals so we can later correct the offending reconstructed data points. The problem of encoding these positions and values is similar to compacting sparse matrices used in solving large linear systems. The most commonly employed methods for storing such matrices are Compressed Sparse Row (CSR) or Compressed Sparse Column (CSC) storage. However, CSR and CSC are far from optimal in our application because they still use naive storage to record element positions and values. Another approach is to record positions using bitmap coding [29], [30], and to handle correction values using, for example, variablelength coding (e.g., universal codes [31]). The SZ family of compressors takes this approach: quantized outlier correction values are stored as non-zero integers and then Huffman coded together with zero-valued inliers. As we shall show, our outlier coder accomplishes these two separate tasks in a unified manner with better efficiency.\n\n\nIII. WAVELET TRANSFORMS AND CODING\n\nThis section describes the specifics of our implementation of a biorthogonal wavelet transform and the SPECK wavelet coding algorithm, paying particular attention to improvements that we have made to better control the amounts of resulting outliers. Given the close ties between SPECK and our outlier coding algorithm (more detail in Section IV), this section also serves as a quick review of the essence of SPECK.\n\n\nA. Wavelet Transforms\n\nWe have chosen to use the CDF 9/7 biorthogonal wavelet transform [32] among a large selection of available wavelets, because it has proven to perform well in lossy scientific data compression, and possesses desirable properties such as near orthogonality and the ability to efficiently handle non-periodic input [20]- [24]. We have borrowed a lifting [33] implementation of the CDF 9/7 transform from QccPack, an excellent collection of signal processing tools [34]. This implementation uses symmetric boundary handling and wavelet basis functions with approximately unit norm. Because the CDF 9/7 wavelet functions are near-orthogonal and normalized, any L 2 error in wavelet coefficients introduced during encoding is approximately equal to the L 2 error in the reconstructed data.\n\nIn practice, wavelet transforms are applied recursively to an input array. The longer the array, the more transform passes can be applied, leading to better information compaction and thus compression. In our implementation, with an input array of length N , the number of recursive transforms applied is min(6, log 2 N \u2212 2). We choose to cap the recursion depth at six because of the diminishing benefit of deeply recursive wavelet transforms. Finally, for 3D volumes or 2D slices, transforms are separately applied along each axis to take advantage of data coherence in each dimension.\n\n\nB. Overview of the Classic SPECK Algorithm\n\nThe classic SPECK algorithm is designed to locate wavelet coefficients, and encode them bitplane-by-bitplane, from most to least significand [15], [16]. Given a bitplane with place value 2 n , all coefficients with magnitude at least 2 n -deemed significant coefficients-are located first. SPECK performs this task by \"zooming in\" from the full data volume or slice to individual significant coefficients. Specifically, SPECK performs spatial divisions to partition 3D volumes into octrees or 2D slices into quadtrees. Each subtree is then examined and designated as significant or insignificant based on whether or not it contains at least one significant coefficient, during which one bit per significance test is recorded. Only significant subtrees are further partitioned, until individual significant coefficients (leaf nodes in the tree) are reached. With the storage of the significance information of subtrees at each level, this \"zoom in\" procedure can be replayed during decoding. In the second iteration, which processes bitplane 2 n\u22121 , more coefficients will be deemed significant and recorded. Note that once a significant coefficient is discovered, it is stored together with all significant coefficients discovered from previous iterations, and does not partake in further significance tests.\n\nSignificant coefficients need to have their values recorded, which is accomplished with progressive precision using a quantized representation. Coefficients already found significant are maintained in a separate list, and a refinement pass appends the next significant bit of such coefficients before moving on to the next bitplane. The final reconstructed (nonzero) value usually has one additional one-bit appended as the least significant bit, resulting in mid-riser quantization that centers the value in its refined interval. Mid-tread quantization can also be achieved by rounding before applying SPECK if the place value of the last bitplane coded is known a priori.\n\nThe bitplane-by-bitplane fashion of SPECK coding naturally enables fixed-size compression, because the encoding process can terminate whenever a user-prescribed output size is reached, and the already-produced bitstream is always valid for decoding. This fixed-size compression capability is shared by most other transform-based compressors, such as ZFP.\n\n\nC. Arbitrary Quantization Thresholds\n\nWhen the original SPECK algorithm processes bitplanes with place value 2 n , n is always an integer. In our implementation, we have relaxed the integer requirement so any real value can be used in the algorithm, and more importantly, in the progressive quantization step. For example, denoting the finest quantization step size by q, the successive larger steps are 2q, 4q, 8q, etc., none necessarily being an integer power of two. Note that one way to look at this modification is that we pre-scale all coefficients by the reciprocal of q and then apply the original SPECK algorithm with n \u2265 0.\n\nSupporting arbitrary quantization thresholds provides us with critical control of compression quality. The quantization scheme used in our implementation explains how the finest quantization step size q affects the compression quality. First, this scheme has a dead zone of [\u2212q, q]; wavelet coefficients in the dead zone are not encoded and will be reconstructed as zeroes during decoding. Second, coefficients with a magnitude greater than q are quantized using a mid-riser quantizer, so that all values in the range of (iq, (i+1)q] are quantized to (i+ 1 2 )q, where i is an integer. The maximum quantization error for these coefficients is thus q 2 . Note that the error in the wavelet reconstruction may exceed q 2 as errors in overlapping wavelets may compound. Using this quantization scheme, a smaller q means both a smaller dead zone and smaller quantization error for every encoded coefficient, leading to a higher compression quality; a larger q, similarly, leads to a lower quality. Naturally, the better compression quality, the less outliers are produced, and vice versa. As a result, q controls not only SPECK compression quality, but also the amount of outliers need to be corrected. This relationship is demonstrated in Figure 2. Thus, allowing q to be an arbitrary value gives us great flexibility to direct SPECK to yield a desired amount of outliers, which is further explored in Section IV-D.\n\n\nD. Embarrassingly Parallel Computation\n\nOur implementation also includes an embarrassingly parallel strategy to take advantage of multi-core CPUs. Specifically, a big input volume is divided into multiple smaller chunks, and each chunk is processed individually in parallel. The output of each chunk is a separate bitstream, so these bitstreams are concatenated together to form the final compressed output. Our implementation uses OpenMP to orchestrate the parallel execution, and can support cases where the volume dimension is not divisible by the chunk dimension. Admittedly, this strategy imposes a limit on the degree of parallelization achievable-it cannot exceed the number of chunks. We plan to explore means to expose more parallelism in the future.\n\n\nIV. CODING OF OUTLIERS\n\nThis section describes in detail a SPECK-inspired algorithm that efficiently encodes outliers; this algorithm enables SPERR to bound a prescribed PWE tolerance. This section also investigates how to achieve a desirable storage balance between coefficient coding and outlier coding in SPERR.\n\n\nA. Algorithm Overview\n\nThe problem of outlier coding can be summarized as the following: by comparing the original data and wavelet reconstructed data, one can find a list of outliers that do not satisfy the user-defined PWE tolerance t, i.e.,\n|err i | = |x i \u2212x i | > t, wherex is the wavelet reconstruction of the original value x.\nThe task then is to record tuples of (pos, corr ) where posis the outlier's position within the input data, and corr is the correction value. A perfect correction value is corr = x\u2212x, so the SPERR reconstruction z would be perfect with z =x + corr . However, an error up to t is allowed, so we look at the PWE with an imperfect correction valuecorr :\n|z \u2212x| = |x+corr \u2212x| = |corr \u2212(x\u2212x)| = |corr \u2212err |. (1)\nThis equation shows that bounding the SPERR reconstruction PWE |z \u2212 x| is equivalent to bounding the correction value in the tuple (pos, corr ) so that |corr \u2212 err | \u2264 t. Now the problem of outlier coding is very similar to what SPECK was designed to address: both encode a tuple of position and value, where value is a quantized approximation of the original (see Section III-B). Outlier coding has a very specific termination criterion though, which is the point where all outliers are corrected to be within the tolerance. Another difference is that in our implementation, we linearize 2D or 3D inputs to 1D arrays. We will discuss the decision to linearize higher-dimension data in Section IV-C.\n\n\nB. Algorithm Description\n\nWe define the input to the outlier coding algorithm as:\n\n\u2022 Length of the 1D array: N ; \u2022 A user-defined PWE tolerance: t (t > 0); and \u2022 A list of outliers, each representing the position and correction value of an outlier: (pos, corr ) i = (pos, x \u2212 x) i . The algorithm output is a compact bitstream that can be used at a later time to reconstruct individual outliers (pos,corr ) i , where pos is exact andcorr i is an approximation.\n\nThe algorithm also uses the concept of significancewith respect to a quantization threshold thrd . If any one or more members in a set of data points (in this case, outliers) have a value magnitude greater than thrd , then this set is significant. Otherwise, this set is insignificant. Note that as outlier coding is a completely separate procedure from the coding of wavelet coefficients, thrd values used here are completely independent from those used in coefficient coding. SortingPass(thrd ) {details in Listing 2} 8: RefinementPass(thrd ) {details in Listing 3} 9: n \u2190 n \u2212 1 10: end while Listing 1 contains the pseudo-code of the encoding algorithm. At a high-level, this algorithm starts from the largest possible threshold that is an integer power-of-two multiple of the tolerance, locates significant data points with respect to that threshold (SortingPass()), and then refines them using quantization and bitplane coding (RefinementPass()). At the end of this iteration, outliers deemed significant with respect to thrd are located and refined by thrd . In the next iteration, the threshold is halved, thus more outliers will be deemed significant. New outliers, along with those identified from previous iterations, will be further refined with respect to the halved threshold. This operation iterates with smaller and smaller thresholds until thrd equals t, at which point all outliers will be deemed significant and encoded, and their reconstruction values will not deviate from the true values by more than t 2 , satisfying the PWE tolerance. Listing 2: Algorithm for SortingPass().\n\nAlgorithm: SortingPass(thrd ) 1: In increasing order of their sizes, for every set S that is already in LIS , do Process(S, thrd ).\n\n\n2:\n\nProcess(S, thrd )\n\n\n3:\n\nOutput a bit indicating if S is significant w.r.t. thrd .\n\n\n4:\n\nif output was true then 5: if S is a single point then 6: Output the sign of S.\n\n\n7:\n\nPut S in LNSP .  Code(S) 14: Equally divide S into two disjoint subsets: sub1 and sub2. 15: Put sub1 and sub2 in LIS . 16: Process(sub1, thrd ) 17:\nProcess(sub2, thrd ) {End of Code(S)}\nListing 2 presents the pseudo-code of SortingPass(), a subroutine that locates and moves from LIS to LNSP all previously insignificant points that become significant with respect to the current thrd . This process involves repeated binary set partitions until reaching individual significant data points (recursive function calls between Process() and Code()).\n\nDuring this process, more insignificant sets are generated and added to LIS , waiting to be processed in the next iteration, when a smaller threshold may test some of them significant. Listing 3 presents the pseudo-code of RefinementPass(), a subroutine that performs one level of refinement to the previously identified significant points, and also quantizes newlydiscovered significant points, both with respect to the current threshold thrd . When invoked repeatedly with decreasing thresholds, it refines these points with each iteration specifying a narrower range. During decoding, though the reconstructed value can be anywhere within a specified range, it is chosen at the middle of the range, same as the mid-riser quantizer described in Section III-C. Lines 5, 7, and 12 show how this quantization approach reconstructs correctors in a decoder.\n\nNote that all the algorithm output is in binary form, thus taking exactly one bit of storage. Every eight bits are then packed into a byte in the output bitstream. In this bitstream, every bit contains one of the following three types of information, depending on which step it was output from: 1) set significance (Line 3 of Listing 2); 2) outlier sign (Line 6 of Listing 2); or 3) direction in which to refine a value (Line 2 of Listing 3). A decoder then uses the same execution paths as the encoder, with significance test results coming from the bitstream. Quantized outlier values are also restored along the way, and decoding terminates when the bitstream is exhausted.\n\n\nC. Choice of Linearization\n\nOur outlier coding algorithm is inspired by SPECK, which is designed to take advantage of clustering of significant wavelet coefficients. As seen in Figure 1, unlike wavelet coefficients, very little spatial correlation exists between outliers; rather, they tend to appear at random positions. With little or no spatial correlation to exploit, we choose to flatten the sparse, multi-dimensional outlier arrays into a 1D array prior to encoding, which simplifies the software implementation.  Though there is little correlation for our outlier encoder to exploit, we do benefit from another important property of SPECK: the ability to efficiently encode position information along with variable-length coding for values. Section V-A and VI-E provide quantitative evaluations of this approach.\n\n\nD. Balance Between Wavelet Coefficient and Outlier Coding\n\nSPERR's primary goal is to minimize the storage cost for a given PWE tolerance. Secondarily, it also strives for a low average error. The total storage cost of SPERR consists of two components: the coding cost for wavelet coefficients and for One millionth of the data range 30\nRange 2 30 \u2248 Range \u00d7 10 \u22129\nOne billionth of the data range 40\nRange 2 40 \u2248 Range \u00d7 10 \u221212\nOne trillionth of the data range outliers. This section investigates how to balance the relative storage allocation between the two to best achieve our goals. A basic property of SPECK coding for wavelet coefficients is that the more bits are produced, the less average error it introduces. Most often, less average error leads to fewer outliers and less storage cost for outlier coding. This inverse relationship between the coding cost for wavelet coefficients and outliers is demonstrated in Figure 2, with the technique to adjust the storage balance formally explained later in this section. Given that both coding costs (coefficients and outliers) constitute the total SPERR storage and their inverse relationship, it is reasonable to hypothesize that there is a sweet spot where the total storage is minimal, as suggested by Figure 2. However, we have found that it is nontrivial to analytically model this relationship because it depends on the characteristics of the data set and on the user-provided PWE tolerance. As a result, we resort to empirical solutions.\n\nWe run a series of experiments with multiple data sets and compression settings to better understand where the sweet spot might lie. Here we report results of four fields from two data sets, which are representative of all of the data sets we have tested. Section VI-B provides more detail on these data sets. For each data field, we test four (for single-precision input) or five (for double-precision input) tolerance levels indicated by idx ; the actual PWE tolerance t is derived as\nt = Range 2 idx = max(f )\u2212min(f ) 2 idx\nof a data field f . Table I provides an intuitive understanding of this translation. The parameter passed to the compressor is the actual PWE tolerance, t, while idx merely serves as a label facilitating discussion in this paper. For each tolerance t, we then shift the balance between wavelet coefficient coding and outlier coding by adjusting q, the quantization step in coefficient coding (see Section III-C). The smaller q is, the larger coefficient coding output will be, using more coefficient coding. The bigger q is, the smaller coefficient coding output will be, using more outlier coding. In practice, we have observed that the optimal q is very close in magnitude to t, so for convenience, we express q in multiples of t. This experiment tests q ranging from q = t to q = 3t.\n\nThe top row of Figure 3 plots the relationship between q (horizontal axis) and the overall bitrate (vertical axis). The vertical axis represents the increased cost in overall bitrate relative to the minimum bitrate observed over all q settings. Not surprisingly, most curves take on a U shape, indicating the existence of a sweet spot that gives the lowest bitrate. The exact sweet spot varies from one field to another and from one compression level to another, but they are mostly in between q = 1.4t and q = 1.8t. There are also occasional occurrences where bitrate drops unexpectedly with large q values in fields Miranda Viscosity and Nyx Dark Matter Density. This behavior happens when as q increases, the storage reduced by wavelet coefficient coding outpaces the additional storage incurred by outlier coding. In our experience, this phenomenon appears to occur rather rarely with larger q values only, and has no practical impact on the location of the minimum.\n\nA secondary consideration of SPERR is the average error. Because outlier coding is almost always less efficient than wavelet coefficient coding at reducing average error, a modest increase in coefficient coding bitrate can potentially lead to a substantial average error reduction. The bottom row of Figure 3 examines this consideration by plotting the difference in achieved peak-signal-to-noise ratio (PSNR), which is inversely proportional to the logarithm of root-mean-square error, at the same experiment settings (q at the same steps). As before, these curves are drawn in comparison with the lowest achieved PSNR for each idx setting; the vertical axis shows the increase in PSNR (higher is better). This time, all the curves are monotonically decreasing, suggesting that shifting the balance to use more outlier coding only increases the average error. These plots also show that though two different points on the U-shaped curves in the top row can be using the same amount of storage to satisfy a PWE tolerance, they will yield quite different average errors. Combining these two considerations, and the sweet spot range of q = 1.4t to q = 1.8t, we conservatively choose q = 1.5t in our software implementation. Further evaluation in Section VI-C will show that this choice is satisfactory at providing competitive ratedistortion curves.\n\n\nV. EVALUATION: ASPECTS OF SPERR\n\nOur compressor SPERR comprises coding of wavelet coefficients and outliers (Section IV and III, respectively), with each step producing a bitstream. These two bitstreams are then concatenated and losslessly compressed by ZSTD [36] to become the final SPERR output. This section evaluates aspects of SPERR and our design choices.\n\n\nA. Outlier Coding Efficiency\n\nWe evaluate coding efficiency of the outlier coding algorithm, which is measured by bitrate of the outliers, the number of bits encoding all outliers over the number of outliers. Note that our implementation uses a header of a fixed size of twenty bytes; this cost is included in all evaluations in this paper.  Figure 4 presents this evaluation. It uses two data fields and two tolerance levels on each field. Again we vary the quantization step q for wavelet coefficient coding to adjust its quality, consequently the number of outliers produced. The percentage of outliers at each q is also plotted.\n\nThis evaluation shows that the cost of outlier coding (solid lines) is mostly between 6 to 16 bits per outlier. As q increases, more data points are identified as outliers, as the percentage curves (dashed lines) show. However, the amortized cost to encode individual outliers decreases, as the bitrate lines show, because each set significance test (Line 3 in Listing 2) catches a greater number of outliers that share the cost of that test. At q = 1.5t, the value our software implementation uses (see the last part of Section IV-D), this cost is approximately 10 bits per outlier. In our experience, this number is quite consistent across data sets. Section VI-E will provide a more detailed comparison between our strategy and the outlier coding method used by the SZ family of compressors. \n\n\nB. Chunk Size Impact on Compression Efficiency\n\nSPERR achieves parallelization by dividing large 3D volumes into smaller chunks. The size of chunks, however, has impact on compression efficiency: the smaller the chunks are, the more boundaries they produce, and wavelet compression tends to handle boundaries less effectively. Further, small chunk sizes limit the number of passes of wavelet transforms that can be performed (see Section III-A), which also negatively impacts compression efficiency. This section evaluates the impact on compression efficiency by chunk sizes.\n\nWe use accuracy gain, explored by Lindstrom [37], to measure compression efficiency. Accuracy gain is defined as\ngain = log 2 \u03c3 E \u2212 R (2)\nwhere \u03c3 is the standard deviation of the original data, E is the root-mean-square error, and R is the bitrate in BPP. A high accuracy gain is preferred and achieved when either the error, E, or rate, R, is low; intuitively, it measures the amount of information inferred by a compressor that need not be stored. Accuracy gain relates to the more common signal-to-noise ratio (SNR) by gain = SNR 20 log 10 2 \u2212R \u2248 SNR 6.02 \u2212R and essentially flattens the 6.02 dB/bit slope commonly exhibited by SNR plots, thus compacting the vertical range and emphasizing differences in quality. Another benefit of accuracy gain is that it incorporates both average error and the storage used into a single number, allowing comparisons of two lossy-compressed data sets when neither their rate nor error matches. Figure 5 plots differences in accuracy gain with various chunk sizes, using a cutout of 1, 024 3 at the center of the 3, 072 3 Miranda Density field. Unsurprisingly, bigger chunks bring higher accuracy gain. At the same time, the benefit diminishes with very large chunks. One may also observe that efficiency is even more impacted by chunk sizes for smaller error tolerances (bigger idx ). Given that the chunk size also dictates the degree of parallelism that may be achieved (see Section III-D), in practice, we find that the range between 128 3 and 256 3 is preferable because it provides good compression efficiency and also enables multi-way parallelization. Our software uses a default chunk size of 256 3 , but it needs not to be powers of two nor divisible by the volume dimensions.\n\n\nC. Compression Time Breakdown\n\nSPERR has four major steps in its data processing pipeline: 1) wavelet transform on the input data; 2) SPECK coding of  Fig. 7: Scalability test using up to 126 CPU cores with three tolerance levels. Note that the y-axis is on a logarithmic scale. wavelet coefficients; 3) locating outliers, which consists of an inverse wavelet transform and a comparison with the original input data; and 4) encoding located outliers. This section reports computational time on these four steps separately. We run this experiment on a compute node equipped with two 64-core AMD Epyc 7763 CPUs (128 cores in total), 256 GB of system memory, and SPERR compiled using GCC 11.2.0. Figure 6 reports this time breakdown in a serial compression of Miranda Viscosity at 384 2 \u00d7256. Though results on only one field are reported here, the time breakdown is representative of all fields that we have tested. There are five PWE tolerance levels tested, which are labeled by idx (see idx explanation in the third paragraph of Section IV-D). The result shows an increasing total compression time as the PWE tolerance tightens, mostly due to the increased SPECK coding time. SPECK time increases because tighter PWE tolerances lead to both more wavelet coefficients being located and that they are encoded with finer quantization precisions (see Section III-C). Wavelet transform time remains constant because those transforms are performed regardless of the tolerance level. Outlier locating and coding time also remains quite stable because the number of outliers does not fluctuate much, which is a design objective and is achieved by our strategy of choosing the quantization step size q, as discussed in Section IV-D.\n\n\nD. Scalability Test\n\nThis section explores how well SPERR scales on modern multi-core CPU architectures by conducting a strong scaling experiment. The test platform is the same 128-core compute node described in Section V-C. This experiment uses a cutout of 2, 048 3 from the 3, 072 3 Miranda Density field in single precision; the cutout is due to memory capacity limits on the test platform. With SPERR's default chunk size of 256 3 , 512-way parallelization is possible. We test on three PWE tolerance levels labeled as idx = 10, 15, and 20. For each tolerance level, we increase the number of OpenMP threads, thus CPU cores in use, from one to 126. (We do not use up all 128 CPU cores because it is recommended practice to leave a few CPU cores for system processes.) Figure 7 reports achieved parallel speedups compared to the serial execution time in a compression task. The result shows a close-to-linear speedup with up to 16 cores, and a gradually slower increase afterwards. Further, the speedup is seemingly approaching a plateau after 64 cores, signaling limitations of our embarrassingly parallel strategy.\n\n\nVI. EVALUATION: COMPARING WITH OTHER TOOLS\n\n\nA. Other Compressors And Test Environment\n\nWe have used SPERR version 0.4 for all comparisons in this study. Other scientific data compressors used in these comparisons are SZ3 [5], ZFP [8], TTHRESH [18], and MGARD [2]. All compressors, including our SPERR, are open source and publicly available. Section II has a brief description of these compressors, and this section provides the specifics in our experiment. OpenMP backend. All five compressors are compiled using GCC 11.2.0. All comparisons in this section are conducted on a compute node equipped with two 18-core Intel Xeon 6240 processors and 394 GB system memory, running CentOS 7 operating system.\n\n\nB. Testing Data Sets\n\nOur tests use openly available data sets from SDRBench, a collection of representative scientific data sets frequently used in literature for compression evaluation [38]. Specifically, we have chosen four simulations briefly described blow. The SDRBench website has more details on each of them.\n\n\u2022 Miranda: hydrodynamics turbulence simulation with double-precision output at 384 2 \u00d7 256 and singleprecision output at 3, 072 3 ; \u2022 S3D: combustion, double-precision output at 500 3 ; \u2022 Nyx: cosmology, single-precision output at 512 3 ; and \u2022 QMCPACK: ab initio quantum Monte Carlo simulation, single-precision output at 69 2 \u00d7 115 \u00d7 288 (288 orbitals). The QMCPACK data set is essentially a stack of 3D volumes of size 69 2 \u00d7 115, which is best to be compressed as 288 individual volumes. SPERR is configured to do so with its chunk size specified as 69 2 \u00d7 115. The other compressors are configured to process a single 3D volume of size 69 2 \u00d7 33120 (33120 = 115 \u00d7 288), which is less than ideal, but is also listed as example configurations on SDRBench.\n\n\nC. Comparison Of Compression Efficiency\n\nThis section compares the efficiency of SPERR with other leading compressors. We again use accuracy gain (see Section V-B) as the main metric, and an integer idx to label PWE tolerance levels (see Table I). Note that it is the PWE tolerance t, rather than idx , passed in as the quality control parameter to compressors. We increment idx from zero to the point where t is approaching machine epsilon. For singleprecision input (Nyx and QMCPACK), the upper bound on idx reaches between 25 and 35, and for double-precision input (S3D and Miranda), the upper bound reaches between 50 and 60. We note that when t is tight MGARD cannot bound the error tolerance 1 and TTHRESH starts to use significantly more bits without reducing average error nor the maximum PWE. When such unexpected behavior occurs, the offending test is terminated, and not included in the presentation here. (TTHRESH requires some special attention: it supports a target average error (e.g., PSNR) but not a PWE guarantee. As a result, at each idx we prescribe for TTHRESH PSNR = (20 log 10 2) \u00d7 idx , which results in a halving of root-mean-square error for each increment to idx . Lastly, TTHRESH was not able to finish computation on data set QMCPACK, so is not included in results for QMCPACK.) Figure 8 presents comparison results over nine data fields using rate-distortion curves. We use logarithmic scale on the x-axis so low-rate, low-quality compression regions are clearly shown. Compression in these regions, for example most compressors achieve around 50 dB in PSNR at 1 32 BPP, may already provide sufficient quality for applications such as visualization. There are a few interesting observations. First, the curves increase at low rates, indicating that significant compression occurs, where each halving in error, E, incurs less than one additional bit of compressed storage, R (see Equation 2). Most curves then reach a stable plateau, where each additional bit encoded halves the error, indicating that random trailing significant bits have been reached. This plateau spans a wide rate range, which is often 10-to 20-bit wide, though it appears horizontally \"compacted\" by the logarithmic scale in these plots. Second, SPERR exhibits a clear advantage in achieving high accuracy gains at mid-to-high rates (i.e., more than 2 BPP), while remains competitive at low rates (i.e., less than 1 BPP). In most scientific applications involving quantitative analyses, we argue that high-rate, high-quality compression is much more useful and frequently requested than low-rate, low-quality compression. Third, not all curves exhibit the same level of smoothness, which indicates different degrees of predictability of a compressor's behavior. On this account, SPERR is one of the more predictable compressors. Overall, these rate-distortion plots suggest SPERR's outstanding compression efficiency over a wide range of bitrates.\n\nAnother aspect of the efficiency comparison is how many bits a compressor needs to satisfy a PWE tolerance, regardless of the overall average error. We perform this test using the same data fields as in Figure 8 with a selection of tolerance Fig. 8: Rate-distortion curves comparing five compressors on nine data fields. The x-axis is the achieved bitrate measured in BPP, and the y-axis is accuracy gain as defined in Section V-B. Note that the x-axis is plotted in logarithmic scale.  Table II.  levels. This time TTHRESH is not tested because it does not have an error-bounded compression mode. MGARD is also not presented at idx = 40 tolerance levels because it gives results obviously exceeding the error tolerance. Figure 9 presents our test results, using abbreviations for field names and tolerance levels explained in Table II. These results show that SPERR uses the least number of bits to guarantee a given PWE tolerance in all but two cases, again highlighting the superior compression efficiency of SPERR. Finally, we note that the main evaluation metric used here, accuracy gain, is generic in nature and aims to provide an overview of SPERR's characteristics. Evaluations using more domain-specific metrics (e.g., SSIM [39]) are likely necessary to determine SPERR's applicability in a particular use case.\n\n\nD. Comparison of Runtime Performance\n\nThis section compares parallel runtime performance on multi-core CPUs. All five compressors support parallelization through OpenMP, which we have enabled during compilation. This experiment uses the same data fields and tolerance levels summarized in Table II. There are two considerations: first, MGARD is not presented at idx = 40 tolerance levels, because it gives results obviously exceeding the error tolerance. Second, TTHRESH takes in PSNR targets, instead of PWE  Table II. Fig. 11: Comparison of outlier coding efficiency between SPERR and SZ. The x-axis is different data fields and tolerance levels (see Table II), and the y-axis is the coding cost of outliers. Note that only the cost above 5 bits is shown.\n\ntolerances, because average errors are the only parameters that TTHRESH accepts. Using the same formula discussed and used in Section VI-C, these PSNR targets for TTHRESH are determined as 120.41 dB at idx = 20, and 240.82 dB at idx = 40. In fact, these PSNR targets are quite close to the PSNR values SPERR achieved at respective tolerance levels. Figure 10 presents this runtime comparison using four OpenMP threads. The reported numbers are wall clock times of invoking respective command-line tools to perform a compression task. Note that eight runs have time exceeding the graph scale, so their respective bars are truncated with a text label indicating the actual time. These results show that SZ3 and ZFP are comparable and are extremely fast compared to the rest compressors. SPERR runs a few times slower than SZ3 and ZFP, but is also significantly faster than TTHRESH. Finally, SPERR performs comparably with MGARD.\n\n\nE. Comparison Of Outlier Coding Efficiency\n\nTo the best of our knowledge, SPERR's approach to providing a PWE guarantee-explicitly encoding corrections to outliers that violate the PWE tolerance t-is shared by only one other lossy compressor: SZ. SZ uses a different scheme to encode outliers. It first quantizes prediction errors to integer multiples of 2t, analogous to SPERR's correction values. SZ then uses Huffman coding to efficiently encode these integers [6]. The Huffman codes and tree are finally compressed by ZSTD [36]. We perform experiments to compare outlier coding efficiency between SPERR and SZ by first intercepting SPERR's data processing pipeline to obtain the list of outliers, and then feeding the same list of outliers to respective outlier coding schemes (SPERR's and SZ's).\n\nTo prepare the outliers for SZ, we first quantize the SPERR outlier correction values as multiples of the PWE tolerance. SZ encodes a correction value for every data point, where inliers are represented as zero-valued correctors. Hence, SZ does not code the positions of outliers. Unlike in SZ, which usually uses thousands of quantization bins, the SPERR correctors are usually small; we observed no corrector outside the range of {\u22124, . . . , 4}. Finally, we note that SZ provides a separate tool, compressQuantBins, in its QCAT package [40] to perform this coding task. We used compressQuantBins in this experiment. Figure 11 presents this comparison using the same data fields and compression levels described in Table II. It uses bitper-outlier, the average number of bits to encode an outlier, to measure the coding efficiency. This figure shows that SPERR uses around ten bits to encode one outlier across all experiment settings, consistent with our findings in Section V-A. It also shows that SPERR consistently uses fewer bits than SZ to encode the same set of outliers, usually by a 1-to 2-bit margin.\n\n\nVII. FUTURE WORK AND CONCLUSION\n\nSPERR has great potential to grow its capabilities. First, the property of roughly equal root-mean-square error between wavelet coefficients and their inversely transformed reconstruction (see Section III-A) enables estimating compression error without much computational overhead, thus compression targeting an average error is feasible. Second, wavelet transforms naturally represent data as hierarchies with selfsimilarities, i.e., each coarsened hierarchy level resembles the full-resolution data. This hierarchy enables multi-level reconstruction that is useful in areas such as explorative analysis. Third, the bitplane-by-bitplane quantization scheme in SPECK means that the output bitstream is embedded, so any prefix of the bitstream can reconstruct a less-accurate version of the data, in addition to the error-bounded version provided by the full bitstream. This embedded property makes SPERR suitable for streaming applications where data reconstruction using a partially transmitted bitstream, though less accurate, may still be appreciated. Fourth, as we have alluded, there is still great opportunity to improve SPERR's runtime performance, potentially through parallelization schemes that are more sophisticated and allow for porting the algorithm to GPUs.\n\nIn conclusion, this paper has introduced SPERR, a new tool for lossy scientific data compression on HPCs. SPERR can provide both size-bounded and maximum point-wise error (PWE) bounded compression; the latter ability is provided by a novel use of an outlier coding algorithm that explicitly corrects data points exceeding a prescribed PWE tolerance. We have thoroughly evaluated SPERR and compared it against leading scientific data compressors; these studies demonstrate that SPERR has one of the most competitive compression efficiencies with the drawback of a higher computational cost.\n\nListing 1 :\n1Overall outlier encoding algorithm. The three data structures LSP , LNSP and LIS are globally accessible.Algorithm: OutlierCoder () Require: A PWE tolerance t, a list of outlier tuples, (pos, corr )i, and the array length N . 1: Save the signs of corr i separately, then corr i \u2190 |corr i|. 2: Create two global lists representing existing and newly-found significant points, LSP and LNSP . Both are initially empty. 3: Create a global list representing insignificant sets, LIS , which is initialized to be the entire array [0, N ). 4: Find the maximum integer n \u2265 0 so that 2 n t < max(corr i). 5: while n \u2265\n\n\n{End of Process(S, thrd )} 13:\n\nFig. 1 :\n1A typical outlier heat map (produced from the Lighthouse image from the Kodak Image Suite[35]) where brown dots represent the positions of outliers. Little, if any, correlation between outlier positions can be observed. The subfigures represent three outlier percentage levels (noted above figures) which are controlled by the coefficient-outlier coding balance from q = 1.3t to q = 1.7t (more details in Section IV-D).\n\nFig. 2 :\n2Total coding cost as a function of quantization step, q, expressed in units of the error tolerance, t, for the Miranda Pressure field at tolerance level t = 3.64 \u00d7 10 \u221211 . The coding cost is broken out into the cost to code wavelet coefficients and outliers; the percentage labels indicate the portion of the coding cost associated with outliers. Note that only the cost above 10 BPP is shown in this plot.\n\nFig. 3 :\n3Relative difference of bitrate (top) and average error (bottom) compared to the lowest observed values. The x-axis is quantization step q in coefficient coding expressed in units of the error tolerance t, and the y-axis is difference in BPP (top) and decibel, dB (bottom). For each field, multiple tolerance levels, t \u221d 2 \u2212idx , are tested, as the idx numbers indicate.\n\nFig. 4 :\n4Outlier bitrate (solid lines) and percentage (dashed lines) at different q values expressed in units of the PWE tolerance t. The bitrate is calculated over the number of outliers. Visc-20 and Visc-40 are Miranda Viscosity at tolerance level idx = 20 and idx = 40, and Nyx-20 and Nyx-30 are Nyx Dark Matter Density at idx = 20 and idx = 30.\n\nFig. 5 :\n5Difference in accuracy gain with various chunk sizes.Three idx values represent three tolerance levels: t = Range 2 idx .\n\nFig. 6 :\n6Execution time breakdown on field Miranda Viscosity.\n\n\u2022\nSZ3: version 3.1.5.4 with the default config file except for enabled OpenMP. \u2022 ZFP: version 1.0.0 with default configurations. \u2022 TTHRESH: commit number ae58002 (the latest code as of Sep. 9th 2022) with default configurations. \u2022 MGARD: version 1.3.0 with default configurations and\n\nFig. 9 :\n9Achieved bitrate on multiple data sets and tolerance levels. Abbreviations on the x-axis are explained in\n\n\n, idx = 20 and 40 Temp-20, Temp-40 S3D Temperature, idx = 20 and 40 VX1-20, VX1-40 S3D X Velocity, idx = 20 and 40 Press-20, Press-40 Miranda Pressure, idx = 20 and 40 Visc-20, Visc-40 Miranda Viscosity, idx = 20 and 40 VX2-20, VX2-40 Miranda X Velocity, idx = 20 and 40 QMC-20 QMCPACK, idx = 20 Nyx-20 Nyx Dark Matter Density, idx = 20 VX3-20 Nyx X Velocity, idx = 20\n\nFig. 10 :\n10Compression time on multiple data sets and tolerance levels. Abbreviations on the x-axis are explained in\n\n\nListing 3: Algorithm for RefinementPass().Output a bit indicating if corr > thrd . Append LNSP to the end of LSP , then reset LNSP to empty.Algorithm: RefinementPass(thrd ) \n1: for all p = (pos, corr ) \u2208 LSP do \n\n2: \n\n3: \n\nif output was true then \n\n4: \n\nEncoder: corr \u2190 corr \u2212 thrd \n\n5: \n\nDecoder: corr \u2190 corr + thrd \n\n2 \n\n6: \n\nelse \n\n7: \n\nDecoder: corr \u2190 corr \u2212 thrd \n\n2 \n\n8: \n\nend if \n9: end for \n10: for all p = (pos, corr ) \u2208 LNSP do \n\n11: \n\nEncoder: corr \u2190 corr \u2212 thrd \n\n12: \n\nDecoder: corr \u2190 3 \n2 thrd \n13: end for \n14: \n\nTABLE I :\nIGiven a field with a data range Range, translate a label idx to an actual PWE tolerance t.idx \nPWE Tolerance t \nUnderstanding of t \n10 \n\nRange \n\n2 10 \u2248 Range \u00d7 10 \u22123 \nOne thousandth of the data range \n20 \n\nRange \n\n2 20 \u2248 Range \u00d7 10 \u22126 \n\n\nTABLE II :\nIIExplanation of abbreviations for data fields and compression levels used inFigures 9, 10, and 11.\nAuthorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.\nSuspected bugs in MGARD have been reported to MGARD authors.1014Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.\nAuthorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply.\nACKNOWLEDGMENT This material is based upon work supported by the National Center for Atmospheric Research, which is a major facility sponsored by the National Science Foundation under Cooperative Agreement No. 1852977.This work was performed in part under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344 and was supported in part by the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research.\nBit grooming: statistically accurate precision-preserving quantization with compression. C S Zender, Geoscientific Model Development. 99evaluated in the netcdf operators (nco, v4. 4.4.8+C. S. Zender, \"Bit grooming: statistically accurate precision-preserving quantization with compression, evaluated in the netcdf operators (nco, v4. 4.4.8+),\" Geoscientific Model Development, vol. 9, no. 9, pp. 3199- 3211, 2016.\n\nMultilevel techniques for compression and reduction of scientific data-the univariate case. M Ainsworth, O Tugluk, B Whitney, S Klasky, Computing and Visualization in Science. 195M. Ainsworth, O. Tugluk, B. Whitney, and S. Klasky, \"Multilevel tech- niques for compression and reduction of scientific data-the univariate case,\" Computing and Visualization in Science, vol. 19, no. 5, pp. 65-76, 2018.\n\nMultilevel techniques for compression and reduction of scientific data-the multivariate case. SIAM Journal on Scientific Computing. 412--, \"Multilevel techniques for compression and reduction of scientific data-the multivariate case,\" SIAM Journal on Scientific Computing, vol. 41, no. 2, pp. A1278-A1303, 2019.\n\nSz3: A modular framework for composing prediction-based error-bounded lossy compressors. X Liang, K Zhao, S Di, S Li, R Underwood, A M Gok, J Tian, J Deng, J C Calhoun, D Tao, IEEE Transactions on Big Data. X. Liang, K. Zhao, S. Di, S. Li, R. Underwood, A. M. Gok, J. Tian, J. Deng, J. C. Calhoun, D. Tao et al., \"Sz3: A modular framework for composing prediction-based error-bounded lossy compressors,\" IEEE Transactions on Big Data, 2022.\n\nOptimizing error-bounded lossy compression for scientific data by dynamic spline interpolation. K Zhao, S Di, M Dmitriev, T.-L D Tonellot, Z Chen, F Cappello, 2021 IEEE 37th International Conference on Data Engineering (ICDE). IEEEK. Zhao, S. Di, M. Dmitriev, T.-L. D. Tonellot, Z. Chen, and F. Cappello, \"Optimizing error-bounded lossy compression for scientific data by dy- namic spline interpolation,\" in 2021 IEEE 37th International Conference on Data Engineering (ICDE). IEEE, 2021, pp. 1643-1654.\n\nSignificantly improving lossy compression for scientific data sets based on multidimensional prediction and error-controlled quantization. D Tao, S Di, Z Chen, F Cappello, 2017 IEEE International Parallel and Distributed Processing Symposium. IEEED. Tao, S. Di, Z. Chen, and F. Cappello, \"Significantly improving lossy compression for scientific data sets based on multidimensional prediction and error-controlled quantization,\" in 2017 IEEE International Parallel and Distributed Processing Symposium. IEEE, 2017, pp. 1129-1139.\n\nError-controlled lossy compression optimized for high compression ratios of scientific datasets. X Liang, S Di, D Tao, S Li, S Li, H Guo, Z Chen, F Cappello, 2018 IEEE International Conference on Big Data (Big Data. IEEEX. Liang, S. Di, D. Tao, S. Li, S. Li, H. Guo, Z. Chen, and F. Cappello, \"Error-controlled lossy compression optimized for high compression ratios of scientific datasets,\" in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 438-447.\n\nFixed-rate compressed floating-point arrays. P Lindstrom, IEEE Transactions on Visualization and Computer Graphics. 2012P. Lindstrom, \"Fixed-rate compressed floating-point arrays,\" IEEE Transactions on Visualization and Computer Graphics, vol. 20, no. 12, pp. 2674-2683, 2014.\n\nFast and efficient compression of floating-point data. P Lindstrom, M Isenburg, IEEE Transactions on Visualization and Computer Graphics. 125P. Lindstrom and M. Isenburg, \"Fast and efficient compression of floating-point data,\" IEEE Transactions on Visualization and Computer Graphics, vol. 12, no. 5, pp. 1245-1250, 2006.\n\nUsing neural networks for two dimensional scientific data compression. L Hayne, J Clyne, S Li, 2021 IEEE International Conference on Big Data (Big Data). L. Hayne, J. Clyne, and S. Li, \"Using neural networks for two di- mensional scientific data compression,\" in 2021 IEEE International Conference on Big Data (Big Data), 2021, pp. 2956-2965.\n\nThe community earth system model (cesm) large ensemble project: A community resource for studying climate change in the presence of internal climate variability. J E Kay, C Deser, A Phillips, A Mai, C Hannay, G Strand, J M Arblaster, S Bates, G Danabasoglu, J Edwards, Bulletin of the American Meteorological Society. 968J. E. Kay, C. Deser, A. Phillips, A. Mai, C. Hannay, G. Strand, J. M. Arblaster, S. Bates, G. Danabasoglu, J. Edwards et al., \"The community earth system model (cesm) large ensemble project: A community resource for studying climate change in the presence of internal climate variability,\" Bulletin of the American Meteorological Society, vol. 96, no. 8, pp. 1333-1349, 2015.\n\nThe community earth system model (cesm) large ensemble project. J Kay, C Deser, J. Kay and C. Deser, \"The community earth system model (cesm) large ensemble project,\" 2016. [Online]. Available: http: //www.cesm.ucar.edu/projects/community-projects/LENS\n\nA public turbulence database cluster and applications to study lagrangian evolution of velocity increments in turbulence. Y Li, E Perlman, M Wan, Y Yang, C Meneveau, R Burns, S Chen, A Szalay, G Eyink, Journal of Turbulence. 931Y. Li, E. Perlman, M. Wan, Y. Yang, C. Meneveau, R. Burns, S. Chen, A. Szalay, and G. Eyink, \"A public turbulence database cluster and applications to study lagrangian evolution of velocity increments in turbulence,\" Journal of Turbulence, no. 9, p. N31, 2008.\n\nData exploration of turbulence simulations using a database cluster. E Perlman, R Burns, Y Li, C Meneveau, Proceedings of the 2007 ACM/IEEE conference on Supercomputing. the 2007 ACM/IEEE conference on SupercomputingE. Perlman, R. Burns, Y. Li, and C. Meneveau, \"Data exploration of turbulence simulations using a database cluster,\" in Proceedings of the 2007 ACM/IEEE conference on Supercomputing, 2007, pp. 1-11.\n\nEfficient, lowcomplexity image coding with a set-partitioning embedded block coder. W A Pearlman, A Islam, N Nagaraj, A Said, IEEE Transactions on Circuits and Systems for Video Technology. 14W. A. Pearlman, A. Islam, N. Nagaraj, and A. Said, \"Efficient, low- complexity image coding with a set-partitioning embedded block coder,\" IEEE Transactions on Circuits and Systems for Video Technology, vol. 14, no. 11, pp. 1219-1235, 2004.\n\nThree-dimensional wavelet-based compression of hyperspectral images. X Tang, W A Pearlman, Hyperspectral Data Compression. SpringerX. Tang and W. A. Pearlman, \"Three-dimensional wavelet-based com- pression of hyperspectral images,\" in Hyperspectral Data Compression. Springer, 2006, pp. 273-308.\n\nProgressive data access for regular grids. J Clyne, High Performance Visualization, E. W. Bethel, H. Childs, and C. HansenJ. Clyne, \"Progressive data access for regular grids,\" in High Perfor- mance Visualization, E. W. Bethel, H. Childs, and C. Hansen, Eds. CRC, 2012, pp. 145-169.\n\nTTHRESH: Tensor compression for multidimensional visual data. R Ballester-Ripoll, P Lindstrom, R Pajarola, IEEE Transactions on Visualization and Computer Graphics. 269R. Ballester-Ripoll, P. Lindstrom, and R. Pajarola, \"TTHRESH: Tensor compression for multidimensional visual data,\" IEEE Transactions on Visualization and Computer Graphics, vol. 26, no. 9, pp. 2891-2903, 2019.\n\nData reduction techniques for simulation, visualization and data analysis. S Li, N Marsaglia, C Garth, J Woodring, J Clyne, H Childs, Computer Graphics Forum. 376S. Li, N. Marsaglia, C. Garth, J. Woodring, J. Clyne, and H. Childs, \"Data reduction techniques for simulation, visualization and data anal- ysis,\" Computer Graphics Forum, vol. 37, no. 6, pp. 422-447, 2018.\n\nWavelet filter evaluation for image compression. J D Villasenor, B Belzer, J Liao, IEEE Transactions on Image Processing. 48J. D. Villasenor, B. Belzer, and J. Liao, \"Wavelet filter evaluation for image compression,\" IEEE Transactions on Image Processing, vol. 4, no. 8, pp. 1053-1060, 1995.\n\nCompression of wavelet decompositions. R A Devore, B Jawerth, V Popov, American Journal of Mathematics. 1144R. A. DeVore, B. Jawerth, and V. Popov, \"Compression of wavelet decompositions,\" American Journal of Mathematics, vol. 114, no. 4, pp. 737-785, 1992.\n\nA tutorial on modern lossy wavelet image compression: foundations of JPEG 2000. B E Usevitch, IEEE Signal Processing Magazine. 185B. E. Usevitch, \"A tutorial on modern lossy wavelet image compression: foundations of JPEG 2000,\" IEEE Signal Processing Magazine, vol. 18, no. 5, pp. 22-35, 2001.\n\nEvaluating the efficacy of wavelet configurations on turbulent-flow data. S Li, K Gruchalla, K Potter, J Clyne, H Childs, IEEE 5th Symposium on Large Data Analysis and Visualization. S. Li, K. Gruchalla, K. Potter, J. Clyne, and H. Childs, \"Evaluating the efficacy of wavelet configurations on turbulent-flow data,\" in IEEE 5th Symposium on Large Data Analysis and Visualization, 2015, pp. 81-89.\n\nIn situ wavelet compression on supercomputers for post hoc exploration. S Li, J Clyne, H Childs, Situ Visualization for Computational Science. J. C. Bennett, and C. GarthSpringer International PublishingS. Li, J. Clyne, and H. Childs, \"In situ wavelet compression on supercomputers for post hoc exploration,\" in In Situ Visualization for Computational Science, H. Childs, J. C. Bennett, and C. Garth, Eds. Cham: Springer International Publishing, 2022, pp. 37-59.\n\nTensor decompositions and applications. T G Kolda, B W Bader, SIAM Review. 513T. G. Kolda and B. W. Bader, \"Tensor decompositions and applications,\" SIAM Review, vol. 51, no. 3, pp. 455-500, 2009.\n\nEmbedded and efficient low-complexity hierarchical image coder. A Islam, W A Pearlman, Visual Communications and Image Processing'99. 3653A. Islam and W. A. Pearlman, \"Embedded and efficient low-complexity hierarchical image coder,\" in Visual Communications and Image Pro- cessing'99, vol. 3653. International Society for Optics and Photonics, 1998, pp. 294-305.\n\nEmbedded image coding using zerotrees of wavelet coefficients. J M Shapiro, IEEE Transactions on Signal Processing. 4112J. M. Shapiro, \"Embedded image coding using zerotrees of wavelet coefficients,\" IEEE Transactions on Signal Processing, vol. 41, no. 12, pp. 3445-3462, 1993.\n\nHigh performance scalable image compression with EBCOT. D Taubman, IEEE Transactions on Image Processing. 97D. Taubman, \"High performance scalable image compression with EBCOT,\" IEEE Transactions on Image Processing, vol. 9, no. 7, pp. 1158-1170, 2000.\n\nThe emerging JBIG2 standard. P Howard, F Kossentini, B Martins, S Forchhammer, W Rucklidge, IEEE Transactions on Circuits and Systems for Video Technology. 8P. Howard, F. Kossentini, B. Martins, S. Forchhammer, and W. Ruck- lidge, \"The emerging JBIG2 standard,\" IEEE Transactions on Circuits and Systems for Video Technology, vol. 8, no. 7, pp. 838-848, 1998.\n\nOptimizing bitmap indices with efficient compression. K Wu, E J Otoo, A Shoshani, ACM Transactions on Database Systems. 311K. Wu, E. J. Otoo, and A. Shoshani, \"Optimizing bitmap indices with efficient compression,\" ACM Transactions on Database Systems, vol. 31, no. 1, pp. 1-38, 2006.\n\nUniversal codeword sets and representations of the integers. P Elias, IEEE Transactions on Information Theory. 212P. Elias, \"Universal codeword sets and representations of the integers,\" IEEE Transactions on Information Theory, vol. 21, no. 2, pp. 194-203, 1975.\n\nBiorthogonal bases of compactly supported wavelets. A Cohen, I Daubechies, J.-C Feauveau, Communications on Pure and Applied Mathematics. 455A. Cohen, I. Daubechies, and J.-C. Feauveau, \"Biorthogonal bases of compactly supported wavelets,\" Communications on Pure and Applied Mathematics, vol. 45, no. 5, pp. 485-560, 1992.\n\nFactoring wavelet transforms into lifting steps. I Daubechies, W Sweldens, Journal of Fourier analysis and applications. 43I. Daubechies and W. Sweldens, \"Factoring wavelet transforms into lifting steps,\" Journal of Fourier analysis and applications, vol. 4, no. 3, pp. 247-269, 1998.\n\nQccPack: An open-source software library for quantization, compression, and coding. J E Fowler, Applications of Digital Image Processing XXIII. 4115J. E. Fowler, \"QccPack: An open-source software library for quan- tization, compression, and coding,\" in Applications of Digital Image Processing XXIII, vol. 4115. International Society for Optics and Photonics, 2000, pp. 294-301.\n\nKodak lossless true color image suite. R Franzen, 4R. Franzen, \"Kodak lossless true color image suite,\" source: https://r0k.us/graphics/kodak, vol. 4, no. 2, 1999.\n\nZstandard-fast real-time compression algorithm. Facebook, Facebook, \"Zstandard-fast real-time compression algorithm.\" [Online]. Available: https://github.com/facebook/zstd\n\nMULTIPOSITS: Universal coding of R n. P Lindstrom, Conference on Next Generation Arithmetic. J. Gustafson and V. DimitrovSpringer International PublishingP. Lindstrom, \"MULTIPOSITS: Universal coding of R n ,\" in Conference on Next Generation Arithmetic, J. Gustafson and V. Dimitrov, Eds. Springer International Publishing, 2022, pp. 66-83.\n\nSDRBench: Scientific data reduction benchmark for lossy compressors. K Zhao, S Di, X Lian, S Li, D Tao, J Bessac, Z Chen, F Cappello, IEEE International Conference on Big Data (Big Data). K. Zhao, S. Di, X. Lian, S. Li, D. Tao, J. Bessac, Z. Chen, and F. Cappello, \"SDRBench: Scientific data reduction benchmark for lossy compressors,\" in IEEE International Conference on Big Data (Big Data), 2020, pp. 2716-2724. [Online]. Available: https://sdrbench.github.io/\n\nA universal image quality index. Z Wang, A C Bovik, IEEE signal processing letters. 93Z. Wang and A. C. Bovik, \"A universal image quality index,\" IEEE signal processing letters, vol. 9, no. 3, pp. 81-84, 2002.\n\nQuick Compression Analysis Toolkit (QCAT). S Di, S. Di, \"Quick Compression Analysis Toolkit (QCAT).\" [Online]. Available: https://github.com/szcompressor/qcat\n", "annotations": {"author": "[{\"end\":167,\"start\":48},{\"end\":291,\"start\":168},{\"end\":410,\"start\":292}]", "publisher": null, "author_last_name": "[{\"end\":59,\"start\":57},{\"end\":183,\"start\":174},{\"end\":302,\"start\":297}]", "author_first_name": "[{\"end\":56,\"start\":48},{\"end\":173,\"start\":168},{\"end\":296,\"start\":292}]", "author_affiliation": "[{\"end\":166,\"start\":61},{\"end\":290,\"start\":185},{\"end\":409,\"start\":304}]", "title": "[{\"end\":45,\"start\":1},{\"end\":455,\"start\":411}]", "venue": null, "abstract": "[{\"end\":1142,\"start\":486}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2081,\"start\":2078},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2087,\"start\":2083},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2613,\"start\":2609},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2619,\"start\":2615},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2727,\"start\":2723},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2733,\"start\":2729},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4155,\"start\":4151},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4161,\"start\":4157},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6080,\"start\":6077},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6086,\"start\":6082},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6092,\"start\":6088},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6098,\"start\":6094},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6364,\"start\":6361},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6369,\"start\":6366},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6462,\"start\":6459},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6526,\"start\":6523},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6859,\"start\":6856},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6864,\"start\":6861},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6881,\"start\":6877},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6936,\"start\":6932},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6942,\"start\":6938},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7147,\"start\":7143},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7991,\"start\":7987},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7997,\"start\":7993},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8003,\"start\":7999},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8009,\"start\":8005},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8033,\"start\":8029},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8039,\"start\":8035},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8818,\"start\":8814},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8824,\"start\":8820},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8927,\"start\":8923},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9771,\"start\":9767},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10018,\"start\":10014},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10024,\"start\":10020},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10057,\"start\":10053},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10167,\"start\":10163},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11266,\"start\":11262},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11272,\"start\":11268},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14654,\"start\":14653},{\"end\":19020,\"start\":19018},{\"end\":19068,\"start\":19066},{\"end\":20129,\"start\":20127},{\"end\":20349,\"start\":20347},{\"end\":20380,\"start\":20378},{\"end\":20437,\"start\":20434},{\"end\":20500,\"start\":20497},{\"end\":20531,\"start\":20528},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20555,\"start\":20553},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":28713,\"start\":28709},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":30871,\"start\":30867},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":35625,\"start\":35622},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":35634,\"start\":35631},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":35648,\"start\":35644},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":35663,\"start\":35660},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":36298,\"start\":36294},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":38779,\"start\":38778},{\"end\":39106,\"start\":39096},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":41375,\"start\":41371},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":43616,\"start\":43613},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":43680,\"start\":43676},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":44494,\"start\":44490},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":47722,\"start\":47718}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":47584,\"start\":46963},{\"attributes\":{\"id\":\"fig_2\"},\"end\":47617,\"start\":47585},{\"attributes\":{\"id\":\"fig_3\"},\"end\":48048,\"start\":47618},{\"attributes\":{\"id\":\"fig_4\"},\"end\":48467,\"start\":48049},{\"attributes\":{\"id\":\"fig_5\"},\"end\":48848,\"start\":48468},{\"attributes\":{\"id\":\"fig_6\"},\"end\":49199,\"start\":48849},{\"attributes\":{\"id\":\"fig_7\"},\"end\":49332,\"start\":49200},{\"attributes\":{\"id\":\"fig_8\"},\"end\":49396,\"start\":49333},{\"attributes\":{\"id\":\"fig_9\"},\"end\":49681,\"start\":49397},{\"attributes\":{\"id\":\"fig_10\"},\"end\":49798,\"start\":49682},{\"attributes\":{\"id\":\"fig_11\"},\"end\":50169,\"start\":49799},{\"attributes\":{\"id\":\"fig_12\"},\"end\":50288,\"start\":50170},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":50817,\"start\":50289},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":51066,\"start\":50818},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":51178,\"start\":51067}]", "paragraph": "[{\"end\":2006,\"start\":1161},{\"end\":3085,\"start\":2008},{\"end\":4062,\"start\":3087},{\"end\":4743,\"start\":4064},{\"end\":5441,\"start\":4745},{\"end\":5803,\"start\":5443},{\"end\":6274,\"start\":5805},{\"end\":6745,\"start\":6276},{\"end\":7317,\"start\":6747},{\"end\":8115,\"start\":7319},{\"end\":9223,\"start\":8117},{\"end\":9676,\"start\":9262},{\"end\":10485,\"start\":9702},{\"end\":11074,\"start\":10487},{\"end\":12429,\"start\":11121},{\"end\":13104,\"start\":12431},{\"end\":13460,\"start\":13106},{\"end\":14096,\"start\":13501},{\"end\":15510,\"start\":14098},{\"end\":16272,\"start\":15553},{\"end\":16589,\"start\":16299},{\"end\":16835,\"start\":16615},{\"end\":17276,\"start\":16926},{\"end\":18033,\"start\":17334},{\"end\":18117,\"start\":18062},{\"end\":18496,\"start\":18119},{\"end\":20095,\"start\":18498},{\"end\":20228,\"start\":20097},{\"end\":20252,\"start\":20235},{\"end\":20316,\"start\":20259},{\"end\":20402,\"start\":20323},{\"end\":20556,\"start\":20409},{\"end\":20955,\"start\":20595},{\"end\":21811,\"start\":20957},{\"end\":22489,\"start\":21813},{\"end\":23311,\"start\":22520},{\"end\":23650,\"start\":23373},{\"end\":23712,\"start\":23678},{\"end\":24811,\"start\":23741},{\"end\":25299,\"start\":24813},{\"end\":26126,\"start\":25340},{\"end\":27098,\"start\":26128},{\"end\":28447,\"start\":27100},{\"end\":28811,\"start\":28483},{\"end\":29446,\"start\":28844},{\"end\":30243,\"start\":29448},{\"end\":30821,\"start\":30294},{\"end\":30935,\"start\":30823},{\"end\":32548,\"start\":30961},{\"end\":34275,\"start\":32582},{\"end\":35397,\"start\":34299},{\"end\":36104,\"start\":35488},{\"end\":36424,\"start\":36129},{\"end\":37184,\"start\":36426},{\"end\":40135,\"start\":37228},{\"end\":41458,\"start\":40137},{\"end\":42218,\"start\":41499},{\"end\":43146,\"start\":42220},{\"end\":43949,\"start\":43193},{\"end\":45063,\"start\":43951},{\"end\":46371,\"start\":45099},{\"end\":46962,\"start\":46373}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":16925,\"start\":16836},{\"attributes\":{\"id\":\"formula_1\"},\"end\":17333,\"start\":17277},{\"attributes\":{\"id\":\"formula_2\"},\"end\":20594,\"start\":20557},{\"attributes\":{\"id\":\"formula_3\"},\"end\":23677,\"start\":23651},{\"attributes\":{\"id\":\"formula_4\"},\"end\":23740,\"start\":23713},{\"attributes\":{\"id\":\"formula_5\"},\"end\":25339,\"start\":25300},{\"attributes\":{\"id\":\"formula_6\"},\"end\":30960,\"start\":30936}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":25367,\"start\":25360},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":37432,\"start\":37425},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":40632,\"start\":40624},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":40972,\"start\":40964},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":41758,\"start\":41750},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":41979,\"start\":41971},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":42122,\"start\":42114},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":44676,\"start\":44668}]", "section_header": "[{\"end\":1159,\"start\":1144},{\"end\":9260,\"start\":9226},{\"end\":9700,\"start\":9679},{\"end\":11119,\"start\":11077},{\"end\":13499,\"start\":13463},{\"end\":15551,\"start\":15513},{\"end\":16297,\"start\":16275},{\"end\":16613,\"start\":16592},{\"end\":18060,\"start\":18036},{\"end\":20233,\"start\":20231},{\"end\":20257,\"start\":20255},{\"end\":20321,\"start\":20319},{\"end\":20407,\"start\":20405},{\"end\":22518,\"start\":22492},{\"end\":23371,\"start\":23314},{\"end\":28481,\"start\":28450},{\"end\":28842,\"start\":28814},{\"end\":30292,\"start\":30246},{\"end\":32580,\"start\":32551},{\"end\":34297,\"start\":34278},{\"end\":35442,\"start\":35400},{\"end\":35486,\"start\":35445},{\"end\":36127,\"start\":36107},{\"end\":37226,\"start\":37187},{\"end\":41497,\"start\":41461},{\"end\":43191,\"start\":43149},{\"end\":45097,\"start\":45066},{\"end\":46975,\"start\":46964},{\"end\":47627,\"start\":47619},{\"end\":48058,\"start\":48050},{\"end\":48477,\"start\":48469},{\"end\":48858,\"start\":48850},{\"end\":49209,\"start\":49201},{\"end\":49342,\"start\":49334},{\"end\":49399,\"start\":49398},{\"end\":49691,\"start\":49683},{\"end\":50180,\"start\":50171},{\"end\":50828,\"start\":50819},{\"end\":51078,\"start\":51068}]", "table": "[{\"end\":50817,\"start\":50431},{\"end\":51066,\"start\":50920}]", "figure_caption": "[{\"end\":47584,\"start\":46977},{\"end\":47617,\"start\":47587},{\"end\":48048,\"start\":47629},{\"end\":48467,\"start\":48060},{\"end\":48848,\"start\":48479},{\"end\":49199,\"start\":48860},{\"end\":49332,\"start\":49211},{\"end\":49396,\"start\":49344},{\"end\":49681,\"start\":49400},{\"end\":49798,\"start\":49693},{\"end\":50169,\"start\":49801},{\"end\":50288,\"start\":50183},{\"end\":50431,\"start\":50291},{\"end\":50920,\"start\":50830},{\"end\":51178,\"start\":51081}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":15342,\"start\":15334},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":22677,\"start\":22669},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24244,\"start\":24236},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24580,\"start\":24572},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":26151,\"start\":26143},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27408,\"start\":27400},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":29164,\"start\":29156},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":31765,\"start\":31757},{\"end\":32708,\"start\":32702},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":33252,\"start\":33244},{\"end\":35058,\"start\":35050},{\"end\":38503,\"start\":38495},{\"end\":40348,\"start\":40340},{\"end\":40385,\"start\":40379},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":40866,\"start\":40858},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":41988,\"start\":41981},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":42578,\"start\":42569},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":44579,\"start\":44570}]", "bib_author_first_name": "[{\"end\":52175,\"start\":52174},{\"end\":52177,\"start\":52176},{\"end\":52593,\"start\":52592},{\"end\":52606,\"start\":52605},{\"end\":52616,\"start\":52615},{\"end\":52627,\"start\":52626},{\"end\":53304,\"start\":53303},{\"end\":53313,\"start\":53312},{\"end\":53321,\"start\":53320},{\"end\":53327,\"start\":53326},{\"end\":53333,\"start\":53332},{\"end\":53346,\"start\":53345},{\"end\":53348,\"start\":53347},{\"end\":53355,\"start\":53354},{\"end\":53363,\"start\":53362},{\"end\":53371,\"start\":53370},{\"end\":53373,\"start\":53372},{\"end\":53384,\"start\":53383},{\"end\":53753,\"start\":53752},{\"end\":53761,\"start\":53760},{\"end\":53767,\"start\":53766},{\"end\":53782,\"start\":53778},{\"end\":53784,\"start\":53783},{\"end\":53796,\"start\":53795},{\"end\":53804,\"start\":53803},{\"end\":54300,\"start\":54299},{\"end\":54307,\"start\":54306},{\"end\":54313,\"start\":54312},{\"end\":54321,\"start\":54320},{\"end\":54789,\"start\":54788},{\"end\":54798,\"start\":54797},{\"end\":54804,\"start\":54803},{\"end\":54811,\"start\":54810},{\"end\":54817,\"start\":54816},{\"end\":54823,\"start\":54822},{\"end\":54830,\"start\":54829},{\"end\":54838,\"start\":54837},{\"end\":55217,\"start\":55216},{\"end\":55505,\"start\":55504},{\"end\":55518,\"start\":55517},{\"end\":55845,\"start\":55844},{\"end\":55854,\"start\":55853},{\"end\":55863,\"start\":55862},{\"end\":56280,\"start\":56279},{\"end\":56282,\"start\":56281},{\"end\":56289,\"start\":56288},{\"end\":56298,\"start\":56297},{\"end\":56310,\"start\":56309},{\"end\":56317,\"start\":56316},{\"end\":56327,\"start\":56326},{\"end\":56337,\"start\":56336},{\"end\":56339,\"start\":56338},{\"end\":56352,\"start\":56351},{\"end\":56361,\"start\":56360},{\"end\":56376,\"start\":56375},{\"end\":56880,\"start\":56879},{\"end\":56887,\"start\":56886},{\"end\":57192,\"start\":57191},{\"end\":57198,\"start\":57197},{\"end\":57209,\"start\":57208},{\"end\":57216,\"start\":57215},{\"end\":57224,\"start\":57223},{\"end\":57236,\"start\":57235},{\"end\":57245,\"start\":57244},{\"end\":57253,\"start\":57252},{\"end\":57263,\"start\":57262},{\"end\":57629,\"start\":57628},{\"end\":57640,\"start\":57639},{\"end\":57649,\"start\":57648},{\"end\":57655,\"start\":57654},{\"end\":58060,\"start\":58059},{\"end\":58062,\"start\":58061},{\"end\":58074,\"start\":58073},{\"end\":58083,\"start\":58082},{\"end\":58094,\"start\":58093},{\"end\":58479,\"start\":58478},{\"end\":58487,\"start\":58486},{\"end\":58489,\"start\":58488},{\"end\":58750,\"start\":58749},{\"end\":59053,\"start\":59052},{\"end\":59073,\"start\":59072},{\"end\":59086,\"start\":59085},{\"end\":59446,\"start\":59445},{\"end\":59452,\"start\":59451},{\"end\":59465,\"start\":59464},{\"end\":59474,\"start\":59473},{\"end\":59486,\"start\":59485},{\"end\":59495,\"start\":59494},{\"end\":59791,\"start\":59790},{\"end\":59793,\"start\":59792},{\"end\":59807,\"start\":59806},{\"end\":59817,\"start\":59816},{\"end\":60074,\"start\":60073},{\"end\":60076,\"start\":60075},{\"end\":60086,\"start\":60085},{\"end\":60097,\"start\":60096},{\"end\":60374,\"start\":60373},{\"end\":60376,\"start\":60375},{\"end\":60663,\"start\":60662},{\"end\":60669,\"start\":60668},{\"end\":60682,\"start\":60681},{\"end\":60692,\"start\":60691},{\"end\":60701,\"start\":60700},{\"end\":61059,\"start\":61058},{\"end\":61065,\"start\":61064},{\"end\":61074,\"start\":61073},{\"end\":61492,\"start\":61491},{\"end\":61494,\"start\":61493},{\"end\":61503,\"start\":61502},{\"end\":61505,\"start\":61504},{\"end\":61714,\"start\":61713},{\"end\":61723,\"start\":61722},{\"end\":61725,\"start\":61724},{\"end\":62077,\"start\":62076},{\"end\":62079,\"start\":62078},{\"end\":62349,\"start\":62348},{\"end\":62576,\"start\":62575},{\"end\":62586,\"start\":62585},{\"end\":62600,\"start\":62599},{\"end\":62611,\"start\":62610},{\"end\":62626,\"start\":62625},{\"end\":62962,\"start\":62961},{\"end\":62968,\"start\":62967},{\"end\":62970,\"start\":62969},{\"end\":62978,\"start\":62977},{\"end\":63255,\"start\":63254},{\"end\":63510,\"start\":63509},{\"end\":63519,\"start\":63518},{\"end\":63536,\"start\":63532},{\"end\":63831,\"start\":63830},{\"end\":63845,\"start\":63844},{\"end\":64152,\"start\":64151},{\"end\":64154,\"start\":64153},{\"end\":64487,\"start\":64486},{\"end\":64824,\"start\":64823},{\"end\":65197,\"start\":65196},{\"end\":65205,\"start\":65204},{\"end\":65211,\"start\":65210},{\"end\":65219,\"start\":65218},{\"end\":65225,\"start\":65224},{\"end\":65232,\"start\":65231},{\"end\":65242,\"start\":65241},{\"end\":65250,\"start\":65249},{\"end\":65625,\"start\":65624},{\"end\":65633,\"start\":65632},{\"end\":65635,\"start\":65634},{\"end\":65846,\"start\":65845}]", "bib_author_last_name": "[{\"end\":52184,\"start\":52178},{\"end\":52603,\"start\":52594},{\"end\":52613,\"start\":52607},{\"end\":52624,\"start\":52617},{\"end\":52634,\"start\":52628},{\"end\":53310,\"start\":53305},{\"end\":53318,\"start\":53314},{\"end\":53324,\"start\":53322},{\"end\":53330,\"start\":53328},{\"end\":53343,\"start\":53334},{\"end\":53352,\"start\":53349},{\"end\":53360,\"start\":53356},{\"end\":53368,\"start\":53364},{\"end\":53381,\"start\":53374},{\"end\":53388,\"start\":53385},{\"end\":53758,\"start\":53754},{\"end\":53764,\"start\":53762},{\"end\":53776,\"start\":53768},{\"end\":53793,\"start\":53785},{\"end\":53801,\"start\":53797},{\"end\":53813,\"start\":53805},{\"end\":54304,\"start\":54301},{\"end\":54310,\"start\":54308},{\"end\":54318,\"start\":54314},{\"end\":54330,\"start\":54322},{\"end\":54795,\"start\":54790},{\"end\":54801,\"start\":54799},{\"end\":54808,\"start\":54805},{\"end\":54814,\"start\":54812},{\"end\":54820,\"start\":54818},{\"end\":54827,\"start\":54824},{\"end\":54835,\"start\":54831},{\"end\":54847,\"start\":54839},{\"end\":55227,\"start\":55218},{\"end\":55515,\"start\":55506},{\"end\":55527,\"start\":55519},{\"end\":55851,\"start\":55846},{\"end\":55860,\"start\":55855},{\"end\":55866,\"start\":55864},{\"end\":56286,\"start\":56283},{\"end\":56295,\"start\":56290},{\"end\":56307,\"start\":56299},{\"end\":56314,\"start\":56311},{\"end\":56324,\"start\":56318},{\"end\":56334,\"start\":56328},{\"end\":56349,\"start\":56340},{\"end\":56358,\"start\":56353},{\"end\":56373,\"start\":56362},{\"end\":56384,\"start\":56377},{\"end\":56884,\"start\":56881},{\"end\":56893,\"start\":56888},{\"end\":57195,\"start\":57193},{\"end\":57206,\"start\":57199},{\"end\":57213,\"start\":57210},{\"end\":57221,\"start\":57217},{\"end\":57233,\"start\":57225},{\"end\":57242,\"start\":57237},{\"end\":57250,\"start\":57246},{\"end\":57260,\"start\":57254},{\"end\":57269,\"start\":57264},{\"end\":57637,\"start\":57630},{\"end\":57646,\"start\":57641},{\"end\":57652,\"start\":57650},{\"end\":57664,\"start\":57656},{\"end\":58071,\"start\":58063},{\"end\":58080,\"start\":58075},{\"end\":58091,\"start\":58084},{\"end\":58099,\"start\":58095},{\"end\":58484,\"start\":58480},{\"end\":58498,\"start\":58490},{\"end\":58756,\"start\":58751},{\"end\":59070,\"start\":59054},{\"end\":59083,\"start\":59074},{\"end\":59095,\"start\":59087},{\"end\":59449,\"start\":59447},{\"end\":59462,\"start\":59453},{\"end\":59471,\"start\":59466},{\"end\":59483,\"start\":59475},{\"end\":59492,\"start\":59487},{\"end\":59502,\"start\":59496},{\"end\":59804,\"start\":59794},{\"end\":59814,\"start\":59808},{\"end\":59822,\"start\":59818},{\"end\":60083,\"start\":60077},{\"end\":60094,\"start\":60087},{\"end\":60103,\"start\":60098},{\"end\":60385,\"start\":60377},{\"end\":60666,\"start\":60664},{\"end\":60679,\"start\":60670},{\"end\":60689,\"start\":60683},{\"end\":60698,\"start\":60693},{\"end\":60708,\"start\":60702},{\"end\":61062,\"start\":61060},{\"end\":61071,\"start\":61066},{\"end\":61081,\"start\":61075},{\"end\":61500,\"start\":61495},{\"end\":61511,\"start\":61506},{\"end\":61720,\"start\":61715},{\"end\":61734,\"start\":61726},{\"end\":62087,\"start\":62080},{\"end\":62357,\"start\":62350},{\"end\":62583,\"start\":62577},{\"end\":62597,\"start\":62587},{\"end\":62608,\"start\":62601},{\"end\":62623,\"start\":62612},{\"end\":62636,\"start\":62627},{\"end\":62965,\"start\":62963},{\"end\":62975,\"start\":62971},{\"end\":62987,\"start\":62979},{\"end\":63261,\"start\":63256},{\"end\":63516,\"start\":63511},{\"end\":63530,\"start\":63520},{\"end\":63545,\"start\":63537},{\"end\":63842,\"start\":63832},{\"end\":63854,\"start\":63846},{\"end\":64161,\"start\":64155},{\"end\":64495,\"start\":64488},{\"end\":64668,\"start\":64660},{\"end\":64834,\"start\":64825},{\"end\":65202,\"start\":65198},{\"end\":65208,\"start\":65206},{\"end\":65216,\"start\":65212},{\"end\":65222,\"start\":65220},{\"end\":65229,\"start\":65226},{\"end\":65239,\"start\":65233},{\"end\":65247,\"start\":65243},{\"end\":65259,\"start\":65251},{\"end\":65630,\"start\":65626},{\"end\":65641,\"start\":65636},{\"end\":65849,\"start\":65847}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":2141635},\"end\":52498,\"start\":52085},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":57661300},\"end\":52899,\"start\":52500},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":149921864},\"end\":53212,\"start\":52901},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":242757607},\"end\":53654,\"start\":53214},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":235616415},\"end\":54158,\"start\":53656},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2178023},\"end\":54689,\"start\":54160},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":53358213},\"end\":55169,\"start\":54691},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":15277033},\"end\":55447,\"start\":55171},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":12262331},\"end\":55771,\"start\":55449},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":245934928},\"end\":56115,\"start\":55773},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":123568133},\"end\":56813,\"start\":56117},{\"attributes\":{\"id\":\"b11\"},\"end\":57067,\"start\":56815},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":15768582},\"end\":57557,\"start\":57069},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":11021323},\"end\":57973,\"start\":57559},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":347171},\"end\":58407,\"start\":57975},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":16240973},\"end\":58704,\"start\":58409},{\"attributes\":{\"id\":\"b16\"},\"end\":58988,\"start\":58706},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":49268164},\"end\":59368,\"start\":58990},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":51876445},\"end\":59739,\"start\":59370},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":16495525},\"end\":60032,\"start\":59741},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":120101972},\"end\":60291,\"start\":60034},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":14239868},\"end\":60586,\"start\":60293},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":16113566},\"end\":60984,\"start\":60588},{\"attributes\":{\"id\":\"b23\"},\"end\":61449,\"start\":60986},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":16074195},\"end\":61647,\"start\":61451},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":18692719},\"end\":62011,\"start\":61649},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":18047405},\"end\":62290,\"start\":62013},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":1990936},\"end\":62544,\"start\":62292},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":17115133},\"end\":62905,\"start\":62546},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":15748413},\"end\":63191,\"start\":62907},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":6752631},\"end\":63455,\"start\":63193},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":48351350},\"end\":63779,\"start\":63457},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":121331324},\"end\":64065,\"start\":63781},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":28343931},\"end\":64445,\"start\":64067},{\"attributes\":{\"id\":\"b34\"},\"end\":64610,\"start\":64447},{\"attributes\":{\"id\":\"b35\"},\"end\":64783,\"start\":64612},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":250625549},\"end\":65125,\"start\":64785},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":231573358},\"end\":65589,\"start\":65127},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":14488670},\"end\":65800,\"start\":65591},{\"attributes\":{\"id\":\"b39\"},\"end\":65960,\"start\":65802}]", "bib_title": "[{\"end\":52172,\"start\":52085},{\"end\":52590,\"start\":52500},{\"end\":52993,\"start\":52901},{\"end\":53301,\"start\":53214},{\"end\":53750,\"start\":53656},{\"end\":54297,\"start\":54160},{\"end\":54786,\"start\":54691},{\"end\":55214,\"start\":55171},{\"end\":55502,\"start\":55449},{\"end\":55842,\"start\":55773},{\"end\":56277,\"start\":56117},{\"end\":57189,\"start\":57069},{\"end\":57626,\"start\":57559},{\"end\":58057,\"start\":57975},{\"end\":58476,\"start\":58409},{\"end\":59050,\"start\":58990},{\"end\":59443,\"start\":59370},{\"end\":59788,\"start\":59741},{\"end\":60071,\"start\":60034},{\"end\":60371,\"start\":60293},{\"end\":60660,\"start\":60588},{\"end\":61056,\"start\":60986},{\"end\":61489,\"start\":61451},{\"end\":61711,\"start\":61649},{\"end\":62074,\"start\":62013},{\"end\":62346,\"start\":62292},{\"end\":62573,\"start\":62546},{\"end\":62959,\"start\":62907},{\"end\":63252,\"start\":63193},{\"end\":63507,\"start\":63457},{\"end\":63828,\"start\":63781},{\"end\":64149,\"start\":64067},{\"end\":64821,\"start\":64785},{\"end\":65194,\"start\":65127},{\"end\":65622,\"start\":65591}]", "bib_author": "[{\"end\":52186,\"start\":52174},{\"end\":52605,\"start\":52592},{\"end\":52615,\"start\":52605},{\"end\":52626,\"start\":52615},{\"end\":52636,\"start\":52626},{\"end\":53312,\"start\":53303},{\"end\":53320,\"start\":53312},{\"end\":53326,\"start\":53320},{\"end\":53332,\"start\":53326},{\"end\":53345,\"start\":53332},{\"end\":53354,\"start\":53345},{\"end\":53362,\"start\":53354},{\"end\":53370,\"start\":53362},{\"end\":53383,\"start\":53370},{\"end\":53390,\"start\":53383},{\"end\":53760,\"start\":53752},{\"end\":53766,\"start\":53760},{\"end\":53778,\"start\":53766},{\"end\":53795,\"start\":53778},{\"end\":53803,\"start\":53795},{\"end\":53815,\"start\":53803},{\"end\":54306,\"start\":54299},{\"end\":54312,\"start\":54306},{\"end\":54320,\"start\":54312},{\"end\":54332,\"start\":54320},{\"end\":54797,\"start\":54788},{\"end\":54803,\"start\":54797},{\"end\":54810,\"start\":54803},{\"end\":54816,\"start\":54810},{\"end\":54822,\"start\":54816},{\"end\":54829,\"start\":54822},{\"end\":54837,\"start\":54829},{\"end\":54849,\"start\":54837},{\"end\":55229,\"start\":55216},{\"end\":55517,\"start\":55504},{\"end\":55529,\"start\":55517},{\"end\":55853,\"start\":55844},{\"end\":55862,\"start\":55853},{\"end\":55868,\"start\":55862},{\"end\":56288,\"start\":56279},{\"end\":56297,\"start\":56288},{\"end\":56309,\"start\":56297},{\"end\":56316,\"start\":56309},{\"end\":56326,\"start\":56316},{\"end\":56336,\"start\":56326},{\"end\":56351,\"start\":56336},{\"end\":56360,\"start\":56351},{\"end\":56375,\"start\":56360},{\"end\":56386,\"start\":56375},{\"end\":56886,\"start\":56879},{\"end\":56895,\"start\":56886},{\"end\":57197,\"start\":57191},{\"end\":57208,\"start\":57197},{\"end\":57215,\"start\":57208},{\"end\":57223,\"start\":57215},{\"end\":57235,\"start\":57223},{\"end\":57244,\"start\":57235},{\"end\":57252,\"start\":57244},{\"end\":57262,\"start\":57252},{\"end\":57271,\"start\":57262},{\"end\":57639,\"start\":57628},{\"end\":57648,\"start\":57639},{\"end\":57654,\"start\":57648},{\"end\":57666,\"start\":57654},{\"end\":58073,\"start\":58059},{\"end\":58082,\"start\":58073},{\"end\":58093,\"start\":58082},{\"end\":58101,\"start\":58093},{\"end\":58486,\"start\":58478},{\"end\":58500,\"start\":58486},{\"end\":58758,\"start\":58749},{\"end\":59072,\"start\":59052},{\"end\":59085,\"start\":59072},{\"end\":59097,\"start\":59085},{\"end\":59451,\"start\":59445},{\"end\":59464,\"start\":59451},{\"end\":59473,\"start\":59464},{\"end\":59485,\"start\":59473},{\"end\":59494,\"start\":59485},{\"end\":59504,\"start\":59494},{\"end\":59806,\"start\":59790},{\"end\":59816,\"start\":59806},{\"end\":59824,\"start\":59816},{\"end\":60085,\"start\":60073},{\"end\":60096,\"start\":60085},{\"end\":60105,\"start\":60096},{\"end\":60387,\"start\":60373},{\"end\":60668,\"start\":60662},{\"end\":60681,\"start\":60668},{\"end\":60691,\"start\":60681},{\"end\":60700,\"start\":60691},{\"end\":60710,\"start\":60700},{\"end\":61064,\"start\":61058},{\"end\":61073,\"start\":61064},{\"end\":61083,\"start\":61073},{\"end\":61502,\"start\":61491},{\"end\":61513,\"start\":61502},{\"end\":61722,\"start\":61713},{\"end\":61736,\"start\":61722},{\"end\":62089,\"start\":62076},{\"end\":62359,\"start\":62348},{\"end\":62585,\"start\":62575},{\"end\":62599,\"start\":62585},{\"end\":62610,\"start\":62599},{\"end\":62625,\"start\":62610},{\"end\":62638,\"start\":62625},{\"end\":62967,\"start\":62961},{\"end\":62977,\"start\":62967},{\"end\":62989,\"start\":62977},{\"end\":63263,\"start\":63254},{\"end\":63518,\"start\":63509},{\"end\":63532,\"start\":63518},{\"end\":63547,\"start\":63532},{\"end\":63844,\"start\":63830},{\"end\":63856,\"start\":63844},{\"end\":64163,\"start\":64151},{\"end\":64497,\"start\":64486},{\"end\":64670,\"start\":64660},{\"end\":64836,\"start\":64823},{\"end\":65204,\"start\":65196},{\"end\":65210,\"start\":65204},{\"end\":65218,\"start\":65210},{\"end\":65224,\"start\":65218},{\"end\":65231,\"start\":65224},{\"end\":65241,\"start\":65231},{\"end\":65249,\"start\":65241},{\"end\":65261,\"start\":65249},{\"end\":65632,\"start\":65624},{\"end\":65643,\"start\":65632},{\"end\":65851,\"start\":65845}]", "bib_venue": "[{\"end\":57775,\"start\":57729},{\"end\":52217,\"start\":52186},{\"end\":52674,\"start\":52636},{\"end\":53031,\"start\":52995},{\"end\":53419,\"start\":53390},{\"end\":53881,\"start\":53815},{\"end\":54401,\"start\":54332},{\"end\":54905,\"start\":54849},{\"end\":55285,\"start\":55229},{\"end\":55585,\"start\":55529},{\"end\":55925,\"start\":55868},{\"end\":56433,\"start\":56386},{\"end\":56877,\"start\":56815},{\"end\":57292,\"start\":57271},{\"end\":57727,\"start\":57666},{\"end\":58163,\"start\":58101},{\"end\":58530,\"start\":58500},{\"end\":58747,\"start\":58706},{\"end\":59153,\"start\":59097},{\"end\":59527,\"start\":59504},{\"end\":59861,\"start\":59824},{\"end\":60136,\"start\":60105},{\"end\":60418,\"start\":60387},{\"end\":60769,\"start\":60710},{\"end\":61127,\"start\":61083},{\"end\":61524,\"start\":61513},{\"end\":61781,\"start\":61736},{\"end\":62127,\"start\":62089},{\"end\":62396,\"start\":62359},{\"end\":62700,\"start\":62638},{\"end\":63025,\"start\":62989},{\"end\":63302,\"start\":63263},{\"end\":63593,\"start\":63547},{\"end\":63900,\"start\":63856},{\"end\":64209,\"start\":64163},{\"end\":64484,\"start\":64447},{\"end\":64658,\"start\":64612},{\"end\":64876,\"start\":64836},{\"end\":65313,\"start\":65261},{\"end\":65673,\"start\":65643},{\"end\":65843,\"start\":65802}]"}}}, "year": 2023, "month": 12, "day": 17}