{"id": 235593036, "updated": "2023-10-06 02:08:45.445", "metadata": {"title": "NuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles", "authors": "[{\"first\":\"Holger\",\"last\":\"Caesar\",\"middle\":[]},{\"first\":\"Juraj\",\"last\":\"Kabzan\",\"middle\":[]},{\"first\":\"Kok\",\"last\":\"Tan\",\"middle\":[\"Seang\"]},{\"first\":\"Whye\",\"last\":\"Fong\",\"middle\":[\"Kit\"]},{\"first\":\"Eric\",\"last\":\"Wolff\",\"middle\":[]},{\"first\":\"Alex\",\"last\":\"Lang\",\"middle\":[]},{\"first\":\"Luke\",\"last\":\"Fletcher\",\"middle\":[]},{\"first\":\"Oscar\",\"last\":\"Beijbom\",\"middle\":[]},{\"first\":\"Sammy\",\"last\":\"Omari\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "In this work, we propose the world's first closed-loop ML-based planning benchmark for autonomous driving. While there is a growing body of ML-based motion planners, the lack of established datasets and metrics has limited the progress in this area. Existing benchmarks for autonomous vehicle motion prediction have focused on short-term motion forecasting, rather than long-term planning. This has led previous works to use open-loop evaluation with L2-based metrics, which are not suitable for fairly evaluating long-term planning. Our benchmark overcomes these limitations by introducing a large-scale driving dataset, lightweight closed-loop simulator, and motion-planning-specific metrics. We provide a high-quality dataset with 1500h of human driving data from 4 cities across the US and Asia with widely varying traffic patterns (Boston, Pittsburgh, Las Vegas and Singapore). We will provide a closed-loop simulation framework with reactive agents and provide a large set of both general and scenario-specific planning metrics. We plan to release the dataset at NeurIPS 2021 and organize benchmark challenges starting in early 2022.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2106.11810", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2106-11810", "doi": null}}, "content": {"source": {"pdf_hash": "b88b38ec61a4881173ab94647d1e97500f4af15b", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2106.11810v4.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "766160fc1b1430da78216fdf87487656536fe077", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b88b38ec61a4881173ab94647d1e97500f4af15b.txt", "contents": "\nnuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles\n\n\nHolger Caesar \nJuraj Kabzan \nKok Seang \nTan Whye \nKit Fong \nEric Wolff \nAlex Lang \nLuke Fletcher \nOscar Beijbom \nSammy Omari Motional \nnuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles\n\nIn this work, we propose the world's first closed-loop ML-based planning benchmark for autonomous driving. While there is a growing body of ML-based motion planners, the lack of established datasets and metrics has limited the progress in this area. Existing benchmarks for autonomous vehicle motion prediction have focused on short-term motion forecasting, rather than long-term planning. This has led previous works to use open-loop evaluation with L2-based metrics, which are not suitable for fairly evaluating long-term planning. Our benchmark overcomes these limitations by introducing a largescale driving dataset, lightweight closed-loop simulator, and motion-planning-specific metrics. We provide a highquality dataset with 1500h of human driving data from 4 cities across the US and Asia with widely varying traffic patterns (Boston, Pittsburgh, Las Vegas and Singapore). We will provide a closed-loop simulation framework with reactive agents and provide a large set of both general and scenario-specific planning metrics. We plan to release the dataset at NeurIPS 2021 and organize benchmark challenges starting in early 2022.\n\nIntroduction\n\nLarge-scale human labeled datasets in combination with deep Convolutional Neural Networks have led to an impressive performance increase in autonomous vehicle (AV) perception over the last few years [9,4]. In contrast, existing solutions for AV planning are still primarily based on carefully engineered expert systems, that require significant amounts of engineering to adapt to new geographies and do not scale with more training data. We believe that providing suitable data and metrics will enable ML-based planning and pave the way towards a full \"Software 2.0\" stack.\n\nExisting real-world benchmarks are focused on shortterm motion forecasting, also known as prediction [6,4,11,8], rather than planning. This is evident in the lack of high-level goals, the choice of metrics, and the openloop evaluation. Prediction focuses on the behavior of other agents, while planning relates to the ego vehicle behavior. Prediction is typically multi-modal, which means that for each agent we predict the N most likely trajectories. In contrast, planning is typically uni-modal (except for contingency planning) and we predict a single trajectory. As an example, in Fig. 1a, turning left or right at an intersection are equally likely options. Prediction datasets lack a baseline navigation route to indicate the high-level goals of the agents. In Fig. 1b, the options of merging immediately or later are both equally valid, but the commonly used L2 distance-based metrics (minADE, minFDE, and miss rate) penalize the option that was not observed in the data. Intuitively, the distance between the predicted trajectory and the observed trajectory is not a suitable indicator in a multimodal scenario. In Fig. 1c, the decision whether to continue to overtake or get back into the lane should be based on the consecutive actions of all agent vehicles, which is not possible in open-loop evaluation. Lack of closed-loop evaluation leads to systematic drift, making it difficult to evaluate beyond a short time horizon (3-8s).\n\nWe instead provide a planning benchmark to address these shortcomings. Our main contributions are:\n\n\u2022 The largest existing public real-world dataset for autonomous driving with high quality autolabeled tracks from 4 cities.\n\n\u2022 Planning metrics related to traffic rule violation, human driving similarity, vehicle dynamics, goal achievement, as well as scenario-based.\n\n\u2022 The first public benchmark for real-world data with a closed-loop planner evaluation protocol. \n\n\nRelated Work\n\nWe review the relevant literature for prediction and planning datasets, simulation, and ML-based planning.\n\nPrediction datasets. Table 1 shows a comparison between our dataset and relevant prediction datasets. Argoverse Motion Forecasting [6] was the first large-scale prediction dataset. With 320h of driving data, it was unprecedented in size and provides simple semantic maps with centerlines and driveable area annotations. However, the autolabeled trajectories in the dataset are of lower quality due to the state of object detection field at the time and the insufficient amount of human-labeled training data (113 scenes).\n\nThe nuScenes prediction [4] challenge consists of 850 human-labeled scenes from the nuScenes dataset. While the annotations are high quality and sensor data is provided, the small scale limits the number of driving variations. The Lyft Level 5 Prediction Dataset [11] contains 1118h of data from a single route of 6.8 miles. It features detailed semantic maps, aerial maps, and dynamic traffic light status. While the scale is unprecedented, the autolabeled tracks are often noisy and geographic diversity is limited. The Waymo Open Motion Dataset [8] focuses specifically on the interactions between agents, but does so using open-loop evaluation. While the dataset size is smaller than existing datasets at 570h, the autolabeled tracks are of high quality [17]. They provide semantic maps and dynamic traffic light status.\n\nThese datasets focus on prediction, rather than planning. In this work we aim to overcome this limitation by using planning metrics and closed-loop evaluation. We are the first large-scale dataset to provide sensor data.\n\nPlanning datasets. CommonRoad [1] provides a first of its kind planning benchmark, that is composed of different vehicle models, cost functions and scenarios (including goals and constraints). There are both pre-recorded and interactive scenarios. With 5700 scenarios in total, the scale of the dataset does not support training modern deep learning based methods. All scenarios lack sensor data.\n\nSimulation. Simulators have enabled breakthroughs in planning and reinforcement learning with their ability to simulate physics, agents, and environmental conditions in a closed-loop environment.\n\nAirSim [19] is a high-fidelty simulator for AVs, such as drones and cars. It includes a physics engine that can operate at a high frequency for real-time hardware-in-the-loop simulation. CARLA [7] supports the training and validation of autonomous urban driving systems. It allows for flexible specification of sensor suites and environmental conditions. In the CARLA Autonomous Driving Challenge 1 the goal is to navigate a set of waypoints using different combinations of sensor data and HD maps. Alternatively, users can use scene abstraction to omit the perception task and focus on planning and control aspects. This challenge is conceptually similar to what we propose, but does not use real world data and provides less detailed planning metrics.\n\nSim-to-real transfer is an active research area for diverse tasks such as localization, perception, prediction, planning and control. [21] show that the domain gap between simulated and real-world data remains an issue, by transferring a synthetically trained tracking model to the KITTI [9] dataset. To overcome the domain gap, they jointly train their model using real-world data for visible and simulation data for occluded objects. [3] learn how to drive by transferring a vision-based lane following driving policy from simulation to the real world without any real-world labels. [14] use reinforcement learning in simulation to obtain a driving system controlling a full-size real-world vehicle. They use mostly synthetic data, with labelled real-world data appearing only in the training of the segmentation network.\n\nHowever, all simulations have fundamental limits since they introduce systematic biases. More work is required to plausibly emulate real-world sensors, e.g. to generate photo-realistic camera images.\n\nML-based planning. A new emerging research field is ML-based planning for AVs using real-world data. However, the field has yet to converge on a common input/output space, dataset, or metrics. A jointly learnable behavior and trajectory planner is proposed in [18]. An interpretable cost function is learned on top of models for perception, prediction and vehicle dynamics, and evaluated in open-loop on two unpublished datasets. An end-to-end interpretable neural motion planner [24] takes raw lidar point clouds and dynamic map data as inputs and predicts a cost map for planning. They evaluate in open-loop on an unpublished dataset, with a planning horizon of only 3s. Chauffeur-Net [2] finds that standard behavior cloning is insufficient for handling complex driving scenarios, even when using as many as 30 million examples. They propose exposing the learner to synthesized data in the form of perturbations to the expert's driving and augment the imitation loss with additional losses that penalize undesirable events and encour-age progress. Their unpublished dataset contains 26 million examples which correspond to 60 days of continuous driving. The method is evaluated in a closed-loop and an openloop setup, as well as in the real world. They also show that open-loop evaluation can be misleading compared to closed-loop. MP3 [5] proposes an end-to-end approach to mapless driving, where the input is raw lidar data and a high-level navigation goal. They evaluate on an unpublished dataset in open and closed-loop. Multi-modal methods have also been explored in recent works [16,20,13]. These approaches explore different strategies for fusing various modality representations in order to predict future waypoints or control commands. Neural planners were also used in [15,10] to evaluate an object detector using the KL divergence of the planned trajectory and the observed route.\n\nExisting works evaluate on different metrics which are inconsistent across the literature. TransFuser [16] evaluates its method on the number of infractions, the percentage of the route distance completed, and the route completion weighted by an infraction multiplier. Infractions include collisions with other agents, and running red lights. [20] evaluates its planner using off-road time, off-lane time and number of crashes, while [13,22] report the success rate of reaching a given destination within a fixed time window. [13] also introduces another metric which measures the average percentage of distance travelled to the goal.\n\nWhile ML-based planning has been studied in great detail, the lack of published datasets and a standard set of metrics that provide a common framework for closed-loop evaluation has limited the progress in this area. We aim to fill this gap by providing an ML-based planning dataset and metrics.\n\n\nDataset\n\nOverview. We plan to release 1500 hours of data from Las Vegas, Boston, Pittsburgh, and Singapore. Each city provides its unique driving challenges. For example, Las Vegas includes bustling casino pick-up and drop-off points (PUDOs) with complex interactions and busy intersections with up to 8 parallel driving lanes per direction, Boston routes include drivers who love to double park, Pittsburgh has its own custom precedence pattern for left turns at intersections, and Singapore features left hand traffic. For each city we provide semantic maps and an API for efficient map queries. The dataset includes lidar point clouds, camera images, localization information and steering inputs. While we release autolabeled agent trajectories on the entire dataset, we make only a subset of the sensor data available due to the vast scale of the dataset (200+ TB).\n\nAutolabeling. We use an offline perception system to label the large-scale dataset at high accuracy, without the realtime constraints imposed on the online perception system of an AV. We use PointPillars [12] with CenterPoint [23], a modified version multi-view fusion (MVF++) [17], and non-causal tracking to achieve near-human labeling performance.\n\nScenarios. To enable scenario-based metrics, we automatically annotate intervals with tags for complex scenarios. These scenarios include merges, lane changes, protected or unprotected left or right turns, interaction with cyclists, interaction with pedestrians at crosswalks or elsewhere, interactions with close proximity or high acceleration, double parked vehicles, stop controlled intersections and driving in construction zones.\n\n\nBenchmarks\n\nTo further the state of the art in ML-based planning, we organize benchmark challenges with the tasks and metrics described below.\n\n\nOverview\n\nTo evaluate a proposed method against the benchmark dataset, users submit ML-based planning code to our evaluation server. The code must follow a provided template. Contrary to most benchmarks, the code is containerized for portability in order to enable closed-loop evaluation on a secret test set. The planner operates either on the autolabeled trajectories or, for end-to-end open-loop approaches, directly on the raw sensor data. When queried for a particular timestep, the planner returns the planned position and heading of the ego vehicle. A provided controller will then drive a vehicle while closely tracking the planned trajectory. We use a predefined motion model to simulate the ego vehicle motion in order to approximate a real system. The final driven trajectory is then scored against the metrics defined in Sec 4.2.\n\n\nTasks\n\nWe present the three different tasks for our dataset with increasing difficulty.\n\nOpen-loop. In the first challenge, we task the planning system to mimic a human driver. For every timestep, the trajectory is scored based on predefined metrics. It is not used to control the vehicle. In this case, no interactions are considered.\n\nClosed-loop. In the closed-loop setup the planner outputs a planned trajectory using the information available at each timestep, similar to the previous case. However, the proposed trajectory is used as a reference for a controller, and thus, the planning system is gradually corrected at each timestep with the new state of the vehicle. While the new state of the vehicle may not coincide with that of the recorded state, leading to different camera views or lidar point clouds, we will not perform any sensor data warping or novel view synthesis. In this set, we distinguish between two tasks. In the Non-reactive closed-loop task we do not make any assumptions on other agents behavior and simply use the observed agent trajectories. As shown in [11], the vast majority of interventions in closed-loop simulation is due to the non-reactive nature, e.g. vehicles naively colliding with the ego vehicle. In the reactive closed-loop task we provide a planning model for all other agents that are tracked like the ego vehicle.\n\n\nMetrics\n\nWe split the metrics into two categories, common metrics, which are computed for every scenario and scenariobased metrics, which are tailored to predefined scenarios.\n\n\nCommon metrics.\n\n\u2022 Traffic rule violation is used to measure compliance with common traffic rules. We compute the rate of collisions with other agents, rate of off-road trajectories, the time gap to lead agents, time to collision and the relative velocity while passing an agents as a function of the passing distance.\n\n\u2022 Human driving similarity is used to quantify a maneuver satisfaction in comparison to a human, e.g. longitudinal velocity error, longitudinal stop position error and lateral position error. In addition, the resulting jerk/acceleration is compared to the human-level jerk/acceleration.\n\n\u2022 Vehicle dynamics quantify rider comfort and feasibility of a trajectory. Rider comfort is measured by jerk, acceleration, steering rate and vehicle oscillation. Feasibility is measured by violation of predefined limits of the same criteria.\n\n\u2022 Goal achievement measures the route progress towards a goal waypoint on the map using L2 distance.\n\nScenario-based metrics. Based on the scenario tags from Sec. 3, we use additional metrics for challenging maneuvers. For lane change, time to collision and time gap to lead/rear agent on the target lane is measured and scored. For pedestrian/cyclist interaction, we quantify the passing relative velocity while differentiating their location. Furthermore, we compare the agreement between decisions made by a planner and human for crosswalks and unprotected turns (right of way).\n\nCommunity feedback. Note that the metrics shown here are an initial proposal and do not form an exhaustive list.\n\nWe will work closely with the community to add novel scenarios and metrics to achieve consensus across the community. Likewise, for the main challenge metric we see multiple options, such as a weighted sum of metrics, a weighted sum of metric violations above a predefined threshold or a hierarchy of metrics. We invite the community to collaborate with us to define the metrics that will drive this field forward.\n\n\nConclusion\n\nIn this work we proposed the first ML-based planning benchmark for AVs. Contrary to existing forecasting benchmarks, we focus on goal-based planning, planning metrics and closed-loop evaluation. We hope that by providing a common benchmark, we will pave a path towards progress in ML-based planning, which is one of the final frontiers in autonomous driving.\n\nFigure 1 .\n1We show different driving scenarios to emphasize the limitations of existing benchmarks. The observed driving route of the ego vehicle in shown in white and the hypothetical planner route in red. (a) The absence of a goal leads to ambiguity at intersections. (b) Displacement metrics do not take into account the multi-modal nature of driving. (c) open-loop evaluation does not take into account agent interaction.\nSee carlachallenge.org\n\nCommonRoad: Composable benchmarks for motion planning on roads. Matthias Althoff, Markus Koschi, Stefanie Manzinger, Proc. of the IEEE Intelligent Vehicles Symposium. of the IEEE Intelligent Vehicles SymposiumMatthias Althoff, Markus Koschi, and Stefanie Manzinger. CommonRoad: Composable benchmarks for motion plan- ning on roads. In Proc. of the IEEE Intelligent Vehicles Sym- posium, 2017. 2\n\nChauffeurnet: Learning to drive by imitating the best and synthesizing the worst. Mayank Bansal, Alex Krizhevsky, Abhijit Ogale, RSS. Mayank Bansal, Alex Krizhevsky, and Abhijit Ogale. Chauf- feurnet: Learning to drive by imitating the best and synthe- sizing the worst. In RSS, 2019. 2\n\nLearning to drive from simulation without real world labels. Alex Bewley, Jessica Rigley, Yuxuan Liu, Jeffrey Hawke, Richard Shen, Vinh-Dieu Lam, Alex Kendall, ICRA. Alex Bewley, Jessica Rigley, Yuxuan Liu, Jeffrey Hawke, Richard Shen, Vinh-Dieu Lam, and Alex Kendall. Learning to drive from simulation without real world labels. In ICRA, 2019. 2\n\nnuscenes: A multimodal dataset for autonomous driving. Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, Oscar Beijbom, CVPR. 1Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi- ancarlo Baldan, and Oscar Beijbom. nuscenes: A multi- modal dataset for autonomous driving. In CVPR, 2020. 1, 2\n\nMP3: A unified model to map, perceive, predict and plan. Sergio Casas, Abbas Sadat, Raquel Urtasun, CVPR. Sergio Casas, Abbas Sadat, and Raquel Urtasun. MP3: A unified model to map, perceive, predict and plan. In CVPR, 2021. 3\n\nArgoverse: 3d tracking and forecasting with rich maps. Ming-Fang Chang, John W Lambert, Patsorn Sangkloy, Jagjeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Peter Carr, Simon Lucey, Deva Ramanan, James Hays, CVPR. 1Ming-Fang Chang, John W Lambert, Patsorn Sangkloy, Jag- jeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Peter Carr, Simon Lucey, Deva Ramanan, and James Hays. Argo- verse: 3d tracking and forecasting with rich maps. In CVPR, 2019. 1, 2\n\nCARLA: An open urban driving simulator. Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, Vladlen Koltun, CoRR. 2Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. CARLA: An open urban driving simulator. CoRR, 2017. 2\n\nLarge scale interactive motion forecasting for autonomous driving: The Waymo Open Motion Dataset. Scott Ettinger, Shuyang Cheng, Benjamin Caine, arXiv:2104.101331arXiv preprintScott Ettinger, Shuyang Cheng, and Benjamin Caine et al. Large scale interactive motion forecasting for autonomous driving: The Waymo Open Motion Dataset. arXiv preprint arXiv:2104.10133, 2021. 1, 2\n\nVision meets robotics: The KITTI dataset. Andreas Geiger, Philip Lenz, Christoph Stiller, Raquel Urtasun, IJRRAndreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The KITTI dataset. IJRR, 32(11):1231-1237, 2013. 1, 2\n\nThe efficacy of neural planning metrics: A meta-analysis of PKL on nuscenes. Yiluan Guo, Holger Caesar, Oscar Beijbom, Jonah Philion, Sanja Fidler, IROS Workshop on Benchmarking Progress in Autonomous Driving. Yiluan Guo, Holger Caesar, Oscar Beijbom, Jonah Philion, and Sanja Fidler. The efficacy of neural planning metrics: A meta-analysis of PKL on nuscenes. In IROS Workshop on Benchmarking Progress in Autonomous Driving, 2020. 3\n\nJohn Houston, Guido Zuidhof, Luca Bergamini, arXiv:2006.14480One thousand and one hours: Self-driving motion prediction dataset. arXiv preprintJohn Houston, Guido Zuidhof, and Luca Bergamini et al. One thousand and one hours: Self-driving motion prediction dataset. arXiv preprint arXiv:2006.14480, 2020. 1, 2, 4\n\nPointpillars: Fast encoders for object detection from point clouds. Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, Oscar Beijbom, CVPR. Alex H. Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders for object detection from point clouds. In CVPR, 2019. 3\n\nConditional imitation learning driving considering camera and lidar fusion. Eraqi Hesham, M Mohamed, N Moustafa, Jens Honer, NeurIPS. 2020Eraqi Hesham M., Mohamed N. Moustafa, and Jens Honer. Conditional imitation learning driving considering camera and lidar fusion. In NeurIPS, 2020. 3\n\nSimulation-based reinforcement learning for real-world autonomous driving. Blazej Osinski, Adam Jakubowski, Pawel Ziecina, Piotr Milos, Christopher Galias, Silviu Homoceanu, Henryk Michalewski, ICRA. 2020Blazej Osinski, Adam Jakubowski, Pawel Ziecina, Piotr Milos, Christopher Galias, Silviu Homoceanu, and Henryk Michalewski. Simulation-based reinforcement learning for real-world autonomous driving. In ICRA, 2020. 2\n\nLearning to evaluate perception models using planner-centric metrics. Jonah Philion, Amlan Kar, Sanja Fidler, CVPR. 2020Jonah Philion, Amlan Kar, and Sanja Fidler. Learning to evaluate perception models using planner-centric metrics. In CVPR, 2020. 3\n\nMultimodal fusion transformer for end-to-end autonomous driving. Aditya Prakash, Kashyap Chitta, Andreas Geiger, 2021. 3IEEE Conference on Computer Vision and Pattern Recognition. Aditya Prakash, Kashyap Chitta, and Andreas Geiger. Multi- modal fusion transformer for end-to-end autonomous driv- ing. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 3\n\nOffboard 3d object detection from point cloud sequences. Charles R Qi, Yin Zhou, Mahyar Najibi, Pei Sun, Khoa Vo, Boyang Deng, Dragomir Anguelov, arXiv:2103.0507323arXiv preprintCharles R. Qi, Yin Zhou, Mahyar Najibi, Pei Sun, Khoa Vo, Boyang Deng, and Dragomir Anguelov. Offboard 3d ob- ject detection from point cloud sequences. arXiv preprint arXiv:2103.05073, 2021. 2, 3\n\nJointly learnable behavior and trajectory planning for self-driving vehicles. Abbas Sadat, Mengye Ren, Andrei Pokrovsky, Yen-Chen Lin, Ersin Yumer, Raquel Urtasun, IROS. Abbas Sadat, Mengye Ren, Andrei Pokrovsky, Yen-Chen Lin, Ersin Yumer, and Raquel Urtasun. Jointly learnable be- havior and trajectory planning for self-driving vehicles. In IROS, 2019. 2\n\nAirSim: High-fidelity visual and physical simulation for autonomous vehicles. Shital Shah, Debadeepta Dey, Chris Lovett, Ashish Kapoor, Field and Service Robotics. Shital Shah, Debadeepta Dey, Chris Lovett, and Ashish Kapoor. AirSim: High-fidelity visual and physical simula- tion for autonomous vehicles. In Field and Service Robotics, 2017. 2\n\nEnd-to-end multi-modal sensors fusion system for urban automated driving. Ibrahim Sobh, Loay Amin, Sherif Abdelkarim, Khaled Elmadawy, Mahmoud Saeed, Omar Abdeltawab, Mostafa Gamal, Ahmad El Sallab, In NeurIPS. 3Ibrahim Sobh, Loay Amin, Sherif Abdelkarim, Khaled Elmadawy, Mahmoud Saeed, Omar Abdeltawab, Mostafa Gamal, and Ahmad El Sallab. End-to-end multi-modal sen- sors fusion system for urban automated driving. In NeurIPS, 2018. 3\n\nLearning to track with object permanence. Pavel Tokmakov, Jie Li, Wolfram Burgard, Adrien Gaidon, arXiv:2103.14258arXiv preprintPavel Tokmakov, Jie Li, Wolfram Burgard, and Adrien Gaidon. Learning to track with object permanence. arXiv preprint arXiv:2103.14258, 2021. 2\n\nMultimodal end-to-end autonomous driving. Yi Xiao, Felipe Codevilla, Akhil Gurram, Onay Urfalioglu, Antonio M L\u00f3pez, arXiv:1906.03199arXiv preprintYi Xiao, Felipe Codevilla, Akhil Gurram, Onay Urfalioglu, and Antonio M. L\u00f3pez. Multimodal end-to-end autonomous driving. arXiv preprint arXiv:1906.03199, 2019. 3\n\nTianwei Yin, Xingyi Zhou, Philipp Kr\u00e4henb\u00fchl, arXiv:2006.11275Centerbased 3d object detection and tracking. arXiv preprintTianwei Yin, Xingyi Zhou, and Philipp Kr\u00e4henb\u00fchl. Center- based 3d object detection and tracking. arXiv preprint arXiv:2006.11275, 2020. 3\n\nAbbas Sadat, Bin Yang, Sergio Casas, and Raquel Urtasun. End-to-end interpretable neural motion planner. Wenyuan Zeng, Wenjie Luo, Simon Suo, CVPR. Wenyuan Zeng, Wenjie Luo, Simon Suo, Abbas Sadat, Bin Yang, Sergio Casas, and Raquel Urtasun. End-to-end inter- pretable neural motion planner. In CVPR, 2021. 2\n", "annotations": {"author": "[{\"end\":91,\"start\":77},{\"end\":105,\"start\":92},{\"end\":116,\"start\":106},{\"end\":126,\"start\":117},{\"end\":136,\"start\":127},{\"end\":148,\"start\":137},{\"end\":159,\"start\":149},{\"end\":174,\"start\":160},{\"end\":189,\"start\":175},{\"end\":211,\"start\":190}]", "publisher": null, "author_last_name": "[{\"end\":90,\"start\":84},{\"end\":104,\"start\":98},{\"end\":115,\"start\":110},{\"end\":125,\"start\":121},{\"end\":135,\"start\":131},{\"end\":147,\"start\":142},{\"end\":158,\"start\":154},{\"end\":173,\"start\":165},{\"end\":188,\"start\":181},{\"end\":210,\"start\":202}]", "author_first_name": "[{\"end\":83,\"start\":77},{\"end\":97,\"start\":92},{\"end\":109,\"start\":106},{\"end\":120,\"start\":117},{\"end\":130,\"start\":127},{\"end\":141,\"start\":137},{\"end\":153,\"start\":149},{\"end\":164,\"start\":160},{\"end\":180,\"start\":175},{\"end\":195,\"start\":190},{\"end\":201,\"start\":196}]", "author_affiliation": null, "title": "[{\"end\":74,\"start\":1},{\"end\":285,\"start\":212}]", "venue": null, "abstract": "[{\"end\":1424,\"start\":287}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1642,\"start\":1639},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1644,\"start\":1642},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2119,\"start\":2116},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2121,\"start\":2119},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2124,\"start\":2121},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2126,\"start\":2124},{\"end\":3455,\"start\":3449},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4183,\"start\":4180},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4599,\"start\":4596},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4839,\"start\":4835},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5123,\"start\":5120},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5334,\"start\":5330},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5653,\"start\":5650},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6226,\"start\":6222},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6411,\"start\":6408},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7108,\"start\":7104},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7261,\"start\":7258},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7409,\"start\":7406},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7559,\"start\":7555},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8260,\"start\":8256},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8480,\"start\":8476},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8686,\"start\":8683},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9338,\"start\":9335},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9588,\"start\":9584},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9591,\"start\":9588},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9594,\"start\":9591},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9782,\"start\":9778},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9785,\"start\":9782},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9998,\"start\":9994},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10239,\"start\":10235},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10330,\"start\":10326},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10333,\"start\":10330},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10422,\"start\":10418},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11905,\"start\":11901},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11927,\"start\":11923},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11978,\"start\":11974},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":14565,\"start\":14561}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":17782,\"start\":17355}]", "paragraph": "[{\"end\":2013,\"start\":1440},{\"end\":3456,\"start\":2015},{\"end\":3556,\"start\":3458},{\"end\":3681,\"start\":3558},{\"end\":3825,\"start\":3683},{\"end\":3924,\"start\":3827},{\"end\":4047,\"start\":3941},{\"end\":4570,\"start\":4049},{\"end\":5396,\"start\":4572},{\"end\":5618,\"start\":5398},{\"end\":6016,\"start\":5620},{\"end\":6213,\"start\":6018},{\"end\":6968,\"start\":6215},{\"end\":7793,\"start\":6970},{\"end\":7994,\"start\":7795},{\"end\":9890,\"start\":7996},{\"end\":10526,\"start\":9892},{\"end\":10823,\"start\":10528},{\"end\":11695,\"start\":10835},{\"end\":12047,\"start\":11697},{\"end\":12483,\"start\":12049},{\"end\":12628,\"start\":12498},{\"end\":13472,\"start\":12641},{\"end\":13562,\"start\":13482},{\"end\":13810,\"start\":13564},{\"end\":14837,\"start\":13812},{\"end\":15015,\"start\":14849},{\"end\":15336,\"start\":15035},{\"end\":15624,\"start\":15338},{\"end\":15868,\"start\":15626},{\"end\":15970,\"start\":15870},{\"end\":16451,\"start\":15972},{\"end\":16565,\"start\":16453},{\"end\":16981,\"start\":16567},{\"end\":17354,\"start\":16996}]", "formula": null, "table_ref": "[{\"end\":4077,\"start\":4070}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1438,\"start\":1426},{\"attributes\":{\"n\":\"2.\"},\"end\":3939,\"start\":3927},{\"attributes\":{\"n\":\"3.\"},\"end\":10833,\"start\":10826},{\"attributes\":{\"n\":\"4.\"},\"end\":12496,\"start\":12486},{\"attributes\":{\"n\":\"4.1.\"},\"end\":12639,\"start\":12631},{\"attributes\":{\"n\":\"4.2.\"},\"end\":13480,\"start\":13475},{\"attributes\":{\"n\":\"4.3.\"},\"end\":14847,\"start\":14840},{\"end\":15033,\"start\":15018},{\"attributes\":{\"n\":\"5.\"},\"end\":16994,\"start\":16984},{\"end\":17366,\"start\":17356}]", "table": null, "figure_caption": "[{\"end\":17782,\"start\":17368}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2607,\"start\":2600},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2789,\"start\":2782},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3145,\"start\":3138}]", "bib_author_first_name": "[{\"end\":17879,\"start\":17871},{\"end\":17895,\"start\":17889},{\"end\":17912,\"start\":17904},{\"end\":18291,\"start\":18285},{\"end\":18304,\"start\":18300},{\"end\":18324,\"start\":18317},{\"end\":18556,\"start\":18552},{\"end\":18572,\"start\":18565},{\"end\":18587,\"start\":18581},{\"end\":18600,\"start\":18593},{\"end\":18615,\"start\":18608},{\"end\":18631,\"start\":18622},{\"end\":18641,\"start\":18637},{\"end\":18900,\"start\":18894},{\"end\":18914,\"start\":18909},{\"end\":18928,\"start\":18924},{\"end\":18930,\"start\":18929},{\"end\":18944,\"start\":18937},{\"end\":18957,\"start\":18951},{\"end\":18962,\"start\":18958},{\"end\":18975,\"start\":18970},{\"end\":18985,\"start\":18980},{\"end\":18998,\"start\":18996},{\"end\":19013,\"start\":19004},{\"end\":19027,\"start\":19022},{\"end\":19335,\"start\":19329},{\"end\":19348,\"start\":19343},{\"end\":19362,\"start\":19356},{\"end\":19564,\"start\":19555},{\"end\":19576,\"start\":19572},{\"end\":19578,\"start\":19577},{\"end\":19595,\"start\":19588},{\"end\":19613,\"start\":19606},{\"end\":19629,\"start\":19621},{\"end\":19641,\"start\":19635},{\"end\":19654,\"start\":19652},{\"end\":19666,\"start\":19661},{\"end\":19678,\"start\":19673},{\"end\":19690,\"start\":19686},{\"end\":19705,\"start\":19700},{\"end\":20006,\"start\":20000},{\"end\":20026,\"start\":20020},{\"end\":20038,\"start\":20032},{\"end\":20057,\"start\":20050},{\"end\":20072,\"start\":20065},{\"end\":20331,\"start\":20326},{\"end\":20349,\"start\":20342},{\"end\":20365,\"start\":20357},{\"end\":20653,\"start\":20646},{\"end\":20668,\"start\":20662},{\"end\":20684,\"start\":20675},{\"end\":20700,\"start\":20694},{\"end\":20943,\"start\":20937},{\"end\":20955,\"start\":20949},{\"end\":20969,\"start\":20964},{\"end\":20984,\"start\":20979},{\"end\":20999,\"start\":20994},{\"end\":21300,\"start\":21296},{\"end\":21315,\"start\":21310},{\"end\":21329,\"start\":21325},{\"end\":21682,\"start\":21678},{\"end\":21684,\"start\":21683},{\"end\":21698,\"start\":21691},{\"end\":21711,\"start\":21705},{\"end\":21726,\"start\":21720},{\"end\":21738,\"start\":21733},{\"end\":21750,\"start\":21745},{\"end\":22020,\"start\":22015},{\"end\":22030,\"start\":22029},{\"end\":22041,\"start\":22040},{\"end\":22056,\"start\":22052},{\"end\":22309,\"start\":22303},{\"end\":22323,\"start\":22319},{\"end\":22341,\"start\":22336},{\"end\":22356,\"start\":22351},{\"end\":22375,\"start\":22364},{\"end\":22390,\"start\":22384},{\"end\":22408,\"start\":22402},{\"end\":22723,\"start\":22718},{\"end\":22738,\"start\":22733},{\"end\":22749,\"start\":22744},{\"end\":22971,\"start\":22965},{\"end\":22988,\"start\":22981},{\"end\":23004,\"start\":22997},{\"end\":23344,\"start\":23337},{\"end\":23346,\"start\":23345},{\"end\":23354,\"start\":23351},{\"end\":23367,\"start\":23361},{\"end\":23379,\"start\":23376},{\"end\":23389,\"start\":23385},{\"end\":23400,\"start\":23394},{\"end\":23415,\"start\":23407},{\"end\":23739,\"start\":23734},{\"end\":23753,\"start\":23747},{\"end\":23765,\"start\":23759},{\"end\":23785,\"start\":23777},{\"end\":23796,\"start\":23791},{\"end\":23810,\"start\":23804},{\"end\":24098,\"start\":24092},{\"end\":24115,\"start\":24105},{\"end\":24126,\"start\":24121},{\"end\":24141,\"start\":24135},{\"end\":24441,\"start\":24434},{\"end\":24452,\"start\":24448},{\"end\":24465,\"start\":24459},{\"end\":24484,\"start\":24478},{\"end\":24502,\"start\":24495},{\"end\":24514,\"start\":24510},{\"end\":24534,\"start\":24527},{\"end\":24547,\"start\":24542},{\"end\":24550,\"start\":24548},{\"end\":24845,\"start\":24840},{\"end\":24859,\"start\":24856},{\"end\":24871,\"start\":24864},{\"end\":24887,\"start\":24881},{\"end\":25114,\"start\":25112},{\"end\":25127,\"start\":25121},{\"end\":25144,\"start\":25139},{\"end\":25157,\"start\":25153},{\"end\":25177,\"start\":25170},{\"end\":25179,\"start\":25178},{\"end\":25388,\"start\":25381},{\"end\":25400,\"start\":25394},{\"end\":25414,\"start\":25407},{\"end\":25755,\"start\":25748},{\"end\":25768,\"start\":25762},{\"end\":25779,\"start\":25774}]", "bib_author_last_name": "[{\"end\":17887,\"start\":17880},{\"end\":17902,\"start\":17896},{\"end\":17922,\"start\":17913},{\"end\":18298,\"start\":18292},{\"end\":18315,\"start\":18305},{\"end\":18330,\"start\":18325},{\"end\":18563,\"start\":18557},{\"end\":18579,\"start\":18573},{\"end\":18591,\"start\":18588},{\"end\":18606,\"start\":18601},{\"end\":18620,\"start\":18616},{\"end\":18635,\"start\":18632},{\"end\":18649,\"start\":18642},{\"end\":18907,\"start\":18901},{\"end\":18922,\"start\":18915},{\"end\":18935,\"start\":18931},{\"end\":18949,\"start\":18945},{\"end\":18968,\"start\":18963},{\"end\":18978,\"start\":18976},{\"end\":18994,\"start\":18986},{\"end\":19002,\"start\":18999},{\"end\":19020,\"start\":19014},{\"end\":19035,\"start\":19028},{\"end\":19341,\"start\":19336},{\"end\":19354,\"start\":19349},{\"end\":19370,\"start\":19363},{\"end\":19570,\"start\":19565},{\"end\":19586,\"start\":19579},{\"end\":19604,\"start\":19596},{\"end\":19619,\"start\":19614},{\"end\":19633,\"start\":19630},{\"end\":19650,\"start\":19642},{\"end\":19659,\"start\":19655},{\"end\":19671,\"start\":19667},{\"end\":19684,\"start\":19679},{\"end\":19698,\"start\":19691},{\"end\":19710,\"start\":19706},{\"end\":20018,\"start\":20007},{\"end\":20030,\"start\":20027},{\"end\":20048,\"start\":20039},{\"end\":20063,\"start\":20058},{\"end\":20079,\"start\":20073},{\"end\":20340,\"start\":20332},{\"end\":20355,\"start\":20350},{\"end\":20371,\"start\":20366},{\"end\":20660,\"start\":20654},{\"end\":20673,\"start\":20669},{\"end\":20692,\"start\":20685},{\"end\":20708,\"start\":20701},{\"end\":20947,\"start\":20944},{\"end\":20962,\"start\":20956},{\"end\":20977,\"start\":20970},{\"end\":20992,\"start\":20985},{\"end\":21006,\"start\":21000},{\"end\":21308,\"start\":21301},{\"end\":21323,\"start\":21316},{\"end\":21339,\"start\":21330},{\"end\":21689,\"start\":21685},{\"end\":21703,\"start\":21699},{\"end\":21718,\"start\":21712},{\"end\":21731,\"start\":21727},{\"end\":21743,\"start\":21739},{\"end\":21758,\"start\":21751},{\"end\":22027,\"start\":22021},{\"end\":22038,\"start\":22031},{\"end\":22050,\"start\":22042},{\"end\":22062,\"start\":22057},{\"end\":22317,\"start\":22310},{\"end\":22334,\"start\":22324},{\"end\":22349,\"start\":22342},{\"end\":22362,\"start\":22357},{\"end\":22382,\"start\":22376},{\"end\":22400,\"start\":22391},{\"end\":22420,\"start\":22409},{\"end\":22731,\"start\":22724},{\"end\":22742,\"start\":22739},{\"end\":22756,\"start\":22750},{\"end\":22979,\"start\":22972},{\"end\":22995,\"start\":22989},{\"end\":23011,\"start\":23005},{\"end\":23349,\"start\":23347},{\"end\":23359,\"start\":23355},{\"end\":23374,\"start\":23368},{\"end\":23383,\"start\":23380},{\"end\":23392,\"start\":23390},{\"end\":23405,\"start\":23401},{\"end\":23424,\"start\":23416},{\"end\":23745,\"start\":23740},{\"end\":23757,\"start\":23754},{\"end\":23775,\"start\":23766},{\"end\":23789,\"start\":23786},{\"end\":23802,\"start\":23797},{\"end\":23818,\"start\":23811},{\"end\":24103,\"start\":24099},{\"end\":24119,\"start\":24116},{\"end\":24133,\"start\":24127},{\"end\":24148,\"start\":24142},{\"end\":24446,\"start\":24442},{\"end\":24457,\"start\":24453},{\"end\":24476,\"start\":24466},{\"end\":24493,\"start\":24485},{\"end\":24508,\"start\":24503},{\"end\":24525,\"start\":24515},{\"end\":24540,\"start\":24535},{\"end\":24557,\"start\":24551},{\"end\":24854,\"start\":24846},{\"end\":24862,\"start\":24860},{\"end\":24879,\"start\":24872},{\"end\":24894,\"start\":24888},{\"end\":25119,\"start\":25115},{\"end\":25137,\"start\":25128},{\"end\":25151,\"start\":25145},{\"end\":25168,\"start\":25158},{\"end\":25185,\"start\":25180},{\"end\":25392,\"start\":25389},{\"end\":25405,\"start\":25401},{\"end\":25425,\"start\":25415},{\"end\":25760,\"start\":25756},{\"end\":25772,\"start\":25769},{\"end\":25783,\"start\":25780}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":3007213},\"end\":18201,\"start\":17807},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":54457648},\"end\":18489,\"start\":18203},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":54461709},\"end\":18837,\"start\":18491},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":85517967},\"end\":19270,\"start\":18839},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":231632538},\"end\":19498,\"start\":19272},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":198162846},\"end\":19958,\"start\":19500},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":5550767},\"end\":20226,\"start\":19960},{\"attributes\":{\"doi\":\"arXiv:2104.10133\",\"id\":\"b7\"},\"end\":20602,\"start\":20228},{\"attributes\":{\"id\":\"b8\"},\"end\":20858,\"start\":20604},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":224704936},\"end\":21294,\"start\":20860},{\"attributes\":{\"doi\":\"arXiv:2006.14480\",\"id\":\"b10\"},\"end\":21608,\"start\":21296},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":55701967},\"end\":21937,\"start\":21610},{\"attributes\":{\"id\":\"b12\"},\"end\":22226,\"start\":21939},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":208512696},\"end\":22646,\"start\":22228},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":215827670},\"end\":22898,\"start\":22648},{\"attributes\":{\"doi\":\"2021. 3\",\"id\":\"b15\",\"matched_paper_id\":233148602},\"end\":23278,\"start\":22900},{\"attributes\":{\"doi\":\"arXiv:2103.05073\",\"id\":\"b16\"},\"end\":23654,\"start\":23280},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":204009108},\"end\":24012,\"start\":23656},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":20999239},\"end\":24358,\"start\":24014},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":53350266},\"end\":24796,\"start\":24360},{\"attributes\":{\"doi\":\"arXiv:2103.14258\",\"id\":\"b20\"},\"end\":25068,\"start\":24798},{\"attributes\":{\"doi\":\"arXiv:1906.03199\",\"id\":\"b21\"},\"end\":25379,\"start\":25070},{\"attributes\":{\"doi\":\"arXiv:2006.11275\",\"id\":\"b22\"},\"end\":25641,\"start\":25381},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":198906510},\"end\":25951,\"start\":25643}]", "bib_title": "[{\"end\":17869,\"start\":17807},{\"end\":18283,\"start\":18203},{\"end\":18550,\"start\":18491},{\"end\":18892,\"start\":18839},{\"end\":19327,\"start\":19272},{\"end\":19553,\"start\":19500},{\"end\":19998,\"start\":19960},{\"end\":20935,\"start\":20860},{\"end\":21676,\"start\":21610},{\"end\":22013,\"start\":21939},{\"end\":22301,\"start\":22228},{\"end\":22716,\"start\":22648},{\"end\":22963,\"start\":22900},{\"end\":23732,\"start\":23656},{\"end\":24090,\"start\":24014},{\"end\":24432,\"start\":24360},{\"end\":25746,\"start\":25643}]", "bib_author": "[{\"end\":17889,\"start\":17871},{\"end\":17904,\"start\":17889},{\"end\":17924,\"start\":17904},{\"end\":18300,\"start\":18285},{\"end\":18317,\"start\":18300},{\"end\":18332,\"start\":18317},{\"end\":18565,\"start\":18552},{\"end\":18581,\"start\":18565},{\"end\":18593,\"start\":18581},{\"end\":18608,\"start\":18593},{\"end\":18622,\"start\":18608},{\"end\":18637,\"start\":18622},{\"end\":18651,\"start\":18637},{\"end\":18909,\"start\":18894},{\"end\":18924,\"start\":18909},{\"end\":18937,\"start\":18924},{\"end\":18951,\"start\":18937},{\"end\":18970,\"start\":18951},{\"end\":18980,\"start\":18970},{\"end\":18996,\"start\":18980},{\"end\":19004,\"start\":18996},{\"end\":19022,\"start\":19004},{\"end\":19037,\"start\":19022},{\"end\":19343,\"start\":19329},{\"end\":19356,\"start\":19343},{\"end\":19372,\"start\":19356},{\"end\":19572,\"start\":19555},{\"end\":19588,\"start\":19572},{\"end\":19606,\"start\":19588},{\"end\":19621,\"start\":19606},{\"end\":19635,\"start\":19621},{\"end\":19652,\"start\":19635},{\"end\":19661,\"start\":19652},{\"end\":19673,\"start\":19661},{\"end\":19686,\"start\":19673},{\"end\":19700,\"start\":19686},{\"end\":19712,\"start\":19700},{\"end\":20020,\"start\":20000},{\"end\":20032,\"start\":20020},{\"end\":20050,\"start\":20032},{\"end\":20065,\"start\":20050},{\"end\":20081,\"start\":20065},{\"end\":20342,\"start\":20326},{\"end\":20357,\"start\":20342},{\"end\":20373,\"start\":20357},{\"end\":20662,\"start\":20646},{\"end\":20675,\"start\":20662},{\"end\":20694,\"start\":20675},{\"end\":20710,\"start\":20694},{\"end\":20949,\"start\":20937},{\"end\":20964,\"start\":20949},{\"end\":20979,\"start\":20964},{\"end\":20994,\"start\":20979},{\"end\":21008,\"start\":20994},{\"end\":21310,\"start\":21296},{\"end\":21325,\"start\":21310},{\"end\":21341,\"start\":21325},{\"end\":21691,\"start\":21678},{\"end\":21705,\"start\":21691},{\"end\":21720,\"start\":21705},{\"end\":21733,\"start\":21720},{\"end\":21745,\"start\":21733},{\"end\":21760,\"start\":21745},{\"end\":22029,\"start\":22015},{\"end\":22040,\"start\":22029},{\"end\":22052,\"start\":22040},{\"end\":22064,\"start\":22052},{\"end\":22319,\"start\":22303},{\"end\":22336,\"start\":22319},{\"end\":22351,\"start\":22336},{\"end\":22364,\"start\":22351},{\"end\":22384,\"start\":22364},{\"end\":22402,\"start\":22384},{\"end\":22422,\"start\":22402},{\"end\":22733,\"start\":22718},{\"end\":22744,\"start\":22733},{\"end\":22758,\"start\":22744},{\"end\":22981,\"start\":22965},{\"end\":22997,\"start\":22981},{\"end\":23013,\"start\":22997},{\"end\":23351,\"start\":23337},{\"end\":23361,\"start\":23351},{\"end\":23376,\"start\":23361},{\"end\":23385,\"start\":23376},{\"end\":23394,\"start\":23385},{\"end\":23407,\"start\":23394},{\"end\":23426,\"start\":23407},{\"end\":23747,\"start\":23734},{\"end\":23759,\"start\":23747},{\"end\":23777,\"start\":23759},{\"end\":23791,\"start\":23777},{\"end\":23804,\"start\":23791},{\"end\":23820,\"start\":23804},{\"end\":24105,\"start\":24092},{\"end\":24121,\"start\":24105},{\"end\":24135,\"start\":24121},{\"end\":24150,\"start\":24135},{\"end\":24448,\"start\":24434},{\"end\":24459,\"start\":24448},{\"end\":24478,\"start\":24459},{\"end\":24495,\"start\":24478},{\"end\":24510,\"start\":24495},{\"end\":24527,\"start\":24510},{\"end\":24542,\"start\":24527},{\"end\":24559,\"start\":24542},{\"end\":24856,\"start\":24840},{\"end\":24864,\"start\":24856},{\"end\":24881,\"start\":24864},{\"end\":24896,\"start\":24881},{\"end\":25121,\"start\":25112},{\"end\":25139,\"start\":25121},{\"end\":25153,\"start\":25139},{\"end\":25170,\"start\":25153},{\"end\":25187,\"start\":25170},{\"end\":25394,\"start\":25381},{\"end\":25407,\"start\":25394},{\"end\":25427,\"start\":25407},{\"end\":25762,\"start\":25748},{\"end\":25774,\"start\":25762},{\"end\":25785,\"start\":25774}]", "bib_venue": "[{\"end\":17972,\"start\":17924},{\"end\":18335,\"start\":18332},{\"end\":18655,\"start\":18651},{\"end\":19041,\"start\":19037},{\"end\":19376,\"start\":19372},{\"end\":19716,\"start\":19712},{\"end\":20085,\"start\":20081},{\"end\":20324,\"start\":20228},{\"end\":20644,\"start\":20604},{\"end\":21068,\"start\":21008},{\"end\":21423,\"start\":21357},{\"end\":21764,\"start\":21760},{\"end\":22071,\"start\":22064},{\"end\":22426,\"start\":22422},{\"end\":22762,\"start\":22758},{\"end\":23078,\"start\":23020},{\"end\":23335,\"start\":23280},{\"end\":23824,\"start\":23820},{\"end\":24176,\"start\":24150},{\"end\":24569,\"start\":24559},{\"end\":24838,\"start\":24798},{\"end\":25110,\"start\":25070},{\"end\":25487,\"start\":25443},{\"end\":25789,\"start\":25785},{\"end\":18016,\"start\":17974}]"}}}, "year": 2023, "month": 12, "day": 17}