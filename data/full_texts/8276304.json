{"id": 8276304, "updated": "2022-03-23 05:38:09.672", "metadata": {"title": "Moving camera background-subtraction for obstacle detection on railway tracks", "authors": "[{\"first\":\"Hiroki\",\"last\":\"Mukojima\",\"middle\":[]},{\"first\":\"Daisuke\",\"last\":\"Deguchi\",\"middle\":[]},{\"first\":\"Yasutomo\",\"last\":\"Kawanishi\",\"middle\":[]},{\"first\":\"Ichiro\",\"last\":\"Ide\",\"middle\":[]},{\"first\":\"Hiroshi\",\"last\":\"Murase\",\"middle\":[]},{\"first\":\"Masato\",\"last\":\"Ukai\",\"middle\":[]},{\"first\":\"Nozomi\",\"last\":\"Nagamine\",\"middle\":[]},{\"first\":\"Ryuta\",\"last\":\"Nakasone\",\"middle\":[]}]", "venue": "2016 IEEE International Conference on Image Processing (ICIP)", "journal": "2016 IEEE International Conference on Image Processing (ICIP)", "publication_date": {"year": 2016, "month": null, "day": null}, "abstract": "We propose a method for detecting obstacles by comparing input and reference train frontal view camera images. In the field of obstacle detection, most methods employ a machine learning approach, so they can only detect pre-trained classes, such as pedestrian, bicycle, etc. This means that obstacles of unknown classes cannot be detected. To overcome this problem, we propose a background subtraction method that can be applied to moving cameras. First, the proposed method computes frame-by-frame correspondences between the current and the reference (database) image sequences. Then, obstacles are detected by applying image subtraction to corresponding frames. To confirm the effectiveness of the proposed method, we conducted an experiment using several image sequences captured on an experimental track. Its results showed that the proposed method could detect various obstacles accurately and effectively.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2512305642", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icip/MukojimaDKIMUNN16", "doi": "10.1109/icip.2016.7533104"}}, "content": {"source": {"pdf_hash": "9598364c34be276a178962cddeb7d3616539fffe", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "7e005db1c0356b7839493c544a432657b26a08de", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9598364c34be276a178962cddeb7d3616539fffe.txt", "contents": "\nMOVING CAMERA BACKGROUND-SUBTRACTION FOR OBSTACLE DETECTION ON RAILWAY TRACKS\n\n\nHiroki Mukojima \nGraduate School of Information Science\nNagoya University\n\n\nDaisuke Deguchi \nInformation Strategy Office\nNagoya University\nFuro-cho, Chikusa-ku, Nagoya-shiAichiJapan\n\nYasutomo Kawanishi \nGraduate School of Information Science\nNagoya University\n\n\nIchiro Ide \nGraduate School of Information Science\nNagoya University\n\n\nHiroshi Murase \nGraduate School of Information Science\nNagoya University\n\n\nMasato Ukai \nRailway Technical Research Institute\n2-8-38 Hikari-cho, Kokubunji-shiTokyoJapan\n\nNozomi Nagamine \nRailway Technical Research Institute\n2-8-38 Hikari-cho, Kokubunji-shiTokyoJapan\n\nRyuta Nakasone \nRailway Technical Research Institute\n2-8-38 Hikari-cho, Kokubunji-shiTokyoJapan\n\nMOVING CAMERA BACKGROUND-SUBTRACTION FOR OBSTACLE DETECTION ON RAILWAY TRACKS\nIndex Terms-Railway safetyObject detectionSub- traction techniques\nWe propose a method for detecting obstacles by comparing input and reference train frontal view camera images. In the field of obstacle detection, most methods employ a machine learning approach, so they can only detect pre-trained classes, such as pedestrian, bicycle, etc. This means that obstacles of unknown classes cannot be detected. To overcome this problem, we propose a background subtraction method that can be applied to moving cameras. First, the proposed method computes frame-by-frame correspondences between the current and the reference (database) image sequences. Then, obstacles are detected by applying image subtraction to corresponding frames. To confirm the effectiveness of the proposed method, we conducted an experiment using several image sequences captured on an experimental track. Its results showed that the proposed method could detect various obstacles accurately and effectively.\n\nINTRODUCTION\n\nRailway accidents caused by obstacles are one of the most important issues that should be solved. There is a demand for obstacle detection systems, and accordingly, a surveillance system for detecting obstacles in level crossings has been developed [1]. However, the area that can be monitored by this system is restricted due to a fixed camera.\n\nOn the other hand, various sensing devices can be used for obstacle detection by being mounted on the front of a train. Since these devices do not require large modifications to the current railway system, especially to ground-side facilities, they may be easily introduced. Therefore, obstacle detection methods using frontal view sensors are expected [2,3,4,5,6,7]. However, in the case of railway, distant obstacles must be detected since the braking distance of a train is very long. Therefore, using millimeter-wave RADAR and LIDAR is not an option due to their low resolutions. In addition, using multiple sensors increases the cost. From this point of view, a train frontal view camera can be considered as the option for obstacle detection in a railway system.\n\nObject detection by camera is one of the most active research areas in the computer vision field, and numerous methods have been proposed [6,7,8,9,10]. Most methods employ a machine learning approach, and they can detect pre-trained objects, such as pedestrian, bicycle, etc. However, unknown objects cannot be detected by these methods. Although background subtraction could be a solution, it cannot be simply applied to a train frontal view camera, since it moves together with the train. Therefore, it is important to develop a method for forward obstacles detection based on background subtraction that can be applied to a train frontal view camera moving together with the train.\n\nAlthough few research groups have tackled background subtraction for moving cameras, most of them use a single image sequence and only moving objects can be detected [11,12]. Meanwhile, Kyutoku et al. proposed a method for detecting general obstacles by a car mounted camera by subtracting the current image sequence from the reference (database) image sequence [13]. By assuming that these two image sequences are captured on slightly different driving paths, this method succeeded to accurately align two image sequences with the metric. This assumption requires sufficient base-line length between cameras capturing the two image sequences to compute the metric between the sequences. However, in the case of railway, sufficient base-line length cannot be obtained since trains always run on the same tracks. In addition, since this method only aligns road surfaces between two image sequences, a large registration error will occur outside of it. Thus, distant / small obstacles cannot be distinguished accurately due to noise caused by the image registration error.\n\nTherefore, we propose a moving camera background subtraction method,which method detects obstacles by compar- ing input and reference images. The contributions of this paper are:\n\n1. Introduction of a new metric that can align two image sequences even if the base-line length between two cameras is small.\n\n2. Detection of arbitrary distant obstacles by pixel-wise image registration and integration of multiple image subtraction mechanisms.\n\n\nMOVING CAMERA BACKGROUND SUBTRACTION FOR OBSTACLE DETECTION\n\nTo detect obstacles by subtracting two image sequences, pixel-level alignment is needed. In the case of a train frontal view camera, since an image sequence is captured from a moving train, two image sequences must be aligned both spatially and temporally. To solve this, the proposed method first finds a reference frame captured at the most similar location to the current frame by image sequence matching. Then, it performs pixel-wise registration between the current frame and its corresponding reference frame. Finally, multiple image subtraction methods are applied to compute the image difference between the two frames, and obstacles are detected by integrating their outputs. Figure 1 shows the framework of the proposed method.\n\n\nTemporal alignment: Computation of frame-byframe correspondences\n\nIn the case of railway, train frontal view cameras always take the same trajectory since trains run on the same track. This results in a very short base-line length between cameras of the current and the reference image sequences. To cope with this situation, the proposed method introduces a new metric to align the two image sequences. Figure 2 shows close and distant train frontal views of the current and the reference image sequences. Let the current and the reference image sequences be F = {f 1 , f 2 , ..., f p } and G = {g 1 , g 2 , ..., g q }, respectively. Here, f i denotes the i-th frame of the current image sequence, and g j denotes the j-th frame of the reference image sequence. First, the proposed method computes the frame-by-frame correspondences between sequences F and G. Next, the distance d(i, j) between the current frame f i and the reference frame g j is calculated as the variance of angles between corresponding key-points as,\nd(i, j) = 1 nij nij k=1 h(\u03b8 ijk \u2212 m ij ) (n ij \u2265 \u03b1) \u221e (otherwise) ,(1)h(x) = min {x 2 , (x + 2\u03c0) 2 , (x \u2212 2\u03c0) 2 },(2)\nwhere n ij is the number of corresponding key-point pairs between f i and g j , \u03b8 ijk is the angle of the k-th key-point pair represented by the polar coordinate system, m ij is the mean of \u03b8 ijk , and \u03b1 is a positive constant. Here, the angle is represented by relative angle from the x-axis. In this equation, if the current frame is captured at a camera position close to the reference frame, the variance becomes small. Moreover, it can be computed regardless of the base-line length between two cameras. Finally, frame correspondences (f i , g j ) between the current and the reference image sequences are obtained by applying Dynamic Time Warping to minimize d(i, j). Figure 3 shows an example of corresponding frames of the current and the reference image sequences.\n\n\nSpatial alignment: Computation of pixel-wise image registration for temporally aligned frames\n\nTo obtain accurate image alignment, the proposed method performs pixel-wise image registration against corresponding frames f i and g j obtained in the previous step. Here, DeepFlow [14] is used for calculating the deformation field from g j to f i . Then, completely aligned image g j is obtained by applying the deformation field to g j . Figure 4 shows the absolute image difference between the frames in Fig. 3. Figure 4(a) shows the image difference between the original frames |f i \u2212 g j |, and Fig. 4(b) shows the image difference  after pixel-wise alignment |f i \u2212 g j |. In these images, darker pixels indicate larger image errors.\n\n\nImage subtraction for completely aligned images\n\nRobustness against lighting conditions is one of the most important issues when developing a system for railways, since it needs to handle various environments. Here, multiple image subtraction metrics are combined to solve this problem. First, two types of image subtraction metrics are calculated from f i and g j . The first one is Normalized Vector Distance (NVD), and is calculated as,\nNVD(a, b) = a a \u2212 b b .(3)\nHere, a and b are image patches represented in vectors consisting of RGB channels. The second one is Radial Reach Filter (RRF) proposed by Satoh et al. [15]. RRF is calculated by comparing the intensity of each RGB channel between the target pixel and its surroundings. Next, to reduce noise, Gaussian filter is applied to difference images obtained by NVD and RRF. Then, two binary images d 1 ij and d 2 ij are obtained by thresholding. Here, the threshold T for the binarization is determined as,\nT = \u03bc ij + n\u03c3 ij ,(4)\nwhere \u03bc ij and \u03c3 ij are the average and the variance of each difference image, respectively. Finally, the extracted pixels are considered as candidates of obstacles.  Figure 5 shows examples of image differences calculated by NVD and RRF, and Fig. 6 shows the obstacles detected by thresholding the difference images in Fig. 5. In Fig. 5, the black pixels indicate large image differences and in Fig. 6, the white pixels indicate obstacle candidates.\n\nTo reduce the false positives of obstacles, the proposed method integrates the above NVD and RRF images. Before integration, to reduce salt and pepper noise caused by ballast, grasses, and shadow borders, morphological operations (opening and closing) are applied to both d 1 ij and d 2 ij . Then, connected-component labeling is applied, and the bounding-box of each connected-component is extracted.\n\nHere, there is a relation between the size of a bounding-box and the distance between the camera and the obstacle; A distant obstacle is observed as a small bounding-box, and a closer obstacle is observed as a large bounding-box. Therefore, the proposed method removes bounding-boxes that do not satisfy this relation. To evaluate this relation, the proposed method refers to the gauge (the width of the track) measured from the image, whose actual width is fixed and known. Then, the size of each bounding-box is compared with the gauge in the image, and small bounding-boxes are removed. In addition, the proposed method removes bounding-boxes outside the track because they do not obstruct the train. Here, the method by Nasuu et al. [16] is used for detecting tracks from a train frontal view image. These steps are applied to obstacle candidates of NVD and RRF, separately.\n\nFinally, the proposed method outputs obstacles detected by both NVD and RRF. Here, it outputs only overlapped bounding-boxes between NVD and RRF.   Fig. 9. Frame accuracy rate.\n\n\nEXPERIMENTS AND DISCUSSIONS\n\nTo evaluate the effectiveness of the proposed method, we prepared train frontal view images captured on a test line in the premises of the Railway Technical Research Institute, Japan. Grasshopper3 (Point Grey Research, Inc.) was mounted on 2.5 m height of the front view of a railway trolley. The size of captured images was 1,920 \u00d7 1,440 pixels, and the frame rate was 10 fps. The focal length of the camera was 25 mm, and the pixel pitch was 4.54 \u03bcm. In this experiment, the railway trolley was controlled manually. A total of 2,117 frames were contained in the dataset which was constructed by extracting frames in five frames interval from the recorded five videos. No obstacle existed in three videos, and the other two videos included a pedestrian and a box as obstacles, respectively. Bounding-boxes of all obstacles were annotated manually. One of the videos including no obstacle was used as the reference image sequence, and the other videos were used as the current image sequences. The result was evaluated by changing parameter n that controls the threshold in Eq. (4). The detection accuracy was evaluated by the following criteria: Here, if the detected bounding-box covered more than 10 % of an annotated obstacle region, we considered that it was detected correctly. Otherwise, it was considered as a false positive. \u0394x and \u0394y are the differences of the size between the detected obstacle and the annotated obstacle in x (horizontal) and y (vertical) coordinates. If the following condition was not satisfied, it was considered as a false positive.\n\n2\u0394x \u2264 w and 2\u0394y \u2264 h\n\nHere, w and h are the width and the height of the manually annotated obstacle, respectively. Frame correctness was cal-culated by counting the number of frames where obstacles were detected correctly. Based on these criteria, the proposed method was compared with methods using either NVD or RRF. Figure 8 shows the ROC curve of each method. As seen here, the proposed method could achieve the highest accuracy. In addition, from Fig. 9, we can see that the proposed method obtained the best frame correctness of 85.8 % when n = 3.8.\n\nAlthough the proposed method could obtain the best, it sometimes failed to detect obstacles. This could be explained that since the proposed method integrated NVD and RRF for detecting obstacles, the performance degradation of NVD or RRF affected the result of the proposed method. To overcome this problem, we will investigate other metrics for image subtraction.\n\n\nCONCLUSIONS\n\nThis paper proposed a method of moving camera background subtraction for forward obstacle detection from a train frontal view camera. To detect general obstacles, frame-by-frame correspondences between the current and the reference image sequences of train frontal view were computed based on the angle difference of corresponding key-points. After, pixelwise image registration, obstacles were detected by integrating two kinds of subtraction methods.\n\nTo demonstrate the effectiveness of the proposed method, experiments were conducted by capturing train frontal view image sequences on an experimental track. Its results showed the effectiveness of the proposed method.\n\nFuture works include introduction of a background modeling method and evaluation in various lighting conditions, seasons, and weathers.\n\nFig. 1 .\n1Framework of the proposed method.\n\nFig. 2 .\n2Differences of view by varying capturing locations.\n\n(a) Current frame f i (b) Reference frame g j Fig. 3 .\nij3Corresponding frames (f i , g j ) in the current and the reference image sequences.\n\nFig. 4 .\n4Examples of image difference between the corresponding frames inFig. 3.\n\nFig. 5 .Fig. 6 .\n56Image difference calculated by NVD and RRF. Obstacles detected by NVD and RRF.\n\nFig. 7 .\n7Example of a detected obstacle.\n\nFig. 8 .\n8ROC curves.\nAcknowledgementParts of this research were supported by JSPS Grant-in-Aid for Scientific Research.\nVisual monitoring of railroad grade crossing. Y Sheikh, Y Zhai, K Shafique, M Shah, Proc. SPIE, Sensors, and Command, Control, Communications, and Intelligence (C3I) Technologies for Homeland Security and Homeland Defense III. SPIE, Sensors, and Command, Control, Communications, and Intelligence (C3I) Technologies for Homeland Security and Homeland Defense III5403Y. Sheikh, Y. Zhai, K. Shafique, and M. Shah, \"Visual monitoring of railroad grade crossing,\" in Proc. SPIE, Sensors, and Command, Control, Communications, and Intelligence (C3I) Technologies for Homeland Security and Homeland Defense III, 2004, vol. 5403, pp. 654- 660.\n\nObstacle detection using millimeter-wave RADAR and its visualization on image sequence. S Sugimoto, H Tateda, H Takahashi, M Okutomi, Proc. 17th Int. 17th IntS. Sugimoto, H. Tateda, H. Takahashi, and M. Okutomi, \"Obstacle detection using millimeter-wave RADAR and its visualization on image sequence,\" in Proc. 17th Int.\n\nConf. on Pattern Recognition (ICPR 2004). 3Conf. on Pattern Recognition (ICPR 2004), 2004, vol. 3, pp. 342-345.\n\nExploiting LIDAR-based features on pedestrian detection in urban scenarios. C Premebida, O Ludwig, U Nunes, Proc. 12th IEEE Int. Conf. on Intelligent Transportation Systems. 12th IEEE Int. Conf. on Intelligent Transportation SystemsC. Premebida, O. Ludwig, and U. Nunes, \"Exploiting LIDAR-based features on pedestrian detection in urban scenarios,\" in Proc. 12th IEEE Int. Conf. on Intelligent Transportation Systems, 2009, pp. 1-6.\n\nMulti-sensor obstacle detection on railway tracks. S Mockel, F Scherer, P F Schuster, Proc. 2003 IEEE Intelligent Vehicles Symp. (IV2003). 2003 IEEE Intelligent Vehicles Symp. (IV2003)S. Mockel, F. Scherer, and P. F. Schuster, \"Multi-sensor obstacle detection on railway tracks,\" in Proc. 2003 IEEE Intelligent Vehicles Symp. (IV2003), 2003, pp. 42- 46.\n\nAn obstacle detection system for automated trains. M Ruder, N Mohler, F Ahmed, Proc. 2003 IEEE Intelligent Vehicles Symp. (IV2003). 2003 IEEE Intelligent Vehicles Symp. (IV2003)M. Ruder, N. Mohler, and F. Ahmed, \"An obstacle detection system for automated trains,\" in Proc. 2003 IEEE Intelligent Vehicles Symp. (IV2003), 2003, pp. 180-185.\n\nHistograms of oriented gradients for human detection. N Dalal, B Triggs, Proc. 2005 IEEE Computer Society Conf. on Computer Vision and Pattern Recognition (CVPR2005). 2005 IEEE Computer Society Conf. on Computer Vision and Pattern Recognition (CVPR2005)1N. Dalal and B. Triggs, \"Histograms of oriented gradi- ents for human detection,\" in Proc. 2005 IEEE Com- puter Society Conf. on Computer Vision and Pattern Recognition (CVPR2005), 2005, vol. 1, pp. 886-893.\n\nA discriminatively trained, multiscale, Deformable Part Model. P Felzenszwalb, D Mcallester, D Ramanan, Proc. 2008 IEEE Computer Society Conf. on Computer Vision and Pattern Recognition (CVPR2008). 2008 IEEE Computer Society Conf. on Computer Vision and Pattern Recognition (CVPR2008)P. Felzenszwalb, D. McAllester, and D. Ramanan, \"A discriminatively trained, multiscale, Deformable Part Model,\" in Proc. 2008 IEEE Computer Society Conf. on Computer Vision and Pattern Recognition (CVPR2008), 2008, pp. 1-8.\n\nRapid object detection using a boosted cascade of simple features. P Viola, M Jones, Proc. 2001 IEEE Computer Society Conf. on Computer Vision and Pattern Recognition (CVPR2001). 2001 IEEE Computer Society Conf. on Computer Vision and Pattern Recognition (CVPR2001)1P. Viola and M. Jones, \"Rapid object detection using a boosted cascade of simple features,\" in Proc. 2001 IEEE Computer Society Conf. on Computer Vision and Pattern Recognition (CVPR2001), 2001, vol. 1, pp. 511-518.\n\nAdaptive background mixture models for real-time tracking. C Stauffer, W E L Grimson, Proc. 1999 IEEE Computer Society Conf. on Computer Vision and Pattern Recognition (CVPR1999). 1999 IEEE Computer Society Conf. on Computer Vision and Pattern Recognition (CVPR1999)2C. Stauffer and W. E. L. Grimson, \"Adaptive back- ground mixture models for real-time tracking,\" in Proc. 1999 IEEE Computer Society Conf. on Computer Vision and Pattern Recognition (CVPR1999), 1999, vol. 2, pp. 246-252.\n\nViBe: A universal background subtraction algorithm for video sequences. O Barnich, M V Droogenbroeck, IEEE Trans. on Image Processing. 206O. Barnich and M. V. Droogenbroeck, \"ViBe: A uni- versal background subtraction algorithm for video se- quences,\" IEEE Trans. on Image Processing, vol. 20, no. 6, pp. 1709-1724, 2010.\n\nOnline moving camera background subtraction. A Elqursh, A , Proc. 12th European Conf. on Computer Vision (ECCV2012). 12th European Conf. on Computer Vision (ECCV2012)6A. Elqursh and A. Elgammal, \"Online moving camera background subtraction,\" in Proc. 12th European Conf. on Computer Vision (ECCV2012), 2012, vol. 6, pp. 228- 241.\n\nBackground subtraction for freely moving cameras. Y Sheikh, O Javed, T Kanade, Proc. 12th IEEE Int. Conf. on Computer Vision (ICCV2009). 12th IEEE Int. Conf. on Computer Vision (ICCV2009)Y. Sheikh, O. Javed, and T. Kanade, \"Background sub- traction for freely moving cameras,\" in Proc. 12th IEEE Int. Conf. on Computer Vision (ICCV2009), 2009, pp. 1219-1225.\n\nSubtraction-based forward obstacle detection using illumination insensitive feature for driving-support. H Kyutoku, D Deguchi, T Takahashi, Y Mekada, I Ide, H Murase, Proc. Computer Vision in Vehicle Technology: From Earth to Mars (CVVT2012), Lecture Notes in Computer Science. Computer Vision in Vehicle Technology: From Earth to Mars (CVVT2012), Lecture Notes in Computer Science7584H. Kyutoku, D. Deguchi, T. Takahashi, Y. Mekada, I. Ide, and H. Murase, \"Subtraction-based forward ob- stacle detection using illumination insensitive feature for driving-support,\" in Proc. Computer Vision in Vehi- cle Technology: From Earth to Mars (CVVT2012), Lec- ture Notes in Computer Science, 2012, vol. 7584, pp. 515-525.\n\nDeepFlow: Large displacement optical flow with deep matching. P Weinzaepfel, J Revaud, Z Harchaoui, C Schmid, Proc. 14th IEEE Int. Conf. on Computer Vision (ICCV2013). 14th IEEE Int. Conf. on Computer Vision (ICCV2013)P. Weinzaepfel, J. Revaud, Z. Harchaoui, and C. Schmid, \"DeepFlow: Large displacement optical flow with deep matching,\" in Proc. 14th IEEE Int. Conf. on Computer Vision (ICCV2013), 2013, pp. 1385-1392.\n\nRobust event detection by Radial Reach Filter (RRF). Y Satoh, H Tanahashi, C Wang, S Kaneko, S Igarashi, Y Niwa, K Yamamoto, Proc. 16th IAPR Int. Conf. on Pattern Recognition (ICPR2002). 16th IAPR Int. Conf. on Pattern Recognition (ICPR2002)2Y. Satoh, H. Tanahashi, C. Wang, S. Kaneko, S. Igarashi, Y. Niwa, and K. Yamamoto, \"Robust event detection by Radial Reach Filter (RRF),\" in Proc. 16th IAPR Int. Conf. on Pattern Recognition (ICPR2002), 2002, vol. 2, pp. 623-626.\n\nA vision-based approach for rail extraction and its application in a camera pantilt control system. B T Nassu, M Ukai, IEEE Trans. on Intelligent Transportation Systems. 134B. T. Nassu and M. Ukai, \"A vision-based approach for rail extraction and its application in a camera pantilt con- trol system,\" IEEE Trans. on Intelligent Transportation Systems, vol. 13, no. 4, pp. 1763-1771, 2012.\n", "annotations": {"author": "[{\"end\":156,\"start\":81},{\"end\":263,\"start\":157},{\"end\":342,\"start\":264},{\"end\":413,\"start\":343},{\"end\":488,\"start\":414},{\"end\":582,\"start\":489},{\"end\":680,\"start\":583},{\"end\":777,\"start\":681}]", "publisher": null, "author_last_name": "[{\"end\":96,\"start\":88},{\"end\":172,\"start\":165},{\"end\":282,\"start\":273},{\"end\":353,\"start\":350},{\"end\":428,\"start\":422},{\"end\":500,\"start\":496},{\"end\":598,\"start\":590},{\"end\":695,\"start\":687}]", "author_first_name": "[{\"end\":87,\"start\":81},{\"end\":164,\"start\":157},{\"end\":272,\"start\":264},{\"end\":349,\"start\":343},{\"end\":421,\"start\":414},{\"end\":495,\"start\":489},{\"end\":589,\"start\":583},{\"end\":686,\"start\":681}]", "author_affiliation": "[{\"end\":155,\"start\":98},{\"end\":262,\"start\":174},{\"end\":341,\"start\":284},{\"end\":412,\"start\":355},{\"end\":487,\"start\":430},{\"end\":581,\"start\":502},{\"end\":679,\"start\":600},{\"end\":776,\"start\":697}]", "title": "[{\"end\":78,\"start\":1},{\"end\":855,\"start\":778}]", "venue": null, "abstract": "[{\"end\":1835,\"start\":923}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2103,\"start\":2100},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2554,\"start\":2551},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2556,\"start\":2554},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2558,\"start\":2556},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2560,\"start\":2558},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2562,\"start\":2560},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2564,\"start\":2562},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3109,\"start\":3106},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3111,\"start\":3109},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3113,\"start\":3111},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3115,\"start\":3113},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3118,\"start\":3115},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3824,\"start\":3820},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3827,\"start\":3824},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4020,\"start\":4016},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8169,\"start\":8165},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9249,\"start\":9245},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11210,\"start\":11206}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":14914,\"start\":14870},{\"attributes\":{\"id\":\"fig_1\"},\"end\":14977,\"start\":14915},{\"attributes\":{\"id\":\"fig_2\"},\"end\":15120,\"start\":14978},{\"attributes\":{\"id\":\"fig_3\"},\"end\":15203,\"start\":15121},{\"attributes\":{\"id\":\"fig_4\"},\"end\":15302,\"start\":15204},{\"attributes\":{\"id\":\"fig_5\"},\"end\":15345,\"start\":15303},{\"attributes\":{\"id\":\"fig_6\"},\"end\":15368,\"start\":15346}]", "paragraph": "[{\"end\":2196,\"start\":1851},{\"end\":2966,\"start\":2198},{\"end\":3652,\"start\":2968},{\"end\":4724,\"start\":3654},{\"end\":4904,\"start\":4726},{\"end\":5031,\"start\":4906},{\"end\":5167,\"start\":5033},{\"end\":5968,\"start\":5231},{\"end\":6993,\"start\":6037},{\"end\":7885,\"start\":7112},{\"end\":8623,\"start\":7983},{\"end\":9065,\"start\":8675},{\"end\":9591,\"start\":9093},{\"end\":10064,\"start\":9614},{\"end\":10467,\"start\":10066},{\"end\":11347,\"start\":10469},{\"end\":11525,\"start\":11349},{\"end\":13122,\"start\":11557},{\"end\":13143,\"start\":13124},{\"end\":13678,\"start\":13145},{\"end\":14044,\"start\":13680},{\"end\":14512,\"start\":14060},{\"end\":14732,\"start\":14514},{\"end\":14869,\"start\":14734}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7064,\"start\":6994},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7111,\"start\":7064},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9092,\"start\":9066},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9613,\"start\":9592}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1849,\"start\":1837},{\"attributes\":{\"n\":\"2.\"},\"end\":5229,\"start\":5170},{\"attributes\":{\"n\":\"2.1.\"},\"end\":6035,\"start\":5971},{\"attributes\":{\"n\":\"2.2.\"},\"end\":7981,\"start\":7888},{\"attributes\":{\"n\":\"2.3.\"},\"end\":8673,\"start\":8626},{\"attributes\":{\"n\":\"3.\"},\"end\":11555,\"start\":11528},{\"attributes\":{\"n\":\"4.\"},\"end\":14058,\"start\":14047},{\"end\":14879,\"start\":14871},{\"end\":14924,\"start\":14916},{\"end\":15033,\"start\":14979},{\"end\":15130,\"start\":15122},{\"end\":15221,\"start\":15205},{\"end\":15312,\"start\":15304},{\"end\":15355,\"start\":15347}]", "table": null, "figure_caption": "[{\"end\":14914,\"start\":14881},{\"end\":14977,\"start\":14926},{\"end\":15120,\"start\":15037},{\"end\":15203,\"start\":15132},{\"end\":15302,\"start\":15224},{\"end\":15345,\"start\":15314},{\"end\":15368,\"start\":15357}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5924,\"start\":5916},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6383,\"start\":6375},{\"end\":7794,\"start\":7786},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":8332,\"start\":8324},{\"end\":8397,\"start\":8391},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":8407,\"start\":8399},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":8493,\"start\":8484},{\"end\":9789,\"start\":9781},{\"end\":9863,\"start\":9857},{\"end\":9951,\"start\":9934},{\"end\":10016,\"start\":10010},{\"end\":11503,\"start\":11497},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":13450,\"start\":13442},{\"end\":13581,\"start\":13575}]", "bib_author_first_name": "[{\"end\":15515,\"start\":15514},{\"end\":15525,\"start\":15524},{\"end\":15533,\"start\":15532},{\"end\":15545,\"start\":15544},{\"end\":16195,\"start\":16194},{\"end\":16207,\"start\":16206},{\"end\":16217,\"start\":16216},{\"end\":16230,\"start\":16229},{\"end\":16618,\"start\":16617},{\"end\":16631,\"start\":16630},{\"end\":16641,\"start\":16640},{\"end\":17027,\"start\":17026},{\"end\":17037,\"start\":17036},{\"end\":17048,\"start\":17047},{\"end\":17050,\"start\":17049},{\"end\":17382,\"start\":17381},{\"end\":17391,\"start\":17390},{\"end\":17401,\"start\":17400},{\"end\":17726,\"start\":17725},{\"end\":17735,\"start\":17734},{\"end\":18198,\"start\":18197},{\"end\":18214,\"start\":18213},{\"end\":18228,\"start\":18227},{\"end\":18712,\"start\":18711},{\"end\":18721,\"start\":18720},{\"end\":19187,\"start\":19186},{\"end\":19199,\"start\":19198},{\"end\":19203,\"start\":19200},{\"end\":19689,\"start\":19688},{\"end\":19700,\"start\":19699},{\"end\":19702,\"start\":19701},{\"end\":19985,\"start\":19984},{\"end\":19996,\"start\":19995},{\"end\":20321,\"start\":20320},{\"end\":20331,\"start\":20330},{\"end\":20340,\"start\":20339},{\"end\":20736,\"start\":20735},{\"end\":20747,\"start\":20746},{\"end\":20758,\"start\":20757},{\"end\":20771,\"start\":20770},{\"end\":20781,\"start\":20780},{\"end\":20788,\"start\":20787},{\"end\":21408,\"start\":21407},{\"end\":21423,\"start\":21422},{\"end\":21433,\"start\":21432},{\"end\":21446,\"start\":21445},{\"end\":21820,\"start\":21819},{\"end\":21829,\"start\":21828},{\"end\":21842,\"start\":21841},{\"end\":21850,\"start\":21849},{\"end\":21860,\"start\":21859},{\"end\":21872,\"start\":21871},{\"end\":21880,\"start\":21879},{\"end\":22340,\"start\":22339},{\"end\":22342,\"start\":22341},{\"end\":22351,\"start\":22350}]", "bib_author_last_name": "[{\"end\":15522,\"start\":15516},{\"end\":15530,\"start\":15526},{\"end\":15542,\"start\":15534},{\"end\":15550,\"start\":15546},{\"end\":16204,\"start\":16196},{\"end\":16214,\"start\":16208},{\"end\":16227,\"start\":16218},{\"end\":16238,\"start\":16231},{\"end\":16628,\"start\":16619},{\"end\":16638,\"start\":16632},{\"end\":16647,\"start\":16642},{\"end\":17034,\"start\":17028},{\"end\":17045,\"start\":17038},{\"end\":17059,\"start\":17051},{\"end\":17388,\"start\":17383},{\"end\":17398,\"start\":17392},{\"end\":17407,\"start\":17402},{\"end\":17732,\"start\":17727},{\"end\":17742,\"start\":17736},{\"end\":18211,\"start\":18199},{\"end\":18225,\"start\":18215},{\"end\":18236,\"start\":18229},{\"end\":18718,\"start\":18713},{\"end\":18727,\"start\":18722},{\"end\":19196,\"start\":19188},{\"end\":19211,\"start\":19204},{\"end\":19697,\"start\":19690},{\"end\":19716,\"start\":19703},{\"end\":19993,\"start\":19986},{\"end\":20328,\"start\":20322},{\"end\":20337,\"start\":20332},{\"end\":20347,\"start\":20341},{\"end\":20744,\"start\":20737},{\"end\":20755,\"start\":20748},{\"end\":20768,\"start\":20759},{\"end\":20778,\"start\":20772},{\"end\":20785,\"start\":20782},{\"end\":20795,\"start\":20789},{\"end\":21420,\"start\":21409},{\"end\":21430,\"start\":21424},{\"end\":21443,\"start\":21434},{\"end\":21453,\"start\":21447},{\"end\":21826,\"start\":21821},{\"end\":21839,\"start\":21830},{\"end\":21847,\"start\":21843},{\"end\":21857,\"start\":21851},{\"end\":21869,\"start\":21861},{\"end\":21877,\"start\":21873},{\"end\":21889,\"start\":21881},{\"end\":22348,\"start\":22343},{\"end\":22356,\"start\":22352}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":16783578},\"end\":16104,\"start\":15468},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":62718001},\"end\":16426,\"start\":16106},{\"attributes\":{\"id\":\"b2\"},\"end\":16539,\"start\":16428},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":14349099},\"end\":16973,\"start\":16541},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":111008669},\"end\":17328,\"start\":16975},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":109966275},\"end\":17669,\"start\":17330},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":206590483},\"end\":18132,\"start\":17671},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":14327585},\"end\":18642,\"start\":18134},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":2715202},\"end\":19125,\"start\":18644},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":8195115},\"end\":19614,\"start\":19127},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":783186},\"end\":19937,\"start\":19616},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":130496},\"end\":20268,\"start\":19939},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":15909542},\"end\":20628,\"start\":20270},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":20052132},\"end\":21343,\"start\":20630},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":206769904},\"end\":21764,\"start\":21345},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":950961},\"end\":22237,\"start\":21766},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":15907009},\"end\":22628,\"start\":22239}]", "bib_title": "[{\"end\":15512,\"start\":15468},{\"end\":16192,\"start\":16106},{\"end\":16615,\"start\":16541},{\"end\":17024,\"start\":16975},{\"end\":17379,\"start\":17330},{\"end\":17723,\"start\":17671},{\"end\":18195,\"start\":18134},{\"end\":18709,\"start\":18644},{\"end\":19184,\"start\":19127},{\"end\":19686,\"start\":19616},{\"end\":19982,\"start\":19939},{\"end\":20318,\"start\":20270},{\"end\":20733,\"start\":20630},{\"end\":21405,\"start\":21345},{\"end\":21817,\"start\":21766},{\"end\":22337,\"start\":22239}]", "bib_author": "[{\"end\":15524,\"start\":15514},{\"end\":15532,\"start\":15524},{\"end\":15544,\"start\":15532},{\"end\":15552,\"start\":15544},{\"end\":16206,\"start\":16194},{\"end\":16216,\"start\":16206},{\"end\":16229,\"start\":16216},{\"end\":16240,\"start\":16229},{\"end\":16630,\"start\":16617},{\"end\":16640,\"start\":16630},{\"end\":16649,\"start\":16640},{\"end\":17036,\"start\":17026},{\"end\":17047,\"start\":17036},{\"end\":17061,\"start\":17047},{\"end\":17390,\"start\":17381},{\"end\":17400,\"start\":17390},{\"end\":17409,\"start\":17400},{\"end\":17734,\"start\":17725},{\"end\":17744,\"start\":17734},{\"end\":18213,\"start\":18197},{\"end\":18227,\"start\":18213},{\"end\":18238,\"start\":18227},{\"end\":18720,\"start\":18711},{\"end\":18729,\"start\":18720},{\"end\":19198,\"start\":19186},{\"end\":19213,\"start\":19198},{\"end\":19699,\"start\":19688},{\"end\":19718,\"start\":19699},{\"end\":19995,\"start\":19984},{\"end\":19999,\"start\":19995},{\"end\":20330,\"start\":20320},{\"end\":20339,\"start\":20330},{\"end\":20349,\"start\":20339},{\"end\":20746,\"start\":20735},{\"end\":20757,\"start\":20746},{\"end\":20770,\"start\":20757},{\"end\":20780,\"start\":20770},{\"end\":20787,\"start\":20780},{\"end\":20797,\"start\":20787},{\"end\":21422,\"start\":21407},{\"end\":21432,\"start\":21422},{\"end\":21445,\"start\":21432},{\"end\":21455,\"start\":21445},{\"end\":21828,\"start\":21819},{\"end\":21841,\"start\":21828},{\"end\":21849,\"start\":21841},{\"end\":21859,\"start\":21849},{\"end\":21871,\"start\":21859},{\"end\":21879,\"start\":21871},{\"end\":21891,\"start\":21879},{\"end\":22350,\"start\":22339},{\"end\":22358,\"start\":22350}]", "bib_venue": "[{\"end\":15830,\"start\":15695},{\"end\":16264,\"start\":16256},{\"end\":16773,\"start\":16715},{\"end\":17159,\"start\":17114},{\"end\":17507,\"start\":17462},{\"end\":17924,\"start\":17838},{\"end\":18418,\"start\":18332},{\"end\":18909,\"start\":18823},{\"end\":19393,\"start\":19307},{\"end\":20105,\"start\":20056},{\"end\":20457,\"start\":20407},{\"end\":21011,\"start\":20908},{\"end\":21563,\"start\":21513},{\"end\":22007,\"start\":21953},{\"end\":15693,\"start\":15552},{\"end\":16254,\"start\":16240},{\"end\":16468,\"start\":16428},{\"end\":16713,\"start\":16649},{\"end\":17112,\"start\":17061},{\"end\":17460,\"start\":17409},{\"end\":17836,\"start\":17744},{\"end\":18330,\"start\":18238},{\"end\":18821,\"start\":18729},{\"end\":19305,\"start\":19213},{\"end\":19749,\"start\":19718},{\"end\":20054,\"start\":19999},{\"end\":20405,\"start\":20349},{\"end\":20906,\"start\":20797},{\"end\":21511,\"start\":21455},{\"end\":21951,\"start\":21891},{\"end\":22407,\"start\":22358}]"}}}, "year": 2023, "month": 12, "day": 17}