{"id": 2156851, "updated": "2023-04-05 17:54:10.668", "metadata": {"title": "Learning Generative Visual Models from Few Training Examples: An Incremental Bayesian Approach Tested on 101 Object Categories", "authors": "[{\"first\":\"Li\",\"last\":\"Fei-Fei\",\"middle\":[]},{\"first\":\"R.\",\"last\":\"Fergus\",\"middle\":[]},{\"first\":\"P.\",\"last\":\"Perona\",\"middle\":[]}]", "venue": "2004 Conference on Computer Vision and Pattern Recognition Workshop", "journal": "2004 Conference on Computer Vision and Pattern Recognition Workshop", "publication_date": {"year": 2004, "month": null, "day": null}, "abstract": "Current computational approaches to learning visual object categories require thousands of training images, are slow, cannot learn in an incremental manner and cannot incorporate prior information into the learning process. In addition, no algorithm presented in the literature has been tested on more than a handful of object categories. We present an method for learning object categories from just a few training images. It is quick and it uses prior information in a principled way. We test it on a dataset composed of images of objects belonging to 101 widely varied categories. Our proposed method is based on making use of prior information, assembled from (unrelated) object categories which were previously learnt. A generative probabilistic model is used, which represents the shape and appearance of a constellation of features belonging to the object. The parameters of the model are learnt incrementally in a Bayesian manner. Our incremental algorithm is compared experimentally to an earlier batch Bayesian algorithm, as well as to one based on maximum-likelihood. The incremental and batch versions have comparable classification performance on small training sets, but incremental learning is significantly faster, making real-time learning feasible. Both Bayesian methods outperform maximum likelihood on small training sets.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2166049352", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/cviu/Fei-FeiFP07", "doi": "10.1109/cvpr.2004.383"}}, "content": {"source": {"pdf_hash": "f6fffe049408c7b0343a1bdcefe0dc3d0256646d", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": null, "status": "CLOSED"}}, "grobid": {"id": "9103675863f4207a41ee9f6ba401611147f52192", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f6fffe049408c7b0343a1bdcefe0dc3d0256646d.txt", "contents": "\nLearning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories\nAvailable online 9 March 2007\n\nLi Fei-Fei \nPrinceton University\n35 Olden St08540PrincetonNJUSA\n\nRob Fergus \nOxford University\nParks RoadOX1 3PJOxfordUK\n\nPietro Perona \nCalifornia Institute of Technology\n136-93 Mail Code91125PasadenaCAUSA\n\nLearning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories\nAvailable online 9 March 200710.1016/j.cviu.2005.09.012Received 15 August 2005; accepted 19 September 2005Communicated by Arthur E. C. PeceObject recognitionCategorizationGenerative modelIncremental learningBayesian model\nCurrent computational approaches to learning visual object categories require thousands of training images, are slow, cannot learn in an incremental manner and cannot incorporate prior information into the learning process. In addition, no algorithm presented in the literature has been tested on more than a handful of object categories. We present an method for learning object categories from just a few training images. It is quick and it uses prior information in a principled way. We test it on a dataset composed of images of objects belonging to 101 widely varied categories. Our proposed method is based on making use of prior information, assembled from (unrelated) object categories which were previously learnt. A generative probabilistic model is used, which represents the shape and appearance of a constellation of features belonging to the object. The parameters of the model are learnt incrementally in a Bayesian manner. Our incremental algorithm is compared experimentally to an earlier batch Bayesian algorithm, as well as to one based on maximum likelihood. The incremental and batch versions have comparable classification performance on small training sets, but incremental learning is significantly faster, making real-time learning feasible. Both Bayesian methods outperform maximum likelihood on small training sets.\n\nIntroduction\n\nOne of the most exciting and difficult open problems of machine vision is enabling a machine to recognize objects and object categories in images. Significant progress has been made on the issues of representation of objects [17,18] and object categories [2,3,[5][6][7][8] with a broad agreement for models that are composed of 'parts' (textured patches, features) and 'geometry' (or mutual position of the parts). Much work remains to be done on the issue of category learning, i.e. estimating the model parameters that are to be associated to a given category. Three difficulties face us at the moment. First, a human operator must identify explicitly each category to be learned, while it would be desirable to let a machine identify automatically each category from a broad collection of images. While Weber et al. [5] have demonstrated encouraging results on identifying automatically three categories in a limited image database, in order to reach human performance one would like to see tens of thousands of categories identified automatically from possibly millions of images [1]. Second, most algorithms require the image of each exemplar object to be geometrically normalized and aligned with a prototype (e.g. clicking on eyes, nose and mouse corners on face images). This is expensive and tedious; furthermore, it is problematic when fiducial points are not readily identifiable (can we find a natural alignment for images of octopus, of cappuccino machines, of human bodies in different poses?). Progress on this topic has been recently reported by Weber et al. [5] who proposed and demonstrated a method for training in clutter without supervision (see also the follow-up paper by Fergus et al. [8] proposing an improved scale-invariant algorithm). The third challenge has to do with the size of the training set that is required: as many as 10 4 training examples for some algorithms. This is not surprising: a well-known rule-of-thumb says that the number of training examples has to be 5-10 times the number of object parameters-hence the large training sets for models containing hundreds of parameters. Yet, humans are often able to learn new categories from a much smaller training set (how many cell-phones did we need to see in order to learn to recognize one?). On this issue Fei-Fei et al. [4] recently proposed a Bayesian framework to use priors derived from previously learned classes in order to speed up learning of a new class. In a limited set of experiments on four categories they showed that 1-3 training examples are sufficient to learn a new category. Their method is batch, in that the training examples need to be considered simultaneously.\n\nIn this paper, we explore the third issue further. First, we argue that batch learning is an undesirable limitation. An organism, or a machine, exploring the environment should not be required to store explicitly the set of training images that it has encountered so far. The current best estimate of a given class should be a sufficient memory of past experience. To this end we develop an incremental Bayesian algorithm and we study the performance/memory trade-off. Second, we collected a training set of 101 categories and we assess the new incremental Bayesian algorithm against the batch Bayesian algorithm of Fei-Fei et al. and against the maximum likelihood method of Fergus et al. We wish to emphasize at this point that previous work on object categories has been tested for the most part on 1 or 2 categories [3] with the exception of Weber et al. [5] who tested on four and Fergus et al. [8] who tested on six. We consider the effort to collect and test on dataset that is 15 times larger one of the major contributions of this paper.\n\nIn Section 2 we outline the generative framework to represent object classes. Although some of the details can be found in [4], we reproduce them here for the sake of clarity. In Section 3, we show how to apply variational inference to learn the parameters of the constellation model. In [4], a batch learning algorithm was developed for the same framework. Here we extend this into an incremental method in which the algorithm is only given a single training image at a time during the learning stage, a process much more natural for living organisms. As an overview, in Fig. 1 we schematically compare and contrast the structure of the Bayesian algorithm in learning and recognition with the Maximum likelihood algorithm used by [5,7,8]. We then discuss in details the experimental setup, with emphasis on our dataset of 101 object categories in Section 4. Finally in Section 5 we present experimental results on the 101 object categories. We conclude this paper with a discussion in Section 6.\n\n\nThe generative model\n\nTo illustrate the generative model, we start with a learnt object class model and its corresponding model distribution p(h), where h is a set of model parameters for the distribution. We are then presented with a new image and we must decide if it contains an instance of our object class or not. In this query image we have identified N interesting features with locations X, and appearances A. We now make a Bayesian decision, R. For clarity, we explicitly express training images through the detected feature locations X t and appearances A t . \nR \u00bc p\u00f0ObjectjX; A; X t ; A t \u00de p\u00f0No ObjectjX; A; X t ; A t \u00de\u00f01\u00de\nNote the ratio of p\u00f0Object\u00de p\u00f0No Object\u00de in Eq. (2) is usually set manually to 1, hence omitted in Eq. (3).\n\n\nThe constellation model\n\nOur chosen representation for object categories is based on the constellation model introduced by Burl [7] and developed further by Weber et al. [5] and Fergus et al. [8]. A constellation model consists of a number of parts, each encoding information on both the shape and appearance. The appearance of each part is modeled and the shape of the object is represented by the mutual position of the parts [8]. The entire model is generative and probabilistic, so appearance and shape are all modeled by probability density functions, which are Gaussians. The model is best explained by first considering recognition. We have learned a b Fig. 1. Schematic comparison between maximum likelihood algorithm [5,7,8] and the Bayesian learning algorithm [4]. Note the different number of training images, the different learning algorithm and recognition criterion. a generative object model, with P parts and a posterior distribution on the parameters h: p\u00f0hjX t ; A t \u00de where X t and A t are the location and appearances of interesting features found in the training data. We assume that all non-object images can also be modeled by a background with a single set of parameters h bg which are fixed. The ratio of the priors may be estimated from the training set or set manually (usually to 1). Our decision then requires the calculation of the ratio of the two likelihood functions. In order to do this, the likelihoods may be factored as follows:\np\u00f0X; Ajh\u00de \u00bc X h2H p\u00f0X; A;\nhjh\u00de\n\u00bc X h2H p\u00f0Ajh; h\u00de |fflfflfflfflffl ffl{zfflfflfflfflffl ffl} Appearance p\u00f0Xjh; h\u00de |fflfflfflfflffl{zfflfflfflfflffl} Shape\u00f04\u00de\nSince our model only has P (typically 3-7) parts but there are N (up to 100) features in the image, we introduce an indexing variable h which we call a hypothesis which allocates each image feature either to an object or to the background.\n\n\nAppearance\n\nEach feature's appearance is represented as a point in some appearance space. Each part p has a Gaussian density (denoted by G) within this space, with mean and precision parameters h A p \u00bc fl A p ; C A p g which is independent of other parts' densities.\n\n\nShape\n\nThe shape is represented by a joint Gaussian density of the locations of features within a hypothesis. For each hypothesis, the coordinates of all parts are subtracted off from the left most part coordinates. Additionally, it is scale is used to normalize the constellation. This enables our model to achieve scale and translational invariance. The density has parameters h X \u00bc fl X ; C X g.\n\n\nModel distribution\n\nLet us consider a mixture model of constellation models with X components. Each component x has a mixing coefficient p x ; a mean of shape and appearance l X x ; l A x ; a precision matrix of shape and appearance C X x ; C A x . The X and A superscripts denote shape and appearance terms respectively. Collecting all mixture components and their corresponding parameters together, we obtain an overall parameter vector h \u00bc fp; l X ; l A ; C X ; C A g. Assuming we have now learnt the model distribution p\u00f0hjX t ; A t \u00de from a set of training data X t and A t , we define the model distribution in the following way\np\u00f0hjX t ; A t \u00de \u00bc p\u00f0p\u00de Y x p\u00f0C X x \u00dep\u00f0l X x jC X x \u00dep\u00f0C A x \u00dep\u00f0l A x jC A x \u00de\u00f05\u00de\nwhere the mixing component is a symmetric Dirichlet:\np\u00f0p\u00de \u00bc Dir\u00f0k x I X \u00de, the distribution over the shape preci- sions is a Wishart p\u00f0C X x \u00de \u00bc W\u00f0C X x ja X x ; B X\nx \u00de and the distribu-tion over the shape mean conditioned on the precision matrix is Normal:\np\u00f0l X x jC X x \u00de \u00bc G\u00f0l X x jm X x ; b X x C X x \u00de.\nTogether the shape distribution p\u00f0l X x ; C X x \u00de is a Normal-Wishart density [9,13]. Note that the set of variables {k x , a x , B x , m x , b x } are hyper-parameters for defining their corresponding distributions of model parameters. Identical expressions apply to the appearance component in Eq. (5). A graphical model representation is shown in Fig. 2, where only one mixture component is used.\n\n\nBayesian decision\n\nRecall that for the query image we wish to calculate the ratio of p\u00f0ObjectjX; A; X t ; A t \u00de and p\u00f0No ObjectjX; A; X t ; A t \u00de. It is reasonable to assume a fixed value for all model parameters when the object is not present, hence the latter term may be calculated once for all. For the former term, we use Bayes's rule to obtain the likelihood expression: p\u00f0X; AjX t ; A t ; Object\u00de which expands to R p\u00f0X; Ajh\u00dep\u00f0hjX t ; A t \u00de dh. Since the likelihood p\u00f0X; Ajh\u00de contains Gaussian densities and the parameter posterior, p\u00f0hjX t ; A t \u00de is its conjugate density (a Normal-Wishart) the integral has a closed form solution of a multivariate Student's T distribution (denoted by S):\np\u00f0X; AjX t ; A t ; Object\u00de \u00bc X X x\u00bc1 X jH n j h\u00bc1p x S X h jg X x ; m X x ; K X x \u00c0 \u00c1 \u00c2 S A h jg A x ; m A x ; K A x \u00c0 \u00c1 \u00f06\u00de g x \u00bc a x \u00fe 1 \u00c0 d K x \u00bc b x \u00fe 1 b x g x B x p x \u00bc k x P x 0 k x 0\nNote d is the dimensionality of the parameter vector. If the ratio of posteriors, R in Eq. (3), calculated using the likelihood expression above exceeds a pre-defined threshold, then the image is assumed to contain an occurrence of the learnt object category. \n\n\nLearning the generative model: batch vs. incremental\n\nThe task in learning is to estimate the density p\u00f0hjX t ; A t \u00de. This is done using the Variational Bayes procedure [9][10][11]. It approximates the posterior distribution p\u00f0hjX t ; A t \u00de by q(h, x, h). x is the mixture component label and h is the hypothesis. Using Bayes' rule: q\u00f0h; x; h\u00de % p\u00f0hjX t ; A t \u00de / p\u00f0X t ; A t jh\u00dep\u00f0h\u00de. The likelihood terms use Gaussian densities and by assuming priors of a conjugate form, in this case a Normal-Wishart, our posterior q-function is also a Normal-Wishart density. The variational Bayes procedure is a variant of EM which iteratively updates the hyper-parameters and latent variables to monotonically reduces the Kullback-Liebler distance between p\u00f0hjX t ; A t \u00de and q(h,x,h). Using this approach allows us to incorporate prior information in a systematic way and is far more powerful that a maximum likelihood approach used in [8]. We first briefly give an overview of the algorithm [4], based on [9], which is a batch learning algorithm. Then we introduce the new incremental version of the algorithm.\n\n\nBatch learning\n\nThere are two stages to learning: an E-step where the responsibilities of the hidden variables are calculated and an M-step where we update the hyper-parameters of q(h,x,h), H = {k,m,b,a,B}. For each image, n we calculate the responsibilities:\nc n x;h \u00bcp xcx \u00f0X n h \u00dec x \u00f0A n h \u00de \u00f0 7\u00de\nusing the update rules given in [9]. The hyper-parameters are updated from these responsibilities. This is done by computing the sufficient statistics. While the update rules for the shape components are shown, they are of the same form for the appearance terms. The sufficient statistics, for mixture component x are calculated as follows:\np x \u00bc 1 N X N n\u00bc1 X jH n j h\u00bc1 c n x;h and N x \u00bc N p x \u00f08\u00de l X x \u00bc 1 N x X N n\u00bc1 X jH n j h\u00bc1 c n x;h X n h and\u00f09\u00deR X x \u00bc 1 N x X N n\u00bc1 X jH n j h\u00bc1 c n x;h \u00f0X n h \u00c0 l X x \u00de\u00f0X n h \u00c0 l X x \u00de T\u00f010\u00de\nNote that to compute these, we need the responsibilities from across all images. From these we can update the hyper-parameters (update rules are in [9]).\n\n\nIncremental learning\n\nWe now give an incremental version of the update rules, based on Neal and Hinton's adaptation of conventional EM [16]. Let us assume that we have a model with hyper-parameters H = {k, m, b, a, B}, estimated using M previous images (M P 0) and we have N new images (N P 1) with which we wish to update the model. From the M previous images, we have retained sufficient statistics p e\n\nx ; l e x ; R e x for each mixture component x. We then compute the responsibilities for the new images, i.e. c n x;h for n = 1,. . . , N and from them, the sufficient statistics, p x ; l x ; R x using Eqs. (8) and (10). In the Incremental Mstep we then combine the sufficient statistics from these new images with the existing set of sufficient statistics from the previous M images. Then the overall sufficient statistics, p x ;l x ;R x are computed:\np x \u00bc Mp e x \u00fe N p x M \u00fe N\u00f011\u00del x \u00bc Ml e x \u00fe N l x M \u00fe N\u00f012\u00deR x \u00bc MR e x \u00fe N R x M \u00fe N\u00f013\u00de\nFrom these we can then update the model hyper-parameters. Note the existing sufficient statistics are not updated within the update loop. When the model converges, the final value of the sufficient statistics from the new images are combined with the existing set, ready for the next update: p e\n\nx \u00bcp x ; l e x \u00bcl x ; R e x \u00bcR x . Initially M = 0, so p e x ; l e x ; R e x drop from our equations and our model hyper-parameters are set randomly (within some sensible range).\n\n\nMethods\n\n\n101 object categories\n\nWe test our Bayesian algorithms (Incremental and Batch) using 101 assorted object categories. The names of 101 categories were generated by flipping through the pages of the Webster Collegiate Dictionary [12], picking a subset of categories that were associated with a drawing. After we generated the list of category names, we used Google Image Search engine to collect as many images as possible for each category. Two graduate students not associated with the experiment then sorted through each category, mostly getting rid of irrelevant images (e.g. a zebra-patterned shirt for the ''zebra'' category). Fig. 9 shows examples of both the 101 foreground object categories as well as the background clutter category. Minimal preprocessing was performed on the categories. Categories such as motorbike, airplane, cannon, etc. where two mirror image views were present, were manually flipped, so all instances faced in the same direction. Additionally, categories with a predominantly vertical structure were rotated to an arbitrary angle, as the model parts are ordered by their x-coordinate, so have trouble with vertical structures. One could also avoid rotating the image by choosing the y-coordinate as ordering reference. This rotation is used for the sake of programming simplicity. At last, images were scaled roughly to around 300 pixels wide.\n\n\nFeature detection\n\nFeature points are found using the detector of Kadir and Brady [14]. This method finds regions that are salient over both location and scale. Gray-scale images are used as the input. The most salient regions are clustered over location and scale to give a reasonable number of features per image, each with an associated scale. The coordinates of the center of each feature give us X. Once the regions are identified, they are cropped from the image and rescaled to the size of a small (11 \u00b7 11) pixel patch. Each patch exists in a 121 dimensional space. We then reduce this dimensionality by using PCA [8]. A fixed PCA basis, pre-calculated from the background datasets, is used for this task, which gives us the first 10 principal components from each patch. The principal components from all patches and images form A. Note that the same set of parameters were used for feature detection for all 101 object categories. Figs. 4a and 5a show examples of feature detection on some training images.\n\n\nPrior and initialization\n\nOne critical issue is the choice of priors for the Normal-Wishart distributions. In this paper, learning is performed using a single mixture component. The choice of prior is itself a topic worth full investigation. In order to keep the consistency and prove the concept, we use here a single prior distribution for all 101 object categories. Since prior knowledge should reflect some information of the realworld object categories, we estimated this prior distribution using well-learned maximum likelihood (ML) models of faces, spotted cats and airplanes in [8]. It is important to point out that the idea of using prior information from unrelated categories was proposed independently by Miller et al. [15]. The ML models are simply averaged together for each parameter to obtain a model distribution for the prior. Fig. 3a and b shows the prior shape and appearance model for each category.\n\nInitial conditions are chosen in exactly the same way as [4]. Again, the same initialization is used for all 101 object categories.\n\n\nExperimental setup for each category\n\nEach experiment was carried out under identical conditions. For each category dataset, N training images are drawn randomly first. Then 50 testing images are randomly drawn from the remaining images in the dataset. For some dataset, less than 50 images are left after training images are drawn. In this case we use all the remaining ones as testing images. We then learn models using both Bayesian and ML approaches and evaluate their performance on the test set. For evaluation purposes, we also use 50 images from a background dataset of assorted junk images from the Internet. For each category, we vary N at 0, 1, 3, 6, 15, repeating the experiments 10 times for each value (using a randomly chosen N training images each time) to obtain a more robust estimate of performance. When N = 0, we use the prior model alone to perform object categorization without any training data. Only the Bayesian algorithm is used in this case. In addition, when N = 1, ML fails to converge, so we only show results for the Bayesian methods in this case.\n\nWhen evaluating the models, the decision is a simple object present/absent one. Under these conditions, an algorithm performing at random has a 50% performance rate.  3. (a and b) Prior distribution for shape mean (l X ) and appearance mean (l A ) for all the categories to be learned. Each prior's hyper-parameters are estimated from models learned with maximum likelihood methods, using ''the other'' datasets [8]. Only the first three PCA dimensions of the appearance priors are displayed. All four parts of the appearance begin with the same prior distribution for each PCA dimension.  image = 20. All parameters remain the same for learning different categories of objects. Fig. 4 illustrates the recognition performance at Training Number = 1, 3, 6, 15 for the different algorithms: Bayesian incremental, Bayesian batch and maximum likelihood. The performance of each method is compared with the performance of the prior-model on the given category. Despite the rudimentary prior, a strong performance gain is observed for the majority of the 101 categories, even when only a few training examples are used. With one training example, the Bayesian method achieves an average performance of 71%, with the best results being over 80%. At Training Number = 3, maximum likelihood has a performance at chance while both Bayesian methods achieve a performance near 75%. At Training Number = 15 that the maximum likelihood catches up the recognition performance with the Bayesian methods. At Training Number = 15, the average Bayesian incremental method is not as reliable as the Bayesian batch method. In general the incremental method is much more sensitive to the quality of the training images and to the order in with they are presented to the algorithm. Therefore it is more likely to form suboptimal models.\n\n\nExperimental results\n\nWhile performance is a key measurement for recognition algorithms, efficiency in training and learning are also important. One big advantage that we have gained from the Bayesian Incremental method is its fast speed in training. Fig. 5 shows the comparison of average learning time across all 101 categories between these three methods. All methods show approximately linear increase in learning time as the number of training images increases. The incremental method, particularly, shows a very small slope. In our Matlab implementation, it takes approximately 6 s per image to train in Bayesian incremental method, while the batch and maximum likelihood methods take 6 times as long.\n\nIn Figs. 6 and 7 we show in details the results from the grand-piano and cougar-face categories, both of which performed well (ROC Areas 16% and 15%, respectively, for Training Number = 15). As the number of training examples increases, we observe that the shape model is more defined and structured with reducing variance. This is expected since the algorithm should be more and more confident of what is to be learned. Figs. 6c and 7c shows examples of the part appearance that are closest to the mean distribution of the appearance. Notice that critical features such as keyboards for the piano and eyes or whiskers for the cougar-face are successfully learned by the algorithm. Three learning methods' performances are compared in Figs. 6d and 7d. The Bayesian methods clearly show a big advantage over the ML method when training number is small. Bayesian Incremental, however, shows more greater performance fluctuations as compared to the Bayesian Batch method. Finally, we show some classified test images, using an incremental model trained from a single image.\n\nIt is also useful to look at the other end of the performance spectrum-those categories that have low recognition performance. We give some insights into the cause of the poor performance.\n\nFeature detection is a crucial step for both learning and recognition. On both the crocodile and mayfly figures in Fig. 8, notice that some testing images marked ''INCOR-RECT'' have few detection points on the target object itself. When feature detection fails either in learning or recognition, it affects the performance results greatly. Furthermore, Fig. 8a shows that a variety of viewpoints is present in each category. In this set of experiments we have only used one mixture component, hence only a single viewpoint can be accommodated. Our model is also a simplified version Burl, Weber and Fergus' constellation model [5,7,8] as it ignores the possibility of occluded parts.\n\nA great source of improvement could potentially come from prior model information. The prior model is currently rather weak and improvements in this area would undoubtedly improve the models performance. However, the performance could be degraded if the model was to incorporate misleading information-as illustrated in Fig. 8b. Our choice of prior for this paper is kept as simple as possible to facilitate the experiments. We expect further exploration into this topic can help improving recognition performances greatly.\n\n\nSummary and discussion\n\nWe presented a Bayesian incremental algorithm for learning generative models of object categories from a very limited training set. Our work is an extension of Fei-Fei et al.'s Bayesian batch method [4]. We have tested both methods using a prior derived from three unrelated categories, alongside a simplified version of Fergus et al.'s maximum likelihood method, on a large dataset of 101 object categories.   contrast the Bayesian methods achieve better than chance performance on 90 higher than 80. Second, the desirable incremental learning feature leads to much faster learning speed (Fig. 3), but is paid with worse recognition performance for larger training set sizes. We conjecture that this is due to the fact that less information is carried along by the incremental algorithm from one learning epoch to the next, while the batch algorithm has all training images available at the same time thus allowing, for example, to test a larger number of hypotheses on how foreground and clutter features should be assigned in each training image.\n\nThird, the maximum likelihood method matches the performance of the Bayesian methods when the training set reaches size 15. This is surprising, given that the number of parameters in each model is 50, and therefore a few examples Shape Fig. 9. The 101 object categories and the background clutter category. Each category contains between 45 and 400 images. Two randomly chosen samples are shown for each category. The categories were selected prior to the experimentation, collected by operators not associated with the experiment, and no category was excluded from the experiments. The last row shows examples from the background dataset. This dataset is obtained by collecting images through the Google image search engine (www.google.com). The keyword ''things'' is used to obtain hundreds of random images. Note only gray-scale information is used in our system. Complete datasets can be found at http://vision.caltech.edu.\n\nhundred training examples are in principle required by a maximum likelihood method-one might have expected the Bayesian methods to be bettered by ML only around 100 training examples. The most likely reason for this result is that the prior that we employ is too simple. Bayesian methods live and die by the quality of the prior that is used. Our prior density is derived from only three object categories. Given the variability of our training set, it is realistic that we would need many more categories to train a reasonable prior. Fourth, the good news is that the problem of recognizing automatically hundreds, perhaps thousands, of object categories does not belong to a hopelessly far future. We hope that the success of our method on the large majority of 101 very diverse and challenging categories, despite the simplicity of our implementation and the rudimentary prior we employ, will encourage other vision researchers to test their algorithms on larger and more diverse datasets.\n\nFig. 2 .\n2A graphical model representation of the constellation model. Note that we assume only one mixture component in this representation.\n\n\nAll performance values are quoted as area under the receiver-operating characteristic (ROC). ROC curve is obtained by testing the model on 50 foreground test images and 50 background images. In all the experiments, the following parameters are used: number of parts in model = 4; number of PCA dimensions for each part appearance = 10; and average number of detections of interest point for each\n\nFig. 4 .\n4(a-d) Performance comparison between ML, Bayesian batch and Bayesian incremental methods for all 101 object categories at Training Number =(1,3,6,15). In each plot, a category has three markers: red-circle represents Bayesian incremental method, green-plus Bayesian batch method and blue-diamond maximum likelihood method. For each panel, the x-axis indicates Bayesian method categorization performance with only the prior model. Note with one training image, the Bayesian incremental method and Batch method are exactly the same. The y-axis indicates categorization performance for each of the three methods. There is no maximum likelihood performance in the degenerate case of Training Number = 1. Moreover, Bayesian Incremental method and Batch method are also the same for Training Number = 1. (e) Category name list is sorted according to the Bayesian Incremental method performance at Training Number = 15.\n\nFig. 6 .\n6Results for the ''grand-piano'' category. (a) Examples of feature detection. (b) The shape models learned at Training Number = (1, 3, 6, 15). Similarly to Fig. 3a, the x-axis represents the x position, measured by pixels, and the y-axis represents the y position, measured by pixels. (c) The appearance patches for the model learned at Training Number = (1, 3, 6, 15). (d) The comparative results between ML, Bayesian batch and Bayesian incremental methods (the error bars show the variation over the 10 runs). (e) Recognition result for the incremental method at Training Number = 1. Pink dots indicate the center of detected interest points.\n\nFig. 7 .\n7Results for the ''cougar face'' category.\n\n\u00bc p\u00f0X ;\np\u00f0XAjX t ; A t ; Object\u00dep\u00f0Object\u00de p\u00f0X; AjX t ; A t ; Noobject\u00dep\u00f0No Object\u00de \u00f02\u00de % R p\u00f0X; Ajh; Object\u00dep\u00f0hjX t ; A t ; Object\u00de dh R p\u00f0X; Ajh bg ; No Object\u00dep\u00f0h bg jX t ; A t ; No Object\u00de dh bg\n\n\nPerformance Comparison: Train Number = 145 \n\n55 \n65 \n75 \n85 \n95 \n45 \n\n55 \n\n65 \n\n75 \n\n85 \n\n95 \n\nPrior Performance (percent correct) \nPerformance (percent correct) \n\nBayesian Batch \nBayesian Incremental \n\nMaximum Likelihood \nBayesian Batch \nBayesian Incremental \n\n45 \n55 \n65 \n75 \n85 \n95 \n45 \n\n55 \n\n65 \n\n75 \n\n85 \n\n95 \n\nPrior Performance (percent correct) \nPerformance (percent correct) \n\nPerformance Comparison: Train Number = 6 \n\nMaximum Likelihood \nBayesian Batch \nBayesian Incremental \n\n45 \n55 \n65 \n75 \n85 \n95 \n45 \n\n55 \n\n65 \n\n75 \n\n85 \n\n95 \n\nPrior Performance (percent correct) \nPerformance (percent correct) \n\nPerformance Comparison: Train Number = 3 \n\nMaximum Likelihood \nBayesian Batch \nBayesian Incremental \n\n45 \n55 \n65 \n75 \n85 \n95 \n45 \n\n55 \n\n65 \n\n75 \n\n85 \n\n95 \n\nPrior Performance (percent correct) \nPerformance (percent correct) \n\nPerformance Comparison: Train Number = 15 \n\n1. trilobite \n2. face \n3. pagoda \n4. tick \n5. inlineskate \n6. metronome \n7. accordion \n8. yinyang \n9. soccerball \n10. spotted cat \n11. nautilus \n12. grand-piano \n13. crayfish \n14. headphone \n15. hawksbill \n16. ferry \n17. cougar-face \n\n18. bass \n19. ketch \n20. lobster \n21. pyramid \n22. rooster \n23. laptop \n24. waterlilly \n25. wrench \n26. strawberry \n27. starfish \n28. ceilingfan \n29. seahorse \n30. stapler \n31. stop-sign \n32. zebra \n33. brontosaurus \n34. emu \n\n35. snoopy \n36. okapi \n37. schooner \n38. binocular \n39. motorbike \n40. hedgehog \n41. garfield \n42. airplane \n43. umbrella \n44. panda \n45. crocodile-head \n46. llama \n47. windsor-chair \n48. car-side \n49. pizza \n50. minaret \n51. dollarbill \n\n52. gerenuk \n53. sunflower \n54. rhino \n55. cougar-body \n56. crab \n57. ibis \n58. helicopter \n59. dalmatian \n60. scorpion \n61. revolver \n62. beaver \n63. saxophone \n64. kangaroo \n65. euphonium \n66. flamingo \n67. flamingo-head \n68. elephant \n\n69. cellphone \n70. gramophone \n71. bonsai \n72. lotus \n73. cannon \n74. wheel-chair \n75. dolphin \n76. stegosaurus \n77. brain \n78. menorah \n79. chandelier \n80. camera \n81. ant \n82. scissors \n83. butterfly \n84. wildcat \n85. crocodile \n\n86. barrel \n87. joshua-tree \n88. pigeon \n89. watch \n90. dragonfly \n91. mayfly \n92. cup \n93. ewer \n94. octopus \n95. platypus \n96. buddha \n97. chair \n98. anchor \n99. mandolin \n100. electric-guitar \n101. lamp \n\n\n\n\nTraining Number Learning Time (seconds)Learning speed comparison among different learning methodsFig. 5. Average learning time for ML, Bayesian batch and Bayesian incremental methods over all 101 categories.First of all, it is possible to train complex models of object categories with a handful of images. It is clear that Bayesian methods allow category learning from small training sets. On one, three and six training examples the maximum likelihood method is unable to discriminate any category from images containing random clutter. ByMaximum Likelihood \nBayesian Batch \nBayesian Incremental \n\nExamples \nShape \nAppearance \n\n100 \n50 \n0 \n50 \n100 \n\n50 \n\n0 \n\n50 \n\nShape Model: Train = 1 \n\nPart 1 \n\nPart 2 \n\nPart 3 \n\nPart 4 \n\n100 \n50 \n0 \n50 \n100 \n\n50 \n\n0 \n\n50 \n\nShape Model: Train = 3 \n\nPart 1 \n\nPart 2 \n\nPart 3 \n\nPart 4 \n\n100 \n50 \n0 \n50 \n100 \n\n50 \n\n0 \n\n50 \n\nShape Model: Train = 6 \n\nPart 1 \n\nPart 2 \n\nPart 3 \n\nPart 4 \n\n100 \n50 \n0 \n50 \n100 \n\n50 \n\n0 \n\n50 \n\nShape Model: Train = 15 \n\nPart 1 \n\nPart 2 \n\nPart 3 \n\nPart 4 \n\n0 \n2 \n4 \n6 \n8 \n10 12 14 \n0 \n\n10 \n\n20 \n\n30 \n\n40 \n\n50 \n\nTraining Number \n\nPerformance Error \n\nM.L. \nBayesian Batch \n\nPerformance Comparison \n\nBayesian Incr \n\nINCORRECT Correct \nCorrect \nCorrect \n\nCorrect \nCorrect \nCorrect \nINCORRECT \n\nCorrect \nCorrect \nCorrect \nCorrect \n\na \nb \nc \n\nd \ne \n\n\nAcknowledgmentThe authors thank Marc'Aurelio Ranzato and Marco Andreetto for their help in image collection.\nRecognition-by-components: a theory of human image understanding. I Biederman, Psychol. Rev. 94I. Biederman, Recognition-by-components: a theory of human image understanding, Psychol. Rev. 94 (1987) 115-147.\n\nA computational model for visual selection. Y Amit, D Geman, Neural Comput. 117Y. Amit, D. Geman, A computational model for visual selection, Neural Comput. 11 (7) (1999) 1691-1715.\n\nA statistical approach to 3D object detection applied to faces and cars. H Schneiderman, T Kanade, Proc. Conf. on Computer Vision Pattern and Recognition. Conf. on Computer Vision Pattern and RecognitionH. Schneiderman, T. Kanade, A statistical approach to 3D object detection applied to faces and cars, in: Proc. Conf. on Computer Vision Pattern and Recognition, 2000, pp. 746-751.\n\nA Bayesian approach to unsupervised learning of object categories. L Fei-Fei, R Fergus, P Perona, Proc. Int. Conf. on Computer Vision. Int. Conf. on Computer VisionL. Fei-Fei, R. Fergus, P. Perona, A Bayesian approach to unsuper- vised learning of object categories, in: Proc. Int. Conf. on Computer Vision, 2003, pp. 1134-1141.\n\nUnsupervised learning of models for recognition. M Weber, M Welling, P Perona, Proc. 6th Europ. Conf. on Computer Vision. 6th Europ. Conf. on Computer Vision2M. Weber, M. Welling, P. Perona, Unsupervised learning of models for recognition, in: Proc. 6th Europ. Conf. on Computer Vision, vol. 2, 2000, pp. 101-108.\n\nRapid object detection using a boosted cascade of simple features. P Viola, M Jones, Proc. Conf. on Computer Vision Pattern and Recognition. Conf. on Computer Vision Pattern and Recognition1P. Viola, M. Jones, Rapid object detection using a boosted cascade of simple features, in: Proc. Conf. on Computer Vision Pattern and Recognition, vol. 1, 2001, pp. 511-518.\n\nA probabilistic approach to object recognition using local photometry and global geometry. M C Burl, M Weber, P Perona, Proc. Europ. Conf. on Computer Vision. Europ. Conf. on Computer VisionM.C. Burl, M. Weber, P. Perona, A probabilistic approach to object recognition using local photometry and global geometry, in: Proc. Europ. Conf. on Computer Vision, pp. 628-641.\n\nObject class recognition by unsupervised scale-invariant learning. R Fergus, P Perona, A Zisserman, Proc. Conf. on Computer Vision Pattern and Recognition. Conf. on Computer Vision Pattern and Recognition2R. Fergus, P. Perona, A. Zisserman, Object class recognition by unsupervised scale-invariant learning, in: Proc. Conf. on Computer Vision Pattern and Recognition, vol. 2, 2003, pp. 264-271.\n\nInferring parameters and structure of latent variable models by variational bayes. H Attias, 15th Conf. on Uncertainty in Artificial Intelligence. H. Attias, Inferring parameters and structure of latent variable models by variational bayes, in: 15th Conf. on Uncertainty in Artificial Intelligence, 1999, pp. 21-30.\n\nAn introduction to variational methods for graphical models. M I Jordan, Z Ghahramani, T S Jaakkola, L K Saul, Mach. Learn. 37M.I. Jordan, Z. Ghahramani, T.S. Jaakkola, L.K. Saul, An introduction to variational methods for graphical models, Mach. Learn. 37 (1999) 183-233.\n\n. S R Waterhouse, D J.C Mackay, A J Robinson, Bayesian Methods for Mixtures of Experts. NIPSS.R. Waterhouse, D.J.C. MacKay, A.J. Robinson, Bayesian Methods for Mixtures of Experts, NIPS, 1996.\n\n. Merriam-Webster, USAs collegiate dictionary. 10th ed. Springfield, MassachusettsMerriam-Webster's collegiate dictionary, 10th ed. Springfield, Mas- sachusetts, USA, 1994.\n\nVariational Bayes for d-dimensional Gaussian mixture models. W D Penny, University College LondonTech. Rep.W.D. Penny, Variational Bayes for d-dimensional Gaussian mixture models, Tech. Rep., University College London, 2001.\n\nScale, saliency and image description. T Kadir, M Brady, Int. J. Comp. Vis. 452T. Kadir, M. Brady, Scale, saliency and image description, Int. J. Comp. Vis. 45 (2) (2001) 83-105.\n\nLearning from one example through shared densities on transforms. E Miller, N Matsakis, P Viola, Proc. Conf. on Computer Vision Pattern and Recognition. Conf. on Computer Vision Pattern and RecognitionE. Miller, N. Matsakis, P. Viola, Learning from one example through shared densities on transforms, in: Proc. Conf. on Computer Vision Pattern and Recognition, 2000, pp. 464-471.\n\nA view of the EM algorithm that justifies incremental, sparse and other variants. R M Neal, G E Hinton, Learning in Graphical Models. M.I. JordanNorwellKluwer academic pressR.M. Neal, G.E. Hinton, A view of the EM algorithm that justifies incremental, sparse and other variants, in: M.I. Jordan (Ed.), Learning in Graphical Models, Kluwer academic press, Norwell, 1998, pp. 355-368.\n\nObject recognition from local scale-invariant features. D Lowe, Proc. Int. Conf. on Computer Vision. Int. Conf. on Computer VisionD. Lowe, Object recognition from local scale-invariant features, in: Proc. Int. Conf. on Computer Vision, 1999, pp. 1150-1157.\n\nLocal Greyvalue Invariants for Image Retrieval. C Schmid, R Mohr, IEEE Trans. Pattern Anal. Machine Intell. 195C. Schmid, R. Mohr, Local Greyvalue Invariants for Image Retrieval, IEEE Trans. Pattern Anal. Machine Intell. 19 (5) (1997) 530-534.\n", "annotations": {"author": "[{\"end\":223,\"start\":159},{\"end\":280,\"start\":224},{\"end\":366,\"start\":281}]", "publisher": null, "author_last_name": "[{\"end\":169,\"start\":162},{\"end\":234,\"start\":228},{\"end\":294,\"start\":288}]", "author_first_name": "[{\"end\":161,\"start\":159},{\"end\":227,\"start\":224},{\"end\":287,\"start\":281}]", "author_affiliation": "[{\"end\":222,\"start\":171},{\"end\":279,\"start\":236},{\"end\":365,\"start\":296}]", "title": "[{\"end\":127,\"start\":1},{\"end\":493,\"start\":367}]", "venue": null, "abstract": "[{\"end\":2058,\"start\":716}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2303,\"start\":2299},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2306,\"start\":2303},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2332,\"start\":2329},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2334,\"start\":2332},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2337,\"start\":2334},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2340,\"start\":2337},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2343,\"start\":2340},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2346,\"start\":2343},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2896,\"start\":2893},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3161,\"start\":3158},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3652,\"start\":3649},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3786,\"start\":3783},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3934,\"start\":3933},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4391,\"start\":4388},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5576,\"start\":5573},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5615,\"start\":5612},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5656,\"start\":5653},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5927,\"start\":5924},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6092,\"start\":6089},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6535,\"start\":6532},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6537,\"start\":6535},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6539,\"start\":6537},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7676,\"start\":7673},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7718,\"start\":7715},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7740,\"start\":7737},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7976,\"start\":7973},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8274,\"start\":8271},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8276,\"start\":8274},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8278,\"start\":8276},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8318,\"start\":8315},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11187,\"start\":11184},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11190,\"start\":11187},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11409,\"start\":11406},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12834,\"start\":12831},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12838,\"start\":12834},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":12842,\"start\":12838},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":13591,\"start\":13588},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13647,\"start\":13644},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":13661,\"start\":13658},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14102,\"start\":14099},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14755,\"start\":14752},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14899,\"start\":14895},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15376,\"start\":15373},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":15385,\"start\":15381},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16429,\"start\":16425},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17662,\"start\":17658},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18201,\"start\":18198},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19184,\"start\":19181},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19330,\"start\":19326},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":19577,\"start\":19574},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21147,\"start\":21144},{\"end\":23446,\"start\":23441},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25149,\"start\":25146},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25151,\"start\":25149},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":25153,\"start\":25151},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25956,\"start\":25953},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":29421,\"start\":29418},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":29423,\"start\":29421},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":29425,\"start\":29423},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":29428,\"start\":29425}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28869,\"start\":28727},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29267,\"start\":28870},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30191,\"start\":29268},{\"attributes\":{\"id\":\"fig_4\"},\"end\":30846,\"start\":30192},{\"attributes\":{\"id\":\"fig_5\"},\"end\":30899,\"start\":30847},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":31098,\"start\":30900},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":33360,\"start\":31099},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":34669,\"start\":33361}]", "paragraph": "[{\"end\":4751,\"start\":2074},{\"end\":5799,\"start\":4753},{\"end\":6797,\"start\":5801},{\"end\":7370,\"start\":6822},{\"end\":7542,\"start\":7435},{\"end\":9010,\"start\":7570},{\"end\":9041,\"start\":9037},{\"end\":9407,\"start\":9168},{\"end\":9676,\"start\":9422},{\"end\":10077,\"start\":9686},{\"end\":10714,\"start\":10100},{\"end\":10848,\"start\":10796},{\"end\":11054,\"start\":10962},{\"end\":11505,\"start\":11106},{\"end\":12206,\"start\":11527},{\"end\":12658,\"start\":12398},{\"end\":13763,\"start\":12715},{\"end\":14025,\"start\":13782},{\"end\":14407,\"start\":14067},{\"end\":14757,\"start\":14604},{\"end\":15164,\"start\":14782},{\"end\":15618,\"start\":15166},{\"end\":16005,\"start\":15710},{\"end\":16185,\"start\":16007},{\"end\":17573,\"start\":16221},{\"end\":18592,\"start\":17595},{\"end\":19515,\"start\":18621},{\"end\":19648,\"start\":19517},{\"end\":20730,\"start\":19689},{\"end\":22545,\"start\":20732},{\"end\":23255,\"start\":22570},{\"end\":24327,\"start\":23257},{\"end\":24517,\"start\":24329},{\"end\":25202,\"start\":24519},{\"end\":25727,\"start\":25204},{\"end\":26803,\"start\":25754},{\"end\":27732,\"start\":26805},{\"end\":28726,\"start\":27734}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7434,\"start\":7371},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9036,\"start\":9011},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9167,\"start\":9042},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10795,\"start\":10715},{\"attributes\":{\"id\":\"formula_5\"},\"end\":10961,\"start\":10849},{\"attributes\":{\"id\":\"formula_6\"},\"end\":11105,\"start\":11055},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12397,\"start\":12207},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14066,\"start\":14026},{\"attributes\":{\"id\":\"formula_9\"},\"end\":14522,\"start\":14408},{\"attributes\":{\"id\":\"formula_10\"},\"end\":14603,\"start\":14522},{\"attributes\":{\"id\":\"formula_11\"},\"end\":15649,\"start\":15619},{\"attributes\":{\"id\":\"formula_12\"},\"end\":15679,\"start\":15649},{\"attributes\":{\"id\":\"formula_13\"},\"end\":15709,\"start\":15679}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2072,\"start\":2060},{\"attributes\":{\"n\":\"2.\"},\"end\":6820,\"start\":6800},{\"attributes\":{\"n\":\"2.1.\"},\"end\":7568,\"start\":7545},{\"attributes\":{\"n\":\"2.1.1.\"},\"end\":9420,\"start\":9410},{\"attributes\":{\"n\":\"2.1.2.\"},\"end\":9684,\"start\":9679},{\"attributes\":{\"n\":\"2.2.\"},\"end\":10098,\"start\":10080},{\"attributes\":{\"n\":\"2.3.\"},\"end\":11525,\"start\":11508},{\"attributes\":{\"n\":\"3.\"},\"end\":12713,\"start\":12661},{\"attributes\":{\"n\":\"3.1.\"},\"end\":13780,\"start\":13766},{\"attributes\":{\"n\":\"3.2.\"},\"end\":14780,\"start\":14760},{\"attributes\":{\"n\":\"4.\"},\"end\":16195,\"start\":16188},{\"attributes\":{\"n\":\"4.1.\"},\"end\":16219,\"start\":16198},{\"attributes\":{\"n\":\"4.2.\"},\"end\":17593,\"start\":17576},{\"attributes\":{\"n\":\"4.3.\"},\"end\":18619,\"start\":18595},{\"attributes\":{\"n\":\"4.4.\"},\"end\":19687,\"start\":19651},{\"attributes\":{\"n\":\"5.\"},\"end\":22568,\"start\":22548},{\"attributes\":{\"n\":\"6.\"},\"end\":25752,\"start\":25730},{\"end\":28736,\"start\":28728},{\"end\":29277,\"start\":29269},{\"end\":30201,\"start\":30193},{\"end\":30856,\"start\":30848},{\"end\":30908,\"start\":30901}]", "table": "[{\"end\":33360,\"start\":31141},{\"end\":34669,\"start\":33904}]", "figure_caption": "[{\"end\":28869,\"start\":28738},{\"end\":29267,\"start\":28872},{\"end\":30191,\"start\":29279},{\"end\":30846,\"start\":30203},{\"end\":30899,\"start\":30858},{\"end\":31098,\"start\":30912},{\"end\":31141,\"start\":31101},{\"end\":33904,\"start\":33363}]", "figure_ref": "[{\"end\":6379,\"start\":6373},{\"end\":8211,\"start\":8205},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11462,\"start\":11456},{\"end\":16835,\"start\":16829},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18532,\"start\":18517},{\"end\":19447,\"start\":19440},{\"end\":20911,\"start\":20899},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21417,\"start\":21411},{\"end\":22805,\"start\":22799},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":23693,\"start\":23678},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24007,\"start\":23992},{\"end\":24640,\"start\":24634},{\"end\":24879,\"start\":24872},{\"end\":25531,\"start\":25524},{\"end\":26351,\"start\":26343},{\"end\":27047,\"start\":27041}]", "bib_author_first_name": "[{\"end\":34846,\"start\":34845},{\"end\":35033,\"start\":35032},{\"end\":35041,\"start\":35040},{\"end\":35245,\"start\":35244},{\"end\":35261,\"start\":35260},{\"end\":35623,\"start\":35622},{\"end\":35634,\"start\":35633},{\"end\":35644,\"start\":35643},{\"end\":35935,\"start\":35934},{\"end\":35944,\"start\":35943},{\"end\":35955,\"start\":35954},{\"end\":36268,\"start\":36267},{\"end\":36277,\"start\":36276},{\"end\":36657,\"start\":36656},{\"end\":36659,\"start\":36658},{\"end\":36667,\"start\":36666},{\"end\":36676,\"start\":36675},{\"end\":37003,\"start\":37002},{\"end\":37013,\"start\":37012},{\"end\":37023,\"start\":37022},{\"end\":37415,\"start\":37414},{\"end\":37710,\"start\":37709},{\"end\":37712,\"start\":37711},{\"end\":37722,\"start\":37721},{\"end\":37736,\"start\":37735},{\"end\":37738,\"start\":37737},{\"end\":37750,\"start\":37749},{\"end\":37752,\"start\":37751},{\"end\":37925,\"start\":37924},{\"end\":37927,\"start\":37926},{\"end\":37941,\"start\":37940},{\"end\":37945,\"start\":37942},{\"end\":37955,\"start\":37954},{\"end\":37957,\"start\":37956},{\"end\":38352,\"start\":38351},{\"end\":38354,\"start\":38353},{\"end\":38556,\"start\":38555},{\"end\":38565,\"start\":38564},{\"end\":38763,\"start\":38762},{\"end\":38773,\"start\":38772},{\"end\":38785,\"start\":38784},{\"end\":39160,\"start\":39159},{\"end\":39162,\"start\":39161},{\"end\":39170,\"start\":39169},{\"end\":39172,\"start\":39171},{\"end\":39518,\"start\":39517},{\"end\":39768,\"start\":39767},{\"end\":39778,\"start\":39777}]", "bib_author_last_name": "[{\"end\":34856,\"start\":34847},{\"end\":35038,\"start\":35034},{\"end\":35047,\"start\":35042},{\"end\":35258,\"start\":35246},{\"end\":35268,\"start\":35262},{\"end\":35631,\"start\":35624},{\"end\":35641,\"start\":35635},{\"end\":35651,\"start\":35645},{\"end\":35941,\"start\":35936},{\"end\":35952,\"start\":35945},{\"end\":35962,\"start\":35956},{\"end\":36274,\"start\":36269},{\"end\":36283,\"start\":36278},{\"end\":36664,\"start\":36660},{\"end\":36673,\"start\":36668},{\"end\":36683,\"start\":36677},{\"end\":37010,\"start\":37004},{\"end\":37020,\"start\":37014},{\"end\":37033,\"start\":37024},{\"end\":37422,\"start\":37416},{\"end\":37719,\"start\":37713},{\"end\":37733,\"start\":37723},{\"end\":37747,\"start\":37739},{\"end\":37757,\"start\":37753},{\"end\":37938,\"start\":37928},{\"end\":37952,\"start\":37946},{\"end\":37966,\"start\":37958},{\"end\":38133,\"start\":38118},{\"end\":38360,\"start\":38355},{\"end\":38562,\"start\":38557},{\"end\":38571,\"start\":38566},{\"end\":38770,\"start\":38764},{\"end\":38782,\"start\":38774},{\"end\":38791,\"start\":38786},{\"end\":39167,\"start\":39163},{\"end\":39179,\"start\":39173},{\"end\":39523,\"start\":39519},{\"end\":39775,\"start\":39769},{\"end\":39783,\"start\":39779}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":8054340},\"end\":34986,\"start\":34779},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":7784637},\"end\":35169,\"start\":34988},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":15424450},\"end\":35553,\"start\":35171},{\"attributes\":{\"id\":\"b3\"},\"end\":35883,\"start\":35555},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":8970876},\"end\":36198,\"start\":35885},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2715202},\"end\":36563,\"start\":36200},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":11023003},\"end\":36933,\"start\":36565},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":5745749},\"end\":37329,\"start\":36935},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":13371224},\"end\":37646,\"start\":37331},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":2073260},\"end\":37920,\"start\":37648},{\"attributes\":{\"id\":\"b10\"},\"end\":38114,\"start\":37922},{\"attributes\":{\"id\":\"b11\"},\"end\":38288,\"start\":38116},{\"attributes\":{\"id\":\"b12\"},\"end\":38514,\"start\":38290},{\"attributes\":{\"id\":\"b13\"},\"end\":38694,\"start\":38516},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2699786},\"end\":39075,\"start\":38696},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":17947141},\"end\":39459,\"start\":39077},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":5258236},\"end\":39717,\"start\":39461},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":325871},\"end\":39962,\"start\":39719}]", "bib_title": "[{\"end\":34843,\"start\":34779},{\"end\":35030,\"start\":34988},{\"end\":35242,\"start\":35171},{\"end\":35620,\"start\":35555},{\"end\":35932,\"start\":35885},{\"end\":36265,\"start\":36200},{\"end\":36654,\"start\":36565},{\"end\":37000,\"start\":36935},{\"end\":37412,\"start\":37331},{\"end\":37707,\"start\":37648},{\"end\":38553,\"start\":38516},{\"end\":38760,\"start\":38696},{\"end\":39157,\"start\":39077},{\"end\":39515,\"start\":39461},{\"end\":39765,\"start\":39719}]", "bib_author": "[{\"end\":34858,\"start\":34845},{\"end\":35040,\"start\":35032},{\"end\":35049,\"start\":35040},{\"end\":35260,\"start\":35244},{\"end\":35270,\"start\":35260},{\"end\":35633,\"start\":35622},{\"end\":35643,\"start\":35633},{\"end\":35653,\"start\":35643},{\"end\":35943,\"start\":35934},{\"end\":35954,\"start\":35943},{\"end\":35964,\"start\":35954},{\"end\":36276,\"start\":36267},{\"end\":36285,\"start\":36276},{\"end\":36666,\"start\":36656},{\"end\":36675,\"start\":36666},{\"end\":36685,\"start\":36675},{\"end\":37012,\"start\":37002},{\"end\":37022,\"start\":37012},{\"end\":37035,\"start\":37022},{\"end\":37424,\"start\":37414},{\"end\":37721,\"start\":37709},{\"end\":37735,\"start\":37721},{\"end\":37749,\"start\":37735},{\"end\":37759,\"start\":37749},{\"end\":37940,\"start\":37924},{\"end\":37954,\"start\":37940},{\"end\":37968,\"start\":37954},{\"end\":38135,\"start\":38118},{\"end\":38362,\"start\":38351},{\"end\":38564,\"start\":38555},{\"end\":38573,\"start\":38564},{\"end\":38772,\"start\":38762},{\"end\":38784,\"start\":38772},{\"end\":38793,\"start\":38784},{\"end\":39169,\"start\":39159},{\"end\":39181,\"start\":39169},{\"end\":39525,\"start\":39517},{\"end\":39777,\"start\":39767},{\"end\":39785,\"start\":39777}]", "bib_venue": "[{\"end\":34870,\"start\":34858},{\"end\":35062,\"start\":35049},{\"end\":35324,\"start\":35270},{\"end\":35688,\"start\":35653},{\"end\":36005,\"start\":35964},{\"end\":36339,\"start\":36285},{\"end\":36722,\"start\":36685},{\"end\":37089,\"start\":37035},{\"end\":37476,\"start\":37424},{\"end\":37770,\"start\":37759},{\"end\":38008,\"start\":37968},{\"end\":38349,\"start\":38290},{\"end\":38590,\"start\":38573},{\"end\":38847,\"start\":38793},{\"end\":39209,\"start\":39181},{\"end\":39560,\"start\":39525},{\"end\":39825,\"start\":39785},{\"end\":35374,\"start\":35326},{\"end\":35719,\"start\":35690},{\"end\":36042,\"start\":36007},{\"end\":36389,\"start\":36341},{\"end\":36755,\"start\":36724},{\"end\":37139,\"start\":37091},{\"end\":38897,\"start\":38849},{\"end\":39229,\"start\":39222},{\"end\":39591,\"start\":39562}]"}}}, "year": 2023, "month": 12, "day": 17}