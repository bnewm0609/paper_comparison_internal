{"id": 210919925, "updated": "2023-10-06 19:29:39.878", "metadata": {"title": "Learning Canonical Shape Space for Category-Level 6D Object Pose and Size Estimation", "authors": "[{\"first\":\"Dengsheng\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Jun\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Zheng\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Kai\",\"last\":\"Xu\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "We present a novel approach to category-level 6D object pose and size estimation. To tackle intra-class shape variations, we learn canonical shape space (CASS), a unified representation for a large variety of instances of a certain object category. In particular, CASS is modeled as the latent space of a deep generative model of canonical 3D shapes with normalized pose. We train a variational auto-encoder (VAE) for generating 3D point clouds in the canonical space from an RGBD image. The VAE is trained in a cross-category fashion, exploiting the publicly available large 3D shape repositories. Since the 3D point cloud is generated in normalized pose (with actual size), the encoder of the VAE learns view-factorized RGBD embedding. It maps an RGBD image in arbitrary view into a pose-independent 3D shape representation. Object pose is then estimated via contrasting it with a pose-dependent feature of the input RGBD extracted with a separate deep neural networks. We integrate the learning of CASS and pose and size estimation into an end-to-end trainable network, achieving the state-of-the-art performance.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2001.09322", "mag": "3034597466", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/ChenLWX20", "doi": "10.1109/cvpr42600.2020.01199"}}, "content": {"source": {"pdf_hash": "5887c12275bd463145adf9dafe149da81d046182", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2001.09322v3.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2001.09322", "status": "GREEN"}}, "grobid": {"id": "5410a680fb6f1d11ea87560029544cb2adfa0692", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/5887c12275bd463145adf9dafe149da81d046182.txt", "contents": "\nLearning Canonical Shape Space for Category-Level 6D Object Pose and Size Estimation\n\n\nDengsheng Chen \nNational University of Defense\n\n\nJun Li \nNational University of Defense\n\n\nZheng Wang \nNational University of Defense\n\n\nKai Xu \nNational University of Defense\n\n\nLearning Canonical Shape Space for Category-Level 6D Object Pose and Size Estimation\n\nWe present a novel approach to category-level 6D object pose and size estimation. To tackle intra-class shape variations, we learn canonical shape space (CASS), a unified representation for a large variety of instances of a certain object category. In particular, CASS is modeled as the latent space of a deep generative model of canonical 3D shapes with normalized pose. We train a variational auto-encoder (VAE) for generating 3D point clouds in the canonical space from an RGBD image. The VAE is trained in a cross-category fashion, exploiting the publicly available large 3D shape repositories. Since the 3D point cloud is generated in normalized pose (with actual size), the encoder of the VAE learns view-factorized RGBD embedding. It maps an RGBD image in arbitrary view into a poseindependent 3D shape representation. Object pose is then estimated via contrasting it with a pose-dependent feature of the input RGBD extracted with a separate deep neural networks. We integrate the learning of CASS and pose and size estimation into an end-to-end trainable network, achieving the state-of-the-art performance.\n\nIntroduction\n\n6D object pose estimation based on a single-view RGB(D) image is an essential building block for several real-world applications ranging from robotic navigation and manipulation to augmented reality. Most existing works have so far been addressing instance-level 6D pose estimation where each target object has a corresponding CAD model with exact shape and size [17]. Thereby, the problem is largely reduced to finding sparse or dense correspondence between the target object and the stock 3D model. Pose hypotheses can then be generated and verified based on the correspondences. Although enjoying high pose accuracy, the requirement of exact CAD models by these techniques hinders their practical use in many application scenarios. * Joint first authors \u2020 Corresponding author: kevin.kai.xu@gmail.com Figure 1: We present a method for category-level 6D object pose and size estimation (left) via learning canonical shape space. The input RGBD image is embedded into the shape space, resulting in a view-factorized RGBD embedding. Object pose is then estimated via contrasting it with a pose-dependent feature of the input RGBD. A side-product of our method is the full-shape reconstruction of the input single-view RGBD, which supports not only size calculation but also precise robotic grasping. In the figure, the reconstructed 3D point clouds are placed into the scene point cloud (unprojected from the input depth map). Two of them are highlighted in the middle.\n\nRecently, category-level 6D object pose estimation starts to gain attention [21,29]. In this problem, the target object of a shape category is unseen before and no CAD model is available, although some other instances of the same category may have been seen. Therefore, the major challenge is how to deal with intra-class variation [20]. In general, household objects could exhibit significant variations in color, texture, shape and size even within the same category. Without an exactly same CAD model, correspondence-based approach would find difficulty given the considerable intraclass shape variation.\n\nTo resolve this, a unified representation for a variety of instances of an object category is needed, to which the target object in observation could be \"matched\". The recently proposed Normalized Object Coordinate Space (NOCS) [29] is a nice example of such unified representation. Based on this representation, category-level object poses can be estimated with high accuracy through mapping each pixel of the in-put image to a point in NOCS. Finding dense mappings between NOCS and an unseen object, however, is an ill-posed problem under significant shape variation. Therefore, a generalizable mapping function accommodating large amount of unknown shape variants can be difficult to learn.\n\nIn this work, we propose to learn a canonical shape space (CASS) as our unified representation. CASS is modeled by the latent space of a deep generative model of canonical 3D shapes with normalized pose and actual metric size. In particular, we train a variational auto-encoder (VAE) for generating 3D point clouds in the canonical space from an RGBD image. The VAE is trained in a cross-category fashion, exploiting the publicly available large 3D shape repositories. Since the 3D point cloud is generated with normalized pose and metric size, the encoder of the VAE learns viewfactorized RGBD embedding. It maps an RGBD image in arbitrary view into a pose-independent 3D shape representation. Object pose can then be estimated via contrasting it with a pose-dependent feature of the input RGBD extracted with a separate deep neural networks ( Figure 1). This circumvents the difficulty in estimating dense correspondence between two representations as in other methods [17,29].\n\nWe integrate the learning of CASS and pose and size estimation into an end-to-end trainable network which involves several key designs. First, through learning the canonical shape space with plentiful shape variants, we obtain a unified representation encompassing adequate shape variations. Second, to overcome the lack of real-world training images with 3D point clouds, we enhance the encoder of the VAE to take both RGBD images and 3D shapes as input. This allows us to train the VAE through exploiting off-the-shelf 3D shape repositories. Third, in realizing pose estimation, we opt for feature contrasting over dense correspondence, leading to better generality to unseen instances. Meanwhile, to match the distributions of pose-dependent and pose-independent features so that pose estimation can be easily trained, we propose a few crucial designs, e.g., network weight sharing and training batch mixing. Last, our VAE model is able to reconstruct a 3D point cloud of the target object with metric size, which reduces the learning difficulty by decoupling the estimation of pose and size.\n\nThrough evaluating on public category-level datasets, we show that our method archives the state-of-the-art pose accuracy and comparably high size accuracy. Our work makes the following contributions:\n\n\u2022 We propose a novel correspondence-free approach to category-level object pose and size estimation based on learned canonical shape space.\n\n\u2022 We design an end-to-end trainable deep neural network for jointly learning the canonical shape space and estimating object pose and size.\n\n\u2022 We devise several key designs to ease the network training such as distribution matching between posedependent and pose-independent features.\n\n\nRelated Work\n\nInstance-level approaches. Many works on instance-level 6D pose estimation adopt template-based methods [11,15]. In these methods, a set of RGB(D) templates rendered from CAD models in various poses are matched against the input image in a sliding-window fashion, based on hand-designed or learned feature descriptors. The final pose is retrieved from the best matched template or estimated by 3D model registration. Another group of works pursue to match the target object to the corresponding 3D model. Depending on the input modality, the core task is to find 2D-to-3D [8,12], 2.5D(depth)-to-3D [5,6] or 3D-to-3D correspondence [1]. Several other works opt to learn a 6D object pose regressor directly from the feature descriptions [22,25]. Brachmann et al. [3] learn to regress an object coordinate representation which can then be used in pose estimation.\n\nLearning effective feature representation using convolutional neural networks (CNNs) for robust matching has become a main focus of the recent literature [2,14,24,30,33]. Another line of works utilize CNNs to detect feature points or corner points [19,26]. PVNet [17] is a unique approach of feature point detection using CNNs: A vector field is estimated for the input RGB image based on which the feature points are voted. Some other works choose to learn an end-to-end deep model that can directly regress 6D object pose from the raw RGB(D) input [7,16,31]. SSD-6D [13] combines single-shot object detection in RGB images with pose hypothesis regression and verification. Similar approach has also been used for multi-view active pose estimation [23]. Wang et al. [28] propose DenseFusion to learn pixel-wise feature extraction and pose estimation. The network can also predict a confidence for each pose hypothesis for final pose selection.\n\nXiao et al. [32] train a CNN that takes as input both an image and a CAD model, and outputs object pose with respect to the 3D model. This model can generalize to the target objects which are unseen during training. However, they still require the target CAD model during inference. Thus, we classify it as an instance-level work.\n\nCategory-level approaches. There has been a large body of works on category-level object detection and 2D/3D/4D pose estimation. However, methods designed for estimating 6D poses is still scarce [20]. Sahin et al. [21] introduce a part-based random forest approach for this task. In their method, parts extracted from CAD model instances of some category are represented with skeletons, which are fed into a random forest for hypothesizing 6D poses. Due to the reliance on purely geometric features, this method mainly deals with depth input. Wang et al. [29] introduce Normalized Object Coordinate Space (NOCS) as a shared canonical representation of object instances within a category. They train a region-based neural network to directly infer the pixel-wise correspondence between an RGB image to NOCS. Together with instance mask and depth map, 6D object pose is estimated using shape matching. Recently, Wang et al. [27] realized category-level 6D pose tracking based on keypoint matching.\n\nCASS vs. NOCS. Although both CASS and NOCS can be regarded as a unified shape space spanning intra-class variations, there are several substantial differences. First, NOCS is explicitly defined through consistently aligning all object instances of a category in a normalized 3D space. Our CASS is a shape embedding space implicitly learned with a generative model. Second, when conducting object pose estimation, NOCS is used as the target of pixel-wise correspondence, based on which 6D pose is computed geometrically. In contrast, CASS is treated as a normalized, holistic shape representation from which pose is estimated in an end-to-end and correspondence-free manner. Third, different from NOCS where the coordinates are regressed only for visible area, our network learns to reconstruct a complete 3D shape in CASS which is a global shape understanding beneficial to pose estimation.\n\n\nModel\n\nOur model is an end-to-end trainable network integrating the learning of both shape space and pose estimation. We first provide an overview of the network architecture and then elaborate the various network modules. Training details such as loss functions, parameter setting and training protocol will then follow.\n\nArchitecture overview. Figure 2 shows an overview of our network architecture. The input to the network is a calibrated RGBD image. The one output is the 6DoF pose of the object of interest, represented by a rigid transformation [R|t] with R \u2208 SO(3) and t \u2208 R 3 . The other output is a reconstructed 3D point cloud of the object in normalized pose but with metric size. To handle multiple objects in a cluttered scene, we employ an off-the-shelf object detector to detect and segment the individual object instances. For each detected object, we crop the RGB image with the bounding box of the segmentation mask and segment it out of the point cloud (converted from the depth image) using the mask, resulting in an image crop and a point patch for the object, respectively. Both the image crop and the point patch are sent to the main part of our network.\n\nOur core network is composed of three modules responsible for 1) Canonical Shape Space learning, view-factorizing RGBD embedding and point cloud reconstruction, 2) posedependent feature extraction and 3) pose estimation, respec-  tively. The three components are tightly coupled and jointly trained using both synthetic and real-world data. Next, we elaborate the design of the three modules.\n\n\nCanonical Shape Space and View Factorization\n\nOur goal is to learn a shape space spanning as many shape variants of a category as possible, where all shapes are posenormalized but with actual metric size. Moreover, to work with RGBD inputs, we also need a function to map an RGBG image to the point in that space representing the corresponding full shape in normalized pose and metric size. Such a mapping factorizes the view in the RGBD image so that the RGBD feature embedding is view-factorized.\n\nLearning Canonical Shape Space. We model the space of canonical shapes with the latent space of a deep generative model of pose-normalized shapes. In achieving so, we leverage the publicly available 3D shape repositories such as ShapeNet [4]. The 3D models in ShapeNet are consistently oriented and properly scaled within each category. We sample each model into a point cloud of M = 500 points. The point sampled 3D shapes, X 3D , are used to train a variational auto-encoder (VAE). The encoder employs the geometric embedding network for 3D point clouds proposed in [28], which is a variant of PointNet [18]. Based on the learned feature, the decoder warps a point cloud of 3D ellipsoid to match the shape of the input point cloud.\n\nWe turn the auto-encoder into a VAE by adding a sampling layer between the encoder and decoder. The learned posterior distribution z \u223c p(z|X 3D ) models the space of canonical shapes.\n\nLearning view-factorizing RGBD embedding. Having learned the CASS, our next task is to project an RGBD image in arbitrary view to the space so that the projector functions as view factorization. Such cross-modality data projection task could be finished with the help of data correspondence between the two modalities [9], where metric loss is used to optimize the projector. Let us refer to this solution as correspondence-based projection. This approach can be adapted to VAE straightforwardly where a projector is trained to map data in one modality to the latent space learned for another modality based on cross-modality data correspondence. However, we found that this method leads to suboptimal point cloud reconstruction and pose estimation due to 1) possibly incorrect correspondences and 2) the compromise between the metric loss and other losses.\n\nTo address these issues, we opt for a joint embedding approach. Specifically, we learn a VAE which has two encoders mapping both RGBD images and 3D point clouds to a shared latent space. Whilst the 3D encoder adopts Point-Net, the RGBD encoder employs the dense fusion architecture proposed in [28] (we use the global feature for the whole image instead of the pixel-wise features). Our crucial design is that the two encoders, albeit having different network architectures, are trained with mixed training batch and shared training gradients. The latter means that gradients computed for either modality are back-propagated to tune both encoders. Through such mixed training, the learned shared latent space spans the joint feature space of the both modalities.\n\nCompared to correspondence-based approaches, our joint embedding has the following advantages: Firstly, our model can be trained in a correspondence-free or unpaired fashion. This means the two modalities do not have to share object instances: It is unnecessary for the object in an RGBD image to have a corresponding 3D model in the training shape set. Secondly, our model introduces no extra loss function other than the basic ones of conventional VAEs. Thirdly and most importantly, the mixed training of the two encoders help to match the feature distributions of the two data modalities (see Figure 4), leading to better model generality and domain transferability.\n\nIn summary, the learning of the CASS and the RGBD feature embedding (denoted by F vf ) optimizes the following loss functions:\nL CASS = L recon (X 3D , X R 3D ) + L recon (X R rgbd , X * 3D ) + L KL ,(1)\nwhere L recon and L KL are the reconstruction loss and KL divergence loss, respectively. X 3D and X R 3D are the input and reconstructed 3D point clouds, and X R rgbd and X * 3D the 3D point cloud reconstructed from the input RGBD and the corresponding ground-truth, respectively. We use Chamfer distance to measure reconstruction loss. During test, the 3D point cloud encoder (corresponding to the network branch with red arrows in Figure 2) is discarded and only the RGBD encoder is used for feature extraction.\n\nThe RGBD encoder factorizes image view, resulting in pose-independent RGBD feature (CASS code). However,  the 3D encoder does not factorize object pose or size. This is because both the input and output of the 3D encoder are pose-normalized and metrically sized. It simply maps a pose-normalized shape to the canonical shape space, without processing on its pose or size. Figure 3 gives an illustrative summary of the view/pose-factorization ability for all network modules, and the pose-dependency of all involved data and features. Figure 5 (top row) shows the t-SNE visualization of the two feature embeddings. In the plot of viewfactorized RGBD features, objects are clustered by category with different poses mixed together, indicating the factorization of view. The plot of geometric features of canonical point clouds (no pose) also demonstrates category-based clustering effect.\n\n\nPose-Dependent Feature Extraction\n\nTo facilitate pose estimation from an input RGBD image, we also extract pose-dependent features for the RGBD image. We devise two networks for extracting photometric and geometric features separately, based on the RGB and the depth images, respectively. In our network, these features are used for pose estimation through comparing against pose-dependent features, they are expected to encode the information of pose-color and pose-geometry correlations, respectively. Figure 5 (bottom row) shows the t-SNE plots of the two features, which both exhibit pose-induced subspace clustering effect.\n\nPhotometric feature extraction. Given the image crop containing the object of interest, we train a fully convolutional network that processes the color information into a color feature F pho . Similar to [28], the image embedding network is an auto-encoder architecture that maps an image of size H \u00d7 W \u00d7 3 to a pixel-wise feature map H \u00d7 W \u00d7 N . Each pixel has a N -dimensional vector. We then perform an average pooling over all pixel-wise features, obtaining a N -dimensional feature for the full image.  dim geometric feature F geo . Here, a key design is that this point-based feature extractor can share the same network of the PointNet-based geometric feature encoder trained for CASS learning. As mentioned above, the geometry encoder is not pose-factorizing. Consequently, it can be used to extract pose-dependent geometric features. Consequently, we have a Siamese network of PointNet-based encoders, one for pose-independent CASS embedding and the other for pose-dependent geometric feature extraction (see Figure 2). Having these two tasks share network weights reduces the amount of parameters to be learned. Furthermore, it helps to match the distributions of the CASS codes and the geometric features. This makes them more comparable in facilitating feature-comparison-based pose estimation.\n\n\nPose and Size Estimation\n\nWe concatenate F vf , F pho and F geo into a feature vector of 3N length and then feed it into a CNN with 1D convolutions. The output contains a rotation represented by a quaternion q and a 3D translation vector t. The loss function for pose prediction is defined as the discrepancy between the object point clouds transformed by the groundtruth pose and by the predicted one:\nL pose = 1 M i (Rx i + t) \u2212 (R * x i + t * ) ,(2)\nwhere x i is the i-th point of the M = 500 sampled points for the object. [R * |t * ] and [R|t] are the ground-truth and predicted poses, respectively. To handle the alignment ambiguity of symmetric objects, we relax the point-wise matching loss to Chamfer distance, similar to [28]. Object size is calculated as the dimension of the axis-aligned bounding box (AABB) of the reconstructed 3D point cloud.\n\n\nTraining Details\n\nNetwork settings. The input to our method is a 640 \u00d7 480 RGBD image. With the RGB image, we perform object detection and segmentation. Any off-the-shelf method can be used. For example, we utilize Mask-RCNN [10] for the CAMERA dataset. The image crops do not need to be resize as they are fed into pixel-wise CNNs. All point patches and 3D models are re-sampled into 500 points for Point-Net feature encoding. The dimensionality of CASS code and all other features is N = 1024. The DenseFusion and FoldingNet modules involved in the various network components use the same network configuration as the original works. The configuration of all other network modules such as CNNs and MLPs are given in Figure 2 (e.g., \"4L\" means four layers and \"1D\" means 1D convolutional layers). For each convolutional layer in the various modules, we add a Batch Normalization layer followed by an ReLU nonlinearity. See more details in the supplemental material.\n\nTraining protocol. We adopt a three-stage training. The first stage trains the VAE for CASS learning and viewfactorizing RGBD embedding (the part shaded in light blue in Figure 2) for 80K iterations. The size of mixed batch is 8 which randomly mixes the training data of RGBD encoding and 3D encoding. In the second stage, we fix the VAE and jointly train pose-dependent feature extraction (the light green part) and pose estimation (the light red part) for 80K iterations. The third stage then jointly fine-tunes all parts for 40K iterations. All training batch has the size of 8. We use an initial learning rate of 0.0001 and the ADAM optimizer (\u03b2 1 = 0.9 and \u03b2 2 = 0.999) with a 1 \u00d7 10 \u22126 weight decay. In each stage, we decrease the learning rate by a factor of 10 for every 40K iterations.\n\n\nResults and evaluations\n\nIn this section, we aim to answer the following questions with both qualitative and quantitative evaluations. 1) Whether are the various network modules and design choices necessary? 2) How does our method perform in terms of pose accuracy and when does it outperform the state-of-the-arts? 3) How capable is our network in terms of single-view shape reconstruction?\n\n\nDatasets\n\nWe use the datasets from NOCS [29] which contains six categories: bottle, bowl, camera, can, laptop, and mug. The dataset has two parts: a real-world dataset with 4.3K RGBD images from 7 scene videos (3 instances per category) and a synthetic dataset with 275K rendered images generated with 1085 model instances from ShapeNet-Core [4] under random views. We evaluate our method on the NOCS-REAL275 dataset, which contains 2.75K real scene images with 3 unseen instances per category. In learning the CASS, we also utilized the 3D models from the ShapeNetCore dataset.\n\n\nEvaluation Metrics\n\nWe follow the evaluation metrics in NOCS [29] which jointly measure the object detection and pose estimation:\n\n\u2022 IoU25 & IoU50: the average precision of object instances for which the 3D overlap between the two bounding boxes is larger than 25% or 50% under predicted and ground truth poses respectively;\n\n\u2022 5 \u2022 5cm, 10 \u2022 5cm & 10 \u2022 10cm: the average precision of object instances for which the the error is less than n \u2022 for rotation and m cm for translation. We choose 5 \u2022 5cm, 10 \u2022 5cm, 10 \u2022 10cm similar to [29].\n\nAdditionally, we employ the Chamfer Distance (CD) and Earth Mover's Distance (EMD) to evaluate shape reconstruction from single-view RGBD images.\n\n\nEvaluation on NOCS-REAL275 Dataset\n\nCategory-level pose and size estimation. In Table 1, we compare our method against NOCS [29], which is the stateof-the-art method for category-level 6D object pose and size estimation. In their method, the network is trained to find a normalized coordinate for each pixel and then solve for the pose and size with the help of depth map. On the contrary, our method directly regresses the 6D pose by comparing pose-independent and pose-dependent features. We report the results of NOCS with 32 pose classification bins, which is its best-performing variant. Like NOCS, our results were not post-processed, e.g., by ICP refinement, although that is potentially facilitated by our point cloud reconstruction.\n\nThe results show that our method outperforms NOCS in all metrics except the IoU metrics. The slightly lower IoU values are caused by our less accurate size calculation based on point cloud reconstruction. Direct regression of 6D pose is a hard problem. The success of our method is mainly attributed to the strong view-factorized (pose-independent) feature learning with the help of CASS learning and its RGBD embedding. Figure 6 shows more detailed analysis with category-wise plots of the various evaluation metrics.\n\nShape reconstruction. Table 2 reports a quantitative evaluation of 3D point cloud reconstruction from RGBD input, based on the test set of NOCS-REAL275. From the table, batch mixing leads to much higher reconstruction accuracy in terms of both Chamfer Distance (CD) and Earth Mover's Distance (EMD) metrics. This is because batch mixing ensures the distributions matching between the RGBD embedding and canonical point cloud embedding. This leads to a more accurate RGBD projection (embedding) into the latent shape space.\n\n\nAblation Studies\n\nTo experimentally justify the various design choices of our method, we make the following ablations (or their combinations) to our model:\n\n\u2022 w/o CASS. Train the pose & size estimation network without the CASS code as an input. \u2022 w/o Distribution Matching (DM). Replace the Siamese network with two independent modules, without matching the distribution of the CASS codes and the geometric features.  From the results reported in Table 3, we can see that CASS learning is the most important component for our method. Without CASS learning, the accuracy drops the most especially on the xx \u2022 yy-cm metrics. Next to CASS learning, batch mixing is also very important factors. VAE is beneficial to model generalization to unseen objects since it helps learning a well-spanned CASS space with the normal distribution prior. However, it does lead to blurred 3D reconstruction at the same time, which may sacrifices size accuracy (see the IoU comparison in Table 1). Nevertheless, all factors together contributes the high-precision (5 \u2022 5cm) estimation of pose and size.\n\n\nQualitative results\n\nVisual results of pose and size estimation. tated bounding box for each detected instance overlaid on top of the input RGB images. As can be observed, our method achieves better accuracy especially for size estimation under object occlusion and background distraction.\n\nVisual results of shape reconstruction. Figure 8 shows visual results of shape reconstruction. Our method is able to reconstruct full 3D shapes in point cloud from singleview RGBD images, contrasting them with the point clouds unprojected from depth maps.\n\n\nConclusion\n\nWe have presented a novel correspondence-free approach to category-level object pose and size estimation. This is achieved by learning a shape space of 3D models in normalized pose and metric size based on deep generative model. The input RGBD image is embedded into the shape space, extracting pose-independent features. Pose estimation is realized by comparing the pose-independent and pose-dependent features. Evaluation shows that our method arrives at the state-of-the-art performance.\n\nLimitations and future work. Our current method has a few limitations on which we aim to improve as future work. First, our method cannot handle well very complex shapes due to the difficulty in reconstructing shapes with complicated geometry (e.g. high genus). In this aspect, our method can be enhanced by learning a more powerful shape reconstruction with, e.g., volumetric 3D representation. Second, our current method does not close the loop in terms of utilizing the reconstructed shape geometry to guide/supervise the training of pose estimation. This may lead to a unsupervised or self-taught approach which we plan to investigate in a future work. Third, our method still cannot achieve very high precision, as reflected by the relatively lower accuracy for the 5 \u2022 5cm metric. This may be an inherent limitation for a correspondence-free or sparse approach. Note, however, our method did not use ICP to refine the pose or size. Last, we plan to extend our current framework to online object pose tracking similar to [27].\n\nFigure 2 :\n2An overview of our network architecture. A pre-processing (left) is devised to produce image crop and point patch of the object of interest which are fed into the main network (right). The main network is composed of three modules: 1) CASS learning, pose-factorizing embedding and point cloud reconstruction (background shaded in light blue), 2) posedependent feature extraction (light green), and 3) pose estimation (light red). The network branch indicated with red arrows is used only in training.\n\nFigure 3 :\n3Illustrating the flow of pose information in our network. Data and features are depicted with rounded boxes and networks with rectangles. Data/features shaded in light blue contain pose information; no shading means posenormalized. Networks are shaded in light green if they are pose factorizing and no shading otherwise.\n\nFigure 4 :\n4Comparing the t-SNE plot of view-factorized RGBD features and the geometric features of canonical point clouds for with and without batch mixing. Batch mixing helps to match the distributions of the two features.\n\n\nGeometric feature extraction. Given the corresponding point patch, we utilize point-based CNNs to extract an N -View-factorized RGBD features Geometric features of canonical point clouds Geometric features of point patch Photometric feature of image crop\n\nFigure 5 :\n5Comparing t-SNE visualization of the various features involved in our network. Different symbols indicate different object categories while distinct colors correspond to different poses.\n\nFigure 6 :\n6Result on NOCS REAL275 test dataset, average precision (AP) vs. different thresholds on 3D IoU, rotation error, and translation error.\n\nFigure 7 :Figure 8 :\n78Figure 7shows some visual comparisons between our method and NOCS. According to the estimate pose and scale, we draw orien-NOCS CASS Qualitative comparison with NOCS[29] for pose and size estimation (depicted with reconstructed point clouds). 3D Reconstruction from single-view RGBD. The point clouds in grey color are unprojected from depth maps. The blue point clouds are reconstructed by our network.\n\nTable 1 :\n1Quantitative comparison with NOCS [29] (we use \nits best-performing variant, i.e., 32-bin NOCS map classifi-\ncation). \n\nMethod \n\nmAP \n\nIoU 25 IoU 50 \n5 \u2022 \n5cm \n\n10 \u2022 \n5cm \n\n10 \u2022 \n10cm \nNOCS \n84.4 \n79.3 16.1 43.7 43.1 \nOurs \n84.2 \n77.7 23.5 58.0 58.3 \n\n\n\nTable 2 :\n2Evaluation of point cloud reconstruction accuracy with CD (\u00d710 \u22123 ) and EMD metrics. The results show that method with batch mixing achieves uniformly higher reconstruction accuracy.w/o Batch Mixing \nw/ Batch Mixing \nCD \nEMD \nCD \nEMD \nbottle \n1.71 \n0.24 \n0.75 \n0.04 \nbow \n0.93 \n0.07 \n0.38 \n0.04 \ncamera \n5.26 \n0.22 \n0.77 \n0.05 \ncan \n1.79 \n0.20 \n0.42 \n0.04 \nlaptop \n1.94 \n0.10 \n3.73 \n0.09 \nmug \n2.40 \n0.11 \n0.32 \n0.03 \noverall \n2.33 \n0.16 \n1.06 \n0.05 \n\n\n\nTable 3 :\n3Ablation study of our model. The results show that our full method works the best for most criteria.\u2022 w/o Batch Mixing (BM). Remove batch mixing and use L 2 distance as an additional loss to train the projection from an RGBD image in arbitrary view to the canonical shape space. \u2022 w/o VAE. Replacing the VAE with AE.Method \n\nmAP \n\nIoU 25 IoU 50 \n5 \u2022 \n5cm \n\n10 \u2022 \n5cm \n\n10 \u2022 \n10cm \nw/o CASS 83.8 \n76.2 \n4.2 \n29.5 30.0 \nw/o BM \n83.6 \n77.3 \n4.7 \n31.8 32.7 \nw/o DM \n84.0 \n79.0 \n8.4 \n39.5 40.2 \nw/o VAE \n83.7 \n77.0 \n17.0 42.1 43.6 \nFull \n84.2 \n77.7 \n23.5 58.0 58.3 \n\n\nAcknowledgementWe thank the anonymous reviewers for the valuable suggestions. We are grateful to Chen Wang, one of the authors of DenseFusion, for the help and discussion. This work\nScan2cad: Learning cad model alignment in rgb-d scans. Armen Avetisyan, Manuel Dahnert, Angela Dai, Manolis Savva, X Angel, Matthias Chang, Nie\u00dfner, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionArmen Avetisyan, Manuel Dahnert, Angela Dai, Manolis Savva, Angel X Chang, and Matthias Nie\u00dfner. Scan2cad: Learning cad model alignment in rgb-d scans. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2614-2623, 2019.\n\nRigas Kouskouridas, and Tae-Kyun Kim. Pose guided rgbd feature learning for 3d object pose estimation. Vassileios Balntas, Andreas Doumanoglou, Caner Sahin, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionJuil SockVassileios Balntas, Andreas Doumanoglou, Caner Sahin, Juil Sock, Rigas Kouskouridas, and Tae-Kyun Kim. Pose guided rgbd feature learning for 3d object pose estimation. In Pro- ceedings of the IEEE International Conference on Computer Vision, pages 3856-3864, 2017.\n\nLearning 6d object pose estimation using 3d object coordinates. Eric Brachmann, Alexander Krull, Frank Michel, Stefan Gumhold, Jamie Shotton, Carsten Rother, European conference on computer vision. SpringerEric Brachmann, Alexander Krull, Frank Michel, Stefan Gumhold, Jamie Shotton, and Carsten Rother. Learning 6d object pose estimation using 3d object coordinates. In European conference on computer vision, pages 536-551. Springer, 2014.\n\nX Angel, Thomas Chang, Leonidas Funkhouser, Pat Guibas, Qixing Hanrahan, Zimo Huang, Silvio Li, Manolis Savarese, Shuran Savva, Hao Song, Su, arXiv:1512.03012An information-rich 3d model repository. arXiv preprintAngel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012, 2015.\n\n3d pose estimation of daily objects using an rgb-d camera. Changhyun Choi, Henrik I Christensen, Changhyun Choi and Henrik I Christensen. 3d pose esti- mation of daily objects using an rgb-d camera. In 2012\n\nIEEE/RSJ International Conference on Intelligent Robots and Systems. IEEEIEEE/RSJ International Conference on Intelligent Robots and Systems, pages 3342-3349. IEEE, 2012.\n\nVoting-based pose estimation for robotic assembly using a 3d sensor. Changhyun Choi, Yuichi Taguchi, Oncel Tuzel, Ming-Yu Liu, Srikumar Ramalingam, 2012 IEEE International Conference on Robotics and Automation. IEEEChanghyun Choi, Yuichi Taguchi, Oncel Tuzel, Ming-Yu Liu, and Srikumar Ramalingam. Voting-based pose estima- tion for robotic assembly using a 3d sensor. In 2012 IEEE In- ternational Conference on Robotics and Automation, pages 1724-1731. IEEE, 2012.\n\nThanh-Toan Do, Ming Cai, Trung Pham, Ian Reid, arXiv:1802.10367Deep-6dpose: Recovering 6d object pose from a single rgb image. arXiv preprintThanh-Toan Do, Ming Cai, Trung Pham, and Ian Reid. Deep-6dpose: Recovering 6d object pose from a single rgb image. arXiv preprint arXiv:1802.10367, 2018.\n\nMatching rgb images to cad models for object pose estimation. Georgios Georgakis, Srikrishna Karanam, Ziyan Wu, Jana Kosecka, arXiv:1811.07249arXiv preprintGeorgios Georgakis, Srikrishna Karanam, Ziyan Wu, and Jana Kosecka. Matching rgb images to cad models for object pose estimation. arXiv preprint arXiv:1811.07249, 2018.\n\nLearning a predictable and generative vector representation for objects. Rohit Girdhar, F David, Mikel Fouhey, Abhinav Rodriguez, Gupta, European Conference on Computer Vision. SpringerRohit Girdhar, David F Fouhey, Mikel Rodriguez, and Ab- hinav Gupta. Learning a predictable and generative vector representation for objects. In European Conference on Com- puter Vision, pages 484-499. Springer, 2016.\n\nPiotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. Kaiming He, Georgia Gkioxari, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionKaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Gir- shick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961-2969, 2017.\n\nManolis Lourakis,\u0160t\u011bp\u00e1n Obdr\u017e\u00e1lek, and Ji\u0159\u00ed Matas. Detection and fine 3d pose estimation of texture-less objects in rgb-d images. Tom\u00e1\u0161 Hoda\u0148, Xenophon Zabulis, Proc. IROS. IROSIEEETom\u00e1\u0161 Hoda\u0148, Xenophon Zabulis, Manolis Lourakis,\u0160t\u011bp\u00e1n Obdr\u017e\u00e1lek, and Ji\u0159\u00ed Matas. Detection and fine 3d pose es- timation of texture-less objects in rgb-d images. In Proc. IROS, pages 4421-4428. IEEE, 2015.\n\nHamid Izadinia, Qi Shan, Steven M Seitz, Im2cad, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionHamid Izadinia, Qi Shan, and Steven M Seitz. Im2cad. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5134-5143, 2017.\n\nSSD-6D: Making rgb-based 3d detection and 6d pose estimation great again. Wadim Kehl, Fabian Manhardt, Federico Tombari, Slobodan Ilic, Nassir Navab, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionWadim Kehl, Fabian Manhardt, Federico Tombari, Slobodan Ilic, and Nassir Navab. SSD-6D: Making rgb-based 3d de- tection and 6d pose estimation great again. In Proceedings of the IEEE International Conference on Computer Vision, pages 1521-1529, 2017.\n\nDeep learning of local rgb-d patches for 3d object detection and 6d pose estimation. Wadim Kehl, Fausto Milletari, Federico Tombari, Slobodan Ilic, Nassir Navab, European Conference on Computer Vision. SpringerWadim Kehl, Fausto Milletari, Federico Tombari, Slobodan Ilic, and Nassir Navab. Deep learning of local rgb-d patches for 3d object detection and 6d pose estimation. In European Conference on Computer Vision, pages 205-220. Springer, 2016.\n\nYoshinori Konishi, Kosuke Hattori, Manabu Hashimoto, arXiv:1811.08588Real-time 6d object pose estimation on cpu. arXiv preprintYoshinori Konishi, Kosuke Hattori, and Manabu Hashimoto. Real-time 6d object pose estimation on cpu. arXiv preprint arXiv:1811.08588, 2018.\n\nDeepim: Deep iterative matching for 6d pose estimation. Yi Li, Gu Wang, Xiangyang Ji, Yu Xiang, Dieter Fox, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Yi Li, Gu Wang, Xiangyang Ji, Yu Xiang, and Dieter Fox. Deepim: Deep iterative matching for 6d pose estimation. In Proceedings of the European Conference on Computer Vi- sion (ECCV), pages 683-698, 2018.\n\nPVNet: Pixel-wise voting network for 6dof pose estimation. Sida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, Hujun Bao, Proc. CVPR. CVPRSida Peng, Yuan Liu, Qixing Huang, Xiaowei Zhou, and Hu- jun Bao. PVNet: Pixel-wise voting network for 6dof pose estimation. In Proc. CVPR, pages 4561-4570, 2019.\n\nPointnet: Deep learning on point sets for 3d classification and segmentation. Hao Charles R Qi, Kaichun Su, Leonidas J Mo, Guibas, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionCharles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 652-660, 2017.\n\nBb8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth. Mahdi Rad, Vincent Lepetit, Proc. ICCV. ICCVMahdi Rad and Vincent Lepetit. Bb8: A scalable, accurate, robust to partial occlusion method for predicting the 3d poses of challenging objects without using depth. In Proc. ICCV, pages 3828-3836, 2017.\n\nInstance-and category-level 6d object pose estimation. Caner Sahin, Guillermo Garcia-Hernando, Juil Sock, Tae-Kyun Kim, arXiv:1903.04229arXiv preprintCaner Sahin, Guillermo Garcia-Hernando, Juil Sock, and Tae-Kyun Kim. Instance-and category-level 6d object pose estimation. arXiv preprint arXiv:1903.04229, 2019.\n\nCategory-level 6d object pose recovery in depth images. Caner Sahin, Tae-Kyun Kim, Proceedings of the ECCV Workshop. the ECCV WorkshopCaner Sahin and Tae-Kyun Kim. Category-level 6d object pose recovery in depth images. In Proceedings of the ECCV Workshop, 2018.\n\nIterative hough forest with histogram of control points for 6 dof object registration from depth images. Caner Sahin, Rigas Kouskouridas, Tae-Kyun Kim, Proc. IROS. IROSIEEECaner Sahin, Rigas Kouskouridas, and Tae-Kyun Kim. Iter- ative hough forest with histogram of control points for 6 dof object registration from depth images. In Proc. IROS, pages 4113-4118. IEEE, 2016.\n\nMulti-view 6d object pose estimation and camera motion planning using rgbd images. Juil Sock, Luis Seabra Hamidreza Kasaei, Tae-Kyun Lopes, Kim, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionJuil Sock, S Hamidreza Kasaei, Luis Seabra Lopes, and Tae- Kyun Kim. Multi-view 6d object pose estimation and camera motion planning using rgbd images. In Proceedings of the IEEE International Conference on Computer Vision, pages 2228-2235, 2017.\n\nMulti-task deep networks for depth-based 6d object pose and joint registration in crowd scenarios. Juil Sock, Kwang In, Caner Kim, Tae-Kyun Sahin, Kim, arXiv:1806.03891arXiv preprintJuil Sock, Kwang In Kim, Caner Sahin, and Tae-Kyun Kim. Multi-task deep networks for depth-based 6d object pose and joint registration in crowd scenarios. arXiv preprint arXiv:1806.03891, 2018.\n\nLatent-class hough forests for 3d object detection and pose estimation. Alykhan Tejani, Danhang Tang, Rigas Kouskouridas, Tae-Kyun Kim, Proc. ECCV. ECCVSpringerAlykhan Tejani, Danhang Tang, Rigas Kouskouridas, and Tae-Kyun Kim. Latent-class hough forests for 3d object de- tection and pose estimation. In Proc. ECCV, pages 462-477. Springer, 2014.\n\nReal-time seamless single shot 6d object pose prediction. Bugra Tekin, N Sudipta, Pascal Sinha, Fua, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionBugra Tekin, Sudipta N Sinha, and Pascal Fua. Real-time seamless single shot 6d object pose prediction. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 292-301, 2018.\n\n6-pack: Category-level 6d pose tracker with anchor-based keypoints. Chen Wang, Roberto Mart\u00edn-Mart\u00edn, Danfei Xu, Jun Lv, Cewu Lu, Li Fei-Fei, Silvio Savarese, Yuke Zhu, arXiv:1910.10750arXiv preprintChen Wang, Roberto Mart\u00edn-Mart\u00edn, Danfei Xu, Jun Lv, Cewu Lu, Li Fei-Fei, Silvio Savarese, and Yuke Zhu. 6-pack: Category-level 6d pose tracker with anchor-based keypoints. arXiv preprint arXiv:1910.10750, 2019.\n\nDensefusion: 6d object pose estimation by iterative dense fusion. Chen Wang, Danfei Xu, Yuke Zhu, Roberto Mart\u00edn-Mart\u00edn, Cewu Lu, Li Fei-Fei, Silvio Savarese, Proc. CVPR. CVPRChen Wang, Danfei Xu, Yuke Zhu, Roberto Mart\u00edn-Mart\u00edn, Cewu Lu, Li Fei-Fei, and Silvio Savarese. Densefusion: 6d object pose estimation by iterative dense fusion. In Proc. CVPR, pages 3343-3352, 2019.\n\nNormalized object coordinate space for category-level 6d object pose and size estimation. He Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, Leonidas J Guibas, Proc. CVPR. CVPRHe Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and Leonidas J Guibas. Normalized object coordinate space for category-level 6d object pose and size estimation. In Proc. CVPR, pages 2642-2651, 2019.\n\nLearning descriptors for object recognition and 3d pose estimation. Paul Wohlhart, Vincent Lepetit, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionPaul Wohlhart and Vincent Lepetit. Learning descriptors for object recognition and 3d pose estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3109-3118, 2015.\n\nPosecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes. Yu Xiang, Tanner Schmidt, Venkatraman Narayanan, Dieter Fox, arXiv:1711.00199arXiv preprintYu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox. Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes. arXiv preprint arXiv:1711.00199, 2017.\n\nPose from shape: Deep pose estimation for arbitrary 3d objects. Yang Xiao, Xuchong Qiu, Pierre-Alain Langlois, Mathieu Aubry, Renaud Marlet, arXiv:1906.05105arXiv preprintYang Xiao, Xuchong Qiu, Pierre-Alain Langlois, Math- ieu Aubry, and Renaud Marlet. Pose from shape: Deep pose estimation for arbitrary 3d objects. arXiv preprint arXiv:1906.05105, 2019.\n\nHolistic and local patch framework for 6d object pose estimation in rgb-d images. Haoruo Zhang, Qixin Cao, Computer Vision and Image Understanding. 180Haoruo Zhang and Qixin Cao. Holistic and local patch framework for 6d object pose estimation in rgb-d images. Computer Vision and Image Understanding, 180:59-73, 2019.\n", "annotations": {"author": "[{\"end\":136,\"start\":88},{\"end\":177,\"start\":137},{\"end\":222,\"start\":178},{\"end\":263,\"start\":223}]", "publisher": null, "author_last_name": "[{\"end\":102,\"start\":98},{\"end\":143,\"start\":141},{\"end\":188,\"start\":184},{\"end\":229,\"start\":227}]", "author_first_name": "[{\"end\":97,\"start\":88},{\"end\":140,\"start\":137},{\"end\":183,\"start\":178},{\"end\":226,\"start\":223}]", "author_affiliation": "[{\"end\":135,\"start\":104},{\"end\":176,\"start\":145},{\"end\":221,\"start\":190},{\"end\":262,\"start\":231}]", "title": "[{\"end\":85,\"start\":1},{\"end\":348,\"start\":264}]", "venue": null, "abstract": "[{\"end\":1465,\"start\":350}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1848,\"start\":1844},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3032,\"start\":3028},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3035,\"start\":3032},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3288,\"start\":3284},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3793,\"start\":3789},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5231,\"start\":5227},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5234,\"start\":5231},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7086,\"start\":7082},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7089,\"start\":7086},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7553,\"start\":7550},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7556,\"start\":7553},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7579,\"start\":7576},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7581,\"start\":7579},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7612,\"start\":7609},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7717,\"start\":7713},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7720,\"start\":7717},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7742,\"start\":7739},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7997,\"start\":7994},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8000,\"start\":7997},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8003,\"start\":8000},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8006,\"start\":8003},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8009,\"start\":8006},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8092,\"start\":8088},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8095,\"start\":8092},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8107,\"start\":8103},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8393,\"start\":8390},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8396,\"start\":8393},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8399,\"start\":8396},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8412,\"start\":8408},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8593,\"start\":8589},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8611,\"start\":8607},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8802,\"start\":8798},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9317,\"start\":9313},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9336,\"start\":9332},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9677,\"start\":9673},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10044,\"start\":10040},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13324,\"start\":13321},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":13655,\"start\":13651},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13692,\"start\":13688},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14324,\"start\":14321},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":15160,\"start\":15156},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":18744,\"start\":18740},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":20580,\"start\":20576},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":20933,\"start\":20929},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":22907,\"start\":22903},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":23208,\"start\":23205},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":23509,\"start\":23505},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":23979,\"start\":23975},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":24258,\"start\":24254},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":29086,\"start\":29082},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":30961,\"start\":30957}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29601,\"start\":29088},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29936,\"start\":29602},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30162,\"start\":29937},{\"attributes\":{\"id\":\"fig_4\"},\"end\":30419,\"start\":30163},{\"attributes\":{\"id\":\"fig_5\"},\"end\":30619,\"start\":30420},{\"attributes\":{\"id\":\"fig_6\"},\"end\":30767,\"start\":30620},{\"attributes\":{\"id\":\"fig_7\"},\"end\":31195,\"start\":30768},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":31460,\"start\":31196},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":31925,\"start\":31461},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":32500,\"start\":31926}]", "paragraph": "[{\"end\":2950,\"start\":1481},{\"end\":3559,\"start\":2952},{\"end\":4254,\"start\":3561},{\"end\":5235,\"start\":4256},{\"end\":6332,\"start\":5237},{\"end\":6534,\"start\":6334},{\"end\":6675,\"start\":6536},{\"end\":6816,\"start\":6677},{\"end\":6961,\"start\":6818},{\"end\":7838,\"start\":6978},{\"end\":8784,\"start\":7840},{\"end\":9116,\"start\":8786},{\"end\":10113,\"start\":9118},{\"end\":11005,\"start\":10115},{\"end\":11329,\"start\":11015},{\"end\":12186,\"start\":11331},{\"end\":12580,\"start\":12188},{\"end\":13081,\"start\":12629},{\"end\":13816,\"start\":13083},{\"end\":14001,\"start\":13818},{\"end\":14860,\"start\":14003},{\"end\":15624,\"start\":14862},{\"end\":16296,\"start\":15626},{\"end\":16424,\"start\":16298},{\"end\":17015,\"start\":16502},{\"end\":17903,\"start\":17017},{\"end\":18534,\"start\":17941},{\"end\":19842,\"start\":18536},{\"end\":20247,\"start\":19871},{\"end\":20701,\"start\":20298},{\"end\":21670,\"start\":20722},{\"end\":22466,\"start\":21672},{\"end\":22860,\"start\":22494},{\"end\":23441,\"start\":22873},{\"end\":23573,\"start\":23464},{\"end\":23768,\"start\":23575},{\"end\":23980,\"start\":23770},{\"end\":24127,\"start\":23982},{\"end\":24871,\"start\":24166},{\"end\":25391,\"start\":24873},{\"end\":25915,\"start\":25393},{\"end\":26073,\"start\":25936},{\"end\":27000,\"start\":26075},{\"end\":27292,\"start\":27024},{\"end\":27549,\"start\":27294},{\"end\":28054,\"start\":27564},{\"end\":29087,\"start\":28056}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":16501,\"start\":16425},{\"attributes\":{\"id\":\"formula_1\"},\"end\":20297,\"start\":20248}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":24217,\"start\":24210},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":25422,\"start\":25415},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":26372,\"start\":26365},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26893,\"start\":26886}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1479,\"start\":1467},{\"attributes\":{\"n\":\"2.\"},\"end\":6976,\"start\":6964},{\"attributes\":{\"n\":\"3.\"},\"end\":11013,\"start\":11008},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12627,\"start\":12583},{\"attributes\":{\"n\":\"3.2.\"},\"end\":17939,\"start\":17906},{\"attributes\":{\"n\":\"3.3.\"},\"end\":19869,\"start\":19845},{\"attributes\":{\"n\":\"3.4.\"},\"end\":20720,\"start\":20704},{\"attributes\":{\"n\":\"4.\"},\"end\":22492,\"start\":22469},{\"attributes\":{\"n\":\"4.1.\"},\"end\":22871,\"start\":22863},{\"attributes\":{\"n\":\"4.2.\"},\"end\":23462,\"start\":23444},{\"attributes\":{\"n\":\"4.3.\"},\"end\":24164,\"start\":24130},{\"attributes\":{\"n\":\"4.4.\"},\"end\":25934,\"start\":25918},{\"attributes\":{\"n\":\"4.5.\"},\"end\":27022,\"start\":27003},{\"attributes\":{\"n\":\"5.\"},\"end\":27562,\"start\":27552},{\"end\":29099,\"start\":29089},{\"end\":29613,\"start\":29603},{\"end\":29948,\"start\":29938},{\"end\":30431,\"start\":30421},{\"end\":30631,\"start\":30621},{\"end\":30789,\"start\":30769},{\"end\":31206,\"start\":31197},{\"end\":31471,\"start\":31462},{\"end\":31936,\"start\":31927}]", "table": "[{\"end\":31460,\"start\":31208},{\"end\":31925,\"start\":31655},{\"end\":32500,\"start\":32254}]", "figure_caption": "[{\"end\":29601,\"start\":29101},{\"end\":29936,\"start\":29615},{\"end\":30162,\"start\":29950},{\"end\":30419,\"start\":30165},{\"end\":30619,\"start\":30433},{\"end\":30767,\"start\":30633},{\"end\":31195,\"start\":30792},{\"end\":31655,\"start\":31473},{\"end\":32254,\"start\":31938}]", "figure_ref": "[{\"end\":2293,\"start\":2285},{\"end\":5109,\"start\":5101},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11362,\"start\":11354},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16231,\"start\":16223},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16943,\"start\":16935},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17397,\"start\":17389},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":17559,\"start\":17551},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":18431,\"start\":18410},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19562,\"start\":19554},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":21430,\"start\":21422},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":21850,\"start\":21842},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":25302,\"start\":25294},{\"end\":27342,\"start\":27334}]", "bib_author_first_name": "[{\"end\":32743,\"start\":32738},{\"end\":32761,\"start\":32755},{\"end\":32777,\"start\":32771},{\"end\":32790,\"start\":32783},{\"end\":32799,\"start\":32798},{\"end\":32815,\"start\":32807},{\"end\":33346,\"start\":33336},{\"end\":33363,\"start\":33356},{\"end\":33382,\"start\":33377},{\"end\":33854,\"start\":33850},{\"end\":33875,\"start\":33866},{\"end\":33888,\"start\":33883},{\"end\":33903,\"start\":33897},{\"end\":33918,\"start\":33913},{\"end\":33935,\"start\":33928},{\"end\":34230,\"start\":34229},{\"end\":34244,\"start\":34238},{\"end\":34260,\"start\":34252},{\"end\":34276,\"start\":34273},{\"end\":34291,\"start\":34285},{\"end\":34306,\"start\":34302},{\"end\":34320,\"start\":34314},{\"end\":34332,\"start\":34325},{\"end\":34349,\"start\":34343},{\"end\":34360,\"start\":34357},{\"end\":34749,\"start\":34740},{\"end\":34762,\"start\":34756},{\"end\":34764,\"start\":34763},{\"end\":35139,\"start\":35130},{\"end\":35152,\"start\":35146},{\"end\":35167,\"start\":35162},{\"end\":35182,\"start\":35175},{\"end\":35196,\"start\":35188},{\"end\":35538,\"start\":35528},{\"end\":35547,\"start\":35543},{\"end\":35558,\"start\":35553},{\"end\":35568,\"start\":35565},{\"end\":35894,\"start\":35886},{\"end\":35916,\"start\":35906},{\"end\":35931,\"start\":35926},{\"end\":35940,\"start\":35936},{\"end\":36228,\"start\":36223},{\"end\":36239,\"start\":36238},{\"end\":36252,\"start\":36247},{\"end\":36268,\"start\":36261},{\"end\":36606,\"start\":36599},{\"end\":36618,\"start\":36611},{\"end\":37058,\"start\":37053},{\"end\":37074,\"start\":37066},{\"end\":37317,\"start\":37312},{\"end\":37330,\"start\":37328},{\"end\":37343,\"start\":37337},{\"end\":37345,\"start\":37344},{\"end\":37740,\"start\":37735},{\"end\":37753,\"start\":37747},{\"end\":37772,\"start\":37764},{\"end\":37790,\"start\":37782},{\"end\":37803,\"start\":37797},{\"end\":38274,\"start\":38269},{\"end\":38287,\"start\":38281},{\"end\":38307,\"start\":38299},{\"end\":38325,\"start\":38317},{\"end\":38338,\"start\":38332},{\"end\":38644,\"start\":38635},{\"end\":38660,\"start\":38654},{\"end\":38676,\"start\":38670},{\"end\":38961,\"start\":38959},{\"end\":38968,\"start\":38966},{\"end\":38984,\"start\":38975},{\"end\":38991,\"start\":38989},{\"end\":39005,\"start\":38999},{\"end\":39394,\"start\":39390},{\"end\":39405,\"start\":39401},{\"end\":39417,\"start\":39411},{\"end\":39432,\"start\":39425},{\"end\":39444,\"start\":39439},{\"end\":39711,\"start\":39708},{\"end\":39733,\"start\":39726},{\"end\":39746,\"start\":39738},{\"end\":39748,\"start\":39747},{\"end\":40280,\"start\":40275},{\"end\":40293,\"start\":40286},{\"end\":40583,\"start\":40578},{\"end\":40600,\"start\":40591},{\"end\":40622,\"start\":40618},{\"end\":40637,\"start\":40629},{\"end\":40898,\"start\":40893},{\"end\":40914,\"start\":40906},{\"end\":41211,\"start\":41206},{\"end\":41224,\"start\":41219},{\"end\":41247,\"start\":41239},{\"end\":41563,\"start\":41559},{\"end\":41574,\"start\":41570},{\"end\":41581,\"start\":41575},{\"end\":41608,\"start\":41600},{\"end\":42093,\"start\":42089},{\"end\":42115,\"start\":42110},{\"end\":42129,\"start\":42121},{\"end\":42446,\"start\":42439},{\"end\":42462,\"start\":42455},{\"end\":42474,\"start\":42469},{\"end\":42497,\"start\":42489},{\"end\":42788,\"start\":42787},{\"end\":42804,\"start\":42798},{\"end\":43240,\"start\":43236},{\"end\":43254,\"start\":43247},{\"end\":43276,\"start\":43270},{\"end\":43284,\"start\":43281},{\"end\":43293,\"start\":43289},{\"end\":43300,\"start\":43298},{\"end\":43316,\"start\":43310},{\"end\":43331,\"start\":43327},{\"end\":43650,\"start\":43646},{\"end\":43663,\"start\":43657},{\"end\":43672,\"start\":43668},{\"end\":43685,\"start\":43678},{\"end\":43705,\"start\":43701},{\"end\":43712,\"start\":43710},{\"end\":43728,\"start\":43722},{\"end\":44049,\"start\":44047},{\"end\":44063,\"start\":44056},{\"end\":44080,\"start\":44073},{\"end\":44094,\"start\":44088},{\"end\":44111,\"start\":44105},{\"end\":44126,\"start\":44118},{\"end\":44128,\"start\":44127},{\"end\":44448,\"start\":44444},{\"end\":44466,\"start\":44459},{\"end\":44919,\"start\":44917},{\"end\":44933,\"start\":44927},{\"end\":44954,\"start\":44943},{\"end\":44972,\"start\":44966},{\"end\":45272,\"start\":45268},{\"end\":45286,\"start\":45279},{\"end\":45304,\"start\":45292},{\"end\":45322,\"start\":45315},{\"end\":45336,\"start\":45330},{\"end\":45650,\"start\":45644},{\"end\":45663,\"start\":45658}]", "bib_author_last_name": "[{\"end\":32753,\"start\":32744},{\"end\":32769,\"start\":32762},{\"end\":32781,\"start\":32778},{\"end\":32796,\"start\":32791},{\"end\":32805,\"start\":32800},{\"end\":32821,\"start\":32816},{\"end\":32830,\"start\":32823},{\"end\":33354,\"start\":33347},{\"end\":33375,\"start\":33364},{\"end\":33388,\"start\":33383},{\"end\":33864,\"start\":33855},{\"end\":33881,\"start\":33876},{\"end\":33895,\"start\":33889},{\"end\":33911,\"start\":33904},{\"end\":33926,\"start\":33919},{\"end\":33942,\"start\":33936},{\"end\":34236,\"start\":34231},{\"end\":34250,\"start\":34245},{\"end\":34271,\"start\":34261},{\"end\":34283,\"start\":34277},{\"end\":34300,\"start\":34292},{\"end\":34312,\"start\":34307},{\"end\":34323,\"start\":34321},{\"end\":34341,\"start\":34333},{\"end\":34355,\"start\":34350},{\"end\":34365,\"start\":34361},{\"end\":34369,\"start\":34367},{\"end\":34754,\"start\":34750},{\"end\":34776,\"start\":34765},{\"end\":35144,\"start\":35140},{\"end\":35160,\"start\":35153},{\"end\":35173,\"start\":35168},{\"end\":35186,\"start\":35183},{\"end\":35207,\"start\":35197},{\"end\":35541,\"start\":35539},{\"end\":35551,\"start\":35548},{\"end\":35563,\"start\":35559},{\"end\":35573,\"start\":35569},{\"end\":35904,\"start\":35895},{\"end\":35924,\"start\":35917},{\"end\":35934,\"start\":35932},{\"end\":35948,\"start\":35941},{\"end\":36236,\"start\":36229},{\"end\":36245,\"start\":36240},{\"end\":36259,\"start\":36253},{\"end\":36278,\"start\":36269},{\"end\":36285,\"start\":36280},{\"end\":36609,\"start\":36607},{\"end\":36627,\"start\":36619},{\"end\":37064,\"start\":37059},{\"end\":37082,\"start\":37075},{\"end\":37326,\"start\":37318},{\"end\":37335,\"start\":37331},{\"end\":37351,\"start\":37346},{\"end\":37359,\"start\":37353},{\"end\":37745,\"start\":37741},{\"end\":37762,\"start\":37754},{\"end\":37780,\"start\":37773},{\"end\":37795,\"start\":37791},{\"end\":37809,\"start\":37804},{\"end\":38279,\"start\":38275},{\"end\":38297,\"start\":38288},{\"end\":38315,\"start\":38308},{\"end\":38330,\"start\":38326},{\"end\":38344,\"start\":38339},{\"end\":38652,\"start\":38645},{\"end\":38668,\"start\":38661},{\"end\":38686,\"start\":38677},{\"end\":38964,\"start\":38962},{\"end\":38973,\"start\":38969},{\"end\":38987,\"start\":38985},{\"end\":38997,\"start\":38992},{\"end\":39009,\"start\":39006},{\"end\":39399,\"start\":39395},{\"end\":39409,\"start\":39406},{\"end\":39423,\"start\":39418},{\"end\":39437,\"start\":39433},{\"end\":39448,\"start\":39445},{\"end\":39724,\"start\":39712},{\"end\":39736,\"start\":39734},{\"end\":39751,\"start\":39749},{\"end\":39759,\"start\":39753},{\"end\":40284,\"start\":40281},{\"end\":40301,\"start\":40294},{\"end\":40589,\"start\":40584},{\"end\":40616,\"start\":40601},{\"end\":40627,\"start\":40623},{\"end\":40641,\"start\":40638},{\"end\":40904,\"start\":40899},{\"end\":40918,\"start\":40915},{\"end\":41217,\"start\":41212},{\"end\":41237,\"start\":41225},{\"end\":41251,\"start\":41248},{\"end\":41568,\"start\":41564},{\"end\":41598,\"start\":41582},{\"end\":41614,\"start\":41609},{\"end\":41619,\"start\":41616},{\"end\":42098,\"start\":42094},{\"end\":42108,\"start\":42100},{\"end\":42119,\"start\":42116},{\"end\":42135,\"start\":42130},{\"end\":42140,\"start\":42137},{\"end\":42453,\"start\":42447},{\"end\":42467,\"start\":42463},{\"end\":42487,\"start\":42475},{\"end\":42501,\"start\":42498},{\"end\":42785,\"start\":42774},{\"end\":42796,\"start\":42789},{\"end\":42810,\"start\":42805},{\"end\":42815,\"start\":42812},{\"end\":43245,\"start\":43241},{\"end\":43268,\"start\":43255},{\"end\":43279,\"start\":43277},{\"end\":43287,\"start\":43285},{\"end\":43296,\"start\":43294},{\"end\":43308,\"start\":43301},{\"end\":43325,\"start\":43317},{\"end\":43335,\"start\":43332},{\"end\":43655,\"start\":43651},{\"end\":43666,\"start\":43664},{\"end\":43676,\"start\":43673},{\"end\":43699,\"start\":43686},{\"end\":43708,\"start\":43706},{\"end\":43720,\"start\":43713},{\"end\":43737,\"start\":43729},{\"end\":44054,\"start\":44050},{\"end\":44071,\"start\":44064},{\"end\":44086,\"start\":44081},{\"end\":44103,\"start\":44095},{\"end\":44116,\"start\":44112},{\"end\":44135,\"start\":44129},{\"end\":44457,\"start\":44449},{\"end\":44474,\"start\":44467},{\"end\":44925,\"start\":44920},{\"end\":44941,\"start\":44934},{\"end\":44964,\"start\":44955},{\"end\":44976,\"start\":44973},{\"end\":45277,\"start\":45273},{\"end\":45290,\"start\":45287},{\"end\":45313,\"start\":45305},{\"end\":45328,\"start\":45323},{\"end\":45343,\"start\":45337},{\"end\":45656,\"start\":45651},{\"end\":45667,\"start\":45664}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":53827470},\"end\":33231,\"start\":32683},{\"attributes\":{\"id\":\"b1\"},\"end\":33784,\"start\":33233},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":16080844},\"end\":34227,\"start\":33786},{\"attributes\":{\"doi\":\"arXiv:1512.03012\",\"id\":\"b3\"},\"end\":34679,\"start\":34229},{\"attributes\":{\"id\":\"b4\"},\"end\":34887,\"start\":34681},{\"attributes\":{\"id\":\"b5\"},\"end\":35059,\"start\":34889},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":14410257},\"end\":35526,\"start\":35061},{\"attributes\":{\"doi\":\"arXiv:1802.10367\",\"id\":\"b7\"},\"end\":35822,\"start\":35528},{\"attributes\":{\"doi\":\"arXiv:1811.07249\",\"id\":\"b8\"},\"end\":36148,\"start\":35824},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":6598262},\"end\":36552,\"start\":36150},{\"attributes\":{\"id\":\"b10\"},\"end\":36921,\"start\":36554},{\"attributes\":{\"id\":\"b11\"},\"end\":37310,\"start\":36923},{\"attributes\":{\"id\":\"b12\"},\"end\":37659,\"start\":37312},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":10655945},\"end\":38182,\"start\":37661},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2123767},\"end\":38633,\"start\":38184},{\"attributes\":{\"doi\":\"arXiv:1811.08588\",\"id\":\"b15\"},\"end\":38901,\"start\":38635},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":4557621},\"end\":39329,\"start\":38903},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":57189382},\"end\":39628,\"start\":39331},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":5115938},\"end\":40139,\"start\":39630},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":4392433},\"end\":40521,\"start\":40141},{\"attributes\":{\"doi\":\"arXiv:1903.04229\",\"id\":\"b20\"},\"end\":40835,\"start\":40523},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":51892714},\"end\":41099,\"start\":40837},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":2082300},\"end\":41474,\"start\":41101},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":4733806},\"end\":41988,\"start\":41476},{\"attributes\":{\"doi\":\"arXiv:1806.03891\",\"id\":\"b24\"},\"end\":42365,\"start\":41990},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":4649936},\"end\":42714,\"start\":42367},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":4929149},\"end\":43166,\"start\":42716},{\"attributes\":{\"doi\":\"arXiv:1910.10750\",\"id\":\"b27\"},\"end\":43578,\"start\":43168},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":58006460},\"end\":43955,\"start\":43580},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":57761160},\"end\":44374,\"start\":43957},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":1418616},\"end\":44824,\"start\":44376},{\"attributes\":{\"doi\":\"arXiv:1711.00199\",\"id\":\"b31\"},\"end\":45202,\"start\":44826},{\"attributes\":{\"doi\":\"arXiv:1906.05105\",\"id\":\"b32\"},\"end\":45560,\"start\":45204},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":127812857},\"end\":45880,\"start\":45562}]", "bib_title": "[{\"end\":32736,\"start\":32683},{\"end\":33334,\"start\":33233},{\"end\":33848,\"start\":33786},{\"end\":35128,\"start\":35061},{\"end\":36221,\"start\":36150},{\"end\":36597,\"start\":36554},{\"end\":37051,\"start\":36923},{\"end\":37733,\"start\":37661},{\"end\":38267,\"start\":38184},{\"end\":38957,\"start\":38903},{\"end\":39388,\"start\":39331},{\"end\":39706,\"start\":39630},{\"end\":40273,\"start\":40141},{\"end\":40891,\"start\":40837},{\"end\":41204,\"start\":41101},{\"end\":41557,\"start\":41476},{\"end\":42437,\"start\":42367},{\"end\":42772,\"start\":42716},{\"end\":43644,\"start\":43580},{\"end\":44045,\"start\":43957},{\"end\":44442,\"start\":44376},{\"end\":45642,\"start\":45562}]", "bib_author": "[{\"end\":32755,\"start\":32738},{\"end\":32771,\"start\":32755},{\"end\":32783,\"start\":32771},{\"end\":32798,\"start\":32783},{\"end\":32807,\"start\":32798},{\"end\":32823,\"start\":32807},{\"end\":32832,\"start\":32823},{\"end\":33356,\"start\":33336},{\"end\":33377,\"start\":33356},{\"end\":33390,\"start\":33377},{\"end\":33866,\"start\":33850},{\"end\":33883,\"start\":33866},{\"end\":33897,\"start\":33883},{\"end\":33913,\"start\":33897},{\"end\":33928,\"start\":33913},{\"end\":33944,\"start\":33928},{\"end\":34238,\"start\":34229},{\"end\":34252,\"start\":34238},{\"end\":34273,\"start\":34252},{\"end\":34285,\"start\":34273},{\"end\":34302,\"start\":34285},{\"end\":34314,\"start\":34302},{\"end\":34325,\"start\":34314},{\"end\":34343,\"start\":34325},{\"end\":34357,\"start\":34343},{\"end\":34367,\"start\":34357},{\"end\":34371,\"start\":34367},{\"end\":34756,\"start\":34740},{\"end\":34778,\"start\":34756},{\"end\":35146,\"start\":35130},{\"end\":35162,\"start\":35146},{\"end\":35175,\"start\":35162},{\"end\":35188,\"start\":35175},{\"end\":35209,\"start\":35188},{\"end\":35543,\"start\":35528},{\"end\":35553,\"start\":35543},{\"end\":35565,\"start\":35553},{\"end\":35575,\"start\":35565},{\"end\":35906,\"start\":35886},{\"end\":35926,\"start\":35906},{\"end\":35936,\"start\":35926},{\"end\":35950,\"start\":35936},{\"end\":36238,\"start\":36223},{\"end\":36247,\"start\":36238},{\"end\":36261,\"start\":36247},{\"end\":36280,\"start\":36261},{\"end\":36287,\"start\":36280},{\"end\":36611,\"start\":36599},{\"end\":36629,\"start\":36611},{\"end\":37066,\"start\":37053},{\"end\":37084,\"start\":37066},{\"end\":37328,\"start\":37312},{\"end\":37337,\"start\":37328},{\"end\":37353,\"start\":37337},{\"end\":37361,\"start\":37353},{\"end\":37747,\"start\":37735},{\"end\":37764,\"start\":37747},{\"end\":37782,\"start\":37764},{\"end\":37797,\"start\":37782},{\"end\":37811,\"start\":37797},{\"end\":38281,\"start\":38269},{\"end\":38299,\"start\":38281},{\"end\":38317,\"start\":38299},{\"end\":38332,\"start\":38317},{\"end\":38346,\"start\":38332},{\"end\":38654,\"start\":38635},{\"end\":38670,\"start\":38654},{\"end\":38688,\"start\":38670},{\"end\":38966,\"start\":38959},{\"end\":38975,\"start\":38966},{\"end\":38989,\"start\":38975},{\"end\":38999,\"start\":38989},{\"end\":39011,\"start\":38999},{\"end\":39401,\"start\":39390},{\"end\":39411,\"start\":39401},{\"end\":39425,\"start\":39411},{\"end\":39439,\"start\":39425},{\"end\":39450,\"start\":39439},{\"end\":39726,\"start\":39708},{\"end\":39738,\"start\":39726},{\"end\":39753,\"start\":39738},{\"end\":39761,\"start\":39753},{\"end\":40286,\"start\":40275},{\"end\":40303,\"start\":40286},{\"end\":40591,\"start\":40578},{\"end\":40618,\"start\":40591},{\"end\":40629,\"start\":40618},{\"end\":40643,\"start\":40629},{\"end\":40906,\"start\":40893},{\"end\":40920,\"start\":40906},{\"end\":41219,\"start\":41206},{\"end\":41239,\"start\":41219},{\"end\":41253,\"start\":41239},{\"end\":41570,\"start\":41559},{\"end\":41600,\"start\":41570},{\"end\":41616,\"start\":41600},{\"end\":41621,\"start\":41616},{\"end\":42100,\"start\":42089},{\"end\":42110,\"start\":42100},{\"end\":42121,\"start\":42110},{\"end\":42137,\"start\":42121},{\"end\":42142,\"start\":42137},{\"end\":42455,\"start\":42439},{\"end\":42469,\"start\":42455},{\"end\":42489,\"start\":42469},{\"end\":42503,\"start\":42489},{\"end\":42787,\"start\":42774},{\"end\":42798,\"start\":42787},{\"end\":42812,\"start\":42798},{\"end\":42817,\"start\":42812},{\"end\":43247,\"start\":43236},{\"end\":43270,\"start\":43247},{\"end\":43281,\"start\":43270},{\"end\":43289,\"start\":43281},{\"end\":43298,\"start\":43289},{\"end\":43310,\"start\":43298},{\"end\":43327,\"start\":43310},{\"end\":43337,\"start\":43327},{\"end\":43657,\"start\":43646},{\"end\":43668,\"start\":43657},{\"end\":43678,\"start\":43668},{\"end\":43701,\"start\":43678},{\"end\":43710,\"start\":43701},{\"end\":43722,\"start\":43710},{\"end\":43739,\"start\":43722},{\"end\":44056,\"start\":44047},{\"end\":44073,\"start\":44056},{\"end\":44088,\"start\":44073},{\"end\":44105,\"start\":44088},{\"end\":44118,\"start\":44105},{\"end\":44137,\"start\":44118},{\"end\":44459,\"start\":44444},{\"end\":44476,\"start\":44459},{\"end\":44927,\"start\":44917},{\"end\":44943,\"start\":44927},{\"end\":44966,\"start\":44943},{\"end\":44978,\"start\":44966},{\"end\":45279,\"start\":45268},{\"end\":45292,\"start\":45279},{\"end\":45315,\"start\":45292},{\"end\":45330,\"start\":45315},{\"end\":45345,\"start\":45330},{\"end\":45658,\"start\":45644},{\"end\":45669,\"start\":45658}]", "bib_venue": "[{\"end\":32909,\"start\":32832},{\"end\":33457,\"start\":33390},{\"end\":33982,\"start\":33944},{\"end\":34426,\"start\":34387},{\"end\":34738,\"start\":34681},{\"end\":34956,\"start\":34889},{\"end\":35270,\"start\":35209},{\"end\":35653,\"start\":35591},{\"end\":35884,\"start\":35824},{\"end\":36325,\"start\":36287},{\"end\":36696,\"start\":36629},{\"end\":37094,\"start\":37084},{\"end\":37438,\"start\":37361},{\"end\":37878,\"start\":37811},{\"end\":38384,\"start\":38346},{\"end\":38746,\"start\":38704},{\"end\":39075,\"start\":39011},{\"end\":39460,\"start\":39450},{\"end\":39838,\"start\":39761},{\"end\":40313,\"start\":40303},{\"end\":40576,\"start\":40523},{\"end\":40952,\"start\":40920},{\"end\":41263,\"start\":41253},{\"end\":41688,\"start\":41621},{\"end\":42087,\"start\":41990},{\"end\":42513,\"start\":42503},{\"end\":42894,\"start\":42817},{\"end\":43234,\"start\":43168},{\"end\":43749,\"start\":43739},{\"end\":44147,\"start\":44137},{\"end\":44553,\"start\":44476},{\"end\":44915,\"start\":44826},{\"end\":45266,\"start\":45204},{\"end\":45708,\"start\":45669},{\"end\":32973,\"start\":32911},{\"end\":33511,\"start\":33459},{\"end\":36750,\"start\":36698},{\"end\":37100,\"start\":37096},{\"end\":37502,\"start\":37440},{\"end\":37932,\"start\":37880},{\"end\":39126,\"start\":39077},{\"end\":39466,\"start\":39462},{\"end\":39902,\"start\":39840},{\"end\":40319,\"start\":40315},{\"end\":40971,\"start\":40954},{\"end\":41269,\"start\":41265},{\"end\":41742,\"start\":41690},{\"end\":42519,\"start\":42515},{\"end\":42958,\"start\":42896},{\"end\":43755,\"start\":43751},{\"end\":44153,\"start\":44149},{\"end\":44617,\"start\":44555}]"}}}, "year": 2023, "month": 12, "day": 17}