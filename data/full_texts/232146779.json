{"id": 232146779, "updated": "2023-12-13 00:51:20.002", "metadata": {"title": "RFN-Nest: An end-to-end residual fusion network for infrared and visible images", "authors": "[{\"first\":\"Hui\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Xiao-Jun\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Josef\",\"last\":\"Kittler\",\"middle\":[]}]", "venue": "Inf. Fusion", "journal": "Inf. Fusion", "publication_date": {"year": 2021, "month": 3, "day": 7}, "abstract": "In the image fusion field, the design of deep learning-based fusion methods is far from routine. It is invariably fusion-task specific and requires a careful consideration. The most difficult part of the design is to choose an appropriate strategy to generate the fused image for a specific task in hand. Thus, devising learnable fusion strategy is a very challenging problem in the community of image fusion. To address this problem, a novel end-to-end fusion network architecture (RFN-Nest) is developed for infrared and visible image fusion. We propose a residual fusion network (RFN) which is based on a residual architecture to replace the traditional fusion approach. A novel detail-preserving loss function, and a feature enhancing loss function are proposed to train RFN. The fusion model learning is accomplished by a novel two-stage training strategy. In the first stage, we train an auto-encoder based on an innovative nest connection (Nest) concept. Next, the RFN is trained using the proposed loss functions. The experimental results on public domain data sets show that, compared with the existing methods, our end-to-end fusion network delivers a better performance than the state-of-the-art methods in both subjective and objective evaluation. The code of our fusion method is available at https://github.com/hli1221/imagefusion-rfn-nest", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3133700567", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/inffus/LiWK21", "doi": "10.1016/j.inffus.2021.02.023"}}, "content": {"source": {"pdf_hash": "30d191b63102a138b10e05606688e30621d795b2", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2103.04286v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2103.04286", "status": "GREEN"}}, "grobid": {"id": "09ccc0032132f7dca9c32c9be3cb84d6227b93e7", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/30d191b63102a138b10e05606688e30621d795b2.txt", "contents": "\nRFN-Nest: An end-to-end residual fusion network for infrared and visible images\n\n\nHui Li \nSchool of Artificial Intelligence and Computer Science\nJiangsu Provincial Engineering Laboratory of Pattern Recognition and Computational Intelligence\nJiangnan University\n214122WuxiChina\n\nXiao-Jun Wu \nSchool of Artificial Intelligence and Computer Science\nJiangsu Provincial Engineering Laboratory of Pattern Recognition and Computational Intelligence\nJiangnan University\n214122WuxiChina\n\nJosef Kittler \nThe Center for Vision, Speech and Signal Processing\nUniversity of Surrey\nGU2 7XHGuildfordUK\n\nRFN-Nest: An end-to-end residual fusion network for infrared and visible images\nimage fusionend-to-end networknest connectionresidual networkinfrared imagevisible image\nIn the image fusion field, the design of deep learning-based fusion methods is far from routine. It is invariably fusion-task specific and requires a careful consideration. The most difficult part of the design is to choose an appropriate strategy to generate the fused image for a specific task in hand. Thus, devising learnable fusion strategy is a very challenging problem in the community of image fusion. To address this problem, a novel end-to-end fusion network architecture (RFN-Nest) is developed for infrared and visible image fusion. We propose a residual fusion network (RFN) which is based on a residual architecture to replace the traditional fusion approach. A novel detail-preserving loss function, and a feature enhancing loss function are proposed to train RFN. The fusion model learning is accomplished by a novel two-stage training strategy. In the first stage, we train an auto-encoder based on an innovative nest connection (Nest) concept. Next, the RFN is trained using the proposed loss functions. The experimental results on public domain data sets show that, compared with the existing methods, our end-to-end fusion network delivers a better performance than the state-of-the-art methods in both subjective and objective evaluation. The code of our fusion method is available at https://github.com/hli1221/imagefusion-rfn-nest.\n\nIntroduction\n\nDue to the physical limitations of imaging sensors, it is very difficult to capture an image of a scene that is of uniformly good quality. Image fusion plays an important role in this context. Its aim is to reconstruct a perfect image of the scene from multiple samples that provide complementary information about the visual content. It has many applications, such as object tracking [1] [2] [3], self-driving and video surveillance [4]. The fusion task requires algorithms to generate a single image which amalgamates the complementary information conveyed by different source images [5][6] [7].\n\nImage fusion involves three key processes: feature extraction, fusion strategy and reconstruction. Most of the existing fusion research focuses on one or more of these elements to improve the fusion performance. The existing fusion methods can be classified into two categories: traditional algorithms and deep learning-based methods. In the traditional algorithm category, multi-scale transform methods [8] [9][10] [11] are widely applied to extract multi-scale features from the source images. The feature channels are combined by an appropriate fusion strategy. Finally, the fused image is reconstructed by an inverse multiscale transform. Obviously, the fusion performance of these algorithms is highly dependent on the feature extraction method used. * Corresponding author email: wu xiaojun@jiangnan.edu.cn Following this direction, sparse representation (SR) [12] and low-rank representation (LRR) [13] [14] have been applied to extract salient features from the source images. In SR and LRR based fusion methods [15] [16] [17] [18], the sliding window technique is used to decompose source images into image patches. A matrix is constructed using these image patches, in which each column is a reshaped image patch. This matrix is fed into SR (or LRR) to calculate SR (or LRR) coefficients which are considered as image features. By virtue of this operation, the image fusion problem is transformed to the one of coefficient fusion. The fused coefficients are generated by an appropriate fusion strategy and used to reconstruct the fused image in the SR (or LRR) framework. Beside the above, other SR based approaches and other signal processing methods [19] [20] have been suggested in the literature.\n\nAlthough the traditional fusion methods have achieved good fusion performance, they have drawbacks: (1) The fusion performance highly depends on handcrafted features [15] [18] [21], as it is difficult to find a universal feature extraction method for different fusion tasks; (2) Different fusion strategies may be required to work with different features; (3) For SR and LRR based methods, the dictionary learning is very time-consuming; (4) Complex source images pose a challenge for SR (or LRR) based fusion methods.\n\nTo overcome these drawbacks, deep learning based fusion methods have been developed, which can be grouped into three categories according to the three key elements of the fusion pro-cess: deep feature extraction, fusion strategy and end-to-end training. In the feature extraction direction, deep learning methods are utilized to extract deep representation of the information conveyed by the source images [22] [23] [24] [25] [26]. Different fusion strategies have been suggested to reconstruct the fused image. In other fusion methods [27] [28], deep learning is also used to design the fusion strategy. In [27] [28], convolutional sparse representation and convolutional neural network are utilized to generate a decision map for the source images. Using the learned decision map, the fused images are obtained by appropriate post-processing. Although these fusion methods achieve good fusion performance, the fusion strategy and the post-processing are tricky to design. To avoid the limitations of handcrafted solutions, some end-to-end fusion frameworks were presented (FusionGAN [29], FusionGANv2 [30], DDcGAN [31]). These frameworks are based on adversarial learning which avoids the shortcoming of handcrafted features and fusion strategies. However, even the state of the art methods, FusionGANv2 [30] and DDcGAN [31], face challenges to preserve image detail adequately. To preserve more detail background information from visible images, a nest connection based autoencoder fusion network (NestFuse [32]) was proposed. Although NestFuse obtains good performance in detail information preservation, the fusion strategy is still not learnable.\n\nTo address these problems, in this paper, we propose a novel end-to-end fusion framework (RFN-Nest). Our network contains three parts: an encoder network, residual fusion network (RFN) which is designed to extract fused multi-scale deep features, and a decoder network based on nest connection [33]. Although the encoder and decoder architecture of the proposed network is similar to the NestFuse [32], the fusion strategy, the training strategy and the loss function are totally different.\n\nFirstly, instead of fusing handcrafted features in NestFuse [32], several simple yet efficient learnable fusion networks (RFN) have been designed and inserted into the autoencoder architecture. With the RFN, the autoencoder-based fusion network is upgraded to an end-to-end fusion network. Secondly, as RFN is a learnable structure, it is important that the encoder and decoder exhibit powerful feature extraction and feature reconstruction abilities, respectively. Thus, we develop a two-stage training strategy to train our fusion network (encoder, decoder and RFN networks). Thirdly, to train the proposed RFN networks, we design a new loss function (L RFN ) to preserve the detail information from visible image and maintain the salient features from infrared image, simultaneously.\n\nThe main contributions of RFN-Nest can be summarized as follows,\n\n(1) A novel residual fusion network(RFN) is proposed to supersede handcrafted fusion strategies. Although many methods [22] [24][34] [25] now use deep features to achieve good performance, the heuristic approach to selecting a suitable fusion strategy is their weakness. The proposed RFN is a learnable fusion network that overcomes this weakness.\n\n(2) A two-stage training strategy is developed to design our network. The feature extraction and feature reconstruction abilities are the key for the encoder and decoder networks. Us-ing only one stage training strategy to simultaneously train the whole network (encoder, decoder and RFN networks) is insufficient. Inspired by [25], firstly, the encoder and the decoder network are trained as an auto-encoder. With the fixed encoder and decoder, the RFN networks are trained using an appropriate loss function.\n\n(3) A loss function capable of preserving the image detail, together with a feature enhancing loss function are designed to train our RFN networks. We show that with these loss functions, more detail information and image salient features are preserved in the fused image.\n\n(4) We show that, compared with the state-of-the-art fusion methods, the proposed RFN-Nest framework exhibits better fusion performance on public datasets in both subjective visual assessment and objective evaluation.\n\nThe rest of our paper is structured as follows. In Section 2, we briefly review the related work on deep learning-based fusion. The proposed fusion framework is described in detail in Section 3. The experimental results are presented in Section 4 and Section 5. Finally, we draw the paper to conclusion in Section 6.\n\n\nRelated Works\n\nRecently, many deep learning methods have been developed for image fusion. Most of them are based on convolutional neural networks (CNN). These methods can be classified into the non end-to-end learning and end-to-end learning categories. In this section, we briefly overview the most representative deep learning based methods from these two categories.\n\n\nNon End-to-end Methods\n\nIn the early days, deep learning neural networks were used to extract deep features as a bank of \"decision\" maps [22][23] [24]. In [22], Li et al. proposed a fusion framework based on a pre-trained network (VGG-19 [35]). Firstly, the source images are decomposed into salient parts (texture and edges) and base parts (contour and luminance). Then, VGG-19 is used to extract multi-level deep features from the salient parts. At each level, the decision maps are computed from the deep features and a candidate fused salient part is generated. The fused image is reconstructed by combining the fused base parts and the fused salient parts using an appropriate fusion strategy. In [23], the pre-trained ResNet-50 [36] is utilized to extract deep features from the source images directly. A decision map is obtained by zero-phase component analysis(ZCA) and l 1 -norm. The PCANet-based fusion method [24] also follows this framework to generate the fused image, in which PCANet, instead of VGG-19 or ResNet-50, is used to extract the features.\n\nIn addition to pure feature extraction, in [27][28], the two key processes (feature extraction and fusion strategy) are implemented by a single network. In [28], a decision map is generated by a CNN trained on image patches of multiple blurred versions of the input image. In [27], the convolutional sparse representation instead of CNN is utilized to extract features and to generate a decision map. From the generated decision map, the fused image can easily be reconstructed.  Besides the above methods, a deep auto-encoder network based fusion framework was proposed in [25]. Inspired by DeepFuse [34], the authors proposed a novel network architecture which contains an encoder, a fusion layer and a decoder. The dense block [37] based encoder network was adopted as it extracts more complementary deep features from the source images. In their framework, the fusion strategy becomes very important.\n\nInspired by DenseFuse [25] and the architecture in [33], Li et al. proposed NestFuse [32] to preserve more detail background information from visible images, while enhancing the salient features in infrared images. Additionally, a novel spatial/channel attention models are designed to fuse the multiscale deep features. Although these frameworks achieve good fusion performance, it is very difficult to find an effective handcrafted fusion strategy for image fusion.\n\n\nEnd-to-end Methods\n\nTo eliminate the arbitrariness of handcrafted features and fusion strategies, several end-to-end fusion frameworks have been suggested [29] [30] [38] [39] [31] [40].\n\nIn [29], a GAN-based fusion framework (FusionGAN) was introduced to the infrared and visible image fusion field. The generator network is the engine which computes the fused image, while the discriminator network constrains the fused image to contain the detail information from the visible image. The loss function has two terms: content loss and discriminator loss. Due to the content loss, the fused image tends to become similar to the infrared image, failing to preserve the image detail, in spite of the discriminator network.\n\nTo preserve more detail information from the visible images, the authors of [30] proposed a new version of Fusion-GAN which was named FusionGANv2. In this new version, the authors deepen the generator and discriminator networks, endowing them with more powerful feature representation ability. In addition, two new loss functions, namely detail loss and target edge-enhancement loss, were presented to preserve the detail information. With these improvements, the fused images reconstructed more scene details, with clearly highlighted edgesharpened targets.\n\nA general end-to-end image fusion network (IFCNN) [38] was also proposed, which is a simple yet effective fusion method. In IFCNN, two convolutional layers are utilized to extract deep features from the source images. Elementwise fusion rules (elementwise-maximum, elementwise-sum, elementwise-mean) are used to fuse the deep features. The fused image is generated from the fused deep features by two convolutional layers. Although IFCNN achieves a satisfactory fusion performance in multiple image fusion tasks, its architecture is too simplistic to extract powerful deep features, and the fusion strategies designed using a traditional way are not optimal.\n\n\nThe Proposed Fusion Framework\n\nThe proposed fusion network is introduced in this section. Firstly, the architecture of our network is presented in Section 3.1. The advocated two-stage training strategy is described in Section 3.2.\n\n\nThe Architecture of the Fusion Network\n\nThe RFN-Nest is an end-to-end fusion network, the architecture of which is shown in Fig.1. RFN-Nest contains three parts: encoder (left part), residual fusion network (RFN 1\u22124 ) and decoder (right part). For a convolutional layer, \"k \u00d7 k, (in, out)\" means the kernel size is k \u00d7 k, input channel is in and output channel is out.\n\nWith the max pooling operation in the encoder network, multi-scale deep features can be extracted from the source images. The RFN is utilized to fuse multi-modal deep features extracted at each scale. While the shallow layer features preserve more detail information, the deeper layer features convey semantic information, which is important for reconstructing the salient features. Finally, the fused image is reconstructed by the nest connection-based decoder network, which fully exploits the multi-scale structure of the features.\n\nAs shown in Fig.1, I ir and I vi indicate the source images (infrared image and visible image). O denotes the output of RFN-Nest, that is the fused image. \"RFN m \" means one residual fusion network for deep features at scale m. The architecture of the encoder in our framework is constituted by four RFN networks, m \u2208 {1, 2, 3, 4}. These RFN networks share the same architecture but with different weights.\n\nWe now introduce the RFN and the decoder in detail.\n\n\nResidual Fusion Network (RFN)\n\nThe RFN is based on the concept of residual block [36] which has been adapted to the task of image fusion. The RFN architecture is shown in Fig.2. In Fig.2, \u03a6 m ir and \u03a6 m vi indicate the m-th scale deep features extracted by the encoder network, with m \u2208 {1, 2, 3, 4} indicating the index of the RFN network. \"Conv1-6\" denote six convolutional layers in RFN. In this residual architecture, the outputs of \"Conv1\" and \"Conv2\" are concatenated as the input of \"Conv3\". \"Conv6\" is the first fusion layer to generate initial fused features. With this architecture, RFN can easily be optimized by our training strategy. The convolutional operations produce the fused deep features \u03a6 m f , which are fed into the decoder network.\n\nThanks to the multi-scale deep features and the proposed learning process, both the image detail and salient structures are preserved by the shallow RFN networks and deep RFN networks, respectively.\n\n\nDecoder Network\n\nThe decoder network based on the nest connection architecture is shown in Fig.3. Compared to UNet++ [33], regarding the image fusion task, we simplify the network architecture to make it light yet effective to reconstruct fused images, this architecture was also utilized in [32]. \u03a6 m f (m \u2208 {1, 2, 3, 4}) denote the fused multi-scale features obtained by the RFN networks. \"DCB\" indicates a decoder convolutional block, which has two convolutional layers. In each row, these blocks are connected by short connections which are similar to the dense block architecture [37]. The cross-layer links connect multi-scale deep features in the decoder network.\n\nThe output of the network is the fused image reconstructed from the fused multi-scale features.\n\n\nTwo-stage Training Strategy\n\nNote that the ability of the encoder in our network to perform feature extraction and that of the decoder to conduct feature reconstruction are absolutely crucial for successful operation. Accordingly, we develop a two-stage training strategy to make sure that each part in our network can achieve the expected performance.\n\nFirstly, the encoder and the decoder are trained as an autoencoder network to reconstruct the input image. After learning the encoder and decoder networks, in the second training stage, several RFN networks are trained to fuse the multi-scale deep features.\n\nIn this section, a novel two-stage training strategy is introduced in detail.\n\n\nTraining of the Auto-encoder Network\n\nInspired by DenseFuse [25], in the first stage, the encoder network is trained to extract multi-scale deep features. The decoder network is trained to reconstruct the input image with multi-scale deep features. The auto-encoder network training framework is shown in Fig.4. In Fig.4, Input and Output denote the input image and the output image (both indicate one image), respectively. In contrast to [34][25] [29][30], our feature extraction part includes a down sampling operation (max pooling), which extracts deep features at four scales. These multi-scale deep features are fed into the decoder network to reconstruct the input image. With short cross-layer connections, the multi-scale deep features are fully used to reconstruct the input image.\n\n\nDecoder Encoder\n\nThe auto-encoder network is trained using the loss function, L auto defined as follows,\nL auto = L pixel + \u03bbL ssim(1)\nwhere L pixel and L ssim denote the pixel loss and the structure similarity (SSIM) loss between the input image (Input) and the output image (Output). \u03bb is the trade-off parameter between L pixel and L ssim . The pixel loss (L pixel ) is calculated by Eq.2,\nL pixel = ||Output \u2212 Input|| 2 F(2)\nwhere the || \u00b7 || F is the Frobenius norm. L pixel constrains the reconstructed image to be like the input image at the pixel level. The SSIM loss (L ssim ) is defined as,\nL ssim = 1 \u2212 S S I M(Output, Input)(3)\nwhere S S I M(\u00b7) 1 is the structural similarity measure [41] which quantifies the structural similarity of the two images. The structural similarity between Input and Output is constrained by L ssim .\n\n\nTraining of the RFN\n\nThe RFN is proposed to implement a fully learnable fusion strategy. In the second stage, with the encoder and decoder fixed, the RFN is trained with an appropriate loss function. The training process is shown in Fig.5. To train our RFN, we propose a novel loss function L RFN , which is defined as,\nL RFN = \u03b1L detail + L f eature(4)\nwhere L detail and L f eature indicate the background detail preservation loss function and the target feature enhancement loss function, respectively. \u03b1 is a trade-off parameter.\n\nIn the case of infrared and visible image fusion, most of the background detail information comes from the visible image. L detail aims to preserve the detail information and structural features from visible image, which is defined as\nL detail = 1 \u2212 S S I M(O, I vi )(5)\nAs the infrared image contains more salient target features than the visible image, the loss function L f eature is designed to constrain the fused deep features so as to preserve the salient structures. The L f eature is defined as,\nL f eature = M m=1 w 1 (m)||\u03a6 m f \u2212 (w vi \u03a6 m vi + w ir \u03a6 m ir )|| 2 F(6)\nIn Eq.6, M is the number of the multi-scale deep features, which is set to 4. Owing to the magnitude difference between the scales, w 1 is a trade-off parameter vector for balancing the loss magnitudes. It assumes four values {1, 10, 100, 1000}. w vi and w ir control the relative influence of the visible and infrared features in the fused feature map \u03a6 m f . As the visible information is constrained by L detail and the aim of L f eature is to preserve salient features from the infrared image, in Eq.6, w ir is usually greater than w vi . \n\n\nExperimental Validation\n\nIn this section, we conduct an experimental validation of the proposed fusion method. After detailing the experimental settings in the training phase and the test phase, we present several ablation studies to investigate the effect of different elements of the proposed fusion network. Finally, we compare our fusion framework with other existing algorithms qualitatively. For this purpose, we use several performance metrics to evaluate the fusion performance objectively.\n\nOur network is implemented on the NVIDIA TITAN Xp GPU using PyTorch as a programming environment.\n\n\nExperimental Settings in the Training Phase\n\nIn this section, we introduce the training datasets used in our two-stage training strategy.\n\nIn the first stage, we use the dataset MS-COCO [42] to train our auto-encoder network. 80000 images are chosen to constitute the training set. These images are converted to gray scale and reshaped to 256 \u00d7 256. In Eq. 1, the parameter \u03bb is set to 100 to balance the magnitude difference between L pixel and For the second training stage, we choose the KAIST [43] dataset to train our RFN networks. It contains almost 90000 pairs of images. In this dataset, 80000 pairs of infrared and visible images are chosen for training. These images are also converted to gray scale and resized to 256 \u00d7 256. The batch size and epoch are set to 4 and 2, respectively. The learning rate was also set to 1 \u00d7 10 \u22124 , as in the first stage.\n\n\nExperimental Settings in the Test Phase\n\nOur test images come from two datasets which were collected from TNO [44] and VOT2020-RGBT [45]. These images are available at [46]. Some samples of these images are shown in Fig.6. The first dataset contains 21 pairs of infrared and visible images collected from TNO. The second dataset contains 40 pairs of infrared and visible images, which were collected from TNO and VOT2020-RGBT.\n\nWe use six quality metrics 2 to evaluate our fusion algorithm objectively. These include: entropy(En) [47]; standard deviation (S D) [48]; mutual information(MI) [49]; modified fusion artifacts measure(N ab f ) [50], which evaluates the noise information in fused images; the sum of the correlations of differences(S CD) [51]; and the multi-scale structural similarity(MS-SSIM) [52]. The fusion performance improves with the increasing numerical index of all these seven metrics.\n\n\nAblation Study for L detail and L f eature\n\nIn this section, we discuss the effect of L detail and tune the parameters in L f eature . Then, we investigate the impact of the relative weights of the visible and infrared features on the fusion performance.\n\nOnce the auto-encoder network is trained in the first stage, the parameters of the encoder and decoder are fixed and we use L RFN to train four RFN networks. As discussed in Section 3.2.2, due to the magnitude difference between L detail and L f eature , the value of the parameter \u03b1 should be large. Furthermore, the role of L detail is to preserve the detail information from the visible image. Based on the above considerations, in this experiment, \u03b1 is set to 0 and 700 to analyze its influence on our network.\n\nIn L f eature , w 1 is a trade-off vector to balance the L f eature values between the scales. To preserve the salient features from the infrared image, w vi and w ir should be set appropriately. In view of the role of L detail , w vi should be relatively small to reduce any redundancy in reconstructing the image detail information.\n\nIn contrast, w ir should be large to preserve the complementary salient features in the infrared image. However, if w vi is set to 0, which constrains the fused features to mirror the infrared features, the network fails to converge due to the conflicting constraints of L detail and L f eature . So, in our experiment, w vi is set to a non-zero value.\n\nAs different combination of w vi and w ir can lead to different fusion results, we analyze the influence of these two parameters for different values from the range of [0.5, 6.0].\n\nFirstly, when \u03b1 = 0, which means only L f eature is utilized to train RFN networks, some of the fusion results with different w vi and w ir are shown in Fig.7.\n\nIn Fig.7, when w ir is small, the fused images are similar to the visible image and the salient features in the infrared images are suppressed (as shown in first two rows). On the contrary, when w ir is large (greater than 3.0), the salient features in the infrared image are retained. In contrast, the detail information in the visible image is not preserved.\n\nTo capture both types of information, for \u03b1 = 0, we choose  Fig.7) to perform the objective evaluation. The evaluation metrics for different w vi and w ir are presented in Table 1. The best values are indicated in bold.\n\nWhen \u03b1 = 700, the detail information is preserved by L detail . The aim of L f eature is to promote the salient features conveyed by the source images. Accordingly, the values of w vi must be smaller than w ir . We choose different combinations of w vi and w ir (the red boxes in Fig.7) to find the best values of w vi and w ir . The fusion results obtained with (\u03b1 = 700) or without (\u03b1 = 0) L detail in the same combinations of w vi and w ir are shown in Fig.8.\n\nIn Fig.8 (right part), the fusion results in the red boxes contain more detail information from the source images, yet the infrared features are still maintained. Compared with the left part, the fusion results on the right (red boxes) evidently preserve more detail information. When \u03b1 = 700, the objective values for different parameters (Fig.8 (right part), red boxes) are also presented in Table 1.\n\nFrom Fig.7 and Table 1, the different values of \u03b1, w vi and w ir have a significant influence on the results. If the detail preservation loss function (L detail ) is not used (\u03b1 = 0) in the training phase, the proposed fusion network fails to obtain acceptable fusion results. Although the fusion performance appears to be comparable in subjective evaluation (Fig.7, yellow and pink boxes), the subjective and objective assessments indicates a notable degradation compared with the optimal parameter combination (\u03b1 = 700, w ir = 6.0 and w vi = 3.0).\n\nWhen the detail preservation loss function (L detail ) is switched on \u03b1 = 700, our RFN-Nest fusion network scores the comparable metrics values of six metrics with w ir = 6.0 and w vi = 3.0. Based on this analysis, we set w ir = 6.0 and w vi = 3.0 in our next experiments.\n\nIn next section, we will analyze the impact of parameter \u03b1 in our loss function.\n\n\nAblation Study for \u03b1 in L RFN\n\nAs discussed in Section 4.3, when the detail preserving loss function L detail is discarded (\u03b1 = 0), both the subjective and objectively measured fusion performance will be poor. It is evident from from Fig.8 and Table 1 that our fusion network can achieve better fusion performance when \u03b1 is not 0. Thus, choosing an optimal value of \u03b1 becomes an important issue.\n\nIn our study, the parameters of w vi and w ir are set to 3.0 and 6.0, respectively. w 1 is set to {1, 10, 100, 1000} to balance the discrepancy in the orders of magnitude of different scales. To find the optimal \u03b1, we set it to {10, 100, 200, 500, 700, 1000} and compute the results.\n\nSome examples of the fusion results are sown in Fig.9. With the increase \u03b1 (1000), the salient features (man in the yellow box) are not clear, even suppressed, which makes the fused image similar to the visible image. When \u03b1 is set to 500 and 700, the fusion results contain more detail information and the salient features are also maintained.\n\nBased on these observations, we objectively evaluate our fusion method with \u03b1 set to 10, 100, 200, 500, 700, 1000. The metrics values of the fusion results with different \u03b1 are shown in Table 2. The best values are indicated in bold.\n\nAs shown in Fig.9 and Table 2, when \u03b1 is 700, the proposed fusion network achieves better fusion performance in both subjective and objective evaluation. In Table 2, the proposed network scores best in four out of seven metrics with \u03b1 = 700. Thus, in our next experiments, the parameter \u03b1 is set to 700.  \n\n\nAblation Study for Training Strategy\n\nThe proposed two-stage training strategy is a critical operation in our training phase. In this section, we discuss why this strategy is effective, and show its relative merits compared to the one-stage strategy.\n\nOne-stage training strategy means the encoder, RFN and decoder are trained, simultaneously. The training framework is shown in Fig.10, where both the encoder and the decoder are free to adapt their weights. The loss function and the parameter settings are the same as L RFN , which means \u03b1 = 700, w 1 = {1, 10, 100, 1000}, w vi = 3.0 and w ir = 6.0. The fusion results obtained by these two training strategies are shown in Fig.11. Figure 10: The training framework for the one-stage training strategy.\n\n\nEncoder Decoder\n\nIn Fig.11 (c), the visible spectrum detail information is enhanced with the one-stage training strategy. However, the salient objects in infrared image are lost. The premise of image fusion is not realised. In contrast, the two-stage training strategy (Fig.11, d) enables the fused image to preserve the salient infrared objects and contain more detail information from visible images.\n\nThe reason is that the encoder and the decoder may not have the desirable feature extraction and reconstruction ability when designed using the one-stage training strategy. More importantly, as the RFN is the key in our fusion network, it should be trained carefully to obtain good fusion performance.\n\nIn conclusion, we use the two-stage training strategy to train our fusion network. In the first training stage, the encoder is trained to extract powerful multi-scale deep features, to be used by the decoder for image reconstruction. In the second stage, with the fixed encoder and decoder, the RFN networks are trained to fuse the multi-scale deep features, to enhance the detail information from the visible spectrum image and to preserve salient features from the infrared source image.\n\n\nAblation Study for Nest Connection in Decoder\n\nIn this section, we discuss the influence of the nest connection in the decoder. Fig.12 shows the decoder network structure without nest connection (remove the short connections between \"DCB\"). We train this new decoder architecture with the same training strategy and the same loss functions as discussed in Section 3.2.\n\nThe values of seven quality metrics are shown in Table 3. The \"No-nest\" denotes the decoder without nest-connection archi- Table 3: The quality metrics values of two ablation studies. \"No-nest\" indicates the decoder without nest-connection architecture. \"Encoder & Decoder\" denotes that the encoder and the decoder are fixed, the fusion strategy is \"add\", \"max\", \"l 1 -norm\", \"l * -norm\" or \"SCA\". \"RFN-Nest\" means the proposed fusion network.\n\nEn [47] S D [48] MI [49] N ab f [50] S CD [51] MS-SSIM [52]   tecture. The best values, the second-best values and the thirdbest values are indicated in bold, red and italic and blue and italic, respectively. Compared with \"No-nest\", the RFN-Nest (the decoder with nest connection) obtains one best metrics value, three secondbest metrics values and one third-best metrics value. This indicates that the nest connection architecture plays an important role in boosting the reconstruction ability of the decoder network. With the nest connection, the decoder is able to preserve more image information conveyed by the multiscale deep features (MI, FF MI dct , FF MIw) and generate more natural and clearer fused image (EN, S D, V IF).\n\n\nAblation Study for Fusion Strategy\n\nIn this section, we analyze the importance of RFN as an adaptive fusion mechanism in our fusion network. We choose five classical handcrafted fusion strategies (\"add\", \"max\", \"l 1norm\", \"l * -norm\" and \"SCA\") which are used in existing fusion networks [25][38] [32] to do the experiments.\n\nThe trained encoder and decoder are utilized to extract the multi-scale deep features and generate the final image from the fused features, respectively.\n\nLet  Table 4.\n\n\"add\", means the fused features are obtained by adding the source features, directly. In \"max\" strategy, max(\u00b7) denotes an \n\n\nFusion Strategy\n\nFormula add [25] \u03a6 [32] Spatial and channel attention fusion strategy element wise choose-max strategy [38]. For the \"l 1 -norm\" strategy, l 1 (\u00b7), the weights are calculated based on l 1 -norm. For details on how to calculate these weights, please refer to [25]. For \"l * -norm\" (known as nuclear-norm), l * (\u00b7) calculates the sum of singular values of a matrix involved in the global pooling operation of deep features to obtain the fusion weights.\nm f = \u03a6 m ir + \u03a6 m vi max[38] \u03a6 m f = max(\u03a6 m ir , \u03a6 m vi ) l 1 -norm[25] \u03a6 m f = l 1 (\u03a6 m ir , \u03a6 m vi ) l * -norm[32] \u03a6 m f = l * (\u03a6 m ir , \u03a6 m vi ) SCA\nThe \"SCA\" indicates the Spatial/Channel Attention fusion strategy which was utilized in NestFuse [32]. In this experiment 3 , the l 1 -norm is used to do the spatial attention fusion and the average pooling is utilized to calculate the channel attention.\n\nSome examples of the images fused using different fusion strategies are shown in Fig.13. Compared with other handcrafted fusion strategies, the fused image obtained by the RFNbased network preserves more detail information from visible image (blue boxes) and the fused image contains less artefacts (red boxes).\n\nThe results of fusing 21 pairs of infrared and visible images have been evaluated in terms of the seven quality metrics. The metrics values are shown in Table 3. The table also reports the results obtained with other fusion strategies. The RFN-based network (RFN-Nest) achieves five best values. This indicates that when the learnable fusion network is used as a fusion strategy, the detail image information will be boosted (En, S D) thanks to the proposed loss function. Regarding feature preservation, the proposed strategy still obtains three best values (MI, FF MI dct and FF MI w ) and two comparable results (S S I M a and V IF).\n\nIn Section 5, we adopt this learnable fusion network (RFN) for the object tracking task to illustrate the effectiveness of RFN-based fusion strategy in other vision task.\n\n\nInfrared image\n\nVisible image ) \"add\" ) \"max\"\n) \" 1 -norm\" ) RFN-Nest ) \" * -norm\"\n) \"SCA\" \n\n\nFusion Results Analysis on 21 pairs Images\n\nTo compare the fusion performance of the proposed method with the state-of-the-art algorithms, eleven representative fusion methods are chosen, including discrete cosine harmonic wavelet transform(DCHWT) [50], gradient transfer and total variation minimization(GTF) [53], convolutional sparse representation(ConvSR) [27], multi-layer deep features fusion method(VggML) [22], DenseFuse [25], FusionGAN [29], IFCNN [38] (elementwise-maximum), NestFuse [32], PMGI [39], DDcGAN [31] and U2Fusion [40].\n\nFor DenseFuse, we choose the sum strategy and set the tradeoff parameter to 1 \u00d7 10 2 . For NestFuse, the average pooling is utilized for the channel attention fusion strategy. All these fusion methods are implemented using publicly available codes, and their parameters are set by referring to their original reports.\n\nTo evaluate the visual effects of the fusion results 4 , two pairs of visible and infrared images are selected, namely \"man\" and \"umbrella\". The fused images obtained by the existing methods and our fusion method (RFN-Nest) are shown in Fig.14 and Fig.15, respectively.\n\nIn Fig.14 and Fig.15, the fused images obtained by DCHWT are more noisy and contain image artefacts. The fused images obtained by GTF and FusionGAN exhibit clearer features and more detailed background information. Although these fused images retain more complementary information, they look more like the infrared image, especially in the background. In view of the importance of the background information, ConvSR, VggML, DenseFuse, IFCNN, NestFuse, PMGI and U2Fusion are designed to preserve more detail information from the visible image. These methods appear to reduce the salient infrared features, compared with GTF and FusionGAN, producing acceptable fusion results. Although DDcGAN is also 4 More experimental results are shown in our supplementary material. designed to maintain more detail information from visible images, in Fig.14 (l), it injects more noise into the fused image and the infrared targets are blurred.\n\nAlthough the target features are not enhanced too much in the fused image, the contrast is better than in the visible image. Moreover, for the detail preservation, in Fig.14, compared with the other fusion methods, the 'tree' and 'street lamp' (red box) are clearer in the fused image obtained by our proposed method. The detail textures of 'bushes' (green box) are also preserved into the fused image.\n\nIn Fig.15, in the green box, many fusion methods are unable to preserve the salient features of 'pavilion' from the visible image except IFCNN, NestFuse and the proposed method, which means these fusion methods fuse too much background information from the infrared image. Compared with all these fusion methods, in the red box, the detail information of the 'tree' reconstructed by the proposed method is clearer in the fused image ( Fig.15 (n)).\n\nThe background and the context around salient parts are not very clear and sometime even invisible because of the difficulty in extracting salient features from source images, as shown in Fig.14 and Fig.15. This drawback will cause a performance degradation when the image fusion algorithms are used in other computer vision tasks, such as RGB-T visual object tracking. In contrast, our RFN-Nest fusion network is able to preserve more detail information and to maintain the contrast of infrared parts.\n\nCompared with all the above fusion methods, the fused image obtained by the proposed method appears to retain a better balance between the visible background information and the infrared features.\n\nWe evaluate the fusion performance objectively using the seven quality metrics to compare the seventeen existing fusion methods and our proposed fusion framework. The values of these metrics averaged over all fused images are shown in Ta-    why DDcGAN obtains larger values of En, S D and MI is that DDcGAN introduces more noise and artefacts into the fused image. Our fusion network achieves good fusion performance, producing sharper content and exhibiting more visual information fidelity.\n\n\nFurther Analysis on 40 Pairs Images\n\nThe previous ablation studies and experiments are conducted on one test dataset which contains 21 pairs of infrared and visible images. To verify the generalization performance of the proposed fusion network, a new test dataset is created. It contains 40 pairs of infrared and visible images which are collected from TNO [44] and VOT2020-RGBT [45].\n\nIn this section, we choose several state-of-the-art deep learning based fusion methods to perform comparative experiments. These methods include DenseFuse [25] which is a classical autoencoder-based fusion method, NestFuse [32] which has the same backbone (encoder and decoder) with the proposed method, and three latest fusion methods (PMGI [39], DDcGAN [31] and U2Fusion [40]).\n\nAn example of the fused images obtained by these fusion methods and the proposed network is shown in Fig.16. The unnatural textures in the sky are introduced by the infrared image ( Fig.16 (b), red box) should be looked more natural in the fused image. It is observed that DenseFuse, PMGI and the proposed method generate fused images of natural appearance. Moreover, compared with the existing fusion methods, our network also preserves more detail information from both infrared and visible images (Fig.16, the house in yellow box). Note, the DD-cGAN again introduces noise into the fused image and blurs the salient feature content.\n\nThe same six quality metrics are used for comparative evaluation. The average values of these metrics are shown in Table  6. The best values are indicated in bold, the second-best values are denoted in red and italic and the third-best values are denoted in blue and italic.\n\nCompared with the results on the 21 pairs of images, the proposed network exhibits even better performance on the 40 image pairs. The method achieves one best value (S CD), one second-best values (N ab f ) and two third-best values (S D, MS- \n\n\nExperiments on RGBT Object Tracking\n\nOver the past two years, multi-modality object tracking has been of interest in many vision applications. In Vision Object Tracking challenge (VOT) 2019 [54], for the first time, the committee introduced two new sub-challenges (RGBD and RGBT), in which each sequence in the dataset of RGBD or RGBT contains two modalities (RGB image and depth image, RGB image and infrared image) as the input. As we focus on the fusion of infrared and visible images, the RGBT sub-challenge data is used to evaluate the performance of the proposed learnable fusion network (RFN) and the novel loss functions.\n\nIn VOT2020 [45], the video sequences are the same as VOT2019 [54], but a new performance evaluation protocol is introduced for short-term tracker evaluation (includes RGBT sub-challenge). The new protocol avoids tracker-dependent resets and reduces the variance of the performance evaluation measures.\n\nA state-of-the-art siamese-based tracker AFAT [55] is chosen to be the base tracker. In AFAT, a failure-aware system, realized by a Quality Prediction Network (QPN), based on convolutional and LSTM modules was proposed and obtained better tracking performance in many datasets. For RGBT object tracking, the proposed fusion strategy network (RFN) and the proposed loss function are incorporated into AFAT.\n\n\nThe RFN and The Loss Function For RGBT Tracking\n\nAs we discussed, the proposed residual fusion network (RFN) is a learnable fusion strategy. Thus, ideally, when RFN is applied into AFAT [55], it needs a sufficient quantity of data to train the whole model.\n\nHowever, due to the lack of labeled training data, we were forced to simplify the architecture of RFN, by reducing the number of convolutional layers, as shown in Fig.17. In the training phase, we only train the RFN module, the AFAT modules are fixed to reduce the number of learnable parameters 5 . 5 The framework of RFN-base AFAT is shown in our supplementary material. Three RGBT datasets are used to train our RFN module, namely GTOT [56], VT821 [57], VT1000 [58]. These datasets only contain 17.6k frames in total. GTOT is a dataset for RGBT tracking, the VT821 and the VT1000 are built for RGBT salient object detection.\n\nTo train the RFN module, the proposed loss function (L RFN , Section 3.2.2) is used in the AFAT training. As RGBT tracking does not involve image generation, it is inevitable that the background detail preservation loss function (L detail ) needs to be modified to become applicable to the tracking task. L detail is defined as follows,\nL detail = 1 \u2212 S S I M(\u03a6 m f , l 1 (\u03a6 m rgb , \u03a6 m ir ))(7)\nwhere \u03a6 m f denotes the fused deep feature obtained by the RFN module, and l 1 indicates the \"l 1 -norm\" based fusion strategy discussed in Section 4.7. The target feature enhancement loss function (L f eature ) is the same as in Section 3.2.2.\n\n\nThe Tracking Results on VOT-RGBT\n\nThe video sequences in VOT2020-RGBT are the same as in VOT2019-RGBT. Thus, we only present a few tracking results on VOT2020-RGBT in Fig.18. The 'RFN' denotes the RFNbased AFAT.\n\nTo evaluate the tracking performance, three measures [54] were selected: Accuracy (A), Robustness (R) and Expected Average Overlap (EAO). (1) Accuracy denotes the average overlap between the ground truth and the predicted bounding boxes;\n\n(2) Robustness evaluates how many times the tracker loses the \" \" \"crouch\" \"fog6\" \" \" \"woman89\" \" \"crouch\" \"fog6\" \" \" \"woman89\" Figure 18: Some tracking results on VOT2020-RGBT. The frames in first row and second row are RGB frames and infrared frames, respectively. Five pairs of RGB and infrared frames in columns are collected from different video sequences (\"carnotfar\", \"crouch\", \"fog6\", \"twowoman\" and \"woman89\"). AFAT is the base tracker. In these frames, blue boxes denote the ground truth, green boxes and yellow boxes are the tracking results obtained by AFAT with only RGB frames or infrared frames fed as input. Red boxes indicate the tracking results obtained by the RFN-based tracker.\n\ntarget (fails) during tracking (3) EAO is an estimator of the average overlap of a tracker. For the detail of Accuracy, Robustness and EAO please refer to [59].\n\nIn VOT2020-RGBT [45] 6 , for the Accuracy, Robustness and EAO, they have the same meanings but their calculation methods are re-defined by the committee. Thus, these metrics are indicated as EAO new , A new and R new . The higher values of EAO, EAO new , A, A new , R and R new , the better the tracker.\n\nIn addition to the base tracker (AFAT), we choose two further trackers for each dataset (VOT2019, VOT2020) to analyze the tracking performance of RFN-based AFAT. In the VOT2019-RGBT competition, mfDiMP and FSRPN won the third and fourth place on the public dataset, respectively. For VOT2020-RGBT, DFAT and M2C2Frgbt won third and seventh place on the public dataset, respectively. Note that DFAT is the winner on VOT2020-RGBT challenge. All the metrics values are provided by the VOT committee and available on the VOT reports [54] [45].\n\nThe tracking results of RFN-based AFAT and other trackers are shown in Table 7 and Table 8. RGB and In f rared indicate that only one modality (RGB or infrared) is fed into AFAT. From these two tables, compared with just feeding one modality into AFAT, the RFN-based AFAT delivers better tracking performance in all measures both on VOT-RGBT2019 and 6 The toolkit version of VOT2020-RGBT is 0.2.0. on VOT-RGBT2020. On VOT2019-RGBT, although mfDiMP achieves the best performance, the results produced by the RFN-based tracker are comparable (EAO) and the accuracy is better. On VOT2020-RGBT, even compared with the winning tracker, DFAT, our tracker is also competitive. These experiments demonstrate that even with insufficient training data, the tracker performance is improved by incorporating the proposed residual fusion network (RFN) into the AFAT tracking framework. When more training data becomes available, we believe the RFN-based tracker will achieve even better tracking performance.\n\n\nConclusions\n\nMotivated by the weakness of the existing fusion methods in preserving image detail, in this paper, we proposed a novel endto-end fusion framework (RFN-Nest) which is based on the nest connection incorporated into a residual fusion network. To design our RFN-Nest, a two-stage training strategy was presented. In the proposed scheme, an auto-encoder network is trained using the SSIM loss function (L ssim ) and the pixel loss function (L pixel ). The trained encoder is utilized to extract multi-scale features from the source images and the nest connection-based decoder network is designed to reconstruct the fused images using the fused multi-scale features. The key component of RFN-Nest is the residual fusion network (RFN). In the second stage of the training strategy, four residual fusion networks (RFN) are trained to preserve the image detail, and preserve the salient features using L detail and L f eature , respectively. Once the two-stage training is accomplished, the fused image is reconstructed using the encoder, the RFN networks and the decoder. Compared to seventeen existing fusion methods, the RFN-Nest achieves the best fusion performance in both subjective and objective evaluation.\n\nTo validate the generality of the fusion network, we also applied the proposed RFN and the novel loss functions to a state-of-the-art tracker to perform a multimodal tracking task (RGBT tracking). Compared with single modality, the RFNbased tracker delivers better tracking performance in all measures on VOT2019 and VOT2020. Even compared with the state of the art RGBT trackers, the RFN-based tracker achieves very good performance. This demonstrates that with this proposed innovations, the RFN-Nest network has a wide applicability, extending beyond image fusion.\n\nFigure 1 :\n1The framework of RFN-Nest. 'RFN 1\u22124 ' denote the residual fusion network. The nest connection-based decoder network('Decoder') will be introduced later. '3 \u00d7 3,(16,8)' means the kernel size is 3 \u00d7 3, input channel is 16 and output channel is 8 in a convolutional layer.\n\nFigure 2 :\n2The architecture of RFN m .\n\nFigure 3 :\n3The architecture of the decoder.\n\nFigure 4 :\n4The training of the auto-encoder network.\n\nFigure 5 :\n5The training of RFN. In the RFN, inputs \u03a6 m ir and \u03a6 m vi denote infrared and visible deep features, respectively. \u03a6 m f represents the fused deep features obtained by RFN m . In our framework, m \u2208 {1, 2, 3, 4} indicates the scale of deep features. The fixed encoder network is utilized to extract multi-scale deep features (\u03a6 m ir and \u03a6 m vi ) from the source images. For each scale, an RFN is used to fuse these deep features. Then, the fused multi-scale features (\u03a6 m f ) are fed into the fixed decoder network.\n\nFigure 6 :Figure 7 :\n67Five pairs of source images. The top row contains visible images, and the second row contains infrared images. The fusion results obtained with different values of w vi and w ir , when \u03b1 = 0. In L f eature , w 1 \u2208 {1, 10, 100, 1000}.\n\nFigure 8 :\n8The fusion results obtained without (\u03b1 = 0) or with (\u03b1 = 700) L detail . In L f eature , w 1 \u2208 {1, 10, 100, 1000}. L ssim . The batch size and epoch are set to 4 and 2, respectively. The learning rate is set to 1 \u00d7 10 \u22124 .\n\nFigure 9 :\n9The fusion results obtained with different \u03b1.\n\nFigure 11 :\n11The fusion results obtained by one-stage and two-stage training strategy. (a) Visible images; (b) Infrared images; (c) Fused images obtained by one-stage strategy; (d) Fused images obtained by two-stage strategy.\n\nFigure 12 :\n12The decoder network without short connection (\"No-nest\").\n\n\n\u03a6 m ir and \u03a6 m vi denote the multi-scale deep features extracted by the trained encoder from the infrared and visible image, respectively. \u03a6 m f are the fused deep features. m indicates the scale of the deep features. The formulas of these five strategies are shown in\n\nFigure 13 :\n13The fusion results with different fusion strategies. \"RFN-Nest\" means the adaptive RFN networks are utilized in the fusion operation.\n\nFigure 14 :\n14The experimental results on \"man\" images. (a) Visible; (b) Infrared; (c) DCHWT; (d) GTF; (e) ConvSR; (f) VggML; (g) DenseFuse; (h) FusionGAN; (i) IFCNN; (j) NestFuse; (k) PMGI; (l) DDcGAN; (m) U2Fusion; (n) proposed.\n\nFigure 15 :\n15The experimental results on \"umbrella\" images. (a) Visible; (b) Infrared; (c) DCHWT; (d) GTF; (e) ConvSR; (f) VggML; (g) DenseFuse; (h) FusionGAN; (i) IFCNN; (j) NestFuse; (k) PMGI; (l) DDcGAN; (m) U2Fusion; (n) proposed. ble 5. The best values, the second-best values and the third-best values are indicated in bold, red and italic and blue and italic, respectively. From Table 5, the proposed fusion framework (RFN-Nest) obtains one best values (S CD) and three third-best values (S D, N ab f , MS-SSIM) compared to the other methods. The reason\n\nFigure 16 :\n16The experimental results on \"street\" images. (a) Visible; (b) Infrared; (c) DenseFuse; (d) NestFuse; (e) PMGI; (f) DDcGAN; (g) U2Fusion; (h) proposed.\n\nFigure 17 :\n17The RFN architecture for RGBT tracking.\n\nTable 1 :\n1The average values of the objective metrics obtained with different parameters (\u03b1, w vi , w ir ) on 21 pairs of infrared and visible images.\u03b1 \nw ir \nw vi \nEn[47] \nS D[48] \nMI[49] \nN ab f [50] \nS CD[51] \nMS-SSIM[52] \n\n0 \n\n0.5 \n0.5 \n6.71845 \n67.66313 \n13.43690 \n0.09354 \n1.83520 \n0.92903 \n\n2.0 \n\n2.0 \n6.71557 \n67.63524 \n13.43114 \n0.09252 \n1.83495 \n0.92887 \n3.0 \n6.80410 \n74.73724 \n13.60821 \n0.09240 \n1.82712 \n0.92294 \n4.0 \n6.83492 \n79.75125 \n13.66983 \n0.09419 \n1.78649 \n0.90543 \n\n3.0 \n\n3.0 \n6.72263 \n67.83451 \n13.44526 \n0.09339 \n1.83713 \n0.92988 \n4.0 \n6.78738 \n72.45840 \n13.57476 \n0.09230 \n1.83518 \n0.92715 \n5.0 \n6.81292 \n76.36078 \n13.62583 \n0.09324 \n1.81501 \n0.91696 \n\n4.0 \n\n4.0 \n6.72150 \n67.53190 \n13.44299 \n0.09355 \n1.83367 \n0.92842 \n5.0 \n6.77188 \n70.98434 \n13.54376 \n0.09208 \n1.83538 \n0.92775 \n6.0 \n6.80239 \n74.64694 \n13.60478 \n0.09218 \n1.82594 \n0.92263 \n\n5.0 \n5.0 \n6.71684 \n67.48675 \n13.43368 \n0.09218 \n1.83366 \n0.92847 \n6.0 \n6.76875 \n70.35820 \n13.53750 \n0.08944 \n1.83707 \n0.92870 \n6.0 \n6.0 \n6.72585 \n67.82480 \n13.45170 \n0.09209 \n1.83665 \n0.92949 \n\n700 \n\n5.0 \n0.5 \n6.95916 \n91.41847 \n13.9183 \n0.14375 \n1.58717 \n0.84109 \n\n6.0 \n0.5 \n6.79112 \n68.28532 \n13.58224 \n0.07838 \n1.78391 \n0.88602 \n3.0 \n6.84134 \n71.90131 \n13.68269 \n0.07288 \n1.83676 \n0.91456 \n\na middle value (yellow and pink boxes in \n\nTable 2 :\n2The average metrics values of the proposed fusion network with different \u03b1 on 21 pairs of infrared and visible images.\u03b1 \nEn[47] \nS D[48] \nMI[49] \nN ab f [50] \nS CD[51] \nMS-SSIM[52] \n10 \n6.66878 \n62.83593 \n13.33757 \n0.08012 \n1.77192 \n0.87593 \n100 \n6.70939 \n63.77416 \n13.41878 \n0.07129 \n1.79329 \n0.90096 \n200 \n6.75446 \n66.01632 \n13.50893 \n0.06680 \n1.80880 \n0.91177 \n500 \n6.82103 \n70.34117 \n13.64206 \n0.06768 \n1.83252 \n0.91453 \n700 \n6.84134 \n71.90131 \n13.68269 \n0.07288 \n1.83676 \n0.91456 \n\n\n\nTable 4 :\n4The formulas of different fusion strategies.\n\nTable 5 :\n5The values of seven quality metrics averaged over the fused images on 21 pairs of infrared and visible images.En[47] \nS D[48] \nMI[49] \nN ab f [50] \nS CD[51] \nMS-SSIM[52] \nDCHWT[50] \n6.56777 \n64.97891 \n13.13553 \n0.12295 \n1.60993 \n0.84326 \nGTF[53] \n6.63433 \n67.54361 \n13.26865 \n0.07951 \n1.00488 \n0.80844 \nConvSR[27] \n6.25869 \n50.74372 \n12.51737 \n0.01958 \n1.64823 \n0.90281 \nVggML[22] \n6.18260 \n48.15779 \n12.36521 \n0.00120 \n1.63522 \n0.87478 \nDenseFuse[25] \n6.67158 \n67.57282 \n13.34317 \n0.09214 \n1.83502 \n0.92896 \nFusionGan[29] \n6.36285 \n54.35752 \n12.72570 \n0.06706 \n1.45685 \n0.73182 \nIFCNN[38] \n6.59545 \n66.87578 \n13.19090 \n0.17959 \n1.71375 \n0.90527 \nNestFuse[32] \n6.91971 \n82.75242 \n13.83942 \n0.13405 \n1.73353 \n0.86248 \nPMGI[39] \n6.93391 \n71.54806 \n13.86783 \n0.13525 \n1.78242 \n0.88934 \nDDcGAN[31] \n7.47310 \n100.34809 \n14.94620 \n0.33784 \n1.60926 \n0.76636 \nU2Fusion[40] \n6.75708 \n64.91158 \n13.51416 \n0.29088 \n1.79837 \n0.92533 \nproposed \n6.84134 \n71.90131 \n13.68269 \n0.07288 \n1.83676 \n0.91456 \n\n\n\nTable 6 :\n6The values of seven quality metrics averaged over the fused images on 40 pairs of infrared and visible images which collected from TNO and VOT2020. SSIM). Even compared with DDcGAN, our fusion network performance is comparable. This confirms that our fusion network trained by the two-stage fusion strategy and the novel loss function demonstrates better generalization.En[47] \nS D[48] \nMI[49] \nN ab f [50] \nS CD[51] \nMS-SSIM[52] \nDenseFuse[25] \n6.77630 \n73.63462 \n13.55261 \n0.06346 \n1.74862 \n0.92944 \nNestFuse[32] \n6.99347 \n90.28951 \n13.98693 \n0.11138 \n1.67540 \n0.88611 \nPMGI[39] \n6.96974 \n77.25462 \n13.93948 \n0.11434 \n1.68523 \n0.88830 \nDDcGAN[31] \n7.50173 \n106.99113 \n15.00346 \n0.30998 \n1.55359 \n0.78419 \nU2Fusion[40] \n6.94970 \n76.80347 \n13.89939 \n0.28363 \n1.74780 \n0.93141 \nproposed \n6.92952 \n78.22247 \n13.85904 \n0.06357 \n1.76116 \n0.90894 \n\n\n\nTable 7 :\n7The tracking results obtained on the VOT2019-RGBT dataset. AFAT is used as the base tracker to evaluate the RFN network fusion strategy.VOT2019 \nEAO \nA \nR \nFSRPN[54] \n0.3553 \n0.6362 \n0.7069 \nmfDiMP[54] \n0.3879 \n0.6019 \n0.8036 \n\nAFAT[55] \nRGB \n0.32590 \n0.61130 \n0.5700 \nInfrared \n0.18120 \n0.56740 \n0.1800 \nRFN-based \n0.35840 \n0.64470 \n0.6500 \n\n\n\nTable 8 :\n8The tracking results obtained on the VOT2020-RGBT dataset. AFAT is set as the base tracker to evaluate the RFN network fusion strategy.VOT2020 \nEAO new \nA new \nR new \nM2C2Frgbt[45] \n0.332 \n0.636 \n0.722 \nDFAT[45] \n0.390 \n0.672 \n0.779 \n\nAFAT[55] \nRGB \n0.329 \n0.635 \n0.669 \nInfrared \n0.265 \n0.573 \n0.588 \nRFN-based \n0.371 \n0.668 \n0.726 \n\n\nThe definition of S S I M(\u00b7) is introduced in our supplementary material (Section 1).\nThe definitions of these metrics are introduced in our supplementary material.\nThe \"SCA\" fusion strategy is used in NestFuse.\n\nRGB-T object tracking: benchmark and baseline. C Li, X Liang, Y Lu, N Zhao, J Tang, Pattern Recognition. 96106977C. Li, X. Liang, Y. Lu, N. Zhao, J. Tang, RGB-T object tracking: bench- mark and baseline, Pattern Recognition 96 (2019) 106977.\n\nLearning Local-Global Multi-Graph Descriptors for RGB-T Object Tracking. C Li, C Zhu, J Zhang, B Luo, X Wu, J Tang, IEEE Transactions on Circuits and Systems for Video Technology. C. Li, C. Zhu, J. Zhang, B. Luo, X. Wu, J. Tang, Learning Local-Global Multi-Graph Descriptors for RGB-T Object Tracking, IEEE Transactions on Circuits and Systems for Video Technology (2018).\n\nThermal infrared and visible sequences fusion tracking based on a hybrid tracking framework with adaptive weighting scheme. C Luo, B Sun, K Yang, T Lu, W.-C Yeh, Infrared Physics & Technology. 99C. Luo, B. Sun, K. Yang, T. Lu, W.-C. Yeh, Thermal infrared and visi- ble sequences fusion tracking based on a hybrid tracking framework with adaptive weighting scheme, Infrared Physics & Technology 99 (2019) 265-276.\n\nIR and Visible Video Fusion for Surveillance. V Shrinidhi, P Yadav, N Venkateswaran, 2018 International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET). IEEEV. Shrinidhi, P. Yadav, N. Venkateswaran, IR and Visible Video Fusion for Surveillance, in: 2018 International Conference on Wireless Commu- nications, Signal Processing and Networking (WiSPNET), IEEE, 2018, pp. 1-6.\n\nInfrared and visible image fusion methods and applications: A survey. J Ma, Y Ma, C Li, Information Fusion. 45J. Ma, Y. Ma, C. Li, Infrared and visible image fusion methods and ap- plications: A survey, Information Fusion 45 (2019) 153-178.\n\nPixel-level image fusion: A survey of the state of the art. S Li, X Kang, L Fang, J Hu, H Yin, Information Fusion. 33S. Li, X. Kang, L. Fang, J. Hu, H. Yin, Pixel-level image fusion: A survey of the state of the art, Information Fusion 33 (2017) 100-112.\n\nDeep learning for pixel-level image fusion: Recent advances and future prospects. Y Liu, X Chen, Z Wang, Z J Wang, R K Ward, X Wang, Information Fusion. 42Y. Liu, X. Chen, Z. Wang, Z. J. Wang, R. K. Ward, X. Wang, Deep learn- ing for pixel-level image fusion: Recent advances and future prospects, Information Fusion 42 (2018) 158-173.\n\nA wavelet-based image fusion tutorial. G Pajares, J M De La, Cruz, Pattern recognition. 37G. Pajares, J. M. De La Cruz, A wavelet-based image fusion tutorial, Pattern recognition 37 (2004) 1855-1872.\n\nA multiscale approach to pixel-level image fusion. A Ben Hamza, Y He, H Krim, A Willsky, Integrated Computer-Aided Engineering. 12A. Ben Hamza, Y. He, H. Krim, A. Willsky, A multiscale approach to pixel-level image fusion, Integrated Computer-Aided Engineering 12 (2005) 135-146.\n\n. S Yang, M Wang, L Jiao, R Wu, Z Wang, Information Fusion. 11S. Yang, M. Wang, L. Jiao, R. Wu, Z. Wang, Image fusion based on a new contourlet packet, Information Fusion 11 (2010) 78-84.\n\nImage fusion with guided filtering. S Li, X Kang, J Hu, IEEE Transactions on Image processing. 22S. Li, X. Kang, J. Hu, Image fusion with guided filtering, IEEE Transac- tions on Image processing 22 (2013) 2864-2875.\n\nRobust face recognition via sparse representation. J Wright, A Y Yang, A Ganesh, S S Sastry, Y Ma, J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, Y. Ma, Robust face recog- nition via sparse representation, IEEE transactions on pattern analysis and machine intelligence 31 (2008) 210-227.\n\nRobust subspace segmentation by low-rank representation. G Liu, Z Lin, Y Yu, 18G. Liu, Z. Lin, Y. Yu, Robust subspace segmentation by low-rank repre- sentation., in: ICML, volume 1, 2010, p. 8.\n\nRobust recovery of subspace structures by low-rank representation. G Liu, Z Lin, S Yan, J Sun, Y Yu, Y Ma, IEEE transactions. 35G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, Y. Ma, Robust recovery of sub- space structures by low-rank representation, IEEE transactions on pattern analysis and machine intelligence 35 (2012) 171-184.\n\nDictionary learning method for joint sparse representation-based image fusion. Q Zhang, Y Fu, H Li, J Zou, Optical Engineering. 5257006Q. Zhang, Y. Fu, H. Li, J. Zou, Dictionary learning method for joint sparse representation-based image fusion, Optical Engineering 52 (2013) 057006.\n\nInfrared and visible image fusion method based on saliency detection in sparse domain. C Liu, Y Qi, W Ding, Infrared Physics & Technology. 83C. Liu, Y. Qi, W. Ding, Infrared and visible image fusion method based on saliency detection in sparse domain, Infrared Physics & Technology 83 (2017) 94-102.\n\nImage fusion with cosparse analysis operator. R Gao, S A Vorobyov, H Zhao, IEEE Signal Processing Letters. 24R. Gao, S. A. Vorobyov, H. Zhao, Image fusion with cosparse analysis operator, IEEE Signal Processing Letters 24 (2017) 943-947.\n\nMulti-focus image fusion using dictionary learning and low-rank representation. H Li, X.-J Wu, International Conference on Image and Graphics. Cham, SwitzerlandSpringerH. Li, X.-J. Wu, Multi-focus image fusion using dictionary learning and low-rank representation, in: International Conference on Image and Graphics, Cham, Switzerland: Springer, 2017, pp. 675-686.\n\nThe infrared and visible image fusion algorithm based on target separation and sparse representation. X Lu, B Zhang, Y Zhao, H Liu, H Pei, Infrared Physics & Technology. 67X. Lu, B. Zhang, Y. Zhao, H. Liu, H. Pei, The infrared and visible im- age fusion algorithm based on target separation and sparse representation, Infrared Physics & Technology 67 (2014) 397-407.\n\nA novel infrared and visible image fusion algorithm based on shift-invariant dual-tree complex shearlet transform and sparse representation. M Yin, P Duan, W Liu, X Liang, Neurocomputing. 226M. Yin, P. Duan, W. Liu, X. Liang, A novel infrared and visible image fusion algorithm based on shift-invariant dual-tree complex shearlet trans- form and sparse representation, Neurocomputing 226 (2017) 182-191.\n\nInfrared and visible image fusion method based on saliency detection in sparse domain. C Liu, Y Qi, W Ding, Infrared Physics & Technology. 83C. Liu, Y. Qi, W. Ding, Infrared and visible image fusion method based on saliency detection in sparse domain, Infrared Physics & Technology 83 (2017) 94-102.\n\nInfrared and Visible Image Fusion using a Deep Learning Framework. H Li, X.-J Wu, J Kittler, 24th International Conference on Pattern Recognition (ICPR). IEEEH. Li, X.-J. Wu, J. Kittler, Infrared and Visible Image Fusion using a Deep Learning Framework, in: 2018 24th International Conference on Pattern Recognition (ICPR), IEEE, 2018, pp. 2705-2710.\n\nInfrared and Visible Image Fusion with ResNet and zero-phase component analysis. H Li, X.-J Wu, T S Durrani, Infrared Physics & Technology. 102103039H. Li, X.-J. Wu, T. S. Durrani, Infrared and Visible Image Fusion with ResNet and zero-phase component analysis, Infrared Physics & Technol- ogy 102 (2019) 103039.\n\nMulti-focus Image Fusion with PCA Filters of PCANet. X Song, X.-J Wu, IAPR Workshop on Multimodal Pattern Recognition of Social Signals in Human-Computer Interaction. SpringerX. Song, X.-J. Wu, Multi-focus Image Fusion with PCA Filters of PCANet, in: IAPR Workshop on Multimodal Pattern Recognition of So- cial Signals in Human-Computer Interaction, Springer, 2018, pp. 1-17.\n\nH Li, X.-J Wu, DenseFuse: A Fusion Approach to Infrared and Visible Images. 28H. Li, X.-J. Wu, DenseFuse: A Fusion Approach to Infrared and Visible Images, IEEE Transactions on Image Processing 28 (2019) 2614-2623.\n\nMDLatLRR: A novel decomposition method for infrared and visible image fusion. H Li, X.-J Wu, J Kittler, Doi:10.1109/TIP.2020.2975984IEEE Transactions on Image Processing. H. Li, X.-J. Wu, J. Kittler, MDLatLRR: A novel decomposition method for infrared and visible image fusion, IEEE Transactions on Image Pro- cessing (2020). Doi: 10.1109/TIP.2020.2975984.\n\nImage fusion with convolutional sparse representation. Y Liu, X Chen, R K Ward, Z J Wang, IEEE signal processing letters. 23Y. Liu, X. Chen, R. K. Ward, Z. J. Wang, Image fusion with convolutional sparse representation, IEEE signal processing letters 23 (2016) 1882- 1886.\n\nMulti-focus image fusion with a deep convolutional neural network. Y Liu, X Chen, H Peng, Z Wang, Information Fusion. 36Y. Liu, X. Chen, H. Peng, Z. Wang, Multi-focus image fusion with a deep convolutional neural network, Information Fusion 36 (2017) 191-207.\n\nFusionGAN: A generative adversarial network for infrared and visible image fusion. J Ma, W Yu, P Liang, C Li, J Jiang, Information Fusion. 48J. Ma, W. Yu, P. Liang, C. Li, J. Jiang, FusionGAN: A generative adver- sarial network for infrared and visible image fusion, Information Fusion 48 (2019) 11-26.\n\nInfrared and visible image fusion via detail preserving adversarial learning. J Ma, P Liang, W Yu, C Chen, X Guo, J Wu, J Jiang, Information Fusion. 54J. Ma, P. Liang, W. Yu, C. Chen, X. Guo, J. Wu, J. Jiang, Infrared and vis- ible image fusion via detail preserving adversarial learning, Information Fusion 54 (2020) 85-98.\n\nDDcGAN: A Dual-Discriminator Conditional Generative Adversarial Network for Multi-Resolution Image Fusion. J Ma, H Xu, J Jiang, X Mei, X.-P Zhang, IEEE Transactions on Image Processing. 29J. Ma, H. Xu, J. Jiang, X. Mei, X.-P. Zhang, DDcGAN: A Dual- Discriminator Conditional Generative Adversarial Network for Multi- Resolution Image Fusion, IEEE Transactions on Image Processing 29 (2020) 4980-4995.\n\nNestFuse: An Infrared and Visible Image Fusion Architecture based on Nest Connection and Spatial/Channel Attention Models. H Li, X.-J Wu, T Durrani, Doi:10.1109/TIM.2020.3005230IEEE Transactions on Instrumentation and Measurement. H. Li, X.-J. Wu, T. Durrani, NestFuse: An Infrared and Visible Image Fusion Architecture based on Nest Connection and Spatial/Channel At- tention Models, IEEE Transactions on Instrumentation and Measurement (2020). Doi: 10.1109/TIM.2020.3005230.\n\nUnet++: A nested u-net architecture for medical image segmentation. Z Zhou, M M R Siddiquee, N Tajbakhsh, J Liang, Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support. Granada, SpainSpringerZ. Zhou, M. M. R. Siddiquee, N. Tajbakhsh, J. Liang, Unet++: A nested u-net architecture for medical image segmentation, in: Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, Granada, Spain, Springer, 2018, pp. 3-11.\n\nDeepfuse: a deep unsupervised approach for exposure fusion with extreme exposure image pairs. K Ram Prabhakar, V Srikar, R Venkatesh, Babu, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionK. Ram Prabhakar, V. Sai Srikar, R. Venkatesh Babu, Deepfuse: a deep unsupervised approach for exposure fusion with extreme exposure image pairs, in: Proceedings of the IEEE International Conference on Computer Vision, 2017, pp. 4714-4722.\n\nK Simonyan, A Zisserman, arXiv:1409.1556Very deep convolutional networks for largescale image recognition. arXiv preprintK. Simonyan, A. Zisserman, Very deep convolutional networks for large- scale image recognition, arXiv preprint arXiv:1409.1556 (2014).\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionK. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recog- nition, in: Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770-778.\n\nDensely connected convolutional networks. G Huang, Z Liu, L Van Der Maaten, K Q Weinberger, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionG. Huang, Z. Liu, L. Van Der Maaten, K. Q. Weinberger, Densely con- nected convolutional networks, in: Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 4700-4708.\n\nIFCNN: A general image fusion framework based on convolutional neural network, Information Fusion. Y Zhang, Y Liu, P Sun, H Yan, X Zhao, L Zhang, 54Y. Zhang, Y. Liu, P. Sun, H. Yan, X. Zhao, L. Zhang, IFCNN: A general image fusion framework based on convolutional neural network, Infor- mation Fusion 54 (2020) 99-118.\n\nRethinking the image fusion: A fast unified image fusion network based on proportional maintenance of gradient and intensity. H Zhang, H Xu, Y Xiao, X Guo, J Ma, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2020H. Zhang, H. Xu, Y. Xiao, X. Guo, J. Ma, Rethinking the image fusion: A fast unified image fusion network based on proportional maintenance of gradient and intensity, in: Proceedings of the AAAI Conference on Artificial Intelligence, 2020, pp. 12797-12804.\n\nH Xu, J Ma, J Jiang, X Guo, H Ling, Doi:10.1109/TPAMI.2020.3012548U2Fusion: A Unified Unsupervised Image Fusion Network, IEEE Transactions on Pattern Analysis and Machine Intelligence. H. Xu, J. Ma, J. Jiang, X. Guo, H. Ling, U2Fusion: A Unified Unsuper- vised Image Fusion Network, IEEE Transactions on Pattern Analysis and Machine Intelligence (2020). Doi: 10.1109/TPAMI.2020.3012548.\n\nImage quality assessment: from error visibility to structural similarity. Z Wang, A C Bovik, H R Sheikh, E P Simoncelli, IEEE transactions on image processing. 13Z. Wang, A. C. Bovik, H. R. Sheikh, E. P. Simoncelli, et al., Image quality assessment: from error visibility to structural similarity, IEEE transactions on image processing 13 (2004) 600-612.\n\nT.-Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Doll\u00e1r, C L Zitnick, Microsoft coco: Common objects in context. Zurich, SwitzerlandSpringerEuropean conference on computer visionT.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r, C. L. Zitnick, Microsoft coco: Common objects in context, in: European conference on computer vision, Zurich, Switzerland, Springer, 2014, pp. 740-755.\n\nS Hwang, J Park, N Kim, Y Choi, I So Kweon, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionMultispectral pedestrian detection: Benchmark dataset and baselineS. Hwang, J. Park, N. Kim, Y. Choi, I. So Kweon, Multispectral pedes- trian detection: Benchmark dataset and baseline, in: Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 1037-1045.\n\nTNO Image Fusion Dataset. A Toet, A. Toet, TNO Image Fusion Dataset, 2014. https://figshare.com/ articles/TN_Image_Fusion_Dataset/1008029.\n\nThe eighth visual object tracking VOT2020 challenge results. M Kristan, J Matas, A Leonardis, M Felsberg, Proc. 16th Eur. Conf. Comput. Vis. Workshop. 16th Eur. Conf. Comput. Vis. WorkshopM. Kristan, J. Matas, A. Leonardis, M. Felsberg, et al., The eighth visual object tracking VOT2020 challenge results, in: Proc. 16th Eur. Conf. Comput. Vis. Workshop, 2020.\n\nCode of RFN-Nest. H Li, H. Li, Code of RFN-Nest, 2020. https://github.com/hli1221/ imagefusion-rfn-nest.\n\nAssessment of image fusion procedures using entropy, image quality, and multispectral classification. J W Roberts, J A Van Aardt, F B Ahmed, Journal of Applied Remote Sensing. 223522J. W. Roberts, J. A. Van Aardt, F. B. Ahmed, Assessment of image fusion procedures using entropy, image quality, and multispectral classification, Journal of Applied Remote Sensing 2 (2008) 023522.\n\nIn-fibre Bragg grating sensors. Y.-J Rao, Measurement science and technology. 8355Y.-J. Rao, In-fibre Bragg grating sensors, Measurement science and tech- nology 8 (1997) 355.\n\nInformation measure for performance of image fusion. G Qu, D Zhang, P Yan, Electronics letters. 38G. Qu, D. Zhang, P. Yan, Information measure for performance of image fusion, Electronics letters 38 (2002) 313-315.\n\nMultifocus and multispectral image fusion based on pixel significance using discrete cosine harmonic wavelet transform, Signal, Image and Video Processing. B S Kumar, 7B. S. Kumar, Multifocus and multispectral image fusion based on pixel significance using discrete cosine harmonic wavelet transform, Signal, Image and Video Processing 7 (2013) 1125-1143.\n\nA new image quality metric for image fusion: The sum of the correlations of differences. V Aslantas, E Bendes, Aeu-international Journal of electronics and communications. 69V. Aslantas, E. Bendes, A new image quality metric for image fusion: The sum of the correlations of differences, Aeu-international Journal of electronics and communications 69 (2015) 1890-1896.\n\nPerceptual quality assessment for multiexposure image fusion. K Ma, K Zeng, Z Wang, IEEE Transactions on Image Processing. 24K. Ma, K. Zeng, Z. Wang, Perceptual quality assessment for multi- exposure image fusion, IEEE Transactions on Image Processing 24 (2015) 3345-3356.\n\nInfrared and visible image fusion via gradient transfer and total variation minimization. J Ma, C Chen, C Li, J Huang, Information Fusion. 31J. Ma, C. Chen, C. Li, J. Huang, Infrared and visible image fusion via gradient transfer and total variation minimization, Information Fusion 31 (2016) 100-109.\n\nThe seventh visual object tracking vot2019 challenge results. M Kristan, J Matas, A Leonardis, M Felsberg, R Pflugfelder, J.-K Kamarainen, L Zajc, O Drbohlav, A Lukezic, A Berg, Proceedings of the IEEE International Conference on Computer Vision Workshops. the IEEE International Conference on Computer Vision WorkshopsM. Kristan, J. Matas, A. Leonardis, M. Felsberg, R. Pflugfelder, J.-K. Ka- marainen, L. Cehovin Zajc, O. Drbohlav, A. Lukezic, A. Berg, et al., The seventh visual object tracking vot2019 challenge results, in: Proceedings of the IEEE International Conference on Computer Vision Workshops, 2019, pp. 1-36.\n\nT Xu, Z.-H Feng, X.-J Wu, J Kittler, arXiv:2005.13708v1AFAT: Adaptive Failure-Aware Tracker for Robust Visual Object Tracking. arXiv preprintT. Xu, Z.-H. Feng, X.-J. Wu, J. Kittler, AFAT: Adaptive Failure- Aware Tracker for Robust Visual Object Tracking, arXiv preprint arXiv:2005.13708v1 (2020).\n\nLearning collaborative sparse representation for grayscale-thermal tracking. C Li, H Cheng, S Hu, X Liu, J Tang, L Lin, IEEE Transactions on Image Processing. 25C. Li, H. Cheng, S. Hu, X. Liu, J. Tang, L. Lin, Learning collaborative sparse representation for grayscale-thermal tracking, IEEE Transactions on Image Processing 25 (2016) 5743-5756.\n\nJ Tang, D Fan, X Wang, Z Tu, C Li, RGBT Salient Object Detection: Benchmark and A Novel Cooperative Ranking Approach, IEEE Transactions on Circuits and Systems for Video Technology. J. Tang, D. Fan, X. Wang, Z. Tu, C. Li, RGBT Salient Object Detection: Benchmark and A Novel Cooperative Ranking Approach, IEEE Transac- tions on Circuits and Systems for Video Technology (2019).\n\nRGB-T Image Saliency Detection via Collaborative Graph Learning. Z Tu, T Xia, C Li, X Wang, Y Ma, J Tang, IEEE Transactions on Multimedia. 22Z. Tu, T. Xia, C. Li, X. Wang, Y. Ma, J. Tang, RGB-T Image Saliency Detection via Collaborative Graph Learning, IEEE Transactions on Mul- timedia 22 (2019) 160-173.\n\nThe visual object tracking vot2015 challenge results. M Kristan, J Matas, A Leonardis, M Felsberg, L Cehovin, G Fernandez, T Vojir, G Hager, G Nebehay, R Pflugfelder, Proceedings of the IEEE international conference on computer vision workshops. the IEEE international conference on computer vision workshopsM. Kristan, J. Matas, A. Leonardis, M. Felsberg, L. Cehovin, G. Fernan- dez, T. Vojir, G. Hager, G. Nebehay, R. Pflugfelder, The visual object tracking vot2015 challenge results, in: Proceedings of the IEEE interna- tional conference on computer vision workshops, 2015, pp. 1-23.\n", "annotations": {"author": "[{\"end\":278,\"start\":83},{\"end\":479,\"start\":279},{\"end\":587,\"start\":480}]", "publisher": null, "author_last_name": "[{\"end\":89,\"start\":87},{\"end\":290,\"start\":288},{\"end\":493,\"start\":486}]", "author_first_name": "[{\"end\":86,\"start\":83},{\"end\":287,\"start\":279},{\"end\":485,\"start\":480}]", "author_affiliation": "[{\"end\":277,\"start\":91},{\"end\":478,\"start\":292},{\"end\":586,\"start\":495}]", "title": "[{\"end\":80,\"start\":1},{\"end\":667,\"start\":588}]", "venue": null, "abstract": "[{\"end\":2111,\"start\":757}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2515,\"start\":2512},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2523,\"start\":2520},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2564,\"start\":2561},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2716,\"start\":2713},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2723,\"start\":2720},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3133,\"start\":3130},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3137,\"start\":3134},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3146,\"start\":3142},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3596,\"start\":3592},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3635,\"start\":3631},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3640,\"start\":3636},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3750,\"start\":3746},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3755,\"start\":3751},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3760,\"start\":3756},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3765,\"start\":3761},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4392,\"start\":4388},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4397,\"start\":4393},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4541,\"start\":4538},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4608,\"start\":4604},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4613,\"start\":4609},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4618,\"start\":4614},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5368,\"start\":5364},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5373,\"start\":5369},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5378,\"start\":5374},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5383,\"start\":5379},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5388,\"start\":5384},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5498,\"start\":5494},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5503,\"start\":5499},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5570,\"start\":5566},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5575,\"start\":5571},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6047,\"start\":6043},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6065,\"start\":6061},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6078,\"start\":6074},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6268,\"start\":6264},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6284,\"start\":6280},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6472,\"start\":6468},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6910,\"start\":6906},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7013,\"start\":7009},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7168,\"start\":7164},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8081,\"start\":8077},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8086,\"start\":8082},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8095,\"start\":8091},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8638,\"start\":8634},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10144,\"start\":10140},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10153,\"start\":10149},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10162,\"start\":10158},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10245,\"start\":10241},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10709,\"start\":10705},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10741,\"start\":10737},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10927,\"start\":10923},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11115,\"start\":11111},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":11228,\"start\":11224},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11348,\"start\":11344},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11646,\"start\":11642},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11673,\"start\":11669},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11802,\"start\":11798},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12000,\"start\":11996},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":12029,\"start\":12025},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12063,\"start\":12059},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12603,\"start\":12599},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12613,\"start\":12609},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12623,\"start\":12619},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":12628,\"start\":12624},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12638,\"start\":12634},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":13245,\"start\":13241},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":13779,\"start\":13775},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":16072,\"start\":16068},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":17066,\"start\":17062},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":17241,\"start\":17237},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":17534,\"start\":17530},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":18472,\"start\":18468},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":18851,\"start\":18847},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":18860,\"start\":18856},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":19901,\"start\":19897},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":22494,\"start\":22490},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":22805,\"start\":22801},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":23284,\"start\":23280},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":23306,\"start\":23302},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":23342,\"start\":23338},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":23704,\"start\":23700},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":23735,\"start\":23731},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":23764,\"start\":23760},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":23813,\"start\":23809},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":23923,\"start\":23919},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":23980,\"start\":23976},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":32592,\"start\":32588},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":32601,\"start\":32597},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":32609,\"start\":32605},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":32621,\"start\":32617},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":32631,\"start\":32627},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":32644,\"start\":32640},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":33613,\"start\":33609},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":33622,\"start\":33618},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":33976,\"start\":33972},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":33983,\"start\":33979},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":34067,\"start\":34063},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":34222,\"start\":34218},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":34666,\"start\":34662},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":36291,\"start\":36287},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":36353,\"start\":36349},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":36403,\"start\":36399},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":36456,\"start\":36452},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":36472,\"start\":36468},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":36488,\"start\":36484},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":36500,\"start\":36496},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":36537,\"start\":36533},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":36548,\"start\":36544},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":36561,\"start\":36557},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":36579,\"start\":36575},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":37872,\"start\":37871},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":40516,\"start\":40512},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":40538,\"start\":40534},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":40700,\"start\":40696},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":40768,\"start\":40764},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":40887,\"start\":40883},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":40900,\"start\":40896},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":40918,\"start\":40914},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":42274,\"start\":42270},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":42726,\"start\":42722},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":42776,\"start\":42772},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":43064,\"start\":43060},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":43612,\"start\":43608},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":43981,\"start\":43980},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":44123,\"start\":44119},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":44135,\"start\":44131},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":44148,\"start\":44144},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":45222,\"start\":45218},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":46263,\"start\":46259},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":46286,\"start\":46282},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":47103,\"start\":47099},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":47108,\"start\":47104},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":47462,\"start\":47461},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":50076,\"start\":50072},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":50078,\"start\":50076}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":50181,\"start\":49899},{\"attributes\":{\"id\":\"fig_2\"},\"end\":50222,\"start\":50182},{\"attributes\":{\"id\":\"fig_3\"},\"end\":50268,\"start\":50223},{\"attributes\":{\"id\":\"fig_4\"},\"end\":50323,\"start\":50269},{\"attributes\":{\"id\":\"fig_5\"},\"end\":50851,\"start\":50324},{\"attributes\":{\"id\":\"fig_6\"},\"end\":51109,\"start\":50852},{\"attributes\":{\"id\":\"fig_7\"},\"end\":51345,\"start\":51110},{\"attributes\":{\"id\":\"fig_8\"},\"end\":51404,\"start\":51346},{\"attributes\":{\"id\":\"fig_9\"},\"end\":51632,\"start\":51405},{\"attributes\":{\"id\":\"fig_10\"},\"end\":51705,\"start\":51633},{\"attributes\":{\"id\":\"fig_11\"},\"end\":51976,\"start\":51706},{\"attributes\":{\"id\":\"fig_12\"},\"end\":52125,\"start\":51977},{\"attributes\":{\"id\":\"fig_13\"},\"end\":52357,\"start\":52126},{\"attributes\":{\"id\":\"fig_14\"},\"end\":52920,\"start\":52358},{\"attributes\":{\"id\":\"fig_15\"},\"end\":53086,\"start\":52921},{\"attributes\":{\"id\":\"fig_16\"},\"end\":53141,\"start\":53087},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":54447,\"start\":53142},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":54947,\"start\":54448},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":55004,\"start\":54948},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":56006,\"start\":55005},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":56863,\"start\":56007},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":57219,\"start\":56864},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":57567,\"start\":57220}]", "paragraph": "[{\"end\":2724,\"start\":2127},{\"end\":4436,\"start\":2726},{\"end\":4956,\"start\":4438},{\"end\":6610,\"start\":4958},{\"end\":7102,\"start\":6612},{\"end\":7890,\"start\":7104},{\"end\":7956,\"start\":7892},{\"end\":8305,\"start\":7958},{\"end\":8817,\"start\":8307},{\"end\":9091,\"start\":8819},{\"end\":9310,\"start\":9093},{\"end\":9628,\"start\":9312},{\"end\":10000,\"start\":9646},{\"end\":11066,\"start\":10027},{\"end\":11972,\"start\":11068},{\"end\":12441,\"start\":11974},{\"end\":12629,\"start\":12464},{\"end\":13163,\"start\":12631},{\"end\":13723,\"start\":13165},{\"end\":14383,\"start\":13725},{\"end\":14616,\"start\":14417},{\"end\":14987,\"start\":14659},{\"end\":15523,\"start\":14989},{\"end\":15931,\"start\":15525},{\"end\":15984,\"start\":15933},{\"end\":16742,\"start\":16018},{\"end\":16942,\"start\":16744},{\"end\":17615,\"start\":16962},{\"end\":17712,\"start\":17617},{\"end\":18067,\"start\":17744},{\"end\":18326,\"start\":18069},{\"end\":18405,\"start\":18328},{\"end\":19198,\"start\":18446},{\"end\":19305,\"start\":19218},{\"end\":19593,\"start\":19336},{\"end\":19801,\"start\":19630},{\"end\":20041,\"start\":19841},{\"end\":20363,\"start\":20065},{\"end\":20577,\"start\":20398},{\"end\":20813,\"start\":20579},{\"end\":21083,\"start\":20850},{\"end\":21701,\"start\":21158},{\"end\":22202,\"start\":21729},{\"end\":22301,\"start\":22204},{\"end\":22441,\"start\":22349},{\"end\":23167,\"start\":22443},{\"end\":23596,\"start\":23211},{\"end\":24077,\"start\":23598},{\"end\":24334,\"start\":24124},{\"end\":24850,\"start\":24336},{\"end\":25186,\"start\":24852},{\"end\":25540,\"start\":25188},{\"end\":25721,\"start\":25542},{\"end\":25882,\"start\":25723},{\"end\":26244,\"start\":25884},{\"end\":26465,\"start\":26246},{\"end\":26929,\"start\":26467},{\"end\":27333,\"start\":26931},{\"end\":27884,\"start\":27335},{\"end\":28158,\"start\":27886},{\"end\":28240,\"start\":28160},{\"end\":28638,\"start\":28274},{\"end\":28923,\"start\":28640},{\"end\":29269,\"start\":28925},{\"end\":29504,\"start\":29271},{\"end\":29811,\"start\":29506},{\"end\":30064,\"start\":29852},{\"end\":30568,\"start\":30066},{\"end\":30973,\"start\":30588},{\"end\":31276,\"start\":30975},{\"end\":31767,\"start\":31278},{\"end\":32138,\"start\":31817},{\"end\":32583,\"start\":32140},{\"end\":33318,\"start\":32585},{\"end\":33645,\"start\":33357},{\"end\":33800,\"start\":33647},{\"end\":33815,\"start\":33802},{\"end\":33940,\"start\":33817},{\"end\":34410,\"start\":33960},{\"end\":34819,\"start\":34565},{\"end\":35132,\"start\":34821},{\"end\":35770,\"start\":35134},{\"end\":35942,\"start\":35772},{\"end\":35990,\"start\":35961},{\"end\":36036,\"start\":36028},{\"end\":36580,\"start\":36083},{\"end\":36899,\"start\":36582},{\"end\":37170,\"start\":36901},{\"end\":38101,\"start\":37172},{\"end\":38505,\"start\":38103},{\"end\":38954,\"start\":38507},{\"end\":39458,\"start\":38956},{\"end\":39656,\"start\":39460},{\"end\":40151,\"start\":39658},{\"end\":40539,\"start\":40191},{\"end\":40920,\"start\":40541},{\"end\":41557,\"start\":40922},{\"end\":41833,\"start\":41559},{\"end\":42077,\"start\":41835},{\"end\":42709,\"start\":42117},{\"end\":43012,\"start\":42711},{\"end\":43419,\"start\":43014},{\"end\":43678,\"start\":43471},{\"end\":44307,\"start\":43680},{\"end\":44645,\"start\":44309},{\"end\":44949,\"start\":44705},{\"end\":45163,\"start\":44986},{\"end\":45402,\"start\":45165},{\"end\":46102,\"start\":45404},{\"end\":46264,\"start\":46104},{\"end\":46569,\"start\":46266},{\"end\":47109,\"start\":46571},{\"end\":48106,\"start\":47111},{\"end\":49329,\"start\":48122},{\"end\":49898,\"start\":49331}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":19335,\"start\":19306},{\"attributes\":{\"id\":\"formula_1\"},\"end\":19629,\"start\":19594},{\"attributes\":{\"id\":\"formula_2\"},\"end\":19840,\"start\":19802},{\"attributes\":{\"id\":\"formula_3\"},\"end\":20397,\"start\":20364},{\"attributes\":{\"id\":\"formula_4\"},\"end\":20849,\"start\":20814},{\"attributes\":{\"id\":\"formula_5\"},\"end\":21157,\"start\":21084},{\"attributes\":{\"id\":\"formula_6\"},\"end\":34564,\"start\":34411},{\"attributes\":{\"id\":\"formula_7\"},\"end\":36027,\"start\":35991},{\"attributes\":{\"id\":\"formula_8\"},\"end\":44704,\"start\":44646}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":26425,\"start\":26418},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":27332,\"start\":27325},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":27357,\"start\":27350},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":28494,\"start\":28487},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":29464,\"start\":29457},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":29535,\"start\":29528},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":29670,\"start\":29663},{\"end\":32196,\"start\":32189},{\"end\":32270,\"start\":32263},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":33814,\"start\":33807},{\"end\":35294,\"start\":35287},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":41682,\"start\":41674},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":47201,\"start\":47182}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2125,\"start\":2113},{\"attributes\":{\"n\":\"2.\"},\"end\":9644,\"start\":9631},{\"attributes\":{\"n\":\"2.1.\"},\"end\":10025,\"start\":10003},{\"attributes\":{\"n\":\"2.2.\"},\"end\":12462,\"start\":12444},{\"attributes\":{\"n\":\"3.\"},\"end\":14415,\"start\":14386},{\"attributes\":{\"n\":\"3.1.\"},\"end\":14657,\"start\":14619},{\"attributes\":{\"n\":\"3.1.1.\"},\"end\":16016,\"start\":15987},{\"attributes\":{\"n\":\"3.1.2.\"},\"end\":16960,\"start\":16945},{\"attributes\":{\"n\":\"3.2.\"},\"end\":17742,\"start\":17715},{\"attributes\":{\"n\":\"3.2.1.\"},\"end\":18444,\"start\":18408},{\"end\":19216,\"start\":19201},{\"attributes\":{\"n\":\"3.2.2.\"},\"end\":20063,\"start\":20044},{\"attributes\":{\"n\":\"4.\"},\"end\":21727,\"start\":21704},{\"attributes\":{\"n\":\"4.1.\"},\"end\":22347,\"start\":22304},{\"attributes\":{\"n\":\"4.2.\"},\"end\":23209,\"start\":23170},{\"attributes\":{\"n\":\"4.3.\"},\"end\":24122,\"start\":24080},{\"attributes\":{\"n\":\"4.4.\"},\"end\":28272,\"start\":28243},{\"attributes\":{\"n\":\"4.5.\"},\"end\":29850,\"start\":29814},{\"end\":30586,\"start\":30571},{\"attributes\":{\"n\":\"4.6.\"},\"end\":31815,\"start\":31770},{\"attributes\":{\"n\":\"4.7.\"},\"end\":33355,\"start\":33321},{\"end\":33958,\"start\":33943},{\"end\":35959,\"start\":35945},{\"attributes\":{\"n\":\"4.8.\"},\"end\":36081,\"start\":36039},{\"attributes\":{\"n\":\"4.9.\"},\"end\":40189,\"start\":40154},{\"attributes\":{\"n\":\"5.\"},\"end\":42115,\"start\":42080},{\"attributes\":{\"n\":\"5.1.\"},\"end\":43469,\"start\":43422},{\"attributes\":{\"n\":\"5.2.\"},\"end\":44984,\"start\":44952},{\"attributes\":{\"n\":\"6.\"},\"end\":48120,\"start\":48109},{\"end\":49910,\"start\":49900},{\"end\":50193,\"start\":50183},{\"end\":50234,\"start\":50224},{\"end\":50280,\"start\":50270},{\"end\":50335,\"start\":50325},{\"end\":50873,\"start\":50853},{\"end\":51121,\"start\":51111},{\"end\":51357,\"start\":51347},{\"end\":51417,\"start\":51406},{\"end\":51645,\"start\":51634},{\"end\":51989,\"start\":51978},{\"end\":52138,\"start\":52127},{\"end\":52370,\"start\":52359},{\"end\":52933,\"start\":52922},{\"end\":53099,\"start\":53088},{\"end\":53152,\"start\":53143},{\"end\":54458,\"start\":54449},{\"end\":54958,\"start\":54949},{\"end\":55015,\"start\":55006},{\"end\":56017,\"start\":56008},{\"end\":56874,\"start\":56865},{\"end\":57230,\"start\":57221}]", "table": "[{\"end\":54447,\"start\":53294},{\"end\":54947,\"start\":54578},{\"end\":56006,\"start\":55127},{\"end\":56863,\"start\":56389},{\"end\":57219,\"start\":57012},{\"end\":57567,\"start\":57367}]", "figure_caption": "[{\"end\":50181,\"start\":49912},{\"end\":50222,\"start\":50195},{\"end\":50268,\"start\":50236},{\"end\":50323,\"start\":50282},{\"end\":50851,\"start\":50337},{\"end\":51109,\"start\":50876},{\"end\":51345,\"start\":51123},{\"end\":51404,\"start\":51359},{\"end\":51632,\"start\":51420},{\"end\":51705,\"start\":51648},{\"end\":51976,\"start\":51708},{\"end\":52125,\"start\":51992},{\"end\":52357,\"start\":52141},{\"end\":52920,\"start\":52373},{\"end\":53086,\"start\":52936},{\"end\":53141,\"start\":53102},{\"end\":53294,\"start\":53154},{\"end\":54578,\"start\":54460},{\"end\":55004,\"start\":54960},{\"end\":55127,\"start\":55017},{\"end\":56389,\"start\":56019},{\"end\":57012,\"start\":56876},{\"end\":57367,\"start\":57232}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14748,\"start\":14743},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15542,\"start\":15537},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16163,\"start\":16158},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16173,\"start\":16168},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17041,\"start\":17036},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":18718,\"start\":18713},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":18728,\"start\":18723},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":20282,\"start\":20277},{\"end\":23391,\"start\":23386},{\"end\":25881,\"start\":25876},{\"end\":25892,\"start\":25887},{\"end\":26311,\"start\":26306},{\"end\":26753,\"start\":26747},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":26928,\"start\":26923},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":26952,\"start\":26934},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":27290,\"start\":27271},{\"end\":27345,\"start\":27340},{\"end\":27700,\"start\":27694},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":28482,\"start\":28477},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":28978,\"start\":28973},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":29523,\"start\":29518},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":30199,\"start\":30193},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":30496,\"start\":30490},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":30507,\"start\":30498},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":30601,\"start\":30591},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":30851,\"start\":30840},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":31904,\"start\":31898},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":34908,\"start\":34902},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":37144,\"start\":37138},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":37155,\"start\":37149},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":37192,\"start\":37175},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38019,\"start\":38009},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38276,\"start\":38270},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38516,\"start\":38510},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38952,\"start\":38942},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":39161,\"start\":39144},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":41029,\"start\":41023},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":41114,\"start\":41104},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":41429,\"start\":41422},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":43849,\"start\":43843},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":45125,\"start\":45119},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":45541,\"start\":45532}]", "bib_author_first_name": "[{\"end\":57829,\"start\":57828},{\"end\":57835,\"start\":57834},{\"end\":57844,\"start\":57843},{\"end\":57850,\"start\":57849},{\"end\":57858,\"start\":57857},{\"end\":58098,\"start\":58097},{\"end\":58104,\"start\":58103},{\"end\":58111,\"start\":58110},{\"end\":58120,\"start\":58119},{\"end\":58127,\"start\":58126},{\"end\":58133,\"start\":58132},{\"end\":58523,\"start\":58522},{\"end\":58530,\"start\":58529},{\"end\":58537,\"start\":58536},{\"end\":58545,\"start\":58544},{\"end\":58554,\"start\":58550},{\"end\":58859,\"start\":58858},{\"end\":58872,\"start\":58871},{\"end\":58881,\"start\":58880},{\"end\":59292,\"start\":59291},{\"end\":59298,\"start\":59297},{\"end\":59304,\"start\":59303},{\"end\":59524,\"start\":59523},{\"end\":59530,\"start\":59529},{\"end\":59538,\"start\":59537},{\"end\":59546,\"start\":59545},{\"end\":59552,\"start\":59551},{\"end\":59802,\"start\":59801},{\"end\":59809,\"start\":59808},{\"end\":59817,\"start\":59816},{\"end\":59825,\"start\":59824},{\"end\":59827,\"start\":59826},{\"end\":59835,\"start\":59834},{\"end\":59837,\"start\":59836},{\"end\":59845,\"start\":59844},{\"end\":60096,\"start\":60095},{\"end\":60107,\"start\":60106},{\"end\":60109,\"start\":60108},{\"end\":60309,\"start\":60308},{\"end\":60322,\"start\":60321},{\"end\":60328,\"start\":60327},{\"end\":60336,\"start\":60335},{\"end\":60541,\"start\":60540},{\"end\":60549,\"start\":60548},{\"end\":60557,\"start\":60556},{\"end\":60565,\"start\":60564},{\"end\":60571,\"start\":60570},{\"end\":60764,\"start\":60763},{\"end\":60770,\"start\":60769},{\"end\":60778,\"start\":60777},{\"end\":60997,\"start\":60996},{\"end\":61007,\"start\":61006},{\"end\":61009,\"start\":61008},{\"end\":61017,\"start\":61016},{\"end\":61027,\"start\":61026},{\"end\":61029,\"start\":61028},{\"end\":61039,\"start\":61038},{\"end\":61293,\"start\":61292},{\"end\":61300,\"start\":61299},{\"end\":61307,\"start\":61306},{\"end\":61498,\"start\":61497},{\"end\":61505,\"start\":61504},{\"end\":61512,\"start\":61511},{\"end\":61519,\"start\":61518},{\"end\":61526,\"start\":61525},{\"end\":61532,\"start\":61531},{\"end\":61836,\"start\":61835},{\"end\":61845,\"start\":61844},{\"end\":61851,\"start\":61850},{\"end\":61857,\"start\":61856},{\"end\":62129,\"start\":62128},{\"end\":62136,\"start\":62135},{\"end\":62142,\"start\":62141},{\"end\":62389,\"start\":62388},{\"end\":62396,\"start\":62395},{\"end\":62398,\"start\":62397},{\"end\":62410,\"start\":62409},{\"end\":62662,\"start\":62661},{\"end\":62671,\"start\":62667},{\"end\":63050,\"start\":63049},{\"end\":63056,\"start\":63055},{\"end\":63065,\"start\":63064},{\"end\":63073,\"start\":63072},{\"end\":63080,\"start\":63079},{\"end\":63457,\"start\":63456},{\"end\":63464,\"start\":63463},{\"end\":63472,\"start\":63471},{\"end\":63479,\"start\":63478},{\"end\":63808,\"start\":63807},{\"end\":63815,\"start\":63814},{\"end\":63821,\"start\":63820},{\"end\":64089,\"start\":64088},{\"end\":64098,\"start\":64094},{\"end\":64104,\"start\":64103},{\"end\":64455,\"start\":64454},{\"end\":64464,\"start\":64460},{\"end\":64470,\"start\":64469},{\"end\":64472,\"start\":64471},{\"end\":64741,\"start\":64740},{\"end\":64752,\"start\":64748},{\"end\":65065,\"start\":65064},{\"end\":65074,\"start\":65070},{\"end\":65359,\"start\":65358},{\"end\":65368,\"start\":65364},{\"end\":65374,\"start\":65373},{\"end\":65694,\"start\":65693},{\"end\":65701,\"start\":65700},{\"end\":65709,\"start\":65708},{\"end\":65711,\"start\":65710},{\"end\":65719,\"start\":65718},{\"end\":65721,\"start\":65720},{\"end\":65980,\"start\":65979},{\"end\":65987,\"start\":65986},{\"end\":65995,\"start\":65994},{\"end\":66003,\"start\":66002},{\"end\":66257,\"start\":66256},{\"end\":66263,\"start\":66262},{\"end\":66269,\"start\":66268},{\"end\":66278,\"start\":66277},{\"end\":66284,\"start\":66283},{\"end\":66556,\"start\":66555},{\"end\":66562,\"start\":66561},{\"end\":66571,\"start\":66570},{\"end\":66577,\"start\":66576},{\"end\":66585,\"start\":66584},{\"end\":66592,\"start\":66591},{\"end\":66598,\"start\":66597},{\"end\":66911,\"start\":66910},{\"end\":66917,\"start\":66916},{\"end\":66923,\"start\":66922},{\"end\":66932,\"start\":66931},{\"end\":66942,\"start\":66938},{\"end\":67329,\"start\":67328},{\"end\":67338,\"start\":67334},{\"end\":67344,\"start\":67343},{\"end\":67752,\"start\":67751},{\"end\":67760,\"start\":67759},{\"end\":67764,\"start\":67761},{\"end\":67777,\"start\":67776},{\"end\":67790,\"start\":67789},{\"end\":68273,\"start\":68272},{\"end\":68277,\"start\":68274},{\"end\":68290,\"start\":68289},{\"end\":68300,\"start\":68299},{\"end\":68681,\"start\":68680},{\"end\":68693,\"start\":68692},{\"end\":68984,\"start\":68983},{\"end\":68990,\"start\":68989},{\"end\":68999,\"start\":68998},{\"end\":69006,\"start\":69005},{\"end\":69380,\"start\":69379},{\"end\":69389,\"start\":69388},{\"end\":69396,\"start\":69395},{\"end\":69414,\"start\":69413},{\"end\":69416,\"start\":69415},{\"end\":69874,\"start\":69873},{\"end\":69883,\"start\":69882},{\"end\":69890,\"start\":69889},{\"end\":69897,\"start\":69896},{\"end\":69904,\"start\":69903},{\"end\":69912,\"start\":69911},{\"end\":70221,\"start\":70220},{\"end\":70230,\"start\":70229},{\"end\":70236,\"start\":70235},{\"end\":70244,\"start\":70243},{\"end\":70251,\"start\":70250},{\"end\":70628,\"start\":70627},{\"end\":70634,\"start\":70633},{\"end\":70640,\"start\":70639},{\"end\":70649,\"start\":70648},{\"end\":70656,\"start\":70655},{\"end\":71090,\"start\":71089},{\"end\":71098,\"start\":71097},{\"end\":71100,\"start\":71099},{\"end\":71109,\"start\":71108},{\"end\":71111,\"start\":71110},{\"end\":71121,\"start\":71120},{\"end\":71123,\"start\":71122},{\"end\":71375,\"start\":71371},{\"end\":71382,\"start\":71381},{\"end\":71391,\"start\":71390},{\"end\":71403,\"start\":71402},{\"end\":71411,\"start\":71410},{\"end\":71421,\"start\":71420},{\"end\":71432,\"start\":71431},{\"end\":71442,\"start\":71441},{\"end\":71444,\"start\":71443},{\"end\":71793,\"start\":71792},{\"end\":71802,\"start\":71801},{\"end\":71810,\"start\":71809},{\"end\":71817,\"start\":71816},{\"end\":71825,\"start\":71824},{\"end\":71828,\"start\":71826},{\"end\":72294,\"start\":72293},{\"end\":72469,\"start\":72468},{\"end\":72480,\"start\":72479},{\"end\":72489,\"start\":72488},{\"end\":72502,\"start\":72501},{\"end\":72788,\"start\":72787},{\"end\":72978,\"start\":72977},{\"end\":72980,\"start\":72979},{\"end\":72991,\"start\":72990},{\"end\":72993,\"start\":72992},{\"end\":73006,\"start\":73005},{\"end\":73008,\"start\":73007},{\"end\":73292,\"start\":73288},{\"end\":73487,\"start\":73486},{\"end\":73493,\"start\":73492},{\"end\":73502,\"start\":73501},{\"end\":73806,\"start\":73805},{\"end\":73808,\"start\":73807},{\"end\":74096,\"start\":74095},{\"end\":74108,\"start\":74107},{\"end\":74438,\"start\":74437},{\"end\":74444,\"start\":74443},{\"end\":74452,\"start\":74451},{\"end\":74740,\"start\":74739},{\"end\":74746,\"start\":74745},{\"end\":74754,\"start\":74753},{\"end\":74760,\"start\":74759},{\"end\":75015,\"start\":75014},{\"end\":75026,\"start\":75025},{\"end\":75035,\"start\":75034},{\"end\":75048,\"start\":75047},{\"end\":75060,\"start\":75059},{\"end\":75078,\"start\":75074},{\"end\":75092,\"start\":75091},{\"end\":75100,\"start\":75099},{\"end\":75112,\"start\":75111},{\"end\":75123,\"start\":75122},{\"end\":75578,\"start\":75577},{\"end\":75587,\"start\":75583},{\"end\":75598,\"start\":75594},{\"end\":75604,\"start\":75603},{\"end\":75953,\"start\":75952},{\"end\":75959,\"start\":75958},{\"end\":75968,\"start\":75967},{\"end\":75974,\"start\":75973},{\"end\":75981,\"start\":75980},{\"end\":75989,\"start\":75988},{\"end\":76223,\"start\":76222},{\"end\":76231,\"start\":76230},{\"end\":76238,\"start\":76237},{\"end\":76246,\"start\":76245},{\"end\":76252,\"start\":76251},{\"end\":76667,\"start\":76666},{\"end\":76673,\"start\":76672},{\"end\":76680,\"start\":76679},{\"end\":76686,\"start\":76685},{\"end\":76694,\"start\":76693},{\"end\":76700,\"start\":76699},{\"end\":76963,\"start\":76962},{\"end\":76974,\"start\":76973},{\"end\":76983,\"start\":76982},{\"end\":76996,\"start\":76995},{\"end\":77008,\"start\":77007},{\"end\":77019,\"start\":77018},{\"end\":77032,\"start\":77031},{\"end\":77041,\"start\":77040},{\"end\":77050,\"start\":77049},{\"end\":77061,\"start\":77060}]", "bib_author_last_name": "[{\"end\":57832,\"start\":57830},{\"end\":57841,\"start\":57836},{\"end\":57847,\"start\":57845},{\"end\":57855,\"start\":57851},{\"end\":57863,\"start\":57859},{\"end\":58101,\"start\":58099},{\"end\":58108,\"start\":58105},{\"end\":58117,\"start\":58112},{\"end\":58124,\"start\":58121},{\"end\":58130,\"start\":58128},{\"end\":58138,\"start\":58134},{\"end\":58527,\"start\":58524},{\"end\":58534,\"start\":58531},{\"end\":58542,\"start\":58538},{\"end\":58548,\"start\":58546},{\"end\":58558,\"start\":58555},{\"end\":58869,\"start\":58860},{\"end\":58878,\"start\":58873},{\"end\":58895,\"start\":58882},{\"end\":59295,\"start\":59293},{\"end\":59301,\"start\":59299},{\"end\":59307,\"start\":59305},{\"end\":59527,\"start\":59525},{\"end\":59535,\"start\":59531},{\"end\":59543,\"start\":59539},{\"end\":59549,\"start\":59547},{\"end\":59556,\"start\":59553},{\"end\":59806,\"start\":59803},{\"end\":59814,\"start\":59810},{\"end\":59822,\"start\":59818},{\"end\":59832,\"start\":59828},{\"end\":59842,\"start\":59838},{\"end\":59850,\"start\":59846},{\"end\":60104,\"start\":60097},{\"end\":60115,\"start\":60110},{\"end\":60121,\"start\":60117},{\"end\":60319,\"start\":60310},{\"end\":60325,\"start\":60323},{\"end\":60333,\"start\":60329},{\"end\":60344,\"start\":60337},{\"end\":60546,\"start\":60542},{\"end\":60554,\"start\":60550},{\"end\":60562,\"start\":60558},{\"end\":60568,\"start\":60566},{\"end\":60576,\"start\":60572},{\"end\":60767,\"start\":60765},{\"end\":60775,\"start\":60771},{\"end\":60781,\"start\":60779},{\"end\":61004,\"start\":60998},{\"end\":61014,\"start\":61010},{\"end\":61024,\"start\":61018},{\"end\":61036,\"start\":61030},{\"end\":61042,\"start\":61040},{\"end\":61297,\"start\":61294},{\"end\":61304,\"start\":61301},{\"end\":61310,\"start\":61308},{\"end\":61502,\"start\":61499},{\"end\":61509,\"start\":61506},{\"end\":61516,\"start\":61513},{\"end\":61523,\"start\":61520},{\"end\":61529,\"start\":61527},{\"end\":61535,\"start\":61533},{\"end\":61842,\"start\":61837},{\"end\":61848,\"start\":61846},{\"end\":61854,\"start\":61852},{\"end\":61861,\"start\":61858},{\"end\":62133,\"start\":62130},{\"end\":62139,\"start\":62137},{\"end\":62147,\"start\":62143},{\"end\":62393,\"start\":62390},{\"end\":62407,\"start\":62399},{\"end\":62415,\"start\":62411},{\"end\":62665,\"start\":62663},{\"end\":62674,\"start\":62672},{\"end\":63053,\"start\":63051},{\"end\":63062,\"start\":63057},{\"end\":63070,\"start\":63066},{\"end\":63077,\"start\":63074},{\"end\":63084,\"start\":63081},{\"end\":63461,\"start\":63458},{\"end\":63469,\"start\":63465},{\"end\":63476,\"start\":63473},{\"end\":63485,\"start\":63480},{\"end\":63812,\"start\":63809},{\"end\":63818,\"start\":63816},{\"end\":63826,\"start\":63822},{\"end\":64092,\"start\":64090},{\"end\":64101,\"start\":64099},{\"end\":64112,\"start\":64105},{\"end\":64458,\"start\":64456},{\"end\":64467,\"start\":64465},{\"end\":64480,\"start\":64473},{\"end\":64746,\"start\":64742},{\"end\":64755,\"start\":64753},{\"end\":65068,\"start\":65066},{\"end\":65077,\"start\":65075},{\"end\":65362,\"start\":65360},{\"end\":65371,\"start\":65369},{\"end\":65382,\"start\":65375},{\"end\":65698,\"start\":65695},{\"end\":65706,\"start\":65702},{\"end\":65716,\"start\":65712},{\"end\":65726,\"start\":65722},{\"end\":65984,\"start\":65981},{\"end\":65992,\"start\":65988},{\"end\":66000,\"start\":65996},{\"end\":66008,\"start\":66004},{\"end\":66260,\"start\":66258},{\"end\":66266,\"start\":66264},{\"end\":66275,\"start\":66270},{\"end\":66281,\"start\":66279},{\"end\":66290,\"start\":66285},{\"end\":66559,\"start\":66557},{\"end\":66568,\"start\":66563},{\"end\":66574,\"start\":66572},{\"end\":66582,\"start\":66578},{\"end\":66589,\"start\":66586},{\"end\":66595,\"start\":66593},{\"end\":66604,\"start\":66599},{\"end\":66914,\"start\":66912},{\"end\":66920,\"start\":66918},{\"end\":66929,\"start\":66924},{\"end\":66936,\"start\":66933},{\"end\":66948,\"start\":66943},{\"end\":67332,\"start\":67330},{\"end\":67341,\"start\":67339},{\"end\":67352,\"start\":67345},{\"end\":67757,\"start\":67753},{\"end\":67774,\"start\":67765},{\"end\":67787,\"start\":67778},{\"end\":67796,\"start\":67791},{\"end\":68287,\"start\":68278},{\"end\":68297,\"start\":68291},{\"end\":68310,\"start\":68301},{\"end\":68316,\"start\":68312},{\"end\":68690,\"start\":68682},{\"end\":68703,\"start\":68694},{\"end\":68987,\"start\":68985},{\"end\":68996,\"start\":68991},{\"end\":69003,\"start\":69000},{\"end\":69010,\"start\":69007},{\"end\":69386,\"start\":69381},{\"end\":69393,\"start\":69390},{\"end\":69411,\"start\":69397},{\"end\":69427,\"start\":69417},{\"end\":69880,\"start\":69875},{\"end\":69887,\"start\":69884},{\"end\":69894,\"start\":69891},{\"end\":69901,\"start\":69898},{\"end\":69909,\"start\":69905},{\"end\":69918,\"start\":69913},{\"end\":70227,\"start\":70222},{\"end\":70233,\"start\":70231},{\"end\":70241,\"start\":70237},{\"end\":70248,\"start\":70245},{\"end\":70254,\"start\":70252},{\"end\":70631,\"start\":70629},{\"end\":70637,\"start\":70635},{\"end\":70646,\"start\":70641},{\"end\":70653,\"start\":70650},{\"end\":70661,\"start\":70657},{\"end\":71095,\"start\":71091},{\"end\":71106,\"start\":71101},{\"end\":71118,\"start\":71112},{\"end\":71134,\"start\":71124},{\"end\":71379,\"start\":71376},{\"end\":71388,\"start\":71383},{\"end\":71400,\"start\":71392},{\"end\":71408,\"start\":71404},{\"end\":71418,\"start\":71412},{\"end\":71429,\"start\":71422},{\"end\":71439,\"start\":71433},{\"end\":71452,\"start\":71445},{\"end\":71799,\"start\":71794},{\"end\":71807,\"start\":71803},{\"end\":71814,\"start\":71811},{\"end\":71822,\"start\":71818},{\"end\":71834,\"start\":71829},{\"end\":72299,\"start\":72295},{\"end\":72477,\"start\":72470},{\"end\":72486,\"start\":72481},{\"end\":72499,\"start\":72490},{\"end\":72511,\"start\":72503},{\"end\":72791,\"start\":72789},{\"end\":72988,\"start\":72981},{\"end\":73003,\"start\":72994},{\"end\":73014,\"start\":73009},{\"end\":73296,\"start\":73293},{\"end\":73490,\"start\":73488},{\"end\":73499,\"start\":73494},{\"end\":73506,\"start\":73503},{\"end\":73814,\"start\":73809},{\"end\":74105,\"start\":74097},{\"end\":74115,\"start\":74109},{\"end\":74441,\"start\":74439},{\"end\":74449,\"start\":74445},{\"end\":74457,\"start\":74453},{\"end\":74743,\"start\":74741},{\"end\":74751,\"start\":74747},{\"end\":74757,\"start\":74755},{\"end\":74766,\"start\":74761},{\"end\":75023,\"start\":75016},{\"end\":75032,\"start\":75027},{\"end\":75045,\"start\":75036},{\"end\":75057,\"start\":75049},{\"end\":75072,\"start\":75061},{\"end\":75089,\"start\":75079},{\"end\":75097,\"start\":75093},{\"end\":75109,\"start\":75101},{\"end\":75120,\"start\":75113},{\"end\":75128,\"start\":75124},{\"end\":75581,\"start\":75579},{\"end\":75592,\"start\":75588},{\"end\":75601,\"start\":75599},{\"end\":75612,\"start\":75605},{\"end\":75956,\"start\":75954},{\"end\":75965,\"start\":75960},{\"end\":75971,\"start\":75969},{\"end\":75978,\"start\":75975},{\"end\":75986,\"start\":75982},{\"end\":75993,\"start\":75990},{\"end\":76228,\"start\":76224},{\"end\":76235,\"start\":76232},{\"end\":76243,\"start\":76239},{\"end\":76249,\"start\":76247},{\"end\":76255,\"start\":76253},{\"end\":76670,\"start\":76668},{\"end\":76677,\"start\":76674},{\"end\":76683,\"start\":76681},{\"end\":76691,\"start\":76687},{\"end\":76697,\"start\":76695},{\"end\":76705,\"start\":76701},{\"end\":76971,\"start\":76964},{\"end\":76980,\"start\":76975},{\"end\":76993,\"start\":76984},{\"end\":77005,\"start\":76997},{\"end\":77016,\"start\":77009},{\"end\":77029,\"start\":77020},{\"end\":77038,\"start\":77033},{\"end\":77047,\"start\":77042},{\"end\":77058,\"start\":77051},{\"end\":77073,\"start\":77062}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":43942930},\"end\":58022,\"start\":57781},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":69981130},\"end\":58396,\"start\":58024},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":145976150},\"end\":58810,\"start\":58398},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":53950673},\"end\":59219,\"start\":58812},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":52073194},\"end\":59461,\"start\":59221},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":9263669},\"end\":59717,\"start\":59463},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":46849537},\"end\":60054,\"start\":59719},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":2352996},\"end\":60255,\"start\":60056},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":6073926},\"end\":60536,\"start\":60257},{\"attributes\":{\"id\":\"b9\"},\"end\":60725,\"start\":60538},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":493320},\"end\":60943,\"start\":60727},{\"attributes\":{\"id\":\"b11\"},\"end\":61233,\"start\":60945},{\"attributes\":{\"id\":\"b12\"},\"end\":61428,\"start\":61235},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2105943},\"end\":61754,\"start\":61430},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":62186022},\"end\":62039,\"start\":61756},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":126012070},\"end\":62340,\"start\":62041},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":8839886},\"end\":62579,\"start\":62342},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":5076623},\"end\":62945,\"start\":62581},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":121382884},\"end\":63313,\"start\":62947},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":45708458},\"end\":63718,\"start\":63315},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":126012070},\"end\":64019,\"start\":63720},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":5024784},\"end\":64371,\"start\":64021},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":49311462},\"end\":64685,\"start\":64373},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":167222154},\"end\":65062,\"start\":64687},{\"attributes\":{\"id\":\"b24\"},\"end\":65278,\"start\":65064},{\"attributes\":{\"doi\":\"Doi:10.1109/TIP.2020.2975984\",\"id\":\"b25\",\"matched_paper_id\":211677495},\"end\":65636,\"start\":65280},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":18649677},\"end\":65910,\"start\":65638},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":11925688},\"end\":66171,\"start\":65912},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":71142966},\"end\":66475,\"start\":66173},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":199688641},\"end\":66801,\"start\":66477},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":212708911},\"end\":67203,\"start\":66803},{\"attributes\":{\"doi\":\"Doi:10.1109/TIM.2020.3005230\",\"id\":\"b31\",\"matched_paper_id\":220280328},\"end\":67681,\"start\":67205},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":50786304},\"end\":68176,\"start\":67683},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":216738},\"end\":68678,\"start\":68178},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b34\"},\"end\":68935,\"start\":68680},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":206594692},\"end\":69335,\"start\":68937},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":9433631},\"end\":69772,\"start\":69337},{\"attributes\":{\"id\":\"b37\"},\"end\":70092,\"start\":69774},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":214195213},\"end\":70625,\"start\":70094},{\"attributes\":{\"doi\":\"Doi:10.1109/TPAMI.2020.3012548\",\"id\":\"b39\"},\"end\":71013,\"start\":70627},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":207761262},\"end\":71369,\"start\":71015},{\"attributes\":{\"id\":\"b41\"},\"end\":71790,\"start\":71371},{\"attributes\":{\"id\":\"b42\"},\"end\":72265,\"start\":71792},{\"attributes\":{\"id\":\"b43\"},\"end\":72405,\"start\":72267},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":231732518},\"end\":72767,\"start\":72407},{\"attributes\":{\"id\":\"b45\"},\"end\":72873,\"start\":72769},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":119424378},\"end\":73254,\"start\":72875},{\"attributes\":{\"id\":\"b47\"},\"end\":73431,\"start\":73256},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":119542121},\"end\":73647,\"start\":73433},{\"attributes\":{\"id\":\"b49\"},\"end\":74004,\"start\":73649},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":120241709},\"end\":74373,\"start\":74006},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":4828378},\"end\":74647,\"start\":74375},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":205432776},\"end\":74950,\"start\":74649},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":207925044},\"end\":75575,\"start\":74952},{\"attributes\":{\"doi\":\"arXiv:2005.13708v1\",\"id\":\"b54\"},\"end\":75873,\"start\":75577},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":15621045},\"end\":76220,\"start\":75875},{\"attributes\":{\"id\":\"b56\"},\"end\":76599,\"start\":76222},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":155100124},\"end\":76906,\"start\":76601},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":85454762},\"end\":77495,\"start\":76908}]", "bib_title": "[{\"end\":57826,\"start\":57781},{\"end\":58095,\"start\":58024},{\"end\":58520,\"start\":58398},{\"end\":58856,\"start\":58812},{\"end\":59289,\"start\":59221},{\"end\":59521,\"start\":59463},{\"end\":59799,\"start\":59719},{\"end\":60093,\"start\":60056},{\"end\":60306,\"start\":60257},{\"end\":60761,\"start\":60727},{\"end\":61495,\"start\":61430},{\"end\":61833,\"start\":61756},{\"end\":62126,\"start\":62041},{\"end\":62386,\"start\":62342},{\"end\":62659,\"start\":62581},{\"end\":63047,\"start\":62947},{\"end\":63454,\"start\":63315},{\"end\":63805,\"start\":63720},{\"end\":64086,\"start\":64021},{\"end\":64452,\"start\":64373},{\"end\":64738,\"start\":64687},{\"end\":65356,\"start\":65280},{\"end\":65691,\"start\":65638},{\"end\":65977,\"start\":65912},{\"end\":66254,\"start\":66173},{\"end\":66553,\"start\":66477},{\"end\":66908,\"start\":66803},{\"end\":67326,\"start\":67205},{\"end\":67749,\"start\":67683},{\"end\":68270,\"start\":68178},{\"end\":68981,\"start\":68937},{\"end\":69377,\"start\":69337},{\"end\":70218,\"start\":70094},{\"end\":71087,\"start\":71015},{\"end\":72466,\"start\":72407},{\"end\":72975,\"start\":72875},{\"end\":73286,\"start\":73256},{\"end\":73484,\"start\":73433},{\"end\":74093,\"start\":74006},{\"end\":74435,\"start\":74375},{\"end\":74737,\"start\":74649},{\"end\":75012,\"start\":74952},{\"end\":75950,\"start\":75875},{\"end\":76664,\"start\":76601},{\"end\":76960,\"start\":76908}]", "bib_author": "[{\"end\":57834,\"start\":57828},{\"end\":57843,\"start\":57834},{\"end\":57849,\"start\":57843},{\"end\":57857,\"start\":57849},{\"end\":57865,\"start\":57857},{\"end\":58103,\"start\":58097},{\"end\":58110,\"start\":58103},{\"end\":58119,\"start\":58110},{\"end\":58126,\"start\":58119},{\"end\":58132,\"start\":58126},{\"end\":58140,\"start\":58132},{\"end\":58529,\"start\":58522},{\"end\":58536,\"start\":58529},{\"end\":58544,\"start\":58536},{\"end\":58550,\"start\":58544},{\"end\":58560,\"start\":58550},{\"end\":58871,\"start\":58858},{\"end\":58880,\"start\":58871},{\"end\":58897,\"start\":58880},{\"end\":59297,\"start\":59291},{\"end\":59303,\"start\":59297},{\"end\":59309,\"start\":59303},{\"end\":59529,\"start\":59523},{\"end\":59537,\"start\":59529},{\"end\":59545,\"start\":59537},{\"end\":59551,\"start\":59545},{\"end\":59558,\"start\":59551},{\"end\":59808,\"start\":59801},{\"end\":59816,\"start\":59808},{\"end\":59824,\"start\":59816},{\"end\":59834,\"start\":59824},{\"end\":59844,\"start\":59834},{\"end\":59852,\"start\":59844},{\"end\":60106,\"start\":60095},{\"end\":60117,\"start\":60106},{\"end\":60123,\"start\":60117},{\"end\":60321,\"start\":60308},{\"end\":60327,\"start\":60321},{\"end\":60335,\"start\":60327},{\"end\":60346,\"start\":60335},{\"end\":60548,\"start\":60540},{\"end\":60556,\"start\":60548},{\"end\":60564,\"start\":60556},{\"end\":60570,\"start\":60564},{\"end\":60578,\"start\":60570},{\"end\":60769,\"start\":60763},{\"end\":60777,\"start\":60769},{\"end\":60783,\"start\":60777},{\"end\":61006,\"start\":60996},{\"end\":61016,\"start\":61006},{\"end\":61026,\"start\":61016},{\"end\":61038,\"start\":61026},{\"end\":61044,\"start\":61038},{\"end\":61299,\"start\":61292},{\"end\":61306,\"start\":61299},{\"end\":61312,\"start\":61306},{\"end\":61504,\"start\":61497},{\"end\":61511,\"start\":61504},{\"end\":61518,\"start\":61511},{\"end\":61525,\"start\":61518},{\"end\":61531,\"start\":61525},{\"end\":61537,\"start\":61531},{\"end\":61844,\"start\":61835},{\"end\":61850,\"start\":61844},{\"end\":61856,\"start\":61850},{\"end\":61863,\"start\":61856},{\"end\":62135,\"start\":62128},{\"end\":62141,\"start\":62135},{\"end\":62149,\"start\":62141},{\"end\":62395,\"start\":62388},{\"end\":62409,\"start\":62395},{\"end\":62417,\"start\":62409},{\"end\":62667,\"start\":62661},{\"end\":62676,\"start\":62667},{\"end\":63055,\"start\":63049},{\"end\":63064,\"start\":63055},{\"end\":63072,\"start\":63064},{\"end\":63079,\"start\":63072},{\"end\":63086,\"start\":63079},{\"end\":63463,\"start\":63456},{\"end\":63471,\"start\":63463},{\"end\":63478,\"start\":63471},{\"end\":63487,\"start\":63478},{\"end\":63814,\"start\":63807},{\"end\":63820,\"start\":63814},{\"end\":63828,\"start\":63820},{\"end\":64094,\"start\":64088},{\"end\":64103,\"start\":64094},{\"end\":64114,\"start\":64103},{\"end\":64460,\"start\":64454},{\"end\":64469,\"start\":64460},{\"end\":64482,\"start\":64469},{\"end\":64748,\"start\":64740},{\"end\":64757,\"start\":64748},{\"end\":65070,\"start\":65064},{\"end\":65079,\"start\":65070},{\"end\":65364,\"start\":65358},{\"end\":65373,\"start\":65364},{\"end\":65384,\"start\":65373},{\"end\":65700,\"start\":65693},{\"end\":65708,\"start\":65700},{\"end\":65718,\"start\":65708},{\"end\":65728,\"start\":65718},{\"end\":65986,\"start\":65979},{\"end\":65994,\"start\":65986},{\"end\":66002,\"start\":65994},{\"end\":66010,\"start\":66002},{\"end\":66262,\"start\":66256},{\"end\":66268,\"start\":66262},{\"end\":66277,\"start\":66268},{\"end\":66283,\"start\":66277},{\"end\":66292,\"start\":66283},{\"end\":66561,\"start\":66555},{\"end\":66570,\"start\":66561},{\"end\":66576,\"start\":66570},{\"end\":66584,\"start\":66576},{\"end\":66591,\"start\":66584},{\"end\":66597,\"start\":66591},{\"end\":66606,\"start\":66597},{\"end\":66916,\"start\":66910},{\"end\":66922,\"start\":66916},{\"end\":66931,\"start\":66922},{\"end\":66938,\"start\":66931},{\"end\":66950,\"start\":66938},{\"end\":67334,\"start\":67328},{\"end\":67343,\"start\":67334},{\"end\":67354,\"start\":67343},{\"end\":67759,\"start\":67751},{\"end\":67776,\"start\":67759},{\"end\":67789,\"start\":67776},{\"end\":67798,\"start\":67789},{\"end\":68289,\"start\":68272},{\"end\":68299,\"start\":68289},{\"end\":68312,\"start\":68299},{\"end\":68318,\"start\":68312},{\"end\":68692,\"start\":68680},{\"end\":68705,\"start\":68692},{\"end\":68989,\"start\":68983},{\"end\":68998,\"start\":68989},{\"end\":69005,\"start\":68998},{\"end\":69012,\"start\":69005},{\"end\":69388,\"start\":69379},{\"end\":69395,\"start\":69388},{\"end\":69413,\"start\":69395},{\"end\":69429,\"start\":69413},{\"end\":69882,\"start\":69873},{\"end\":69889,\"start\":69882},{\"end\":69896,\"start\":69889},{\"end\":69903,\"start\":69896},{\"end\":69911,\"start\":69903},{\"end\":69920,\"start\":69911},{\"end\":70229,\"start\":70220},{\"end\":70235,\"start\":70229},{\"end\":70243,\"start\":70235},{\"end\":70250,\"start\":70243},{\"end\":70256,\"start\":70250},{\"end\":70633,\"start\":70627},{\"end\":70639,\"start\":70633},{\"end\":70648,\"start\":70639},{\"end\":70655,\"start\":70648},{\"end\":70663,\"start\":70655},{\"end\":71097,\"start\":71089},{\"end\":71108,\"start\":71097},{\"end\":71120,\"start\":71108},{\"end\":71136,\"start\":71120},{\"end\":71381,\"start\":71371},{\"end\":71390,\"start\":71381},{\"end\":71402,\"start\":71390},{\"end\":71410,\"start\":71402},{\"end\":71420,\"start\":71410},{\"end\":71431,\"start\":71420},{\"end\":71441,\"start\":71431},{\"end\":71454,\"start\":71441},{\"end\":71801,\"start\":71792},{\"end\":71809,\"start\":71801},{\"end\":71816,\"start\":71809},{\"end\":71824,\"start\":71816},{\"end\":71836,\"start\":71824},{\"end\":72301,\"start\":72293},{\"end\":72479,\"start\":72468},{\"end\":72488,\"start\":72479},{\"end\":72501,\"start\":72488},{\"end\":72513,\"start\":72501},{\"end\":72793,\"start\":72787},{\"end\":72990,\"start\":72977},{\"end\":73005,\"start\":72990},{\"end\":73016,\"start\":73005},{\"end\":73298,\"start\":73288},{\"end\":73492,\"start\":73486},{\"end\":73501,\"start\":73492},{\"end\":73508,\"start\":73501},{\"end\":73816,\"start\":73805},{\"end\":74107,\"start\":74095},{\"end\":74117,\"start\":74107},{\"end\":74443,\"start\":74437},{\"end\":74451,\"start\":74443},{\"end\":74459,\"start\":74451},{\"end\":74745,\"start\":74739},{\"end\":74753,\"start\":74745},{\"end\":74759,\"start\":74753},{\"end\":74768,\"start\":74759},{\"end\":75025,\"start\":75014},{\"end\":75034,\"start\":75025},{\"end\":75047,\"start\":75034},{\"end\":75059,\"start\":75047},{\"end\":75074,\"start\":75059},{\"end\":75091,\"start\":75074},{\"end\":75099,\"start\":75091},{\"end\":75111,\"start\":75099},{\"end\":75122,\"start\":75111},{\"end\":75130,\"start\":75122},{\"end\":75583,\"start\":75577},{\"end\":75594,\"start\":75583},{\"end\":75603,\"start\":75594},{\"end\":75614,\"start\":75603},{\"end\":75958,\"start\":75952},{\"end\":75967,\"start\":75958},{\"end\":75973,\"start\":75967},{\"end\":75980,\"start\":75973},{\"end\":75988,\"start\":75980},{\"end\":75995,\"start\":75988},{\"end\":76230,\"start\":76222},{\"end\":76237,\"start\":76230},{\"end\":76245,\"start\":76237},{\"end\":76251,\"start\":76245},{\"end\":76257,\"start\":76251},{\"end\":76672,\"start\":76666},{\"end\":76679,\"start\":76672},{\"end\":76685,\"start\":76679},{\"end\":76693,\"start\":76685},{\"end\":76699,\"start\":76693},{\"end\":76707,\"start\":76699},{\"end\":76973,\"start\":76962},{\"end\":76982,\"start\":76973},{\"end\":76995,\"start\":76982},{\"end\":77007,\"start\":76995},{\"end\":77018,\"start\":77007},{\"end\":77031,\"start\":77018},{\"end\":77040,\"start\":77031},{\"end\":77049,\"start\":77040},{\"end\":77060,\"start\":77049},{\"end\":77075,\"start\":77060}]", "bib_venue": "[{\"end\":62741,\"start\":62724},{\"end\":67907,\"start\":67893},{\"end\":68439,\"start\":68387},{\"end\":69153,\"start\":69091},{\"end\":69570,\"start\":69508},{\"end\":70365,\"start\":70319},{\"end\":71516,\"start\":71497},{\"end\":71977,\"start\":71915},{\"end\":72595,\"start\":72558},{\"end\":75271,\"start\":75209},{\"end\":77216,\"start\":77154},{\"end\":57884,\"start\":57865},{\"end\":58202,\"start\":58140},{\"end\":58589,\"start\":58560},{\"end\":58997,\"start\":58897},{\"end\":59327,\"start\":59309},{\"end\":59576,\"start\":59558},{\"end\":59870,\"start\":59852},{\"end\":60142,\"start\":60123},{\"end\":60383,\"start\":60346},{\"end\":60596,\"start\":60578},{\"end\":60820,\"start\":60783},{\"end\":60994,\"start\":60945},{\"end\":61290,\"start\":61235},{\"end\":61554,\"start\":61537},{\"end\":61882,\"start\":61863},{\"end\":62178,\"start\":62149},{\"end\":62447,\"start\":62417},{\"end\":62722,\"start\":62676},{\"end\":63115,\"start\":63086},{\"end\":63501,\"start\":63487},{\"end\":63857,\"start\":63828},{\"end\":64173,\"start\":64114},{\"end\":64511,\"start\":64482},{\"end\":64852,\"start\":64757},{\"end\":65138,\"start\":65079},{\"end\":65449,\"start\":65412},{\"end\":65758,\"start\":65728},{\"end\":66028,\"start\":66010},{\"end\":66310,\"start\":66292},{\"end\":66624,\"start\":66606},{\"end\":66987,\"start\":66950},{\"end\":67434,\"start\":67382},{\"end\":67891,\"start\":67798},{\"end\":68385,\"start\":68318},{\"end\":68785,\"start\":68720},{\"end\":69089,\"start\":69012},{\"end\":69506,\"start\":69429},{\"end\":69871,\"start\":69774},{\"end\":70317,\"start\":70256},{\"end\":70810,\"start\":70693},{\"end\":71173,\"start\":71136},{\"end\":71495,\"start\":71454},{\"end\":71913,\"start\":71836},{\"end\":72291,\"start\":72267},{\"end\":72556,\"start\":72513},{\"end\":72785,\"start\":72769},{\"end\":73049,\"start\":73016},{\"end\":73332,\"start\":73298},{\"end\":73527,\"start\":73508},{\"end\":73803,\"start\":73649},{\"end\":74176,\"start\":74117},{\"end\":74496,\"start\":74459},{\"end\":74786,\"start\":74768},{\"end\":75207,\"start\":75130},{\"end\":75702,\"start\":75632},{\"end\":76032,\"start\":75995},{\"end\":76402,\"start\":76257},{\"end\":76738,\"start\":76707},{\"end\":77152,\"start\":77075}]"}}}, "year": 2023, "month": 12, "day": 17}