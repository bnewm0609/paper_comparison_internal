{"id": 206596482, "updated": "2023-09-29 10:21:11.156", "metadata": {"title": "CNN-SLAM: Real-time dense monocular SLAM with learned depth prediction", "authors": "[{\"first\":\"Keisuke\",\"last\":\"Tateno\",\"middle\":[]},{\"first\":\"Federico\",\"last\":\"Tombari\",\"middle\":[]},{\"first\":\"Iro\",\"last\":\"Laina\",\"middle\":[]},{\"first\":\"Nassir\",\"last\":\"Navab\",\"middle\":[]}]", "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2017, "month": 4, "day": 11}, "abstract": "Given the recent advances in depth prediction from Convolutional Neural Networks (CNNs), this paper investigates how predicted depth maps from a deep neural network can be deployed for accurate and dense monocular reconstruction. We propose a method where CNN-predicted dense depth maps are naturally fused together with depth measurements obtained from direct monocular SLAM. Our fusion scheme privileges depth prediction in image locations where monocular SLAM approaches tend to fail, e.g. along low-textured regions, and vice-versa. We demonstrate the use of depth prediction for estimating the absolute scale of the reconstruction, hence overcoming one of the major limitations of monocular SLAM. Finally, we propose a framework to efficiently fuse semantic labels, obtained from a single frame, with dense SLAM, yielding semantically coherent scene reconstruction from a single view. Evaluation results on two benchmark datasets show the robustness and accuracy of our approach.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1704.03489", "mag": "2952280228", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/TatenoTLN17", "doi": "10.1109/cvpr.2017.695"}}, "content": {"source": {"pdf_hash": "53c9d4729c8a40ee32f2452ff6b8023b5de48f0e", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1704.03489v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1704.03489", "status": "GREEN"}}, "grobid": {"id": "d8f7ec6fde745777b239339d5a139cc60198794d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/53c9d4729c8a40ee32f2452ff6b8023b5de48f0e.txt", "contents": "\nCNN-SLAM: Real-time dense monocular SLAM with learned depth prediction\n\n\nKeisuke Tateno tateno@in.tum.de \nCAMP -TU Munich 2 Canon Inc\n\n\nFederico Tombari tombari@in.tum.de \nCAMP -TU Munich 2 Canon Inc\n\n\nIro Laina laina@in.tum.de \nCAMP -TU Munich 2 Canon Inc\n\n\nNassir Navab navab@in.tum.de \nCAMP -TU Munich 2 Canon Inc\n\n\nJohns Hopkins University Munich\nGermany Tokyo, BaltimoreUSJapan\n\nCNN-SLAM: Real-time dense monocular SLAM with learned depth prediction\n\nGiven the recent advances in depth prediction from Convolutional Neural Networks (CNNs), this paper investigates how predicted depth maps from a deep neural network can be deployed for accurate and dense monocular reconstruction. We propose a method where CNN-predicted dense depth maps are naturally fused together with depth measurements obtained from direct monocular SLAM. Our fusion scheme privileges depth prediction in image locations where monocular SLAM approaches tend to fail, e.g. along low-textured regions, and vice-versa. We demonstrate the use of depth prediction for estimating the absolute scale of the reconstruction, hence overcoming one of the major limitations of monocular SLAM. Finally, we propose a framework to efficiently fuse semantic labels, obtained from a single frame, with dense SLAM, yielding semantically coherent scene reconstruction from a single view. Evaluation results on two benchmark datasets show the robustness and accuracy of our approach.\n\nIntroduction\n\nStructure-from-Motion (SfM) and Simultaneous Localization and Mapping (SLAM) are umbrella names for a highly active research area in the field of computer vision and robotics for the goal of 3D scene reconstruction and camera pose estimation from 3D and imaging sensors. Recently, real-time SLAM methods aimed at fusing together range maps obtained from a moving depth sensor have witnessed an increased popularity, since they can be employed for navigation and mapping of several types of autonomous agents, from mobile robots to drones, as well as for many augmented reality and computer graphics applications. This is the case of volumetric fusion approaches such as Kinect Fusion [21], as well as dense SLAM methods based on RGB-D data [30,11], which, in addition to navigation and mapping, can also be employed for accurate scene recon- * The first two authors contribute equally to this paper.\n\n\nScale\n\nIncorrect (c) Our Joint 3D and Semantic Reconstruction (b) LSD-SLAM [3] (a) Proposed method Figure 1. The proposed monocular SLAM approach (a) can estimate a much better absolute scale than the state of the art (b), which is necessary for many SLAM applications such as AR, e.g. the skeleton is augmented into the reconstruction. c) our approach can yield joint 3D and semantic reconstruction from a single view. struction. However, a main drawback of such approaches is that depth cameras have several limitations: indeed, most of them have a limited working range, and those based on active sensing cannot work (or perform poorly) under sunlight, thus making reconstruction and mapping less precise if not impossible in outdoor environments.\n\nIn general, since depth cameras are not as ubiquitous as color cameras, a lot of research interest has been focused on dense and semi-dense SLAM methods from a single camera [22,4,20]. These approaches aim at real-time monocular scene reconstruction by estimating the depth map of the current viewpoint through small-baseline stereo matching over pairs of nearby frames. The working assumption is that the camera translates in space over time, so that pairs of consecutive frames can be treated as composing a stereo rig. Stereo matching is usually carried out through color consistency or by relying on keypoint extraction and matching.\n\nOne main limitation of monocular SLAM approaches is the estimation of the absolute scale. Indeed, even if camera pose estimation and scene reconstruction are carried out accurately, the absolute scale of such reconstruction remains inherently ambiguous, limiting the use of monocular SLAM within most aforementioned applications in the field of augmented reality and robotics (an example is shown in Fig.  1,b). Some approaches suggest solving the issue via object detection by matching the scene with a pre-defined set of 3D models, so to recover the initial scale based on the estimated object size [6], which nevertheless fails in absence of known shapes in the scene. Another main limitation of monocular SLAM is represented by pose estimation under pure rotational camera motion, in which case stereo estimation cannot be applied due to the lack of a stereo baseline, resulting in tracking failures.\n\nRecently, a new avenue of research has emerged that addresses depth prediction from a single image by means of learned approaches. In particular, the use of deep Convolutional Neural Networks (CNNs) [16,2,3] in an end-to-end fashion has demonstrated the potential of regressing depth maps at a relatively high resolution and with a good absolute accuracy even under the absence of monocular cues (texture, repetitive patterns) to drive the depth estimation task. One advantage of deep learning approaches is that the absolute scale can be learned from examples and thus predicted from a single image without the need of scene-based assumptions or geometric constraints, unlike [10,18,1]. A major limitation of such depth maps is the fact that, although globally accurate, depth borders tend to be locally blurred: hence, if such depths are fused together for scene reconstruction as in [16], the reconstructed scene will overall lack shape details.\n\nRelevantly, despite the few methods proposed for single view depth prediction, the application of depth prediction to higher-level computer vision tasks has been mostly overlooked so far, with just a few examples existing in literature [16]. The main idea behind this work is to exploit the best from both worlds and propose a monocular SLAM approach that fuses together depth prediction via deep networks and direct monocular depth estimation so to yield a dense scene reconstruction that is at the same time unambiguous in terms of absolute scale and robust in terms of tracking. To recover blurred depth borders, the CNNpredicted depth map is used as initial guess for dense reconstruction and successively refined by means of a direct SLAM scheme relying on small-baseline stereo matching similar to the one in [4]. Importantly, small-baseline stereo matching holds the potential to refine edge regions on the predicted depth image, which is where they tend to be more blurred. At the same time, the initial guess obtained from the CNN-predicted depth map can provide absolute scale information to drive pose estimation, so that the estimated pose trajectory and scene reconstruction can be significantly more accurate in terms of absolute scale compared to the state of the art in monocular SLAM. Fig. 1, a) shows an example illustrating the usefulness of carrying out scene reconstruction with a precise absolute scale such as the one proposed in this work. Moreover, tracking can be made more robust, as the CNN-predicted depth does not suffer from the aforementioned problem of pure rotations, as it is estimated on each frame individually. Last but not least, this framework can run in real-time since the two processes of depth prediction from CNNs and depth refinement can be simultaneously carried out on different computational resources of the same architecture -respectively, the GPU and the CPU.\n\nAnother relevant aspect of recent CNNs is that the same network architecture can be successfully employed for different high-dimensional regression tasks rather than just depth estimation: one typical example is semantic segmentation [3,29]. We leverage this aspect to propose an extension of our framework that uses pixel-wise labels to coherently and efficiently fuse semantic labels with dense SLAM, so to attain semantically coherent scene reconstruction from a single view: an example is shown in Fig. 1, c). Notably, to the best of our knowledge semantic reconstruction has been shown only recently and only based on stereo [28] or RGB-D data [15], i.e. never in the monocular case.\n\nWe validate our method with a comparison on two public SLAM benchmarks against the state of the art in monocular SLAM and depth estimation, focusing on the accuracy of pose estimation and reconstruction. Since the CNNpredicted depth relies on a training procedure, we show experiments where the training set is taken from a completely different environment and a different RGB sensor than those available in the evaluated benchmarks, so to portray the capacity of our approach -particularly relevant for practical uses -to generalize to novel, unseen environments. We also show qualitative results of our joint scene reconstruction and semantic label fusion in a real environment.\n\n\nRelated work\n\nIn this Section we review related work with respect to the two fields that we integrate within our framework, i.e. SLAM and depth prediction.\n\nSLAM There exists a vast literature on SLAM. From the point of view of the type of input data being processed, approaches can be classified into either depth camera-based [21,30,11] and monocular camera-based [22,4,20]. Instead, from a methodological viewpoint, they are classified as either feature-based [12,13,20] and direct [22,5,4]. Given the scope of this paper, we will focus here only on monocular SLAM approaches.\n\nAs for feature-based monocular SLAM, ORB-SLAM [20] is arguably the state of the art in terms of pose estimation accuracy. This method relies on the extraction of sparse ORB features from the input image to carry out a sparse reconstruction of the scene as well as to estimate the camera pose, also employing local bundle adjustment and pose graph optimization. As for direct monocular SLAM, the Dense Tracking and Mapping (DTAM) of [22] achieved dense reconstruction in real-time on s GPU by using shortbaseline multiple-view stereo matching with a regularization scheme, so that depth estimation is smoother on lowtextured regions in the color image. Moreover, the Large-Scale Direct SLAM (LSD-SLAM) algorithm [4] proposed the use of a semi-dense map representation which keeps track of depth values only on gradient areas of the input image, this allowing enough efficiency to enable direct SLAM in real-time on a CPU. An extension of LSD-SLAM is the recent Multi-level mapping (MLM) algorithm [7], which proposed the use of a dense approach on top of LSD-SLAM in order to increase its density and improve the reconstruction accuracy.\n\nDepth prediction from single view Depth prediction from single view has gained increasing attention in the computer vision community thanks to the recent advances in deep learning. Classic depth prediction approaches employ hand-crafted features and probabilistic graphical models [10,18] to yield regularized depth maps, usually making strong assumptions on the scene geometry. Recently developed deep convolutional architectures significantly outperformed previous methods in terms of depth estimation accuracy [16,2,3,29,19,17]. Interestingly, the work of [16] reports qualitative results of employing depth predictions for dense SLAM as an application. In particular, the predicted depth map is used as input for Keller's Point-Based Fusion RGB-D SLAM algorithm [11], showing that SLAM-based scene reconstruction can be obtained using depth prediction, although it lacks shape details, mostly due to the aforementioned blurring artifacts that are associated with the loss of fine spatial information through the contractive part of a CNN.\n\n\nProposed Monocular Semantic SLAM\n\nIn this section, we illustrate the proposed framework for 3D reconstruction, where CNN-predicted dense depth maps are fused together with depth measurements obtained from direct monocular SLAM. Additionally, we show how CNN-predicted semantic segmentation can also be coherently fused with the global reconstruction model. The flow diagram in Fig. 2 sketches the pipeline of our framework. We employ a key-frame based SLAM paradigm [12,4,20], in particular we use as baseline the direct semi-dense approach in [4]. Within such approach, a subset of visually distinct frames is collected as key-frames, whose pose is subject to global refinement based on pose graph optimization. At the same time, camera pose estimation is carried out at each input frame, by estimating the transformation between the frame and its nearest key-frame.\n\nTo maintain a high frame-rate, we propose to predict a depth map via CNN only on key-frames. In particular, if the currently estimated pose is far from that of existing keyframes, a new key-frame is created out of the current frame and its depth estimated via CNN. Moreover an uncertainty map is constructed by measuring the pixel-wise confidence of each depth prediction. Since in most cases the camera used for SLAM differs from the one used to acquire the dataset on which the CNN is trained, we propose a specific normalization procedure of the depth map designed to gain robustness towards different intrinsic camera parameters. When additionally carrying out semantic label fusion, we employ a second convolutional network to predict a semantic segmentation of the input frame. Finally, a pose graph on key-frames is created so to globally optimize their relative pose.\n\nA particularly important stage of the framework, also representing one main contribution of our proposal, is the scheme employed to refine the CNN-predicted depth map associated to each key-frame via small-baseline stereo matching, by enforcing color consistency minimization between a key-frame and associated input frames. In particular, depth values will be mostly refined around image regions with gradients, i.e. where epipolar matching can provide improved accuracy. This will be outlined in Subsections 3.3 and 3.4. Relevantly, the way refined depths are propagated is driven by the uncertainty associated to each depth value, estimated according to a specifically proposed confidence measure (defined in Subsec. 3.3). Every stage of the framework is now detailed in the following Subsections.\n\n\nCamera Pose Estimation\n\nThe camera pose estimation is inspired by the key-frame approach in [4]. In particular, the system holds a set of keyframes k 1 , .., k n \u2208 K as structural elements on which to perform SLAM reconstruction. Each key-frame k i is associated to a key-frame pose T ki , a depth map D ki , and a depth uncertainty map U ki . In contrast to [4], our depth map is dense because it is generated via CNN-based depth prediction (see Subsec. 3.2). The uncertainty map measures the confidence of each depth value. As opposed to [4] that initializes the uncertainty to a large, constant value, our approach initializes it according to the measured confidence of the depth prediction (described in Subsec. 3.3). In the following, we will refer to a generic depth map element as u = (x, y), which ranges in the image domain, i.e. u \u2208 \u2126 \u2282 R 2 , withu being its homogeneous representation.\n\nAt each frame t, we aim to estimate the current camera pose (3), i.e. the transformation between the nearest key-frame k i and frame t, composed of a 3\u00d73 rotation matrix R t \u2208 SO(3) and a 3D translation vector t t \u2208 R 3 . This transformation is estimated by minimizing the photometric residual between the intensity image I t of the current frame and the intensity image I ki of the nearest key-frame k i , via weighted Gauss-Newton optimization based on the objective function\nT ki t = [R t , t t ] \u2208 SEE(T ki t ) = \u0169\u2208\u2126 \u03c1 \uf8eb \uf8ed r \u0169, T ki t \u03c3 r \u0169, T ki t \uf8f6 \uf8f8 (1)\nwhere \u03c1 is the Huber norm and \u03c3 is a function measuring the residual uncertainty [4]. Here, r is the photometric residual defined as\nr \u0169, T ki t = I ki (\u0169) \u2212 I t \u03c0 KT ki t\u1e7cki (\u0169) .(2)\nConsidering that our depth map is dense, for the sake of efficiency, we limit the computation of the photometric residual only on the subset of pixels lying within high color gradient regions, defined by the image domain subset\u0169 \u2282 u \u2208 \u2126. Also, in (2), \u03c0 represents the perspective projection function mapping a 3D point to a 2D image coordinate\n\u03c0 [xyz] T = (x/z, y/z) T(3)\nwhile V ki (u) represents a 3D element of the vertex map computed from the key-frame's depth map\nV ki (u) = K \u22121u D ki (u)(4)\nwhere K is the camera intrinsic matrix. Once T ki t is obtained, the current camera pose in world coordinate system is computed as T t = T ki t T ki .\n\n\nCNN-based Depth Prediction and Semantic Segmentation\n\nEvery time a new key-frame is created, an associated depth map is predicted via CNN. The depth prediction architecture that we employ is the state-of-the-art approach proposed in [16], based on the extension of the Residual Network (ResNet) architecture [9] to a Fully Convolutional network. In particular, the first part of the architecture is based on ResNet-50 [9] and initialized with pre-trained weights on ImageNet [24]. The second part of the architecture replaces the last pooling and fully connected layers originally presented in ResNet-50 with a sequence of residual up-sampling blocks composed of a combination of unpooling and convolutional layers. After up-sampling, drop-out is applied before a final convolutional layer which outputs a 1-channel output map representing the predicted depth map. The loss function is based on the reverse Huber function [16].\n\nFollowing the successful paradigm of other approaches that employed the same architecture for both depth prediction and semantic segmentation tasks [3,29], we also retrained this network for predicting pixel-wise semantic labels from RGB images. To deal with this task, we modified the network so that it has as many output channels as the number of categories and employed a soft-max layer and a cross-entropy loss function to be minimized via backpropagation and Stochastic Gradient Descent (SGD). It is important to point out that, although in principle any semantic segmentation algorithm could be used, the primary objective of this work is to showcase how frame-wise segmentation maps can be successfully fused within our monocular SLAM framework (see Subsec. 3.5).\n\n\nKey-frame Creation and Pose Graph Optimization\n\nOne limitation of using a pre-trained CNN for depth prediction is that, if the sensor used for SLAM has different intrinsic parameters from those used to capture the training set, the resulting absolute scale of the 3D reconstruction will be inaccurate. To ameliorate this issue, we propose to adjust the depth regressed via CNN with the ratio between the focal length of the current camera, f cur and that of the sensor used for training, f tr as\nD ki (u) = f cur f trD ki (u)(5)\nwhereD ki is the depth map directly regressed by the CNN from the current key-frame image I i . Fig. 3 shows the usefulness of the adjustment procedure defined in (5), on a sequence of the benchmark ICL-NUIM dataset [8] (compare (a) with (b) ). As shown, the performance after the adjustment procedure is significantly improved over that of using the depth map as directly predicted by the CNN. The improvement shows both in terms of depth accuracy as well as pose trajectory accuracy. In addition, we associate each depth map D ki to an uncertainty map U ki . In [4], this map is initialized by setting each element to a large, constant value. Since the CNN provides us with dense maps at each frame but without relying on any temporal regularization, we propose to instead initialize our uncertainty map by computing a confidence value based on the difference between the current depth map and its respective scene point on the nearest keyframe. Thus, this confidence measures how coherent each predicted depth value is across different frames: for those elements associated to a high confidence, the successive refinement process will be much faster and effective than the one in [4].\n\nSpecifically, the uncertainty map U ki is defined as the element-wise squared difference between the depth map of the current key-frame k i and that of the nearest key-frame k j , warped according to the estimated transformation T ki\nkj from k i to k j U ki (u) = D ki (u) \u2212 D kj \u03c0 KT ki kj V ki (u) 2 .(6)\nTo further improve the accuracy of each newly initialized key-frame, we propose to fuse its depth map and uncertainty map with those propagated from the nearest key-frame (this will obviously not apply to the very first key-frame) after they have been refined with new input frames (the depth refinement process is described in Subsection 3.4). To achieve this goal, we first define a propagated uncertainty map from the nearest key-frame k j as\nU kj (v) = D kj (v) D ki (u) U kj (v) + \u03c3 2 p(7)\nwhere v = \u03c0 KT ki kj V ki (u) while, following [4], \u03c3 2 p is the white noise variance used to increase the propagated uncertainty. Then, the two depth maps and uncertainty maps are fused together according to the weighted scheme\nD ki (u) =\u0168 k j (v)\u00b7D k i (u)+U k i (u)\u00b7D k j (v) U k i (u)+\u0168 k j (v) (8) U ki (u) =\u0168 k j (v)\u00b7U k i (u) U k i (u)+\u0168 k j (v) .(9)\nFinally, the pose graph is also updated at each new keyframe, by creating new edges with the key-frames already present in the graph that share a similar field of view (i.e., having a small relative pose) with the newly added keyframe. Moreover, the pose of the key-frames is each time globally refined via pose graph optimization [14].\n\n\nFrame-wise Depth Refinement\n\nThe goal of this stage is to continuously refine the depth map of the currently active key-frame based on the depth maps estimated at each new frame. To achieve this goal, we use the small baseline stereo matching strategy described in the semi-dense scheme of [5], by computing at each pixel of the current frame t a depth map D t and an uncertainty map U t based on the 5-pixel matching along the epipolar line. These two maps are aligned with the key-frame k i based on the estimated camera pose T ki t . The estimated depth map and uncertainty map are then directly fused with those of the nearest key-frame k i as follows:\nD ki (u) = Ut(u)\u00b7D k i (u)+U k i (u)\u00b7Dt(u) U k i (u)+Ut(u) (10) U ki (u) = Ut(u)\u00b7U k i (u) U k i (u)+Ut(u)(11)\nImportantly, since the key-frame is associated to a dense depth map thanks to the proposed CNN-based prediction, this process can be carried out densely, i.e. every element of the key-frame is refined, in contrast to [5] that only refines depth values along high gradient regions. Since the observed depths within low-textured regions tend to have a high-uncertainty (i.e., a high value in U t ), the proposed approach will naturally lead to a refined depth map where elements in proximity of high intensity gradients will be refined by the depth estimated at each frame, while elements within more and more low-textured regions will gradually hold the predicted depth value from the CNN, without being affected from uncertain depth observations. Fig. 3 demonstrates the effectiveness of the proposed depth map refinement procedure on a sequence of the benchmark ICL-NUIM dataset [8]. The Figure reports, in (c), the performance obtained after both adjustment and depth refinement of the depth map, showing a significant improvement of both depth estimation and pose trajectory with respect to the previous cases.\n\n\nGlobal Model and Semantic Label Fusion\n\nThe obtained set of key-frames can be fused together to generate a 3D global model of the reconstructed scene. Since the CNN is trained to provide semantic labels in addition to depth maps, semantic information can be also associated to each element of the 3D global model, through a process that we denote as semantic label fusion.\n\nIn our framework, we employ the real-time scheme proposed in [27], which aims at incrementally fusing together the depth map and the connected component map obtained from each frame of a RGB-D sequence. This approach uses a Global Segmentation Model (GSM) to average the assignment of labels to each 3D element over time, so to be robust to noise in the frame-wise segmentation. In our case, the pose estimation is provided as input to the algorithm, since camera poses are estimated via monocular SLAM, while input depth maps are those associated to the set of collected key-frames only. Here, instead of connected component maps as in [27], we use semantic segmentation maps. The result is a 3D reconstruction of the scene, incrementally built over new key-frames, where each 3D element is associated to a semantic class from the set used to train the CNN.\n\n\nEvaluation\n\nWe provide here an experimental evaluation to validate the contributions of our method in terms of tracking and reconstruction accuracy, by means of a quantitative comparison against the state of the art on two public benchmark datasets (Subsec. 4.1), as well as a qualitative assessment in terms of robustness against pure rotational camera motions (Subsec. 4.2) and accuracy of semantic label fusion (Subsec. 4.3).\n\nThe evaluation is carried out on a desktop PC with an Intel Xeon CPU at 2.4GHz with 16GB of RAM and a Nvidia Quadro K5200 GPU with 8GB of VRAM. As for the implementation of our method, although the CNN network works on an input/output resolution of 304\u00d7228 [16], both the input frame and the predicted depth map are converted to 320\u00d7240 as input for all other stages. Also, the CNNbased depth prediction and semantic segmentation are run on the GPU, while all other stages are implemented on the CPU, and run on two different CPU threads, one devoted to frame-wise processing stages (camera pose estimation and depth refinement), the other carrying out key-frame related processing stages (key-frame initialization, pose graph optimization and global map and semantic label fusion), so to allow our entire framework to run in real-time.\n\nWe use sequences from two public benchmark datasets, i.e. the ICL-NUIM dataset [8] and TUM RGB-D SLAM dataset [26], the former synthetic, the latter acquired with a Kinect sensor. Both datasets provide ground truth in terms of camera trajectory and depth maps. In all our experiments, we used the CNN model trained on the indoor sequences of the NYU Depth v2 dataset [25], to test the generalization capability of the network to unseen environments; also because this dataset includes both depth ground-truth (represented by depth maps acquired with a Microsoft Kinect camera) and pixel-wise semantic label annotations, necessary for semantic label fusion. In particular, we train the semantic segmentation network on the official train split of the labeled subset, while the depth network is trained using more frames from the raw NYU dataset, as reported in [16]. Semantic annotations consist of the 4 super-classes floor, vertical structure, large structure/furniture, small structure. Noteworthy, the settings of the training dataset are quite different from those on which we evaluate our method, since they encompass different camera sensors, viewpoints and scene layouts. For example, NYU Depth v2 includes many living rooms, kitchens and bedrooms, which are missing in TUM RGB-D SLAM, being focused on office rooms with desks, objects and people.\n\n\nComparison against SLAM state of the art\n\nWe compare our approach against the publicly available implementations of LSD-SLAM 1 [4] and ORB-SLAM 2 [20], two state-of-the-art methods in monocular SLAM representatives of, respectively, direct and feature-based methods. For completeness, we also compare against REMODE [23], state-of-the-art approach focused on dense monocular depth map estimation. The implementation of REMODE has been taken from the author's code 3 . Finally, we also compare our method to the one in [16], that uses the CNNpredicted depth maps as input for a state-of-the-art depthbased SLAM method (point-based fusion [11,27]), based on the available implementation from the authors of [27] 4 . Given the ambiguity of monocular SLAM approaches to estimate absolute scale, we also evaluate LSD-SLAM by bootstrapping its initial scale using the ground-truth depth map, as done in the evaluation in [4,20]. As for REMODE, since it requires as input the camera pose estimation at each frame, we use the trajectory and key-frames estimated by LSD-SLAM with bootstrapping.\n\nFollowing the evaluation methodology proposed in [26], Table 1 reports the camera pose accuracy based on the Absolute Trajectory Error (ATE), computed as the root mean  Figure 4. Comparison in terms of depth map accuracy and density among (from the left) the ground-truth, a refined key-frame from our approach, the corresponding raw depth prediction from the CNN, the refined key-frame from LSD-SLAM [4] with bootstrapping and estimated dense depth map from REMODE [23], on the (office2) sequence from the ICL-NUIM dataset [8]. The accuracy value means correctly estimated depth density on this key-frame.\n\nsquare error between the estimated camera translation and the ground-truth camera translation for each evaluated sequence. In addition, we assess both reconstruction accuracy and density, by evaluating the percentage of depth values whose difference with the corresponding ground truth depth is less than 10%. Given the observations in the Table, our approach is able to always report a much higher pose trajectory accuracy with respect to monocular methods, due to the their aforementioned absolute scale ambiguity. Interestingly, the pose accuracy of our technique is on average higher than that of LSD-SLAM even after applying bootstrapping, implying an inherent effectiveness of the proposed depth fusion approach rather than just estimating the correct scaling factor. The same benefits are present in terms of reconstruction, being the estimated key-frames not only dramatically more accurate, but also much denser than those reported by LSD-SLAM and ORB-SLAM. Moreover, our approach also reports a better performance in terms of both pose and reconstruction accuracy, also comparing to the technique in [16], where CNN-predicted depths are used as input for SLAM without any refinement, this again demonstrating the effectiveness of the proposed scheme to refine the blurred edges and wrongly estimated depth values predicted by the CNN. Finally, we clearly outperform also REMODE in terms of depth map accuracy. The increased accuracy with respect to the depth maps estimated by the CNN (as employed in [16]) and by RE-MODE, as well as the higher density with respect to LSD-SLAM is also shown in Fig. 4. The figure compares the ground-truth with, a refined key-frame using our approach, the corresponding raw depth prediction from the CNN, the refined key-frame from LSD-SLAM [4] using bootstrapping and the estimated dense depth map from REMODE on a sequence of the ICL-NUIM dataset. Not only our approach demonstrates a much higher density with respect to LSD-SLAM, but the refinement procedure helps to drastically reduce the blurring artifacts of the CNN-based prediction, increasing the overall depth accuracy. Also, we can note that REMODE tends to fail along low-textured regions, as opposed to our method which can estimate depth densely over such areas by leveraging the CNN-predicted depth values.\n\n\nAccuracy under pure rotational motion\n\nAs mentioned, one of the advantages of our approach compared to standard monocular SLAM is that, under pure rotational motion, the reconstruction can still be obtained by relying on CNN-predicted depths, while other methods would fail given the absence of a stereo baseline between consecutive frames. To portray this benefit, we evaluate our method on the (fr1/rpy) sequence from the TUM dataset, mostly consisting of just rotational camera motion. The reconstruction obtained by, respectively, our approach and LSD-SLAM compared to ground-truth are shown in Fig-Ours LSD-SLAM Ground Truth Figure 5. Comparison on a sequence that includes mostly pure rotational camera motion between the reconstruction obtained by ground truth depth (left), proposed method (middle) and LSD-SLAM [4] (right).\n\n:Vertical structure/Wall :Small structure :Floor :Large structure/furniture Figure 6. The results of reconstruction and semantic label fusion on the office sequence (top, acquire by our own) and one sequence (kitchen 0046) from the NYU Depth V2 dataset [25] (bottom). Reconstruction is shown with colors (left) and with semantic labels (right). ure 5. As it can be seen, our method can reconstruct the scene structure even if the camera motion is purely rotational, while the result of LSD-SLAM is significantly noisy, since the stereo baseline required to estimate depth is for most frames not sufficient. We also tried ORB-SLAM on this sequence but it completely fails, given the lack of the necessary baseline to initialize the algorithm.\n\n\nJoint 3D and semantic reconstruction\n\nFinally, we show some qualitative results of the joint 3D and semantic reconstruction achieved by our method. Three examples are shown in Fig. 6, which reports an office scene reconstructed from a sequence acquired with our own setup and two sequences from the test set of the NYU Depth V2 dataset [25]. Another example from the sequence living0 of the ICL-NUIM dataset is shown in Fig.1,c). The Figures also report, in green, the estimated camera trajectory. To the best of our knowledge, this is the first demonstration of joint 3D and semantic reconstruction with a monocular camera. Additional qualitative results in terms of pose and reconstruction quality as well as semantic label fusion are included in the supplementary material.\n\n\nConclusion\n\nWe have shown how the integration of SLAM with depth prediction via a deep neural network is a promising direction to solve inherent limitations of traditional monocular reconstruction, especially with respect to estimating the absolute scale, obtaining dense depths along texture-less regions and dealing with pure rotational motions. The proposed approach to refine CNN-predicted depth maps with small baseline stereo matching naturally overcomes these issues while retaining the robustness and accuracy of direct monocular SLAM in presence of camera translations and high image gradients. The overall framework is capable of jointly reconstructing the scene while fusing semantic segmentation labels with the global 3D model, opening new perspectives towards scene understanding with a monocular camera. A future research avenue is represented by closing the loop with depth prediction, i.e. improving depth estimation by means of geometrically refined depth maps.\n\nFigure 2 .\n2CNN-SLAM Overview.\n\nFigure 3 .\n3Comparison among (a) direct CNN-depth prediction, (b) after depth adjustment and (c) after depth adjustment and refinement, in terms of (A) pose trajectory accuracy and (B) depth estimation accuracy. Blue pixels depict correctly estimated depths, i.e. within 10 % of ground-truth. The comparison is done on one sequence of the ICL-NUIM dataset[8].\n\nTable 1 .\n1Comparison in terms of Absolute Trajectory Error [m] and percentage of correctly estimated depth on ICL-NUIM and TUM datasets (TUM/seq1: fr3/long office household, TUM/seq2: fr3/nostructure texture near withloop, TUM/seq3: fr3/structure texture far.Abs. Trajectory Error \nPerc. Correct Depth \nOur \nLSD-BS LSD ORB Laina \nOur \nLSD-BS LSD ORB \nLaina \nRemode \nMethod \n[4] \n[4] \n[20] \n[16] \nMethod \n[4] \n[4] \n[20] \n[16] \n[23] \nICL/office0 \n0.266 \n0.587 \n0.528 0.430 0.337 19.410 \n0.603 \n0.335 0.018 17.194 \n4.479 \nICL/office1 \n0.157 \n0.790 \n0.768 0.780 0.218 29.150 \n4.759 \n0.038 0.023 20.838 \n3.132 \nICL/office2 \n0.213 \n0.172 \n0.794 0.860 0.509 37.226 \n1.435 \n0.078 0.040 30.639 16.7081 \nICL/living0 \n0.196 \n0.894 \n0.516 0.493 0.230 \n12.840 \n1.443 \n0.360 0.027 15.008 \n4.479 \nICL/living1 \n0.059 \n0.540 \n0.480 0.129 0.060 13.038 \n3.030 \n0.057 0.021 11.449 \n2.427 \nICL/living2 \n0.323 \n0.211 \n0.667 0.663 0.380 \n26.560 \n1.807 \n0.167 0.014 33.010 \n8.681 \nTUM/seq1 \n0.542 \n1.717 \n1.826 1.206 0.809 \n12.477 \n3.797 \n0.086 0.031 12.982 \n9.548 \nTUM/seq2 \n0.243 \n0.106 \n0.436 0.495 1.337 24.077 \n3.966 \n0.882 0.059 15.412 \n12.651 \nTUM/seq3 \n0.214 \n0.037 \n0.937 0.733 0.724 27.396 \n6.449 \n0.035 0.027 \n9.450 \n6.739 \nAvg. \n0.246 \n0.562 \n0.772 0.643 0.512 22.464 \n3.032 \n0.226 0.029 18.452 \n7.649 \n\nOurs \nREMODE \nGround Truth \nRaw Depth Prediction \n\nAccuracy: \n57.15% \n66.18% \n12.26% \n\nLSD-SLAM \n\n11.91% \n\nColor \n\n\ngithub.com/tum-vision/lsd_slam 2 github.com/raulmur/ORB_SLAM2 3 https://www.github.com/uzh-rpg/rpg_open_remode 4 campar.in.tum.de/view/Chair/ProjectInSeg\n\nA dynamic bayesian network model for autonomous 3d reconstruction from a single indoor image. E Delage, H Lee, A Y Ng, Proc. Int. Conf. on Computer Vision and Pattern Recognition (CVPR). Int. Conf. on Computer Vision and Pattern Recognition (CVPR)E. Delage, H. Lee, and A. Y. Ng. A dynamic bayesian net- work model for autonomous 3d reconstruction from a single indoor image. In Proc. Int. Conf. on Computer Vision and Pattern Recognition (CVPR), 2006. 2\n\nPredicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. D Eigen, R Fergus, Proc. Int. Conf. Computer Vision (ICCV). Int. Conf. Computer Vision (ICCV)23D. Eigen and R. Fergus. Predicting depth, surface normals and semantic labels with a common multi-scale convolu- tional architecture. In In Proc. Int. Conf. Computer Vision (ICCV), 2015. 2, 3\n\nPrediction from a single image using a multi-scale deep network. D Eigen, C Puhrsch, R Fergus, Proc. Conf. Neural Information Processing Systems (NIPS). Conf. Neural Information essing Systems (NIPS)34D. Eigen, C. Puhrsch, and R. Fergus. Prediction from a sin- gle image using a multi-scale deep network. In Proc. Conf. Neural Information Processing Systems (NIPS), 2014. 2, 3, 4\n\nLSD-SLAM: Large-Scale Direct Monocular SLAM. J Engel, T Schps, D Cremers, European Conference on Computer Vision (ECCV). 7J. Engel, T. Schps, and D. Cremers. LSD-SLAM: Large- Scale Direct Monocular SLAM. In European Conference on Computer Vision (ECCV), 2014. 1, 2, 3, 4, 5, 6, 7, 8\n\nSemi-dense visual odometry for a monocular camera. J Engel, J Sturm, D Cremers, IEEE International Conference on Computer Vision (ICCV). 25J. Engel, J. Sturm, and D. Cremers. Semi-dense visual odom- etry for a monocular camera. In IEEE International Confer- ence on Computer Vision (ICCV), December 2013. 2, 5\n\nReal-time monocular object slam. D G\u00e1lvez-L\u00f3pez, M Salas, J D Tard\u00f3s, J Montiel, Robot. Auton. Syst. 75PBD. G\u00e1lvez-L\u00f3pez, M. Salas, J. D. Tard\u00f3s, and J. Mon- tiel. Real-time monocular object slam. Robot. Auton. Syst., 75(PB), jan 2016. 2\n\nMultilevel mapping: Real-time dense monocular slam. W N Greene, K Ok, P Lommel, N Roy, 2016 IEEE International Conference on Robotics and Automation (ICRA). W. N. Greene, K. Ok, P. Lommel, and N. Roy. Multi- level mapping: Real-time dense monocular slam. In 2016 IEEE International Conference on Robotics and Automation (ICRA), May 2016. 3\n\nA benchmark for RGB-D visual odometry, 3D reconstruction and SLAM. A Handa, T Whelan, J Mcdonald, A Davison, IEEE Intl. Conf. on Robotics and Automation. ICRA, Hong Kong, China67A. Handa, T. Whelan, J. McDonald, and A. Davison. A benchmark for RGB-D visual odometry, 3D reconstruction and SLAM. In IEEE Intl. Conf. on Robotics and Automa- tion, ICRA, Hong Kong, China, May 2014. 4, 5, 6, 7\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proc. Conf. Computer Vision and Pattern Recognition (CVPR). Conf. Computer Vision and Pattern Recognition (CVPR)K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn- ing for image recognition. Proc. Conf. Computer Vision and Pattern Recognition (CVPR), 2016. 4\n\nGeometric context from a single image. D Hoiem, A Efros, M Hebert, Computer Vision and Pattern Recognition (CVPR). 23D. Hoiem, A. Efros, and M. Hebert. Geometric context from a single image. In In Computer Vision and Pattern Recogni- tion (CVPR), 2005. 2, 3\n\nReal-Time 3D Reconstruction in Dynamic Scenes Using Point-Based Fusion. M Keller, D Lefloch, M Lambers, S Izadi, T Weyrich, A Kolb, International Conference on 3D Vision (3DV). Ieee6M. Keller, D. Lefloch, M. Lambers, S. Izadi, T. Weyrich, and A. Kolb. Real-Time 3D Reconstruction in Dynamic Scenes Using Point-Based Fusion. In International Conference on 3D Vision (3DV), pages 1-8. Ieee, 2013. 1, 2, 3, 6\n\nParallel Tracking and Mapping for Small AR Workspaces. G Klein, D Murray, Proc. International Symposium on Mixed and Augmented Reality (ISMAR). International Symposium on Mixed and Augmented Reality (ISMAR)23G. Klein and D. Murray. Parallel Tracking and Mapping for Small AR Workspaces. In In Proc. International Symposium on Mixed and Augmented Reality (ISMAR), 2007. 2, 3\n\nImproving the agility of keyframebased SLAM. G Klein, D Murray, European Conference on Computer Vision (ECCV). G. Klein and D. Murray. Improving the agility of keyframe- based SLAM. In European Conference on Computer Vision (ECCV), 2008. 2\n\ng2o: A General Framework for Graph Optimization. R Kuemmerle, G Grisetti, H Strasdat, K Konolige, W Burgard, IEEE International Conference on Robotics and Automation (ICRA). R. Kuemmerle, G. Grisetti, H. Strasdat, K. Konolige, and W. Burgard. g2o: A General Framework for Graph Opti- mization. In IEEE International Conference on Robotics and Automation (ICRA), 2011. 5\n\nUnsupervised feature learning for 3d scene labeling. K Lai, L Bo, D Fox, Int. Conf. on Robotics and Automation (ICRA). K. Lai, L. Bo, and D. Fox. Unsupervised feature learning for 3d scene labeling. In Int. Conf. on Robotics and Automation (ICRA), 2014. 2\n\nDeeper depth prediction with fully convolutional residual networks. I Laina, C Rupprecht, V Belagiannis, F Tombari, N Navab, IEEE International Conference on 3D. I. Laina, C. Rupprecht, V. Belagiannis, F. Tombari, and N. Navab. Deeper depth prediction with fully convolutional residual networks. In IEEE International Conference on 3D\n\narXiv:1606.00373Vision (3DV). Vision (3DV) (arXiv:1606.00373), October 2016. 2, 3, 4, 6, 7\n\nDepth and surface normal estimation from monocular images using regression on deep features and hierarchical CRFs. B Li, C Shen, Y Dai, A V Hengel, M He, Proc. Conf. Computer Vision and Pattern Recognition (CVPR). Conf. Computer Vision and Pattern Recognition (CVPR)B. Li, C. Shen, Y. Dai, A. V. den Hengel, and M. He. Depth and surface normal estimation from monocular images using regression on deep features and hierarchical CRFs. In Proc. Conf. Computer Vision and Pattern Recognition (CVPR), pages 1119-1127, 2015. 3\n\nSingle image depth estimation from predicted semantic labels. B Liu, S Gould, D Koller, Computer Vision and Pattern Recognition (CVPR). 23B. Liu, S. Gould, and D. Koller. Single image depth estima- tion from predicted semantic labels. In In Computer Vision and Pattern Recognition (CVPR), 2010. 2, 3\n\nDeep convolutional neural fields for depth estimation from a single image. F Liu, C Shen, G Lin, Proc. Conf. Computer Vision and Pattern Recognition (CVPR). Conf. Computer Vision and Pattern Recognition (CVPR)F. Liu, C. Shen, and G. Lin. Deep convolutional neural fields for depth estimation from a single image. In Proc. Conf. Computer Vision and Pattern Recognition (CVPR), pages 5162-5170, 2015. 3\n\nOrb-slam: A versatile and accurate monocular slam system. R Mur-Artal, J M M Montiel, J D Tards, IEEE Trans. Robotics. 3157R. Mur-Artal, J. M. M. Montiel, and J. D. Tards. Orb-slam: A versatile and accurate monocular slam system. IEEE Trans. Robotics, 31(5):1147-1163, 2015. 1, 2, 3, 6, 7\n\nKinectFusion: Real-time dense surface mapping and tracking. R A Newcombe, A J Davison, S Izadi, P Kohli, O Hilliges, J Shotton, D Molyneaux, S Hodges, D Kim, A Fitzgibbon, 10th IEEE International Symposium on Mixed and Augmented Reality. R. A. Newcombe, A. J. Davison, S. Izadi, P. Kohli, O. Hilliges, J. Shotton, D. Molyneaux, S. Hodges, D. Kim, and A. Fitzgibbon. KinectFusion: Real-time dense surface mapping and tracking. In 10th IEEE International Sympo- sium on Mixed and Augmented Reality, pages 127-136, oct 2011. 1, 2\n\nDtam: Dense tracking and mapping in real-time. R A Newcombe, S Lovegrove, A J Davison, IEEE International Conference on Computer Vision (ICCV). 13R. A. Newcombe, S. Lovegrove, and A. J. Davison. Dtam: Dense tracking and mapping in real-time. In IEEE Interna- tional Conference on Computer Vision (ICCV), pages 2320- 2327, 2011. 1, 2, 3\n\nREMODE: Probabilistic, monocular dense reconstruction in real time. M Pizzoli, C Forster, D Scaramuzza, IEEE International Conference on Robotics and Automation (ICRA). 67M. Pizzoli, C. Forster, and D. Scaramuzza. REMODE: Prob- abilistic, monocular dense reconstruction in real time. In IEEE International Conference on Robotics and Automation (ICRA), 2014. 6, 7\n\nImageNet Large Scale Visual Recognition Challenge. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, A C Berg, L Fei-Fei, International Journal of Computer Vision (IJCV). 1153O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211-252, 2015. 4\n\nIndoor segmentation and support inference from rgbd images. N Silberman, D Hoiem, P Kohli, R Fergus, ECCV. 6N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, 2012. 6, 8\n\nA benchmark for the evaluation of RGB-D SLAM systems. J Sturm, N Engelhard, F Endres, W Burgard, D Cremers, 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. J. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cre- mers. A benchmark for the evaluation of RGB-D SLAM systems. In 2012 IEEE/RSJ International Conference on In- telligent Robots and Systems, pages 573-580, oct 2012. 6\n\nReal-time and scalable incremental segmentation on dense slam. K Tateno, F Tombari, N Navab, K. Tateno, F. Tombari, and N. Navab. Real-time and scalable incremental segmentation on dense slam. 2015. 6\n\nIncremental dense semantic stereo fusion for large-scale semantic scene reconstruction. V Vineet, O Miksik, M Lidegaard, M Nie\u00dfner, S Golodetz, V A Prisacariu, O K\u00e4hler, D W Murray, S Izadi, P Perez, P H S Torr, IEEE International Conference on Robotics and Automation (ICRA). V. Vineet, O. Miksik, M. Lidegaard, M. Nie\u00dfner, S. Golodetz, V. A. Prisacariu, O. K\u00e4hler, D. W. Murray, S. Izadi, P. Perez, and P. H. S. Torr. Incremental dense se- mantic stereo fusion for large-scale semantic scene recon- struction. In IEEE International Conference on Robotics and Automation (ICRA), 2015. 2\n\nTowards unified depth and semantic prediction from a single image. P Wang, X Shen, Z Lin, S Cohen, B Price, A L Yuille, Proc. Conf. Computer Vision and Pattern Recognition (CVPR). Conf. Computer Vision and Pattern Recognition (CVPR)24P. Wang, X. Shen, Z. Lin, S. Cohen, B. Price, and A. L. Yuille. Towards unified depth and semantic prediction from a single image. In Proc. Conf. Computer Vision and Pattern Recognition (CVPR), pages 2800-2809, 2015. 2, 3, 4\n\nReal-time large scale dense RGB-D SLAM with volumetric fusion. T Whelan, M Kaess, H Johannsson, M Fallon, J J Leonard, J Mcdonald, Intl. J. of Robotics Research. 12IJRRT. Whelan, M. Kaess, H. Johannsson, M. Fallon, J. J. Leonard, and J. Mcdonald. Real-time large scale dense RGB-D SLAM with volumetric fusion. Intl. J. of Robotics Research, IJRR, 2014. 1, 2\n", "annotations": {"author": "[{\"end\":136,\"start\":74},{\"end\":202,\"start\":137},{\"end\":259,\"start\":203},{\"end\":384,\"start\":260}]", "publisher": null, "author_last_name": "[{\"end\":88,\"start\":82},{\"end\":153,\"start\":146},{\"end\":212,\"start\":207},{\"end\":272,\"start\":267}]", "author_first_name": "[{\"end\":81,\"start\":74},{\"end\":145,\"start\":137},{\"end\":206,\"start\":203},{\"end\":266,\"start\":260}]", "author_affiliation": "[{\"end\":135,\"start\":107},{\"end\":201,\"start\":173},{\"end\":258,\"start\":230},{\"end\":318,\"start\":290},{\"end\":383,\"start\":320}]", "title": "[{\"end\":71,\"start\":1},{\"end\":455,\"start\":385}]", "venue": null, "abstract": "[{\"end\":1441,\"start\":457}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2145,\"start\":2141},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2201,\"start\":2197},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2204,\"start\":2201},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2437,\"start\":2434},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3289,\"start\":3285},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3291,\"start\":3289},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3294,\"start\":3291},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4354,\"start\":4351},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4859,\"start\":4855},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4861,\"start\":4859},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4863,\"start\":4861},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5337,\"start\":5333},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5340,\"start\":5337},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5342,\"start\":5340},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5546,\"start\":5542},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5846,\"start\":5842},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6424,\"start\":6421},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7756,\"start\":7753},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7759,\"start\":7756},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8153,\"start\":8149},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8172,\"start\":8168},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9224,\"start\":9220},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9227,\"start\":9224},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9230,\"start\":9227},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9262,\"start\":9258},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9264,\"start\":9262},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9267,\"start\":9264},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9359,\"start\":9355},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9362,\"start\":9359},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9365,\"start\":9362},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9381,\"start\":9377},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9383,\"start\":9381},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9385,\"start\":9383},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9523,\"start\":9519},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9909,\"start\":9905},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10187,\"start\":10184},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10472,\"start\":10469},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10896,\"start\":10892},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10899,\"start\":10896},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11128,\"start\":11124},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11130,\"start\":11128},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11132,\"start\":11130},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11135,\"start\":11132},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11138,\"start\":11135},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11141,\"start\":11138},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11174,\"start\":11170},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11381,\"start\":11377},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12126,\"start\":12122},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12128,\"start\":12126},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12131,\"start\":12128},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12203,\"start\":12200},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14300,\"start\":14297},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14567,\"start\":14564},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14748,\"start\":14745},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15748,\"start\":15745},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16737,\"start\":16733},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16811,\"start\":16808},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16921,\"start\":16918},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16979,\"start\":16975},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":17426,\"start\":17422},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17580,\"start\":17577},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":17583,\"start\":17580},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18951,\"start\":18948},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":19299,\"start\":19296},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":19918,\"start\":19915},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":20773,\"start\":20770},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21416,\"start\":21412},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21713,\"start\":21710},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22408,\"start\":22405},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":23071,\"start\":23068},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":23743,\"start\":23739},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":24319,\"start\":24315},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":25230,\"start\":25226},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":25889,\"start\":25886},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":25921,\"start\":25917},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":26178,\"start\":26174},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":26671,\"start\":26667},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27294,\"start\":27291},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":27314,\"start\":27310},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":27484,\"start\":27480},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":27686,\"start\":27682},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27805,\"start\":27801},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":27808,\"start\":27805},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":27873,\"start\":27869},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27875,\"start\":27874},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":28082,\"start\":28079},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":28085,\"start\":28082},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":28304,\"start\":28300},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":28655,\"start\":28652},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":28721,\"start\":28717},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":28778,\"start\":28775},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":29973,\"start\":29969},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":30374,\"start\":30370},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":30647,\"start\":30644},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":32001,\"start\":31998},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":32269,\"start\":32265},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":33096,\"start\":33092},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":34906,\"start\":34903}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":34546,\"start\":34515},{\"attributes\":{\"id\":\"fig_1\"},\"end\":34907,\"start\":34547},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":36317,\"start\":34908}]", "paragraph": "[{\"end\":2356,\"start\":1457},{\"end\":3109,\"start\":2366},{\"end\":3748,\"start\":3111},{\"end\":4654,\"start\":3750},{\"end\":5604,\"start\":4656},{\"end\":7517,\"start\":5606},{\"end\":8207,\"start\":7519},{\"end\":8889,\"start\":8209},{\"end\":9047,\"start\":8906},{\"end\":9471,\"start\":9049},{\"end\":10609,\"start\":9473},{\"end\":11653,\"start\":10611},{\"end\":12523,\"start\":11690},{\"end\":13400,\"start\":12525},{\"end\":14202,\"start\":13402},{\"end\":15101,\"start\":14229},{\"end\":15580,\"start\":15103},{\"end\":15796,\"start\":15664},{\"end\":16192,\"start\":15848},{\"end\":16317,\"start\":16221},{\"end\":16497,\"start\":16347},{\"end\":17427,\"start\":16554},{\"end\":18200,\"start\":17429},{\"end\":18698,\"start\":18251},{\"end\":19919,\"start\":18732},{\"end\":20154,\"start\":19921},{\"end\":20673,\"start\":20228},{\"end\":20951,\"start\":20723},{\"end\":21417,\"start\":21081},{\"end\":22076,\"start\":21449},{\"end\":23301,\"start\":22188},{\"end\":23676,\"start\":23344},{\"end\":24536,\"start\":23678},{\"end\":24967,\"start\":24551},{\"end\":25805,\"start\":24969},{\"end\":27161,\"start\":25807},{\"end\":28249,\"start\":27206},{\"end\":28857,\"start\":28251},{\"end\":31175,\"start\":28859},{\"end\":32010,\"start\":31217},{\"end\":32753,\"start\":32012},{\"end\":33532,\"start\":32794},{\"end\":34514,\"start\":33547}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15607,\"start\":15581},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15663,\"start\":15607},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15847,\"start\":15797},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16220,\"start\":16193},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16346,\"start\":16318},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18731,\"start\":18699},{\"attributes\":{\"id\":\"formula_6\"},\"end\":20227,\"start\":20155},{\"attributes\":{\"id\":\"formula_7\"},\"end\":20722,\"start\":20674},{\"attributes\":{\"id\":\"formula_8\"},\"end\":21080,\"start\":20952},{\"attributes\":{\"id\":\"formula_9\"},\"end\":22187,\"start\":22077}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":28313,\"start\":28306}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1455,\"start\":1443},{\"end\":2364,\"start\":2359},{\"attributes\":{\"n\":\"2.\"},\"end\":8904,\"start\":8892},{\"attributes\":{\"n\":\"3.\"},\"end\":11688,\"start\":11656},{\"attributes\":{\"n\":\"3.1.\"},\"end\":14227,\"start\":14205},{\"attributes\":{\"n\":\"3.2.\"},\"end\":16552,\"start\":16500},{\"attributes\":{\"n\":\"3.3.\"},\"end\":18249,\"start\":18203},{\"attributes\":{\"n\":\"3.4.\"},\"end\":21447,\"start\":21420},{\"attributes\":{\"n\":\"3.5.\"},\"end\":23342,\"start\":23304},{\"attributes\":{\"n\":\"4.\"},\"end\":24549,\"start\":24539},{\"attributes\":{\"n\":\"4.1.\"},\"end\":27204,\"start\":27164},{\"attributes\":{\"n\":\"4.2.\"},\"end\":31215,\"start\":31178},{\"attributes\":{\"n\":\"4.3.\"},\"end\":32792,\"start\":32756},{\"attributes\":{\"n\":\"5.\"},\"end\":33545,\"start\":33535},{\"end\":34526,\"start\":34516},{\"end\":34558,\"start\":34548},{\"end\":34918,\"start\":34909}]", "table": "[{\"end\":36317,\"start\":35169}]", "figure_caption": "[{\"end\":34546,\"start\":34528},{\"end\":34907,\"start\":34560},{\"end\":35169,\"start\":34920}]", "figure_ref": "[{\"end\":2466,\"start\":2458},{\"end\":4160,\"start\":4150},{\"end\":6918,\"start\":6908},{\"end\":8031,\"start\":8021},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12039,\"start\":12033},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18834,\"start\":18828},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22941,\"start\":22935},{\"end\":28428,\"start\":28420},{\"end\":30470,\"start\":30464},{\"end\":31785,\"start\":31777},{\"end\":31816,\"start\":31808},{\"end\":32096,\"start\":32088},{\"end\":32938,\"start\":32932},{\"end\":33184,\"start\":33176}]", "bib_author_first_name": "[{\"end\":36568,\"start\":36567},{\"end\":36578,\"start\":36577},{\"end\":36585,\"start\":36584},{\"end\":36587,\"start\":36586},{\"end\":37038,\"start\":37037},{\"end\":37047,\"start\":37046},{\"end\":37391,\"start\":37390},{\"end\":37400,\"start\":37399},{\"end\":37411,\"start\":37410},{\"end\":37752,\"start\":37751},{\"end\":37761,\"start\":37760},{\"end\":37770,\"start\":37769},{\"end\":38042,\"start\":38041},{\"end\":38051,\"start\":38050},{\"end\":38060,\"start\":38059},{\"end\":38335,\"start\":38334},{\"end\":38351,\"start\":38350},{\"end\":38360,\"start\":38359},{\"end\":38362,\"start\":38361},{\"end\":38372,\"start\":38371},{\"end\":38593,\"start\":38592},{\"end\":38595,\"start\":38594},{\"end\":38605,\"start\":38604},{\"end\":38611,\"start\":38610},{\"end\":38621,\"start\":38620},{\"end\":38949,\"start\":38948},{\"end\":38958,\"start\":38957},{\"end\":38968,\"start\":38967},{\"end\":38980,\"start\":38979},{\"end\":39319,\"start\":39318},{\"end\":39325,\"start\":39324},{\"end\":39334,\"start\":39333},{\"end\":39341,\"start\":39340},{\"end\":39653,\"start\":39652},{\"end\":39662,\"start\":39661},{\"end\":39671,\"start\":39670},{\"end\":39945,\"start\":39944},{\"end\":39955,\"start\":39954},{\"end\":39966,\"start\":39965},{\"end\":39977,\"start\":39976},{\"end\":39986,\"start\":39985},{\"end\":39997,\"start\":39996},{\"end\":40335,\"start\":40334},{\"end\":40344,\"start\":40343},{\"end\":40700,\"start\":40699},{\"end\":40709,\"start\":40708},{\"end\":40945,\"start\":40944},{\"end\":40958,\"start\":40957},{\"end\":40970,\"start\":40969},{\"end\":40982,\"start\":40981},{\"end\":40994,\"start\":40993},{\"end\":41320,\"start\":41319},{\"end\":41327,\"start\":41326},{\"end\":41333,\"start\":41332},{\"end\":41592,\"start\":41591},{\"end\":41601,\"start\":41600},{\"end\":41614,\"start\":41613},{\"end\":41629,\"start\":41628},{\"end\":41640,\"start\":41639},{\"end\":42067,\"start\":42066},{\"end\":42073,\"start\":42072},{\"end\":42081,\"start\":42080},{\"end\":42088,\"start\":42087},{\"end\":42090,\"start\":42089},{\"end\":42100,\"start\":42099},{\"end\":42537,\"start\":42536},{\"end\":42544,\"start\":42543},{\"end\":42553,\"start\":42552},{\"end\":42851,\"start\":42850},{\"end\":42858,\"start\":42857},{\"end\":42866,\"start\":42865},{\"end\":43236,\"start\":43235},{\"end\":43249,\"start\":43248},{\"end\":43253,\"start\":43250},{\"end\":43264,\"start\":43263},{\"end\":43266,\"start\":43265},{\"end\":43528,\"start\":43527},{\"end\":43530,\"start\":43529},{\"end\":43542,\"start\":43541},{\"end\":43544,\"start\":43543},{\"end\":43555,\"start\":43554},{\"end\":43564,\"start\":43563},{\"end\":43573,\"start\":43572},{\"end\":43585,\"start\":43584},{\"end\":43596,\"start\":43595},{\"end\":43609,\"start\":43608},{\"end\":43619,\"start\":43618},{\"end\":43626,\"start\":43625},{\"end\":44043,\"start\":44042},{\"end\":44045,\"start\":44044},{\"end\":44057,\"start\":44056},{\"end\":44070,\"start\":44069},{\"end\":44072,\"start\":44071},{\"end\":44401,\"start\":44400},{\"end\":44412,\"start\":44411},{\"end\":44423,\"start\":44422},{\"end\":44748,\"start\":44747},{\"end\":44763,\"start\":44762},{\"end\":44771,\"start\":44770},{\"end\":44777,\"start\":44776},{\"end\":44787,\"start\":44786},{\"end\":44799,\"start\":44798},{\"end\":44805,\"start\":44804},{\"end\":44814,\"start\":44813},{\"end\":44826,\"start\":44825},{\"end\":44836,\"start\":44835},{\"end\":44849,\"start\":44848},{\"end\":44851,\"start\":44850},{\"end\":44859,\"start\":44858},{\"end\":45247,\"start\":45246},{\"end\":45260,\"start\":45259},{\"end\":45269,\"start\":45268},{\"end\":45278,\"start\":45277},{\"end\":45479,\"start\":45478},{\"end\":45488,\"start\":45487},{\"end\":45501,\"start\":45500},{\"end\":45511,\"start\":45510},{\"end\":45522,\"start\":45521},{\"end\":45896,\"start\":45895},{\"end\":45906,\"start\":45905},{\"end\":45917,\"start\":45916},{\"end\":46123,\"start\":46122},{\"end\":46133,\"start\":46132},{\"end\":46143,\"start\":46142},{\"end\":46156,\"start\":46155},{\"end\":46167,\"start\":46166},{\"end\":46179,\"start\":46178},{\"end\":46181,\"start\":46180},{\"end\":46195,\"start\":46194},{\"end\":46205,\"start\":46204},{\"end\":46207,\"start\":46206},{\"end\":46217,\"start\":46216},{\"end\":46226,\"start\":46225},{\"end\":46235,\"start\":46234},{\"end\":46239,\"start\":46236},{\"end\":46691,\"start\":46690},{\"end\":46699,\"start\":46698},{\"end\":46707,\"start\":46706},{\"end\":46714,\"start\":46713},{\"end\":46723,\"start\":46722},{\"end\":46732,\"start\":46731},{\"end\":46734,\"start\":46733},{\"end\":47147,\"start\":47146},{\"end\":47157,\"start\":47156},{\"end\":47166,\"start\":47165},{\"end\":47180,\"start\":47179},{\"end\":47190,\"start\":47189},{\"end\":47192,\"start\":47191},{\"end\":47203,\"start\":47202}]", "bib_author_last_name": "[{\"end\":36575,\"start\":36569},{\"end\":36582,\"start\":36579},{\"end\":36590,\"start\":36588},{\"end\":37044,\"start\":37039},{\"end\":37054,\"start\":37048},{\"end\":37397,\"start\":37392},{\"end\":37408,\"start\":37401},{\"end\":37418,\"start\":37412},{\"end\":37758,\"start\":37753},{\"end\":37767,\"start\":37762},{\"end\":37778,\"start\":37771},{\"end\":38048,\"start\":38043},{\"end\":38057,\"start\":38052},{\"end\":38068,\"start\":38061},{\"end\":38348,\"start\":38336},{\"end\":38357,\"start\":38352},{\"end\":38369,\"start\":38363},{\"end\":38380,\"start\":38373},{\"end\":38602,\"start\":38596},{\"end\":38608,\"start\":38606},{\"end\":38618,\"start\":38612},{\"end\":38625,\"start\":38622},{\"end\":38955,\"start\":38950},{\"end\":38965,\"start\":38959},{\"end\":38977,\"start\":38969},{\"end\":38988,\"start\":38981},{\"end\":39322,\"start\":39320},{\"end\":39331,\"start\":39326},{\"end\":39338,\"start\":39335},{\"end\":39345,\"start\":39342},{\"end\":39659,\"start\":39654},{\"end\":39668,\"start\":39663},{\"end\":39678,\"start\":39672},{\"end\":39952,\"start\":39946},{\"end\":39963,\"start\":39956},{\"end\":39974,\"start\":39967},{\"end\":39983,\"start\":39978},{\"end\":39994,\"start\":39987},{\"end\":40002,\"start\":39998},{\"end\":40341,\"start\":40336},{\"end\":40351,\"start\":40345},{\"end\":40706,\"start\":40701},{\"end\":40716,\"start\":40710},{\"end\":40955,\"start\":40946},{\"end\":40967,\"start\":40959},{\"end\":40979,\"start\":40971},{\"end\":40991,\"start\":40983},{\"end\":41002,\"start\":40995},{\"end\":41324,\"start\":41321},{\"end\":41330,\"start\":41328},{\"end\":41337,\"start\":41334},{\"end\":41598,\"start\":41593},{\"end\":41611,\"start\":41602},{\"end\":41626,\"start\":41615},{\"end\":41637,\"start\":41630},{\"end\":41646,\"start\":41641},{\"end\":42070,\"start\":42068},{\"end\":42078,\"start\":42074},{\"end\":42085,\"start\":42082},{\"end\":42097,\"start\":42091},{\"end\":42103,\"start\":42101},{\"end\":42541,\"start\":42538},{\"end\":42550,\"start\":42545},{\"end\":42560,\"start\":42554},{\"end\":42855,\"start\":42852},{\"end\":42863,\"start\":42859},{\"end\":42870,\"start\":42867},{\"end\":43246,\"start\":43237},{\"end\":43261,\"start\":43254},{\"end\":43272,\"start\":43267},{\"end\":43539,\"start\":43531},{\"end\":43552,\"start\":43545},{\"end\":43561,\"start\":43556},{\"end\":43570,\"start\":43565},{\"end\":43582,\"start\":43574},{\"end\":43593,\"start\":43586},{\"end\":43606,\"start\":43597},{\"end\":43616,\"start\":43610},{\"end\":43623,\"start\":43620},{\"end\":43637,\"start\":43627},{\"end\":44054,\"start\":44046},{\"end\":44067,\"start\":44058},{\"end\":44080,\"start\":44073},{\"end\":44409,\"start\":44402},{\"end\":44420,\"start\":44413},{\"end\":44434,\"start\":44424},{\"end\":44760,\"start\":44749},{\"end\":44768,\"start\":44764},{\"end\":44774,\"start\":44772},{\"end\":44784,\"start\":44778},{\"end\":44796,\"start\":44788},{\"end\":44802,\"start\":44800},{\"end\":44811,\"start\":44806},{\"end\":44823,\"start\":44815},{\"end\":44833,\"start\":44827},{\"end\":44846,\"start\":44837},{\"end\":44856,\"start\":44852},{\"end\":44867,\"start\":44860},{\"end\":45257,\"start\":45248},{\"end\":45266,\"start\":45261},{\"end\":45275,\"start\":45270},{\"end\":45285,\"start\":45279},{\"end\":45485,\"start\":45480},{\"end\":45498,\"start\":45489},{\"end\":45508,\"start\":45502},{\"end\":45519,\"start\":45512},{\"end\":45530,\"start\":45523},{\"end\":45903,\"start\":45897},{\"end\":45914,\"start\":45907},{\"end\":45923,\"start\":45918},{\"end\":46130,\"start\":46124},{\"end\":46140,\"start\":46134},{\"end\":46153,\"start\":46144},{\"end\":46164,\"start\":46157},{\"end\":46176,\"start\":46168},{\"end\":46192,\"start\":46182},{\"end\":46202,\"start\":46196},{\"end\":46214,\"start\":46208},{\"end\":46223,\"start\":46218},{\"end\":46232,\"start\":46227},{\"end\":46244,\"start\":46240},{\"end\":46696,\"start\":46692},{\"end\":46704,\"start\":46700},{\"end\":46711,\"start\":46708},{\"end\":46720,\"start\":46715},{\"end\":46729,\"start\":46724},{\"end\":46741,\"start\":46735},{\"end\":47154,\"start\":47148},{\"end\":47163,\"start\":47158},{\"end\":47177,\"start\":47167},{\"end\":47187,\"start\":47181},{\"end\":47200,\"start\":47193},{\"end\":47212,\"start\":47204}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":14075351},\"end\":36927,\"start\":36473},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":102496818},\"end\":37323,\"start\":36929},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2255738},\"end\":37704,\"start\":37325},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":14547347},\"end\":37988,\"start\":37706},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":7110290},\"end\":38299,\"start\":37990},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":8170864},\"end\":38538,\"start\":38301},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":2317982},\"end\":38879,\"start\":38540},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":206850587},\"end\":39270,\"start\":38881},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":206594692},\"end\":39611,\"start\":39272},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":206769405},\"end\":39870,\"start\":39613},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":17260589},\"end\":40277,\"start\":39872},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":206986664},\"end\":40652,\"start\":40279},{\"attributes\":{\"id\":\"b12\"},\"end\":40893,\"start\":40654},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":206849534},\"end\":41264,\"start\":40895},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":15015476},\"end\":41521,\"start\":41266},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":11091110},\"end\":41857,\"start\":41523},{\"attributes\":{\"doi\":\"arXiv:1606.00373\",\"id\":\"b16\"},\"end\":41949,\"start\":41859},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":206592782},\"end\":42472,\"start\":41951},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3018706},\"end\":42773,\"start\":42474},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":13153},\"end\":43175,\"start\":42775},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":206775100},\"end\":43465,\"start\":43177},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":11830123},\"end\":43993,\"start\":43467},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":1336659},\"end\":44330,\"start\":43995},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":9632354},\"end\":44694,\"start\":44332},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2930547},\"end\":45184,\"start\":44696},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":545361},\"end\":45422,\"start\":45186},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":206942855},\"end\":45830,\"start\":45424},{\"attributes\":{\"id\":\"b27\"},\"end\":46032,\"start\":45832},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":6544192},\"end\":46621,\"start\":46034},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":5979036},\"end\":47081,\"start\":46623},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":7604429},\"end\":47440,\"start\":47083}]", "bib_title": "[{\"end\":36565,\"start\":36473},{\"end\":37035,\"start\":36929},{\"end\":37388,\"start\":37325},{\"end\":37749,\"start\":37706},{\"end\":38039,\"start\":37990},{\"end\":38332,\"start\":38301},{\"end\":38590,\"start\":38540},{\"end\":38946,\"start\":38881},{\"end\":39316,\"start\":39272},{\"end\":39650,\"start\":39613},{\"end\":39942,\"start\":39872},{\"end\":40332,\"start\":40279},{\"end\":40697,\"start\":40654},{\"end\":40942,\"start\":40895},{\"end\":41317,\"start\":41266},{\"end\":41589,\"start\":41523},{\"end\":42064,\"start\":41951},{\"end\":42534,\"start\":42474},{\"end\":42848,\"start\":42775},{\"end\":43233,\"start\":43177},{\"end\":43525,\"start\":43467},{\"end\":44040,\"start\":43995},{\"end\":44398,\"start\":44332},{\"end\":44745,\"start\":44696},{\"end\":45244,\"start\":45186},{\"end\":45476,\"start\":45424},{\"end\":46120,\"start\":46034},{\"end\":46688,\"start\":46623},{\"end\":47144,\"start\":47083}]", "bib_author": "[{\"end\":36577,\"start\":36567},{\"end\":36584,\"start\":36577},{\"end\":36592,\"start\":36584},{\"end\":37046,\"start\":37037},{\"end\":37056,\"start\":37046},{\"end\":37399,\"start\":37390},{\"end\":37410,\"start\":37399},{\"end\":37420,\"start\":37410},{\"end\":37760,\"start\":37751},{\"end\":37769,\"start\":37760},{\"end\":37780,\"start\":37769},{\"end\":38050,\"start\":38041},{\"end\":38059,\"start\":38050},{\"end\":38070,\"start\":38059},{\"end\":38350,\"start\":38334},{\"end\":38359,\"start\":38350},{\"end\":38371,\"start\":38359},{\"end\":38382,\"start\":38371},{\"end\":38604,\"start\":38592},{\"end\":38610,\"start\":38604},{\"end\":38620,\"start\":38610},{\"end\":38627,\"start\":38620},{\"end\":38957,\"start\":38948},{\"end\":38967,\"start\":38957},{\"end\":38979,\"start\":38967},{\"end\":38990,\"start\":38979},{\"end\":39324,\"start\":39318},{\"end\":39333,\"start\":39324},{\"end\":39340,\"start\":39333},{\"end\":39347,\"start\":39340},{\"end\":39661,\"start\":39652},{\"end\":39670,\"start\":39661},{\"end\":39680,\"start\":39670},{\"end\":39954,\"start\":39944},{\"end\":39965,\"start\":39954},{\"end\":39976,\"start\":39965},{\"end\":39985,\"start\":39976},{\"end\":39996,\"start\":39985},{\"end\":40004,\"start\":39996},{\"end\":40343,\"start\":40334},{\"end\":40353,\"start\":40343},{\"end\":40708,\"start\":40699},{\"end\":40718,\"start\":40708},{\"end\":40957,\"start\":40944},{\"end\":40969,\"start\":40957},{\"end\":40981,\"start\":40969},{\"end\":40993,\"start\":40981},{\"end\":41004,\"start\":40993},{\"end\":41326,\"start\":41319},{\"end\":41332,\"start\":41326},{\"end\":41339,\"start\":41332},{\"end\":41600,\"start\":41591},{\"end\":41613,\"start\":41600},{\"end\":41628,\"start\":41613},{\"end\":41639,\"start\":41628},{\"end\":41648,\"start\":41639},{\"end\":42072,\"start\":42066},{\"end\":42080,\"start\":42072},{\"end\":42087,\"start\":42080},{\"end\":42099,\"start\":42087},{\"end\":42105,\"start\":42099},{\"end\":42543,\"start\":42536},{\"end\":42552,\"start\":42543},{\"end\":42562,\"start\":42552},{\"end\":42857,\"start\":42850},{\"end\":42865,\"start\":42857},{\"end\":42872,\"start\":42865},{\"end\":43248,\"start\":43235},{\"end\":43263,\"start\":43248},{\"end\":43274,\"start\":43263},{\"end\":43541,\"start\":43527},{\"end\":43554,\"start\":43541},{\"end\":43563,\"start\":43554},{\"end\":43572,\"start\":43563},{\"end\":43584,\"start\":43572},{\"end\":43595,\"start\":43584},{\"end\":43608,\"start\":43595},{\"end\":43618,\"start\":43608},{\"end\":43625,\"start\":43618},{\"end\":43639,\"start\":43625},{\"end\":44056,\"start\":44042},{\"end\":44069,\"start\":44056},{\"end\":44082,\"start\":44069},{\"end\":44411,\"start\":44400},{\"end\":44422,\"start\":44411},{\"end\":44436,\"start\":44422},{\"end\":44762,\"start\":44747},{\"end\":44770,\"start\":44762},{\"end\":44776,\"start\":44770},{\"end\":44786,\"start\":44776},{\"end\":44798,\"start\":44786},{\"end\":44804,\"start\":44798},{\"end\":44813,\"start\":44804},{\"end\":44825,\"start\":44813},{\"end\":44835,\"start\":44825},{\"end\":44848,\"start\":44835},{\"end\":44858,\"start\":44848},{\"end\":44869,\"start\":44858},{\"end\":45259,\"start\":45246},{\"end\":45268,\"start\":45259},{\"end\":45277,\"start\":45268},{\"end\":45287,\"start\":45277},{\"end\":45487,\"start\":45478},{\"end\":45500,\"start\":45487},{\"end\":45510,\"start\":45500},{\"end\":45521,\"start\":45510},{\"end\":45532,\"start\":45521},{\"end\":45905,\"start\":45895},{\"end\":45916,\"start\":45905},{\"end\":45925,\"start\":45916},{\"end\":46132,\"start\":46122},{\"end\":46142,\"start\":46132},{\"end\":46155,\"start\":46142},{\"end\":46166,\"start\":46155},{\"end\":46178,\"start\":46166},{\"end\":46194,\"start\":46178},{\"end\":46204,\"start\":46194},{\"end\":46216,\"start\":46204},{\"end\":46225,\"start\":46216},{\"end\":46234,\"start\":46225},{\"end\":46246,\"start\":46234},{\"end\":46698,\"start\":46690},{\"end\":46706,\"start\":46698},{\"end\":46713,\"start\":46706},{\"end\":46722,\"start\":46713},{\"end\":46731,\"start\":46722},{\"end\":46743,\"start\":46731},{\"end\":47156,\"start\":47146},{\"end\":47165,\"start\":47156},{\"end\":47179,\"start\":47165},{\"end\":47189,\"start\":47179},{\"end\":47202,\"start\":47189},{\"end\":47214,\"start\":47202}]", "bib_venue": "[{\"end\":36720,\"start\":36660},{\"end\":37130,\"start\":37097},{\"end\":37524,\"start\":37478},{\"end\":39057,\"start\":39035},{\"end\":39459,\"start\":39407},{\"end\":40485,\"start\":40423},{\"end\":42217,\"start\":42165},{\"end\":42984,\"start\":42932},{\"end\":46855,\"start\":46803},{\"end\":36658,\"start\":36592},{\"end\":37095,\"start\":37056},{\"end\":37476,\"start\":37420},{\"end\":37825,\"start\":37780},{\"end\":38125,\"start\":38070},{\"end\":38400,\"start\":38382},{\"end\":38695,\"start\":38627},{\"end\":39033,\"start\":38990},{\"end\":39405,\"start\":39347},{\"end\":39726,\"start\":39680},{\"end\":40047,\"start\":40004},{\"end\":40421,\"start\":40353},{\"end\":40763,\"start\":40718},{\"end\":41067,\"start\":41004},{\"end\":41383,\"start\":41339},{\"end\":41683,\"start\":41648},{\"end\":41887,\"start\":41875},{\"end\":42163,\"start\":42105},{\"end\":42608,\"start\":42562},{\"end\":42930,\"start\":42872},{\"end\":43294,\"start\":43274},{\"end\":43703,\"start\":43639},{\"end\":44137,\"start\":44082},{\"end\":44499,\"start\":44436},{\"end\":44916,\"start\":44869},{\"end\":45291,\"start\":45287},{\"end\":45604,\"start\":45532},{\"end\":45893,\"start\":45832},{\"end\":46309,\"start\":46246},{\"end\":46801,\"start\":46743},{\"end\":47243,\"start\":47214}]"}}}, "year": 2023, "month": 12, "day": 17}