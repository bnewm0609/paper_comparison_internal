{"id": 219530894, "updated": "2023-10-06 14:24:31.221", "metadata": {"title": "Conservative Q-Learning for Offline Reinforcement Learning", "authors": "[{\"first\":\"Aviral\",\"last\":\"Kumar\",\"middle\":[]},{\"first\":\"Aurick\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"George\",\"last\":\"Tucker\",\"middle\":[]},{\"first\":\"Sergey\",\"last\":\"Levine\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 6, "day": 8}, "abstract": "Effectively leveraging large, previously collected datasets in reinforcement learning (RL) is a key challenge for large-scale real-world applications. Offline RL algorithms promise to learn effective policies from previously-collected, static datasets without further interaction. However, in practice, offline RL presents a major challenge, and standard off-policy RL methods can fail due to overestimation of values induced by the distributional shift between the dataset and the learned policy, especially when training on complex and multi-modal data distributions. In this paper, we propose conservative Q-learning (CQL), which aims to address these limitations by learning a conservative Q-function such that the expected value of a policy under this Q-function lower-bounds its true value. We theoretically show that CQL produces a lower bound on the value of the current policy and that it can be incorporated into a principled policy improvement procedure. In practice, CQL augments the standard Bellman error objective with a simple Q-value regularizer which is straightforward to implement on top of existing deep Q-learning and actor-critic implementations. On both discrete and continuous control domains, we show that CQL substantially outperforms existing offline RL methods, often learning policies that attain 2-5 times higher final return, especially when learning from complex and multi-modal data distributions.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2006.04779", "mag": "3102848167", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/KumarZTL20", "doi": null}}, "content": {"source": {"pdf_hash": "fe2ac46ca18e8a33c7232f07861ffa4dbcfa2d70", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2006.04779v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "90d8c5c93cd114ab060e49a6a04dd42f5a85c4c4", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/fe2ac46ca18e8a33c7232f07861ffa4dbcfa2d70.txt", "contents": "\nConservative Q-Learning for Offline Reinforcement Learning\n\n\nAviral Kumar aviralk@berkeley.edu \nUC Berkeley\n\n\nAurick Zhou \nUC Berkeley\n\n\nGeorge Tucker \nGoogle Research\nBrain Team\n\nSergey Levine \nUC Berkeley\n\n\nGoogle Research\nBrain Team\n\nConservative Q-Learning for Offline Reinforcement Learning\n\nEffectively leveraging large, previously collected datasets in reinforcement learning (RL) is a key challenge for large-scale real-world applications. Offline RL algorithms promise to learn effective policies from previously-collected, static datasets without further interaction. However, in practice, offline RL presents a major challenge, and standard off-policy RL methods can fail due to overestimation of values induced by the distributional shift between the dataset and the learned policy, especially when training on complex and multi-modal data distributions. In this paper, we propose conservative Q-learning (CQL), which aims to address these limitations by learning a conservative Q-function such that the expected value of a policy under this Q-function lower-bounds its true value. We theoretically show that CQL produces a lower bound on the value of the current policy and that it can be incorporated into a principled policy improvement procedure. In practice, CQL augments the standard Bellman error objective with a simple Q-value regularizer which is straightforward to implement on top of existing deep Q-learning and actor-critic implementations. On both discrete and continuous control domains, we show that CQL substantially outperforms existing offline RL methods, often learning policies that attain 2-5 times higher final return, especially when learning from complex and multi-modal data distributions.Preprint. Under review.\n\nIntroduction\n\nRecent advances in reinforcement learning (RL), especially when combined with expressive deep network function approximators, have produced promising results in domains ranging from robotics [26] to strategy games [3] and recommendation systems [31]. However, applying RL to real-world problems consistently poses practical challenges: in contrast to the kinds of data-driven methods that have been successful in supervised learning [21,9], RL is classically regarded as an active learning process, where each training run requires active interaction with the environment. Interaction with the real world can be costly and dangerous, and the quantities of data that can be gathered online are substantially lower than the offline datasets that are used in supervised learning [8], which only need to be collected once. Offline RL, also known as batch RL, offers an appealing alternative [10,14,27,2,25,46,30]. Offline RL algorithms learn from large, previously collected datasets, without interaction. This in principle can make it possible to leverage large datasets, but in practice fully offline RL methods pose major technical difficulties, stemming from the distributional shift between the policy that collected the data and the learned policy. This has made current results fall short of the full promise of such methods.\n\nDirectly utilizing existing value-based off-policy RL algorithms in an offline setting generally results in poor performance, due to issues with bootstrapping from out-of-distribution actions [27,14] and overfitting [12,27,2]. This typically manifests as erroneously optimistic value function estimates. If we can instead learn a conservative estimate of the value function, which provides a lower bound on the true values, this overestimation problem could be addressed. In fact, because policy evaluation and improvement typically only use the value of the policy, we can learn a less conservative lower bound Q-function, such that only the expected value of Q-function under the policy is lower-bounded, as opposed to a point-wise lower bound. We propose a novel method for learning such conservative Qfunctions via a simple modification to standard value-based RL algorithms. The key idea behind our method is to minimize values under an appropriately chosen distribution over state-action tuples, and then further tighten this bound by also incorporating a maximization term over the data distribution.\n\nOur primary contribution is an algorithmic framework, which we call conservative Q-learning (CQL), for learning conservative, lower-bound estimates of the value function, by regularizing the Q-values during training. Our theoretical analysis of CQL shows that only the expected value of this Q-function under the policy lower-bounds the true policy value, preventing extra under-estimation that can arise with point-wise lower-bounded Q-functions, that have typically been explored in the opposite context in exploration literature [39,24]. We also empirically demonstrate the robustness of our approach to Q-function estimation error. Our practical algorithm uses these conservative estimates for policy evaluation and offline RL. CQL can be implemented with less than 20 lines of code on top of a number of standard, online RL algorithms [18,7], simply by adding the CQL regularization terms to the Q-function update. In our experiments, we demonstrate the efficacy of CQL for offline RL, in domains with complex dataset compositions, where prior methods are typically known to perform poorly [11] and domains with high-dimensional visual inputs [4,2]. CQL outperforms prior methods by as much as 2-5x on many benchmark tasks, and is the only method that can outperform simple behavioral cloning on a number of realistic datasets collected from human interaction.\n\n\nPreliminaries\n\nThe goal in reinforcement learning is to learn a policy that maximizes the expected cumulative discounted reward in a Markov decision process (MDP), which is defined by a tuple (S, A, T, r, \u03b3). S, A represent state and action spaces, T (s |s, a) and r(s, a) represent the dynamics and reward function, and \u03b3 \u2208 (0, 1) represents the discount factor. \u03c0 \u03b2 (a|s) represents the action-conditional in the dataset, D, and d \u03c0 \u03b2 (s) is the discounted marginal state-distribution of \u03c0 \u03b2 (a|s). The dataset D is sampled from d \u03c0 \u03b2 (s)\u03c0 \u03b2 (a|s). On all states s 0 \u2208 D, let\u03c0 \u03b2 (a 0 |s 0 ) := s,a\u2208D 1[s=s0,a=s0] s\u2208D 1[s=s0] denote the empirical behavior policy, at that state.\n\nOff-policy RL algorithms based on dynamic programming maintain a parametric Q-function Q \u03b8 (s, a) and, optionally, a parametric policy, \u03c0 \u03c6 (a|s). Q-learning methods train the Q-function by iteratively applying the Bellman optimality operator B * Q(s, a) = r(s, a) + \u03b3E s \u223cP (s |s,a) [max a Q(s , a )], and use exact or an approximate maximization scheme, such as CEM [26] to recover the greedy policy. In an actor-critic algorithm, a separate policy is trained to maximize the Q-value. Actorcritic methods alternate between computing Q \u03c0 via (partial) policy evaluation, by iterating the Bellman operator, B \u03c0 Q = r + \u03b3P \u03c0 Q, where P \u03c0 is the transition matrix coupled with the policy: P \u03c0 Q(s, a) = E s \u223cT (s |s,a),a \u223c\u03c0(a |s ) [Q(s , a )] , and improving the policy \u03c0(a|s) by updating it towards actions that maximize the expected Q-value. Since D typically does not contain all possible transitions (s, a, s ), the policy evaluation step actually uses an empirical Bellman operator that only backs up a single sample. We denote this operatorB \u03c0 . Given a dataset D = {(s, a, s )} of tuples from trajectories collected under a behavior policy \u03c0 \u03b2 : Offline RL algorithms based on this basic recipe suffer from action distribution shift [27,48,25,30] during training, because the target values for Bellman backups in policy evaluation use actions sampled from the learned policy, \u03c0 k , but the Q-function is trained only on actions sampled from the behavior policy that produced the dataset D, \u03c0 \u03b2 . We also remark that Q-function training in offline RL does not suffer from state distribution shift, because the Q-function is never queried on out-of-distribution states. Since \u03c0 is trained to maximize Q-values, it may be biased towards out-of-distribution (OOD) actions with erroneously high Q-values. In standard RL, such errors can be corrected by attempting an action in the environment and observing its actual value. However, the inability to interact with the environment makes it challenging to deal with Q-values for OOD actions in offline RL. Typical offline RL methods [27,25,48,46] mitigate this problem by constraining the learned policy [30] away from OOD actions.\nQ k+1 \u2190 arg min Q E s,\n\nThe Conservative Q-Learning (CQL) Framework\n\nIn this section, we develop a conservative Q-learning (CQL) algorithm, such that the expected value of a policy under the learned Q-function lower-bounds its true value. A lower bound on the Q-value prevents the over-estimation that is common in offline RL settings due to OOD actions and function approximation error [30,27]. We use the term CQL to refer broadly to both Q-learning methods and actor-critic methods, though the latter also use an explicit policy. We start by focusing on the policy evaluation step in CQL, which can be used by itself as an off-policy evaluation procedure, or integrated into a complete offline RL algorithm, as we will discuss in Section 3.2.\n\n\nConservative Off-Policy Evaluation\n\nWe aim to estimate the value V \u03c0 (s) of a target policy \u03c0 given access to a dataset, D, generated by following a behavior policy \u03c0 \u03b2 (a|s). Because we are interested in preventing overestimation of the policy value, we learn a conservative, lower-bound Q-function by additionally minimizing Q-values alongside a standard Bellman error objective. Our choice of penalty is to minimize the expected Qvalue under a particular distribution of state-action pairs, \u00b5(s, a). Since standard Q-function training does not query the Q-function value at unobserved states, but queries the Q-function at unseen actions, we restrict \u00b5 to match the state-marginal in the dataset, such that \u00b5(s, a) = d \u03c0 \u03b2 (s)\u00b5(a|s). This gives rise to the iterative update for training the Q-function, as a function of a tradeoff factor \u03b1:\nQ k+1 \u2190 arg min Q \u03b1 E s\u223cD,a\u223c\u00b5(a|s) [Q(s, a)] + 1 2 Es,a\u223cD Q(s, a) \u2212B \u03c0Qk (s, a) 2 ,(1)\nIn Theorem 3.1, we show that the resulting Q-function,Q \u03c0 := lim k\u2192\u221eQ k , lower-bounds Q \u03c0 at all (s, a). However, we can substantially tighten this bound if we are only interested in estimating V \u03c0 (s). If we only require that the expected value of theQ \u03c0 under \u03c0(a|s) lower-bound V \u03c0 , we can improve the bound by introducing an additional Q-value maximization term under the data distribution, \u03c0 \u03b2 (a|s), resulting in the iterative update (changes from Equation 1 in red):\nQ k+1 \u2190 arg min Q \u03b1 \u00b7 E s\u223cD,a\u223c\u00b5(a|s) [Q(s, a)] \u2212 E s\u223cD,a\u223c\u03c0 \u03b2 (a|s) [Q(s, a)] + 1 2 E s,a,s \u223cD Q(s, a) \u2212B \u03c0Qk (s, a) 2 .(2)\nIn Theorem 3.2, we show that, while the resulting Q-valueQ \u03c0 may not be a point-wise lowerbound, we have E \u03c0(a|s) [Q \u03c0 (s, a)] \u2264 V \u03c0 (s) when \u00b5(a|s) = \u03c0(a|s). Intuitively, since Equation 2 maximizes Q-values under the behavior policy\u03c0 \u03b2 , Q-values for actions that are likely under\u03c0 \u03b2 might be overestimated, and henceQ \u03c0 may not lower-bound Q \u03c0 pointwise. While in principle the maximization term can utilize other distributions besides\u03c0 \u03b2 (a|s), we prove in Appendix D.2 that the resulting value is not guaranteed to be a lower bound for other distribution besides\u03c0 \u03b2 (a|s).\n\nTheoretical analysis. We first note that Equations 1 and 2 use the empirical Bellman operator, B \u03c0 , instead of the actual Bellman operator, B \u03c0 . Following [40,24,38], we use concentration properties ofB \u03c0 to control this error. Formally, for all s, a \u2208 D, with probability \u2265 1 \u2212 \u03b4,\n|B \u03c0 \u2212 B \u03c0 |(s, a) \u2264 C r,T ,\u03b4 \u221a |D(s,a)|\n, where C r,T,\u03b4 is a constant dependent on the concentration properties (variance) of r(s, a) and T (s |s, a), and \u03b4 \u2208 (0, 1) (see Appendix D.3 for more details). Now, we show that the conservative Q-function learned by iterating Equation 1 lower-bounds the true Q-function. Proofs can be found in Appendix C.\n\nTheorem 3.1. For any \u00b5(a|s) with supp \u00b5 \u2282 supp\u03c0 \u03b2 , with probability \u2265 1 \u2212 \u03b4,Q \u03c0 (the Q-function obtained by iterating Equation 1) satisifies:\n\u2200s, a,Q \u03c0 (s, a) \u2264 Q \u03c0 (s, a) \u2212 \u03b1 (I \u2212 \u03b3P \u03c0 ) \u22121 \u00b5(a|s) \u03c0 \u03b2 (a|s) (s, a) + (I \u2212 \u03b3P \u03c0 ) \u22121 C r,T,\u03b4 R max (1 \u2212 \u03b3) .\nThus, if \u03b1 > C r,T Rmax 1\u2212\u03b3 \u00b7 max s,a\u2208D\u03c0 \u03b2 (a|s) \u00b5(a|s) , thenQ \u03c0 (s, a) \u2264 Q \u03c0 (s, a), \u2200s, a. WhenB \u03c0 = B \u03c0 , any \u03b1 > 0 guaranteesQ \u03c0 (s, a) \u2264 Q \u03c0 (s, a), \u2200s, a.\n\nNext, we show that Equation 2 lower-bounds the expected value under the policy \u03c0, when \u00b5 = \u03c0. We also show that Equation 2 does not lower-bound the Q-value estimates pointwise.  The value of the policy under the Qfunction from Equation 2,V \u03c0 (s) = E \u03c0(a|s) [Q \u03c0 (s, a)], lower-bounds the true value of the policy obtained via exact policy evaluation, V \u03c0 (s) = E \u03c0(a|s) [Q \u03c0 (s, a)], when \u00b5 = \u03c0, according to:\n\u2200s,V \u03c0 (s) \u2264 V \u03c0 (s) \u2212 \u03b1 (I \u2212 \u03b3P \u03c0 ) \u22121 E \u03c0(a|s) \u03c0(a|s) \u03c0 \u03b2 (a|s) \u2212 1 (s) + (I \u2212 \u03b3P \u03c0 ) \u22121 C r,T,\u03b4 R max (1 \u2212 \u03b3) . Thus, if \u03b1 > C r,T Rmax 1\u2212\u03b3 \u00b7 max s\u2208D a \u03c0(a|s)( \u03c0(a|s) \u03c0 \u03b2 (a|s)) \u2212 1) \u22121 , \u2200s \u2208 D,V \u03c0 (s) \u2264 V \u03c0 (s), with probability \u2265 1 \u2212 \u03b4. WhenB \u03c0 = B \u03c0 , then any \u03b1 > 0 guaranteesV \u03c0 (s) \u2264 V \u03c0 (s), \u2200s \u2208 D.\nThe analysis presented above assumes that no function approximation is used in the Q-function, meaning that each iterate can be represented exactly. We can further generalize the result in Theorem 3.2 to the case of both linear function approximators and non-linear neural network function approximators, where the latter builds on the neural tangent kernel (NTK) framework [23]. Due to space constraints, we present these results in Theorem D.1 and Theorem D.2 in Appendix D.1.\n\nIn summary, we showed that the basic CQL evaluation in Equation 1 learns a Q-function that lower-bounds the true Q-function Q \u03c0 , and the evaluation in Equation 2 provides a tighter lower bound on the expected Q-value of the policy \u03c0. For suitable \u03b1, both bounds hold under sampling error and function approximation. Next, we will extend on this result into a complete RL algorithm.\n\n\nConservative Q-Learning for Offline RL\n\nWe now present a general approach for offline policy learning, which we refer to as conservative Q-learning (CQL). As discussed in Section 3.1, we can obtain Q-values that lower-bound the value of a policy \u03c0 by solving Equation 2 with \u00b5 = \u03c0. How should we utilize this for policy optimization?\n\nWe could alternate between performing full off-policy evaluation for each policy iterate,\u03c0 k , and one step of policy improvement. However, this can be computationally expensive. Alternatively, since the policy\u03c0 k is typically derived from the Q-function, we could instead choose \u00b5(a|s) to approximate the policy that would maximize the current Q-function iterate, thus giving rise to an online algorithm.\n\nWe can formally capture such online algorithms by defining a family of optimization problems over \u00b5(a|s), presented below, with modifications from Equation 2 marked in red. An instance of this family is denoted by CQL(R) and is characterized by a particular choice of regularizer R(\u00b5):\nmin Q max \u00b5 \u03b1 E s\u223cD,a\u223c\u00b5(a|s) [Q(s, a)] \u2212 E s\u223cD,a\u223c\u03c0 \u03b2 (a|s) [Q(s, a)] + 1 2 E s,a,s \u223cD Q(s, a) \u2212B \u03c0 kQ k (s, a) 2 + R(\u00b5) (CQL(R)) . (3)\nVariants of CQL. To demonstrate the generality of the CQL family of optimization problems, we discuss two specific instances within this family that are of special interest, and we evaluate them empirically in Section 6. If we choose R(\u00b5) to be the KL-divergence against a prior distribution, \u03c1(a|s), i.e., R(\u00b5) = \u2212D KL (\u00b5, \u03c1), then we get \u00b5(a|s) \u221d \u03c1(a|s) \u00b7 exp(Q(s, a)) (for a derivation, see Appendix A). Frist, if \u03c1 = Unif(a), then the first term in Equation 3 corresponds to a soft-maximum of the Q-values at any state s and gives rise to the following variant of Equation 3, called CQL(H):\nmin Q \u03b1Es\u223cD log a exp(Q(s, a))\u2212E a\u223c\u03c0 \u03b2 (a|s) [Q(s, a)] + 1 2 E s,a,s \u223cD Q \u2212B \u03c0 kQ k 2 .(4)\nSecond, if \u03c1(a|s) is chosen to be the previous policy\u03c0 k\u22121 , the first term in Equation 4 is replaced by an exponential weighted average of Q-values of actions from the chosen\u03c0 k\u22121 (a|s). Empirically, we find that this variant can be more stable with high-dimensional action spaces (e.g., Table 2) where it is challenging to estimate log a exp via sampling due to high variance. In Appendix A, we discuss an additional variant of CQL, drawing connections to distributionally robust optimization [37]. We will discuss a practical instantiation of a CQL deep RL algorithm in Section 4. CQL can be instantiated as either a Q-learning algorithm (with B * instead of B \u03c0 in Equations 3, 4) or as an actor-critic algorithm.\n\nTheoretical analysis of CQL. Next, we will theoretically analyze CQL to show that the policy updates derived in this way are indeed \"conservative\", in the sense that each successive policy iterate is optimized against a lower bound on its value. For clarity, we state the results in the absence of finitesample error, in this section, but sampling error can be incorporated in the same way as Theorems 3.1 and 3.2, and we discuss this in Appendix C. Theorem 3.3 shows that any variant of the CQL family learns Q-value estimates that lower-bound the actual Q-function under the action-distribution defined by the policy, \u03c0 k , under mild regularity conditions (slow updates on the policy).\n\n\nTheorem 3.3 (CQL learns lower-bounded Q-values).\n\nLet \u03c0Q k (a|s) \u221d exp(Q k (s, a)) and assume that D TV (\u03c0 k+1 , \u03c0Q k ) \u2264 \u03b5 (i.e.,\u03c0 k+1 changes slowly w.r.t toQ k ). Then, the policy value underQ k , lower-bounds the actual policy value,\nV k+1 (s) \u2264 V k+1 (s)\u2200s if E \u03c0Q k (a|s) \u03c0Q k (a|s) \u03c0 \u03b2 (a|s) \u2212 1 \u2265 max a s.t.\u03c0 \u03b2 (a|s)>0 \u03c0Q k (a|s) \u03c0 \u03b2 (a|s) \u00b7 \u03b5.\nThe LHS of this inequality is equal to the amount of conservatism induced in the value,V k+1 in iteration k + 1 of the CQL update, if the learned policy were equal to soft-optimal policy forQ k , i.e., when\u03c0 k+1 = \u03c0Q k . However, as the actual policy,\u03c0 k+1 , may be different, the RHS is the maximal amount of potential overestimation due to this difference. To get a lower bound, we require the amount of underestimation to be higher, which is obtained if \u03b5 is small, i.e. the policy changes slowly.\n\nOur final result shows that CQL Q-function update is \"gap-expanding\", by which we mean that the difference in Q-values at in-distribution actions and over-optimistically erroneous out-of-distribution actions is higher than the corresponding difference under the actual Q-function. This implies that the policy \u03c0 k (a|s) \u221d exp(Q k (s, a)), is constrained to be closer to the dataset distribution,\u03c0 \u03b2 (a|s), thus the CQL update implicitly prevents the detrimental effects of OOD action and distribution shift, which has been a major concern in offline RL settings [27,30,14]. Theorem 3.4 (CQL is gap-expanding). At any iteration k, CQL expands the difference in expected Q-values under the behavior policy \u03c0 \u03b2 (a|s) and \u00b5 k , such that for large enough values of \u03b1 k , we have that \u2200s,\nE \u03c0 \u03b2 (a|s) [Q k (s, a)] \u2212 E \u00b5 k (a|s) [Q k (s, a)] > E \u03c0 \u03b2 (a|s) [Q k (s, a)] \u2212 E \u00b5 k (a|s) [Q k (s, a)].\nWhen function approximation or sampling error makes OOD actions have higher learned Q-values, CQL backups are expected to be more robust, in that the policy is updated using Q-values that prefer in-distribution actions. As we will empirically show in Appendix B, prior offline RL methods that do not explicitly constrain or regularize the Q-function may not enjoy such robustness properties.\n\nTo summarize, we showed that the CQL RL algorithm learns lower-bound Q-values with large enough \u03b1, meaning that the final policy attains at least the estimated value. We also showed that the Q-function is gap-expanding, meaning that it should only ever over-estimate the gap between in-distribution and out-of-distribution actions, preventing OOD actions.\n\n\nPractical Algorithm and Implementation Details\n\nAlgorithm 1 Conservative Q-Learning (both variants) 1: Initialize Q-function, Q \u03b8 , and optionally a policy, \u03c0 \u03c6 . 2: for step t in {1, . . . , N} do 3: Train the Q-function using GQ gradient steps on objective from Equation 4 \u03b8t := \u03b8t\u22121 \u2212 \u03b7Q\u2207 \u03b8 CQL(R)(\u03b8) (Use B * for Q-learning, B \u03c0 \u03c6 t for actor-critic) 4:\n\n(only with actor-critic) Improve policy \u03c0 \u03c6 via G\u03c0 gradient steps on \u03c6 with SAC-style entropy regularization: \u03c6t := \u03c6t\u22121 + \u03b7\u03c0E s\u223cD,a\u223c\u03c0 \u03c6 (\u00b7|s) [Q \u03b8 (s, a)\u2212log \u03c0 \u03c6 (a|s)] 5: end for We now describe two practical offline deep reinforcement learning methods based on CQL: an actor-critic variant and a Q-learning variant. Pseudocode is shown in Algorithm 1, with differences from conventional actorcritic algorithms (e.g., SAC [18]) and deep Q-learning algorithms (e.g., DQN [35]) in red. Our algorithm uses the CQL(H) (or CQL(R) in general) objective from the CQL framework for training the Q-function Q \u03b8 , which is parameterized by a neural network with parameters \u03b8. For the actor-critic algorithm, a policy \u03c0 \u03c6 is trained as well. Our algorithm modifies the objective for the Q-function (swaps out Bellman error with CQL(H)) or CQL(\u03c1) in a standard actor-critic or Q-learning setting, as shown in Line 3. As discussed in Section 3.2, due to the explicit penalty on the Q-function, CQL methods do not use a policy constraint, unlike prior offline RL methods [27,48,46,30]. Hence, we do not require fitting an additional behavior policy estimator, simplifying our method.  [7] for the discrete control experiments. The tradeoff factor, \u03b1 is automatically tuned via Lagrangian dual gradient descent for continuous control, and is fixed at constant values described in Appendix F for discrete control. We use default hyperparameters from SAC, except that the learning rate for the policy is chosen to be 3e-5 (vs 3e-4 or 1e-4 for the Q-function), as dictated by Theorem 3.3. Elaborate details are provided in Appendix F.\n\n\nRelated Work\n\nWe now briefly discuss prior work in offline RL and off-policy evaluation, comparing and contrasting these works with our approach. More technical discussion of related work is provided in Appendix E.\n\nOff-policy evaluation (OPE). Several different paradigms have been used to perform off-policy evaluation. Earlier works [43,42,44] used per-action importance sampling on Monte-Carlo returns to obtain an OPE return estimator. Recent approaches [32, 16,36,49] use marginalized importance sampling by directly estimating the state-distribution importance ratios via some form of dynamic programming [30] and typically exhibit less variance than per-action importance sampling at the cost of bias. Because these methods use dynamic programming, they can suffer from OOD actions [30, 16,19,36]. In contrast, the regularizer in CQL explicitly addresses the impact of OOD actions due to its gap-expanding behavior, and obtains conservative value estimates.\n\nOffline RL. As discussed in Section 2, offline Q-learning methods suffer from issues pertaining to OOD actions. Prior works have attempted to solve this problem by constraining the learned policy to be \"close\" to the behavior policy, for example as measured by KL-divergence [25, 48,41,46], Wasserstein distance [48], or MMD [27], and then only using actions sampled from this constrained policy in the Bellman backup or applying a value penalty. Most of these methods require a separately estimated model to the behavior policy, \u03c0 \u03b2 (a|s) [14,27,48,25,46], and are thus limited by their ability to accurately estimate the unknown behavior policy, which might be especially complex in settings where the data is collected from multiple sources [30]. In contrast, CQL does not require estimating the behavior policy. Prior work has explored some forms of Q-function penalties [22,47], but only in the standard online RL setting augmented with demonstrations. Luo et al.\n\n[34] learn a conservatively-extrapolated value function, V (s) by enforcing a linear extrapolation property over the state-space, and use it alongside a learned dynamics model to obtain policies for goal-reaching tasks. Alternate prior approaches to offline RL estimate some sort of uncertainty to determine the trustworthiness of a Q-value prediction [27, 2, 30], typically using uncertainty estimation techniques from exploration in online RL [40,24,39,6]. These methods have not been generally performant in offline RL [14,27,30] due to the high-fidelity requirements of uncertainty estimates in offline RL [30]. The gap-expanding property of CQL backups, shown in Theorem 3.4, is related to how gap-increasing Bellman backup operators [5,33] are more robust to estimation error in online RL.\n\n\nExperimental Evaluation\n\nWe compare CQL to prior offline RL methods on a range of domains and dataset compositions, including continuous and discrete action spaces, state observations of varying dimensionality, and high-dimensional image inputs. We first evaluate actor-critic CQL, using CQL(H) from Algorithm 1, on continuous control datasets from the D4RL benchmark [11]. We compare to: prior offline RL methods that use a policy constraint -BEAR [27] and BRAC [48]; SAC [18], an off-policy actor-critic method that we adapt to offline setting; and behavioral cloning (BC).\n\nGym domains. Results for the gym domains are shown in Table 1. The results for BEAR, BRAC, SAC, and BC are based on numbers reported by Fu et al. [11]. On the datasets generated from a single policy, marked as \"-random\", \"-expert\" and \"-medium\", CQL roughly matches or exceeds the best prior methods, but by a small margin. However, on datasets that combine multiple policies (\"-mixed\", \"-medium-expert\" and \"-random-expert\"), that are more likely to be common in practical datasets, CQL outperforms prior methods by large margins, sometimes as much as 2-3x.\n\nAdroit tasks. The more complex Adroit [45] tasks (shown on the right) in D4RL require controlling a 24-DoF robotic hand, using limited data from human demonstrations. These tasks are substantially more difficult than the gym tasks in terms of both the dataset composition and high dimensionality. Prior offline RL methods generally struggle to learn meaningful behaviors on these tasks, and the strongest baseline is BC. As shown in Table 2, CQL variants are the only methods that improve over BC, attaining scores that are 2-9x those of the next best offline RL method. CQL(\u03c1) with \u03c1 =\u03c0 k\u22121 (the previous policy) outperforms CQL(H) on a number of these tasks, due  Table 1: Performance of CQL(H) and prior methods on gym domains from D4RL, on the normalized return metric, averaged over 4 seeds. Note that CQL performs similarly or better than the best prior method with simple datasets, and greatly outperforms prior methods with complex distributions (\"-mixed\", \"-random-expert\", \"-medium-expert\").\n\nto the higher action dimensionality resulting in higher variance for the CQL(H) importance weights. Both variants outperform prior methods.  Table 2: Normalized scores of all methods on AntMaze, Adroit, and kitchen domains from D4RL, averaged across 4 seeds. On the harder mazes, CQL is the only method that attains non-zero returns, and is the only method to outperform simple behavioral cloning on Adroit tasks with human demonstrations. We observed that the CQL(\u03c1) variant, which avoids importance weights, trains more stably, with no sudden fluctuations in policy performance over the course of training, on the higher-dimensional Adroit tasks.\nDomain Task Name BC SAC BEAR BRAC-p BRAC-v CQL(H) CQL(\u03c1)\nAntMaze. These D4RL tasks require composing parts of suboptimal trajectories to form more optimal policies for reaching goals on a MuJoco Ant robot. Prior methods make some progress on the simpler U-maze, but only CQL is able to make meaningful progress on the much harder medium and large mazes, outperforming prior methods by a very wide margin.\n\nKitchen tasks. Next, we evaluate CQL on the Franka kitchen domain [17] from D4RL [13]. The goal is to control a 9-DoF robot to manipulate multiple objects (microwave, kettle, etc.) sequentially, in a single episode to reach a desired configuration, with only sparse 0-1 completion reward for every object that attains the target configuration. These tasks are especially challenging, since they require composing parts of trajectories, precise long-horizon manipulation, and handling human-provided teleoperation data. As shown in Table 2, CQL outperforms prior methods in this setting, and is the only method that outperforms behavioral cloning, attaining over 40% success rate on all tasks.\n\nOffline RL on Atari games. Lastly, we evaluate a discrete-action Q-learning variant of CQL (Algorithm 1) on offline, image-based Atari games [4]. We compare CQL to REM [2] and QR-DQN [7] on the five Atari tasks (Pong, Breakout, Qbert, Seaquest and Asterix) that are evaluated in detail by Agarwal et al. [2], using the dataset released by the authors. provided with only the first 20% of the samples of an online DQN run. Note that CQL is able to learn stably on 3 out of 4 games, and its performance does not degrade as steeply as QR-DQN on Seaquest.\n\nFollowing the evaluation protocol of Agarwal et al. [2], we evaluated on two types of datasets, both of which were generated from the DQN-replay dataset, released by [2]: (1) a dataset consisting of the first 20% of the samples observed by an online DQN agent and (2) datasets consisting of only 1% and 10% of all samples observed by an online DQN agent ( Figures 6 and 7 in [2]). In setting (1), shown in Figure 1, CQL generally achieves similar or better performance throughout as QR-DQN and REM. When only using only 1% or 10% of the data, in setting (2) (   We also present an empirical analysis to show that Theorem 3.4, that CQL is gap-expanding, holds in practice in Appendix B, and present an ablation study on various design choices used in CQL in Appendix G.\n\n\nDiscussion\n\nWe proposed conservative Q-learning (CQL), an algorithmic framework for offline RL that learns a lower bound on the policy value. Empirically, we demonstrate that CQL outperforms prior offline RL methods on a wide range of offline RL benchmark tasks, including complex control tasks and tasks with raw image observations. In many cases, the performance of CQL is substantially better than the best-performing prior methods, exceeding their final returns by 2-5x. The simplicity and efficacy of CQL make it a promising choice for a wide range of real-world offline RL problems. However, a number of challenges remain. While we prove that CQL learns lower bounds on the Q-function in the tabular, linear, and a subset of non-linear function approximation cases, a rigorous theoretical analysis of CQL with deep neural nets, is left for future work. Additionally, offline RL methods are liable to suffer from overfitting in the same way as standard supervised methods, so another important challenge for future work is to devise simple and effective early stopping methods, analogous to validation error in supervised learning.\n\n[ \n\n\nAppendices A Discussion of CQL Variants\n\nWe derive several variants of CQL in Section 3.2. Here, we discuss these variants on more detail and describe their specific properties. We first derive the variants: CQL(H), CQL(\u03c1), and then present another variant of CQL, which we call CQL(var). This third variant has strong connections to distributionally robust optimization [37].\n\nCQL(H). In order to derive CQL(H), we substitute R = H(\u00b5), and solve the optimization over \u00b5 in closed form for a given Q-function. For an optimization problem of the form:\nmax \u00b5 E x\u223c\u00b5(x) [f (x)] + H(\u00b5) s.t. x \u00b5(x) = 1, \u00b5(x) \u2265 0 \u2200x,\nthe optimal solution is equal to \u00b5 * (x) = 1 Z exp(f (x)), where Z is a normalizing factor. Plugging this into Equation 3, we exactly obtain Equation 4.\n\nCQL(\u03c1). In order to derive CQL(\u03c1), we follow the above derivation, but our regularizer is a KLdivergence regularizer instead of entropy.\nmax \u00b5 E x\u223c\u00b5(x) [f (x)] + D KL (\u00b5||\u03c1) s.t. x \u00b5(x) = 1, \u00b5(x) \u2265 0 \u2200x.\nThe optimal solution is given by,\n\u00b5 * (x) = 1 Z \u03c1(x) exp(f (x)),\nwhere Z is a normalizing factor. Plugging this back into the CQL family (Equation 3), we obtain the following objective for training the Q-function (modulo some normalization terms):\nmin Q \u03b1E s\u223cd \u03c0 \u03b2 (s) E a\u223c\u03c1(a|s) Q(s, a) exp(Q(s, a)) Z \u2212 E a\u223c\u03c0 \u03b2 (a|s) [Q(s, a)] + 1 2 E s,a,s \u223cD Q \u2212 B \u03c0 kQ k 2 .(5)\nCQL(var). Finally, we derive a CQL variant that is inspired from the perspective of distributionally robust optimization (DRO) [37]. This version penalizes the variance in the Q-function across actions at all states s, under some action-conditional distribution of our choice. In order to derive a canonical form of this variant, we invoke an identity from Namkoong and Duchi [37], which helps us simplify Equation 3. To start, we define the notion of \"robust expectation\": for any function f (x), and any empirical distributionP (x) over a dataset {x 1 , \u00b7 \u00b7 \u00b7 , x N } of N elements, the \"robust\" expectation defined by:\nR N (P ) := max \u00b5(x) E x\u223c\u00b5(x) [f (x)] s.t. D f (\u00b5(x),P (x)) \u2264 \u03b4 N ,\ncan be approximated using the following upper-bound:\nR N (P ) \u2264 E x\u223cP (x) [f (x)] + 2\u03b4 varP (x) (f (x)) N ,(6)\nwhere the gap between the two sides of the inequality decays inversely w.r.t. to the dataset size, O(1/N ). By using Equation 6 to simplify Equation 3, we obtain an objective for training the Q-function that penalizes the variance of Q-function predictions under the distributionP . The only remaining decision is the choice ofP , which can be chosen to be the inverse of the empirical action distribution in the dataset,P (a|s) \u221d 1 D(a|s) , or even uniform over actions,P (a|s) = Unif(a), to obtain this variant of variance-regularized CQL.\n\n\nB Discussion of Gap-Expanding Behavior of CQL Backups\n\nIn this section, we discuss in detail the consequences of the gap-expanding behavior of CQL backups over prior methods based on policy constraints that, as we show in this section, may not exhibit such gap-expanding behavior in practice. To recap, Theorem 3.4 shows that the CQL backup operator increases the difference between expected Q-value at in-distribution (a \u223c \u03c0 \u03b2 (a|s)) and out-ofdistribution (a s.t. \u00b5 k (a|s) \u03c0 \u03b2 (a|s) << 1) actions. We refer to this property as the gap-expanding property of the CQL update operator.\n\nFunction approximation may give rise to erroneous Q-values at OOD actions. We start by discussing the behavior of prior methods based on policy constraints [27,14,25,48] in the presence of function approximation. To recap, because computing the target value requires E \u03c0 [Q(s, a)], constraining \u03c0 to be close to \u03c0 \u03b2 will avoid evaluatingQ on OOD actions. These methods typically do not impose any further form of regularization on the learned Q-function. Even with policy constraints, because function approximation used to represent the Q-function, learned Q-values at two distinct state-action pairs are coupled together. As prior work has argued and shown [1,12,28], the \"generalization\" or the coupling effects of the function approximator may be heavily influenced by the properties of the data distribution [12,28]. For instance, Fu et al. [12] empirically shows that when the dataset distribution is narrow (i.e. state-action marginal entropy, H(d \u03c0 \u03b2 (s, a)), is low [12]), the coupling effects of the Q-function approximator can give rise to incorrect Q-values at different states, though this behavior is absent without function approximation, and is not as severe with high-entropy (e.g. Uniform) state-action marginal distributions.\n\nIn offline RL, we will shortly present empirical evidence on high-dimensional MuJoCo tasks showing that certain dataset distributions, D, may cause the learned Q-value at an OOD action a at a state s, to in fact take on high values than Q-values at in-distribution actions at intermediate iterations of learning. This problem persists even when a large number of samples (e.g. 1M ) are provided for training, and the agent cannot correct these errors due to no active data collection.\n\nSince actor-critic methods, including those with policy constraints, use the learned Q-function to train the policy, in an iterative online policy evaluation and policy improvement cycle, as discussed in Section 2, the errneous Q-function may push the policy towards OOD actions, especially when no policy constraints are used. Of course, policy constraints should prevent the policy from choosing OOD actions, however, as we will show that in certain cases, policy constraint methods might also fail to prevent the effects on the policy due to incorrectly high Q-values at OOD actions.\n\nHow can CQL address this problem? As we show in Theorem 3.4, the difference between expected Q-values at in-distribution actions and out-of-distribution actions is expanded by the CQL update. This property is a direct consequence of the specific nature of the CQL regularizer -that maximizes Q-values under the dataset distribution, and minimizes them otherwise. This difference depends upon the choice of \u03b1 k , which can directly be controlled, since it is a free parameter. Thus, by effectively controlling \u03b1 k , CQL can push down the learned Q-value at out-of-distribution actions as much is desired, correcting for the erroneous overestimation error in the process.\n\nEmpirical evidence on high-dimensional benchmarks with neural networks. We next empirically demonstrate the existence of of such Q-function estimation error on high-dimensional MuJoCo domains when deep neural network function approximators are used with stochastic optimization techniques. In order to measure this error, we plot the difference in expected Q-value under actions sampled from the behavior distribution, a \u223c \u03c0 \u03b2 (a|s), and the maximum Q-value over actions sampled from a uniformly random policy, a \u223c Unif(a|s). That is, we plot the quantit\u0177\n\u2206 k = E s,a\u223cD max a 1 ,\u00b7\u00b7\u00b7 ,a N \u223cUnif(a ) [Q k (s, a )] \u2212Q k (s, a)(8)\nover the iterations of training, indexed by k. This quantity, intuitively, represents an estimate of the \"advantage\" of an action a, under the Q-function, with respect to the optimal action max a Q k (s, a ).\n\nSince, we cannot perform exact maximization over the learned Q-function in a continuous action space to compute \u2206, we estimate it via sampling described in Equation 8.\n\nWe present these plots in Figure 2 on two datasets: hopper-expert and hopper-medium. The expert dataset is generated from a near-deterministic, expert policy, exhibits a narrow coverage of the state-action space, and limited to only a few directed trajectories. On this dataset, we find that\u2206 k is always positive for the policy constraint method (Figure 2(a)) and increases during training -note, the continuous rise in\u2206 k values, in the case of the policy-constraint method, shown in Figure 2(a). This means that even if the dataset is generated from an expert policy, and policy constraints correct target values for OOD actions, incorrect Q-function generalization may make an out-of-distribution action appear promising. For the more stochastic hopper-medium dataset, that consists of a more diverse set of trajectories, shown in Figure 2(b), we still observe that\u2206 k > 0 for the policy-constraint method, however, the relative magnitude is smaller than hopper-expert.\n\nIn contrast, Q-functions learned by CQL, generally satisfy\u2206 k < 0, as is seen and these values are clearly smaller than those for the policy-constraint method. This provides some empirical evidence for Theorem 3.4, in that, the maximum Q-value at a randomly chosen action from the uniform distribution the action space is smaller than the Q-value at in-distribution actions.\n\nOn the hopper-expert task, as we show in Figure 2(a) (right), we eventually observe an \"unlearning\" effect, in the policy-constraint method where the policy performance deteriorates after a extra iterations in training. This \"unlearning\" effect is similar to what has been observed when standard off-policy Q-learning algorithms without any policy constraint are used in the offline regime [27, 30], on the other hand this effect is absent in the case of CQL, even after equally many training steps. The performance in the more-stochastic hopper-medium dataset fluctuates, but does not deteriorate suddenly.\n\nTo summarize this discussion, we concretely observed the following points via empirical evidence:\n\n\u2022 CQL backups are gap expanding in practice, as justified by the negative\u2206 k values in Figure 2. \u2022 Policy constraint methods, that do not impose any regularization on the Q-function may observe highly positive\u2206 k values during training, especially with narrow data distributions, indicating that gap-expansion may be absent. \u2022 When\u2206 k values continuously grow during training, the policy might eventually suffer from an unlearning effect [30], as shown in Figure 2(a).\n\n\nC Theorem Proofs\n\nIn this section, we provide proofs of the theorems in Sections 3.1 and 3.2. We first redefine notation for clarity and then provide the proofs of the results in the main paper.\n\nNotation. Let k \u2208 N denote an iteration of policy evaluation (in Section 3.1) or Q-iteration (in Section 3.2). In an iteration k, the objective -Equation 2 or Equation 3 -is optimized using the previous iterate (i.e.Q k\u22121 ) as the target value in the backup. Q k denotes the true, tabular Q-function iterate in the MDP, without any correction. In an iteration, say k + 1, the current tabular Q-function iterate, Q k+1 is related to the previous tabular Q-function iterate Q k as: Q k+1 = B \u03c0 Q k (for policy evaluation) or Q k+1 = B \u03c0 k Q k (for policy learning). LetQ k denote the k-th Q-function iterate obtained from CQL. LetV k denote the value function,V k := E a\u223c\u03c0(a|s) [Q k (s, a)].\n\nA note on the value of \u03b1. Before proving the theorems, we remark that while the statements of Theorems 3.2, 3.1 and D.1 (we discuss this in Appendix D) show that CQL produces lower bounds if \u03b1 is larger than some threshold, so as to overcome either sampling error (Theorems 3.2 and 3.1) or function approximation error (Theorem D.1). While the optimal \u03b1 k in some of these cases depends on the current Q-value,Q k , we can always choose a worst-case value of \u03b1 k by using the inequalit\u0177 Q k \u2264 R max /(1 \u2212 \u03b3), still guaranteeing a lower bound.\n\nWe first prove Theorem 3.1, which shows that policy evaluation using a simplified version of CQL (Equation 1) results in a point-wise lower-bound on the Q-function.\n\nProof of Theorem 3.1. In order to start, we first note that the form of the resulting Q-function iterate, Q k , in the setting without function approximation. By setting the derivative of Equation 1 to 0, we obtain the following expression forQ k+1 in terms ofQ k , \u2200 s, a \u2208 D, k,Q k+1 (s, a) =B \u03c0Qk (s, a) \u2212 \u03b1 \u00b5(a|s) \u03c0 \u03b2 (a|s) .\n\nNow, since, \u00b5(a|s) > 0, \u03b1 > 0,\u03c0 \u03b2 (a|s) > 0, we observe that at each iteration we underestimate the next Q-value iterate, i.e.Q k+1 \u2264B \u03c0Qk .\n\nAccounting for sampling error. Note that so far we have only shown that the Q-values are upperbounded by the the \"empirical Bellman targets\" given by,B \u03c0Qk . In order to relateQ k to the true Q-value iterate, Q k , we need to relate the empirical Bellman operator,B \u03c0 to the actual Bellman operator, B \u03c0 . In Appendix D.3, we show that if the reward function r(s, a) and the transition function, T (s |s, a) satisfy \"concentration\" properties, meaning that the difference between the observed reward sample, r (s, a, r, s ) \u2208 D) and the actual reward function r(s, a) (and analogously for the transition matrix) is bounded with high probability, then overestimation due to the empirical Backup operator is bounded. Formally, with high probability (w.h.p.) \u2265 1 \u2212 \u03b4, \u03b4 \u2208 (0, 1),\n\u2200Q, s, a \u2208 D, B \u03c0 Q(s, a) \u2212 B \u03c0 Q(s, a) \u2264 C r,T,\u03b4 R max 1 \u2212 \u03b3 .\nHence, the following can be obtained, w.h.p.:\n\nQ k+1 (s, a) = B \u03c0Qk (s, a) \u2264 B \u03c0Qk (s, a) \u2212 \u03b1 \u00b5(a|s) \u03c0 \u03b2 (a|s) + C r,T,\u03b4 R max 1 \u2212 \u03b3 .\n\nNow we need to reason about the fixed point of the update procedure in Equation 9. The fixed point of Equation 9 is given by:\nQ \u03c0 \u2264 B \u03c0Q\u03c0 \u2212\u03b1 \u00b5(a|s) \u03c0 \u03b2 (a|s) + C r,T,\u03b4 R max 1 \u2212 \u03b3 =\u21d2Q \u03c0 \u2264 (I\u2212\u03b3P \u03c0 ) \u22121 R \u2212 \u03b1 \u00b5(a|s) \u03c0 \u03b2 (a|s) + C r,T,\u03b4 R max 1 \u2212 \u03b3 Q \u03c0 (s, a) \u2264 Q \u03c0 (s, a) \u2212 \u03b1 (I \u2212 \u03b3P \u03c0 ) \u22121 \u00b5(a|s) \u03c0 \u03b2 (a|s) (s, a) + (I \u2212 \u03b3P \u03c0 ) \u22121 C r,T,\u03b4 R max 1 \u2212 \u03b3 ,\nthus proving the relationship in Theorem 3.1.\n\nIn order to guarantee a lower bound, \u03b1 can be chosen to cancel any potential overestimation incurred by C r,T ,\u03b4 Rmax 1\u2212\u03b3 . Note that this choice works, since (I \u2212 \u03b3P \u03c0 ) \u22121 is a matrix with all non-negative entries. The choice of \u03b1 that guarantees a lower bound is then given by:\n\u03b1 \u00b7 min s,a \u00b5(a|s) \u03c0 \u03b2 (a|s) \u2265 C r,T,\u03b4 R max 1 \u2212 \u03b3 =\u21d2 \u03b1 \u2265 C r,T,\u03b4 R max 1 \u2212 \u03b3 \u00b7 max s,a \u00b5(a|s) \u03c0 \u03b2 (a|s) \u22121 .\nOf course, we need \u00b5(a|s) > 0 whenever\u03c0 \u03b2 (a|s) > 0, for this to hold, and this is assumed in the theorem statament. Note that since, C r,T ,\u03b4 Rmax 1\u2212\u03b3 = 0, whenB \u03c0 = B \u03c0 , any \u03b1 \u2265 0 guarantees a lower bound.\n\nNext, we prove Theorem 3.3 that shows that the additional term that maximizes the expected Q-value under the dataset distribution, D(s, a), (or d \u03c0 \u03b2 (s)\u03c0 \u03b2 (a|s), in the absence of sampling error), results in a lower-bound on only the expected value of the policy at a state, and not a pointwise lower-bound on Q-values at all actions.\n\nProof of Theorem 3.2. We first prove this theorem in the absence of sampling error, and then incorporate sampling error at the end, using a technique similar to the previous proof. In the tabular setting, we can set the derivative of the modified objective in Equation 2, and compute the Q-function update induced in the exact, tabular setting (this assumesB \u03c0 = B \u03c0 ) and \u03c0 \u03b2 (a|s) =\u03c0 \u03b2 (a|s)).\n\n\u2200 s, a, kQ k+1 (s, a) = B \u03c0Qk (s, a) \u2212 \u03b1 \u00b5(a|s) \u03c0 \u03b2 (a|s) \u2212 1 .\n\nNote that for state-action pairs, (s, a), such that, \u00b5(a|s) < \u03c0 \u03b2 (a|s), we are infact adding a positive quantity, 1 \u2212 \u00b5(a|s) \u03c0 \u03b2 (a|s) , to the Q-function obtained, and this we cannot guarantee a point-wise lower bound, i.e . \u2203 s, a, s.t.Q k+1 (s, a) \u2265 Q k+1 (s, a). To formally prove this, we can construct a counter-example three-state, two-action MDP, and choose a specific behavior policy \u03c0(a|s), such that this is indeed the case.\n\nThe value of the policy, on the other hand,V k+1 is underestimated, since:\nV k+1 (s) := E a\u223c\u03c0(a|s) Q k+1 (s, a) = B \u03c0V k (s) \u2212 \u03b1E a\u223c\u03c0(a|s) \u00b5(a|s) \u03c0 \u03b2 (a|s) \u2212 1 .(12)\nand we can show that D CQL (s) := a \u03c0(a|s) \u00b5(a|s) \u03c0 \u03b2 (a|s) \u2212 1 is always positive, when \u03c0(a|s) = \u00b5(a|s). To note this, we present the following derivation:\nD CQL (s) := a \u03c0(a|s) \u00b5(a|s) \u03c0 \u03b2 (a|s) \u2212 1 = a\n(\u03c0(a|s) \u2212 \u03c0 \u03b2 (a|s) + \u03c0 \u03b2 (a|s)) \u00b5(a|s) \u03c0 \u03b2 (a|s) \u2212 1 = a (\u03c0(a|s) \u2212 \u03c0 \u03b2 (a|s)) \u03c0(a|s) \u2212 \u03c0 \u03b2 (a|s) \u03c0 \u03b2 (a|s + a \u03c0 \u03b2 (a|s) \u00b5(a|s) \u03c0 \u03b2 (a|s) \u2212 1 = a (\u03c0(a|s) \u2212 \u03c0 \u03b2 (a|s)) 2 \u03c0 \u03b2 (a|s) \u22650 + 0 since, a \u03c0(a|s) = a \u03c0 \u03b2 (a|s) = 1.\n\nNote that the marked term, is positive since both the numerator and denominator are positive, and this implies that D CQL (s) \u2265 0. Also, note that D CQL (s) = 0, iff \u03c0(a|s) = \u03c0 \u03b2 (a|s). This implies that each value iterate incurs some underestimation,V k+1 (s) \u2264 B \u03c0V k (s). Now, we can compute the fixed point of the recursion in Equation 12, and this gives us the following estimated policy value:\nV \u03c0 (s) = V \u03c0 (s) \u2212 \u03b1 (I \u2212 \u03b3P \u03c0 ) \u22121 non-negative entries E \u03c0(a|s) \u03c0(a|s) \u03c0 \u03b2 (a|s) \u2212 1 \u22650 ,\nthus showing that in the absence of sampling error, Theorem 3.2 gives a lower bound. It is straightforward to note that this expression is tighter than the expression for policy value in Proposition 3.2, since, we explicitly subtract 1 in the expression of Q-values (in the exact case) from the previous proof.\n\nIncorporating sampling error. To extend this result to the setting with sampling error, similar to the previous result, the maximal overestimation at each iteration k, is bounded by\nC r,T ,\u03b4 Rmax 1\u2212\u03b3 .\nThe resulting value-function satisfies (w.h.p.),\nV \u03c0 (s) \u2264 V \u03c0 (s) \u2212 \u03b1(I \u2212 \u03b3P \u03c0 ) \u22121 E \u03c0(a|s) \u03c0(a|s) \u03c0 \u03b2 (a|s) \u2212 1 + (I \u2212 \u03b3P \u03c0 ) \u22121 C r,T,\u03b4 R max 1 \u2212 \u03b3 ,\nthus proving the theorem statement. In this case, the choice of \u03b1, that prevents overestimation w.h.p. is given by:\n\u03b1 \u2265 C r,T R max 1 \u2212 \u03b3 \u00b7 max\ns\u2208D a\u2208D \u03c0(a|s) \u03c0(a|s) \u03c0 \u03b2 (a|s)) \u2212 1 \u22121 .\n\nNext we provide a proof for Theorem 3.3.\n\nProof of Theorem 3.3. In order to prove this theorem, we compute the difference induced in the policy value,V k+1 , derived from the Q-value iterate,Q k+1 , with respect to the previous iterate B \u03c0Qk . If this difference is negative at each iteration, then the resulting Q-values are guaranteed to lower bound the true policy value.\nE\u03c0k+1 (a|s) [Q k+1 (s, a)] = E\u03c0k+1 (a|s) B \u03c0Qk (s, a) \u2212 E\u03c0k+1 (a|s) \u03c0Q k (a|s) \u03c0 \u03b2 (a|s) \u2212 1 = E\u03c0k+1 (a|s) B \u03c0Qk (s, a) \u2212 E \u03c0Q k (a|s) \u03c0Q k (a|s) \u03c0 \u03b2 (a|s) \u2212 1 underestimation, (a) + a \u03c0Q k (a|s) \u2212\u03c0 k+1 (a|s) (b), \u2264DTV(\u03c0Q k ,\u03c0 k+1 ) \u03c0Q k (a|s) \u03c0 \u03b2 (a|s)\nIf (a) has a larger magnitude than (b), then the learned Q-value induces an underestimation in an iteration k + 1, and hence, by a recursive argument, the learned Q-value underestimates the optimal Q-value. We note that by upper bounding term (b) by D TV (\u03c0Q k ,\u03c0 k+1 ) \u00b7 max a \u03c0Q k (a|s) \u03c0 \u03b2 (a|s) , and writing out (a) > upper-bound on (b), we obtain the desired result.\n\nFinally, we show that under specific choices of \u03b1 1 , \u00b7 \u00b7 \u00b7 , \u03b1 k , the CQL backup is gap-expanding by providing a proof for Theorem 3.4\n\nProof of Theorem 3.4 (CQL is gap-expanding). For this theorem, we again first present the proof in the absence of sampling error, and then incorporate sampling error into the choice of \u03b1. We follow the strategy of observing the Q-value update in one iteration. Recall that the expression for the Q-value iterate at iteration k is given by:\nQ k+1 (s, a) = B \u03c0 kQ k (s, a) \u2212 \u03b1 k \u00b5 k (a|s) \u2212 \u03c0 \u03b2 (a|s) \u03c0 \u03b2 (a|s) .\nNow, the value of the policy \u00b5 k (a|s) underQ k+1 is given by:\nE a\u223c\u00b5 k (a|s) [Q k+1 (s, a)] = E a\u223c\u00b5 k (a|s) [B \u03c0 kQ k (s, a)] \u2212 \u03b1 k \u00b5 T k \u00b5 k (a|s) \u2212 \u03c0 \u03b2 (a|s) \u03c0 \u03b2 (a|s)\n:=\u2206 k ,\u22650, by proof of Theorem 3.2. Now, we also note that the expected amount of extra underestimation introduced at iteration k under action sampled from the behavior policy \u03c0 \u03b2 (a|s) is 0, as,\nE a\u223c\u03c0 \u03b2 (a|s) [Q k+1 (s, a)] = E a\u223c\u03c0 \u03b2 (a|s) [B \u03c0 kQ k (s, a)] \u2212 \u03b1 k \u03c0 \u03b2 T \u00b5 k (a|s) \u2212 \u03c0 \u03b2 (a|s) \u03c0 \u03b2 (a|s) =0 .\nwhere the marked quantity is equal to 0 since it is equal since \u03c0 \u03b2 (a|s) in the numerator cancels with the denominator, and the remaining quantity is a sum of difference between two density functions, a \u00b5 k (a|s) \u2212 \u03c0 \u03b2 (a|s), which is equal to 0. Thus, we have shown that,\nE \u03c0 \u03b2 (a|s) [Q k+1 (s, a)]\u2212E \u00b5 k (a|s) [Q k+1 (s, a)] = E \u03c0 \u03b2 (a|s) [B \u03c0 kQ k (s, a)]\u2212E \u00b5 k (a|s) [B \u03c0 kQ k (s, a)]\u2212\u03b1 k\u2206 k . Now subtracting the difference, E \u03c0 \u03b2 (a|s) [Q k+1 (s, a)] \u2212 E \u00b5 k (a|s) [Q k+1 (s, a)]\n, computed under the tabular Q-function iterate, Q k+1 , from the previous equation, we obtain that\nE a\u223c\u03c0 \u03b2 (a|s) [Q k+1 (s, a)] \u2212 E \u03c0 \u03b2 (a|s) [Q k+1 (s, a)] = E \u00b5 k (a|s) [Q k+1 (s, a)] \u2212 E \u00b5 k (a|s) [Q k+1 (s, a)]+ (\u00b5 k (a|s) \u2212 \u03c0 \u03b2 (a|s)) T B \u03c0 k Q k \u2212 Q k (s, \u00b7) (a) \u2212\u03b1 k\u2206 k .\nNow, by choosing \u03b1 k , such that any positive bias introduced by the quantity (\u00b5 k (a|s) \u2212 \u03c0 \u03b2 (a|s)) T (a) is cancelled out, we obtain the following gap-expanding relationship:\nE a\u223c\u03c0 \u03b2 (a|s) [Q k+1 (s, a)] \u2212 E \u03c0 \u03b2 (a|s) [Q k+1 (s, a)] > E \u00b5 k (a|s) [Q k+1 (s, a)] \u2212 E \u00b5 k (a|s) [Q k+1 (s, a)]\nfor, \u03b1 k satisfying,\n\u03b1 k > max \uf8eb \uf8ed (\u03c0 \u03b2 (a|s) \u2212 \u00b5 k (a|s)) T B \u03c0 k Q k \u2212 Q k (s, \u00b7) \u2206k , 0 \uf8f6 \uf8f8 ,\nthus proving the desired result.\n\nTo avoid the dependency on the true Q-value iterate, Q k , we can upper-bound Q k by Rmax 1\u2212\u03b3 , and upperbound (\u03c0 \u03b2 (a|s) \u2212 \u00b5 k (a|s)) T B \u03c0 k Q k (s, \u00b7) by D TV (\u03c0 \u03b2 , \u00b5 k ) \u00b7 Rmax 1\u2212\u03b3 , and use this in the expression for \u03b1 k . While this bound may be loose, it still guarantees the gap-expanding property, and we indeed empirically show the existence of this property in practice in Appendix B.\n\nTo incorporate sampling error, we can follow a similar strategy as previous proofs: the worst case overestimation due to sampling error is given by\nC r,T ,\u03b4 Rmax 1\u2212\u03b3 .\nIn this case, we note that, w.h.p.,\nB \u03c0 k Q k \u2212 Q k \u2212 B \u03c0 k Q k \u2212 Q k \u2264 2 \u00b7 C r,T,\u03b4 R max 1 \u2212 \u03b3 .\nHence, the presence of sampling error adds D TV (\u03c0 \u03b2 , \u00b5 k ) \u00b7 2\u00b7C r,T ,\u03b4 Rmax 1\u2212\u03b3 to the value of \u03b1 k , giving rise to the following, sufficient condition on \u03b1 k for the gap-expanding property:\n\u03b1 k > max \uf8eb \uf8ed (\u03c0 \u03b2 (a|s) \u2212 \u00b5 k (a|s)) T B \u03c0 k Q k \u2212 Q k (s, \u00b7) \u2206k + D TV (\u03c0 \u03b2 , \u00b5 k ) \u00b7 2 \u00b7 C r,T,\u03b4 R max 1 \u2212 \u03b3 , 0 \uf8f6 \uf8f8 ,\nconcluding the proof of this theorem.\n\n\nD Additional Theoretical Analysis\n\nIn this section, we present a theoretical analysis of additional properties of CQL. For ease of presentation, we state and prove theorems in Appendices D.1 and D.2 in the absence of sampling error, but as discussed extensively in Appendix C, we can extend each of these results by adding extra terms induced due to sampling error.\n\n\nD.1 CQL with Linear and Non-Linear Function Approximation\n\nTheorem D.1. Assume that the Q-function is represented as a linear function of given state-action feature vectors F, i.e., Q(s, a) = w T F(s, a). Let D = diag (d \u03c0 \u03b2 (s)\u03c0 \u03b2 (a|s)) denote the diagonal matrix with data density, and assume that F T DF is invertible. Then, the expected value of the policy under Q-value from Eqn 2 at iteration k + 1, a)], lower-bounds the corresponding tabular value,\nE d \u03c0 \u03b2 (a) [V k+1 (s)] = E d \u03c0 \u03b2 (s),\u03c0(a|s) [Q k+1 (s,E d \u03c0 \u03b2 (s) [V k+1 (s)] = E d \u03c0 \u03b2 (s),\u03c0(a|s) [Q k+1 (s, a)], if \u03b1 k \u2265 max \uf8eb \uf8ed D T F F T DF \u22121 F T \u2212 I (B \u03c0Qk )(s, a) D T F (F T DF) \u22121 F T D \u03c0(a|s)\u2212\u03c0 \u03b2 (a|s) \u03c0 \u03b2 (a|s) , 0 \uf8f6 \uf8f8 .\nThe choice of \u03b1 k in Theorem D.1 intuitively amounts to compensating for overestimation in value induced if the true value function cannot be represented in the chosen linear function class (numerator), by the potential decrease in value due to the CQL regularizer (denominator). This implies that if the actual value function can be represented in the linear function class, such that the numerator can be made 0, then any \u03b1 > 0 is sufficient to obtain a lower bound. We now prove the theorem.\n\nProof. In order to extend the result of Theorem 3.2 to account for function approximation, we follow the similar recipe as before. We obtain the optimal solution to the optimization problem below in the family of linearly expressible Q-functions, i.e. Q := {Fw|w \u2208 R dim(F) }.\nmin Q\u2208Q \u03b1 k \u00b7 E d \u03c0 \u03b2 (s),\u00b5(a|s) [Q(s, a)] \u2212 E d \u03c0 \u03b2 (s),\u03c0 \u03b2 (a|s) [Q(s, a)] + 1 2 E D Q(s, a) \u2212 B \u03c0Qk (s, a) 2 .\nBy substituting Q(s, a) = w T F(s, a), and setting the derivative with respect to w to be 0, we obtain, \u03b1 s,a d \u03c0 \u03b2 (s) \u00b7 (\u00b5(a|s) \u2212 \u03c0 \u03b2 (a|s)) F(s, a) + s,a d \u03c0 \u03b2 (s)\u03c0 \u03b2 (a|s) Q(s, a) \u2212 B \u03c0Qk (s, a) F(s, a) = 0.\n\nBy re-arranging terms, and converting it to vector notation, defining D = diag(d \u03c0 \u03b2 (s)\u03c0 \u03b2 (s)), and referring to the parameter w at the k-iteration as w k we obtain:\nF T DF w k+1 = F T D B \u03c0Qk LSTD iterate \u2212 \u03b1 k F T diag [d \u03c0 \u03b2 (s) \u00b7 (\u00b5(a|s) \u2212 \u03c0 \u03b2 (a|s))]\nunderestimation . Now, our task is to show that the term labelled as \"underestimation\" is indeed negative in expectation under \u00b5(a|s) (This is analogous to our result in the tabular setting that shows underestimated values).\n\nIn order to show this, we write out the expression for the value, under the linear weights w k+1 at state s, after substituting \u00b5 = \u03c0,\nV k+1 (s) := \u03c0(a|s) T Fw k+1 (13) = \u03c0(a|s) T F F T DF \u22121 F T D B \u03c0Qk value under LSTD-Q [29] \u2212\u03b1 k \u03c0(a|s) T F F T DF \u22121 F T D \u03c0(a|s) \u2212 \u03c0 \u03b2 (a|s) \u03c0 \u03b2 (a|s) .(14)\nNow, we need to reason about the penalty term. Defining, P F := F F T DF \u22121 F T D as the projection matrix onto the subspace of features F, we need to show that the product that appears as a penalty is positive: \u03c0(a|s) T P F \u03c0(a|s)\u2212\u03c0 \u03b2 (a|s) \u03c0 \u03b2 (a|s) \u2265 0. In order to show this, we compute minimum value of this product optimizing over \u03c0. If the minimum value is 0, then we are done.\n\nLet's define f (\u03c0) = \u03c0(a|s) T P F \u03c0(a|s)\u2212\u03c0 \u03b2 (a|s) \u03c0 \u03b2 (a|s)\n\n, our goal is to solve for min \u03c0 f (\u03c0). Setting the derivative of f (\u03c0) with respect to \u03c0 to be equal to 0, we obtain (including Lagrange multiplier \u03b7 that guarantees a \u03c0(a|s) = 1, P F + P T F \u03c0(a|s) \u03c0 \u03b2 (a|s) = P F 1 + \u03b7 1.\n\nBy solving for \u03b7 (using the condition that a density function sums to 1), we obtain that the minimum value of f (\u03c0) occurs at a \u03c0 * (a|s), which satisfies the following condition,\nP F + P T F \u03c0 * (a|s) \u03c0 \u03b2 (a|s) = P F + P T F 1.\nUsing this relation to compute f , we obtain, f (\u03c0 * ) = 0, indicating that the minimum value of 0 occurs when the projected density ratio matches under the matrix (P F + P T F ) is equal to the projection of a vector of ones, 1. Thus, \u2200 \u03c0(a|s), f (\u03c0) = \u03c0(a|s) T P F \u03c0(a|s) \u2212 \u03c0 \u03b2 (a|s) \u03c0 \u03b2 (a|s) \u2265 0.\n\nThis means that: \u2200 s,V k+1 (s) \u2264V k+1 LSTD-Q (s) given identical previousQ k values. This result indicates, that if \u03b1 k \u2265 0, the resulting CQL value estimate with linear function approximation is guaranteed to lower-bound the value estimate obtained from a least-squares temporal difference Q-learning algorithm (which only minimizes Bellman error assuming a linear Q-function parameterization), such as LSTD-Q [29], since at each iteration, CQL induces a lower-bound with respect to the previous value iterate, whereas this underestimation is absent in LSTD-Q, and an inductive argument is applicable.\n\nSo far, we have only shown that the learned value iterate,V k+1 (s) lower-bounds the value iterate obtained from LSTD-Q, \u2200 s,V k+1 (s) \u2264V k+1 LSTD-Q (s). But, our final aim is to prove a stronger result, that the learned value iterate,V k+1 , lower bounds the exact tabular value function iterate, V k+1 , at each iteration. The reason why our current result does not guarantee this is because function approximation may induce overestimation error in the linear approximation of the Q-function.\n\nIn order to account for this change, we make a simple change: we choose \u03b1 k such that the resulting penalty nullifes the effect of any over-estimation caused due to the inability to fit the true value function iterate in the linear function class parameterized by F. Formally, this means:\nE d \u03c0 \u03b2 (s) V k+1 (s) \u2264 E d \u03c0 \u03b2 (s) V k+1 LSTD-Q (s) \u2212 \u03b1 k E d \u03c0 \u03b2 (s) [f (\u03c0(a|s))] \u2264 E d \u03c0 \u03b2 (s) V k+1 (s) \u2212 E d \u03c0 \u03b2 (s) V k+1 LSTD-Q (s) \u2212 V k+1 (s) \u2212 \u03b1 k E d \u03c0 \u03b2 (s) [f (\u03c0(a|s))]\nchoose \u03b1 k to make this negative\n\u2264 E d \u03c0 \u03b2 (s) V k+1 (s)\nAnd the choice of \u03b1 k in that case is given by:\n\u03b1 k \u2265 max \uf8eb \uf8ed E d \u03c0 \u03b2 (s) V k+1 LSTD-Q (s) \u2212 V k+1 (s) E d \u03c0 \u03b2 (s) [f (\u03c0(a|s))] , 0 \uf8f6 \uf8f8 \u2265 max \uf8eb \uf8ed D T F F T DF \u22121 F T \u2212 I (B \u03c0Qk )(s, a) D T F (F T DF) \u22121 F T D \u03c0(a|s)\u2212\u03c0 \u03b2 (a|s) \u03c0 \u03b2 (a|s) , 0 \uf8f6 \uf8f8 .\nFinally, we note that since this choice of \u03b1 k induces under-estimation in the next iterate,V k+1 with respect to the previous iterate,V k , for all k \u2208 N, by induction, we can claim that this choice of \u03b1 1 , \u00b7 \u00b7 \u00b7 , \u03b1 k is sufficient to makeV k+1 lower-bound the tabular, exact value-function iterate. V k+1 , for all k, thus completing our proof.\n\nWe can generalize Theorem D.1 to non-linear function approximation, such as neural networks, under the standard NTK framework [23], assuming that each iteration k is performed by a single step of gradient descent on Equation 2, rather than a complete minimization of this objective. As we show in Theorem D.2, CQL learns lower bounds in this case for an appropriate choice of \u03b1 k . We will also empirically show in Appendix G that CQL can learn effective conservative Q-functions with multilayer neural networks. Theorem D.2 (Extension to non-linear function approximation). Assume that the Q-function is represented by a general non-linear function approximator parameterized by \u03b8, Q \u03b8 (s, a). let D = diag(d \u03c0 \u03b2 (s)\u03c0 \u03b2 (a|s)) denote the matrix with the data density on the diagonal, and assume that \u2207 \u03b8 Q T \u03b8 D\u2207 \u03b8 Q \u03b8 is invertible. Then, the expected value of the policy under the Q-function obtained by taking a gradient step on Equation 2, at iteration k + 1 lower-bounds the corresponding tabular function iterate if:\n\nProof. Our proof strategy is to reduce the non-linear optimization problem into a linear one, with features F (in Theorem D.1) replaced with features given by the gradient of the current Q-function Q k \u03b8 with respect to parameters \u03b8, i.e. \u2207 \u03b8Q k . To see, this we start by writing down the expression for Q k+1 \u03b8 obtained via one step of gradient descent with step size \u03b7, on the objective in Equation 2.\n\u03b8 k+1 = \u03b8 k \u2212 \u03b7\u03b1 k E d \u03c0 \u03b2 (s),\u00b5(a|s) \u2207 \u03b8Q k (s, a) \u2212 E d \u03c0 \u03b2 (s),\u03c0 \u03b2 (a|s) \u2207 \u03b8Q k (s, a)\n\u2212 \u03b7E d \u03c0 \u03b2 (s),\u03c0 \u03b2 (a|s) Q k \u2212 B \u03c0Qk (s, a) \u00b7 \u2207 \u03b8Q k (s, a) .\n\nUsing the above equation and making an approximation linearization assumption on the non-linear Q-function, for small learning rates \u03b7 << 1, as has been commonly used by prior works on the neural tangent kernel (NTK) in deep learning theory [23] in order to explain neural network learning dynamics in the infinite-width limit, we can write out the expression for the next Q-function iterate, Q k+1 \u03b8 in terms ofQ k \u03b8 as [1,23]:\nQ k+1 \u03b8 (s, a) \u2248Q k \u03b8 (s, a) + \u03b8 k+1 \u2212 \u03b8 k T \u2207 \u03b8Q k \u03b8 (s, a) (under NTK assumptions) =Q k \u03b8 (s, a) \u2212 \u03b7\u03b1 k E d \u03c0 \u03b2 (s ),\u00b5(a |s ) \u2207 \u03b8Q k (s , a ) T \u2207 \u03b8Q k (s, a) + \u03b7\u03b1 k E d \u03c0 \u03b2 (s ),\u03c0 \u03b2 (a |s ) \u2207 \u03b8Q k (s , a ) T \u2207 \u03b8Q k (s, a) \u2212 \u03b7E d \u03c0 \u03b2 (s ),\u03c0 \u03b2 (a |s ) Q k \u2212 B \u03c0Qk (s , a ) \u00b7 \u2207 \u03b8Q k (s , a ) T \u2207 \u03b8Q k (s, a) .\nTo simplify presentation, we convert into matrix notation, where we define the |S||A| \u00d7 |S||A| matrix,\nM k = \u2207 \u03b8Q k T\n\u2207 \u03b8Q k , as the neural tangent kernel matrix of the Q-function at iteration k. Then, the vectorizedQ k+1 (with \u00b5 = \u03c0) is given by,\nQ k+1 =Q k \u2212 \u03b7\u03b1 k M k D \u03c0(a|s) \u2212 \u03c0 \u03b2 (a|s) \u03c0 \u03b2 (a|s) + \u03b7M k D B \u03c0Qk \u2212Q k .\nFinally, the value of the policy is given by:\nV k+1 := \u03c0(a|s) TQk (s, a) + \u03b7\u03c0(a|s)M k D B \u03c0Qk \u2212Q k (a) unpenalized value \u2212 \u03b7\u03b1 k \u03c0(a|s) T M k D \u03c0(a|s) \u2212 \u03c0 \u03b2 (a|s) \u03c0 \u03b2 (a|s) (b) penalty .\n(15) Term marked (b) in the above equation is similar to the penalty term shown in Equation 14, and by performing a similar analysis, we can show that (b) \u2265 0. Again similar to howV k+1 LSTD-Q appeared in Equation 14, we observe that here we obtain the value function corresponding to a regular gradientstep on the Bellman error objective.\n\nAgain similar to before, term (a) can introduce overestimation relative to the tabular counterpart, starting atQ k : Q k+1 =Q k \u2212 \u03b7 B \u03c0Qk \u2212Q k , and we can choose \u03b1 k to compensate for this potential increase as in the proof of Theorem D.1. As the last step, we can recurse this argument to obtain our final result, for underestimation.\n\n\nD.2 Choice of Distribution to Maximize Expected Q-Value in Equation 2\n\nIn Section 3.1, we introduced a term that maximizes Q-values under the dataset d \u03c0 \u03b2 (s)\u03c0 \u03b2 (a|s) distribution when modifying Equation 1 to Equation 2. Theorem 3.2 indicates the \"sufficiency\" of maximizing Q-values under the dataset distribution -this guarantees a lower-bound on value. We now investigate the neccessity of this assumption: We ask the formal question: For which other choices of \u03bd(a|s) for the maximization term, is the value of the policy under the learned Q-value,Q k+1 \u03bd guaranteed to be a lower bound on the actual value of the policy? To recap and define notation, we restate the objective from Equation 1 below.\nQ k+1 \u2190 arg min Q \u03b1 E s\u223cd \u03c0 \u03b2 (s),a\u223c\u00b5(a|s) [Q(s, a)] + 1 2 E s,a,s \u223cD Q(s, a) \u2212 B \u03c0Qk (s, a) 2 .(16)\nWe define a general family of objectives from Equation 2, parameterized by a distribution \u03bd which is chosen to maximize Q-values as shown below (CQL is a special case, with \u03bd(a|s) = \u03c0 \u03b2 (a|s)): In order to answer our question, we prove the following result:\n\nTheorem D.3 (Necessity of maximizing Q-values under \u03c0 \u03b2 (a|s).). For any policy \u03c0(a|s), any \u03b1 > 0, and for all k > 0, the value of the policy.,V k+1 \u03bd under Q-function iterates from \u03bd\u2212CQL,Q k+1 \u03bd (s, a) is guaranteed to be a lower bound on the exact value iterate,V k+1 , only if \u03bd(a|s) = \u03c0 \u03b2 (a|s).\n\nProof. We start by noting the parametric form of the resulting tabular Q-value iterate:\nQ k+1 \u03bd (s, a) = B \u03c0Qk \u03bd (s, a) \u2212 \u03b1 k \u00b5(a|s) \u2212 \u03bd(a|s) \u03c0 \u03b2 (a|s) .(18)\nThe value of the policy under this Q-value iterate, when distribution \u00b5 is chosen to be the target policy \u03c0(a|s) i.e. \u00b5(a|s) = \u03c0(a|s) is given by:\nV k+1 \u03bd (s) := E a\u223c\u03c0(a|s) Q k+1 \u03bd (s, a) = E a\u223c\u03c0(a|s) B \u03c0Qk \u03bd (s, a) \u2212 \u03b1 k \u03c0(a|s) T \u03c0(a|s) \u2212 \u03bd(a|s) \u03c0 \u03b2 (a|s) .(19)\nWe are interested in conditions on \u03bd(a|s) such that the penalty term in the above equation is positive.\n\nIt is clear that choosing \u03bd(a|s) = \u03c0 \u03b2 (a|s) returns a policy that satisfies the requirement, as shown in the proof for Theorem 3.2. In order to obtain other choices of \u03bd(a|s) that guarantees a lower bound for all possible choices of \u03c0(a|s), we solve the following concave-convex maxmin optimization problem, that computes a \u03bd(a|s) for which a lower-bound is guaranteed for all choices of \u00b5(a|s): max \u03bd(a|s) min \u03c0(a|s) a \u03c0(a|s) \u00b7 \u03c0(a|s) \u2212 \u03bd(a|s) \u03c0 \u03b2 (a|s)\ns.t.\na \u03c0(a|s) = 1, a \u03bd(a|s) = 1, \u03bd(a|s) \u2265 0, \u03c0(a|s) \u2265 0.\n\nWe first solve the inner minimization over \u03c0(a|s) for a fixed \u03bd(a|s), by writing out the Lagrangian and setting the gradient of the Lagrangian to be 0, we obtain:\n\u2200a, 2 \u00b7 \u03c0 * (a|s) \u03c0 \u03b2 (a|s) \u2212 \u03bd(a|s) \u03c0 \u03b2 (a|s) \u2212 \u03b6(a|s) + \u03b7 = 0,\nwhere \u03b6(a|s) is the Lagrange dual variable for the positivity constraints on \u03c0(a|s), and \u03b7 is the Lagrange dual variable for the normalization constraint on \u03c0. If \u03c0(a|s) is full support (for example, when it is chosen to be a Boltzmann policy), KKT conditions imply that, \u03b6(a|s) = 0, and computing \u03b7 by summing up over actions, a, the optimal choice of \u03c0 for the inner minimization is given by:\n\n\u03c0 * (a|s) = 1 2 \u03bd(a|s) + 1 2 \u03c0 \u03b2 (a|s).\n\nNow, plugging Equation 20 in the original optimization problem, we obtain the following optimization over only \u03bd(a|s):\nmax \u03bd(a|s) a \u03c0 \u03b2 (a|s) \u00b7 1 2 \u2212 \u03bd(a|s) 2\u03c0 \u03b2 (a|s) \u00b7 1 2 + \u03bd(a|s) 2\u03c0 \u03b2 (a|s) s.t. a \u03bd(a|s) = 1, \u03bd(a|s) \u2265 0. (21)\nSolving this optimization, we find that the optimal distribution, \u03bd(a|s) is equal to \u03c0 \u03b2 (a|s). and the optimal value of penalty, which is also the objective for the problem above is equal to 0. Since we are maximizing over \u03bd, this indicates for other choices of \u03bd = \u03c0 \u03b2 , we can find a \u03c0 so that the penalty is negative, and hence a lower-bound is not guaranteed. Therefore, we find that with a worst case choice of \u03c0(a|s), a lower bound can only be guaranteed only if \u03bd(a|s) = \u03c0 \u03b2 (a|s). This justifies the necessity of \u03c0 \u03b2 (a|s) for maximizing Q-values in Equation 2. The above analysis doesn't take into account the effect of function approximation or sampling error. We can, however, generalize this result to those settings, by following a similar strategy of appropriately choosing \u03b1 k , as previously utilized in Theorem D.1.\n\n\nD.3 CQL with Empirical Dataset Distributions\n\nThe results in Sections 3.1 and 3.2 account for sampling error due to the finite size of the dataset D.\n\nIn our practical implementation as well, we optimize a sample-based version of Equation 2, as shown below:\nQ k+1 \u2190 arg min Q \u03b1 \u00b7 s\u2208D E a\u223c\u00b5(a|s) [Q(s, a)] \u2212 s\u2208D E a\u223c\u03c0 \u03b2 (a|s) [Q(s, a)] + 1 2|D| s,a,s \u2208D Q(s, a) \u2212B \u03c0Qk (s, a) 2 ,(22)\nwhereB \u03c0 denotes the \"empirical\" Bellman operator computed using samples in D as follows:\n\u2200s, a \u2208 D, B \u03c0Qk (s, a) = r + \u03b3 s T (s |s, a)E a \u223c\u03c0(a |s ) Q k (s , a ) ,(23)\nwhere r is the empirical average reward obtained in the dataset when executing an action a at state s, i.e. r = 1 |D(s,a)| si,aiinD 1 si=s,ai=a \u00b7 r(s, a), andT (s |s, a) is the empirical transition matrix. Note that expectation under \u03c0(a|s) can be computed exactly, since it does not depend on the dataset. The empirical Bellman operator can take higher values as compared to the actual Bellman operator, B \u03c0 , for instance, in an MDP with stochastic dynamics, where D may not contain transitions to all possible next-states s that can be reached by executing action a at state s, and only contains an optimistic transition.\n\nWe next show how the CQL lower bound result (Theorem 3.2) can be modified to guarantee a lower bound even in this presence of sampling error. To note this, following prior work [24, 39], we assume concentration properties of the reward function and the transition dynamics: Assumption D.1. \u2200 s, a \u2208 D, the following relationships hold with high probability,\n\u2265 1 \u2212 \u03b4 |r \u2212 r(s, a)| \u2264 C r,\u03b4 |D(s, a)| , ||T (s |s, a) \u2212 T (s |s, a)|| 1 \u2264 C T,\u03b4 |D(s, a)| .\nUnder this assumption, the difference between the empirical Bellman operator and the actual Bellman operator can be bounded: \nB\u2264 C r + \u03b3C T R max /(1 \u2212 \u03b3) |D(s, a)| \u2264 C r + \u03b3C T R max /(1 \u2212 \u03b3) \u2264 C r,T,\u03b4 R max 1 \u2212 \u03b3 .\nThis gives us an expression to bound the potential overestimation that can occur due to sampling error, as a function of a constant, C r,T,\u03b4 .\n\n\nE Extended Related Work and Connections to Prior Methods\n\nIn this section, we discuss related works to supplement Section 5. Specifically, we discuss the relationships between CQL and uncertainty estimation and policy-constraint methods.\n\nRelationship to uncertainty estimation in offline RL. A number of prior approaches to offline RL estimate some sort of epistemic uncertainty to determine the trustworthiness of a Q-value prediction [27, 14, 2, 30]. The policy is then optimized with respect to lower-confidence estimates derived using the uncertainty metric. However, it has been empirically noted that uncertainty-based methods are not sufficient to prevent against OOD actions [14,27] in and of themselves, are often augmented with policy constraints due to the inability to estimate tight and calibrated uncertainty sets. Such loose or uncalibrated uncertainty sets are still effective in providing exploratory behavior in standard, online RL [40,39], where these methods were originally developed. However, offline RL places high demands on the fidelity of such sets [30], making it hard to directly use these methods.\n\nHow does CQL relate to prior uncertainty estimation methods? Typical uncertainty estimation methods rely on learning a pointwise upper bound on the Q-function that depends on epistemic uncertainty [24,39] and these upper-confidence bound values are then used for exploration. In the context of offline RL, this means learning a pointwise lower-bound on the Q-function. We show in Section 3.1 that, with a na\u00efve choice of regularizer (Equation 1), we can learn a uniform lower-bound on the Q-function, however, we then showed that we can improve this bound since the value of the policy is the primary quantity of interest that needs to be lower-bounded. This implies that CQL strengthens the popular practice of point-wise lower-bounds made by uncertainty estimation methods.\n\nCan we make CQL dependent on uncertainty? We can slightly modify CQL to make it be account for epistemic uncertainty under certain statistical concentration assumptions. Typical uncertainty estimation methods in RL [40,24] assume the applicability of concentration inequalities (for example, by making sub-Gaussian assumptions on the reward and dynamics), to obtain upper or lower-confidence bounds and the canonical amount of over-(under-) estimation is usually given by, O where C r and C T are constants, that depend on the concentration properties of the MDP, and by appropriately choosing \u03b1, i.e. \u03b1 = \u2126(n(s, a)), such that the learned Q-function still lower-bounds the actual Q-function (by nullifying the possible overestimation that appears due to finite samples), we are still guaranteed a lower bound.\n\n\nF Additional Experimental Setup and Implementation Details\n\nIn this section, we discuss some additional implementation details related to our method. As discussed in Section 4, CQL can be implemented as either a Q-learning or an actor-critic method. For our experiments on D4RL benchmarks [11], we implemented CQL on top of soft actor-critic (SAC) [18], and for experiments on discrete-action Atari tasks, we implemented CQL on top of QR-DQN [7]. We experimented with two ways of implementing CQL, first with a fixed \u03b1, where we chose \u03b1 = 5.0, and second with a varying \u03b1 chosen via dual gradient-descent. The latter formulation automates the choice of \u03b1 by introducing a \"budget\" parameter, \u03c4 , as shown below:\nmin Q max \u03b1\u22650 \u03b1 E s\u223cd \u03c0 \u03b2 (s) log a exp(Q(s, a))\u2212E a\u223c\u03c0 \u03b2 (a|s) [Q(s, a)] \u2212 \u03c4 + 1 2 E s,a,s \u223cD Q \u2212 B \u03c0 kQ k 2 .(24)\nEquation 24 implies that if the expected difference in Q-values is less than the specified threshold \u03c4 , \u03b1 will adjust to be close to 0, whereas if the difference in Q-values is higher than the specified threshold, \u03c4 , then \u03b1 is likely to take on high values, and thus more aggressively penalize Q-values. We refer to this version as CQL-Lagrange, and we found that this version outperforms the version with a fixed \u03b1 on the gym-MuJoCo D4RL benchmarks, and drastically outperforms the fixed \u03b1 version on the more complex AntMazes.  Table 3 (with 10% data), \u03b1 = 4.0 chosen uniformly for Table 3 (with 1% data), and \u03b1 = 0.5 for Figure 1.\n\nComputing log a exp(Q(s, a). CQL(H) uses log-sum-exp in the objective for training the Qfunction (Equation 4). In discrete action domains, we compute the log-sum-exp exactly by invoking the standard tf.reduce_logsumexp() (or torch.logsumexp()) functions provided by autodiff libraries. In continuous action tasks, CQL(H) uses importance sampling to compute this quantity, where in practice, we sampled 10 action samples each at every state s from a uniform-at-random Unif(a) and the current policy, \u03c0(a|s) and used these alongside importance sampling to compute it as follows using N = 10 action samples: Hyperparameters. For the D4RL tasks, we built CQL on top of the implementation of SAC provided at: https://github.com/vitchyr/rlkit/. Our implementation mimicked the RLkit SAC algorithm implementation, with the exception of a smaller policy learning rate, which was chosen to be 3e-5 or 1e-4 for continuous control tasks. Following the convention set by D4RL [11], we report the normalized, smooth average undiscounted return over 4 seeds for in our results in Section 6.\n\nThe other hyperparameters we evaluated on during our preliminary experiments, and might be helpful guidelines for using CQL are as follows:\n\n\u2022 Q-function learning rate. We tried two learning rate values [1e \u2212 4, 3e \u2212 4] for the Qfunction. We didn't observe a significant difference in performance across these values. 3e \u2212 4 which is the SAC default was chosen to be the default for CQL.\n\n\u2022 Policy learning rate. We evaluated CQL with a policy learning rate in the range of [3e \u2212 5, 1e \u2212 4, 3e \u2212 4. We found 3e \u2212 5 to almost uniformly attain good performance. While 1e \u2212 4 seemed to be better on some of our experiments (such as hopper-medium-v0), but it performed badly with the real-human demonstration datasets, such as the Adroit tasks. We chose 3e \u2212 5 as the default across all environments.\n\n\u2022 Lagrange threshold \u03c4 . We ran our preliminary experiments with three values of threshold, \u03c4 = [2.0, 5.0, 10.0]. However, we found that \u03c4 = 2.0, led to a huge increase in the value of \u03b1 (sometimes upto the order of millions), and as a result, highly underestimated Q-functions on all domains (sometimes upto the order of -1e6). On the datasets with human demonstrations -the Franka Kitchen and Adroit domains, we found that \u03c4 = 5.0 obtained lower-bounds on Q-values, whereas \u03c4 = 10.0 was unable to prevent overestimation in Q-values in a number of cases and Q-values diverged to highly positive values (> 1e+6). For the MuJoCo domains, we observed that \u03c4 = 10.0 gave rise to a stable curve of Q-values, and hence this threshold was chosen for these experiments. Note that none of these hyperparameter selections required any notion of onine evaluation, since these choices were made based on the predicted Q-values on dataset state-actions pairs.\n\n\u2022 Number of gradient steps. We evaluated our method on varying number of gradient steps. Since CQL uses a reduced policy learning rate (3e-5), we trained CQL methods for 1M gradient steps. Due to a lack of a proper valdiation error metric for offline Q-learning methods, deciding the number of gradient steps dynamically has been an open problem in offline RL. [30]. Different prior methods choose the number of steps differently. For our experiments, we used 1M gradient steps for all D4RL domains, and followed the convention from Agarwal et al. [2], to report returns after 5X gradient steps of training for the Atari results in Table 3.\n\nOther hyperparameters, were kept identical to SAC on the D4RL tasks, including the twin Q-function trick, soft-target updates, etc. In the Atari domain, we based our implementation of CQL on top of the QR-DQN implementation provided by Agarwal et al. [2]. We did not tune any parameter from the QR-DQN implementation released with the official codebase with [2].\n\n\nG Ablation Studies\n\nIn this section. we describe the experimental findings of some ablations for CQL. Specifically we aim to answer the following questions: We start with question (1). On three MuJoCo tasks from D4RL, we evaluate the performance of both CQL(H) and CQL(\u03c1), as shown in Table 5. We observe that on these tasks, CQL(H) generally performs better than CQL(\u03c1). However, when a sampled estimate of log-sum-exp of the Q-values becomes inaccurate due to high variance importance weights, especially in large action spaces, such as in the Adroit environments, we observe in Table 2 that CQL(\u03c1) tends to perform better. Next, we evaluate the answer to question (2). On three MuJoCo tasks from D4RL, as shown in Table 6, we evaluate the performance of CQL(H), with and without the dataset maximization term in Equation 2. We observe that omitting this term generally seems to decrease performance, especially in cases when the dataset distribution is generated from a single policy, for example, hopper-medium.  Table 6: Average return obtained by CQL(H) and CQL(H) without the dataset average Q-value maximization term. The latter formulation corresponds to Equation 1, which is void of the dataset Q-value maximization term. We show in Theorem 3.2 that Equation 1 results in a weaker lower-bound. In this experiment, we also observe that this approach is generally outperformed by CQL(H).\n\nNext, we answer question (3). Table 7 shows the performance of CQL and CQL-Lagrange on some gym-MuJoCo and AntMaze tasks from D4RL. In all of these tasks, we observe that the Lagrange version (Equation 24, which automates the choice of \u03b1 using dual gradient descent on the CQL regularizer, performs better than the non-Lagrange version (Equation 4). In some cases, for example, the AntMazes, the difference can be as high as 30% of the maximum achievable performance. On the gym MuJoCo tasks, we did not observe a large benefit of using the Lagrange version, however, there are some clear benefits, for instance in the setting when learning from hopper-mixed dataset.  Table 7: Average return obtained by CQL(H) and CQL(H) with automatic tuning for \u03b1 by using a Lagrange version. Observe that both versions are generally comparable, except in the AntMaze tasks, where an adaptive value of \u03b1 greatly outperforms a single chosen value of \u03b1.\n\n\na,s \u223cD (r(s, a) + \u03b3E a \u223c\u03c0 k (a |s ) [Q k (s , a )]) \u2212 Q(s, a) 2 (policy evaluation) \u03c0 k+1 \u2190 arg max \u03c0 E s\u223cD,a\u223c\u03c0 k (a|s) Q k+1 (s, a) (policy improvement)\n\n\nImplementation details. Our algorithm requires an addition of only 20 lines of code on top of standard implementations of soft actor-critic (SAC) [18] for continuous control experiments and on top of QR-DQN\n\nFigure 1 :\n1Performance of CQL, QR-DQN and REM as a function of training steps (x-axis) in setting (1) when\n\n+\ns,a,s \u223cD Q \u2212 B \u03c0 kQ k 2 + \u03b1E s\u223cd \u03c0 \u03b2 \u03b1E s\u223cd \u03c0 \u03b2 (s) EP (a|s)[Q(s, a)] \u2212 E \u03c0 \u03b2 (a|s) [Q(s, a)] (7)\n\nFigure 2 :\n2\u2206 k as a function of training iterations for hopper-expert and hopper-medium datasets. Note that CQL (left) generally has negative values of \u2206, whereas BEAR (right) generally has positive \u2206 values, which also increase during training with increasing k values.\n\n\u03b1\n\u00b7 E s\u223cd \u03c0 \u03b2 (s),a\u223c\u00b5(a|s) [Q(s, a)] \u2212 E s\u223cd \u03c0 \u03b2 (s),a\u223c\u03bd(a|s) [Q(s, a)] + 1 2 E s,a,s \u223cD Q(s, a) \u2212 B \u03c0Qk (s, a) 2 . (\u03bd-CQL) (17)\n\n\n\u03c0Qk \u2212 B \u03c0Qk = (r \u2212 r(s, a)) + \u03b3 s T (s |s, a) \u2212 T (s |s, a) E \u03c0(a |s ) Q k (s , a ) \u2264 |r \u2212 r(s, a)| + \u03b3 s T (s |s, a) \u2212 T (s |s, a) E \u03c0(a |s ) Q k (s , a )\n\n\n(s, a) is the number of times a state-action pair (s, a) is observed in the dataset. We can incorporate such behavior in CQL by modifying Equation 3 to update Bellman error weighted by the cardinality of the dataset, |D|, which gives rise to the following effective Q-function update in the tabular setting, without function approximation:Q k+1 (s, a) = B \u03c0Qk (s, a) \u2212 \u03b1 \u00b5(a|s) \u2212 \u03c0 \u03b2 (a|s) n(s, a) \u2192 B \u03c0Qk (s, a) as n(s, a) \u2192 \u221e.In the limit of infinite data, i.e. n(s, a) \u2192 \u221e, we find that the amount of underestimation tends to 0. When only a finite-sized dataset is provided, i.e. n(s, a) < N , for some N , we observe that by making certain assumptions, previously used in prior work[24,40] on the concentration properties of the reward value, r(s, a) and the dynamics function, T (s |s, a), such as follows: ||r(s, a) \u2212 r(s, a)|| \u2264 C r n(s, a) and ||T (s |s, a) \u2212 T (s |s, a)|| \u2264 C T n(s, a) ,\n\n\nChoice of \u03b1. Our experiments in Section 6 use the Lagrange version to automatically tune \u03b1 during training. For our experiments, across D4RL Gym MuJoCo domains, we choose \u03c4 = 10.0. For the other D4RL domains (Franka Kitchen and Adroit), we chose \u03c4 = 5.0. However, for our Atari experiments, we used a fixed penalty, with \u03b1 = 1.0 chosen uniformly for\n\nTable 3 ), CQL\n3Task Name \nQR-DQN \nREM \nCQL(H) \nPong (1%) \n-13.8 \n-6.9 \n19.3 \nBreakout \n7.9 \n11.0 \n61.1 \nQ*bert \n383.6 \n343.4 \n14012.0 \nSeaquest \n672.9 \n499.8 \n779.4 \nAsterix \n166.3 \n386.5 \n592.4 \nPong (10%) \n15.1 \n8.9 \n18.5 \nBreakout \n151.2 \n86.7 \n269.3 \nQ*bert \n7091.3 \n8624.3 \n13855.6 \nSeaquest \n2984.8 \n3936.6 \n3674.1 \nAsterix \n189.2 \n75.1 \n156.3 \n\n\n\nTable 3 :\n3CQL, REM and QR-DQN in setting (1) with 1% data (top), and 10% data (bottom). CQL drastically outperforms prior methods with 1% data, and usually attains better performance with 10% data.substantially outperforms REM and QR-DQN, especially in the harder 1% condition, achieving 36x and 6x times the return of the best prior method on Q*bert and Breakout, respectively.Analysis of CQL. Finally, we perform empirical evaluation to verify that CQL indeed lower-bounds the value function, thus verifying Theorems 3.2, Appendix D.1 empirically. To this end, we estimate the average value of the learned policy predicted by CQL, E s\u223cD [V k (s)], and report the difference against the actual discounted return of the policy \u03c0 k inTable 4. We also estimate these values for baselines, including the minimum predicted Q-value under an ensemble[18,15] of Qfunctions with varying ensemble sizes, which is a standard technique to prevent overestimed Qvalues[15, 18, 20]  and BEAR [27], a policy constraint method. The results show that CQL learns a lower bound for all three tasks, whereas the baselines are prone to overestimation. We also evaluate a variant of CQL that uses Equation 1, and observe that the resulting values are lower (that is, underestimate the true values) as compared to CQL(H). This provides empirical evidence that CQL(H) attains a tighter lower bound than the point-wise bound in Equation 1, as per Theorem 3.2.Task Name \nCQL(H) \nCQL (Eqn. 1) \nEnsemble(2) \nEns.(4) \nEns.(10) Ens.(20) \nBEAR \nhopper-medium-expert \n-43.20 \n-151.36 \n3.71e6 \n2.93e6 \n0.32e6 \n24.05e3 \n65.93 \nhopper-mixed \n-10.93 \n-22.87 \n15.00e6 \n59.93e3 \n8.92e3 \n2.47e3 \n1399.46 \nhopper-medium \n-7.48 \n-156.70 \n26.03e12 \n437.57e6 \n1.12e12 \n885e3 \n4.32 \n\n\n\nTable 4 :\n4Difference between policy values predicted by each algorithm and the true policy value for CQL, a variant of CQL that uses Equation 1, the minimum of an ensemble of varying sizes, and BEAR [27] on three D4RL datasets. CQL is the only method that lower-bounds the actual return (i.e., has negative differences), and CQL(H) is much less conservative than CQL (Eqn. 1).\n\n\n17] Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. arXiv preprint arXiv:1910.11956, 2019. [18] T. Haarnoja, H. Tang, P. Abbeel, and S. Levine. Reinforcement learning with deep energy-based policies. In International Conference on Machine Learning (ICML), 2017. [19] Assaf Hallak and Shie Mannor. Consistent on-line off-policy evaluation. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1372-1383. JMLR. org, 2017. [20] Hado V Hasselt. Double q-learning. In Advances in neural information processing systems, pages 2613-2621, 2010. [21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. [22] Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal Piot, Dan Horgan, John Quan, Andrew Sendonaris, Ian Osband, et al. Deep q-learning from demonstrations. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018. [23] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Convergence and generalization in neural networks. In Advances in Neural Information Processing Systems 31. 2018. [24] Thomas Jaksch, Ronald Ortner, and Peter Auer. Near-optimal regret bounds for reinforcement learning. Journal of Machine Learning Research, 11(Apr):1563-1600, 2010. [25] Natasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, Agata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard. Way off-policy batch deep reinforcement learning of implicit human preferences in dialog. arXiv preprint arXiv:1907.00456, 2019. [26] Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Scalable deep reinforcement learning for vision-based robotic manipulation. In Conference on Robot Learning, pages 651-673, 2018. [27] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via bootstrapping error reduction. In Advances in Neural Information Processing Systems, pages 11761-11771, 2019. [28] Aviral Kumar, Abhishek Gupta, and Sergey Levine. Discor: Corrective feedback in reinforcement learning via distribution correction. arXiv preprint arXiv:2003.07305, 2020. [29] Michail G Lagoudakis and Ronald Parr. Least-squares policy iteration. Journal of machine learning research, 4(Dec):1107-1149, 2003. [30] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020. [31] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web, pages 661-670, 2010. [32] Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Infinite-horizon off-policy estimation. In Advances in Neural Information Processing Systems, pages 5356-5366, 2018. [33] Yingdong Lu, Mark S Squillante, and Chai Wah Wu. A general family of robust stochastic operators for reinforcement learning. arXiv preprint arXiv:1805.08122, 2018. [34] Yuping Luo, Huazhe Xu, and Tengyu Ma. Learning self-correctable policies and value functions from demonstrations with negative sampling. arXiv preprint arXiv:1907.05634, 2019. [35] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. [36] Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections. In Advances in Neural Information Processing Systems, pages 2315-2325, 2019.\n\n1 .\n1How does CQL(H) compare to CQL(\u03c1), with \u03c1 =\u03c0 k\u22121 , the previous policy? 2. How does the CQL variant which uses Equation 1 compare to CQL(H) variant in Equation 4, that results in a tighter lower-bound theoretically? 3. How do the Lagrange (Equation 24) and non-Lagrange (Equation 4) formulations of CQL(H) empirically compare to each other?\n\n\n: Average return obtained by CQL(H), and CQL(\u03c1) on three D4RL MuJoCo environments. Observe that on these environments, CQL(H) generally outperforms CQL(\u03c1).Task Name \nCQL(H) CQL(\u03c1) \nhalfcheetah-medium-expert \n7234.5 \n3995.6 \nwalker2d-mixed \n1227.2 \n812.7 \nhopper-medium \n1866.1 \n1166.1 \nTable 5\nAcknowledgementsWe thank Mohammad Norouzi, Oleh Rybkin, Anton Raichuk, Vitchyr Pong and anonymous reviewers from the Robotic AI and Learning Lab at UC Berkeley for their feedback on an earlier version of this paper. We thank Rishabh Agarwal for help with the Atari QR-DQN/REM codebase and for sharing baseline results. This research was funded by the DARPA Assured Autonomy program, and compute support from Google, Amazon, and NVIDIA.\nTowards characterizing divergence in deep q-learning. Joshua Achiam, Ethan Knight, Pieter Abbeel, arXiv:1903.08894arXiv preprintJoshua Achiam, Ethan Knight, and Pieter Abbeel. Towards characterizing divergence in deep q-learning. arXiv preprint arXiv:1903.08894, 2019.\n\nAn optimistic perspective on offline reinforcement learning. Rishabh Agarwal, Dale Schuurmans, Mohammad Norouzi, Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline reinforcement learning. 2019.\n\nMastering the real-time strategy game starcraft ii. Deepmind Alphastar, DeepMind AlphaStar. Mastering the real-time strategy game starcraft ii. URL: https://deepmind. com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii.\n\nThe arcade learning environment: An evaluation platform for general agents. Yavar Marc G Bellemare, Joel Naddaf, Michael Veness, Bowling, Journal of Artificial Intelligence Research. 47Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279, 2013.\n\nIncreasing the action gap: New operators for reinforcement learning. Georg Marc G Bellemare, Arthur Ostrovski, Guez, S Philip, R\u00e9mi Thomas, Munos, Thirtieth AAAI Conference on Artificial Intelligence. Marc G Bellemare, Georg Ostrovski, Arthur Guez, Philip S Thomas, and R\u00e9mi Munos. Increas- ing the action gap: New operators for reinforcement learning. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.\n\nYuri Burda, Harrison Edwards, Amos Storkey, Oleg Klimov, arXiv:1810.12894Exploration by random network distillation. arXiv preprintYuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov. Exploration by random network distillation. arXiv preprint arXiv:1810.12894, 2018.\n\nDistributional reinforcement learning with quantile regression. Will Dabney, Mark Rowland, G Marc, R\u00e9mi Bellemare, Munos, Thirty-Second AAAI Conference on Artificial Intelligence. Will Dabney, Mark Rowland, Marc G Bellemare, and R\u00e9mi Munos. Distributional reinforcement learning with quantile regression. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018.\n\nImagenet: A largescale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, 2009 IEEE conference on computer vision and pattern recognition. IeeeJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large- scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248-255. Ieee, 2009.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nTree-based batch mode reinforcement learning. Damien Ernst, Pierre Geurts, Louis Wehenkel, Journal of Machine Learning Research. 6Damien Ernst, Pierre Geurts, and Louis Wehenkel. Tree-based batch mode reinforcement learning. Journal of Machine Learning Research, 6(Apr):503-556, 2005.\n\nD4rl: Datasets for deep data-driven reinforcement learning. J Fu, A Kumar, O Nachum, G Tucker, S Levine, arXiv, 2020J. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine. D4rl: Datasets for deep data-driven reinforcement learning. In arXiv, 2020. URL https://arxiv.org/pdf/2004.07219.\n\nJustin Fu, Aviral Kumar, Matthew Soh, Sergey Levine, arXiv:1902.10250Diagnosing bottlenecks in deep Q-learning algorithms. arXiv preprintJustin Fu, Aviral Kumar, Matthew Soh, and Sergey Levine. Diagnosing bottlenecks in deep Q-learning algorithms. arXiv preprint arXiv:1902.10250, 2019.\n\nD4rl: Datasets for data-driven deep reinforcement learning. Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, Sergey Levine, Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for data-driven deep reinforcement learning. https://github.com/rail-berkeley/d4rl/ wiki/New-Franka-Kitchen-Tasks, 2020. Github repository.\n\nOff-policy deep reinforcement learning without exploration. Scott Fujimoto, David Meger, Doina Precup, arXiv:1812.02900arXiv preprintScott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without exploration. arXiv preprint arXiv:1812.02900, 2018.\n\nAddressing function approximation error in actor-critic methods. Scott Fujimoto, David Herke Van Hoof, Meger, International Conference on Machine Learning (ICML). Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In International Conference on Machine Learning (ICML), pages 1587-1596, 2018.\n\nOff-policy deep reinforcement learning by bootstrapping the covariate shift. Carles Gelada, G Marc, Bellemare, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Carles Gelada and Marc G Bellemare. Off-policy deep reinforcement learning by bootstrapping the covariate shift. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 3647-3655, 2019.\n\nVariance-based regularization with convex objectives. Hongseok Namkoong, C John, Duchi, Advances in neural information processing systems. Hongseok Namkoong and John C Duchi. Variance-based regularization with convex objectives. In Advances in neural information processing systems, pages 2971-2980, 2017.\n\nO&apos; Brendan, Donoghue, arXiv:1807.09647Variational bayesian reinforcement learning with regret bounds. arXiv preprintBrendan O'Donoghue. Variational bayesian reinforcement learning with regret bounds. arXiv preprint arXiv:1807.09647, 2018.\n\nWhy is posterior sampling better than optimism for reinforcement learning?. Ian Osband, Benjamin Van Roy, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70Ian Osband and Benjamin Van Roy. Why is posterior sampling better than optimism for reinforcement learning? In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 2701-2710. JMLR. org, 2017.\n\nDeep exploration via bootstrapped dqn. Ian Osband, Charles Blundell, Alexander Pritzel, Benjamin Van Roy, Advances in neural information processing systems. Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped dqn. In Advances in neural information processing systems, pages 4026-4034, 2016.\n\nAdvantage-weighted regression: Simple and scalable off-policy reinforcement learning. Aviral Xue Bin Peng, Grace Kumar, Sergey Zhang, Levine, arXiv:1910.00177arXiv preprintXue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression: Simple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.\n\nLearning from scarce experience. Leonid Peshkin, Christian R Shelton, cs/0204043arXiv preprintLeonid Peshkin and Christian R Shelton. Learning from scarce experience. arXiv preprint cs/0204043, 2002.\n\nEligibility traces for off-policy policy evaluation. Doina Precup, Computer Science Department Faculty Publication Series. 80Doina Precup. Eligibility traces for off-policy policy evaluation. Computer Science Department Faculty Publication Series, page 80, 2000.\n\nOff-policy temporal-difference learning with function approximation. Doina Precup, S Richard, Sanjoy Sutton, Dasgupta, ICML. Doina Precup, Richard S Sutton, and Sanjoy Dasgupta. Off-policy temporal-difference learning with function approximation. In ICML, pages 417-424, 2001.\n\nLearning complex dexterous manipulation with deep reinforcement learning and demonstrations. Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, Sergey Levine, Robotics: Science and Systems. Aravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel Todorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement learning and demonstrations. In Robotics: Science and Systems, 2018.\n\nKeep doing what worked: Behavioral modelling priors for offline reinforcement learning. Y Noah, Jost Tobias Siegel, Felix Springenberg, Abbas Berkenkamp, Michael Abdolmaleki, Thomas Neunert, Roland Lampe, Martin Hafner, Riedmiller, arXiv:2002.08396arXiv preprintNoah Y Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael Neunert, Thomas Lampe, Roland Hafner, and Martin Riedmiller. Keep doing what worked: Behavioral modelling priors for offline reinforcement learning. arXiv preprint arXiv:2002.08396, 2020.\n\nMel Vecerik, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal Piot, Nicolas Heess, Thomas Roth\u00f6rl, Thomas Lampe, Martin Riedmiller, arXiv:1707.08817Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards. arXiv preprintMel Vecerik, Todd Hester, Jonathan Scholz, Fumin Wang, Olivier Pietquin, Bilal Piot, Nicolas Heess, Thomas Roth\u00f6rl, Thomas Lampe, and Martin Riedmiller. Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards. arXiv preprint arXiv:1707.08817, 2017.\n\nBehavior regularized offline reinforcement learning. Yifan Wu, George Tucker, Ofir Nachum, arXiv:1911.11361arXiv preprintYifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning. arXiv preprint arXiv:1911.11361, 2019.\n\nGendice: Generalized offline estimation of stationary values. Ruiyi Zhang, Bo Dai, Lihong Li, Dale Schuurmans, International Conference on Learning Representations. Ruiyi Zhang, Bo Dai, Lihong Li, and Dale Schuurmans. Gendice: Generalized offline estimation of stationary values. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=HkxlcnVFwB.\n", "annotations": {"author": "[{\"end\":110,\"start\":62},{\"end\":137,\"start\":111},{\"end\":180,\"start\":138},{\"end\":237,\"start\":181}]", "publisher": null, "author_last_name": "[{\"end\":74,\"start\":69},{\"end\":122,\"start\":118},{\"end\":151,\"start\":145},{\"end\":194,\"start\":188}]", "author_first_name": "[{\"end\":68,\"start\":62},{\"end\":117,\"start\":111},{\"end\":144,\"start\":138},{\"end\":187,\"start\":181}]", "author_affiliation": "[{\"end\":109,\"start\":97},{\"end\":136,\"start\":124},{\"end\":179,\"start\":153},{\"end\":208,\"start\":196},{\"end\":236,\"start\":210}]", "title": "[{\"end\":59,\"start\":1},{\"end\":296,\"start\":238}]", "venue": null, "abstract": "[{\"end\":1752,\"start\":298}]", "bib_ref": "[{\"end\":1963,\"start\":1959},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1985,\"start\":1982},{\"end\":2017,\"start\":2013},{\"end\":2205,\"start\":2201},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2207,\"start\":2205},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2547,\"start\":2544},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2659,\"start\":2655},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2662,\"start\":2659},{\"end\":2665,\"start\":2662},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2667,\"start\":2665},{\"end\":2670,\"start\":2667},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2673,\"start\":2670},{\"end\":2676,\"start\":2673},{\"end\":3294,\"start\":3290},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3297,\"start\":3294},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3318,\"start\":3314},{\"end\":3321,\"start\":3318},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3323,\"start\":3321},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4743,\"start\":4739},{\"end\":4746,\"start\":4743},{\"end\":5051,\"start\":5047},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5053,\"start\":5051},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5306,\"start\":5302},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5358,\"start\":5355},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5360,\"start\":5358},{\"end\":6201,\"start\":6194},{\"end\":6628,\"start\":6624},{\"end\":7498,\"start\":7494},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7501,\"start\":7498},{\"end\":7504,\"start\":7501},{\"end\":7507,\"start\":7504},{\"end\":8342,\"start\":8338},{\"end\":8345,\"start\":8342},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8348,\"start\":8345},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8351,\"start\":8348},{\"end\":8828,\"start\":8824},{\"end\":8831,\"start\":8828},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11454,\"start\":11450},{\"end\":11457,\"start\":11454},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11460,\"start\":11457},{\"end\":13448,\"start\":13444},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":16283,\"start\":16279},{\"end\":18615,\"start\":18611},{\"end\":18618,\"start\":18615},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":18621,\"start\":18618},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19792,\"start\":19791},{\"end\":20526,\"start\":20518},{\"end\":21113,\"start\":21109},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":21116,\"start\":21113},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21119,\"start\":21116},{\"end\":21122,\"start\":21119},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21226,\"start\":21223},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22011,\"start\":22007},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":22014,\"start\":22011},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22017,\"start\":22014},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":22138,\"start\":22135},{\"end\":22141,\"start\":22138},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":22144,\"start\":22141},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":22469,\"start\":22466},{\"end\":22472,\"start\":22469},{\"end\":22475,\"start\":22472},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22921,\"start\":22918},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":22924,\"start\":22921},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":22927,\"start\":22924},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":22954,\"start\":22950},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":23182,\"start\":23178},{\"end\":23185,\"start\":23182},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":23188,\"start\":23185},{\"end\":23191,\"start\":23188},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":23194,\"start\":23191},{\"end\":23386,\"start\":23382},{\"end\":23517,\"start\":23513},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":23520,\"start\":23517},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":24057,\"start\":24053},{\"end\":24060,\"start\":24057},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24063,\"start\":24060},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":24065,\"start\":24063},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24134,\"start\":24130},{\"end\":24137,\"start\":24134},{\"end\":24140,\"start\":24137},{\"end\":24222,\"start\":24218},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24350,\"start\":24347},{\"end\":24353,\"start\":24350},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":24778,\"start\":24774},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":24873,\"start\":24869},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":25133,\"start\":25129},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25585,\"start\":25581},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":27686,\"start\":27682},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":28439,\"start\":28436},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":28466,\"start\":28463},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":28481,\"start\":28478},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":28602,\"start\":28599},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":28903,\"start\":28900},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":29017,\"start\":29014},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":29226,\"start\":29223},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":31137,\"start\":31133},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":32228,\"start\":32224},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":32477,\"start\":32473},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":32513,\"start\":32512},{\"end\":34188,\"start\":34184},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":34191,\"start\":34188},{\"end\":34194,\"start\":34191},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":34197,\"start\":34194},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":34690,\"start\":34687},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":34693,\"start\":34690},{\"end\":34696,\"start\":34693},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":34845,\"start\":34841},{\"end\":34848,\"start\":34845},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":34878,\"start\":34874},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":35007,\"start\":35003},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":38022,\"start\":38021},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":43683,\"start\":43682},{\"end\":45210,\"start\":45200},{\"end\":46774,\"start\":46763},{\"end\":60219,\"start\":60215},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":60398,\"start\":60395},{\"end\":60401,\"start\":60398},{\"end\":61316,\"start\":61305},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":61438,\"start\":61436},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":68719,\"start\":68715},{\"end\":68722,\"start\":68719},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":68986,\"start\":68982},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":68989,\"start\":68986},{\"end\":69361,\"start\":69357},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":69364,\"start\":69361},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":70156,\"start\":70152},{\"end\":70159,\"start\":70156},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":71043,\"start\":71039},{\"end\":71102,\"start\":71098},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":71195,\"start\":71192},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":73182,\"start\":73178},{\"end\":75404,\"start\":75400},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":75590,\"start\":75587},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":75935,\"start\":75932},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":76042,\"start\":76039},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":80213,\"start\":80210},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":81978,\"start\":81975},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":82086,\"start\":82082}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":78537,\"start\":78382},{\"attributes\":{\"id\":\"fig_2\"},\"end\":78746,\"start\":78538},{\"attributes\":{\"id\":\"fig_4\"},\"end\":78855,\"start\":78747},{\"attributes\":{\"id\":\"fig_5\"},\"end\":78956,\"start\":78856},{\"attributes\":{\"id\":\"fig_6\"},\"end\":79229,\"start\":78957},{\"attributes\":{\"id\":\"fig_7\"},\"end\":79359,\"start\":79230},{\"attributes\":{\"id\":\"fig_8\"},\"end\":79517,\"start\":79360},{\"attributes\":{\"id\":\"fig_9\"},\"end\":80417,\"start\":79518},{\"attributes\":{\"id\":\"fig_10\"},\"end\":80769,\"start\":80418},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":81124,\"start\":80770},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":82867,\"start\":81125},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":83246,\"start\":82868},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":87279,\"start\":83247},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":87626,\"start\":87280},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":87922,\"start\":87627}]", "paragraph": "[{\"end\":3096,\"start\":1768},{\"end\":4205,\"start\":3098},{\"end\":5572,\"start\":4207},{\"end\":6254,\"start\":5590},{\"end\":8436,\"start\":6256},{\"end\":9182,\"start\":8506},{\"end\":10028,\"start\":9221},{\"end\":10591,\"start\":10116},{\"end\":11291,\"start\":10715},{\"end\":11576,\"start\":11293},{\"end\":11927,\"start\":11618},{\"end\":12071,\"start\":11929},{\"end\":12347,\"start\":12186},{\"end\":12758,\"start\":12349},{\"end\":13548,\"start\":13070},{\"end\":13932,\"start\":13550},{\"end\":14268,\"start\":13975},{\"end\":14675,\"start\":14270},{\"end\":14962,\"start\":14677},{\"end\":15692,\"start\":15098},{\"end\":16501,\"start\":15784},{\"end\":17191,\"start\":16503},{\"end\":17431,\"start\":17244},{\"end\":18047,\"start\":17547},{\"end\":18832,\"start\":18049},{\"end\":19331,\"start\":18940},{\"end\":19688,\"start\":19333},{\"end\":20048,\"start\":19739},{\"end\":21668,\"start\":20050},{\"end\":21885,\"start\":21685},{\"end\":22636,\"start\":21887},{\"end\":23606,\"start\":22638},{\"end\":24403,\"start\":23608},{\"end\":24981,\"start\":24431},{\"end\":25541,\"start\":24983},{\"end\":26544,\"start\":25543},{\"end\":27194,\"start\":26546},{\"end\":27599,\"start\":27252},{\"end\":28293,\"start\":27601},{\"end\":28846,\"start\":28295},{\"end\":29616,\"start\":28848},{\"end\":30755,\"start\":29631},{\"end\":30759,\"start\":30757},{\"end\":31138,\"start\":30803},{\"end\":31312,\"start\":31140},{\"end\":31525,\"start\":31373},{\"end\":31663,\"start\":31527},{\"end\":31764,\"start\":31731},{\"end\":31978,\"start\":31796},{\"end\":32718,\"start\":32097},{\"end\":32839,\"start\":32787},{\"end\":33439,\"start\":32898},{\"end\":34026,\"start\":33497},{\"end\":35272,\"start\":34028},{\"end\":35758,\"start\":35274},{\"end\":36346,\"start\":35760},{\"end\":37017,\"start\":36348},{\"end\":37574,\"start\":37019},{\"end\":37854,\"start\":37646},{\"end\":38023,\"start\":37856},{\"end\":38998,\"start\":38025},{\"end\":39374,\"start\":39000},{\"end\":39983,\"start\":39376},{\"end\":40082,\"start\":39985},{\"end\":40552,\"start\":40084},{\"end\":40749,\"start\":40573},{\"end\":41440,\"start\":40751},{\"end\":41984,\"start\":41442},{\"end\":42150,\"start\":41986},{\"end\":42481,\"start\":42152},{\"end\":42623,\"start\":42483},{\"end\":43401,\"start\":42625},{\"end\":43511,\"start\":43466},{\"end\":43600,\"start\":43513},{\"end\":43727,\"start\":43602},{\"end\":43999,\"start\":43954},{\"end\":44281,\"start\":44001},{\"end\":44600,\"start\":44392},{\"end\":44938,\"start\":44602},{\"end\":45335,\"start\":44940},{\"end\":45400,\"start\":45337},{\"end\":45838,\"start\":45402},{\"end\":45914,\"start\":45840},{\"end\":46162,\"start\":46006},{\"end\":46430,\"start\":46210},{\"end\":46831,\"start\":46432},{\"end\":47235,\"start\":46925},{\"end\":47418,\"start\":47237},{\"end\":47487,\"start\":47439},{\"end\":47708,\"start\":47593},{\"end\":47778,\"start\":47737},{\"end\":47820,\"start\":47780},{\"end\":48154,\"start\":47822},{\"end\":48781,\"start\":48409},{\"end\":48919,\"start\":48783},{\"end\":49260,\"start\":48921},{\"end\":49394,\"start\":49332},{\"end\":49697,\"start\":49502},{\"end\":50083,\"start\":49810},{\"end\":50396,\"start\":50297},{\"end\":50754,\"start\":50577},{\"end\":50891,\"start\":50871},{\"end\":51000,\"start\":50968},{\"end\":51398,\"start\":51002},{\"end\":51547,\"start\":51400},{\"end\":51603,\"start\":51568},{\"end\":51860,\"start\":51666},{\"end\":52020,\"start\":51983},{\"end\":52388,\"start\":52058},{\"end\":52848,\"start\":52450},{\"end\":53576,\"start\":53082},{\"end\":53854,\"start\":53578},{\"end\":54180,\"start\":53969},{\"end\":54349,\"start\":54182},{\"end\":54664,\"start\":54440},{\"end\":54800,\"start\":54666},{\"end\":55345,\"start\":54961},{\"end\":55407,\"start\":55347},{\"end\":55633,\"start\":55409},{\"end\":55814,\"start\":55635},{\"end\":56164,\"start\":55864},{\"end\":56768,\"start\":56166},{\"end\":57265,\"start\":56770},{\"end\":57555,\"start\":57267},{\"end\":57770,\"start\":57738},{\"end\":57842,\"start\":57795},{\"end\":58389,\"start\":58041},{\"end\":59414,\"start\":58391},{\"end\":59820,\"start\":59416},{\"end\":59972,\"start\":59911},{\"end\":60402,\"start\":59974},{\"end\":60814,\"start\":60712},{\"end\":60960,\"start\":60830},{\"end\":61081,\"start\":61036},{\"end\":61561,\"start\":61222},{\"end\":61899,\"start\":61563},{\"end\":62607,\"start\":61973},{\"end\":62966,\"start\":62709},{\"end\":63267,\"start\":62968},{\"end\":63356,\"start\":63269},{\"end\":63573,\"start\":63427},{\"end\":63793,\"start\":63690},{\"end\":64250,\"start\":63795},{\"end\":64307,\"start\":64256},{\"end\":64471,\"start\":64309},{\"end\":64931,\"start\":64537},{\"end\":64972,\"start\":64933},{\"end\":65092,\"start\":64974},{\"end\":66037,\"start\":65204},{\"end\":66189,\"start\":66086},{\"end\":66297,\"start\":66191},{\"end\":66512,\"start\":66423},{\"end\":67215,\"start\":66591},{\"end\":67574,\"start\":67217},{\"end\":67794,\"start\":67669},{\"end\":68028,\"start\":67886},{\"end\":68268,\"start\":68089},{\"end\":69158,\"start\":68270},{\"end\":69935,\"start\":69160},{\"end\":70747,\"start\":69937},{\"end\":71461,\"start\":70810},{\"end\":72212,\"start\":71577},{\"end\":73290,\"start\":72214},{\"end\":73431,\"start\":73292},{\"end\":73679,\"start\":73433},{\"end\":74088,\"start\":73681},{\"end\":75037,\"start\":74090},{\"end\":75679,\"start\":75039},{\"end\":76043,\"start\":75681},{\"end\":77441,\"start\":76066},{\"end\":78381,\"start\":77443}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8459,\"start\":8437},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10115,\"start\":10029},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10714,\"start\":10592},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11617,\"start\":11577},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12185,\"start\":12072},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13069,\"start\":12759},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15097,\"start\":14963},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15783,\"start\":15693},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17546,\"start\":17432},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18939,\"start\":18833},{\"attributes\":{\"id\":\"formula_10\"},\"end\":27251,\"start\":27195},{\"attributes\":{\"id\":\"formula_11\"},\"end\":31372,\"start\":31313},{\"attributes\":{\"id\":\"formula_12\"},\"end\":31730,\"start\":31664},{\"attributes\":{\"id\":\"formula_13\"},\"end\":31795,\"start\":31765},{\"attributes\":{\"id\":\"formula_14\"},\"end\":32096,\"start\":31979},{\"attributes\":{\"id\":\"formula_15\"},\"end\":32786,\"start\":32719},{\"attributes\":{\"id\":\"formula_16\"},\"end\":32897,\"start\":32840},{\"attributes\":{\"id\":\"formula_17\"},\"end\":37645,\"start\":37575},{\"attributes\":{\"id\":\"formula_19\"},\"end\":43465,\"start\":43402},{\"attributes\":{\"id\":\"formula_21\"},\"end\":43953,\"start\":43728},{\"attributes\":{\"id\":\"formula_22\"},\"end\":44391,\"start\":44282},{\"attributes\":{\"id\":\"formula_24\"},\"end\":46005,\"start\":45915},{\"attributes\":{\"id\":\"formula_25\"},\"end\":46209,\"start\":46163},{\"attributes\":{\"id\":\"formula_26\"},\"end\":46924,\"start\":46832},{\"attributes\":{\"id\":\"formula_27\"},\"end\":47438,\"start\":47419},{\"attributes\":{\"id\":\"formula_28\"},\"end\":47592,\"start\":47488},{\"attributes\":{\"id\":\"formula_29\"},\"end\":47736,\"start\":47709},{\"attributes\":{\"id\":\"formula_30\"},\"end\":48408,\"start\":48155},{\"attributes\":{\"id\":\"formula_31\"},\"end\":49331,\"start\":49261},{\"attributes\":{\"id\":\"formula_32\"},\"end\":49501,\"start\":49395},{\"attributes\":{\"id\":\"formula_33\"},\"end\":49809,\"start\":49698},{\"attributes\":{\"id\":\"formula_34\"},\"end\":50296,\"start\":50084},{\"attributes\":{\"id\":\"formula_35\"},\"end\":50512,\"start\":50397},{\"attributes\":{\"id\":\"formula_36\"},\"end\":50576,\"start\":50512},{\"attributes\":{\"id\":\"formula_37\"},\"end\":50870,\"start\":50755},{\"attributes\":{\"id\":\"formula_38\"},\"end\":50967,\"start\":50892},{\"attributes\":{\"id\":\"formula_39\"},\"end\":51567,\"start\":51548},{\"attributes\":{\"id\":\"formula_40\"},\"end\":51665,\"start\":51604},{\"attributes\":{\"id\":\"formula_41\"},\"end\":51982,\"start\":51861},{\"attributes\":{\"id\":\"formula_42\"},\"end\":52904,\"start\":52849},{\"attributes\":{\"id\":\"formula_43\"},\"end\":53081,\"start\":52904},{\"attributes\":{\"id\":\"formula_44\"},\"end\":53968,\"start\":53855},{\"attributes\":{\"id\":\"formula_45\"},\"end\":54439,\"start\":54350},{\"attributes\":{\"id\":\"formula_46\"},\"end\":54960,\"start\":54801},{\"attributes\":{\"id\":\"formula_47\"},\"end\":55863,\"start\":55815},{\"attributes\":{\"id\":\"formula_48\"},\"end\":57737,\"start\":57556},{\"attributes\":{\"id\":\"formula_49\"},\"end\":57794,\"start\":57771},{\"attributes\":{\"id\":\"formula_50\"},\"end\":58040,\"start\":57843},{\"attributes\":{\"id\":\"formula_51\"},\"end\":59910,\"start\":59821},{\"attributes\":{\"id\":\"formula_52\"},\"end\":60711,\"start\":60403},{\"attributes\":{\"id\":\"formula_53\"},\"end\":60829,\"start\":60815},{\"attributes\":{\"id\":\"formula_54\"},\"end\":61035,\"start\":60961},{\"attributes\":{\"id\":\"formula_55\"},\"end\":61221,\"start\":61082},{\"attributes\":{\"id\":\"formula_56\"},\"end\":62708,\"start\":62608},{\"attributes\":{\"id\":\"formula_57\"},\"end\":63426,\"start\":63357},{\"attributes\":{\"id\":\"formula_58\"},\"end\":63689,\"start\":63574},{\"attributes\":{\"id\":\"formula_59\"},\"end\":64255,\"start\":64251},{\"attributes\":{\"id\":\"formula_60\"},\"end\":64536,\"start\":64472},{\"attributes\":{\"id\":\"formula_62\"},\"end\":65203,\"start\":65093},{\"attributes\":{\"id\":\"formula_63\"},\"end\":66422,\"start\":66298},{\"attributes\":{\"id\":\"formula_64\"},\"end\":66590,\"start\":66513},{\"attributes\":{\"id\":\"formula_65\"},\"end\":67668,\"start\":67575},{\"attributes\":{\"id\":\"formula_66\"},\"end\":67796,\"start\":67795},{\"attributes\":{\"id\":\"formula_67\"},\"end\":67885,\"start\":67796},{\"attributes\":{\"id\":\"formula_68\"},\"end\":71576,\"start\":71462}]", "table_ref": "[{\"end\":12586,\"start\":12527},{\"end\":16080,\"start\":16073},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":25044,\"start\":25037},{\"end\":25983,\"start\":25976},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":26216,\"start\":26209},{\"end\":26694,\"start\":26687},{\"end\":28139,\"start\":28132},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":72116,\"start\":72109},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":72170,\"start\":72163},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":75678,\"start\":75671},{\"end\":76338,\"start\":76331},{\"end\":76634,\"start\":76627},{\"end\":76770,\"start\":76763},{\"end\":77070,\"start\":77063},{\"end\":77480,\"start\":77473},{\"end\":78119,\"start\":78112}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1766,\"start\":1754},{\"attributes\":{\"n\":\"2\"},\"end\":5588,\"start\":5575},{\"attributes\":{\"n\":\"3\"},\"end\":8504,\"start\":8461},{\"attributes\":{\"n\":\"3.1\"},\"end\":9219,\"start\":9185},{\"attributes\":{\"n\":\"3.2\"},\"end\":13973,\"start\":13935},{\"end\":17242,\"start\":17194},{\"attributes\":{\"n\":\"4\"},\"end\":19737,\"start\":19691},{\"attributes\":{\"n\":\"5\"},\"end\":21683,\"start\":21671},{\"attributes\":{\"n\":\"6\"},\"end\":24429,\"start\":24406},{\"attributes\":{\"n\":\"7\"},\"end\":29629,\"start\":29619},{\"end\":30801,\"start\":30762},{\"end\":33495,\"start\":33442},{\"end\":40571,\"start\":40555},{\"end\":52056,\"start\":52023},{\"end\":52448,\"start\":52391},{\"end\":61971,\"start\":61902},{\"end\":66084,\"start\":66040},{\"end\":68087,\"start\":68031},{\"end\":70808,\"start\":70750},{\"end\":76064,\"start\":76046},{\"end\":78758,\"start\":78748},{\"end\":78858,\"start\":78857},{\"end\":78968,\"start\":78958},{\"end\":79232,\"start\":79231},{\"end\":80785,\"start\":80771},{\"end\":81135,\"start\":81126},{\"end\":82878,\"start\":82869},{\"end\":87284,\"start\":87281}]", "table": "[{\"end\":81124,\"start\":80787},{\"end\":82867,\"start\":82561},{\"end\":87922,\"start\":87784}]", "figure_caption": "[{\"end\":78537,\"start\":78384},{\"end\":78746,\"start\":78540},{\"end\":78855,\"start\":78760},{\"end\":78956,\"start\":78859},{\"end\":79229,\"start\":78970},{\"end\":79359,\"start\":79233},{\"end\":79517,\"start\":79362},{\"end\":80417,\"start\":79520},{\"end\":80769,\"start\":80420},{\"end\":82561,\"start\":81137},{\"end\":83246,\"start\":82880},{\"end\":87279,\"start\":83249},{\"end\":87626,\"start\":87286},{\"end\":87784,\"start\":87629}]", "figure_ref": "[{\"end\":29219,\"start\":29204},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29262,\"start\":29254},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":38059,\"start\":38051},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":38384,\"start\":38372},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":38522,\"start\":38511},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":38868,\"start\":38860},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":39425,\"start\":39417},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":40179,\"start\":40171},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":40548,\"start\":40540},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":45668,\"start\":45627},{\"end\":52800,\"start\":52798},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":72211,\"start\":72203}]", "bib_author_first_name": "[{\"end\":88419,\"start\":88413},{\"end\":88433,\"start\":88428},{\"end\":88448,\"start\":88442},{\"end\":88697,\"start\":88690},{\"end\":88711,\"start\":88707},{\"end\":88732,\"start\":88724},{\"end\":88926,\"start\":88918},{\"end\":89182,\"start\":89177},{\"end\":89205,\"start\":89201},{\"end\":89221,\"start\":89214},{\"end\":89566,\"start\":89561},{\"end\":89591,\"start\":89585},{\"end\":89610,\"start\":89609},{\"end\":89623,\"start\":89619},{\"end\":89913,\"start\":89909},{\"end\":89929,\"start\":89921},{\"end\":89943,\"start\":89939},{\"end\":89957,\"start\":89953},{\"end\":90253,\"start\":90249},{\"end\":90266,\"start\":90262},{\"end\":90277,\"start\":90276},{\"end\":90288,\"start\":90284},{\"end\":90613,\"start\":90610},{\"end\":90623,\"start\":90620},{\"end\":90637,\"start\":90630},{\"end\":90652,\"start\":90646},{\"end\":90660,\"start\":90657},{\"end\":90667,\"start\":90665},{\"end\":90972,\"start\":90967},{\"end\":90989,\"start\":90981},{\"end\":91003,\"start\":90997},{\"end\":91017,\"start\":91009},{\"end\":91027,\"start\":91018},{\"end\":91380,\"start\":91374},{\"end\":91394,\"start\":91388},{\"end\":91408,\"start\":91403},{\"end\":91675,\"start\":91674},{\"end\":91681,\"start\":91680},{\"end\":91690,\"start\":91689},{\"end\":91700,\"start\":91699},{\"end\":91710,\"start\":91709},{\"end\":91905,\"start\":91899},{\"end\":91916,\"start\":91910},{\"end\":91931,\"start\":91924},{\"end\":91943,\"start\":91937},{\"end\":92253,\"start\":92247},{\"end\":92264,\"start\":92258},{\"end\":92276,\"start\":92272},{\"end\":92291,\"start\":92285},{\"end\":92306,\"start\":92300},{\"end\":92608,\"start\":92603},{\"end\":92624,\"start\":92619},{\"end\":92637,\"start\":92632},{\"end\":92893,\"start\":92888},{\"end\":92909,\"start\":92904},{\"end\":93263,\"start\":93257},{\"end\":93273,\"start\":93272},{\"end\":93678,\"start\":93670},{\"end\":93690,\"start\":93689},{\"end\":93930,\"start\":93923},{\"end\":94247,\"start\":94244},{\"end\":94264,\"start\":94256},{\"end\":94667,\"start\":94664},{\"end\":94683,\"start\":94676},{\"end\":94703,\"start\":94694},{\"end\":94721,\"start\":94713},{\"end\":95062,\"start\":95056},{\"end\":95082,\"start\":95077},{\"end\":95096,\"start\":95090},{\"end\":95367,\"start\":95361},{\"end\":95587,\"start\":95582},{\"end\":95867,\"start\":95862},{\"end\":95877,\"start\":95876},{\"end\":95893,\"start\":95887},{\"end\":96171,\"start\":96164},{\"end\":96190,\"start\":96184},{\"end\":96206,\"start\":96198},{\"end\":96220,\"start\":96214},{\"end\":96234,\"start\":96230},{\"end\":96252,\"start\":96245},{\"end\":96268,\"start\":96262},{\"end\":96648,\"start\":96647},{\"end\":96659,\"start\":96655},{\"end\":96666,\"start\":96660},{\"end\":96680,\"start\":96675},{\"end\":96700,\"start\":96695},{\"end\":96720,\"start\":96713},{\"end\":96740,\"start\":96734},{\"end\":96756,\"start\":96750},{\"end\":96770,\"start\":96764},{\"end\":97099,\"start\":97096},{\"end\":97113,\"start\":97109},{\"end\":97130,\"start\":97122},{\"end\":97144,\"start\":97139},{\"end\":97158,\"start\":97151},{\"end\":97174,\"start\":97169},{\"end\":97188,\"start\":97181},{\"end\":97202,\"start\":97196},{\"end\":97218,\"start\":97212},{\"end\":97232,\"start\":97226},{\"end\":97726,\"start\":97721},{\"end\":97737,\"start\":97731},{\"end\":97750,\"start\":97746},{\"end\":97991,\"start\":97986},{\"end\":98001,\"start\":97999},{\"end\":98013,\"start\":98007},{\"end\":98022,\"start\":98018}]", "bib_author_last_name": "[{\"end\":88426,\"start\":88420},{\"end\":88440,\"start\":88434},{\"end\":88455,\"start\":88449},{\"end\":88705,\"start\":88698},{\"end\":88722,\"start\":88712},{\"end\":88740,\"start\":88733},{\"end\":88936,\"start\":88927},{\"end\":89199,\"start\":89183},{\"end\":89212,\"start\":89206},{\"end\":89228,\"start\":89222},{\"end\":89237,\"start\":89230},{\"end\":89583,\"start\":89567},{\"end\":89601,\"start\":89592},{\"end\":89607,\"start\":89603},{\"end\":89617,\"start\":89611},{\"end\":89630,\"start\":89624},{\"end\":89637,\"start\":89632},{\"end\":89919,\"start\":89914},{\"end\":89937,\"start\":89930},{\"end\":89951,\"start\":89944},{\"end\":89964,\"start\":89958},{\"end\":90260,\"start\":90254},{\"end\":90274,\"start\":90267},{\"end\":90282,\"start\":90278},{\"end\":90298,\"start\":90289},{\"end\":90305,\"start\":90300},{\"end\":90618,\"start\":90614},{\"end\":90628,\"start\":90624},{\"end\":90644,\"start\":90638},{\"end\":90655,\"start\":90653},{\"end\":90663,\"start\":90661},{\"end\":90675,\"start\":90668},{\"end\":90979,\"start\":90973},{\"end\":90995,\"start\":90990},{\"end\":91007,\"start\":91004},{\"end\":91032,\"start\":91028},{\"end\":91386,\"start\":91381},{\"end\":91401,\"start\":91395},{\"end\":91417,\"start\":91409},{\"end\":91678,\"start\":91676},{\"end\":91687,\"start\":91682},{\"end\":91697,\"start\":91691},{\"end\":91707,\"start\":91701},{\"end\":91717,\"start\":91711},{\"end\":91908,\"start\":91906},{\"end\":91922,\"start\":91917},{\"end\":91935,\"start\":91932},{\"end\":91950,\"start\":91944},{\"end\":92256,\"start\":92254},{\"end\":92270,\"start\":92265},{\"end\":92283,\"start\":92277},{\"end\":92298,\"start\":92292},{\"end\":92313,\"start\":92307},{\"end\":92617,\"start\":92609},{\"end\":92630,\"start\":92625},{\"end\":92644,\"start\":92638},{\"end\":92902,\"start\":92894},{\"end\":92924,\"start\":92910},{\"end\":92931,\"start\":92926},{\"end\":93270,\"start\":93264},{\"end\":93278,\"start\":93274},{\"end\":93289,\"start\":93280},{\"end\":93687,\"start\":93679},{\"end\":93695,\"start\":93691},{\"end\":93702,\"start\":93697},{\"end\":93938,\"start\":93931},{\"end\":93948,\"start\":93940},{\"end\":94254,\"start\":94248},{\"end\":94272,\"start\":94265},{\"end\":94674,\"start\":94668},{\"end\":94692,\"start\":94684},{\"end\":94711,\"start\":94704},{\"end\":94729,\"start\":94722},{\"end\":95075,\"start\":95063},{\"end\":95088,\"start\":95083},{\"end\":95102,\"start\":95097},{\"end\":95110,\"start\":95104},{\"end\":95375,\"start\":95368},{\"end\":95396,\"start\":95377},{\"end\":95594,\"start\":95588},{\"end\":95874,\"start\":95868},{\"end\":95885,\"start\":95878},{\"end\":95900,\"start\":95894},{\"end\":95910,\"start\":95902},{\"end\":96182,\"start\":96172},{\"end\":96196,\"start\":96191},{\"end\":96212,\"start\":96207},{\"end\":96228,\"start\":96221},{\"end\":96243,\"start\":96235},{\"end\":96260,\"start\":96253},{\"end\":96275,\"start\":96269},{\"end\":96653,\"start\":96649},{\"end\":96673,\"start\":96667},{\"end\":96693,\"start\":96681},{\"end\":96711,\"start\":96701},{\"end\":96732,\"start\":96721},{\"end\":96748,\"start\":96741},{\"end\":96762,\"start\":96757},{\"end\":96777,\"start\":96771},{\"end\":96789,\"start\":96779},{\"end\":97107,\"start\":97100},{\"end\":97120,\"start\":97114},{\"end\":97137,\"start\":97131},{\"end\":97149,\"start\":97145},{\"end\":97167,\"start\":97159},{\"end\":97179,\"start\":97175},{\"end\":97194,\"start\":97189},{\"end\":97210,\"start\":97203},{\"end\":97224,\"start\":97219},{\"end\":97243,\"start\":97233},{\"end\":97729,\"start\":97727},{\"end\":97744,\"start\":97738},{\"end\":97757,\"start\":97751},{\"end\":97997,\"start\":97992},{\"end\":98005,\"start\":98002},{\"end\":98016,\"start\":98014},{\"end\":98033,\"start\":98023}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1903.08894\",\"id\":\"b0\"},\"end\":88627,\"start\":88359},{\"attributes\":{\"id\":\"b1\"},\"end\":88864,\"start\":88629},{\"attributes\":{\"id\":\"b2\"},\"end\":89099,\"start\":88866},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1552061},\"end\":89490,\"start\":89101},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1907310},\"end\":89907,\"start\":89492},{\"attributes\":{\"doi\":\"arXiv:1810.12894\",\"id\":\"b5\"},\"end\":90183,\"start\":89909},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":139930},\"end\":90556,\"start\":90185},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":206597351},\"end\":90965,\"start\":90558},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b8\"},\"end\":91326,\"start\":90967},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":724794},\"end\":91612,\"start\":91328},{\"attributes\":{\"doi\":\"arXiv, 2020\",\"id\":\"b10\"},\"end\":91897,\"start\":91614},{\"attributes\":{\"doi\":\"arXiv:1902.10250\",\"id\":\"b11\"},\"end\":92185,\"start\":91899},{\"attributes\":{\"id\":\"b12\"},\"end\":92541,\"start\":92187},{\"attributes\":{\"doi\":\"arXiv:1812.02900\",\"id\":\"b13\"},\"end\":92821,\"start\":92543},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3544558},\"end\":93178,\"start\":92823},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":58993601},\"end\":93614,\"start\":93180},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":951180},\"end\":93921,\"start\":93616},{\"attributes\":{\"doi\":\"arXiv:1807.09647\",\"id\":\"b17\"},\"end\":94166,\"start\":93923},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1763828},\"end\":94623,\"start\":94168},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":5865729},\"end\":94968,\"start\":94625},{\"attributes\":{\"doi\":\"arXiv:1910.00177\",\"id\":\"b20\"},\"end\":95326,\"start\":94970},{\"attributes\":{\"doi\":\"cs/0204043\",\"id\":\"b21\"},\"end\":95527,\"start\":95328},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":1153355},\"end\":95791,\"start\":95529},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":7784749},\"end\":96069,\"start\":95793},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":4780901},\"end\":96557,\"start\":96071},{\"attributes\":{\"doi\":\"arXiv:2002.08396\",\"id\":\"b25\"},\"end\":97094,\"start\":96559},{\"attributes\":{\"doi\":\"arXiv:1707.08817\",\"id\":\"b26\"},\"end\":97666,\"start\":97096},{\"attributes\":{\"doi\":\"arXiv:1911.11361\",\"id\":\"b27\"},\"end\":97922,\"start\":97668},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":209319223},\"end\":98314,\"start\":97924}]", "bib_title": "[{\"end\":89175,\"start\":89101},{\"end\":89559,\"start\":89492},{\"end\":90247,\"start\":90185},{\"end\":90608,\"start\":90558},{\"end\":91372,\"start\":91328},{\"end\":92886,\"start\":92823},{\"end\":93255,\"start\":93180},{\"end\":93668,\"start\":93616},{\"end\":94242,\"start\":94168},{\"end\":94662,\"start\":94625},{\"end\":95580,\"start\":95529},{\"end\":95860,\"start\":95793},{\"end\":96162,\"start\":96071},{\"end\":97984,\"start\":97924}]", "bib_author": "[{\"end\":88428,\"start\":88413},{\"end\":88442,\"start\":88428},{\"end\":88457,\"start\":88442},{\"end\":88707,\"start\":88690},{\"end\":88724,\"start\":88707},{\"end\":88742,\"start\":88724},{\"end\":88938,\"start\":88918},{\"end\":89201,\"start\":89177},{\"end\":89214,\"start\":89201},{\"end\":89230,\"start\":89214},{\"end\":89239,\"start\":89230},{\"end\":89585,\"start\":89561},{\"end\":89603,\"start\":89585},{\"end\":89609,\"start\":89603},{\"end\":89619,\"start\":89609},{\"end\":89632,\"start\":89619},{\"end\":89639,\"start\":89632},{\"end\":89921,\"start\":89909},{\"end\":89939,\"start\":89921},{\"end\":89953,\"start\":89939},{\"end\":89966,\"start\":89953},{\"end\":90262,\"start\":90249},{\"end\":90276,\"start\":90262},{\"end\":90284,\"start\":90276},{\"end\":90300,\"start\":90284},{\"end\":90307,\"start\":90300},{\"end\":90620,\"start\":90610},{\"end\":90630,\"start\":90620},{\"end\":90646,\"start\":90630},{\"end\":90657,\"start\":90646},{\"end\":90665,\"start\":90657},{\"end\":90677,\"start\":90665},{\"end\":90981,\"start\":90967},{\"end\":90997,\"start\":90981},{\"end\":91009,\"start\":90997},{\"end\":91034,\"start\":91009},{\"end\":91388,\"start\":91374},{\"end\":91403,\"start\":91388},{\"end\":91419,\"start\":91403},{\"end\":91680,\"start\":91674},{\"end\":91689,\"start\":91680},{\"end\":91699,\"start\":91689},{\"end\":91709,\"start\":91699},{\"end\":91719,\"start\":91709},{\"end\":91910,\"start\":91899},{\"end\":91924,\"start\":91910},{\"end\":91937,\"start\":91924},{\"end\":91952,\"start\":91937},{\"end\":92258,\"start\":92247},{\"end\":92272,\"start\":92258},{\"end\":92285,\"start\":92272},{\"end\":92300,\"start\":92285},{\"end\":92315,\"start\":92300},{\"end\":92619,\"start\":92603},{\"end\":92632,\"start\":92619},{\"end\":92646,\"start\":92632},{\"end\":92904,\"start\":92888},{\"end\":92926,\"start\":92904},{\"end\":92933,\"start\":92926},{\"end\":93272,\"start\":93257},{\"end\":93280,\"start\":93272},{\"end\":93291,\"start\":93280},{\"end\":93689,\"start\":93670},{\"end\":93697,\"start\":93689},{\"end\":93704,\"start\":93697},{\"end\":93940,\"start\":93923},{\"end\":93950,\"start\":93940},{\"end\":94256,\"start\":94244},{\"end\":94274,\"start\":94256},{\"end\":94676,\"start\":94664},{\"end\":94694,\"start\":94676},{\"end\":94713,\"start\":94694},{\"end\":94731,\"start\":94713},{\"end\":95077,\"start\":95056},{\"end\":95090,\"start\":95077},{\"end\":95104,\"start\":95090},{\"end\":95112,\"start\":95104},{\"end\":95377,\"start\":95361},{\"end\":95398,\"start\":95377},{\"end\":95596,\"start\":95582},{\"end\":95876,\"start\":95862},{\"end\":95887,\"start\":95876},{\"end\":95902,\"start\":95887},{\"end\":95912,\"start\":95902},{\"end\":96184,\"start\":96164},{\"end\":96198,\"start\":96184},{\"end\":96214,\"start\":96198},{\"end\":96230,\"start\":96214},{\"end\":96245,\"start\":96230},{\"end\":96262,\"start\":96245},{\"end\":96277,\"start\":96262},{\"end\":96655,\"start\":96647},{\"end\":96675,\"start\":96655},{\"end\":96695,\"start\":96675},{\"end\":96713,\"start\":96695},{\"end\":96734,\"start\":96713},{\"end\":96750,\"start\":96734},{\"end\":96764,\"start\":96750},{\"end\":96779,\"start\":96764},{\"end\":96791,\"start\":96779},{\"end\":97109,\"start\":97096},{\"end\":97122,\"start\":97109},{\"end\":97139,\"start\":97122},{\"end\":97151,\"start\":97139},{\"end\":97169,\"start\":97151},{\"end\":97181,\"start\":97169},{\"end\":97196,\"start\":97181},{\"end\":97212,\"start\":97196},{\"end\":97226,\"start\":97212},{\"end\":97245,\"start\":97226},{\"end\":97731,\"start\":97721},{\"end\":97746,\"start\":97731},{\"end\":97759,\"start\":97746},{\"end\":97999,\"start\":97986},{\"end\":98007,\"start\":97999},{\"end\":98018,\"start\":98007},{\"end\":98035,\"start\":98018}]", "bib_venue": "[{\"end\":88411,\"start\":88359},{\"end\":88688,\"start\":88629},{\"end\":88916,\"start\":88866},{\"end\":89282,\"start\":89239},{\"end\":89691,\"start\":89639},{\"end\":90024,\"start\":89982},{\"end\":90363,\"start\":90307},{\"end\":90740,\"start\":90677},{\"end\":91124,\"start\":91050},{\"end\":91455,\"start\":91419},{\"end\":91672,\"start\":91614},{\"end\":92020,\"start\":91968},{\"end\":92245,\"start\":92187},{\"end\":92601,\"start\":92543},{\"end\":92984,\"start\":92933},{\"end\":93352,\"start\":93291},{\"end\":93753,\"start\":93704},{\"end\":94028,\"start\":93966},{\"end\":94342,\"start\":94274},{\"end\":94780,\"start\":94731},{\"end\":95054,\"start\":94970},{\"end\":95359,\"start\":95328},{\"end\":95650,\"start\":95596},{\"end\":95916,\"start\":95912},{\"end\":96306,\"start\":96277},{\"end\":96645,\"start\":96559},{\"end\":97359,\"start\":97261},{\"end\":97719,\"start\":97668},{\"end\":98087,\"start\":98035},{\"end\":93400,\"start\":93354},{\"end\":94397,\"start\":94344}]"}}}, "year": 2023, "month": 12, "day": 17}