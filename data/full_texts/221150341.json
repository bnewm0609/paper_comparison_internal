{"id": 221150341, "updated": "2023-11-28 14:40:01.967", "metadata": {"title": "S3-Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization", "authors": "[{\"first\":\"Kun\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Hui\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Wayne\",\"last\":\"Zhao\",\"middle\":[\"Xin\"]},{\"first\":\"Yutao\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Sirui\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Fuzheng\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Zhongyuan\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Ji-Rong\",\"last\":\"Wen\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 29th ACM International Conference on Information & Knowledge Management", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Recently, significant progress has been made in sequential recommendation with deep learning. Existing neural sequential recommendation models usually rely on the item prediction loss to learn model parameters or data representations. However, the model trained with this loss is prone to suffer from data sparsity problem. Since it overemphasizes the final performance, the association or fusion between context data and sequence data has not been well captured and utilized for sequential recommendation. To tackle this problem, we propose the model S3-Rec, which stands for Self-Supervised learning for Sequential Recommendation, based on the self-attentive neural architecture. The main idea of our approach is to utilize the intrinsic data correlation to derive self-supervision signals and enhance the data representations via pre-training methods for improving sequential recommendation. For our task, we devise four auxiliary self-supervised objectives to learn the correlations among attribute, item, subsequence, and sequence by utilizing the mutual information maximization (MIM) principle. MIM provides a unified way to characterize the correlation between different types of data, which is particularly suitable in our scenario. Extensive experiments conducted on six real-world datasets demonstrate the superiority of our proposed method over existing state-of-the-art methods, especially when only limited training data is available. Besides, we extend our self-supervised learning method to other recommendation models, which also improve their performance.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3100260481", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cikm/ZhouWZZWZWW20", "doi": "10.1145/3340531.3411954"}}, "content": {"source": {"pdf_hash": "398d55cabeebb616da6203bd4df2663e0c2eae21", "pdf_src": "ACM", "pdf_uri": "[\"https://arxiv.org/pdf/2008.07873v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "https://arxiv.org/pdf/2008.07873", "status": "GREEN"}}, "grobid": {"id": "5625609a86d0ebef1f9dfd8d6d7f4029ce7df87a", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/398d55cabeebb616da6203bd4df2663e0c2eae21.txt", "contents": "\nVirtual Event, Ireland,3 . 2020. S 3 -Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization\nOctober 19-23, 2020. October 19-23, 2020\n\nKun Zhou \nSchool of Information\nRenmin University of China\n\n\nHui Wang \nSchool of Information\nRenmin University of China\n\n\nWayne Xin Zhao \nGaoling School of Artificial Intelligence\nRenmin University of China\n\n\nBeijing Key Laboratory of Big Data Management and Analysis Methods 4 Meituan-Dianping Group\n\n\nYutao Zhu yutao.zhu@umontreal.ca \nUniversit\u00e9 de Montr\u00e9al\nMontr\u00e9alQu\u00e9becCanada\n\nSirui Wang wangsirui@meituan.com \nFuzheng Zhang \nZhongyuan Wang \nJi-Rong Wen jrwen@ruc.edu.cn \nGaoling School of Artificial Intelligence\nRenmin University of China\n\n\nBeijing Key Laboratory of Big Data Management and Analysis Methods 4 Meituan-Dianping Group\n\n\nKun Zhou \nSchool of Information\nRenmin University of China\n\n\nHui Wang \nSchool of Information\nRenmin University of China\n\n\nWayne Xin Zhao \nGaoling School of Artificial Intelligence\nRenmin University of China\n\n\nBeijing Key Laboratory of Big Data Management and Analysis Methods 4 Meituan-Dianping Group\n\n\nYutao Zhu \nUniversit\u00e9 de Montr\u00e9al\nMontr\u00e9alQu\u00e9becCanada\n\nSirui Wang \nFuzheng Zhang \nZhongyuan Wang \nJi-Rong Wen \nGaoling School of Artificial Intelligence\nRenmin University of China\n\n\nVirtual Event, Ireland,3 . 2020. S 3 -Rec: Self-Supervised Learning for Sequential Recommendation with Mutual Information Maximization\n\nThe 29th ACM International Conference on Information and Knowledge Management (CIKM '20)\nOctober 19-23, 2020. October 19-23, 202010.1145/3340531.3411954* Corresponding author. ACM ISBN 978-1-4503-6859-9/20/10. . . $15.00 Virtual Event, Ireland. ACM, New York, NY, USA, 10 pages. https://doi.org/CCS CONCEPTS \u2022 Information systems \u2192 Recommender systems \u2020 Equal contribution\nRecently, significant progress has been made in sequential recommendation with deep learning. Existing neural sequential recommendation models usually rely on the item prediction loss to learn model parameters or data representations. However, the model trained with this loss is prone to suffer from data sparsity problem. Since it overemphasizes the final performance, the association or fusion between context data and sequence data has not been well captured and utilized for sequential recommendation.To tackle this problem, we propose the model S 3 -Rec, which stands for Self-Supervised learning for Sequential Recommendation, based on the self-attentive neural architecture. The main idea of our approach is to utilize the intrinsic data correlation to derive self-supervision signals and enhance the data representations via pre-training methods for improving sequential recommendation. For our task, we devise four auxiliary self-supervised objectives to learn the correlations among attribute, item, subsequence, and sequence by utilizing the mutual information maximization (MIM) principle. MIM provides a unified way to characterize the correlation between different types of data, which is particularly suitable in our scenario. Extensive experiments conducted on six real-world datasets demonstrate the superiority of our proposed method over existing state-of-the-art methods, especially when only limited training data is available. Besides, we extend our self-supervised learning method to other recommendation models, which also improve their performance.\n\nINTRODUCTION\n\nRecent years have witnessed the great success of many online platforms, such as Amazon and Taobao. Within online platforms, users' behaviors are dynamic and evolving over time. Thus it is critical to capture the dynamics of sequential user behaviors for making appropriate recommendations. In order to accurately characterize user interests and provide high-quality recommendations, the task of sequential recommendation has been widely studied in the literature [3,8,20,21,24].\n\nTypically, sequential recommendation methods [3,8,21,24] capture useful sequential patterns from users' historical behaviors. Such motivation has been extensively explored with deep learning. Various methods using recurrent neural networks (RNNs) [3], convolutional neural networks (CNNs) [24], and self-attention mechanisms [8] have been proposed to learn good representations of user preference and characterize sequential user-item interactions.\n\nFurthermore, researchers have incorporated rich contextual information (such as item attributes) to neural sequential recommenders [4,6,29]. It has been demonstrated that contextual information is important to consider for improving the performance of sequential recommender systems.\n\nAlthough existing methods have been shown effective to some extent, there are two major shortcomings that are likely to affect the recommendation performance. First, they rely on the item prediction loss to learn the entire model. When context data is incorporated, the involved parameters are also learned through the only optimization objective. It has been found that such an optimization way is easy to suffer from issues such as data sparsity [21,22]. Second, they overemphasize the final performance, while the association or fusion between context data and sequence data has not been well captured in data representations. As shown in increasing evidence from various fields [1,5,10], effective data representation (e.g., pre-trained contextualized embedding) has been a key factor to improve the performance of existing models or architectures. Therefore, there is a need to rethink the learning paradigm to develop more effective sequential recommender systems.\n\nTo address the above issues, we borrow the idea of self-supervised learning for improving sequential recommendation. Self-supervised learning [1,15] is a newly emerging paradigm, which aims to let the model learn from the intrinsic structure of the raw data. A general framework of self-supervised learning is to first construct training signals directly from the raw data and then pre-train the model parameters with additionally devised optimization objectives. As previously discussed, limited supervision signals and ineffective data representations are the two major learning issues with existing neural sequential methods. Fortunately, self-supervised learning seems to provide a promising solution to both problems: it utilizes the intrinsic data correlation to devise auxiliary training objectives and enhances the data representations via pre-trained methods with rich self-supervised signals. However, for sequential recommendation, the context information exists in different forms or with varying intrinsics, including item, attribute, subsequence, or sequence. It is not easy to develop a unified approach to characterizing such data correlations. For this problem, we are inspired by the recently proposed mutual information maximization (MIM) method [5,10,11,30]. It has been shown to be particularly effective to capture the correlation between different views (or parts) of the original input by maximizing the mutual information between the encoded representations of these views.\n\nTo this end, in this paper, we propose a novel Self-Supervised learning approach to improve Sequential Recommendation with MIM, which is called S 3 -Rec. Based on a self-attentive recommender architecture [8], we propose to first pre-train the sequential recommender with self-supervised signals and then fine-tune the model parameters according to the recommendation task. The major novelty lies in the pre-training stage. In particular, we carefully devise four self-supervised optimization objectives for capturing item-attribute, sequence-item, sequence-attribute and sequencesubsequence correlations, respectively. These optimization objectives are developed in a unified form of MIM. As such, S 3 -Rec is able to characterize the correlation in varying levels of granularity or between different forms in a general way. It is also flexible to adapt to new data types or new correlation patterns. Via such a pre-trained method, we can effectively fuse various kinds of context data, and learn attribute-aware contextualized data representations. Finally, the learned data representations are fed into the neural recommender, which will be optimized according to the recommendation performance.\n\nTo validate the effectiveness of our proposed S 3 -Rec method, we conduct extensive experiments on six real-world recommendation datasets of different domains. Experimental results show that S 3 -Rec achieves state-of-the-art performance compared to a number of competitive methods, especially when training data is limited.\n\nWe also show that our S 3 -Rec is effective to adapt to other classes of neural architectures, such as GRU and CNN.\n\nOur main contributions are summarized as follows: (1) To the best of our knowledge, it is the first time that self-supervised learning with MIM has been applied to improve the sequential recommendation task; (2) We propose four self-supervised optimization objectives to maximize the mutual information of context information in different forms or granularities; (3) Extensive experiments conducted on six real-world datasets demonstrate the effectiveness of our proposed approach.\n\n\nRELATED WORK 2.1 Sequential Recommendation\n\nEarly works on sequential recommendation are based on the Markov Chain assumption. MC-based methods [20] estimated an item-item transition probability matrix and utilized it to predict the next item given the last interaction of a user. A series of works follow this line and extend it for high-order MCs [4,8,24]. With the development of the neural networks, Hidasi et al. [3] firstly introduced Gated Recurrent Units (GRU) to the session-based recommendation and a surge of following variants modified this model by introducing pair-wise loss functions [4], memory networks [6,7], hierarchical structures [17], copy mechanism [18] and reinforcement learning [27], etc. There are also studies that leverage other architectures [8,23,24] for sequential recommendation. However, these approaches neglect the rich attribute information about items. To tackle this problem, TransFM [16] utilized Factorization Machines to incorporate arbitrary real-valued features to the sequential recommendation. FDSA [29] employed a feature-level self-attention block to leverage the attribute information about items in user history. Despite the remarkable success of these sequential recommendation models, the correlations among attribute, item, and sequence are still not utilized and modeled sufficiently.\n\n\nSelf-supervised Learning\n\nSelf-supervised learning [1,5,15] aims at training a network on an auxiliary objective where the ground-truth samples are obtained from the raw data automatically. The general framework is to construct training signals directly from the correlation within the raw data and utilize them to train the model. The correlation information learned through self-supervised learning can then be easily utilized to benefit other tasks. Several self-supervised objectives have been introduced to use non-visual but intrinsically correlated features to guide the visual feature learning [5]. As for language modeling [1,15], it is a popular self-supervised objective for natural language processing, where the model learns to predict the next word or sentence given the previous sequences. The learned representations of words or sequences can improve the performance of downstream tasks such as machine reading comprehension [1] and natural language understanding [10].\n\nMutual information maximization [5,10,11] is a special branch of the self-supervised learning. It is inspired by the InfoMax principle [11] and has made important progress in several domains such as computer vision [5], audio processing [25], and nature language understanding [10]. This method splits the input data into multiple (possibly overlapping) views and maximizes the mutual information between representations of these views. The views derived from other inputs are used as negative samples. Different from the above approaches, our work is the first to consider the correlations within the contextual information as the self-supervised signals in sequential recommendation. We maximize the mutual information among the views of the attribute, item, and sequence, which are in different levels of granularity of the contextual information. The enhanced data representations can improve recommendation performance.\n\n\nPRELIMINARIES\n\nIn this section, we first formulate the sequential recommendation problem and then introduce the technique of mutual information maximization.\n\n\nProblem Statement\n\nAssume that we have a set of users and items, denoted by U and I, respectively, where u \u2208 U denotes a user and i \u2208 I denotes an item. The numbers of users and items are denoted as |U| and |I|, respectively. Generally, a user u has a chronologically-ordered interaction sequence with items: {i 1 , \u00b7 \u00b7 \u00b7 , i n }, where n is the number of interactions and i t is the t-th item that the user u has interacted with. For convenience, we use i j:k to denote the subsequence, i.e., i j:k = {i j , \u00b7 \u00b7 \u00b7 , i k } where 1 \u2264 j < k \u2264 n. Besides, each item i is associated with several attributes A i = {a 1 , \u00b7 \u00b7 \u00b7 , a m }. For example, a song is typical with auxiliary information such as artist, album, and popularity for music recommender. All attributes constitute an attribute set A, and the number of attributes is donated as |A|.\n\nBased on the above notations, we now define the task of sequential recommendation. Formally, given the historical behaviors of a user {i 1 , \u00b7 \u00b7 \u00b7 , i n } and the attributes A i of each item i, the task of sequential recommendation is to predict the next item that the user is likely to interact with at the (n + 1)-th step.\n\n\nMutual Information Maximization\n\nAn important technique in our approach is the Mutual Information Maximization (MIM). It is developed on the core concept of mutual information, which measures dependencies between random variables. Given two random variables X and Y , it can be understood as how much knowing X reduces the uncertainty in Y or vice versa. Formally, the mutual information between X and Y is:\nI (X, Y ) = H (X ) \u2212 H (X |Y ) = H (Y ) \u2212 H (Y |X ).(1)\nMaximizing mutual information directly is usually intractable. Thus we resort to a lower bound on I (X, Y ). One particular lower bound that has been shown to work well in practice is InfoNCE [10,12,25], which is based on Noise Contrastive Estimation (NCE) [2]. InfoNCE is defined as:\nE p(X ,Y ) [f \u03b8 (x, y) \u2212 E q(\u1ef8 ) [log \u1ef9 \u2208\u1ef8 exp f \u03b8 (x,\u1ef9)]] + log |\u1ef8 |,(2)\nwhere x and y are different views of an input, and f \u03b8 is a function parameterized by \u03b8 (e.g., a dot product between encoded representations of a word and its context [10] or a dot product between encoded representations of an image and the local regions of the image [5]), and\u1ef8 is a set of samples drawn from a proposal distribution q(\u1ef8 ), which contains a positive sample y and |\u1ef8 | \u2212 1 negative samples.\n\nNote that InfoNCE is related to the cross-entropy. If\u1ef8 always includes all possible values of the random variable Y (i.e.,\u1ef8 = Y ) and they are uniformly distributed, maximizing InfoNCE is analogous to maximize the standard cross-entropy loss:\nE p(X ,Y ) [f \u03b8 (x, y) \u2212 log \u1ef9 \u2208Y exp f \u03b8 (x,\u1ef9)].\n(\n\nThis equation shows that InfoNCE is related to maximize p \u03b8 (y|x), and it approximates the summation over elements in Y (i.e.,, the partition function) by negative sampling. Based on this formula, we can utilize specific X, Y to maximize the mutual information between different views of the raw data, e.g., an item and its attributes, or a sequence and the items that it contains.\n\n\nAPPROACH 4.1 Overview\n\nExisting studies [3,4,8,24] mainly emphasize the effect of sequential characteristics using an item-level optimization objective alone. Inspired by recent progress with MIM [5,28], we take a different perspective to develop neural sequential recommenders by maximizing the mutual information among different views of the raw data.\n\nThe basic idea of our approach is to incorporate several elaborately designed self-supervised learning objectives for enhancing the original model. To develop such objectives, we leverage effective correlation signals reflected in the intrinsic characteristics of the input. For our task, we consider the information in different levels of granularity, including attribute, item, segment (i.e., subsequence), and sequence, which are considered as different views of the input. By capturing the multi-view correlation, we unify these self-supervised learning objectives with the recently proposed pretraining framework in language modeling [1].\n\nThe overview of S 3 -Rec is presented in Fig. 1. In the following sections, we first introduce the base model of our proposed approach that is developed on the Transformer architecture [8]. Then, we will describe how we utilize the correlation signals among attributes, items, segments, and sequences to enhance the data representations based on the InfoNCE [10,25] method. Finally, we present the discussions on our approach.\n\n\nBase Model\n\nWe develop the basic framework for sequential recommendation model by stacking the embedding layer, self-attention blocks, and the prediction layer.\n\n\nEmbedding Layer.\n\nIn the embedding mapping stage, we maintains an item embedding matrix M I \u2208 R | I |\u00d7d and an attribute embedding matrix M A \u2208 R | A |\u00d7d . The two matrices project the high-dimensional one-hot representation of an item or attribute to low-dimensional dense representations. Given a n-length item sequence, we apply a look-up operation from M I to form the input embedding matrix E \u2208 R n\u00d7d . Besides, we incorporate a learnable Full Paper Track CIKM '20, October 19-23, 2020, Virtual Event, Ireland\n\n\nItems Attributes\n\nItem Embedding Bidirectional Self-Attenion ...\n\n\nSequence-Item MIM\n\n\nItem Embedding\n\n\nSequence-Attribute MIM\n\n\nItem-Attribute MIM\n\n\nAttribute Embedding\n\nItem Embedding\n\n\nSequence-Sequence MIM\n\n\nBidirectional\n\nSelf-Attenion\n\n\n(1) Associated Attribute Prediction (2) Masked Item Prediction (3) Masked Attribute Prediction (4) Segment Prediction\n\nItem Embedding  Figure 1: The overview of S 3 -Rec in the pre-training stage. We assume that the user sequence is {i 1 , \u00b7 \u00b7 \u00b7 , i n } and each item i is associated with several attributes A i = {a 1 , \u00b7 \u00b7 \u00b7 , a m }. We incorporate four self-supervised learning objectives: (1)  position encoding matrix P \u2208 R n\u00d7d to enhance the input representation of the item sequence. By this means, the sequence representation E I \u2208 R n\u00d7d can be obtained by summing two embedding matrices: E I = E + P. Since our task utilizes auxiliary context data, we also form an embedding matrix E A \u2208 R k \u00d7d for each item from the entire attribute embedding matrix M A , where k is the number of item attributes.\n\n\nSelf-Attention Block.\n\nBased on the embedding layer, we develop the item encoder by stacking multiple self-attention blocks. A self-attention block generally consists of two sub-layers, i.e., a multi-head self-attention layer and a point-wise feed-forward network. The multi-head self-attention mechanism has been adopted for effectively extracting the information selectively from different representation subspaces. Specifically, the multi-head self-attention is defined as:\nMultiHeadAttn(F l ) = [head 1 , head 2 , ..., head h ]W O ,(4)head i = Attention(F l W Q i , F l W K i , F l W V i ),(5)\nwhere the F l is the input for the l-th layer. When l = 0, we set F 0 = E I , and the projection matrix W\nQ i \u2208 R d\u00d7d/h , W K i \u2208 R d\u00d7d/h , W Q V \u2208 R d\u00d7d/h and W O \u2208 R d\u00d7d\nare the corresponding learnable parameters for each attention head. The attention function is implemented by scaled dot-product operation:\nAttention(Q, K, V) = softmax( QK \u22a4 d/h )V,(6)\nwhere\nQ = F l W Q i , K = F l W K i , and V = F l W V i\nare the linear transformations of the input embedding matrix, and d/h is the scale factor to avoid large values of the inner product.\n\nSince the multi-head attention function is mainly built on the linear projections. We endow the non-linearity of the self-attention block by applying a point-wise feed-forward network. The computation is defined as:\nF l = [FFN(F l 1 ) \u22a4 ; \u00b7 \u00b7 \u00b7 ; FFN(F l n ) \u22a4 ],(7)FFN(x) = (ReLU(xW 1 + b 1 ))W 2 + b 2 ,(8)\nwhere W 1 ,b 1 ,W 2 ,b 2 are trainable parameters.\n\nIn sequential recommendation, only the information before the current time step can be utilized, thus we apply the mask operation for the output of the multi-head self-attention function to remove all connections between Q i and K i . Inspired by BERT [1], at the pre-training stage, we remove the mask mechanism to acquire the bidirectional context-aware representation of each item in an item sequence. It is beneficial to incorporate context from both directions for sequence representation learning [1,23].\n\n\nPrediction Layer.\n\nIn the final layer of S 3 -Rec, we calculate the user's preference score for the item i in the step (t + 1) under the context from user history as:\nP(i t +1 = i |i 1:t ) = e \u22a4 i \u00b7 F L t ,(9)\nwhere e i is the representation of item i from item embedding matrix M I , F L t is the output of the L-layer self-attention block at step t and L is the number of self-attention blocks.\n\n\nSelf-supervised Learning with MIM\n\nBased on the above self-attention model, we further incorporate additional self-supervised signals with MIM to enhance the representations of input data. We adopt a pre-training way to construct different loss functions based on the multi-view correlation.\n\n\nModeling Item-Attribute Correlation.\n\nWe first maximize the mutual information between items and attributes. For each item, the attributes provide fine-grained information about it. Therefore, we aim to fuse item-and attribute-level information through modeling item-attribute correlation. In this way, it is expected to inject useful attribute information into item representations.\n\nGiven an item i and the attribute set A i = {a 1 , \u00b7 \u00b7 \u00b7 , a k }, we treat the item itself and its associated attributes as two different views. Formally, let e i denote the item embedding obtained by the embedding layer, and e a j denote the embedding for the j-th attribute a j \u2208 A i . We design a loss function by the contrastive learning framework that maximizes the mutual information between the two views. Following Eq. 3, we minimize the Associated Attribute Prediction (AAP) loss by: (10) where we sample negative attributes\u00e3 that enhance the association between the item i and the ground-truth attributes, \"\\\" defines set subtraction operation. The function f (\u00b7, \u00b7) is implemented with a simple bilinear network:\nL AAP (i, A i ) = E a j \u2208A i [f (i, a j ) \u2212 log \u00e3 \u2208A\\A i exp(f (i,\u00e3))],f (i, a j ) = \u03c3 e \u22a4 i \u00b7 W AAP \u00b7 e a j ,(11)\nwhere W AAP \u2208 R d\u00d7d is a parameter matrix to learn and \u03c3 (.) is the sigmoid function. Note that for clarity, we give the loss definition L AAP for a single item. It will be easy to define this loss over the entire item set.\n\n\nModeling Sequence-Item Correlation.\n\nConventional sequential recommendation models are usually trained to predict the item at the next step. This approach only considers the sequential characteristics in an item sequence from left to right. While it is noted that the entire interaction sequence is indeed observed by the model in the training process. Inspired by the masked language model like BERT [1], we propose to model the bidirectional information in item sequence by a Cloze task. For our task, the Cloze setting is described as below: at each training step, we randomly mask a proportion of items in the input sequence (i.e., replace them with special tokens \"[mask]\"). Then we predict the masked items from the original sequence based on the surrounding context in both directions. Therefore, the second loss we consider is to recover the actual item with the bidirectional context from the input sequences. For this purpose, we prepare a pre-trained version of the base model in Section 4.2, which is a bidirectional Transformer architecture. As illustration, let us mask the t-th item i t in a sequence {i 1 , \u00b7 \u00b7 \u00b7 , i t , \u00b7 \u00b7 \u00b7 , i n }. We treat the rest sequence {i 1 , \u00b7 \u00b7 \u00b7 , mask, \u00b7 \u00b7 \u00b7 , i n } as the surrounding context for i t , denoted by C i t . Given the surrounding context C i t and the masked item i t , we treat them as two different views to fuse for learning data representations. Following Eq. 3, we minimize the Masked Item Prediction (MIP) loss by:\nL M I P (C i t , i t ) = f (C i t , i t ) \u2212 log[ \u0129 \u2208I\\{i t } f (C i t , i t )],(12)\nwhere\u0129 denotes an irrelevant item, and f (\u00b7, \u00b7) is implemented according to the following formula:\nf (C i t , i t ) = \u03c3 F \u22a4 t \u00b7 W M I P \u00b7 e i t ,(13)\nwhere W M I P \u2208 R d\u00d7d is a parameter matrix to learn and F t is the learned representation for the t-th position using the bidirectional Transformer architecture obtained in the same way as Eq. 7.\n\n\nModeling Sequence-Attribute Correlation.\n\nHaving modeled both item-attribute and sequence-item correlations, we further consider directly fusing attribute information with sequential contexts. Specifically, we adopt a similar way as in Section 4.3.2 to recover the attributes of a masked item based on surrounding contexts. Given a masked item i t , we treat its surrounding context C i t and its attribute set A i t as two different views for MIM. As such, we can develop the following Masked Attribute Prediction (MAP) loss by:\nL M AP (C i t , A i t ) =E a \u2208A i t [f (C i t , a) \u2212 log \u00e3 \u2208A\\A i exp(f (C i t ,\u00e3))],(14)\nwhere f (\u00b7, \u00b7) is implemented according to the following formula:\nf (C i t , a) = \u03c3 F \u22a4 t \u00b7 W M AP \u00b7 e a ,(15)\nwhere W M AP \u2208 R d\u00d7d is a parameter matrix to learn. Note that existing methods [4,8,24] seldom directly model the correlation between the sequential context and attribute information. While, we would like to explicitly model the correlation to derive more meaningful supervision signals, which is useful to improve the data representations for multi-granularity information.\n\n\n4.3.4\n\nModeling Sequence-Segment Correlation. As shown above, the Cloze learning strategy plays a key role in our pre-trained approach in fusing sequential contexts with target information. However, a major difference between item sequence with word sequence is that a single target item may not be highly related to surrounding contexts. For example, a user has bought some products just because they were on sale. Based on this concern, we extend the Cloze strategy from a single item to item subsequence (i.e., called segment). Apparently, an item segment reflects more clear, stable user preference than a single item. Therefore, we follow a similar strategy in Section 4.3.2 to recover an item subsequence from surrounding contexts. It is expected to enhance the self-supervised learning signal and improve the pre-trained performance. Let i j 1 :j 2 denote the subsequence from item i j 1 to i j 2 , and C i j 1 :j 2 denote the context for i j 1 :j 2 within the entire sequence. Similar to Eq. 12, we can recover the missing item segment with a MIM formulation, which is so called the Segment Prediction (SP) loss as:\nL S P (C i j 1 :j 2 , i j 1 :j 2 ) =f (C i j 1 :j 2 , i j 1 :j 2 ) \u2212 log \u0129 j 1 :j 2 exp f (C i j 1 :j 2 ,\u0129 j 1 :j 2 ) ,(16)\nwhere\u0129 j 1 ,j 2 is the corrupted negative subsequence and f (\u00b7, \u00b7) is implemented according to the following formula:\nf (C i j 1 :j 2 , i j 1 :j 2 ) = \u03c3 s \u22a4 \u00b7 W S P \u00b7s ,(17)\nwhere W S P \u2208 R d\u00d7d is a parameter matrix to learn, and s ands are the learned representations for the contexts C i j 1 :j 2 and subsequence i j 1 :j 2 , respectively. In order to learn s ands, we apply the bidirectional Transformer to obtain the state representations of the last position in a sequence. \n\n\nLearning and Discussion\n\nIn this part, we present the learning and related discussions of our S 3 -Rec for sequential recommendation.\n\n\n4.4.1\n\nLearning. The entire procedure of S 3 -Rec consists of two important stages, namely pre-training and fine-tuning stages. We adopt bidirectional and unidirectional Transformer [26] architectures for the two stages, respectively. At the pre-trained stage, we optimize the self-supervised learning objectives by considering four different kinds of correlations (Eq. 10, Eq. 12, Eq. 14 and Eq. 16); at the fine-tuning stage, we utilize the learned parameters from the pre-trained stage to initialize the parameters of the unidirectional Transformer, and then utilize the left-to-right supervised signals to train the network. We adopt the pairwise rank loss to optimize its parameters as:\nL main = \u2212 u \u2208U n t =1 log \u03c3 P(i t +1 |i 1:t ) \u2212 P(i \u2212 t +1 |i 1:t ) ,(18)\nwhere we pair each ground-truth item i t +1 with a negative item i \u2212 t +1 that is randomly sampled.\n\n\nDiscussion\n\n. Our work provides a novel self-supervised approach to capturing the intrinsic data correlation from the input as an additional signal through the pre-trained models. This approach is quite general so that many existing methods can be included in this framework. We make a brief discussion below.\n\nFeature-based approaches such as Factorization Machine [20] and AutoInt [22] mainly learn data representations through the interaction of context features. The final prediction is made according to the actual interaction results between the user and item features. In S 3 -Rec, the associated attribute prediction loss L AAP in Eq. 10 and the masked attribute prediction loss L M AP in Eq. 14 have the similar effect in feature interaction. However, we do not explicitly model the interaction between attributes. Instead, we focus on capturing the association between attribute information and item/sequential contexts. A major difference in our work is to utilize feature interaction as additional supervision signals to enhance data representations instead of making predictions.\n\nSequential models such as GRU4Rec [21] and SASRec [8] mainly focus on modeling the sequential dependencies between contextual items and the target item in a left-to-right order. S 3 -Rec additionally incorporates a pre-trained stage that leverages four different kinds of self-supervised learning signals for enhancing data representations. In particular, the masked item prediction loss L M I P in Eq. 12 has a similar effect to capture sequential dependencies as in [8,21] except that it can also utilize bidirectional sequential information.\n\nAttribute-aware sequential models such as TransFM [16] and FDSA [29] leverage the contextual features to improve the sequential recommender models, in which these features are treated as auxiliary information to enhance the representation of items or sequences. In our S 3 -Rec, the L AAP loss and L M AP loss aim to fuse attribute with items or sequential contexts, which is able to achieve the same effect as previous methods [16,29]. Besides, the pre-trained data representations can be also applied to improve existing methods.  Table 1.\n\n(1) Meituan 1 : this dataset consists of six-year (from Jan. 2014 to Jan. 2020) transaction records in Beijing on the Meituan platform. We select categories, locations, and the keywords extracted from customer reviews as attributes.\n\n(2) Amazon Beauty, Sports, and Toys: these three datasets are obtained from Amazon review datasets in [14]. In this work, we select three subcategories: \"Beauty\", \"Sports and Outdoors\", and \"Toys and Games\", and utilize the fine-grained categories and the brands of the goods as attributes.\n\n(3) Yelp 2 : this is a popular dataset for business recommendation. As it is very large, we only use the transaction records after January 1st, 2019. We treat the categories of businesses as attributes.\n\n(4) LastFM 3 : this is a music artist recommendation dataset and contains user tagging behaviors for artists. In this dataset, the tags of the artists given by the users are used as attributes.\n\nFor all datasets, we group the interaction records by users and sort them by the interaction timestamps ascendingly. Following [21,29], we only keep the 5-core datasets, and filter unpopular items and inactive users with fewer than five interaction records.\n\n\nEvaluation Metrics.\n\nWe employ top-k Hit Ratio (HR@k), topk Normalized Discounted Cumulative Gain (NDCG@k), and Mean Reciprocal Rank (MRR) to evaluate the performance, which are widely used in related works [21,29]. Since HR@1 is equal to NDCG@1, we report results on HR@{1, 5, 10}, NGCG@{5, 10}, and MRR. Following previous works [8,19,23], we apply the leave-oneout strategy for evaluation. Concretely, for each user interaction sequence, the last item is used as the test data, the item before the last one is used as the validation data, and the remaining data is used for training. Since the item set is large, it is time-consuming to use all items as candidates for testing. Following the common strategy [7,8], we pair the ground-truth item with 99 randomly sampled negative items that the user has not interacted with. We calculate all metrics according to the ranking of the items and report the average score over all test users.\n\n\nBaseline Models.\n\nWe compare our proposed approach with the following eleven baseline methods:\n\n(1) PopRec is a non-personalized method that ranks items according to popularity measured by the number of interactions.\n\n(2) FM [20] characterizes the pairwise interactions between variables using factorized model.\n\n(3) AutoInt [22] utilizes the multi-head self-attentive neural network to learn the feature interaction.\n\n(4) GRU4Rec [3] applies GRU to model user click sequence for session-based recommendation. We represent the items using embedding vectors rather than one-hot vectors.\n\n(5) Caser [24] is a CNN-based method capturing high-order Markov Chains by applying horizontal and vertical convolutional operations for sequential recommendation.\n\n(6) SASRec [8] is a self-attention based sequential recommendation model, which uses the multi-head attention mechanism to recommend the next item.\n\n(7) BERT4Rec [23] uses a Cloze objective loss for sequential recommendation by the bidirectional self-attention mechanism.\n\n(8) HGN [13] is recently proposed and adopts hierarchical gating networks to capture long-term and short-term user interests.\n\n(9) GRU4Rec F [4] is an improved version of GRU4Rec, which leverages attributes to improve the performance.\n\n(10) SASRec F is our extension of SASRec, which concatenates the representations of item and attribute as the input to the model.\n\n(11) FDSA [29] constructs a feature sequence and uses a featurelevel self-attention block to model the feature transition patterns. This is the state-of-the-art model in sequential recommendation.\n\n\nImplementation Details.\n\nFor Caser and HGN, we use the source code provided by their authors. For other methods, we implement them by PyTorch. All hyper-parameters are set following the suggestions from the original papers.\n\nFor our proposed S 3 -Rec, we set the number of the self-attention blocks and the attention heads as 2. The dimension of the embedding is 64, and the maximum sequence length is 50 (following [8]). Note that our training phase contains two stages (i.e., pre-training and fine-tuning stage), the learned parameters in the pre-training stage are used to initialize the embedding layers and self-attention layers of our model in the fine-tuning stage.\n\nIn the pre-training stage, the mask proportion of item is set as 0.2 and the weights for the four losses (i.e., AAP, MIP, MAP, and SP) are set as 0.2, 1.0, 1.0, and 0.5, respectively, based on our empirical experiments. We use the Adam optimizer [9] with a learning rate of 0.001, where the batch size is set as 200 and 256 in the pre-training and the fine-tuning stage, respectively. We pre-train our model for 100 epochs and fine-tune it on the recommendation task. The code and data set are available at the link: https://github.com/RUCAIBox/ CIKM2020-S3Rec 4 .\n\n\nExperimental Results\n\nThe results of different methods on all datasets are shown in Table 2. Based on the results, we can find:\n\nFor three non-sequential recommendation baselines, the performance order is consistent across all datasets, i.e., PopRec > AutoInt > FM. Due to the \"rich-gets-richer\" effect in product adoption, PopRec is a robust baseline. AutoInt performs better than FM on most datasets because the multi-head self-attention mechanism has a stronger capacity to model attributes. However, the performance of AutoInt is worse than that of FM on Meituan dataset. A potential reason is that the multi-head self-attention may incorporate more noise from the attributes since they are keywords extracted from the reviews on Meituan platform. In general, non-sequential recommendation methods perform worse than sequential recommendation methods, since the sequential pattern is important to consider in our task.\n\nAs for sequential recommendation baseline methods, SASRec and BERT4Rec utilize the unidirectional and bidirectional selfattention mechanism respectively, and achieve better performance than GRU4Rec and Caser. It indicates that self-attentive architecture is particularly suitable for modeling sequential data. However, their improvements are not stable when training with the conventional next-item prediction loss. Besides, HGN achieves comparable performance with SASRec and BERT4Rec. This indicates the hierarchical gating network can well model the relations between closely relevant items. However, when directly injecting the attribute information into GRU4Rec and SASRec (i.e., GRU4Rec F and SASRec F ), the performance improvement is not consistent. This method yields improvement on Beauty, Sports, Toys, and Yelp datasets, but has a negative influence on other datasets. One possible reason is that simply concatenating item representations and its attributes representations cannot effectively fuse the two kinds of information. In most cases, FDSA achieves the best performance among all baselines. This suggests that the feature-level self-attention blocks can capture useful sequential feature interaction patterns.\n\nFinally, by comparing our approach with all the baselines, it is clear to see that S 3 -Rec performs consistently better than them by a large margin on six datasets. Different from these baselines, we adopt the self-supervised learning to enhance the representations of the attribute, item, and sequence for the recommendation task, which incorporates four pre-training objectives to model multiple data correlations by MIM. This result also shows that the selfsupervised approach is effective to improve the performance of the self-attention architecture for sequential recommendation.\n\n\nFurther Analysis\n\nNext, we continue to study whether S 3 -Rec works well in more detailed analysis.\n\n\nAblation Study.\n\nOur proposed self-supervised approach S 3 -Rec designs four pre-training objectives based on MIM. To verify the effectiveness of each objective, we conduct the ablation study on Meituan, Beauty, Sports, and Toys datasets to analyze the contribution of each objective. NDCG@10 is adopted for this evaluation. The results from the best baseline FDSA are also provided for comparison.\n\nFrom the results in Fig. 2, we can observe that removing any self-supervised objective would lead to the performance decrease. It indicates all the objectives are useful to improve the recommendation performance. Besides, the importance of these objectives Table 2: Performance comparison of different methods on six datasets. The best performance and the second best performance methods are denoted in bold and underlined fonts respectively. \" * \" indicates the statistical significance for p < 0.01 compared to the best baseline method.  Figure 2: Ablation study of our approach on four datasets (NDCG@10). \"\u00ac\" indicates that the corresponding objective is removed in the pre-training stage, while the rest objectives are kept.\n\nis varying on different datasets. Overall, the AAP (Associated Attribute Prediction) and the MAP (Masked Attribute Prediction) are more important than the other objectives. Removing each of them yields a larger drop of performance on all datasets. One possible reason is that these two objectives enhance the representations of item and sequence with the attributes information.   It is clearly seen that all model variants are better than the best baseline FDSA, which is trained only with next-item predication loss.\n\n\n5.\n\n3.2 Applying Self-Supervised Learning to Other Models. Since self-supervised learning itself is a learning paradigm, it can generally apply to various models. Thus, in this part, we conduct an experiment to examine whether our method can bring improvements to other models. We use the self-supervised approach to pre-training some baseline models on Beauty and Toys datasets. For GRU4Rec, GRU4Rec F , SASRec, and SASRec F , we directly apply our pre-training objectives to improve them. It is worth noting that GRU4Rec and SASRec are unidirectional models, so we maintain the unidirectional encoder layer in the pre-training stage. For Au-toInt and Caser, since their architectures do not support some of the pre-training objectives 5 , we only utilize the pre-trained parameters to initialize the parameters of the embedding layers.\n\nThe results of NDCG@10 on Beauty and Toys datasets are shown in Fig. 3. First, after pre-training by our approach, all the baselines achieve better performance. This shows that self-supervised learning can also be applied to improve their performance. Second, S 3 -Rec outperforms all the baselines after pre-training. This is because 5 Because their base models do not support the mask operations. our model adopts the bidirectional Transformer encoder in the pretraining stage, which is more suitable for our approach. Third, we can see the GRU-based models achieve less improvement than the other models. One possible reason is that RNN-based architecture limits the potential of self-supervised learning.\n\n\nPerformance Comparison w.r.t. the Amount of Training Data.\n\nConventional recommendation systems require a considerable amount of training data, thus they are likely to suffer from the cold start problem in real-world applications. This problem can be alleviated by our method because the proposed self-supervised learning approach can better utilize the data correlation from input. We simulate the data sparsity scenarios by using different proportions of the full dataset, i.e., 20%, 40%, 60%, 80%, and 100%. Fig. 4 shows the evaluation results on Sports and Yelp datasets. As we can see, the performance substantially drops when less training data is used. While, S 3 -Rec is consistently better than baselines in all cases, especially in an extreme sparsity level (20%). This observation implies that S 3 -Rec is able to make better use of the data with the self-supervised method, which alleviates the influence of data sparsity problem for sequential recommendation to some extent.\n\n\nPerformance Comparison w.r.t. the Number of Pre-training\n\nEpochs. Our approach consists of a pre-training stage and a finetuning stage. In the pre-training stage, our model can learn the enhanced representations of the attribute, item, subsequence, and sequence for the recommendation task. The number of pre-training epochs affects the performance of the recommendation task. To investigate this, we pre-train our model with a varying number of epochs and fine-tune it on the recommendation task. Fig. 5 presents the results on Beauty and Toys datasets. The horizontal dash lines represent the performance without pre-training. We can see that our model benefits mostly from the first 20 pretraining epochs. And after that, the performance improves slightly. Based on this observation, we can conclude that the correlations among different views (i.e., the attribute, item, subsequence, and sequence) can be well-captured by our self-supervised learning approach through pre-training within a small number of epochs. So that the enhanced data representations can improve the performance of sequential recommendation. our model on the recommendation task. To examine the convergence speed on the final recommendation task, we gradually increase the number of epochs for the fine-tuning stage and compare the performance of our model and other baselines. Fig. 6 shows the results on Beauty and Toys datasets. It can be observed that our model converges quickly and achieves the best performance after about 40 epochs. In contrast to our model, the comparison models need more epochs to achieve stable performance. This result shows that our approach can utilize pre-trained parameters to help the model converge faster and achieve better performance.\n\n\nCONCLUSION\n\nIn this paper, we proposed a self-supervised sequential recommendation model S 3 -Rec based on the mutual information maximization (MIM) principle. In our approach, we adopted the self-attentive recommender architecture as the base model and devised four selfsupervised learning objectives to learn the correlations within the raw data. Based on MIM, the four objectives can learn the correlations among attribute, item, segment, and sequence, which enhances the data representations for sequential recommendation. Experimental results have shown that our approach outperforms several competitive baselines.\n\nIn the future, we will investigate how to design other forms of self-supervised optimization objectives. We will also consider applying our approach to more complex recommendation tasks, such as conversational recommendation and multimedia recommendation.\n\n\nAssociated Attribute Prediction (AAP), (2) Masked Item Prediction (MIP), (3) Masked Attribute Prediction (MAP), and (4) Segment Prediction (SP). The embedding layers and bidirectional self-attention blocks are shared by the four pre-training objectives.\n\nFigure 4 :\n4Performance (NDCG@10) comparison w.r.t. different sparsity levels on Sport and Yelp datasets.\n\nFigure 5 :\n5Performance (NDCG@10) comparison w.r.t. different numbers of pre-training epochs on Beauty and Toys datasets.\n\nFigure 6 :\n65.3.5 Convergence Speed Comparison.After obtaining the enhanced representations of the attribute, item, and sequence, we fine-tune Performance tuning (NDCG@10) of our approach and other baselines with the increasing iterations in the finetuning stage.\n\nTable 1 :\n1Statistics of the datasets after preprocessing.Dataset \nMeituan Beauty Sports \nToys \nYelp LastFM \n\n# Users \n13,622 22,363 25,598 19,412 30,431 1,090 \n# Items \n20,062 12,101 18,357 11,924 20,033 3,646 \n# Avg. Actions / User \n54.9 \n8.9 \n8.3 \n8.6 \n10.4 \n48.2 \n# Avg. Actions / Item \n37.3 \n16.4 \n16.1 \n14.1 \n15.8 \n14.4 \n# Actions \n747,827 198,502 296,337 167,597 316,354 52,551 \nSparsity \n99.73% 99.93% 99.95% 99.93% 99.95% 98.68% \n# Attributes \n331 1,221 2,277 1,027 1,001 \n388 \n# Avg. Attribute / Item \n8.8 \n5.1 \n6.0 \n4.3 \n4.8 \n31.5 \n\n5 EXPERIMENT \n5.1 Experimental Setup \n\n5.1.1 Dataset. We conduct experiments on six datasets collected \nfrom four real-world platforms with varying domains and spar-\nsity levels. The statistics of these datasets after preprocessing are \nsummarized in \n\n\nFigure 3: Performance (NDCG@10) comparison of different models enhanced by our self-supervised learning approach on Beauty and Toys datasets.Full Paper Track \nCIKM '20, October 19-23, 2020, Virtual Event, Ireland \n0.15 \n\n0.20 \n\n0.25 \n\n0.30 \n\n0.35 \n\n0.40 \nBeauty \n\nA u to In t C a s e r \nG R U 4 R e c \nG R U 4 R e cF \nS A S R e c \nS A S R e cF S \n3 -R e c \n\n0.15 \n\n0.20 \n\n0.25 \n\n0.30 \n\n0.35 \n\n0.40 \nToys \n\nOriginal \nMIM \n\n100% 80% 60% 40% 20% \n0.10 \n\n0.15 \n\n0.20 \n\n0.25 \n\n0.30 \n\n0.35 \n\n0.40 \n\nSASRec \nSASRecF \n\nFDSA \nS 3 -Rec \n\n(a) Sports \n\n100% 80% 60% 40% 20% \n0.25 \n\n0.30 \n\n0.35 \n\n0.40 \n\n0.45 \n\n0.50 \n\n0.55 \n\nSASRec \nSASRecF \n\nFDSA \nS 3 -Rec \n\n\nhttps://www.meituan.com 2 https://www.yelp.com/dataset 3 https://grouplens.org/datasets/hetrec-2011/\nTo further verify the effectiveness of our method, we have performed the experiments that rank the ground-truth item with all the items as candidates. The complete results are shown on our project website at this link.\nACKNOWLEDGEMENT\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, NAACL-HLT 2019. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL-HLT 2019. 4171-4186.\n\nNoise-Contrastive Estimation of Unnormalized Statistical Models, with Applications to Natural Image Statistics. M Gutmann, A Hyv\u00e4rinen, J. Mach. Learn. Res. 13M. Gutmann and A. Hyv\u00e4rinen. 2012. Noise-Contrastive Estimation of Unnor- malized Statistical Models, with Applications to Natural Image Statistics. J. Mach. Learn. Res. 13 (2012), 307-361.\n\nSession-based Recommendations with Recurrent Neural Networks. B Hidasi, A Karatzoglou, L Baltrunas, D Tikk, B. Hidasi, A. Karatzoglou, L. Baltrunas, and D. Tikk. 2016. Session-based Recom- mendations with Recurrent Neural Networks. In ICLR 2016.\n\nParallel Recurrent Neural Network Architectures for Feature-rich Session-based Recommendations. B Hidasi, M Quadrana, A Karatzoglou, D Tikk, RecSys. B. Hidasi, M. Quadrana, A. Karatzoglou, and D. Tikk. 2016. Parallel Recurrent Neural Network Architectures for Feature-rich Session-based Recommendations. In RecSys 2016. 241-248.\n\nLearning deep representations by mutual information estimation and maximization. R D Hjelm, A Fedorov, S Lavoie-Marchildon, K Grewal, P Bachman, A Trischler, Y Bengio, ICLR. R. D. Hjelm, A. Fedorov, S. Lavoie-Marchildon, K. Grewal, P. Bachman, A. Trischler, and Y. Bengio. 2019. Learning deep representations by mutual in- formation estimation and maximization. In ICLR 2019.\n\nTaxonomy-Aware Multi-Hop Reasoning Networks for Sequential Recommendation. J Huang, Z Ren, W X Zhao, G He, J.-R Wen, D Dong, WSDM 2019. J. Huang, Z. Ren, W. X. Zhao, G. He, J.-R. Wen, and D. Dong. 2019. Taxonomy- Aware Multi-Hop Reasoning Networks for Sequential Recommendation. In WSDM 2019. 573-581.\n\nImproving Sequential Recommendation with Knowledge-Enhanced Memory Networks. J Huang, W X Zhao, H Dou, J.-R Wen, E Y Chang, SIGIR 2018. J. Huang, W. X. Zhao, H. Dou, J.-R. Wen, and E. Y. Chang. 2018. Improving Sequential Recommendation with Knowledge-Enhanced Memory Networks. In SIGIR 2018. 505-514.\n\nSelf-Attentive Sequential Recommendation. W.-C Kang, J J Mcauley, ICDM 2018. W.-C. Kang and J. J. McAuley. 2018. Self-Attentive Sequential Recommendation. In ICDM 2018. 197-206.\n\nAdam: A Method for Stochastic Optimization. D P Kingma, J Ba, ICLR. D. P. Kingma and J. Ba. 2015. Adam: A Method for Stochastic Optimization. In ICLR 2015.\n\nA Mutual Information Maximization Perspective of Language Representation Learning. L Kong, C De Masson D&apos;autume, L Yu, W Ling, Z Dai, D Yogatama, 2020L. Kong, C. de Masson d'Autume, L. Yu, W. Ling, Z. Dai, and D. Yogatama. 2020. A Mutual Information Maximization Perspective of Language Representation Learning. In ICLR 2020.\n\nSelf-Organization in a Perceptual Network. R Linsker, IEEE Computer. 21R. Linsker. 1988. Self-Organization in a Perceptual Network. IEEE Computer 21, 3 (1988), 105-117.\n\nAn efficient framework for learning sentence representations. L Logeswaran, H Lee, L. Logeswaran and H. Lee. 2018. An efficient framework for learning sentence representations. In ICLR 2018.\n\nHierarchical Gating Networks for Sequential Recommendation. C Ma, P Kang, X Liu, KDD 2019. C. Ma, P. Kang, and X. Liu. 2019. Hierarchical Gating Networks for Sequential Recommendation. In KDD 2019. 825-833.\n\nImage-Based Recommendations on Styles and Substitutes. J J Mcauley, C Targett, Q Shi, A Van Den, Hengel, SIGIR 2015. J. J. McAuley, C. Targett, Q. Shi, and A. van den Hengel. 2015. Image-Based Recommendations on Styles and Substitutes. In SIGIR 2015. 43-52.\n\nDistributed Representations of Words and Phrases and their Compositionality. T Mikolov, I Sutskever, K Chen, G S Corrado, J Dean, NeurIPS 2013. T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. 2013. Distributed Representations of Words and Phrases and their Compositionality. In NeurIPS 2013. 3111-3119.\n\nTranslation-based factorization machines for sequential recommendation. R Pasricha, J J Mcauley, RecSys. R. Pasricha and J. J. McAuley. 2018. Translation-based factorization machines for sequential recommendation. In RecSys 2018. 63-71.\n\nPersonalizing Session-based Recommendations with Hierarchical Recurrent Neural Networks. M Quadrana, A Karatzoglou, B Hidasi, P Cremonesi, RecSys 2017. M. Quadrana, A. Karatzoglou, B. Hidasi, and P. Cremonesi. 2017. Personalizing Session-based Recommendations with Hierarchical Recurrent Neural Networks. In RecSys 2017. 130-137.\n\nRepeatNet: A Repeat Aware Neural Recommendation Machine for Session-Based Recommendation. Pengjie Ren, Zhumin Chen, Jing Li, Zhaochun Ren, Jun Ma, Maarten De Rijke, AAAI 2019. Pengjie Ren, Zhumin Chen, Jing Li, Zhaochun Ren, Jun Ma, and Maarten de Rijke. 2019. RepeatNet: A Repeat Aware Neural Recommendation Machine for Session-Based Recommendation. In AAAI 2019. 4806-4813.\n\nSequential Recommendation with Self-Attentive Multi-Adversarial Network. R Ren, Z Liu, Y Li, W X Zhao, H Wang, B Ding, J.-R Wen, SIGIR 2020. R. Ren, Z. Liu, Y. Li, W. X. Zhao, H. Wang, B. Ding, and J.-R. Wen. 2020. Sequential Recommendation with Self-Attentive Multi-Adversarial Network. In SIGIR 2020. 89-98.\n\nFactorization Machines. S Rendle, ICDM 2010. S. Rendle. 2010. Factorization Machines. In ICDM 2010. 995-1000.\n\nFactorizing personalized Markov chains for next-basket recommendation. S Rendle, C Freudenthaler, L Schmidt-Thieme, WWW 2010. S. Rendle, C. Freudenthaler, and L. Schmidt-Thieme. 2010. Factorizing personal- ized Markov chains for next-basket recommendation. In WWW 2010. 811-820.\n\nAutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks. W Song, C Shi, Z Xiao, Z Duan, Y Xu, M Zhang, J Tang, CIKM 2019. W. Song, C. Shi, Z. Xiao, Z. Duan, Y. Xu, M. Zhang, and J. Tang. 2019. AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks. In CIKM 2019. 1161-1170.\n\nBERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer. F Sun, J Liu, J Wu, C Pei, X Lin, W Ou, P Jiang, CIKM 2019. F. Sun, J. Liu, J. Wu, C. Pei, X. Lin, W. Ou, and P. Jiang. 2019. BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer. In CIKM 2019. 1441-1450.\n\nPersonalized Top-N Sequential Recommendation via Convolutional Sequence Embedding. J Tang, K Wang, WSDM 2018. J. Tang and K. Wang. 2018. Personalized Top-N Sequential Recommendation via Convolutional Sequence Embedding. In WSDM 2018. 565-573.\n\nRepresentation Learning with Contrastive Predictive Coding. A Van Den Oord, Y Li, O Vinyals, arXiv:1807.03748A. van den Oord, Y. Li, and O. Vinyals. 2018. Representation Learning with Contrastive Predictive Coding. CoRR abs/1807.03748 (2018). arXiv:1807.03748\n\nAttention is All you Need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. 2017. Attention is All you Need. In NeurIPS 2017. 5998-6008.\n\nSelf-Supervised Reinforcement Learning for Recommender Systems. Xin Xin, Alexandros Karatzoglou, Ioannis Arapakis, Joemon M Jose, SIGIR 2020. Xin Xin, Alexandros Karatzoglou, Ioannis Arapakis, and Joemon M. Jose. 2020. Self-Supervised Reinforcement Learning for Recommender Systems. In SIGIR 2020. 931-940.\n\nQAInfomax: Learning Robust Question Answering System by Mutual Information Maximization. Y.-T Yeh, Y.-N Chen, EMNLP-IJCNLP 2019. Y.-T. Yeh and Y.-N. Chen. 2019. QAInfomax: Learning Robust Question Answering System by Mutual Information Maximization. In EMNLP-IJCNLP 2019. 3368-3373.\n\nFeature-level Deeper Self-Attention Network for Sequential Recommendation. T Zhang, P Zhao, Y Liu, V S Sheng, J Xu, D Wang, G Liu, X Zhou, IJCAI 2019. T. Zhang, P. Zhao, Y. Liu, V. S. Sheng, J. Xu, D. Wang, G. Liu, and X. Zhou. 2019. Feature-level Deeper Self-Attention Network for Sequential Recommendation. In IJCAI 2019. 4320-4326.\n\nImproving Conversational Recommender Systems via Knowledge Graph based Semantic Fusion. Kun Zhou, Wayne Xin Zhao, Shuqing Bian, Yuanhang Zhou, Ji-Rong Wen, Jingsong Yu, KDD 2020. Kun Zhou, Wayne Xin Zhao, Shuqing Bian, Yuanhang Zhou, Ji-Rong Wen, and Jingsong Yu. 2020. Improving Conversational Recommender Systems via Knowl- edge Graph based Semantic Fusion. In KDD 2020.\n", "annotations": {"author": "[{\"end\":238,\"start\":178},{\"end\":299,\"start\":239},{\"end\":480,\"start\":300},{\"end\":559,\"start\":481},{\"end\":593,\"start\":560},{\"end\":608,\"start\":594},{\"end\":624,\"start\":609},{\"end\":819,\"start\":625},{\"end\":880,\"start\":820},{\"end\":941,\"start\":881},{\"end\":1122,\"start\":942},{\"end\":1178,\"start\":1123},{\"end\":1190,\"start\":1179},{\"end\":1205,\"start\":1191},{\"end\":1221,\"start\":1206},{\"end\":1305,\"start\":1222}]", "publisher": null, "author_last_name": "[{\"end\":186,\"start\":182},{\"end\":247,\"start\":243},{\"end\":314,\"start\":310},{\"end\":490,\"start\":487},{\"end\":570,\"start\":566},{\"end\":607,\"start\":602},{\"end\":623,\"start\":619},{\"end\":636,\"start\":633},{\"end\":828,\"start\":824},{\"end\":889,\"start\":885},{\"end\":956,\"start\":952},{\"end\":1132,\"start\":1129},{\"end\":1189,\"start\":1185},{\"end\":1204,\"start\":1199},{\"end\":1220,\"start\":1216},{\"end\":1233,\"start\":1230}]", "author_first_name": "[{\"end\":181,\"start\":178},{\"end\":242,\"start\":239},{\"end\":305,\"start\":300},{\"end\":309,\"start\":306},{\"end\":486,\"start\":481},{\"end\":565,\"start\":560},{\"end\":601,\"start\":594},{\"end\":618,\"start\":609},{\"end\":632,\"start\":625},{\"end\":823,\"start\":820},{\"end\":884,\"start\":881},{\"end\":947,\"start\":942},{\"end\":951,\"start\":948},{\"end\":1128,\"start\":1123},{\"end\":1184,\"start\":1179},{\"end\":1198,\"start\":1191},{\"end\":1215,\"start\":1206},{\"end\":1229,\"start\":1222}]", "author_affiliation": "[{\"end\":237,\"start\":188},{\"end\":298,\"start\":249},{\"end\":385,\"start\":316},{\"end\":479,\"start\":387},{\"end\":558,\"start\":515},{\"end\":724,\"start\":655},{\"end\":818,\"start\":726},{\"end\":879,\"start\":830},{\"end\":940,\"start\":891},{\"end\":1027,\"start\":958},{\"end\":1121,\"start\":1029},{\"end\":1177,\"start\":1134},{\"end\":1304,\"start\":1235}]", "title": "[{\"end\":135,\"start\":1},{\"end\":1440,\"start\":1306}]", "venue": "[{\"end\":1530,\"start\":1442}]", "abstract": "[{\"end\":3389,\"start\":1815}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3871,\"start\":3868},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3873,\"start\":3871},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3876,\"start\":3873},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3879,\"start\":3876},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3882,\"start\":3879},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3933,\"start\":3930},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3935,\"start\":3933},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3938,\"start\":3935},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3941,\"start\":3938},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4135,\"start\":4132},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4178,\"start\":4174},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4213,\"start\":4210},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4469,\"start\":4466},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4471,\"start\":4469},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4474,\"start\":4471},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5072,\"start\":5068},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5075,\"start\":5072},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5305,\"start\":5302},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5307,\"start\":5305},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5310,\"start\":5307},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5737,\"start\":5734},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5740,\"start\":5737},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6860,\"start\":6857},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6863,\"start\":6860},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6866,\"start\":6863},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6869,\"start\":6866},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7300,\"start\":7297},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9367,\"start\":9363},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9571,\"start\":9568},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9573,\"start\":9571},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9576,\"start\":9573},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9640,\"start\":9637},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9821,\"start\":9818},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9842,\"start\":9839},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9844,\"start\":9842},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9874,\"start\":9870},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9895,\"start\":9891},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9927,\"start\":9923},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9994,\"start\":9991},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9997,\"start\":9994},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10000,\"start\":9997},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10146,\"start\":10142},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10268,\"start\":10264},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10614,\"start\":10611},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10616,\"start\":10614},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10619,\"start\":10616},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11165,\"start\":11162},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11195,\"start\":11192},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11198,\"start\":11195},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11504,\"start\":11501},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11544,\"start\":11540},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11582,\"start\":11579},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11585,\"start\":11582},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11588,\"start\":11585},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11686,\"start\":11682},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11765,\"start\":11762},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11788,\"start\":11784},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11828,\"start\":11824},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14466,\"start\":14462},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14469,\"start\":14466},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14472,\"start\":14469},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14530,\"start\":14527},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14800,\"start\":14796},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14900,\"start\":14897},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15760,\"start\":15757},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15762,\"start\":15760},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15764,\"start\":15762},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":15767,\"start\":15764},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15916,\"start\":15913},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15919,\"start\":15916},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16714,\"start\":16711},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16905,\"start\":16902},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17079,\"start\":17075},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":17082,\"start\":17079},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20642,\"start\":20639},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20893,\"start\":20890},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20896,\"start\":20893},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":23447,\"start\":23444},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25773,\"start\":25770},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":25775,\"start\":25773},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":25778,\"start\":25775},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":28120,\"start\":28116},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":29173,\"start\":29169},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29190,\"start\":29186},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":29935,\"start\":29931},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":29950,\"start\":29947},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30368,\"start\":30365},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":30371,\"start\":30368},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":30497,\"start\":30493},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":30511,\"start\":30507},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":30875,\"start\":30871},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":30878,\"start\":30875},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31326,\"start\":31322},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":32042,\"start\":32038},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":32045,\"start\":32042},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":32382,\"start\":32378},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":32385,\"start\":32382},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":32505,\"start\":32502},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":32508,\"start\":32505},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":32511,\"start\":32508},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":32885,\"start\":32882},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":32887,\"start\":32885},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":33342,\"start\":33338},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":33442,\"start\":33438},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":33547,\"start\":33544},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":33714,\"start\":33710},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":33879,\"start\":33876},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":34150,\"start\":34146},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":34282,\"start\":34279},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":34519,\"start\":34515},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":35123,\"start\":35120},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":35627,\"start\":35624},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":41618,\"start\":41617}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":45867,\"start\":45612},{\"attributes\":{\"id\":\"fig_2\"},\"end\":45974,\"start\":45868},{\"attributes\":{\"id\":\"fig_3\"},\"end\":46097,\"start\":45975},{\"attributes\":{\"id\":\"fig_4\"},\"end\":46362,\"start\":46098},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":47159,\"start\":46363},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":47809,\"start\":47160}]", "paragraph": "[{\"end\":3883,\"start\":3405},{\"end\":4333,\"start\":3885},{\"end\":4618,\"start\":4335},{\"end\":5590,\"start\":4620},{\"end\":7090,\"start\":5592},{\"end\":8290,\"start\":7092},{\"end\":8616,\"start\":8292},{\"end\":8733,\"start\":8618},{\"end\":9216,\"start\":8735},{\"end\":10557,\"start\":9263},{\"end\":11545,\"start\":10586},{\"end\":12471,\"start\":11547},{\"end\":12631,\"start\":12489},{\"end\":13477,\"start\":12653},{\"end\":13803,\"start\":13479},{\"end\":14213,\"start\":13839},{\"end\":14554,\"start\":14270},{\"end\":15035,\"start\":14629},{\"end\":15279,\"start\":15037},{\"end\":15331,\"start\":15330},{\"end\":15714,\"start\":15333},{\"end\":16070,\"start\":15740},{\"end\":16715,\"start\":16072},{\"end\":17143,\"start\":16717},{\"end\":17306,\"start\":17158},{\"end\":17823,\"start\":17327},{\"end\":17890,\"start\":17844},{\"end\":18011,\"start\":17997},{\"end\":18066,\"start\":18053},{\"end\":18877,\"start\":18188},{\"end\":19356,\"start\":18903},{\"end\":19583,\"start\":19478},{\"end\":19788,\"start\":19650},{\"end\":19840,\"start\":19835},{\"end\":20024,\"start\":19891},{\"end\":20241,\"start\":20026},{\"end\":20385,\"start\":20335},{\"end\":20897,\"start\":20387},{\"end\":21066,\"start\":20919},{\"end\":21296,\"start\":21110},{\"end\":21590,\"start\":21334},{\"end\":21976,\"start\":21631},{\"end\":22701,\"start\":21978},{\"end\":23040,\"start\":22817},{\"end\":24525,\"start\":23080},{\"end\":24708,\"start\":24610},{\"end\":24956,\"start\":24760},{\"end\":25488,\"start\":25001},{\"end\":25644,\"start\":25579},{\"end\":26065,\"start\":25690},{\"end\":27191,\"start\":26075},{\"end\":27433,\"start\":27316},{\"end\":27795,\"start\":27490},{\"end\":27931,\"start\":27823},{\"end\":28625,\"start\":27941},{\"end\":28800,\"start\":28701},{\"end\":29112,\"start\":28815},{\"end\":29895,\"start\":29114},{\"end\":30441,\"start\":29897},{\"end\":30984,\"start\":30443},{\"end\":31218,\"start\":30986},{\"end\":31510,\"start\":31220},{\"end\":31714,\"start\":31512},{\"end\":31909,\"start\":31716},{\"end\":32168,\"start\":31911},{\"end\":33110,\"start\":32192},{\"end\":33207,\"start\":33131},{\"end\":33329,\"start\":33209},{\"end\":33424,\"start\":33331},{\"end\":33530,\"start\":33426},{\"end\":33698,\"start\":33532},{\"end\":33863,\"start\":33700},{\"end\":34012,\"start\":33865},{\"end\":34136,\"start\":34014},{\"end\":34263,\"start\":34138},{\"end\":34372,\"start\":34265},{\"end\":34503,\"start\":34374},{\"end\":34701,\"start\":34505},{\"end\":34927,\"start\":34729},{\"end\":35376,\"start\":34929},{\"end\":35942,\"start\":35378},{\"end\":36072,\"start\":35967},{\"end\":36867,\"start\":36074},{\"end\":38098,\"start\":36869},{\"end\":38686,\"start\":38100},{\"end\":38788,\"start\":38707},{\"end\":39189,\"start\":38808},{\"end\":39920,\"start\":39191},{\"end\":40440,\"start\":39922},{\"end\":41280,\"start\":40447},{\"end\":41990,\"start\":41282},{\"end\":42980,\"start\":42053},{\"end\":44732,\"start\":43041},{\"end\":45354,\"start\":44747},{\"end\":45611,\"start\":45356}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14269,\"start\":14214},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14628,\"start\":14555},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15329,\"start\":15280},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19419,\"start\":19357},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19477,\"start\":19419},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19649,\"start\":19584},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19834,\"start\":19789},{\"attributes\":{\"id\":\"formula_8\"},\"end\":19890,\"start\":19841},{\"attributes\":{\"id\":\"formula_9\"},\"end\":20292,\"start\":20242},{\"attributes\":{\"id\":\"formula_10\"},\"end\":20334,\"start\":20292},{\"attributes\":{\"id\":\"formula_11\"},\"end\":21109,\"start\":21067},{\"attributes\":{\"id\":\"formula_12\"},\"end\":22773,\"start\":22702},{\"attributes\":{\"id\":\"formula_13\"},\"end\":22816,\"start\":22773},{\"attributes\":{\"id\":\"formula_14\"},\"end\":24609,\"start\":24526},{\"attributes\":{\"id\":\"formula_15\"},\"end\":24759,\"start\":24709},{\"attributes\":{\"id\":\"formula_16\"},\"end\":25578,\"start\":25489},{\"attributes\":{\"id\":\"formula_17\"},\"end\":25689,\"start\":25645},{\"attributes\":{\"id\":\"formula_18\"},\"end\":27315,\"start\":27192},{\"attributes\":{\"id\":\"formula_19\"},\"end\":27489,\"start\":27434},{\"attributes\":{\"id\":\"formula_20\"},\"end\":28700,\"start\":28626}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":30983,\"start\":30976},{\"end\":36036,\"start\":36029},{\"end\":39455,\"start\":39448}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3403,\"start\":3391},{\"attributes\":{\"n\":\"2\"},\"end\":9261,\"start\":9219},{\"attributes\":{\"n\":\"2.2\"},\"end\":10584,\"start\":10560},{\"attributes\":{\"n\":\"3\"},\"end\":12487,\"start\":12474},{\"attributes\":{\"n\":\"3.1\"},\"end\":12651,\"start\":12634},{\"attributes\":{\"n\":\"3.2\"},\"end\":13837,\"start\":13806},{\"attributes\":{\"n\":\"4\"},\"end\":15738,\"start\":15717},{\"attributes\":{\"n\":\"4.2\"},\"end\":17156,\"start\":17146},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":17325,\"start\":17309},{\"end\":17842,\"start\":17826},{\"end\":17910,\"start\":17893},{\"end\":17927,\"start\":17913},{\"end\":17952,\"start\":17930},{\"end\":17973,\"start\":17955},{\"end\":17995,\"start\":17976},{\"end\":18035,\"start\":18014},{\"end\":18051,\"start\":18038},{\"end\":18186,\"start\":18069},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":18901,\"start\":18880},{\"attributes\":{\"n\":\"4.2.3\"},\"end\":20917,\"start\":20900},{\"attributes\":{\"n\":\"4.3\"},\"end\":21332,\"start\":21299},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":21629,\"start\":21593},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":23078,\"start\":23043},{\"attributes\":{\"n\":\"4.3.3\"},\"end\":24999,\"start\":24959},{\"end\":26073,\"start\":26068},{\"attributes\":{\"n\":\"4.4\"},\"end\":27821,\"start\":27798},{\"end\":27939,\"start\":27934},{\"attributes\":{\"n\":\"4.4.2\"},\"end\":28813,\"start\":28803},{\"attributes\":{\"n\":\"5.1.2\"},\"end\":32190,\"start\":32171},{\"attributes\":{\"n\":\"5.1.3\"},\"end\":33129,\"start\":33113},{\"attributes\":{\"n\":\"5.1.4\"},\"end\":34727,\"start\":34704},{\"attributes\":{\"n\":\"5.2\"},\"end\":35965,\"start\":35945},{\"attributes\":{\"n\":\"5.3\"},\"end\":38705,\"start\":38689},{\"attributes\":{\"n\":\"5.3.1\"},\"end\":38806,\"start\":38791},{\"end\":40445,\"start\":40443},{\"attributes\":{\"n\":\"5.3.3\"},\"end\":42051,\"start\":41993},{\"attributes\":{\"n\":\"5.3.4\"},\"end\":43039,\"start\":42983},{\"attributes\":{\"n\":\"6\"},\"end\":44745,\"start\":44735},{\"end\":45879,\"start\":45869},{\"end\":45986,\"start\":45976},{\"end\":46109,\"start\":46099},{\"end\":46373,\"start\":46364}]", "table": "[{\"end\":47159,\"start\":46422},{\"end\":47809,\"start\":47303}]", "figure_caption": "[{\"end\":45867,\"start\":45614},{\"end\":45974,\"start\":45881},{\"end\":46097,\"start\":45988},{\"end\":46362,\"start\":46111},{\"end\":46422,\"start\":46375},{\"end\":47303,\"start\":47162}]", "figure_ref": "[{\"end\":16764,\"start\":16758},{\"end\":18212,\"start\":18204},{\"end\":39217,\"start\":39211},{\"end\":39739,\"start\":39731},{\"end\":41352,\"start\":41346},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":42510,\"start\":42504},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":43487,\"start\":43481},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":44343,\"start\":44337}]", "bib_author_first_name": "[{\"end\":48229,\"start\":48228},{\"end\":48242,\"start\":48238},{\"end\":48251,\"start\":48250},{\"end\":48258,\"start\":48257},{\"end\":48568,\"start\":48567},{\"end\":48579,\"start\":48578},{\"end\":48868,\"start\":48867},{\"end\":48878,\"start\":48877},{\"end\":48893,\"start\":48892},{\"end\":48906,\"start\":48905},{\"end\":49149,\"start\":49148},{\"end\":49159,\"start\":49158},{\"end\":49171,\"start\":49170},{\"end\":49186,\"start\":49185},{\"end\":49464,\"start\":49463},{\"end\":49466,\"start\":49465},{\"end\":49475,\"start\":49474},{\"end\":49486,\"start\":49485},{\"end\":49507,\"start\":49506},{\"end\":49517,\"start\":49516},{\"end\":49528,\"start\":49527},{\"end\":49541,\"start\":49540},{\"end\":49835,\"start\":49834},{\"end\":49844,\"start\":49843},{\"end\":49851,\"start\":49850},{\"end\":49853,\"start\":49852},{\"end\":49861,\"start\":49860},{\"end\":49870,\"start\":49866},{\"end\":49877,\"start\":49876},{\"end\":50140,\"start\":50139},{\"end\":50149,\"start\":50148},{\"end\":50151,\"start\":50150},{\"end\":50159,\"start\":50158},{\"end\":50169,\"start\":50165},{\"end\":50176,\"start\":50175},{\"end\":50178,\"start\":50177},{\"end\":50410,\"start\":50406},{\"end\":50418,\"start\":50417},{\"end\":50420,\"start\":50419},{\"end\":50588,\"start\":50587},{\"end\":50590,\"start\":50589},{\"end\":50600,\"start\":50599},{\"end\":50784,\"start\":50783},{\"end\":50792,\"start\":50791},{\"end\":50819,\"start\":50818},{\"end\":50825,\"start\":50824},{\"end\":50833,\"start\":50832},{\"end\":50840,\"start\":50839},{\"end\":51076,\"start\":51075},{\"end\":51265,\"start\":51264},{\"end\":51279,\"start\":51278},{\"end\":51455,\"start\":51454},{\"end\":51461,\"start\":51460},{\"end\":51469,\"start\":51468},{\"end\":51658,\"start\":51657},{\"end\":51660,\"start\":51659},{\"end\":51671,\"start\":51670},{\"end\":51682,\"start\":51681},{\"end\":51689,\"start\":51688},{\"end\":51939,\"start\":51938},{\"end\":51950,\"start\":51949},{\"end\":51963,\"start\":51962},{\"end\":51971,\"start\":51970},{\"end\":51973,\"start\":51972},{\"end\":51984,\"start\":51983},{\"end\":52253,\"start\":52252},{\"end\":52265,\"start\":52264},{\"end\":52267,\"start\":52266},{\"end\":52508,\"start\":52507},{\"end\":52520,\"start\":52519},{\"end\":52535,\"start\":52534},{\"end\":52545,\"start\":52544},{\"end\":52846,\"start\":52839},{\"end\":52858,\"start\":52852},{\"end\":52869,\"start\":52865},{\"end\":52882,\"start\":52874},{\"end\":52891,\"start\":52888},{\"end\":52903,\"start\":52896},{\"end\":53200,\"start\":53199},{\"end\":53207,\"start\":53206},{\"end\":53214,\"start\":53213},{\"end\":53220,\"start\":53219},{\"end\":53222,\"start\":53221},{\"end\":53230,\"start\":53229},{\"end\":53238,\"start\":53237},{\"end\":53249,\"start\":53245},{\"end\":53462,\"start\":53461},{\"end\":53620,\"start\":53619},{\"end\":53630,\"start\":53629},{\"end\":53647,\"start\":53646},{\"end\":53913,\"start\":53912},{\"end\":53921,\"start\":53920},{\"end\":53928,\"start\":53927},{\"end\":53936,\"start\":53935},{\"end\":53944,\"start\":53943},{\"end\":53950,\"start\":53949},{\"end\":53959,\"start\":53958},{\"end\":54256,\"start\":54255},{\"end\":54263,\"start\":54262},{\"end\":54270,\"start\":54269},{\"end\":54276,\"start\":54275},{\"end\":54283,\"start\":54282},{\"end\":54290,\"start\":54289},{\"end\":54296,\"start\":54295},{\"end\":54588,\"start\":54587},{\"end\":54596,\"start\":54595},{\"end\":54809,\"start\":54808},{\"end\":54825,\"start\":54824},{\"end\":54831,\"start\":54830},{\"end\":55037,\"start\":55036},{\"end\":55048,\"start\":55047},{\"end\":55059,\"start\":55058},{\"end\":55069,\"start\":55068},{\"end\":55082,\"start\":55081},{\"end\":55091,\"start\":55090},{\"end\":55093,\"start\":55092},{\"end\":55102,\"start\":55101},{\"end\":55112,\"start\":55111},{\"end\":55356,\"start\":55353},{\"end\":55372,\"start\":55362},{\"end\":55393,\"start\":55386},{\"end\":55410,\"start\":55404},{\"end\":55412,\"start\":55411},{\"end\":55690,\"start\":55686},{\"end\":55700,\"start\":55696},{\"end\":55957,\"start\":55956},{\"end\":55966,\"start\":55965},{\"end\":55974,\"start\":55973},{\"end\":55981,\"start\":55980},{\"end\":55983,\"start\":55982},{\"end\":55992,\"start\":55991},{\"end\":55998,\"start\":55997},{\"end\":56006,\"start\":56005},{\"end\":56013,\"start\":56012},{\"end\":56308,\"start\":56305},{\"end\":56320,\"start\":56315},{\"end\":56324,\"start\":56321},{\"end\":56338,\"start\":56331},{\"end\":56353,\"start\":56345},{\"end\":56367,\"start\":56360},{\"end\":56381,\"start\":56373}]", "bib_author_last_name": "[{\"end\":48236,\"start\":48230},{\"end\":48248,\"start\":48243},{\"end\":48255,\"start\":48252},{\"end\":48268,\"start\":48259},{\"end\":48576,\"start\":48569},{\"end\":48589,\"start\":48580},{\"end\":48875,\"start\":48869},{\"end\":48890,\"start\":48879},{\"end\":48903,\"start\":48894},{\"end\":48911,\"start\":48907},{\"end\":49156,\"start\":49150},{\"end\":49168,\"start\":49160},{\"end\":49183,\"start\":49172},{\"end\":49191,\"start\":49187},{\"end\":49472,\"start\":49467},{\"end\":49483,\"start\":49476},{\"end\":49504,\"start\":49487},{\"end\":49514,\"start\":49508},{\"end\":49525,\"start\":49518},{\"end\":49538,\"start\":49529},{\"end\":49548,\"start\":49542},{\"end\":49841,\"start\":49836},{\"end\":49848,\"start\":49845},{\"end\":49858,\"start\":49854},{\"end\":49864,\"start\":49862},{\"end\":49874,\"start\":49871},{\"end\":49882,\"start\":49878},{\"end\":50146,\"start\":50141},{\"end\":50156,\"start\":50152},{\"end\":50163,\"start\":50160},{\"end\":50173,\"start\":50170},{\"end\":50184,\"start\":50179},{\"end\":50415,\"start\":50411},{\"end\":50428,\"start\":50421},{\"end\":50597,\"start\":50591},{\"end\":50603,\"start\":50601},{\"end\":50789,\"start\":50785},{\"end\":50816,\"start\":50793},{\"end\":50822,\"start\":50820},{\"end\":50830,\"start\":50826},{\"end\":50837,\"start\":50834},{\"end\":50849,\"start\":50841},{\"end\":51084,\"start\":51077},{\"end\":51276,\"start\":51266},{\"end\":51283,\"start\":51280},{\"end\":51458,\"start\":51456},{\"end\":51466,\"start\":51462},{\"end\":51473,\"start\":51470},{\"end\":51668,\"start\":51661},{\"end\":51679,\"start\":51672},{\"end\":51686,\"start\":51683},{\"end\":51697,\"start\":51690},{\"end\":51705,\"start\":51699},{\"end\":51947,\"start\":51940},{\"end\":51960,\"start\":51951},{\"end\":51968,\"start\":51964},{\"end\":51981,\"start\":51974},{\"end\":51989,\"start\":51985},{\"end\":52262,\"start\":52254},{\"end\":52275,\"start\":52268},{\"end\":52517,\"start\":52509},{\"end\":52532,\"start\":52521},{\"end\":52542,\"start\":52536},{\"end\":52555,\"start\":52546},{\"end\":52850,\"start\":52847},{\"end\":52863,\"start\":52859},{\"end\":52872,\"start\":52870},{\"end\":52886,\"start\":52883},{\"end\":52894,\"start\":52892},{\"end\":52912,\"start\":52904},{\"end\":53204,\"start\":53201},{\"end\":53211,\"start\":53208},{\"end\":53217,\"start\":53215},{\"end\":53227,\"start\":53223},{\"end\":53235,\"start\":53231},{\"end\":53243,\"start\":53239},{\"end\":53253,\"start\":53250},{\"end\":53469,\"start\":53463},{\"end\":53627,\"start\":53621},{\"end\":53644,\"start\":53631},{\"end\":53662,\"start\":53648},{\"end\":53918,\"start\":53914},{\"end\":53925,\"start\":53922},{\"end\":53933,\"start\":53929},{\"end\":53941,\"start\":53937},{\"end\":53947,\"start\":53945},{\"end\":53956,\"start\":53951},{\"end\":53964,\"start\":53960},{\"end\":54260,\"start\":54257},{\"end\":54267,\"start\":54264},{\"end\":54273,\"start\":54271},{\"end\":54280,\"start\":54277},{\"end\":54287,\"start\":54284},{\"end\":54293,\"start\":54291},{\"end\":54302,\"start\":54297},{\"end\":54593,\"start\":54589},{\"end\":54601,\"start\":54597},{\"end\":54822,\"start\":54810},{\"end\":54828,\"start\":54826},{\"end\":54839,\"start\":54832},{\"end\":55045,\"start\":55038},{\"end\":55056,\"start\":55049},{\"end\":55066,\"start\":55060},{\"end\":55079,\"start\":55070},{\"end\":55088,\"start\":55083},{\"end\":55099,\"start\":55094},{\"end\":55109,\"start\":55103},{\"end\":55123,\"start\":55113},{\"end\":55360,\"start\":55357},{\"end\":55384,\"start\":55373},{\"end\":55402,\"start\":55394},{\"end\":55417,\"start\":55413},{\"end\":55694,\"start\":55691},{\"end\":55705,\"start\":55701},{\"end\":55963,\"start\":55958},{\"end\":55971,\"start\":55967},{\"end\":55978,\"start\":55975},{\"end\":55989,\"start\":55984},{\"end\":55995,\"start\":55993},{\"end\":56003,\"start\":55999},{\"end\":56010,\"start\":56007},{\"end\":56018,\"start\":56014},{\"end\":56313,\"start\":56309},{\"end\":56329,\"start\":56325},{\"end\":56343,\"start\":56339},{\"end\":56358,\"start\":56354},{\"end\":56371,\"start\":56368},{\"end\":56384,\"start\":56382}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":52967399},\"end\":48453,\"start\":48146},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":11583904},\"end\":48803,\"start\":48455},{\"attributes\":{\"id\":\"b2\"},\"end\":49050,\"start\":48805},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":8264369},\"end\":49380,\"start\":49052},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":52055130},\"end\":49757,\"start\":49382},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":59528258},\"end\":50060,\"start\":49759},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":49644765},\"end\":50362,\"start\":50062},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":52127932},\"end\":50541,\"start\":50364},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":6628106},\"end\":50698,\"start\":50543},{\"attributes\":{\"id\":\"b9\"},\"end\":51030,\"start\":50700},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1527671},\"end\":51200,\"start\":51032},{\"attributes\":{\"id\":\"b11\"},\"end\":51392,\"start\":51202},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":195316714},\"end\":51600,\"start\":51394},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1012652},\"end\":51859,\"start\":51602},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":16447573},\"end\":52178,\"start\":51861},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":52086387},\"end\":52416,\"start\":52180},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":10174110},\"end\":52747,\"start\":52418},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":54447414},\"end\":53124,\"start\":52749},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":218763428},\"end\":53435,\"start\":53126},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":17265929},\"end\":53546,\"start\":53437},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":207178809},\"end\":53826,\"start\":53548},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":53100214},\"end\":54156,\"start\":53828},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":119181611},\"end\":54502,\"start\":54158},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":39847715},\"end\":54746,\"start\":54504},{\"attributes\":{\"doi\":\"arXiv:1807.03748\",\"id\":\"b24\"},\"end\":55007,\"start\":54748},{\"attributes\":{\"id\":\"b25\"},\"end\":55287,\"start\":55009},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":219559385},\"end\":55595,\"start\":55289},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":202122788},\"end\":55879,\"start\":55597},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":199465766},\"end\":56215,\"start\":55881},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":220404390},\"end\":56589,\"start\":56217}]", "bib_title": "[{\"end\":48226,\"start\":48146},{\"end\":48565,\"start\":48455},{\"end\":49146,\"start\":49052},{\"end\":49461,\"start\":49382},{\"end\":49832,\"start\":49759},{\"end\":50137,\"start\":50062},{\"end\":50404,\"start\":50364},{\"end\":50585,\"start\":50543},{\"end\":51073,\"start\":51032},{\"end\":51452,\"start\":51394},{\"end\":51655,\"start\":51602},{\"end\":51936,\"start\":51861},{\"end\":52250,\"start\":52180},{\"end\":52505,\"start\":52418},{\"end\":52837,\"start\":52749},{\"end\":53197,\"start\":53126},{\"end\":53459,\"start\":53437},{\"end\":53617,\"start\":53548},{\"end\":53910,\"start\":53828},{\"end\":54253,\"start\":54158},{\"end\":54585,\"start\":54504},{\"end\":55351,\"start\":55289},{\"end\":55684,\"start\":55597},{\"end\":55954,\"start\":55881},{\"end\":56303,\"start\":56217}]", "bib_author": "[{\"end\":48238,\"start\":48228},{\"end\":48250,\"start\":48238},{\"end\":48257,\"start\":48250},{\"end\":48270,\"start\":48257},{\"end\":48578,\"start\":48567},{\"end\":48591,\"start\":48578},{\"end\":48877,\"start\":48867},{\"end\":48892,\"start\":48877},{\"end\":48905,\"start\":48892},{\"end\":48913,\"start\":48905},{\"end\":49158,\"start\":49148},{\"end\":49170,\"start\":49158},{\"end\":49185,\"start\":49170},{\"end\":49193,\"start\":49185},{\"end\":49474,\"start\":49463},{\"end\":49485,\"start\":49474},{\"end\":49506,\"start\":49485},{\"end\":49516,\"start\":49506},{\"end\":49527,\"start\":49516},{\"end\":49540,\"start\":49527},{\"end\":49550,\"start\":49540},{\"end\":49843,\"start\":49834},{\"end\":49850,\"start\":49843},{\"end\":49860,\"start\":49850},{\"end\":49866,\"start\":49860},{\"end\":49876,\"start\":49866},{\"end\":49884,\"start\":49876},{\"end\":50148,\"start\":50139},{\"end\":50158,\"start\":50148},{\"end\":50165,\"start\":50158},{\"end\":50175,\"start\":50165},{\"end\":50186,\"start\":50175},{\"end\":50417,\"start\":50406},{\"end\":50430,\"start\":50417},{\"end\":50599,\"start\":50587},{\"end\":50605,\"start\":50599},{\"end\":50791,\"start\":50783},{\"end\":50818,\"start\":50791},{\"end\":50824,\"start\":50818},{\"end\":50832,\"start\":50824},{\"end\":50839,\"start\":50832},{\"end\":50851,\"start\":50839},{\"end\":51086,\"start\":51075},{\"end\":51278,\"start\":51264},{\"end\":51285,\"start\":51278},{\"end\":51460,\"start\":51454},{\"end\":51468,\"start\":51460},{\"end\":51475,\"start\":51468},{\"end\":51670,\"start\":51657},{\"end\":51681,\"start\":51670},{\"end\":51688,\"start\":51681},{\"end\":51699,\"start\":51688},{\"end\":51707,\"start\":51699},{\"end\":51949,\"start\":51938},{\"end\":51962,\"start\":51949},{\"end\":51970,\"start\":51962},{\"end\":51983,\"start\":51970},{\"end\":51991,\"start\":51983},{\"end\":52264,\"start\":52252},{\"end\":52277,\"start\":52264},{\"end\":52519,\"start\":52507},{\"end\":52534,\"start\":52519},{\"end\":52544,\"start\":52534},{\"end\":52557,\"start\":52544},{\"end\":52852,\"start\":52839},{\"end\":52865,\"start\":52852},{\"end\":52874,\"start\":52865},{\"end\":52888,\"start\":52874},{\"end\":52896,\"start\":52888},{\"end\":52914,\"start\":52896},{\"end\":53206,\"start\":53199},{\"end\":53213,\"start\":53206},{\"end\":53219,\"start\":53213},{\"end\":53229,\"start\":53219},{\"end\":53237,\"start\":53229},{\"end\":53245,\"start\":53237},{\"end\":53255,\"start\":53245},{\"end\":53471,\"start\":53461},{\"end\":53629,\"start\":53619},{\"end\":53646,\"start\":53629},{\"end\":53664,\"start\":53646},{\"end\":53920,\"start\":53912},{\"end\":53927,\"start\":53920},{\"end\":53935,\"start\":53927},{\"end\":53943,\"start\":53935},{\"end\":53949,\"start\":53943},{\"end\":53958,\"start\":53949},{\"end\":53966,\"start\":53958},{\"end\":54262,\"start\":54255},{\"end\":54269,\"start\":54262},{\"end\":54275,\"start\":54269},{\"end\":54282,\"start\":54275},{\"end\":54289,\"start\":54282},{\"end\":54295,\"start\":54289},{\"end\":54304,\"start\":54295},{\"end\":54595,\"start\":54587},{\"end\":54603,\"start\":54595},{\"end\":54824,\"start\":54808},{\"end\":54830,\"start\":54824},{\"end\":54841,\"start\":54830},{\"end\":55047,\"start\":55036},{\"end\":55058,\"start\":55047},{\"end\":55068,\"start\":55058},{\"end\":55081,\"start\":55068},{\"end\":55090,\"start\":55081},{\"end\":55101,\"start\":55090},{\"end\":55111,\"start\":55101},{\"end\":55125,\"start\":55111},{\"end\":55362,\"start\":55353},{\"end\":55386,\"start\":55362},{\"end\":55404,\"start\":55386},{\"end\":55419,\"start\":55404},{\"end\":55696,\"start\":55686},{\"end\":55707,\"start\":55696},{\"end\":55965,\"start\":55956},{\"end\":55973,\"start\":55965},{\"end\":55980,\"start\":55973},{\"end\":55991,\"start\":55980},{\"end\":55997,\"start\":55991},{\"end\":56005,\"start\":55997},{\"end\":56012,\"start\":56005},{\"end\":56020,\"start\":56012},{\"end\":56315,\"start\":56305},{\"end\":56331,\"start\":56315},{\"end\":56345,\"start\":56331},{\"end\":56360,\"start\":56345},{\"end\":56373,\"start\":56360},{\"end\":56386,\"start\":56373}]", "bib_venue": "[{\"end\":48284,\"start\":48270},{\"end\":48610,\"start\":48591},{\"end\":48865,\"start\":48805},{\"end\":49199,\"start\":49193},{\"end\":49554,\"start\":49550},{\"end\":49893,\"start\":49884},{\"end\":50196,\"start\":50186},{\"end\":50439,\"start\":50430},{\"end\":50609,\"start\":50605},{\"end\":50781,\"start\":50700},{\"end\":51099,\"start\":51086},{\"end\":51262,\"start\":51202},{\"end\":51483,\"start\":51475},{\"end\":51717,\"start\":51707},{\"end\":52003,\"start\":51991},{\"end\":52283,\"start\":52277},{\"end\":52568,\"start\":52557},{\"end\":52923,\"start\":52914},{\"end\":53265,\"start\":53255},{\"end\":53480,\"start\":53471},{\"end\":53672,\"start\":53664},{\"end\":53975,\"start\":53966},{\"end\":54313,\"start\":54304},{\"end\":54612,\"start\":54603},{\"end\":54806,\"start\":54748},{\"end\":55034,\"start\":55009},{\"end\":55429,\"start\":55419},{\"end\":55724,\"start\":55707},{\"end\":56030,\"start\":56020},{\"end\":56394,\"start\":56386}]"}}}, "year": 2023, "month": 12, "day": 17}