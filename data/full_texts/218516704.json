{"id": 218516704, "updated": "2023-10-06 16:25:46.075", "metadata": {"title": "TAG : Type Auxiliary Guiding for Code Comment Generation", "authors": "[{\"first\":\"Ruichu\",\"last\":\"Cai\",\"middle\":[]},{\"first\":\"Zhihao\",\"last\":\"Liang\",\"middle\":[]},{\"first\":\"Boyan\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"zijian\",\"last\":\"li\",\"middle\":[]},{\"first\":\"Yuexing\",\"last\":\"Hao\",\"middle\":[]},{\"first\":\"Yao\",\"last\":\"Chen\",\"middle\":[]}]", "venue": "ACL", "journal": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Existing leading code comment generation approaches with the structure-to-sequence framework ignores the type information of the interpretation of the code, e.g., operator, string, etc. However, introducing the type information into the existing framework is non-trivial due to the hierarchical dependence among the type information. In order to address the issues above, we propose a Type Auxiliary Guiding encoder-decoder framework for the code comment generation task which considers the source code as an N-ary tree with type information associated with each node. Specifically, our framework is featured with a Type-associated Encoder and a Type-restricted Decoder which enables adaptive summarization of the source code. We further propose a hierarchical reinforcement learning method to resolve the training difficulties of our proposed framework. Extensive evaluations demonstrate the state-of-the-art performance of our framework with both the auto-evaluated metrics and case studies.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2005.02835", "mag": "3034716028", "acl": "2020.acl-main.27", "pubmed": null, "pubmedcentral": null, "dblp": "conf/acl/CaiLXLHC20", "doi": "10.18653/v1/2020.acl-main.27"}}, "content": {"source": {"pdf_hash": "8f579e9f8a7739cdca3c376a252235281940405e", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclweb.org/anthology/2020.acl-main.27.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://www.aclweb.org/anthology/2020.acl-main.27.pdf", "status": "HYBRID"}}, "grobid": {"id": "0557264d96bb00a7c865cbcaf0ae081f026fac99", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/8f579e9f8a7739cdca3c376a252235281940405e.txt", "contents": "\nTAG : Type Auxiliary Guiding for Code Comment Generation\nAssociation for Computational LinguisticsCopyright Association for Computational LinguisticsJuly 5 -10, 2020. 2020\n\nRuichu Cai cairuichu@gmail.com \nSchool of Computer Science\nGuangdong University of Technology\nChina\n\nZhihao Liang \nSchool of Computer Science\nGuangdong University of Technology\nChina\n\nBoyan Xu \nSchool of Computer Science\nGuangdong University of Technology\nChina\n\nZijian Li \nYuexing Hao \nSchool of Computer Science\nGuangdong University of Technology\nChina\n\nRutgers University New Brunswick\nUSA\n\nYao Chen yao.chen@adsc-create.edu.sg \nAdvanced Digital Sciences Center\nSingapore\n\nTAG : Type Auxiliary Guiding for Code Comment Generation\n\nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics\nthe 58th Annual Meeting of the Association for Computational LinguisticsAssociation for Computational LinguisticsJuly 5 -10, 2020. 2020291\nExisting leading code comment generation approaches with the structure-to-sequence framework ignores the type information of the interpretation of the code, e.g., operator, string, etc. However, introducing the type information into the existing framework is non-trivial due to the hierarchical dependence among the type information. In order to address the issues above, we propose a Type Auxiliary Guiding encoder-decoder framework for the code comment generation task which considers the source code as an N-ary tree with type information associated with each node. Specifically, our framework is featured with a Typeassociated Encoder and a Type-restricted Decoder which enables adaptive summarization of the source code. We further propose a hierarchical reinforcement learning method to resolve the training difficulties of our proposed framework. Extensive evaluations demonstrate the state-of-the-art performance of our framework with both the auto-evaluated metrics and case studies.\n\nIntroduction\n\nThe comment for the programming code is critical for software development, which is crucial to the further maintenance of the project codebase with significant improvement of the readability (Aggarwal et al., 2002;Tenny, 1988). Code comment generation aims to automatically transform program code into natural language with the help of deep learning technologies to boost the efficiency of the code development.\n\nExisting leading approaches address the code comment generation task under the structure-tosequence (Struct2Seq) framework with an encoderdecoder manner by taking advantage of the inherent structural properties of the code. For instance, existing solutions leverage the syntactic structure of abstract syntax trees (AST) or parse trees from * * Corresponding author source code have shown significant improvement to the quality of the generated comments (Liang and Zhu, 2018;Alon et al., 2018;Hu et al., 2018;Wan et al., 2018); Solutions representing source code as graphs have also shown high-quality comment generation abilities by taking advantage of extracting the structural information of the codes (Xu et al., 2018a,b;Fernandes et al., 2018).\n\nAlthough promising results were reported, we observe that the information of the node type in the code is not considered in these aforementioned Struct2Seq based solutions. The lack of such essential information lead to the following common limitations: 1) Losing the accuracy for encoding the source code with the same structure but has different types. As shown in Fig. 1(a), a Tree-LSTM (Tai et al., 2015) encoder is illustrated to extract the structural information, the two subtrees of the code 'Select' and 'Compare' in the dashed box have the same structure but different types, with the ignorance of the type information, the traditional encoders illustrate the same set of neural network parameters to encode the tree, which leads to an inaccurate generation of the comment. 2) Losing both the efficiency and accuracy for searching the large vocabulary in the decoding procedure, especially for the out-of-vocabulary (OOV) words that exist in the source code but not in the target dictionary. As shown in the Fig. 1(a), missing the type of 'ACL' node usually results in an unknown word 'UNK' in the generated comments. Thus, the key to tackle these limitations is efficiently utilizing the node type information in the encoder-decoder framework.\n\nTo well utilize the type information, we propose a Type Auxiliary Guiding (TAG) encoder-decoder framework. As shown in Fig. 1(b), in the encoding phase, we devise a Type-associated encoder to encode the type information in the encoding of the N-ary tree. In the decoding phase, we facilitate the generation of the comments with the help of type information in a two-stage process naming operation selection and word selection to reduce the searching space for the comment output and avoid the out-of-vocabulary situation. Considering that there is no ground-truth labels for the operation selection results in the two-stage generation process, we further devised a Hierarchical Reinforcement Learning (HRL) method to resolve the training of our framework. Our proposed framework makes the following contributions:\n\n\u2022 An adaptive Type-associated encoder which can summarize the information according to the node type; \u2022 A Type-restricted decoder with a two-stage process to reduce the search space for the code comment generation; \u2022 A hierarchical reinforcement learning approach that jointly optimizes the operation selection and word selection stages.\n\n\nRelated Work\n\nCode comment generation frameworks generate natural language from source code snippets, e.g. SQL, lambda-calculus expression and other programming languages. As a specified natural language generation task, the mainstream approaches could be categorized into textual based method and structure-based method. The textual-based method is the most straightforward solution which only considers the sequential text information of the source code. For instance, Movshovitz-Attias and Cohen (2013) uses topic models and n-grams to predict comments with given source code snippets; Iyer et al. (2016) presents a language model Code-NN using LSTM networks with attention to generate descriptions about C# and SQL; Allamanis et al. (2016) predicts summarization of code snippets using a convolutional attention network; Wong and Mooney (2007) presents a learning system to generate sentences from lambda-calculus expressions by inverting semantic parser into statistical machine translation methods.\n\nThe structure-based methods take the structure information into consideration and outperform the textual-based methods. Alon et al. (2018) processes a code snippet into the set of compositional paths in its AST and uses attention mechanism to select the relevant paths during the decoding. Hu et al. (2018) presents a Neural Machine Translation based model which takes AST node sequences as input and captures the structure and semantic of Java codes. Wan et al. (2018) combines the syntactic level representation with lexical level representation by adopting a tree-to-sequence (Eriguchi et al., 2016) based model. Xu et al. (2018b) considers a SQL query as a directed graph and adopts a graph-to-sequence model to encode the global structure information.\n\nCopying mechanism is utilized to address the OOV issues in the natural language generation tasks by reusing parts of the inputs instead of selecting words from the target vocabulary. See et al. (2017) presents a hybrid pointer-generator network by introducing pointer network (Vinyals et al., 2015) into a standard sequence-to-sequence (Seq2Seq) model for abstractive text summarization. COPYNET from Gu et al. (2016) incorporates the conventional copying mechanism into Seq2Seq model and selectively copy input segments to the output sequence. In addition, Ling et al. (2016) uses the copying mechanism to copy strings from the code.\n\nOur targeted task is considered as the opposite process of natural language to programming code (NL-to-code) task. So some of the NL-to-code solutions are also taken as our references. Dong and Lapata (2016) distinguishes types of nodes in the logical form by whether nodes have child nodes. Yin and Neubig (2017); Rabinovich et al. (2017); Xu et al. (2018a) take the types of AST nodes into account and generate the corresponding programming codes. Cai et al. (2018) \n\ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd\ufffd\ufffd Word Selection Stage Word Distribution (Copying) Y N \ufffd \ufffd \ufffd = \ufffd\ufffd\ufffd\ufffd? <start> \ufffd \ufffd\ufffd\ufffd \ufffd \ufffd \ufffd \ufffd\ufffd \ufffd \ufffd\ufffd \ufffd \ufffd\ufffd \ufffd \ufffd\ufffd \ufffd \ufffd \ufffd \ufffd \u2026 \ufffd Neural Network Pointwise Operation \ufffd \ufffd (\ufffd \ufffd ) \ufffd \ufffd\ufffd , \ufffd \ufffd\ufffd \ufffd \ufffd\ufffd , \ufffd \ufffd\ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd\ufffd \ufffd \ufffd\ufffd \u2026 \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd \ufffd\n\nOperation Distribution\n\nEncoding Process in Cell\n\nType-associated Encoder information of the code, our solution differs from the existing method with a Type-associated Encoder that encodes the type information during the substructure summarization and a Type-restricted Decoder that can reduce search space for the code comment generation. In addition, two improvements are developed according to our objectives. First, we design a type-restricted copying mechanism to reduce the difficulty of extracting complex grammar structure from the source code. Second, we use a hierarchical reinforcement learning methods to train the model in our framework to learn to select from either copy or other actions, the details will be presented in Section 3.\n\n\nModel Overview\n\nWe first make the necessary definition and formulation for the input data and the code comment generation problem for our Type Auxiliary Guiding (TAG) encoder-decoder framework. Definition 1 Token-type-tree.\n\nToken-type-tree T x,\u03c4 represents the source code with the node set V , which is a rooted N-ary tree.\nAnd V = {v 1 , v 2 , .., v |V | } denotes a partial order nodes set satisfying v 1 v 2 ..., v |V | . Let internal node v j = {x j , \u03c4 j },\nwhere x j denotes the token sequence and \u03c4 j denotes a type from grammar type set T .\n\nToken-type-tree can be easily constructed from token information of the original source code and type information of its AST or parse tree. According to Definition 1, we formulate the code comment generation task as follows.\n\n\nFormulation 1 Code Comment Generation with\n\nToken-type-tree as the Input. Let S denote training dataset and labeled sample (T x,\u03c4 , y) \u2208 S, where T x,\u03c4 is the input token-type-tree, y = (y 1 , y 2 , \u00b7 \u00b7 \u00b7 , y M ) is the ground truth comment with M words. The task of code comment generation is to design a model which takes the unlabeled sample T x,\u03c4 as input and predicts the output as its comment, denoted as y.\n\nOur framework follows the encoder-decoder manner, and consists of the revised two major components, namely the Type-associated Encoder and Type-restricted Decoder. As shown in Fig. 2.\n\nThe Type-associated Encoder, as shown in Fig. 2, recursively takes the token-type-tree T x,\u03c4 as input, and maintains the semantic information of the source code in the hidden states. Instead of using the same parameter sets to learn the whole tokentype-tree, Type-associated Encoder utilizes multiple sets of parameters to learn the different type of nodes. The parameters of the cells are adaptively invoked according to the type of the current node during the processing of the input token-type-tree. Such a procedure enables the structured semantic representation to contain the type information of the source code.\n\nThe Type-restricted Decoder, as shown in the right part of Figure 2, takes the original toke-typetree T x,\u03c4 and its semantic representation from encoder as input and generates the corresponding comment. Different from conventional decoders which generate output only based on the target dictionary, our Type-restricted Decoder considers both input code to the encoder and target dictionary as the source of output. Attention mechanism is employed to compute an attention vector which is used to generate the output words through a two-stage process: (1) Determine either to copy from the original token-type-tree or to generate from the current hidden state according to the distribution of the operation.\n\n(2) If the copying operation is selected, the words are copied from the selected node from the token-type-tree T x,\u03c4 with restricted types; otherwise, the candidate word will be selected from the target dictionary. The above two-stage process is guided by the type which is extracted from the hidden state of encoder with the help of attention mechanism. Such a process enables adaptive switching between copying and generation processes, and not only reduces the search space of the generation process but also addresses the OOV problem with the copying mechanism.\n\nAlthough the proposed framework provides an efficient solution with the utilization of the type information in the code, training obstacles are raised accordingly: (1) No training labels are provided for the operation selection stage. (2) There is a mismatch between the evaluation metric and the objective function. Thus, we further devised an HRL method to train our TAG model. In the HRL training, the TAG model feeds back the evaluation metric as the learning reward to train the two-stage sampling process without relying on the groundtruth label of operation selection stage.\n\n\nType-associated Encoder\n\nThe encoder network aims to learn a semantic representation of the input source code. The key challenge is to provide distinct summarization for the sub-trees with the same structure but different semantics. As shown in the Type-associated Encoder in Fig. 1, the blue and red dashed blocks have the same 3-ary substructure. The sub-tree in the blue box shares the same sub-structure with the tree in the red box, which is usually falsely processed by the same cell in a vanilla Tree-LSTM. By introducing the type information, the semantics of the two subtrees are distinguished from each other.\n\nOur proposed Type-associated Encoder is designed as a variant N -ary Tree-LSTM. Instead of directly inputting type information as features into the encoder for learning, we integrate the type information as the index of the learning parameter sets of the encoder network. More specifically, differ-ent sets of parameters are defined through different types, which provides a more detailed summarization of the input. As is shown in Fig. 1(b), the two sub-trees in our proposed Type-associated Encoder are distinguished by the type information. The tree contains N ordered child nodes, which are indexed from 1 to N . For the j-th node, the hidden state and memory cell of its k-th child node is denoted as h jk and c jk , respectively. In order to effectively capture the type information, we set W \u03c4 j and b \u03c4 j to be the weight and bias of the j-th node, and U \u03c4 jk be the weight of the k-th child of the j-th node. The transition equation of the variant N -ary Tree-LSTM is shown as follow:\nij = \u03c3 W (i) \u03c4 j \u03c6 (xj) + N l=1 U (i) \u03c4 jl h jl + b (i) \u03c4 j ,(1)f jk = \u03c3 W (f ) \u03c4 jk \u03c6 (xj) + N l=1 U (f ) \u03c4 jl,k h jl + b (f ) \u03c4 jk , (2) oj = \u03c3 W (o) \u03c4 j \u03c6 (xj) + N l=1 U (o) \u03c4 jl h jl + b (o) \u03c4 j , (3) uj = tanh W (u) \u03c4 j \u03c6 (xj) + N l=1 U (u) \u03c4 jl h jl + b (u) \u03c4 j ,(4)cj = ij uj + N l=1 f jl c jl ,(5)\nhj = oj tanh (cj) ,\n\nWe employ the forget gate (Tai et al., 2015) for the Tree-LSTM, the parameters for the k-th child of the j-th node's is denoted as f jk . U \u03c4 jl,k is used to represent the weight of the type for the l-th child of the j-th node in the k-th forget gate. The major difference between our variants and the traditional Tree-LSTM is that the parameter set (W \u03c4 , U \u03c4 , b \u03c4 ) are specified for each type \u03c4 .\n\n\nType-restricted Decoder\n\nFollowing with the Type-associated Encoder, we propose a Type-restricted Decoder for the decoding phase, which incorporates the type information into its two-stage generation process. First of all, an attention mechanism is adopted in the decoding phase which takes hidden states from the encoder as input and generates the attention vector. The resulted attention vector is used as input to the following two-stage process, named operation selection stage and word selection stage, respectively. The operation selection stage selects between generation operation and copying operation for the following word selection stage. If the generation operation is selected, the predicted word will be generated from the targeted dictionary. If the copying operation is selected, then a type-restricted copying mechanism is enabled to restrict the search space by masking down the illegal grammar types. Furthermore, a copying decay strategy is illustrated to solve the issue of repetitively focusing on specific nodes caused by the attention mechanism. The details of each part are given below.\n\nAttention Mechanism: The encoder extracts the semantic representation as the hidden state of the rooted nodes, denoted as h r , which are used to initialize the hidden state of the decoder, z 0 \u2190 h r . At time step m, given output y m\u22121 and the hidden state of the decoder z m\u22121 at last time step m \u2212 1, the hidden state z m is recursively calculated by the LSTM cells in the decoder,\nz m = LST M (z m\u22121 , y m\u22121 ).(7)\nThe attention vector q is calculate with:\n\u03b1 mj = exp h j z m |Vx| j=1 exp h j z m , q m = |Vx| j=1 \u03b1 mj h j , q m = tanh (W q [ q, z m ]) ,(8)\nwhere W q is the parameters of the attention mechanism. The attention vector contains the token and type information, which is further facilitated in the following operation selection and word selection stages. Operation Selection Stage: Operation Selection Stage determines either using the copying operation or the generation operation to select the words based on the attention vector and hidden states from the encoder. Specifically, given the attention vector q m at time step m, Operation Selection Stage estimates the conditional probabilities as the distribution of the operation p(\u00e2 m |\u0177 <m ; T x,\u03c4 ), wher\u00ea a m \u2208 {0, 1} and 0 and 1 represents the copy and the generation operations, respectively. A fully connected layer followed by a softmax is implemented to compute the distribution of the operations.\n\np(\u00e2 m |\u0177 <m ; T x,\u03c4 ) = sof tmax(W s q m ), (9) The W s in the Eq. 9 is the trainable parameters. Since there is no ground-truth label for operation selection, we employ an HRL method to jointly train the operation selection stage and the following stage, the details are provided in Section 6.\n\nWord Selection Stage: Word Selection Stage also contains two branches. The selection between them is determined by the previous stage. If the generation operation is selected in the Operation Selectoin Stage, the attention vector will be fed into a softmax layer to predict the distribution of the target word, formulated as p(y m |\u00e2 m = 1,\u0177 <m ; T x,\u03c4 ) = sof tmax (W g q m ) , (10) where W g is the trainable parameters of the output layer. Otherwise, if the copy operation is selected, we employ the dot-product score function to calculate score vector s m of the hidden state of the node and the attention vector. Similarly, score vector s m will be fed into a softmax layer to predict the distribution of the input word, noted as:\ns m = h 1 , h 2 , \u00b7 \u00b7 \u00b7 , h |Vx| q m p(y m |\u00e2 m = 0;\u0177 <m ; T x,\u03c4 ) = sof tmax (s m ) .(11)\nOne step further, to filter out the illegally copied candidates, we involve a grammar-type based mask vector d m \u2208 R |Vx| at each decoding step m. Each dimension of d m corresponds to each node of the token-type-tree. If the mask of the node in tokentype-tree indicates the node should be filtered out, then the corresponding dimension is set as negative infinite. Otherwise, it is set to 0. Thus, the restricted copying stage is formulated as p(y m |\u00e2 m = 0,\u0177 <m ; T x,\u03c4 ) = sof tmax (s m + d m ) .\n\n(12) The word distribution of the two branches is represented with a softmax over input words or target dictionary words in Eq. 10 and Eq. 12. At each time step, the word with the highest probability in the word distribution will be selected.\n\nCopying Decay Strategy: Similar to the conventional copying mechanism, we also use the attention vector as a pointer to guide the copying process. The type-restricted copying mechanism tends to pay more attention to specific nodes, resulting in the ignorance of other available nodes, which makes certain copied tokens repeatedly active in a short distance in a single generated text, lead to a great redundancy of the content.\n\nSo we design a Copying Decay Strategy to smoothly penalize certain probabilities of outstand-ingly copied nodes. We define a copy time-based decay rate \u03bb mi for the i-th tree node x i in the m-th decoding step. If one node is copied in time step m, its decay rate is initialized as 1. In the next time step m + 1, it is scaled by a coefficient \u03b3 \u2208 (0, 1):\n\u03bb m+1,i = \u03b3\u03bb m,i(13)\nThe overall formulation for the Type-restricted Decoder is:\np(y m |\u00e2 m = 0,\u0177 <m ; T x,\u03c4 ) = sof tmax (s m + d m ) (1 \u2212 \u03bb m )(14)\n6 Hierarchical Reinforcement Learning\n\nThere remain two challenges to train our proposed framework, which are 1) the lack of ground truth label for the operation selection stage and 2) the mismatch between the evaluation metric and objective function. Although it is possible to train our framework by using the maximum likelihood estimation (MLE) method which constructs pseudo-labels or marginalize all the operations in the operation selection stage (Jia and Liang, 2016;Gu et al., 2016), the loss-evaluation mismatch between MLE loss for training and non-differentiable evaluation metrics for testing lead to inconsistent results (Keneshloo et al., 2019;Ranzato et al., 2015). To address these issues, we propose a Hierarchical Reinforcement Learning method to train the operation selection stage and word selection stage jointly. We set the objective of the HRL as maximizing the expectation of the reward R(\u0177, y) between the predicted sequence\u0177 and the ground-truth sequence y, denoted as L r . It could be formulated as a function of the input tuple {T x,\u03c4 , y} as,\nL r = 1 |S| (Tx,\u03c4 ,y)\u2208S E\u0177 \u223cp(\u0177|Tx,\u03c4 ) [R(\u0177, y)] = 1 |S| (Tx,\u03c4 ,y)\u2208S \u0177\u2208Y p(\u0177|T x,\u03c4 )R(\u0177, y),(15)\nHere, Y is the set of the candidate comment sequences. The reward R((y), y) is the nondifferentiable evaluation metric, i.e., BLEU and ROUGE (details are in Section 7). The expectation in Eq. (15) is approximated via sampling\u0177 from the distribution p(\u0177|T x,\u03c4 ). The procedure of sampling\u0177 from p(\u0177|T x,\u03c4 ) is composed of the subprocedures of sampling\u0177 m from p(\u0177 m |\u0177 <m ; T x,\u03c4 ) in each decoding step m.\n\nAs mentioned above, the predicted sequence\u0177 comes from the two branches of Word Selection Stage, depending on the Operation Selection Stage. a is defined as the action of the Operation selection stage. After involving the action a m in time step m, Eq. (15) can be constructed by the joint distribution of the two stages: \n\nAs shown in Eq. (16), the model finally selects the word\u0177 m in time step m from the word distribution conditioned on\u0177 <m , T x,\u03c4 and the operation a m which is determined in the operation selection stage. In other words, there is a hierarchical dependency between the word selection stage and the operation selection stage.\n\nAs mentioned above, Y represents the space for all candidate comments, which is too large to practically maximize L r . Since decoding is constructed via sampling from p(\u0177 m |\u00e2 m ,\u0177 <m ; T x,\u03c4 ) and p(\u00e2 m |\u0177 <m ; T x,\u03c4 ), We adopt the Gumbel-Max solution (Gumbel, 1954) for the following sampling procedure:\na m \u223c p(\u00e2 m |\u0177 <m ; T x,\u03c4 ), y m \u223c p(\u0177 m |\u00e2 m ,\u0177 <m ; T x,\u03c4 ).(17)\nThrough the maximum sampling step M, Eq. (16) could be further approximated as the following equation:L\nr = 1 |S| y\u2208S R(\u0177, y)(18)\nThe objective in Eq. (18) remains another challenge: for the entire sequence\u0177, there is only a final reward R(\u0177, y) available for model training, which is a sparse reward and leads to inefficient training of the model. So we introduce reward shaping (Ng et al., 1999) strategy to provide intermediate rewards to proceed towards the training goal, which adopts the accumulation of the intermediate rewards to update the model.\n\nTo further stabilize the HRL training process, we combine our HRL objective with the maximumlikelihood estimation(MLE) function according to Wu et al. ( , 2016; Li et al. (2017); :\nL e = 1 |S| (Tx,\u03c4 ,y)\u2208S \u0177\u2208Y logp(y|T x,\u03c4 ) L = \u00b5L e + (1 \u2212 \u00b5)L r ,(19)\nwhere \u00b5 is a variational controlling factor that controls the trade-off between maximum-likelihood estimation function and our HRL objective. In the current training step tr, \u00b5 varies according to the training step tt as follows:\n\u00b5 = 1 \u2212 tr tt(20)\n7 Evaluation and Analysis 7.1 Experimental Setup\n\n\nDatasets\n\nWe evaluate our TAG framework on three widely used benchmark data sets, which are WikiSQL (Zhong et al., 2017), ATIS (Dong and Lapata, 2016) and CoNaLa (Yin et al., 2018 We transfer the SQL queries of WikiSQL into ASTs with 6 types according to the Abstract Syntax Description Language (ASDL) grammar, where the ASDL grammar for SQL queries is proposed in Yin and Neubig (2017). We transfer the lambdacalculus logical forms of ATIS to tree structure with 7 types according to the method proposed in Dong and Lapata (2016). The python snippets of CoNaLa are transformed into ASTs with 20 types, following the official ASDL grammar of python 1 . The data of the ASTs of these datasets is shown in Table 1, where the maximum depth of ASTs (Max-Tree-Depth), the maximum number of child 1 https://docs.python.org/3.5/library/ast.html nodes in ASTs (Max-Child-Count) and the average number of tree nodes in ASTs (Avg-Tree-Node-Count) are shown. \n\n\nBaselines Frameworks\n\nWe choose the representative designs for code comment generation as our baselines for comparison. Code-NN (Iyer et al., 2016) is chosen because of it is the first model to transform the source code into sentences. Pointer Generator (See et al., 2017) (P-G) is a seq2seq based model with a standard copying mechanism. In addition, we choose the attention based Tree-to-Sequence (Tree2Seq) model proposed by Eriguchi et al. (2016). Moreover, we also add the copying mechanism into Tree2Seq model as another baseline (T2S+CP). We choose Graph-to-Sequence (Graph2Seq) (Xu et al., 2018b) as a graph-based baseline for comparison. Since the authors have not released the code for datapreprocessing, we convert the tree-structured representation for the source code of SQL data into directed graphs for our replication.\n\n\nHyperparameters\n\nCode-NN uses embedding size and hidden size both as 400, and applies random uniform initializer with 0.35 initialized weight, and adopts stochastic gradient descent algorithm to train the model with a learning rate at 0.5. P-G uses 128 embedding size, 256 hidden size and applies random uniform initializer with 0.02 initialized weights for initialization and Adam optimizer to train the model with 0.001 learning rate. Graph2Seq uses 100 embedding size, 200 hidden size and applies the truncated normal initializer for initialization. Adam optimizer is used to train the model with a 0.001 learning rate. We use the Xavier initializer (Glorot and Bengio, 2010) to initialize the parameters of our proposed TAG framework. The size of embeddings is equivalent to the dimensions of LSTM states and hidden layers, which is 64 for ATIS and CoNaLa and 128 for WikiSQL. TAG is trained using the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 0.001. In order to reduce the size of the vocabulary, low-frequency words are not kept in both the  vocabulary for the source codes and the vocabulary for target comments. Specifically, the minimum threshold frequency for WikiSQL and ATIS is set as 4 while for CoNaLa it is set as 2. The hyperparameters of Tree2Seq and T2S+CP is equivalent to ours. The minibatch size of all the baseline models and ours are set to 32.\n\n\nEvaluation Metric\n\nWe illustrate the n-gram based BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004) evaluations to evaluate the quality of our generated comments and also use them to set the reward in the HRL based training. Specifically, BLEU-4, ROUGE-2 and ROUGE-L are used to evaluate the performance of our model since they are the most representative evaluation metric for context-based text generation. Table 2 presents the evaluation results of the baseline frameworks and our proposed ones. Since our HRL could be switched to different reward functions, we evaluate both the BLEU oriented and ROUGE oriented training of our framework, denoted as TAG(B) and TAG(R). The results of TAG(B) and TAG(R) varies slightly compared to each other. However, both of them are significantly higher than all the selected counterparts, which demonstrates the state-of-the-art generation quality of our framework on all the datasets with different programming languages. Specifically, TAG improves over 15% of BLEU-4, over 10% of ROUGE-2 and 6% of ROUGE-L on WikiSQL when compared to T2S+CP, which is the best one among all the baseline target for all the evaluations. For the lambda-calculus related corpus, TAG improves 1.0% of BLEU, 0.2% ROUGE-2 and 0.5% ROUGE-L on ATIS. The performance is more difficult to be improved on ATIS  than the other two corpora due to the great dissimilarity of sub-trees of the lambda-calculus logical forms in it. In terms of the python related corpus, TAG improves 6% of BLEU, 6.4% of ROUGE-2 and 2.2% of ROUGE-L on CoNaLa when compared to the best one in our baselines. The low evaluation score and improvement of CoNaLa are due to the complex grammatical structures and lack of sufficient training samples, i.e., 20 types across only 2174 training samples, which result in an inadequately use of the advantage of our approach. However, our TAG framework still outperforms all the counterparts on these two datasets.\n\n\nResults and Analysis\n\n\nComparison with the Baselines\n\n\nAblation Study\n\nTo investigate the performance of each component in our model, we conduct ablation studies on the development sets. Since all the trends are the same, we omit the results on the other data sets and only present the ones of WikiSQL. The variants of our model are as follows:\n\n\u2022 TAG-TA: remove Type-associated Encoder, use Tree-LSTM instead. \u2022 TAG-MV: remove the mask vector d m .\n\n\u2022 TAG-CD: remove Copying Decay Strategy.\n\n\u2022 TAG-RL replace HRL with MLE, marginalize the actions of the operation selection. The results of the ablation study are given in Table 3. Overall, all the components are necessary to TAG framework and providing important contributions to the final output. When compared to TAG-TA, the high performance of standard TAG Ground-Truth: remove key 'c' from dictionary 'd' Code-NN: remove all keys from a dictionary 'd' P-G: select a string 'c' in have end of a list 'd' Tree2Seq: get a key 'key' one ',' one ',' <unk> Graph2Seq: filter a dictionary of dictionaries from a dictionary 'd' where a dictionary of dictionaries 'd' T2S+CP: find all the values in dictionary 'd' from a dictionary 'd' TAG: remove the key 'c' if a dictionary 'd' benefits from the Type-associated Encoder which adaptively processes the nodes with different types and extracts a better summarization of the source code. The downgraded performance of TAG-MV and TAG-CD indicates the advantages of the typerestricted masking vector and Copying Decay Strategy. These together ensure the accurate execution of the copy and word selection. The comparison of TAG and TAG-RL shows the necessity of the HRL for the training of our framework.\n\n\nCase Study\n\nIn order to show the effectiveness of our framework in a more obvious way, some cases generated by TAG are shown in Table 4. SQL and Python are taken as the targeted programming languages. The comments generated by TAG show great improvements when compared to the baselines. Specifically, for the case in SQL, the keyword \"Otkrytie Area\" is missing in all the baselines but accurately generated by our framework. For the case in Python, the comment generated by TAG is more readable than the others. These cases demonstrate the high quality of the comments generated by our TAG framework.\n\n\nConclusion\n\nIn this paper, we present a Type Auxiliary Guiding encoder-decoder framework for the code comment generation task. Our proposed framework takes full advantage of the type information associated with the code through the well designed Type-associated Encoder and Type-restricted Decoder. In addition, a hierarchical reinforcement learning method is provided for the training of our framework. The ex-perimental results demonstrate significant improvements over state-of-the-art approaches and strong applicable potential in software development. Our proposed framework also verifies the necessity of the type information in the code translation related tasks with a practical framework and good results. As future work, we will extend our framework to more complex contexts by devising efficient learning algorithms.\n\nFigure 1 :\n1Comment generation frameworks. Different types are denoted as different colors and shapes in (b).\n\nFigure 2 :\n2TAG Encoder and Decoder framework.\n\n\nborrows the idea of Automata theory and considers the specific types of SQL grammar in Backus-Naur form (BNF) and generates accurate SQL queries with the help of it.Inspired by the methods considering the type \n\ufffd \ufffd \ns \n\nLSTM \nLSTM \nLSTM \n\u2026 \n\n\u2026 \n\ngen \ncopy \n\nOperation \nSelection \nStage \n\nwhat \nof \nSELECT \nACL \n\nWord \nDistribution \n(Generation) \n\n\n\nTable 2 :\n2Comparisons with baseline models on different test sets.\n\nTable 3 :\n3Ablation study of TAG framework.\n\n\nSQL: SELECT MAX(Capacity) FROMtable WHERE Stadium = \"Otkrytie Arena\" Ground-Truth: What is the maximum capacity of the Otkrytie Arena Stadium ? Code-NN: What is the highest attendance for ? P-G: Who is the % that 's position at 51 ? Tree2Seq: What is the highest capacity at <unk> at arena ? Graph2Seq: What is the highest capacity for arena arena ? T2S+CP: What is the highest capacity for the stadium ? TAG: What is the highest capacity for the stadium of Otkrytie Arena ? Python: i: d [i] for i in d if i != 'c'Code \nComment \n\n\n\nTable 4 :\n4Case study comparisons.\nAcknowledgmentsThis research was supported in part by Natural Science Foundation of China (61876043, 61976052), Natural Science Foundation of Guangdong (2014A030306004, 2014A030308008), Science and Technology Planning Project of Guangzhou (201902010058). Besides, this project is also partly supported by the National Research Foundation, Prime Minister's Office, Singapore under its Campus for Research Excellence and Technological Enterprise (CREATE) programme. This research was also made possible by NPRP grant NPRP10-0208-170408 from the Qatar National Research Fund (a member of Qatar Foundation). The findings herein reflect the work, and are solely the responsibility of the authors.\nAn integrated measure of software maintainability. K Krishan, Yogesh Aggarwal, Jitender Kumar Singh, Chhabra, Annual Reliability and Maintainability Symposium. 2002 Proceedings (Cat. No. 02CH37318). IEEEKrishan K Aggarwal, Yogesh Singh, and Jitender Ku- mar Chhabra. 2002. An integrated measure of soft- ware maintainability. In Annual Reliability and Maintainability Symposium. 2002 Proceedings (Cat. No. 02CH37318), pages 235-241. IEEE.\n\nA convolutional attention network for extreme summarization of source code. Miltiadis Allamanis, Hao Peng, Charles Sutton, International Conference on Machine Learning. Miltiadis Allamanis, Hao Peng, and Charles Sutton. 2016. A convolutional attention network for ex- treme summarization of source code. In Inter- national Conference on Machine Learning, pages 2091-2100.\n\nOmer Levy, and Eran Yahav. Uri Alon, Shaked Brody, arXiv:1808.01400Generating sequences from structured representations of code. 2arXiv preprintUri Alon, Shaked Brody, Omer Levy, and Eran Ya- hav. 2018. code2seq: Generating sequences from structured representations of code. arXiv preprint arXiv:1808.01400.\n\nAn encoderdecoder framework translating natural language to database queries. Ruichu Cai, Boyan Xu, Zhenjie Zhang, Xiaoyan Yang, Zijian Li, Zhihao Liang, Proceedings of the 27th International Joint Conference on Artificial Intelligence. the 27th International Joint Conference on Artificial IntelligenceAAAI PressRuichu Cai, Boyan Xu, Zhenjie Zhang, Xiaoyan Yang, Zijian Li, and Zhihao Liang. 2018. An encoder- decoder framework translating natural language to database queries. In Proceedings of the 27th Inter- national Joint Conference on Artificial Intelligence, pages 3977-3983. AAAI Press.\n\nLanguage to logical form with neural attention. Li Dong, Mirella Lapata, 10.18653/v1/P16-1004Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyLong Papers1Association for Computational LinguisticsLi Dong and Mirella Lapata. 2016. Language to logi- cal form with neural attention. In Proceedings of the 54th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 33-43, Berlin, Germany. Association for Computa- tional Linguistics.\n\nTree-to-sequence attentional neural machine translation. Akiko Eriguchi, Kazuma Hashimoto, Yoshimasa Tsuruoka, 10.18653/v1/P16-1078Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyLong Papers1Association for Computational LinguisticsAkiko Eriguchi, Kazuma Hashimoto, and Yoshimasa Tsuruoka. 2016. Tree-to-sequence attentional neu- ral machine translation. In Proceedings of the 54th Annual Meeting of the Association for Computa- tional Linguistics (Volume 1: Long Papers), pages 823-833, Berlin, Germany. Association for Compu- tational Linguistics.\n\nPatrick Fernandes, Miltiadis Allamanis, Marc Brockschmidt, arXiv:1811.01824Structured neural summarization. arXiv preprintPatrick Fernandes, Miltiadis Allamanis, and Marc Brockschmidt. 2018. Structured neural summariza- tion. arXiv preprint arXiv:1811.01824.\n\nUnderstanding the difficulty of training deep feedforward neural networks. Xavier Glorot, Yoshua Bengio, Proceedings of the thirteenth international conference on artificial intelligence and statistics. the thirteenth international conference on artificial intelligence and statisticsXavier Glorot and Yoshua Bengio. 2010. Understand- ing the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth interna- tional conference on artificial intelligence and statis- tics, pages 249-256.\n\nIncorporating copying mechanism in sequence-to-sequence learning. Jiatao Gu, Zhengdong Lu, Hang Li, O K Victor, Li, 10.18653/v1/P16-1154Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyAssociation for Computational Linguistics1Jiatao Gu, Zhengdong Lu, Hang Li, and Victor O.K. Li. 2016. Incorporating copying mechanism in sequence-to-sequence learning. In Proceedings of the 54th Annual Meeting of the Association for Com- putational Linguistics (Volume 1: Long Papers), pages 1631-1640, Berlin, Germany. Association for Computational Linguistics.\n\nStatistical theory of extreme values and some practical applications: a series of lectures. Emil Julius Gumbel, US Government Printing Office. 33Emil Julius Gumbel. 1954. Statistical theory of ex- treme values and some practical applications: a se- ries of lectures, volume 33. US Government Print- ing Office.\n\nDeep code comment generation. Xing Hu, Ge Li, Xin Xia, David Lo, Zhi Jin, Proceedings of the 26th Conference on Program Comprehension. the 26th Conference on Program ComprehensionACMXing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep code comment generation. In Proceedings of the 26th Conference on Program Comprehension, pages 200-210. ACM.\n\nSummarizing source code using a neural attention model. Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Luke Zettlemoyer, 10.18653/v1/P16-1195Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyAssociation for Computational Linguistics1Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016. Summarizing source code using a neural attention model. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2073-2083, Berlin, Germany. Association for Computational Linguistics.\n\nData recombination for neural semantic parsing. Robin Jia, Percy Liang, 10.18653/v1/P16-1002Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, GermanyLong Papers1Association for Computational LinguisticsRobin Jia and Percy Liang. 2016. Data recombination for neural semantic parsing. In Proceedings of the 54th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 12-22, Berlin, Germany. Association for Computa- tional Linguistics.\n\nDeep reinforcement learning for sequence-to-sequence models. Yaser Keneshloo, Tian Shi, Naren Ramakrishnan, Chandan K Reddy, IEEE Transactions on Neural Networks and Learning Systems. Yaser Keneshloo, Tian Shi, Naren Ramakrishnan, and Chandan K Reddy. 2019. Deep reinforcement learn- ing for sequence-to-sequence models. IEEE Trans- actions on Neural Networks and Learning Systems.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.\n\nAdversarial learning for neural dialogue generation. Jiwei Li, Will Monroe, Tianlin Shi, S\u00e9bastien Jean, Alan Ritter, Dan Jurafsky, 10.18653/v1/D17-1230Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingCopenhagen, DenmarkAssociation for Computational LinguisticsJiwei Li, Will Monroe, Tianlin Shi, S\u00e9bastien Jean, Alan Ritter, and Dan Jurafsky. 2017. Adversarial learning for neural dialogue generation. In Proceed- ings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2157-2169, Copenhagen, Denmark. Association for Computa- tional Linguistics.\n\nAutomatic generation of text descriptive comments for code blocks. Yuding Liang, Kenny Qili Zhu, Thirty-Second AAAI Conference on Artificial Intelligence. Yuding Liang and Kenny Qili Zhu. 2018. Automatic generation of text descriptive comments for code blocks. In Thirty-Second AAAI Conference on Ar- tificial Intelligence.\n\nRouge: A package for automatic evaluation of summaries. Chin-Yew Lin, Text summarization branches out. Chin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74-81.\n\nLatent predictor networks for code generation. Wang Ling, Phil Blunsom, Edward Grefenstette, Karl Moritz Hermann, Tom\u00e1\u0161 Ko\u010disk\u00fd, Fumin Wang, Andrew Senior, 10.18653/v1/P16-1057Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsBerlin, Germany1Long Papers). Association for Computational LinguisticsWang Ling, Phil Blunsom, Edward Grefenstette, Karl Moritz Hermann, Tom\u00e1\u0161 Ko\u010disk\u00fd, Fumin Wang, and Andrew Senior. 2016. Latent predictor networks for code generation. In Proceedings of the 54th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), pages 599-609, Berlin, Germany. Association for Compu- tational Linguistics.\n\nNatural language models for predicting programming comments. Dana Movshovitz, - Attias, William W Cohen, Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics. the 51st Annual Meeting of the Association for Computational LinguisticsSofia, BulgariaShort Papers2Association for Computational LinguisticsDana Movshovitz-Attias and William W. Cohen. 2013. Natural language models for predicting program- ming comments. In Proceedings of the 51st An- nual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 35-40, Sofia, Bulgaria. Association for Computational Lin- guistics.\n\nPolicy invariance under reward transformations: Theory and application to reward shaping. Y Andrew, Daishi Ng, Stuart Harada, Russell, ICML. 99Andrew Y Ng, Daishi Harada, and Stuart Russell. 1999. Policy invariance under reward transforma- tions: Theory and application to reward shaping. In ICML, volume 99, pages 278-287.\n\nBleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, 10.3115/1073083.1073135Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. the 40th Annual Meeting of the Association for Computational LinguisticsPhiladelphia, Pennsylvania, USAAssociation for Computational LinguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Com- putational Linguistics, pages 311-318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.\n\nAbstract syntax networks for code generation and semantic parsing. Maxim Rabinovich, Mitchell Stern, Dan Klein, 10.18653/v1/P17-1105Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaLong Papers1Association for Computational LinguisticsMaxim Rabinovich, Mitchell Stern, and Dan Klein. 2017. Abstract syntax networks for code generation and semantic parsing. In Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1139- 1149, Vancouver, Canada. Association for Computa- tional Linguistics.\n\nAurelio Marc, Sumit Ranzato, Michael Chopra, Wojciech Auli, Zaremba, arXiv:1511.06732Sequence level training with recurrent neural networks. arXiv preprintMarc'Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. 2015. Sequence level train- ing with recurrent neural networks. arXiv preprint arXiv:1511.06732.\n\nGet to the point: Summarization with pointergenerator networks. Abigail See, J Peter, Christopher D Liu, Manning, 10.18653/v1/P17-1099Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaLong Papers1Association for Computational LinguisticsAbigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get to the point: Summarization with pointer- generator networks. In Proceedings of the 55th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073- 1083, Vancouver, Canada. Association for Computa- tional Linguistics.\n\nImproved semantic representations from tree-structured long short-term memory networks. Kai Sheng Tai, Richard Socher, Christopher D Manning, 10.3115/v1/P15-1150Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language ProcessingBeijing, ChinaAssociation for Computational Linguistics1Kai Sheng Tai, Richard Socher, and Christopher D. Manning. 2015. Improved semantic representations from tree-structured long short-term memory net- works. In Proceedings of the 53rd Annual Meet- ing of the Association for Computational Linguistics and the 7th International Joint Conference on Natu- ral Language Processing (Volume 1: Long Papers), pages 1556-1566, Beijing, China. Association for Computational Linguistics.\n\nProgram readability: Procedures versus comments. Ted Tenny, IEEE Transactions on Software Engineering. 149Ted Tenny. 1988. Program readability: Procedures ver- sus comments. IEEE Transactions on Software En- gineering, 14(9):1271-1279.\n\nPointer networks. Oriol Vinyals, Meire Fortunato, Navdeep Jaitly, Advances in Neural Information Processing Systems. Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly. 2015. Pointer networks. In Advances in Neural In- formation Processing Systems, pages 2692-2700.\n\nImproving automatic source code summarization via deep reinforcement learning. Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, Philip S Yu, Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering. the 33rd ACM/IEEE International Conference on Automated Software EngineeringACMYao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, and Philip S Yu. 2018. Im- proving automatic source code summarization via deep reinforcement learning. In Proceedings of the 33rd ACM/IEEE International Conference on Auto- mated Software Engineering, pages 397-407. ACM.\n\nGeneration by inverting a semantic parser that uses statistical machine translation. Yuk Wah Wong, Raymond Mooney, Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference. Rochester, New YorkAssociation for Computational LinguisticsYuk Wah Wong and Raymond Mooney. 2007. Genera- tion by inverting a semantic parser that uses statisti- cal machine translation. In Human Language Tech- nologies 2007: The Conference of the North Amer- ican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 172-179, Rochester, New York. Association for Computational Linguistics.\n\nA study of reinforcement learning for neural machine translation. Lijun Wu, Fei Tian, Tao Qin, Jianhuang Lai, Tie-Yan Liu, 10.18653/v1/D18-1397Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsLijun Wu, Fei Tian, Tao Qin, Jianhuang Lai, and Tie- Yan Liu. 2018a. A study of reinforcement learning for neural machine translation. In Proceedings of the 2018 Conference on Empirical Methods in Nat- ural Language Processing, pages 3612-3621, Brus- sels, Belgium. Association for Computational Lin- guistics.\n\nAdversarial neural machine translation. Lijun Wu, Yingce Xia, Fei Tian, Li Zhao, Tao Qin, Jianhuang Lai, Tie-Yan Liu, Asian Conference on Machine Learning. Lijun Wu, Yingce Xia, Fei Tian, Li Zhao, Tao Qin, Jianhuang Lai, and Tie-Yan Liu. 2018b. Adversar- ial neural machine translation. In Asian Conference on Machine Learning, pages 534-549.\n\nGoogle's neural machine translation system: Bridging the gap between human and machine translation. Yonghui Wu, Mike Schuster, Zhifeng Chen, V Quoc, Mohammad Le, Wolfgang Norouzi, Maxim Macherey, Yuan Krikun, Qin Cao, Klaus Gao, Macherey, arXiv:1609.08144arXiv preprintYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google's neural machine translation system: Bridging the gap between hu- man and machine translation. arXiv preprint arXiv:1609.08144.\n\nSQL-to-text generation with graph-to-sequence model. Kun Xu, Lingfei Wu, Zhiguo Wang, Yansong Feng, Vadim Sheinin, 10.18653/v1/D18-1112Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingBrussels, BelgiumAssociation for Computational LinguisticsKun Xu, Lingfei Wu, Zhiguo Wang, Yansong Feng, and Vadim Sheinin. 2018a. SQL-to-text generation with graph-to-sequence model. In Proceedings of the 2018 Conference on Empirical Methods in Nat- ural Language Processing, pages 931-936, Brus- sels, Belgium. Association for Computational Lin- guistics.\n\nGraph2seq: Graph to sequence learning with attention-based neural networks. Kun Xu, Lingfei Wu, Zhiguo Wang, Yansong Feng, Michael Witbrock, Vadim Sheinin, arXiv:1804.00823arXiv preprintKun Xu, Lingfei Wu, Zhiguo Wang, Yansong Feng, Michael Witbrock, and Vadim Sheinin. 2018b. Graph2seq: Graph to sequence learning with attention-based neural networks. arXiv preprint arXiv:1804.00823.\n\nLearning to mine aligned code and natural language pairs from stack overflow. Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, Graham Neubig, 2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR). IEEEPengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig. 2018. Learning to mine aligned code and natural language pairs from stack overflow. In 2018 IEEE/ACM 15th Interna- tional Conference on Mining Software Repositories (MSR), pages 476-486. IEEE.\n\nA syntactic neural model for general-purpose code generation. Pengcheng Yin, Graham Neubig, 10.18653/v1/P17-1041Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. the 55th Annual Meeting of the Association for Computational LinguisticsVancouver, CanadaAssociation for Computational Linguistics1Pengcheng Yin and Graham Neubig. 2017. A syntactic neural model for general-purpose code generation. In Proceedings of the 55th Annual Meeting of the As- sociation for Computational Linguistics (Volume 1: Long Papers), pages 440-450, Vancouver, Canada. Association for Computational Linguistics.\n\nVictor Zhong, Caiming Xiong, Richard Socher, arXiv:1709.00103Seq2sql: Generating structured queries from natural language using reinforcement learning. arXiv preprintVictor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2sql: Generating structured queries from natural language using reinforcement learning. arXiv preprint arXiv:1709.00103.\n", "annotations": {"author": "[{\"end\":274,\"start\":174},{\"end\":357,\"start\":275},{\"end\":436,\"start\":358},{\"end\":447,\"start\":437},{\"end\":567,\"start\":448},{\"end\":649,\"start\":568}]", "publisher": "[{\"end\":99,\"start\":58},{\"end\":909,\"start\":868}]", "author_last_name": "[{\"end\":184,\"start\":181},{\"end\":287,\"start\":282},{\"end\":366,\"start\":364},{\"end\":446,\"start\":444},{\"end\":459,\"start\":456},{\"end\":576,\"start\":572}]", "author_first_name": "[{\"end\":180,\"start\":174},{\"end\":281,\"start\":275},{\"end\":363,\"start\":358},{\"end\":443,\"start\":437},{\"end\":455,\"start\":448},{\"end\":571,\"start\":568}]", "author_affiliation": "[{\"end\":273,\"start\":206},{\"end\":356,\"start\":289},{\"end\":435,\"start\":368},{\"end\":528,\"start\":461},{\"end\":566,\"start\":530},{\"end\":648,\"start\":606}]", "title": "[{\"end\":57,\"start\":1},{\"end\":706,\"start\":650}]", "venue": "[{\"end\":795,\"start\":708}]", "abstract": "[{\"end\":1927,\"start\":935}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2157,\"start\":2134},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2169,\"start\":2157},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2831,\"start\":2810},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2849,\"start\":2831},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2865,\"start\":2849},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2882,\"start\":2865},{\"end\":3081,\"start\":3061},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3104,\"start\":3081},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3515,\"start\":3497},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6023,\"start\":5989},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6125,\"start\":6107},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6261,\"start\":6238},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6365,\"start\":6343},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6662,\"start\":6644},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6830,\"start\":6814},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6993,\"start\":6976},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7126,\"start\":7103},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7157,\"start\":7140},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7482,\"start\":7465},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7580,\"start\":7558},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7699,\"start\":7683},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7858,\"start\":7840},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8125,\"start\":8103},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8231,\"start\":8210},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8257,\"start\":8233},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8276,\"start\":8259},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8385,\"start\":8368},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":15242,\"start\":15224},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21369,\"start\":21348},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21385,\"start\":21369},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21553,\"start\":21529},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":21574,\"start\":21553},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23390,\"start\":23376},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23893,\"start\":23876},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":24212,\"start\":24194},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":24230,\"start\":24214},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":24724,\"start\":24704},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24754,\"start\":24731},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":24783,\"start\":24766},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":24991,\"start\":24970},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25135,\"start\":25113},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25703,\"start\":25684},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25828,\"start\":25810},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":26006,\"start\":25984},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":26160,\"start\":26142},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":27071,\"start\":27046},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":27860,\"start\":27837},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":27882,\"start\":27871}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":32970,\"start\":32860},{\"attributes\":{\"id\":\"fig_1\"},\"end\":33018,\"start\":32971},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":33368,\"start\":33019},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":33437,\"start\":33369},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":33482,\"start\":33438},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":34015,\"start\":33483},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":34051,\"start\":34016}]", "paragraph": "[{\"end\":2354,\"start\":1943},{\"end\":3105,\"start\":2356},{\"end\":4361,\"start\":3107},{\"end\":5176,\"start\":4363},{\"end\":5515,\"start\":5178},{\"end\":6522,\"start\":5532},{\"end\":7280,\"start\":6524},{\"end\":7916,\"start\":7282},{\"end\":8386,\"start\":7918},{\"end\":8697,\"start\":8673},{\"end\":9396,\"start\":8699},{\"end\":9622,\"start\":9415},{\"end\":9724,\"start\":9624},{\"end\":9949,\"start\":9864},{\"end\":10175,\"start\":9951},{\"end\":10591,\"start\":10222},{\"end\":10776,\"start\":10593},{\"end\":11396,\"start\":10778},{\"end\":12103,\"start\":11398},{\"end\":12670,\"start\":12105},{\"end\":13253,\"start\":12672},{\"end\":13875,\"start\":13281},{\"end\":14870,\"start\":13877},{\"end\":15196,\"start\":15177},{\"end\":15598,\"start\":15198},{\"end\":16713,\"start\":15626},{\"end\":17099,\"start\":16715},{\"end\":17174,\"start\":17133},{\"end\":18090,\"start\":17276},{\"end\":18386,\"start\":18092},{\"end\":19123,\"start\":18388},{\"end\":19714,\"start\":19215},{\"end\":19958,\"start\":19716},{\"end\":20387,\"start\":19960},{\"end\":20744,\"start\":20389},{\"end\":20825,\"start\":20766},{\"end\":20932,\"start\":20895},{\"end\":21967,\"start\":20934},{\"end\":22470,\"start\":22065},{\"end\":22794,\"start\":22472},{\"end\":23119,\"start\":22796},{\"end\":23428,\"start\":23121},{\"end\":23599,\"start\":23496},{\"end\":24051,\"start\":23626},{\"end\":24233,\"start\":24053},{\"end\":24534,\"start\":24305},{\"end\":24601,\"start\":24553},{\"end\":25553,\"start\":24614},{\"end\":26390,\"start\":25578},{\"end\":27779,\"start\":26410},{\"end\":29727,\"start\":27801},{\"end\":30074,\"start\":29801},{\"end\":30179,\"start\":30076},{\"end\":30221,\"start\":30181},{\"end\":31426,\"start\":30223},{\"end\":32029,\"start\":31441},{\"end\":32859,\"start\":32044}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8647,\"start\":8387},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9863,\"start\":9725},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14935,\"start\":14871},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15143,\"start\":14935},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15176,\"start\":15143},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17132,\"start\":17100},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17275,\"start\":17175},{\"attributes\":{\"id\":\"formula_8\"},\"end\":19214,\"start\":19124},{\"attributes\":{\"id\":\"formula_9\"},\"end\":20765,\"start\":20745},{\"attributes\":{\"id\":\"formula_10\"},\"end\":20894,\"start\":20826},{\"attributes\":{\"id\":\"formula_11\"},\"end\":22064,\"start\":21968},{\"attributes\":{\"id\":\"formula_13\"},\"end\":23495,\"start\":23429},{\"attributes\":{\"id\":\"formula_14\"},\"end\":23625,\"start\":23600},{\"attributes\":{\"id\":\"formula_15\"},\"end\":24304,\"start\":24234},{\"attributes\":{\"id\":\"formula_16\"},\"end\":24552,\"start\":24535}]", "table_ref": "[{\"end\":25316,\"start\":25309},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":28199,\"start\":28192},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":30360,\"start\":30353},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":31564,\"start\":31557}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1941,\"start\":1929},{\"attributes\":{\"n\":\"2\"},\"end\":5530,\"start\":5518},{\"end\":8671,\"start\":8649},{\"attributes\":{\"n\":\"3\"},\"end\":9413,\"start\":9399},{\"end\":10220,\"start\":10178},{\"attributes\":{\"n\":\"4\"},\"end\":13279,\"start\":13256},{\"attributes\":{\"n\":\"5\"},\"end\":15624,\"start\":15601},{\"attributes\":{\"n\":\"7.1.1\"},\"end\":24612,\"start\":24604},{\"attributes\":{\"n\":\"7.1.2\"},\"end\":25576,\"start\":25556},{\"attributes\":{\"n\":\"7.1.3\"},\"end\":26408,\"start\":26393},{\"attributes\":{\"n\":\"7.1.4\"},\"end\":27799,\"start\":27782},{\"attributes\":{\"n\":\"7.2\"},\"end\":29750,\"start\":29730},{\"attributes\":{\"n\":\"7.2.1\"},\"end\":29782,\"start\":29753},{\"attributes\":{\"n\":\"7.2.2\"},\"end\":29799,\"start\":29785},{\"attributes\":{\"n\":\"7.2.3\"},\"end\":31439,\"start\":31429},{\"attributes\":{\"n\":\"8\"},\"end\":32042,\"start\":32032},{\"end\":32871,\"start\":32861},{\"end\":32982,\"start\":32972},{\"end\":33379,\"start\":33370},{\"end\":33448,\"start\":33439},{\"end\":34026,\"start\":34017}]", "table": "[{\"end\":33368,\"start\":33186},{\"end\":34015,\"start\":33999}]", "figure_caption": "[{\"end\":32970,\"start\":32873},{\"end\":33018,\"start\":32984},{\"end\":33186,\"start\":33021},{\"end\":33437,\"start\":33381},{\"end\":33482,\"start\":33450},{\"end\":33999,\"start\":33485},{\"end\":34051,\"start\":34028}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3483,\"start\":3474},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4134,\"start\":4125},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4491,\"start\":4482},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10775,\"start\":10769},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10825,\"start\":10819},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11465,\"start\":11457},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13538,\"start\":13532},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14318,\"start\":14309}]", "bib_author_first_name": "[{\"end\":34796,\"start\":34795},{\"end\":34812,\"start\":34806},{\"end\":34837,\"start\":34823},{\"end\":35269,\"start\":35260},{\"end\":35284,\"start\":35281},{\"end\":35298,\"start\":35291},{\"end\":35587,\"start\":35584},{\"end\":35600,\"start\":35594},{\"end\":35950,\"start\":35944},{\"end\":35961,\"start\":35956},{\"end\":35973,\"start\":35966},{\"end\":35988,\"start\":35981},{\"end\":36001,\"start\":35995},{\"end\":36012,\"start\":36006},{\"end\":36513,\"start\":36511},{\"end\":36527,\"start\":36520},{\"end\":37125,\"start\":37120},{\"end\":37142,\"start\":37136},{\"end\":37163,\"start\":37154},{\"end\":37749,\"start\":37742},{\"end\":37770,\"start\":37761},{\"end\":37786,\"start\":37782},{\"end\":38083,\"start\":38077},{\"end\":38098,\"start\":38092},{\"end\":38595,\"start\":38589},{\"end\":38609,\"start\":38600},{\"end\":38618,\"start\":38614},{\"end\":38624,\"start\":38623},{\"end\":38626,\"start\":38625},{\"end\":39295,\"start\":39291},{\"end\":39302,\"start\":39296},{\"end\":39545,\"start\":39541},{\"end\":39552,\"start\":39550},{\"end\":39560,\"start\":39557},{\"end\":39571,\"start\":39566},{\"end\":39579,\"start\":39576},{\"end\":39928,\"start\":39918},{\"end\":39942,\"start\":39935},{\"end\":39957,\"start\":39952},{\"end\":39970,\"start\":39966},{\"end\":40601,\"start\":40596},{\"end\":40612,\"start\":40607},{\"end\":41210,\"start\":41205},{\"end\":41226,\"start\":41222},{\"end\":41237,\"start\":41232},{\"end\":41261,\"start\":41252},{\"end\":41572,\"start\":41571},{\"end\":41588,\"start\":41583},{\"end\":41803,\"start\":41798},{\"end\":41812,\"start\":41808},{\"end\":41828,\"start\":41821},{\"end\":41843,\"start\":41834},{\"end\":41854,\"start\":41850},{\"end\":41866,\"start\":41863},{\"end\":42508,\"start\":42502},{\"end\":42526,\"start\":42516},{\"end\":42824,\"start\":42816},{\"end\":43040,\"start\":43036},{\"end\":43051,\"start\":43047},{\"end\":43067,\"start\":43061},{\"end\":43086,\"start\":43082},{\"end\":43093,\"start\":43087},{\"end\":43108,\"start\":43103},{\"end\":43123,\"start\":43118},{\"end\":43136,\"start\":43130},{\"end\":43824,\"start\":43820},{\"end\":43838,\"start\":43837},{\"end\":43854,\"start\":43847},{\"end\":43856,\"start\":43855},{\"end\":44494,\"start\":44493},{\"end\":44509,\"start\":44503},{\"end\":44520,\"start\":44514},{\"end\":44799,\"start\":44792},{\"end\":44815,\"start\":44810},{\"end\":44828,\"start\":44824},{\"end\":44843,\"start\":44835},{\"end\":45497,\"start\":45492},{\"end\":45518,\"start\":45510},{\"end\":45529,\"start\":45526},{\"end\":46118,\"start\":46111},{\"end\":46130,\"start\":46125},{\"end\":46147,\"start\":46140},{\"end\":46164,\"start\":46156},{\"end\":46507,\"start\":46500},{\"end\":46514,\"start\":46513},{\"end\":46533,\"start\":46522},{\"end\":46535,\"start\":46534},{\"end\":47226,\"start\":47217},{\"end\":47239,\"start\":47232},{\"end\":47259,\"start\":47248},{\"end\":47261,\"start\":47260},{\"end\":48133,\"start\":48130},{\"end\":48341,\"start\":48336},{\"end\":48356,\"start\":48351},{\"end\":48375,\"start\":48368},{\"end\":48667,\"start\":48664},{\"end\":48677,\"start\":48673},{\"end\":48687,\"start\":48684},{\"end\":48702,\"start\":48694},{\"end\":48714,\"start\":48707},{\"end\":48725,\"start\":48721},{\"end\":48738,\"start\":48730},{\"end\":49292,\"start\":49289},{\"end\":49296,\"start\":49293},{\"end\":49310,\"start\":49303},{\"end\":49995,\"start\":49990},{\"end\":50003,\"start\":50000},{\"end\":50013,\"start\":50010},{\"end\":50028,\"start\":50019},{\"end\":50041,\"start\":50034},{\"end\":50641,\"start\":50636},{\"end\":50652,\"start\":50646},{\"end\":50661,\"start\":50658},{\"end\":50670,\"start\":50668},{\"end\":50680,\"start\":50677},{\"end\":50695,\"start\":50686},{\"end\":50708,\"start\":50701},{\"end\":51047,\"start\":51040},{\"end\":51056,\"start\":51052},{\"end\":51074,\"start\":51067},{\"end\":51082,\"start\":51081},{\"end\":51097,\"start\":51089},{\"end\":51110,\"start\":51102},{\"end\":51125,\"start\":51120},{\"end\":51140,\"start\":51136},{\"end\":51152,\"start\":51149},{\"end\":51163,\"start\":51158},{\"end\":51552,\"start\":51549},{\"end\":51564,\"start\":51557},{\"end\":51575,\"start\":51569},{\"end\":51589,\"start\":51582},{\"end\":51601,\"start\":51596},{\"end\":52228,\"start\":52225},{\"end\":52240,\"start\":52233},{\"end\":52251,\"start\":52245},{\"end\":52265,\"start\":52258},{\"end\":52279,\"start\":52272},{\"end\":52295,\"start\":52290},{\"end\":52623,\"start\":52614},{\"end\":52634,\"start\":52629},{\"end\":52646,\"start\":52641},{\"end\":52659,\"start\":52653},{\"end\":52677,\"start\":52671},{\"end\":53114,\"start\":53105},{\"end\":53126,\"start\":53120},{\"end\":53678,\"start\":53672},{\"end\":53693,\"start\":53686},{\"end\":53708,\"start\":53701}]", "bib_author_last_name": "[{\"end\":34804,\"start\":34797},{\"end\":34821,\"start\":34813},{\"end\":34843,\"start\":34838},{\"end\":34852,\"start\":34845},{\"end\":35279,\"start\":35270},{\"end\":35289,\"start\":35285},{\"end\":35305,\"start\":35299},{\"end\":35592,\"start\":35588},{\"end\":35606,\"start\":35601},{\"end\":35954,\"start\":35951},{\"end\":35964,\"start\":35962},{\"end\":35979,\"start\":35974},{\"end\":35993,\"start\":35989},{\"end\":36004,\"start\":36002},{\"end\":36018,\"start\":36013},{\"end\":36518,\"start\":36514},{\"end\":36534,\"start\":36528},{\"end\":37134,\"start\":37126},{\"end\":37152,\"start\":37143},{\"end\":37172,\"start\":37164},{\"end\":37759,\"start\":37750},{\"end\":37780,\"start\":37771},{\"end\":37799,\"start\":37787},{\"end\":38090,\"start\":38084},{\"end\":38105,\"start\":38099},{\"end\":38598,\"start\":38596},{\"end\":38612,\"start\":38610},{\"end\":38621,\"start\":38619},{\"end\":38633,\"start\":38627},{\"end\":38637,\"start\":38635},{\"end\":39309,\"start\":39303},{\"end\":39548,\"start\":39546},{\"end\":39555,\"start\":39553},{\"end\":39564,\"start\":39561},{\"end\":39574,\"start\":39572},{\"end\":39583,\"start\":39580},{\"end\":39933,\"start\":39929},{\"end\":39950,\"start\":39943},{\"end\":39964,\"start\":39958},{\"end\":39982,\"start\":39971},{\"end\":40605,\"start\":40602},{\"end\":40618,\"start\":40613},{\"end\":41220,\"start\":41211},{\"end\":41230,\"start\":41227},{\"end\":41250,\"start\":41238},{\"end\":41267,\"start\":41262},{\"end\":41581,\"start\":41573},{\"end\":41595,\"start\":41589},{\"end\":41599,\"start\":41597},{\"end\":41806,\"start\":41804},{\"end\":41819,\"start\":41813},{\"end\":41832,\"start\":41829},{\"end\":41848,\"start\":41844},{\"end\":41861,\"start\":41855},{\"end\":41875,\"start\":41867},{\"end\":42514,\"start\":42509},{\"end\":42530,\"start\":42527},{\"end\":42828,\"start\":42825},{\"end\":43045,\"start\":43041},{\"end\":43059,\"start\":43052},{\"end\":43080,\"start\":43068},{\"end\":43101,\"start\":43094},{\"end\":43116,\"start\":43109},{\"end\":43128,\"start\":43124},{\"end\":43143,\"start\":43137},{\"end\":43835,\"start\":43825},{\"end\":43845,\"start\":43839},{\"end\":43862,\"start\":43857},{\"end\":44501,\"start\":44495},{\"end\":44512,\"start\":44510},{\"end\":44527,\"start\":44521},{\"end\":44536,\"start\":44529},{\"end\":44808,\"start\":44800},{\"end\":44822,\"start\":44816},{\"end\":44833,\"start\":44829},{\"end\":44847,\"start\":44844},{\"end\":45508,\"start\":45498},{\"end\":45524,\"start\":45519},{\"end\":45535,\"start\":45530},{\"end\":46123,\"start\":46119},{\"end\":46138,\"start\":46131},{\"end\":46154,\"start\":46148},{\"end\":46169,\"start\":46165},{\"end\":46178,\"start\":46171},{\"end\":46511,\"start\":46508},{\"end\":46520,\"start\":46515},{\"end\":46539,\"start\":46536},{\"end\":46548,\"start\":46541},{\"end\":47230,\"start\":47227},{\"end\":47246,\"start\":47240},{\"end\":47269,\"start\":47262},{\"end\":48139,\"start\":48134},{\"end\":48349,\"start\":48342},{\"end\":48366,\"start\":48357},{\"end\":48382,\"start\":48376},{\"end\":48671,\"start\":48668},{\"end\":48682,\"start\":48678},{\"end\":48692,\"start\":48688},{\"end\":48705,\"start\":48703},{\"end\":48719,\"start\":48715},{\"end\":48728,\"start\":48726},{\"end\":48741,\"start\":48739},{\"end\":49301,\"start\":49297},{\"end\":49317,\"start\":49311},{\"end\":49998,\"start\":49996},{\"end\":50008,\"start\":50004},{\"end\":50017,\"start\":50014},{\"end\":50032,\"start\":50029},{\"end\":50045,\"start\":50042},{\"end\":50644,\"start\":50642},{\"end\":50656,\"start\":50653},{\"end\":50666,\"start\":50662},{\"end\":50675,\"start\":50671},{\"end\":50684,\"start\":50681},{\"end\":50699,\"start\":50696},{\"end\":50712,\"start\":50709},{\"end\":51050,\"start\":51048},{\"end\":51065,\"start\":51057},{\"end\":51079,\"start\":51075},{\"end\":51087,\"start\":51083},{\"end\":51100,\"start\":51098},{\"end\":51118,\"start\":51111},{\"end\":51134,\"start\":51126},{\"end\":51147,\"start\":51141},{\"end\":51156,\"start\":51153},{\"end\":51167,\"start\":51164},{\"end\":51177,\"start\":51169},{\"end\":51555,\"start\":51553},{\"end\":51567,\"start\":51565},{\"end\":51580,\"start\":51576},{\"end\":51594,\"start\":51590},{\"end\":51609,\"start\":51602},{\"end\":52231,\"start\":52229},{\"end\":52243,\"start\":52241},{\"end\":52256,\"start\":52252},{\"end\":52270,\"start\":52266},{\"end\":52288,\"start\":52280},{\"end\":52303,\"start\":52296},{\"end\":52627,\"start\":52624},{\"end\":52639,\"start\":52635},{\"end\":52651,\"start\":52647},{\"end\":52669,\"start\":52660},{\"end\":52684,\"start\":52678},{\"end\":53118,\"start\":53115},{\"end\":53133,\"start\":53127},{\"end\":53684,\"start\":53679},{\"end\":53699,\"start\":53694},{\"end\":53715,\"start\":53709}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":60639321},\"end\":35182,\"start\":34744},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":2723946},\"end\":35555,\"start\":35184},{\"attributes\":{\"doi\":\"arXiv:1808.01400\",\"id\":\"b2\"},\"end\":35864,\"start\":35557},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":1406542},\"end\":36461,\"start\":35866},{\"attributes\":{\"doi\":\"10.18653/v1/P16-1004\",\"id\":\"b4\",\"matched_paper_id\":15412473},\"end\":37061,\"start\":36463},{\"attributes\":{\"doi\":\"10.18653/v1/P16-1078\",\"id\":\"b5\",\"matched_paper_id\":12851711},\"end\":37740,\"start\":37063},{\"attributes\":{\"doi\":\"arXiv:1811.01824\",\"id\":\"b6\"},\"end\":38000,\"start\":37742},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":5575601},\"end\":38521,\"start\":38002},{\"attributes\":{\"doi\":\"10.18653/v1/P16-1154\",\"id\":\"b8\",\"matched_paper_id\":8174613},\"end\":39197,\"start\":38523},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":125881359},\"end\":39509,\"start\":39199},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":49584534},\"end\":39860,\"start\":39511},{\"attributes\":{\"doi\":\"10.18653/v1/P16-1195\",\"id\":\"b11\",\"matched_paper_id\":8820379},\"end\":40546,\"start\":39862},{\"attributes\":{\"doi\":\"10.18653/v1/P16-1002\",\"id\":\"b12\",\"matched_paper_id\":7218315},\"end\":41142,\"start\":40548},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":44063077},\"end\":41525,\"start\":41144},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b14\"},\"end\":41743,\"start\":41527},{\"attributes\":{\"doi\":\"10.18653/v1/D17-1230\",\"id\":\"b15\",\"matched_paper_id\":98180},\"end\":42433,\"start\":41745},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":19194935},\"end\":42758,\"start\":42435},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":964287},\"end\":42987,\"start\":42760},{\"attributes\":{\"doi\":\"10.18653/v1/P16-1057\",\"id\":\"b18\",\"matched_paper_id\":14434979},\"end\":43757,\"start\":42989},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":6706547},\"end\":44401,\"start\":43759},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":5730166},\"end\":44726,\"start\":44403},{\"attributes\":{\"doi\":\"10.3115/1073083.1073135\",\"id\":\"b21\",\"matched_paper_id\":11080756},\"end\":45423,\"start\":44728},{\"attributes\":{\"doi\":\"10.18653/v1/P17-1105\",\"id\":\"b22\",\"matched_paper_id\":13529592},\"end\":46109,\"start\":45425},{\"attributes\":{\"doi\":\"arXiv:1511.06732\",\"id\":\"b23\"},\"end\":46434,\"start\":46111},{\"attributes\":{\"doi\":\"10.18653/v1/P17-1099\",\"id\":\"b24\",\"matched_paper_id\":8314118},\"end\":47127,\"start\":46436},{\"attributes\":{\"doi\":\"10.3115/v1/P15-1150\",\"id\":\"b25\",\"matched_paper_id\":3033526},\"end\":48079,\"start\":47129},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":14526326},\"end\":48316,\"start\":48081},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":5692837},\"end\":48583,\"start\":48318},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":52069701},\"end\":49202,\"start\":48585},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":799077},\"end\":49922,\"start\":49204},{\"attributes\":{\"doi\":\"10.18653/v1/D18-1397\",\"id\":\"b30\",\"matched_paper_id\":52100616},\"end\":50594,\"start\":49924},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":12267036},\"end\":50938,\"start\":50596},{\"attributes\":{\"doi\":\"arXiv:1609.08144\",\"id\":\"b32\"},\"end\":51494,\"start\":50940},{\"attributes\":{\"doi\":\"10.18653/v1/D18-1112\",\"id\":\"b33\",\"matched_paper_id\":52282359},\"end\":52147,\"start\":51496},{\"attributes\":{\"doi\":\"arXiv:1804.00823\",\"id\":\"b34\"},\"end\":52534,\"start\":52149},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":43922261},\"end\":53041,\"start\":52536},{\"attributes\":{\"doi\":\"10.18653/v1/P17-1041\",\"id\":\"b36\",\"matched_paper_id\":12718048},\"end\":53670,\"start\":53043},{\"attributes\":{\"doi\":\"arXiv:1709.00103\",\"id\":\"b37\"},\"end\":54016,\"start\":53672}]", "bib_title": "[{\"end\":34793,\"start\":34744},{\"end\":35258,\"start\":35184},{\"end\":35582,\"start\":35557},{\"end\":35942,\"start\":35866},{\"end\":36509,\"start\":36463},{\"end\":37118,\"start\":37063},{\"end\":38075,\"start\":38002},{\"end\":38587,\"start\":38523},{\"end\":39289,\"start\":39199},{\"end\":39539,\"start\":39511},{\"end\":39916,\"start\":39862},{\"end\":40594,\"start\":40548},{\"end\":41203,\"start\":41144},{\"end\":41796,\"start\":41745},{\"end\":42500,\"start\":42435},{\"end\":42814,\"start\":42760},{\"end\":43034,\"start\":42989},{\"end\":43818,\"start\":43759},{\"end\":44491,\"start\":44403},{\"end\":44790,\"start\":44728},{\"end\":45490,\"start\":45425},{\"end\":46498,\"start\":46436},{\"end\":47215,\"start\":47129},{\"end\":48128,\"start\":48081},{\"end\":48334,\"start\":48318},{\"end\":48662,\"start\":48585},{\"end\":49287,\"start\":49204},{\"end\":49988,\"start\":49924},{\"end\":50634,\"start\":50596},{\"end\":51547,\"start\":51496},{\"end\":52612,\"start\":52536},{\"end\":53103,\"start\":53043}]", "bib_author": "[{\"end\":34806,\"start\":34795},{\"end\":34823,\"start\":34806},{\"end\":34845,\"start\":34823},{\"end\":34854,\"start\":34845},{\"end\":35281,\"start\":35260},{\"end\":35291,\"start\":35281},{\"end\":35307,\"start\":35291},{\"end\":35594,\"start\":35584},{\"end\":35608,\"start\":35594},{\"end\":35956,\"start\":35944},{\"end\":35966,\"start\":35956},{\"end\":35981,\"start\":35966},{\"end\":35995,\"start\":35981},{\"end\":36006,\"start\":35995},{\"end\":36020,\"start\":36006},{\"end\":36520,\"start\":36511},{\"end\":36536,\"start\":36520},{\"end\":37136,\"start\":37120},{\"end\":37154,\"start\":37136},{\"end\":37174,\"start\":37154},{\"end\":37761,\"start\":37742},{\"end\":37782,\"start\":37761},{\"end\":37801,\"start\":37782},{\"end\":38092,\"start\":38077},{\"end\":38107,\"start\":38092},{\"end\":38600,\"start\":38589},{\"end\":38614,\"start\":38600},{\"end\":38623,\"start\":38614},{\"end\":38635,\"start\":38623},{\"end\":38639,\"start\":38635},{\"end\":39311,\"start\":39291},{\"end\":39550,\"start\":39541},{\"end\":39557,\"start\":39550},{\"end\":39566,\"start\":39557},{\"end\":39576,\"start\":39566},{\"end\":39585,\"start\":39576},{\"end\":39935,\"start\":39918},{\"end\":39952,\"start\":39935},{\"end\":39966,\"start\":39952},{\"end\":39984,\"start\":39966},{\"end\":40607,\"start\":40596},{\"end\":40620,\"start\":40607},{\"end\":41222,\"start\":41205},{\"end\":41232,\"start\":41222},{\"end\":41252,\"start\":41232},{\"end\":41269,\"start\":41252},{\"end\":41583,\"start\":41571},{\"end\":41597,\"start\":41583},{\"end\":41601,\"start\":41597},{\"end\":41808,\"start\":41798},{\"end\":41821,\"start\":41808},{\"end\":41834,\"start\":41821},{\"end\":41850,\"start\":41834},{\"end\":41863,\"start\":41850},{\"end\":41877,\"start\":41863},{\"end\":42516,\"start\":42502},{\"end\":42532,\"start\":42516},{\"end\":42830,\"start\":42816},{\"end\":43047,\"start\":43036},{\"end\":43061,\"start\":43047},{\"end\":43082,\"start\":43061},{\"end\":43103,\"start\":43082},{\"end\":43118,\"start\":43103},{\"end\":43130,\"start\":43118},{\"end\":43145,\"start\":43130},{\"end\":43837,\"start\":43820},{\"end\":43847,\"start\":43837},{\"end\":43864,\"start\":43847},{\"end\":44503,\"start\":44493},{\"end\":44514,\"start\":44503},{\"end\":44529,\"start\":44514},{\"end\":44538,\"start\":44529},{\"end\":44810,\"start\":44792},{\"end\":44824,\"start\":44810},{\"end\":44835,\"start\":44824},{\"end\":44849,\"start\":44835},{\"end\":45510,\"start\":45492},{\"end\":45526,\"start\":45510},{\"end\":45537,\"start\":45526},{\"end\":46125,\"start\":46111},{\"end\":46140,\"start\":46125},{\"end\":46156,\"start\":46140},{\"end\":46171,\"start\":46156},{\"end\":46180,\"start\":46171},{\"end\":46513,\"start\":46500},{\"end\":46522,\"start\":46513},{\"end\":46541,\"start\":46522},{\"end\":46550,\"start\":46541},{\"end\":47232,\"start\":47217},{\"end\":47248,\"start\":47232},{\"end\":47271,\"start\":47248},{\"end\":48141,\"start\":48130},{\"end\":48351,\"start\":48336},{\"end\":48368,\"start\":48351},{\"end\":48384,\"start\":48368},{\"end\":48673,\"start\":48664},{\"end\":48684,\"start\":48673},{\"end\":48694,\"start\":48684},{\"end\":48707,\"start\":48694},{\"end\":48721,\"start\":48707},{\"end\":48730,\"start\":48721},{\"end\":48743,\"start\":48730},{\"end\":49303,\"start\":49289},{\"end\":49319,\"start\":49303},{\"end\":50000,\"start\":49990},{\"end\":50010,\"start\":50000},{\"end\":50019,\"start\":50010},{\"end\":50034,\"start\":50019},{\"end\":50047,\"start\":50034},{\"end\":50646,\"start\":50636},{\"end\":50658,\"start\":50646},{\"end\":50668,\"start\":50658},{\"end\":50677,\"start\":50668},{\"end\":50686,\"start\":50677},{\"end\":50701,\"start\":50686},{\"end\":50714,\"start\":50701},{\"end\":51052,\"start\":51040},{\"end\":51067,\"start\":51052},{\"end\":51081,\"start\":51067},{\"end\":51089,\"start\":51081},{\"end\":51102,\"start\":51089},{\"end\":51120,\"start\":51102},{\"end\":51136,\"start\":51120},{\"end\":51149,\"start\":51136},{\"end\":51158,\"start\":51149},{\"end\":51169,\"start\":51158},{\"end\":51179,\"start\":51169},{\"end\":51557,\"start\":51549},{\"end\":51569,\"start\":51557},{\"end\":51582,\"start\":51569},{\"end\":51596,\"start\":51582},{\"end\":51611,\"start\":51596},{\"end\":52233,\"start\":52225},{\"end\":52245,\"start\":52233},{\"end\":52258,\"start\":52245},{\"end\":52272,\"start\":52258},{\"end\":52290,\"start\":52272},{\"end\":52305,\"start\":52290},{\"end\":52629,\"start\":52614},{\"end\":52641,\"start\":52629},{\"end\":52653,\"start\":52641},{\"end\":52671,\"start\":52653},{\"end\":52686,\"start\":52671},{\"end\":53120,\"start\":53105},{\"end\":53135,\"start\":53120},{\"end\":53686,\"start\":53672},{\"end\":53701,\"start\":53686},{\"end\":53717,\"start\":53701}]", "bib_venue": "[{\"end\":34941,\"start\":34854},{\"end\":35351,\"start\":35307},{\"end\":35684,\"start\":35624},{\"end\":36101,\"start\":36020},{\"end\":36643,\"start\":36556},{\"end\":37281,\"start\":37194},{\"end\":37848,\"start\":37817},{\"end\":38203,\"start\":38107},{\"end\":38746,\"start\":38659},{\"end\":39340,\"start\":39311},{\"end\":39644,\"start\":39585},{\"end\":40091,\"start\":40004},{\"end\":40727,\"start\":40640},{\"end\":41326,\"start\":41269},{\"end\":41569,\"start\":41527},{\"end\":41983,\"start\":41897},{\"end\":42588,\"start\":42532},{\"end\":42861,\"start\":42830},{\"end\":43252,\"start\":43165},{\"end\":43951,\"start\":43864},{\"end\":44542,\"start\":44538},{\"end\":44959,\"start\":44872},{\"end\":45644,\"start\":45557},{\"end\":46250,\"start\":46196},{\"end\":46657,\"start\":46570},{\"end\":47451,\"start\":47290},{\"end\":48182,\"start\":48141},{\"end\":48433,\"start\":48384},{\"end\":48834,\"start\":48743},{\"end\":49482,\"start\":49319},{\"end\":50153,\"start\":50067},{\"end\":50750,\"start\":50714},{\"end\":51038,\"start\":50940},{\"end\":51717,\"start\":51631},{\"end\":52223,\"start\":52149},{\"end\":52767,\"start\":52686},{\"end\":53242,\"start\":53155},{\"end\":53822,\"start\":53733},{\"end\":36169,\"start\":36103},{\"end\":36732,\"start\":36645},{\"end\":37370,\"start\":37283},{\"end\":38286,\"start\":38205},{\"end\":38835,\"start\":38748},{\"end\":39690,\"start\":39646},{\"end\":40180,\"start\":40093},{\"end\":40816,\"start\":40729},{\"end\":42075,\"start\":41985},{\"end\":43341,\"start\":43254},{\"end\":44040,\"start\":43953},{\"end\":45064,\"start\":44961},{\"end\":45735,\"start\":45646},{\"end\":46748,\"start\":46659},{\"end\":47613,\"start\":47453},{\"end\":48912,\"start\":48836},{\"end\":49503,\"start\":49484},{\"end\":50243,\"start\":50155},{\"end\":51807,\"start\":51719},{\"end\":53333,\"start\":53244}]"}}}, "year": 2023, "month": 12, "day": 17}