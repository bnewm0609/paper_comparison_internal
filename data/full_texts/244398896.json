{"id": 244398896, "updated": "2022-09-30 01:50:10.893", "metadata": {"title": "Mitigating Intensity Bias in Shadow Detection via Feature Decomposition and Reweighting", "authors": "[{\"first\":\"Lei\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Ke\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Zhanghan\",\"last\":\"Ke\",\"middle\":[]},{\"first\":\"Rynson\",\"last\":\"Lau\",\"middle\":[\"W.H.\"]}]", "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "journal": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Although CNNs have achieved remarkable progress on the shadow detection task, they tend to make mistakes in dark non-shadow regions and relatively bright shadow regions. They are also susceptible to brightness change. These two phenomenons reveal that deep shadow detectors heavily depend on the intensity cue, which we refer to as intensity bias. In this paper, we propose a novel feature decomposition and reweighting scheme to mitigate this intensity bias, in which multi-level integrated features are decomposed into intensity-variant and intensity-invariant components through self-supervision. By reweighting these two types of features, our method can reallocate the attention to the corresponding latent semantics and achieves balanced exploitation of them. Extensive experiments on three popular datasets show that the proposed method outperforms state-of-the-art shadow detectors.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/Zhu0KL21", "doi": "10.1109/iccv48922.2021.00466"}}, "content": {"source": {"pdf_hash": "66bbf0108546d00abc74868342b966393e83538f", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4c6c674bd13db8d565837d55ae7579508e9ff96e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/66bbf0108546d00abc74868342b966393e83538f.txt", "contents": "\nMitigating Intensity Bias in Shadow Detection via Feature Decomposition and Reweighting\n\n\nLei Zhu \nCity University of Hong Kong\n\n\nKe Xu \nShanghai Jiao Tong University\n\n\nZhanghan Ke zhanghake2-c@my.cityu.edu.hk \nCity University of Hong Kong\n\n\nRynson W H Lau rynson.lau@cityu.edu.hk \nCity University of Hong Kong\n\n\nMitigating Intensity Bias in Shadow Detection via Feature Decomposition and Reweighting\n10.1109/ICCV48922.2021.00466\nAlthough CNNs have achieved remarkable progress on the shadow detection task, they tend to make mistakes in dark non-shadow regions and relatively bright shadow regions. They are also susceptible to brightness change. These two phenomenons reveal that deep shadow detectors heavily depend on the intensity cue, which we refer to as intensity bias. In this paper, we propose a novel feature decomposition and reweighting scheme to mitigate this intensity bias, in which multi-level integrated features are decomposed into intensity-variant and intensity-invariant components through self-supervision. By reweighting these two types of features, our method can reallocate the attention to the corresponding latent semantics and achieves balanced exploitation of them. Extensive experiments on three popular datasets show that the proposed method outperforms state-of-the-art shadow detectors.\n\nIntroduction\n\nShadows may appear when lights cannot directly reach an object surface. While they provide cues on light source directions and scene illuminations, which facilitate scene understanding [20,22], they may adversely affect the performance of computer vision tasks [7,28]. Hence, shadow detection is crucial.\n\nEarlier, many works were proposed to detect shadows using hand-crafted features. However, these methods are unreliable and may fail in complex scenes. Recently, deep learning based shadow detection methods have shown superior performance over traditional methods by a large margin. Being trained in an end-to-end manner, deep shadow detectors can automatically learn discriminative features for detection, without the efforts to specify which cues to use and how they should be represented. However, there is a cost to this convenience. There are signs that existing deep shadow detectors heavily rely on the intensity cue. While they may mis-recognize relatively bright shadow regions \u2020 Ke Xu and Rynson Lau are joint corresponding authors. Rynson Lau leads this project.\n\n(a) Input Image (b) DSC [16] (c) DSD [50] (d) MTMT [5] (e) Ours (f) GT Figure 1. Intensity bias in shadow detection. Rows 1 and 3 show two original images, while rows 2 and 4 show the two images with 20% increase in intensity. Existing methods [16,50,5] heavily rely on the intensity cue, and suffer from two problems. On the one hand, they mis-recognize a relatively brighter region inside the shadow as non-shadow (e.g., yellow lanes in row 1), and dark non-shadow region as shadow (e.g., the traffic cone in row 3). On the other hand, their predictions change significantly due to the brightness change (rows 2 and 4). Our method mitigates this intensity bias and produces more consistent and accurate results.\n\nas non-shadows ( Fig. 1 row 1) or dark non-shadow regions as shadows ( Fig. 1 row 3), with a small shift in brightness (which should not change the image semantics), the detection results may change significantly ( Fig. 1 rows 2 and 4). Although low intensity is a strong indication of shadows, other cues such as object-shadow correspondences, shadow edges, and region connectivity may also contribute to the shadow detection task [33]. However, deep models tend to be attracted by some dominant cues while leaving the less dominant ones underexplored. For example, Geirhos et al. [12] show that ImageNet-trained classifiers typically have a bias towards textures, and increasing the attention to shapes helps improve classification accuracy and robustness. Choi et al. [6] also show that reducing scene bias improves the generalization of action recognition networks.\n\nTo mitigate such a bias towards the intensity cue in shadow detection, the most straightforward solution is to Figure 2. Augmenting the training data with a brightness shift of different extents (vertical bars) increases the balanced error rate (BER, lower is better), compared to the result without using data augmentation (dash horizontal line). Our method mitigates the intensity bias and produces a much better performance. All results are from a FPN-like network [26] trained on the SBU dataset [40].\n\napply data augmentation, such as random brightness shift, to implicitly impose the intensity-invariant constraint on the network. Unfortunately, such a simple strategy does not work and may even degrade the detection accuracy in practice, as shown in Fig. 2. This is due to distributional discrepancy between the true data and the augmented data [5]. Alternatively, existing works have tried to explicitly introduce or reinforce other cues. For example, Hu et al. [16] propose a module to model the contrast information in a direction-aware manner. Chen et al. [5] propose to incorporate shadow edges and shadow count in the detection. However, introducing these specific cues cannot address this intensity bias problem well, as demonstrated in Fig. 1.\n\nInstead, we propose in this paper a novel feature decomposition and reweighting scheme to combat the undue attention to intensity. Specifically, we first decompose deep shadow features into intensity-variant (i.e., responding to intensity changes) and intensity-invariant (i.e., without responding to intensity changes) components, so that the network can mine the two types of features individually, and then re-integrate them with appropriate weights.\n\nThe challenge of this approach is how to decompose highly coupled features into intensity-variant and intensityinvariant components. To guide the learning of such decomposition, we construct two novel self-supervised tasks. While one aims to minimize the difference between the intensity-invariant features extracted from the input image and its brightness-shifted counterpart, the other is to predict the brightness shift from the intensity-variant features (Sec. 3.1). To reallocate the attention to the decomposed features, during the training stage, we gradually shift the learning focus from the intensity-variant features to the intensity-invariant features via cumulative learning [51]. We then search for an optimal weight on the validation set and fix it for the inference stage (Sec. 3.2). In summary, our main contributions are three-fold:\n\n\u2022 We propose a novel feature decomposition and reweight-ing scheme to mitigate the intensity bias in shadow detection, which allows our shadow detector to reallocate its attention between the intensity-variant and intensityinvariant features.\n\n\u2022 We propose a novel self-supervised approach, which is tailored for shadow detection, to guide the decomposition of deep features.\n\n\u2022 Extensive experiments on three public datasets demonstrate that the proposed method outperforms state-of-theart shadow detection methods.\n\n\nRelated Works\n\nShadow detection. To detect shadows in a single image, earlier works propose physical models [11,10,38] or machine learning classifiers based on handcrafted features [23,53,14]. Typically, they exploit one or more heuristic cues, such as chromacity [11,10,23,14,39], edge [11,23,53,17], intensity [38,14,53,17,39], and texture [53,14,39]. However, these methods cannot handle real-world complex scenes well, when the assumptions (e.g., uniform illumination) made in these physical models are violated, or the hand-crafted features used in the traditional machine learning classifiers fail to represent the shadow patterns.\n\nRecently, deep learning based methods have shown great success in shadow detection. Nguyen et al. [29] first propose a tailored conditional GAN for this task. Hu et al. [16] propose to detect shadows by learning global contextual features in a direction-aware manner. Le et al. [24] propose to jointly learn a shadow detector with another network for generating augmented training data using adversarial training. Zhu et al. [55] propose to fuse multi-level features recursively and bidirectionally. Wang et al. [41] propose a stacked conditional GAN to jointly learn shadow detection and removal. Zheng et al. [50] propose to learn distraction-aware features for shadow detection by learning from the false predictions of other deep shadow detectors. Most recently, Chen et al. [5] introduce a teacher-student framework [37] to exploit additional unannotated shadow images. They also explicitly detect shadow boundaries to improve the detection accuracy.\n\nWhile compelling results are obtained by deep shadow detectors, we note that they tend to predict a brighter region within a shadow as non-shadow and a dark region as shadow. In addition, their predictions may change significantly as we adjust the brightness of the input image. Such phenomenons indicate that they rely too much on the intensity cue to make their predictions. This motivates our work in attempting to balance the impact of intensity-variant and intensity-invariant features.\n\nSelf-supervised learning. Towards task-agnostic image representation learning, self-supervised learning has re-ceived remarkable attention in the representation learning community recently. As its name implies, in self-supervised learning, the supervision signals are derived not from human annotations, but from the input images themselves. One line of self-supervised learning relies on elaborate pretext tasks such as predicting relative patches [9], solving jigsaw puzzles [31], colorization [47], and predicting the degree of image rotation [13,4,25]. Without explicitly constructing artificial labels, another line of works adopt the contrastive learning strategy [44,2,3], in which the image representation is obtained by contrasting positive and negative pairs in the feature embedding space.\n\nUnlike those methods designed for task-agnostic visual representation, we apply self-supervised learning for a taskspecific objective: decomposing the discriminative shadow features into intensity-variant and intensity-invariant components. In addition, instead of constructing a single selfsuperivised task, a pair of self-supervised tasks, including a novel pretext task and the contrastive learning approach, are jointly exploited to learn such a decomposition.\n\n\nProposed Method\n\nExisting deep shadow detectors place undue importance to the intensity cue. To mitigate such a bias, our key idea is to readjust the network's focus on the dominant intensity cue and excavate other less dominant cues. However, as deep features encode all cues together in a coupled manner, it is not easy to reallocate the attention to some specific ones. Hence, we propose a feature decomposition and reweighting (FDR) scheme to achieve such controllability. Fig. 3 shows the workflow of the proposed FDR scheme, with both training and inference stages.\n\n\nSelf-supervised Feature Decomposition\n\nWe introduce a pair of contradictory self-supervised tasks to guide the learning of intensity-variant and intensityinvariant features in the training stage. They are separately applied to bilateral branches to encourage such a mutually complementary decomposition.\n\nFormally, given a training image I as input, we first randomly shift its brightness to produce a counterpart I 0 :\nI 0 = I + ,(1)\nwhere is the shift amount. It is a random variable uniformly sampled from the range of [ , ]. We then feed both I and I 0 to the Feature Extraction Subnetwork followed by the FDR module. The bilateral projection branches of the FDR modules output four intermediate feature maps:\nF i (intensity-invariant features of I), F v (intensity-variant features of I), F 0 i (intensity-invariant features of I 0 ), and F 0 v (intensity-variant features of I 0 ).\nSince we do not expect the intensity-invariant features to change under such a brightness shift, we introduce the first self-supervised task here to force these two intensityinvariant features (i.e., F 0 i and F i ) to be consistent, as:\nL i = MAE (F i , F 0 i ),(2)\nwhere MAE (\u00b7, \u00b7) is a mean-absolute-error loss function.\n\nIn addition, we also expect that the intensity related information is encoded in the intensity-variant features. Hence, we formulate the second self-supervised task as learning to predict the perturbation amount from F 0 v . Specifically, we attach an auxiliary regression head (\u00b7; \u2713) parameterized by global average pooling (GAP), followed by a fully connected (FC) layer. It takes F 0 v as input to predict , and the corresponding pretext loss is defined as:\nL v = MAE ( (F 0 v ; \u2713), ).(3)\nObviously, predicting the brightness shift from I 0 without referring to the original image I is ill-posed and challenging. However, it is worth noting that an ill-posed pretext task is widely used in self-supervised representation learning, such as predicting image rotation [13,4,25] or flip [27] without reference. On the one hand, for representation learning, it is crucial to avoid the network learning to solve the pretext task by exploiting low-level visual features [32], which can easily happen when we provide reference images. On the other hand, as a pretext task, what we concern about is the representation induced by it, instead of how well the network can do for itself. In our case, predicting the brightness shift without reference images forces the network to encode the intensity prior based on how a given image should look like at the average exposure level. Our experiments (Sec. 5.5.1) show that providing the reference image I (i.e., using a well-posed pretext task) would deteriorate the final detection performance.\n\nThe two contradictory losses L i and L v guide the network to decompose coupled features into intensity-invariant and intensity-variant features, allowing sufficient excavation of both and further re-evaluation of their individual contributions to the final prediction.\n\n\nFeature Reweighting via Cumulative Learning\n\nAs discussed above, since intensity is still a dominant cue in shadow detection, we should include both intensityvariant features F v and intensity-invariant features F i into the shadow detection task. What we need is to balance the impact of the two types of features. Since the whole model is trained in an end-to-end manner, we may formulate the feature fusion step with a summation, as in the feature pyramid network [26]. In addition, to re-weight the contributions of these features, we introduce a trade-off parameter \u00b5 to formulate feature reweighting as: where F r denotes the output shadow features.\nF r = \u00b5F v + (1 \u00b5)F i ,(4)\nA straightforward choice to determine \u00b5 is to learn it from data, by setting it as a differentiable parameter [18] or predicting it with a network branch [19]. However, these two strategies do not work in our case, as the intensity bias comes from the data. In fact, through experiments (more details in Sec. 5.5.2), we observe that \u00b5 would continue to grow to near 1 as training proceeds while the detection performance shows no improvement. Instead, in the training stage, we adopt cumulative learning [51] to gradually shift the focus of the network from intensity-variant features to intensity-invariant features. Given the current training epoch T and total training epoch T max , \u00b5 is:\n\u00b5 = 1 ( T T max ) ,(5)\nwhere is a hyper-parameter to control the descending pace of \u00b5 over the training stage.A larger will induce a smoother focus transition from intensity-variant features to intensity variant features at the beginning of training.\n\nOnce the training is finished, we need to determine a proper \u00b5 for inference. Since \u00b5 is a scalar between [0, 1], its value can be obtained through a grid search on the validation set. In practice, we set the search step to 0.1.\n\nThe cumulative learning strategy is similar to Dropout [35]. While in Dropout, some neurons are randomly dropped in a hard manner, in cumulative learning, we progressively drop intensity-variant features in a soft manner. The choice of \u00b5 is then to obtain the suitable weights for the two types of features. This implicitly creates an ensemble of the two detectors to exploit both.  [8]. Such a replacement should not affect our shadow detection performance and analysis.\n\nWith EfficientNet-B3 [36] as the backbone, we extract multi-level features from every two consecutive blocks, producing 13 groups of feature maps in total. We use bilinear upsampling to keep their spatial resolutions to remain half of that of the input, and use 1\u21e51 convolution to reduce their channels into 16. These features are then concatenated and fused into 32-channel features via 1 \u21e5 1 convolution, producing the multi-layer integrated features (MLIF) for further feature decomposition and reweighting.\n\nLoss function. Considering the imbalanced numbers of 4685 shadow and non-shadow pixels in natural scenes, we adopt the balanced binary cross entropy as shadow detection loss:\nL ce (M,M) = X i \u21e5 N n N M i logM i + N p N (1 M i ) log(1 M i ) \u21e4 ,(6)\nwhere i is the index of the spatial location. M is the ground truth shadow map.M is the predicted shadow mask. N p , N n and N are the number of shadow and non-shadow pixels, and the total number of pixels in the image, respectively. Together with the two proposed loss terms L i and L v for feature decomposition, the final loss function L total is:\nL total = L ce + i L i + v L v ,(7)\nwhere i and v are two balancing parameters, which are empirically set to 1 and 0.1, respectively.\n\nTraining details. Following recent state-of-the-art shadow detectors [50,5,55,16], we initialize the backbone with weights pretrained on ImageNet [8]. Other newly introduced trainable parameters are randomly initialized. We optimize the whole network for 10 epochs using the Adam optimizer, with an initial learning rate of 5e 4, which is adjusted by the exponential decay strategy (decay rate = 0.7).\n\nIn each training iteration, the input images are resized to a resolution of 400 \u21e5 400 and fed into the network with a mini-batch size of 6. We apply random horizontal flipping for data augmentation.\n\n(for determining the maximum amount of intensity shift for feature decomposition) is set to 0.3, and (for controlling the cumulative learning pace) is set to 2. The training is run on a single RTX2080Ti GPU. As the datasets (SBU [40] and ISTD [41]) used in our training do not provide validation split, as suggested in [1], we randomly hold out 10% of the data in the training set for validation. For fair comparison to existing works, once the optimal \u00b5 is determined, we put them back and retrain the model with the whole training set. Training takes 2 hours on SBU [40] and 1 hour on ISTD [41].\n\nInference. Following recent shadow detection works [16,55,24,50,5], we use the fully connected CRF [21] to refine our predictions (with a threshold value of 0.5) to obtain the final shadow mask. The inference of images at a resolution of 400 \u21e5 400 runs at 161 frames per second.\n\n\nExperiments\n\n\nEvaluation Datasets and Metric\n\nEvaluation datasets. We conduct our experiments on three public datasets, i.e., SBU [40], UCF [54] and ISTD [41], to evaluate our shadow detector. The UCF dataset contains 135 training images and 110 test images. The SBU dataset contains 4,089 training images and 638 test images. The ISTD dataset contains 1,330 training images and 540 test images. We follow previous shadow detection methods to train on the SBU training set, and test on both SBU and UCF test sets. For the evaluation on the ISTD test set, we train our model on its training set.\n\nEvaluation metric. For quantitative performance evaluation, we use the popular metric, balanced error rate (BER):\nBER = (1 1 2 ( T P T P + F N + T N T N + F P )) \u21e5 100,(8)\nwhere TP, TN, FP, and FN denote the number of true positives, true negatives, false positives, and false negatives.\n\n\nMethods for comparison.\n\nWe first compare our method with 11 state-of-the-art shadow detectors, including MTMT [5], DSD [50], DC-DSPF [43], AD-Net [24], DSC [16], BDRAR [55], ST-CGAN [41], patched-CNN [15], scGAN [30], stacked-CNN [40], and Unary-Pairwise [14]. All of them are deep-learning based methods, except Unary-Pairwise [14], which is based on handcrafted features.\n\nAs shadow detection is a kind of pixel-level classification problem, it is related to saliency object detection (SOD) and semantic segmentation. For a comprehensive study, we also compare our method with four state-of-the-art SOD methods, EGNet [49], ITSD [52], SRM [42], and Amulet [46], and one semantic segmentation method, PSPNet [48]. All these methods are deep learning based. They are trained and tested in the same way as the deep shadow detectors. Table 2 shows the quantitative results on the three benchmark datasets. We can see that our method achieves the best BER scores over all state-of-the-art methods, on the three datasets. Compared to the second best-performing method, MTMT-Net [5], our method reduces the BER scores by 3.49%, 2.14% and 9.9% on SBU [40], UCF [54], and ISTD [41], respectively. Note that MTMT-Net is a semisupervised method that exploits both full labeled shadow images as well as extra unlabeled shadow images. Our proposed method does not require additional training data. This demonstrates the effectiveness of our proposed decomposition and reweighting scheme on mining both intensityvariant and intensity-invariant features.\n\n\nQuantitative Comparison\n\n\nQualitative Comparison\n\nWe further compare our method with the most recent shadow detectors qualitatively, as shown in Fig. 4. We can see that our method has clear visual advantages over existing shadow detection methods on challenging scenes. When shadows are cast on dark objects (e.g., first three rows) or regions with drastically varying colors (e.g., last three Table 2. Quantitative comparison of our method with the state-of-the-art methods on three shadow detection benchmark datasets. For each dataset, we list the error rates for shadow region and non-shadow region as well as the balanced error rate (BER). The best results are marked in bold. (*) MTMT is trained with extra unlabelled data; (**) DSD is trained with extra supervision from other models. SBU [40] UCF [54] ISTD [ rows), existing methods fail to differentiate such differences well, by either over-segmenting or under-segmenting the shadow regions. In contrast, our method detects the shadows more accurately with fine structures and details. This again demonstrates the effectiveness of our proposed decomposition and reweighting scheme in mitigating the intensity bias in shadow detection.\n\n\nFeature Visualisation\n\nIn Fig. 5, we give an example of feature visualization using GradCAM [34]. In this example, we show the original image and its two brightness shifted versions (one brighter and one darker). We can see that: (1) while the activation map of the intensity-invariant features (column 3) is uniformly distributed inside the shadow region, that of the intensity-variant features (column 4) is highly correlated to pixel intensity; and (2) under brightness shift, the activation of intensity-invariant features remains stable, but that of the intensity-variant features changes accordingly. These visualizations show that our method can successfully decompose these two types of features. More visualization results are provided in supplemental materials.\n\n\nAblation Study\n\nWe first perform an ablation study on the SBU dataset, to verify the design choices of our network, including the components for feature decomposition and different cumulative learning strategies. The average BER scores with CRF refinement are reported. We then analyze the model's sensitivity to \u00b5 in the test phase, to shed some light on the importance of feature reweighting.\n\n\nComponents for Feature Decomposition\n\nTo verify the effectiveness of the components used in the proposed feature decomposition, we compare the full model Table 3. Ablation study on the components of the feature decomposition. We show BER scores on SBU.\n\nBB\nLi Lv BER # basic \u21e5 \u21e5 \u21e5 3.32 basic+BB X \u21e5 \u21e5 3.32 basic+BB+Li X X \u21e5 3.24 basic+BB+Lv X \u21e5 X 3.36 well-posed Lv X X X 3.21 Ours X X X 3.04\n(Ours) with the following configurations:\n\n\u2022 Basic: we remove the bilateral branches after multi-level integrated feature extraction, and the two corresponding loss functions (i.e., L i and L v ) used for feature decomposition. This forms our baseline.\n\n\u2022 Basic+BB: we add the bilateral branches after the extraction of multi-level integrated features, without the two decomposition losses (i.e. L i and L v ).\n\n\u2022 Basic+BB+L i : we remove the L v loss for guiding intensity-variant projection, from our full model.\n\n\u2022 Basic+BB+L v : we remove the L i loss for guiding intensity-invariant projection, from our full model.\n\u2022 Well-posed L v : in our full model, we replace L v loss (Eq. 3) with L v = MAE ( (F 0 v F v ; \u2713), ).\nIt yields a deterministic pretext task where the brightness shift is predicted with reference to both the brightness shifted and the original image.\n\nAs shown in Table 3, with only the bilateral branches added, the detection accuracy shows no improvement. This means that a deeper architecture does not help improve detection accuracy. In addition, by adding only L i , the performance is improved as it moderately suppressed the ex-(a) Input Image (b) DSC [16] (c) ADNet [24] (d) BDRAR [55] (e) DSD [50] (f) MTMT [5] (g) ITSD [52] (h) Ours (i) GT cessive attention to intensity. In contrast, adding only L v further emphasizes on intensity, and the result gets worse. Overall, we can see that our improvement mainly comes from the use of paired self-supervision losses (both L v and L i ), instead of from just one of them. Intuitively, a joint usage of them is crucial for feature decomposition: it guarantees that the two feature maps F v and F i projected by the bilateral branches encode the expected information, as these two tasks are contrary and complementary (i.e., while one forces a part of the network to encode intensity related information, the other one imposes intensity-invariant constraint on the other part of the network). In contrast, if only one of them is used, as the bilateral branches stem from the same backbone, the constraint will affect both of the projected features, making it difficult to decompose them well.\n\nFinally, the last second row shows that using well-posed L v , i.e. predicting brightness shift with reference to both the brightness shifted and the original image, produces a performance similar to the case where only L i is added. This is because such a pretext task becomes too simple, as the network can even solve it by just trivially learning an identity mapping from input to the intensity-variant features. As a result, the intensity-variant features may fail to encode high-level semantics for shadow detection.\n\n\nDifferent Cumulative Learning Strategies\n\nWe observe from our experiment in Fig. 7 that if we just leave the weight parameter \u00b5 learnable (i.e., setting it differentiable [18]) or using an extra branch to predict it [19], \u00b5 will continuously increase to \u21e01 before the training ends. This verifies our observation that deep shadow detectors tends to be biased to the intensity-variant features. Intuitively, this is because CNNs are biased towards local features, and intensity is exactly a local feature. Besides, intensity is indeed the most evident cue for shadow detection. To avoid this, the cumulative learning strategy is introduced to gradually shift the learning focus of the network from the intensity-variant features to the intensity-invariant features. Table 4 compares different strategies for adjusting \u00b5 during the training stage. It is worth noting that decay strategies (i.e., linear decay and parabolic decay) perform better than increment strategies (i.e., linear increment and parabolic increment). This suggests that we should gradually shift the attention from intensity-variant features to intensity-invariant ones. In addition, the parabolic decay strategy gives better performance than the linear decay one.   This implies that a smooth transition at the beginning of training is important, as it allows the detector to thoroughly exploit intensity-variant features first.\n\n\nTest-phase Sensitivity to \u00b5\n\nWhile we have determined an optimal value for \u00b5 by grid search on the validation set (Sec. 3.2), we alter it here in the test stage to see how the shadow detection performance may respond to the change of \u00b5. In Fig. 6(left), the BER score with respect to the changing \u00b5 on SBU testset shows that there is an optimal value of \u00b5, which provides the best trade-off between intensity-variant and intensity-invariant features: putting more or less emphasis on the intensityvariant features will degrade the detection performance. Fig. 6(right) shows two visual examples to illustrate the effect of changing \u00b5. Note that setting \u00b5 = 1 means that only the intensity-variant features are used for shadow prediction, where the results are indeed susceptible to pixel intensity (e.g., the regions pointed to by arrows in Fig. 6(e)). In contrast, if we set \u00b5 = 0 (Fig. 6(d)), which means that only the intensity-invariant features are used, there is only a small proportion of shadow pixels being mis-classified as non-shadow pixels. This justifies that the intensity-invariant cues provide discriminative information for shadow detection. However, the best results are achieved when \u00b5 is set to neither extreme. Overall, these observations verify the importance of our idea to reallocate the attention between intensity-variant and intensity-invariant features. \n\n\nConclusion\n\nThis paper has presented a novel feature decomposition and reweighting scheme to mitigate the bias of deep shadow detectors to the intensity cue. The key idea is to decompose multi-level integrated features into intensityvariant and intensity-invariant components, and then reallocate the attention between them. We have introduced two auxiliary self-supervised tasks to guide the learning of this task-specific feature decomposition. Experimental results on three datasets show that our model achieves favorable performances, compared to the state-of-the-art methods.\n\nAlthough our deep shadow detector achieves a balanced exploitation of intensity-variant and intensity-invariant features, which helps mitigate the intensity bias, in inference time it still relies on the prior learned from the training set. Hence, it may fail if the illumination of the image strongly deviates from that of the training set, as shown in Fig. 8. As a future work, we plan to explore domain adaptation for shadow detection.\n\nFigure 3 .\n3Training and inference pipeline of the proposed method. Our network incorporates three modules: (a) Feature Extraction Subnetwork for extracting multi-level integrated features F; (b) Feature Decomposition and Reweighting Module for decomposing F into intensity-variant features Fv and intensity-invariant features Fi, which are further re-combined by a weighted summation to produce reweighted features Fr; and (c) Shadow Detection Head for predicting the shadow maskM. (d) In the training stage, we construct two auxiliary self-supervised tasks to guide the learning of feature decomposition: jointly optimizing two auxiliary self-supervision losses Lv and Li with the shadow detection loss Lce. Note that operation nodes with the same color share their parameters.\n\nFigure 4 .Figure 5 .\n45Qualitative comparison of the proposed method with the most recent state-of-the-art methods. Feature map visualization using GradCAM[34]. We show the original image (top row), its brighter counterpart (second row) and its darker counterpart (third row). From left to right: (a) input image, (b) F: features before decomposition, (c) Fi: intensity-invariant features, (d) Fv: intensity-variant features, (d) Fr: recombined features.\n\nFigure 6 .\n6Effect of varying \u00b5 in the test phase. Left: the BER score with respect to \u00b5 on SBU testset. Right: two visual examples. Setting \u00b5 = 0 blocks intensity-variant features, \u00b5 = 1 blocks intensity-invariant features, and \u00b5 = 0.4 provides best trade-off of using the two.\n\nFigure 7 .\n7A learnable \u00b5 continuously increases to a \u21e01, suggesting that the network is biased to the intensity-variant features. Forward: we add an extra branch consisting of global average pooling and a fully connected layer to predict \u00b5 from multi-layer integrated features. Backward: we set \u00b5 as a differentiable parameter and let it be optimized together with the network parameters.\n\nFigure 8 .\n8Failure case. It may fail on night-time image as the training set contains only day-time images.\n\nTable 1 .\n1EfficientNet-B3[36] has a similar classification performance to ResNext101[45] on ImageNet[8], but contains significantly fewer parameters and requires less computation.backbone \n#Params #FLOPS Top-1 Acc. Top-5 Acc. \nEfficientNet-B3 \n12M \n1.8 \n81.1 \n95.5 \nResNext101 \n84M \n32 \n80.9 \n95.6 \n\n4. Training and Test Strategies \n\nChoice of backbone. We first explain our choice of \nthe backbone network for extracting the multi-level in-\ntegrated features (MLIF). Recent state-of-the-art shadow \ndetectors [55, 50, 5] rely on a heavy backbone (e.g., \nResNext101 [45]) to extract the backbone features. How-\never, this is not suitable for us, as our method requires extra \nforward and backward pass on brightness shifted input in \ntraining stage. To maintain a stable training with a reason-\nable batch size, we choose the light-weight EfficientNet-\nB3 [36] as our backbone. Nonetheless, as shown in Ta-\nble 1, EfficientNet-B3 [36] has a similar classification per-\nformance to ResNext101 [45] on ImageNet \n\nTable 4 .\n4Ablation study of different cumulative strategies.Strategy \n\u00b5 \nBER # \nConstant \n0.5 \n3.45 \nBackward \n-\n3.47 \nForward \n-\n3.42 \nLinear increment \n\nT \nTmax \n\n3.63 \n\nParabolic increment \n( T \nTmax ) 2 \n3.73 \n\nLinear decay \n1 \n\nT \nTmax \n\n3.26 \n\nParabolic decay (Ours) 1 ( T \nTmax ) 2 \n3.04 \n\n\n\nThere are many consistent explanations of unlabeled data: Why you should average. Ben Athiwaratkun, Marc Finzi, Pavel Izmailov, Andrew Gordon Wilson, ICLR. Ben Athiwaratkun, Marc Finzi, Pavel Izmailov, and An- drew Gordon Wilson. There are many consistent explana- tions of unlabeled data: Why you should average. In ICLR, 2019. 5\n\nTing Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, arXiv:2002.05709A simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge- offrey Hinton. A simple framework for contrastive learning of visual representations. arXiv:2002.05709, 2020. 3\n\nBig self-supervised models are strong semi-supervised learners. Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, Geoffrey Hinton, arXiv:2006.10029Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big self-supervised mod- els are strong semi-supervised learners. arXiv:2006.10029, 2020. 3\n\nSelf-supervised gans via auxiliary rotation loss. Ting Chen, Xiaohua Zhai, Marvin Ritter, Mario Lucic, Neil Houlsby, CVPR. Ting Chen, Xiaohua Zhai, Marvin Ritter, Mario Lucic, and Neil Houlsby. Self-supervised gans via auxiliary rotation loss. In CVPR, 2019. 3\n\nA multi-task mean teacher for semisupervised shadow detection. Zhihao Chen, Lei Zhu, Liang Wan, Song Wang, Wei Feng, Pheng-Ann Heng, CVPR. 67Zhihao Chen, Lei Zhu, Liang Wan, Song Wang, Wei Feng, and Pheng-Ann Heng. A multi-task mean teacher for semi- supervised shadow detection. In CVPR, 2020. 1, 2, 4, 5, 6, 7\n\nWhy can't i dance in the mall? learning to mitigate scene bias in action recognition. Jinwoo Choi, Chen Gao, C E Joseph, Jia-Bin Messou, Huang, NeurIPS. Jinwoo Choi, Chen Gao, Joseph CE Messou, and Jia-Bin Huang. Why can't i dance in the mall? learning to mitigate scene bias in action recognition. In NeurIPS, 2019. 1\n\nDetecting moving objects, ghosts, and shadows in video streams. Rita Cucchiara, Costantino Grana, Massimo Piccardi, Andrea Prati, TPAMI. 2510Rita Cucchiara, Costantino Grana, Massimo Piccardi, and Andrea Prati. Detecting moving objects, ghosts, and shad- ows in video streams. TPAMI, 25(10):1337-1342, 2003. 1\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, CVPR. 45Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. 4, 5\n\nUnsupervised visual representation learning by context prediction. Carl Doersch, Abhinav Gupta, Alexei A Efros, ICCV. Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsuper- vised visual representation learning by context prediction. In ICCV, 2015. 3\n\nEntropy minimization for shadow removal. Graham D Finlayson, S Mark, Cheng Drew, Lu, IJCV. 2Graham D Finlayson, Mark S Drew, and Cheng Lu. Entropy minimization for shadow removal. IJCV, 2009. 2\n\nOn the removal of shadows from images. Graham D Finlayson, D Steven, Cheng Hordley, Mark S Lu, Drew, TPAMI. 2Graham D Finlayson, Steven D Hordley, Cheng Lu, and Mark S Drew. On the removal of shadows from images. TPAMI, 2005. 2\n\nImagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Wieland Felix A Wichmann, Brendel, ICLR. Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. In ICLR, 2019. 1\n\nUnsupervised representation learning by predicting image rotations. Spyros Gidaris, Praveer Singh, Nikos Komodakis, In ICLR. 3Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Un- supervised representation learning by predicting image rota- tions. In ICLR, 2018. 3\n\nSingle-image shadow detection and removal using paired regions. Ruiqi Guo, Qieyun Dai, Derek Hoiem, CVPR. 6Ruiqi Guo, Qieyun Dai, and Derek Hoiem. Single-image shadow detection and removal using paired regions. In CVPR, 2011. 2, 5, 6\n\nFast shadow detection from a single image using a patched convolutional neural network. Sepideh Hosseinzadeh, Moein Shakeri, Hong Zhang, In IROS. 56Sepideh Hosseinzadeh, Moein Shakeri, and Hong Zhang. Fast shadow detection from a single image using a patched convolutional neural network. In IROS, 2018. 5, 6\n\nDirection-aware spatial context features for shadow detection. Xiaowei Hu, Lei Zhu, Chi-Wing Fu, Jing Qin, Pheng-Ann Heng, CVPR. 67Xiaowei Hu, Lei Zhu, Chi-Wing Fu, Jing Qin, and Pheng- Ann Heng. Direction-aware spatial context features for shadow detection. In CVPR, 2018. 1, 2, 5, 6, 7\n\nWhat characterizes a shadow boundary under the sun and sky? In ICCV. Xiang Huang, Gang Hua, Jack Tumblin, Lance Williams, Xiang Huang, Gang Hua, Jack Tumblin, and Lance Williams. What characterizes a shadow boundary under the sun and sky? In ICCV, 2011. 2\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. Sergey Ioffe, Christian Szegedy, ICML. 47Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In ICML, pages 448-456, 2015. 4, 7\n\nFew-shot object detection via feature reweighting. Bingyi Kang, Zhuang Liu, Xin Wang, Fisher Yu, Jiashi Feng, Trevor Darrell, ICCV. 47Bingyi Kang, Zhuang Liu, Xin Wang, Fisher Yu, Jiashi Feng, and Trevor Darrell. Few-shot object detection via feature reweighting. In ICCV, 2019. 4, 7\n\nRendering synthetic objects into legacy photographs. Kevin Karsch, Varsha Hedau, David Forsyth, Derek Hoiem, Kevin Karsch, Varsha Hedau, David Forsyth, and Derek Hoiem. Rendering synthetic objects into legacy photographs.\n\n. Acm Tog, 30ACM TOG, 30(6):1-12, 2011. 1\n\nEfficient inference in fully connected crfs with gaussian edge potentials. Philipp Kr\u00e4henb\u00fchl, Vladlen Koltun, NeurIPS. Philipp Kr\u00e4henb\u00fchl and Vladlen Koltun. Efficient inference in fully connected crfs with gaussian edge potentials. In NeurIPS, 2011. 5\n\nEstimating natural illumination from a single outdoor image. Jean-Fran\u00e7ois Lalonde, Alexei A Efros, G Srinivasa, Narasimhan, ICCV. Jean-Fran\u00e7ois Lalonde, Alexei A Efros, and Srinivasa G Narasimhan. Estimating natural illumination from a single outdoor image. In ICCV, pages 183-190, 2009. 1\n\nDetecting ground shadows in outdoor consumer photographs. Jean-Fran\u00e7ois Lalonde, Alexei A Efros, G Srinivasa, Narasimhan, ECCV. Jean-Fran\u00e7ois Lalonde, Alexei A Efros, and Srinivasa G Narasimhan. Detecting ground shadows in outdoor con- sumer photographs. In ECCV, 2010. 2\n\nA+d net: Training a shadow detector with adversarial shadow attenuation. Hieu Le, Tomas F Yago Vicente, Vu Nguyen, Minh Hoai, Dimitris Samaras, ECCV. 67Hieu Le, Tomas F. Yago Vicente, Vu Nguyen, Minh Hoai, and Dimitris Samaras. A+d net: Training a shadow detector with adversarial shadow attenuation. In ECCV, 2018. 2, 5, 6, 7\n\nRethinking data augmentation: Self-supervision and self-distillation. Hankook Lee, Sung Ju Hwang, Jinwoo Shin, arXiv:1910.05872Hankook Lee, Sung Ju Hwang, and Jinwoo Shin. Rethink- ing data augmentation: Self-supervision and self-distillation. arXiv:1910.05872, 2019. 3\n\nKaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, CVPR. 23Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In CVPR, 2017. 2, 3\n\nVisual chirality. Zhiqiu Lin, Jin Sun, Abe Davis, Noah Snavely, CVPR. 2020Zhiqiu Lin, Jin Sun, Abe Davis, and Noah Snavely. Visual chirality. In CVPR, pages 12295-12303, 2020. 3\n\nPhysical models for moving shadow and object detection in video. Sohail Nadimi, Bir Bhanu, TPAMISohail Nadimi and Bir Bhanu. Physical models for moving shadow and object detection in video. TPAMI, 26(8):1079- 1087, 2004. 1\n\nShadow detection with conditional generative adversarial networks. Tomas F Vu Nguyen, Maozheng Yago Vicente, Minh Zhao, Dimitris Hoai, Samaras, ICCV. Vu Nguyen, Tomas F. Yago Vicente, Maozheng Zhao, Minh Hoai, and Dimitris Samaras. Shadow detection with condi- tional generative adversarial networks. In ICCV, 2017. 2\n\nShadow detection with conditional generative adversarial networks. Tomas F Yago Vu Nguyen, Maozheng Vicente, Minh Zhao, Dimitris Hoai, Samaras, ICCV. 56Vu Nguyen, Tomas F Yago Vicente, Maozheng Zhao, Minh Hoai, and Dimitris Samaras. Shadow detection with condi- tional generative adversarial networks. In ICCV, 2017. 5, 6\n\nUnsupervised learning of visual representations by solving jigsaw puzzles. Mehdi Noroozi, Paolo Favaro, ECCV. Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In ECCV, 2016. 3\n\nMulti-view relighting using a geometry-aware network. Julien Philip, Micha\u00ebl Gharbi, Tinghui Zhou, Alexei A Efros, George Drettakis, ACM TOG. 3Julien Philip, Micha\u00ebl Gharbi, Tinghui Zhou, Alexei A Efros, and George Drettakis. Multi-view relighting using a geometry-aware network. ACM TOG, 2019. 3\n\nShadow detection: A survey and comparative evaluation of recent methods. Andres Sanin, Conrad Sanderson, Brian C Lovell, Pattern Recognition. 1Andres Sanin, Conrad Sanderson, and Brian C Lovell. Shadow detection: A survey and comparative evaluation of recent methods. Pattern Recognition, 2012. 1\n\nGrad-cam: Visual explanations from deep networks via gradient-based localization. R Ramprasaath, Michael Selvaraju, Abhishek Cogswell, Ramakrishna Das, Devi Vedantam, Dhruv Parikh, Batra, ICCV. 67Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In ICCV, pages 618-626, 2017. 6, 7\n\nDropout: a simple way to prevent neural networks from overfitting. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, The Journal of Machine Learning Research. 151Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research, 15(1):1929-1958, 2014. 4\n\nEfficientnet: Rethinking model scaling for convolutional neural networks. Mingxing Tan, Quoc Le, ICML. Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In ICML, pages 6105-6114, 2019. 4\n\nMean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Antti Tarvainen, Harri Valpola, NeurIPS. Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In NeurIPS, 2017. 2\n\nNew spectrum ratio properties and features for shadow detection. Jiandong Tian, Xiaojun Qi, Liangqiong Qu, Yandong Tang, Pattern Recognition. 2Jiandong Tian, Xiaojun Qi, Liangqiong Qu, and Yandong Tang. New spectrum ratio properties and features for shadow detection. Pattern Recognition, 2016. 2\n\nLeave-one-out kernel optimization for shadow detection and removal. Tomas F Yago Vicente, Minh Hoai, Dimitris Samaras, TPAMI. 2Tomas F Yago Vicente, Minh Hoai, and Dimitris Samaras. Leave-one-out kernel optimization for shadow detection and removal. TPAMI, 2017. 2\n\nLarge-scale training of shadow detectors with noisily-annotated shadow examples. Tom\u00e1s F Yago, Le Vicente, Chen-Ping Hou, Minh Yu, Dimitris Hoai, Samaras, ECCV. 6Tom\u00e1s F Yago Vicente, Le Hou, Chen-Ping Yu, Minh Hoai, and Dimitris Samaras. Large-scale training of shadow de- tectors with noisily-annotated shadow examples. In ECCV, 2016. 2, 5, 6\n\nStacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal. Jifeng Wang, Xiang Li, Jian Yang, CVPR. 6Jifeng Wang, Xiang Li, and Jian Yang. Stacked conditional generative adversarial networks for jointly learning shadow detection and shadow removal. In CVPR, 2018. 2, 5, 6\n\nA stagewise refinement model for detecting salient objects in images. Tiantian Wang, Ali Borji, Lihe Zhang, Pingping Zhang, Huchuan Lu, ICCV. 56Tiantian Wang, Ali Borji, Lihe Zhang, Pingping Zhang, and Huchuan Lu. A stagewise refinement model for detecting salient objects in images. In ICCV, 2017. 5, 6\n\nDensely cascaded shadow detection network via deeply supervised parallel fusion. Yupei Wang, Xin Zhao, Yin Li, Xuecai Hu, Kaiqi Huang, IJCAI. 56Yupei Wang, Xin Zhao, Yin Li, Xuecai Hu, and Kaiqi Huang. Densely cascaded shadow detection network via deeply su- pervised parallel fusion. In IJCAI, 2018. 5, 6\n\nUnsupervised feature learning via non-parametric instance discrimination. Zhirong Wu, Yuanjun Xiong, X Stella, Dahua Yu, Lin, CVPR. Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In CVPR, 2018. 3\n\nAggregated residual transformations for deep neural networks. Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, Kaiming He, CVPR. Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In CVPR, 2017. 4\n\nAmulet: Aggregating multi-level convolutional features for salient object detection. Pingping Zhang, Dong Wang, Huchuan Lu, Hongyu Wang, Xiang Ruan, ICCV. 56Pingping Zhang, Dong Wang, Huchuan Lu, Hongyu Wang, and Xiang Ruan. Amulet: Aggregating multi-level convolu- tional features for salient object detection. In ICCV, 2017. 5, 6\n\nColorful image colorization. Richard Zhang, Phillip Isola, Alexei A Efros, ECCV. Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016. 3\n\nPyramid scene parsing network. Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia, CVPR. 56Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In CVPR, 2017. 5, 6\n\nEgnet: Edge guidance network for salient object detection. Jia-Xing Zhao, Jiang-Jiang Liu, Deng-Ping Fan, Yang Cao, Jufeng Yang, Ming-Ming Cheng, ICCV. 56Jia-Xing Zhao, Jiang-Jiang Liu, Deng-Ping Fan, Yang Cao, Jufeng Yang, and Ming-Ming Cheng. Egnet: Edge guidance network for salient object detection. In ICCV, 2019. 5, 6\n\nDistraction-aware shadow detection. Quanlong Zheng, Xiaotian Qiao, Ying Cao, Rynson W H Lau, CVPR. 67Quanlong Zheng, Xiaotian Qiao, Ying Cao, and Ryn- son W.H. Lau. Distraction-aware shadow detection. In CVPR, 2019. 1, 2, 4, 5, 6, 7\n\nBbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition. Boyan Zhou, Quan Cui, Xiu-Shen Wei, Zhao-Min Chen, CVPR, 2020. 24Boyan Zhou, Quan Cui, Xiu-Shen Wei, and Zhao-Min Chen. Bbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition. In CVPR, 2020. 2, 4\n\nInteractive two-stream decoder for accurate and fast saliency detection. Huajun Zhou, Xiaohua Xie, Jian-Huang Lai, Zixuan Chen, Lingxiao Yang, CVPR, 2020. 5. 67Huajun Zhou, Xiaohua Xie, Jian-Huang Lai, Zixuan Chen, and Lingxiao Yang. Interactive two-stream decoder for ac- curate and fast saliency detection. In CVPR, 2020. 5, 6, 7\n\nLearning to recognize shadows in monochromatic natural images. Jiejie Zhu, G G Kegan, Samuel, Z Syed, Marshall F Masood, Tappen, CVPR. Jiejie Zhu, Kegan GG Samuel, Syed Z Masood, and Mar- shall F Tappen. Learning to recognize shadows in monochro- matic natural images. In CVPR, 2010. 2\n\nLearning to recognize shadows in monochromatic natural images. Jiejie Zhu, G G Kegan, Samuel, Z Syed, Marshall F Masood, Tappen, CVPR. 56Jiejie Zhu, Kegan GG Samuel, Syed Z Masood, and Mar- shall F Tappen. Learning to recognize shadows in monochro- matic natural images. In CVPR, 2010. 5, 6\n\nBidirectional feature pyramid network with recurrent attention residual modules for shadow detection. Lei Zhu, Zijun Deng, Xiaowei Hu, Chi-Wing Fu, Xuemiao Xu, Jing Qin, Pheng-Ann Heng, ECCV. 67Lei Zhu, Zijun Deng, Xiaowei Hu, Chi-Wing Fu, Xuemiao Xu, Jing Qin, and Pheng-Ann Heng. Bidirectional feature pyramid network with recurrent attention residual modules for shadow detection. In ECCV, 2018. 2, 4, 5, 6, 7\n", "annotations": {"author": "[{\"end\":130,\"start\":91},{\"end\":169,\"start\":131},{\"end\":242,\"start\":170},{\"end\":313,\"start\":243}]", "publisher": null, "author_last_name": "[{\"end\":98,\"start\":95},{\"end\":136,\"start\":134},{\"end\":181,\"start\":179},{\"end\":257,\"start\":254}]", "author_first_name": "[{\"end\":94,\"start\":91},{\"end\":133,\"start\":131},{\"end\":178,\"start\":170},{\"end\":249,\"start\":243},{\"end\":253,\"start\":250}]", "author_affiliation": "[{\"end\":129,\"start\":100},{\"end\":168,\"start\":138},{\"end\":241,\"start\":212},{\"end\":312,\"start\":283}]", "title": "[{\"end\":88,\"start\":1},{\"end\":401,\"start\":314}]", "venue": null, "abstract": "[{\"end\":1321,\"start\":431}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b19\"},\"end\":1526,\"start\":1522},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":1529,\"start\":1526},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1601,\"start\":1598},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":1604,\"start\":1601},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2445,\"start\":2441},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":2458,\"start\":2454},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2471,\"start\":2468},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2665,\"start\":2661},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":2668,\"start\":2665},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2670,\"start\":2668},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3568,\"start\":3564},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3718,\"start\":3714},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3906,\"start\":3903},{\"end\":4122,\"start\":4114},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":4475,\"start\":4471},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":4507,\"start\":4503},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4859,\"start\":4856},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4978,\"start\":4974},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5074,\"start\":5071},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":6411,\"start\":6407},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7202,\"start\":7198},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7205,\"start\":7202},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7208,\"start\":7205},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7275,\"start\":7271},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":7278,\"start\":7275},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7281,\"start\":7278},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7358,\"start\":7354},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7361,\"start\":7358},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7364,\"start\":7361},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7367,\"start\":7364},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7370,\"start\":7367},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7381,\"start\":7377},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7384,\"start\":7381},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":7387,\"start\":7384},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7390,\"start\":7387},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7406,\"start\":7402},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7409,\"start\":7406},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":7412,\"start\":7409},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7415,\"start\":7412},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7418,\"start\":7415},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":7436,\"start\":7432},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7439,\"start\":7436},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7442,\"start\":7439},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7831,\"start\":7827},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7902,\"start\":7898},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8011,\"start\":8007},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":8158,\"start\":8154},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8245,\"start\":8241},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8344,\"start\":8340},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8511,\"start\":8508},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8554,\"start\":8550},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9631,\"start\":9628},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9660,\"start\":9656},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9679,\"start\":9675},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9729,\"start\":9725},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9731,\"start\":9729},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9734,\"start\":9731},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9853,\"start\":9849},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9855,\"start\":9853},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9857,\"start\":9855},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":13007,\"start\":13003},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":13009,\"start\":13007},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":13012,\"start\":13009},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13025,\"start\":13021},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":13205,\"start\":13201},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14513,\"start\":14509},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14839,\"start\":14835},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14883,\"start\":14879},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":15233,\"start\":15229},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":15958,\"start\":15954},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16285,\"start\":16282},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":16397,\"start\":16393},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":17690,\"start\":17686},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17692,\"start\":17690},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":17695,\"start\":17692},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":17698,\"start\":17695},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17766,\"start\":17763},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":18453,\"start\":18449},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":18467,\"start\":18463},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18542,\"start\":18539},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":18792,\"start\":18788},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":18816,\"start\":18812},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18874,\"start\":18870},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":18877,\"start\":18874},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":18880,\"start\":18877},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":18883,\"start\":18880},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":18885,\"start\":18883},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18922,\"start\":18918},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":19234,\"start\":19230},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":19244,\"start\":19240},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":19258,\"start\":19254},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20100,\"start\":20097},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":20110,\"start\":20106},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":20124,\"start\":20120},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":20137,\"start\":20133},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20147,\"start\":20143},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":20159,\"start\":20155},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":20173,\"start\":20169},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":20191,\"start\":20187},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":20203,\"start\":20199},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":20221,\"start\":20217},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20246,\"start\":20242},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20319,\"start\":20315},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":20611,\"start\":20607},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":20622,\"start\":20618},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":20632,\"start\":20628},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":20649,\"start\":20645},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":20700,\"start\":20696},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":21064,\"start\":21061},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":21136,\"start\":21132},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":21146,\"start\":21142},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":21161,\"start\":21157},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":22331,\"start\":22327},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":22340,\"start\":22336},{\"end\":22347,\"start\":22346},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":22824,\"start\":22820},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":25477,\"start\":25473},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25492,\"start\":25488},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":25507,\"start\":25503},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":25520,\"start\":25516},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":25533,\"start\":25530},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":25547,\"start\":25543},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":27160,\"start\":27156},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":27205,\"start\":27201},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":31731,\"start\":31727},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":32839,\"start\":32835},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":32898,\"start\":32894},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":32913,\"start\":32910}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":31570,\"start\":30790},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32026,\"start\":31571},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32306,\"start\":32027},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32697,\"start\":32307},{\"attributes\":{\"id\":\"fig_4\"},\"end\":32807,\"start\":32698},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":33819,\"start\":32808},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":34119,\"start\":33820}]", "paragraph": "[{\"end\":1641,\"start\":1337},{\"end\":2415,\"start\":1643},{\"end\":3130,\"start\":2417},{\"end\":4001,\"start\":3132},{\"end\":4508,\"start\":4003},{\"end\":5262,\"start\":4510},{\"end\":5717,\"start\":5264},{\"end\":6569,\"start\":5719},{\"end\":6813,\"start\":6571},{\"end\":6946,\"start\":6815},{\"end\":7087,\"start\":6948},{\"end\":7727,\"start\":7105},{\"end\":8684,\"start\":7729},{\"end\":9177,\"start\":8686},{\"end\":9979,\"start\":9179},{\"end\":10445,\"start\":9981},{\"end\":11019,\"start\":10465},{\"end\":11325,\"start\":11061},{\"end\":11441,\"start\":11327},{\"end\":11735,\"start\":11457},{\"end\":12147,\"start\":11910},{\"end\":12233,\"start\":12177},{\"end\":12695,\"start\":12235},{\"end\":13768,\"start\":12727},{\"end\":14039,\"start\":13770},{\"end\":14697,\"start\":14087},{\"end\":15416,\"start\":14725},{\"end\":15667,\"start\":15440},{\"end\":15897,\"start\":15669},{\"end\":16370,\"start\":15899},{\"end\":16882,\"start\":16372},{\"end\":17058,\"start\":16884},{\"end\":17481,\"start\":17131},{\"end\":17615,\"start\":17518},{\"end\":18018,\"start\":17617},{\"end\":18218,\"start\":18020},{\"end\":18817,\"start\":18220},{\"end\":19097,\"start\":18819},{\"end\":19694,\"start\":19146},{\"end\":19809,\"start\":19696},{\"end\":19983,\"start\":19868},{\"end\":20360,\"start\":20011},{\"end\":21528,\"start\":20362},{\"end\":22725,\"start\":21581},{\"end\":23499,\"start\":22751},{\"end\":23896,\"start\":23518},{\"end\":24151,\"start\":23937},{\"end\":24155,\"start\":24153},{\"end\":24333,\"start\":24292},{\"end\":24544,\"start\":24335},{\"end\":24702,\"start\":24546},{\"end\":24806,\"start\":24704},{\"end\":24912,\"start\":24808},{\"end\":25164,\"start\":25016},{\"end\":26459,\"start\":25166},{\"end\":26982,\"start\":26461},{\"end\":28382,\"start\":27027},{\"end\":29766,\"start\":28414},{\"end\":30349,\"start\":29781},{\"end\":30789,\"start\":30351}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11456,\"start\":11442},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11909,\"start\":11736},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12176,\"start\":12148},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12726,\"start\":12696},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14724,\"start\":14698},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15439,\"start\":15417},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17130,\"start\":17059},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17517,\"start\":17482},{\"attributes\":{\"id\":\"formula_8\"},\"end\":19867,\"start\":19810},{\"attributes\":{\"id\":\"formula_9\"},\"end\":24291,\"start\":24156},{\"attributes\":{\"id\":\"formula_10\"},\"end\":25015,\"start\":24913}]", "table_ref": "[{\"end\":20826,\"start\":20819},{\"end\":21932,\"start\":21925},{\"end\":24060,\"start\":24053},{\"end\":25185,\"start\":25178},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":27757,\"start\":27750}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1335,\"start\":1323},{\"attributes\":{\"n\":\"2.\"},\"end\":7103,\"start\":7090},{\"attributes\":{\"n\":\"3.\"},\"end\":10463,\"start\":10448},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11059,\"start\":11022},{\"attributes\":{\"n\":\"3.2.\"},\"end\":14085,\"start\":14042},{\"attributes\":{\"n\":\"5.\"},\"end\":19111,\"start\":19100},{\"attributes\":{\"n\":\"5.1.\"},\"end\":19144,\"start\":19114},{\"end\":20009,\"start\":19986},{\"attributes\":{\"n\":\"5.2.\"},\"end\":21554,\"start\":21531},{\"attributes\":{\"n\":\"5.3.\"},\"end\":21579,\"start\":21557},{\"attributes\":{\"n\":\"5.4.\"},\"end\":22749,\"start\":22728},{\"attributes\":{\"n\":\"5.5.\"},\"end\":23516,\"start\":23502},{\"attributes\":{\"n\":\"5.5.1\"},\"end\":23935,\"start\":23899},{\"attributes\":{\"n\":\"5.5.2\"},\"end\":27025,\"start\":26985},{\"attributes\":{\"n\":\"5.5.3\"},\"end\":28412,\"start\":28385},{\"attributes\":{\"n\":\"6.\"},\"end\":29779,\"start\":29769},{\"end\":30801,\"start\":30791},{\"end\":31592,\"start\":31572},{\"end\":32038,\"start\":32028},{\"end\":32318,\"start\":32308},{\"end\":32709,\"start\":32699},{\"end\":32818,\"start\":32809},{\"end\":33830,\"start\":33821}]", "table": "[{\"end\":33819,\"start\":32989},{\"end\":34119,\"start\":33882}]", "figure_caption": "[{\"end\":31570,\"start\":30803},{\"end\":32026,\"start\":31595},{\"end\":32306,\"start\":32040},{\"end\":32697,\"start\":32320},{\"end\":32807,\"start\":32711},{\"end\":32989,\"start\":32820},{\"end\":33882,\"start\":33832}]", "figure_ref": "[{\"end\":2496,\"start\":2488},{\"end\":3161,\"start\":3149},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3216,\"start\":3203},{\"end\":3367,\"start\":3347},{\"end\":4767,\"start\":4761},{\"end\":5261,\"start\":5255},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10931,\"start\":10925},{\"end\":21682,\"start\":21676},{\"end\":22760,\"start\":22754},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27067,\"start\":27061},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":28637,\"start\":28625},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":28945,\"start\":28939},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":29234,\"start\":29225},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":29276,\"start\":29266},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":30711,\"start\":30705}]", "bib_author_first_name": "[{\"end\":34206,\"start\":34203},{\"end\":34225,\"start\":34221},{\"end\":34238,\"start\":34233},{\"end\":34255,\"start\":34249},{\"end\":34262,\"start\":34256},{\"end\":34457,\"start\":34453},{\"end\":34469,\"start\":34464},{\"end\":34489,\"start\":34481},{\"end\":34507,\"start\":34499},{\"end\":34838,\"start\":34834},{\"end\":34850,\"start\":34845},{\"end\":34867,\"start\":34862},{\"end\":34885,\"start\":34877},{\"end\":34903,\"start\":34895},{\"end\":35157,\"start\":35153},{\"end\":35171,\"start\":35164},{\"end\":35184,\"start\":35178},{\"end\":35198,\"start\":35193},{\"end\":35210,\"start\":35206},{\"end\":35434,\"start\":35428},{\"end\":35444,\"start\":35441},{\"end\":35455,\"start\":35450},{\"end\":35465,\"start\":35461},{\"end\":35475,\"start\":35472},{\"end\":35491,\"start\":35482},{\"end\":35770,\"start\":35764},{\"end\":35781,\"start\":35777},{\"end\":35788,\"start\":35787},{\"end\":35790,\"start\":35789},{\"end\":35806,\"start\":35799},{\"end\":36066,\"start\":36062},{\"end\":36088,\"start\":36078},{\"end\":36103,\"start\":36096},{\"end\":36120,\"start\":36114},{\"end\":36365,\"start\":36362},{\"end\":36375,\"start\":36372},{\"end\":36389,\"start\":36382},{\"end\":36404,\"start\":36398},{\"end\":36412,\"start\":36409},{\"end\":36419,\"start\":36417},{\"end\":36653,\"start\":36649},{\"end\":36670,\"start\":36663},{\"end\":36684,\"start\":36678},{\"end\":36686,\"start\":36685},{\"end\":36898,\"start\":36897},{\"end\":36910,\"start\":36905},{\"end\":37091,\"start\":37090},{\"end\":37105,\"start\":37100},{\"end\":37121,\"start\":37115},{\"end\":37372,\"start\":37366},{\"end\":37390,\"start\":37382},{\"end\":37407,\"start\":37400},{\"end\":37427,\"start\":37419},{\"end\":37443,\"start\":37436},{\"end\":37784,\"start\":37778},{\"end\":37801,\"start\":37794},{\"end\":37814,\"start\":37809},{\"end\":38047,\"start\":38042},{\"end\":38059,\"start\":38053},{\"end\":38070,\"start\":38065},{\"end\":38308,\"start\":38301},{\"end\":38328,\"start\":38323},{\"end\":38342,\"start\":38338},{\"end\":38593,\"start\":38586},{\"end\":38601,\"start\":38598},{\"end\":38615,\"start\":38607},{\"end\":38624,\"start\":38620},{\"end\":38639,\"start\":38630},{\"end\":38886,\"start\":38881},{\"end\":38898,\"start\":38894},{\"end\":38908,\"start\":38904},{\"end\":38923,\"start\":38918},{\"end\":39169,\"start\":39163},{\"end\":39186,\"start\":39177},{\"end\":39429,\"start\":39423},{\"end\":39442,\"start\":39436},{\"end\":39451,\"start\":39448},{\"end\":39464,\"start\":39458},{\"end\":39475,\"start\":39469},{\"end\":39488,\"start\":39482},{\"end\":39715,\"start\":39710},{\"end\":39730,\"start\":39724},{\"end\":39743,\"start\":39738},{\"end\":39758,\"start\":39753},{\"end\":39885,\"start\":39882},{\"end\":40005,\"start\":39998},{\"end\":40025,\"start\":40018},{\"end\":40252,\"start\":40239},{\"end\":40268,\"start\":40262},{\"end\":40270,\"start\":40269},{\"end\":40279,\"start\":40278},{\"end\":40541,\"start\":40528},{\"end\":40557,\"start\":40551},{\"end\":40559,\"start\":40558},{\"end\":40568,\"start\":40567},{\"end\":40820,\"start\":40816},{\"end\":40830,\"start\":40825},{\"end\":40837,\"start\":40831},{\"end\":40849,\"start\":40847},{\"end\":40862,\"start\":40858},{\"end\":40877,\"start\":40869},{\"end\":41148,\"start\":41141},{\"end\":41158,\"start\":41154},{\"end\":41161,\"start\":41159},{\"end\":41175,\"start\":41169},{\"end\":41448,\"start\":41440},{\"end\":41459,\"start\":41454},{\"end\":41472,\"start\":41468},{\"end\":41677,\"start\":41671},{\"end\":41686,\"start\":41683},{\"end\":41695,\"start\":41692},{\"end\":41707,\"start\":41703},{\"end\":41903,\"start\":41897},{\"end\":41915,\"start\":41912},{\"end\":42128,\"start\":42123},{\"end\":42130,\"start\":42129},{\"end\":42150,\"start\":42142},{\"end\":42169,\"start\":42165},{\"end\":42184,\"start\":42176},{\"end\":42454,\"start\":42442},{\"end\":42474,\"start\":42466},{\"end\":42488,\"start\":42484},{\"end\":42503,\"start\":42495},{\"end\":42778,\"start\":42773},{\"end\":42793,\"start\":42788},{\"end\":42993,\"start\":42987},{\"end\":43009,\"start\":43002},{\"end\":43025,\"start\":43018},{\"end\":43038,\"start\":43032},{\"end\":43040,\"start\":43039},{\"end\":43054,\"start\":43048},{\"end\":43310,\"start\":43304},{\"end\":43324,\"start\":43318},{\"end\":43341,\"start\":43336},{\"end\":43343,\"start\":43342},{\"end\":43612,\"start\":43611},{\"end\":43633,\"start\":43626},{\"end\":43653,\"start\":43645},{\"end\":43675,\"start\":43664},{\"end\":43685,\"start\":43681},{\"end\":43701,\"start\":43696},{\"end\":44025,\"start\":44019},{\"end\":44046,\"start\":44038},{\"end\":44059,\"start\":44055},{\"end\":44076,\"start\":44072},{\"end\":44094,\"start\":44088},{\"end\":44467,\"start\":44459},{\"end\":44477,\"start\":44473},{\"end\":44749,\"start\":44744},{\"end\":44766,\"start\":44761},{\"end\":45035,\"start\":45027},{\"end\":45049,\"start\":45042},{\"end\":45064,\"start\":45054},{\"end\":45076,\"start\":45069},{\"end\":45340,\"start\":45328},{\"end\":45354,\"start\":45350},{\"end\":45369,\"start\":45361},{\"end\":45623,\"start\":45621},{\"end\":45642,\"start\":45633},{\"end\":45652,\"start\":45648},{\"end\":45665,\"start\":45657},{\"end\":45988,\"start\":45982},{\"end\":46000,\"start\":45995},{\"end\":46009,\"start\":46005},{\"end\":46273,\"start\":46265},{\"end\":46283,\"start\":46280},{\"end\":46295,\"start\":46291},{\"end\":46311,\"start\":46303},{\"end\":46326,\"start\":46319},{\"end\":46586,\"start\":46581},{\"end\":46596,\"start\":46593},{\"end\":46606,\"start\":46603},{\"end\":46617,\"start\":46611},{\"end\":46627,\"start\":46622},{\"end\":46888,\"start\":46881},{\"end\":46900,\"start\":46893},{\"end\":46909,\"start\":46908},{\"end\":46923,\"start\":46918},{\"end\":47155,\"start\":47148},{\"end\":47165,\"start\":47161},{\"end\":47181,\"start\":47176},{\"end\":47197,\"start\":47190},{\"end\":47209,\"start\":47202},{\"end\":47463,\"start\":47455},{\"end\":47475,\"start\":47471},{\"end\":47489,\"start\":47482},{\"end\":47500,\"start\":47494},{\"end\":47512,\"start\":47507},{\"end\":47739,\"start\":47732},{\"end\":47754,\"start\":47747},{\"end\":47768,\"start\":47762},{\"end\":47770,\"start\":47769},{\"end\":47922,\"start\":47912},{\"end\":47937,\"start\":47929},{\"end\":47951,\"start\":47943},{\"end\":47964,\"start\":47956},{\"end\":47976,\"start\":47971},{\"end\":48183,\"start\":48175},{\"end\":48201,\"start\":48190},{\"end\":48216,\"start\":48207},{\"end\":48226,\"start\":48222},{\"end\":48238,\"start\":48232},{\"end\":48254,\"start\":48245},{\"end\":48485,\"start\":48477},{\"end\":48501,\"start\":48493},{\"end\":48512,\"start\":48508},{\"end\":48524,\"start\":48518},{\"end\":48528,\"start\":48525},{\"end\":48771,\"start\":48766},{\"end\":48782,\"start\":48778},{\"end\":48796,\"start\":48788},{\"end\":48810,\"start\":48802},{\"end\":49077,\"start\":49071},{\"end\":49091,\"start\":49084},{\"end\":49107,\"start\":49097},{\"end\":49119,\"start\":49113},{\"end\":49134,\"start\":49126},{\"end\":49400,\"start\":49394},{\"end\":49407,\"start\":49406},{\"end\":49409,\"start\":49408},{\"end\":49426,\"start\":49425},{\"end\":49441,\"start\":49433},{\"end\":49443,\"start\":49442},{\"end\":49687,\"start\":49681},{\"end\":49694,\"start\":49693},{\"end\":49696,\"start\":49695},{\"end\":49713,\"start\":49712},{\"end\":49728,\"start\":49720},{\"end\":49730,\"start\":49729},{\"end\":50015,\"start\":50012},{\"end\":50026,\"start\":50021},{\"end\":50040,\"start\":50033},{\"end\":50053,\"start\":50045},{\"end\":50065,\"start\":50058},{\"end\":50074,\"start\":50070},{\"end\":50089,\"start\":50080}]", "bib_author_last_name": "[{\"end\":34219,\"start\":34207},{\"end\":34231,\"start\":34226},{\"end\":34247,\"start\":34239},{\"end\":34269,\"start\":34263},{\"end\":34462,\"start\":34458},{\"end\":34479,\"start\":34470},{\"end\":34497,\"start\":34490},{\"end\":34514,\"start\":34508},{\"end\":34843,\"start\":34839},{\"end\":34860,\"start\":34851},{\"end\":34875,\"start\":34868},{\"end\":34893,\"start\":34886},{\"end\":34910,\"start\":34904},{\"end\":35162,\"start\":35158},{\"end\":35176,\"start\":35172},{\"end\":35191,\"start\":35185},{\"end\":35204,\"start\":35199},{\"end\":35218,\"start\":35211},{\"end\":35439,\"start\":35435},{\"end\":35448,\"start\":35445},{\"end\":35459,\"start\":35456},{\"end\":35470,\"start\":35466},{\"end\":35480,\"start\":35476},{\"end\":35496,\"start\":35492},{\"end\":35775,\"start\":35771},{\"end\":35785,\"start\":35782},{\"end\":35797,\"start\":35791},{\"end\":35813,\"start\":35807},{\"end\":35820,\"start\":35815},{\"end\":36076,\"start\":36067},{\"end\":36094,\"start\":36089},{\"end\":36112,\"start\":36104},{\"end\":36126,\"start\":36121},{\"end\":36370,\"start\":36366},{\"end\":36380,\"start\":36376},{\"end\":36396,\"start\":36390},{\"end\":36407,\"start\":36405},{\"end\":36415,\"start\":36413},{\"end\":36427,\"start\":36420},{\"end\":36661,\"start\":36654},{\"end\":36676,\"start\":36671},{\"end\":36692,\"start\":36687},{\"end\":36895,\"start\":36877},{\"end\":36903,\"start\":36899},{\"end\":36915,\"start\":36911},{\"end\":36919,\"start\":36917},{\"end\":37088,\"start\":37070},{\"end\":37098,\"start\":37092},{\"end\":37113,\"start\":37106},{\"end\":37124,\"start\":37122},{\"end\":37130,\"start\":37126},{\"end\":37380,\"start\":37373},{\"end\":37398,\"start\":37391},{\"end\":37417,\"start\":37408},{\"end\":37434,\"start\":37428},{\"end\":37460,\"start\":37444},{\"end\":37469,\"start\":37462},{\"end\":37792,\"start\":37785},{\"end\":37807,\"start\":37802},{\"end\":37824,\"start\":37815},{\"end\":38051,\"start\":38048},{\"end\":38063,\"start\":38060},{\"end\":38076,\"start\":38071},{\"end\":38321,\"start\":38309},{\"end\":38336,\"start\":38329},{\"end\":38348,\"start\":38343},{\"end\":38596,\"start\":38594},{\"end\":38605,\"start\":38602},{\"end\":38618,\"start\":38616},{\"end\":38628,\"start\":38625},{\"end\":38644,\"start\":38640},{\"end\":38892,\"start\":38887},{\"end\":38902,\"start\":38899},{\"end\":38916,\"start\":38909},{\"end\":38932,\"start\":38924},{\"end\":39175,\"start\":39170},{\"end\":39194,\"start\":39187},{\"end\":39434,\"start\":39430},{\"end\":39446,\"start\":39443},{\"end\":39456,\"start\":39452},{\"end\":39467,\"start\":39465},{\"end\":39480,\"start\":39476},{\"end\":39496,\"start\":39489},{\"end\":39722,\"start\":39716},{\"end\":39736,\"start\":39731},{\"end\":39751,\"start\":39744},{\"end\":39764,\"start\":39759},{\"end\":39889,\"start\":39886},{\"end\":40016,\"start\":40006},{\"end\":40032,\"start\":40026},{\"end\":40260,\"start\":40253},{\"end\":40276,\"start\":40271},{\"end\":40289,\"start\":40280},{\"end\":40301,\"start\":40291},{\"end\":40549,\"start\":40542},{\"end\":40565,\"start\":40560},{\"end\":40578,\"start\":40569},{\"end\":40590,\"start\":40580},{\"end\":40823,\"start\":40821},{\"end\":40845,\"start\":40838},{\"end\":40856,\"start\":40850},{\"end\":40867,\"start\":40863},{\"end\":40885,\"start\":40878},{\"end\":41152,\"start\":41149},{\"end\":41167,\"start\":41162},{\"end\":41180,\"start\":41176},{\"end\":41452,\"start\":41449},{\"end\":41466,\"start\":41460},{\"end\":41481,\"start\":41473},{\"end\":41681,\"start\":41678},{\"end\":41690,\"start\":41687},{\"end\":41701,\"start\":41696},{\"end\":41715,\"start\":41708},{\"end\":41910,\"start\":41904},{\"end\":41921,\"start\":41916},{\"end\":42140,\"start\":42131},{\"end\":42163,\"start\":42151},{\"end\":42174,\"start\":42170},{\"end\":42189,\"start\":42185},{\"end\":42198,\"start\":42191},{\"end\":42464,\"start\":42455},{\"end\":42482,\"start\":42475},{\"end\":42493,\"start\":42489},{\"end\":42508,\"start\":42504},{\"end\":42517,\"start\":42510},{\"end\":42786,\"start\":42779},{\"end\":42800,\"start\":42794},{\"end\":43000,\"start\":42994},{\"end\":43016,\"start\":43010},{\"end\":43030,\"start\":43026},{\"end\":43046,\"start\":43041},{\"end\":43064,\"start\":43055},{\"end\":43316,\"start\":43311},{\"end\":43334,\"start\":43325},{\"end\":43350,\"start\":43344},{\"end\":43624,\"start\":43613},{\"end\":43643,\"start\":43634},{\"end\":43662,\"start\":43654},{\"end\":43679,\"start\":43676},{\"end\":43694,\"start\":43686},{\"end\":43708,\"start\":43702},{\"end\":43715,\"start\":43710},{\"end\":44036,\"start\":44026},{\"end\":44053,\"start\":44047},{\"end\":44070,\"start\":44060},{\"end\":44086,\"start\":44077},{\"end\":44108,\"start\":44095},{\"end\":44471,\"start\":44468},{\"end\":44480,\"start\":44478},{\"end\":44759,\"start\":44750},{\"end\":44774,\"start\":44767},{\"end\":45040,\"start\":45036},{\"end\":45052,\"start\":45050},{\"end\":45067,\"start\":45065},{\"end\":45081,\"start\":45077},{\"end\":45348,\"start\":45341},{\"end\":45359,\"start\":45355},{\"end\":45377,\"start\":45370},{\"end\":45619,\"start\":45607},{\"end\":45631,\"start\":45624},{\"end\":45646,\"start\":45643},{\"end\":45655,\"start\":45653},{\"end\":45670,\"start\":45666},{\"end\":45679,\"start\":45672},{\"end\":45993,\"start\":45989},{\"end\":46003,\"start\":46001},{\"end\":46014,\"start\":46010},{\"end\":46278,\"start\":46274},{\"end\":46289,\"start\":46284},{\"end\":46301,\"start\":46296},{\"end\":46317,\"start\":46312},{\"end\":46329,\"start\":46327},{\"end\":46591,\"start\":46587},{\"end\":46601,\"start\":46597},{\"end\":46609,\"start\":46607},{\"end\":46620,\"start\":46618},{\"end\":46633,\"start\":46628},{\"end\":46891,\"start\":46889},{\"end\":46906,\"start\":46901},{\"end\":46916,\"start\":46910},{\"end\":46926,\"start\":46924},{\"end\":46931,\"start\":46928},{\"end\":47159,\"start\":47156},{\"end\":47174,\"start\":47166},{\"end\":47188,\"start\":47182},{\"end\":47200,\"start\":47198},{\"end\":47212,\"start\":47210},{\"end\":47469,\"start\":47464},{\"end\":47480,\"start\":47476},{\"end\":47492,\"start\":47490},{\"end\":47505,\"start\":47501},{\"end\":47517,\"start\":47513},{\"end\":47745,\"start\":47740},{\"end\":47760,\"start\":47755},{\"end\":47776,\"start\":47771},{\"end\":47927,\"start\":47923},{\"end\":47941,\"start\":47938},{\"end\":47954,\"start\":47952},{\"end\":47969,\"start\":47965},{\"end\":47980,\"start\":47977},{\"end\":48188,\"start\":48184},{\"end\":48205,\"start\":48202},{\"end\":48220,\"start\":48217},{\"end\":48230,\"start\":48227},{\"end\":48243,\"start\":48239},{\"end\":48260,\"start\":48255},{\"end\":48491,\"start\":48486},{\"end\":48506,\"start\":48502},{\"end\":48516,\"start\":48513},{\"end\":48532,\"start\":48529},{\"end\":48776,\"start\":48772},{\"end\":48786,\"start\":48783},{\"end\":48800,\"start\":48797},{\"end\":48815,\"start\":48811},{\"end\":49082,\"start\":49078},{\"end\":49095,\"start\":49092},{\"end\":49111,\"start\":49108},{\"end\":49124,\"start\":49120},{\"end\":49139,\"start\":49135},{\"end\":49404,\"start\":49401},{\"end\":49415,\"start\":49410},{\"end\":49423,\"start\":49417},{\"end\":49431,\"start\":49427},{\"end\":49450,\"start\":49444},{\"end\":49458,\"start\":49452},{\"end\":49691,\"start\":49688},{\"end\":49702,\"start\":49697},{\"end\":49710,\"start\":49704},{\"end\":49718,\"start\":49714},{\"end\":49737,\"start\":49731},{\"end\":49745,\"start\":49739},{\"end\":50019,\"start\":50016},{\"end\":50031,\"start\":50027},{\"end\":50043,\"start\":50041},{\"end\":50056,\"start\":50054},{\"end\":50068,\"start\":50066},{\"end\":50078,\"start\":50075},{\"end\":50094,\"start\":50090}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":108297336},\"end\":34451,\"start\":34121},{\"attributes\":{\"doi\":\"arXiv:2002.05709\",\"id\":\"b1\"},\"end\":34768,\"start\":34453},{\"attributes\":{\"doi\":\"arXiv:2006.10029\",\"id\":\"b2\"},\"end\":35101,\"start\":34770},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":104292237},\"end\":35363,\"start\":35103},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":219615222},\"end\":35676,\"start\":35365},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":209202660},\"end\":35996,\"start\":35678},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":2259236},\"end\":36307,\"start\":35998},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":57246310},\"end\":36580,\"start\":36309},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":9062671},\"end\":36834,\"start\":36582},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":3353774},\"end\":37029,\"start\":36836},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1800511},\"end\":37258,\"start\":37031},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":54101493},\"end\":37708,\"start\":37260},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":4009713},\"end\":37976,\"start\":37710},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":6137921},\"end\":38211,\"start\":37978},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3988220},\"end\":38521,\"start\":38213},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":8182535},\"end\":38810,\"start\":38523},{\"attributes\":{\"id\":\"b16\"},\"end\":39067,\"start\":38812},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":5808102},\"end\":39370,\"start\":39069},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":54459557},\"end\":39655,\"start\":39372},{\"attributes\":{\"id\":\"b19\"},\"end\":39878,\"start\":39657},{\"attributes\":{\"id\":\"b20\"},\"end\":39921,\"start\":39880},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":5574079},\"end\":40176,\"start\":39923},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":15842367},\"end\":40468,\"start\":40178},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":10124817},\"end\":40741,\"start\":40470},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":51881368},\"end\":41069,\"start\":40743},{\"attributes\":{\"doi\":\"arXiv:1910.05872\",\"id\":\"b25\"},\"end\":41340,\"start\":41071},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":10716717},\"end\":41651,\"start\":41342},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":219631799},\"end\":41830,\"start\":41653},{\"attributes\":{\"id\":\"b28\"},\"end\":42054,\"start\":41832},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":25120102},\"end\":42373,\"start\":42056},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":25120102},\"end\":42696,\"start\":42375},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":187547},\"end\":42931,\"start\":42698},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":150381897},\"end\":43229,\"start\":42933},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":18546648},\"end\":43527,\"start\":43231},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":15019293},\"end\":43950,\"start\":43529},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":6844431},\"end\":44383,\"start\":43952},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":167217261},\"end\":44621,\"start\":44385},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":2759724},\"end\":44960,\"start\":44623},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":27750744},\"end\":45258,\"start\":44962},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":35488672},\"end\":45524,\"start\":45260},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":17623309},\"end\":45870,\"start\":45526},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":413731},\"end\":46193,\"start\":45872},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":28544003},\"end\":46498,\"start\":46195},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":51604870},\"end\":46805,\"start\":46500},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":4591284},\"end\":47084,\"start\":46807},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":8485068},\"end\":47368,\"start\":47086},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":10212545},\"end\":47701,\"start\":47370},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":50698},\"end\":47879,\"start\":47703},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":5299559},\"end\":48114,\"start\":47881},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":201319385},\"end\":48439,\"start\":48116},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":195513288},\"end\":48673,\"start\":48441},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":208637033},\"end\":48996,\"start\":48675},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":219633658},\"end\":49329,\"start\":48998},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":15765187},\"end\":49616,\"start\":49331},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":15765187},\"end\":49908,\"start\":49618},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":52953674},\"end\":50322,\"start\":49910}]", "bib_title": "[{\"end\":34201,\"start\":34121},{\"end\":35151,\"start\":35103},{\"end\":35426,\"start\":35365},{\"end\":35762,\"start\":35678},{\"end\":36060,\"start\":35998},{\"end\":36360,\"start\":36309},{\"end\":36647,\"start\":36582},{\"end\":36875,\"start\":36836},{\"end\":37068,\"start\":37031},{\"end\":37364,\"start\":37260},{\"end\":37776,\"start\":37710},{\"end\":38040,\"start\":37978},{\"end\":38299,\"start\":38213},{\"end\":38584,\"start\":38523},{\"end\":39161,\"start\":39069},{\"end\":39421,\"start\":39372},{\"end\":39996,\"start\":39923},{\"end\":40237,\"start\":40178},{\"end\":40526,\"start\":40470},{\"end\":40814,\"start\":40743},{\"end\":41438,\"start\":41342},{\"end\":41669,\"start\":41653},{\"end\":42121,\"start\":42056},{\"end\":42440,\"start\":42375},{\"end\":42771,\"start\":42698},{\"end\":42985,\"start\":42933},{\"end\":43302,\"start\":43231},{\"end\":43609,\"start\":43529},{\"end\":44017,\"start\":43952},{\"end\":44457,\"start\":44385},{\"end\":44742,\"start\":44623},{\"end\":45025,\"start\":44962},{\"end\":45326,\"start\":45260},{\"end\":45605,\"start\":45526},{\"end\":45980,\"start\":45872},{\"end\":46263,\"start\":46195},{\"end\":46579,\"start\":46500},{\"end\":46879,\"start\":46807},{\"end\":47146,\"start\":47086},{\"end\":47453,\"start\":47370},{\"end\":47730,\"start\":47703},{\"end\":47910,\"start\":47881},{\"end\":48173,\"start\":48116},{\"end\":48475,\"start\":48441},{\"end\":48764,\"start\":48675},{\"end\":49069,\"start\":48998},{\"end\":49392,\"start\":49331},{\"end\":49679,\"start\":49618},{\"end\":50010,\"start\":49910}]", "bib_author": "[{\"end\":34221,\"start\":34203},{\"end\":34233,\"start\":34221},{\"end\":34249,\"start\":34233},{\"end\":34271,\"start\":34249},{\"end\":34464,\"start\":34453},{\"end\":34481,\"start\":34464},{\"end\":34499,\"start\":34481},{\"end\":34516,\"start\":34499},{\"end\":34845,\"start\":34834},{\"end\":34862,\"start\":34845},{\"end\":34877,\"start\":34862},{\"end\":34895,\"start\":34877},{\"end\":34912,\"start\":34895},{\"end\":35164,\"start\":35153},{\"end\":35178,\"start\":35164},{\"end\":35193,\"start\":35178},{\"end\":35206,\"start\":35193},{\"end\":35220,\"start\":35206},{\"end\":35441,\"start\":35428},{\"end\":35450,\"start\":35441},{\"end\":35461,\"start\":35450},{\"end\":35472,\"start\":35461},{\"end\":35482,\"start\":35472},{\"end\":35498,\"start\":35482},{\"end\":35777,\"start\":35764},{\"end\":35787,\"start\":35777},{\"end\":35799,\"start\":35787},{\"end\":35815,\"start\":35799},{\"end\":35822,\"start\":35815},{\"end\":36078,\"start\":36062},{\"end\":36096,\"start\":36078},{\"end\":36114,\"start\":36096},{\"end\":36128,\"start\":36114},{\"end\":36372,\"start\":36362},{\"end\":36382,\"start\":36372},{\"end\":36398,\"start\":36382},{\"end\":36409,\"start\":36398},{\"end\":36417,\"start\":36409},{\"end\":36429,\"start\":36417},{\"end\":36663,\"start\":36649},{\"end\":36678,\"start\":36663},{\"end\":36694,\"start\":36678},{\"end\":36897,\"start\":36877},{\"end\":36905,\"start\":36897},{\"end\":36917,\"start\":36905},{\"end\":36921,\"start\":36917},{\"end\":37090,\"start\":37070},{\"end\":37100,\"start\":37090},{\"end\":37115,\"start\":37100},{\"end\":37126,\"start\":37115},{\"end\":37132,\"start\":37126},{\"end\":37382,\"start\":37366},{\"end\":37400,\"start\":37382},{\"end\":37419,\"start\":37400},{\"end\":37436,\"start\":37419},{\"end\":37462,\"start\":37436},{\"end\":37471,\"start\":37462},{\"end\":37794,\"start\":37778},{\"end\":37809,\"start\":37794},{\"end\":37826,\"start\":37809},{\"end\":38053,\"start\":38042},{\"end\":38065,\"start\":38053},{\"end\":38078,\"start\":38065},{\"end\":38323,\"start\":38301},{\"end\":38338,\"start\":38323},{\"end\":38350,\"start\":38338},{\"end\":38598,\"start\":38586},{\"end\":38607,\"start\":38598},{\"end\":38620,\"start\":38607},{\"end\":38630,\"start\":38620},{\"end\":38646,\"start\":38630},{\"end\":38894,\"start\":38881},{\"end\":38904,\"start\":38894},{\"end\":38918,\"start\":38904},{\"end\":38934,\"start\":38918},{\"end\":39177,\"start\":39163},{\"end\":39196,\"start\":39177},{\"end\":39436,\"start\":39423},{\"end\":39448,\"start\":39436},{\"end\":39458,\"start\":39448},{\"end\":39469,\"start\":39458},{\"end\":39482,\"start\":39469},{\"end\":39498,\"start\":39482},{\"end\":39724,\"start\":39710},{\"end\":39738,\"start\":39724},{\"end\":39753,\"start\":39738},{\"end\":39766,\"start\":39753},{\"end\":39891,\"start\":39882},{\"end\":40018,\"start\":39998},{\"end\":40034,\"start\":40018},{\"end\":40262,\"start\":40239},{\"end\":40278,\"start\":40262},{\"end\":40291,\"start\":40278},{\"end\":40303,\"start\":40291},{\"end\":40551,\"start\":40528},{\"end\":40567,\"start\":40551},{\"end\":40580,\"start\":40567},{\"end\":40592,\"start\":40580},{\"end\":40825,\"start\":40816},{\"end\":40847,\"start\":40825},{\"end\":40858,\"start\":40847},{\"end\":40869,\"start\":40858},{\"end\":40887,\"start\":40869},{\"end\":41154,\"start\":41141},{\"end\":41169,\"start\":41154},{\"end\":41182,\"start\":41169},{\"end\":41454,\"start\":41440},{\"end\":41468,\"start\":41454},{\"end\":41483,\"start\":41468},{\"end\":41683,\"start\":41671},{\"end\":41692,\"start\":41683},{\"end\":41703,\"start\":41692},{\"end\":41717,\"start\":41703},{\"end\":41912,\"start\":41897},{\"end\":41923,\"start\":41912},{\"end\":42142,\"start\":42123},{\"end\":42165,\"start\":42142},{\"end\":42176,\"start\":42165},{\"end\":42191,\"start\":42176},{\"end\":42200,\"start\":42191},{\"end\":42466,\"start\":42442},{\"end\":42484,\"start\":42466},{\"end\":42495,\"start\":42484},{\"end\":42510,\"start\":42495},{\"end\":42519,\"start\":42510},{\"end\":42788,\"start\":42773},{\"end\":42802,\"start\":42788},{\"end\":43002,\"start\":42987},{\"end\":43018,\"start\":43002},{\"end\":43032,\"start\":43018},{\"end\":43048,\"start\":43032},{\"end\":43066,\"start\":43048},{\"end\":43318,\"start\":43304},{\"end\":43336,\"start\":43318},{\"end\":43352,\"start\":43336},{\"end\":43626,\"start\":43611},{\"end\":43645,\"start\":43626},{\"end\":43664,\"start\":43645},{\"end\":43681,\"start\":43664},{\"end\":43696,\"start\":43681},{\"end\":43710,\"start\":43696},{\"end\":43717,\"start\":43710},{\"end\":44038,\"start\":44019},{\"end\":44055,\"start\":44038},{\"end\":44072,\"start\":44055},{\"end\":44088,\"start\":44072},{\"end\":44110,\"start\":44088},{\"end\":44473,\"start\":44459},{\"end\":44482,\"start\":44473},{\"end\":44761,\"start\":44744},{\"end\":44776,\"start\":44761},{\"end\":45042,\"start\":45027},{\"end\":45054,\"start\":45042},{\"end\":45069,\"start\":45054},{\"end\":45083,\"start\":45069},{\"end\":45350,\"start\":45328},{\"end\":45361,\"start\":45350},{\"end\":45379,\"start\":45361},{\"end\":45621,\"start\":45607},{\"end\":45633,\"start\":45621},{\"end\":45648,\"start\":45633},{\"end\":45657,\"start\":45648},{\"end\":45672,\"start\":45657},{\"end\":45681,\"start\":45672},{\"end\":45995,\"start\":45982},{\"end\":46005,\"start\":45995},{\"end\":46016,\"start\":46005},{\"end\":46280,\"start\":46265},{\"end\":46291,\"start\":46280},{\"end\":46303,\"start\":46291},{\"end\":46319,\"start\":46303},{\"end\":46331,\"start\":46319},{\"end\":46593,\"start\":46581},{\"end\":46603,\"start\":46593},{\"end\":46611,\"start\":46603},{\"end\":46622,\"start\":46611},{\"end\":46635,\"start\":46622},{\"end\":46893,\"start\":46881},{\"end\":46908,\"start\":46893},{\"end\":46918,\"start\":46908},{\"end\":46928,\"start\":46918},{\"end\":46933,\"start\":46928},{\"end\":47161,\"start\":47148},{\"end\":47176,\"start\":47161},{\"end\":47190,\"start\":47176},{\"end\":47202,\"start\":47190},{\"end\":47214,\"start\":47202},{\"end\":47471,\"start\":47455},{\"end\":47482,\"start\":47471},{\"end\":47494,\"start\":47482},{\"end\":47507,\"start\":47494},{\"end\":47519,\"start\":47507},{\"end\":47747,\"start\":47732},{\"end\":47762,\"start\":47747},{\"end\":47778,\"start\":47762},{\"end\":47929,\"start\":47912},{\"end\":47943,\"start\":47929},{\"end\":47956,\"start\":47943},{\"end\":47971,\"start\":47956},{\"end\":47982,\"start\":47971},{\"end\":48190,\"start\":48175},{\"end\":48207,\"start\":48190},{\"end\":48222,\"start\":48207},{\"end\":48232,\"start\":48222},{\"end\":48245,\"start\":48232},{\"end\":48262,\"start\":48245},{\"end\":48493,\"start\":48477},{\"end\":48508,\"start\":48493},{\"end\":48518,\"start\":48508},{\"end\":48534,\"start\":48518},{\"end\":48778,\"start\":48766},{\"end\":48788,\"start\":48778},{\"end\":48802,\"start\":48788},{\"end\":48817,\"start\":48802},{\"end\":49084,\"start\":49071},{\"end\":49097,\"start\":49084},{\"end\":49113,\"start\":49097},{\"end\":49126,\"start\":49113},{\"end\":49141,\"start\":49126},{\"end\":49406,\"start\":49394},{\"end\":49417,\"start\":49406},{\"end\":49425,\"start\":49417},{\"end\":49433,\"start\":49425},{\"end\":49452,\"start\":49433},{\"end\":49460,\"start\":49452},{\"end\":49693,\"start\":49681},{\"end\":49704,\"start\":49693},{\"end\":49712,\"start\":49704},{\"end\":49720,\"start\":49712},{\"end\":49739,\"start\":49720},{\"end\":49747,\"start\":49739},{\"end\":50021,\"start\":50012},{\"end\":50033,\"start\":50021},{\"end\":50045,\"start\":50033},{\"end\":50058,\"start\":50045},{\"end\":50070,\"start\":50058},{\"end\":50080,\"start\":50070},{\"end\":50096,\"start\":50080}]", "bib_venue": "[{\"end\":34275,\"start\":34271},{\"end\":34601,\"start\":34532},{\"end\":34832,\"start\":34770},{\"end\":35224,\"start\":35220},{\"end\":35502,\"start\":35498},{\"end\":35829,\"start\":35822},{\"end\":36133,\"start\":36128},{\"end\":36433,\"start\":36429},{\"end\":36698,\"start\":36694},{\"end\":36925,\"start\":36921},{\"end\":37137,\"start\":37132},{\"end\":37475,\"start\":37471},{\"end\":37833,\"start\":37826},{\"end\":38082,\"start\":38078},{\"end\":38357,\"start\":38350},{\"end\":38650,\"start\":38646},{\"end\":38879,\"start\":38812},{\"end\":39200,\"start\":39196},{\"end\":39502,\"start\":39498},{\"end\":39708,\"start\":39657},{\"end\":40041,\"start\":40034},{\"end\":40307,\"start\":40303},{\"end\":40596,\"start\":40592},{\"end\":40891,\"start\":40887},{\"end\":41139,\"start\":41071},{\"end\":41487,\"start\":41483},{\"end\":41721,\"start\":41717},{\"end\":41895,\"start\":41832},{\"end\":42204,\"start\":42200},{\"end\":42523,\"start\":42519},{\"end\":42806,\"start\":42802},{\"end\":43073,\"start\":43066},{\"end\":43371,\"start\":43352},{\"end\":43721,\"start\":43717},{\"end\":44150,\"start\":44110},{\"end\":44486,\"start\":44482},{\"end\":44783,\"start\":44776},{\"end\":45102,\"start\":45083},{\"end\":45384,\"start\":45379},{\"end\":45685,\"start\":45681},{\"end\":46020,\"start\":46016},{\"end\":46335,\"start\":46331},{\"end\":46640,\"start\":46635},{\"end\":46937,\"start\":46933},{\"end\":47218,\"start\":47214},{\"end\":47523,\"start\":47519},{\"end\":47782,\"start\":47778},{\"end\":47986,\"start\":47982},{\"end\":48266,\"start\":48262},{\"end\":48538,\"start\":48534},{\"end\":48827,\"start\":48817},{\"end\":49154,\"start\":49141},{\"end\":49464,\"start\":49460},{\"end\":49751,\"start\":49747},{\"end\":50100,\"start\":50096}]"}}}, "year": 2023, "month": 12, "day": 17}