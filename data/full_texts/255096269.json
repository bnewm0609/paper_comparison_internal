{"id": 255096269, "updated": "2023-10-05 05:57:39.914", "metadata": {"title": "OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization", "authors": "[{\"first\":\"Srinivasan\",\"last\":\"Iyer\",\"middle\":[]},{\"first\":\"Xi\",\"last\":\"Lin\",\"middle\":[\"Victoria\"]},{\"first\":\"Ramakanth\",\"last\":\"Pasunuru\",\"middle\":[]},{\"first\":\"Todor\",\"last\":\"Mihaylov\",\"middle\":[]},{\"first\":\"Daniel\",\"last\":\"Simig\",\"middle\":[]},{\"first\":\"Ping\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Kurt\",\"last\":\"Shuster\",\"middle\":[]},{\"first\":\"Tianlu\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Qing\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Punit\",\"last\":\"Koura\",\"middle\":[\"Singh\"]},{\"first\":\"Xian\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Brian\",\"last\":\"O'Horo\",\"middle\":[]},{\"first\":\"Gabriel\",\"last\":\"Pereyra\",\"middle\":[]},{\"first\":\"Jeff\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Christopher\",\"last\":\"Dewan\",\"middle\":[]},{\"first\":\"Asli\",\"last\":\"Celikyilmaz\",\"middle\":[]},{\"first\":\"Luke\",\"last\":\"Zettlemoyer\",\"middle\":[]},{\"first\":\"Ves\",\"last\":\"Stoyanov\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Recent work has shown that fine-tuning large pre-trained language models on a collection of tasks described via instructions, a.k.a. instruction-tuning, improves their zero and few-shot generalization to unseen tasks. However, there is a limited understanding of the performance trade-offs of different decisions made during the instruction-tuning process. These decisions include the scale and diversity of the instruction-tuning benchmark, different task sampling strategies, fine-tuning with and without demonstrations, training using specialized datasets for reasoning and dialogue, and finally, the fine-tuning objectives themselves. In this paper, we characterize the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes. To this end, we create OPT-IML Bench: a large benchmark for Instruction Meta-Learning (IML) of 2000 NLP tasks consolidated into task categories from 8 existing benchmarks, and prepare an evaluation framework to measure three types of model generalizations: to tasks from fully held-out categories, to held-out tasks from seen categories, and to held-out instances from seen tasks. Through the lens of this framework, we first present insights about instruction-tuning decisions as applied to OPT-30B and further exploit these insights to train OPT-IML 30B and 175B, which are instruction-tuned versions of OPT. OPT-IML demonstrates all three generalization abilities at both scales on four different evaluation benchmarks with diverse tasks and input formats -- PromptSource, FLAN, Super-NaturalInstructions, and UnifiedSKG. Not only does it significantly outperform OPT on all benchmarks but is also highly competitive with existing models fine-tuned on each specific benchmark. We release OPT-IML at both scales, together with the OPT-IML Bench evaluation framework.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2212.12017", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2212-12017", "doi": "10.48550/arxiv.2212.12017"}}, "content": {"source": {"pdf_hash": "e965e93e76a9e6c4e4863d145b5c007b540d575d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2212.12017v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4d08b7c44a204bf64997d550f41960a223bce518", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e965e93e76a9e6c4e4863d145b5c007b540d575d.txt", "contents": "\nOPT-IML : Scaling Language Model Instruction Meta Learning through the Lens of Generalization\n\n\nSrinivasan Iyer \nMetaAI\n\nVictoria Xi \nMetaAI\n\nLin \nMetaAI\n\nRamakanth Pasunuru \nMetaAI\n\nTodor Mihaylov \nMetaAI\n\nD\u00e1niel Simig \nMetaAI\n\nPing Yu \nMetaAI\n\nKurt Shuster \nMetaAI\n\nTianlu Wang \nMetaAI\n\nPunitQing Liu \nMetaAI\n\nSingh Koura \nMetaAI\n\nXian Li \nMetaAI\n\nBrian O&apos;horo \nMetaAI\n\nGabriel Pereyra \nMetaAI\n\nJeff Wang \nMetaAI\n\nChristopher Dewan \nMetaAI\n\nAsli Celikyilmaz \nMetaAI\n\nLuke Zettlemoyer \nMetaAI\n\nVes Stoyanov \nMetaAI\n\nOPT-IML : Scaling Language Model Instruction Meta Learning through the Lens of Generalization\n\nRecent work has shown that fine-tuning large pre-trained language models on a collection of tasks described via instructions, a.k.a. instruction-tuning, improves their zero and few-shot generalization to unseen tasks. However, there is a limited understanding of the performance trade-offs of different decisions made during the instruction-tuning process. These decisions include the scale and diversity of the instruction-tuning benchmark, different task sampling strategies, fine-tuning with and without demonstrations, training using specialized datasets for reasoning and dialogue, and finally, the fine-tuning objectives themselves. In this paper, we characterize the effect of instruction-tuning decisions on downstream task performance when scaling both model and benchmark sizes. To this end, we create OPT-\n\nIntroduction\n\nInstruction fine-tuning is shown (Wei et al., 2022a;Sanh et al., 2022;Chung et al., 2022a) to significantly improve the zero-and few-shot performance of large pretrained LMs (LLM). It involves fine-tuning LLMs on collections of NLP tasks using instructional style input formats. Successful instruction-tuning of LLMs depends on a number of aspects such as the objectives used for finetuning, the distribution and diversity of the fine-tuning tasks, the inclusion of specialized datasets related to reasoning and dialogue, fine-tuning with demonstrations, and also, the comprehensiveness of the evaluation framework. In this paper, we develop an extensive large-scale fine-tuning and evaluation framework of 2000 NLP tasks (which we call OPT-IML Bench) and use it to characterize the tradeoffs of different decisions relating to instruction meta-learning (IML) on the OPT models . We exploit insights gathered from this process, to train OPT-IML 30B and 175B, instruction-tuned versions of OPT.\n\nThere are a growing number of large meta-datasets of NLP tasks such as Super-NaturalInstructions , FLAN (Wei et al., 2022a) and PromptSource (Sanh et al., 2022). Recent instruction-tuning work has demonstrated success using these individual benchmarks and their combinations (Chung et al., 2022b), with a general recommendation for scaling up the number of tasks. Source OQA A very large cinnamon color, it gazed right back. That moment of mutual recognition is always the same. A dozen thoughts windmill through my head.\n\n\nAdditional Datasets\n\nAnswer the following math question by reasoning step by step. Consider the function $g(x)=3x-4$. For what value of $a$ is $g(a)=0$? A: Since $g(a) = 3a-4$, the equation $g(a)=0$ means $3a-4=0$. Solving this equation gives $a = \\boxed{\\frac{4}{3}}$.\n\nReas.\n\n\nPre-Train\n\nThey just don't make cartoons like they used to. This one had wit, great characters, \u2026 What is the sentiment of this review? : We fine-tune OPT on a large collection of 1500+ NLP tasks divided into task categories (left hand side) to create OPT-IML. Each category contains multiple related tasks, as well as multiple prompts for the same task (e.g. IMDB), aggregated from multiple benchmarks. We evaluate OPT-IML on a set of evaluation categories (right hand-side) which can be disjoint, partially overlap or fully-overlap with the categories used for tuning (e.g. Sentiment Analysis fully overlaps and QA partially overlaps), corresponding to evaluating model generalization to tasks from fully held-out categories, to tasks from categories seen during training, and to instances from tasks seen during training. We release this evaluation framework as OPT-IML Bench.\n\nWe follow this recommendation by consolidating 8 meta-datasets into a large collection of 1,991 NLP tasks containing instructions with multiple prompts and grouping them into more than 100 task categories such as Question Answering and Sentiment Analysis ( Figure 1). Furthermore, we transform this collection into an evaluation framework for comprehensively evaluating large-scale instruction-tuned models across three levels of generalization: 1) model performance on tasks from fully held-out task categories not used for tuning, as in prior work (Wei et al., 2022a;Sanh et al., 2022), and additionally, 2) performance on unseen tasks from categories seen during instructiontuning, and, 3) performance on held-out instances of tasks seen during tuning. The former two settings evaluates the cross-task generalization of instruction-tuning while the last setting evaluates the generalization of supervised multi-task learning (McCann et al., 2018). We refer to the resulting instruction-tuning framework as OPT-IML Bench and illustrate its composition in Figure 1 where the right hand side depicts evaluation categories, which can be completely disjoint, partially overlap, or completely overlap with the categories used for tuning on the left. Each category comprises datasets that can belong to multiple benchmarks and be associated with multiple prompts.\n\nThe effectiveness of instruction-tuning on LLMs depends on factors such as the diversity and distribution of tuning-tasks, the formatting of their prompts, and the objectives used for fine-tuning. Several recent works on instruction-tuning explore these factors by grouping tasks into categories and evaluating performance on tasks from completely held-out task categories (Sanh et al., 2022;Wei et al., 2022a;. Using our evaluation framework that considers multiple levels of generalization, we are able to comprehensively characterize the tradeoffs relating to these different factors when scaling up instruction-tuning to an aggregate of 8 different benchmarks. By instruction tuning OPT 30B  on OPT-IML Bench, we outline the tradeoffs of dataset and benchmark sampling strategies during tuning, the scaling laws with respect to tasks and categories, the effects of approaches to incorporating task demonstrations into instruction-tuning based on Min et al. (2021), as well as instruction-tuning with specialized datasets that contain reasoning chains (Kojima et al., 2022; and dialogue . These experiments can serve to establish best practices for large scale instruction-tuning of LLMs.\n\nGiven the insights gathered from our generalization experiments on OPT-IML bench, we train OPT-IML. OPT-IML significantly improves over its base pre-trained model at both 30B and 175B scales on four different instruction-tuning benchmarks: PromptSource (Sanh et al., 2022), FLAN (Wei et al., 2022a), Super-NaturalInstructions , and UnifiedSKG (Xie et al., 2022). Additionally, the OPT-IML models also perform competitively in comparison with each of the prior instruction-tuned models individually tuned on these benchmarks on both zero and fewshot performance. Recently, along similar lines as this work, Chung et al. (2022b) achieve impressive gains on the challenging benchmarks of MMLU (Hendrycks et al., 2020) and Big-Bench Hard (Suzgun et al., 2022) by instruction-tuning PaLM  and T5 (Raffel et al., 2020) on a scaled-up collection of 1.8K tasks. OPT-IML trained under similar settings still underperforms in comparison on these challenging benchmarks and we discuss this in Section 6. Following OPT , we will responsibly share versions of OPT-IML at both scales, and also release our OPT-IML Bench evaluation framework to facilitate future work in this direction.\n\n\nScaling up Multi-task Benchmarks\n\nTo characterize the effects of extreme task scaling on instruction tuning, we build on recent task collections such as Super-NaturalInstructions  and PromptSource (Sanh et al., 2022), and aggregate 8 such collections to create the OPT-IML Benchmark for massive instruction fine-tuning and evaluation over diverse task categories, instruction types and prompting setups (Table 1).\n\nFor the remainder of this paper, we use the terms task and dataset interchangeably; each task/dataset can be instantiated using multiple prompt templates. We refer to the original data from which the tasks are created as a data source; multiple tasks can be created from the same data source (e.g. question answering and question rewriting). A benchmark comprises multiple tasks, where each task belongs to a single task category/cluster.\n\n\nTask Curation\n\nWe expand the Super-NaturalInstructions benchmark of 1600+ tasks by  with the task collections from multiple existing work on instruction-tuning: FLAN (Wei et al., 2022a), T0 (Sanh et al., 2022); prompt crowdsourcing: PromptSource (Bach et al., 2022); cross-task transfer studies: ExMix (Aribandi et al., 2022), T5 (Raffel et al., 2020), CrossFit (Ye et al., 2021); and areaspecific task consolidation: Structured Knowledge Grounding (Xie et al., 2022), Dialogue  and Chain-of-thought Reasoning 1 (Chung et al., 2022b). The curation process of all these benchmarks can be found in Appendix A.1.\n\nThere is a significant overlap between the datasets in these benchmarks. For example, popular datasets such as SQuAD v1/v2 (Rajpurkar et al., 2016(Rajpurkar et al., , 2018 appear in almost all benchmarks. In addition, while Super-NaturalInstructions, PromptSource, FLAN and Chain-of-thought Reasoning contain long-form human-written instructions or reasoning chains, the rest of the benchmarks are designed for multi-task learning and the prompt templates often only consist of short field or task prefixes (e.g. \"question:\", \"label:\"). Therefore, we only kept tasks from the CrossFit, ExMix and T5 collections that do not appear in any other benchmarks. Since we're exploring a large number of tasks, we take maximally 100k examples (at random) per task from all benchmarks except FLAN, where we take maximally 30k examples per task following the same practice as Wei et al. (2022a Wei et al. (2022a). \u2020 We only manually unify the task categorization in our evaluation sets. The estimation of the number of task clusters in our train set is based on a coarse union of the clustering tags from each original benchmark.\n\n\nBenchmark Consolidation\n\nInstruction schema. Each benchmark adopts different instruction and language styles. In Table 2, we broadly classify their instructions into two categories: dataset-level and instance-level. Dataset-level instructions define the overall task and may include auxiliary information such as positive/negative examples and explanations. The model is expected to learn the definition of the task based on this and apply the knowledge to each example coming after it. Instance-level instructions are templates to be instantiated for each example individually and is sometimes designed in the cloze-style to solicit the desired output for the example. We cast all tasks across the benchmarks we collect into the bipartite prompt formulation that include \"instructions\" and \"output\" segments (Table 2). For CrossFit, ExMix and T5, since the original benchmarks do not provide natural language instructions, we manually write a simple instruction sentence for each of the included tasks and use them at the instance level. For example, the instructions for the GPT-2 Deepfake Detection task (Radford et al., 2021) in ExMix reads \"Is the following text produced by GPT-2?\".\n\nTask categorization. We categorize the tasks under the conventional NLP categories following the practice of previous work (Wei et al., 2022a;Sanh et al., 2022;Ye et al., 2021). Such grouping offers a convenient scaffold to study the generalization of models cross-and within categories. We primarily follow the 76-category taxonomy defined by Super-NaturalInstructions. The other benchmarks also provide their own task clusters. We perform a coarse unification of the task clusters manually, e.g. merging \"hate speech detection\" with \"toxic language detection\". Besides this, benchmarks such as CrossFit and PromptSource adopt a finer-grained task categorization compared to Super-NaturalInstructions, e.g. CrossFit identifies multiple sub-classes of Question Answering. In such cases, we adopt the more coarse-grained assignment of Super-NaturalInstructions. This results in a single-level taxonomy with over 100 task categories (Table 1).\n\nInst. Type\n\n\nInstructions Output\n\nSuperNatInst task-level inst.\n\nInstructions: Given a premise and two alternatives, choose the alternative that is a more plausible cause or effect of the situation described by the premise. The input format is \"premise (1) alternative_1 (2) alternative_2\", the output should either be \"1\" or \"2\" based on your judgment.   (Roemmele et al., 2011) from Super-NaturalInstructions, PromptSource, FLAN and CrossFit. CrossFit does not provide natural language instructions, which requires the models to rely on the data presentation to infer task requirements.\n\n\nCreating Benchmark Splits\n\nTrain, validation and test splits. We split the set of all tasks in a way that allows us to perform massive instruction fine-tuning and evaluate the resulting model with respect to three levels of generalization. First, we hold out several task categories to evaluate model generalization to new categories of tasks. Second, we select a subset of the remaining categories as partially held-out categories. 2 We divide the datasets in these categories into train and evaluation and use them to test model generalization to new datasets from seen task categories. We select the fully and partially held-out categories by largely staying consistent with previous instruction fine-tuning work Wei et al., 2022a;Sanh et al., 2022) to allow direct comparison. Finally, for a subset of the training tasks, we hold out the validation and test sets from the original data release, and use them to test model generalization in the standard multi-task learning setting, i.e. new examples from seen tasks. We reserve 35 evaluation tasks spanning 9 task categories from the evaluation tasks as the validation set 3 , and use them to characterize the tradeoffs of different instruction-tuning strategies in \u00a74. The details of our validation tasks including their evaluation metrics are shown in Table 15.\n\nTask de-duplication. We make sure that the train and evaluation tasks do not overlap on the data source they were created from, to prevent leakage 4 , following the practice of . For each pair of train and eval tasks, we compute the fraction of examples that have any 13-gram overlap between the instantiated sequences from those examples. We manually examine every pair where more than 1% of the eval set overlaps with the training set (\u223c14,000 pairs) to confirm whether tuning on the train task can unfairly benefit the eval task, and decide either to remove the train or the eval task in confirmed cases. The task pairs that share a broad contextual resource such as Wikipedia but otherwise contain unrelated output labels are retained. Table 1 shows the statistics of our task splits.\n\n\nTask Prompt Construction\n\nEach example in the zero-shot setting is formatted using the bipartite instruction scheme as described in Section 2.1. We insert a delimiter between the instructions and the output if the instructions do not end with a \":\". Similar to Chung et al. (2022b), for each example we randomly sample a delimiter from a small set 5 to mitigate overfitting. For few-shot prompts, we place the demonstration examples between the task descriptions and the target example for benchmarks that adopt task-level instructions such as Super-NaturalInstructions, and before the task example for benchmarks that adopt instance-level instructions such as FLAN and PromptSource. Examples of prompts for each of the tasks can be found in Appendix C. The FLAN and PromptSource benchmarks contain multiple manually-written templates per task. To further increase task diversity, some templates in these benchmarks altered the original task semantics (e.g. \"question answering\" \u2192 \"question generation\"). We manually examined all task templates in these benchmarks and removed the templates that altered the original task semantics to refine our task categories.\n\n\nInstruction Fine-tuning\n\nWe use the OPT-IML Bench presented in Section 2 to fine-tune OPT , a suite of open-source decoder-only transformer language models released in scales from 125M to 175B parameters that performs similar to GPT-3 (Brown et al., 2020a) on a collection of standard NLP tasks. OPT is trained on 180B unique tokens from a combination of the datasets used in RoBERTa (Liu et al., 2019), the Pile (Gao et al., 2020), and PushShift.io Reddit (Baumgartner et al., 2020;Roller et al., 2020) using a next-word prediction objective. We describe the process of instructiontuning OPT at the scales of 30B and 175B in this section.\n\n\nFine-tuning Objective\n\nWe finetune OPT in a manner similar to pre-training using a next-word prediction objective conditioned on all previous tokens as context. However, we separate the training sequence into a source context sequence and a target sequence and only include loss terms from the tokens in the target sequence (label-loss). We treat the task instructions and inputs as source tokens and the label tokens as target tokens. Formally, for a fine-tuning dataset D comprising source instances s i and their corresponding target tokens t i = {t ij }, a pre-trained model with parameters \u03b8 is fine-tuned to minimize the following loss over the target tokens conditioned on the source tokens and previously seen target tokens.\nL(D; \u03b8) = \u2212 i j log p \u03b8 (t ij |s i , t i,<j )(1)\nWe minimize this loss across all datasets in our OPT-IML Bench by mixing examples from different datasets based on their sizes and proportions assigned to the benchmarks they come from (more details in Section 4).\n\n\nPacking and Document Attention\n\nIn order to utilize the maximum sequence length for computational efficiency, we pack multiple examples (source and target) together as a sequence of 2048 tokens (Raffel et al., 2020), separated by <eos> tokens. One consequence of packing is that the tokens belonging to one example can attend to tokens from previously packed examples in the same sequence. To mitigate this, we use 5. The set includes \"\\nAnswer:\", \" Answer:\", \"\\nA:\", \" A:\", \"\\nOutput:\", \" Output:\", \"\\nanswer:\", \"\\output:\". document attention masking i.e. we modify the token attention mask in causal LMs to attend only to the tokens that are part of the same example, rather than all the previous tokens in the sequence. This changes the attention mask from a triangular to a block triangular mask and improves both stability and performance in our experiments.\n\n\nFine-tuning Hyperparameters\n\nWe fine-tune all 30B models on 64 40GB A100s, and 175B models on 128 40GB A100s. Following OPT, we use Fully Sharded Data Parallel (Artetxe et al., 2021) and the Megatron-LM Tensor Parallelism (Shoeybi et al., 2019). We inherit most model hyper-parameters for each model scale following OPT. We pack our training examples into sequences of length 2048, left-truncating examples that overflow. We use Adam (Kingma and Ba, 2014) with 32-bit state with (\u03b2 1 , \u03b2 2 ) = (0.9, 0.95), linearly warming up the learning rate for 60 steps to the maximum, followed by linearly decaying it to 0. We conduct preliminary experiments to select learning rates from {1e \u22125 , 3e \u22125 , 5e \u22125 , 6e \u22125 } and per-GPU batch sizes from {2, 4, 8} using our validation split from \u00a72. The resulting hyperparameters are listed in Table 3. We use a dropout of 0.1 (including embedding dropout) and clip gradient norms to 1.0, and use dynamic loss scaling to prevent underflows (Micikevicius et al., 2018). During fine-tuning, our models saw approximately 2 billion tokens, which is only 0.6% of the pre-training budget of OPT (Table 3).  \n\n\nWhat Matters for Instruction Fine-tuning?\n\nRecent works have explored a number of instruction fine-tuning techniques to optimize the performance of the resulting model on specific kinds of downstream tasks, and also to improve their robustness against variations in prompts, instruction styles and prompting setups. Using an OPT 30B model with the basic hyper-parameter settings chosen in \u00a73.3, we run experiments to characterize the effects of dataset proportions, number of tasks and diversity, using pre-training, dialogue, and reasoning datasets, and training using demonstrations, on instruction-tuning with respect to our three levels of model generalization: fully held-out, partially held-out and fully supervised. We aggregate performance along several dimensions such as clusters, and benchmarks to determine the best settings.\n\n\nExperimental Setup\n\nThe goal of our experimental setup is first, to characterize the effects of a multitude of factors related to the fine-tuning process, on instruction-tuning performance, and second, to use these findings to effectively instruction-tune OPT models. The factors that we experiment with are 1) the composition of the fine-tuning dataset mixture, 2) the number and diversity of the tasks used for fine-tuning, 3) using additional datasets relating to pre-training, reasoning and dialogue as part of the fine-tuning mix, and 4) different ways of fine-tuning with demonstrations.\n\nPrompt construction details. To compile our train data, we merged all prompt data for a task with N examples and randomly take N prompts from the pool such that the training task distribution is kept the same regardless of how many prompts are given for the tasks. We merged the prompts for each task in a similar manner in our validation set, and randomly sample a maximum of 250 prompts per task to report the validation results. For our test tasks, we keep all prompt variations and all examples.\n\nGeneralization levels. Starting with a baseline instruction-tuned model, we independently characterize the effect of each factor, by tuning models with several variations of that factor and evaluating the models on the tasks from our validation split from Section 2, separated into three generalization levels: a) tasks from clusters not included in training (Fully Held-out), b) tasks unseen during training but from seen clusters (Partially Supervised), and c) tasks seen during training (Fully Supervised). An instruction-tuning setting is desirable if it improves performance on fully held-out and partially supervised tasks without sacrificing performance on fully supervised tasks. We use average performance across all three generalization levels on both 0-shot and 5-shot settings on the validation/test sets of the tasks in the validation split to determine the best settings for each factor.\n\nDecoding. Our evaluation data comprises tasks with answer candidates (of which one is correct), as well as tasks with multiple gold reference sequences. For the former set of tasks, we use rank classification similar to Brown et al. (2020b), where we score each candidate based on their likelihood and output the highest-scoring candidate as the answer. This candidate is used to compute accuracy on the task. For tasks without candidates, we perform greedy decoding until an <eos> token is predicted or a maximum of N=256 tokens are generated. Based on the generated sequence and the references, we then compute either Exact-match or Rouge-L F1 scores.\n\nModel selection. For all experiments, we first aggregate results separately for 0-shot and 5-shot across task subtypes. For example, pro and anti versions of type 1 and type 2 Winobias (Zhao et al., 2018) tasks from PromptSource, and all 57 subtasks of MMLU (Hendrycks et al., 2020), would be aggregated to get per task performance. If the same task exists across multiple benchmarks, we then average performance across benchmarks as well. We then compute 0-shot and 5-shot averages of all tasks within a category (or benchmark depending on the experiment), and finally, compute a combined average of all 0 and 5-shot scores of each category (or benchmark), which we use for model selection. We tune each model for 4000 steps and evaluate on our validation split on both 0-shot and 5-shot settings, using 250 examples from each task for compute-efficiency. As described in Section 2, our validation splits for each task include a mix of multiple prompts for FLAN and PromptSource. All but four validation tasks are generation-style tasks (where we report Rouge-L F1). We compute accuracy based on scoring for the remaining tasks and aggregate them together with Rouge-L for presentation purposes. We refer to Table 15 in the Appendix for full details about the tasks in our validation split.\n\n\nEffects of varying task mixing-rate maximum\n\nPrior work (Raffel et al., 2020;Wei et al., 2022a) typically uses example-proportional sampling and builds batches by sampling from datasets proportional to their sizes, while enforcing a maximum size parameter (EPS) to prevent large datasets from overwhelming the batch. To understand how this maximum mixing rate (EPS) affects performance across the different generalization levels, we perform experiments with EPS \u2208 {128, 256, 512, 1024EPS \u2208 {128, 256, 512, , 2048 } and report results in Table 4. An EPS of 512 causes 97% datasets to hit their maximum, while an EPS of 8192 causes 16% datasets to hit their maximum. We also experiment without using EPS i.e. EPS=100K.\n\nOverall, we find that while EPS is important to instruction-tuning i.e. on average all models that use EPS outperform the model without it, after a certain threshold i.e. less than 4096 in our case, there is minimal variation in performance across all generalization levels. While based on the highest average performance, we choose 4096 (also corresponds to 50% of the dataset lengths being capped) for our other experiments and the final OPT-IML models, we find that all values below 4096 also perform quite well, with EPS=128 closely matching 4096. Also note that changing EPS  Table 4: Performance variation across different task categories with different maximum mixing rates (EPS), for each generalization level on OPT-IML 30B, after 4000 steps. Results are in the format of 0-shot/5-shot. We use only 0-shot performance for summarization tasks. Most tasks are generation tasks, for which we report Rouge-L. We report accuracy for MMLU. Some tasks in the Cause Effect Cluster also use accuracy, which is averaged with Rouge-L for presentation purposes. We select models based on their average performance aggregated per category, benchmark and shot.\n\nimplicitly changes the proportion of fine-tuning data from each benchmark, which we control for explicitly in the next Section.\n\n\nEffects of varying benchmark proportions\n\nIn Section 2, we describe the multiple tasks and prompt repositories (Sanh et al., 2022;Wei et al., 2022a;Ye et al., 2021;Aribandi et al., 2022) that we unify to massively scale the number of tasks used for instruction-tuning. However, using multiple benchmarks for training, together with only example-proportional sampling, results in benchmarks with more tasks overwhelming the batch composition. For example, in our benchmark, 71% of training examples would come from SuperNatInst, with 18% from PromptSource, and only 5% from FLAN. Since each benchmark is associated with a specific task format, this can bias the resulting model towards certain input-output formats. We vary the proportions of different benchmarks to evaluate their effect on downstream task performance on our three generalization levels and present results in Table 5. For this experiment, we compare models based on their aggregate performance on each benchmark instead of task category, since we would like to choose the parameters that perform well on a maximum number of benchmarks. First, we look at performance improvements within the same benchmark where the proportions were changed. As we increase the proportion of FLAN from 5% to 25%, its performance improves significantly on both the fully-held out and the partially held-out generalization levels, with no notable improvement on the fully-supervised tasks. SuperNatInstshows a similar trend on partiallysupervised tasks, but surprisingly, not so much on fully held-out tasks. It is possible that the very specific input-output format of SuperNatInstmakes it such that changing proportions of unrelated clusters provides no benefit to its fully held-out clusters. PromptSource is relatively unchanged on fully supervised clusters and partially supervised clusters, possibly owing to reaching performance saturation with even an 18% proportion. However, it benefits with more proportion on the fully-held out clusters.\n\nSecondly, we also observe benchmarks complementing each other. For example, the highest accuracy on fully held-out FLAN i.e. 88.8/83.6%, is achieved, not with the highest proportion of FLAN, but with improving the proportions of PromptSource and Crossfit. Similarly, the highest generation performance on fully-held out PromptSource of 79.7/83.5% is achieved with 25% PS, and not with 45% PS proportions. We also observe certain tradeoffs, for example, the best proportions for FLAN and PromptSource result in a sharp drop in performance on reasoning datasets, and vice versa. Finally, setting Crossfit, Exmix, T5 and Unified-SKG proportions to 0 results in the  Table 5: Per-benchmark performance variation at each generalization level with varying benchmark proportions; The first row represents the original proportions in the OPT-IML benchmark. Results are in the format of 0-shot/5-shot. We use only 0-shot performance for Summarization tasks. Most tasks are generation tasks, for which we report Rouge-L. We report accuracy for MMLU. Four tasks in the Cause Effect Cluster also use accuracy, which is averaged with Rouge-L for presentation purposes. We select models based on their average performance aggregated per benchmark and shot. worst model, demonstrating the benefits of using a diverse set of benchmarks for instruction-tuning. Based on average performance across benchmarks, \"2/1/27/40/27/1/2\", \"7/1/35/25/28/2/2\" and \"4/2/20/25/45/2/2\" performed the best and we choose the last one as the proportion for our final OPT-IML models. Despite our choice, instruction-tuned models with different end-goals (for example, producing reasoning chains) would benefit from choosing differently. We also explore methods to improve performance on reasoning datasets in Section 4.6.\n\n\nEffects of Scaling Tasks or Categories\n\nPrevious work has shown that scaling the number of training tasks or clusters improves the overall performance of the model on the fully held-out generalization setting (Wei et al., 2022a;. We study effects along similar axes but with more generalization settings such as fully held-out, partially supervised, and fully supervised tasks/categories. We use cluster/category interchangeably in this section. For the task scaling study, we randomly sample 16, 64, 256, and 1024 sets of tasks such that smaller sets are subset of bigger sets, and fully supervised tasks are always selected. Figure 2 (full results in Appendix Table 17) presents these task scaling studies on the three generalization levels, aggregated at the cluster-level for both 0 and 5-shot performance. We observe that both fully held-out and partially supervised tasks get the most improvements with the increase in the number of training tasks. Interestingly, fully supervised tasks' performance remains unchanged even when more relevant tasks are seen from the fully supervised tasks' clusters, as we increase the training tasks. In the fully held-out setting, Cause Effect Classification and Word Analogy clusters see the biggest improvements in zero-shot and few-shot, respectively. On the partially supervised, Question Answering and Toxic Language Detection clusters see the biggest improvements on both zero-shot and few-shot. For the cluster scaling study, we order the clusters based on the decreasing order of the number of tasks present in each cluster and select the first 4, 16, 64, and 93 (i.e., all) clusters. Additionally, we make sure that Question Answering, Summarization, and Dialogue Generation clusters are always represented since our fully supervised validation tasks belong to these three clusters. Figure 3 (full results in Appendix Table 18) presents the corresponding results on all three generalization levels for both zero-shot and few-shot settings. We observe that as we increase the training clusters, the performance on fully supervised tasks either stay the same or slightly drop in the few-shot setting. On the fully held-out and partially supervised levels, the results on the zeroshot settings improve an increase in the number of clusters and the results are a bit mixed for the fewshot setting, but overall they tend to decrease with cluster scaling. Note that the first 4 clusters already cover 673 tasks (clusters belonging to the fully supervised setting have a lot of tasks). Hence, the model starts with strong performance, which might lead to the mixed results that we observe. Based on these experiments, we use all tasks and clusters to train our final OPT-IML models.\n\n\nEffects of Pre-training during Instruction-Tuning\n\nWe observe that using pre-training style updates on entire sequences during fine-tuning can make training more stable, so we explore the performance effects of using pre-training data on our three generalization levels. Table 6 shows an example used in the pre-training style updates. Following , we use the last shard of the corpus used to train OPT  as our pre-training data for fine-tuning, since it is seen only once during the pre-training stage of OPT. We experiment with adding pre-training data by proportion in the increasing amounts of 1%, 5%, Dataset Example (Input Prompt and Output)\n\nPre-training You could make it a full group party with the kids and wives. Don't make it just about books. So have A movie night My parents made a movie group they go out to dinner then see a movie then dicuss it. You could play card games. Watch some comedy. Ask the members. Do a music night when one of you has to bring a selection of their fav music. Great. What did you work and retire from? I was a tailor. Table 6: Examples from the pre-training, reasoning, and dialogue datasets. For pre-training and dialogue data, the source is empty and the entire text sequence is considered as the target.  Figure 4: Effect of performing pre-training updates on entire sequences, together with instructiontuning on each generalization level for OPT-IML 30B in the 5-shot setting, aggregated by task category. The x-axis represents the % of pre-training updates performed w.r.t the total number of updates.\n\n\nReasoning\n\n10%, and 50%, and present results for the 5-shot setting, aggregated by task category, in Figure 4 (full 0 and 5-shot results in Appendix Table 19).\n\nOverall, for the fully held-out and partially supervised generalization levels, we observe that the model improves while adding pre-training data for up to 10% and then starts deteriorating after that. We also observe that using more pre-training data leads to better Rouge-L F1 scores but lower accuracy scores, partly owing to the influence of pre-training data on the remaining proportions of generation vs. classification tasks. Based on the average scores across generalization levels (see Appendix Table 19), we choose to include 5% pre-training data in instruction-tuning our OPT-IML models.\n\n\nEffects of Adding Reasoning Datasets\n\nRecent work Kojima et al., 2022) has illustrated improvements in the performance of LLMs on reasoning tasks, when prompted to generate a reasoning chain in natural language before generating the answer. Based on these findings, we attempt to explicitly fine-tune LLMs to perform reasoning by compiling a set of 14 reasoning datasets (see Appendix A.1 for a list of  Figure 5: Effect of fine-tuning using reasoning datasets on each generalization level for OPT-IML 30B in a 5-shot setting, aggregated by task category. We experiment with adding 1%, 2% and 4% reasoning datasets by proportion. Note that the baseline for this experiment is based on a different proportion than other experiments.\n\nthese datasets), where the output includes a rationale before the answer and by including these datasets during instruction-tuning. This set includes the 9 datasets used by Chung et al. (2022b) in their CoT category as well as some additional datasets. Each dataset has a single prompt that uses an instruction, that explicitly asks the model to generate a reasoning chain (Kojima et al., 2022), followed by examples in the few-shot setting that illustrate how the reasoning chain should be produced before the answer. We show an example with such a prompt in Table 6. Using benchmark proportions of \"2/1/27/40/27/1/2\" as a baseline (see Section 4.3), we experiment with adding 1%, 2%, and 4% proportions of reasoning data (by reducing the proportion of the highest proportion benchmark i.e. SuperNatInst), and present results for the 5-shot setting in Figure 5 (full 0 and 5-shot results in Appendix Table 20) by generalization level and task category. We see a substantial performance improvement on the 2/14 held-out validation reasoning tasks (Rouge-L from 12.2% to 31.6%) when we instruction-tune with reasoning datasets, but alongside, we also see improvements on other held-out task categories such as Cause-Effect, Stereotype Detection, Toxicity Detection, and Word Analogy. Furthermore, adding 1% reasoning data results in the largest gains overall, beyond which, the gains start to reduce on MMLU, Cause-Effect Accuracy, Toxicity, and Dialogue (averaged over 0 and 5-shot). On the other hand, the Summarization cluster (only 0-shot, see Appendix) continues to benefit from higher proportions of reasoning data. Based on average performance across categories and generalization levels, we use 1% reasoning data for our final OPT-IML models.\n\n\nEffects of Adding Dialogue Datasets\n\nWe experiment with adding dialogues as auxiliary fine-tuning data to test if it can improve the LM's ability to respond to directional input and understand referential expressions. Another goal is to evaluate if this approach can induce chat-bot behaviors  and make the resulting models more conversational. Using a subset of dialogue datasets 6 used for training BlenderBot 3 , we process the dialogues into sequences of turns separated by a single newline token (see Table 6 for an example). The data consists of 320,543 unique dialogues and we fine-tune the model to predict the entire dialogue sequence. We set the proportion of the included dialogue data to be 0.5% and present 0 and 5-shot results by task category and generalization level on our validation split in Table 7.\n\nWe observe that adding even just 0.5% of the aforementioned dialogue data lowers 0-shot performance while 5-shot performance remains unchanged. Specifically, 0-shot performance suffers mainly on stereotype detection and word analogy. On examining model predictions on these categories, we 6. Appendix A.2 list the dialogue datasets we used in this experiments.  Table 7: Effect of fine-tuning with 0.5% dialogue data on each generalization level for OPT-IML 30B after 4000 steps, aggregated by task category. Results are presented in the format 0-shot/5shot. Most categories use Rouge-L F1, MMLU uses accuracy. Some Cause-Effect tasks use accuracy, which is averaged with Rouge-L F1 for presentation purposes.\n\nfound that they are primarily generation tasks whose references are either a single word or a short piece of text with a specific format (for example, a pair of phrases from the original input that refer to each other). Training with BB3 data weakened the model's ability to conform to the required format. 7 It also significantly lowered the 5-shot performance of toxicity detection. An error analysis revealed a similar problem i.e. the model tends to perform worse on tasks that require generating a special set of decision words rather than simply generating \"yes\" or \"no\". Owing to severe model degeneration on these tasks, we do not add dialogue data while tuning OPT-IML.\n\n\nEffects of Meta-Training for In-Context Learning\n\nRecent work has shown that fine-tuning language models with demonstration examples in the instructions improves their ability to learn from the examples in context (Min et al., 2021;Chung et al., 2022b). Both Min et al. (2021) and   Because the demonstration examples significantly increase the prompt lengths, including too many few-shot training examples often leads to worse performance and reduced learning stability, owing to sparsity in the loss and lower batch diversity. As a result, we choose D to be the Zipf distribution 10 , which can be heavily tilted towards k = 0. We train MetaICL models with different D's by adjusting the shape parameter a of the Zipf distribution. When a = 4, 92. 7. On one hand this behavior demonstrates a weakened instruction-following ability for the underlying model. On the other hand, it exposes a caveat in measuring model performance on tasks with instructions -model performance on a specific task category is often the result of multiple factors and underperforming on a particular task category may not offer a useful atomic diagnosis. As in our case, we found the model to perform worse on stereotype detection tasks because it cannot parse the required output format, not because it is a more biased model. 8. In preliminary experiments, we found models trained with k exemplars tend to perform worse when a different number of exemplars is used during inference time. Table 12 shows a similar effect on the Tk-Instruct models. 9. For any k > K, we set k = K. 10. https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.zipf.html Figure 6: We experiment with two types of training losses for MetaICL: the generation loss over the label of the target example as proposed by Min et al. (2021), and the generation loss over the label of the first demonstration example and the complete sequences of the following examples.\n\nMetaICL with suffix loss. To further address the loss sparsity problem, we also experiment with a variation of the original MetaICL loss, illustrated in Figure 6. Given an example with instructions and exemplars, rather than training the model to produce the target label, we train the model to produce the target label of the first exemplar followed by the complete sequences of the remaining exemplars. This effectively turns the demonstration examples into training examples as well, and mitigates the loss sparsity problem given it is now spread over more tokens.\n\nPerformance degradation on generation tasks. We present validation set results for instructiontuning with different settings for MetaICL, aggregated by generalization level and task category under both 0 and 5-shot settings, in Table 8. We observe that adding MetaICL training leads to worse performance in both 0-shot and 5-shot setups in most cases, while MetaICL with the suffix loss outperforms regular MetaICL, especially in the 0-shot setup. Further examination of per-category performance reveals that while MetaICL models show reasonable improvements in multiple 5-shot evaluations, the 5-shot performances on Stereotype Detection and Word Analogy degrade significantly. An error analysis reveals a similar problem as in \u00a74.7 -the MetaICL models tend to lose the ability to strictly follow the output pattern in the presence of in-context exemplars. In addition, the standard MetaICL loss significantly hurts reasoning tasks. The resulting models tend to generate short answers despite the presence of reasoning chains in the in-context learning examples. Further investigation reveals that the model could be over-fitting to the demonstration separators and modifying them at inference time can significantly mitigate these problems (Table 21). 11 Interestingly, MetaICL degrades performance only for generation tasks, but is overall beneficial for scoring based classification tasks such as MMLU. However, owing to severe output degeneration in the regular setting, we decide to not use MetaICL to train our OPT-IML models.  11. Fine-tuning with random demonstration separators may effectively mitigate these issues and we will investigate this approach.\n\n\nOPT-IML Models\n\nUsing the best settings for instruction tuning from our experiments in Section 4, we instruction tune OPT 30B and 175B to create OPT-IML 30B and 175B models. Specifically, we choose the best values for EPS and benchmark proportions, include all tasks in the training split, add 1% datasets with reasoning chains, and 5% data from the OPT pre-training corpus, and choose to leave out training with demonstrations i.e. MetaICL, as well as dialogue datasets. We tune OPT-IML 30B for 4000 steps, while we tune OPT-IML 175B for double the number of steps with half the batch size (for purposes of memory efficiency). Based on periodic validation set metrics, we decide to use the last checkpoint as the final model. We evaluate our OPT-IML models on the OPT evaluation tasks as well as on four multi-task benchmarks from prior work (Wei et al., 2022a;Sanh et al., 2022;Xie et al., 2022; in both zero and 5-shot settings, directly comparing them to individual benchmark specific instruction-tuned models released by prior work. Thus, we compare with baseline OPT models on the evaluation sets used by OPT, with FLAN-137B on the evaluation sets of FLAN (Wei et al., 2022a), with T0pp 11B on the evaluation sets from PromptSource (Sanh et al., 2022), with Tk-Instruct 11B on the evaluation sets from Super-NaturalInstructions , and on joint modeling of text with code/structs on three tasks from the UnifiedSKG (Xie et al., 2022) benchmark. We examine these results in the following sections, and find that OPT-IML outperforms OPT on all benchmarks and is competitive with the individual benchmark specific instruction-tuned models on both zero-and few-shot performance.\n\n\nOPT Evaluations\n\nWe evaluate OPT-IML on a subset of 14 standard NLP tasks reported by OPT  on zero and few shot settings at 30B and 175B scales, using the same prompts released by OPT (a single prompt per task). All these tasks are classification-style tasks with multiple candidates, so similar to OPT, we use the candidate with the highest likelihood as the model prediction and report accuracies in Table 9. Additionally, all these tasks are held-out during training, some from our fully-held out categories and some from our partially held-out categories. For the few-shot setting, we use the same examples and number of shots used by OPT i.e. 32-shots, but truncated to fit within the model's maximum sequence length.  Table 9: Accuracies of OPT-IML compared with OPT on the 14 standard NLP tasks from  in the format of 0-shot/32-shot. For ARC, (e) denotes (Easy) and (c) denotes (Challenge).\n\nOn average, OPT-IML improves over OPT with approximately 6-7% on 0-shot accuracy at both 30B and 175B model scales. For 32-shot accuracy, we see significant improvements on the 30B model, and milder improvements on 175B. While the improvements are significant for certain tasks such as RTE, WSC, BoolQ, ARC, CB, and WiC, our instruction-tuning does not improve performance for other tasks such as StoryCloze, PIQA, Winograd, and Winogrande. Some of these latter results are specific to the prompts used by OPT. For example, we observe improvements on StoryCloze and Winogrande, when evaluated on a collection of prompt templates as part of PromptSource in Section 5.2. One reason for this is that OPT prompts were originally adopted from GPT-3 (Brown et al., 2020a) and have gone through a process of prompt engineering for optimal performance, while FLAN and PromptSource evaluate accuracies as averages using a diverse collection of prompts, including sub-optimal prompts. Thus, an advantage of instruction-tuning for these tasks can be to improve model robustness and reduce the need for prompt engineering.\n\n\nEvaluations on PromptSource\n\nSanh et al. (2022) fine-tune an LM adapted version of T5 11B (Raffel et al., 2020;Lester et al., 2021) on 50 datasets from PromptSource (called T0) and evaluate on a set of 11 held-out tasks which are part of their 4 fully held-out categories. Each task is associated with multiple prompt templates, contributed by the research community with the help of their prompting tool. Since all these tasks are also part of held-out categories in OPT-IML, we use a similar evaluation setup, with some additional tasks as well. Most tasks are classification tasks where we score candidates based on likelihood and report accuracy, with the exception of Blended Skill Talk, which is a generation task where we report Rouge-L F1 scores. Since each task uses multiple prompts, we report metrics averaged across prompts under 0-shot and 5-shot settings in Table 10.  Table 10: Zero-and 5-shot performance of OPT-IML 30B and 175B compared with baseline OPT models as well as the T0-original-task-only 11B model on the evaluation tasks of Sanh et al. (2022). We report Rouge-L F1 for Blended Skill Talk and use accuracy for all other tasks. Each task metric is reported as an average over multiple original-task prompts for that task. All tasks are held out for both OPT-IML as well as T0.\n\nSome of the prompts gathered in PromptSource are for an inverted version of the task. For example, the inverted task for QA is question generation. We do not train or evaluate using these prompts, since they are problematic when tasks are assigned to categories. We compare OPT-IML with the T0-original-task-only model which corresponds to our held-out setup (Sanh et al. (2022) also release T0p and T0pp trained with additional tasks), and is also trained only on prompts that adhere to the original task.\n\nOPT-IML 175B matches the zero-shot performance of T0-original-task (11b) and outperforms it significantly on 5-shot performance. While both models were not trained on demonstrations, causal LMs like OPT demonstrate stronger generalization to the few-shot setting than encoderdecoder models like T0, and the latter could benefit from MetaICL training to improve its few-shot performance, as explored by Chung et al. (2022b). Similarly, on the Blended Skill Talk generation task, T0 underperforms causal LMs, which could be attributed to the large scale of the tuning data for OPT-IML, or may highlight a difficulty for encoder-decoder models to generalize to new generation tasks. At both scales, OPT-IML outperforms baseline OPT models on almost every task except Crows Pairs. As described in Section 5.1, this evaluation uses multiple prompts per task and rewards models that are more robust to the input prompts. Additionally, note that OPT-IML 30B outperforms baseline OPT 175B on average, demonstrating that instruction-tuning can be a way to make smaller-scale resource-efficient models more competitive.\n\nFollowing Sanh et al. (2022), we also evaluate on the WinoGender Schemas  cast as a textual entailment task (Poliak et al., 2018), which measures the extent of gender bias in LLMs, and find that instruction-tuning vastly improves accuracy on this task. Finally, we evaluate on Crows Pairs (Nangia et al., 2020) formulated as a boolean QA task about whether a sentence illustrates a stereotype or not (using a single prompt), and see a deterioration in performance on OPT-IML 175B over OPT, but not on the 30B model. It is possible that other formulations of this task, for example, predicting which sentence is a stereotype, may show different trends. Note that these two tasks are not from held-out clusters, so there may be other training datasets that are beneficial.\n\n\nEvaluations on FLAN\n\nTogether with the FLAN instruction-tuning benchmark comprising 62 datasets, which we include in OPT-IML Bench, Wei et al. (2022a) also use it to instruction-tune Lamda-PT (Thoppilan et al., 2022), a 137B causal LM trained on 1.5T words of public dialog data and web text. They evaluate instruction-tuning using FLAN-137B on fully held-out task categories, by using a leave-one-out strategy i.e. they tune on all other categories, thus producing a different model to evaluate each test category. This presents an opportunity for evaluating OPT-IML models on the same task categories to assess the improvements that can be achieved by scaling up the instruction tuning benchmark to 1500 tasks using a single instruction-tuned model.  We report accuracy scores in the format of 0-shot/k-shot, where k=5 for our models whereas FLAN uses a different k for each task. There is no few-shot setting for WSC. FLAN-137B performance is based on multiple models trained using a leave-one-category-out strategy.\n\nWe evaluate our OPT-IML models on a subset of tasks used by FLAN-137B, and based on our splits, some tasks are from fully-held out categories (ANLI, CB, MNLI, RTE, SNLI, WNLI, Winogrande, WSC), while the remaining are from partially held-out categories (BoolQ, OpenBookQA, ARC). All these tasks use a classification style with answer candidates, which we evaluate by scoring based on likelihood, and we report zero-shot and few-shot accuracies in Table 11. Note that each task is associated with 7-10 templates, and we report average accuracy across all templates. Some templates invert the task (for example, QA becomes question generation), and we do not evaluate on these templates. Also, while FLAN-137B uses a different number of shots for each task for their few-shot evaluation, we report 5-shot results for all tasks.\n\nWe find that instruction-tuning significantly improves performance over baseline OPT models at 30B as well as 175B scales on each of the 15 tasks individually. While Wei et al. (2022a) found instruction-tuning to hurt fully-held out tasks at 8B and lower scales, but showing emergent behavior at a scale of 66B parameters and beyond, our experiments do not show this emergent behavior i.e. both 30B and 175B OPT-IML models achieve more than 20% average improvement over the respective untuned models under 0-shot and few-shot settings. Additionally, our 30B OPT-IML model outperforms the 175B base OPT model by 20% on 0-shot and 12% on 5-shot, illustrating that instruction-tuned models at lower scales can be strong resource-efficient alternatives to larger untuned models. Compared with FLAN-137B, OPT-IML 175B performs competitively on 5-shot performance, and yields an improvement of 3% on average on 0-shot performance. Nevertheless, the various differences in experimental setup relating to the held-out clusters, model sizes and the number of pre-training tokens, make it difficult to definitively attribute these improvements to scaling up the instruction-tuning benchmark.\n\n\nEvaluations on Super-NaturalInstructions\n\nDifferent from the evaluations seen so far, Super-NaturalInstructions uses a strict instructional format (Section 2), where a formal instruction block is provided at the start of the prompt, detailing option candidates and resolving task ambiguities, followed by multiple demonstrations, and can help assess the ability of our models to generalize to different instruction formats.  subdivide the SuperNatInstbenchmark into training and held-out categories, and train Tk-Instruct 3B and 11B, which are instruction-tuned versions of LM-adapted T5 models. They evaluate Tk-Instruct on 12 categories representing 154 tasks for fully held-out generalization. Of these 12 categories, Textual Entailment, Coreference Resolution and Dialogue Act Recognition are fully heldout in our evaluation framework. We evaluate OPT-IML on these three categories in 0-shot, 2-shot and 5-shot settings and report Rouge-L F1 scores in Table 12. These three categories comprise 44 tasks and we evaluate on the top-100 examples from these tasks following , with each task using a single prompt. In all cases, we generate a maximum of 256 tokens for each test example. For comparison, we also re-evaluate Tk-Instruct 11B on these clusters under the same evaluation framework. We use the version of Tk-Instruct 11B that performs best overall i.e. the version trained with instructions + 2 positive demonstrations and no negative demonstrations.\n\n\nModel\n\nTextual   . We report Rouge-L F1 scores in the format of 0-shot/2shot/5-shot performance. We use the version of Tk-Instruct trained with instructions + 2 positive demonstrations and no negative demonstrations.\n\nSince Tk-Instruct is trained and evaluated under a 2-shot setting, we additionally report results on the 2-shot setting for this evaluation. First, OPT-IML models outperform baseline OPT models on each cluster at both scales, under 0-shot and all few-shot settings and once again we observe that an instruction tuned 30B model outperforms an untuned 175B model. Also, while both OPT 30B and 175B perform comparably at all shots, the instruction-tuned version of 175B vastly outperforms OPT-IML 30B, showing that larger models can benefit more from instruction tuning. Note that different from the Textual Entailment and other tasks from previous evaluations, all tasks here are evaluated under the generation setting (as opposed to scoring), which makes it significantly harder for untuned models. OPT-IML 175B outperforms Tk-Instruct 11B on 0-shot formats despite the former being tuned on a mixed-set of diverse formats from multiple benchmarks, whilst the latter being specifically tuned for this benchmark. The trend is reversed for the 2-shot and 5-shot settings where Tk-Instruct outperforms OPT-IML. Here, OPT-IML shows uniform performance under both settings whereas Tk-Instruct is heavily biased towards the 2-shot setting for which it was trained. Thus, the performance of Tk-Instruct drops from 65.3 to 58.4, from 2-shot to 5-shot.\n\n\nEvaluations on UnifiedSKG\n\nUnifiedSKG (Xie et al., 2022) is a collection of 21 tasks related to Structured Knowledge Grounding with heterogeneous inputs such as databases, dialogue states, SQL queries, etc., which we include in OPT-IML Bench purposefully to equip the model with capabilities for handling structured knowledge. To evaluate these capabilities, we compare OPT-IML models with baseline OPT on three UnifiedSKG tasks formatted as text-to-text: DART (Nan et al., 2020), which is a held-out datato-text task for transforming data triples to text, Spider (Yu et al., 2018), a SQL query generation task given a database and an input query, and fully supervised in our framework, and MultiWoZ (Budzianowski et al., 2018), is a held-out dialogue state tracking task. All three tasks are generation tasks where we decode 256 tokens before stopping and report Rouge-L F1 scores under 0-shot and 5-shot settings in Table 13.   (Nan et al., 2020), Text to SQL Generation (Spider) (Yu et al., 2018), and Dialog State Tracking (MultiWoZ) (Budzianowski et al., 2018). We report Rouge-L scores in the format of 0-shot/5-shot.\n\nOn Spider, which is a fully supervised setting, OPT-IML models retain high performance close to a Rouge-L F1 score of 85 despite the presence of numerous other tasks in the instruction-tuning mix. On DART, OPT-IML shows modest gains in the 5-shot setting, but significantly outperforms OPT models on the zero-shot setting, with OPT-IML 30B outperforming OPT 175B. MultiWoZ, on the other hand shows significant deterioration with instruction tuning at both model scales.\n\n\nDiscussion and Limitations\n\nIn the previous section, we demonstrated on multiple evaluation benchmarks that effectively instructiontuned models can obtain significant improvements over untuned models on both zero-and few-shot settings. We achieved this by first scaling up the instruction-tuning datasets to encompass 8 large collections of NLP tasks, which we transform into an evaluation framework that tests three levels of model generalization on downstream tasks. Using this framework, we characterized the tradeoffs of different factors on instruction tuning such as 1) the number and diversity of input tasks, 2) the distribution of different tasks and instruction styles, 3) the inclusion of specialized datasets relating to reasoning chains and dialogue, and 4) fine-tuning with demonstrations. This exploration helped us choose the best settings to instruction tune OPT-IML models at 30B and 175B scales, which perform competitively on an extensive set of benchmarks.\n\nIn this section, we report additional results on instruction fine-tuning using our full task collection and discuss the limitations of our current approach.  While we transform our massively scaled instruction-tuning benchmark into an evaluation framework to study instruction-tuning techniques, recently Chung et al. (2022b) also scaled up instruction fine-tuning up to 1,836 tasks from 4 benchmarks using the PaLM models  up to 540B and T5 models (Raffel et al., 2020) up to 11B 12 . The resulting models, namely the FLAN-PaLM and FLAN-T5 series were evaluated on several challenging language model benchmarks including MMLU (Hendrycks et al., 2021a), and Big Bench Hard (BBH) (Srivastava et al., 2022). In order to establish the performance of OPT-IML in a similar setting (and additionally, on RAFT (Alex et al., 2021)), we instruction-tune OPT 30B and 175B on our entire benchmark of 1,991 tasks, which we call OPT-IML-Max. We use option scoring for the two classification benchmarks MMLU and RAFT, and generation with Exact Match for BBH. We evaluate on the test sets for MMLU and BBH and on the evaluation split for RAFT released by the HELM benchmark (Liang et al., 2022). We report these results in Table 14 together with other large pre-trained and instruction-tuned models. Additionally, we also train and present results for OPT-IML-Max at the 1.3B scale (using the same settings as OPT-IML-Max 30B). On all three datasets, OPT-IML-Max outperforms its untuned counterparts at all scales (except 1.3B on BBH). While, OPT-IML-Max is competitive with FLAN-T5 11B on RAFT, its performance lags behind FLAN-T5, FLAN-PaLM and the family of instruction-tuned GPT-3 models (*-davinci-*) on MMLU and BBH. While the scale of the instruction-tuning benchmark remains similar across these models, there are many other underlying differences. There is a large variation with respect to the number of tokens used to train the respective underlying pre-training models. For example, T5 is trained on 1T tokens, FLAN-PaLM on 800B and OPT on 180B. There are also differences relating to the composition of the pre-training data and the respective modeling architectures.  find that encoder-decoder models can fine-tune more effectively than decoder only models at similar scales, and massively scaling up decoder-only models can make them more competitive. Finally, there are also differences in the fine-tuning algorithms used, for example, some of the Ope-nAI davinci models use RLHF (Christiano et al., 2017) on feedback signals gathered from their API in addition to supervised fine-tuning. While we found that using Meta-ICL ( \u00a74) did not yield a holistically better model and did not include it in our final models, they yielded 2-3% improvements on MMLU and BBH. All these factors make it difficult to explain the gap in performance on these benchmarks, but nevertheless, these evaluations serve to establish the effects of our instruction tuning decisions with respect to OPT models on these challenging external benchmarks.\n\n\nEvaluations on MMLU, BBH and RAFT\n\n\nLimitations\n\nWe use our evaluation framework to characterize the tradeoffs of various instruction-tuning variables on OPT 30B independently of each other. Although resource intensive to test, it is possible for these 12. Our work started concurrently.\n\nvariables to interact with each other resulting in a different choice of the best tuning settings (for example, adding reasoning datasets may affect the choice of benchmark proportions). Furthermore, all tradeoffs studied on 30B instruction tuning may not show the same trends at larger scales. While we study instruction tuning tradeoffs using a comprehensive set of splits of fully held-out, partially supervised and fully supervised categories, choosing a different set of categories may result in prioritizing different decisions than those we took in this paper. Although we assign tasks to categories based on the underlying formats, such an assignment can be subjective and a different category assignment might change the optimal factors for instruction-tuning. For example, tasks that require different skills such as detecting toxicity can also be cast as textual entailment tasks.\n\n\nResponsible AI\n\nWhile OPT-IML models outperform baseline OPT on an extensive set of evaluations (Section 5), nevertheless, they are susceptible to the various risks associated with using large language models relating to factual correctness (Thoppilan et al., 2022;Brown et al., 2020a;, generation of toxic language (Gehman et al., 2020) and enforcing stereotypes. While we release our OPT-IML models to proliferate future work on instruction-tuning and to improve the availability of large instruction-tuned causal LMs over 100B parameters, the use of these models should be accompanied with responsible best practices.\n\n\nRelated Work\n\nOur work on fine-tuning large language models to follow instructions span multiple areas such as multi-task learning, prompting, and meta-training of in-context learning. We discuss these areas below within the scope that most closely relate to our work.\n\nInstruction Tuning. Language models are trained to predict the next token in a sequence with self-supervised learning (Brown et al., 2020a;. Prompt engineering and in-context learning has become a dominant approach to leverage these models to solve many NLP tasks. In order to align these models to follow natural instructions and avoid prompt engineering, recent works have proposed instruction fine-tuning (Ouyang et al., 2022;Wei et al., 2022a;Chung et al., 2022b;. Some of these works focus on fine-tuning the model on a wide range of tasks using human annotated prompts and feedback (Ouyang et al., 2022), whereas the others focusing on supervised fine-tuning using academic benchmarks and datasets augmented with manually or automatically generated instructions Wei et al., 2022a;Sanh et al., 2022;Zhong et al., 2021). In our work, we focus on the second approach and consolidate a massive collection of publicly available datasets with instructions to finetune OPT. Concurrent to our work, Chung et al. (2022a) also proposes a similar instruction benchmark scaling approach to 1836 tasks from 4 benchmarks. While they focus on fine-tuning using the entire benchmark in order to push the limits of performance on several challenging held-out tasks that test the model's world knowledge and reasoning capabilities such as MMLU (Hendrycks et al., 2020) and Big-Bench Hard (BBH) (Suzgun et al., 2022), we focus on characterizing the tradeoffs of various instruction-tuning decisions that can affect downstream performance.\n\nPrompting and Meta-Training Zero-and few-shot learning (a.k.a. in-context learning) that leverages very few examples to solve any NLP task by effectively prompting the language models, is becoming a dominant paradigm in recent years (Brown et al., 2020a). Prompting involves modifying the input and output space of a given task that can effectively leverage the knowledge of the language model to solve it. Various approaches have proposed better prompting ways to improve generalization performance Lu et al., 2021). Furthermore, recent developments have shown ways to improve in-context learning (ICL) by meta-tuning language models to better adapt for ICL (Min et al., 2022(Min et al., , 2021. In our work, we leverage both the variants of prompts available from different benchmarks, as well as meta-training with demonstrations from a large pool of tasks, to study the effective settings for instruction-based fine-tuning that induce robustness against different prompting language and setups.\n\nLearning to Reason. Despite the progress of in-context learning, state-of-the-art LLMs still struggle with reasoning tasks such as commonsense reasoning (West et al., 2022), and math word problems (Hendrycks et al., 2021b) which require arithmetic reasoning, etc. To solve these challenging tasks, recent work used different prompting methods which include a rationale with the final answer in the form of a scratchpad for arithmetic and logical reasoning (Nye et al., 2021), provided chain-of-thought prompts in demonstrations , or added trigger phrases such as let's think step-by-step to prompt models to generate explanations (Kojima et al., 2022). In addition to changing prompts, Chung et al. (2022a) integrated step-by-step explanations into the instruction tuning stage. Following Chung et al. (2022a), we further expand the set of reasoning datasets to 14 datasets and study the effects of different proportions of reasoning data on different held-out task clusters.\n\nMulti-task Learning. Instruction-based fine-tuning can be considered as a formulation of multitask Learning (MTL). MTL is a popular paradigm that improves the generalization performances of a task when combined with related tasks by sharing common parameters or representations (Caruana, 1997;Kumar and Daume III, 2012). MTL has been applied to many NLP scenarios in recent years primarily focusing on improving the performance on the training tasks or to new domains by leveraging the signal from related tasks (Collobert and Weston, 2008;McCann et al., 2018;Raffel et al., 2020;Vu et al., 2020). In contrast, instruction-based fine-tuning allows us to improve the generalization performance to new tasks that are never seen during training. This is achieved by unifying all the tasks into a common format (Kumar et al., 2016;Khashabi et al., 2020) via instructions, and training them together by sharing all the weights of the model across all tasks.\n\nContinuous Learning. Existing work also address continuous adaptation of language models by revisiting the instructions  or examples (Scialom et al., 2022) of previously learned tasks when fine-tuning with a new task to prevent catastrophic forgetting. The results show that LMs can be adapted effectively to new tasks without losing sight of the previously learned tasks. Other work enable the LM to perform new tasks via arithmetic combination of learned task vectors (Ilharco et al., 2022) or soft prompts (Anonymous, 2023) patched to the base LM without changing its parameters. We focus on the (massively) multi-task adaptation setting by fine-tuning the LM with 2000 tasks at once. Continuously adapting the resulting model to new data, new tasks and new domain would be an interesting and important future direction.\n\n\nConclusions\n\nInstruction-tuning of LLMs has emerged as an effective means to improve their zero and few-shot generalization abilities. We make three main contributions to instruction-tuning in this paper. First, we curate a large scale benchmark for instruction-tuning comprising 2000 NLP tasks from 8 dataset collections, annotated into task categories. We strategically produce evaluation splits on this benchmark to evaluate three different types of model generalization abilities: 1) fully-supervised performance, 2) performance on unseen tasks from seen task categories, and 3) performance on tasks from completely held-out categories. Second, using our evaluation suite, we establish tradeoffs and best practices of many aspects of instruction-tuning, such as different sampling methods of finetuning tasks and categories, fine-tuning with task demonstrations, and fine-tuning with specialized datasets for reasoning and dialogue. Finally, using the best settings from our experiments, we train and release OPT-IML 30B and 175B instruction-tuned models based on OPT, that strongly outperform OPT on five evaluation benchmarks and are competitive with recent instruction-tuned models that are tuned on individual benchmarks. \n\n\nAppendix A. Benchmark Preparation Details\n\nWe present details relating to downloading and pre-processing tasks from the benchmarks we use in this paper.\n\n\nA.1 Data Curation\n\nWe download all benchmarks from the official data release, except for ExMix (Aribandi et al., 2022) where the official data is not open-sourced.\n\nSuper-NaturalInstructions. We downloaded the data from https://github.com/allenai/ natural-instructions/tree/v2.6.\n\nPromptSource. We download the data from https://github.com/bigscience-workshop/ promptsource (commit #0cc4b0c). Each task can be instantiated with multiple crowd-sourced templates. We only use the templates meant for the original task. For the validation and test splits, if a template does not apply to all examples, we ignore this template -with the exception of the turk dataset, where we allow this.\n\nCrossFit. We download the data from https://github.com/INK-USC/CrossFit (commit #56285ca).\n\n\nFLAN.\n\nWe use the FLAN codebase (https://github.com/google-research/FLAN) and the seqio and Tensorflow Datasets 13 Python libraries to download all instantiations of each task example. We only use the templates that represent the original task, and ignore the templates that invert the task.\n\nExMix. We use the following URLs to download tasks from ExMiX that do not overlap with other benchmarks: COGS from https://github.com/najoungkim/COGS (commit #bf1efc) Shakespearizing-Modern-English from https://github.com/harsh19/ Shakespearizing-Modern-English (commit #e2669e) StylePTB from https://github.com/lvyiwei1/StylePTB (commit #2b7258) gpt-2-output-dataset from https://github.com/openai/gpt-2-output-dataset (commit #2c1024) Parsing to FunQL from https://github.com/JasperGuo/Unimer (commit #b61e8e) UKP from https://tudatalib.ulb.tu-darmstadt.de/handle/tudatalib/2345 NewsQuizQA from https://github.com/google-research-datasets/NewsQuizQA (commit #648246) Dialoglue from https://github.com/alexa/dialoglue (commit #683663) KILT from huggingface:kilt_tasks/wned and huggingface:kilt_tasks/aidayago2 Wiki Toxicity from tfds:wikipedia_toxicity_subtypes T5. We use the T5 codebase (https://github.com/google-research/ text-to-text-transfer-transformer) and the seqio and Tensorflow Datasets Python libraries to download all instantiations of each task example. After removing tasks that overlap with the other benchmarks, we kept 7 datasets from this benchmark: wmt14_ende_v003, wmt14_enfr_v003, wmt15_enfr_v003, wmt16_enro_v003, wmt19_ende_v003, wmt_t2t_ende_v003, cnn_dailymail_v002 14 .\n\nUnifiedSKG. We download the instantiated examples from the Google Drive link provided by the authors: https://drive.google.com/drive/folders/1GXigUv3MU-Sh4XiY6Wz3xVeNT_s0SuON.\n\n13. https://www.tensorflow.org/datasets 14. Other benchmarks contain CNN-dailymail as well. We kept this dataset owing to its high quality.\n\nChain-of-thought Reasoning. Our reasoning benchmark contains 14 datasets: GSM8K (Cobbe et al., 2021), StrategyQA (Geva et al., 2021), AQUA-RAT (Ling et al., 2017), CoQA (Reddy et al., 2019), CoS-E (Rajani et al., 2019), CREAK (Onoe et al., 2021), ECQA (Aggarwal et al., 2021), e-SNLI (Camburu et al., 2018), MATH (Hendrycks et al., 2021b), ProofWriter , QASC , QED (Lamm et al., 2021), SenseMaking (Wang et al., 2019), WinoWhy .\n\n\nA.2 Details of Dialogue Datasets\n\nWe considered 6 dialogue datasets in total for the experiments in \u00a74.7: Wizard of Internet , Wizard of Wikipedia (Dinan et al., 2019b), Blended Skill Talk , ConvAI2 (Dinan et al., 2019a), Multi-Session Chat  and Light+ Wild (Urbanek et al., 2019;Shuster et al., 2021). These are a subset of those used by .\n\n\nA.3 Details on Validation Tasks\n\nOur experimental studies in Section 4 present results on our validation set which is comprised of several tasks and categories.  \n\n\nAppendix B. Additional Experimental Results\n\n\nB.1 Results at Additional Model Scales\n\nIn addition to the results presented in Section 5.1, we also trained OPT-IML 1.3B by fine-tuning OPT 1.3B (using the same settings as used for OPT-IML 30B). We extend Table 9 that presents results on 14 standard NLP tasks reported by OPT  on zero and few-shot settings, by including results at the 1.3B scale, in Table 16.  \n\n\nB.2 Additional Results on Factors Affecting Instruction-Tuning\n\nSection 4 presents our experimental studies on a multitude of factors related to our fine-tuning process. Here, we present additional results accompanying those studies. Table 17 presents the full results of our task scaling studies at all three generalization levels. Table 18 presents our study on the effect of scaling the number of task clusters. Table 19 presents the study on the impact of using pre-training data during fine-tuning. Table 20 presents results on the impact of using reasoning datasets during fine-tuning at each generalization level. Finally, Table 21 presents a version of our MetaICL experiments presented in Section 4.8 using a \"\\n\\n\" example separator during inference. For all tables, results are in the format of 0-shot/5-shot. We use only 0-shot performance for summarization tasks. Most tasks are generation tasks, for which we report Rouge-L. We report accuracy for MMLU. Some tasks in the Cause Effect Cluster also use accuracy, which is averaged with Rouge-L for presentation purposes.      Table 21: A repeat of the MetaICL experiments reported in \u00a74.8 using \"\\n\\n\" as the example separator during inference. Under this setting, all MetaICL models outperform the baseline model. \u2020 The 5-shot baseline performance is not comparable with those in the other experiment tables since we also include the 5-shot performance on summarization tasks here.\n\n\nAppendix C. Examples of Prompts from All Benchmarks\n\nIn this section, we present several examples from all the benchmarks to get an overview of how the prompts look like that are used for our fine-tuning. In all the examples, 'black' colored text represents prompt and 'green' colored text represents the output that we optimize in our loss function.\n\nInstructions: You are provided with an arithmetic question. Your task is to compute the solution using the given arithmetic operations. The only arithmetic operators needed to answer the questions are '+'(addition) and '-'(subtraction). The answer should be correct to one decimal place. Sam went to 14 football games this year. He went to 29 games lastyear. How many football games did Sam go to in all? Output: 43.0 In this task you will be given a list of numbers and you should remove all duplicates in the list. If every number is repeated in the list an empty list should be returned. Your list should be numbers inside brackets, just like the given list.\n\n[0, 0, 3, 6, 5, 3, 0, 4, 1, 5] A: [6, 4, 1]\n\nFigure 9: Zero-shot example of task097_conala_remove_duplicates task from program execution category of Super-NaturalInstructions benchmark.\n\nIn this task you will be given a list of integers. You should remove all of the integers that are divisible by 3 from the list. If every integer in the input list is divisible by 3 then an empty list should be returned. Zero is divisible by 3. Input: [-71, 46, 80, -17, -57, 67, -90, -65, 93, 17, 48] Answer: [-71, 46, 80, -17, 67, -65, 17]  Instructions: This task is about reading the given passage and construct a question about the information present in the passage. Construct a question in such a way that (i) it is unambiguous, (ii) it is answerable from the passage, (iii) the answer is unique, (iv) its answer is a continous text span from the paragraph. Avoid creating questions that (i) can be answered correctly without actually understanding the paragraph and (ii) uses same words or phrases given in the passage Czechoslovakia or Czecho-Slovakia (; Czech and , \"\u010cesko-Slovensko\") was a sovereign state in Central Europe that existed from October 1918, when it declared its independence from the Austro-Hungarian Empire, until its peaceful dissolution into the Czech Republic and Slovakia on 1 January 1993.\n\nFrom 1939 to 1945, following its forced division and partial incorporation into Nazi Germany, the state did not \"de facto\" exist but its government-in-exile continued to operate.\n\nFrom 1948 to 1990, Czechoslovakia was part of the Soviet bloc with a command economy. Its economic status was formalized in membership of Comecon from 1949, and its defense status in the Warsaw Pact of May 1955. A period of political liberalization in 1968, known as the Prague Spring, was forcibly ended when the Soviet Union, assisted by several other Warsaw Pact countries, invaded. In 1989, as Marxist-Leninist governments and communism were ending all over Europe, Czechoslovaks peacefully deposed their government in the Velvet Revolution; state price controls were removed after a period of preparation. In 1993, Czechoslovakia split into the two sovereign states of the Czech Republic and Slovakia.\n\nThe country was of generally irregular terrain. The western area was part of the north-central European uplands. The eastern region was composed of the northern reaches of the Carpathian Mountains and lands of the Danube River basin. answer: Was Czechoslovakia ever apart of the Soviet bloc? Figure 12: Zero-shot example of task917_coqa_question_generation task from question generation category of Super-NaturalInstructions benchmark.\n\nClassify given movie review into two categories: positive, or negative based on its content. director jay russell stomps in hobnail boots over natalie babbitt's gentle , endearing 1975 children's novel . A: negative Figure 13: Zero-shot example of task888_reviews_classification task from sentiment analysis category of Super-NaturalInstructions benchmark.\n\nInstructions: You are provided with an \"Event\" and it's \"Intent\" related to PersonX. Determine the sentiment value of the given input as either \"Positive\", \"Negative\", and \"Unknown\". Event:PersonX is PersonY's last day. Intent: 1) eat the same thing. output: Negative Figure 14: Zero-shot example of task923_event2mind_classifier task from sentiment analysis category of Super-NaturalInstructions benchmark.\n\nIn this task, you are given a text of news article and corresponding headline of an article. Your task is to give label \"match\" if headline is correct for article, otherwise give label \"no\". Article: Mike Snoei coached side Pune FC bounced back to winning ways with a 2-0 win over Mohammedan Sporting in an I-League round 21 encounter at the Balewadi sports complex, here on Wednesday. Headline: Pfc bounce back to winning ways Answer: match Figure 15: Zero-shot example of task1354_sent_comp_classification task from text matching category of Super-NaturalInstructions benchmark.\n\nIn this task, you are given two sentences. Your task is to classify the given sentences as \"Yes\" if they have same meaning; otherwise, classify them as \"No\". Sentence-1: I'd prefer to go to the beach. sep Sentence-2: I would like to go out for a drink . answer: No Figure 16: Zero-shot example of task566_circa_classification task from text matching category of Super-NaturalInstructions benchmark.\n\nRead the following article and answer the question.\n\nSo now I have no problems . In fact , everything is getting better . I got a job at pizza hut , and it looks like the school paper hired me as their cartoonist as well , so basically everything 's okay now .\n\nWhat may be an invalid reason I am feeling better now ? OPTIONS: -I may be able to show my imagination .\n\n-I may write a lot of papers for my English class .\n\n-None of the above choices .\n\n-I may be able to save money . Answer: I may write a lot of papers for my English class . The album featured the number-one song \"Single Ladies (Put a Ring on It)\" and the top-five songs \"If I Were a Boy\" and \"Halo\". Achieving the accomplishment of becoming her longest-running Hot 100 single in her career, \"Halo\"'s success in the US helped Beyonc\u00e9 attain more top-ten singles on the list than any other woman during the 2000s. It also included the successful \"Sweet Dreams\", and singles \"Diva\", \"Ego\", \"Broken-Hearted Girl\" and \"Video Phone\". The music video for \"Single Ladies\" has been parodied and imitated around the world, spawning the \"first major dance craze\" the head of the united nations nuclear watchdog expressed concern thursday about iran 's growing nuclear capacity and his organization 's powerlessness to monitor the program .\n\nWrite a brief summary in a sentence or less Output: iaea chief concerned about inability to monitor iran nuclear program Figure 19: Zero-shot example of gigaword task from summarization category of FLAN benchmark.\n\nWrite highlights for this article:\n\nBy . Adrian Durham . Follow @@talkSPORTDrive . So the English obsession with Andrea Pirlo continues. It's as if a load of FIFA-playing so-called football fans have discovered an alien species. A player who fundamentally stays in one place on the field, and shifts the ball about relying not on pace or physical strength, but on spatial awareness and control of the football. These English 'Pirlovers' think he comes from another planet. Of course the suave appearance, the masterful beard, and the 'couldn't give a f***' autobiography all help add to the mystery and attraction of this Italian God- figure. VIDEO  The WHIMS study found that combination hormone therapy doubled the risk for probable dementia in women 65 and older and did not prevent mild cognitive impairment . One study found that combination therapy doubled the risk of probable dementia and did not prevent less-severe mental decline . Are these two sentences paraphrases of each other? OPTIONS: -no -yes output: yes Keywords: cat, look, mouse\n\nWhat is a sentence that includes all these keywords? Answer: a cat is looking at a dead mouse he killed Figure 25: Zero-shot example of common_gen task from data to text category of FLAN benchmark.\n\nKeywords: cat, look, mouse\n\nWhat is a sentence that includes all these keywords? Answer: a cat is looking at a dead mouse he killed Figure 26: Zero-shot example of common_gen task from data to text category of FLAN benchmark.\n\nHow great it's the sound?\n\nAnswer using extracts from the following context. If you can't find an answer, return Unanswerable\n\nContext: When hubby was at work, I would often turn on a computer radio program to listen to while I did housework. Unfortunately, he was seriously injured at work almost 2 years ago. His sleep is erractic and pain makes the only place sleep is possible, is his recliner. Since I never know when he will be able to sleep, I stopped listening to the programs I had enjoyed, so it wouldn't bother him. My brother suggested I check out wireless headphones. He told me a very little bit, but mostly look for the kind that were wireless and not line of sigth, in other words the kind that could go through walls, ceilings, etc. So, I started researching and reading. I thought I would just pick up a pair at Best Buy or even Walmart. Well, the ones they both carried didn't have great write ups nor were they any cheaper than these Sennheiser RS120's from Amazon's seller Electronic Expo. Obviously, cost is important with out current situation. I'd never heard of Sennheiser. Left to buy by brand, I would have bought something familiar like Sony, but I kept reading good things about Sennheiser on every site I checked. The price was as good or better than the brands I had heard before. So I did rely heavy on the comments of others -thanks to you who posted. I am not an audiophile or techie at all.So now you know why I bought them, on to the headphones and how they work. In one word -excellent! I love, love, love them! I put them on and listen to the computer radio programs as I do housework and they don't disturb hubby from sleeping. Finally, once again I can enjoy what I want, when I want. I have only had them a month or so but I use them almost everyday from 1-2 hours, to all day long because they are compfortable enough to wear for hours, if I choose. Yeah, the ears will get a little warm but really not bad at all since they are not cupped over them. What little sound may escape doesn't bother hubby as I am not setting beside him when I use them and I doubt they would do so even then, because I don't have anything that close to my.. Question: How many punts were returned for a score? Answer based on following passage.\n\nComing off their Thanksgiving win over the Raiders, the Cowboys went to Giants Stadium for the final time to face off against their NFC East rival the New York Giants. With a scoreless first quarter the Cowboys scored a field goal and touchdown by Tony Romo to Roy Williams. However, the Giants countered by scoring two touchdowns in the second quarter. Nick Folk missed a 57-yard field goal just before halftime.In the third quarter, both teams scored touchdowns and Nick Folk missed a 42-yard field goal which would have put the Cowboys in front. In the fourth quarter the Giants scored two touchdowns, one being a 79-yard punt return by Domenik Hixon. Dallas would score a late touchdown from Romo to Miles Austin but the game ended with the Giants sweeping the series.\n\nAnswer: 1 Democrat says in a statement that she \"received information from an individual concerning the nomination.\" Feinstein isn't saying who that person is or describing the information in any way. She says the person \"strongly requested confidentiality, declined to come forward or press the matter further, and I have honored that decision.\" Feinstein-the top-ranking Democrat on the Senate Judiciary Committee, which just finished confirmation hearings for Kavanagh-says she has \"referred the matter to federal investigative authorities.\" Another committee Democrat, Illinois Sen. Dick Durbin, says the matter has been referred to the FBI. Fox News refers to the whole thing as \"cryptic.\" Confirmed details are scarce, but the Intercept says the information Feinstein is referring to came to her via a letter that was first sent to US Rep. Anna Eshoo from someone affiliated with Stanford University. (On Wednesday night, Sen. Cory Booker released confidential documents relating to Kavanaugh.)   Given the following review: Really liked the case at first but in a few days after normal use, developed a lot of scratches on the back since it is hard clear plastic. I Sent it back and ordered the Tottallee Clear case which I like a LOT better and it was less $ predict the associated rating from the following choices (1 being lowest and 5 being highest) -1 -2 -3 -4 -5 answer: 3\n\nFigure 33: Zero-shot example of amazon_us_reviews__wireless_v1_00 task from text scoring category of PromptSource benchmark.\n\nRate the product by the number of stars based on the review below: (1 being the lowest and 5 the highest) === Smaller than expected. Model in picture is misleading, much smaller than expected Product category: shoes answer: 3\n\nFigure 34: Zero-shot example of amazon_reviews_multi__en task from text scoring category of PromptSource benchmark.\n\nClassify this request into one of the following intents: alarm query, alarm remove, alarm set, audio volume down, audio volume mute, audio volume other, audio volume up, calendar query, calendar remove, calendar set, cooking query, cooking recipe, date time convert, date time query, email add contact, email query, email query contact, email send email, general affirm, general command stop, general confirm, general don't care, general explain, general greet, general joke, general negate, general praise, general quirky, general repeat, internet of things cleaning, internet of things coffee, internet of things hue light change, internet of things hue light dim, internet of things hue light off, internet of things hue light on, internet of things hue light up, internet of things wemo off, internet of things wemo on, lists create or add, lists query, lists remove, music dislikeness, music likeness, music query, music settings, news query, play audiobook, play game, play music, play podcasts, play radio, question answer currency, question answer definition, question answer factoid, question answer maths, question answer stock, recommendation events, recommendation locations, recommendation movies, social post, social query, takeaway order, takeaway query, transport query, transport taxi, transport ticket, transport traffic or weather query. play i walk the line by johnny cash Output: play music   circuitid , circuitref , name , location , country , lat , lng , alt , url | races : raceid , year , round , circuitid , name , date , time , url | drivers : driverid , driverref , number , code , forename , surname , dob , nationality , url | status : statusid , status | seasons : year , url | constructors : constructorid , constructorref , name , nationality , url | constructorstandings : constructorstandingsid , raceid , constructorid , points , position , positiontext , wins | results : resultid , raceid , driverid , constructorid , number , grid , position , positiontext , positionorder , points , laps , time , milliseconds , fastestlap , rank , fastestlaptime , fastestlapspeed , statusid | driverstandings : driverstandingsid , raceid , driverid , points , position , positiontext , wins | constructorresults : constructorresultsid , raceid , constructorid , points , status | qualifying : qualifyid , raceid , driverid , constructorid , number , position , q1 , q2 , q3 | pitstops : raceid , driverid , stop , lap , time , duration , milliseconds | laptimes : raceid , driverid , lap , position , time , milliseconds question sequence: How many races has there been? SQL query: select count ( * ) from races    SQL: SELECT LName FROM STUDENT WHERE Sex = \"F\" ORDER BY Age DESC query in natural language: Find the last name of female (sex is F) students in the descending order of age.\n\nFigure 41: Zero-shot example of sql2text task from formal-language-to-text category of UnifiedSKG benchmark. Figure 42: Zero-shot example of logic2text task from formal-language-to-text category of Unified-SKG benchmark.  Select the sentence without English grammar error from the given pair. sentence 1: Kimberley has really ever found those actresses to compete. sentence 2: Kimberley has not ever found those actresses to compete. A: sentence 2 Figure 45: Zero-shot example of blimp__sentential_negation_npi_licensor_present task from linguistic probing category of CrossFit benchmark.\n\nWhy do some pimples hurt and swell more than others? Answer: Scalp pimples are the best: you're out and about with your day and feel an itch on your head and go to scratch it only to apply too much pressure to the overzealous zit and it bursts in an orgiastic explosion of bliss. Why do some pimples hurt and swell more than others? Answer: Scalp pimples are the best: you're out and about with your day and feel an itch on your head and go to scratch it only to apply too much pressure to the overzealous zit and it bursts in an orgiastic explosion of bliss. In this sentence, replace verbs with their synonyms. renewed efforts to fight drugs and crime will be costly Answer: renewed efforts to fight_back drugs and crime will be costly How would Shakespeare write the following sentence?\n\nTrue , \" she said , \"a fine little one.\" \"No , \" I said , \"a huge wit.\" \"Right , \" she said , \"a hugely awful one.\" \"No , \" I said , \"he has a good wit.\" \"Exactly , \" she said , \"it's good and mild ; it doesn't hurt anyone.\" \"No , \" I said , \"Benedick is wise.\" \"He is certainly , \" she said , She cried a lot at that , and said she didn't care .\n\nAnswer: True , \" said she , \"a fine little one.\" \"No , \" said I , \"a great wit.\" \"Right , \" says she , \"a great gross one.\" \"Nay , \" said I , \"a good wit.\" \"Just , \" said she , \"it hurts nobody.\" \"Nay , \" said I , \"the gentleman is wise.\" \"Certain , \" said she , \"a wise gentleman.\" \"Nay , \" said I , \"he hath the tongues.\" \"That I believe , \" said she , \"for he swore a thing to me on Monday night , which he forswore on Tuesday morning ; there's a double tongue , there's two tongues.\" Thus did she an hour together transshape thy particular virtues . what is the cost of a flight from boston to san francisco Answer: answer(_fare(_flight(intersection(_from_2(city_name(boston)),_to_2(city_name(san_francisco)))))) Figure 54: Zero-shot example of unimer task from semantic parsing category of ExMix benchmark.\n\nConstruct a semantic representation of the following sentence.\n\nVictoria awarded a cake to Olivia .\n\nAnswer: award . agent ( x _ 1 , Victoria ) AND award . theme ( x _ 1 , x _ 3 ) AND award . recipient ( x _ 1 , Olivia ) AND cake ( x _ 3 ) Afghan leader tells U.S. Congressman of peace plan . Sayed Salahuddin KABUL 1996-08-27 Afghan government military chief Ahmad Shah Masood briefed visiting U.S. Congressman Dana Rohrabacher on Tuesday on a peace plan for his wartorn country . A spokesman for Masood said he had told the California Republican at a meeting in northern Kabul that President Burhanuddin Rabbani 's government favoured talks with all Afghan factions to set up an interim government . The factions should agree to appoint a transitional leader , draft a new constitution , collect heavy weapons , create a national army and hold free elections in which the transitional leader would be barred from standing , he added . Rohrabacher flew into Bagram military airbase north of Kabul in a Red Cross plane on Tuesday after meeting northern opposition militia leader General Abdul Rashid Dostum . Masood 's spokesman Amrollah ( one name ) said Rohrabacher had recently visited Italy , Saudi Arabia and Pakistan as part of a mission to promote peace in [START_ENT] Afghanistan [END_ENT] . \" We are certainly serious more than before to find a solution to the Afghan problem and support every U.N. plan , \" Amrollah quoted Rohrabacher as saying . However , a spokesman for Prime Minister Gulbuddin Hekmatyar , a long-time rival of Masood , expressed concern at signs of renewed U.S. interest in Afghanistan . \" America wants to block the establishment of a strong Islamic government in Afghanistan and the U.S. intends to neutralise the Afghan peace process initiated by the Afghans themselves , \" said the spokesman , Hamid Ibrahimi . \" A great game has been started in Afghanistan as America feels that Tehran and Moscow have got stronger in the Afghan picture -something Washington wants to change , \" he said . Rohrabacher was expected to visit neutral faction leaders in the eastern city of Jalalabad and meet leaders of the rebel Islamic Taleban militia in the southern city of Kandahar . Afghan guerrilla factions have been locked in a bloody power struggle since the fall of the communist government in April 1992 . Hekmatyar , once Rabbani 's main rival , made a peace pact with him and rejoined the government as prime minister in June .\n\n\nAnswer: Afghanistan\n\nFigure 57: Zero-shot example of aidayago2 task from knowledge-intensive language tasks category of ExMix benchmark.\n\nWhat Lakers legend visited LeBron James less than 24 hours before the free agent became a Laker? A: Magic Johnson Is the following comment toxic? \":Thanks for the comment and bringing this to my attention. This is my first time I have voted using that format. I didn't notice the comments at top and bottom instead, I went directly to my comment. If you look look at the change I made, I added two words (my original vote with a strikethrough) that I had mistakenly deleted, instead of using the common method of altering one's vote by doing a strikethrough. So, I re-added the deleted words of my previous vote with a strikethrough.\n\nOn reflection, I think there are meaningful exceptions to rules. The change does not alter my comment or revised \"\"vote.\"\" It preserves information about the change in status of my vote. In fact, it clarifies the vote change. So, in this case, as the intention of the request of do not edit is to keep a clear record of the vote, I think my very minor edit actually serves that. So, I'll leave it as is. Thanks, \" answer: No Figure 60: Zero-shot example of wikipedia_toxicity_subtypes task from toxic language detection category of ExMix benchmark.\n\nIs the following comment toxic? \":Thanks for the comment and bringing this to my attention. This is my first time I have voted using that format. I didn't notice the comments at top and bottom instead, I went directly to my comment. If you look look at the change I made, I added two words (my original vote with a strikethrough) that I had mistakenly deleted, instead of using the common method of altering one's vote by doing a strikethrough. So, I re-added the deleted words of my previous vote with a strikethrough.\n\nOn reflection, I think there are meaningful exceptions to rules. The change does not alter my comment or revised \"\"vote.\"\" It preserves information about the change in status of my vote. In fact, it clarifies the vote change. So, in this case, as the intention of the request of do not edit is to keep a clear record of the vote, I think my very minor edit actually serves that. So, I'll leave it as is. Thanks, summarize: Editor's note: Ruben Navarrette is a nationally syndicated columnist and a member of the editorial board of the San Diego Union-Tribune. Read his column here. Ruben Navarrette asks whether Obama's cool, calm demeanor will be a plus in negotiating with world leaders. SAN DIEGO, California (CNN) -Make no mistake, Barack Obama is one cool customer. Now, after the last debate, it seems all but certain that the Iceman cometh to the White House. Radio talk show hosts and rank-and-file Republicans spent the last few weeks pleading with John McCain to take the gloves off and take the fight to Obama. How's that working out, folks? In this week's match-up, Obama snatched the gloves out of McCain's hands and slapped him silly with them. I suppose the hope was that Obama would get rattled and make a mistake. But Obama doesn't get rattled or make many mistakes. I still have no idea what type of president Obama would make. But he's an extraordinary politician. In fact, he may even be better than Bill Clinton who, while he had the IQ and EQ, also had the burden of a legendary red-hot temper. Obviously, it takes a lot to get under Obama's skin. McCain sure tried. Maybe this is the guy we want negotiating with world leaders. Maybe after eight years of George W. Bush stubbornness, on the heels of eight years of Clinton emotiveness, we need to send out for ice. In a CNN/Opinion Research Corp. poll, 58 percent of those who watched the debate said Obama did the better job and 31 percent said that about McCain. That makes three skins for Obama. In earlier polls, 54 percent of those who watched thought Obama won the second debate, and 51 percent thought he won the first one. This week, McCain turned in his best performance of the debates, and the first 30 minutes -with McCain bringing up Obama's problematic encounter with the now famous Joe the Plumber; and the quip about how he isn't Bush and how Obama should have run four years ago -were near flawless for the Republican. iReport.com: Are you Joe the Plumber? Get out your plunger and share your thoughts. McCain put Obama on the defensive, and it showed. If McCain had been that aggressive throughout the first two debates -firm but not necessary unlikable -we might be looking at a different race right now. But, over the next hour, Obama regained his stride and eventually dominated the exchange. And, in the end, with his sarcastic crack about school vouchers -\"Because there's not enough vouchers; therefore, we shouldn't do it, even though it's working. I got it.\" -McCain was profoundly unlikable. So said the polls. In the CNN/Opinion Research Corp. poll, 70 percent of debate watchers found Obama more likable. Only 22 percent said that about McCain. McCain's supporters wanted him to bring up some of the allegedly shady characters from Obama's circle of acquaintances that give some Americans pause and lead them to question the Democrat's values. There are good reasons to have that conversation, and bad ones. A friend and fellow journalist told me Obama's Chicago posse was important because it formed \"the political womb where the fetal Obama grew into a politician. [...continued] But, unless the political wind changes -and quickly -a promotion to the other end of Pennsylvania Avenue doesn't appear to be in the cards. The opinions expressed in this commentary are solely those of Ruben Navarrette. answer: Ruben Navarrette: McCain tried to get under Obama's skin with attacks. Obama remained cool and wound up ending strongly in the debate, he says. Navarrette says Ayers attacks backfired even though issue is legitimate. America may need the cool-headedness of Barack Obama, he says. Figure 62: Zero-shot example of cnn_dailymail_v002 task from summarization category of T5 benchmark. We remove some text from the input and replace it with '[...continued]' for presentation purpose.\n\nAnswer the following question by reasoning step by step.\n\n\" Ai n't No Sunshine \" is a song by Bill Withers from his 1971 album Just As I Am , produced by Booker T. Jones . The record featured musicians Donald \" Duck \" Dunn on bass guitar , Al Jackson , Jr. on drums and Stephen Stills on guitar . String arrangements were done by Booker T. Jones , and recorded in Memphis by engineer Terry Manning . The song is in the key of A minor . who did ain't no sunshine when she's gone Output: \" Ai n't No Sunshine \" is a song by Bill Withers from his 1971 album Just As I Am , produced by Booker T. Jones . The answer is Bill Withers Figure 63: Zero-shot example of qed task from Reasoning benchmark. Answer the following question by reasoning step by step.\n\nHow do most people feel about a person they love? popularity, know all, own house, care about, flu Output: we care about people we love. The answer is care about Figure 64: Zero-shot example of cose task from Reasoning benchmark.\n\n\nIn this task, you need to identify the sentiment of the given sentence as one of 'positive' or 'negative. Input: with pale blue berries. in these peaceful shades-rotated the steering wheel. What is the effect?OPTIONS: -The car halted.-The car turned. Answer: The car turned.Based on the following sentence, what is the effect?The driver rotated the steering wheel.effect: OPTIONS: -The car halted.-The car turned. Answer:The car turned.FLANCause-Effect Cluster (Fully Held-out)Instruction: You should complete the given text with another \u2026 Input: The physician misdiagnosed the patient, so Output: the surgery had to be is outside the radiative zone? (A) diffusion zone (B) peripheral zone (C) activation zone (D) convection zone. convection zone. What is the energy called that is stored in matter? (A) potential (B) mechanical (C) possible (\n\nFigure 1\n1Figure 1: We fine-tune OPT on a large collection of 1500+ NLP tasks divided into task categories (left hand side) to create OPT-IML. Each category contains multiple related tasks, as well as multiple prompts for the same task (e.g. IMDB), aggregated from multiple benchmarks. We evaluate OPT-IML on a set of evaluation categories (right hand-side) which can be disjoint, partially overlap or fully-overlap with the categories used for tuning (e.g. Sentiment Analysis fully overlaps and QA partially overlaps), corresponding to evaluating model generalization to tasks from fully held-out categories, to tasks from categories seen during training, and to instances from tasks seen during training. We release this evaluation framework as OPT-IML Bench.\n\nFigure 2 :\n2Effect of scaling the number of training tasks on each generalization level for OPT-IML 30B under both 0-shot and 5-shot settings, aggregated by task category.\n\nFigure 3 :\n3Effect of scaling the number of training categories on each generalization level for OPT-IML 30B under both 0-shot and 5-shot settings.\n\n\nexperimented with the setup where a constant number of k demonstration examples are added to each training example. The models are evaluated with the same number of k demonstration examples during inference. Chung et al. (2022b) used a mixture of data with and without exemplars. However, the proportion of each type of data used and how many exemplars were included are not clear. We attempt to train models that are better in-context few-shot learners, and also robust to the number of demonstration examples used during inference time. 8 We experiment with a simple way of creating training examples that include varying numbers of demonstration examples. For each example e, we sample k from a distribution D with cap 9 K, and randomly select k other examples E d = {e 1 , . . . , e k }, e i = e, from the train set, if k > 0. We add E d as the demonstration examples in e's prompt, where the examples are separated by a special token [SEP]. For benchmarks with tasklevel instructions such as Super-NaturalInstructions, we place the demonstration examples before e and after the instruction field; for benchmarks with instance-level instructions such as FLAN and PromptSource, we place the demonstration examples before e.\n\n\n5% of the examples are zero-shot examples; and when a = 2, 67.1% of the examples are zero-shot examples. We set K = 5 and use three consecutive newline tokens as [SEP] following Min et al. (2021).\n\nFigure 7 :Figure 8 :\n78Zero-shot example of task745_ai2_arithmetic_questions_arithmetic task from question answering category of Super-NaturalInstructions benchmark.In this task, you are given two facts, and a multiple-choice question. Based on the given facts, answer the question with index of the correct option (e.g, \"A\").Input: Fact1: plasma is formed by electrons separating from atoms in stars, Fact2: Gas Ionization Losing electrons ionizes the atoms in a gas., Question: You cannot have matter in a plasma state without what? (A) energy (B) Energy. (C) voltage (D) ionization (E) nutrients (F) light (G) heat energy (H) kinetic energy Answer: D Zero-shot example of task1297_qasc_question_answering task from question answering category of Super-NaturalInstructions benchmark.\n\nFigure 10 :Figure 11 :\n1011Zero-shot example of task370_synthetic_remove_divisible_by_3 task from program execution category of Super-NaturalInstructions benchmark. Task: You are given a concept, and a list of answers. You should generate a question about the concept that leads to the given answer(s). Input: concept: John Steinbeck answers: ['The Grapes of Wrath'] answer: what book did john steinbeck wrote about the people in the dust bowl? Zero-shot example of task1602_webquestion_question_genreation task from question generation category of Super-NaturalInstructions benchmark.\n\nFigure 17 :\n17Zero-shot example of cosmos_qa task from question answering category of FLAN benchmark. Article: On April 4, 2008, Beyonc\u00e9 married Jay Z. She publicly revealed their marriage in a video montage at the listening party for her third studio album, I Am... Sasha Fierce, in Manhattan's Sony Club on October 22, 2008. I Am... Sasha Fierce was released on November 18, 2008 in the United States. The album formally introduces Beyonc\u00e9's alter ego Sasha Fierce, conceived during the making of her 2003 single \"Crazy in Love\", selling 482,000 copies in its first week, debuting atop the Billboard 200, and giving Beyonc\u00e9 her third consecutive number-one album in the US.\n\n2008 Figure 18 :\n200818of the Internet age according to the Toronto Star. The video has won several awards, including Best Video at the 2009 MTV Europe Music Awards, the 2009 Scottish MOBO Awards, and the 2009 BET Awards. At the 2009 MTV Video Music Awards, the video was nominated for nine awards, ultimately winning three including Video of the Year. Its failure to win the Best Female Video category, which went to American country pop singer Taylor Swift's \"You Belong with Me\", led to Kanye West interrupting the ceremony and Beyonc\u00e9 improvising a re-presentation of Swift's award during her own acceptance speech. In March 2009, Beyonc\u00e9 embarked on the I Am... World Tour, her second headlining worldwide concert tour, consisting of 108 shows, grossing $119.5 million.Now answer this question: When did Beyonc\u00e9 get married? Answer: April 4, Zero-shot example of squad_v1 task from question answering category of FLAN benchmark.\n\nFigure 20 :\n20Zero-shot example of cnn_dailymail task from summarization category of FLAN benchmark.\n\nFigure 21 :Figure 22 :Figure 23 :Figure 24 :\n21222324Zero-shot example of glue_mrpc task from paraphrasing category of FLAN benchmark. Do these mean the same? Since 2008 , Hoyer toured Canada on several occasions , including twice with Michael Rault , once with Sean Nicholas Savage and once with The Joe . Since 2008 , Hoyer has been touring Canada several times , once with Sean Nicholas Savage , once with Michael Rault , and twice with The Joe . Zero-shot example of paws_wiki task from paraphrasing category of FLAN benchmark.How might one describe the sentiment of this review? Ran over a piece of metal, I had 2 discount tires from a few months and it happened to be one from them that got the metal stuck in, went down and under warranty only paid $16. Can'Zero-shot example of yelp_polarity_reviews task from sentiment analysis category of FLAN benchmark.it's going to be a long day How would the sentiment of this tweet be described? Zero-shot example of sentiment140 task from sentiment analysis category of FLAN benchmark.\n\nFigure 27 :\n27Zero-shot example of subjqa__electronics task from question answering category of PromptSource benchmark.\n\nFigure 29 :\n29Zero-shot example of multi_news task from summarization category of PromptSource benchmark. We remove some text from the input and replace it with '[...continued]' for presentation purpose.Write a scientific title for the following abstract: OBJECTIVES Sepsis is characterised by the frequent presence of organ failure and marked immunologic alterations. We studied the association between the extent of organ failure and the transcriptomic response of septic patients. METHODS Gene expression profiles in the blood of 74 surgical patients with sepsis were compared with those of 30 surgical patients with no sepsis. Differentially expressed genes were assessed for their correlation with the sequential organ failure (SOFA) score. RESULTS The expression levels of a group of genes participating in the cell cycle (HIST1H1C, CKS2, CCNA2, CDK1, CCNB2, CIT, CCNB1, AURKA, RAD51), neutrophil protease activity (ELANE, ADORA3, MPO, MMP8, CTSG), IL-1R and IL-18R response correlated directly with SOFA and mortality. Genes involved in T cell (LCK, CD3G, CD3D, ZAP70, ICOS, CD3E, CD28, IL2RB, CD8B, CD8A, CD40LG, IL23A, CCL5, SH2D1A, ITK, CD247, TBX21, GATA3, CCR7, LEF1, STAT4) and NK cell immunity (CD244, KLRK1, KLRD1) were inversely associated with SOFA and mortality. CONCLUSIONS The extent of organ failure in sepsis correlates directly with the existence of imbalanced innate and adaptive responses at the transcriptomic level. Quantification of the expression levels of the genes identified here could contribute to the simultaneous assessment of disease severity and immunological alterations in sepsis. Title: Transcriptomic correlates of organ failure extent in sepsis.\n\nFigure 32 :\n32Zero-shot example of scan__template_jump_around_right task from semantic parsing category of PromptSource benchmark.\n\nFigure 35 :\n35Zero-shot example of nlu_evaluation_data task from intent classification category of PromptSource benchmark.\n\nFigure 38 :\n38Zero-shot example of cosql task from semantic parsing category of UnifiedSKG benchmark.\n\nFigure 39 :\n39Zero-shot example of sqa task from question answering category of UnifiedSKG benchmark.\n\nFigure 40 :\n40Zero-shot example of wikisql task from question answering category of UnifiedSKG benchmark.\n\nFigure 43 : 2 Figure 44 :\n43244Zero-shot example of totto task from data to text category of UnifiedSKG benchmark.Select the sentence without English grammar error from the given pair. sentence 1: The took lights weren't orange. sentence 2: The taken lights weren't orange. A: sentence Zero-shot example of blimp__irregular_past_participle_adjectives task from linguistic probing category of CrossFit benchmark.\n\nFigure 46 :\n46Zero-shot example of eli5__eli5 task from question answering category of CrossFit benchmark.\n\nFigure 47 :Figure 48 :Figure 49 :Figure 50 :Figure 51 :\n4748495051Zero-shot example of eli5__eli5 task from question answering category of CrossFit benchmark.Predict the content that fills in the[MASK]  placeholder to complete each fact statement: Tiv people is located in[MASK]  . Output: Nigeria Zero-shot example of lama__trex task from information extraction category of CrossFit benchmark.Predict the content that fills in the [MASK] placeholder to complete each fact statement: Something you might do while helping someone is[MASK]. output: dying Zero-shot example of lama__conceptnet task from information extraction category of CrossFit benchmark.Generate a normalized form of the entity denoted in the given passage with [START_ENT] and [END_ENT]. Passage: ATHLETICS -SALAH HISSOU BREAKS 10,000 METRES WORLD RECORD . [START_ENT] BRUSSELS [END_ENT] 1996-08-23 Morocco 's Salah Hissou broke the men 's 10,000 metres world record on Friday when he clocked 26 minutes 38.08 seconds at the Brussels grand prix on Friday . The previous mark of 26:43.53 was set by Ethiopia 's Haile Gebreselassie in the Dutch town of Hengelo in June last year . Output: Brussels Zero-shot example of kilt_ay2 task from entity linking category of CrossFit benchmark. Generate a normalized form of the entity denoted in the given passage with [START_ENT] and [END_ENT]. Passage: ATHLETICS -SALAH HISSOU BREAKS 10,000 METRES WORLD RECORD . [START_ENT] BRUSSELS [END_ENT] 1996-08-23 Morocco 's Salah Hissou broke the men 's 10,000 metres world record on Friday when he clocked 26 minutes 38.08 seconds at the Brussels grand prix on Friday . The previous mark of 26:43.53 was set by Ethiopia 's Haile Gebreselassie in the Dutch town of Hengelo in June last year .Output: Brussels Zero-shot example of kilt_ay2 task from entity linking category of CrossFit benchmark.\n\nFigure 52 :\n52Zero-shot example of styleptb task from style transfer category of ExMix benchmark.\n\nFigure 53 :\n53Zero-shot example of shakespearizingmodernenglish task from style transfer category of ExMix benchmark.Transform the following sentence into funql.\n\nFigure 55 :Figure 56 :\n5556Zero-shot example of cogs task from semantic parsing category of ExMix benchmark.Which Wikipedia entity does the marked entity correspond to?Gmina Jaraczewo is a rural gmina ( administrative district ) in Jarocin County , Greater Poland Voivodeship , in west-central Poland . Its seat is the village of Jaraczewo , which lies approximately west of Jarocin and south-east of the regional capital Pozna\u0144 . The gmina covers an area of , and as of 2006 its total population is 8,281 . Gmina Jaraczewo contains the villages and settlements of Bielejewo , Brzost\u00f3w , [START_ENT] Cerekwica [END_ENT] , Gola , G\u00f3ra , Jaraczewo , \u0141obez , \u0141obzowiec , \u0141ow\u0119cice , \u0141ukaszewo , Nied\u017awiady , Nosk\u00f3w , Nowa Cerekwica , Panienka , Parz\u0119czew , Por\u0119ba , Rusko , Strzy\u017cewko , Suchorzewko , Wojciechowo and Zalesie . Gmina Jaraczewo is bordered by the gminas of Borek Wielkopolski , Dolsk , Jarocin , Ko\u017amin Wielkopolski , Ksi\u0105\u017c Wielkopolski and Nowe Miasto nad Wart\u0105 Answer: Cerekwica, Jarocin County Zero-shot example of wned task from knowledge-intensive language tasks category of ExMix benchmark.Which Wikipedia entity does the marked entity correspond to?\n\nFigure 58 :Figure 59 :\n5859Zero-shot example of newsquizqa task from question answering category of ExMix benchmark.What Lakers legend visited LeBron James less than 24 hours before the free agent became a Laker? A: Magic Johnson Zero-shot example of newsquizqa task from question answering category of ExMix benchmark.\n\nFigure 61 :\n61Zero-shot example of wikipedia_toxicity_subtypes task from toxic language detection category of ExMix benchmark.\n\n\n).Benchmark \nInstruct. \ntype \n\n# \nclusters \n\n# \ntasks \n\n# total \nexamples \n\nAvg. # \nprompts / task \n\nprompt length \nmean \nstd \n\nSuper-NaturalInstructions \ntask inst. \n76 \n1613 \n12.4M \n1.0 \n287 \n882 \nPromptSource \ninstance inst. \n51 \n280 \n12.8M \n5.7 \n179 \n222 \nCrossFit \nkeywords \n32 \n159 \n7.1M \n1.0 \n117 \n258 \nFLAN \ninstance inst. \n12 \n70 \n4.4M \n8.5 \n193 \n375 \nExMix  \u2021 \nkeywords \n10 \n14 \n0.5M \n1.0 \n132 \n191 \nT5 \nkeywords \n9 \n36 \n1.9M \n1.0 \n111 \n167 \nUnifiedSKG \nkeywords \n7 \n21 \n0.8M \n1.0 \n444 \n297 \nReasoning \ntask inst. \n1 \n14 \n0.4M \n1.0 \n146 \n122 \nOPT-IML Bench (train) \nmixed \n93  \u2020 \n1,545 \n17.9M \n1.7 \n261 \n631 \nOPT-IML Bench (dev) \nmixed \n7 \n35 \n145K \n2.9 \n-\n-\nOPT-IML Bench (test) \nmixed \n10 \n87 \n321K \n4.6 \n-\n-\n\n\n\nTable 1 :\n1Details of OPT-IML Bench. The statistics of each existing benchmark is calculated using the original data we downloaded. The statistics of OPT-IML Bench is calculated using the data after we performed task filtering and taking a maximum of M examples per tasks. For all benchmarks except FLAN, we set M = 100k; for FLAN, we set M = 30k following\n\nTable 2 :\n2Different prompt formulations of the COPA task\n\n\nModel # Gpus Batch Size Learning Rate Steps Warm-up Steps FT Time (h) # TokensOPT-IML 30B \n64 \n256 \n5e-05 \n4000 \n60 \n19 \n2B \nOPT-IML 175B \n128 \n128 \n5e-05 \n8000 \n60 \n72 \n2B \n\n\n\nTable 3 :\n3Fine-tuning parameters for all OPT-IML models, including the fine-tuning times and the number of fine-tuning tokens.\n\n\nIdentify the sentiment: Great!\u2026 A: PositiveIdentify the sentiment: Meh\u2026 A: Negative Identify the sentiment: OMG\u2026 A: PositiveIdentify the sentiment: Great!\u2026 A: Positive \nIdentify the sentiment: Meh\u2026 A: Negative \nIdentify the sentiment: OMG\u2026 A: Positive \n\nStandard Loss \n\nSuffix Loss \n\nLM Loss \n\nLM Loss \n\n[SEP] \n[SEP] \n\n[SEP] \n[SEP] \n\n\n\n\nZipf a=4 sf. 59.8/62.0 85.1/87.2 52.9/67.6 12.2/42.9 2.7/20.7 41.0/38.7 64.3/61.6 18.4 66.3/66.2 15.9/16.2 85.9/85.2 29.5 44.5/54.8 Zipf a=2 sf. 56.1/64.3 87.6/88.1 60.8/65.9 14.5/35.9 2.6/16.9 39.7/38.0 63.4/62.1 19.1 65.2/75.3 16.2/16.9 85.4/86.2 31.5 45.2/55.0Fully Held Out \nPartially Supervised \nFully Supervised \nAverage \nEPS \nCause \nEffect \n\nGram. \nCorr. \n\nStereo. \nDet. \n\nWord \nAna. \nReas. \nMMLU \nQA \nSumm. \nToxic \nDet. \n\nDial \nogue. \nQA \nSumm. \n\nBaseline \n62.1/59.6 85.4/87.4 56.8/79.9 13.5/55.9 2.6/18.3 39.3/36.0 65.1/58.0 17.8 61.6/66.9 16.4/16.2 86.4/81.5 29.7 44.7/56.0 \nZipf a=4 \n60.5/61.4 84.7/87.5 53.0/67.6 13.8/36.5 2.9/3.3 37.9/35.9 63.6/59.7 18.8 59.5/62.2 15.5/15.3 86.1/86.3 30.2 43.9/51.6 \nZipf a=2 \n61.6/62.0 84.2/87.0 48.0/69.1 11.0/41.2 2.6/5.2 37.9/36.4 63.7/64.9 20.2 65.1/72.8 16.1/14.5 85.6/84.8 29.8 43.8/53.8 \n\n\nTable 8 :\n8Effects of MetaICL fine-tuning on each generalization level for OPT-IML 30B after 2000 steps, aggregated by task category. Results are presented as 0-shot/5-shot. We underline categories where the MetaICL model outputs demonstrate severe degeneration compared to the baseline model.\n\nTable 11 :\n11Comparing the performances of OPT-IML and FLAN models(Wei et al., 2022a) on \n\n\nTable 12 :\n12Comparing OPT-IML with baseline OPT and Tk-Instruct 11b on three fully held-out task categories from\n\nTable 13 :\n13Comparing the performance of baseline OPT with OPT-IML models on the test sets of three datasets from the UnifiedSKG benchmark, evaluating Database to Text Generation (DART)\n\nTable 14 :\n14Test-set performance of OPT-IML-Max, trained on all tasks in our benchmark, on Big-Bench Hard, MMLU, and RAFT.\n\n\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Ordonez, and Kai-Wei Chang. Gender bias in coreference resolution: Evaluation and debiasing methods. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 15-20, 2018.Ruiqi Zhong, Kristy Lee, Zheng Zhang, and Dan Klein. Adapting Language Models for Zero-shot \nLearning by Meta-tuning on Dataset and Prompt Collections. In Conference on Empirical Methods \nin Natural Language Processing (EMNLP) -Findings, 2021. \n\n\n\nTable 15presents all of these tasks, along with their category, benchmark, generalizaton level, and evaluation metric information.Task \nCategory \nBenchmark \nGeneralization Level Metric \n\nWinobias \nStereotype Detection \nPromptSource Fully Held-Out \nRouge-L F1 \nWinobias \nStereotype Detection \nSuperNatInst \nFully Held-Out \nRouge-L F1 \nBard Analogical Reasoning Word Analogy \nSuperNatInst \nFully Held-Out \nRouge-L F1 \nCause-Effect \nCause Effect Classification SuperNatInst \nFully Held-Out \nRouge-L F1 \nCOPA \nCause Effect Classification PromptSource Fully Held-Out \nAccuracy \nCOPA \nCause Effect Classification SuperNatInst \nFully Held-Out \nRouge-L F1 \nCOPA \nCause Effect Classification FLAN \nFully Held-Out \nAccuracy \nCOPA CommonSense \nCause Effect Classification SuperNatInst \nFully Held-Out \nRouge-L F1 \nGlucose \nCause Effect Classification SuperNatInst \nFully Held-Out \nRouge-L F1 \nJfleg \nGrammar Error Correction SuperNatInst \nFully Held-Out \nRouge-L F1 \nMMLU \nMMLU \nMMLU \nPartially Supervised Accuracy \nCivil Comments \nToxic Language Detection \nSuperNatInst \nPartially Supervised Rouge-L F1 \nJigsaw \nToxic Language Detection \nPromptSource Partially Supervised Rouge-L F1 \nNewsroom \nSummarization \nFLAN \nPartially Supervised Rouge-L F1 \nRace \nQuestion Answering \nPromptSource Partially Supervised Accuracy \nRace \nQuestion Answering \nSuperNatInst \nPartially Supervised Rouge-L F1 \nStrategyQA \nReasoning \nReasoning \nPartially Supervised Rouge-L F1 \nGSM8K \nReasoning \nReasoning \nPartially Supervised Rouge-L F1 \nSQuAD v1 \nQuestion Answering \nFLAN \nFully Supervised \nRouge-L F1 \nSQuAD v1 \nQuestion Answering \nPromptSource Fully Supervised \nRouge-L F1 \nBlended Skill Talk \nDialogue Generation \nPromptSource Fully Supervised \nRouge-L F1 \nCNNDM \nSummarization \nPromptSource Fully Supervised \nRouge-L F1 \n\n\n\nTable 15 :\n15Full details of validation tasks used in our experimental studies presented in Section 4. Some of these tasks contain sub-tasks (e.g., MMLU) which we did not list in this table.\n\nTable 16 :\n16Accuracies of OPT-IML compared with OPT on the 14 standard NLP tasks from Zhang \net al. (2022) in the format of 0-shot/32-shot. For ARC, (e) denotes (Easy) and (c) denotes (Chal-\nlenge). \n\n\n\nTable 17 :\n17Effect of scaling the number of training tasks on each generalization level for OPT-IML 30B after 2000 steps of training, aggregated by task category. Results are in the format of 0-shot/5-shot.Fully Held Out \n\nPartially Supervised \nFully Supervised \nAverage \n# Tasks \nCause \nEffect \n\nGram. \nCorr. \n\nStereo. \nDet. \n\nWord \nAna. \nReas. MMLU \nQA \nSumm. \nToxic \nDet. \n\nDial \nogue. \nQA \nSumm. \n\n4 \n60.3/62.6 65.6/87.5 51.1/81.5 32.5/55.8 2.4/19.0 38.4/36.8 66.1/57.9 21.3 36.4/68.4 16.2/16.8 85.6/83.1 30.8 42.2/56.9 \n16 \n61.1/61.5 83.8/87.8 47.8/82.4 11.4/55.8 2.6/20.6 38.0/35.9 64.4/55.8 20.8 53.2/68.5 16.4/16.3 86.1/83.6 29.4 42.9/56.8 \n64 \n59.6/59.9 83.2/87.8 51.9/84.0 13.6/53.2 2.6/17.7 40.3/35.1 67.0/60.4 20.0 63.2/70.2 15.5/15.6 84.9/83.6 30.0 44.3/56.7 \n93 \n62.1/59.6 85.4/87.4 56.8/79.9 13.5/55.9 2.6/18.3 39.3/36.0 65.1/58.0 17.8 61.6/66.9 16.4/16.2 86.4/81.5 29.7 44.7/56.0 \n\n\n\nTable 18 :\n18Effect of scaling the number of training clusters on each generalization level for OPT-\nIML 30B after 2000 steps of training, aggregated by task category. Results are in the format of \n0-shot/5-shot. \n\nFully Held Out \nPartially Supervised \nFully Supervised \nAverage \n% Pre-train \nCause \nEffect \n\nGram. \nCorr. \n\nStereo. \nDet. \n\nWord \nAna. \nReas. MMLU \nQA \nSumm. \nToxic \nDet. \n\nDial \nogue. \nQA \nSumm. \n\n0 (Baseline) 63.5/62.5 86.1/87.5 58.9/82.3 17.2/57.8 2.6/20.4 41.5/37.0 69.3/58.9 18.1 60.0/70.0 16.1/15.8 87.6/83.5 31.3 46.0/57.6 \n1 \n62.3/60.8 86.6/88.4 54.4/81.7 13.9/59.3 2.6/26.5 39.6/36.4 66.6/59.8 21.0 60.2/66.9 16.3/15.6 86.4/85.2 31.1 45.1/58.1 \n5 \n62.2/61.9 86.9/88.2 57.8/83.5 20.0/59.6 2.6/28.3 39.0/37.5 65.9/63.8 20.1 58.3/69.9 16.2/17.2 86.4/83.7 30.6 45.5/59.4 \n10 \n60.8/63.2 86.6/88.4 55.5/83.4 23.7/57.3 2.8/27.3 39.2/38.5 65.8/61.1 19.6 58.8/70.4 15.9/15.9 86.3/85.2 30.5 45.5/59.1 \n50 \n59.5/60.3 87.8/88.4 62.8/85.1 21.2/54.0 2.9/29.4 37.2/34.6 58.5/56.4 21.5 58.0/66.7 15.7/15.0 84.7/83.6 28.3 44.8/57.4 \n\n\n\nTable 19 :\n19Effect of % of pre-training data on each generalization level for OPT-IML 30B after 4000 steps of training, aggregated by task category. Results are in the format of 0-shot/5-shot.Fully Held Out \nPartially Supervised \nFully Supervised \nCause \nEffect \n\nGram. \nCorr. \n\nStereo. \nDet. \n\nWord \nAna. \nReas. \nMMLU \nQA \nSumm. \nToxic \nDet. \n\nDial \nogue. \nQA \nSumm. \n\nBaseline \n58.5/61.0 87.1/87.6 53.8/79.8 12.7/51.5 2.4/22.0 40.9/36.8 69.6/60.6 \n19.9 \n61.5/59.3 15.5/15.3 86.1/83.9 \n31.3 \n1% Reas. 60.8/61.8 86.8/88.0 55.9/80.9 14.9/55.2 31.3/32.0 40.6/36.4 68.5/60.4 \n18.8 \n62.3/67.7 16.1/14.6 86.9/84.3 \n31.1 \n2% Reas. 61.4/63.0 86.4/87.7 50.9/81.3 14.5/61.9 30.0/31.1 38.7/35.9 68.2/60.1 \n19.0 \n57.1/60.8 15.4/14.9 86.2/84.1 \n31.0 \n4% Reas. 59.9/63.7 86.4/87.9 51.0/82.3 14.7/54.7 30.5/30.8 40.6/33.2 68.3/62.2 \n20.8 \n59.4/57.7 14.4/14.7 85.1/83.2 \n32.0 \n\n\n\nTable 20 :\n20Effect of fine-tuning with Reasoning data on each generalization level for OPT-IML 30B after 4000 steps, aggregated by task category. Results are in the format of 0-shot/5-shot.Fully Held Out \nPartially Supervised \nFully Supervised \nAvg. \nEPS \nCause \nEffect \n\nGram. \nCorr. \n\nStereo. \nDet. \n\nWord \nAna. \nReas. \nMMLU \nQA \nSumm. \nToxic \nDet. \n\nDial \nogue. \nQA \nSumm. \n\nBaseline \n62.1/59.5 85.4/87.6 56.8/79.8 13.5/55.4 2.6/18.3 39.3/36.0 65.1/56.6 17.8/15.2 61.6/65.7 16.4/16.5 86.4/82.4 29.7/19.0 44.7/49.3  \u2020 \nZipf a=4 \n60.5/60.6 84.7/88.1 54.1/81.2 13.8/55.8 2.9/9.7 38.4/37.3 64.4/62.8 18.8/19.5 59.5/65.8 15.5/15.3 86.1/85.4 30.2/29.5 44.1/50.9 \nZipf a=4 sf. 59.8/61.5 85.1/87.4 52.9/79.4 12.2/52.8 2.7/24.6 41.0/38.7 64.3/59.6 18.4/20.3 66.3/67.3 15.9/15.9 85.9/85.0 29.5/26.9 44.5/51.6 \nZipf a=2 \n61.6/61.9 84.2/87.7 48.0/80.1 11.0/55.2 2.6/15.1 37.9/36.4 63.7/61.9 20.2/21.5 65.1/75.3 16.1/15.3 85.6/85.0 29.8/28.1 43.8/52.0 \nZipf a=2 sf. 56.1/63.5 87.6/88.2 60.8/75.0 14.5/44.7 2.6/20.3 39.7/38.0 63.4/60.5 19.1/20.7 65.2/76.0 16.2/16.2 85.4/86.3 31.5/28.8 45.2/51.5 \n\n\n\n\nPirlo and nutmegging him, or Wayne Rooney leaving a foot in to upset -not injurethe Italian, upsetting him so much he wants to get off the field. That's what England should have done. More of this: England showed Pirlo too much respect and should have closed him down, as Jack Wilshere did . Must do better: Raheem Sterling could have run at Pirlo and Wayne Rooney should have closed him down . Instead we stood off him, and a load of dreamy-eyes glossed over back home at this Azzuri legend. 'If only we had him,' the English teenagers cried.Scroll down to watch The best of Andreas Pirlo from Italy's \ntraining sessions . Back to work: Andrea Pirlo trains with the Italy squad ahead of their second \nmatch against Costa Rica . Running the show: Pirlo was at his masterful best against England as \nhe inspired Italy to victory in Manaus . The tragedy in Manaus was that the English players, and \nprobably even Roy Hodgson decided he was untouchable when actually we needed to get close to \nhim, kick his backside, nutmeg him a few times and bring him back down to earth. He'd soon have \ncome off with a strained calf or broken bootlace. We respected Pirlo far too much on Saturday, \nwhen it should have been our chance to put him in his place after that disrespectful chipped \npenalty in the shoot-out at Euro 2012. Remember Stuart Pearce's contorted face after burying \nthat spot-kick in 1996, 6 years on from the pain of missing from the spot at Italia 90? Remember \nDavid Beckham slamming in that penalty in 2002 against Argentina after the injustice of the red \ncard in St Etienne four years earlier? That was revenge, that was redemption. Raheem Sterling \nrunning at 'England \nhave never produced a player like that,' said the older generation. 'He dummied the ball! Did \nyou see that? He actually dummied the ball!' screamed a gaggle of World Cup groupies who have \nprobably never been to a game of any level in England in their lives. The English obsession with \nall things foreign takes over yet again. Oh, if only we could produce a precious Pirlo. The sad \nthing is English football did produce a Pirlo. His name is Michael Carrick and after being left out \nof Roy Hodgson's squad this summer he will end his career having played in just one game at a \nWorld Cup finals. We won it 1-0. Lone appearance: Michael Carrick's only World Cup appearance \ncame against Ecuador in 2006 . Other plans: Carrick has been holidaying this summer after he \nwas overlooked for a place at the World Cup . I don't know if Carrick would have had as good an \ninternational career as Pirlo, but I do know the job he did in a very successful Manchester United \nside under Sir Alex Ferguson for many seasons. All these England managers felt they knew better \nthan Sir Alex. They all failed. It's all about opinions, and I'd probably conclude Pirlo is a better \nplayer than Carrick. But if England had used him in their midfield instead of messing around \ntrying to use Lampard and Gerrard together, we may have been more successful. Anyone scoffing \nat this can't prove me wrong. If you love Pirlo, you'll appreciate what Carrick has done for United \nall these years. If you can't see the connection, then English football's problems run worryingly \ndeep. Mainstay: Carrick has been an almost ever-present at the heart of the Manchester United \nmidfield . \nanswer: England gave Andrea Pirlo far too much respect in Manaus . \nWayne Rooney should have closed him down and put him off his game . \nRaheem Sterling could have sold him a nutmeg . \nEngland DID produce their own Pirlo -Michael Carrick . \nThe Manchester United midfielder has played one game at a World Cup . \n\n\n\n\n\"I have received information from an individual concerning the nomination of Brett Kavanaugh to the Supreme Court,\" Feinstein said in her surprise statement. \"That individual strongly requested confidentiality, declined to come forward or press the matter further, and I have honored that decision. I have, however, referred the matter to federal investigative authorities.\"[...continued]    Figure 28: Zero-shot example of drop task from question answering category of PromptSource \nbenchmark. \nSynthesize these documents into a single one: \n\n-Democrats on the Senate Judiciary Committee have privately requested to view a Brett \nKavanaugh-related document in possession of the panel's top Democrat, Dianne Feinstein, but \nthe senior California senator has so far refused, according to multiple sources familiar with the \nsituation. The specific content of the document, which is a letter from a California constituent, is \nunclear, but Feinstein's refusal to share the letter has created tension on the committee, particu-\nlarly after Feinstein largely took a back seat to her more junior colleagues last week, as they took \nover Kavanaugh's confirmation hearings with protests around access to documents.[...continued] \n\n-Sen. Dianne Feinstein, D-Calif., on Thursday threw a cryptic curveball at Brett Kavanaugh, in-\nsinuating the Supreme Court nominee could be guilty of a crime even as Democrats on the Senate \nJudiciary Committee seek to delay his confirmation. \n\nThe vague accusation comes after the Senate Judiciary Committee already grilled Kavanaugh and \nother witnesses and prepares to vote on sending his nomination to the full Senate. The White \nHouse blasted the ambiguous charge as a last minute gambit. \n\n-Senate Judiciary Committee Chairman Chuck Grassley, R-Iowa, left, accompanied by Sen. Di-\nanne Feinstein, D-Calif., the ranking member, center, speaks with Sen. Patrick Leahy, D-Vt., \nright, during a Senate... (Associated Press) [...continued] Dick Durbin, says the matter has been \nreferred to the FBI. \n\nRepublicans... \nOutput: Sen. Dianne Feinstein says she's notified federal investigators about information she \nreceived concerning Supreme Court nominee Brett Kavanaugh, the AP reports. The California \n\n\n\nFigure 30: Zero-shot example of cord19__metadata task from summarization category of Prompt-Source benchmark. walk around left thrice and jump right thrice Given the commands above, what is the corresponding correct sequence of actions (commaseparated)? answer: turn left, walk, turn left, walk, turn left, walk, turn left, walk, turn left, walk, turn left, walk, turn left, walk, turn left, walk, turn left, walk, turn left, walk, turn left, walk, turn left, walk, turn right, jump, turn right, jump, turn right, jump Figure 31: Zero-shot example of scan__template_opposite_right task from semantic parsing category of PromptSource benchmark. Given the commands below, what is the corresponding correct sequence of actions (commaseparated)? run opposite left twice after turn left Answer: turn left, turn left, turn left, run, turn left, turn left, run\n\n\nThis search query talks about the coronavirus and was published on 2020-09-24. In what country was it issued? bupa covid test answer: United KingdomFigure 36: Zero-shot example of bing_coronavirus_query_set task from intent classification category of PromptSource benchmark. omy.galactic_super_cluster.galaxy_filament question: which galactic cluster is in the galaxy supercluster of virgo super-cluster? omy.galactic_cluster.galaxy_supercluster m.01825t)) Figure 37: Zero-shot example of grailqa task from semantic parsing category of UnifiedSKG benchmark.database: | formula_1 | circuits :knowledge \ngraph: \nvirgo \nsuper-cluster: \nm.01825t \n| \ncom-\nmon.image.appears_in_topic_gallery \nastronomy.galactic_super_cluster \nastron-\nomy.galactic_group \nastronomy.galactic_super_cluster.galaxy_clusters \nastron-\nomy.galactic_group.galaxy_cluster \nastronomy.galactic_cluster.galaxy_supercluster \nastronomy.galactic_cluster.galaxy_groups \nastronomy.galactic_filament \nastron-\nomy.galactic_cluster \nastronomy.galactic_filament.galaxy_superclusters \nastron-\nKG \nquery: \n(AND \nastronomy.galactic_cluster \n(JOIN \nastron-\n\n\ntable :\n:col : ndeg | name | position | date of birth | nationality row 1 : 1 | marcus popp | s | 23 settembre 1981 | germany row 2 : 2 | stanislav simin | c | 4 ottobre 1986 | serbia row 3 : 3 | gerald hardy-dessources | c | 9 febbraio 1983 | france row 4 : 4 | soane falafala | s | 16 aprile 1993 | france row 5 : 5 | cyril guittet | l | 13 agosto 1992 | france row 6 : 6 | david konecny | s/o | 10 ottobre 1982 | czech republic row 7 : 7 | jean-francois exiga | l | 9 marzo 1982 | france row 8 : 8 | nuno pinheiro | p | 31 dicembre 1984 | portugal row 9 : 10 | guillaume di betta | s | 8 settembre 1994 | france row 10 : 12 | maxime dillies | p | 11 aprile 1984 | france row 11 : 13 | kamil baranek | s | 2 maggio 1983 | czech republic row 12 : 14 | renaud lachaise | p | 12 maggio 1991 | francerow 13 : 15 | david smith | c | 15 maggio 1985 | united states row 14 : 16 | emmanuel ragondet | s | 6 agosto 1987 | france row 15 : 17 | victor le guennec | s | 19 giugno 1994 | france row 16 : 18 | thibault szymkowiak | c | 19 settembre 1991 | france question: what are the birth dates of soane falafala and david smith? || who are all the player that competed in the tours vb? answer: 16 aprile 1993, 15 maggio 1985\n\ntable :\n:Caption: black swan -class sloop | name, pennant, builder, laid down, launched, commissioned query: eq hop argmin all_rows ; laid down ; name ; chanticleer = true query description: the chanticleer was the first sloop to be laid down in the black swan -class sloop .\n\ntable fact :\nfactpage_title Bruce Weber (basketball) /page_title section_title Head coaching record /section_title table cell Illinois Fighting Illini (Big Ten Conference) (2003-2012) col_header Season /col_header col_header Team /col_header col_header Overall /col_header col_header Conference /col_header col_header Standing /col_header col_header Postseason /col_header /cell cell 2007-08 col_header Season /col_header /cell cell 16-19 col_header Overall /col_header /cell cell 5-13 col_header Conference /col_header /cell /table textual description: Weber finished the 2007-08 Illini season with an overall record of 16-19 and 5-13 in the Big Ten.\nAcknowledgmentsWe would like to thank Stephen Roller, Susan Zhang, and  Naman Goyal for help with fine-tuning OPT using the metaseq codebase and with our model release; Lili Yu for help with infrastructure and evaluations; Sewon Min for discussions related to meta-training for in-context learning; and Omer Levy, Timo Schick, and Scott Yih for helpful discussions related to instruction-tuning.\nExplanations for commonsenseqa: New dataset and models. Shourya Aggarwal, Divyanshu Mandowara, Vishwajeet Agrawal, Dinesh Khandelwal, Parag Singla, Dinesh Garg, ACL. Shourya Aggarwal, Divyanshu Mandowara, Vishwajeet Agrawal, Dinesh Khandelwal, Parag Singla, and Dinesh Garg. Explanations for commonsenseqa: New dataset and models. In ACL, pages 3050-3065, 2021.\n\nRaft: A real-world few-shot text classification benchmark. Neel Alex, Eli Lifland, Lewis Tunstall, Abhishek Thakur, Pegah Maham, Jess Riedel, Emmie Hine, Carolyn Ashurst, Paul Sedille, Alexis Carlier, Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track. 2021Round 2Neel Alex, Eli Lifland, Lewis Tunstall, Abhishek Thakur, Pegah Maham, C Jess Riedel, Emmie Hine, Carolyn Ashurst, Paul Sedille, Alexis Carlier, et al. Raft: A real-world few-shot text classification benchmark. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.\n\nProgressive prompts: Continual learning for language models without forgetting. Anonymous, Submitted to The Eleventh International Conference on Learning Representations. Anonymous. Progressive prompts: Continual learning for language models without forgetting. In Submitted to The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=UJTgQBc91_. under review.\n\nExT5: Towards Extreme Multi-Task Scaling for Transfer Learning. Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Q Vinh, Dara Tran, Jianmo Bahri, Ni, International Conference on Learning Representations (ICLR. 2022Vamsi Aribandi, Yi Tay, Tal Schuster, Jinfeng Rao, Huaixiu Steven Zheng, Sanket Vaibhav Mehta, Honglei Zhuang, Vinh Q Tran, Dara Bahri, Jianmo Ni, et al. ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning. In International Conference on Learning Representations (ICLR), 2022.\n\nEfficient large scale language modeling with mixtures of experts. Mikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, arXiv:2112.10684arXiv preprintMikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victo- ria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, et al. Efficient large scale language modeling with mixtures of experts. arXiv preprint arXiv:2112.10684, 2021.\n\nPromptSource: An Integrated Development Environment and Repository for Natural Language Prompts. H Stephen, Victor Bach, Zheng-Xin Sanh, Albert Yong, Colin Webson, Raffel, V Nihal, Abheesht Nayak, Taewoon Sharma, Kim, Thibault Bari, Fevry, Annual Meeting of the Association for Computational Linguistics (ACL) -System Demonstrations. Stephen H Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel, Nihal V Nayak, Ab- heesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, et al. PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts. In Annual Meeting of the Association for Computational Linguistics (ACL) -System Demonstrations, 2022.\n\nThe pushshift reddit dataset. Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, Jeremy Blackburn, Proceedings of the international AAAI conference on web and social media. the international AAAI conference on web and social media14Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn. The pushshift reddit dataset. In Proceedings of the international AAAI conference on web and social media, volume 14, pages 830-839, 2020.\n\nAlec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Advances in Neural Information Processing Systems (NeurIPS). H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. LinScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlishTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari- wal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language Models are Few-Shot Learners. In H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Sys- tems (NeurIPS), 2020a.\n\nLanguage models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 33Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020b.\n\nMultiwoz-a large-scale multi-domain wizard-of-oz dataset for taskoriented dialogue modelling. Pawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Inigo Casanueva, Stefan Ultes, Milica Osman Ramadan, Ga\u0161i\u0107, arXiv:1810.00278arXiv preprintPawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, Inigo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Ga\u0161i\u0107. Multiwoz-a large-scale multi-domain wizard-of-oz dataset for task- oriented dialogue modelling. arXiv preprint arXiv:1810.00278, 2018.\n\ne-SNLI: Natural language inference with natural language explanations. Oana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, Phil Blunsom, Advances in Neural Information Processing Systems. 31Oana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. e-SNLI: Natural language inference with natural language explanations. Advances in Neural Information Processing Systems, 31, 2018.\n\n. Rich Caruana. Multitask Learning. Machine learning. 281Rich Caruana. Multitask Learning. Machine learning, 28(1):41-75, 1997.\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won, Charles Chung, Sebastian Sutton, Gehrmann, arXiv:2204.02311Scaling language modeling with pathways. arXiv preprintAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n\nDeep reinforcement learning from human preferences. Advances in neural information processing systems. Jan Paul F Christiano, Tom Leike, Miljan Brown, Shane Martic, Dario Legg, Amodei, 30Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep re- inforcement learning from human preferences. Advances in neural information processing systems, 30, 2017.\n\nSiddhartha Brahma, et al. Scaling instruction-finetuned language models. Hyung Won, Le Chung, Shayne Hou, Barret Longpre, Yi Zoph, William Tay, Eric Fedus, Xuezhi Li, Mostafa Wang, Dehghani, arXiv:2210.11416arXiv preprintHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language mod- els. arXiv preprint arXiv:2210.11416, 2022a.\n\nScaling instruction-finetuned language models. Hyung Won, Le Chung, Shayne Hou, Barret Longpre, Yi Zoph, William Tay, Yunxuan Fedus, Xuezhi Li, Mostafa Wang, Siddhartha Dehghani, Albert Brahma, Webson, Shane Shixiang, Zhuyun Gu, Mirac Dai, Xinyun Suzgun, Aakanksha Chen, Alex Chowdhery, Marie Castro-Ros, Kevin Pellat, Dasha Robinson, Sharan Valter, Gaurav Narang, Adams Mishra, Vincent Yu, Yanping Zhao, Andrew Huang, Hongkun Dai, Slav Yu, Ed H Petrov, Jeff Chi, Jacob Dean, Adam Devlin, Denny Roberts, Quoc V Zhou, Jason Le, Wei, Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pel- lat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-finetuned language models, 2022b. URL https://arxiv.org/abs/2210.11416.\n\nTraining verifiers to solve math word problems. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, John Schulman, arXiv:2110.14168arXiv preprintKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021.\n\nA unified architecture for natural language processing: Deep neural networks with multitask learning. Ronan Collobert, Jason Weston, International Conference on Machine Learning (ICML). Ronan Collobert and Jason Weston. A unified architecture for natural language processing: Deep neural networks with multitask learning. In International Conference on Machine Learning (ICML), 2008.\n\nThe second conversational intelligence challenge (convai2). Emily Dinan, Varvara Logacheva, Valentin Malykh, Alexander H Miller, Kurt Shuster, Jack Urbanek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, Shrimai Prabhumoye, Alan W Black, Alexander I Rudnicky, Jason D Williams, Joelle Pineau, Mikhail S Burtsev, Jason Weston, abs/1902.00098CoRREmily Dinan, Varvara Logacheva, Valentin Malykh, Alexander H. Miller, Kurt Shuster, Jack Ur- banek, Douwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, Shrimai Prabhumoye, Alan W. Black, Alexander I. Rudnicky, Jason D. Williams, Joelle Pineau, Mikhail S. Burtsev, and Ja- son Weston. The second conversational intelligence challenge (convai2). CoRR, abs/1902.00098, 2019a. URL http://arxiv.org/abs/1902.00098.\n\nWizard of wikipedia: Knowledge-powered conversational agents. Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, Jason Weston, 7th International Conference on Learning Representations, ICLR 2019. New Orleans, LA, USAEmily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. Wiz- ard of wikipedia: Knowledge-powered conversational agents. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019b. URL https://openreview.net/forum?id=r1l73iRqKm.\n\nThe pile: An 800gb dataset of diverse text for language modeling. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, arXiv:2101.00027arXiv preprintLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\n\nRealtoxicityprompts: Evaluating neural toxic degeneration in language models. Suchin Samuel Gehman, Maarten Gururangan, Yejin Sap, Noah A Choi, Smith, Findings of the Association for Computational Linguistics: EMNLP 2020. Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. Realtoxici- typrompts: Evaluating neural toxic degeneration in language models. In Findings of the Associ- ation for Computational Linguistics: EMNLP 2020, pages 3356-3369, 2020.\n\nDid aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, Jonathan Berant, Transactions of the Association for Computational Linguistics. 9Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346-361, 2021.\n\nMeasuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, International Conference on Learning Representations. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2020.\n\nMeasuring massive multitask language understanding. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, Jacob Steinhardt, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. Proceedings of the International Conference on Learning Representations (ICLR), 2021a.\n\nMeasuring mathematical problem solving with the math dataset. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, Jacob Steinhardt, NeurIPS. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. NeurIPS, 2021b.\n\nEditing models with task arithmetic. Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, Ali Farhadi, Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic, 2022. URL https: //arxiv.org/abs/2212.04089.\n\nUnifiedQA: Crossing Format Boundaries With a Single QA System. Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, Hannaneh Hajishirzi, Conference on Empirical Methods in Natural Language Processing (EMNLP) -Findings. Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. UnifiedQA: Crossing Format Boundaries With a Single QA System. In Conference on Empirical Methods in Natural Language Processing (EMNLP) -Findings, 2020.\n\nQASC: A dataset for question answering via sentence composition. Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, Ashish Sabharwal, Proceedings of AAAI. AAAITushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, and Ashish Sabharwal. QASC: A dataset for question answering via sentence composition. In Proceedings of AAAI, 2020.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\n\nLarge language models are zero-shot reasoners. Takeshi Kojima, Shane Shixiang, Machel Gu, Yutaka Reid, Yusuke Matsuo, Iwasawa, arXiv:2205.11916arXiv preprintTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. arXiv preprint arXiv:2205.11916, 2022.\n\nAssociation for Computational Linguistics. Mojtaba Komeili, Kurt Shuster, Jason Weston, 10.18653/v1/2022.acl-long.579Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. Smaranda Muresan, Preslav Nakov, and Aline Villavicenciothe 60th Annual Meeting of the Association for Computational LinguisticsDublin, Ireland1Internet-augmented dialogue generationMojtaba Komeili, Kurt Shuster, and Jason Weston. Internet-augmented dialogue generation. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th An- nual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 8460-8478. Association for Computational Lin- guistics, 2022. doi: 10.18653/v1/2022.acl-long.579. URL https://doi.org/10.18653/v1/2022. acl-long.579.\n\nLearning task grouping and overlap in multi-task learning. Abhishek Kumar, Hal Daume, Iii , ICML. Abhishek Kumar and Hal Daume III. Learning task grouping and overlap in multi-task learning. In ICML, 2012.\n\nAsk me anything: Dynamic memory networks for natural language processing. Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, Richard Socher, Proceedings of The 33rd International Conference on Machine Learning. Maria Florina Balcan and Kilian Q. WeinbergerThe 33rd International Conference on Machine LearningNew York, New York, USA48Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher. Ask me anything: Dynamic memory networks for natural language processing. In Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pages 1378-1387, New York, New York, USA, 20-22 Jun 2016. PMLR. URL https://proceedings.mlr.press/v48/kumar16.html.\n\nQED: A framework and dataset for explanations in question answering. Matthew Lamm, Jennimaria Palomaki, Chris Alberti, Daniel Andor, Eunsol Choi, Livio Baldini Soares, Michael Collins, Transactions of the Association for Computational Linguistics. 9Matthew Lamm, Jennimaria Palomaki, Chris Alberti, Daniel Andor, Eunsol Choi, Livio Baldini Soares, and Michael Collins. QED: A framework and dataset for explanations in question answer- ing. Transactions of the Association for Computational Linguistics, 9:790-806, 2021.\n\nThe Power of Scale for Parameter-Efficient Prompt Tuning. Brian Lester, Rami Al-Rfou, Noah Constant, Conference on Empirical Methods in Natural Language Processing. 2021Brian Lester, Rami Al-Rfou, and Noah Constant. The Power of Scale for Parameter-Efficient Prompt Tuning. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021.\n\nHolistic evaluation of language models. Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, arXiv:2211.09110arXiv preprintPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, et al. Holistic evaluation of language models. arXiv preprint arXiv:2211.09110, 2022.\n\nProgram induction by rationale generation: Learning to solve and explain algebraic word problems. Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom, ACL. Wang Ling, Dani Yogatama, Chris Dyer, and Phil Blunsom. Program induction by rationale gener- ation: Learning to solve and explain algebraic word problems. In ACL, 2017.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar S Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke S Zettlemoyer, Veselin Stoyanov, Roberta, A robustly optimized bert pretraining approach. arXiv. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar S. Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke S. Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized bert pre- training approach. arXiv, 2019. URL http://arxiv.org/abs/1907.11692.\n\nFantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus Stenetorp, arXiv:2104.08786arXiv preprintYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786, 2021.\n\nThe natural language decathlon: Multitask learning as question answering. CoRR, abs/1806.08730. Bryan Mccann, Nitish Shirish Keskar, Caiming Xiong, Richard Socher, Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. The natural language decathlon: Multitask learning as question answering. CoRR, abs/1806.08730, 2018. URL http: //arxiv.org/abs/1806.08730.\n\nGanesh Venkatesh, et al. Mixed precision training. Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, International Conference on Learning Representations. Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training. In International Conference on Learning Representations, 2018.\n\nSewon Min, Mike Lewis, Luke Zettlemoyer, Hannaneh Hajishirzi, Metaicl, arXiv:2110.15943Learning to Learn In Context. arXiv preprintSewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. MetaICL: Learning to Learn In Context. arXiv preprint arXiv:2110.15943, 2021.\n\nRethinking the Role of Demonstrations: What Makes In-Context Learning Work?. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, Luke Zettlemoyer, arXiv:2202.12837arXiv preprintSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the Role of Demonstrations: What Makes In-Context Learning Work? arXiv preprint arXiv:2202.12837, 2022.\n\nLinyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, arXiv:2007.02871Open-domain structured data record to text generation. arXiv preprintLinyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, et al. Dart: Open-domain structured data record to text generation. arXiv preprint arXiv:2007.02871, 2020.\n\nCrows-pairs: A challenge dataset for measuring social biases in masked language models. Nikita Nangia, Clara Vania, Rasika Bhalerao, Samuel Bowman, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel Bowman. Crows-pairs: A challenge dataset for measuring social biases in masked language models. In Proceedings of the 2020 Con- ference on Empirical Methods in Natural Language Processing (EMNLP), pages 1953-1967, 2020.\n\nShow your work: Scratchpads for intermediate computation with language models. Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, arXiv:2112.00114arXiv preprintMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. Show your work: Scratchpads for intermediate computation with language models. arXiv preprint arXiv:2112.00114, 2021.\n\nCREAK: A dataset for commonsense reasoning over entity knowledge. Yasumasa Onoe, J Q Michael, Eunsol Zhang, Greg Choi, Durrett, arXiv:2109.01653arXiv preprintYasumasa Onoe, Michael JQ Zhang, Eunsol Choi, and Greg Durrett. CREAK: A dataset for commonsense reasoning over entity knowledge. arXiv preprint arXiv:2109.01653, 2021.\n\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, L Carroll, Pamela Wainwright, Chong Mishkin, Sandhini Zhang, Katarina Agarwal, Alex Slama, Ray, arXiv:2203.02155Training Language Models to Follow Instructions with Human Feedback. arXiv preprintLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training Language Models to Follow Instructions with Human Feedback. arXiv preprint arXiv:2203.02155, 2022.\n\nCollecting diverse natural language inference problems for sentence representation evaluation. Adam Poliak, Aparajita Haldar, Rachel Rudinger, J Edward Hu, Ellie Pavlick, Aaron Steven White, Benjamin Van Durme, Conference on Empirical Methods in Natural Language Processing (EMNLP). Adam Poliak, Aparajita Haldar, Rachel Rudinger, J. Edward Hu, Ellie Pavlick, Aaron Steven White, and Benjamin Van Durme. Collecting diverse natural language inference problems for sentence representation evaluation. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018.\n\nGpt-2 output dataset. Alec Radford, Jong Wook Kim, Jeff Wu, Alec Radford, Jong Wook Kim, and Jeff Wu. Gpt-2 output dataset. https://github.com/openai/ gpt-2-output-dataset, 2021. Last Updated: 02-17-21.\n\nExploring the Limits of Transfer Learning with a Unified Textto-Text Transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 2020JMLRColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the Limits of Transfer Learning with a Unified Text- to-Text Transformer. Journal of Machine Learning Research (JMLR), 2020.\n\nExplain yourself! leveraging language models for commonsense reasoning. Bryan Nazneen Fatema Rajani, Caiming Mccann, Richard Xiong, Socher, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsNazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself! leveraging language models for commonsense reasoning. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4932-4942, 2019.\n\nSQuAD: 100,000+ Questions for Machine Comprehension of Text. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, Percy Liang, Conference on Empirical Methods in Natural Language Processing (EMNLP). Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ Questions for Machine Comprehension of Text. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016.\n\nKnow what you don't know: Unanswerable questions for SQuAD. Pranav Rajpurkar, Robin Jia, Percy Liang, 10.18653/v1/P18-2124Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsMelbourne, AustraliaAssociation for Computational Linguistics2Short Papers)Pranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don't know: Unanswerable ques- tions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Com- putational Linguistics (Volume 2: Short Papers), pages 784-789, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-2124. URL https: //aclanthology.org/P18-2124.\n\nCoqa: A conversational question answering challenge. Siva Reddy, Danqi Chen, Christopher D Manning, Transactions of the Association for Computational Linguistics. 7Siva Reddy, Danqi Chen, and Christopher D Manning. Coqa: A conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249-266, 2019.\n\nChoice of plausible alternatives: An evaluation of commonsense causal reasoning. Melissa Roemmele, Andrew S Cosmin Adrian Bejan, Gordon, AAAI spring symposium: logical formalizations of commonsense reasoning. Melissa Roemmele, Cosmin Adrian Bejan, and Andrew S Gordon. Choice of plausible alternatives: An evaluation of commonsense causal reasoning. In AAAI spring symposium: logical formaliza- tions of commonsense reasoning, 2011.\n\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M Smith, arXiv:2004.13637Recipes for building an open-domain chatbot. arXiv preprintStephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Kurt Shuster, Eric M Smith, et al. Recipes for building an open-domain chatbot. arXiv preprint arXiv:2004.13637, 2020.\n\nGender bias in coreference resolution. Rachel Rudinger, Jason Naradowsky, Brian Leonard, Benjamin Van Durme, Conference of the North American Chapter of the Association for Computational Linguistics -Human Language Technologies (NAACL-HLT). Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. Gender bias in coreference resolution. In Conference of the North American Chapter of the Association for Com- putational Linguistics -Human Language Technologies (NAACL-HLT), 2018.\n\nMultitask Prompted Training Enables Zero-Shot Task Generalization. Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, Canwen Bari, Urmish Xu, Shanya Thakker, Eliza Sharma Sharma, Taewoon Szczechla, Gunjan Kim, Nihal Chhablani, Debajyoti Nayak, Jonathan Datta, Mike Chang, Tian-Jian, Han Jiang, Matteo Wang, Sheng Manica, Shen, International Conference on Learning Representations (ICLR. Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le ScaoStella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush2022Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, De- bajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Ab- heesht Sharma, Andrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas Wolf, and Alexander M Rush. Multitask Prompted Training Enables Zero-Shot Task Generalization. In International Conference on Learning Representations (ICLR), 2022.\n\nContinual-t0: Progressively instructing 50+ tasks to language models without forgetting. CoRR, abs/2205.12393. Thomas Scialom, Tuhin Chakrabarty, Smaranda Muresan, 10.48550/arXiv.2205.123932022Thomas Scialom, Tuhin Chakrabarty, and Smaranda Muresan. Continual-t0: Progressively in- structing 50+ tasks to language models without forgetting. CoRR, abs/2205.12393, 2022. doi: 10.48550/arXiv.2205.12393. URL https://doi.org/10.48550/arXiv.2205.12393.\n\nMegatron-lm: Training multi-billion parameter language models using model parallelism. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick Legresley, Jared Casper, Bryan Catanzaro, arXiv:1909.08053arXiv preprintMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model paral- lelism. arXiv preprint arXiv:1909.08053, 2019.\n\nDialogue in the wild: Learning from a deployed role-playing game with humans and bots. Kurt Shuster, Jack Urbanek, Emily Dinan, Arthur Szlam, Jason Weston, 10.18653/v1/2021.findings-acl.54Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Association for Computational LinguisticsKurt Shuster, Jack Urbanek, Emily Dinan, Arthur Szlam, and Jason Weston. Dialogue in the wild: Learning from a deployed role-playing game with humans and bots. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 611-624, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-acl.54. URL https://aclanthology.org/2021.findings-acl.54.\n\n. Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, arXiv:2208.03188arXiv preprintBlenderbot 3: a deployed conversational agent that continually learns to responsibly engageKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, et al. Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage. arXiv preprint arXiv:2208.03188, 2022.\n\nCan you put it all together: Evaluating conversational agents' ability to blend skills. Eric Michael Smith, Mary Williamson, Kurt Shuster, Jason Weston, Y-Lan Boureau, 10.18653/v1/2020.acl-main.183Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreaultthe 58th Annual Meeting of the Association for Computational LinguisticsOnlineAssociation for Computational Linguistics2020Eric Michael Smith, Mary Williamson, Kurt Shuster, Jason Weston, and Y-Lan Boureau. Can you put it all together: Evaluating conversational agents' ability to blend skills. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 2021- 2030. Association for Computational Linguistics, 2020. doi: 10.18653/v1/2020.acl-main.183. URL https://doi.org/10.18653/v1/2020.acl-main.183.\n\nBeyond the imitation game: Quantifying and extrapolating the capabilities of language models. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, R Adam, Adam Brown, Aditya Santoro, Adri\u00e0 Gupta, Garriga-Alonso, arXiv:2206.04615arXiv preprintAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615, 2022.\n\nChallenging big-bench tasks and whether chain-of-thought can solve them. Mirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won, Aakanksha Chung, Chowdhery, V Quoc, Ed H Le, Denny Chi, Zhou, arXiv:2210.09261arXiv preprintMirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022.\n\nOyvind Tafjord, Peter Bhavana Dalvi Mishra, Clark, arXiv:2012.13048ProofWriter: Generating implications, proofs, and abductive statements over natural language. arXiv preprintOyvind Tafjord, Bhavana Dalvi Mishra, and Peter Clark. ProofWriter: Generating implications, proofs, and abductive statements over natural language. arXiv preprint arXiv:2012.13048, 2020.\n\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze, Alicia Cheng, Taylor Jin, Leslie Bos, Yu Baker, Du, arXiv:2201.08239Language models for dialog applications. arXiv preprintRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al. Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239, 2022.\n\nLearning to speak and act in a fantasy text adventure game. Jack Urbanek, Angela Fan, Siddharth Karamcheti, Saachi Jain, Samuel Humeau, Emily Dinan, Tim Rockt\u00e4schel, Douwe Kiela, Arthur Szlam, Jason Weston, 10.18653/v1/D19-1062Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)Hong Kong, ChinaAssociation for Computational LinguisticsJack Urbanek, Angela Fan, Siddharth Karamcheti, Saachi Jain, Samuel Humeau, Emily Dinan, Tim Rockt\u00e4schel, Douwe Kiela, Arthur Szlam, and Jason Weston. Learning to speak and act in a fantasy text adventure game. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 673-683, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1062. URL https://aclanthology.org/ D19-1062.\n\nExploring and predicting transferability across nlp tasks. Tu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessandro Sordoni, Adam Trischler, Andrew Mattarella-Micke, Subhransu Maji, Mohit Iyyer, EMNLP. Tu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessandro Sordoni, Adam Trischler, Andrew Mattarella-Micke, Subhransu Maji, and Mohit Iyyer. Exploring and predicting transferability across nlp tasks. In EMNLP, 2020.\n\nDoes it make sense? and why? a pilot study for sense making and explanation. Cunxiang Wang, Shuailong Liang, Yue Zhang, Xiaonan Li, Tian Gao, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsCunxiang Wang, Shuailong Liang, Yue Zhang, Xiaonan Li, and Tian Gao. Does it make sense? and why? a pilot study for sense making and explanation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4020-4026, 2019.\n\nBenchmarking generalization via in. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan Pathak, Giannis Karamanolakis, Gary Haizhi, Ishan Lai, Ishani Purohit, Jacob Mondal, Kirby Anderson, Krima Kuznia, Maitreya Doshi, Patel, Kuntal Kumar Pal, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Shailaja Keyur Sampat, Savan Doshi, Siddhartha Mishra, Sujan Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi, and Daniel Khashabicontext instructions on 1,600+ language tasksYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, An- jana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Gary Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Maitreya Patel, Kuntal Kumar Pal, Mehrad Moradshahi, Mihir Par- mar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Shailaja Keyur Sampat, Savan Doshi, Siddhartha Mishra, Sujan Reddy, Sumanta Patro, Tanay Dixit, Xudong Shen, Chitta Baral, Yejin Choi, Noah A. Smith, Hannaneh Hajishirzi, and Daniel Khashabi. Benchmarking generalization via in-context instructions on 1,600+ language tasks, 2022. URL https://arxiv.org/abs/2204.07705.\n\nFinetuned Language Models are Zero-Shot Learners. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, Quoc V Le, International Conference on Learning Representations (ICLR). Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. Finetuned Language Models are Zero-Shot Learners. In Inter- national Conference on Learning Representations (ICLR), 2022a.\n\nChain of thought prompting elicits reasoning in large language models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, Denny Zhou, arXiv:2201.11903arXiv preprintJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022b.\n\nSymbolic knowledge distillation: from general language models to commonsense models. Peter West, Chandra Bhagavatula, Jack Hessel, Jena Hwang, Liwei Jiang, Ximing Ronan Le Bras, Sean Lu, Yejin Welleck, Choi, 10.18653/v1/2022.naacl-main.341Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSeattle, United StatesAssociation for Computational LinguisticsPeter West, Chandra Bhagavatula, Jack Hessel, Jena Hwang, Liwei Jiang, Ronan Le Bras, Ximing Lu, Sean Welleck, and Yejin Choi. Symbolic knowledge distillation: from general language models to commonsense models. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4602-4625, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/ 2022.naacl-main.341. URL https://aclanthology.org/2022.naacl-main.341.\n\nTianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien-Sheng Wu, Ming Zhong, Pengcheng Yin, I Sida, Wang, arXiv:2201.05966Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models. arXiv preprintTianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong, Torsten Scholak, Michihiro Yasunaga, Chien- Sheng Wu, Ming Zhong, Pengcheng Yin, Sida I Wang, et al. UnifiedSKG: Unifying and Multi- Tasking Structured Knowledge Grounding with Text-to-Text Language Models. arXiv preprint arXiv:2201.05966, 2022.\n\nBeyond goldfish memory: Long-term open-domain conversation. Jing Xu, Arthur Szlam, Jason Weston, 10.18653/v1/2022.acl-long.356Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, Ireland2022Association for Computational LinguisticsJing Xu, Arthur Szlam, and Jason Weston. Beyond goldfish memory: Long-term open-domain conversation. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Pro- ceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Vol- ume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 5180-5197. Asso- ciation for Computational Linguistics, 2022. doi: 10.18653/v1/2022.acl-long.356. URL https: //doi.org/10.18653/v1/2022.acl-long.356.\n\nCrossFit: A Few-shot Learning Challenge for Crosstask Generalization in NLP. Qinyuan Ye, Xiang Bill Yuchen Lin, Ren, Conference on Empirical Methods in Natural Language Processing. 2021Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. CrossFit: A Few-shot Learning Challenge for Cross- task Generalization in NLP. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2021.\n\nContintin: Continual learning from task instructions. Wenpeng Yin, Jia Li, Caiming Xiong, 10.18653/v1/2022.acl-long.218Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, Ireland1Association for Computational LinguisticsWenpeng Yin, Jia Li, and Caiming Xiong. Contintin: Continual learning from task instructions. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 3062-3072. Association for Computational Lin- guistics, 2022. doi: 10.18653/v1/2022.acl-long.218. URL https://doi.org/10.18653/v1/2022. acl-long.218.\n\nSpider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, arXiv:1809.08887arXiv preprintTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, et al. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. arXiv preprint arXiv:1809.08887, 2018.\n\nWinowhy: A deep diagnosis of essential commonsense knowledge for answering winograd schema challenge. Hongming Zhang, Xinran Zhao, Yangqiu Song, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsHongming Zhang, Xinran Zhao, and Yangqiu Song. Winowhy: A deep diagnosis of essential com- monsense knowledge for answering winograd schema challenge. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5736-5745, 2020.\n\n| recreation ground | burntisland shipyard | fife cup 2007-08 | raith rovers official site. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.010682022. table: col : date | venue | opponents | competition | match report row 1 : 12 july 2008 | ochilview park | stenhousemuir | f | raith rovers official site. 2arXiv preprint| forthbank stadium | stirling albion | second division | raith rovers official site. dead link ] row 31 : 17 january 2009 | stark's park | stranraer | second division | raith rovers official site [ dead link ] question: what is the match report from the game played on 25 april 2009? answer: raith rovers official site. dead linkSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. table: col : date | venue | opponents | competition | match report row 1 : 12 july 2008 | ochilview park | stenhousemuir | f | raith rovers official site [ dead link ] row 2 : 15 july 2008 | stark's park | hibs | f | raith rovers official site [ dead link ] row 3 : 19 july 2008 | stark's park | dundee united | f | raith rovers official site [ dead link ] row 4 : 26 july 2008 | stark's park | rangers | f | raith rovers official site [ dead link ] row 5 : 30 july 2008 | recreation ground | burntisland shipyard | fife cup 2007-08 | raith rovers official site [ dead link ] row 6 : 2 august 2008 | somerset park | ayr united | second division | raith rovers official site [ dead link ] row 7 : 5 august 2008 | cliftonhill | albion rovers | slc | raith rovers official site [ dead link ] row 8 : 9 august 2008 | stark's park | queen's park | second division | raith rovers official site [ dead link ] row 9 : 12 august 2008 | stark's park | ross county | scc | raith rovers official site [ dead link ] row 10 : 16 august 2008 | recreation park | alloa athletic | second division | raith rovers official site [ dead link ] row 11 : 23 august 2008 | stark's park | stranraer | second division | raith rovers official site [ dead link ] row 12 : 26 august 2008 | stark's park | falkirk | slc | raith rovers official site [ dead link ] row 13 : 30 august 2008 | balmoor | peterhead | second division | raith rovers official site [ dead link ] row 14 : 13 september 2008 | stark's park | east fife | second division | raith rovers official site [ dead link ] row 15 : 20 september 2008 | stark's park | stirling albion | second division | raith rovers official site [ dead link ] row 16 : 27 september 2008 | glebe park | brechin city | second division | raith rovers official site [ dead link ] row 17 : 4 october 2008 | gayfield park | arbroath | second division | raith rovers official site [ dead link ] row 18 : 18 october 2008 | stark's park | ayr united | second division | raith rovers official site [ dead link ] row 19 : 25 october 2008 | stair park | stranraer | second division | raith rovers official site [ dead link ] row 20 : 1 november 2008 | stark's park | alloa athletic | second division | raith rovers official site [ dead link ] row 21 : 8 november 2008 | bayview stadium | east fife | second division | raith rovers official site [ dead link ] row 22 : 15 november 2008 | stark's park | peterhead | second division | raith rovers official site [ dead link ] row 23 : 22 november 2008 | stark's park | brechin city | second division | raith rovers official site [ dead link ] row 24 : 29 november 2008 | stark's park | alloa athletic | sc | raith rovers official site [ dead link ] row 25 : 9 december 2008 | recreation park | alloa athletic | sc | raith rovers official site [ dead link ] row 26 : 13 december 2008 | hampden park | queen's park | second division | raith rovers official site [ dead link ] row 27 : 20 december 2008 | stark's park | arbroath | second division | raith rovers official site [ dead link ] row 28 : 27 december 2008 | balmoor | peterhead | second division | raith rovers official site [ dead link ] row 29 : 3 january 2009 | stark's park | east fife | second division | raith rovers official site [ dead link ] row 30 : 10 january 2009 | forthbank stadium | stirling albion | second division | raith rovers official site [ dead link ] row 31 : 17 january 2009 | stark's park | stranraer | second division | raith rovers official site [ dead link ] question: what is the match report from the game played on 25 april 2009? answer: raith rovers official site [ dead link ]\n", "annotations": {"author": "[{\"end\":121,\"start\":97},{\"end\":142,\"start\":122},{\"end\":155,\"start\":143},{\"end\":183,\"start\":156},{\"end\":207,\"start\":184},{\"end\":229,\"start\":208},{\"end\":246,\"start\":230},{\"end\":268,\"start\":247},{\"end\":289,\"start\":269},{\"end\":312,\"start\":290},{\"end\":333,\"start\":313},{\"end\":350,\"start\":334},{\"end\":377,\"start\":351},{\"end\":402,\"start\":378},{\"end\":421,\"start\":403},{\"end\":448,\"start\":422},{\"end\":474,\"start\":449},{\"end\":500,\"start\":475},{\"end\":522,\"start\":501}]", "publisher": null, "author_last_name": "[{\"end\":112,\"start\":108},{\"end\":133,\"start\":131},{\"end\":146,\"start\":143},{\"end\":174,\"start\":166},{\"end\":198,\"start\":190},{\"end\":220,\"start\":215},{\"end\":237,\"start\":235},{\"end\":259,\"start\":252},{\"end\":280,\"start\":276},{\"end\":303,\"start\":300},{\"end\":324,\"start\":319},{\"end\":341,\"start\":339},{\"end\":368,\"start\":357},{\"end\":393,\"start\":386},{\"end\":412,\"start\":408},{\"end\":439,\"start\":434},{\"end\":465,\"start\":454},{\"end\":491,\"start\":480},{\"end\":513,\"start\":505}]", "author_first_name": "[{\"end\":107,\"start\":97},{\"end\":130,\"start\":122},{\"end\":165,\"start\":156},{\"end\":189,\"start\":184},{\"end\":214,\"start\":208},{\"end\":234,\"start\":230},{\"end\":251,\"start\":247},{\"end\":275,\"start\":269},{\"end\":299,\"start\":295},{\"end\":318,\"start\":313},{\"end\":338,\"start\":334},{\"end\":356,\"start\":351},{\"end\":385,\"start\":378},{\"end\":407,\"start\":403},{\"end\":433,\"start\":422},{\"end\":453,\"start\":449},{\"end\":479,\"start\":475},{\"end\":504,\"start\":501}]", "author_affiliation": "[{\"end\":120,\"start\":114},{\"end\":141,\"start\":135},{\"end\":154,\"start\":148},{\"end\":182,\"start\":176},{\"end\":206,\"start\":200},{\"end\":228,\"start\":222},{\"end\":245,\"start\":239},{\"end\":267,\"start\":261},{\"end\":288,\"start\":282},{\"end\":311,\"start\":305},{\"end\":332,\"start\":326},{\"end\":349,\"start\":343},{\"end\":376,\"start\":370},{\"end\":401,\"start\":395},{\"end\":420,\"start\":414},{\"end\":447,\"start\":441},{\"end\":473,\"start\":467},{\"end\":499,\"start\":493},{\"end\":521,\"start\":515}]", "title": "[{\"end\":94,\"start\":1},{\"end\":616,\"start\":523}]", "venue": null, "abstract": "[{\"end\":1434,\"start\":618}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b73\"},\"end\":1502,\"start\":1483},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1520,\"start\":1502},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1540,\"start\":1520},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":2568,\"start\":2549},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2605,\"start\":2586},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2741,\"start\":2720},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":4698,\"start\":4679},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4716,\"start\":4698},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5078,\"start\":5057},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5882,\"start\":5863},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":5900,\"start\":5882},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6457,\"start\":6440},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6566,\"start\":6545},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6955,\"start\":6936},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":6981,\"start\":6962},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":7044,\"start\":7026},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7309,\"start\":7289},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7397,\"start\":7373},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":7438,\"start\":7417},{\"end\":7494,\"start\":7474},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8073,\"start\":8054},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":8898,\"start\":8879},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8922,\"start\":8903},{\"end\":9038,\"start\":9015},{\"end\":9064,\"start\":9043},{\"end\":9092,\"start\":9075},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":9180,\"start\":9162},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9246,\"start\":9225},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":9470,\"start\":9447},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":9495,\"start\":9470},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":10206,\"start\":10189},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":10225,\"start\":10207},{\"end\":10228,\"start\":10227},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":11574,\"start\":11552},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":11777,\"start\":11758},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11795,\"start\":11777},{\"end\":11811,\"start\":11795},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":12957,\"start\":12934},{\"end\":13603,\"start\":13602},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":13903,\"start\":13885},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13921,\"start\":13903},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15560,\"start\":15540},{\"end\":16699,\"start\":16673},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":16846,\"start\":16828},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16875,\"start\":16857},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16927,\"start\":16901},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":16947,\"start\":16927},{\"end\":18299,\"start\":18278},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19132,\"start\":19110},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":19194,\"start\":19172},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":19953,\"start\":19926},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":23169,\"start\":23149},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":23866,\"start\":23842},{\"end\":24955,\"start\":24934},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":24973,\"start\":24955},{\"end\":25362,\"start\":25336},{\"end\":25390,\"start\":25362},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27013,\"start\":26994},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":27031,\"start\":27013},{\"end\":27047,\"start\":27031},{\"end\":27069,\"start\":27047},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":30898,\"start\":30879},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":35782,\"start\":35762},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":36638,\"start\":36618},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":36839,\"start\":36818},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":40641,\"start\":40623},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":40661,\"start\":40641},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":40685,\"start\":40668},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":42208,\"start\":42191},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":45437,\"start\":45418},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":45455,\"start\":45437},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":45472,\"start\":45455},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":45756,\"start\":45737},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":45832,\"start\":45813},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":46012,\"start\":45994},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":47920,\"start\":47899},{\"end\":48379,\"start\":48358},{\"end\":48399,\"start\":48379},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":49339,\"start\":49321},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":49951,\"start\":49932},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":50503,\"start\":50483},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":51220,\"start\":51202},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":51321,\"start\":51300},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":51502,\"start\":51481},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":52115,\"start\":52097},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":52181,\"start\":52157},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":53997,\"start\":53979},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":58079,\"start\":58062},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":58503,\"start\":58485},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":58605,\"start\":58588},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":58751,\"start\":58724},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":58972,\"start\":58954},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":59023,\"start\":59006},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":59089,\"start\":59062},{\"end\":61070,\"start\":61049},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":61252,\"start\":61227},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":61304,\"start\":61279},{\"end\":61421,\"start\":61403},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":61779,\"start\":61759},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":65078,\"start\":65054},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":65098,\"start\":65078},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":65845,\"start\":65824},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":66135,\"start\":66114},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":66153,\"start\":66135},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":66173,\"start\":66153},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":66315,\"start\":66294},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":66492,\"start\":66474},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":66510,\"start\":66492},{\"end\":66529,\"start\":66510},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":67062,\"start\":67038},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":67109,\"start\":67088},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":67487,\"start\":67466},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":67749,\"start\":67733},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":67909,\"start\":67892},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":67928,\"start\":67909},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":68405,\"start\":68386},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":68455,\"start\":68430},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":68707,\"start\":68689},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":68884,\"start\":68863},{\"end\":69503,\"start\":69488},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":69529,\"start\":69503},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":69750,\"start\":69722},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":69770,\"start\":69750},{\"end\":69790,\"start\":69770},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":69806,\"start\":69790},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":70037,\"start\":70017},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":70059,\"start\":70037},{\"end\":70319,\"start\":70297},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":70656,\"start\":70634},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":70690,\"start\":70673},{\"end\":72496,\"start\":72473},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":75168,\"start\":75148},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":75200,\"start\":75181},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":75230,\"start\":75211},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":75257,\"start\":75237},{\"end\":75286,\"start\":75259},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":75313,\"start\":75294},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":75343,\"start\":75320},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":75374,\"start\":75352},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":75406,\"start\":75381},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":75452,\"start\":75433},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":75485,\"start\":75466},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":75667,\"start\":75646},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":75719,\"start\":75698},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":75779,\"start\":75757},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":75800,\"start\":75779},{\"end\":79370,\"start\":79321},{\"end\":79410,\"start\":79379},{\"end\":93661,\"start\":92568},{\"end\":97808,\"start\":97799}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":106933,\"start\":106088},{\"attributes\":{\"id\":\"fig_1\"},\"end\":107696,\"start\":106934},{\"attributes\":{\"id\":\"fig_2\"},\"end\":107869,\"start\":107697},{\"attributes\":{\"id\":\"fig_3\"},\"end\":108018,\"start\":107870},{\"attributes\":{\"id\":\"fig_6\"},\"end\":109247,\"start\":108019},{\"attributes\":{\"id\":\"fig_7\"},\"end\":109446,\"start\":109248},{\"attributes\":{\"id\":\"fig_8\"},\"end\":110233,\"start\":109447},{\"attributes\":{\"id\":\"fig_9\"},\"end\":110820,\"start\":110234},{\"attributes\":{\"id\":\"fig_10\"},\"end\":111497,\"start\":110821},{\"attributes\":{\"id\":\"fig_11\"},\"end\":112432,\"start\":111498},{\"attributes\":{\"id\":\"fig_12\"},\"end\":112534,\"start\":112433},{\"attributes\":{\"id\":\"fig_13\"},\"end\":113570,\"start\":112535},{\"attributes\":{\"id\":\"fig_14\"},\"end\":113691,\"start\":113571},{\"attributes\":{\"id\":\"fig_15\"},\"end\":115381,\"start\":113692},{\"attributes\":{\"id\":\"fig_16\"},\"end\":115513,\"start\":115382},{\"attributes\":{\"id\":\"fig_17\"},\"end\":115637,\"start\":115514},{\"attributes\":{\"id\":\"fig_18\"},\"end\":115740,\"start\":115638},{\"attributes\":{\"id\":\"fig_19\"},\"end\":115843,\"start\":115741},{\"attributes\":{\"id\":\"fig_20\"},\"end\":115950,\"start\":115844},{\"attributes\":{\"id\":\"fig_21\"},\"end\":116363,\"start\":115951},{\"attributes\":{\"id\":\"fig_22\"},\"end\":116471,\"start\":116364},{\"attributes\":{\"id\":\"fig_23\"},\"end\":118320,\"start\":116472},{\"attributes\":{\"id\":\"fig_24\"},\"end\":118419,\"start\":118321},{\"attributes\":{\"id\":\"fig_25\"},\"end\":118582,\"start\":118420},{\"attributes\":{\"id\":\"fig_26\"},\"end\":119751,\"start\":118583},{\"attributes\":{\"id\":\"fig_27\"},\"end\":120072,\"start\":119752},{\"attributes\":{\"id\":\"fig_28\"},\"end\":120200,\"start\":120073},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":120925,\"start\":120201},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":121283,\"start\":120926},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":121342,\"start\":121284},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":121520,\"start\":121343},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":121649,\"start\":121521},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":121986,\"start\":121650},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":122832,\"start\":121987},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":123127,\"start\":122833},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":123219,\"start\":123128},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":123334,\"start\":123220},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":123522,\"start\":123335},{\"attributes\":{\"id\":\"tab_22\",\"type\":\"table\"},\"end\":123647,\"start\":123523},{\"attributes\":{\"id\":\"tab_23\",\"type\":\"table\"},\"end\":124233,\"start\":123648},{\"attributes\":{\"id\":\"tab_24\",\"type\":\"table\"},\"end\":126035,\"start\":124234},{\"attributes\":{\"id\":\"tab_25\",\"type\":\"table\"},\"end\":126227,\"start\":126036},{\"attributes\":{\"id\":\"tab_27\",\"type\":\"table\"},\"end\":126431,\"start\":126228},{\"attributes\":{\"id\":\"tab_29\",\"type\":\"table\"},\"end\":127332,\"start\":126432},{\"attributes\":{\"id\":\"tab_30\",\"type\":\"table\"},\"end\":128376,\"start\":127333},{\"attributes\":{\"id\":\"tab_31\",\"type\":\"table\"},\"end\":129242,\"start\":128377},{\"attributes\":{\"id\":\"tab_32\",\"type\":\"table\"},\"end\":130332,\"start\":129243},{\"attributes\":{\"id\":\"tab_33\",\"type\":\"table\"},\"end\":133993,\"start\":130333},{\"attributes\":{\"id\":\"tab_34\",\"type\":\"table\"},\"end\":136226,\"start\":133994},{\"attributes\":{\"id\":\"tab_35\",\"type\":\"table\"},\"end\":137082,\"start\":136227},{\"attributes\":{\"id\":\"tab_36\",\"type\":\"table\"},\"end\":138195,\"start\":137083},{\"attributes\":{\"id\":\"tab_37\",\"type\":\"table\"},\"end\":139413,\"start\":138196},{\"attributes\":{\"id\":\"tab_38\",\"type\":\"table\"},\"end\":139690,\"start\":139414},{\"attributes\":{\"id\":\"tab_39\",\"type\":\"table\"},\"end\":140343,\"start\":139691}]", "paragraph": "[{\"end\":2443,\"start\":1450},{\"end\":2966,\"start\":2445},{\"end\":3238,\"start\":2990},{\"end\":3245,\"start\":3240},{\"end\":4127,\"start\":3259},{\"end\":5488,\"start\":4129},{\"end\":6681,\"start\":5490},{\"end\":7854,\"start\":6683},{\"end\":8270,\"start\":7891},{\"end\":8710,\"start\":8272},{\"end\":9322,\"start\":8728},{\"end\":10442,\"start\":9324},{\"end\":11633,\"start\":10470},{\"end\":12576,\"start\":11635},{\"end\":12588,\"start\":12578},{\"end\":12641,\"start\":12612},{\"end\":13166,\"start\":12643},{\"end\":14486,\"start\":13196},{\"end\":15276,\"start\":14488},{\"end\":16441,\"start\":15305},{\"end\":17083,\"start\":16469},{\"end\":17818,\"start\":17109},{\"end\":18081,\"start\":17868},{\"end\":18947,\"start\":18116},{\"end\":20087,\"start\":18979},{\"end\":20927,\"start\":20133},{\"end\":21523,\"start\":20950},{\"end\":22024,\"start\":21525},{\"end\":22927,\"start\":22026},{\"end\":23582,\"start\":22929},{\"end\":24875,\"start\":23584},{\"end\":25594,\"start\":24923},{\"end\":26751,\"start\":25596},{\"end\":26880,\"start\":26753},{\"end\":28880,\"start\":26925},{\"end\":30667,\"start\":28882},{\"end\":33395,\"start\":30710},{\"end\":34044,\"start\":33449},{\"end\":34947,\"start\":34046},{\"end\":35109,\"start\":34961},{\"end\":35709,\"start\":35111},{\"end\":36443,\"start\":35750},{\"end\":38194,\"start\":36445},{\"end\":39015,\"start\":38234},{\"end\":39726,\"start\":39017},{\"end\":40406,\"start\":39728},{\"end\":42337,\"start\":40459},{\"end\":42906,\"start\":42339},{\"end\":44572,\"start\":42908},{\"end\":46253,\"start\":44591},{\"end\":47153,\"start\":46273},{\"end\":48265,\"start\":47155},{\"end\":49571,\"start\":48297},{\"end\":50079,\"start\":49573},{\"end\":51190,\"start\":50081},{\"end\":51962,\"start\":51192},{\"end\":52984,\"start\":51986},{\"end\":53811,\"start\":52986},{\"end\":54994,\"start\":53813},{\"end\":56458,\"start\":55039},{\"end\":56677,\"start\":56468},{\"end\":58021,\"start\":56679},{\"end\":59147,\"start\":58051},{\"end\":59618,\"start\":59149},{\"end\":60598,\"start\":59649},{\"end\":63627,\"start\":60600},{\"end\":63917,\"start\":63679},{\"end\":64810,\"start\":63919},{\"end\":65433,\"start\":64829},{\"end\":65704,\"start\":65450},{\"end\":67231,\"start\":65706},{\"end\":68231,\"start\":67233},{\"end\":69208,\"start\":68233},{\"end\":70162,\"start\":69210},{\"end\":70987,\"start\":70164},{\"end\":72220,\"start\":71003},{\"end\":72375,\"start\":72266},{\"end\":72541,\"start\":72397},{\"end\":72657,\"start\":72543},{\"end\":73062,\"start\":72659},{\"end\":73154,\"start\":73064},{\"end\":73448,\"start\":73164},{\"end\":74748,\"start\":73450},{\"end\":74925,\"start\":74750},{\"end\":75066,\"start\":74927},{\"end\":75496,\"start\":75068},{\"end\":75839,\"start\":75533},{\"end\":76004,\"start\":75875},{\"end\":76417,\"start\":76093},{\"end\":77865,\"start\":76484},{\"end\":78218,\"start\":77921},{\"end\":78881,\"start\":78220},{\"end\":78926,\"start\":78883},{\"end\":79068,\"start\":78928},{\"end\":80190,\"start\":79070},{\"end\":80370,\"start\":80192},{\"end\":81078,\"start\":80372},{\"end\":81515,\"start\":81080},{\"end\":81873,\"start\":81517},{\"end\":82282,\"start\":81875},{\"end\":82864,\"start\":82284},{\"end\":83264,\"start\":82866},{\"end\":83317,\"start\":83266},{\"end\":83526,\"start\":83319},{\"end\":83632,\"start\":83528},{\"end\":83685,\"start\":83634},{\"end\":83715,\"start\":83687},{\"end\":84562,\"start\":83717},{\"end\":84777,\"start\":84564},{\"end\":84813,\"start\":84779},{\"end\":85828,\"start\":84815},{\"end\":86027,\"start\":85830},{\"end\":86055,\"start\":86029},{\"end\":86254,\"start\":86057},{\"end\":86281,\"start\":86256},{\"end\":86381,\"start\":86283},{\"end\":88521,\"start\":86383},{\"end\":89295,\"start\":88523},{\"end\":90682,\"start\":89297},{\"end\":90808,\"start\":90684},{\"end\":91035,\"start\":90810},{\"end\":91152,\"start\":91037},{\"end\":93967,\"start\":91154},{\"end\":94557,\"start\":93969},{\"end\":95348,\"start\":94559},{\"end\":95696,\"start\":95350},{\"end\":96509,\"start\":95698},{\"end\":96573,\"start\":96511},{\"end\":96610,\"start\":96575},{\"end\":98968,\"start\":96612},{\"end\":99107,\"start\":98992},{\"end\":99742,\"start\":99109},{\"end\":100292,\"start\":99744},{\"end\":100813,\"start\":100294},{\"end\":105104,\"start\":100815},{\"end\":105162,\"start\":105106},{\"end\":105856,\"start\":105164},{\"end\":106087,\"start\":105858}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":17867,\"start\":17819}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":8269,\"start\":8260},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":11263,\"start\":11254},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":12575,\"start\":12566},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":14485,\"start\":14477},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":15235,\"start\":15228},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":19787,\"start\":19780},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":20084,\"start\":20075},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":24801,\"start\":24793},{\"end\":25422,\"start\":25415},{\"end\":26184,\"start\":26177},{\"end\":27767,\"start\":27760},{\"end\":29552,\"start\":29545},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":31340,\"start\":31332},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":32546,\"start\":32538},{\"end\":33676,\"start\":33669},{\"end\":34466,\"start\":34459},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":35107,\"start\":35090},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":35623,\"start\":35606},{\"end\":37012,\"start\":37005},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":37354,\"start\":37346},{\"end\":38710,\"start\":38703},{\"end\":39014,\"start\":39007},{\"end\":39386,\"start\":39379},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":41886,\"start\":41878},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":43143,\"start\":43136},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":44160,\"start\":44150},{\"end\":46665,\"start\":46658},{\"end\":46987,\"start\":46980},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":49148,\"start\":49140},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":49159,\"start\":49151},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":55961,\"start\":55953},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":58950,\"start\":58942},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":61816,\"start\":61808},{\"end\":76267,\"start\":76260},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":76414,\"start\":76406},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":76662,\"start\":76654},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":76761,\"start\":76753},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":76843,\"start\":76835},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":76932,\"start\":76924},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":77058,\"start\":77050},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":77517,\"start\":77509}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1448,\"start\":1436},{\"end\":2988,\"start\":2969},{\"end\":3257,\"start\":3248},{\"attributes\":{\"n\":\"2.\"},\"end\":7889,\"start\":7857},{\"attributes\":{\"n\":\"2.1\"},\"end\":8726,\"start\":8713},{\"attributes\":{\"n\":\"2.2\"},\"end\":10468,\"start\":10445},{\"end\":12610,\"start\":12591},{\"attributes\":{\"n\":\"2.3\"},\"end\":13194,\"start\":13169},{\"attributes\":{\"n\":\"2.4\"},\"end\":15303,\"start\":15279},{\"attributes\":{\"n\":\"3.\"},\"end\":16467,\"start\":16444},{\"attributes\":{\"n\":\"3.1\"},\"end\":17107,\"start\":17086},{\"attributes\":{\"n\":\"3.2\"},\"end\":18114,\"start\":18084},{\"attributes\":{\"n\":\"3.3\"},\"end\":18977,\"start\":18950},{\"attributes\":{\"n\":\"4.\"},\"end\":20131,\"start\":20090},{\"attributes\":{\"n\":\"4.1\"},\"end\":20948,\"start\":20930},{\"attributes\":{\"n\":\"4.2\"},\"end\":24921,\"start\":24878},{\"attributes\":{\"n\":\"4.3\"},\"end\":26923,\"start\":26883},{\"attributes\":{\"n\":\"4.4\"},\"end\":30708,\"start\":30670},{\"attributes\":{\"n\":\"4.5\"},\"end\":33447,\"start\":33398},{\"end\":34959,\"start\":34950},{\"attributes\":{\"n\":\"4.6\"},\"end\":35748,\"start\":35712},{\"attributes\":{\"n\":\"4.7\"},\"end\":38232,\"start\":38197},{\"attributes\":{\"n\":\"4.8\"},\"end\":40457,\"start\":40409},{\"attributes\":{\"n\":\"5.\"},\"end\":44589,\"start\":44575},{\"attributes\":{\"n\":\"5.1\"},\"end\":46271,\"start\":46256},{\"attributes\":{\"n\":\"5.2\"},\"end\":48295,\"start\":48268},{\"attributes\":{\"n\":\"5.3\"},\"end\":51984,\"start\":51965},{\"attributes\":{\"n\":\"5.4\"},\"end\":55037,\"start\":54997},{\"end\":56466,\"start\":56461},{\"attributes\":{\"n\":\"5.5\"},\"end\":58049,\"start\":58024},{\"attributes\":{\"n\":\"6.\"},\"end\":59647,\"start\":59621},{\"attributes\":{\"n\":\"6.1\"},\"end\":63663,\"start\":63630},{\"attributes\":{\"n\":\"6.2\"},\"end\":63677,\"start\":63666},{\"attributes\":{\"n\":\"6.3\"},\"end\":64827,\"start\":64813},{\"attributes\":{\"n\":\"7.\"},\"end\":65448,\"start\":65436},{\"attributes\":{\"n\":\"8.\"},\"end\":71001,\"start\":70990},{\"end\":72264,\"start\":72223},{\"end\":72395,\"start\":72378},{\"end\":73162,\"start\":73157},{\"end\":75531,\"start\":75499},{\"end\":75873,\"start\":75842},{\"end\":76050,\"start\":76007},{\"end\":76091,\"start\":76053},{\"end\":76482,\"start\":76420},{\"end\":77919,\"start\":77868},{\"end\":98990,\"start\":98971},{\"end\":106943,\"start\":106935},{\"end\":107708,\"start\":107698},{\"end\":107881,\"start\":107871},{\"end\":109468,\"start\":109448},{\"end\":110257,\"start\":110235},{\"end\":110833,\"start\":110822},{\"end\":111515,\"start\":111499},{\"end\":112445,\"start\":112434},{\"end\":112580,\"start\":112536},{\"end\":113583,\"start\":113572},{\"end\":113704,\"start\":113693},{\"end\":115394,\"start\":115383},{\"end\":115526,\"start\":115515},{\"end\":115650,\"start\":115639},{\"end\":115753,\"start\":115742},{\"end\":115856,\"start\":115845},{\"end\":115977,\"start\":115952},{\"end\":116376,\"start\":116365},{\"end\":116528,\"start\":116473},{\"end\":118333,\"start\":118322},{\"end\":118432,\"start\":118421},{\"end\":118606,\"start\":118584},{\"end\":119775,\"start\":119753},{\"end\":120085,\"start\":120074},{\"end\":120936,\"start\":120927},{\"end\":121294,\"start\":121285},{\"end\":121531,\"start\":121522},{\"end\":122843,\"start\":122834},{\"end\":123139,\"start\":123129},{\"end\":123231,\"start\":123221},{\"end\":123346,\"start\":123336},{\"end\":123534,\"start\":123524},{\"end\":126047,\"start\":126037},{\"end\":126239,\"start\":126229},{\"end\":126443,\"start\":126433},{\"end\":127344,\"start\":127334},{\"end\":128388,\"start\":128378},{\"end\":129254,\"start\":129244},{\"end\":138204,\"start\":138197},{\"end\":139422,\"start\":139415},{\"end\":139704,\"start\":139692}]", "table": "[{\"end\":120925,\"start\":120205},{\"end\":121520,\"start\":121423},{\"end\":121986,\"start\":121776},{\"end\":122832,\"start\":122252},{\"end\":123219,\"start\":123206},{\"end\":124233,\"start\":123988},{\"end\":126035,\"start\":124366},{\"end\":126431,\"start\":126242},{\"end\":127332,\"start\":126640},{\"end\":128376,\"start\":127347},{\"end\":129242,\"start\":128571},{\"end\":130332,\"start\":129434},{\"end\":133993,\"start\":130878},{\"end\":136226,\"start\":134388},{\"end\":138195,\"start\":137676}]", "figure_caption": "[{\"end\":106933,\"start\":106090},{\"end\":107696,\"start\":106945},{\"end\":107869,\"start\":107710},{\"end\":108018,\"start\":107883},{\"end\":109247,\"start\":108021},{\"end\":109446,\"start\":109250},{\"end\":110233,\"start\":109471},{\"end\":110820,\"start\":110262},{\"end\":111497,\"start\":110836},{\"end\":112432,\"start\":111522},{\"end\":112534,\"start\":112448},{\"end\":113570,\"start\":112589},{\"end\":113691,\"start\":113586},{\"end\":115381,\"start\":113707},{\"end\":115513,\"start\":115397},{\"end\":115637,\"start\":115529},{\"end\":115740,\"start\":115653},{\"end\":115843,\"start\":115756},{\"end\":115950,\"start\":115859},{\"end\":116363,\"start\":115983},{\"end\":116471,\"start\":116379},{\"end\":118320,\"start\":116539},{\"end\":118419,\"start\":118336},{\"end\":118582,\"start\":118435},{\"end\":119751,\"start\":118611},{\"end\":120072,\"start\":119780},{\"end\":120200,\"start\":120088},{\"end\":120205,\"start\":120203},{\"end\":121283,\"start\":120938},{\"end\":121342,\"start\":121296},{\"end\":121423,\"start\":121345},{\"end\":121649,\"start\":121533},{\"end\":121776,\"start\":121652},{\"end\":122252,\"start\":121989},{\"end\":123127,\"start\":122845},{\"end\":123206,\"start\":123142},{\"end\":123334,\"start\":123234},{\"end\":123522,\"start\":123349},{\"end\":123647,\"start\":123537},{\"end\":123988,\"start\":123650},{\"end\":124366,\"start\":124236},{\"end\":126227,\"start\":126050},{\"end\":126640,\"start\":126446},{\"end\":128571,\"start\":128391},{\"end\":129434,\"start\":129257},{\"end\":130878,\"start\":130335},{\"end\":134388,\"start\":133996},{\"end\":137082,\"start\":136229},{\"end\":137676,\"start\":137085},{\"end\":139413,\"start\":138206},{\"end\":139690,\"start\":139424},{\"end\":140343,\"start\":139709}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":4394,\"start\":4386},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":5194,\"start\":5186},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":31305,\"start\":31297},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":32511,\"start\":32503},{\"end\":34657,\"start\":34649},{\"end\":35059,\"start\":35051},{\"end\":36124,\"start\":36116},{\"end\":37306,\"start\":37298},{\"end\":42500,\"start\":42492},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":81381,\"start\":81372},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":81742,\"start\":81733},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":82152,\"start\":82143},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":82735,\"start\":82726},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":83140,\"start\":83131},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":84694,\"start\":84685},{\"end\":85421,\"start\":85414},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":85943,\"start\":85934},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":86170,\"start\":86161},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":104915,\"start\":104906},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":105742,\"start\":105733},{\"end\":106029,\"start\":106020}]", "bib_author_first_name": "[{\"end\":140803,\"start\":140796},{\"end\":140823,\"start\":140814},{\"end\":140845,\"start\":140835},{\"end\":140861,\"start\":140855},{\"end\":140879,\"start\":140874},{\"end\":140894,\"start\":140888},{\"end\":141166,\"start\":141162},{\"end\":141176,\"start\":141173},{\"end\":141191,\"start\":141186},{\"end\":141210,\"start\":141202},{\"end\":141224,\"start\":141219},{\"end\":141236,\"start\":141232},{\"end\":141250,\"start\":141245},{\"end\":141264,\"start\":141257},{\"end\":141278,\"start\":141274},{\"end\":141294,\"start\":141288},{\"end\":142220,\"start\":142215},{\"end\":142233,\"start\":142231},{\"end\":142242,\"start\":142239},{\"end\":142260,\"start\":142253},{\"end\":142273,\"start\":142266},{\"end\":142294,\"start\":142288},{\"end\":142317,\"start\":142310},{\"end\":142327,\"start\":142326},{\"end\":142338,\"start\":142334},{\"end\":142351,\"start\":142345},{\"end\":142787,\"start\":142782},{\"end\":142803,\"start\":142797},{\"end\":142818,\"start\":142813},{\"end\":142831,\"start\":142826},{\"end\":142846,\"start\":142842},{\"end\":142855,\"start\":142852},{\"end\":142868,\"start\":142866},{\"end\":142890,\"start\":142883},{\"end\":142905,\"start\":142895},{\"end\":142921,\"start\":142912},{\"end\":143325,\"start\":143324},{\"end\":143341,\"start\":143335},{\"end\":143357,\"start\":143348},{\"end\":143370,\"start\":143364},{\"end\":143382,\"start\":143377},{\"end\":143400,\"start\":143399},{\"end\":143416,\"start\":143408},{\"end\":143431,\"start\":143424},{\"end\":143453,\"start\":143445},{\"end\":143956,\"start\":143951},{\"end\":143976,\"start\":143970},{\"end\":143993,\"start\":143988},{\"end\":144007,\"start\":144002},{\"end\":144022,\"start\":144016},{\"end\":144484,\"start\":144481},{\"end\":144500,\"start\":144492},{\"end\":144511,\"start\":144507},{\"end\":144526,\"start\":144519},{\"end\":144541,\"start\":144536},{\"end\":144543,\"start\":144542},{\"end\":144560,\"start\":144552},{\"end\":144577,\"start\":144571},{\"end\":144597,\"start\":144591},{\"end\":144611,\"start\":144605},{\"end\":144626,\"start\":144620},{\"end\":144643,\"start\":144635},{\"end\":144658,\"start\":144653},{\"end\":144681,\"start\":144673},{\"end\":144694,\"start\":144691},{\"end\":144710,\"start\":144705},{\"end\":144724,\"start\":144718},{\"end\":144739,\"start\":144733},{\"end\":144756,\"start\":144749},{\"end\":144768,\"start\":144761},{\"end\":144782,\"start\":144777},{\"end\":144794,\"start\":144790},{\"end\":144805,\"start\":144801},{\"end\":144821,\"start\":144814},{\"end\":145732,\"start\":145729},{\"end\":145748,\"start\":145740},{\"end\":145759,\"start\":145755},{\"end\":145774,\"start\":145767},{\"end\":145789,\"start\":145784},{\"end\":145791,\"start\":145790},{\"end\":145808,\"start\":145800},{\"end\":145825,\"start\":145819},{\"end\":145845,\"start\":145839},{\"end\":145859,\"start\":145853},{\"end\":145874,\"start\":145868},{\"end\":146308,\"start\":146303},{\"end\":146334,\"start\":146323},{\"end\":146349,\"start\":146340},{\"end\":146362,\"start\":146357},{\"end\":146380,\"start\":146374},{\"end\":146394,\"start\":146388},{\"end\":146782,\"start\":146772},{\"end\":146795,\"start\":146792},{\"end\":146815,\"start\":146809},{\"end\":146833,\"start\":146829},{\"end\":147242,\"start\":147233},{\"end\":147260,\"start\":147254},{\"end\":147274,\"start\":147269},{\"end\":147290,\"start\":147283},{\"end\":147304,\"start\":147298},{\"end\":147317,\"start\":147313},{\"end\":147331,\"start\":147327},{\"end\":147358,\"start\":147351},{\"end\":147375,\"start\":147366},{\"end\":147825,\"start\":147822},{\"end\":147848,\"start\":147845},{\"end\":147862,\"start\":147856},{\"end\":147875,\"start\":147870},{\"end\":147889,\"start\":147884},{\"end\":148194,\"start\":148192},{\"end\":148208,\"start\":148202},{\"end\":148220,\"start\":148214},{\"end\":148232,\"start\":148230},{\"end\":148246,\"start\":148239},{\"end\":148256,\"start\":148252},{\"end\":148270,\"start\":148264},{\"end\":148282,\"start\":148275},{\"end\":148622,\"start\":148620},{\"end\":148636,\"start\":148630},{\"end\":148648,\"start\":148642},{\"end\":148660,\"start\":148658},{\"end\":148674,\"start\":148667},{\"end\":148687,\"start\":148680},{\"end\":148701,\"start\":148695},{\"end\":148713,\"start\":148706},{\"end\":148730,\"start\":148720},{\"end\":148747,\"start\":148741},{\"end\":148769,\"start\":148764},{\"end\":148786,\"start\":148780},{\"end\":148796,\"start\":148791},{\"end\":148808,\"start\":148802},{\"end\":148826,\"start\":148817},{\"end\":148837,\"start\":148833},{\"end\":148854,\"start\":148849},{\"end\":148872,\"start\":148867},{\"end\":148886,\"start\":148881},{\"end\":148903,\"start\":148897},{\"end\":148918,\"start\":148912},{\"end\":148932,\"start\":148927},{\"end\":148948,\"start\":148941},{\"end\":148960,\"start\":148953},{\"end\":148973,\"start\":148967},{\"end\":148988,\"start\":148981},{\"end\":148998,\"start\":148994},{\"end\":149005,\"start\":149003},{\"end\":149007,\"start\":149006},{\"end\":149020,\"start\":149016},{\"end\":149031,\"start\":149026},{\"end\":149042,\"start\":149038},{\"end\":149056,\"start\":149051},{\"end\":149070,\"start\":149066},{\"end\":149072,\"start\":149071},{\"end\":149084,\"start\":149079},{\"end\":149730,\"start\":149726},{\"end\":149744,\"start\":149738},{\"end\":149763,\"start\":149755},{\"end\":149779,\"start\":149774},{\"end\":149797,\"start\":149788},{\"end\":149817,\"start\":149806},{\"end\":149829,\"start\":149825},{\"end\":150183,\"start\":150178},{\"end\":150200,\"start\":150195},{\"end\":150526,\"start\":150521},{\"end\":150541,\"start\":150534},{\"end\":150561,\"start\":150553},{\"end\":150579,\"start\":150570},{\"end\":150581,\"start\":150580},{\"end\":150594,\"start\":150590},{\"end\":150608,\"start\":150604},{\"end\":150623,\"start\":150618},{\"end\":150637,\"start\":150631},{\"end\":150651,\"start\":150645},{\"end\":150664,\"start\":150660},{\"end\":150678,\"start\":150671},{\"end\":150695,\"start\":150691},{\"end\":150697,\"start\":150696},{\"end\":150714,\"start\":150705},{\"end\":150716,\"start\":150715},{\"end\":150732,\"start\":150727},{\"end\":150734,\"start\":150733},{\"end\":150751,\"start\":150745},{\"end\":150767,\"start\":150760},{\"end\":150769,\"start\":150768},{\"end\":150784,\"start\":150779},{\"end\":151290,\"start\":151285},{\"end\":151305,\"start\":151298},{\"end\":151318,\"start\":151314},{\"end\":151334,\"start\":151328},{\"end\":151347,\"start\":151340},{\"end\":151359,\"start\":151354},{\"end\":151858,\"start\":151855},{\"end\":151870,\"start\":151864},{\"end\":151884,\"start\":151881},{\"end\":151900,\"start\":151892},{\"end\":151916,\"start\":151910},{\"end\":151931,\"start\":151924},{\"end\":151945,\"start\":151940},{\"end\":151959,\"start\":151953},{\"end\":151969,\"start\":151964},{\"end\":151980,\"start\":151977},{\"end\":152356,\"start\":152350},{\"end\":152379,\"start\":152372},{\"end\":152397,\"start\":152392},{\"end\":152409,\"start\":152403},{\"end\":152849,\"start\":152846},{\"end\":152862,\"start\":152856},{\"end\":152877,\"start\":152873},{\"end\":152891,\"start\":152885},{\"end\":152901,\"start\":152898},{\"end\":152916,\"start\":152908},{\"end\":153303,\"start\":153300},{\"end\":153321,\"start\":153315},{\"end\":153335,\"start\":153329},{\"end\":153348,\"start\":153344},{\"end\":153360,\"start\":153354},{\"end\":153374,\"start\":153370},{\"end\":153386,\"start\":153381},{\"end\":153727,\"start\":153724},{\"end\":153745,\"start\":153739},{\"end\":153759,\"start\":153753},{\"end\":153772,\"start\":153768},{\"end\":153784,\"start\":153778},{\"end\":153798,\"start\":153794},{\"end\":153810,\"start\":153805},{\"end\":154274,\"start\":154271},{\"end\":154292,\"start\":154286},{\"end\":154306,\"start\":154300},{\"end\":154321,\"start\":154317},{\"end\":154335,\"start\":154329},{\"end\":154348,\"start\":154344},{\"end\":154359,\"start\":154355},{\"end\":154371,\"start\":154366},{\"end\":154633,\"start\":154626},{\"end\":154648,\"start\":154643},{\"end\":154654,\"start\":154649},{\"end\":154672,\"start\":154664},{\"end\":154689,\"start\":154683},{\"end\":154708,\"start\":154702},{\"end\":154726,\"start\":154718},{\"end\":154742,\"start\":154739},{\"end\":155034,\"start\":155028},{\"end\":155050,\"start\":155045},{\"end\":155062,\"start\":155056},{\"end\":155075,\"start\":155069},{\"end\":155093,\"start\":155087},{\"end\":155108,\"start\":155103},{\"end\":155124,\"start\":155116},{\"end\":155558,\"start\":155552},{\"end\":155570,\"start\":155565},{\"end\":155584,\"start\":155578},{\"end\":155600,\"start\":155595},{\"end\":155615,\"start\":155609},{\"end\":155872,\"start\":155871},{\"end\":155888,\"start\":155883},{\"end\":156099,\"start\":156092},{\"end\":156113,\"start\":156108},{\"end\":156130,\"start\":156124},{\"end\":156141,\"start\":156135},{\"end\":156154,\"start\":156148},{\"end\":156422,\"start\":156415},{\"end\":156436,\"start\":156432},{\"end\":156451,\"start\":156446},{\"end\":157302,\"start\":157294},{\"end\":157313,\"start\":157310},{\"end\":157324,\"start\":157321},{\"end\":157521,\"start\":157516},{\"end\":157533,\"start\":157529},{\"end\":157546,\"start\":157541},{\"end\":157562,\"start\":157557},{\"end\":157575,\"start\":157570},{\"end\":157592,\"start\":157586},{\"end\":157610,\"start\":157604},{\"end\":157624,\"start\":157618},{\"end\":157640,\"start\":157633},{\"end\":158430,\"start\":158423},{\"end\":158447,\"start\":158437},{\"end\":158463,\"start\":158458},{\"end\":158479,\"start\":158473},{\"end\":158493,\"start\":158487},{\"end\":158513,\"start\":158500},{\"end\":158529,\"start\":158522},{\"end\":158938,\"start\":158933},{\"end\":158951,\"start\":158947},{\"end\":158965,\"start\":158961},{\"end\":159276,\"start\":159271},{\"end\":159289,\"start\":159284},{\"end\":159305,\"start\":159301},{\"end\":159319,\"start\":159311},{\"end\":159335,\"start\":159329},{\"end\":159352,\"start\":159343},{\"end\":159367,\"start\":159363},{\"end\":159381,\"start\":159375},{\"end\":159399,\"start\":159393},{\"end\":159410,\"start\":159404},{\"end\":159784,\"start\":159780},{\"end\":159795,\"start\":159791},{\"end\":159811,\"start\":159806},{\"end\":159822,\"start\":159818},{\"end\":160014,\"start\":160008},{\"end\":160024,\"start\":160020},{\"end\":160035,\"start\":160030},{\"end\":160050,\"start\":160043},{\"end\":160061,\"start\":160055},{\"end\":160063,\"start\":160062},{\"end\":160076,\"start\":160071},{\"end\":160087,\"start\":160083},{\"end\":160098,\"start\":160094},{\"end\":160110,\"start\":160106},{\"end\":160112,\"start\":160111},{\"end\":160133,\"start\":160126},{\"end\":160563,\"start\":160560},{\"end\":160571,\"start\":160568},{\"end\":160589,\"start\":160581},{\"end\":160606,\"start\":160597},{\"end\":160621,\"start\":160615},{\"end\":160981,\"start\":160976},{\"end\":160996,\"start\":160990},{\"end\":161020,\"start\":161013},{\"end\":161035,\"start\":161028},{\"end\":161315,\"start\":161308},{\"end\":161336,\"start\":161330},{\"end\":161350,\"start\":161345},{\"end\":161365,\"start\":161358},{\"end\":161379,\"start\":161374},{\"end\":161392,\"start\":161387},{\"end\":161406,\"start\":161401},{\"end\":161424,\"start\":161417},{\"end\":161441,\"start\":161434},{\"end\":161770,\"start\":161765},{\"end\":161780,\"start\":161776},{\"end\":161792,\"start\":161788},{\"end\":161814,\"start\":161806},{\"end\":162123,\"start\":162118},{\"end\":162134,\"start\":162129},{\"end\":162143,\"start\":162140},{\"end\":162159,\"start\":162154},{\"end\":162173,\"start\":162169},{\"end\":162189,\"start\":162181},{\"end\":162206,\"start\":162202},{\"end\":162479,\"start\":162472},{\"end\":162493,\"start\":162485},{\"end\":162504,\"start\":162501},{\"end\":162517,\"start\":162512},{\"end\":162531,\"start\":162523},{\"end\":162552,\"start\":162544},{\"end\":162567,\"start\":162560},{\"end\":162579,\"start\":162574},{\"end\":162590,\"start\":162586},{\"end\":162604,\"start\":162598},{\"end\":163043,\"start\":163037},{\"end\":163057,\"start\":163052},{\"end\":163071,\"start\":163065},{\"end\":163088,\"start\":163082},{\"end\":163635,\"start\":163628},{\"end\":163647,\"start\":163641},{\"end\":163653,\"start\":163648},{\"end\":163669,\"start\":163666},{\"end\":163685,\"start\":163679},{\"end\":163704,\"start\":163699},{\"end\":163718,\"start\":163713},{\"end\":163732,\"start\":163727},{\"end\":163745,\"start\":163740},{\"end\":163764,\"start\":163757},{\"end\":163777,\"start\":163772},{\"end\":164170,\"start\":164162},{\"end\":164178,\"start\":164177},{\"end\":164180,\"start\":164179},{\"end\":164196,\"start\":164190},{\"end\":164208,\"start\":164204},{\"end\":164428,\"start\":164424},{\"end\":164441,\"start\":164437},{\"end\":164448,\"start\":164446},{\"end\":164461,\"start\":164456},{\"end\":164472,\"start\":164471},{\"end\":164488,\"start\":164482},{\"end\":164506,\"start\":164501},{\"end\":164524,\"start\":164516},{\"end\":164540,\"start\":164532},{\"end\":164554,\"start\":164550},{\"end\":165023,\"start\":165019},{\"end\":165041,\"start\":165032},{\"end\":165056,\"start\":165050},{\"end\":165068,\"start\":165067},{\"end\":165075,\"start\":165069},{\"end\":165085,\"start\":165080},{\"end\":165100,\"start\":165095},{\"end\":165107,\"start\":165101},{\"end\":165123,\"start\":165115},{\"end\":165531,\"start\":165527},{\"end\":165545,\"start\":165541},{\"end\":165550,\"start\":165546},{\"end\":165560,\"start\":165556},{\"end\":165796,\"start\":165791},{\"end\":165809,\"start\":165805},{\"end\":165823,\"start\":165819},{\"end\":165842,\"start\":165833},{\"end\":165854,\"start\":165848},{\"end\":165870,\"start\":165863},{\"end\":165884,\"start\":165879},{\"end\":165894,\"start\":165891},{\"end\":165906,\"start\":165899},{\"end\":166296,\"start\":166291},{\"end\":166327,\"start\":166320},{\"end\":166343,\"start\":166336},{\"end\":166847,\"start\":166841},{\"end\":166863,\"start\":166859},{\"end\":166881,\"start\":166871},{\"end\":166896,\"start\":166891},{\"end\":167252,\"start\":167246},{\"end\":167269,\"start\":167264},{\"end\":167280,\"start\":167275},{\"end\":167987,\"start\":167983},{\"end\":168000,\"start\":167995},{\"end\":168020,\"start\":168007},{\"end\":168367,\"start\":168360},{\"end\":168386,\"start\":168378},{\"end\":168720,\"start\":168713},{\"end\":168734,\"start\":168729},{\"end\":168747,\"start\":168742},{\"end\":168757,\"start\":168755},{\"end\":168766,\"start\":168762},{\"end\":168785,\"start\":168779},{\"end\":168795,\"start\":168791},{\"end\":168804,\"start\":168800},{\"end\":168814,\"start\":168810},{\"end\":168828,\"start\":168824},{\"end\":168830,\"start\":168829},{\"end\":169175,\"start\":169169},{\"end\":169191,\"start\":169186},{\"end\":169209,\"start\":169204},{\"end\":169227,\"start\":169219},{\"end\":169701,\"start\":169695},{\"end\":169714,\"start\":169708},{\"end\":169728,\"start\":169723},{\"end\":169744,\"start\":169737},{\"end\":169758,\"start\":169751},{\"end\":169773,\"start\":169769},{\"end\":169791,\"start\":169784},{\"end\":169807,\"start\":169801},{\"end\":169822,\"start\":169818},{\"end\":169834,\"start\":169829},{\"end\":169846,\"start\":169840},{\"end\":169859,\"start\":169853},{\"end\":169870,\"start\":169864},{\"end\":169885,\"start\":169880},{\"end\":169908,\"start\":169901},{\"end\":169926,\"start\":169920},{\"end\":169937,\"start\":169932},{\"end\":169958,\"start\":169949},{\"end\":169974,\"start\":169966},{\"end\":169986,\"start\":169982},{\"end\":170008,\"start\":170005},{\"end\":170022,\"start\":170016},{\"end\":170034,\"start\":170029},{\"end\":171220,\"start\":171214},{\"end\":171235,\"start\":171230},{\"end\":171257,\"start\":171249},{\"end\":171647,\"start\":171639},{\"end\":171664,\"start\":171657},{\"end\":171678,\"start\":171674},{\"end\":171692,\"start\":171685},{\"end\":171709,\"start\":171704},{\"end\":171723,\"start\":171718},{\"end\":172085,\"start\":172081},{\"end\":172099,\"start\":172095},{\"end\":172114,\"start\":172109},{\"end\":172128,\"start\":172122},{\"end\":172141,\"start\":172136},{\"end\":172714,\"start\":172710},{\"end\":172728,\"start\":172724},{\"end\":172740,\"start\":172733},{\"end\":172752,\"start\":172750},{\"end\":172761,\"start\":172757},{\"end\":172769,\"start\":172762},{\"end\":172784,\"start\":172777},{\"end\":172798,\"start\":172793},{\"end\":172808,\"start\":172804},{\"end\":172821,\"start\":172815},{\"end\":172835,\"start\":172829},{\"end\":173327,\"start\":173323},{\"end\":173335,\"start\":173328},{\"end\":173347,\"start\":173343},{\"end\":173364,\"start\":173360},{\"end\":173379,\"start\":173374},{\"end\":173393,\"start\":173388},{\"end\":174339,\"start\":174333},{\"end\":174359,\"start\":174352},{\"end\":174377,\"start\":174369},{\"end\":174394,\"start\":174383},{\"end\":174410,\"start\":174402},{\"end\":174421,\"start\":174417},{\"end\":174430,\"start\":174429},{\"end\":174441,\"start\":174437},{\"end\":174455,\"start\":174449},{\"end\":174470,\"start\":174465},{\"end\":174903,\"start\":174898},{\"end\":174918,\"start\":174912},{\"end\":174936,\"start\":174927},{\"end\":174955,\"start\":174946},{\"end\":174968,\"start\":174966},{\"end\":174994,\"start\":174985},{\"end\":175014,\"start\":175013},{\"end\":175023,\"start\":175021},{\"end\":175025,\"start\":175024},{\"end\":175035,\"start\":175030},{\"end\":175350,\"start\":175344},{\"end\":175365,\"start\":175360},{\"end\":175713,\"start\":175708},{\"end\":175731,\"start\":175725},{\"end\":175734,\"start\":175732},{\"end\":175749,\"start\":175744},{\"end\":175760,\"start\":175756},{\"end\":175776,\"start\":175770},{\"end\":175807,\"start\":175801},{\"end\":175821,\"start\":175815},{\"end\":175833,\"start\":175827},{\"end\":175841,\"start\":175839},{\"end\":176227,\"start\":176223},{\"end\":176243,\"start\":176237},{\"end\":176258,\"start\":176249},{\"end\":176277,\"start\":176271},{\"end\":176290,\"start\":176284},{\"end\":176304,\"start\":176299},{\"end\":176315,\"start\":176312},{\"end\":176334,\"start\":176329},{\"end\":176348,\"start\":176342},{\"end\":176361,\"start\":176356},{\"end\":177395,\"start\":177393},{\"end\":177404,\"start\":177400},{\"end\":177421,\"start\":177411},{\"end\":177444,\"start\":177434},{\"end\":177458,\"start\":177454},{\"end\":177476,\"start\":177470},{\"end\":177504,\"start\":177495},{\"end\":177516,\"start\":177511},{\"end\":177827,\"start\":177819},{\"end\":177843,\"start\":177834},{\"end\":177854,\"start\":177851},{\"end\":177869,\"start\":177862},{\"end\":177878,\"start\":177874},{\"end\":178350,\"start\":178343},{\"end\":178364,\"start\":178357},{\"end\":178378,\"start\":178373},{\"end\":178404,\"start\":178397},{\"end\":178420,\"start\":178412},{\"end\":178436,\"start\":178430},{\"end\":178453,\"start\":178448},{\"end\":178465,\"start\":178461},{\"end\":178494,\"start\":178487},{\"end\":178506,\"start\":178501},{\"end\":178519,\"start\":178513},{\"end\":178535,\"start\":178528},{\"end\":178555,\"start\":178551},{\"end\":178569,\"start\":178564},{\"end\":178581,\"start\":178575},{\"end\":178596,\"start\":178591},{\"end\":178610,\"start\":178605},{\"end\":178626,\"start\":178621},{\"end\":178643,\"start\":178635},{\"end\":179893,\"start\":179888},{\"end\":179906,\"start\":179899},{\"end\":179921,\"start\":179914},{\"end\":179934,\"start\":179928},{\"end\":179945,\"start\":179940},{\"end\":179949,\"start\":179946},{\"end\":179959,\"start\":179954},{\"end\":179971,\"start\":179968},{\"end\":179982,\"start\":179976},{\"end\":179984,\"start\":179983},{\"end\":179996,\"start\":179990},{\"end\":180380,\"start\":180375},{\"end\":180392,\"start\":180386},{\"end\":180403,\"start\":180399},{\"end\":180423,\"start\":180416},{\"end\":180433,\"start\":180431},{\"end\":180443,\"start\":180439},{\"end\":180453,\"start\":180448},{\"end\":180781,\"start\":180776},{\"end\":180795,\"start\":180788},{\"end\":180813,\"start\":180809},{\"end\":180826,\"start\":180822},{\"end\":180839,\"start\":180834},{\"end\":180853,\"start\":180847},{\"end\":180873,\"start\":180869},{\"end\":180883,\"start\":180878},{\"end\":181815,\"start\":181808},{\"end\":181825,\"start\":181821},{\"end\":181831,\"start\":181826},{\"end\":181840,\"start\":181836},{\"end\":181851,\"start\":181846},{\"end\":181866,\"start\":181859},{\"end\":181885,\"start\":181876},{\"end\":181907,\"start\":181896},{\"end\":181916,\"start\":181912},{\"end\":181933,\"start\":181924},{\"end\":181940,\"start\":181939},{\"end\":182438,\"start\":182434},{\"end\":182449,\"start\":182443},{\"end\":182462,\"start\":182457},{\"end\":183294,\"start\":183287},{\"end\":183304,\"start\":183299},{\"end\":183661,\"start\":183654},{\"end\":183670,\"start\":183667},{\"end\":183682,\"start\":183675},{\"end\":184530,\"start\":184527},{\"end\":184538,\"start\":184535},{\"end\":184549,\"start\":184546},{\"end\":184565,\"start\":184556},{\"end\":184582,\"start\":184576},{\"end\":184594,\"start\":184589},{\"end\":184604,\"start\":184599},{\"end\":184614,\"start\":184609},{\"end\":184627,\"start\":184619},{\"end\":184641,\"start\":184633},{\"end\":185070,\"start\":185062},{\"end\":185084,\"start\":185078},{\"end\":185098,\"start\":185091},{\"end\":185630,\"start\":185625},{\"end\":185645,\"start\":185638},{\"end\":185659,\"start\":185654},{\"end\":185672,\"start\":185667},{\"end\":185686,\"start\":185682},{\"end\":185700,\"start\":185693},{\"end\":185718,\"start\":185707},{\"end\":185730,\"start\":185726},{\"end\":185741,\"start\":185737},{\"end\":185748,\"start\":185746}]", "bib_author_last_name": "[{\"end\":140812,\"start\":140804},{\"end\":140833,\"start\":140824},{\"end\":140853,\"start\":140846},{\"end\":140872,\"start\":140862},{\"end\":140886,\"start\":140880},{\"end\":140899,\"start\":140895},{\"end\":141171,\"start\":141167},{\"end\":141184,\"start\":141177},{\"end\":141200,\"start\":141192},{\"end\":141217,\"start\":141211},{\"end\":141230,\"start\":141225},{\"end\":141243,\"start\":141237},{\"end\":141255,\"start\":141251},{\"end\":141272,\"start\":141265},{\"end\":141286,\"start\":141279},{\"end\":141302,\"start\":141295},{\"end\":141826,\"start\":141817},{\"end\":142229,\"start\":142221},{\"end\":142237,\"start\":142234},{\"end\":142251,\"start\":142243},{\"end\":142264,\"start\":142261},{\"end\":142286,\"start\":142274},{\"end\":142308,\"start\":142295},{\"end\":142324,\"start\":142318},{\"end\":142332,\"start\":142328},{\"end\":142343,\"start\":142339},{\"end\":142357,\"start\":142352},{\"end\":142361,\"start\":142359},{\"end\":142795,\"start\":142788},{\"end\":142811,\"start\":142804},{\"end\":142824,\"start\":142819},{\"end\":142840,\"start\":142832},{\"end\":142850,\"start\":142847},{\"end\":142864,\"start\":142856},{\"end\":142881,\"start\":142869},{\"end\":142893,\"start\":142891},{\"end\":142910,\"start\":142906},{\"end\":142930,\"start\":142922},{\"end\":143333,\"start\":143326},{\"end\":143346,\"start\":143342},{\"end\":143362,\"start\":143358},{\"end\":143375,\"start\":143371},{\"end\":143389,\"start\":143383},{\"end\":143397,\"start\":143391},{\"end\":143406,\"start\":143401},{\"end\":143422,\"start\":143417},{\"end\":143438,\"start\":143432},{\"end\":143443,\"start\":143440},{\"end\":143458,\"start\":143454},{\"end\":143465,\"start\":143460},{\"end\":143968,\"start\":143957},{\"end\":143986,\"start\":143977},{\"end\":144000,\"start\":143994},{\"end\":144014,\"start\":144008},{\"end\":144032,\"start\":144023},{\"end\":144490,\"start\":144485},{\"end\":144505,\"start\":144501},{\"end\":144517,\"start\":144512},{\"end\":144534,\"start\":144527},{\"end\":144550,\"start\":144544},{\"end\":144569,\"start\":144561},{\"end\":144589,\"start\":144578},{\"end\":144603,\"start\":144598},{\"end\":144618,\"start\":144612},{\"end\":144633,\"start\":144627},{\"end\":144651,\"start\":144644},{\"end\":144671,\"start\":144659},{\"end\":144689,\"start\":144682},{\"end\":144703,\"start\":144695},{\"end\":144716,\"start\":144711},{\"end\":144731,\"start\":144725},{\"end\":144747,\"start\":144740},{\"end\":144759,\"start\":144757},{\"end\":144775,\"start\":144769},{\"end\":144788,\"start\":144783},{\"end\":144799,\"start\":144795},{\"end\":144812,\"start\":144806},{\"end\":144828,\"start\":144822},{\"end\":145738,\"start\":145733},{\"end\":145753,\"start\":145749},{\"end\":145765,\"start\":145760},{\"end\":145782,\"start\":145775},{\"end\":145798,\"start\":145792},{\"end\":145817,\"start\":145809},{\"end\":145837,\"start\":145826},{\"end\":145851,\"start\":145846},{\"end\":145866,\"start\":145860},{\"end\":145881,\"start\":145875},{\"end\":146321,\"start\":146309},{\"end\":146338,\"start\":146335},{\"end\":146355,\"start\":146350},{\"end\":146372,\"start\":146363},{\"end\":146386,\"start\":146381},{\"end\":146408,\"start\":146395},{\"end\":146415,\"start\":146410},{\"end\":146790,\"start\":146783},{\"end\":146807,\"start\":146796},{\"end\":146827,\"start\":146816},{\"end\":146841,\"start\":146834},{\"end\":147252,\"start\":147243},{\"end\":147267,\"start\":147261},{\"end\":147281,\"start\":147275},{\"end\":147296,\"start\":147291},{\"end\":147311,\"start\":147305},{\"end\":147325,\"start\":147318},{\"end\":147338,\"start\":147332},{\"end\":147349,\"start\":147340},{\"end\":147364,\"start\":147359},{\"end\":147382,\"start\":147376},{\"end\":147392,\"start\":147384},{\"end\":147843,\"start\":147826},{\"end\":147854,\"start\":147849},{\"end\":147868,\"start\":147863},{\"end\":147882,\"start\":147876},{\"end\":147894,\"start\":147890},{\"end\":147902,\"start\":147896},{\"end\":148190,\"start\":148181},{\"end\":148200,\"start\":148195},{\"end\":148212,\"start\":148209},{\"end\":148228,\"start\":148221},{\"end\":148237,\"start\":148233},{\"end\":148250,\"start\":148247},{\"end\":148262,\"start\":148257},{\"end\":148273,\"start\":148271},{\"end\":148287,\"start\":148283},{\"end\":148297,\"start\":148289},{\"end\":148618,\"start\":148609},{\"end\":148628,\"start\":148623},{\"end\":148640,\"start\":148637},{\"end\":148656,\"start\":148649},{\"end\":148665,\"start\":148661},{\"end\":148678,\"start\":148675},{\"end\":148693,\"start\":148688},{\"end\":148704,\"start\":148702},{\"end\":148718,\"start\":148714},{\"end\":148739,\"start\":148731},{\"end\":148754,\"start\":148748},{\"end\":148762,\"start\":148756},{\"end\":148778,\"start\":148770},{\"end\":148789,\"start\":148787},{\"end\":148800,\"start\":148797},{\"end\":148815,\"start\":148809},{\"end\":148831,\"start\":148827},{\"end\":148847,\"start\":148838},{\"end\":148865,\"start\":148855},{\"end\":148879,\"start\":148873},{\"end\":148895,\"start\":148887},{\"end\":148910,\"start\":148904},{\"end\":148925,\"start\":148919},{\"end\":148939,\"start\":148933},{\"end\":148951,\"start\":148949},{\"end\":148965,\"start\":148961},{\"end\":148979,\"start\":148974},{\"end\":148992,\"start\":148989},{\"end\":149001,\"start\":148999},{\"end\":149014,\"start\":149008},{\"end\":149024,\"start\":149021},{\"end\":149036,\"start\":149032},{\"end\":149049,\"start\":149043},{\"end\":149064,\"start\":149057},{\"end\":149077,\"start\":149073},{\"end\":149087,\"start\":149085},{\"end\":149092,\"start\":149089},{\"end\":149736,\"start\":149731},{\"end\":149753,\"start\":149745},{\"end\":149772,\"start\":149764},{\"end\":149786,\"start\":149780},{\"end\":149804,\"start\":149798},{\"end\":149823,\"start\":149818},{\"end\":149838,\"start\":149830},{\"end\":150193,\"start\":150184},{\"end\":150207,\"start\":150201},{\"end\":150532,\"start\":150527},{\"end\":150551,\"start\":150542},{\"end\":150568,\"start\":150562},{\"end\":150588,\"start\":150582},{\"end\":150602,\"start\":150595},{\"end\":150616,\"start\":150609},{\"end\":150629,\"start\":150624},{\"end\":150643,\"start\":150638},{\"end\":150658,\"start\":150652},{\"end\":150669,\"start\":150665},{\"end\":150689,\"start\":150679},{\"end\":150703,\"start\":150698},{\"end\":150725,\"start\":150717},{\"end\":150743,\"start\":150735},{\"end\":150758,\"start\":150752},{\"end\":150777,\"start\":150770},{\"end\":150791,\"start\":150785},{\"end\":151296,\"start\":151291},{\"end\":151312,\"start\":151306},{\"end\":151326,\"start\":151319},{\"end\":151338,\"start\":151335},{\"end\":151352,\"start\":151348},{\"end\":151366,\"start\":151360},{\"end\":151862,\"start\":151859},{\"end\":151879,\"start\":151871},{\"end\":151890,\"start\":151885},{\"end\":151908,\"start\":151901},{\"end\":151922,\"start\":151917},{\"end\":151938,\"start\":151932},{\"end\":151951,\"start\":151946},{\"end\":151962,\"start\":151960},{\"end\":151975,\"start\":151970},{\"end\":151990,\"start\":151981},{\"end\":152370,\"start\":152357},{\"end\":152390,\"start\":152380},{\"end\":152401,\"start\":152398},{\"end\":152414,\"start\":152410},{\"end\":152421,\"start\":152416},{\"end\":152854,\"start\":152850},{\"end\":152871,\"start\":152863},{\"end\":152883,\"start\":152878},{\"end\":152896,\"start\":152892},{\"end\":152906,\"start\":152902},{\"end\":152923,\"start\":152917},{\"end\":153313,\"start\":153304},{\"end\":153327,\"start\":153322},{\"end\":153342,\"start\":153336},{\"end\":153352,\"start\":153349},{\"end\":153368,\"start\":153361},{\"end\":153379,\"start\":153375},{\"end\":153397,\"start\":153387},{\"end\":153737,\"start\":153728},{\"end\":153751,\"start\":153746},{\"end\":153766,\"start\":153760},{\"end\":153776,\"start\":153773},{\"end\":153792,\"start\":153785},{\"end\":153803,\"start\":153799},{\"end\":153821,\"start\":153811},{\"end\":154284,\"start\":154275},{\"end\":154298,\"start\":154293},{\"end\":154315,\"start\":154307},{\"end\":154327,\"start\":154322},{\"end\":154342,\"start\":154336},{\"end\":154353,\"start\":154349},{\"end\":154364,\"start\":154360},{\"end\":154382,\"start\":154372},{\"end\":154641,\"start\":154634},{\"end\":154662,\"start\":154655},{\"end\":154681,\"start\":154673},{\"end\":154700,\"start\":154690},{\"end\":154716,\"start\":154709},{\"end\":154737,\"start\":154727},{\"end\":154750,\"start\":154743},{\"end\":155043,\"start\":155035},{\"end\":155054,\"start\":155051},{\"end\":155067,\"start\":155063},{\"end\":155085,\"start\":155076},{\"end\":155101,\"start\":155094},{\"end\":155114,\"start\":155109},{\"end\":155135,\"start\":155125},{\"end\":155563,\"start\":155559},{\"end\":155576,\"start\":155571},{\"end\":155593,\"start\":155585},{\"end\":155607,\"start\":155601},{\"end\":155625,\"start\":155616},{\"end\":155881,\"start\":155873},{\"end\":155895,\"start\":155889},{\"end\":155899,\"start\":155897},{\"end\":156106,\"start\":156100},{\"end\":156122,\"start\":156114},{\"end\":156133,\"start\":156131},{\"end\":156146,\"start\":156142},{\"end\":156161,\"start\":156155},{\"end\":156170,\"start\":156163},{\"end\":156430,\"start\":156423},{\"end\":156444,\"start\":156437},{\"end\":156458,\"start\":156452},{\"end\":157308,\"start\":157303},{\"end\":157319,\"start\":157314},{\"end\":157527,\"start\":157522},{\"end\":157539,\"start\":157534},{\"end\":157555,\"start\":157547},{\"end\":157568,\"start\":157563},{\"end\":157584,\"start\":157576},{\"end\":157602,\"start\":157593},{\"end\":157616,\"start\":157611},{\"end\":157631,\"start\":157625},{\"end\":157647,\"start\":157641},{\"end\":158435,\"start\":158431},{\"end\":158456,\"start\":158448},{\"end\":158471,\"start\":158464},{\"end\":158485,\"start\":158480},{\"end\":158498,\"start\":158494},{\"end\":158520,\"start\":158514},{\"end\":158537,\"start\":158530},{\"end\":158945,\"start\":158939},{\"end\":158959,\"start\":158952},{\"end\":158974,\"start\":158966},{\"end\":159282,\"start\":159277},{\"end\":159299,\"start\":159290},{\"end\":159309,\"start\":159306},{\"end\":159327,\"start\":159320},{\"end\":159341,\"start\":159336},{\"end\":159361,\"start\":159353},{\"end\":159373,\"start\":159368},{\"end\":159391,\"start\":159382},{\"end\":159402,\"start\":159400},{\"end\":159416,\"start\":159411},{\"end\":159789,\"start\":159785},{\"end\":159804,\"start\":159796},{\"end\":159816,\"start\":159812},{\"end\":159830,\"start\":159823},{\"end\":160018,\"start\":160015},{\"end\":160028,\"start\":160025},{\"end\":160041,\"start\":160036},{\"end\":160053,\"start\":160051},{\"end\":160069,\"start\":160064},{\"end\":160081,\"start\":160077},{\"end\":160092,\"start\":160088},{\"end\":160104,\"start\":160099},{\"end\":160124,\"start\":160113},{\"end\":160142,\"start\":160134},{\"end\":160151,\"start\":160144},{\"end\":160566,\"start\":160564},{\"end\":160579,\"start\":160572},{\"end\":160595,\"start\":160590},{\"end\":160613,\"start\":160607},{\"end\":160631,\"start\":160622},{\"end\":160988,\"start\":160982},{\"end\":161011,\"start\":160997},{\"end\":161026,\"start\":161021},{\"end\":161042,\"start\":161036},{\"end\":161328,\"start\":161316},{\"end\":161343,\"start\":161337},{\"end\":161356,\"start\":161351},{\"end\":161372,\"start\":161366},{\"end\":161385,\"start\":161380},{\"end\":161399,\"start\":161393},{\"end\":161415,\"start\":161407},{\"end\":161432,\"start\":161425},{\"end\":161450,\"start\":161442},{\"end\":161774,\"start\":161771},{\"end\":161786,\"start\":161781},{\"end\":161804,\"start\":161793},{\"end\":161825,\"start\":161815},{\"end\":161834,\"start\":161827},{\"end\":162127,\"start\":162124},{\"end\":162138,\"start\":162135},{\"end\":162152,\"start\":162144},{\"end\":162167,\"start\":162160},{\"end\":162179,\"start\":162174},{\"end\":162200,\"start\":162190},{\"end\":162218,\"start\":162207},{\"end\":162483,\"start\":162480},{\"end\":162499,\"start\":162494},{\"end\":162510,\"start\":162505},{\"end\":162521,\"start\":162518},{\"end\":162542,\"start\":162532},{\"end\":162558,\"start\":162553},{\"end\":162572,\"start\":162568},{\"end\":162584,\"start\":162580},{\"end\":162596,\"start\":162591},{\"end\":162612,\"start\":162605},{\"end\":163050,\"start\":163044},{\"end\":163063,\"start\":163058},{\"end\":163080,\"start\":163072},{\"end\":163095,\"start\":163089},{\"end\":163639,\"start\":163636},{\"end\":163664,\"start\":163654},{\"end\":163677,\"start\":163670},{\"end\":163697,\"start\":163686},{\"end\":163711,\"start\":163705},{\"end\":163725,\"start\":163719},{\"end\":163738,\"start\":163733},{\"end\":163755,\"start\":163746},{\"end\":163770,\"start\":163765},{\"end\":163782,\"start\":163778},{\"end\":164175,\"start\":164171},{\"end\":164188,\"start\":164181},{\"end\":164202,\"start\":164197},{\"end\":164213,\"start\":164209},{\"end\":164222,\"start\":164215},{\"end\":164435,\"start\":164429},{\"end\":164444,\"start\":164442},{\"end\":164454,\"start\":164449},{\"end\":164469,\"start\":164462},{\"end\":164480,\"start\":164473},{\"end\":164499,\"start\":164489},{\"end\":164514,\"start\":164507},{\"end\":164530,\"start\":164525},{\"end\":164548,\"start\":164541},{\"end\":164560,\"start\":164555},{\"end\":164565,\"start\":164562},{\"end\":165030,\"start\":165024},{\"end\":165048,\"start\":165042},{\"end\":165065,\"start\":165057},{\"end\":165078,\"start\":165076},{\"end\":165093,\"start\":165086},{\"end\":165113,\"start\":165108},{\"end\":165133,\"start\":165124},{\"end\":165539,\"start\":165532},{\"end\":165554,\"start\":165551},{\"end\":165563,\"start\":165561},{\"end\":165803,\"start\":165797},{\"end\":165817,\"start\":165810},{\"end\":165831,\"start\":165824},{\"end\":165846,\"start\":165843},{\"end\":165861,\"start\":165855},{\"end\":165877,\"start\":165871},{\"end\":165889,\"start\":165885},{\"end\":165897,\"start\":165895},{\"end\":165910,\"start\":165907},{\"end\":166318,\"start\":166297},{\"end\":166334,\"start\":166328},{\"end\":166349,\"start\":166344},{\"end\":166357,\"start\":166351},{\"end\":166857,\"start\":166848},{\"end\":166869,\"start\":166864},{\"end\":166889,\"start\":166882},{\"end\":166902,\"start\":166897},{\"end\":167262,\"start\":167253},{\"end\":167273,\"start\":167270},{\"end\":167286,\"start\":167281},{\"end\":167993,\"start\":167988},{\"end\":168005,\"start\":168001},{\"end\":168028,\"start\":168021},{\"end\":168376,\"start\":168368},{\"end\":168406,\"start\":168387},{\"end\":168414,\"start\":168408},{\"end\":168727,\"start\":168721},{\"end\":168740,\"start\":168735},{\"end\":168753,\"start\":168748},{\"end\":168760,\"start\":168758},{\"end\":168777,\"start\":168767},{\"end\":168789,\"start\":168786},{\"end\":168798,\"start\":168796},{\"end\":168808,\"start\":168805},{\"end\":168822,\"start\":168815},{\"end\":168836,\"start\":168831},{\"end\":169184,\"start\":169176},{\"end\":169202,\"start\":169192},{\"end\":169217,\"start\":169210},{\"end\":169237,\"start\":169228},{\"end\":169706,\"start\":169702},{\"end\":169721,\"start\":169715},{\"end\":169735,\"start\":169729},{\"end\":169749,\"start\":169745},{\"end\":169767,\"start\":169759},{\"end\":169782,\"start\":169774},{\"end\":169799,\"start\":169792},{\"end\":169816,\"start\":169808},{\"end\":169827,\"start\":169823},{\"end\":169838,\"start\":169835},{\"end\":169851,\"start\":169847},{\"end\":169862,\"start\":169860},{\"end\":169878,\"start\":169871},{\"end\":169899,\"start\":169886},{\"end\":169918,\"start\":169909},{\"end\":169930,\"start\":169927},{\"end\":169947,\"start\":169938},{\"end\":169964,\"start\":169959},{\"end\":169980,\"start\":169975},{\"end\":169992,\"start\":169987},{\"end\":170003,\"start\":169994},{\"end\":170014,\"start\":170009},{\"end\":170027,\"start\":170023},{\"end\":170041,\"start\":170035},{\"end\":170047,\"start\":170043},{\"end\":171228,\"start\":171221},{\"end\":171247,\"start\":171236},{\"end\":171265,\"start\":171258},{\"end\":171655,\"start\":171648},{\"end\":171672,\"start\":171665},{\"end\":171683,\"start\":171679},{\"end\":171702,\"start\":171693},{\"end\":171716,\"start\":171710},{\"end\":171733,\"start\":171724},{\"end\":172093,\"start\":172086},{\"end\":172107,\"start\":172100},{\"end\":172120,\"start\":172115},{\"end\":172134,\"start\":172129},{\"end\":172148,\"start\":172142},{\"end\":172722,\"start\":172715},{\"end\":172731,\"start\":172729},{\"end\":172748,\"start\":172741},{\"end\":172755,\"start\":172753},{\"end\":172775,\"start\":172770},{\"end\":172791,\"start\":172785},{\"end\":172802,\"start\":172799},{\"end\":172813,\"start\":172809},{\"end\":172827,\"start\":172822},{\"end\":172840,\"start\":172836},{\"end\":173341,\"start\":173336},{\"end\":173358,\"start\":173348},{\"end\":173372,\"start\":173365},{\"end\":173386,\"start\":173380},{\"end\":173401,\"start\":173394},{\"end\":174350,\"start\":174340},{\"end\":174367,\"start\":174360},{\"end\":174381,\"start\":174378},{\"end\":174400,\"start\":174395},{\"end\":174415,\"start\":174411},{\"end\":174427,\"start\":174422},{\"end\":174435,\"start\":174431},{\"end\":174447,\"start\":174442},{\"end\":174463,\"start\":174456},{\"end\":174476,\"start\":174471},{\"end\":174492,\"start\":174478},{\"end\":174910,\"start\":174904},{\"end\":174925,\"start\":174919},{\"end\":174944,\"start\":174937},{\"end\":174964,\"start\":174956},{\"end\":174972,\"start\":174969},{\"end\":174983,\"start\":174974},{\"end\":175000,\"start\":174995},{\"end\":175011,\"start\":175002},{\"end\":175019,\"start\":175015},{\"end\":175028,\"start\":175026},{\"end\":175039,\"start\":175036},{\"end\":175045,\"start\":175041},{\"end\":175358,\"start\":175351},{\"end\":175386,\"start\":175366},{\"end\":175393,\"start\":175388},{\"end\":175723,\"start\":175714},{\"end\":175742,\"start\":175735},{\"end\":175754,\"start\":175750},{\"end\":175768,\"start\":175761},{\"end\":175789,\"start\":175777},{\"end\":175799,\"start\":175791},{\"end\":175813,\"start\":175808},{\"end\":175825,\"start\":175822},{\"end\":175837,\"start\":175834},{\"end\":175847,\"start\":175842},{\"end\":175851,\"start\":175849},{\"end\":176235,\"start\":176228},{\"end\":176247,\"start\":176244},{\"end\":176269,\"start\":176259},{\"end\":176282,\"start\":176278},{\"end\":176297,\"start\":176291},{\"end\":176310,\"start\":176305},{\"end\":176327,\"start\":176316},{\"end\":176340,\"start\":176335},{\"end\":176354,\"start\":176349},{\"end\":176368,\"start\":176362},{\"end\":177398,\"start\":177396},{\"end\":177409,\"start\":177405},{\"end\":177432,\"start\":177422},{\"end\":177452,\"start\":177445},{\"end\":177468,\"start\":177459},{\"end\":177493,\"start\":177477},{\"end\":177509,\"start\":177505},{\"end\":177522,\"start\":177517},{\"end\":177832,\"start\":177828},{\"end\":177849,\"start\":177844},{\"end\":177860,\"start\":177855},{\"end\":177872,\"start\":177870},{\"end\":177882,\"start\":177879},{\"end\":178355,\"start\":178351},{\"end\":178371,\"start\":178365},{\"end\":178395,\"start\":178379},{\"end\":178410,\"start\":178405},{\"end\":178428,\"start\":178421},{\"end\":178446,\"start\":178437},{\"end\":178459,\"start\":178454},{\"end\":178485,\"start\":178466},{\"end\":178499,\"start\":178495},{\"end\":178511,\"start\":178507},{\"end\":178526,\"start\":178520},{\"end\":178549,\"start\":178536},{\"end\":178562,\"start\":178556},{\"end\":178573,\"start\":178570},{\"end\":178589,\"start\":178582},{\"end\":178603,\"start\":178597},{\"end\":178619,\"start\":178611},{\"end\":178633,\"start\":178627},{\"end\":178649,\"start\":178644},{\"end\":178656,\"start\":178651},{\"end\":179897,\"start\":179894},{\"end\":179912,\"start\":179907},{\"end\":179926,\"start\":179922},{\"end\":179938,\"start\":179935},{\"end\":179952,\"start\":179950},{\"end\":179966,\"start\":179960},{\"end\":179974,\"start\":179972},{\"end\":179988,\"start\":179985},{\"end\":179999,\"start\":179997},{\"end\":180384,\"start\":180381},{\"end\":180397,\"start\":180393},{\"end\":180414,\"start\":180404},{\"end\":180429,\"start\":180424},{\"end\":180437,\"start\":180434},{\"end\":180446,\"start\":180444},{\"end\":180458,\"start\":180454},{\"end\":180786,\"start\":180782},{\"end\":180807,\"start\":180796},{\"end\":180820,\"start\":180814},{\"end\":180832,\"start\":180827},{\"end\":180845,\"start\":180840},{\"end\":180867,\"start\":180854},{\"end\":180876,\"start\":180874},{\"end\":180891,\"start\":180884},{\"end\":180897,\"start\":180893},{\"end\":181819,\"start\":181816},{\"end\":181834,\"start\":181832},{\"end\":181844,\"start\":181841},{\"end\":181857,\"start\":181852},{\"end\":181874,\"start\":181867},{\"end\":181894,\"start\":181886},{\"end\":181910,\"start\":181908},{\"end\":181922,\"start\":181917},{\"end\":181937,\"start\":181934},{\"end\":181945,\"start\":181941},{\"end\":181951,\"start\":181947},{\"end\":182441,\"start\":182439},{\"end\":182455,\"start\":182450},{\"end\":182469,\"start\":182463},{\"end\":183297,\"start\":183295},{\"end\":183320,\"start\":183305},{\"end\":183325,\"start\":183322},{\"end\":183665,\"start\":183662},{\"end\":183673,\"start\":183671},{\"end\":183688,\"start\":183683},{\"end\":184533,\"start\":184531},{\"end\":184544,\"start\":184539},{\"end\":184554,\"start\":184550},{\"end\":184574,\"start\":184566},{\"end\":184587,\"start\":184583},{\"end\":184597,\"start\":184595},{\"end\":184607,\"start\":184605},{\"end\":184617,\"start\":184615},{\"end\":184631,\"start\":184628},{\"end\":184647,\"start\":184642},{\"end\":185076,\"start\":185071},{\"end\":185089,\"start\":185085},{\"end\":185103,\"start\":185099},{\"end\":185636,\"start\":185631},{\"end\":185652,\"start\":185646},{\"end\":185665,\"start\":185660},{\"end\":185680,\"start\":185673},{\"end\":185691,\"start\":185687},{\"end\":185705,\"start\":185701},{\"end\":185724,\"start\":185719},{\"end\":185735,\"start\":185731},{\"end\":185744,\"start\":185742},{\"end\":185761,\"start\":185749}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":236459873},\"end\":141101,\"start\":140740},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":238215290},\"end\":141735,\"start\":141103},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":256390383},\"end\":142149,\"start\":141737},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":244478674},\"end\":142714,\"start\":142151},{\"attributes\":{\"doi\":\"arXiv:2112.10684\",\"id\":\"b4\"},\"end\":143225,\"start\":142716},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":246485605},\"end\":143919,\"start\":143227},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":210868223},\"end\":144392,\"start\":143921},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":218971783},\"end\":145688,\"start\":144394},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":218971783},\"end\":146207,\"start\":145690},{\"attributes\":{\"doi\":\"arXiv:1810.00278\",\"id\":\"b9\"},\"end\":146699,\"start\":146209},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":54040953},\"end\":147102,\"start\":146701},{\"attributes\":{\"id\":\"b11\"},\"end\":147231,\"start\":147104},{\"attributes\":{\"doi\":\"arXiv:2204.02311\",\"id\":\"b12\"},\"end\":147717,\"start\":147233},{\"attributes\":{\"id\":\"b13\"},\"end\":148106,\"start\":147719},{\"attributes\":{\"doi\":\"arXiv:2210.11416\",\"id\":\"b14\"},\"end\":148560,\"start\":148108},{\"attributes\":{\"id\":\"b15\"},\"end\":149676,\"start\":148562},{\"attributes\":{\"doi\":\"arXiv:2110.14168\",\"id\":\"b16\"},\"end\":150074,\"start\":149678},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":2617020},\"end\":150459,\"start\":150076},{\"attributes\":{\"doi\":\"abs/1902.00098\",\"id\":\"b18\"},\"end\":151221,\"start\":150461},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":53218829},\"end\":151787,\"start\":151223},{\"attributes\":{\"doi\":\"arXiv:2101.00027\",\"id\":\"b20\"},\"end\":152270,\"start\":151789},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":221878771},\"end\":152749,\"start\":152272},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":230799347},\"end\":153246,\"start\":152751},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":221516475},\"end\":153670,\"start\":153248},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":221516475},\"end\":154207,\"start\":153672},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":232134851},\"end\":154587,\"start\":154209},{\"attributes\":{\"id\":\"b26\"},\"end\":154963,\"start\":154589},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":218487109},\"end\":155485,\"start\":154965},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":204915921},\"end\":155825,\"start\":155487},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b29\"},\"end\":156043,\"start\":155827},{\"attributes\":{\"doi\":\"arXiv:2205.11916\",\"id\":\"b30\"},\"end\":156370,\"start\":156045},{\"attributes\":{\"doi\":\"10.18653/v1/2022.acl-long.579\",\"id\":\"b31\",\"matched_paper_id\":207783692},\"end\":157233,\"start\":156372},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":9494747},\"end\":157440,\"start\":157235},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":2319779},\"end\":158352,\"start\":157442},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":221655495},\"end\":158873,\"start\":158354},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":233296808},\"end\":159229,\"start\":158875},{\"attributes\":{\"doi\":\"arXiv:2211.09110\",\"id\":\"b36\"},\"end\":159680,\"start\":159231},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":12777818},\"end\":160006,\"start\":159682},{\"attributes\":{\"id\":\"b38\"},\"end\":160458,\"start\":160008},{\"attributes\":{\"doi\":\"arXiv:2104.08786\",\"id\":\"b39\"},\"end\":160878,\"start\":160460},{\"attributes\":{\"id\":\"b40\"},\"end\":161255,\"start\":160880},{\"attributes\":{\"id\":\"b41\"},\"end\":161763,\"start\":161257},{\"attributes\":{\"doi\":\"arXiv:2110.15943\",\"id\":\"b42\"},\"end\":162039,\"start\":161765},{\"attributes\":{\"doi\":\"arXiv:2202.12837\",\"id\":\"b43\"},\"end\":162470,\"start\":162041},{\"attributes\":{\"doi\":\"arXiv:2007.02871\",\"id\":\"b44\"},\"end\":162947,\"start\":162472},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":222090785},\"end\":163547,\"start\":162949},{\"attributes\":{\"doi\":\"arXiv:2112.00114\",\"id\":\"b46\"},\"end\":164094,\"start\":163549},{\"attributes\":{\"doi\":\"arXiv:2109.01653\",\"id\":\"b47\"},\"end\":164422,\"start\":164096},{\"attributes\":{\"doi\":\"arXiv:2203.02155\",\"id\":\"b48\"},\"end\":164922,\"start\":164424},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":52123220},\"end\":165503,\"start\":164924},{\"attributes\":{\"id\":\"b50\"},\"end\":165707,\"start\":165505},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":204838007},\"end\":166217,\"start\":165709},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":174803111},\"end\":166778,\"start\":166219},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":11816014},\"end\":167184,\"start\":166780},{\"attributes\":{\"doi\":\"10.18653/v1/P18-2124\",\"id\":\"b54\",\"matched_paper_id\":47018994},\"end\":167928,\"start\":167186},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":52055325},\"end\":168277,\"start\":167930},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":434646},\"end\":168711,\"start\":168279},{\"attributes\":{\"doi\":\"arXiv:2004.13637\",\"id\":\"b57\"},\"end\":169128,\"start\":168713},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":13756572},\"end\":169626,\"start\":169130},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":239009562},\"end\":171101,\"start\":169628},{\"attributes\":{\"doi\":\"10.48550/arXiv.2205.12393\",\"id\":\"b60\"},\"end\":171550,\"start\":171103},{\"attributes\":{\"doi\":\"arXiv:1909.08053\",\"id\":\"b61\"},\"end\":171992,\"start\":171552},{\"attributes\":{\"doi\":\"10.18653/v1/2021.findings-acl.54\",\"id\":\"b62\",\"matched_paper_id\":236478290},\"end\":172706,\"start\":171994},{\"attributes\":{\"doi\":\"arXiv:2208.03188\",\"id\":\"b63\"},\"end\":173233,\"start\":172708},{\"attributes\":{\"doi\":\"10.18653/v1/2020.acl-main.183\",\"id\":\"b64\",\"matched_paper_id\":215827653},\"end\":174237,\"start\":173235},{\"attributes\":{\"doi\":\"arXiv:2206.04615\",\"id\":\"b65\"},\"end\":174823,\"start\":174239},{\"attributes\":{\"doi\":\"arXiv:2210.09261\",\"id\":\"b66\"},\"end\":175342,\"start\":174825},{\"attributes\":{\"doi\":\"arXiv:2012.13048\",\"id\":\"b67\"},\"end\":175706,\"start\":175344},{\"attributes\":{\"doi\":\"arXiv:2201.08239\",\"id\":\"b68\"},\"end\":176161,\"start\":175708},{\"attributes\":{\"doi\":\"10.18653/v1/D19-1062\",\"id\":\"b69\",\"matched_paper_id\":71144630},\"end\":177332,\"start\":176163},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":218487733},\"end\":177740,\"start\":177334},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":173990423},\"end\":178305,\"start\":177742},{\"attributes\":{\"id\":\"b72\"},\"end\":179836,\"start\":178307},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":237416585},\"end\":180302,\"start\":179838},{\"attributes\":{\"doi\":\"arXiv:2201.11903\",\"id\":\"b74\"},\"end\":180689,\"start\":180304},{\"attributes\":{\"doi\":\"10.18653/v1/2022.naacl-main.341\",\"id\":\"b75\",\"matched_paper_id\":238857304},\"end\":181806,\"start\":180691},{\"attributes\":{\"doi\":\"arXiv:2201.05966\",\"id\":\"b76\"},\"end\":182372,\"start\":181808},{\"attributes\":{\"doi\":\"10.18653/v1/2022.acl-long.356\",\"id\":\"b77\",\"matched_paper_id\":236034497},\"end\":183208,\"start\":182374},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":233296709},\"end\":183598,\"start\":183210},{\"attributes\":{\"doi\":\"10.18653/v1/2022.acl-long.218\",\"id\":\"b79\",\"matched_paper_id\":247476090},\"end\":184413,\"start\":183600},{\"attributes\":{\"doi\":\"arXiv:1809.08887\",\"id\":\"b80\"},\"end\":184958,\"start\":184415},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":218595822},\"end\":185531,\"start\":184960},{\"attributes\":{\"doi\":\"arXiv:2205.01068\",\"id\":\"b82\"},\"end\":190138,\"start\":185533}]", "bib_title": "[{\"end\":140794,\"start\":140740},{\"end\":141160,\"start\":141103},{\"end\":141815,\"start\":141737},{\"end\":142213,\"start\":142151},{\"end\":143322,\"start\":143227},{\"end\":143949,\"start\":143921},{\"end\":144479,\"start\":144394},{\"end\":145727,\"start\":145690},{\"end\":146770,\"start\":146701},{\"end\":150176,\"start\":150076},{\"end\":151283,\"start\":151223},{\"end\":152348,\"start\":152272},{\"end\":152844,\"start\":152751},{\"end\":153298,\"start\":153248},{\"end\":153722,\"start\":153672},{\"end\":154269,\"start\":154209},{\"end\":155026,\"start\":154965},{\"end\":155550,\"start\":155487},{\"end\":156413,\"start\":156372},{\"end\":157292,\"start\":157235},{\"end\":157514,\"start\":157442},{\"end\":158421,\"start\":158354},{\"end\":158931,\"start\":158875},{\"end\":159778,\"start\":159682},{\"end\":161306,\"start\":161257},{\"end\":163035,\"start\":162949},{\"end\":165017,\"start\":164924},{\"end\":165789,\"start\":165709},{\"end\":166289,\"start\":166219},{\"end\":166839,\"start\":166780},{\"end\":167244,\"start\":167186},{\"end\":167981,\"start\":167930},{\"end\":168358,\"start\":168279},{\"end\":169167,\"start\":169130},{\"end\":169693,\"start\":169628},{\"end\":172079,\"start\":171994},{\"end\":173321,\"start\":173235},{\"end\":176221,\"start\":176163},{\"end\":177391,\"start\":177334},{\"end\":177817,\"start\":177742},{\"end\":179886,\"start\":179838},{\"end\":180774,\"start\":180691},{\"end\":182432,\"start\":182374},{\"end\":183285,\"start\":183210},{\"end\":183652,\"start\":183600},{\"end\":185060,\"start\":184960},{\"end\":185623,\"start\":185533}]", "bib_author": "[{\"end\":140814,\"start\":140796},{\"end\":140835,\"start\":140814},{\"end\":140855,\"start\":140835},{\"end\":140874,\"start\":140855},{\"end\":140888,\"start\":140874},{\"end\":140901,\"start\":140888},{\"end\":141173,\"start\":141162},{\"end\":141186,\"start\":141173},{\"end\":141202,\"start\":141186},{\"end\":141219,\"start\":141202},{\"end\":141232,\"start\":141219},{\"end\":141245,\"start\":141232},{\"end\":141257,\"start\":141245},{\"end\":141274,\"start\":141257},{\"end\":141288,\"start\":141274},{\"end\":141304,\"start\":141288},{\"end\":141828,\"start\":141817},{\"end\":142231,\"start\":142215},{\"end\":142239,\"start\":142231},{\"end\":142253,\"start\":142239},{\"end\":142266,\"start\":142253},{\"end\":142288,\"start\":142266},{\"end\":142310,\"start\":142288},{\"end\":142326,\"start\":142310},{\"end\":142334,\"start\":142326},{\"end\":142345,\"start\":142334},{\"end\":142359,\"start\":142345},{\"end\":142363,\"start\":142359},{\"end\":142797,\"start\":142782},{\"end\":142813,\"start\":142797},{\"end\":142826,\"start\":142813},{\"end\":142842,\"start\":142826},{\"end\":142852,\"start\":142842},{\"end\":142866,\"start\":142852},{\"end\":142883,\"start\":142866},{\"end\":142895,\"start\":142883},{\"end\":142912,\"start\":142895},{\"end\":142932,\"start\":142912},{\"end\":143335,\"start\":143324},{\"end\":143348,\"start\":143335},{\"end\":143364,\"start\":143348},{\"end\":143377,\"start\":143364},{\"end\":143391,\"start\":143377},{\"end\":143399,\"start\":143391},{\"end\":143408,\"start\":143399},{\"end\":143424,\"start\":143408},{\"end\":143440,\"start\":143424},{\"end\":143445,\"start\":143440},{\"end\":143460,\"start\":143445},{\"end\":143467,\"start\":143460},{\"end\":143970,\"start\":143951},{\"end\":143988,\"start\":143970},{\"end\":144002,\"start\":143988},{\"end\":144016,\"start\":144002},{\"end\":144034,\"start\":144016},{\"end\":144492,\"start\":144481},{\"end\":144507,\"start\":144492},{\"end\":144519,\"start\":144507},{\"end\":144536,\"start\":144519},{\"end\":144552,\"start\":144536},{\"end\":144571,\"start\":144552},{\"end\":144591,\"start\":144571},{\"end\":144605,\"start\":144591},{\"end\":144620,\"start\":144605},{\"end\":144635,\"start\":144620},{\"end\":144653,\"start\":144635},{\"end\":144673,\"start\":144653},{\"end\":144691,\"start\":144673},{\"end\":144705,\"start\":144691},{\"end\":144718,\"start\":144705},{\"end\":144733,\"start\":144718},{\"end\":144749,\"start\":144733},{\"end\":144761,\"start\":144749},{\"end\":144777,\"start\":144761},{\"end\":144790,\"start\":144777},{\"end\":144801,\"start\":144790},{\"end\":144814,\"start\":144801},{\"end\":144830,\"start\":144814},{\"end\":145740,\"start\":145729},{\"end\":145755,\"start\":145740},{\"end\":145767,\"start\":145755},{\"end\":145784,\"start\":145767},{\"end\":145800,\"start\":145784},{\"end\":145819,\"start\":145800},{\"end\":145839,\"start\":145819},{\"end\":145853,\"start\":145839},{\"end\":145868,\"start\":145853},{\"end\":145883,\"start\":145868},{\"end\":146323,\"start\":146303},{\"end\":146340,\"start\":146323},{\"end\":146357,\"start\":146340},{\"end\":146374,\"start\":146357},{\"end\":146388,\"start\":146374},{\"end\":146410,\"start\":146388},{\"end\":146417,\"start\":146410},{\"end\":146792,\"start\":146772},{\"end\":146809,\"start\":146792},{\"end\":146829,\"start\":146809},{\"end\":146843,\"start\":146829},{\"end\":147254,\"start\":147233},{\"end\":147269,\"start\":147254},{\"end\":147283,\"start\":147269},{\"end\":147298,\"start\":147283},{\"end\":147313,\"start\":147298},{\"end\":147327,\"start\":147313},{\"end\":147340,\"start\":147327},{\"end\":147351,\"start\":147340},{\"end\":147366,\"start\":147351},{\"end\":147384,\"start\":147366},{\"end\":147394,\"start\":147384},{\"end\":147845,\"start\":147822},{\"end\":147856,\"start\":147845},{\"end\":147870,\"start\":147856},{\"end\":147884,\"start\":147870},{\"end\":147896,\"start\":147884},{\"end\":147904,\"start\":147896},{\"end\":148192,\"start\":148181},{\"end\":148202,\"start\":148192},{\"end\":148214,\"start\":148202},{\"end\":148230,\"start\":148214},{\"end\":148239,\"start\":148230},{\"end\":148252,\"start\":148239},{\"end\":148264,\"start\":148252},{\"end\":148275,\"start\":148264},{\"end\":148289,\"start\":148275},{\"end\":148299,\"start\":148289},{\"end\":148620,\"start\":148609},{\"end\":148630,\"start\":148620},{\"end\":148642,\"start\":148630},{\"end\":148658,\"start\":148642},{\"end\":148667,\"start\":148658},{\"end\":148680,\"start\":148667},{\"end\":148695,\"start\":148680},{\"end\":148706,\"start\":148695},{\"end\":148720,\"start\":148706},{\"end\":148741,\"start\":148720},{\"end\":148756,\"start\":148741},{\"end\":148764,\"start\":148756},{\"end\":148780,\"start\":148764},{\"end\":148791,\"start\":148780},{\"end\":148802,\"start\":148791},{\"end\":148817,\"start\":148802},{\"end\":148833,\"start\":148817},{\"end\":148849,\"start\":148833},{\"end\":148867,\"start\":148849},{\"end\":148881,\"start\":148867},{\"end\":148897,\"start\":148881},{\"end\":148912,\"start\":148897},{\"end\":148927,\"start\":148912},{\"end\":148941,\"start\":148927},{\"end\":148953,\"start\":148941},{\"end\":148967,\"start\":148953},{\"end\":148981,\"start\":148967},{\"end\":148994,\"start\":148981},{\"end\":149003,\"start\":148994},{\"end\":149016,\"start\":149003},{\"end\":149026,\"start\":149016},{\"end\":149038,\"start\":149026},{\"end\":149051,\"start\":149038},{\"end\":149066,\"start\":149051},{\"end\":149079,\"start\":149066},{\"end\":149089,\"start\":149079},{\"end\":149094,\"start\":149089},{\"end\":149738,\"start\":149726},{\"end\":149755,\"start\":149738},{\"end\":149774,\"start\":149755},{\"end\":149788,\"start\":149774},{\"end\":149806,\"start\":149788},{\"end\":149825,\"start\":149806},{\"end\":149840,\"start\":149825},{\"end\":150195,\"start\":150178},{\"end\":150209,\"start\":150195},{\"end\":150534,\"start\":150521},{\"end\":150553,\"start\":150534},{\"end\":150570,\"start\":150553},{\"end\":150590,\"start\":150570},{\"end\":150604,\"start\":150590},{\"end\":150618,\"start\":150604},{\"end\":150631,\"start\":150618},{\"end\":150645,\"start\":150631},{\"end\":150660,\"start\":150645},{\"end\":150671,\"start\":150660},{\"end\":150691,\"start\":150671},{\"end\":150705,\"start\":150691},{\"end\":150727,\"start\":150705},{\"end\":150745,\"start\":150727},{\"end\":150760,\"start\":150745},{\"end\":150779,\"start\":150760},{\"end\":150793,\"start\":150779},{\"end\":151298,\"start\":151285},{\"end\":151314,\"start\":151298},{\"end\":151328,\"start\":151314},{\"end\":151340,\"start\":151328},{\"end\":151354,\"start\":151340},{\"end\":151368,\"start\":151354},{\"end\":151864,\"start\":151855},{\"end\":151881,\"start\":151864},{\"end\":151892,\"start\":151881},{\"end\":151910,\"start\":151892},{\"end\":151924,\"start\":151910},{\"end\":151940,\"start\":151924},{\"end\":151953,\"start\":151940},{\"end\":151964,\"start\":151953},{\"end\":151977,\"start\":151964},{\"end\":151992,\"start\":151977},{\"end\":152372,\"start\":152350},{\"end\":152392,\"start\":152372},{\"end\":152403,\"start\":152392},{\"end\":152416,\"start\":152403},{\"end\":152423,\"start\":152416},{\"end\":152856,\"start\":152846},{\"end\":152873,\"start\":152856},{\"end\":152885,\"start\":152873},{\"end\":152898,\"start\":152885},{\"end\":152908,\"start\":152898},{\"end\":152925,\"start\":152908},{\"end\":153315,\"start\":153300},{\"end\":153329,\"start\":153315},{\"end\":153344,\"start\":153329},{\"end\":153354,\"start\":153344},{\"end\":153370,\"start\":153354},{\"end\":153381,\"start\":153370},{\"end\":153399,\"start\":153381},{\"end\":153739,\"start\":153724},{\"end\":153753,\"start\":153739},{\"end\":153768,\"start\":153753},{\"end\":153778,\"start\":153768},{\"end\":153794,\"start\":153778},{\"end\":153805,\"start\":153794},{\"end\":153823,\"start\":153805},{\"end\":154286,\"start\":154271},{\"end\":154300,\"start\":154286},{\"end\":154317,\"start\":154300},{\"end\":154329,\"start\":154317},{\"end\":154344,\"start\":154329},{\"end\":154355,\"start\":154344},{\"end\":154366,\"start\":154355},{\"end\":154384,\"start\":154366},{\"end\":154643,\"start\":154626},{\"end\":154664,\"start\":154643},{\"end\":154683,\"start\":154664},{\"end\":154702,\"start\":154683},{\"end\":154718,\"start\":154702},{\"end\":154739,\"start\":154718},{\"end\":154752,\"start\":154739},{\"end\":155045,\"start\":155028},{\"end\":155056,\"start\":155045},{\"end\":155069,\"start\":155056},{\"end\":155087,\"start\":155069},{\"end\":155103,\"start\":155087},{\"end\":155116,\"start\":155103},{\"end\":155137,\"start\":155116},{\"end\":155565,\"start\":155552},{\"end\":155578,\"start\":155565},{\"end\":155595,\"start\":155578},{\"end\":155609,\"start\":155595},{\"end\":155627,\"start\":155609},{\"end\":155883,\"start\":155871},{\"end\":155897,\"start\":155883},{\"end\":155901,\"start\":155897},{\"end\":156108,\"start\":156092},{\"end\":156124,\"start\":156108},{\"end\":156135,\"start\":156124},{\"end\":156148,\"start\":156135},{\"end\":156163,\"start\":156148},{\"end\":156172,\"start\":156163},{\"end\":156432,\"start\":156415},{\"end\":156446,\"start\":156432},{\"end\":156460,\"start\":156446},{\"end\":157310,\"start\":157294},{\"end\":157321,\"start\":157310},{\"end\":157327,\"start\":157321},{\"end\":157529,\"start\":157516},{\"end\":157541,\"start\":157529},{\"end\":157557,\"start\":157541},{\"end\":157570,\"start\":157557},{\"end\":157586,\"start\":157570},{\"end\":157604,\"start\":157586},{\"end\":157618,\"start\":157604},{\"end\":157633,\"start\":157618},{\"end\":157649,\"start\":157633},{\"end\":158437,\"start\":158423},{\"end\":158458,\"start\":158437},{\"end\":158473,\"start\":158458},{\"end\":158487,\"start\":158473},{\"end\":158500,\"start\":158487},{\"end\":158522,\"start\":158500},{\"end\":158539,\"start\":158522},{\"end\":158947,\"start\":158933},{\"end\":158961,\"start\":158947},{\"end\":158976,\"start\":158961},{\"end\":159284,\"start\":159271},{\"end\":159301,\"start\":159284},{\"end\":159311,\"start\":159301},{\"end\":159329,\"start\":159311},{\"end\":159343,\"start\":159329},{\"end\":159363,\"start\":159343},{\"end\":159375,\"start\":159363},{\"end\":159393,\"start\":159375},{\"end\":159404,\"start\":159393},{\"end\":159418,\"start\":159404},{\"end\":159791,\"start\":159780},{\"end\":159806,\"start\":159791},{\"end\":159818,\"start\":159806},{\"end\":159832,\"start\":159818},{\"end\":160020,\"start\":160008},{\"end\":160030,\"start\":160020},{\"end\":160043,\"start\":160030},{\"end\":160055,\"start\":160043},{\"end\":160071,\"start\":160055},{\"end\":160083,\"start\":160071},{\"end\":160094,\"start\":160083},{\"end\":160106,\"start\":160094},{\"end\":160126,\"start\":160106},{\"end\":160144,\"start\":160126},{\"end\":160153,\"start\":160144},{\"end\":160568,\"start\":160560},{\"end\":160581,\"start\":160568},{\"end\":160597,\"start\":160581},{\"end\":160615,\"start\":160597},{\"end\":160633,\"start\":160615},{\"end\":160990,\"start\":160976},{\"end\":161013,\"start\":160990},{\"end\":161028,\"start\":161013},{\"end\":161044,\"start\":161028},{\"end\":161330,\"start\":161308},{\"end\":161345,\"start\":161330},{\"end\":161358,\"start\":161345},{\"end\":161374,\"start\":161358},{\"end\":161387,\"start\":161374},{\"end\":161401,\"start\":161387},{\"end\":161417,\"start\":161401},{\"end\":161434,\"start\":161417},{\"end\":161452,\"start\":161434},{\"end\":161776,\"start\":161765},{\"end\":161788,\"start\":161776},{\"end\":161806,\"start\":161788},{\"end\":161827,\"start\":161806},{\"end\":161836,\"start\":161827},{\"end\":162129,\"start\":162118},{\"end\":162140,\"start\":162129},{\"end\":162154,\"start\":162140},{\"end\":162169,\"start\":162154},{\"end\":162181,\"start\":162169},{\"end\":162202,\"start\":162181},{\"end\":162220,\"start\":162202},{\"end\":162485,\"start\":162472},{\"end\":162501,\"start\":162485},{\"end\":162512,\"start\":162501},{\"end\":162523,\"start\":162512},{\"end\":162544,\"start\":162523},{\"end\":162560,\"start\":162544},{\"end\":162574,\"start\":162560},{\"end\":162586,\"start\":162574},{\"end\":162598,\"start\":162586},{\"end\":162614,\"start\":162598},{\"end\":163052,\"start\":163037},{\"end\":163065,\"start\":163052},{\"end\":163082,\"start\":163065},{\"end\":163097,\"start\":163082},{\"end\":163641,\"start\":163628},{\"end\":163666,\"start\":163641},{\"end\":163679,\"start\":163666},{\"end\":163699,\"start\":163679},{\"end\":163713,\"start\":163699},{\"end\":163727,\"start\":163713},{\"end\":163740,\"start\":163727},{\"end\":163757,\"start\":163740},{\"end\":163772,\"start\":163757},{\"end\":163784,\"start\":163772},{\"end\":164177,\"start\":164162},{\"end\":164190,\"start\":164177},{\"end\":164204,\"start\":164190},{\"end\":164215,\"start\":164204},{\"end\":164224,\"start\":164215},{\"end\":164437,\"start\":164424},{\"end\":164446,\"start\":164437},{\"end\":164456,\"start\":164446},{\"end\":164471,\"start\":164456},{\"end\":164482,\"start\":164471},{\"end\":164501,\"start\":164482},{\"end\":164516,\"start\":164501},{\"end\":164532,\"start\":164516},{\"end\":164550,\"start\":164532},{\"end\":164562,\"start\":164550},{\"end\":164567,\"start\":164562},{\"end\":165032,\"start\":165019},{\"end\":165050,\"start\":165032},{\"end\":165067,\"start\":165050},{\"end\":165080,\"start\":165067},{\"end\":165095,\"start\":165080},{\"end\":165115,\"start\":165095},{\"end\":165135,\"start\":165115},{\"end\":165541,\"start\":165527},{\"end\":165556,\"start\":165541},{\"end\":165565,\"start\":165556},{\"end\":165805,\"start\":165791},{\"end\":165819,\"start\":165805},{\"end\":165833,\"start\":165819},{\"end\":165848,\"start\":165833},{\"end\":165863,\"start\":165848},{\"end\":165879,\"start\":165863},{\"end\":165891,\"start\":165879},{\"end\":165899,\"start\":165891},{\"end\":165912,\"start\":165899},{\"end\":166320,\"start\":166291},{\"end\":166336,\"start\":166320},{\"end\":166351,\"start\":166336},{\"end\":166359,\"start\":166351},{\"end\":166859,\"start\":166841},{\"end\":166871,\"start\":166859},{\"end\":166891,\"start\":166871},{\"end\":166904,\"start\":166891},{\"end\":167264,\"start\":167246},{\"end\":167275,\"start\":167264},{\"end\":167288,\"start\":167275},{\"end\":167995,\"start\":167983},{\"end\":168007,\"start\":167995},{\"end\":168030,\"start\":168007},{\"end\":168378,\"start\":168360},{\"end\":168408,\"start\":168378},{\"end\":168416,\"start\":168408},{\"end\":168729,\"start\":168713},{\"end\":168742,\"start\":168729},{\"end\":168755,\"start\":168742},{\"end\":168762,\"start\":168755},{\"end\":168779,\"start\":168762},{\"end\":168791,\"start\":168779},{\"end\":168800,\"start\":168791},{\"end\":168810,\"start\":168800},{\"end\":168824,\"start\":168810},{\"end\":168838,\"start\":168824},{\"end\":169186,\"start\":169169},{\"end\":169204,\"start\":169186},{\"end\":169219,\"start\":169204},{\"end\":169239,\"start\":169219},{\"end\":169708,\"start\":169695},{\"end\":169723,\"start\":169708},{\"end\":169737,\"start\":169723},{\"end\":169751,\"start\":169737},{\"end\":169769,\"start\":169751},{\"end\":169784,\"start\":169769},{\"end\":169801,\"start\":169784},{\"end\":169818,\"start\":169801},{\"end\":169829,\"start\":169818},{\"end\":169840,\"start\":169829},{\"end\":169853,\"start\":169840},{\"end\":169864,\"start\":169853},{\"end\":169880,\"start\":169864},{\"end\":169901,\"start\":169880},{\"end\":169920,\"start\":169901},{\"end\":169932,\"start\":169920},{\"end\":169949,\"start\":169932},{\"end\":169966,\"start\":169949},{\"end\":169982,\"start\":169966},{\"end\":169994,\"start\":169982},{\"end\":170005,\"start\":169994},{\"end\":170016,\"start\":170005},{\"end\":170029,\"start\":170016},{\"end\":170043,\"start\":170029},{\"end\":170049,\"start\":170043},{\"end\":171230,\"start\":171214},{\"end\":171249,\"start\":171230},{\"end\":171267,\"start\":171249},{\"end\":171657,\"start\":171639},{\"end\":171674,\"start\":171657},{\"end\":171685,\"start\":171674},{\"end\":171704,\"start\":171685},{\"end\":171718,\"start\":171704},{\"end\":171735,\"start\":171718},{\"end\":172095,\"start\":172081},{\"end\":172109,\"start\":172095},{\"end\":172122,\"start\":172109},{\"end\":172136,\"start\":172122},{\"end\":172150,\"start\":172136},{\"end\":172724,\"start\":172710},{\"end\":172733,\"start\":172724},{\"end\":172750,\"start\":172733},{\"end\":172757,\"start\":172750},{\"end\":172777,\"start\":172757},{\"end\":172793,\"start\":172777},{\"end\":172804,\"start\":172793},{\"end\":172815,\"start\":172804},{\"end\":172829,\"start\":172815},{\"end\":172842,\"start\":172829},{\"end\":173343,\"start\":173323},{\"end\":173360,\"start\":173343},{\"end\":173374,\"start\":173360},{\"end\":173388,\"start\":173374},{\"end\":173403,\"start\":173388},{\"end\":174352,\"start\":174333},{\"end\":174369,\"start\":174352},{\"end\":174383,\"start\":174369},{\"end\":174402,\"start\":174383},{\"end\":174417,\"start\":174402},{\"end\":174429,\"start\":174417},{\"end\":174437,\"start\":174429},{\"end\":174449,\"start\":174437},{\"end\":174465,\"start\":174449},{\"end\":174478,\"start\":174465},{\"end\":174494,\"start\":174478},{\"end\":174912,\"start\":174898},{\"end\":174927,\"start\":174912},{\"end\":174946,\"start\":174927},{\"end\":174966,\"start\":174946},{\"end\":174974,\"start\":174966},{\"end\":174985,\"start\":174974},{\"end\":175002,\"start\":174985},{\"end\":175013,\"start\":175002},{\"end\":175021,\"start\":175013},{\"end\":175030,\"start\":175021},{\"end\":175041,\"start\":175030},{\"end\":175047,\"start\":175041},{\"end\":175360,\"start\":175344},{\"end\":175388,\"start\":175360},{\"end\":175395,\"start\":175388},{\"end\":175725,\"start\":175708},{\"end\":175744,\"start\":175725},{\"end\":175756,\"start\":175744},{\"end\":175770,\"start\":175756},{\"end\":175791,\"start\":175770},{\"end\":175801,\"start\":175791},{\"end\":175815,\"start\":175801},{\"end\":175827,\"start\":175815},{\"end\":175839,\"start\":175827},{\"end\":175849,\"start\":175839},{\"end\":175853,\"start\":175849},{\"end\":176237,\"start\":176223},{\"end\":176249,\"start\":176237},{\"end\":176271,\"start\":176249},{\"end\":176284,\"start\":176271},{\"end\":176299,\"start\":176284},{\"end\":176312,\"start\":176299},{\"end\":176329,\"start\":176312},{\"end\":176342,\"start\":176329},{\"end\":176356,\"start\":176342},{\"end\":176370,\"start\":176356},{\"end\":177400,\"start\":177393},{\"end\":177411,\"start\":177400},{\"end\":177434,\"start\":177411},{\"end\":177454,\"start\":177434},{\"end\":177470,\"start\":177454},{\"end\":177495,\"start\":177470},{\"end\":177511,\"start\":177495},{\"end\":177524,\"start\":177511},{\"end\":177834,\"start\":177819},{\"end\":177851,\"start\":177834},{\"end\":177862,\"start\":177851},{\"end\":177874,\"start\":177862},{\"end\":177884,\"start\":177874},{\"end\":178357,\"start\":178343},{\"end\":178373,\"start\":178357},{\"end\":178397,\"start\":178373},{\"end\":178412,\"start\":178397},{\"end\":178430,\"start\":178412},{\"end\":178448,\"start\":178430},{\"end\":178461,\"start\":178448},{\"end\":178487,\"start\":178461},{\"end\":178501,\"start\":178487},{\"end\":178513,\"start\":178501},{\"end\":178528,\"start\":178513},{\"end\":178551,\"start\":178528},{\"end\":178564,\"start\":178551},{\"end\":178575,\"start\":178564},{\"end\":178591,\"start\":178575},{\"end\":178605,\"start\":178591},{\"end\":178621,\"start\":178605},{\"end\":178635,\"start\":178621},{\"end\":178651,\"start\":178635},{\"end\":178658,\"start\":178651},{\"end\":179899,\"start\":179888},{\"end\":179914,\"start\":179899},{\"end\":179928,\"start\":179914},{\"end\":179940,\"start\":179928},{\"end\":179954,\"start\":179940},{\"end\":179968,\"start\":179954},{\"end\":179976,\"start\":179968},{\"end\":179990,\"start\":179976},{\"end\":180001,\"start\":179990},{\"end\":180386,\"start\":180375},{\"end\":180399,\"start\":180386},{\"end\":180416,\"start\":180399},{\"end\":180431,\"start\":180416},{\"end\":180439,\"start\":180431},{\"end\":180448,\"start\":180439},{\"end\":180460,\"start\":180448},{\"end\":180788,\"start\":180776},{\"end\":180809,\"start\":180788},{\"end\":180822,\"start\":180809},{\"end\":180834,\"start\":180822},{\"end\":180847,\"start\":180834},{\"end\":180869,\"start\":180847},{\"end\":180878,\"start\":180869},{\"end\":180893,\"start\":180878},{\"end\":180899,\"start\":180893},{\"end\":181821,\"start\":181808},{\"end\":181836,\"start\":181821},{\"end\":181846,\"start\":181836},{\"end\":181859,\"start\":181846},{\"end\":181876,\"start\":181859},{\"end\":181896,\"start\":181876},{\"end\":181912,\"start\":181896},{\"end\":181924,\"start\":181912},{\"end\":181939,\"start\":181924},{\"end\":181947,\"start\":181939},{\"end\":181953,\"start\":181947},{\"end\":182443,\"start\":182434},{\"end\":182457,\"start\":182443},{\"end\":182471,\"start\":182457},{\"end\":183299,\"start\":183287},{\"end\":183322,\"start\":183299},{\"end\":183327,\"start\":183322},{\"end\":183667,\"start\":183654},{\"end\":183675,\"start\":183667},{\"end\":183690,\"start\":183675},{\"end\":184535,\"start\":184527},{\"end\":184546,\"start\":184535},{\"end\":184556,\"start\":184546},{\"end\":184576,\"start\":184556},{\"end\":184589,\"start\":184576},{\"end\":184599,\"start\":184589},{\"end\":184609,\"start\":184599},{\"end\":184619,\"start\":184609},{\"end\":184633,\"start\":184619},{\"end\":184649,\"start\":184633},{\"end\":185078,\"start\":185062},{\"end\":185091,\"start\":185078},{\"end\":185105,\"start\":185091},{\"end\":185638,\"start\":185625},{\"end\":185654,\"start\":185638},{\"end\":185667,\"start\":185654},{\"end\":185682,\"start\":185667},{\"end\":185693,\"start\":185682},{\"end\":185707,\"start\":185693},{\"end\":185726,\"start\":185707},{\"end\":185737,\"start\":185726},{\"end\":185746,\"start\":185737},{\"end\":185763,\"start\":185746}]", "bib_venue": "[{\"end\":144165,\"start\":144108},{\"end\":145028,\"start\":144954},{\"end\":151457,\"start\":151437},{\"end\":153966,\"start\":153903},{\"end\":155652,\"start\":155648},{\"end\":156721,\"start\":156634},{\"end\":157840,\"start\":157764},{\"end\":163272,\"start\":163193},{\"end\":166520,\"start\":166448},{\"end\":167489,\"start\":167397},{\"end\":170350,\"start\":170291},{\"end\":173664,\"start\":173586},{\"end\":176743,\"start\":176567},{\"end\":178045,\"start\":177973},{\"end\":181223,\"start\":181074},{\"end\":182676,\"start\":182589},{\"end\":183895,\"start\":183808},{\"end\":185266,\"start\":185194},{\"end\":140904,\"start\":140901},{\"end\":141398,\"start\":141304},{\"end\":141906,\"start\":141828},{\"end\":142421,\"start\":142363},{\"end\":142780,\"start\":142716},{\"end\":143559,\"start\":143467},{\"end\":144106,\"start\":144034},{\"end\":144889,\"start\":144830},{\"end\":145932,\"start\":145883},{\"end\":146301,\"start\":146209},{\"end\":146892,\"start\":146843},{\"end\":147156,\"start\":147106},{\"end\":147449,\"start\":147410},{\"end\":147820,\"start\":147719},{\"end\":148179,\"start\":148108},{\"end\":148607,\"start\":148562},{\"end\":149724,\"start\":149678},{\"end\":150260,\"start\":150209},{\"end\":150519,\"start\":150461},{\"end\":151435,\"start\":151368},{\"end\":151853,\"start\":151789},{\"end\":152492,\"start\":152423},{\"end\":152986,\"start\":152925},{\"end\":153451,\"start\":153399},{\"end\":153901,\"start\":153823},{\"end\":154391,\"start\":154384},{\"end\":154624,\"start\":154589},{\"end\":155217,\"start\":155137},{\"end\":155646,\"start\":155627},{\"end\":155869,\"start\":155827},{\"end\":156090,\"start\":156045},{\"end\":156576,\"start\":156489},{\"end\":157331,\"start\":157327},{\"end\":157717,\"start\":157649},{\"end\":158600,\"start\":158539},{\"end\":159038,\"start\":158976},{\"end\":159269,\"start\":159231},{\"end\":159835,\"start\":159832},{\"end\":160206,\"start\":160153},{\"end\":160558,\"start\":160460},{\"end\":160974,\"start\":160880},{\"end\":161504,\"start\":161452},{\"end\":161880,\"start\":161852},{\"end\":162116,\"start\":162041},{\"end\":162683,\"start\":162630},{\"end\":163191,\"start\":163097},{\"end\":163626,\"start\":163549},{\"end\":164160,\"start\":164096},{\"end\":164650,\"start\":164583},{\"end\":165205,\"start\":165135},{\"end\":165525,\"start\":165505},{\"end\":165948,\"start\":165912},{\"end\":166446,\"start\":166359},{\"end\":166974,\"start\":166904},{\"end\":167395,\"start\":167308},{\"end\":168091,\"start\":168030},{\"end\":168486,\"start\":168416},{\"end\":168897,\"start\":168854},{\"end\":169369,\"start\":169239},{\"end\":170107,\"start\":170049},{\"end\":171212,\"start\":171103},{\"end\":171637,\"start\":171552},{\"end\":172256,\"start\":172182},{\"end\":173519,\"start\":173432},{\"end\":174331,\"start\":174239},{\"end\":174896,\"start\":174825},{\"end\":175503,\"start\":175411},{\"end\":175908,\"start\":175869},{\"end\":176565,\"start\":176390},{\"end\":177529,\"start\":177524},{\"end\":177971,\"start\":177884},{\"end\":178341,\"start\":178307},{\"end\":180060,\"start\":180001},{\"end\":180373,\"start\":180304},{\"end\":181072,\"start\":180930},{\"end\":182060,\"start\":181969},{\"end\":182587,\"start\":182500},{\"end\":183389,\"start\":183327},{\"end\":183806,\"start\":183719},{\"end\":184525,\"start\":184415},{\"end\":185192,\"start\":185105},{\"end\":185938,\"start\":185779}]"}}}, "year": 2023, "month": 12, "day": 17}