{"id": 220961484, "updated": "2023-10-06 12:34:29.279", "metadata": {"title": "To BERT or Not To BERT: Comparing Speech and Language-based Approaches for Alzheimer's Disease Detection", "authors": "[{\"first\":\"Aparna\",\"last\":\"Balagopalan\",\"middle\":[]},{\"first\":\"Benjamin\",\"last\":\"Eyre\",\"middle\":[]},{\"first\":\"Frank\",\"last\":\"Rudzicz\",\"middle\":[]},{\"first\":\"Jekaterina\",\"last\":\"Novikova\",\"middle\":[]}]", "venue": "Interspeech 2020", "journal": "Interspeech 2020", "publication_date": {"year": 2020, "month": 7, "day": 26}, "abstract": "Research related to automatically detecting Alzheimer's disease (AD) is important, given the high prevalence of AD and the high cost of traditional methods. Since AD significantly affects the content and acoustics of spontaneous speech, natural language processing and machine learning provide promising techniques for reliably detecting AD. We compare and contrast the performance of two such approaches for AD detection on the recent ADReSS challenge dataset: 1) using domain knowledge-based hand-crafted features that capture linguistic and acoustic phenomena, and 2) fine-tuning Bidirectional Encoder Representations from Transformer (BERT)-based sequence classification models. We also compare multiple feature-based regression models for a neuropsychological score task in the challenge. We observe that fine-tuned BERT models, given the relative importance of linguistics in cognitive impairment detection, outperform feature-based approaches on the AD detection task.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2008.01551", "mag": "3096912371", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/interspeech/BalagopalanERN20", "doi": "10.21437/interspeech.2020-2557"}}, "content": {"source": {"pdf_hash": "00c5abdffe51ab33e745e6804d4821ca59db52d8", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2008.01551v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2008.01551", "status": "GREEN"}}, "grobid": {"id": "17540269a3dd7c787b3dcef5fd7eca1789956802", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/00c5abdffe51ab33e745e6804d4821ca59db52d8.txt", "contents": "\nTo BERT or Not To BERT: Comparing Speech and Language-based Approaches for Alzheimers Disease Detection\n\n\nAparna Balagopalan aparna@winterlightlabs.com \nWinterlight Labs Inc\nTorontoCanada\n\nBenjamin Eyre benjamin@winterlightlabs.com \nWinterlight Labs Inc\nTorontoCanada\n\nFrank Rudzicz \nDepartment of Computer Science / Vector Institute for Artificial Intelligence\nTorontoCanada\n\nLi Ka Shing Knowledge Institute\nSt Michaels Hospital\nTorontoCanada\n\nJekaterina Novikova jekaterina@winterlightlabs.com \nWinterlight Labs Inc\nTorontoCanada\n\nTo BERT or Not To BERT: Comparing Speech and Language-based Approaches for Alzheimers Disease Detection\nIndex Terms: Alzheimers diseaseADReSSdementia detec- tionMMSE regressionBERTfeature engineeringtransfer learning\nResearch related to automatically detecting Alzheimer's disease (AD) is important, given the high prevalence of AD and the high cost of traditional methods. Since AD significantly affects the content and acoustics of spontaneous speech, natural language processing and machine learning provide promising techniques for reliably detecting AD. We compare and contrast the performance of two such approaches for AD detection on the recent ADReSS challenge dataset [1]: 1) using domain knowledge-based hand-crafted features that capture linguistic and acoustic phenomena, and 2) fine-tuning Bidirectional Encoder Representations from Transformer (BERT)based sequence classification models. We also compare multiple feature-based regression models for a neuropsychological score task in the challenge. We observe that fine-tuned BERT models, given the relative importance of linguistics in cognitive impairment detection, outperform feature-based approaches on the AD detection task.\n\nIntroduction\n\nAlzheimer's disease (AD) is a progressive neurodegenerative disease that causes problems with memory, thinking, and behaviour. AD affects over 40 million people worldwide with high costs of acute and long-term care [2]. Current forms of diagnosis are both time consuming and expensive [3], which might explain why almost half of those living with AD do not receive a timely diagnosis [4].\n\nStudies have shown that valuable clinical information indicative of cognition can be obtained from spontaneous speech elicited using pictures [5]. Several studies have used speech analysis, natural language processing (NLP), and ML to distinguish between healthy and cognitively impaired speech of participants in picture description datasets [6,7]. These serve as quick, objective, and non-invasive assessments of an individual's cognitive status. However, although ML methods for automatic AD-detection using such speech datasets achieve high classification performance (between 82%-93% accuracy) [6,8,9], the field still lacks publicly-available, balanced, and standardised benchmark datasets. The ongoing ADReSS challenge [1] provides an age/sex-matched balanced speech dataset, which consists of speech from AD and non-AD participants describing a picture. The challenge consists of two key tasks: 1) Speech classification task: classifying speech as AD or non-AD. 2) Neuropsychological score regression task:\n\npredicting Mini-Mental State Examination (MMSE) [10] scores from speech.\n\nIn this work, we develop ML models to detect AD from speech using picture description data of the demographicallymatched ADReSS Challenge speech dataset [1], and compare the following training regimes and input representations to detect AD:\n\n1. Using domain knowledge: with this approach, we extract linguistic features from transcripts of speech, and acoustic features from corresponding audio files for binary AD vs non-AD classification and MMSE score regression. The features extracted are informed by previous clinical and ML research in the space of cognitive impairment detection [6].\n\n2. Using transfer learning: with this approach, we finetune pre-trained BERT [11] text classification models at transcript-level. BERT achieved state-of-the-art results on a wide variety of NLP tasks when fine-tuned [11]. Our motivation is to benchmark a similar training procedure on transcripts from a pathological speech dataset, and evaluate the effectiveness of high-level language representations from BERT in detecting AD.\n\nIn this paper, we evaluate performance of these two methods on both the ADReSS train dataset, and on the unseen test set. We find that fine-tuned BERT-based text sequence classification models achieve the highest AD detection accuracy with an accuracy of 83.3% on the test set. With the feature-based models, the highest accuracy of 81.3% is achieved by the SVM with RBF kernel model. The lowest root mean squared error obtained for the MMSE prediction task is 4.56, with a featurebased L2 regularized linear regression model. The main contributions of our paper are as follows:\n\n\u2022 We employ a domain knowledge-based approach and compare a number of AD detection and MMSE regression models with an extensive list of pre-defined linguistic and acoustic features as input representations from speech (Section 5 and 6).\n\n\u2022 We employ a transfer learning-based approach and benchmark fine-tuned BERT models for the AD vs non-AD classification task (Section 5 and 6).\n\n\u2022 We contrast the performance of the two approaches on the classification task, and discuss the reasons for existing differences (Section 7). Previous work has focused on automatic AD detection from speech using acoustic features (such as zero-crossing rate, Melfrequency cepstral coefficients) and linguistic features (such as proportions of various part-of-speech (POS) tags [12,6,8]) from speech transcripts. Fraser et al. [6] extracted 370 linguistic and acoustic features from picture descriptions in the Dementia-Bank dataset, and obtained an AD detection accuracy of 82% at transcript-level. More recent studies showed the addition of normative data helped increase accuracy up to 93% [8,13] . Yancheva et al. [14] showed ML models are capable of predicting the MMSE scores from features of speech elicited via picture descriptions, with mean absolute error of 2.91-3.83.\n\nDetecting AD or predicting scores like MMSE with preengineered features of speech and thereby infusing domain knowledge into the task has several advantages, such as more interpretable model decisions and potentially lower resource requirement (when paired with conventional ML models). However, there are also a few disadvantages, e.g. a time consuming process of feature engineering, and a risk of missing highly relevant features.\n\n\nTransfer Learning-based Approach\n\nIn the recent years, transfer learning in the form of pre-trained language models has become ubiquitous in NLP [15] and has contributed to the state-of-the-art on a wide range of tasks. One of the most popular transfer learning models is BERT [11], which builds on Transformer networks [16] to pre-train bidirectional representations of text by conditioning on both left and right contexts jointly in all layers.\n\nBERT uses powerful attention mechanisms to encode global dependencies between the input and output. This allows it to achieve state-of-the-art results on a suite of benchmarks [11]. Fine-tuning BERT for a few epochs can potentially attain good performance even on small datasets. However, such models are not directly interpretable, unlike feature-based ones.\n\n\nDataset\n\nWe use the ADReSS Challenge dataset [1], which consists of 156 speech samples and associated transcripts from non-AD (N =78) and AD (N =78) English-speaking participants. Speech is elicited from participants through the Cookie Theft picture from the Boston Diagnostic Aphasia exam [5]. In contrast to other speech datasets for AD detection such as Dementia-Bank's English Pitt Corpus [17], the ADReSS challenge dataset is matched for age and gender ( Table 1). The speech dataset is divided into standard train and test sets. MMSE [10] scores are available for all but one of the participants in the train set.\n\n\nFeature Extraction\n\nThe speech transcripts in the dataset are manually transcribed as per the CHAT protocol [18], and include speech segments from both the participant and an investigator. We only use the portion of the transcripts corresponding to the participant. Additionally, we combine all participant speech segments corresponding to a single picture description for extracting acoustic features.\n\nWe extract 509 manually-engineered features from transcripts and associated audio files (see Appendix A for a list of all features). These features are identified as indicators of 1. Lexico-syntactic features (297): Frequencies of various production rules from the constituency parsing tree of the transcripts [19], speech-graph based features [20], lexical norm-based features (e.g. average sentiment valence of all words in a transcript, average imageability of all words in a transcript [21]), features indicative of lexical richness. We also extract syntactic features [22] such as the proportion of various POS-tags, and similarity between consecutive utterances.\n\n\nAcoustic features (187):\n\nMel-frequency cepstral coefficients (MFCCs), fundamental frequency, statistics related to zero-crossing rate, as well as proportion of various pauses [23] (for example, filled and unfilled pauses, ratio of a number of pauses to a number of words etc.)\n\n3. Semantic features based on picture description content (25): Proportions of various information content units used in the picture, identified as being relevant to memory impairment in prior literature [24].\n\n\nExperiments\n\n\nAD vs non-AD Classification\n\n\nTraining Regimes\n\nWe benchmark the following training regimes for classification: classifying features extracted at transcript-level and a BERT model fine-tuned on transcripts. Domain knowledge-based approach: We classify lexicosyntactic, semantic, and acoustic features extracted at transcript-level with four conventional ML models (SVM, neural network (NN), random forest (RF), na\u00efve Bayes (NB) 1 .\n\nHyperparameter tuning: We optimize each model to the best possible hyper-parameter setting using grid-search 10fold cross-validation (CV). We perform feature selection by choosing top-k number of features, based on ANOVA F-value between label/features. The number of features is jointly optimized with the classification model parameters (see Appendix B for a full list of parameters).\n\nTransfer learning-based approach: In order to leverage the language information encoded by BERT [11], we use pretrained model weights to initialize our classification model. We add a classification layer mapping representations from the final BERT layer to binary class labels [25] for the AD vs non-AD classification task. The model is fine-tuned on training data with 10-fold CV. Adam optimizer [26] and linear scheduling for the learning rate [27] are used.\n\nHyperparameter tuning: We optimize the number of epochs to 10 by varying it from 1 to 12 during CV. Learning rate and other optimization parameters (scheduling, optimizers etc.) are set based on prior work on fine-tuning BERT [11,25].\n\n\nEvaluation\n\nCross-validation on ADReSS train set: We use two CV strategies in our work -leave-one-subject-out CV (LOSO CV) and 10-fold CV. We report evaluation metrics with LOSO CV for all models except fine-tuned BERT for direct comparison with challenge baseline. Due to computational constraints of GPU memory, we are unable to perform LOSO CV for the BERT model. Hence, we perform 10-fold CV to compare featurebased classification models with fine-tuned BERT. Values of performance metrics for each model are averaged across three runs of 10-fold CV with different random seeds. Predictions on ADReSS test set: We generate three predictions with different seeds from each hyperparameter-optimized classifier trained on the complete train set, and then produce a majority prediction to avoid overfitting. We report performance on the challenge test set, as obtained from the challenge organizers (see Appendix D for more details).\n\nWe evaluate task performance primarily using accuracy scores, since all train/test sets are known to be balanced. We also report precision, recall, specificity and F1 with respect to the positive class (AD).\n\n\nMMSE Score Regression\n\n\nTraining Regimes\n\nDomain knowledge-based approach: For this task, we benchmark two kinds of regression models, linear and ridge, using pre-engineered features as input. MMSE scores are always within the range of 0-30, and so predictions are clipped to a range between 0 and 30.\n\nHyperparameter tuning: Each model's performance is optimized using hyperparameters selected from grid-search LOSO CV. We perform feature selection by choosing top-k number of features, based on an F-Score computed from the correlation of each feature with MMSE score. The number of features is optimized for all models. For ridge regression, the number of features is jointly optimized with the coefficient for L2 regularization, \u03b1.\n\n\nEvaluation\n\nWe report root mean squared error (RMSE) and mean absolute error (MAE) for the predictions produced by each of the models on the training set with LOSO CV. In addition, we include the RMSE for two models' predictions on the ADReSS test set. Hyperparameters for these models were selected based on performance in grid-search 10-fold cross validation on the training set, motivated by the thought that 10-fold CV better demonstrates how well a model will generalize to the test set.\n\n\nResults\n\n\nAD vs non-AD Classification\n\nIn Table 3, the classification performance with all the models evaluated on the train set via 10-fold CV is displayed. We observe that BERT outperforms all domain knowledge-based ML models with respect to all metrics. SVM is the bestperforming domain knowledge-based model. However, accu-racy of the fine-tuned BERT model is not significantly higher than that of the SVM classifier based on an Kruskal-Wallis Htest (H = 0.4838, p > 0.05).\n\nWe also report the performance of all our classification models with LOSO CV. Each of our classification models outperform the challenge baselines by a large margin (+30% accuracy for the best performing model). It is important to note that feature selection results in accuracy increase of about 13% for the SVM classifier.\n\nPerformance results on the unseen, held-out challenge test set are shown in Table 5 and follow the trend of the crossvalidated performance in terms of accuracy, with BERT outperforming the best feature-based classification model SVM.\n\n\nMMSE Score Regression\n\nPerformance of regression models evaluated on both train and test sets is shown in Table 6. Ridge regression with 25 features selected attains the lowest RMSE of 4.56 during LOSO-CV on the training set, a decrease of 2.7 from the challenge baseline. The results show that feature selection is impactful for performance and helps achieve a decrease of up to 1.5 RMSE points (and up to 0.86 of MAE) for a ridge regressor. Furthermore, a ridge regressor is able to achieve an RMSE of 4.56 on the ADReSS test set, a decrease of 1.6 from the baseline.\n\n\nDiscussion\n\n\nFeature Differentiation Analysis\n\nWe extract a large number of features to capture a wide range of linguistic and acoustic phenomena, based on a survey of prior literature in automatic cognitive impairment detection [6,14,29,30]. In order to identify the most differentiating features between AD and non-AD speech, we perform independent ttests between feature means for each class in the ADReSS training set. 87 features are significantly different between the two groups at p < 0.05. 79 of these are text-based lexicosyntactic and semantic features, while 8 are acoustic. These 8 acoustic features include the number of long pauses, pause duration, and mean/skewness/variance-statistics of various MFCC coefficients. However, after Bonferroni correction for multiple testing, we identify that only 13 features are significantly different between AD and non-AD speech at p < 9e \u2212 5, and none of these features are acoustic (Table 2). This implies that linguistic features are particularly differentiating between the AD/non-AD classes here, which explains why models trained on linguistic features only attain performance well above random chance (see Fig. 1 in Appendix for visualization of class separability).\n\n\nAnalysing AD Detection Performance Differences\n\nComparing classification performance across all model settings, we observe that BERT outperforms the best domain knowledgebased model in terms of accuracy and all performance metrics with respect to the positive class both on the train set (10-fold CV; though accuracy is not significantly higher) and on the test set (no significance testing possible since only single set of performance scores are available per model; see Appendix D for procedure for submitting challenge predictions). Based on feature differentiation analysis, we hypothesize that good performance with a text-focused BERT model on this speech classification task is due to the strong utility of linguistic features on this dataset. BERT captures a wide range of linguistic phenomena due to its training methodology, potentially encapsulating Table 2: Feature differentiation analysis results for the most important features, based on ADReSS train set. \u00b5AD and \u00b5non\u2212AD show the means of the 13 significantly different features at p<9e-5 (after Bonferroni correction) for the AD and non-AD group respectively. We also show Spearman correlation between MMSE score and features, and regression weights of the features associated with the five greatest and five lowest regression weights from our regression experiments. * next to correlation indicates significance at p<9e-5.\n\n\nFeature\n\nFeature type \u00b5AD \u00b5non\u2212AD Correlation Weight\n\nAverage cosine distance between utterances Semantic 0.91 0.94 --Fraction of pairs of utterances below a similarity threshold (0.5) Semantic 0.03 0.01 --Average cosine distance between 300-dimensional word2vec [28]    most of the important lexico-syntactic and semantic features. It is thus able to use information present in the lexicon, syntax, and semantics of the transcribed speech after fine-tuning [31].\n\nWe also see a trend of better performance when increasing the number of folds (see SVM in Table 4 and Table 3) in crossvalidation. We postulate that this is due to the small size of the dataset, and hence differences in training set size in each fold (Ntrain = 107 with LOSO, Ntrain = 98 with 10-fold CV).\n\n\nRegression Weights\n\nTo assess the relative importance of individual input features for MMSE prediction, we report features with the five highest and five lowest regression weights in Table 2. Each presented value is the average weight assigned to that feature across each of the LOSO CV folds. We also present the correlation with MMSE score coefficients for those 10 features, as well as their significance, in Table 2. We observe that for each of these highly weighted features, a positive or negative correlation coefficient is accompanied by a positive or negative regression weight, respectively. This demonstrates that these 10 features are so distinguishing that, even in the presence of other regressors, their relationship with MMSE score remains the same. We also note that all 10 of these are linguistic features, further demonstrating  \n\n\nConclusions\n\nIn this paper, we compare two widely used approaches -explicit features engineering based on domain knowledge, and transfer learning using fine-tuned BERT classification model. Our results show that pre-trained models that are fine-tuned for the AD classification task are capable of performing well on AD detection, and outperforming hand-crafted feature engineering. A direction for future work is developing ML models that combine representations from BERT and hand-crafted features [32]. Such feature-fusion approaches could potentially boost performance on the cognitive impairment detection task.\n\n\nA. List of features\n\nList of lexico-syntactic features is in Table 7, acoustic features in Table 8 and semantic in Table 9, all with brief descriptions and counts of sub-types.\n\n\nB. Hyper-parameter Settings\n\nHyper-parameters were tuned using grid search with 10-fold cross validation on the ADReSS challenge 'train' set. The random forest classifier fits 200 decision trees and considers \u221a f eatures when looking for the best split. The minimum number of samples required to split an internal node is 2, and the minimum number of samples required to be at a leaf node is 2. Bootstrap samples are used when building trees. All other parameters are set to the default value.\n\nThe Gaussian Naive Bayes classifier is fit with balanced priors and variance smoothing coefficient set to 1e \u2212 10 and all other parameters default in each case..\n\nThe SVM is trained with a radial basis function kernel with kernel coefficient(\u03b3) 0.001, and regularization parameter set to 100.\n\nThe NN used consists of 2 layers of 10 units each (note we varied both the number of units and number of layers while tuning for the optimal hyperparameter setting). The ReLU activation function is used at each hidden layer. The model is trained using Adam for 200 epochs and with a batch size of number of samples in train set in each fold. All other parameters are default.  Table 2) in feature representation for this plot.\n\n\nC. t-SNE Visualization\n\nIn order to visualize the class-separability of the featurebased representations, we visualize t-SNE [36] plots in Figure 1. We observe strong class-separation between the two classes, indicating that a non-linear model would be capable of good AD detection performance with these representations.\n\n\nD. Test Performance Metrics\n\nThe procedure for obtaining performance metrics on the test set was as follows:\n\n1. Predictions from up to 5 models are sent to the challenge organizer for each prediction task -we sent predictions from 5 AD vs non-AD classification models (SVM, NN, RF, NB, BERT) and 5 linear regression models.\n\n2. Organizers send performance scores on the test set for each prediction set, which are then reported in Table 5 and Table 6.  Table 8: Summary of all acoustic features extracted. The number of features in each subtype is shown in the second column (titled \"#features\").\n\n\nFeature type #Features Brief Description\n\nPauses and fillers 9\n\nTotal and mean duration of pauses;long and short pause counts; pause to word ratio; fillers(um,uh); duration of pauses to word durations Fundamental frequency 4 Avg/min/max/median fundamental frequency of audio Duration-related 2 Duration of audio and spoken segment of audio Zero-crossing rate 4 Avg/variance/skewness/kurtosis of zero-crossing rate Mel-frequency Cepstral Coefficients (MFCC) 168 Avg/variance/skewness/kurtosis of 42 MFCC coefficients Table 9: Summary of all semantic features extracted. The number of features in each subtype is shown in the second column (titled \"#features\").\n\n\nFeature type #Features Brief Description\n\nWord frequency 10 Proportion of lemmatized words, relating to the Cookie Theft picture content units to total number of content units Global coherence 15 Avg/min/max cosine distance between word2vec [28] utterances and picture content units, with varying dimensions of word2vec\n\nFigure 1 :\n1A t-SNE plot showing class separation. Note we only use the 13 features significantly different between classes (see\n\nTable 1 :\n1Basic characteristics of the patients in each group in the ADReSS challenge dataset are more balanced in comparison to DementiaBank.Dataset \nClass \nAD Non-AD \n\nADReSS \nTrain \nMale \n24 \n24 \nFemale \n30 \n30 \n\nADReSS \nTest \nMale \n11 \n11 \nFemale \n13 \n13 \n\nDementiaBank [17] \n-\nMale \n125 \n83 \nFemale 197 \n146 \n\ncognitive impairment in previous literature, and hence encode \ndomain knowledge. All of them are divided into 3 categories: \n\n\n\nTable 3 :\n310-fold CV results averaged across 3 runs with differ-\nent random seeds on the ADReSS train set. Accuracy for BERT \nis higher, but not significantly so from SVM (H = 0.4838, p > \n0.05 Kruskal-Wallis H test). Bold indicates the best result. \n\nModel #Features Accuracy Precision Recall Specificity \nF1 \nSVM \n10 \n0.796 \n0.81 \n0.78 \n0.82 \n0.79 \nNN \n10 \n0.762 \n0.77 \n0.75 \n0.77 \n0.76 \nRF \n50 \n0.738 \n0.73 \n0.76 \n0.72 \n0.74 \nNB \n80 \n0.750 \n0.76 \n0.74 \n0.76 \n0.75 \nBERT \n-\n0.818 \n0.84 \n0.79 \n0.85 \n0.81 \n\n\n\nTable 4 :\n4LOSO-CV results averaged across 3 runs with different random seeds on the ADReSS train set. Accuracy for SVM is significantly higher than NN (H = 4.50, p = 0.034 Kruskal-Wallis H test). Bold indicates the best result.Model \n#Features Accuracy Precision Recall Specificity \nF1 \nBaseline [1] \n-\n0.574 \n0.57 \n0.52 \n-\n0.54 \nSVM \n509 \n0.741 \n0.75 \n0.72 \n0.76 \n0.74 \nSVM \n10 \n0.870 \n0.90 \n0.83 \n0.91 \n0.87 \nNN \n10 \n0.836 \n0.86 \n0.81 \n0.86 \n0.83 \nRF \n50 \n0.778 \n0.79 \n0.77 \n0.79 \n0.78 \nNB \n80 \n0.787 \n0.80 \n0.76 \n0.82 \n0.78 \n\n\n\nTable 5 :\n5Resultson unseen, held-out ADReSS test set .We \npresent test results in same format as the baseline paper [1]. \nBold indicates the best result. \n\nModel \n#Features \nClass \nAccuracy Precision Recall Specificity \nF1 \n\nBaseline [1] \n-\nnon-AD \n0.625 \n0.67 \n0.50 \n-\n0.57 \nAD \n0.60 \n0.75 \n-\n0.67 \n\nSVM \n10 \nnon-AD \n0.813 \n0.83 \n0.79 \n-\n0.81 \nAD \n0.80 \n0.83 \n-\n0.82 \n\nNN \n10 \nnon-AD \n0.771 \n0.78 \n0.75 \n-\n0.77 \nAD \n0.76 \n0.79 \n-\n0.78 \n\nRF \n50 \nnon-AD \n0.750 \n0.71 \n0.83 \n-\n0.77 \nAD \n0.80 \n0.67 \n-\n0.73 \n\nNB \n80 \nnon-AD \n0.729 \n0.69 \n0.83 \n-\n0.75 \nAD \n0.79 \n0.63 \n-\n0.70 \n\nBERT \n-\nnon-AD \n0.833 \n0.86 \n0.79 \n-\n0.83 \nAD \n0.81 \n0.88 \n-\n0.84 \n\n\n\nTable 6 :\n6LOSO-CV MMSE regression results on the ADReSS train and test sets. Bold indicates the best result.Model \n#Features \u03b1 \nRMSE MAE RMSE \nTrain set \nTest set \nBaseline [1] \n-\n-\n7.28 \n6.14 \nLR \n15 \n-\n5.37 \n4.18 \n4.94 \nLR \n20 \n-\n4.94 \n3.72 \n-\nRidge \n509 \n12 \n6.06 \n4.36 \n-\nRidge \n35 \n12 \n4.87 \n3.79 \n4.56 \nRidge \n25 \n10 \n4.56 \n3.50 \n-\n\nthat linguistic information is particularly distinguishing when it \ncomes to predicting the severity of a patient's AD. \n\n\n\nTable 7 :\n7Summary of all lexico-syntactic features extracted. The number of features in each subtype is shown in the second column (titled \"#features\"). Syntactic Complexity Analyzer[33] features; max/min utterance length, depth of syntactic parse tree Production Rules 104 Number of times a production type occurs divided by total number of productions Phrasal type ratios13 Proportion, average length and rate of phrase types Lexical norm-based 12Average norms across all words, across nouns only and across verbs only for imageability, age of acquisition, familiarity and frequency (commonness)Proportion of demonstratives (e.g., \"this\"), function words, light verbs and inflected verbs, and propositions (POS tag verb, adjective, adverb, Fraction of pairs of utterances below a similarity threshold (0.5,0.3,0); avg/min distance Speech-graph features13 Representing words as nodes in a graph and computing density, number of loops etc. Utterance cohesion 1 Number of switches in verb tense across utterances divided by total number of utterances Rate 2 Ratios -number of words: duration of audio; number of syllables: duration of speech, Invalid words 1 Proportion of words not in the English dictionary Sentiment norm-based 9Average sentiment valence, arousal and dominance across all words, noun and verbsFeature type \n#Features \nBrief Description \n\nSyntactic Complexity \n36 \nL2 Lexical richness \n6 \nType-token ratios (including moving window); brunet; Honors statistic \n\nWord category \n5 \n\nconjunction, or preposition) \nNoun ratio \n3 \nRatios nouns:(nouns+verbs); nouns:verbs; pronouns:(nouns+pronouns) \nLength measures \n1 \nAverage word length \nUniversal POS proportions \n18 \nProportions of Spacy univeral POS tags [34] \nPOS tag proportions \n53 \nProportions of Penn Treebank [35] POS tags \nLocal coherence \n15 \nAvg/max/min similarity between word2vec [28] representations of utterances (with different dimensions) \nUtterance distances \n5 \n\nhttps://scikit-learn.org/stable/\nAcknowledgements\nAlzheimer's dementia recognition through spontaneous speech: The adress challenge. S Luz, F Haider, S De La Fuente, D Fromm, B Macwhinney, S. Luz, F. Haider, S. de la Fuente, D. Fromm, and B. MacWhin- ney, \"Alzheimer's dementia recognition through spontaneous speech: The adress challenge,\" 2020.\n\nWorld alzheimer report 2016: improving healthcare for people living with dementia: coverage, quality and costs now and in the future. M Prince, A Comas-Herrera, M Knapp, M Guerchet, M Karagiannidou, M. Prince, A. Comas-Herrera, M. Knapp, M. Guerchet, and M. Karagiannidou, \"World alzheimer report 2016: improving healthcare for people living with dementia: coverage, quality and costs now and in the future,\" 2016.\n\nAnalysis of structure and cost in a longitudinal study of alzheimers disease. G Prabhakaran, R Bakshi, Journal of Health Care Finance. G. Prabhakaran, R. Bakshi et al., \"Analysis of structure and cost in a longitudinal study of alzheimers disease,\" Journal of Health Care Finance, 2018.\n\nMachine-learning based identification of undiagnosed dementia in primary care: a feasibility study. E A Jammeh, B C Camille, W P Stephen, J Escudero, A Anastasiou, P Zhao, T Chenore, J Zajicek, E Ifeachor, BJGP Open. E. A. Jammeh, B. C. Camille, W. P. Stephen, J. Escudero, A. Anastasiou, P. Zhao, T. Chenore, J. Zajicek, and E. Ifeachor, \"Machine-learning based identification of undiagnosed demen- tia in primary care: a feasibility study,\" BJGP Open, p. bjg- popen18X101589, 2018.\n\nBDAE-3: Boston Diagnostic Aphasia Examination-Third Edition. H Goodglass, E Kaplan, B Barresi, Lippincott Williams & WilkinsPhiladelphia, PAH. Goodglass, E. Kaplan, and B. Barresi, BDAE-3: Boston Diag- nostic Aphasia Examination-Third Edition. Lippincott Williams & Wilkins Philadelphia, PA, 2001.\n\nLinguistic features identify Alzheimers disease in narrative speech. K C Fraser, J A Meltzer, F Rudzicz, Journal of Alzheimer's Disease. 492K. C. Fraser, J. A. Meltzer, and F. Rudzicz, \"Linguistic fea- tures identify Alzheimers disease in narrative speech,\" Journal of Alzheimer's Disease, vol. 49, no. 2, pp. 407-422, 2016.\n\nSemi-supervised classification by reaching consensus among modalities. Z Zhu, J Novikova, F Rudzicz, arXiv:1805.09366arXiv preprintZ. Zhu, J. Novikova, and F. Rudzicz, \"Semi-supervised classifi- cation by reaching consensus among modalities,\" arXiv preprint arXiv:1805.09366, 2018.\n\nOn the importance of normative data in speech-based assessment. Z Noorian, C Pou-Prom, F Rudzicz, arXiv:1712.00069arXiv preprintZ. Noorian, C. Pou-Prom, and F. Rudzicz, \"On the importance of normative data in speech-based assessment,\" arXiv preprint arXiv:1712.00069, 2017.\n\nDetecting linguistic characteristics of alzheimer's dementia by interpreting neural models. S Karlekar, T Niu, M Bansal, arXiv:1804.06440arXiv preprintS. Karlekar, T. Niu, and M. Bansal, \"Detecting linguistic charac- teristics of alzheimer's dementia by interpreting neural models,\" arXiv preprint arXiv:1804.06440, 2018.\n\nPrinciples and practice of geriatric psychiatry. J R Cockrell, M F Folstein, Mini-mental state examinationJ. R. Cockrell and M. F. Folstein, \"Mini-mental state examina- tion,\" Principles and practice of geriatric psychiatry, pp. 140- 141, 2002.\n\nBert: Pretraining of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies1J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \"Bert: Pre- training of deep bidirectional transformers for language under- standing,\" in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguis- tics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019, pp. 4171-4186.\n\nLearning linguistic biomarkers for predicting mild cognitive impairment using compound skip-grams. S O Orimaye, K Y Tai, J S , .-M Wong, C P Wong, arXiv:1511.02436arXiv preprintS. O. Orimaye, K. Y. Tai, J. S.-M. Wong, and C. P. Wong, \"Learning linguistic biomarkers for predicting mild cogni- tive impairment using compound skip-grams,\" arXiv preprint arXiv:1511.02436, 2015.\n\nThe effect of heterogeneous data for alzheimer's disease detection from speech. A Balagopalan, J Novikova, F Rudzicz, M Ghassemi, arXiv:1811.12254arXiv preprintA. Balagopalan, J. Novikova, F. Rudzicz, and M. Ghassemi, \"The effect of heterogeneous data for alzheimer's disease detection from speech,\" arXiv preprint arXiv:1811.12254, 2018.\n\nUsing linguistic features longitudinally to predict clinical scores for alzheimers disease and related dementias. M Yancheva, K C Fraser, F Rudzicz, Proceedings of SLPAT 2015: 6th Workshop on Speech and Language Processing for Assistive Technologies. SLPAT 2015: 6th Workshop on Speech and Language Processing for Assistive TechnologiesM. Yancheva, K. C. Fraser, and F. Rudzicz, \"Using linguistic fea- tures longitudinally to predict clinical scores for alzheimers dis- ease and related dementias,\" in Proceedings of SLPAT 2015: 6th Workshop on Speech and Language Processing for Assistive Tech- nologies, 2015, pp. 134-139.\n\nRecent trends in deep learning based natural language processing. T Young, D Hazarika, S Poria, E Cambria, ieee Computational intelligenCe magazine. 133T. Young, D. Hazarika, S. Poria, and E. Cambria, \"Recent trends in deep learning based natural language processing,\" ieee Compu- tational intelligenCe magazine, vol. 13, no. 3, pp. 55-75, 2018.\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, \u0141 Kaiser, I Polosukhin, Advances in neural information processing systems. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \"Attention is all you need,\" in Advances in neural information processing systems, 2017, pp. 5998-6008.\n\nThe natural history of alzheimer's disease: description of study cohort and accuracy of diagnosis. J T Becker, F Boiler, O L Lopez, J Saxton, K L Mcgonigle, Archives of Neurology. 516J. T. Becker, F. Boiler, O. L. Lopez, J. Saxton, and K. L. McGo- nigle, \"The natural history of alzheimer's disease: description of study cohort and accuracy of diagnosis,\" Archives of Neurology, vol. 51, no. 6, pp. 585-594, 1994.\n\nThe CHILDES project: Tools for analyzing talk, Volume I: Transcription format and programs. B Macwhinney, Psychology PressB. MacWhinney, The CHILDES project: Tools for analyzing talk, Volume I: Transcription format and programs. Psychology Press, 2014.\n\nPredicting the fluency of text with shallow structural features: Case studies of machine tanslation and human-written text. J Chae, A Nenkova, J. Chae and A. Nenkova, \"Predicting the fluency of text with shal- low structural features: Case studies of machine tanslation and human-written text,\" 2009.\n\nSpeech graphs provide a quantitative measure of thought disorder in psychosis. N B Mota, N A Vasconcelos, N Lemos, A C Pieretti, O Kinouchi, G A Cecchi, M Copelli, S Ribeiro, PloS one. 74N. B. Mota, N. A. Vasconcelos, N. Lemos, A. C. Pieretti, O. Ki- nouchi, G. A. Cecchi, M. Copelli, and S. Ribeiro, \"Speech graphs provide a quantitative measure of thought disorder in psychosis,\" PloS one, vol. 7, no. 4, 2012.\n\nNorms of valence, arousal, and dominance for 13,915 english lemmas. A B Warriner, V Kuperman, M Brysbaert, Behavior research methods. 45A. B. Warriner, V. Kuperman, and M. Brysbaert, \"Norms of va- lence, arousal, and dominance for 13,915 english lemmas,\" Be- havior research methods, vol. 45, no. 4, pp. 1191-1207, 2013.\n\nA web-based system for automatic measurement of lexical complexity. H Ai, X Lu, 27th Annual Symposium of the Computer-Assisted Language Consortium. H. Ai and X. Lu, \"A web-based system for automatic mea- surement of lexical complexity,\" in 27th Annual Symposium of the Computer-Assisted Language Consortium (CALICO-10).\n\n. M A Amherst, Amherst, MA. June, 2010, pp. 8-12.\n\nExamining pauses in alzheimer's discourse. B H Davis, M Maclagan, American Journal of Alzheimer's Disease & Other Dementias\u00ae. 242B. H. Davis and M. Maclagan, \"Examining pauses in alzheimer's discourse,\" American Journal of Alzheimer's Disease & Other Dementias\u00ae, vol. 24, no. 2, pp. 141-154, 2009.\n\nComparative study of oral and written picture description in patients with alzheimer's disease. B Croisile, B Ska, M.-J Brabant, A Duchene, Y Lepage, G Aimard, M Trillet, Brain and language. 531B. Croisile, B. Ska, M.-J. Brabant, A. Duchene, Y. Lepage, G. Aimard, and M. Trillet, \"Comparative study of oral and writ- ten picture description in patients with alzheimer's disease,\" Brain and language, vol. 53, no. 1, pp. 1-19, 1996.\n\nHuggingfaces transformers: State-of-the-art natural language processing. T Wolf, L Debut, V Sanh, J Chaumond, C Delangue, A Moi, P Cistac, T Rault, R Louf, M Funtowicz, abs/1910.03771ArXiv. T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz et al., \"Hugging- faces transformers: State-of-the-art natural language processing,\" ArXiv, abs/1910.03771, 2019.\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980arXiv preprintD. P. Kingma and J. Ba, \"Adam: A method for stochastic opti- mization,\" arXiv preprint arXiv:1412.6980, 2014.\n\nPytorch: An imperative style, high-performance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, Advances in Neural Information Processing Systems. A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., \"Pytorch: An imperative style, high-performance deep learning library,\" in Advances in Neural Information Processing Systems, 2019, pp. 8024-8035.\n\nDistributed representations of words and phrases and their compositionality. T Mikolov, I Sutskever, K Chen, G S Corrado, J Dean, Advances in neural information processing systems. T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, \"Distributed representations of words and phrases and their com- positionality,\" in Advances in neural information processing sys- tems, 2013, pp. 3111-3119.\n\nLearning multiview embeddings for assessing dementia. C Pou-Prom, F Rudzicz, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingC. Pou-Prom and F. Rudzicz, \"Learning multiview embeddings for assessing dementia,\" in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2018, pp. 2812-2817.\n\nDetecting cognitive impairments by agreeing on interpretations of linguistic features. Z Zhu, J Novikova, F Rudzicz, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies1Z. Zhu, J. Novikova, and F. Rudzicz, \"Detecting cognitive im- pairments by agreeing on interpretations of linguistic features,\" in Proceedings of the 2019 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Lan- guage Technologies, Volume 1 (Long and Short Papers), 2019, pp. 1431-1441.\n\nWhat does bert learn about the structure of language. G Jawahar, B Sagot, D Seddah, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsG. Jawahar, B. Sagot, and D. Seddah, \"What does bert learn about the structure of language?\" in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019, pp. 3651-3657.\n\nCombining word embeddings and feature embeddings for fine-grained relation extraction. M Yu, M R Gormley, M Dredze, Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesM. Yu, M. R. Gormley, and M. Dredze, \"Combining word em- beddings and feature embeddings for fine-grained relation extrac- tion,\" in Proceedings of the 2015 Conference of the North Amer- ican Chapter of the Association for Computational Linguistics: Human Language Technologies, 2015, pp. 1374-1379.\n\nAutomatic analysis of syntactic complexity in second language writing. X Lu, International journal of corpus linguistics. 154X. Lu, \"Automatic analysis of syntactic complexity in second language writing,\" International journal of corpus linguistics, vol. 15, no. 4, pp. 474-496, 2010.\n\nspacy 2: Natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing. M Honnibal, I Montani, To appear. 71M. Honnibal and I. Montani, \"spacy 2: Natural language under- standing with bloom embeddings, convolutional neural networks and incremental parsing,\" To appear, vol. 7, no. 1, 2017.\n\nThe penn treebank: annotating predicate argument structure. M Marcus, G Kim, M A Marcinkiewicz, R Macintyre, A Bies, M Ferguson, K Katz, B Schasberger, Proceedings of the workshop on Human Language Technology. the workshop on Human Language TechnologyAssociation for Computational LinguisticsM. Marcus, G. Kim, M. A. Marcinkiewicz, R. MacIntyre, A. Bies, M. Ferguson, K. Katz, and B. Schasberger, \"The penn treebank: annotating predicate argument structure,\" in Proceedings of the workshop on Human Language Technology. Association for Computational Linguistics, 1994, pp. 114-119.\n\nVisualizing data using t-sne. L V D Maaten, G Hinton, Journal of machine learning research. 9L. v. d. Maaten and G. Hinton, \"Visualizing data using t-sne,\" Journal of machine learning research, vol. 9, no. Nov, pp. 2579- 2605, 2008.\n", "annotations": {"author": "[{\"end\":189,\"start\":107},{\"end\":269,\"start\":190},{\"end\":445,\"start\":270},{\"end\":533,\"start\":446}]", "publisher": null, "author_last_name": "[{\"end\":125,\"start\":114},{\"end\":203,\"start\":199},{\"end\":283,\"start\":276},{\"end\":465,\"start\":457}]", "author_first_name": "[{\"end\":113,\"start\":107},{\"end\":198,\"start\":190},{\"end\":275,\"start\":270},{\"end\":456,\"start\":446}]", "author_affiliation": "[{\"end\":188,\"start\":154},{\"end\":268,\"start\":234},{\"end\":376,\"start\":285},{\"end\":444,\"start\":378},{\"end\":532,\"start\":498}]", "title": "[{\"end\":104,\"start\":1},{\"end\":637,\"start\":534}]", "venue": null, "abstract": "[{\"end\":1729,\"start\":751}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1963,\"start\":1960},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2033,\"start\":2030},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2132,\"start\":2129},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2280,\"start\":2277},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2481,\"start\":2478},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2483,\"start\":2481},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2737,\"start\":2734},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2739,\"start\":2737},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2741,\"start\":2739},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2864,\"start\":2861},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3203,\"start\":3199},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3381,\"start\":3378},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3815,\"start\":3812},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3899,\"start\":3895},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4038,\"start\":4034},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5593,\"start\":5589},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5595,\"start\":5593},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5597,\"start\":5595},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":5641,\"start\":5638},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5907,\"start\":5904},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5910,\"start\":5907},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5933,\"start\":5929},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6677,\"start\":6673},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6809,\"start\":6805},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6852,\"start\":6848},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7156,\"start\":7152},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7386,\"start\":7383},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7631,\"start\":7628},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7735,\"start\":7731},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7882,\"start\":7878},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8072,\"start\":8068},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8678,\"start\":8674},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8712,\"start\":8708},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8858,\"start\":8854},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8941,\"start\":8937},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9215,\"start\":9211},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9376,\"start\":9372},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9522,\"start\":9518},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9969,\"start\":9968},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10460,\"start\":10456},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10641,\"start\":10637},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10761,\"start\":10757},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10810,\"start\":10806},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11052,\"start\":11048},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11055,\"start\":11052},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15282,\"start\":15279},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15285,\"start\":15282},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":15288,\"start\":15285},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15291,\"start\":15288},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":17940,\"start\":17936},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":18135,\"start\":18131},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":19800,\"start\":19796},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":21441,\"start\":21437},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":23093,\"start\":23091},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":23143,\"start\":23139},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":26132,\"start\":26128},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":26321,\"start\":26319},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":26802,\"start\":26800}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":23347,\"start\":23218},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":23791,\"start\":23348},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":24302,\"start\":23792},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":24834,\"start\":24303},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":25479,\"start\":24835},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":25943,\"start\":25480},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":27891,\"start\":25944}]", "paragraph": "[{\"end\":2133,\"start\":1745},{\"end\":3149,\"start\":2135},{\"end\":3223,\"start\":3151},{\"end\":3465,\"start\":3225},{\"end\":3816,\"start\":3467},{\"end\":4247,\"start\":3818},{\"end\":4827,\"start\":4249},{\"end\":5065,\"start\":4829},{\"end\":5210,\"start\":5067},{\"end\":6090,\"start\":5212},{\"end\":6525,\"start\":6092},{\"end\":6974,\"start\":6562},{\"end\":7335,\"start\":6976},{\"end\":7957,\"start\":7347},{\"end\":8362,\"start\":7980},{\"end\":9032,\"start\":8364},{\"end\":9312,\"start\":9061},{\"end\":9523,\"start\":9314},{\"end\":9971,\"start\":9588},{\"end\":10358,\"start\":9973},{\"end\":10820,\"start\":10360},{\"end\":11056,\"start\":10822},{\"end\":11992,\"start\":11071},{\"end\":12201,\"start\":11994},{\"end\":12505,\"start\":12246},{\"end\":12939,\"start\":12507},{\"end\":13434,\"start\":12954},{\"end\":13914,\"start\":13476},{\"end\":14240,\"start\":13916},{\"end\":14475,\"start\":14242},{\"end\":15047,\"start\":14501},{\"end\":16276,\"start\":15097},{\"end\":17670,\"start\":16327},{\"end\":17725,\"start\":17682},{\"end\":18136,\"start\":17727},{\"end\":18443,\"start\":18138},{\"end\":19294,\"start\":18466},{\"end\":19912,\"start\":19310},{\"end\":20091,\"start\":19936},{\"end\":20587,\"start\":20123},{\"end\":20750,\"start\":20589},{\"end\":20881,\"start\":20752},{\"end\":21309,\"start\":20883},{\"end\":21633,\"start\":21336},{\"end\":21744,\"start\":21665},{\"end\":21960,\"start\":21746},{\"end\":22233,\"start\":21962},{\"end\":22298,\"start\":22278},{\"end\":22895,\"start\":22300},{\"end\":23217,\"start\":22940}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":7805,\"start\":7798},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":13486,\"start\":13479},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":14325,\"start\":14318},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":14591,\"start\":14584},{\"end\":15995,\"start\":15987},{\"end\":17148,\"start\":17141},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":18235,\"start\":18228},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":18247,\"start\":18240},{\"end\":18636,\"start\":18629},{\"end\":18865,\"start\":18858},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":19983,\"start\":19976},{\"end\":20037,\"start\":20006},{\"end\":21267,\"start\":21260},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":22075,\"start\":22068},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":22087,\"start\":22080},{\"end\":22097,\"start\":22090},{\"end\":22759,\"start\":22752}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1743,\"start\":1731},{\"attributes\":{\"n\":\"2.2.\"},\"end\":6560,\"start\":6528},{\"attributes\":{\"n\":\"3.\"},\"end\":7345,\"start\":7338},{\"attributes\":{\"n\":\"4.\"},\"end\":7978,\"start\":7960},{\"attributes\":{\"n\":\"2.\"},\"end\":9059,\"start\":9035},{\"attributes\":{\"n\":\"5.\"},\"end\":9537,\"start\":9526},{\"attributes\":{\"n\":\"5.1.\"},\"end\":9567,\"start\":9540},{\"attributes\":{\"n\":\"5.1.1.\"},\"end\":9586,\"start\":9570},{\"attributes\":{\"n\":\"5.1.2.\"},\"end\":11069,\"start\":11059},{\"attributes\":{\"n\":\"5.2.\"},\"end\":12225,\"start\":12204},{\"attributes\":{\"n\":\"5.2.1.\"},\"end\":12244,\"start\":12228},{\"attributes\":{\"n\":\"5.2.2.\"},\"end\":12952,\"start\":12942},{\"attributes\":{\"n\":\"6.\"},\"end\":13444,\"start\":13437},{\"attributes\":{\"n\":\"6.1.\"},\"end\":13474,\"start\":13447},{\"attributes\":{\"n\":\"6.2.\"},\"end\":14499,\"start\":14478},{\"attributes\":{\"n\":\"7.\"},\"end\":15060,\"start\":15050},{\"attributes\":{\"n\":\"7.1.\"},\"end\":15095,\"start\":15063},{\"attributes\":{\"n\":\"7.2.\"},\"end\":16325,\"start\":16279},{\"end\":17680,\"start\":17673},{\"attributes\":{\"n\":\"7.3.\"},\"end\":18464,\"start\":18446},{\"attributes\":{\"n\":\"8.\"},\"end\":19308,\"start\":19297},{\"end\":19934,\"start\":19915},{\"end\":20121,\"start\":20094},{\"end\":21334,\"start\":21312},{\"end\":21663,\"start\":21636},{\"end\":22276,\"start\":22236},{\"end\":22938,\"start\":22898},{\"end\":23229,\"start\":23219},{\"end\":23358,\"start\":23349},{\"end\":23802,\"start\":23793},{\"end\":24313,\"start\":24304},{\"end\":24845,\"start\":24836},{\"end\":25490,\"start\":25481},{\"end\":25954,\"start\":25945}]", "table": "[{\"end\":23791,\"start\":23492},{\"end\":24302,\"start\":23804},{\"end\":24834,\"start\":24532},{\"end\":25479,\"start\":24854},{\"end\":25943,\"start\":25590},{\"end\":27891,\"start\":27257}]", "figure_caption": "[{\"end\":23347,\"start\":23231},{\"end\":23492,\"start\":23360},{\"end\":24532,\"start\":24315},{\"end\":24854,\"start\":24847},{\"end\":25590,\"start\":25492},{\"end\":27257,\"start\":25956}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16222,\"start\":16216},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":21459,\"start\":21451}]", "bib_author_first_name": "[{\"end\":28026,\"start\":28025},{\"end\":28033,\"start\":28032},{\"end\":28043,\"start\":28042},{\"end\":28059,\"start\":28058},{\"end\":28068,\"start\":28067},{\"end\":28375,\"start\":28374},{\"end\":28385,\"start\":28384},{\"end\":28402,\"start\":28401},{\"end\":28411,\"start\":28410},{\"end\":28423,\"start\":28422},{\"end\":28735,\"start\":28734},{\"end\":28750,\"start\":28749},{\"end\":29045,\"start\":29044},{\"end\":29047,\"start\":29046},{\"end\":29057,\"start\":29056},{\"end\":29059,\"start\":29058},{\"end\":29070,\"start\":29069},{\"end\":29072,\"start\":29071},{\"end\":29083,\"start\":29082},{\"end\":29095,\"start\":29094},{\"end\":29109,\"start\":29108},{\"end\":29117,\"start\":29116},{\"end\":29128,\"start\":29127},{\"end\":29139,\"start\":29138},{\"end\":29491,\"start\":29490},{\"end\":29504,\"start\":29503},{\"end\":29514,\"start\":29513},{\"end\":29798,\"start\":29797},{\"end\":29800,\"start\":29799},{\"end\":29810,\"start\":29809},{\"end\":29812,\"start\":29811},{\"end\":29823,\"start\":29822},{\"end\":30126,\"start\":30125},{\"end\":30133,\"start\":30132},{\"end\":30145,\"start\":30144},{\"end\":30402,\"start\":30401},{\"end\":30413,\"start\":30412},{\"end\":30425,\"start\":30424},{\"end\":30705,\"start\":30704},{\"end\":30717,\"start\":30716},{\"end\":30724,\"start\":30723},{\"end\":30985,\"start\":30984},{\"end\":30987,\"start\":30986},{\"end\":30999,\"start\":30998},{\"end\":31001,\"start\":31000},{\"end\":31263,\"start\":31262},{\"end\":31276,\"start\":31272},{\"end\":31285,\"start\":31284},{\"end\":31292,\"start\":31291},{\"end\":32018,\"start\":32017},{\"end\":32020,\"start\":32019},{\"end\":32031,\"start\":32030},{\"end\":32033,\"start\":32032},{\"end\":32040,\"start\":32039},{\"end\":32042,\"start\":32041},{\"end\":32048,\"start\":32045},{\"end\":32056,\"start\":32055},{\"end\":32058,\"start\":32057},{\"end\":32376,\"start\":32375},{\"end\":32391,\"start\":32390},{\"end\":32403,\"start\":32402},{\"end\":32414,\"start\":32413},{\"end\":32750,\"start\":32749},{\"end\":32762,\"start\":32761},{\"end\":32764,\"start\":32763},{\"end\":32774,\"start\":32773},{\"end\":33328,\"start\":33327},{\"end\":33337,\"start\":33336},{\"end\":33349,\"start\":33348},{\"end\":33358,\"start\":33357},{\"end\":33636,\"start\":33635},{\"end\":33647,\"start\":33646},{\"end\":33658,\"start\":33657},{\"end\":33668,\"start\":33667},{\"end\":33681,\"start\":33680},{\"end\":33690,\"start\":33689},{\"end\":33692,\"start\":33691},{\"end\":33701,\"start\":33700},{\"end\":33711,\"start\":33710},{\"end\":34082,\"start\":34081},{\"end\":34084,\"start\":34083},{\"end\":34094,\"start\":34093},{\"end\":34104,\"start\":34103},{\"end\":34106,\"start\":34105},{\"end\":34115,\"start\":34114},{\"end\":34125,\"start\":34124},{\"end\":34127,\"start\":34126},{\"end\":34490,\"start\":34489},{\"end\":34776,\"start\":34775},{\"end\":34784,\"start\":34783},{\"end\":35033,\"start\":35032},{\"end\":35035,\"start\":35034},{\"end\":35043,\"start\":35042},{\"end\":35045,\"start\":35044},{\"end\":35060,\"start\":35059},{\"end\":35069,\"start\":35068},{\"end\":35071,\"start\":35070},{\"end\":35083,\"start\":35082},{\"end\":35095,\"start\":35094},{\"end\":35097,\"start\":35096},{\"end\":35107,\"start\":35106},{\"end\":35118,\"start\":35117},{\"end\":35436,\"start\":35435},{\"end\":35438,\"start\":35437},{\"end\":35450,\"start\":35449},{\"end\":35462,\"start\":35461},{\"end\":35758,\"start\":35757},{\"end\":35764,\"start\":35763},{\"end\":36013,\"start\":36012},{\"end\":36015,\"start\":36014},{\"end\":36105,\"start\":36104},{\"end\":36107,\"start\":36106},{\"end\":36116,\"start\":36115},{\"end\":36457,\"start\":36456},{\"end\":36469,\"start\":36468},{\"end\":36479,\"start\":36475},{\"end\":36490,\"start\":36489},{\"end\":36501,\"start\":36500},{\"end\":36511,\"start\":36510},{\"end\":36521,\"start\":36520},{\"end\":36867,\"start\":36866},{\"end\":36875,\"start\":36874},{\"end\":36884,\"start\":36883},{\"end\":36892,\"start\":36891},{\"end\":36904,\"start\":36903},{\"end\":36916,\"start\":36915},{\"end\":36923,\"start\":36922},{\"end\":36933,\"start\":36932},{\"end\":36942,\"start\":36941},{\"end\":36950,\"start\":36949},{\"end\":37248,\"start\":37247},{\"end\":37250,\"start\":37249},{\"end\":37260,\"start\":37259},{\"end\":37476,\"start\":37475},{\"end\":37486,\"start\":37485},{\"end\":37495,\"start\":37494},{\"end\":37504,\"start\":37503},{\"end\":37513,\"start\":37512},{\"end\":37525,\"start\":37524},{\"end\":37535,\"start\":37534},{\"end\":37546,\"start\":37545},{\"end\":37553,\"start\":37552},{\"end\":37567,\"start\":37566},{\"end\":37971,\"start\":37970},{\"end\":37982,\"start\":37981},{\"end\":37995,\"start\":37994},{\"end\":38003,\"start\":38002},{\"end\":38005,\"start\":38004},{\"end\":38016,\"start\":38015},{\"end\":38351,\"start\":38350},{\"end\":38363,\"start\":38362},{\"end\":38817,\"start\":38816},{\"end\":38824,\"start\":38823},{\"end\":38836,\"start\":38835},{\"end\":39508,\"start\":39507},{\"end\":39519,\"start\":39518},{\"end\":39528,\"start\":39527},{\"end\":39993,\"start\":39992},{\"end\":39999,\"start\":39998},{\"end\":40001,\"start\":40000},{\"end\":40012,\"start\":40011},{\"end\":40665,\"start\":40664},{\"end\":40998,\"start\":40997},{\"end\":41010,\"start\":41009},{\"end\":41277,\"start\":41276},{\"end\":41287,\"start\":41286},{\"end\":41294,\"start\":41293},{\"end\":41296,\"start\":41295},{\"end\":41313,\"start\":41312},{\"end\":41326,\"start\":41325},{\"end\":41334,\"start\":41333},{\"end\":41346,\"start\":41345},{\"end\":41354,\"start\":41353},{\"end\":41830,\"start\":41829},{\"end\":41834,\"start\":41831},{\"end\":41844,\"start\":41843}]", "bib_author_last_name": "[{\"end\":28030,\"start\":28027},{\"end\":28040,\"start\":28034},{\"end\":28056,\"start\":28044},{\"end\":28065,\"start\":28060},{\"end\":28079,\"start\":28069},{\"end\":28382,\"start\":28376},{\"end\":28399,\"start\":28386},{\"end\":28408,\"start\":28403},{\"end\":28420,\"start\":28412},{\"end\":28437,\"start\":28424},{\"end\":28747,\"start\":28736},{\"end\":28757,\"start\":28751},{\"end\":29054,\"start\":29048},{\"end\":29067,\"start\":29060},{\"end\":29080,\"start\":29073},{\"end\":29092,\"start\":29084},{\"end\":29106,\"start\":29096},{\"end\":29114,\"start\":29110},{\"end\":29125,\"start\":29118},{\"end\":29136,\"start\":29129},{\"end\":29148,\"start\":29140},{\"end\":29501,\"start\":29492},{\"end\":29511,\"start\":29505},{\"end\":29522,\"start\":29515},{\"end\":29807,\"start\":29801},{\"end\":29820,\"start\":29813},{\"end\":29831,\"start\":29824},{\"end\":30130,\"start\":30127},{\"end\":30142,\"start\":30134},{\"end\":30153,\"start\":30146},{\"end\":30410,\"start\":30403},{\"end\":30422,\"start\":30414},{\"end\":30433,\"start\":30426},{\"end\":30714,\"start\":30706},{\"end\":30721,\"start\":30718},{\"end\":30731,\"start\":30725},{\"end\":30996,\"start\":30988},{\"end\":31010,\"start\":31002},{\"end\":31270,\"start\":31264},{\"end\":31282,\"start\":31277},{\"end\":31289,\"start\":31286},{\"end\":31302,\"start\":31293},{\"end\":32028,\"start\":32021},{\"end\":32037,\"start\":32034},{\"end\":32053,\"start\":32049},{\"end\":32063,\"start\":32059},{\"end\":32388,\"start\":32377},{\"end\":32400,\"start\":32392},{\"end\":32411,\"start\":32404},{\"end\":32423,\"start\":32415},{\"end\":32759,\"start\":32751},{\"end\":32771,\"start\":32765},{\"end\":32782,\"start\":32775},{\"end\":33334,\"start\":33329},{\"end\":33346,\"start\":33338},{\"end\":33355,\"start\":33350},{\"end\":33366,\"start\":33359},{\"end\":33644,\"start\":33637},{\"end\":33655,\"start\":33648},{\"end\":33665,\"start\":33659},{\"end\":33678,\"start\":33669},{\"end\":33687,\"start\":33682},{\"end\":33698,\"start\":33693},{\"end\":33708,\"start\":33702},{\"end\":33722,\"start\":33712},{\"end\":34091,\"start\":34085},{\"end\":34101,\"start\":34095},{\"end\":34112,\"start\":34107},{\"end\":34122,\"start\":34116},{\"end\":34137,\"start\":34128},{\"end\":34501,\"start\":34491},{\"end\":34781,\"start\":34777},{\"end\":34792,\"start\":34785},{\"end\":35040,\"start\":35036},{\"end\":35057,\"start\":35046},{\"end\":35066,\"start\":35061},{\"end\":35080,\"start\":35072},{\"end\":35092,\"start\":35084},{\"end\":35104,\"start\":35098},{\"end\":35115,\"start\":35108},{\"end\":35126,\"start\":35119},{\"end\":35447,\"start\":35439},{\"end\":35459,\"start\":35451},{\"end\":35472,\"start\":35463},{\"end\":35761,\"start\":35759},{\"end\":35767,\"start\":35765},{\"end\":36023,\"start\":36016},{\"end\":36113,\"start\":36108},{\"end\":36125,\"start\":36117},{\"end\":36466,\"start\":36458},{\"end\":36473,\"start\":36470},{\"end\":36487,\"start\":36480},{\"end\":36498,\"start\":36491},{\"end\":36508,\"start\":36502},{\"end\":36518,\"start\":36512},{\"end\":36529,\"start\":36522},{\"end\":36872,\"start\":36868},{\"end\":36881,\"start\":36876},{\"end\":36889,\"start\":36885},{\"end\":36901,\"start\":36893},{\"end\":36913,\"start\":36905},{\"end\":36920,\"start\":36917},{\"end\":36930,\"start\":36924},{\"end\":36939,\"start\":36934},{\"end\":36947,\"start\":36943},{\"end\":36960,\"start\":36951},{\"end\":37257,\"start\":37251},{\"end\":37263,\"start\":37261},{\"end\":37483,\"start\":37477},{\"end\":37492,\"start\":37487},{\"end\":37501,\"start\":37496},{\"end\":37510,\"start\":37505},{\"end\":37522,\"start\":37514},{\"end\":37532,\"start\":37526},{\"end\":37543,\"start\":37536},{\"end\":37550,\"start\":37547},{\"end\":37564,\"start\":37554},{\"end\":37574,\"start\":37568},{\"end\":37979,\"start\":37972},{\"end\":37992,\"start\":37983},{\"end\":38000,\"start\":37996},{\"end\":38013,\"start\":38006},{\"end\":38021,\"start\":38017},{\"end\":38360,\"start\":38352},{\"end\":38371,\"start\":38364},{\"end\":38821,\"start\":38818},{\"end\":38833,\"start\":38825},{\"end\":38844,\"start\":38837},{\"end\":39516,\"start\":39509},{\"end\":39525,\"start\":39520},{\"end\":39535,\"start\":39529},{\"end\":39996,\"start\":39994},{\"end\":40009,\"start\":40002},{\"end\":40019,\"start\":40013},{\"end\":40668,\"start\":40666},{\"end\":41007,\"start\":40999},{\"end\":41018,\"start\":41011},{\"end\":41284,\"start\":41278},{\"end\":41291,\"start\":41288},{\"end\":41310,\"start\":41297},{\"end\":41323,\"start\":41314},{\"end\":41331,\"start\":41327},{\"end\":41343,\"start\":41335},{\"end\":41351,\"start\":41347},{\"end\":41366,\"start\":41355},{\"end\":41841,\"start\":41835},{\"end\":41851,\"start\":41845}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":28238,\"start\":27942},{\"attributes\":{\"id\":\"b1\"},\"end\":28654,\"start\":28240},{\"attributes\":{\"id\":\"b2\"},\"end\":28942,\"start\":28656},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":49320361},\"end\":29427,\"start\":28944},{\"attributes\":{\"id\":\"b4\"},\"end\":29726,\"start\":29429},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":7357141},\"end\":30052,\"start\":29728},{\"attributes\":{\"doi\":\"arXiv:1805.09366\",\"id\":\"b6\"},\"end\":30335,\"start\":30054},{\"attributes\":{\"doi\":\"arXiv:1712.00069\",\"id\":\"b7\"},\"end\":30610,\"start\":30337},{\"attributes\":{\"doi\":\"arXiv:1804.06440\",\"id\":\"b8\"},\"end\":30933,\"start\":30612},{\"attributes\":{\"id\":\"b9\"},\"end\":31179,\"start\":30935},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":52967399},\"end\":31916,\"start\":31181},{\"attributes\":{\"doi\":\"arXiv:1511.02436\",\"id\":\"b11\"},\"end\":32293,\"start\":31918},{\"attributes\":{\"doi\":\"arXiv:1811.12254\",\"id\":\"b12\"},\"end\":32633,\"start\":32295},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":891184},\"end\":33259,\"start\":32635},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3397190},\"end\":33606,\"start\":33261},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":13756489},\"end\":33980,\"start\":33608},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":32595556},\"end\":34395,\"start\":33982},{\"attributes\":{\"id\":\"b17\"},\"end\":34649,\"start\":34397},{\"attributes\":{\"id\":\"b18\"},\"end\":34951,\"start\":34651},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":9506186},\"end\":35365,\"start\":34953},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":16918336},\"end\":35687,\"start\":35367},{\"attributes\":{\"id\":\"b21\"},\"end\":36008,\"start\":35689},{\"attributes\":{\"id\":\"b22\"},\"end\":36059,\"start\":36010},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":46465620},\"end\":36358,\"start\":36061},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":36544389},\"end\":36791,\"start\":36360},{\"attributes\":{\"doi\":\"abs/1910.03771\",\"id\":\"b25\"},\"end\":37201,\"start\":36793},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b26\"},\"end\":37403,\"start\":37203},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":202786778},\"end\":37891,\"start\":37405},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":16447573},\"end\":38294,\"start\":37893},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":53081632},\"end\":38727,\"start\":38296},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":51938927},\"end\":39451,\"start\":38729},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":195477534},\"end\":39903,\"start\":39453},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":14414445},\"end\":40591,\"start\":39905},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":17189214},\"end\":40877,\"start\":40593},{\"attributes\":{\"id\":\"b34\"},\"end\":41214,\"start\":40879},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":5151364},\"end\":41797,\"start\":41216},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":5855042},\"end\":42031,\"start\":41799}]", "bib_title": "[{\"end\":28732,\"start\":28656},{\"end\":29042,\"start\":28944},{\"end\":29795,\"start\":29728},{\"end\":31260,\"start\":31181},{\"end\":32747,\"start\":32635},{\"end\":33325,\"start\":33261},{\"end\":33633,\"start\":33608},{\"end\":34079,\"start\":33982},{\"end\":35030,\"start\":34953},{\"end\":35433,\"start\":35367},{\"end\":35755,\"start\":35689},{\"end\":36102,\"start\":36061},{\"end\":36454,\"start\":36360},{\"end\":36864,\"start\":36793},{\"end\":37473,\"start\":37405},{\"end\":37968,\"start\":37893},{\"end\":38348,\"start\":38296},{\"end\":38814,\"start\":38729},{\"end\":39505,\"start\":39453},{\"end\":39990,\"start\":39905},{\"end\":40662,\"start\":40593},{\"end\":40995,\"start\":40879},{\"end\":41274,\"start\":41216},{\"end\":41827,\"start\":41799}]", "bib_author": "[{\"end\":28032,\"start\":28025},{\"end\":28042,\"start\":28032},{\"end\":28058,\"start\":28042},{\"end\":28067,\"start\":28058},{\"end\":28081,\"start\":28067},{\"end\":28384,\"start\":28374},{\"end\":28401,\"start\":28384},{\"end\":28410,\"start\":28401},{\"end\":28422,\"start\":28410},{\"end\":28439,\"start\":28422},{\"end\":28749,\"start\":28734},{\"end\":28759,\"start\":28749},{\"end\":29056,\"start\":29044},{\"end\":29069,\"start\":29056},{\"end\":29082,\"start\":29069},{\"end\":29094,\"start\":29082},{\"end\":29108,\"start\":29094},{\"end\":29116,\"start\":29108},{\"end\":29127,\"start\":29116},{\"end\":29138,\"start\":29127},{\"end\":29150,\"start\":29138},{\"end\":29503,\"start\":29490},{\"end\":29513,\"start\":29503},{\"end\":29524,\"start\":29513},{\"end\":29809,\"start\":29797},{\"end\":29822,\"start\":29809},{\"end\":29833,\"start\":29822},{\"end\":30132,\"start\":30125},{\"end\":30144,\"start\":30132},{\"end\":30155,\"start\":30144},{\"end\":30412,\"start\":30401},{\"end\":30424,\"start\":30412},{\"end\":30435,\"start\":30424},{\"end\":30716,\"start\":30704},{\"end\":30723,\"start\":30716},{\"end\":30733,\"start\":30723},{\"end\":30998,\"start\":30984},{\"end\":31012,\"start\":30998},{\"end\":31272,\"start\":31262},{\"end\":31284,\"start\":31272},{\"end\":31291,\"start\":31284},{\"end\":31304,\"start\":31291},{\"end\":32030,\"start\":32017},{\"end\":32039,\"start\":32030},{\"end\":32045,\"start\":32039},{\"end\":32055,\"start\":32045},{\"end\":32065,\"start\":32055},{\"end\":32390,\"start\":32375},{\"end\":32402,\"start\":32390},{\"end\":32413,\"start\":32402},{\"end\":32425,\"start\":32413},{\"end\":32761,\"start\":32749},{\"end\":32773,\"start\":32761},{\"end\":32784,\"start\":32773},{\"end\":33336,\"start\":33327},{\"end\":33348,\"start\":33336},{\"end\":33357,\"start\":33348},{\"end\":33368,\"start\":33357},{\"end\":33646,\"start\":33635},{\"end\":33657,\"start\":33646},{\"end\":33667,\"start\":33657},{\"end\":33680,\"start\":33667},{\"end\":33689,\"start\":33680},{\"end\":33700,\"start\":33689},{\"end\":33710,\"start\":33700},{\"end\":33724,\"start\":33710},{\"end\":34093,\"start\":34081},{\"end\":34103,\"start\":34093},{\"end\":34114,\"start\":34103},{\"end\":34124,\"start\":34114},{\"end\":34139,\"start\":34124},{\"end\":34503,\"start\":34489},{\"end\":34783,\"start\":34775},{\"end\":34794,\"start\":34783},{\"end\":35042,\"start\":35032},{\"end\":35059,\"start\":35042},{\"end\":35068,\"start\":35059},{\"end\":35082,\"start\":35068},{\"end\":35094,\"start\":35082},{\"end\":35106,\"start\":35094},{\"end\":35117,\"start\":35106},{\"end\":35128,\"start\":35117},{\"end\":35449,\"start\":35435},{\"end\":35461,\"start\":35449},{\"end\":35474,\"start\":35461},{\"end\":35763,\"start\":35757},{\"end\":35769,\"start\":35763},{\"end\":36025,\"start\":36012},{\"end\":36115,\"start\":36104},{\"end\":36127,\"start\":36115},{\"end\":36468,\"start\":36456},{\"end\":36475,\"start\":36468},{\"end\":36489,\"start\":36475},{\"end\":36500,\"start\":36489},{\"end\":36510,\"start\":36500},{\"end\":36520,\"start\":36510},{\"end\":36531,\"start\":36520},{\"end\":36874,\"start\":36866},{\"end\":36883,\"start\":36874},{\"end\":36891,\"start\":36883},{\"end\":36903,\"start\":36891},{\"end\":36915,\"start\":36903},{\"end\":36922,\"start\":36915},{\"end\":36932,\"start\":36922},{\"end\":36941,\"start\":36932},{\"end\":36949,\"start\":36941},{\"end\":36962,\"start\":36949},{\"end\":37259,\"start\":37247},{\"end\":37265,\"start\":37259},{\"end\":37485,\"start\":37475},{\"end\":37494,\"start\":37485},{\"end\":37503,\"start\":37494},{\"end\":37512,\"start\":37503},{\"end\":37524,\"start\":37512},{\"end\":37534,\"start\":37524},{\"end\":37545,\"start\":37534},{\"end\":37552,\"start\":37545},{\"end\":37566,\"start\":37552},{\"end\":37576,\"start\":37566},{\"end\":37981,\"start\":37970},{\"end\":37994,\"start\":37981},{\"end\":38002,\"start\":37994},{\"end\":38015,\"start\":38002},{\"end\":38023,\"start\":38015},{\"end\":38362,\"start\":38350},{\"end\":38373,\"start\":38362},{\"end\":38823,\"start\":38816},{\"end\":38835,\"start\":38823},{\"end\":38846,\"start\":38835},{\"end\":39518,\"start\":39507},{\"end\":39527,\"start\":39518},{\"end\":39537,\"start\":39527},{\"end\":39998,\"start\":39992},{\"end\":40011,\"start\":39998},{\"end\":40021,\"start\":40011},{\"end\":40670,\"start\":40664},{\"end\":41009,\"start\":40997},{\"end\":41020,\"start\":41009},{\"end\":41286,\"start\":41276},{\"end\":41293,\"start\":41286},{\"end\":41312,\"start\":41293},{\"end\":41325,\"start\":41312},{\"end\":41333,\"start\":41325},{\"end\":41345,\"start\":41333},{\"end\":41353,\"start\":41345},{\"end\":41368,\"start\":41353},{\"end\":41843,\"start\":41829},{\"end\":41853,\"start\":41843}]", "bib_venue": "[{\"end\":31575,\"start\":31448},{\"end\":32971,\"start\":32886},{\"end\":38532,\"start\":38461},{\"end\":39117,\"start\":38990},{\"end\":39698,\"start\":39626},{\"end\":40292,\"start\":40165},{\"end\":41467,\"start\":41426},{\"end\":28023,\"start\":27942},{\"end\":28372,\"start\":28240},{\"end\":28789,\"start\":28759},{\"end\":29159,\"start\":29150},{\"end\":29488,\"start\":29429},{\"end\":29863,\"start\":29833},{\"end\":30123,\"start\":30054},{\"end\":30399,\"start\":30337},{\"end\":30702,\"start\":30612},{\"end\":30982,\"start\":30935},{\"end\":31446,\"start\":31304},{\"end\":32015,\"start\":31918},{\"end\":32373,\"start\":32295},{\"end\":32884,\"start\":32784},{\"end\":33408,\"start\":33368},{\"end\":33773,\"start\":33724},{\"end\":34160,\"start\":34139},{\"end\":34487,\"start\":34397},{\"end\":34773,\"start\":34651},{\"end\":35136,\"start\":35128},{\"end\":35499,\"start\":35474},{\"end\":35835,\"start\":35769},{\"end\":36185,\"start\":36127},{\"end\":36549,\"start\":36531},{\"end\":36981,\"start\":36976},{\"end\":37245,\"start\":37203},{\"end\":37625,\"start\":37576},{\"end\":38072,\"start\":38023},{\"end\":38459,\"start\":38373},{\"end\":38988,\"start\":38846},{\"end\":39624,\"start\":39537},{\"end\":40163,\"start\":40021},{\"end\":40713,\"start\":40670},{\"end\":41029,\"start\":41020},{\"end\":41424,\"start\":41368},{\"end\":41889,\"start\":41853}]"}}}, "year": 2023, "month": 12, "day": 17}