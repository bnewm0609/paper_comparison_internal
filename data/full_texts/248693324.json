{"id": 248693324, "updated": "2023-10-05 14:31:57.925", "metadata": {"title": "Quantum Self-Attention Neural Networks for Text Classification", "authors": "[{\"first\":\"Guangxi\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Xuanqiang\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Xin\",\"last\":\"Wang\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "An emerging direction of quantum computing is to establish meaningful quantum applications in various fields of artificial intelligence, including natural language processing (NLP). Although some efforts based on syntactic analysis have opened the door to research in Quantum NLP (QNLP), limitations such as heavy syntactic preprocessing and syntax-dependent network architecture make them impracticable on larger and real-world data sets. In this paper, we propose a new simple network architecture, called the quantum self-attention neural network (QSANN), which can compensate for these limitations. Specifically, we introduce the self-attention mechanism into quantum neural networks and then utilize a Gaussian projected quantum self-attention serving as a sensible quantum version of self-attention. As a result, QSANN is effective and scalable on larger data sets and has the desirable property of being implementable on near-term quantum devices. In particular, our QSANN outperforms the best existing QNLP model based on syntactic analysis as well as a simple classical self-attention neural network in numerical experiments of text classification tasks on public data sets. We further show that our method exhibits robustness to low-level quantum noises and showcases resilience to quantum neural network architectures.", "fields_of_study": "[\"Physics\",\"Computer Science\"]", "external_ids": {"arxiv": "2205.05625", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2205-05625", "doi": "10.48550/arxiv.2205.05625"}}, "content": {"source": {"pdf_hash": "339156f79f2f5d06b85726aba84a24ebae0e9954", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2205.05625v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "541a5458db79ad71ed6f27d5eaa132be8505910d", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/339156f79f2f5d06b85726aba84a24ebae0e9954.txt", "contents": "\nQuantum Self-Attention Neural Networks for Text Classification\n\n\nGuangxi Li \nInstitute for Quantum Computing\nBaidu Research\n100193BeijingChina\n\nCentre for Quantum Software and Information\nUniversity of Technology Sydney\n2007NSWAustralia\n\nXuanqiang Zhao \nInstitute for Quantum Computing\nBaidu Research\n100193BeijingChina\n\nDepartment of Computer Science\nQICI\nThe University of Hong Kong\nHong KongChina\n\nXin Wang \nInstitute for Quantum Computing\nBaidu Research\n100193BeijingChina\n\nThrust of Artificial Intelligence\nHong Kong University of Science and Technology (Guangzhou)\nInformation HubNanshaChina\n\nQuantum Self-Attention Neural Networks for Text Classification\n(Dated: September 29, 2023)\nAn emerging direction of quantum computing is to establish meaningful quantum applications in various fields of artificial intelligence, including natural language processing (NLP). Although some efforts based on syntactic analysis have opened the door to research in Quantum NLP (QNLP), limitations such as heavy syntactic preprocessing and syntax-dependent network architecture make them impracticable on larger and real-world data sets. In this paper, we propose a new simple network architecture, called the quantum self-attention neural network (QSANN), which can compensate for these limitations. Specifically, we introduce the self-attention mechanism into quantum neural networks and then utilize a Gaussian projected quantum self-attention serving as a sensible quantum version of self-attention. As a result, QSANN is effective and scalable on larger data sets and has the desirable property of being implementable on near-term quantum devices. In particular, our QSANN outperforms the best existing QNLP model based on syntactic analysis as well as a simple classical self-attention neural network in numerical experiments of text classification tasks on public data sets. We further show that our method exhibits robustness to low-level quantum noises and showcases resilience to quantum neural network architectures.\n\nI. INTRODUCTION\n\nQuantum computing is a promising paradigm [1] for fast computations that can provide substantial advantages in solving valuable problems [2][3][4][5][6]. With major academic and industry efforts on developing quantum algorithms and quantum hardware, it has led to an increasing number of powerful applications in areas including optimization [7], cryptography [8], chemistry [9,10], and machine learning [6,[11][12][13].\n\nQuantum devices available currently known as the noisy intermediate-scale quantum (NISQ) devices [14] have up to a few hundred physical qubits. They are affected by coherent and incoherent noise, making the practical implementation of many advantageous quantum algorithms less feasible. But such devices with 50-100 qubits already allow one to achieve quantum advantage against the most powerful classical supercomputers on certain carefully designed tasks [15,16]. To explore practical applications with near-term quantum devices, plenty of NISQ algorithms [17][18][19] appear to be the best hope for obtaining quantum advantage in fields such as quantum chemistry [20], optimization [21], and machine learning [22][23][24][25][26][27]. In particular, those algorithms dealing with machine learning problems, by employing parameterized quantum circuits (PQCs) [28] (also called quantum neural networks (QNNs) [29]), show great potential in the field of quantum machine learning (QML). See [30][31][32][33][34][35][36][37][38][39][40][41] for some recent progresses on the theory and applications of quantum neural networks in many directions. However, in artificial intelligence (AI), the study of QML in the NISQ era is still in its infancy. Thus it is desirable to explore more QML algorithms exploiting the power that lies within the NISQ devices.\n\nNatural language processing (NLP) is a key subfield of AI that aims to give machines the ability to understand human language. Common NLP tasks include speech recognition, machine translation, text classification, etc., many of which have greatly facilitated our life. Due to human language's high complexity and flexibility, NLP tasks are generally challenging to implement. Thus, it is natural to think about whether and how quantum computing can enhance machines' performance on NLP. Some works focus on quantum-inspired language models [42][43][44][45] with borrowed ideas from quantum mechanics. Another approach, known as quantum natural language processing (QNLP), seeks to develop quantum-native NLP models that can be implemented on quantum devices [46][47][48][49]. Most of these QNLP proposals, though at the frontier, lack scalability as they are based on syntactic analysis, which is a preprocessing task requiring significant effort, especially for large data sets. Furthermore, these syntax-based methods employ different PQCs for sentences with different syntactical structures and thus are not flexible enough to process the innumerable complex expressions possible in human language.\n\nTo overcome these drawbacks in current QNLP models, we propose the quantum self-attention neural network (QSANN), where the self-attention mechanism is introduced into quantum neural networks. Our motivation comes from the excellent performance of self-attention on various NLP tasks such as language modeling [50], machine translation [51], question answering [52], and text classification [53]. We also note that a recently proposed method [54] for quantum state tomography, an important task in quantum computing, adopts the self-attention mechanism and achieves decent results.\n\nIn each quantum self-attention layer of QSANN, we first encode the inputs into high-dimensional quantum states, then apply PQCs on them according to the layout of the self-attention neural networks, and finally adopt a Gaussian projected quantum self-attention (GPQSA) to obtain the output effectively. To evaluate the performance of our model, we conduct numerical experiments of text classification with different data sets. The results show that QSANN outperforms the currently best known QNLP model as well as a simple classical self-attention neural network on test accuracy, implying potential quantum advantages of our method. Our contributions are multi-fold:\n\n\u2022 Our proposal is the first QNLP algorithm with a detailed circuit implementation scheme based on the self-attention mechanism. This method can be implemented on NISQ devices and is more practicable on large data sets compared with previously known QNLP methods based on syntactic analysis.\n\n\u2022 In QSANN, we introduce the Gaussian projected quantum self-attention, which can efficiently dig out the correlations between words in high-dimensional quantum feature space. Furthermore, visualization of self-attention coefficients on text classification tasks confirms its ability to focus on the most relevant words.\n\n\u2022 We experimentally demonstrate that QSANN outperforms existing QNLP methods based on syntactic analysis [55] and simple classical self-attention neural networks on several public data sets for text classification. Numerical results also imply that QSANN is resilient to both quantum noises and quantum neural network architectures.\n\n\nA. Preliminaries and Notations\n\na. Quantum Basics Here, some basic concepts about quantum computing necessary for this paper are briefly introduced (for more details, see [56]). In quantum computing, quantum information is usually represented by n-qubit (pure) quantum states over Hilbert space C 2 n . In particular, a pure quantum state could be represented by a unit vector |\u03c8\u27e9 \u2208 C 2 n (or \u27e8\u03c8|), where the ket notation |\u27e9 denotes a column vector and the bra notation \u27e8\u03c8| = |\u03c8\u27e9 \u2020 with \u2020 referring to conjugate transpose denotes a row vector.\n\nThe evolution of a pure quantum state |\u03c8\u27e9 is mathematically described by applying a quantum circuit (or a quantum gate), i.e., |\u03c8 \u2032 \u27e9 = U |\u03c8\u27e9, where U is the unitary operator (matrix) representing the quantum circuit and |\u03c8 \u2032 \u27e9 is the quantum state after evolution. Common single-qubit quantum gates include Hadamard gate H and Pauli operators\nH := 1 \u221a 2 1 1 1 \u22121 , X := 0 1 1 0 , Y := 0 \u2212i i 0 , Z := 1 0 0 \u22121 ,(1)\nand their corresponding rotation gates denoted by R P (\u03b8) := exp(\u2212i\u03b8P/2) = cos \u03b8 2 I \u2212 i sin \u03b8 2 P , where the rotation angle \u03b8 \u2208 [0, 2\u03c0) and P \u2208 {X, Y, Z}. In this paper, multiple-qubit quantum gates mainly include the identity gate I, the CNOT gate and the tensor product of single-qubit gates, e.g., Z \u2297 Z, Z \u2297 I, Z \u2297n and so on.\n\nQuantum measurement is a way to extract classical information from a quantum state. For instance, given a quantum state |\u03c8\u27e9 and an observable O, one could design quantum measurements to obtain the information \u27e8\u03c8| O |\u03c8\u27e9. This work focuses on the hardware-efficient Pauli measurements, i.e., setting O as Pauli operators or their tensor products. For instance, we could choose\nZ 1 = Z \u2297 I \u2297(n\u22121) , X 2 = I \u2297 X \u2297 I \u2297(n\u22122) , Z 1 Z 2 = Z \u2297 Z \u2297 I \u2297(n\u22122)\n, etc., with n qubits in total.\n\nb. Text Classification As one of the central and basic tasks in NLP field, text classification is to assign a given text sequence to one of the predefined categories. Examples of text classification tasks considered in this paper include topic classification and sentiment analysis. A commonly adopted approach in machine learning is to train a model with a set of pre-labeled sequences. When fed a new sequence, the trained model will be able to predict its category based on the experience learned from the training data set. c. Self-Attention Mechanism In a self-attention neural network layer [51], the input data {x s \u2208 R d } S s=1 are linearly mapped via three weight matrices, i.e., query W q \u2208 R d\u00d7d , key W k \u2208 R d\u00d7d and value W v \u2208 R d\u00d7d , to three parts W q x s , W k x s , W v x s , respectively, and by applying the inner product on the query and key parts, the output is computed as\ny s = S j=1 a s,j \u00b7 W v x j with a s,j = e x \u22a4 s W \u22a4 q W k xj S l=1 e x \u22a4 s W \u22a4 q W k x l ,(2)\nwhere a s,j denote the self-attention coefficients. } are used as the rotation angles of quantum ansatzes (purple dashed boxes) to encode them into their corresponding quantum states {|\u03c8s\u27e9}. Then, a set of three ansatzes (in red dashed boxes) representing query, key, and value is applied to each state. Note that it is the same set of ansatzes applied to all the input states. On classical computers, the measurement outputs of the query part \u27e8Zq\u27e9s and the key part \u27e8Z k \u27e9j are computed through a Gaussian function to obtain the quantum self-attention coefficients \u03b1s,j (green circles); we calculate classically weighted sums of the measurement outputs of the value part (small colored squares) and add the inputs to get the outputs {y \n\n\nII. METHOD\n\nIn this section, we will introduce the QSANN in detail, which mainly consists of quantum self-attention layer (QSAL), loss function, analytical gradients and analysis.\n\n\nA. Quantum Self-Attention Layer\n\nIn the classical self-attention mechanism [51], there are mainly three components (vectors), i.e., queries, keys and values, where queries and keys are computed as weights assigned to corresponding values to obtain final outputs. Inspired by this mechanism, in QSAL we design the quantum analogs of these components. The overall picture of QSAL is illustrated in Fig. 1.\n\nFor the classical input data {y (l\u22121) s \u2208 R d } of the l-th QSAL, we first use a quantum ansatz U enc to encode them into an n-qubit quantum Hilbert space, i.e.,\n|\u03c8 s \u27e9 = U enc (y (l\u22121) s )H \u2297n |0 n \u27e9 , 1 \u2264 s \u2264 S,(3)\nwhere H denotes the Hadamard gate and S denotes the number of input vectors in a data sample. Then we use another three quantum ansatzes, i.e., U q , U k , U v with parameters \u03b8 q , \u03b8 k , \u03b8 v , to represent the query, key and value parts, respectively. Concretely, for each input state |\u03c8 s \u27e9, we denote by \u27e8Z q \u27e9 s and \u27e8Z k \u27e9 s the Pauli-Z 1 measurement outputs\nRx(\u03b80,1) Ry(\u03b80,5) \u2022 Ry(\u03b81,1) Rx(\u03b80,2) Ry(\u03b80,6) \u2022 Ry(\u03b81,2) Rx(\u03b80,3) Ry(\u03b80,7) \u2022 Ry(\u03b81,3) Rx(\u03b80,4) Ry(\u03b80,8) \u2022 Ry(\u03b81,4) \u00d7D FIG. 2:\nThe ansatz used in QSANN. The first two columns denote the Rx-Ry rotations on each single-qubit subspace, followed by repeated CNOT gates and single-qubit Ry rotations. The block circuit in the dashed box is repeated D times to enhance the expressive power of the ansatz.\n\nof the query and key parts, respectively, where\n\u27e8Z q \u27e9 s := \u27e8\u03c8 s | U \u2020 q (\u03b8 q )Z 1 U q (\u03b8 q ) |\u03c8 s \u27e9 , \u27e8Z k \u27e9 s := \u27e8\u03c8 s | U \u2020 k (\u03b8 k )Z 1 U k (\u03b8 k ) |\u03c8 s \u27e9 .(4)\nThe measurement outputs of the value part are represented by a d-dimensional vector\no s := \u27e8P 1 \u27e9 s \u27e8P 2 \u27e9 s \u00b7 \u00b7 \u00b7 \u27e8P d \u27e9 s \u22a4 ,(5)\nwhere\n\u27e8P j \u27e9 s = \u27e8\u03c8 s | U \u2020 v (\u03b8 v )P j U v (\u03b8 v ) |\u03c8 s \u27e9. Here, each P j \u2208 {I, X, Y, Z} \u2297n denotes a Pauli observable.\nFinally, by combining Eqs. (4) and (5), the classical output {y (l) s \u2208 R d } of the l-th QSAL are computed as follows:\ny (l) s = y (l\u22121) s + S j=1\u03b1 s,j \u00b7 o j ,(6)\nwhere\u03b1 s,j denotes the normalized quantum self-attention coefficient between the s-th and the j-th input vectors and is calculated by the corresponding query and key parts:\n\u03b1 s,j = \u03b1 s,j S m=1 \u03b1 s,m with \u03b1 s,j := e \u2212(\u27e8Zq\u27e9s\u2212\u27e8Z k \u27e9j ) 2 .(7)\nHere in Eq. (6), we adopt a residual scheme when computing the output, which is analogous to [51]. a. Gaussian Projected Quantum Self-Attention When designing a quantum version of self-attention, a natural and direct extension of the inner-product self-attention to consider is \u03b1 s,j := | \u27e8\u03c8 s | U \u2020 q U k |\u03c8 j \u27e9 | 2 . However, due to the unitary nature of quantum circuits, \u27e8\u03c8 s | U \u2020 q U k can be regarded as rotating |\u03c8 s \u27e9 by an angle, which makes it difficult for |\u03c8 s \u27e9 to simultaneously correlate those |\u03c8 j \u27e9 that are far away. In a word, this direct extension is not suitable or reasonable for working as the quantum self-attention. Instead, the particular quantum self-attention proposed in Eq. (7), which we call Gaussian projected quantum self-attention (GPQSA), could overcome the above drawback. In GPQSA, the states U q |\u03c8 s \u27e9 (and U k |\u03c8 j \u27e9) in large quantum Hilbert space are projected to classical representations \u27e8Z q \u27e9 s (and \u27e8Z k \u27e9 j ) in one-dimensional 1 classical space via quantum measurements and a Gaussian function is applied to these classical representations. As U q and U k are separated, it is pretty easy to correlate |\u03c8 s \u27e9 to any |\u03c8 j \u27e9, making GPQSA more suitable to serve as a quantum self-attention. Here, we utilize the Gaussian function [27,57] mainly because it contains infinite-dimensional feature space and is well-studied in classical machine learning. Numerical experiments also verify our choice of Gaussian function. We also note that other choices for building quantum self-attention are also worth future study.\n\nRemark. During the preparation of this manuscript, we became aware that Ref. [58] also made initial attempts to employ the attention mechanism in QNNs. In that work, the authors mentioned a possible quantum extension towards a quantum Transformer where the straightforward inner-product self-attention is adopted. As discussed above, the inner-product selfattention may not be reasonable for dealing with quantum data. In this work, we present that GPQSA is more suitable for the quantum version of self-attention and show the validity of our method via numerical experiments on several public data sets. b. Ansatz Selection In QSAL, we employ multiple ansatzes for the various components, i.e., data encoding, query, key and value. Hence, we give a brief review of it here.\n\nIn general, an ansatz, a.k.a. parameterized quantum circuit [28], has the form\nU (\u03b8) = j U j (\u03b8 j )V j , where U j (\u03b8 j ) = exp(\u2212i\u03b8 j P j /2)\nand V j denotes a fixed operator such as Identity, CNOT and so on. Here, P j denotes a Pauli operator. Due to the numerous choices of the form of V j , various kinds of ansatzes can be used. In this paper, we use the strongly entangled ansatz [23] shown in Fig. 2 in QSAL. This circuit has n(D + 2) parameters in total for n qubits and D repeated layers.\n\n\nB. Loss Function\n\nConsider the data set D :\n= {(x m;1 , x m;2 , . . ., x m;Sm ), y m } Ns m=1 ,\nwhere there are in total N s sequences or samples and each has S m words with a label y m \u2208 {0, 1}. Here, we assume each word is embedded as a d-dimensional vector, i.e., x m;s \u2208 R d . The whole procedure of QSANN is depicted in Fig. 3, which mainly consists of L QSALs to extract hidden features and one fully-connected layer to complete the binary prediction task. Here, the mean squared error [59] is employed as the loss function:\nL (\u0398, w, b; D) = 1 2N s Ns m=1 (\u0177 m \u2212 y m ) 2 + RegTerm,(8)\nwhere the predicted value\u0177 m is defined as\u0177 m := \u03c3 w \u22a4 \u00b7 1 Sm Sm s=1 y (L) m;s + b with w \u2208 R d and b \u2208 R denoting the weights and bias of the final fully-connected layer, \u0398 denoting all parameters in the ansatz, \u03c3 denoting the sigmoid activation function and 'RegTerm' being the regularization term to avoid overfitting in the training process.\n\nCombining Eqs. (3) - (7), we know each output of QSAL is dependent on all its inputs, i.e.,\ny (l) m;s :=y (l) m;s \u03b8 (l) q , \u03b8 (l) k , \u03b8 (l) v ; {y (l\u22121) m;i } Sm i=1 =y (l\u22121) m;s + Sm j=1\u03b1 (l) s,j \u03b8 (l) q , \u03b8 (l) k ; {y (l\u22121) m;i } Sm i=1 \u00b7 o (l) j \u03b8 (l) v ; y (l\u22121) m;j ,(9)\nwhere y (0) m;s = x m;s and 1 \u2264 s \u2264 S m , 1 \u2264 l \u2264 L. Here, the regularization term is defined as\nRegTerm := \u03bb 2d \u2225w\u2225 2 + \u03b3 2d Sm s=1 \u2225x m;s \u2225 2 ,(10)\nwhere \u03bb, \u03b3 \u2265 0 are two regularization coefficients.\n\nWith the loss function defined in Eq. (8), we can optimize its parameters by (stochastic) gradient-descent [60]. The analytical gradient analysis can be found in Sec. II C. Finally, with the above preparation, we could train our QSANN to get the optimal (or sub-optimal) parameters. See Algorithm 1 for details on the training procedure. We remark that if the loss converges during training or the maximum number of iterations is reached, the optimization stops.\n\n\nC. Analytical Gradients\n\nHere, we give the stochastic analytical partial gradients of the loss function with regard to its parameters as follows. We first consider the parameters in the last quantum self-attention neural network layer, i.e., \u03b8\n(L) q , \u03b8 (L) k , \u03b8 (L)\nv , and the final fullyconnected layer, i.e., w, b. Then the parameters in the front layers could be evaluated similarly and be updated through the back-propagation algorithm [61]. Given the m-th data sample {(x 1 , x 2 , . . . , x Sm ) , y} (here, we omit m in the subscript for writing convenience, the same below), we have\n\u2202L \u2202w =\u03c3 \u00b7 1 S m Sm s=1 y (L) s + \u03bb d w, \u2202L \u2202b =\u03c3,(11)\n\u2202L \u2202y\n(L) s =\u03c3 \u00b7 1 S m \u00b7 w,(12)\nwhere\u03c3 = (\u03c3 \u2212 y) \u00b7 \u03c3 (1 \u2212 \u03c3) and \u03c3 denotes the abbreviation of \u03c3 w \u22a4 \u00b7 1 Sm Sm s=1 y\n(L) s + b . We also have \u2202L \u2202\u03b8 (L) v = Sm s=1 \u2202L \u2202y (L) s \u22a4 Sm j=1 \u2202y (L) s \u2202o (L) j \u00b7 \u2202o (L) j \u2202\u03b8 (L) v ,(13)\u2202L \u2202\u03b8 (L) q = Sm s=1 \u2202L \u2202y (L) s \u22a4 Sm j=1 \u2202y (L) s \u2202\u03b1 (L) s,j \u00b7 \u2202\u03b1 (L) s,j \u2202\u27e8Zq\u27e9s \u00b7 \u2202\u27e8Zq\u27e9s \u2202\u03b8 (L) q ,(14)\n\u2202L \u2202\u03b8\n(L) k = Sm s=1 \u2202L \u2202y (L) s \u22a4 Sm j=1 \u2202y (L) s \u2202\u03b1 (L) s,j \u00b7 Sm i=1 \u2202\u03b1 (L) s,j \u2202\u27e8Z k \u27e9i \u00b7 \u2202\u27e8Z k \u27e9i \u2202\u03b8 (L) k ,(15)\nwhere \u2202y\n(L) s /\u2202o (L) j = \u03b1 (L) s,j , \u2202y (L) s /\u2202\u03b1 (L) s,j = o (L) j , \u2202\u03b1 (L) s,j /\u2202\u27e8Z q \u27e9 s = \u2212 Sm i=1 \u2202\u03b1 (L) s,j /\u2202\u27e8Z k \u27e9 i and \u2202\u03b1 (L) s,j \u2202\u27e8Z k \u27e9 i = \u2212\u03b1 (L) s,j \u03b1 (L) s,i \u2212 \u03b4 ij \u00b7 2 (\u27e8Z q \u27e9 s \u2212 \u27e8Z k \u27e9 i ) , \u03b4 ij = 1, i = j 0, otherwise.(16)\nFurthermore, the last three partial derivatives of Eqs. (13), (14) and (15) could be evaluated directly on the quantum computers via the parameter shift rule [24]. For example,\n\u2202\u27e8Z q \u27e9 s \u2202\u03b8 (L) q,j = 1 2 (\u27e8Z q \u27e9 s,+ \u2212 \u27e8Z q \u27e9 s,\u2212 ) ,(17)\nwhere \u27e8Z q \u27e9 s,\u00b1 := \u27e8\u03c8 s | U \u2020 q,\u00b1 ZU q,\u00b1 |\u03c8 s \u27e9 and U q,\u00b1 := U q \u03b8\n(L) q,\u2212j , \u03b8 (L)\nq,j \u00b1 \u03c0 2 . Finally, in order to derive the partial derivatives of the parameters in the front layers, we also need the following: \n\nwhere the four terms denote the residual, value, query and key parts, respectively, and each sub-term can be evaluated similarly to the above analysis. With the above preparation, we could easily calculate every parameter's gradient and update these parameters accordingly. According to the definition of the Quantum Self-Attention Layer, for a sequence with S words, we need S(d + 2) Pauli measurements to obtain the d-dimensional value vectors as well as the queries and keys for all words from the quantum device. After that, we need to compute S 2 self-attention coefficients for all S 2 pairs of words on the classical computer. In general, QSANN takes advantage of quantum devices' efficiency in processing high-dimensional data while outsourcing some calculations to classical computers. This approach keeps the quantum circuit depth low and thus makes QSANN robust to low-level noise common in near-term quantum devices. This beneficial attribute is further verified by numerical results in the next section, where we test QSANN against noise.\n\n\nAlgorithm 1 QSANN training for text classification\n\nIn short, our QSANN first encodes words into a large quantum Hilbert space as the feature space and then projects them back to low-dimensional classical feature space by quantum measurement. Recent works have proved rigorous quantum advantages on some classification tasks by utilizing high-dimensional quantum feature space [62] and projected quantum models [12]. Thus, we expect that our QSANN might also have the potential advantage of digging out some hidden features that are classically intractable. Furthermore, the low-parameter variational quantum circuit exhibits the ability to achieve low generalization error [63] with few training data [64], providing further evidence for the effectiveness of our QSANN method. In the following section, we carry out numerical simulations of QSANN on several data sets to evaluate its performance on binary text classification tasks.\n\n\nIII. NUMERICAL RESULTS\n\nIn order to demonstrate the performance of our proposed QSANN, we have conducted numerical experiments on public data sets, where the quantum part was accomplished via classical simulation. Concretely, we first exhibit the better performance of QSANN by comparing it with i) the syntactic analysis-based quantum model [55] on two simple tasks, i.e., MC and RP, ii) the classical self-attention neural network (CSANN) and the naive method on three public sentiment analysis data sets, i.e., Yelp, IMDb and Amazon [65]. Then we show the reasonableness of our particular quantum self-attention GPQSA via visualization of self-attention coefficients. Next, we perform noisy experiments to show the robustness of QSANN to noisy quantum channels. Finally, we perform noisy experiments with different ansatzes to demonstrate the resilience of QSANN to the architectures of quantum neural networks. All the simulations and optimization loops are implemented via Paddle Quantum 2 on the PaddlePaddle Deep Learning Platform [66].\n\na. Data Sets The two simple synthetic data sets we employed come directly from [55], which are named MC and RP, respectively. MC contains 17 words and 130 sentences (70 train + 30 development + 30 test) with 3 or 4 words each; RP has 115 words and 105 sentences (74 train + 31 test) with 4 words in each one. The other three data sets we use are real-world data sets available at [67] as the Sentiment Labelled Sentences Data Set. These data sets consist of reviews of restaurants, movies and products selected from Yelp, IMDb and Amazon, respectively. Each of the three data sets contains 1000 sequences, where half are labeled as '0' (for negative) and the other half as '1' (for positive). And each sequence contains several to dozens of words. We randomly select 80% as training sequences and the rest 20% as test ones. b. Experimental Setting In the experiments, we use a single self-attention layer for both QSANN and CSANN. As a comparison, we also perform the most straightforward method, i.e., directly averaging the embedded vectors of a sequence, followed by a fully-connected layer, which we call the 'Naive' method, on the three data sets of reviews. Here, we note that only comparing these simple classical models is because there are still significant restrictions on current quantum hardware. It is pretty unfair to compare with the most potent classical models.\n\nRemark. We note that due to the current limitations of quantum hardware, using mini-or small-scale tasks for benchmarking is a common practice in current QNLP research. Additionally, the quantum transformer is still in its infancy, and it may not be fair to directly compare it with the most advanced classical transformers or hybrid transformers [25] currently available. Despite all this, we believe QSANN provides a good starting point for demonstrating the potential advantages and applications of quantum computing in NLP, providing valuable experience and insights for more in-depth research in the future.\n\nIn QSANN, all the encoder, query, key and value ansatzes have the same qubit number and are constructed according to Fig.  2, which are easily implementable on the NISQ devices. Specifically, assuming the n-qubit encoder ansatz has D enc layers with n(D enc + 2) parameters, we just set the dimension of the input vectors as d = n(D enc + 2). The depths of the query, key and value ansatzes are set to the same and are, at most, the polynomial size of the qubit number n. The actual hyper-parameter settings on different data sets are concluded in Table I. In addition, we choose Z 1 , . . . , Z n , X 1 , . . . , X n , Y 1 , . . . , Y n as the Pauli observables P j in Eq. (5). For example, it is just required 3n observables when D enc = 1. However, if D enc > 1, we could also choose two-qubit observables Z 12 , Z 23 and so on. All the ansatz parameters \u0398 and weight w are initialized from a Gaussian distribution with zero mean and 0.01 standard deviation, and the bias b is initialized to zero. Here, the ansatz parameters are not initialized uniformly from [0, 2\u03c0) is mainly due to the residual scheme applied in Eq. (6). During the optimization iteration, we use Adam optimizer [68]. And we repeat each experiment 9 times with different parameter initializations to collect the average accuracy and the corresponding fluctuations.\n\nIn CSANN, we set d = 16 and the classical query, key and value matrices are also initialized from a Gaussian distribution with zero mean and 0.01 standard deviation. Except for these, almost all other parameters are set the same as QSANN. These settings and initializations are the same in the naive method as well.\n\nc. Results on MC and RP Tasks The results on MC and RP tasks are summarized in Table II. In the MC task, our method QSANN could easily achieve a 100% test accuracy while requiring only 25 parameters (18 in query-key-value part and 7 in fully-connected part). However, in DisCoCat, the authors use 40 parameters but get a test accuracy lower than 80%. This result strongly demonstrates the powerful ability of QSANN for binary text classification. Here, the parameters in the encoder part are not counted as they could be replaced by fixed representations such as pre-trained word embeddings. In the RP task, we get a higher training accuracy but a slightly lower test accuracy. However, we observe that both test accuracies are pretty low when compared with the training accuracy. It is mainly because there is a massive bias between the training set and test set, i.e., more than half of the words in the test set have not appeared in the training one. Hence, the test accuracy highly depends on random guessing.\n\nd. Results on Yelp, IMDb and Amazon Data Sets As there are no quantum algorithms for text classification on these three data sets before, we benchmark our QSANN with the classical self-attention neural network (CSANN). The naive method is also listed for comparison. The results on Yelp, IMDb and Amazon data sets are summarized in Table III. We can intuitively see that QSANN outperforms CSANN and the naive method on all three data sets. Specifically, CSANN has 785 parameters (768 in classical query-key-value part and 17 in fully-connected part) on all data sets. In comparison, QSANN has only 49 parameters (36 in query-key-value part and 13 in fully-connected part) on the Yelp and IMDb data sets and 61 parameters (48 in query-keyvalue part and 13 in fully-connected part) on the Amazon data set, improving the test accuracy by about 1% as well as saving more than 10 times the number of parameters. Therefore, QSANN could have a potential advantage for text classification. e. Visualization of Self-Attention Coefficient To intuitively demonstrate the reasonableness of the Gaussian projected quantum self-attention, in Fig. 4 we visualize the averaged quantum self-attention coefficients of some selected test sequences from the Yelp data set. Concretely, for a sequence, we calculate 1 S S s=1\u03b1 s,j for j = 1, . . . , S and visualize them via a heat map, where S is the number of words in this sequence and\u03b1 s,j is the quantum self-attention coefficient. As shown in the figure, words with higher quantum self-attention coefficients are indeed those that determine the emotion of a sequence, implying the power of QSANN for capturing the most relevant words in a sequence on text classification tasks.\n\nf. Noisy Experimental Results on Yelp Data Set Due to the limitations of the near-term quantum computers, we add experiments with noisy quantum circuits to demonstrate the robustness of QSANN on the Yelp data set. We consider the representative channels [56] such as the depolarizing channel E D (\u03c1) and the amplitude-damping channel E AD (\u03c1):\nE D (\u03c1) := (1 \u2212 p) \u03c1 + p 3 (X\u03c1X + Y \u03c1Y + Z\u03c1Z) ,(19)E AD (\u03c1) := E 0 \u03c1E \u2020 0 + E 1 \u03c1E \u2020 1 ,(20)\nwith E 0 = |0\u27e9\u27e80| + \u221a 1 \u2212 p|1\u27e9\u27e81| and E 1 = \u221a p|0\u27e9\u27e81| denoting the Kraus operators. Here, \u03c1 = |\u03c8\u27e9\u27e8\u03c8| for a pure quantum state |\u03c8\u27e9 and p denotes the noise level. As a regular way to analyze the effect of quantum noises, we add these single-qubit noisy channels in the final circuit layer to represent the whole system's noise, which is illustrated in Fig. 5(a). We take the noise level p as 0.01, 0.1, 0.2 for these two noisy channels, respectively, and the box plots of test accuracies are depicted in Fig. 5(b). From the picture, we see the test accuracy of our QSANN almost does not decrease when the noise level  is less than 0.1, and even when the noise level is up to 0.2, the overall test accuracy has only decreased a little, showing that QSANN is robust to these quantum noises.\n\ng. Noisy Experimental Results with Different Ansatzes Given the recent limitations of quantum hardware topology, some ansatzes are easier to implement than others. As such, exploring the performance of QSANN under different ansatzes is crucial to determining the difficulty level in deploying QSANN on current quantum hardware. Additionally, it is worth investigating which ansatz can most easily achieve optimal performance of QSANN for specific practical tasks.\n\nIn this subsection, we test QSANN using different ansatzes on both MC and RP data sets. As depicted in Fig. 6, these ansatzes utilize different entanglement layers while keeping the single-qubit gates and the total number of parameters unchanged. Furthermore, a depolarizing channel with p = 0.1 is added to each ansatz, as shown in Eq. (19). Other settings remain the same as in Table I. The final results are shown in Table IV, where we see that the performance of the four ansatz types is virtually identical. This directly indicates that QSANN is resilient to ansatz architectures.  \n\n\nIV. DISCUSSIONS\n\nWe have proposed a quantum self-attention neural network (QSANN) by introducing the self-attention mechanism to quantum neural networks. Specifically, the adopted Gaussian projected quantum self-attention exploits the exponentially large quantum Hilbert space as the quantum feature space, making QSANN have the potential advantage of mining some hidden correlations between words that are difficult to dig out classically. Numerical results show that QSANN outperforms the best-known QNLP method and a simple classical self-attention neural network for text classification on several public data sets. Moreover, using only shallow quantum circuits and Pauli measurements, QSANN can be easily implemented on near-term quantum devices and is noise-resilient, as implied by simulation results. We believe that this attempt to combine self-attention and quantum neural networks would open up new avenues for QNLP as well as QML.\n\nAs a future direction, more advanced techniques such as positional encoding and multi-head attention can be employed in quantum neural networks for generative models and other more complicated tasks. Another exciting future research direction is to move toward large language models. However, we must realize that there are still many challenges and limitations to overcome, particularly in the NISQ era. Despite these challenges, our work represents a promising step towards this goal, and we are optimistic about the potential of quantum computing in NLP. As quantum hardware continues to evolve and improve, we anticipate that our methods can be gradually extended to more complex algorithms and tasks, unlocking new possibilities for QNLP research.\n\nFIG. 1 :\n1Sketch of a quantum self-attention layer (QSAL). On quantum devices, the classical inputs {y (l\u22121) s\n\n\n, where the weights are the normalized coefficients\u03b1s,j, cf. Eq. (7).\n\nFIG. 3 :\n3Sketch of QSANN, where a sequence of classical vectors {xs} firstly goes through L QSALs to obtain the corresponding sequence of feature vectors {y (L)s }, then through the average operation, and finally through the fully-connected layer for the binary prediction task.\n\nInput:\nThe training data set D := {(xm;1, xm;2, . . ., xm;S m ), ym} Ns m=1 , EP OCH, number of QSALs L and optimization procedure Output: The final ansatz parameters \u0398 * , weight w * , b * Initialize the ansatz parameters \u0398, weight w from Gaussian distribution N(0, 0.01) and the bias b to 0. for ep = 1, . . . , EP OCH do for m = 1, . . . , Ns do Apply the encoder ansatz Uenc to each of xm;s to get the corresponding quantum state |\u03c8s\u27e9, cf.(3). Apply Uq and U k to |\u03c8s\u27e9 and measure the Pauli-Z expectations to get \u27e8Zq\u27e9s, \u27e8Z k \u27e9s, cf. (4), and then calculate the quantum selfattention coefficients \u03b1s,j, cf.(7). Apply Uv and measure a series of Pauli expectations to get os, cf.(5), and then compute the output {y(l) s } of the l-th QSAL, cf. (6). Repeat 4-6 L times to get the output {y (L) s } of the L-th QSAL. Average {y (L) s } and through a fully-connected layer to obtain the predicted value\u0177m. Calculate the mean squared error in (8) and update the parameters through the optimization procedure. end for if the stopping condition is met then Break. end if end for D. Analysis of QSANN\n\nFIG. 5 :\n5(a) The diagram for adding depolarizing channels in our simulated experiments. The amplitude-damping channels are added in the same way. (b) Box plots of test accuracy on Yelp data set with depolarizing and amplitude damping noises. Each box contains nine repeated experiments. The absence of a notable decrease in accuracy implies the noise-resilience attribute of QSANN.\n\nTABLE I :\nIOverview of hyper-parameter settings. Here, 'LR' denotes learning rate, Denc, Dq, D k , Dv denote the depths of the corresponding ansatzes and d = n(Denc + 2).Data set n d Denc D q/k/v \u03bb \n\u03b3 \nLR \n\nMC 2 6 \n1 \n1 \n0 \n0 0.008 \n\nRP \n4 24 4 \n5 \n0.2 0.4 0.008 \n\nYelp 4 12 1 \n1 \n0.2 0.2 0.008 \n\nIMDb 4 12 1 \n1 \n0.002 0.002 0.002 \n\nAmazon 4 12 1 \n2 \n0.2 0.2 0.008 \n\nTABLE II: Training accuracy and test accuracy of QSANN as well as DisCoCat on MC and RP tasks. \n\nMethod \nMC \nRP \n\n# Paras TrainAcc(%) TestAcc(%) # Paras TrainAcc(%) TestAcc(%) \n\nDisCoCat [55] 40 \n83.10 \n79.80 \n168 \n90.60 \n72.30 \n\nQSANN \n25 \n100.00 \n100.00 \n109 95.35\u00b11.95 67.74\u00b10.00 \n\n\n\nTABLE III :\nIIITest accuracy of QSANN compared to CSANN and the naive method on Yelp, IMDb, and Amazon data sets. The highest accuracy in each column is indicated in bold font. On all the three data sets, QSANN achieves the highest accuracies among the three methods while using much fewer parameters than CSANN.FIG. 4: Heat maps of the averaged quantum self-attention coefficients for some selected test sequences from the Yelp data set, where a deeper color indicates a higher coefficient. Words that are more sentiment-related are generally assigned higher self-attention coefficients by our Gaussian projected quantum self-attention, implying the validity and interpretability of QSANN.Method \nYelp \nIMDb \nAmazon \n\n# Paras TestAcc (%) # Paras TestAcc (%) # Paras TestAcc (%) \n\nNaive \n17 \n82.78\u00b10.78 \n17 79.33\u00b10.67 \n17 80.39\u00b10.61 \n\nCSANN 785 83.11\u00b10.89 785 79.67\u00b10.83 785 83.22\u00b11.28 \n\nQSANN 49 \n84.79\u00b11.29 \n49 \n80.28\u00b11.78 \n61 \n84.25\u00b11.75 \n\n\n\n\nFIG. 6: Four types of ansatzes used in QSANN. Each has a different entangled layer.Rx \nRy \n\u2022 \nRy \n\nRx \nRy \n\u2022 \nRy \n\nRx \nRy \n\u2022 \nRy \n\nRx \nRy \n\u2022 Ry \n\u00d7D \n\n(a) Ansatz-0 \n\nRx \nRy \n\u2022 \n\u2022 \nRy \n\nRx \nRy \n\u2022 \nRy \n\nRx \nRy \n\u2022 \nRy \n\nRx \nRy \nRy \n\u00d7D \n\n(b) Ansatz-1 \n\nRx \nRy \n\u2022 \nRy \n\nRx \nRy \n\u2022 \nRy \n\nRx \nRy \n\u2022 Ry \n\nRx \nRy \nRy \n\u00d7D \n\n(c) Ansatz-2 \n\nRx Ry \u2022 \u2022 \u2022 \nRy \n\nRx Ry \n\u2022 \u2022 \nRy \n\nRx Ry \n\u2022 Ry \n\nRx Ry \nRy \n\u00d7D \n\n(d) Ansatz-3 \n\n\n\nTABLE IV :\nIVTraining accuracy and test accuracy of QSANN with four different ansatzes on MC and RP tasks.\nMulti-dimension is also possible by choosing multiple measurement results, like the value part.\nhttps://github.com/paddlepaddle/Quantum\n\nJohn Preskill, arXiv:2106.10522Quantum computing 40 years later. arXiv preprintJohn Preskill. Quantum computing 40 years later. arXiv preprint arXiv:2106.10522, 2021.\n\nQuantum computational supremacy. W Aram, Ashley Harrow, Montanaro, Nature. 5497671Aram W Harrow and Ashley Montanaro. Quantum computational supremacy. Nature, 549(7671):203-209, 2017.\n\nQuantum algorithms for algebraic problems. M Andrew, Wim Childs, Van Dam, Reviews of Modern Physics. 821Andrew M Childs and Wim van Dam. Quantum algorithms for algebraic problems. Reviews of Modern Physics, 82(1):1-52, jan 2010.\n\nQuantum algorithms: an overview. Ashley Montanaro, npj Quantum Information. 215023Ashley Montanaro. Quantum algorithms: an overview. npj Quantum Information, 2(1):15023, nov 2016.\n\nToward the first quantum simulation with quantum speedup. Andrew M Childs, Dmitri Maslov, Yunseong Nam, Neil J Ross, Yuan Su, Proceedings of the National Academy of Sciences. 11538Andrew M. Childs, Dmitri Maslov, Yunseong Nam, Neil J. Ross, and Yuan Su. Toward the first quantum simulation with quantum speedup. Proceedings of the National Academy of Sciences, 115(38):9456-9461, sep 2018.\n\nQuantum machine learning. Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, Seth Lloyd, Nature. 5497671Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth Lloyd. Quantum machine learning. Nature, 549(7671):195-202, Sep 2017.\n\nQuantum Speed-Ups for Solving Semidefinite Programs. G S L Fernando, Krysta M Brandao, Svore, 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS). IEEEFernando G.S.L. Brandao and Krysta M. Svore. Quantum Speed-Ups for Solving Semidefinite Programs. In 2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS), pages 415-426. IEEE, oct 2017.\n\nSecure quantum key distribution with realistic devices. Feihu Xu, Xiongfeng Ma, Qiang Zhang, Hoi-Kwong Lo, Jian-Wei Pan, Reviews of Modern Physics. 92225002Feihu Xu, Xiongfeng Ma, Qiang Zhang, Hoi-Kwong Lo, and Jian-Wei Pan. Secure quantum key distribution with realistic devices. Reviews of Modern Physics, 92(2):25002, 2020.\n\nQuantum computational chemistry. Sam Mcardle, Suguru Endo, Al\u00e1n Aspuru-Guzik, Simon C Benjamin, Xiao Yuan, Reviews of Modern Physics. 92115003Sam McArdle, Suguru Endo, Al\u00e1n Aspuru-Guzik, Simon C. Benjamin, and Xiao Yuan. Quantum computational chemistry. Reviews of Modern Physics, 92(1):015003, mar 2020.\n\nYudong Cao, Jonathan Romero, Jonathan P Olson, Matthias Degroote, Peter D Johnson, M\u00e1ria Kieferov\u00e1, Ian D Kivlichan, Tim Menke, Borja Peropadre, P D Nicolas, Sukin Sawaya, Sim, Libor Veis, and Al\u00e1n Aspuru-Guzik. Quantum Chemistry in the Age of Quantum Computing. 119Yudong Cao, Jonathan Romero, Jonathan P Olson, Matthias Degroote, Peter D. Johnson, M\u00e1ria Kieferov\u00e1, Ian D. Kivlichan, Tim Menke, Borja Peropadre, Nicolas P D Sawaya, Sukin Sim, Libor Veis, and Al\u00e1n Aspuru-Guzik. Quantum Chemistry in the Age of Quantum Computing. Chemical Reviews, 119(19):10856-10915, oct 2019.\n\nQuantum support vector machine for big data classification. Patrick Rebentrost, Masoud Mohseni, Seth Lloyd, Physical Review Letters. 1133130503Patrick Rebentrost, Masoud Mohseni, and Seth Lloyd. Quantum support vector machine for big data classification. Physical Review Letters, 113(3):130503, Sep 2014.\n\nPower of data in quantum machine learning. Hsin-Yuan, Michael Huang, Masoud Broughton, Ryan Mohseni, Sergio Babbush, Hartmut Boixo, Jarrod R Neven, Mcclean, Nature Communications. 121Hsin-Yuan Huang, Michael Broughton, Masoud Mohseni, Ryan Babbush, Sergio Boixo, Hartmut Neven, and Jarrod R McClean. Power of data in quantum machine learning. Nature Communications, 12(1):1-9, 2021.\n\nMaria Schuld, Francesco Petruccione, Machine Learning with Quantum Computers. Maria Schuld and Francesco Petruccione. Machine Learning with Quantum Computers. 2021.\n\nJohn Preskill, Quantum Computing in the NISQ era and beyond. Quantum. 279John Preskill. Quantum Computing in the NISQ era and beyond. Quantum, 2:79, Aug 2018.\n\nQuantum supremacy using a programmable superconducting processor. Frank Arute, Kunal Arya, Ryan Babbush, Dave Bacon, C Joseph, Rami Bardin, Rupak Barends, Sergio Biswas, Boixo, Gsl Fernando, David A Brandao, Buell, Nature. 5747779Frank Arute, Kunal Arya, Ryan Babbush, Dave Bacon, Joseph C Bardin, Rami Barends, Rupak Biswas, Sergio Boixo, Fernando GSL Brandao, David A Buell, et al. Quantum supremacy using a programmable superconducting processor. Nature, 574(7779):505-510, 2019.\n\nQuantum computational advantage using photons. Han-Sen Zhong, Hui Wang, Yu-Hao Deng, Ming-Cheng Chen, Li-Chao Peng, Yi-Han Luo, Jian Qin, Dian Wu, Xing Ding, Yi Hu, Science. 3706523Han-Sen Zhong, Hui Wang, Yu-Hao Deng, Ming-Cheng Chen, Li-Chao Peng, Yi-Han Luo, Jian Qin, Dian Wu, Xing Ding, Yi Hu, et al. Quantum computational advantage using photons. Science, 370(6523):1460-1463, 2020.\n\nNoisy intermediate-scale quantum (nisq) algorithms. Kishor Bharti, Alba Cervera-Lierta, Thi Ha Kyaw, Tobias Haug, Sumner Alperin-Lea, Abhinav Anand, Matthias Degroote, Hermanni Heimonen, Jakob S Kottmann, Tim Menke, arXiv:2101.08448arXiv preprintKishor Bharti, Alba Cervera-Lierta, Thi Ha Kyaw, Tobias Haug, Sumner Alperin-Lea, Abhinav Anand, Matthias Degroote, Hermanni Heimonen, Jakob S Kottmann, Tim Menke, et al. Noisy intermediate-scale quantum (nisq) algorithms. arXiv preprint arXiv:2101.08448, 2021.\n\nVariational quantum algorithms. M Cerezo, Andrew Arrasmith, Ryan Babbush, Simon C Benjamin, Suguru Endo, Keisuke Fujii, Jarrod R Mcclean, Kosuke Mitarai, Xiao Yuan, Lukasz Cincio, Patrick J Coles, Nature Reviews Physics. M. Cerezo, Andrew Arrasmith, Ryan Babbush, Simon C. Benjamin, Suguru Endo, Keisuke Fujii, Jarrod R. McClean, Kosuke Mitarai, Xiao Yuan, Lukasz Cincio, and Patrick J. Coles. Variational quantum algorithms. Nature Reviews Physics, pages 1-29, aug 2021.\n\nHybrid Quantum-Classical Algorithms and Quantum Error Mitigation. Suguru Endo, Zhenyu Cai, C Simon, Xiao Benjamin, Yuan, Journal of the Physical Society of Japan. 90332001Suguru Endo, Zhenyu Cai, Simon C Benjamin, and Xiao Yuan. Hybrid Quantum-Classical Algorithms and Quantum Error Mitigation. Journal of the Physical Society of Japan, 90(3):032001, mar 2021.\n\nA variational eigenvalue solver on a photonic quantum processor. Alberto Peruzzo, Jarrod Mcclean, Peter Shadbolt, Man-Hong Yung, Xiao-Qi Zhou, Peter J Love, Al\u00e1n Aspuru-Guzik, Jeremy L O&apos;brien, Nature Communications. 514213Alberto Peruzzo, Jarrod McClean, Peter Shadbolt, Man-Hong Yung, Xiao-Qi Zhou, Peter J. Love, Al\u00e1n Aspuru-Guzik, and Jeremy L. O'Brien. A variational eigenvalue solver on a photonic quantum processor. Nature Communications, 5(1):4213, dec 2014.\n\nEdward Farhi, Jeffrey Goldstone, Sam Gutmann, arXiv:1411.4028A Quantum Approximate Optimization Algorithm. Edward Farhi, Jeffrey Goldstone, and Sam Gutmann. A Quantum Approximate Optimization Algorithm. arXiv:1411.4028, pages 1-16, Nov 2014.\n\nSupervised learning with quantum-enhanced feature spaces. Vojt\u011bch Havl\u00ed\u010dek, Antonio D C\u00f3rcoles, Kristan Temme, Aram W Harrow, Abhinav Kandala, Jerry M Chow, Jay M Gambetta, Nature. 5677747Vojt\u011bch Havl\u00ed\u010dek, Antonio D. C\u00f3rcoles, Kristan Temme, Aram W. Harrow, Abhinav Kandala, Jerry M. Chow, and Jay M. Gambetta. Supervised learning with quantum-enhanced feature spaces. Nature, 567(7747):209-212, Mar 2019.\n\nCircuit-centric quantum classifiers. Maria Schuld, Alex Bocharov, Krysta M Svore, Nathan Wiebe, Physical Review A. 101332308Maria Schuld, Alex Bocharov, Krysta M. Svore, and Nathan Wiebe. Circuit-centric quantum classifiers. Physical Review A, 101(3):032308, Mar 2020.\n\nQuantum circuit learning. K Mitarai, M Negoro, M Kitagawa, K Fujii, Physical Review A. 98332309K. Mitarai, M. Negoro, M. Kitagawa, and K. Fujii. Quantum circuit learning. Physical Review A, 98(3):032309, Sep 2018.\n\nWhen bert meets quantum temporal convolution learning for text classification in heterogeneous computing. Chao-Han Huck Yang, Jun Qi, Samuel Yen-Chi Chen, Yu Tsao, Pin-Yu Chen, ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEEChao-Han Huck Yang, Jun Qi, Samuel Yen-Chi Chen, Yu Tsao, and Pin-Yu Chen. When bert meets quantum temporal convolution learning for text classification in heterogeneous computing. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8602-8606. IEEE, 2022.\n\nClassical-to-quantum transfer learning for spoken command recognition based on quantum neural networks. Jun Qi, Javier Tejedor, ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEEJun Qi and Javier Tejedor. Classical-to-quantum transfer learning for spoken command recognition based on quantum neural networks. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8627-8631. IEEE, 2022.\n\nA quantum kernel learning approach to acoustic modeling for spoken command recognition. Chao-Han Huck Yang, Bo Li, Yu Zhang, Nanxin Chen, Tara N Sainath, Sabato Marco Siniscalchi, Chin-Hui Lee, ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEEChao-Han Huck Yang, Bo Li, Yu Zhang, Nanxin Chen, Tara N Sainath, Sabato Marco Siniscalchi, and Chin-Hui Lee. A quantum kernel learning approach to acoustic modeling for spoken command recognition. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1-5. IEEE, 2023.\n\nParameterized quantum circuits as machine learning models. Marcello Benedetti, Erika Lloyd, Stefan Sack, Mattia Fiorentini, Quantum Science and Technology. 4443001Marcello Benedetti, Erika Lloyd, Stefan Sack, and Mattia Fiorentini. Parameterized quantum circuits as machine learning models. Quantum Science and Technology, 4(4):043001, Jun 2019.\n\nEdward Farhi, Hartmut Neven, arXiv:1802.06002Classification with Quantum Neural Networks on Near Term Processors. Edward Farhi and Hartmut Neven. Classification with Quantum Neural Networks on Near Term Processors. arXiv:1802.06002, pages 1-21, Feb 2018.\n\nPower and limitations of single-qubit native quantum neural networks. Zhan Yu, Hongshun Yao, Mujin Li, Xin Wang ; In, S Koyejo, Mohamed, D Agarwal, Belgrave, A Cho, Oh, Advances in Neural Information Processing Systems. Curran Associates, Inc35Zhan Yu, Hongshun Yao, Mujin Li, and Xin Wang. Power and limitations of single-qubit native quantum neural networks. In S Koyejo, S Mohamed, A Agarwal, D Belgrave, K Cho, and A Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 27810-27823. Curran Associates, Inc., 2022.\n\nGeneralization in quantum machine learning from few training data. Matthias C Caro, Hsin-Yuan Huang, M Cerezo, Kunal Sharma, Andrew Sornborger, Lukasz Cincio, Patrick J Coles, Nature Communications. 1314919Matthias C. Caro, Hsin-Yuan Huang, M. Cerezo, Kunal Sharma, Andrew Sornborger, Lukasz Cincio, and Patrick J. Coles. Generalization in quantum machine learning from few training data. Nature Communications, 13(1):4919, aug 2022.\n\nConcentration of Data Encoding in Parameterized Quantum Circuits. Guangxi Li, Ruilin Ye, Xuanqiang Zhao, Xin Wang, 36th Conference on Neural Information Processing Systems. 2022Guangxi Li, Ruilin Ye, Xuanqiang Zhao, and Xin Wang. Concentration of Data Encoding in Parameterized Quantum Circuits. In 36th Conference on Neural Information Processing Systems (NeurIPS 2022), 2022.\n\nEfficient measure for the expressivity of variational quantum algorithms. Yuxuan Du, Zhuozhuo Tu, Xiao Yuan, Dacheng Tao, Physical Review Letters. 128880506Yuxuan Du, Zhuozhuo Tu, Xiao Yuan, and Dacheng Tao. Efficient measure for the expressivity of variational quantum algorithms. Physical Review Letters, 128(8):80506, 2022.\n\nQuantum machine learning beyond kernel methods. Sofiene Jerbi, Lukas J Fiderer, Hendrik Poulsen Nautrup, Jonas M K\u00fcbler, Hans J Briegel, Vedran Dunjko, Nature Communications. 141517Sofiene Jerbi, Lukas J. Fiderer, Hendrik Poulsen Nautrup, Jonas M. K\u00fcbler, Hans J. Briegel, and Vedran Dunjko. Quantum machine learning beyond kernel methods. Nature Communications, 14(1):517, jan 2023.\n\nOptimal Quantum Dataset for Learning a Unitary Transformation. Zhan Yu, Xuanqiang Zhao, Benchi Zhao, Xin Wang, Physical Review Applied. 19334017Zhan Yu, Xuanqiang Zhao, Benchi Zhao, and Xin Wang. Optimal Quantum Dataset for Learning a Unitary Transformation. Physical Review Applied, 19(3):034017, mar 2023.\n\nPower of data in quantum machine learning. Hsin-Yuan, Michael Huang, Masoud Broughton, Ryan Mohseni, Sergio Babbush, Hartmut Boixo, Jarrod R Neven, Mcclean, Nature Communications. 1212631Hsin-Yuan Huang, Michael Broughton, Masoud Mohseni, Ryan Babbush, Sergio Boixo, Hartmut Neven, and Jarrod R. McClean. Power of data in quantum machine learning. Nature Communications, 12(1):2631, dec 2021.\n\nDetecting and quantifying entanglement on near-term quantum devices. npj Quantum Information. Kun Wang, Zhixin Song, Xuanqiang Zhao, Zihe Wang, Xin Wang, 852Kun Wang, Zhixin Song, Xuanqiang Zhao, Zihe Wang, and Xin Wang. Detecting and quantifying entanglement on near-term quantum devices. npj Quantum Information, 8(1):52, dec 2022.\n\nPractical distributed quantum information processing with LOCCNet. Xuanqiang Zhao, Benchi Zhao, Zihe Wang, Zhixin Song, Xin Wang, npj Quantum Information. 7159Xuanqiang Zhao, Benchi Zhao, Zihe Wang, Zhixin Song, and Xin Wang. Practical distributed quantum information processing with LOCCNet. npj Quantum Information, 7(1):159, dec 2021.\n\nJinkai Tian, Xiaoyu Sun, Yuxuan Du, Shanshan Zhao, Qing Liu, Kaining Zhang, Wei Yi, Wanrong Huang, Chaoyue Wang, Xingyao Wu, Min-Hsiu Hsieh, arXiv:2206.03066Tongliang Liu, Wenjing Yang, and Dacheng Tao. Recent Advances for Quantum Neural Networks in Generative Learning. Jinkai Tian, Xiaoyu Sun, Yuxuan Du, Shanshan Zhao, Qing Liu, Kaining Zhang, Wei Yi, Wanrong Huang, Chaoyue Wang, Xingyao Wu, Min-Hsiu Hsieh, Tongliang Liu, Wenjing Yang, and Dacheng Tao. Recent Advances for Quantum Neural Networks in Generative Learning. arXiv:2206.03066, jun 2022.\n\nA hybrid quantum-classical Hamiltonian learning algorithm. Youle Wang, Guangxi Li, Xin Wang, SCIENCE CHINA-INFORMATION SCIENCES. 6622023Youle Wang, Guangxi Li, and Xin Wang. A hybrid quantum-classical Hamiltonian learning algorithm. SCIENCE CHINA- INFORMATION SCIENCES, 66(2), 2023.\n\nThe power of quantum neural networks. Amira Abbas, David Sutter, Christa Zoufal, Aurelien Lucchi, Alessio Figalli, Stefan Woerner, Nature Computational Science. 16Amira Abbas, David Sutter, Christa Zoufal, Aurelien Lucchi, Alessio Figalli, and Stefan Woerner. The power of quantum neural networks. Nature Computational Science, 1(6):403-409, jun 2021.\n\nModeling term dependencies with quantum language models for IR. Alessandro Sordoni, Jian-Yun Nie, Yoshua Bengio, Proceedings of the 36th international ACM SIGIR conference on Research and development in information retrieval -SIGIR '13. the 36th international ACM SIGIR conference on Research and development in information retrieval -SIGIR '13New York, New York, USAACM Press653Alessandro Sordoni, Jian-Yun Nie, and Yoshua Bengio. Modeling term dependencies with quantum language models for IR. In Proceed- ings of the 36th international ACM SIGIR conference on Research and development in information retrieval -SIGIR '13, page 653, New York, New York, USA, 2013. ACM Press.\n\nEnd-to-End Quantum-Like Language Models with Application to Question Answering. Peng Zhang, Jiabin Niu, Zhan Su, Benyou Wang, Liqun Ma, Dawei Song, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence32Peng Zhang, Jiabin Niu, Zhan Su, Benyou Wang, Liqun Ma, and Dawei Song. End-to-End Quantum-Like Language Models with Application to Question Answering. Proceedings of the AAAI Conference on Artificial Intelligence, 32(1), 2018.\n\nA quantum-inspired sentiment representation model for twitter sentiment analysis. Yazhou Zhang, Dawei Song, Peng Zhang, Xiang Li, Panpan Wang, Applied Intelligence. 498Yazhou Zhang, Dawei Song, Peng Zhang, Xiang Li, and Panpan Wang. A quantum-inspired sentiment representation model for twitter sentiment analysis. Applied Intelligence, 49(8):3093-3108, 2019.\n\nTowards quantum language models. Ivano Basile, Fabio Tamburini, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingIvano Basile and Fabio Tamburini. Towards quantum language models. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1840-1849, 2017.\n\nWilliam Zeng, Bob Coecke, arXiv:1608.01406Quantum algorithms for compositional natural language processing. arXiv preprintWilliam Zeng and Bob Coecke. Quantum algorithms for compositional natural language processing. arXiv preprint arXiv:1608.01406, 2016.\n\nQuantum natural language processing on near-term quantum computers. Konstantinos Meichanetzidis, Stefano Gogioso, Giovanni De Felice, Nicol\u00f2 Chiappori, Alexis Toumi, Bob Coecke, arXiv:2005.04147arXiv preprintKonstantinos Meichanetzidis, Stefano Gogioso, Giovanni De Felice, Nicol\u00f2 Chiappori, Alexis Toumi, and Bob Coecke. Quantum natural language processing on near-term quantum computers. arXiv preprint arXiv:2005.04147, 2020.\n\nNathan Wiebe, Alex Bocharov, Paul Smolensky, Matthias Troyer, Krysta M Svore, arXiv:1902.05162Quantum language processing. arXiv preprintNathan Wiebe, Alex Bocharov, Paul Smolensky, Matthias Troyer, and Krysta M Svore. Quantum language processing. arXiv preprint arXiv:1902.05162, 2019.\n\nSamuel Yen-Chi Chen, Shinjae Yoo, Yao-Lung L Fang, arXiv:2009.01783Quantum long short-term memory. arXiv preprintSamuel Yen-Chi Chen, Shinjae Yoo, and Yao-Lung L Fang. Quantum long short-term memory. arXiv preprint arXiv:2009.01783, 2020.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Proceedings of the 31st International Conference on Neural Information Processing Systems. the 31st International Conference on Neural Information Processing SystemsAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems, pages 6000- 6010, 2017.\n\nBeyond rnns: Positional selfattention with co-attention for video question answering. Xiangpeng Li, Jingkuan Song, Lianli Gao, Xianglong Liu, Wenbing Huang, Xiangnan He, Chuang Gan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Xiangpeng Li, Jingkuan Song, Lianli Gao, Xianglong Liu, Wenbing Huang, Xiangnan He, and Chuang Gan. Beyond rnns: Positional self- attention with co-attention for video question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 8658-8665, 2019.\n\nMulti-scale self-attention for text classification. Qipeng Guo, Xipeng Qiu, Pengfei Liu, Xiangyang Xue, Zheng Zhang, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence34Qipeng Guo, Xipeng Qiu, Pengfei Liu, Xiangyang Xue, and Zheng Zhang. Multi-scale self-attention for text classification. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7847-7854, 2020.\n\nPeter Cha, Paul Ginsparg, Felix Wu, Juan Carrasquilla, L Peter, Eun-Ah Mcmahon, Kim, arXiv:2006.12469Attention-based quantum tomography. arXiv preprintPeter Cha, Paul Ginsparg, Felix Wu, Juan Carrasquilla, Peter L McMahon, and Eun-Ah Kim. Attention-based quantum tomography. arXiv preprint arXiv:2006.12469, 2020.\n\nQnlp in practice: Running compositional models of meaning on a quantum computer. Robin Lorenz, Anna Pearson, Konstantinos Meichanetzidis, Dimitri Kartsaklis, Bob Coecke, arXiv:2102.12846arXiv preprintRobin Lorenz, Anna Pearson, Konstantinos Meichanetzidis, Dimitri Kartsaklis, and Bob Coecke. Qnlp in practice: Running composi- tional models of meaning on a quantum computer. arXiv preprint arXiv:2102.12846, 2021.\n\nQuantum Computation and Quantum Information. A Michael, Isaac Nielsen, Chuang, American Journal of Physics. 705Michael A. Nielsen and Isaac Chuang. Quantum Computation and Quantum Information. American Journal of Physics, 70(5):558-559, May 2002.\n\nUniversal kernels. Yuesheng Charles A Micchelli, Haizhang Xu, Zhang, Journal of Machine Learning Research. 712Charles A Micchelli, Yuesheng Xu, and Haizhang Zhang. Universal kernels. Journal of Machine Learning Research, 7(12), 2006.\n\nThe dawn of quantum natural language processing. Riccardo Di Sipio, Jia-Hong Huang, Samuel Yen-Chi Chen, Stefano Mangini, Marcel Worring, ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEERiccardo Di Sipio, Jia-Hong Huang, Samuel Yen-Chi Chen, Stefano Mangini, and Marcel Worring. The dawn of quantum natural language processing. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8612-8616. IEEE, 2022.\n\nTheory of Point Estimation. Eric R Ziegel, E L Lehmann, George Casella, Technometrics. 413274Eric R. Ziegel, E. L. Lehmann, and George Casella. Theory of Point Estimation. Technometrics, 41(3):274, Aug 1999.\n\nStochastic Learning. L\u00e9on Bottou, Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics). 3176L\u00e9on Bottou. Stochastic Learning. In Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), volume 3176, pages 146-168. 2004.\n\nDeep Learning. Ian Goodfellow, Yoshua Bengio, Aaron Courville, MIT PressIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http://www.deeplearningbook.org.\n\nA rigorous and robust quantum speed-up in supervised machine learning. Yunchao Liu, Srinivasan Arunachalam, Kristan Temme, Nature Physics. Yunchao Liu, Srinivasan Arunachalam, and Kristan Temme. A rigorous and robust quantum speed-up in supervised machine learning. Nature Physics, pages 1-5, 2021.\n\nTheoretical error performance analysis for variational quantum circuit based functional regression. Jun Qi, Chao-Han Huck Yang, Pin-Yu Chen, Min-Hsiu Hsieh, npj Quantum Information. 94Jun Qi, Chao-Han Huck Yang, Pin-Yu Chen, and Min-Hsiu Hsieh. Theoretical error performance analysis for variational quantum circuit based functional regression. npj Quantum Information, 9(1):4, 2023.\n\nGeneralization in quantum machine learning from few training data. C Matthias, Hsin-Yuan Caro, Marco Huang, Kunal Cerezo, Andrew Sharma, Lukasz Sornborger, Patrick J Cincio, Coles, Nature communications. 1314919Matthias C Caro, Hsin-Yuan Huang, Marco Cerezo, Kunal Sharma, Andrew Sornborger, Lukasz Cincio, and Patrick J Coles. General- ization in quantum machine learning from few training data. Nature communications, 13(1):4919, 2022.\n\nFrom group to individual labels using deep features. Dimitrios Kotzias, Misha Denil, Padhraic Nando De Freitas, Smyth, Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data MiningDimitrios Kotzias, Misha Denil, Nando De Freitas, and Padhraic Smyth. From group to individual labels using deep features. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 597-606, 2015.\n\nPaddlePaddle: An Open-Source Deep Learning Platform from Industrial Practice. Yanjun Ma, Dianhai Yu, Tian Wu, Haifeng Wang, Frontiers of Data and Domputing. 11Yanjun Ma, Dianhai Yu, Tian Wu, and Haifeng Wang. PaddlePaddle: An Open-Source Deep Learning Platform from Industrial Practice. Frontiers of Data and Domputing, 1(1):105-115, 2019.\n\nUCI machine learning repository. Dheeru Dua, Casey Graff, Dheeru Dua and Casey Graff. UCI machine learning repository, 2017.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Lei Kingma, Ba, 3rd International Conference on Learning Representations, ICLR 2015 -Conference Track Proceedings. Diederik P. Kingma and Jimmy Lei Ba. Adam: A method for stochastic optimization. 3rd International Conference on Learning Representations, ICLR 2015 -Conference Track Proceedings, Dec 2015.\n", "annotations": {"author": "[{\"end\":238,\"start\":66},{\"end\":401,\"start\":239},{\"end\":599,\"start\":402}]", "publisher": null, "author_last_name": "[{\"end\":76,\"start\":74},{\"end\":253,\"start\":249},{\"end\":410,\"start\":406}]", "author_first_name": "[{\"end\":73,\"start\":66},{\"end\":248,\"start\":239},{\"end\":405,\"start\":402}]", "author_affiliation": "[{\"end\":143,\"start\":78},{\"end\":237,\"start\":145},{\"end\":320,\"start\":255},{\"end\":400,\"start\":322},{\"end\":477,\"start\":412},{\"end\":598,\"start\":479}]", "title": "[{\"end\":63,\"start\":1},{\"end\":662,\"start\":600}]", "venue": null, "abstract": "[{\"end\":2020,\"start\":691}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2084,\"start\":2081},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2179,\"start\":2176},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2182,\"start\":2179},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2185,\"start\":2182},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2188,\"start\":2185},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2191,\"start\":2188},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2384,\"start\":2381},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2402,\"start\":2399},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2417,\"start\":2414},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2420,\"start\":2417},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2446,\"start\":2443},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2450,\"start\":2446},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2454,\"start\":2450},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2458,\"start\":2454},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2562,\"start\":2558},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2922,\"start\":2918},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2925,\"start\":2922},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3023,\"start\":3019},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3027,\"start\":3023},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3031,\"start\":3027},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3131,\"start\":3127},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3150,\"start\":3146},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3177,\"start\":3173},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3181,\"start\":3177},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3185,\"start\":3181},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3189,\"start\":3185},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3193,\"start\":3189},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3197,\"start\":3193},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3326,\"start\":3322},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3375,\"start\":3371},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3455,\"start\":3451},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3459,\"start\":3455},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3463,\"start\":3459},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3467,\"start\":3463},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3471,\"start\":3467},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3475,\"start\":3471},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3479,\"start\":3475},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3483,\"start\":3479},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3487,\"start\":3483},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3491,\"start\":3487},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3495,\"start\":3491},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3499,\"start\":3495},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":4358,\"start\":4354},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":4362,\"start\":4358},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4366,\"start\":4362},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":4370,\"start\":4366},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":4576,\"start\":4572},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":4580,\"start\":4576},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":4584,\"start\":4580},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":4588,\"start\":4584},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":5331,\"start\":5327},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":5357,\"start\":5353},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":5382,\"start\":5378},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":5412,\"start\":5408},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":5463,\"start\":5459},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":6992,\"start\":6988},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":7393,\"start\":7389},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":9595,\"start\":9591},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":10987,\"start\":10983},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":13206,\"start\":13202},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14391,\"start\":14387},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":14394,\"start\":14391},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":14754,\"start\":14750},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15513,\"start\":15509},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":15838,\"start\":15834},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":16444,\"start\":16440},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":17476,\"start\":17472},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":18277,\"start\":18273},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":19234,\"start\":19230},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":19240,\"start\":19236},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19336,\"start\":19332},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":21064,\"start\":21060},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":21098,\"start\":21094},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":21361,\"start\":21357},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":21389,\"start\":21385},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":21965,\"start\":21961},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":22159,\"start\":22155},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":22661,\"start\":22657},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":22747,\"start\":22743},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":23048,\"start\":23044},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":24395,\"start\":24391},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25785,\"start\":25782},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":25848,\"start\":25844},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":29301,\"start\":29297},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":31074,\"start\":31070},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":34169,\"start\":34166}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":33131,\"start\":33020},{\"attributes\":{\"id\":\"fig_1\"},\"end\":33203,\"start\":33132},{\"attributes\":{\"id\":\"fig_2\"},\"end\":33484,\"start\":33204},{\"attributes\":{\"id\":\"fig_4\"},\"end\":34580,\"start\":33485},{\"attributes\":{\"id\":\"fig_5\"},\"end\":34964,\"start\":34581},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":35618,\"start\":34965},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":36563,\"start\":35619},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":36973,\"start\":36564},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":37081,\"start\":36974}]", "paragraph": "[{\"end\":2459,\"start\":2039},{\"end\":3812,\"start\":2461},{\"end\":5015,\"start\":3814},{\"end\":5598,\"start\":5017},{\"end\":6267,\"start\":5600},{\"end\":6559,\"start\":6269},{\"end\":6881,\"start\":6561},{\"end\":7215,\"start\":6883},{\"end\":7761,\"start\":7250},{\"end\":8106,\"start\":7763},{\"end\":8511,\"start\":8179},{\"end\":8887,\"start\":8513},{\"end\":8992,\"start\":8961},{\"end\":9890,\"start\":8994},{\"end\":10723,\"start\":9986},{\"end\":10905,\"start\":10738},{\"end\":11311,\"start\":10941},{\"end\":11474,\"start\":11313},{\"end\":11892,\"start\":11530},{\"end\":12291,\"start\":12020},{\"end\":12340,\"start\":12293},{\"end\":12537,\"start\":12454},{\"end\":12590,\"start\":12585},{\"end\":12824,\"start\":12705},{\"end\":13041,\"start\":12869},{\"end\":14671,\"start\":13109},{\"end\":15447,\"start\":14673},{\"end\":15527,\"start\":15449},{\"end\":15945,\"start\":15591},{\"end\":15991,\"start\":15966},{\"end\":16478,\"start\":16044},{\"end\":16884,\"start\":16539},{\"end\":16977,\"start\":16886},{\"end\":17258,\"start\":17162},{\"end\":17363,\"start\":17312},{\"end\":17827,\"start\":17365},{\"end\":18073,\"start\":17855},{\"end\":18423,\"start\":18098},{\"end\":18484,\"start\":18479},{\"end\":18595,\"start\":18511},{\"end\":18817,\"start\":18812},{\"end\":18937,\"start\":18929},{\"end\":19350,\"start\":19174},{\"end\":19478,\"start\":19411},{\"end\":19627,\"start\":19496},{\"end\":20680,\"start\":19629},{\"end\":21616,\"start\":20735},{\"end\":22662,\"start\":21643},{\"end\":24042,\"start\":22664},{\"end\":24656,\"start\":24044},{\"end\":25996,\"start\":24658},{\"end\":26313,\"start\":25998},{\"end\":27328,\"start\":26315},{\"end\":29041,\"start\":27330},{\"end\":29386,\"start\":29043},{\"end\":30266,\"start\":29480},{\"end\":30731,\"start\":30268},{\"end\":31320,\"start\":30733},{\"end\":32265,\"start\":31340},{\"end\":33019,\"start\":32267}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8178,\"start\":8107},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8960,\"start\":8888},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9985,\"start\":9891},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11529,\"start\":11475},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12019,\"start\":11893},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12453,\"start\":12341},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12584,\"start\":12538},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12704,\"start\":12591},{\"attributes\":{\"id\":\"formula_8\"},\"end\":12868,\"start\":12825},{\"attributes\":{\"id\":\"formula_9\"},\"end\":13108,\"start\":13042},{\"attributes\":{\"id\":\"formula_10\"},\"end\":15590,\"start\":15528},{\"attributes\":{\"id\":\"formula_11\"},\"end\":16043,\"start\":15992},{\"attributes\":{\"id\":\"formula_12\"},\"end\":16538,\"start\":16479},{\"attributes\":{\"id\":\"formula_13\"},\"end\":17161,\"start\":16978},{\"attributes\":{\"id\":\"formula_14\"},\"end\":17311,\"start\":17259},{\"attributes\":{\"id\":\"formula_15\"},\"end\":18097,\"start\":18074},{\"attributes\":{\"id\":\"formula_16\"},\"end\":18478,\"start\":18424},{\"attributes\":{\"id\":\"formula_17\"},\"end\":18510,\"start\":18485},{\"attributes\":{\"id\":\"formula_18\"},\"end\":18706,\"start\":18596},{\"attributes\":{\"id\":\"formula_19\"},\"end\":18811,\"start\":18706},{\"attributes\":{\"id\":\"formula_20\"},\"end\":18928,\"start\":18818},{\"attributes\":{\"id\":\"formula_21\"},\"end\":19173,\"start\":18938},{\"attributes\":{\"id\":\"formula_22\"},\"end\":19410,\"start\":19351},{\"attributes\":{\"id\":\"formula_23\"},\"end\":19495,\"start\":19479},{\"attributes\":{\"id\":\"formula_25\"},\"end\":29438,\"start\":29387},{\"attributes\":{\"id\":\"formula_26\"},\"end\":29479,\"start\":29438}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":25213,\"start\":25206},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":26403,\"start\":26394},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":27671,\"start\":27662},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":31161,\"start\":31113}]", "section_header": "[{\"end\":2037,\"start\":2022},{\"end\":7248,\"start\":7218},{\"end\":10736,\"start\":10726},{\"end\":10939,\"start\":10908},{\"end\":15964,\"start\":15948},{\"end\":17853,\"start\":17830},{\"end\":20733,\"start\":20683},{\"end\":21641,\"start\":21619},{\"end\":31338,\"start\":31323},{\"end\":33029,\"start\":33021},{\"end\":33213,\"start\":33205},{\"end\":33492,\"start\":33486},{\"end\":34590,\"start\":34582},{\"end\":34975,\"start\":34966},{\"end\":35631,\"start\":35620},{\"end\":36985,\"start\":36975}]", "table": "[{\"end\":35618,\"start\":35136},{\"end\":36563,\"start\":36310},{\"end\":36973,\"start\":36649}]", "figure_caption": "[{\"end\":33131,\"start\":33031},{\"end\":33203,\"start\":33134},{\"end\":33484,\"start\":33215},{\"end\":34580,\"start\":33493},{\"end\":34964,\"start\":34592},{\"end\":35136,\"start\":34977},{\"end\":36310,\"start\":35635},{\"end\":36649,\"start\":36566},{\"end\":37081,\"start\":36988}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11310,\"start\":11304},{\"end\":15854,\"start\":15848},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16279,\"start\":16273},{\"end\":24782,\"start\":24775},{\"end\":28464,\"start\":28458},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":29836,\"start\":29830},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":29991,\"start\":29982},{\"end\":30842,\"start\":30836}]", "bib_author_first_name": "[{\"end\":37223,\"start\":37219},{\"end\":37421,\"start\":37420},{\"end\":37434,\"start\":37428},{\"end\":37616,\"start\":37615},{\"end\":37628,\"start\":37625},{\"end\":37841,\"start\":37835},{\"end\":38047,\"start\":38041},{\"end\":38049,\"start\":38048},{\"end\":38064,\"start\":38058},{\"end\":38081,\"start\":38073},{\"end\":38091,\"start\":38087},{\"end\":38093,\"start\":38092},{\"end\":38104,\"start\":38100},{\"end\":38405,\"start\":38400},{\"end\":38421,\"start\":38416},{\"end\":38436,\"start\":38430},{\"end\":38454,\"start\":38447},{\"end\":38473,\"start\":38467},{\"end\":38485,\"start\":38481},{\"end\":38723,\"start\":38722},{\"end\":38727,\"start\":38724},{\"end\":38744,\"start\":38738},{\"end\":38746,\"start\":38745},{\"end\":39111,\"start\":39106},{\"end\":39125,\"start\":39116},{\"end\":39135,\"start\":39130},{\"end\":39152,\"start\":39143},{\"end\":39165,\"start\":39157},{\"end\":39414,\"start\":39411},{\"end\":39430,\"start\":39424},{\"end\":39441,\"start\":39437},{\"end\":39461,\"start\":39456},{\"end\":39463,\"start\":39462},{\"end\":39478,\"start\":39474},{\"end\":39690,\"start\":39684},{\"end\":39704,\"start\":39696},{\"end\":39721,\"start\":39713},{\"end\":39723,\"start\":39722},{\"end\":39739,\"start\":39731},{\"end\":39755,\"start\":39750},{\"end\":39757,\"start\":39756},{\"end\":39772,\"start\":39767},{\"end\":39787,\"start\":39784},{\"end\":39789,\"start\":39788},{\"end\":39804,\"start\":39801},{\"end\":39817,\"start\":39812},{\"end\":39832,\"start\":39829},{\"end\":39847,\"start\":39842},{\"end\":40331,\"start\":40324},{\"end\":40350,\"start\":40344},{\"end\":40364,\"start\":40360},{\"end\":40631,\"start\":40624},{\"end\":40645,\"start\":40639},{\"end\":40661,\"start\":40657},{\"end\":40677,\"start\":40671},{\"end\":40694,\"start\":40687},{\"end\":40708,\"start\":40702},{\"end\":40710,\"start\":40709},{\"end\":40959,\"start\":40954},{\"end\":40977,\"start\":40968},{\"end\":41124,\"start\":41120},{\"end\":41351,\"start\":41346},{\"end\":41364,\"start\":41359},{\"end\":41375,\"start\":41371},{\"end\":41389,\"start\":41385},{\"end\":41398,\"start\":41397},{\"end\":41411,\"start\":41407},{\"end\":41425,\"start\":41420},{\"end\":41441,\"start\":41435},{\"end\":41460,\"start\":41457},{\"end\":41476,\"start\":41471},{\"end\":41478,\"start\":41477},{\"end\":41818,\"start\":41811},{\"end\":41829,\"start\":41826},{\"end\":41842,\"start\":41836},{\"end\":41859,\"start\":41849},{\"end\":41873,\"start\":41866},{\"end\":41886,\"start\":41880},{\"end\":41896,\"start\":41892},{\"end\":41906,\"start\":41902},{\"end\":41915,\"start\":41911},{\"end\":41924,\"start\":41922},{\"end\":42212,\"start\":42206},{\"end\":42225,\"start\":42221},{\"end\":42245,\"start\":42242},{\"end\":42248,\"start\":42246},{\"end\":42261,\"start\":42255},{\"end\":42274,\"start\":42268},{\"end\":42295,\"start\":42288},{\"end\":42311,\"start\":42303},{\"end\":42330,\"start\":42322},{\"end\":42346,\"start\":42341},{\"end\":42348,\"start\":42347},{\"end\":42362,\"start\":42359},{\"end\":42696,\"start\":42695},{\"end\":42711,\"start\":42705},{\"end\":42727,\"start\":42723},{\"end\":42742,\"start\":42737},{\"end\":42744,\"start\":42743},{\"end\":42761,\"start\":42755},{\"end\":42775,\"start\":42768},{\"end\":42789,\"start\":42783},{\"end\":42791,\"start\":42790},{\"end\":42807,\"start\":42801},{\"end\":42821,\"start\":42817},{\"end\":42834,\"start\":42828},{\"end\":42850,\"start\":42843},{\"end\":42852,\"start\":42851},{\"end\":43208,\"start\":43202},{\"end\":43221,\"start\":43215},{\"end\":43228,\"start\":43227},{\"end\":43240,\"start\":43236},{\"end\":43570,\"start\":43563},{\"end\":43586,\"start\":43580},{\"end\":43601,\"start\":43596},{\"end\":43620,\"start\":43612},{\"end\":43634,\"start\":43627},{\"end\":43646,\"start\":43641},{\"end\":43648,\"start\":43647},{\"end\":43659,\"start\":43655},{\"end\":43680,\"start\":43674},{\"end\":43682,\"start\":43681},{\"end\":43977,\"start\":43971},{\"end\":43992,\"start\":43985},{\"end\":44007,\"start\":44004},{\"end\":44279,\"start\":44272},{\"end\":44297,\"start\":44290},{\"end\":44299,\"start\":44298},{\"end\":44317,\"start\":44310},{\"end\":44329,\"start\":44325},{\"end\":44331,\"start\":44330},{\"end\":44347,\"start\":44340},{\"end\":44362,\"start\":44357},{\"end\":44364,\"start\":44363},{\"end\":44374,\"start\":44371},{\"end\":44376,\"start\":44375},{\"end\":44663,\"start\":44658},{\"end\":44676,\"start\":44672},{\"end\":44693,\"start\":44687},{\"end\":44695,\"start\":44694},{\"end\":44709,\"start\":44703},{\"end\":44918,\"start\":44917},{\"end\":44929,\"start\":44928},{\"end\":44939,\"start\":44938},{\"end\":44951,\"start\":44950},{\"end\":45225,\"start\":45212},{\"end\":45235,\"start\":45232},{\"end\":45254,\"start\":45240},{\"end\":45263,\"start\":45261},{\"end\":45276,\"start\":45270},{\"end\":45808,\"start\":45805},{\"end\":45819,\"start\":45813},{\"end\":46298,\"start\":46285},{\"end\":46307,\"start\":46305},{\"end\":46314,\"start\":46312},{\"end\":46328,\"start\":46322},{\"end\":46339,\"start\":46335},{\"end\":46341,\"start\":46340},{\"end\":46357,\"start\":46351},{\"end\":46363,\"start\":46358},{\"end\":46385,\"start\":46377},{\"end\":46887,\"start\":46879},{\"end\":46904,\"start\":46899},{\"end\":46918,\"start\":46912},{\"end\":46931,\"start\":46925},{\"end\":47173,\"start\":47167},{\"end\":47188,\"start\":47181},{\"end\":47497,\"start\":47493},{\"end\":47510,\"start\":47502},{\"end\":47521,\"start\":47516},{\"end\":47529,\"start\":47526},{\"end\":47542,\"start\":47541},{\"end\":47561,\"start\":47560},{\"end\":47582,\"start\":47581},{\"end\":48045,\"start\":48037},{\"end\":48047,\"start\":48046},{\"end\":48063,\"start\":48054},{\"end\":48072,\"start\":48071},{\"end\":48086,\"start\":48081},{\"end\":48101,\"start\":48095},{\"end\":48120,\"start\":48114},{\"end\":48136,\"start\":48129},{\"end\":48138,\"start\":48137},{\"end\":48478,\"start\":48471},{\"end\":48489,\"start\":48483},{\"end\":48503,\"start\":48494},{\"end\":48513,\"start\":48510},{\"end\":48864,\"start\":48858},{\"end\":48877,\"start\":48869},{\"end\":48886,\"start\":48882},{\"end\":48900,\"start\":48893},{\"end\":49167,\"start\":49160},{\"end\":49180,\"start\":49175},{\"end\":49182,\"start\":49181},{\"end\":49199,\"start\":49192},{\"end\":49207,\"start\":49200},{\"end\":49222,\"start\":49217},{\"end\":49224,\"start\":49223},{\"end\":49237,\"start\":49233},{\"end\":49239,\"start\":49238},{\"end\":49255,\"start\":49249},{\"end\":49564,\"start\":49560},{\"end\":49578,\"start\":49569},{\"end\":49591,\"start\":49585},{\"end\":49601,\"start\":49598},{\"end\":49867,\"start\":49860},{\"end\":49881,\"start\":49875},{\"end\":49897,\"start\":49893},{\"end\":49913,\"start\":49907},{\"end\":49930,\"start\":49923},{\"end\":49944,\"start\":49938},{\"end\":49946,\"start\":49945},{\"end\":50297,\"start\":50294},{\"end\":50310,\"start\":50304},{\"end\":50326,\"start\":50317},{\"end\":50337,\"start\":50333},{\"end\":50347,\"start\":50344},{\"end\":50611,\"start\":50602},{\"end\":50624,\"start\":50618},{\"end\":50635,\"start\":50631},{\"end\":50648,\"start\":50642},{\"end\":50658,\"start\":50655},{\"end\":50880,\"start\":50874},{\"end\":50893,\"start\":50887},{\"end\":50905,\"start\":50899},{\"end\":50918,\"start\":50910},{\"end\":50929,\"start\":50925},{\"end\":50942,\"start\":50935},{\"end\":50953,\"start\":50950},{\"end\":50965,\"start\":50958},{\"end\":50980,\"start\":50973},{\"end\":50994,\"start\":50987},{\"end\":51007,\"start\":50999},{\"end\":51493,\"start\":51488},{\"end\":51507,\"start\":51500},{\"end\":51515,\"start\":51512},{\"end\":51756,\"start\":51751},{\"end\":51769,\"start\":51764},{\"end\":51785,\"start\":51778},{\"end\":51802,\"start\":51794},{\"end\":51818,\"start\":51811},{\"end\":51834,\"start\":51828},{\"end\":52140,\"start\":52130},{\"end\":52158,\"start\":52150},{\"end\":52170,\"start\":52164},{\"end\":52828,\"start\":52824},{\"end\":52842,\"start\":52836},{\"end\":52852,\"start\":52848},{\"end\":52863,\"start\":52857},{\"end\":52875,\"start\":52870},{\"end\":52885,\"start\":52880},{\"end\":53320,\"start\":53314},{\"end\":53333,\"start\":53328},{\"end\":53344,\"start\":53340},{\"end\":53357,\"start\":53352},{\"end\":53368,\"start\":53362},{\"end\":53631,\"start\":53626},{\"end\":53645,\"start\":53640},{\"end\":54005,\"start\":53998},{\"end\":54015,\"start\":54012},{\"end\":54335,\"start\":54323},{\"end\":54359,\"start\":54352},{\"end\":54377,\"start\":54369},{\"end\":54395,\"start\":54389},{\"end\":54413,\"start\":54407},{\"end\":54424,\"start\":54421},{\"end\":54691,\"start\":54685},{\"end\":54703,\"start\":54699},{\"end\":54718,\"start\":54714},{\"end\":54738,\"start\":54730},{\"end\":54753,\"start\":54747},{\"end\":54755,\"start\":54754},{\"end\":54987,\"start\":54973},{\"end\":55001,\"start\":54994},{\"end\":55017,\"start\":55007},{\"end\":55218,\"start\":55213},{\"end\":55235,\"start\":55227},{\"end\":55249,\"start\":55243},{\"end\":55263,\"start\":55255},{\"end\":55273,\"start\":55264},{\"end\":55607,\"start\":55601},{\"end\":55621,\"start\":55617},{\"end\":55635,\"start\":55631},{\"end\":55649,\"start\":55644},{\"end\":55666,\"start\":55661},{\"end\":55679,\"start\":55674},{\"end\":55681,\"start\":55680},{\"end\":55695,\"start\":55689},{\"end\":55709,\"start\":55704},{\"end\":56253,\"start\":56244},{\"end\":56266,\"start\":56258},{\"end\":56279,\"start\":56273},{\"end\":56294,\"start\":56285},{\"end\":56307,\"start\":56300},{\"end\":56323,\"start\":56315},{\"end\":56334,\"start\":56328},{\"end\":56798,\"start\":56792},{\"end\":56810,\"start\":56804},{\"end\":56823,\"start\":56816},{\"end\":56838,\"start\":56829},{\"end\":56849,\"start\":56844},{\"end\":57195,\"start\":57190},{\"end\":57205,\"start\":57201},{\"end\":57221,\"start\":57216},{\"end\":57230,\"start\":57226},{\"end\":57246,\"start\":57245},{\"end\":57260,\"start\":57254},{\"end\":57591,\"start\":57586},{\"end\":57604,\"start\":57600},{\"end\":57626,\"start\":57614},{\"end\":57650,\"start\":57643},{\"end\":57666,\"start\":57663},{\"end\":57967,\"start\":57966},{\"end\":57982,\"start\":57977},{\"end\":58196,\"start\":58188},{\"end\":58226,\"start\":58218},{\"end\":58461,\"start\":58453},{\"end\":58464,\"start\":58462},{\"end\":58480,\"start\":58472},{\"end\":58502,\"start\":58488},{\"end\":58516,\"start\":58509},{\"end\":58532,\"start\":58526},{\"end\":58953,\"start\":58949},{\"end\":58955,\"start\":58954},{\"end\":58965,\"start\":58964},{\"end\":58967,\"start\":58966},{\"end\":58983,\"start\":58977},{\"end\":59155,\"start\":59151},{\"end\":59526,\"start\":59523},{\"end\":59545,\"start\":59539},{\"end\":59559,\"start\":59554},{\"end\":59776,\"start\":59769},{\"end\":59792,\"start\":59782},{\"end\":59813,\"start\":59806},{\"end\":60101,\"start\":60098},{\"end\":60119,\"start\":60106},{\"end\":60132,\"start\":60126},{\"end\":60147,\"start\":60139},{\"end\":60451,\"start\":60450},{\"end\":60471,\"start\":60462},{\"end\":60483,\"start\":60478},{\"end\":60496,\"start\":60491},{\"end\":60511,\"start\":60505},{\"end\":60526,\"start\":60520},{\"end\":60548,\"start\":60539},{\"end\":60884,\"start\":60875},{\"end\":60899,\"start\":60894},{\"end\":60915,\"start\":60907},{\"end\":61456,\"start\":61450},{\"end\":61468,\"start\":61461},{\"end\":61477,\"start\":61473},{\"end\":61489,\"start\":61482},{\"end\":61752,\"start\":61746},{\"end\":61763,\"start\":61758},{\"end\":61884,\"start\":61883},{\"end\":61900,\"start\":61895},{\"end\":61904,\"start\":61901}]", "bib_author_last_name": "[{\"end\":37232,\"start\":37224},{\"end\":37426,\"start\":37422},{\"end\":37441,\"start\":37435},{\"end\":37452,\"start\":37443},{\"end\":37623,\"start\":37617},{\"end\":37635,\"start\":37629},{\"end\":37644,\"start\":37637},{\"end\":37851,\"start\":37842},{\"end\":38056,\"start\":38050},{\"end\":38071,\"start\":38065},{\"end\":38085,\"start\":38082},{\"end\":38098,\"start\":38094},{\"end\":38107,\"start\":38105},{\"end\":38414,\"start\":38406},{\"end\":38428,\"start\":38422},{\"end\":38445,\"start\":38437},{\"end\":38465,\"start\":38455},{\"end\":38479,\"start\":38474},{\"end\":38491,\"start\":38486},{\"end\":38736,\"start\":38728},{\"end\":38754,\"start\":38747},{\"end\":38761,\"start\":38756},{\"end\":39114,\"start\":39112},{\"end\":39128,\"start\":39126},{\"end\":39141,\"start\":39136},{\"end\":39155,\"start\":39153},{\"end\":39169,\"start\":39166},{\"end\":39422,\"start\":39415},{\"end\":39435,\"start\":39431},{\"end\":39454,\"start\":39442},{\"end\":39472,\"start\":39464},{\"end\":39483,\"start\":39479},{\"end\":39694,\"start\":39691},{\"end\":39711,\"start\":39705},{\"end\":39729,\"start\":39724},{\"end\":39748,\"start\":39740},{\"end\":39765,\"start\":39758},{\"end\":39782,\"start\":39773},{\"end\":39799,\"start\":39790},{\"end\":39810,\"start\":39805},{\"end\":39827,\"start\":39818},{\"end\":39840,\"start\":39833},{\"end\":39854,\"start\":39848},{\"end\":39859,\"start\":39856},{\"end\":40342,\"start\":40332},{\"end\":40358,\"start\":40351},{\"end\":40370,\"start\":40365},{\"end\":40622,\"start\":40613},{\"end\":40637,\"start\":40632},{\"end\":40655,\"start\":40646},{\"end\":40669,\"start\":40662},{\"end\":40685,\"start\":40678},{\"end\":40700,\"start\":40695},{\"end\":40716,\"start\":40711},{\"end\":40725,\"start\":40718},{\"end\":40966,\"start\":40960},{\"end\":40989,\"start\":40978},{\"end\":41133,\"start\":41125},{\"end\":41357,\"start\":41352},{\"end\":41369,\"start\":41365},{\"end\":41383,\"start\":41376},{\"end\":41395,\"start\":41390},{\"end\":41405,\"start\":41399},{\"end\":41418,\"start\":41412},{\"end\":41433,\"start\":41426},{\"end\":41448,\"start\":41442},{\"end\":41455,\"start\":41450},{\"end\":41469,\"start\":41461},{\"end\":41486,\"start\":41479},{\"end\":41493,\"start\":41488},{\"end\":41824,\"start\":41819},{\"end\":41834,\"start\":41830},{\"end\":41847,\"start\":41843},{\"end\":41864,\"start\":41860},{\"end\":41878,\"start\":41874},{\"end\":41890,\"start\":41887},{\"end\":41900,\"start\":41897},{\"end\":41909,\"start\":41907},{\"end\":41920,\"start\":41916},{\"end\":41927,\"start\":41925},{\"end\":42219,\"start\":42213},{\"end\":42240,\"start\":42226},{\"end\":42253,\"start\":42249},{\"end\":42266,\"start\":42262},{\"end\":42286,\"start\":42275},{\"end\":42301,\"start\":42296},{\"end\":42320,\"start\":42312},{\"end\":42339,\"start\":42331},{\"end\":42357,\"start\":42349},{\"end\":42368,\"start\":42363},{\"end\":42703,\"start\":42697},{\"end\":42721,\"start\":42712},{\"end\":42735,\"start\":42728},{\"end\":42753,\"start\":42745},{\"end\":42766,\"start\":42762},{\"end\":42781,\"start\":42776},{\"end\":42799,\"start\":42792},{\"end\":42815,\"start\":42808},{\"end\":42826,\"start\":42822},{\"end\":42841,\"start\":42835},{\"end\":42858,\"start\":42853},{\"end\":43213,\"start\":43209},{\"end\":43225,\"start\":43222},{\"end\":43234,\"start\":43229},{\"end\":43249,\"start\":43241},{\"end\":43255,\"start\":43251},{\"end\":43578,\"start\":43571},{\"end\":43594,\"start\":43587},{\"end\":43610,\"start\":43602},{\"end\":43625,\"start\":43621},{\"end\":43639,\"start\":43635},{\"end\":43653,\"start\":43649},{\"end\":43672,\"start\":43660},{\"end\":43695,\"start\":43683},{\"end\":43983,\"start\":43978},{\"end\":44002,\"start\":43993},{\"end\":44015,\"start\":44008},{\"end\":44288,\"start\":44280},{\"end\":44308,\"start\":44300},{\"end\":44323,\"start\":44318},{\"end\":44338,\"start\":44332},{\"end\":44355,\"start\":44348},{\"end\":44369,\"start\":44365},{\"end\":44385,\"start\":44377},{\"end\":44670,\"start\":44664},{\"end\":44685,\"start\":44677},{\"end\":44701,\"start\":44696},{\"end\":44715,\"start\":44710},{\"end\":44926,\"start\":44919},{\"end\":44936,\"start\":44930},{\"end\":44948,\"start\":44940},{\"end\":44957,\"start\":44952},{\"end\":45230,\"start\":45226},{\"end\":45238,\"start\":45236},{\"end\":45259,\"start\":45255},{\"end\":45268,\"start\":45264},{\"end\":45281,\"start\":45277},{\"end\":45811,\"start\":45809},{\"end\":45827,\"start\":45820},{\"end\":46303,\"start\":46299},{\"end\":46310,\"start\":46308},{\"end\":46320,\"start\":46315},{\"end\":46333,\"start\":46329},{\"end\":46349,\"start\":46342},{\"end\":46375,\"start\":46364},{\"end\":46389,\"start\":46386},{\"end\":46897,\"start\":46888},{\"end\":46910,\"start\":46905},{\"end\":46923,\"start\":46919},{\"end\":46942,\"start\":46932},{\"end\":47179,\"start\":47174},{\"end\":47194,\"start\":47189},{\"end\":47500,\"start\":47498},{\"end\":47514,\"start\":47511},{\"end\":47524,\"start\":47522},{\"end\":47539,\"start\":47530},{\"end\":47549,\"start\":47543},{\"end\":47558,\"start\":47551},{\"end\":47569,\"start\":47562},{\"end\":47579,\"start\":47571},{\"end\":47586,\"start\":47583},{\"end\":47590,\"start\":47588},{\"end\":48052,\"start\":48048},{\"end\":48069,\"start\":48064},{\"end\":48079,\"start\":48073},{\"end\":48093,\"start\":48087},{\"end\":48112,\"start\":48102},{\"end\":48127,\"start\":48121},{\"end\":48144,\"start\":48139},{\"end\":48481,\"start\":48479},{\"end\":48492,\"start\":48490},{\"end\":48508,\"start\":48504},{\"end\":48518,\"start\":48514},{\"end\":48867,\"start\":48865},{\"end\":48880,\"start\":48878},{\"end\":48891,\"start\":48887},{\"end\":48904,\"start\":48901},{\"end\":49173,\"start\":49168},{\"end\":49190,\"start\":49183},{\"end\":49215,\"start\":49208},{\"end\":49231,\"start\":49225},{\"end\":49247,\"start\":49240},{\"end\":49262,\"start\":49256},{\"end\":49567,\"start\":49565},{\"end\":49583,\"start\":49579},{\"end\":49596,\"start\":49592},{\"end\":49606,\"start\":49602},{\"end\":49858,\"start\":49849},{\"end\":49873,\"start\":49868},{\"end\":49891,\"start\":49882},{\"end\":49905,\"start\":49898},{\"end\":49921,\"start\":49914},{\"end\":49936,\"start\":49931},{\"end\":49952,\"start\":49947},{\"end\":49961,\"start\":49954},{\"end\":50302,\"start\":50298},{\"end\":50315,\"start\":50311},{\"end\":50331,\"start\":50327},{\"end\":50342,\"start\":50338},{\"end\":50352,\"start\":50348},{\"end\":50616,\"start\":50612},{\"end\":50629,\"start\":50625},{\"end\":50640,\"start\":50636},{\"end\":50653,\"start\":50649},{\"end\":50663,\"start\":50659},{\"end\":50885,\"start\":50881},{\"end\":50897,\"start\":50894},{\"end\":50908,\"start\":50906},{\"end\":50923,\"start\":50919},{\"end\":50933,\"start\":50930},{\"end\":50948,\"start\":50943},{\"end\":50956,\"start\":50954},{\"end\":50971,\"start\":50966},{\"end\":50985,\"start\":50981},{\"end\":50997,\"start\":50995},{\"end\":51013,\"start\":51008},{\"end\":51498,\"start\":51494},{\"end\":51510,\"start\":51508},{\"end\":51520,\"start\":51516},{\"end\":51762,\"start\":51757},{\"end\":51776,\"start\":51770},{\"end\":51792,\"start\":51786},{\"end\":51809,\"start\":51803},{\"end\":51826,\"start\":51819},{\"end\":51842,\"start\":51835},{\"end\":52148,\"start\":52141},{\"end\":52162,\"start\":52159},{\"end\":52177,\"start\":52171},{\"end\":52834,\"start\":52829},{\"end\":52846,\"start\":52843},{\"end\":52855,\"start\":52853},{\"end\":52868,\"start\":52864},{\"end\":52878,\"start\":52876},{\"end\":52890,\"start\":52886},{\"end\":53326,\"start\":53321},{\"end\":53338,\"start\":53334},{\"end\":53350,\"start\":53345},{\"end\":53360,\"start\":53358},{\"end\":53373,\"start\":53369},{\"end\":53638,\"start\":53632},{\"end\":53655,\"start\":53646},{\"end\":54010,\"start\":54006},{\"end\":54022,\"start\":54016},{\"end\":54350,\"start\":54336},{\"end\":54367,\"start\":54360},{\"end\":54387,\"start\":54378},{\"end\":54405,\"start\":54396},{\"end\":54419,\"start\":54414},{\"end\":54431,\"start\":54425},{\"end\":54697,\"start\":54692},{\"end\":54712,\"start\":54704},{\"end\":54728,\"start\":54719},{\"end\":54745,\"start\":54739},{\"end\":54761,\"start\":54756},{\"end\":54992,\"start\":54988},{\"end\":55005,\"start\":55002},{\"end\":55022,\"start\":55018},{\"end\":55225,\"start\":55219},{\"end\":55241,\"start\":55236},{\"end\":55253,\"start\":55250},{\"end\":55278,\"start\":55274},{\"end\":55615,\"start\":55608},{\"end\":55629,\"start\":55622},{\"end\":55642,\"start\":55636},{\"end\":55659,\"start\":55650},{\"end\":55672,\"start\":55667},{\"end\":55687,\"start\":55682},{\"end\":55702,\"start\":55696},{\"end\":55720,\"start\":55710},{\"end\":56256,\"start\":56254},{\"end\":56271,\"start\":56267},{\"end\":56283,\"start\":56280},{\"end\":56298,\"start\":56295},{\"end\":56313,\"start\":56308},{\"end\":56326,\"start\":56324},{\"end\":56338,\"start\":56335},{\"end\":56802,\"start\":56799},{\"end\":56814,\"start\":56811},{\"end\":56827,\"start\":56824},{\"end\":56842,\"start\":56839},{\"end\":56855,\"start\":56850},{\"end\":57199,\"start\":57196},{\"end\":57214,\"start\":57206},{\"end\":57224,\"start\":57222},{\"end\":57243,\"start\":57231},{\"end\":57252,\"start\":57247},{\"end\":57268,\"start\":57261},{\"end\":57273,\"start\":57270},{\"end\":57598,\"start\":57592},{\"end\":57612,\"start\":57605},{\"end\":57641,\"start\":57627},{\"end\":57661,\"start\":57651},{\"end\":57673,\"start\":57667},{\"end\":57975,\"start\":57968},{\"end\":57990,\"start\":57983},{\"end\":57998,\"start\":57992},{\"end\":58216,\"start\":58197},{\"end\":58229,\"start\":58227},{\"end\":58236,\"start\":58231},{\"end\":58470,\"start\":58465},{\"end\":58486,\"start\":58481},{\"end\":58507,\"start\":58503},{\"end\":58524,\"start\":58517},{\"end\":58540,\"start\":58533},{\"end\":58962,\"start\":58956},{\"end\":58975,\"start\":58968},{\"end\":58991,\"start\":58984},{\"end\":59162,\"start\":59156},{\"end\":59537,\"start\":59527},{\"end\":59552,\"start\":59546},{\"end\":59569,\"start\":59560},{\"end\":59780,\"start\":59777},{\"end\":59804,\"start\":59793},{\"end\":59819,\"start\":59814},{\"end\":60104,\"start\":60102},{\"end\":60124,\"start\":60120},{\"end\":60137,\"start\":60133},{\"end\":60153,\"start\":60148},{\"end\":60460,\"start\":60452},{\"end\":60476,\"start\":60472},{\"end\":60489,\"start\":60484},{\"end\":60503,\"start\":60497},{\"end\":60518,\"start\":60512},{\"end\":60537,\"start\":60527},{\"end\":60555,\"start\":60549},{\"end\":60562,\"start\":60557},{\"end\":60892,\"start\":60885},{\"end\":60905,\"start\":60900},{\"end\":60932,\"start\":60916},{\"end\":60939,\"start\":60934},{\"end\":61459,\"start\":61457},{\"end\":61471,\"start\":61469},{\"end\":61480,\"start\":61478},{\"end\":61494,\"start\":61490},{\"end\":61756,\"start\":61753},{\"end\":61769,\"start\":61764},{\"end\":61893,\"start\":61885},{\"end\":61911,\"start\":61905},{\"end\":61915,\"start\":61913}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":37385,\"start\":37219},{\"attributes\":{\"id\":\"b1\"},\"end\":37570,\"start\":37387},{\"attributes\":{\"id\":\"b2\"},\"end\":37800,\"start\":37572},{\"attributes\":{\"id\":\"b3\"},\"end\":37981,\"start\":37802},{\"attributes\":{\"id\":\"b4\"},\"end\":38372,\"start\":37983},{\"attributes\":{\"id\":\"b5\"},\"end\":38667,\"start\":38374},{\"attributes\":{\"id\":\"b6\"},\"end\":39048,\"start\":38669},{\"attributes\":{\"id\":\"b7\"},\"end\":39376,\"start\":39050},{\"attributes\":{\"id\":\"b8\"},\"end\":39682,\"start\":39378},{\"attributes\":{\"id\":\"b9\"},\"end\":40262,\"start\":39684},{\"attributes\":{\"id\":\"b10\"},\"end\":40568,\"start\":40264},{\"attributes\":{\"id\":\"b11\"},\"end\":40952,\"start\":40570},{\"attributes\":{\"id\":\"b12\"},\"end\":41118,\"start\":40954},{\"attributes\":{\"id\":\"b13\"},\"end\":41278,\"start\":41120},{\"attributes\":{\"id\":\"b14\"},\"end\":41762,\"start\":41280},{\"attributes\":{\"id\":\"b15\"},\"end\":42152,\"start\":41764},{\"attributes\":{\"id\":\"b16\"},\"end\":42661,\"start\":42154},{\"attributes\":{\"id\":\"b17\"},\"end\":43134,\"start\":42663},{\"attributes\":{\"id\":\"b18\"},\"end\":43496,\"start\":43136},{\"attributes\":{\"id\":\"b19\"},\"end\":43969,\"start\":43498},{\"attributes\":{\"id\":\"b20\"},\"end\":44212,\"start\":43971},{\"attributes\":{\"id\":\"b21\"},\"end\":44619,\"start\":44214},{\"attributes\":{\"id\":\"b22\"},\"end\":44889,\"start\":44621},{\"attributes\":{\"id\":\"b23\"},\"end\":45104,\"start\":44891},{\"attributes\":{\"id\":\"b24\"},\"end\":45699,\"start\":45106},{\"attributes\":{\"id\":\"b25\"},\"end\":46195,\"start\":45701},{\"attributes\":{\"id\":\"b26\"},\"end\":46818,\"start\":46197},{\"attributes\":{\"id\":\"b27\"},\"end\":47165,\"start\":46820},{\"attributes\":{\"id\":\"b28\"},\"end\":47421,\"start\":47167},{\"attributes\":{\"id\":\"b29\"},\"end\":47968,\"start\":47423},{\"attributes\":{\"id\":\"b30\"},\"end\":48403,\"start\":47970},{\"attributes\":{\"id\":\"b31\"},\"end\":48782,\"start\":48405},{\"attributes\":{\"id\":\"b32\"},\"end\":49110,\"start\":48784},{\"attributes\":{\"id\":\"b33\"},\"end\":49495,\"start\":49112},{\"attributes\":{\"id\":\"b34\"},\"end\":49804,\"start\":49497},{\"attributes\":{\"id\":\"b35\"},\"end\":50198,\"start\":49806},{\"attributes\":{\"id\":\"b36\"},\"end\":50533,\"start\":50200},{\"attributes\":{\"id\":\"b37\"},\"end\":50872,\"start\":50535},{\"attributes\":{\"id\":\"b38\"},\"end\":51427,\"start\":50874},{\"attributes\":{\"id\":\"b39\"},\"end\":51711,\"start\":51429},{\"attributes\":{\"id\":\"b40\"},\"end\":52064,\"start\":51713},{\"attributes\":{\"id\":\"b41\"},\"end\":52742,\"start\":52066},{\"attributes\":{\"id\":\"b42\"},\"end\":53230,\"start\":52744},{\"attributes\":{\"id\":\"b43\"},\"end\":53591,\"start\":53232},{\"attributes\":{\"id\":\"b44\"},\"end\":53996,\"start\":53593},{\"attributes\":{\"id\":\"b45\"},\"end\":54253,\"start\":53998},{\"attributes\":{\"id\":\"b46\"},\"end\":54683,\"start\":54255},{\"attributes\":{\"id\":\"b47\"},\"end\":54971,\"start\":54685},{\"attributes\":{\"id\":\"b48\"},\"end\":55211,\"start\":54973},{\"attributes\":{\"id\":\"b49\"},\"end\":55572,\"start\":55213},{\"attributes\":{\"id\":\"b50\"},\"end\":56156,\"start\":55574},{\"attributes\":{\"id\":\"b51\"},\"end\":56738,\"start\":56158},{\"attributes\":{\"id\":\"b52\"},\"end\":57188,\"start\":56740},{\"attributes\":{\"id\":\"b53\"},\"end\":57503,\"start\":57190},{\"attributes\":{\"id\":\"b54\"},\"end\":57919,\"start\":57505},{\"attributes\":{\"id\":\"b55\"},\"end\":58167,\"start\":57921},{\"attributes\":{\"id\":\"b56\"},\"end\":58402,\"start\":58169},{\"attributes\":{\"id\":\"b57\"},\"end\":58919,\"start\":58404},{\"attributes\":{\"id\":\"b58\"},\"end\":59128,\"start\":58921},{\"attributes\":{\"id\":\"b59\"},\"end\":59506,\"start\":59130},{\"attributes\":{\"id\":\"b60\"},\"end\":59696,\"start\":59508},{\"attributes\":{\"id\":\"b61\"},\"end\":59996,\"start\":59698},{\"attributes\":{\"id\":\"b62\"},\"end\":60381,\"start\":59998},{\"attributes\":{\"id\":\"b63\"},\"end\":60820,\"start\":60383},{\"attributes\":{\"id\":\"b64\"},\"end\":61370,\"start\":60822},{\"attributes\":{\"id\":\"b65\"},\"end\":61711,\"start\":61372},{\"attributes\":{\"id\":\"b66\"},\"end\":61837,\"start\":61713},{\"attributes\":{\"id\":\"b67\"},\"end\":62205,\"start\":61839}]", "bib_title": "[{\"end\":37418,\"start\":37387},{\"end\":37613,\"start\":37572},{\"end\":37833,\"start\":37802},{\"end\":38039,\"start\":37983},{\"end\":38398,\"start\":38374},{\"end\":38720,\"start\":38669},{\"end\":39104,\"start\":39050},{\"end\":39409,\"start\":39378},{\"end\":40322,\"start\":40264},{\"end\":40611,\"start\":40570},{\"end\":41344,\"start\":41280},{\"end\":41809,\"start\":41764},{\"end\":42693,\"start\":42663},{\"end\":43200,\"start\":43136},{\"end\":43561,\"start\":43498},{\"end\":44270,\"start\":44214},{\"end\":44656,\"start\":44621},{\"end\":44915,\"start\":44891},{\"end\":45210,\"start\":45106},{\"end\":45803,\"start\":45701},{\"end\":46283,\"start\":46197},{\"end\":46877,\"start\":46820},{\"end\":47491,\"start\":47423},{\"end\":48035,\"start\":47970},{\"end\":48469,\"start\":48405},{\"end\":48856,\"start\":48784},{\"end\":49158,\"start\":49112},{\"end\":49558,\"start\":49497},{\"end\":49847,\"start\":49806},{\"end\":50600,\"start\":50535},{\"end\":51486,\"start\":51429},{\"end\":51749,\"start\":51713},{\"end\":52128,\"start\":52066},{\"end\":52822,\"start\":52744},{\"end\":53312,\"start\":53232},{\"end\":53624,\"start\":53593},{\"end\":55599,\"start\":55574},{\"end\":56242,\"start\":56158},{\"end\":56790,\"start\":56740},{\"end\":57964,\"start\":57921},{\"end\":58186,\"start\":58169},{\"end\":58451,\"start\":58404},{\"end\":58947,\"start\":58921},{\"end\":59149,\"start\":59130},{\"end\":59767,\"start\":59698},{\"end\":60096,\"start\":59998},{\"end\":60448,\"start\":60383},{\"end\":60873,\"start\":60822},{\"end\":61448,\"start\":61372},{\"end\":61881,\"start\":61839}]", "bib_author": "[{\"end\":37234,\"start\":37219},{\"end\":37428,\"start\":37420},{\"end\":37443,\"start\":37428},{\"end\":37454,\"start\":37443},{\"end\":37625,\"start\":37615},{\"end\":37637,\"start\":37625},{\"end\":37646,\"start\":37637},{\"end\":37853,\"start\":37835},{\"end\":38058,\"start\":38041},{\"end\":38073,\"start\":38058},{\"end\":38087,\"start\":38073},{\"end\":38100,\"start\":38087},{\"end\":38109,\"start\":38100},{\"end\":38416,\"start\":38400},{\"end\":38430,\"start\":38416},{\"end\":38447,\"start\":38430},{\"end\":38467,\"start\":38447},{\"end\":38481,\"start\":38467},{\"end\":38493,\"start\":38481},{\"end\":38738,\"start\":38722},{\"end\":38756,\"start\":38738},{\"end\":38763,\"start\":38756},{\"end\":39116,\"start\":39106},{\"end\":39130,\"start\":39116},{\"end\":39143,\"start\":39130},{\"end\":39157,\"start\":39143},{\"end\":39171,\"start\":39157},{\"end\":39424,\"start\":39411},{\"end\":39437,\"start\":39424},{\"end\":39456,\"start\":39437},{\"end\":39474,\"start\":39456},{\"end\":39485,\"start\":39474},{\"end\":39696,\"start\":39684},{\"end\":39713,\"start\":39696},{\"end\":39731,\"start\":39713},{\"end\":39750,\"start\":39731},{\"end\":39767,\"start\":39750},{\"end\":39784,\"start\":39767},{\"end\":39801,\"start\":39784},{\"end\":39812,\"start\":39801},{\"end\":39829,\"start\":39812},{\"end\":39842,\"start\":39829},{\"end\":39856,\"start\":39842},{\"end\":39861,\"start\":39856},{\"end\":40344,\"start\":40324},{\"end\":40360,\"start\":40344},{\"end\":40372,\"start\":40360},{\"end\":40624,\"start\":40613},{\"end\":40639,\"start\":40624},{\"end\":40657,\"start\":40639},{\"end\":40671,\"start\":40657},{\"end\":40687,\"start\":40671},{\"end\":40702,\"start\":40687},{\"end\":40718,\"start\":40702},{\"end\":40727,\"start\":40718},{\"end\":40968,\"start\":40954},{\"end\":40991,\"start\":40968},{\"end\":41135,\"start\":41120},{\"end\":41359,\"start\":41346},{\"end\":41371,\"start\":41359},{\"end\":41385,\"start\":41371},{\"end\":41397,\"start\":41385},{\"end\":41407,\"start\":41397},{\"end\":41420,\"start\":41407},{\"end\":41435,\"start\":41420},{\"end\":41450,\"start\":41435},{\"end\":41457,\"start\":41450},{\"end\":41471,\"start\":41457},{\"end\":41488,\"start\":41471},{\"end\":41495,\"start\":41488},{\"end\":41826,\"start\":41811},{\"end\":41836,\"start\":41826},{\"end\":41849,\"start\":41836},{\"end\":41866,\"start\":41849},{\"end\":41880,\"start\":41866},{\"end\":41892,\"start\":41880},{\"end\":41902,\"start\":41892},{\"end\":41911,\"start\":41902},{\"end\":41922,\"start\":41911},{\"end\":41929,\"start\":41922},{\"end\":42221,\"start\":42206},{\"end\":42242,\"start\":42221},{\"end\":42255,\"start\":42242},{\"end\":42268,\"start\":42255},{\"end\":42288,\"start\":42268},{\"end\":42303,\"start\":42288},{\"end\":42322,\"start\":42303},{\"end\":42341,\"start\":42322},{\"end\":42359,\"start\":42341},{\"end\":42370,\"start\":42359},{\"end\":42705,\"start\":42695},{\"end\":42723,\"start\":42705},{\"end\":42737,\"start\":42723},{\"end\":42755,\"start\":42737},{\"end\":42768,\"start\":42755},{\"end\":42783,\"start\":42768},{\"end\":42801,\"start\":42783},{\"end\":42817,\"start\":42801},{\"end\":42828,\"start\":42817},{\"end\":42843,\"start\":42828},{\"end\":42860,\"start\":42843},{\"end\":43215,\"start\":43202},{\"end\":43227,\"start\":43215},{\"end\":43236,\"start\":43227},{\"end\":43251,\"start\":43236},{\"end\":43257,\"start\":43251},{\"end\":43580,\"start\":43563},{\"end\":43596,\"start\":43580},{\"end\":43612,\"start\":43596},{\"end\":43627,\"start\":43612},{\"end\":43641,\"start\":43627},{\"end\":43655,\"start\":43641},{\"end\":43674,\"start\":43655},{\"end\":43697,\"start\":43674},{\"end\":43985,\"start\":43971},{\"end\":44004,\"start\":43985},{\"end\":44017,\"start\":44004},{\"end\":44290,\"start\":44272},{\"end\":44310,\"start\":44290},{\"end\":44325,\"start\":44310},{\"end\":44340,\"start\":44325},{\"end\":44357,\"start\":44340},{\"end\":44371,\"start\":44357},{\"end\":44387,\"start\":44371},{\"end\":44672,\"start\":44658},{\"end\":44687,\"start\":44672},{\"end\":44703,\"start\":44687},{\"end\":44717,\"start\":44703},{\"end\":44928,\"start\":44917},{\"end\":44938,\"start\":44928},{\"end\":44950,\"start\":44938},{\"end\":44959,\"start\":44950},{\"end\":45232,\"start\":45212},{\"end\":45240,\"start\":45232},{\"end\":45261,\"start\":45240},{\"end\":45270,\"start\":45261},{\"end\":45283,\"start\":45270},{\"end\":45813,\"start\":45805},{\"end\":45829,\"start\":45813},{\"end\":46305,\"start\":46285},{\"end\":46312,\"start\":46305},{\"end\":46322,\"start\":46312},{\"end\":46335,\"start\":46322},{\"end\":46351,\"start\":46335},{\"end\":46377,\"start\":46351},{\"end\":46391,\"start\":46377},{\"end\":46899,\"start\":46879},{\"end\":46912,\"start\":46899},{\"end\":46925,\"start\":46912},{\"end\":46944,\"start\":46925},{\"end\":47181,\"start\":47167},{\"end\":47196,\"start\":47181},{\"end\":47502,\"start\":47493},{\"end\":47516,\"start\":47502},{\"end\":47526,\"start\":47516},{\"end\":47541,\"start\":47526},{\"end\":47551,\"start\":47541},{\"end\":47560,\"start\":47551},{\"end\":47571,\"start\":47560},{\"end\":47581,\"start\":47571},{\"end\":47588,\"start\":47581},{\"end\":47592,\"start\":47588},{\"end\":48054,\"start\":48037},{\"end\":48071,\"start\":48054},{\"end\":48081,\"start\":48071},{\"end\":48095,\"start\":48081},{\"end\":48114,\"start\":48095},{\"end\":48129,\"start\":48114},{\"end\":48146,\"start\":48129},{\"end\":48483,\"start\":48471},{\"end\":48494,\"start\":48483},{\"end\":48510,\"start\":48494},{\"end\":48520,\"start\":48510},{\"end\":48869,\"start\":48858},{\"end\":48882,\"start\":48869},{\"end\":48893,\"start\":48882},{\"end\":48906,\"start\":48893},{\"end\":49175,\"start\":49160},{\"end\":49192,\"start\":49175},{\"end\":49217,\"start\":49192},{\"end\":49233,\"start\":49217},{\"end\":49249,\"start\":49233},{\"end\":49264,\"start\":49249},{\"end\":49569,\"start\":49560},{\"end\":49585,\"start\":49569},{\"end\":49598,\"start\":49585},{\"end\":49608,\"start\":49598},{\"end\":49860,\"start\":49849},{\"end\":49875,\"start\":49860},{\"end\":49893,\"start\":49875},{\"end\":49907,\"start\":49893},{\"end\":49923,\"start\":49907},{\"end\":49938,\"start\":49923},{\"end\":49954,\"start\":49938},{\"end\":49963,\"start\":49954},{\"end\":50304,\"start\":50294},{\"end\":50317,\"start\":50304},{\"end\":50333,\"start\":50317},{\"end\":50344,\"start\":50333},{\"end\":50354,\"start\":50344},{\"end\":50618,\"start\":50602},{\"end\":50631,\"start\":50618},{\"end\":50642,\"start\":50631},{\"end\":50655,\"start\":50642},{\"end\":50665,\"start\":50655},{\"end\":50887,\"start\":50874},{\"end\":50899,\"start\":50887},{\"end\":50910,\"start\":50899},{\"end\":50925,\"start\":50910},{\"end\":50935,\"start\":50925},{\"end\":50950,\"start\":50935},{\"end\":50958,\"start\":50950},{\"end\":50973,\"start\":50958},{\"end\":50987,\"start\":50973},{\"end\":50999,\"start\":50987},{\"end\":51015,\"start\":50999},{\"end\":51500,\"start\":51488},{\"end\":51512,\"start\":51500},{\"end\":51522,\"start\":51512},{\"end\":51764,\"start\":51751},{\"end\":51778,\"start\":51764},{\"end\":51794,\"start\":51778},{\"end\":51811,\"start\":51794},{\"end\":51828,\"start\":51811},{\"end\":51844,\"start\":51828},{\"end\":52150,\"start\":52130},{\"end\":52164,\"start\":52150},{\"end\":52179,\"start\":52164},{\"end\":52836,\"start\":52824},{\"end\":52848,\"start\":52836},{\"end\":52857,\"start\":52848},{\"end\":52870,\"start\":52857},{\"end\":52880,\"start\":52870},{\"end\":52892,\"start\":52880},{\"end\":53328,\"start\":53314},{\"end\":53340,\"start\":53328},{\"end\":53352,\"start\":53340},{\"end\":53362,\"start\":53352},{\"end\":53375,\"start\":53362},{\"end\":53640,\"start\":53626},{\"end\":53657,\"start\":53640},{\"end\":54012,\"start\":53998},{\"end\":54024,\"start\":54012},{\"end\":54352,\"start\":54323},{\"end\":54369,\"start\":54352},{\"end\":54389,\"start\":54369},{\"end\":54407,\"start\":54389},{\"end\":54421,\"start\":54407},{\"end\":54433,\"start\":54421},{\"end\":54699,\"start\":54685},{\"end\":54714,\"start\":54699},{\"end\":54730,\"start\":54714},{\"end\":54747,\"start\":54730},{\"end\":54763,\"start\":54747},{\"end\":54994,\"start\":54973},{\"end\":55007,\"start\":54994},{\"end\":55024,\"start\":55007},{\"end\":55227,\"start\":55213},{\"end\":55243,\"start\":55227},{\"end\":55255,\"start\":55243},{\"end\":55280,\"start\":55255},{\"end\":55617,\"start\":55601},{\"end\":55631,\"start\":55617},{\"end\":55644,\"start\":55631},{\"end\":55661,\"start\":55644},{\"end\":55674,\"start\":55661},{\"end\":55689,\"start\":55674},{\"end\":55704,\"start\":55689},{\"end\":55722,\"start\":55704},{\"end\":56258,\"start\":56244},{\"end\":56273,\"start\":56258},{\"end\":56285,\"start\":56273},{\"end\":56300,\"start\":56285},{\"end\":56315,\"start\":56300},{\"end\":56328,\"start\":56315},{\"end\":56340,\"start\":56328},{\"end\":56804,\"start\":56792},{\"end\":56816,\"start\":56804},{\"end\":56829,\"start\":56816},{\"end\":56844,\"start\":56829},{\"end\":56857,\"start\":56844},{\"end\":57201,\"start\":57190},{\"end\":57216,\"start\":57201},{\"end\":57226,\"start\":57216},{\"end\":57245,\"start\":57226},{\"end\":57254,\"start\":57245},{\"end\":57270,\"start\":57254},{\"end\":57275,\"start\":57270},{\"end\":57600,\"start\":57586},{\"end\":57614,\"start\":57600},{\"end\":57643,\"start\":57614},{\"end\":57663,\"start\":57643},{\"end\":57675,\"start\":57663},{\"end\":57977,\"start\":57966},{\"end\":57992,\"start\":57977},{\"end\":58000,\"start\":57992},{\"end\":58218,\"start\":58188},{\"end\":58231,\"start\":58218},{\"end\":58238,\"start\":58231},{\"end\":58472,\"start\":58453},{\"end\":58488,\"start\":58472},{\"end\":58509,\"start\":58488},{\"end\":58526,\"start\":58509},{\"end\":58542,\"start\":58526},{\"end\":58964,\"start\":58949},{\"end\":58977,\"start\":58964},{\"end\":58993,\"start\":58977},{\"end\":59164,\"start\":59151},{\"end\":59539,\"start\":59523},{\"end\":59554,\"start\":59539},{\"end\":59571,\"start\":59554},{\"end\":59782,\"start\":59769},{\"end\":59806,\"start\":59782},{\"end\":59821,\"start\":59806},{\"end\":60106,\"start\":60098},{\"end\":60126,\"start\":60106},{\"end\":60139,\"start\":60126},{\"end\":60155,\"start\":60139},{\"end\":60462,\"start\":60450},{\"end\":60478,\"start\":60462},{\"end\":60491,\"start\":60478},{\"end\":60505,\"start\":60491},{\"end\":60520,\"start\":60505},{\"end\":60539,\"start\":60520},{\"end\":60557,\"start\":60539},{\"end\":60564,\"start\":60557},{\"end\":60894,\"start\":60875},{\"end\":60907,\"start\":60894},{\"end\":60934,\"start\":60907},{\"end\":60941,\"start\":60934},{\"end\":61461,\"start\":61450},{\"end\":61473,\"start\":61461},{\"end\":61482,\"start\":61473},{\"end\":61496,\"start\":61482},{\"end\":61758,\"start\":61746},{\"end\":61771,\"start\":61758},{\"end\":61895,\"start\":61883},{\"end\":61913,\"start\":61895},{\"end\":61917,\"start\":61913}]", "bib_venue": "[{\"end\":37282,\"start\":37250},{\"end\":37460,\"start\":37454},{\"end\":37671,\"start\":37646},{\"end\":37876,\"start\":37853},{\"end\":38156,\"start\":38109},{\"end\":38499,\"start\":38493},{\"end\":38836,\"start\":38763},{\"end\":39196,\"start\":39171},{\"end\":39510,\"start\":39485},{\"end\":39945,\"start\":39861},{\"end\":40395,\"start\":40372},{\"end\":40748,\"start\":40727},{\"end\":41030,\"start\":40991},{\"end\":41188,\"start\":41135},{\"end\":41501,\"start\":41495},{\"end\":41936,\"start\":41929},{\"end\":42204,\"start\":42154},{\"end\":42882,\"start\":42860},{\"end\":43297,\"start\":43257},{\"end\":43718,\"start\":43697},{\"end\":44076,\"start\":44032},{\"end\":44393,\"start\":44387},{\"end\":44734,\"start\":44717},{\"end\":44976,\"start\":44959},{\"end\":45381,\"start\":45283},{\"end\":45927,\"start\":45829},{\"end\":46489,\"start\":46391},{\"end\":46974,\"start\":46944},{\"end\":47279,\"start\":47212},{\"end\":47641,\"start\":47592},{\"end\":48167,\"start\":48146},{\"end\":48576,\"start\":48520},{\"end\":48929,\"start\":48906},{\"end\":49285,\"start\":49264},{\"end\":49631,\"start\":49608},{\"end\":49984,\"start\":49963},{\"end\":50292,\"start\":50200},{\"end\":50688,\"start\":50665},{\"end\":51143,\"start\":51031},{\"end\":51556,\"start\":51522},{\"end\":51872,\"start\":51844},{\"end\":52301,\"start\":52179},{\"end\":52953,\"start\":52892},{\"end\":53395,\"start\":53375},{\"end\":53743,\"start\":53657},{\"end\":54104,\"start\":54040},{\"end\":54321,\"start\":54255},{\"end\":54806,\"start\":54779},{\"end\":55070,\"start\":55040},{\"end\":55370,\"start\":55296},{\"end\":55811,\"start\":55722},{\"end\":56401,\"start\":56340},{\"end\":56918,\"start\":56857},{\"end\":57325,\"start\":57291},{\"end\":57584,\"start\":57505},{\"end\":58027,\"start\":58000},{\"end\":58274,\"start\":58238},{\"end\":58640,\"start\":58542},{\"end\":59006,\"start\":58993},{\"end\":59296,\"start\":59164},{\"end\":59521,\"start\":59508},{\"end\":59835,\"start\":59821},{\"end\":60178,\"start\":60155},{\"end\":60585,\"start\":60564},{\"end\":61039,\"start\":60941},{\"end\":61527,\"start\":61496},{\"end\":61744,\"start\":61713},{\"end\":62014,\"start\":61917},{\"end\":52433,\"start\":52303},{\"end\":53001,\"start\":52955},{\"end\":53816,\"start\":53745},{\"end\":55887,\"start\":55813},{\"end\":56449,\"start\":56403},{\"end\":56966,\"start\":56920},{\"end\":61124,\"start\":61041}]"}}}, "year": 2023, "month": 12, "day": 17}