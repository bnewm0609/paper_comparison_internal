{"id": 204972004, "updated": "2023-10-04 23:22:40.498", "metadata": {"title": "Grandmaster level in StarCraft II using multi-agent reinforcement learning", "authors": "[{\"first\":\"Oriol\",\"last\":\"Vinyals\",\"middle\":[]},{\"first\":\"Igor\",\"last\":\"Babuschkin\",\"middle\":[]},{\"first\":\"Wojciech\",\"last\":\"Czarnecki\",\"middle\":[\"M.\"]},{\"first\":\"Micha\u00ebl\",\"last\":\"Mathieu\",\"middle\":[]},{\"first\":\"Andrew\",\"last\":\"Dudzik\",\"middle\":[]},{\"first\":\"Junyoung\",\"last\":\"Chung\",\"middle\":[]},{\"first\":\"David\",\"last\":\"Choi\",\"middle\":[\"H.\"]},{\"first\":\"Richard\",\"last\":\"Powell\",\"middle\":[]},{\"first\":\"Timo\",\"last\":\"Ewalds\",\"middle\":[]},{\"first\":\"Petko\",\"last\":\"Georgiev\",\"middle\":[]},{\"first\":\"Junhyuk\",\"last\":\"Oh\",\"middle\":[]},{\"first\":\"Dan\",\"last\":\"Horgan\",\"middle\":[]},{\"first\":\"Manuel\",\"last\":\"Kroiss\",\"middle\":[]},{\"first\":\"Ivo\",\"last\":\"Danihelka\",\"middle\":[]},{\"first\":\"Aja\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Laurent\",\"last\":\"Sifre\",\"middle\":[]},{\"first\":\"Trevor\",\"last\":\"Cai\",\"middle\":[]},{\"first\":\"John\",\"last\":\"Agapiou\",\"middle\":[\"P.\"]},{\"first\":\"Max\",\"last\":\"Jaderberg\",\"middle\":[]},{\"first\":\"Alexander\",\"last\":\"Vezhnevets\",\"middle\":[\"S.\"]},{\"first\":\"R\u00e9mi\",\"last\":\"Leblond\",\"middle\":[]},{\"first\":\"Tobias\",\"last\":\"Pohlen\",\"middle\":[]},{\"first\":\"Valentin\",\"last\":\"Dalibard\",\"middle\":[]},{\"first\":\"David\",\"last\":\"Budden\",\"middle\":[]},{\"first\":\"Yury\",\"last\":\"Sulsky\",\"middle\":[]},{\"first\":\"James\",\"last\":\"Molloy\",\"middle\":[]},{\"first\":\"Tom\",\"last\":\"Paine\",\"middle\":[\"L.\"]},{\"first\":\"Caglar\",\"last\":\"Gulcehre\",\"middle\":[]},{\"first\":\"Ziyu\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Tobias\",\"last\":\"Pfaff\",\"middle\":[]},{\"first\":\"Yuhuai\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Roman\",\"last\":\"Ring\",\"middle\":[]},{\"first\":\"Dani\",\"last\":\"Yogatama\",\"middle\":[]},{\"first\":\"Dario\",\"last\":\"W\u00fcnsch\",\"middle\":[]},{\"first\":\"Katrina\",\"last\":\"McKinney\",\"middle\":[]},{\"first\":\"Oliver\",\"last\":\"Smith\",\"middle\":[]},{\"first\":\"Tom\",\"last\":\"Schaul\",\"middle\":[]},{\"first\":\"Timothy\",\"last\":\"Lillicrap\",\"middle\":[]},{\"first\":\"Koray\",\"last\":\"Kavukcuoglu\",\"middle\":[]},{\"first\":\"Demis\",\"last\":\"Hassabis\",\"middle\":[]},{\"first\":\"Chris\",\"last\":\"Apps\",\"middle\":[]},{\"first\":\"David\",\"last\":\"Silver\",\"middle\":[]}]", "venue": "Nature", "journal": "Nature", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1\u20133, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8% of officially ranked human players. AlphaStar uses a multi-agent reinforcement learning algorithm and has reached Grandmaster level, ranking among the top 0.2% of human players for the real-time strategy game StarCraft II.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": "2982316857", "acl": null, "pubmed": "31666705", "pubmedcentral": null, "dblp": "journals/nature/VinyalsBCMDCCPE19", "doi": "10.1038/s41586-019-1724-z"}}, "content": {"source": {"pdf_hash": "6111d123f09912c2934a077707746ed06723ca00", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "a7bff1fc09fe28a34291101797560448a7576c1b", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6111d123f09912c2934a077707746ed06723ca00.txt", "contents": "\nGrandmaster level in StarCraft II using multi-agent reinforcement learning\n14 November 2019\n\nOriol Vinyals vinyals@google.com \nIgor Babuschkin \nWojciech M Czarnecki \nMicha\u00ebl Mathieu \nAndrew Dudzik \nJunyoung Chung \nDavid H Choi \nRichard Powell \nTimo Ewalds \nPetko Georgiev \nJunhyuk Oh \nDan Horgan \nManuel Kroiss \nIvo Danihelka \nAja Huang \nLaurent Sifre \nTrevor Cai \nJohn P Agapiou \nMax Jaderberg \nAlexander S Vezhnevets \nR\u00e9mi Leblond \nTobias Pohlen \nValentin Dalibard \nDavid Budden \nYury Sulsky \nJames Molloy \nTom L Paine \nCaglar Gulcehre \nZiyu Wang \nTobias Pfaff \nYuhuai Wu \nRoman Ring \nDani Yogatama \nDario W\u00fcnsch \nKatrina Mckinney \nOliver Smith \nTom Schaul \nTimothy Lillicrap \nKoray Kavukcuoglu \nDemis Hassabis \nChris Apps \nDavid Silver davidsilver@google.com \nOriol Vinyals \nIgor Babuschkin \nWojciech M Czarnecki \nMicha\u00ebl Mathieu \nAndrew Dudzik \nJunyoung Chung \nDavid H Choi \nRichard Powell \nTimo Ewalds \nPetko Georgiev \nJunhyuk Oh \nDan Horgan \nManuel Kroiss \nIvo Danihelka \nAja Huang \nLaurent Sifre \nTrevor Cai \nJohn P Agapiou \nChris Apps \nDavid Silver \nGrandmaster level in StarCraft II using multi-agent reinforcement learning\n\n350 | Nature |\n57514 November 201910.1038/s41586-019-1724-zReceived: 30 August 2019 Accepted: 10 October 2019Article 1 DeepMind, London, UK. 2 Team Liquid, Utrecht, Netherlands. 3 These authors contributed equally:\nMany real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions 1-3 , the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems 4 . Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using generalpurpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks 5,6 . We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8% of officially ranked human players.StarCraft is a real-time strategy game in which players balance highlevel economic decisions with individual control of hundreds of units. This domain raises important game-theoretic challenges: it features a vast space of cyclic, non-transitive strategies and counter-strategies; discovering novel strategies is intractable with naive self-play exploration methods; and those strategies may not be effective when deployed in real-world play with humans. Furthermore, StarCraft has a combinatorial action space, a planning horizon that extends over thousands of real-time decisions, and imperfect information 7 .Each game consists of tens of thousands of time-steps and thousands of actions, selected in real-time throughout approximately ten minutes of gameplay. At each step t, our agent AlphaStar receives an observation o t that includes a list of all observable units and their attributes. This information is imperfect; the game includes only opponent units seen by the player's own units, and excludes some opponent unit attributes outside the camera view.Each action a t is highly structured: it selects what action type, out of several hundred (for example, move or build worker); who to issue that action to, for any subset of the agent's units; where to target, among locations on the map or units within the camera view; and when to observe and act next(Fig. 1a). This representation of actions results in approximately 10 26 possible choices at each step. Similar to human players, a special action is available to move the camera view, so as to gather more information.Humans play StarCraft under physical constraints that limit their reaction time and the rate of their actions. The game was designed with those limitations in mind, and removing those constraints changes the nature of the game. We therefore chose to impose constraints upon AlphaStar: it suffers from delays due to network latency and computation time; and its actions per minute (APM) are limited, with peak statistics substantially lower than those of humans (Figs. 2c, 3g for performance analysis). AlphaStar's play with this interface and these https://doi.\n\nMany real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions 1-3 , the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems 4 . Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using generalpurpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks 5,6 . We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8% of officially ranked human players.\n\nStarCraft is a real-time strategy game in which players balance highlevel economic decisions with individual control of hundreds of units. This domain raises important game-theoretic challenges: it features a vast space of cyclic, non-transitive strategies and counter-strategies; discovering novel strategies is intractable with naive self-play exploration methods; and those strategies may not be effective when deployed in real-world play with humans. Furthermore, StarCraft has a combinatorial action space, a planning horizon that extends over thousands of real-time decisions, and imperfect information 7 .\n\nEach game consists of tens of thousands of time-steps and thousands of actions, selected in real-time throughout approximately ten minutes of gameplay. At each step t, our agent AlphaStar receives an observation o t that includes a list of all observable units and their attributes. This information is imperfect; the game includes only opponent units seen by the player's own units, and excludes some opponent unit attributes outside the camera view.\n\nEach action a t is highly structured: it selects what action type, out of several hundred (for example, move or build worker); who to issue that action to, for any subset of the agent's units; where to target, among locations on the map or units within the camera view; and when to observe and act next (Fig. 1a). This representation of actions results in approximately 10 26 possible choices at each step. Similar to human players, a special action is available to move the camera view, so as to gather more information.\n\nHumans play StarCraft under physical constraints that limit their reaction time and the rate of their actions. The game was designed with those limitations in mind, and removing those constraints changes the nature of the game. We therefore chose to impose constraints upon AlphaStar: it suffers from delays due to network latency and computation time; and its actions per minute (APM) are limited, with peak statistics substantially lower than those of humans (Figs. 2c, 3g for performance analysis). AlphaStar's play with this interface and these , represented by a neural network with parameters \u03b8 that receives all observations s t = (o 1:t , a 1:t \u2212 1 ) from the start of the game as inputs, and selects actions as outputs. The policy is also conditioned on a statistic z that summarizes a strategy sampled from human data (for example, a build order).\n\nOur agent architecture consists of general-purpose neural network components that handle StarCraft's raw complexity. Observations of player and opponent units are processed using a self-attention mechanism 8 . To integrate spatial and non-spatial information, we introduce scatter connections. To deal with partial observability, the temporal sequence of observations is processed by a deep long short-term memory (LSTM) system 9 . To manage the structured, combinatorial action space, the agent uses an auto-regressive policy 7,10,11 and recurrent pointer network 12 . Extended Data Fig. 3 summarizes the architecture and Fig. 3f shows an ablation of each component.\n\nAgent parameters were initially trained by supervised learning. Games were sampled from a publicly available dataset of anonymized human replays. The policy was then trained to predict each action a t , conditioned either solely on s t , or also on z. This results in a diverse set of strategies that reflects the modes of human play.\n\nThe agent parameters were subsequently trained by a reinforcement learning algorithm that is designed to maximize the win rate (that is, compute a best response) against a mixture of opponents. The choice of opponent is determined by a multi-agent procedure, described   Fig. 1 | Training setup. a, AlphaStar observes the game through an overview map and list of units. To act, the agent outputs what action type to issue (for example, build), who it is applied to, where it targets, and when the next action will be issued. Actions are sent to the game through a monitoring layer that limits action rate. AlphaStar contends with delays from network latency and processing time. b, AlphaStar is trained via both supervised learning and reinforcement learning. In supervised learning (bottom), the parameters are updated to optimize Kullback-Leibler (KL) divergence between its output and human actions sampled from a collection of replays. In reinforcement learning (top), human data are used to sample the statistic z, and agent experience is collected to update the policy and value outputs via reinforcement learning (TD(\u03bb), V-trace, UPGO) combined with a KL loss towards the supervised agent. c, Three pools of agents, each initialized by supervised learning, were subsequently trained with reinforcement learning. As they train, these agents intermittently add copies of themselves-'players' that are frozen at a specific point-to the league. The main agents train against all of these past players, as well as themselves. The league exploiters train against all past players. The main exploiters train against the main agents. Main exploiters and league exploiters can be reset to the supervised agent when they add a player to the league. Images from StarCraft reproduced with permission from Blizzard Entertainment.\n\n\nArticle\n\nbelow. AlphaStar's reinforcement learning algorithm is based on a policy gradient algorithm similar to advantage actor-critic 13 . Updates were applied asynchronously 14 on replayed experiences 15 . This requires an approach known as off-policy learning 5 , that is, updating the current policy from experience generated by a previous policy. Our solution is motivated by the observation that, in large action spaces, the current and previous policies are highly unlikely to match over many steps. We therefore use a combination of techniques that can learn effectively despite the mismatch: temporal difference learning (TD(\u03bb)) 16 , clipped importance sampling (V-trace) 14 , and a new self-imitation 17 algorithm (UPGO) that moves the policy towards trajectories with betterthan-average reward. To reduce variance, during training only, the value function is estimated using information from both the player's and the opponent's perspectives. Figure 3i, k analyses the relative importance of these components.\n\nOne of the main challenges in StarCraft is to discover novel strategies. Consider a policy that has learned to build and utilize the micro-tactics of ground units. Any deviation that builds and naively uses air units will reduce performance. It is highly improbable that naive exploration will execute a precise sequence of instructions, over thousands of steps, that constructs air units and effectively utilizes their micro-tactics. To address this issue, and to encourage robust behaviour against likely human play, we utilize human data. Each agent is initialized to the parameters of the supervised learning agent. Subsequently, during reinforcement learning, we either condition the agent on a statistic z, in which case agents receive a reward for following the strategy corresponding to z, or train the agent unconditionally, in which case the agent is free to choose its own strategy. Agents also receive a penalty whenever their action probabilities differ from the supervised policy. This human exploration ensures that a wide variety of relevant modes of play continue to be explored throughout training. Figure 3e shows the importance of human data in AlphaStar.\n\nTo address the game-theoretic challenges, we introduce league training, an algorithm for multi-agent reinforcement learning (Fig. 1b, c). Self-play algorithms, similar to those used in chess and Go 18 , learn rapidly but may chase cycles (for example, where A defeats B, and B defeats C, but A loses to C) indefinitely without making progress 19 . Fictitious self-play (FSP) [20][21][22] avoids cycles by computing a best response against a uniform mixture of all previous policies; the mixture converges to a Nash equilibrium in two-player zero-sum games 20 . We extend this approach to compute a best response against a non-uniform mixture of opponents. This league of potential opponents includes a diverse range of agents (Fig. 4d), as well as their policies from both current and previous iterations. At each iteration, each agent plays games against opponents sampled from a mixture policy specific to that agent. The parameters of the agent are updated from the outcomes of those games by the actor-critic reinforcement learning procedure described above.\n\nThe league consists of three distinct types of agent, differing primarily in their mechanism for selecting the opponent mixture. First, the main agents utilize a prioritized fictitious self-play (PFSP) mechanism that adapts the mixture probabilities proportionally to the win rate of each opponent against the agent; this provides our agent with more opportunities to overcome the most problematic opponents. With fixed probability, a main agent is selected as an opponent; this recovers the rapid learning of self-play (Fig. 3c). Second, main exploiter agents play only against the current iteration of main agents. Their purpose is to identify potential exploits in the main agents; the main agents are thereby encouraged to address their weaknesses. Third, league exploiter agents use a similar PFSP mechanism to the main agents, but are not targeted by main exploiter agents. Their purpose is to find systemic weaknesses of the entire league. Both main exploiters and league exploiters are periodically reinitialized to encourage more diversity and may rapidly discover specialist strategies that are not necessarily robust against exploitation. Figure 3b analyses the choice of agents within the league.  In StarCraft, each player chooses one of three races-Terran, Protoss or Zerg-each with distinct mechanics. We trained the league using three main agents (one for each StarCraft race), three main exploiter agents (one for each race), and six league exploiter agents (two for each race). Each agent was trained using 32 third-generation tensor processing units (TPUs 23 ) over 44 days. During league training almost 900 distinct players were created.\n\n\nEmpirical evaluation\n\nWe evaluated the three main Terran, Protoss and Zerg AlphaStar agents using the unconditional policy on the official online matchmaking system Battle.net. Each agent was assessed at three different snapshots during training: after supervised training only (AlphaStar Supervised), after 27 days of league training (AlphaStar Mid), and after 44 days of league training (AlphaStar Final). AlphaStar Supervised and AlphaStar Mid were evaluated starting from an unranked rating on Battle.net for 30 and 60 games, respectively, for each race; AlphaStar Final was evaluated from AlphaStar Mid's rating for an additional 30 games for each race. The Battle.net matchmaking procedure selected maps and opponents. Matches were played under blind conditions: AlphaStar was not provided with the opponent's identity, and played under an anonymous account. These conditions were selected to estimate AlphaStar's strength under approximately stationary conditions, but do not directly measure its susceptibility to exploitation under repeated play.\n\nAlphaStar Final achieved ratings of 6,275 Match Making Rating (MMR) for Protoss, 6,048 MMR for Terran and 5,835 MMR for Zerg, placing it above 99.8% of ranked human players, and at Grandmaster level for all three races (Fig. 2a, Extended Data Fig. 7 (analysis), Supplementary Data, Replays (game replays)). AlphaStar Supervised reached an average rating of 3,699, which places it above 84% of human players and shows the effectiveness of supervised learning.\n\nTo further analyse AlphaStar we also ran several internal ablations ( Fig. 3) and evaluations (Fig. 4). For multi-agent dynamics, we ran a round-robin tournament of all players throughout league training and a second tournament of main agents against held-out validation agents trained to follow specific human strategies. The main agent performance improved steadily across all three races. The performance of the main exploiters actually reduced over time and main agents performed better against the held-out validation agents, both of which suggest that the main agent grew increasingly robust. The league Nash equilibrium over all players at each point in time assigns small probabilities to players from previous iterations, suggesting that the learning algorithm does not cycle or regress. Finally, the unit composition changed throughout league training, which indicates a diverse strategic progression.\n\n\nConclusion\n\nAlphaStar is the first agent to achieve Grandmaster level in StarCraft II, and the first to reach the highest league of human players in a widespread professional esport without simplification of the game. Like StarCraft, real-world domains such as personal assistants, self-driving cars, or robotics require real-time decisions, over combinatorial or structured action spaces, given imperfectly observed information. Furthermore, similar to StarCraft, many applications have complex strategy spaces that contain cycles or hard exploration landscapes, and agents may encounter unexpected strategies or complex edge cases when deployed in the real world. The success of AlphaStar in StarCraft II suggests that general-purpose machine learning algorithms may have a substantial effect on complex real-world problems.  These experiments use a simplified setup: one map (Kairos Junction), one race match-up (Protoss versus Protoss), reinforcement learning and league experiments limited to 10 10 steps, only main agents, and a 50%-50% mix of self-play and PFSP, unless stated otherwise (see Methods). The first column shows Elo ratings 24 against ablation test agents (each rating was estimated with 11,000 full games of StarCraft II). a, b, Comparing different league compositions using Elo of the main agents (a) and relative population performance of the whole leagues (b), which measures exploitability. c, d, Comparing different multi-agent learning algorithms using Elo (c) and a proxy for forgetting: the minimum win rate against all past versions, averaged over time (d). Naive self-play has a high Elo, but is more forgetful. See Extended Data Fig. 5 for more in-depth comparison. e, Ablation study of the different mechanisms to use human data. Human init, supervised learning initialization of parameters of the neural network. g, APM limits relative to those used in AlphaStar. Reducing APM substantially reduces performance. Unexpectedly, increasing APM also reduces performance, possibly because the agent spends more effort on refining micro-tactics than on learning diverse strategies. f, h, Comparison of architectures using the win rate of supervised agents (trained in Protoss versus all) against the builtin elite bot. j, Elo scores of StarCraft II built-in bots. Ratings are anchored by a bot that never acts. i, k, Reinforcement learning ablations, measured by training a best response against fixed opponents to avoid multi-agent dynamics.\n\n\nArticle\n\n\nOnline content\n\nAny methods, additional references, Nature Research reporting summaries, source data, extended data, supplementary information, acknowledgements, peer review information; details of author contributions and competing interests; and statements of data and code availability are available at https://doi.org/10.1038/s41586-019-1724-z. Elo scores of agents in the league during the 44 days of training. Each point represents a past player, evaluated against the entire league and the elite built-in bot (whose Elo is set to 0). b, Proportion of validation agents that beat the main agents in more than 80 out of 160 games. This value increased steadily over time, which shows the robustness of league training to unseen strategies. c, The Nash distribution (mixture of the least exploitable players) of the players in the league, as training progressed. It puts the most weight on recent players, suggesting that the latest strategies largely dominate earlier ones, without much forgetting or cycling. For example, player 40 was part of the Nash distribution from its creation at day 20 until 5 days later, when it was completely dominated by newer agents. d, Average number of each unit built by the Protoss agents over the course of league training, normalized by the most common unit. Unlike the main agents, the exploiters rapidly explore different unit compositions. Worker units have been removed for clarity. There is no universally accepted notion of fairness in real-time human-computer matches, so our match conditions, interface, camera view, action rate limits, and delays were developed in consultation with professional StarCraft II players and Blizzard employees. AlphaStar's play under these conditions was professional-player approved (see the Professional Player Statement, below). At each agent step, the policy receives an observation o t and issues an action a t (Extended Data Tables 1, 2) through the game interface. There can be several game time-steps (each 45 ms) per agent step.\n\n\nMethods\n\n\nGame and interface\n\nCamera view. Humans play StarCraft through a screen that displays only part of the map along with a high-level view of the entire map (to avoid information overload, for example). The agent interacts with the game through a similar camera-like interface, which naturally imposes an economy of attention, so that the agent chooses which area it fully sees and interacts with. The agent can move the camera as an action.\n\nOpponent units outside the camera have certain information hidden, and the agent can only target within the camera for certain actions (for example, building structures). AlphaStar can target locations more accurately than humans outside the camera, although less accurately within it because target locations (selected on a 256 \u00d7 256 grid) are treated the same inside and outside the camera. Agents can also select sets of units anywhere, which humans can do less flexibly using control groups. In practice, the agent does not seem to exploit these extra capabilities (see the Professional Player Statement, below), because of the human prior. Ablation data in Fig. 3h shows that using this camera view reduces performance.\n\n\nAPM limits.\n\nHumans are physically limited in the number of actions per minute (APM) they can execute. Our agent has a monitoring layer that enforces APM limitations. This introduces an action economy that requires actions to be prioritized. Agents are limited to executing at most 22 non-duplicate actions per 5-s window. Converting between actions and the APM measured by the game is non-trivial, and agent actions are hard to compare with human actions (computers can precisely execute different actions from step to step). See Fig. 2c and Extended Data Fig. 1 for APM details.\n\nDelays. Humans are limited in how quickly they react to new information; AlphaStar has two sources of delays. First, in real-time evaluation (not training), AlphaStar has a delay of about 110 ms between when a frame is observed and when an action is executed, owing to latency, observation processing, and inference. Second, because agents decide ahead of time when to observe next (on average 370 ms, but possibly multiple seconds), they may react late to unexpected situations. The distribution of these delays is shown in Extended Data Fig. 2.\n\n\nRelated work\n\nGames have been a focus of artificial intelligence research for decades as a stepping stone towards more general applications. Classic board games such as chess 25 and Go 26 have been mastered using generalpurpose reinforcement learning and planning algorithms 18 . Reinforcement learning methods have achieved substantial successes in video games such as those on the Atari platform 27 , Super Mario Bros 28 , Quake III Arena Capture the Flag 29 , and Dota 2 30 .\n\nReal-time strategy (RTS) games are recognized for their gametheoretic and domain complexities 31 . Many sub-problems of RTS games, for example, micromanagement, base economy, or build order optimization, have been studied in depth 7,[32][33][34][35] , often in small-scale environments 36,37 . For the combined challenge, the StarCraft domain has emerged by consensus as a research focus 1,7 . StarCraft: Brood War has an active competitive AI research community 38 , and most bots combine rule-based heuristics with other AI techniques such as search 4,39 , data-driven build-order selection 40 , and simulation 41 . Reinforcement learning has also been studied to control units in the game 7,34,42-44 , and imitation learning has been proposed to learn unit and building compositions 45 . Most recently, deep learning has been used to predict future game states 46 . StarCraft II similarly has an active bot community 3 since the release of a public application programming interface (API) 7 . No StarCraft bots have defeated professional players, or even high-level casual players 47 , and the most successful bots have used superhuman capabilities, such as executing tens of thousands of APM or viewing the entire map at once. These capabilities make comparisons against humans hard, and invalidate certain strategies. Some of the most recent approaches use reinforcement learning to play the full game, with hand-crafted, high-level actions 48 , or rule-based systems with machine learning incrementally replacing components 43 . By contrast, AlphaStar uses a model-free, end-to-end learning approach to playing StarCraft II that sidesteps the difficulties of search-based methods that result from imperfect models, and is applicable to any domain that shares some of the challenges present in StarCraft.\n\nDota 2 is a modern competitive team game that shares some complexities of RTS games such as StarCraft (including imperfect information and large time horizons). Recently, OpenAI Five defeated a team of professional Dota 2 players and 99.4% of online players 30 . The hero units of OpenAI Five are controlled by a team of agents, trained together with a scaled up version of PPO 49 , based on handcrafted rewards. However, unlike AlphaStar, some game rules were simplified, players were restricted to a subset of heroes, agents used hard-coded sub-systems for certain aspects of the game, and agents did not limit their perception to a camera view.\n\nAlphaStar relies on imitation learning combined with reinforcement learning, which has been used several times in the past. Similarly to the training pipeline of AlphaStar, the original AlphaGo initialized a policy network by supervised learning from human games, which was then used as a prior in Monte-Carlo tree search 26 . Similar to our statistic z, other work attempted to train reward functions from human preferences and use them to guide reinforcement learning 50,51 or learned goals from human intervention 52 .\n\nRelated to the league, recent progress in multi-agent research has led to agents performing at human level in the Capture the Flag team mode of Quake III Arena 29 . These results were obtained using populationbased training of several agents competing with each other, which used pseudo-reward evolution to deal with the hard credit assignment problem. Similarly, the Policy Space Response Oracle framework 53 is related to league training, although league training specifies unique targets for approximate best responses (that is, PFSP and exploiters).\n\n\nArchitecture\n\nThe policy of AlphaStar is a function \u03c0 \u03b8 (a t | s t ,z) that maps all previous observations and actions s t = o 1:t , a 1:t \u2212 1 (defined in Extended Data Tables 1, 2) and z (representing strategy statistics) to a probability distribution over actions a t for the current step. \u03c0 \u03b8 is implemented as a deep neural network with the following structure.\n\nThe observations o t are encoded into vector representations, combined, and processed by a deep LSTM 9 , which maintains memory between steps. The action arguments a t are sampled autoregressively 10 , conditioned on the outputs of the LSTM and the observation encoders. There is a value function for each of the possible rewards (see Reinforcement learning).\n\nArchitecture components were chosen and tuned with respect to their performance in supervised learning, and include many recent advances in deep learning architectures 7,8,12,54,55 . A high-level overview of the agent architecture is given in Extended Data Fig. 3, with more detailed descriptions in Supplementary Data, Detailed Architecture. AlphaStar has 139 million weights, but only 55 million weights are required during inference. Ablation Fig. 3f compares the impact of scatter connections, transformer, and pointer network.\n\n\nSupervised learning\n\nEach agent is initially trained through supervised learning on replays to imitate human actions. Supervised learning is used both to initialize the agent and to maintain diverse exploration 56 . Because of this, the primary goal is to produce a diverse policy that captures StarCraft's complexities.\n\nWe use a dataset of 971,000 replays played on StarCraft II versions 4.8.2 to 4.8.6 by players with MMR scores (Blizzard's metric, similar to Elo) greater than 3,500, that is, from the top 22% of players. Instructions for downloading replays can be found at https://github.com/ Blizzard/s2client-proto. The observations and actions are returned by the game's raw interface (Extended Data Tables 1, 2). We train one policy for each race, with the same architecture as the one used during reinforcement learning.\n\nFrom each replay, we extract a statistic z that encodes each player's build order, defined as the first 20 constructed buildings and units, and cumulative statistics, defined as the units, buildings, effects, and upgrades that were present during a game. We condition the policy on z in both supervised and reinforcement learning, and in supervised learning we set z to zero 10% of the time.\n\nTo train the policy, at each step we input the current observations and output a probability distribution over each action argument (Extended Data Table 2). For these arguments, we compute the KL divergence between human actions and the policy's outputs, and apply updates using the Adam optimizer 57 . We also apply L 2 regularization 58 . The pseudocode of the supervised training algorithm can be found in Supplementary Data, Pseudocode.\n\nWe further fine-tune the policy using only winning replays with MMR above 6,200 (16,000 games). Fine-tuning improved the win rate against the built-in elite bot from 87% to 96% in Protoss versus Protoss games. The fine-tuned supervised agents were rated at 3,947 MMR for Terran, 3,607 MMR for Protoss and 3,544 MMR for Zerg. They are capable of building all units in the game, and are qualitatively diverse from game to game (Extended Data Fig. 4).\n\n\nReinforcement learning\n\nWe apply reinforcement learning to improve the performance of AlphaStar based on agent-versus-agent games. We use the match outcome (\u22121 on a loss, 0 on a draw and +1 on a win) as the terminal reward r T , without a discount to accurately reflect the true goal of winning games. Following the actor-critic paradigm 14 , a value function V \u03b8 (s t , z) is trained to predict r t , and used to update the policy \u03c0 \u03b8 (a t | s t , z).\n\nStarCraft poses several challenges when viewed as a reinforcement learning problem: exploration is difficult, owing to domain complexity and reward sparsity; policies need to be capable of executing diverse strategies throughout training; and off-policy learning is difficult, owing to large time horizons and the complex action space.\n\nExploration and diversity. We use human data to aid in exploration and to preserve strategic diversity throughout training. First, we initialize the policy parameters to the supervised policy and continually minimize the KL divergence between the supervised and current policy 59,60 . Second, we train the main agents with pseudo-rewards to follow a strategy statistic z, which we randomly sample from human data. These pseudo-rewards measure the edit distance between sampled and executed build orders, and the Hamming distance between sampled and executed cumulative statistics (see Supplementary Data, Detailed Architecture). Each type of pseudo-reward is active (that is, non-zero) with probability 25%, and separate value functions and losses are computed for each pseudo-reward. We found our use of human data to be critical in achieving good performance with reinforcement learning (Fig. 3e).\n\nValue and policy updates. New trajectories are generated by actors. Asynchronously, model parameters are updated by learners, using a replay buffer that stores trajectories. Because of this, AlphaStar is subject to off-policy data, which potentially requires off-policy corrections. We found that existing off-policy correction methods 14,61 can be inefficient in large, structured action spaces such as that used for StarCraft, because distinct actions can result in similar (or even identical) behaviour. We addressed this by using a hybrid approach that combines off-policy corrections for the policy (which avoids instability), with an uncorrected update of the value function (which introduces bias but reduces variance). Specifically, the policy is updated using V-trace and the value estimates are updated using TD(\u03bb) 5 (ablation in Fig. 3i). When applying V-trace to the policy in large action spaces, the off-policy corrections truncate the trace early; to mitigate this problem, we assume independence between the action type, delay, and all other arguments, and so update the components of the policy separately. To decrease the variance of the value estimates, we also use the opponent's observations as input to the value functions (ablation in Fig. 3k). Note that these are used only during training, as value functions are unnecessary during evaluation.\n\nIn addition to the V-trace policy update, we introduce an upgoing policy update (UPGO), which updates the policy parameters in the direction of \u03c1 G V s z \u03c0 a s z ( \u2212 ( , ))\u2207 log ( | , )\nt t \u03b8 t \u03b8 \u03b8 t t U where \uf8f4 \uf8f4 \uf8f1 \uf8f2 \uf8f3 G r G Q s a z V s z r V s z = + i f ( , , ) \u2265 ( , ) + ( , ) otherwise t t t t t \u03b8 t t \u03b8 t U + 1 U +1 +1 +1 +1\nis an upgoing return, Q(s t ,a t ,z) is an action-value estimate,\n( ) \u03c1 = min , 1 t \u03c0 a s z \u03c0 a s z ( | , ) ( | , ) \u03b8 t t \u03b8 t t \u2032\nis a clipped importance ratio, and \u03c0 \u03b8\u2032 is the policy that generated the trajectory in the actor. Similar to selfimitation learning 17 , the idea is to update the policy from partial trajectories with better-than-expected returns by bootstrapping when the behaviour policy takes a worse-than-average action (ablation in Fig. 3i). Owing to the difficulty of approximating Q(s t , a t , z) over the large action space of StarCraft, we estimate action-values with a one-step target, Q(s t , a t , z) = r t + V \u03b8 (s t + 1 , z).\n\nThe overall loss is a weighted sum of the policy and value function losses described above, corresponding to the win-loss reward r t as well as pseudo-rewards based on human data, the KL divergence loss with respect to the supervised policy, and the standard entropy regularization loss 13 . We optimize the overall loss using Adam 57 . The pseudocode of the reinforcement learning algorithm can be found in Supplementary Data, Pseudocode.\n\n\nMulti-agent learning\n\nLeague training is a multi-agent reinforcement learning algorithm that is designed both to address the cycles commonly encountered during self-play training and to integrate a diverse range of strategies. During training, we populate the league by regularly saving the parameters from our agents (that are being trained by the RL algorithm) as new players (which have fixed, frozen parameters). We also continuously re-evaluate the internal payoff estimation, giving agents up-to-date information about their performance against all players in the league (see evaluators in Extended Data Fig. 6).\n\nPrioritized fictitious self-play. Our self-play algorithm plays games between the latest agents for all three races. This approach may chase cycles in strategy space and does not work well in isolation (Fig. 3d). FSP [20][21][22] avoids cycles by playing against all previous players in the league. However, many games are wasted against players that are defeated in almost 100% of games. Consequently, we introduce PFSP. Instead of uniformly sampling opponents in the league, we use a matchmaking mechanism to provide a good learning signal. Given a learning agent A, we sample the frozen opponent B from a candidate set C with probability\nC \u2119 \u2119 f A B f A C ( [ beats ]) \u2211 ( [ beats ]) C\u2208 Where f: [0, 1] \u2192 [0, \u221e) is some weighting function.\nChoosing f hard (x) = (1 \u2212 x) p makes PFSP focus on the hardest players, where p \u2208 \u211d + controls how entropic the resulting distribution is. As f hard (1) = 0, no games are played against opponents that the agent already beats. By focusing on the hardest players, the agent must beat everyone in the league rather than maximizing average performance, which is even more important in highly non-transitive games such as StarCraft (Extended Data Fig. 8), where the pursuit of the mean win rate might lead to policies that are easy to exploit. This scheme is used as the default weighting of PFSP. Consequently, on the theoretical side, one can view f hard as a form of smooth approximation of max-min optimization, as opposed to max-avg, which is imposed by FSP. In particular, this helps with integrating information from exploits, as these are strong but rare counter strategies, and a uniform mixture would be able to just ignore them (Extended Data Fig. 5).\n\nOnly playing against the hardest opponents can waste games against much stronger opponents, so PFSP also uses an alternative curriculum, f var (x) = x(1 \u2212 x), where the agent preferentially plays against opponents around its own level. We use this curriculum for main exploiters and struggling main agents.\n\nPopulating the league. During training we used three agent types that differ only in the distribution of opponents they train against, when they are snapshotted to create a new player, and the probability of resetting to the supervised parameters.\n\nMain agents are trained with a proportion of 35% SP, 50% PFSP against all past players in the league, and an additional 15% of PFSP matches against forgotten main players the agent can no longer beat and past main exploiters. If there are no forgotten players or strong exploiters, the 15% is used for self-play instead. Every 2 \u00d7 10 9 steps, a copy of the agent is added as a new player to the league. Main agents never reset.\n\nLeague exploiters are trained using PFSP and their frozen copies are added to the league when they defeat all players in the league in more than 70% of games, or after a timeout of 2 \u00d7 10 9 steps. At this point there is a 25% probability that the agent is reset to the supervised parameters. The intuition is that league exploiters identify global blind spots in the league (strategies that no player in the league can beat, but that are not necessarily robust themselves).\n\nMain exploiters play against main agents. Half of the time, and if the current probability of winning is lower than 20%, exploiters use PFSP with f var weighting over players created by the main agents. This forms a curriculum that facilitates learning. Otherwise there is enough learning signal and it plays against the current main agents. These agents are added to the league whenever all three main agents are defeated in more than 70% of games, or after a timeout of 4 \u00d7 10 9 steps. They are then reset to the supervised parameters. Main exploiters identify weaknesses of main agents, and consequently make them more robust.\n\nFor more details refer to the Supplementary Data, Pseudocode.\n\n\nInfrastructure\n\nIn order to train the league, we run a large number of StarCraft II matches in parallel and update the parameters of the agents on the basis of data from those games. To manage this, we developed a highly scalable training setup with different types of distributed workers. For every training agent in the league, we run 16,000 concurrent StarCraft II matches and 16 actor tasks (each using a TPU v3 device with eight TPU cores 23 ) to perform inference. The game instances progress asynchronously on preemptible CPUs (roughly equivalent to 150 processors with 28 physical cores each), but requests for agent steps are batched together dynamically to make efficient use of the TPU. Using TPUs for batched inference provides large efficiency gains over previous work 14,29 .\n\nActors send sequences of observations, actions, and rewards over the network to a central 128-core TPU learner worker, which updates the parameters of the training agent. The received data are buffered in memory and replayed twice. The learner worker performs large-batch synchronous updates. Each TPU core processes a mini-batch of four sequences, for a total batch size of 512. The learner processes about 50,000 agent steps per second. The actors update their copy of the parameters from the learner every 10 s.\n\nWe instantiate 12 separate copies of this actor-learner setup: one main agent, one main exploiter and two league exploiter agents for each StarCraft race. One central coordinator maintains an estimate of the payoff matrix, samples new matches on request, and resets main and league exploiters. Additional evaluator workers (running on the CPU) are used to supplement the payoff estimates. See Extended Data Fig. 6 for an overview of the training setup.\n\n\nEvaluation\n\nAlphaStar Battle.net evaluation. AlphaStar agents were evaluated against humans on Battle.net, Blizzard's online matchmaking system based on MMR ratings, on StarCraft II balance patch 4.9.3. AlphaStar Final was rated at Grandmaster level, above 99.8% of human players who were active enough in the past months to be placed into a league on the European server (about 90,000 players).\n\nAlphaStar played only opponents who opted to participate in the experiment (the majority of players opted in) 62 , used an anonymous account name, and played on four maps: Cyber Forest, Kairos Junction, King's Cove, and New Repugnancy. Blizzard updated the map pool a few weeks before testing. Instead of retraining AlphaStar, we simply played on the four common maps that were kept in the pool of seven available maps. Humans also must select at least four maps and frequently play under anonymous account names. Each agent ran on a single high-end consumer GPU. We evaluated at three points during training: supervised, midpoint, and final.\n\nFor the supervised and midpoint evaluations, each agent began with a fresh, unranked account. Their MMR was updated on Battle.net as for humans. The supervised and midpoint evaluations played 30 and 60 games, respectively. The midpoint evaluation was halted while still increasing because the anonymity constraint was compromised after 50 games.\n\nFor the final Battle.net evaluation, we used several accounts to parallelize the games and help to avoid identification. The MMRs of our accounts were seeded randomly from the distribution of combined, estimated, midpoint MMRs. Consequently, we no longer used the iterative MMR estimation provided in Battle.net, and instead used the underlying probabilistic model provided by Blizzard: given our rating r with uncertainty u, and opponent rating r i with uncertainty u i \u2208 [0.  63 . In particular, the agent did not have a limited camera, was less restricted in how often it could act, and played for and against a single StarCraft II race on a single map. AlphaStar won all ten games in both five-game series, although an early camera prototype lost a follow-up game against MaNa.\n\n\nAnalysis\n\nAgent sets. For validation agents, we validated league robustness against a set of 17 strategies trained using only main agents and no exploiters, and fixing z to a hand-curated set of interesting strategies (for example, a cannon rush or early flying units).\n\nAblation test agents included the validation agents, and the first (that is, weaker) 20 main and 20 league exploiter Protoss agents created by full league training.\n\nFor fixed opponents, to evaluate our reinforcement learning algorithms, we computed the best response against a uniform mixed strategy composed of the first ten league exploiter Protoss agents created by league training. where r 1 and r 2 are the Elo ratings of both players. As the Elo rating has no intrinsic absolute scale, we ground it by setting the rating of the built-in elite bot to 0. RPP is the expected outcome of the meta-game between two populations after they reach the Nash equilibrium 19 . Given a payoff matrix between all agents in leagues A and B of sizes N and M, respectively, P AB \u2208 [0, 1] N \u00d7 M :\n\n\nMetrics used in\nP P P P RPP( ) = Nash( ) Nash( ) AB AB T AB BA\nwhere Nash(X) \u2208 [0, 1] K is a vector of probabilities assigned to playing each agent, in league X of size K, in the Nash equilibrium. High RPP means that league A consists of agents that can form a mixed strategy that can exploit agents from league B, while not being too exploitable by any mixed strategy from league B.\n\n\nAlphaStar generality\n\nTo address the complexity and game-theoretic challenges of StarCraft, AlphaStar uses a combination of new and existing general-purpose techniques for neural network architectures, imitation learning, reinforcement learning, and multi-agent learning. These techniques and their combination are widely applicable. The neural network architecture components, including the new scatter connections, are all generally applicable to any domain whose observations comprise a combination of images, lists, and sets, all of which are present in StarCraft.\n\nAlphaStar's action space is defined as a set of functions with typed arguments. Any domain which defines a similar API can be tackled with the same decomposition of complex, structured action spaces, whose joint probability is decomposed via the chain rule (akin to, for example, language modelling 10 or theorem proving).\n\nImitation learning for AlphaStar requires a large number of human demonstrations to be effective, and thus is applicable only to those domains that provide such a set of demonstrations. Using a latent variable z to induce exploration is not specific to StarCraft, but the particular choice of statistics required domain knowledge. In particular, we chose z to encode openings and units in StarCraft. Pseudo-rewards were based on appropriate distance metrics for these statistics, such as edit distance or Hamming distance.\n\nAlphaStar's underlying reinforcement learning algorithm can be applied to any reinforcement learning environment. The use of an opponent's observations for a lower-variance baseline and new components, such as hybrid off-policy learning, UPGO, and distillation towards an imitation policy, are also widely applicable.\n\nLast, we propose a new multi-agent training regime with different kinds of exploiters whose purpose is to strengthen the main agents. Together with PFSP, these are all general-purpose techniques that can be applied to any multiplayer domain.\n\n\nProfessional player statement\n\nThe following quote describes our interface and limitations from StarCraft II professional player Dario 'TLO' W\u00fcnsch (who is part of the team and an author of this paper).\n\n\"The limitations that have been put in place for AlphaStar now mean that it feels very different from the initial show match in January. While AlphaStar has excellent and precise control it doesn't feel superhumancertainly not on a level that a human couldn't theoretically achieve. It is better in some aspects than humans and then also worse in others, but of course there are going to be unavoidable differences between AlphaStar and human players.\n\nI've had the pleasure of providing consultation to the AlphaStar team to help ensure that DeepMind's system does not have any unfair advantages over human players. Overall, it feels very fair, like it is playing a 'real' game of StarCraft and doesn't completely throw the balance off by having unrealistic capabilities. Now that it has limited camera view, when I multi-task it doesn't always catch everything at the same time, so that aspect also feels very fair and more human-like.\"\n\n\nReporting summary\n\nFurther information on research design is available in the Nature Research Reporting Summary linked to this paper. . Each Gaussian represents an opponent MMR (with uncertainty): AlphaStar won against opponents shown in green and lost to those shown in red. Blue is our MMR estimate, and black is the MMR reported by StarCraft II. The orange background is the Grandmaster league range. Bottom, win probability versus gap in MMR. The shaded grey region shows MMR model predictions when players' uncertainty is varied. The red and blue line are empirical win rates for players above 6,000 MMR and AlphaStar Final, respectively. Both human and AlphaStar win rates closely follow the MMR model.\n\n\nExtended Data Fig. 8 | Payoff matrix (limited to only Protoss versus\n\nProtoss games for simplicity) split into agent types of the league. Blue means a row agent wins, red loses, and white draws. The main agents behave transitively: the more recent agents win consistently against older main agents and exploiters. Interactions between exploiters are highly non-transitive: across the full payoff, there are around 3,000,000 rock-paper-scissor cycles (with requirement of at least 70% win rates to form a cycle) that involve at least one exploiter, and around 200 that involve only main agents.\n\n\nArticle\n\n\nExtended Data Table 1 | Agent input space\n\nThe observations received by the agent through the raw interface. Information is hidden if it would be hidden from a human player. For example, AlphaStar will not see most information about invisible opponent units unless there is a detector; opponent units hidden by the fog of war will not appear in the list of units; opponent units outside the agent's camera view will have only the owner, display type, and position; and opponent's cloaked units will appear in the list only if they are within the agent's camera view. Note that this interface displays information that must be inferred or remembered by humans, such as the armour upgrades of a visible opponent unit, attack cool-downs, or entities that are occluded by other entities.\n\n\nExtended Data Table 2 | Agent action space\n\nThe action arguments that agents can submit through the raw interface as part of an action. Some fields may be ignored, depending on the action type. \n\n\nStatistics\n\nFor all statistical analyses, confirm that the following items are present in the figure legend, table legend, main text, or Methods section.\n\n\nn/a Confirmed\n\nThe exact sample size (n) for each experimental group/condition, given as a discrete number and unit of measurement A statement on whether measurements were taken from distinct samples or whether the same sample was measured repeatedly\n\nThe statistical test(s) used AND whether they are one-or two-sided Only common tests should be described solely by name; describe more complex techniques in the Methods section.\n\nA description of all covariates tested A description of any assumptions or corrections, such as tests of normality and adjustment for multiple comparisons A full description of the statistical parameters including central tendency (e.g. means) or other basic estimates (e.g. regression coefficient) AND variation (e.g. standard deviation) or associated estimates of uncertainty (e.g. confidence intervals)\n\nFor null hypothesis testing, the test statistic (e.g. F, t, r) with confidence intervals, effect sizes, degrees of freedom and P value noted \n\n\nData analysis\n\nWe used the open source environment to interact with the game of StarCraft II, provided by Blizzard and DeepMind (https://github.com/ deepmind/pysc2), using the game version 4.10. The networks used the TensorFlow 1.0 library with custom extensions. Analysis was performed with custom code written in Python 2.7. We additionally provide pseudocode for all algorithms described in the paper.\n\nFor manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published literature, software must be made available to editors/reviewers. We strongly encourage code deposition in a community repository (e.g. GitHub). See the Nature Research guidelines for submitting code & software for further information.\n\n\nData\n\nPolicy information about availability of data All manuscripts must include a data availability statement. This statement should provide the following information, where applicable:\n\n-Accession codes, unique identifiers, or web links for publicly available datasets -A list of figures that have associated raw data -A description of any restrictions on data availability\n\nWe did provide both the raw data used in the paper from the online experiment, and all the evaluation games played in the StarCraft II standard Replay format. The dataset containing all the replays used for imitation learning are distributed by Blizzard using a specific API: https://github.com/Blizzard/s2client-proto nature research | reporting summary\n\n\nOctober 2018\n\nField-specific reporting Please select the one below that is the best fit for your research. If you are not sure, read the appropriate sections before making your selection.\n\n\nLife sciences Behavioural & social sciences Ecological, evolutionary & environmental sciences\n\nFor a reference copy of the document with all sections, see nature.com/documents/nr-reporting-summary-flat.pdf\n\n\nLife sciences study design\n\nAll studies must disclose on these points even when the disclosure is negative.\n\n\nSample size\n\nTo study our agents performance, we played a total of 360 games online against the population of players that play StarCraft II in the European servers. The sample size was determined with consultation with Blizzard and professional players, who deemed that 60 games would be sufficient to estimate performance of a new professional level player reliably with low uncertainty (less than 50 MMR). We did play 90 games total, per race, plus 30, per race, for supervised agents. For the league analysis we used around 130,000,000 full games of agent vs agent, and for ablations, we used around 20,000,000 games.\n\nData exclusions No data was excluded from the study.\n\n\nReplication\n\nBecause of the nature of the game, we did perform three independent experiments, using three distinct races. From the total of 9 runs, we did not observe any significant deviation, and thus we reproduced the intended conditions of the experiment ourselves. Because we played anonymously, reproducing the same conditions in future studies should be relatively easy, assuming care is taken to remain anonymous.\n\nRandomization The players and order in which we played against them was determined by the matchmaking algorithm that Blizzard employs to create matches in their online service, which was designed many years before our study, and which the authors of this manuscript had no control over. Such system is solely based on the skill level of players, and is thus random and the authors of this manuscript were blind to group allocation.\n\nFig. 3 |\n3Ablations for key components of AlphaStar.\n\n\nFig. 4 | AlphaStar training progression. a, Training Elo scores of agents in the league during the 44 days of training. Each point represents a past player, evaluated against the entire league and the elite built-in bot (whose Elo is set to 0). b, Proportion of validation agents that beat the main agents in more than 80 out of 160 games. This value increased steadily over time, which shows the robustness of league training to unseen strategies. c, The Nash distribution (mixture of the least exploitable players) of the players in the league, as training progressed. It puts the most weight on recent players, suggesting that the latest strategies largely dominate earlier ones, without much forgetting or cycling. For example, player 40 was part of the Nash distribution from its creation at day 20 until 5 days later, when it was completely dominated by newer agents. d, Average number of each unit built by the Protoss agents over the course of league training, normalized by the most common unit. Unlike the main agents, the exploiters rapidly explore different unit compositions. Worker units have been removed for clarity.\n\n\nData Fig. 5 | A more detailed analysis of multi-agent ablations from Fig. 3c, d. PFSP-based training outperforms FSP under all measures considered: it has a stronger population measured by relative population performance, provides a less exploitable solution, and has better final agent performance against the corresponding league. Extended Data Fig. 6 | Training infrastructure. Diagram of the training setup for the entire league. Article Extended Data Fig. 7 | Battle.net performance details. Top, visualization of all the matches played by AlphaStar Final (right) and matches against opponents above 4,500 MMR of AlphaStar Mid (left)\n\n\nwishes to improve the reproducibility of the work that we publish. This form provides structure for consistency and transparency in reporting. For further information on Nature Research policies, see Authors & Referees and the Editorial Policy Checklist.\n\n\nGive P values as exact values whenever suitable.For Bayesian analysis, information on the choice of priors and Markov chain Monte Carlo settings For hierarchical and complex designs, identification of the appropriate level for tests and full reporting of outcomes Estimates of effect sizes (e.g. Cohen's d, Pearson's r), indicating how they were calculated Our web collection on statistics for biologists contains articles on many of the points above.Software and codePolicy information about availability of computer code Data collection Data was collected using the publicly available version of StarCraft II (versions 4.8.2 to 4.10), developed by Blizzard Entertainment.\n\n\nGame environment. StarCraft is a real-time strategy game that takes place in a science fiction universe. The franchise, from Blizzard Entertainment, comprises StarCraft: Brood War and StarCraft II. In this paper, we used StarCraft II. Since StarCraft was released in 1998, there has been a strong competitive community with tens of millions of dollars of prize money. The most common competitive setting of StarCraft II is 1v1, where each player chooses one of the three available races-Terran, Protoss, and Zerg-which all have distinct units and buildings, exhibit different mechanics, and necessitate different strategies when playing for and against. There is also a Random race, where the game selects the player's race at random. Players begin with a small base and a few worker units, which gather resources to build additional units and buildings, scout the opponent, and research new technologies. A player is defeated if they lose all buildings.\n\n\n1, 1.0], the probability of the outcome o i \u2208 {\u22121, 1} is\uf8ec \n\uf8f7 \n\n\uf8eb \n\n\uf8ed \n\n\uf8ec \n\uf8ec \n\n\uf8f6 \n\n\uf8f8 \n\n\uf8f7 \n\uf8f7 \n\n\uf8eb \n\uf8ed \n\n\uf8f6 \n\uf8f8 \n\n\u2119 \n\u2119 \no \nr u r u \no \nr u r u \n\n\u03a6 \nr r \n\nu u \n\n\u03a6 \nr r \n\n[ =1 , , , ] =1 \u2212 [ =\u2212 1 , , , ] \n\n= \n\u2212 \n\n400 2+ + \n\n\u2248 \n\u2212 \n568 \n\ni \ni i \ni \ni i \n\ni \n\ni \n\ni \n\n2 \n2 \n\nwhere \u03a6 is the cumulative distribution function (CDF) of a standard \nGaussian distribution, and where we used Battle.net's minimum uncer-\ntainties u = u i = 0.1. \nUnder independent and identically distributed (IID) assumptions \nof match results and a uniform prior over MMRs, we can compute our \nrating as \n\n\u2119 \n\u2119 \n\n\u2119 \n\nr \nr U r \n\no r r \n\nargmax [ results] =argmax [results ] ( ) \n\n= argmax \u220f [ \n, ] \n\nr \nr \n\nr \ni \n\nN \n\ni \ni \n\n\u2208\u2115 \n\u2208\u2115 \n\n\u2208\u2115 \n=1 \n\nWe validated our MMR computation on the 200 most recent matches of \nDario 'TLO' W\u00fcnsch, a professional StarCraft II player, and obtained an \nMMR estimate of 6,334; the average MMR reported by Battle.net was 6,336. \n\nStarCraft demonstration evaluation. In December 2018, we played \ntwo five-game series against StarCraft II professional players Grzegorz \n'MaNa' Komincz and Dario 'TLO' W\u00fcnsch, although TLO did not play the \nsame StarCraft II race that he plays professionally. These games took \nplace with a different, preliminary version of AlphaStar \n\n\nFigures.To compute internal Elo ratings of the league, we added the built-in bots, and used them to estimate Elo with the following model:\uf8ec \n\uf8f7 \n\n\uf8eb \n\uf8ed \n\n\uf8f6 \n\uf8f8 \n\u2119 r \nr \ne \n\u03a6 \nr r \n[ beats ] = \n1 \n\n1 + \n\u2248 \n\u2212 \n400 \n\nr r \n1 \n2 \n\u2212( \u2212 )/400 \n\n1 \n2 \n\n1 2 \n\n\nAcknowledgements We thank Blizzard for creating StarCraft and for their continued support of the research environment, and for enabling AlphaStar to participate in Battle.net. In particular, we thank A. Hudelson, C. Lee, K. Calderone, and T. Morten. We also thank StarCraft II professional players G.BlindingThe authors were blind to group allocation. See \"Randomization\".Reporting for specific materials, systems and methodsWe require information from authors about some types of materials, experimental systems and methods used in many studies. Here, indicate whether each material, system or method listed is relevant to your study. If you are not sure if a list item applies to your research, read the appropriate section before selecting a response.Data availabilityAll the games that AlphaStar played online can be found in the file 'replays.zip' in the Supplementary Data, and the raw data from the Battle.net experiment can be found in 'bnet.json' in the Supplementary Data.Code availabilityThe StarCraft II environment was open sourced in 2017 by Blizzard and DeepMind 7 . All the human replays used for imitation learning can be found at https://github.com/Blizzard/s2client-proto. The pseudocode for the supervised learning, reinforcement learning, and multi-agent learning components of AlphaStar can be found in the file 'pseudocode. zip' in the Supplementary Data. All the neural architecture details and hyper-parameters can be found in the file 'detailed-architecture.txt' in the Supplementary Data.\n. A I Aiide Starcraft, Competition, Student StarCraft AI Tournament and Ladder. AIIDE StarCraft AI Competition. https://www.cs.mun.ca/dchurchill/starcraftaicomp/. 2. Student StarCraft AI Tournament and Ladder. https://sscaitournament.com/.\n\nStarcraft 2 AI ladder. Starcraft 2 AI ladder. https://sc2ai.net/.\n\nAn analysis of model-based heuristic search techniques for StarCraft combat scenarios. D Churchill, Z Lin, G Synnaeve, Artificial Intelligence and Interactive Digital Entertainment Conf. Churchill, D., Lin, Z. & Synnaeve, G. An analysis of model-based heuristic search techniques for StarCraft combat scenarios. in Artificial Intelligence and Interactive Digital Entertainment Conf. (AAAI, 2017).\n\nReinforcement Learning: An Introduction. R Sutton, A Barto, MIT PressSutton, R. & Barto, A. Reinforcement Learning: An Introduction (MIT Press, 1998).\n\nDeep learning. Y Lecun, Y Bengio, G Hinton, Nature. 521LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. Nature 521, 436-444 (2015).\n\nStarCraft II: a new challenge for reinforcement learning. O Vinyals, Preprint atVinyals, O. et al. StarCraft II: a new challenge for reinforcement learning. Preprint at https://arxiv.org/abs/1708.04782 (2017).\n\nAttention is all you need. A Vaswani, Adv. Neural Information Process. Syst. 30Vaswani, A. et al. Attention is all you need. Adv. Neural Information Process. Syst. 30, 5998-6008 (2017).\n\nLong short-term memory. S Hochreiter, J Schmidhuber, Neural Comput. 9Hochreiter, S. & Schmidhuber, J. Long short-term memory. Neural Comput. 9, 1735-1780 (1997).\n\nRecurrent neural network based language model. T Mikolov, M Karafiat, L Burget, J Cernocky, S Khudanpur, INTERSPEECH-2010 1045-1048Mikolov, T., Karafiat, M., Burget, L., Cernocky, J. & Khudanpur, S. Recurrent neural network based language model. INTERSPEECH-2010 1045-1048 (2010).\n\nDiscrete sequential prediction of continuous actions for deep RL. L Metz, J Ibarz, N Jaitly, J Davidson, Preprint atMetz, L., Ibarz, J., Jaitly, N. & Davidson, J. Discrete sequential prediction of continuous actions for deep RL. Preprint at https://arxiv.org/abs/1705.05035v3 (2017).\n\n. O Vinyals, M Fortunato, N Jaitly, Pointer, Networks, Adv. Neural Information Process. Syst. 28Vinyals, O., Fortunato, M. & Jaitly, N. Pointer networks. Adv. Neural Information Process. Syst. 28, 2692-2700 (2015).\n\nAsynchronous methods for deep reinforcement learning. V Mnih, Proc. Machine Learning Res. Machine Learning Res48Mnih, V. et al. Asynchronous methods for deep reinforcement learning. Proc. Machine Learning Res. 48, 1928-1937 (2016).\n\nIMPALA: scalable distributed deep-RL with importance weighted actor-learner architectures. L Espeholt, Proc. Machine Learning Res. 80Espeholt, L. et al. IMPALA: scalable distributed deep-RL with importance weighted actor-learner architectures. Proc. Machine Learning Res. 80, 1407-1416 (2018).\n\nSample efficient actor-critic with experience replay. Z Wang, Preprint atWang, Z. et al. Sample efficient actor-critic with experience replay. Preprint at https:// arxiv.org/abs/1611.01224v2 (2017).\n\nLearning to predict by the method of temporal differences. R Sutton, Mach. Learn. 3Sutton, R. Learning to predict by the method of temporal differences. Mach. Learn. 3, 9-44 (1988).\n\nSelf-Imitation Learning. J Oh, Y Guo, S Singh, H Lee, Proc. Machine Learning Res. 80Oh, J., Guo, Y., Singh, S. & Lee, H. Self-Imitation Learning. Proc. Machine Learning Res. 80, 3875-3884 (2018).\n\nA general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. D Silver, Science. 362Silver, D. et al. A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. Science 362, 1140-1144 (2018).\n\nOpen-ended learning in symmetric zero-sum games. D Balduzzi, Proc. Machine Learning Res. Machine Learning Res97Balduzzi, D. et al. Open-ended learning in symmetric zero-sum games. Proc. Machine Learning Res. 97, 434-443 (2019).\n\nIterative solution of games by fictitious play. G W Brown, Act. Anal. Prod. Alloc. 13Brown, G. W. Iterative solution of games by fictitious play. Act. Anal. Prod. Alloc. 13, 374-376 (1951).\n\nGeneralised weakened fictitious play. D S Leslie, E J Collins, Games Econ. Behav. 56Leslie, D. S. & Collins, E. J. Generalised weakened fictitious play. Games Econ. Behav. 56, 285-298 (2006).\n\nFictitious self-play in extensive-form games. J Heinrich, M Lanctot, D Silver, Proc. Intl Conf. Machine Learning. Intl Conf. Machine Learning32Heinrich, J., Lanctot, M. & Silver, D. Fictitious self-play in extensive-form games. Proc. Intl Conf. Machine Learning 32, 805-813 (2015).\n\nIn-datacenter performance analysis of a tensor processing unit. N P Jouppi, Preprint atJouppi, N. P. et al. In-datacenter performance analysis of a tensor processing unit. Preprint at https://arxiv.org/abs/1704.04760v1 (2017).\n\nThe Rating of Chessplayers, Past and Present. A E Elo, ArcoElo, A. E. The Rating of Chessplayers, Past and Present (Arco, 2017).\n\n. M Campbell, A Hoane, F Deep Hsu, Blue, Artif. Intell. 134Campbell, M., Hoane, A. & Hsu, F. Deep Blue. Artif. Intell. 134, 57-83 (2002).\n\nMastering the game of Go with deep neural networks and tree search. D Silver, Nature. 529Silver, D. et al. Mastering the game of Go with deep neural networks and tree search. Nature 529, 484-489 (2016).\n\nHuman-level control through deep reinforcement learning. V Mnih, Nature. 518Mnih, V. et al. Human-level control through deep reinforcement learning. Nature 518, 529-533 (2015).\n\nCuriosity-driven exploration by selfsupervised prediction. D Pathak, P Agrawal, A A Efros, T Darrell, Proc. IEEE Conf. Computer Vision Pattern Recognition Workshops. IEEE Conf. Computer Vision Pattern Recognition WorkshopsIEEEPathak, D., Agrawal, P., Efros, A. A. & Darrell, T. Curiosity-driven exploration by self- supervised prediction. Proc. IEEE Conf. Computer Vision Pattern Recognition Workshops 16-17 (IEEE, 2017).\n\nHuman-level performance in 3D multiplayer games with populationbased reinforcement learning. M Jaderberg, Science. 364Jaderberg, M. et al. Human-level performance in 3D multiplayer games with population- based reinforcement learning. Science 364, 859-865 (2019).\n\n. Openai Openai Five, OpenAI OpenAI Five. https://blog.openai.com/openai-five/ (2018).\n\nM Buro, Real-time strategy games: a new AI research challenge. Intl Joint Conf. Artificial Intelligence. Buro, M. Real-time strategy games: a new AI research challenge. Intl Joint Conf. Artificial Intelligence 1534-1535 (2003).\n\nThe StarCraft multi-agent challenge. M Samvelyan, Intl Conf. Autonomous Agents and MultiAgent Systems. Samvelyan, M. et al. The StarCraft multi-agent challenge. Intl Conf. Autonomous Agents and MultiAgent Systems 2186-2188 (2019).\n\nRelational deep reinforcement learning. V Zambaldi, Preprint atZambaldi, V. et al. Relational deep reinforcement learning. Preprint at https://arxiv.org/ abs/1806.01830v2 (2018).\n\nEpisodic exploration for deep deterministic policies: an application to StarCraft micromanagement tasks. N Usunier, G Synnaeve, Z Lin, S Chintala, Preprint atUsunier, N., Synnaeve, G., Lin, Z. & Chintala, S. Episodic exploration for deep deterministic policies: an application to StarCraft micromanagement tasks. Preprint at https://arxiv.org/ abs/1609.02993v3 (2017).\n\nCase-based reasoning for build order in real-time strategy games. B G Weber, M Mateas, AIIDE '09 Proc. 5th AAAI Conf. Artificial Intelligence and Interactive Digital Entertainment. Weber, B. G. & Mateas, M. Case-based reasoning for build order in real-time strategy games. AIIDE '09 Proc. 5th AAAI Conf. Artificial Intelligence and Interactive Digital Entertainment 106-111 (2009).\n\nORTS: a hack-free RTS game environment. M Buro, Intl Conf. Computers and Games. SpringerBuro, M. ORTS: a hack-free RTS game environment. Intl Conf. Computers and Games 280-291 (Springer, 2002).\n\nSparCraft: open source StarCraft combat simulation. D Churchill, Churchill, D. SparCraft: open source StarCraft combat simulation. https://code.google. com/archive/p/sparcraft/ (2013).\n\nStarCraft competition. B G Weber, Aiide, Artificial Intelligence and Interactive Digital Entertainment Conf. Weber, B. G. AIIDE 2010 StarCraft competition. Artificial Intelligence and Interactive Digital Entertainment Conf. (2010).\n\nImproving Monte Carlo tree search policies in StarCraft via probabilistic models learned from replay data. Artificial Intelligence and Interactive Digital Entertainment Conf. A Uriarte, S Onta\u00f1\u00f3n, Uriarte, A. & Onta\u00f1\u00f3n, S. Improving Monte Carlo tree search policies in StarCraft via probabilistic models learned from replay data. Artificial Intelligence and Interactive Digital Entertainment Conf. 101-106 (2016).\n\nBuilding a player strategy model by analyzing replays of real-time strategy games. J.-L Hsieh, C.-T Sun, IEEE Intl Joint Conf. Neural Networks. Hsieh, J.-L. & Sun, C.-T. Building a player strategy model by analyzing replays of real-time strategy games. IEEE Intl Joint Conf. Neural Networks 3106-3111 (2008).\n\nA Bayesian model for plan recognition in RTS games applied to StarCraft. Artificial Intelligence and Interactive Digital Entertainment Conf. G Synnaeve, P Bessiere, Synnaeve, G. & Bessiere, P. A Bayesian model for plan recognition in RTS games applied to StarCraft. Artificial Intelligence and Interactive Digital Entertainment Conf. 79-84 (2011).\n\nStarCraft micromanagement with reinforcement learning and curriculum transfer learning. K Shao, Y Zhu, D Zhao, IEEE Trans. Emerg. Top. Comput. Intell. 3Shao, K., Zhu, Y. & Zhao, D. StarCraft micromanagement with reinforcement learning and curriculum transfer learning. IEEE Trans. Emerg. Top. Comput. Intell. 3, 73-84 (2019).\n\n. Facebook Cherrypi, Facebook CherryPi. https://torchcraft.github.io/TorchCraftAI/.\n\n. Berkeley Overmind, Berkeley Overmind. https://www.icsi.berkeley.edu/icsi/news/2010/10/klein-berkeley- overmind (2010).\n\nLearning macromanagement in StarCraft from replays using deep learning. N Justesen, S Risi, IEEE Conf. Computational Intelligence and Games (CIG). Justesen, N. & Risi, S. Learning macromanagement in StarCraft from replays using deep learning. IEEE Conf. Computational Intelligence and Games (CIG) 162-169 (2017).\n\nForward modeling for partial observation strategy games-a StarCraft defogger. G Synnaeve, Adv. Neural Information Process. Syst. 31Synnaeve, G. et al. Forward modeling for partial observation strategy games-a StarCraft defogger. Adv. Neural Information Process. Syst. 31, 10738-10748 (2018).\n\n. S S Farooq, I.-S Oh, M.-J Kim, K J Kim, StarCraft AI competition report. AI Mag. 37Farooq, S. S., Oh, I.-S., Kim, M.-J. & Kim, K. J. StarCraft AI competition report. AI Mag. 37, 102-107 (2016).\n\nTStarBots: defeating the cheating level builtin AI in StarCraft II in the full game. P Sun, Preprint atSun, P. et al. TStarBots: defeating the cheating level builtin AI in StarCraft II in the full game. Preprint at https://arxiv.org/abs/1809.07193v3 (2018).\n\nProximal policy optimization algorithms. J Schulman, F Wolski, P Dhariwal, A Radford, O Klimov, Preprint atSchulman, J., Wolski, F., Dhariwal, P., Radford, A. & Klimov, O. Proximal policy optimization algorithms. Preprint at https://arxiv.org/abs/1707.06347v2 (2017).\n\nReward learning from human preferences and demonstrations in Atari. B Ibarz, Adv. Neural Information Process. Syst. 31Ibarz, B. et al. Reward learning from human preferences and demonstrations in Atari. Adv. Neural Information Process. Syst. 31, 8011-8023 (2018).\n\nOvercoming exploration in reinforcement learning with demonstrations. A Nair, B Mcgrew, M Andrychowicz, W Zaremba, P Abbeel, IEEE Intl Conf. Robotics and Automation. Nair, A., McGrew, B., Andrychowicz, M., Zaremba, W. & Abbeel, P. Overcoming exploration in reinforcement learning with demonstrations. IEEE Intl Conf. Robotics and Automation 6292-6299 (2018).\n\nDeep reinforcement learning from human preferences. P F Christiano, Adv. Neural Information Process. Syst. 30Christiano, P. F. et al. Deep reinforcement learning from human preferences. Adv. Neural Information Process. Syst. 30, 4299-4307 (2017).\n\nA unified game-theoretic approach to multiagent reinforcement learning. M Lanctot, Adv. Neural Information Process. Syst. 30Lanctot, M. et al. A unified game-theoretic approach to multiagent reinforcement learning. Adv. Neural Information Process. Syst. 30, 4190-4203 (2017).\n\nFiLM: visual reasoning with a general conditioning layer. E Perez, F Strub, H De Vries, V Dumoulin, A Courville, Preprint atPerez, E., Strub, F., De Vries, H., Dumoulin, V. & Courville, A. FiLM: visual reasoning with a general conditioning layer. Preprint at https://arxiv.org/abs/1709.07871v2 (2018).\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proc. IEEE Conf. Computer Vision and Pattern Recognition. IEEE Conf. Computer Vision and Pattern RecognitionHe, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. Proc. IEEE Conf. Computer Vision and Pattern Recognition 770-778 (2016).\n\nDistilling the knowledge in a neural network. G Hinton, O Vinyals, J Dean, Preprint atHinton, G., Vinyals, O. & Dean, J. Distilling the knowledge in a neural network. Preprint at https://arxiv.org/abs/1503.02531v1 (2015).\n\nAdam: a method for stochastic optimization. D P Kingma, J Ba, Preprint atKingma, D. P. & Ba, J. Adam: a method for stochastic optimization. Preprint at https://arxiv. org/abs/1412.6980v9 (2014).\n\nC M Bishop, Pattern Recognition and Machine Learning. SpringerBishop, C. M. Pattern Recognition and Machine Learning (Springer, 2006).\n\n. A A Rusu, Policy distillation. Preprint atRusu, A. A. et al. Policy distillation. Preprint at https://arxiv.org/abs/1511.06295 (2016).\n\nActor-mimic: deep multitask and transfer reinforcement learning. E Parisotto, J Ba, R Salakhutdinov, Preprint atParisotto, E., Ba, J. & Salakhutdinov, R. Actor-mimic: deep multitask and transfer reinforcement learning. Preprint at https://arxiv.org/abs/1511.06342 (2016).\n\nD Precup, R S Sutton, S P Singh, Eligibility traces for off-policy policy evaluation. ICML '00 Proc. 17th Intl Conf. Machine Learning. Precup, D., Sutton, R. S. & Singh, S. P. Eligibility traces for off-policy policy evaluation. ICML '00 Proc. 17th Intl Conf. Machine Learning 759-766 (2016).\n\n. DeepMind Research on Ladder. DeepMind Research on Ladder. https://starcraft2.com/en-us/news/22933138 (2019).\n\nAlphaStar: mastering the real-time strategy game StarCraft II. O Vinyals, DeepMindVinyals, O. et al. AlphaStar: mastering the real-time strategy game StarCraft II https:// deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii (DeepMind, 2019).\n", "annotations": {"author": "[{\"end\":127,\"start\":94},{\"end\":144,\"start\":128},{\"end\":166,\"start\":145},{\"end\":183,\"start\":167},{\"end\":198,\"start\":184},{\"end\":214,\"start\":199},{\"end\":228,\"start\":215},{\"end\":244,\"start\":229},{\"end\":257,\"start\":245},{\"end\":273,\"start\":258},{\"end\":285,\"start\":274},{\"end\":297,\"start\":286},{\"end\":312,\"start\":298},{\"end\":327,\"start\":313},{\"end\":338,\"start\":328},{\"end\":353,\"start\":339},{\"end\":365,\"start\":354},{\"end\":381,\"start\":366},{\"end\":396,\"start\":382},{\"end\":420,\"start\":397},{\"end\":434,\"start\":421},{\"end\":449,\"start\":435},{\"end\":468,\"start\":450},{\"end\":482,\"start\":469},{\"end\":495,\"start\":483},{\"end\":509,\"start\":496},{\"end\":522,\"start\":510},{\"end\":539,\"start\":523},{\"end\":550,\"start\":540},{\"end\":564,\"start\":551},{\"end\":575,\"start\":565},{\"end\":587,\"start\":576},{\"end\":602,\"start\":588},{\"end\":616,\"start\":603},{\"end\":634,\"start\":617},{\"end\":648,\"start\":635},{\"end\":660,\"start\":649},{\"end\":679,\"start\":661},{\"end\":698,\"start\":680},{\"end\":714,\"start\":699},{\"end\":726,\"start\":715},{\"end\":763,\"start\":727},{\"end\":778,\"start\":764},{\"end\":795,\"start\":779},{\"end\":817,\"start\":796},{\"end\":834,\"start\":818},{\"end\":849,\"start\":835},{\"end\":865,\"start\":850},{\"end\":879,\"start\":866},{\"end\":895,\"start\":880},{\"end\":908,\"start\":896},{\"end\":924,\"start\":909},{\"end\":936,\"start\":925},{\"end\":948,\"start\":937},{\"end\":963,\"start\":949},{\"end\":978,\"start\":964},{\"end\":989,\"start\":979},{\"end\":1004,\"start\":990},{\"end\":1016,\"start\":1005},{\"end\":1032,\"start\":1017},{\"end\":1044,\"start\":1033},{\"end\":1058,\"start\":1045}]", "publisher": null, "author_last_name": "[{\"end\":107,\"start\":100},{\"end\":143,\"start\":133},{\"end\":165,\"start\":156},{\"end\":182,\"start\":175},{\"end\":197,\"start\":191},{\"end\":213,\"start\":208},{\"end\":227,\"start\":223},{\"end\":243,\"start\":237},{\"end\":256,\"start\":250},{\"end\":272,\"start\":264},{\"end\":284,\"start\":282},{\"end\":296,\"start\":290},{\"end\":311,\"start\":305},{\"end\":326,\"start\":317},{\"end\":337,\"start\":332},{\"end\":352,\"start\":347},{\"end\":364,\"start\":361},{\"end\":380,\"start\":373},{\"end\":395,\"start\":386},{\"end\":419,\"start\":409},{\"end\":433,\"start\":426},{\"end\":448,\"start\":442},{\"end\":467,\"start\":459},{\"end\":481,\"start\":475},{\"end\":494,\"start\":488},{\"end\":508,\"start\":502},{\"end\":521,\"start\":516},{\"end\":538,\"start\":530},{\"end\":549,\"start\":545},{\"end\":563,\"start\":558},{\"end\":574,\"start\":572},{\"end\":586,\"start\":582},{\"end\":601,\"start\":593},{\"end\":615,\"start\":609},{\"end\":633,\"start\":625},{\"end\":647,\"start\":642},{\"end\":659,\"start\":653},{\"end\":678,\"start\":669},{\"end\":697,\"start\":686},{\"end\":713,\"start\":705},{\"end\":725,\"start\":721},{\"end\":739,\"start\":733},{\"end\":777,\"start\":770},{\"end\":794,\"start\":784},{\"end\":816,\"start\":807},{\"end\":833,\"start\":826},{\"end\":848,\"start\":842},{\"end\":864,\"start\":859},{\"end\":878,\"start\":874},{\"end\":894,\"start\":888},{\"end\":907,\"start\":901},{\"end\":923,\"start\":915},{\"end\":935,\"start\":933},{\"end\":947,\"start\":941},{\"end\":962,\"start\":956},{\"end\":977,\"start\":968},{\"end\":988,\"start\":983},{\"end\":1003,\"start\":998},{\"end\":1015,\"start\":1012},{\"end\":1031,\"start\":1024},{\"end\":1043,\"start\":1039}]", "author_first_name": "[{\"end\":99,\"start\":94},{\"end\":132,\"start\":128},{\"end\":153,\"start\":145},{\"end\":155,\"start\":154},{\"end\":174,\"start\":167},{\"end\":190,\"start\":184},{\"end\":207,\"start\":199},{\"end\":220,\"start\":215},{\"end\":222,\"start\":221},{\"end\":236,\"start\":229},{\"end\":249,\"start\":245},{\"end\":263,\"start\":258},{\"end\":281,\"start\":274},{\"end\":289,\"start\":286},{\"end\":304,\"start\":298},{\"end\":316,\"start\":313},{\"end\":331,\"start\":328},{\"end\":346,\"start\":339},{\"end\":360,\"start\":354},{\"end\":370,\"start\":366},{\"end\":372,\"start\":371},{\"end\":385,\"start\":382},{\"end\":406,\"start\":397},{\"end\":408,\"start\":407},{\"end\":425,\"start\":421},{\"end\":441,\"start\":435},{\"end\":458,\"start\":450},{\"end\":474,\"start\":469},{\"end\":487,\"start\":483},{\"end\":501,\"start\":496},{\"end\":513,\"start\":510},{\"end\":515,\"start\":514},{\"end\":529,\"start\":523},{\"end\":544,\"start\":540},{\"end\":557,\"start\":551},{\"end\":571,\"start\":565},{\"end\":581,\"start\":576},{\"end\":592,\"start\":588},{\"end\":608,\"start\":603},{\"end\":624,\"start\":617},{\"end\":641,\"start\":635},{\"end\":652,\"start\":649},{\"end\":668,\"start\":661},{\"end\":685,\"start\":680},{\"end\":704,\"start\":699},{\"end\":720,\"start\":715},{\"end\":732,\"start\":727},{\"end\":769,\"start\":764},{\"end\":783,\"start\":779},{\"end\":804,\"start\":796},{\"end\":806,\"start\":805},{\"end\":825,\"start\":818},{\"end\":841,\"start\":835},{\"end\":858,\"start\":850},{\"end\":871,\"start\":866},{\"end\":873,\"start\":872},{\"end\":887,\"start\":880},{\"end\":900,\"start\":896},{\"end\":914,\"start\":909},{\"end\":932,\"start\":925},{\"end\":940,\"start\":937},{\"end\":955,\"start\":949},{\"end\":967,\"start\":964},{\"end\":982,\"start\":979},{\"end\":997,\"start\":990},{\"end\":1011,\"start\":1005},{\"end\":1021,\"start\":1017},{\"end\":1023,\"start\":1022},{\"end\":1038,\"start\":1033},{\"end\":1050,\"start\":1045},{\"end\":1057,\"start\":1051}]", "author_affiliation": null, "title": "[{\"end\":75,\"start\":1},{\"end\":1133,\"start\":1059}]", "venue": "[{\"end\":1149,\"start\":1135}]", "abstract": "[{\"end\":4866,\"start\":1350}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5507,\"start\":5506},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5989,\"start\":5987},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5990,\"start\":5989},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6852,\"start\":6851},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8898,\"start\":8897},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9120,\"start\":9119},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9258,\"start\":9256},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11659,\"start\":11657},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11727,\"start\":11725},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11786,\"start\":11785},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12162,\"start\":12160},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12205,\"start\":12203},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14066,\"start\":14064},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14100,\"start\":14096},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14104,\"start\":14100},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":14108,\"start\":14104},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14279,\"start\":14277},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":25866,\"start\":25864},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25876,\"start\":25874},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":25966,\"start\":25964},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":26089,\"start\":26087},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":26265,\"start\":26263},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":26402,\"start\":26400},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":26406,\"start\":26402},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":26410,\"start\":26406},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":26414,\"start\":26410},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":26418,\"start\":26414},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":26458,\"start\":26455},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":26460,\"start\":26458},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":26559,\"start\":26557},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":26560,\"start\":26559},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":26634,\"start\":26632},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":26764,\"start\":26762},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":26784,\"start\":26782},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":26957,\"start\":26955},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":27035,\"start\":27033},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27162,\"start\":27161},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":27255,\"start\":27253},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":27617,\"start\":27615},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":27701,\"start\":27699},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":28240,\"start\":28238},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":28953,\"start\":28951},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":29102,\"start\":29099},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":29104,\"start\":29102},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":29148,\"start\":29146},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":30606,\"start\":30604},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":30608,\"start\":30606},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30611,\"start\":30608},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":30614,\"start\":30611},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":30616,\"start\":30614},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":31183,\"start\":31181},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":32534,\"start\":32532},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":33429,\"start\":33427},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":34160,\"start\":34157},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":34162,\"start\":34160},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":35120,\"start\":35117},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":35122,\"start\":35120},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":36745,\"start\":36743},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":37425,\"start\":37423},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":37470,\"start\":37468},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":38419,\"start\":38415},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":38423,\"start\":38419},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":38427,\"start\":38423},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":42842,\"start\":42839},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":42844,\"start\":42842},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":44328,\"start\":44326},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":45687,\"start\":45685},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":46931,\"start\":46929}]", "figure": "[{\"attributes\":{\"id\":\"fig_2\"},\"end\":57596,\"start\":57543},{\"attributes\":{\"id\":\"fig_3\"},\"end\":58731,\"start\":57597},{\"attributes\":{\"id\":\"fig_4\"},\"end\":59372,\"start\":58732},{\"attributes\":{\"id\":\"fig_5\"},\"end\":59629,\"start\":59373},{\"attributes\":{\"id\":\"fig_6\"},\"end\":60305,\"start\":59630},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":61262,\"start\":60306},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":62524,\"start\":61263},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":62775,\"start\":62525}]", "paragraph": "[{\"end\":6240,\"start\":4868},{\"end\":6854,\"start\":6242},{\"end\":7307,\"start\":6856},{\"end\":7830,\"start\":7309},{\"end\":8689,\"start\":7832},{\"end\":9358,\"start\":8691},{\"end\":9694,\"start\":9360},{\"end\":11519,\"start\":9696},{\"end\":12542,\"start\":11531},{\"end\":13719,\"start\":12544},{\"end\":14783,\"start\":13721},{\"end\":16443,\"start\":14785},{\"end\":17501,\"start\":16468},{\"end\":17961,\"start\":17503},{\"end\":18874,\"start\":17963},{\"end\":21347,\"start\":18889},{\"end\":23378,\"start\":21376},{\"end\":23829,\"start\":23411},{\"end\":24555,\"start\":23831},{\"end\":25138,\"start\":24571},{\"end\":25686,\"start\":25140},{\"end\":26167,\"start\":25703},{\"end\":27978,\"start\":26169},{\"end\":28627,\"start\":27980},{\"end\":29150,\"start\":28629},{\"end\":29705,\"start\":29152},{\"end\":30073,\"start\":29722},{\"end\":30434,\"start\":30075},{\"end\":30967,\"start\":30436},{\"end\":31290,\"start\":30991},{\"end\":31801,\"start\":31292},{\"end\":32194,\"start\":31803},{\"end\":32636,\"start\":32196},{\"end\":33086,\"start\":32638},{\"end\":33541,\"start\":33113},{\"end\":33878,\"start\":33543},{\"end\":34779,\"start\":33880},{\"end\":36149,\"start\":34781},{\"end\":36336,\"start\":36151},{\"end\":36546,\"start\":36481},{\"end\":37134,\"start\":36611},{\"end\":37575,\"start\":37136},{\"end\":38196,\"start\":37600},{\"end\":38838,\"start\":38198},{\"end\":39899,\"start\":38941},{\"end\":40207,\"start\":39901},{\"end\":40456,\"start\":40209},{\"end\":40885,\"start\":40458},{\"end\":41360,\"start\":40887},{\"end\":41991,\"start\":41362},{\"end\":42054,\"start\":41993},{\"end\":42846,\"start\":42073},{\"end\":43362,\"start\":42848},{\"end\":43816,\"start\":43364},{\"end\":44214,\"start\":43831},{\"end\":44858,\"start\":44216},{\"end\":45205,\"start\":44860},{\"end\":45988,\"start\":45207},{\"end\":46260,\"start\":46001},{\"end\":46426,\"start\":46262},{\"end\":47047,\"start\":46428},{\"end\":47433,\"start\":47113},{\"end\":48004,\"start\":47458},{\"end\":48328,\"start\":48006},{\"end\":48852,\"start\":48330},{\"end\":49171,\"start\":48854},{\"end\":49414,\"start\":49173},{\"end\":49619,\"start\":49448},{\"end\":50072,\"start\":49621},{\"end\":50559,\"start\":50074},{\"end\":51270,\"start\":50581},{\"end\":51866,\"start\":51343},{\"end\":52662,\"start\":51922},{\"end\":52859,\"start\":52709},{\"end\":53015,\"start\":52874},{\"end\":53268,\"start\":53033},{\"end\":53447,\"start\":53270},{\"end\":53854,\"start\":53449},{\"end\":53997,\"start\":53856},{\"end\":54404,\"start\":54015},{\"end\":54765,\"start\":54406},{\"end\":54954,\"start\":54774},{\"end\":55143,\"start\":54956},{\"end\":55499,\"start\":55145},{\"end\":55689,\"start\":55516},{\"end\":55897,\"start\":55787},{\"end\":56007,\"start\":55928},{\"end\":56631,\"start\":56023},{\"end\":56685,\"start\":56633},{\"end\":57109,\"start\":56701},{\"end\":57542,\"start\":57111}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":36480,\"start\":36337},{\"attributes\":{\"id\":\"formula_1\"},\"end\":36610,\"start\":36547},{\"attributes\":{\"id\":\"formula_2\"},\"end\":38940,\"start\":38839},{\"attributes\":{\"id\":\"formula_3\"},\"end\":47112,\"start\":47066}]", "table_ref": "[{\"end\":29889,\"start\":29877},{\"end\":32350,\"start\":32343}]", "section_header": "[{\"end\":11529,\"start\":11522},{\"end\":16466,\"start\":16446},{\"end\":18887,\"start\":18877},{\"end\":21357,\"start\":21350},{\"end\":21374,\"start\":21360},{\"end\":23388,\"start\":23381},{\"end\":23409,\"start\":23391},{\"end\":24569,\"start\":24558},{\"end\":25701,\"start\":25689},{\"end\":29720,\"start\":29708},{\"end\":30989,\"start\":30970},{\"end\":33111,\"start\":33089},{\"end\":37598,\"start\":37578},{\"end\":42071,\"start\":42057},{\"end\":43829,\"start\":43819},{\"end\":45999,\"start\":45991},{\"end\":47065,\"start\":47050},{\"end\":47456,\"start\":47436},{\"end\":49446,\"start\":49417},{\"end\":50579,\"start\":50562},{\"end\":51341,\"start\":51273},{\"end\":51876,\"start\":51869},{\"end\":51920,\"start\":51879},{\"end\":52707,\"start\":52665},{\"end\":52872,\"start\":52862},{\"end\":53031,\"start\":53018},{\"end\":54013,\"start\":54000},{\"end\":54772,\"start\":54768},{\"end\":55514,\"start\":55502},{\"end\":55785,\"start\":55692},{\"end\":55926,\"start\":55900},{\"end\":56021,\"start\":56010},{\"end\":56699,\"start\":56688},{\"end\":57552,\"start\":57544}]", "table": "[{\"end\":62524,\"start\":61321},{\"end\":62775,\"start\":62665}]", "figure_caption": "[{\"end\":57596,\"start\":57554},{\"end\":58731,\"start\":57599},{\"end\":59372,\"start\":58734},{\"end\":59629,\"start\":59375},{\"end\":60305,\"start\":59632},{\"end\":61262,\"start\":60308},{\"end\":61321,\"start\":61265},{\"end\":62665,\"start\":62527}]", "figure_ref": "[{\"end\":7621,\"start\":7612},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":9281,\"start\":9275},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":9321,\"start\":9314},{\"end\":9973,\"start\":9967},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12485,\"start\":12476},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13670,\"start\":13661},{\"end\":13857,\"start\":13845},{\"end\":14456,\"start\":14447},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15314,\"start\":15305},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15944,\"start\":15935},{\"end\":17730,\"start\":17722},{\"end\":17752,\"start\":17746},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18039,\"start\":18033},{\"end\":18065,\"start\":18057},{\"end\":20544,\"start\":20538},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24500,\"start\":24493},{\"end\":25096,\"start\":25089},{\"end\":25121,\"start\":25115},{\"end\":25685,\"start\":25679},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":30699,\"start\":30693},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":30889,\"start\":30882},{\"end\":33084,\"start\":33078},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":34778,\"start\":34769},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":35628,\"start\":35621},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":36046,\"start\":36039},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":36938,\"start\":36931},{\"end\":38194,\"start\":38188},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":38409,\"start\":38400},{\"end\":39391,\"start\":39384},{\"end\":39897,\"start\":39891},{\"end\":43777,\"start\":43771}]", "bib_author_first_name": "[{\"end\":64295,\"start\":64294},{\"end\":64297,\"start\":64296},{\"end\":64688,\"start\":64687},{\"end\":64701,\"start\":64700},{\"end\":64708,\"start\":64707},{\"end\":65040,\"start\":65039},{\"end\":65050,\"start\":65049},{\"end\":65166,\"start\":65165},{\"end\":65175,\"start\":65174},{\"end\":65185,\"start\":65184},{\"end\":65343,\"start\":65342},{\"end\":65523,\"start\":65522},{\"end\":65707,\"start\":65706},{\"end\":65721,\"start\":65720},{\"end\":65893,\"start\":65892},{\"end\":65904,\"start\":65903},{\"end\":65916,\"start\":65915},{\"end\":65926,\"start\":65925},{\"end\":65938,\"start\":65937},{\"end\":66194,\"start\":66193},{\"end\":66202,\"start\":66201},{\"end\":66211,\"start\":66210},{\"end\":66221,\"start\":66220},{\"end\":66415,\"start\":66414},{\"end\":66426,\"start\":66425},{\"end\":66439,\"start\":66438},{\"end\":66683,\"start\":66682},{\"end\":66953,\"start\":66952},{\"end\":67211,\"start\":67210},{\"end\":67416,\"start\":67415},{\"end\":67565,\"start\":67564},{\"end\":67571,\"start\":67570},{\"end\":67578,\"start\":67577},{\"end\":67587,\"start\":67586},{\"end\":67833,\"start\":67832},{\"end\":68050,\"start\":68049},{\"end\":68278,\"start\":68277},{\"end\":68280,\"start\":68279},{\"end\":68459,\"start\":68458},{\"end\":68461,\"start\":68460},{\"end\":68471,\"start\":68470},{\"end\":68473,\"start\":68472},{\"end\":68660,\"start\":68659},{\"end\":68672,\"start\":68671},{\"end\":68683,\"start\":68682},{\"end\":68961,\"start\":68960},{\"end\":68963,\"start\":68962},{\"end\":69171,\"start\":69170},{\"end\":69173,\"start\":69172},{\"end\":69257,\"start\":69256},{\"end\":69269,\"start\":69268},{\"end\":69278,\"start\":69277},{\"end\":69283,\"start\":69279},{\"end\":69462,\"start\":69461},{\"end\":69655,\"start\":69654},{\"end\":69835,\"start\":69834},{\"end\":69845,\"start\":69844},{\"end\":69856,\"start\":69855},{\"end\":69858,\"start\":69857},{\"end\":69867,\"start\":69866},{\"end\":70292,\"start\":70291},{\"end\":70551,\"start\":70550},{\"end\":70817,\"start\":70816},{\"end\":71052,\"start\":71051},{\"end\":71297,\"start\":71296},{\"end\":71308,\"start\":71307},{\"end\":71320,\"start\":71319},{\"end\":71327,\"start\":71326},{\"end\":71628,\"start\":71627},{\"end\":71630,\"start\":71629},{\"end\":71639,\"start\":71638},{\"end\":71985,\"start\":71984},{\"end\":72192,\"start\":72191},{\"end\":72349,\"start\":72348},{\"end\":72351,\"start\":72350},{\"end\":72734,\"start\":72733},{\"end\":72745,\"start\":72744},{\"end\":73060,\"start\":73056},{\"end\":73072,\"start\":73068},{\"end\":73425,\"start\":73424},{\"end\":73437,\"start\":73436},{\"end\":73721,\"start\":73720},{\"end\":73729,\"start\":73728},{\"end\":73736,\"start\":73735},{\"end\":73969,\"start\":73961},{\"end\":74054,\"start\":74046},{\"end\":74239,\"start\":74238},{\"end\":74251,\"start\":74250},{\"end\":74559,\"start\":74558},{\"end\":74776,\"start\":74775},{\"end\":74778,\"start\":74777},{\"end\":74791,\"start\":74787},{\"end\":74800,\"start\":74796},{\"end\":74807,\"start\":74806},{\"end\":74809,\"start\":74808},{\"end\":75056,\"start\":75055},{\"end\":75271,\"start\":75270},{\"end\":75283,\"start\":75282},{\"end\":75293,\"start\":75292},{\"end\":75305,\"start\":75304},{\"end\":75316,\"start\":75315},{\"end\":75567,\"start\":75566},{\"end\":75834,\"start\":75833},{\"end\":75842,\"start\":75841},{\"end\":75852,\"start\":75851},{\"end\":75868,\"start\":75867},{\"end\":75879,\"start\":75878},{\"end\":76176,\"start\":76175},{\"end\":76178,\"start\":76177},{\"end\":76444,\"start\":76443},{\"end\":76707,\"start\":76706},{\"end\":76716,\"start\":76715},{\"end\":76725,\"start\":76724},{\"end\":76737,\"start\":76736},{\"end\":76749,\"start\":76748},{\"end\":76998,\"start\":76997},{\"end\":77004,\"start\":77003},{\"end\":77013,\"start\":77012},{\"end\":77020,\"start\":77019},{\"end\":77338,\"start\":77337},{\"end\":77348,\"start\":77347},{\"end\":77359,\"start\":77358},{\"end\":77559,\"start\":77558},{\"end\":77561,\"start\":77560},{\"end\":77571,\"start\":77570},{\"end\":77711,\"start\":77710},{\"end\":77713,\"start\":77712},{\"end\":77849,\"start\":77848},{\"end\":77851,\"start\":77850},{\"end\":78050,\"start\":78049},{\"end\":78063,\"start\":78062},{\"end\":78069,\"start\":78068},{\"end\":78258,\"start\":78257},{\"end\":78268,\"start\":78267},{\"end\":78270,\"start\":78269},{\"end\":78280,\"start\":78279},{\"end\":78282,\"start\":78281},{\"end\":78727,\"start\":78726}]", "bib_author_last_name": "[{\"end\":64313,\"start\":64298},{\"end\":64326,\"start\":64315},{\"end\":64698,\"start\":64689},{\"end\":64705,\"start\":64702},{\"end\":64717,\"start\":64709},{\"end\":65047,\"start\":65041},{\"end\":65056,\"start\":65051},{\"end\":65172,\"start\":65167},{\"end\":65182,\"start\":65176},{\"end\":65192,\"start\":65186},{\"end\":65351,\"start\":65344},{\"end\":65531,\"start\":65524},{\"end\":65718,\"start\":65708},{\"end\":65733,\"start\":65722},{\"end\":65901,\"start\":65894},{\"end\":65913,\"start\":65905},{\"end\":65923,\"start\":65917},{\"end\":65935,\"start\":65927},{\"end\":65948,\"start\":65939},{\"end\":66199,\"start\":66195},{\"end\":66208,\"start\":66203},{\"end\":66218,\"start\":66212},{\"end\":66230,\"start\":66222},{\"end\":66423,\"start\":66416},{\"end\":66436,\"start\":66427},{\"end\":66446,\"start\":66440},{\"end\":66455,\"start\":66448},{\"end\":66465,\"start\":66457},{\"end\":66688,\"start\":66684},{\"end\":66962,\"start\":66954},{\"end\":67216,\"start\":67212},{\"end\":67423,\"start\":67417},{\"end\":67568,\"start\":67566},{\"end\":67575,\"start\":67572},{\"end\":67584,\"start\":67579},{\"end\":67591,\"start\":67588},{\"end\":67840,\"start\":67834},{\"end\":68059,\"start\":68051},{\"end\":68286,\"start\":68281},{\"end\":68468,\"start\":68462},{\"end\":68481,\"start\":68474},{\"end\":68669,\"start\":68661},{\"end\":68680,\"start\":68673},{\"end\":68690,\"start\":68684},{\"end\":68970,\"start\":68964},{\"end\":69177,\"start\":69174},{\"end\":69266,\"start\":69258},{\"end\":69275,\"start\":69270},{\"end\":69287,\"start\":69284},{\"end\":69293,\"start\":69289},{\"end\":69469,\"start\":69463},{\"end\":69660,\"start\":69656},{\"end\":69842,\"start\":69836},{\"end\":69853,\"start\":69846},{\"end\":69864,\"start\":69859},{\"end\":69875,\"start\":69868},{\"end\":70302,\"start\":70293},{\"end\":70482,\"start\":70464},{\"end\":70556,\"start\":70552},{\"end\":70827,\"start\":70818},{\"end\":71061,\"start\":71053},{\"end\":71305,\"start\":71298},{\"end\":71317,\"start\":71309},{\"end\":71324,\"start\":71321},{\"end\":71336,\"start\":71328},{\"end\":71636,\"start\":71631},{\"end\":71646,\"start\":71640},{\"end\":71990,\"start\":71986},{\"end\":72202,\"start\":72193},{\"end\":72357,\"start\":72352},{\"end\":72364,\"start\":72359},{\"end\":72742,\"start\":72735},{\"end\":72753,\"start\":72746},{\"end\":73066,\"start\":73061},{\"end\":73076,\"start\":73073},{\"end\":73434,\"start\":73426},{\"end\":73446,\"start\":73438},{\"end\":73726,\"start\":73722},{\"end\":73733,\"start\":73730},{\"end\":73741,\"start\":73737},{\"end\":73978,\"start\":73970},{\"end\":74063,\"start\":74055},{\"end\":74248,\"start\":74240},{\"end\":74256,\"start\":74252},{\"end\":74568,\"start\":74560},{\"end\":74785,\"start\":74779},{\"end\":74794,\"start\":74792},{\"end\":74804,\"start\":74801},{\"end\":74813,\"start\":74810},{\"end\":75060,\"start\":75057},{\"end\":75280,\"start\":75272},{\"end\":75290,\"start\":75284},{\"end\":75302,\"start\":75294},{\"end\":75313,\"start\":75306},{\"end\":75323,\"start\":75317},{\"end\":75573,\"start\":75568},{\"end\":75839,\"start\":75835},{\"end\":75849,\"start\":75843},{\"end\":75865,\"start\":75853},{\"end\":75876,\"start\":75869},{\"end\":75886,\"start\":75880},{\"end\":76189,\"start\":76179},{\"end\":76452,\"start\":76445},{\"end\":76713,\"start\":76708},{\"end\":76722,\"start\":76717},{\"end\":76734,\"start\":76726},{\"end\":76746,\"start\":76738},{\"end\":76759,\"start\":76750},{\"end\":77001,\"start\":76999},{\"end\":77010,\"start\":77005},{\"end\":77017,\"start\":77014},{\"end\":77024,\"start\":77021},{\"end\":77345,\"start\":77339},{\"end\":77356,\"start\":77349},{\"end\":77364,\"start\":77360},{\"end\":77568,\"start\":77562},{\"end\":77574,\"start\":77572},{\"end\":77720,\"start\":77714},{\"end\":77856,\"start\":77852},{\"end\":78060,\"start\":78051},{\"end\":78066,\"start\":78064},{\"end\":78083,\"start\":78070},{\"end\":78265,\"start\":78259},{\"end\":78277,\"start\":78271},{\"end\":78288,\"start\":78283},{\"end\":78735,\"start\":78728}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":64531,\"start\":64292},{\"attributes\":{\"id\":\"b1\"},\"end\":64598,\"start\":64533},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":34654401},\"end\":64996,\"start\":64600},{\"attributes\":{\"id\":\"b3\"},\"end\":65148,\"start\":64998},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1779661},\"end\":65282,\"start\":65150},{\"attributes\":{\"id\":\"b5\"},\"end\":65493,\"start\":65284},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":13756489},\"end\":65680,\"start\":65495},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1915014},\"end\":65843,\"start\":65682},{\"attributes\":{\"doi\":\"INTERSPEECH-2010 1045-1048\",\"id\":\"b8\"},\"end\":66125,\"start\":65845},{\"attributes\":{\"id\":\"b9\"},\"end\":66410,\"start\":66127},{\"attributes\":{\"id\":\"b10\"},\"end\":66626,\"start\":66412},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6875312},\"end\":66859,\"start\":66628},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":3645060},\"end\":67154,\"start\":66861},{\"attributes\":{\"id\":\"b13\"},\"end\":67354,\"start\":67156},{\"attributes\":{\"id\":\"b14\"},\"end\":67537,\"start\":67356},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":49211075},\"end\":67734,\"start\":67539},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":54457125},\"end\":67998,\"start\":67736},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":59222682},\"end\":68227,\"start\":68000},{\"attributes\":{\"id\":\"b18\"},\"end\":68418,\"start\":68229},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":15005894},\"end\":68611,\"start\":68420},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":13937012},\"end\":68894,\"start\":68613},{\"attributes\":{\"id\":\"b21\"},\"end\":69122,\"start\":68896},{\"attributes\":{\"id\":\"b22\"},\"end\":69252,\"start\":69124},{\"attributes\":{\"id\":\"b23\"},\"end\":69391,\"start\":69254},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":515925},\"end\":69595,\"start\":69393},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":205242740},\"end\":69773,\"start\":69597},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":20045336},\"end\":70196,\"start\":69775},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":49561741},\"end\":70460,\"start\":70198},{\"attributes\":{\"id\":\"b28\"},\"end\":70548,\"start\":70462},{\"attributes\":{\"id\":\"b29\"},\"end\":70777,\"start\":70550},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":60440549},\"end\":71009,\"start\":70779},{\"attributes\":{\"id\":\"b31\"},\"end\":71189,\"start\":71011},{\"attributes\":{\"id\":\"b32\"},\"end\":71559,\"start\":71191},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":6870456},\"end\":71942,\"start\":71561},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":206722020},\"end\":72137,\"start\":71944},{\"attributes\":{\"id\":\"b35\"},\"end\":72323,\"start\":72139},{\"attributes\":{\"id\":\"b36\"},\"end\":72556,\"start\":72325},{\"attributes\":{\"id\":\"b37\"},\"end\":72971,\"start\":72558},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":8192090},\"end\":73281,\"start\":72973},{\"attributes\":{\"id\":\"b39\"},\"end\":73630,\"start\":73283},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":4559747},\"end\":73957,\"start\":73632},{\"attributes\":{\"id\":\"b41\"},\"end\":74042,\"start\":73959},{\"attributes\":{\"id\":\"b42\"},\"end\":74164,\"start\":74044},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":30805},\"end\":74478,\"start\":74166},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":54083409},\"end\":74771,\"start\":74480},{\"attributes\":{\"id\":\"b45\"},\"end\":74968,\"start\":74773},{\"attributes\":{\"id\":\"b46\"},\"end\":75227,\"start\":74970},{\"attributes\":{\"id\":\"b47\"},\"end\":75496,\"start\":75229},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":53424488},\"end\":75761,\"start\":75498},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":3543784},\"end\":76121,\"start\":75763},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":4787508},\"end\":76369,\"start\":76123},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":9072400},\"end\":76646,\"start\":76371},{\"attributes\":{\"id\":\"b52\"},\"end\":76949,\"start\":76648},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":206594692},\"end\":77289,\"start\":76951},{\"attributes\":{\"id\":\"b54\"},\"end\":77512,\"start\":77291},{\"attributes\":{\"id\":\"b55\"},\"end\":77708,\"start\":77514},{\"attributes\":{\"id\":\"b56\"},\"end\":77844,\"start\":77710},{\"attributes\":{\"id\":\"b57\"},\"end\":77982,\"start\":77846},{\"attributes\":{\"id\":\"b58\"},\"end\":78255,\"start\":77984},{\"attributes\":{\"id\":\"b59\"},\"end\":78549,\"start\":78257},{\"attributes\":{\"id\":\"b60\"},\"end\":78661,\"start\":78551},{\"attributes\":{\"id\":\"b61\"},\"end\":78935,\"start\":78663}]", "bib_title": "[{\"end\":64685,\"start\":64600},{\"end\":65163,\"start\":65150},{\"end\":65520,\"start\":65495},{\"end\":65704,\"start\":65682},{\"end\":66680,\"start\":66628},{\"end\":66950,\"start\":66861},{\"end\":67413,\"start\":67356},{\"end\":67562,\"start\":67539},{\"end\":67830,\"start\":67736},{\"end\":68047,\"start\":68000},{\"end\":68275,\"start\":68229},{\"end\":68456,\"start\":68420},{\"end\":68657,\"start\":68613},{\"end\":69459,\"start\":69393},{\"end\":69652,\"start\":69597},{\"end\":69832,\"start\":69775},{\"end\":70289,\"start\":70198},{\"end\":70814,\"start\":70779},{\"end\":71625,\"start\":71561},{\"end\":71982,\"start\":71944},{\"end\":72346,\"start\":72325},{\"end\":73054,\"start\":72973},{\"end\":73718,\"start\":73632},{\"end\":74236,\"start\":74166},{\"end\":74556,\"start\":74480},{\"end\":75564,\"start\":75498},{\"end\":75831,\"start\":75763},{\"end\":76173,\"start\":76123},{\"end\":76441,\"start\":76371},{\"end\":76995,\"start\":76951}]", "bib_author": "[{\"end\":64315,\"start\":64294},{\"end\":64328,\"start\":64315},{\"end\":64700,\"start\":64687},{\"end\":64707,\"start\":64700},{\"end\":64719,\"start\":64707},{\"end\":65049,\"start\":65039},{\"end\":65058,\"start\":65049},{\"end\":65174,\"start\":65165},{\"end\":65184,\"start\":65174},{\"end\":65194,\"start\":65184},{\"end\":65353,\"start\":65342},{\"end\":65533,\"start\":65522},{\"end\":65720,\"start\":65706},{\"end\":65735,\"start\":65720},{\"end\":65903,\"start\":65892},{\"end\":65915,\"start\":65903},{\"end\":65925,\"start\":65915},{\"end\":65937,\"start\":65925},{\"end\":65950,\"start\":65937},{\"end\":66201,\"start\":66193},{\"end\":66210,\"start\":66201},{\"end\":66220,\"start\":66210},{\"end\":66232,\"start\":66220},{\"end\":66425,\"start\":66414},{\"end\":66438,\"start\":66425},{\"end\":66448,\"start\":66438},{\"end\":66457,\"start\":66448},{\"end\":66467,\"start\":66457},{\"end\":66690,\"start\":66682},{\"end\":66964,\"start\":66952},{\"end\":67218,\"start\":67210},{\"end\":67425,\"start\":67415},{\"end\":67570,\"start\":67564},{\"end\":67577,\"start\":67570},{\"end\":67586,\"start\":67577},{\"end\":67593,\"start\":67586},{\"end\":67842,\"start\":67832},{\"end\":68061,\"start\":68049},{\"end\":68288,\"start\":68277},{\"end\":68470,\"start\":68458},{\"end\":68483,\"start\":68470},{\"end\":68671,\"start\":68659},{\"end\":68682,\"start\":68671},{\"end\":68692,\"start\":68682},{\"end\":68972,\"start\":68960},{\"end\":69179,\"start\":69170},{\"end\":69268,\"start\":69256},{\"end\":69277,\"start\":69268},{\"end\":69289,\"start\":69277},{\"end\":69295,\"start\":69289},{\"end\":69471,\"start\":69461},{\"end\":69662,\"start\":69654},{\"end\":69844,\"start\":69834},{\"end\":69855,\"start\":69844},{\"end\":69866,\"start\":69855},{\"end\":69877,\"start\":69866},{\"end\":70304,\"start\":70291},{\"end\":70484,\"start\":70464},{\"end\":70558,\"start\":70550},{\"end\":70829,\"start\":70816},{\"end\":71063,\"start\":71051},{\"end\":71307,\"start\":71296},{\"end\":71319,\"start\":71307},{\"end\":71326,\"start\":71319},{\"end\":71338,\"start\":71326},{\"end\":71638,\"start\":71627},{\"end\":71648,\"start\":71638},{\"end\":71992,\"start\":71984},{\"end\":72204,\"start\":72191},{\"end\":72359,\"start\":72348},{\"end\":72366,\"start\":72359},{\"end\":72744,\"start\":72733},{\"end\":72755,\"start\":72744},{\"end\":73068,\"start\":73056},{\"end\":73078,\"start\":73068},{\"end\":73436,\"start\":73424},{\"end\":73448,\"start\":73436},{\"end\":73728,\"start\":73720},{\"end\":73735,\"start\":73728},{\"end\":73743,\"start\":73735},{\"end\":73980,\"start\":73961},{\"end\":74065,\"start\":74046},{\"end\":74250,\"start\":74238},{\"end\":74258,\"start\":74250},{\"end\":74570,\"start\":74558},{\"end\":74787,\"start\":74775},{\"end\":74796,\"start\":74787},{\"end\":74806,\"start\":74796},{\"end\":74815,\"start\":74806},{\"end\":75062,\"start\":75055},{\"end\":75282,\"start\":75270},{\"end\":75292,\"start\":75282},{\"end\":75304,\"start\":75292},{\"end\":75315,\"start\":75304},{\"end\":75325,\"start\":75315},{\"end\":75575,\"start\":75566},{\"end\":75841,\"start\":75833},{\"end\":75851,\"start\":75841},{\"end\":75867,\"start\":75851},{\"end\":75878,\"start\":75867},{\"end\":75888,\"start\":75878},{\"end\":76191,\"start\":76175},{\"end\":76454,\"start\":76443},{\"end\":76715,\"start\":76706},{\"end\":76724,\"start\":76715},{\"end\":76736,\"start\":76724},{\"end\":76748,\"start\":76736},{\"end\":76761,\"start\":76748},{\"end\":77003,\"start\":76997},{\"end\":77012,\"start\":77003},{\"end\":77019,\"start\":77012},{\"end\":77026,\"start\":77019},{\"end\":77347,\"start\":77337},{\"end\":77358,\"start\":77347},{\"end\":77366,\"start\":77358},{\"end\":77570,\"start\":77558},{\"end\":77576,\"start\":77570},{\"end\":77722,\"start\":77710},{\"end\":77858,\"start\":77848},{\"end\":78062,\"start\":78049},{\"end\":78068,\"start\":78062},{\"end\":78085,\"start\":78068},{\"end\":78267,\"start\":78257},{\"end\":78279,\"start\":78267},{\"end\":78290,\"start\":78279},{\"end\":78737,\"start\":78726}]", "bib_venue": "[{\"end\":66738,\"start\":66718},{\"end\":68109,\"start\":68089},{\"end\":68754,\"start\":68727},{\"end\":69997,\"start\":69941},{\"end\":77134,\"start\":77084},{\"end\":64370,\"start\":64328},{\"end\":64554,\"start\":64533},{\"end\":64785,\"start\":64719},{\"end\":65037,\"start\":64998},{\"end\":65200,\"start\":65194},{\"end\":65340,\"start\":65284},{\"end\":65570,\"start\":65533},{\"end\":65748,\"start\":65735},{\"end\":65890,\"start\":65845},{\"end\":66191,\"start\":66127},{\"end\":66504,\"start\":66467},{\"end\":66716,\"start\":66690},{\"end\":66990,\"start\":66964},{\"end\":67208,\"start\":67156},{\"end\":67436,\"start\":67425},{\"end\":67619,\"start\":67593},{\"end\":67849,\"start\":67842},{\"end\":68087,\"start\":68061},{\"end\":68310,\"start\":68288},{\"end\":68500,\"start\":68483},{\"end\":68725,\"start\":68692},{\"end\":68958,\"start\":68896},{\"end\":69168,\"start\":69124},{\"end\":69308,\"start\":69295},{\"end\":69477,\"start\":69471},{\"end\":69668,\"start\":69662},{\"end\":69939,\"start\":69877},{\"end\":70311,\"start\":70304},{\"end\":70653,\"start\":70558},{\"end\":70880,\"start\":70829},{\"end\":71049,\"start\":71011},{\"end\":71294,\"start\":71191},{\"end\":71740,\"start\":71648},{\"end\":72022,\"start\":71992},{\"end\":72189,\"start\":72139},{\"end\":72432,\"start\":72366},{\"end\":72731,\"start\":72558},{\"end\":73115,\"start\":73078},{\"end\":73422,\"start\":73283},{\"end\":73781,\"start\":73743},{\"end\":74311,\"start\":74258},{\"end\":74607,\"start\":74570},{\"end\":74854,\"start\":74815},{\"end\":75053,\"start\":74970},{\"end\":75268,\"start\":75229},{\"end\":75612,\"start\":75575},{\"end\":75927,\"start\":75888},{\"end\":76228,\"start\":76191},{\"end\":76491,\"start\":76454},{\"end\":76704,\"start\":76648},{\"end\":77082,\"start\":77026},{\"end\":77335,\"start\":77291},{\"end\":77556,\"start\":77514},{\"end\":77762,\"start\":77722},{\"end\":78047,\"start\":77984},{\"end\":78390,\"start\":78290},{\"end\":78580,\"start\":78553},{\"end\":78724,\"start\":78663}]"}}}, "year": 2023, "month": 12, "day": 17}