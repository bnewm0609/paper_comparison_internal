{"id": 220069708, "updated": "2023-10-06 13:38:14.008", "metadata": {"title": "A Motion Taxonomy for Manipulation Embedding", "authors": "[{\"first\":\"David\",\"last\":\"Paulius\",\"middle\":[]},{\"first\":\"Nicholas\",\"last\":\"Eales\",\"middle\":[]},{\"first\":\"Yu\",\"last\":\"Sun\",\"middle\":[]}]", "venue": "Proceedings of Robotics: Science and Systems 2020", "journal": null, "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "To represent motions from a mechanical point of view, this paper explores motion embedding using the motion taxonomy. With this taxonomy, manipulations can be described and represented as binary strings called motion codes. Motion codes capture mechanical properties, such as contact type and trajectory, that should be used to define suitable distance metrics between motions or loss functions for deep learning and reinforcement learning. Motion codes can also be used to consolidate aliases or cluster motion types that share similar properties. Using existing data sets as a reference, we discuss how motion codes can be created and assigned to actions that are commonly seen in activities of daily living based on intuition as well as real data. Motion codes are compared to vectors from pre-trained Word2Vec models, and we show that motion codes maintain distances that closely match the reality of manipulation.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2007.06695", "mag": "3100118710", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/rss/PauliusE020", "doi": "10.15607/rss.2020.xvi.045"}}, "content": {"source": {"pdf_hash": "aaa9745eb8898772e1e47a3212e11ab382eaa2c0", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2007.06695v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://doi.org/10.15607/rss.2020.xvi.045", "status": "BRONZE"}}, "grobid": {"id": "9ac8a53d7312b3efab020e37c8f9645d655ea430", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/aaa9745eb8898772e1e47a3212e11ab382eaa2c0.txt", "contents": "\nA Motion Taxonomy for Manipulation Embedding\n\n\nDavid Paulius \nNicholas Eales \nYu Sun \nA Motion Taxonomy for Manipulation Embedding\n\nTo represent motions from a mechanical point of view, this paper explores motion embedding using the motion taxonomy. With this taxonomy, manipulations can be described and represented as binary strings called motion codes. Motion codes capture mechanical properties, such as contact type and trajectory, that should be used to define suitable distance metrics between motions or loss functions for deep learning and reinforcement learning. Motion codes can also be used to consolidate aliases or cluster motion types that share similar properties. Using existing data sets as a reference, we discuss how motion codes can be created and assigned to actions that are commonly seen in activities of daily living based on intuition as well as real data. Motion codes are compared to vectors from pre-trained Word2Vec models, and we show that motion codes maintain distances that closely match the reality of manipulation.\n\nI. INTRODUCTION\n\nIn robotics and AI, motion recognition is a crucial component to the understanding of the intent of humans and learning manipulations directly from demonstration. In learning to recognize manipulations in activities of daily living (ADL), it is important to properly define motions or actions for use in classifiers. However, it is very difficult to appropriately define or describe motions -which we describe in human language in words -in a way that is understood by robots.\n\nTypically, motion recognition is achieved using classifiers, such as neural networks, to detect actions in sequences of events. Networks are conventionally trained using one-hot vectors (for each motion class) for motion recognition; however, distances between motions -i.e. distinguishing what makes one motion different to another -are not innately measurable with such vectors. Instead, word embedding allows us to obtain a better vectorized representation of human language describing those actions, which can then be used to draw meaningful relationships in a high-dimensional space. Essentially, words can be measured against one another for a variety of tasks, and they have been applied to affordance learning and grounding [1], [2], [3]. One popular approach to learn embedding directly from natural language is Word2Vec [4], [5]. However, the major drawback to Word2Vec, when applied in classifiers for motion recognition and analysis, is that vectors have no innate sense of mechanics or functionality required to execute those motions since they are trained on text. A similar argument is made in other works that also consider embedding derived directly from manipulations over Word2Vec [2]. To elaborate further, with Word2Vec embedding, we cannot explain the David Paulius, Nicholas Eales, and Yu Sun are with the Department of Computer Science and Engineering at the University of South Florida, Tampa, FL, USA. They are all members of the Robot Perception and Action Lab (RPAL). Nicholas was an undergraduate student when this work was done. (Contact email: {davidpaulius,yusun}@usf.edu) difference between two types of actions, which may or may not share overlapping features, as distances between vectors are not functionally valid. For instance, let us consider the labels 'pour', 'tap', and 'poke'; when comparing them in pairs, with a pre-trained Word2Vec from Wikipedia [6], the labels 'pour' and 'tap' are closer to one another than 'tap' and 'poke', where the latter is considered to be mechanically closer. Furthermore, Word2Vec embedding cannot capture multiple meanings of words. In the prior example, the label 'tap' can also refer to the noun for a source of water, which possibly explains why it is deemed more similar to 'pour' than to 'poke'.\n\nOur objective in this paper is to introduce a suitable embedding of manipulations that considers mechanics of manipulations (from the point of view of the robot) for measuring distances. Such a representation from the viewpoint of robots is important for robot learning and understanding [7] With suitable distance metrics, motion classifiers can better discern between motion types or, in the event of uncertainty, suggest similar yet accurate labels for activity understanding. We coined the term motion taxonomy for this representation [8]. With regards to existing taxonomies in robotics, grasp taxonomies have proved to be extremely useful in robotic grasp planning and in defining grasping strategies [9], [10], [11], [12], [13], [14], [15], [16]. These studies further explored the dichotomy between power and precision grasps based on the way fingers secure objects contained within the hand. However, there are no taxonomies that primarily focus on the mechanics of manipulation motions -specifically contact and trajectory for each end-effector -for defining motions and suitable metrics for motion types. Such a taxonomy can also be used for consolidating motion aliases (i.e. words or expressions in human language) by representing them as binary codes, which may help to enforce grasp taxonomies for learning from demonstration. This taxonomy can be used in a deep neural network that takes a manipulation sequence as input and outputs a representation of the motion type in the form of a binary-encoded string or code. These codes can potentially be used for motion recognition, analysis, and generation. In terms of robotic manipulation, using binary codes as word vectors for motions is better than word embedding from natural language like Word2Vec. Furthermore, the taxonomy can be used in manipulation learning to identify skills that can be extended to other motions and to facilitate affordance learning similar to prior work [17], [18], [19], [20]. In this paper, we show how this taxonomy can be used to define a representation that properly defines distances by comparing projections of various motions from the taxonomical embedding of labels to those from existing pre-trained Word2Vec models [4], [6], [21].\n\n\nII. EXAMINING MOTION CODES\n\nIn this section, we describe the various attributes that are used to represent manipulations as motion codes using the motion taxonomy. Briefly, the purpose of this taxonomy is to translate manipulations into a machine language for the purpose of motion recognition, analysis and generation. Here, we define a manipulation motion to be any atomic action between active and passive objects; an active object is defined as a tool or utensil (or the combination of a robot's gripper or human hand and tool) that acts upon passive objects, which are objects that are acted upon as a result of motion. Motions can be effectively embedded as vectors that relates them to motion feature space. Motions labelled with motion codes avoid ambiguity from aliases for motion types, especially when translating between human languages.\n\n\nA. A Case for the Motion Taxonomy\n\nDeriving a representation of motions using the motion taxonomy was partially inspired by our own experiences with annotating labels for robot knowledge. We have observed that among several annotators, inconsistency of labelling and defining motions was prevalent. This happens especially with certain motion types that are hard to discern (such as deciding between the labels 'cut', 'slice' or 'chop'), which requires revisiting all labels given to videos to ensure consistency. Furthermore, this is also a problem encountered when using annotated data sets such as the MPII Cooking Activities Dataset [22] or EPIC-KITCHENS [23] since they may have their own labels that may not overlap with each other. In some cases, labels can be very ambiguous and could be better described when adopting data sets for affordance learning For instance, in EPIC-KITCHENS, one verb class provided is 'adjust', which turns out to encompass several actions such as tapping, poking, pressing or rotating depending on types of switches; another example is the 'insert' class, which encompasses actions such as pouring to picking-and-placing.\n\nTo potentially resolve these issues, we propose a representation scheme that deviates from natural language since an effective representation is important for robot learning [7]. Binary-encoded strings called motion codes will inherently define motions based on key traits defined in the taxonomy. Ambiguity in human language labels or classes can be better handled if we represent them in an attribute space, especially if these can be automatically obtained from demonstration. Different from this representation, neural networks that are used for motion recognition typically encode motion labels using one-hot vectors. When training such networks, we use the cross entropy loss function, which is defined as:\nL = \u2212 N k=1 x k logx k\nwhere N is the total number of classes, x k is the ground-truth distribution, andx k is the predicted distribution. For instance, if we have three labels 'pour', 'sprinkle', and 'cut', these may be encoded with vectors [1, 0, 0], [0, 1, 0], and [0, 0, 1] respectively; during the prediction stage, we can predict the label for a given manipulation sequence with the highest confidence using this equation. Since cross entropy is used to determine how close predicted distributions are to the actual distribution using one-hot vectors, distances between classes would not matter since one-hot vectors are equidistant from one another. Although we can consider this as a distance metric between probabilities, this does not consider class features that can provide a better label for class instances. Following the prior example, we do not get a sense of similarity between motions: pouring and sprinkling can be considered as closer motions than to cutting in terms of manipulation mechanics.\n\nWith Word2Vec embeddings [4], cosine distances between vectors suggest relatedness between word labels, where relatedness is determined by context. These models are trained either using continuous bag-of-words (CBOW), n-grams or skip-grams to identify word pairs that are frequently used or seen together. However, these vectors do not explicitly define why motions differ, which is one key purpose of motion codes; since Word2Vec derives vectors for singular words, we also can run into issues when defining variations of motions. For example, pushing a solid or rigid object is mechanically different to pushing a soft object since the object we are pushing changes in shape, but we cannot represent these variations with Word2Vec. With motion codes, we can be more descriptive with our characterization of motions. It is important to note that the proposed motion taxonomy is not claimed to be the ideal way of representing motions; rather, it can be used to tentatively reduce the amount of features needed to label and compute meaningful distances between motions.\n\n\nB. Examining Characteristics of Motion Codes\n\nThe mechanics of motions can be broken down into contact, force and trajectory. Hence, our taxonomy considers the following attributes based on contact and trajectory information: contact interaction type, engagement type, contact duration, trajectory type and motion recurrence. Motion codes also indicate whether the active object is solely a hand/gripper or if it is a combination of a hand or tool. Motion codes can be defined for each end-effector used in the manipulation. When considering contact, we examine whether objects used in the manipulation make contact with one another and we describe what happens to these objects when this contact is established. These features, shown in Figure 1, are further described below.\n\n\n1) Describing Contact Type and Features:\n\nMotion types can be classified as contact or non-contact interaction types. Contact motion types are those that require contact between an active object (i.e. the actor's hands or the object that is typically grasped in the actor's hands) and passive object(s) (i.e. the object(s) that is/are manipulated upon when in contact with an active object) in the work space. As opposed to the taxonomy found in [8], we may consider the hand or endeffector as a tool. Conversely, non-contact motion types are those where no contact is established between active and passive objects or there is no force exerted upon passive   Fig. 1: Illustration of the hierarchy of attributes in the motion taxonomy. A motion code is formed by appending contact features, the active object's structural bits, the passive object's structural bits, the active trajectory and passive trajectory bits, and a bit descriptor of the active object by following the tree.\n\nobjects. Contact can be observed with vision (for instance, by the objects' borders or bounding boxes overlapping) or using force sensors mounted on objects. An example of a contact motion is mixing, where the active tool makes contact with contents within a passive container. As for a non-contact motion, pouring is a prime example: when pouring from one container to another, the active container held in the hand is not required to make contact with the passive receiving container.\n\nOnce an object interacts with another through physical contact, we classify their engagement as either rigid or soft. Rigid engagement is where an active object's interaction with passive objects does not result in deformation -i.e. their structure is not compromised or changed -, whereas soft engagement is where objects deform as a result of the interaction or the objects allow admittance or are permeable. Furthermore, we can also consider the structural integrity (or state) of the objects used in order to describe deformation. Active and passive objects can either undergo no deformation (non-deforming) or structural change (deforming). We consider the cutting action as a soft engagement motion, as an active knife object will permanently deform the passive object into smaller pieces or units; even in the action of mixing items within a bowl, the contents within the bowl can be regarded as the passive objects being acted upon and deformed. As for a rigid motion, actions such as tapping or poking a solid object show no structural change among objects as a result of contact. In spreading with a knife, neither the knife nor the surface (like bread) incurs a significant change in their shape. Deformation can be further distinguished as temporary or permanent, which is attributed to an object's material or texture. For instance, when we squeeze a passive sponge object, it returns to its original shape, signifying that this motion temporarily deforms it. However, in the cutting example from before, this state change is permanent. Poking or tapping an object could be classified as soft engagement if we engage an elastic object.\n\nIn addition to the prior attributes, it may be useful to note if the active tool has persistent contact with passive objects. If the actor only makes contact for a short duration in the manipulation, we consider that contact to be discontinuous; however, if the contact between the active tool and passive object is persistent, we consider that contact to be continuous. However, this perspective changes depending on what is considered to be the active object. If we consider the robot's hand to be the active tool only, then we can assume that once it is grasping a tool for manipulation, there would be continuous contact between the hand and the tool. This is why we consider the active object to be either the hand (if no tool acts upon other objects) or both the hand and tool as a unit (if there are other objects in the manipulation). Contact duration can be determined visually (by timing the overlap of bounding boxes, for instance) or physically with force sensors. For the sake of this work, we rely on demonstration videos to identify contact duration by intuition.\n\n2) Describing Trajectory of Motion: We can describe an object's trajectory as prismatic (or translational), revolute (or rotational), or both. Previously, we solely encoded the active object's observed trajectory in motion codes [8], but in this work, we have included the passive object's trajectory. Prismatic motions are manipulations where the object is moved along a certain axis or plane of translation. Prismatic motions can be 1-dimensional (along a single axis), 2-dimensional (confined to a plane) or 3-dimensional (confined to a manifold space); this can be interpreted as having 1 to 3 DOF of translation. Revolute motions, on the other hand, are manipulations where the object is rotated about an axis or plane of rotation; a robot performing such motions would rely on revolute joints to execute manipulations of this nature. Similar to prismatic motions, revolute motions can also range from 1dimensional to 3-dimensional motion (i.e. from 1 to 3 DOF of rotation); typically, revolute motions are confined to a single axis of rotation in world space. A motion is not limited to one trajectory type, as these properties are not mutually exclusive; therefore, we can say that a motion can be prismatic-only,  Figure  1. The attributes of each motion correspond to those observed in source demonstrations. These codes are best viewed in colour, as each binary bit is colour-coded based on Figure  1. Motion codes are 17 bits long. Underlined substrings correspond to the active object.\n\n\nMotion Code\n\nMotion Types revolute-only, neither prismatic nor revolute or both prismatic and revolute. From the perspective of the active object, an example of a prismatic-only manipulation is chopping with a knife since the knife's orientation is usually fixed, while an example of a revolute-only motion is fastening a screw into a surface using a screwdriver. However, a motion such as scooping with a spoon will usually require both prismatic and revolute movements to complete the action. We can also describe a motion's trajectory by its recurrence, which describes whether the motion exhibits repetitive behaviour in the tool's movement. A motion can be acyclical or cyclical, which may be useful depending on the context of motion. For instance, mixing ingredients in a bowl may be repeated until the ingredients have fully blended together, or in the case of loosening a screw, the screwdriver will be rotated until the screw is completely out of the surface. Learned acyclical motions can be made cyclical simply by repeating them, which is a decision that can be left up to the robot during motion generation if it is not finished with its task or it failed to execute the manipulation successfully.\n\n\nC. Translating Motions to Code\n\nWe now discuss how motion codes can be assigned to motions using the example of the cutting action. Using the flowchart shown as Figure 1, we construct codes in the follow-ing manner: first, we ascertain whether the motion is contact or non-contact. In cutting, the active knife object makes contact with the passive object, and so we will follow the contact branch. If the motion was better described as non-contact, then we will start with the string '000'. Since there is contact, we then describe the type of engagement between the objects and how long the contact duration is throughout the action. Following our example, the knife cuts through an object and maintains contact with it for the entirety of the manipulation, hence making it a soft engagement ('11') and with continuous contact ('1'). After describing contact, we describe the state of the active and passive objects after the manipulation occurs. In our example, the active object does not deform ('00') while the passive object deforms permanently since the knife cuts it into a different state ('11'). After describing the structural integrity of the objects, we then describe their trajectories. When cutting an object, the active trajectory is typically a 1D prismatic motion as we swing the knife up and down and without any rotation ('00100'), while there is no passive trajectory ('00000'), as the passive object is usually immobile. If we are observing repetition in cutting, then we would assign the recurrent bit '1' instead of '0' in the active trajectory substring. Finally, we indicate whether the active object is solely the hand or hand/tool pair; in our example, we would assign it a bit of '1' since we have a hand and knife pairing as an active object. With all of these substrings, we end up with the single motion code '11100110010000001'.\n\nWe compiled a list of motion labels that can be found across several sources of manipulation data such as EPIC-KITCHENS, MPII Cooking Activities, FOON [24], [25] (inspired by our prior work on object affordance for robot manipulation [26], [27]), and Daily Interactive Manipulations (DIM) [28], and we show their respective codes in Table I. Several motions can share the same motion code due to common mechanics, such as cutting and peeling since they are both 1D-prismatic motions that permanently deform the passive objects. We can also account for variations in manipulation; for instance, certain motions like mixing and stirring can either temporarily deform or permanently deform the target passive object, which depends on its state of matter, or we can identify non-recurrent or recurrent variants of motion. It is important to note that motion codes can be assigned to each hand or end-effector used in a manipulation since they are not necessary to perform the same manipulation in the same action. For instance, when chopping items, usually it is necessary to hold the object in place with one hand and then use the knife to chop with the other. Because of this, the structural or state outcome of performing those actions could be extrinsic to the actions; in the aforementioned example, the passive object deforms but it is not directly an outcome of just holding the object. In Table I, we simplify this to the single-handed perspective of performing those actions.\n\n\nD. Obtaining Motion Codes from Demonstration\n\nIdeally, a neural network (or a collection of networks for a group of attributes) can be developed to output codes for  Fig. 2: An illustration of adapter used for data collection in [28] (best viewed in colour). The Patriot sensor (depicted in blue) records position and orientation, while the ATI Mini40 sensor (depicted in green) records force and torque. They are aligned to the world frame (depicted in purple) for analysis. different motion types. In detail, such a network structure would assign motion codes to a series of segmented actions as seen in demonstration videos; rather than learning to detect and output a single motion code, an ensemble of classifiers that can separately identify parts of the hierarchy in Figure 1 can be used to build substrings that could then be appended together as a single, representative string. As a result of such a network structure, one could also obtain motion features that may facilitate motion recognition tasks.\n\nRepresenting manipulations in an attribute space as motion codes can be likened to the idea behind zero-shot learning (ZSL); just as in ZSL, even if certain class instances are not known, motion code vectors can be used as a guide to assign codes to unknown manipulations and to possibly learn new actions, granted that we know how to execute similar actions.\n\n\nIII. EVALUATION OF THE TAXONOMY\n\nHaving understood the taxonomy and identified motion codes for manipulations in ADL, we demonstrate how suitable they are for representing motion labels. In particular, we focus on how motion codes can produce embeddings whose distances are meaningful based on their attributes. Our evaluation is done in two parts: first, we show how the motion code assignment corresponds to actual data. Second, we contrast motion codes to the unsupervised word embedding method Word2Vec [4], [5], which learns vectorized representations of words directly from natural language, to show that it is not suitable to derive accurate motion embedding. We used pre-trained models trained on Google News [5], [4], Wikipedia [6], and Concept-Net [21]; although these are not solely trained with knowledge sources of manipulations nor physical properties, these models are commonly used for deep learning tasks in robotics, AI, and computer vision.\n\n\nA. Support for Motion Codes\n\nPreferably, motion codes are derived directly from demonstration data. Several modalities of data such as trajectory, force, and vision can be used to determine the attributes that best describe said manipulations. Using provided position and orientation data, which is available in data sets such as DIM [28], we can ascertain the trajectory type for several motions in which there is an active tool or object being manipulated.  Figure 3a, the trajectory's points lie on a plane, thus suggesting that this is a 2D prismatic motion. In Figure 3b, which shows a histogram of the number of velocity vectors and their similarity to each PC, it further supports that the motion primarily lies in PCs 1 and 2 (capturing \u223c99% of variance). It can also be observed from the projection that this trajectory is recurrent since the motion is cyclical.\n\nTo determine the prismatic trajectory type, we can use methods such as principal component analysis (PCA) to find the number of axes (which would be transformed into principal components, or PCs) that captures the most variance of the trajectory. PCA has conventionally been used for feature extraction and dimension reduction, where one can obtain a subset of features (i.e. DOF) that will sufficiently capture data. Here, we considered that the number of DOF for a motion is reflected by the number of PCs that would capture about 90% of variance. Motions such as flipping with a turner are effectively 1D (and in minor cases 2D) motions because a single PC captures about 90% of the variance of those trials. Mixing, beating and stirring (which are all variations of the same motion) data confirm that the motion is 2D since both the 1st and 2nd PCs met our requirements; this can be observed in the projection shown as Figure 3. One can compare the derived PCs to the velocity (i.e. directional vectors between trajectory frames) to also clarify whether or not motions exist within those dimensions using cosine similarity. Should the velocity vectors align with the PCs, we would expect values closer to 0 \u2022 or 180 \u2022 . In Figure 3b, not only does the 3rd PC contribute very little to capture the motion, but it is normal to velocity (since the histogram shows a prevalence of vectors with cosine similarity peaking around 90 \u2022 ).  Figure 4a, based on Figure 2, we show how the axis vector K (in blue), which is obtained from rotation matrices [29], aligns with the tool's principal axis y (in red) at each trajectory point. This is further supported by Figure 4b, which compares each frame's axis K to the tool's principal axes with cosine similarity. In Figure 4c, we graph the change in rotation about each axis with respect to the last frame's orientation. Figures 4b and 4c suggest rotation about the y-axis, hence making it a 1D revolute motion.\n\nTo determine the revolute trajectory type, we can convert the position and orientation data to rotation matrices and measure the amount of rotation about the principal axis of the active tool. The axis-angle representation (which represents a frame as a vector K and an angle of rotation \u03b8) derived from rotation matrices can also be used to compute the angle of rotation based on \u03b8. A significant rotation about this principal axis suggests that there is at least one axis of rotation. In Figure  4, we illustrate how we can extract revolute properties for the motion of loosening a screw. Given the tool's principal axes are defined as in Figure 2, we expect that the operation of a screwdriver would require major rotation about the y-axis. In Figure 4a, one can see that the axis vector K is predominantly pointing in the opposite direction of the tool's axis (which is also supported by Figure 4b, which shows that the cosine similarity values peak at 0 \u2022 or 180 \u2022 ), suggesting that there is anti-clockwise (or negative) rotation. Rotation about this axis is further supported by Figures 4b and 4c.\n\n\nB. Comparing Motion Codes to Word2Vec Embedding\n\nTo show how motion codes produce more ideal distances between motion types, we show how vectors from Word2Vec, which are derived from natural language, are not sufficient to represent manipulations for classification. As mentioned before, Word2Vec is an unsupervised method to derive multidimensional vectors for words in natural language processing tasks with neural networks. Typically, all words in a vocabulary are initialized as random vectors, whose distances are incrementally adjusted with respect to other word vectors. Words are related based on locality; that is to say, if one word is frequently seen among neighbours of that word in source text, then its vector along with its neighbouring words' vectors will be closer to one another than those vectors representing other words in the vocabulary.\n\nTo compare motion codes to Word2Vec embeddings, we applied dimension reduction with PCA and then used t-SNE [30] to visualize these embeddings and their relative distances in 2D. The t-SNE algorithm (short for t-distributed Stochastic Neighbor Embedding) is an approach that is often used to visualize high-dimensional representations or embedding, such as word vectors from Word2Vec, in a low-dimensional space. Although certain motions will be assigned the same code, the t-SNE algorithm will position their projected vectors in close yet non-overlapping positions; similar motions would be clustered near each other since t-SNE preserves local neighbours while keeping dissimilar motions far from one another. By default, the distances between the naturally occurring clusters are set further apart than by default, which is reflected by an early exaggeration value of 36 (as opposed to 12); in addition, the number of neighbours used to decide on placement, which is known as perplexity, was set to 12. Since word vectors from Word2Vec are associated with single words, vectors of functional variants of labels that we have listed in Table  I cannot be found directly. For instance, the labels 'mix' and 'mix (liquid)' are different based on the permanence of deformation. To circumvent this limitation, some motions were substituted with other words, such as 'pick-and-place' to 'move', that may capture the meaning of the original label.\n\nIn Figures 5a, 5b and 5c, we show 2-dimensional projections based on motion codes, while in Figures 5d, 5e and 5f, we see the 2-dimensional projection of motions based on pretrained Word2Vec models from Concept-Net, Google News and Wikipedia. Distances in t-SNE for Word2Vec vectors were measured using the cosine similarity metric; with motion codes, we used the regular Hamming metric (Figure 5c)   a weighted distance metric that we defined ourselves. Using a weighted approach allows us to emphasize dissimilarity based on key motion taxonomy attributes rather than the regular Hamming metric, which measures the degree of dissimilarity among bits with no considerations for their meanings. Rather than simply setting the penalty of dissimilarity to 1 for different We defined two weighted values, \u03b1 and \u03b2, which are used to set the priority of contact or trajectory types when measuring distances. \u03b1 is a penalty applied when two motions are of different interaction type (i.e. contact versus non-contact), as well as contact duration and engagement type, which is reflected by the 1st to 7th most significant bits (MSB); \u03b2 is a penalty applied for trajectory types, reflected by the 8th to 12th MSB (active trajectory) and 13th to 17th MSB (passive trajectory) of the motion code; specifically, if one motion code exhibits movement and another does not, \u03b2 is added to their distance value, but if they simply differ by the number of axes (1, 2 or 3 DOF), then only half of \u03b2 is added. All other attributes were measured normally with a penalty of 1. We illustrate the difference between these distance variations for t-SNE as Figures 5a and 5b respectively. In Figure 5a, a higher weight is assigned when two motion code vectors are different in interaction type (contact), while Figure 5b places more emphasis on motion trajectory type. In these figures, we also highlight naturally occurring clusters and neighbouring motion codes that share common attributes.\n\n\nC. Discussion on Word2Vec Comparison\n\nAs seen in the t-SNE plots in Figure 5, using motion codes (from Table I) for embedding will result in the placement of functionally similar motions close to one another (while distancing those that are functionally different as other clusters) in a different way to Word2Vec embeddings. Using a weighted approach rather than the Hamming distance between motion codes preserves neighbours better. The major disadvantage of Word2Vec vectors is that we are unable to capture multiple senses or meanings for a single word label. Furthermore, there is no way of discerning between different forms of a word such as parts of speech. For instance, in Figures 5e to 5f, 'pour' is placed closest to the word 'tap', just as we introduced before. Since the word 'tap' in the English language can either be a verb or noun, the word was interpreted in the context of the noun, as water usually flows or pours out of the tap. The same can be said of the pair 'move' and 'turn', which perhaps emphasize the noun meaning more than their verbal meaning.\n\nHowever, when considering the manipulation in a mechanical sense, it does not match our expectation since their functional attributes are different, where 'tap' is considered as contact and prismatic and 'pour' is non-contact and revolute. Instead, using motion codes, if we prioritize trajectory type (Figure 5b), the label 'pour' is placed to other revoluteonly motions such as 'turn (key)', and 'fasten (screw, nut)' (although being a cyclical motion); if we prioritize contact interaction type (Figure 5a), the label 'pour' was placed closest to the label 'sprinkle' since it is also non-contact while being placed further away from contact engagement mo-tions. Other Word2Vec results that do not match functionality (which we highlight with red ellipses) include 'beat'/'sweep', 'stir'/'sprinkle' (Figures 5e and 5f), 'dip'/'tap', and 'mash' and 'mix'. Other than the highlighted motion pairs, Word2Vec embedding generally captured the connection between certain labels such as 'cut'/'slice'/'chop' and 'sprinkle'/'pour' since these are synonymous to one another. Another shortcoming of Word2Vec embeddings is that we are unable to compare functional variants of motion types, which was the reason behind simplifying labels to single words. However, this leads to ambiguity in motion labels since we cannot be very descriptive using one word. For example, the labels 'open door' and 'open jar' were simplified as 'open', but the sense of opening can differ depending on the manipulated object. With the two separations 'open door' and 'open jar', although they serve a similar purpose, the way the motion is executed is different, and these mechanics should be considered when evaluating differences between motions. Such pairs include 'shake' (prismatic and revolute), 'mix' (liquid and non-liquid) and 'brush' (surface and non-surface).\n\n\nIV. CONCLUSION AND FUTURE WORK\n\nTo conclude, in this paper, based on our work in [8], we proposed an embedding for manipulations better suited for motion recognition in robotics and AI using the motion taxonomy. Embedding with this taxonomy circumvents the issue of language where words can take on multiple meanings. One can represent motions using attributes defined in the taxonomy as binary bits, and vectors will describe the mechanics of motions from the robot's point of view. In our experiments, we demonstrated that these motion codes, when compared to Word2Vec (which uses natural language for training), produce embeddings that provide better metrics for classification. Furthermore, these features can be extracted directly from demonstration data; with a suitable model, motion codes can be automatically generated. Motion code features are not limited to those mentioned in this paper, as other attributes could be included that can be extracted directly from data and are more representative depending on the context. In the future, we will demonstrate how a neural network can automatically generate codes for manipulations in video sequences, after which it would be established whether motion codes improve accuracy in motion recognition tasks.\n\n\nV. ACKNOWLEDGEMENTS\n\nThis material is based upon work supported by the National Science Foundation under Grant Nos. 1812933 and 1910040.\n\nFig. 3 :\n3Example of how PCA can be applied to recorded position data to derive prismatic bits for the 'stir' motion. In\n\nFig. 4 :\n4K vs. tool's principal axis y (loosen screw) (X-Y view) Similarity: Tool's Principal Axes versus Axis (K) (b) Cosine similarity: axis K vs. tool's principal axes of Rotation about Tool's Principal Axes (degrees)(c) Degree of rotation about all principal axes Example of how the axis-angle representation can be used to identify revolute properties for the 'loosen screw' motion (best viewed in colour). In\n\nFig. 5 :\n5Graphs showing the 2D projection of vectors as a result of t-SNE from: a) motion codes with more weight on contact features, b) motion codes with higher weight on trajectory features, c) motion codes with regular Hamming distance, and Word2Vec embeddings from d) Concept-Net, e) Google News, and f) Wikipedia 2018 (best viewed in colour). We highlight certain examples of motions that do not share mechanical equivalences or similarities in d) -f) in red, and we highlight clusters of similar motions produced by motion codes in a) and b), which use weights to determine distances (in varying colours to distinguish by characteristics).\n\nTABLE I :\nIMotion codes for manipulations based on the taxonomy illustrated in\n\n\nandcontinuous, rigid \n\nnon-contact \n\ncontinuous, soft \n\ndiscontinuous, soft \n\ncontinuous, soft \n\ncontinuous, rigid \n\n(a) Motion Codes (Contact) \n\npure rotation \n\npure prismatic (1D) \n\npure prismatic (2D) \n\nrotation + prismatic \n\nno motion \n\n(b) Motion Codes (Trajectory) \n\n(c) Motion Codes (Hamming distance) \n(d) Concept-Net [21] \n\n(e) Google News [4] \n(f) Wikipedia 2018 [6] \n\n\n\nDemo2vec: Reasoning object affordances from online videos. K Fang, T.-L Wu, D Yang, S Savarese, J J Lim, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionK. Fang, T.-L. Wu, D. Yang, S. Savarese, and J. J. Lim, \"Demo2vec: Reasoning object affordances from online videos,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 2139-2147.\n\nRoboCSE: Robot Common Sense Embedding. A Daruna, W Liu, Z Kira, S Chernova, arXiv:1903.00412arXiv preprintA. Daruna, W. Liu, Z. Kira, and S. Chernova, \"RoboCSE: Robot Common Sense Embedding,\" arXiv preprint arXiv:1903.00412, 2019.\n\nEvaluation of word representations in grounding natural language instructions through computational human-robot interaction. O Roesler, A Aly, T Taniguchi, Y Hayashi, 2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI). IEEEO. Roesler, A. Aly, T. Taniguchi, and Y. Hayashi, \"Evaluation of word representations in grounding natural language instructions through computational human-robot interaction,\" in 2019 14th ACM/IEEE Inter- national Conference on Human-Robot Interaction (HRI). IEEE, 2019, pp. 307-316.\n\nEfficient estimation of word representations in vector space. T Mikolov, K Chen, G Corrado, J Dean, arXiv:1301.3781arXiv preprintT. Mikolov, K. Chen, G. Corrado, and J. Dean, \"Efficient estimation of word representations in vector space,\" arXiv preprint arXiv:1301.3781, 2013.\n\nDistributed representations of words and phrases and their compositionality. T Mikolov, I Sutskever, K Chen, G S Corrado, J Dean, Advances in neural information processing systems. T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, \"Distributed representations of words and phrases and their composi- tionality,\" in Advances in neural information processing systems, 2013, pp. 3111-3119.\n\nI Yamada, A Asai, H Shindo, H Takeda, Y Takefuji, 1812.06280Wikipedia2Vec: An Optimized Tool for Learning Embeddings of Words and Entities from Wikipedia. arXiv preprintI. Yamada, A. Asai, H. Shindo, H. Takeda, and Y. Takefuji, \"Wikipedia2Vec: An Optimized Tool for Learning Embeddings of Words and Entities from Wikipedia,\" arXiv preprint 1812.06280, 2018.\n\nA survey of knowledge representation in service robotics. D Paulius, Y Sun, Robotics and Autonomous Systems. 118D. Paulius and Y. Sun, \"A survey of knowledge representation in service robotics,\" Robotics and Autonomous Systems, vol. 118, pp. 13-30, 2019.\n\nManipulation Motion Taxonomy and Coding for Robots. D Paulius, Y Huang, J Meloncon, Y Sun, 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEED. Paulius, Y. Huang, J. Meloncon, and Y. Sun, \"Manipulation Motion Taxonomy and Coding for Robots,\" in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2019, pp. 5596-5601.\n\nOn grasp choice, grasp models, and the design of hands for manufacturing tasks. M R Cutkosky, IEEE Transactions on robotics and automation. 53M. R. Cutkosky, \"On grasp choice, grasp models, and the design of hands for manufacturing tasks,\" IEEE Transactions on robotics and automation, vol. 5, no. 3, pp. 269-279, 1989.\n\nA simple ontology of manipulation actions based on hand-object relations. F W\u00f6rg\u00f6tter, E E Aksoy, N Kr\u00fcger, J Piater, A Ude, M Tamosiunaite, IEEE Transactions on Autonomous Mental Development. 52F. W\u00f6rg\u00f6tter, E. E. Aksoy, N. Kr\u00fcger, J. Piater, A. Ude, and M. Tamosiu- naite, \"A simple ontology of manipulation actions based on hand-object relations,\" IEEE Transactions on Autonomous Mental Development, vol. 5, no. 2, pp. 117-134, 2013.\n\nA hand-centric classification of human and robot dexterous manipulation. I M Bullock, R R Ma, A M Dollar, IEEE transactions on Haptics. 62I. M. Bullock, R. R. Ma, and A. M. Dollar, \"A hand-centric classification of human and robot dexterous manipulation,\" IEEE transactions on Haptics, vol. 6, no. 2, pp. 129-144, 2013.\n\nFunctional analysis of grasping motion. W Dai, Y Sun, X Qian, Intelligent Robots and Systems (IROS), 2013 IEEE/RSJ International Conference on. IEEEW. Dai, Y. Sun, and X. Qian, \"Functional analysis of grasping motion,\" in Intelligent Robots and Systems (IROS), 2013 IEEE/RSJ International Conference on. IEEE, 2013, pp. 3507-3513.\n\nThe GRASP taxonomy of human grasp types. T Feix, J Romero, H.-B Schmiedmayer, A M Dollar, D Kragic, IEEE Transactions on Human-Machine Systems. 461T. Feix, J. Romero, H.-B. Schmiedmayer, A. M. Dollar, and D. Kragic, \"The GRASP taxonomy of human grasp types,\" IEEE Transactions on Human-Machine Systems, vol. 46, no. 1, pp. 66-77, 2016.\n\nGrasp taxonomy based on force distribution. B Abbasi, E Noohi, S Parastegari, M \u017defran, Robot and Human Interactive Communication (RO-MAN). IEEE25th IEEE International Symposium onB. Abbasi, E. Noohi, S. Parastegari, and M.\u017defran, \"Grasp taxonomy based on force distribution,\" in Robot and Human Interactive Communi- cation (RO-MAN), 2016 25th IEEE International Symposium on. IEEE, 2016, pp. 1098-1103.\n\nData-driven human grasp movement analysis. H Marino, M Gabiccini, A Leonardis, A Bicchi, ISR 2016: 47st International Symposium on Robotics; Proceedings of. VDE. H. Marino, M. Gabiccini, A. Leonardis, and A. Bicchi, \"Data-driven human grasp movement analysis,\" in ISR 2016: 47st International Symposium on Robotics; Proceedings of. VDE, 2016, pp. 1-8.\n\nThe complexities of grasping in the wild. Y C Nakamura, D M Troniak, A Rodriguez, M T Mason, N S Pollard, 2017 IEEE-RAS 17th International Conference on Humanoid Robotics (Humanoids). Y. C. Nakamura, D. M. Troniak, A. Rodriguez, M. T. Mason, and N. S. Pollard, \"The complexities of grasping in the wild,\" in 2017 IEEE-RAS 17th International Conference on Humanoid Robotics (Humanoids).\n\n. IEEE. IEEE, 2017, pp. 233-240.\n\nAffordance-based imitation learning in robots. M Lopes, F S Melo, L Montesano, 2007 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEEM. Lopes, F. S. Melo, and L. Montesano, \"Affordance-based imitation learning in robots,\" in 2007 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 2007, pp. 1015-1021.\n\nBootstrapping paired-object affordance learning with learned single-affordance features. E Ugur, S Szedmak, J Piater, 4th International Conference on Development and Learning and on Epigenetic Robotics. IEEEE. Ugur, S. Szedmak, and J. Piater, \"Bootstrapping paired-object af- fordance learning with learned single-affordance features,\" in 4th Inter- national Conference on Development and Learning and on Epigenetic Robotics. IEEE, 2014, pp. 476-481.\n\nRelational affordances for multiple-object manipulation. B Moldovan, P Moreno, D Nitti, J Santos-Victor, L De Raedt, Autonomous Robots. 421B. Moldovan, P. Moreno, D. Nitti, J. Santos-Victor, and L. De Raedt, \"Relational affordances for multiple-object manipulation,\" Autonomous Robots, vol. 42, no. 1, pp. 19-44, 2018.\n\nAffordance discovery using simulated exploration. A Allevato, A Thomaz, M Pryor, Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems, ser. AAMAS 18. Richland, SC: International Foundation for Autonomous Agents and Multiagent Systems. the 17th International Conference on Autonomous Agents and MultiAgent Systems, ser. AAMAS 18. Richland, SC: International Foundation for Autonomous Agents and Multiagent Systems21742176A. Allevato, A. Thomaz, and M. Pryor, \"Affordance discovery using simulated exploration,\" in Proceedings of the 17th International Con- ference on Autonomous Agents and MultiAgent Systems, ser. AAMAS 18. Richland, SC: International Foundation for Autonomous Agents and Multiagent Systems, 2018, p. 21742176.\n\nConceptNet 5.5: An Open Multilingual Graph of General Knowledge. R Speer, J Chin, C Havasi, AAAI Conference on Artificial Intelligence. R. Speer, J. Chin, and C. Havasi, \"ConceptNet 5.5: An Open Multilingual Graph of General Knowledge,\" in AAAI Conference on Artificial Intelligence, 2017, pp. 4444-4451. [Online]. Available: http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972\n\nA database for fine grained activity detection of cooking activities. M Rohrbach, S Amin, M Andriluka, B Schiele, CVPR. IEEE Computer Society. M. Rohrbach, S. Amin, M. Andriluka, and B. Schiele, \"A database for fine grained activity detection of cooking activities.\" in CVPR. IEEE Computer Society, 2012, pp. 1194-1201. [Online]. Available: http://dblp.uni-trier.de/db/conf/cvpr/cvpr2012.html#RohrbachAAS12\n\nScaling Egocentric Vision: The EPIC-KITCHENS Dataset. D Damen, H Doughty, G M Farinella, S Fidler, A Furnari, E Kazakos, D Moltisanti, J Munro, T Perrett, W Price, M Wray, European Conference on Computer Vision (ECCV). D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kaza- kos, D. Moltisanti, J. Munro, T. Perrett, W. Price, and M. Wray, \"Scaling Egocentric Vision: The EPIC-KITCHENS Dataset,\" in European Con- ference on Computer Vision (ECCV), 2018.\n\nFunctional Object-Oriented Network for Manipulation Learning. D Paulius, Y Huang, R Milton, W D Buchanan, J Sam, Y Sun, 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEED. Paulius, Y. Huang, R. Milton, W. D. Buchanan, J. Sam, and Y. Sun, \"Functional Object-Oriented Network for Manipulation Learning,\" in 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2016, pp. 2655-2662.\n\nFunctional Object-Oriented Network: Construction & Expansion. D Paulius, A B Jelodar, Y Sun, ICRA 2018 -IEEE International Conference on Robotics and Automation. Brisbane, AustraliaD. Paulius, A. B. Jelodar, and Y. Sun, \"Functional Object-Oriented Net- work: Construction & Expansion,\" in ICRA 2018 -IEEE International Conference on Robotics and Automation, Brisbane, Australia, May 2018.\n\nHuman-object-object-interaction affordance. S Ren, Y Sun, Workshop on Robot Vision. S. Ren and Y. Sun, \"Human-object-object-interaction affordance,\" in Workshop on Robot Vision, 2013.\n\nRobot grasp planning based on demonstrated grasp strategies. Y Lin, Y Sun, The International Journal of Robotics Research. 341Y. Lin and Y. Sun, \"Robot grasp planning based on demonstrated grasp strategies,\" The International Journal of Robotics Research, vol. 34, no. 1, pp. 26-42, 2015.\n\nA dataset of daily interactive manipulation. Y Huang, Y Sun, 10.1177/0278364919849091The International Journal of Robotics Research. 388Y. Huang and Y. Sun, \"A dataset of daily interactive manipulation,\" The International Journal of Robotics Research, vol. 38, no. 8, pp. 879-886, 2019. [Online]. Available: https://doi.org/10.1177/0278364919849091\n\nRobot modeling and control. M W Spong, S Hutchinson, M Vidyasagar, John Wiley & SonsM. W. Spong, S. Hutchinson, and M. Vidyasagar, Robot modeling and control. John Wiley & Sons, 2020.\n\nVisualizing data using t-SNE. L V D Maaten, G Hinton, Journal of machine learning research. 9L. v. d. Maaten and G. Hinton, \"Visualizing data using t-SNE,\" Journal of machine learning research, vol. 9, no. Nov, pp. 2579-2605, 2008.\n", "annotations": {"author": "[{\"end\":62,\"start\":48},{\"end\":78,\"start\":63},{\"end\":86,\"start\":79}]", "publisher": null, "author_last_name": "[{\"end\":61,\"start\":54},{\"end\":77,\"start\":72},{\"end\":85,\"start\":82}]", "author_first_name": "[{\"end\":53,\"start\":48},{\"end\":71,\"start\":63},{\"end\":81,\"start\":79}]", "author_affiliation": null, "title": "[{\"end\":45,\"start\":1},{\"end\":131,\"start\":87}]", "venue": null, "abstract": "[{\"end\":1051,\"start\":133}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2283,\"start\":2280},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2288,\"start\":2285},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2293,\"start\":2290},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2381,\"start\":2378},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2386,\"start\":2383},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2751,\"start\":2748},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3444,\"start\":3441},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4116,\"start\":4113},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4367,\"start\":4364},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4535,\"start\":4532},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4541,\"start\":4537},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4547,\"start\":4543},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4553,\"start\":4549},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4559,\"start\":4555},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4565,\"start\":4561},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4571,\"start\":4567},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4577,\"start\":4573},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5776,\"start\":5772},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5782,\"start\":5778},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5788,\"start\":5784},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5794,\"start\":5790},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6047,\"start\":6044},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6052,\"start\":6049},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6058,\"start\":6054},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7555,\"start\":7551},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7577,\"start\":7573},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8250,\"start\":8247},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9830,\"start\":9827},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":12102,\"start\":12099},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16086,\"start\":16083},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":20586,\"start\":20582},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20592,\"start\":20588},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":20669,\"start\":20665},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":20675,\"start\":20671},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":20724,\"start\":20720},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":22147,\"start\":22143},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":23800,\"start\":23797},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":23805,\"start\":23802},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24010,\"start\":24007},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24015,\"start\":24012},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":24030,\"start\":24027},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24052,\"start\":24048},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":24590,\"start\":24586},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":26677,\"start\":26673},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":29162,\"start\":29158},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":35473,\"start\":35470}]", "figure": "[{\"attributes\":{\"id\":\"fig_2\"},\"end\":36912,\"start\":36791},{\"attributes\":{\"id\":\"fig_3\"},\"end\":37329,\"start\":36913},{\"attributes\":{\"id\":\"fig_4\"},\"end\":37977,\"start\":37330},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":38057,\"start\":37978},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":38439,\"start\":38058}]", "paragraph": "[{\"end\":1546,\"start\":1070},{\"end\":3823,\"start\":1548},{\"end\":6059,\"start\":3825},{\"end\":6911,\"start\":6090},{\"end\":8071,\"start\":6949},{\"end\":8785,\"start\":8073},{\"end\":9800,\"start\":8809},{\"end\":10871,\"start\":9802},{\"end\":11650,\"start\":10920},{\"end\":12634,\"start\":11695},{\"end\":13122,\"start\":12636},{\"end\":14772,\"start\":13124},{\"end\":15852,\"start\":14774},{\"end\":17351,\"start\":15854},{\"end\":18565,\"start\":17367},{\"end\":20429,\"start\":18600},{\"end\":21911,\"start\":20431},{\"end\":22926,\"start\":21960},{\"end\":23287,\"start\":22928},{\"end\":24249,\"start\":23323},{\"end\":25123,\"start\":24281},{\"end\":27080,\"start\":25125},{\"end\":28186,\"start\":27082},{\"end\":29048,\"start\":28238},{\"end\":30493,\"start\":29050},{\"end\":32463,\"start\":30495},{\"end\":33541,\"start\":32504},{\"end\":35386,\"start\":33543},{\"end\":36651,\"start\":35421},{\"end\":36790,\"start\":36675}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8808,\"start\":8786}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":20771,\"start\":20764},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":21831,\"start\":21824},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":30196,\"start\":30188},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":32576,\"start\":32569}]", "section_header": "[{\"end\":1068,\"start\":1053},{\"end\":6088,\"start\":6062},{\"end\":6947,\"start\":6914},{\"end\":10918,\"start\":10874},{\"end\":11693,\"start\":11653},{\"end\":17365,\"start\":17354},{\"end\":18598,\"start\":18568},{\"end\":21958,\"start\":21914},{\"end\":23321,\"start\":23290},{\"end\":24279,\"start\":24252},{\"end\":28236,\"start\":28189},{\"end\":32502,\"start\":32466},{\"end\":35419,\"start\":35389},{\"end\":36673,\"start\":36654},{\"end\":36800,\"start\":36792},{\"end\":36922,\"start\":36914},{\"end\":37339,\"start\":37331},{\"end\":37988,\"start\":37979}]", "table": "[{\"end\":38439,\"start\":38063}]", "figure_caption": "[{\"end\":36912,\"start\":36802},{\"end\":37329,\"start\":36924},{\"end\":37977,\"start\":37341},{\"end\":38057,\"start\":37990},{\"end\":38063,\"start\":38060}]", "figure_ref": "[{\"end\":11620,\"start\":11612},{\"end\":12319,\"start\":12313},{\"end\":17085,\"start\":17076},{\"end\":17264,\"start\":17255},{\"end\":18737,\"start\":18729},{\"end\":22086,\"start\":22080},{\"end\":22696,\"start\":22688},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24721,\"start\":24712},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24827,\"start\":24818},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26056,\"start\":26048},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":26361,\"start\":26352},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26570,\"start\":26561},{\"end\":26589,\"start\":26581},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26792,\"start\":26783},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26894,\"start\":26885},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27007,\"start\":26990},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27581,\"start\":27572},{\"end\":27731,\"start\":27723},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27838,\"start\":27829},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27983,\"start\":27974},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28185,\"start\":28168},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":30512,\"start\":30498},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":30608,\"start\":30587},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":30893,\"start\":30882},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":32171,\"start\":32162},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":32290,\"start\":32281},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":32542,\"start\":32534},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":33856,\"start\":33845},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":34051,\"start\":34041},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":34363,\"start\":34345}]", "bib_author_first_name": "[{\"end\":38501,\"start\":38500},{\"end\":38512,\"start\":38508},{\"end\":38518,\"start\":38517},{\"end\":38526,\"start\":38525},{\"end\":38538,\"start\":38537},{\"end\":38540,\"start\":38539},{\"end\":38948,\"start\":38947},{\"end\":38958,\"start\":38957},{\"end\":38965,\"start\":38964},{\"end\":38973,\"start\":38972},{\"end\":39266,\"start\":39265},{\"end\":39277,\"start\":39276},{\"end\":39284,\"start\":39283},{\"end\":39297,\"start\":39296},{\"end\":39738,\"start\":39737},{\"end\":39749,\"start\":39748},{\"end\":39757,\"start\":39756},{\"end\":39768,\"start\":39767},{\"end\":40031,\"start\":40030},{\"end\":40042,\"start\":40041},{\"end\":40055,\"start\":40054},{\"end\":40063,\"start\":40062},{\"end\":40065,\"start\":40064},{\"end\":40076,\"start\":40075},{\"end\":40355,\"start\":40354},{\"end\":40365,\"start\":40364},{\"end\":40373,\"start\":40372},{\"end\":40383,\"start\":40382},{\"end\":40393,\"start\":40392},{\"end\":40772,\"start\":40771},{\"end\":40783,\"start\":40782},{\"end\":41022,\"start\":41021},{\"end\":41033,\"start\":41032},{\"end\":41042,\"start\":41041},{\"end\":41054,\"start\":41053},{\"end\":41439,\"start\":41438},{\"end\":41441,\"start\":41440},{\"end\":41754,\"start\":41753},{\"end\":41767,\"start\":41766},{\"end\":41769,\"start\":41768},{\"end\":41778,\"start\":41777},{\"end\":41788,\"start\":41787},{\"end\":41798,\"start\":41797},{\"end\":41805,\"start\":41804},{\"end\":42191,\"start\":42190},{\"end\":42193,\"start\":42192},{\"end\":42204,\"start\":42203},{\"end\":42206,\"start\":42205},{\"end\":42212,\"start\":42211},{\"end\":42214,\"start\":42213},{\"end\":42479,\"start\":42478},{\"end\":42486,\"start\":42485},{\"end\":42493,\"start\":42492},{\"end\":42812,\"start\":42811},{\"end\":42820,\"start\":42819},{\"end\":42833,\"start\":42829},{\"end\":42849,\"start\":42848},{\"end\":42851,\"start\":42850},{\"end\":42861,\"start\":42860},{\"end\":43152,\"start\":43151},{\"end\":43162,\"start\":43161},{\"end\":43171,\"start\":43170},{\"end\":43186,\"start\":43185},{\"end\":43556,\"start\":43555},{\"end\":43566,\"start\":43565},{\"end\":43579,\"start\":43578},{\"end\":43592,\"start\":43591},{\"end\":43908,\"start\":43907},{\"end\":43910,\"start\":43909},{\"end\":43922,\"start\":43921},{\"end\":43924,\"start\":43923},{\"end\":43935,\"start\":43934},{\"end\":43948,\"start\":43947},{\"end\":43950,\"start\":43949},{\"end\":43959,\"start\":43958},{\"end\":43961,\"start\":43960},{\"end\":44334,\"start\":44333},{\"end\":44343,\"start\":44342},{\"end\":44345,\"start\":44344},{\"end\":44353,\"start\":44352},{\"end\":44727,\"start\":44726},{\"end\":44735,\"start\":44734},{\"end\":44746,\"start\":44745},{\"end\":45147,\"start\":45146},{\"end\":45159,\"start\":45158},{\"end\":45169,\"start\":45168},{\"end\":45178,\"start\":45177},{\"end\":45195,\"start\":45194},{\"end\":45460,\"start\":45459},{\"end\":45472,\"start\":45471},{\"end\":45482,\"start\":45481},{\"end\":46244,\"start\":46243},{\"end\":46253,\"start\":46252},{\"end\":46261,\"start\":46260},{\"end\":46635,\"start\":46634},{\"end\":46647,\"start\":46646},{\"end\":46655,\"start\":46654},{\"end\":46668,\"start\":46667},{\"end\":47027,\"start\":47026},{\"end\":47036,\"start\":47035},{\"end\":47047,\"start\":47046},{\"end\":47049,\"start\":47048},{\"end\":47062,\"start\":47061},{\"end\":47072,\"start\":47071},{\"end\":47083,\"start\":47082},{\"end\":47094,\"start\":47093},{\"end\":47108,\"start\":47107},{\"end\":47117,\"start\":47116},{\"end\":47128,\"start\":47127},{\"end\":47137,\"start\":47136},{\"end\":47505,\"start\":47504},{\"end\":47516,\"start\":47515},{\"end\":47525,\"start\":47524},{\"end\":47535,\"start\":47534},{\"end\":47537,\"start\":47536},{\"end\":47549,\"start\":47548},{\"end\":47556,\"start\":47555},{\"end\":47955,\"start\":47954},{\"end\":47966,\"start\":47965},{\"end\":47968,\"start\":47967},{\"end\":47979,\"start\":47978},{\"end\":48327,\"start\":48326},{\"end\":48334,\"start\":48333},{\"end\":48529,\"start\":48528},{\"end\":48536,\"start\":48535},{\"end\":48803,\"start\":48802},{\"end\":48812,\"start\":48811},{\"end\":49136,\"start\":49135},{\"end\":49138,\"start\":49137},{\"end\":49147,\"start\":49146},{\"end\":49161,\"start\":49160},{\"end\":49323,\"start\":49322},{\"end\":49327,\"start\":49324},{\"end\":49337,\"start\":49336}]", "bib_author_last_name": "[{\"end\":38506,\"start\":38502},{\"end\":38515,\"start\":38513},{\"end\":38523,\"start\":38519},{\"end\":38535,\"start\":38527},{\"end\":38544,\"start\":38541},{\"end\":38955,\"start\":38949},{\"end\":38962,\"start\":38959},{\"end\":38970,\"start\":38966},{\"end\":38982,\"start\":38974},{\"end\":39274,\"start\":39267},{\"end\":39281,\"start\":39278},{\"end\":39294,\"start\":39285},{\"end\":39305,\"start\":39298},{\"end\":39746,\"start\":39739},{\"end\":39754,\"start\":39750},{\"end\":39765,\"start\":39758},{\"end\":39773,\"start\":39769},{\"end\":40039,\"start\":40032},{\"end\":40052,\"start\":40043},{\"end\":40060,\"start\":40056},{\"end\":40073,\"start\":40066},{\"end\":40081,\"start\":40077},{\"end\":40362,\"start\":40356},{\"end\":40370,\"start\":40366},{\"end\":40380,\"start\":40374},{\"end\":40390,\"start\":40384},{\"end\":40402,\"start\":40394},{\"end\":40780,\"start\":40773},{\"end\":40787,\"start\":40784},{\"end\":41030,\"start\":41023},{\"end\":41039,\"start\":41034},{\"end\":41051,\"start\":41043},{\"end\":41058,\"start\":41055},{\"end\":41450,\"start\":41442},{\"end\":41764,\"start\":41755},{\"end\":41775,\"start\":41770},{\"end\":41785,\"start\":41779},{\"end\":41795,\"start\":41789},{\"end\":41802,\"start\":41799},{\"end\":41818,\"start\":41806},{\"end\":42201,\"start\":42194},{\"end\":42209,\"start\":42207},{\"end\":42221,\"start\":42215},{\"end\":42483,\"start\":42480},{\"end\":42490,\"start\":42487},{\"end\":42498,\"start\":42494},{\"end\":42817,\"start\":42813},{\"end\":42827,\"start\":42821},{\"end\":42846,\"start\":42834},{\"end\":42858,\"start\":42852},{\"end\":42868,\"start\":42862},{\"end\":43159,\"start\":43153},{\"end\":43168,\"start\":43163},{\"end\":43183,\"start\":43172},{\"end\":43193,\"start\":43187},{\"end\":43563,\"start\":43557},{\"end\":43576,\"start\":43567},{\"end\":43589,\"start\":43580},{\"end\":43599,\"start\":43593},{\"end\":43919,\"start\":43911},{\"end\":43932,\"start\":43925},{\"end\":43945,\"start\":43936},{\"end\":43956,\"start\":43951},{\"end\":43969,\"start\":43962},{\"end\":44340,\"start\":44335},{\"end\":44350,\"start\":44346},{\"end\":44363,\"start\":44354},{\"end\":44732,\"start\":44728},{\"end\":44743,\"start\":44736},{\"end\":44753,\"start\":44747},{\"end\":45156,\"start\":45148},{\"end\":45166,\"start\":45160},{\"end\":45175,\"start\":45170},{\"end\":45192,\"start\":45179},{\"end\":45204,\"start\":45196},{\"end\":45469,\"start\":45461},{\"end\":45479,\"start\":45473},{\"end\":45488,\"start\":45483},{\"end\":46250,\"start\":46245},{\"end\":46258,\"start\":46254},{\"end\":46268,\"start\":46262},{\"end\":46644,\"start\":46636},{\"end\":46652,\"start\":46648},{\"end\":46665,\"start\":46656},{\"end\":46676,\"start\":46669},{\"end\":47033,\"start\":47028},{\"end\":47044,\"start\":47037},{\"end\":47059,\"start\":47050},{\"end\":47069,\"start\":47063},{\"end\":47080,\"start\":47073},{\"end\":47091,\"start\":47084},{\"end\":47105,\"start\":47095},{\"end\":47114,\"start\":47109},{\"end\":47125,\"start\":47118},{\"end\":47134,\"start\":47129},{\"end\":47142,\"start\":47138},{\"end\":47513,\"start\":47506},{\"end\":47522,\"start\":47517},{\"end\":47532,\"start\":47526},{\"end\":47546,\"start\":47538},{\"end\":47553,\"start\":47550},{\"end\":47560,\"start\":47557},{\"end\":47963,\"start\":47956},{\"end\":47976,\"start\":47969},{\"end\":47983,\"start\":47980},{\"end\":48331,\"start\":48328},{\"end\":48338,\"start\":48335},{\"end\":48533,\"start\":48530},{\"end\":48540,\"start\":48537},{\"end\":48809,\"start\":48804},{\"end\":48816,\"start\":48813},{\"end\":49144,\"start\":49139},{\"end\":49158,\"start\":49148},{\"end\":49172,\"start\":49162},{\"end\":49334,\"start\":49328},{\"end\":49344,\"start\":49338}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":13803549},\"end\":38906,\"start\":38441},{\"attributes\":{\"doi\":\"arXiv:1903.00412\",\"id\":\"b1\"},\"end\":39138,\"start\":38908},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":85502928},\"end\":39673,\"start\":39140},{\"attributes\":{\"doi\":\"arXiv:1301.3781\",\"id\":\"b3\"},\"end\":39951,\"start\":39675},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":16447573},\"end\":40352,\"start\":39953},{\"attributes\":{\"doi\":\"1812.06280\",\"id\":\"b5\"},\"end\":40711,\"start\":40354},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":85497954},\"end\":40967,\"start\":40713},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":203610140},\"end\":41356,\"start\":40969},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":34048367},\"end\":41677,\"start\":41358},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":18994953},\"end\":42115,\"start\":41679},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":14441327},\"end\":42436,\"start\":42117},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":652473},\"end\":42768,\"start\":42438},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":4527673},\"end\":43105,\"start\":42770},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":20957995},\"end\":43510,\"start\":43107},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":63305297},\"end\":43863,\"start\":43512},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":28271704},\"end\":44250,\"start\":43865},{\"attributes\":{\"id\":\"b16\"},\"end\":44284,\"start\":44252},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":3910540},\"end\":44635,\"start\":44286},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":10031338},\"end\":45087,\"start\":44637},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":43648144},\"end\":45407,\"start\":45089},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":51882872},\"end\":46176,\"start\":45409},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":15206880},\"end\":46562,\"start\":46178},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":9349950},\"end\":46970,\"start\":46564},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":4710439},\"end\":47440,\"start\":46972},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":3147412},\"end\":47890,\"start\":47442},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":49651284},\"end\":48280,\"start\":47892},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":10293417},\"end\":48465,\"start\":48282},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":10178250},\"end\":48755,\"start\":48467},{\"attributes\":{\"doi\":\"10.1177/0278364919849091\",\"id\":\"b28\",\"matched_paper_id\":49564706},\"end\":49105,\"start\":48757},{\"attributes\":{\"id\":\"b29\"},\"end\":49290,\"start\":49107},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":5855042},\"end\":49523,\"start\":49292}]", "bib_title": "[{\"end\":38498,\"start\":38441},{\"end\":39263,\"start\":39140},{\"end\":40028,\"start\":39953},{\"end\":40769,\"start\":40713},{\"end\":41019,\"start\":40969},{\"end\":41436,\"start\":41358},{\"end\":41751,\"start\":41679},{\"end\":42188,\"start\":42117},{\"end\":42476,\"start\":42438},{\"end\":42809,\"start\":42770},{\"end\":43149,\"start\":43107},{\"end\":43553,\"start\":43512},{\"end\":43905,\"start\":43865},{\"end\":44331,\"start\":44286},{\"end\":44724,\"start\":44637},{\"end\":45144,\"start\":45089},{\"end\":45457,\"start\":45409},{\"end\":46241,\"start\":46178},{\"end\":46632,\"start\":46564},{\"end\":47024,\"start\":46972},{\"end\":47502,\"start\":47442},{\"end\":47952,\"start\":47892},{\"end\":48324,\"start\":48282},{\"end\":48526,\"start\":48467},{\"end\":48800,\"start\":48757},{\"end\":49320,\"start\":49292}]", "bib_author": "[{\"end\":38508,\"start\":38500},{\"end\":38517,\"start\":38508},{\"end\":38525,\"start\":38517},{\"end\":38537,\"start\":38525},{\"end\":38546,\"start\":38537},{\"end\":38957,\"start\":38947},{\"end\":38964,\"start\":38957},{\"end\":38972,\"start\":38964},{\"end\":38984,\"start\":38972},{\"end\":39276,\"start\":39265},{\"end\":39283,\"start\":39276},{\"end\":39296,\"start\":39283},{\"end\":39307,\"start\":39296},{\"end\":39748,\"start\":39737},{\"end\":39756,\"start\":39748},{\"end\":39767,\"start\":39756},{\"end\":39775,\"start\":39767},{\"end\":40041,\"start\":40030},{\"end\":40054,\"start\":40041},{\"end\":40062,\"start\":40054},{\"end\":40075,\"start\":40062},{\"end\":40083,\"start\":40075},{\"end\":40364,\"start\":40354},{\"end\":40372,\"start\":40364},{\"end\":40382,\"start\":40372},{\"end\":40392,\"start\":40382},{\"end\":40404,\"start\":40392},{\"end\":40782,\"start\":40771},{\"end\":40789,\"start\":40782},{\"end\":41032,\"start\":41021},{\"end\":41041,\"start\":41032},{\"end\":41053,\"start\":41041},{\"end\":41060,\"start\":41053},{\"end\":41452,\"start\":41438},{\"end\":41766,\"start\":41753},{\"end\":41777,\"start\":41766},{\"end\":41787,\"start\":41777},{\"end\":41797,\"start\":41787},{\"end\":41804,\"start\":41797},{\"end\":41820,\"start\":41804},{\"end\":42203,\"start\":42190},{\"end\":42211,\"start\":42203},{\"end\":42223,\"start\":42211},{\"end\":42485,\"start\":42478},{\"end\":42492,\"start\":42485},{\"end\":42500,\"start\":42492},{\"end\":42819,\"start\":42811},{\"end\":42829,\"start\":42819},{\"end\":42848,\"start\":42829},{\"end\":42860,\"start\":42848},{\"end\":42870,\"start\":42860},{\"end\":43161,\"start\":43151},{\"end\":43170,\"start\":43161},{\"end\":43185,\"start\":43170},{\"end\":43195,\"start\":43185},{\"end\":43565,\"start\":43555},{\"end\":43578,\"start\":43565},{\"end\":43591,\"start\":43578},{\"end\":43601,\"start\":43591},{\"end\":43921,\"start\":43907},{\"end\":43934,\"start\":43921},{\"end\":43947,\"start\":43934},{\"end\":43958,\"start\":43947},{\"end\":43971,\"start\":43958},{\"end\":44342,\"start\":44333},{\"end\":44352,\"start\":44342},{\"end\":44365,\"start\":44352},{\"end\":44734,\"start\":44726},{\"end\":44745,\"start\":44734},{\"end\":44755,\"start\":44745},{\"end\":45158,\"start\":45146},{\"end\":45168,\"start\":45158},{\"end\":45177,\"start\":45168},{\"end\":45194,\"start\":45177},{\"end\":45206,\"start\":45194},{\"end\":45471,\"start\":45459},{\"end\":45481,\"start\":45471},{\"end\":45490,\"start\":45481},{\"end\":46252,\"start\":46243},{\"end\":46260,\"start\":46252},{\"end\":46270,\"start\":46260},{\"end\":46646,\"start\":46634},{\"end\":46654,\"start\":46646},{\"end\":46667,\"start\":46654},{\"end\":46678,\"start\":46667},{\"end\":47035,\"start\":47026},{\"end\":47046,\"start\":47035},{\"end\":47061,\"start\":47046},{\"end\":47071,\"start\":47061},{\"end\":47082,\"start\":47071},{\"end\":47093,\"start\":47082},{\"end\":47107,\"start\":47093},{\"end\":47116,\"start\":47107},{\"end\":47127,\"start\":47116},{\"end\":47136,\"start\":47127},{\"end\":47144,\"start\":47136},{\"end\":47515,\"start\":47504},{\"end\":47524,\"start\":47515},{\"end\":47534,\"start\":47524},{\"end\":47548,\"start\":47534},{\"end\":47555,\"start\":47548},{\"end\":47562,\"start\":47555},{\"end\":47965,\"start\":47954},{\"end\":47978,\"start\":47965},{\"end\":47985,\"start\":47978},{\"end\":48333,\"start\":48326},{\"end\":48340,\"start\":48333},{\"end\":48535,\"start\":48528},{\"end\":48542,\"start\":48535},{\"end\":48811,\"start\":48802},{\"end\":48818,\"start\":48811},{\"end\":49146,\"start\":49135},{\"end\":49160,\"start\":49146},{\"end\":49174,\"start\":49160},{\"end\":49336,\"start\":49322},{\"end\":49346,\"start\":49336}]", "bib_venue": "[{\"end\":38687,\"start\":38625},{\"end\":45861,\"start\":45684},{\"end\":48073,\"start\":48054},{\"end\":38623,\"start\":38546},{\"end\":38945,\"start\":38908},{\"end\":39383,\"start\":39307},{\"end\":39735,\"start\":39675},{\"end\":40132,\"start\":40083},{\"end\":40507,\"start\":40414},{\"end\":40820,\"start\":40789},{\"end\":41139,\"start\":41060},{\"end\":41496,\"start\":41452},{\"end\":41870,\"start\":41820},{\"end\":42251,\"start\":42223},{\"end\":42580,\"start\":42500},{\"end\":42912,\"start\":42870},{\"end\":43245,\"start\":43195},{\"end\":43672,\"start\":43601},{\"end\":44047,\"start\":43971},{\"end\":44258,\"start\":44254},{\"end\":44437,\"start\":44365},{\"end\":44838,\"start\":44755},{\"end\":45223,\"start\":45206},{\"end\":45682,\"start\":45490},{\"end\":46312,\"start\":46270},{\"end\":46705,\"start\":46678},{\"end\":47189,\"start\":47144},{\"end\":47641,\"start\":47562},{\"end\":48052,\"start\":47985},{\"end\":48364,\"start\":48340},{\"end\":48588,\"start\":48542},{\"end\":48888,\"start\":48842},{\"end\":49133,\"start\":49107},{\"end\":49382,\"start\":49346}]"}}}, "year": 2023, "month": 12, "day": 17}