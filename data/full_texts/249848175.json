{"id": 249848175, "updated": "2023-10-05 13:30:22.522", "metadata": {"title": "VLMbench: A Compositional Benchmark for Vision-and-Language Manipulation", "authors": "[{\"first\":\"Kaizhi\",\"last\":\"Zheng\",\"middle\":[]},{\"first\":\"Xiaotong\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Odest\",\"last\":\"Jenkins\",\"middle\":[\"Chadwicke\"]},{\"first\":\"Xin\",\"last\":\"Wang\",\"middle\":[\"Eric\"]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Benefiting from language flexibility and compositionality, humans naturally intend to use language to command an embodied agent for complex tasks such as navigation and object manipulation. In this work, we aim to fill the blank of the last mile of embodied agents -- object manipulation by following human guidance, e.g.,\"move the red mug next to the box while keeping it upright.\"To this end, we introduce an Automatic Manipulation Solver (AMSolver) system and build a Vision-and-Language Manipulation benchmark (VLMbench) based on it, containing various language instructions on categorized robotic manipulation tasks. Specifically, modular rule-based task templates are created to automatically generate robot demonstrations with language instructions, consisting of diverse object shapes and appearances, action types, and motion constraints. We also develop a keypoint-based model 6D-CLIPort to deal with multi-view observations and language input and output a sequence of 6 degrees of freedom (DoF) actions. We hope the new simulator and benchmark will facilitate future research on language-guided robotic manipulation.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2206.08522", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/ZhengCJW22", "doi": "10.48550/arxiv.2206.08522"}}, "content": {"source": {"pdf_hash": "c6cb024e25ba5136aa914ee007453dd0af6891cc", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2206.08522v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4b60bec55dc10088528fc744fb82882c3d56dff7", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c6cb024e25ba5136aa914ee007453dd0af6891cc.txt", "contents": "\nVLMbench: A Compositional Benchmark for Vision-and-Language Manipulation\n\n\nKaizhi Zheng kzheng31@ucsc.edu \nUniversity of California\nSanta Cruz\n\nXiaotong Chen \nUniversity of Michigan\nAnn Arbor\n\nOdest Chadwicke Jenkins \nUniversity of Michigan\nAnn Arbor\n\nEric Xin \nUniversity of California\nSanta Cruz\n\nWang \nUniversity of California\nSanta Cruz\n\nVLMbench: A Compositional Benchmark for Vision-and-Language Manipulation\n\nBenefiting from language flexibility and compositionality, humans naturally intend to use language to command an embodied agent for complex tasks such as navigation and object manipulation. In this work, we aim to fill the blank of the last mile of embodied agents-object manipulation by following human guidance, e.g., \"move the red mug next to the box while keeping it upright.\" To this end, we introduce an Automatic Manipulation Solver (AMSolver) system and build a Vision-and-Language Manipulation benchmark (VLMbench) based on it, containing various language instructions on categorized robotic manipulation tasks. Specifically, modular rule-based task templates are created to automatically generate robot demonstrations with language instructions, consisting of diverse object shapes and appearances, action types, and motion constraints. We also develop a keypoint-based model 6D-CLIPort to deal with multi-view observations and language input and output a sequence of 6 degrees of freedom (DoF) actions. We hope the new simulator and benchmark will facilitate future research on language-guided robotic manipulation.Preprint. Under review.\n\nIntroduction\n\n\"Can you help me to clean the disks in the sink?\" -humans communicate with each other using language to issue tasks and specify the requirements. Although recent progress in embodied AI pushes intelligent robotic systems to reality closer than any other time before, it is still an open question how the agent learn to manipulate objects following language instructions. Therefore, we introduce the task of Vision-and-Language Manipulation (VLM), where the agent is required to follow language instructions to do robotic manipulation. There are recent benchmarks developed to evaluate robotic manipulation tasks with language guidance and visual input [10,1,33]. However, the collected task demonstrations are not modular and can hardly scale because they lack (1) adaptation to novel objects (2) categorization for modular and flexible composition to complex tasks. Additionally, the lack of variations in language also lead to biases for visual reasoning learning. To deal with these problems, we expect an inclusive, modular, and scalable benchmark to evaluate embodied agents for various language-guided manipulation tasks.\n\nAn ideal VLM benchmark should have at least three characteristics: The first one is scalability. Such a benchmark should automatically generate various physics-realistic 6 degrees of freedom (DoF) interactions with affordable objects and expand new tasks effortlessly. The second one is task categorization, which exploits commonality concerning robot motion between different semantic tasks and is almost ignored in existing works. The third one is reasonable language generation, Figure 1: Given the language instructions and observations, the VLMbench requires the agent to generate an executable manipulation trajectory for specific task goals. On the left, we show that the complex tasks can be divided into the unit tasks according to the constraints of the end-effector, like \"Open the door of the dishwasher\" and \"Open the door of the fridge\" should both follow the rotation constraints of the revolute joint. On the right, we show examples of object-centric representations, where all graspable objects or parts will generate local grasping poses as their attributes. Depending on the modular design, we can generate reasonable VLM data automatically.\n\nwhich requires the benchmark can generate language instructions for testing diverse visual reasoning abilities without biases. However, existing benchmarks [30,10,33,1] lack at least one characteristic for VLM tasks. Motivated by these attributes, we present VLMbench, a highly categorical robotic manipulation benchmark with compositional language for visual reasoning. To build and scale VLMbench, we propose AMSolver, an automatic unit task builder that can compose unit tasks to create complex multi-step tasks and seamlessly adapt to novel objects. Compared to previous benchmarks, VLMbench categorizes manipulation tasks into various meta manipulation actions according to the constraints of robot trajectories for the first time. Meanwhile, the combinations of compositional language templates and object-centric representations provide numerous variations for visual reasoning in VLMbench, as shown in Figure 1.\n\nTo investigate the difficulty of the benchmark, we test them with several partially modal methods and a keypoint-based method, 6D-CLIPort, modified from the state-of-the-art language-guided manipulation method CLIPort [25]. The results show that there is still a massive room for improvement in the robust manipulation action generations and accurate language-guided visual understanding. To sum up, our contributions in this work include:\n\n\u2022 AMSolver, an automatic demonstration generator for various task semantics, motion constraints, object types and states defined in a novel task template formulation.\n\n\u2022 VLMbench, a robot manipulation benchmark on 3D tasks with visual observation and compositional language instructions, where we categorize the manipulation tasks by constraints and provide variations with minimal biases in the first time.\n\n\u2022 6D-CLIPort, a general vision-and-language manipulation baseline model evaluated on all kinds of VLMbench tasks.\n\n\nRelated Work\n\nRobotic Manipulation Benchmarks There are plenty of benchmarks proposed related to visuallanguage robotic tasks. ALFRED [26] was proposed to do virtual object rearrangement tasks CausalWorld [1] MetaWorld [33] ManipulaTHOR [6] Habitat 2.0 [30] Robomimic [17] BEHAVIOR [27] RLBench [10] CALVIN [19] VLMbench (ours) Table 1: Comparison of existing robotic manipulation benchmarks to VLMbench. guided by visual observation and language instruction between different room-scale locations. Ma-nipulaTHOR [6] introduced realistic interaction with objects using a 6 DoF configurable mobile manipulator. Habitat 2.0 [30] and BEHAVIOR [27] incorporated real robot navigation with simple object interaction implementation for more comprehensive mobile manipulation tasks. CALVIN [19] collects 24 hours playing data with natural language instructions for long-horizon manipulation tasks. Regarding static manipulation benchmarks, MetaWorld [33] collects a series of translation-only tasks with demonstrations for reinforcement learning. CausalWorld [1] focused on causal inference within manipulation and showed examples on several simple object rearrangement tasks. RLBench [10] collected 100 different tasks by specifying robot arm end-effector waypoints for each of them for robot learning. Besides, there are also crowd-sourcing platforms that collect human demonstrations of a variety of common manipulation tasks through VR/AR devices, such as Robosuite [38] and Robomimic [17]. Compared to these works, our VLMbench includes both high-level task descriptions and low-level robot action translations and realizes automatic task builders for easier generation of complex tasks.\n\nVision and Language Tasks for Embodied AI Tasks such as Vision Question Answering (VQA) [3,5], Video Captioning [7,31], Image-Text Retrieval [37,4], etc., connect vision and NLP research to produce semantics from combined embedding features. Recent works such as referring object spatial relations [13], dynamic events [32] provide valuable reasoning result for the agent to make actions. Vision-and-Language Navigation (VLN) [2,15,21,11] proceeds another step to use visual observations and language instructions for robotic navigation directly. Recent benchmarks [26,30,27] combined VLN with abstracted object interaction to include more object rearrangement tasks. Compared to the tasks mentioned above, our benchmark is designed for more complex robotic manipulation that simulates physics-realistic 6DoF grasping and requires the reasoning ability from the visual-language space to the executable action space.\n\nLanguage-Instructed Manipulation Recently, various manipulation tasks have been researched with language input either describing the entire task, or serving interactive input for task specifications. HULC [18] proposes a hierarchical network for long-horizon manipulation tasks, which contains multi-modal transformers for task planning and a policy network for action generation. Structformer [14] proposes an object selection network from language and visual encodings, as well as a language conditioned pose generator for semantic object rearrangement. Stepputtis et al. [28] proposed a closed-loop control model for pouring tasks. CLIPort [25] proposed a two-stream framework to learn a spatial attention map for 2D object manipulations. Lynch et al. [16] fed natural language instructions to a goal-conditioned policy pretrained using imitation learning for various tasks. INVIGORATE [36] proposed an interactive system that takes language input to correct false estimations and re-plan for object grasping in clutter. Shao et al. [24] trained a joint language-vision model on 7-DoF goal trajectory estimation with another task classifier for multi-task training. Goyal et al. [8] learned zero-shot task adaptation to accomplish novel tasks through differences described in natural language. In this paper, our baseline evaluated on VLM is built over CLIPort, which learns full 6D grasping included, variate-length robot manipulation tasks from language instructions.\n\n\nAMSolver: Automatic Manipulation Solver\n\nWe consider a rule-based task categorization that supports most of daily manipulation tasks, which follows a unified formulation. To this extent, we propose Automatic Manipulation Solver (AM- Figure 2: The unit task templates of AMSolver. On the left, we show three unit task templates parameterized by position and orientation constraints over the robot end-effector on either goal pose or entire path. By combining these unit task templates, various task examples can be generated. For example, on the right we show three main common task types of household tasks and long-horizon tasks composed of unit tasks. Solver) 1 . We focus on simple unit tasks and introduce a unit task template that categorizes motion constraints and generates a wide range of household tasks (see examples in Figure 2). The formulation treats objects as a representation that could have variations in appearance, and enables automatic demonstration generation using off-the-shelf task and motion planners.\n\n\nRule-based Unit Tasks\n\nSince the tasks have notable variations, we assume that each complex task can be decomposed into the combinations of several unit tasks from the aspects of end-effector trajectories. We define a unit task as the semantic step of completing a sub-goal of the entire task. Specifically, a unit task is defined in a formula of 'take action on an object under certain constraints' where a unit task can be parameterized by two constraints: (a) position constraints and (b) orientation constraints, which describe the valid spatial space or orientation range, respectively, of the end-effector for a specific task. We propose three unit task templates detailed below that can compose the aforementioned complex tasks.\n\n(1) Control is a preparation or ending step of a task, which models the transition of the object state, where the state indicates whether the robot can move the object or not. In this work, we specify one way of transition: to obtain control by grasping it and release control by opening the gripper. There are other ways to obtain control, like pushing, hitting, etc. However, these transitions will naturally lead to the constraints in the following sub-tasks, so they cannot be considered a general component for any complex task. In this unit task, the position and orientation constraints depend on the geometry of object instances.\n\n(2) M1 denotes moving the target object with goal pose constraints, which can be modeled as a 6 DoF transform in the robot's workspace. The position constraints define a bounded goal space in R 3 , while the orientation constraints define a valid 3D orientation SO(3). We consider four types of position constraints: (a1) Free, (a2) Plane, (a3) Line, and (a4) Fixed, which means the goal position is any point inside a 3D space (a1), constrained in a 2D plane (a2), constrained in a line-shape area (a3), or fixed to a certain point (a4). There are three types of orientation constraints: (b1) Free, (b2) Axis, and (b3) Fixed, which means the goal orientation is unlimited in 3D rotation space (b1), only rotated along an axis in space (b2), or fixed at a given orientation (b3). For example, M1[a2, b1] can represent placing the object on a tabletop (with plane position constraint), while M1[a3, b2] can represent moving the object to a position on one line and ending with a constrained orientation of one axis, like dropping a stick into the hole.\n\n(3) M2 denotes moving the target object along a trajectory while satisfying the motion constraints during the entire path, which implies a more strict condition than M1. The constraints are mostly from object articulation, like revolute joints on doors, or task-specific requirements, like keeping the opening upward for a full filled mug. Therefore, there are four kinds of position constraints: (a1) Free, (a2) Plane, (a3) Line, and (a4) Fixed trajectory, which means any position inside a space (a1), a plane (a2), a line (a3) or a predefined path (a4) should be feasible for the trajectory, and four kinds of orientation constraints: (b1) Free, (b2) Axis, (b3) Fixed, and (b4) Depend on position, which means every pose in the trajectory should be unlimited orientations (b1), at most rotated along an axis (b2), fixed to a certain orientation (b3), or depended on the corresponding positions. For example, M2[a2, b2] means moving the object inside a 2D plane while maintaining the orientation of one axis, like wiping the table, while M2[a4, b2] means moving the object along a fixed trajectory with maintaining the orientation of one axis, like using a screwdriver to tighten a screw. It is worth mentioning that M2[a1,b1] will degenerate to M1[a1,b1]. According to our unit task definition, we have covered every feasible 6 DoF poses of the end-effector. Therefore, we have reason to believe that these unit tasks can represent any complex task in the action space.\n\n\nObject-centric Representation\n\nSome recent works [12,34] have used object-centric representations for manipulation. Since the properties are defined on objects, these representations can easily cross the variations of environments, agents, and tasks. Our benchmark assumes that objects used in the tasks are rigid, and their fundamental properties will not change during the tasks. Therefore, we can parameterize the objects as a set of configurations, including class, color, size, and geometry shape. If the object is articulated, its whole configuration will contain the configuration of each part and the physical constraint of each connection. For example, a door consists of three parts: the door base, plank, and handle, so its configuration will contain each part's configuration and record the positions and ranges of two revolute joints.\n\n\nAutomatic Demonstration Generation\n\nTo create a task example, the user could compose a task template from the unit library of formulation, such as a unit task of object rearrangement as Control-M1 or a more complex task of object stacking as Control-M1-Control-M1-Control-M1, etc., and then select the objects from a given set to be included in the scene as object to be manipulated and distractor objects. In the simulation the object placement could be randomized with custom specification. The corresponding language descriptions could also be generated from templates.\n\nTo implement Control as grasping, we create an object-wise grasp pose dictionary. Specifically, a point cloud-based grasping pose detection algorithm [20] is implemented to search all feasible grasping poses, given the object's shape and robot gripper parameters. The grasp poses are saved and transformed to world space for a particular task by simply multiplying with the object's pose. To implement M1 and M2, we integrate customized motion constraints in the OMPL motion planner library so that the calculated trajectory satisfies the constraints automatically. For more details and task examples, please refer to Appendix B.\n\n\nVLMbench: Visual-and-Language Manipulation Benchmark\n\n\nProblem Definition\n\nGiven language instructions, the Vision-and-Language Manipulation (VLM) task requires an embodied agent to follow the instructions to complete tabletop manipulation tasks. Formally, at the beginning of the task, the agent receives a set of language instructions L = {L 1 , L 2 , ..., L n }, where L i denotes one sentence of arbitrary length. The initial state s 0 contains multi-view RGB images, depth images, segmentation information, and robot states, including joint angles, velocities and torques, and end-effector pose. Given the observations and language instructions, the agent needs to estimate an executable action command a 0 , directly working on the end-effector or joints. Then, at each step t, the agent receives new observations o t and generates the action a t = f (s t |s 0 , s 1 , ..., s t\u22121 , L) for the\n\n\nGoal Pose Constraints Tasks\n\nFigure 3: We show four task categories with goal-pose constraints on the left side, and on the right side, we show four task categories with path-pose constraints. For each task category, we visualize the observations of the overhead view and list the main unit task, variations, and instruction templates. The red words with brackets indicate the blank where the variations descriptions can fill in. The blue words indicate the variations in the tasks. The combinations of variations will lead to various instance-level tasks.\n\nnext step. The step loop will repeat until the agent sends a stop action or should be terminated, e.g., achieve the success conditions or the limitation steps. The agent should obey the constraints provided by language instructions during the whole running.\n\n\nTasks and Dataset\n\nIn previous works, the researchers manually designed manipulation tasks by implicit prior knowledge without categories. Instead, we are trying to build tasks from the perspectives of elementary manipulation abilities. In other words, since different tasks have various semantic meanings, we consider the task with the same unit tasks combination should be in the same category from the aspect of the action space. For example, \"Open the door of the fridge\" and \"Open the door of the microwave\" require the same action ability except for semantic meanings and grasping poses which depend on the object geometry. Therefore, we define eight general task categories, represented by one typical task in each category, shown in Table 4.2 and Fig. 3. We use the definitions in the unit task templates to represent the main constraints of each task category. The task details can be found in Appendix A and dataset statistics can be found in Appendix C.\n\n\nTask Variations\n\nThe manipulated object's properties can randomly change for each task category, and every combination leads to a task instance. In the VLMbench, we use 8 different variations: color, size, relative position, shape, direction, level, amount, and action type. The variations are from two perspectives: object and motion. Object variations include color, size, shape, relative position, and direction. Here, the color is chosen from 20 colors in the seen settings. The size contains the relative descriptions between two objects, \"smaller\" and \"larger\", and descriptions between three objects, which are \"large\",\"medium\", and \"small\". The shape contains 5 types of objects for the seen and unseen settings. The relative position describes the spatial relationship between two objects, like the top, front, rear, left, and right. The direction contains two descriptions for rectangular prism in a plane:\"horizontal\" and \"vertical\". For the objects that have the vertical structure, the level includes \"top\", \"middle,\" and \"bottom.\" From the motion view, the variations are amount and action type. The amount means how far the task needs to be done, consisting of \"fully\" and \"slightly.\" The action type includes \"open\" and \"close\", especially for the articulated objects. The table of these variations and models used can be found in Appendix A.\n\nUnseen Settings All tasks in the unseen settings are unseen <color,shape> combinations from an unseen color collection and an unseen shape collection (where the shapes include all object classes   and variants). The unseen color collection has five new colors that do not appear during training, including brown, gold, pink, chocolate, and coral. As for the unseen shape collection, it has some overlap with the seen library for the tasks with color variations (but the <color,shape> combinations are always unseen), and is exclusive for the tasks without color variations (e.g., we introduce a new door model with a rotatable handle for the unseen setting of Door tasks). So there are mainly three kinds of unseen combinations: <unseen color,unseen shape>, <unseen shape>, and <unseen color,seen shape>. The exact object models used for each task can be found in Appendix A.\n\n\nVision-and-Language Manipulation Agent\n\nTo provide a baseline method that can solve VLM tasks, we propose a neural-network based agent 6D-CLIPort that takes in the input of multi-view RGB-D observations and task language descriptions and outputs 6 DoF pose keypoints along the path that can accomplish a specific task. For instance, for pick-and-place task, the agent will iterate twice and output the object's 6D pose for pick, and place, separately.\n\nThe overall flow of 6D-CLIPort 2 is shown in Figure 4. The input data passes through visual and language encoders and the embedded features are fused together to obtain a pixel-wise feature map that shows the probability of object of manipulation interest. The encoding module is reused in 3 models (Attention, Value, and Key Module) as follows: The Attention module takes maximum of the feature map to get 2D image sample crops. The Key module takes input of the rotated crops and output feature map crops. Then, the feature map crops are used as convolution kernels and pass through the feature map of the entire image from Value module to get a 3D pose heatmap (x-y 2D position and yaw rotation, and their estimations are obtained by maximizing the probability). Three fully connected neural network modules will regress one of the remaining 3 dimensions each (z position and roll, pitch rotation) from this heatmap. Finally, a full 6 DoF pose is composed. Compared to the state-of-the-art method CLIPort [25], we resolved its two constraints: 1. increase 3 DoF only motions to full 6 DoF; 2. output arbitrary number of poses than fixed 2 outputs for pick and place actions. Besides, we introduce some details below.\n\n\nMulti-view Vision Fusion\n\nTo get a better RGB-D input, we first fuse RGB-D input from several cameras with known poses into a 3D colored point cloud, and then project it along the vertical direction facing towards the table plane to get a top-view RGB-D image.\n\nEncoding Module The agent has two feature extraction streams: semantic and spatial streams. The agent uses the pretrained CLIP's ResNet50 and Transformer model [22] to encode the RGB and languages in the semantic stream. An untrained Transporter ResNet [35], which has 43 layers and 8 strides, is used in the spatial stream to encode the RGB-D image. Then, the decoder fuses these latent space features by concatenation, fully convolution, and up-sampling layers, and predicts a dense pixel-wise feature map. More details can be found in [25].\n\nImplementation Details We separately train the agent for each task category. For example, the agent for pick and place tasks will jointly train on the data of all variations mentioned in Table 4.2. In details, since the VLMbench is built by the unit task templates, the input demonstrations D = {L, \u03b6 1 , \u03b6 2 , ..., \u03b6 i } can be divided into different sub demonstrations by the waypoints generated from unit task templates, where each step \u03b6 i = (L i , o i , g i ) consists of language instruction L i , observation o i , and the 6 DoF sub-goal waypoint pose g i for the current step. For step i, we use the observations in the first frame of this step as o i and prompt high-level instruction L with \"Step i\" as L i . The input RGB-D image has resolution 160 \u00d7 128. The feature heatmap is with dimension 16 in x-y 2D plane, and the crops from Attention Module are rotated 36 times before feeding into the Key Module.\n\n\nExperiments\n\n\nExperimental Setup\n\nEvaluation Settings Before testing each baseline, we preprocess the tasks to help the agent eliminate trivial steps. 1) we divided tasks into the sub-goal sequences by the ground truth waypoints generated by AMSolver, so that the agent only needs to estimate the actions of the predefined unit task sequence for each task. 2) Since we have the ground truth gripper state for each sub-goal, we also provide whether the gripper should open or close for each action estimation. 3) To increase the grasping stability, we use the pre-generated grasping pose instead of estimations if these two poses are close enough (distance is less than 5 cm and the rotation is less than 10 degrees). 4) To reduce the failure grasping cases due to the motion planning, we use a predefined pre-grasping offset, which is 8cm backward along the z-axis of the target grasping pose, and a post-grasping offset, which is 8cm upward along the z-axis of the world frame.\n\nIn each episode of one task variation, the simulator imports a test configuration from the pre-collected test dataset, which includes the initialization poses of objects, the success conditions of the task, and a set of waypoints that can finish the task for reference. The agent should solve the task in the online simulator within a limited number of steps. Success rate is used as the primary evaluation metric, calculated by dividing the number of success conditions satisfied by the number of tests. We use the average success rate of all variations for each task category. The success conditions are mainly determined by an object detector or a joint detector. The object detector returns true if particular objects have moved inside the predefined space, and the joint detector returns true when the joint angle reaches the predefined range. The success conditions of each task can be found in Appendix A.  Table 3: Success rates of all task categories, including both seen and unseen settings.\n\nBaselines In addition to the 6D-CLIPort model, we provide two kinds of baseline models for comparison, one with partial input modalities and the other with partial ground-truth predictions.\n\nTo test the influence of different input modalities, we traine two other agents with partial modalities: a Language-Only agent and a Vision-Only agent. These agents use the same model architecture as 6D-CLIPort but have different input modalities. The Language-Only agent uses the CLIP transformer for language encoding and the visual inputs are all zeros. The Vision-Only agent use the same RGBD inputs as in the 6D-CLIPort agent, but its language input will only include the prompt for steps indication like \"Step zero\" without any high-level language instructions.\n\nTo measure the capabilities and limitations of 6D-CLIPort, we individually test its position or orientation estimation abilities by giving the ground-truth values of the other. Although the trajectories of finishing the tasks are various and we cannot obtain the optimal position and rotation information, we can regard the poses of waypoints as sub-optimal solutions. GT Pos means given ground truth x/y/z positions while the other three parameters for 3D orientation are estimated, and GT Ori suggests the contrary (known orientation, using estimated 3D position).\n\n\nResult Analysis\n\nMain Results on Different Task Categories and Variations The main results on different task categories are shown in Table 3. We observe that 6D-CLIPort performs better on the tasks that have lower rotation variances, including \"Pick&Place,\" \"Stack,\"\"Shape Sorter,\" \"Wipe,\" and \"Drawer\". It indicates that 6D-CLIPort can better estimate the positions than orientations in the 3D spaces. Moreover, 6D-CLIPort performs poorly on the \"Pour\" tasks since the task needs to adjust the pouring poses following the grasping pose, which introduces additional difficulties. Moreover, although the success rates in the unseen settings are generally lower than those in the seen settings, the performance drop is reasonable and not dramatic, showing that 6D-CLIPort can transfer the learned manipulation knowledge from seen objects to unseen objects, benefiting from the powerful transfer ability of the pre-trained CLIP model [22].\n\nWe also show the results from the perspective of variations across task categories in Table 4. The results show that the agent is more sensitive to the novel compositions and thus has larger seen-unseen performance drop on variations such as \"Shape\", \"Level\", \"Action Type\", and \"Amount\". Table 3 Table 4: Success rates of all variations, including both seen and unseen settings. chance to close the drawer by collisions. (3) Without the language guidance, the Vision-Only agent has a significant performance degradation on all tasks. It fails completely on tasks that requires more strict pose constraints, including \"Drop\", \"Shape Sorter\", \"Pour\", and \"Door\". For other tasks such as Pick&Place\", \"Stack,\" \"Wipe,\" and \"Drawer\", the Vision-Only agent can succeed on few cases (though with pretty low success rates) by randomly grasping an object in the scene for manipulation. Because those tasks have lower variance of the grasping actions and following movements.\n\n\nImpact of Different Input Modalities\n\n\nAblation Study on Position and Orientation Estimation\n\nWe provide partial ground-truth predictions and do a unit test of the agent's position and orientation estimation abilities. The results are shown in Table 3 and Table 4. From the results, we can see the position estimation ability significantly limits the performance of 6D-CLIPort in those tasks which need correct object localization, such as \"Pick & Place\", \"Stack\", and \"Shape Sorter\" tasks. One primary reason why GT position brings more improvement is that it eliminates the difficulties of localizing the target object, one of the main challenges in compositional reasoning. For example, the instruction \"place the red cube into the green container\" requires the model to localize the correct cube and container, and providing GT position makes the task much easier. Besides, from the perspective of pose variances, when we divide the task into steps, the orientation of each step has fewer variances than the position in many tasks such as picking, stacking, and wiping. Furthermore, for the tasks requiring the cooperation of position and orientation such as \"Pour\" and \"Door\" tasks, we observe that giving partially ground truth may still not guarantee better task completion results.\n\nMore results and analysis can be found in Appendix D.\n\n\nConclusion and Future Work\n\nVision-and-Language Manipulation (VLM) tasks are essential since they are inevitable for embodied AI. For further research in this area, we propose VLMbench, which includes various VLM tasks, and AMSolver, used for automatic VLM task generation. In addition, we test the 6D-CLIPort agent, a keypoint-based 6 DoF agent, on the benchmark. The results show that the current models can finish VLM tasks, but it is still a new area needed to be explored. We hope the VLMbench can push the research in finding general language-guided manipulation agents.\n\nLimitations Our work still has limitations that can be improved by future work. First, we only consider rigid body object manipulation in the VLMbench. It is important to include the soft material objects in the future. Second, indirect manipulation tasks, like throwing the ball and playing billiards, are not included in the VLMbench. Third, since we generated data with template languages in simulator, the gap between the virtual environment and real world cannot be ignored.\n\n\nEthical Concerns\n\nWe do not see significant risks of security threats or human rights violations in our work. Since our work contributes to the field of language-guided manipulations, we do not encourage real world robot experiments depending on our benchmark without any real world data fine-tuning. Due to the gap between the simulator and real world, the agents may execute unexpected actions.\n\nProject website:https://sites.google.com/ucsc.edu/vlmbench/home A Task Details   In the VLMbench, we show eight task categories:\"Pick & Place objects\", \"Stack objects\", \"Drop pencil\", \"Put into shape sorter\", \"Pour water\", \"Wiper table\", \"Use drawer\", and \"Use door\". Here, we list variations used for these tasks in Table. 5. For each demonstration, all things in the scene will change the pose at the beginning. When building an instance-level task with one variation, the other variations will also randomly change. For example, in the demonstrations of \"Pick & Place objects\" with \"size\" variation, all objects' color and relative positions, including targets and distractors, will randomly change. In the dataset, we have five types of objects, shown in Table 6. We will explain each task in detail as follows. Visualizations can be found in the project website.\n\n\nA.1 Pick & Place Objects\n\nTask Definition: The agent needs to distinguish the specific object to grasp and then place it into a particular container. The object can be placed anywhere with any orientation inside the container. Task Templates: Unit task sequence: (Control, target object)+(M1[a1,b1], target object); Goal Conditions: A 3D bounding box without orientation constraints inside the empty space of target container, for (M1[a1,b1], target object).\n\n\nSuccess Conditions:\n\nThe object detector inside the target container only can be triggered by the specific object. When the detector is triggered, the task considers a success. Object models: One container model is used in both seen and unseen. In the seen settings, five object models: star, triangular, cylinder, cube, and moon. In the unseen settings, four object models: cube, the letter of 't,' cross, and flower.\n\nVariations and scene settings: All objects are randomly changing colors, size, and positions in each demonstration. Color: There are two same-shape objects and two same-shape containers in the scene initialization. All colors are randomly sampled from the color library. The object description is \"[color] object\"; The container description is \"[color] container.\" Size: There are two same-shape objects and two same-shape containers in the scene initialization. One object and one container are randomly magnified while others are randomly shrunk. The object description is \"[larger/smaller] object\"; The container description is \"[larger/smaller] container.\" Relative Position: There are two same-shape objects and two same-shape containers in the scene initialization. All objects are randomly sampled in the workspace until they obey the predefined relative positions. The object description is \"[front/rear/left/right] object\"; The container description is \"[front/rear/left/right] container.\" Shape: There are two same-shape containers and more than two objects with different shapes in the scene initialization. The number of objects varies from two to the length of the object library. The object description is \"[shape]\"; The container descriptions is \"[color] container.\"\n\n\nA.2 Stack Objects\n\nTask Definition: The agent needs to distinguish the specific two objects and then stack them in a particular sequence. Since the objects have different shapes and some surfaces are hard to maintain for stacking, we use plane surfaces. Therefore, the goal pose of the above object should be inside the top plane of the below object and only can rotate along the axis which is perpendicular to the plane. Task Templates. Unit task sequence: (Control, target object)+(M1[a2,b2], above object); Goal Conditions: A plane on the surface of the below object with orientation constraints along the perpendicular axis, for (M1[a2,b2], above object).\n\nSuccess Conditions. The object detector attached to the bottom object will be triggered when the specific thing is above. Object models: In the seen settings, five object models: star, triangular, cylinder, cube, moon. In the unseen settings, four object models: cube, the letter of 't', cross, flower.\n\nVariations and scene settings: All objects are randomly changing colors, size, and positions in each demonstration. Color: There are four same-shape objects in the scene initialization. All colors are randomly sampled from the color library. The object descriptions are all \"[color] [shape].\" Size: There are three same-shape objects in the scene initialization. One model is randomly magnified while another is randomly shrunk. The object description is \"[large/medium/small] [shape].\" Relative Position: There are four objects with two different shapes in the scene initialization. All objects are randomly sampled in the workspace until the relative position of the same-shape objects obey the predefined relative positions. The below object description is \"[front/rear/left/right] [shape 1]\"; The above object description is \"[front/rear/left/right] [shape 2].\" Shape: There are more than two objects with different shapes in the scene initialization. The number of objects varies from two to the length of the object library. The below object description is \"[shape 1]\"; The above object description is \"[shape 2].\"\n\n\nA.3 Drop Pencil\n\nTask Definition: The agent must distinguish the specific pencil and then drop it into an upright container. Only when the pencil is vertical down above the container can it fall appropriately. Object models: One pencil and one basket for both seen and unseen.\n\nVariations and scene settings: All objects are randomly changing colors and positions in each demonstration. Color: There are two same-shape pencils and two same-shape baskets in the scene initialization. All colors are randomly sampled from the color library. The object description is \"[color] pencil\"; The container description is \"[color] container.\" Size: There are two same-shape pencils and two same-shape baskets in the scene initialization. One pencil and one container are randomly magnified while others are randomly shrunk. The object description is \"[larger/smaller] pencil\"; The basket description is \"[larger/smaller] container.\" Relative Position: There are two same-shape pencils and two same-shape baskets in the scene initialization. All objects are randomly sampled in the workspace until they obey the predefined relative positions. The object description is \"[front/rear/left/right] object\"; The basket description is \"[front/rear/left/right] container.\"\n\n\nA.4 Put Into Shape Sorter\n\nTask Definition: There is a shape sorter in the environment, and the agent needs to distinguish the specific object and then put it through the hole of the sorter. Only when the object's shape and pose fit the hole can it fall. Object models: One shape sorter container for both seen and unseen. In the seen settings, four object models: star, triangular, cylinder, and cube. In the unseen settings, five object models: star, triangular, cylinder, cube, and moon.\n\nVariations and scene settings: All objects are randomly changing colors and positions in each demonstration.\n\nColor: There are three same-shape objects and one shape sorter in the scene initialization. All colors are randomly sampled from the color library. The object description is \"[color] [shape].\" Relative Position: There are two same-shape objects and one shape sorter in the scene initialization. All objects are randomly sampled in the workspace until they obey the predefined relative positions. The object description is \"[front/rear/left/right] [shape].\" Shape: There are one shape sorter and more than two objects with different shapes in the scene initialization. The number of objects varies from two to the length of the object library. The object description is \"[shape].\"\n\n\nA.5 Pour Water\n\nTask Definition: The agent needs to pour the water from the specific source mug into the particular container mug. Here we use 50 small particles to simulate the water.\n\nVariations: Color, Size, and Relative Position. These variations work on all mugs. Task Templates. Unit task sequence: (Control, source mug)+ (M2[a1,b2], source mug)+(M2[a4, b4], source mug); Goal Conditions: For (M2[a1,b2], source mug), we need to keep the opening of the source mug is upward with the rotations along the axis of the opening directions. Further, the goal position of this step needs to make the horizontal distance between the geometry centers of two mugs less than the half-height of the source mug, and the vertical distance should be larger than the height of the container mug. For (M2[a4, b4], source mug), the source mug needs to make the opening point to the container mug. Therefore, the end-effector should follow a particular path, keeping the source mug in the same position but rotating it to the required orientation. Therefore, the goal condition is a set of poses, calculated by a function using the geometry and poses of two mugs.\n\nSuccess Conditions. The object detector attached to the container mug will be triggered when more than half particles are inside the mug. Object models: Four different mugs in seen scenes and two different mugs in unseen scenes.\n\nVariations and scene settings: All objects are randomly changing colors and positions in each demonstration.\n\nColor: There are three same-shape mugs in the scene initialization. All colors are randomly sampled from the color library. The object description is \"[color] mug.\" Relative Position: There are two same-shape mugs in the scene initialization. All objects are randomly sampled in the workspace until they obey the predefined relative positions. The object description is \"[front/rear/left/right] mug.\" Size: There are two same-shape mugs in the scene initialization. One mug is randomly magnified while another is randomly shrunk. The object description is \"[larger/smaller] mug.\" Table   Task Definition: The agent needs to wipe the particular area with a sponge, which means moving the sponge from one side of the area to another with keeping it contacting with the area. Success Conditions. We create 50 small invisible particles in the target area, and these particles will be removed if the sponge surface touches them. The successor attached to the target area will be triggered when more than half particles have been removed. Object models: One sponge for both seen and unseen. The four planes in the seen scenes: rectangle, round, star, and triangle; The two planes in the unseen scenes: cross, flower.\n\n\nA.6 Wipe\n\nVariations and scene settings: All objects are randomly changing colors and positions in each demonstration.\n\nColor: There are two same-shape planes and one sponge in the scene initialization. All colors are randomly sampled from the color library. The plane description is \"[color] area.\" Size: There are two same-shape planes and one sponge in the scene initialization. One plane is randomly magnified while another is randomly shrunk. The plane description is \"[larger/smaller] area.\" Relative Position: There are two same-shape planes and one sponge in the scene initialization. All planes are randomly sampled in the workspace until they obey the predefined relative positions. The object description is \"[front/rear/left/right] area.\" Direction: There are two same-shape directional planes and one sponge in the scene initialization. One plane is horizontal to the width of table while another is vertical. The plane description is \"[horizontal/vertical] area.\" Shape: There are one sponge and more than two planes with different shapes in the scene initialization. The number of planes varies from two to the length of the object library. The plane description is \"[shape] area.\" Object models: Two cabinets in seen scenes and one cabinet in unseen scenes. The cabinets have different appearance but all have three vertical drawers.\n\nVariations and scene settings: The cabinet is randomly changing the pose in the workspace for each demonstration.\n\nLevel, Action Type, and Amount: One cabinet is sampled from the object list. These variations are all working on the drawer at the same time. For example, we regard \"Fully open the top drawer.\" as one situation.\n\n\nA.8 Use Door\n\nTask Definition: The agent needs to execute the correct action with appropriate degrees for the door. If the door is closed, the agent needs to rotate the door handle to unlock it for opening it. Like the use drawer tasks, the initial states will change according to the action type.  (M2[a4,b4],door handle joint), the agent needs to rotate the joint, which connects the handle and plank, to a degree for unlock. Therefore, the goal conditions are a set of the end-effector poses whose positions are along an arc, and the orientations are depended on the positions. For ((M2[a4,b4], door plank joint), the goal conditions are the same as the previous one, but the joint connects the plank and base.\n\nSuccess Conditions. The successor will be triggered when the plank and base joint has reached a particular angle. Object models: One door in seen scenes and one door in unseen scenes. The doors have different appearance but all have the rotatable handle.\n\nVariations and scene settings: The door is randomly changing the pose in the workspace for each demonstration.\n\nAction Type and Amount: One door is sampled from the object list. These variations are all working on the door at the same time. For example, we regard \"Fully open the door.\" as one situation.\n\nB Implementation Details B.1 AMSolver Task Creation To generate demonstrations for semantic tasks, we need to use the predefined unit task sequence combined with object configurations and goal conditions. The goal conditions are the feasible goal pose sets of the current manipulated object for each unit task. The goal conditions can be formatted as bounding boxes, planes, lines, joint angles, or generated by functions for special pose sets. For example, a rearrangement task \"Put the ball into the basket\" needs to transit a ball into the empty space of the basket without any constraints requirement, which means the goal condition of the ball is a set of poses inside the blank space of basket. Then, we can format this task as (Control, ball) + (M 1[a1, b1], ball). Note that ball can be replaced with other objects, so (Control, object) + (M 1[a1, b1], object) can represent any pick-and-place task without specific constraints. Another example is \"Pour the water from one mug to another\". This task needs to keep the mug opening upward during the move towards the top of another mug. We can format this task as (Control, source mug) + (M 2[a1, b2], source mug) + (M 2[a4, b4], source mug). For the first (M 2[a1, b2], source mug), the goal condition should make the horizontal distance between the geometry centers of two mugs less than the half-height of the source mug, and the vertical distance should be larger than the height of the container mug. Therefore, the goal condition is a set of poses, calculated by a function using the geometry and poses of two mugs.\n\nImplementation The AMSolver is built inside the CoppeliaSim [23] environment and written in Python and Lua based on RLbench [10] and PyRep [9]. The agent is a Franka Emika Panda robot arm with 7 DoF, standing on a wooden table. To achieve the grasping ability for any object shape, we implement one version of the algorithm of ten Pas et al. [20]. The generator will calculate the normals and principal axis of partial point clouds. Then, the normals, principal axis, and crossproduct will establish the grasp poses. Finally, the agent will calculate inverse kinematic to check the validation. More details can be found in [20]. Each unit task will generate one waypoint as the sub-goal of the end-effector or one fixed cartesian path as the fixed sub-trajectory for the end-effector in the environment. We use the OMPL library [29] to find a valid trajectory between two waypoint configurations. In the end, we will get a valid path from the beginning to the task finish, consisting of the critical waypoints information. The object detector used in the simulator is a 3D box, where the poses (positions and orientations) and properties (length, width, height) can be modified. The object detector is placed in the target destinations of the task. When the target object mesh has overlapped with the detector, it will return the true value for checking the object.\n\n\nB.2 VLMbench\n\nDataset Generation We collect the VLMbench dataset in the environment of RLbench with AM-Solver. There are five RGB-D cameras in the environment: front view, left view, right view, overhead view, and wrist view. We use the image resolution of 360 \u00d7 360 in this dataset. As mentioned in section 3.4, the AMSolver will output the trajectories with waypoints. Therefore, the robot arm will use motion planning to transfer from one waypoint to another, and the simulator will record the observation with the time step 50 ms. Each camera can produce an RGB-D image with point cloud and instance masks for the observation. We also record the low-level states for the robot joints, including joint velocity, joint torque, joint position, and end-effector pose. In addition, the observations record the object pose at each time step and the corresponding relationship between the frames and waypoints so that the demonstrations can be automatically divided into the sub-demonstrations Figure 5: The environment used in VLMbench. A 7 DoF robot arm stands on the table with five cameras from different views. Meanwhile, the images from the camera views are shown aside. We also can see the predefine of the paths in the environment, which are the ground truth waypoints generated by the AMSolver.\n\nwith the critical frame. The objects will change the poses among different demonstrations, and the distractors will be randomly added to the environment.\n\nFor language instructions, we predefined some templates for each task category to quickly generate various language instructions by filling the object properties' descriptions, shown in Fig. 3. For example, the \"Pick & Place objects\" task has the template \"Pick [Object] and place it into [Container],\" where [Object] denotes the target object descriptions corresponding to variations mentioned above. Meanwhile, by defining the structured language templates on the unit task, we can simultaneously generate the low-level instructions in the dataset, which structurally describe the basic actions corresponding to the frames, e.g. \"Move to [relative position] [manipulated object]; Grasp [manipulated object]\" correspond to the pre-grasp action and grasp action in the Control unit task. We hope this information will be helpful for further research in this area.\n\n\nB.3 6D-CLIPort\n\nIn this section, we provide hyperparameter values in our 6D-CLIPort training. We have trained each agent on eight A5000 GPUs for one hour with a batch size of 16, an epoch of 15, and a learning rate of 1e-3. The pixel-wise feature maps from the attention module have one dimension. The output feature maps of the value and key module have four dimensions in each pixel to estimate 2D positions and the rotation along the perpendicular axis. Meanwhile, The output feature maps have 12 dimensions in each pixel for the other three regressors and are chunked into three parts for separate convolutions, where the convolutions of value and key feature maps are input to each regressor.\n\n\nC Dataset Details\n\nWe sample VLMbench in the CoppeliaSim simulator, shown in Fig. 5. The simulator collects the data with 20 Hz frequency, and at each step, the state record all visual, robot, and objects information.\n\nTo illuminate the dataset's structure, we have a folder for each instance-level task, e.g., pouring water with color variation, and each variation value creates a folder under the task folder, e.g., the target mug is red in the pouring water with color variation. Inside one variation value folder, we have demonstration folders that contain the state record for demonstrations. Then, we save the RGB-D, segmentation, and point cloud from each camera in one demonstration folder. Another file contains all low-level robot states, object states, and waypoint information for every step. All instance-level tasks are shown in Fig. 6. Demonstrations demos can be found in the video on the website.   Color  400  100  25  100  100  Relative  80  20  20  100  100  Shape  80  20  20  100  100  Size  40  10  10  100  100  Direction  40  10  10  100  100   Pour   Color  400  100  25  100  100  Relative  80  20  20  100  100  Size  40  10  10  100  100  Door  action&amount  80  20  20  100  100  Door  action&amount&level  240  60  60  96  96  Total  4680 1170  705  2380  2380 Task Statistics In total, we have 24 instance-level tasks with 234 variations, which include 120 color variations, 18 size variations, 60 relative position variations, 18 shape variations, 2 direction variations, 16 variations of the combinations of level, amount, and action type. The benchmark have 6,555 manipulation demonstrations which contain 4,680 train demonstrations, 1,170 seen validation demonstrations, and 705 unseen validation demonstrations. Since the agent test in the online simulator, we have generated 2,380 seen test settings and 2,380 unseen test settings for all instance-level tasks. The detail numbers can be found in Table 7. All demonstrations are generated by two servers with 64-Core CPUs and 8 GPUs within 24 hours.\n\n\nD Additional Experiments\n\nWe have shown task success rates of each baseline agent in both seen and unseen settings. Here, we provide the results of step success for both seen and unseen settings. Then, we want to provide more explanation and analysis of failure cases and ablations study.\n\n\nD.1 Goal-Conditioned Success Rates\n\nIn Table 8 and 9, we show the success rates of each step for every agent. For tasks in the VLMbench, each task can be considered as a multi-step task, since each task at least consists of one grasping step and one following movement step. Therefore, we used other two metrics for the step's success: Goal-condition Grasp (G-G) success rate and Goal-conditioned Movement (G-M) success rate. The Goal-conditioned Grasp (G-G) success rates indicate whether the agent has grasped the correct object for the first step. The Goal-conditioned Movement (G-M) success rates indicate whether the agent can satisfy the success conditions in the following movement by using the pre-generated grasp poses       Table 9: Results of all variations. The notations are used the same as in Table 3. for the grasping step. We can find that the agent has high grasping successes in both seen and unseen settings for most tasks, including pick, stack, drop, shape sorter, pour, and wiper, which indicates that 6D-CLIPort can successfully reason the correct grasping objects with both seen and unseen settings. For these tasks, the prominent failure cases come from the following movement, where the conclusion also can be gotten from the minor difference between the goal-conditioned movement and total success rate. In the wiping tasks, since we only have one sponge shape model in both training and testing, we can see a high grasping success rates on all settings. We also can find that the Vision-Only agent has a higher grasping success rate than 6D-CLIPort in the \"Drawer\" tasks with seen settings, but it has a much lower grasping success rate in the unseen settings, which indicates that the Vision-Only agent has overfitted on the grasping step in the \"Drawer\" tasks. For opening/closing drawer and door tasks, we find that goal-conditioned movement success rates are similar for seen and unseen settings, meaning the agent performs similarly after the agent has correct grasping. In opening/closing drawer tasks, we also find that the goal-condition grasp is smaller than total success, which means some tasks reach the success conditions without grasping the handle of doors. It also makes sense that closing the drawer can be finished by pushing instead of grasping the handle first.\nG-G G-M SC G-G G-M SC G-G G-M SC G-G G-M SC G-G G-M SC G-G G-M SC G-G G-M SC G-G G-MG-G G-M SC G-G G-M SC G-G G-M SC G-G G-M SC G-G G-M SC G-G G-M SC G-G G-M SC G-G G-M SC Language-G-G G-M SC G-G G-M SC G-G G-M SC G-G G-M SC G-G G-M SC G-G G-M SC G-G G-M SC G-G G-MG-G G-M SC G-G G-M SC G-G G-M SC G-G G-M SC G-G G-M SC G-G G-M SC G-G G-M SC G-G G-M\n\nGround Truth Demonstration\n\n\n6D-CLIPort Estimation\n\n\n6D-CLIPort Estimation With GT Orientations\n\n\nWaypoint Positions\n\nWaypoint Orientations Figure 8: 6D-CLIPort with ground truth waypoint information. The instruction here is \"Pour water from the blue mug to the green mug.\" Since the orientations and positions are unmatched, we can see 6D-CLIPort with ground truth orientation can pour the water but have collisions with the container mug. In this situation, 6D-CLIPort with ground truth positions cannot find a valid path by motion planning.\n\n\nD.2 Failure Cases\n\nThe results show that the 6D-CLIPort agent has failed in many tasks. Except the unreachable poses due to the errors in the position and orientation estimations, we summarize the four kinds of failure cases, shown in Fig. 7: (a) Wrong grasping pose, which means the agent cannot grasp any object. (b) Wrong grasping object, which means the agent grasps the incorrect object. (c) Wrong destination pose, which means the destination pose of the agent cannot finish any semantic task. (d) Wrong destination object, which means the estimation destination is inconsistent with the instructions. More visualization can be found in the attached video.\n\n\nD.3 Partially Modal Agents\n\nIn the experiment section, we have tested the 6D-CLIPort with the ground truth waypoints' positions or orientations. We can see the agent still fail the tasks even with this privileged information. The reason is the unmatched positions and orientations. The ground truth positions and orientations are obtained from the pre-generated waypoints. Therefore, this ground truth information is not the optimal parameter associated with the estimation. The unmatched positions and orientations can lead to a failure or even an unreachable pose where the motion planner cannot find a feasible path. For the tasks that need the cooperation of position and orientation such as dropping, pouring, and opening the door, we can figure out that giving partially ground truth still cannot finish the task correctly. We shown a example in Fig 8.Since the estimation trajectory and ground truth trajectory can be valid but different solutions for the same task. The combination of the positions and orientations will fail the task. Furthermore, the wrong estimation of positions can lead to task failure even given the ground truth orientation since the tasks in the VLMbench require correct compositional reasoning. More visualization can be found in the video on the project web page.\n\nFigure 4 :\n4The structure of 6D-CLIPort. Please refer to Sec. 5 for details.\n\n\nalso compares 6D-CLIPort with Language-Only and Vision-Only agents, demonstrating the impact of different input modalities. (1) The baseline visionand-language manipulation model 6D-CLIPort performs the best on all tasks, showing the importance of both visual observations and language guidance in VLMbench tasks. (2) The Language-Only agent nearly fails at all the tasks as it is visually blind and thus unable to localize the objects in the 3D space. For \"Drawer\" tasks, since language instructions can provide the level information (e.g., top, middle, and bottom) and action directions (e.g., open and close), the Language-Only agent has some\n\nInstruction\nTemplates: High-level instructions: \"Pick up [target object description] and place it into [target container description].\"; Low-level instructions: (\"Move to the top of [target object description]; Grasp [target object description].\", \"Move the object into [target container description]; Release the gripper.\").\n\nInstruction\nTemplates. High-level instructions: \"Stack [below object description] and [above object description] in sequence.\"; Low-level instructions: (\"Move to the top of [above object description]; Grasp [above object description].\", \"Move the object on [below object description]; Release the gripper.\").\n\nTask\nTemplates. Unit task sequence: (Control, target pencil)+(M1[a3,b2], target pencil); Goal Conditions: A line, which is perpendicular to the opening of target container, with orientation constraints along the line, for (M1[a3,b2], target pencil).Success Conditions. The object detector attached to the target container will be triggered when the specific pencil is inside.Instruction Templates. High-level instructions: \"Drop [target pencil description] into [target container description].\"; Low-level instructions: (\"Move to the top of [target pencil description]; Grasp [target pencil description].\", \"Move the object above [target container description]; Release the gripper.\").\n\n\nVariations: Color, Shape, and Relative Position. These variations work on all objects except the sorter. Task Templates. Unit task sequence: (Control, target pencil)+(M1[a4,b3], target object); Goal Conditions: A fixed pose related to the sorter for each object shape, for (M1[a4,b3], target object).Success Conditions. The object detector attached to the sorter will be triggered when the specific object is inside.Instruction Templates. High-level instructions: \"Put [target object description] through the hole of [target hole description].\"; Low-level instructions: (\"Move to the top of [target object description]; Grasp [target object description].\", \"Move the object to the hole of [target hole description]; Release the gripper.\").\n\nInstruction\nTemplates. High-level instructions: \"Pour the water from [source mug description] to [container mug description].\"; Low-level instructions: (\"Move to the [relative position] of [source mug description]; Grasp [source mug description].\", \"Move the object to the top of [container mug description] with the opening upwards.\", \"Rotate [source mug description] toward [container mug description]\").\n\nTask\nTemplates. Unit task sequence: (Control, sponge)+(M1[a4,b2], sponge) +(M2[a2, b2], sponge); Goal Conditions: For (M1[a4,b2], sponge), the sponge needs to move to one side of the target area with the surface face contacting the area. For (M2[a2, b2], sponge), the sponge needs to move from the current side to another side with keeping contact. The goal conditions are both a set of poses with the fixed position and rotations along the axis perpendicular to the table.\n\nInstruction\nTemplates. High-level instructions: \"Wipe [target area description] with a sponge.\"; Low-level instructions: (\"Move to the top of the sponge; Grasp the sponge.\", \"Move the object to the side of [target area description].\", \"Move the object along the main direction of [target area description]\").\n\n\nThe agent needs to figure out the exact level of the drawers to use and execute the correct action with appropriate degrees. The initial states of the drawer will change according to the action types. For example, if the task is to open the top drawer, the top drawer will be closed at first.Task Templates. Unit task sequence: (Control, target drawer handle)+(M2[a3,b3], target drawer joint); Goal Conditions: For (M2[a3,b3], target drawer), the agent needs to move the target drawer to a fixed position with the linear physic constraints on the joints of the drawer. Therefore, the goal conditions are a set of the end-effector poses whose positions are along an axis, and the orientations are fixed. Success Conditions. The successor will be triggered when the joint of the target drawer have moved a particular length. Instruction Templates. High-level instructions: \"[Amount] [Action Type] the [Level] drawer.\"; Low-level instructions: (\"Move to the front of the handle of [target drawer description]; Grasp the handle of [target drawer description].\", \"Move along the axis of [target drawer description] for [Amount] [Action Type].\").\n\nInstruction\nTemplates. High-level instructions: \"[Amount] [Action Type] the door.\"; Low-level instructions: (\"Move to the front of the handle of the door; Grasp the handle of the door.\", \"Rotate around the axis of the handle joint.\", \"Rotate around the axis of the door joint for [Amount] [Action Type].\").\n\nFigure 6 :\n6All instance-level tasks in VLMbench.\n\n( b )Figure 7 :\nb7Wrong Grasping Object \"Pick up the cylinder and place it into the yellow container.\" (a) Wrong Grasping Pose \"Drop the pink pencil into the pink container.\" (c) Wrong Destination Pose \"Pour water from the green mug to the gray mug.\" (d) Wrong Destination Object \"Stack the green cube and the navy cube in sequence.\" Four failure cases in the results of the 6D-CLIPort. Details can be found in D.2.\n\nTable 2 :\n2The table contains the category-level tasks in our dataset, with their main constraints, variations and instruction samples.Input Data \n\nEncoding Module \n\n\"Stack the \nblack cube \nand the \nyellow cube \nin sequence. \nStep Zero.\" \n\nCLIP-\nResNet50 \n\nCLIP-\nTransformer \n\nTransporter \nResNet \n\nFeature \nFusion & \nDecoder \n\nRGB-D \n\nRGB \n\nLanguage \n\nPixel-wise \nFeature \nMap \n\nAttention \nModule \n\nValue \nModule \n\nKey \nModule \n\nZ \nRegressor \n\nRoll \nRegressor \n\nPitch \nRegressor \n\nConvolution \n\n6 DoF \nPose \n\nRGB-D \nRGB \nLanguage \n\nInput Data \n\nRotate \n\nLanguage \n\nArgmax \n\nX \nY \nYaw \n\nZ \nRoll \nPitch \n\n\n\nTable 5 :\n5All task variations except shape used in VLMbench. The shape variation of each task can be found in the detail descriptions of each task category.Variations Totals \nValues \n\nColor \n25 \n\nseen:red, maroon, lime, green, blue,navy, yellow, cyan, magenta, \nsilver, gray, olive, purple, teal, azure, violet, rose, black, white \nunseen: brown, gold, pink, chocolate, coral \n\nSize \n5 \nlarger, smaller, large, medium, small \n\nRelative \nPosition \n5 \ntop, front, rear, left, right \n\nLevel \n3 \ntop, middle, bottom \n\nAmount \n2 \nfully, slightly \n\nAction \nType \n2 \nopen, close \n\n\n\nTable 6 :\n6All object models used in VLMbench. The number behind the object class indicate the instance number of that class.Object type \nNumber of classes \nClasses \n\n\n\nTable 7 :\n7The number of episodes for each task in the dataset.Task category Variation \nTrain \nValid \nTest \n\nSeen Unseen Seen Unseen \n\nPick \n\nColor \n400 \n100 \n25 \n100 \n100 \nRelative \n320 \n80 \n80 \n96 \n96 \nShape \n100 \n25 \n20 \n100 \n100 \nSize \n80 \n20 \n20 \n100 \n100 \n\nStack \n\nColor \n400 \n100 \n25 \n100 \n100 \nRelative \n320 \n80 \n80 \n96 \n96 \nShape \n100 \n25 \n20 \n100 \n100 \nSize \n120 \n30 \n30 \n96 \n96 \n\nDrop \n\nColor \n400 \n100 \n25 \n100 \n100 \nRelative \n320 \n80 \n80 \n96 \n96 \nSize \n80 \n20 \n20 \n100 \n100 \n\nPlace \n\nColor \n400 \n100 \n25 \n100 \n100 \nRelative \n80 \n20 \n20 \n100 \n100 \nShape \n80 \n20 \n25 \n100 \n100 \n\nWipe \n\n\n\n\n6D-CLIPort (GT Ori) 60.61 42.93 28.03 65.40 36.87 26.26 56.38 44.64 26.53 51.02 48.98 26.02 65.88 31.08 17.91 62.16 28.72 16.22 72.67 32.67 24.00 67.33 24.00 15.67 6D-CLIPort (GT Pos) 94.95 88.64 83.84 95.96 83.33 75.25 96.94 69.39 58.93 96.43 47.70 50.51 95.61 13.85 16.89 94.26 13.18 11.82 92.00 19.33 18.00 94.00 22.67 17.33SC \nLanguage-Only \n0.00 \n0.00 \n0.00 \n0.00 \n0.00 \n0.00 \n0.00 \n0.00 \n0.00 \n0.00 \n0.00 \n0.00 \n0.00 \n0.00 \n0.00 \n0.00 \n0.00 \n0.00 \n0.00 \n0.00 \n0.00 \n0.00 \n0.00 \n0.00 \nVision-Only \n29.55 29.80 \n6.31 36.62 26.52 \n9.85 \n34.95 32.65 \n6.89 32.65 22.45 \n1.79 10.47 \n0.00 \n0.00 16.89 \n0.00 \n0.00 \n8.33 \n8.67 \n0.00 \n4.00 \n6.33 \n0.33 \n6D-CLIPort \n58.33 43.33 28.28 63.14 41.16 27.53 \n53.06 44.64 22.19 48.21 28.06 18.37 70.61 \n9.12 \n6.42 65.88 \n8.78 \n6.42 73.00 23.67 17.33 66.67 18.67 12.33 \n\nPour \nWipe \nDoor \nDrawer \n\nSeen \nUnseen \nSeen \nUnseen \nSeen \nUnseen \nSeen \nUnseen \n\n\n\nTable 8 :\n8Results of all tasks, including both seen and unseen settings. In the table, G-G denotes Goal-conditioned Grasp success rate, which reflect whether the agent can correctly grasp the target. G-M denotes Goal-conditioned Movement success rate, which reflect whether the agent can finish the task by using the pre-generated grasping poses. SC denotes success rates for the whole estimation trajectory. More explanations can be found in section D.1.Color \nShape \nSize \nRelative Position \n\nAgent \nSeen \nUnseen \nSeen \nUnseen \nSeen \nUnseen \nSeen \nUnseen \n\n\n\n\n6D-CLIPort (GTOri) 99.00 26.00 26.00 98.00 29.00 27.00 31.25 30.00 23.96 8.33 23.96 17.71 40.31 22.45 14.80 6.63 19.39 11.22 40.31 22.45 14.80 6.63 19.39 11.22 6D-CLIPort (GT Pos) 100.00 63.00 53.00 100.00 56.00 41.00 79.17 43.75 43.75 87.50 50.00 52.08 79.59 44.90 35.20 85.20 49.49 39.29 79.59 44.90 35.20 85.20 49.49 39.29SC \nLanguage-Only \n0.00 \n0.00 \n0.00 \n0.00 \n0.00 \n0.00 \n5.21 15.63 \n4.17 \n3.13 13.54 \n1.04 \n2.55 \n7.65 \n2.04 \n1.53 \n6.63 \n0.51 \n2.55 \n7.65 \n2.04 \n1.53 \n6.63 \n0.51 \nVision-Only \n99.00 15.00 21.00 \n95.00 22.00 24.00 26.04 21.88 14.58 \n1.04 18.75 \n7.29 12.76 10.71 \n7.14 \n0.51 \n9.18 \n3.57 12.76 10.71 \n7.14 \n0.51 \n9.18 \n3.57 \n6D-CLIPort \n98.00 19.00 21.00 100.00 22.00 26.00 22.92 25.00 22.92 \n8.33 21.88 15.63 26.02 19.39 14.29 \n6.63 18.37 10.20 26.02 19.39 14.29 \n6.63 18.37 10.20 \n\n\nAMSolver is implemented in CoppeliaSim[23] (Free Educational License) and codes are based on RLbench[10] (RLBench Software License) and PyRep[9] (MIT License).\n6D-CLIPort is implemented based on CLIPort[25] (Apache-2.0 License)\n\nO Ahmed, F Tr\u00e4uble, A Goyal, A Neitz, Y Bengio, B Sch\u00f6lkopf, M W\u00fcthrich, S Bauer, arXiv:2010.04296Causalworld: A robotic manipulation benchmark for causal structure and transfer learning. arXiv preprintAhmed, O., Tr\u00e4uble, F., Goyal, A., Neitz, A., Bengio, Y., Sch\u00f6lkopf, B., W\u00fcthrich, M., Bauer, S.: Causalworld: A robotic manipulation benchmark for causal structure and transfer learning. arXiv preprint arXiv:2010.04296 (2020)\n\nVision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments pp. P Anderson, Q Wu, D Teney, J Bruce, M Johnson, N S\u00fcnderhauf, I Reid, S Gould, Van Den, A Hengel, Anderson, P., Wu, Q., Teney, D., Bruce, J., Johnson, M., S\u00fcnderhauf, N., Reid, I., Gould, S., Van Den Hengel, A.: Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments pp. 3674-3683 (2018)\n\nS Antol, A Agrawal, J Lu, M Mitchell, D Batra, C L Zitnick, D Parikh, Vqa: Visual question answering pp. Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L., Parikh, D.: Vqa: Visual question answering pp. 2425-2433 (2015)\n\nImram: Iterative matching with recurrent attention memory for cross-modal image-text retrieval pp. H Chen, G Ding, X Liu, Z Lin, J Liu, J Han, Chen, H., Ding, G., Liu, X., Lin, Z., Liu, J., Han, J.: Imram: Iterative matching with recurrent attention memory for cross-modal image-text retrieval pp. 12655-12663 (2020)\n\nEmbodied question answering pp. A Das, S Datta, G Gkioxari, S Lee, D Parikh, D Batra, Das, A., Datta, S., Gkioxari, G., Lee, S., Parikh, D., Batra, D.: Embodied question answering pp. 1-10 (2018)\n\nManipulathor: A framework for visual object manipulation pp. K Ehsani, W Han, A Herrasti, E Vanderbilt, L Weihs, E Kolve, A Kembhavi, R Mottaghi, Ehsani, K., Han, W., Herrasti, A., VanderBilt, E., Weihs, L., Kolve, E., Kembhavi, A., Mottaghi, R.: Manipulathor: A framework for visual object manipulation pp. 4497-4506 (2021)\n\nVideo captioning with attention-based lstm and semantic consistency. L Gao, Z Guo, H Zhang, X Xu, H T Shen, IEEE Transactions on Multimedia. 199Gao, L., Guo, Z., Zhang, H., Xu, X., Shen, H.T.: Video captioning with attention-based lstm and semantic consistency. IEEE Transactions on Multimedia 19(9), 2045-2055 (2017)\n\nP Goyal, R J Mooney, S Niekum, arXiv:2106.02972Zero-shot task adaptation using natural language. arXiv preprintGoyal, P., Mooney, R.J., Niekum, S.: Zero-shot task adaptation using natural language. arXiv preprint arXiv:2106.02972 (2021)\n\nS James, M Freese, A J Davison, arXiv:1906.11176Pyrep: Bringing v-rep to deep robot learning. arXiv preprintJames, S., Freese, M., Davison, A.J.: Pyrep: Bringing v-rep to deep robot learning. arXiv preprint arXiv:1906.11176 (2019)\n\nRlbench: The robot learning benchmark & learning environment. S James, Z Ma, D R Arrojo, A J Davison, IEEE Robotics and Automation Letters. 52James, S., Ma, Z., Arrojo, D.R., Davison, A.J.: Rlbench: The robot learning benchmark & learning environment. IEEE Robotics and Automation Letters 5(2), 3019-3026 (2020)\n\nGenerative language-grounded policy in vision-and-language navigation with bayes' rule. S Kurita, K Cho, arXiv:2009.07783arXiv preprintKurita, S., Cho, K.: Generative language-grounded policy in vision-and-language navigation with bayes' rule. arXiv preprint arXiv:2009.07783 (2020)\n\nIgibson 2.0: Object-centric simulation for robot learning of everyday household tasks. C Li, F Xia, R Mart\u00edn-Mart\u00edn, M Lingelbach, S Srivastava, B Shen, K Vainio, C Gokmen, G Dharan, T Jain, arXiv:2108.03272arXiv preprintLi, C., Xia, F., Mart\u00edn-Mart\u00edn, R., Lingelbach, M., Srivastava, S., Shen, B., Vainio, K., Gokmen, C., Dharan, G., Jain, T., et al.: Igibson 2.0: Object-centric simulation for robot learning of everyday household tasks. arXiv preprint arXiv:2108.03272 (2021)\n\nR Liu, C Liu, Y Bai, A L Yuille, Clevr-ref+: Diagnosing visual reasoning with referring expressions pp. Liu, R., Liu, C., Bai, Y., Yuille, A.L.: Clevr-ref+: Diagnosing visual reasoning with referring expressions pp. 4185-4194 (2019)\n\nStructformer: Learning spatial structure for languageguided semantic rearrangement of novel objects. W Liu, C Paxton, T Hermans, D Fox, arXiv:2110.10189arXiv preprintLiu, W., Paxton, C., Hermans, T., Fox, D.: Structformer: Learning spatial structure for language- guided semantic rearrangement of novel objects. arXiv preprint arXiv:2110.10189 (2021)\n\nVilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. J Lu, D Batra, D Parikh, S Lee, Advances in neural information processing systems. 32Lu, J., Batra, D., Parikh, D., Lee, S.: Vilbert: Pretraining task-agnostic visiolinguistic represen- tations for vision-and-language tasks. Advances in neural information processing systems 32 (2019)\n\nLanguage conditioned imitation learning over unstructured data. C Lynch, P Sermanet, arXiv:2005.07648arXiv preprintLynch, C., Sermanet, P.: Language conditioned imitation learning over unstructured data. arXiv preprint arXiv:2005.07648 (2020)\n\nA Mandlekar, D Xu, J Wong, S Nasiriany, C Wang, R Kulkarni, L Fei-Fei, S Savarese, Y Zhu, R Mart\u00edn-Mart\u00edn, arXiv:2108.03298What matters in learning from offline human demonstrations for robot manipulation. arXiv preprintMandlekar, A., Xu, D., Wong, J., Nasiriany, S., Wang, C., Kulkarni, R., Fei-Fei, L., Savarese, S., Zhu, Y., Mart\u00edn-Mart\u00edn, R.: What matters in learning from offline human demonstrations for robot manipulation. arXiv preprint arXiv:2108.03298 (2021)\n\nWhat matters in language conditioned robotic imitation learning. O Mees, L Hermann, W Burgard, arXiv:2204.06252arXiv preprintMees, O., Hermann, L., Burgard, W.: What matters in language conditioned robotic imitation learning. arXiv preprint arXiv:2204.06252 (2022)\n\nCalvin: A benchmark for languageconditioned policy learning for long-horizon robot manipulation tasks. O Mees, L Hermann, E Rosete-Beas, W Burgard, IEEE Robotics and Automation Letters (RA-L). 73Mees, O., Hermann, L., Rosete-Beas, E., Burgard, W.: Calvin: A benchmark for language- conditioned policy learning for long-horizon robot manipulation tasks. IEEE Robotics and Automation Letters (RA-L) 7(3), 7327-7334 (2022)\n\nGrasp pose detection in point clouds. A Ten Pas, M Gualtieri, K Saenko, R Platt, The International Journal of Robotics Research. 36ten Pas, A., Gualtieri, M., Saenko, K., Platt, R.: Grasp pose detection in point clouds. The International Journal of Robotics Research 36(13-14), 1455-1473 (2017)\n\nEpisodic transformer for vision-and-language navigation pp. A Pashevich, C Schmid, C Sun, Pashevich, A., Schmid, C., Sun, C.: Episodic transformer for vision-and-language navigation pp. 15942-15952 (2021)\n\nLearning transferable visual models from natural language supervision pp. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from natural language supervision pp. 8748-8763 (2021)\n\nV-rep: A versatile and scalable robot simulation framework pp. E Rohmer, S P N Singh, M Freese, 10.1109/IROS.2013.6696520Rohmer, E., Singh, S.P.N., Freese, M.: V-rep: A versatile and scalable robot simulation framework pp. 1321-1326 (2013). https://doi.org/10.1109/IROS.2013.6696520\n\nConcept2robot: Learning manipulation concepts from instructions and human demonstrations. L Shao, T Migimatsu, Q Zhang, K Yang, J Bohg, The International Journal of Robotics Research. 40Shao, L., Migimatsu, T., Zhang, Q., Yang, K., Bohg, J.: Concept2robot: Learning manipulation concepts from instructions and human demonstrations. The International Journal of Robotics Research 40(12-14), 1419-1434 (2021)\n\nCliport: What and where pathways for robotic manipulation pp. M Shridhar, L Manuelli, D Fox, Shridhar, M., Manuelli, L., Fox, D.: Cliport: What and where pathways for robotic manipulation pp. 894-906 (2022)\n\nALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. M Shridhar, J Thomason, D Gordon, Y Bisk, W Han, R Mottaghi, L Zettlemoyer, D Fox, Shridhar, M., Thomason, J., Gordon, D., Bisk, Y., Han, W., Mottaghi, R., Zettlemoyer, L., Fox, D.: ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks (2020), https://arxiv.org/abs/1912.01734\n\nBehavior: Benchmark for everyday household activities in virtual. S Srivastava, C Li, M Lingelbach, R Mart\u00edn-Mart\u00edn, F Xia, K E Vainio, Z Lian, C Gokmen, S Buch, K Liu, Srivastava, S., Li, C., Lingelbach, M., Mart\u00edn-Mart\u00edn, R., Xia, F., Vainio, K.E., Lian, Z., Gokmen, C., Buch, S., Liu, K., et al.: Behavior: Benchmark for everyday household activities in virtual, interactive, and ecological environments pp. 477-490 (2022)\n\nLanguage-conditioned imitation learning for robot manipulation tasks. S Stepputtis, J Campbell, M Phielipp, S Lee, C Baral, H B Amor, Stepputtis, S., Campbell, J., Phielipp, M., Lee, S., Baral, C., Amor, H.B.: Language-conditioned imitation learning for robot manipulation tasks (2020)\n\nI A \u015eucan, M Moll, L E Kavraki, The Open Motion Planning Library. \u015eucan, I.A., Moll, M., Kavraki, L.E.: The Open Motion Planning Library.\n\n. IEEE Robotics & Automation Magazine. 194IEEE Robotics & Automation Magazine 19(4), 72-82 (December 2012).\n\n. 10.1109/MRA.2012.2205651https://doi.org/10.1109/MRA.2012.2205651, https://ompl.kavrakilab.org\n\nHabitat 2.0: Training home assistants to rearrange their habitat. A Szot, A Clegg, E Undersander, E Wijmans, Y Zhao, J Turner, N Maestre, M Mukadam, D S Chaplot, O Maksymets, Advances in Neural Information Processing Systems. 34Szot, A., Clegg, A., Undersander, E., Wijmans, E., Zhao, Y., Turner, J., Maestre, N., Mukadam, M., Chaplot, D.S., Maksymets, O., et al.: Habitat 2.0: Training home assistants to rearrange their habitat. Advances in Neural Information Processing Systems 34 (2021)\n\nReconstruction network for video captioning pp. B Wang, L Ma, W Zhang, W Liu, Wang, B., Ma, L., Zhang, W., Liu, W.: Reconstruction network for video captioning pp. 7622-7631 (2018)\n\nK Yi, C Gan, Y Li, P Kohli, J Wu, A Torralba, J B Tenenbaum, arXiv:1910.01442Clevrer: Collision events for video representation and reasoning. arXiv preprintYi, K., Gan, C., Li, Y., Kohli, P., Wu, J., Torralba, A., Tenenbaum, J.B.: Clevrer: Collision events for video representation and reasoning. arXiv preprint arXiv:1910.01442 (2019)\n\nMeta-world: A benchmark and evaluation for multi-task and meta reinforcement learning pp. T Yu, D Quillen, Z He, R Julian, K Hausman, C Finn, S Levine, Yu, T., Quillen, D., He, Z., Julian, R., Hausman, K., Finn, C., Levine, S.: Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning pp. 1094-1100 (2020)\n\nSornet: Spatial object-centric representations for sequential manipulation pp. W Yuan, C Paxton, K Desingh, D Fox, Yuan, W., Paxton, C., Desingh, K., Fox, D.: Sornet: Spatial object-centric representations for sequential manipulation pp. 148-157 (2022)\n\nTransporter networks: Rearranging the visual world for robotic manipulation. A Zeng, P Florence, J Tompson, S Welker, J Chien, M Attarian, T Armstrong, I Krasin, D Duong, V Sindhwani, J Lee, Conference on Robot Learning (CoRL). Zeng, A., Florence, P., Tompson, J., Welker, S., Chien, J., Attarian, M., Armstrong, T., Krasin, I., Duong, D., Sindhwani, V., Lee, J.: Transporter networks: Rearranging the visual world for robotic manipulation. Conference on Robot Learning (CoRL) (2020)\n\nH Zhang, Y Lu, C Yu, D Hsu, X La, N Zheng, arXiv:2108.11092Invigorate: Interactive visual grounding and grasping in clutter. arXiv preprintZhang, H., Lu, Y., Yu, C., Hsu, D., La, X., Zheng, N.: Invigorate: Interactive visual grounding and grasping in clutter. arXiv preprint arXiv:2108.11092 (2021)\n\nContext-aware attention network for image-text retrieval pp. Q Zhang, Z Lei, Z Zhang, S Z Li, Zhang, Q., Lei, Z., Zhang, Z., Li, S.Z.: Context-aware attention network for image-text retrieval pp. 3536-3545 (2020)\n\nrobosuite: A modular simulation framework and benchmark for robot learning. Y Zhu, J Wong, A Mandlekar, R Mart\u00edn-Mart\u00edn, arXiv:2009.12293arXiv preprintZhu, Y., Wong, J., Mandlekar, A., Mart\u00edn-Mart\u00edn, R.: robosuite: A modular simulation frame- work and benchmark for robot learning. arXiv preprint arXiv:2009.12293 (2020)\n", "annotations": {"author": "[{\"end\":144,\"start\":76},{\"end\":193,\"start\":145},{\"end\":252,\"start\":194},{\"end\":299,\"start\":253},{\"end\":342,\"start\":300}]", "publisher": null, "author_last_name": "[{\"end\":88,\"start\":83},{\"end\":158,\"start\":154},{\"end\":217,\"start\":210},{\"end\":261,\"start\":258},{\"end\":304,\"start\":300}]", "author_first_name": "[{\"end\":82,\"start\":76},{\"end\":153,\"start\":145},{\"end\":199,\"start\":194},{\"end\":209,\"start\":200},{\"end\":257,\"start\":253}]", "author_affiliation": "[{\"end\":143,\"start\":108},{\"end\":192,\"start\":160},{\"end\":251,\"start\":219},{\"end\":298,\"start\":263},{\"end\":341,\"start\":306}]", "title": "[{\"end\":73,\"start\":1},{\"end\":415,\"start\":343}]", "venue": null, "abstract": "[{\"end\":1566,\"start\":417}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2238,\"start\":2234},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2240,\"start\":2238},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2243,\"start\":2240},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4033,\"start\":4029},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4036,\"start\":4033},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4039,\"start\":4036},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4041,\"start\":4039},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5016,\"start\":5012},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5898,\"start\":5894},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5968,\"start\":5965},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5983,\"start\":5979},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6000,\"start\":5997},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6017,\"start\":6013},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6032,\"start\":6028},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6046,\"start\":6042},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6059,\"start\":6055},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6071,\"start\":6067},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6276,\"start\":6273},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6386,\"start\":6382},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6404,\"start\":6400},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6547,\"start\":6543},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6707,\"start\":6703},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6815,\"start\":6812},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6942,\"start\":6938},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7227,\"start\":7223},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7246,\"start\":7242},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7538,\"start\":7535},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7540,\"start\":7538},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7562,\"start\":7559},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7565,\"start\":7562},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7592,\"start\":7588},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7594,\"start\":7592},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7749,\"start\":7745},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7770,\"start\":7766},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7876,\"start\":7873},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7879,\"start\":7876},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7882,\"start\":7879},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7885,\"start\":7882},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8016,\"start\":8012},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8019,\"start\":8016},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8022,\"start\":8019},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8573,\"start\":8569},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8762,\"start\":8758},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8942,\"start\":8938},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9011,\"start\":9007},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9123,\"start\":9119},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9257,\"start\":9253},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9404,\"start\":9400},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9549,\"start\":9546},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14825,\"start\":14821},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14828,\"start\":14825},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":16350,\"start\":16346},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":23217,\"start\":23213},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":23853,\"start\":23849},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":23946,\"start\":23942},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":24231,\"start\":24227},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29401,\"start\":29397},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":48147,\"start\":48143},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":48211,\"start\":48207},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":48225,\"start\":48222},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":48429,\"start\":48425},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":48710,\"start\":48706},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":48915,\"start\":48911},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":70140,\"start\":70136},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":70202,\"start\":70198},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":70242,\"start\":70239},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":70304,\"start\":70300}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":60054,\"start\":59977},{\"attributes\":{\"id\":\"fig_2\"},\"end\":60702,\"start\":60055},{\"attributes\":{\"id\":\"fig_4\"},\"end\":61029,\"start\":60703},{\"attributes\":{\"id\":\"fig_5\"},\"end\":61339,\"start\":61030},{\"attributes\":{\"id\":\"fig_6\"},\"end\":62026,\"start\":61340},{\"attributes\":{\"id\":\"fig_7\"},\"end\":62768,\"start\":62027},{\"attributes\":{\"id\":\"fig_8\"},\"end\":63176,\"start\":62769},{\"attributes\":{\"id\":\"fig_9\"},\"end\":63651,\"start\":63177},{\"attributes\":{\"id\":\"fig_10\"},\"end\":63961,\"start\":63652},{\"attributes\":{\"id\":\"fig_11\"},\"end\":65104,\"start\":63962},{\"attributes\":{\"id\":\"fig_12\"},\"end\":65412,\"start\":65105},{\"attributes\":{\"id\":\"fig_13\"},\"end\":65463,\"start\":65413},{\"attributes\":{\"id\":\"fig_14\"},\"end\":65880,\"start\":65464},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":66486,\"start\":65881},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":67063,\"start\":66487},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":67232,\"start\":67064},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":67831,\"start\":67233},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":68726,\"start\":67832},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":69288,\"start\":68727},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":70097,\"start\":69289}]", "paragraph": "[{\"end\":2709,\"start\":1582},{\"end\":3871,\"start\":2711},{\"end\":4792,\"start\":3873},{\"end\":5233,\"start\":4794},{\"end\":5401,\"start\":5235},{\"end\":5642,\"start\":5403},{\"end\":5757,\"start\":5644},{\"end\":7445,\"start\":5774},{\"end\":8362,\"start\":7447},{\"end\":9836,\"start\":8364},{\"end\":10865,\"start\":9880},{\"end\":11603,\"start\":10891},{\"end\":12242,\"start\":11605},{\"end\":13295,\"start\":12244},{\"end\":14769,\"start\":13297},{\"end\":15619,\"start\":14803},{\"end\":16194,\"start\":15658},{\"end\":16825,\"start\":16196},{\"end\":17726,\"start\":16903},{\"end\":18285,\"start\":17758},{\"end\":18544,\"start\":18287},{\"end\":19511,\"start\":18566},{\"end\":20872,\"start\":19531},{\"end\":21749,\"start\":20874},{\"end\":22203,\"start\":21792},{\"end\":23424,\"start\":22205},{\"end\":23687,\"start\":23453},{\"end\":24232,\"start\":23689},{\"end\":25151,\"start\":24234},{\"end\":26132,\"start\":25188},{\"end\":27135,\"start\":26134},{\"end\":27326,\"start\":27137},{\"end\":27895,\"start\":27328},{\"end\":28463,\"start\":27897},{\"end\":29402,\"start\":28483},{\"end\":30370,\"start\":29404},{\"end\":31662,\"start\":30467},{\"end\":31717,\"start\":31664},{\"end\":32296,\"start\":31748},{\"end\":32777,\"start\":32298},{\"end\":33176,\"start\":32798},{\"end\":34045,\"start\":33178},{\"end\":34506,\"start\":34074},{\"end\":34927,\"start\":34530},{\"end\":36210,\"start\":34929},{\"end\":36872,\"start\":36232},{\"end\":37176,\"start\":36874},{\"end\":38298,\"start\":37178},{\"end\":38577,\"start\":38318},{\"end\":39555,\"start\":38579},{\"end\":40048,\"start\":39585},{\"end\":40158,\"start\":40050},{\"end\":40839,\"start\":40160},{\"end\":41026,\"start\":40858},{\"end\":41992,\"start\":41028},{\"end\":42222,\"start\":41994},{\"end\":42332,\"start\":42224},{\"end\":43544,\"start\":42334},{\"end\":43665,\"start\":43557},{\"end\":44896,\"start\":43667},{\"end\":45011,\"start\":44898},{\"end\":45224,\"start\":45013},{\"end\":45940,\"start\":45241},{\"end\":46196,\"start\":45942},{\"end\":46308,\"start\":46198},{\"end\":46502,\"start\":46310},{\"end\":48081,\"start\":46504},{\"end\":49448,\"start\":48083},{\"end\":50751,\"start\":49465},{\"end\":50906,\"start\":50753},{\"end\":51771,\"start\":50908},{\"end\":52471,\"start\":51790},{\"end\":52691,\"start\":52493},{\"end\":54511,\"start\":52693},{\"end\":54802,\"start\":54540},{\"end\":57115,\"start\":54841},{\"end\":58010,\"start\":57585},{\"end\":58675,\"start\":58032},{\"end\":59976,\"start\":58706}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":57200,\"start\":57116},{\"attributes\":{\"id\":\"formula_1\"},\"end\":57297,\"start\":57200},{\"attributes\":{\"id\":\"formula_2\"},\"end\":57381,\"start\":57297},{\"attributes\":{\"id\":\"formula_3\"},\"end\":57465,\"start\":57381}]", "table_ref": "[{\"end\":6095,\"start\":6088},{\"end\":19295,\"start\":19288},{\"end\":24428,\"start\":24421},{\"end\":27055,\"start\":27048},{\"end\":28606,\"start\":28599},{\"end\":29497,\"start\":29490},{\"end\":29700,\"start\":29693},{\"end\":29708,\"start\":29701},{\"end\":30636,\"start\":30617},{\"end\":33501,\"start\":33495},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":33944,\"start\":33937},{\"end\":42926,\"start\":42914},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":53766,\"start\":53390},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":54416,\"start\":54409},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":54851,\"start\":54844},{\"end\":55546,\"start\":55539},{\"end\":55620,\"start\":55613}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1580,\"start\":1568},{\"attributes\":{\"n\":\"2\"},\"end\":5772,\"start\":5760},{\"attributes\":{\"n\":\"3\"},\"end\":9878,\"start\":9839},{\"attributes\":{\"n\":\"3.1\"},\"end\":10889,\"start\":10868},{\"attributes\":{\"n\":\"3.2\"},\"end\":14801,\"start\":14772},{\"attributes\":{\"n\":\"3.3\"},\"end\":15656,\"start\":15622},{\"attributes\":{\"n\":\"4\"},\"end\":16880,\"start\":16828},{\"attributes\":{\"n\":\"4.1\"},\"end\":16901,\"start\":16883},{\"end\":17756,\"start\":17729},{\"attributes\":{\"n\":\"4.2\"},\"end\":18564,\"start\":18547},{\"end\":19529,\"start\":19514},{\"attributes\":{\"n\":\"5\"},\"end\":21790,\"start\":21752},{\"end\":23451,\"start\":23427},{\"attributes\":{\"n\":\"6\"},\"end\":25165,\"start\":25154},{\"attributes\":{\"n\":\"6.1\"},\"end\":25186,\"start\":25168},{\"attributes\":{\"n\":\"6.2\"},\"end\":28481,\"start\":28466},{\"end\":30409,\"start\":30373},{\"end\":30465,\"start\":30412},{\"attributes\":{\"n\":\"7\"},\"end\":31746,\"start\":31720},{\"end\":32796,\"start\":32780},{\"end\":34072,\"start\":34048},{\"end\":34528,\"start\":34509},{\"end\":36230,\"start\":36213},{\"end\":38316,\"start\":38301},{\"end\":39583,\"start\":39558},{\"end\":40856,\"start\":40842},{\"end\":43555,\"start\":43547},{\"end\":45239,\"start\":45227},{\"end\":49463,\"start\":49451},{\"end\":51788,\"start\":51774},{\"end\":52491,\"start\":52474},{\"end\":54538,\"start\":54514},{\"end\":54839,\"start\":54805},{\"end\":57493,\"start\":57467},{\"end\":57517,\"start\":57496},{\"end\":57562,\"start\":57520},{\"end\":57583,\"start\":57565},{\"end\":58030,\"start\":58013},{\"end\":58704,\"start\":58678},{\"end\":59988,\"start\":59978},{\"end\":60715,\"start\":60704},{\"end\":61042,\"start\":61031},{\"end\":61345,\"start\":61341},{\"end\":62781,\"start\":62770},{\"end\":63182,\"start\":63178},{\"end\":63664,\"start\":63653},{\"end\":65117,\"start\":65106},{\"end\":65424,\"start\":65414},{\"end\":65480,\"start\":65465},{\"end\":65891,\"start\":65882},{\"end\":66497,\"start\":66488},{\"end\":67074,\"start\":67065},{\"end\":67243,\"start\":67234},{\"end\":68737,\"start\":68728}]", "table": "[{\"end\":66486,\"start\":66017},{\"end\":67063,\"start\":66645},{\"end\":67232,\"start\":67190},{\"end\":67831,\"start\":67297},{\"end\":68726,\"start\":68161},{\"end\":69288,\"start\":69184},{\"end\":70097,\"start\":69616}]", "figure_caption": "[{\"end\":60054,\"start\":59990},{\"end\":60702,\"start\":60057},{\"end\":61029,\"start\":60716},{\"end\":61339,\"start\":61043},{\"end\":62026,\"start\":61346},{\"end\":62768,\"start\":62029},{\"end\":63176,\"start\":62782},{\"end\":63651,\"start\":63183},{\"end\":63961,\"start\":63665},{\"end\":65104,\"start\":63964},{\"end\":65412,\"start\":65118},{\"end\":65463,\"start\":65426},{\"end\":65880,\"start\":65483},{\"end\":66017,\"start\":65893},{\"end\":66645,\"start\":66499},{\"end\":67190,\"start\":67076},{\"end\":67297,\"start\":67245},{\"end\":68161,\"start\":67834},{\"end\":69184,\"start\":68739},{\"end\":69616,\"start\":69291}]", "figure_ref": "[{\"end\":3201,\"start\":3193},{\"end\":4791,\"start\":4783},{\"end\":10080,\"start\":10072},{\"end\":10677,\"start\":10669},{\"end\":19308,\"start\":19302},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22258,\"start\":22250},{\"end\":41180,\"start\":41170},{\"end\":41251,\"start\":41241},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":41643,\"start\":41632},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":45536,\"start\":45526},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":45823,\"start\":45812},{\"end\":50450,\"start\":50442},{\"end\":51100,\"start\":51094},{\"end\":52557,\"start\":52551},{\"attributes\":{\"ref_id\":\"fig_13\"},\"end\":53323,\"start\":53317},{\"end\":57615,\"start\":57607},{\"end\":58254,\"start\":58248},{\"end\":59536,\"start\":59530}]", "bib_author_first_name": "[{\"end\":70328,\"start\":70327},{\"end\":70337,\"start\":70336},{\"end\":70348,\"start\":70347},{\"end\":70357,\"start\":70356},{\"end\":70366,\"start\":70365},{\"end\":70376,\"start\":70375},{\"end\":70389,\"start\":70388},{\"end\":70401,\"start\":70400},{\"end\":70870,\"start\":70869},{\"end\":70882,\"start\":70881},{\"end\":70888,\"start\":70887},{\"end\":70897,\"start\":70896},{\"end\":70906,\"start\":70905},{\"end\":70917,\"start\":70916},{\"end\":70931,\"start\":70930},{\"end\":70939,\"start\":70938},{\"end\":70957,\"start\":70956},{\"end\":71211,\"start\":71210},{\"end\":71220,\"start\":71219},{\"end\":71231,\"start\":71230},{\"end\":71237,\"start\":71236},{\"end\":71249,\"start\":71248},{\"end\":71258,\"start\":71257},{\"end\":71260,\"start\":71259},{\"end\":71271,\"start\":71270},{\"end\":71552,\"start\":71551},{\"end\":71560,\"start\":71559},{\"end\":71568,\"start\":71567},{\"end\":71575,\"start\":71574},{\"end\":71582,\"start\":71581},{\"end\":71589,\"start\":71588},{\"end\":71803,\"start\":71802},{\"end\":71810,\"start\":71809},{\"end\":71819,\"start\":71818},{\"end\":71831,\"start\":71830},{\"end\":71838,\"start\":71837},{\"end\":71848,\"start\":71847},{\"end\":72029,\"start\":72028},{\"end\":72039,\"start\":72038},{\"end\":72046,\"start\":72045},{\"end\":72058,\"start\":72057},{\"end\":72072,\"start\":72071},{\"end\":72081,\"start\":72080},{\"end\":72090,\"start\":72089},{\"end\":72102,\"start\":72101},{\"end\":72363,\"start\":72362},{\"end\":72370,\"start\":72369},{\"end\":72377,\"start\":72376},{\"end\":72386,\"start\":72385},{\"end\":72392,\"start\":72391},{\"end\":72394,\"start\":72393},{\"end\":72613,\"start\":72612},{\"end\":72622,\"start\":72621},{\"end\":72624,\"start\":72623},{\"end\":72634,\"start\":72633},{\"end\":72851,\"start\":72850},{\"end\":72860,\"start\":72859},{\"end\":72870,\"start\":72869},{\"end\":72872,\"start\":72871},{\"end\":73145,\"start\":73144},{\"end\":73154,\"start\":73153},{\"end\":73160,\"start\":73159},{\"end\":73162,\"start\":73161},{\"end\":73172,\"start\":73171},{\"end\":73174,\"start\":73173},{\"end\":73484,\"start\":73483},{\"end\":73494,\"start\":73493},{\"end\":73767,\"start\":73766},{\"end\":73773,\"start\":73772},{\"end\":73780,\"start\":73779},{\"end\":73797,\"start\":73796},{\"end\":73811,\"start\":73810},{\"end\":73825,\"start\":73824},{\"end\":73833,\"start\":73832},{\"end\":73843,\"start\":73842},{\"end\":73853,\"start\":73852},{\"end\":73863,\"start\":73862},{\"end\":74160,\"start\":74159},{\"end\":74167,\"start\":74166},{\"end\":74174,\"start\":74173},{\"end\":74181,\"start\":74180},{\"end\":74183,\"start\":74182},{\"end\":74495,\"start\":74494},{\"end\":74502,\"start\":74501},{\"end\":74512,\"start\":74511},{\"end\":74523,\"start\":74522},{\"end\":74844,\"start\":74843},{\"end\":74850,\"start\":74849},{\"end\":74859,\"start\":74858},{\"end\":74869,\"start\":74868},{\"end\":75194,\"start\":75193},{\"end\":75203,\"start\":75202},{\"end\":75374,\"start\":75373},{\"end\":75387,\"start\":75386},{\"end\":75393,\"start\":75392},{\"end\":75401,\"start\":75400},{\"end\":75414,\"start\":75413},{\"end\":75422,\"start\":75421},{\"end\":75434,\"start\":75433},{\"end\":75445,\"start\":75444},{\"end\":75457,\"start\":75456},{\"end\":75464,\"start\":75463},{\"end\":75909,\"start\":75908},{\"end\":75917,\"start\":75916},{\"end\":75928,\"start\":75927},{\"end\":76213,\"start\":76212},{\"end\":76221,\"start\":76220},{\"end\":76232,\"start\":76231},{\"end\":76247,\"start\":76246},{\"end\":76569,\"start\":76568},{\"end\":76580,\"start\":76579},{\"end\":76593,\"start\":76592},{\"end\":76603,\"start\":76602},{\"end\":76887,\"start\":76886},{\"end\":76900,\"start\":76899},{\"end\":76910,\"start\":76909},{\"end\":77107,\"start\":77106},{\"end\":77118,\"start\":77117},{\"end\":77120,\"start\":77119},{\"end\":77127,\"start\":77126},{\"end\":77138,\"start\":77137},{\"end\":77148,\"start\":77147},{\"end\":77155,\"start\":77154},{\"end\":77166,\"start\":77165},{\"end\":77176,\"start\":77175},{\"end\":77186,\"start\":77185},{\"end\":77197,\"start\":77196},{\"end\":77488,\"start\":77487},{\"end\":77498,\"start\":77497},{\"end\":77502,\"start\":77499},{\"end\":77511,\"start\":77510},{\"end\":77799,\"start\":77798},{\"end\":77807,\"start\":77806},{\"end\":77820,\"start\":77819},{\"end\":77829,\"start\":77828},{\"end\":77837,\"start\":77836},{\"end\":78179,\"start\":78178},{\"end\":78191,\"start\":78190},{\"end\":78203,\"start\":78202},{\"end\":78404,\"start\":78403},{\"end\":78416,\"start\":78415},{\"end\":78428,\"start\":78427},{\"end\":78438,\"start\":78437},{\"end\":78446,\"start\":78445},{\"end\":78453,\"start\":78452},{\"end\":78465,\"start\":78464},{\"end\":78480,\"start\":78479},{\"end\":78772,\"start\":78771},{\"end\":78786,\"start\":78785},{\"end\":78792,\"start\":78791},{\"end\":78806,\"start\":78805},{\"end\":78823,\"start\":78822},{\"end\":78830,\"start\":78829},{\"end\":78832,\"start\":78831},{\"end\":78842,\"start\":78841},{\"end\":78850,\"start\":78849},{\"end\":78860,\"start\":78859},{\"end\":78868,\"start\":78867},{\"end\":79203,\"start\":79202},{\"end\":79217,\"start\":79216},{\"end\":79229,\"start\":79228},{\"end\":79241,\"start\":79240},{\"end\":79248,\"start\":79247},{\"end\":79257,\"start\":79256},{\"end\":79259,\"start\":79258},{\"end\":79420,\"start\":79419},{\"end\":79422,\"start\":79421},{\"end\":79431,\"start\":79430},{\"end\":79439,\"start\":79438},{\"end\":79441,\"start\":79440},{\"end\":79831,\"start\":79830},{\"end\":79839,\"start\":79838},{\"end\":79848,\"start\":79847},{\"end\":79863,\"start\":79862},{\"end\":79874,\"start\":79873},{\"end\":79882,\"start\":79881},{\"end\":79892,\"start\":79891},{\"end\":79903,\"start\":79902},{\"end\":79914,\"start\":79913},{\"end\":79916,\"start\":79915},{\"end\":79927,\"start\":79926},{\"end\":80305,\"start\":80304},{\"end\":80313,\"start\":80312},{\"end\":80319,\"start\":80318},{\"end\":80328,\"start\":80327},{\"end\":80439,\"start\":80438},{\"end\":80445,\"start\":80444},{\"end\":80452,\"start\":80451},{\"end\":80458,\"start\":80457},{\"end\":80467,\"start\":80466},{\"end\":80473,\"start\":80472},{\"end\":80485,\"start\":80484},{\"end\":80487,\"start\":80486},{\"end\":80867,\"start\":80866},{\"end\":80873,\"start\":80872},{\"end\":80884,\"start\":80883},{\"end\":80890,\"start\":80889},{\"end\":80900,\"start\":80899},{\"end\":80911,\"start\":80910},{\"end\":80919,\"start\":80918},{\"end\":81192,\"start\":81191},{\"end\":81200,\"start\":81199},{\"end\":81210,\"start\":81209},{\"end\":81221,\"start\":81220},{\"end\":81444,\"start\":81443},{\"end\":81452,\"start\":81451},{\"end\":81464,\"start\":81463},{\"end\":81475,\"start\":81474},{\"end\":81485,\"start\":81484},{\"end\":81494,\"start\":81493},{\"end\":81506,\"start\":81505},{\"end\":81519,\"start\":81518},{\"end\":81529,\"start\":81528},{\"end\":81538,\"start\":81537},{\"end\":81551,\"start\":81550},{\"end\":81852,\"start\":81851},{\"end\":81861,\"start\":81860},{\"end\":81867,\"start\":81866},{\"end\":81873,\"start\":81872},{\"end\":81880,\"start\":81879},{\"end\":81886,\"start\":81885},{\"end\":82213,\"start\":82212},{\"end\":82222,\"start\":82221},{\"end\":82229,\"start\":82228},{\"end\":82238,\"start\":82237},{\"end\":82240,\"start\":82239},{\"end\":82442,\"start\":82441},{\"end\":82449,\"start\":82448},{\"end\":82457,\"start\":82456},{\"end\":82470,\"start\":82469}]", "bib_author_last_name": "[{\"end\":70334,\"start\":70329},{\"end\":70345,\"start\":70338},{\"end\":70354,\"start\":70349},{\"end\":70363,\"start\":70358},{\"end\":70373,\"start\":70367},{\"end\":70386,\"start\":70377},{\"end\":70398,\"start\":70390},{\"end\":70407,\"start\":70402},{\"end\":70879,\"start\":70871},{\"end\":70885,\"start\":70883},{\"end\":70894,\"start\":70889},{\"end\":70903,\"start\":70898},{\"end\":70914,\"start\":70907},{\"end\":70928,\"start\":70918},{\"end\":70936,\"start\":70932},{\"end\":70945,\"start\":70940},{\"end\":70954,\"start\":70947},{\"end\":70964,\"start\":70958},{\"end\":71217,\"start\":71212},{\"end\":71228,\"start\":71221},{\"end\":71234,\"start\":71232},{\"end\":71246,\"start\":71238},{\"end\":71255,\"start\":71250},{\"end\":71268,\"start\":71261},{\"end\":71278,\"start\":71272},{\"end\":71557,\"start\":71553},{\"end\":71565,\"start\":71561},{\"end\":71572,\"start\":71569},{\"end\":71579,\"start\":71576},{\"end\":71586,\"start\":71583},{\"end\":71593,\"start\":71590},{\"end\":71807,\"start\":71804},{\"end\":71816,\"start\":71811},{\"end\":71828,\"start\":71820},{\"end\":71835,\"start\":71832},{\"end\":71845,\"start\":71839},{\"end\":71854,\"start\":71849},{\"end\":72036,\"start\":72030},{\"end\":72043,\"start\":72040},{\"end\":72055,\"start\":72047},{\"end\":72069,\"start\":72059},{\"end\":72078,\"start\":72073},{\"end\":72087,\"start\":72082},{\"end\":72099,\"start\":72091},{\"end\":72111,\"start\":72103},{\"end\":72367,\"start\":72364},{\"end\":72374,\"start\":72371},{\"end\":72383,\"start\":72378},{\"end\":72389,\"start\":72387},{\"end\":72399,\"start\":72395},{\"end\":72619,\"start\":72614},{\"end\":72631,\"start\":72625},{\"end\":72641,\"start\":72635},{\"end\":72857,\"start\":72852},{\"end\":72867,\"start\":72861},{\"end\":72880,\"start\":72873},{\"end\":73151,\"start\":73146},{\"end\":73157,\"start\":73155},{\"end\":73169,\"start\":73163},{\"end\":73182,\"start\":73175},{\"end\":73491,\"start\":73485},{\"end\":73498,\"start\":73495},{\"end\":73770,\"start\":73768},{\"end\":73777,\"start\":73774},{\"end\":73794,\"start\":73781},{\"end\":73808,\"start\":73798},{\"end\":73822,\"start\":73812},{\"end\":73830,\"start\":73826},{\"end\":73840,\"start\":73834},{\"end\":73850,\"start\":73844},{\"end\":73860,\"start\":73854},{\"end\":73868,\"start\":73864},{\"end\":74164,\"start\":74161},{\"end\":74171,\"start\":74168},{\"end\":74178,\"start\":74175},{\"end\":74190,\"start\":74184},{\"end\":74499,\"start\":74496},{\"end\":74509,\"start\":74503},{\"end\":74520,\"start\":74513},{\"end\":74527,\"start\":74524},{\"end\":74847,\"start\":74845},{\"end\":74856,\"start\":74851},{\"end\":74866,\"start\":74860},{\"end\":74873,\"start\":74870},{\"end\":75200,\"start\":75195},{\"end\":75212,\"start\":75204},{\"end\":75384,\"start\":75375},{\"end\":75390,\"start\":75388},{\"end\":75398,\"start\":75394},{\"end\":75411,\"start\":75402},{\"end\":75419,\"start\":75415},{\"end\":75431,\"start\":75423},{\"end\":75442,\"start\":75435},{\"end\":75454,\"start\":75446},{\"end\":75461,\"start\":75458},{\"end\":75478,\"start\":75465},{\"end\":75914,\"start\":75910},{\"end\":75925,\"start\":75918},{\"end\":75936,\"start\":75929},{\"end\":76218,\"start\":76214},{\"end\":76229,\"start\":76222},{\"end\":76244,\"start\":76233},{\"end\":76255,\"start\":76248},{\"end\":76577,\"start\":76570},{\"end\":76590,\"start\":76581},{\"end\":76600,\"start\":76594},{\"end\":76609,\"start\":76604},{\"end\":76897,\"start\":76888},{\"end\":76907,\"start\":76901},{\"end\":76914,\"start\":76911},{\"end\":77115,\"start\":77108},{\"end\":77124,\"start\":77121},{\"end\":77135,\"start\":77128},{\"end\":77145,\"start\":77139},{\"end\":77152,\"start\":77149},{\"end\":77163,\"start\":77156},{\"end\":77173,\"start\":77167},{\"end\":77183,\"start\":77177},{\"end\":77194,\"start\":77187},{\"end\":77203,\"start\":77198},{\"end\":77495,\"start\":77489},{\"end\":77508,\"start\":77503},{\"end\":77518,\"start\":77512},{\"end\":77804,\"start\":77800},{\"end\":77817,\"start\":77808},{\"end\":77826,\"start\":77821},{\"end\":77834,\"start\":77830},{\"end\":77842,\"start\":77838},{\"end\":78188,\"start\":78180},{\"end\":78200,\"start\":78192},{\"end\":78207,\"start\":78204},{\"end\":78413,\"start\":78405},{\"end\":78425,\"start\":78417},{\"end\":78435,\"start\":78429},{\"end\":78443,\"start\":78439},{\"end\":78450,\"start\":78447},{\"end\":78462,\"start\":78454},{\"end\":78477,\"start\":78466},{\"end\":78484,\"start\":78481},{\"end\":78783,\"start\":78773},{\"end\":78789,\"start\":78787},{\"end\":78803,\"start\":78793},{\"end\":78820,\"start\":78807},{\"end\":78827,\"start\":78824},{\"end\":78839,\"start\":78833},{\"end\":78847,\"start\":78843},{\"end\":78857,\"start\":78851},{\"end\":78865,\"start\":78861},{\"end\":78872,\"start\":78869},{\"end\":79214,\"start\":79204},{\"end\":79226,\"start\":79218},{\"end\":79238,\"start\":79230},{\"end\":79245,\"start\":79242},{\"end\":79254,\"start\":79249},{\"end\":79264,\"start\":79260},{\"end\":79428,\"start\":79423},{\"end\":79436,\"start\":79432},{\"end\":79449,\"start\":79442},{\"end\":79836,\"start\":79832},{\"end\":79845,\"start\":79840},{\"end\":79860,\"start\":79849},{\"end\":79871,\"start\":79864},{\"end\":79879,\"start\":79875},{\"end\":79889,\"start\":79883},{\"end\":79900,\"start\":79893},{\"end\":79911,\"start\":79904},{\"end\":79924,\"start\":79917},{\"end\":79937,\"start\":79928},{\"end\":80310,\"start\":80306},{\"end\":80316,\"start\":80314},{\"end\":80325,\"start\":80320},{\"end\":80332,\"start\":80329},{\"end\":80442,\"start\":80440},{\"end\":80449,\"start\":80446},{\"end\":80455,\"start\":80453},{\"end\":80464,\"start\":80459},{\"end\":80470,\"start\":80468},{\"end\":80482,\"start\":80474},{\"end\":80497,\"start\":80488},{\"end\":80870,\"start\":80868},{\"end\":80881,\"start\":80874},{\"end\":80887,\"start\":80885},{\"end\":80897,\"start\":80891},{\"end\":80908,\"start\":80901},{\"end\":80916,\"start\":80912},{\"end\":80926,\"start\":80920},{\"end\":81197,\"start\":81193},{\"end\":81207,\"start\":81201},{\"end\":81218,\"start\":81211},{\"end\":81225,\"start\":81222},{\"end\":81449,\"start\":81445},{\"end\":81461,\"start\":81453},{\"end\":81472,\"start\":81465},{\"end\":81482,\"start\":81476},{\"end\":81491,\"start\":81486},{\"end\":81503,\"start\":81495},{\"end\":81516,\"start\":81507},{\"end\":81526,\"start\":81520},{\"end\":81535,\"start\":81530},{\"end\":81548,\"start\":81539},{\"end\":81555,\"start\":81552},{\"end\":81858,\"start\":81853},{\"end\":81864,\"start\":81862},{\"end\":81870,\"start\":81868},{\"end\":81877,\"start\":81874},{\"end\":81883,\"start\":81881},{\"end\":81892,\"start\":81887},{\"end\":82219,\"start\":82214},{\"end\":82226,\"start\":82223},{\"end\":82235,\"start\":82230},{\"end\":82243,\"start\":82241},{\"end\":82446,\"start\":82443},{\"end\":82454,\"start\":82450},{\"end\":82467,\"start\":82458},{\"end\":82484,\"start\":82471}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2010.04296\",\"id\":\"b0\"},\"end\":70755,\"start\":70327},{\"attributes\":{\"id\":\"b1\"},\"end\":71208,\"start\":70757},{\"attributes\":{\"id\":\"b2\"},\"end\":71450,\"start\":71210},{\"attributes\":{\"id\":\"b3\"},\"end\":71768,\"start\":71452},{\"attributes\":{\"id\":\"b4\"},\"end\":71965,\"start\":71770},{\"attributes\":{\"id\":\"b5\"},\"end\":72291,\"start\":71967},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":25497516},\"end\":72610,\"start\":72293},{\"attributes\":{\"doi\":\"arXiv:2106.02972\",\"id\":\"b7\"},\"end\":72848,\"start\":72612},{\"attributes\":{\"doi\":\"arXiv:1906.11176\",\"id\":\"b8\"},\"end\":73080,\"start\":72850},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":202889132},\"end\":73393,\"start\":73082},{\"attributes\":{\"doi\":\"arXiv:2009.07783\",\"id\":\"b10\"},\"end\":73677,\"start\":73395},{\"attributes\":{\"doi\":\"arXiv:2108.03272\",\"id\":\"b11\"},\"end\":74157,\"start\":73679},{\"attributes\":{\"id\":\"b12\"},\"end\":74391,\"start\":74159},{\"attributes\":{\"doi\":\"arXiv:2110.10189\",\"id\":\"b13\"},\"end\":74743,\"start\":74393},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":199453025},\"end\":75127,\"start\":74745},{\"attributes\":{\"doi\":\"arXiv:2005.07648\",\"id\":\"b15\"},\"end\":75371,\"start\":75129},{\"attributes\":{\"doi\":\"arXiv:2108.03298\",\"id\":\"b16\"},\"end\":75841,\"start\":75373},{\"attributes\":{\"doi\":\"arXiv:2204.06252\",\"id\":\"b17\"},\"end\":76107,\"start\":75843},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":244908821},\"end\":76528,\"start\":76109},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":3990641},\"end\":76824,\"start\":76530},{\"attributes\":{\"id\":\"b20\"},\"end\":77030,\"start\":76826},{\"attributes\":{\"id\":\"b21\"},\"end\":77422,\"start\":77032},{\"attributes\":{\"doi\":\"10.1109/IROS.2013.6696520\",\"id\":\"b22\"},\"end\":77706,\"start\":77424},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":220069237},\"end\":78114,\"start\":77708},{\"attributes\":{\"id\":\"b24\"},\"end\":78322,\"start\":78116},{\"attributes\":{\"id\":\"b25\"},\"end\":78703,\"start\":78324},{\"attributes\":{\"id\":\"b26\"},\"end\":79130,\"start\":78705},{\"attributes\":{\"id\":\"b27\"},\"end\":79417,\"start\":79132},{\"attributes\":{\"id\":\"b28\"},\"end\":79556,\"start\":79419},{\"attributes\":{\"id\":\"b29\"},\"end\":79665,\"start\":79558},{\"attributes\":{\"doi\":\"10.1109/MRA.2012.2205651\",\"id\":\"b30\"},\"end\":79762,\"start\":79667},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":235658123},\"end\":80254,\"start\":79764},{\"attributes\":{\"id\":\"b32\"},\"end\":80436,\"start\":80256},{\"attributes\":{\"doi\":\"arXiv:1910.01442\",\"id\":\"b33\"},\"end\":80774,\"start\":80438},{\"attributes\":{\"id\":\"b34\"},\"end\":81110,\"start\":80776},{\"attributes\":{\"id\":\"b35\"},\"end\":81364,\"start\":81112},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":225076003},\"end\":81849,\"start\":81366},{\"attributes\":{\"doi\":\"arXiv:2108.11092\",\"id\":\"b37\"},\"end\":82149,\"start\":81851},{\"attributes\":{\"id\":\"b38\"},\"end\":82363,\"start\":82151},{\"attributes\":{\"doi\":\"arXiv:2009.12293\",\"id\":\"b39\"},\"end\":82685,\"start\":82365}]", "bib_title": "[{\"end\":72360,\"start\":72293},{\"end\":73142,\"start\":73082},{\"end\":74841,\"start\":74745},{\"end\":76210,\"start\":76109},{\"end\":76566,\"start\":76530},{\"end\":77796,\"start\":77708},{\"end\":79828,\"start\":79764},{\"end\":81441,\"start\":81366}]", "bib_author": "[{\"end\":70336,\"start\":70327},{\"end\":70347,\"start\":70336},{\"end\":70356,\"start\":70347},{\"end\":70365,\"start\":70356},{\"end\":70375,\"start\":70365},{\"end\":70388,\"start\":70375},{\"end\":70400,\"start\":70388},{\"end\":70409,\"start\":70400},{\"end\":70881,\"start\":70869},{\"end\":70887,\"start\":70881},{\"end\":70896,\"start\":70887},{\"end\":70905,\"start\":70896},{\"end\":70916,\"start\":70905},{\"end\":70930,\"start\":70916},{\"end\":70938,\"start\":70930},{\"end\":70947,\"start\":70938},{\"end\":70956,\"start\":70947},{\"end\":70966,\"start\":70956},{\"end\":71219,\"start\":71210},{\"end\":71230,\"start\":71219},{\"end\":71236,\"start\":71230},{\"end\":71248,\"start\":71236},{\"end\":71257,\"start\":71248},{\"end\":71270,\"start\":71257},{\"end\":71280,\"start\":71270},{\"end\":71559,\"start\":71551},{\"end\":71567,\"start\":71559},{\"end\":71574,\"start\":71567},{\"end\":71581,\"start\":71574},{\"end\":71588,\"start\":71581},{\"end\":71595,\"start\":71588},{\"end\":71809,\"start\":71802},{\"end\":71818,\"start\":71809},{\"end\":71830,\"start\":71818},{\"end\":71837,\"start\":71830},{\"end\":71847,\"start\":71837},{\"end\":71856,\"start\":71847},{\"end\":72038,\"start\":72028},{\"end\":72045,\"start\":72038},{\"end\":72057,\"start\":72045},{\"end\":72071,\"start\":72057},{\"end\":72080,\"start\":72071},{\"end\":72089,\"start\":72080},{\"end\":72101,\"start\":72089},{\"end\":72113,\"start\":72101},{\"end\":72369,\"start\":72362},{\"end\":72376,\"start\":72369},{\"end\":72385,\"start\":72376},{\"end\":72391,\"start\":72385},{\"end\":72401,\"start\":72391},{\"end\":72621,\"start\":72612},{\"end\":72633,\"start\":72621},{\"end\":72643,\"start\":72633},{\"end\":72859,\"start\":72850},{\"end\":72869,\"start\":72859},{\"end\":72882,\"start\":72869},{\"end\":73153,\"start\":73144},{\"end\":73159,\"start\":73153},{\"end\":73171,\"start\":73159},{\"end\":73184,\"start\":73171},{\"end\":73493,\"start\":73483},{\"end\":73500,\"start\":73493},{\"end\":73772,\"start\":73766},{\"end\":73779,\"start\":73772},{\"end\":73796,\"start\":73779},{\"end\":73810,\"start\":73796},{\"end\":73824,\"start\":73810},{\"end\":73832,\"start\":73824},{\"end\":73842,\"start\":73832},{\"end\":73852,\"start\":73842},{\"end\":73862,\"start\":73852},{\"end\":73870,\"start\":73862},{\"end\":74166,\"start\":74159},{\"end\":74173,\"start\":74166},{\"end\":74180,\"start\":74173},{\"end\":74192,\"start\":74180},{\"end\":74501,\"start\":74494},{\"end\":74511,\"start\":74501},{\"end\":74522,\"start\":74511},{\"end\":74529,\"start\":74522},{\"end\":74849,\"start\":74843},{\"end\":74858,\"start\":74849},{\"end\":74868,\"start\":74858},{\"end\":74875,\"start\":74868},{\"end\":75202,\"start\":75193},{\"end\":75214,\"start\":75202},{\"end\":75386,\"start\":75373},{\"end\":75392,\"start\":75386},{\"end\":75400,\"start\":75392},{\"end\":75413,\"start\":75400},{\"end\":75421,\"start\":75413},{\"end\":75433,\"start\":75421},{\"end\":75444,\"start\":75433},{\"end\":75456,\"start\":75444},{\"end\":75463,\"start\":75456},{\"end\":75480,\"start\":75463},{\"end\":75916,\"start\":75908},{\"end\":75927,\"start\":75916},{\"end\":75938,\"start\":75927},{\"end\":76220,\"start\":76212},{\"end\":76231,\"start\":76220},{\"end\":76246,\"start\":76231},{\"end\":76257,\"start\":76246},{\"end\":76579,\"start\":76568},{\"end\":76592,\"start\":76579},{\"end\":76602,\"start\":76592},{\"end\":76611,\"start\":76602},{\"end\":76899,\"start\":76886},{\"end\":76909,\"start\":76899},{\"end\":76916,\"start\":76909},{\"end\":77117,\"start\":77106},{\"end\":77126,\"start\":77117},{\"end\":77137,\"start\":77126},{\"end\":77147,\"start\":77137},{\"end\":77154,\"start\":77147},{\"end\":77165,\"start\":77154},{\"end\":77175,\"start\":77165},{\"end\":77185,\"start\":77175},{\"end\":77196,\"start\":77185},{\"end\":77205,\"start\":77196},{\"end\":77497,\"start\":77487},{\"end\":77510,\"start\":77497},{\"end\":77520,\"start\":77510},{\"end\":77806,\"start\":77798},{\"end\":77819,\"start\":77806},{\"end\":77828,\"start\":77819},{\"end\":77836,\"start\":77828},{\"end\":77844,\"start\":77836},{\"end\":78190,\"start\":78178},{\"end\":78202,\"start\":78190},{\"end\":78209,\"start\":78202},{\"end\":78415,\"start\":78403},{\"end\":78427,\"start\":78415},{\"end\":78437,\"start\":78427},{\"end\":78445,\"start\":78437},{\"end\":78452,\"start\":78445},{\"end\":78464,\"start\":78452},{\"end\":78479,\"start\":78464},{\"end\":78486,\"start\":78479},{\"end\":78785,\"start\":78771},{\"end\":78791,\"start\":78785},{\"end\":78805,\"start\":78791},{\"end\":78822,\"start\":78805},{\"end\":78829,\"start\":78822},{\"end\":78841,\"start\":78829},{\"end\":78849,\"start\":78841},{\"end\":78859,\"start\":78849},{\"end\":78867,\"start\":78859},{\"end\":78874,\"start\":78867},{\"end\":79216,\"start\":79202},{\"end\":79228,\"start\":79216},{\"end\":79240,\"start\":79228},{\"end\":79247,\"start\":79240},{\"end\":79256,\"start\":79247},{\"end\":79266,\"start\":79256},{\"end\":79430,\"start\":79419},{\"end\":79438,\"start\":79430},{\"end\":79451,\"start\":79438},{\"end\":79838,\"start\":79830},{\"end\":79847,\"start\":79838},{\"end\":79862,\"start\":79847},{\"end\":79873,\"start\":79862},{\"end\":79881,\"start\":79873},{\"end\":79891,\"start\":79881},{\"end\":79902,\"start\":79891},{\"end\":79913,\"start\":79902},{\"end\":79926,\"start\":79913},{\"end\":79939,\"start\":79926},{\"end\":80312,\"start\":80304},{\"end\":80318,\"start\":80312},{\"end\":80327,\"start\":80318},{\"end\":80334,\"start\":80327},{\"end\":80444,\"start\":80438},{\"end\":80451,\"start\":80444},{\"end\":80457,\"start\":80451},{\"end\":80466,\"start\":80457},{\"end\":80472,\"start\":80466},{\"end\":80484,\"start\":80472},{\"end\":80499,\"start\":80484},{\"end\":80872,\"start\":80866},{\"end\":80883,\"start\":80872},{\"end\":80889,\"start\":80883},{\"end\":80899,\"start\":80889},{\"end\":80910,\"start\":80899},{\"end\":80918,\"start\":80910},{\"end\":80928,\"start\":80918},{\"end\":81199,\"start\":81191},{\"end\":81209,\"start\":81199},{\"end\":81220,\"start\":81209},{\"end\":81227,\"start\":81220},{\"end\":81451,\"start\":81443},{\"end\":81463,\"start\":81451},{\"end\":81474,\"start\":81463},{\"end\":81484,\"start\":81474},{\"end\":81493,\"start\":81484},{\"end\":81505,\"start\":81493},{\"end\":81518,\"start\":81505},{\"end\":81528,\"start\":81518},{\"end\":81537,\"start\":81528},{\"end\":81550,\"start\":81537},{\"end\":81557,\"start\":81550},{\"end\":81860,\"start\":81851},{\"end\":81866,\"start\":81860},{\"end\":81872,\"start\":81866},{\"end\":81879,\"start\":81872},{\"end\":81885,\"start\":81879},{\"end\":81894,\"start\":81885},{\"end\":82221,\"start\":82212},{\"end\":82228,\"start\":82221},{\"end\":82237,\"start\":82228},{\"end\":82245,\"start\":82237},{\"end\":82448,\"start\":82441},{\"end\":82456,\"start\":82448},{\"end\":82469,\"start\":82456},{\"end\":82486,\"start\":82469}]", "bib_venue": "[{\"end\":70513,\"start\":70425},{\"end\":70867,\"start\":70757},{\"end\":71313,\"start\":71280},{\"end\":71549,\"start\":71452},{\"end\":71800,\"start\":71770},{\"end\":72026,\"start\":71967},{\"end\":72432,\"start\":72401},{\"end\":72707,\"start\":72659},{\"end\":72942,\"start\":72898},{\"end\":73220,\"start\":73184},{\"end\":73481,\"start\":73395},{\"end\":73764,\"start\":73679},{\"end\":74261,\"start\":74192},{\"end\":74492,\"start\":74393},{\"end\":74924,\"start\":74875},{\"end\":75191,\"start\":75129},{\"end\":75577,\"start\":75496},{\"end\":75906,\"start\":75843},{\"end\":76300,\"start\":76257},{\"end\":76657,\"start\":76611},{\"end\":76884,\"start\":76826},{\"end\":77104,\"start\":77032},{\"end\":77485,\"start\":77424},{\"end\":77890,\"start\":77844},{\"end\":78176,\"start\":78116},{\"end\":78401,\"start\":78324},{\"end\":78769,\"start\":78705},{\"end\":79200,\"start\":79132},{\"end\":79483,\"start\":79451},{\"end\":79595,\"start\":79560},{\"end\":79988,\"start\":79939},{\"end\":80302,\"start\":80256},{\"end\":80579,\"start\":80515},{\"end\":80864,\"start\":80776},{\"end\":81189,\"start\":81112},{\"end\":81592,\"start\":81557},{\"end\":81974,\"start\":81910},{\"end\":82210,\"start\":82151},{\"end\":82439,\"start\":82365}]"}}}, "year": 2023, "month": 12, "day": 17}