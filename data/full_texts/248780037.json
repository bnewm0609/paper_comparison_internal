{"id": 248780037, "updated": "2023-11-08 07:30:12.909", "metadata": {"title": "CoCoLM: Complex Commonsense Enhanced Language Model with Discourse Relations", "authors": "[{\"first\":\"Changlong\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Hongming\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Yangqiu\",\"last\":\"Song\",\"middle\":[]},{\"first\":\"Wilfred\",\"last\":\"Ng\",\"middle\":[]}]", "venue": "FINDINGS", "journal": "Findings of the Association for Computational Linguistics: ACL 2022", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Large-scale pre-trained language models have demonstrated strong knowledge representation ability. However, recent studies suggest that even though these giant models contain rich simple commonsense knowledge (e.g., bird can fly and fish can swim.), they often struggle with complex commonsense knowledge that involves multiple eventualities (verb-centric phrases, e.g., identifying the relationship between \u201cJim yells at Bob\u201d and \u201cBob is upset\u201d). To address this issue, in this paper, we propose to help pre-trained language models better incorporate complex commonsense knowledge. Unlike direct fine-tuning approaches, we do not focus on a specific task and instead propose a general language model named CoCoLM. Through the careful training over a large-scale eventuality knowledge graph ASER, we successfully teach pre-trained language models (i.e., BERT and RoBERTa) rich multi-hop commonsense knowledge among eventualities.Experiments on multiple commonsense tasks that require the correct understanding of eventualities demonstrate the effectiveness of CoCoLM.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2012.15643", "mag": null, "acl": "2022.findings-acl.93", "pubmed": null, "pubmedcentral": null, "dblp": "conf/acl/YuZSN22", "doi": "10.18653/v1/2022.findings-acl.93"}}, "content": {"source": {"pdf_hash": "773d3de28eed7f0a405cd4c739c32e156a39b642", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclanthology.org/2022.findings-acl.93.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "39a112abad4a54c0552052012b1f0b218ff0d22c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/773d3de28eed7f0a405cd4c739c32e156a39b642.txt", "contents": "\nCoCoLM: Complex Commonsense Enhanced Language Model with Discourse Relations\nMay 22-27, 2022\n\nChanglong Yu \nHKUST\nHong KongChina\n\nHongming Zhang hongmzhang@tencent.com \nHKUST\nHong KongChina\n\nTencent AI Lab\nSeattleU.S\n\nYangqiu Song \nHKUST\nHong KongChina\n\nWilfred Ng wilfred@cse.ust.hk \nHKUST\nHong KongChina\n\nCoCoLM: Complex Commonsense Enhanced Language Model with Discourse Relations\n\nAssociation for Computational Linguistics: ACL 2022\nMay 22-27, 2022\nLarge-scale pre-trained language models have demonstrated strong knowledge representation ability. However, recent studies suggest that even though these giant models contain rich simple commonsense knowledge (e.g., bird can fly and fish can swim.), they often struggle with complex commonsense knowledge that involves multiple eventualities (verbcentric phrases, e.g., identifying the relationship between \"Jim yells at Bob\" and \"Bob is upset\"). To address this issue, in this paper, we propose to help pre-trained language models better incorporate complex commonsense knowledge. Unlike direct finetuning approaches, we do not focus on a specific task and instead propose a general language model named CoCoLM. Through the careful training over a large-scale eventuality knowledge graph ASER, we successfully teach pre-trained language models (i.e., BERT and RoBERTa) rich discourse-level commonsense knowledge among eventualities. Experiments on multiple commonsense tasks that require the correct understanding of eventualities demonstrate the effectiveness of CoCoLM.\n\nIntroduction\n\nRecently, large-scale pre-trained language representation models (LMs) (Devlin et al., 2019; have demonstrated the strong ability to discover useful linguistic properties of syntax and remember an impressive amount of knowledge with self-supervised training over a large unlabeled corpus (Petroni et al., 2019;Jiang et al., 2020). On top of that, with the help of the fine-tuning step, LMs can learn how to use the memorized knowledge for different tasks, and thus achieve outstanding performance on many downstream natural language processing (NLP) tasks.\n\nAs discussed in Verga et al. (2020), while language models have already captured rich knowl- * Equal contribution.\n\n\nQuery Answer\n\nBirds can [MASK]. fly Cars are used for [MASK]. transport Jim yells at Bob, [MASK] Jim is upset. but Jim yells at Bob, [MASK] Bob is upset. but Table 1: Exploring knowledge contained in pre-trained language models following LAMA (Petroni et al., 2019). Queries and prediction returned by BERT-large are presented. Semantically plausible and implausible prediction are indicated with blue and red colors.\n\nedge, they often only perform well when the semantic unit is a single token while poorly when the semantic unit is more complex (e.g., a multi-token named entity or an eventuality, which is a linguistic term for verb-centric phrases covering activities, states and events (Bach, 1986;Araki and Mitamura, 2018)). For example, as shown in Table 1, if we follow LAMA (Petroni et al., 2019) to analyze the knowledge contained in BERT-large (Devlin et al., 2019) with a token prediction task, we can find out that BERT can understand that birds can fly, and a car is used for transportation, but it fails to understand the relation between \"Jim yells at Bob\" and relevant eventualities. An important reason behind this is that current language models heavily rely on token-level masked language models (MLMs) as the loss function, which can effectively represent and memorize token co-occurrence statistics 1 but struggle at perceiving multi-token concepts.\n\nTo address this problem and equip LMs with complex and accurate human knowledge, several recent works attempt to integrate entity representations from external knowledge graphs. While those approaches have been proved effective in merging structured knowledge into the LMs, they still have two limitations when applying to eventuality representations: (1) The first line of work (Verga Type Sequences Temporal I had a dream. Precedence (Before) I met with you yesterday. Succession (After) There were so many matters.\n\n\nCasual\n\nI go to supermarket. Reason (Because) I have a coupon. Result (So) The price is great.\n\nOthers You can come with me. Alternative (Or) You can stand here. Contrast (But) The situation remains unchanged.  Shen et al., 2020;F\u00e9vry et al., 2020) restricts a fixed set of named entities or concepts to be linked to KGs while the eventualities are not easily canonicalized. There are enormous eventualities, which many of them refer to similar meanings such as \"Tom is upset\" and \"Alice is upset\".\n\n(2) The second class of methods uses powerful contextualized representations to encode one-hop triplets from KGs (Bosselut et al., 2019;Yao et al., 2019) for the task of KG completion. However, it is not sufficient for tasks that require the understanding of complex discourse relations in the event sequences or chains. For example pretrained LMs on the story ending prediction task (Mostafazadeh et al., 2016) have gaps with human performance . Besides that, different types of relations (casual or temporal) make high-order inference over eventualities difficult and challenging.\n\nIn this paper, to effectively inject eventuality knowledge into pre-trained language representation models, we propose a knowledge injection framework CoCoLM, which requires no concept or eventuality linking and encodes multi-hop eventuality information as well as their discourse relations. The starting point is a large-scale eventuality knowledge graph, ASER (Zhang et al., 2020b), where the edges are discourse relations among eventualities (e.g., \"being hungry\" can cause \"eat food\" and \"being hungry\" often happens at the same time as \"being tired\"). First, we go beyond one-hop modeling (Yao et al., 2019;Bosselut et al., 2019) and carefully conduct weighted random walk over ASER to harvest multi-hop eventuality sequences connected by discourse relations ( \u00a72.1). Individual eventualities are contextualized by coherent sequences (examples in Table 2). Second, we finetune pretrained LMs on the sampled sequences and reformulate the masked language modeling objective to new eventuality-level masking to perceive the eventualities as independent semantic units ( \u00a7 2.2). In addition, two auxiliary tasks of discourse relation prediction are proposed to make implicit commonsense inferences ( \u00a7 2.3). For ex-ample, the new tasks explicitly reinforce the casual relation prediction between \"I have a coupon\" and \"The price is great\". By doing so, we successfully expose and inject fruitful high-order information between eventualities to pretrained LMs. To understand the impact of our proposed CoCoLM, we conduct experiments on three tasks that require the understanding of temporal, causal, mixed (multiple) relations respectively. The results show that our method achieves substantial improvements on the multiple-relation task while competitive performance on single-relation tasks. Extensive analyses are conducted to show the effect and contribution of all components in CoCoLM. Our main contributions are as follows:\n\n\u2022 We propose CoCoLM, a new contextualized language model enhanced by complex commonsense knowledge from high-order discourse relations. CoCoLM is trained to predict the whole eventuality among the sequences using a largescale eventuality KG.\n\n\u2022 We introduce two auxiliary discourse tasks to help incorporate discourse-related knowledge into pre-trained language models, which complement the special eventuality masking strategy.\n\n\u2022 CoCoLM achieves stronger performance than the baseline LMs on multiple datasets that require the understanding of complex commonsense knowledge about eventualities. 2\n\n\nMethods\n\nThe overall framework of CoCoLM is presented in Figure 1. Given a pre-trained language model, we inject complex commonsense knowledge about eventualities by adding one adaptive pre-training stage (Gururangan et al., 2020). Specifically, we first generate eventuality sequences based on carefully controlled walks over existing eventuality knowledge graphs and then use the sequences as  Figure 1: The overall framework of CoCoLM. On top of base pre-trained LMs, complex commonsense knowledge from the eventuality sequences is injected by fine-tuning MLMs and auxiliary discourse tasks.\n\nthe context to help LMs handle eventualities. Besides the original token-level MLM objective, we also introduce the eventuality-level masking strategy and several auxiliary tasks to assist the training. As the training is not task-specific, the resulting LM can be easily applied to any downstream tasks via another task-specific fine-tuning stage.\n\n\nEventuality Sequence Generation\n\nMulti-hop path information have been shown useful and interpretable to provide extra context knowledge by connecting the concepts from Concept-Net (Speer et al., 2017) for commonsense reasoning tasks (Lin et al., 2019;Wang et al., 2020a). Similarly, we generate eventuality sequences by leveraging ASER, which uses eventualities as nodes and the discourse relations as edges. ASER extracts rich eventuality knowledge from diverse corpus, such as \"being hungry\" and \"being tired\" often happen together and people often \"make a call\" before they go. It contains much larger scale of discourse relations than DisSent (Nie et al., 2019). Interestingly, beyond the single edges, higher-order connections over ASER can also reflect insightful eventuality knowledge. For example, \"sleep\" and \"go\" are not likely to happen at the same time because \"sleep\" can be caused by \"being tired\" and there exist contrast connections between \"being tired\" and \"go\". To include higher-order knowledge into the model, we propose to take the whole graphical structure into consideration rather than single-hop edges. Motivated by DeepWalk (Perozzi et al., 2014), we randomly sample paths to simulate the overall graph structure and generate eventuality-level co-occurrence information.\n\nGiven the initial knowledge graph G = (E, R), where E is the eventuality set and R is the relation set, we conduct the weighted random walk based on the edge weights over G to sample eventuality paths. We denote each path as (E 0 , r 0 , E 1 , r 1 , ..., r l\u22121 , E l ), where E means an eventuality, r a discourse edge connecting two eventualities, and l the numbers of eventualities along the sequence. To convert the sampled sentence into a token list, we keep all words in each event as a sentence and use representative connectives for each discourse relation to connect them (examples in the Table 2; full list in the Appendix Table 9). As ASER is automatically extracted from raw corpus, it may contain noise. To minimize the influence of the noise and improve the informativeness, the selected paths should fulfill:\n\n1. To filter out rare eventualities, the frequency of starting eventualities has to be larger than five.\n\n2. Other than the relations that have the transitive property (e.g., Precedence, Result), each selected path should not contain successive edges with repeated relations.\n\n3. We manually improve the sampling probability of selecting sub-sequence patterns like \"E i Condition E j Reason E k \". Since it has been proven that if -then rules  and if -then-because rules (Arabshahi et al., 2020) are crucial for reasoning.\n\n\nEventuality-Level Mask\n\nMasking strategy plays a crucial role in the training of language representation models. Besides the random token-level masking strategy, many other masking strategies have been explored by previous literature such as the-whole-word masking (Devlin et al., 2019;Cui et al., 2019), entity masking Shen et al., 2020) or text span masking (Joshi et al., 2020) 3 . Similarly, to effectively help the model view each eventuality as an independent semantic unit, we propose the following two masking strategies: (1) Whole Eventuality Masking: Similar to the whole word masking or entity masking strategies, the whole eventuality masking aims to reduce the prior biases of eventuality tokens. For example, given an eventuality sequence \"I feel sleepy because I drink a cup of [MASK].\", Co-Occurrence Prediction Figure 2: Illustration of CoCoLM-(Complex commonsense pre-training stage). Given an eventuality sequence, it is either masked by the whole eventuality masking (in blue) or discourse connective masking strategy (in pink). Besides the regular masked language model, the discourse relation labels are jointly predicted for masked connective tokens (on x 4 , x 8 and x 12 ). Co-occurrence prediction (on x 1 ) is conducted for both masking strategies.\n\nBERT would easily predict \"coffee\" or \"tea\" because of the prior knowledge of \"cup of\" inside the eventuality. Instead of that, masking the whole \"I drink a cup of coffee\" would encourage the prediction to treat each eventuality as an independent semantic unit and focus on the relations between them. For each sampled sequence, we randomly mask at most one eventuality to fulfill the masking budget, which is typically 25% of the sequence token length.\n\n(2) Discourse Connective Masking: Besides masking the eventualities, to effectively encode the discourse information, we also tried masking the discourse connectives. Examples of two masking strategies are shown in Figure 2. It is worth mentioning that for each sequence, we only randomly select one type of masking strategy to guarantee that enough information is kept in the left tokens for the prediction. The formal masking objective is defined as follows. Given a tokenized sampled sequence X = (x 1 , x 2 , ..., x n ), after masking several tokens, we pass it to a transformer encoder (Vaswani et al., 2017) and denote the resulting vector representations as x 1 , x 2 , ...x n . The training loss L mlm can thus be defined as:\nL mlm = \u2212 1 |M| i\u2208M logP (x i |x i ),(1)\nwhere M means the set of masked tokens following the aforementioned masking strategies.\n\n\nAuxiliary Tasks\n\nA limitation of the MLM loss is that the prediction is over the entire vocabulary, and as a result, the model could not effectively learn the connection between eventualities and connective words.\n\nTo remedy this and force the model to learn the discourse knowledge, we propose to add an additional classification layer after the last layer of transformer encoder and it feeds the output vector x i of connective token x i into a softmax layer over the set of discourse relation labels as follows.\nP (l i |x i ) = softmax(x i W + b),(2)L rel = \u2212 i\u2208M R log P (l i = l i |x i ),(3)\nwhere M R is the index set of masked discourse connective tokens (e.g., because, and, so) in Figure 2, l i is the predicted discourse relation label, and l i the label provided by ASER (10 relations in Table 9). W and b are trainable parameters. Besides the aforementioned discourse relations, ASER also provides the Co-occurrence relations between eventualities, which mean that two eventualities appear in the same sentence, but there are no explicit discourse markers between them. Cooccurrence information has been used for narrative event prediction (Chambers and Jurafsky, 2008) Though Co-occurrence relations are less informative, high frequency pairs still reflect rich knowledge about eventualities. Motivated by this, we propose another auxiliary task to help the model to learn such knowledge. Specially, given an eventuality sequence S = (E 0 , r 0 , E 1 , r 1 , ..., r l\u22121 , E l ) and an eventuality E c , we format the input 4 as \"[CLS] S [SEP] E c [SEP]\". We set 50% of the time E c to be the positive co-occurred eventuality with one of the eventualities in the sequence while 50% of the time E c is randomly sampled negative in ASER. Similar to the next sentence prediction in the original BERT, on top of the vector representation of token [CLS], i.e., x cls , we add another classification layer to predict whether the Co-occurrence relations hold or not. The training objective L occur for binary classification is similar to L rel :\nL occur = \u2212 log P (l i = l i , |x cls ),(4)\nwhere l i is the true co-occurrence label (positive or negative) for the sequence. Merging all three losses together, we can then define the overall loss function L as:\nL = L mlm + L rel + L occur .(5)\n3 Experiments\n\n\nImplementation Details\n\nIn this work, we use the released ASER-core version 5 extracted from multi-domain corpora, which contains over 27.6 million eventualities and 8.8 million relations. We follow the heuristic rules in Sec. 2.1 to sample eventuality sequences for pretraining. Overall we generated 4,041,572 eventuality sequences (sentences), ranging from one to five hops and the one-hop sequence means the direct (first-order) edge in the ASER. We also downsample eventuality nodes with extremely high frequency such as I see. Sequence examples are listed in Table 2 and more examples as well as sequence distribution over different lengths are appended in the Appendix. We select base and large version of BERT (Devlin et al., 2019), RoBERTa  as the base LM. For the continual complex commonsense pre-training phase, we use the Adam optimizer for 10 epochs with batch size 128, learning rate 1e-5 and weight decay 0.01. Considering the relative longer span of masked eventualities, we enlarge the masking proportion from 15% to 25%, which averagely add 1.7 more masked tokens in the sequences. We implemented the pretraining with Huggingface library (Wolf et al., 2020) and running CoCoLM pretraining on eight Nvidia V100 32GB GPUs took four days. Pretraining introduces two classification layers with thousands of parameters.  \n\n\nDatasets and Evaluations\n\nIn this section, we introduce evaluation datasets and settings as follows:\n\nROCStories ( COPA (Gordon et al., 2012) is a binary-choice commonsense causal reasoning task, which requires models to predict which the candidate hypothesis is the plausible effect/cause of the given premise. We follow the training/dev/test split in SuperGLUE . The statistics of the three selected datasets are presented in Table 3. For fine-tuning experiments, we select the learning rate from {2e-5, 1e-5, 5e-6}, and maximize the sequence length and batch size such that they can fit into GPU memory. Finetuning was much faster due to fewer new parameters from classification layers.\n\nDifferent from MATRES and COPA, solving the story ending tasks of ROCStories requires multitype relation inferences including causal, temporal etc. Moreover, as mentioned in Sharma et al. (2018), there is a strong bias about the humancreated negative endings such that the model can distinguish the positive and negative endings without seeing the first four events. Even though Sharma et al. (2018) tried to filter the annotations, the bias still cannot be fully relieved. As a result, to clearly show the effect of adding complex  knowledge about events into the LM, besides the most widely used supervised setting, we also report the performance of a debiased setting, where the model randomly selects events from other stories as the negative ending during the training. The debiased setting is indicated with \"D\". Following previous works, we report accuracy for the ROC-Stories, MATRES and COPA tasks. For MATRES, we also report F1 scores by considering the task as general relation extraction and treating the label of vague as no relation (Ning et al., 2019). All models are trained until convergence and the best model on the dev set is selected to be evaluated.\n\n\nExperimental Results\n\nThe results are presented in Table 4, from which we can see that CoCoLM consistently outperforms all the baselines on all three commonsense tasks, especially on the debiased setting of ROCStories. Besides that, we can make the following observations. First, the improvement of our model is more significant on ROCStories than COPA and MATRES, which is mainly because multiple relation combinations in the eventuality sequences bring high-order information and thus help complex reasoning. Second, CoCoLM achieves significant improvement on lower-capacity LMs trained on small corpora. For example, CoCoLM brings up to 59.2% improvement over BERT-base on the ROCStories dataset. Third, compared with the original supervised setting, the debiased setting is more challenging for all models, which helps verify our assumptions that previous models might benefit from the bias. Here the debiased setting should be more fair for comparison. When we dig into the MATRES dataset, event pairs (typically verb pairs) are associated with the context, which some of  could be easily inferred from the local context with obvious clues (Ballesteros et al., 2020) while the others may need external commonsense knowledge that can be memorized by the language models. As a comparison, both the ROCStories and COPA do not have any extra context, and thus require the pre-trained LMs to know the essential knowledge to solve those problems.\n\nIn the rest of this section, we conduct extensive experiments and case studies to demonstrate the contribution of different components. In all following analysis experiments, we use BERT-large as the base language model and ROCStories as the evaluation dataset.\n\n\nAblation Study\n\nWe conduct an ablation study in Table 5 via LM pretraining with different settings and then finetuning. We can see that all components contribute to the final success of our model, especially the Relation loss. This result again verified that discourse connective prediction is a much more challenging pretraining task as shown in Malmi et al. (2018). CoCoLM is optimized to memorize high-order discourse knowledge that strongly correlates with downstream tasks and thus brings  Table 6: Effect of different event knowledge resources. \"M\" and \"S\" pertain to \"multi-hop\" and \"single-hop\". more performance boost. Besides, when replacing the whole eventuality masking with random token masking, we can observe 0.8% (1.0%) accuracy drop, which indicates the usefulness of eventualitylevel masking. The relative better improvement of Co-occurrence loss suggests our previous assumption that even though compared with other discourse relations (e.g., Before and Cause), the Co-occurrence relations have relatively weaker semantic, it still can help models to better understand events due to its large scale.\n\nTo further verify the effectiveness of proposed methods, we compare with the baseline that the BERT-large models are fine-tuned with only token-level MLM objective on syntactic eventuality sequences. The performance dropped close to finetuning over original BERT-large, which shows that the gains of COCOLM are not simply from the MLM objective and the new proposed objectives as well as masking strategies contributed largely.\n\n\nEffect of Different Knowledge Resources\n\nTo access the effect of high-order ASER commonsense knowledge, we compare with the performance of directly integrating single-hop edges from ASER. We decompose the multi-hop sequences into single-hop edges and keep the comparable size of single-hop edges with multi-hop ones. The results are shown in Table 6, from which we can see that there is still a notable gap between multi-hop and single-hop knowledge injection at the comparable size. Hence multi-hop knowledge is crucial for LMs to understand eventualities. Besides ASER, another important event knowledge resource is ATOMIC , which is a crowdsourced commonsense knowledge graph that contains nine types of if-then casual relations between social-centric events. However, it is a bipartite graph, which symbolically random walk over ATOMIC is impossible like normal graphs. Nevertheless, we are interested in the differences of injecting human-annotated and auto-extracted triplets into LMs. Though relation types and triplet size 6 may vary from other other, Fang et al. (2021) successfully converts discourse knowledge in ASER to if-then knowledge in ATOMIC and shows the former might roughly cover the latter. When injecting into LMs, we can see in Table 6 that ATOMIC can outperform the single-hop version of ASER since ATOMIC is cleaner with human annotations. We leave how to combine ASER and other event knowledge resources (Mostafazadeh et al., 2020;Hwang et al., 2020) to get more high-quality multihop event knowledge as our future work.\n\n\nEffects of Knowledge Retrieval\n\nWe also study the effect of other knowledge injection methods, for example simple retrieving relevant nodes from ASER. We use the BM25 algorithm to retrieve Top 5 relevant nodes for each event in the ROCStories. Following Petroni et al. (2020), retrieved nodes are appended at the end of story context and separated by the [SEP] token. The results show no obvious improvements over baselines. The reason might be that single event nodes could not provide more information and all the tasks require relational knowledge. Advanced integration methods with retrieved knowledge like Lv et al. (2020) and Guu et al. (2020) are worthy to be deeply explored in the future.\n\n\nProbing Experiments\n\nWe present one case study from the probing analysis experiment in Table 7 to further investigate the discourse-aware nature of our proposed language models. Motivated by Petroni et al. (2019), we put a [MASK] token between two events and try to ask the model to predict the connective. Take the case from COPA dataset as an example, connectives predicted by CoCoLM clearly show the effect relation between two events. However predictions from the baseline models reveal weaker (temporal, conjunction) or wrong (contrast) relations. Similar observations could be drawn from another two datasets. Like Table 1, we also sample 300 high-frequency pairs from ASER to predict connectives. The P@1 for CoCoLM (BERT-large) has 15.2% improvement over BERT-large. These observations show that CoCoLM manages to memorize richer discourse knowledge about daily events (Note that connective probing analysis does not mean strong correlations with downstream task performance).  \n\n\nRelated Work\n\nUnderstanding Events. It is important to represent and learn the commonsense knowledge for deeply understanding the causality and correlation between events. Recently various kinds of tasks requiring multiple dimensional event knowledge are proposed such as story ending prediction (Mostafazadeh et al., 2016), event temporal ordering prediction (Ning et al., 2018a), and event casual reasoning (Gordon et al., 2012). Prior studies have incorporated external commonsense knowledge from ConceptNet (Speer et al., 2017) and ATOMIC  for solving event representation , story generation tasks (Guan et al., 2020), KG completion (Bosselut et al., 2019. However, their event-level knowledge is sparse and incomplete due to the human-annotated acquisition, which thus limits the model capacity, especially when injecting into LMs. Zhang et al. (2020b) builds a large-scale eventuality knowledge graph, ASER, by specifying eventuality relations mined from discourse connectives. It explicitly provides structural high-order discourse information between events spanning from temporal, casual to co-occurred relations, which has been proven to be transferable to human-defined commonsense (Zhang et al., 2020a;Fang et al., 2021) and help with script learning (Lv et al., 2020). In this work, we aim at making full use of multidimensional high-order event knowledge in the ASER to help pretrained LMs understand events.  (Yao et al., 2019;Wang et al., 2020b) or descriptions (Wang et al., 2021b;Yu et al., 2020). Besides that, linguistic knowledge (e.g.,synonym/hypernym relations (Lauscher et al., 2020), word-supersense (Levine et al., 2020), dependency parsing (Wang et al., 2020b), and constituent parsing (Zhou et al., 2019)) also plays a critical role to improve LMs. Simple commonsense knowledge from Concept-Net (Speer et al., 2017) is injected into LMs via linked entity-level MLMs and a new distractor loss function (Shen et al., 2020). Last but not least, domain-specific knowledge is also customized to improve relevant tasks such as mined sentiment word (Tian et al., 2020), event temporal patterns , and numerical reasoning data (Geva et al., 2020). We refer readers to Safavi and Koutra (2021) for the comprehensive survey. In this work, we aim at injecting complex commonsense into pre-trained LMs with two significant difference against previous works: 1) we use the event rather than tokens as the semantic unit, and propose to use an eventuality-based masking strategy as well as two auxiliary tasks to help LMs understand events; 2) We first leverage the random walk process on a large-scale knowledge graph to include multi-hop knowledge.\n\n\nConclusion and Future Work\n\nIn this work, we aim at helping pre-trained language models understand complex commonsense about eventualities. Specifically, we first conduct the random walk over a large-scale eventualitybased knowledge graph to collect multi-hop event knowledge and then inject the knowledge into the pre-trained LMs with an eventuality-based mask strategy as well as two auxiliary tasks. Experiments on three downstream tasks as well as extensive analysis demonstrate the effectiveness of the proposed model. As our approach is a general solution, we believe that it can also be helpful for other tasks that require complex commonsense about events.\n\nFor future work, we would sample sub-graph structures to explore more meaningful event-centric commonsense knowledge (Wang et al., 2021a). Moreover, we will equip our models with generative abilities by finetuning powerful T5 (Raffel et al., 2020) or BART  models to help narrative story completion (Ji et al., 2020), commonsense inference (Gabriel et al., 2021), event infilling tasks (Lin et al., 2021). Unified event-aware language models like Zhou et al. (2022) would be promising and interesting directions.\n\n\nA Appendices\n\n\nA.1 Examples of Evaluation Datasets\n\nWe select one example for each commonsense evaluation dataset and list in Table 8. In terms of MA-TRES, it has 13,577 event pairs among 256 articles with 4 temporal relations, i.e., Before (6,874), After(4,570), Equal (470) and Vague (1,656). Compared with MATRES and COPA, solving the ROCStories requires more complex commonsense knowledge to understand the whole narrative and multiple types of relations across events.\n\n\nA.2 ASER Discourse Relations\n\nIn the Table 9, we list ten discourse relations as well as representative connectives (markers) used to train CoCoLM. We further categorize them into  three types: \"temporal\", \"casual\" and \"others\". We refer the readers to original ASER papers (Zhang et al., 2020b) for detailed relation analysis.  Table 9: The discourse relations as well as representative markers in the ASER knowledge graph.\n\n\nA.3 Eventuality Sequences\n\nWe append more sampled eventuality sequences from random walk. Also we organize the sequences into several meta-paths (the paths with same relation patterns). Here only 2-hop and 3-hop sequences are listed and we could observe meaningful high-order connections between eventualities. The sequence distributions with different lengths and types of relations are shown in the Figure 3. We can see that casual relations take up a small share, which again show that causal knowledge tends to be implicit and hard to acquire.\n\n\nA.4 ATOMIC V.S. ASER\n\nIn this section, we summarize the nine casual/effect relations from ATOMIC  in the Table 2. Fang et al. (2021) shows ASER's discourse relations could be converted to causal knowl- Figure 3: The distribution of lengths along with relation edges for generated eventuality sequences. edge in the ATOMIC. Thus ASER might roughly cover the knowledge in the ATOMIC. Moreover, ASER also covers agentless events such as \"the weather is good\", which was partially covered by GLUCOSE (Mostafazadeh et al., 2020) However it contains noise compared with ATOMIC. CoCoLMexperiments show ATOMIC (877K edges) performs better than ASER(4.4 M -5\u00d7larger).\n\n\nIf-Then Types\n\nRelations Causal Types  The if-then types and causal relations between events in the ATOMIC knowledge graph. For relations, \"x\" and \"o\" refer to PersonX and others while \"xAttr\", \"xIntent\", \"xReact\" mean the attribute, intent, reaction of PersonX etc.\n\nTable 2 :\n2Examples of eventuality sequences with different types of discourse relations (highlighted with pink) and \nconnectives (bolded). Note there may exist multi-relational eventuality pairs \n\net al., 2020; \n\n\n5 https://github.com/HKUST-KnowComp/ ASERDataset \nType \n# Train # Dev # Test \n\nROCStories Narrative \n(Multiple) \n\n1,771 \n100 \n1,871 \n\nMATRES \nTemporal 231 * \n25 \n20 \nCOPA \nCausal \n400 \n100 \n500 \n\n\n\nTable 3 :\n3The statistics of evaluation datasets (See exam-\nples in A.1). The tasks are binary or multiple classifica-\ntion problems. Note the dataset of MATRES is split at \nthe article level following Ballesteros et al. (2020). \n\n\n\nTable 4 :\n4Evaluation results on three commonsense task (top scores in boldface). We report the accuracy of ROCStories dataset under normal supervised setting and debiased (D) setting mentioned in the \u00a73.2.\n\nTable 5 :\n5Ablation study on ROCStories test set by removing different model components. occur and rel are discourse relation and co-occurrence loss respectively.\n\nTable 7 :\n7Examples from evaluation datasets. Connectives in blue are predicted by the BERT-large model \nand ones in pink are predicted by CoCoLM (BERT-large). \"P\" and \"N\" represent the positive and negative \ncandidates. \n\n\n\n\nThey mainly differ from knowledge resources, masking strategies, and training objectives. From the resource side, entity-centric KGs are infused into LMs in the form of linked entitiesPeters et al., 2019;Xiong et al., 2020). tripletsInjecting Knowledge into LMs. Though Petroni \net al. (2019) shows that pre-trained LMs store fac-\ntual knowledge without fine-tuning, still, LMs can \nnot handle knowledge-intensive tasks such as open-\n\ndomain question answering or commonsense rea-\nsoning. Previous works explore different ways to \ninject various knowledge into pre-trained LMs for \ndownstream tasks. \n\n\nROC StoriesContext: The Mills next door had a new car. The car was stolen during the weekend. They came to my house and asked me if I knew anything. I told them I didn't, but for some reason they suspected me.Positive Ending: They called the police to come to my house. Negative Ending: They liked me a lot after that. MATRES Fidel Castro {e1: invited} John Paul to {e2: come} for a reason. Label: BEFORE COPA Premise: I knocked on my neighbor's door. Positive Hypothesis: My neighbor invited me in. Negative Hypothesis: My neighbor left his house.Dataset Example \n\n\n\nTable 8 :\n8The examples for all commonsense evaluation datasets.\n\n\nIf-Event-Then-StatexIntent \nCause \nxReact \nEffect \noReact \nEffect \n\nIf-Event-Then-Event \n\nxNeed \nCause \nxEffect \nEffect \nxWant \nEffect \noEffect \nEffect \noWant \nEffect \n\nIf-Event-Then-Persona \nxAttr \nStative \n\n\n\nTable 10 :\n10\nSinha et al. (2021) also explains the success of LMs due to distributional information. These models pretrained over sentences with shuffled word order still achieve high accuracy.\nOur code and models are available at https:// github.com/HKUST-KnowComp/Co2LM.\nUnlike SpanBERT, we have discourse connectives as span boundaries and do not need the SBO objective.\nThe special tokens are based on the base model, i.e., we add \"[CLS]\" and \"[SEP]\" for BERT models and add \"<s>\" and \"</s>\" for RoBERTa models. All notations in the rest of this paragraph are based on BERT.\nThe detailed comparison is included in the AppendixA.4\nAcknowledgementsYangqiu Song was supported by the NSFC Fund (U20B2053) from the NSFC of China, the RIF (R6020-19 and R6021-20) and the GRF (16211520) from RGC of Hong Kong, the MHKJFS (MHP/001/19) from ITC of Hong Kong with special thanks to HKMAAC and CUSBLT, and the Jiangsu Province Science and Technology Collaboration Fund (BZ2021065). We would also like to thank Wei Wang for insightful dicussions.\nConversational neuro-symbolic commonsense reasoning. Forough Arabshahi, Jennifer Lee, Mikayla Gawarecki, Kathryn Mazaitis, Amos Azaria, Tom Mitchell, arXiv:2006.10022arXiv preprintForough Arabshahi, Jennifer Lee, Mikayla Gawarecki, Kathryn Mazaitis, Amos Azaria, and Tom Mitchell. 2020. Conversational neuro-symbolic common- sense reasoning. arXiv preprint arXiv:2006.10022.\n\nOpen-domain event detection using distant supervision. Jun Araki, Teruko Mitamura, Proceedings of the 27th International Conference on Computational Linguistics. the 27th International Conference on Computational LinguisticsSanta Fe, New Mexico, USAAssociation for Computational LinguisticsJun Araki and Teruko Mitamura. 2018. Open-domain event detection using distant supervision. In Pro- ceedings of the 27th International Conference on Computational Linguistics, pages 878-891, Santa Fe, New Mexico, USA. Association for Computa- tional Linguistics.\n\nThe algebra of events. Emmon Bach, Linguistics and philosophy. 91Emmon Bach. 1986. The algebra of events. Linguistics and philosophy, 9(1):5-16.\n\nSevering the edge between before and after: Neural architectures for temporal ordering of events. Miguel Ballesteros, Rishita Anubhai, Shuai Wang, Nima Pourdamghani, Yogarshi Vyas, Jie Ma, Parminder Bhatia, Kathleen Mckeown, Yaser Al-Onaizan, Proceedings of the EMNLP 2020. the EMNLP 2020Miguel Ballesteros, Rishita Anubhai, Shuai Wang, Nima Pourdamghani, Yogarshi Vyas, Jie Ma, Par- minder Bhatia, Kathleen McKeown, and Yaser Al- Onaizan. 2020. Severing the edge between before and after: Neural architectures for temporal ordering of events. In Proceedings of the EMNLP 2020.\n\nComet: Commonsense transformers for automatic knowledge graph construction. Antoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, Yejin Choi, Proceedings of ACL. ACLAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai- tanya Malaviya, Asli Celikyilmaz, and Yejin Choi. 2019. Comet: Commonsense transformers for au- tomatic knowledge graph construction. In Proceed- ings of ACL, pages 4762-4779.\n\nUnsupervised learning of narrative event chains. Nathanael Chambers, Dan Jurafsky, Proceedings of ACL-08: HLT. ACL-08: HLTColumbus, OhioAssociation for Computational LinguisticsNathanael Chambers and Dan Jurafsky. 2008. Unsuper- vised learning of narrative event chains. In Proceed- ings of ACL-08: HLT, pages 789-797, Columbus, Ohio. Association for Computational Linguistics.\n\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, Guoping Hu, arXiv:1906.08101Pre-training with whole word masking for chinese bert. arXiv preprintYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, and Guoping Hu. 2019. Pre-training with whole word masking for chinese bert. arXiv preprint arXiv:1906.08101.\n\nBERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, 10.18653/v1/N19-1423Proceedings of NAACL. NAACLMinneapolis, MinnesotaAssociation for Computational LinguisticsJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language un- derstanding. In Proceedings of NAACL, pages 4171-4186, Minneapolis, Minnesota. Association for Computational Linguistics.\n\nEvent representation learning enhanced with external commonsense knowledge. Xiao Ding, Kuo Liao, Ting Liu, Zhongyang Li, Junwen Duan, Proceedings of EMNLP-IJCNLP. EMNLP-IJCNLPXiao Ding, Kuo Liao, Ting Liu, Zhongyang Li, and Junwen Duan. 2019. Event representation learn- ing enhanced with external commonsense knowl- edge. In Proceedings of EMNLP-IJCNLP, pages 4896-4905.\n\nDiscos: Bridging the gap between discourse knowledge and commonsense knowledge. Tianqing Fang, Hongming Zhang, Weiqi Wang, Y Song, Bin He, abs/2101.00154ArXiv. Tianqing Fang, Hongming Zhang, Weiqi Wang, Y. Song, and Bin He. 2021. Discos: Bridging the gap between discourse knowledge and common- sense knowledge. ArXiv, abs/2101.00154.\n\nEntities as experts: Sparse memory access with entity supervision. Thibault F\u00e9vry, Baldini Livio, Nicholas Soares, Eunsol Fitzgerald, Tom Choi, Kwiatkowski, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Thibault F\u00e9vry, Livio Baldini Soares, Nicholas FitzGer- ald, Eunsol Choi, and Tom Kwiatkowski. 2020. En- tities as experts: Sparse memory access with entity supervision. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process- ing (EMNLP), pages 4937-4951.\n\nParagraph-level commonsense transformers with recurrent memory. Saadia Gabriel, Chandra Bhagavatula, Vered Shwartz, Le Ronan, Maxwell Bras, Yejin Forbes, Choi, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence35Saadia Gabriel, Chandra Bhagavatula, Vered Shwartz, Ronan Le Bras, Maxwell Forbes, and Yejin Choi. 2021. Paragraph-level commonsense transformers with recurrent memory. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12857-12865.\n\nInjecting numerical reasoning skills into language models. Mor Geva, Ankit Gupta, Jonathan Berant, Proceedings of ACL. ACLMor Geva, Ankit Gupta, and Jonathan Berant. 2020. Injecting numerical reasoning skills into language models. In Proceedings of ACL.\n\nSemEval-2012 task 7: Choice of plausible alternatives: An evaluation of commonsense causal reasoning. Andrew Gordon, Zornitsa Kozareva, Melissa Roemmele, Proceedings of SemEval 2012. SemEval 2012Montr\u00e9al, CanadaAndrew Gordon, Zornitsa Kozareva, and Melissa Roemmele. 2012. SemEval-2012 task 7: Choice of plausible alternatives: An evaluation of common- sense causal reasoning. In Proceedings of SemEval 2012, pages 394-398, Montr\u00e9al, Canada.\n\nA knowledge-enhanced pretraining model for commonsense story generation. Jian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, Minlie Huang, TACL. 8Jian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, and Minlie Huang. 2020. A knowledge-enhanced pre- training model for commonsense story generation. TACL, 8:93-108.\n\n. Ana Suchin Gururangan, Swabha Marasovi\u0107, Kyle Swayamdipta, Iz Lo, Doug Beltagy, Noah A Downey, Smith. 2020. Don't stop pretraining: Adapt language models to domains and tasks. ArXiv, abs/2004.10964Suchin Gururangan, Ana Marasovi\u0107, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith. 2020. Don't stop pretraining: Adapt language models to domains and tasks. ArXiv, abs/2004.10964.\n\nRetrieval augmented language model pre-training. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, Ming-Wei Chang, PMLRProceedings of the 37th International Conference on Machine Learning. the 37th International Conference on Machine LearningVirtual Event2020Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pa- supat, and Ming-Wei Chang. 2020. Retrieval aug- mented language model pre-training. In Proceed- ings of the 37th International Conference on Ma- chine Learning, ICML 2020, 13-18 July 2020, Vir- tual Event, volume 119 of Proceedings of Machine Learning Research, pages 3929-3938. PMLR.\n\nComet-atomic 2020: On symbolic and neural commonsense knowledge graphs. Jena D Hwang, Chandra Bhagavatula, Jeff Ronan Le Bras, Keisuke Da, Antoine Sakaguchi, Yejin Bosselut, Choi, abs/2010.05953ArXiv. Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jeff Da, Keisuke Sakaguchi, Antoine Bosselut, and Yejin Choi. 2020. Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs. ArXiv, abs/2010.05953.\n\nLanguage generation with multi-hop reasoning on commonsense knowledge graph. Haozhe Ji, Pei Ke, Shaohan Huang, Furu Wei, Xiaoyan Zhu, Minlie Huang, 10.18653/v1/2020.emnlp-main.54Proceedings of the EMNLP. the EMNLPHaozhe Ji, Pei Ke, Shaohan Huang, Furu Wei, Xi- aoyan Zhu, and Minlie Huang. 2020. Language gen- eration with multi-hop reasoning on commonsense knowledge graph. In Proceedings of the EMNLP, pages 725-736, Online. Association for Computa- tional Linguistics.\n\nHow can we know what language models know?. Zhengbao Jiang, Frank F Xu, Jun Araki, Graham Neubig, 10.1162/tacl_a_00324Transactions of the Association for Computational Linguistics. 8Zhengbao Jiang, Frank F. Xu, Jun Araki, and Graham Neubig. 2020. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423-438.\n\nSpanbert: Improving pre-training by representing and predicting spans. Mandar Joshi, Danqi Chen, Yinhan Liu, S Daniel, Luke Weld, Omer Zettlemoyer, Levy, TACL. 8Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy. 2020. Spanbert: Improving pre-training by representing and predict- ing spans. TACL, 8:64-77.\n\nSpecializing unsupervised pretraining models for word-level semantic similarity. Anne Lauscher, Ivan Vuli\u0107, Maria Edoardo, Anna Ponti, Goran Korhonen, Glava\u0161, Proceedings of COLING. COLINGBarcelona, SpainOnlineAnne Lauscher, Ivan Vuli\u0107, Edoardo Maria Ponti, Anna Korhonen, and Goran Glava\u0161. 2020. Specializ- ing unsupervised pretraining models for word-level semantic similarity. In Proceedings of COLING, pages 1371-1383, Barcelona, Spain (Online).\n\nSenseBERT: Driving some sense into BERT. Yoav Levine, Barak Lenz, Or Dagan, Ori Ram, Dan Padnos, Or Sharir, Shai Shalev-Shwartz, Amnon Shashua, Yoav Shoham, 10.18653/v1/2020.acl-main.423Proceedings of ACL. ACLOnline. Association for Computational LinguisticsYoav Levine, Barak Lenz, Or Dagan, Ori Ram, Dan Padnos, Or Sharir, Shai Shalev-Shwartz, Amnon Shashua, and Yoav Shoham. 2020. SenseBERT: Driving some sense into BERT. In Proceedings of ACL, pages 4656-4667, Online. Association for Computational Linguistics.\n\nBART: Denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension. Mike Lewis, Yinhan Liu, Naman Goyal ; Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, Luke Zettlemoyer, 10.18653/v1/2020.acl-main.703Proceedings of the ACL. the ACLOnline. Association for Computational LinguisticsMike Lewis, Yinhan Liu, Naman Goyal, Mar- jan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre- training for natural language generation, translation, and comprehension. In Proceedings of the ACL, pages 7871-7880, Online. Association for Compu- tational Linguistics.\n\nStory ending prediction by transferable bert. Zhongyang Li, Xiao Ding, Ting Liu, Proceedings of IJCAI. IJCAIAAAI PressZhongyang Li, Xiao Ding, and Ting Liu. 2019. Story ending prediction by transferable bert. In Proceed- ings of IJCAI, pages 1800-1806. AAAI Press.\n\nKagNet: Knowledge-aware graph networks for commonsense reasoning. Xinyue Bill Yuchen Lin, Jamin Chen, Xiang Chen, Ren, 10.18653/v1/D19-1282Proceedings of the EMNLP-IJCNLP. the EMNLP-IJCNLPHong Kong, ChinaAssociation for Computational LinguisticsBill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang Ren. 2019. KagNet: Knowledge-aware graph net- works for commonsense reasoning. In Proceedings of the EMNLP-IJCNLP, pages 2829-2839, Hong Kong, China. Association for Computational Lin- guistics.\n\nConditional generation of temporally-ordered event sequences. Nathanael Shih-Ting Lin, Greg Chambers, Durrett, 10.18653/v1/2021.acl-long.555Proceedings of the ACL. the ACLOnline. Association for Computational LinguisticsShih-Ting Lin, Nathanael Chambers, and Greg Durrett. 2021. Conditional generation of temporally-ordered event sequences. In Proceedings of the ACL, pages 7142-7157, Online. Association for Computational Linguistics.\n\nHaotang Deng, and Ping Wang. 2020. K-BERT: enabling language representation with knowledge graph. Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, AAAI. Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, and Ping Wang. 2020. K-BERT: enabling language representation with knowledge graph. In AAAI February 7-12, 2020, pages 2901- 2908.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. arXiv preprintYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining ap- proach. arXiv preprint arXiv:1907.11692.\n\nIntegrating external event knowledge for script learning. Shangwen Lv, Fuqing Zhu, Songlin Hu, 10.18653/v1/2020.coling-main.27Proceedings of COLING. COLINGBarcelona, SpainOnlineShangwen Lv, Fuqing Zhu, and Songlin Hu. 2020. In- tegrating external event knowledge for script learn- ing. In Proceedings of COLING, pages 306-315, Barcelona, Spain (Online).\n\nAutomatic prediction of discourse connectives. Eric Malmi, Daniele Pighin, Sebastian Krause, Mikhail Kozhevnikov, Proceedings of. nullEric Malmi, Daniele Pighin, Sebastian Krause, and Mikhail Kozhevnikov. 2018. Automatic prediction of discourse connectives. In Proceedings of LREC 2018.\n\nA corpus and cloze evaluation for deeper understanding of commonsense stories. Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, James Allen, Proceedings of the NAACL. the NAACLSan Diego, CaliforniaNasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016. A cor- pus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of the NAACL, pages 839-849, San Diego, California.\n\nGlucose: Generalized and contextualized story explanations. Nasrin Mostafazadeh, Aditya Kalyanpur, Lori Moon, David Buchanan, Lauren Berkowitz, Or Biran, Jennifer Chu-Carroll, EMNLP. Nasrin Mostafazadeh, Aditya Kalyanpur, Lori Moon, David Buchanan, Lauren Berkowitz, Or Biran, and Jennifer Chu-Carroll. 2020. Glucose: Generalized and contextualized story explanations. In EMNLP.\n\nDisSent: Learning sentence representations from explicit discourse relations. Allen Nie, Erin Bennett, Noah Goodman, 10.18653/v1/P19-1442Proceedings of the ACL. the ACLFlorence, ItalyAllen Nie, Erin Bennett, and Noah Goodman. 2019. DisSent: Learning sentence representations from ex- plicit discourse relations. In Proceedings of the ACL, Florence, Italy.\n\nJoint reasoning for temporal and causal relations. Qiang Ning, Zhili Feng, Hao Wu, Dan Roth, 10.18653/v1/P18-1212Proceedings of ACL. ACLMelbourne, AustraliaQiang Ning, Zhili Feng, Hao Wu, and Dan Roth. 2018a. Joint reasoning for temporal and causal relations. In Proceedings of ACL, pages 2278-2288, Melbourne, Australia.\n\nAn improved neural baseline for temporal relation extraction. Qiang Ning, Sanjay Subramanian, Dan Roth, Proceedings of the EMNLP-IJCNLP. the EMNLP-IJCNLPHong Kong, ChinaAssociation for Computational LinguisticsQiang Ning, Sanjay Subramanian, and Dan Roth. 2019. An improved neural baseline for temporal relation extraction. In Proceedings of the EMNLP-IJCNLP, pages 6203-6209, Hong Kong, China. Association for Computational Linguistics.\n\nA multiaxis annotation scheme for event temporal relations. Qiang Ning, Hao Wu, Dan Roth, Proceedings of the ACL. the ACLMelbourne, AustraliaQiang Ning, Hao Wu, and Dan Roth. 2018b. A multi- axis annotation scheme for event temporal relations. In Proceedings of the ACL, pages 1318-1328, Mel- bourne, Australia.\n\nDeepwalk: online learning of social representations. Bryan Perozzi, Rami Al-Rfou, Steven Skiena, Proceedings of KDD. KDDBryan Perozzi, Rami Al-Rfou, and Steven Skiena. 2014. Deepwalk: online learning of social represen- tations. In Proceedings of KDD, pages 701-710.\n\nKnowledge enhanced contextual word representations. E Matthew, Mark Peters, Robert Neumann, Roy Logan, Vidur Schwartz, Sameer Joshi, Noah A Singh, Smith, Proceedings of the EMNLP-IJCNLP. the EMNLP-IJCNLPMatthew E Peters, Mark Neumann, Robert Logan, Roy Schwartz, Vidur Joshi, Sameer Singh, and Noah A Smith. 2019. Knowledge enhanced contextual word representations. In Proceedings of the EMNLP- IJCNLP, pages 43-54.\n\nHow context affects language models' factual predictions. Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt\u00e4schel, Yuxiang Wu, Alexander H Miller, Sebastian Riedel, arXiv:2005.04611arXiv preprintFabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rockt\u00e4schel, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. 2020. How context affects lan- guage models' factual predictions. arXiv preprint arXiv:2005.04611.\n\nLanguage models as knowledge bases?. Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, S H Patrick, Anton Lewis, Yuxiang Bakhtin, Alexander H Wu, Miller, Proceedings of EMNLP-IJCNLP 2019. EMNLP-IJCNLP 2019Fabio Petroni, Tim Rockt\u00e4schel, Sebastian Riedel, Patrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander H. Miller. 2019. Language models as knowledge bases? In Proceedings of EMNLP- IJCNLP 2019, pages 2463-2473.\n\nExploring the limits of transfer learning with a unified text-totext transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. 21140Colin Raffel, Noam Shazeer, Adam Roberts, Kather- ine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to- text transformer. Journal of Machine Learning Re- search, 21(140):1-67.\n\nRelational World Knowledge Representation in Contextual Language Models: A Review. Tara Safavi, Danai Koutra, Proceedings of the EMNLP. the EMNLPOnline and Punta Cana, Dominican Republic. Association for Computational LinguisticsTara Safavi and Danai Koutra. 2021. Relational World Knowledge Representation in Contextual Language Models: A Review. In Proceedings of the EMNLP, pages 1053-1067, Online and Punta Cana, Domini- can Republic. Association for Computational Lin- guistics.\n\nAtomic: An atlas of machine commonsense for ifthen reasoning. Maarten Sap, Emily Ronan Le Bras, Chandra Allaway, Nicholas Bhagavatula, Hannah Lourie, Brendan Rashkin, Roof, A Noah, Yejin Smith, Choi, Proceedings of the AAAI. the AAAI33Maarten Sap, Ronan Le Bras, Emily Allaway, Chan- dra Bhagavatula, Nicholas Lourie, Hannah Rashkin, Brendan Roof, Noah A Smith, and Yejin Choi. 2019. Atomic: An atlas of machine commonsense for if- then reasoning. In Proceedings of the AAAI, vol- ume 33, pages 3027-3035.\n\nTackling the story ending biases in the story cloze test. Rishi Sharma, James Allen, Proceedings of ACL 2018. ACL 2018Omid Bakhshandeh, and Nasrin MostafazadehRishi Sharma, James Allen, Omid Bakhshandeh, and Nasrin Mostafazadeh. 2018. Tackling the story end- ing biases in the story cloze test. In Proceedings of ACL 2018, pages 752-757.\n\nExploiting structured knowledge in text via graph-guided representation learning. Tao Shen, Yi Mao, Pengcheng He, Guodong Long, Adam Trischler, Weizhu Chen, Online. Association for Computational Linguistics. Proceedings of EMNLPTao Shen, Yi Mao, Pengcheng He, Guodong Long, Adam Trischler, and Weizhu Chen. 2020. Exploit- ing structured knowledge in text via graph-guided representation learning. In Proceedings of EMNLP, pages 8980-8994, Online. Association for Computa- tional Linguistics.\n\nMasked language modeling and the distributional hypothesis: Order word matters pre-training for little. Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, Adina Williams, Douwe Kiela, 10.18653/v1/2021.emnlp-main.230Proceedings of the EMNLP. the EMNLPDominican RepublicOnline and Punta CanaKoustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, Adina Williams, and Douwe Kiela. 2021. Masked language modeling and the distributional hypothesis: Order word matters pre-training for lit- tle. In Proceedings of the EMNLP, pages 2888- 2913, Online and Punta Cana, Dominican Republic.\n\nConceptnet 5.5: An open multilingual graph of general knowledge. Robyn Speer, Joshua Chin, Catherine Havasi, Robyn Speer, Joshua Chin, and Catherine Havasi. 2017. Conceptnet 5.5: An open multilingual graph of gen- eral knowledge. pages 4444-4451.\n\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hua Hao Tian, Wu, arXiv:1904.09223Ernie: Enhanced representation through knowledge integration. arXiv preprintYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, and Hua Wu. 2019. Ernie: Enhanced rep- resentation through knowledge integration. arXiv preprint arXiv:1904.09223.\n\nSKEP: Sentiment knowledge enhanced pre-training for sentiment analysis. Can Hao Tian, Xinyan Gao, Hao Xiao, Bolei Liu, Hua He, Haifeng Wu, Feng Wang, Wu, Proceedings of ACL. ACLHao Tian, Can Gao, Xinyan Xiao, Hao Liu, Bolei He, Hua Wu, Haifeng Wang, and Feng Wu. 2020. SKEP: Sentiment knowledge enhanced pre-training for sen- timent analysis. In Proceedings of ACL, pages 4067- 4076, Online. Association for Computational Lin- guistics.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information pro- cessing systems, pages 5998-6008.\n\nFacts as experts: Adaptable and interpretable neural memory over symbolic knowledge. Pat Verga, Haitian Sun, Baldini Livio, William W Soares, Cohen, abs/2007.00849CoRRPat Verga, Haitian Sun, Livio Baldini Soares, and William W. Cohen. 2020. Facts as experts: Adapt- able and interpretable neural memory over symbolic knowledge. CoRR, abs/2007.00849.\n\nSuperglue: A stickier benchmark for general-purpose language understanding systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel Bowman, Advances in Neural Information Processing Systems. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: A stickier benchmark for general-purpose language un- derstanding systems. In Advances in Neural Infor- mation Processing Systems, pages 3266-3280.\n\nConnecting the dots: A knowledgeable path generator for commonsense question answering. Peifeng Wang, Nanyun Peng, Filip Ilievski, Pedro Szekely, Xiang Ren, 10.18653/v1/2020.findings-emnlp.369Findings of the Association for Computational Linguistics: EMNLP 2020. OnlinePeifeng Wang, Nanyun Peng, Filip Ilievski, Pedro Szekely, and Xiang Ren. 2020a. Connecting the dots: A knowledgeable path generator for common- sense question answering. In Findings of the Associ- ation for Computational Linguistics: EMNLP 2020, pages 4129-4140, Online.\n\nContextualized scene imagination for generative commonsense reasoning. Peifeng Wang, Jonathan Zamora, Junfeng Liu, Filip Ilievski, Muhao Chen, Xiang Ren, arXiv:2112.06318arXiv preprintPeiFeng Wang, Jonathan Zamora, Junfeng Liu, Filip Ilievski, Muhao Chen, and Xiang Ren. 2021a. Contextualized scene imagination for gen- erative commonsense reasoning. arXiv preprint arXiv:2112.06318.\n\n. Ruize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Cuihong Cao, Daxin Jiang, Ming Zhou, arXiv:2002.01808arXiv preprintet al. 2020b. K-adapter: Infusing knowledge into pre-trained models with adaptersRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xuanjing Huang, Cuihong Cao, Daxin Jiang, Ming Zhou, et al. 2020b. K-adapter: Infusing knowl- edge into pre-trained models with adapters. arXiv preprint arXiv:2002.01808.\n\nKepler: A unified model for knowledge embedding and pre-trained language representation. Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, Jian Tang, Transactions of the Association for Computational Linguistics. 9Xiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan Zhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021b. Kepler: A unified model for knowledge embedding and pre-trained language representation. Transactions of the Association for Computational Linguistics, 9:176-194.\n\nTransformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Le Xu, Sylvain Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest, Rush, 10.18653/v1/2020.emnlp-demos.6Proceedings of EMNLP. EMNLPAssociation for Computational LinguisticsOnlineThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, Remi Louf, Morgan Funtow- icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Trans- formers: State-of-the-art natural language process- ing. In Proceedings of EMNLP, pages 38-45, On- line. Association for Computational Linguistics.\n\nPretrained encyclopedia: Weakly supervised knowledge-pretrained language model. Wenhan Xiong, Jingfei Du, William Yang Wang, Veselin Stoyanov, International Conference on Learning Representations. Wenhan Xiong, Jingfei Du, William Yang Wang, and Veselin Stoyanov. 2020. Pretrained encyclopedia: Weakly supervised knowledge-pretrained language model. In International Conference on Learning Representations.\n\nLiang Yao, Chengsheng Mao, Yuan Luo, arXiv:1909.03193Kgbert: Bert for knowledge graph completion. arXiv preprintLiang Yao, Chengsheng Mao, and Yuan Luo. 2019. Kg- bert: Bert for knowledge graph completion. arXiv preprint arXiv:1909.03193.\n\nDonghan Yu, Chenguang Zhu, Yiming Yang, Michael Zeng, arXiv:2010.00796Jaket: Joint pre-training of knowledge graph and language understanding. arXiv preprintDonghan Yu, Chenguang Zhu, Yiming Yang, and Michael Zeng. 2020. Jaket: Joint pre-training of knowledge graph and language understanding. arXiv preprint arXiv:2010.00796.\n\nTransomcs: From linguistic graphs to commonsense knowledge. Hongming Zhang, Daniel Khashabi, Yangqiu Song, Dan Roth, Proceedings of IJCAI. IJCAIHongming Zhang, Daniel Khashabi, Yangqiu Song, and Dan Roth. 2020a. Transomcs: From linguistic graphs to commonsense knowledge. In Proceedings of IJCAI.\n\nAser: A largescale eventuality knowledge graph. Hongming Zhang, Xin Liu, Haojie Pan, Yangqiu Song, Cane Wing, -Ki Leung, Proceedings of The Web Conference 2020. The Web Conference 2020Hongming Zhang, Xin Liu, Haojie Pan, Yangqiu Song, and Cane Wing-Ki Leung. 2020b. Aser: A large- scale eventuality knowledge graph. In Proceedings of The Web Conference 2020, pages 201-211.\n\nErnie: Enhanced language representation with informative entities. Zhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, Qun Liu, Proceedings of ACL. ACLZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. 2019. Ernie: Enhanced language representation with informative entities. In Proceedings of ACL, pages 1441-1451.\n\nTemporal common sense acquisition with minimal supervision. Ben Zhou, Qiang Ning, Daniel Khashabi, D Roth, Proceedings of ACL. ACLBen Zhou, Qiang Ning, Daniel Khashabi, and D. Roth. 2020. Temporal common sense acquisition with minimal supervision. In Proceedings of ACL.\n\nJunru Zhou, Zhuosheng Zhang, Hai Zhao, arXiv:1910.14296Limit-bert: Linguistic informed multi-task bert. arXiv preprintJunru Zhou, Zhuosheng Zhang, and Hai Zhao. 2019. Limit-bert: Linguistic informed multi-task bert. arXiv preprint arXiv:1910.14296.\n\nClaret: Pre-training a correlation-aware context-to-event transformer for event-centric generation and classification. Yucheng Zhou, Tao Shen, Xiubo Geng, Guodong Long, Daxin Jiang, arXiv:2203.02225arXiv preprintYucheng Zhou, Tao Shen, Xiubo Geng, Guodong Long, and Daxin Jiang. 2022. Claret: Pre-training a correlation-aware context-to-event transformer for event-centric generation and classification. arXiv preprint arXiv:2203.02225.\n", "annotations": {"author": "[{\"end\":130,\"start\":95},{\"end\":218,\"start\":131},{\"end\":254,\"start\":219},{\"end\":307,\"start\":255}]", "publisher": null, "author_last_name": "[{\"end\":107,\"start\":105},{\"end\":145,\"start\":140},{\"end\":231,\"start\":227},{\"end\":265,\"start\":263}]", "author_first_name": "[{\"end\":104,\"start\":95},{\"end\":139,\"start\":131},{\"end\":226,\"start\":219},{\"end\":262,\"start\":255}]", "author_affiliation": "[{\"end\":129,\"start\":109},{\"end\":190,\"start\":170},{\"end\":217,\"start\":192},{\"end\":253,\"start\":233},{\"end\":306,\"start\":286}]", "title": "[{\"end\":77,\"start\":1},{\"end\":384,\"start\":308}]", "venue": "[{\"end\":437,\"start\":386}]", "abstract": "[{\"end\":1526,\"start\":454}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1634,\"start\":1613},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":1852,\"start\":1830},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":1871,\"start\":1852},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":2135,\"start\":2116},{\"end\":2247,\"start\":2241},{\"end\":2277,\"start\":2271},{\"end\":2356,\"start\":2350},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2482,\"start\":2460},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2920,\"start\":2908},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2945,\"start\":2920},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3022,\"start\":3000},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3092,\"start\":3072},{\"end\":3980,\"start\":3969},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":4339,\"start\":4321},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4358,\"start\":4339},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4746,\"start\":4723},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":4763,\"start\":4746},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5021,\"start\":4994},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":5577,\"start\":5556},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":5806,\"start\":5788},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5828,\"start\":5806},{\"end\":7957,\"start\":7932},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8874,\"start\":8854},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8925,\"start\":8907},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":8944,\"start\":8925},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9339,\"start\":9321},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9847,\"start\":9825},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11292,\"start\":11268},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11608,\"start\":11587},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11625,\"start\":11608},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":11660,\"start\":11642},{\"end\":11704,\"start\":11682},{\"end\":12121,\"start\":12115},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":13667,\"start\":13645},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15100,\"start\":15071},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":17407,\"start\":17388},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":17709,\"start\":17689},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":18454,\"start\":18434},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":18659,\"start\":18639},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":19326,\"start\":19307},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":20605,\"start\":20579},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":21511,\"start\":21492},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23773,\"start\":23755},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":24153,\"start\":24126},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":24172,\"start\":24153},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":24520,\"start\":24499},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":24872,\"start\":24856},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":24894,\"start\":24877},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":25157,\"start\":25136},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":26257,\"start\":26230},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":26314,\"start\":26294},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":26364,\"start\":26343},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":26465,\"start\":26445},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":26554,\"start\":26536},{\"end\":26593,\"start\":26554},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":26791,\"start\":26771},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":27148,\"start\":27127},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":27166,\"start\":27148},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":27214,\"start\":27197},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":27376,\"start\":27358},{\"end\":27395,\"start\":27376},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":27432,\"start\":27412},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":27448,\"start\":27432},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":27541,\"start\":27518},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":27580,\"start\":27559},{\"end\":27621,\"start\":27601},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":27666,\"start\":27647},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":27777,\"start\":27757},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":27882,\"start\":27863},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":28099,\"start\":28080},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":28145,\"start\":28121},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":29402,\"start\":29382},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":29512,\"start\":29491},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":29581,\"start\":29564},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":29627,\"start\":29605},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":29669,\"start\":29651},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":29730,\"start\":29712},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":30551,\"start\":30530},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":31365,\"start\":31347},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":31756,\"start\":31729},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":33610,\"start\":33590},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":33629,\"start\":33610},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":34888,\"start\":34869}]", "figure": "[{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32374,\"start\":32161},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":32573,\"start\":32375},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":32806,\"start\":32574},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":33014,\"start\":32807},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":33178,\"start\":33015},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":33403,\"start\":33179},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":34006,\"start\":33404},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":34575,\"start\":34007},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":34641,\"start\":34576},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":34853,\"start\":34642},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":34868,\"start\":34854}]", "paragraph": "[{\"end\":2098,\"start\":1542},{\"end\":2214,\"start\":2100},{\"end\":2634,\"start\":2231},{\"end\":3588,\"start\":2636},{\"end\":4107,\"start\":3590},{\"end\":4204,\"start\":4118},{\"end\":4608,\"start\":4206},{\"end\":5192,\"start\":4610},{\"end\":7124,\"start\":5194},{\"end\":7367,\"start\":7126},{\"end\":7554,\"start\":7369},{\"end\":7724,\"start\":7556},{\"end\":8321,\"start\":7736},{\"end\":8671,\"start\":8323},{\"end\":9971,\"start\":8707},{\"end\":10795,\"start\":9973},{\"end\":10901,\"start\":10797},{\"end\":11072,\"start\":10903},{\"end\":11319,\"start\":11074},{\"end\":12597,\"start\":11346},{\"end\":13052,\"start\":12599},{\"end\":13787,\"start\":13054},{\"end\":13916,\"start\":13829},{\"end\":14132,\"start\":13936},{\"end\":14433,\"start\":14134},{\"end\":15969,\"start\":14516},{\"end\":16182,\"start\":16014},{\"end\":16229,\"start\":16216},{\"end\":17566,\"start\":16256},{\"end\":17669,\"start\":17595},{\"end\":18258,\"start\":17671},{\"end\":19431,\"start\":18260},{\"end\":20879,\"start\":19456},{\"end\":21142,\"start\":20881},{\"end\":22263,\"start\":21161},{\"end\":22692,\"start\":22265},{\"end\":24242,\"start\":22736},{\"end\":24942,\"start\":24277},{\"end\":25931,\"start\":24966},{\"end\":28596,\"start\":25948},{\"end\":29263,\"start\":28627},{\"end\":29777,\"start\":29265},{\"end\":30253,\"start\":29832},{\"end\":30680,\"start\":30286},{\"end\":31230,\"start\":30710},{\"end\":31891,\"start\":31255},{\"end\":32160,\"start\":31909}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13828,\"start\":13788},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14472,\"start\":14434},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14515,\"start\":14472},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16013,\"start\":15970},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16215,\"start\":16183}]", "table_ref": "[{\"end\":2382,\"start\":2375},{\"end\":2980,\"start\":2973},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":6053,\"start\":6046},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":10577,\"start\":10570},{\"end\":10612,\"start\":10605},{\"end\":14725,\"start\":14718},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":16803,\"start\":16796},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":18004,\"start\":17997},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":19492,\"start\":19485},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":21200,\"start\":21193},{\"end\":21647,\"start\":21640},{\"end\":23044,\"start\":23037},{\"end\":23954,\"start\":23947},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":25039,\"start\":25032},{\"end\":25573,\"start\":25566},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":29913,\"start\":29906},{\"end\":30300,\"start\":30293},{\"end\":30592,\"start\":30585},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":31345,\"start\":31338}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1540,\"start\":1528},{\"end\":2229,\"start\":2217},{\"end\":4116,\"start\":4110},{\"attributes\":{\"n\":\"2\"},\"end\":7734,\"start\":7727},{\"attributes\":{\"n\":\"2.1\"},\"end\":8705,\"start\":8674},{\"attributes\":{\"n\":\"2.2\"},\"end\":11344,\"start\":11322},{\"attributes\":{\"n\":\"2.3\"},\"end\":13934,\"start\":13919},{\"attributes\":{\"n\":\"3.1\"},\"end\":16254,\"start\":16232},{\"attributes\":{\"n\":\"3.2\"},\"end\":17593,\"start\":17569},{\"attributes\":{\"n\":\"4\"},\"end\":19454,\"start\":19434},{\"attributes\":{\"n\":\"4.1\"},\"end\":21159,\"start\":21145},{\"attributes\":{\"n\":\"4.2\"},\"end\":22734,\"start\":22695},{\"attributes\":{\"n\":\"4.3\"},\"end\":24275,\"start\":24245},{\"attributes\":{\"n\":\"4.4\"},\"end\":24964,\"start\":24945},{\"attributes\":{\"n\":\"5\"},\"end\":25946,\"start\":25934},{\"attributes\":{\"n\":\"6\"},\"end\":28625,\"start\":28599},{\"end\":29792,\"start\":29780},{\"end\":29830,\"start\":29795},{\"end\":30284,\"start\":30256},{\"end\":30708,\"start\":30683},{\"end\":31253,\"start\":31233},{\"end\":31907,\"start\":31894},{\"end\":32171,\"start\":32162},{\"end\":32584,\"start\":32575},{\"end\":32817,\"start\":32808},{\"end\":33025,\"start\":33016},{\"end\":33189,\"start\":33180},{\"end\":34586,\"start\":34577},{\"end\":34865,\"start\":34855}]", "table": "[{\"end\":32374,\"start\":32173},{\"end\":32573,\"start\":32418},{\"end\":32806,\"start\":32586},{\"end\":33403,\"start\":33191},{\"end\":34006,\"start\":33639},{\"end\":34575,\"start\":34557},{\"end\":34853,\"start\":34663}]", "figure_caption": "[{\"end\":32418,\"start\":32377},{\"end\":33014,\"start\":32819},{\"end\":33178,\"start\":33027},{\"end\":33639,\"start\":33406},{\"end\":34557,\"start\":34009},{\"end\":34641,\"start\":34588},{\"end\":34663,\"start\":34644}]", "figure_ref": "[{\"end\":7792,\"start\":7784},{\"end\":8131,\"start\":8123},{\"end\":12158,\"start\":12150},{\"end\":13277,\"start\":13269},{\"end\":30073,\"start\":30060},{\"end\":31092,\"start\":31084},{\"end\":31443,\"start\":31435}]", "bib_author_first_name": "[{\"end\":35955,\"start\":35948},{\"end\":35975,\"start\":35967},{\"end\":35988,\"start\":35981},{\"end\":36007,\"start\":36000},{\"end\":36022,\"start\":36018},{\"end\":36034,\"start\":36031},{\"end\":36329,\"start\":36326},{\"end\":36343,\"start\":36337},{\"end\":36853,\"start\":36848},{\"end\":37075,\"start\":37069},{\"end\":37096,\"start\":37089},{\"end\":37111,\"start\":37106},{\"end\":37122,\"start\":37118},{\"end\":37145,\"start\":37137},{\"end\":37155,\"start\":37152},{\"end\":37169,\"start\":37160},{\"end\":37186,\"start\":37178},{\"end\":37201,\"start\":37196},{\"end\":37633,\"start\":37626},{\"end\":37650,\"start\":37644},{\"end\":37667,\"start\":37660},{\"end\":37682,\"start\":37673},{\"end\":37697,\"start\":37693},{\"end\":37716,\"start\":37711},{\"end\":38034,\"start\":38025},{\"end\":38048,\"start\":38045},{\"end\":38361,\"start\":38355},{\"end\":38375,\"start\":38367},{\"end\":38385,\"start\":38381},{\"end\":38395,\"start\":38391},{\"end\":38407,\"start\":38401},{\"end\":38420,\"start\":38414},{\"end\":38434,\"start\":38427},{\"end\":38794,\"start\":38789},{\"end\":38811,\"start\":38803},{\"end\":38825,\"start\":38819},{\"end\":38839,\"start\":38831},{\"end\":39307,\"start\":39303},{\"end\":39317,\"start\":39314},{\"end\":39328,\"start\":39324},{\"end\":39343,\"start\":39334},{\"end\":39354,\"start\":39348},{\"end\":39688,\"start\":39680},{\"end\":39703,\"start\":39695},{\"end\":39716,\"start\":39711},{\"end\":39724,\"start\":39723},{\"end\":39734,\"start\":39731},{\"end\":40011,\"start\":40003},{\"end\":40026,\"start\":40019},{\"end\":40042,\"start\":40034},{\"end\":40057,\"start\":40051},{\"end\":40073,\"start\":40070},{\"end\":40627,\"start\":40621},{\"end\":40644,\"start\":40637},{\"end\":40663,\"start\":40658},{\"end\":40675,\"start\":40673},{\"end\":40690,\"start\":40683},{\"end\":40702,\"start\":40697},{\"end\":41156,\"start\":41153},{\"end\":41168,\"start\":41163},{\"end\":41184,\"start\":41176},{\"end\":41457,\"start\":41451},{\"end\":41474,\"start\":41466},{\"end\":41492,\"start\":41485},{\"end\":41869,\"start\":41865},{\"end\":41879,\"start\":41876},{\"end\":41893,\"start\":41887},{\"end\":41907,\"start\":41900},{\"end\":41919,\"start\":41913},{\"end\":42103,\"start\":42100},{\"end\":42129,\"start\":42123},{\"end\":42145,\"start\":42141},{\"end\":42161,\"start\":42159},{\"end\":42170,\"start\":42166},{\"end\":42184,\"start\":42180},{\"end\":42186,\"start\":42185},{\"end\":42557,\"start\":42551},{\"end\":42569,\"start\":42563},{\"end\":42579,\"start\":42575},{\"end\":42594,\"start\":42586},{\"end\":42612,\"start\":42604},{\"end\":43174,\"start\":43170},{\"end\":43176,\"start\":43175},{\"end\":43191,\"start\":43184},{\"end\":43209,\"start\":43205},{\"end\":43232,\"start\":43225},{\"end\":43244,\"start\":43237},{\"end\":43261,\"start\":43256},{\"end\":43597,\"start\":43591},{\"end\":43605,\"start\":43602},{\"end\":43617,\"start\":43610},{\"end\":43629,\"start\":43625},{\"end\":43642,\"start\":43635},{\"end\":43654,\"start\":43648},{\"end\":44039,\"start\":44031},{\"end\":44052,\"start\":44047},{\"end\":44054,\"start\":44053},{\"end\":44062,\"start\":44059},{\"end\":44076,\"start\":44070},{\"end\":44429,\"start\":44423},{\"end\":44442,\"start\":44437},{\"end\":44455,\"start\":44449},{\"end\":44462,\"start\":44461},{\"end\":44475,\"start\":44471},{\"end\":44486,\"start\":44482},{\"end\":44779,\"start\":44775},{\"end\":44794,\"start\":44790},{\"end\":44807,\"start\":44802},{\"end\":44821,\"start\":44817},{\"end\":44834,\"start\":44829},{\"end\":45190,\"start\":45186},{\"end\":45204,\"start\":45199},{\"end\":45213,\"start\":45211},{\"end\":45224,\"start\":45221},{\"end\":45233,\"start\":45230},{\"end\":45244,\"start\":45242},{\"end\":45257,\"start\":45253},{\"end\":45279,\"start\":45274},{\"end\":45293,\"start\":45289},{\"end\":45780,\"start\":45776},{\"end\":45794,\"start\":45788},{\"end\":45805,\"start\":45800},{\"end\":45839,\"start\":45835},{\"end\":45853,\"start\":45846},{\"end\":45868,\"start\":45864},{\"end\":46399,\"start\":46390},{\"end\":46408,\"start\":46404},{\"end\":46419,\"start\":46415},{\"end\":46682,\"start\":46676},{\"end\":46705,\"start\":46700},{\"end\":46717,\"start\":46712},{\"end\":47174,\"start\":47165},{\"end\":47194,\"start\":47190},{\"end\":47644,\"start\":47638},{\"end\":47654,\"start\":47650},{\"end\":47664,\"start\":47661},{\"end\":47677,\"start\":47671},{\"end\":47686,\"start\":47684},{\"end\":47902,\"start\":47896},{\"end\":47912,\"start\":47908},{\"end\":47923,\"start\":47918},{\"end\":47938,\"start\":47931},{\"end\":47949,\"start\":47943},{\"end\":47962,\"start\":47957},{\"end\":47973,\"start\":47969},{\"end\":47984,\"start\":47980},{\"end\":47996,\"start\":47992},{\"end\":48017,\"start\":48010},{\"end\":48418,\"start\":48410},{\"end\":48429,\"start\":48423},{\"end\":48442,\"start\":48435},{\"end\":48758,\"start\":48754},{\"end\":48773,\"start\":48766},{\"end\":48791,\"start\":48782},{\"end\":48807,\"start\":48800},{\"end\":49080,\"start\":49074},{\"end\":49104,\"start\":49095},{\"end\":49123,\"start\":49115},{\"end\":49132,\"start\":49128},{\"end\":49146,\"start\":49141},{\"end\":49158,\"start\":49154},{\"end\":49180,\"start\":49172},{\"end\":49193,\"start\":49188},{\"end\":49609,\"start\":49603},{\"end\":49630,\"start\":49624},{\"end\":49646,\"start\":49642},{\"end\":49658,\"start\":49653},{\"end\":49675,\"start\":49669},{\"end\":49689,\"start\":49687},{\"end\":49705,\"start\":49697},{\"end\":50006,\"start\":50001},{\"end\":50016,\"start\":50012},{\"end\":50030,\"start\":50026},{\"end\":50336,\"start\":50331},{\"end\":50348,\"start\":50343},{\"end\":50358,\"start\":50355},{\"end\":50366,\"start\":50363},{\"end\":50670,\"start\":50665},{\"end\":50683,\"start\":50677},{\"end\":50700,\"start\":50697},{\"end\":51107,\"start\":51102},{\"end\":51117,\"start\":51114},{\"end\":51125,\"start\":51122},{\"end\":51413,\"start\":51408},{\"end\":51427,\"start\":51423},{\"end\":51443,\"start\":51437},{\"end\":51676,\"start\":51675},{\"end\":51690,\"start\":51686},{\"end\":51705,\"start\":51699},{\"end\":51718,\"start\":51715},{\"end\":51731,\"start\":51726},{\"end\":51748,\"start\":51742},{\"end\":51762,\"start\":51756},{\"end\":52103,\"start\":52098},{\"end\":52120,\"start\":52113},{\"end\":52138,\"start\":52128},{\"end\":52150,\"start\":52147},{\"end\":52171,\"start\":52164},{\"end\":52185,\"start\":52176},{\"end\":52187,\"start\":52186},{\"end\":52205,\"start\":52196},{\"end\":52506,\"start\":52501},{\"end\":52519,\"start\":52516},{\"end\":52542,\"start\":52533},{\"end\":52552,\"start\":52551},{\"end\":52554,\"start\":52553},{\"end\":52569,\"start\":52564},{\"end\":52584,\"start\":52577},{\"end\":52603,\"start\":52594},{\"end\":52605,\"start\":52604},{\"end\":52977,\"start\":52972},{\"end\":52990,\"start\":52986},{\"end\":53004,\"start\":53000},{\"end\":53023,\"start\":53014},{\"end\":53035,\"start\":53029},{\"end\":53051,\"start\":53044},{\"end\":53065,\"start\":53060},{\"end\":53075,\"start\":53072},{\"end\":53085,\"start\":53080},{\"end\":53087,\"start\":53086},{\"end\":53496,\"start\":53492},{\"end\":53510,\"start\":53505},{\"end\":53963,\"start\":53956},{\"end\":53974,\"start\":53969},{\"end\":53997,\"start\":53990},{\"end\":54015,\"start\":54007},{\"end\":54035,\"start\":54029},{\"end\":54051,\"start\":54044},{\"end\":54068,\"start\":54067},{\"end\":54080,\"start\":54075},{\"end\":54464,\"start\":54459},{\"end\":54478,\"start\":54473},{\"end\":54825,\"start\":54822},{\"end\":54834,\"start\":54832},{\"end\":54849,\"start\":54840},{\"end\":54861,\"start\":54854},{\"end\":54872,\"start\":54868},{\"end\":54890,\"start\":54884},{\"end\":55344,\"start\":55337},{\"end\":55357,\"start\":55352},{\"end\":55370,\"start\":55363},{\"end\":55385,\"start\":55379},{\"end\":55399,\"start\":55394},{\"end\":55415,\"start\":55410},{\"end\":55891,\"start\":55886},{\"end\":55905,\"start\":55899},{\"end\":55921,\"start\":55912},{\"end\":56071,\"start\":56069},{\"end\":56085,\"start\":56077},{\"end\":56097,\"start\":56092},{\"end\":56108,\"start\":56102},{\"end\":56119,\"start\":56115},{\"end\":56129,\"start\":56126},{\"end\":56140,\"start\":56137},{\"end\":56155,\"start\":56147},{\"end\":56164,\"start\":56161},{\"end\":56564,\"start\":56561},{\"end\":56581,\"start\":56575},{\"end\":56590,\"start\":56587},{\"end\":56602,\"start\":56597},{\"end\":56611,\"start\":56608},{\"end\":56623,\"start\":56616},{\"end\":56632,\"start\":56628},{\"end\":56960,\"start\":56954},{\"end\":56974,\"start\":56970},{\"end\":56988,\"start\":56984},{\"end\":57002,\"start\":56997},{\"end\":57019,\"start\":57014},{\"end\":57032,\"start\":57027},{\"end\":57034,\"start\":57033},{\"end\":57048,\"start\":57042},{\"end\":57062,\"start\":57057},{\"end\":57446,\"start\":57443},{\"end\":57461,\"start\":57454},{\"end\":57474,\"start\":57467},{\"end\":57489,\"start\":57482},{\"end\":57491,\"start\":57490},{\"end\":57797,\"start\":57793},{\"end\":57808,\"start\":57804},{\"end\":57830,\"start\":57824},{\"end\":57848,\"start\":57839},{\"end\":57862,\"start\":57856},{\"end\":57877,\"start\":57872},{\"end\":57888,\"start\":57884},{\"end\":57901,\"start\":57895},{\"end\":58343,\"start\":58336},{\"end\":58356,\"start\":58350},{\"end\":58368,\"start\":58363},{\"end\":58384,\"start\":58379},{\"end\":58399,\"start\":58394},{\"end\":58867,\"start\":58860},{\"end\":58882,\"start\":58874},{\"end\":58898,\"start\":58891},{\"end\":58909,\"start\":58904},{\"end\":58925,\"start\":58920},{\"end\":58937,\"start\":58932},{\"end\":59181,\"start\":59176},{\"end\":59192,\"start\":59188},{\"end\":59202,\"start\":59199},{\"end\":59216,\"start\":59209},{\"end\":59230,\"start\":59222},{\"end\":59245,\"start\":59238},{\"end\":59256,\"start\":59251},{\"end\":59268,\"start\":59264},{\"end\":59700,\"start\":59693},{\"end\":59713,\"start\":59707},{\"end\":59728,\"start\":59719},{\"end\":59742,\"start\":59734},{\"end\":59757,\"start\":59750},{\"end\":59769,\"start\":59763},{\"end\":59778,\"start\":59774},{\"end\":60182,\"start\":60176},{\"end\":60197,\"start\":60189},{\"end\":60211,\"start\":60205},{\"end\":60224,\"start\":60218},{\"end\":60242,\"start\":60235},{\"end\":60260,\"start\":60253},{\"end\":60273,\"start\":60266},{\"end\":60285,\"start\":60282},{\"end\":60297,\"start\":60293},{\"end\":60310,\"start\":60304},{\"end\":60325,\"start\":60322},{\"end\":60338,\"start\":60335},{\"end\":60354,\"start\":60349},{\"end\":60381,\"start\":60375},{\"end\":60392,\"start\":60386},{\"end\":60408,\"start\":60402},{\"end\":60419,\"start\":60414},{\"end\":60422,\"start\":60420},{\"end\":60434,\"start\":60427},{\"end\":60448,\"start\":60441},{\"end\":60464,\"start\":60457},{\"end\":60481,\"start\":60472},{\"end\":61176,\"start\":61170},{\"end\":61191,\"start\":61184},{\"end\":61203,\"start\":61196},{\"end\":61208,\"start\":61204},{\"end\":61222,\"start\":61215},{\"end\":61503,\"start\":61498},{\"end\":61519,\"start\":61509},{\"end\":61529,\"start\":61525},{\"end\":61745,\"start\":61738},{\"end\":61759,\"start\":61750},{\"end\":61771,\"start\":61765},{\"end\":61785,\"start\":61778},{\"end\":62134,\"start\":62126},{\"end\":62148,\"start\":62142},{\"end\":62166,\"start\":62159},{\"end\":62176,\"start\":62173},{\"end\":62420,\"start\":62412},{\"end\":62431,\"start\":62428},{\"end\":62443,\"start\":62437},{\"end\":62456,\"start\":62449},{\"end\":62467,\"start\":62463},{\"end\":62477,\"start\":62474},{\"end\":62814,\"start\":62806},{\"end\":62824,\"start\":62822},{\"end\":62837,\"start\":62830},{\"end\":62846,\"start\":62843},{\"end\":62861,\"start\":62854},{\"end\":62870,\"start\":62867},{\"end\":63150,\"start\":63147},{\"end\":63162,\"start\":63157},{\"end\":63175,\"start\":63169},{\"end\":63187,\"start\":63186},{\"end\":63364,\"start\":63359},{\"end\":63380,\"start\":63371},{\"end\":63391,\"start\":63388},{\"end\":63735,\"start\":63728},{\"end\":63745,\"start\":63742},{\"end\":63757,\"start\":63752},{\"end\":63771,\"start\":63764},{\"end\":63783,\"start\":63778}]", "bib_author_last_name": "[{\"end\":35965,\"start\":35956},{\"end\":35979,\"start\":35976},{\"end\":35998,\"start\":35989},{\"end\":36016,\"start\":36008},{\"end\":36029,\"start\":36023},{\"end\":36043,\"start\":36035},{\"end\":36335,\"start\":36330},{\"end\":36352,\"start\":36344},{\"end\":36858,\"start\":36854},{\"end\":37087,\"start\":37076},{\"end\":37104,\"start\":37097},{\"end\":37116,\"start\":37112},{\"end\":37135,\"start\":37123},{\"end\":37150,\"start\":37146},{\"end\":37158,\"start\":37156},{\"end\":37176,\"start\":37170},{\"end\":37194,\"start\":37187},{\"end\":37212,\"start\":37202},{\"end\":37642,\"start\":37634},{\"end\":37658,\"start\":37651},{\"end\":37671,\"start\":37668},{\"end\":37691,\"start\":37683},{\"end\":37709,\"start\":37698},{\"end\":37721,\"start\":37717},{\"end\":38043,\"start\":38035},{\"end\":38057,\"start\":38049},{\"end\":38365,\"start\":38362},{\"end\":38379,\"start\":38376},{\"end\":38389,\"start\":38386},{\"end\":38399,\"start\":38396},{\"end\":38412,\"start\":38408},{\"end\":38425,\"start\":38421},{\"end\":38437,\"start\":38435},{\"end\":38801,\"start\":38795},{\"end\":38817,\"start\":38812},{\"end\":38829,\"start\":38826},{\"end\":38849,\"start\":38840},{\"end\":39312,\"start\":39308},{\"end\":39322,\"start\":39318},{\"end\":39332,\"start\":39329},{\"end\":39346,\"start\":39344},{\"end\":39359,\"start\":39355},{\"end\":39693,\"start\":39689},{\"end\":39709,\"start\":39704},{\"end\":39721,\"start\":39717},{\"end\":39729,\"start\":39725},{\"end\":39737,\"start\":39735},{\"end\":40017,\"start\":40012},{\"end\":40032,\"start\":40027},{\"end\":40049,\"start\":40043},{\"end\":40068,\"start\":40058},{\"end\":40078,\"start\":40074},{\"end\":40091,\"start\":40080},{\"end\":40635,\"start\":40628},{\"end\":40656,\"start\":40645},{\"end\":40671,\"start\":40664},{\"end\":40681,\"start\":40676},{\"end\":40695,\"start\":40691},{\"end\":40709,\"start\":40703},{\"end\":40715,\"start\":40711},{\"end\":41161,\"start\":41157},{\"end\":41174,\"start\":41169},{\"end\":41191,\"start\":41185},{\"end\":41464,\"start\":41458},{\"end\":41483,\"start\":41475},{\"end\":41501,\"start\":41493},{\"end\":41874,\"start\":41870},{\"end\":41885,\"start\":41880},{\"end\":41898,\"start\":41894},{\"end\":41911,\"start\":41908},{\"end\":41925,\"start\":41920},{\"end\":42121,\"start\":42104},{\"end\":42139,\"start\":42130},{\"end\":42157,\"start\":42146},{\"end\":42164,\"start\":42162},{\"end\":42178,\"start\":42171},{\"end\":42193,\"start\":42187},{\"end\":42561,\"start\":42558},{\"end\":42573,\"start\":42570},{\"end\":42584,\"start\":42580},{\"end\":42602,\"start\":42595},{\"end\":42618,\"start\":42613},{\"end\":43182,\"start\":43177},{\"end\":43203,\"start\":43192},{\"end\":43223,\"start\":43210},{\"end\":43235,\"start\":43233},{\"end\":43254,\"start\":43245},{\"end\":43270,\"start\":43262},{\"end\":43276,\"start\":43272},{\"end\":43600,\"start\":43598},{\"end\":43608,\"start\":43606},{\"end\":43623,\"start\":43618},{\"end\":43633,\"start\":43630},{\"end\":43646,\"start\":43643},{\"end\":43660,\"start\":43655},{\"end\":44045,\"start\":44040},{\"end\":44057,\"start\":44055},{\"end\":44068,\"start\":44063},{\"end\":44083,\"start\":44077},{\"end\":44435,\"start\":44430},{\"end\":44447,\"start\":44443},{\"end\":44459,\"start\":44456},{\"end\":44469,\"start\":44463},{\"end\":44480,\"start\":44476},{\"end\":44498,\"start\":44487},{\"end\":44504,\"start\":44500},{\"end\":44788,\"start\":44780},{\"end\":44800,\"start\":44795},{\"end\":44815,\"start\":44808},{\"end\":44827,\"start\":44822},{\"end\":44843,\"start\":44835},{\"end\":44851,\"start\":44845},{\"end\":45197,\"start\":45191},{\"end\":45209,\"start\":45205},{\"end\":45219,\"start\":45214},{\"end\":45228,\"start\":45225},{\"end\":45240,\"start\":45234},{\"end\":45251,\"start\":45245},{\"end\":45272,\"start\":45258},{\"end\":45287,\"start\":45280},{\"end\":45300,\"start\":45294},{\"end\":45786,\"start\":45781},{\"end\":45798,\"start\":45795},{\"end\":45833,\"start\":45806},{\"end\":45844,\"start\":45840},{\"end\":45862,\"start\":45854},{\"end\":45880,\"start\":45869},{\"end\":46402,\"start\":46400},{\"end\":46413,\"start\":46409},{\"end\":46423,\"start\":46420},{\"end\":46698,\"start\":46683},{\"end\":46710,\"start\":46706},{\"end\":46722,\"start\":46718},{\"end\":46727,\"start\":46724},{\"end\":47188,\"start\":47175},{\"end\":47203,\"start\":47195},{\"end\":47212,\"start\":47205},{\"end\":47648,\"start\":47645},{\"end\":47659,\"start\":47655},{\"end\":47669,\"start\":47665},{\"end\":47682,\"start\":47678},{\"end\":47689,\"start\":47687},{\"end\":47906,\"start\":47903},{\"end\":47916,\"start\":47913},{\"end\":47929,\"start\":47924},{\"end\":47941,\"start\":47939},{\"end\":47955,\"start\":47950},{\"end\":47967,\"start\":47963},{\"end\":47978,\"start\":47974},{\"end\":47990,\"start\":47985},{\"end\":48008,\"start\":47997},{\"end\":48026,\"start\":48018},{\"end\":48421,\"start\":48419},{\"end\":48433,\"start\":48430},{\"end\":48445,\"start\":48443},{\"end\":48764,\"start\":48759},{\"end\":48780,\"start\":48774},{\"end\":48798,\"start\":48792},{\"end\":48819,\"start\":48808},{\"end\":49093,\"start\":49081},{\"end\":49113,\"start\":49105},{\"end\":49126,\"start\":49124},{\"end\":49139,\"start\":49133},{\"end\":49152,\"start\":49147},{\"end\":49170,\"start\":49159},{\"end\":49186,\"start\":49181},{\"end\":49199,\"start\":49194},{\"end\":49622,\"start\":49610},{\"end\":49640,\"start\":49631},{\"end\":49651,\"start\":49647},{\"end\":49667,\"start\":49659},{\"end\":49685,\"start\":49676},{\"end\":49695,\"start\":49690},{\"end\":49717,\"start\":49706},{\"end\":50010,\"start\":50007},{\"end\":50024,\"start\":50017},{\"end\":50038,\"start\":50031},{\"end\":50341,\"start\":50337},{\"end\":50353,\"start\":50349},{\"end\":50361,\"start\":50359},{\"end\":50371,\"start\":50367},{\"end\":50675,\"start\":50671},{\"end\":50695,\"start\":50684},{\"end\":50705,\"start\":50701},{\"end\":51112,\"start\":51108},{\"end\":51120,\"start\":51118},{\"end\":51130,\"start\":51126},{\"end\":51421,\"start\":51414},{\"end\":51435,\"start\":51428},{\"end\":51450,\"start\":51444},{\"end\":51684,\"start\":51677},{\"end\":51697,\"start\":51691},{\"end\":51713,\"start\":51706},{\"end\":51724,\"start\":51719},{\"end\":51740,\"start\":51732},{\"end\":51754,\"start\":51749},{\"end\":51768,\"start\":51763},{\"end\":51775,\"start\":51770},{\"end\":52111,\"start\":52104},{\"end\":52126,\"start\":52121},{\"end\":52145,\"start\":52139},{\"end\":52162,\"start\":52151},{\"end\":52174,\"start\":52172},{\"end\":52194,\"start\":52188},{\"end\":52212,\"start\":52206},{\"end\":52514,\"start\":52507},{\"end\":52531,\"start\":52520},{\"end\":52549,\"start\":52543},{\"end\":52562,\"start\":52555},{\"end\":52575,\"start\":52570},{\"end\":52592,\"start\":52585},{\"end\":52608,\"start\":52606},{\"end\":52616,\"start\":52610},{\"end\":52984,\"start\":52978},{\"end\":52998,\"start\":52991},{\"end\":53012,\"start\":53005},{\"end\":53027,\"start\":53024},{\"end\":53042,\"start\":53036},{\"end\":53058,\"start\":53052},{\"end\":53070,\"start\":53066},{\"end\":53078,\"start\":53076},{\"end\":53091,\"start\":53088},{\"end\":53503,\"start\":53497},{\"end\":53517,\"start\":53511},{\"end\":53967,\"start\":53964},{\"end\":53988,\"start\":53975},{\"end\":54005,\"start\":53998},{\"end\":54027,\"start\":54016},{\"end\":54042,\"start\":54036},{\"end\":54059,\"start\":54052},{\"end\":54065,\"start\":54061},{\"end\":54073,\"start\":54069},{\"end\":54086,\"start\":54081},{\"end\":54092,\"start\":54088},{\"end\":54471,\"start\":54465},{\"end\":54484,\"start\":54479},{\"end\":54830,\"start\":54826},{\"end\":54838,\"start\":54835},{\"end\":54852,\"start\":54850},{\"end\":54866,\"start\":54862},{\"end\":54882,\"start\":54873},{\"end\":54895,\"start\":54891},{\"end\":55350,\"start\":55345},{\"end\":55361,\"start\":55358},{\"end\":55377,\"start\":55371},{\"end\":55392,\"start\":55386},{\"end\":55408,\"start\":55400},{\"end\":55421,\"start\":55416},{\"end\":55897,\"start\":55892},{\"end\":55910,\"start\":55906},{\"end\":55928,\"start\":55922},{\"end\":56075,\"start\":56072},{\"end\":56090,\"start\":56086},{\"end\":56100,\"start\":56098},{\"end\":56113,\"start\":56109},{\"end\":56124,\"start\":56120},{\"end\":56135,\"start\":56130},{\"end\":56145,\"start\":56141},{\"end\":56159,\"start\":56156},{\"end\":56173,\"start\":56165},{\"end\":56177,\"start\":56175},{\"end\":56573,\"start\":56565},{\"end\":56585,\"start\":56582},{\"end\":56595,\"start\":56591},{\"end\":56606,\"start\":56603},{\"end\":56614,\"start\":56612},{\"end\":56626,\"start\":56624},{\"end\":56637,\"start\":56633},{\"end\":56641,\"start\":56639},{\"end\":56968,\"start\":56961},{\"end\":56982,\"start\":56975},{\"end\":56995,\"start\":56989},{\"end\":57012,\"start\":57003},{\"end\":57025,\"start\":57020},{\"end\":57040,\"start\":57035},{\"end\":57055,\"start\":57049},{\"end\":57073,\"start\":57063},{\"end\":57452,\"start\":57447},{\"end\":57465,\"start\":57462},{\"end\":57480,\"start\":57475},{\"end\":57498,\"start\":57492},{\"end\":57505,\"start\":57500},{\"end\":57802,\"start\":57798},{\"end\":57822,\"start\":57809},{\"end\":57837,\"start\":57831},{\"end\":57854,\"start\":57849},{\"end\":57870,\"start\":57863},{\"end\":57882,\"start\":57878},{\"end\":57893,\"start\":57889},{\"end\":57908,\"start\":57902},{\"end\":58348,\"start\":58344},{\"end\":58361,\"start\":58357},{\"end\":58377,\"start\":58369},{\"end\":58392,\"start\":58385},{\"end\":58403,\"start\":58400},{\"end\":58872,\"start\":58868},{\"end\":58889,\"start\":58883},{\"end\":58902,\"start\":58899},{\"end\":58918,\"start\":58910},{\"end\":58930,\"start\":58926},{\"end\":58941,\"start\":58938},{\"end\":59186,\"start\":59182},{\"end\":59197,\"start\":59193},{\"end\":59207,\"start\":59203},{\"end\":59220,\"start\":59217},{\"end\":59236,\"start\":59231},{\"end\":59249,\"start\":59246},{\"end\":59262,\"start\":59257},{\"end\":59273,\"start\":59269},{\"end\":59705,\"start\":59701},{\"end\":59717,\"start\":59714},{\"end\":59732,\"start\":59729},{\"end\":59748,\"start\":59743},{\"end\":59761,\"start\":59758},{\"end\":59772,\"start\":59770},{\"end\":59783,\"start\":59779},{\"end\":60187,\"start\":60183},{\"end\":60203,\"start\":60198},{\"end\":60216,\"start\":60212},{\"end\":60233,\"start\":60225},{\"end\":60251,\"start\":60243},{\"end\":60264,\"start\":60261},{\"end\":60280,\"start\":60274},{\"end\":60291,\"start\":60286},{\"end\":60302,\"start\":60298},{\"end\":60320,\"start\":60311},{\"end\":60333,\"start\":60326},{\"end\":60347,\"start\":60339},{\"end\":60373,\"start\":60355},{\"end\":60384,\"start\":60382},{\"end\":60400,\"start\":60393},{\"end\":60412,\"start\":60409},{\"end\":60425,\"start\":60423},{\"end\":60439,\"start\":60435},{\"end\":60455,\"start\":60449},{\"end\":60470,\"start\":60465},{\"end\":60488,\"start\":60482},{\"end\":60494,\"start\":60490},{\"end\":61182,\"start\":61177},{\"end\":61194,\"start\":61192},{\"end\":61213,\"start\":61209},{\"end\":61231,\"start\":61223},{\"end\":61507,\"start\":61504},{\"end\":61523,\"start\":61520},{\"end\":61533,\"start\":61530},{\"end\":61748,\"start\":61746},{\"end\":61763,\"start\":61760},{\"end\":61776,\"start\":61772},{\"end\":61790,\"start\":61786},{\"end\":62140,\"start\":62135},{\"end\":62157,\"start\":62149},{\"end\":62171,\"start\":62167},{\"end\":62181,\"start\":62177},{\"end\":62426,\"start\":62421},{\"end\":62435,\"start\":62432},{\"end\":62447,\"start\":62444},{\"end\":62461,\"start\":62457},{\"end\":62472,\"start\":62468},{\"end\":62483,\"start\":62478},{\"end\":62820,\"start\":62815},{\"end\":62828,\"start\":62825},{\"end\":62841,\"start\":62838},{\"end\":62852,\"start\":62847},{\"end\":62865,\"start\":62862},{\"end\":62874,\"start\":62871},{\"end\":63155,\"start\":63151},{\"end\":63167,\"start\":63163},{\"end\":63184,\"start\":63176},{\"end\":63192,\"start\":63188},{\"end\":63369,\"start\":63365},{\"end\":63386,\"start\":63381},{\"end\":63396,\"start\":63392},{\"end\":63740,\"start\":63736},{\"end\":63750,\"start\":63746},{\"end\":63762,\"start\":63758},{\"end\":63776,\"start\":63772},{\"end\":63789,\"start\":63784}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2006.10022\",\"id\":\"b0\"},\"end\":36269,\"start\":35895},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":52012669},\"end\":36823,\"start\":36271},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":60498119},\"end\":36969,\"start\":36825},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":215548441},\"end\":37548,\"start\":36971},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":189762527},\"end\":37974,\"start\":37550},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":529375},\"end\":38353,\"start\":37976},{\"attributes\":{\"doi\":\"arXiv:1906.08101\",\"id\":\"b6\"},\"end\":38705,\"start\":38355},{\"attributes\":{\"doi\":\"10.18653/v1/N19-1423\",\"id\":\"b7\",\"matched_paper_id\":52967399},\"end\":39225,\"start\":38707},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":202558451},\"end\":39598,\"start\":39227},{\"attributes\":{\"doi\":\"abs/2101.00154\",\"id\":\"b9\",\"matched_paper_id\":230437545},\"end\":39934,\"start\":39600},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":215768768},\"end\":40555,\"start\":39936},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":222134165},\"end\":41092,\"start\":40557},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":215548225},\"end\":41347,\"start\":41094},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":434646},\"end\":41790,\"start\":41349},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":210701352},\"end\":42096,\"start\":41792},{\"attributes\":{\"id\":\"b15\"},\"end\":42500,\"start\":42098},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b16\",\"matched_paper_id\":225626501},\"end\":43096,\"start\":42502},{\"attributes\":{\"doi\":\"abs/2010.05953\",\"id\":\"b17\",\"matched_paper_id\":222310337},\"end\":43512,\"start\":43098},{\"attributes\":{\"doi\":\"10.18653/v1/2020.emnlp-main.54\",\"id\":\"b18\",\"matched_paper_id\":221879025},\"end\":43985,\"start\":43514},{\"attributes\":{\"doi\":\"10.1162/tacl_a_00324\",\"id\":\"b19\",\"matched_paper_id\":208513249},\"end\":44350,\"start\":43987},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":198229624},\"end\":44692,\"start\":44352},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":215827509},\"end\":45143,\"start\":44694},{\"attributes\":{\"doi\":\"10.18653/v1/2020.acl-main.423\",\"id\":\"b22\",\"matched_paper_id\":199668663},\"end\":45660,\"start\":45145},{\"attributes\":{\"doi\":\"10.18653/v1/2020.acl-main.703\",\"id\":\"b23\",\"matched_paper_id\":204960716},\"end\":46342,\"start\":45662},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":159040000},\"end\":46608,\"start\":46344},{\"attributes\":{\"doi\":\"10.18653/v1/D19-1282\",\"id\":\"b25\",\"matched_paper_id\":202540096},\"end\":47101,\"start\":46610},{\"attributes\":{\"doi\":\"10.18653/v1/2021.acl-long.555\",\"id\":\"b26\",\"matched_paper_id\":229924289},\"end\":47538,\"start\":47103},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":202583325},\"end\":47894,\"start\":47540},{\"attributes\":{\"doi\":\"arXiv:1907.11692\",\"id\":\"b28\"},\"end\":48350,\"start\":47896},{\"attributes\":{\"doi\":\"10.18653/v1/2020.coling-main.27\",\"id\":\"b29\",\"matched_paper_id\":227231627},\"end\":48705,\"start\":48352},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":2967131},\"end\":48993,\"start\":48707},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":1726501},\"end\":49541,\"start\":48995},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":221739295},\"end\":49921,\"start\":49543},{\"attributes\":{\"doi\":\"10.18653/v1/P19-1442\",\"id\":\"b33\",\"matched_paper_id\":196181734},\"end\":50278,\"start\":49923},{\"attributes\":{\"doi\":\"10.18653/v1/P18-1212\",\"id\":\"b34\",\"matched_paper_id\":51878335},\"end\":50601,\"start\":50280},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":202537879},\"end\":51040,\"start\":50603},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":5066019},\"end\":51353,\"start\":51042},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":3051291},\"end\":51621,\"start\":51355},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":202542757},\"end\":52038,\"start\":51623},{\"attributes\":{\"doi\":\"arXiv:2005.04611\",\"id\":\"b39\"},\"end\":52462,\"start\":52040},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":202539551},\"end\":52888,\"start\":52464},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":204838007},\"end\":53407,\"start\":52890},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":233219920},\"end\":53892,\"start\":53409},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":53170360},\"end\":54399,\"start\":53894},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":51878517},\"end\":54738,\"start\":54401},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":216642086},\"end\":55231,\"start\":54740},{\"attributes\":{\"doi\":\"10.18653/v1/2021.emnlp-main.230\",\"id\":\"b46\",\"matched_paper_id\":233231592},\"end\":55819,\"start\":55233},{\"attributes\":{\"id\":\"b47\"},\"end\":56067,\"start\":55821},{\"attributes\":{\"doi\":\"arXiv:1904.09223\",\"id\":\"b48\"},\"end\":56487,\"start\":56069},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":218595658},\"end\":56925,\"start\":56489},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":13756489},\"end\":57356,\"start\":56927},{\"attributes\":{\"doi\":\"abs/2007.00849\",\"id\":\"b51\"},\"end\":57707,\"start\":57358},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":143424870},\"end\":58246,\"start\":57709},{\"attributes\":{\"doi\":\"10.18653/v1/2020.findings-emnlp.369\",\"id\":\"b53\",\"matched_paper_id\":218487288},\"end\":58787,\"start\":58248},{\"attributes\":{\"doi\":\"arXiv:2112.06318\",\"id\":\"b54\"},\"end\":59172,\"start\":58789},{\"attributes\":{\"doi\":\"arXiv:2002.01808\",\"id\":\"b55\"},\"end\":59602,\"start\":59174},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":208006241},\"end\":60114,\"start\":59604},{\"attributes\":{\"doi\":\"10.18653/v1/2020.emnlp-demos.6\",\"id\":\"b57\",\"matched_paper_id\":208117506},\"end\":61088,\"start\":60116},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":209439872},\"end\":61496,\"start\":61090},{\"attributes\":{\"doi\":\"arXiv:1909.03193\",\"id\":\"b59\"},\"end\":61736,\"start\":61498},{\"attributes\":{\"doi\":\"arXiv:2010.00796\",\"id\":\"b60\"},\"end\":62064,\"start\":61738},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":218470275},\"end\":62362,\"start\":62066},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":141406559},\"end\":62737,\"start\":62364},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":158046772},\"end\":63085,\"start\":62739},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":218581125},\"end\":63357,\"start\":63087},{\"attributes\":{\"doi\":\"arXiv:1910.14296\",\"id\":\"b65\"},\"end\":63607,\"start\":63359},{\"attributes\":{\"doi\":\"arXiv:2203.02225\",\"id\":\"b66\"},\"end\":64045,\"start\":63609}]", "bib_title": "[{\"end\":36324,\"start\":36271},{\"end\":36846,\"start\":36825},{\"end\":37067,\"start\":36971},{\"end\":37624,\"start\":37550},{\"end\":38023,\"start\":37976},{\"end\":38787,\"start\":38707},{\"end\":39301,\"start\":39227},{\"end\":39678,\"start\":39600},{\"end\":40001,\"start\":39936},{\"end\":40619,\"start\":40557},{\"end\":41151,\"start\":41094},{\"end\":41449,\"start\":41349},{\"end\":41863,\"start\":41792},{\"end\":42549,\"start\":42502},{\"end\":43168,\"start\":43098},{\"end\":43589,\"start\":43514},{\"end\":44029,\"start\":43987},{\"end\":44421,\"start\":44352},{\"end\":44773,\"start\":44694},{\"end\":45184,\"start\":45145},{\"end\":45774,\"start\":45662},{\"end\":46388,\"start\":46344},{\"end\":46674,\"start\":46610},{\"end\":47163,\"start\":47103},{\"end\":47636,\"start\":47540},{\"end\":48408,\"start\":48352},{\"end\":48752,\"start\":48707},{\"end\":49072,\"start\":48995},{\"end\":49601,\"start\":49543},{\"end\":49999,\"start\":49923},{\"end\":50329,\"start\":50280},{\"end\":50663,\"start\":50603},{\"end\":51100,\"start\":51042},{\"end\":51406,\"start\":51355},{\"end\":51673,\"start\":51623},{\"end\":52499,\"start\":52464},{\"end\":52970,\"start\":52890},{\"end\":53490,\"start\":53409},{\"end\":53954,\"start\":53894},{\"end\":54457,\"start\":54401},{\"end\":54820,\"start\":54740},{\"end\":55335,\"start\":55233},{\"end\":56559,\"start\":56489},{\"end\":56952,\"start\":56927},{\"end\":57791,\"start\":57709},{\"end\":58334,\"start\":58248},{\"end\":59691,\"start\":59604},{\"end\":60174,\"start\":60116},{\"end\":61168,\"start\":61090},{\"end\":62124,\"start\":62066},{\"end\":62410,\"start\":62364},{\"end\":62804,\"start\":62739},{\"end\":63145,\"start\":63087}]", "bib_author": "[{\"end\":35967,\"start\":35948},{\"end\":35981,\"start\":35967},{\"end\":36000,\"start\":35981},{\"end\":36018,\"start\":36000},{\"end\":36031,\"start\":36018},{\"end\":36045,\"start\":36031},{\"end\":36337,\"start\":36326},{\"end\":36354,\"start\":36337},{\"end\":36860,\"start\":36848},{\"end\":37089,\"start\":37069},{\"end\":37106,\"start\":37089},{\"end\":37118,\"start\":37106},{\"end\":37137,\"start\":37118},{\"end\":37152,\"start\":37137},{\"end\":37160,\"start\":37152},{\"end\":37178,\"start\":37160},{\"end\":37196,\"start\":37178},{\"end\":37214,\"start\":37196},{\"end\":37644,\"start\":37626},{\"end\":37660,\"start\":37644},{\"end\":37673,\"start\":37660},{\"end\":37693,\"start\":37673},{\"end\":37711,\"start\":37693},{\"end\":37723,\"start\":37711},{\"end\":38045,\"start\":38025},{\"end\":38059,\"start\":38045},{\"end\":38367,\"start\":38355},{\"end\":38381,\"start\":38367},{\"end\":38391,\"start\":38381},{\"end\":38401,\"start\":38391},{\"end\":38414,\"start\":38401},{\"end\":38427,\"start\":38414},{\"end\":38439,\"start\":38427},{\"end\":38803,\"start\":38789},{\"end\":38819,\"start\":38803},{\"end\":38831,\"start\":38819},{\"end\":38851,\"start\":38831},{\"end\":39314,\"start\":39303},{\"end\":39324,\"start\":39314},{\"end\":39334,\"start\":39324},{\"end\":39348,\"start\":39334},{\"end\":39361,\"start\":39348},{\"end\":39695,\"start\":39680},{\"end\":39711,\"start\":39695},{\"end\":39723,\"start\":39711},{\"end\":39731,\"start\":39723},{\"end\":39739,\"start\":39731},{\"end\":40019,\"start\":40003},{\"end\":40034,\"start\":40019},{\"end\":40051,\"start\":40034},{\"end\":40070,\"start\":40051},{\"end\":40080,\"start\":40070},{\"end\":40093,\"start\":40080},{\"end\":40637,\"start\":40621},{\"end\":40658,\"start\":40637},{\"end\":40673,\"start\":40658},{\"end\":40683,\"start\":40673},{\"end\":40697,\"start\":40683},{\"end\":40711,\"start\":40697},{\"end\":40717,\"start\":40711},{\"end\":41163,\"start\":41153},{\"end\":41176,\"start\":41163},{\"end\":41193,\"start\":41176},{\"end\":41466,\"start\":41451},{\"end\":41485,\"start\":41466},{\"end\":41503,\"start\":41485},{\"end\":41876,\"start\":41865},{\"end\":41887,\"start\":41876},{\"end\":41900,\"start\":41887},{\"end\":41913,\"start\":41900},{\"end\":41927,\"start\":41913},{\"end\":42123,\"start\":42100},{\"end\":42141,\"start\":42123},{\"end\":42159,\"start\":42141},{\"end\":42166,\"start\":42159},{\"end\":42180,\"start\":42166},{\"end\":42195,\"start\":42180},{\"end\":42563,\"start\":42551},{\"end\":42575,\"start\":42563},{\"end\":42586,\"start\":42575},{\"end\":42604,\"start\":42586},{\"end\":42620,\"start\":42604},{\"end\":43184,\"start\":43170},{\"end\":43205,\"start\":43184},{\"end\":43225,\"start\":43205},{\"end\":43237,\"start\":43225},{\"end\":43256,\"start\":43237},{\"end\":43272,\"start\":43256},{\"end\":43278,\"start\":43272},{\"end\":43602,\"start\":43591},{\"end\":43610,\"start\":43602},{\"end\":43625,\"start\":43610},{\"end\":43635,\"start\":43625},{\"end\":43648,\"start\":43635},{\"end\":43662,\"start\":43648},{\"end\":44047,\"start\":44031},{\"end\":44059,\"start\":44047},{\"end\":44070,\"start\":44059},{\"end\":44085,\"start\":44070},{\"end\":44437,\"start\":44423},{\"end\":44449,\"start\":44437},{\"end\":44461,\"start\":44449},{\"end\":44471,\"start\":44461},{\"end\":44482,\"start\":44471},{\"end\":44500,\"start\":44482},{\"end\":44506,\"start\":44500},{\"end\":44790,\"start\":44775},{\"end\":44802,\"start\":44790},{\"end\":44817,\"start\":44802},{\"end\":44829,\"start\":44817},{\"end\":44845,\"start\":44829},{\"end\":44853,\"start\":44845},{\"end\":45199,\"start\":45186},{\"end\":45211,\"start\":45199},{\"end\":45221,\"start\":45211},{\"end\":45230,\"start\":45221},{\"end\":45242,\"start\":45230},{\"end\":45253,\"start\":45242},{\"end\":45274,\"start\":45253},{\"end\":45289,\"start\":45274},{\"end\":45302,\"start\":45289},{\"end\":45788,\"start\":45776},{\"end\":45800,\"start\":45788},{\"end\":45835,\"start\":45800},{\"end\":45846,\"start\":45835},{\"end\":45864,\"start\":45846},{\"end\":45882,\"start\":45864},{\"end\":46404,\"start\":46390},{\"end\":46415,\"start\":46404},{\"end\":46425,\"start\":46415},{\"end\":46700,\"start\":46676},{\"end\":46712,\"start\":46700},{\"end\":46724,\"start\":46712},{\"end\":46729,\"start\":46724},{\"end\":47190,\"start\":47165},{\"end\":47205,\"start\":47190},{\"end\":47214,\"start\":47205},{\"end\":47650,\"start\":47638},{\"end\":47661,\"start\":47650},{\"end\":47671,\"start\":47661},{\"end\":47684,\"start\":47671},{\"end\":47691,\"start\":47684},{\"end\":47908,\"start\":47896},{\"end\":47918,\"start\":47908},{\"end\":47931,\"start\":47918},{\"end\":47943,\"start\":47931},{\"end\":47957,\"start\":47943},{\"end\":47969,\"start\":47957},{\"end\":47980,\"start\":47969},{\"end\":47992,\"start\":47980},{\"end\":48010,\"start\":47992},{\"end\":48028,\"start\":48010},{\"end\":48423,\"start\":48410},{\"end\":48435,\"start\":48423},{\"end\":48447,\"start\":48435},{\"end\":48766,\"start\":48754},{\"end\":48782,\"start\":48766},{\"end\":48800,\"start\":48782},{\"end\":48821,\"start\":48800},{\"end\":49095,\"start\":49074},{\"end\":49115,\"start\":49095},{\"end\":49128,\"start\":49115},{\"end\":49141,\"start\":49128},{\"end\":49154,\"start\":49141},{\"end\":49172,\"start\":49154},{\"end\":49188,\"start\":49172},{\"end\":49201,\"start\":49188},{\"end\":49624,\"start\":49603},{\"end\":49642,\"start\":49624},{\"end\":49653,\"start\":49642},{\"end\":49669,\"start\":49653},{\"end\":49687,\"start\":49669},{\"end\":49697,\"start\":49687},{\"end\":49719,\"start\":49697},{\"end\":50012,\"start\":50001},{\"end\":50026,\"start\":50012},{\"end\":50040,\"start\":50026},{\"end\":50343,\"start\":50331},{\"end\":50355,\"start\":50343},{\"end\":50363,\"start\":50355},{\"end\":50373,\"start\":50363},{\"end\":50677,\"start\":50665},{\"end\":50697,\"start\":50677},{\"end\":50707,\"start\":50697},{\"end\":51114,\"start\":51102},{\"end\":51122,\"start\":51114},{\"end\":51132,\"start\":51122},{\"end\":51423,\"start\":51408},{\"end\":51437,\"start\":51423},{\"end\":51452,\"start\":51437},{\"end\":51686,\"start\":51675},{\"end\":51699,\"start\":51686},{\"end\":51715,\"start\":51699},{\"end\":51726,\"start\":51715},{\"end\":51742,\"start\":51726},{\"end\":51756,\"start\":51742},{\"end\":51770,\"start\":51756},{\"end\":51777,\"start\":51770},{\"end\":52113,\"start\":52098},{\"end\":52128,\"start\":52113},{\"end\":52147,\"start\":52128},{\"end\":52164,\"start\":52147},{\"end\":52176,\"start\":52164},{\"end\":52196,\"start\":52176},{\"end\":52214,\"start\":52196},{\"end\":52516,\"start\":52501},{\"end\":52533,\"start\":52516},{\"end\":52551,\"start\":52533},{\"end\":52564,\"start\":52551},{\"end\":52577,\"start\":52564},{\"end\":52594,\"start\":52577},{\"end\":52610,\"start\":52594},{\"end\":52618,\"start\":52610},{\"end\":52986,\"start\":52972},{\"end\":53000,\"start\":52986},{\"end\":53014,\"start\":53000},{\"end\":53029,\"start\":53014},{\"end\":53044,\"start\":53029},{\"end\":53060,\"start\":53044},{\"end\":53072,\"start\":53060},{\"end\":53080,\"start\":53072},{\"end\":53093,\"start\":53080},{\"end\":53505,\"start\":53492},{\"end\":53519,\"start\":53505},{\"end\":53969,\"start\":53956},{\"end\":53990,\"start\":53969},{\"end\":54007,\"start\":53990},{\"end\":54029,\"start\":54007},{\"end\":54044,\"start\":54029},{\"end\":54061,\"start\":54044},{\"end\":54067,\"start\":54061},{\"end\":54075,\"start\":54067},{\"end\":54088,\"start\":54075},{\"end\":54094,\"start\":54088},{\"end\":54473,\"start\":54459},{\"end\":54486,\"start\":54473},{\"end\":54832,\"start\":54822},{\"end\":54840,\"start\":54832},{\"end\":54854,\"start\":54840},{\"end\":54868,\"start\":54854},{\"end\":54884,\"start\":54868},{\"end\":54897,\"start\":54884},{\"end\":55352,\"start\":55337},{\"end\":55363,\"start\":55352},{\"end\":55379,\"start\":55363},{\"end\":55394,\"start\":55379},{\"end\":55410,\"start\":55394},{\"end\":55423,\"start\":55410},{\"end\":55899,\"start\":55886},{\"end\":55912,\"start\":55899},{\"end\":55930,\"start\":55912},{\"end\":56077,\"start\":56069},{\"end\":56092,\"start\":56077},{\"end\":56102,\"start\":56092},{\"end\":56115,\"start\":56102},{\"end\":56126,\"start\":56115},{\"end\":56137,\"start\":56126},{\"end\":56147,\"start\":56137},{\"end\":56161,\"start\":56147},{\"end\":56175,\"start\":56161},{\"end\":56179,\"start\":56175},{\"end\":56575,\"start\":56561},{\"end\":56587,\"start\":56575},{\"end\":56597,\"start\":56587},{\"end\":56608,\"start\":56597},{\"end\":56616,\"start\":56608},{\"end\":56628,\"start\":56616},{\"end\":56639,\"start\":56628},{\"end\":56643,\"start\":56639},{\"end\":56970,\"start\":56954},{\"end\":56984,\"start\":56970},{\"end\":56997,\"start\":56984},{\"end\":57014,\"start\":56997},{\"end\":57027,\"start\":57014},{\"end\":57042,\"start\":57027},{\"end\":57057,\"start\":57042},{\"end\":57075,\"start\":57057},{\"end\":57454,\"start\":57443},{\"end\":57467,\"start\":57454},{\"end\":57482,\"start\":57467},{\"end\":57500,\"start\":57482},{\"end\":57507,\"start\":57500},{\"end\":57804,\"start\":57793},{\"end\":57824,\"start\":57804},{\"end\":57839,\"start\":57824},{\"end\":57856,\"start\":57839},{\"end\":57872,\"start\":57856},{\"end\":57884,\"start\":57872},{\"end\":57895,\"start\":57884},{\"end\":57910,\"start\":57895},{\"end\":58350,\"start\":58336},{\"end\":58363,\"start\":58350},{\"end\":58379,\"start\":58363},{\"end\":58394,\"start\":58379},{\"end\":58405,\"start\":58394},{\"end\":58874,\"start\":58860},{\"end\":58891,\"start\":58874},{\"end\":58904,\"start\":58891},{\"end\":58920,\"start\":58904},{\"end\":58932,\"start\":58920},{\"end\":58943,\"start\":58932},{\"end\":59188,\"start\":59176},{\"end\":59199,\"start\":59188},{\"end\":59209,\"start\":59199},{\"end\":59222,\"start\":59209},{\"end\":59238,\"start\":59222},{\"end\":59251,\"start\":59238},{\"end\":59264,\"start\":59251},{\"end\":59275,\"start\":59264},{\"end\":59707,\"start\":59693},{\"end\":59719,\"start\":59707},{\"end\":59734,\"start\":59719},{\"end\":59750,\"start\":59734},{\"end\":59763,\"start\":59750},{\"end\":59774,\"start\":59763},{\"end\":59785,\"start\":59774},{\"end\":60189,\"start\":60176},{\"end\":60205,\"start\":60189},{\"end\":60218,\"start\":60205},{\"end\":60235,\"start\":60218},{\"end\":60253,\"start\":60235},{\"end\":60266,\"start\":60253},{\"end\":60282,\"start\":60266},{\"end\":60293,\"start\":60282},{\"end\":60304,\"start\":60293},{\"end\":60322,\"start\":60304},{\"end\":60335,\"start\":60322},{\"end\":60349,\"start\":60335},{\"end\":60375,\"start\":60349},{\"end\":60386,\"start\":60375},{\"end\":60402,\"start\":60386},{\"end\":60414,\"start\":60402},{\"end\":60427,\"start\":60414},{\"end\":60441,\"start\":60427},{\"end\":60457,\"start\":60441},{\"end\":60472,\"start\":60457},{\"end\":60490,\"start\":60472},{\"end\":60496,\"start\":60490},{\"end\":61184,\"start\":61170},{\"end\":61196,\"start\":61184},{\"end\":61215,\"start\":61196},{\"end\":61233,\"start\":61215},{\"end\":61509,\"start\":61498},{\"end\":61525,\"start\":61509},{\"end\":61535,\"start\":61525},{\"end\":61750,\"start\":61738},{\"end\":61765,\"start\":61750},{\"end\":61778,\"start\":61765},{\"end\":61792,\"start\":61778},{\"end\":62142,\"start\":62126},{\"end\":62159,\"start\":62142},{\"end\":62173,\"start\":62159},{\"end\":62183,\"start\":62173},{\"end\":62428,\"start\":62412},{\"end\":62437,\"start\":62428},{\"end\":62449,\"start\":62437},{\"end\":62463,\"start\":62449},{\"end\":62474,\"start\":62463},{\"end\":62485,\"start\":62474},{\"end\":62822,\"start\":62806},{\"end\":62830,\"start\":62822},{\"end\":62843,\"start\":62830},{\"end\":62854,\"start\":62843},{\"end\":62867,\"start\":62854},{\"end\":62876,\"start\":62867},{\"end\":63157,\"start\":63147},{\"end\":63169,\"start\":63157},{\"end\":63186,\"start\":63169},{\"end\":63194,\"start\":63186},{\"end\":63371,\"start\":63359},{\"end\":63388,\"start\":63371},{\"end\":63398,\"start\":63388},{\"end\":63742,\"start\":63728},{\"end\":63752,\"start\":63742},{\"end\":63764,\"start\":63752},{\"end\":63778,\"start\":63764},{\"end\":63791,\"start\":63778}]", "bib_venue": "[{\"end\":35946,\"start\":35895},{\"end\":36431,\"start\":36354},{\"end\":36886,\"start\":36860},{\"end\":37243,\"start\":37214},{\"end\":37741,\"start\":37723},{\"end\":38085,\"start\":38059},{\"end\":38508,\"start\":38455},{\"end\":38891,\"start\":38871},{\"end\":39388,\"start\":39361},{\"end\":39758,\"start\":39753},{\"end\":40187,\"start\":40093},{\"end\":40778,\"start\":40717},{\"end\":41211,\"start\":41193},{\"end\":41530,\"start\":41503},{\"end\":41931,\"start\":41927},{\"end\":42692,\"start\":42624},{\"end\":43297,\"start\":43292},{\"end\":43716,\"start\":43692},{\"end\":44166,\"start\":44105},{\"end\":44510,\"start\":44506},{\"end\":44874,\"start\":44853},{\"end\":45349,\"start\":45331},{\"end\":45933,\"start\":45911},{\"end\":46445,\"start\":46425},{\"end\":46780,\"start\":46749},{\"end\":47265,\"start\":47243},{\"end\":47695,\"start\":47691},{\"end\":48099,\"start\":48044},{\"end\":48499,\"start\":48478},{\"end\":48835,\"start\":48821},{\"end\":49225,\"start\":49201},{\"end\":49724,\"start\":49719},{\"end\":50082,\"start\":50060},{\"end\":50411,\"start\":50393},{\"end\":50738,\"start\":50707},{\"end\":51154,\"start\":51132},{\"end\":51470,\"start\":51452},{\"end\":51808,\"start\":51777},{\"end\":52096,\"start\":52040},{\"end\":52650,\"start\":52618},{\"end\":53129,\"start\":53093},{\"end\":53543,\"start\":53519},{\"end\":54117,\"start\":54094},{\"end\":54509,\"start\":54486},{\"end\":54946,\"start\":54897},{\"end\":55478,\"start\":55454},{\"end\":55884,\"start\":55821},{\"end\":56255,\"start\":56195},{\"end\":56661,\"start\":56643},{\"end\":57124,\"start\":57075},{\"end\":57441,\"start\":57358},{\"end\":57959,\"start\":57910},{\"end\":58509,\"start\":58440},{\"end\":58858,\"start\":58789},{\"end\":59846,\"start\":59785},{\"end\":60546,\"start\":60526},{\"end\":61285,\"start\":61233},{\"end\":61594,\"start\":61551},{\"end\":61879,\"start\":61808},{\"end\":62203,\"start\":62183},{\"end\":62523,\"start\":62485},{\"end\":62894,\"start\":62876},{\"end\":63212,\"start\":63194},{\"end\":63461,\"start\":63414},{\"end\":63726,\"start\":63609},{\"end\":36520,\"start\":36433},{\"end\":37259,\"start\":37245},{\"end\":37746,\"start\":37743},{\"end\":38112,\"start\":38087},{\"end\":38920,\"start\":38893},{\"end\":39402,\"start\":39390},{\"end\":40268,\"start\":40189},{\"end\":40826,\"start\":40780},{\"end\":41216,\"start\":41213},{\"end\":41560,\"start\":41532},{\"end\":42747,\"start\":42694},{\"end\":43727,\"start\":43718},{\"end\":44898,\"start\":44876},{\"end\":45354,\"start\":45351},{\"end\":45942,\"start\":45935},{\"end\":46452,\"start\":46447},{\"end\":46814,\"start\":46782},{\"end\":47274,\"start\":47267},{\"end\":48523,\"start\":48501},{\"end\":48841,\"start\":48837},{\"end\":49257,\"start\":49227},{\"end\":50106,\"start\":50084},{\"end\":50436,\"start\":50413},{\"end\":50772,\"start\":50740},{\"end\":51183,\"start\":51156},{\"end\":51475,\"start\":51472},{\"end\":51826,\"start\":51810},{\"end\":52669,\"start\":52652},{\"end\":53554,\"start\":53545},{\"end\":54127,\"start\":54119},{\"end\":54519,\"start\":54511},{\"end\":55507,\"start\":55480},{\"end\":56666,\"start\":56663},{\"end\":58517,\"start\":58511},{\"end\":60553,\"start\":60548},{\"end\":62210,\"start\":62205},{\"end\":62548,\"start\":62525},{\"end\":62899,\"start\":62896},{\"end\":63217,\"start\":63214}]"}}}, "year": 2023, "month": 12, "day": 17}