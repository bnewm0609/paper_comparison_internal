{"id": 253420328, "updated": "2023-11-26 15:51:17.309", "metadata": {"title": "SpeechMatrix: A Large-Scale Mined Corpus of Multilingual Speech-to-Speech Translations", "authors": "[{\"first\":\"Paul-Ambroise\",\"last\":\"Duquenne\",\"middle\":[]},{\"first\":\"Hongyu\",\"last\":\"Gong\",\"middle\":[]},{\"first\":\"Ning\",\"last\":\"Dong\",\"middle\":[]},{\"first\":\"Jingfei\",\"last\":\"Du\",\"middle\":[]},{\"first\":\"Ann\",\"last\":\"Lee\",\"middle\":[]},{\"first\":\"Vedanuj\",\"last\":\"Goswami\",\"middle\":[]},{\"first\":\"Changhan\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Juan\",\"last\":\"Pino\",\"middle\":[]},{\"first\":\"Beno\u00eet\",\"last\":\"Sagot\",\"middle\":[]},{\"first\":\"Holger\",\"last\":\"Schwenk\",\"middle\":[]}]", "venue": "ACL", "journal": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "We present SpeechMatrix, a large-scale multilingual corpus of speech-to-speech translations mined from real speech of European Parliament recordings. It contains speech alignments in 136 language pairs with a total of 418 thousand hours of speech. To evaluate the quality of this parallel speech, we train bilingual speech-to-speech translation models on mined data only and establish extensive baseline results on EuroParl-ST, VoxPopuli and FLEURS test sets. Enabled by the multilinguality of SpeechMatrix, we also explore multilingual speech-to-speech translation, a topic which was addressed by few other works. We also demonstrate that model pre-training and sparse scaling using Mixture-of-Experts bring large gains to translation performance. The mined data and models will be publicly released", "fields_of_study": "[\"Computer Science\",\"Engineering\"]", "external_ids": {"arxiv": null, "mag": null, "acl": "2023.acl-long.899", "pubmed": null, "pubmedcentral": null, "dblp": "conf/acl/DuquenneGDD0GW023", "doi": "10.18653/v1/2023.acl-long.899"}}, "content": {"source": {"pdf_hash": "5cfeda087fb0294a90ac58259de0c63683ef3a40", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclanthology.org/2023.acl-long.899.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "f928d602a3894d423981a6588bb91373901c5256", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/5cfeda087fb0294a90ac58259de0c63683ef3a40.txt", "contents": "\nSpeechMatrix: A Large-Scale Mined Corpus of Multilingual Speech-to-Speech Translations\nLong PapersCopyright Long PapersJuly 9-14, 2023\n\nPaul-Ambroise Duquenne \nMeta AI Research\n2 Inria\n\nHongyu Gong hygong@meta.com \nMeta AI Research\n2 Inria\n\nNing Dong \nMeta AI Research\n2 Inria\n\nJingfei Du jingfeidu@meta.com \nMeta AI Research\n2 Inria\n\nAnn Lee \nMeta AI Research\n2 Inria\n\nVedanuj Goswami \nMeta AI Research\n2 Inria\n\nChanghan Wang \nMeta AI Research\n2 Inria\n\nJuan Pino \nMeta AI Research\n2 Inria\n\nBeno\u00eet Sagot \nMeta AI Research\n2 Inria\n\nHolger Schwenk schwenk@meta.com \nMeta AI Research\n2 Inria\n\nSpeechMatrix: A Large-Scale Mined Corpus of Multilingual Speech-to-Speech Translations\n\nProceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nthe 61st Annual Meeting of the Association for Computational LinguisticsLong Papers1July 9-14, 2023\nWe present SpeechMatrix, a large-scale multilingual corpus of speech-to-speech translations (S2ST) mined from real speech of European Parliament recordings. It contains speech alignments in 136 language pairs with a total of 418 thousand hours of speech. To evaluate the quality of this parallel speech, we train bilingual speech-to-speech translation models on mined data only and establish extensive baseline results on Europarl-ST, VoxPopuli and FLEURS test sets. Enabled by the multilinguality of SpeechMatrix, we also explore multilingual speech-to-speech translation, a topic which was addressed by few other works. We also demonstrate that model pre-training and sparse scaling using Mixture-of-Experts bring large gains to translation performance. We are open-sourcing the mined data, speech encoders used for mining, multilingual HuBERT models in four language families for target unit generation, language-specific vocoders for speech synthesis from discrete units, and S2S models trained and presented in this work. 1 * Equal contributions\n\nIntroduction\n\nResearch has progressed in the area of speech-tospeech translation with the goal of seamless communication among people who speak different languages. Direct S2ST models attract increasing research interest, e.g. (Jia et al., 2019). Compared to conventional cascaded models, direct models do not rely on intermediate text representations which make them applicable to the translation of languages without a well-defined writing script. Moreover, direct S2ST have the advantage of higher training and inference efficiency (Lee et al., 2022a).\n\nDespite the benefits of direct approaches, their training is faced with the major issue of data scarcity in parallel speech. Human labeled speech data is expensive to create, there are very few data resources providing speech alignments, and the data amount is quite limited. To mitigate the data scarcity, some works have leveraged multitask learning (Jia et al., 2019;Lee et al., 2022a), data augmentation with speech variations (Jia et al., 2019), or with synthesized speech (Jia et al., 2022a;Nguyen et al., 2022). It is also shown useful to leverage knowledge transferred from pre-trained models (Lee et al., 2022b; such as HuBERT (Hsu et al., 2021), wav2vec 2.0 (Baevski et al., 2020) and mBART (Liu et al., 2020). Recently, Duquenne et al. (2021) is the first work to make speech mining efforts by learning a shared multilingual speech and text embedding space. Speech content is encoded by speech encoders into fixed-size representations which are then used for aligning speech and text across different languages. It demonstrates good empirical gains to train direct speech-to-text and speech-tospeech translation systems with the mined data (Duquenne et al., 2021;Lee et al., 2022b).\n\nIn this work, we trained speech encoders for 17 languages 2 and mined speech-to-speech alignments for all possible language pairs from VoxPopuli (Wang et al., 2021a), a collection of European Parliament recordings. To the best of our knowledge, SpeechMatrix is by far the largest freely available speech-to-speech translation corpus, with 136 language directions and an average of 1,537 hours of source speech in each direction for a total of 418 thousand hours. We demonstrate that strong S2ST models can be trained with these mined data and validate the good quality of the speech alignments across languages. We are open-sourcing the mined data and the speech encoders used for mining, which could pave the way for future research on S2ST. Moreover, for reproducibility, we will release model components including multilingual HuBERT models in four language families for target unit generation, language-specific vocoders for speech synthesis from discrete units, and S2S models trained and presented in this work.\n\n\nRelated Works\n\nFrom bitext mining to speech mining. Bitext mining is to find parallel sentences from monolingual resources, which provides a large amount of training data for machine translation models. Early works on bitext mining used document metainformation (Resnik, 1999), cross-lingual document retrieval (Munteanu and Marcu, 2005) or information retrieval (Abdul-Rauf and Schwenk, 2009;Bouamor and Sajjad, 2018). More recent works use multilingual sentence embeddings (Artetxe and Schwenk, 2018;Yang et al., 2019;Schwenk et al., 2021a). The embedding based approach can be extended to new languages (Reimers and Gurevych, 2020;Heffernan et al., 2022) or the speech modality (Duquenne et al., 2021;Khurana et al., 2022) with knowledge distillation, also called teacher-student approach. These multilingual and multimodal sentence embeddings enabled us to perform large-scale speech-text mining, or speech-speech mining for a small set of languages.\n\nSpeech-to-speech translation (S2ST). S2ST started from cascaded systems consisting of automatic speech recognition (ASR), machine translation (MT) and text-to-speech synthesis (TTS) (Nakamura et al., 2006;Do et al., 2015). The reliance on intermediate text outputs poses limitations on cascaded models to support efficient inference and unwritten languages. Given these challenges, there has been a recent surge of research interest in direct approaches to speech translation without the need of texts. Translatotron (Jia et al., 2019) and Translatotron2 (Jia et al., 2022b) train end-to-end S2ST to generate target spectrograms with multitask learning. Another line of research replaces the target spectrograms in S2ST modeling with discrete units which are learned from a large amount of unlabeled speech (Lee et al., 2022a,b). Discrete units have shown to better capture linguistic content than spectrograms. Despite these progress on direct S2ST, it is faced with the challenge of limited parallel speech.\n\nSpeech translation corpora. The Fisher dataset, a collection of approximately 170 hours of telephone conversations in Spanish (Post et al., 2014), is commonly used as training data for Spanish-English S2ST. However, it does not provide parallel English speech. Previous works generate synthesized English speech from English text translations provided by Fisher. Another S2S dataset containing synthesized speech is CVSS, which covers parallel S2ST translations from 21 languages into English. It is derived from Common Voice (Ardila et al., 2020) and CoVoST 2 (Wang et al., 2021b), and it synthesizes speech from translated texts. The release of VoxPopuli dataset provides the largest S2S translations in real speech so far (Wang et al., 2021a). It covers pairwise speech-to-speech translations among 15 languages, and each direction has less than 500 hours of speech. In another initiative named FLEURS, the text-to-text evaluation data of the FLoRes-101 benchmark (Goyal et al., 2022) was extended to the speech modality. Supporting 102 languages, FLEURS has a larger language coverage than VoxPopuli, but it only contains around 12 hours of speech per language and it is intended to be used asN -way parallel test data.\n\nIn this work, we present SpeechMatrix, a largescale multilingual speech-to-speech corpus mined from VoxPopuli (Wang et al., 2021a). It contains speech alignments in 136 language pairs with an average of 1, 537-hour source speech per direction. The main characteristics of these speech corpora are summarized in Table 1.\n\n\nSpeech-to-Speech Mining\n\nThe mining approach of this work is built upon the idea of encoding multilingual speech utterances into a shared embedding space. Speech encoders project utterances with similar semantic content to fixed-size representations which are close in the embedding space regardless of their languages. The closeness of embeddings reflects the similarity of speech content, and is used as the alignment score in the mining process. In this section, we discuss speech encoders and speech mining.\n\n\nSpeech Encoders\n\nWe followed the teacher-student approach introduced in (Duquenne et al., 2021)   target vectors for speech encoder training. During training, we minimize the cosine loss between fixed-size representations output by speech encoders, and the outputs of LASER text encoder (whose weights are frozen during training). Speech encoders are initialized with the 2B-parameter XLS-R model (Babu et al., 2021), which was pre-trained on nearly half a million hours of publicly available audios in 128 languages. Following (Duquenne et al., 2022), the fixed-size representation for speech is obtained with max pooling of the encoder outputs which appeared to work better compared to other pooling methods. We summarize the architecture of the speech encoder in Figure 1. We used various publicly available ASR data sets which cover our languages to train the speech encoders, including CoVoST 2 (Wang et al., 2020(Wang et al., , 2021b, Common Voice (Ardila et al., 2020), Europarl (Ardila et al., 2020), mTedx (Salesky et al., 2021), Must-C (Di Gangi et al., 2019) and VoxPopuli (Wang et al., 2021a), as well as speech translation data from the foreign languages into English and from English into German. We removed training samples whose transcription or the written translation consisted of multiple sentences, as LASER has been trained on single sentences only. For better training efficiency, we trained speech encoders for each language family instead of each language. The language grouping is provided in Appendix. To better handle imbalanced training data, we sample the training data from different languages with the same approach as (Duquenne et al., 2021). For English (en), Slovenian (sl), Lithuanian (lt) and Dutch (nl), we also trained separate monolingual speech encoders that had lower valid cosine loss compared to multilingual encoders, and these four monolingual encoders were used for mining.\n\n\nEvaluation of speech encoders\n\nSimilarity search is frequently used to evaluate multilingual text encoders (Artetxe and Schwenk, 2018;Feng et al., 2020;Heffernan et al., 2022). We use the following score to measure similarity between the source audio, and the target transcriptions or translations:\nsim(x,y) (1) = cos(x, y) \u2212 z\u2208N N k (x) cos(x, z) 2k + z\u2208N N k (y) cos(y, z) 2k\nwhere x and y are the source and target embeddings, and N N k (x) denotes the k nearest neighbors of x. We used k = 4. We evaluated similarity search of audios against transcriptions on VoxPopuli ASR test set in Table 2, which is our target domain as we plan to mine unlabeled speech from VoxPopuli (see subsection 3.3). We also evaluated similarity search of audio against written translations or transcriptions on CoVoST 2 test set in order to compare with speech encoders in previous work (see detailed analysis in Appendix A). Finally, we report text-to-text similarity search using the LASER text encoder as lower bound for the speech translation similarity search error rate since we use gold transcriptions to search against written translations. We report error rates (in %) that are percentage of audio utterances incorrectly matched with text transcripts from the same test set. We note that error rates are very low for all languages (below 5% and around 1 or 2% for most languages), which is an initial validation of good-quality speech encoders before the large-scale mining.  \n\n\nLarge-scale speech mining\n\nWe used VoxPopuli as our source of unlabeled unsegmented speech for 17 languages in focus. In principle, performing speech-to-speech or speechto-text mining can be done with exactly the same pipeline as text-to-text mining but with different encoders. We follow the global mining approach as described in Schwenk et al. (2021a) and compare all segments in the source language with all segments in the target language. Similarity scores are calculated in both directions using the margin as described in Equation 1 considering k = 16 neighbors. Segments are considered to be parallel if the margin score exceeds a threshold, we use 1.06 if not specified otherwise. The reader is referred to Schwenk et al. (2021a) for a detailed description of the generic mining pipeline. There is however one important difference when processing speech: it is not straightforward to segment the audio signal into parts which have the optimal granularity for mining. The VoxPopuli recordings have a rather long duration, e.g. one hour and a half on average for English. We apply Voice Activity Detection (VAD) using Silero-VAD (Silero-Team, 2021) which supports over 100 languages. The resulting segments do not necessarily correspond to complete sentences. On one hand, there may be silence in the middle of an utterance, e.g. a hesitation. On the other hand, two sentences may follow each other without a long silence separating them. We follow the \"over segmentation\" approach outlined in Duquenne et al. (2021): several possible segments are created and we let the mining algorithm decide which ones match the best. Initial experiments suggest that segments shorter than 1 second or longer than 20 seconds are unlikely to be aligned and therefore were excluded.\n\nAfter mining, the resulting speech alignments may have overlap as we over-segment the unlabeled speech. A post-processing method Duquenne et al.\n\n(2021) is introduced to remove overlaps between mined speech segments on the source speech side. We relax the post-processing of the mined data, allowing for some overlap between mined speech segments: for two audio segments that overlap on the source side, if the overlap represents more than 20% of the first segment and of the second segment, we discard the alignment with the lowest mining score. We did an ablation study on different thresholds of overlap ratio for one low-resource, one mid-resource and one high-resource direction and found that 20% was the best overlap threshold in all settings.\n\nWe report the statistics of the mined speech-tospeech translation pairs in Table 3, with a mining score threshold of 1.06. The mined data totals 418k hours of parallel speech with an average of 1,537 hours of source speech in all translation directions. While some high resource languages like English (en), Spanish (es) or French (fr) can reach up to 5k hours of aligned speech with other spoken languages; lower resource languages such as Estonian (et) and Lithuanian (lt) obtain much fewer alignments, with only a few hours of aligned speech for Lithuanian. We also performed mining of the source speech in sixteen languages against more than twenty billion English sentences from Common Crawl. This yielded speech-text alignments between 827 and 3, 966 hours (c.f. the last column of Table 3). Training and evaluation of speech-totext translation are left for future research.\n\n\nEvaluation Data\n\nBesides the speech-to-speech data mined as the train set, we leverage labeled public speech datasets as the evaluation sets.\n\nTest set. In our experiments, we derive test sets in speech translation from three public corpora, evaluating translation models trained on mined data across different domains.\n\n(1) Europarl-ST (EPST) (Iranzo-S\u00e1nchez et al., 2020). It is a multilingual speech-to-text translation corpus built on recordings of debates from the European Parliament, containing 72 translation directions in 9 languages. 3 (2) VoxPopuli (Wang et al., 2021a). S2S data, as part of VoxPopuli release, provides aligned source and target speech together with source transcriptions. We prepare the speech-to-text data with 3 en, fr, de, it, es, pt, pl, ro Table 3: Duration statistics (hours of source speech) of speech-to-speech alignments for each pair of 17 languages (for mining threshold of 1.06). The last column provides statistics for alignments of source speech against 21.5 billion sentences of English texts. The last row provides duration of raw speech from VoxPopuli used for mining.\n\ntarget speech and source transcription as our test set. To ensure that there is no overlap between the mined data and VoxPopuli test sets, we need to remove speech from mined alignments which are from the same session as test samples. In order to keep as much mined data as possible, we use VoxPopuli test set only when a language direction is not covered by EPST considering their domain similarity. Moreover, similarity scores are provided to indicate the quality of VoxPopuli samples. To choose high-quality data, we sort all sessions in the VoxPopuli S2S data in a decreasing order of the average similarity score of their samples. We keep adding samples from highly ranked sessions to the test set until the test size reaches 1000.\n\n(3) FLEURS (Conneau et al., 2022). Built upon N-way text translations from FLoRes (Goyal et al., 2022), FLEURS provides speech for aligned texts and creates speech-to-speech data covering all mined directions. We take its source speech and target texts as the test data. In the case where multiple utterances correspond to one piece of source text, we generate one test pair for each source utterance respectively. FLEURS texts are from English Wikipedia, which is a different domain from VoxPopuli and EPST.\n\nValid set. Valid sets are prepared for S2S modeling using VoxPopuli and FLEURS data in a similar way as test sets. For VoxPopuli, we extract a valid set of about 1000 samples by adding data from highly scored sessions which are not in the test set. FLEURS valid set is derived from its valid sam-ples. We prepare speech-to-unit data from these selected valid samples by transforming the target speech into target units for speech-to-unit modeling, which will be discussed in section 4.\n\n\nExperiments & Results\n\nTo evaluate the quality of the mined data, we trained S2ST models on SpeechMatrix data and report the translation performance. We hope that these results will serve as baselines for future studies in speech-to-speech translation.    \net - - - - - - - - - - - - - - - - - fi 3.\n\nExperimental Setup\n\nThe training and evaluation pipeline of speech-tospeech translation is shown in Figure 2. Recent progress in speech-to-speech modeling suggests to discretize the target speech waveform into a unit sequence, relieving models from the complexity of predicting continuous waveform values. We borrow the idea of training speech-to-unit (S2U) model where units are pre-generated from target speech with a pre-trained HuBERT model (Lee et al., 2022a). During S2U training, models are periodically evaluated on the valid set of speechto-unit samples, and the best checkpoint with the lowest valid loss is saved for model inference.\n\nWhen it comes to inference, speech could be synthesized from the predicted units with a vocoder, as the output of the S2S pipeline. It is then transcribed into texts by an off-the-shelf ASR model. The BLEU score is calculated by comparing the transcriptions against the ground truth target texts, which serves as the quantitative metric of mined data quality. We note that the ASR BLEU score is not a perfect metric for data quality, as it is unavoidably affected by the quality of ASR models. Next we discuss each module of the pipeline.\n\nSpeech-to-Unit. The S2U model takes the source speech and predicts a sequence of target units. It typically has an encoder-decoder architecture, where the encoder consists of convolutional and Transformer encoder layers, and the decoder is a Transformer decoder. We have experimented with different model variants, and discuss bilingual and multilingual training in section 5 and section 6.\n\nHuBERT. HuBERT is used to extract speech features of audio frames, which are then grouped into k-means clusters. The continuous features are thus mapped to corresponding clusters. In this way, speech could be discretized into unit sequence where units are basically indices of clusters. We reuse the same HuBERT model and k-means clusters for English, Spanish and French as in (Lee et al., 2022b) for a fair comparison with existing results. We also train multilingual HuBERT models to cover other languages in SpeechMatrix, and more HuBERT training details can be found in Appendix B.1.\n\nVocoder. Unit-based HiFi-GAN vocoders are trained to synthesize speech from unit sequence (Polyak et al., 2021). In our experiments, vocoders are separately trained from S2U model. We train vocoders on three datasets:\n\n(1) CSS10 (Park and Mulc, 2019). It is a singlespeaker corpus which we use to train vocoders in German, Finnish, Hungarian and Dutch.\n\n(2) VoxPopuli (Wang et al., 2021a). Given its ASR data with speaker id, we sort speakers based on their speech duration, and keep adding the top speakers until the speech is more than 20 hours.\n\n(3) Common Voice (Ardila et al., 2020). Portuguese and Estonian are not covered by the two corpora above, and thus we turn to Common Voice. Again, we select top speakers and prepare 12-hour and 10-hour speech for the vocoder training in Portuguese and Estonian respectively.\n\n\n16256\n\nData preprocessing and training are included in Appendix B.3.\n\nASR. We use off-the-shelf ASR models to transcribe the speech generated by vocoders. Details about the ASR models and their benchmark results of word error rates are provided in Appendix B.2.\n\n\nBilingual Speech-to-Speech Baselines\n\nIn this part, we discuss the bilingual S2S models trained in each of 272 language directions in SpeechMatrix. The architecture of Textless model is used for bilingual translation in our experiments (Lee et al., 2022a). A Textless model consists of a speech encoder, Transformer encoder and decoder.\n\nTraining. For a given direction, we extract units for source and target speech with their corresponding HuBERT models (Hsu et al., 2021). Taking source speech, the model is trained to predict target unit sequence with cross-entropy loss as well as source unit reconstruction as an auxiliary task.\n\nFor the training efficiency of extensive S2ST experiments, we use a subset of mined data as the train set. Mined samples are selected if their alignment scores are above a preset threshold. We performed an analysis of the threshold selection in subsection B.4.\n\nComparison with existing results. Since we adopt the same model as the previous work (Lee et al., 2022a) and the only difference lies in the train set, it is straightforward to compare with existing results. Table 4 shows the results of S2ST models which are trained on our SpeechMatrix mined data compared to VoxPopuli S2S data in each of four language directions: es-en, fr-en, en-es and enfr. The threshold of mined data is set as 1.09 to these four directions, yielding an average of 1, 436hour train set. Compared with 480-hour labeled speech from VoxPopuli, SpeechMatrix achieves an an average improvement of 5.4 BLEU, indicating the good quality and usefulness of the mined data.\n\n\nLarge-Scale Bilingual Evaluation\n\nA large-scale evaluation is launched covering 272 mined languages directions, and bilingual models are trained for each direction to establish baseline results in speech-to-speech translation. Table 5 and Table 6 summarize performance of bilingual S2ST models on three test sets. In each direction, Table 5 reports BLEU scores in European Parliament domain, either EPST or VoxPopuli set. EPST BLEU is underlined to be distinguished from VoxPopuli BLEU. Table 6 reports BLEU in Wikipedia domain, i.e., FLEURS test data.\n\nBilingual results. Empirically we find that translations into high-resource languages such as en, es and fr outperform those into low-resource languages such as lt and sl based on the speech amount of these languages in Table 3. Another observation is the performance difference across test domains, and BLEU on FLEURS is lower than that on EPST and VoxPopuli data, likely because of the domain mismatch between train and test data.\n\nIt is also found that translation results are not symmetric for some language pairs, for example, ro-en has a BLEU of 22.6 while en-ro BLEU is only 7.6 on EPST. Besides different complexity levels of target languages and test sets, such asymmetry also results from the dependency of BLEU scores on the speech synthesis quality of the vocoder and transcription quality of the ASR model. For languages whose vocoder and ASR models are not good, they are likely to receive low BLEU scores. In this case, Romanian vocoder and ASR are not as strong as English models as reflected by its higher word error rate in speech resynthesis as reported in Appendix B.3.\n\n\nMultilingual Speech-to-Speech Translation\n\nMultilingual modeling has been explored in tasks of language understanding and machine translation, demonstrating knowledge transfer among languages. However, to our best knowledge, there are few studies of multilingual S2ST on real speech, partially due to the lack of multilingual speech-tospeech resources. With the massively multilingual data we have mined, we are able to explore multilingual S2ST training.\n\nIn this work, we focus on many-to-English translation, studying the translation from 6 Slavic languages to English in subsection 6.1 and the translation from all 16 languages in SpeechMatrix to English in subsection 6.2. English-to-many or manyto-many translation are left to future work. We present here multilingual models used in our experiments (more details can be found in Appendix C:\n\n(1) Textless model. The same model with 70M parameters that we use for bilingual evaluation is reused in the multilingual experiments. Given diverse multilingual data, we increase the model size for larger model capacity, trying multilingual models with 70M and 260M parameters.  (2) XM Transformer. Inspired by the recent finding that crossmodal pre-training is beneficial for speech translation (Popuri et al., 2022), we apply XM Transformer to multilingual training, whose encoder is initialized from pre-trained XLS-R model with 1B parameters (Babu et al., 2021) and decoder is initialized from a unit decoder pretrained in an mBART style . With multilingual speech-to-unit data, the model is further finetuned to minimize the cross-entropy loss in target unit prediction.\n\n(3) XM Transformer with Sparsity. Sparse modeling, in particular Mixture-of-Experts (MoE), has been widely studied in multilingual machine translation. MoE increases the number of parameters without sacrificing computation efficiency.\n\nGShard. GShard is a sparse scaling technique proposed in (Lepikhin et al.). We replace every other Transformer layer with an MoE layer. FFN modules in an MoE transformer layer are shared across experts. A learnable gating function routes input tokens to different experts (NLLB Team et al., 2022). We apply GShard architecture on the decoder of XM Transformer, and expert weights are all initialized with the pretrained unit mBART.\n\n\nSlavic-to-English Translation\n\nThe six Slavic languages include Czech (cs), Croatian (hr), Lituanian (lt), Polish (pl), Slovak (sk), and Slovenian (sl). In the multilingual setting, all mined data into English are combined from each  Slavic language as the train set.\n\nWe summarize ASR BLEU scores of different models averaged over six Slavic-to-English directions in Table 7. For completeness, we report BLEU of each direction in Appendix C. As is shown, Textless model benefits from the parameter increase to 260M, and multilingual training further brings BLEU gains of 5.6 and 4.7 in EP/VP and FLEURS. We tried larger models than 260M but didn't see more gains.\n\nComparing against bilingual Textless model (70M), bilingual XM Transformer achieves +3.8 BLEU in EP/VP and +5.0 BLEU in FLEURS. Multilingual training further improves dense XM Transformer by 7.9 and 5.1 BLEU. GShard with 64 experts brings +1.0 BLEU over dense XM Transformer to EP/VP, and +0.3 BLEU to FLEURS. Overall the best Slavic-to-English translation is achieved by XM Transformer with GShard trained in multilingual setting. This demonstrates that multilinguality, pre-training and model sparsity are of help to speech-to-speech translation.  \n\n\nAll-to-English Translation\n\nWe move forward to a larger-scale multilinguality by extending from Slavic language family to all languages in SpeechMatrix. We adopt the best models in Slavic-to-English translation, i.e., multilingual XM Transformer with both dense and sparse architectures.\n\nResults. Compared with XM Transformer (1.2B) dense model, MoE-GShard64 (4.3B) with the same forward computation time brings gains of +0.9 and +0.2 BLEU to EP/VP and FLEURS respectively. Similar to our findings in Slavic-to-English setting, increasing the capacity with sparse modeling benefits in-domain (EP/VP) more than out-of-domain FLEURS test set.\n\nGiven sparse architecture of XM Transformer with GShard, all-to-English model shows +0.6 and -0.4 BLEU difference compared with Slavic-to-English model on EP/VP and FLEURS respectively, averaged over Slavic languages. Multilingual sparse model benefits from the additional indomain data in other languages when evaluated in EP/VP domain, while sees performance degradation in out-of-domain data.\n\n\nLimitations and Risks\n\nLimitations. The HuBERT model quality is critical to speech-to-speech translation performance, as its extracted units are used by both speech-tounit model and vocoder. We have not explored the optimal strategy of multilingual HuBERT training. One research question is how to choose a group of languages so that a multilingual HuBERT model could be well trained. For example, it is arguable whether Lithuanian (lt) should be included in Slavic or Uralic family. Other questions could be whether a larger HuBERT with more model capacity should be used and how we should deal with language imbalance in multilingual training.\n\nWe provide benchmark results of bilingual speech translation with mined data selected by heuristics. One of our future directions is to come up with a better strategy of mined data selection to improve translation performance and training efficiency.\n\nAs mentioned in our results analysis, the reported BLEU scores are heavily dependent on the ASR quality, which may not reflect the speech translation performance accurately. Future directions could be improving ASR quality or exploring other evaluation metrics without reliance on ASR models.\n\nPotential Risks. As a technology used for speech generation, the presented speech translation models or the translation models that will be trained with SpeechMatrix dataset might have systemic bias or produce inappropriate outputs.\n\n\nConclusion\n\nIn this paper, we introduce a large-scale multilingual speech-to-speech corpus mined from VoxPopuli. It is the largest resource of speech alignments with a coverage of 17 languages. We perform an extensive evaluation of the mined parallel speech, showing good quality of the speech alignments. Multilingual speech-to-speech models can be efficiently trained on this corpus and we suggest different methods, such as sparse scaling using Mixture-of-Experts, to further boost translation performance in the multilingual setting. \n\n\nA Speech Encoder\n\n\nA.1 Similarity search on CoVoST\n\nWe compared our similarity search results with previous work (Duquenne et al., 2021) in Table 9. We notice that our new speech encoders have lower error rates compared to previous work.  We also provide similarity search of audios against written translations or transcriptions on CoVoST 2 test set for other languages covered by our speech encoders in Table 10, in order to evaluate cross-modal similarity search.  \n\n\nB Bilingual Speech-to-Speech Translation\n\nWe describe experiment details of bilingual speechto-speech translation.\n\n\nB.1 HuBERT Family Languages\n\nRomance es, fr, it, pt, ro Slavic cs, pl, sk, sl, hr, lt Germanic en, de, nl Uralic fi, et, hu We train a multilingual HuBERT model for each family on the collection of speech in each component language as shown in Table 11. We collect unlabeled VoxPopuli speech for all languages of the same family as the training data. The HuBERT model consists of 7 convolutional layers and 12 Transformer encoder layers. Each encoder layer has 12 attention heads, the embedding dimension of 768 and the forward dimension of 3072. Models are trained for 3 iterations, and in each iteration pseudo-labels are prepared as the training target for 12 16262 jonatasgrosman/wav2vec2-xls-r-1b-dutch jonatasgrosman/wav2vec2-xls-r-1b-polish Lang pt ro ASR jonatasgrosman/wav2vec2-xls-r-1b-portuguese gigant/romanian-wav2vec2 Lang sk sl ASR anuragshas/wav2vec2-xls-r-300m-sk-cv8-with-lm anuragshas/wav2vec2-xls-r-300m-sl-cv8-with-lm utterances. In the first iteration, the target labels are MFCC features. In the second iteration, we extract speech features from the 6-th layer of the trained HuBERT model and apply k-means clustering to derive a set of 500 labels. In the third iteration, speech features from the 9-th layer are clustered into 500 labels. Lastly after these three iterations, we try feature extraction from different layers including layer 10, 11 and 12 of trained HuBERT. As for feature clustering, we also try different numbers of clusters, 800, 1000 and 1200, to derive multiple sets of target units.\n\nTo choose the optimal setup, we launch a resynthesis evaluation to select the HuBERT layer to extract speech features and the number of k-means clusters. We train a vocoder on each set of target units, i.e., vocoder takes the units and synthesizes target speech. The synthesized speech is sent to off-the-shelf ASR models, and Word Error Rate (WER) is reported to measure the speech quality. The resynthesis experiments are discussed in subsection B.3. The optimal HuBERT layer and label size is selected if their corresponding vocoder achieves the lowest WER.\n\n\nB.2 ASR models\n\nWe use ASR models publicly released on Hugging-Face to transcribe the generated speech in order to calculate WER or BLEU scores in comparison with ground truth texts. ASR models used in our evaluation are listed in Table 12.\n\n\nB.3 Vocoder\n\nData preprocessing. We applied a denoiser 4 (Defossez et al., 2020) to the speech of VoxPopuli and Common Voice as the speech preprocessing to increase signal-to-noise ratio (SNR) given that they are noisier than CSS10 audios. Then we prepare vocoder labels with HuBERT models generating k-means cluster labels for each utterance. Single-speaker vocoders are trained in CSS10, and languages from VoxPopuli and Common Voice have multi-speaker vocoders where speaker embeddings are learned. During inference, we select the speaker with the longest speech duration to synthesize speech from predicted unit sequences, who has the most data for the vocoder to learn good speaker embeddings.\n\nVocoder training and evaluation. Vocoders are trained to synthesize speech from a given sequence of units. The train sets are speech data from CSS10, VoxPopuli and Common Voice. As mentioned before, units are derived from HuBERT models for these speech. Table 13 summarizes WER of ASR models, which reflects the transcription quality in each language. Besides, we report the training data, vocoder WER of synthesized speech from vocoders, and here we include the vocoder results obtained from the optimal HuBERT layer and kmeans cluster size. Layer 11 is the best HuBERT layer for feature extraction in all languages, and most languages have the best k-means size of 1000 except Italian (it) whose best label size is 800.\n\nAs shown in  \n\n\nB.4 Training\n\nTextless model. A Textless model consists of a speech encoder with 2 convolution layers and 12 Transformer encoder layers. Transformer layer has the embedding dimension of 512 and the forward dimension of 2048. It has two unit decoders with 6 and 2 Transformer decoder layers for target and source unit prediction respectively. The target unit decoder has the embedding dimension of 512 and the forward dimension of 2048, and the source unit decoder's dimensions are 256 and 2048. Hyperparameters. We tried learning rates of 0.0003 and 0.0005, and dropout rates of 0.1 and 0.3. The best setup is a learning rate of 0.0005 and a dropout of 0.3 for bilingual Textless model training. Bilingual models are trained with a batch of 20k tokens for 400k steps. A label smoothing weight of 0.2 is applied to the cross-entropy loss.\n\nAs for decoding of speech-to-unit models, we set the beam size of 10 in all bilingual and multilingual experiments.\n\nMined data selection. We performed an analysis of translation performance varying with thresholds from 1.06 to 1.09 on three language pairs: es-en, ro-en and hr-en. Figure 3 shows the thresh- old, the corresponding speech data size and BLEU score.\n\nFor low-resource directions such as hr-en, it is best to include all the mined data. For high-and medium-resource directions, es-en and ro-en, the optimal amount of mined data is around 1k hours and it does not bring further gains to go beyond that data size. Given these observations, we choose the highest threshold that keeps the source speech duration in mined data more than 1k hour for each direction. For example, we use a threshold of 1.09 for es-en and of 1.06 for hr-en.\n\nComputation. Each bilingual model is trained on 16 A100 GPUs for 3 days on average.\n\n\nC Multilingual Speech-to-Speech Translation\n\nWe provide details of models and experiment setups in multilingual speech-to-speech translation.\n\n\nC.1 Slavic-to-English Translation\n\nTextless model. Textless model (260M) has a speech encoder with 4 convolution layers and 12\n\nTransformer encoder layers with the embedding dimension of 1024 and the forward dimension of 4096. It has two unit decoders with 6 and 2 Transformer decoder layers for target and source unit prediction respectively. The target unit decoder has the embedding dimension of 1024 and the forward dimension of 4096, and the source unit decoder's dimensions are 256 and 2048. For the Textless model (424M), its speech encoder contains 6 convolution layers and 16 Transformer encoder layers with the embedding dimension of 1024 and the forward dimension of 4096. It has two unit decoders with 12 and 2 Transformer decoder layers for target and source unit prediction respectively. The target unit decoder has the embedding dimension of 1024 and the forward dimension of 4096, and the source unit decoder's dimensions are 256 and 2048.\n\nXM Transformer. XM Transformer (1.2B) is initialized from XLS-R encoder with 7 convolution layers and 48 Transformer encoder layers with the embedding dimension of 1280 and the forward dimension of 5120. Its unit decoder is initialized from a pre-trained mbart-style decoder with 12 layers, embedding dimension of 1024 and forward dimension of 4096.\n\nHyperparameters. For Textless model, we reuse a learning rate of 0.0005, a dropout of 0.3   With the Textless model size fixed as 70M, multilingual training hurts the performance of most languages compared with bilingual training. This is due to the insufficient model capacity, and the language interference is reflected by an average of \u22122.6 BLEU in FLEURS. We increase model parameters to 260M in both bilingual and multilingual settings. With a larger model capacity, bilingual models achieve gains in high-resource languages including cs, pl and sk, while suffering from performance loss in low-resource directions such as hr, lt and sl.\n\nGiven model sizes of 260M, we observe consistent gains of multilingual models over the bilingual models across different language directions and test domains. An average gain of 5.6 BLEU is achieved in EP/VP and the gain of 4.7 BLEU in FLEURS. It demonstrates the positive transfer enabled by multilingual training. As the multilingual model size continues to increase to 424M, we don't observe further gains likely due to the bottleneck of training data amount.\n\nXM Transformer leveraging pre-trained modules is also trained on Slavic-to-English data. Pre-training is shown to be beneficial, and results are reported in Table 15. Comparing against bilingual Textless model (70M), bilingual XM Transformer outperforms it in all directions except lt-en. The gain in EP/VP is 3.8 BLEU on average, and a larger gain of 5.0 BLEU is achieved in FLEURS. Multilingual training brings further gains to XM Transformer with +7.9 and +5.1 BLEU over bilingual training in EP/VP and FLEURS test set 15 respectively.\n\nComparing against dense XM Transformer, GShard with 64 experts has 1.0 BLEU gains on average over 5 directions on EP/VP, and +0.3 BLEU gains for FLEURS. We believe that it is due to a phenomena mentioned in (Zoph et al., 2022), i.e., MoE specializes in multilingual settings but not by language. GShard in our setting brings larger improvements to in-domain test sets.\n\nComputation. Textless models used 32 A100 GPUs, the 70M model was trained for 3 days, the 260M model was for 5 days, and the 424M model was for 6 days. It took 2 days to train XM Transformer on 32 A100 GPUs for Slavic-to-English translation.\n\n\nC.2 All-to-English Translation\n\nIn this work, we experiment with two variants of sparse modeling, GShard and Base Layer.  XM Transformer-GShard. XM Transformer (1.2B) is initialized with the same XLS-R encoder and unit decoder used in Slavic-to-English experiments. On the decoder side of XM Transformer-GShard, each expert is initialized with the same unit decoder. We set MoE frequency as 2, i.e., every other Transformer layer is an MoE layer.\n\nXM Transformer-Base Layer. For our XM Transformer with Base Layer sparsity (1.7B), the encoder is initialized with the same XLS-R encoder, and the dense layers of the decoder is initialized with the same unit decoder as GShard. We add an additional Base Layer which is randomly initialized as the 7th layer of decoder. There is one expert in each GPU and we used 64 GPUs in our experiments, which means we have 64 Base Layer experts in total.\n\nThe sparse variant, Base Layer (1.7B) performs comparably to the dense XM Transformer, with an average of +0.4 BLEU in EP/VP test sets and -0.4 BLEU in FLEURS. The sparsity in Base Layer does not bring obvious gains to all-to-English translation. This is likely because we only add one Base Layer to the decoder with a small expert size. The number of increased model parameters is only 0.5B in Base Layer, while it is 3.1B in GShard. As suggested by (Lewis et al., 2021), the Base Layer performance might improve with more GPUs and a larger expert size.\n\nHyperparameters. For dense XM Transformer, hyperparameters are the same as that for Slavic-to-English. GShard also shares the same set of hyperparameters. As for expert-specific parameters, we use 64 experts with each running on a single GPU with the frequency of 2 so that every other Transformer decoder layer becomes an MoE layer. The capacity token fraction is set as 0.5 so that if more than half of tokens in a sample get routed to one expert, extra tokens would overflow and get dropped.\n\nComputation. It took 3 days to train dense XM Transformer for all-to-English with 32 A100 GPUs. It took 5 days to train the GShard counterpart with 64 A100 GPUs.\n\n\nD License and Terms of Scientific Artifacts D.1 Third-Party Artifacts\n\nData. \n\n\nD.2 SpeechMatrix and translation models\n\nThe mined resource, SpeechMatrix, will be released under CC0 license, and the trained speechto-speech translation models will be released under CC BY-NC 4.0. The data and models are intended for research purposes.\n\n\n16267\n\n\nACL 2023 Responsible NLP Checklist\n\nA For every submission:\n\n\nA1. Did you describe the limitations of your work?\n\nLimitations of this work is discussed in Section 7.\n\nA2. Did you discuss any potential risks of your work?\n\nPotential risks of this work are discussed in Section 7.\n\nA3. Do the abstract and introduction summarize the paper's main claims?\n\nThis paper's main claims are summarized in abstract and introduction (Section 1).\n\nA4. Have you used AI writing assistants when working on this paper?\n\nLeft blank.\n\n\nB Did you use or create scientific artifacts?\n\nSections 2-6.\n\n\nB1. Did you cite the creators of artifacts you used?\n\nSpeechMatrix is mined from VoxPopuli, a raw speech corpus, which is cited in Section 2. We are using public datasets in speech recognition and translation to train speech encoders, and the dataset creators are cited in Section 3.1. Speech translation datasets which are used as testsets for speechto-speech translation models are cited in Section 3.4. As for the model, speech encoder is initialized from XLS-R model, which is cited in Section 3.1. The code and implementations which are used for speech-to-speech translation in our experiments are cited in Section 4, 5 and 6.\n\nB2. Did you discuss the license or terms for use and / or distribution of any artifacts?\n\nThe license and terms are discussed in Appendix D.\n\nB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? Appendix D discusses the use of existing artifacts and our created artifacts.\n\nB4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? The datasets used in this work such as Common Voice and CoVoST have anonymized speakers and require dataset users not to attempt to identify their identities.\n\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? We provide domain coverage and language coverage in Section 3.\n\nB6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. We report the data size of train, valid and test samples in Section 3.\n\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\n\nFigure 1 :\n1Architecture of speech encoders training.\n\nFigure 3 :\n3Bilingual S2S BLEU by mined data at different thresholds.\n\n\nand trained speech encoders with the supervision of the multilingual LASER text encoder (Schwenk et al., 2021b). Transcriptions or written translation of the audio utterances are encoded with LASER text encoder as2 \n16252 \n\nDataset \n\n# of Languages Avg. duration (h) \nSource speech \nTarget speech \n\nFisher (Post et al., 2014) \n2 \n127 \nTelephone conversation \nSynthetic \nMaSS (Boito et al., 2020) \n8 \n20 \nBible reading \nBible reading \nVoxPopuli (Wang et al., 2021a) \n15 \n82 \nEuropean Parliament speech Simultaneous interpretation \nCVSS (C+T) (Jia et al., 2022c) \n21 \n181 \nRead \nSynthetic \nFLEURS (Conneau et al., 2022) \n102 \n12 \nRead \nRead \nSpeechMatrix (ours) \n17 \n1537 \nEuropean Parliament speech European Parliament speech \n\n\n\nTable 1 :\n1A comparison of existing speech-to-speech datasets.\n\nTable 2 :\n2Similarity search error rates (in %) on VoxPopuli ASR test set.\n\nTable 4 :\n4BLEU scores on EPST test sets by S2ST models with different training data.5 \n\nTable 5 :\n5BLEU scores of bilingual S2S models on EP/VP test sets. EPST score is underscored.\n\nTable 6 :\n6BLEU scores of bilingual S2S models on FLEURS sets.\n\n\nBilingualMultilingual EP/VP FL EP/VP FL EP/VP FL EP/VPFL \nTextless \n70M \n260M \n70M \n260M \nAvg. \n14.3 \n5.1 \n16.8 \n6.5 \n14.1 \n2.5 \n22.4 \n11.2 \nXM \nDense(1.2B) \nDense (1.2B) GShard (4.3B) \nAvg. \n18.1 \n10.1 \n26.0 \n15.2 \n27.0 \n15.5 \n\n\n\nTable 7 :\n7Average BLEU of Slavic-to-English models in EP/VP and FLEURS (FL) domains.\n\nTable 8 :\n8BLEU of All-to-English multilingual models across FLEURS (FL) and EP/VP domains (for EP/VP column, underlined scores are on EPST data, and others on VoxPopuli data).\n\n\nComparable Corpora Using Multilingual Sentence Embeddings. In BUCC. Douglas W. Oard, and Matt Post. 2021. The multilingual tedx corpus for speech recognition and translation. In Interspeech 2021, 22nd Annual Conference of the International Speech Communication Association, Brno, Czechia, 30 August -3 September 2021, pages 3655-3659. ISCA.Rosana Ardila, Megan Branson, Kelly Davis, Michael \nKohler, Josh Meyer, Michael Henretty, Reuben \nMorais, Lindsay Saunders, Francis M. Tyers, and \nGregor Weber. 2020. Common voice: A massively-\nmultilingual speech corpus. In Proceedings of The \n12th Language Resources and Evaluation Confer-\nence, LREC 2020, Marseille, France, May 11-16, \n2020, pages 4218-4222. European Language Re-\nsources Association. \n\nMikel Artetxe and Holger Schwenk. 2018. Margin-\nbased Parallel Corpus Mining with Multilingual \nSentence Embeddings. https://arxiv.org/ \nabs/1811.01136. \n\nArun Babu, Changhan Wang, Andros Tjandra, Kushal \nLakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, \nPatrick von Platen, Yatharth Saraf, Juan Pino, Alexei \nBaevski, Alexis Conneau, and Michael Auli. 2021. \nXLS-R: self-supervised cross-lingual speech repre-\nsentation learning at scale. CoRR, abs/2111.09296. \n\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, \nand Michael Auli. 2020. wav2vec 2.0: A framework \nfor self-supervised learning of speech representations. \nIn Advances in Neural Information Processing Sys-\ntems 33: Annual Conference on Neural Information \nProcessing Systems 2020, NeurIPS 2020, December \n6-12, 2020, virtual. \n\nMarcely Zanon Boito, William Havard, Mahault Gar-\nnerin, \u00c9ric Le Ferrand, and Laurent Besacier. 2020. \nMass: A large and clean multilingual corpus of \nsentence-aligned spoken utterances extracted from \nthe bible. In Proceedings of The 12th Language \nResources and Evaluation Conference, LREC 2020, \nMarseille, France, May 11-16, 2020, pages 6486-\n6493. European Language Resources Association. \n\nHouda Bouamor and Hassan Sajjad. 2018. \nH2@BUCC18: \nParallel Sentence Extraction \nfrom Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, \nVera Axelrod, Siddharth Dalmia, Jason Riesa, Clara \nRivera, and Ankur Bapna. 2022. FLEURS: few-shot \nlearning evaluation of universal representations of \nspeech. CoRR, abs/2205.12446. \n\nAlexandre Defossez, Gabriel Synnaeve, and Yossi Adi. \n2020. Real time speech enhancement in the wave-\nform domain. In Interspeech. \n\nMattia A. Di Gangi, Roldano Cattoni, Luisa Bentivogli, \nMatteo Negri, and Marco Turchi. 2019. MuST-C: a \nMultilingual Speech Translation Corpus. In Proceed-\nings of the 2019 Conference of the North American \nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1 \n(Long and Short Papers), pages 2012-2017, Min-\nneapolis, Minnesota. Association for Computational \nLinguistics. \n\nQuoc Truong Do, Sakriani Sakti, Graham Neubig, \nTomoki Toda, and Satoshi Nakamura. 2015. Im-\nproving translation of emphasis with pause predic-\ntion in speech-to-speech translation systems. In Pro-\nceedings of the 12th International Workshop on Spo-\nken Language Translation: Papers, IWSLT 2015, Da \nNang, Vietnam, December 3-4, 2015. \n\nPaul-Ambroise Duquenne, Hongyu Gong, Beno\u00eet Sagot, \nand Holger Schwenk. 2022. T-modules: Translation \nmodules for zero-shot cross-modal machine transla-\ntion. In EMNLP. \n\nPaul-Ambroise Duquenne, Hongyu Gong, and Holger \nSchwenk. 2021. Multimodal and multilingual em-\nbeddings for large-scale speech mining. Advances in \nNeural Information Processing Systems, 34:15748-\n15761. \n\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen \nArivazhagan, and Wei Wang. 2020. Language-\nagnostic bert sentence embedding. arXiv preprint \narXiv:2007.01852. \n\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-\nJen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-\nishnan, Marc'Aurelio Ranzato, Francisco Guzm\u00e1n, \nand Angela Fan. 2022. The flores-101 evaluation \nbenchmark for low-resource and multilingual ma-\nchine translation. Trans. Assoc. Comput. Linguistics, \n10:522-538. \n\nKevin Heffernan, Onur \u00c7elebi, and Holger Schwenk. \n2022. Bitext mining using distilled sentence repre-\nsentations for low-resource languages. In EMNLP. \n\nWei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, \nKushal Lakhotia, Ruslan Salakhutdinov, and Abdel-\nrahman Mohamed. 2021. Hubert: Self-supervised \nspeech representation learning by masked prediction \nof hidden units. IEEE ACM Trans. Audio Speech \nLang. Process., 29:3451-3460. \n\nJavier Iranzo-S\u00e1nchez, Joan Albert Silvestre-Cerd\u00e0, \nJavier Jorge, Nahuel Rosell\u00f3, Adri\u00e0 Gim\u00e9nez, Al-\nbert Sanch\u00eds, Jorge Civera, and Alfons Juan. 2020. \nEuroparl-st: A multilingual corpus for speech transla-\ntion of parliamentary debates. In 2020 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal \nProcessing, ICASSP 2020, Barcelona, Spain, May \n4-8, 2020, pages 8229-8233. IEEE. \n\nYe Jia, Yifan Ding, Ankur Bapna, Colin Cherry, \nYu Zhang, Alexis Conneau, and Nobuyuki Morioka. \n2022a. \nLeveraging unsupervised and weakly-\nsupervised data to improve direct speech-to-speech \ntranslation. CoRR, abs/2203.13339. \n\nYe Jia, Michelle Tadmor Ramanovich, Tal Remez, \nand Roi Pomerantz. 2022b. Translatotron 2: High-\nquality direct speech-to-speech translation with voice \npreservation. In International Conference on Ma-\nchine Learning, ICML 2022, 17-23 July 2022, Balti-\nmore, Maryland, USA, volume 162 of Proceedings \nof Machine Learning Research, pages 10120-10134. \nPMLR. \n\n10 \n16260 \nHolger Schwenk, Guillaume Wenzek, Sergey Edunov, \nEdouard Grave, Armand Joulin, and Angela Fan. \n2021a. CCMatrix: Mining billions of high-quality \nparallel sentences on the web. In ACL, pages 6490-\n6500. \n\nHolger Schwenk, Guillaume Wenzek, Sergey Edunov, \nEdouard Grave, Armand Joulin, and Angela Fan. \n2021b. Ccmatrix: Mining billions of high-quality \nparallel sentences on the web. In Proceedings of the \n59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International \nJoint Conference on Natural Language Processing, \nACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual \nEvent, August 1-6, 2021, pages 6490-6500. Associa-\ntion for Computational Linguistics. \n\nSilero-Team. 2021. Silero vad: pre-trained enterprise-\ngrade voice activity detector (vad), number detector \nand language classifier. https://github.com/ \nsnakers4/silero-vad. \n\nChanghan Wang, Juan Pino, Anne Wu, and Jiatao Gu. \n2020. Covost: A diverse multilingual speech-to-text \ntranslation corpus. In Proceedings of the 12th Lan-\nguage Resources and Evaluation Conference, pages \n4197-4203. \n\nChanghan Wang, Morgane Rivi\u00e8re, Ann Lee, Anne Wu, \nChaitanya Talnikar, Daniel Haziza, Mary Williamson, \nJuan Miguel Pino, and Emmanuel Dupoux. 2021a. \nVoxpopuli: A large-scale multilingual speech corpus \nfor representation learning, semi-supervised learning \nand interpretation. In Proceedings of the 59th Annual \nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference \non Natural Language Processing, ACL/IJCNLP 2021, \n(Volume 1: Long Papers), Virtual Event, August 1-6, \n2021, pages 993-1003. Association for Computa-\ntional Linguistics. \n\nChanghan Wang, Anne Wu, Jiatao Gu, and Juan Pino. \n2021b. Covost 2 and massively multilingual speech \ntranslation. In Interspeech, pages 2247-2251. \n\nYinfei Yang, Gustavo Hernandez Abrego, Steve Yuan, \nQinlan Shen Mandy Guo, Daniel Cer, Brian Strope \nYun-hsuan Sun and, and Ray Kurzweil. 2019. Im-\nproving multilingual sentence embedding using bi-\ndirectional dual encoder with additive margin soft-\nmax. In IJCAI, pages 5370-5378. \n\nBarret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yan-\nping Huang, Jeff Dean, Noam Shazeer, and William \nFedus. 2022. Designing effective sparse expert mod-\nels. arXiv preprint arXiv:2202.08906. \n\n\n\nTable 9 :\n9Similarity search error rates (in %) on \nCoVoST 2 test set. \n\n\n\nTable 10 :\n10Similarity search error rates (in %) on \nCoVoST 2 test set. \n\n\n\nTable 11 :\n11Language families in VoxPopuli data.\n\nTable 12 :\n12HuggingFace ASR models for each language.\n\nTable 13 ,\n13ASR models are of good quality for high-resource languages such as de, fi4 https://github.com/facebookresearch/ \ndenoiser \n\n13 \n16263 \n\n\n\nTable 13 :\n13Benchmark results of ASR models and vocoder resynthesis.and pt, while suffering from high error rates in lan-\nguages such as ro, lt and sl. It is expected to have \nhigher vocoder WER than ASR WER since the \nformer is for synthesized speech. By measuring \nthe gap between the two error rates, we can tell \nhow good a vocoder is and also infer the quality of \nHuBERT units. For et, pt and lt, the gaps are obvi-\nously larger than other languages. It not surprising \nsince we do not have much good-quality vocoder \ndata for these languages. For example, there is only \naround 10-hour noisy speech from Common Voice \nfor et and pt vocoder training. \n\n\n\n\nVP FL EP/VP FL EP/VP FL EP/VP FL EP/VP FLBilingual \nMultilingual \n70M \n260M \n70M \n260M \n424M \nEP/cs \n22.7 \n4.2 \n24.7 \n11.2 \n19.7 \n2.3 \n27.5 \n13.7 \n25.3 \n10.2 \nhr \n-\n7.7 \n-\n4.6 \n-\n3.1 \n-\n12.8 \n-\n9.2 \nlt \n3.1 \n0.9 \n0.2 \n0.0 \n2.8 \n0.3 \n14.7 \n4.8 \n10.7 \n3.3 \npl \n4.9 \n4.9 \n17.6 \n7.7 \n14.4 \n1.9 \n19.9 \n9.5 \n16.4 \n6.9 \nsk \n21.4 \n5.5 \n24.4 \n11.0 \n18.9 \n4.1 \n27.2 \n15.4 \n24.9 \n11.1 \nsl \n19.5 \n7.3 \n16.9 \n4.7 \n14.6 \n3.1 \n22.9 \n10.7 \n21.0 \n7.6 \navg \n14.3 \n5.1 \n16.8 \n6.5 \n14.1 \n2.5 \n22.4 \n11.2 \n19.7 \n8.1 \n\n\n\nTable 14 :\n14BLEU of Slavic-to-English multilingual Textless model across FLEURS (FL) and EP/VP domains (for EP/VP column, underlined scores are on EPST data, and others on VoxPopuli data).Bilingual (1.2B) Multiling. Dense (1.2B) Multiling. GShard (4.3B) \nEP/VP \nFL \nEP/VP \nFL \nEP/VP \nFL \ncs \n28.3 \n17.8 \n29.7 \n18.2 \n30.6 \n19.3 \nhr \n-\n12.1 \n-\n17.1 \n-\n17.6 \nlt \n0.0 \n0.0 \n20.9 \n9.6 \n22.2 \n10.2 \npl \n17.4 \n7.4 \n21.1 \n12.9 \n21.4 \n12.6 \nsk \n24.7 \n14.5 \n30.8 \n19.3 \n31.8 \n20.0 \nsl \n20.1 \n8.5 \n27.4 \n14.0 \n29.1 \n13.0 \navg \n18.1 \n10.1 \n26.0 \n15.2 \n27.0 \n15.5 \n\n\n\nTable 15 :\n15BLEU of Slavic-to-English multilingual XM Transformer models across FLEURS (FL) and EP/VP domains (for EP/VP column, underlined scores are on EPST data, and others on VoxPopuli data). and a label smoothing weight of 0.2 for Slavic-to-English training. The 70M model has 20k tokens in one batch. The 260M model has batch tokens of 6k and a update frequency of 4. The 424M model has tokens of 4000 and a update frequency of 6. For XM Transformer model, we use a learning rate of 0.0001, a dropout of 0.1 and a label smoothing weight of 0.2. In a batch, token sizes of 1500 and 9000 with update frequency of 15 and 2 are used for V100 and A100 training respectively.Results. We first extend Textless model from the bilingual to multilingual setting. Translation results are presented for Textless models with different parameter sizes inTable 14. Multilingual Textless model works best with 260M parameters. Compared with its bilingual counterparts, an average gain of 5.6 BLEU is achieved in EP/VP and the gain of 4.7 BLEU in FLEURS.\n\n\nDense (1.2B) MoE-GShard64 (4.3B) Base Layer (1.7B) EP/VP FL EP/VPFL \nEP/VP \nFL \ncs \n29.9 \n18.7 \n30.9 \n18.2 \n29.9 \n17.3 \nde \n18.8 \n19.0 \n19.3 \n20.3 \n19.4 \n19.5 \nes \n22.8 \n15.2 \n23.3 \n15.9 \n22.9 \n14.9 \net \n-\n16.7 \n-\n16.7 \n-\n16.4 \nfi \n26.8 \n14.1 \n28.2 \n14.0 \n28.5 \n13.9 \nfr \n23.5 \n18.3 \n24.1 \n18.9 \n23.4 \n18.2 \nhr \n-\n16.6 \n-\n16.8 \n-\n16.3 \nhu \n20.2 \n12.0 \n21.3 \n12.5 \n20.5 \n12.1 \nit \n36.3 \n16.2 \n37.8 \n14.9 \n37.4 \n14.0 \nlt \n21.9 \n9.8 \n23.8 \n10.3 \n23.4 \n10.0 \nnl \n21.4 \n16.4 \n22.1 \n17.3 \n21.5 \n16.6 \npl \n21.2 \n12.4 \n21.3 \n13.4 \n20.9 \n12.5 \npt \n23.8 \n21.8 \n24.2 \n22.3 \n23.8 \n21.1 \nro \n25.1 \n19.7 \n25.0 \n19.8 \n25.3 \n19.0 \nsk \n30.8 \n19.6 \n32.2 \n18.2 \n31.5 \n18.4 \nsl \n28.3 \n13.7 \n29.9 \n13.7 \n28.8 \n13.5 \navg \n25.1 \n16.3 \n26.0 \n16.5 \n25.5 \n15.9 \n\n\n\nTable 16 :\n16BLEU of All-to-English multilingual models across FLEURS (FL) and EP/VP domains (for EP/VP column, underlined scores are on EPST data, and others on VoxPopuli data).\n\n\nCommon Voice is released under CC0 license, VoxPopuli and CoVoST 2 data are also under CC0 license. As for EuroParl, it is released under a Creative Commons license. The multilingual TEDx corpus is released under a CC BY-NC-ND 4.0 license. FLEURS dataset is under Creative Commons license (CC-BY-4.0). These datasets are publicly accessible and freely downloadable for research purposes. Models. XLS-R model used for the speech encoder initialization is open sourced under Apache-2.0 license. Text LASER used as the teacher model in training is released under BSD license. ASR models avaliable on HuggingFace are released under Apache-2.0 license. These models are publicly 16 16266 available. Code. The implementations of Textless model, XM Transformer, HuBERT and Vocoder are open sourced under MIT license.\nCzech (cs), German (de), English (en), Spanish (es), Estonian (et), Finnish (fi), French (fr), Croatian (hr), Hungarian (hu), Italian (it), Lithuanian (lt), Dutch (nl), Polish (pl), Portuguese (pt), Romanian (ro), Slovak (sk) and Slovenian (sl).\nAppendix B.4. C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? We report the parameter size and computational costs in Appendix B.4.C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? No response.C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? We report the experiment results from single runs in Section 4, 5 and 6 of the main paper, as well as Appendix B and C.C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?We report the evaluation setup and parameter setting in Appendix B and C.D Did you use human annotators (e.g., crowdworkers) or research with human participants?Left blank.D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? No response.D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)? No response.D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? No response.D4. Was the data collection protocol approved (or determined exempt) by an ethics review board? No response.D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? No response.\nOn the Use of Comparable Corpora to Improve SMT performance. Sadaf Abdul-Rauf, Holger Schwenk, EACL. Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the Use of Comparable Corpora to Improve SMT perfor- mance. In EACL, pages 16-23.\n\nCVSS corpus and massively multilingual speech-to-speech translation. Ye Jia, Michelle Tadmor Ramanovich, Quan Wang, Heiga Zen, abs/2201.03713CoRRYe Jia, Michelle Tadmor Ramanovich, Quan Wang, and Heiga Zen. 2022c. CVSS corpus and massively multilingual speech-to-speech translation. CoRR, abs/2201.03713.\n\nDirect speech-to-speech translation with a sequence-to-sequence model. Ye Jia, Ron J Weiss, Fadi Biadsy, Wolfgang Macherey, Melvin Johnson, Zhifeng Chen, Yonghui Wu, Interspeech 2019, 20th Annual Conference of the International Speech Communication Association. Graz, AustriaISCAYe Jia, Ron J. Weiss, Fadi Biadsy, Wolfgang Macherey, Melvin Johnson, Zhifeng Chen, and Yonghui Wu. 2019. Direct speech-to-speech translation with a sequence-to-sequence model. In Interspeech 2019, 20th Annual Conference of the International Speech Communication Association, Graz, Austria, 15-19 September 2019, pages 1123-1127. ISCA.\n\nSamu-xlsr: Semantically-aligned multimodal utterance-level cross-lingual speech representation. Sameer Khurana, Antoine Laurent, James Glass, arXiv:2205.08180arXiv preprintSameer Khurana, Antoine Laurent, and James Glass. 2022. Samu-xlsr: Semantically-aligned multimodal utterance-level cross-lingual speech representation. arXiv preprint arXiv:2205.08180.\n\nDirect speech-to-speech translation with discrete units. Ann Lee, Peng-Jen Chen, Changhan Wang, Jiatao Gu, Sravya Popuri, Xutai Ma, Adam Polyak, Yossi Adi, Qing He, Yun Tang, Juan Pino, Wei-Ning Hsu, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics. the 60th Annual Meeting of the Association for Computational LinguisticsDublin, IrelandAssociation for Computational Linguistics1ACL 2022Ann Lee, Peng-Jen Chen, Changhan Wang, Jiatao Gu, Sravya Popuri, Xutai Ma, Adam Polyak, Yossi Adi, Qing He, Yun Tang, Juan Pino, and Wei-Ning Hsu. 2022a. Direct speech-to-speech translation with dis- crete units. In Proceedings of the 60th Annual Meet- ing of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 3327-3339. Association for Computational Linguistics.\n\nTextless speech-to-speech translation on real data. Ann Lee, Hongyu Gong, Holger Paul-Ambroise Duquenne, Peng-Jen Schwenk, Changhan Chen, Sravya Wang, Yossi Popuri, Juan Adi, Jiatao Pino, Wei-Ning Gu, Hsu, Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022. the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022Seattle, WA, United StatesAssociation for Computational LinguisticsAnn Lee, Hongyu Gong, Paul-Ambroise Duquenne, Holger Schwenk, Peng-Jen Chen, Changhan Wang, Sravya Popuri, Yossi Adi, Juan Pino, Jiatao Gu, and Wei-Ning Hsu. 2022b. Textless speech-to-speech translation on real data. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL 2022, Seattle, WA, United States, July 10-15, 2022, pages 860-872. As- sociation for Computational Linguistics.\n\nGshard: Scaling giant models with conditional computation and automatic sharding. Dmitry Lepikhin, Hyoukjoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. AustriaDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional computation and automatic sharding. In 9th International Con- ference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.\n\nBase layers: Simplifying training of large, sparse models. Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, Luke Zettlemoyer, PMLRInternational Conference on Machine Learning. Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. 2021. Base layers: Simplifying training of large, sparse models. In In- ternational Conference on Machine Learning, pages 6265-6274. PMLR.\n\nMultilingual denoising pretraining for neural machine translation. Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, Luke Zettlemoyer, Transactions of the Association for Computational Linguistics. 8Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020. Multilingual denoising pre- training for neural machine translation. Transac- tions of the Association for Computational Linguis- tics, 8:726-742.\n\nImproving Machine Translation Performance by Exploiting Non-Parallel Corpora. Stefan Dragos, Daniel Munteanu, Marcu, Computational Linguistics. 314Dragos Stefan Munteanu and Daniel Marcu. 2005. Im- proving Machine Translation Performance by Ex- ploiting Non-Parallel Corpora. Computational Lin- guistics, 31(4):477-504.\n\nThe ATR multilingual speech-to-speech translation system. Satoshi Nakamura, Konstantin Markov, Hiromi Nakaiwa, Gen-Ichiro Kikui, Hisashi Kawai, Takatoshi Jitsuhiro, Jinsong Zhang, Hirofumi Yamamoto, Eiichiro Sumita, Seiichi Yamamoto, IEEE Trans. Speech Audio Process. 142Satoshi Nakamura, Konstantin Markov, Hiromi Nakaiwa, Gen-ichiro Kikui, Hisashi Kawai, Takatoshi Jitsuhiro, Jinsong Zhang, Hirofumi Yamamoto, Eiichiro Sumita, and Seiichi Yamamoto. 2006. The ATR multilingual speech-to-speech translation system. IEEE Trans. Speech Audio Process., 14(2):365-376.\n\nImproving speech-to-speech translation through unlabeled text. Xuan-Phi Nguyen, Sravya Popuri, Changhan Wang, Yun Tang, Ilia Kulikov, Hongyu Gong, 10.48550/arXiv.2210.14514abs/2210.14514CoRRXuan-Phi Nguyen, Sravya Popuri, Changhan Wang, Yun Tang, Ilia Kulikov, and Hongyu Gong. 2022. Improv- ing speech-to-speech translation through unlabeled text. CoRR, abs/2210.14514.\n", "annotations": {"author": "[{\"end\":186,\"start\":137},{\"end\":241,\"start\":187},{\"end\":278,\"start\":242},{\"end\":335,\"start\":279},{\"end\":370,\"start\":336},{\"end\":413,\"start\":371},{\"end\":454,\"start\":414},{\"end\":491,\"start\":455},{\"end\":531,\"start\":492},{\"end\":590,\"start\":532}]", "publisher": "[{\"end\":99,\"start\":88},{\"end\":850,\"start\":839}]", "author_last_name": "[{\"end\":159,\"start\":151},{\"end\":198,\"start\":194},{\"end\":251,\"start\":247},{\"end\":289,\"start\":287},{\"end\":343,\"start\":340},{\"end\":386,\"start\":379},{\"end\":427,\"start\":423},{\"end\":464,\"start\":460},{\"end\":504,\"start\":499},{\"end\":546,\"start\":539}]", "author_first_name": "[{\"end\":150,\"start\":137},{\"end\":193,\"start\":187},{\"end\":246,\"start\":242},{\"end\":286,\"start\":279},{\"end\":339,\"start\":336},{\"end\":378,\"start\":371},{\"end\":422,\"start\":414},{\"end\":459,\"start\":455},{\"end\":498,\"start\":492},{\"end\":538,\"start\":532}]", "author_affiliation": "[{\"end\":185,\"start\":161},{\"end\":240,\"start\":216},{\"end\":277,\"start\":253},{\"end\":334,\"start\":310},{\"end\":369,\"start\":345},{\"end\":412,\"start\":388},{\"end\":453,\"start\":429},{\"end\":490,\"start\":466},{\"end\":530,\"start\":506},{\"end\":589,\"start\":565}]", "title": "[{\"end\":87,\"start\":1},{\"end\":677,\"start\":591}]", "venue": "[{\"end\":766,\"start\":679}]", "abstract": "[{\"end\":1917,\"start\":867}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2164,\"start\":2146},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2473,\"start\":2454},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2846,\"start\":2828},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2864,\"start\":2846},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2925,\"start\":2907},{\"end\":2973,\"start\":2954},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2993,\"start\":2973},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3096,\"start\":3077},{\"end\":3130,\"start\":3105},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3195,\"start\":3177},{\"end\":3229,\"start\":3197},{\"end\":3650,\"start\":3627},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3668,\"start\":3650},{\"end\":3836,\"start\":3816},{\"end\":4967,\"start\":4953},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5028,\"start\":5002},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5084,\"start\":5054},{\"end\":5109,\"start\":5084},{\"end\":5193,\"start\":5166},{\"end\":5211,\"start\":5193},{\"end\":5233,\"start\":5211},{\"end\":5325,\"start\":5297},{\"end\":5348,\"start\":5325},{\"end\":5395,\"start\":5372},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5416,\"start\":5395},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5852,\"start\":5829},{\"end\":5868,\"start\":5852},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6182,\"start\":6164},{\"end\":6221,\"start\":6202},{\"end\":6475,\"start\":6454},{\"end\":6803,\"start\":6784},{\"end\":7205,\"start\":7184},{\"end\":7239,\"start\":7210},{\"end\":7403,\"start\":7383},{\"end\":7645,\"start\":7625},{\"end\":8013,\"start\":7983},{\"end\":8814,\"start\":8791},{\"end\":9135,\"start\":9116},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9270,\"start\":9247},{\"end\":9637,\"start\":9619},{\"end\":9658,\"start\":9637},{\"end\":9756,\"start\":9734},{\"end\":9788,\"start\":9758},{\"end\":9823,\"start\":9793},{\"end\":10392,\"start\":10369},{\"end\":10775,\"start\":10748},{\"end\":10793,\"start\":10775},{\"end\":10816,\"start\":10793},{\"end\":12466,\"start\":12444},{\"end\":13636,\"start\":13614},{\"end\":15897,\"start\":15868},{\"end\":16069,\"start\":16068},{\"end\":16104,\"start\":16084},{\"end\":16297,\"start\":16275},{\"end\":17411,\"start\":17389},{\"end\":17480,\"start\":17460},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19141,\"start\":19122},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":20651,\"start\":20632},{\"end\":20955,\"start\":20934},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22189,\"start\":22170},{\"end\":22408,\"start\":22390},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22936,\"start\":22917},{\"end\":26582,\"start\":26563},{\"end\":27104,\"start\":27087},{\"end\":27326,\"start\":27302},{\"end\":31829,\"start\":31806},{\"end\":34696,\"start\":34673},{\"end\":41154,\"start\":41135},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":42905,\"start\":42885}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":47140,\"start\":47086},{\"attributes\":{\"id\":\"fig_4\"},\"end\":47211,\"start\":47141},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":47941,\"start\":47212},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":48005,\"start\":47942},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":48081,\"start\":48006},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":48170,\"start\":48082},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":48265,\"start\":48171},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":48329,\"start\":48266},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":48561,\"start\":48330},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":48648,\"start\":48562},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":48826,\"start\":48649},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":56582,\"start\":48827},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":56657,\"start\":56583},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":56734,\"start\":56658},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":56785,\"start\":56735},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":56841,\"start\":56786},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":56992,\"start\":56842},{\"attributes\":{\"id\":\"tab_21\",\"type\":\"table\"},\"end\":57654,\"start\":56993},{\"attributes\":{\"id\":\"tab_22\",\"type\":\"table\"},\"end\":58154,\"start\":57655},{\"attributes\":{\"id\":\"tab_23\",\"type\":\"table\"},\"end\":58710,\"start\":58155},{\"attributes\":{\"id\":\"tab_24\",\"type\":\"table\"},\"end\":59756,\"start\":58711},{\"attributes\":{\"id\":\"tab_25\",\"type\":\"table\"},\"end\":60496,\"start\":59757},{\"attributes\":{\"id\":\"tab_26\",\"type\":\"table\"},\"end\":60676,\"start\":60497},{\"attributes\":{\"id\":\"tab_27\",\"type\":\"table\"},\"end\":61488,\"start\":60677}]", "paragraph": "[{\"end\":2474,\"start\":1933},{\"end\":3669,\"start\":2476},{\"end\":4688,\"start\":3671},{\"end\":5645,\"start\":4706},{\"end\":6656,\"start\":5647},{\"end\":7881,\"start\":6658},{\"end\":8202,\"start\":7883},{\"end\":8716,\"start\":8230},{\"end\":10638,\"start\":8736},{\"end\":10939,\"start\":10672},{\"end\":12109,\"start\":11019},{\"end\":13887,\"start\":12139},{\"end\":14033,\"start\":13889},{\"end\":14639,\"start\":14035},{\"end\":15521,\"start\":14641},{\"end\":15665,\"start\":15541},{\"end\":15843,\"start\":15667},{\"end\":16638,\"start\":15845},{\"end\":17376,\"start\":16640},{\"end\":17886,\"start\":17378},{\"end\":18373,\"start\":17888},{\"end\":18632,\"start\":18399},{\"end\":19321,\"start\":18697},{\"end\":19861,\"start\":19323},{\"end\":20253,\"start\":19863},{\"end\":20842,\"start\":20255},{\"end\":21061,\"start\":20844},{\"end\":21196,\"start\":21063},{\"end\":21391,\"start\":21198},{\"end\":21667,\"start\":21393},{\"end\":21738,\"start\":21677},{\"end\":21931,\"start\":21740},{\"end\":22270,\"start\":21972},{\"end\":22568,\"start\":22272},{\"end\":22830,\"start\":22570},{\"end\":23518,\"start\":22832},{\"end\":24073,\"start\":23555},{\"end\":24507,\"start\":24075},{\"end\":25164,\"start\":24509},{\"end\":25622,\"start\":25210},{\"end\":26014,\"start\":25624},{\"end\":26792,\"start\":26016},{\"end\":27028,\"start\":26794},{\"end\":27461,\"start\":27030},{\"end\":27731,\"start\":27495},{\"end\":28128,\"start\":27733},{\"end\":28680,\"start\":28130},{\"end\":28970,\"start\":28711},{\"end\":29324,\"start\":28972},{\"end\":29721,\"start\":29326},{\"end\":30369,\"start\":29747},{\"end\":30621,\"start\":30371},{\"end\":30915,\"start\":30623},{\"end\":31149,\"start\":30917},{\"end\":31690,\"start\":31164},{\"end\":32161,\"start\":31745},{\"end\":32278,\"start\":32206},{\"end\":33808,\"start\":32310},{\"end\":34370,\"start\":33810},{\"end\":34613,\"start\":34389},{\"end\":35314,\"start\":34629},{\"end\":36037,\"start\":35316},{\"end\":36052,\"start\":36039},{\"end\":36892,\"start\":36069},{\"end\":37009,\"start\":36894},{\"end\":37258,\"start\":37011},{\"end\":37740,\"start\":37260},{\"end\":37825,\"start\":37742},{\"end\":37969,\"start\":37873},{\"end\":38098,\"start\":38007},{\"end\":38927,\"start\":38100},{\"end\":39278,\"start\":38929},{\"end\":39922,\"start\":39280},{\"end\":40386,\"start\":39924},{\"end\":40926,\"start\":40388},{\"end\":41296,\"start\":40928},{\"end\":41539,\"start\":41298},{\"end\":41988,\"start\":41574},{\"end\":42432,\"start\":41990},{\"end\":42988,\"start\":42434},{\"end\":43484,\"start\":42990},{\"end\":43647,\"start\":43486},{\"end\":43727,\"start\":43721},{\"end\":43984,\"start\":43771},{\"end\":44054,\"start\":44031},{\"end\":44160,\"start\":44109},{\"end\":44215,\"start\":44162},{\"end\":44273,\"start\":44217},{\"end\":44346,\"start\":44275},{\"end\":44429,\"start\":44348},{\"end\":44498,\"start\":44431},{\"end\":44511,\"start\":44500},{\"end\":44574,\"start\":44561},{\"end\":45208,\"start\":44631},{\"end\":45298,\"start\":45210},{\"end\":45350,\"start\":45300},{\"end\":45799,\"start\":45352},{\"end\":46193,\"start\":45801},{\"end\":46411,\"start\":46195},{\"end\":46951,\"start\":46413},{\"end\":47085,\"start\":46953}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11018,\"start\":10940},{\"attributes\":{\"id\":\"formula_1\"},\"end\":18675,\"start\":18633}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":8201,\"start\":8194},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":11238,\"start\":11231},{\"end\":14723,\"start\":14716},{\"end\":15436,\"start\":15429},{\"end\":16305,\"start\":16298},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":23047,\"start\":23040},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":23755,\"start\":23748},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":23767,\"start\":23760},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":23861,\"start\":23854},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":24015,\"start\":24008},{\"end\":24302,\"start\":24295},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":27839,\"start\":27832},{\"attributes\":{\"ref_id\":\"tab_15\"},\"end\":31840,\"start\":31833},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":32106,\"start\":32098},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":32533,\"start\":32525},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":34612,\"start\":34604},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":35578,\"start\":35570},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":40553,\"start\":40545}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1931,\"start\":1919},{\"attributes\":{\"n\":\"2\"},\"end\":4704,\"start\":4691},{\"attributes\":{\"n\":\"3\"},\"end\":8228,\"start\":8205},{\"attributes\":{\"n\":\"3.1\"},\"end\":8734,\"start\":8719},{\"attributes\":{\"n\":\"3.2\"},\"end\":10670,\"start\":10641},{\"attributes\":{\"n\":\"3.3\"},\"end\":12137,\"start\":12112},{\"attributes\":{\"n\":\"3.4\"},\"end\":15539,\"start\":15524},{\"attributes\":{\"n\":\"4\"},\"end\":18397,\"start\":18376},{\"attributes\":{\"n\":\"4.1\"},\"end\":18695,\"start\":18677},{\"attributes\":{\"n\":\"6\"},\"end\":21675,\"start\":21670},{\"attributes\":{\"n\":\"5\"},\"end\":21970,\"start\":21934},{\"attributes\":{\"n\":\"5.1\"},\"end\":23553,\"start\":23521},{\"attributes\":{\"n\":\"6\"},\"end\":25208,\"start\":25167},{\"attributes\":{\"n\":\"6.1\"},\"end\":27493,\"start\":27464},{\"attributes\":{\"n\":\"6.2\"},\"end\":28709,\"start\":28683},{\"attributes\":{\"n\":\"7\"},\"end\":29745,\"start\":29724},{\"attributes\":{\"n\":\"8\"},\"end\":31162,\"start\":31152},{\"end\":31709,\"start\":31693},{\"end\":31743,\"start\":31712},{\"end\":32204,\"start\":32164},{\"end\":32308,\"start\":32281},{\"end\":34387,\"start\":34373},{\"end\":34627,\"start\":34616},{\"end\":36067,\"start\":36055},{\"end\":37871,\"start\":37828},{\"end\":38005,\"start\":37972},{\"end\":41572,\"start\":41542},{\"end\":43719,\"start\":43650},{\"end\":43769,\"start\":43730},{\"attributes\":{\"n\":\"17\"},\"end\":43992,\"start\":43987},{\"end\":44029,\"start\":43995},{\"end\":44107,\"start\":44057},{\"end\":44559,\"start\":44514},{\"end\":44629,\"start\":44577},{\"end\":47097,\"start\":47087},{\"end\":47152,\"start\":47142},{\"end\":47952,\"start\":47943},{\"end\":48016,\"start\":48007},{\"end\":48092,\"start\":48083},{\"end\":48181,\"start\":48172},{\"end\":48276,\"start\":48267},{\"end\":48572,\"start\":48563},{\"end\":48659,\"start\":48650},{\"end\":56593,\"start\":56584},{\"end\":56669,\"start\":56659},{\"end\":56746,\"start\":56736},{\"end\":56797,\"start\":56787},{\"end\":56853,\"start\":56843},{\"end\":57004,\"start\":56994},{\"end\":58166,\"start\":58156},{\"end\":58722,\"start\":58712},{\"end\":60508,\"start\":60498}]", "table": "[{\"end\":47941,\"start\":47427},{\"end\":48170,\"start\":48168},{\"end\":48561,\"start\":48386},{\"end\":56582,\"start\":49169},{\"end\":56657,\"start\":56595},{\"end\":56734,\"start\":56672},{\"end\":56992,\"start\":56929},{\"end\":57654,\"start\":57063},{\"end\":58154,\"start\":57698},{\"end\":58710,\"start\":58345},{\"end\":60496,\"start\":59824}]", "figure_caption": "[{\"end\":47140,\"start\":47099},{\"end\":47211,\"start\":47154},{\"end\":47427,\"start\":47214},{\"end\":48005,\"start\":47954},{\"end\":48081,\"start\":48018},{\"end\":48168,\"start\":48094},{\"end\":48265,\"start\":48183},{\"end\":48329,\"start\":48278},{\"end\":48386,\"start\":48332},{\"end\":48648,\"start\":48574},{\"end\":48826,\"start\":48661},{\"end\":49169,\"start\":48829},{\"end\":56785,\"start\":56749},{\"end\":56841,\"start\":56800},{\"end\":56929,\"start\":56856},{\"end\":57063,\"start\":57007},{\"end\":57698,\"start\":57657},{\"end\":58345,\"start\":58169},{\"end\":59756,\"start\":58725},{\"end\":59824,\"start\":59759},{\"end\":60676,\"start\":60511},{\"end\":61488,\"start\":60679}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9493,\"start\":9485},{\"end\":18785,\"start\":18777},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":37184,\"start\":37176}]", "bib_author_first_name": "[{\"end\":63791,\"start\":63786},{\"end\":63810,\"start\":63804},{\"end\":64026,\"start\":64024},{\"end\":64040,\"start\":64032},{\"end\":64047,\"start\":64041},{\"end\":64064,\"start\":64060},{\"end\":64076,\"start\":64071},{\"end\":64334,\"start\":64332},{\"end\":64343,\"start\":64340},{\"end\":64345,\"start\":64344},{\"end\":64357,\"start\":64353},{\"end\":64374,\"start\":64366},{\"end\":64391,\"start\":64385},{\"end\":64408,\"start\":64401},{\"end\":64422,\"start\":64415},{\"end\":64979,\"start\":64973},{\"end\":64996,\"start\":64989},{\"end\":65011,\"start\":65006},{\"end\":65295,\"start\":65292},{\"end\":65309,\"start\":65301},{\"end\":65324,\"start\":65316},{\"end\":65337,\"start\":65331},{\"end\":65348,\"start\":65342},{\"end\":65362,\"start\":65357},{\"end\":65371,\"start\":65367},{\"end\":65385,\"start\":65380},{\"end\":65395,\"start\":65391},{\"end\":65403,\"start\":65400},{\"end\":65414,\"start\":65410},{\"end\":65429,\"start\":65421},{\"end\":66152,\"start\":66149},{\"end\":66164,\"start\":66158},{\"end\":66177,\"start\":66171},{\"end\":66210,\"start\":66202},{\"end\":66228,\"start\":66220},{\"end\":66241,\"start\":66235},{\"end\":66253,\"start\":66248},{\"end\":66266,\"start\":66262},{\"end\":66278,\"start\":66272},{\"end\":66293,\"start\":66285},{\"end\":67236,\"start\":67230},{\"end\":67257,\"start\":67247},{\"end\":67272,\"start\":67263},{\"end\":67282,\"start\":67277},{\"end\":67294,\"start\":67289},{\"end\":67309,\"start\":67302},{\"end\":67322,\"start\":67317},{\"end\":67335,\"start\":67331},{\"end\":67352,\"start\":67345},{\"end\":67842,\"start\":67838},{\"end\":67856,\"start\":67850},{\"end\":67869,\"start\":67866},{\"end\":67885,\"start\":67880},{\"end\":67897,\"start\":67893},{\"end\":68251,\"start\":68245},{\"end\":68263,\"start\":68257},{\"end\":68273,\"start\":68268},{\"end\":68285,\"start\":68281},{\"end\":68296,\"start\":68290},{\"end\":68311,\"start\":68305},{\"end\":68331,\"start\":68327},{\"end\":68343,\"start\":68339},{\"end\":68775,\"start\":68769},{\"end\":68790,\"start\":68784},{\"end\":69077,\"start\":69070},{\"end\":69098,\"start\":69088},{\"end\":69113,\"start\":69107},{\"end\":69133,\"start\":69123},{\"end\":69148,\"start\":69141},{\"end\":69165,\"start\":69156},{\"end\":69184,\"start\":69177},{\"end\":69200,\"start\":69192},{\"end\":69219,\"start\":69211},{\"end\":69235,\"start\":69228},{\"end\":69649,\"start\":69641},{\"end\":69664,\"start\":69658},{\"end\":69681,\"start\":69673},{\"end\":69691,\"start\":69688},{\"end\":69702,\"start\":69698},{\"end\":69718,\"start\":69712}]", "bib_author_last_name": "[{\"end\":63802,\"start\":63792},{\"end\":63818,\"start\":63811},{\"end\":64030,\"start\":64027},{\"end\":64058,\"start\":64048},{\"end\":64069,\"start\":64065},{\"end\":64080,\"start\":64077},{\"end\":64338,\"start\":64335},{\"end\":64351,\"start\":64346},{\"end\":64364,\"start\":64358},{\"end\":64383,\"start\":64375},{\"end\":64399,\"start\":64392},{\"end\":64413,\"start\":64409},{\"end\":64425,\"start\":64423},{\"end\":64987,\"start\":64980},{\"end\":65004,\"start\":64997},{\"end\":65017,\"start\":65012},{\"end\":65299,\"start\":65296},{\"end\":65314,\"start\":65310},{\"end\":65329,\"start\":65325},{\"end\":65340,\"start\":65338},{\"end\":65355,\"start\":65349},{\"end\":65365,\"start\":65363},{\"end\":65378,\"start\":65372},{\"end\":65389,\"start\":65386},{\"end\":65398,\"start\":65396},{\"end\":65408,\"start\":65404},{\"end\":65419,\"start\":65415},{\"end\":65433,\"start\":65430},{\"end\":66156,\"start\":66153},{\"end\":66169,\"start\":66165},{\"end\":66200,\"start\":66178},{\"end\":66218,\"start\":66211},{\"end\":66233,\"start\":66229},{\"end\":66246,\"start\":66242},{\"end\":66260,\"start\":66254},{\"end\":66270,\"start\":66267},{\"end\":66283,\"start\":66279},{\"end\":66296,\"start\":66294},{\"end\":66301,\"start\":66298},{\"end\":67245,\"start\":67237},{\"end\":67261,\"start\":67258},{\"end\":67275,\"start\":67273},{\"end\":67287,\"start\":67283},{\"end\":67300,\"start\":67295},{\"end\":67315,\"start\":67310},{\"end\":67329,\"start\":67323},{\"end\":67343,\"start\":67336},{\"end\":67357,\"start\":67353},{\"end\":67848,\"start\":67843},{\"end\":67864,\"start\":67857},{\"end\":67878,\"start\":67870},{\"end\":67891,\"start\":67886},{\"end\":67909,\"start\":67898},{\"end\":68255,\"start\":68252},{\"end\":68266,\"start\":68264},{\"end\":68279,\"start\":68274},{\"end\":68288,\"start\":68286},{\"end\":68303,\"start\":68297},{\"end\":68325,\"start\":68312},{\"end\":68337,\"start\":68332},{\"end\":68355,\"start\":68344},{\"end\":68782,\"start\":68776},{\"end\":68799,\"start\":68791},{\"end\":68806,\"start\":68801},{\"end\":69086,\"start\":69078},{\"end\":69105,\"start\":69099},{\"end\":69121,\"start\":69114},{\"end\":69139,\"start\":69134},{\"end\":69154,\"start\":69149},{\"end\":69175,\"start\":69166},{\"end\":69190,\"start\":69185},{\"end\":69209,\"start\":69201},{\"end\":69226,\"start\":69220},{\"end\":69244,\"start\":69236},{\"end\":69656,\"start\":69650},{\"end\":69671,\"start\":69665},{\"end\":69686,\"start\":69682},{\"end\":69696,\"start\":69692},{\"end\":69710,\"start\":69703},{\"end\":69723,\"start\":69719}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":8731445},\"end\":63953,\"start\":63725},{\"attributes\":{\"doi\":\"abs/2201.03713\",\"id\":\"b1\"},\"end\":64259,\"start\":63955},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":118641713},\"end\":64875,\"start\":64261},{\"attributes\":{\"doi\":\"arXiv:2205.08180\",\"id\":\"b3\"},\"end\":65233,\"start\":64877},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":235794968},\"end\":66095,\"start\":65235},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":245144752},\"end\":67146,\"start\":66097},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":220265858},\"end\":67777,\"start\":67148},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b7\",\"matched_paper_id\":232428341},\"end\":68176,\"start\":67779},{\"attributes\":{\"id\":\"b8\"},\"end\":68689,\"start\":68178},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":15289038},\"end\":69010,\"start\":68691},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1317435},\"end\":69576,\"start\":69012},{\"attributes\":{\"doi\":\"10.48550/arXiv.2210.14514\",\"id\":\"b11\"},\"end\":69948,\"start\":69578}]", "bib_title": "[{\"end\":63784,\"start\":63725},{\"end\":64330,\"start\":64261},{\"end\":65290,\"start\":65235},{\"end\":66147,\"start\":66097},{\"end\":67228,\"start\":67148},{\"end\":67836,\"start\":67779},{\"end\":68243,\"start\":68178},{\"end\":68767,\"start\":68691},{\"end\":69068,\"start\":69012}]", "bib_author": "[{\"end\":63804,\"start\":63786},{\"end\":63820,\"start\":63804},{\"end\":64032,\"start\":64024},{\"end\":64060,\"start\":64032},{\"end\":64071,\"start\":64060},{\"end\":64082,\"start\":64071},{\"end\":64340,\"start\":64332},{\"end\":64353,\"start\":64340},{\"end\":64366,\"start\":64353},{\"end\":64385,\"start\":64366},{\"end\":64401,\"start\":64385},{\"end\":64415,\"start\":64401},{\"end\":64427,\"start\":64415},{\"end\":64989,\"start\":64973},{\"end\":65006,\"start\":64989},{\"end\":65019,\"start\":65006},{\"end\":65301,\"start\":65292},{\"end\":65316,\"start\":65301},{\"end\":65331,\"start\":65316},{\"end\":65342,\"start\":65331},{\"end\":65357,\"start\":65342},{\"end\":65367,\"start\":65357},{\"end\":65380,\"start\":65367},{\"end\":65391,\"start\":65380},{\"end\":65400,\"start\":65391},{\"end\":65410,\"start\":65400},{\"end\":65421,\"start\":65410},{\"end\":65435,\"start\":65421},{\"end\":66158,\"start\":66149},{\"end\":66171,\"start\":66158},{\"end\":66202,\"start\":66171},{\"end\":66220,\"start\":66202},{\"end\":66235,\"start\":66220},{\"end\":66248,\"start\":66235},{\"end\":66262,\"start\":66248},{\"end\":66272,\"start\":66262},{\"end\":66285,\"start\":66272},{\"end\":66298,\"start\":66285},{\"end\":66303,\"start\":66298},{\"end\":67247,\"start\":67230},{\"end\":67263,\"start\":67247},{\"end\":67277,\"start\":67263},{\"end\":67289,\"start\":67277},{\"end\":67302,\"start\":67289},{\"end\":67317,\"start\":67302},{\"end\":67331,\"start\":67317},{\"end\":67345,\"start\":67331},{\"end\":67359,\"start\":67345},{\"end\":67850,\"start\":67838},{\"end\":67866,\"start\":67850},{\"end\":67880,\"start\":67866},{\"end\":67893,\"start\":67880},{\"end\":67911,\"start\":67893},{\"end\":68257,\"start\":68245},{\"end\":68268,\"start\":68257},{\"end\":68281,\"start\":68268},{\"end\":68290,\"start\":68281},{\"end\":68305,\"start\":68290},{\"end\":68327,\"start\":68305},{\"end\":68339,\"start\":68327},{\"end\":68357,\"start\":68339},{\"end\":68784,\"start\":68769},{\"end\":68801,\"start\":68784},{\"end\":68808,\"start\":68801},{\"end\":69088,\"start\":69070},{\"end\":69107,\"start\":69088},{\"end\":69123,\"start\":69107},{\"end\":69141,\"start\":69123},{\"end\":69156,\"start\":69141},{\"end\":69177,\"start\":69156},{\"end\":69192,\"start\":69177},{\"end\":69211,\"start\":69192},{\"end\":69228,\"start\":69211},{\"end\":69246,\"start\":69228},{\"end\":69658,\"start\":69641},{\"end\":69673,\"start\":69658},{\"end\":69688,\"start\":69673},{\"end\":69698,\"start\":69688},{\"end\":69712,\"start\":69698},{\"end\":69725,\"start\":69712}]", "bib_venue": "[{\"end\":63824,\"start\":63820},{\"end\":64022,\"start\":63955},{\"end\":64521,\"start\":64427},{\"end\":64971,\"start\":64877},{\"end\":65522,\"start\":65435},{\"end\":66457,\"start\":66303},{\"end\":67441,\"start\":67359},{\"end\":67959,\"start\":67915},{\"end\":68418,\"start\":68357},{\"end\":68833,\"start\":68808},{\"end\":69278,\"start\":69246},{\"end\":69639,\"start\":69578},{\"end\":64536,\"start\":64523},{\"end\":65611,\"start\":65524},{\"end\":66624,\"start\":66459},{\"end\":67450,\"start\":67443}]"}}}, "year": 2023, "month": 12, "day": 17}