{"id": 227209335, "updated": "2023-10-06 08:23:58.699", "metadata": {"title": "Score-Based Generative Modeling through Stochastic Differential Equations", "authors": "[{\"first\":\"Yang\",\"last\":\"Song\",\"middle\":[]},{\"first\":\"Jascha\",\"last\":\"Sohl-Dickstein\",\"middle\":[]},{\"first\":\"Diederik\",\"last\":\"Kingma\",\"middle\":[\"P.\"]},{\"first\":\"Abhishek\",\"last\":\"Kumar\",\"middle\":[]},{\"first\":\"Stefano\",\"last\":\"Ermon\",\"middle\":[]},{\"first\":\"Ben\",\"last\":\"Poole\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 11, "day": 26}, "abstract": "Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in diffusion probabilistic modeling and score-based generative modeling, and allows for new sampling procedures. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, which enables exact likelihood computation, and improved sampling efficiency. In addition, our framework enables conditional generation with an unconditional model, as we demonstrate with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 3.10 bits/dim, and demonstrate high fidelity generation of $1024 \\times 1024$ images for the first time from a score-based generative model.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2011.13456", "mag": "3110257065", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iclr/0011SKKEP21", "doi": null}}, "content": {"source": {"pdf_hash": "633e2fbfc0b21e959a244100937c5853afca4853", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2011.13456v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "787f734969c2e9e7d6f7007da77a30cd8cc64168", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/633e2fbfc0b21e959a244100937c5853afca4853.txt", "contents": "\nSCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS\n\n\nYang Songs yangsong@cs.stanford.edu \ntanford University\nStanford University\n\n\nJascha Sohl-Dickstein jaschasd@google.com \ntanford University\nStanford University\n\n\nGoogle Brain \ntanford University\nStanford University\n\n\nDiederik P Kingma \ntanford University\nStanford University\n\n\nGoogle Brain \ntanford University\nStanford University\n\n\nAbhishek Kumar \ntanford University\nStanford University\n\n\nGoogle Brain \ntanford University\nStanford University\n\n\nStefano Ermon ermon@cs.stanford.edu \ntanford University\nStanford University\n\n\nBen Poole pooleb@google.com \ntanford University\nStanford University\n\n\nGoogle Brain \ntanford University\nStanford University\n\n\nSCORE-BASED GENERATIVE MODELING THROUGH STOCHASTIC DIFFERENTIAL EQUATIONS\nPreprint. Work in progress.\nCreating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reversetime SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in diffusion probabilistic modeling and scorebased generative modeling, and allows for new sampling procedures. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, which enables exact likelihood computation, and improved sampling efficiency. In addition, our framework enables conditional generation with an unconditional model, as we demonstrate with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 3.10 bits/dim, and demonstrate high fidelity generation of 1024\u02c61024 images for the first time from a score-based generative model.\n\nINTRODUCTION\n\nTwo successful classes of probabilistic generative models involve sequentially corrupting training data with slowly increasing noise, and then learning to reverse this corruption in order to form a generative model of the data. Score matching with Langevin dynamics (SMLD)  estimates the score (i.e., the gradient of the log probability density) at each noise scale, and then uses Langevin dynamics to sample from a sequence of decreasing noise scales during generation. Denoising diffusion probabilistic modeling (DDPM) (Sohl-Dickstein et al., 2015;Ho et al., 2020) trains a sequence of probabilistic models to reverse each step of the noise corruption, using knowledge of the functional form of the reverse distributions to make training tractable. For continuous state spaces, the DDPM training objective implicitly computes scores at each noise scale. We therefore refer to these two model classes together as score-based generative models.\n\nScore-based generative models, and related techniques (Bordes et al., 2017;Goyal et al., 2017), have proven effective at generation of images Ho et al., 2020), audio (Chen et al., 2020;Kong et al., 2020), graphs (Niu et al., 2020), and shapes (Cai et al., 2020). However, the Work done during an internship at Google Brain. \u2192 data) score function Figure 1: Solving a reversetime SDE yields a score-based generative model. Transforming data to a simple noise distribution can be accomplished with a continuous-time SDE. This SDE can be reversed if we know the score of the distribution at each intermediate time step, \u2207 x log p t pxq.\n\npractical performance of these two model classes is often quite different for reasons that are not fully understood. More generally, despite obvious surface similarities, the relationship between SMLD and DDPM approaches is largely unexplored.\n\nWe address these open questions by unifying and generalizing both approaches through the lens of stochastic differential equations (SDEs). Instead of perturbing data with a finite number of noise distributions, we consider a continuum of distributions that evolve over time according to a diffusion process. This process progressively diffuses a data point into random noise, and is given by a prescribed SDE that does not depend on the data and has no trainable parameters. By reversing this process, we can smoothly mold random noise into data for sample generation. Crucially, this reverse process satisfies a reverse-time SDE (Anderson, 1982), which can be derived from the forward SDE given the score of the marginal probability densities as a function of time. We can therefore approximate the reverse-time SDE by training a time-dependent neural network to estimate the scores, and then produce samples using numerical SDE solvers. Our key idea is summarized in Fig. 1.\n\nOur proposed framework has several theoretical and practical contributions:\n\nFlexible sampling: We can employ any general-purpose SDE solver to integrate the reverse-time SDE for sampling. In addition, we propose two special methods not viable for general SDEs: (i) Predictor-Corrector (PC) samplers that combine numerical SDE solvers with score-based MCMC approaches, such as Langevin MCMC (Parisi, 1981;Grenander & Miller, 1994); and (ii) deterministic samplers based on the probability flow ordinary differential equation (ODE). The former unifies and improves over existing sampling methods for score-based models. The latter allows for exact likelihood computation, efficient and adaptive sampling via black-box ODE solvers, flexible data manipulation via latent codes, and a unique encoding.\n\nControllable generation: We can modulate the generation process by conditioning on information not available during training, because the conditional reverse-time SDE can be efficiently computed from unconditional scores. This enables applications such as class-conditional generation, image inpainting, and colorization using a single unconditional score-based model without re-training.\n\nUnified picture: The methods of SMLD and DDPM can be unified into our framework as discretizations of different SDEs. Although Ho et al. (2020) has reported higher sample quality than , we show that with better architectures and our new sampling algorithms, the latter can achieve new state-of-the-art Inception and FID scores on CIFAR-10, as well as high-fidelity generation of 1024\u02c61024 samples. This indicates that either SDE could be advantageous, and should be tuned jointly with other design choices for score-based generative modeling.\n\n\nBACKGROUND\n\n\nDENOISING SCORE MATCHING WITH LANGEVIN DYNAMICS (SMLD)\n\nLet p \u03c3 px | xq fi N px; x, \u03c3 2 Iq be a perturbation kernel, and p \u03c3 pxq fi \u015f p data pxqp \u03c3 px | xqdx, where p data pxq denotes the data distribution. Consider a sequence of positive noise scales \u03c3 min \" \u03c3 1 \u0103 \u03c3 2 \u0103\u00a8\u00a8\u00a8\u0103 \u03c3 N \" \u03c3 max . Typically, \u03c3 min is small enough such that p \u03c3min pxq \u00ab p data pxq, and \u03c3 max is large enough such that p \u03c3max pxq \u00ab N px; 0, \u03c3 2 max Iq.  propose to train a Noise Conditional Score Network (NCSN), denoted by s \u03b8 px, \u03c3q, with a weighted sum of denoising score matching (Vincent, 2011) objectives:\n\n\u03b8\u02da\" arg min \u03b8 N \u00ff i\"1 \u03c3 2 i E p data pxq E p\u03c3 i px|xq \" s \u03b8 px, \u03c3 i q\u00b4\u2207x log p \u03c3i px | xq 2 2 \u2030 .\n\n(1)\n\nGiven sufficient data and model capacity, the optimal score-based model s \u03b8\u02dap x, \u03c3q matches \u2207 x log p \u03c3 pxq almost everywhere for \u03c3 P t\u03c3 i u N i\"1 . For sampling,  run M steps of Langevin MCMC to get a sample for each p \u03c3i pxq sequentially:\nx m i \" x m\u00b41 i` i s \u03b8\u02dap x m\u00b41 i , \u03c3 i q`?2 i z m i , m \" 1, 2,\u00a8\u00a8\u00a8, M,(2)\nwhere i \u0105 0 is the step size, and z m i is standard normal. The above is repeated for i \" N, N1 ,\u00a8\u00a8\u00a8, 1 in turn with x 0 N \" N px | 0, \u03c3 2 max Iq and x 0 i \" x M i`1 when i \u0103 N . As M \u00d1 8 and i \u00d1 0 for all i, x N 1 becomes an exact sample from p \u03c3min pxq \u00ab p data pxq under some regularity conditions.\n\n\nDENOISING DIFFUSION PROBABILISTIC MODELS (DDPM)\n\nSohl-Dickstein et al. (2015); Ho et al. (2020) consider a sequence of positive noise scales 0 \u0103 \u03b2 1 , \u03b2 2 ,\u00a8\u00a8\u00a8, \u03b2 N \u0103 1. For each training data point x 0 \" p data pxq, a discrete Markov chain\ntx 0 , x 1 ,\u00a8\u00a8\u00a8, x N u is constructed such that ppx i | x i\u00b41 q \" N px i ; ? 1\u00b4\u03b2 i x i\u00b41 , \u03b2 i Iq, and therefore p \u03b1i px i | x 0 q \" N px i ; ? \u03b1 i x 0 , p1\u00b4\u03b1 i qIq, where \u03b1 i fi \u015b i j\"1 p1\u00b4\u03b2 j q.\nSimilar to SMLD, we can denote the perturbed data distribution as p \u03b1i pxq fi \u015f p data pxqp \u03b1i px | xqdx. The noise scales are prescribed such that x N is an approximate sample from N p0, Iq. A variational Markov chain in the reverse direction is parameterized with p \u03b8 px i\u00b41 |x i q \" N px i\u00b41 ; 1 ? 1\u00b4\u03b2i px i`\u03b2i s \u03b8 px i , iqq, \u03b2 i Iq, and trained with a re-weighted variant of the evidence lower bound (ELBO):\n\u03b8\u02da\" arg min \u03b8 N \u00ff i\"1 p1\u00b4\u03b1 i qE p data pxq E p\u03b1 i px|xq r s \u03b8 px, iq\u00b4\u2207x log p \u03b1i px | xq 2 2 s.(3)\nAfter solving Eq. (3) to get the optimal model s \u03b8\u02dap x, iq, samples can be generated by starting from x N \" N p0, Iq and following the estimated reverse Markov chain as below\nx i\u00b41 \" 1 ? 1\u00b4\u03b2 i px i`\u03b2i s \u03b8\u02dap x i , iqq`a\u03b2 i z i , i \" N, N\u00b41,\u00a8\u00a8\u00a8, 1.(4)\nWe call this method ancestral sampling, since it amounts to performing ancestral sampling from the graphical model \u015b N i\"1 p \u03b8 px i\u00b41 | x i q. The objective Eq. (3) described here is equivalent to L simple in Ho et al. (2020), but we re-write it in a slightly different form to expose more similarity to Eq. (1). Like Eq. (1), Eq. (3) is also a weighted sum of denoising score matching objectives, which implies that the optimal model, s \u03b8\u02dapx , iq, matches the score of the perturbed data distribution, \u2207 x log p \u03b1i pxq. Moreover, we observe that the weights of the i-th summand in Eq. (1) and Eq. (3), namely \u03c3 2 i and p1\u00b4\u03b1 i q, are related to corresponding perturbation kernels in the same functional form: \u03c3 2 i 91{Er \u2207 x log p \u03c3i px | xq 2 2 s and p1\u00b4\u03b1 i q91{Er \u2207 x log p \u03b1i px | xq 2 2 s.\n\n\nSCORE-BASED GENERATIVE MODELING WITH SDES\n\nPerturbing data with multiple noise scales is key to the success of previous methods. We propose to generalize this idea further to an infinite number of noise scales, such that perturbed data distributions evolve according to an SDE as the noise intensifies. An overview of our framework is given in Fig. 2.\n\n\nPERTURBING DATA WITH SDES\n\nOur goal is to construct a diffusion process txptqu T t\"0 indexed by a continuous time variable t P r0, T s, such that xp0q \" p 0 , for which we have a dataset of i.i.d. samples, and xpT q \" p T , for which we have a tractable form to generate samples efficiently. In other words, p 0 is the data distribution and p T is the prior distribution. This diffusion process can be modeled as the solution to an It\u00f4 SDE:\ndx \" f px, tqdt`gptqdw,(5)\nForward SDE Data Prior Data Reverse SDE Figure 2: Overview of score-based generative modeling through SDEs. We can map data to a noise distribution (the prior) with an SDE (Section 3.1), and reverse this SDE for generative modeling (Section 3.2). We can also reverse the associated probability flow ODE (Section 4.3), which yields a deterministic process that samples from the same distribution as the SDE. Both the reverse-time SDE and probability flow ODE can be obtained by estimating the score \u2207 x log p t pxq (Section 3.3).\n\nwhere w is the standard Wiener process (a.k.a., Brownian motion), f p\u00a8, tq : R d \u00d1 R d is a vectorvalued function called the drift coefficient of xptq, and gp\u00a8q : R \u00d1 R is a scalar function known as the diffusion coefficient of xptq. For ease of presentation we assume the diffusion coefficient is a scalar (instead of a d\u02c6d matrix) and does not depend on x, but our theory can be generalized to hold in those cases (see Appendix A). The SDE has a unique strong solution when the coefficients are globally Lipschitz in both state and time (\u00d8ksendal, 2003). We hereafter denote by p t pxq the probability density of xptq, and use p st pxptq | xpsqq to denote the transition kernel from xpsq to xptq, where 0 \u010f s \u0103 t \u010f T .\n\nTypically, p T is an unstructured prior distribution that contains no information of p 0 , such as a Gaussian distribution with simple mean and variance. There are various ways of designing the SDE in Eq. (5) such that it diffuses the data distribution into a fixed prior distribution. We provide two examples later in Section 3.4.\n\n\nGENERATING SAMPLES BY REVERSING THE SDE\n\nBy starting from samples of xpT q \" p T and reversing the process, we can obtain samples xp0q \" p 0 . A remarkable result from Anderson (1982) states that the reverse of a diffusion process is also a diffusion process, running backwards in time and given by the reverse-time SDE:\n\ndx \" rf px, tq\u00b4gptq 2 \u2207 x log p t pxqsdt`gptqdw,\n\nwherew is a standard Wiener process when time flows backwards from T to 0, and dt is an infinitesimal negative timestep. Once the score of each marginal distribution, \u2207 x log p t pxq, is known for all t, we can derive the reverse diffusion process from Eq. (6) and simulate it to sample from p 0 .\n\n\nESTIMATING SCORES FOR THE SDE\n\nThe score of a distribution can be estimated by training a score-based model on samples with score matching (Hyv\u00e4rinen, 2005;Song et al., 2019a). To estimate \u2207 x log p t pxq, we can train a time-dependent score-based model s \u03b8 px, tq via a continuous generalization to Eqs. (1) and (3):\n\u03b8\u02da\" arg min \u03b8 E t ! \u03bbptqE xp0q E xptq \" s \u03b8 pxptq, tq\u00b4\u2207 xptq log p 0t pxptq | xp0qq 2 2 \u2030 ) .(7)\nHere \u03bb : r0, T s \u00d1 R`is a positive weighting function, t is sampled from a uniform distribution over r0, T s, xp0q \" p 0 pxq and xptq \" p 0t pxptq | xp0qq. With sufficient data and model capacity, score matching ensures that the optimal solution to Eq. (7), denoted by s \u03b8\u02dap x, tq, equals \u2207 x log p t pxq for almost all x and t. As in SMLD and DDPM, we can typically choose \u03bb91{E \" \u2207 xptq log p 0t pxptq | xp0qq 2 2 \u2030 . Note that Eq. (7) uses denoising score matching, but other score matching objectives, such as sliced score matching (Song et al., 2019a) and finite-difference score matching (Pang et al., 2020) are also applicable here.\n\nWe need to know the transition kernel p 0t pxptq | xp0qq to efficiently solve Eq. (7). When f p\u00a8, tq is affine, the transition kernel is always a Gaussian distribution, where the mean and variance are often known in closed-forms and can be obtained with standard techniques (see Section 5.5 in S\u00e4rkk\u00e4 & Solin (2019)). For more general SDEs, we may solve Kolmogorov's forward equation (\u00d8ksendal, 2003) to obtain p 0t pxptq | xp0qq. Alternatively, we can simulate the SDE to sample from p 0t pxptq | xp0qq and replace denoising score matching in Eq. (7) with sliced score matching for model training, which does not require computing \u2207 xptq log p 0t pxptq | xp0qq (see Appendix A).\n\n\nSPECIAL CASES: VE AND VP SDES\n\nThe noise perturbations used in SMLD and DDPM can be regarded as discretizations of two different SDEs, which we name as Variance Exploding (VE) and Variance Preserving (VP) SDEs respectively. Below we provide a brief discussion and relegate more details to Appendix B.\n\nWhen using a total of N noise scales, each perturbation kernel p \u03c3i px | x 0 q of SMLD corresponds to the distribution of x i in the following Markov chain:\nx i \" x i\u00b41`b \u03c3 2 i\u00b4\u03c3 2 i\u00b41 z i\u00b41 , i \" 1,\u00a8\u00a8\u00a8, N,(8)\nwhere z i\u00b41 \" N p0, Iq, and we have introduced \u03c3 0 \" 0 to simplify the notation. In the limit of N \u00d1 8, t\u03c3 i u N i\"1 becomes a function \u03c3ptq, z i becomes zptq, and the Markov chain tx i u N i\"1 becomes a continuous stochastic process txptqu 1 t\"0 , where we have used a continuous time variable t P r0, 1s for indexing, rather than an integer i. The process txptqu 1 t\"0 is given by the following SDE dx \"\nc d r\u03c3 2 ptqs dt dw.(9)\nLikewise for the perturbation kernels tp \u03b1i px | x 0 qu N i\"1 of DDPM, the discrete Markov chain is\nx i \" a 1\u00b4\u03b2 i x i\u00b41`a \u03b2 i z i\u00b41 , i \" 1,\u00a8\u00a8\u00a8, N.(10)\nAs N \u00d1 8, Eq. (10) converges to the following SDE, dx \"\u00b41 2 \u03b2ptqx dt`a\u03b2ptq dw.\n\nTherefore, the noise perturbations used in SMLD and DDPM correspond to discretizations of SDEs Eq. (9) and Eq. (11) respectively. Interestingly, the SDE of Eq. (9) always gives a process with exploding variance when t \u00d1 8, whilst the SDE of Eq. (11) yields a process with bounded variance (proof in Appendix B). Due to this difference, we hereafter refer to Eq. (9) as the Variance Exploding (VE) SDE, and Eq. (11) the Variance Preserving (VP) SDE. Since both VE and VP SDEs have affine drift coefficients, their perturbation kernels p 0t pxptq | xp0qq are both Gaussian and can be computed in closed-form, as discussed in Section 3.3. This makes training with Eq. (7) particularly efficient.\n\n\nSOLVING THE REVERSE SDE\n\nAfter training a time-dependent score-based model, we can use it to construct the reverse-time SDE and then solve it with numerical approaches to generate samples from p 0 .\n\n\nGENERAL-PURPOSE NUMERICAL SDE SOLVERS\n\nNumerical solvers provide approximate trajectories from SDEs. Many general-purpose numerical methods exist for solving SDEs, such as Euler-Maruyama and stochastic Runge-Kutta methods (Kloeden & Platen, 2013), which correspond to different discretizations of the stochastic dynamics. We can apply any of them to the reverse-time SDE for sample generation.\n\nAncestral sampling-the sampling method of DDPM (Eq. (4))-actually corresponds to one special discretization of the reverse-time VP SDE (Eq. (11)) (see Appendix E). In theory, other discretizations Algorithm 1 PC sampling (VE SDE) 1: xN \" N p0, \u03c3 2 max Iq 2: for i \" N\u00b41 to 0 do 3:\nx 1 i \u00d0 xi`1`p\u03c3 2 i`1\u00b4\u03c3 2 i qs \u03b8\u02dap xi`1, \u03c3i`1q 4: z \" N p0, Iq 5: xi \u00d0 x 1 i`b \u03c3 2 i`1\u00b4\u03c3 2 i z 6: for j \" 1 to M do 7:\nz \" N p0, Iq 8:\n\nxi \u00d0 xi` is\u03b8\u02dapxi, \u03c3iq`?2 iz 9: return x0 Algorithm 2 PC sampling (VP SDE) 1: xN \" N p0, Iq 2: for i \" N\u00b41 to 0 do 3: x 1 i \u00d0 p2\u00b4?1\u00b4\u03b2i`1qxi`1`\u03b2i`1s \u03b8\u02dap xi`1, i`1q 4: z \" N p0, Iq 5: xi \u00d0 x 1 i`? \u03b2i`1z 6: for j \" 1 to M do 7:\n\nz \" N p0, Iq 8:\n\nxi \u00d0 xi` is\u03b8\u02dapxi, iq`?2 iz 9: return x0 Figure 3: PC sampling for LSUN bedroom and church. The vertical axis corresponds to the total computation, and the horizontal axis represents the amount of computation allocated to the corrector. Samples are the best when computation is split between the predictor and corrector.\n\nshould be able to perform comparably or better. To verify this, we propose to apply the same discretization as the forward SDE to the reverse-time SDE (details in Appendix E). The resulting samplers for SMLD and DDPM are given as the \"predictor\" part in Algs. 1 and 2, and we call this family of methods the reverse diffusion sampler. As shown in Tab. 1, these sampling methods perform slightly better than ancestral sampling for both SMLD and DDPM models on CIFAR-10 (DDPM-type ancestral sampling can be applied to SMLD models, see Appendix F).\n\n\nPREDICTOR-CORRECTOR SAMPLERS\n\nUnlike generic SDEs, we have additional information that can be used to improve solutions. Since we have a score-based model s \u03b8\u02dap x, tq \u00ab \u2207 x log p t pxq, we can employ score-based MCMC approaches, such as Langevin MCMC (Parisi, 1981;Grenander & Miller, 1994) or HMC (Neal et al., 2011) to sample from p t directly, and correct the solution of a numerical SDE solver. At each time step, the numerical SDE solver gives an estimate of the sample at the next time step, playing the role of a \"predictor\". The score-based MCMC approach corrects the marginal distribution of the estimated sample, playing the role of a \"corrector\". The idea is analogous to Predictor-Corrector methods for solving systems of equations (Allgower & Georg, 2012), and we similarly name our hybrid sampling algorithms Predictor-Corrector (PC) samplers.\n\nWhen using the reverse diffusion SDE solver (Appendix E) as the predictor, and annealed Langevin dynamics  as the corrector, we have Algs. 1 and 2 for VE and VP SDEs respectively, where t i u N\u00b41 i\"0 are step sizes for Langevin dynamics (specified in Appendix G). PC samplers generalize the original sampling methods of SMLD and DDPM: We can recover annealed Langevin dynamics for SMLD by removing the predictor part, or regain the ancestral sampling method for DDPM when using it as the predictor and removing the corrector part.\n\nWe test PC samplers on SMLD/DDPM models trained with Eqs. (1) and (3), instead of our new continuous objective Eq. (7). This exhibits the compatibility of PC samplers to score-based models trained with a fixed number of noise scales. We summarize the performance of different samplers in Tab. 1, where probability flow is a predictor to be discussed in Section 4.3. Detailed experimental Table 1: Comparing different reverse-time SDE solvers on CIFAR-10. Shaded regions are obtained with the same computation (number of score function evaluations). Mean and standard deviation are reported over five sampling runs. \"P1000\" or \"P2000\": predictor-only samplers using 1000 or 2000 steps. \"C2000\": corrector-only samplers using 2000 steps. \"PC1000\": Predictor-Corrector (PC) samplers using 1000 predictor and 1000 corrector steps. Interpolation Figure 4: Probability flow ODE enables fast sampling with adaptive step-sizes as the numerical precision is varied (left), and reduces the number of score function evaluations (NFE) without harming quality (middle). The invertible mapping from latents to images allows for interpolations (right).\n\nsettings are given in Appendix G. We observe that our reverse diffusion sampler always outperform ancestral sampling, and corrector-only methods (C2000) perform worse than other competitors (P2000, PC1000) with the same computation (In fact, we need way more corrector steps per noise scale, and thus more computation, to match the performance of other samplers.) For all predictors, adding one corrector step for each predictor step (PC1000) doubles computation but always improves sample quality (against P1000). Moreover, it is typically better than doubling the number of predictor steps (P2000), where we have to interpolate between noise scales in an ad hoc manner (detailed in Appendix G) for SMLD/DDPM models. In Fig. 3, we additionally provide qualitative comparison for models trained with the continuous objective Eq. (7), where PC samplers clearly surpass predictoronly samplers under comparable computation, when using a proper number of corrector steps.\n\n\nPROBABILITY FLOW AND EQUIVALENCE TO NEURAL ODES\n\nScore-based models enable another numerical method for solving the reverse-time SDE. For all diffusion processes, there exists a corresponding deterministic process whose trajectories share the same marginal probability densities tp t pxqu T t\"0 . This deterministic process satisfies an ODE (more details in Appendix D.1):\n\ndx \"\n\" f px, tq\u00b41 2 gptq 2 \u2207 x log p t pxq \u0131 dt,(12)\nwhich can be determined from the SDE once scores are known. We name the ODE in Eq. (12) the probability flow ODE. As the score functions are typically parameterized by a neural network, this is an example of a neural ODE .\n\nEfficient sampling As with neural ODEs, we can sample xp0q \" p 0 by solving Eq. (12) from different final conditions xpT q \" p T . Using a fixed discretization strategy we can generate competitive samples, especially when used in conjuction with correctors (Tab. 1, \"probability flow sampler\", details in Appendix D.3). Using a black-box ODE solver (Dormand & Prince, 1980) allows us to explicitly trade-off accuracy for efficiency. With a larger error tolerance, the number of function evaluations can be reduced by over 90% without affecting the visual quality of samples (Fig. 4).  (Dinh et al., 2016) 3.49 -iResNet  3.45 -Glow (Kingma & Dhariwal, 2018) 3.35 -MintNet (Song et al., 2019b) 3.32 -Residual Flow  3.28 46.37 FFJORD (Grathwohl et al., 2018) 3  Exact likelihood computation Leveraging the connection to neural ODEs, we can compute the density defined by Eq. (12) via the instantaneous change of variables formula . This allows us to compute the exact likelihood on any input data (details in Appendix D.2). As an example, we report negative log-likelihoods (NLLs) measured in bits/dim on the CIFAR-10 dataset in Tab. 2.\n\nWe compute log-likelihoods on uniformly dequantized data, and only compare to models evaluated in the same way (excluding models evaluated with variational dequantization (Ho et al., 2019) or discrete data). Main results: (i) For the same DDPM model in Ho et al. (2020), we obtain better bits/dim compared to the upper bound given by ELBO, since our likelihoods are exact; (ii) Using the same architecture, we trained another DDPM model with our continuous denoising score matching objective in Eq. (7) on uniformly dequantized data. This continuously-trained model (DDPM cont. in Tab. 2) has much lower bits/dim compared to the original one, setting a new record bits/dim on uniformly dequantized CIFAR-10 even without maximum likelihood training.\n\nManipulating latent representations By integrating Eq. (12), we can encode any datapoint xp0q into a latent space xpT q. Decoding can be achieved by integrating a corresponding ODE for the reverse-time SDE. As is done with other invertible models such as neural ODEs and normalizing flows (Dinh et al., 2016;Song et al., 2019b), we can manipulate this latent representation for image editing, such as interpolation, and temperature scaling (see Fig. 4 and Appendix D.4).\n\nUniquely identifiable encoding Unlike all current invertible models, our encoding is uniquely identifiable, meaning that with sufficient training data, model capacity, and optimization accuracy, the encoding for an input is uniquely determined by the data distribution (Roeder et al., 2020). This is because our forward SDE, Eq. (5), has no trainable parameters, and its associated probability flow ODE, Eq. (12), provides the same trajectories given perfectly estimated scores. We provide additional empirical verification on this property in Appendix D.5.\n\n\nARCHITECTURE CHOICES\n\nAlthough SMLD and DDPM can be viewed as different instantiations of a unified framework, there is a performance gap reported in previous papers. The best FID values of SMLD models on CIFAR-10 is 10.23 (Song & Ermon, 2020), whereas for DDPM it is 3.17 (Ho et al., 2020). With PC samplers and the same model architecture in Ho et al. (2020), the gaps can be reduced, but score-based models trained with the VE SDE still perform slightly worse (see Tab. 1). This raises the question of whether the variance preserving property of the VP SDE is always preferable or whether there is an interaction with model architecture.\n\nTo answer this question, we performed an extensive architecture search (see Appendix H). Our optimal architecture for the VE SDE, dubbed NCSN++, achieves an FID of 2.45 on CIFAR-10 with PC samplers, whereas the optimal architecture for the VP SDE over the same architecture sweep obtains an FID of 2.88. This indicates that the VE SDE can be advantageous, and practitioners likely need to experiment with different SDEs for new domains and architectures.\n\nBased on these improved architectures, we were able to further improve FID with our continuous training objective in Eq. (7) \n\n\nCONTROLLABLE GENERATION\n\nThe continuous structure of our framework allows us to not only produce data samples from p 0 , but also from p 0 pxp0q | yq if p t py | xptqq is known. Given a forward SDE as in Eq. (5), we can sample from p t pxptq | yq by starting from p T pxpT q | yq and solving a conditional reverse-time SDE:\n\ndx \" tf px, tq\u00b4gptq 2 r\u2207 x log p t pxq`\u2207 x log p t py | xqsudt`gptqdw\n\nWhen y represents class labels, we can train a time-dependent classifier p t py | xptqq for classconditional sampling. Since the forward SDE is tractable, we can easily create training data pxptq, yq for the time-dependent classifier by first sampling pxp0q, yq from a dataset, and then sampling xptq \" p 0t pxptq | xp0qq. Afterwards, we may employ a mixture of cross-entropy losses over different time steps, like Eq. (7), to train the time-dependent classifier p t py | xptqq. Class-conditional CIFAR-10 samples can be found in Fig. 5 (left). See more details and results in Appendix I.\n\nImputation is a special case of conditional sampling. Suppose we have an incomplete data point y where only some subset, \u2126pyq is known. Imputation amounts to sampling from ppxp0q | \u2126pyqq, which we can accomplish using an unconditional model (see Appendix I.2). Colorization is a special case of imputation, except that the known data dimensions are coupled. We can decouple these data dimensions with an orthogonal linear transformation, and perform imputation in the transformed space (details in Appendix I.3). Fig. 5 (right) shows results for inpainting and colorization achieved with unconditional time-dependent score-based models.\n\n\nDISCUSSION\n\nWe presented a framework for score-based generative modeling based on SDEs. Our work enables a better understanding of existing approaches, new sampling algorithms, exact likelihood computation, uniquely identifiable encoding, latent code manipulation, and brings new conditional generation abilities to the family of score-based generative models.\n\nWhile our proposed sampling approaches improve results and enable more efficient sampling, they remain slower at sampling than GANs (Goodfellow et al., 2014) on the same datasets. Identifying ways of combining the stable learning of score-based generative models with the fast sampling of implicit models like GANs remains an important research direction. Additionally, the breadth of samplers one can use when given access to score functions introduces a number of hyper-parameters. Future work would benefit from improved methods to automatically select and tune these hyperparameters, as well as more extensive investigation on the merits and limitations of various samplers. \n\n\nAPPENDIX\n\nWe include several appendices with additional details, derivations, and results. Our framework allows general SDEs with matrix-valued diffusion coefficients that depend on the state, for which we provide a detailed discussion in Appendix A. We give a full derivation of VE and VP SDEs in Appendix B, and discuss how to use them from a practitioner's perspective in Appendix C. We elaborate on the probability flow formulation of our framework in Appendix D, including a derivation of the probability flow ODE (Appendix D.1), exact likelihood computation (Appendix D.2), probability flow sampling (Appendices D.3 and D.4) and experimental verification on uniquely identifiable encoding (Appendix D.5). We give a full description of the reverse diffusion sampler in Appendix E, and the DDPM-type ancestral sampler for SMLD models in Appendix F. We provide full experimental details on Predictor-Corrector samplers in Appendix G. We explain our model architectures and detailed experimental settings in Appendix H, with 1024\u02c61024 CelebA-HQ samples therein. Finally, we detail on the algorithms for controllable generation in Appendix I, and include extended results for class-conditional generation (Appendix I.1), image inpainting (Appendix I.2) and colorization (Appendix I.3).\n\n\nA THE FRAMEWORK FOR MORE GENERAL SDES\n\nIn the main text, we introduced our framework based on a simplified SDE Eq. (5) where the diffusion coefficient is independent of xptq. It turns out that our framework can be extended to hold for more general diffusion coefficients. We can consider SDEs in the following form:\ndx \" f px, tqdt`Gpx, tqdw,(14)\nwhere f p\u00a8, tq : R d \u00d1 R d and Gp\u00a8, tq : R d \u00d1 R d\u02c6d . We follow the It\u00f4 interpretation of SDEs throughout this paper.\n\nAccording to (Anderson, 1982), the reverse-time SDE is given by (cf ., Eq. (6)) dx \" tf px, tq\u00b4\u2207\u00a8rGpx, tqGpx, tq T s\u00b4Gpx, tqGpx, tq T \u2207 x log p t pxqudt`Gpx, tqdw, (15) where we define \u2207\u00a8Fpxq :\" p\u2207\u00a8f 1 pxq, \u2207\u00a8f 2 pxq,\u00a8\u00a8\u00a8, \u2207\u00a8f d pxqq T for a matrix-valued function Fpxq :\" pf 1 pxq, f 2 pxq,\u00a8\u00a8\u00a8, f d pxqq T throughout the paper.\n\nThe probability flow ODE corresponding to Eq. (14) has the following form (cf ., Eq. (12), see a detailed derivation in Appendix D.1):\n\ndx \"\n\" f px, tq\u00b41 2 \u2207\u00a8rGpx, tqGpx, tq T s\u00b41 2 Gpx, tqGpx, tq T \u2207 x log p t pxq * dt.(16)\nFinally for conditional generation with the general SDE Eq. (14), we can solve the conditional reverse-time SDE below (cf ., Eq. (13), details in Appendix I):\n\ndx \" tf px, tq\u00b4\u2207\u00a8rGpx, tqGpx, tq T s\u00b4Gpx, tqGpx, tq T \u2207 x log p t pxq Gpx, tqGpx, tq T \u2207 x log p t py | xqudt`Gpx, tqdw. (17) When the drift and diffusion coefficient of an SDE are not affine, it can be difficult to compute the transition kernel p 0t pxptq | xp0qq in closed form. This hinders the training of score-based models, because Eq. (7) requires knowing \u2207 xptq log p 0t pxptq | xp0qq. To overcome this difficulty, we can replace denoising score matching in Eq. (7) with other efficient variants of score matching that do not require computing \u2207 xptq log p 0t pxptq | xp0qq. For example, when using sliced score matching (Song et al., 2019a), our training objective Eq. (7) becomes \u03b8\u02da\" arg min\n\u03b8 E t \" \u03bbptqE xp0q E xptq E v\"pv \" 1 2 s \u03b8 pxptq, tq 2 2`v T s \u03b8 pxptq, tqv \uf6be* ,(18)\nwhere \u03bb : r0, T s \u00d1 R`is a positive weighting function, t \" Up0, T q, Ervs \" 0, and Covrvs \" I. We can always simulate the SDE to sample from p 0t pxptq | xp0qq, and solve Eq. (18) to train the time-dependent score-based model s \u03b8 px, tq.\n\n\nB DERIVATIONS OF VE AND VP SDES\n\nBelow we provide detailed derivations to show that the noise perturbations of SMLD and DDPM are discretizations of the Variance Exploding (VE) and Variance Preserving (VP) SDEs respectively.\n\nFirst, when using a total of N noise scales, each perturbation kernel p \u03c3i px | x 0 q of SMLD can be derived from the following Markov chain:\nx i \" x i\u00b41`b \u03c3 2 i\u00b4\u03c3 2 i\u00b41 z i\u00b41 , i \" 1,\u00a8\u00a8\u00a8, N,(19)\nwhere z i\u00b41 \" N p0, Iq, x 0 \" p data , and we have introduced \u03c3 0 \" 0 to simplify the notation. In the limit of N \u00d1 8, the Markov chain tx i u N i\"1 becomes a continuous stochastic process txptqu 1 t\"0 , t\u03c3 i u N i\"1 becomes a function \u03c3ptq, and z i becomes zptq, where we have used a continuous time variable t P r0, 1s for indexing, rather than an integer i P t1, 2,\u00a8\u00a8\u00a8, N u. Let \u2206t \" 1 N\u00b41 , x\u00b4i\u00b41 N\u00b41\u00af\" x i , \u03c3\u00b4i\u00b41 N\u00b41\u00af\" \u03c3 i , and z\u00b4i\u00b41 N\u00b41\u00af\" z i . Eq. (19) becomes the following at t P 0, 1 N\u00b41 ,\u00a8\u00a8\u00a8, N\u00b42 N\u00b41 ( :\nxpt`\u2206tq \" xptq`a\u03c3 2 pt`\u2206tq\u00b4\u03c3 2 ptq zptq \u00ab xptq`c d r\u03c3 2 ptqs dt \u2206t zptq,\nwhere the approximate equality holds when \u2206t ! 1. In the limit of \u2206t \u00d1 0, this converges to dx \"\nc d r\u03c3 2 ptqs dt dw,(20)\nwhich is the VE SDE. Note that xp0q \" x 1 \" x 0`\u03c31 z 0 \" x 0`\u03c3 2 p0qz 0 and x 0 \" p data , so we have p 0 pxp0qq \" \u015f p data pxqN pxp0q; x, \u03c3 2 1 Iqdx. In other words, p 0 pxp0qq for the VE SDE is a perturbed data distribution with a small Gaussian noise.\n\nFor the perturbation kernels tp \u03b1i px | x 0 qu N i\"1 used in DDPM, the discrete Markov chain is\nx i \" a 1\u00b4\u03b2 i x i\u00b41`a \u03b2 i z i\u00b41 , i \" 1,\u00a8\u00a8\u00a8, N,(21)\nwhere z i\u00b41 \" N p0, Iq. To obtain the limit of this Markov chain when N \u00d1 8, we define an auxiliary set of noise scales t\u03b2 i \" pN\u00b41q\u03b2 i u N i\"1 , and re-write Eq. (21) as below\nx i \" d 1\u00b4\u03b2 i N\u00b41 x i\u00b41`d\u03b2 i N\u00b41 z i\u00b41 , i \" 1,\u00a8\u00a8\u00a8, N.(22)\nIn the limit of N \u00d1 8, t\u03b2 i u N i\"1 becomes a function \u03b2ptq indexed by t P r0, 1s. Let \u03b2\u00b4i\u00b41 N\u00b41\u00af\"\u03b2 i , xp i\u00b41 N\u00b41 q \" x i , zp i\u00b41 N\u00b41 q \" z i and \u2206t \" 1 N\u00b41 . The Markov chain Eq. (22) becomes the following at t P t0, 1,\u00a8\u00a8\u00a8, N\u00b42 N\u00b41 u xpt`\u2206tq \" a 1\u00b4\u03b2ptq\u2206t xptq`a\u03b2ptq\u2206t zptq \u00ab\u00b41 2 \u03b2ptq\u2206t xptq`a\u03b2ptq\u2206t zptq, (23) where the approximate equality holds when \u2206t ! 1. Therefore, in the limit of \u2206t \u00d1 0, Eq. (23) converges to the following VP SDE:\ndx \"\u00b41 2 \u03b2ptqx dt`a\u03b2ptq dw.(24)Because xp0q \" lim N \u00d18 ? 1\u00b4\u03b2 1 x 0`? \u03b2 1 z 0 \" lim N \u00d18 b 1\u00b4\u03b2 p0q N\u00b41 x 0`b \u03b2p0q\nN\u00b41 z 0 \" x 0 and x 0 \" p data , we have p 0 pxp0qq \" p data pxq.\n\nSo far, we have demonstrated that the noise perturbations used in SMLD and DDPM correspond to discretizations of VE and VP SDEs respectively. The VE SDE always yields a process with exploding variance when t \u00d1 8. In contrast, the VP SDE yields a process with bounded variance. In addition, the process has a constant unit variance for all t P r0, 8q when ppxp0qq has a unit variance. Since the VP SDE has affine drift and diffusion coefficients, we can use Eq. (5.51) in S\u00e4rkk\u00e4 & Solin (2019) to obtain an ODE that governs the evolution of variance\nd\u03a3ptq dt \" \u03b2ptqpI\u00b4\u03a3ptqq,\nwhere \u03a3ptq \" Covrxptqs. Solving this ODE, we obtain\n\n\u03a3ptq \" I`e \u015f T 0\u00b4\u03b2 ptqdt p\u03a3p0q\u00b4Iq, from which it is clear that the variance \u03a3ptq is always bounded given \u03a3p0q. Moreover, \u03a3ptq \" I if \u03a3p0q \" I. Due to this difference, we name Eq. (9) as the Variance Exploding (VE) SDE, and Eq. (11) the Variance Preserving (VP) SDE.\n\nSince both VE and VP SDEs have affine drift coefficients, their perturbation kernels p 0t pxptq | xp0qq are both Gaussian and can be computed with Eqs. (5.50) and (5.51) in S\u00e4rkk\u00e4 & Solin (2019):\np 0t pxptq | xp0qq \" # N`xptq; xp0q, r\u03c3 2 ptq\u00b4\u03c3 2 p0qsI\u02d8, (VE SDE) N`xptq; xp0qe\u00b41 2 \u015f t 0 \u03b2psqds , I\u00b4Ie\u00b4\u015f t 0 \u03b2psqds\u02d8( VP SDE)\n.\n\n(25) Therefore, both SDEs allow efficient training with the objective in Eq. (7).\n\n\nC SDES IN THE WILD\n\nBelow we discuss SDEs that directly correspond to implementations for practical SMLD and DDPM models. In SMLD, the noise scales t\u03c3 i u N i\"1 is typically a geometric sequence where \u03c3 min is fixed to 0.01 and \u03c3 max is chosen according to Technique 1 in Song & Ermon (2020). Usually, SMLD models normalize image inputs to the range r0, 1s. Since t\u03c3 i u N i\"1 is a geometric sequence, we have \u03c3 i \" \u03c3 min\u00b4\u03c3 max \u03c3min\u00afi\u00b41 N\u00b41 and therefore \u03c3ptq \" \u03c3 min\u00b4\u03c3 max \u03c3min\u00aft . The VE SDE in this case is\ndx \" \u03c3 min\u02c6\u03c3 max \u03c3 min\u02d9t c 2 log \u03c3 max \u03c3 min dw,(26)\nwhere xp0q \" \u015f p data pxqN pxp0q; x, \u03c3 2 min Iqdx. The perturbation kernel can be computed via Eq. (25):\np 0t pxptq | xp0qq \" N\u02c6xptq; xp0q, \" \u03c3 2 min\u00b4\u03c3 max \u03c3 min\u00af2 t\u00b4\u03c3 2 min \u0131 I\u02d9.(27)\nFor DDPM models, t\u03b2 i u N i\"1 is typically an arithmetic sequence where \u03b2 i \"\u03b2 min N\u00b41`i\u00b41 pN\u00b41q 2 p\u03b2 max\u03b2 min q. Therefore, \u03b2ptq \"\u03b2 min`t p\u03b2 max\u00b4\u03b2min q. This corresponds to the following instantiation of the VP SDE:\n\ndx \"\u00b41 2 p\u03b2 min`t p\u03b2 max\u00b4\u03b2min qqxdt`b\u03b2 min`t p\u03b2 max\u00b4\u03b2min qdw,\n\nwhere xp0q \" p data pxq. In our experiments, we let\u03b2 min \" 0.1 and\u03b2 max \" 20, which correspond to the settings in Ho et al. (2020). The perturbation kernel is given by p 0t pxptq | xp0qq \" N\u00b4xptq; e\u00b41 4 t 2 p\u03b2max\u00b4\u03b2minq\u00b41 2 t\u03b2min xp0q, I\u00b4Ie\u00b41 2 t 2 p\u03b2max\u00b4\u03b2minq\u00b4t\u03b2min\u00af.\n\nAs a sanity check for our SDE generalizations to SMLD and DDPM, we compare the perturbation kernels of SDEs and original discrete Markov chains in Fig. 6. The SMLD and DDPM models both use N \" 1000 noise scales. For SMLD, we only need to compare the variances of perturbation kernels. For DDPM, we compare the scaling factors of means and the variances. As demonstrated in Fig. 6, the discrete perturbation kernels of original SMLD and DDPM models align well with perturbation kernels derived from VE and VP SDEs. \nBp t pxq Bt \"\u00b4d \u00ff i\"1 B Bx i rf i px, tqp t pxqs`1 2 d \u00ff i\"1 d \u00ff j\"1 B 2 Bx i Bx j \" d \u00ff k\"1 G ik px, tqG jk px, tqp t pxq \u0131\n. (30) We can easily rewrite Eq. (30) to obtain\nBp t pxq Bt \"\u00b4d \u00ff i\"1 B Bx i rf i px, tqp t pxqs`1 2 d \u00ff i\"1 d \u00ff j\"1 B 2 Bx i Bx j \" d \u00ff k\"1 G ik px, tqG jk px, tqp t pxq \u0131 \"\u00b4d \u00ff i\"1 B Bx i rf i px, tqp t pxqs`1 2 d \u00ff i\"1 B Bx i \" d \u00ff j\"1 B Bx j \" d \u00ff k\"1 G ik px, tqG jk px, tqp t pxq \u0131\u0131 . (31) Note that d \u00ff j\"1 B Bx j \" d \u00ff k\"1 G ik px, tqG jk px, tqp t pxq \u0131 \" d \u00ff j\"1 B Bx j \" d \u00ff k\"1 G ik px, tqG jk px, tq \u0131 p t pxq`d \u00ff j\"1 d \u00ff k\"1 G ik px, tqG jk px, tqp t pxq B Bx j log p t pxq \"p t pxq\u2207\u00a8rGpx, tqGpx, tq T s`p t pxqGpx, tqGpx, tq T \u2207 x log p t pxq,\nbased on which we can continue the rewriting of Eq. (31) to obtain\nBp t pxq Bt \"\u00b4d \u00ff i\"1 B Bx i rf i px, tqp t pxqs`1 2 d \u00ff i\"1 B Bx i \" d \u00ff j\"1 B Bx j \" d \u00ff k\"1 G ik px, tqG jk px, tqp t pxq \u0131\u0131 \"\u00b4d \u00ff i\"1 B Bx i rf i px, tqp t pxqs 1 2 d \u00ff i\"1 B Bx i \" p t pxq\u2207\u00a8rGpx, tqGpx, tq T s`p t pxqGpx, tqGpx, tq T \u2207 x log p t pxq \u0131 \"\u00b4d \u00ff i\"1 B Bx i ! f i px, tqp t pxq 1 2 \" \u2207\u00a8rGpx, tqGpx, tq T s`Gpx, tqGpx, tq T \u2207 x log p t pxq \u0131 p t pxq ) \"\u00b4d \u00ff i\"1 B Bx i rf i px, tqp t pxqs,(32)\nwhere we defin\u1ebd f px, tq fi f px, tq\u00b41 2 \u2207\u00a8rGpx, tqGpx, tq T s\u00b41 2 Gpx, tqGpx, tq T \u2207 x log p t pxq.\n\nInspecting Eq. (32), we observe that it equates the forward Kolmogorov equation of the following SDE withGpx, tq fi 0 (the forward Kolmogorov equation holds in this case, as can be justified by the instantaneous change of variables formula in )\n\ndx \"f px, tqdt`Gpx, tqdw, which is essentially an ODE:\n\ndx \"f px, tqdt, same as the probability flow ODE given by Eq. (16). Therefore, we have shown that the probability flow ODE Eq. (16) induces the same marginal probability density p t pxq as the SDE in Eq. (14).\n\n\nD.2 LIKELIHOOD COMPUTATION\n\nThe probability flow ODE in Eq. (16) has the following form when we replace the score \u2207 x log p t pxq with the time-dependent score-based model s \u03b8 px, tq:\n\ndx \"\n\" f px, tq\u00b41 2 \u2207\u00a8rGpx, tqGpx, tq T s\u00b41 2 Gpx, tqGpx, tq T s \u03b8 px, tq * loooooooooooooooooooooooooooooooooooooooooomoooooooooooooooooooooooooooooooooooooooooon fif \u03b8 px,tq dt.(33)\nWith the instantaneous change of variables formula , we can compute the loglikelihood of p 0 pxq using\nlog p 0 pxp0qq \" log p T pxpT qq\u00b4\u017c T 0 \u2207\u00a8f \u03b8 pxptq, tqdt,(34)\nwhere the random variable xptq as a function of t can be obtained by solving the probability flow ODE in Eq. (33). In many cases computing \u2207\u00a8f \u03b8 px, tq is expensive, so we follow Grathwohl et al. (2018) to estimate it with the Skilling-Hutchinson trace estimator (Skilling, 1989;Hutchinson, 1990).\n\nIn particular, we have \u2207\u00a8f \u03b8 px, tq \" E pp q r T \u2207f \u03b8 px, tq s,\n\nwhere \u2207f \u03b8 denotes the Jacobian off \u03b8 p\u00a8, tq, and the random variable satisfies E pp q r s \" 0 and Cov pp q r s \" I. The vector-Jacobian product T \u2207f \u03b8 px, tq can be efficiently computed using reversemode automatic differentiation, at approximately the same cost as evaluatingf \u03b8 px, tq. As a result, we can sample \" pp q and then compute an efficient unbiased estimate to \u2207\u00a8f \u03b8 px, tq using T \u2207f \u03b8 px, tq . Since this estimator is unbiased, we can attain an arbitrarily small error by averaging over a sufficient number of runs. Therefore, by applying the Skilling-Hutchinson estimator Eq. (35) to Eq. (34), we can compute the log-likelihood to any accuracy.\n\n\nD.3 PROBABILITY FLOW SAMPLING\n\nSuppose we have a forward SDE dx \" f px, tqdt`Gptqdw, and one of its discretization\nx i`1 \" x i`fi px i q`G i z i , i \" 0, 1,\u00a8\u00a8\u00a8, N\u00b41,(36)\nwhere z i \" N p0, Iq. We assume the discretization schedule of time is fixed beforehand, and thus we absorb the dependency on \u2206t into the notations of f i and G i . Using Eq. (16), we can obtain the following probability flow ODE:\n\ndx \"\n\" f px, tq\u00b41 2 GptqGptq T \u2207 x log p t pxq * dt.(37)\nWe may employ any numerical method to integrate the probability flow ODE backwards in time for sample generation. In particular, we propose a discretization in a similar functional form to Eq. (36):\nx i \" x i`1\u00b4fi`1 px i`1 q`1 2 G i`1 G T i`1 s \u03b8\u02dap x i`1 , i`1q, i \" 0, 1,\u00a8\u00a8\u00a8, N\u00b41,\nwhere the score-based model s \u03b8\u02dap x i , iq is conditioned on the iteration number i. This is a deterministic iteration rule. Unlike reverse diffusion samplers or ancestral sampling, there is no additional randomness once the initial sample x N is obtained from the prior distribution. When applied to SMLD models, we can get the following iteration rule for probability flow sampling:\nx i \" x i`1`1 2 p\u03c3 2 i`1\u00b4\u03c3 2 i qs \u03b8\u02dap x i`1 , \u03c3 i`1 q, i \" 0, 1,\u00a8\u00a8\u00a8, N\u00b41.(38)\nSimilarly, for DDPM models, we have \nx i \" p2\u00b4a1\u00b4\u03b2 i`1 qx i`1`1 2 \u03b2 i`1 s \u03b8\u02dap x i`1 , i`1q, i \" 0, 1,\u00a8\u00a8\u00a8, N\u00b41.(39)\n\nD.5 UNIQUELY IDENTIFIABLE ENCODING\n\nAs a sanity check, we train two models (denoted as \"Model A\" and \"Model B\") with different architectures using the VE SDE on CIFAR-10. Here Model A is an NCSN++ model with 4 layers per resolution trained using the continuous objective in Eq. (7), and Model B is all the same except that it uses 8 layers per resolution. Model definitions are in Appendix H.\n\nWe report the latent codes obtained by Model A and Model B for a random CIFAR-10 image in Fig. 8. In Fig. 9, we show the dimension-wise differences and correlation coefficients between latent encodings on a total of 16 CIFAR-10 images. Our results demonstrate that for the same inputs, Model A and Model B provide encodings that are close in every dimension, despite having different model architectures and training runs.\n\n\nE REVERSE DIFFUSION SAMPLING\n\n\nGiven a forward SDE\n\ndx \" f px, tqdt`Gptqdw, and suppose the following iteration rule is a discretization of it:\nx i`1 \" x i`fi px i q`G i z i , i \" 0, 1,\u00a8\u00a8\u00a8, N\u00b41(40)\nwhere z i \" N p0, Iq. Here we assume the discretization schedule of time is fixed beforehand, and thus we can absorb it into the notations of f i and G i .\n\nBased on Eq. (40), we propose to discretize the reverse-time SDE dx \" rf px, tq\u00b4GptqGptq T \u2207 x log p t pxqsdt`Gptqdw, with a similar functional form, which gives the following iteration rule for i P t0, 1,\u00a8\u00a8\u00a8, N\u00b41u: x 1 (T) Figure 9: Left: The dimension-wise difference between encodings obtained by Model A and B. As a baseline, we also report the difference between shuffled representations of these two models. Right:\nx i \" x i`1\u00b4fi`1 px i`1 q`G i`1 G T i`1 s \u03b8\u02dap x i`1 , i`1q`G i`1 z i`1 ,(41)\nThe dimension-wise correlation coefficients of encodings obtained by Model A and Model B.\n\nwhere our trained score-based model s \u03b8\u02dap x i , iq is conditioned on iteration number i.\n\nWhen applying Eq. (41) to Eqs. (10) and (19), we obtain a new set of numerical solvers for the reverse-time VE and VP SDEs, resulting in sampling algorithms as shown in the \"predictor\" part of Algs. 1 and 2. We name these sampling methods (that are based on the discretization strategy in Eq. (41)) reverse diffusion samplers.\n\nNote that the ancestral sampling of DDPM (Ho et al., 2020) (Eq. (4)) matches its reverse diffusion counterpart when \u03b2 i \u00d1 0 for all i (which happens when \u2206t \u00d1 0 since \u03b2 i \"\u03b2 i \u2206t), because\nx i \" 1 a 1\u00b4\u03b2 i`1 px i`1`\u03b2i`1 s \u03b8\u02dap x i`1 , i`1qq`a\u03b2 i`1 z i`1 \"\u02c61`1 2 \u03b2 i`1`o p\u03b2 i`1 q\u02d9px i`1`\u03b2i`1 s \u03b8\u02dap x i`1 , i`1qq`a\u03b2 i`1 z i`1 \u00ab\u02c61`1 2 \u03b2 i`1\u02d9p x i`1`\u03b2i`1 s \u03b8\u02dap x i`1 , i`1qq`a\u03b2 i`1 z i`1 \"\u02c61`1 2 \u03b2 i`1\u02d9xi`1`\u03b2i`1 s \u03b8\u02dap x i`1 , i`1q`1 2 \u03b2 2 i`1 s \u03b8\u02dap x i`1 , i`1q`a\u03b2 i`1 z i`1 \u00ab\u02c61`1 2 \u03b2 i`1\u02d9xi`1`\u03b2i`1 s \u03b8\u02dap x i`1 , i`1q`a\u03b2 i`1 z i`1 \" \" 2\u00b4\u02c61\u00b41 2 \u03b2 i`1\u02d9\uf6be x i`1`\u03b2i`1 s \u03b8\u02dap x i`1 , i`1q`a\u03b2 i`1 z i`1 \u00ab \" 2\u00b4\u02c61\u00b41 2 \u03b2 i`1\u02d9`o p\u03b2 i`1 q \uf6be x i`1`\u03b2i`1 s \u03b8\u02dap x i`1 , i`1q`a\u03b2 i`1 z i`1 \"p2\u00b4a1\u00b4\u03b2 i`1 qx i`1`\u03b2i`1 s \u03b8\u02dap x i`1 , i`1q`a\u03b2 i`1 z i`1 .\nTherefore, the original ancestral sampler of Eq. (4) is essentially a different discretization to the same reverse-time SDE. This unifies the sampling method in Ho et al. (2020) as a numerical solver to the reverse-time VP SDE in our continuous framework.\n\n\nF ANCESTRAL SAMPLING FOR SMLD MODELS\n\nThe ancestral sampling method for DDPM models can also be adapted to SMLD models. Consider a sequence of noise scales \u03c3 1 \u0103 \u03c3 2 \u0103\u00a8\u00a8\u00a8\u0103 \u03c3 N as in SMLD. By perturbing a data point x 0 with these noise scales sequentially, we obtain a Markov chain\nx 0 \u00d1 x 1 \u00d1\u00a8\u00a8\u00a8\u00d1 x N , where ppx i | x i\u00b41 q \" N px i ; x i\u00b41 , p\u03c3 2 i\u00b4\u03c3 2 i\u00b41 qIq, i \" 1, 2,\u00a8\u00a8\u00a8, N.\nHere we assume \u03c3 0 \" 0 to simplify notations. Following Ho et al. (2020), we can compute\nqpx i\u00b41 | x i , x 0 q \" N\u02c6x i\u00b41 ; \u03c3 2 i\u00b41 \u03c3 2 i x i`\u00b41\u00b4\u03c3 2 i\u00b41 \u03c3 2 i\u00afx 0 , \u03c3 2 i\u00b41 p\u03c3 2 i\u00b4\u03c3 2 i\u00b41 q \u03c3 2 i I\u02d9.\nIf we parameterize the reverse transition kernel as\np \u03b8 px i\u00b41 | x i q \" N px i\u00b41 ; \u00b5 \u03b8 px i , iq, \u03c4 2 i Iq, then L t\u00b41 \" E q rD KL pqpx i\u00b41 | x i , x 0 qq } p \u03b8 px i\u00b41 | x i qs \" E q \u00ab 1 2\u03c4 2 i \u03c3 2 i\u00b41 \u03c3 2 i x i`\u00b41\u00b4\u03c3 2 i\u00b41 \u03c3 2 i\u00afx 0\u00b4\u00b5\u03b8 px i , iq 2 2 ff`C \" E x0,z \u00ab 1 2\u03c4 2 i x i px 0 , zq\u00b4\u03c3 2 i\u00b4\u03c3 2 i\u00b41 \u03c3 i z\u00b4\u00b5 \u03b8 px i px 0 , zq, iq 2 2 ff`C ,\nwhere L t\u00b41 is one representative term in the ELBO objective (see Eq. (8) in Ho et al. (2020)), C is a constant that does not depend on \u03b8, z \" N p0, Iq, and x i px 0 , zq \" x 0`\u03c3i z. We can therefore parameterize \u00b5 \u03b8 px i , iq via\n\u00b5 \u03b8 px i , iq \" x i`p \u03c3 2 i\u00b4\u03c3 2 i\u00b41 qs \u03b8 px i , iq,\nwhere s \u03b8 px i , iq is to estimate z{\u03c3 i . As in Ho et al. (2020), we let \u03c4 i \" c \u03c3 2 i\u00b41 p\u03c3 2 i\u00b4\u03c3 2 i\u00b41 q \u03c3 2 i . Through ancestral sampling on \u015b N i\"1 p \u03b8 px i\u00b41 | x i q, we obtain the following iteration rule\nx i\u00b41 \" x i`p \u03c3 2 i\u00b4\u03c3 2 i\u00b41 qs \u03b8\u02dap x i , iq`d \u03c3 2 i\u00b41 p\u03c3 2 i\u00b4\u03c3 2 i\u00b41 q \u03c3 2 i z i , i \" 1, 2,\u00a8\u00a8\u00a8, N,(42)\nwhere x N \" N p0, \u03c3 2 N Iq, \u03b8\u02dadenotes the optimal parameter of s \u03b8 , and z i \" N p0, Iq. We call Eq. (42) the ancestral sampling method for SMLD models.\n\n\nG ADDITIONAL DETAILS ON PREDICTOR-CORRECTOR SAMPLERS\n\nTraining We use the same architecture in Ho et al. (2020) for our score-based models. For the VE SDE, we train a model with the original SMLD objective in Eq. (1); similarly for the VP SDE, we use the original DDPM objective in Eq. (3). We apply a total number of 1000 noise scales for training both models. For results in Fig. 3, we train an NCSN++ model (definition in Appendix H) on 256\u02c6256 LSUN bedroom and church outdoor (Yu et al., 2015) datasets with the VE SDE and our continuous objective Eq. (7).\n\nThe corrector algorithms We take the schedule of annealed Langevin dynamics in , but re-frame it with slight modifications in order to get better interpretability and empirical performance. We provide the corrector algorithms in Algs. 3 and 4 respectively, where we call r the \"signal-to-noise\" ratio. We determine the step size using the norm of the Gaussian noise z 2 , norm of the score-based model s \u03b8\u02da 2 and the signal-to-noise ratio r. When sampling a large batch of samples together, we replace the norm \u00a8 2 with the average norm across the mini-batch. When the batch size is small, we suggest replacing z 2 with ? d, where d represents the dimensionality of z.\n\n\nAlgorithm 3 Corrector algorithm (VE SDE).\n\nRequire: t\u03c3 i u N i\"1 , r, N, M . 1: x 0 N \" N p0, \u03c3 2 max Iq 2: for i \u00d0 N to 1 do 3: for j \u00d0 1 to M do 4:\n\nz \" N p0, Iq 5:\ng \u00d0 s \u03b8\u02dap x j\u00b41 i , \u03c3 i q 6: \u00d0 2pr z 2 { g 2 q 2 7: x j i \u00d0 x j\u00b41 i` g`?2 z 8: x 0 i\u00b41 \u00d0 x M i return x 0 0\nAlgorithm 4 Corrector algorithm (VP SDE).\n\nRequire: t\u03b2 i u N i\"1 , t\u03b1 i u N i\"1 , r, N, M . 1: x 0 N \" N p0, Iq 2: for i \u00d0 N to 1 do 3: for j \u00d0 1 to M do 4:\n\nz \" N p0, Iq 5:\ng \u00d0 s \u03b8\u02dap x j\u00b41 i , iq 6: \u00d0 2\u03b1 i pr z 2 { g 2 q 2 7: x j i \u00d0 x j\u00b41 i` g`?2 z 8: x 0 i\u00b41 \u00d0 x M i return x 0 0\nDenoising For both SMLD and DDPM models, the generated samples typically contain small noise that is hard to detect by humans. As noted by Jolicoeur-Martineau et al. (2020), FIDs can be significantly worse without removing this noise. This unfortunate sensitivity to noise is also part of the reason why NCSN models trained with SMLD has been performing worse than DDPM models in terms of FID, because the former does not use a denoising step at the end of sampling, while the latter does. In all experiments of this paper we ensure there is a single denoising step at the end of sampling, using Tweedie's formula (Efron, 2011).  (2019)). Specifically, for SMLD models, we keep \u03c3 min and \u03c3 max fixed and double the number of time steps. For DDPM models, we halve\u03b2 min and\u03b2 max before doubling the number of time steps. Suppose ts \u03b8 px, iqu N\u00b41 i\"0 is a score-based model trained on N time steps, and let ts 1 \u03b8 px, iqu 2N\u00b41\n\ni\"0 denote the corresponding interpolated score-based model at 2N time steps. We test two different interpolation strategies for time steps: linear interpolation where s 1 \u03b8 px, iq \" s \u03b8 px, i{2q and rounding interpolation where s 1 \u03b8 px, iq \" s \u03b8 px, ti{2uq. We provide results with linear interpolation in Tab. 1, and give results of rounding interpolation in Tab. 4. We observe that different interpolation methods result in performance differences but maintain the general trend of predictor-corrector methods performing on par or better than predictor-only or corrector-only samplers.\n\nHyper-parameters of the samplers For Predictor-Corrector and corrector-only samplers on CIFAR-10, we search for the best signal-to-noise ratio (r) over a grid that increments at 0.01. We report the best r in Tab. 5. For LSUN bedroom/church outdoor, we fix r to 0.075. Unless otherwise noted, we use one corrector step per noise scale for all PC samplers. We use two corrector steps per noise scale for corrector-only samplers on CIFAR-10. We always use a batch size of 128 for training. For sample generation, the batch size is 1024 on CIFAR-10 and 8 on LSUN bedroom/church outdoor.\n\n\nH ARCHITECTURAL IMPROVEMENTS FOR MODELS WITH VE PERTURBATIONS\n\nTo study whether the VE perturbations are inherently weaker than VP perturbations, we conducted a comprehensive architecture search to compare their best performance. Our results demonstrate that the VE perturbation can be advantageous in certain cases. Further, our endeavor gives rise to new state-of-the-art models on CIFAR-10, and enables the first high-fidelity image samples of resolution 1024\u02c61024 from score-based generative models. Code and checkpoints will be open sourced.   Our architecture is mostly based on Ho et al. (2020). We additionally search over the following components to explore the potential of score-based models trained with VE perturbations.\n\n\u2022 Upsampling and downsampling images with anti-aliasing based on Finite Impulse Response (FIR) (Zhang, 2019). We follow the same implementation and hyper-parameters in StyleGAN-2 (Karras et al., 2020b). \u2022 Rescaling all skip connections by 1 { ?\n\n2. This has been demonstrated effective in several works, including ProgressiveGAN (Karras et al., 2018), StyleGAN (Karras et al., 2019) and StyleGAN-2 (Karras et al., 2020b). \u2022 Replacing the original residual blocks in DDPM with residual blocks from BigGAN (Brock et al., 2018). \u2022 Increasing the number of residual blocks per resolution from 2 to 4. \u2022 Incorporating progressive growing architectures. We consider two progressive architectures for input: \"input skip\" and \"residual\", and two progressive architectures for output: \"output skip\" and \"residual\". These progressive architectures are defined and implemented according to StyleGAN-2.\n\nWe also tested equalized learning rates, a trick used in very successful models like Progressive-GAN (Karras et al., 2018) and StyleGAN (Karras et al., 2019). However, we found it harmful at an early stage of our experiments, and therefore decided not to include it in the architecture search.\n\nThe exponential moving average (EMA) rate has a significant impact on performance. For models trained with VE perturbations, we notice that 0.999 works better than 0.9999, whereas for models trained with VP perturbations it is the opposite. We therefore use an EMA rate of 0.999 and 0.9999 for VE and VP models respectively.\n\nFor sampling from models with VE perturbations, we use the PC sampler discretized at 1000 noise scales with one Langevin step per scale and a signal-of-noise ratio of 0.16. For VP perturbations, we use the reverse diffusion sampler discretized at 1000 noise scales without correctors.\n\n\nH.2 RESULTS ON CIFAR-10\n\nWe provide the architecture search results in Fig. 10. The box plots demonstrate the importance of each component when other components can vary freely. On both CIFAR-10 and CelebA, the additional components that we searched over can always improve the performance on average. For progressive growing, it is not clear which combination of configurations consistently performs the best, but the results are typically better than when no progressive growing architecture is used. Our best score-based model uses FIR upsampling/downsampling, rescales skip connections, employs BigGAN-type residual blocks, uses 4 residual blocks per resolution instead of 2, and uses \"residual\" for input and no progressive growing architecture for output. We name this model \"NCSN++\", following the naming convention of previous SMLD models .  2020), the FID value reported here is the lowest over the course of training, rather than the average over checkpoints after 0.5M iterations (as done in our architecture search). We conducted the same architecture search for a score-based model trained with the DDPM objective Eq. (3). The optimal model obtains an FID of 2.88 on CIFAR-10, significantly worse than the model trained with VE perturbations. Interestingly, the optimal score-based model for VP perturbations has significantly different configurations than those trained with VE perturbation. Some of the components like rescaling skip connections and progressive growing architectures seem to be harmful for them. This further confirms that the relative advantage of VE/VP SDEs is strongly correlated with model architecture, and should be jointly tuned as part of the design choices.\n\nTo further improve the NCSN++ model upon conditioning on continuous time variables, we change positional embeddings, the layers in Ho et al. (2020) for conditioning on discrete time steps, to random Fourier feature embeddings, as advocated in Tancik et al. (2020). The scale parameter of these random Fourier feature embeddings is set to 16. We additionally train the model with our continuous objective Eq. (7). These changes improve the FID on CIFAR-10 from 2.45 to 2.38, resulting in a model dubbed NCSN++ cont. (4 blocks/resolution). Finally, we can further improve    \n\nFigure 6 :\n6idea of probability flow ODE is inspired byMaoutsa et al. (2020), and one can find the derivation of a simplified case therein. Below we provide a derivation for the fully general ODE in Eq. (16). We consider the SDE in Eq.(14), which possesses the following form:dx \" f px, tqdt`Gpx, tqdw, Discrete-time perturbation kernels and our continuous generalizations match each other almost exactly. (a) compares the variance of perturbation kernels for SMLD and VE SDE; (b) compares the scaling factors of means of perturbation kernels for DDPM and VP SDE; and (c) compares the variance of perturbation kernels for DDPM and VP SDE.where f p\u00a8, tq : R d \u00d1 R d and Gp\u00a8, tq : R d \u00d1 R d\u02c6d . The marginal probability density p t pxptqq evolves according to the forward Kolmogorov equation (Fokker-Planck equation)(\u00d8ksendal, 2003)    \n\nD. 4\n4PROBABILITY FLOW SAMPLING WITH BLACK-BOX ODE SOLVERS For producing figures in Fig. 4, we use a DDPM model trained on 256\u02c6256 CelebA-HQ with the same settings in Ho et al. (2020). We use the RK45 ODE solver (Dormand & Prince, 1980) provided by scipy.integrate.solve_ivp in all cases. The bits/dim values in Tab. 2 are computed with atol=1e-5 and rtol=1e-5, same as Grathwohl et al. (2018). To give the results for DDPM (probability flow) and DDPM cont. (probability flow) in Tab. 2, we average the bits/dim obtained on the test dataset over five different runs. The standard deviation is below 0.005 for both of them. FID scores are computed on samples from the probability flow sampler. Aside from the interpolation results in Fig. 4, we demonstrate more examples of latent space manipulation in Fig. 7, including interpolation and temperature scaling. The model tested here is a DDPM model trained with the same settings in Ho et al. (2020).\n\nFigure 7 :Figure 8 :\n78Samples from the probability flow ODE on 256\u02c6256 CelebA-HQ with VP perturbations. Top: spherical interpolations between random samples. Bottom: temperature rescaling (reducing norm of embedding). Comparing the first 100 dimensions of the latent code obtained for a random CIFAR-10 image. \"Model A\" and \"Model B\" are separately trained with different architectures.\n\nFigure 10 :\n10The effects of different architecture components for score-based models trained with VE perturbations.\n\n\nThe basic NCSN++ model with 4 residual blocks achieves an FID of 2.45 on CIFAR-10. Here in order to match the convention used in Karras et al. (2018); Song & Ermon (2019); Ho et al. (\n\nFigure 12 :\n12Class-conditional image generation by solving the conditional reverse-time SDE with PC. The curve shows the accuracy of our noise-conditional classifier over different noise scales.\n\nFigure 14 :\n14Extended inpainting results for 256\u02c6256 church images.\n\nFigure 15 :\n15Extended colorization results for 256\u02c6256 bedroom images.\n\nFigure 16 :\n16Extended colorization results for 256\u02c6256 church images.\n\nTable 2 :\n2NLLs on CIFAR-10.Model \nNLL Test \u00d3 FID \u00d3 \n\nRealNVP \n\nTable 3 :\n3CIFAR-10 sample quality.Model \nFID\u00d3 \nIS\u00d2 \n\nConditional \n\nBigGAN (Brock et al., 2018) \n14.73 \n9.22 \nStyleGAN2-ADA (Karras et al., 2020a) 2.42 \n10.14 \n\nUnconditional \n\nStyleGAN2-ADA (Karras et al., 2020a) 2.92 \n9.83 \nNCSN (Song & Ermon, 2019) \n25.32 8.87\u02d8.12 \nNCSNv2 (Song & Ermon, 2020) \n10.87 8.40\u02d8.07 \nDDPM (Ho et al., 2020) \n3.17 9.46\u02d8.11 \n\nNCSN++ (4 blocks/resolution) \n2.45 \n9.73 \nNCSN++ cont. (4 blocks/resolution) \n2.38 \n9.83 \nNCSN++ cont. (8 blocks/resolution) \n2.20 \n9.89 \n\n\n\n\n(FID 2.38), and increasing the number of residual blocks (FID 2.2, see Tab. 3 and Appendix H.2). This model sets new records for both inception score and FID on unconditional generation for CIFAR-10. Surprisingly, we can achieve better FID than the previousFigure 5: Left: Class-conditional samples on 32\u02c632 CIFAR-10. Top four rows are automobiles and bottom four rows are horses. Right: Inpainting (top two rows) and colorization (bottom two rows) results on 256\u02c6256 LSUN. First column is the original image, second column is the masked/grayscale image, remaining columns are sampled image completions or colorizations.best conditional generative model without requiring labeled data. Finally, with all improvements together, we are able to obtain the first set of high-fidelity samples on CelebA-HQ 1024\u02c61024 from score-based models (see Appendix H.3).\n\n\nJonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving flowbased generative models with variational dequantization and architecture design. In International Conference on Machine Learning, pp. 2722-2730, 2019. Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. Advances in Neural Information Processing Systems, 33, 2020a. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of StyleGAN. In Proc. CVPR, 2020b. Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. arXiv preprint arXiv:2009.09761, 2020. Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in \nNeural Information Processing Systems, 33, 2020. \n\nMichael F Hutchinson. A stochastic estimator of the trace of the influence matrix for Laplacian \nsmoothing splines. Communications in Statistics-Simulation and Computation, 19(2):433-450, \n1990. \n\nAapo Hyv\u00e4rinen. Estimation of non-normalized statistical models by score matching. Journal of \nMachine Learning Research, 6(Apr):695-709, 2005. \n\nAlexia Jolicoeur-Martineau, R\u00e9mi Pich\u00e9-Taillefer, R\u00e9mi Tachet des Combes, and Ioannis Mitliagkas. \nAdversarial score matching and improved sampling for image generation. arXiv preprint \narXiv:2009.05475, 2020. \n\nTero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for \nimproved quality, stability, and variation. In International Conference on Learning Representations, \n2018. \n\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative \nadversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern \nRecognition, pp. 4401-4410, 2019. \n\nDurk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. In \nAdvances in Neural Information Processing Systems, pp. 10215-10224, 2018. \n\nPeter E Kloeden and Eckhard Platen. Numerical solution of stochastic differential equations, vol-\nume 23. Springer Science & Business Media, 2013. \n\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In \nProceedings of International Conference on Computer Vision (ICCV), December 2015. \n\nDimitra Maoutsa, Sebastian Reich, and Manfred Opper. Interacting particle solutions of fokker-planck \nequations through gradient-log-density estimation. arXiv preprint arXiv:2006.00702, 2020. \n\nRadford M Neal et al. Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo, \n2(11):2, 2011. \n\nChenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and Stefano Ermon. Per-\nmutation invariant graph generation via score-based generative modeling. volume 108 of Pro-\nceedings of Machine Learning Research, pp. 4474-4484, Online, 26-28 Aug 2020. PMLR. URL \nhttp://proceedings.mlr.press/v108/niu20a.html. \n\nBernt \u00d8ksendal. Stochastic differential equations. In Stochastic differential equations, pp. 65-84. \nSpringer, 2003. \n\nTianyu Pang, Kun Xu, Chongxuan Li, Yang Song, Stefano Ermon, and Jun Zhu. Efficient learning of \ngenerative models via finite-difference score matching. arXiv preprint arXiv:2007.03317, 2020. \n\n\n\nAd-hoc interpolation methods for noise scales Models in this experiment are all trained with 1000 noise scales. To get results for P2000 (predictor-only sampler using 2000 steps) which requires 2000 noise scales, we need to interpolate between 1000 noise scales at test time. The specific architecture of the noise-conditional score-based model in Ho et al. (2020) uses sinusoidal positional embeddings for conditioning on integer time steps. This allows us to interpolate between noise scales at test time in an ad-hoc way (while it is hard to do so for other architectures like the one in Song & Ermon\n\nTable 4 :\n4Comparing different samplers on CIFAR-10, where \"P2000\" uses the rounding interpolation between noise scales. Shaded regions are obtained with the same computation (number of score function evaluations). Mean and standard deviation are reported over five sampling runs.Variance Exploding SDE (SMLD) \nVariance Preserving SDE (DDPM) \n\nPredictor \n\nFID\u00d3 \nSampler \nP1000 \nP2000 \nC2000 \nPC1000 \nP1000 \nP2000 \nC2000 \nPC1000 \n\nancestral sampling 4.98\u02d8.06 \n4.92\u02d8.02 \n3.62\u02d8.03 3.24\u02d8.02 3.11\u02d8.03 \n3.21\u02d8.02 \nreverse diffusion \n4.79\u02d8.07 \n4.72\u02d8.07 \n3.60\u02d8.02 3.21\u02d8.02 3.10\u02d8.03 \n3.18\u02d8.01 \nprobability flow \n15.41\u02d8.15 12.87\u02d8.09 \n\n20.43\u02d8.07 \n3.51\u02d8.04 3.59\u02d8.04 3.25\u02d8.04 \n\n19.06\u02d8.06 \n3.06\u02d8.03 \n\n\n\nTable 5 :\n5Optimal signal-to-noise ratios of different samplers. \"P1000\" or \"P2000\": predictor-only samplers using 1000 or 2000 steps. \"C2000\": corrector-only samplers using 2000 steps. \"PC1000\": PC samplers using 1000 predictor and 1000 corrector steps.We consider two datasets: 32\u02c632 CIFAR-10 (Krizhevsky et al., 2009) and 64\u02c664 CelebA(Liu et al.,  2015), which is pre-processed followingSong & Ermon (2020). All models are trained with 1.3M iterations, and we save one checkpoint per 50k iterations. We compare different configurations based on their FID scores averaged over checkpoints after 0.5M iterations. The FIDs are computed on 50k samples with TF-GAN. For sampling, we use the PC sampler discretized at 1000 noise scales. We followHo et al. (2020)  for optimization, including the learning rate, gradient clipping, and learning rate warm-up schedules. Unless otherwise noted, all models are trained with the original SMLD objective Eq. (1) and use a batch size of 128.VE SDE (SMLD) \nVP SDE (DDPM) \n\nPredictor \n\nr \nSampler \nP1000 P2000 C2000 PC1000 P1000 P2000 C2000 PC1000 \n\nancestral sampling \n-\n-\n0.17 \n-\n-\n0.01 \nreverse diffusion \n-\n-\n0.16 \n-\n-\n0.01 \nprobability flow \n-\n-\n\n0.22 \n0.17 \n-\n-\n\n0.27 \n0.04 \n\nH.1 SETTINGS \n\n\nPreprint. Work in progress.Figure 13: Extended inpainting results for 256\u02c6256 bedroom images.\nACKNOWLEDGEMENTSWe would like to thank Nanxin Chen, Ruiqi Gao, Jonathan Ho, and Tim Salimans for their insightful discussions during the course of this project.For imputation, our goal is to sample from pp\u03a9pxp0qq | \u2126pxp0qq \" yq. Define a new diffusion process zptq \"\u03a9pxptqq, and note that the SDE for zptq can be written as dz \" f\u03a9pz, tqdt`G\u03a9pz, tqdw.The reverse-time SDE, conditioned on \u2126pxp0qq \" y, is given byAlthough p t pzptq | \u2126pxp0qq \" yq is in general intractable, it can be approximated. Let A denote the event \u2126pxp0qq \" y. We havewhere\u03a9pxptqq is a random sample from p t p\u2126pxptqq | Aq, which is typically a tractable distribution. Therefore,where rzptq;\u03a9pxptqqs denotes a vector uptq such that \u2126puptqq \"\u03a9pxptqq and\u03a9puptqq \" zptq, and the identity holds because \u2207 z log p t przptq;\u03a9pxptqqsq \" \u2207 z log p t pzptq |\u03a9pxptqqq\u2207 z log pp\u03a9pxptqqq \" \u2207 z log p t pzptq |\u03a9pxptqqq. We provided an extended set of inpainting results in Figs. 13 and 14.I.3 COLORIZATIONColorization is a special case of imputation, except that the known data dimensions are coupled. We can decouple these data dimensions by using an orthogonal linear transformation to map the gray-scale image to a separate channel in a different space, and then perform imputation to complete the other channels before transforming everything back to the original image space. The orthogonal matrix we used to decouple color channels is 0.577\u00b40.816 0 0.577 0.408 0.707 0.577 0.408\u00b40.707\u00b8.Because the transformations are all orthogonal matrices, the standard Wiener process wptq will still be a standard Wiener process in the transformed space, allowing us to build an SDE and use the same imputation method in Appendix I.2.We provide an extended set of colorization results in Figs. 15 and 16.\nNumerical continuation methods: an introduction. L Eugene, Kurt Allgower, Georg, Springer Science & Business Media13Eugene L Allgower and Kurt Georg. Numerical continuation methods: an introduction, volume 13. Springer Science & Business Media, 2012.\n\nReverse-time diffusion equation models. D O Brian, Anderson, Stochastic Process. Appl. 123Brian D O Anderson. Reverse-time diffusion equation models. Stochastic Process. Appl., 12(3): 313-326, May 1982.\n\nInvertible residual networks. Jens Behrmann, Will Grathwohl, T Q Ricky, David Chen, J\u00f6rn-Henrik Duvenaud, Jacobsen, International Conference on Machine Learning. Jens Behrmann, Will Grathwohl, Ricky TQ Chen, David Duvenaud, and J\u00f6rn-Henrik Jacobsen. Invertible residual networks. In International Conference on Machine Learning, pp. 573-582, 2019.\n\nLearning to generate samples from noise through infusion training. Florian Bordes, Sina Honari, Pascal Vincent, arXiv:1703.06975arXiv preprintFlorian Bordes, Sina Honari, and Pascal Vincent. Learning to generate samples from noise through infusion training. arXiv preprint arXiv:1703.06975, 2017.\n\nLarge scale gan training for high fidelity natural image synthesis. Andrew Brock, Jeff Donahue, Karen Simonyan, International Conference on Learning Representations. Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. In International Conference on Learning Representations, 2018.\n\nLearning gradient fields for shape generation. Ruojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun Hao, Serge Belongie, Noah Snavely, Bharath Hariharan, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)2020Ruojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun Hao, Serge Belongie, Noah Snavely, and Bharath Hariharan. Learning gradient fields for shape generation. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.\n\nWavegrad: Estimating gradients for waveform generation. Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, William Chan, arXiv:2009.00713arXiv preprintNanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. arXiv preprint arXiv:2009.00713, 2020.\n\nNeural ordinary differential equations. T Q Ricky, Yulia Chen, Jesse Rubanova, David K Bettencourt, Duvenaud, Advances in neural information processing systems. Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations. In Advances in neural information processing systems, pp. 6571-6583, 2018.\n\nResidual flows for invertible generative modeling. T Q Ricky, Jens Chen, Behrmann, K David, J\u00f6rn-Henrik Duvenaud, Jacobsen, Advances in Neural Information Processing Systems. Ricky TQ Chen, Jens Behrmann, David K Duvenaud, and J\u00f6rn-Henrik Jacobsen. Residual flows for invertible generative modeling. In Advances in Neural Information Processing Systems, pp. 9916-9926, 2019.\n\nLaurent Dinh, arXiv:1605.08803Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv preprintLaurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv preprint arXiv:1605.08803, 2016.\n\nA family of embedded runge-kutta formulae. R John, Dormand, J Peter, Prince, Journal of computational and applied mathematics. 61John R Dormand and Peter J Prince. A family of embedded runge-kutta formulae. Journal of computational and applied mathematics, 6(1):19-26, 1980.\n\nTweedie's formula and selection bias. Bradley Efron, Journal of the American Statistical Association. 106496Bradley Efron. Tweedie's formula and selection bias. Journal of the American Statistical Association, 106(496):1602-1614, 2011.\n\nGenerative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Advances in neural information processing systems. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa- tion processing systems, pp. 2672-2680, 2014.\n\nVariational walkback: Learning a transition operator as a stochastic recurrent net. Anirudh Goyal Alias Parth Goyal, Nan Rosemary Ke, Surya Ganguli, Yoshua Bengio, Advances in Neural Information Processing Systems. Anirudh Goyal Alias Parth Goyal, Nan Rosemary Ke, Surya Ganguli, and Yoshua Bengio. Variational walkback: Learning a transition operator as a stochastic recurrent net. In Advances in Neural Information Processing Systems, pp. 4392-4402, 2017.\n\nFfjord: Free-form continuous dynamics for scalable reversible generative models. Will Grathwohl, T Q Ricky, Jesse Chen, Ilya Bettencourt, David Sutskever, Duvenaud, International Conference on Learning Representations. Will Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. Ffjord: Free-form continuous dynamics for scalable reversible generative models. In International Confer- ence on Learning Representations, 2018.\n\nRepresentations of knowledge in complex systems. Ulf Grenander, Michael, Miller, Journal of the Royal Statistical Society: Series B (Methodological). 564Ulf Grenander and Michael I Miller. Representations of knowledge in complex systems. Journal of the Royal Statistical Society: Series B (Methodological), 56(4):549-581, 1994.\n\nCorrelation functions and computer simulations. Giorgio Parisi, Nuclear Physics B. 1803Giorgio Parisi. Correlation functions and computer simulations. Nuclear Physics B, 180(3):378-384, 1981.\n\nGenerating diverse high-fidelity images with vq-vae-2. Ali Razavi, Aaron Van Den Oord, Oriol Vinyals, Advances in Neural Information Processing Systems. Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. In Advances in Neural Information Processing Systems, pp. 14837-14847, 2019.\n\nGeoffrey Roeder, Luke Metz, Diederik P Kingma, arXiv:2007.00810On linear identifiability of learned representations. arXiv preprintGeoffrey Roeder, Luke Metz, and Diederik P Kingma. On linear identifiability of learned representa- tions. arXiv preprint arXiv:2007.00810, 2020.\n\nApplied stochastic differential equations. Simo S\u00e4rkk\u00e4, Arno Solin, Cambridge University Press10Simo S\u00e4rkk\u00e4 and Arno Solin. Applied stochastic differential equations, volume 10. Cambridge University Press, 2019.\n\nThe eigenvalues of mega-dimensional matrices. John Skilling, Maximum Entropy and Bayesian Methods. SpringerJohn Skilling. The eigenvalues of mega-dimensional matrices. In Maximum Entropy and Bayesian Methods, pp. 455-466. Springer, 1989.\n\nDeep unsupervised learning using nonequilibrium thermodynamics. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, Surya Ganguli, International Conference on Machine Learning. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pp. 2256-2265, 2015.\n\nGenerative modeling by estimating gradients of the data distribution. Yang Song, Stefano Ermon, Advances in Neural Information Processing Systems. Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In Advances in Neural Information Processing Systems, pp. 11895-11907, 2019.\n\nImproved techniques for training score-based generative models. Yang Song, Stefano Ermon, Advances in Neural Information Processing Systems. 33Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. Advances in Neural Information Processing Systems, 33, 2020.\n\nSliced score matching: A scalable approach to density and score estimation. Yang Song, Sahaj Garg, Jiaxin Shi, Stefano Ermon, Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI 2019. the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI 2019Tel Aviv, Israel204Yang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach to density and score estimation. In Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, UAI 2019, Tel Aviv, Israel, July 22-25, 2019, pp. 204, 2019a. URL http://auai.org/uai2019/proceedings/papers/204.pdf.\n\nMintnet: Building invertible neural networks with masked convolutions. Yang Song, Chenlin Meng, Stefano Ermon, Advances in Neural Information Processing Systems. Yang Song, Chenlin Meng, and Stefano Ermon. Mintnet: Building invertible neural networks with masked convolutions. In Advances in Neural Information Processing Systems, pp. 11002-11012, 2019b.\n\nFourier features let networks learn high frequency functions in low dimensional domains. Matthew Tancik, P Pratul, Ben Srinivasan, Sara Mildenhall, Nithin Fridovich-Keil, Utkarsh Raghavan, Ravi Singhal, Jonathan T Ramamoorthi, Ren Barron, Ng, NeurIPSMatthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimensional domains. NeurIPS, 2020.\n\nA connection between score matching and denoising autoencoders. Pascal Vincent, 23Neural computationPascal Vincent. A connection between score matching and denoising autoencoders. Neural computa- tion, 23(7):1661-1674, 2011.\n\nFisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, Jianxiong Xiao, Lsun, arXiv:1506.03365Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprintFisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.\n\nWide residual networks. Sergey Zagoruyko, Nikos Komodakis, arXiv:1605.07146arXiv preprintSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.\n\nMaking convolutional networks shift-invariant again. Richard Zhang, ICML. Preprint. Work in progressRichard Zhang. Making convolutional networks shift-invariant again. In ICML, 2019. Preprint. Work in progress.\n\nthe FID from 2.38 to 2.2 by doubling the number of residual blocks per resolution, resulting in the model denoted as NCSN++ cont. blocks/resolution). All results are summarized in Tab. 3the FID from 2.38 to 2.2 by doubling the number of residual blocks per resolution, resulting in the model denoted as NCSN++ cont. (8 blocks/resolution). All results are summarized in Tab. 3.\n\n)) for around 700k iterations. We use the PC sampler discretized at 2000 steps with one Langevin step per noise scale and a signal-to-noise ratio of 0.15. The scale parameter for the random Fourier feature embeddings is fixed to 4. We use the \"residual\" progressive architecture for the input, and \"output skip\" progressive architecture for the output. We provide samples in Fig. 11. Although these samples are not perfect (e.g., there are visible flaws on facial symmetry), we believe these results are encouraging and can demonstrate the scalability of our approach. Karras, HIGH RESOLUTION IMAGES Encouraged by the success of NCSN++ on CIFAR-10, we proceed to test it on 1024\u02c61024 CelebA-HQ. We used a batch size of 8, increased the EMA rate to 0.9999, and trained the NCSN++ model with the continuous objective (Eq. Future work on more effective architectures are likely to significantly advance the performance of score-based generative models on this taskH.3 HIGH RESOLUTION IMAGES Encouraged by the success of NCSN++ on CIFAR-10, we proceed to test it on 1024\u02c61024 CelebA- HQ (Karras et al., 2018), a task that was previously only achievable by some GAN models and VQ-VAE-2 (Razavi et al., 2019). We used a batch size of 8, increased the EMA rate to 0.9999, and trained the NCSN++ model with the continuous objective (Eq. (7)) for around 700k iterations. We use the PC sampler discretized at 2000 steps with one Langevin step per noise scale and a signal-to-noise ratio of 0.15. The scale parameter for the random Fourier feature embeddings is fixed to 4. We use the \"residual\" progressive architecture for the input, and \"output skip\" progressive architecture for the output. We provide samples in Fig. 11. Although these samples are not perfect (e.g., there are visible flaws on facial symmetry), we believe these results are encouraging and can demonstrate the scalability of our approach. Future work on more effective architectures are likely to significantly advance the performance of score-based generative models on this task.\n\nConsider a forward SDE with the following general form dx \" f px, tqdt`Gpx, tqdw, and suppose the initial state distribution is p 0 pxp0q | yq. The density at time t is p t pxptq | yq when conditioned on y. Therefore, using Anderson (1982), the reverse-time SDE is given by dx. I Controllable Generation, tf px, tq\u00b4\u2207\u00a8rGpx, tqGpx, tq T s\u00b4Gpx, tqGpx, tq T \u2207 x log p t px | yqudt`Gpx, tqdw. (43I CONTROLLABLE GENERATION Consider a forward SDE with the following general form dx \" f px, tqdt`Gpx, tqdw, and suppose the initial state distribution is p 0 pxp0q | yq. The density at time t is p t pxptq | yq when conditioned on y. Therefore, using Anderson (1982), the reverse-time SDE is given by dx \" tf px, tq\u00b4\u2207\u00a8rGpx, tqGpx, tq T s\u00b4Gpx, tqGpx, tq T \u2207 x log p t px | yqudt`Gpx, tqdw. (43)\n\nSince p t pxptq | yq9p t pxptqqppy | xptqq, the score \u2207 x log p t pxptq | yq can be computed easily by \u2207 x log p t pxptq | yq \" \u2207 x log p t pxptqq`\u2207 x log ppy | xptqq. Since p t pxptq | yq9p t pxptqqppy | xptqq, the score \u2207 x log p t pxptq | yq can be computed easily by \u2207 x log p t pxptq | yq \" \u2207 x log p t pxptqq`\u2207 x log ppy | xptqq.\n\nAll sampling methods we have discussed so far can be applied to the conditional reverse-time SDE for sample generation. This subsumes the conditional reverse-time SDE in Eq. (13) as a special caseThis subsumes the conditional reverse-time SDE in Eq. (13) as a special case. All sampling methods we have discussed so far can be applied to the conditional reverse-time SDE for sample generation.\n\nCLASS-CONDITIONAL SAMPLING. I , I.1 CLASS-CONDITIONAL SAMPLING\n\nSince the forward SDE is tractable, we can easily create a pair of training data pxptq, yq by first sampling pxp0q, yq from a dataset and then obtaining xptq. When y represents class labels, we can train a time-dependent classifier p t py | xptqq for classconditional sampling. p 0t pxptq | xp0qqWhen y represents class labels, we can train a time-dependent classifier p t py | xptqq for class- conditional sampling. Since the forward SDE is tractable, we can easily create a pair of training data pxptq, yq by first sampling pxp0q, yq from a dataset and then obtaining xptq \" p 0t pxptq | xp0qq.\n\nWide-ResNet-28-10) on CIFAR-10 with VE perturbations. The classifier is conditioned on log \u03c3 i using random Fourier features (Tancik et al., 2020), and the training objective is a simple sum of cross-entropy losses sampled at different scales. We provide a plot to show the accuracy of this classifier over noise scales in Fig. 12. The score-based model is an unconditional NCSN++ (4 blocks/resolution) in Tab. 3, and we generate samples using the PC algorithm with 2000 discretization steps. The class-conditional samples are provided in Fig. Afterwards, entropy losses over different time steps, like Eq. (7), to train the time-dependent classifier p t py | xptqq. To test this idea, we trained a Wide ResNet. Zagoruyko & Komodakis5and an extended set of conditional samples is given in Fig. 12Afterwards, we may employ a mixture of cross-entropy losses over different time steps, like Eq. (7), to train the time-dependent classifier p t py | xptqq. To test this idea, we trained a Wide ResNet (Zagoruyko & Komodakis, 2016) (Wide-ResNet-28-10) on CIFAR-10 with VE perturbations. The classifier is condi- tioned on log \u03c3 i using random Fourier features (Tancik et al., 2020), and the training objective is a simple sum of cross-entropy losses sampled at different scales. We provide a plot to show the accuracy of this classifier over noise scales in Fig. 12. The score-based model is an unconditional NCSN++ (4 blocks/resolution) in Tab. 3, and we generate samples using the PC algorithm with 2000 discretization steps. The class-conditional samples are provided in Fig. 5, and an extended set of conditional samples is given in Fig. 12.\n\nDenote by \u2126pxq and\u03a9pxq the known and unknown dimensions of x respectively, and let f\u03a9p\u00a8, tq and G\u03a9p\u00a8, tq denote f p\u00a8, tq and Gp\u00a8, tq restricted to the unknown dimensions. For VE/VP SDEs, the drift coefficient f p\u00a8, tq is element-wise, and the diffusion coefficient Gp\u00a8, tq is diagonal. When f p\u00a8, tq is element-wise, f\u03a9p\u00a8, tq denotes the same element-wise function applied only to the unknown dimensions. When Gp\u00a8. 2IMPUTATION Imputation is a special case of conditional sampling. tq is diagonal, G\u03a9p\u00a8, tq denotes the sub-matrix restricted to unknown dimensionsI.2 IMPUTATION Imputation is a special case of conditional sampling. Denote by \u2126pxq and\u03a9pxq the known and un- known dimensions of x respectively, and let f\u03a9p\u00a8, tq and G\u03a9p\u00a8, tq denote f p\u00a8, tq and Gp\u00a8, tq restricted to the unknown dimensions. For VE/VP SDEs, the drift coefficient f p\u00a8, tq is element-wise, and the diffusion coefficient Gp\u00a8, tq is diagonal. When f p\u00a8, tq is element-wise, f\u03a9p\u00a8, tq denotes the same element-wise function applied only to the unknown dimensions. When Gp\u00a8, tq is diagonal, G\u03a9p\u00a8, tq denotes the sub-matrix restricted to unknown dimensions.\n", "annotations": {"author": "[{\"end\":154,\"start\":77},{\"end\":238,\"start\":155},{\"end\":293,\"start\":239},{\"end\":353,\"start\":294},{\"end\":408,\"start\":354},{\"end\":465,\"start\":409},{\"end\":520,\"start\":466},{\"end\":598,\"start\":521},{\"end\":668,\"start\":599},{\"end\":723,\"start\":669}]", "publisher": null, "author_last_name": "[{\"end\":87,\"start\":82},{\"end\":176,\"start\":162},{\"end\":251,\"start\":246},{\"end\":311,\"start\":305},{\"end\":366,\"start\":361},{\"end\":423,\"start\":418},{\"end\":478,\"start\":473},{\"end\":534,\"start\":529},{\"end\":608,\"start\":603},{\"end\":681,\"start\":676}]", "author_first_name": "[{\"end\":81,\"start\":77},{\"end\":161,\"start\":155},{\"end\":245,\"start\":239},{\"end\":302,\"start\":294},{\"end\":304,\"start\":303},{\"end\":360,\"start\":354},{\"end\":417,\"start\":409},{\"end\":472,\"start\":466},{\"end\":528,\"start\":521},{\"end\":602,\"start\":599},{\"end\":675,\"start\":669}]", "author_affiliation": "[{\"end\":153,\"start\":114},{\"end\":237,\"start\":198},{\"end\":292,\"start\":253},{\"end\":352,\"start\":313},{\"end\":407,\"start\":368},{\"end\":464,\"start\":425},{\"end\":519,\"start\":480},{\"end\":597,\"start\":558},{\"end\":667,\"start\":628},{\"end\":722,\"start\":683}]", "title": "[{\"end\":74,\"start\":1},{\"end\":797,\"start\":724}]", "venue": null, "abstract": "[{\"end\":2525,\"start\":826}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3091,\"start\":3062},{\"end\":3107,\"start\":3091},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3562,\"start\":3541},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3581,\"start\":3562},{\"end\":3645,\"start\":3629},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3672,\"start\":3653},{\"end\":3690,\"start\":3672},{\"end\":3717,\"start\":3699},{\"end\":3748,\"start\":3730},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5013,\"start\":4997},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5750,\"start\":5736},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5775,\"start\":5750},{\"end\":5875,\"start\":5870},{\"end\":6677,\"start\":6661},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7666,\"start\":7651},{\"end\":12306,\"start\":12290},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12990,\"start\":12975},{\"end\":13635,\"start\":13618},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":13654,\"start\":13635},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14449,\"start\":14430},{\"end\":14507,\"start\":14488},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":14850,\"start\":14829},{\"end\":14935,\"start\":14919},{\"end\":17533,\"start\":17509},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19475,\"start\":19461},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19500,\"start\":19475},{\"end\":19527,\"start\":19508},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23965,\"start\":23946},{\"end\":24017,\"start\":23992},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":24052,\"start\":24032},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24116,\"start\":24092},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":25554,\"start\":25535},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25573,\"start\":25554},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26008,\"start\":25987},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":26521,\"start\":26501},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29648,\"start\":29623},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":31958,\"start\":31942},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":33292,\"start\":33272},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":36559,\"start\":36538},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":37155,\"start\":37134},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":37663,\"start\":37644},{\"end\":38530,\"start\":38514},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":41696,\"start\":41673},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":41773,\"start\":41757},{\"end\":41790,\"start\":41773},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":49179,\"start\":49162},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":51100,\"start\":51087},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":53417,\"start\":53404},{\"end\":53510,\"start\":53477},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":53659,\"start\":53638},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":53691,\"start\":53670},{\"end\":53729,\"start\":53696},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":53833,\"start\":53813},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":54323,\"start\":54302},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":54358,\"start\":54337},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":57072,\"start\":57052},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":66861,\"start\":66842}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":58218,\"start\":57383},{\"attributes\":{\"id\":\"fig_2\"},\"end\":59168,\"start\":58219},{\"attributes\":{\"id\":\"fig_3\"},\"end\":59557,\"start\":59169},{\"attributes\":{\"id\":\"fig_4\"},\"end\":59675,\"start\":59558},{\"attributes\":{\"id\":\"fig_5\"},\"end\":59861,\"start\":59676},{\"attributes\":{\"id\":\"fig_6\"},\"end\":60058,\"start\":59862},{\"attributes\":{\"id\":\"fig_7\"},\"end\":60128,\"start\":60059},{\"attributes\":{\"id\":\"fig_8\"},\"end\":60201,\"start\":60129},{\"attributes\":{\"id\":\"fig_9\"},\"end\":60273,\"start\":60202},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":60337,\"start\":60274},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":60832,\"start\":60338},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":61689,\"start\":60833},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":65156,\"start\":61690},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":65762,\"start\":65157},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":66450,\"start\":65763},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":67686,\"start\":66451}]", "paragraph": "[{\"end\":3485,\"start\":2541},{\"end\":4120,\"start\":3487},{\"end\":4365,\"start\":4122},{\"end\":5343,\"start\":4367},{\"end\":5420,\"start\":5345},{\"end\":6142,\"start\":5422},{\"end\":6532,\"start\":6144},{\"end\":7076,\"start\":6534},{\"end\":7678,\"start\":7148},{\"end\":7777,\"start\":7680},{\"end\":7782,\"start\":7779},{\"end\":8024,\"start\":7784},{\"end\":8400,\"start\":8099},{\"end\":8643,\"start\":8452},{\"end\":9253,\"start\":8841},{\"end\":9527,\"start\":9353},{\"end\":10396,\"start\":9603},{\"end\":10750,\"start\":10442},{\"end\":11193,\"start\":10780},{\"end\":11749,\"start\":11221},{\"end\":12471,\"start\":11751},{\"end\":12804,\"start\":12473},{\"end\":13127,\"start\":12848},{\"end\":13177,\"start\":13129},{\"end\":13476,\"start\":13179},{\"end\":13796,\"start\":13510},{\"end\":14533,\"start\":13894},{\"end\":15214,\"start\":14535},{\"end\":15517,\"start\":15248},{\"end\":15675,\"start\":15519},{\"end\":16134,\"start\":15729},{\"end\":16258,\"start\":16159},{\"end\":16389,\"start\":16311},{\"end\":17083,\"start\":16391},{\"end\":17284,\"start\":17111},{\"end\":17680,\"start\":17326},{\"end\":17962,\"start\":17682},{\"end\":18097,\"start\":18082},{\"end\":18322,\"start\":18099},{\"end\":18339,\"start\":18324},{\"end\":18660,\"start\":18341},{\"end\":19207,\"start\":18662},{\"end\":20067,\"start\":19240},{\"end\":20599,\"start\":20069},{\"end\":21738,\"start\":20601},{\"end\":22707,\"start\":21740},{\"end\":23082,\"start\":22759},{\"end\":23088,\"start\":23084},{\"end\":23359,\"start\":23137},{\"end\":24494,\"start\":23361},{\"end\":25244,\"start\":24496},{\"end\":25716,\"start\":25246},{\"end\":26275,\"start\":25718},{\"end\":26918,\"start\":26300},{\"end\":27374,\"start\":26920},{\"end\":27501,\"start\":27376},{\"end\":27827,\"start\":27529},{\"end\":27898,\"start\":27829},{\"end\":28488,\"start\":27900},{\"end\":29126,\"start\":28490},{\"end\":29489,\"start\":29141},{\"end\":30170,\"start\":29491},{\"end\":31459,\"start\":30183},{\"end\":31777,\"start\":31501},{\"end\":31927,\"start\":31809},{\"end\":32256,\"start\":31929},{\"end\":32392,\"start\":32258},{\"end\":32398,\"start\":32394},{\"end\":32641,\"start\":32483},{\"end\":33344,\"start\":32643},{\"end\":33668,\"start\":33430},{\"end\":33894,\"start\":33704},{\"end\":34037,\"start\":33896},{\"end\":34609,\"start\":34092},{\"end\":34779,\"start\":34683},{\"end\":35059,\"start\":34805},{\"end\":35156,\"start\":35061},{\"end\":35385,\"start\":35209},{\"end\":35886,\"start\":35445},{\"end\":36065,\"start\":36000},{\"end\":36615,\"start\":36067},{\"end\":36692,\"start\":36641},{\"end\":36959,\"start\":36694},{\"end\":37156,\"start\":36961},{\"end\":37286,\"start\":37285},{\"end\":37369,\"start\":37288},{\"end\":37881,\"start\":37392},{\"end\":38039,\"start\":37935},{\"end\":38335,\"start\":38119},{\"end\":38398,\"start\":38337},{\"end\":38667,\"start\":38400},{\"end\":39183,\"start\":38669},{\"end\":39356,\"start\":39309},{\"end\":39934,\"start\":39868},{\"end\":40444,\"start\":40344},{\"end\":40690,\"start\":40446},{\"end\":40746,\"start\":40692},{\"end\":40957,\"start\":40748},{\"end\":41143,\"start\":40988},{\"end\":41149,\"start\":41145},{\"end\":41431,\"start\":41329},{\"end\":41791,\"start\":41494},{\"end\":41856,\"start\":41793},{\"end\":42517,\"start\":41858},{\"end\":42634,\"start\":42551},{\"end\":42920,\"start\":42690},{\"end\":42926,\"start\":42922},{\"end\":43177,\"start\":42979},{\"end\":43645,\"start\":43261},{\"end\":43760,\"start\":43724},{\"end\":44232,\"start\":43876},{\"end\":44656,\"start\":44234},{\"end\":44802,\"start\":44711},{\"end\":45012,\"start\":44857},{\"end\":45434,\"start\":45014},{\"end\":45601,\"start\":45512},{\"end\":45691,\"start\":45603},{\"end\":46019,\"start\":45693},{\"end\":46209,\"start\":46021},{\"end\":47000,\"start\":46745},{\"end\":47284,\"start\":47041},{\"end\":47473,\"start\":47385},{\"end\":47635,\"start\":47584},{\"end\":48158,\"start\":47928},{\"end\":48422,\"start\":48211},{\"end\":48679,\"start\":48527},{\"end\":49242,\"start\":48736},{\"end\":49912,\"start\":49244},{\"end\":50064,\"start\":49958},{\"end\":50081,\"start\":50066},{\"end\":50231,\"start\":50190},{\"end\":50346,\"start\":50233},{\"end\":50363,\"start\":50348},{\"end\":51396,\"start\":50473},{\"end\":51987,\"start\":51398},{\"end\":52571,\"start\":51989},{\"end\":53307,\"start\":52637},{\"end\":53553,\"start\":53309},{\"end\":54199,\"start\":53555},{\"end\":54494,\"start\":54201},{\"end\":54820,\"start\":54496},{\"end\":55106,\"start\":54822},{\"end\":56807,\"start\":55134},{\"end\":57382,\"start\":56809}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8098,\"start\":8025},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8840,\"start\":8644},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9352,\"start\":9254},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9602,\"start\":9528},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11220,\"start\":11194},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13893,\"start\":13797},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15728,\"start\":15676},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16158,\"start\":16135},{\"attributes\":{\"id\":\"formula_9\"},\"end\":16310,\"start\":16259},{\"attributes\":{\"id\":\"formula_11\"},\"end\":18081,\"start\":17963},{\"attributes\":{\"id\":\"formula_12\"},\"end\":23136,\"start\":23089},{\"attributes\":{\"id\":\"formula_14\"},\"end\":31808,\"start\":31778},{\"attributes\":{\"id\":\"formula_15\"},\"end\":32482,\"start\":32399},{\"attributes\":{\"id\":\"formula_16\"},\"end\":33429,\"start\":33345},{\"attributes\":{\"id\":\"formula_17\"},\"end\":34091,\"start\":34038},{\"attributes\":{\"id\":\"formula_18\"},\"end\":34682,\"start\":34610},{\"attributes\":{\"id\":\"formula_19\"},\"end\":34804,\"start\":34780},{\"attributes\":{\"id\":\"formula_20\"},\"end\":35208,\"start\":35157},{\"attributes\":{\"id\":\"formula_21\"},\"end\":35444,\"start\":35386},{\"attributes\":{\"id\":\"formula_22\"},\"end\":35918,\"start\":35887},{\"attributes\":{\"id\":\"formula_23\"},\"end\":35999,\"start\":35918},{\"attributes\":{\"id\":\"formula_24\"},\"end\":36640,\"start\":36616},{\"attributes\":{\"id\":\"formula_25\"},\"end\":37284,\"start\":37157},{\"attributes\":{\"id\":\"formula_26\"},\"end\":37934,\"start\":37882},{\"attributes\":{\"id\":\"formula_27\"},\"end\":38118,\"start\":38040},{\"attributes\":{\"id\":\"formula_30\"},\"end\":39308,\"start\":39184},{\"attributes\":{\"id\":\"formula_31\"},\"end\":39867,\"start\":39357},{\"attributes\":{\"id\":\"formula_32\"},\"end\":40343,\"start\":39935},{\"attributes\":{\"id\":\"formula_33\"},\"end\":41328,\"start\":41150},{\"attributes\":{\"id\":\"formula_34\"},\"end\":41493,\"start\":41432},{\"attributes\":{\"id\":\"formula_36\"},\"end\":42689,\"start\":42635},{\"attributes\":{\"id\":\"formula_37\"},\"end\":42978,\"start\":42927},{\"attributes\":{\"id\":\"formula_38\"},\"end\":43260,\"start\":43178},{\"attributes\":{\"id\":\"formula_39\"},\"end\":43723,\"start\":43646},{\"attributes\":{\"id\":\"formula_40\"},\"end\":43838,\"start\":43761},{\"attributes\":{\"id\":\"formula_41\"},\"end\":44856,\"start\":44803},{\"attributes\":{\"id\":\"formula_42\"},\"end\":45511,\"start\":45435},{\"attributes\":{\"id\":\"formula_43\"},\"end\":46744,\"start\":46210},{\"attributes\":{\"id\":\"formula_44\"},\"end\":47384,\"start\":47285},{\"attributes\":{\"id\":\"formula_45\"},\"end\":47583,\"start\":47474},{\"attributes\":{\"id\":\"formula_46\"},\"end\":47927,\"start\":47636},{\"attributes\":{\"id\":\"formula_47\"},\"end\":48210,\"start\":48159},{\"attributes\":{\"id\":\"formula_48\"},\"end\":48526,\"start\":48423},{\"attributes\":{\"id\":\"formula_49\"},\"end\":50189,\"start\":50082},{\"attributes\":{\"id\":\"formula_50\"},\"end\":50472,\"start\":50364}]", "table_ref": "[{\"end\":20996,\"start\":20989}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2539,\"start\":2527},{\"attributes\":{\"n\":\"2\"},\"end\":7089,\"start\":7079},{\"attributes\":{\"n\":\"2.1\"},\"end\":7146,\"start\":7092},{\"attributes\":{\"n\":\"2.2\"},\"end\":8450,\"start\":8403},{\"attributes\":{\"n\":\"3\"},\"end\":10440,\"start\":10399},{\"attributes\":{\"n\":\"3.1\"},\"end\":10778,\"start\":10753},{\"attributes\":{\"n\":\"3.2\"},\"end\":12846,\"start\":12807},{\"attributes\":{\"n\":\"3.3\"},\"end\":13508,\"start\":13479},{\"attributes\":{\"n\":\"3.4\"},\"end\":15246,\"start\":15217},{\"attributes\":{\"n\":\"4\"},\"end\":17109,\"start\":17086},{\"attributes\":{\"n\":\"4.1\"},\"end\":17324,\"start\":17287},{\"attributes\":{\"n\":\"4.2\"},\"end\":19238,\"start\":19210},{\"attributes\":{\"n\":\"4.3\"},\"end\":22757,\"start\":22710},{\"attributes\":{\"n\":\"4.4\"},\"end\":26298,\"start\":26278},{\"attributes\":{\"n\":\"5\"},\"end\":27527,\"start\":27504},{\"attributes\":{\"n\":\"6\"},\"end\":29139,\"start\":29129},{\"end\":30181,\"start\":30173},{\"end\":31499,\"start\":31462},{\"end\":33702,\"start\":33671},{\"end\":37390,\"start\":37372},{\"end\":40986,\"start\":40960},{\"end\":42549,\"start\":42520},{\"end\":43874,\"start\":43840},{\"end\":44687,\"start\":44659},{\"end\":44709,\"start\":44690},{\"end\":47039,\"start\":47003},{\"end\":48734,\"start\":48682},{\"end\":49956,\"start\":49915},{\"end\":52635,\"start\":52574},{\"end\":55132,\"start\":55109},{\"end\":57394,\"start\":57384},{\"end\":58224,\"start\":58220},{\"end\":59190,\"start\":59170},{\"end\":59570,\"start\":59559},{\"end\":59874,\"start\":59863},{\"end\":60071,\"start\":60060},{\"end\":60141,\"start\":60130},{\"end\":60214,\"start\":60203},{\"end\":60284,\"start\":60275},{\"end\":60348,\"start\":60339},{\"end\":65773,\"start\":65764},{\"end\":66461,\"start\":66452}]", "table": "[{\"end\":60337,\"start\":60303},{\"end\":60832,\"start\":60374},{\"end\":65156,\"start\":62582},{\"end\":66450,\"start\":66044},{\"end\":67686,\"start\":67432}]", "figure_caption": "[{\"end\":58218,\"start\":57396},{\"end\":59168,\"start\":58226},{\"end\":59557,\"start\":59193},{\"end\":59675,\"start\":59573},{\"end\":59861,\"start\":59678},{\"end\":60058,\"start\":59877},{\"end\":60128,\"start\":60074},{\"end\":60201,\"start\":60144},{\"end\":60273,\"start\":60217},{\"end\":60303,\"start\":60286},{\"end\":60374,\"start\":60350},{\"end\":61689,\"start\":60835},{\"end\":62582,\"start\":61692},{\"end\":65762,\"start\":65159},{\"end\":66044,\"start\":65775},{\"end\":67432,\"start\":66463}]", "figure_ref": "[{\"end\":3842,\"start\":3834},{\"end\":5342,\"start\":5336},{\"end\":10749,\"start\":10743},{\"end\":11269,\"start\":11261},{\"end\":18389,\"start\":18381},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21450,\"start\":21442},{\"end\":22467,\"start\":22461},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23943,\"start\":23935},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25697,\"start\":25691},{\"end\":28436,\"start\":28430},{\"end\":29017,\"start\":29003},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":38822,\"start\":38816},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":39048,\"start\":39042},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":40814,\"start\":40810},{\"end\":44330,\"start\":44324},{\"end\":44341,\"start\":44335},{\"end\":45246,\"start\":45238},{\"end\":49065,\"start\":49059},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":55187,\"start\":55180}]", "bib_author_first_name": "[{\"end\":69588,\"start\":69587},{\"end\":69601,\"start\":69597},{\"end\":69833,\"start\":69830},{\"end\":70028,\"start\":70024},{\"end\":70043,\"start\":70039},{\"end\":70056,\"start\":70055},{\"end\":70058,\"start\":70057},{\"end\":70071,\"start\":70066},{\"end\":70089,\"start\":70078},{\"end\":70417,\"start\":70410},{\"end\":70430,\"start\":70426},{\"end\":70445,\"start\":70439},{\"end\":70715,\"start\":70709},{\"end\":70727,\"start\":70723},{\"end\":70742,\"start\":70737},{\"end\":71040,\"start\":71034},{\"end\":71053,\"start\":71046},{\"end\":71065,\"start\":71060},{\"end\":71086,\"start\":71081},{\"end\":71097,\"start\":71092},{\"end\":71112,\"start\":71108},{\"end\":71129,\"start\":71122},{\"end\":71556,\"start\":71550},{\"end\":71565,\"start\":71563},{\"end\":71578,\"start\":71573},{\"end\":71587,\"start\":71584},{\"end\":71589,\"start\":71588},{\"end\":71605,\"start\":71597},{\"end\":71622,\"start\":71615},{\"end\":71879,\"start\":71878},{\"end\":71881,\"start\":71880},{\"end\":71894,\"start\":71889},{\"end\":71906,\"start\":71901},{\"end\":71922,\"start\":71917},{\"end\":71924,\"start\":71923},{\"end\":72239,\"start\":72238},{\"end\":72241,\"start\":72240},{\"end\":72253,\"start\":72249},{\"end\":72271,\"start\":72270},{\"end\":72290,\"start\":72279},{\"end\":72570,\"start\":72563},{\"end\":72855,\"start\":72854},{\"end\":72872,\"start\":72871},{\"end\":73132,\"start\":73125},{\"end\":73356,\"start\":73353},{\"end\":73373,\"start\":73369},{\"end\":73394,\"start\":73389},{\"end\":73406,\"start\":73402},{\"end\":73416,\"start\":73411},{\"end\":73438,\"start\":73431},{\"end\":73451,\"start\":73446},{\"end\":73469,\"start\":73463},{\"end\":73874,\"start\":73849},{\"end\":73885,\"start\":73882},{\"end\":73894,\"start\":73886},{\"end\":73904,\"start\":73899},{\"end\":73920,\"start\":73914},{\"end\":74309,\"start\":74305},{\"end\":74322,\"start\":74321},{\"end\":74324,\"start\":74323},{\"end\":74337,\"start\":74332},{\"end\":74348,\"start\":74344},{\"end\":74367,\"start\":74362},{\"end\":74728,\"start\":74725},{\"end\":75060,\"start\":75053},{\"end\":75256,\"start\":75253},{\"end\":75270,\"start\":75265},{\"end\":75290,\"start\":75285},{\"end\":75543,\"start\":75535},{\"end\":75556,\"start\":75552},{\"end\":75573,\"start\":75563},{\"end\":75860,\"start\":75856},{\"end\":75873,\"start\":75869},{\"end\":76076,\"start\":76072},{\"end\":76335,\"start\":76329},{\"end\":76356,\"start\":76352},{\"end\":76368,\"start\":76364},{\"end\":76391,\"start\":76386},{\"end\":76732,\"start\":76728},{\"end\":76746,\"start\":76739},{\"end\":77050,\"start\":77046},{\"end\":77064,\"start\":77057},{\"end\":77360,\"start\":77356},{\"end\":77372,\"start\":77367},{\"end\":77385,\"start\":77379},{\"end\":77398,\"start\":77391},{\"end\":78013,\"start\":78009},{\"end\":78027,\"start\":78020},{\"end\":78041,\"start\":78034},{\"end\":78390,\"start\":78383},{\"end\":78400,\"start\":78399},{\"end\":78412,\"start\":78409},{\"end\":78429,\"start\":78425},{\"end\":78448,\"start\":78442},{\"end\":78472,\"start\":78465},{\"end\":78487,\"start\":78483},{\"end\":78505,\"start\":78497},{\"end\":78507,\"start\":78506},{\"end\":78524,\"start\":78521},{\"end\":78878,\"start\":78872},{\"end\":79040,\"start\":79034},{\"end\":79048,\"start\":79045},{\"end\":79060,\"start\":79055},{\"end\":79074,\"start\":79068},{\"end\":79087,\"start\":79081},{\"end\":79109,\"start\":79100},{\"end\":79492,\"start\":79486},{\"end\":79509,\"start\":79504},{\"end\":79713,\"start\":79706},{\"end\":82566,\"start\":82565},{\"end\":83833,\"start\":83832}]", "bib_author_last_name": "[{\"end\":69595,\"start\":69589},{\"end\":69610,\"start\":69602},{\"end\":69617,\"start\":69612},{\"end\":69839,\"start\":69834},{\"end\":69849,\"start\":69841},{\"end\":70037,\"start\":70029},{\"end\":70053,\"start\":70044},{\"end\":70064,\"start\":70059},{\"end\":70076,\"start\":70072},{\"end\":70098,\"start\":70090},{\"end\":70108,\"start\":70100},{\"end\":70424,\"start\":70418},{\"end\":70437,\"start\":70431},{\"end\":70453,\"start\":70446},{\"end\":70721,\"start\":70716},{\"end\":70735,\"start\":70728},{\"end\":70751,\"start\":70743},{\"end\":71044,\"start\":71041},{\"end\":71058,\"start\":71054},{\"end\":71079,\"start\":71066},{\"end\":71090,\"start\":71087},{\"end\":71106,\"start\":71098},{\"end\":71120,\"start\":71113},{\"end\":71139,\"start\":71130},{\"end\":71561,\"start\":71557},{\"end\":71571,\"start\":71566},{\"end\":71582,\"start\":71579},{\"end\":71595,\"start\":71590},{\"end\":71613,\"start\":71606},{\"end\":71627,\"start\":71623},{\"end\":71887,\"start\":71882},{\"end\":71899,\"start\":71895},{\"end\":71915,\"start\":71907},{\"end\":71936,\"start\":71925},{\"end\":71946,\"start\":71938},{\"end\":72247,\"start\":72242},{\"end\":72258,\"start\":72254},{\"end\":72268,\"start\":72260},{\"end\":72277,\"start\":72272},{\"end\":72299,\"start\":72291},{\"end\":72309,\"start\":72301},{\"end\":72575,\"start\":72571},{\"end\":72860,\"start\":72856},{\"end\":72869,\"start\":72862},{\"end\":72878,\"start\":72873},{\"end\":72886,\"start\":72880},{\"end\":73138,\"start\":73133},{\"end\":73367,\"start\":73357},{\"end\":73387,\"start\":73374},{\"end\":73400,\"start\":73395},{\"end\":73409,\"start\":73407},{\"end\":73429,\"start\":73417},{\"end\":73444,\"start\":73439},{\"end\":73461,\"start\":73452},{\"end\":73476,\"start\":73470},{\"end\":73880,\"start\":73875},{\"end\":73897,\"start\":73895},{\"end\":73912,\"start\":73905},{\"end\":73927,\"start\":73921},{\"end\":74319,\"start\":74310},{\"end\":74330,\"start\":74325},{\"end\":74342,\"start\":74338},{\"end\":74360,\"start\":74349},{\"end\":74377,\"start\":74368},{\"end\":74387,\"start\":74379},{\"end\":74738,\"start\":74729},{\"end\":74747,\"start\":74740},{\"end\":74755,\"start\":74749},{\"end\":75067,\"start\":75061},{\"end\":75263,\"start\":75257},{\"end\":75283,\"start\":75271},{\"end\":75298,\"start\":75291},{\"end\":75550,\"start\":75544},{\"end\":75561,\"start\":75557},{\"end\":75580,\"start\":75574},{\"end\":75867,\"start\":75861},{\"end\":75879,\"start\":75874},{\"end\":76085,\"start\":76077},{\"end\":76350,\"start\":76336},{\"end\":76362,\"start\":76357},{\"end\":76384,\"start\":76369},{\"end\":76399,\"start\":76392},{\"end\":76737,\"start\":76733},{\"end\":76752,\"start\":76747},{\"end\":77055,\"start\":77051},{\"end\":77070,\"start\":77065},{\"end\":77365,\"start\":77361},{\"end\":77377,\"start\":77373},{\"end\":77389,\"start\":77386},{\"end\":77404,\"start\":77399},{\"end\":78018,\"start\":78014},{\"end\":78032,\"start\":78028},{\"end\":78047,\"start\":78042},{\"end\":78397,\"start\":78391},{\"end\":78407,\"start\":78401},{\"end\":78423,\"start\":78413},{\"end\":78440,\"start\":78430},{\"end\":78463,\"start\":78449},{\"end\":78481,\"start\":78473},{\"end\":78495,\"start\":78488},{\"end\":78519,\"start\":78508},{\"end\":78531,\"start\":78525},{\"end\":78535,\"start\":78533},{\"end\":78886,\"start\":78879},{\"end\":79043,\"start\":79041},{\"end\":79053,\"start\":79049},{\"end\":79066,\"start\":79061},{\"end\":79079,\"start\":79075},{\"end\":79098,\"start\":79088},{\"end\":79114,\"start\":79110},{\"end\":79120,\"start\":79116},{\"end\":79502,\"start\":79493},{\"end\":79519,\"start\":79510},{\"end\":79719,\"start\":79714},{\"end\":80818,\"start\":80812},{\"end\":82590,\"start\":82567},{\"end\":85020,\"start\":85010}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":69788,\"start\":69538},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3897405},\"end\":69992,\"start\":69790},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":53295271},\"end\":70341,\"start\":69994},{\"attributes\":{\"doi\":\"arXiv:1703.06975\",\"id\":\"b3\"},\"end\":70639,\"start\":70343},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":52889459},\"end\":70985,\"start\":70641},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":221139756},\"end\":71492,\"start\":70987},{\"attributes\":{\"doi\":\"arXiv:2009.00713\",\"id\":\"b6\"},\"end\":71836,\"start\":71494},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":49310446},\"end\":72185,\"start\":71838},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":174801267},\"end\":72561,\"start\":72187},{\"attributes\":{\"doi\":\"arXiv:1605.08803\",\"id\":\"b9\"},\"end\":72809,\"start\":72563},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":122754533},\"end\":73085,\"start\":72811},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":23284154},\"end\":73322,\"start\":73087},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":1033682},\"end\":73763,\"start\":73324},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":27248616},\"end\":74222,\"start\":73765},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":52908831},\"end\":74674,\"start\":74224},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":117719085},\"end\":75003,\"start\":74676},{\"attributes\":{\"id\":\"b16\"},\"end\":75196,\"start\":75005},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":173990382},\"end\":75533,\"start\":75198},{\"attributes\":{\"doi\":\"arXiv:2007.00810\",\"id\":\"b18\"},\"end\":75811,\"start\":75535},{\"attributes\":{\"id\":\"b19\"},\"end\":76024,\"start\":75813},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":117844915},\"end\":76263,\"start\":76026},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":14888175},\"end\":76656,\"start\":76265},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":196470871},\"end\":76980,\"start\":76658},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":219708245},\"end\":77278,\"start\":76982},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":158047026},\"end\":77936,\"start\":77280},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":197544848},\"end\":78292,\"start\":77938},{\"attributes\":{\"id\":\"b26\"},\"end\":78806,\"start\":78294},{\"attributes\":{\"id\":\"b27\"},\"end\":79032,\"start\":78808},{\"attributes\":{\"doi\":\"arXiv:1506.03365\",\"id\":\"b28\"},\"end\":79460,\"start\":79034},{\"attributes\":{\"doi\":\"arXiv:1605.07146\",\"id\":\"b29\"},\"end\":79651,\"start\":79462},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":53416878},\"end\":79863,\"start\":79653},{\"attributes\":{\"id\":\"b31\"},\"end\":80241,\"start\":79865},{\"attributes\":{\"id\":\"b32\"},\"end\":82285,\"start\":80243},{\"attributes\":{\"id\":\"b33\"},\"end\":83070,\"start\":82287},{\"attributes\":{\"id\":\"b34\"},\"end\":83407,\"start\":83072},{\"attributes\":{\"id\":\"b35\"},\"end\":83802,\"start\":83409},{\"attributes\":{\"id\":\"b36\"},\"end\":83866,\"start\":83804},{\"attributes\":{\"id\":\"b37\"},\"end\":84464,\"start\":83868},{\"attributes\":{\"id\":\"b38\"},\"end\":86105,\"start\":84466},{\"attributes\":{\"id\":\"b39\"},\"end\":87235,\"start\":86107}]", "bib_title": "[{\"end\":69828,\"start\":69790},{\"end\":70022,\"start\":69994},{\"end\":70707,\"start\":70641},{\"end\":71032,\"start\":70987},{\"end\":71876,\"start\":71838},{\"end\":72236,\"start\":72187},{\"end\":72852,\"start\":72811},{\"end\":73123,\"start\":73087},{\"end\":73351,\"start\":73324},{\"end\":73847,\"start\":73765},{\"end\":74303,\"start\":74224},{\"end\":74723,\"start\":74676},{\"end\":75051,\"start\":75005},{\"end\":75251,\"start\":75198},{\"end\":76070,\"start\":76026},{\"end\":76327,\"start\":76265},{\"end\":76726,\"start\":76658},{\"end\":77044,\"start\":76982},{\"end\":77354,\"start\":77280},{\"end\":78007,\"start\":77938},{\"end\":79704,\"start\":79653},{\"end\":80810,\"start\":80243},{\"end\":85008,\"start\":84466},{\"end\":86510,\"start\":86107}]", "bib_author": "[{\"end\":69597,\"start\":69587},{\"end\":69612,\"start\":69597},{\"end\":69619,\"start\":69612},{\"end\":69841,\"start\":69830},{\"end\":69851,\"start\":69841},{\"end\":70039,\"start\":70024},{\"end\":70055,\"start\":70039},{\"end\":70066,\"start\":70055},{\"end\":70078,\"start\":70066},{\"end\":70100,\"start\":70078},{\"end\":70110,\"start\":70100},{\"end\":70426,\"start\":70410},{\"end\":70439,\"start\":70426},{\"end\":70455,\"start\":70439},{\"end\":70723,\"start\":70709},{\"end\":70737,\"start\":70723},{\"end\":70753,\"start\":70737},{\"end\":71046,\"start\":71034},{\"end\":71060,\"start\":71046},{\"end\":71081,\"start\":71060},{\"end\":71092,\"start\":71081},{\"end\":71108,\"start\":71092},{\"end\":71122,\"start\":71108},{\"end\":71141,\"start\":71122},{\"end\":71563,\"start\":71550},{\"end\":71573,\"start\":71563},{\"end\":71584,\"start\":71573},{\"end\":71597,\"start\":71584},{\"end\":71615,\"start\":71597},{\"end\":71629,\"start\":71615},{\"end\":71889,\"start\":71878},{\"end\":71901,\"start\":71889},{\"end\":71917,\"start\":71901},{\"end\":71938,\"start\":71917},{\"end\":71948,\"start\":71938},{\"end\":72249,\"start\":72238},{\"end\":72260,\"start\":72249},{\"end\":72270,\"start\":72260},{\"end\":72279,\"start\":72270},{\"end\":72301,\"start\":72279},{\"end\":72311,\"start\":72301},{\"end\":72577,\"start\":72563},{\"end\":72862,\"start\":72854},{\"end\":72871,\"start\":72862},{\"end\":72880,\"start\":72871},{\"end\":72888,\"start\":72880},{\"end\":73140,\"start\":73125},{\"end\":73369,\"start\":73353},{\"end\":73389,\"start\":73369},{\"end\":73402,\"start\":73389},{\"end\":73411,\"start\":73402},{\"end\":73431,\"start\":73411},{\"end\":73446,\"start\":73431},{\"end\":73463,\"start\":73446},{\"end\":73478,\"start\":73463},{\"end\":73882,\"start\":73849},{\"end\":73899,\"start\":73882},{\"end\":73914,\"start\":73899},{\"end\":73929,\"start\":73914},{\"end\":74321,\"start\":74305},{\"end\":74332,\"start\":74321},{\"end\":74344,\"start\":74332},{\"end\":74362,\"start\":74344},{\"end\":74379,\"start\":74362},{\"end\":74389,\"start\":74379},{\"end\":74740,\"start\":74725},{\"end\":74749,\"start\":74740},{\"end\":74757,\"start\":74749},{\"end\":75069,\"start\":75053},{\"end\":75265,\"start\":75253},{\"end\":75285,\"start\":75265},{\"end\":75300,\"start\":75285},{\"end\":75552,\"start\":75535},{\"end\":75563,\"start\":75552},{\"end\":75582,\"start\":75563},{\"end\":75869,\"start\":75856},{\"end\":75881,\"start\":75869},{\"end\":76087,\"start\":76072},{\"end\":76352,\"start\":76329},{\"end\":76364,\"start\":76352},{\"end\":76386,\"start\":76364},{\"end\":76401,\"start\":76386},{\"end\":76739,\"start\":76728},{\"end\":76754,\"start\":76739},{\"end\":77057,\"start\":77046},{\"end\":77072,\"start\":77057},{\"end\":77367,\"start\":77356},{\"end\":77379,\"start\":77367},{\"end\":77391,\"start\":77379},{\"end\":77406,\"start\":77391},{\"end\":78020,\"start\":78009},{\"end\":78034,\"start\":78020},{\"end\":78049,\"start\":78034},{\"end\":78399,\"start\":78383},{\"end\":78409,\"start\":78399},{\"end\":78425,\"start\":78409},{\"end\":78442,\"start\":78425},{\"end\":78465,\"start\":78442},{\"end\":78483,\"start\":78465},{\"end\":78497,\"start\":78483},{\"end\":78521,\"start\":78497},{\"end\":78533,\"start\":78521},{\"end\":78537,\"start\":78533},{\"end\":78888,\"start\":78872},{\"end\":79045,\"start\":79034},{\"end\":79055,\"start\":79045},{\"end\":79068,\"start\":79055},{\"end\":79081,\"start\":79068},{\"end\":79100,\"start\":79081},{\"end\":79116,\"start\":79100},{\"end\":79122,\"start\":79116},{\"end\":79504,\"start\":79486},{\"end\":79521,\"start\":79504},{\"end\":79721,\"start\":79706},{\"end\":80820,\"start\":80812},{\"end\":82592,\"start\":82565},{\"end\":83836,\"start\":83832},{\"end\":85022,\"start\":85010}]", "bib_venue": "[{\"end\":71256,\"start\":71207},{\"end\":77597,\"start\":77502},{\"end\":69585,\"start\":69538},{\"end\":69875,\"start\":69851},{\"end\":70154,\"start\":70110},{\"end\":70408,\"start\":70343},{\"end\":70805,\"start\":70753},{\"end\":71205,\"start\":71141},{\"end\":71548,\"start\":71494},{\"end\":71997,\"start\":71948},{\"end\":72360,\"start\":72311},{\"end\":72666,\"start\":72593},{\"end\":72936,\"start\":72888},{\"end\":73187,\"start\":73140},{\"end\":73527,\"start\":73478},{\"end\":73978,\"start\":73929},{\"end\":74441,\"start\":74389},{\"end\":74824,\"start\":74757},{\"end\":75086,\"start\":75069},{\"end\":75349,\"start\":75300},{\"end\":75650,\"start\":75598},{\"end\":75854,\"start\":75813},{\"end\":76123,\"start\":76087},{\"end\":76445,\"start\":76401},{\"end\":76803,\"start\":76754},{\"end\":77121,\"start\":77072},{\"end\":77500,\"start\":77406},{\"end\":78098,\"start\":78049},{\"end\":78381,\"start\":78294},{\"end\":78870,\"start\":78808},{\"end\":79225,\"start\":79138},{\"end\":79484,\"start\":79462},{\"end\":79725,\"start\":79721},{\"end\":79993,\"start\":79865},{\"end\":80936,\"start\":80820},{\"end\":82563,\"start\":82287},{\"end\":83238,\"start\":83072},{\"end\":83527,\"start\":83409},{\"end\":83830,\"start\":83804},{\"end\":84025,\"start\":83868},{\"end\":85176,\"start\":85022},{\"end\":86520,\"start\":86512}]"}}}, "year": 2023, "month": 12, "day": 17}