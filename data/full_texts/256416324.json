{"id": 256416324, "updated": "2023-10-05 04:26:26.173", "metadata": {"title": "Image Shortcut Squeezing: Countering Perturbative Availability Poisons with Compression", "authors": "[{\"first\":\"Zhuoran\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Zhengyu\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Martha\",\"last\":\"Larson\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Perturbative availability poisons (PAPs) add small changes to images to prevent their use for model training. Current research adopts the belief that practical and effective approaches to countering PAPs do not exist. In this paper, we argue that it is time to abandon this belief. We present extensive experiments showing that 12 state-of-the-art PAP methods are vulnerable to Image Shortcut Squeezing (ISS), which is based on simple compression. For example, on average, ISS restores the CIFAR-10 model accuracy to $81.73\\%$, surpassing the previous best preprocessing-based countermeasures by $37.97\\%$ absolute. ISS also (slightly) outperforms adversarial training and has higher generalizability to unseen perturbation norms and also higher efficiency. Our investigation reveals that the property of PAP perturbations depends on the type of surrogate model used for poison generation, and it explains why a specific ISS compression yields the best performance for a specific type of PAP perturbation. We further test stronger, adaptive poisoning, and show it falls short of being an ideal defense against ISS. Overall, our results demonstrate the importance of considering various (simple) countermeasures to ensure the meaningfulness of analysis carried out during the development of PAP methods.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2301.13838", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/Liu0L23", "doi": "10.48550/arxiv.2301.13838"}}, "content": {"source": {"pdf_hash": "9e3125c041e96be417ae53c7f9d02508234e4751", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2301.13838v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "959f436f17373f594e6077aeb050cd3bda130684", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9e3125c041e96be417ae53c7f9d02508234e4751.txt", "contents": "\nImage Shortcut Squeezing: Countering Perturbative Availability Poisons with Compression\n2023\n\nZhuoran Liu \nRadboud University\nNijmegenNetherlands\n\nZhengyu Zhao \nXi'an Jiaotong University\nXi'anChina\n\nCISPA Helmholtz Center for Information Security\nSaarbr\u00fcckenGermany\n\nMartha Larson <m.larson@cs.ru.nl>. \nRadboud University\nNijmegenNetherlands\n\nImage Shortcut Squeezing: Countering Perturbative Availability Poisons with Compression\n\nProceedings of the 40 th International Conference on Machine Learning\nthe 40 th International Conference on Machine LearningHonolulu, Hawaii, USA2023Correspondence to: Zhuoran Liu <z.liu@cs.ru.nl>, Zhengyu Zhao <zhengyu.zhao@cispa.de>, Martha Larson\nPerturbative availability poisons (PAPs) add small changes to images to prevent their use for model training. Current research adopts the belief that practical and effective approaches to countering PAPs do not exist. In this paper, we argue that it is time to abandon this belief. We present extensive experiments showing that 12 state-of-the-art PAP methods are vulnerable to Image Shortcut Squeezing (ISS), which is based on simple compression. For example, on average, ISS restores the CIFAR-10 model accuracy to 81.73%, surpassing the previous best preprocessing-based countermeasures by 37.97% absolute. ISS also (slightly) outperforms adversarial training and has higher generalizability to unseen perturbation norms and also higher efficiency. Our investigation reveals that the property of PAP perturbations depends on the type of surrogate model used for poison generation, and it explains why a specific ISS compression yields the best performance for a specific type of PAP perturbation. We further test stronger, adaptive poisoning, and show it falls short of being an ideal defense against ISS. Overall, our results demonstrate the importance of considering various (simple) countermeasures to ensure the meaningfulness of analysis carried out during the development of PAP methods. Our code is available at https://github.com/ liuzrcc/ImageShortcutSqueezing.\n\nIntroduction\n\nThe ever-growing amount of data that is easily available online has driven the tremendous advances of deep neural networks (DNNs) (Schmidhuber, 2015;LeCun et al., 2015;He et al., 2016;Brown et al., 2020). However, online data may be proprietary or contain private information, raising concerns about unauthorized use. Perturbative availability poisons (PAPs) are recognized as a promising approach to data protection and recently a large number of PAP methods have been proposed that add perturbations to images which block training by acting as shortcuts Huang et al., 2021;Fowl et al., 2021b;a). As illustrated by Figure 1 (a)\u2192(b), the high test accuracy of a DNN model is substantially reduced by PAPs.\n\nExisting research has shown that PAPs can be compromised to a limited extent by preprocessing-based-countermeasures, such as data augmentations (Huang et al., 2021;Fowl et al., 2021b) and pre-filtering Chen et al., 2023). However, a widely adopted belief is that no approaches exist that are capable of effectively countering PAPs. Adversarial training (AT) has been proven to be a strong countermeasure Wen et al., 2023). However, it is not considered to be a practical one, since it requires a large amount of computation and also gives rise to a non-negligible trade-off in test accuracy of the clean (non-poisoned) model (Madry et al., 2018;Zhang et al., 2019). Further, AT trained with a specific L p norm is hard to generalize to other norms (Tramer & Boneh, 2019;Laidlaw et al., 2021).\n\nIn this paper, we challenge the belief that it is impossible to counter PAP methods both easily and effectively by demonstrating that they are vulnerable to simple compression. First, we categorize 12 PAP methods into three categories with respect to the surrogate models they use during poison generation: slightly-trained Huang et al., 2021;Yuan & Wu, 2021;van Vlijmen et al., 2022), fully-trained Fowl et al., 2021b;Chen et al., 2023), and surrogatefree Yu et al., 2022;. Then, we analyze perturbations/shortcuts that are learned with these methods and demonstrate that they are strongly dependent on features that are learned in different training stages of the model. Specifically, we find that the methods using a slightly-trained surrogate model prefer lowfrequency shortcuts, while those using a fully-trained model prefer high-frequency shortcuts.\n\nBuilding on this new understanding, we propose Image Shortcut Squeezing (ISS), a simple, compression-based approach to countering PAPs. As illustrated by Figure 1 (b)\u2192(c), the low test accuracy of the poisoned DNN model is restored by our ISS to be close to the original accuracy. In particular, grayscale compression is used to eliminate low-frequency shortcuts, and JPEG compression is used to eliminate high-frequency shortcuts. We also show that our understanding of high vs. low frequency can also help eliminate surrogate-free PAPs Yu et al., 2022;. Our ISS substantially outperforms previously studied data augmentation and prefiltering countermeasures. ISS also achieves comparable results to adversarial training and has three main advantages: 1) generalizability to multiple L p norms, 2) efficiency, and 3) low trade-off in clean model accuracy (see Section 4.2 for details).\n\nWe further test the performance of ISS against potentially stronger PAP methods that are aware of ISS and can be adapted to it. We show that they are not ideal against our ISS. Overall, we hope our study can inspire more meaningful analyses of PAP methods and encourage future research to evaluate various (simple) countermeasures when developing new PAP methods.\n\nIn sum, we make the following main contributions:\n\n\u2022 We identify the strong dependency of the perturbation frequency patterns on the nature of the surrogate model. Based on this new insight, we show that 12 existing perturbative availability poison (PAP) methods are indeed very vulnerable to simple image compression.\n\n\u2022 We propose Image Shortcut Squeezing (ISS), a simple yet effective approach to countering PAPs. ISS applies image compression operations, such as JPEG and grayscale, to poisoned images for restoring the model accuracy.\n\n\u2022 We demonstrate that ISS outperforms existing data augmentation and pre-filtering countermeasures by a large margin and is comparable to adversarial training but is more generalizable to multiple L p norms and more efficient.\n\n\u2022 We explore stronger, adaptive poisons against our ISS and provide interesting insights into understanding PAPs, e.g., about the model learning preference of different perturbations.\n\n\nRelated Work\n\n\nPerturbative Availability Poison (PAP)\n\nPerturbative availability poison (PAP) has been extensively studied. TensorClog (TC)  optimizes the poisons by exploiting the parameters of a pre-trained surrogate to cause the gradient to vanish. Deep Confuse (DC)  collects the training trajectories of a surrogate classifier for learning a poison generator, which is computationally intensive. Error-Minimizing (EM) poisons (Huang et al., 2021) minimizes the classification errors of images on a surrogate classifier with respect to their original labels in order to make them \"unlearnable examples\". The surrogate is also alternatively updated to mimic the model training dynamics during poison generation. Hypocritical (HYPO)  follows a similar idea to EM but uses a pre-trained surrogate rather than the above bi-level optimization. Targeted Adversarial Poisoning (TAP)  also exploits a pretrained model but minimizes classification errors of images with respect to incorrect target labels rather than original labels.\n\nRobust Error-Minimizing (REM)  improves the poisoning effects against adversarial training (with a relatively small norm) by replacing the normally-trained surrogate in EM with an adversarially-trained model. Similar approaches (Wang et al., 2021;Wen et al., 2023) on poisoning against adversarial training are also proposed. The usability of poisoning is also validated in scenarios requiring transferability (Ren et al., 2023) or involving unsupervised learning (He et al., 2023;Zhang et al., 2023).\n\nThere are also studies focusing on revising the surrogate, e.g., Self-Ensemble Protection , which aggregates multiple training model checkpoints, and NTGA (Yuan & Wu, 2021), which adopts the generalized neural tangent kernel to model the surrogate as Gaussian\n\nProcesses (Jacot et al., 2018). ShortcutGen (SG) (van Vlijmen et al., 2022) learns a poison generator based on a randomly initialized fixed surrogate and shows its efficiency compared to the earlier generative method, Deep Confuse.\n\nDifferent from all the above methods, recent studies also explore surrogate-free PAPs (Evtimov et al., 2021;Yu et al., 2022;. Intuitively, simple patterns, such as random noise (Huang et al., 2021) and semantics (e.g., MNIST-like digits) (Evtimov et al., 2021), can be used as learning shortcuts when they form different distributions for different classes. Very recent studies also synthesize more complex, linear separable patterns to boost the poisoning performance based on sampling from a high dimensional Gaussian distribution (Yu et al., 2022) and further refining it by introducing the autoregressive process . One Pixel Shortcut (OPS) specifically explores the model vulnerability to sparse poisons and shows that perturbing only one pixel is sufficient to generate strong poisons .\n\nIn the domain of facial recognition, PAP methods, e.g., Fawkes (Shan et al., 2020) and LowKey (Cherepanova et al., 2021), have also been studied. However, their protection algorithms closely resemble the PAPs as discussed above. Specifically, Fawkes adopts a feature-layer loss similar to SEP and a robust surrogate model similar to REM, to boost transferability. LowKey adopts ensemble surrogate models similar to SEP and a pre-processing step similar to TAP, to boost transferability and imperceptibility.\n\nIn this paper, we evaluate our ISS against 12 representative PAP methods as presented above. In particular, we consider poisons constrained by different L p norms. Because of their technical similarity to two of the 12 approaches, we do not consider Fawkes and LowKey in our evaluation.\n\n\nPAP Countermeasures\n\nAs mentioned in Section 1, existing research has mainly relied on adversarial training (AT) for countering PAPs Wen et al., 2023). However, AT is not practical due to the requirement of large computations and the non-negligible trade-off in test accuracy of the clean model (Madry et al., 2018;Zhang et al., 2019). In addition, image preprocessing, e.g., data augmentations (Huang et al., 2021;Fowl et al., 2021b) and pre-filtering Chen et al., 2023), also show substantial effects but not comparable to AT. In the domain of face recognition, countermeasures are also discussed but either require stronger assumptions or lack a concrete algorithm (Radiya-Dixit et al., 2022). See more discussions in Appendix D.\n\nIn this paper, we compare our ISS against existing countermeasures and particularly highlight its generalizability to unknown norms (Tramer & Boneh, 2019;Laidlaw et al., 2021) and simplicity.\n\n\nAdversarial Perturbations and Countermeasures\n\nSimple image compressions, such as JPEG, bit depth reduction, and smoothing, are effective for countering adversarial perturbations based on the assumption that they are inherently high-frequency noise (Dziugaite et al., 2016;Das et al., 2017;Xu et al., 2017). Other image transformations commonly used for data augmentations, e.g., resizing, rotating, and shifting, are also shown to be effective Tian et al., 2018;Dong et al., 2019). However, such image pre-processing operations may be bypassed when the attacker is aware of them and then adapted to them (Carlini et al., 2019). Differently, adversarial training (AT) is effective against adaptive attacks and is considered to be the most powerful defense so far (Tramer et al., 2020).\n\nBesides (training-time) data poisons, adversarial perturbations can also be used for data protection, but at inference time. Related research has explored person-related recognition (Oh et al., 2016;Sattar et al., 2020;Rajabi et al., 2021) and social media mining (Larson et al., 2018;Li et al., 2019;Liu et al., 2020). An overview of inference-time data protection in images is provided by (Orekondy et al., 2017).\n\nOur ISS is based on compression. We specifically evaluate its compression effects in Section 4.5.\n\n\nAnalysis of Perturbative Availability Poisons\n\n\nProblem Formulation\n\nWe formulate the problem of countering perturbative availability poisons (PAPs) in the context of image classification. There are two parties involved, the data protector and exploiter. The data protector poisons their own images to prevent them from being used by the exploiter for training a well-generalizable classifier. Specifically, here the poisoning is achieved by adding imperceptible perturbations. The data exploiter is aware that their collected images may contain poisons and so apply countermeasures to ensure their trained classifier is still well-generalizable. The success of the countermeasure is measured by the accuracy of the classifier on clean test images, and the higher, the more successful.\n\nFormally stated, the protector aims to make a classifier F generalize poorly on the clean image distribution D, from which the clean training set S is sampled:  where \u03b8(\u03b4) represents the parameters of the poisoned classifier, F , where \u03b4 denotes the additive perturbations with \u03f5 as the L p bound. L(\u00b7; \u00b7) is the cross-entropy loss, which takes as input a pair of model output F (x i ; \u03b8) and the corresponding label y i .\nmax \u03b4 E (x,y)\u223cD L F (x; \u03b8 \u2032 (\u03b4)), y(1)s.t. \u03b8 \u2032 (\u03b4) = argmin \u03b8(\u03b4) (xi,yi)\u2208S L(F (x i + \u03b4 i ; \u03b8(\u03b4), y i ),(2)\nThe exploiter aims to counter the poisons by applying a countermeasure C to restore the model accuracy even when it is trained on poisoned data P:\nmin \u03b8 (xi,yi)\u2208P L(F (C(x i + \u03b4 i ); \u03b8), y i ).(3)\n\nCategorization of Existing PAP Methods\n\nWe carried out an extensive survey of existing PAP methods, which allowed us to identify three categories of them regarding the type of their used surrogate classifiers. These three categories are: Generating poisons 1) with a slightlytrained surrogate, 2) with a fully-trained surrogate, and 3) in a surrogate-free manner. Table 1 provides an overview of this categorization. In the first category, the surrogate is at its early training stage. Existing methods in this category either fixes (Yuan & Wu, 2021;van Vlijmen et al., 2022) or alternatively updates Huang et al., 2021; the surrogate during optimizing the poisons.\n\nIn the second category, the surrogate has been fully trained. Existing methods in this category fix the surrogate Fowl et al., 2021b;Chen et al., 2023) but in principle, it may also be possible that the model is alternatively updated. In the third category, no surrogate is used but the poisons are synthesized by sampling from Gaussian distributions (Yu et al., 2022; or optimized with a perceptual loss .\n\n\nFrequency-based Interpretation of Perturbations\n\nPoisoned CIFAR-10 images and their corresponding perturbations for the 12 methods are visualized in Figure 2. As can be seen, the four methods that adopt a fully-trained surrogate tend to generate perturbations in patterns having a high spatial frequency. This is consistent with the common  finding in the adversarial example literature that adversarial perturbations are normally high-frequency (Guo et al., 2019). In contrast, the five methods that adopt a slighttrained surrogate exhibit spatially low-frequency patterns but large differences across color channels.\n\nWe hypothesize that the above phenomenon can be explained by the frequency principle (Rahaman et al., 2019;Xu et al., 2019;Luo et al., 2021), that is, deep neural networks often fit target functions from low to high frequencies during training. Accordingly, the poisons optimized against a slightly-trained model capture low-frequency patterns while those optimized against a fully-trained model capture high-frequency patterns (Rahaman et al., 2019;Xu et al., 2019;Luo et al., 2021). In order to validate this hypothesis, we further try optimizing poisons using either the error-minimizing or error-maximizing loss against the surrogate at its various training epochs. We visualize the resulting poisoned images and their corresponding perturbations in Figure 3. As can be been, the spatial frequency of the perturbations gets increasingly higher as the surrogate goes to a later training epoch.\n\nDifferent from those surrogate-based methods, the three surrogate-free methods have full control of the perturbation patterns they aim to synthesize. However, we notice that they still follow our frequency-based interpretation of perturbation patterns. Specifically, the perturbations of LSP (Yu et al., 2022) are uniformly upsampled from a Gaussian distribution and so exhibit patch-based low-frequency patterns. On the other hand, the perturbations of AR  are generally based on sliding convolutions over the image and so exhibit texture-based highfrequency patterns. OPS  perturbations only contain one pixel and so can be treated as an extreme case of high-frequency patterns.\n\n\nOur Image Shortcut Squeezing\n\nBased on the above new frequency-based interpretation, we propose Image Shortcut Squeezing (ISS), a simple, image compression-based countermeasure against PAPs. We rely on different compression operations suitable for eliminating different types of perturbations. Overall, a specific compression operation is applied to the C(\u00b7) in Eq. 3.\n\nFor perturbations with low frequency but large differences across color channels, we use grayscale transformation to suppress such color differences. We expect grayscale transformation to not sacrifice too much the test accuracy of a clean model because color information is known to contribute little to the DNNs' performance in differentiating objects (Xie & Richmond, 2018). For perturbations with high frequency, we follow existing research on eliminating adversarial perturbations to use common image compression operations, such as JPEG and bit depth reduction (BDR) (Dziugaite et al., 2016;Das et al., 2017;Xu et al., 2017). We expect grayscale transformation to not sacrifice too much the test accuracy of a clean model because DNNs are known to be resilient to small amounts of image compression, e.g., JPEG with a higher quality factor than 10 (Dodge & Karam, 2016).\n\n\nExperiments\n\nIn this section, we evaluate our Image Shortcut Squeezing (ISS) and other countermeasures against 12 representative PAP methods. We focus our experiments on the basic setting in which the surrogate (if it is used) and target models are the same and the whole training set is poisoned. We also explore more challenging poisoning scenarios with unseen target models or partial poisoning (poisoning a randomly selected proportion or a specific class).\n\n\nExperimental Settings\n\nDatasets and models. We consider three datasets: CIFAR-10 (Krizhevsky, 2009), CIFAR-100 (Krizhevsky, 2009), and a 100-class subset of ImageNet (Deng et al., 2009). If not mentioned specifically, on CIFAR-10 and CIFAR-100, we use 50000 images for training and 10000 images for testing. For the ImageNet subset, we select 20% images from the first 100 classes of the official ImageNet training set for training and all corresponding images in the official validation set for testing. If not mentioned specifically, ResNet-18 (RN-18) (He et al., 2016) is used as the surrogate model and target model. To study transferability, we consider target models with diverse architectures: ResNet-34 (He et al., 2016), VGG-19 , DenseNet-121 (Huang et al., 2017), MobileNet-V2 (Sandler et al., 2018), and ViT (Dosovitskiy et al., 2021).\n\nTraining and poisoning settings. We train the CIFAR-10/100 models for 60 epochs and the ImageNet models for 100 epochs. We use SGD with a momentum of 0.9, a learning rate of 0.025, and cosine weight decay. We adopt the torchvision transforms module for implementing Grayscale, JPEG, and bit depth reduction (BDR) in our Image Shortcut Squeezing (ISS). We consider 12 representative existing poisoning methods as listed in Table 1 under various L p norm bounds. A brief description of 12 methods can be found in Appendix A. Specifically, we follow existing work and use L \u221e = 8, L 2 = 1.0, and L 0 = 1.\n\n\nEvaluation in the Common Scenario\n\nWe first evaluate our ISS against 12 representative poisoning methods in the common scenario where the surrogate and target models are the same and the whole training dataset is poisoned. Experimental results on CIFAR-10 shown in Table 2 demonstrate that ISS can substantially restore the clean test accuracy of poisoned models in all cases. Consistent with our new insight in Section 3.3, grayscale yields the best performance in countering methods that rely on low-frequency perturbations with large color differences (see more results by other color compression methods on EM in Appendix C). In contrast, JPEG and BDR are the best against methods that rely on high-frequency perturbations. Additional results for other hyperparameters of JPEG and BDR in Table 13 of Appendix B show that milder settings yield worse results. In addition, we can also apply ISS without determining the specific poisons by directly using Gray+JPEG. The results demonstrate that this combination is globally effective against all 12 PAP methods, with only a small decrease in clean model accuracy.\n\nOur ISS also outperforms other countermeasures. Specifically, data augmentations applied to clean models increase test accuracy but they are not effective against poisons. Image Smoothing is sometimes effective, e.g., median filtering performs the best against OPS as expected since it is effective against impulsive noise. Adversarial training (AT) achieves comparable performance to our ISS for L \u221e and L 2 norms but much worse performance for the L 0 norm. This verifies the higher generalizability of our ISS to unseen norms. It is worth noting that the ISS training time is only 1 7 of the AT training time on CIFAR-10. The efficiency of our ISS becomes more critical when the dataset is larger and the image resolution is higher. Additional experimental results for L \u221e = 16 shown in Table 3 confirm the general effectiveness of our ISS.\n\nWe further conduct experiments on CIFAR-100 and Ima-geNet. Note that for CIFAR-100, we only test the PAP methods that include CIFAR-100 experiments in their original work. For ImageNet, the poison optimization process is very time-consuming, especially for NTGA (Yuan & Wu, 2021) and Deep Confuse . Therefore, following the original work, these two methods are tested with only two classes. Note that such time-consuming PAP methods are not good candidates for data protection in practice. Experimental results on CIFAR-100 and ImageNet shown in Table 4 and Table 5 confirm the general effectiveness of our ISS.  \n\n\nEvaluation in Challenging Scenarios\n\nPartial poisoning. In practical scenarios, it is common that only a proportion of the training data can be poisoned. Therefore, we follow existing work Huang et al., 2020) to test such partial poisoning settings. We poison a certain proportion of the training data and mix it with the rest clean data for training the target model.\n\nSpecifically, we test two partial poisoning settings: first, randomly selecting a certain proportion of the images, and, second, selecting a specific class. In the first setting, as shown in Table 6, the poisons are effective only when a very large proportion of the training data is poisoned. For example, on average, even when 80% of data are poisoned, the model accuracy is only reduced by about 10 %. In the second setting, we choose to poison all training samples from class automobile on CIFAR-10. Table 7 demonstrates that almost all poisoning methods are very effective in the full poisoning setting. In both settings, our ISS is effective against all PAP methods.\n\nTransferability to unseen models. In realistic scenarios, the protector may not know the details of the target model. In this case, the transferability of the poisons is desirable. Table 8 demonstrates that all PAP methods achieve high transferability to diverse model architectures and our ISS is effective against all of them. It is also worth noting that there is no clear correlation between the transferability and the similarity between the surrogate and target models. For example, transferring from ResNet-18 to ViT is not always harder than to other CNN models.\n\n\nAdaptive Poisons to ISS\n\nIn the adversarial example literature, image compression operations can be bypassed when the attacker is adapted to them (Shin & Song, 2017;Carlini et al., 2019). Similarly, we evaluate strong adaptive poisons against our ISS using two PAP methods, EM (L \u221e ) and LSP (L 2 ). We assume that the protector can be adapted to grayscale and/or JPEG in our ISS. Specifically, for EM, we add a differentiable JPEG compression module (Shin & Song, 2017) and/or a differentiable grayscale module into its bi-level poison optimization process. For LSP, we increase the patch size to 16\u00d716 to decrease high-frequency features so that JPEG will be less effective, and we make sure the pixel values are the same for three channels to bypass grayscale. Table 9 demonstrates that for EM, the adaptive grayscale poisons are effective against grayscale, but adaptive JPEG-10 noises are not effective against JPEG. As hinted by (Shin & Song, 2017), using an ensemble of JPEG with different quality factors might be necessary for better adaptive poisoning. We also implement BPDA (Athalye et al., 2018a) with the same JPEG quality factor (i.e., JPEG-10) and find that our ISS still ensures a very high model accuracy, i.e., 83.70 %. For LSP, we observe that even though adaptive LSP is more effective against the combination of JPEG and grayscale than the other two individual compressions, it is insufficient to serve as a good adaptive protector. On the other hand, adaptive LSP also fails against the model without ISS, indicating that the additional operations (grayscale and larger patches) largely constrain its poisoning effects.\n\nGiven that the protector may have full knowledge of our  ISS, we believe that better-designed adaptive poisons can bypass our ISS in the future.\n\n\nFurther Analyses\n\nWorking Mechanism of ISS. Here we illustrate the working mechanism of our ISS by ensuring that the poisons are the exact factor that is used by the poisoned model for prediction. To this end, we follow  to use poisoned images to both train and test the model. In this case, if the test accuracy (on poisoned images) is high, it demonstrates that the perturbations in the poisoned images are actually learned by the model. In addition, we also train and test the model on poisoned images but differently, the testing (poisoned) images are pre-processed using our ISS. In this case, if the test accuracy (on poisoned images) decreases, it demonstrates that ISS can suppress the perturbations at inference time. The results in Table 10 validate our hypotheses.\n\nRelative Model Preference of different poisons. We explore the relative model preference of low-frequency vs. high-frequency poisons. This scenario is practically interesting because the same online data might be poisoned by different methods. Inspired by the experiments on the model preference of MNIST vs. CIFAR data in (Shah et al., 2020), we simply add up the EM and TAP perturbations for each As shown in Figure 4, the model converges fast and reaches a high test accuracy on EM but not on the TAP. It indicates that TAP perturbations are less preferred than EM perturbations by the model during training.\n\n\nISS for a combination of different types of poisons.\n\nWe create poisons by combining the two well-known lowfrequency and high-frequency methods, i.e., EM and TAP. Specifically, we take the average of the perturbations of these two methods. As shown in Table 11, our ISS is still effective against this combination.\n\nISS for both training and testing. Our ISS only applies to the training data for removing the poisons. However, in this case, it may cause a possible distribution shift between the training and test data. Here we explore such a shift by comparing ISS with another variant that applies compression to both the training and test data. Table 12 demonstrates that in most cases, these two versions of ISS do not lead to substantial differences.\n\n\nConclusion and Outlook\n\nIn this paper, we challenge the common belief that there are no practical and effective countermeasures to perturbative availability poisons (PAPs). Specifically, we show that 12 state-of-the-art PAP methods can be substantially countered by Image Shortcut Squeezing (ISS), which is based on simple compression. ISS outperforms other previously studied countermeasures, such as data augmentations and adversarial training. Our in-depth investigation leads to a For future work, on the countermeasure side, we would further improve ISS on the trade-off between its effectiveness and the decrease of clean model accuracy by exploring other (simple) accuracy-preserving operations. In addition, to achieve a countermeasure that is more effective against unknown poisons, it would be promising to explore more advanced combination strategies of operations or conduct automatic attack identification and then apply attack-specific operations. On the protection side, we encourage future work to develop effective (adaptive) protection methods against our ISS and other potential countermeasures.  Table 16. Clean test accuracy (%) of ResNet-18 trained on EMs that are pre-processed by another three channel-wise color suppression methods. C-mean calculates the mean value and copies the mean to three channels. R-3/G-3/B-3 copies the values from the red/green/blue channel to three channels. Their second countermeasure is more conceptual, which is to \"wait for better facial recognition systems to be developed in the future.\" This method clearly depends on the potential progress of future models and obviously cannot act as an effective solution at this moment. In contrast, our ISS requires no change to the existing model but only applies pre-processing operations.\n\nFigure 2 .\n2Poisoned CIFAR-10 images with corresponding perturbations. Perturbations are re-scaled to [0, 1] for visualization.\n\nFigure 3 .\n3Perturbation visualizations for poisons generated using surrogate at its various training epochs. Perturbations with L\u221e = 8 (top) and L2 = 1 (bottom) are shown. Both the error minimizing and maximizing losses are considered. Perturbations at later epochs exhibit higher frequency.\n\nFigure 4 .\n4Relative model preference of different poisons.\n\nTable 1 .\n1Categorization of existing PAP methods. TAP SEP LSP(Yu et al., 2022) Surrogate-Free AR OPS PAP Methods \nSurrogate Model \n\nDC (Feng et al., 2019) \n\nSlightly-Trained \n\nNTGA (Yuan & Wu, 2021) \nEM (Huang et al., 2021) \nREM (Fu et al., 2021) \nSG (van Vlijmen et al., 2022) \nTC (Shen et al., 2019) \n\nFully-Trained \nHYPO (\n\nTable 2 .\n2Clean test accuracy (%) of models trained on CIFAR-10 poisons and with our Image Shortcut Squeezing (Gray and JPEG) vs. other countermeasures. Note that TC is known to not work well under small norms, e.g., our L\u221e = 8. Hyperparameters for different countermeasures can be found in Appendix B.Norm \nPoisons/Countermeasures \nw/o \nCutout CutMix Mixup Gaussian Mean Median BDR \nGray JPEG Gray+JPEG \nAT \n\nClean (no poison) \n94.68 \n95.10 \n95.50 \n95.01 \n94.17 \n45.32 \n85.94 \n88.65 92.41 85.38 \n83.79 \n84.99 \n\nL\u221e = 8 \n\nDC (Feng et al., 2019) \n16.30 \n15.14 \n17.99 \n19.39 \n17.21 \n19.57 \n15.82 \n61.10 93.07 81.84 \n83.09 \n78.00 \nNTGA (Yuan & Wu, 2021) \n42.46 \n42.07 \n27.16 \n43.03 \n42.84 \n37.49 \n42.91 \n62.50 74.32 69.49 \n69.86 \n70.05 \nEM (Huang et al., 2021) \n21.05 \n20.63 \n26.19 \n32.83 \n12.41 \n20.60 \n21.70 \n36.46 93.01 81.50 \n83.06 \n84.80 \nREM (Fu et al., 2021) \n25.44 \n26.54 \n29.02 \n34.48 \n27.44 \n25.35 \n31.57 \n40.77 92.84 82.28 \n83.00 \n82.99 \nSG (van Vlijmen et al., 2022) \n33.05 \n24.12 \n29.46 \n39.66 \n31.92 \n46.87 \n49.53 \n70.14 86.42 79.49 \n79.21 \n76.38 \nTC (Shen et al., 2019) \n88.70 \n86.70 \n88.43 \n88.19 \n82.58 \n72.25 \n84.27 \n84.85 79.75 85.29 \n82.43 \n84.55 \nHYPO (Tao et al., 2021) \n71.54 \n70.60 \n67.54 \n72.54 \n72.46 \n40.27 \n65.53 \n83.50 61.86 85.45 \n82.94 \n84.91 \nTAP (Fowl et al., 2021b) \n8.17 \n10.04 \n10.73 \n19.14 \n9.26 \n21.82 \n32.75 \n45.99 \n9.11 \n83.87 \n81.94 \n83.31 \nSEP (Chen et al., 2023) \n3.85 \n4.47 \n9.41 \n15.59 \n3.96 \n14.43 \n35.65 \n47.43 \n3.57 \n84.37 \n82.18 \n84.12 \n\nL2 = 1.0 \nLSP (Yu et al., 2022) \n19.07 \n19.87 \n20.89 \n26.99 \n19.25 \n28.85 \n29.85 \n66.19 82.47 83.01 \n79.05 \n84.59 \nAR (Sandoval-Segura et al., 2022) 13.28 \n12.07 \n12.39 \n13.25 \n15.45 \n45.15 \n70.96 \n31.54 34.04 85.15 \n82.81 \n83.17 \n\nL0 = 1 \nOPS (Wu et al., 2023) \n36.55 \n67.94 \n76.40 \n45.06 \n19.29 \n23.50 \n85.16 \n53.76 42.44 82.53 \n79.10 \n14.41 \n\n\n\nTable 3 .\n3Additional results on CIFAR-10 with larger perturbation norms: L2 = 2.0 for LSP and L\u221e = 16 for the rest.Poisons \nw/o \nCutout CutMix Mixup Gray JPEG \nAT \n\nClean \n94.68 \n95.10 \n95.50 \n95.01 \n92.41 88.65 84.99 \n\nEM \n16.33 \n14.0 \n13.41 \n20.22 \n60.85 63.44 61.58 \nREM \n24.89 \n25.0 \n22.85 \n29.51 \n42.85 76.59 80.14 \nHYPO \n58.3 \n54.22 \n48.26 \n57.27 \n45.38 85.07 84.90 \nTAP \n10.98 \n10.96 \n9.46 \n17.97 \n6.94 \n84.19 83.35 \nSEP \n3.84 \n8.90 \n15.79 \n9.27 \n5.70 \n84.35 84.07 \n\nLSP \n19.07 \n19.87 \n20.89 \n26.99 \n82.47 83.01 84.59 \n\n\n\nTable 4 .\n4Additional results on CIFAR-100.Poisons \nw/o \nCutout CutMix Mixup Gray JPEG \n\nClean \n77.44 \n76.72 \n80.50 \n78.56 \n71.79 57.79 \n\nEM \n7.25 \n6.70 \n7.03 \n10.68 \n67.46 56.01 \nREM \n9.37 \n12.46 \n10.40 \n15.05 \n57.27 55.77 \nTC \n57.52 \n60.56 \n59.19 \n59.77 \n47.93 58.94 \nTAP \n9.00 \n10.30 \n8.73 \n19.16 \n8.84 \n83.77 \nSEP \n3.21 \n3.21 \n3.98 \n7.49 \n2.10 \n58.18 \n\nLSP \n3.06 \n4.43 \n6.12 \n5.61 \n44.62 53.49 \nAR \n3.01 \n2.85 \n3.49 \n2.19 \n24.99 57.87 \n\nOPS \n23.78 \n57.98 \n56.03 \n22.71 \n32.62 54.92 \n\n\n\nTable 5 .\n5Additional results on ImageNet subset. Following their original papers, NTGA and DC are tested with only two classes.Poisons \nw/o \nCutout CutMix Mixup Gray JPEG \n\nClean \n62.04 \n61.14 \n65.100 \n64.32 \n58.24 58.20 \n\nEM \n31.52 \n30.42 \n42.98 \n21.44 \n49.78 49.88 \nREM \n11.12 \n11.62 \n12.50 \n17.62 \n44.70 18.16 \nTAP \n24.64 \n23.00 \n18.72 \n28.62 \n24.30 44.74 \nLSP \n26.32 \n27.64 \n17.22 \n2.5 \n31.42 30.78 \n\nNTGA \n70.79 \n63.42 \n70.53 \n68.42 \n83.42 76.58 \nDC \n65.00 \n-\n-\n-\n85.00 74.00 \n\n\n\nTable 6 .Table 7 .\n67Clean test accuracy (%) of CIFAR-10 target models under different poisoning proportions. TC is tested with L\u221e = 26. Partial poisoning for class automobile on CIFAR-10. TC is tested with L\u221e = 26.Poisons \nISS \n0.1 \n0.2 \n0.4 \n0.6 \n0.8 \n0.9 \n\nDC \n\nw/o \n94.29 94.26 93.20 91.66 87.19 80.14 \nGray \n92.73 92.57 92.37 91.51 90.49 89.50 \nJPEG 84.89 85.26 84.43 83.61 83.02 82.69 \n\nEM \n\nw/o \n94.37 93.63 92.62 91.07 86.63 79.57 \nGray \n92.60 92.62 92.52 92.23 90.96 89.69 \nJPEG 84.61 84.79 84.96 84.86 84.93 84.40 \n\nREM \n\nw/o \n94.39 94.56 94.37 94.43 94.19 81.39 \nGray \n92.63 92.81 92.78 92.82 92.73 86.62 \nJPEG 84.64 85.53 84.82 85.37 85.38 82.44 \n\nSG \n\nw/o \n94.47 94.40 93.46 91.21 87.75 83.40 \nGray \n92.81 92.65 91.90 90.65 88.44 85.26 \nJPEG 84.94 84.61 84.11 82.66 80.76 79.38 \n\nTC \n\nw/o \n93.81 94.09 93.70 93.59 93.02 91.47 \nGray \n91.98 92.38 92.03 91.96 91.03 87.71 \nJPEG 85.24 85.01 85.23 85.28 85.23 84.37 \n\nHYPO \n\nw/o \n93.94 94.43 93.34 92.56 90.64 89.35 \nGray \n92.59 92.39 91.37 90.06 88.03 86.37 \nJPEG 85.61 85.18 85.39 85.21 85.25 85.10 \n\nTAP \n\nw/o \n94.09 93.94 92.75 91.27 88.42 85.98 \nGray \n92.62 91.94 90.73 89.26 85.93 83.18 \nJPEG 85.24 84.42 84.86 84.98 84.51 84.36 \n\nSEP \n\nw/o \n94.12 93.45 92.76 91.22 87.82 85.01 \nGray \n92.57 92.04 91.09 89.25 86.31 82.95 \nJPEG 85.27 85.27 85.25 84.71 84.07 84.80 \n\nLSP \n\nw/o \n94.69 94.42 92.81 91.38 88.07 82.26 \nGray \n93.12 92.56 92.67 92.20 90.78 89.65 \nJPEG 85.01 84.58 84.88 83.49 83.27 81.67 \n\nAR \n\nw/o \n94.66 94.38 93.82 91.80 88.42 82.36 \nGray \n92.85 92.69 92.53 91.24 89.88 85.35 \nJPEG 85.37 84.75 85.35 85.35 85.07 87.27 \n\nOPS \n\nw/o \n94.47 94.11 92.61 91.49 87.19 82.65 \nGray \n92.65 92.27 91.36 89.34 85.24 81.37 \nJPEG 84.75 84.88 84.55 83.98 82.87 81.33 \n\nPoisons \nw/o \nGray JPEG BDR \n\nDC \n1.60 \n69.00 88.30 52.20 \nNTGA \n51.70 94.20 90.40 75.30 \nEM \n0.10 \n48.60 94.30 \n9.60 \nREM \n0.80 \n34.40 90.40 \n2.50 \nSG \n27.75 88.39 78.59 70.05 \nTC \n0.50 \n0.20 \n92.50 37.20 \nHYPO \n4.00 \n3.00 \n94.90 56.80 \nTAP \n0.00 \n0.10 \n93.90 38.10 \nSEP \n0.00 \n0.00 \n94.70 15.50 \nLSP \n67.30 86.90 95.10 83.20 \nAR \n97.70 97.60 94.60 95.10 \nOPS \n28.90 28.50 93.60 72.10 \n\n\n\nTable 8 .\n8Cleantest accuracy (%) of CIFAR-10 target models in \nthe transfer setting. Note that AR, LSP, and OPS are surrogate-\nfree. Four CNN models (ResNet-34, VGG-19, DenseNet-121, and \nMobileNet-V2) and one ViT are considered as the target model. \nTensorClog (TC) is tested with L\u221e = 26. \n\nPoisons \nISS \nR34 \nV19 \nD121 \nM2 \nViT \n\nDC \n\nw/o \n18.06 16.59 16.05 17.81 24.09 \nGray \n83.13 80.32 83.93 78.78 44.83 \nJPEG 82.64 80.34 83.38 80.30 53.35 \n\nNTGA \n\nw/o \n40.19 47.13 16.67 40.75 31.82 \nGray \n71.84 76.89 64.07 62.28 58.25 \nJPEG 67.00 72.17 73.76 70.18 53.00 \n\nEM \n\nw/o \n29.96 34.70 30.61 30.10 18.84 \nGray \n86.97 87.03 84.84 82.81 63.28 \nJPEG 84.21 82.46 84.86 82.20 56.33 \n\nREM \n\nw/o \n25.88 29.04 28.31 24.08 32.22 \nGray \n75.20 77.99 70.53 66.21 63.00 \nJPEG 82.35 80.70 81.74 80.01 56.13 \n\nSG \n\nw/o \n29.64 \n48.5 \n28.88 30.75 18.11 \nGray \n86.53 87.12 86.07 81.34 42.22 \nJPEG 79.57 77.78 79.77 75.87 56.27 \n\nTC \n\nw/o \n87.71 85.47 78.04 78.51 69.86 \nGray \n78.48 75.14 66.72 62.39 61.86 \nJPEG 84.56 82.66 83.95 82.60 55.51 \n\nHYPO \n\nw/o \n80.64 81.59 81.48 78.27 67.49 \nGray \n75.25 76.65 74.29 69.81 53.02 \nJPEG 85.55 83.39 85.03 83.95 55.17 \n\nTAP \n\nw/o \n7.89 \n8.59 \n8.64 \n10.02 41.32 \nGray \n9.38 \n11.51 \n8.77 \n8.29 \n42.49 \nJPEG 84.42 81.95 84.28 82.24 57.35 \n\nSEP \n\nw/o \n3.11 \n6.70 \n4.41 \n5.29 \n25.56 \nGray \n4.00 \n5.40 \n4.20 \n4.70 \n22.23 \nJPEG 84.64 83.38 84.55 83.25 56.94 \n\nLSP \n\nw/o \n15.98 17.39 19.79 17.32 26.65 \nGray \n71.10 82.11 73.06 70.61 53.36 \nJPEG 79.57 78.72 79.66 76.79 60.41 \n\nAR \n\nw/o \n21.31 19.78 13.54 16.08 22.91 \nGray \n70.54 76.92 67.35 62.01 53.22 \nJPEG 85.62 83.95 85.46 83.50 54.88 \n\nOPS \n\nw/o \n37.06 \n36.3 \n40.03 27.35 30.25 \nGray \n44.29 42.21 38.32 38.71 21.77 \nJPEG 82.84 80.70 82.83 80.42 62.93 \n\n\n\nTable 9 .\n9Clean test accuracy (%) of four different target models under EM poisoning and its adaptive variants on CIFAR-10. Results are reported for L\u221e = 8 andTable 14in Appendix reports results of EM for L\u221e = 16, which follow the same pattern.Poisons \nw/o \nGray JPEG \nG&J \nAve. \n\nEM \n21.05 93.01 81.50 83.06 69.66 \nEM-Gray \n17.81 16.60 76.71 74.16 46.32 \nEM-JPEG 17.11 89.18 83.11 82.85 68.06 \nEM-G&J \n48.93 46.29 69.48 66.26 57.74 \n\nLSP \n19.07 82.47 83.01 79.05 65.90 \nLSP-G&J \n93.01 90.34 84.38 82.13 87.47 \n\n0 \n10 20 30 40 50 60 \nEpochs \n\n20 \n\n40 \n\n60 \n\n80 \n\n100 \n\nAccuracy \n\nTrain \nTest (EM) \nTest (TAP) \n\n\n\nTable 10 .\n10Test accuracy (%) on clean, poisoned, and ISS-preprocessed poisoned test sets of models that are trained on different poisons.Table 11. Clean test accuracy (%) of models trained on CIFAR-10 poisons that is a combination of low-frequency poison EM and high-frequency TAP.image. The perturbation norm is doubled accordingly. For example, for perturbations with L \u221e = 8, the composite perturbations range from \u221216 to 16. We train a model (using the original image labels) on the composite perturbations of EM and TAP and test it on either EM or TAP perturbations.Test/ Poisons \nDC \nNTGA \nEM \nREM \nSG \nTC \nHYPO \nTAP \nSEP \nLSP \nAR \nOPS \n\nClean \n17.96 \n-\n16.77 26.04 37.50 87.86 \n73.06 \n11.63 \n4.91 \n15.29 16.37 17.50 \nPoisoned \n97.20 \n97.85 \n99.85 99.97 96.72 93.79 \n99.98 \n100.0 99.99 100.0 99.94 99.83 \nPoisoned+ISS 11.17 \n24.86 \n11.89 20.68 25.39 24.16 \n16.68 \n11.33 10.13 10.06 13.83 12.61 \n\nPoisons/ISS \nw/o \nGray JPEG \n\nEM \n21.05 93.01 81.50 \nTAP \n8.17 \n9.11 \n83.87 \nEM+TAP \n36.07 18.93 84.62 \n\n\n\nTable 12 .\n12Clean test accuracy (%) for ISS (Gray and JPEG), which applies compression only to training data or to both training and test data (denoted with suffix-TT).new insight that the property of PAP perturbations depends on the type of surrogate model used during poison generation. We also show the ineffectiveness of adaptive poisons to ISS. We hope that further studies could consider various (simple) countermeasures during the development of new PAP methods.Poisons Gray-TT Gray \nJPEG-TT JPEG \n\nClean \n92.62 \n92.41 \n79.56 \n85.38 \n\nDC \n83.79 \n93.07 \n79.41 \n81.84 \nNTGA \n65.42 \n74.32 \n62.84 \n69.49 \nEM \n90.75 \n93.01 \n78.96 \n81.50 \nREM \n73.38 \n92.84 \n79.39 \n82.28 \nSG \n88.26 \n86.42 \n72.96 \n79.49 \nTC \n76.41 \n75.88 \n79.42 \n83.69 \nHYPO \n75.20 \n61.86 \n79.63 \n85.60 \nTAP \n9.53 \n9.11 \n78.65 \n83.87 \nSEP \n2.93 \n3.57 \n79.28 \n84.37 \nLSP \n76.23 \n75.77 \n68.73 \n78.69 \nAR \n68.95 \n69.37 \n79.26 \n85.38 \nOPS \n46.53 \n42.44 \n76.87 \n82.53 \n\n\n\nTable 13 .\n13JPEG with different quality factors and BDR with different bit depth.Table 15. Clean test accuracy (%) of CIFAR-10 when train and test on different poisons.Poisons \nw/o \nJPEG Compression \nBit depth reduction \n10 \n30 \n50 \n70 \n90 \n2 \n3 \n4 \n5 \n6 \n\nAcknowledgementsWe are grateful to Alex Kolmus, Dirren van Vlijmen, Rui Wen, Tijn Berns, Tom Heskes, Twan van Laarhoven, and the anonymous reviewers for discussion and feedback on this work. We thank Shutong Wu, Jie Ren, and Hao He for providing source codes. Part of this work was carried out on the Dutch national e-infrastructure with the support of SURF Cooperative.\nObfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. A Athalye, N Carlini, D Wagner, ICML. Athalye, A., Carlini, N., and Wagner, D. Obfuscated gra- dients give a false sense of security: Circumventing de- fenses to adversarial examples. In ICML, 2018a.\n\nSynthesizing robust adversarial examples. A Athalye, L Engstrom, A Ilyas, K Kwok, ICML. Athalye, A., Engstrom, L., Ilyas, A., and Kwok, K. Synthe- sizing robust adversarial examples. In ICML, 2018b.\n\nLanguage models are few-shot learners. T Brown, B Mann, N Ryder, M Subbiah, J D Kaplan, P Dhariwal, A Neelakantan, P Shyam, G Sastry, A Askell, NeurIPS. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. In NeurIPS, 2020.\n\nN Carlini, A Athalye, N Papernot, W Brendel, J Rauber, D Tsipras, I Goodfellow, A Madry, A Kurakin, arXiv:1902.06705On evaluating adversarial robustness. Carlini, N., Athalye, A., Papernot, N., Brendel, W., Rauber, J., Tsipras, D., Goodfellow, I., Madry, A., and Kurakin, A. On evaluating adversarial robustness. arXiv:1902.06705, 2019.\n\nSelf-ensemble protection: Training checkpoints are good data protectors. S Chen, G Yuan, X Cheng, Y Gong, M Qin, Y Wang, X Huang, ICLR. Chen, S., Yuan, G., Cheng, X., Gong, Y., Qin, M., Wang, Y., and Huang, X. Self-ensemble protection: Training checkpoints are good data protectors. In ICLR, 2023.\n\nLeveraging adversarial attacks to protect social media users from facial recognition. V Cherepanova, M Goldblum, H Foley, S Duan, J Dickerson, G Taylor, T Goldstein, Lowkey, ICLR. 2021Cherepanova, V., Goldblum, M., Foley, H., Duan, S., Dicker- son, J., Taylor, G., and Goldstein, T. Lowkey: Leveraging adversarial attacks to protect social media users from facial recognition. In ICLR, 2021.\n\nKeeping the bad guys out: Protecting and vaccinating deep learning with jpeg compression. N Das, M Shanbhogue, S.-T Chen, F Hohman, L Chen, M E Kounavis, D H Chau, arXiv:1705.02900Das, N., Shanbhogue, M., Chen, S.-T., Hohman, F., Chen, L., Kounavis, M. E., and Chau, D. H. Keeping the bad guys out: Protecting and vaccinating deep learning with jpeg compression. arXiv:1705.02900, 2017.\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, CVPR. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In CVPR, pp. 248-255, 2009.\n\nUnderstanding how image quality affects deep neural networks. S Dodge, L Karam, QoMEX. Dodge, S. and Karam, L. Understanding how image quality affects deep neural networks. In QoMEX, 2016.\n\nEvading defenses to transferable adversarial examples by translation-invariant attacks. Y Dong, T Pang, H Su, J Zhu, CVPR. Dong, Y., Pang, T., Su, H., and Zhu, J. Evading defenses to transferable adversarial examples by translation-invariant attacks. In CVPR, 2019.\n\nAn image is worth 16x16 words: Transformers for image recognition at scale. A Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, M Dehghani, M Minderer, G Heigold, S Gelly, ICLR. 2021Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\n\nG K Dziugaite, Z Ghahramani, D M Roy, arXiv:1608.00853A study of the effect of jpg compression on adversarial images. Dziugaite, G. K., Ghahramani, Z., and Roy, D. M. A study of the effect of jpg compression on adversarial images. arXiv:1608.00853, 2016.\n\nDisrupting model training with adversarial shortcuts. I Evtimov, I Covert, A Kusupati, T Kohno, ICML Workshop AML. Evtimov, I., Covert, I., Kusupati, A., and Kohno, T. Disrupt- ing model training with adversarial shortcuts. In ICML Workshop AML, 2021.\n\nLearning to confuse: generating training time adversarial data with autoencoder. J Feng, Q.-Z Cai, Z.-H Zhou, NeurIPS. Feng, J., Cai, Q.-Z., and Zhou, Z.-H. Learning to con- fuse: generating training time adversarial data with auto- encoder. In NeurIPS, 2019.\n\nPreventing unauthorized use of proprietary data: Poisoning for secure dataset release. L Fowl, P.-Y Chiang, M Goldblum, J Geiping, A Bansal, W Czaja, T Goldstein, arXiv:2103.02683Fowl, L., Chiang, P.-y., Goldblum, M., Geiping, J., Bansal, A., Czaja, W., and Goldstein, T. Preventing unautho- rized use of proprietary data: Poisoning for secure dataset release. arXiv:2103.02683, 2021a.\n\nAdversarial examples make strong poisons. L Fowl, M Goldblum, P.-Y Chiang, J Geiping, W Czaja, T Goldstein, NeurIPS. Fowl, L., Goldblum, M., Chiang, P.-y., Geiping, J., Czaja, W., and Goldstein, T. Adversarial examples make strong poisons. In NeurIPS, 2021b.\n\nRobust unlearnable examples: Protecting data privacy against adversarial learning. S Fu, F He, Y Liu, L Shen, D Tao, ICLR. 2021Fu, S., He, F., Liu, Y., Shen, L., and Tao, D. Robust unlearn- able examples: Protecting data privacy against adversarial learning. In ICLR, 2021.\n\nLow frequency adversarial perturbation. C Guo, J S Frank, K Q Weinberger, UAI. Guo, C., Frank, J. S., and Weinberger, K. Q. Low frequency adversarial perturbation. In UAI, 2019.\n\nIndiscriminate poisoning attacks on unsupervised contrastive learning. H He, K Zha, D Katabi, ICLR. He, H., Zha, K., and Katabi, D. Indiscriminate poisoning attacks on unsupervised contrastive learning. In ICLR, 2023.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In CVPR, 2016.\n\nDensely connected convolutional networks. G Huang, Z Liu, L Van Der Maaten, K Q Weinberger, CVPR. Huang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q. Densely connected convolutional networks. In CVPR, 2017.\n\nUnlearnable examples: Making personal data unexploitable. H Huang, X Ma, S M Erfani, J Bailey, Wang , Y , ICLR. 2021Huang, H., Ma, X., Erfani, S. M., Bailey, J., and Wang, Y. Unlearnable examples: Making personal data unex- ploitable. In ICLR, 2021.\n\nMetaPoison: Practical general-purpose cleanlabel data poisoning. W R Huang, J Geiping, L Fowl, G Taylor, T Goldstein, NeurIPS. Huang, W. R., Geiping, J., Fowl, L., Taylor, G., and Gold- stein, T. MetaPoison: Practical general-purpose clean- label data poisoning. In NeurIPS, 2020.\n\nNeural tangent kernel: Convergence and generalization in neural networks. A Jacot, F Gabriel, C Hongler, In NeurIPS. Jacot, A., Gabriel, F., and Hongler, C. Neural tangent kernel: Convergence and generalization in neural networks. In NeurIPS, 2018.\n\nSpatial transformer networks. M Jaderberg, K Simonyan, A Zisserman, NeurIPS. Jaderberg, M., Simonyan, K., Zisserman, A., et al. Spatial transformer networks. In NeurIPS, 2015.\n\nLearning multiple layers of features from tiny images. A Krizhevsky, University of TorontoTechnical reportKrizhevsky, A. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.\n\nPerceptual adversarial robustness: Defense against unseen threat models. C Laidlaw, S Singla, S Feizi, ICLR. 2021Laidlaw, C., Singla, S., and Feizi, S. Perceptual adversarial robustness: Defense against unseen threat models. In ICLR, 2021.\n\nPixel privacy. increasing image appeal while blocking automatic inference of sensitive scene information. M Larson, Z Liu, S Brugman, Z Zhao, In MediaEval. Larson, M., Liu, Z., Brugman, S., and Zhao, Z. Pixel pri- vacy. increasing image appeal while blocking automatic inference of sensitive scene information. In MediaEval, 2018.\n\nDeep learning. Nature. Y Lecun, Y Bengio, G Hinton, 521LeCun, Y., Bengio, Y., and Hinton, G. Deep learning. Na- ture, 521(7553):436-444, 2015.\n\nScene privacy protection. C Y Li, A S Shamsabadi, R Sanchez-Matilla, R Mazzon, A Cavallaro, ICASSP. Li, C. Y., Shamsabadi, A. S., Sanchez-Matilla, R., Maz- zon, R., and Cavallaro, A. Scene privacy protection. In ICASSP, 2019.\n\nExploring quality camouflage for social images. Z Liu, Z Zhao, M Larson, Amsaleg , L , MediaEval. Liu, Z., Zhao, Z., Larson, M., and Amsaleg, L. Exploring quality camouflage for social images. In MediaEval, 2020.\n\nTheory of the frequency principle for general deep neural networks. T Luo, Z Ma, Z.-Q J Xu, Y Zhang, CSIAM Trans. Appl. Math. Luo, T., Ma, Z., Xu, Z.-Q. J., and Zhang, Y. Theory of the frequency principle for general deep neural networks. CSIAM Trans. Appl. Math., 2021.\n\nTowards deep learning models resistant to adversarial attacks. A Madry, A Makelov, L Schmidt, D Tsipras, A Vladu, In ICLR. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models resistant to adversarial attacks. In ICLR, 2018.\n\nCross-domain transferability of adversarial perturbations. M M Naseer, S H Khan, M H Khan, F Shahbaz Khan, F Porikli, NeurIPS. Naseer, M. M., Khan, S. H., Khan, M. H., Shahbaz Khan, F., and Porikli, F. Cross-domain transferability of adversarial perturbations. In NeurIPS, 2019.\n\nFaceless person recognition: Privacy implications in social media. S J Oh, R Benenson, M Fritz, B Schiele, ECCV. Oh, S. J., Benenson, R., Fritz, M., and Schiele, B. Faceless person recognition: Privacy implications in social media. In ECCV, 2016.\n\nAdversarial image perturbation for privacy protection a game theory perspective. S J Oh, M Fritz, B Schiele, Oh, S. J., Fritz, M., and Schiele, B. Adversarial image per- turbation for privacy protection a game theory perspective. In ICCV, 2017.\n\nTowards a visual privacy advisor: Understanding and predicting privacy risks in images. T Orekondy, B Schiele, Fritz , M , ICCV. Orekondy, T., Schiele, B., and Fritz, M. Towards a visual privacy advisor: Understanding and predicting privacy risks in images. In ICCV, 2017.\n\nData poisoning won't save you from facial recognition. E Radiya-Dixit, S Hong, N Carlini, F Tram\u00e8r, ICLR. 2022Radiya-Dixit, E., Hong, S., Carlini, N., and Tram\u00e8r, F. Data poisoning won't save you from facial recognition. In ICLR, 2022.\n\nOn the spectral bias of neural networks. N Rahaman, A Baratin, D Arpit, F Draxler, M Lin, F Hamprecht, Y Bengio, A Courville, ICML. Rahaman, N., Baratin, A., Arpit, D., Draxler, F., Lin, M., Hamprecht, F., Bengio, Y., and Courville, A. On the spectral bias of neural networks. In ICML, 2019.\n\nOn the (im) practicality of adversarial perturbation for image privacy. A Rajabi, R B Bobba, M Rosulek, C Wright, W Feng, PETS. 2021Rajabi, A., Bobba, R. B., Rosulek, M., Wright, C., and Feng, W.-c. On the (im) practicality of adversarial perturbation for image privacy. In PETS, 2021.\n\n. J Ren, H Xu, Y Wan, X Ma, L Sun, Tang, J. Transferable unlearnable examples. In ICLR. Ren, J., Xu, H., Wan, Y., Ma, X., Sun, L., and Tang, J. Transferable unlearnable examples. In ICLR, 2023.\n\nU-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, MICCAI. Ronneberger, O., Fischer, P., and Brox, T. U-net: Convolu- tional networks for biomedical image segmentation. In MICCAI, 2015.\n\nM Sandler, A Howard, M Zhu, A Zhmoginov, Chen , L.-C , Mobilenetv2: Inverted residuals and linear bottlenecks. CVPRSandler, M., Howard, A., Zhu, M., Zhmoginov, A., and Chen, L.-C. Mobilenetv2: Inverted residuals and linear bottlenecks. In CVPR, 2018.\n\nAutoregressive perturbations for data poisoning. P Sandoval-Segura, V Singla, J Geiping, M Goldblum, T Goldstein, D W Jacobs, NeurIPS. 2022Sandoval-Segura, P., Singla, V., Geiping, J., Goldblum, M., Goldstein, T., and Jacobs, D. W. Autoregressive perturba- tions for data poisoning. In NeurIPS, 2022.\n\nBody shape privacy in images: Understanding privacy and preventing automatic shape extraction. H Sattar, K Krombholz, G Pons-Moll, Fritz , M , ECCV. Sattar, H., Krombholz, K., Pons-Moll, G., and Fritz, M. Body shape privacy in images: Understanding privacy and preventing automatic shape extraction. In ECCV, 2020.\n\nDeep learning in neural networks: An overview. J Schmidhuber, Neural Networks. 61Schmidhuber, J. Deep learning in neural networks: An overview. Neural Networks, 61:85-117, 2015.\n\nH Shah, K Tamuly, A Raghunathan, P Jain, P Netrapalli, The pitfalls of simplicity bias in neural networks. In NeurIPSw. Shah, H., Tamuly, K., Raghunathan, A., Jain, P., and Netra- palli, P. The pitfalls of simplicity bias in neural networks. In NeurIPSw, 2020.\n\nFawkes: Protecting privacy against unauthorized deep learning models. S Shan, E Wenger, J Zhang, H Li, H Zheng, B Y Zhao, USENIX Security. Shan, S., Wenger, E., Zhang, J., Li, H., Zheng, H., and Zhao, B. Y. Fawkes: Protecting privacy against unauthorized deep learning models. In USENIX Security, 2020.\n\nJ Shen, X Zhu, D Ma, Tensorclog, An imperceptible poisoning attack on deep neural network applications. IEEE Access. 7Shen, J., Zhu, X., and Ma, D. TensorClog: An imperceptible poisoning attack on deep neural network applications. IEEE Access, 7:41498-41506, 2019.\n\nJpeg-resistant adversarial images. R Shin, D Song, NeurIPSw. Shin, R. and Song, D. Jpeg-resistant adversarial images. In NeurIPSw, 2017.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, ICLR. Simonyan, K. and Zisserman, A. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.\n\nBetter safe than sorry: Preventing delusive adversaries with adversarial training. L Tao, L Feng, J Yi, S.-J Huang, Chen , S , NeurIPS. 2021Tao, L., Feng, L., Yi, J., Huang, S.-J., and Chen, S. Better safe than sorry: Preventing delusive adversaries with adversarial training. In NeurIPS, 2021.\n\nDetecting adversarial examples through image transformation. S Tian, G Yang, Y Cai, AAAI. Tian, S., Yang, G., and Cai, Y. Detecting adversarial exam- ples through image transformation. In AAAI, 2018.\n\nAdversarial training and robustness for multiple perturbations. F Tramer, D Boneh, NeurIPS. Tramer, F. and Boneh, D. Adversarial training and robust- ness for multiple perturbations. In NeurIPS, 2019.\n\nOn adaptive attacks to adversarial example defenses. F Tramer, N Carlini, W Brendel, A Madry, D Vlijmen, A Kolmus, Z Liu, Z Zhao, M Larson, NeurIPS, 2020. van. 2022ECCVwTramer, F., Carlini, N., Brendel, W., and Madry, A. On adap- tive attacks to adversarial example defenses. In NeurIPS, 2020. van Vlijmen, D., Kolmus, A., Liu, Z., Zhao, Z., and Larson, M. Generative poisoning using random discriminators. In ECCVw, 2022.\n\nFooling adversarial training with inducing noise. Z Wang, Y Wang, Wang , Y , arXiv:2111.10130Wang, Z., Wang, Y., and Wang, Y. Fooling adversarial training with inducing noise. arXiv:2111.10130, 2021.\n\nIs adversarial training really a silver bullet for mitigating data poisoning? In ICLR. R Wen, Z Zhao, Z Liu, M Backes, T Wang, Y Zhang, Wen, R., Zhao, Z., Liu, Z., Backes, M., Wang, T., and Zhang, Y. Is adversarial training really a silver bullet for mitigating data poisoning? In ICLR, 2023.\n\nOne-pixel shortcut: on the learning preference of deep neural networks. S Wu, S Chen, C Xie, X Huang, ICLR. Wu, S., Chen, S., Xie, C., and Huang, X. One-pixel shortcut: on the learning preference of deep neural networks. In ICLR, 2023.\n\nMitigating adversarial effects through randomization. ICLR. C Xie, J Wang, Z Zhang, Z Ren, Yuille , A , Xie, C., Wang, J., Zhang, Z., Ren, Z., and Yuille, A. Miti- gating adversarial effects through randomization. ICLR, 2018.\n\nPre-training on grayscale imagenet improves medical image classification. Y Xie, D Richmond, ECCVw. Xie, Y. and Richmond, D. Pre-training on grayscale ima- genet improves medical image classification. In ECCVw, 2018.\n\nFeature squeezing: Detecting adversarial examples in deep neural networks. W Xu, D Evans, Y Qi, NDSS. Xu, W., Evans, D., and Qi, Y. Feature squeezing: Detecting adversarial examples in deep neural networks. In NDSS, 2017.\n\nTraining behavior of deep neural network in frequency domain. Z.-Q J Xu, Y Zhang, Xiao , Y , ICONIP. Xu, Z.-Q. J., Zhang, Y., and Xiao, Y. Training behavior of deep neural network in frequency domain. In ICONIP, 2019.\n\nAvailability attacks create shortcuts. D Yu, H Zhang, W Chen, J Yin, T.-Y Liu, KDD. 2022Yu, D., Zhang, H., Chen, W., Yin, J., and Liu, T.-Y. Avail- ability attacks create shortcuts. In KDD, 2022.\n\nNeural tangent generalization attacks. C.-H Yuan, S.-H Wu, ICML. 2021Yuan, C.-H. and Wu, S.-H. Neural tangent generalization attacks. In ICML, 2021.\n\nTheoretically principled trade-off between robustness and accuracy. H Zhang, Y Yu, J Jiao, E Xing, L El Ghaoui, Jordan , M , ICML. Zhang, H., Yu, Y., Jiao, J., Xing, E., El Ghaoui, L., and Jordan, M. Theoretically principled trade-off between robustness and accuracy. In ICML, 2019.\n\nUnlearnable clusters: Towards label-agnostic unlearnable examples. J Zhang, X Ma, Q Yi, J Sang, Y Jiang, Y Wang, C Xu, CVPR. Zhang, J., Ma, X., Yi, Q., Sang, J., Jiang, Y., Wang, Y., and Xu, C. Unlearnable clusters: Towards label-agnostic unlearnable examples. In CVPR, 2023.\n\nCIFAR-10 and encoder-decoder model on two-class ImageNet. The generators are trained on the output of a pseudoupdated classifier, where the classification model is first trained on clean data and then trained on adversarial data to update the generator. ( Feng, A. Brief Descriptions of Implemented PAP Methods \u2022 Deep Confuse (DC). Perturbations are generated from a U-Net. We use the implementation from the official GitHub repository. 1A. Brief Descriptions of Implemented PAP Methods \u2022 Deep Confuse (DC) (Feng et al., 2019): Perturbations are generated from a U-Net (Ronneberger et al., 2015) on CIFAR-10 and encoder-decoder model on two-class ImageNet. The generators are trained on the output of a pseudo- updated classifier, where the classification model is first trained on clean data and then trained on adversarial data to update the generator. We use the implementation from the official GitHub repository. 1\n\n2021) (target model-agnostic): NTGA uses Neural Tangent Kernels to approximate target networks and then leverages the approximation to generate perturbations. We use the poisons provided in the official GitHub repository. \u2022 Neural Tangent Generalization Attacks (NTGA) (Yuan & Wu\u2022 Neural Tangent Generalization Attacks (NTGA) (Yuan & Wu, 2021) (target model-agnostic): NTGA uses Neural Tangent Kernels to approximate target networks and then leverages the approximation to generate perturbations. We use the poisons provided in the official GitHub repository. 2\n\nBi-level optimizing error-minimizing perturbations after certain steps of training on perturbed samples that are from the last iteration. The surrogate model is trained on-the-fly with perturbed training samples. Huang, \u2022 Error-Minimizing perturbations (EM. We use the implementation from the official GitHub repository. 3\u2022 Error-Minimizing perturbations (EM) (Huang et al., 2020): Bi-level optimizing error-minimizing perturbations after certain steps of training on perturbed samples that are from the last iteration. The surrogate model is trained on-the-fly with perturbed training samples. We use the implementation from the official GitHub repository. 3\n\nSame as EM, but the model is adversarially trained and the perturbations generation is equipped with expectation over transformation technique (EOT. Fu, \u2022 Robust Error-Minimizing perturbations (REMWe use the implementation from the official GitHub repository. 4\u2022 Robust Error-Minimizing perturbations (REM) (Fu et al., 2021): Same as EM, but the model is adversarially trained and the perturbations generation is equipped with expectation over transformation technique (EOT) (Athalye et al., 2018b). We use the implementation from the official GitHub repository. 4\n\nDifferent from another generative poisoning Deep Confuse, the discriminator model is randomly initialized without training. We use the CIFAR-10 poisons (version 'SG'). \u2022 Shortcut Generator ; Van Vlijmen, 2022): Perturbations are generated from a ResNet-like encoder-decoder model from. provided by the authors by private communication\u2022 Shortcut generator (SG) (van Vlijmen et al., 2022): Perturbations are generated from a ResNet-like encoder-decoder model from (Naseer et al., 2019). Different from another generative poisoning Deep Confuse, the discriminator model is randomly initialized without training. We use the CIFAR-10 poisons (version 'SG') provided by the authors by private communication.\n\nA second-order derivative with respect to training data is calculated to iteratively optimize the perturbations to minimize the gradients of model loss with respect to the weights of model layers. We use the implementation from the official GitHub repository for poisons (L \u221e = 26) on CIFAR-10 5. \u2022 Tensorclog, ; Tc) (shen, L \u221e = 8, 16) on CIFAR-10\u2022 TensorClog (TC) (Shen et al., 2019): A second-order derivative with respect to training data is calculated to iteratively optimize the perturbations to minimize the gradients of model loss with respect to the weights of model layers. We use the implementation from the official GitHub repository for poisons (L \u221e = 26) on CIFAR-10 5 . We also use the implementation from https://github.com/lhfowl/adversarial_poisons for poisons (L \u221e = 8, 16) on CIFAR-10.\n\nSimilar to EM, but the error-minimizing perturbations are generated on a pre-trained surrogate model which is trained on clean data. We use the implementation from the official GitHub repository. ( Tao, \u2022 Hypocritical perturbations (HYPO). \u2022 Hypocritical perturbations (HYPO) (Tao et al., 2021): Similar to EM, but the error-minimizing perturbations are generated on a pre-trained surrogate model which is trained on clean data. We use the implementation from the official GitHub repository. 6\n\nThe poisoning target labels are different from the original labels. ( Fowl, \u2022 Targeted Adversarial Poisoning (TAP). 2018) and Spatial Transformer Networks (STN) module. but the target labels are the same for poisoning images whose clean versions are from the same class. We use the implementation from the official GitHub repository\u2022 Targeted Adversarial Poisoning (TAP) (Fowl et al., 2021b): Targeted adversarial examples by PGD (Madry et al., 2018) and Spatial Transformer Networks (STN) module (Jaderberg et al., 2015). The poisoning target labels are different from the original labels, but the target labels are the same for poisoning images whose clean versions are from the same class. We use the implementation from the official GitHub repository. 7\n\n2023) SEP ensembles intermediate checkpoints when training on the clean training set to create perturbations. SEP is currently the state-of-the-art protection on CIFAR-10. We use the implementation from the official GitHub repository. Chen , SEP\u2022 Self-Ensemble Protection\u2022 Self-Ensemble Protection (SEP) (Chen et al., 2023) SEP ensembles intermediate checkpoints when training on the clean training set to create perturbations. SEP is currently the state-of-the-art protection on CIFAR-10. We use the implementation from the official GitHub repository. 8\n\nLinearly separable Gaussian samples are listed by order and then up-scaled to the size of the image. Yu , \u2022 Linear separable Synthetic Perturbations (LSP. Perturbations that are sampled from the same Gaussian are added to the same class. We use the implementation from the official GitHub repository. 9\u2022 Linear separable Synthetic Perturbations (LSP) (Yu et al., 2022): Linearly separable Gaussian samples are listed by order and then up-scaled to the size of the image. Perturbations that are sampled from the same Gaussian are added to the same class. We use the implementation from the official GitHub repository. 9\n\nAutoregressive process generates perturbations that CNN favors during training. We use the CIFAR-10 poisons provided by the authors in the official GitHub repository. \u2022 Autoregressive, Sandoval-Segura, AR\u2022 AutoRegressive poisoning (AR) (Sandoval-Segura et al., 2022) Autoregressive process generates perturbations that CNN favors during training. We use the CIFAR-10 poisons provided by the authors in the official GitHub repository. 10\n\n. Dc (feng, 81.84 79.35 69.69 58.53 34.79 61.10 27.03 17.34 16.42 15.113016DC (Feng et al., 2019) 16.30 81.84 79.35 69.69 58.53 34.79 61.10 27.03 17.34 16.42 15.11\n\n. &amp; Ntga (yuan, Wu, 69.49 66.83 64.28 60.19 53.24 62.58 53.48 47.30 44.39 43.294642NTGA (Yuan & Wu, 2021) 42.46 69.49 66.83 64.28 60.19 53.24 62.58 53.48 47.30 44.39 43.29\n\n. Em (huang, 2021) 21.05 81.50 70.48 54.22 42.23 21.98 36.46 24.99 22.57 21.54 20.60EM (Huang et al., 2021) 21.05 81.50 70.48 54.22 42.23 21.98 36.46 24.99 22.57 21.54 20.60\n\n. Rem (fu, 82.28 77.73 71.19 63.39 37.89 40.77 28.81 28.39 25.38 26.494425REM (Fu et al., 2021) 25.44 82.28 77.73 71.19 63.39 37.89 40.77 28.81 28.39 25.38 26.49\n\n. Sg (van Vlijmen, 2022) 33.05 79.49 77.15 74.49 73.03 70.76 69.32 58.03 47.33 31.67 31.56SG (van Vlijmen et al., 2022) 33.05 79.49 77.15 74.49 73.03 70.76 69.32 58.03 47.33 31.67 31.56\n\n. Hypo (tao, 85.45 89.14 90.16 88.10 70.66 83.17 80.33 76.91 73.22 72.055471HYPO (Tao et al., 2021) 71.54 85.45 89.14 90.16 88.10 70.66 83.17 80.33 76.91 73.22 72.05\n\n. Tap (fowl, TAP (Fowl et al., 2021b)\n\n. Sep (chen, SEP (Chen et al., 2023)\n\n. Lsp (yu, 2022) 15.09 78.69 42.11 33.99 29.19 26.66 48.27 29.56 25.14 16.88 14.27LSP (Yu et al., 2022) 15.09 78.69 42.11 33.99 29.19 26.66 48.27 29.56 25.14 16.88 14.27\n\n. Ar (sandoval-Segura, 13AR (Sandoval-Segura et al., 2022) 13\n\n. Ops (wu, 82.53 79.01 68.58 59.81 53.02 53.76 48.46 46.79 38.44 42.275536OPS (Wu et al., 2023) 36.55 82.53 79.01 68.58 59.81 53.02 53.76 48.46 46.79 38.44 42.27\n\nClean test accuracy (%) of target models under EM poisoning and its adaptive variants on CIFAR-10. Results are reported for L\u221e = 16. Poisons w/o Gray. JPEG G&J Ave. EM 19.32 80.60 84.32 82.12 66.59Table 14Table 14. Clean test accuracy (%) of target models under EM poisoning and its adaptive variants on CIFAR-10. Results are reported for L\u221e = 16. Poisons w/o Gray JPEG G&J Ave. EM 19.32 80.60 84.32 82.12 66.59\n\n. Em-Gray , 10.01 12.14 50.14 52.07 31.09EM-Gray 10.01 12.14 50.14 52.07 31.09\n\n. Em-G&amp;j, 19.71 22.68 28.94 30.51 25.46EM-G&J 19.71 22.68 28.94 30.51 25.46\n\nOPS generates one pixel shortcut by searching the pixel that creates the most significant mean pixel value change for all images from one class. The perturbations are dataset-dependent. We use the implementation from the official GitHub repository. Wu, \u2022 One Pixel Shortcut (OPS11\u2022 One Pixel Shortcut (OPS) (Wu et al., 2023): OPS generates one pixel shortcut by searching the pixel that creates the most significant mean pixel value change for all images from one class. The perturbations are dataset-dependent. We use the implementation from the official GitHub repository. 11\n\nHyperparameters for Different Countermeasures If not explicitly mentioned, we use JPEG with a quality factor of 10 and bit depth reduction (BDR) with 2 bits. For grayscale compression, we use the torchvision implementation 12 where the weighted sum of three channels are first calculated and then copied to all three channels. For adversarial training (AT), PGD-10 is used with a step size of 2 255 , where the model is trained on CIFAR-10 for 100 epochs. B , and Gaussian smoothing. with aB. Hyperparameters for Different Countermeasures If not explicitly mentioned, we use JPEG with a quality factor of 10 and bit depth reduction (BDR) with 2 bits. For grayscale compression, we use the torchvision implementation 12 where the weighted sum of three channels are first calculated and then copied to all three channels. For adversarial training (AT), PGD-10 is used with a step size of 2 255 , where the model is trained on CIFAR-10 for 100 epochs. We use a kernel size of 3 for both median, mean, and Gaussian smoothing (with a\n\nTheir first countermeasure is based on robust training via data augmentation and assumes that an additional clean pre-trained model is available to the data exploiter. In contrast, our work explores robust training, via adversarial training, but does not assume the exploiter has access to additional (clean) data and model. Table 15 demonstrates that models trained by their robust training on one type of poison would not generalize to others, limiting the effectiveness of the robust data augmentation against PAPs. D Pap ; Radiya-Dixit, Countermeasures in Facial Recognition In the domain of facial recognition. 112022) propose two countermeasures against two PAP methodsD. PAP Countermeasures in Facial Recognition In the domain of facial recognition, (Radiya-Dixit et al., 2022) propose two countermeasures against two PAP methods, Fawkes (Shan et al., 2020) and LowKey (Cherepanova et al., 2021). Their first countermeasure is based on robust training via data augmentation and assumes that an additional clean pre-trained model is available to the data exploiter. In contrast, our work explores robust training, via adversarial training, but does not assume the exploiter has access to additional (clean) data and model. Table 15 demonstrates that models trained by their robust training on one type of poison would not generalize to others, limiting the effectiveness of the robust data augmentation against PAPs. 11 https://github.com/cychomatica/One-Pixel-Shotcut 12 https://pytorch.org/vision/stable/generated/torchvision.transforms.Grayscale\n", "annotations": {"author": "[{\"end\":147,\"start\":95},{\"end\":267,\"start\":148},{\"end\":343,\"start\":268}]", "publisher": null, "author_last_name": "[{\"end\":106,\"start\":103},{\"end\":160,\"start\":156},{\"end\":281,\"start\":275}]", "author_first_name": "[{\"end\":102,\"start\":95},{\"end\":155,\"start\":148},{\"end\":274,\"start\":268}]", "author_affiliation": "[{\"end\":146,\"start\":108},{\"end\":198,\"start\":162},{\"end\":266,\"start\":200},{\"end\":342,\"start\":304}]", "title": "[{\"end\":88,\"start\":1},{\"end\":431,\"start\":344}]", "venue": "[{\"end\":502,\"start\":433}]", "abstract": "[{\"end\":2056,\"start\":683}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b45\"},\"end\":2221,\"start\":2202},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2240,\"start\":2221},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2256,\"start\":2240},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2275,\"start\":2256},{\"end\":2647,\"start\":2628},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2666,\"start\":2647},{\"end\":2668,\"start\":2666},{\"end\":2943,\"start\":2923},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2962,\"start\":2943},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2999,\"start\":2981},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":3200,\"start\":3183},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3424,\"start\":3404},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":3443,\"start\":3424},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":3549,\"start\":3527},{\"end\":3570,\"start\":3549},{\"end\":3916,\"start\":3897},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":3932,\"start\":3916},{\"end\":3957,\"start\":3932},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3992,\"start\":3973},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4010,\"start\":3992},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":4046,\"start\":4030},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":4985,\"start\":4969},{\"end\":7090,\"start\":7070},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":7916,\"start\":7897},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":7933,\"start\":7916},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8097,\"start\":8079},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8150,\"start\":8133},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":8169,\"start\":8150},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":8344,\"start\":8327},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8463,\"start\":8443},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8774,\"start\":8752},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":8790,\"start\":8774},{\"end\":8863,\"start\":8843},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8926,\"start\":8904},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":9216,\"start\":9199},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9541,\"start\":9522},{\"attributes\":{\"ref_id\":\"b93\"},\"end\":9579,\"start\":9553},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":10407,\"start\":10390},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10572,\"start\":10552},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":10591,\"start\":10572},{\"end\":10672,\"start\":10652},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10691,\"start\":10672},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10728,\"start\":10710},{\"end\":10952,\"start\":10925},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":11145,\"start\":11123},{\"end\":11166,\"start\":11145},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11458,\"start\":11434},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11475,\"start\":11458},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":11491,\"start\":11475},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":11648,\"start\":11630},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11666,\"start\":11648},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11812,\"start\":11790},{\"end\":11969,\"start\":11948},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12171,\"start\":12154},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":12191,\"start\":12171},{\"end\":12211,\"start\":12191},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12257,\"start\":12236},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12273,\"start\":12257},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12289,\"start\":12273},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12386,\"start\":12363},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":14555,\"start\":14538},{\"end\":14580,\"start\":14555},{\"end\":14625,\"start\":14606},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14805,\"start\":14786},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14823,\"start\":14805},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":15040,\"start\":15023},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":15545,\"start\":15527},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":15808,\"start\":15786},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":15824,\"start\":15808},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":15841,\"start\":15824},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":16151,\"start\":16129},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":16167,\"start\":16151},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":16184,\"start\":16167},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":16908,\"start\":16891},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":18028,\"start\":18006},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":18249,\"start\":18225},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":18266,\"start\":18249},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":18282,\"start\":18266},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":18527,\"start\":18506},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":19124,\"start\":19106},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19179,\"start\":19161},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":19566,\"start\":19549},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":19723,\"start\":19706},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":19767,\"start\":19747},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":19804,\"start\":19782},{\"end\":19840,\"start\":19814},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":22687,\"start\":22670},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23232,\"start\":23213},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":24806,\"start\":24787},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24827,\"start\":24806},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":25111,\"start\":25092},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":25595,\"start\":25576},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25750,\"start\":25727},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":27551,\"start\":27532},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":30936,\"start\":30919}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30500,\"start\":30372},{\"attributes\":{\"id\":\"fig_1\"},\"end\":30794,\"start\":30501},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30855,\"start\":30795},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":31183,\"start\":30856},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":33015,\"start\":31184},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":33545,\"start\":33016},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":34035,\"start\":33546},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":34521,\"start\":34036},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":36641,\"start\":34522},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":38369,\"start\":36642},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":38983,\"start\":38370},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":39994,\"start\":38984},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":40929,\"start\":39995},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":41188,\"start\":40930}]", "paragraph": "[{\"end\":2777,\"start\":2072},{\"end\":3571,\"start\":2779},{\"end\":4429,\"start\":3573},{\"end\":5317,\"start\":4431},{\"end\":5682,\"start\":5319},{\"end\":5733,\"start\":5684},{\"end\":6002,\"start\":5735},{\"end\":6223,\"start\":6004},{\"end\":6451,\"start\":6225},{\"end\":6636,\"start\":6453},{\"end\":7667,\"start\":6694},{\"end\":8170,\"start\":7669},{\"end\":8431,\"start\":8172},{\"end\":8664,\"start\":8433},{\"end\":9457,\"start\":8666},{\"end\":9966,\"start\":9459},{\"end\":10254,\"start\":9968},{\"end\":10989,\"start\":10278},{\"end\":11182,\"start\":10991},{\"end\":11970,\"start\":11232},{\"end\":12387,\"start\":11972},{\"end\":12486,\"start\":12389},{\"end\":13274,\"start\":12558},{\"end\":13698,\"start\":13276},{\"end\":13953,\"start\":13807},{\"end\":14670,\"start\":14045},{\"end\":15078,\"start\":14672},{\"end\":15699,\"start\":15130},{\"end\":16597,\"start\":15701},{\"end\":17279,\"start\":16599},{\"end\":17650,\"start\":17312},{\"end\":18528,\"start\":17652},{\"end\":18992,\"start\":18544},{\"end\":19841,\"start\":19018},{\"end\":20444,\"start\":19843},{\"end\":21561,\"start\":20482},{\"end\":22406,\"start\":21563},{\"end\":23021,\"start\":22408},{\"end\":23392,\"start\":23061},{\"end\":24066,\"start\":23394},{\"end\":24638,\"start\":24068},{\"end\":26283,\"start\":24666},{\"end\":26429,\"start\":26285},{\"end\":27207,\"start\":26450},{\"end\":27820,\"start\":27209},{\"end\":28137,\"start\":27877},{\"end\":28579,\"start\":28139},{\"end\":30371,\"start\":28606}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13737,\"start\":13699},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13806,\"start\":13737},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14003,\"start\":13954}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":14376,\"start\":14369},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":20272,\"start\":20265},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":20719,\"start\":20712},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":21247,\"start\":21239},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":22360,\"start\":22353},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":22961,\"start\":22954},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":22973,\"start\":22966},{\"end\":23592,\"start\":23585},{\"end\":23905,\"start\":23898},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":24256,\"start\":24249},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":25412,\"start\":25405},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":27182,\"start\":27174},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":28083,\"start\":28075},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":28480,\"start\":28472},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":29706,\"start\":29698}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2070,\"start\":2058},{\"attributes\":{\"n\":\"2.\"},\"end\":6651,\"start\":6639},{\"attributes\":{\"n\":\"2.1.\"},\"end\":6692,\"start\":6654},{\"attributes\":{\"n\":\"2.2.\"},\"end\":10276,\"start\":10257},{\"attributes\":{\"n\":\"2.3.\"},\"end\":11230,\"start\":11185},{\"attributes\":{\"n\":\"3.\"},\"end\":12534,\"start\":12489},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12556,\"start\":12537},{\"attributes\":{\"n\":\"3.2.\"},\"end\":14043,\"start\":14005},{\"attributes\":{\"n\":\"3.3.\"},\"end\":15128,\"start\":15081},{\"attributes\":{\"n\":\"3.4.\"},\"end\":17310,\"start\":17282},{\"attributes\":{\"n\":\"4.\"},\"end\":18542,\"start\":18531},{\"attributes\":{\"n\":\"4.1.\"},\"end\":19016,\"start\":18995},{\"attributes\":{\"n\":\"4.2.\"},\"end\":20480,\"start\":20447},{\"attributes\":{\"n\":\"4.3.\"},\"end\":23059,\"start\":23024},{\"attributes\":{\"n\":\"4.4.\"},\"end\":24664,\"start\":24641},{\"attributes\":{\"n\":\"4.5.\"},\"end\":26448,\"start\":26432},{\"end\":27875,\"start\":27823},{\"attributes\":{\"n\":\"5.\"},\"end\":28604,\"start\":28582},{\"end\":30383,\"start\":30373},{\"end\":30512,\"start\":30502},{\"end\":30806,\"start\":30796},{\"end\":30866,\"start\":30857},{\"end\":31194,\"start\":31185},{\"end\":33026,\"start\":33017},{\"end\":33556,\"start\":33547},{\"end\":34046,\"start\":34037},{\"end\":34541,\"start\":34523},{\"end\":36652,\"start\":36643},{\"end\":38380,\"start\":38371},{\"end\":38995,\"start\":38985},{\"end\":40006,\"start\":39996},{\"end\":40941,\"start\":40931}]", "table": "[{\"end\":31183,\"start\":30959},{\"end\":33015,\"start\":31488},{\"end\":33545,\"start\":33133},{\"end\":34035,\"start\":33590},{\"end\":34521,\"start\":34165},{\"end\":36641,\"start\":34738},{\"end\":38369,\"start\":36659},{\"end\":38983,\"start\":38616},{\"end\":39994,\"start\":39558},{\"end\":40929,\"start\":40466},{\"end\":41188,\"start\":41100}]", "figure_caption": "[{\"end\":30500,\"start\":30385},{\"end\":30794,\"start\":30514},{\"end\":30855,\"start\":30808},{\"end\":30959,\"start\":30868},{\"end\":31488,\"start\":31196},{\"end\":33133,\"start\":33028},{\"end\":33590,\"start\":33558},{\"end\":34165,\"start\":34048},{\"end\":34738,\"start\":34544},{\"end\":36659,\"start\":36654},{\"end\":38616,\"start\":38382},{\"end\":39558,\"start\":38998},{\"end\":40466,\"start\":40009},{\"end\":41100,\"start\":40944}]", "figure_ref": "[{\"end\":2696,\"start\":2688},{\"end\":4593,\"start\":4585},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15238,\"start\":15230},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16463,\"start\":16455},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27628,\"start\":27620}]", "bib_author_first_name": "[{\"end\":41662,\"start\":41661},{\"end\":41673,\"start\":41672},{\"end\":41684,\"start\":41683},{\"end\":41905,\"start\":41904},{\"end\":41916,\"start\":41915},{\"end\":41928,\"start\":41927},{\"end\":41937,\"start\":41936},{\"end\":42102,\"start\":42101},{\"end\":42111,\"start\":42110},{\"end\":42119,\"start\":42118},{\"end\":42128,\"start\":42127},{\"end\":42139,\"start\":42138},{\"end\":42141,\"start\":42140},{\"end\":42151,\"start\":42150},{\"end\":42163,\"start\":42162},{\"end\":42178,\"start\":42177},{\"end\":42187,\"start\":42186},{\"end\":42197,\"start\":42196},{\"end\":42407,\"start\":42406},{\"end\":42418,\"start\":42417},{\"end\":42429,\"start\":42428},{\"end\":42441,\"start\":42440},{\"end\":42452,\"start\":42451},{\"end\":42462,\"start\":42461},{\"end\":42473,\"start\":42472},{\"end\":42487,\"start\":42486},{\"end\":42496,\"start\":42495},{\"end\":42818,\"start\":42817},{\"end\":42826,\"start\":42825},{\"end\":42834,\"start\":42833},{\"end\":42843,\"start\":42842},{\"end\":42851,\"start\":42850},{\"end\":42858,\"start\":42857},{\"end\":42866,\"start\":42865},{\"end\":43130,\"start\":43129},{\"end\":43145,\"start\":43144},{\"end\":43157,\"start\":43156},{\"end\":43166,\"start\":43165},{\"end\":43174,\"start\":43173},{\"end\":43187,\"start\":43186},{\"end\":43197,\"start\":43196},{\"end\":43527,\"start\":43526},{\"end\":43534,\"start\":43533},{\"end\":43551,\"start\":43547},{\"end\":43559,\"start\":43558},{\"end\":43569,\"start\":43568},{\"end\":43577,\"start\":43576},{\"end\":43579,\"start\":43578},{\"end\":43591,\"start\":43590},{\"end\":43593,\"start\":43592},{\"end\":43878,\"start\":43877},{\"end\":43886,\"start\":43885},{\"end\":43894,\"start\":43893},{\"end\":43907,\"start\":43903},{\"end\":43913,\"start\":43912},{\"end\":43919,\"start\":43918},{\"end\":44147,\"start\":44146},{\"end\":44156,\"start\":44155},{\"end\":44363,\"start\":44362},{\"end\":44371,\"start\":44370},{\"end\":44379,\"start\":44378},{\"end\":44385,\"start\":44384},{\"end\":44618,\"start\":44617},{\"end\":44633,\"start\":44632},{\"end\":44642,\"start\":44641},{\"end\":44656,\"start\":44655},{\"end\":44671,\"start\":44670},{\"end\":44679,\"start\":44678},{\"end\":44694,\"start\":44693},{\"end\":44706,\"start\":44705},{\"end\":44718,\"start\":44717},{\"end\":44729,\"start\":44728},{\"end\":44987,\"start\":44986},{\"end\":44989,\"start\":44988},{\"end\":45002,\"start\":45001},{\"end\":45016,\"start\":45015},{\"end\":45018,\"start\":45017},{\"end\":45297,\"start\":45296},{\"end\":45308,\"start\":45307},{\"end\":45318,\"start\":45317},{\"end\":45330,\"start\":45329},{\"end\":45577,\"start\":45576},{\"end\":45588,\"start\":45584},{\"end\":45598,\"start\":45594},{\"end\":45844,\"start\":45843},{\"end\":45855,\"start\":45851},{\"end\":45865,\"start\":45864},{\"end\":45877,\"start\":45876},{\"end\":45888,\"start\":45887},{\"end\":45898,\"start\":45897},{\"end\":45907,\"start\":45906},{\"end\":46186,\"start\":46185},{\"end\":46194,\"start\":46193},{\"end\":46209,\"start\":46205},{\"end\":46219,\"start\":46218},{\"end\":46230,\"start\":46229},{\"end\":46239,\"start\":46238},{\"end\":46487,\"start\":46486},{\"end\":46493,\"start\":46492},{\"end\":46499,\"start\":46498},{\"end\":46506,\"start\":46505},{\"end\":46514,\"start\":46513},{\"end\":46719,\"start\":46718},{\"end\":46726,\"start\":46725},{\"end\":46728,\"start\":46727},{\"end\":46737,\"start\":46736},{\"end\":46739,\"start\":46738},{\"end\":46929,\"start\":46928},{\"end\":46935,\"start\":46934},{\"end\":46942,\"start\":46941},{\"end\":47123,\"start\":47122},{\"end\":47129,\"start\":47128},{\"end\":47138,\"start\":47137},{\"end\":47145,\"start\":47144},{\"end\":47302,\"start\":47301},{\"end\":47311,\"start\":47310},{\"end\":47318,\"start\":47317},{\"end\":47336,\"start\":47335},{\"end\":47338,\"start\":47337},{\"end\":47536,\"start\":47535},{\"end\":47545,\"start\":47544},{\"end\":47551,\"start\":47550},{\"end\":47553,\"start\":47552},{\"end\":47563,\"start\":47562},{\"end\":47576,\"start\":47572},{\"end\":47580,\"start\":47579},{\"end\":47794,\"start\":47793},{\"end\":47796,\"start\":47795},{\"end\":47805,\"start\":47804},{\"end\":47816,\"start\":47815},{\"end\":47824,\"start\":47823},{\"end\":47834,\"start\":47833},{\"end\":48085,\"start\":48084},{\"end\":48094,\"start\":48093},{\"end\":48105,\"start\":48104},{\"end\":48291,\"start\":48290},{\"end\":48304,\"start\":48303},{\"end\":48316,\"start\":48315},{\"end\":48493,\"start\":48492},{\"end\":48735,\"start\":48734},{\"end\":48746,\"start\":48745},{\"end\":48756,\"start\":48755},{\"end\":49009,\"start\":49008},{\"end\":49019,\"start\":49018},{\"end\":49026,\"start\":49025},{\"end\":49037,\"start\":49036},{\"end\":49258,\"start\":49257},{\"end\":49267,\"start\":49266},{\"end\":49277,\"start\":49276},{\"end\":49405,\"start\":49404},{\"end\":49407,\"start\":49406},{\"end\":49413,\"start\":49412},{\"end\":49415,\"start\":49414},{\"end\":49429,\"start\":49428},{\"end\":49448,\"start\":49447},{\"end\":49458,\"start\":49457},{\"end\":49654,\"start\":49653},{\"end\":49661,\"start\":49660},{\"end\":49669,\"start\":49668},{\"end\":49685,\"start\":49678},{\"end\":49689,\"start\":49688},{\"end\":49888,\"start\":49887},{\"end\":49895,\"start\":49894},{\"end\":49904,\"start\":49900},{\"end\":49906,\"start\":49905},{\"end\":49912,\"start\":49911},{\"end\":50155,\"start\":50154},{\"end\":50164,\"start\":50163},{\"end\":50175,\"start\":50174},{\"end\":50186,\"start\":50185},{\"end\":50197,\"start\":50196},{\"end\":50417,\"start\":50416},{\"end\":50419,\"start\":50418},{\"end\":50429,\"start\":50428},{\"end\":50431,\"start\":50430},{\"end\":50439,\"start\":50438},{\"end\":50441,\"start\":50440},{\"end\":50449,\"start\":50448},{\"end\":50465,\"start\":50464},{\"end\":50705,\"start\":50704},{\"end\":50707,\"start\":50706},{\"end\":50713,\"start\":50712},{\"end\":50725,\"start\":50724},{\"end\":50734,\"start\":50733},{\"end\":50967,\"start\":50966},{\"end\":50969,\"start\":50968},{\"end\":50975,\"start\":50974},{\"end\":50984,\"start\":50983},{\"end\":51220,\"start\":51219},{\"end\":51232,\"start\":51231},{\"end\":51247,\"start\":51242},{\"end\":51251,\"start\":51250},{\"end\":51461,\"start\":51460},{\"end\":51477,\"start\":51476},{\"end\":51485,\"start\":51484},{\"end\":51496,\"start\":51495},{\"end\":51684,\"start\":51683},{\"end\":51695,\"start\":51694},{\"end\":51706,\"start\":51705},{\"end\":51715,\"start\":51714},{\"end\":51726,\"start\":51725},{\"end\":51733,\"start\":51732},{\"end\":51746,\"start\":51745},{\"end\":51756,\"start\":51755},{\"end\":52008,\"start\":52007},{\"end\":52018,\"start\":52017},{\"end\":52020,\"start\":52019},{\"end\":52029,\"start\":52028},{\"end\":52040,\"start\":52039},{\"end\":52050,\"start\":52049},{\"end\":52225,\"start\":52224},{\"end\":52232,\"start\":52231},{\"end\":52238,\"start\":52237},{\"end\":52245,\"start\":52244},{\"end\":52251,\"start\":52250},{\"end\":52483,\"start\":52482},{\"end\":52498,\"start\":52497},{\"end\":52509,\"start\":52508},{\"end\":52653,\"start\":52652},{\"end\":52664,\"start\":52663},{\"end\":52674,\"start\":52673},{\"end\":52681,\"start\":52680},{\"end\":52697,\"start\":52693},{\"end\":52704,\"start\":52700},{\"end\":52954,\"start\":52953},{\"end\":52973,\"start\":52972},{\"end\":52983,\"start\":52982},{\"end\":52994,\"start\":52993},{\"end\":53006,\"start\":53005},{\"end\":53019,\"start\":53018},{\"end\":53021,\"start\":53020},{\"end\":53302,\"start\":53301},{\"end\":53312,\"start\":53311},{\"end\":53325,\"start\":53324},{\"end\":53342,\"start\":53337},{\"end\":53346,\"start\":53345},{\"end\":53570,\"start\":53569},{\"end\":53702,\"start\":53701},{\"end\":53710,\"start\":53709},{\"end\":53720,\"start\":53719},{\"end\":53735,\"start\":53734},{\"end\":53743,\"start\":53742},{\"end\":54034,\"start\":54033},{\"end\":54042,\"start\":54041},{\"end\":54052,\"start\":54051},{\"end\":54061,\"start\":54060},{\"end\":54067,\"start\":54066},{\"end\":54076,\"start\":54075},{\"end\":54078,\"start\":54077},{\"end\":54268,\"start\":54267},{\"end\":54276,\"start\":54275},{\"end\":54283,\"start\":54282},{\"end\":54569,\"start\":54568},{\"end\":54577,\"start\":54576},{\"end\":54740,\"start\":54739},{\"end\":54752,\"start\":54751},{\"end\":54969,\"start\":54968},{\"end\":54976,\"start\":54975},{\"end\":54984,\"start\":54983},{\"end\":54993,\"start\":54989},{\"end\":55005,\"start\":55001},{\"end\":55009,\"start\":55008},{\"end\":55243,\"start\":55242},{\"end\":55251,\"start\":55250},{\"end\":55259,\"start\":55258},{\"end\":55447,\"start\":55446},{\"end\":55457,\"start\":55456},{\"end\":55638,\"start\":55637},{\"end\":55648,\"start\":55647},{\"end\":55659,\"start\":55658},{\"end\":55670,\"start\":55669},{\"end\":55679,\"start\":55678},{\"end\":55690,\"start\":55689},{\"end\":55700,\"start\":55699},{\"end\":55707,\"start\":55706},{\"end\":55715,\"start\":55714},{\"end\":56059,\"start\":56058},{\"end\":56067,\"start\":56066},{\"end\":56078,\"start\":56074},{\"end\":56082,\"start\":56081},{\"end\":56297,\"start\":56296},{\"end\":56304,\"start\":56303},{\"end\":56312,\"start\":56311},{\"end\":56319,\"start\":56318},{\"end\":56329,\"start\":56328},{\"end\":56337,\"start\":56336},{\"end\":56576,\"start\":56575},{\"end\":56582,\"start\":56581},{\"end\":56590,\"start\":56589},{\"end\":56597,\"start\":56596},{\"end\":56801,\"start\":56800},{\"end\":56808,\"start\":56807},{\"end\":56816,\"start\":56815},{\"end\":56825,\"start\":56824},{\"end\":56837,\"start\":56831},{\"end\":56841,\"start\":56840},{\"end\":57042,\"start\":57041},{\"end\":57049,\"start\":57048},{\"end\":57261,\"start\":57260},{\"end\":57267,\"start\":57266},{\"end\":57276,\"start\":57275},{\"end\":57474,\"start\":57470},{\"end\":57476,\"start\":57475},{\"end\":57482,\"start\":57481},{\"end\":57494,\"start\":57490},{\"end\":57498,\"start\":57497},{\"end\":57667,\"start\":57666},{\"end\":57673,\"start\":57672},{\"end\":57682,\"start\":57681},{\"end\":57690,\"start\":57689},{\"end\":57700,\"start\":57696},{\"end\":57867,\"start\":57863},{\"end\":57878,\"start\":57874},{\"end\":58043,\"start\":58042},{\"end\":58052,\"start\":58051},{\"end\":58058,\"start\":58057},{\"end\":58066,\"start\":58065},{\"end\":58074,\"start\":58073},{\"end\":58092,\"start\":58086},{\"end\":58096,\"start\":58095},{\"end\":58326,\"start\":58325},{\"end\":58335,\"start\":58334},{\"end\":58341,\"start\":58340},{\"end\":58347,\"start\":58346},{\"end\":58355,\"start\":58354},{\"end\":58364,\"start\":58363},{\"end\":58372,\"start\":58371},{\"end\":58790,\"start\":58789},{\"end\":61415,\"start\":61414},{\"end\":62247,\"start\":62246},{\"end\":62261,\"start\":62260},{\"end\":62953,\"start\":62952},{\"end\":63320,\"start\":63319},{\"end\":64249,\"start\":64245},{\"end\":64669,\"start\":64667},{\"end\":65796,\"start\":65791},{\"end\":67554,\"start\":67547},{\"end\":68742,\"start\":68741},{\"end\":69835,\"start\":69834}]", "bib_author_last_name": "[{\"end\":41670,\"start\":41663},{\"end\":41681,\"start\":41674},{\"end\":41691,\"start\":41685},{\"end\":41913,\"start\":41906},{\"end\":41925,\"start\":41917},{\"end\":41934,\"start\":41929},{\"end\":41942,\"start\":41938},{\"end\":42108,\"start\":42103},{\"end\":42116,\"start\":42112},{\"end\":42125,\"start\":42120},{\"end\":42136,\"start\":42129},{\"end\":42148,\"start\":42142},{\"end\":42160,\"start\":42152},{\"end\":42175,\"start\":42164},{\"end\":42184,\"start\":42179},{\"end\":42194,\"start\":42188},{\"end\":42204,\"start\":42198},{\"end\":42415,\"start\":42408},{\"end\":42426,\"start\":42419},{\"end\":42438,\"start\":42430},{\"end\":42449,\"start\":42442},{\"end\":42459,\"start\":42453},{\"end\":42470,\"start\":42463},{\"end\":42484,\"start\":42474},{\"end\":42493,\"start\":42488},{\"end\":42504,\"start\":42497},{\"end\":42823,\"start\":42819},{\"end\":42831,\"start\":42827},{\"end\":42840,\"start\":42835},{\"end\":42848,\"start\":42844},{\"end\":42855,\"start\":42852},{\"end\":42863,\"start\":42859},{\"end\":42872,\"start\":42867},{\"end\":43142,\"start\":43131},{\"end\":43154,\"start\":43146},{\"end\":43163,\"start\":43158},{\"end\":43171,\"start\":43167},{\"end\":43184,\"start\":43175},{\"end\":43194,\"start\":43188},{\"end\":43207,\"start\":43198},{\"end\":43215,\"start\":43209},{\"end\":43531,\"start\":43528},{\"end\":43545,\"start\":43535},{\"end\":43556,\"start\":43552},{\"end\":43566,\"start\":43560},{\"end\":43574,\"start\":43570},{\"end\":43588,\"start\":43580},{\"end\":43598,\"start\":43594},{\"end\":43883,\"start\":43879},{\"end\":43891,\"start\":43887},{\"end\":43901,\"start\":43895},{\"end\":43910,\"start\":43908},{\"end\":43916,\"start\":43914},{\"end\":43927,\"start\":43920},{\"end\":44153,\"start\":44148},{\"end\":44162,\"start\":44157},{\"end\":44368,\"start\":44364},{\"end\":44376,\"start\":44372},{\"end\":44382,\"start\":44380},{\"end\":44389,\"start\":44386},{\"end\":44630,\"start\":44619},{\"end\":44639,\"start\":44634},{\"end\":44653,\"start\":44643},{\"end\":44668,\"start\":44657},{\"end\":44676,\"start\":44672},{\"end\":44691,\"start\":44680},{\"end\":44703,\"start\":44695},{\"end\":44715,\"start\":44707},{\"end\":44726,\"start\":44719},{\"end\":44735,\"start\":44730},{\"end\":44999,\"start\":44990},{\"end\":45013,\"start\":45003},{\"end\":45022,\"start\":45019},{\"end\":45305,\"start\":45298},{\"end\":45315,\"start\":45309},{\"end\":45327,\"start\":45319},{\"end\":45336,\"start\":45331},{\"end\":45582,\"start\":45578},{\"end\":45592,\"start\":45589},{\"end\":45603,\"start\":45599},{\"end\":45849,\"start\":45845},{\"end\":45862,\"start\":45856},{\"end\":45874,\"start\":45866},{\"end\":45885,\"start\":45878},{\"end\":45895,\"start\":45889},{\"end\":45904,\"start\":45899},{\"end\":45917,\"start\":45908},{\"end\":46191,\"start\":46187},{\"end\":46203,\"start\":46195},{\"end\":46216,\"start\":46210},{\"end\":46227,\"start\":46220},{\"end\":46236,\"start\":46231},{\"end\":46249,\"start\":46240},{\"end\":46490,\"start\":46488},{\"end\":46496,\"start\":46494},{\"end\":46503,\"start\":46500},{\"end\":46511,\"start\":46507},{\"end\":46518,\"start\":46515},{\"end\":46723,\"start\":46720},{\"end\":46734,\"start\":46729},{\"end\":46750,\"start\":46740},{\"end\":46932,\"start\":46930},{\"end\":46939,\"start\":46936},{\"end\":46949,\"start\":46943},{\"end\":47126,\"start\":47124},{\"end\":47135,\"start\":47130},{\"end\":47142,\"start\":47139},{\"end\":47149,\"start\":47146},{\"end\":47308,\"start\":47303},{\"end\":47315,\"start\":47312},{\"end\":47333,\"start\":47319},{\"end\":47349,\"start\":47339},{\"end\":47542,\"start\":47537},{\"end\":47548,\"start\":47546},{\"end\":47560,\"start\":47554},{\"end\":47570,\"start\":47564},{\"end\":47802,\"start\":47797},{\"end\":47813,\"start\":47806},{\"end\":47821,\"start\":47817},{\"end\":47831,\"start\":47825},{\"end\":47844,\"start\":47835},{\"end\":48091,\"start\":48086},{\"end\":48102,\"start\":48095},{\"end\":48113,\"start\":48106},{\"end\":48301,\"start\":48292},{\"end\":48313,\"start\":48305},{\"end\":48326,\"start\":48317},{\"end\":48504,\"start\":48494},{\"end\":48743,\"start\":48736},{\"end\":48753,\"start\":48747},{\"end\":48762,\"start\":48757},{\"end\":49016,\"start\":49010},{\"end\":49023,\"start\":49020},{\"end\":49034,\"start\":49027},{\"end\":49042,\"start\":49038},{\"end\":49264,\"start\":49259},{\"end\":49274,\"start\":49268},{\"end\":49284,\"start\":49278},{\"end\":49410,\"start\":49408},{\"end\":49426,\"start\":49416},{\"end\":49445,\"start\":49430},{\"end\":49455,\"start\":49449},{\"end\":49468,\"start\":49459},{\"end\":49658,\"start\":49655},{\"end\":49666,\"start\":49662},{\"end\":49676,\"start\":49670},{\"end\":49892,\"start\":49889},{\"end\":49898,\"start\":49896},{\"end\":49909,\"start\":49907},{\"end\":49918,\"start\":49913},{\"end\":50161,\"start\":50156},{\"end\":50172,\"start\":50165},{\"end\":50183,\"start\":50176},{\"end\":50194,\"start\":50187},{\"end\":50203,\"start\":50198},{\"end\":50426,\"start\":50420},{\"end\":50436,\"start\":50432},{\"end\":50446,\"start\":50442},{\"end\":50462,\"start\":50450},{\"end\":50473,\"start\":50466},{\"end\":50710,\"start\":50708},{\"end\":50722,\"start\":50714},{\"end\":50731,\"start\":50726},{\"end\":50742,\"start\":50735},{\"end\":50972,\"start\":50970},{\"end\":50981,\"start\":50976},{\"end\":50992,\"start\":50985},{\"end\":51229,\"start\":51221},{\"end\":51240,\"start\":51233},{\"end\":51474,\"start\":51462},{\"end\":51482,\"start\":51478},{\"end\":51493,\"start\":51486},{\"end\":51503,\"start\":51497},{\"end\":51692,\"start\":51685},{\"end\":51703,\"start\":51696},{\"end\":51712,\"start\":51707},{\"end\":51723,\"start\":51716},{\"end\":51730,\"start\":51727},{\"end\":51743,\"start\":51734},{\"end\":51753,\"start\":51747},{\"end\":51766,\"start\":51757},{\"end\":52015,\"start\":52009},{\"end\":52026,\"start\":52021},{\"end\":52037,\"start\":52030},{\"end\":52047,\"start\":52041},{\"end\":52055,\"start\":52051},{\"end\":52229,\"start\":52226},{\"end\":52235,\"start\":52233},{\"end\":52242,\"start\":52239},{\"end\":52248,\"start\":52246},{\"end\":52255,\"start\":52252},{\"end\":52261,\"start\":52257},{\"end\":52495,\"start\":52484},{\"end\":52506,\"start\":52499},{\"end\":52514,\"start\":52510},{\"end\":52661,\"start\":52654},{\"end\":52671,\"start\":52665},{\"end\":52678,\"start\":52675},{\"end\":52691,\"start\":52682},{\"end\":52970,\"start\":52955},{\"end\":52980,\"start\":52974},{\"end\":52991,\"start\":52984},{\"end\":53003,\"start\":52995},{\"end\":53016,\"start\":53007},{\"end\":53028,\"start\":53022},{\"end\":53309,\"start\":53303},{\"end\":53322,\"start\":53313},{\"end\":53335,\"start\":53326},{\"end\":53582,\"start\":53571},{\"end\":53707,\"start\":53703},{\"end\":53717,\"start\":53711},{\"end\":53732,\"start\":53721},{\"end\":53740,\"start\":53736},{\"end\":53754,\"start\":53744},{\"end\":54039,\"start\":54035},{\"end\":54049,\"start\":54043},{\"end\":54058,\"start\":54053},{\"end\":54064,\"start\":54062},{\"end\":54073,\"start\":54068},{\"end\":54083,\"start\":54079},{\"end\":54273,\"start\":54269},{\"end\":54280,\"start\":54277},{\"end\":54286,\"start\":54284},{\"end\":54298,\"start\":54288},{\"end\":54574,\"start\":54570},{\"end\":54582,\"start\":54578},{\"end\":54749,\"start\":54741},{\"end\":54762,\"start\":54753},{\"end\":54973,\"start\":54970},{\"end\":54981,\"start\":54977},{\"end\":54987,\"start\":54985},{\"end\":54999,\"start\":54994},{\"end\":55248,\"start\":55244},{\"end\":55256,\"start\":55252},{\"end\":55263,\"start\":55260},{\"end\":55454,\"start\":55448},{\"end\":55463,\"start\":55458},{\"end\":55645,\"start\":55639},{\"end\":55656,\"start\":55649},{\"end\":55667,\"start\":55660},{\"end\":55676,\"start\":55671},{\"end\":55687,\"start\":55680},{\"end\":55697,\"start\":55691},{\"end\":55704,\"start\":55701},{\"end\":55712,\"start\":55708},{\"end\":55722,\"start\":55716},{\"end\":56064,\"start\":56060},{\"end\":56072,\"start\":56068},{\"end\":56301,\"start\":56298},{\"end\":56309,\"start\":56305},{\"end\":56316,\"start\":56313},{\"end\":56326,\"start\":56320},{\"end\":56334,\"start\":56330},{\"end\":56343,\"start\":56338},{\"end\":56579,\"start\":56577},{\"end\":56587,\"start\":56583},{\"end\":56594,\"start\":56591},{\"end\":56603,\"start\":56598},{\"end\":56805,\"start\":56802},{\"end\":56813,\"start\":56809},{\"end\":56822,\"start\":56817},{\"end\":56829,\"start\":56826},{\"end\":57046,\"start\":57043},{\"end\":57058,\"start\":57050},{\"end\":57264,\"start\":57262},{\"end\":57273,\"start\":57268},{\"end\":57279,\"start\":57277},{\"end\":57479,\"start\":57477},{\"end\":57488,\"start\":57483},{\"end\":57670,\"start\":57668},{\"end\":57679,\"start\":57674},{\"end\":57687,\"start\":57683},{\"end\":57694,\"start\":57691},{\"end\":57704,\"start\":57701},{\"end\":57872,\"start\":57868},{\"end\":57881,\"start\":57879},{\"end\":58049,\"start\":58044},{\"end\":58055,\"start\":58053},{\"end\":58063,\"start\":58059},{\"end\":58071,\"start\":58067},{\"end\":58084,\"start\":58075},{\"end\":58332,\"start\":58327},{\"end\":58338,\"start\":58336},{\"end\":58344,\"start\":58342},{\"end\":58352,\"start\":58348},{\"end\":58361,\"start\":58356},{\"end\":58369,\"start\":58365},{\"end\":58375,\"start\":58373},{\"end\":58795,\"start\":58791},{\"end\":60237,\"start\":60232},{\"end\":60831,\"start\":60829},{\"end\":61448,\"start\":61416},{\"end\":62258,\"start\":62248},{\"end\":62271,\"start\":62262},{\"end\":62957,\"start\":62954},{\"end\":63325,\"start\":63321},{\"end\":65369,\"start\":65353},{\"end\":65386,\"start\":65371},{\"end\":65634,\"start\":65626},{\"end\":65807,\"start\":65797},{\"end\":65811,\"start\":65809},{\"end\":65977,\"start\":65968},{\"end\":66150,\"start\":66143},{\"end\":66321,\"start\":66306},{\"end\":66502,\"start\":66493},{\"end\":66669,\"start\":66660},{\"end\":66708,\"start\":66699},{\"end\":66744,\"start\":66737},{\"end\":66927,\"start\":66908},{\"end\":66978,\"start\":66971},{\"end\":67637,\"start\":67627},{\"end\":67957,\"start\":67955},{\"end\":69854,\"start\":69836}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":3310672},\"end\":41860,\"start\":41560},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":2645819},\"end\":42060,\"start\":41862},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":218971783},\"end\":42404,\"start\":42062},{\"attributes\":{\"doi\":\"arXiv:1902.06705\",\"id\":\"b3\"},\"end\":42742,\"start\":42406},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":253761231},\"end\":43041,\"start\":42744},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":231648420},\"end\":43434,\"start\":43043},{\"attributes\":{\"doi\":\"arXiv:1705.02900\",\"id\":\"b6\"},\"end\":43822,\"start\":43436},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":57246310},\"end\":44082,\"start\":43824},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":12047850},\"end\":44272,\"start\":44084},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":102350868},\"end\":44539,\"start\":44274},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":225039882},\"end\":44984,\"start\":44541},{\"attributes\":{\"doi\":\"arXiv:1608.00853\",\"id\":\"b11\"},\"end\":45240,\"start\":44986},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":235422571},\"end\":45493,\"start\":45242},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":162168672},\"end\":45754,\"start\":45495},{\"attributes\":{\"doi\":\"arXiv:2103.02683\",\"id\":\"b14\"},\"end\":46141,\"start\":45756},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":235489699},\"end\":46401,\"start\":46143},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":251649167},\"end\":46676,\"start\":46403},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":52814523},\"end\":46855,\"start\":46678},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":247058667},\"end\":47074,\"start\":46857},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":206594692},\"end\":47257,\"start\":47076},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":9433631},\"end\":47475,\"start\":47259},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":231592390},\"end\":47726,\"start\":47477},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":214743130},\"end\":48008,\"start\":47728},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":219661119},\"end\":48258,\"start\":48010},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":6099034},\"end\":48435,\"start\":48260},{\"attributes\":{\"id\":\"b25\"},\"end\":48659,\"start\":48437},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":219981806},\"end\":48900,\"start\":48661},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":57013724},\"end\":49232,\"start\":48902},{\"attributes\":{\"id\":\"b28\"},\"end\":49376,\"start\":49234},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":146008016},\"end\":49603,\"start\":49378},{\"attributes\":{\"id\":\"b30\"},\"end\":49817,\"start\":49605},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":195317121},\"end\":50089,\"start\":49819},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":3488815},\"end\":50355,\"start\":50091},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":167217657},\"end\":50635,\"start\":50357},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":186363},\"end\":50883,\"start\":50637},{\"attributes\":{\"id\":\"b35\"},\"end\":51129,\"start\":50885},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":6029196},\"end\":51403,\"start\":51131},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":235658909},\"end\":51640,\"start\":51405},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":53012119},\"end\":51933,\"start\":51642},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":226968537},\"end\":52220,\"start\":51935},{\"attributes\":{\"id\":\"b40\"},\"end\":52415,\"start\":52222},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":3719281},\"end\":52650,\"start\":52417},{\"attributes\":{\"id\":\"b42\"},\"end\":52902,\"start\":52652},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":249461756},\"end\":53204,\"start\":52904},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":220425507},\"end\":53520,\"start\":53206},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":11715509},\"end\":53699,\"start\":53522},{\"attributes\":{\"id\":\"b46\"},\"end\":53961,\"start\":53701},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":219983181},\"end\":54265,\"start\":53963},{\"attributes\":{\"id\":\"b48\"},\"end\":54531,\"start\":54267},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":204804905},\"end\":54669,\"start\":54533},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":14124313},\"end\":54883,\"start\":54671},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":239886011},\"end\":55179,\"start\":54885},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":19116245},\"end\":55380,\"start\":55181},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":140311498},\"end\":55582,\"start\":55382},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":211171611},\"end\":56006,\"start\":55584},{\"attributes\":{\"doi\":\"arXiv:2111.10130\",\"id\":\"b55\"},\"end\":56207,\"start\":56008},{\"attributes\":{\"id\":\"b56\"},\"end\":56501,\"start\":56209},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":249017593},\"end\":56738,\"start\":56503},{\"attributes\":{\"id\":\"b58\"},\"end\":56965,\"start\":56740},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":59222483},\"end\":57183,\"start\":56967},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":3851184},\"end\":57406,\"start\":57185},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":49562099},\"end\":57625,\"start\":57408},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":249282169},\"end\":57822,\"start\":57627},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":235804333},\"end\":57972,\"start\":57824},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":59222747},\"end\":58256,\"start\":57974},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":255393958},\"end\":58533,\"start\":58258},{\"attributes\":{\"id\":\"b66\"},\"end\":59454,\"start\":58535},{\"attributes\":{\"id\":\"b67\"},\"end\":60017,\"start\":59456},{\"attributes\":{\"id\":\"b68\"},\"end\":60678,\"start\":60019},{\"attributes\":{\"id\":\"b69\"},\"end\":61244,\"start\":60680},{\"attributes\":{\"id\":\"b70\"},\"end\":61947,\"start\":61246},{\"attributes\":{\"id\":\"b71\"},\"end\":62754,\"start\":61949},{\"attributes\":{\"id\":\"b72\"},\"end\":63249,\"start\":62756},{\"attributes\":{\"id\":\"b73\"},\"end\":64008,\"start\":63251},{\"attributes\":{\"id\":\"b74\"},\"end\":64564,\"start\":64010},{\"attributes\":{\"id\":\"b75\"},\"end\":65184,\"start\":64566},{\"attributes\":{\"id\":\"b76\"},\"end\":65622,\"start\":65186},{\"attributes\":{\"doi\":\"81.84 79.35 69.69 58.53 34.79 61.10 27.03 17.34 16.42 15.11\",\"id\":\"b77\"},\"end\":65787,\"start\":65624},{\"attributes\":{\"doi\":\"69.49 66.83 64.28 60.19 53.24 62.58 53.48 47.30 44.39 43.29\",\"id\":\"b78\"},\"end\":65964,\"start\":65789},{\"attributes\":{\"doi\":\"2021) 21.05 81.50 70.48 54.22 42.23 21.98 36.46 24.99 22.57 21.54 20.60\",\"id\":\"b79\"},\"end\":66139,\"start\":65966},{\"attributes\":{\"doi\":\"82.28 77.73 71.19 63.39 37.89 40.77 28.81 28.39 25.38 26.49\",\"id\":\"b80\"},\"end\":66302,\"start\":66141},{\"attributes\":{\"doi\":\"2022) 33.05 79.49 77.15 74.49 73.03 70.76 69.32 58.03 47.33 31.67 31.56\",\"id\":\"b81\"},\"end\":66489,\"start\":66304},{\"attributes\":{\"doi\":\"85.45 89.14 90.16 88.10 70.66 83.17 80.33 76.91 73.22 72.05\",\"id\":\"b82\"},\"end\":66656,\"start\":66491},{\"attributes\":{\"id\":\"b83\"},\"end\":66695,\"start\":66658},{\"attributes\":{\"id\":\"b84\"},\"end\":66733,\"start\":66697},{\"attributes\":{\"doi\":\"2022) 15.09 78.69 42.11 33.99 29.19 26.66 48.27 29.56 25.14 16.88 14.27\",\"id\":\"b85\"},\"end\":66904,\"start\":66735},{\"attributes\":{\"id\":\"b86\"},\"end\":66967,\"start\":66906},{\"attributes\":{\"doi\":\"82.53 79.01 68.58 59.81 53.02 53.76 48.46 46.79 38.44 42.27\",\"id\":\"b87\"},\"end\":67130,\"start\":66969},{\"attributes\":{\"doi\":\"JPEG G&J Ave. EM 19.32 80.60 84.32 82.12 66.59\",\"id\":\"b88\"},\"end\":67543,\"start\":67132},{\"attributes\":{\"doi\":\"10.01 12.14 50.14 52.07 31.09\",\"id\":\"b89\"},\"end\":67623,\"start\":67545},{\"attributes\":{\"doi\":\"19.71 22.68 28.94 30.51 25.46\",\"id\":\"b90\"},\"end\":67704,\"start\":67625},{\"attributes\":{\"id\":\"b91\"},\"end\":68283,\"start\":67706},{\"attributes\":{\"id\":\"b92\"},\"end\":69313,\"start\":68285},{\"attributes\":{\"id\":\"b93\"},\"end\":70869,\"start\":69315}]", "bib_title": "[{\"end\":41659,\"start\":41560},{\"end\":41902,\"start\":41862},{\"end\":42099,\"start\":42062},{\"end\":42815,\"start\":42744},{\"end\":43127,\"start\":43043},{\"end\":43875,\"start\":43824},{\"end\":44144,\"start\":44084},{\"end\":44360,\"start\":44274},{\"end\":44615,\"start\":44541},{\"end\":45294,\"start\":45242},{\"end\":45574,\"start\":45495},{\"end\":46183,\"start\":46143},{\"end\":46484,\"start\":46403},{\"end\":46716,\"start\":46678},{\"end\":46926,\"start\":46857},{\"end\":47120,\"start\":47076},{\"end\":47299,\"start\":47259},{\"end\":47533,\"start\":47477},{\"end\":47791,\"start\":47728},{\"end\":48082,\"start\":48010},{\"end\":48288,\"start\":48260},{\"end\":48732,\"start\":48661},{\"end\":49006,\"start\":48902},{\"end\":49402,\"start\":49378},{\"end\":49651,\"start\":49605},{\"end\":49885,\"start\":49819},{\"end\":50152,\"start\":50091},{\"end\":50414,\"start\":50357},{\"end\":50702,\"start\":50637},{\"end\":51217,\"start\":51131},{\"end\":51458,\"start\":51405},{\"end\":51681,\"start\":51642},{\"end\":52005,\"start\":51935},{\"end\":52480,\"start\":52417},{\"end\":52951,\"start\":52904},{\"end\":53299,\"start\":53206},{\"end\":53567,\"start\":53522},{\"end\":54031,\"start\":53963},{\"end\":54566,\"start\":54533},{\"end\":54737,\"start\":54671},{\"end\":54966,\"start\":54885},{\"end\":55240,\"start\":55181},{\"end\":55444,\"start\":55382},{\"end\":55635,\"start\":55584},{\"end\":56573,\"start\":56503},{\"end\":57039,\"start\":56967},{\"end\":57258,\"start\":57185},{\"end\":57468,\"start\":57408},{\"end\":57664,\"start\":57627},{\"end\":57861,\"start\":57824},{\"end\":58040,\"start\":57974},{\"end\":58323,\"start\":58258},{\"end\":58787,\"start\":58535},{\"end\":62950,\"start\":62756},{\"end\":63317,\"start\":63251},{\"end\":69832,\"start\":69315}]", "bib_author": "[{\"end\":41672,\"start\":41661},{\"end\":41683,\"start\":41672},{\"end\":41693,\"start\":41683},{\"end\":41915,\"start\":41904},{\"end\":41927,\"start\":41915},{\"end\":41936,\"start\":41927},{\"end\":41944,\"start\":41936},{\"end\":42110,\"start\":42101},{\"end\":42118,\"start\":42110},{\"end\":42127,\"start\":42118},{\"end\":42138,\"start\":42127},{\"end\":42150,\"start\":42138},{\"end\":42162,\"start\":42150},{\"end\":42177,\"start\":42162},{\"end\":42186,\"start\":42177},{\"end\":42196,\"start\":42186},{\"end\":42206,\"start\":42196},{\"end\":42417,\"start\":42406},{\"end\":42428,\"start\":42417},{\"end\":42440,\"start\":42428},{\"end\":42451,\"start\":42440},{\"end\":42461,\"start\":42451},{\"end\":42472,\"start\":42461},{\"end\":42486,\"start\":42472},{\"end\":42495,\"start\":42486},{\"end\":42506,\"start\":42495},{\"end\":42825,\"start\":42817},{\"end\":42833,\"start\":42825},{\"end\":42842,\"start\":42833},{\"end\":42850,\"start\":42842},{\"end\":42857,\"start\":42850},{\"end\":42865,\"start\":42857},{\"end\":42874,\"start\":42865},{\"end\":43144,\"start\":43129},{\"end\":43156,\"start\":43144},{\"end\":43165,\"start\":43156},{\"end\":43173,\"start\":43165},{\"end\":43186,\"start\":43173},{\"end\":43196,\"start\":43186},{\"end\":43209,\"start\":43196},{\"end\":43217,\"start\":43209},{\"end\":43533,\"start\":43526},{\"end\":43547,\"start\":43533},{\"end\":43558,\"start\":43547},{\"end\":43568,\"start\":43558},{\"end\":43576,\"start\":43568},{\"end\":43590,\"start\":43576},{\"end\":43600,\"start\":43590},{\"end\":43885,\"start\":43877},{\"end\":43893,\"start\":43885},{\"end\":43903,\"start\":43893},{\"end\":43912,\"start\":43903},{\"end\":43918,\"start\":43912},{\"end\":43929,\"start\":43918},{\"end\":44155,\"start\":44146},{\"end\":44164,\"start\":44155},{\"end\":44370,\"start\":44362},{\"end\":44378,\"start\":44370},{\"end\":44384,\"start\":44378},{\"end\":44391,\"start\":44384},{\"end\":44632,\"start\":44617},{\"end\":44641,\"start\":44632},{\"end\":44655,\"start\":44641},{\"end\":44670,\"start\":44655},{\"end\":44678,\"start\":44670},{\"end\":44693,\"start\":44678},{\"end\":44705,\"start\":44693},{\"end\":44717,\"start\":44705},{\"end\":44728,\"start\":44717},{\"end\":44737,\"start\":44728},{\"end\":45001,\"start\":44986},{\"end\":45015,\"start\":45001},{\"end\":45024,\"start\":45015},{\"end\":45307,\"start\":45296},{\"end\":45317,\"start\":45307},{\"end\":45329,\"start\":45317},{\"end\":45338,\"start\":45329},{\"end\":45584,\"start\":45576},{\"end\":45594,\"start\":45584},{\"end\":45605,\"start\":45594},{\"end\":45851,\"start\":45843},{\"end\":45864,\"start\":45851},{\"end\":45876,\"start\":45864},{\"end\":45887,\"start\":45876},{\"end\":45897,\"start\":45887},{\"end\":45906,\"start\":45897},{\"end\":45919,\"start\":45906},{\"end\":46193,\"start\":46185},{\"end\":46205,\"start\":46193},{\"end\":46218,\"start\":46205},{\"end\":46229,\"start\":46218},{\"end\":46238,\"start\":46229},{\"end\":46251,\"start\":46238},{\"end\":46492,\"start\":46486},{\"end\":46498,\"start\":46492},{\"end\":46505,\"start\":46498},{\"end\":46513,\"start\":46505},{\"end\":46520,\"start\":46513},{\"end\":46725,\"start\":46718},{\"end\":46736,\"start\":46725},{\"end\":46752,\"start\":46736},{\"end\":46934,\"start\":46928},{\"end\":46941,\"start\":46934},{\"end\":46951,\"start\":46941},{\"end\":47128,\"start\":47122},{\"end\":47137,\"start\":47128},{\"end\":47144,\"start\":47137},{\"end\":47151,\"start\":47144},{\"end\":47310,\"start\":47301},{\"end\":47317,\"start\":47310},{\"end\":47335,\"start\":47317},{\"end\":47351,\"start\":47335},{\"end\":47544,\"start\":47535},{\"end\":47550,\"start\":47544},{\"end\":47562,\"start\":47550},{\"end\":47572,\"start\":47562},{\"end\":47579,\"start\":47572},{\"end\":47583,\"start\":47579},{\"end\":47804,\"start\":47793},{\"end\":47815,\"start\":47804},{\"end\":47823,\"start\":47815},{\"end\":47833,\"start\":47823},{\"end\":47846,\"start\":47833},{\"end\":48093,\"start\":48084},{\"end\":48104,\"start\":48093},{\"end\":48115,\"start\":48104},{\"end\":48303,\"start\":48290},{\"end\":48315,\"start\":48303},{\"end\":48328,\"start\":48315},{\"end\":48506,\"start\":48492},{\"end\":48745,\"start\":48734},{\"end\":48755,\"start\":48745},{\"end\":48764,\"start\":48755},{\"end\":49018,\"start\":49008},{\"end\":49025,\"start\":49018},{\"end\":49036,\"start\":49025},{\"end\":49044,\"start\":49036},{\"end\":49266,\"start\":49257},{\"end\":49276,\"start\":49266},{\"end\":49286,\"start\":49276},{\"end\":49412,\"start\":49404},{\"end\":49428,\"start\":49412},{\"end\":49447,\"start\":49428},{\"end\":49457,\"start\":49447},{\"end\":49470,\"start\":49457},{\"end\":49660,\"start\":49653},{\"end\":49668,\"start\":49660},{\"end\":49678,\"start\":49668},{\"end\":49688,\"start\":49678},{\"end\":49692,\"start\":49688},{\"end\":49894,\"start\":49887},{\"end\":49900,\"start\":49894},{\"end\":49911,\"start\":49900},{\"end\":49920,\"start\":49911},{\"end\":50163,\"start\":50154},{\"end\":50174,\"start\":50163},{\"end\":50185,\"start\":50174},{\"end\":50196,\"start\":50185},{\"end\":50205,\"start\":50196},{\"end\":50428,\"start\":50416},{\"end\":50438,\"start\":50428},{\"end\":50448,\"start\":50438},{\"end\":50464,\"start\":50448},{\"end\":50475,\"start\":50464},{\"end\":50712,\"start\":50704},{\"end\":50724,\"start\":50712},{\"end\":50733,\"start\":50724},{\"end\":50744,\"start\":50733},{\"end\":50974,\"start\":50966},{\"end\":50983,\"start\":50974},{\"end\":50994,\"start\":50983},{\"end\":51231,\"start\":51219},{\"end\":51242,\"start\":51231},{\"end\":51250,\"start\":51242},{\"end\":51254,\"start\":51250},{\"end\":51476,\"start\":51460},{\"end\":51484,\"start\":51476},{\"end\":51495,\"start\":51484},{\"end\":51505,\"start\":51495},{\"end\":51694,\"start\":51683},{\"end\":51705,\"start\":51694},{\"end\":51714,\"start\":51705},{\"end\":51725,\"start\":51714},{\"end\":51732,\"start\":51725},{\"end\":51745,\"start\":51732},{\"end\":51755,\"start\":51745},{\"end\":51768,\"start\":51755},{\"end\":52017,\"start\":52007},{\"end\":52028,\"start\":52017},{\"end\":52039,\"start\":52028},{\"end\":52049,\"start\":52039},{\"end\":52057,\"start\":52049},{\"end\":52231,\"start\":52224},{\"end\":52237,\"start\":52231},{\"end\":52244,\"start\":52237},{\"end\":52250,\"start\":52244},{\"end\":52257,\"start\":52250},{\"end\":52263,\"start\":52257},{\"end\":52497,\"start\":52482},{\"end\":52508,\"start\":52497},{\"end\":52516,\"start\":52508},{\"end\":52663,\"start\":52652},{\"end\":52673,\"start\":52663},{\"end\":52680,\"start\":52673},{\"end\":52693,\"start\":52680},{\"end\":52700,\"start\":52693},{\"end\":52707,\"start\":52700},{\"end\":52972,\"start\":52953},{\"end\":52982,\"start\":52972},{\"end\":52993,\"start\":52982},{\"end\":53005,\"start\":52993},{\"end\":53018,\"start\":53005},{\"end\":53030,\"start\":53018},{\"end\":53311,\"start\":53301},{\"end\":53324,\"start\":53311},{\"end\":53337,\"start\":53324},{\"end\":53345,\"start\":53337},{\"end\":53349,\"start\":53345},{\"end\":53584,\"start\":53569},{\"end\":53709,\"start\":53701},{\"end\":53719,\"start\":53709},{\"end\":53734,\"start\":53719},{\"end\":53742,\"start\":53734},{\"end\":53756,\"start\":53742},{\"end\":54041,\"start\":54033},{\"end\":54051,\"start\":54041},{\"end\":54060,\"start\":54051},{\"end\":54066,\"start\":54060},{\"end\":54075,\"start\":54066},{\"end\":54085,\"start\":54075},{\"end\":54275,\"start\":54267},{\"end\":54282,\"start\":54275},{\"end\":54288,\"start\":54282},{\"end\":54300,\"start\":54288},{\"end\":54576,\"start\":54568},{\"end\":54584,\"start\":54576},{\"end\":54751,\"start\":54739},{\"end\":54764,\"start\":54751},{\"end\":54975,\"start\":54968},{\"end\":54983,\"start\":54975},{\"end\":54989,\"start\":54983},{\"end\":55001,\"start\":54989},{\"end\":55008,\"start\":55001},{\"end\":55012,\"start\":55008},{\"end\":55250,\"start\":55242},{\"end\":55258,\"start\":55250},{\"end\":55265,\"start\":55258},{\"end\":55456,\"start\":55446},{\"end\":55465,\"start\":55456},{\"end\":55647,\"start\":55637},{\"end\":55658,\"start\":55647},{\"end\":55669,\"start\":55658},{\"end\":55678,\"start\":55669},{\"end\":55689,\"start\":55678},{\"end\":55699,\"start\":55689},{\"end\":55706,\"start\":55699},{\"end\":55714,\"start\":55706},{\"end\":55724,\"start\":55714},{\"end\":56066,\"start\":56058},{\"end\":56074,\"start\":56066},{\"end\":56081,\"start\":56074},{\"end\":56085,\"start\":56081},{\"end\":56303,\"start\":56296},{\"end\":56311,\"start\":56303},{\"end\":56318,\"start\":56311},{\"end\":56328,\"start\":56318},{\"end\":56336,\"start\":56328},{\"end\":56345,\"start\":56336},{\"end\":56581,\"start\":56575},{\"end\":56589,\"start\":56581},{\"end\":56596,\"start\":56589},{\"end\":56605,\"start\":56596},{\"end\":56807,\"start\":56800},{\"end\":56815,\"start\":56807},{\"end\":56824,\"start\":56815},{\"end\":56831,\"start\":56824},{\"end\":56840,\"start\":56831},{\"end\":56844,\"start\":56840},{\"end\":57048,\"start\":57041},{\"end\":57060,\"start\":57048},{\"end\":57266,\"start\":57260},{\"end\":57275,\"start\":57266},{\"end\":57281,\"start\":57275},{\"end\":57481,\"start\":57470},{\"end\":57490,\"start\":57481},{\"end\":57497,\"start\":57490},{\"end\":57501,\"start\":57497},{\"end\":57672,\"start\":57666},{\"end\":57681,\"start\":57672},{\"end\":57689,\"start\":57681},{\"end\":57696,\"start\":57689},{\"end\":57706,\"start\":57696},{\"end\":57874,\"start\":57863},{\"end\":57883,\"start\":57874},{\"end\":58051,\"start\":58042},{\"end\":58057,\"start\":58051},{\"end\":58065,\"start\":58057},{\"end\":58073,\"start\":58065},{\"end\":58086,\"start\":58073},{\"end\":58095,\"start\":58086},{\"end\":58099,\"start\":58095},{\"end\":58334,\"start\":58325},{\"end\":58340,\"start\":58334},{\"end\":58346,\"start\":58340},{\"end\":58354,\"start\":58346},{\"end\":58363,\"start\":58354},{\"end\":58371,\"start\":58363},{\"end\":58377,\"start\":58371},{\"end\":58797,\"start\":58789},{\"end\":60239,\"start\":60232},{\"end\":60833,\"start\":60829},{\"end\":61450,\"start\":61414},{\"end\":62260,\"start\":62246},{\"end\":62273,\"start\":62260},{\"end\":62959,\"start\":62952},{\"end\":63327,\"start\":63319},{\"end\":64252,\"start\":64245},{\"end\":64672,\"start\":64667},{\"end\":65371,\"start\":65353},{\"end\":65388,\"start\":65371},{\"end\":65636,\"start\":65626},{\"end\":65809,\"start\":65791},{\"end\":65813,\"start\":65809},{\"end\":65979,\"start\":65968},{\"end\":66152,\"start\":66143},{\"end\":66323,\"start\":66306},{\"end\":66504,\"start\":66493},{\"end\":66671,\"start\":66660},{\"end\":66710,\"start\":66699},{\"end\":66746,\"start\":66737},{\"end\":66929,\"start\":66908},{\"end\":66980,\"start\":66971},{\"end\":67557,\"start\":67547},{\"end\":67639,\"start\":67627},{\"end\":67959,\"start\":67955},{\"end\":68745,\"start\":68741},{\"end\":69856,\"start\":69834}]", "bib_venue": "[{\"end\":41697,\"start\":41693},{\"end\":41948,\"start\":41944},{\"end\":42213,\"start\":42206},{\"end\":42558,\"start\":42522},{\"end\":42878,\"start\":42874},{\"end\":43221,\"start\":43217},{\"end\":43524,\"start\":43436},{\"end\":43933,\"start\":43929},{\"end\":44169,\"start\":44164},{\"end\":44395,\"start\":44391},{\"end\":44741,\"start\":44737},{\"end\":45102,\"start\":45040},{\"end\":45355,\"start\":45338},{\"end\":45612,\"start\":45605},{\"end\":45841,\"start\":45756},{\"end\":46258,\"start\":46251},{\"end\":46524,\"start\":46520},{\"end\":46755,\"start\":46752},{\"end\":46955,\"start\":46951},{\"end\":47155,\"start\":47151},{\"end\":47355,\"start\":47351},{\"end\":47587,\"start\":47583},{\"end\":47853,\"start\":47846},{\"end\":48125,\"start\":48115},{\"end\":48335,\"start\":48328},{\"end\":48490,\"start\":48437},{\"end\":48768,\"start\":48764},{\"end\":49056,\"start\":49044},{\"end\":49255,\"start\":49234},{\"end\":49476,\"start\":49470},{\"end\":49701,\"start\":49692},{\"end\":49943,\"start\":49920},{\"end\":50212,\"start\":50205},{\"end\":50482,\"start\":50475},{\"end\":50748,\"start\":50744},{\"end\":50964,\"start\":50885},{\"end\":51258,\"start\":51254},{\"end\":51509,\"start\":51505},{\"end\":51772,\"start\":51768},{\"end\":52061,\"start\":52057},{\"end\":52308,\"start\":52263},{\"end\":52522,\"start\":52516},{\"end\":52761,\"start\":52707},{\"end\":53037,\"start\":53030},{\"end\":53353,\"start\":53349},{\"end\":53599,\"start\":53584},{\"end\":53819,\"start\":53756},{\"end\":54100,\"start\":54085},{\"end\":54382,\"start\":54300},{\"end\":54592,\"start\":54584},{\"end\":54768,\"start\":54764},{\"end\":55019,\"start\":55012},{\"end\":55269,\"start\":55265},{\"end\":55472,\"start\":55465},{\"end\":55742,\"start\":55724},{\"end\":56056,\"start\":56008},{\"end\":56294,\"start\":56209},{\"end\":56609,\"start\":56605},{\"end\":56798,\"start\":56740},{\"end\":57065,\"start\":57060},{\"end\":57285,\"start\":57281},{\"end\":57507,\"start\":57501},{\"end\":57709,\"start\":57706},{\"end\":57887,\"start\":57883},{\"end\":58103,\"start\":58099},{\"end\":58381,\"start\":58377},{\"end\":58865,\"start\":58797},{\"end\":59676,\"start\":59456},{\"end\":60230,\"start\":60019},{\"end\":60827,\"start\":60680},{\"end\":61412,\"start\":61246},{\"end\":62244,\"start\":61949},{\"end\":62994,\"start\":62959},{\"end\":63365,\"start\":63327},{\"end\":64243,\"start\":64010},{\"end\":64665,\"start\":64566},{\"end\":65351,\"start\":65186},{\"end\":67281,\"start\":67132},{\"end\":67953,\"start\":67706},{\"end\":68739,\"start\":68285},{\"end\":69929,\"start\":69856}]"}}}, "year": 2023, "month": 12, "day": 17}