{"id": 248834053, "updated": "2023-10-05 14:19:16.644", "metadata": {"title": "Disentangling Visual Embeddings for Attributes and Objects", "authors": "[{\"first\":\"Nirat\",\"last\":\"Saini\",\"middle\":[]},{\"first\":\"Khoi\",\"last\":\"Pham\",\"middle\":[]},{\"first\":\"Abhinav\",\"last\":\"Shrivastava\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "We study the problem of compositional zero-shot learning for object-attribute recognition. Prior works use visual features extracted with a backbone network, pre-trained for object classification and thus do not capture the subtly distinct features associated with attributes. To overcome this challenge, these studies employ supervision from the linguistic space, and use pre-trained word embeddings to better separate and compose attribute-object pairs for recognition. Analogous to linguistic embedding space, which already has unique and agnostic embeddings for object and attribute, we shift the focus back to the visual space and propose a novel architecture that can disentangle attribute and object features in the visual space. We use visual decomposed features to hallucinate embeddings that are representative for the seen and novel compositions to better regularize the learning of our model. Extensive experiments show that our method outperforms existing work with significant margin on three datasets: MIT-States, UT-Zappos, and a new benchmark created based on VAW. The code, models, and dataset splits are publicly available at https://github.com/nirat1606/OADis.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2205.08536", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/SainiPS22", "doi": "10.1109/cvpr52688.2022.01329"}}, "content": {"source": {"pdf_hash": "b3c514de08f3953e00464f63464e1241396b54b8", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2205.08536v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4675a10e5da2156cc82d1fd405cb9b7dd6b80e41", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b3c514de08f3953e00464f63464e1241396b54b8.txt", "contents": "\nDisentangling Visual Embeddings for Attributes and Objects\n\n\nNirat Saini \nUniversity of Maryland\nCollege Park\n\nKhoi Pham \nUniversity of Maryland\nCollege Park\n\nAbhinav Shrivastava \nUniversity of Maryland\nCollege Park\n\nDisentangling Visual Embeddings for Attributes and Objects\n\nWe study the problem of compositional zero-shot learning for object-attribute recognition. Prior works use visual features extracted with a backbone network, pre-trained for object classification and thus do not capture the subtly distinct features associated with attributes. To overcome this challenge, these studies employ supervision from the linguistic space, and use pre-trained word embeddings to better separate and compose attribute-object pairs for recognition. Analogous to linguistic embedding space, which already has unique and agnostic embeddings for object and attribute, we shift the focus back to the visual space and propose a novel architecture that can disentangle attribute and object features in the visual space. We use visual decomposed features to hallucinate embeddings that are representative for the seen and novel compositions to better regularize the learning of our model. Extensive experiments show that our method outperforms existing work with significant margin on three datasets: MIT-States, UT-Zappos, and a new benchmark created based on VAW. The code, models, and dataset splits are publicly available at https: //github.com/nirat1606/OADis.\n\nIntroduction\n\nObjects in the real world can appear with different properties, i.e., different color, shape, material, etc. For instance, an apple can be red or green, cut or peeled, raw or ripe, and even dirty or clean. Understanding object properties can greatly benefit various applications, e.g., robust object detection [7,17,18,29], human object interaction [10,53,55], and activity recognition [3,5,6,19,21,37]. Since the total number of possible attribute-object pairs in the real world is prohibitively large, it is impractical to collect image examples and train multiple classifiers. Prior works proposed compositional learning, i.e., learning to compose knowledge of known attributes and object concepts to recognize a new attribute-object composition. Datasets such as MIT-States [27] and UT-Zappos [60] are commonly used to study this task, with joint attribute-object recognition for a diverse, yet limited set of objects and attributes. (1) one with same object, different attribute Iobjsliced apple, (2) one with same attribute, different object Iattrpeeled orange. We propose a novel architecture that takes I and Iattr, and extracts their visual similarity features for peeled and visual dissimilarity features for orange. Similarly, using I and Iobj, the visual similarity features for apple, and the dissimilarity features for sliced can be extracted. We compose these primitive visual features to hallucinate a seen pair peeled apple, and a novel unseen pair sliced orange to be used for regularizing our embedding space. Note that this is a visualization of embedding space composition, we do not generate images.\n\nCompositional learning refers to combining simple primitive concepts to understand a complex concept. This idea dates back to Recognition and Composition theory by Biederman [8], and early work in the visual domain by Hoffman [25], which proposed recognition by parts for pose estimation. Prior works explore compositionality to a certain degree, e.g., via feature sharing and shared embeddings space. Among them, most works use linguistically inspired losses to separate attributes and objects in the shared embedding space, then use that primitive knowledge to compose new complex pairs. Using linguistic embeddings is helpful since: (1) there is a clear distinction between attribute and object in the embedding space, and (2) these embeddings already contain semantic knowledge of similar objects and attributes, which is helpful for composition. However, unlike word embedding, it is difficult to discrimi-nate the object and attribute in the visual embedding space. This is due to the fact that image feature extractor is usually pre-trained for object classification, often along with image augmentation (e.g., color jitter) that tends to produce attribute-invariant image representation, thus does not learn objects and attributes separately. In this paper, we propose a new direction that focuses on visual cues, instead of using linguistic cues explicitly for novel compositions.\n\nAnalogous to linguistic embedding, our work focuses on disentangling attribute and object in the visual space. Our method, Object Attribute Disentanglement (OADis), learns distinct and independent visual embeddings for peeled and apple from the visual feature of peeled apple. As shown in Figure 1, for image I of peeled apple, we use two other images: one with same object and different attribute I obj (e.g., sliced apple), and one with same attribute and different object I attr (e.g., peeled orange). OADis takes I and I obj and learns the similarity (apple) and dissimilarity (sliced) of the second image with respect to the first one. Similarly, using I and I attr , the commonality between them (peeled) and the left out dissimilarity (orange) can also be extracted. Further, composition of these extracted visual primitives are used to hallucinate seen and unseen pair, peeled apple and sliced orange respectively.\n\nFor compositional learning, it is necessary to decompose first before composing new unseen attribute-object pairs. As humans, we have the ability to imagine an unseen complex concept using previous knowledge of its primitive concepts. For example, if someone has seen a clown and a unicycle, they can imagine clown on a unicycle even if they have never seen this combination in real life [23,47]. This quality of imagination is the basis of various works such as GANs [15], CLIP [51] and DALL-E [52]. However, these works rely on larger datasets and high computation power for training. We study this idea of imagination for a smaller setup by composing newer complex concepts using disentangled attributes and object visual features. Our work focuses on answering the question, can there be visual embedding of peeled and apple, disentangled separately from visual feature of peeled apple? Our contributions are as follows:\n\n\u2022 We propose a novel approach, OADis, to disentangle attribute and object visual features, where visual embedding for peeled is distinct and independent of embedding for apple. \u2022 We compose unseen pairs in the visual space using the disentangled features. Following Compositional Zeroshot Learning (CZSL) setup, we show competitive improvement over prior works on standard datasets [27,60]. \u2022 We propose a new large-scale benchmark for CZSL using an existing attribute dataset VAW [49], and show that OADis outperforms existing baselines.\n\n\nRelated Work\n\nVisual Attributes. Visual attributes have been studied widely to understand visual properties and low-level semantics of objects. These attributes help further improve on various downstream tasks such as object detection [7,14,17,18,29,40], action recognition [3,5,6,19,21,37], image captioning [28,44], and zero-shot and semi-supervised classification [4,13,14,30,43,45,54]. Similar to multi-class classification for objects, initial work for attribute understanding used discriminative models [29,46], without understanding attributes. Other works [11,18,26,35] explored the relation between the same attributes and different objects, to learn visual attributes. Particularly, disentangling object features from attribute features are explored in [20,22]. Although, these works use clustering and probabilistic models to learn the attributes of objects. Compositional Zero-shot Learning. Concept of compositional learning was first introduced in Recognition by Parts [25]. Initially, [39] employed this concept for objects and attributes. Unlike zero-shot learning (ZSL), CZSL requires the model to learn to compose unseen concepts from already learned primitive components. [11,39] proposed separate classifiers for primitive components, and merged all into a final classifier. Most prior works use linguistically inspired auxiliary loss terms to regularize training for embedding space, such as: [42] models attributes as a linear transformation of objects, [33] uses rules of symmetry for understanding states, and [59] learns composition and decomposition of attributes hierarchically. Another set of studies uses language priors to learn unseen attributeobject pairs, either in feature space or with multiple networks [34,50,56]. Other recent works use graph structure to leverage information transfer between seen to unseen pairs using Graph Convolutional Networks [36,41], and [58] uses key-query based attention, along with modular network with message passing for learning relation between primitive concepts.\n\n\nObject Attribute Disentanglement (OADis)\n\nContrary to prior works [33,41,42,59], we explicitly focus on separating attributes and object features in the visual space. More precisely, TMN [50] uses word embeddings to generate attention layers to probe image features corresponding to a given pair, GraphEmbedding [41] exploits the dependency between word embeddings of the labels, and HiDC [59] mainly uses word embeddings to compose novel pairs and generate more examples for their triplet loss. To the best of our knowledge, none of the existing works have explored visual feature disentanglement of attributes and objects. We hypothesize that attribute and object visual features can be separated when considering visual feature similarities and differences between image pairs. Composing these disentangled elements help regularize the com-  mon embedding space to improve recognition performance. More concretely, we take cues from [20] and [39,59], to learn to compose unseen attribute-object pairs leveraging visual attributes based on auxiliary losses.\n\n\nTask Formulation\n\nWe follow the conventional Compositional Zero-shot Learning (CZSL) setup, where distinct attribute-object compositions are used at training and testing. Each image I is labeled with y = y attr,obj \u2208 Y , where y attr and y obj are respectively the attribute and object label. The dataset is divided into two parts, seen pairs y s \u2208 Y s and unseen pairs\ny u \u2208 Y u , such that Y = Y s \u222a Y u , Y s \u2229 Y u = \u2205.\nAlthough y u = y attr,obj \u2208 Y u consists of attribute y attr and object y obj that are never seen together in training, they are separately seen. We employ the Generalized CZSL setup defined in [50], which has seen Y s and unseen pairs Y u in the validation and test sets as detailed in Table 1. As shown in Figure 2, for image I, with label peeled apple, we choose two additional images: one with same object and different attribute I obj (e.g., sliced apple), and another image with same attribute and different object I attr (e.g., peeled orange). Note that the subscript of image symbol, e.g., attr in I attr , shows similarity with I, whereas superscript denotes seen and unseen sets.\n\n\nDisentangling Visual Features\n\nWe extract image and label embedding features from pre-trained networks (ResNet [24] and GloVe [48]). As seen in Figure 2, we use Image Encoder (IE) and Object Conditioned Network (OCN), for image and word embedding features respectively. Similar to [42], we use Label Embedder (LE) as an additional FC-Layer for the image feature. LE and OCN learn image and word embeddings and embed those in a common pair embedding space. Next, visual similarity between I and I obj is computed using Object Affinity Network, which extracts visual features for object, v obj . Whatever is not similar is considered dissimilar. Hence, visual features of I obj that are least similar to visual features of I are considered as the attribute feature v \u2032 attr in I obj , which is sliced in this example. Similarly, Attribute Affinity Network takes I and I attr , and extracts visual similarity feature v attr for peeled, and dissimilar visual features of I attr , as object feature v \u2032 obj for orange. The disentangled features are then used to compose seen and unseen pairs. We discuss the details in the following sections:\n\nImage Encoder (IE). We use the second last layer before AveragePool of an ImageNet-pretrained ResNet-18 [16,24] to extract features for all images. IE is a single convolutional layer that is shared across images I, I attr and I obj to generate their image features, represented as f , f attr and f obj respectively, where each f \u2208 R n\u00d749 and n is the output dimension of IE.\n\nLabel Embedder (LE). Inspired by [42], our LE inputs spatial feature from ResNet [24], AveragePools and passes through a linear layer to extract final feature v attr,obj for pair embedding, which has same dimension as the word embedding final feature w attr,obj , extracted from Object Conditioned Network (OCN) (Figure 2). This is the main branch, and is used for input image I only.\n\nObject Conditioned Network (OCN). This takes word embeddings of attribute emb attr and object emb obj , concatenates the features and passes through multiple layers. Object-conditioned is named because a residual connection for the object feature is concatenated with the final attribute  3), then apply row-wise and column-wise softmax (A and A \u2032 ), followed by a respective column-sum and row-sum to obtain m and mattr. m represents regions where fattr is highly similar to f (hence, we reshape and multiply m with fattr) and mattr represents regions where f is highly similar to fattr (thus, mattr \u00b7 f ). Similarly, S \u2032 represents the regions where feature fattr is not similar to feature f (more details in Section 3.2). The last row shows real samples and generated attention maps overlayed on images. Give image ruffled bag and ruffled flower, we show that attribute ruffle is highlighted in the center mattr \u00b7 f and m \u00b7 fattr. Whereas, m \u2032 obj \u00b7 fattr shows the dissimilar regions of Iattr w.r.t I. (b) Shows the three embedding spaces learnt with different losses. Same notation is used as Figure 2. feature, and the output feature is w attr,obj \u2208 Y . We discuss the motivation for this in Section 4.3. Cosine Classifier (CosCls). Analogous to compatibility function used in [36,41], we use cross-entropy along with cosine similarity to get the final score for each pair. For visual features v attr,obj (from LE), and composed word embeddings w attr,obj (from OCN), CosCls provides logits for an image I. For instance, let us assume v : X \u2192 Z and w : Y \u2192 Z. Z is the common embedding space for word embeddings w and visual embeddings v. Then classifier unit CosCls gives the score for label y \u2208 Y s is C:\nh(v, w) = cos(v, w) = \u03b4 \u00b7 v T w \u2225v\u2225 \u2225w\u2225\n(1)\nC(v, w) = e h(v,w) y\u2208Y s e h(v,y)(2)\nwhere \u03b4 is the temperature variable. Each loss function uses same CosCls score evaluator, with different inputs. Object and Attribute Similarity Modules. Our main contribution is the proposed affinity modules and compositional losses. Inspired by image captioning [12,31,32], OADis uses image similarities and differences to identify visual features corresponding to attributes and objects. Object Affinity Network (OAN) uses f and f obj , whereas Attribute Affinity Network (AAN) uses f and f attr . For brevity, we explain the AAN, while the OAN follows the same architecture. Reminded that both f and f attr \u2208 R n\u00d749 .\n\nSimilar to [57], which computes attention between word concepts with corresponding visual blocks, we compute attention between two images I and I attr . Since both images have the same attribute, i.e., peeled, our affinity network learns visual similarity between the images, which represents the attribute. Similarity matrix S is the cosine similarity between f and f attr , such that S \u2208 R 49\u00d749 as:\nS = f T f attr \u2225f \u2225 2 \u2225f attr \u2225 2(3)\nwhere element s ij represents the similarity between i th element of f with j th element of f attr . Moreover, let s i * and s * j represent the i th row and j th column of S respectively. Then, s i * captures the similarity of all the elements in f attr with respect to i th element of f . To know the most similar element among f attr with respect to i th element of f , we can take a row-wise softmax over S. Similarly, for j th element of f attr , column s * j represents the similarity with all the elements of f . Using a column-wise softmax, we can interpret the most similar and least similar element of f with respect to j th element of f attr , as shown in Figure 3. Therefore, by applying column-wise and row-wise softmax, we get two matrices, A and A \u2032 (A, A \u2032 \u2208 R d\u00d7d , d = 49),\nA i = e \u03bbs i * d j=1 e \u03bbsij and A \u2032 j = e \u03bbs * j d i=1 e \u03bbsij ,(4)\nwhere \u03bb is the inverse temperature parameter. We compute row and column sum for A and A \u2032 respectively, to get final similarity maps, m and m attr ,\nm j = d i=1 A ij and m attri = d j=1 A \u2032 ij .(5)\nSimilarly, the difference between these two images f and f attr is the object label, y obj . Hence, we use the negative of S as the image difference, denoted as S \u2032 . Then, difference of f attr with respect to f would be row-wise softmax of difference matrix, denoted by D. Hence, by performing column-sum over D, we get difference map, m \u2032 obj ,\nD j = e \u03b3s \u2032 * j d i=1 e \u03b3s \u2032 ij and m \u2032 obj i = d j=1 D ij .(6)\nThe final disentangled features for attribute v attr and object v \u2032 obj , for both AAN and OAN, can be computed as:\nv attr = m \u00b7 f attr + m attr \u00b7 f and v \u2032 obj = m \u2032 obj \u00b7 f attr v obj = m \u00b7 f obj + m obj \u00b7 f and v \u2032 attr = m \u2032 attr \u00b7 f obj .(7)\nMore details using a toy example can be seen in Figure 3.\n\nUsing concatenation of v attr and v obj along with a single Linear layer, composes the pair peeled apple, represented by (v attr , v obj ). Similarly, the disentangled visual features v \u2032 attr and v \u2032 obj , are used to compose unseen pair sliced orange, and is represented as\n(v \u2032 attr , v \u2032 obj ).\n\nEmbedding Space Learning objectives\n\nAs shown in Figure 3b, we learn three embedding spaces: (1) attributes space, (2) object space, and (3) attribute-object pair space. The attribute and object spaces are used for disentangling the two, whereas pair embedding is used for final pair composition and inference. OADis has separate loss functions for disentangling and composing. All loss functions are expressed in terms of CosCls defined previously.\n\nThe loss function for main branch, L cls uses combined visual feature v attr,obj from LE and word embedding feature w attr,obj from OCN. L cls is used for the pair embedding space. Similarly, L attr and L obj are used to learn the visual attribute and object feature, in their respective embedding spaces. L attr pushes the visual feature of attribute, closer to the word embedding. L obj does the same for objects in object embedding space Figure 3b. These losses cover the concept of disentanglement, and can be represented as:\n\nL cls = C(v attr,obj , w attr,obj )\nL attr = C(v attr , w attr ); L obj = C(v obj , w obj )(8)\nFor composition, we use L seen and L unseen . Among the images seen (I, I attr , and I obj ), disentangled features v obj and v attr , composes the same pair as (v attr , v obj ), which we refer to as the seen composition. Note that (v attr , v obj ) is different from v attr,obj , as the former is hallucinated feature with combination of disentangled attribute and object visual features, Table 1. This table shows dataset splits. Y s and Y u are seen and unseen compositions respectively. We propose a new benchmarck, VAW-CZSL [49], which has more than 10\u00d7 compositions in each split compared to other datasets.\n\n\nTrain set\n\nVal set Test set and latter is the combined visual feature extracted with LE.\nDatasets attr. obj. Y s Y s /Y u Y s /Y u MIT-\nHere, we use L seen loss which takes the composition of disentangled features and learns to put the composition closer to w attr,obj . Moreover, the dissimilarity aspect from OAN and AAN extracts v \u2032 attr and v \u2032 obj , which composes an unseen pair\n(v \u2032 attr , v \u2032 obj )\n. We use L unseen as unseen loss since the hallucinated composition is never seen among I , I attr , and I obj .\nL seen = C((v attr , v obj ), w attr,obj ) L unseen = C((v \u2032 attr , v \u2032 obj ), w \u2032 attr,obj )(9)\nThe combined loss function L is minimized over all the training images, to train OADis end-to-end. The weights for each loss (\u03b1) are empirically computed:\nL = L cls + \u03b1 1 L attr + \u03b1 2 L obj + \u03b1 3 L seen + \u03b1 4 L unseen .\n\nExperiment\n\n\nDatasets and Metrics\n\nWe show results on three datasets: MIT-states [27], UT-Zappos [60], and a new benchmark for evaluating CZSL on images of objects in-the-wild, referred as VAW-CZSL. VAW-CZSL is created based on images with object and attribute labels from the VAW dataset [49]. Both MITstates [27] and UT-Zappos [60] are common datasets used for this task in previous studies. MIT-states covers wide range of objects (i.e., laptop, fruits, fish, room, etc.) and attributes (i.e., mossy, dirty, raw, etc.), whereas UT-zappos has fewer objects (i.e., shoes type: boots, slippers, sandals) and fine-grained attributes (i.e., leather, fur, etc.). Proposed New Benchmark. While experimenting with MIT-states [27] and UT-Zappos [60], we found several shortcomings with these datasets and discovered issues across all baselines using these datasets: \u2022 Both datasets are small, with a maximum of 2000 attribute-object pairs and 30k images, leading to overfitting fairly quickly. \u2022 Random seed initialization makes performance fluctuate significantly (0.2-0.4% AUC). Moreover, [4] found 70% noise in human-annotated labels on MIT-States [27]. \u2022 A new dataset C-GQA was introduced in [41], but the dataset is still small and we found a lot of discrepancies (kindly refer to the suppl.).  To address these limitations, we propose a new benchmark VAW-CZSL, a subset of VAW [49], which is a multilabel attribute-object dataset. We sample one attribute per image, leading to much larger dataset in comparison to previous datasets as shown in Table 1 (details in the suppl.). Evaluation. We use Generalized CZSL setup, defined in [50], with dataset statistics presented in Table 1. As observed in prior works [41,50], a model trained on a set of labels Y s , does not generalize well on unseen pairs Y u . Therefore, [41,50] use a scalar term for overcoming the negative bias for unseen pairs. We use the same evaluation protocol, which computes Area Under the Curve (AUC) (in %) between the accuracy on seen and unseen compositions with different bias terms [50]. Larger bias term leads to better results for unseen pairs whereas smaller bias leads to better results for seen pairs. Harmonic mean is reported, to balance the bias. We also report the attribute and object accuracy for unseen pairs, to show improvement due to visual disentanglement of features. Our new benchmark subset for VAW [49], follows the similar split as other datasets. In addition, we conduct all experiments with image augmentation for all methods (discussed in Section 4.3).\n\n\nResults and Discussion\n\nBaselines. We compare with related recent and prominent prior works: AttrOp [42], LabelEmbed+ [42], TMN [50], Symnet [33], CompCos [36] and GraphEmb [41]. We do not compare with BMP [58], since it uses the concatenation of features from all four ResNet blocks (960-d features), resulting in higher input features and the number of network parameters than all other setups. Moreover, GraphEmb [41] is state-of-the-art; hence, comparing with that makes our work comparable to other baselines that [41] already outperforms. To be consistent, we state the performance of all models (including GraphEmb [41]) using frozen backbone ResNet without fine-tuning the image features, and using GloVe [48] for the object and attribute word embeddings. Before passing through backbone, training images are augmented with horizontal flip and random crop. Compared to other baselines, OADis uses convolutional features rather than AvgPooled, since it is easier to segregate visual features in the spatial domain for attributes and objects. Moreover, other studies [36,41] have also used additional FC layers on top of IE, which we argue makes it fair for us to use pre-pooled features for OADis. Results on MIT-States. MIT-states has considerable label noise [4], but still is a standard dataset for this task. We show significant improvement on this dataset (reported in Table 2), from previous state-of-the-art GraphEmb, which has 7.2 Val AUC and 5.3 Test AUC. Note that we do not report GraphEmb results with fine-tuning backbone, as we find it incomparable with other baselines that did not incorporate fine-tuning as part of their proposed methods. Overall, our model performs significantly better than GraphEmb on all metrics. Results on UT-Zappos. Similar improvement trends hold for UT-Zapopos as well (see Table 2). Although, as explained for GraphEmb, it is difficult to balance the best performance for Val and Test set in this dataset. The problem is that 7/36 (\u223c20%) attributes in Test set do not appear in Val set. Hence, improving Val set AUC, does not necessarily improve Test AUC for UT-Zappos. Similar trend can be seen for other baselines: CompCos has best Val AUC, but does not perform well on Test set, compared to TMN and Symnet. Even GraphEmb in their final table show the frozen backbone network has much lower performance than TMN. However, OADis performs well on UT-Zappos overall, with \u223c4.0 improvement for Val and Test AUC, HM, unseen and object accuracy. \n\n\n(a)\n\n\nBlack -Sign\n\n\nHanging -Picture Black -Sign Hanging -Painting\n\n\nStained -Wall\n\n\nMarble -Counter Gray -Wall Stained -Wall\n\n\nOn the wall -Clock\n\nOn the wall -Clock Large -Clock Ornate -Clock\n\n\nRelaxing -Man\n\n\nUsed -Bench Sitting -Man Crossing legs -Man\n\n\nYellow -Banana\n\n\nRipe -Banana Piled -Banana Many -Banana\n\n\nBlack lettered -Sign\n\n\nWhite -Sign Blue -Sky Black lettered -Sign\n\n\nCut -Sandwich\n\nHalf -Sandwich Toasted -Bread Cut -Sandwich   Table 4. We quantitatively show that the proposed architecture and different losses help in disentanglement and composition of unseen pairs. The experiments are conducted on MIT-States [27], where change in accuracy is shown with green and red based on increment or decrement respectively from the previous row. A dash (-) represents no change more than (\u00b1 0.1). Refer to Section 4.2 for details.\n\n\nHazy -Mountain\n\n\nHazy -Mountain Far away -Mountain Wide -Mountain\n\n\nPuffy -Couch\n\n\nLosses\n\nVal Results on VAW-CZSL. Our model performs well on VAW-CZSL, and is consistently better than other methods across almost all metrics. As shown in Table 1, VAW-CZSL has \u223c6-8 times more pairs in each split than MIT-States, which shows how challenging the benchmark is. Due to top-1 AUC being too small to quantify any learning and comparing between methods, we report top-3 and top-5 AUC instead. This is also because objects in-the-wild tend to depict multiple possible attributes; hence, evaluating only the top-1 prediction is insufficient. We provide qualitative results of how our model makes object-attribute composition prediction on VAW-CZSL in the suppl. Is disentangling and hallucinating pairs helpful? Prior works rely heavily on word embeddings for this task, but to improve the capabilities of visual systems, it is imperative to explore what is possible in the visual domain. We do an extensive study to understand if our intuition aligns with OADis (Table 4). Here are some takeaways:\n\n\u2022 Using only L cls , we get a benchmark performance based on the architectural contributions, such as LE and ONC.\n\nWhen L attr is added, significant performance boost for attribute accuracy can be seen in Table 4. \u2022 Adding object loss L obj with L cls , makes object accuracy better but no change in Val and Test AUC. This indicates the need of both losses to balance the effects. Using both L attr and L obj gives improvement in all measures. \u2022 Adding L seen results in boost for seen AUC, but drop in Test AUC, which has unseen pairs along with seen pairs. Using unseen loss L unseen leads to increase in both Test and attribute accuracy. \u2022 Finally adding unseen composition loss L unseen along with seen loss L seen , the model improves on most metrics. Each loss plays a role and regularizes effects from other losses.\n\nIs visual disentangling actually happening? Visual disentanglement in feature space is challenging to visualize since: (a) parts of an image for attributes and objects are hard to distinguish, as attributes are aspects of an object; (b) OADis is end-to-end trained with losses to disentangle features for attribute and object embeddings, which is separate from pair embedding space. Inspired by [33,42], we show a few qualitative results in Figure 5. Using all training images, prototype features V attr for each attribute can be computed by averaging features for all images containing that attributes v attr using AAN. Similarly, with OAN, prototype object features are also computed. For each test image, we find top-3 nearest neighbors from these prototype fea-  tures ( Figure 5). Hence, the disentangled prototype features of attributes and objects are used for classifying unseen images. Note that results reported in Table 1 use pair embedding space for attribute and object classification, whereas here we use auxiliary attribute and object embedding spaces (in Figure 3b) for the same task. If disentanglement features are not robust, then composition features will also not be efficient. We also show that using the composition of disentangled features for unseen pairs, relevant images from the test set can be found in suppl.\n\nLimitations. Despite OADis outperforming prior works on all benchmarks, we still notice some outstanding deficiencies in this problem domain. First, similar to [41], OADis often struggles on images containing multiple objects, where it does not know which object to make prediction on. One possible solution is to utilize an objectconditioned attention that allows the model to focus and possibly output attribute for multiple objects. Second, from qualitative studies on VAW-CZSL, we notice there are multiple cases where OADis makes the correct prediction but is considered incorrect by the image label. This is due to the fact that objects in-the-wild are mostly multi-label (containing multiple attributes), which none of the current singlelabel benchmarks have attempted to address.\n\n\nAblation Studies\n\nIn this section, we show experiments to support our design choices for OADis. All the ablations are done for MITstates [27], for one random seed initialization, and are consistent for other datasets as well. Empirical results for \u03bb, \u03b4 and different word embeddings can be found in suppl.\n\n\nWhy Object-Conditioned Network?\n\nLabel Embedder [42] uses a linear layer and concatenates word embeddings for attributes and objects. We experiment with other networks: MLP with more parameters with two layers and ReLU and Object-conditioned network that uses a residual connection for object embedding. Our intuition is that same attribute contributes differently to each object, i.e., ruffled bag is very different from ruffled flower. Hence, attributes are conditioned on object. Adding a residual connection for object embeddings to the final attribute embedding helps condition the attribute. We empirically demonstrate that object-conditioning helps in Table 5 (refer to the suppl.).\n\nTo augment or not to augment? Augmentation is a common technique to reduce over-fitting and improve generalization. Surprisingly, prior works do not use any image augmentation. OADis without augmentation gives 6.7% AUC on Val and 5.1% AUC on Test set for MIT-states. Hence, we use augmentation for OADis and re-implemented rest of the baselines in Table 2, showing that augmentation helps improving all methods \u223c1.0-1.5% AUC. We use horizontal flip and random crop as augmentation.\n\n\nQualitative results\n\nTo qualitatively analyze our hallucinated compositions, we perform a nearest neighbor search on all three datasets. We pick the unseen compositions composed using the disentangled features, and find their top-5 nearest neighbors from the validation and test set. Figure 4(a) illustrates a few of our results. Note that these pairs are never seen in training. Based on the hallucinated compositions of disentangled attributes and objects, we are able to retrieve samples from these unseen compositions.\n\nIn Figure 4(b), we show the top-3 predictions of OADis on VAW-CSZL. Column 1 shows results for seen, and columns 2 and 3 show unseen compositions, with the ground-truth label on top (bold black). In all examples, our top-3 predictions describe the visual content of the images accurately, even though in many cases the ground-truth label is not predicted in top-1. For column 3, we purposely show examples where our model predictions totally differ from the ground-truth label, but still correctly describe the visual information in each image. Similar to [41], this explains the multi-label nature of object-attribute recognition, and why we report top-3 and top-5 metrics for the VAW-CZSL benchmark.\n\n\nConclusion\n\nIn this work, we demonstrated the ability to disentangle object and attribute in the visual feature space, that are used for hallucinating novel complex concepts, as well as regularizing and obtaining a better object-attribute recognition model. Through extensive experiments, we show the efficacy of our method, and surpass previous methods across three different benchmarks. In addition, we also propose a new benchmark for the compositional zero-shot learning task with images of objects in-the-wild, which we believe can help shift the focus of the community towards images in more complex scenes. Finally, we also highlight limitations of our work, including the notable problem of multi-label in object attributes, which we hope would encourage future works to start tackling CSZL for more realistic scenarios.\n\n\nAppendix\n\nA. Dataset issues of C-GQA [41] GraphEmb [41] proposes a new benchmark for compositional zero-shot learning. However, there are some issues with the dataset have been raised on their official github page [1, 2]. These issues are related to (1) the attributeobject pairs being placed into the incorrect train, validation, and test subset, and (2) there are missing images for a decent amount of pairs (20%), which could potentially affect the final experiment results. Due to [41] being unable to provide a corrected version of the dataset in time before the CVPR 2022 deadline, we were unable to run any experiments for C-GQA. Post the deadline, we did run some preliminary results where our method outperformed GraphEmb [41]. Although, a major issue we observed was for OADis, C-GQA [41] training set did not have similar attributes and objects samples for constructing I attr and I obj . However, we propose for learning compositional concepts, firstly disentangled concepts must be learnt, and for that, we require I attr and I obj . Hence, we do not report results on C-GQA for OADis.\n\n\nB. Dataset Creation: VAW-CZSL\n\nWe propose a new benchmark for the compositional zeroshot learning task (CZSL), focusing on images of objects and attributes in the wild that span across a much larger number of categories. We select the VAW dataset [49] to create our benchmark. VAW contains images originally from Visual Genome (thus objects and attributes in the wild). Every image of an object instance contains an object label and one (or possibly multiple) attribute labels. In the followings, we describe our steps in creating the VAW-CZSL benchmark, which shares some similarities with the C-GQA dataset.\n\nDifferent from C-GQA, we consider object instances whose bounding boxes are larger than 50 x 50. C-GQA selected instances whose boxes are larger than 112 x 112, which could possibly leave out small, narrow objects that are still recognizable from images. For every object instance, among its possibly multiple attributes label, we keep only one attribute that has the lowest frequency in the dataset (i.e., the uncommon attribute) to be consistent with the standard CZSL benchmark. By keeping the most uncommon attribute and using the top-3 & 5 evaluation metrics, all methods will be evaluated based on whether they are able to rank this uncommon (but still representative) attribute in its top-3 & 5 predictions rather than always predicting the most frequent attributes. From this, we follow the similar steps from [41] to merge plurals and synonyms (e.g., {airplane, plane, aeroplane, airplanes...}, {rock, stone, rocks...}). We then keep only those attribute and object categories with frequency greater than 30 to make sure all primitive concepts have a decent amount of data for training and evaluating.\n\nWe use images in VAW-training as our training set, and use images in VAW-val and VAW-test for creating the validation and testing splits following the standard generalized benchmark in CZSL. We first merge VAW-val and VAW-test in one set, and follow similar steps mentioned in [41] to create a validation and test set of seen and unseen attribute-object pairs. At the end, we remove objects and attributes that no longer appear in the training set. This is because a model that has never seen an attribute (or object) will find it impossible to generalize to unseen pairs containing this attribute (or object). This problem happens with the C-GQA dataset where 8% of attribute and 22% of object categories do not exist in their training set. More details about dataset can be found in Table 6. The dataset splits are made publicly available at https://github.com/nirat1606/OADis.\n\n\nC. Implementation Details\n\nFollowing baselines, we use ResNet18 [24] pre-trained on Imagenet [16] as backbone feature extractor. Since, proposed auxiliary losses leverage image features, we use a single convolutional layer with Batch Normalization, ReLU and dropout for Image embedder with output dimension 1024 and dropout as 0.3. Note that we extract ResNet features before average pool. For word embeddings, we initialize with GLoVe [48]. Object Conditioned network, uses multiple linear layers, first for objects and attributes separately, then for concatenated features. Label embedder takes 1024-d feature, performs AveragePool and finally embeds in a 300-d space. Each loss uses compatibility function, i.e. cosine similarity, followed by cross-entropy loss over the compatibility function. Object similarity and attribute similarity modules also use two linear layers with dropout 0.05. On UT-Zappos, because the dataset is very small, we find using a linear layer (a smaller and simpler module than OCN) with dropout 0.1 results in better performance. We use Adam optimizer with weight decay 5e \u22125 , and learning rate 2.5e \u22126 for the GLoVe embedding. The learning rate for the rest of the model is 3e  \n\n\nD. Ablation studies (extension)\n\nAs mentioned in the paper, we show ablation for various other parameters. All the ablations are done for MITstates [27], for one random seed initialization, and are consistent for other datasets as well.\n\n\nD.1. Choice of word embeddings\n\nPrior works [33,36,41] experiment with various kinds of word embeddings. In fact, GraphEmb [41] has more advantages over all other baselines, since they use a combination of word embeddings word2vec [38] and fasttext [9], whereas rest of the works use GloVe [48] only. To keep the results fair between all methods, we run all the baselines, even GraphEmb [41] with only GloVe [48], and report the accuracy in Table 1, in the main paper. Results for using different embedding combinations is shown in Table 7.\n\nOverall, since our method uses word embeddings for visual disentanglement, the choice of word embeddings does not impact the performance much. Although, empirically, we found our model performs best when GloVe embeddings are used.\n\n\nD.2. Object-conditioned network\n\nWe experiment with different networks on top of word embeddings, namely Linear, MLP and Object-Conditioned. Object conditioned network uses word embedding for object to concatenate with attribute-object composition embeddings. We show in Figure 6, the diagrammatic representation of different networks.\n\n\nD.3. Values for \u03bb and \u03b4\n\nWe find the temperature variables \u03bb and \u03b4 empirically. The values \u03bb = 10 and \u03b4 = 0.05 works best for OADis. Ta-  \n\n\nD.4. Different weights for losses\n\nWe mention different weights for each loss function in the paper, in Section 3.3. Each \u03b1 value is empirically found, and is used in the following equation for final loss function: L = L cls + \u03b1 1 L attr + \u03b1 2 L obj + \u03b1 3 L seen + \u03b1 4 L unseen Note that L cls is the main branch. The object and attribute losses are complementary, as shown in paper (Table 4).  Hence, \u03b1 1 and \u03b1 2 , which are the weights for L attr and L obj share the same values, i.e. 0.5. Finally, \u03b1 4 and \u03b1 5 have the same value since both are composition losses for seen and unseen pairs, i.e. 0.05. The chosen weights for \u03b1 values are in bold in Table 9.\n\n\nE. Qualitative results\n\nWe show more qualitative results to support our architecture for different datasets.\n\n\nE.1. UT-Zappos.\n\nWe show nearest neighbor results in paper for MIT-States [27] (Fig. 4(a)). Here, we show similar study for UT-Zappos [60] in Figure 8. Using the hallucinated composed features of unseen pairs, we find the top 5 nearest neighbors from test set. The red boxes show incorrect labels, where green show the correct labels.\n\n\nE.2. Attention Maps\n\nIn Figure 7 and 9, we show the qualitative results on MIT-States [27] and VAW-CZSL, with examples f and f attr and overlayed feature maps. To re-iterate, for images with features f and f attr , m attr \u00b7 f shows how the regions in f which are most similar to f attr , and m\u00b7f attr shows the regions in f attr which are most similar to regions in f . Lastly, m \u2032 obj \u00b7 f attr shows the regions of f attr which are most dissimilar to f . Although, the overlayed attention maps for similarity and dissimilarity make sense most of the times ( Figure 7(b)), due to some inconsistencies in dataset, we still find some samples where is it difficult to disentangle the attribute and object features. The main reasons why this happens is:\n\n\u2022 Some concepts are abstract, such as clear sky, pressed metal, dirty floor ( fig. 7(a)), since it is very difficult to separate dirty from floor. Hence, the attention maps for similarity and dissimilarity do not make much sense. \u2022 Some images in MIT-States and even in other dataset  fig. 7(a)), which makes it difficult to learn attributes from those. \u2022 Finally, for some cases, like narrow valley, our method fails to disentangle attribute and object similarity, due to various objects in the scene. For future work, using a foreground and background separator before finding similarities and dissimilarities between features can be helpful.\n\n\nF. Negative Impact of our work\n\nOur work is a new initiative in the direction of learning visual features for objects and it's attributes. We present it as a prototype, or an alternative direction for understanding attributes-object pairs. Similar to any other work in vision, learning attributes of objects can have various positive implications, e.g. in object detection, knowing attributes can provide additional knowledge about the objects. However, knowing the additional information about attributes, it can be used for persuasion for marketing policies, for even worse factors. Even though it seems very far fetched ideas, but using attribute classification along with object detection, knowing the attributes people can build weapons and ammunition to either counter attack the present ammunition. Attribute classification can also be used on humans, to detect certain traits of human for bypassing large-scale surveillance applications. In general, attributes provide additional information for objects, which can be used negatively or positively.\n\n\nG. Dataset license\n\nBecause we are creating the VAW-CZSL dataset based on the existing VAW dataset, as per the guideline of CVPR 2022, we provide the VAW dataset URL and license as follows:\n\n\u2022 URL: https://vawdataset.com \u2022 License: https : / / github . com / adoberesearch / vaw _ dataset / blob / main / LICENSE.md\n\nFigure 1 .\n1Method illustration: Given an input image I of peeled apple, we use two other images:\n\nFigure 2 .\n2System Overview: Given an image I, for peeled apple, we consider two images:, one with same object: Iobj, sliced apple, and one with same attribute, Iattr peeled orange.(1) The Object-Conditioned Network composes pair word embedding, using GloVe word embeddings for labels. (2) Label Embedder uses the image I and embeds visual feature vattr,obj along with word embedding wattr,obj, using loss Lcls. (3) Attribute Affinity Network and Object Affinity Network, disentangles the same attribute and object from the pair of images I, Iattr and I, Iobj respectively. Disentangled visual features for peeled (vattr) and apple (vobj) are used along with word embeddings of attribute (wattr) and objects (wobj), to compute Lattr and Lobj. (4) Using disentangled features, we compose seen pair peeled apple (vattr, vobj) and unseen pair sliced orange (v \u2032 attr , v \u2032 obj ), for composition losses Lseen and Lunseen.\n\nFigure 3 .\n3(a) Attribute Affinity Module: We compute the cosine similarity between blocks in f and fattr (S in Eq.\n\nFigure 4 .\n4Qualitative Results: We show the nearest neighbors using the hallucinated unseen composition features for MIT-states and UT-Zappos. Although, all the neighbors are not correct (represented with red outline), they look very similar to true class labels: (a) First row: pureed fruit, Second row: engraved coin, Third row: huge tower. (b) We show top-3 predictions for images in VAW-CZSL.\n\nFigure 5 .\n5Qualitative results showing top 3 attributes and objects from test images, using prototype disentangled features computed on training data.\n\n\n\u22124 on MIT-States, and 1e \u22124 on UT-Zappos and VAW-CZSL. We decay the learning rate by 10 at epoch 30 and 40 on MIT-States, at epoch 50 on UT-Zappos, and at epoch 70 on VAW-CZSL. OADis needs to be trained for 70-150 epochs depending on the dataset, and training time is comparable with other methods (5-7 hours). These implementation details are also provided in our released source code.\n\nFigure 6 .\n6We show the different networks used on top of word embeddings. Empirically and following our intuition, Object-Conditioned network works best among the three (rest two are Linear and MLP). (Sec D.2)\n\nFigure 7 .\n7f: browned cake fattr: browned chicken f : clear lake fattr: clear sky f: caramelized nuts fattr: caramelized fish f: coiled bracelet fattr: coiled rug f: pierced basket fattr: pierced brass f: ripe coffee fattr: ripe berries f : dirty floor fattr: dirty pool fattr: narrow cabinet f : narrow valley f : pressed metal fattr: pressed wood f : whipped foam fattr: (a) Failure Cases: Shows the image pairs, f and fattr, and the similarity and dissimilarity map overlayed (details in Sec 7). Moreover, we show for some cases for MIT-States, the examples are very vague or incorrect to actually capture attribute and object concepts separately. For instance, in clear lake and clear sky, it is very difficult to distinguish lake and sky. Hence the similarity and dissimilarity maps do not perform very well. Other examples are also of failure cases where the overlayed similarity and dissimilarity maps do not make sense. (b) Correct Examples: This shows some good examples, where the similarity and dissimilarity maps capture the attibutes and objects correctly for MIT-States.\n\nFigure 8 .Figure 9 .\n89We show the top 5 nearest neighbors using the hallucinated unseen composition features for UT-Zappos. All the neighbors with correct labels are represented by green, whereas incorrect ones are represented with red outline.f : red hat f attr : red jacket f : small tree f attr : small salad f : floral carpet f attr : floral couch Correct Examples: We show the similarity and dissimilarity attention maps overlayed on images for VAW-CZSL as well. To re-iterate, for images with features f and fattr, mattr \u00b7 f shows how the regions in f which are most similar to fattr, and m \u00b7 fattr shows the regions in fattr which are most similar to regions in f . Lastly, m \u2032 obj \u00b7 fattr shows the regions of fattr which are most dissimilar to f . are mislabelled (e.g. whipped foam in\n\nTable 2 .\n2We show results on MIT-states[27] and UT-Zappos[60]. Following[41,50], we use AUC in % between seen and unseen compositions with different bias terms, along with Val, Test, attribute and object accuracy. HM is Harmonic Mean. OADis consistently outperforms on most categories with significant increment.MIT-States \nUT-Zappos \n\nModel \nVal@1 Test@1 HM Seen Unseen Attribute Object \nVal@1 Test@1 HM Seen Unseen Attribute Object \n\nAttrOpr [42] \n2.5 \n2.0 \n10.7 16.6 \n18.4 \n22.9 \n24.7 \n29.9 \n22.8 \n38.1 55.5 \n54.4 \n38.6 \n70.0 \nLabelEmbed+ [42] \n3.5 \n2.3 \n11.5 16.2 \n21.2 \n25.6 \n27.5 \n35.5 \n22.6 \n37.7 53.3 \n58.6 \n40.9 \n69.1 \nTMN [50] \n3.3 \n2.6 \n11.8 22.7 \n17.1 \n21.3 \n24.2 \n35.9 \n28.4 \n44.0 58.2 \n58.0 \n40.8 \n68.4 \nSymnet [33] \n4.5 \n3.4 \n13.8 24.8 \n20.0 \n26.1 \n25.7 \n27.4 \n27.7 \n42.5 56.7 \n61.6 \n44.0 \n70.6 \nCompCos [36] \n6.9 \n4.8 \n16.9 26.9 \n24.5 \n28.3 \n31.9 \n40.8 \n26.9 \n41.1 57.7 \n62.8 \n43.3 \n73.0 \nGraphEmb [41] \n7.2 \n5.3 \n18.1 28.9 \n25.0 \n27.2 \n32.5 \n33.9 \n24.7 \n38.9 58.8 \n61.0 \n44.0 \n72.6 \n\nOADis \n7.6 \n5.9 \n18.9 31.1 \n25.6 \n28.4 \n33.2 \n40.8 \n30.0 \n44.4 59.5 \n65.5 \n46.5 \n75.5 \n\n\n\nTable 3 .\n3We show results on VAW-CZSL. Since it is a much more \nchallenging dataset, with significantly large number of composi-\ntions, to discriminate performance among different baseline, we \nshow top-3 and top-5 AUC (in %) for Val and Test sets. \n\nVal. Set \nTest Set \n\nModel \nV@3 V@5 V@3 V@5 HM Seen Unseen Attr. Obj. \n\nAttrOpr [42] \n1.4 \n2.5 \n1.4 \n2.6 9.1 16.4 11.7 13.7 34.9 \nLabelEmbed+ [42] 1.5 \n2.8 \n1.6 \n2.8 9.8 16.2 13.2 13.4 35.1 \nSymnet [33] \n2.3 \n3.9 \n2.3 \n3.9 12.2 19.1 15.8 18.6 40.9 \nTMN [50] \n2.2 \n3.9 \n2.3 \n4.0 11.9 19.9 15.4 15.9 38.3 \nCompCos [36] \n3.1 \n5.6 \n3.2 \n5.6 14.2 23.9 18.0 16.9 41.9 \nGraphEmb [41] \n2.7 \n5.3 \n2.9 \n5.1 13.0 23.4 16.8 16.9 40.8 \n\nOADis \n3.5 \n6.0 \n3.6 \n6.1 15.2 24.9 18.7 17.5 43.3 \n\n\n\n\nLcls + Lattr + Lobj + Lseen + Lunseen 7.62 (+0.AUC@1 Test AUC@1 Seen \nUnseen \nAttribute \nObject \n\nLcls \n7.24 \n5.43 \n29.92 \n25.33 \n28.03 \n33.10 \nLcls + Lattr \n-\n-\n31.09 (+2.0) -\n28.30 (+0.3) -\nLcls + Lobj \n-\n-\n-\n25.50 (+0.2) -\n33.38 (+0.2) \nLcls + Lattr + Lobj \n7.49 (+0.2) \n5.73 (+0.2) \n-\n-\n28.50 (+0.2) -\nLcls + Lattr + Lobj + Lseen \n-\n5.44 (-0.5) \n31.21 (+0.2) -\n28.18 (-0.4) -\nLcls + Lattr + Lobj + Lunseen \n-\n5.73 (+0.3) \n-\n25.80 (+0.4) 28.51 (+0.4) -\n2) \n5.94 (+0.2) \n31.64 (+0.4) 25.60 (-0.2) 28.51 \n33.20 \n\nTable 5. Results with different networks for word-embeddings. \nObject-conditioning with attribute performs the best, and is there-\nfore used for OADis (Section 4.3). \n\nLinear MLP Obj-cond. Network \n\nVal@1 \n6.6 \n7.0 \n7.6 \nTest@1 \n5.0 \n5.2 \n5.9 \n\n\n\nTable 6 .\n6Dataset Details: This table shows the statistics for different datasets and their splits. The proposed VAW-CZSL benchmark significantly increases the number of attributes and objects.Table 7. Results with pre-trained word-embeddings. GloVe[48] performs the best, and is therefore used for OADis.(Sec D.1)Train set \nVal set \nTest set \n\nDatasets: \nAttr. Obj. Seen Pairs. # Images Seen Pairs Unseen Pairs # Images Seen Pairs Unseen Pairs # Images \n\nMIT-States [27] 115 245 \n1262 \n30338 \n300 \n300 \n10420 \n400 \n400 \n12995 \nUT-Zappos [60] 16 12 \n83 \n22998 \n15 \n15 \n3214 \n18 \n18 \n2914 \nVAW-CZSL [49] 440 541 \n11175 \n72203 \n2121 \n2322 \n9524 \n2449 \n2470 \n10856 \n\nWord Embs \nVal AUC@1 Test AUC@1 \n\nGlove \n7.6 \n5.9 \nFasttext \n7.4 \n5.3 \nWord2vec \n7.5 \n5.4 \nGlove+fasttext \n7.4 \n5.5 \nGlove+word2vec \n7.5 \n5.6 \nFasttext+word2vec \n7.4 \n5.6 \n\n\n\nTable 8 .\n8Results with pre-trained word-embeddings. GloVe[48] performs the best, and is therefore used for OADis. (Sec D.3) ble 8 shows the results for all the different configurations. To understand the effect of each temperature variable, we keep all the rest of the parameters constant and only change the studied parameter.\u03bb \nVal AUC@1 Test AUC@1 \n\n0.01 \n7.5 \n5.6 \n0.1 \n7.5 \n5.7 \n1 \n7.4 \n5.7 \n10 \n7.6 \n5.9 \n100 \n7.4 \n5.7 \n\n\u03b4 \nVal AUC@1 Test AUC@1 \n\n0.01 \n6.4 \n4.8 \n0.05 \n7.6 \n5.9 \n0.1 \n6.7 \n5.2 \n\n\n\nTable 9 .\n9We show empirical weights of each loss function in this table. (Sec D.4) \u03b1 1 and \u03b1 2 \u03b1 3 and \u03b1 4 Val AUC@1 Test AUC@10.1 \n0.05 \n7.1 \n5.7 \n0.5 \n0.1 \n7.0 \n5.3 \n0.1 \n0.05 \n7.5 \n5.8 \n0.5 \n0.05 \n7.6 \n5.9 \n1.0 \n0.05 \n7.3 \n5.6 \n\n\nAcknowledgements. This work was supported by the Air Force (STTR awards FA865019P6014, FA864920C0010), DARPA SAILON program (W911NF2020009) and gifts from Adobe collaboration support fund.\nJoint discovery of object states and manipulation actions. Josef Jean-Baptiste Alayrac, I Sivic, S Laptev, Lacoste-Julien, IEEE International Conference on Computer Vision (ICCV). 1Jean-Baptiste Alayrac, Josef Sivic, I. Laptev, and S. Lacoste- Julien. Joint discovery of object states and manipulation ac- tions. 2017 IEEE International Conference on Computer Vi- sion (ICCV), pages 2146-2155, 2017. 1, 2\n\nA causal view of compositional zero-shot recognition. ArXiv, abs. Y Atzmon, F Kreuk, U Shalit, Gal Chechik, 6Y. Atzmon, F. Kreuk, U. Shalit, and Gal Chechik. A causal view of compositional zero-shot recognition. ArXiv, abs/2006.14610, 2020. 2, 5, 6\n\nRecognizing manipulation actions from state-transformations. Nachwa Abou Bakr, J Crowley, R\u00e9mi Ronfard, Nachwa Abou Bakr, J. Crowley, and R\u00e9mi Ronfard. Rec- ognizing manipulation actions from state-transformations.\n\n. Arxiv, ArXiv, abs/1906.05147, 2019. 1, 2\n\nRecognition and localization of food in cooking videos. R\u00e9mi Nachwa Abou Bakr, J Ronfard, Crowley, CEA/MADiMa '18. 1Nachwa Abou Bakr, R\u00e9mi Ronfard, and J. Crowley. Recog- nition and localization of food in cooking videos. In CEA/MADiMa '18, 2018. 1, 2\n\nCobe: Contextualized object embeddings from narrated instructional video. Gedas Bertasius, Lorenzo Torresani, 1Gedas Bertasius and Lorenzo Torresani. Cobe: Contextu- alized object embeddings from narrated instructional video, 2020. 1, 2\n\nRecognition-by-components: a theory of human image understanding. Irving Biederman, Psychological review. 941Irving Biederman. Recognition-by-components: a theory of human image understanding. Psychological review, 94 2:115-147, 1987. 1\n\nEnriching word vectors with subword information. Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov, Transactions of the Association for Computational Linguistics. 512Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword infor- mation. Transactions of the Association for Computational Linguistics, 5:135-146, 2017. 12\n\nLearning to detect human-object interactions. Yu-Wei Chao, Yunfan Liu, Xieyang Liu, Huayi Zeng, Jia Deng, IEEE Winter Conference on Applications of Computer Vision (WACV). Yu-Wei Chao, Yunfan Liu, Xieyang Liu, Huayi Zeng, and Jia Deng. Learning to detect human-object interactions. 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 381-389, 2018. 1\n\nInferring analogous attributes. Kristen Chao-Yeh Chen, Grauman, IEEE Conference on Computer Vision and Pattern Recognition. Chao-Yeh Chen and Kristen Grauman. Inferring analogous attributes. 2014 IEEE Conference on Computer Vision and Pattern Recognition, pages 200-207, 2014. 2\n\nImram: Iterative matching with recurrent attention memory for cross-modal image-text retrieval. Hui Chen, Guiguang Ding, Xudong Liu, Zijia Lin, Ji Liu, Jungong Han, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Hui Chen, Guiguang Ding, Xudong Liu, Zijia Lin, Ji Liu, and Jungong Han. Imram: Iterative matching with recur- rent attention memory for cross-modal image-text retrieval. 2020 IEEE/CVF Conference on Computer Vision and Pat- tern Recognition (CVPR), pages 12652-12660, 2020. 4\n\nLearning to infer unseen attribute-object compositions. Hui Chen, Zhixiong Nan, Jingjing Jiang, Nanning Zheng, arXiv:2010.14343arXiv preprintHui Chen, Zhixiong Nan, Jingjing Jiang, and Nanning Zheng. Learning to infer unseen attribute-object composi- tions. arXiv preprint arXiv:2010.14343, 2020. 2\n\nNEIL: Extracting Visual Knowledge from Web Data. Xinlei Chen, Abhinav Shrivastava, Abhinav Gupta, IEEE International Conference on Computer Vision (ICCV). Xinlei Chen, Abhinav Shrivastava, and Abhinav Gupta. NEIL: Extracting Visual Knowledge from Web Data. In IEEE International Conference on Computer Vision (ICCV), 2013. 2\n\nStargan v2: Diverse image synthesis for multiple domains. Yunjey Choi, Youngjung Uh, Jaejun Yoo, Jung-Woo Ha, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. 2020 IEEE/CVF Conference on Computer Vision and Pat- tern Recognition (CVPR), pages 8185-8194, 2020. 2\n\nImagenet: A large-scale hierarchical image database. Jia Deng, W Dong, R Socher, L Li, K Li, Li Fei-Fei, 311Jia Deng, W. Dong, R. Socher, L. Li, K. Li, and Li Fei- Fei. Imagenet: A large-scale hierarchical image database. In CVPR 2009, 2009. 3, 11\n\nAttribute-centric recognition for cross-category generalization. Ali Farhadi, Ian Endres, Derek Hoiem, IEEE Computer Society Conference on Computer Vision and Pattern Recognition. 1Ali Farhadi, Ian Endres, and Derek Hoiem. Attribute-centric recognition for cross-category generalization. 2010 IEEE Computer Society Conference on Computer Vision and Pat- tern Recognition, pages 2352-2359, 2010. 1, 2\n\nDescribing objects by their attributes. Ali Farhadi, Ian Endres, Derek Hoiem, David Alexander Forsyth, IEEE Conference on Computer Vision and Pattern Recognition. 1Ali Farhadi, Ian Endres, Derek Hoiem, and David Alexan- der Forsyth. Describing objects by their attributes. 2009 IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 1778-1785, 2009. 1, 2\n\nModeling actions through state changes. Alireza Fathi, James M Rehg, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern Recognition1Alireza Fathi and James M Rehg. Modeling actions through state changes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2579- 2586, 2013. 1, 2\n\nLearning visual attributes. Vittorio Ferrari, Andrew Zisserman, NIPS. 23Vittorio Ferrari and Andrew Zisserman. Learning visual at- tributes. In NIPS, 2007. 2, 3\n\nLearning perceptual causality from video. A Fire, S Zhu, ACM Trans. Intell. Syst. Technol. 72A. Fire and S. Zhu. Learning perceptual causality from video. ACM Trans. Intell. Syst. Technol., 7:23:1-23:22, 2015. 1, 2\n\nWhat visual attributes characterize an object class? In ACCV. Jianlong Fu, Jinqiao Wang, Xin-Jing Wang, Yong Rui, Hanqing Lu, Jianlong Fu, Jinqiao Wang, Xin-Jing Wang, Yong Rui, and Hanqing Lu. What visual attributes characterize an object class? In ACCV, 2014. 2\n\nThe 'power of then. Liane Gabora, The uniquely human capacity to imagine beyond the present. arXiv: Neurons and Cognition. Liane Gabora. The 'power of then': The uniquely human capacity to imagine beyond the present. arXiv: Neurons and Cognition, 2015. 2\n\nDeep residual learning for image recognition. X Kaiming He, Shaoqing Zhang, Jian Ren, Sun, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 311Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2016 IEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR), pages 770-778, 2016. 3, 11\n\nParts of recognition. D Donald, Whitman Hoffman, Richards, Cognition. 182Donald D. Hoffman and Whitman Richards. Parts of recog- nition. Cognition, 18:65-96, 1984. 1, 2\n\nSharing features between objects and their attributes. Sung Ju Hwang, Fei Sha, Kristen Grauman, Sung Ju Hwang, Fei Sha, and Kristen Grauman. Sharing features between objects and their attributes. CVPR 2011, pages 1761-1768, 2011. 2\n\nDiscovering states and transformations in image collections. Phillip Isola, Joseph J Lim, E Adelson, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 1213Phillip Isola, Joseph J. Lim, and E. Adelson. Discover- ing states and transformations in image collections. 2015 IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR), pages 1383-1391, 2015. 1, 2, 5, 6, 7, 8, 12, 13\n\nBabytalk: Understanding and generating simple image descriptions. Girish Kulkarni, Visruth Premraj, Vicente Ordonez, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C Berg, Tamara L Berg, IEEE Trans. Pattern Anal. Mach. Intell. 352Girish Kulkarni, Visruth Premraj, Vicente Ordonez, Sag- nik Dhar, Siming Li, Yejin Choi, Alexander C. Berg, and Tamara L. Berg. Babytalk: Understanding and generating simple image descriptions. IEEE Trans. Pattern Anal. Mach. Intell., 35:2891-2903, 2013. 2\n\nLearning to detect unseen object classes by betweenclass attribute transfer. Christoph H Lampert, Hannes Nickisch, Stefan Harmeling, IEEE Conference on Computer Vision and Pattern Recognition. 1Christoph H. Lampert, Hannes Nickisch, and Stefan Harmel- ing. Learning to detect unseen object classes by between- class attribute transfer. 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 951-958, 2009. 1, 2\n\nLearning grassmann manifolds for object state discovery. Hao-Wei Lee, Chia-Po Wei, Yu-Chiang Frank Wang, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Hao-Wei Lee, Chia-Po Wei, and Yu-Chiang Frank Wang. Learning grassmann manifolds for object state discovery. 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1223-1227, 2017. 2\n\nStacked cross attention for image-text matching. Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, Xiaodong He, abs/1803.08024ArXiv. 4Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xi- aodong He. Stacked cross attention for image-text matching. ArXiv, abs/1803.08024, 2018. 4\n\nVisual semantic reasoning for imagetext matching. Kunpeng Li, Yulun Zhang, K Li, Yuanyuan Li, Yun Raymond Fu, IEEE/CVF International Conference on Computer Vision (ICCV). Kunpeng Li, Yulun Zhang, K. Li, Yuanyuan Li, and Yun Raymond Fu. Visual semantic reasoning for image- text matching. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 4653-4661, 2019. 4\n\nSymmetry and group in attribute-object compositions. Yong-Lu Li, Yue Xu, Xiaohan Mao, Cewu Lu, Yong-Lu Li, Yue Xu, Xiaohan Mao, and Cewu Lu. Sym- metry and group in attribute-object compositions. 2020\n\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 712IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11313-11322, 2020. 2, 6, 7, 12\n\nVisual relationship detection with language priors. Cewu Lu, Ranjay Krishna, Michael S Bernstein, Li Fei-Fei, ECCV. Cewu Lu, Ranjay Krishna, Michael S. Bernstein, and Li Fei- Fei. Visual relationship detection with language priors. In ECCV, 2016. 2\n\nA joint learning framework for attribute models and object descriptions. Sundararajan Dhruv Kumar Mahajan, Vinod Sellamanickam, Nair, International Conference on Computer Vision. Dhruv Kumar Mahajan, Sundararajan Sellamanickam, and Vinod Nair. A joint learning framework for attribute models and object descriptions. 2011 International Conference on Computer Vision, pages 1227-1234, 2011. 2\n\nLearning graph embeddings for open world compositional zero-shot learning. ArXiv, abs. Massimiliano Mancini, Muhammad Ferjad Naeem, Yongqin Xian, Zeynep Akata, 612Massimiliano Mancini, Muhammad Ferjad Naeem, Yongqin Xian, and Zeynep Akata. Learning graph embeddings for open world compositional zero-shot learning. ArXiv, abs/2105.01017, 2021. 2, 4, 6, 12\n\nObject-centric spatiotemporal pyramids for egocentric activity recognition. T Mccandless, K Grauman, BMVC. 1T. McCandless and K. Grauman. Object-centric spatio- temporal pyramids for egocentric activity recognition. In BMVC, 2013. 1, 2\n\nEfficient estimation of word representations in vector space. Tomas Mikolov, Kai Chen, Gregory S Corrado, Jeffrey Dean, ICLR. 12Tomas Mikolov, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. Efficient estimation of word representations in vector space. In ICLR, 2013. 12\n\nFrom red wine to red tomato: Composition with context. I Misra, A Gupta, M Hebert, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 23I. Misra, A. Gupta, and M. Hebert. From red wine to red tomato: Composition with context. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1160-1169, 2017. 2, 3\n\nCross-stitch Networks for Multi-task Learning. Ishan Misra, * , Abhinav Shrivastava, * , Abhinav Gupta, Martial Hebert, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Ishan Misra*, Abhinav Shrivastava*, Abhinav Gupta, and Martial Hebert. Cross-stitch Networks for Multi-task Learn- ing. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 2\n\nLearning graph embeddings for compositional zero-shot learning. ArXiv, abs. Yongqin Muhammad Ferjad Naeem, Federico Xian, Zeynep Tombari, Akata, 12Muhammad Ferjad Naeem, Yongqin Xian, Federico Tombari, and Zeynep Akata. Learning graph embeddings for compositional zero-shot learning. ArXiv, abs/2102.01987, 2021. 2, 4, 5, 6, 8, 11, 12\n\nAttributes as operators: Factorizing unseen attribute-object compositions. Tushar Nagarajan, K Grauman, ECCV. 7Tushar Nagarajan and K. Grauman. Attributes as operators: Factorizing unseen attribute-object compositions. In ECCV, 2018. 2, 3, 6, 7, 8\n\nRecognizing unseen attribute-object pair with generative model. Z Nan, Y Liu, N Zheng, S Zhu, AAAI. Z. Nan, Y. Liu, N. Zheng, and S. Zhu. Recognizing unseen attribute-object pair with generative model. In AAAI, 2019. 2\n\nIm2text: Describing images using 1 million captioned photographs. Vicente Ordonez, Girish Kulkarni, Tamara L Berg, NIPS. Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg. Im2text: Describing images using 1 million captioned pho- tographs. In NIPS, 2011. 2\n\nDevi Parikh, Kristen Grauman, Relative attributes. 2011 International Conference on Computer Vision. Devi Parikh and Kristen Grauman. Relative attributes. 2011 International Conference on Computer Vision, pages 503- 510, 2011. 2\n\nCoco attributes: Attributes for people, animals, and objects. Genevieve Patterson, James Hays, ECCV. Genevieve Patterson and James Hays. Coco attributes: At- tributes for people, animals, and objects. In ECCV, 2016. 2\n\nThe human imagination: the cognitive neuroscience of visual mental imagery. Joel Pearson, Nature Reviews Neuroscience. 2Joel Pearson. The human imagination: the cognitive neu- roscience of visual mental imagery. Nature Reviews Neuro- science, pages 1-11, 2019. 2\n\nGlove: Global vectors for word representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, EMNLP. 1112Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representation. In EMNLP, 2014. 3, 6, 11, 12\n\nLearning to predict visual attributes in the wild. Khoi Pham, Kushal Kafle, Zhe Lin, Zhihong Ding, Scott Cohen, Quan Tran, Abhinav Shrivastava, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)1112Khoi Pham, Kushal Kafle, Zhe Lin, Zhihong Ding, Scott Cohen, Quan Tran, and Abhinav Shrivastava. Learning to predict visual attributes in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021. 2, 5, 6, 11, 12\n\nTask-driven modular networks for zero-shot compositional learning. Senthil Purushwalkam, Maximilian Nickel, Abhinav Gupta, Marc&apos;aurelio Ranzato, IEEE/CVF International Conference on Computer Vision (ICCV). 26Senthil Purushwalkam, Maximilian Nickel, Abhinav Gupta, and Marc'Aurelio Ranzato. Task-driven modular networks for zero-shot compositional learning. 2019 IEEE/CVF In- ternational Conference on Computer Vision (ICCV), pages 3592-3601, 2019. 2, 3, 6\n\nLearning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, abs/2103.00020CoRRAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable vi- sual models from natural language supervision. CoRR, abs/2103.00020, 2021. 2\n\nZero-shot text-to-image generation. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever, abs/2102.12092CoRRAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. CoRR, abs/2102.12092, 2021. 2\n\nScaling human-object interaction recognition through zero-shot learning. Liyue Shen, Serena Yeung, Judy Hoffman, Greg Mori, Li Fei-Fei, IEEE Winter Conference on Applications of Computer Vision (WACV). Liyue Shen, Serena Yeung, Judy Hoffman, Greg Mori, and Li Fei-Fei. Scaling human-object interaction recognition through zero-shot learning. 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 1568- 1576, 2018. 1\n\nConstrained semi-supervised learning using attributes and comparative attributes. Abhinav Shrivastava, Saurabh Singh, Abhinav Gupta, European Conference on Computer Vision. Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Constrained semi-supervised learning using attributes and comparative attributes. In European Conference on Com- puter Vision, 2012. 2\n\nLearning human-object interaction detection using interaction points. Tiancai Wang, Tong Yang, Martin Danelljan, Fahad Shahbaz Khan, X Zhang, Jian Sun, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Tiancai Wang, Tong Yang, Martin Danelljan, Fahad Shahbaz Khan, X. Zhang, and Jian Sun. Learning human-object in- teraction detection using interaction points. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4115-4124, 2020. 1\n\nAdversarial fine-grained composition learning for unseen attribute-object recognition. Kun-Juan Wei, Muli Yang, H Wang, Cheng Deng, Xianglong Liu, IEEE/CVF International Conference on Computer Vision (ICCV). Kun-Juan Wei, Muli Yang, H. Wang, Cheng Deng, and Xi- anglong Liu. Adversarial fine-grained composition learning for unseen attribute-object recognition. 2019 IEEE/CVF In- ternational Conference on Computer Vision (ICCV), pages 3740-3748, 2019. 2\n\n. Guangyue Xu, Parisa Kordjamshidi, Joyce Yue Chai, Zero-shot compositional concept learning. ArXiv, abs/2107.05176, 2021. 4Guangyue Xu, Parisa Kordjamshidi, and Joyce Yue Chai. Zero-shot compositional concept learning. ArXiv, abs/2107.05176, 2021. 4\n\nRelation-aware compositional zeroshot learning for attribute-object pair recognition. Ziwei Xu, Guangzhi Wang, Yongkang Wong, Mohan S Kankanhalli, abs/2108.04603ArXiv. 26Ziwei Xu, Guangzhi Wang, Yongkang Wong, and Mo- han S. Kankanhalli. Relation-aware compositional zero- shot learning for attribute-object pair recognition. ArXiv, abs/2108.04603, 2021. 2, 6\n\nLearning unseen concepts via hierarchical decomposition and composition. Muli Yang, Cheng Deng, Junchi Yan, Xianglong Liu, Dacheng Tao, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 23Muli Yang, Cheng Deng, Junchi Yan, Xianglong Liu, and Dacheng Tao. Learning unseen concepts via hierarchical de- composition and composition. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10245-10253, 2020. 2, 3\n\nFine-grained visual comparisons with local learning. A Yu, K Grauman, IEEE Conference on Computer Vision and Pattern Recognition. 1213A. Yu and K. Grauman. Fine-grained visual comparisons with local learning. 2014 IEEE Conference on Computer Vision and Pattern Recognition, pages 192-199, 2014. 1, 2, 5, 6, 12, 13\n", "annotations": {"author": "[{\"end\":111,\"start\":62},{\"end\":159,\"start\":112},{\"end\":217,\"start\":160}]", "publisher": null, "author_last_name": "[{\"end\":73,\"start\":68},{\"end\":121,\"start\":117},{\"end\":179,\"start\":168}]", "author_first_name": "[{\"end\":67,\"start\":62},{\"end\":116,\"start\":112},{\"end\":167,\"start\":160}]", "author_affiliation": "[{\"end\":110,\"start\":75},{\"end\":158,\"start\":123},{\"end\":216,\"start\":181}]", "title": "[{\"end\":59,\"start\":1},{\"end\":276,\"start\":218}]", "venue": null, "abstract": "[{\"end\":1459,\"start\":278}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1788,\"start\":1785},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1791,\"start\":1788},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":1794,\"start\":1791},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":1797,\"start\":1794},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1828,\"start\":1824},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":1831,\"start\":1828},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":1834,\"start\":1831},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1864,\"start\":1861},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1866,\"start\":1864},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1868,\"start\":1866},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1871,\"start\":1868},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":1874,\"start\":1871},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":1877,\"start\":1874},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2257,\"start\":2253},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":2276,\"start\":2272},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3275,\"start\":3272},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3328,\"start\":3324},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5805,\"start\":5801},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":5808,\"start\":5805},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5885,\"start\":5881},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":5896,\"start\":5892},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":5912,\"start\":5908},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6725,\"start\":6721},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":6728,\"start\":6725},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":6824,\"start\":6820},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7118,\"start\":7115},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7121,\"start\":7118},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7124,\"start\":7121},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7127,\"start\":7124},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7130,\"start\":7127},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7133,\"start\":7130},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7157,\"start\":7154},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7159,\"start\":7157},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7161,\"start\":7159},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7164,\"start\":7161},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7167,\"start\":7164},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7170,\"start\":7167},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7193,\"start\":7189},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7196,\"start\":7193},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7250,\"start\":7247},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7253,\"start\":7250},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7256,\"start\":7253},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7259,\"start\":7256},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7262,\"start\":7259},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":7265,\"start\":7262},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":7268,\"start\":7265},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7393,\"start\":7389},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7396,\"start\":7393},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7448,\"start\":7444},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7451,\"start\":7448},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7454,\"start\":7451},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7457,\"start\":7454},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7647,\"start\":7643},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7650,\"start\":7647},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7867,\"start\":7863},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7884,\"start\":7880},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8075,\"start\":8071},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8078,\"start\":8075},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8298,\"start\":8294},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8360,\"start\":8356},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":8418,\"start\":8414},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8623,\"start\":8619},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":8626,\"start\":8623},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":8629,\"start\":8626},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8771,\"start\":8767},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8774,\"start\":8771},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":8784,\"start\":8780},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8987,\"start\":8983},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8990,\"start\":8987},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8993,\"start\":8990},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":8996,\"start\":8993},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":9108,\"start\":9104},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9233,\"start\":9229},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":9310,\"start\":9306},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9857,\"start\":9853},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9866,\"start\":9862},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":9869,\"start\":9866},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":10600,\"start\":10596},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11209,\"start\":11205},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":11224,\"start\":11220},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11379,\"start\":11375},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12341,\"start\":12337},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12344,\"start\":12341},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":12646,\"start\":12642},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12694,\"start\":12690},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":14282,\"start\":14278},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":14285,\"start\":14282},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":15057,\"start\":15053},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":15060,\"start\":15057},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15063,\"start\":15060},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":15427,\"start\":15423},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":19537,\"start\":19533},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20543,\"start\":20539},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":20559,\"start\":20555},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":20751,\"start\":20747},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20772,\"start\":20768},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":20791,\"start\":20787},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21182,\"start\":21178},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":21201,\"start\":21197},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":21546,\"start\":21543},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21607,\"start\":21603},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":21653,\"start\":21649},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":21840,\"start\":21836},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":22094,\"start\":22090},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":22173,\"start\":22169},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":22176,\"start\":22173},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":22281,\"start\":22277},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":22284,\"start\":22281},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":22523,\"start\":22519},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":22859,\"start\":22855},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":23120,\"start\":23116},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":23138,\"start\":23134},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":23148,\"start\":23144},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":23161,\"start\":23157},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":23175,\"start\":23171},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":23193,\"start\":23189},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":23226,\"start\":23222},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":23436,\"start\":23432},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":23539,\"start\":23535},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":23642,\"start\":23638},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":23733,\"start\":23729},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":24093,\"start\":24089},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":24096,\"start\":24093},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24287,\"start\":24284},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":26147,\"start\":26143},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":28672,\"start\":28668},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":28675,\"start\":28672},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":29777,\"start\":29773},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":30544,\"start\":30540},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":30763,\"start\":30759},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":32970,\"start\":32966},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":33986,\"start\":33982},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":34000,\"start\":33996},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":34434,\"start\":34430},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":34680,\"start\":34676},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":34743,\"start\":34739},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":35297,\"start\":35293},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":36479,\"start\":36475},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":37050,\"start\":37046},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":37719,\"start\":37715},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":37748,\"start\":37744},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":38091,\"start\":38087},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":39017,\"start\":39013},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":39152,\"start\":39148},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":39155,\"start\":39152},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":39158,\"start\":39155},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":39231,\"start\":39227},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":39339,\"start\":39335},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":39356,\"start\":39353},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":39398,\"start\":39394},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":39495,\"start\":39491},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":39516,\"start\":39512},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":41210,\"start\":41206},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":41270,\"start\":41266},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":41559,\"start\":41555},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":48460,\"start\":48456},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":48478,\"start\":48474},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":48493,\"start\":48489},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":48496,\"start\":48493},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":51255,\"start\":51251},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":51903,\"start\":51899},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":51965,\"start\":51963}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":44340,\"start\":44242},{\"attributes\":{\"id\":\"fig_1\"},\"end\":45260,\"start\":44341},{\"attributes\":{\"id\":\"fig_3\"},\"end\":45377,\"start\":45261},{\"attributes\":{\"id\":\"fig_6\"},\"end\":45776,\"start\":45378},{\"attributes\":{\"id\":\"fig_7\"},\"end\":45929,\"start\":45777},{\"attributes\":{\"id\":\"fig_8\"},\"end\":46318,\"start\":45930},{\"attributes\":{\"id\":\"fig_9\"},\"end\":46530,\"start\":46319},{\"attributes\":{\"id\":\"fig_10\"},\"end\":47617,\"start\":46531},{\"attributes\":{\"id\":\"fig_11\"},\"end\":48414,\"start\":47618},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":49506,\"start\":48415},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":50237,\"start\":49507},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":50999,\"start\":50238},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":51839,\"start\":51000},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":52343,\"start\":51840},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":52578,\"start\":52344}]", "paragraph": "[{\"end\":3096,\"start\":1475},{\"end\":4487,\"start\":3098},{\"end\":5411,\"start\":4489},{\"end\":6337,\"start\":5413},{\"end\":6877,\"start\":6339},{\"end\":8914,\"start\":6894},{\"end\":9976,\"start\":8959},{\"end\":10348,\"start\":9997},{\"end\":11091,\"start\":10402},{\"end\":12231,\"start\":11125},{\"end\":12607,\"start\":12233},{\"end\":12993,\"start\":12609},{\"end\":14707,\"start\":12995},{\"end\":14751,\"start\":14748},{\"end\":15410,\"start\":14789},{\"end\":15813,\"start\":15412},{\"end\":16642,\"start\":15851},{\"end\":16858,\"start\":16710},{\"end\":17254,\"start\":16908},{\"end\":17435,\"start\":17320},{\"end\":17624,\"start\":17567},{\"end\":17901,\"start\":17626},{\"end\":18375,\"start\":17963},{\"end\":18906,\"start\":18377},{\"end\":18943,\"start\":18908},{\"end\":19617,\"start\":19003},{\"end\":19708,\"start\":19631},{\"end\":20004,\"start\":19756},{\"end\":20139,\"start\":20027},{\"end\":20391,\"start\":20237},{\"end\":23013,\"start\":20493},{\"end\":25509,\"start\":23040},{\"end\":25705,\"start\":25660},{\"end\":26354,\"start\":25912},{\"end\":27447,\"start\":26448},{\"end\":27562,\"start\":27449},{\"end\":28271,\"start\":27564},{\"end\":29611,\"start\":28273},{\"end\":30400,\"start\":29613},{\"end\":30708,\"start\":30421},{\"end\":31400,\"start\":30744},{\"end\":31883,\"start\":31402},{\"end\":32408,\"start\":31907},{\"end\":33111,\"start\":32410},{\"end\":33942,\"start\":33126},{\"end\":35043,\"start\":33955},{\"end\":35655,\"start\":35077},{\"end\":36767,\"start\":35657},{\"end\":37648,\"start\":36769},{\"end\":38862,\"start\":37678},{\"end\":39101,\"start\":38898},{\"end\":39644,\"start\":39136},{\"end\":39876,\"start\":39646},{\"end\":40214,\"start\":39912},{\"end\":40355,\"start\":40242},{\"end\":41018,\"start\":40393},{\"end\":41129,\"start\":41045},{\"end\":41466,\"start\":41149},{\"end\":42218,\"start\":41490},{\"end\":42864,\"start\":42220},{\"end\":43923,\"start\":42899},{\"end\":44115,\"start\":43946},{\"end\":44241,\"start\":44117}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10401,\"start\":10349},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14747,\"start\":14708},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14788,\"start\":14752},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15850,\"start\":15814},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16709,\"start\":16643},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16907,\"start\":16859},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17319,\"start\":17255},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17566,\"start\":17436},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17924,\"start\":17902},{\"attributes\":{\"id\":\"formula_9\"},\"end\":19002,\"start\":18944},{\"attributes\":{\"id\":\"formula_10\"},\"end\":19755,\"start\":19709},{\"attributes\":{\"id\":\"formula_11\"},\"end\":20026,\"start\":20005},{\"attributes\":{\"id\":\"formula_12\"},\"end\":20236,\"start\":20140},{\"attributes\":{\"id\":\"formula_13\"},\"end\":20456,\"start\":20392}]", "table_ref": "[{\"end\":10696,\"start\":10689},{\"end\":19401,\"start\":19394},{\"end\":22010,\"start\":22003},{\"end\":22140,\"start\":22133},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":24404,\"start\":24397},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":24847,\"start\":24840},{\"end\":25965,\"start\":25958},{\"end\":26602,\"start\":26595},{\"end\":27420,\"start\":27412},{\"end\":27661,\"start\":27654},{\"end\":29205,\"start\":29198},{\"end\":31377,\"start\":31370},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":31757,\"start\":31750},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":37561,\"start\":37554},{\"end\":39552,\"start\":39545},{\"end\":39643,\"start\":39636},{\"end\":40750,\"start\":40741},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":41017,\"start\":41010}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1473,\"start\":1461},{\"attributes\":{\"n\":\"2.\"},\"end\":6892,\"start\":6880},{\"attributes\":{\"n\":\"3.\"},\"end\":8957,\"start\":8917},{\"attributes\":{\"n\":\"3.1.\"},\"end\":9995,\"start\":9979},{\"attributes\":{\"n\":\"3.2.\"},\"end\":11123,\"start\":11094},{\"attributes\":{\"n\":\"3.3.\"},\"end\":17961,\"start\":17926},{\"end\":19629,\"start\":19620},{\"attributes\":{\"n\":\"4.\"},\"end\":20468,\"start\":20458},{\"attributes\":{\"n\":\"4.1.\"},\"end\":20491,\"start\":20471},{\"attributes\":{\"n\":\"4.2.\"},\"end\":23038,\"start\":23016},{\"end\":25515,\"start\":25512},{\"end\":25529,\"start\":25518},{\"end\":25578,\"start\":25532},{\"end\":25594,\"start\":25581},{\"end\":25637,\"start\":25597},{\"end\":25658,\"start\":25640},{\"end\":25721,\"start\":25708},{\"end\":25767,\"start\":25724},{\"end\":25784,\"start\":25770},{\"end\":25826,\"start\":25787},{\"end\":25849,\"start\":25829},{\"end\":25894,\"start\":25852},{\"end\":25910,\"start\":25897},{\"end\":26371,\"start\":26357},{\"end\":26422,\"start\":26374},{\"end\":26437,\"start\":26425},{\"end\":26446,\"start\":26440},{\"attributes\":{\"n\":\"4.3.\"},\"end\":30419,\"start\":30403},{\"end\":30742,\"start\":30711},{\"attributes\":{\"n\":\"4.4.\"},\"end\":31905,\"start\":31886},{\"attributes\":{\"n\":\"5.\"},\"end\":33124,\"start\":33114},{\"end\":33953,\"start\":33945},{\"end\":35075,\"start\":35046},{\"end\":37676,\"start\":37651},{\"end\":38896,\"start\":38865},{\"end\":39134,\"start\":39104},{\"end\":39910,\"start\":39879},{\"end\":40240,\"start\":40217},{\"end\":40391,\"start\":40358},{\"end\":41043,\"start\":41021},{\"end\":41147,\"start\":41132},{\"end\":41488,\"start\":41469},{\"end\":42897,\"start\":42867},{\"end\":43944,\"start\":43926},{\"end\":44253,\"start\":44243},{\"end\":44352,\"start\":44342},{\"end\":45272,\"start\":45262},{\"end\":45389,\"start\":45379},{\"end\":45788,\"start\":45778},{\"end\":46330,\"start\":46320},{\"end\":46542,\"start\":46532},{\"end\":47639,\"start\":47619},{\"end\":48425,\"start\":48416},{\"end\":49517,\"start\":49508},{\"end\":51010,\"start\":51001},{\"end\":51850,\"start\":51841},{\"end\":52354,\"start\":52345}]", "table": "[{\"end\":49506,\"start\":48729},{\"end\":50237,\"start\":49519},{\"end\":50999,\"start\":50287},{\"end\":51839,\"start\":51316},{\"end\":52343,\"start\":52169},{\"end\":52578,\"start\":52473}]", "figure_caption": "[{\"end\":44340,\"start\":44255},{\"end\":45260,\"start\":44354},{\"end\":45377,\"start\":45274},{\"end\":45776,\"start\":45391},{\"end\":45929,\"start\":45790},{\"end\":46318,\"start\":45932},{\"end\":46530,\"start\":46332},{\"end\":47617,\"start\":46544},{\"end\":48414,\"start\":47642},{\"end\":48729,\"start\":48427},{\"end\":50287,\"start\":50240},{\"end\":51316,\"start\":51012},{\"end\":52169,\"start\":51852},{\"end\":52473,\"start\":52356}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4786,\"start\":4778},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10718,\"start\":10710},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11246,\"start\":11238},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12930,\"start\":12921},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14102,\"start\":14093},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16526,\"start\":16518},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17623,\"start\":17615},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17984,\"start\":17975},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18827,\"start\":18818},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":28722,\"start\":28714},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":29057,\"start\":29048},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29354,\"start\":29344},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":32178,\"start\":32170},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":32421,\"start\":32413},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":40158,\"start\":40150},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":41221,\"start\":41211},{\"end\":41282,\"start\":41274},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":41501,\"start\":41493},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":42039,\"start\":42026},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":42307,\"start\":42298},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":42514,\"start\":42505}]", "bib_author_first_name": "[{\"end\":52832,\"start\":52827},{\"end\":52857,\"start\":52856},{\"end\":52866,\"start\":52865},{\"end\":53241,\"start\":53240},{\"end\":53251,\"start\":53250},{\"end\":53260,\"start\":53259},{\"end\":53272,\"start\":53269},{\"end\":53491,\"start\":53485},{\"end\":53504,\"start\":53503},{\"end\":53518,\"start\":53514},{\"end\":53744,\"start\":53740},{\"end\":53764,\"start\":53763},{\"end\":54016,\"start\":54011},{\"end\":54035,\"start\":54028},{\"end\":54247,\"start\":54241},{\"end\":54467,\"start\":54462},{\"end\":54487,\"start\":54480},{\"end\":54501,\"start\":54495},{\"end\":54515,\"start\":54510},{\"end\":54845,\"start\":54839},{\"end\":54858,\"start\":54852},{\"end\":54871,\"start\":54864},{\"end\":54882,\"start\":54877},{\"end\":54892,\"start\":54889},{\"end\":55209,\"start\":55202},{\"end\":55549,\"start\":55546},{\"end\":55564,\"start\":55556},{\"end\":55577,\"start\":55571},{\"end\":55588,\"start\":55583},{\"end\":55596,\"start\":55594},{\"end\":55609,\"start\":55602},{\"end\":56022,\"start\":56019},{\"end\":56037,\"start\":56029},{\"end\":56051,\"start\":56043},{\"end\":56066,\"start\":56059},{\"end\":56318,\"start\":56312},{\"end\":56332,\"start\":56325},{\"end\":56353,\"start\":56346},{\"end\":56653,\"start\":56647},{\"end\":56669,\"start\":56660},{\"end\":56680,\"start\":56674},{\"end\":56694,\"start\":56686},{\"end\":57044,\"start\":57041},{\"end\":57052,\"start\":57051},{\"end\":57060,\"start\":57059},{\"end\":57070,\"start\":57069},{\"end\":57076,\"start\":57075},{\"end\":57083,\"start\":57081},{\"end\":57305,\"start\":57302},{\"end\":57318,\"start\":57315},{\"end\":57332,\"start\":57327},{\"end\":57681,\"start\":57678},{\"end\":57694,\"start\":57691},{\"end\":57708,\"start\":57703},{\"end\":57721,\"start\":57716},{\"end\":57731,\"start\":57722},{\"end\":58054,\"start\":58047},{\"end\":58438,\"start\":58430},{\"end\":58454,\"start\":58448},{\"end\":58607,\"start\":58606},{\"end\":58615,\"start\":58614},{\"end\":58850,\"start\":58842},{\"end\":58862,\"start\":58855},{\"end\":58877,\"start\":58869},{\"end\":58888,\"start\":58884},{\"end\":58901,\"start\":58894},{\"end\":59070,\"start\":59065},{\"end\":59348,\"start\":59347},{\"end\":59369,\"start\":59361},{\"end\":59381,\"start\":59377},{\"end\":59683,\"start\":59682},{\"end\":59699,\"start\":59692},{\"end\":59889,\"start\":59885},{\"end\":59892,\"start\":59890},{\"end\":59903,\"start\":59900},{\"end\":59916,\"start\":59909},{\"end\":60131,\"start\":60124},{\"end\":60145,\"start\":60139},{\"end\":60147,\"start\":60146},{\"end\":60154,\"start\":60153},{\"end\":60539,\"start\":60533},{\"end\":60557,\"start\":60550},{\"end\":60574,\"start\":60567},{\"end\":60590,\"start\":60584},{\"end\":60603,\"start\":60597},{\"end\":60613,\"start\":60608},{\"end\":60629,\"start\":60620},{\"end\":60631,\"start\":60630},{\"end\":60644,\"start\":60638},{\"end\":60646,\"start\":60645},{\"end\":61040,\"start\":61031},{\"end\":61042,\"start\":61041},{\"end\":61058,\"start\":61052},{\"end\":61075,\"start\":61069},{\"end\":61446,\"start\":61439},{\"end\":61459,\"start\":61452},{\"end\":61480,\"start\":61465},{\"end\":61852,\"start\":61842},{\"end\":61860,\"start\":61858},{\"end\":61871,\"start\":61867},{\"end\":61884,\"start\":61877},{\"end\":61897,\"start\":61889},{\"end\":62128,\"start\":62121},{\"end\":62138,\"start\":62133},{\"end\":62147,\"start\":62146},{\"end\":62160,\"start\":62152},{\"end\":62168,\"start\":62165},{\"end\":62176,\"start\":62169},{\"end\":62511,\"start\":62504},{\"end\":62519,\"start\":62516},{\"end\":62531,\"start\":62524},{\"end\":62541,\"start\":62537},{\"end\":62892,\"start\":62888},{\"end\":62903,\"start\":62897},{\"end\":62920,\"start\":62913},{\"end\":62922,\"start\":62921},{\"end\":62936,\"start\":62934},{\"end\":63171,\"start\":63159},{\"end\":63198,\"start\":63193},{\"end\":63578,\"start\":63566},{\"end\":63596,\"start\":63588},{\"end\":63603,\"start\":63597},{\"end\":63618,\"start\":63611},{\"end\":63631,\"start\":63625},{\"end\":63913,\"start\":63912},{\"end\":63927,\"start\":63926},{\"end\":64140,\"start\":64135},{\"end\":64153,\"start\":64150},{\"end\":64167,\"start\":64160},{\"end\":64169,\"start\":64168},{\"end\":64186,\"start\":64179},{\"end\":64401,\"start\":64400},{\"end\":64410,\"start\":64409},{\"end\":64419,\"start\":64418},{\"end\":64740,\"start\":64735},{\"end\":64749,\"start\":64748},{\"end\":64759,\"start\":64752},{\"end\":64774,\"start\":64773},{\"end\":64784,\"start\":64777},{\"end\":64799,\"start\":64792},{\"end\":65157,\"start\":65150},{\"end\":65189,\"start\":65181},{\"end\":65202,\"start\":65196},{\"end\":65491,\"start\":65485},{\"end\":65504,\"start\":65503},{\"end\":65724,\"start\":65723},{\"end\":65731,\"start\":65730},{\"end\":65738,\"start\":65737},{\"end\":65747,\"start\":65746},{\"end\":65952,\"start\":65945},{\"end\":65968,\"start\":65962},{\"end\":65985,\"start\":65979},{\"end\":65987,\"start\":65986},{\"end\":66144,\"start\":66140},{\"end\":66160,\"start\":66153},{\"end\":66441,\"start\":66432},{\"end\":66458,\"start\":66453},{\"end\":66669,\"start\":66665},{\"end\":66907,\"start\":66900},{\"end\":66927,\"start\":66920},{\"end\":66947,\"start\":66936},{\"end\":66949,\"start\":66948},{\"end\":67166,\"start\":67162},{\"end\":67179,\"start\":67173},{\"end\":67190,\"start\":67187},{\"end\":67203,\"start\":67196},{\"end\":67215,\"start\":67210},{\"end\":67227,\"start\":67223},{\"end\":67241,\"start\":67234},{\"end\":67760,\"start\":67753},{\"end\":67785,\"start\":67775},{\"end\":67801,\"start\":67794},{\"end\":67826,\"start\":67809},{\"end\":68223,\"start\":68219},{\"end\":68237,\"start\":68233},{\"end\":68242,\"start\":68238},{\"end\":68253,\"start\":68248},{\"end\":68269,\"start\":68263},{\"end\":68285,\"start\":68278},{\"end\":68299,\"start\":68291},{\"end\":68315,\"start\":68309},{\"end\":68330,\"start\":68324},{\"end\":68345,\"start\":68339},{\"end\":68359,\"start\":68355},{\"end\":68375,\"start\":68367},{\"end\":68389,\"start\":68385},{\"end\":68751,\"start\":68745},{\"end\":68767,\"start\":68760},{\"end\":68783,\"start\":68776},{\"end\":68794,\"start\":68789},{\"end\":68808,\"start\":68801},{\"end\":68819,\"start\":68815},{\"end\":68833,\"start\":68829},{\"end\":68844,\"start\":68840},{\"end\":69134,\"start\":69129},{\"end\":69147,\"start\":69141},{\"end\":69159,\"start\":69155},{\"end\":69173,\"start\":69169},{\"end\":69182,\"start\":69180},{\"end\":69585,\"start\":69578},{\"end\":69606,\"start\":69599},{\"end\":69621,\"start\":69614},{\"end\":69937,\"start\":69930},{\"end\":69948,\"start\":69944},{\"end\":69961,\"start\":69955},{\"end\":69978,\"start\":69973},{\"end\":69994,\"start\":69993},{\"end\":70006,\"start\":70002},{\"end\":70439,\"start\":70431},{\"end\":70449,\"start\":70445},{\"end\":70457,\"start\":70456},{\"end\":70469,\"start\":70464},{\"end\":70485,\"start\":70476},{\"end\":70810,\"start\":70802},{\"end\":70821,\"start\":70815},{\"end\":70845,\"start\":70836},{\"end\":71143,\"start\":71138},{\"end\":71156,\"start\":71148},{\"end\":71171,\"start\":71163},{\"end\":71183,\"start\":71178},{\"end\":71185,\"start\":71184},{\"end\":71490,\"start\":71486},{\"end\":71502,\"start\":71497},{\"end\":71515,\"start\":71509},{\"end\":71530,\"start\":71521},{\"end\":71543,\"start\":71536},{\"end\":71925,\"start\":71924},{\"end\":71931,\"start\":71930}]", "bib_author_last_name": "[{\"end\":52854,\"start\":52833},{\"end\":52863,\"start\":52858},{\"end\":52873,\"start\":52867},{\"end\":52889,\"start\":52875},{\"end\":53248,\"start\":53242},{\"end\":53257,\"start\":53252},{\"end\":53267,\"start\":53261},{\"end\":53280,\"start\":53273},{\"end\":53501,\"start\":53492},{\"end\":53512,\"start\":53505},{\"end\":53526,\"start\":53519},{\"end\":53647,\"start\":53642},{\"end\":53761,\"start\":53745},{\"end\":53772,\"start\":53765},{\"end\":53781,\"start\":53774},{\"end\":54026,\"start\":54017},{\"end\":54045,\"start\":54036},{\"end\":54257,\"start\":54248},{\"end\":54478,\"start\":54468},{\"end\":54493,\"start\":54488},{\"end\":54508,\"start\":54502},{\"end\":54523,\"start\":54516},{\"end\":54850,\"start\":54846},{\"end\":54862,\"start\":54859},{\"end\":54875,\"start\":54872},{\"end\":54887,\"start\":54883},{\"end\":54897,\"start\":54893},{\"end\":55223,\"start\":55210},{\"end\":55232,\"start\":55225},{\"end\":55554,\"start\":55550},{\"end\":55569,\"start\":55565},{\"end\":55581,\"start\":55578},{\"end\":55592,\"start\":55589},{\"end\":55600,\"start\":55597},{\"end\":55613,\"start\":55610},{\"end\":56027,\"start\":56023},{\"end\":56041,\"start\":56038},{\"end\":56057,\"start\":56052},{\"end\":56072,\"start\":56067},{\"end\":56323,\"start\":56319},{\"end\":56344,\"start\":56333},{\"end\":56359,\"start\":56354},{\"end\":56658,\"start\":56654},{\"end\":56672,\"start\":56670},{\"end\":56684,\"start\":56681},{\"end\":56697,\"start\":56695},{\"end\":57049,\"start\":57045},{\"end\":57057,\"start\":57053},{\"end\":57067,\"start\":57061},{\"end\":57073,\"start\":57071},{\"end\":57079,\"start\":57077},{\"end\":57091,\"start\":57084},{\"end\":57313,\"start\":57306},{\"end\":57325,\"start\":57319},{\"end\":57338,\"start\":57333},{\"end\":57689,\"start\":57682},{\"end\":57701,\"start\":57695},{\"end\":57714,\"start\":57709},{\"end\":57739,\"start\":57732},{\"end\":58060,\"start\":58055},{\"end\":58074,\"start\":58062},{\"end\":58446,\"start\":58439},{\"end\":58464,\"start\":58455},{\"end\":58612,\"start\":58608},{\"end\":58619,\"start\":58616},{\"end\":58853,\"start\":58851},{\"end\":58867,\"start\":58863},{\"end\":58882,\"start\":58878},{\"end\":58892,\"start\":58889},{\"end\":58904,\"start\":58902},{\"end\":59077,\"start\":59071},{\"end\":59359,\"start\":59349},{\"end\":59375,\"start\":59370},{\"end\":59385,\"start\":59382},{\"end\":59390,\"start\":59387},{\"end\":59690,\"start\":59684},{\"end\":59707,\"start\":59700},{\"end\":59717,\"start\":59709},{\"end\":59898,\"start\":59893},{\"end\":59907,\"start\":59904},{\"end\":59924,\"start\":59917},{\"end\":60137,\"start\":60132},{\"end\":60151,\"start\":60148},{\"end\":60162,\"start\":60155},{\"end\":60548,\"start\":60540},{\"end\":60565,\"start\":60558},{\"end\":60582,\"start\":60575},{\"end\":60595,\"start\":60591},{\"end\":60606,\"start\":60604},{\"end\":60618,\"start\":60614},{\"end\":60636,\"start\":60632},{\"end\":60651,\"start\":60647},{\"end\":61050,\"start\":61043},{\"end\":61067,\"start\":61059},{\"end\":61085,\"start\":61076},{\"end\":61450,\"start\":61447},{\"end\":61463,\"start\":61460},{\"end\":61485,\"start\":61481},{\"end\":61856,\"start\":61853},{\"end\":61865,\"start\":61861},{\"end\":61875,\"start\":61872},{\"end\":61887,\"start\":61885},{\"end\":61900,\"start\":61898},{\"end\":62131,\"start\":62129},{\"end\":62144,\"start\":62139},{\"end\":62150,\"start\":62148},{\"end\":62163,\"start\":62161},{\"end\":62179,\"start\":62177},{\"end\":62514,\"start\":62512},{\"end\":62522,\"start\":62520},{\"end\":62535,\"start\":62532},{\"end\":62544,\"start\":62542},{\"end\":62895,\"start\":62893},{\"end\":62911,\"start\":62904},{\"end\":62932,\"start\":62923},{\"end\":62944,\"start\":62937},{\"end\":63191,\"start\":63172},{\"end\":63212,\"start\":63199},{\"end\":63218,\"start\":63214},{\"end\":63586,\"start\":63579},{\"end\":63609,\"start\":63604},{\"end\":63623,\"start\":63619},{\"end\":63637,\"start\":63632},{\"end\":63924,\"start\":63914},{\"end\":63935,\"start\":63928},{\"end\":64148,\"start\":64141},{\"end\":64158,\"start\":64154},{\"end\":64177,\"start\":64170},{\"end\":64191,\"start\":64187},{\"end\":64407,\"start\":64402},{\"end\":64416,\"start\":64411},{\"end\":64426,\"start\":64420},{\"end\":64746,\"start\":64741},{\"end\":64771,\"start\":64760},{\"end\":64790,\"start\":64785},{\"end\":64806,\"start\":64800},{\"end\":65179,\"start\":65158},{\"end\":65194,\"start\":65190},{\"end\":65210,\"start\":65203},{\"end\":65217,\"start\":65212},{\"end\":65501,\"start\":65492},{\"end\":65512,\"start\":65505},{\"end\":65728,\"start\":65725},{\"end\":65735,\"start\":65732},{\"end\":65744,\"start\":65739},{\"end\":65751,\"start\":65748},{\"end\":65960,\"start\":65953},{\"end\":65977,\"start\":65969},{\"end\":65992,\"start\":65988},{\"end\":66151,\"start\":66145},{\"end\":66168,\"start\":66161},{\"end\":66451,\"start\":66442},{\"end\":66463,\"start\":66459},{\"end\":66677,\"start\":66670},{\"end\":66918,\"start\":66908},{\"end\":66934,\"start\":66928},{\"end\":66957,\"start\":66950},{\"end\":67171,\"start\":67167},{\"end\":67185,\"start\":67180},{\"end\":67194,\"start\":67191},{\"end\":67208,\"start\":67204},{\"end\":67221,\"start\":67216},{\"end\":67232,\"start\":67228},{\"end\":67253,\"start\":67242},{\"end\":67773,\"start\":67761},{\"end\":67792,\"start\":67786},{\"end\":67807,\"start\":67802},{\"end\":67834,\"start\":67827},{\"end\":68231,\"start\":68224},{\"end\":68246,\"start\":68243},{\"end\":68261,\"start\":68254},{\"end\":68276,\"start\":68270},{\"end\":68289,\"start\":68286},{\"end\":68307,\"start\":68300},{\"end\":68322,\"start\":68316},{\"end\":68337,\"start\":68331},{\"end\":68353,\"start\":68346},{\"end\":68365,\"start\":68360},{\"end\":68383,\"start\":68376},{\"end\":68399,\"start\":68390},{\"end\":68758,\"start\":68752},{\"end\":68774,\"start\":68768},{\"end\":68787,\"start\":68784},{\"end\":68799,\"start\":68795},{\"end\":68813,\"start\":68809},{\"end\":68827,\"start\":68820},{\"end\":68838,\"start\":68834},{\"end\":68854,\"start\":68845},{\"end\":69139,\"start\":69135},{\"end\":69153,\"start\":69148},{\"end\":69167,\"start\":69160},{\"end\":69178,\"start\":69174},{\"end\":69190,\"start\":69183},{\"end\":69597,\"start\":69586},{\"end\":69612,\"start\":69607},{\"end\":69627,\"start\":69622},{\"end\":69942,\"start\":69938},{\"end\":69953,\"start\":69949},{\"end\":69971,\"start\":69962},{\"end\":69991,\"start\":69979},{\"end\":70000,\"start\":69995},{\"end\":70010,\"start\":70007},{\"end\":70443,\"start\":70440},{\"end\":70454,\"start\":70450},{\"end\":70462,\"start\":70458},{\"end\":70474,\"start\":70470},{\"end\":70489,\"start\":70486},{\"end\":70813,\"start\":70811},{\"end\":70834,\"start\":70822},{\"end\":70850,\"start\":70846},{\"end\":71146,\"start\":71144},{\"end\":71161,\"start\":71157},{\"end\":71176,\"start\":71172},{\"end\":71197,\"start\":71186},{\"end\":71495,\"start\":71491},{\"end\":71507,\"start\":71503},{\"end\":71519,\"start\":71516},{\"end\":71534,\"start\":71531},{\"end\":71547,\"start\":71544},{\"end\":71928,\"start\":71926},{\"end\":71939,\"start\":71932}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":2292730},\"end\":53172,\"start\":52768},{\"attributes\":{\"id\":\"b1\"},\"end\":53422,\"start\":53174},{\"attributes\":{\"id\":\"b2\"},\"end\":53638,\"start\":53424},{\"attributes\":{\"id\":\"b3\"},\"end\":53682,\"start\":53640},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":49868985},\"end\":53935,\"start\":53684},{\"attributes\":{\"id\":\"b5\"},\"end\":54173,\"start\":53937},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":8054340},\"end\":54411,\"start\":54175},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":207556454},\"end\":54791,\"start\":54413},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":3608725},\"end\":55168,\"start\":54793},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":11936001},\"end\":55448,\"start\":55170},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":212633580},\"end\":55961,\"start\":55450},{\"attributes\":{\"doi\":\"arXiv:2010.14343\",\"id\":\"b11\"},\"end\":56261,\"start\":55963},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":12350611},\"end\":56587,\"start\":56263},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":208617800},\"end\":56986,\"start\":56589},{\"attributes\":{\"id\":\"b14\"},\"end\":57235,\"start\":56988},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":14171860},\"end\":57636,\"start\":57237},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":14940757},\"end\":58005,\"start\":57638},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":17187414},\"end\":58400,\"start\":58007},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":10004927},\"end\":58562,\"start\":58402},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":320199},\"end\":58778,\"start\":58564},{\"attributes\":{\"id\":\"b20\"},\"end\":59043,\"start\":58780},{\"attributes\":{\"id\":\"b21\"},\"end\":59299,\"start\":59045},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":206594692},\"end\":59658,\"start\":59301},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":6397710},\"end\":59828,\"start\":59660},{\"attributes\":{\"id\":\"b24\"},\"end\":60061,\"start\":59830},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":15870772},\"end\":60465,\"start\":60063},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":53307035},\"end\":60952,\"start\":60467},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":10301835},\"end\":61380,\"start\":60954},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":26847954},\"end\":61791,\"start\":61382},{\"attributes\":{\"doi\":\"abs/1803.08024\",\"id\":\"b29\",\"matched_paper_id\":3994012},\"end\":62069,\"start\":61793},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":202234815},\"end\":62449,\"start\":62071},{\"attributes\":{\"id\":\"b31\"},\"end\":62651,\"start\":62451},{\"attributes\":{\"id\":\"b32\"},\"end\":62834,\"start\":62653},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":8701238},\"end\":63084,\"start\":62836},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":9484146},\"end\":63477,\"start\":63086},{\"attributes\":{\"id\":\"b35\"},\"end\":63834,\"start\":63479},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":10863482},\"end\":64071,\"start\":63836},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":5959482},\"end\":64343,\"start\":64073},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":19886856},\"end\":64686,\"start\":64345},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":1923223},\"end\":65072,\"start\":64688},{\"attributes\":{\"id\":\"b40\"},\"end\":65408,\"start\":65074},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":52000169},\"end\":65657,\"start\":65410},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":53973364},\"end\":65877,\"start\":65659},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":14579301},\"end\":66138,\"start\":65879},{\"attributes\":{\"id\":\"b44\"},\"end\":66368,\"start\":66140},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":14849501},\"end\":66587,\"start\":66370},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":199449027},\"end\":66851,\"start\":66589},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":1957433},\"end\":67109,\"start\":66853},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":235458006},\"end\":67684,\"start\":67111},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":155089387},\"end\":68146,\"start\":67686},{\"attributes\":{\"doi\":\"abs/2103.00020\",\"id\":\"b50\"},\"end\":68707,\"start\":68148},{\"attributes\":{\"doi\":\"abs/2102.12092\",\"id\":\"b51\"},\"end\":69054,\"start\":68709},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":26126851},\"end\":69494,\"start\":69056},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":2651840},\"end\":69858,\"start\":69496},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":214728232},\"end\":70342,\"start\":69860},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":204957123},\"end\":70798,\"start\":70344},{\"attributes\":{\"id\":\"b56\"},\"end\":71050,\"start\":70800},{\"attributes\":{\"doi\":\"abs/2108.04603\",\"id\":\"b57\",\"matched_paper_id\":236965864},\"end\":71411,\"start\":71052},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":219630137},\"end\":71869,\"start\":71413},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":7698906},\"end\":72184,\"start\":71871}]", "bib_title": "[{\"end\":52825,\"start\":52768},{\"end\":53738,\"start\":53684},{\"end\":54239,\"start\":54175},{\"end\":54460,\"start\":54413},{\"end\":54837,\"start\":54793},{\"end\":55200,\"start\":55170},{\"end\":55544,\"start\":55450},{\"end\":56310,\"start\":56263},{\"end\":56645,\"start\":56589},{\"end\":57300,\"start\":57237},{\"end\":57676,\"start\":57638},{\"end\":58045,\"start\":58007},{\"end\":58428,\"start\":58402},{\"end\":58604,\"start\":58564},{\"end\":59063,\"start\":59045},{\"end\":59345,\"start\":59301},{\"end\":59680,\"start\":59660},{\"end\":60122,\"start\":60063},{\"end\":60531,\"start\":60467},{\"end\":61029,\"start\":60954},{\"end\":61437,\"start\":61382},{\"end\":61840,\"start\":61793},{\"end\":62119,\"start\":62071},{\"end\":62886,\"start\":62836},{\"end\":63157,\"start\":63086},{\"end\":63910,\"start\":63836},{\"end\":64133,\"start\":64073},{\"end\":64398,\"start\":64345},{\"end\":64733,\"start\":64688},{\"end\":65483,\"start\":65410},{\"end\":65721,\"start\":65659},{\"end\":65943,\"start\":65879},{\"end\":66430,\"start\":66370},{\"end\":66663,\"start\":66589},{\"end\":66898,\"start\":66853},{\"end\":67160,\"start\":67111},{\"end\":67751,\"start\":67686},{\"end\":69127,\"start\":69056},{\"end\":69576,\"start\":69496},{\"end\":69928,\"start\":69860},{\"end\":70429,\"start\":70344},{\"end\":71136,\"start\":71052},{\"end\":71484,\"start\":71413},{\"end\":71922,\"start\":71871}]", "bib_author": "[{\"end\":52856,\"start\":52827},{\"end\":52865,\"start\":52856},{\"end\":52875,\"start\":52865},{\"end\":52891,\"start\":52875},{\"end\":53250,\"start\":53240},{\"end\":53259,\"start\":53250},{\"end\":53269,\"start\":53259},{\"end\":53282,\"start\":53269},{\"end\":53503,\"start\":53485},{\"end\":53514,\"start\":53503},{\"end\":53528,\"start\":53514},{\"end\":53649,\"start\":53642},{\"end\":53763,\"start\":53740},{\"end\":53774,\"start\":53763},{\"end\":53783,\"start\":53774},{\"end\":54028,\"start\":54011},{\"end\":54047,\"start\":54028},{\"end\":54259,\"start\":54241},{\"end\":54480,\"start\":54462},{\"end\":54495,\"start\":54480},{\"end\":54510,\"start\":54495},{\"end\":54525,\"start\":54510},{\"end\":54852,\"start\":54839},{\"end\":54864,\"start\":54852},{\"end\":54877,\"start\":54864},{\"end\":54889,\"start\":54877},{\"end\":54899,\"start\":54889},{\"end\":55225,\"start\":55202},{\"end\":55234,\"start\":55225},{\"end\":55556,\"start\":55546},{\"end\":55571,\"start\":55556},{\"end\":55583,\"start\":55571},{\"end\":55594,\"start\":55583},{\"end\":55602,\"start\":55594},{\"end\":55615,\"start\":55602},{\"end\":56029,\"start\":56019},{\"end\":56043,\"start\":56029},{\"end\":56059,\"start\":56043},{\"end\":56074,\"start\":56059},{\"end\":56325,\"start\":56312},{\"end\":56346,\"start\":56325},{\"end\":56361,\"start\":56346},{\"end\":56660,\"start\":56647},{\"end\":56674,\"start\":56660},{\"end\":56686,\"start\":56674},{\"end\":56699,\"start\":56686},{\"end\":57051,\"start\":57041},{\"end\":57059,\"start\":57051},{\"end\":57069,\"start\":57059},{\"end\":57075,\"start\":57069},{\"end\":57081,\"start\":57075},{\"end\":57093,\"start\":57081},{\"end\":57315,\"start\":57302},{\"end\":57327,\"start\":57315},{\"end\":57340,\"start\":57327},{\"end\":57691,\"start\":57678},{\"end\":57703,\"start\":57691},{\"end\":57716,\"start\":57703},{\"end\":57741,\"start\":57716},{\"end\":58062,\"start\":58047},{\"end\":58076,\"start\":58062},{\"end\":58448,\"start\":58430},{\"end\":58466,\"start\":58448},{\"end\":58614,\"start\":58606},{\"end\":58621,\"start\":58614},{\"end\":58855,\"start\":58842},{\"end\":58869,\"start\":58855},{\"end\":58884,\"start\":58869},{\"end\":58894,\"start\":58884},{\"end\":58906,\"start\":58894},{\"end\":59079,\"start\":59065},{\"end\":59361,\"start\":59347},{\"end\":59377,\"start\":59361},{\"end\":59387,\"start\":59377},{\"end\":59392,\"start\":59387},{\"end\":59692,\"start\":59682},{\"end\":59709,\"start\":59692},{\"end\":59719,\"start\":59709},{\"end\":59900,\"start\":59885},{\"end\":59909,\"start\":59900},{\"end\":59926,\"start\":59909},{\"end\":60139,\"start\":60124},{\"end\":60153,\"start\":60139},{\"end\":60164,\"start\":60153},{\"end\":60550,\"start\":60533},{\"end\":60567,\"start\":60550},{\"end\":60584,\"start\":60567},{\"end\":60597,\"start\":60584},{\"end\":60608,\"start\":60597},{\"end\":60620,\"start\":60608},{\"end\":60638,\"start\":60620},{\"end\":60653,\"start\":60638},{\"end\":61052,\"start\":61031},{\"end\":61069,\"start\":61052},{\"end\":61087,\"start\":61069},{\"end\":61452,\"start\":61439},{\"end\":61465,\"start\":61452},{\"end\":61487,\"start\":61465},{\"end\":61858,\"start\":61842},{\"end\":61867,\"start\":61858},{\"end\":61877,\"start\":61867},{\"end\":61889,\"start\":61877},{\"end\":61902,\"start\":61889},{\"end\":62133,\"start\":62121},{\"end\":62146,\"start\":62133},{\"end\":62152,\"start\":62146},{\"end\":62165,\"start\":62152},{\"end\":62181,\"start\":62165},{\"end\":62516,\"start\":62504},{\"end\":62524,\"start\":62516},{\"end\":62537,\"start\":62524},{\"end\":62546,\"start\":62537},{\"end\":62897,\"start\":62888},{\"end\":62913,\"start\":62897},{\"end\":62934,\"start\":62913},{\"end\":62946,\"start\":62934},{\"end\":63193,\"start\":63159},{\"end\":63214,\"start\":63193},{\"end\":63220,\"start\":63214},{\"end\":63588,\"start\":63566},{\"end\":63611,\"start\":63588},{\"end\":63625,\"start\":63611},{\"end\":63639,\"start\":63625},{\"end\":63926,\"start\":63912},{\"end\":63937,\"start\":63926},{\"end\":64150,\"start\":64135},{\"end\":64160,\"start\":64150},{\"end\":64179,\"start\":64160},{\"end\":64193,\"start\":64179},{\"end\":64409,\"start\":64400},{\"end\":64418,\"start\":64409},{\"end\":64428,\"start\":64418},{\"end\":64748,\"start\":64735},{\"end\":64752,\"start\":64748},{\"end\":64773,\"start\":64752},{\"end\":64777,\"start\":64773},{\"end\":64792,\"start\":64777},{\"end\":64808,\"start\":64792},{\"end\":65181,\"start\":65150},{\"end\":65196,\"start\":65181},{\"end\":65212,\"start\":65196},{\"end\":65219,\"start\":65212},{\"end\":65503,\"start\":65485},{\"end\":65514,\"start\":65503},{\"end\":65730,\"start\":65723},{\"end\":65737,\"start\":65730},{\"end\":65746,\"start\":65737},{\"end\":65753,\"start\":65746},{\"end\":65962,\"start\":65945},{\"end\":65979,\"start\":65962},{\"end\":65994,\"start\":65979},{\"end\":66153,\"start\":66140},{\"end\":66170,\"start\":66153},{\"end\":66453,\"start\":66432},{\"end\":66465,\"start\":66453},{\"end\":66679,\"start\":66665},{\"end\":66920,\"start\":66900},{\"end\":66936,\"start\":66920},{\"end\":66959,\"start\":66936},{\"end\":67173,\"start\":67162},{\"end\":67187,\"start\":67173},{\"end\":67196,\"start\":67187},{\"end\":67210,\"start\":67196},{\"end\":67223,\"start\":67210},{\"end\":67234,\"start\":67223},{\"end\":67255,\"start\":67234},{\"end\":67775,\"start\":67753},{\"end\":67794,\"start\":67775},{\"end\":67809,\"start\":67794},{\"end\":67836,\"start\":67809},{\"end\":68233,\"start\":68219},{\"end\":68248,\"start\":68233},{\"end\":68263,\"start\":68248},{\"end\":68278,\"start\":68263},{\"end\":68291,\"start\":68278},{\"end\":68309,\"start\":68291},{\"end\":68324,\"start\":68309},{\"end\":68339,\"start\":68324},{\"end\":68355,\"start\":68339},{\"end\":68367,\"start\":68355},{\"end\":68385,\"start\":68367},{\"end\":68401,\"start\":68385},{\"end\":68760,\"start\":68745},{\"end\":68776,\"start\":68760},{\"end\":68789,\"start\":68776},{\"end\":68801,\"start\":68789},{\"end\":68815,\"start\":68801},{\"end\":68829,\"start\":68815},{\"end\":68840,\"start\":68829},{\"end\":68856,\"start\":68840},{\"end\":69141,\"start\":69129},{\"end\":69155,\"start\":69141},{\"end\":69169,\"start\":69155},{\"end\":69180,\"start\":69169},{\"end\":69192,\"start\":69180},{\"end\":69599,\"start\":69578},{\"end\":69614,\"start\":69599},{\"end\":69629,\"start\":69614},{\"end\":69944,\"start\":69930},{\"end\":69955,\"start\":69944},{\"end\":69973,\"start\":69955},{\"end\":69993,\"start\":69973},{\"end\":70002,\"start\":69993},{\"end\":70012,\"start\":70002},{\"end\":70445,\"start\":70431},{\"end\":70456,\"start\":70445},{\"end\":70464,\"start\":70456},{\"end\":70476,\"start\":70464},{\"end\":70491,\"start\":70476},{\"end\":70815,\"start\":70802},{\"end\":70836,\"start\":70815},{\"end\":70852,\"start\":70836},{\"end\":71148,\"start\":71138},{\"end\":71163,\"start\":71148},{\"end\":71178,\"start\":71163},{\"end\":71199,\"start\":71178},{\"end\":71497,\"start\":71486},{\"end\":71509,\"start\":71497},{\"end\":71521,\"start\":71509},{\"end\":71536,\"start\":71521},{\"end\":71549,\"start\":71536},{\"end\":71930,\"start\":71924},{\"end\":71941,\"start\":71930}]", "bib_venue": "[{\"end\":58217,\"start\":58155},{\"end\":67418,\"start\":67345},{\"end\":52946,\"start\":52891},{\"end\":53238,\"start\":53174},{\"end\":53483,\"start\":53424},{\"end\":53797,\"start\":53783},{\"end\":54009,\"start\":53937},{\"end\":54279,\"start\":54259},{\"end\":54586,\"start\":54525},{\"end\":54963,\"start\":54899},{\"end\":55292,\"start\":55234},{\"end\":55684,\"start\":55615},{\"end\":56017,\"start\":55963},{\"end\":56416,\"start\":56361},{\"end\":56768,\"start\":56699},{\"end\":57039,\"start\":56988},{\"end\":57415,\"start\":57340},{\"end\":57799,\"start\":57741},{\"end\":58153,\"start\":58076},{\"end\":58470,\"start\":58466},{\"end\":58653,\"start\":58621},{\"end\":58840,\"start\":58780},{\"end\":59166,\"start\":59079},{\"end\":59457,\"start\":59392},{\"end\":59728,\"start\":59719},{\"end\":59883,\"start\":59830},{\"end\":60229,\"start\":60164},{\"end\":60691,\"start\":60653},{\"end\":61145,\"start\":61087},{\"end\":61568,\"start\":61487},{\"end\":61921,\"start\":61916},{\"end\":62240,\"start\":62181},{\"end\":62502,\"start\":62451},{\"end\":62722,\"start\":62653},{\"end\":62950,\"start\":62946},{\"end\":63263,\"start\":63220},{\"end\":63564,\"start\":63479},{\"end\":63941,\"start\":63937},{\"end\":64197,\"start\":64193},{\"end\":64493,\"start\":64428},{\"end\":64873,\"start\":64808},{\"end\":65148,\"start\":65074},{\"end\":65518,\"start\":65514},{\"end\":65757,\"start\":65753},{\"end\":65998,\"start\":65994},{\"end\":66239,\"start\":66170},{\"end\":66469,\"start\":66465},{\"end\":66706,\"start\":66679},{\"end\":66964,\"start\":66959},{\"end\":67343,\"start\":67255},{\"end\":67895,\"start\":67836},{\"end\":68217,\"start\":68148},{\"end\":68743,\"start\":68709},{\"end\":69256,\"start\":69192},{\"end\":69667,\"start\":69629},{\"end\":70081,\"start\":70012},{\"end\":70550,\"start\":70491},{\"end\":71218,\"start\":71213},{\"end\":71618,\"start\":71549},{\"end\":71999,\"start\":71941}]"}}}, "year": 2023, "month": 12, "day": 17}