{"id": 3654323, "updated": "2023-09-29 00:51:53.335", "metadata": {"title": "Deep Visual Domain Adaptation: A Survey", "authors": "[{\"first\":\"Mei\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Weihong\",\"last\":\"Deng\",\"middle\":[]}]", "venue": "Neurocomputing", "journal": "Neurocomputing", "publication_date": {"year": 2018, "month": 2, "day": 10}, "abstract": "Deep domain adaption has emerged as a new learning technique to address the lack of massive amounts of labeled data. Compared to conventional methods, which learn shared feature subspaces or reuse important source instances with shallow representations, deep domain adaption methods leverage deep networks to learn more transferable representations by embedding domain adaptation in the pipeline of deep learning. There have been comprehensive surveys for shallow domain adaption, but few timely reviews the emerging deep learning based methods. In this paper, we provide a comprehensive survey of deep domain adaptation methods for computer vision applications with four major contributions. First, we present a taxonomy of different deep domain adaption scenarios according to the properties of data that define how two domains are diverged. Second, we summarize deep domain adaption approaches into several categories based on training loss, and analyze and compare briefly the state-of-the-art methods under these categories. Third, we overview the computer vision applications that go beyond image classification, such as face recognition, semantic segmentation and object detection. Fourth, some potential deficiencies of current methods and several future directions are highlighted.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1802.03601", "mag": "2963082247", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/ijon/WangD18", "doi": "10.1016/j.neucom.2018.05.083"}}, "content": {"source": {"pdf_hash": "4b0cb6e979679f616d461252d9fc191e56968910", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1802.03601v4.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1802.03601", "status": "GREEN"}}, "grobid": {"id": "b4b1e4df54e33c381ea3a93c3b37a18737762dd7", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/4b0cb6e979679f616d461252d9fc191e56968910.txt", "contents": "\nDeep Visual Domain Adaptation: A Survey\n\n\nMei Wang \nSchool of Information and Communication Engineering\nBeijing University of Posts and Telecommunications\nBeijingChina\n\nWeihong Deng whdeng@bupt.edu.cn \nSchool of Information and Communication Engineering\nBeijing University of Posts and Telecommunications\nBeijingChina\n\nDeep Visual Domain Adaptation: A Survey\nManuscript accepted by Neurocomputing 2018 1\nDeep domain adaptation has emerged as a new learning technique to address the lack of massive amounts of labeled data. Compared to conventional methods, which learn shared feature subspaces or reuse important source instances with shallow representations, deep domain adaptation methods leverage deep networks to learn more transferable representations by embedding domain adaptation in the pipeline of deep learning. There have been comprehensive surveys for shallow domain adaptation, but few timely reviews the emerging deep learning based methods. In this paper, we provide a comprehensive survey of deep domain adaptation methods for computer vision applications with four major contributions. First, we present a taxonomy of different deep domain adaptation scenarios according to the properties of data that define how two domains are diverged. Second, we summarize deep domain adaptation approaches into several categories based on training loss, and analyze and compare briefly the state-of-the-art methods under these categories. Third, we overview the computer vision applications that go beyond image classification, such as face recognition, semantic segmentation and object detection. Fourth, some potential deficiencies of current methods and several future directions are highlighted.\n\nI. INTRODUCTION\n\nOver the past few years, machine learning has achieved great success and has benefited real-world applications. However, collecting and annotating datasets for every new task and domain are extremely expensive and time-consuming processes, sufficient training data may not always be available. Fortunately, the big data era makes a large amount of data available for other domains and tasks. For instance, although large-scale labeled video databases that are publicly available only contain a small number of samples, statistically, the YouTube face dataset (YTF) consists of 3.4K videos. The number of labeled still images is more than sufficient [107]. Hence, skillfully using the auxiliary data for the current task with scarce data will be helpful for real-world applications.\n\nHowever, due to many factors (e.g., illumination, pose, and image quality), there is always a distribution change or domain shift between two domains that can degrade the performance, as shown in Fig. 1. Mimicking the human vision system, domain adaptation (DA) is a particular case of transfer learning (TL) that utilizes labeled data in one or more relevant source domains to execute new tasks in a target domain. Over the past decades, various shallow DA methods have been proposed to solve a domain shift between the source and target domains. The common algorithms for shallow DA can mainly be categorized into two classes: instance-based DA [6], [18] and feature-based DA [37], [82], [30], [81]. The first class reduces the discrepancy by reweighting the source samples, and it trains on the weighted source samples. For the second class, a common shared space is generally learned in which the distributions of the two datasets are matched.\n\nRecently, neural-network-based deep learning approaches have achieved many inspiring results in visual categorization applications, such as image classification [62], face recognition [112], and object detection [35]. Simulating the perception of the human brain, deep networks can represent high-level abstractions by multiple layers of non-linear transformations. Existing deep network architectures [71] include convolutional neural networks (CNNs) [62], [106], [110], [44], deep belief networks (DBNs) [46], and stacked autoencoders (SAEs) [122], among others. Although some studies have shown that deep networks can learn more transferable representations that disentangle the exploratory factors of variations underlying the data samples and group features hierarchically in accordance with their relatedness to invariant factors, Donahue et al. [22] showed that a domain shift still affects their performance. The deep features would eventually transition from general to specific, and the transferability of the representation sharply decreases in higher layers. Therefore, recent work has addressed this problem by deep DA, which combines deep learning and DA.\n\nThere have been other surveys on TL and DA over the past few years [83], [101], [20], [84], [137], [19]. Pan et al. [83] categorized TL under three subsettings, including inductive TL, transductive TL, and unsupervised TL, but they only studied homogeneous feature spaces. Shao et al. [101] categorized TL techniques into feature-representation-level knowledge transfer and classifier-level knowledge transfer. The survey written by Patel [84] only focused on DA, a subtopic of TL. [20] discussed 38 methods for heterogeneous TL that operate under various settings, requirements, and domains. Zhang et al. [137] were the first to summarize several transferring criteria in detail from the concept level. These five surveys mentioned above only cover the methodologies on shallow TL or DA. The work presented by Csurka et al. [19] briefly analyzed the state-of-the-art shallow DA methods and categorized the deep DA methods into three subsettings based on training loss: classification loss, discrepancy loss and adversarial loss. However, Csurka's work mainly focused on shallow methods, and it only discussed deep DA in image classification applications.\n\nIn this paper, we focus on analyzing and discussing deep DA methods. Specifically, the key contributions of this survey are as follows: 1) we present a taxonomy of different deep DA scenarios according to the properties of data that define how two domains are diverged. 2) extending Csurka's work, we improve and detail the three subsettings ( classification loss, discrepancy loss and adversarial loss) and summarize different approaches used in different DA scenes.\n\n3) Considering the distance of the source and target domains, multi-step DA methods are studied and categorized into handcrafted, feature-based and representation-based mechanisms. 4) We provide a survey of many computer vision applications, such as image classification, face recognition, style translation, object detection, semantic segmentation and person reidentification.\n\nThe remainder of this survey is structured as follows. In Section II, we first define some notations, and then we categorize deep DA into different settings (given in Fig. 2). In the next three sections, different approaches are discussed for each setting, which are given in Table I and Table II in detail. Then, in Section VI, we introduce some successful computer vision applications of deep DA. Finally, the conclusion of this paper and discussion of future works are presented in Section VII.\n\n\nII. OVERVIEW\n\n\nA. Notations and Definitions\n\nIn this section, we introduce some notations and definitions that are used in this survey. The notations and definitions match those from the survey papers by [83], [19] to maintain consistency across surveys. A domain D consists of a feature space X and a marginal probability distribution P (X), where X = {x 1 , ..., x n } \u2208 X . Given a specific domain D = {X , P (X)}, a task T consists of a feature space Y and an objective predictive function f (\u00b7), which can also be viewed as a conditional probability distribution P (Y |X) from a probabilistic perspective. In general, we can learn P (Y |X) in a supervised manner from the labeled data {x i , y i }, where x i \u2208 X and y i \u2208 Y.\n\nAssume that we have two domains: the training dataset with sufficient labeled data is the source domain D s = {X s , P (X) s }, and the test dataset with a small amount of labeled data or no labeled data is the target domain D t = {X t , P (X) t }. We see that the partially labeled part, D tl , and the unlabeled parts, D tu , form the entire target domain, that is, D t = D tl \u222a D tu . Each domain is together with its task: the former is T s = {Y s , P (Y s |X s )}, and the latter is\nT t = {Y t , P (Y t |X t )}. Similarly, P (Y s |X s ) can be learned from the source labeled data {x s i , y s i }, while P (Y t |X t ) can be learned from labeled target data {x tl i , y tl i } and unlabeled data {x tu i }.\n\nB. Different Settings of Domain Adaptation\n\nThe case of traditional machine learning is D s = D t and T s = T t . For TL, Pan et al. [83] summarized that the differences between different datasets can be caused by domain divergence D s = D t (i.e., distribution shift or feature space difference) or task divergence T s = T t (i.e., conditional distribution shift or label space difference), or both. Based on this summary, Pan et al. categorized TL into three main groups: inductive, transductive and unsupervised TL.\n\nAccording to this classification, DA methods are transductive TL solutions with the assumption that the tasks are the same, i.e., T s = T t , and the differences are only caused by domain divergence, D s = D t . Therefore, DA can be split into two main categories based on different domain divergences (distribution shift or feature space difference): homogeneous and heterogeneous DA. Then, we can further categorize DA into supervised, semi-supervised and unsupervised DA in consideration of labeled data of the target domain. The classification is given in Fig. 2.\n\n\u2022 In the homogeneous DA setting, the feature spaces between the source and target domains are identical (X s = X t ) with the same dimension (d s = d t ). Hence, the source and target datasets are generally different in terms of data distributions (P (X) s = P (X) t ). In addition, we can further categorize the homogeneous DA setting into three cases: 1) In the supervised DA, a small amount of labeled target data, D tl , are present. However, the labeled data are commonly not sufficient for tasks. \u2022 In the heterogeneous DA setting, the feature spaces between the source and target domains are nonequivalent (X s = X t ), and the dimensions may also generally differ (d s = d t ). Similar to the homogeneous setting, the heterogeneous DA setting can also be divided into supervised, semi-supervised and unsupervised DA.\n\nAll of the above DA settings assumed that the source and target domains are directly related; thus, transferring knowledge can be accomplished in one step. We call them onestep DA. In reality, however, this assumption is occasionally unavailable. There is little overlap between the two domains, and performing one-step DA will not be effective. Fortunately, there are some intermediate domains that are able to draw the source and target domains closer than their original distance. Thus, we use a series of intermediate bridges to connect two seemingly unrelated domains and then perform onestep DA via this bridge, named multi-step (or transitive) DA [113], [114]. For example, face images and vehicle images are dissimilar between each other due to different shapes or other aspects, and thus, one-step DA would fail. However, some intermediate images, such as 'football helmet', can be introduced to be an intermediate domain and have a smooth knowledge transfer. Fig. 3 shows the differences between the learning processes of one-step and multi-step DA techniques.\n\n\nIII. APPROACHES OF DEEP DOMAIN ADAPTATION\n\nIn a broad sense, deep DA is a method that utilizes a deep network to enhance the performance of DA. Under this definition, shallow methods with deep features [22], [49], [88], [80], [138] can be considered as a deep DA approach. DA is adopted by shallow methods, whereas deep networks only extract vectorial features and are not helpful for transferring knowledge directly. For example, [76] extracted the convolutional activations from a CNN as the tensor representation, and then performed tensor-aligned invariant subspace learning to realize DA. This approach reliably outperforms current state-of-the-art approaches based on traditional hand-crafted features because sufficient representational and transferable features can be extracted through deep networks, which can work better on discrimination tasks [22].\n\nIn a narrow sense, deep DA is based on deep learning architectures designed for DA and can obtain a firsthand effect from deep networks via back-propagation. The intuitive idea is to embed DA into the process of learning representation and to learn a deep feature representation that is both semantically meaningful and domain invariant. With the \"good\" feature representations, the performance of the target task would improve significantly. In this paper, we focus on the narrow definition and discuss how to utilize deep networks to learn \"good\" feature representations with extra training criteria.\n\n\nA. Categorization of One-Step Domain Adaptation\n\nIn one-step DA, the deep approaches can be summarized into three cases, which refers to [19]. Table 1 shows these three cases and brief descriptions. The first case is the discrepancybased deep DA approach, which assumes that fine-tuning the deep network model with labeled or unlabeled target data can diminish the shift between the two domains. Class criterion, statistic criterion, architecture criterion and geometric criterion are four major techniques for performing fine-tuning.\n\n\u2022 Class Criterion: uses the class label information as a guide for transferring knowledge between different domains. When the labeled samples from the target domain are available in supervised DA, soft label and metric learning are always effective [118], [86], [53], [45], [79]. When such samples are unavailable, some other techniques can be adopted to substitute for class labeled data, such as pseudo labels [75], [139], [130], [98] and attribute representation [29], [118].\n\n\u2022 Statistic Criterion: aligns the statistical distribution shift between the source and target domains using some mechanisms. The most commonly used methods for comparing and reducing distribution shift are maximum mean discrepancy (MMD) [74], [130], [73], [75], [120], [32], correlation alignment (CORAL) [109], [87], Kullback-Leibler (KL) divergence [144] and H divergence, among others.\n\n\u2022   Discrepancy-based fine-tuning the deep network with labeled or unlabeled target data to diminish the domain shift class criterion [118], [86], [79], [98] [53], [45], [75], [139], [130], [29], [118], [28] statistic criterion [74], [130], [73] [75], [120], [32], [109], [87], [144] architecture criterion [69], [54], [68], [95], [128], [89] geometric criterion [16] Adversarial-based using domain discriminators to encourage domain confusion through an adversarial objective generative models [70], [4], [57] non-generative models [119], [118], [26], [25], [117] [85] Reconstructionbased using the data reconstruction as an auxiliary task to ensure feature invariance encoder-decoder reconstruction [5], [33], [31], [144] adversarial reconstruction [131], [143], [59]  Instance-based selecting certain parts of data from the auxiliary datasets to compose the intermediate domains [114], [16] Representation-based freeze weights of one network and use their intermediate representations as input to the new network [96] proven to be cost effective include adaptive batch normalization (BN) [69], [54], [68], weak-related weight [95], domain-guided dropout [128], and so forth.\n\n\u2022 Geometric Criterion: bridges the source and target domains according to their geometrical properties. This criterion assumes that the relationship of geometric structures can reduce the domain shift [16].\n\nThe second case can be referred to as an adversarial-based deep DA approach [26]. In this case, a domain discriminator that classifies whether a data point is drawn from the source or target domain is used to encourage domain confusion through an adversarial objective to minimize the distance between the empirical source and target mapping distributions. Furthermore, the adversarial-based deep DA approach can be categorized into two cases based on whether there are generative models.\n\n\u2022 Generative Models: combine the discriminative model with a generative component in general based on generative adversarial networks (GANs). One of the typical cases is to use source images, noise vectors or both to generate simulated samples that are similar to the target samples and preserve the annotation information of the source domain [70], [4], [57].\n\n\u2022 Non-Generative Models: rather than generating models with input image distributions, the feature extractor learns a discriminative representation using the labels in the source domain and maps the target data to the same space through a domain-confusion loss, thus resulting in the domain-invariant representations [119], [118], [26], [25], [117].\n\nThe third case can be referred to as a reconstruction-based DA approach, which assumes that the data reconstruction of the source or target samples can be helpful for improving the performance of DA. The reconstructor can ensure both specificity of intra-domain representations and indistinguishability of inter-domain representations.\n\n\u2022 Encoder-Decoder Reconstruction: by using stacked autoencoders (SAEs), encoder-decoder reconstruction methods combine the encoder network for representation learning with a decoder network for data reconstruction [5], [33], [31], [144].\n\n\u2022 Adversarial Reconstruction: the reconstruction error is measured as the difference between the reconstructed and original images within each image domain by a cyclic mapping obtained via a GAN discriminator, such as dual GAN [131], cycle GAN [143] and disco GAN [59]. \n\n\nB. Categorization of Multi-Step Domain Adaptation\n\nIn multi-step DA, we first determine the intermediate domains that are more related with the source and target domains than their direct connection. Second, the knowledge transfer process will be performed between the source, intermediate and target domains by one-step DA with less information loss. Thus, the key of multi-step DA is how to select and utilize intermediate domains; additionally, it can fall into three categories referring to [83]: hand-crafted, feature-based and representation-based selection mechanisms.\n\n\u2022 Hand-Crafted: users determine the intermediate domains based on experience [129].\n\n\u2022 Instance-Based: selecting certain parts of data from the auxiliary datasets to compose the intermediate domains to train the deep network [114], [16].\n\n\u2022 Representation-Based: transfer is enabled via freezing the previously trained network and using their intermediate representations as input to the new one [96].\n\n\nIV. ONE-STEP DOMAIN ADAPTATION\n\nAs mentioned in Section II-A, the data in the target domain have three types regardless of homogeneous or heterogeneous DA: 1) supervised DA with labeled data, 2) semi-supervised DA with labeled and unlabeled data and 3) non-supervised DA with unlabeled data. The second setting is able to be accomplished by combining the methods of setting 1 and setting 3; thus, we only focus on the first and third settings in this paper. The cases where the different approaches are mainly used for each DA setting are shown in Table III. As shown, more work is focused on unsupervised scenes because supervised DA has its limitations. When only few labeled data in the target domain are available, using the source and target labeled data to train parameters of models typically results in overfitting to the source distribution. In addition, the discrepancy-based approaches have been studied for years and produced more methods in many research works, whereas the adversarial-based and reconstruction-based approaches are a relatively new research topic but have recently been attracting more attention.\n\nA. Homogeneous Domain Adaptation 1) Discrepancy-Based Approaches: Yosinski et al. [133] proved that transferable features learned by deep networks have limitations due to fragile co-adaptation and representation specificity and that fine-tuning can enhance generalization performance. Fine-tuning (can also be viewed as a discrepancybased deep DA approach) is to train a base network with source data and then directly reuse the first n layers to conduct a target network. The remaining layers of the target network are randomly initialized and trained with loss based on discrepancy. During training, the first n layers of the target network can be fine-tuned or frozen depending on the size of the target dataset and its similarity to the source dataset [17]. Some common rules of thumb for navigating the 4 major scenarios are given in Table IV. \n\n\n\u2022 Class Criterion\n\nThe class criterion is the most basic training loss in deep DA. After pre-training the network with source data, the remaining layers of the target model use the class label information as a guide to train the network. Hence, a small number of labeled samples from the target dataset is assumed to be available.\n\nIdeally, the class label information is given directly in supervised DA. Most work commonly uses the negative loglikelihood of the ground truth class with softmax as their training loss, L = \u2212 N i=0 y i log\u0177 i (\u0177 i are the softmax predictions of the model, which represent class probabilities) [118], [86], [53], [126]. To extend this, Hinton et al. [45] modified the softmax function to soft label loss:  Try Freeze or Tune Tune  between  Medium Try Freeze or Tune  Tune  Tune  Source and Target  High  Try Freeze or Tune  Tune  Tune where z i is the logit output computed for each class. T is a temperature that is normally set to 1 in standard softmax, but it takes a higher value to produce a softer probability distribution over classes. By using it, much of the information about the learned function that resides in the ratios of very small probabilities can be obtained. For example, when recognizing digits, one version of 2 may obtain a probability of 10 6 of being a 3 and 10 9 of being a 7; in other words, this version of 2 looks more similar to 3 than 7. Inspired by Hinton, [118] fine-tuned the network by simultaneously minimizing the domain confusion loss (belonging to adversarial-based approaches, which will be presented in Section IV-A2) and soft label loss. Using soft labels rather than hard labels can preserve the relationships between classes across domains. Gebru et al. [29] modified existing adaptation algorithms based on [118] and utilized soft label loss at the fine-grained class level L csof t and attribute level L asof t . In addition to softmax loss, there are other methods that can be used as training loss to fine-tune the target model in supervised DA. Embedding metric learning in deep networks is another method that can make the distance of samples from different domains with the same labels be closer while those with different labels are far away. Based on this idea, [79] constructed the semantic alignment loss and the separation loss accordingly. Deep transfer metric learning is proposed by [53], which applies the marginal Fisher analysis criterion and MMD criterion (described in Statistic Criterion) to minimize their distribution difference:\nq i = exp(z i /T ) j (exp(z j /T ))(1)min J = S (M ) c \u2212 \u03b1S (M ) b + \u03b2D (M ) ts X s , X t +\u03b3 M m=1 ( W (m) 2 F + b (m) 2 2 )(2)\nwhere \u03b1, \u03b2 and \u03b3 are regularization parameters and W (m) and b (m) are the weights and biases of the m th layer of the network. D However, what can we do if there is no class label information in the target domain directly? As we all know, humans can identify unseen classes given only a high-level description. For instance, when provided the description \"tall brown animals with long necks\", we are able to recognize giraffes. To imitate the ability of humans, [64] introduced high-level semantic attributes per class. Assume that a c = (a c 1 , ..., a c m ) is the attribute representation for class c, which has fixedlength binary values with m attributes in all the classes. The classifiers provide estimates of p(a m |x) for each attribute a m . In the test stage, each target class y obtains its attribute vector a y in a deterministic way, i.e., p(a|y) = [[a = a y ]]. By applying Bayes rule, p(y|a) = p(y) p(a y ) [[a = a y ]], the posterior of a test class can be calculated as follows:\np(y|x) = a\u2208{0,1} M p(y|a)p(a|x) = p(y) p(a y ) M m=1 p(a y m |x) (3)\nGebru et al. [29] drew inspiration from these works and leveraged attributes to improve performance in the DA of finegrained recognition. There are multiple independent softmax losses that simultaneously perform attribute and class level to fine-tune the target model. To prevent the independent classifiers from obtaining conflicting labels with attribute and class level, an attribute consistency loss is also implemented.\n\nOccasionally, when fine-tuning the network in unsupervised DA, a label of target data, which is called a pseudo label, can preliminarily be obtained based on the maximum posterior probability. Yan et al. [130] initialized the target model using the source data and then defined the class posterior probability p(y t j = c|x t j ) by the output of the target model. With p(y t j = c|x t j ), they assigned pseudo-label y t j to x t j by y t j = arg max c p(y t j = c|x t j ). In [98], two different networks assign pseudo-labels to unlabeled samples, another network is trained by the samples to obtain target discriminative representations. The deep transfer network (DTN) [139] used some base classifiers, e.g., SVMs and MLPs, to obtain the pseudo labels for the target samples to estimate the conditional distribution of the target samples and match both the marginal and the conditional distributions with the MMD criterion. When casting the classifier adaptation into the residual learning framework, [75] used the pseudo label to build the conditional entropy E(D t , f t ), which ensures that the target classifier f t fits the target-specific structures well.\n\n\u2022 Statistic Criterion Although some discrepancy-based approaches search for pseudo labels, attribute labels or other substitutes to labeled target data, more work focuses on learning domain-invariant representations via minimizing the domain distribution discrepancy in unsupervised DA.\n\nMMD is an effective metric for comparing the distributions between two datasets by a kernel two-sample test [3]. Given two distributions s and t, the MMD is defined as follows:\nM M D 2 (s, t) = sup \u03c6 H \u22641 E x s \u223cs [\u03c6(x s )] \u2212 E x t \u223cs [\u03c6(x t )] 2 H (4)\nwhere \u03c6 represents the kernel function that maps the original data to a reproducing kernel Hilbert space (RKHS) and \u03c6 H \u2264 1 defines a set of functions in the unit ball of RKHS H.\n\nBased on the above, Ghifary et al. [32] proposed a model that introduced the MMD metric in feedforward neural networks with a single hidden layer. The MMD metric is computed between representations of each domain to reduce the distribution mismatch in the latent space. The empirical estimate of MMD is as follows:\nM M D 2 (D s , D t ) = 1 M M i=1 \u03c6(x s i )\u2212 1 N N j=1 \u03c6(x t j ) 2 H(5)\nSubsequently, Tzeng et al. [120] and Long et al. [73] extended MMD to a deep CNN model and achieved great success. The deep domain confusion network (DDC) by Tzeng et al. [120] used two CNNs for the source and target domains with shared weights. The network is optimized for classification loss in the source domain, while domain difference is measured by an adaptation layer with the MMD metric.\nL=L C (X L , y) + \u03bbM M D 2 (X s X t )(6)\nwhere the hyperparameter \u03bb is a penalty parameter. L C (X L , y) denotes classification loss on the available labeled data, X L , and the ground-truth labels, y. M M D 2 (X s X t ) denotes the distance between the source and target data. DDC only adapts one layer of the network, resulting in a reduction in the transferability of multiple layers. Rather than using a single layer and linear MMD, Long et al. [73] proposed the deep adaptation network (DAN) that matches the shift in marginal distributions across domains by adding multiple adaptation layers and exploring multiple kernels, assuming that the conditional distributions remain unchanged. However, this assumption is rather strong in practical applications; in other words, the source classifier cannot be directly used in the target domain. To make it more generalized, a joint adaptation network (JAN) [74] aligns the shift in the joint distributions of input features and output labels in multiple domain-specific layers based on a joint maximum mean discrepancy (JMMD) criterion. [139] proposed DTN, where both the marginal and the conditional distributions are matched based on MMD. The shared feature extraction layer learns a subspace to match the marginal distributions of the source and the target samples, and the discrimination layer matches the conditional distributions by classifier transduction. In addition to adapting features using MMD, residual transfer networks (RTNs) [75] added a gated residual layer for classifier adaptation. More recently, [130] proposed a weighted MMD model that introduces an auxiliary weight for each class in the source domain when the class weights in the target domain are not the same as those in the source domain.\n\nIf \u03c6 is a characteristic kernel (i.e., Gaussian kernel or Laplace kernel), MMD will compare all the orders of statistic moments. In contrast to MMD, CORAL [108] learned a linear transformation that aligns the second-order statistics between domains. Sun et al. [109] extended CORAL to deep neural networks (deep CORAL) with a nonlinear transformation.\nL CORAL = 1 4d 2 C S \u2212 C T 2 F(7)\nwhere \u00b7 2 F denotes the squared matrix Frobenius norm. C S and C T denote the covariance matrices of the source and target data, respectively.\n\nBy the Taylor expansion of the Gaussian kernel, MMD can be viewed as minimizing the distance between the weighted sums of all raw moments [67]. The interpretation of MMD as moment matching procedures motivated Zellinger et al. [134] to match the higher-order moments of the domain distributions, which we call central moment discrepancy (CMD). An empirical estimate of the CMD metric for the domain discrepancy in the activation space [a, b] N is given by\nCM D K (X s , X t ) = 1 (b \u2212 a) E(X s ) \u2212 E(X t ) 2 + K k=2 1 |b \u2212 a| k C k (X s ) \u2212 C k (X t ) 2 (8) where C k (X) = E((x \u2212 E(X)) k is the vector of all k th - order sample central moments and E(X) = 1 |X| x\u2208X x is the empirical expectation.\nThe association loss L assoc proposed by [42] is an alternative discrepancy measure, it enforces statistical associations between source and target data by making the two-step roundtrip probabilities P aba ij be similar to the uniform distribution over the class labels.\n\n\u2022 Architecture Criterion Some other methods optimize the architecture of the network to minimize the distribution discrepancy. This adaptation behavior can be achieved in most deep DA models, such as supervised and unsupervised settings.\n\nRozantsev et al. [95] considered that the weights in corresponding layers are not shared but related by a weight regularizer r w (\u00b7) to account for the differences between the two domains. The weight regularizer r w (\u00b7) can be expressed as the exponential loss function:\nr w (\u03b8 s j , \u03b8 t j ) = exp \u03b8 s j \u2212 \u03b8 t j 2 \u2212 1(9)\nwhere \u03b8 s j and \u03b8 t j denote the parameters of the j th layer of the source and target models, respectively. To further relax this restriction, they allow the weights in one stream to undergo a linear transformation:\nr w (\u03b8 s j , \u03b8 t j ) = exp( a j \u03b8 s j + b j \u2212 \u03b8 t j 2 ) \u2212 1(10)\nwhere a j and b j are scalar parameters that encode the linear transformation. The work of Shu et al. [105] is similar to [95] using weakly parameter-shared layers. The penalty term \u2126 controls the relatedness of parameters.\n\u2126= L i=1 ( W (l) S \u2212 W (l) T 2 F + b (l) S \u2212 b (l) T 2 F )(11)\nwhere {W  Li et al. [69] hypothesized that the class-related knowledge is stored in the weight matrix, whereas domain-related knowledge is represented by the statistics of the batch normalization (BN) layer [56]. BN normalizes the mean and standard deviation for each individual feature channel such that each layer receives data from a similar distribution, irrespective of whether it comes from the source or the target domain. Therefore, Li et al. used BN to align the distribution for recomputing the mean and standard deviation in the target domain.\nBN (X t ) = \u03bb x \u2212 \u00b5(X t ) \u03c3(X t ) + \u03b2(12)\nwhere \u03bb and \u03b2 are parameters learned from the target data and \u00b5(x) and \u03c3(x) are the mean and standard deviation computed independently for each feature channel. Based on [69], [9] endowed BN layers with a set of alignment parameters which can be learned automatically and can decide the degree of feature alignment required at different levels of the deep network. Furthermore, Ulyanov et al. [121] found that when replacing BN layers with instance normalization (IN) layers, where \u00b5(x) and \u03c3(x) are computed independently for each channel and each sample, the performance of DA can be further improved.\n\nOccasionally, neurons are not effective for all domains because of the presence of domain biases. For example, when recognizing people, the target domain typically contains one person centered with minimal background clutter, whereas the source dataset contains many people with more clutter. Thus, the neurons that capture the features of other people and clutter are useless. Domain-guided dropout was proposed by [128] to solve the problem of multi-DA, and it mutes non-related neurons for each domain. Rather than assigning dropout with a specific dropout rate, it depends on the gain of the loss function of each neuron on the domain sample when the neuron is removed.\ns i = L(g(x) \\i ) \u2212 L(g(x))(13)\nwhere L is the softmax loss function and g(x) \\i is the feature vector after setting the response of the i th neuron to zero. In [66], each source domain is assigned with different parameters, 2) Adversarial-Based Approaches: Recently, great success has been achieved by the GAN method [39], which estimates generative models via an adversarial process. GAN consists of two models: a generative model G that extracts the data distribution and a discriminative model D that distinguishes whether a sample is from G or training datasets by predicting a binary label. The networks are trained on the label prediction loss in a mini-max fashion: simultaneously optimizing G to minimize the loss while also training D to maximize the probability of assigning the correct label:\n\u0398 (i) = \u0398 (0) + \u2206 (i) , where \u0398 (0) ismin G max D V (D, G) = E x\u223cp data (x) [log D(x)] +E z\u223cpz(z) [log(1 \u2212 D(G(z)))](14)\nIn DA, this principle has been employed to ensure that the network cannot distinguish between the source and target domains. [119] proposed a unified framework for adversarialbased approaches and summarized the existing approaches according to whether to use a generator, which loss function to employ, or whether to share weights across domains. In this paper, we only categorize the adversarial-based approaches into two subsettings: generative models and non-generative models. \u2022 Generative Models Synthetic target data with ground-truth annotations are an appealing alternative to address the problem of a lack of training data. First, with the help of source data, generators render unlimited quantities of synthetic target data, which are paired with synthetic source data to share labels or appear as if they were sampled from the target domain while maintaining labels, or something else. Then, synthetic data with labels are used to train the target model as if no DA were required. Adversarial-based approaches with generative models are able to learn such a transformation in an unsupervised manner based on GAN.\n\nThe core idea of CoGAN [70] is to generate synthetic target data that are paired with synthetic source ones. It consists of a pair of GANs: GAN 1 for generating source data and GAN 2 for generating target data. The weights of the first few layers in the generative models and the last few layers in the discriminative models are tied. This weight-sharing constraint allows CoGAN to achieve a domain-invariant feature space without correspondence supervision. A trained CoGAN can adapt the input noise vector to paired images that are from the two distributions and share the labels. Therefore, the shared labels of synthetic target samples can be used to train the target model. Fig. 9. The CoGAN architecture. [70] More work focuses on generating synthetic data that are similar to the target data while maintaining annotations. Yoo et al. [132] transferred knowledge from the source domain to pixel-level target images with GANs. A domain discriminator ensures the invariance of content to the source domain, and a real/fake discriminator supervises the generator to produce similar images to the target domain. Shrivastava et al. [104] developed a method for simulated+unsupervised (S+U) learning that uses a combined objective of minimizing an adversarial loss and a self-regularization loss, where the goal is to improve the realism of synthetic images using unlabeled real data. In contrast to other works in which the generator is conditioned only on a noise vector or source images, Bousmalis et al. [4] proposed a model that exploits GANs conditioned on both. The classifier T is trained to predict class labels of both source and synthetic images, while the discriminator is trained to predict the domain labels of target and synthetic images. In addition, to expect synthetic images with similar foregrounds and different backgrounds from the same source images, a content similarity is used that penalizes large differences between source and synthetic images for foreground pixels only by a masked pairwise mean squared error [24]. The goal of the network is to learn G, D and T by solving the optimization problem:\nmin G,T max D V (D, G) = \u03b1L d (D, G) +\u03b2L t (T, G) + \u03b3L c (G)(15)\nwhere \u03b1, \u03b2, and \u03b3 are parameters that control the trade-off between the losses. L d , L t and L c are the adversarial loss, softmax loss and content-similarity loss, respectively.\n\n\u2022 Non-Generative Models The key of deep DA is learning domain-invariant representations from source and target samples. With these representations, the distribution of both domains can be similar enough such that the classifier is fooled and can be directly used in the target domain even if it is trained on source samples. Therefore, whether the representations are domain-confused or not is crucial to transferring knowledge. Inspired by GAN, domain confusion loss, which is produced by the discriminator, is introduced to improve the performance of deep DA without generators. Fig. 11. The domain-adversarial neural network (DANN) architecture. [25] The domain-adversarial neural network (DANN) [25] integrates a gradient reversal layer (GRL) into the standard architecture to ensure that the feature distributions over the two domains are made similar. The network consists of shared feature extraction layers and two classifiers. DANN minimizes the domain confusion loss (for all samples) and label prediction loss (for source samples) while maximizing domain confusion loss via the use of the GRL. In contrast to the above methods, the adversarial discriminative domain adaptation (ADDA) [119] considers independent source and target mappings by untying the weights, and the parameters of the target model are initialized by the pre-trained source one. This is more flexible because of allowing more domainspecific feature extractions to be learned. ADDA minimizes the source and target representation distances through iteratively minimizing these following functions, which is most similar to the original GAN:\nmin M s ,C L cls (X s , Y s ) = \u2212 E (x s ,y s )\u223c(X s ,Y s ) K k=1 1 [k=y s ] log C(M s (x s )) min D L advD (X s ,X t , M s , M t ) = \u2212 E (x s )\u223c(X s ) [log D(M s (x s ))] \u2212 E (x t )\u223c(X t ) [log(1 \u2212 D(M t (x t )))] min M s ,M t L advM (M s , M t ) = \u2212 E (x t )\u223c(X t ) [log D(M t (x t ))](16)\nwhere the mappings M s and M t are learned from the source and target data, X s and X t . C represents a classifier working on the source domain. The first classification loss function L cls is optimized by training the source model using the labeled source data. The second function L advD is minimized to train the discriminator, while the third function L advM is learning a representation that is domain invariant. Tzeng et al. [118] proposed adding an additional domain classification layer that performs binary domain classification and designed a domain confusion loss to encourage its prediction to be as close as possible to a uniform distribution over binary labels. Unlike previous methods that match the entire source and target domains, Cao et al. introduced a selective adversarial network (SAN) [8] to address partial transfer learning from large domains to small domains, which assumes that the target label space is a subspace of the source label space. It simultaneously avoids negative transfer by filtering out outlier source classes, and it promotes positive transfer by matching the data distributions in the shared label space via splitting the domain discriminator into many class-wise domain discriminators. [78] encoded domain labels and class labels to produce four groups of pairs, and replaced the typical binary adversarial discriminator by a four-class discriminator. Volpi et al. [123] trained a feature generator (S) to perform data augmentation in the source feature space and obtained a domain invariant feature through playing a minimax game against features from S.\n\nRather than using discriminator to classify domain label, some papers make some other explorations. Inspired by Wasserstein GAN [1], Shen et al. [102] utilized discriminator to estimate empirical Wasserstein distance between the source and target samples and optimized the feature extractor network to minimize the distance in an adversarial manner. In [99], two classifiers are treated as discriminators and are trained to maximize the discrepancy to detect target samples outside the support of the source, while a feature extractor is trained to minimize the discrepancy by generating target features near the support.\n\n3) Reconstruction-Based Approaches: In DA, the data reconstruction of source or target samples is an auxiliary task that simultaneously focuses on creating a shared representation between the two domains and keeping the individual characteristics of each domain.\n\n\n\u2022 Encoder-Decoder Reconstruction\n\nThe basic autoencoder framework [2] is a feedforward neural network that includes the encoding and decoding processes. The autoencoder first encodes an input to some hidden representation, and then it decodes this hidden representation back to a reconstructed version. The DA approaches based on encoder-decoder reconstruction typically learn the domaininvariant representation by a shared encoder and maintain the domain-special representation by a reconstruction loss in the source and target domains.\n\nXavier and Bengio [36] proposed extracting a high-level representation based on stacked denoising autoencoders (SDA) [122]. By reconstructing the union of data from various domains with the same network, the high-level representations can represent both the source and target domain data. Thus, a linear classifier that is trained on the labeled data of the source domain can make predictions on the target domain data with these representations. Despite their remarkable results, SDAs are limited by their high computational cost and lack of scalability to high-dimensional features. To address these crucial limitations, Chen et al. [10] proposed the marginalized SDA (mSDA), which marginalizes noise with linear denoisers; thus, parameters can be computed in closed-form and do not require stochastic gradient descent.\n\nThe deep reconstruction classification network (DRCN) proposed in [33] learns a shared encoding representation that provides useful information for cross-domain object recognition. DRCN is a CNN architecture that combines two pipelines with a shared encoder. After a representation is provided by the encoder, the first pipeline, which is a CNN, works for supervised classification with source labels, whereas the second pipeline, which is a deconvolutional network, optimizes for unsupervised reconstruction with target data.\nmin \u03bbL c ({\u03b8 enc , \u03b8 lab }) + (1 \u2212 \u03bb)L r ({\u03b8 enc , \u03b8 dec })(17)\nwhere \u03bb is a hyper-parameter that controls the trade-off between classification and reconstruction. \u03b8 enc , \u03b8 dec and \u03b8 lab denote the parameters of the encoder, decoder and source classifier, respectively. L c is cross-entropy loss for classification, and L r is squared loss x \u2212 f r (x) 2 2 for reconstruction in which f r (x) is the reconstruction of x. Fig. 13. The deep reconstruction classification network (DRCN) architecture. [33] Domain separation networks (DSNs) [5] explicitly and jointly model both private and shared components of the domain representations. A shared-weight encoder learns to capture shared representations, while a private encoder is used for domain-specific components in each domain. Additionally, a shared decoder learns to reconstruct the input samples by both the private and shared representations. Then, a classifier is trained on the shared representation. By partitioning the space in such a manner, the shared representations will not be influenced by domain-specific representations such that a better transfer ability can be obtained. Finding that the separation loss is simple and that the private features are only used for reconstruction in DSNs, [116] reinforced them by incorporating a hybrid adversarial learning in a separation network and an adaptation network.\n\nZhuang et al. [144] proposed transfer learning with deep autoencoders (TLDA), which consists of two encoding layers. The distance in distributions between domains is minimized with KL divergence in the embedding encoding layer, and label information of the source domain is encoded using a softmax loss in the label encoding layer. Ghifary et al. [31] extended the autoencoder into a model that jointly learns two types of data-reconstruction tasks taken from related domains: one is self-domain reconstruction, and the other is betweendomain reconstruction.\n\n\u2022 Adversarial Reconstruction Dual learning was first proposed by Xia et al. [43] to reduce the requirement of labeled data in natural language processing. Dual learning trains two \"opposite\" language translators, e.g., A-to-B and B-to-A. The two translators represent a primaldual pair that evaluates how likely the translated sentences belong to the targeted language, and the closed loop measures the disparity between the reconstructed and the original ones. Inspired by dual learning, adversarial reconstruction is adopted in deep DA with the help of dual GANs.\n\nZhu et al. [143] proposed a cycle GAN that can translate the characteristics of one image domain into the other in the absence of any paired training examples. Compared to dual learning, cycle GAN uses two generators rather than translators, which learn a mapping G : X \u2192 Y and an inverse mapping F : Y \u2192 X. Two discriminators, D X and D Y , measure how realistic the generated image is (G(X) \u2248 Y or G(Y ) \u2248 X) by an adversarial loss and how well the original input is reconstructed after a sequence of two generations (F (G(X)) \u2248 X or G(F (Y )) \u2248 Y ) by a cycle consistency loss (reconstruction loss). Thus, the distribution of images from G(X) (or F (Y )) is indistinguishable from the distribution Y (or X).\nL GAN (G, D Y , X, Y ) = E y\u223cp data (y) [log D Y (y)] +E x\u223cp data (x) [log(1 \u2212 D Y (G(x)))] L cyc (G, F ) = E x\u223cdata(x) [ F (G(x)) \u2212 x 1 ] +E y\u223cdata(y) [ G(F (y)) \u2212 y 1 ](18)\nwhere L GAN is the adversarial loss produced by discriminator D Y with mapping function G : X \u2192 Y . L cyc is the reconstruction loss using L1 norm. The dual GAN [131] and the disco GAN [59] were proposed at the same time, where the core idea is similar to cycle GAN. In dual GAN, the generator is configured with skip connections between mirrored downsampling and upsampling  [143] layers [93], [57], making it a U-shaped net to share low-level information (e.g., object shapes, textures, clutter, and so forth). For discriminators, the Markovian patch-GAN [65] architecture is employed to capture local high-frequency information. In disco GAN, various forms of distance functions, such as mean-square error (MSE), cosine distance, and hinge loss, can be used as the reconstruction loss, and the network is applied to translate images, changing specified attributes including hair color, gender and orientation while maintaining all other components.\n\n4) Hybrid Approaches: To obtain better performance, some of the aforementioned methods have been used simultaneously. [118] combined a domain confusion loss and a soft label loss, while [75] used both statistic (MMD) and architecture criteria (adapt classifier by residual function) for unsupervised DA. [130] introduced class-specific auxiliary weights assigned by the pseudo-labels into the original MMD. In DSNs [5], encoder-decoder reconstruction approaches separate representations into private and shared representations, while the MMD criterion or domain confusion loss is helpful to make the shared representations similar and soft subspace orthogonality constraints ensure dissimilarity between the private and shared representations. [95] used the MMD between the learned source and target representations and also allowed the weights of the corresponding layers to differ. [144] learned domaininvariant representations by encoder-decoder reconstruction approaches and the KL divergence.\n\n\nB. Heterogeneous Domain Adaptation\n\nIn heterogeneous DA, the feature spaces of the source and target domains are not the same, Xs = Xt, and the dimensions of the feature spaces may also differ. According to the divergence of feature spaces, heterogeneous DA can be further divided into two scenarios. In one scenario, the source and target domain both contain images, and the divergence of feature spaces is mainly caused by different sensory devices (e.g., visual light (VIS) vs. near-infrared (NIR) or RGB vs. depth) and different styles of images (e.g., sketches vs. photos). In the other scenario, there are different types of media in source and target domain (e.g., text vs. image and language vs. image). Obviously, the cross-domain gap of the second scenario is much larger.\n\nMost heterogeneous DA with shallow methods fall into two categories: symmetric transformation and asymmetric transformation. The symmetric transformation learns feature transformations to project the source and target features onto a common subspace. Heterogeneous feature augmentation (HFA) [23] first transformed the source and target data into a common subspace using projection matrices P and Q re-spectively, then proposed two new feature mapping functions, \u03d5 s (x s ) = [P x s , x s , 0 dt ] T and \u03d5 t (x t ) = [Qx t , 0 ds , x t ] T , to augment the transformed data with their original features and zeros. These projection matrices are found using standard SVM with hinge loss in both the linear and nonlinear cases and an alternating optimization algorithm is proposed to simultaneously solve the dual SVM and to find the optimal transformations. [124] treated each input domain as a manifold which is represented by a Laplacian matrix, and used labels rather than correspondences to align the manifolds.\n\nThe asymmetric transformation transforms one of source and target features to align with the other. [142] proposed a sparse and class-invariant feature transformation matrix to map the weight vector of classifiers learned from the source domain to the target domain. The asymmetric regularized cross-domain transfer (ARC-t) [63] used asymmetric, nonlinear transformations learned in Gaussian RBF kernel space to map the target data to the source domain. Extended from [97], ARC-t performed asymmetric transformation based on metric learning, and transfer knowledge between domains with different dimensions through changes of the regularizer. Since we focus on deep DA, we refer the interested readers to [20], which summarizes shallow approaches of heterogeneous DA. However, as for deep methods, there is not much work focused on heterogeneous DA so far. The special and effective methods of heterogeneous deep DA have not been proposed, and heterogeneous deep DA is still performed similar to some approaches of homogeneous DA.\n\n1) Discrepancy-Based Approach: In discrepancy-based approaches, the network generally shares or reuses the first n layers between the source and target domains, which limits the feature spaces of the input to the same dimension. However, in heterogeneous DA, the dimensions of the feature spaces of source domain may differ from those of target domain.\n\nIn first scenario of heterogeneous DA, the images in different domains can be directly resized into the same dimensions, so the Class Criterion and Statistic Criterion are still effective and are mainly used. For example, given an RGB image and its paired depth image, [41] used the mid-level representation learned by CNNs as a supervisory signal to re-train a CNN on depth images. To transform an RGB object detector into a RGB-D detector without needing complete RGB-D data, Hoffman et al. [48] first trained an RGB network using labeled RGB data from all categories and finetuned the network with labeled depth data from partial categories, then combined midlevel RGB and depth representations at fc6 to incorporate both modalities into the final object class prediction. [77] first trained the network using large face database of photos and then finetuned it using small database of composite sketches; [72] transferred the VIS deep networks to the NIR domain in the same way.\n\nIn second scenario, the features of different media can not be directly resized into the same dimensions. Therefore, discrepancy-based methods fail to work without extra process. [105] proposed weakly shared DTNs to transfer labeled information across heterogeneous domains, particularly from the text domain to the image domain. DTNs take paired data, such as text and image, as input to two SAEs, followed by weakly parameter-shared network layers at the top. Chen et al. [12] proposed transfer neural trees (TNTs), which consist of two stream networks to learn a domain-invariant feature representation for each modality. Then, a transfer neural decision forest (Transfer-NDF) [94], [61] is used with stochastic pruning for adapting representative neurons in the prediction layer.\n\n2) Adversarial-Based Approach: Using Generative Models can generate the heterogeneous target data while transferring some information of source domain to them. [111] employed a compound loss function that consists of a multiclass GAN loss, a regularizing component and an f-constancy component to transfer unlabeled face photos to emoji images. To generate images for birds and flowers based on text, [90] trained a GAN conditioned on text features encoded by a hybrid characterlevel convolutional-recurrent neural network. [135] proposed stacked generative adversarial networks (StackGAN) with conditioning augmentation for synthesizing photo-realistic images from text. It decomposes the synthesis problem into several sketch-refinement processes. Stage-I GAN sketches the primitive shape and basic colors of the object to yield low-resolution image, and Stage-II GAN completes details of the object to produce a high-resolution photo-realistic image. Fig. 15. The StackGAN architecture. [135] 3) Reconstruction-Based Approach: The Adversarial Reconstruction can be used in heterogeneous DA as well. For example, the cycle GAN [143], dual GAN [131] and disco GAN [59] used two generators, G A and G B , to generate sketches from photos and photos from sketches, respectively. Based on cycle GAN [143], [125] proposed a multi-adversarial network to avoid artifacts of facial photo-sketch synthesis by leveraging the implicit presence of feature maps of different resolutions in the generator subnetwork.\n\n\nV. MULTI-STEP DOMAIN ADAPTATION\n\nFor multi-step DA, the selection of the intermediate domain is problem specific, and different problems may have different strategies.\n\n\nA. Hand-Crafted Approaches\n\nOccasionally, the intermediate domain can be selected by experience, that is, it is decided in advance. For example, when the source domain is image data and the target domain is composed of text data, some annotated images will clearly be crawled as intermediate domain data.\n\nWith the common sense that nighttime light intensities can be used as a proxy for economic activity, Xie et al. [129] transferred knowledge from daytime satellite imagery to poverty prediction with the help of some nighttime light intensity information as an intermediate domain.\n\n\nB. Instance-Based Approaches\n\nIn other problems where there are many candidate intermediate domains, some automatic selection criterion should be considered. Similar to the instance-transfer approaches proposed by Pan [83], because the samples of the source domain cannot be used directly, the mixture of certain parts of the source and target data can be useful for constructing the intermediate domain.\n\nTan et al. [114] proposed  \nJ 1 (f e , f d , v S , v T ) = 1 n S n S i=1 v i S x i S \u2212 x i S 2 2 + 1 n I n I i=1 v i I x i I \u2212 x i I 2 2 + 1 n T n T i=1 x i T \u2212 x i T 2 2 + R(v S , v T )(19)\n\nC. Representation-Based Approaches\n\nRepresentation-based approaches freeze the previously trained network and use their intermediate representations as input to the new network. Rusu et al. [96] introduced progressive networks that have the ability to accumulate and transfer knowledge to new domains over a sequence of experiences. To avoid the target model losing its ability to solve the source domain, they constructed a new neural network for each domain, while transfer is enabled via lateral connections to features of previously learned networks. In the process, the parameters in the latest network are frozen to remember knowledge of intermediate domains. Fig. 16. The progressive network architecture. [96] VI. APPLICATION OF DEEP DOMAIN ADAPTATION Deep DA techniques have recently been successfully applied in many real-world applications, including image classification, object recognition, face recognition, object detection, style translation, and so forth. In this section, we present different application examples using various visual deep DA methods. Because the information of commonly used datasets for evaluating the performance is provided in [137] in detail, we do not introduce it in this paper.\n\n\nA. Image Classification\n\nBecause image classification is a basic task of computer vision applications, most of the algorithms mentioned above were originally proposed to solve such problems. Therefore, we do not discuss this application repeatedly, but we show how much benefit deep DA methods for image classification can bring. Because different papers often use different parameters, experimental protocols and tuning strategies in the preprocessing steps, it is quite difficult to perform a fair comparison among all the methods directly. Thus, similar to the work of Pan [83], we show the comparison results between the proposed deep DA methods and non-adaptation methods using only deep networks. A list of simple experiments taken from some published deep DA papers are presented in Table V.\n\nIn [74], [134], and [118], the authors used the Office-31 dataset 1 as one of the evaluation data sets, as shown in Fig.  1(a). The Office dataset is a computer vision classification data set with images from three distinct domains: Amazon (A), DSLR (D), and Webcam (W). The largest domain, Amazon, has 2817 labeled images and its corresponding 31 classes, which consists of objects commonly encountered in office settings. By using this dataset, previous works can show the performance of methods across all six possible DA tasks. [74] showed comparison experiments among the standard AlexNet [62], the DANN method [25], and the MMD algorithm and its variations, such as DDC [120], DAN [73], JAN [74] and RTN [75]. Zellinger et al. [134] evaluated their proposed CMD algorithm in comparison to other discrepancy-based methods (DDC, deep CROAL [109], DLID [16], AdaBN [69]) and the adversarial-based method DANN. [118] proposed an algorithm combining soft label loss and domain confusion loss, and they also compared them with DANN and DLID under a supervised DA setting.\n\nIn [119], MNIST 2 (M), USPS 3 (U), and SVHN 4 (S) digit datasets (shown in Fig. 1(b)) are used for a cross-domain handwritten digit recognition task, and the experiment showed the comparison results on some adversarial-based methods, such as DANN, CoGAN [70] and ADDA [119], where the baseline is VGG-16 [106].\n\n\nB. Face Recognition\n\nThe performance of face recognition significantly degrades when there are variations in the test images that are not present in the training images. The dataset shift can be caused by poses, resolution, illuminations, expressions, and modality. Kan et al. [58] proposed a bi-shifting auto-encoder network (BAE) for face recognition across view angle, ethnicity, and imaging sensor. In BAE, source domain samples are shifted to the target domain, and sparse reconstruction is used with several local neighbors from the target domain to ensure its correction, and vice versa. Single sample per person domain adaptation network (SSPP-DAN) in [51] generates synthetic images with varying poses to increase the number of samples in the source domain and bridges the gap between the synthetic and source domains by adversarial training with a GRL in realworld face recognition. [107] improved the performance of video face recognition by using an adversarial-based approach with large-scale unlabeled videos, labeled still images and synthesized images. Considering that age variations are difficult problems for smile detection and that networks trained on the current benchmarks do not perform well on young children, Xia et al. [127] applied DAN [73] and JAN [74] (mentioned in Section IV-A1) to two baseline deep models, i.e., AlexNet and ResNet, to transfer the knowledge from adults to infants. \n\n\nC. Object Detection\n\nRecent advances in object detection are driven by regionbased convolutional neural networks (R-CNNs [35], fast R-CNNs [34] and faster R-CNNs [91]). They are composed of a window selection mechanism and classifiers that are pre-trained labeled bounding boxes by using the features extracted from CNNs. At test time, the classifier decides whether a region obtained by sliding windows contains the object. Although the R-CNN algorithm is effective, a large Because R-CNNs train classifiers on regions just like classification, weak labeled data (such as image-level class labels) are directly useful for the detector. Most works learn the detector with limited bounding box labeled data and massive weak labeled data. The large-scale detection through adaptation (LSDA) [47] trains a classification layer for the target domain and then uses a pre-trained source model along with output layer adaptation techniques to update the target classification parameters directly. Rochan et al. [92] used word vectors to establish the semantic relatedness between weak labeled source objects and target objects and then transferred the bounding box labeled information from source objects to target objects based on their relatedness. Extending [47] and [92], Tang et al. [115] transferred visual (based on the LSDA model) and semantic similarity (based on work vectors) for training an object detector on weak labeled category. [13] incorporated both an image-level and an instance-level adaptation component into faster R-CNN and minimized the domain discrepancy based on adversarial training. By using bounding box labeled data in a source domain and weak labeled data in a target domain, [55] progressively fine-tuned the pre-trained model with domain-transfer samples and pseudo-labeling samples.\n\n\nD. Semantic Segmentation\n\nFully convolutional network models (FCNs) for dense prediction have proven to be successful for evaluating semantic segmentation, but their performance will also degrade under domain shifts. Therefore, some work has also explored using weak labels to improve the performance of semantic segmentation. Hong et al. [52] used a novel encoder-decoder architecture with attention model by transferring weak class labeled knowledge in the source domain, while [60], [103] transferred weak object location knowledge.\n\nMuch attention has also been paid to deep unsupervised DA in semantic segmentation. Hoffman et al. [50] first introduced it, in which global domain alignment is performed using FCNs with adversarial-based training, while transferring spatial layout is achieved by leveraging class-aware constrained multiple instance loss. Zhang et al. [140] enhanced the segmentation performance on real images with the help of virtual ones. It uses the global label distribution loss of the images and local label distribution loss of the landmark superpixels in the target domain to effectively regularize the fine-tuning of the semantic segmentation network. Chen et al. [15] proposed a framework for cross-city semantic segmentation. The framework assigns pseudo labels to pixels/grids in the target domain and jointly utilizes global and class-wise alignment by domain adversarial learning to minimize domain shift. In [14], a target guided distillation module adapts the style from the real images by imitating the pre-trained source network, and a spatial-aware adaptation module leverages the intrinsic spatial structure to reduce the domain divergence. Rather than operating a simple adversarial objective on the feature space, [100] used a GAN to address domain shift in which a generator projects the features to the image space and a discriminator operates on this projected image space. Fig. 18. The architecture of pixel-level adversarial and constraint-based adaptation. [50] E. Image-to-Image Translation Image-to-image translation has recently achieved great success with deep DA, and it has been applied to various tasks, such as style transferring. Specially, when the feature spaces of source and target images are not same, image-to-image translation should be performed by heterogeneous DA.\n\nMore approaches of image-to-image translation use a dataset of paired images and incorporate a DA algorithm into generative networks. Isola et al. [57] proposed the pix2pix framework, which uses a conditional GAN to learn a mapping from source to target images. Tzeng et al. [117] utilized domain confusion loss and pairwise loss to adapt from simulation to real-world data in a PR2 robot. However, several other methods also address the unpaired setting, such as CoGAN [70], cycle GAN [143], dual GAN [131] and disco GAN [59].\n\nMatching the statistical distribution by fine-tuning a deep network is another way to achieve image-to-image translation. Gatys et al. [27] fine-tuned the CNN to achieve DA by the total loss, which is a linear combination between the content and the style loss, such that the target image is rendered in the style of the source image maintaining the content. The content loss minimizes the mean squared difference of the feature representation between the original image and generated image in higher layers, while the style loss minimizes the elementwise mean squared difference between the Gram matrix of them on each layer. [68] demonstrated that matching the Gram matrices of feature maps is equivalent to minimizing the MMD. Rather than MMD, [87] proposed a deep generative correlation alignment network (DGCAN) that bridges the domain discrepancy between CAD synthetic and real images by applying the content and CORAL losses to different layers.\n\n\nF. Person Re-identification\n\nIn the community, person re-identification (re-ID) has become increasingly popular. When given video sequences of a person, person re-ID recognizes whether this person has been in another camera to compensate for the limitations of fixed devices. Recently, deep DA methods have been used in re-ID when models trained on one dataset are directly used on another. Xiao et al. [128] proposed the domain-guided dropout algorithm to discard useless neurons for re-identifying persons on multiple datasets simultaneously. Inspired by cycle GAN and Siamese network, the similarity preserving generative adversarial network (SPGAN) [21] translated the labeled source image to the target domain, preserving self similarity and domain-dissimilarity in an unsupervised manner, and then it trains re-ID models with the translated images using supervised feature learning methods.\n\n\nG. Image Captioning\n\nRecently, image captioning, which automatically describes an image with a natural sentence, has been an emerging challenge in computer vision and natural language processing. Due to lacking of paired image-sentence training data, DA leverages different types of data in other source domains to tackle this challenge. Chen et al. [11] proposed a novel adversarial training procedure (captioner v.s. critics) for crossdomain image captioning using paired source data and unpaired target data. One captioner adapts the sentence style from source to target domain, whereas two critics, namely domain critic and multi-modal critic, aim at distinguishing them. Zhao et al. [141] fine-tuned the pre-trained source model on limited data in the target domain via a dual learning mechanism.\n\n\nVII. CONCLUSION\n\nIn a broad sense, deep DA is utilizing deep networks to enhance the performance of DA, such as shallow DA methods with features extracted by deep networks. In a narrow sense, deep DA is based on deep learning architectures designed for DA and optimized by back propagation. In this survey paper, we focus on this narrow definition, and we have reviewed deep DA techniques on visual categorization tasks.\n\nDeep DA is classified as homogeneous DA and heterogeneous DA, and it can be further divided into supervised, semisupervised and unsupervised settings. The first setting is the simplest but is generally limited due to the need for labeled data; thus, most previous works focused on unsupervised cases. Semi-supervised deep DA is a hybrid method that combines the methods of the supervised and unsupervised settings.\n\nFurthermore, the approaches of deep DA can be classified into one-step DA and multi-step DA considering the distance of the source and target domains. When the distance is small, one-step DA can be used based on training loss. It consists of the discrepancy-based approach, the adversarialbased approach, and the reconstruction-based approach. When the source and target domains are not directly related, multistep (or transitive) DA can be used. The key of multi-step DA is to select and utilize intermediate domains, thus falling into three categories, including hand-crafted, feature-based and representation-based selection mechanisms.\n\nAlthough deep DA has achieved success recently, many issues still remain to be addressed. First, most existing algorithms focus on homogeneous deep DA, which assumes that the feature spaces between the source and target domains are the same. However, this assumption may not be true in many applications. We expect to transfer knowledge without this severe limitation and take advantage of existing datasets to help with more tasks. Heterogeneous deep DA may attract increasingly more attention in the future.\n\nIn addition, deep DA techniques have been successfully applied in many real-world applications, including image classification, and style translation. We have also found that only a few papers address adaptation beyond classification and recognition, such as object detection, face recognition, semantic segmentation and person re-identification. How to achieve these tasks with no or a very limited amount of data is probably one of the main challenges that should be addressed by deep DA in the next few years.\n\nFinally, since existing deep DA methods aim at aligning marginal distributions, they commonly assume shared label space across the source and target domains. However, in realistic scenario, the images of the source and target domain may be from the different set of categories or only a few categories of interest are shared. Recently, some papers [8], [7], [136] have begun to focus on this issue and we believe it is worthy of more attention.\n\nFig. 1 .\n1(a) Some object images from the \"Bike\" and \"Laptop\" categories in Amazon, DSLR, Webcam, and Caltech-256 databases. (b) Some digit images from MNIST, USPS, and SVHN databases. (c) Some face images from LFW, BCS and CUFS databases. Realworld computer vision applications, such as face recognition, must learn to adapt to distributions specific to each domain.\n\nFig. 3 .\n3Different learning processes between (a) traditional machine learning, (b) one-step domain adaptation and (c) multi-step domain adaptation[83].\n\nFig. 4 .\n4The average accuracy over the validation set for a network trained with different strategies. Baseline B: the network is trained on dataset B. 2) BnB: the first n layers are reused from baseline B and frozen. The higher layers are trained on dataset B. 3) BnB+: the same as BnB but where all layers are fine-tuned. 4) AnB: the first n layers are reused from the network trained on dataset A and frozen. The higher layers are trained on dataset B. 5) AnB+: the same as AnB but where all layers are fine-tuned[133].\n\nFig. 5 .\n5Deep DA by combining domain confusion loss and soft label loss[118].\n\n\n(X s , X t ) is the MMD between representations of the source and target domains. S c and S b define the intraclass compactness and the interclass separability.\n\nFig. 6 .\n6Different approaches with the MMD metric. (a) The deep adaptation network (DAN) architecture[73], (b) the joint adaptation network (JAN) architecture[74] and (c) the residual transfer network (RTN) architecture[75].\n\nT\n} L l=1 are the parameters of the l th layer in the source and target domains, respectively.\n\nFig. 7 .\n7The two-stream architecture with related weight[95].\n\nFig. 8 .\n8Generalized architecture for adversarial domain adaptation. Existing adversarial adaptation methods can be viewed as instantiations of a framework with different choices regarding their properties.[119] \n\nFig. 10 .\n10The model that exploits GANs conditioned on noise vector and source images.[4] \n\nFig. 12 .\n12The Adversarial discriminative domain adaptation (ADDA) architecture.[119] \n\nFig. 14 .\n14The cycle GAN architecture.\n\n\ndistant domain transfer learning (DDTL), where long-distance domains fail to transfer knowledge by only one intermediate domain but can be related via multiple intermediate domains. DDTL gradually selects unlabeled data from the intermediate domains by minimizing reconstruction errors on the selected instances in the source and intermediate domains and all the instances in the target domain simultaneously. With removal of the unrelated source data, the selected intermediate domains gradually become closer to the target domain from the source domain:\n\n\ni I are reconstructions of source data S i , target data T i and intermediate data I i based on the autoencoder, respectively, and f e and f d are the parameters of the encoder and decoder, respectively. v S = (v 1 S , ..., v n S S ) and v I = (v 1 I , ..., v n I I ) , v i S , v i I \u2208 0, 1 are selection indicators for the i th source and intermediate instance, respectively. R(v S , v T ) is a regularization term that avoids all values of v S and v I being zero. The DLID model [16] mentioned in Section IV-A1 (Geometric Criterion) constructs the intermediate domains with a subset of the source and target domains, where source samples are gradually replaced by target samples.\n\nFig. 17 .\n17The single sample per person domain adaptation network (SSPP-DAN) architecture.[51] \n\n\nFig. 2. An overview of different settings of domain adaptation 2) In the semi-supervised DA, both limited labeled data,One-step \nDomain adaptation \n\nHomogeneous \n\nHeterogeneous \n\nSupervised \n\nSemi-Supervised \n\nUnsupervised \n\nSupervised \n\nSemi-Supervised \n\nUnsupervised \n\nLabeled data are available in \ntarget domain \n\nLabeled+unlabeled data are \navailable in target domain \n\nNo labeled data in target \ndomain \n\nLabeled data are available in \ntarget domain \n\nLabeled+unlabeled data are \navailable in target domain \n\nNo labeled data in target \ndomain \n\nFeature Space is same between \nsource and target domain \n\nFeature Space is different \nbetween source and target \ndomain \n\nMulti-step \nDomain adaptation \n\nDomain \nadaptation \n\nSelect Intermediate Domain \n\nD tl , and redundant unlabeled data, D tu , in the target \ndomain are available in the training stage, which allows \nthe networks to learn the structure information of the \ntarget domain. \n3) In the unsupervised DA, no labeled but sufficient un-\nlabeled target domain data, D tu , are observable when \ntraining the network. \n\n\n\nTABLE I DIFFERENT\nIDEEP APPROACHES TO ONE-STEP DAOne-step DA \nApproaches \nBrief Description \nSubsettings \n\n\n\nTABLE II DIFFERENT\nIIDEEP APPROACHES TO MULTI-STEP DAMulti-step Approaches \nBrief Description \nHand-crafted \nusers determine the intermediate domains based on experience [129] \n\n\n\nTABLE III DIFFERENT\nIIIAPPROACHES USED IN DIFFERENT DOMAIN ADAPTATION SETTINGSSupervised DA Unsupervised DA \n\nDiscrepancy-based \n\nClass Criterion \n\u221a \n\nStatistic Criterion \n\u221a \n\nArchitecture Criterion \n\u221a \n\u221a \n\nGeometric Criterion \n\u221a \n\nAdversarial-based \nGenerative Model \n\u221a \n\nNon-Generative Model \n\u221a \n\nReconstruction-based \nEncoder-Decoder Model \n\u221a \n\nAdversarial Model \n\u221a \n\n\n\nTABLE IV SOME\nIVCOMMON RULES OF THUMB FOR DECIDING FINE-TUNED OR FROZEN IN THE FIRST N LAYERS.[17] The Size of Target Dataset \nLow \nMedium \nHigh \nThe Distance \nLow \nFreeze \n\n\n\nThe geometric criterion mitigates the domain shift by integrating intermediate subspaces on a geodesic path from the source to the target domains. A geodesic flow curve is constructed to connect the source and target domains on the Grassmannian. The source and target subspaces are points on a Grassmann manifold. By sampling a fixed[40] or infinite[38] number of subspaces along the geodesic, we can form the intermediate subspaces to help to find the correlations between domains. Then, both source and target data are projected to the obtained intermediate subspaces to align the distribution.Inspired by the intermediate representations on the geodesic path, Chopra et al.[16] proposed a model called deep learning for DA by interpolating between domains (DLID). DLID generates intermediate datasets, starting with all the source data samples and gradually replacing source data with target data. Each dataset is a single point on an interpolating path between the source and target domains. Once intermediate datasets are generated, a deep nonlinear feature extractor using the predictive sparse decomposition is trained in an unsupervised manner.a domain \ngeneral model, and \u2206 (i) is a domain specific bias term. After \nthe low rank parameterized CNNs are trained, \u0398 (0) can serve \nas the classifier for target domain. \n\u2022 Geometric Criterion \n\n\nTABLE V COMPARISON\nVBETWEEN TRANSFER LEARNING AND NON-ADAPTATION LEARNING METHODS amount of bounding box labeled data is required to train each detection category. To solve the problem of lacking labeled data, considering the window selection mechanism as being domain independent, deep DA methods can be used in classifiers to adapt to the target domain.Data Set \n(reference) \nSource vs. Target \nBaselines \nDeep Domain Adaptation Methods \n\nAlexNet \nDDC \nDAN \nRTN \nJAN \nDANN \nA vs. W \n61.6\u00b10.5 \n61.8\u00b10.4 \n68.5 \n73.3\u00b10.3 \n75.2\u00b10.4 \n73.0\u00b10.5 \nD vs. W \n95.4\u00b10.3 \n95.0\u00b10.5 \n96.0\u00b10.3 \n96.8\u00b10.2 \n96.6\u00b10.2 \n96.4\u00b10.3 \nOffice-31 Dataset \nW vs. D \n99.0\u00b10.2 \n98.5\u00b10.4 \n99.0\u00b10.3 \n99.6\u00b10.1 \n99.6\u00b10.1 \n99.2\u00b10.3 \nACC (unit:%)[74] \nA vs. D \n63.8\u00b10.5 \n64.4\u00b10.3 \n67.0\u00b10.4 \n71.0\u00b10.2 \n72.8\u00b10.3 \n72.3\u00b10.3 \nD vs. A \n51.1\u00b10.6 \n52.1\u00b10.6 \n54.0\u00b10.5 \n50.5\u00b10.3 \n57.5\u00b10.2 \n53.4\u00b10.4 \nW vs. A \n49.8\u00b10.4 \n52.2\u00b10.4 \n53.1\u00b10.5 \n51.0\u00b10.1 \n56.3\u00b10.2 \n51.2\u00b10.5 \nAvg \n70.1 \n70.6 \n72.9 \n73.7 \n76.3 \n74.3 \nAlexNet \nDeep CORAL \nCMD \nDLID \nAdaBN \nDANN \nA vs. W \n61.6 \n66.4 \n77.0\u00b10.6 \n51.9 \n74.2 \n73 \nD vs. W \n95.4 \n95.7 \n96.3\u00b10.4 \n78.2 \n95.7 \n96.4 \nOffice-31 Dataset \nW vs. D \n99.0 \n99.2 \n99.2\u00b10.2 \n89.9 \n99.8 \n99.2 \nACC (unit:%)[134] \nA vs. D \n63.8 \n66.8 \n79.6\u00b10.6 \n-\n73.1 \n-\nD vs. A \n51.1 \n52.8 \n63.8\u00b10.7 \n-\n59.8 \n-\nW vs. A \n49.8 \n51.5 \n63.3\u00b10.6 \n-\n57.4 \n-\nAvg \n70.1 \n72.1 \n79.9 \n-\n76.7 \n-\n\nAlexNet \nDLID \nDANN \nSoft Labels \nDomain \nConfusion \n\nConfusion \n+Soft \nA vs. W \n56.5\u00b10.3 \n51.9 \n53.6\u00b10.2 \n82.7\u00b10.7 \n82.8\u00b10.9 \n82.7\u00b10.8 \nD vs. W \n92.4\u00b10.3 \n78.2 \n71.2\u00b10.0 \n95.9\u00b10.6 \n95.6\u00b10.4 \n95.7\u00b10.5 \nOffice-31 Dataset \nW vs. D \n93.6\u00b10.2 \n89.9 \n83.5\u00b10.0 \n98.3\u00b10.3 \n97.5\u00b10.2 \n97.6\u00b10.2 \nACC (unit:%)[118] \nA vs. D \n64.6\u00b10.4 \n-\n-\n84.9\u00b11.2 \n85.9\u00b11.1 \n86.1\u00b11.2 \nD vs. A \n47.6\u00b10.1 \n-\n-\n66.0\u00b10.5 \n66.2\u00b10.4 \n66.2\u00b10.3 \nW vs. A \n42.7\u00b10.1 \n-\n-\n65.2\u00b10.6 \n64.9\u00b10.5 \n65.0\u00b10.5 \nAvg \n66.2 \n-\n-\n82.17 \n82.13 \n82.22 \nMNIST, USPS, \nVGG-16 \nDANN \nCoGAN \nADDA \nand SVHN \nM vs. U \n75.2\u00b11.6 \n77.1\u00b11.8 \n91.2\u00b10.8 \n89.4\u00b10.2 \ndigits datasets \nU vs. M \n57.1\u00b11.7 \n73.0\u00b12.0 \n89.1\u00b10.8 \n90.1\u00b10.8 \nACC (unit:%)[119] \nS vs. M \n60.1\u00b11.1 \n73.9 \n-\n76.0\u00b11.8 \n\n\nhttps://cs.stanford.edu/\u223cjhoffman/domainadapt/\nhttp://yann.lecun.com/exdb/mnist/ 3 http://statweb.stanford.edu/\u223ctibs/ElemStatLearn/data.html 4 http://ufldl.stanford.edu/housenumbers/\n\n. M Arjovsky, S Chintala, L Bottou, arXiv:1701.07875Wasserstein gan. arXiv preprintM. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875, 2017.\n\nLearning deep architectures for ai. Y Bengio, Machine Learning. 2Y. Bengio. Learning deep architectures for ai. Foundations and Trends in Machine Learning, 2(1):1-127, 2009.\n\nIntegrating structured biological data by kernel maximum mean discrepancy. K M Borgwardt, A Gretton, M J Rasch, H.-P Kriegel, B Sch\u00f6lkopf, A J Smola, Bioinformatics. 2214K. M. Borgwardt, A. Gretton, M. J. Rasch, H.-P. Kriegel, B. Sch\u00f6lkopf, and A. J. Smola. Integrating structured biological data by kernel maximum mean discrepancy. Bioinformatics, 22(14):e49-e57, 2006.\n\nUnsupervised pixel-level domain adaptation with generative adversarial networks. K Bousmalis, N Silberman, D Dohan, D Erhan, D Krishnan, arXiv:1612.05424arXiv preprintK. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Krishnan. Unsupervised pixel-level domain adaptation with generative adversarial networks. arXiv preprint arXiv:1612.05424, 2016.\n\nDomain separation networks. K Bousmalis, G Trigeorgis, N Silberman, D Krishnan, D Erhan, Advances in Neural Information Processing Systems. K. Bousmalis, G. Trigeorgis, N. Silberman, D. Krishnan, and D. Erhan. Domain separation networks. In Advances in Neural Information Processing Systems, pages 343-351, 2016.\n\nDomain adaptation problems: A dasvm classification technique and a circular validation strategy. L Bruzzone, M Marconcini, IEEE transactions on pattern analysis and machine intelligence. 32L. Bruzzone and M. Marconcini. Domain adaptation problems: A dasvm classification technique and a circular validation strategy. IEEE transactions on pattern analysis and machine intelligence, 32(5):770- 787, 2010.\n\nOpen set domain adaptation. P P Busto, J Gall, The IEEE International Conference on Computer Vision (ICCV). 13P. P. Busto and J. Gall. Open set domain adaptation. In The IEEE International Conference on Computer Vision (ICCV), volume 1, page 3, 2017.\n\nPartial transfer learning with selective adversarial networks. Z Cao, M Long, J Wang, M I Jordan, arXiv:1707.07901arXiv preprintZ. Cao, M. Long, J. Wang, and M. I. Jordan. Partial transfer learning with selective adversarial networks. arXiv preprint arXiv:1707.07901, 2017.\n\nAutodial: Automatic domain alignment layers. F M Carlucci, L Porzi, B Caputo, E Ricci, S R Bul\u00f2, International Conference on Computer Vision. F. M. Carlucci, L. Porzi, B. Caputo, E. Ricci, and S. R. Bul\u00f2. Autodial: Automatic domain alignment layers. In International Conference on Computer Vision, 2017.\n\nMarginalized denoising autoencoders for domain adaptation. M Chen, Z Xu, K Weinberger, F Sha, arXiv:1206.4683arXiv preprintM. Chen, Z. Xu, K. Weinberger, and F. Sha. Marginalized denoising autoencoders for domain adaptation. arXiv preprint arXiv:1206.4683, 2012.\n\nShow, adapt and tell: Adversarial training of cross-domain image captioner. T.-H Chen, Y.-H Liao, C.-Y Chuang, W.-T Hsu, J Fu, M Sun, The IEEE International Conference on Computer Vision (ICCV). 2T.-H. Chen, Y.-H. Liao, C.-Y. Chuang, W.-T. Hsu, J. Fu, and M. Sun. Show, adapt and tell: Adversarial training of cross-domain image captioner. In The IEEE International Conference on Computer Vision (ICCV), volume 2, 2017.\n\nTransfer neural trees for heterogeneous domain adaptation. Chen , European Conference on Computer Vision. SpringerChen. Transfer neural trees for heterogeneous domain adaptation. In European Conference on Computer Vision, pages 399-414. Springer, 2016.\n\nDomain adaptive faster r-cnn for object detection in the wild. Y Chen, W Li, C Sakaridis, D Dai, L Van Gool, arXiv:1803.03243arXiv preprintY. Chen, W. Li, C. Sakaridis, D. Dai, and L. Van Gool. Domain adaptive faster r-cnn for object detection in the wild. arXiv preprint arXiv:1803.03243, 2018.\n\nRoad: Reality oriented adaptation for semantic segmentation of urban scenes. Y Chen, W Li, L Van Gool, arXiv:1711.11556arXiv preprintY. Chen, W. Li, and L. Van Gool. Road: Reality oriented adap- tation for semantic segmentation of urban scenes. arXiv preprint arXiv:1711.11556, 2017.\n\nNo more discrimination: Cross city adaptation of road scene segmenters. Y.-H Chen, W.-Y Chen, Y.-T Chen, B.-C Tsai, Y.-C F Wang, M Sun, arXiv:1704.08509arXiv preprintY.-H. Chen, W.-Y. Chen, Y.-T. Chen, B.-C. Tsai, Y.-C. F. Wang, and M. Sun. No more discrimination: Cross city adaptation of road scene segmenters. arXiv preprint arXiv:1704.08509, 2017.\n\nDlid: Deep learning for domain adaptation by interpolating between domains. S Chopra, S Balakrishnan, R Gopalan, ICML workshop on challenges in representation learning. 2S. Chopra, S. Balakrishnan, and R. Gopalan. Dlid: Deep learning for domain adaptation by interpolating between domains. In ICML workshop on challenges in representation learning, volume 2, 2013.\n\nBest practices for fine-tuning visual classifiers to new domains. B Chu, V Madhavan, O Beijbom, J Hoffman, T Darrell, Computer Vision-ECCV 2016 Workshops. SpringerB. Chu, V. Madhavan, O. Beijbom, J. Hoffman, and T. Darrell. Best practices for fine-tuning visual classifiers to new domains. In Computer Vision-ECCV 2016 Workshops, pages 435-442. Springer, 2016.\n\nSelective transfer machine for personalized facial action unit detection. W.-S Chu, F De La Torre, J F Cohn, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionW.-S. Chu, F. De la Torre, and J. F. Cohn. Selective transfer machine for personalized facial action unit detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3515-3522, 2013.\n\nDomain adaptation for visual applications: A comprehensive survey. G Csurka, arXiv:1702.05374arXiv preprintG. Csurka. Domain adaptation for visual applications: A comprehen- sive survey. arXiv preprint arXiv:1702.05374, 2017.\n\nA survey on heterogeneous transfer learning. O Day, T M Khoshgoftaar, Journal of Big Data. 4129O. Day and T. M. Khoshgoftaar. A survey on heterogeneous transfer learning. Journal of Big Data, 4(1):29, 2017.\n\nImage-image domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identification. W Deng, L Zheng, G Kang, Y Yang, Q Ye, J Jiao, arXiv:1711.07027arXiv preprintW. Deng, L. Zheng, G. Kang, Y. Yang, Q. Ye, and J. Jiao. Image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identification. arXiv preprint arXiv:1711.07027, 2017.\n\nDecaf: A deep convolutional activation feature for generic visual recognition. J Donahue, Y Jia, O Vinyals, J Hoffman, N Zhang, E Tzeng, T Darrell, International conference on machine learning. J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In International conference on machine learning, pages 647-655, 2014.\n\nLearning with augmented features for heterogeneous domain adaptation. L Duan, D Xu, I Tsang, arXiv:1206.4660arXiv preprintL. Duan, D. Xu, and I. Tsang. Learning with augmented features for heterogeneous domain adaptation. arXiv preprint arXiv:1206.4660, 2012.\n\nDepth map prediction from a single image using a multi-scale deep network. D Eigen, C Puhrsch, R Fergus, Advances in neural information processing systems. D. Eigen, C. Puhrsch, and R. Fergus. Depth map prediction from a single image using a multi-scale deep network. In Advances in neural information processing systems, pages 2366-2374, 2014.\n\nUnsupervised domain adaptation by backpropagation. Y Ganin, V Lempitsky, International Conference on Machine Learning. Y. Ganin and V. Lempitsky. Unsupervised domain adaptation by backpropagation. In International Conference on Machine Learning, pages 1180-1189, 2015.\n\nDomain-adversarial training of neural networks. Y Ganin, E Ustinova, H Ajakan, P Germain, H Larochelle, F Laviolette, M Marchand, V Lempitsky, Journal of Machine Learning Research. 1759Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Lavio- lette, M. Marchand, and V. Lempitsky. Domain-adversarial training of neural networks. Journal of Machine Learning Research, 17(59):1-35, 2016.\n\nImage style transfer using convolutional neural networks. L A Gatys, A S Ecker, M Bethge, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionL. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer using convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2414-2423, 2016.\n\nBorrowing treasures from the wealthy: Deep transfer learning through selective joint fine-tuning. W Ge, Y Yu, arXiv:1702.08690arXiv preprintW. Ge and Y. Yu. Borrowing treasures from the wealthy: Deep transfer learning through selective joint fine-tuning. arXiv preprint arXiv:1702.08690, 2017.\n\nT Gebru, J Hoffman, L Fei-Fei, arXiv:1709.02476Fine-grained recognition in the wild: A multi-task domain adaptation approach. arXiv preprintT. Gebru, J. Hoffman, and L. Fei-Fei. Fine-grained recognition in the wild: A multi-task domain adaptation approach. arXiv preprint arXiv:1709.02476, 2017.\n\nUnsupervised domain adaptation via representation learning and adaptive classifier learning. M Gheisari, M S Baghshah, Neurocomputing. 165M. Gheisari and M. S. Baghshah. Unsupervised domain adaptation via representation learning and adaptive classifier learning. Neurocomput- ing, 165:300-311, 2015.\n\nDomain generalization for object recognition with multi-task autoencoders. M Ghifary, W Bastiaan Kleijn, M Zhang, D Balduzzi, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionM. Ghifary, W. Bastiaan Kleijn, M. Zhang, and D. Balduzzi. Domain generalization for object recognition with multi-task autoencoders. In Proceedings of the IEEE international conference on computer vision, pages 2551-2559, 2015.\n\nDomain adaptive neural networks for object recognition. M Ghifary, W B Kleijn, M Zhang, Pacific Rim International Conference on Artificial Intelligence. SpringerM. Ghifary, W. B. Kleijn, and M. Zhang. Domain adaptive neural net- works for object recognition. In Pacific Rim International Conference on Artificial Intelligence, pages 898-904. Springer, 2014.\n\nDeep reconstruction-classification networks for unsupervised domain adaptation. M Ghifary, W B Kleijn, M Zhang, D Balduzzi, W Li, European Conference on Computer Vision. SpringerM. Ghifary, W. B. Kleijn, M. Zhang, D. Balduzzi, and W. Li. Deep reconstruction-classification networks for unsupervised domain adap- tation. In European Conference on Computer Vision, pages 597-613. Springer, 2016.\n\nFast r-cnn. R Girshick, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionR. Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 1440-1448, 2015.\n\nRich feature hierarchies for accurate object detection and semantic segmentation. R Girshick, J Donahue, T Darrell, J Malik, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionR. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 580-587, 2014.\n\nDomain adaptation for large-scale sentiment classification: A deep learning approach. X Glorot, A Bordes, Y Bengio, Proceedings of the 28th international conference on machine learning (ICML-11). the 28th international conference on machine learning (ICML-11)X. Glorot, A. Bordes, and Y. Bengio. Domain adaptation for large-scale sentiment classification: A deep learning approach. In Proceedings of the 28th international conference on machine learning (ICML-11), pages 513-520, 2011.\n\nConnecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation. B Gong, K Grauman, F Sha, International Conference on Machine Learning. B. Gong, K. Grauman, and F. Sha. Connecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation. In International Conference on Machine Learning, pages 222-230, 2013.\n\nGeodesic flow kernel for unsupervised domain adaptation. B Gong, Y Shi, F Sha, K Grauman, Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEEB. Gong, Y. Shi, F. Sha, and K. Grauman. Geodesic flow kernel for unsupervised domain adaptation. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 2066-2073. IEEE, 2012.\n\nGenerative adversarial nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Advances in neural information processing systems. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672-2680, 2014.\n\nDomain adaptation for object recognition: An unsupervised approach. R Gopalan, R Li, R Chellappa, Computer Vision (ICCV), 2011 IEEE International Conference on. IEEER. Gopalan, R. Li, and R. Chellappa. Domain adaptation for object recognition: An unsupervised approach. In Computer Vision (ICCV), 2011 IEEE International Conference on, pages 999-1006. IEEE, 2011.\n\nCross modal distillation for supervision transfer. S Gupta, J Hoffman, J Malik, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionS. Gupta, J. Hoffman, and J. Malik. Cross modal distillation for supervision transfer. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2827-2836, 2016.\n\nAssociative domain adaptation. P Haeusser, T Frerix, A Mordvintsev, D Cremers, International Conference on Computer Vision (ICCV). 26P. Haeusser, T. Frerix, A. Mordvintsev, and D. Cremers. Associative domain adaptation. In International Conference on Computer Vision (ICCV), volume 2, page 6, 2017.\n\nDual learning for machine translation. D He, Y Xia, T Qin, L Wang, N Yu, T Liu, W.-Y Ma, Advances in Neural Information Processing Systems. D. He, Y. Xia, T. Qin, L. Wang, N. Yu, T. Liu, and W.-Y. Ma. Dual learning for machine translation. In Advances in Neural Information Processing Systems, pages 820-828, 2016.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016.\n\nDistilling the knowledge in a neural network. G Hinton, O Vinyals, J Dean, arXiv:1503.02531arXiv preprintG. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.\n\nA fast learning algorithm for deep belief nets. G E Hinton, S Osindero, Y.-W Teh, Neural computation. 187G. E. Hinton, S. Osindero, and Y.-W. Teh. A fast learning algorithm for deep belief nets. Neural computation, 18(7):1527-1554, 2006.\n\nLsda: Large scale detection through adaptation. J Hoffman, S Guadarrama, E S Tzeng, R Hu, J Donahue, R Girshick, T Darrell, K Saenko, Advances in Neural Information Processing Systems. J. Hoffman, S. Guadarrama, E. S. Tzeng, R. Hu, J. Donahue, R. Gir- shick, T. Darrell, and K. Saenko. Lsda: Large scale detection through adaptation. In Advances in Neural Information Processing Systems, pages 3536-3544, 2014.\n\nCrossmodal adaptation for rgb-d detection. J Hoffman, S Gupta, J Leong, S Guadarrama, T Darrell, Robotics and Automation (ICRA), 2016 IEEE International Conference on. IEEEJ. Hoffman, S. Gupta, J. Leong, S. Guadarrama, and T. Darrell. Cross- modal adaptation for rgb-d detection. In Robotics and Automation (ICRA), 2016 IEEE International Conference on, pages 5032-5039. IEEE, 2016.\n\nOne-shot adaptation of supervised deep convolutional models. J Hoffman, E Tzeng, J Donahue, Y Jia, K Saenko, T Darrell, arXiv:1312.6204arXiv preprintJ. Hoffman, E. Tzeng, J. Donahue, Y. Jia, K. Saenko, and T. Darrell. One-shot adaptation of supervised deep convolutional models. arXiv preprint arXiv:1312.6204, 2013.\n\nJ Hoffman, D Wang, F Yu, T Darrell, arXiv:1612.02649Fcns in the wild: Pixel-level adversarial and constraint-based adaptation. arXiv preprintJ. Hoffman, D. Wang, F. Yu, and T. Darrell. Fcns in the wild: Pixel-level adversarial and constraint-based adaptation. arXiv preprint arXiv:1612.02649, 2016.\n\nSspp-dan: Deep domain adaptation network for face recognition with single sample per person. S Hong, W Im, J Ryu, H S Yang, arXiv:1702.04069arXiv preprintS. Hong, W. Im, J. Ryu, and H. S. Yang. Sspp-dan: Deep domain adaptation network for face recognition with single sample per person. arXiv preprint arXiv:1702.04069, 2017.\n\nLearning transferrable knowledge for semantic segmentation with deep convolutional neural network. S Hong, J Oh, H Lee, B Han, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionS. Hong, J. Oh, H. Lee, and B. Han. Learning transferrable knowledge for semantic segmentation with deep convolutional neural network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3204-3212, 2016.\n\nDeep transfer metric learning. J Hu, J Lu, Y.-P Tan, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJ. Hu, J. Lu, and Y.-P. Tan. Deep transfer metric learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 325-333, 2015.\n\nX Huang, S Belongie, arXiv:1703.06868Arbitrary style transfer in real-time with adaptive instance normalization. arXiv preprintX. Huang and S. Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. arXiv preprint arXiv:1703.06868, 2017.\n\nCross-domain weakly-supervised object detection through progressive domain adaptation. N Inoue, R Furuta, T Yamasaki, K Aizawa, arXiv:1803.11365arXiv preprintN. Inoue, R. Furuta, T. Yamasaki, and K. Aizawa. Cross-domain weakly-supervised object detection through progressive domain adap- tation. arXiv preprint arXiv:1803.11365, 2018.\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, International Conference on Machine Learning. S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pages 448-456, 2015.\n\nImage-to-image translation with conditional adversarial networks. P Isola, J.-Y Zhu, T Zhou, A A Efros, arXiv:1611.07004arXiv preprintP. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional adversarial networks. arXiv preprint arXiv:1611.07004, 2016.\n\nBi-shifting auto-encoder for unsupervised domain adaptation. M Kan, S Shan, X Chen, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionM. Kan, S. Shan, and X. Chen. Bi-shifting auto-encoder for unsuper- vised domain adaptation. In Proceedings of the IEEE International Conference on Computer Vision, pages 3846-3854, 2015.\n\nLearning to discover crossdomain relations with generative adversarial networks. T Kim, M Cha, H Kim, J Lee, J Kim, arXiv:1703.05192arXiv preprintT. Kim, M. Cha, H. Kim, J. Lee, and J. Kim. Learning to discover cross- domain relations with generative adversarial networks. arXiv preprint arXiv:1703.05192, 2017.\n\nSeed, expand and constrain: Three principles for weakly-supervised image segmentation. A Kolesnikov, C H Lampert, European Conference on Computer Vision. SpringerA. Kolesnikov and C. H. Lampert. Seed, expand and constrain: Three principles for weakly-supervised image segmentation. In European Conference on Computer Vision, pages 695-711. Springer, 2016.\n\nDeep neural decision forests. P Kontschieder, M Fiterau, A Criminisi, S Rota Bulo, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionP. Kontschieder, M. Fiterau, A. Criminisi, and S. Rota Bulo. Deep neural decision forests. In Proceedings of the IEEE International Conference on Computer Vision, pages 1467-1475, 2015.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Advances in neural information processing systems. A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems, pages 1097-1105, 2012.\n\nWhat you saw is not what you get: Domain adaptation using asymmetric kernel transforms. B Kulis, K Saenko, T Darrell, Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on. IEEEB. Kulis, K. Saenko, and T. Darrell. What you saw is not what you get: Domain adaptation using asymmetric kernel transforms. In Computer Vision and Pattern Recognition (CVPR), 2011 IEEE Conference on, pages 1785-1792. IEEE, 2011.\n\nLearning to detect unseen object classes by between-class attribute transfer. C H Lampert, H Nickisch, S Harmeling, Computer Vision and Pattern Recognition. IEEECVPR 2009. IEEE Conference onC. H. Lampert, H. Nickisch, and S. Harmeling. Learning to detect unseen object classes by between-class attribute transfer. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 951-958. IEEE, 2009.\n\nPrecomputed real-time texture synthesis with markovian generative adversarial networks. C Li, M Wand, European Conference on Computer Vision. SpringerC. Li and M. Wand. Precomputed real-time texture synthesis with markovian generative adversarial networks. In European Conference on Computer Vision, pages 702-716. Springer, 2016.\n\nDeeper, broader and artier domain generalization. D Li, Y Yang, Y.-Z Song, T M Hospedales, Computer Vision (ICCV. 2017D. Li, Y. Yang, Y.-Z. Song, and T. M. Hospedales. Deeper, broader and artier domain generalization. In Computer Vision (ICCV), 2017\n\nIEEE International Conference on. IEEEIEEE International Conference on, pages 5543-5551. IEEE, 2017.\n\nGenerative moment matching networks. Y Li, K Swersky, R Zemel, Proceedings of the 32nd International Conference on Machine Learning (ICML-15). the 32nd International Conference on Machine Learning (ICML-15)Y. Li, K. Swersky, and R. Zemel. Generative moment matching networks. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 1718-1727, 2015.\n\nY Li, N Wang, J Liu, X Hou, arXiv:1701.01036Demystifying neural style transfer. arXiv preprintY. Li, N. Wang, J. Liu, and X. Hou. Demystifying neural style transfer. arXiv preprint arXiv:1701.01036, 2017.\n\nRevisiting batch normalization for practical domain adaptation. Y Li, N Wang, J Shi, J Liu, X Hou, arXiv:1603.04779arXiv preprintY. Li, N. Wang, J. Shi, J. Liu, and X. Hou. Revisiting batch normaliza- tion for practical domain adaptation. arXiv preprint arXiv:1603.04779, 2016.\n\nCoupled generative adversarial networks. M.-Y Liu, O Tuzel, Advances in neural information processing systems. M.-Y. Liu and O. Tuzel. Coupled generative adversarial networks. In Advances in neural information processing systems, pages 469-477, 2016.\n\nA survey of deep neural network architectures and their applications. W Liu, Z Wang, X Liu, N Zeng, Y Liu, F E Alsaadi, Neurocomputing. 234W. Liu, Z. Wang, X. Liu, N. Zeng, Y. Liu, and F. E. Alsaadi. A survey of deep neural network architectures and their applications. Neurocomputing, 234:11-26, 2017.\n\nTransferring deep representation for nir-vis heterogeneous face recognition. X Liu, L Song, X Wu, T Tan, Biometrics (ICB), 2016 International Conference on. IEEEX. Liu, L. Song, X. Wu, and T. Tan. Transferring deep representation for nir-vis heterogeneous face recognition. In Biometrics (ICB), 2016 International Conference on, pages 1-8. IEEE, 2016.\n\nLearning transferable features with deep adaptation networks. M Long, Y Cao, J Wang, M Jordan, International Conference on Machine Learning. M. Long, Y. Cao, J. Wang, and M. Jordan. Learning transferable features with deep adaptation networks. In International Conference on Machine Learning, pages 97-105, 2015.\n\nDeep transfer learning with joint adaptation networks. M Long, J Wang, M I Jordan, arXiv:1605.06636arXiv preprintM. Long, J. Wang, and M. I. Jordan. Deep transfer learning with joint adaptation networks. arXiv preprint arXiv:1605.06636, 2016.\n\nUnsupervised domain adaptation with residual transfer networks. M Long, H Zhu, J Wang, M I Jordan, Advances in Neural Information Processing Systems. M. Long, H. Zhu, J. Wang, and M. I. Jordan. Unsupervised domain adaptation with residual transfer networks. In Advances in Neural Information Processing Systems, pages 136-144, 2016.\n\nWhen unsupervised domain adaptation meets tensor representations. H Lu, L Zhang, Z Cao, W Wei, K Xian, C Shen, A Van Den, Hengel, The IEEE International Conference on Computer Vision (ICCV). 2H. Lu, L. Zhang, Z. Cao, W. Wei, K. Xian, C. Shen, and A. van den Hengel. When unsupervised domain adaptation meets tensor represen- tations. In The IEEE International Conference on Computer Vision (ICCV), volume 2, 2017.\n\nComposite sketch recognition via deep network-a transfer learning approach. P Mittal, M Vatsa, R Singh, Biometrics (ICB), 2015 International Conference on. IEEEP. Mittal, M. Vatsa, and R. Singh. Composite sketch recognition via deep network-a transfer learning approach. In Biometrics (ICB), 2015 International Conference on, pages 251-256. IEEE, 2015.\n\nFew-shot adversarial domain adaptation. S Motiian, Q Jones, S Iranmanesh, G Doretto, Advances in Neural Information Processing Systems. S. Motiian, Q. Jones, S. Iranmanesh, and G. Doretto. Few-shot adversarial domain adaptation. In Advances in Neural Information Processing Systems, pages 6673-6683, 2017.\n\nUnified deep supervised domain adaptation and generalization. S Motiian, M Piccirilli, D A Adjeroh, G Doretto, The IEEE International Conference on Computer Vision (ICCV). 2S. Motiian, M. Piccirilli, D. A. Adjeroh, and G. Doretto. Unified deep supervised domain adaptation and generalization. In The IEEE International Conference on Computer Vision (ICCV), volume 2, 2017.\n\nDash-n: Joint hierarchical domain adaptation and feature learning. H V Nguyen, H T Ho, V M Patel, R Chellappa, IEEE Transactions on Image Processing. 2412H. V. Nguyen, H. T. Ho, V. M. Patel, and R. Chellappa. Dash-n: Joint hierarchical domain adaptation and feature learning. IEEE Transactions on Image Processing, 24(12):5479-5491, 2015.\n\nHashing in the zero shot framework with domain adaptation. S Pachori, A Deshpande, S Raman, Neurocomputing. S. Pachori, A. Deshpande, and S. Raman. Hashing in the zero shot framework with domain adaptation. Neurocomputing, 2017.\n\nDomain adaptation via transfer component analysis. S J Pan, I W Tsang, J T Kwok, Q Yang, IEEE Transactions on Neural Networks. 222S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. Domain adaptation via transfer component analysis. IEEE Transactions on Neural Networks, 22(2):199-210, 2011.\n\nA survey on transfer learning. S J Pan, Q Yang, IEEE Transactions on knowledge and data engineering. 2210S. J. Pan and Q. Yang. A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 22(10):1345-1359, 2010.\n\nVisual domain adaptation: A survey of recent advances. V M Patel, R Gopalan, R Li, R Chellappa, IEEE signal processing magazine. 323V. M. Patel, R. Gopalan, R. Li, and R. Chellappa. Visual domain adaptation: A survey of recent advances. IEEE signal processing magazine, 32(3):53-69, 2015.\n\n. K.-C Peng, Z Wu, J Ernst, arXiv:1707.01922Zero-shot deep domain adaptation. arXiv preprintK.-C. Peng, Z. Wu, and J. Ernst. Zero-shot deep domain adaptation. arXiv preprint arXiv:1707.01922, 2017.\n\nFine-to-coarse knowledge transfer for low-res image classification. X Peng, J Hoffman, X Y Stella, K Saenko, Image Processing (ICIP), 2016 IEEE International Conference on. IEEEX. Peng, J. Hoffman, X. Y. Stella, and K. Saenko. Fine-to-coarse knowledge transfer for low-res image classification. In Image Process- ing (ICIP), 2016 IEEE International Conference on, pages 3683-3687. IEEE, 2016.\n\nSynthetic to real adaptation with deep generative correlation alignment networks. X Peng, K Saenko, arXiv:1701.05524arXiv preprintX. Peng and K. Saenko. Synthetic to real adaptation with deep genera- tive correlation alignment networks. arXiv preprint arXiv:1701.05524, 2017.\n\nSubspace alignment based domain adaptation for rcnn detector. A Raj, V P Namboodiri, T Tuytelaars, arXiv:1507.05578arXiv preprintA. Raj, V. P. Namboodiri, and T. Tuytelaars. Subspace alignment based domain adaptation for rcnn detector. arXiv preprint arXiv:1507.05578, 2015.\n\nLearning multiple visual domains with residual adapters. S.-A Rebuffi, H Bilen, A Vedaldi, arXiv:1705.08045arXiv preprintS.-A. Rebuffi, H. Bilen, and A. Vedaldi. Learning multiple visual domains with residual adapters. arXiv preprint arXiv:1705.08045, 2017.\n\nS Reed, Z Akata, X Yan, L Logeswaran, B Schiele, H Lee, arXiv:1605.05396Generative adversarial text to image synthesis. arXiv preprintS. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee. Generative adversarial text to image synthesis. arXiv preprint arXiv:1605.05396, 2016.\n\nFaster r-cnn: Towards real-time object detection with region proposal networks. S Ren, K He, R Girshick, J Sun, Advances in neural information processing systems. S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information processing systems, pages 91-99, 2015.\n\nWeakly supervised localization of novel objects using appearance transfer. M Rochan, Y Wang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionM. Rochan and Y. Wang. Weakly supervised localization of novel ob- jects using appearance transfer. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4315-4324, 2015.\n\nU-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, International Conference on Medical Image Computing and Computer-Assisted Intervention. SpringerO. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 234-241. Springer, 2015.\n\nNeural decision forests for semantic image labelling. S Bulo, P Kontschieder, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionS. Rota Bulo and P. Kontschieder. Neural decision forests for semantic image labelling. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 81-88, 2014.\n\nBeyond sharing weights for deep domain adaptation. A Rozantsev, M Salzmann, P Fua, arXiv:1603.06432arXiv preprintA. Rozantsev, M. Salzmann, and P. Fua. Beyond sharing weights for deep domain adaptation. arXiv preprint arXiv:1603.06432, 2016.\n\nA A Rusu, N C Rabinowitz, G Desjardins, H Soyer, J Kirkpatrick, K Kavukcuoglu, R Pascanu, R Hadsell, arXiv:1606.04671Progressive neural networks. arXiv preprintA. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick, K. Kavukcuoglu, R. Pascanu, and R. Hadsell. Progressive neural networks. arXiv preprint arXiv:1606.04671, 2016.\n\nAdapting visual category models to new domains. K Saenko, B Kulis, M Fritz, T Darrell, European conference on computer vision. SpringerK. Saenko, B. Kulis, M. Fritz, and T. Darrell. Adapting visual category models to new domains. In European conference on computer vision, pages 213-226. Springer, 2010.\n\nAsymmetric tri-training for unsupervised domain adaptation. K Saito, Y Ushiku, T Harada, arXiv:1702.08400arXiv preprintK. Saito, Y. Ushiku, and T. Harada. Asymmetric tri-training for unsupervised domain adaptation. arXiv preprint arXiv:1702.08400, 2017.\n\nMaximum classifier discrepancy for unsupervised domain adaptation. K Saito, K Watanabe, Y Ushiku, T Harada, arXiv:1712.02560arXiv preprintK. Saito, K. Watanabe, Y. Ushiku, and T. Harada. Maximum classi- fier discrepancy for unsupervised domain adaptation. arXiv preprint arXiv:1712.02560, 2017.\n\nLearning from synthetic data: Addressing domain shift for semantic segmentation. S Sankaranarayanan, Y Balaji, A Jain, S N Lim, R Chellappa, S. Sankaranarayanan, Y. Balaji, A. Jain, S. N. Lim, and R. Chellappa. Learning from synthetic data: Addressing domain shift for semantic segmentation. 2017.\n\nTransfer learning for visual categorization: A survey. L Shao, F Zhu, X Li, IEEE transactions on neural networks and learning systems. 26L. Shao, F. Zhu, and X. Li. Transfer learning for visual categorization: A survey. IEEE transactions on neural networks and learning systems, 26(5):1019-1034, 2015.\n\nWasserstein distance guided representation learning for domain adaptation. J Shen, Y Qu, W Zhang, Y Yu, J. Shen, Y. Qu, W. Zhang, and Y. Yu. Wasserstein distance guided representation learning for domain adaptation. 2017.\n\nDistinct class-specific saliency maps for weakly supervised semantic segmentation. W Shimoda, K Yanai, European Conference on Computer Vision. SpringerW. Shimoda and K. Yanai. Distinct class-specific saliency maps for weakly supervised semantic segmentation. In European Conference on Computer Vision, pages 218-234. Springer, 2016.\n\nLearning from simulated and unsupervised images through adversarial training. A Shrivastava, T Pfister, O Tuzel, J Susskind, W Wang, R Webb, arXiv:1612.07828arXiv preprintA. Shrivastava, T. Pfister, O. Tuzel, J. Susskind, W. Wang, and R. Webb. Learning from simulated and unsupervised images through adversarial training. arXiv preprint arXiv:1612.07828, 2016.\n\nWeakly-shared deep transfer networks for heterogeneous-domain knowledge propagation. X Shu, G.-J Qi, J Tang, J Wang, Proceedings of the 23rd ACM international conference on Multimedia. the 23rd ACM international conference on MultimediaACMX. Shu, G.-J. Qi, J. Tang, and J. Wang. Weakly-shared deep transfer networks for heterogeneous-domain knowledge propagation. In Pro- ceedings of the 23rd ACM international conference on Multimedia, pages 35-44. ACM, 2015.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, arXiv:1409.1556arXiv preprintK. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\n\nUnsupervised domain adaptation for face recognition in unlabeled videos. K Sohn, S Liu, G Zhong, X Yu, M.-H Yang, M Chandraker, arXiv:1708.02191arXiv preprintK. Sohn, S. Liu, G. Zhong, X. Yu, M.-H. Yang, and M. Chandraker. Unsupervised domain adaptation for face recognition in unlabeled videos. arXiv preprint arXiv:1708.02191, 2017.\n\nReturn of frustratingly easy domain adaptation. B Sun, J Feng, K Saenko, AAAI. 68B. Sun, J. Feng, and K. Saenko. Return of frustratingly easy domain adaptation. In AAAI, volume 6, page 8, 2016.\n\nDeep coral: Correlation alignment for deep domain adaptation. B Sun, K Saenko, Computer Vision-ECCV 2016 Workshops. SpringerB. Sun and K. Saenko. Deep coral: Correlation alignment for deep domain adaptation. In Computer Vision-ECCV 2016 Workshops, pages 443-450. Springer, 2016.\n\nGoing deeper with convolutions. C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, D Anguelov, D Erhan, V Vanhoucke, A Rabinovich, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionC. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1-9, 2015.\n\nUnsupervised cross-domain image generation. Y Taigman, A Polyak, L Wolf, arXiv:1611.02200arXiv preprintY. Taigman, A. Polyak, and L. Wolf. Unsupervised cross-domain image generation. arXiv preprint arXiv:1611.02200, 2016.\n\nDeepface: Closing the gap to human-level performance in face verification. Y Taigman, M Yang, M Ranzato, L Wolf, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionY. Taigman, M. Yang, M. Ranzato, and L. Wolf. Deepface: Closing the gap to human-level performance in face verification. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1701-1708, 2014.\n\nTransitive transfer learning. B Tan, Y Song, E Zhong, Q Yang, Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data MiningACMB. Tan, Y. Song, E. Zhong, and Q. Yang. Transitive transfer learning. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 1155-1164. ACM, 2015.\n\nDistant domain transfer learning. B Tan, Y Zhang, S J Pan, Q Yang, AAAI. B. Tan, Y. Zhang, S. J. Pan, and Q. Yang. Distant domain transfer learning. In AAAI, pages 2604-2610, 2017.\n\nLarge scale semi-supervised object detection using visual and semantic knowledge transfer. Y Tang, J Wang, B Gao, E Dellandr\u00e9a, R Gaizauskas, L Chen, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionY. Tang, J. Wang, B. Gao, E. Dellandr\u00e9a, R. Gaizauskas, and L. Chen. Large scale semi-supervised object detection using visual and semantic knowledge transfer. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2119-2128, 2016.\n\nAdversarial domain separation and adaptation. J.-C Tsai, J.-T Chien, 2017 IEEE 27th International Workshop on. IEEEMachine Learning for Signal ProcessingJ.-C. Tsai and J.-T. Chien. Adversarial domain separation and adapta- tion. In Machine Learning for Signal Processing (MLSP), 2017 IEEE 27th International Workshop on, pages 1-6. IEEE, 2017.\n\nE Tzeng, C Devin, J Hoffman, C Finn, P Abbeel, S Levine, K Saenko, T Darrell, abs/1511.07111Adapting deep visuomotor representations with weak pairwise constraints. CoRR. E. Tzeng, C. Devin, J. Hoffman, C. Finn, P. Abbeel, S. Levine, K. Saenko, and T. Darrell. Adapting deep visuomotor representations with weak pairwise constraints. CoRR, vol. abs/1511.07111, 2015.\n\nSimultaneous deep transfer across domains and tasks. E Tzeng, J Hoffman, T Darrell, K Saenko, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionE. Tzeng, J. Hoffman, T. Darrell, and K. Saenko. Simultaneous deep transfer across domains and tasks. In Proceedings of the IEEE International Conference on Computer Vision, pages 4068-4076, 2015.\n\nE Tzeng, J Hoffman, K Saenko, T Darrell, arXiv:1702.05464Adversarial discriminative domain adaptation. arXiv preprintE. Tzeng, J. Hoffman, K. Saenko, and T. Darrell. Adversarial discriminative domain adaptation. arXiv preprint arXiv:1702.05464, 2017.\n\nE Tzeng, J Hoffman, N Zhang, K Saenko, T Darrell, arXiv:1412.3474Deep domain confusion: Maximizing for domain invariance. arXiv preprintE. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell. Deep domain confusion: Maximizing for domain invariance. arXiv preprint arXiv:1412.3474, 2014.\n\nImproved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis. D Ulyanov, A Vedaldi, V Lempitsky, arXiv:1701.02096arXiv preprintD. Ulyanov, A. Vedaldi, and V. Lempitsky. Improved texture networks: Maximizing quality and diversity in feed-forward stylization and tex- ture synthesis. arXiv preprint arXiv:1701.02096, 2017.\n\nStacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. P Vincent, H Larochelle, I Lajoie, Y Bengio, P.-A Manzagol, Journal of Machine Learning Research. 11P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research, 11(Dec):3371-3408, 2010.\n\nAdversarial feature augmentation for unsupervised domain adaptation. R Volpi, P Morerio, S Savarese, V Murino, arXiv:1711.08561arXiv preprintR. Volpi, P. Morerio, S. Savarese, and V. Murino. Adversarial feature augmentation for unsupervised domain adaptation. arXiv preprint arXiv:1711.08561, 2017.\n\nHeterogeneous domain adaptation using manifold alignment. C Wang, S Mahadevan, IJCAI proceedings-international joint conference on artificial intelligence. 221541C. Wang and S. Mahadevan. Heterogeneous domain adaptation using manifold alignment. In IJCAI proceedings-international joint conference on artificial intelligence, volume 22, page 1541, 2011.\n\nHigh-quality facial photosketch synthesis using multi-adversarial networks. L Wang, V A Sindagi, V M Patel, arXiv:1710.10182arXiv preprintL. Wang, V. A. Sindagi, and V. M. Patel. High-quality facial photo- sketch synthesis using multi-adversarial networks. arXiv preprint arXiv:1710.10182, 2017.\n\nDeep sketch feature for cross-domain image retrieval. X Wang, X Duan, X Bai, Neurocomputing. 207X. Wang, X. Duan, and X. Bai. Deep sketch feature for cross-domain image retrieval. Neurocomputing, 207:387-397, 2016.\n\nDetecting smiles of young children via deep transfer learning. Y Xia, D Huang, Y Wang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionY. Xia, D. Huang, and Y. Wang. Detecting smiles of young children via deep transfer learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1673-1681, 2017.\n\nLearning deep feature representations with domain guided dropout for person re-identification. T Xiao, H Li, W Ouyang, X Wang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionT. Xiao, H. Li, W. Ouyang, and X. Wang. Learning deep feature repre- sentations with domain guided dropout for person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1249-1258, 2016.\n\nTransfer learning from deep features for remote sensing and poverty mapping. M Xie, N Jean, M Burke, D Lobell, S Ermon, M. Xie, N. Jean, M. Burke, D. Lobell, and S. Ermon. Transfer learning from deep features for remote sensing and poverty mapping. 2015.\n\nMind the class weight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation. H Yan, Y Ding, P Li, Q Wang, Y Xu, W Zuo, arXiv:1705.00609arXiv preprintH. Yan, Y. Ding, P. Li, Q. Wang, Y. Xu, and W. Zuo. Mind the class weight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation. arXiv preprint arXiv:1705.00609, 2017.\n\nUnsupervised dual learning for image-to-image translation. Z Yi, H Zhang, P T Gong, arXiv:1704.02510arXiv preprintZ. Yi, H. Zhang, P. T. Gong, et al. Dualgan: Unsupervised dual learning for image-to-image translation. arXiv preprint arXiv:1704.02510, 2017.\n\nPixel-level domain transfer. D Yoo, N Kim, S Park, A S Paek, I S Kweon, European Conference on Computer Vision. SpringerD. Yoo, N. Kim, S. Park, A. S. Paek, and I. S. Kweon. Pixel-level domain transfer. In European Conference on Computer Vision, pages 517-532. Springer, 2016.\n\nHow transferable are features in deep neural networks?. J Yosinski, J Clune, Y Bengio, H Lipson, Advances in neural information processing systems. J. Yosinski, J. Clune, Y. Bengio, and H. Lipson. How transferable are features in deep neural networks? In Advances in neural information processing systems, pages 3320-3328, 2014.\n\nCentral moment discrepancy (cmd) for domaininvariant representation learning. W Zellinger, T Grubinger, E Lughofer, T Natschl\u00e4ger, S Saminger-Platz, arXiv:1702.08811arXiv preprintW. Zellinger, T. Grubinger, E. Lughofer, T. Natschl\u00e4ger, and S. Saminger-Platz. Central moment discrepancy (cmd) for domain- invariant representation learning. arXiv preprint arXiv:1702.08811, 2017.\n\nStackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. H Zhang, T Xu, H Li, S Zhang, X Huang, X Wang, D Metaxas, IEEE Int. Conf. Comput. Vision (ICCV). H. Zhang, T. Xu, H. Li, S. Zhang, X. Huang, X. Wang, and D. Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked genera- tive adversarial networks. In IEEE Int. Conf. Comput. Vision (ICCV), pages 5907-5915, 2017.\n\nImportance weighted adversarial nets for partial domain adaptation. J Zhang, Z Ding, W Li, P Ogunbona, arXiv:1803.09210arXiv preprintJ. Zhang, Z. Ding, W. Li, and P. Ogunbona. Importance weighted adversarial nets for partial domain adaptation. arXiv preprint arXiv:1803.09210, 2018.\n\nTransfer learning for cross-dataset recognition: A survey. J Zhang, W Li, P Ogunbona, J. Zhang, W. Li, and P. Ogunbona. Transfer learning for cross-dataset recognition: A survey. 2017.\n\nDeep object recognition across domains based on adaptive extreme learning machine. L Zhang, Z He, Y Liu, Neurocomputing. 239L. Zhang, Z. He, and Y. Liu. Deep object recognition across do- mains based on adaptive extreme learning machine. Neurocomputing, 239:194-203, 2017.\n\nDeep transfer network: Unsupervised domain adaptation. X Zhang, F X Yu, S.-F Chang, S Wang, arXiv:1503.00591arXiv preprintX. Zhang, F. X. Yu, S.-F. Chang, and S. Wang. Deep transfer network: Unsupervised domain adaptation. arXiv preprint arXiv:1503.00591, 2015.\n\nCurriculum domain adaptation for semantic segmentation of urban scenes. Y Zhang, P David, B Gong, The IEEE International Conference on Computer Vision (ICCV). 26Y. Zhang, P. David, and B. Gong. Curriculum domain adaptation for semantic segmentation of urban scenes. In The IEEE International Conference on Computer Vision (ICCV), volume 2, page 6, 2017.\n\nDual learning for cross-domain image captioning. W Zhao, W Xu, M Yang, J Ye, Z Zhao, Y Feng, Y Qiao, Proceedings of the 2017 ACM on Conference on Information and Knowledge Management. the 2017 ACM on Conference on Information and Knowledge ManagementACMW. Zhao, W. Xu, M. Yang, J. Ye, Z. Zhao, Y. Feng, and Y. Qiao. Dual learning for cross-domain image captioning. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, pages 29-38. ACM, 2017.\n\nHeterogeneous domain adaptation for multiple classes. J T Zhou, I W Tsang, S J Pan, M Tan, Artificial Intelligence and Statistics. J. T. Zhou, I. W. Tsang, S. J. Pan, and M. Tan. Heterogeneous domain adaptation for multiple classes. In Artificial Intelligence and Statistics, pages 1095-1103, 2014.\n\nUnpaired image-to-image translation using cycle-consistent adversarial networks. J.-Y Zhu, T Park, P Isola, A A Efros, arXiv:1703.10593arXiv preprintJ.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. arXiv preprint arXiv:1703.10593, 2017.\n\nSupervised representation learning: Transfer learning with deep autoencoders. F Zhuang, X Cheng, P Luo, S J Pan, Q He, IJCAI. F. Zhuang, X. Cheng, P. Luo, S. J. Pan, and Q. He. Supervised representation learning: Transfer learning with deep autoencoders. In IJCAI, pages 4119-4125, 2015.\n", "annotations": {"author": "[{\"end\":169,\"start\":43},{\"end\":319,\"start\":170}]", "publisher": null, "author_last_name": "[{\"end\":51,\"start\":47},{\"end\":182,\"start\":178}]", "author_first_name": "[{\"end\":46,\"start\":43},{\"end\":177,\"start\":170}]", "author_affiliation": "[{\"end\":168,\"start\":53},{\"end\":318,\"start\":203}]", "title": "[{\"end\":40,\"start\":1},{\"end\":359,\"start\":320}]", "venue": null, "abstract": "[{\"end\":1705,\"start\":405}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b107\"},\"end\":2378,\"start\":2373},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3157,\"start\":3154},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3163,\"start\":3159},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3189,\"start\":3185},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":3195,\"start\":3191},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3201,\"start\":3197},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":3207,\"start\":3203},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":3621,\"start\":3617},{\"attributes\":{\"ref_id\":\"b112\"},\"end\":3645,\"start\":3640},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3672,\"start\":3668},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":3862,\"start\":3858},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":3912,\"start\":3908},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":3919,\"start\":3914},{\"attributes\":{\"ref_id\":\"b110\"},\"end\":3926,\"start\":3921},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3932,\"start\":3928},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":3966,\"start\":3962},{\"attributes\":{\"ref_id\":\"b122\"},\"end\":4005,\"start\":4000},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4312,\"start\":4308},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":4698,\"start\":4694},{\"attributes\":{\"ref_id\":\"b101\"},\"end\":4705,\"start\":4700},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4711,\"start\":4707},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":4717,\"start\":4713},{\"attributes\":{\"ref_id\":\"b137\"},\"end\":4724,\"start\":4719},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4730,\"start\":4726},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":4747,\"start\":4743},{\"attributes\":{\"ref_id\":\"b101\"},\"end\":4917,\"start\":4912},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":5070,\"start\":5066},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5113,\"start\":5109},{\"attributes\":{\"ref_id\":\"b137\"},\"end\":5238,\"start\":5233},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5456,\"start\":5452},{\"end\":6127,\"start\":6126},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":7340,\"start\":7336},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7346,\"start\":7342},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":8715,\"start\":8711},{\"attributes\":{\"ref_id\":\"b113\"},\"end\":11152,\"start\":11147},{\"attributes\":{\"ref_id\":\"b114\"},\"end\":11159,\"start\":11154},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11772,\"start\":11768},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":11778,\"start\":11774},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":11784,\"start\":11780},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":11790,\"start\":11786},{\"attributes\":{\"ref_id\":\"b138\"},\"end\":11797,\"start\":11792},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":12001,\"start\":11997},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12426,\"start\":12422},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":13175,\"start\":13171},{\"attributes\":{\"ref_id\":\"b118\"},\"end\":13824,\"start\":13819},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":13830,\"start\":13826},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":13836,\"start\":13832},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":13842,\"start\":13838},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":13848,\"start\":13844},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":13986,\"start\":13982},{\"attributes\":{\"ref_id\":\"b139\"},\"end\":13993,\"start\":13988},{\"attributes\":{\"ref_id\":\"b130\"},\"end\":14000,\"start\":13995},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":14006,\"start\":14002},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14040,\"start\":14036},{\"attributes\":{\"ref_id\":\"b118\"},\"end\":14047,\"start\":14042},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":14292,\"start\":14288},{\"attributes\":{\"ref_id\":\"b130\"},\"end\":14299,\"start\":14294},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":14305,\"start\":14301},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":14311,\"start\":14307},{\"attributes\":{\"ref_id\":\"b120\"},\"end\":14318,\"start\":14313},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":14324,\"start\":14320},{\"attributes\":{\"ref_id\":\"b109\"},\"end\":14361,\"start\":14356},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":14367,\"start\":14363},{\"attributes\":{\"ref_id\":\"b144\"},\"end\":14407,\"start\":14402},{\"attributes\":{\"ref_id\":\"b118\"},\"end\":14580,\"start\":14575},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":14586,\"start\":14582},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":14592,\"start\":14588},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":14598,\"start\":14594},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":14609,\"start\":14605},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":14615,\"start\":14611},{\"attributes\":{\"ref_id\":\"b139\"},\"end\":14622,\"start\":14617},{\"attributes\":{\"ref_id\":\"b130\"},\"end\":14629,\"start\":14624},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14635,\"start\":14631},{\"attributes\":{\"ref_id\":\"b118\"},\"end\":14642,\"start\":14637},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":14648,\"start\":14644},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":14673,\"start\":14669},{\"attributes\":{\"ref_id\":\"b130\"},\"end\":14680,\"start\":14675},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":14686,\"start\":14682},{\"attributes\":{\"ref_id\":\"b120\"},\"end\":14698,\"start\":14693},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":14704,\"start\":14700},{\"attributes\":{\"ref_id\":\"b109\"},\"end\":14711,\"start\":14706},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":14717,\"start\":14713},{\"attributes\":{\"ref_id\":\"b144\"},\"end\":14724,\"start\":14719},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":14752,\"start\":14748},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":14758,\"start\":14754},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":14764,\"start\":14760},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":14770,\"start\":14766},{\"attributes\":{\"ref_id\":\"b128\"},\"end\":14777,\"start\":14772},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":14783,\"start\":14779},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14808,\"start\":14804},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":14940,\"start\":14936},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14945,\"start\":14942},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":14951,\"start\":14947},{\"attributes\":{\"ref_id\":\"b119\"},\"end\":14979,\"start\":14974},{\"attributes\":{\"ref_id\":\"b118\"},\"end\":14986,\"start\":14981},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":14992,\"start\":14988},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14998,\"start\":14994},{\"attributes\":{\"ref_id\":\"b117\"},\"end\":15005,\"start\":15000},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15145,\"start\":15142},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":15151,\"start\":15147},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":15157,\"start\":15153},{\"attributes\":{\"ref_id\":\"b144\"},\"end\":15164,\"start\":15159},{\"attributes\":{\"ref_id\":\"b131\"},\"end\":15197,\"start\":15192},{\"attributes\":{\"ref_id\":\"b143\"},\"end\":15204,\"start\":15199},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":15210,\"start\":15206},{\"attributes\":{\"ref_id\":\"b114\"},\"end\":15328,\"start\":15323},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15334,\"start\":15330},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":15461,\"start\":15457},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":15536,\"start\":15532},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":15542,\"start\":15538},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":15548,\"start\":15544},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":15574,\"start\":15570},{\"attributes\":{\"ref_id\":\"b128\"},\"end\":15603,\"start\":15598},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15825,\"start\":15821},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":15908,\"start\":15904},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":16666,\"start\":16662},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":16671,\"start\":16668},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":16677,\"start\":16673},{\"attributes\":{\"ref_id\":\"b119\"},\"end\":17002,\"start\":16997},{\"attributes\":{\"ref_id\":\"b118\"},\"end\":17009,\"start\":17004},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":17015,\"start\":17011},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":17021,\"start\":17017},{\"attributes\":{\"ref_id\":\"b117\"},\"end\":17028,\"start\":17023},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17585,\"start\":17582},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":17591,\"start\":17587},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":17597,\"start\":17593},{\"attributes\":{\"ref_id\":\"b144\"},\"end\":17604,\"start\":17599},{\"attributes\":{\"ref_id\":\"b131\"},\"end\":17839,\"start\":17834},{\"attributes\":{\"ref_id\":\"b143\"},\"end\":17856,\"start\":17851},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":17875,\"start\":17871},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":18379,\"start\":18375},{\"attributes\":{\"ref_id\":\"b129\"},\"end\":18539,\"start\":18534},{\"attributes\":{\"ref_id\":\"b114\"},\"end\":18687,\"start\":18682},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18693,\"start\":18689},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":18857,\"start\":18853},{\"attributes\":{\"ref_id\":\"b133\"},\"end\":20076,\"start\":20071},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":20749,\"start\":20745},{\"attributes\":{\"ref_id\":\"b118\"},\"end\":21472,\"start\":21467},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":21478,\"start\":21474},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":21484,\"start\":21480},{\"attributes\":{\"ref_id\":\"b126\"},\"end\":21491,\"start\":21486},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":21527,\"start\":21523},{\"attributes\":{\"ref_id\":\"b118\"},\"end\":22267,\"start\":22262},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":22575,\"start\":22571},{\"attributes\":{\"ref_id\":\"b118\"},\"end\":22630,\"start\":22625},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":23092,\"start\":23088},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":23219,\"start\":23215},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":23965,\"start\":23961},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":24581,\"start\":24577},{\"attributes\":{\"ref_id\":\"b130\"},\"end\":25199,\"start\":25194},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":25472,\"start\":25468},{\"attributes\":{\"ref_id\":\"b139\"},\"end\":25668,\"start\":25663},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":25999,\"start\":25995},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26557,\"start\":26554},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":26918,\"start\":26914},{\"attributes\":{\"ref_id\":\"b120\"},\"end\":27297,\"start\":27292},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":27318,\"start\":27314},{\"attributes\":{\"ref_id\":\"b120\"},\"end\":27441,\"start\":27436},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":28116,\"start\":28112},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":28574,\"start\":28570},{\"attributes\":{\"ref_id\":\"b139\"},\"end\":28755,\"start\":28750},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":29159,\"start\":29155},{\"attributes\":{\"ref_id\":\"b130\"},\"end\":29236,\"start\":29231},{\"attributes\":{\"ref_id\":\"b108\"},\"end\":29592,\"start\":29587},{\"attributes\":{\"ref_id\":\"b109\"},\"end\":29698,\"start\":29693},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":30104,\"start\":30100},{\"attributes\":{\"ref_id\":\"b134\"},\"end\":30194,\"start\":30189},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":30706,\"start\":30702},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":31193,\"start\":31189},{\"attributes\":{\"ref_id\":\"b105\"},\"end\":31881,\"start\":31876},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":31900,\"start\":31896},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":32085,\"start\":32081},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":32272,\"start\":32268},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":32832,\"start\":32828},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":32837,\"start\":32834},{\"attributes\":{\"ref_id\":\"b121\"},\"end\":33056,\"start\":33051},{\"attributes\":{\"ref_id\":\"b128\"},\"end\":33684,\"start\":33679},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":34102,\"start\":34098},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":34259,\"start\":34255},{\"attributes\":{\"ref_id\":\"b119\"},\"end\":34993,\"start\":34988},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":36015,\"start\":36011},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":36703,\"start\":36699},{\"attributes\":{\"ref_id\":\"b132\"},\"end\":36834,\"start\":36829},{\"attributes\":{\"ref_id\":\"b104\"},\"end\":37126,\"start\":37121},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":37499,\"start\":37496},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":38031,\"start\":38027},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":39016,\"start\":39012},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":39066,\"start\":39062},{\"attributes\":{\"ref_id\":\"b119\"},\"end\":39563,\"start\":39558},{\"attributes\":{\"ref_id\":\"b118\"},\"end\":40712,\"start\":40707},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":41088,\"start\":41085},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":41512,\"start\":41508},{\"attributes\":{\"ref_id\":\"b123\"},\"end\":41692,\"start\":41687},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":42010,\"start\":42007},{\"attributes\":{\"ref_id\":\"b102\"},\"end\":42029,\"start\":42024},{\"attributes\":{\"ref_id\":\"b99\"},\"end\":42236,\"start\":42232},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":42836,\"start\":42833},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":43328,\"start\":43324},{\"attributes\":{\"ref_id\":\"b122\"},\"end\":43428,\"start\":43423},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":43945,\"start\":43941},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":44199,\"start\":44195},{\"end\":45012,\"start\":45009},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":45158,\"start\":45154},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":45196,\"start\":45193},{\"attributes\":{\"ref_id\":\"b116\"},\"end\":45918,\"start\":45913},{\"attributes\":{\"ref_id\":\"b144\"},\"end\":46053,\"start\":46048},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":46385,\"start\":46381},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":46674,\"start\":46670},{\"attributes\":{\"ref_id\":\"b143\"},\"end\":47177,\"start\":47172},{\"attributes\":{\"ref_id\":\"b131\"},\"end\":48213,\"start\":48208},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":48236,\"start\":48232},{\"attributes\":{\"ref_id\":\"b143\"},\"end\":48428,\"start\":48423},{\"attributes\":{\"ref_id\":\"b93\"},\"end\":48440,\"start\":48436},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":48446,\"start\":48442},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":48608,\"start\":48604},{\"attributes\":{\"ref_id\":\"b118\"},\"end\":49123,\"start\":49118},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":49190,\"start\":49186},{\"attributes\":{\"ref_id\":\"b130\"},\"end\":49309,\"start\":49304},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":49418,\"start\":49415},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":49748,\"start\":49744},{\"attributes\":{\"ref_id\":\"b144\"},\"end\":49889,\"start\":49884},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":51080,\"start\":51076},{\"attributes\":{\"ref_id\":\"b124\"},\"end\":51645,\"start\":51640},{\"attributes\":{\"ref_id\":\"b142\"},\"end\":51904,\"start\":51899},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":52127,\"start\":52123},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":52271,\"start\":52267},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":52508,\"start\":52504},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":53458,\"start\":53454},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":53682,\"start\":53678},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":53965,\"start\":53961},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":54098,\"start\":54094},{\"attributes\":{\"ref_id\":\"b105\"},\"end\":54353,\"start\":54348},{\"end\":54647,\"start\":54643},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":54853,\"start\":54849},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":54859,\"start\":54855},{\"attributes\":{\"ref_id\":\"b111\"},\"end\":55119,\"start\":55114},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":55359,\"start\":55355},{\"attributes\":{\"ref_id\":\"b135\"},\"end\":55483,\"start\":55478},{\"attributes\":{\"ref_id\":\"b135\"},\"end\":55949,\"start\":55944},{\"attributes\":{\"ref_id\":\"b143\"},\"end\":56088,\"start\":56083},{\"attributes\":{\"ref_id\":\"b131\"},\"end\":56104,\"start\":56099},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":56123,\"start\":56119},{\"attributes\":{\"ref_id\":\"b143\"},\"end\":56256,\"start\":56251},{\"attributes\":{\"ref_id\":\"b125\"},\"end\":56263,\"start\":56258},{\"attributes\":{\"ref_id\":\"b129\"},\"end\":57054,\"start\":57049},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":57441,\"start\":57437},{\"attributes\":{\"ref_id\":\"b114\"},\"end\":57641,\"start\":57636},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":58011,\"start\":58007},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":58534,\"start\":58530},{\"attributes\":{\"ref_id\":\"b137\"},\"end\":58988,\"start\":58983},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":59620,\"start\":59616},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":59847,\"start\":59843},{\"attributes\":{\"ref_id\":\"b134\"},\"end\":59854,\"start\":59849},{\"attributes\":{\"ref_id\":\"b118\"},\"end\":59865,\"start\":59860},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":60376,\"start\":60372},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":60438,\"start\":60434},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":60460,\"start\":60456},{\"attributes\":{\"ref_id\":\"b120\"},\"end\":60521,\"start\":60516},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":60531,\"start\":60527},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":60541,\"start\":60537},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":60554,\"start\":60550},{\"attributes\":{\"ref_id\":\"b134\"},\"end\":60578,\"start\":60573},{\"attributes\":{\"ref_id\":\"b109\"},\"end\":60689,\"start\":60684},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":60700,\"start\":60696},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":60712,\"start\":60708},{\"attributes\":{\"ref_id\":\"b118\"},\"end\":60758,\"start\":60753},{\"attributes\":{\"ref_id\":\"b119\"},\"end\":60921,\"start\":60916},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":61171,\"start\":61167},{\"attributes\":{\"ref_id\":\"b119\"},\"end\":61186,\"start\":61181},{\"attributes\":{\"ref_id\":\"b106\"},\"end\":61222,\"start\":61217},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":61507,\"start\":61503},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":61890,\"start\":61886},{\"attributes\":{\"ref_id\":\"b107\"},\"end\":62124,\"start\":62119},{\"attributes\":{\"ref_id\":\"b127\"},\"end\":62477,\"start\":62472},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":62494,\"start\":62490},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":62507,\"start\":62503},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":62770,\"start\":62766},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":62788,\"start\":62784},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":62811,\"start\":62807},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":63438,\"start\":63434},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":63653,\"start\":63649},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":63903,\"start\":63899},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":63912,\"start\":63908},{\"attributes\":{\"ref_id\":\"b115\"},\"end\":63931,\"start\":63926},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":64087,\"start\":64083},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":64350,\"start\":64346},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":64801,\"start\":64797},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":64942,\"start\":64938},{\"attributes\":{\"ref_id\":\"b103\"},\"end\":64949,\"start\":64944},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":65098,\"start\":65094},{\"attributes\":{\"ref_id\":\"b140\"},\"end\":65336,\"start\":65331},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":65657,\"start\":65653},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":65907,\"start\":65903},{\"attributes\":{\"ref_id\":\"b100\"},\"end\":66221,\"start\":66216},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":66469,\"start\":66465},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":66944,\"start\":66940},{\"attributes\":{\"ref_id\":\"b117\"},\"end\":67073,\"start\":67068},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":67267,\"start\":67263},{\"attributes\":{\"ref_id\":\"b143\"},\"end\":67284,\"start\":67279},{\"attributes\":{\"ref_id\":\"b131\"},\"end\":67300,\"start\":67295},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":67319,\"start\":67315},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":67461,\"start\":67457},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":67953,\"start\":67949},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":68073,\"start\":68069},{\"attributes\":{\"ref_id\":\"b128\"},\"end\":68685,\"start\":68680},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":68934,\"start\":68930},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":69530,\"start\":69526},{\"attributes\":{\"ref_id\":\"b141\"},\"end\":69869,\"start\":69864},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":72835,\"start\":72832},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":72840,\"start\":72837},{\"attributes\":{\"ref_id\":\"b136\"},\"end\":72847,\"start\":72842},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":73451,\"start\":73447},{\"attributes\":{\"ref_id\":\"b133\"},\"end\":73976,\"start\":73971},{\"attributes\":{\"ref_id\":\"b118\"},\"end\":74056,\"start\":74051},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":74328,\"start\":74324},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":74385,\"start\":74381},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":74446,\"start\":74442},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":74606,\"start\":74602},{\"attributes\":{\"ref_id\":\"b119\"},\"end\":74821,\"start\":74816},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":74914,\"start\":74911},{\"attributes\":{\"ref_id\":\"b119\"},\"end\":75003,\"start\":74998},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":76384,\"start\":76380},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":78231,\"start\":78227},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":78646,\"start\":78642},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":78662,\"start\":78658},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":78989,\"start\":78985}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":73297,\"start\":72929},{\"attributes\":{\"id\":\"fig_1\"},\"end\":73452,\"start\":73298},{\"attributes\":{\"id\":\"fig_2\"},\"end\":73977,\"start\":73453},{\"attributes\":{\"id\":\"fig_3\"},\"end\":74057,\"start\":73978},{\"attributes\":{\"id\":\"fig_4\"},\"end\":74220,\"start\":74058},{\"attributes\":{\"id\":\"fig_5\"},\"end\":74447,\"start\":74221},{\"attributes\":{\"id\":\"fig_6\"},\"end\":74543,\"start\":74448},{\"attributes\":{\"id\":\"fig_7\"},\"end\":74607,\"start\":74544},{\"attributes\":{\"id\":\"fig_8\"},\"end\":74822,\"start\":74608},{\"attributes\":{\"id\":\"fig_9\"},\"end\":74915,\"start\":74823},{\"attributes\":{\"id\":\"fig_10\"},\"end\":75004,\"start\":74916},{\"attributes\":{\"id\":\"fig_11\"},\"end\":75045,\"start\":75005},{\"attributes\":{\"id\":\"fig_12\"},\"end\":75603,\"start\":75046},{\"attributes\":{\"id\":\"fig_13\"},\"end\":76287,\"start\":75604},{\"attributes\":{\"id\":\"fig_14\"},\"end\":76385,\"start\":76288},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":77469,\"start\":76386},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":77578,\"start\":77470},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":77758,\"start\":77579},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":78131,\"start\":77759},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":78306,\"start\":78132},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":79658,\"start\":78307},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":81732,\"start\":79659}]", "paragraph": "[{\"end\":2505,\"start\":1724},{\"end\":3454,\"start\":2507},{\"end\":4625,\"start\":3456},{\"end\":5782,\"start\":4627},{\"end\":6251,\"start\":5784},{\"end\":6630,\"start\":6253},{\"end\":7129,\"start\":6632},{\"end\":7862,\"start\":7177},{\"end\":8351,\"start\":7864},{\"end\":9096,\"start\":8622},{\"end\":9665,\"start\":9098},{\"end\":10491,\"start\":9667},{\"end\":11563,\"start\":10493},{\"end\":12427,\"start\":11609},{\"end\":13031,\"start\":12429},{\"end\":13568,\"start\":13083},{\"end\":14048,\"start\":13570},{\"end\":14439,\"start\":14050},{\"end\":15618,\"start\":14441},{\"end\":15826,\"start\":15620},{\"end\":16316,\"start\":15828},{\"end\":16678,\"start\":16318},{\"end\":17029,\"start\":16680},{\"end\":17366,\"start\":17031},{\"end\":17605,\"start\":17368},{\"end\":17877,\"start\":17607},{\"end\":18455,\"start\":17931},{\"end\":18540,\"start\":18457},{\"end\":18694,\"start\":18542},{\"end\":18858,\"start\":18696},{\"end\":19987,\"start\":18893},{\"end\":20838,\"start\":19989},{\"end\":21171,\"start\":20860},{\"end\":23369,\"start\":21173},{\"end\":24494,\"start\":23498},{\"end\":24988,\"start\":24564},{\"end\":26156,\"start\":24990},{\"end\":26444,\"start\":26158},{\"end\":26622,\"start\":26446},{\"end\":26877,\"start\":26699},{\"end\":27193,\"start\":26879},{\"end\":27661,\"start\":27265},{\"end\":29430,\"start\":27703},{\"end\":29783,\"start\":29432},{\"end\":29960,\"start\":29818},{\"end\":30417,\"start\":29962},{\"end\":30931,\"start\":30661},{\"end\":31170,\"start\":30933},{\"end\":31442,\"start\":31172},{\"end\":31709,\"start\":31493},{\"end\":31997,\"start\":31774},{\"end\":32615,\"start\":32061},{\"end\":33261,\"start\":32658},{\"end\":33936,\"start\":33263},{\"end\":34741,\"start\":33969},{\"end\":35986,\"start\":34863},{\"end\":38116,\"start\":35988},{\"end\":38361,\"start\":38182},{\"end\":39982,\"start\":38363},{\"end\":41877,\"start\":40275},{\"end\":42500,\"start\":41879},{\"end\":42764,\"start\":42502},{\"end\":43304,\"start\":42801},{\"end\":44127,\"start\":43306},{\"end\":44655,\"start\":44129},{\"end\":46032,\"start\":44720},{\"end\":46592,\"start\":46034},{\"end\":47159,\"start\":46594},{\"end\":47871,\"start\":47161},{\"end\":48998,\"start\":48047},{\"end\":49997,\"start\":49000},{\"end\":50782,\"start\":50036},{\"end\":51797,\"start\":50784},{\"end\":52829,\"start\":51799},{\"end\":53183,\"start\":52831},{\"end\":54167,\"start\":53185},{\"end\":54952,\"start\":54169},{\"end\":56458,\"start\":54954},{\"end\":56628,\"start\":56494},{\"end\":56935,\"start\":56659},{\"end\":57216,\"start\":56937},{\"end\":57623,\"start\":57249},{\"end\":57652,\"start\":57625},{\"end\":59037,\"start\":57853},{\"end\":59838,\"start\":59065},{\"end\":60911,\"start\":59840},{\"end\":61223,\"start\":60913},{\"end\":62642,\"start\":61247},{\"end\":64455,\"start\":62666},{\"end\":64993,\"start\":64484},{\"end\":66791,\"start\":64995},{\"end\":67320,\"start\":66793},{\"end\":68274,\"start\":67322},{\"end\":69173,\"start\":68306},{\"end\":69977,\"start\":69197},{\"end\":70400,\"start\":69997},{\"end\":70816,\"start\":70402},{\"end\":71457,\"start\":70818},{\"end\":71968,\"start\":71459},{\"end\":72482,\"start\":71970},{\"end\":72928,\"start\":72484}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8576,\"start\":8352},{\"attributes\":{\"id\":\"formula_1\"},\"end\":23408,\"start\":23370},{\"attributes\":{\"id\":\"formula_2\"},\"end\":23497,\"start\":23408},{\"attributes\":{\"id\":\"formula_3\"},\"end\":24563,\"start\":24495},{\"attributes\":{\"id\":\"formula_4\"},\"end\":26698,\"start\":26623},{\"attributes\":{\"id\":\"formula_5\"},\"end\":27264,\"start\":27194},{\"attributes\":{\"id\":\"formula_6\"},\"end\":27702,\"start\":27662},{\"attributes\":{\"id\":\"formula_7\"},\"end\":29817,\"start\":29784},{\"attributes\":{\"id\":\"formula_8\"},\"end\":30660,\"start\":30418},{\"attributes\":{\"id\":\"formula_9\"},\"end\":31492,\"start\":31443},{\"attributes\":{\"id\":\"formula_10\"},\"end\":31773,\"start\":31710},{\"attributes\":{\"id\":\"formula_11\"},\"end\":32060,\"start\":31998},{\"attributes\":{\"id\":\"formula_12\"},\"end\":32657,\"start\":32616},{\"attributes\":{\"id\":\"formula_13\"},\"end\":33968,\"start\":33937},{\"attributes\":{\"id\":\"formula_14\"},\"end\":34780,\"start\":34742},{\"attributes\":{\"id\":\"formula_15\"},\"end\":34862,\"start\":34780},{\"attributes\":{\"id\":\"formula_16\"},\"end\":38181,\"start\":38117},{\"attributes\":{\"id\":\"formula_17\"},\"end\":40274,\"start\":39983},{\"attributes\":{\"id\":\"formula_18\"},\"end\":44719,\"start\":44656},{\"attributes\":{\"id\":\"formula_19\"},\"end\":48046,\"start\":47872},{\"attributes\":{\"id\":\"formula_20\"},\"end\":57815,\"start\":57653}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":6928,\"start\":6908},{\"end\":13184,\"start\":13177},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":19418,\"start\":19409},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":20836,\"start\":20828},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":21707,\"start\":21579},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":59837,\"start\":59830}]", "section_header": "[{\"end\":1722,\"start\":1707},{\"end\":7144,\"start\":7132},{\"end\":7175,\"start\":7147},{\"end\":8620,\"start\":8578},{\"end\":11607,\"start\":11566},{\"end\":13081,\"start\":13034},{\"end\":17929,\"start\":17880},{\"end\":18891,\"start\":18861},{\"end\":20858,\"start\":20841},{\"end\":42799,\"start\":42767},{\"end\":50034,\"start\":50000},{\"end\":56492,\"start\":56461},{\"end\":56657,\"start\":56631},{\"end\":57247,\"start\":57219},{\"end\":57851,\"start\":57817},{\"end\":59063,\"start\":59040},{\"end\":61245,\"start\":61226},{\"end\":62664,\"start\":62645},{\"end\":64482,\"start\":64458},{\"end\":68304,\"start\":68277},{\"end\":69195,\"start\":69176},{\"end\":69995,\"start\":69980},{\"end\":72938,\"start\":72930},{\"end\":73307,\"start\":73299},{\"end\":73462,\"start\":73454},{\"end\":73987,\"start\":73979},{\"end\":74230,\"start\":74222},{\"end\":74450,\"start\":74449},{\"end\":74553,\"start\":74545},{\"end\":74617,\"start\":74609},{\"end\":74833,\"start\":74824},{\"end\":74926,\"start\":74917},{\"end\":75015,\"start\":75006},{\"end\":76298,\"start\":76289},{\"end\":77488,\"start\":77471},{\"end\":77598,\"start\":77580},{\"end\":77779,\"start\":77760},{\"end\":78146,\"start\":78133},{\"end\":79678,\"start\":79660}]", "table": "[{\"end\":77469,\"start\":76507},{\"end\":77578,\"start\":77520},{\"end\":77758,\"start\":77633},{\"end\":78131,\"start\":77838},{\"end\":78306,\"start\":78232},{\"end\":79658,\"start\":79461},{\"end\":81732,\"start\":80015}]", "figure_caption": "[{\"end\":73297,\"start\":72940},{\"end\":73452,\"start\":73309},{\"end\":73977,\"start\":73464},{\"end\":74057,\"start\":73989},{\"end\":74220,\"start\":74060},{\"end\":74447,\"start\":74232},{\"end\":74543,\"start\":74451},{\"end\":74607,\"start\":74555},{\"end\":74822,\"start\":74619},{\"end\":74915,\"start\":74836},{\"end\":75004,\"start\":74929},{\"end\":75045,\"start\":75018},{\"end\":75603,\"start\":75048},{\"end\":76287,\"start\":75606},{\"end\":76385,\"start\":76301},{\"end\":76507,\"start\":76388},{\"end\":77520,\"start\":77490},{\"end\":77633,\"start\":77601},{\"end\":77838,\"start\":77783},{\"end\":78232,\"start\":78149},{\"end\":79461,\"start\":78309},{\"end\":80015,\"start\":79680}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2709,\"start\":2703},{\"end\":6805,\"start\":6799},{\"end\":9664,\"start\":9658},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11468,\"start\":11462},{\"end\":36673,\"start\":36667},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38951,\"start\":38944},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":45084,\"start\":45077},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":55915,\"start\":55908},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":58490,\"start\":58483},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":59966,\"start\":59956},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":60997,\"start\":60988},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":66386,\"start\":66379}]", "bib_author_first_name": "[{\"end\":81920,\"start\":81919},{\"end\":81932,\"start\":81931},{\"end\":81944,\"start\":81943},{\"end\":82135,\"start\":82134},{\"end\":82349,\"start\":82348},{\"end\":82351,\"start\":82350},{\"end\":82364,\"start\":82363},{\"end\":82375,\"start\":82374},{\"end\":82377,\"start\":82376},{\"end\":82389,\"start\":82385},{\"end\":82400,\"start\":82399},{\"end\":82413,\"start\":82412},{\"end\":82415,\"start\":82414},{\"end\":82727,\"start\":82726},{\"end\":82740,\"start\":82739},{\"end\":82753,\"start\":82752},{\"end\":82762,\"start\":82761},{\"end\":82771,\"start\":82770},{\"end\":83027,\"start\":83026},{\"end\":83040,\"start\":83039},{\"end\":83054,\"start\":83053},{\"end\":83067,\"start\":83066},{\"end\":83079,\"start\":83078},{\"end\":83410,\"start\":83409},{\"end\":83422,\"start\":83421},{\"end\":83745,\"start\":83744},{\"end\":83747,\"start\":83746},{\"end\":83756,\"start\":83755},{\"end\":84032,\"start\":84031},{\"end\":84039,\"start\":84038},{\"end\":84047,\"start\":84046},{\"end\":84055,\"start\":84054},{\"end\":84057,\"start\":84056},{\"end\":84289,\"start\":84288},{\"end\":84291,\"start\":84290},{\"end\":84303,\"start\":84302},{\"end\":84312,\"start\":84311},{\"end\":84322,\"start\":84321},{\"end\":84331,\"start\":84330},{\"end\":84333,\"start\":84332},{\"end\":84608,\"start\":84607},{\"end\":84616,\"start\":84615},{\"end\":84622,\"start\":84621},{\"end\":84636,\"start\":84635},{\"end\":84892,\"start\":84888},{\"end\":84903,\"start\":84899},{\"end\":84914,\"start\":84910},{\"end\":84927,\"start\":84923},{\"end\":84934,\"start\":84933},{\"end\":84940,\"start\":84939},{\"end\":85296,\"start\":85292},{\"end\":85551,\"start\":85550},{\"end\":85559,\"start\":85558},{\"end\":85565,\"start\":85564},{\"end\":85578,\"start\":85577},{\"end\":85585,\"start\":85584},{\"end\":85862,\"start\":85861},{\"end\":85870,\"start\":85869},{\"end\":85876,\"start\":85875},{\"end\":86145,\"start\":86141},{\"end\":86156,\"start\":86152},{\"end\":86167,\"start\":86163},{\"end\":86178,\"start\":86174},{\"end\":86189,\"start\":86185},{\"end\":86191,\"start\":86190},{\"end\":86199,\"start\":86198},{\"end\":86499,\"start\":86498},{\"end\":86509,\"start\":86508},{\"end\":86525,\"start\":86524},{\"end\":86855,\"start\":86854},{\"end\":86862,\"start\":86861},{\"end\":86874,\"start\":86873},{\"end\":86885,\"start\":86884},{\"end\":86896,\"start\":86895},{\"end\":87228,\"start\":87224},{\"end\":87235,\"start\":87234},{\"end\":87250,\"start\":87249},{\"end\":87252,\"start\":87251},{\"end\":87691,\"start\":87690},{\"end\":87896,\"start\":87895},{\"end\":87903,\"start\":87902},{\"end\":87905,\"start\":87904},{\"end\":88175,\"start\":88174},{\"end\":88183,\"start\":88182},{\"end\":88192,\"start\":88191},{\"end\":88200,\"start\":88199},{\"end\":88208,\"start\":88207},{\"end\":88214,\"start\":88213},{\"end\":88544,\"start\":88543},{\"end\":88555,\"start\":88554},{\"end\":88562,\"start\":88561},{\"end\":88573,\"start\":88572},{\"end\":88584,\"start\":88583},{\"end\":88593,\"start\":88592},{\"end\":88602,\"start\":88601},{\"end\":88959,\"start\":88958},{\"end\":88967,\"start\":88966},{\"end\":88973,\"start\":88972},{\"end\":89225,\"start\":89224},{\"end\":89234,\"start\":89233},{\"end\":89245,\"start\":89244},{\"end\":89547,\"start\":89546},{\"end\":89556,\"start\":89555},{\"end\":89814,\"start\":89813},{\"end\":89823,\"start\":89822},{\"end\":89835,\"start\":89834},{\"end\":89845,\"start\":89844},{\"end\":89856,\"start\":89855},{\"end\":89870,\"start\":89869},{\"end\":89884,\"start\":89883},{\"end\":89896,\"start\":89895},{\"end\":90224,\"start\":90223},{\"end\":90226,\"start\":90225},{\"end\":90235,\"start\":90234},{\"end\":90237,\"start\":90236},{\"end\":90246,\"start\":90245},{\"end\":90700,\"start\":90699},{\"end\":90706,\"start\":90705},{\"end\":90897,\"start\":90896},{\"end\":90906,\"start\":90905},{\"end\":90917,\"start\":90916},{\"end\":91287,\"start\":91286},{\"end\":91299,\"start\":91298},{\"end\":91301,\"start\":91300},{\"end\":91570,\"start\":91569},{\"end\":91581,\"start\":91580},{\"end\":91600,\"start\":91599},{\"end\":91609,\"start\":91608},{\"end\":92028,\"start\":92027},{\"end\":92039,\"start\":92038},{\"end\":92041,\"start\":92040},{\"end\":92051,\"start\":92050},{\"end\":92411,\"start\":92410},{\"end\":92422,\"start\":92421},{\"end\":92424,\"start\":92423},{\"end\":92434,\"start\":92433},{\"end\":92443,\"start\":92442},{\"end\":92455,\"start\":92454},{\"end\":92738,\"start\":92737},{\"end\":93074,\"start\":93073},{\"end\":93086,\"start\":93085},{\"end\":93097,\"start\":93096},{\"end\":93108,\"start\":93107},{\"end\":93581,\"start\":93580},{\"end\":93591,\"start\":93590},{\"end\":93601,\"start\":93600},{\"end\":94106,\"start\":94105},{\"end\":94114,\"start\":94113},{\"end\":94125,\"start\":94124},{\"end\":94463,\"start\":94462},{\"end\":94471,\"start\":94470},{\"end\":94478,\"start\":94477},{\"end\":94485,\"start\":94484},{\"end\":94806,\"start\":94805},{\"end\":94820,\"start\":94819},{\"end\":94837,\"start\":94836},{\"end\":94846,\"start\":94845},{\"end\":94852,\"start\":94851},{\"end\":94868,\"start\":94867},{\"end\":94877,\"start\":94876},{\"end\":94890,\"start\":94889},{\"end\":95232,\"start\":95231},{\"end\":95243,\"start\":95242},{\"end\":95249,\"start\":95248},{\"end\":95580,\"start\":95579},{\"end\":95589,\"start\":95588},{\"end\":95600,\"start\":95599},{\"end\":95974,\"start\":95973},{\"end\":95986,\"start\":95985},{\"end\":95996,\"start\":95995},{\"end\":96011,\"start\":96010},{\"end\":96282,\"start\":96281},{\"end\":96288,\"start\":96287},{\"end\":96295,\"start\":96294},{\"end\":96302,\"start\":96301},{\"end\":96310,\"start\":96309},{\"end\":96316,\"start\":96315},{\"end\":96326,\"start\":96322},{\"end\":96605,\"start\":96604},{\"end\":96611,\"start\":96610},{\"end\":96620,\"start\":96619},{\"end\":96627,\"start\":96626},{\"end\":97008,\"start\":97007},{\"end\":97018,\"start\":97017},{\"end\":97029,\"start\":97028},{\"end\":97237,\"start\":97236},{\"end\":97239,\"start\":97238},{\"end\":97249,\"start\":97248},{\"end\":97264,\"start\":97260},{\"end\":97476,\"start\":97475},{\"end\":97487,\"start\":97486},{\"end\":97501,\"start\":97500},{\"end\":97503,\"start\":97502},{\"end\":97512,\"start\":97511},{\"end\":97518,\"start\":97517},{\"end\":97529,\"start\":97528},{\"end\":97541,\"start\":97540},{\"end\":97552,\"start\":97551},{\"end\":97883,\"start\":97882},{\"end\":97894,\"start\":97893},{\"end\":97903,\"start\":97902},{\"end\":97912,\"start\":97911},{\"end\":97926,\"start\":97925},{\"end\":98285,\"start\":98284},{\"end\":98296,\"start\":98295},{\"end\":98305,\"start\":98304},{\"end\":98316,\"start\":98315},{\"end\":98323,\"start\":98322},{\"end\":98333,\"start\":98332},{\"end\":98542,\"start\":98541},{\"end\":98553,\"start\":98552},{\"end\":98561,\"start\":98560},{\"end\":98567,\"start\":98566},{\"end\":98935,\"start\":98934},{\"end\":98943,\"start\":98942},{\"end\":98949,\"start\":98948},{\"end\":98956,\"start\":98955},{\"end\":98958,\"start\":98957},{\"end\":99268,\"start\":99267},{\"end\":99276,\"start\":99275},{\"end\":99282,\"start\":99281},{\"end\":99289,\"start\":99288},{\"end\":99709,\"start\":99708},{\"end\":99715,\"start\":99714},{\"end\":99724,\"start\":99720},{\"end\":100036,\"start\":100035},{\"end\":100045,\"start\":100044},{\"end\":100392,\"start\":100391},{\"end\":100401,\"start\":100400},{\"end\":100411,\"start\":100410},{\"end\":100423,\"start\":100422},{\"end\":100735,\"start\":100734},{\"end\":100744,\"start\":100743},{\"end\":101057,\"start\":101056},{\"end\":101069,\"start\":101065},{\"end\":101076,\"start\":101075},{\"end\":101084,\"start\":101083},{\"end\":101086,\"start\":101085},{\"end\":101339,\"start\":101338},{\"end\":101346,\"start\":101345},{\"end\":101354,\"start\":101353},{\"end\":101753,\"start\":101752},{\"end\":101760,\"start\":101759},{\"end\":101767,\"start\":101766},{\"end\":101774,\"start\":101773},{\"end\":101781,\"start\":101780},{\"end\":102072,\"start\":102071},{\"end\":102086,\"start\":102085},{\"end\":102088,\"start\":102087},{\"end\":102372,\"start\":102371},{\"end\":102388,\"start\":102387},{\"end\":102399,\"start\":102398},{\"end\":102412,\"start\":102411},{\"end\":102417,\"start\":102413},{\"end\":102798,\"start\":102797},{\"end\":102812,\"start\":102811},{\"end\":102825,\"start\":102824},{\"end\":102827,\"start\":102826},{\"end\":103166,\"start\":103165},{\"end\":103175,\"start\":103174},{\"end\":103185,\"start\":103184},{\"end\":103582,\"start\":103581},{\"end\":103584,\"start\":103583},{\"end\":103595,\"start\":103594},{\"end\":103607,\"start\":103606},{\"end\":104015,\"start\":104014},{\"end\":104021,\"start\":104020},{\"end\":104309,\"start\":104308},{\"end\":104315,\"start\":104314},{\"end\":104326,\"start\":104322},{\"end\":104334,\"start\":104333},{\"end\":104336,\"start\":104335},{\"end\":104649,\"start\":104648},{\"end\":104655,\"start\":104654},{\"end\":104666,\"start\":104665},{\"end\":104995,\"start\":104994},{\"end\":105001,\"start\":105000},{\"end\":105009,\"start\":105008},{\"end\":105016,\"start\":105015},{\"end\":105265,\"start\":105264},{\"end\":105271,\"start\":105270},{\"end\":105279,\"start\":105278},{\"end\":105286,\"start\":105285},{\"end\":105293,\"start\":105292},{\"end\":105524,\"start\":105520},{\"end\":105531,\"start\":105530},{\"end\":105802,\"start\":105801},{\"end\":105809,\"start\":105808},{\"end\":105817,\"start\":105816},{\"end\":105824,\"start\":105823},{\"end\":105832,\"start\":105831},{\"end\":105839,\"start\":105838},{\"end\":105841,\"start\":105840},{\"end\":106113,\"start\":106112},{\"end\":106120,\"start\":106119},{\"end\":106128,\"start\":106127},{\"end\":106134,\"start\":106133},{\"end\":106451,\"start\":106450},{\"end\":106459,\"start\":106458},{\"end\":106466,\"start\":106465},{\"end\":106474,\"start\":106473},{\"end\":106758,\"start\":106757},{\"end\":106766,\"start\":106765},{\"end\":106774,\"start\":106773},{\"end\":106776,\"start\":106775},{\"end\":107011,\"start\":107010},{\"end\":107019,\"start\":107018},{\"end\":107026,\"start\":107025},{\"end\":107034,\"start\":107033},{\"end\":107036,\"start\":107035},{\"end\":107347,\"start\":107346},{\"end\":107353,\"start\":107352},{\"end\":107362,\"start\":107361},{\"end\":107369,\"start\":107368},{\"end\":107376,\"start\":107375},{\"end\":107384,\"start\":107383},{\"end\":107392,\"start\":107391},{\"end\":107772,\"start\":107771},{\"end\":107782,\"start\":107781},{\"end\":107791,\"start\":107790},{\"end\":108090,\"start\":108089},{\"end\":108101,\"start\":108100},{\"end\":108110,\"start\":108109},{\"end\":108124,\"start\":108123},{\"end\":108419,\"start\":108418},{\"end\":108430,\"start\":108429},{\"end\":108444,\"start\":108443},{\"end\":108446,\"start\":108445},{\"end\":108457,\"start\":108456},{\"end\":108798,\"start\":108797},{\"end\":108800,\"start\":108799},{\"end\":108810,\"start\":108809},{\"end\":108812,\"start\":108811},{\"end\":108818,\"start\":108817},{\"end\":108820,\"start\":108819},{\"end\":108829,\"start\":108828},{\"end\":109130,\"start\":109129},{\"end\":109141,\"start\":109140},{\"end\":109154,\"start\":109153},{\"end\":109352,\"start\":109351},{\"end\":109354,\"start\":109353},{\"end\":109361,\"start\":109360},{\"end\":109363,\"start\":109362},{\"end\":109372,\"start\":109371},{\"end\":109374,\"start\":109373},{\"end\":109382,\"start\":109381},{\"end\":109622,\"start\":109621},{\"end\":109624,\"start\":109623},{\"end\":109631,\"start\":109630},{\"end\":109883,\"start\":109882},{\"end\":109885,\"start\":109884},{\"end\":109894,\"start\":109893},{\"end\":109905,\"start\":109904},{\"end\":109911,\"start\":109910},{\"end\":110123,\"start\":110119},{\"end\":110131,\"start\":110130},{\"end\":110137,\"start\":110136},{\"end\":110385,\"start\":110384},{\"end\":110393,\"start\":110392},{\"end\":110404,\"start\":110403},{\"end\":110406,\"start\":110405},{\"end\":110416,\"start\":110415},{\"end\":110793,\"start\":110792},{\"end\":110801,\"start\":110800},{\"end\":111050,\"start\":111049},{\"end\":111057,\"start\":111056},{\"end\":111059,\"start\":111058},{\"end\":111073,\"start\":111072},{\"end\":111324,\"start\":111320},{\"end\":111335,\"start\":111334},{\"end\":111344,\"start\":111343},{\"end\":111523,\"start\":111522},{\"end\":111531,\"start\":111530},{\"end\":111540,\"start\":111539},{\"end\":111547,\"start\":111546},{\"end\":111561,\"start\":111560},{\"end\":111572,\"start\":111571},{\"end\":111891,\"start\":111890},{\"end\":111898,\"start\":111897},{\"end\":111904,\"start\":111903},{\"end\":111916,\"start\":111915},{\"end\":112243,\"start\":112242},{\"end\":112253,\"start\":112252},{\"end\":112673,\"start\":112672},{\"end\":112688,\"start\":112687},{\"end\":112699,\"start\":112698},{\"end\":113086,\"start\":113085},{\"end\":113094,\"start\":113093},{\"end\":113492,\"start\":113491},{\"end\":113505,\"start\":113504},{\"end\":113517,\"start\":113516},{\"end\":113684,\"start\":113683},{\"end\":113686,\"start\":113685},{\"end\":113694,\"start\":113693},{\"end\":113696,\"start\":113695},{\"end\":113710,\"start\":113709},{\"end\":113724,\"start\":113723},{\"end\":113733,\"start\":113732},{\"end\":113748,\"start\":113747},{\"end\":113763,\"start\":113762},{\"end\":113774,\"start\":113773},{\"end\":114076,\"start\":114075},{\"end\":114086,\"start\":114085},{\"end\":114095,\"start\":114094},{\"end\":114104,\"start\":114103},{\"end\":114393,\"start\":114392},{\"end\":114402,\"start\":114401},{\"end\":114412,\"start\":114411},{\"end\":114655,\"start\":114654},{\"end\":114664,\"start\":114663},{\"end\":114676,\"start\":114675},{\"end\":114686,\"start\":114685},{\"end\":114965,\"start\":114964},{\"end\":114985,\"start\":114984},{\"end\":114995,\"start\":114994},{\"end\":115003,\"start\":115002},{\"end\":115005,\"start\":115004},{\"end\":115012,\"start\":115011},{\"end\":115238,\"start\":115237},{\"end\":115246,\"start\":115245},{\"end\":115253,\"start\":115252},{\"end\":115561,\"start\":115560},{\"end\":115569,\"start\":115568},{\"end\":115575,\"start\":115574},{\"end\":115584,\"start\":115583},{\"end\":115792,\"start\":115791},{\"end\":115803,\"start\":115802},{\"end\":116121,\"start\":116120},{\"end\":116136,\"start\":116135},{\"end\":116147,\"start\":116146},{\"end\":116156,\"start\":116155},{\"end\":116168,\"start\":116167},{\"end\":116176,\"start\":116175},{\"end\":116490,\"start\":116489},{\"end\":116500,\"start\":116496},{\"end\":116506,\"start\":116505},{\"end\":116514,\"start\":116513},{\"end\":116935,\"start\":116934},{\"end\":116947,\"start\":116946},{\"end\":117199,\"start\":117198},{\"end\":117207,\"start\":117206},{\"end\":117214,\"start\":117213},{\"end\":117223,\"start\":117222},{\"end\":117232,\"start\":117228},{\"end\":117240,\"start\":117239},{\"end\":117510,\"start\":117509},{\"end\":117517,\"start\":117516},{\"end\":117525,\"start\":117524},{\"end\":117719,\"start\":117718},{\"end\":117726,\"start\":117725},{\"end\":117969,\"start\":117968},{\"end\":117980,\"start\":117979},{\"end\":117987,\"start\":117986},{\"end\":117994,\"start\":117993},{\"end\":118006,\"start\":118005},{\"end\":118014,\"start\":118013},{\"end\":118026,\"start\":118025},{\"end\":118035,\"start\":118034},{\"end\":118048,\"start\":118047},{\"end\":118485,\"start\":118484},{\"end\":118496,\"start\":118495},{\"end\":118506,\"start\":118505},{\"end\":118739,\"start\":118738},{\"end\":118750,\"start\":118749},{\"end\":118758,\"start\":118757},{\"end\":118769,\"start\":118768},{\"end\":119175,\"start\":119174},{\"end\":119182,\"start\":119181},{\"end\":119190,\"start\":119189},{\"end\":119199,\"start\":119198},{\"end\":119629,\"start\":119628},{\"end\":119636,\"start\":119635},{\"end\":119645,\"start\":119644},{\"end\":119647,\"start\":119646},{\"end\":119654,\"start\":119653},{\"end\":119868,\"start\":119867},{\"end\":119876,\"start\":119875},{\"end\":119884,\"start\":119883},{\"end\":119891,\"start\":119890},{\"end\":119905,\"start\":119904},{\"end\":119919,\"start\":119918},{\"end\":120383,\"start\":120379},{\"end\":120394,\"start\":120390},{\"end\":120679,\"start\":120678},{\"end\":120688,\"start\":120687},{\"end\":120697,\"start\":120696},{\"end\":120708,\"start\":120707},{\"end\":120716,\"start\":120715},{\"end\":120726,\"start\":120725},{\"end\":120736,\"start\":120735},{\"end\":120746,\"start\":120745},{\"end\":121100,\"start\":121099},{\"end\":121109,\"start\":121108},{\"end\":121120,\"start\":121119},{\"end\":121131,\"start\":121130},{\"end\":121460,\"start\":121459},{\"end\":121469,\"start\":121468},{\"end\":121480,\"start\":121479},{\"end\":121490,\"start\":121489},{\"end\":121712,\"start\":121711},{\"end\":121721,\"start\":121720},{\"end\":121732,\"start\":121731},{\"end\":121741,\"start\":121740},{\"end\":121751,\"start\":121750},{\"end\":122114,\"start\":122113},{\"end\":122125,\"start\":122124},{\"end\":122136,\"start\":122135},{\"end\":122490,\"start\":122489},{\"end\":122501,\"start\":122500},{\"end\":122515,\"start\":122514},{\"end\":122525,\"start\":122524},{\"end\":122538,\"start\":122534},{\"end\":122908,\"start\":122907},{\"end\":122917,\"start\":122916},{\"end\":122928,\"start\":122927},{\"end\":122940,\"start\":122939},{\"end\":123197,\"start\":123196},{\"end\":123205,\"start\":123204},{\"end\":123570,\"start\":123569},{\"end\":123578,\"start\":123577},{\"end\":123580,\"start\":123579},{\"end\":123591,\"start\":123590},{\"end\":123593,\"start\":123592},{\"end\":123845,\"start\":123844},{\"end\":123853,\"start\":123852},{\"end\":123861,\"start\":123860},{\"end\":124070,\"start\":124069},{\"end\":124077,\"start\":124076},{\"end\":124086,\"start\":124085},{\"end\":124530,\"start\":124529},{\"end\":124538,\"start\":124537},{\"end\":124544,\"start\":124543},{\"end\":124554,\"start\":124553},{\"end\":125023,\"start\":125022},{\"end\":125030,\"start\":125029},{\"end\":125038,\"start\":125037},{\"end\":125047,\"start\":125046},{\"end\":125057,\"start\":125056},{\"end\":125300,\"start\":125299},{\"end\":125307,\"start\":125306},{\"end\":125315,\"start\":125314},{\"end\":125321,\"start\":125320},{\"end\":125329,\"start\":125328},{\"end\":125335,\"start\":125334},{\"end\":125621,\"start\":125620},{\"end\":125627,\"start\":125626},{\"end\":125636,\"start\":125635},{\"end\":125638,\"start\":125637},{\"end\":125849,\"start\":125848},{\"end\":125856,\"start\":125855},{\"end\":125863,\"start\":125862},{\"end\":125871,\"start\":125870},{\"end\":125873,\"start\":125872},{\"end\":125881,\"start\":125880},{\"end\":125883,\"start\":125882},{\"end\":126154,\"start\":126153},{\"end\":126166,\"start\":126165},{\"end\":126175,\"start\":126174},{\"end\":126185,\"start\":126184},{\"end\":126506,\"start\":126505},{\"end\":126519,\"start\":126518},{\"end\":126532,\"start\":126531},{\"end\":126544,\"start\":126543},{\"end\":126559,\"start\":126558},{\"end\":126903,\"start\":126902},{\"end\":126912,\"start\":126911},{\"end\":126918,\"start\":126917},{\"end\":126924,\"start\":126923},{\"end\":126933,\"start\":126932},{\"end\":126942,\"start\":126941},{\"end\":126950,\"start\":126949},{\"end\":127301,\"start\":127300},{\"end\":127310,\"start\":127309},{\"end\":127318,\"start\":127317},{\"end\":127324,\"start\":127323},{\"end\":127576,\"start\":127575},{\"end\":127585,\"start\":127584},{\"end\":127591,\"start\":127590},{\"end\":127786,\"start\":127785},{\"end\":127795,\"start\":127794},{\"end\":127801,\"start\":127800},{\"end\":128032,\"start\":128031},{\"end\":128041,\"start\":128040},{\"end\":128043,\"start\":128042},{\"end\":128052,\"start\":128048},{\"end\":128061,\"start\":128060},{\"end\":128312,\"start\":128311},{\"end\":128321,\"start\":128320},{\"end\":128330,\"start\":128329},{\"end\":128644,\"start\":128643},{\"end\":128652,\"start\":128651},{\"end\":128658,\"start\":128657},{\"end\":128666,\"start\":128665},{\"end\":128672,\"start\":128671},{\"end\":128680,\"start\":128679},{\"end\":128688,\"start\":128687},{\"end\":129125,\"start\":129124},{\"end\":129127,\"start\":129126},{\"end\":129135,\"start\":129134},{\"end\":129137,\"start\":129136},{\"end\":129146,\"start\":129145},{\"end\":129148,\"start\":129147},{\"end\":129155,\"start\":129154},{\"end\":129455,\"start\":129451},{\"end\":129462,\"start\":129461},{\"end\":129470,\"start\":129469},{\"end\":129479,\"start\":129478},{\"end\":129481,\"start\":129480},{\"end\":129766,\"start\":129765},{\"end\":129776,\"start\":129775},{\"end\":129785,\"start\":129784},{\"end\":129792,\"start\":129791},{\"end\":129794,\"start\":129793},{\"end\":129801,\"start\":129800}]", "bib_author_last_name": "[{\"end\":81929,\"start\":81921},{\"end\":81941,\"start\":81933},{\"end\":81951,\"start\":81945},{\"end\":82142,\"start\":82136},{\"end\":82361,\"start\":82352},{\"end\":82372,\"start\":82365},{\"end\":82383,\"start\":82378},{\"end\":82397,\"start\":82390},{\"end\":82410,\"start\":82401},{\"end\":82421,\"start\":82416},{\"end\":82737,\"start\":82728},{\"end\":82750,\"start\":82741},{\"end\":82759,\"start\":82754},{\"end\":82768,\"start\":82763},{\"end\":82780,\"start\":82772},{\"end\":83037,\"start\":83028},{\"end\":83051,\"start\":83041},{\"end\":83064,\"start\":83055},{\"end\":83076,\"start\":83068},{\"end\":83085,\"start\":83080},{\"end\":83419,\"start\":83411},{\"end\":83433,\"start\":83423},{\"end\":83753,\"start\":83748},{\"end\":83761,\"start\":83757},{\"end\":84036,\"start\":84033},{\"end\":84044,\"start\":84040},{\"end\":84052,\"start\":84048},{\"end\":84064,\"start\":84058},{\"end\":84300,\"start\":84292},{\"end\":84309,\"start\":84304},{\"end\":84319,\"start\":84313},{\"end\":84328,\"start\":84323},{\"end\":84338,\"start\":84334},{\"end\":84613,\"start\":84609},{\"end\":84619,\"start\":84617},{\"end\":84633,\"start\":84623},{\"end\":84640,\"start\":84637},{\"end\":84897,\"start\":84893},{\"end\":84908,\"start\":84904},{\"end\":84921,\"start\":84915},{\"end\":84931,\"start\":84928},{\"end\":84937,\"start\":84935},{\"end\":84944,\"start\":84941},{\"end\":85556,\"start\":85552},{\"end\":85562,\"start\":85560},{\"end\":85575,\"start\":85566},{\"end\":85582,\"start\":85579},{\"end\":85594,\"start\":85586},{\"end\":85867,\"start\":85863},{\"end\":85873,\"start\":85871},{\"end\":85885,\"start\":85877},{\"end\":86150,\"start\":86146},{\"end\":86161,\"start\":86157},{\"end\":86172,\"start\":86168},{\"end\":86183,\"start\":86179},{\"end\":86196,\"start\":86192},{\"end\":86203,\"start\":86200},{\"end\":86506,\"start\":86500},{\"end\":86522,\"start\":86510},{\"end\":86533,\"start\":86526},{\"end\":86859,\"start\":86856},{\"end\":86871,\"start\":86863},{\"end\":86882,\"start\":86875},{\"end\":86893,\"start\":86886},{\"end\":86904,\"start\":86897},{\"end\":87232,\"start\":87229},{\"end\":87247,\"start\":87236},{\"end\":87257,\"start\":87253},{\"end\":87698,\"start\":87692},{\"end\":87900,\"start\":87897},{\"end\":87918,\"start\":87906},{\"end\":88180,\"start\":88176},{\"end\":88189,\"start\":88184},{\"end\":88197,\"start\":88193},{\"end\":88205,\"start\":88201},{\"end\":88211,\"start\":88209},{\"end\":88219,\"start\":88215},{\"end\":88552,\"start\":88545},{\"end\":88559,\"start\":88556},{\"end\":88570,\"start\":88563},{\"end\":88581,\"start\":88574},{\"end\":88590,\"start\":88585},{\"end\":88599,\"start\":88594},{\"end\":88610,\"start\":88603},{\"end\":88964,\"start\":88960},{\"end\":88970,\"start\":88968},{\"end\":88979,\"start\":88974},{\"end\":89231,\"start\":89226},{\"end\":89242,\"start\":89235},{\"end\":89252,\"start\":89246},{\"end\":89553,\"start\":89548},{\"end\":89566,\"start\":89557},{\"end\":89820,\"start\":89815},{\"end\":89832,\"start\":89824},{\"end\":89842,\"start\":89836},{\"end\":89853,\"start\":89846},{\"end\":89867,\"start\":89857},{\"end\":89881,\"start\":89871},{\"end\":89893,\"start\":89885},{\"end\":89906,\"start\":89897},{\"end\":90232,\"start\":90227},{\"end\":90243,\"start\":90238},{\"end\":90253,\"start\":90247},{\"end\":90703,\"start\":90701},{\"end\":90709,\"start\":90707},{\"end\":90903,\"start\":90898},{\"end\":90914,\"start\":90907},{\"end\":90925,\"start\":90918},{\"end\":91296,\"start\":91288},{\"end\":91310,\"start\":91302},{\"end\":91578,\"start\":91571},{\"end\":91597,\"start\":91582},{\"end\":91606,\"start\":91601},{\"end\":91618,\"start\":91610},{\"end\":92036,\"start\":92029},{\"end\":92048,\"start\":92042},{\"end\":92057,\"start\":92052},{\"end\":92419,\"start\":92412},{\"end\":92431,\"start\":92425},{\"end\":92440,\"start\":92435},{\"end\":92452,\"start\":92444},{\"end\":92458,\"start\":92456},{\"end\":92747,\"start\":92739},{\"end\":93083,\"start\":93075},{\"end\":93094,\"start\":93087},{\"end\":93105,\"start\":93098},{\"end\":93114,\"start\":93109},{\"end\":93588,\"start\":93582},{\"end\":93598,\"start\":93592},{\"end\":93608,\"start\":93602},{\"end\":94111,\"start\":94107},{\"end\":94122,\"start\":94115},{\"end\":94129,\"start\":94126},{\"end\":94468,\"start\":94464},{\"end\":94475,\"start\":94472},{\"end\":94482,\"start\":94479},{\"end\":94493,\"start\":94486},{\"end\":94817,\"start\":94807},{\"end\":94834,\"start\":94821},{\"end\":94843,\"start\":94838},{\"end\":94849,\"start\":94847},{\"end\":94865,\"start\":94853},{\"end\":94874,\"start\":94869},{\"end\":94887,\"start\":94878},{\"end\":94897,\"start\":94891},{\"end\":95240,\"start\":95233},{\"end\":95246,\"start\":95244},{\"end\":95259,\"start\":95250},{\"end\":95586,\"start\":95581},{\"end\":95597,\"start\":95590},{\"end\":95606,\"start\":95601},{\"end\":95983,\"start\":95975},{\"end\":95993,\"start\":95987},{\"end\":96008,\"start\":95997},{\"end\":96019,\"start\":96012},{\"end\":96285,\"start\":96283},{\"end\":96292,\"start\":96289},{\"end\":96299,\"start\":96296},{\"end\":96307,\"start\":96303},{\"end\":96313,\"start\":96311},{\"end\":96320,\"start\":96317},{\"end\":96329,\"start\":96327},{\"end\":96608,\"start\":96606},{\"end\":96617,\"start\":96612},{\"end\":96624,\"start\":96621},{\"end\":96631,\"start\":96628},{\"end\":97015,\"start\":97009},{\"end\":97026,\"start\":97019},{\"end\":97034,\"start\":97030},{\"end\":97246,\"start\":97240},{\"end\":97258,\"start\":97250},{\"end\":97268,\"start\":97265},{\"end\":97484,\"start\":97477},{\"end\":97498,\"start\":97488},{\"end\":97509,\"start\":97504},{\"end\":97515,\"start\":97513},{\"end\":97526,\"start\":97519},{\"end\":97538,\"start\":97530},{\"end\":97549,\"start\":97542},{\"end\":97559,\"start\":97553},{\"end\":97891,\"start\":97884},{\"end\":97900,\"start\":97895},{\"end\":97909,\"start\":97904},{\"end\":97923,\"start\":97913},{\"end\":97934,\"start\":97927},{\"end\":98293,\"start\":98286},{\"end\":98302,\"start\":98297},{\"end\":98313,\"start\":98306},{\"end\":98320,\"start\":98317},{\"end\":98330,\"start\":98324},{\"end\":98341,\"start\":98334},{\"end\":98550,\"start\":98543},{\"end\":98558,\"start\":98554},{\"end\":98564,\"start\":98562},{\"end\":98575,\"start\":98568},{\"end\":98940,\"start\":98936},{\"end\":98946,\"start\":98944},{\"end\":98953,\"start\":98950},{\"end\":98963,\"start\":98959},{\"end\":99273,\"start\":99269},{\"end\":99279,\"start\":99277},{\"end\":99286,\"start\":99283},{\"end\":99293,\"start\":99290},{\"end\":99712,\"start\":99710},{\"end\":99718,\"start\":99716},{\"end\":99728,\"start\":99725},{\"end\":100042,\"start\":100037},{\"end\":100054,\"start\":100046},{\"end\":100398,\"start\":100393},{\"end\":100408,\"start\":100402},{\"end\":100420,\"start\":100412},{\"end\":100430,\"start\":100424},{\"end\":100741,\"start\":100736},{\"end\":100752,\"start\":100745},{\"end\":101063,\"start\":101058},{\"end\":101073,\"start\":101070},{\"end\":101081,\"start\":101077},{\"end\":101092,\"start\":101087},{\"end\":101343,\"start\":101340},{\"end\":101351,\"start\":101347},{\"end\":101359,\"start\":101355},{\"end\":101757,\"start\":101754},{\"end\":101764,\"start\":101761},{\"end\":101771,\"start\":101768},{\"end\":101778,\"start\":101775},{\"end\":101785,\"start\":101782},{\"end\":102083,\"start\":102073},{\"end\":102096,\"start\":102089},{\"end\":102385,\"start\":102373},{\"end\":102396,\"start\":102389},{\"end\":102409,\"start\":102400},{\"end\":102422,\"start\":102418},{\"end\":102809,\"start\":102799},{\"end\":102822,\"start\":102813},{\"end\":102834,\"start\":102828},{\"end\":103172,\"start\":103167},{\"end\":103182,\"start\":103176},{\"end\":103193,\"start\":103186},{\"end\":103592,\"start\":103585},{\"end\":103604,\"start\":103596},{\"end\":103617,\"start\":103608},{\"end\":104018,\"start\":104016},{\"end\":104026,\"start\":104022},{\"end\":104312,\"start\":104310},{\"end\":104320,\"start\":104316},{\"end\":104331,\"start\":104327},{\"end\":104347,\"start\":104337},{\"end\":104652,\"start\":104650},{\"end\":104663,\"start\":104656},{\"end\":104672,\"start\":104667},{\"end\":104998,\"start\":104996},{\"end\":105006,\"start\":105002},{\"end\":105013,\"start\":105010},{\"end\":105020,\"start\":105017},{\"end\":105268,\"start\":105266},{\"end\":105276,\"start\":105272},{\"end\":105283,\"start\":105280},{\"end\":105290,\"start\":105287},{\"end\":105297,\"start\":105294},{\"end\":105528,\"start\":105525},{\"end\":105537,\"start\":105532},{\"end\":105806,\"start\":105803},{\"end\":105814,\"start\":105810},{\"end\":105821,\"start\":105818},{\"end\":105829,\"start\":105825},{\"end\":105836,\"start\":105833},{\"end\":105849,\"start\":105842},{\"end\":106117,\"start\":106114},{\"end\":106125,\"start\":106121},{\"end\":106131,\"start\":106129},{\"end\":106138,\"start\":106135},{\"end\":106456,\"start\":106452},{\"end\":106463,\"start\":106460},{\"end\":106471,\"start\":106467},{\"end\":106481,\"start\":106475},{\"end\":106763,\"start\":106759},{\"end\":106771,\"start\":106767},{\"end\":106783,\"start\":106777},{\"end\":107016,\"start\":107012},{\"end\":107023,\"start\":107020},{\"end\":107031,\"start\":107027},{\"end\":107043,\"start\":107037},{\"end\":107350,\"start\":107348},{\"end\":107359,\"start\":107354},{\"end\":107366,\"start\":107363},{\"end\":107373,\"start\":107370},{\"end\":107381,\"start\":107377},{\"end\":107389,\"start\":107385},{\"end\":107400,\"start\":107393},{\"end\":107408,\"start\":107402},{\"end\":107779,\"start\":107773},{\"end\":107788,\"start\":107783},{\"end\":107797,\"start\":107792},{\"end\":108098,\"start\":108091},{\"end\":108107,\"start\":108102},{\"end\":108121,\"start\":108111},{\"end\":108132,\"start\":108125},{\"end\":108427,\"start\":108420},{\"end\":108441,\"start\":108431},{\"end\":108454,\"start\":108447},{\"end\":108465,\"start\":108458},{\"end\":108807,\"start\":108801},{\"end\":108815,\"start\":108813},{\"end\":108826,\"start\":108821},{\"end\":108839,\"start\":108830},{\"end\":109138,\"start\":109131},{\"end\":109151,\"start\":109142},{\"end\":109160,\"start\":109155},{\"end\":109358,\"start\":109355},{\"end\":109369,\"start\":109364},{\"end\":109379,\"start\":109375},{\"end\":109387,\"start\":109383},{\"end\":109628,\"start\":109625},{\"end\":109636,\"start\":109632},{\"end\":109891,\"start\":109886},{\"end\":109902,\"start\":109895},{\"end\":109908,\"start\":109906},{\"end\":109921,\"start\":109912},{\"end\":110128,\"start\":110124},{\"end\":110134,\"start\":110132},{\"end\":110143,\"start\":110138},{\"end\":110390,\"start\":110386},{\"end\":110401,\"start\":110394},{\"end\":110413,\"start\":110407},{\"end\":110423,\"start\":110417},{\"end\":110798,\"start\":110794},{\"end\":110808,\"start\":110802},{\"end\":111054,\"start\":111051},{\"end\":111070,\"start\":111060},{\"end\":111084,\"start\":111074},{\"end\":111332,\"start\":111325},{\"end\":111341,\"start\":111336},{\"end\":111352,\"start\":111345},{\"end\":111528,\"start\":111524},{\"end\":111537,\"start\":111532},{\"end\":111544,\"start\":111541},{\"end\":111558,\"start\":111548},{\"end\":111569,\"start\":111562},{\"end\":111576,\"start\":111573},{\"end\":111895,\"start\":111892},{\"end\":111901,\"start\":111899},{\"end\":111913,\"start\":111905},{\"end\":111920,\"start\":111917},{\"end\":112250,\"start\":112244},{\"end\":112258,\"start\":112254},{\"end\":112685,\"start\":112674},{\"end\":112696,\"start\":112689},{\"end\":112704,\"start\":112700},{\"end\":113091,\"start\":113087},{\"end\":113107,\"start\":113095},{\"end\":113502,\"start\":113493},{\"end\":113514,\"start\":113506},{\"end\":113521,\"start\":113518},{\"end\":113691,\"start\":113687},{\"end\":113707,\"start\":113697},{\"end\":113721,\"start\":113711},{\"end\":113730,\"start\":113725},{\"end\":113745,\"start\":113734},{\"end\":113760,\"start\":113749},{\"end\":113771,\"start\":113764},{\"end\":113782,\"start\":113775},{\"end\":114083,\"start\":114077},{\"end\":114092,\"start\":114087},{\"end\":114101,\"start\":114096},{\"end\":114112,\"start\":114105},{\"end\":114399,\"start\":114394},{\"end\":114409,\"start\":114403},{\"end\":114419,\"start\":114413},{\"end\":114661,\"start\":114656},{\"end\":114673,\"start\":114665},{\"end\":114683,\"start\":114677},{\"end\":114693,\"start\":114687},{\"end\":114982,\"start\":114966},{\"end\":114992,\"start\":114986},{\"end\":115000,\"start\":114996},{\"end\":115009,\"start\":115006},{\"end\":115022,\"start\":115013},{\"end\":115243,\"start\":115239},{\"end\":115250,\"start\":115247},{\"end\":115256,\"start\":115254},{\"end\":115566,\"start\":115562},{\"end\":115572,\"start\":115570},{\"end\":115581,\"start\":115576},{\"end\":115587,\"start\":115585},{\"end\":115800,\"start\":115793},{\"end\":115809,\"start\":115804},{\"end\":116133,\"start\":116122},{\"end\":116144,\"start\":116137},{\"end\":116153,\"start\":116148},{\"end\":116165,\"start\":116157},{\"end\":116173,\"start\":116169},{\"end\":116181,\"start\":116177},{\"end\":116494,\"start\":116491},{\"end\":116503,\"start\":116501},{\"end\":116511,\"start\":116507},{\"end\":116519,\"start\":116515},{\"end\":116944,\"start\":116936},{\"end\":116957,\"start\":116948},{\"end\":117204,\"start\":117200},{\"end\":117211,\"start\":117208},{\"end\":117220,\"start\":117215},{\"end\":117226,\"start\":117224},{\"end\":117237,\"start\":117233},{\"end\":117251,\"start\":117241},{\"end\":117514,\"start\":117511},{\"end\":117522,\"start\":117518},{\"end\":117532,\"start\":117526},{\"end\":117723,\"start\":117720},{\"end\":117733,\"start\":117727},{\"end\":117977,\"start\":117970},{\"end\":117984,\"start\":117981},{\"end\":117991,\"start\":117988},{\"end\":118003,\"start\":117995},{\"end\":118011,\"start\":118007},{\"end\":118023,\"start\":118015},{\"end\":118032,\"start\":118027},{\"end\":118045,\"start\":118036},{\"end\":118059,\"start\":118049},{\"end\":118493,\"start\":118486},{\"end\":118503,\"start\":118497},{\"end\":118511,\"start\":118507},{\"end\":118747,\"start\":118740},{\"end\":118755,\"start\":118751},{\"end\":118766,\"start\":118759},{\"end\":118774,\"start\":118770},{\"end\":119179,\"start\":119176},{\"end\":119187,\"start\":119183},{\"end\":119196,\"start\":119191},{\"end\":119204,\"start\":119200},{\"end\":119633,\"start\":119630},{\"end\":119642,\"start\":119637},{\"end\":119651,\"start\":119648},{\"end\":119659,\"start\":119655},{\"end\":119873,\"start\":119869},{\"end\":119881,\"start\":119877},{\"end\":119888,\"start\":119885},{\"end\":119902,\"start\":119892},{\"end\":119916,\"start\":119906},{\"end\":119924,\"start\":119920},{\"end\":120388,\"start\":120384},{\"end\":120400,\"start\":120395},{\"end\":120685,\"start\":120680},{\"end\":120694,\"start\":120689},{\"end\":120705,\"start\":120698},{\"end\":120713,\"start\":120709},{\"end\":120723,\"start\":120717},{\"end\":120733,\"start\":120727},{\"end\":120743,\"start\":120737},{\"end\":120754,\"start\":120747},{\"end\":121106,\"start\":121101},{\"end\":121117,\"start\":121110},{\"end\":121128,\"start\":121121},{\"end\":121138,\"start\":121132},{\"end\":121466,\"start\":121461},{\"end\":121477,\"start\":121470},{\"end\":121487,\"start\":121481},{\"end\":121498,\"start\":121491},{\"end\":121718,\"start\":121713},{\"end\":121729,\"start\":121722},{\"end\":121738,\"start\":121733},{\"end\":121748,\"start\":121742},{\"end\":121759,\"start\":121752},{\"end\":122122,\"start\":122115},{\"end\":122133,\"start\":122126},{\"end\":122146,\"start\":122137},{\"end\":122498,\"start\":122491},{\"end\":122512,\"start\":122502},{\"end\":122522,\"start\":122516},{\"end\":122532,\"start\":122526},{\"end\":122547,\"start\":122539},{\"end\":122914,\"start\":122909},{\"end\":122925,\"start\":122918},{\"end\":122937,\"start\":122929},{\"end\":122947,\"start\":122941},{\"end\":123202,\"start\":123198},{\"end\":123215,\"start\":123206},{\"end\":123575,\"start\":123571},{\"end\":123588,\"start\":123581},{\"end\":123599,\"start\":123594},{\"end\":123850,\"start\":123846},{\"end\":123858,\"start\":123854},{\"end\":123865,\"start\":123862},{\"end\":124074,\"start\":124071},{\"end\":124083,\"start\":124078},{\"end\":124091,\"start\":124087},{\"end\":124535,\"start\":124531},{\"end\":124541,\"start\":124539},{\"end\":124551,\"start\":124545},{\"end\":124559,\"start\":124555},{\"end\":125027,\"start\":125024},{\"end\":125035,\"start\":125031},{\"end\":125044,\"start\":125039},{\"end\":125054,\"start\":125048},{\"end\":125063,\"start\":125058},{\"end\":125304,\"start\":125301},{\"end\":125312,\"start\":125308},{\"end\":125318,\"start\":125316},{\"end\":125326,\"start\":125322},{\"end\":125332,\"start\":125330},{\"end\":125339,\"start\":125336},{\"end\":125624,\"start\":125622},{\"end\":125633,\"start\":125628},{\"end\":125643,\"start\":125639},{\"end\":125853,\"start\":125850},{\"end\":125860,\"start\":125857},{\"end\":125868,\"start\":125864},{\"end\":125878,\"start\":125874},{\"end\":125889,\"start\":125884},{\"end\":126163,\"start\":126155},{\"end\":126172,\"start\":126167},{\"end\":126182,\"start\":126176},{\"end\":126192,\"start\":126186},{\"end\":126516,\"start\":126507},{\"end\":126529,\"start\":126520},{\"end\":126541,\"start\":126533},{\"end\":126556,\"start\":126545},{\"end\":126574,\"start\":126560},{\"end\":126909,\"start\":126904},{\"end\":126915,\"start\":126913},{\"end\":126921,\"start\":126919},{\"end\":126930,\"start\":126925},{\"end\":126939,\"start\":126934},{\"end\":126947,\"start\":126943},{\"end\":126958,\"start\":126951},{\"end\":127307,\"start\":127302},{\"end\":127315,\"start\":127311},{\"end\":127321,\"start\":127319},{\"end\":127333,\"start\":127325},{\"end\":127582,\"start\":127577},{\"end\":127588,\"start\":127586},{\"end\":127600,\"start\":127592},{\"end\":127792,\"start\":127787},{\"end\":127798,\"start\":127796},{\"end\":127805,\"start\":127802},{\"end\":128038,\"start\":128033},{\"end\":128046,\"start\":128044},{\"end\":128058,\"start\":128053},{\"end\":128066,\"start\":128062},{\"end\":128318,\"start\":128313},{\"end\":128327,\"start\":128322},{\"end\":128335,\"start\":128331},{\"end\":128649,\"start\":128645},{\"end\":128655,\"start\":128653},{\"end\":128663,\"start\":128659},{\"end\":128669,\"start\":128667},{\"end\":128677,\"start\":128673},{\"end\":128685,\"start\":128681},{\"end\":128693,\"start\":128689},{\"end\":129132,\"start\":129128},{\"end\":129143,\"start\":129138},{\"end\":129152,\"start\":129149},{\"end\":129159,\"start\":129156},{\"end\":129459,\"start\":129456},{\"end\":129467,\"start\":129463},{\"end\":129476,\"start\":129471},{\"end\":129487,\"start\":129482},{\"end\":129773,\"start\":129767},{\"end\":129782,\"start\":129777},{\"end\":129789,\"start\":129786},{\"end\":129798,\"start\":129795},{\"end\":129804,\"start\":129802}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1701.07875\",\"id\":\"b0\"},\"end\":82096,\"start\":81917},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":207178999},\"end\":82271,\"start\":82098},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":13341746},\"end\":82643,\"start\":82273},{\"attributes\":{\"doi\":\"arXiv:1612.05424\",\"id\":\"b3\"},\"end\":82996,\"start\":82645},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":2127515},\"end\":83310,\"start\":82998},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":14739721},\"end\":83714,\"start\":83312},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":19598463},\"end\":83966,\"start\":83716},{\"attributes\":{\"doi\":\"arXiv:1707.07901\",\"id\":\"b7\"},\"end\":84241,\"start\":83968},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":4377230},\"end\":84546,\"start\":84243},{\"attributes\":{\"doi\":\"arXiv:1206.4683\",\"id\":\"b9\"},\"end\":84810,\"start\":84548},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":10319651},\"end\":85231,\"start\":84812},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":34369127},\"end\":85485,\"start\":85233},{\"attributes\":{\"doi\":\"arXiv:1803.03243\",\"id\":\"b12\"},\"end\":85782,\"start\":85487},{\"attributes\":{\"doi\":\"arXiv:1711.11556\",\"id\":\"b13\"},\"end\":86067,\"start\":85784},{\"attributes\":{\"doi\":\"arXiv:1704.08509\",\"id\":\"b14\"},\"end\":86420,\"start\":86069},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":11690995},\"end\":86786,\"start\":86422},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":10247639},\"end\":87148,\"start\":86788},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":6600989},\"end\":87621,\"start\":87150},{\"attributes\":{\"doi\":\"arXiv:1702.05374\",\"id\":\"b18\"},\"end\":87848,\"start\":87623},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":10092092},\"end\":88056,\"start\":87850},{\"attributes\":{\"doi\":\"arXiv:1711.07027\",\"id\":\"b20\"},\"end\":88462,\"start\":88058},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6161478},\"end\":88886,\"start\":88464},{\"attributes\":{\"doi\":\"arXiv:1206.4660\",\"id\":\"b22\"},\"end\":89147,\"start\":88888},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":2255738},\"end\":89493,\"start\":89149},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":6755881},\"end\":89763,\"start\":89495},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":2871880},\"end\":90163,\"start\":89765},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":206593710},\"end\":90599,\"start\":90165},{\"attributes\":{\"doi\":\"arXiv:1702.08690\",\"id\":\"b27\"},\"end\":90894,\"start\":90601},{\"attributes\":{\"doi\":\"arXiv:1709.02476\",\"id\":\"b28\"},\"end\":91191,\"start\":90896},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":2426747},\"end\":91492,\"start\":91193},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":12825123},\"end\":91969,\"start\":91494},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":17674695},\"end\":92328,\"start\":91971},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":52459},\"end\":92723,\"start\":92330},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":206770307},\"end\":92989,\"start\":92725},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":215827080},\"end\":93492,\"start\":92991},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":18235792},\"end\":93979,\"start\":93494},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":16169580},\"end\":94403,\"start\":93981},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":6742009},\"end\":94774,\"start\":94405},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":1033682},\"end\":95161,\"start\":94776},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":10337178},\"end\":95526,\"start\":95163},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":6832420},\"end\":95940,\"start\":95528},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":20186541},\"end\":96240,\"start\":95942},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":5758868},\"end\":96556,\"start\":96242},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":206594692},\"end\":96959,\"start\":96558},{\"attributes\":{\"doi\":\"arXiv:1503.02531\",\"id\":\"b44\"},\"end\":97186,\"start\":96961},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":2309950},\"end\":97425,\"start\":97188},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":14260314},\"end\":97837,\"start\":97427},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":13300851},\"end\":98221,\"start\":97839},{\"attributes\":{\"doi\":\"arXiv:1312.6204\",\"id\":\"b48\"},\"end\":98539,\"start\":98223},{\"attributes\":{\"doi\":\"arXiv:1612.02649\",\"id\":\"b49\"},\"end\":98839,\"start\":98541},{\"attributes\":{\"doi\":\"arXiv:1702.04069\",\"id\":\"b50\"},\"end\":99166,\"start\":98841},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":225718},\"end\":99675,\"start\":99168},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":1010081},\"end\":100033,\"start\":99677},{\"attributes\":{\"doi\":\"arXiv:1703.06868\",\"id\":\"b53\"},\"end\":100302,\"start\":100035},{\"attributes\":{\"doi\":\"arXiv:1803.11365\",\"id\":\"b54\"},\"end\":100638,\"start\":100304},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":5808102},\"end\":100988,\"start\":100640},{\"attributes\":{\"doi\":\"arXiv:1611.07004\",\"id\":\"b56\"},\"end\":101275,\"start\":100990},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":11590103},\"end\":101669,\"start\":101277},{\"attributes\":{\"doi\":\"arXiv:1703.05192\",\"id\":\"b58\"},\"end\":101982,\"start\":101671},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":333277},\"end\":102339,\"start\":101984},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":10738438},\"end\":102730,\"start\":102341},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":195908774},\"end\":103075,\"start\":102732},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":7419723},\"end\":103501,\"start\":103077},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":10301835},\"end\":103924,\"start\":103503},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":18781152},\"end\":104256,\"start\":103926},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":6037691},\"end\":104507,\"start\":104258},{\"attributes\":{\"id\":\"b66\"},\"end\":104609,\"start\":104509},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":536962},\"end\":104992,\"start\":104611},{\"attributes\":{\"doi\":\"arXiv:1701.01036\",\"id\":\"b68\"},\"end\":105198,\"start\":104994},{\"attributes\":{\"doi\":\"arXiv:1603.04779\",\"id\":\"b69\"},\"end\":105477,\"start\":105200},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":10627900},\"end\":105729,\"start\":105479},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":207116476},\"end\":106033,\"start\":105731},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":206743645},\"end\":106386,\"start\":106035},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":556999},\"end\":106700,\"start\":106388},{\"attributes\":{\"doi\":\"arXiv:1605.06636\",\"id\":\"b74\"},\"end\":106944,\"start\":106702},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":745350},\"end\":107278,\"start\":106946},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":177942},\"end\":107693,\"start\":107280},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":21114926},\"end\":108047,\"start\":107695},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":116505},\"end\":108354,\"start\":108049},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":3172259},\"end\":108728,\"start\":108356},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":555537},\"end\":109068,\"start\":108730},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":15510465},\"end\":109298,\"start\":109070},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":788838},\"end\":109588,\"start\":109300},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":740063},\"end\":109825,\"start\":109590},{\"attributes\":{\"id\":\"b84\",\"matched_paper_id\":918513},\"end\":110115,\"start\":109827},{\"attributes\":{\"doi\":\"arXiv:1707.01922\",\"id\":\"b85\"},\"end\":110314,\"start\":110117},{\"attributes\":{\"id\":\"b86\",\"matched_paper_id\":7879025},\"end\":110708,\"start\":110316},{\"attributes\":{\"doi\":\"arXiv:1701.05524\",\"id\":\"b87\"},\"end\":110985,\"start\":110710},{\"attributes\":{\"doi\":\"arXiv:1507.05578\",\"id\":\"b88\"},\"end\":111261,\"start\":110987},{\"attributes\":{\"doi\":\"arXiv:1705.08045\",\"id\":\"b89\"},\"end\":111520,\"start\":111263},{\"attributes\":{\"doi\":\"arXiv:1605.05396\",\"id\":\"b90\"},\"end\":111808,\"start\":111522},{\"attributes\":{\"id\":\"b91\",\"matched_paper_id\":10328909},\"end\":112165,\"start\":111810},{\"attributes\":{\"id\":\"b92\",\"matched_paper_id\":1595578},\"end\":112605,\"start\":112167},{\"attributes\":{\"id\":\"b93\",\"matched_paper_id\":3719281},\"end\":113029,\"start\":112607},{\"attributes\":{\"id\":\"b94\",\"matched_paper_id\":9166592},\"end\":113438,\"start\":113031},{\"attributes\":{\"doi\":\"arXiv:1603.06432\",\"id\":\"b95\"},\"end\":113681,\"start\":113440},{\"attributes\":{\"doi\":\"arXiv:1606.04671\",\"id\":\"b96\"},\"end\":114025,\"start\":113683},{\"attributes\":{\"id\":\"b97\",\"matched_paper_id\":7534823},\"end\":114330,\"start\":114027},{\"attributes\":{\"doi\":\"arXiv:1702.08400\",\"id\":\"b98\"},\"end\":114585,\"start\":114332},{\"attributes\":{\"doi\":\"arXiv:1712.02560\",\"id\":\"b99\"},\"end\":114881,\"start\":114587},{\"attributes\":{\"id\":\"b100\"},\"end\":115180,\"start\":114883},{\"attributes\":{\"id\":\"b101\",\"matched_paper_id\":16081723},\"end\":115483,\"start\":115182},{\"attributes\":{\"id\":\"b102\"},\"end\":115706,\"start\":115485},{\"attributes\":{\"id\":\"b103\",\"matched_paper_id\":13716040},\"end\":116040,\"start\":115708},{\"attributes\":{\"doi\":\"arXiv:1612.07828\",\"id\":\"b104\"},\"end\":116402,\"start\":116042},{\"attributes\":{\"id\":\"b105\",\"matched_paper_id\":207223936},\"end\":116864,\"start\":116404},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b106\"},\"end\":117123,\"start\":116866},{\"attributes\":{\"doi\":\"arXiv:1708.02191\",\"id\":\"b107\"},\"end\":117459,\"start\":117125},{\"attributes\":{\"id\":\"b108\",\"matched_paper_id\":16439870},\"end\":117654,\"start\":117461},{\"attributes\":{\"id\":\"b109\",\"matched_paper_id\":12453047},\"end\":117934,\"start\":117656},{\"attributes\":{\"id\":\"b110\",\"matched_paper_id\":206592484},\"end\":118438,\"start\":117936},{\"attributes\":{\"doi\":\"arXiv:1611.02200\",\"id\":\"b111\"},\"end\":118661,\"start\":118440},{\"attributes\":{\"id\":\"b112\",\"matched_paper_id\":2814088},\"end\":119142,\"start\":118663},{\"attributes\":{\"id\":\"b113\",\"matched_paper_id\":12308197},\"end\":119592,\"start\":119144},{\"attributes\":{\"id\":\"b114\",\"matched_paper_id\":29160099},\"end\":119774,\"start\":119594},{\"attributes\":{\"id\":\"b115\",\"matched_paper_id\":216143736},\"end\":120331,\"start\":119776},{\"attributes\":{\"id\":\"b116\",\"matched_paper_id\":37197494},\"end\":120676,\"start\":120333},{\"attributes\":{\"doi\":\"abs/1511.07111\",\"id\":\"b117\"},\"end\":121044,\"start\":120678},{\"attributes\":{\"id\":\"b118\",\"matched_paper_id\":2655115},\"end\":121457,\"start\":121046},{\"attributes\":{\"doi\":\"arXiv:1702.05464\",\"id\":\"b119\"},\"end\":121709,\"start\":121459},{\"attributes\":{\"doi\":\"arXiv:1412.3474\",\"id\":\"b120\"},\"end\":122000,\"start\":121711},{\"attributes\":{\"doi\":\"arXiv:1701.02096\",\"id\":\"b121\"},\"end\":122371,\"start\":122002},{\"attributes\":{\"id\":\"b122\",\"matched_paper_id\":17804904},\"end\":122836,\"start\":122373},{\"attributes\":{\"doi\":\"arXiv:1711.08561\",\"id\":\"b123\"},\"end\":123136,\"start\":122838},{\"attributes\":{\"id\":\"b124\",\"matched_paper_id\":10360019},\"end\":123491,\"start\":123138},{\"attributes\":{\"doi\":\"arXiv:1710.10182\",\"id\":\"b125\"},\"end\":123788,\"start\":123493},{\"attributes\":{\"id\":\"b126\",\"matched_paper_id\":27377690},\"end\":124004,\"start\":123790},{\"attributes\":{\"id\":\"b127\",\"matched_paper_id\":4729126},\"end\":124432,\"start\":124006},{\"attributes\":{\"id\":\"b128\",\"matched_paper_id\":4234245},\"end\":124943,\"start\":124434},{\"attributes\":{\"id\":\"b129\"},\"end\":125199,\"start\":124945},{\"attributes\":{\"doi\":\"arXiv:1705.00609\",\"id\":\"b130\"},\"end\":125559,\"start\":125201},{\"attributes\":{\"doi\":\"arXiv:1704.02510\",\"id\":\"b131\"},\"end\":125817,\"start\":125561},{\"attributes\":{\"id\":\"b132\",\"matched_paper_id\":1409719},\"end\":126095,\"start\":125819},{\"attributes\":{\"id\":\"b133\",\"matched_paper_id\":362467},\"end\":126425,\"start\":126097},{\"attributes\":{\"doi\":\"arXiv:1702.08811\",\"id\":\"b134\"},\"end\":126804,\"start\":126427},{\"attributes\":{\"id\":\"b135\",\"matched_paper_id\":1277217},\"end\":127230,\"start\":126806},{\"attributes\":{\"doi\":\"arXiv:1803.09210\",\"id\":\"b136\"},\"end\":127514,\"start\":127232},{\"attributes\":{\"id\":\"b137\"},\"end\":127700,\"start\":127516},{\"attributes\":{\"id\":\"b138\",\"matched_paper_id\":12678405},\"end\":127974,\"start\":127702},{\"attributes\":{\"doi\":\"arXiv:1503.00591\",\"id\":\"b139\"},\"end\":128237,\"start\":127976},{\"attributes\":{\"id\":\"b140\",\"matched_paper_id\":11824004},\"end\":128592,\"start\":128239},{\"attributes\":{\"id\":\"b141\",\"matched_paper_id\":3345500},\"end\":129068,\"start\":128594},{\"attributes\":{\"id\":\"b142\",\"matched_paper_id\":15260040},\"end\":129368,\"start\":129070},{\"attributes\":{\"doi\":\"arXiv:1703.10593\",\"id\":\"b143\"},\"end\":129685,\"start\":129370},{\"attributes\":{\"id\":\"b144\",\"matched_paper_id\":15019934},\"end\":129974,\"start\":129687}]", "bib_title": "[{\"end\":82132,\"start\":82098},{\"end\":82346,\"start\":82273},{\"end\":83024,\"start\":82998},{\"end\":83407,\"start\":83312},{\"end\":83742,\"start\":83716},{\"end\":84286,\"start\":84243},{\"end\":84886,\"start\":84812},{\"end\":85290,\"start\":85233},{\"end\":86496,\"start\":86422},{\"end\":86852,\"start\":86788},{\"end\":87222,\"start\":87150},{\"end\":87893,\"start\":87850},{\"end\":88541,\"start\":88464},{\"end\":89222,\"start\":89149},{\"end\":89544,\"start\":89495},{\"end\":89811,\"start\":89765},{\"end\":90221,\"start\":90165},{\"end\":91284,\"start\":91193},{\"end\":91567,\"start\":91494},{\"end\":92025,\"start\":91971},{\"end\":92408,\"start\":92330},{\"end\":92735,\"start\":92725},{\"end\":93071,\"start\":92991},{\"end\":93578,\"start\":93494},{\"end\":94103,\"start\":93981},{\"end\":94460,\"start\":94405},{\"end\":94803,\"start\":94776},{\"end\":95229,\"start\":95163},{\"end\":95577,\"start\":95528},{\"end\":95971,\"start\":95942},{\"end\":96279,\"start\":96242},{\"end\":96602,\"start\":96558},{\"end\":97234,\"start\":97188},{\"end\":97473,\"start\":97427},{\"end\":97880,\"start\":97839},{\"end\":99265,\"start\":99168},{\"end\":99706,\"start\":99677},{\"end\":100732,\"start\":100640},{\"end\":101336,\"start\":101277},{\"end\":102069,\"start\":101984},{\"end\":102369,\"start\":102341},{\"end\":102795,\"start\":102732},{\"end\":103163,\"start\":103077},{\"end\":103579,\"start\":103503},{\"end\":104012,\"start\":103926},{\"end\":104306,\"start\":104258},{\"end\":104646,\"start\":104611},{\"end\":105518,\"start\":105479},{\"end\":105799,\"start\":105731},{\"end\":106110,\"start\":106035},{\"end\":106448,\"start\":106388},{\"end\":107008,\"start\":106946},{\"end\":107344,\"start\":107280},{\"end\":107769,\"start\":107695},{\"end\":108087,\"start\":108049},{\"end\":108416,\"start\":108356},{\"end\":108795,\"start\":108730},{\"end\":109127,\"start\":109070},{\"end\":109349,\"start\":109300},{\"end\":109619,\"start\":109590},{\"end\":109880,\"start\":109827},{\"end\":110382,\"start\":110316},{\"end\":111888,\"start\":111810},{\"end\":112240,\"start\":112167},{\"end\":112670,\"start\":112607},{\"end\":113083,\"start\":113031},{\"end\":114073,\"start\":114027},{\"end\":115235,\"start\":115182},{\"end\":115789,\"start\":115708},{\"end\":116487,\"start\":116404},{\"end\":117507,\"start\":117461},{\"end\":117716,\"start\":117656},{\"end\":117966,\"start\":117936},{\"end\":118736,\"start\":118663},{\"end\":119172,\"start\":119144},{\"end\":119626,\"start\":119594},{\"end\":119865,\"start\":119776},{\"end\":120377,\"start\":120333},{\"end\":121097,\"start\":121046},{\"end\":122487,\"start\":122373},{\"end\":123194,\"start\":123138},{\"end\":123842,\"start\":123790},{\"end\":124067,\"start\":124006},{\"end\":124527,\"start\":124434},{\"end\":125846,\"start\":125819},{\"end\":126151,\"start\":126097},{\"end\":126900,\"start\":126806},{\"end\":127783,\"start\":127702},{\"end\":128309,\"start\":128239},{\"end\":128641,\"start\":128594},{\"end\":129122,\"start\":129070},{\"end\":129763,\"start\":129687}]", "bib_author": "[{\"end\":81931,\"start\":81919},{\"end\":81943,\"start\":81931},{\"end\":81953,\"start\":81943},{\"end\":82144,\"start\":82134},{\"end\":82363,\"start\":82348},{\"end\":82374,\"start\":82363},{\"end\":82385,\"start\":82374},{\"end\":82399,\"start\":82385},{\"end\":82412,\"start\":82399},{\"end\":82423,\"start\":82412},{\"end\":82739,\"start\":82726},{\"end\":82752,\"start\":82739},{\"end\":82761,\"start\":82752},{\"end\":82770,\"start\":82761},{\"end\":82782,\"start\":82770},{\"end\":83039,\"start\":83026},{\"end\":83053,\"start\":83039},{\"end\":83066,\"start\":83053},{\"end\":83078,\"start\":83066},{\"end\":83087,\"start\":83078},{\"end\":83421,\"start\":83409},{\"end\":83435,\"start\":83421},{\"end\":83755,\"start\":83744},{\"end\":83763,\"start\":83755},{\"end\":84038,\"start\":84031},{\"end\":84046,\"start\":84038},{\"end\":84054,\"start\":84046},{\"end\":84066,\"start\":84054},{\"end\":84302,\"start\":84288},{\"end\":84311,\"start\":84302},{\"end\":84321,\"start\":84311},{\"end\":84330,\"start\":84321},{\"end\":84340,\"start\":84330},{\"end\":84615,\"start\":84607},{\"end\":84621,\"start\":84615},{\"end\":84635,\"start\":84621},{\"end\":84642,\"start\":84635},{\"end\":84899,\"start\":84888},{\"end\":84910,\"start\":84899},{\"end\":84923,\"start\":84910},{\"end\":84933,\"start\":84923},{\"end\":84939,\"start\":84933},{\"end\":84946,\"start\":84939},{\"end\":85299,\"start\":85292},{\"end\":85558,\"start\":85550},{\"end\":85564,\"start\":85558},{\"end\":85577,\"start\":85564},{\"end\":85584,\"start\":85577},{\"end\":85596,\"start\":85584},{\"end\":85869,\"start\":85861},{\"end\":85875,\"start\":85869},{\"end\":85887,\"start\":85875},{\"end\":86152,\"start\":86141},{\"end\":86163,\"start\":86152},{\"end\":86174,\"start\":86163},{\"end\":86185,\"start\":86174},{\"end\":86198,\"start\":86185},{\"end\":86205,\"start\":86198},{\"end\":86508,\"start\":86498},{\"end\":86524,\"start\":86508},{\"end\":86535,\"start\":86524},{\"end\":86861,\"start\":86854},{\"end\":86873,\"start\":86861},{\"end\":86884,\"start\":86873},{\"end\":86895,\"start\":86884},{\"end\":86906,\"start\":86895},{\"end\":87234,\"start\":87224},{\"end\":87249,\"start\":87234},{\"end\":87259,\"start\":87249},{\"end\":87700,\"start\":87690},{\"end\":87902,\"start\":87895},{\"end\":87920,\"start\":87902},{\"end\":88182,\"start\":88174},{\"end\":88191,\"start\":88182},{\"end\":88199,\"start\":88191},{\"end\":88207,\"start\":88199},{\"end\":88213,\"start\":88207},{\"end\":88221,\"start\":88213},{\"end\":88554,\"start\":88543},{\"end\":88561,\"start\":88554},{\"end\":88572,\"start\":88561},{\"end\":88583,\"start\":88572},{\"end\":88592,\"start\":88583},{\"end\":88601,\"start\":88592},{\"end\":88612,\"start\":88601},{\"end\":88966,\"start\":88958},{\"end\":88972,\"start\":88966},{\"end\":88981,\"start\":88972},{\"end\":89233,\"start\":89224},{\"end\":89244,\"start\":89233},{\"end\":89254,\"start\":89244},{\"end\":89555,\"start\":89546},{\"end\":89568,\"start\":89555},{\"end\":89822,\"start\":89813},{\"end\":89834,\"start\":89822},{\"end\":89844,\"start\":89834},{\"end\":89855,\"start\":89844},{\"end\":89869,\"start\":89855},{\"end\":89883,\"start\":89869},{\"end\":89895,\"start\":89883},{\"end\":89908,\"start\":89895},{\"end\":90234,\"start\":90223},{\"end\":90245,\"start\":90234},{\"end\":90255,\"start\":90245},{\"end\":90705,\"start\":90699},{\"end\":90711,\"start\":90705},{\"end\":90905,\"start\":90896},{\"end\":90916,\"start\":90905},{\"end\":90927,\"start\":90916},{\"end\":91298,\"start\":91286},{\"end\":91312,\"start\":91298},{\"end\":91580,\"start\":91569},{\"end\":91599,\"start\":91580},{\"end\":91608,\"start\":91599},{\"end\":91620,\"start\":91608},{\"end\":92038,\"start\":92027},{\"end\":92050,\"start\":92038},{\"end\":92059,\"start\":92050},{\"end\":92421,\"start\":92410},{\"end\":92433,\"start\":92421},{\"end\":92442,\"start\":92433},{\"end\":92454,\"start\":92442},{\"end\":92460,\"start\":92454},{\"end\":92749,\"start\":92737},{\"end\":93085,\"start\":93073},{\"end\":93096,\"start\":93085},{\"end\":93107,\"start\":93096},{\"end\":93116,\"start\":93107},{\"end\":93590,\"start\":93580},{\"end\":93600,\"start\":93590},{\"end\":93610,\"start\":93600},{\"end\":94113,\"start\":94105},{\"end\":94124,\"start\":94113},{\"end\":94131,\"start\":94124},{\"end\":94470,\"start\":94462},{\"end\":94477,\"start\":94470},{\"end\":94484,\"start\":94477},{\"end\":94495,\"start\":94484},{\"end\":94819,\"start\":94805},{\"end\":94836,\"start\":94819},{\"end\":94845,\"start\":94836},{\"end\":94851,\"start\":94845},{\"end\":94867,\"start\":94851},{\"end\":94876,\"start\":94867},{\"end\":94889,\"start\":94876},{\"end\":94899,\"start\":94889},{\"end\":95242,\"start\":95231},{\"end\":95248,\"start\":95242},{\"end\":95261,\"start\":95248},{\"end\":95588,\"start\":95579},{\"end\":95599,\"start\":95588},{\"end\":95608,\"start\":95599},{\"end\":95985,\"start\":95973},{\"end\":95995,\"start\":95985},{\"end\":96010,\"start\":95995},{\"end\":96021,\"start\":96010},{\"end\":96287,\"start\":96281},{\"end\":96294,\"start\":96287},{\"end\":96301,\"start\":96294},{\"end\":96309,\"start\":96301},{\"end\":96315,\"start\":96309},{\"end\":96322,\"start\":96315},{\"end\":96331,\"start\":96322},{\"end\":96610,\"start\":96604},{\"end\":96619,\"start\":96610},{\"end\":96626,\"start\":96619},{\"end\":96633,\"start\":96626},{\"end\":97017,\"start\":97007},{\"end\":97028,\"start\":97017},{\"end\":97036,\"start\":97028},{\"end\":97248,\"start\":97236},{\"end\":97260,\"start\":97248},{\"end\":97270,\"start\":97260},{\"end\":97486,\"start\":97475},{\"end\":97500,\"start\":97486},{\"end\":97511,\"start\":97500},{\"end\":97517,\"start\":97511},{\"end\":97528,\"start\":97517},{\"end\":97540,\"start\":97528},{\"end\":97551,\"start\":97540},{\"end\":97561,\"start\":97551},{\"end\":97893,\"start\":97882},{\"end\":97902,\"start\":97893},{\"end\":97911,\"start\":97902},{\"end\":97925,\"start\":97911},{\"end\":97936,\"start\":97925},{\"end\":98295,\"start\":98284},{\"end\":98304,\"start\":98295},{\"end\":98315,\"start\":98304},{\"end\":98322,\"start\":98315},{\"end\":98332,\"start\":98322},{\"end\":98343,\"start\":98332},{\"end\":98552,\"start\":98541},{\"end\":98560,\"start\":98552},{\"end\":98566,\"start\":98560},{\"end\":98577,\"start\":98566},{\"end\":98942,\"start\":98934},{\"end\":98948,\"start\":98942},{\"end\":98955,\"start\":98948},{\"end\":98965,\"start\":98955},{\"end\":99275,\"start\":99267},{\"end\":99281,\"start\":99275},{\"end\":99288,\"start\":99281},{\"end\":99295,\"start\":99288},{\"end\":99714,\"start\":99708},{\"end\":99720,\"start\":99714},{\"end\":99730,\"start\":99720},{\"end\":100044,\"start\":100035},{\"end\":100056,\"start\":100044},{\"end\":100400,\"start\":100391},{\"end\":100410,\"start\":100400},{\"end\":100422,\"start\":100410},{\"end\":100432,\"start\":100422},{\"end\":100743,\"start\":100734},{\"end\":100754,\"start\":100743},{\"end\":101065,\"start\":101056},{\"end\":101075,\"start\":101065},{\"end\":101083,\"start\":101075},{\"end\":101094,\"start\":101083},{\"end\":101345,\"start\":101338},{\"end\":101353,\"start\":101345},{\"end\":101361,\"start\":101353},{\"end\":101759,\"start\":101752},{\"end\":101766,\"start\":101759},{\"end\":101773,\"start\":101766},{\"end\":101780,\"start\":101773},{\"end\":101787,\"start\":101780},{\"end\":102085,\"start\":102071},{\"end\":102098,\"start\":102085},{\"end\":102387,\"start\":102371},{\"end\":102398,\"start\":102387},{\"end\":102411,\"start\":102398},{\"end\":102424,\"start\":102411},{\"end\":102811,\"start\":102797},{\"end\":102824,\"start\":102811},{\"end\":102836,\"start\":102824},{\"end\":103174,\"start\":103165},{\"end\":103184,\"start\":103174},{\"end\":103195,\"start\":103184},{\"end\":103594,\"start\":103581},{\"end\":103606,\"start\":103594},{\"end\":103619,\"start\":103606},{\"end\":104020,\"start\":104014},{\"end\":104028,\"start\":104020},{\"end\":104314,\"start\":104308},{\"end\":104322,\"start\":104314},{\"end\":104333,\"start\":104322},{\"end\":104349,\"start\":104333},{\"end\":104654,\"start\":104648},{\"end\":104665,\"start\":104654},{\"end\":104674,\"start\":104665},{\"end\":105000,\"start\":104994},{\"end\":105008,\"start\":105000},{\"end\":105015,\"start\":105008},{\"end\":105022,\"start\":105015},{\"end\":105270,\"start\":105264},{\"end\":105278,\"start\":105270},{\"end\":105285,\"start\":105278},{\"end\":105292,\"start\":105285},{\"end\":105299,\"start\":105292},{\"end\":105530,\"start\":105520},{\"end\":105539,\"start\":105530},{\"end\":105808,\"start\":105801},{\"end\":105816,\"start\":105808},{\"end\":105823,\"start\":105816},{\"end\":105831,\"start\":105823},{\"end\":105838,\"start\":105831},{\"end\":105851,\"start\":105838},{\"end\":106119,\"start\":106112},{\"end\":106127,\"start\":106119},{\"end\":106133,\"start\":106127},{\"end\":106140,\"start\":106133},{\"end\":106458,\"start\":106450},{\"end\":106465,\"start\":106458},{\"end\":106473,\"start\":106465},{\"end\":106483,\"start\":106473},{\"end\":106765,\"start\":106757},{\"end\":106773,\"start\":106765},{\"end\":106785,\"start\":106773},{\"end\":107018,\"start\":107010},{\"end\":107025,\"start\":107018},{\"end\":107033,\"start\":107025},{\"end\":107045,\"start\":107033},{\"end\":107352,\"start\":107346},{\"end\":107361,\"start\":107352},{\"end\":107368,\"start\":107361},{\"end\":107375,\"start\":107368},{\"end\":107383,\"start\":107375},{\"end\":107391,\"start\":107383},{\"end\":107402,\"start\":107391},{\"end\":107410,\"start\":107402},{\"end\":107781,\"start\":107771},{\"end\":107790,\"start\":107781},{\"end\":107799,\"start\":107790},{\"end\":108100,\"start\":108089},{\"end\":108109,\"start\":108100},{\"end\":108123,\"start\":108109},{\"end\":108134,\"start\":108123},{\"end\":108429,\"start\":108418},{\"end\":108443,\"start\":108429},{\"end\":108456,\"start\":108443},{\"end\":108467,\"start\":108456},{\"end\":108809,\"start\":108797},{\"end\":108817,\"start\":108809},{\"end\":108828,\"start\":108817},{\"end\":108841,\"start\":108828},{\"end\":109140,\"start\":109129},{\"end\":109153,\"start\":109140},{\"end\":109162,\"start\":109153},{\"end\":109360,\"start\":109351},{\"end\":109371,\"start\":109360},{\"end\":109381,\"start\":109371},{\"end\":109389,\"start\":109381},{\"end\":109630,\"start\":109621},{\"end\":109638,\"start\":109630},{\"end\":109893,\"start\":109882},{\"end\":109904,\"start\":109893},{\"end\":109910,\"start\":109904},{\"end\":109923,\"start\":109910},{\"end\":110130,\"start\":110119},{\"end\":110136,\"start\":110130},{\"end\":110145,\"start\":110136},{\"end\":110392,\"start\":110384},{\"end\":110403,\"start\":110392},{\"end\":110415,\"start\":110403},{\"end\":110425,\"start\":110415},{\"end\":110800,\"start\":110792},{\"end\":110810,\"start\":110800},{\"end\":111056,\"start\":111049},{\"end\":111072,\"start\":111056},{\"end\":111086,\"start\":111072},{\"end\":111334,\"start\":111320},{\"end\":111343,\"start\":111334},{\"end\":111354,\"start\":111343},{\"end\":111530,\"start\":111522},{\"end\":111539,\"start\":111530},{\"end\":111546,\"start\":111539},{\"end\":111560,\"start\":111546},{\"end\":111571,\"start\":111560},{\"end\":111578,\"start\":111571},{\"end\":111897,\"start\":111890},{\"end\":111903,\"start\":111897},{\"end\":111915,\"start\":111903},{\"end\":111922,\"start\":111915},{\"end\":112252,\"start\":112242},{\"end\":112260,\"start\":112252},{\"end\":112687,\"start\":112672},{\"end\":112698,\"start\":112687},{\"end\":112706,\"start\":112698},{\"end\":113093,\"start\":113085},{\"end\":113109,\"start\":113093},{\"end\":113504,\"start\":113491},{\"end\":113516,\"start\":113504},{\"end\":113523,\"start\":113516},{\"end\":113693,\"start\":113683},{\"end\":113709,\"start\":113693},{\"end\":113723,\"start\":113709},{\"end\":113732,\"start\":113723},{\"end\":113747,\"start\":113732},{\"end\":113762,\"start\":113747},{\"end\":113773,\"start\":113762},{\"end\":113784,\"start\":113773},{\"end\":114085,\"start\":114075},{\"end\":114094,\"start\":114085},{\"end\":114103,\"start\":114094},{\"end\":114114,\"start\":114103},{\"end\":114401,\"start\":114392},{\"end\":114411,\"start\":114401},{\"end\":114421,\"start\":114411},{\"end\":114663,\"start\":114654},{\"end\":114675,\"start\":114663},{\"end\":114685,\"start\":114675},{\"end\":114695,\"start\":114685},{\"end\":114984,\"start\":114964},{\"end\":114994,\"start\":114984},{\"end\":115002,\"start\":114994},{\"end\":115011,\"start\":115002},{\"end\":115024,\"start\":115011},{\"end\":115245,\"start\":115237},{\"end\":115252,\"start\":115245},{\"end\":115258,\"start\":115252},{\"end\":115568,\"start\":115560},{\"end\":115574,\"start\":115568},{\"end\":115583,\"start\":115574},{\"end\":115589,\"start\":115583},{\"end\":115802,\"start\":115791},{\"end\":115811,\"start\":115802},{\"end\":116135,\"start\":116120},{\"end\":116146,\"start\":116135},{\"end\":116155,\"start\":116146},{\"end\":116167,\"start\":116155},{\"end\":116175,\"start\":116167},{\"end\":116183,\"start\":116175},{\"end\":116496,\"start\":116489},{\"end\":116505,\"start\":116496},{\"end\":116513,\"start\":116505},{\"end\":116521,\"start\":116513},{\"end\":116946,\"start\":116934},{\"end\":116959,\"start\":116946},{\"end\":117206,\"start\":117198},{\"end\":117213,\"start\":117206},{\"end\":117222,\"start\":117213},{\"end\":117228,\"start\":117222},{\"end\":117239,\"start\":117228},{\"end\":117253,\"start\":117239},{\"end\":117516,\"start\":117509},{\"end\":117524,\"start\":117516},{\"end\":117534,\"start\":117524},{\"end\":117725,\"start\":117718},{\"end\":117735,\"start\":117725},{\"end\":117979,\"start\":117968},{\"end\":117986,\"start\":117979},{\"end\":117993,\"start\":117986},{\"end\":118005,\"start\":117993},{\"end\":118013,\"start\":118005},{\"end\":118025,\"start\":118013},{\"end\":118034,\"start\":118025},{\"end\":118047,\"start\":118034},{\"end\":118061,\"start\":118047},{\"end\":118495,\"start\":118484},{\"end\":118505,\"start\":118495},{\"end\":118513,\"start\":118505},{\"end\":118749,\"start\":118738},{\"end\":118757,\"start\":118749},{\"end\":118768,\"start\":118757},{\"end\":118776,\"start\":118768},{\"end\":119181,\"start\":119174},{\"end\":119189,\"start\":119181},{\"end\":119198,\"start\":119189},{\"end\":119206,\"start\":119198},{\"end\":119635,\"start\":119628},{\"end\":119644,\"start\":119635},{\"end\":119653,\"start\":119644},{\"end\":119661,\"start\":119653},{\"end\":119875,\"start\":119867},{\"end\":119883,\"start\":119875},{\"end\":119890,\"start\":119883},{\"end\":119904,\"start\":119890},{\"end\":119918,\"start\":119904},{\"end\":119926,\"start\":119918},{\"end\":120390,\"start\":120379},{\"end\":120402,\"start\":120390},{\"end\":120687,\"start\":120678},{\"end\":120696,\"start\":120687},{\"end\":120707,\"start\":120696},{\"end\":120715,\"start\":120707},{\"end\":120725,\"start\":120715},{\"end\":120735,\"start\":120725},{\"end\":120745,\"start\":120735},{\"end\":120756,\"start\":120745},{\"end\":121108,\"start\":121099},{\"end\":121119,\"start\":121108},{\"end\":121130,\"start\":121119},{\"end\":121140,\"start\":121130},{\"end\":121468,\"start\":121459},{\"end\":121479,\"start\":121468},{\"end\":121489,\"start\":121479},{\"end\":121500,\"start\":121489},{\"end\":121720,\"start\":121711},{\"end\":121731,\"start\":121720},{\"end\":121740,\"start\":121731},{\"end\":121750,\"start\":121740},{\"end\":121761,\"start\":121750},{\"end\":122124,\"start\":122113},{\"end\":122135,\"start\":122124},{\"end\":122148,\"start\":122135},{\"end\":122500,\"start\":122489},{\"end\":122514,\"start\":122500},{\"end\":122524,\"start\":122514},{\"end\":122534,\"start\":122524},{\"end\":122549,\"start\":122534},{\"end\":122916,\"start\":122907},{\"end\":122927,\"start\":122916},{\"end\":122939,\"start\":122927},{\"end\":122949,\"start\":122939},{\"end\":123204,\"start\":123196},{\"end\":123217,\"start\":123204},{\"end\":123577,\"start\":123569},{\"end\":123590,\"start\":123577},{\"end\":123601,\"start\":123590},{\"end\":123852,\"start\":123844},{\"end\":123860,\"start\":123852},{\"end\":123867,\"start\":123860},{\"end\":124076,\"start\":124069},{\"end\":124085,\"start\":124076},{\"end\":124093,\"start\":124085},{\"end\":124537,\"start\":124529},{\"end\":124543,\"start\":124537},{\"end\":124553,\"start\":124543},{\"end\":124561,\"start\":124553},{\"end\":125029,\"start\":125022},{\"end\":125037,\"start\":125029},{\"end\":125046,\"start\":125037},{\"end\":125056,\"start\":125046},{\"end\":125065,\"start\":125056},{\"end\":125306,\"start\":125299},{\"end\":125314,\"start\":125306},{\"end\":125320,\"start\":125314},{\"end\":125328,\"start\":125320},{\"end\":125334,\"start\":125328},{\"end\":125341,\"start\":125334},{\"end\":125626,\"start\":125620},{\"end\":125635,\"start\":125626},{\"end\":125645,\"start\":125635},{\"end\":125855,\"start\":125848},{\"end\":125862,\"start\":125855},{\"end\":125870,\"start\":125862},{\"end\":125880,\"start\":125870},{\"end\":125891,\"start\":125880},{\"end\":126165,\"start\":126153},{\"end\":126174,\"start\":126165},{\"end\":126184,\"start\":126174},{\"end\":126194,\"start\":126184},{\"end\":126518,\"start\":126505},{\"end\":126531,\"start\":126518},{\"end\":126543,\"start\":126531},{\"end\":126558,\"start\":126543},{\"end\":126576,\"start\":126558},{\"end\":126911,\"start\":126902},{\"end\":126917,\"start\":126911},{\"end\":126923,\"start\":126917},{\"end\":126932,\"start\":126923},{\"end\":126941,\"start\":126932},{\"end\":126949,\"start\":126941},{\"end\":126960,\"start\":126949},{\"end\":127309,\"start\":127300},{\"end\":127317,\"start\":127309},{\"end\":127323,\"start\":127317},{\"end\":127335,\"start\":127323},{\"end\":127584,\"start\":127575},{\"end\":127590,\"start\":127584},{\"end\":127602,\"start\":127590},{\"end\":127794,\"start\":127785},{\"end\":127800,\"start\":127794},{\"end\":127807,\"start\":127800},{\"end\":128040,\"start\":128031},{\"end\":128048,\"start\":128040},{\"end\":128060,\"start\":128048},{\"end\":128068,\"start\":128060},{\"end\":128320,\"start\":128311},{\"end\":128329,\"start\":128320},{\"end\":128337,\"start\":128329},{\"end\":128651,\"start\":128643},{\"end\":128657,\"start\":128651},{\"end\":128665,\"start\":128657},{\"end\":128671,\"start\":128665},{\"end\":128679,\"start\":128671},{\"end\":128687,\"start\":128679},{\"end\":128695,\"start\":128687},{\"end\":129134,\"start\":129124},{\"end\":129145,\"start\":129134},{\"end\":129154,\"start\":129145},{\"end\":129161,\"start\":129154},{\"end\":129461,\"start\":129451},{\"end\":129469,\"start\":129461},{\"end\":129478,\"start\":129469},{\"end\":129489,\"start\":129478},{\"end\":129775,\"start\":129765},{\"end\":129784,\"start\":129775},{\"end\":129791,\"start\":129784},{\"end\":129800,\"start\":129791},{\"end\":129806,\"start\":129800}]", "bib_venue": "[{\"end\":82160,\"start\":82144},{\"end\":82437,\"start\":82423},{\"end\":82724,\"start\":82645},{\"end\":83136,\"start\":83087},{\"end\":83497,\"start\":83435},{\"end\":83822,\"start\":83763},{\"end\":84029,\"start\":83968},{\"end\":84383,\"start\":84340},{\"end\":84605,\"start\":84548},{\"end\":85005,\"start\":84946},{\"end\":85337,\"start\":85299},{\"end\":85548,\"start\":85487},{\"end\":85859,\"start\":85784},{\"end\":86139,\"start\":86069},{\"end\":86589,\"start\":86535},{\"end\":86941,\"start\":86906},{\"end\":87336,\"start\":87259},{\"end\":87688,\"start\":87623},{\"end\":87939,\"start\":87920},{\"end\":88172,\"start\":88058},{\"end\":88656,\"start\":88612},{\"end\":88956,\"start\":88888},{\"end\":89303,\"start\":89254},{\"end\":89612,\"start\":89568},{\"end\":89944,\"start\":89908},{\"end\":90332,\"start\":90255},{\"end\":90697,\"start\":90601},{\"end\":91020,\"start\":90943},{\"end\":91326,\"start\":91312},{\"end\":91687,\"start\":91620},{\"end\":92122,\"start\":92059},{\"end\":92498,\"start\":92460},{\"end\":92816,\"start\":92749},{\"end\":93193,\"start\":93116},{\"end\":93688,\"start\":93610},{\"end\":94175,\"start\":94131},{\"end\":94566,\"start\":94495},{\"end\":94948,\"start\":94899},{\"end\":95322,\"start\":95261},{\"end\":95685,\"start\":95608},{\"end\":96071,\"start\":96021},{\"end\":96380,\"start\":96331},{\"end\":96710,\"start\":96633},{\"end\":97005,\"start\":96961},{\"end\":97288,\"start\":97270},{\"end\":97610,\"start\":97561},{\"end\":98005,\"start\":97936},{\"end\":98282,\"start\":98223},{\"end\":98666,\"start\":98593},{\"end\":98932,\"start\":98841},{\"end\":99372,\"start\":99295},{\"end\":99807,\"start\":99730},{\"end\":100146,\"start\":100072},{\"end\":100389,\"start\":100304},{\"end\":100798,\"start\":100754},{\"end\":101054,\"start\":100990},{\"end\":101428,\"start\":101361},{\"end\":101750,\"start\":101671},{\"end\":102136,\"start\":102098},{\"end\":102491,\"start\":102424},{\"end\":102885,\"start\":102836},{\"end\":103266,\"start\":103195},{\"end\":103658,\"start\":103619},{\"end\":104066,\"start\":104028},{\"end\":104370,\"start\":104349},{\"end\":104541,\"start\":104509},{\"end\":104752,\"start\":104674},{\"end\":105072,\"start\":105038},{\"end\":105262,\"start\":105200},{\"end\":105588,\"start\":105539},{\"end\":105865,\"start\":105851},{\"end\":106190,\"start\":106140},{\"end\":106527,\"start\":106483},{\"end\":106755,\"start\":106702},{\"end\":107094,\"start\":107045},{\"end\":107469,\"start\":107410},{\"end\":107849,\"start\":107799},{\"end\":108183,\"start\":108134},{\"end\":108526,\"start\":108467},{\"end\":108878,\"start\":108841},{\"end\":109176,\"start\":109162},{\"end\":109425,\"start\":109389},{\"end\":109689,\"start\":109638},{\"end\":109954,\"start\":109923},{\"end\":110487,\"start\":110425},{\"end\":110790,\"start\":110710},{\"end\":111047,\"start\":110987},{\"end\":111318,\"start\":111263},{\"end\":111640,\"start\":111594},{\"end\":111971,\"start\":111922},{\"end\":112337,\"start\":112260},{\"end\":112792,\"start\":112706},{\"end\":113186,\"start\":113109},{\"end\":113489,\"start\":113440},{\"end\":113827,\"start\":113800},{\"end\":114152,\"start\":114114},{\"end\":114390,\"start\":114332},{\"end\":114652,\"start\":114587},{\"end\":114962,\"start\":114883},{\"end\":115315,\"start\":115258},{\"end\":115558,\"start\":115485},{\"end\":115849,\"start\":115811},{\"end\":116118,\"start\":116042},{\"end\":116587,\"start\":116521},{\"end\":116932,\"start\":116866},{\"end\":117196,\"start\":117125},{\"end\":117538,\"start\":117534},{\"end\":117770,\"start\":117735},{\"end\":118138,\"start\":118061},{\"end\":118482,\"start\":118440},{\"end\":118853,\"start\":118776},{\"end\":119304,\"start\":119206},{\"end\":119665,\"start\":119661},{\"end\":120003,\"start\":119926},{\"end\":120442,\"start\":120402},{\"end\":120847,\"start\":120770},{\"end\":121207,\"start\":121140},{\"end\":121560,\"start\":121516},{\"end\":121831,\"start\":121776},{\"end\":122111,\"start\":122002},{\"end\":122585,\"start\":122549},{\"end\":122905,\"start\":122838},{\"end\":123292,\"start\":123217},{\"end\":123567,\"start\":123493},{\"end\":123881,\"start\":123867},{\"end\":124170,\"start\":124093},{\"end\":124638,\"start\":124561},{\"end\":125020,\"start\":124945},{\"end\":125297,\"start\":125201},{\"end\":125618,\"start\":125561},{\"end\":125929,\"start\":125891},{\"end\":126243,\"start\":126194},{\"end\":126503,\"start\":126427},{\"end\":126997,\"start\":126960},{\"end\":127298,\"start\":127232},{\"end\":127573,\"start\":127516},{\"end\":127821,\"start\":127807},{\"end\":128029,\"start\":127976},{\"end\":128396,\"start\":128337},{\"end\":128776,\"start\":128695},{\"end\":129199,\"start\":129161},{\"end\":129449,\"start\":129370},{\"end\":129811,\"start\":129806},{\"end\":87400,\"start\":87338},{\"end\":90396,\"start\":90334},{\"end\":91741,\"start\":91689},{\"end\":92870,\"start\":92818},{\"end\":93257,\"start\":93195},{\"end\":93753,\"start\":93690},{\"end\":95749,\"start\":95687},{\"end\":96774,\"start\":96712},{\"end\":99436,\"start\":99374},{\"end\":99871,\"start\":99809},{\"end\":101482,\"start\":101430},{\"end\":102545,\"start\":102493},{\"end\":104817,\"start\":104754},{\"end\":112401,\"start\":112339},{\"end\":113250,\"start\":113188},{\"end\":116640,\"start\":116589},{\"end\":118202,\"start\":118140},{\"end\":118917,\"start\":118855},{\"end\":119389,\"start\":119306},{\"end\":120067,\"start\":120005},{\"end\":121261,\"start\":121209},{\"end\":124234,\"start\":124172},{\"end\":124702,\"start\":124640},{\"end\":128844,\"start\":128778}]"}}}, "year": 2023, "month": 12, "day": 17}