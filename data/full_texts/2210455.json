{"id": 2210455, "updated": "2023-09-27 18:56:24.941", "metadata": {"title": "Microsoft COCO Captions: Data Collection and Evaluation Server", "authors": "[{\"first\":\"Xinlei\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Hao\",\"last\":\"Fang\",\"middle\":[]},{\"first\":\"Tsung-Yi\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Ramakrishna\",\"last\":\"Vedantam\",\"middle\":[]},{\"first\":\"Saurabh\",\"last\":\"Gupta\",\"middle\":[]},{\"first\":\"Piotr\",\"last\":\"Dollar\",\"middle\":[]},{\"first\":\"C.\",\"last\":\"Zitnick\",\"middle\":[\"Lawrence\"]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2015, "month": 4, "day": 1}, "abstract": "In this paper we describe the Microsoft COCO Caption dataset and evaluation server. When completed, the dataset will contain over one and a half million captions describing over 330,000 images. For the training and validation images, five independent human generated captions will be provided. To ensure consistency in evaluation of automatic caption generation algorithms, an evaluation server is used. The evaluation server receives candidate captions and scores them using several popular metrics, including BLEU, METEOR, ROUGE and CIDEr. Instructions for using the evaluation server are provided.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1504.00325", "mag": "1889081078", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/ChenFLVGDZ15", "doi": null}}, "content": {"source": {"pdf_hash": "696ca58d93f6404fea0fc75c62d1d7b378f47628", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1504.00325v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "7b819cd4354f68df24df1c9d80045f90115d79b8", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/696ca58d93f6404fea0fc75c62d1d7b378f47628.txt", "contents": "\nMicrosoft COCO Captions: Data Collection and Evaluation Server\n\n\nXinlei Chen \nHao Fang \nTsung-Yi Lin \nRamakrishna Vedantam \nSaurabh Gupta \nPiotr Doll\u00e1r \nC Lawrence Zitnick \nMicrosoft COCO Captions: Data Collection and Evaluation Server\n1\nIn this paper we describe the Microsoft COCO Caption dataset and evaluation server. When completed, the dataset will contain over one and a half million captions describing over 330,000 images. For the training and validation images, five independent human generated captions will be provided. To ensure consistency in evaluation of automatic caption generation algorithms, an evaluation server is used. The evaluation server receives candidate captions and scores them using several popular metrics, including BLEU, METEOR, ROUGE and CIDEr. Instructions for using the evaluation server are provided.!\n\nINTRODUCTION\n\nThe automatic generation of captions for images is a long standing and challenging problem in artificial intelligence [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15], [16], [17], [18], [19]. Research in this area spans numerous domains, such as computer vision, natural language processing, and machine learning. Recently there has been a surprising resurgence of interest in this area [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], due to the renewed interest in neural network learning techniques [31], [32] and increasingly large datasets [33], [34], [35], [7], [36], [37], [38].\n\nIn this paper, we describe our process of collecting captions for the Microsoft COCO Caption dataset, and the evaluation server we have set up to evaluate performance of different algorithms. The MS COCO caption dataset contains human generated captions for images contained in the Microsoft Common Objects in COntext (COCO) dataset [38]. Similar to previous datasets [7], [36], we collect our captions using Amazon's Mechanical Turk (AMT). Upon completion of the dataset it will contain over a million captions.\n\nWhen evaluating image caption generation algorithms, it is essential that a consistent evaluation protocol is used. Comparing results from different approaches can be difficult since numerous evaluation metrics exist [39], [40], [41], [42]. To further complicate matters the implementations of these metrics often differ. To help alleviate these issues, we have built an evaluation server to enable consistency in evaluation of different caption generation approaches. Using the testing data, our evaluation server evaluates captions output by different approaches using numerous automatic metrics: BLEU [39], METEOR [41],\n\n\u2022 Xinlei Chen is with Carnegie Mellon University.  ROUGE [40] and CIDEr [42]. We hope to augment these results with human evaluations on an annual basis. This paper is organized as follows: First we describe the data collection process. Next, we describe the caption evaluation server and the various metrics used. Human performance using these metrics are provided. Finally the annotation format and instructions for using the evaluation server are described for those who wish to submit results. We conclude by discussing future directions and known issues.\n\n\nDATA COLLECTION\n\nIn this section we describe how the data is gathered for the MS COCO captions dataset. For images, we use the dataset collected by Microsoft COCO [38]. These images are split into training, validation and testing sets.\n\n\narXiv:1504.00325v2 [cs.CV] 3 Apr 2015\n\nThe images were gathered by searching for pairs of 80 object categories and various scene types on Flickr. The goal of the MS COCO image collection process was to gather images containing multiple objects in their natural context. Given the visual complexity of most images in the dataset, they pose an interesting and difficult challenge for image captioning.\n\nFor generating a dataset of image captions, the same training, validation and testing sets were used as in the original MS COCO dataset. Two datasets were collected. The first dataset MS COCO c5 contains five reference captions for every image in the MS COCO training, validation and testing datasets. The second dataset MS COCO c40 contains 40 reference sentences for a randomly chosen 5,000 images from the MS COCO testing dataset. MS COCO c40 was created since many automatic evaluation metrics achieve higher correlation with human judgement when given more reference sentences [42]. MS COCO c40 may be expanded to include the MS COCO validation dataset in the future.\n\nOur process for gathering captions received significant inspiration from the work of Young etal. [36] and Hodosh etal. [7] that collected captions on Flickr images using Amazon's Mechanical Turk (AMT). Each of our captions are also generated using human subjects on AMT. Each subject was shown the user interface in Figure 2. The subjects were instructed to:\n\n\u2022 Describe all the important parts of the scene. \u2022 Do not start the sentences with \"There is. \n\n\nCAPTION EVALUATION\n\nIn this section we describe the MS COCO caption evaluation server. Instructions for using the evaluation server are provided in Section 5. As input the evaluation server receives candidate captions for both the validation and testing datasets in the format specified in Section 5. The validation and test images are provided to the submitter. However, the human generated reference sentences are only provided for the validation set. The reference sentences for the testing set are kept private to reduce the risk of overfitting.\n\nNumerous evaluation metrics are computed on both MS COCO c5 and MS COCO c40. These include BLEU-1, BLEU-2, BLEU-3, BLEU-4, ROUGE-L, METEOR and CIDEr-D. The details of the these metrics are described next.\n\n\nTokenization and preprocessing\n\nBoth the candidate captions and the reference captions are pre-processed by the evaluation server. To tokenize the captions, we use Stanford PTBTokenizer in Stanford CoreNLP tools (version 3.4.1) [43] which mimics Penn Treebank 3 tokenization. In addition, punctuations 1 are removed from the tokenized captions.\n\n\nEvaluation metrics\n\nOur goal is to automatically evaluate for an image I i the quality of a candidate caption c i given a set of reference captions S i = {s i1 , . . . , s im } \u2208 S. The caption sentences are represented using sets of n-grams, where an n-gram \u03c9 k \u2208 \u2126 is a set of one or more ordered words. In this paper we explore n-grams with one to four words. No stemming is performed on the words. The number of times an n-gram \u03c9 k occurs in a sentence s ij is denoted\nh k (s ij ) or h k (c i ) for the candidate sentence c i \u2208 C.\n\nBLEU\n\nBLEU [39] is a popular machine translation metric that analyzes the co-occurrences of n-grams between the candidate and reference sentences. It computes a corpuslevel clipped n-gram precision between sentences as follows: where k indexes the set of possible n-grams of length n. The clipped precision metric limits the number of times an n-gram may be counted to the maximum number of times it is observed in a single reference sentence. Note that CP n is a precision score and it favors short sentences. So a brevity penalty is also used:\nCP n (C, S) = i k min(h k (c i ), max j\u2208m h k (s ij )) i k h k (c i ) ,(1)b(C, S) = 1 if l C > l S e 1\u2212l S /l C if l C \u2264 l S ,(2)\nwhere l C is the total length of candidate sentences c i 's and l S is the length of the corpus-level effective reference length. When there are multiple references for a candidate sentence, we choose to use the closest reference length for the brevity penalty. The overall BLEU score is computed using a weighted geometric mean of the individual n-gram precision:\nBLEU N (C, S) = b(C, S) exp N n=1 w n log CP n (C, S) ,(3)\nwhere N = 1, 2, 3, 4 and w n is typically held constant for all n.\n\nBLEU has shown good performance for corpuslevel comparisons over which a high number of ngram matches exist. However, at a sentence-level the n-gram matches for higher n rarely occur. As a result, BLEU performs poorly when comparing individual sentences.\n\n\nROUGE\n\nROUGE [40] is a set of evaluation metrics designed to evaluate text summarization algorithms.\n\n1) ROUGE N : The first ROUGE metric computes a simple n-gram recall over all reference summaries given a candidate sentence:\nROU GE N (c i , S i ) = j k min(h k (c i ), h k (s ij )) j k h k (s ij )(4)\n2) ROUGE L : ROUGE L uses a measure based on the Longest Common Subsequence (LCS). An LCS is a set words shared by two sentences which occur in the same order. However, unlike n-grams there may be words in between the words that create the LCS. Given the length l(c i , s ij ) of the LCS between a pair of sentences, ROUGE L is found by computing an F-measure:\nR l = max j l(c i , s ij ) |s ij |(5)P l = max j l(c i , s ij ) |c i | (6) ROU GE L (c i , S i ) = (1 + \u03b2 2 )R l P l R l + \u03b2 2 P l(7)\nR l and P l are recall and precision of LCS. \u03b2 is usually set to favor recall (\u03b2 = 1.2). Since ngrams are implicit in this measure due to the use of the LCS, they need not be specified. 3) ROUGE S : The final ROUGE metric uses skip bigrams instead of the LCS or n-grams. Skip bi-grams are pairs of ordered words in a sentence. However, similar to the LCS, words may be skipped between pairs of words. Thus, a sentence with 4 words would have C 4 2 = 6 skip bi-grams. Precision and recall are again incorporated to compute an Fmeasure score. If f k (s ij ) is the skip bi-gram count for sentence s ij , ROUGE S is computed as:\nR s = max j k min(f k (c i ), f k (s ij )) k f k (s ij )(8)P s = max j k min(f k (c i ), f k (s ij )) k f k (c i ) (9) ROU GE S (c i , S i ) = (1 + \u03b2 2 )R s P s R s + \u03b2 2 P s(10)\nSkip bi-grams are capable of capturing long range sentence structure. In practice, skip bi-grams are computed so that the component words occur at a distance of at most 4 from each other.\n\n\nMETEOR\n\nMETEOR [41] is calculated by generating an alignment between the words in the candidate and reference sentences, with an aim of 1:1 correspondence. This alignment is computed while minimizing the number of chunks, ch, of contiguous and identically ordered tokens in the sentence pair. The alignment is based on exact token matching, followed by WordNet synonyms [44], stemmed tokens and then paraphrases. Given a set of alignments, m, the METEOR score is the harmonic mean of precision P m and recall R m between the best scoring reference and candidate:\nP en = \u03b3 ch m \u03b8(11)F mean = P m R m \u03b1P m + (1 \u2212 \u03b1)R m(12)P m = |m| k h k (c i )(13)R m = |m| k h k (s ij )(14)\nM ET EOR = (1 \u2212 P en)F mean\n\nThus, the final METEOR score includes a penalty P en based on chunkiness of resolved matches and a harmonic mean term that gives the quality of the resolved matches. The default parameters \u03b1, \u03b3 and \u03b8 are used for this evaluation. Note that similar to BLEU, statistics of precision and recall are first aggregated over the entire corpus, which are then combined to give the corpus-level METEOR score.\n\n\nCIDEr\n\nThe CIDEr metric [42] measures consensus in image captions by performing a Term Frequency Inverse Document Frequency (TF-IDF) weighting for each n-gram. The number of times an n-gram \u03c9 k occurs in a reference sentence s ij is denoted by h k (s ij ) or h k (c i ) for the candidate sentence c i . CIDEr computes the TF-IDF weighting g k (s ij ) for each n-gram \u03c9 k using:\ng k (s ij ) = h k (s ij ) \u03c9 l \u2208\u2126 h l (s ij ) log |I| Ip\u2208I min(1, q h k (s pq )) ,(16)\nwhere \u2126 is the vocabulary of all n-grams and I is the set of all images in the dataset. The first term measures the TF of each n-gram \u03c9 k , and the second term measures the rarity of \u03c9 k using its IDF. Intuitively, TF places higher weight on n-grams that frequently occur in the reference sentences describing an image, while IDF reduces the weight of n-grams that commonly occur across all descriptions. That is, the IDF provides a measure of word saliency by discounting popular words that are likely to be less visually informative. The IDF is computed using the logarithm of the number of images in the dataset |I| divided by the number of images for which \u03c9 k occurs in any of its reference sentences.\n\nThe CIDEr n score for n-grams of length n is computed using the average cosine similarity between the candidate sentence and the reference sentences, which accounts for both precision and recall:\nCIDEr n (c i , S i ) = 1 m j g n (c i ) \u00b7 g n (s ij ) g n (c i ) g n (s ij ) ,(17)\nwhere g n (c i ) is a vector formed by g k (c i ) corresponding to all n-grams of length n and g n (c i ) is the magnitude of the vector g n (c i ). Similarly for g n (s ij ).\n\nHigher order (longer) n-grams to are used to capture grammatical properties as well as richer semantics. Scores from n-grams of varying lengths are combined as follows:\nCIDEr(c i , S i ) = N n=1 w n CIDEr n (c i , S i ),(18)\nUniform weights are used w n = 1/N . N = 4 is used. CIDEr-D is a modification to CIDEr to make it more robust to gaming. Gaming refers to the phenomenon where a sentence that is poorly judged by humans tends to score highly with an automated metric. To defend the CIDEr metric against gaming effects, [42] add clipping and a length based gaussian penalty to the CIDEr metric described above. This results in the following equations for CIDEr-D: \nCIDEr-D n (c i , S i ) = 10 m j e \u2212(l(c i )\u2212l(s ij )) 2 2\u03c3 2 * min(g n (c i ), g n (s ij )) \u00b7 g n (s ij ) g n (c i ) g n (s ij ) ,(19)\nWhere l(c i ) and l(s ij ) denote the lengths of candidate and reference sentences respectively. \u03c3 = 6 is used. A factor of 10 is used in the numerator to make the CIDEr-D scores numerically similar to the other metrics.\n\nThe final CIDEr-D metric is computed in a similar manner to CIDEr (analogous to eqn. 18):\nCIDEr-D(c i , S i ) = N n=1 w n CIDEr-D n (c i , S i ),(20)\nNote that just like the BLEU and ROUGE metrics, CIDEr-D does not use stemming. We adopt the CIDEr-D metric for the evaluation server.\n\n\nHUMAN PERFORMANCE\n\nIn this section, we study the human agreement among humans at this task. We start with analyzing the interhuman agreement for image captioning (Section. 4.1) and then analyze human agreement for the word prediction sub-task and provide a simple model which explains human agreement for this sub-task (Section. 4.2).\n\n\nHuman Agreement for Image Captioning\n\nWhen examining human agreement on captions, it becomes clear that there are many equivalent ways to say essentially the same thing. We quantify this by conducting the following experiment: We collect one additional human caption for each image in the test set and treat this caption as the prediction. Using the MS COCO caption evaluation server we compute the various metrics. The results are tabulated in Table 1.\n\n\nHuman Agreement for Word Prediction\n\nWe can do a similar analysis for human agreement at the sub-task of word prediction. Consider the task of tagging the image with words that occur in the captions. For this task, we can compute the human precision and recall for a given word w by benchmarking words used in the k+1 human caption with respect to words used in the first k reference captions. Note that we use weighted versions of precision and recall, where each negative image has a weight of 1 and each positive image has a weight equal to the number of captions containing the word w. Human precision (H p ) and human recall (H r ) can be computed from the counts of how many subjects out of k use the word w to describe a given image over the whole dataset.\n\nWe plot H p versus H r for a set of nouns, verbs and adjectives, and all 1000 words considered in Figure 3. Nouns referring to animals like 'elephant' have a high recall, which means that if an 'elephant' exists in the image, a subject is likely to talk about it (which makes intuitive sense, given 'elephant' images are somewhat rare, and there are no alternative words that could be used instead of 'elephant'). On the other hand, an adjective like 'bright' is used inconsistently and hence has low recall. Interestingly, words with high recall also have high precision. Indeed, all the points of human agreement appear to lie on a one-dimensional curve in the two-dimension precision-recall space.\n\nThis observation motivates us to propose a simple model for when subjects use a particular word w for describing an image. Let o denote an object or visual concept associated with word w, n be the total number of images, and k be the number of reference captions. Next, let q = P (o = 1) be the probability that object o exists in an image. For clarity these definitions are summarized in Table 2. We make two simplifications. First, we ignore image level saliency and instead focus on word level saliency. Specifically, we only model p = P (w = 1|o = 1), the probability a subject uses w given that o is in the image, without conditioning on the image itself. Second, we assume that P (w = 1|o = 0) = 0, i.e. that a subject does not use w unless o is in the image. As we will show, even with these simplifications our model suffices to explain the empirical observations in Figure 3 to a reasonable degree of accuracy.\n\nGiven these assumptions, we can model human precision H p and recall H r for a word w given only p and k. First, given k captions per image, we need to compute the expected number of (1) captions containing w (cw), \nE[cw] = \u03a3 k i=1 P (w i = 1) = \u03a3 i P (w i = 1|o = 1)P (o = 1) +\u03a3 i P (w i = 1|o = 0)P (o = 0) = kpq + 0 = kpq E[tp] = \u03a3 k i=1 P (w i = 1 \u2227 w k+1 = 1) = \u03a3 i P (w i = 1 \u2227 w k+1 = 1|o = 1)P (o = 1) +\u03a3 i P (w i = 1 \u2227 w k+1 = 1|o = 0)P (o = 0) = kppq + 0 = kp 2 q E[f p] = P (w 1 . . . w k = 0 \u2227 w k+1 = 1) = P (o = 1 \u2227 w 1 . . . w k = 0 \u2227 w k+1 = 1) +P (o = 0 \u2227 w 1 . . . w k = 0 \u2227 w k+1 = 1) = q(1 \u2212 p) k p + 0 = q(1 \u2212 p) k p\nIn the above w i = 1 denotes that w appeared in the i th caption. Note that we are also assuming independence between subjects conditioned on o. We can now define model precision and recall as:\nH p := nE[tp] nE[tp] + nE[f p] = pk pk + (1 \u2212 p) k H r := nE[tp] nE[cw] = p\nNote that these expressions are independent of q and only depend on p. Interestingly, because of the use of weighted precision and recall, the recall for a category comes out to be exactly equal to p, the probability a subject uses w given that o is in the image. We set k = 4 and vary p to plot H p versus H r , getting the curve as shown in blue in Figure 3 (bottom left). The curve explains the observed data quite well, closely matching the precision-recall tradeoffs of the empirical data (although not perfectly). We can also reduce the number of captions from four, and look at how the empirical and predicted precision and recall change. Figure 3 (bottom right), shows this variation as we reduce the number of reference captions per image from four to one annotations. We see that the points of human agreement remain at the same recall value, but decrease in their precision, which is consistent with what the model predicts. Also, the human precision at infinite subjects will approach one, which is again reasonable given that a subject will only use the word w if the corresponding object is in the image (and in the presence of infinite subjects someone else will also use the word w).\n\nIn fact, the fixed recall value can help us recover p, the probability that a subject will use the word w in describing the image given the object is present. Nouns like 'elephant' and 'tennis' have large p, which is reasonable. Verbs and adjectives, on the other hand, have smaller p values, which can be justified from the fact that a) subjects are less likely to describe attributes . We see that the human agreement point remains at the same recall value but dips in precision when using fewer captions.\n\nof objects and b) subjects might use a different word (synonym) to describe the same attribute. This analysis of human agreement also motivates using a different metric for measuring performance. We propose Precision at Human Recall (PHR) as a metric for measuring performance of a vision system performing this task. Given that human recall for a particular word is fixed and precision varies with the number of annotations, we can look at system precision at human recall and compare it with human precision to report the performance of the vision system.\n\n\nEVALUATION SERVER INSTRUCTIONS\n\nDirections on how to use the MS COCO caption evaluation server can be found on the MS COCO website. The evaluation server is hosted by CodaLab. To participate, a user account on CodaLab must be created. The participants need to generate results on both the validation and testing datasets. When training for the generation of results on the test dataset, the training and validation dataset may be used as the participant sees fit. That is, the validation dataset may be used for training if desired. However, when generating results on the validation set, we ask participants to only train on the training dataset, and only use the validation dataset for tuning meta-parameters. Two JSON files should be created corresponding to results on each dataset in the following format:\n\n[{ \"image id\" : int, \"caption\"\n\n: str, }]\n\nThe results may then be placed into a zip file and uploaded to the server for evaluation. Code is also provided on GitHub to evaluate results on the validation dataset without having to upload to the server. The number of submissions per user is limited to a fixed amount.\n\n\nDISCUSSION\n\nMany challenges exist when creating an image caption dataset. As stated in [7], [42], [45] the captions generated by human subjects can vary significantly. However even though two captions may be very different, they may be judged equally \"good\" by human subjects. Designing effective automatic evaluation metrics that are highly correlated with human judgment remains a difficult challenge [7], [42], [45], [46]. We hope that by releasing results on the validation data, we can help enable future research in this area.\n\nSince automatic evaluation metrics do not always correspond to human judgment, we hope to conduct experiments using human subjects to judge the quality of automatically generated captions, which are most similar to human captions, and whether they are grammatically correct [45], [42], [7], [4], [5]. This is essential to determining whether future algorithms are indeed improving, or whether they are merely over fitting to a specific metric. These human experiments will also allow us to evaluate the automatic evaluation metrics themselves, and see which ones are correlated to human judgment.\n\n\u2022\nHao Fang is with the University of Washington. \u2022 T.Y. Lin is with Cornell NYC Tech. \u2022 Ramakrishna Vedantam is with Virginia Tech. \u2022 Saurabh Gupta is with the Univeristy of California, Berkeley. \u2022 P. Doll\u00e1r is with Facebook AI Research. \u2022 C. L. Zitnick is with Microsoft Research, Redmond.\n\nFig. 1 :\n1Example images and captions from the Microsoft COCO Caption dataset.\n\nFig. 2 :\n2Example user interface for the caption gathering task.\n\n1 .\n1The full list of punctuations: {\", \", ', ', -LRB-, -RRB-, -LCB-, -RCB-, ., ?, !, ,, :, -, -, ..., ;}.\n\n\ntrue positives (tp), and (3) false positives (f p). Note that in our definition there can be up to k true positives per image (if cw = k, i.e. each of the k captions contains word w) but at most 1 false positive (if none of the k captions contains w). The expectations, in terms of k, p, and q are:\n\nFig. 3 :\n3Precision-recall points for human agreement: we compute precision and recall by treating one human caption as prediction and benchmark it against the others to obtain points on the precision recall curve. We plot these points for example nouns (top left), adjectives (top center), and verbs (top right), and for all words (bottom left). We also plot the fit of our model for human agreement with the empirical data (bottom left) and show how the human agreement changes with different number of captions being used (bottom right)\n\n\n\u2022 Do not describe unimportant details. \u2022 Do not describe things that might have happened in the future or past. \u2022 Do not describe what a person might say. \u2022 Do not give people proper names. \u2022 The sentences should contain at least 8 words. The number of captions gathered is 413,915 captions for 82,783 images in training, 202,520 captions for 40,504 images in validation and 379,249 captions for 40,775 images in testing including 179,189 for MS COCO c5 and 200,060 for MS COCO c40. For each testing image, we collected one additional caption to compute the scores of human performance for comparing scores of machine generated captions. The total number of collected captions is 1,026,459. We plan to collect captions for the MS COCO 2015 dataset when it is released, which should approximately double the size of the caption dataset. The AMT interface may be obtained from the MS COCO website.\n\nTABLE 1 :\n1Human Agreement for Image Captioning: Various metrics when benchmarking a human generated caption against ground truth captions.Metric Name MS COCO c5 MS COCO c40 \n\nBLEU 1 \n0.663 \n0.880 \nBLEU 2 \n0.469 \n0.744 \nBLEU 3 \n0.321 \n0.603 \nBLEU 4 \n0.217 \n0.471 \n\nMETEOR \n0.252 \n0.335 \nROUGE L \n0.484 \n0.626 \nCIDEr-D \n0.854 \n0.910 \n\n\n\nTABLE 2 :\n2Model defintions.o \n= object or visual concept \nw \n= word associated with o \nn = total number of images \nk \n= number of captions per image \nq \n= P (o = 1) \np \n= P (w = 1|o = 1) \n\n\n\nLearning the semantics of words and pictures. K Barnard, D Forsyth, in ICCV. 2K. Barnard and D. Forsyth, \"Learning the semantics of words and pictures,\" in ICCV, vol. 2, 2001, pp. 408-415.\n\nMatching words and pictures. K Barnard, P Duygulu, D Forsyth, N Freitas, D M Blei, M I Jordan, JMLR. 3K. Barnard, P. Duygulu, D. Forsyth, N. De Freitas, D. M. Blei, and M. I. Jordan, \"Matching words and pictures,\" JMLR, vol. 3, pp. 1107-1135, 2003.\n\nA model for learning the semantics of pictures. V Lavrenko, R Manmatha, J Jeon, NIPS. V. Lavrenko, R. Manmatha, and J. Jeon, \"A model for learning the semantics of pictures,\" in NIPS, 2003.\n\nBaby talk: Understanding and generating simple image descriptions. G Kulkarni, V Premraj, S Dhar, S Li, Y Choi, A C Berg, T L Berg, CVPR. G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C. Berg, and T. L. Berg, \"Baby talk: Understanding and generating simple image descriptions,\" in CVPR, 2011.\n\nMidge: Generating image descriptions from computer vision detections. M Mitchell, X Han, J Dodge, A Mensch, A Goyal, A Berg, K Yamaguchi, T Berg, K Stratos, H Daum\u00e9, EACL. M. Mitchell, X. Han, J. Dodge, A. Mensch, A. Goyal, A. Berg, K. Yamaguchi, T. Berg, K. Stratos, and H. Daum\u00e9 III, \"Midge: Generating image descriptions from computer vision detections,\" in EACL, 2012.\n\nEvery picture tells a story: Generating sentences from images. A Farhadi, M Hejrati, M A Sadeghi, P Young, C Rashtchian, J Hockenmaier, D Forsyth, ECCV. A. Farhadi, M. Hejrati, M. A. Sadeghi, P. Young, C. Rashtchian, J. Hockenmaier, and D. Forsyth, \"Every picture tells a story: Generating sentences from images,\" in ECCV, 2010.\n\nFraming image description as a ranking task: Data, models and evaluation metrics. M Hodosh, P Young, J Hockenmaier, JAIR. 47M. Hodosh, P. Young, and J. Hockenmaier, \"Framing image de- scription as a ranking task: Data, models and evaluation metrics.\" JAIR, vol. 47, pp. 853-899, 2013.\n\nCollective generation of natural image descriptions. P Kuznetsova, V Ordonez, A C Berg, T L Berg, Y Choi, ACL. P. Kuznetsova, V. Ordonez, A. C. Berg, T. L. Berg, and Y. Choi, \"Collective generation of natural image descriptions,\" in ACL, 2012.\n\nCorpusguided sentence generation of natural images. Y Yang, C L Teo, H Daum\u00e9, Iii , Y Aloimonos, EMNLP. Y. Yang, C. L. Teo, H. Daum\u00e9 III, and Y. Aloimonos, \"Corpus- guided sentence generation of natural images,\" in EMNLP, 2011.\n\nChoosing linguistics over vision to describe images. A Gupta, Y Verma, C Jawahar, AAAI. A. Gupta, Y. Verma, and C. Jawahar, \"Choosing linguistics over vision to describe images.\" in AAAI, 2012.\n\nDistributional semantics in technicolor. E Bruni, G Boleda, M Baroni, N.-K Tran, ACL. E. Bruni, G. Boleda, M. Baroni, and N.-K. Tran, \"Distributional semantics in technicolor,\" in ACL, 2012.\n\nAutomatic caption generation for news images. Y Feng, M Lapata, TPAMI. 354Y. Feng and M. Lapata, \"Automatic caption generation for news images,\" TPAMI, vol. 35, no. 4, pp. 797-812, 2013.\n\nImage description using visual dependency representations. D Elliott, F Keller, EMNLP. D. Elliott and F. Keller, \"Image description using visual depen- dency representations.\" in EMNLP, 2013, pp. 1292-1302.\n\nDeep fragment embeddings for bidirectional image sentence mapping. A Karpathy, A Joulin, F.-F Li, NIPS. A. Karpathy, A. Joulin, and F.-F. Li, \"Deep fragment embeddings for bidirectional image sentence mapping,\" in NIPS, 2014.\n\nImproving image-sentence embeddings using large weakly annotated photo collections. Y Gong, L Wang, M Hodosh, J Hockenmaier, S Lazebnik, ECCV. Y. Gong, L. Wang, M. Hodosh, J. Hockenmaier, and S. Lazebnik, \"Improving image-sentence embeddings using large weakly an- notated photo collections,\" in ECCV, 2014, pp. 529-545.\n\nNonparametric method for datadriven image captioning. R Mason, E Charniak, ACL. R. Mason and E. Charniak, \"Nonparametric method for data- driven image captioning,\" in ACL, 2014.\n\nTreetalk: Composition and compression of trees for image descriptions. P Kuznetsova, V Ordonez, T Berg, Y Choi, TACL. 2P. Kuznetsova, V. Ordonez, T. Berg, and Y. Choi, \"Treetalk: Com- position and compression of trees for image descriptions,\" TACL, vol. 2, pp. 351-362, 2014.\n\nAutocaption: Automatic caption generation for personal photos. K Ramnath, S Baker, L Vanderwende, M El-Saban, S N Sinha, A Kannan, N Hassan, M Galley, Y Yang, D Ramanan, A Bergamo, L Torresani, WACV. K. Ramnath, S. Baker, L. Vanderwende, M. El-Saban, S. N. Sinha, A. Kannan, N. Hassan, M. Galley, Y. Yang, D. Ramanan, A. Bergamo, and L. Torresani, \"Autocaption: Automatic caption generation for personal photos,\" in WACV, 2014.\n\nIs this a wampimuk? cross-modal mapping between distributional semantics and the visual world. A Lazaridou, E Bruni, M Baroni, ACL. A. Lazaridou, E. Bruni, and M. Baroni, \"Is this a wampimuk? cross-modal mapping between distributional semantics and the visual world,\" in ACL, 2014.\n\nMultimodal neural language models. R Kiros, R Salakhutdinov, R Zemel, ICML. R. Kiros, R. Salakhutdinov, and R. Zemel, \"Multimodal neural language models,\" in ICML, 2014.\n\nExplain images with multimodal recurrent neural networks. J Mao, W Xu, Y Yang, J Wang, A L Yuille, arXiv:1410.1090arXiv preprintJ. Mao, W. Xu, Y. Yang, J. Wang, and A. L. Yuille, \"Explain im- ages with multimodal recurrent neural networks,\" arXiv preprint arXiv:1410.1090, 2014.\n\nShow and tell: A neural image caption generator. O Vinyals, A Toshev, S Bengio, D Erhan, arXiv:1411.4555arXiv preprintO. Vinyals, A. Toshev, S. Bengio, and D. Erhan, \"Show and tell: A neural image caption generator,\" arXiv preprint arXiv:1411.4555, 2014.\n\nDeep visual-semantic alignments for generating image descriptions. A Karpathy, L Fei-Fei, arXiv:1412.2306arXiv preprintA. Karpathy and L. Fei-Fei, \"Deep visual-semantic alignments for generating image descriptions,\" arXiv preprint arXiv:1412.2306, 2014.\n\nUnifying visualsemantic embeddings with multimodal neural language models. R Kiros, R Salakhutdinov, R S Zemel, arXiv:1411.2539arXiv preprintR. Kiros, R. Salakhutdinov, and R. S. Zemel, \"Unifying visual- semantic embeddings with multimodal neural language models,\" arXiv preprint arXiv:1411.2539, 2014.\n\nLong-term recurrent convolutional networks for visual recognition and description. J Donahue, L A Hendricks, S Guadarrama, M Rohrbach, S Venugopalan, K Saenko, T Darrell, arXiv:1411.4389arXiv preprintJ. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Darrell, \"Long-term recurrent convolutional networks for visual recognition and description,\" arXiv preprint arXiv:1411.4389, 2014.\n\nH Fang, S Gupta, F Iandola, R Srivastava, L Deng, P Doll\u00e1r, J Gao, X He, M Mitchell, J Platt, arXiv:1411.4952From captions to visual concepts and back. arXiv preprintH. Fang, S. Gupta, F. Iandola, R. Srivastava, L. Deng, P. Doll\u00e1r, J. Gao, X. He, M. Mitchell, J. Platt et al., \"From captions to visual concepts and back,\" arXiv preprint arXiv:1411.4952, 2014.\n\nLearning a recurrent visual representation for image caption generation. X Chen, C L Zitnick, arXiv:1411.5654arXiv preprintX. Chen and C. L. Zitnick, \"Learning a recurrent visual representa- tion for image caption generation,\" arXiv preprint arXiv:1411.5654, 2014.\n\nPhrase-based image captioning. R Lebret, P O Pinheiro, R Collobert, arXiv:1502.03671arXiv preprintR. Lebret, P. O. Pinheiro, and R. Collobert, \"Phrase-based image captioning,\" arXiv preprint arXiv:1502.03671, 2015.\n\nSimple image description generator via a linear phrasebased approach. arXiv:1412.8419arXiv preprint--, \"Simple image description generator via a linear phrase- based approach,\" arXiv preprint arXiv:1412.8419, 2014.\n\nCombining language and vision with a multimodal skip-gram model. A Lazaridou, N T Pham, M Baroni, arXiv:1501.02598arXiv preprintA. Lazaridou, N. T. Pham, and M. Baroni, \"Combining language and vision with a multimodal skip-gram model,\" arXiv preprint arXiv:1501.02598, 2015.\n\nImageNet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G Hinton, NIPS. A. Krizhevsky, I. Sutskever, and G. Hinton, \"ImageNet classifica- tion with deep convolutional neural networks,\" in NIPS, 2012.\n\nLong short-term memory. S Hochreiter, J Schmidhuber, Neural computation. 98S. Hochreiter and J. Schmidhuber, \"Long short-term memory,\" Neural computation, vol. 9, no. 8, pp. 1735-1780, 1997.\n\nIm-ageNet: A Large-Scale Hierarchical Image Database. J Deng, W Dong, R Socher, L.-J Li, K Li, L Fei-Fei, CVPR. J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, \"Im- ageNet: A Large-Scale Hierarchical Image Database,\" in CVPR, 2009.\n\nThe iapr tc-12 benchmark: A new evaluation resource for visual information systems. M Grubinger, P Clough, H M\u00fcller, T Deselaers, LREC Workshop on Language Resources for Contentbased Image Retrieval. M. Grubinger, P. Clough, H. M\u00fcller, and T. Deselaers, \"The iapr tc- 12 benchmark: A new evaluation resource for visual information systems,\" in LREC Workshop on Language Resources for Content- based Image Retrieval, 2006.\n\nIm2text: Describing images using 1 million captioned photographs. V Ordonez, G Kulkarni, T Berg, NIPS. V. Ordonez, G. Kulkarni, and T. Berg, \"Im2text: Describing images using 1 million captioned photographs.\" in NIPS, 2011.\n\nFrom image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. P Young, A Lai, M Hodosh, J Hockenmaier, TACL. 2P. Young, A. Lai, M. Hodosh, and J. Hockenmaier, \"From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions,\" TACL, vol. 2, pp. 67- 78, 2014.\n\nD\u00e9j\u00e1 imagecaptions: A corpus of expressive image descriptions in repetition. J Chen, P Kuznetsova, D Warren, Y Choi, NAACL. J. Chen, P. Kuznetsova, D. Warren, and Y. Choi, \"D\u00e9j\u00e1 image- captions: A corpus of expressive image descriptions in repetition,\" in NAACL, 2015.\n\nMicrosoft COCO: Common objects in context. T Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Doll\u00e1r, C L Zitnick, ECCV. T. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r, and C. L. Zitnick, \"Microsoft COCO: Common objects in context,\" in ECCV, 2014.\n\nBleu: a method for automatic evaluation of machine translation. K Papineni, S Roukos, T Ward, W.-J Zhu, ACL. K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, \"Bleu: a method for automatic evaluation of machine translation,\" in ACL, 2002.\n\nRouge: A package for automatic evaluation of summaries. C.-Y. Lin, ACL Workshop. C.-Y. Lin, \"Rouge: A package for automatic evaluation of sum- maries,\" in ACL Workshop, 2004.\n\nMeteor universal: Language specific translation evaluation for any target language. M Denkowski, A Lavie, EACL Workshop on Statistical Machine Translation. M. Denkowski and A. Lavie, \"Meteor universal: Language spe- cific translation evaluation for any target language,\" in EACL Workshop on Statistical Machine Translation, 2014.\n\nCider: Consensus-based image description evaluation. R Vedantam, C L Zitnick, D Parikh, arXiv:1411.5726arXiv preprintR. Vedantam, C. L. Zitnick, and D. Parikh, \"Cider: Consensus-based image description evaluation,\" arXiv preprint arXiv:1411.5726, 2014.\n\nThe Stanford CoreNLP natural language processing toolkit. C D Manning, M Surdeanu, J Bauer, J Finkel, S J Bethard, D Mcclosky, Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations. 52nd Annual Meeting of the Association for Computational Linguistics: System DemonstrationsC. D. Manning, M. Surdeanu, J. Bauer, J. Finkel, S. J. Bethard, and D. McClosky, \"The Stanford CoreNLP natural language processing toolkit,\" in Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, 2014, pp. 55-60. [Online]. Available: http: //www.aclweb.org/anthology/P/P14/P14-5010\n\nWordnet: a lexical database for english. G A Miller, Communications of the ACM. 3811G. A. Miller, \"Wordnet: a lexical database for english,\" Communi- cations of the ACM, vol. 38, no. 11, pp. 39-41, 1995.\n\nComparing automatic evaluation measures for image description. D Elliott, F Keller, Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. the 52nd Annual Meeting of the Association for Computational Linguistics2D. Elliott and F. Keller, \"Comparing automatic evaluation mea- sures for image description,\" in Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, vol. 2, 2014, pp. 452-457.\n\nRe-evaluation the role of bleu in machine translation research. C Callison-Burch, M Osborne, P Koehn, EACL. 6C. Callison-Burch, M. Osborne, and P. Koehn, \"Re-evaluation the role of bleu in machine translation research.\" in EACL, vol. 6, 2006, pp. 249-256.\n", "annotations": {"author": "[{\"end\":78,\"start\":66},{\"end\":88,\"start\":79},{\"end\":102,\"start\":89},{\"end\":124,\"start\":103},{\"end\":139,\"start\":125},{\"end\":153,\"start\":140},{\"end\":173,\"start\":154}]", "publisher": null, "author_last_name": "[{\"end\":77,\"start\":73},{\"end\":87,\"start\":83},{\"end\":101,\"start\":98},{\"end\":123,\"start\":115},{\"end\":138,\"start\":133},{\"end\":152,\"start\":146},{\"end\":172,\"start\":165}]", "author_first_name": "[{\"end\":72,\"start\":66},{\"end\":82,\"start\":79},{\"end\":97,\"start\":89},{\"end\":114,\"start\":103},{\"end\":132,\"start\":125},{\"end\":145,\"start\":140},{\"end\":155,\"start\":154},{\"end\":164,\"start\":156}]", "author_affiliation": null, "title": "[{\"end\":63,\"start\":1},{\"end\":236,\"start\":174}]", "venue": null, "abstract": "[{\"end\":840,\"start\":239}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":977,\"start\":974},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":982,\"start\":979},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":987,\"start\":984},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":992,\"start\":989},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":997,\"start\":994},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1002,\"start\":999},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1007,\"start\":1004},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1012,\"start\":1009},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1017,\"start\":1014},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1023,\"start\":1019},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1029,\"start\":1025},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1035,\"start\":1031},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":1041,\"start\":1037},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1047,\"start\":1043},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1053,\"start\":1049},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1059,\"start\":1055},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":1065,\"start\":1061},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1071,\"start\":1067},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":1077,\"start\":1073},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":1278,\"start\":1274},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":1284,\"start\":1280},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":1290,\"start\":1286},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":1296,\"start\":1292},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1302,\"start\":1298},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":1308,\"start\":1304},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":1314,\"start\":1310},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":1320,\"start\":1316},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":1326,\"start\":1322},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":1332,\"start\":1328},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":1338,\"start\":1334},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":1410,\"start\":1406},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":1416,\"start\":1412},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":1453,\"start\":1449},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":1459,\"start\":1455},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":1465,\"start\":1461},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1470,\"start\":1467},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":1476,\"start\":1472},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":1482,\"start\":1478},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":1488,\"start\":1484},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":1828,\"start\":1824},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1862,\"start\":1859},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":1868,\"start\":1864},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2226,\"start\":2222},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2232,\"start\":2228},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2238,\"start\":2234},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2244,\"start\":2240},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2613,\"start\":2609},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2626,\"start\":2622},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":2690,\"start\":2686},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2705,\"start\":2701},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3358,\"start\":3354},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":4416,\"start\":4412},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4605,\"start\":4601},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4626,\"start\":4623},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":5951,\"start\":5947},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6617,\"start\":6613},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8044,\"start\":8040},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9839,\"start\":9835},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10194,\"start\":10190},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10953,\"start\":10949},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":13083,\"start\":13079},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21446,\"start\":21443},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":21452,\"start\":21448},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":21458,\"start\":21454},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21762,\"start\":21759},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":21768,\"start\":21764},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":21774,\"start\":21770},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":21780,\"start\":21776},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":22168,\"start\":22164},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":22174,\"start\":22170},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":22179,\"start\":22176},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22184,\"start\":22181},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22189,\"start\":22186}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":22778,\"start\":22487},{\"attributes\":{\"id\":\"fig_1\"},\"end\":22858,\"start\":22779},{\"attributes\":{\"id\":\"fig_2\"},\"end\":22924,\"start\":22859},{\"attributes\":{\"id\":\"fig_3\"},\"end\":23032,\"start\":22925},{\"attributes\":{\"id\":\"fig_4\"},\"end\":23333,\"start\":23033},{\"attributes\":{\"id\":\"fig_5\"},\"end\":23874,\"start\":23334},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":24772,\"start\":23875},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":25108,\"start\":24773},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":25300,\"start\":25109}]", "paragraph": "[{\"end\":1489,\"start\":856},{\"end\":2003,\"start\":1491},{\"end\":2627,\"start\":2005},{\"end\":3188,\"start\":2629},{\"end\":3426,\"start\":3208},{\"end\":3828,\"start\":3468},{\"end\":4502,\"start\":3830},{\"end\":4862,\"start\":4504},{\"end\":4958,\"start\":4864},{\"end\":5510,\"start\":4981},{\"end\":5716,\"start\":5512},{\"end\":6063,\"start\":5751},{\"end\":6538,\"start\":6086},{\"end\":7147,\"start\":6608},{\"end\":7642,\"start\":7278},{\"end\":7768,\"start\":7702},{\"end\":8024,\"start\":7770},{\"end\":8127,\"start\":8034},{\"end\":8253,\"start\":8129},{\"end\":8690,\"start\":8330},{\"end\":9450,\"start\":8825},{\"end\":9817,\"start\":9630},{\"end\":10382,\"start\":9828},{\"end\":10521,\"start\":10494},{\"end\":10922,\"start\":10523},{\"end\":11302,\"start\":10932},{\"end\":12095,\"start\":11389},{\"end\":12292,\"start\":12097},{\"end\":12551,\"start\":12376},{\"end\":12721,\"start\":12553},{\"end\":13223,\"start\":12778},{\"end\":13579,\"start\":13359},{\"end\":13670,\"start\":13581},{\"end\":13864,\"start\":13731},{\"end\":14201,\"start\":13886},{\"end\":14657,\"start\":14242},{\"end\":15423,\"start\":14697},{\"end\":16125,\"start\":15425},{\"end\":17046,\"start\":16127},{\"end\":17263,\"start\":17048},{\"end\":17879,\"start\":17686},{\"end\":19155,\"start\":17956},{\"end\":19664,\"start\":19157},{\"end\":20223,\"start\":19666},{\"end\":21036,\"start\":20258},{\"end\":21068,\"start\":21038},{\"end\":21079,\"start\":21070},{\"end\":21353,\"start\":21081},{\"end\":21888,\"start\":21368},{\"end\":22486,\"start\":21890}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6600,\"start\":6539},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7222,\"start\":7148},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7277,\"start\":7222},{\"attributes\":{\"id\":\"formula_3\"},\"end\":7701,\"start\":7643},{\"attributes\":{\"id\":\"formula_4\"},\"end\":8329,\"start\":8254},{\"attributes\":{\"id\":\"formula_5\"},\"end\":8728,\"start\":8691},{\"attributes\":{\"id\":\"formula_6\"},\"end\":8824,\"start\":8728},{\"attributes\":{\"id\":\"formula_7\"},\"end\":9510,\"start\":9451},{\"attributes\":{\"id\":\"formula_8\"},\"end\":9629,\"start\":9510},{\"attributes\":{\"id\":\"formula_9\"},\"end\":10402,\"start\":10383},{\"attributes\":{\"id\":\"formula_10\"},\"end\":10440,\"start\":10402},{\"attributes\":{\"id\":\"formula_11\"},\"end\":10466,\"start\":10440},{\"attributes\":{\"id\":\"formula_12\"},\"end\":10493,\"start\":10466},{\"attributes\":{\"id\":\"formula_14\"},\"end\":11388,\"start\":11303},{\"attributes\":{\"id\":\"formula_15\"},\"end\":12375,\"start\":12293},{\"attributes\":{\"id\":\"formula_16\"},\"end\":12777,\"start\":12722},{\"attributes\":{\"id\":\"formula_17\"},\"end\":13358,\"start\":13224},{\"attributes\":{\"id\":\"formula_18\"},\"end\":13730,\"start\":13671},{\"attributes\":{\"id\":\"formula_20\"},\"end\":17685,\"start\":17264},{\"attributes\":{\"id\":\"formula_21\"},\"end\":17955,\"start\":17880}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":14656,\"start\":14649},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":16523,\"start\":16516}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":854,\"start\":842},{\"attributes\":{\"n\":\"2\"},\"end\":3206,\"start\":3191},{\"end\":3466,\"start\":3429},{\"attributes\":{\"n\":\"3\"},\"end\":4979,\"start\":4961},{\"attributes\":{\"n\":\"3.1\"},\"end\":5749,\"start\":5719},{\"attributes\":{\"n\":\"3.2\"},\"end\":6084,\"start\":6066},{\"attributes\":{\"n\":\"3.3\"},\"end\":6606,\"start\":6602},{\"attributes\":{\"n\":\"3.4\"},\"end\":8032,\"start\":8027},{\"attributes\":{\"n\":\"3.5\"},\"end\":9826,\"start\":9820},{\"attributes\":{\"n\":\"3.6\"},\"end\":10930,\"start\":10925},{\"attributes\":{\"n\":\"4\"},\"end\":13884,\"start\":13867},{\"attributes\":{\"n\":\"4.1\"},\"end\":14240,\"start\":14204},{\"attributes\":{\"n\":\"4.2\"},\"end\":14695,\"start\":14660},{\"attributes\":{\"n\":\"5\"},\"end\":20256,\"start\":20226},{\"attributes\":{\"n\":\"6\"},\"end\":21366,\"start\":21356},{\"end\":22489,\"start\":22488},{\"end\":22788,\"start\":22780},{\"end\":22868,\"start\":22860},{\"end\":22929,\"start\":22926},{\"end\":23343,\"start\":23335},{\"end\":24783,\"start\":24774},{\"end\":25119,\"start\":25110}]", "table": "[{\"end\":25108,\"start\":24913},{\"end\":25300,\"start\":25138}]", "figure_caption": "[{\"end\":22778,\"start\":22490},{\"end\":22858,\"start\":22790},{\"end\":22924,\"start\":22870},{\"end\":23032,\"start\":22931},{\"end\":23333,\"start\":23035},{\"end\":23874,\"start\":23345},{\"end\":24772,\"start\":23877},{\"end\":24913,\"start\":24785},{\"end\":25138,\"start\":25121}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":4828,\"start\":4820},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":15531,\"start\":15523},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":17010,\"start\":17002},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":18315,\"start\":18307},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":18610,\"start\":18602}]", "bib_author_first_name": "[{\"end\":25349,\"start\":25348},{\"end\":25360,\"start\":25359},{\"end\":25522,\"start\":25521},{\"end\":25533,\"start\":25532},{\"end\":25544,\"start\":25543},{\"end\":25555,\"start\":25554},{\"end\":25566,\"start\":25565},{\"end\":25568,\"start\":25567},{\"end\":25576,\"start\":25575},{\"end\":25578,\"start\":25577},{\"end\":25791,\"start\":25790},{\"end\":25803,\"start\":25802},{\"end\":25815,\"start\":25814},{\"end\":26001,\"start\":26000},{\"end\":26013,\"start\":26012},{\"end\":26024,\"start\":26023},{\"end\":26032,\"start\":26031},{\"end\":26038,\"start\":26037},{\"end\":26046,\"start\":26045},{\"end\":26048,\"start\":26047},{\"end\":26056,\"start\":26055},{\"end\":26058,\"start\":26057},{\"end\":26305,\"start\":26304},{\"end\":26317,\"start\":26316},{\"end\":26324,\"start\":26323},{\"end\":26333,\"start\":26332},{\"end\":26343,\"start\":26342},{\"end\":26352,\"start\":26351},{\"end\":26360,\"start\":26359},{\"end\":26373,\"start\":26372},{\"end\":26381,\"start\":26380},{\"end\":26392,\"start\":26391},{\"end\":26672,\"start\":26671},{\"end\":26683,\"start\":26682},{\"end\":26694,\"start\":26693},{\"end\":26696,\"start\":26695},{\"end\":26707,\"start\":26706},{\"end\":26716,\"start\":26715},{\"end\":26730,\"start\":26729},{\"end\":26745,\"start\":26744},{\"end\":27021,\"start\":27020},{\"end\":27031,\"start\":27030},{\"end\":27040,\"start\":27039},{\"end\":27278,\"start\":27277},{\"end\":27292,\"start\":27291},{\"end\":27303,\"start\":27302},{\"end\":27305,\"start\":27304},{\"end\":27313,\"start\":27312},{\"end\":27315,\"start\":27314},{\"end\":27323,\"start\":27322},{\"end\":27522,\"start\":27521},{\"end\":27530,\"start\":27529},{\"end\":27532,\"start\":27531},{\"end\":27539,\"start\":27538},{\"end\":27550,\"start\":27547},{\"end\":27554,\"start\":27553},{\"end\":27752,\"start\":27751},{\"end\":27761,\"start\":27760},{\"end\":27770,\"start\":27769},{\"end\":27935,\"start\":27934},{\"end\":27944,\"start\":27943},{\"end\":27954,\"start\":27953},{\"end\":27967,\"start\":27963},{\"end\":28132,\"start\":28131},{\"end\":28140,\"start\":28139},{\"end\":28333,\"start\":28332},{\"end\":28344,\"start\":28343},{\"end\":28549,\"start\":28548},{\"end\":28561,\"start\":28560},{\"end\":28574,\"start\":28570},{\"end\":28793,\"start\":28792},{\"end\":28801,\"start\":28800},{\"end\":28809,\"start\":28808},{\"end\":28819,\"start\":28818},{\"end\":28834,\"start\":28833},{\"end\":29085,\"start\":29084},{\"end\":29094,\"start\":29093},{\"end\":29281,\"start\":29280},{\"end\":29295,\"start\":29294},{\"end\":29306,\"start\":29305},{\"end\":29314,\"start\":29313},{\"end\":29550,\"start\":29549},{\"end\":29561,\"start\":29560},{\"end\":29570,\"start\":29569},{\"end\":29585,\"start\":29584},{\"end\":29597,\"start\":29596},{\"end\":29599,\"start\":29598},{\"end\":29608,\"start\":29607},{\"end\":29618,\"start\":29617},{\"end\":29628,\"start\":29627},{\"end\":29638,\"start\":29637},{\"end\":29646,\"start\":29645},{\"end\":29657,\"start\":29656},{\"end\":29668,\"start\":29667},{\"end\":30011,\"start\":30010},{\"end\":30024,\"start\":30023},{\"end\":30033,\"start\":30032},{\"end\":30234,\"start\":30233},{\"end\":30243,\"start\":30242},{\"end\":30260,\"start\":30259},{\"end\":30428,\"start\":30427},{\"end\":30435,\"start\":30434},{\"end\":30441,\"start\":30440},{\"end\":30449,\"start\":30448},{\"end\":30457,\"start\":30456},{\"end\":30459,\"start\":30458},{\"end\":30699,\"start\":30698},{\"end\":30710,\"start\":30709},{\"end\":30720,\"start\":30719},{\"end\":30730,\"start\":30729},{\"end\":30973,\"start\":30972},{\"end\":30985,\"start\":30984},{\"end\":31236,\"start\":31235},{\"end\":31245,\"start\":31244},{\"end\":31262,\"start\":31261},{\"end\":31264,\"start\":31263},{\"end\":31548,\"start\":31547},{\"end\":31559,\"start\":31558},{\"end\":31561,\"start\":31560},{\"end\":31574,\"start\":31573},{\"end\":31588,\"start\":31587},{\"end\":31600,\"start\":31599},{\"end\":31615,\"start\":31614},{\"end\":31625,\"start\":31624},{\"end\":31889,\"start\":31888},{\"end\":31897,\"start\":31896},{\"end\":31906,\"start\":31905},{\"end\":31917,\"start\":31916},{\"end\":31931,\"start\":31930},{\"end\":31939,\"start\":31938},{\"end\":31949,\"start\":31948},{\"end\":31956,\"start\":31955},{\"end\":31962,\"start\":31961},{\"end\":31974,\"start\":31973},{\"end\":32323,\"start\":32322},{\"end\":32331,\"start\":32330},{\"end\":32333,\"start\":32332},{\"end\":32547,\"start\":32546},{\"end\":32557,\"start\":32556},{\"end\":32559,\"start\":32558},{\"end\":32571,\"start\":32570},{\"end\":33013,\"start\":33012},{\"end\":33026,\"start\":33025},{\"end\":33028,\"start\":33027},{\"end\":33036,\"start\":33035},{\"end\":33289,\"start\":33288},{\"end\":33303,\"start\":33302},{\"end\":33316,\"start\":33315},{\"end\":33485,\"start\":33484},{\"end\":33499,\"start\":33498},{\"end\":33707,\"start\":33706},{\"end\":33715,\"start\":33714},{\"end\":33723,\"start\":33722},{\"end\":33736,\"start\":33732},{\"end\":33742,\"start\":33741},{\"end\":33748,\"start\":33747},{\"end\":33984,\"start\":33983},{\"end\":33997,\"start\":33996},{\"end\":34007,\"start\":34006},{\"end\":34017,\"start\":34016},{\"end\":34389,\"start\":34388},{\"end\":34400,\"start\":34399},{\"end\":34412,\"start\":34411},{\"end\":34666,\"start\":34665},{\"end\":34675,\"start\":34674},{\"end\":34682,\"start\":34681},{\"end\":34692,\"start\":34691},{\"end\":34993,\"start\":34992},{\"end\":35001,\"start\":35000},{\"end\":35015,\"start\":35014},{\"end\":35025,\"start\":35024},{\"end\":35229,\"start\":35228},{\"end\":35236,\"start\":35235},{\"end\":35245,\"start\":35244},{\"end\":35257,\"start\":35256},{\"end\":35265,\"start\":35264},{\"end\":35275,\"start\":35274},{\"end\":35286,\"start\":35285},{\"end\":35296,\"start\":35295},{\"end\":35298,\"start\":35297},{\"end\":35533,\"start\":35532},{\"end\":35545,\"start\":35544},{\"end\":35555,\"start\":35554},{\"end\":35566,\"start\":35562},{\"end\":35767,\"start\":35762},{\"end\":35967,\"start\":35966},{\"end\":35980,\"start\":35979},{\"end\":36267,\"start\":36266},{\"end\":36279,\"start\":36278},{\"end\":36281,\"start\":36280},{\"end\":36292,\"start\":36291},{\"end\":36526,\"start\":36525},{\"end\":36528,\"start\":36527},{\"end\":36539,\"start\":36538},{\"end\":36551,\"start\":36550},{\"end\":36560,\"start\":36559},{\"end\":36570,\"start\":36569},{\"end\":36572,\"start\":36571},{\"end\":36583,\"start\":36582},{\"end\":37174,\"start\":37173},{\"end\":37176,\"start\":37175},{\"end\":37401,\"start\":37400},{\"end\":37412,\"start\":37411},{\"end\":37861,\"start\":37860},{\"end\":37879,\"start\":37878},{\"end\":37890,\"start\":37889}]", "bib_author_last_name": "[{\"end\":25357,\"start\":25350},{\"end\":25368,\"start\":25361},{\"end\":25530,\"start\":25523},{\"end\":25541,\"start\":25534},{\"end\":25552,\"start\":25545},{\"end\":25563,\"start\":25556},{\"end\":25573,\"start\":25569},{\"end\":25585,\"start\":25579},{\"end\":25800,\"start\":25792},{\"end\":25812,\"start\":25804},{\"end\":25820,\"start\":25816},{\"end\":26010,\"start\":26002},{\"end\":26021,\"start\":26014},{\"end\":26029,\"start\":26025},{\"end\":26035,\"start\":26033},{\"end\":26043,\"start\":26039},{\"end\":26053,\"start\":26049},{\"end\":26063,\"start\":26059},{\"end\":26314,\"start\":26306},{\"end\":26321,\"start\":26318},{\"end\":26330,\"start\":26325},{\"end\":26340,\"start\":26334},{\"end\":26349,\"start\":26344},{\"end\":26357,\"start\":26353},{\"end\":26370,\"start\":26361},{\"end\":26378,\"start\":26374},{\"end\":26389,\"start\":26382},{\"end\":26398,\"start\":26393},{\"end\":26680,\"start\":26673},{\"end\":26691,\"start\":26684},{\"end\":26704,\"start\":26697},{\"end\":26713,\"start\":26708},{\"end\":26727,\"start\":26717},{\"end\":26742,\"start\":26731},{\"end\":26753,\"start\":26746},{\"end\":27028,\"start\":27022},{\"end\":27037,\"start\":27032},{\"end\":27052,\"start\":27041},{\"end\":27289,\"start\":27279},{\"end\":27300,\"start\":27293},{\"end\":27310,\"start\":27306},{\"end\":27320,\"start\":27316},{\"end\":27328,\"start\":27324},{\"end\":27527,\"start\":27523},{\"end\":27536,\"start\":27533},{\"end\":27545,\"start\":27540},{\"end\":27564,\"start\":27555},{\"end\":27758,\"start\":27753},{\"end\":27767,\"start\":27762},{\"end\":27778,\"start\":27771},{\"end\":27941,\"start\":27936},{\"end\":27951,\"start\":27945},{\"end\":27961,\"start\":27955},{\"end\":27972,\"start\":27968},{\"end\":28137,\"start\":28133},{\"end\":28147,\"start\":28141},{\"end\":28341,\"start\":28334},{\"end\":28351,\"start\":28345},{\"end\":28558,\"start\":28550},{\"end\":28568,\"start\":28562},{\"end\":28577,\"start\":28575},{\"end\":28798,\"start\":28794},{\"end\":28806,\"start\":28802},{\"end\":28816,\"start\":28810},{\"end\":28831,\"start\":28820},{\"end\":28843,\"start\":28835},{\"end\":29091,\"start\":29086},{\"end\":29103,\"start\":29095},{\"end\":29292,\"start\":29282},{\"end\":29303,\"start\":29296},{\"end\":29311,\"start\":29307},{\"end\":29319,\"start\":29315},{\"end\":29558,\"start\":29551},{\"end\":29567,\"start\":29562},{\"end\":29582,\"start\":29571},{\"end\":29594,\"start\":29586},{\"end\":29605,\"start\":29600},{\"end\":29615,\"start\":29609},{\"end\":29625,\"start\":29619},{\"end\":29635,\"start\":29629},{\"end\":29643,\"start\":29639},{\"end\":29654,\"start\":29647},{\"end\":29665,\"start\":29658},{\"end\":29678,\"start\":29669},{\"end\":30021,\"start\":30012},{\"end\":30030,\"start\":30025},{\"end\":30040,\"start\":30034},{\"end\":30240,\"start\":30235},{\"end\":30257,\"start\":30244},{\"end\":30266,\"start\":30261},{\"end\":30432,\"start\":30429},{\"end\":30438,\"start\":30436},{\"end\":30446,\"start\":30442},{\"end\":30454,\"start\":30450},{\"end\":30466,\"start\":30460},{\"end\":30707,\"start\":30700},{\"end\":30717,\"start\":30711},{\"end\":30727,\"start\":30721},{\"end\":30736,\"start\":30731},{\"end\":30982,\"start\":30974},{\"end\":30993,\"start\":30986},{\"end\":31242,\"start\":31237},{\"end\":31259,\"start\":31246},{\"end\":31270,\"start\":31265},{\"end\":31556,\"start\":31549},{\"end\":31571,\"start\":31562},{\"end\":31585,\"start\":31575},{\"end\":31597,\"start\":31589},{\"end\":31612,\"start\":31601},{\"end\":31622,\"start\":31616},{\"end\":31633,\"start\":31626},{\"end\":31894,\"start\":31890},{\"end\":31903,\"start\":31898},{\"end\":31914,\"start\":31907},{\"end\":31928,\"start\":31918},{\"end\":31936,\"start\":31932},{\"end\":31946,\"start\":31940},{\"end\":31953,\"start\":31950},{\"end\":31959,\"start\":31957},{\"end\":31971,\"start\":31963},{\"end\":31980,\"start\":31975},{\"end\":32328,\"start\":32324},{\"end\":32341,\"start\":32334},{\"end\":32554,\"start\":32548},{\"end\":32568,\"start\":32560},{\"end\":32581,\"start\":32572},{\"end\":33023,\"start\":33014},{\"end\":33033,\"start\":33029},{\"end\":33043,\"start\":33037},{\"end\":33300,\"start\":33290},{\"end\":33313,\"start\":33304},{\"end\":33323,\"start\":33317},{\"end\":33496,\"start\":33486},{\"end\":33511,\"start\":33500},{\"end\":33712,\"start\":33708},{\"end\":33720,\"start\":33716},{\"end\":33730,\"start\":33724},{\"end\":33739,\"start\":33737},{\"end\":33745,\"start\":33743},{\"end\":33756,\"start\":33749},{\"end\":33994,\"start\":33985},{\"end\":34004,\"start\":33998},{\"end\":34014,\"start\":34008},{\"end\":34027,\"start\":34018},{\"end\":34397,\"start\":34390},{\"end\":34409,\"start\":34401},{\"end\":34417,\"start\":34413},{\"end\":34672,\"start\":34667},{\"end\":34679,\"start\":34676},{\"end\":34689,\"start\":34683},{\"end\":34704,\"start\":34693},{\"end\":34998,\"start\":34994},{\"end\":35012,\"start\":35002},{\"end\":35022,\"start\":35016},{\"end\":35030,\"start\":35026},{\"end\":35233,\"start\":35230},{\"end\":35242,\"start\":35237},{\"end\":35254,\"start\":35246},{\"end\":35262,\"start\":35258},{\"end\":35272,\"start\":35266},{\"end\":35283,\"start\":35276},{\"end\":35293,\"start\":35287},{\"end\":35306,\"start\":35299},{\"end\":35542,\"start\":35534},{\"end\":35552,\"start\":35546},{\"end\":35560,\"start\":35556},{\"end\":35570,\"start\":35567},{\"end\":35771,\"start\":35768},{\"end\":35977,\"start\":35968},{\"end\":35986,\"start\":35981},{\"end\":36276,\"start\":36268},{\"end\":36289,\"start\":36282},{\"end\":36299,\"start\":36293},{\"end\":36536,\"start\":36529},{\"end\":36548,\"start\":36540},{\"end\":36557,\"start\":36552},{\"end\":36567,\"start\":36561},{\"end\":36580,\"start\":36573},{\"end\":36592,\"start\":36584},{\"end\":37183,\"start\":37177},{\"end\":37409,\"start\":37402},{\"end\":37419,\"start\":37413},{\"end\":37876,\"start\":37862},{\"end\":37887,\"start\":37880},{\"end\":37896,\"start\":37891}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":13121800},\"end\":25490,\"start\":25302},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":868535},\"end\":25740,\"start\":25492},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":575890},\"end\":25931,\"start\":25742},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":10116609},\"end\":26232,\"start\":25933},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":2972357},\"end\":26606,\"start\":26234},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":13272863},\"end\":26936,\"start\":26608},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":928608},\"end\":27222,\"start\":26938},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":10315654},\"end\":27467,\"start\":27224},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1539668},\"end\":27696,\"start\":27469},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":16610834},\"end\":27891,\"start\":27698},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":8712237},\"end\":28083,\"start\":27893},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":679163},\"end\":28271,\"start\":28085},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":10282227},\"end\":28479,\"start\":28273},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2315434},\"end\":28706,\"start\":28481},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2241593},\"end\":29028,\"start\":28708},{\"attributes\":{\"id\":\"b15\"},\"end\":29207,\"start\":29030},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":13344783},\"end\":29484,\"start\":29209},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":3518458},\"end\":29913,\"start\":29486},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":15152889},\"end\":30196,\"start\":29915},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":12365096},\"end\":30367,\"start\":30198},{\"attributes\":{\"doi\":\"arXiv:1410.1090\",\"id\":\"b20\"},\"end\":30647,\"start\":30369},{\"attributes\":{\"doi\":\"arXiv:1411.4555\",\"id\":\"b21\"},\"end\":30903,\"start\":30649},{\"attributes\":{\"doi\":\"arXiv:1412.2306\",\"id\":\"b22\"},\"end\":31158,\"start\":30905},{\"attributes\":{\"doi\":\"arXiv:1411.2539\",\"id\":\"b23\"},\"end\":31462,\"start\":31160},{\"attributes\":{\"doi\":\"arXiv:1411.4389\",\"id\":\"b24\"},\"end\":31886,\"start\":31464},{\"attributes\":{\"doi\":\"arXiv:1411.4952\",\"id\":\"b25\"},\"end\":32247,\"start\":31888},{\"attributes\":{\"doi\":\"arXiv:1411.5654\",\"id\":\"b26\"},\"end\":32513,\"start\":32249},{\"attributes\":{\"doi\":\"arXiv:1502.03671\",\"id\":\"b27\"},\"end\":32729,\"start\":32515},{\"attributes\":{\"doi\":\"arXiv:1412.8419\",\"id\":\"b28\"},\"end\":32945,\"start\":32731},{\"attributes\":{\"doi\":\"arXiv:1501.02598\",\"id\":\"b29\"},\"end\":33221,\"start\":32947},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":195908774},\"end\":33458,\"start\":33223},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":1915014},\"end\":33650,\"start\":33460},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":206597351},\"end\":33897,\"start\":33652},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":18883184},\"end\":34320,\"start\":33899},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":14579301},\"end\":34545,\"start\":34322},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":3104920},\"end\":34913,\"start\":34547},{\"attributes\":{\"id\":\"b36\"},\"end\":35183,\"start\":34915},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":14113767},\"end\":35466,\"start\":35185},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":11080756},\"end\":35704,\"start\":35468},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":964287},\"end\":35880,\"start\":35706},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":5923323},\"end\":36211,\"start\":35882},{\"attributes\":{\"doi\":\"arXiv:1411.5726\",\"id\":\"b41\"},\"end\":36465,\"start\":36213},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":14068874},\"end\":37130,\"start\":36467},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":1671874},\"end\":37335,\"start\":37132},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":10297558},\"end\":37794,\"start\":37337},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":7647892},\"end\":38051,\"start\":37796}]", "bib_title": "[{\"end\":25346,\"start\":25302},{\"end\":25519,\"start\":25492},{\"end\":25788,\"start\":25742},{\"end\":25998,\"start\":25933},{\"end\":26302,\"start\":26234},{\"end\":26669,\"start\":26608},{\"end\":27018,\"start\":26938},{\"end\":27275,\"start\":27224},{\"end\":27519,\"start\":27469},{\"end\":27749,\"start\":27698},{\"end\":27932,\"start\":27893},{\"end\":28129,\"start\":28085},{\"end\":28330,\"start\":28273},{\"end\":28546,\"start\":28481},{\"end\":28790,\"start\":28708},{\"end\":29082,\"start\":29030},{\"end\":29278,\"start\":29209},{\"end\":29547,\"start\":29486},{\"end\":30008,\"start\":29915},{\"end\":30231,\"start\":30198},{\"end\":33286,\"start\":33223},{\"end\":33482,\"start\":33460},{\"end\":33704,\"start\":33652},{\"end\":33981,\"start\":33899},{\"end\":34386,\"start\":34322},{\"end\":34663,\"start\":34547},{\"end\":34990,\"start\":34915},{\"end\":35226,\"start\":35185},{\"end\":35530,\"start\":35468},{\"end\":35760,\"start\":35706},{\"end\":35964,\"start\":35882},{\"end\":36523,\"start\":36467},{\"end\":37171,\"start\":37132},{\"end\":37398,\"start\":37337},{\"end\":37858,\"start\":37796}]", "bib_author": "[{\"end\":25359,\"start\":25348},{\"end\":25370,\"start\":25359},{\"end\":25532,\"start\":25521},{\"end\":25543,\"start\":25532},{\"end\":25554,\"start\":25543},{\"end\":25565,\"start\":25554},{\"end\":25575,\"start\":25565},{\"end\":25587,\"start\":25575},{\"end\":25802,\"start\":25790},{\"end\":25814,\"start\":25802},{\"end\":25822,\"start\":25814},{\"end\":26012,\"start\":26000},{\"end\":26023,\"start\":26012},{\"end\":26031,\"start\":26023},{\"end\":26037,\"start\":26031},{\"end\":26045,\"start\":26037},{\"end\":26055,\"start\":26045},{\"end\":26065,\"start\":26055},{\"end\":26316,\"start\":26304},{\"end\":26323,\"start\":26316},{\"end\":26332,\"start\":26323},{\"end\":26342,\"start\":26332},{\"end\":26351,\"start\":26342},{\"end\":26359,\"start\":26351},{\"end\":26372,\"start\":26359},{\"end\":26380,\"start\":26372},{\"end\":26391,\"start\":26380},{\"end\":26400,\"start\":26391},{\"end\":26682,\"start\":26671},{\"end\":26693,\"start\":26682},{\"end\":26706,\"start\":26693},{\"end\":26715,\"start\":26706},{\"end\":26729,\"start\":26715},{\"end\":26744,\"start\":26729},{\"end\":26755,\"start\":26744},{\"end\":27030,\"start\":27020},{\"end\":27039,\"start\":27030},{\"end\":27054,\"start\":27039},{\"end\":27291,\"start\":27277},{\"end\":27302,\"start\":27291},{\"end\":27312,\"start\":27302},{\"end\":27322,\"start\":27312},{\"end\":27330,\"start\":27322},{\"end\":27529,\"start\":27521},{\"end\":27538,\"start\":27529},{\"end\":27547,\"start\":27538},{\"end\":27553,\"start\":27547},{\"end\":27566,\"start\":27553},{\"end\":27760,\"start\":27751},{\"end\":27769,\"start\":27760},{\"end\":27780,\"start\":27769},{\"end\":27943,\"start\":27934},{\"end\":27953,\"start\":27943},{\"end\":27963,\"start\":27953},{\"end\":27974,\"start\":27963},{\"end\":28139,\"start\":28131},{\"end\":28149,\"start\":28139},{\"end\":28343,\"start\":28332},{\"end\":28353,\"start\":28343},{\"end\":28560,\"start\":28548},{\"end\":28570,\"start\":28560},{\"end\":28579,\"start\":28570},{\"end\":28800,\"start\":28792},{\"end\":28808,\"start\":28800},{\"end\":28818,\"start\":28808},{\"end\":28833,\"start\":28818},{\"end\":28845,\"start\":28833},{\"end\":29093,\"start\":29084},{\"end\":29105,\"start\":29093},{\"end\":29294,\"start\":29280},{\"end\":29305,\"start\":29294},{\"end\":29313,\"start\":29305},{\"end\":29321,\"start\":29313},{\"end\":29560,\"start\":29549},{\"end\":29569,\"start\":29560},{\"end\":29584,\"start\":29569},{\"end\":29596,\"start\":29584},{\"end\":29607,\"start\":29596},{\"end\":29617,\"start\":29607},{\"end\":29627,\"start\":29617},{\"end\":29637,\"start\":29627},{\"end\":29645,\"start\":29637},{\"end\":29656,\"start\":29645},{\"end\":29667,\"start\":29656},{\"end\":29680,\"start\":29667},{\"end\":30023,\"start\":30010},{\"end\":30032,\"start\":30023},{\"end\":30042,\"start\":30032},{\"end\":30242,\"start\":30233},{\"end\":30259,\"start\":30242},{\"end\":30268,\"start\":30259},{\"end\":30434,\"start\":30427},{\"end\":30440,\"start\":30434},{\"end\":30448,\"start\":30440},{\"end\":30456,\"start\":30448},{\"end\":30468,\"start\":30456},{\"end\":30709,\"start\":30698},{\"end\":30719,\"start\":30709},{\"end\":30729,\"start\":30719},{\"end\":30738,\"start\":30729},{\"end\":30984,\"start\":30972},{\"end\":30995,\"start\":30984},{\"end\":31244,\"start\":31235},{\"end\":31261,\"start\":31244},{\"end\":31272,\"start\":31261},{\"end\":31558,\"start\":31547},{\"end\":31573,\"start\":31558},{\"end\":31587,\"start\":31573},{\"end\":31599,\"start\":31587},{\"end\":31614,\"start\":31599},{\"end\":31624,\"start\":31614},{\"end\":31635,\"start\":31624},{\"end\":31896,\"start\":31888},{\"end\":31905,\"start\":31896},{\"end\":31916,\"start\":31905},{\"end\":31930,\"start\":31916},{\"end\":31938,\"start\":31930},{\"end\":31948,\"start\":31938},{\"end\":31955,\"start\":31948},{\"end\":31961,\"start\":31955},{\"end\":31973,\"start\":31961},{\"end\":31982,\"start\":31973},{\"end\":32330,\"start\":32322},{\"end\":32343,\"start\":32330},{\"end\":32556,\"start\":32546},{\"end\":32570,\"start\":32556},{\"end\":32583,\"start\":32570},{\"end\":33025,\"start\":33012},{\"end\":33035,\"start\":33025},{\"end\":33045,\"start\":33035},{\"end\":33302,\"start\":33288},{\"end\":33315,\"start\":33302},{\"end\":33325,\"start\":33315},{\"end\":33498,\"start\":33484},{\"end\":33513,\"start\":33498},{\"end\":33714,\"start\":33706},{\"end\":33722,\"start\":33714},{\"end\":33732,\"start\":33722},{\"end\":33741,\"start\":33732},{\"end\":33747,\"start\":33741},{\"end\":33758,\"start\":33747},{\"end\":33996,\"start\":33983},{\"end\":34006,\"start\":33996},{\"end\":34016,\"start\":34006},{\"end\":34029,\"start\":34016},{\"end\":34399,\"start\":34388},{\"end\":34411,\"start\":34399},{\"end\":34419,\"start\":34411},{\"end\":34674,\"start\":34665},{\"end\":34681,\"start\":34674},{\"end\":34691,\"start\":34681},{\"end\":34706,\"start\":34691},{\"end\":35000,\"start\":34992},{\"end\":35014,\"start\":35000},{\"end\":35024,\"start\":35014},{\"end\":35032,\"start\":35024},{\"end\":35235,\"start\":35228},{\"end\":35244,\"start\":35235},{\"end\":35256,\"start\":35244},{\"end\":35264,\"start\":35256},{\"end\":35274,\"start\":35264},{\"end\":35285,\"start\":35274},{\"end\":35295,\"start\":35285},{\"end\":35308,\"start\":35295},{\"end\":35544,\"start\":35532},{\"end\":35554,\"start\":35544},{\"end\":35562,\"start\":35554},{\"end\":35572,\"start\":35562},{\"end\":35773,\"start\":35762},{\"end\":35979,\"start\":35966},{\"end\":35988,\"start\":35979},{\"end\":36278,\"start\":36266},{\"end\":36291,\"start\":36278},{\"end\":36301,\"start\":36291},{\"end\":36538,\"start\":36525},{\"end\":36550,\"start\":36538},{\"end\":36559,\"start\":36550},{\"end\":36569,\"start\":36559},{\"end\":36582,\"start\":36569},{\"end\":36594,\"start\":36582},{\"end\":37185,\"start\":37173},{\"end\":37411,\"start\":37400},{\"end\":37421,\"start\":37411},{\"end\":37878,\"start\":37860},{\"end\":37889,\"start\":37878},{\"end\":37898,\"start\":37889}]", "bib_venue": "[{\"end\":25377,\"start\":25370},{\"end\":25591,\"start\":25587},{\"end\":25826,\"start\":25822},{\"end\":26069,\"start\":26065},{\"end\":26404,\"start\":26400},{\"end\":26759,\"start\":26755},{\"end\":27058,\"start\":27054},{\"end\":27333,\"start\":27330},{\"end\":27571,\"start\":27566},{\"end\":27784,\"start\":27780},{\"end\":27977,\"start\":27974},{\"end\":28154,\"start\":28149},{\"end\":28358,\"start\":28353},{\"end\":28583,\"start\":28579},{\"end\":28849,\"start\":28845},{\"end\":29108,\"start\":29105},{\"end\":29325,\"start\":29321},{\"end\":29684,\"start\":29680},{\"end\":30045,\"start\":30042},{\"end\":30272,\"start\":30268},{\"end\":30425,\"start\":30369},{\"end\":30696,\"start\":30649},{\"end\":30970,\"start\":30905},{\"end\":31233,\"start\":31160},{\"end\":31545,\"start\":31464},{\"end\":32038,\"start\":31997},{\"end\":32320,\"start\":32249},{\"end\":32544,\"start\":32515},{\"end\":32799,\"start\":32731},{\"end\":33010,\"start\":32947},{\"end\":33329,\"start\":33325},{\"end\":33531,\"start\":33513},{\"end\":33762,\"start\":33758},{\"end\":34097,\"start\":34029},{\"end\":34423,\"start\":34419},{\"end\":34710,\"start\":34706},{\"end\":35037,\"start\":35032},{\"end\":35312,\"start\":35308},{\"end\":35575,\"start\":35572},{\"end\":35785,\"start\":35773},{\"end\":36036,\"start\":35988},{\"end\":36264,\"start\":36213},{\"end\":36700,\"start\":36594},{\"end\":37210,\"start\":37185},{\"end\":37508,\"start\":37421},{\"end\":37902,\"start\":37898},{\"end\":36793,\"start\":36702},{\"end\":37582,\"start\":37510}]"}}}, "year": 2023, "month": 12, "day": 17}