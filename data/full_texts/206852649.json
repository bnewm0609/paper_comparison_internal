{"id": 206852649, "updated": "2023-09-29 13:19:32.227", "metadata": {"title": "SemanticFusion: Dense 3D Semantic Mapping with Convolutional Neural Networks", "authors": "[{\"first\":\"John\",\"last\":\"McCormac\",\"middle\":[]},{\"first\":\"Ankur\",\"last\":\"Handa\",\"middle\":[]},{\"first\":\"Andrew\",\"last\":\"Davison\",\"middle\":[]},{\"first\":\"Stefan\",\"last\":\"Leutenegger\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2016, "month": 9, "day": 16}, "abstract": "Ever more robust, accurate and detailed mapping using visual sensing has proven to be an enabling factor for mobile robots across a wide variety of applications. For the next level of robot intelligence and intuitive user interaction, maps need extend beyond geometry and appearence - they need to contain semantics. We address this challenge by combining Convolutional Neural Networks (CNNs) and a state of the art dense Simultaneous Localisation and Mapping (SLAM) system, ElasticFusion, which provides long-term dense correspondence between frames of indoor RGB-D video even during loopy scanning trajectories. These correspondences allow the CNN's semantic predictions from multiple view points to be probabilistically fused into a map. This not only produces a useful semantic 3D map, but we also show on the NYUv2 dataset that fusing multiple predictions leads to an improvement even in the 2D semantic labelling over baseline single frame predictions. We also show that for a smaller reconstruction dataset with larger variation in prediction viewpoint, the improvement over single frame segmentation increases. Our system is efficient enough to allow real-time interactive use at frame-rates of approximately 25Hz.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1609.05130", "mag": "2951884519", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icra/McCormacHDL17", "doi": "10.1109/icra.2017.7989538"}}, "content": {"source": {"pdf_hash": "0c65245bff0004961a5173709400479addbb9ee1", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1609.05130v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1609.05130", "status": "GREEN"}}, "grobid": {"id": "a10a20d32c2047e9d9e8f8bf90ef042dcd8a9830", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0c65245bff0004961a5173709400479addbb9ee1.txt", "contents": "\nSemanticFusion: Dense 3D Semantic Mapping with Convolutional Neural Networks\n\n\nJohn Mccormac \nDyson Robotics Lab\nImperial College London\n\n\nAnkur Handa \nDyson Robotics Lab\nImperial College London\n\n\nAndrew Davison \nDyson Robotics Lab\nImperial College London\n\n\nStefan Leutenegger \nDyson Robotics Lab\nImperial College London\n\n\nSemanticFusion: Dense 3D Semantic Mapping with Convolutional Neural Networks\n\nEver more robust, accurate and detailed mapping using visual sensing has proven to be an enabling factor for mobile robots across a wide variety of applications. For the next level of robot intelligence and intuitive user interaction, maps need extend beyond geometry and appearence -they need to contain semantics. We address this challenge by combining Convolutional Neural Networks (CNNs) and a state of the art dense Simultaneous Localisation and Mapping (SLAM) system, ElasticFusion, which provides long-term dense correspondence between frames of indoor RGB-D video even during loopy scanning trajectories. These correspondences allow the CNN's semantic predictions from multiple view points to be probabilistically fused into a map. This not only produces a useful semantic 3D map, but we also show on the NYUv2 dataset that fusing multiple predictions leads to an improvement even in the 2D semantic labelling over baseline single frame predictions. We also show that for a smaller reconstruction dataset with larger variation in prediction viewpoint, the improvement over single frame segmentation increases. Our system is efficient enough to allow real-time interactive use at frame-rates of \u224825Hz.\n\nI. INTRODUCTION\n\nThe inclusion of rich semantic information within a dense map enables a much greater range of functionality than geometry alone. For instance, in domestic robotics, a simple fetching task requires knowledge of both what something is, as well as where it is located. As a specific example, thanks to sharing of the same spatial and semantic understanding between user and robot, we may issue commands such as 'fetch the coffee mug from the nearest table on your right'. Similarly, the ability to query semantic information within a map is useful for humans directly, providing a database for answering spoken queries about the semantics of a previously made map; 'How many chairs do we have in the conference room? What is the distance between the lectern and its nearest chair?' In this work, we combine the geometric information from a state-of-the-art SLAM system ElasticFusion [25] with recent advances in semantic segmentation using Convolutional Neural Networks (CNNs).\n\nOur approach is to use the SLAM system to provide correspondences from the 2D frame into a globally consistent 3D map. This allows the CNN's semantic predictions from multiple viewpoints to be probabilistically fused into a dense semantically annotated map, as shown in Figure 1. Elas-ticFusion is particularly suitable for fusing semantic labels because its surfel-based surface representation is automatically deformed to remain consistent after the small and large loop closures which would frequently occur during typical interactive use by an agent (whether human or robot). As the surface representation is deformed and corrected, individual surfels remain persistently associated with real-world entities and this enables long-term fusion of per-frame semantic predictions over wide changes in viewpoint.\n\nThe geometry of the map itself also provides useful information which can be used to efficiently regularise the final predictions. Our pipeline is designed to work online, and although we have not focused on performance, the efficiency of each component leads to a real-time capable (\u2248 25Hz) interactive system. The resulting map could also be used as a basis for more expensive offline processing to further improve both the geometry and the semantics; however that has not been explored in the current work.\n\nWe evaluate the accuracy of our system on the NYUv2 dataset, and show that by using information from the unlabelled raw video footage we can improve upon baseline approaches performing segmentation using only a single frame. This suggests the inclusion of SLAM not only provides an immediately useful semantic 3D map, but it suggests that many state-of-the art 2D single frame semantic segmentation approaches may be boosted in performance when linked with SLAM.\n\nThe NYUv2 dataset was not taken with full room reconstruction in mind, and often does not provide significant variations in viewpoints for a given scene. To explore the benefits of SemanticFusion within a more thorough reconstruction, we developed a small dataset of a reconstructed office room, annotated with the NYUv2 semantic classes. Within this dataset we witness a more significant improvement in segmentation accuracy over single frame 2D segmentation. This indicates that the system is particularly well suited to longer duration scans with wide viewpoint variation aiding to disambiguate the single-view 2D semantics.\n\n\nII. RELATED WORK\n\nThe works most closely related are St\u00fcckler et al. [23] and Hermans et al. [8]; both aim towards a dense, semantically annotated 3D map of indoor scenes. They both obtain perpixel label predictions for incoming frames using Random Decision Forests, whereas ours exploits recent advances in Convolutional Neural Networks that provide state-of-the-art accuracy, with a real-time capable run-time performance. They both fuse predictions from different viewpoints in a classic Bayesian framework. St\u00fcckler et al. [23] used a Multi-Resolution Surfel Map-based SLAM system capable of operating at 12.8Hz, however unlike our system they do not maintain a single global semantic map as local key frames store aggregated semantic information and these are subject to graph optimisation in each frame. Hermans et al. [8] did not use the capability of a full SLAM system with explicit loop closure: they registered the predictions in the reference frames using only camera tracking. Their run-time performance was 4.6Hz, which would prohibit processing a live video feed, whereas our system is capable of operating online and interactively. As here, they regularised their predictions using Kr\u00e4henb\u00fchl and Koltun's [13] fully-connected CRF inference scheme to obtain a final semantic map.\n\nPrevious work by Salas-Moreno et al. aimed to create a fully capable SLAM system, SLAM++ [19], which maps indoor scenes at the level of semantically defined objects. However, their method is limited to mapping objects that are present in a pre-defined database. It also does not provide the dense labelling of entire scenes that we aim for in this work, which also includes walls, floors, doors, and windows which are equally important to describe the extent of the room. Additionally, the features they use to match template models are hand-crafted unlike our CNN features that are learned in an end-to-end fashion with large training datasets.\n\nThe majority of other approaches to indoor semantic labelling either focuses on offline batch mapping methods [24], [12] or on single-frame 2D segmentations which do not aim to produce a semantically annotated 3D map [3], [20], [15], [22]. Valentin et al. [24] used a CRF and a perpixel labelling from a variant of TextonBoost to reconstruct semantic maps of both indoor and outdoor scenes. This produces a globally consistent 3D map, however inference is performed on the whole mesh once instead of incrementally fusing the predictions online. Koppula et al. [12] also tackle the problem on a completed 3D map, forming segments of the map into nodes of a graphical model and using handcrafted geometric and visual features as edge potentials to infer the final semantic labelling.\n\nOur semantic mapping pipeline is inspired by the recent success of Convolution Neural Networks in semantic labelling and segmentation tasks [14], [16], [17]. CNNs have proven capable of both state-of-the-art accuracy and efficient test-time performance. They have have exhibited these capabilities on numerous datasets and a variety of data modalities, in particular RGB [17], [16], Depth [1], [7] and Normals [2], [4], [6], [5]. In this work we build on the CNN model proposed by Noh et. al. [17], but modify it to take advantage of the directly available depth data in a manner that does not require significant additional pre-processing.\n\n\nIII. METHOD\n\nOur SemanticFusion pipeline is composed of three separate units; a real-time SLAM system ElasticFusion, a Convolutional Neural Network, and a Bayesian update scheme, as illustrated in Figure 2. The role of the SLAM system is to provide correspondences between frames, and a globally consistent map of fused surfels. Separately, the CNN receives a 2D image (for our architecture this is RGBD, for Eigen et al. [2] it also includes estimated normals), and returns a set of per pixel class probabilities. Finally, a Bayesian update scheme keeps track of the class probability distribution for each surfel, and uses the correspondences provided by the SLAM system to update those probabilities based on the CNN's predictions. Finally, we also experiment with a CRF regularisation scheme to use the geometry of the map itself to improve the semantic predictions [8], [13]. The following section outlines each of these components in more detail.\n\n\nA. SLAM Mapping\n\nWe choose ElasticFusion as our SLAM system. 1 For each arriving frame, k, ElasticFusion tracks the camera pose via a combined ICP and RGB alignment, to yield a new pose T W C , where W denotes the World frame and C the camera frame. New surfels are added into our map using this camera pose, and existing surfel information is combined with new evidence to refine their positions, normals, and colour information. Additional checks for a loop closure event run in parallel and the map is optimised immediately upon a loop closure detection.\n\nThe deformation graph and surfel based representation of ElasticFusion lend themselves naturally to the task at hand, allow probability distributions to be 'carried along' with the surfels during loop closure, and also fusing new depth readings to update the surfel's depth and normal information, without destroying the surfel, or its underlying probability distribution. It operates at real-time frame-rates at VGA resolution and so can be used both interactively by a human or in robotic applications. We used the default parameters in the public implementation, except for the depth cutoff, which we extend from 3m to 8m to allow reconstruction to occur on sequences with geometry outside of the 3m range.\n\n\nB. CNN Architecture\n\nOur CNN is implemented in caffe [11] and adopts the Deconvolutional Semantic Segmentation network architecture proposed by Noh et. al. [17]. Their architecture is itself based on the VGG 16-layer network [21], but with the addition of max unpooling and deconvolutional layers which are trained to output a dense pixel-wise semantic probability map. This CNN was trained for RGB input, and in the following sections when using a network with this setup we describe it RGB-CNN.\n\nGiven the availability of depth data, we modified the original network architecture to accept depth information as a fourth channel. Unfortunately, the depth modality lacks the large scale training datasets of its RGB counterpart. The NYUv2 dataset only consists of 795 labelled training images. To effectively use depth, we initialized the depth filters with the average intensity of the other three inputs, which had already been trained on a large dataset, and converted it from the 0-255 colour range to the 0-8m depth range by increasing the weights by a factor of \u2248 32\u00d7.\n\nWe rescale incoming images to the native 224\u00d7224 resolution for our CNNs, using bilinear interpolation for RGB, and nearest neighbour for depth. In our experiments with Eigen et. al.'s implementation we rescale the inputs in the same manner to 320\u00d7240 resolution. We upsample the network output probabilites to full 640\u00d7480 image resolution using nearest neighbour when applying the update to surfels, described in the section below.\n\n\nC. Incremental Semantic Label Fusion\n\nIn addition to normal and location information, each surfel (index s) in our map, M, stores a discrete probability distribution, P (L s = l i ) over the set of class labels, l i \u2208 L. Each newly generated surfel is initialised with a uniform distribution over the semantic classes, as we begin with no a priori evidence as to its latent classification.\n\nAfter a prespecified number of frames, we perform a forward pass of the CNN with the image I k coming directly from the camera. Depending on the CNN architecture, this image can include any combination of RGB, depth, or normals. Given the data I k of the k th image, the output of the CNN is interpreted in a simplified manner as a per-pixel independent probability distribution over the class labels P (O u = l i |I k ), with u denoting pixel coordinates.\n\nUsing the tracked camera pose T W C , we associate every surfel at a given 3D location W x(s) in the map, with pixel coordinates u via the camera projection u(s, k) = \u03c0(T CW (k) W x(s)), employing the homogeneous transformation matrix T CW (k) = T \u22121 W C (k) and using homogeneous 3D coordinates. This enables us to update all the surfels in the visible set V k \u2286 M with the corresponding probability distribution by means of a recursive Bayesian update\nP (l i |I 1,...,k ) = 1 Z P (l i |I 1,...,k\u22121 )P (O u(s,k) = l i |I k ),(1)\nwhich is applied to all label probabilities per surfel, finally normalising with constant Z to yield a proper distribution.\n\nIt is the SLAM correspondences that allow us to accurately associate label hypotheses from multiple images and combine evidence in a Bayesian way. The following section discusses how the na\u00efve independence approximation employed so far can be mitigated, allowing semantic information to be propagated spatially when semantics are fused from different viewpoints.\n\n\nD. Map Regularisation\n\nWe explore the benefits of using map geometry to regularise predictions by applying a fully-connected CRF with Gaussian edge potentials to surfels in the 3D world frame, as in the work of Hermans et al. [8], [13]. We do not use the CRF to arrive at a final prediction for each surfel, but instead use it incrementally to update the probability distributions. In our work, we treat each surfel as a node in the graph. The algorithm uses the mean-field approximation and a message passing scheme to efficiently infer the latent variables that approximately minimise the Gibbs energy E of a labelling, x, in a fully-connected graph, where x s \u2208 {l i } denotes a given labelling for the surfel with index s.\n\nThe energy E(x) consists of two parts, the unary data term \u03c8 u (x s ) is a function of a given label, and is parameterised by the internal probability distribution of the surfel from fusing multiple CNN predictions as described above. The pairwise smoothness term, \u03c8 p (x s , x s ) is a function of the labelling of two connected surfels in the graph, and is parameterised by the geometry of the map:\nE(x) = s \u03c8 u (x s ) + s<s \u03c8 p (x s , x s ).(2)\nFor the data term we simply use the negative logarithm of the chosen labelling's probability for a given surfel, \u03c8 u (x s ) = \u2212log(P (L s = x s |I 1,...,k )).\n\n(\n\nIn the scheme proposed by Kr\u00e4henb\u00fchl and Koltun [13] the smoothness term is constrained to be a linear combination of K Gaussian edge potential kernels, where f s denotes some feature vector for surfel, s, and in our case \u00b5(x s , x s ) is given by the Potts model, \u00b5(x s , x s ) = [x s = x s ]:\n\u03c8 p (x s , x s ) = \u00b5(x s , x s ) K m=1 w (m) k (m) (f s , f s ) . (4)\nFollowing previous work [8] we use two pairwise potentials; a bilateral appearance potential seeking to closely tie together surfels with both a similar position and appearance, and a spatial smoothing potential which enforces smooth predictions in areas with similar surface normals:\nk 1 (f s , f s ) = exp \u2212 |p s \u2212 p s | 2 2\u03b8 2 \u03b1 \u2212 |c s \u2212 c s | 2 2\u03b8 2 \u03b2 ,(5)k 2 (f s , f s ) = exp \u2212 |p s \u2212 p s | 2 2\u03b8 2 \u03b1 \u2212 |n s \u2212 n s | 2 2\u03b8 2 \u03b3 .(6)\nWe chose unit standard deviations of \u03b8 \u03b1 = 0.05m in the spatial domain, \u03b8 \u03b2 = 20 in the RGB colour domain, and \u03b8 \u03b3 = 0.1 radians in the angular domain. We did not tune these parameters for any particular dataset. We also maintained w 1 of 10 and w 2 of 3 for all experiments. These were the default settings in Kr\u00e4henb\u00fchl and Koltun's public implementation 2 [13] .\n\n\nIV. EXPERIMENTS\n\n\nA. Network Training\n\nWe initialise our CNNs with weights from Noh et. al. [17] trained for segmentation on the PASCAL VOC 2012 segmentation dataset [3]. For depth input we initialise the fourth channel as described in Section III-B, above. We finetuned this network on the training set of the NYUv2 dataset for the 13 semantic classes defined by Couprie et al. [1].\n\nFor optimisation we used standard stochastic gradient descent, with a learning rate of 0.01, momentum of 0.9, and weight decay of 5 \u00d7 10 \u22124 . After 10k iterations we reduced the learning rate to 1 \u00d7 10 \u22123 . We use a mini-batch size of 64, and trained the networks for a total of 20k iterations over the course of 2 days on an Nvidia GTX Titan X.\n\n\nB. Reconstruction Dataset\n\nWe produced a small experimental RGB-D reconstruction dataset, which aimed for a relatively complete reconstruction of an office room. The trajectory used is notably more loopy, both locally and globally, than the NYUv2 dataset which typically consists of a single back and forth sweep. We believe the trajectory in our dataset is more representative of the scanning motion an active agent may perform when inspecting a scene.\n\nWe also took a different approach to manual annotation of this data, by using a 3D tool we developed to annotate the surfels of the final 3D reconstruction with the 13 NYUv2 semantic classes under consideration (only 9 were present). We then automatically generated 2D labellings for any frame in the input sequence via projection. The tool, and the 2 Available from: http://www.philkr.net/home/densecrf Fig. 3: Our office reconstruction dataset: On the left are the captured RGB and Depth images. On the right, is our 3D reconstruction and annotation. Inset into that is the final ground truth rendered labelling we use for testing.\n\nresulting annotations are depicted in Figure 3. Every 100 th frame of the sequence was used as a test sample to validate our predictions against the annotated ground truth, resulting in 49 test frames.\n\n\nC. CNN and CRF Update Frequency Experiments\n\nWe used the dataset to evaluate the accuracy of our system when only performing a CNN prediction on a subset of the incoming video frames. We used the RGB-CNN described above, and evaluated the accuracy of our system when performing a prediction on every 2 n frames, where n \u2208 {0..7}. We calculate the average frame-rate based upon the run-time analysis discussed in Section IV-F. As shown in Figure 4, the accuracy is highest (52.5%) when every frame is processed by the network, however this leads to a significant drop in frame-rate to 8.2Hz. Processing every 10 th frame results in a slightly reduced accuracy (49-51%), but over three times the frame-rate of 25.3Hz. This is the approach taken in all of our subsequent evaluations.\n\nWe also evaluated the effect of varying the number of frames between CRF updates ( Figure 5). We found that when applied too frequently, the CRF can 'drown out' predictions of the CNN, resulting in a significant reduction in accuracy. Performing an update every 500 frames results in a slight improvement, and so we use that as the default update rate in all subsequent experiments.\n\n\nD. Accuracy Evaluation\n\nWe evaluate the accuracy of our SemanticFusion pipeline against the accuracy achieved by a single frame CNN segmentation. The results of this evaluation are summarised in Table I. We observe that in all cases semantically fusing additional viewpoints improved the accuracy of the segmentation over a single frame system. Performance improved from 43.6% for a single frame to 48.3% when projecting the predictions from the 3D SemanticFusion map.\n\nWe also evaluate our system on the office dataset when using predictions from the state-of-the-art CNN developed by Eigen et al. 3   The average class accuracy processing every 10 th frame with a CNN, with a variable number of frames between CRF updates. If applied too frequently the CRF was detrimental to performance, and the performance improvement from the CRF was not significant for this CNN.\n\nconsistency with the rest of the system, we perform only a single forward pass of the network to calculate the output probabilities. The network requires ground truth normal information, and so to ensure the input pipeline is the same as in Eigen et al. [2], we preprocess the sequence with the MATLAB script linked to in the project page to produce the ground truth normals. With this setup we see an improvement of 2.9% over the single frame implementation with SemanticFusion, from 57.1% to 60.0%.\n\nThe performance benefit of the CRF was less clear. It provided a very small improvement of 0.5% for the Eigen network, but a slight detriment to the RGBD-CNN of 0.2%.\n\n\nE. NYU Dataset\n\nWe choose to validate our approach on the NYUv2 dataset [20], as it is one of the few datasets which provides all of the information required to evaluate semantic RGB-D reconstruction. The SUN RGB-D [22], although an order of magnitude larger than NYUv2 in terms of labelled images, does not provide the raw RGB-D videos and therefore is could not be used in our evaluation.\n\nThe NYUv2 dataset itself is still not ideally suited to the role. Many of the 206 test set video sequences exhibit significant drops in frame-rate and thus prove unsuitable for tracking and reconstruction. In our evaluations we excluded any sequence which experienced a frame-rate under 2Hz. The remaining 140 test sequences result in 360 labelled test images of the original 654 image test set in NYUv2. The results of our evaluation are presented in Table II and some qualitative results are shown in Figure 6.\n\nOverall, fusing semantic predictions resulted in a notable improvement over single frame predictions. However, the total relative gains of 2.3% for the RGBD-CNN was approximately half of the 4.7% improvement witnessed in the office reconstruction dataset. We believe this is largely a result of the style of capturing NYUv2 datasets. The primarily rotational scanning pattern often used in test trajectories does not provide as many useful different viewpoints from which to fuse independent predictions. Despite this, there is still a significant accuracy improvement over the single frame predictions.\n\nWe also improved upon the state-of-the-art Eigen et al.\n\n[2] CNN, with the class average accuracy going from 59.9% to 63.2% (+3.3%). This result clearly shows, even on this challenging dataset, the capacity of SemanticFusion to not only provide a useful semantically annotated 3D map, but also to improve the predictions of state-of-the-art 2D semantic segmentation systems.\n\nThe improvement as a result of the CRF was not particularly significant, but positive for both CNNs. Eigen's CNN saw +0.4% improvement, and the RGBD-CNN saw +0.3%. This could possibly be improved with proper tuning of edge potential weights and unit standard deviations, and the potential exists to explore many other kinds of map-based semantic regularisation schemes. We leave these explorations to future work.\n\n\nF. Run-time Performance\n\nWe benchmark the performance of our system on a random sample of 30 sequences from the NYUv2 test set. All tests were performed on an Intel Core i7-5820K 3.30GHz CPU and an NVIDIA Titan Black GPU. Our SLAM system requires 29.3ms on average to process each frame and update the map. For every frame we also update our stored surfel probability table to account for any surfels removed by the SLAM system. This process requires an additional 1.0ms. As discussed above, the other components in our system do not need to be applied for every frame. A forward pass of our CNN requires 51.2ms and our Bayesian update scheme requires a further 41.1ms. Our standard scheme performs   this every 10 frames, resulting in an average frame-rate of 25.3Hz. Our experimental CRF implementation was developed only for the CPU in C++, but the message passing algorithm adopted could lend itself to an optimised GPU implementation. The overhead of copying data from the GPU and performing inference on a single threaded CPU implementation is significant. Therefore on average, it takes 20.3s to perform 10 CRF iterations. In the evaluation above, we perform a CRF update once every 500 frames, but for online use it can be disabled entirely or applied once at the conclusion of a sequence.\n\nV. CONCLUSIONS Our results confirm the strong expectation that using a SLAM system to provide pixel-wise correspondences between frames allows the fusion of per-frame 2D segmentations into a coherent 3D semantic map. It is the first time that this has been demonstrated with a real-time, loop-closure capable approach suitable for interactive room scanning. Not only that, the incorporation of such a map led to a significant improvement in the corresponding 2D segmentation accuracy.\n\nWe exploited the flexibility of CNNs to improve the accuracy of a pretrained RGB network by incoporating an additional depth channel. In this work we opted for the simplest feasible solution to allow this new modality. Some recent work has explored other ways to incorporate depth information [9], but such an approach requires a duplication of the lower network parameters and was infeasible in our system due to GPU memory limitations. However, future research could also incorporate recent breakthroughs in CNN compression [10], which would not only enable improvements to the incorporation of other modalities, but also offer exciting new directions to enable real-time semantic segmentation on low memory and power mobile devices.\n\nWe believe that this is just the start of how knowledge from SLAM and machine-learned labelling can be brought together to enable truly powerful semantic and object-aware mapping. Our own reconstruction-focused dataset shows a much larger improvement in labelling accuracy via fusion than the NYU dataset with less varied trajectories, this underlines the importance of viewpoint variation. It also hints at the improvements that could be achieved with significantly longer trajectories, such as those of an autonomous robot in of SemanticFusion are using the RGBD-CNN with CRF after the completed trajectory, against the same networks single frame predictions. For evaluation, the black regions of SemanticFusion denoting areas without a reconstruction, are replaced with the baseline CNN predictions. Here we show only the semantic reconstruction result for clarity. The first two rows show instances where SemanticFusion has clearly improved the accuracy of the 2D annotations. The third row shows an example of a very rotational trajectory, where there is little difference as a result of fusing predictions. The final row shows an example where the trajectory was clearly not taken with reconstruction in mind, and the distant geometry leads to tracking and mapping problems even within our subset requiring 2Hz frame-rate. Cases such as this provide an advantage to the accuracy of the single frame network. the field making direct use of the semantically annotated 3D map.\n\nGoing further, it is readily apparent, as demonstrated in a so far relatively simple manner in systems like SLAM++ [19] that not just should reconstruction be used to provide correspondence to help labelling, but that labelling/recognition can make reconstruction and SLAM much more accurate and efficient. A loop-closure capable surfel map as in ElasticFusion is highly suitable for applying operations such as classspecific smoothing (as in the extreme case of planar region recognition and fitting [18]), and this will be an interesting direction. More powerful still will be to interface with explicit object instance recognition and to replace elements of the surfel model directly with 3D object models once confidence reaches a suitable threshold.\n\nFig. 1 :\n1The output of our system: On the left, a dense surfel based reconstruction from a video sequence in the NYUv2 test set. On the right the same map, semantically annotated with the classes given in the legend below.\n\nFig. 2 :\n2An overview of our pipeline: Input images are used to produce a SLAM map, and a set of probability prediction maps (here only four are shown). These maps are fused into the final dense semantic map via Bayesian updates.\n\nFig. 4 :\n4based on the VGG architecture. To maintain The class average accuracy of our RGB-CNN on the office reconstruction dataset against the number of frames skipped between fusing semantic predictions. We perform this evaluation without CRF smoothing. The right hand axis shows the estimated run-time performance in terms of FPS.\n\n\nFig. 5: The average class accuracy processing every 10 th frame with a CNN, with a variable number of frames between CRF updates. If applied too frequently the CRF was detrimental to performance, and the performance improvement from the CRF was not significant for this CNN.\n\nFig. 6 :\n6Qualitative NYUv2 test set results: The results\n\nTABLE I :\nIReconstruction dataset results: SF denotes that the labels were produced by SemanticFusion, and the results wherecaptured immediately if a frame with ground truth labelling was present. When no reconstruction is present for a pixel, we fall back to \nthe predictions of the baseline single frame network. All accuracy evaluations were performed at 320 \u00d7 240 resolution. \n\nNYUv2 Test Set: 13 Class Semantic Segmentation \n\nMethod \nbed \nbooks \nceiling \nchair \nfloor \nfurniture \nobjects \npainting \n\nsofa \ntable \ntv \nwall \nwindow \nclass avg. \npixel avg. \n\nRGBD \n62.5 60.5 35.0 51.7 92.1 54.5 61.3 72.1 34.7 26.1 32.4 86.5 53.5 55.6 62.0 \nRGBD-SF \n61.7 58.5 43.4 58.4 92.6 63.7 59.1 66.4 47.3 34.0 33.9 86.0 60.5 58.9 67.5 \nRGBD-SF-CRF \n62.0 58.4 43.3 59.5 92.7 64.4 58.3 65.8 48.7 34.3 34.3 86.3 62.3 59.2 67.9 \n\nEigen [2] \n42.3 49.1 73.1 72.4 85.7 60.8 46.5 57.3 38.9 42.1 68.5 85.5 55.8 59.9 66.5 \nEigen-SF \n47.8 50.8 79.0 73.3 90.5 62.8 46.7 64.5 45.8 46.0 70.7 88.5 55.2 63.2 69.3 \nEigen-SF-CRF \n48.3 51.5 79.0 74.7 90.8 63.5 46.9 63.6 46.5 45.9 71.5 89.4 55.6 63.6 69.9 \n\nHermans et al. [8] \n68.4 45.4 83.4 41.9 91.5 37.1 8.6 \n35.8 28.5 27.7 38.4 71.8 46.1 48.0 54.3 \n\n\n\nTABLE II :\nIINYUv2 test set results: SF denotes that the labels were produced by SemanticFusion, and the results were captured immediately if a keyframe was present. When no reconstruction is present for a pixel, we fall back to the predictions of the baseline single frame network. Note that we calculated the accuracies of[2] using their publicly available implementation. Our results are not directly comparable with Hermans et al.[8] as we only evaluate on a subset of the test set, and their annotations are not available. However, we include their results for reference. Following previous work[8] we exclude pixels without a corresponding depth measurement. All accuracy evaluations were performed at 320 \u00d7 240 resolution.\nAvailable on https://github.com/mp3guy/ElasticFusion\nWe use the publicly available network weights and implementation from: http://www.cs.nyu.edu/\u02dcdeigen/dnl/.\n\nIndoor semantic segmentation using depth information. C Couprie, C Farabet, L Najman, Y Lecun, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)C. Couprie, C. Farabet, L. Najman, and Y. LeCun, \"Indoor semantic segmentation using depth information,\" in Proceedings of the Interna- tional Conference on Learning Representations (ICLR), 2013.\n\nPredicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Architecture. D Eigen, R Fergus, Proceedings of the International Conference on Computer Vision (ICCV). the International Conference on Computer Vision (ICCV)D. Eigen and R. Fergus, \"Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-Scale Convolutional Archi- tecture,\" in Proceedings of the International Conference on Computer Vision (ICCV), 2015.\n\nThe pascal visual object classes (VOC) challenge. M Everingham, L Van Gool, C K Williams, J Winn, A Zisserman, International Journal of Computer Vision. 2M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisser- man, \"The pascal visual object classes (VOC) challenge,\" International Journal of Computer Vision (IJCV), no. 2, pp. 303-338, 2010.\n\nLearning Rich Features from RGB-D Images for Object Detection and Segmentation. S Gupta, R Girshick, P Arbelaez, J Malik, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)S. Gupta, R. Girshick, P. Arbelaez, and J. Malik, \"Learning Rich Fea- tures from RGB-D Images for Object Detection and Segmentation,\" in Proceedings of the European Conference on Computer Vision (ECCV), 2014.\n\nAligning 3D models to RGB-D images of cluttered scenes. S Gupta, P A Arbel\u00e1ez, R B Girshick, J Malik, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)S. Gupta, P. A. Arbel\u00e1ez, R. B. Girshick, and J. Malik, \"Aligning 3D models to RGB-D images of cluttered scenes,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.\n\nAligning 3D Models to RGB-D Images of Cluttered Scenes. S Gupta, P A Arbel\u00e1ez, R B Girshick, J Malika, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)S. Gupta, P. A. Arbel\u00e1ez, R. B. Girshick, and J. Malika, \"Aligning 3D Models to RGB-D Images of Cluttered Scenes,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.\n\nSceneNet: Understanding Real World Indoor Scenes With Synthetic Data. A Handa, V P\u0203tr\u0203ucean, V Badrinarayanan, S Stent, R Cipolla, arXiv:1511.07041arXiv preprintA. Handa, V. P\u0203tr\u0203ucean, V. Badrinarayanan, S. Stent, and R. Cipolla, \"SceneNet: Understanding Real World Indoor Scenes With Synthetic Data,\" arXiv preprint arXiv:1511.07041, 2015.\n\nDense 3d semantic mapping of indoor scenes from rgb-d images. A Hermans, G Floros, B Leibe, Proceedings of the IEEE International Conference on Robotics and Automation (ICRA). the IEEE International Conference on Robotics and Automation (ICRA)A. Hermans, G. Floros, and B. Leibe, \"Dense 3d semantic mapping of indoor scenes from rgb-d images,\" in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2014.\n\nCross-Modal Adaptation for RGB-D Detection. J Hoffman, S Gupta, J Leong, G S , T Darrell, Proceedings of the IEEE International Conference on Robotics and Automation (ICRA). the IEEE International Conference on Robotics and Automation (ICRA)J. Hoffman, S. Gupta, J. Leong, G. S., and T. Darrell, \"Cross- Modal Adaptation for RGB-D Detection,\" in Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), 2016.\n\nSqueezenet: Alexnet-level accuracy with 50x fewer parameters and <1mb model size. F N Iandola, M W Moskewicz, K Ashraf, S Han, W J Dally, K Keutzer, CoRRF. N. Iandola, M. W. Moskewicz, K. Ashraf, S. Han, W. J. Dally, and K. Keutzer, \"Squeezenet: Alexnet-level accuracy with 50x fewer parameters and <1mb model size,\" CoRR, 2016.\n\nCaffe: Convolutional architecture for fast feature embedding. Y Jia, E Shelhamer, J Donahue, S Karayev, J Long, R Girshick, S Guadarrama, T Darrell, arXiv:1408.5093arXiv preprintY. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell, \"Caffe: Convolutional architecture for fast feature embedding,\" arXiv preprint arXiv:1408.5093, 2014.\n\nSemantic Labeling of 3D Point Clouds for Indoor Scenes. H S Koppula, A Anand, T Joachims, A Saxena, Neural Information Processing Systems (NIPS). H. S. Koppula, A. Anand, T. Joachims, and A. Saxena, \"Semantic Labeling of 3D Point Clouds for Indoor Scenes,\" in Neural Information Processing Systems (NIPS), 2011.\n\nEfficient Inference in Fully Connected CRFs with Gaussian Edge Potentials. P Kr\u00e4henb\u00fchl, V Koltun, Neural Information Processing Systems (NIPS). P. Kr\u00e4henb\u00fchl and V. Koltun, \"Efficient Inference in Fully Connected CRFs with Gaussian Edge Potentials,\" in Neural Information Process- ing Systems (NIPS), 2011.\n\nImageNet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G Hinton, Neural Information Processing Systems (NIPS). A. Krizhevsky, I. Sutskever, and G. Hinton, \"ImageNet classification with deep convolutional neural networks,\" in Neural Information Processing Systems (NIPS), 2012.\n\nMicrosoft COCO: Common objects in context. T.-Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Doll\u00e1r, C L Zitnick, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r, and C. L. Zitnick, \"Microsoft COCO: Common objects in context,\" in Proceedings of the European Conference on Computer Vision (ECCV), 2014, pp. 740-755.\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)J. Long, E. Shelhamer, and T. Darrell, \"Fully convolutional networks for semantic segmentation,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015.\n\nLearning deconvolution network for semantic segmentation. H Noh, S Hong, B Han, arXiv:1505.04366arXiv preprintH. Noh, S. Hong, and B. Han, \"Learning deconvolution network for semantic segmentation,\" arXiv preprint arXiv:1505.04366, 2015.\n\nDense Planar SLAM. R F Salas-Moreno, B Glocker, P H J Kelly, A J Davison, Proceedings of the International Symposium on Mixed and Augmented Reality (ISMAR). the International Symposium on Mixed and Augmented Reality (ISMAR)R. F. Salas-Moreno, B. Glocker, P. H. J. Kelly, and A. J. Davison, \"Dense Planar SLAM,\" in Proceedings of the International Symposium on Mixed and Augmented Reality (ISMAR), 2014.\n\nSLAM++: Simultaneous Localisation and Mapping at the Level of Objects. R F Salas-Moreno, R A Newcombe, H Strasdat, P H J Kelly, A J Davison, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)R. F. Salas-Moreno, R. A. Newcombe, H. Strasdat, P. H. J. Kelly, and A. J. Davison, \"SLAM++: Simultaneous Localisation and Mapping at the Level of Objects,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013. [Online].\n\n. 10.1109/CVPR.2013.178Available: http://dx.doi.org/10.1109/CVPR.2013.178\n\nIndoor segmentation and support inference from RGBD images. N Silberman, D Hoiem, P Kohli, R Fergus, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, \"Indoor segmen- tation and support inference from RGBD images,\" in Proceedings of the European Conference on Computer Vision (ECCV), 2012.\n\nVery Deep Convolutional Networks for Large-Scale Image Recognition. K Simonyan, A Zisserman, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)K. Simonyan and A. Zisserman, \"Very Deep Convolutional Networks for Large-Scale Image Recognition,\" in Proceedings of the Interna- tional Conference on Learning Representations (ICLR), 2015.\n\nSUN RGB-D: A RGB-D scene understanding benchmark suite. S Song, S P Lichtenberg, J Xiao, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)S. Song, S. P. Lichtenberg, and J. Xiao, \"SUN RGB-D: A RGB-D scene understanding benchmark suite,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 567-576.\n\nMulti-resolution surfel maps for efficient dense 3d modeling and tracking. J St\u00fcckler, B Waldvogel, H Schulz, S Behnke, Journal of Real-Time Image Processing JRTIP. 104J. St\u00fcckler, B. Waldvogel, H. Schulz, and S. Behnke, \"Multi-resolution surfel maps for efficient dense 3d modeling and tracking,\" Journal of Real-Time Image Processing JRTIP, vol. 10, no. 4, pp. 599-609, 2015.\n\nMesh Based Semantic Modelling for Indoor and Outdoor Scenes. J Valentin, S Sengupta, J Warrell, A Shahrokni, P Torr, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR). the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)J. Valentin, S. Sengupta, J. Warrell, A. Shahrokni, and P. Torr, \"Mesh Based Semantic Modelling for Indoor and Outdoor Scenes,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013.\n\nElasticFusion: Dense SLAM without a pose graph. T Whelan, S Leutenegger, R F Salas-Moreno, B Glocker, A J Davison, Proceedings of Robotics: Science and Systems (RSS). Robotics: Science and Systems (RSS)T. Whelan, S. Leutenegger, R. F. Salas-Moreno, B. Glocker, and A. J. Davison, \"ElasticFusion: Dense SLAM without a pose graph,\" in Proceedings of Robotics: Science and Systems (RSS), 2015.\n", "annotations": {"author": "[{\"end\":139,\"start\":80},{\"end\":197,\"start\":140},{\"end\":258,\"start\":198},{\"end\":323,\"start\":259}]", "publisher": null, "author_last_name": "[{\"end\":93,\"start\":85},{\"end\":151,\"start\":146},{\"end\":212,\"start\":205},{\"end\":277,\"start\":266}]", "author_first_name": "[{\"end\":84,\"start\":80},{\"end\":145,\"start\":140},{\"end\":204,\"start\":198},{\"end\":265,\"start\":259}]", "author_affiliation": "[{\"end\":138,\"start\":95},{\"end\":196,\"start\":153},{\"end\":257,\"start\":214},{\"end\":322,\"start\":279}]", "title": "[{\"end\":77,\"start\":1},{\"end\":400,\"start\":324}]", "venue": null, "abstract": "[{\"end\":1610,\"start\":402}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2513,\"start\":2509},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5096,\"start\":5092},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5119,\"start\":5116},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5554,\"start\":5550},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5851,\"start\":5848},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6249,\"start\":6245},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6413,\"start\":6409},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7081,\"start\":7077},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7087,\"start\":7083},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7187,\"start\":7184},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7193,\"start\":7189},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7199,\"start\":7195},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7205,\"start\":7201},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7227,\"start\":7223},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7531,\"start\":7527},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7894,\"start\":7890},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7900,\"start\":7896},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7906,\"start\":7902},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8125,\"start\":8121},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8131,\"start\":8127},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8142,\"start\":8139},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8147,\"start\":8144},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8163,\"start\":8160},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8168,\"start\":8165},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8173,\"start\":8170},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8178,\"start\":8175},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8247,\"start\":8243},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8818,\"start\":8815},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9266,\"start\":9263},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9272,\"start\":9268},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9410,\"start\":9409},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10676,\"start\":10672},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10779,\"start\":10775},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10848,\"start\":10844},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":14229,\"start\":14226},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14235,\"start\":14231},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":15391,\"start\":15387},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15731,\"start\":15728},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":16503,\"start\":16499},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":16604,\"start\":16600},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16677,\"start\":16674},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":16890,\"start\":16887},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":20302,\"start\":20301},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20830,\"start\":20827},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21320,\"start\":21316},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":21463,\"start\":21459},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":25628,\"start\":25625},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":25862,\"start\":25858},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":27669,\"start\":27665},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":28055,\"start\":28051},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":30941,\"start\":30938},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":31051,\"start\":31048},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":31217,\"start\":31214}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28529,\"start\":28305},{\"attributes\":{\"id\":\"fig_1\"},\"end\":28760,\"start\":28530},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29095,\"start\":28761},{\"attributes\":{\"id\":\"fig_3\"},\"end\":29372,\"start\":29096},{\"attributes\":{\"id\":\"fig_4\"},\"end\":29431,\"start\":29373},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":30612,\"start\":29432},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":31343,\"start\":30613}]", "paragraph": "[{\"end\":2603,\"start\":1629},{\"end\":3416,\"start\":2605},{\"end\":3927,\"start\":3418},{\"end\":4391,\"start\":3929},{\"end\":5020,\"start\":4393},{\"end\":6318,\"start\":5041},{\"end\":6965,\"start\":6320},{\"end\":7748,\"start\":6967},{\"end\":8390,\"start\":7750},{\"end\":9345,\"start\":8406},{\"end\":9905,\"start\":9365},{\"end\":10616,\"start\":9907},{\"end\":11115,\"start\":10640},{\"end\":11693,\"start\":11117},{\"end\":12128,\"start\":11695},{\"end\":12520,\"start\":12169},{\"end\":12978,\"start\":12522},{\"end\":13433,\"start\":12980},{\"end\":13633,\"start\":13510},{\"end\":13997,\"start\":13635},{\"end\":14726,\"start\":14023},{\"end\":15128,\"start\":14728},{\"end\":15334,\"start\":15176},{\"end\":15337,\"start\":15336},{\"end\":15633,\"start\":15339},{\"end\":15988,\"start\":15704},{\"end\":16505,\"start\":16140},{\"end\":16891,\"start\":16547},{\"end\":17238,\"start\":16893},{\"end\":17694,\"start\":17268},{\"end\":18329,\"start\":17696},{\"end\":18532,\"start\":18331},{\"end\":19315,\"start\":18580},{\"end\":19699,\"start\":19317},{\"end\":20170,\"start\":19726},{\"end\":20571,\"start\":20172},{\"end\":21073,\"start\":20573},{\"end\":21241,\"start\":21075},{\"end\":21634,\"start\":21260},{\"end\":22148,\"start\":21636},{\"end\":22753,\"start\":22150},{\"end\":22810,\"start\":22755},{\"end\":23129,\"start\":22812},{\"end\":23544,\"start\":23131},{\"end\":24844,\"start\":23572},{\"end\":25330,\"start\":24846},{\"end\":26067,\"start\":25332},{\"end\":27548,\"start\":26069},{\"end\":28304,\"start\":27550}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13509,\"start\":13434},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15175,\"start\":15129},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15703,\"start\":15634},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16064,\"start\":15989},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16139,\"start\":16064}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":19904,\"start\":19897},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":22096,\"start\":22088}]", "section_header": "[{\"end\":1627,\"start\":1612},{\"end\":5039,\"start\":5023},{\"end\":8404,\"start\":8393},{\"end\":9363,\"start\":9348},{\"end\":10638,\"start\":10619},{\"end\":12167,\"start\":12131},{\"end\":14021,\"start\":14000},{\"end\":16523,\"start\":16508},{\"end\":16545,\"start\":16526},{\"end\":17266,\"start\":17241},{\"end\":18578,\"start\":18535},{\"end\":19724,\"start\":19702},{\"end\":21258,\"start\":21244},{\"end\":23570,\"start\":23547},{\"end\":28314,\"start\":28306},{\"end\":28539,\"start\":28531},{\"end\":28770,\"start\":28762},{\"end\":29382,\"start\":29374},{\"end\":29442,\"start\":29433},{\"end\":30624,\"start\":30614}]", "table": "[{\"end\":30612,\"start\":29557}]", "figure_caption": "[{\"end\":28529,\"start\":28316},{\"end\":28760,\"start\":28541},{\"end\":29095,\"start\":28772},{\"end\":29372,\"start\":29098},{\"end\":29431,\"start\":29384},{\"end\":29557,\"start\":29444},{\"end\":31343,\"start\":30627}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2883,\"start\":2875},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8598,\"start\":8590},{\"end\":18106,\"start\":18100},{\"end\":18377,\"start\":18369},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18981,\"start\":18973},{\"end\":19408,\"start\":19400},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22147,\"start\":22139}]", "bib_author_first_name": "[{\"end\":31560,\"start\":31559},{\"end\":31571,\"start\":31570},{\"end\":31582,\"start\":31581},{\"end\":31592,\"start\":31591},{\"end\":32049,\"start\":32048},{\"end\":32058,\"start\":32057},{\"end\":32460,\"start\":32459},{\"end\":32474,\"start\":32473},{\"end\":32486,\"start\":32485},{\"end\":32488,\"start\":32487},{\"end\":32500,\"start\":32499},{\"end\":32508,\"start\":32507},{\"end\":32845,\"start\":32844},{\"end\":32854,\"start\":32853},{\"end\":32866,\"start\":32865},{\"end\":32878,\"start\":32877},{\"end\":33268,\"start\":33267},{\"end\":33277,\"start\":33276},{\"end\":33279,\"start\":33278},{\"end\":33291,\"start\":33290},{\"end\":33293,\"start\":33292},{\"end\":33305,\"start\":33304},{\"end\":33735,\"start\":33734},{\"end\":33744,\"start\":33743},{\"end\":33746,\"start\":33745},{\"end\":33758,\"start\":33757},{\"end\":33760,\"start\":33759},{\"end\":33772,\"start\":33771},{\"end\":34218,\"start\":34217},{\"end\":34227,\"start\":34226},{\"end\":34241,\"start\":34240},{\"end\":34259,\"start\":34258},{\"end\":34268,\"start\":34267},{\"end\":34553,\"start\":34552},{\"end\":34564,\"start\":34563},{\"end\":34574,\"start\":34573},{\"end\":34973,\"start\":34972},{\"end\":34984,\"start\":34983},{\"end\":34993,\"start\":34992},{\"end\":35002,\"start\":35001},{\"end\":35004,\"start\":35003},{\"end\":35008,\"start\":35007},{\"end\":35448,\"start\":35447},{\"end\":35450,\"start\":35449},{\"end\":35461,\"start\":35460},{\"end\":35463,\"start\":35462},{\"end\":35476,\"start\":35475},{\"end\":35486,\"start\":35485},{\"end\":35493,\"start\":35492},{\"end\":35495,\"start\":35494},{\"end\":35504,\"start\":35503},{\"end\":35758,\"start\":35757},{\"end\":35765,\"start\":35764},{\"end\":35778,\"start\":35777},{\"end\":35789,\"start\":35788},{\"end\":35800,\"start\":35799},{\"end\":35808,\"start\":35807},{\"end\":35820,\"start\":35819},{\"end\":35834,\"start\":35833},{\"end\":36132,\"start\":36131},{\"end\":36134,\"start\":36133},{\"end\":36145,\"start\":36144},{\"end\":36154,\"start\":36153},{\"end\":36166,\"start\":36165},{\"end\":36464,\"start\":36463},{\"end\":36478,\"start\":36477},{\"end\":36763,\"start\":36762},{\"end\":36777,\"start\":36776},{\"end\":36790,\"start\":36789},{\"end\":37059,\"start\":37055},{\"end\":37066,\"start\":37065},{\"end\":37075,\"start\":37074},{\"end\":37087,\"start\":37086},{\"end\":37095,\"start\":37094},{\"end\":37105,\"start\":37104},{\"end\":37116,\"start\":37115},{\"end\":37126,\"start\":37125},{\"end\":37128,\"start\":37127},{\"end\":37540,\"start\":37539},{\"end\":37548,\"start\":37547},{\"end\":37561,\"start\":37560},{\"end\":37978,\"start\":37977},{\"end\":37985,\"start\":37984},{\"end\":37993,\"start\":37992},{\"end\":38178,\"start\":38177},{\"end\":38180,\"start\":38179},{\"end\":38196,\"start\":38195},{\"end\":38207,\"start\":38206},{\"end\":38211,\"start\":38208},{\"end\":38220,\"start\":38219},{\"end\":38222,\"start\":38221},{\"end\":38634,\"start\":38633},{\"end\":38636,\"start\":38635},{\"end\":38652,\"start\":38651},{\"end\":38654,\"start\":38653},{\"end\":38666,\"start\":38665},{\"end\":38678,\"start\":38677},{\"end\":38682,\"start\":38679},{\"end\":38691,\"start\":38690},{\"end\":38693,\"start\":38692},{\"end\":39257,\"start\":39256},{\"end\":39270,\"start\":39269},{\"end\":39279,\"start\":39278},{\"end\":39288,\"start\":39287},{\"end\":39670,\"start\":39669},{\"end\":39682,\"start\":39681},{\"end\":40086,\"start\":40085},{\"end\":40094,\"start\":40093},{\"end\":40096,\"start\":40095},{\"end\":40111,\"start\":40110},{\"end\":40557,\"start\":40556},{\"end\":40569,\"start\":40568},{\"end\":40582,\"start\":40581},{\"end\":40592,\"start\":40591},{\"end\":40922,\"start\":40921},{\"end\":40934,\"start\":40933},{\"end\":40946,\"start\":40945},{\"end\":40957,\"start\":40956},{\"end\":40970,\"start\":40969},{\"end\":41405,\"start\":41404},{\"end\":41415,\"start\":41414},{\"end\":41430,\"start\":41429},{\"end\":41432,\"start\":41431},{\"end\":41448,\"start\":41447},{\"end\":41459,\"start\":41458},{\"end\":41461,\"start\":41460}]", "bib_author_last_name": "[{\"end\":31568,\"start\":31561},{\"end\":31579,\"start\":31572},{\"end\":31589,\"start\":31583},{\"end\":31598,\"start\":31593},{\"end\":32055,\"start\":32050},{\"end\":32065,\"start\":32059},{\"end\":32471,\"start\":32461},{\"end\":32483,\"start\":32475},{\"end\":32497,\"start\":32489},{\"end\":32505,\"start\":32501},{\"end\":32518,\"start\":32509},{\"end\":32851,\"start\":32846},{\"end\":32863,\"start\":32855},{\"end\":32875,\"start\":32867},{\"end\":32884,\"start\":32879},{\"end\":33274,\"start\":33269},{\"end\":33288,\"start\":33280},{\"end\":33302,\"start\":33294},{\"end\":33311,\"start\":33306},{\"end\":33741,\"start\":33736},{\"end\":33755,\"start\":33747},{\"end\":33769,\"start\":33761},{\"end\":33779,\"start\":33773},{\"end\":34224,\"start\":34219},{\"end\":34238,\"start\":34228},{\"end\":34256,\"start\":34242},{\"end\":34265,\"start\":34260},{\"end\":34276,\"start\":34269},{\"end\":34561,\"start\":34554},{\"end\":34571,\"start\":34565},{\"end\":34580,\"start\":34575},{\"end\":34981,\"start\":34974},{\"end\":34990,\"start\":34985},{\"end\":34999,\"start\":34994},{\"end\":35016,\"start\":35009},{\"end\":35458,\"start\":35451},{\"end\":35473,\"start\":35464},{\"end\":35483,\"start\":35477},{\"end\":35490,\"start\":35487},{\"end\":35501,\"start\":35496},{\"end\":35512,\"start\":35505},{\"end\":35762,\"start\":35759},{\"end\":35775,\"start\":35766},{\"end\":35786,\"start\":35779},{\"end\":35797,\"start\":35790},{\"end\":35805,\"start\":35801},{\"end\":35817,\"start\":35809},{\"end\":35831,\"start\":35821},{\"end\":35842,\"start\":35835},{\"end\":36142,\"start\":36135},{\"end\":36151,\"start\":36146},{\"end\":36163,\"start\":36155},{\"end\":36173,\"start\":36167},{\"end\":36475,\"start\":36465},{\"end\":36485,\"start\":36479},{\"end\":36774,\"start\":36764},{\"end\":36787,\"start\":36778},{\"end\":36797,\"start\":36791},{\"end\":37063,\"start\":37060},{\"end\":37072,\"start\":37067},{\"end\":37084,\"start\":37076},{\"end\":37092,\"start\":37088},{\"end\":37102,\"start\":37096},{\"end\":37113,\"start\":37106},{\"end\":37123,\"start\":37117},{\"end\":37136,\"start\":37129},{\"end\":37545,\"start\":37541},{\"end\":37558,\"start\":37549},{\"end\":37569,\"start\":37562},{\"end\":37982,\"start\":37979},{\"end\":37990,\"start\":37986},{\"end\":37997,\"start\":37994},{\"end\":38193,\"start\":38181},{\"end\":38204,\"start\":38197},{\"end\":38217,\"start\":38212},{\"end\":38230,\"start\":38223},{\"end\":38649,\"start\":38637},{\"end\":38663,\"start\":38655},{\"end\":38675,\"start\":38667},{\"end\":38688,\"start\":38683},{\"end\":38701,\"start\":38694},{\"end\":39267,\"start\":39258},{\"end\":39276,\"start\":39271},{\"end\":39285,\"start\":39280},{\"end\":39295,\"start\":39289},{\"end\":39679,\"start\":39671},{\"end\":39692,\"start\":39683},{\"end\":40091,\"start\":40087},{\"end\":40108,\"start\":40097},{\"end\":40116,\"start\":40112},{\"end\":40566,\"start\":40558},{\"end\":40579,\"start\":40570},{\"end\":40589,\"start\":40583},{\"end\":40599,\"start\":40593},{\"end\":40931,\"start\":40923},{\"end\":40943,\"start\":40935},{\"end\":40954,\"start\":40947},{\"end\":40967,\"start\":40958},{\"end\":40975,\"start\":40971},{\"end\":41412,\"start\":41406},{\"end\":41427,\"start\":41416},{\"end\":41445,\"start\":41433},{\"end\":41456,\"start\":41449},{\"end\":41469,\"start\":41462}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":6681692},\"end\":31938,\"start\":31505},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":102496818},\"end\":32407,\"start\":31940},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":4246903},\"end\":32762,\"start\":32409},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":13259596},\"end\":33209,\"start\":32764},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":145508},\"end\":33676,\"start\":33211},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":145508},\"end\":34145,\"start\":33678},{\"attributes\":{\"doi\":\"arXiv:1511.07041\",\"id\":\"b6\"},\"end\":34488,\"start\":34147},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":14281310},\"end\":34926,\"start\":34490},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":13300851},\"end\":35363,\"start\":34928},{\"attributes\":{\"id\":\"b9\"},\"end\":35693,\"start\":35365},{\"attributes\":{\"doi\":\"arXiv:1408.5093\",\"id\":\"b10\"},\"end\":36073,\"start\":35695},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":333838},\"end\":36386,\"start\":36075},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":5574079},\"end\":36695,\"start\":36388},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":195908774},\"end\":37010,\"start\":36697},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":14113767},\"end\":37481,\"start\":37012},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":1629541},\"end\":37917,\"start\":37483},{\"attributes\":{\"doi\":\"arXiv:1505.04366\",\"id\":\"b16\"},\"end\":38156,\"start\":37919},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":14730836},\"end\":38560,\"start\":38158},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":9002995},\"end\":39119,\"start\":38562},{\"attributes\":{\"doi\":\"10.1109/CVPR.2013.178\",\"id\":\"b19\"},\"end\":39194,\"start\":39121},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":545361},\"end\":39599,\"start\":39196},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":14124313},\"end\":40027,\"start\":39601},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":6242669},\"end\":40479,\"start\":40029},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":11828469},\"end\":40858,\"start\":40481},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":15623791},\"end\":41354,\"start\":40860},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":16413702},\"end\":41746,\"start\":41356}]", "bib_title": "[{\"end\":31557,\"start\":31505},{\"end\":32046,\"start\":31940},{\"end\":32457,\"start\":32409},{\"end\":32842,\"start\":32764},{\"end\":33265,\"start\":33211},{\"end\":33732,\"start\":33678},{\"end\":34550,\"start\":34490},{\"end\":34970,\"start\":34928},{\"end\":36129,\"start\":36075},{\"end\":36461,\"start\":36388},{\"end\":36760,\"start\":36697},{\"end\":37053,\"start\":37012},{\"end\":37537,\"start\":37483},{\"end\":38175,\"start\":38158},{\"end\":38631,\"start\":38562},{\"end\":39254,\"start\":39196},{\"end\":39667,\"start\":39601},{\"end\":40083,\"start\":40029},{\"end\":40554,\"start\":40481},{\"end\":40919,\"start\":40860},{\"end\":41402,\"start\":41356}]", "bib_author": "[{\"end\":31570,\"start\":31559},{\"end\":31581,\"start\":31570},{\"end\":31591,\"start\":31581},{\"end\":31600,\"start\":31591},{\"end\":32057,\"start\":32048},{\"end\":32067,\"start\":32057},{\"end\":32473,\"start\":32459},{\"end\":32485,\"start\":32473},{\"end\":32499,\"start\":32485},{\"end\":32507,\"start\":32499},{\"end\":32520,\"start\":32507},{\"end\":32853,\"start\":32844},{\"end\":32865,\"start\":32853},{\"end\":32877,\"start\":32865},{\"end\":32886,\"start\":32877},{\"end\":33276,\"start\":33267},{\"end\":33290,\"start\":33276},{\"end\":33304,\"start\":33290},{\"end\":33313,\"start\":33304},{\"end\":33743,\"start\":33734},{\"end\":33757,\"start\":33743},{\"end\":33771,\"start\":33757},{\"end\":33781,\"start\":33771},{\"end\":34226,\"start\":34217},{\"end\":34240,\"start\":34226},{\"end\":34258,\"start\":34240},{\"end\":34267,\"start\":34258},{\"end\":34278,\"start\":34267},{\"end\":34563,\"start\":34552},{\"end\":34573,\"start\":34563},{\"end\":34582,\"start\":34573},{\"end\":34983,\"start\":34972},{\"end\":34992,\"start\":34983},{\"end\":35001,\"start\":34992},{\"end\":35007,\"start\":35001},{\"end\":35018,\"start\":35007},{\"end\":35460,\"start\":35447},{\"end\":35475,\"start\":35460},{\"end\":35485,\"start\":35475},{\"end\":35492,\"start\":35485},{\"end\":35503,\"start\":35492},{\"end\":35514,\"start\":35503},{\"end\":35764,\"start\":35757},{\"end\":35777,\"start\":35764},{\"end\":35788,\"start\":35777},{\"end\":35799,\"start\":35788},{\"end\":35807,\"start\":35799},{\"end\":35819,\"start\":35807},{\"end\":35833,\"start\":35819},{\"end\":35844,\"start\":35833},{\"end\":36144,\"start\":36131},{\"end\":36153,\"start\":36144},{\"end\":36165,\"start\":36153},{\"end\":36175,\"start\":36165},{\"end\":36477,\"start\":36463},{\"end\":36487,\"start\":36477},{\"end\":36776,\"start\":36762},{\"end\":36789,\"start\":36776},{\"end\":36799,\"start\":36789},{\"end\":37065,\"start\":37055},{\"end\":37074,\"start\":37065},{\"end\":37086,\"start\":37074},{\"end\":37094,\"start\":37086},{\"end\":37104,\"start\":37094},{\"end\":37115,\"start\":37104},{\"end\":37125,\"start\":37115},{\"end\":37138,\"start\":37125},{\"end\":37547,\"start\":37539},{\"end\":37560,\"start\":37547},{\"end\":37571,\"start\":37560},{\"end\":37984,\"start\":37977},{\"end\":37992,\"start\":37984},{\"end\":37999,\"start\":37992},{\"end\":38195,\"start\":38177},{\"end\":38206,\"start\":38195},{\"end\":38219,\"start\":38206},{\"end\":38232,\"start\":38219},{\"end\":38651,\"start\":38633},{\"end\":38665,\"start\":38651},{\"end\":38677,\"start\":38665},{\"end\":38690,\"start\":38677},{\"end\":38703,\"start\":38690},{\"end\":39269,\"start\":39256},{\"end\":39278,\"start\":39269},{\"end\":39287,\"start\":39278},{\"end\":39297,\"start\":39287},{\"end\":39681,\"start\":39669},{\"end\":39694,\"start\":39681},{\"end\":40093,\"start\":40085},{\"end\":40110,\"start\":40093},{\"end\":40118,\"start\":40110},{\"end\":40568,\"start\":40556},{\"end\":40581,\"start\":40568},{\"end\":40591,\"start\":40581},{\"end\":40601,\"start\":40591},{\"end\":40933,\"start\":40921},{\"end\":40945,\"start\":40933},{\"end\":40956,\"start\":40945},{\"end\":40969,\"start\":40956},{\"end\":40977,\"start\":40969},{\"end\":41414,\"start\":41404},{\"end\":41429,\"start\":41414},{\"end\":41447,\"start\":41429},{\"end\":41458,\"start\":41447},{\"end\":41471,\"start\":41458}]", "bib_venue": "[{\"end\":31743,\"start\":31680},{\"end\":32192,\"start\":32138},{\"end\":33001,\"start\":32952},{\"end\":33468,\"start\":33399},{\"end\":33936,\"start\":33867},{\"end\":34733,\"start\":34666},{\"end\":35169,\"start\":35102},{\"end\":37253,\"start\":37204},{\"end\":37726,\"start\":37657},{\"end\":38381,\"start\":38315},{\"end\":38858,\"start\":38789},{\"end\":39412,\"start\":39363},{\"end\":39837,\"start\":39774},{\"end\":40273,\"start\":40204},{\"end\":41132,\"start\":41063},{\"end\":41558,\"start\":41523},{\"end\":31678,\"start\":31600},{\"end\":32136,\"start\":32067},{\"end\":32560,\"start\":32520},{\"end\":32950,\"start\":32886},{\"end\":33397,\"start\":33313},{\"end\":33865,\"start\":33781},{\"end\":34215,\"start\":34147},{\"end\":34664,\"start\":34582},{\"end\":35100,\"start\":35018},{\"end\":35445,\"start\":35365},{\"end\":35755,\"start\":35695},{\"end\":36219,\"start\":36175},{\"end\":36531,\"start\":36487},{\"end\":36843,\"start\":36799},{\"end\":37202,\"start\":37138},{\"end\":37655,\"start\":37571},{\"end\":37975,\"start\":37919},{\"end\":38313,\"start\":38232},{\"end\":38787,\"start\":38703},{\"end\":39361,\"start\":39297},{\"end\":39772,\"start\":39694},{\"end\":40202,\"start\":40118},{\"end\":40644,\"start\":40601},{\"end\":41061,\"start\":40977},{\"end\":41521,\"start\":41471}]"}}}, "year": 2023, "month": 12, "day": 17}