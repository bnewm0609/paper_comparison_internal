{"id": 252762392, "updated": "2023-10-05 09:51:55.253", "metadata": {"title": "Improving Large-scale Paraphrase Acquisition and Generation", "authors": "[{\"first\":\"Yao\",\"last\":\"Dou\",\"middle\":[]},{\"first\":\"Chao\",\"last\":\"Jiang\",\"middle\":[]},{\"first\":\"Wei\",\"last\":\"Xu\",\"middle\":[]}]", "venue": "EMNLP", "journal": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "This paper addresses the quality issues in existing Twitter-based paraphrase datasets, and discusses the necessity of using two separate definitions of paraphrase for identification and generation tasks. We present a new Multi-Topic Paraphrase in Twitter (MultiPIT) corpus that consists of a total of 130k sentence pairs with crowdsoursing (MultiPIT_crowd) and expert (MultiPIT_expert) annotations using two different paraphrase definitions for paraphrase identification, in addition to a multi-reference test set (MultiPIT_NMR) and a large automatically constructed training set (MultiPIT_Auto) for paraphrase generation. With improved data annotation quality and task-specific paraphrase definition, the best pre-trained language model fine-tuned on our dataset achieves the state-of-the-art performance of 84.2 F1 for automatic paraphrase identification. Furthermore, our empirical results also demonstrate that the paraphrase generation models trained on MultiPIT_Auto generate more diverse and high-quality paraphrases compared to their counterparts fine-tuned on other corpora such as Quora, MSCOCO, and ParaNMT.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2210.03235", "mag": null, "acl": "2022.emnlp-main.631", "pubmed": null, "pubmedcentral": null, "dblp": "conf/emnlp/DouJX22", "doi": "10.18653/v1/2022.emnlp-main.631"}}, "content": {"source": {"pdf_hash": "33aa8a525087381103ddf8fb7799131cd7ee5994", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclanthology.org/2022.emnlp-main.631.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "133242a0c611c0c8486b977830f4678dbaad350c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/33aa8a525087381103ddf8fb7799131cd7ee5994.txt", "contents": "\nImproving Large-scale Paraphrase Acquisition and Generation\nDecember 7-11, 2022\n\nYao Dou douy@gatech.edu \nSchool of Interactive Computing\nGeorgia Institute of Technology\n\n\nChao Jiang chaojiang@gatech.edu \nSchool of Interactive Computing\nGeorgia Institute of Technology\n\n\nWei Xu wei.xu@cc.gatech.edu \nSchool of Interactive Computing\nGeorgia Institute of Technology\n\n\nImproving Large-scale Paraphrase Acquisition and Generation\n\nProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing\nthe 2022 Conference on Empirical Methods in Natural Language ProcessingDecember 7-11, 2022\nThis paper addresses the quality issues in existing Twitter-based paraphrase datasets, and discusses the necessity of using two separate definitions of paraphrase for identification and generation tasks. We present a new Multi-Topic Paraphrase in Twitter (MULTIPIT) corpus that consists of a total of 130k sentence pairs with crowdsoursing (MULTIPIT CROWD ) and expert (MULTIPIT EXPERT ) annotations using two different paraphrase definitions for paraphrase identification, in addition to a multi-reference test set (MULTIPIT NMR ) and a large automatically constructed training set (MULTIPIT AUTO ) for paraphrase generation. With improved data annotation quality and task-specific paraphrase definition, the best pre-trained language model fine-tuned on our dataset achieves the stateof-the-art performance of 84.2 F 1 for automatic paraphrase identification. Furthermore, our empirical results also demonstrate that the paraphrase generation models trained on MUL-TIPIT AUTO generate more diverse and highquality paraphrases compared to their counterparts fine-tuned on other corpora such as Quora, MSCOCO, and ParaNMT. tract 19051600004), and Figure Eight AI for Everyone Award.\n\nIntroduction\n\nParaphrases are alternative expressions that convey a similar meaning (Bhagat and Hovy, 2013). Studying paraphrase facilitates research in both natural language understanding and generation. For instance, identifying paraphrases on social media is important for tracking the spread of misinformation (Bakshy et al., 2011) and capturing emerging events (Vosoughi and Roy, 2016). On the other hand, paraphrase generation improves the linguistic diversity in conventional agents (Li et al., 2016) and machine translation (Thompson and Post, 2020). It has also been successfully applied in data argumentation to improve information extraction (Zhang et al., 2015;Ferguson et al., 2018) and question answering systems (Gan and Ng, 2019). 7. In Tibet, climate change causes bigger, faster avalanches.  Many researchers have been leveraging Twitter data to study paraphrase given its lexical and style diversity as well as coverage of up-to-date events. However, existing Twitter-based paraphrase datasets, namely PIT-2015 (Xu et al., 2015) and Twitter-URL (Lan et al., 2017), suffer from quality issues such as topic unbalance and annotation noise, 1 which limit the performance of the models trained using them. Moreover, past efforts on creating paraphrase corpora only consider one paraphrase criteria without taking into account the fact that the desired \"strictness\" of semantic equivalence in paraphrases varies from task to task (Bhagat and Hovy, 2013;Liu and Soh, 2022). For example, for the purpose of tracking unfolding events, \"A tsunami hit Haiti.\" and \"303 people died because of the tsunami in Haiti\" are sufficiently close to be considered as paraphrases; whereas for paraphrase generation, the extra information \"303 people dead\" in the latter sentence may lead models to learn to  Table 1: Statistics of MULTIPIT CROWD and MULTIPIT EXPERT datasets. The sentence/tweet lengths are calculated based on the number of tokens per unique sentence/tweet. %Multi-Ref denotes the percentage of source sentences with more than one paraphrase. Compared with prior work, our MULTIPIT CROWD dataset has a significantly larger size, a higher portion of paraphrases, and a more balanced topic distribution.\n\nhallucinate and generate more unfaithful content.\n\nIn this paper, we present an effective data collection and annotation method to address these issues. We curate the Multi-Topic Paraphrase in Twitter (MULTIPIT) corpus, which includes MUL-TIPIT CROWD , a large crowdsourced set of 125K sentence pairs that is useful for tracking information on Twitter, and MULTIPIT EXPERT , an expert annotated set of 5.5K sentence pairs using a stricter definition that is more suitable for acquiring paraphrases for generation purpose. Compared to PIT-2015 and Twitter-URL, our corpus contains more than twice as much data with more balanced topic distribution and better annotation quality. Two sets of examples from MULTIPIT are shown in Figure 1.\n\nWe extensively evaluate several state-of-the-art neural language models on our datasets to demonstrate the importance of having task-specific paraphrase definition. Our best model achieves 84.2 F 1 for automatic paraphrase identification. In addition, we construct a continually growing paraphrase dataset, MULTIPIT AUTO , by applying the automatic identification model to unlabelled Twitter data. Empirical results and analysis show that generation models fine-tuned on MULTIPIT AUTO generate more diverse and high-quality paraphrases compared to models trained on other corpora, such as MSCOCO (Lin et al., 2014), ParaNMT (Wieting andGimpel, 2018), and Quora. 2 We hope our MULTIPIT corpus will facilitate future innovation in paraphrase research.\n\n\nMulti-Topic PIT Corpus\n\nIn this section, we present our data collection and annotation methodology for creating MULTI-PIT CROWD and MULTIPIT EXPERT datasets. The data statistics is detailed in Table 1.\n\n\nCollection of Tweets\n\nTo gather paraphrases about a diverse set of topics as illustrated in Figure 1, we first group tweets that contain the same trending topic 3 (year 2014-2015) or the same URL (year 2017-2019) retrieved through Twitter public APIs 4 over a long time period. Specifically, for the URL-based method, we extract the URLs embedded in the tweets that are posted by 15 news agency accounts (e.g., NYTScience, CNNPolitics, and ForbesTech). To get cleaner paraphrases, we split the tweets into sentences, eliminating the extra noises caused by multi-sentence tweets. More details of the improvements we made to address the data preprocessing issues in prior work are described in Appendix B.\n\n\nTopic Classification and Balancing\n\nTo avoid a single type of topics dominating the entire dataset as in prior work (Xu et al., 2015;Lan et al., 2017), we manually categorize the topics for each group of tweets and balance their distribution. For trending topics, we ask three in-house annotators to classify them into 4 different categories: sports, entertainment, event, and others. All three Topics breakdown of URL-2017 corpus annotators are college students with varied linguistic annotation experience, and each received an hour-long training session. For URLs, most of them are linked to news articles and have already been categorized by the news agency. 5 We include the tweets grouped by URLs that belong to the science/tech, health, politics, and finance categories.\n\n\nCandidate Selection\n\nThe PIT-2015 (Xu et al., 2015) and Twitter-URL (Lan et al., 2017) corpora contain only 23% and 31% sentence pairs that are paraphrases, respectively. To increase the portion of paraphrases and improve the annotation efficiency, we introduce an additional step to filter out the tweet groups that contain either too much noise or too few paraphrases, and adaptively select sentence pairs for annotation ( \u00a72.4). For each of the trend-based groups, we first select the top 2 sentences using a simple ranking algorithm (Xu et al., 2015) based on the averaged probability of words. We pair each of these two sentences with 10 other sentences that are randomly sampled from the top 20 in each group. Among these 20 sentence pairs, if the annotators found n \u2208 [4, 6] or [7,9] or [10,12] or [13,20] pairs as paraphrases, then we further deploy 20, 30, 40, or 50 sentence pairs for annotation, respectively. We pair one of the top 5 ranked sentences with 10 sentences randomly selected from those ranked between top 6 and top 50. Since the URLbased groups generally contain fewer sentences, we select the top 11 sentences and ask annotators to choose one as the seed sentence that can be paired with the rest 10 sentences to produce at least 3 paraphrase pairs. If such a seed sentence exists, we pair it with the rest 10 sentences and deploy them for annotation. Otherwise, we skip the entire group.\n\n\nCrowd Annotation for Paraphrase Identification\n\nWe then annotate the selected sentence pairs using the crowdsourcing platform Figure-Eight 6 to construct MULTIPIT CROWD .\n\nAnnotation Process. We design a 1-vs-1 annotation schema, where we present one sentence pair to workers at a time and ask them to annotate whether it is a paraphrase pair or not. A screenshot of the annotation interface is provided in Appendix A.1. We collect 6 judgments for every sentence pair and pay $0.2 per annotation (>$7 per hour). For creating MULTIPIT CROWD , with the purpose of identifying similar sentences and tracking information spreading on Twitter in mind, we consider two sentences as paraphrases even if one contains some new information that does not appear in the other sentence (see Figure 3 for examples). As a side note, because these sentences are grouped under the same trend or URL, the new information is always relevant and based on the context, otherwise, we will consider them non-paraphrases.\n\nQuality Control. In every five sentence pairs, we embed one hidden test sentence pair that are pre-labeled by one of the authors, and constantly monitor the workers' performance. Whenever annotators make a mistake on the test pair, they will be alerted and provided with an explanation. Workers can continue in the task if they achieve >85% accuracy on the test pairs and >0.2 Cohen's (Cohen, 1960) kappa when compared with the major vote of other workers. All workers are in the U.S.\n\nInter-Annotator Agreement. The average Cohen's kappa is 0.75 for URL-sourced sentence pairs, 0.69 for Trends-sourced ones, and 0.70 for all. We also sample 400 sampled sentence pairs and hire two experienced in-house annotators to label them. Assuming the in-house annotation is gold, the F 1 of crowdworkers' majority vote is 89.1.\n\nAccessing Topic Diversity. We manually examine 100 sentence pairs randomly sampled from MULTIPIT CROWD , PIT-2015 (Xu et al., 2015) and Twitter-URL (Lan et al., 2017). Figure 2 shows the results of the manual inspection. MULTI-PIT CROWD has a much more balanced topic distribution, compared to prior work where 58% of sentences in PIT-2015 are about sports and 63% of sentences in Twitter-URL are politics-related. This improvement can be attributed to the long time periodd ( \u00a72.1) and topic classification step ( \u00a72.2) in our data collection process. In contrast, PIT-2015 was collected within only 10 days (04/24/2013 -05/03/2013) that was overwhelmed by a popular sports event -the 2013 NFL draft (04/25 -04/27), and Twitter-URL was collected during the 3 months of the 2016 US presidential election.\n\n\nExpert Annotation for Paraphrase Generation\n\nText generation models are prone to memorize training data and generate unfaithful hallucinations (Maynez et al., 2020;Carlini et al., 2021). Including paraphrase pairs that contain extra information other than world or commonsense knowledge in the training data only worsens the problem, as shown in Table 15 in Appendix F. For the purpose of paraphrase generation, we further create MULTI-PIT EXPERT with expert annotations, using a stricter paraphrase definition than the one used in MULTI-PIT CROWD . The different paraphrase criteria used for creating these two datasets and their corresponding examples are illustrated in Figure 3.\n\n\nData Selection.\n\nTo create a high-quality corpus that focuses on differentiating strict paraphrases from the more loosely defined ones, we first use our best paraphrase identifier ( \u00a73) fine-tuned on MUL-TIPIT CROWD to filter the sentence pairs and then have experienced in-house annotators to further annotate them. Specifically, we gather sentence pairs that are identified as paraphrases by the automatic classifier from 9,762 trending topic groups (from Oct-Dec 2021) and 181,254 URL groups (from Jan 2020-Jun 2021). To improve the diversity of our dataset, instead of presenting these pairs directly to the experts for annotation, we cluster the sentences by considering the paraphrase relationship transitive, i.e., if sentence pairs (s 1 , s 2 ) and (s 2 , s 3 ) are both identified as paraphrases, then (s 1 , s 2 , s 3 ) is a cluster. For each trend or URL, we show two seed sentences paired with up to 30 sentences in the largest cluster for the experts to annotate. In total, we have 5,570 sentence pairs annotated for MUL-TIPIT EXPERT , in which 100 sentences sourced by trend and 100 ones sourced by URL have at least 8 corresponding paraphrases. We use these 200 sets to form MULTIPIT NMR , the first multi-reference test set for paraphrase generation evaluation ( \u00a74).\n\nExpert Annotation. We ask two experienced annotators with linguistic backgrounds and rich annotation experience to annotate each sentence pair as paraphrases or not. Annotators thoroughly discuss  pairs that have inconsistent judgments until reaching an agreement. A screenshot of the updated annotation instruction is provided in Appendix A.2.\n\n\nParaphrase Identification\n\nParaphrase identification is a task that determines whether two given sentences are paraphrases or not. The two paraphrase definitions used in MUL-TIPIT CROWD and MULTIPIT EXPERT suit different downstream applications: tracking information on Twitter and acquiring high-quality paraphrase pairs for training generation models. Paraphrase identification models trained on our datasets achieve over 84 F 1 for each case.\n\nExperimental Setup. As each sentence pair in MULTIPIT CROWD has six judgments, we use 3 as the threshold, where pairs with >3 paraphrase judgments are labeled as paraphrase, and the ones with <3 paraphrase judgments are labeled as nonparaphrase. We split MULTIPIT CROWD and MULTI-PIT EXPERT into 80/10/10% for train/dev/test partitions by time such that the oldest data are used for training. More details on the implementation and hyperparameter tuning are in Appendix C.\n\n\nModels\n\nWe consider an encoder-decoder language model, T5 ( Table 2 presents results for the models fine-tuned on each dataset. DeBERTaV3 large achieves the best results with 92 F 1 on MULTIPIT CROWD and 83.2 F 1 on MULTIPIT EXPERT . Transformer-based models consistently outperform BiLSTM-based models, especially on MULTIPIT EXPERT .\n\n\nResults\n\nBeyond Fine-tuning. As MULTIPIT CROWD is a large-scale dataset annotated with a loose paraphrase definition, we test whether leveraging these \"noisy\" data improves model performance on MUL-TIPIT EXPERT . To reduce the noise that comes from the difference in definitions, we first adjust the labeling threshold for MULTIPIT CROWD from 3 to 4. Then we consider two noisy training techniques adopted in prior work (Xie et al., 2020;Zhang and Sabuncu, 2018), namely filtering and flipping. Specifically, we fine-tune a teacher model on MULTIPIT EXPERT and use it to go through MULTI-PIT CROWD as follows: for each sentence pair p, if its label is i (0 for non-paraphrase, 1 for paraphrase) and P teacher (y = i|p) \u2264 \u03bb, we filter out p or flip its label to 1\u2212i (i.e. 0 \u2192 1). 8 Next, we fine-tune a new   Table 3. Compared to finetuning on MULTIPIT EXPERT , adding the original MULTIPIT CROWD to the training data results in a 9.8 and 19.5 points drop in F 1 and precision, respectively, demonstrating the necessity of task-specific paraphrase definition. Among all methods, the flipping approach achieves the best F 1 of 84.2. We thus use it to create MULTIPIT AUTO ( \u00a74). Figure 4 shows test set performance of DeBERTaV3 large fine-tuned on different amounts of data in MULTIPIT EXPERT . As there are 156 trend/URL groups in the train set, we truncate the data by group. With more training data, the model achieves better F 1 and accuracy but in a slower fashion compared to the early stage. This finding suggests that annotating more data can further improve the model's performance.\n\n\nImpact of Data Size\n\n\nParaphrase Generation\n\nParaphrase generation is a task that rewrites the input sentence while preserving its semantic meaning. Since new data is generated on Twitter every day, we introduce MULTIPIT AUTO , an automated continual growing dataset for paraphrase generation. We show that the model fine-tuned on MULTI-PIT AUTO generates more diverse and high-quality paraphrases than other paraphrase datasets.\n\n\nComparison with Existing Datasets\n\nMSCOCO (Lin et al., 2014), and ParaNMT (Wieting and Gimpel, 2018), and Quora 9 are three widely used datasets in paraphrase generation research (Zhou and Bhat, 2021). The Quora dataset contains over 400K question pairs, including 144K pairs labeled as duplicated (i.e., paraphrase), which are split into 134K/5K/5K as train/dev/test sets, respectively. MSCOCO consists of over 120K images, each of which has five captions. Following Chen et al. (2020), for each image, we randomly pick a caption and pair it with each of the other four captions, resulting in about 490K paraphrase pairs. We split them into train/dev/test sets with 330K/80K/80K pairs, respectively. ParaNMT is a dataset with more than 50 million paraphrase pairs that are automatically generated through backtranslation. Since back-translation may introduce noise, we use the manually labeled dev and test sets from Chen et al. (2019), which contain 499 and 871 instances, respectively.\n\nMULTIPIT AUTO . We use the best performing model in Section 3 to extract paraphrase pairs from recent Twitter data (trending topics in Oct-Dec 2021 and URLs in Jan 2020-Jun 2021). We call these automated identified paraphrase pairs MUL-TIPIT AUTO , 10 which contains 302,307 pairs. One of the authors manually annotates 215 paraphrase pairs and uses them as the dev set. We use the multireference MULTIPIT NMR test set ( \u00a72.5) for evaluation. As the test set and MULTIPIT AUTO come from the same time period, we filter out sentence pairs in MULTIPIT AUTO that share similar trends or URLs with the pairs from the test set. This leaves us with 290,395 pairs as the training set.\n\nFollowing Chen et al. (2019), we remove paraphrase pairs with high BLEU scores in each training set to ensure there is enough variation between paraphrases, leaving about 137K pairs for MULTI-PIT AUTO , 47K for Quora, 275K for MSCOCO, and 443K for ParaNMT. Table 14 in Appendix F shows BLEU filtering improves model performance for all datasets. Detailed dataset statistics are provided in Appendix E.\n\n\nEvaluation Metrics\n\nWe consider four automated metrics that are commonly used in previous work (Li et al., 2019;Niu et al., 2021)    computed between the source sentence and the output, which measures surface-form diversity. BERT-Score is also calculated between the source sentence and the output, measuring semantic similarity. BERT-iBLEU is a harmonic mean of BERT-Score and 1\u2212Self-BLEU, encouraging both semantic similarity and diversity. We use SacreBLEU (Post, 2018) to compute BLEU and bert-score 11 to compute BERT-Score.\n\n\nGeneration Models\n\nWe consider two autoregressive language models, GPT-2 (Radford et al., 2019) and GPT-3 12 (Brown et al., 2020), and two encoder-decoder language models, BART (Lewis et al., 2020) and T5 (Raffel et al., 2020). For GPT-3, we try both zero-shot and few-shot (4 examples) setups using in-context learning without any fine-tuning. For other models, we fine-tune seven configurations of them on MULTIPIT AUTO . Table 4 shows the test set results of each model and the diversity of human references measured by Self-BLEU. Among all models, the few-shot setting of GPT-3 achieves the highest BERT-iBLEU score, and the zero-shot setting achieves the second-best number with only 1 point behind, which is not surprising given its size. Compared to GPT-3 generations, human references are 11 https://www.github.com/Tiiiger/bert_score We use DeBERTa-xlarge-mnli since it has the best correlation with human evaluation according to Zhang et al. (2020). 12 We use text-davinci-002, which is the most capable GPT-3 model. much more diverse with a decrease of 24.5 in Self-BLEU under the best case and 13.5 under the average case, indicating that there is still a big gap between large language models and humans. For supervised small-scale models, T5 large outperforms others with the best Self-BLEU and BERT-iBLEU scores. Although BART large gets the highest BLEU score, our experiments in Appendix F show BERT-iBLEU has the best correlation with human evaluation. We thus use T5 large in all the rest experiments. For all models except GPT-3, we use beam search with beam size = 4. Please refer to Appendix C for details on the training setup and hyperparameter tuning. GPT-3 prompting and hyperparameter setup are provided in Appendix D. Generation examples are displayed in Figure 16 in Appendix G.\n\nImpact of Data Size. Figure 5 shows test set performance of T5 large fine-tuned on different amount of data in MULTIPIT AUTO from 1K to 137K. With more training data, the model generates more diverse and high-quality paraphrases as Self-BLEU decreases (improves) and BERT-iBLEU increases. This suggests that the paraphrase generation models will benefit from the continually growing size of our MULTIPIT AUTO corpus.\n\n\nCross-Dataset Generalization\n\nBuilding a paraphrase generation model that generalizes to new data is always an ambitious goal.\n\nTo better understand the generalizability of each dataset, we fine-tune T5 large on MULTIPIT AUTO , Quora, MSCOCO, and ParaNMT separately and evaluate their performance across datasets. For fair comparisons, we use the same architecture, T5 large , in this experiment. Appendix G displays examples generated by these models on each dataset. Table 5 presents automatic evaluation of test set performance across all four datasets. As MUL-TIPIT AUTO and ParaNMT consist of sentences in different styles, models trained on them have better generalizability, achieving the best cross-domain  Table 5: Automatic evaluation of models fine-tuned on four datasets. Here, BL: BLEU, S-B: Self-BLEU, B-S: BERT-Score, B-iB: BERT-iBLEU. Bold: the best, Underline: the second best.   performance. On the contrary, since Quora and MSCOCO contain only questions or captions, models fine-tuned on them always generate questionor description-style sentences. For example, given \"we should take shots.\", model fine-tuned on Quora generates \"Why do we take shots?\". We conduct a human evaluation to further compare MULTIPIT AUTO and ParaNMT datasets, by evaluating 200 randomly sampled generations from the model trained on each corpus. 13 As shown in Table 6, MULTIPIT AUTO 's generations receive the highest scores in all three dimensions: fluency, semantic similarity and diversity. Each generation is rated by three annotators on a 5-point Likertscale per aspect, with 5 being the best. We also show the distribution of human evaluation results on each dimension in Figure 6 for a deeper comparison. Specifically, MULTIPIT AUTO model generates fewer really poor paraphrases (semantic similarity <3) and much more diverse paraphrases (diversity >3). We include our evaluation template in Appendix H. We measure inter-annotator agreement 13 The input is 4 \u00d7 50 sentences from each test set. using ordinal Krippendorff's alpha (Krippendorff, 2011), which yields 0.31 for fluency, 14 0.56 for semantic similarity, and 0.81 for diversity. All values are considered fair to good (Krippendorff, 2004).\n\nAdditionally, we perform a manual inspection and observe that model fine-tuned on MUL-TIPIT AUTO generates more diverse kinds of good paraphrases and much fewer poor paraphrases than the one trained on ParaNMT. We define five good paraphrase types and six poor paraphrase types. The definitions and results are shown in Table 7.\n\n\nOther Related Work\n\nBesides the several frequently used paraphrase datasets we mentioned above, here are a few other paraphrase corpora. The MSR Paraphrase corpus (Dolan and Brockett, 2005) contains 5,801 sentences pairs from news articles, but it has a deficiency that skewing toward over-identification (Das and Smith, 2009) \n\n\nConclusion\n\nIn this paper, we present the Multi-Topic Paraphrase in Twitter (MULTIPIT) corpus. Our work surpasses prior Twitter-based paraphrase corpora in topic diversity as well as the quality and quantity of annotation. Experimental results demonstrate the necessity of defining paraphrases based on downstream tasks. Our paraphrase generation evaluation shows that models trained on our corpus have better generation quality and generalizability compared to models fine-tuned on existing widely-used paraphrase datasets. We believe that MULTIPIT will facilitate further research in both paraphrase identification and paraphrase generation.\n\n\nLimitations\n\nWhile our study shows MULTIPIT AUTO improves paraphrase generation quality and diversity, we observe model sometimes generates Twitter-specific artifacts (i.e. \"@JoeBiden\"). Future work could investigate techniques to mine paraphrases from other social media platforms such as Reddit. Another limitation is that our dataset is only in English, future work could extend this to multilingual as Twitter is used by users from different countries that speak different languages. \n\n\nA Annotation Interface\n\nA.1 Crowdsourcing Figure 9 and Figure 10 display screenshots of the instruction and an example question of our crowdsourcing annotation for MULTIPIT CROWD .\n\nA.2 Expert Figure 11 displays a screenshot of the instruction of our expert annotation for MULTIPIT EXPERT .\n\n\nB Data Pre-processing\n\nBoth PIT-2015 (Xu et al., 2015) and Twitter URL (Lan et al., 2017) datasets share similar preprocessing steps that introduced tokenization and sentence splitting errors. Moreover, PIT-2015 contains some spam patterns, such as \"Follow Me PLEASE\". We improved the quality of our dataset by fixing the pre-processing methods and removing spam patterns. More importantly, we split tweets into sentences to get cleaner paraphrases (see Table  8 for an example), without added noises from extra sentences in the tweet. We improve the sentence splitting script by Xu et al. (2015) and tokenization script by O'Connor et al. (2010) used in prior work with a number of errors fixed: (1) Emojis and most symbols are cleaned while punctuation are kept; (2) Extremely short sentences (< 5 tokens) are filtered out while remaining sentences are deduplicated by comparing lowercased strings w/o any punctuation.\n\n\nRaw Tweets w/o Sent. Splitting\n\n\u2022 Horrible Crash on the Aurora Bridge in Seattle.\n\n\u2022 The crash on the Aurora Bridge in Seattle looks horrible. That was the bridge I took to work everyday. Yikes. Table 8: An example pair of raw tweets from our corpus. Annotating at tweet-level will include mismatched content and ambiguity. Cleaner paraphrase annotations can be acquired after sentence splitting.\n\n\nC Implementation Details\n\nWe use HuggingFace Transformers (Wolf et al., 2020) version of all pre-trained models. We use Python 3.8, PyTorch 1.9.0, and Transformers 4.12.0. For all experiments, we use 4 \u00d7 48GB NVIDIA A40 GPUs.\n\nParaphrase Identification. Hyperparameters for fine-tuning models in paraphrase identification experiments are given in Table 9. For T5 model, we consider learning rates \u2208 {1e-4, 3e-4, 1e-5, 3e-5}. For DeBERTaV3 model, we consider learning rates \u2208 {1e-5, 3e-5, 5e-6, 8e-6} following He et al. (2021). We fine-tune for 5 epochs and eval every 500 steps (every epoch if total training steps is less than 1500) on the dev set.\n\nThe only hyperparameter we tune is the learning rate and use F 1 on the dev set for model selection.\n\nFor Infersent and ESIM models, we use their original implementation initialized with GloVe embedding (Pennington et al., 2014), and also only tune the learning rate based on the dev set.\n\nParaphrase Generation. Hyperparameters for fine-tuning models in paraphrase generation experiments are given in Table 10.  We use perplexity on the dev set for model selection.\n\nAs ParaNMT contains only lowercase letters, we lowercase the input and references for generation and evaluation of the model fine-tuned on ParaNMT and lowercase the other models' generations while evaluating on ParaNMT.\n\n\nD GPT-3 Setup\n\n\nD.1 Hyperparameters\n\nWe use the text-davinci-002 GPT-3 model for paraphrase generation. To generate paraphrase, we use the following hyperparameters: tempera-ture=1, max tokens=100, top-p=0.9, best of=1, frequency penalty=0. 5, presence penalty=0.5, based on Chakrabarty et al. (2021).\n\n\nD.2 Prompts\n\nZero-shot setting: Your task is to generate a diverse paraphrase for a given sentence.\n\n\nSentence: {sentence} Paraphrase:\n\nFew-shot setting: You will be presented with examples of some input sentences and their paraphrases. Your task is to generate a diverse paraphrase for a given sentence.     . Here, * * * : p < 0.0001, * * : p < 0.001, * : p < 0.01. Overall is the summation score of all three aspects.\n\n\nE Generation Dataset Statistics\n\n\nMetric Fluency Semantic Diversity Overall Similarity\n\nSelf-BLEU \u2193 0.043 0.319 * * * -0.638 * * * -0.491 * * * BERT-Score 0.070 0.436 * * * -0.744 * * * -0.561 * * * BERT-iBLEU -0.036 -0.096 0.346 * * * 0.339 * * *  pects and \u223c0.1 with the overall score. Table 13 presents Spearman correlations for Self-BLEU, BERT-Score and BERT-iBLEU on all 400 generations. BERT-iBLEU outperforms the other two metrics. Because Self-BLEU measures diversity and BERT-Score measures semantic similarity, both metrics get the best correlation with human evaluation on the corresponding aspect but the worst correlation on the other one. Notably, Self-BLEU gets the highest correlation with the overall measurement, but the reason behind it is more differentiation in diversity ratings compared to semantic similarity, as shown in Figure 7. This makes diversity the biggest role in the overall score.  BLEU Filtering. We evaluate different BLEU thresholds on the dev set of MULTIPIT AUTO as shown in Figure 8. The model achieves the best performance at the threshold of 14, which is used across our experiments.\n\nNext, we compare model performance on all four datasets with and without BLEU filtering. Results are presented in Table 14. Applying BLEU filtering improves model performance with higher BERT-iBLEU on all datasets.   \n\n\nH Human Evaluation Details\n\nWe display our human evaluation instruction for each aspect (fluency, semantic similarity, diversity) in Figure 12,13,14.\n\nSource origin: MULTIPIT NMR Source sentence: In honor of Veterans Day, we salute all of the men and women who have served in America's armed forces. Generated sentences:\n\nT5 MULTIPIT AUTO \u2192 Today is Veterans Day, a day to remember those who have served. T5 Quora \u2192 On this Veterans Day, we salute all the men and women who have served our country. T5 M SCOCO \u2192 A group of men and women standing next to each other. T5 P araN M T \u2192 in honor of veterans'day, we salute all the men and women who served in america's armed forces.\n\nSource origin: Quora Source sentence: What is the scope for a food technologist in India? Generated sentences:\n\nT5 MULTIPIT AUTO \u2192 What is the job outlook for a food technologist in India? T5 Quora \u2192 What is the scope of food technology in India? T5 M SCOCO \u2192 a man in a white shirt is working in a kitchen T5 P araN M T \u2192 what is the role of a food technologist in india?\n\nSource origin: MSCOCO Source sentence: A bear is taking a walk through the forest. Generated sentences:\n\nT5 MULTIPIT AUTO \u2192 A bear is taking a walk in the woods. T5 Quora \u2192 What is a bear doing in the forest? T5 M SCOCO \u2192 A brown bear walking through a wooded area. T5 P araN M T \u2192 the bear is walking through the forest.\n\nSource origin: ParaNMT Source sentence: someone speaks in english over there. Generated sentences:\n\nT5 MULTIPIT AUTO \u2192 Someone has to speak english in this room. T5 Quora \u2192 What is the best way to learn English from someone who speaks fluently? T5 M SCOCO \u2192 A man standing in front of a wall with a sign on it. T5 P araN M T \u2192 someone's speaking english.   \n\n\nDiversity\n\nTo rate Diversity, you just answer the following question: Is sentence 2 different from sentence 1?\n\nHere is each score (1 to 5) represents: 5 -Uses more than 1 score 4 and 3 changes. Note: must contain at least 1 4 type changes.\n\n4 -Uses one of the following types of change 1 time:\n\nchange of sentence structure simplifying adding new phrase or meaningful word rearranging word order using idiomatic expressions change of part of speech expanding a word in detail synonym replacement phrase-wise (e.g. \"10 years\" <-> \"a decade\", \"hotel employee\" <-> \"bell boy\")\n\nOr uses synonym replacement word-wise more than 2 times.\n\nNote: mark 5 if sentence 1 contains less than 6 words.\n\n3 -Uses synonym replacement word-wise 1 or 2 times. Note: cases like \"is going to\" <-> \"will\", \"wanna\" <-> \"want to\", \"gonna\" <-> \"go to\" are wordwise synonym replacement as well.\n\n2 -Very simple grammatical changes such as: determiners changes (remove or add \"the\", \"the\" <-> \"a\", \"a\" <-> \"one\", \"that\" <-> \"it\", \"his\" <-> \"this\", \"some\" <-> \"any\", ...) contraction changes (\"n't\" <-> \"not\", \"'re\" <-> \"are\", \"will\" <-> \"'ll\", \"let's\" <-> \"let us\", ...) singular and plural switching (\"a\" <-> \"some\", \"are\" <-> \"is\", add \"es/s\", ...) tense changes (\"is\" <-> \"was\", \"did\" <-> \"have done\", \"is doing\" <-> \"do\", \"will\" <-> \"would\", ...) number and text switching (\"7\" <-> \"seven\", \"five\" <-> \"5\", ...) preposition changes (remove or add \"on\", \"at\" <-> \"on\", \"upon\" <-> \"on\", \"of\" <-> \"for\", ...) adding or removing conjunction word or meaningless word (\"... that ...\" <-> ... ...\", \"And ...\" <-> \"...\", \"just\", ...) other cases (\"to\" <-> \"will\") Note: Multiple 2 changes is still a 2.\n\n1 -Copies sentence 1 completely.\n\nNote: we ignore lettercase and punctuation issue.\n\n\nExamples\n\nsentence 1: How beautiful it is . sentence 2: Oh my god, this is so beautiful. Diversity score: 5 Figure 14: Instruction for rating diversity aspect in our human evaluation.\n\nFigure 1 :\n1Two sets of paraphrases in MULTIPIT, discussing a trending topic or a news article, respectively.\n\nFigure 2 :\n2Topic breakdown on 100 randomly sampled sentence pairs from MULTIPIT CROWD , PIT-2015 and Twitter-URL. Our MULTIPIT CROWD corpus has a more balanced topic distribution.\n\n\nRaffel et al., 2020), five masked language models, BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2019), BERTweet (Nguyen et al., 2020), and DeBER-TaV3 (He et al., 2021). We also include two competitive BiLSTM-based models, Infersent (Conneau et al., 2017) and ESIM (Chen et al., 2017), to establish comparison with pre-BERT era work.\n\nFigure 4 :\n4Test set performance of model fine-tuned on varying amounts of data in MULTIPIT EXPERT . model on the combination of MULTIPIT EXPERT and the re-labeled MULTIPIT CROWD . The experimental results are shown in\n\n\nfor paraphrase generation: BLEU (Papineni et al., 2002), Self-BLEU (Liu et al., 2021), BERT-Score (Zhang et al., 2020), and BERT-iBLEU (Niu et al., 2021). Self-BLEU is BLEU Model #Para. LR BL S-B \u2193 B-S B-iB\n\nFigure 5 :\n5Test set performance of model fine-tuned on varying amount of data in MULTIPIT AUTO , in terms of Self-BLEU (lower is better) and BERT-iBLEU.\n\nFigure 6 :\n6Human evaluation distributions on generations by model fine-tuned on MULTIPIT AUTO or ParaNMT.\n\n\n412 * * * -0.655 * * * -0.452 * * * BERT-Score 0.062 0.523 * * * -0.722 * * * -0.507 * * * BERT-iBLEU -0.166 -0.089 0.370 * * 0.381 * * *\n\nFigure 7 :\n7Label distribution of 1200 ratings on 400 generations by models fine-tuned on MULTIPIT AUTO and ParaNMT.\n\nFigure 8 :\n8MULTIPIT AUTO dev set performance on various BLEU filtering thresholds.\n\nFigure 10 : 9320 Figure 12 :\n10932012An example question of our crowdsourcing annotation on the Figure Eight platform for creating MULTIPIT CROWD . Instruction for rating fluency aspect in our human evaluation.\n\nFigure 13 :\n13Instruction for rating semantic similarity aspect in our human evaluation.9322\n\n\nTopic Domains #Train #Dev #Test Sent/Tweet Len %Paraphrase #Trends/URLs #Uniq Sent %Multi-Ref Our Multi-Topic Paraphrase in Twitter (MULTIPIT CROWD ) DatasetTrends \nSports \n25,255 3,157 3,157 \n10.24 / 13.79 \n40.52% \n1,201 \n34,786 \n17.89% \nEntertainment \n11,547 1,443 1,444 \n10.44 / 13.80 \n62.33% \n610 \n15,784 \n18.11% \nEvent \n8,624 1,078 1,079 \n10.86 / 15.32 \n82.83% \n359 \n11,746 \n17.75% \nOthers \n17,751 2,219 2,219 \n10.41 / 14.56 \n67.16% \n817 \n24,286 \n18.33% \n\nURL \n\nScience/Tech \n7,384 \n923 \n923 \n10.94 / 19.17 \n46.13% \n1,032 \n10,327 \n17.74% \nHealth \n9,123 1,140 1,141 \n11.29 / 21.68 \n46.78% \n1,298 \n12,772 \n17.86% \nPolitics \n7,981 \n998 \n998 \n10.95 / 18.48 \n56.56% \n1,063 \n10,999 \n17.68% \nFinance \n4,552 \n569 \n569 \n11.19 / 23.08 \n18.96% \n554 \n5,907 \n20.13% \nTotal \n92,217 11,527 11,530 \n10.62 / 16.10 \n53.73% \n6,934 \n124,438 \n18.65% \n\nOur MULTIPIT EXPERT Dataset 4,458 \n555 \n557 \n12.08 / 17.02 \n53.11% \n200 \n5,743 \n100% \n\nExisting Twitter Paraphrase Datasets \nPIT-2015 (Xu et al.) \n13,063 4,727 \n972 \n11.9 / -\n30.60% \n420 \n19,297 \n24.67% \nTwitter URL (Lan et al.) \n42,200 \n-\n9,324 \n-/ 14.8 \n22.77% \n5,187 \n48,906 \n23.91% \n\n\n\nTable 2 :\n2Results on the test sets of MULTIPIT CROWD and MULTIPIT EXPERT . Models are fine-tuned on the corresponding training set. DeBERTaV3 large performs the best on both datasets. LR: learning rate.\n\nTable 3 :\n3Results of different methods on the test set of MULTIPIT EXPERT . M C : MULTIPIT CROWD , M E : MULTI-PIT EXPERT . We use DeBERTaV3 large in the experiments.\n\nTable 4 :\n4Test set results of different transformer models fine-tuned on MULTIPIT AUTO , except GPT-3, where incontext learning is used. BL: BLEU, S-B: Self-BLEU, B-S: BERT-Score, B-iB: BERT-iBLEU. LR: learning rate. Bold: the best. The Self-BLEU of human reference is calculated by taking the min/avg/max score of the 8 references for each input sentence first, and then averaging across all scores.\n\nTable 6 :\n6Human evaluation results on generations by model fine-tuned on MULTIPIT AUTO or ParaNMT.\n\n\nand having high lexical overlap(Rus et al., 2014).PPDB (Ganitkevitch et al., 2013)   contains over 220 million phrase and lexical paraphrases without any sentence paraphrases.WikiAnswer (Fader et al., 2013)  consists of 18 million word-aligned question pairs. However, same as Quora, WikiAnswer is restricted to only questions. In addition, the Semantic Textual Similarity (STS) shared taskCer et al. (2017)  measures the degree Which is the best GRE coaching centre in Bangalore? Gen: what is the best gre training centre . . . Sent: Daniel Farke sacked by Norwich after first win of Premier League season over Brentford. Gen: Norwich sack Daniel Farke after first win of Premier League season.Type \n\nDefinition \nGeneration Example \nMAUTO ParaNMT \n\nGood Paraphrase Type \nNumber of occurrences per generation: 1.53 \n1.22 \n\nAdd New \nAdd new phrases while keeping \nthe meaning of given sentence. \nSent: relax, take it easy. Gen: Relax, take a deep breath, \nand enjoy the moment. \n0.18 \n0.03 \n\nWord Syn \nSubstitute a word with its syn-\nonym (another word). \nSent: 0.39 \n0.54 \n\nPhrase Syn \nReplace a phrase with synonym \nor expand a word to a phrase. \nSent: it looks goddamned foolish to put an oyster on the \nclam. Gen: Putting an oyster on a clam is a fucking joke. \n0.28 \n0.16 \n\nStructure \nUse different sentence structures \nto express the same thing. \nSent: Two big plates filled with some tasty looking food. \nGen: Two big plates of food, and the food looks good. \n0.28 \n0.23 \n\nSimplification Delete minor details or shorten \nphrases while maintaining the \nmeaning of given sentence. \n\n0.39 \n0.26 \n\nPoor Paraphrase Type \nPercentage in 200 generations: 26% \n44% \n\nCopy \nCopy the given sentence. \nSent: Did you have a good day today? Gen: Did you have a \ngood day today? \n14.5% \n10% \n\nSmall Change Only have small changes such as \nchanging article, tense, or prep. \nSent: FDA approves new test that can detect coronavirus in \n45 minutes. Gen: the fda has approved a new test . . . \n5.5% \n18% \n\nHallucination Add new info that is not com-\nmonsense or world knowledge. \nSent: A dog at a table wearing a birthday hat. Gen: A dog \nwearing a birthday hat at a dinner party. \n2.5% \n0% \n\nMiss Info \nMiss important info in the given \nsentence. \nSent: Very sad though that the amazing AJ and Kai will be \nmissing the final. Gen: AJ and Kai will not be in the final. \n1.5% \n1% \n\nMisinterpret \nMisinterpret or contradict mean-\ning of the given sentence. \nSent: Why are most first basemen left handed? Gen: why do \nmost of the first basemen have left hands? \n2% \n14% \n\nBad Grammar Contain grammar error. \nGen: what is the best earphones for rs 3000? \n0% \n1% \n\n\n\nTable 7 :\n7Paraphrase types with examples and statistics observed in the generations by models fine-tuned on MULTIPIT AUTO (M AUTO ) or ParaNMT. Statistics are based on manual inspection of generations by each model on 200 sampled sentences. The shown generation example for each type is by model with the higher value (bold).to which two sentences are semantically similar to \neach other. Since it doesn't make a binary judgment \nfor paraphrase relationships, it is not frequently \nused in paraphrase research. Recently, Dong et al. \n(2021) presents ParaSci, a large paraphrase dataset \nin the scientific field, and Kim et al. (2021) pro-\nposes BiSECT, a large split and rephrase corpus \nconstructed using machine translation. Our work \nfocuses on creating a large paraphrase corpus that \ncontains more diverse and natural human-authored \ntexts and investigating different paraphrase criteria. \n\n\n\n\nWilliam B. Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on Paraphrasing (IWP2005).Daniel Cer, Mona Diab, Eneko Agirre, I\u00f1igo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017 \ntask 1: Semantic textual similarity multilingual and \ncrosslingual focused evaluation. In Proceedings of \nthe 11th International Workshop on Semantic Evalua-\ntion (SemEval-2017). Association for Computational \nLinguistics. \n\nTuhin Chakrabarty, Debanjan Ghosh, Adam Poliak, and \nSmaranda Muresan. 2021. Figurative language in rec-\nognizing textual entailment. In Findings of the Asso-\nciation for Computational Linguistics: ACL-IJCNLP \n2021. \n\nMingda Chen, Qingming Tang, Sam Wiseman, and \nKevin Gimpel. 2019. Controllable paraphrase gener-\nation with a syntactic exemplar. In Proceedings of \nthe Association for Computational Linguistics. \n\nQian Chen, Xiaodan Zhu, Zhen-Hua Ling, Si Wei, Hui \nJiang, and Diana Inkpen. 2017. Enhanced LSTM for \nnatural language inference. In Proceedings of the \nAssociation for Computational Linguistics. \n\nWenqing Chen, Jidong Tian, Liqiang Xiao, Hao He, \nand Yaohui Jin. 2020. A semantically consistent and \nsyntactically variational encoder-decoder framework \n\nfor paraphrase generation. In Proceedings of Interna-\ntional Conference on Computational Linguistics. \n\nJacob Cohen. 1960. A coefficient of agreement for \nnominal scales. Educational and Psychological Mea-\nsurement. \n\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00efc \nBarrault, and Antoine Bordes. 2017. Supervised \nlearning of universal sentence representations from \nnatural language inference data. In Proceedings of \nEmpirical Methods in Natural Language Processing. \n\nDipanjan Das and Noah A. Smith. 2009. Paraphrase \nidentification as probabilistic quasi-synchronous \nrecognition. In Proceedings of the Joint Conference \nof the 47th Annual Meeting of the ACL and the 4th \nInternational Joint Conference on Natural Language \nProcessing of the AFNLP. \n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and \nKristina Toutanova. 2019. BERT: Pre-training of \ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the Conference of the \nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies. \n\nQingxiu Dong, Xiaojun Wan, and Yue Cao. 2021. \nParasci: A large scientific paraphrase dataset for \nlonger paraphrase generation. In Proceedings of the \nEuropean Chapter of the Association for Computa-\ntional Linguistics. \n\nAnthony Fader, Luke Zettlemoyer, and Oren Etzioni. \n2013. Paraphrase-driven learning for open question \nanswering. In Proceedings of the Association for \nComputational Linguistics. \n\nJames Ferguson, Colin Lockard, Daniel Weld, and Han-\nnaneh Hajishirzi. 2018. Semi-supervised event ex-\ntraction with paraphrase clusters. In Proceedings of \nthe Conference of the North American Chapter of the \nAssociation for Computational Linguistics: Human \nLanguage Technologies. \n\nWee Chung Gan and Hwee Tou Ng. 2019. Improving \nthe robustness of question answering systems to ques-\ntion paraphrasing. In Proceedings of the Association \nfor Computational Linguistics. \n\nJuri Ganitkevitch, Benjamin Van Durme, and Chris \nCallison-Burch. 2013. Ppdb: The paraphrase \ndatabase. In Proceedings of the Conference of the \nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies. \n\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2021. \nDebertav3: Improving deberta using electra-style pre-\ntraining with gradient-disentangled embedding shar-\ning. ArXiv. \nJoongwon Kim, Mounica Maddela, Reno Kriz, Wei Xu, \nand Chris Callison-Burch. 2021. BiSECT: Learning \nto split and rephrase sentences with bitexts. In Pro-\nceedings of Empirical Methods in Natural Language \nProcessing. \n\nKlaus Krippendorff. 2004. Reliability in content analy-\nsis: Some common misconceptions and recommen-\ndations. Human communication research. \n\nKlaus Krippendorff. 2011. Computing krippendorff's \nalpha-reliability. \n\nWuwei Lan, Siyu Qiu, Hua He, and Wei Xu. 2017. \nA continuously growing dataset of sentential para-\nphrases. In Proceedings of Empirical Methods in \nNatural Language Processing. \n\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, \nKevin Gimpel, Piyush Sharma, and Radu Soricut. \n2019. ALBERT: A lite BERT for self-supervised \nlearning of language representations. In Proceedings \nof International Conference on Learning Representa-\ntion. \n\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan \nGhazvininejad, Abdelrahman Mohamed, Omer Levy, \nVeselin Stoyanov, and Luke Zettlemoyer. 2020. \nBART: Denoising sequence-to-sequence pre-training \nfor natural language generation, translation, and com-\nprehension. In Proceedings of the Association for \nComputational Linguistics. \n\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, \nand Bill Dolan. 2016. A diversity-promoting ob-\njective function for neural conversation models. In \nProceedings of the 2016 Conference of the North \nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies, \npages 110-119, San Diego, California. Association \nfor Computational Linguistics. \n\nZichao Li, Xin Jiang, Lifeng Shang, and Qun Liu. 2019. \nDecomposable neural paraphrase generation. In Pro-\nceedings of the Association for Computational Lin-\nguistics. \n\nTsung-Yi Lin, Michael Maire, Serge Belongie, James \nHays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, \nand C Lawrence Zitnick. 2014. Microsoft coco: \nCommon objects in context. In European confer-\nence on computer vision, pages 740-755. \n\nTimothy Liu and De Wen Soh. 2022. Towards better \ncharacterization of paraphrases. In Proceedings of \nthe Association for Computational Linguistics. \n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis, \nLuke Zettlemoyer, and Veselin Stoyanov. 2019. \nRoberta: A robustly optimized bert pretraining ap-\nproach. Proceedings of International Conference on \nLearning Representation. \n\nYixin Liu, Graham Neubig, and John Wieting. 2021. \nOn learning text style transfer with direct rewards. In \nNAACL. \n\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and \nRyan T. McDonald. 2020. On faithfulness and fac-\ntuality in abstractive summarization. Proceedings of \nthe Association for Computational Linguistics. \n\nDat Quoc Nguyen, Thanh Vu, and Anh Tuan Nguyen. \n2020. BERTweet: A pre-trained language model \nfor English tweets. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language \nProcessing: System Demonstrations. \n\nTong Niu, Semih Yavuz, Yingbo Zhou, Nitish Shirish \nKeskar, Huan Wang, and Caiming Xiong. 2021. Un-\nsupervised paraphrasing with pretrained language \nmodels. In Proceedings of Empirical Methods in \nNatural Language Processing. \n\nBrendan O'Connor, Michel Krieger, and David Ahn. \n2010. Tweetmotif: Exploratory search and topic \nsummarization for twitter. In Fourth International \nAAAI Conference on Weblogs and Social Media. \n\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the \nAssociation for Computational Linguistics. \n\nJeffrey Pennington, Richard Socher, and Christopher \nManning. 2014. GloVe: Global vectors for word \nrepresentation. In Proceedings of Empirical Methods \nin Natural Language Processing. \n\nMatt Post. 2018. A call for clarity in reporting BLEU \nscores. In Proceedings of the Third Conference on \nMachine Translation: Research Papers. \n\nAlec Radford, Jeff Wu, Rewon Child, David Luan, \nDario Amodei, and Ilya Sutskever. 2019. Language \nmodels are unsupervised multitask learners. \n\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine \nLee, Sharan Narang, Michael Matena, Yanqi Zhou, \nWei Li, and Peter J Liu. 2020. Exploring the limits \nof transfer learning with a unified text-to-text trans-\nformer. Journal of Machine Learning Research. \n\nVasile Rus, Rajendra Banjade, and Mihai C. Lintean. \n2014. On paraphrase identification corpora. In Pro-\nceedings of the International Conference on Lan-\nguage Resources and Evaluation. \n\nBrian Thompson and Matt Post. 2020. Automatic ma-\nchine translation evaluation in many languages via \nzero-shot paraphrasing. In Proceedings of Empirical \nMethods in Natural Language Processing. \n\nSoroush Vosoughi and Deb Roy. 2016. A semi-\nautomatic method for efficient detection of stories \non social media. In Tenth International AAAI Confer-\nence on Web and Social Media. \n\nJohn Wieting and Kevin Gimpel. 2018. ParaNMT-50M: \nPushing the limits of paraphrastic sentence embed-\ndings with millions of machine translations. In Pro-\nceedings of the Association for Computational Lin-\nguistics. \n\n\nTable 10 :\n10Hyperparameters for paraphrase generation.\n\n\nSentence: Mike Bloomberg is sending $18 million from his defunct presidential campaign to the DNC . Paraphrase: Mike Bloomberg is transferring $18M from his campaign to DNC , stretching campaign finance law . Sentence: Google Assistant on Android can read web pages to you Paraphrase: Google Assist lets your Android devices read entire web pages aloud Sentence: Charlie Patino scored a goal on his debut ! Paraphrase: Charlie Patino's debut and he capped it off with a goal . Sentence: khem birch is the difference maker for the raptors this game Paraphrase: Khem Birch may be the MVP tonight for the Raptors .Sentence: {sentence} \nParaphrase: \n\n\n\nTable 11\n11presents the detailed statistics of MULTI-PIT AUTO , Quora, MSCOCO and ParaNMT. M AUTO Quora MSCOCO ParaNMTGenre \nTwitter Question Description Novels, Laws \nSentence Length 11.34 \n9.66 \n10.49 \n11.33 \nSentence BLEU 24.48 \n26.37 \n9.30 \n24.85 \n\nTrain/dev/test split \n#Train w/o BF 290,395 134,378 331,330 \n50M \n#Train \n136,645 47,393 \n275,583 \n443,512 \n#Dev \n215 \n5,255 \n20,186 \n499 \n#Test \n200 \n5,255 \n20,187 \n781 \n#Test Refs \n8 \n1.34 \n4 \n1 \n\n\n\nTable 11 :\n11Statistics of datasets for paraphrase generation. We calculate sentence length based on the number of tokens per unique sentence. As ParaNMT is too large, we sample 500K for the calculation of sentence length and BLEU. W/o BF denotes without BLEU filtering.\n\nTable 12 :\n12Spearman correlations with human evaluation on 100 generations on MULTIPIT NMR (50 by model trained on MULTIPIT AUTO and 50 by model trained on ParaNMT)\n\nTable 13 :\n13Spearman correlations with human evaluation on all 400 generations. Here, * * * : p < 0.0001, * * : p < 0.001, * : p < 0.01. Analysis. With human evaluation, we calculate Spearman correlation to evaluate automatic metric quality. Since the four test sets have different numbers of references and MUL-TIPIT NMR has the most number of references, to evaluate BLEU, we examine 100 generations on MULTIPIT NMR (50 by T5 large fine-tuned on MULTIPIT AUTO and 50 by T5 large fine-tuned on ParaNMT). Results are shown inTable 12. BLEU gets a weak correlation around |0.2| with all as-Correlation \n\n\nTraining Data LR BL S-B \u2193 B-S B-iBM AUTO w/o BF \n1e-4 45.85 65.03 91.86 59.87 \nM AUTO \n1e-4 41.14 33.34 85.86 77.79 \n\nQuora w/o BF \n3e-4 36.63 54.54 91.27 61.58 \nQuora \n1e-4 28.72 34.23 87.97 73.54 \n\nMSCOCO w/o BF 3e-4 28.15 23.39 82.99 78.89 \nMSCOCO \n1e-4 26.14 15.46 81.00 80.30 \n\nParaNMT w/o BF 1e-5 19.43 37.59 88.07 76.28 \nParaNMT \n3e-5 20.36 33.35 86.90 77.51 \n\n\n\nTable 14 :\n14In-domain test set results of fine-tuning model on data with or without BLEU filtering. w/o BF denotes without BLEU filtering. of Definition. We investigate how different paraphrase definitions affect generation performance. As shown in Table 15, model fine-tuned on MULTIPIT AUTO outperforms fine-tuning on the loosely defined data such as MULTIPIT CROWD . Size BL S-B \u2193 B-S B-iB MULTIPIT CROWD 26,091 36.15 32.09 85.53 74.19 M AUTO -CROWD 326,517 45.55 37.90 85.80 74.12 M AUTO 136,645 41.14 33.34 85.86 77.79Impact Data \n\n\nTable 15 :\n15Test set results of models fine-tuned on data constructed with different paraphrase definitions. MUL-TIPIT CROWD contains its paraphrase pairs. M AUTO -CROWD is the automatically identified paraphrase pairs by the identifier fine-tuned on MULTIPIT CROWD . Generation Examples.Table 16 presents generation examples by GPT-3 and fine-tuned T5 large on MULTIPIT NMR . Table 17 presents generation examples by T5 large fine-tuned on MULTIPIT AUTO , Quora, MSCOCO, and ParaNMT. Multi-Reference Examples. Table 18 displays three examples from the MULTIPIT NMR test set.G Examples \n\n\n\nTable 17 :\n17Paraphrase generation examples by T5 large fine-tuned on MULTIPIT AUTO , Quora, MSCOCO, and ParaNMT on each test set.\n63% of sentences in Twitter-URL are related to the 2016 US presidential election, and 58% of sentences in PIT-2015 are about NFL draft (more detailed analysis in \u00a7 2.4).\nhttps://www.kaggle.com/c/ quora-question-pairs\nhttps://www.twitter.com/explore/tabs/trending 4 https://developer.twitter.com/en/docs/ twitter-api\nFor example, URL https://www.nytimes.com/2019/ 08/09/science/komodo-dragon-genome.html belongs to science topic.\nhttps://www.appen.com/\nThe example C1 and C2 is on the more extreme side of the \"loose\" paraphrase criterion from the linguistic perspective, more average cases are shown inFigure 1.\nWe perform a small grid search on \u03bb over {0.05, 0.15, 0.25, 0.35, 0.45}, and find 0.35 works well for the filtering method and 0.25 for the flipping method.\nhttps://www.kaggle.com/c/ quora-question-pairs\nFuture identified paraphrase pairs will be released every month.\nSince over 95% ratings of fluency fall into the same point (seeFigure 7in the Appendix), Krippendorff's alpha will stay low no matter how often the raters agree.\nInstructionA and B is a paraphrase pair if:\nAcknowledgmentsWe thank Yang Chen as well as three anonymous reviewers for their helpful feedback on this work. We also thank Andrew Duffy, Elizabeth Liu, Ian Ligon, Rachel Choi, Jonathan Zhou, Chase Perry, Panya Bhinder for their help on annotations and human evaluation. This research is supported in part by the NSF awards IIS-2144493 and IIS-2112633, ODNI and IARPA via the BETTER program (con-\nTransformers: State-of-the-art natural language processing. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Clara Patrick Von Platen, Yacine Ma, Julien Jernite, Canwen Plu, Teven Le Xu, Sylvain Scao, Mariama Gugger, Quentin Drame, Alexander Lhoest, Rush, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2020 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, Remi Louf, Morgan Funtow- icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Trans- formers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations.\n\nSelf-training with noisy student improves imagenet classification. Qizhe Xie, Eduard H Hovy, Minh-Thang Luong, V Quoc, Le, IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Qizhe Xie, Eduard H. Hovy, Minh-Thang Luong, and Quoc V. Le. 2020. Self-training with noisy student improves imagenet classification. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recog- nition (CVPR).\n\nSemEval-2015 Task 1: Paraphrase and semantic similarity in Twitter (PIT). Wei Xu, Chris Callison-Burch, William B Dolan, Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval). the 9th International Workshop on Semantic Evaluation (SemEval)Wei Xu, Chris Callison-Burch, and William B. Dolan. 2015. SemEval-2015 Task 1: Paraphrase and seman- tic similarity in Twitter (PIT). In Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval).\n\nExploiting parallel news streams for unsupervised event extraction. Congle Zhang, Stephen Soderland, Daniel S Weld, Transactions of the Association for Computational Linguistics. Congle Zhang, Stephen Soderland, and Daniel S. Weld. 2015. Exploiting parallel news streams for unsuper- vised event extraction. Transactions of the Associa- tion for Computational Linguistics.\n\nBertscore: Evaluating text generation with bert. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, Yoav Artzi, Proceedings of International Conference on Learning Representation. International Conference on Learning RepresentationTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evalu- ating text generation with bert. Proceedings of Inter- national Conference on Learning Representation.\n\nGeneralized cross entropy loss for training deep neural networks with noisy labels. Zhilu Zhang, Mert Rory Sabuncu, Proceedings of Advances in Neural Information Processing Systems. Advances in Neural Information Processing SystemsZhilu Zhang and Mert Rory Sabuncu. 2018. General- ized cross entropy loss for training deep neural net- works with noisy labels. In Proceedings of Advances in Neural Information Processing Systems.\n\nParaphrase generation: A survey of the state of the art. Jianing Zhou, Suma Bhat, Proceedings of Empirical Methods in Natural Language Processing. Empirical Methods in Natural Language ProcessingJianing Zhou and Suma Bhat. 2021. Paraphrase genera- tion: A survey of the state of the art. In Proceedings of Empirical Methods in Natural Language Process- ing.\n\nSource sentence: Today is the winter solstice, marking the beginning of winter in the Northern Hemisphere. Generated sentences: T5 large \u2192 Today marks the beginning of winter in the Northern Hemisphere. GPT-3 zero-shot \u2192 Today is the first day of winter in the Northern HemisphereSource sentence: Today is the winter solstice, marking the beginning of winter in the Northern Hemisphere. Generated sentences: T5 large \u2192 Today marks the beginning of winter in the Northern Hemisphere. GPT-3 zero-shot \u2192 Today is the first day of winter in the Northern Hemisphere.\n\nSource sentence: Elizabeth Warren and Cory Booker test positive for COVID in breakthrough cases Generated sentences: T5 large \u2192 Cory Booker and Elizabeth Warren have tested positive for COVID-19. GPT-3 few-shot \u2192 Today is the winter solstice, which means winter has arrived in the Northern HemisphereGPT-3 few-shot \u2192 Today is the winter solstice, which means winter has arrived in the Northern Hemisphere. Source sentence: Elizabeth Warren and Cory Booker test positive for COVID in breakthrough cases Generated sentences: T5 large \u2192 Cory Booker and Elizabeth Warren have tested positive for COVID-19.\n\nGPT-3 zero-shot \u2192 In a breakthrough, Elizabeth Warren and Cory Booker test positive for COVID. GPT-3 zero-shot \u2192 In a breakthrough, Elizabeth Warren and Cory Booker test positive for COVID.\n\nGPT-3 few-shot \u2192 Elizabeth Warren and Cory Booker have both contracted COVID-19 in what are being called breakthrough cases. GPT-3 few-shot \u2192 Elizabeth Warren and Cory Booker have both contracted COVID-19 in what are being called breakthrough cases.\n\nSource sentence: Former Oklahoma quarterback Spencer Rattler is transferring to University of South Carolina. Generated sentences: T5 large \u2192 Former Oklahoma QB Spencer Rattler is transferring to the University of South Carolina. Source sentence: Former Oklahoma quarterback Spencer Rattler is transferring to University of South Carolina. Generated sentences: T5 large \u2192 Former Oklahoma QB Spencer Rattler is transferring to the University of South Carolina.\n\nGPT-3 zero-shot \u2192 Former Oklahoma quarterback Spencer Rattler is transferring to the University of South Carolina. GPT-3 few-shot \u2192 former OU quarterback Spencer Rattler is transferring to the University of South Carolina. Source sentence: The Vancouver Canucks have fired head coach Travis Green and hired Bruce Boudreau as a replacement. Generated sentences: T5 large \u2192 Canucks fire Travis Green, hire Bruce Boudreau as new head coach GPT-3 zero-shot \u2192 The Vancouver Canucks have replaced head. coach Travis Green with Bruce BoudreauGPT-3 zero-shot \u2192 Former Oklahoma quarterback Spencer Rattler is transferring to the University of South Carolina. GPT-3 few-shot \u2192 former OU quarterback Spencer Rattler is transferring to the University of South Carolina. Source sentence: The Vancouver Canucks have fired head coach Travis Green and hired Bruce Boudreau as a replacement. Generated sentences: T5 large \u2192 Canucks fire Travis Green, hire Bruce Boudreau as new head coach GPT-3 zero-shot \u2192 The Vancouver Canucks have replaced head coach Travis Green with Bruce Boudreau.\n\nGPT-3 few-shot \u2192 The Vancouver Canucks have let go of head coach Travis Green and replaced him with Bruce Boudreau. Source sentence: What an incredible tribute to Eddie Kidd on Top Gear. Generated sentences: secondhand stress? GPT-3 zero-shot \u2192 There are many ways to build immunity against secondhand stress. Some methods include yoga, meditation. and aromatherapyGPT-3 few-shot \u2192 The Vancouver Canucks have let go of head coach Travis Green and replaced him with Bruce Boudreau. Source sentence: What an incredible tribute to Eddie Kidd on Top Gear. Generated sentences: secondhand stress? GPT-3 zero-shot \u2192 There are many ways to build immunity against secondhand stress. Some methods include yoga, meditation, and aromatherapy.\n\nParaphrase generation examples by GPT-3 and fine-tuned T5 large on MULTIPIT NMR. 16Table 16: Paraphrase generation examples by GPT-3 and fine-tuned T5 large on MULTIPIT NMR .\n\nSource sentence: @GovStitt Please grant clemency for Julius Jones, an innocent man scheduled for execution in your state. ReferencesSource sentence: @GovStitt Please grant clemency for Julius Jones, an innocent man scheduled for execution in your state. References:\n\n@GovStitt I join the many , many voices urging you to do the right thing and grant clemency to Julius Jones. @GovStitt I join the many , many voices urging you to do the right thing and grant clemency to Julius Jones.\n\n@GovStitt please do the right thing and don't execute julius jones. @GovStitt please do the right thing and don't execute julius jones.\n\n@OKFirstLady Please urge your husband @GovStitt to grant Julius Jones clemency. @OKFirstLady Please urge your husband @GovStitt to grant Julius Jones clemency.\n\nWhenever you don't absolutely need it, you should go ahead and turn off your Bluetooth. Whenever you don't absolutely need it, you should go ahead and turn off your Bluetooth.\n\nKeep Bluetooth off when you are not using it. Keep Bluetooth off when you are not using it.\n\nWhenever you don't need BlueTooth, you should turn it off. Whenever you don't need BlueTooth, you should turn it off\n\nIf you don't need your Bluetooth enabled, then turn it off! Table 18: Three examples from MULTIPIT NMR. If you don't need your Bluetooth enabled, then turn it off! Table 18: Three examples from MULTIPIT NMR .\n\nInstruction of our crowdsourcing annotation on the Figure Eight platform for creating MULTIPIT CROWD. 9Figure 9: Instruction of our crowdsourcing annotation on the Figure Eight platform for creating MULTIPIT CROWD .\n", "annotations": {"author": "[{\"end\":172,\"start\":82},{\"end\":271,\"start\":173},{\"end\":366,\"start\":272}]", "publisher": null, "author_last_name": "[{\"end\":89,\"start\":86},{\"end\":183,\"start\":178},{\"end\":278,\"start\":276}]", "author_first_name": "[{\"end\":85,\"start\":82},{\"end\":177,\"start\":173},{\"end\":275,\"start\":272}]", "author_affiliation": "[{\"end\":171,\"start\":107},{\"end\":270,\"start\":206},{\"end\":365,\"start\":301}]", "title": "[{\"end\":60,\"start\":1},{\"end\":426,\"start\":367}]", "venue": "[{\"end\":514,\"start\":428}]", "abstract": "[{\"end\":1788,\"start\":606}]", "bib_ref": "[{\"end\":1897,\"start\":1874},{\"end\":2125,\"start\":2104},{\"end\":2180,\"start\":2156},{\"end\":2297,\"start\":2280},{\"end\":2347,\"start\":2322},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2463,\"start\":2443},{\"end\":2485,\"start\":2463},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2538,\"start\":2537},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2837,\"start\":2820},{\"end\":2872,\"start\":2842},{\"end\":3257,\"start\":3234},{\"end\":3275,\"start\":3257},{\"end\":5381,\"start\":5334},{\"end\":5394,\"start\":5381},{\"end\":5408,\"start\":5407},{\"end\":5880,\"start\":5864},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":6540,\"start\":6523},{\"end\":6557,\"start\":6540},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7071,\"start\":7070},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7238,\"start\":7221},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7741,\"start\":7724},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7975,\"start\":7972},{\"end\":7977,\"start\":7975},{\"end\":7985,\"start\":7981},{\"end\":7988,\"start\":7985},{\"end\":7996,\"start\":7992},{\"end\":7999,\"start\":7996},{\"end\":10000,\"start\":9987},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10553,\"start\":10536},{\"end\":10588,\"start\":10558},{\"end\":11393,\"start\":11372},{\"end\":11414,\"start\":11393},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15244,\"start\":15226},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15268,\"start\":15244},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":15586,\"start\":15585},{\"end\":16890,\"start\":16872},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17030,\"start\":17009},{\"end\":19015,\"start\":18998},{\"end\":19032,\"start\":19015},{\"end\":19661,\"start\":19637},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":20392,\"start\":20373},{\"end\":20396,\"start\":20394},{\"end\":23611,\"start\":23609},{\"end\":23866,\"start\":23846},{\"end\":24526,\"start\":24505},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26014,\"start\":25997},{\"end\":26049,\"start\":26031},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26556,\"start\":26540},{\"end\":26606,\"start\":26584},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":27359,\"start\":27340},{\"end\":27808,\"start\":27792},{\"end\":28162,\"start\":28137},{\"end\":28924,\"start\":28865}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":34489,\"start\":34379},{\"attributes\":{\"id\":\"fig_1\"},\"end\":34671,\"start\":34490},{\"attributes\":{\"id\":\"fig_2\"},\"end\":35037,\"start\":34672},{\"attributes\":{\"id\":\"fig_4\"},\"end\":35257,\"start\":35038},{\"attributes\":{\"id\":\"fig_5\"},\"end\":35466,\"start\":35258},{\"attributes\":{\"id\":\"fig_6\"},\"end\":35621,\"start\":35467},{\"attributes\":{\"id\":\"fig_7\"},\"end\":35729,\"start\":35622},{\"attributes\":{\"id\":\"fig_8\"},\"end\":35869,\"start\":35730},{\"attributes\":{\"id\":\"fig_9\"},\"end\":35987,\"start\":35870},{\"attributes\":{\"id\":\"fig_11\"},\"end\":36072,\"start\":35988},{\"attributes\":{\"id\":\"fig_12\"},\"end\":36284,\"start\":36073},{\"attributes\":{\"id\":\"fig_13\"},\"end\":36378,\"start\":36285},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":37505,\"start\":36379},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":37710,\"start\":37506},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":37879,\"start\":37711},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":38282,\"start\":37880},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":38383,\"start\":38283},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":41033,\"start\":38384},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":41932,\"start\":41034},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":50703,\"start\":41933},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":50760,\"start\":50704},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":51410,\"start\":50761},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":51864,\"start\":51411},{\"attributes\":{\"id\":\"tab_21\",\"type\":\"table\"},\"end\":52136,\"start\":51865},{\"attributes\":{\"id\":\"tab_22\",\"type\":\"table\"},\"end\":52303,\"start\":52137},{\"attributes\":{\"id\":\"tab_23\",\"type\":\"table\"},\"end\":52907,\"start\":52304},{\"attributes\":{\"id\":\"tab_24\",\"type\":\"table\"},\"end\":53278,\"start\":52908},{\"attributes\":{\"id\":\"tab_25\",\"type\":\"table\"},\"end\":53817,\"start\":53279},{\"attributes\":{\"id\":\"tab_26\",\"type\":\"table\"},\"end\":54408,\"start\":53818},{\"attributes\":{\"id\":\"tab_27\",\"type\":\"table\"},\"end\":54540,\"start\":54409}]", "paragraph": "[{\"end\":4006,\"start\":1804},{\"end\":4057,\"start\":4008},{\"end\":4743,\"start\":4059},{\"end\":5494,\"start\":4745},{\"end\":5698,\"start\":5521},{\"end\":6404,\"start\":5723},{\"end\":7184,\"start\":6443},{\"end\":8600,\"start\":7208},{\"end\":8773,\"start\":8651},{\"end\":9600,\"start\":8775},{\"end\":10086,\"start\":9602},{\"end\":10420,\"start\":10088},{\"end\":11226,\"start\":10422},{\"end\":11911,\"start\":11274},{\"end\":13197,\"start\":11931},{\"end\":13543,\"start\":13199},{\"end\":13991,\"start\":13573},{\"end\":14465,\"start\":13993},{\"end\":14803,\"start\":14476},{\"end\":16395,\"start\":14815},{\"end\":16827,\"start\":16443},{\"end\":17818,\"start\":16865},{\"end\":18497,\"start\":17820},{\"end\":18900,\"start\":18499},{\"end\":19432,\"start\":18923},{\"end\":21241,\"start\":19454},{\"end\":21659,\"start\":21243},{\"end\":21788,\"start\":21692},{\"end\":23867,\"start\":21790},{\"end\":24197,\"start\":23869},{\"end\":24527,\"start\":24220},{\"end\":25173,\"start\":24542},{\"end\":25664,\"start\":25189},{\"end\":25847,\"start\":25691},{\"end\":25957,\"start\":25849},{\"end\":26880,\"start\":25983},{\"end\":26964,\"start\":26915},{\"end\":27279,\"start\":26966},{\"end\":27507,\"start\":27308},{\"end\":27932,\"start\":27509},{\"end\":28034,\"start\":27934},{\"end\":28222,\"start\":28036},{\"end\":28400,\"start\":28224},{\"end\":28621,\"start\":28402},{\"end\":28925,\"start\":28661},{\"end\":29027,\"start\":28941},{\"end\":29348,\"start\":29064},{\"end\":30477,\"start\":29439},{\"end\":30696,\"start\":30479},{\"end\":30848,\"start\":30727},{\"end\":31019,\"start\":30850},{\"end\":31376,\"start\":31021},{\"end\":31488,\"start\":31378},{\"end\":31750,\"start\":31490},{\"end\":31855,\"start\":31752},{\"end\":32073,\"start\":31857},{\"end\":32173,\"start\":32075},{\"end\":32432,\"start\":32175},{\"end\":32545,\"start\":32446},{\"end\":32675,\"start\":32547},{\"end\":32729,\"start\":32677},{\"end\":33009,\"start\":32731},{\"end\":33067,\"start\":33011},{\"end\":33123,\"start\":33069},{\"end\":33304,\"start\":33125},{\"end\":34107,\"start\":33306},{\"end\":34141,\"start\":34109},{\"end\":34192,\"start\":34143},{\"end\":34378,\"start\":34205}]", "formula": null, "table_ref": "[{\"end\":3603,\"start\":3596},{\"end\":5697,\"start\":5690},{\"attributes\":{\"ref_id\":\"tab_26\"},\"end\":11583,\"start\":11575},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":14535,\"start\":14528},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":15621,\"start\":15614},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":18764,\"start\":18756},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":19866,\"start\":19859},{\"end\":22138,\"start\":22131},{\"end\":22384,\"start\":22377},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":23028,\"start\":23021},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":24196,\"start\":24189},{\"end\":26422,\"start\":26414},{\"end\":27085,\"start\":27078},{\"end\":27636,\"start\":27629},{\"attributes\":{\"ref_id\":\"tab_18\"},\"end\":28344,\"start\":28336},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":29647,\"start\":29639},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":30601,\"start\":30593}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1802,\"start\":1790},{\"attributes\":{\"n\":\"2\"},\"end\":5519,\"start\":5497},{\"attributes\":{\"n\":\"2.1\"},\"end\":5721,\"start\":5701},{\"attributes\":{\"n\":\"2.2\"},\"end\":6441,\"start\":6407},{\"attributes\":{\"n\":\"2.3\"},\"end\":7206,\"start\":7187},{\"attributes\":{\"n\":\"2.4\"},\"end\":8649,\"start\":8603},{\"attributes\":{\"n\":\"2.5\"},\"end\":11272,\"start\":11229},{\"end\":11929,\"start\":11914},{\"attributes\":{\"n\":\"3\"},\"end\":13571,\"start\":13546},{\"attributes\":{\"n\":\"3.1\"},\"end\":14474,\"start\":14468},{\"attributes\":{\"n\":\"3.2\"},\"end\":14813,\"start\":14806},{\"attributes\":{\"n\":\"3.3\"},\"end\":16417,\"start\":16398},{\"attributes\":{\"n\":\"4\"},\"end\":16441,\"start\":16420},{\"attributes\":{\"n\":\"4.1\"},\"end\":16863,\"start\":16830},{\"attributes\":{\"n\":\"4.2\"},\"end\":18921,\"start\":18903},{\"attributes\":{\"n\":\"4.3\"},\"end\":19452,\"start\":19435},{\"attributes\":{\"n\":\"4.4\"},\"end\":21690,\"start\":21662},{\"attributes\":{\"n\":\"5\"},\"end\":24218,\"start\":24200},{\"attributes\":{\"n\":\"6\"},\"end\":24540,\"start\":24530},{\"end\":25187,\"start\":25176},{\"end\":25689,\"start\":25667},{\"end\":25981,\"start\":25960},{\"end\":26913,\"start\":26883},{\"end\":27306,\"start\":27282},{\"end\":28637,\"start\":28624},{\"end\":28659,\"start\":28640},{\"end\":28939,\"start\":28928},{\"end\":29062,\"start\":29030},{\"end\":29382,\"start\":29351},{\"end\":29437,\"start\":29385},{\"end\":30725,\"start\":30699},{\"end\":32444,\"start\":32435},{\"end\":34203,\"start\":34195},{\"end\":34390,\"start\":34380},{\"end\":34501,\"start\":34491},{\"end\":35049,\"start\":35039},{\"end\":35478,\"start\":35468},{\"end\":35633,\"start\":35623},{\"end\":35881,\"start\":35871},{\"end\":35999,\"start\":35989},{\"end\":36102,\"start\":36074},{\"end\":36297,\"start\":36286},{\"end\":37516,\"start\":37507},{\"end\":37721,\"start\":37712},{\"end\":37890,\"start\":37881},{\"end\":38293,\"start\":38284},{\"end\":41044,\"start\":41035},{\"end\":50715,\"start\":50705},{\"end\":51420,\"start\":51412},{\"end\":51876,\"start\":51866},{\"end\":52148,\"start\":52138},{\"end\":52315,\"start\":52305},{\"end\":53290,\"start\":53280},{\"end\":53829,\"start\":53819},{\"end\":54420,\"start\":54410}]", "table": "[{\"end\":37505,\"start\":36538},{\"end\":41033,\"start\":39081},{\"end\":41932,\"start\":41361},{\"end\":50703,\"start\":42118},{\"end\":51410,\"start\":51374},{\"end\":51864,\"start\":51530},{\"end\":52907,\"start\":52895},{\"end\":53278,\"start\":52944},{\"end\":53817,\"start\":53804},{\"end\":54408,\"start\":54395}]", "figure_caption": "[{\"end\":34489,\"start\":34392},{\"end\":34671,\"start\":34503},{\"end\":35037,\"start\":34674},{\"end\":35257,\"start\":35051},{\"end\":35466,\"start\":35260},{\"end\":35621,\"start\":35480},{\"end\":35729,\"start\":35635},{\"end\":35869,\"start\":35732},{\"end\":35987,\"start\":35883},{\"end\":36072,\"start\":36001},{\"end\":36284,\"start\":36111},{\"end\":36378,\"start\":36300},{\"end\":36538,\"start\":36381},{\"end\":37710,\"start\":37518},{\"end\":37879,\"start\":37723},{\"end\":38282,\"start\":37892},{\"end\":38383,\"start\":38295},{\"end\":39081,\"start\":38386},{\"end\":41361,\"start\":41046},{\"end\":42118,\"start\":41935},{\"end\":50760,\"start\":50718},{\"end\":51374,\"start\":50763},{\"end\":51530,\"start\":51423},{\"end\":52136,\"start\":51879},{\"end\":52303,\"start\":52151},{\"end\":52895,\"start\":52318},{\"end\":52944,\"start\":52910},{\"end\":53804,\"start\":53293},{\"end\":54395,\"start\":53832},{\"end\":54540,\"start\":54423}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4742,\"start\":4734},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5801,\"start\":5793},{\"end\":9389,\"start\":9381},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10598,\"start\":10590},{\"end\":11910,\"start\":11902},{\"end\":14527,\"start\":14526},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":15991,\"start\":15983},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":21226,\"start\":21217},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":21272,\"start\":21264},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":23347,\"start\":23339},{\"end\":25717,\"start\":25709},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":25731,\"start\":25722},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":25869,\"start\":25860},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":30205,\"start\":30197},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":30374,\"start\":30366},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30841,\"start\":30832},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34312,\"start\":34303}]", "bib_author_first_name": "[{\"end\":56093,\"start\":56087},{\"end\":56108,\"start\":56100},{\"end\":56122,\"start\":56116},{\"end\":56135,\"start\":56129},{\"end\":56153,\"start\":56146},{\"end\":56171,\"start\":56164},{\"end\":56184,\"start\":56177},{\"end\":56196,\"start\":56193},{\"end\":56208,\"start\":56204},{\"end\":56221,\"start\":56215},{\"end\":56236,\"start\":56233},{\"end\":56249,\"start\":56246},{\"end\":56265,\"start\":56260},{\"end\":56292,\"start\":56286},{\"end\":56303,\"start\":56297},{\"end\":56319,\"start\":56313},{\"end\":56330,\"start\":56325},{\"end\":56333,\"start\":56331},{\"end\":56345,\"start\":56338},{\"end\":56359,\"start\":56352},{\"end\":56375,\"start\":56368},{\"end\":56392,\"start\":56383},{\"end\":57195,\"start\":57190},{\"end\":57207,\"start\":57201},{\"end\":57209,\"start\":57208},{\"end\":57226,\"start\":57216},{\"end\":57235,\"start\":57234},{\"end\":57607,\"start\":57604},{\"end\":57617,\"start\":57612},{\"end\":57641,\"start\":57634},{\"end\":57643,\"start\":57642},{\"end\":58086,\"start\":58080},{\"end\":58101,\"start\":58094},{\"end\":58119,\"start\":58113},{\"end\":58121,\"start\":58120},{\"end\":58441,\"start\":58435},{\"end\":58455,\"start\":58449},{\"end\":58470,\"start\":58465},{\"end\":58481,\"start\":58475},{\"end\":58483,\"start\":58482},{\"end\":58500,\"start\":58496},{\"end\":58922,\"start\":58917},{\"end\":58939,\"start\":58930},{\"end\":59327,\"start\":59320},{\"end\":59338,\"start\":59334}]", "bib_author_last_name": "[{\"end\":56098,\"start\":56094},{\"end\":56114,\"start\":56109},{\"end\":56127,\"start\":56123},{\"end\":56144,\"start\":56136},{\"end\":56162,\"start\":56154},{\"end\":56175,\"start\":56172},{\"end\":56191,\"start\":56185},{\"end\":56202,\"start\":56197},{\"end\":56213,\"start\":56209},{\"end\":56231,\"start\":56222},{\"end\":56244,\"start\":56237},{\"end\":56258,\"start\":56250},{\"end\":56284,\"start\":56266},{\"end\":56295,\"start\":56293},{\"end\":56311,\"start\":56304},{\"end\":56323,\"start\":56320},{\"end\":56336,\"start\":56334},{\"end\":56350,\"start\":56346},{\"end\":56366,\"start\":56360},{\"end\":56381,\"start\":56376},{\"end\":56399,\"start\":56393},{\"end\":56405,\"start\":56401},{\"end\":57199,\"start\":57196},{\"end\":57214,\"start\":57210},{\"end\":57232,\"start\":57227},{\"end\":57240,\"start\":57236},{\"end\":57244,\"start\":57242},{\"end\":57610,\"start\":57608},{\"end\":57632,\"start\":57618},{\"end\":57649,\"start\":57644},{\"end\":58092,\"start\":58087},{\"end\":58111,\"start\":58102},{\"end\":58126,\"start\":58122},{\"end\":58447,\"start\":58442},{\"end\":58463,\"start\":58456},{\"end\":58473,\"start\":58471},{\"end\":58494,\"start\":58484},{\"end\":58506,\"start\":58501},{\"end\":58928,\"start\":58923},{\"end\":58947,\"start\":58940},{\"end\":59332,\"start\":59328},{\"end\":59343,\"start\":59339}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":208117506},\"end\":57121,\"start\":56027},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":207853355},\"end\":57528,\"start\":57123},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":15213264},\"end\":58010,\"start\":57530},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":18340881},\"end\":58384,\"start\":58012},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":127986044},\"end\":58831,\"start\":58386},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":29164161},\"end\":59261,\"start\":58833},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":243865349},\"end\":59620,\"start\":59263},{\"attributes\":{\"id\":\"b7\"},\"end\":60183,\"start\":59622},{\"attributes\":{\"id\":\"b8\"},\"end\":60786,\"start\":60185},{\"attributes\":{\"id\":\"b9\"},\"end\":60977,\"start\":60788},{\"attributes\":{\"id\":\"b10\"},\"end\":61228,\"start\":60979},{\"attributes\":{\"id\":\"b11\"},\"end\":61689,\"start\":61230},{\"attributes\":{\"id\":\"b12\"},\"end\":62761,\"start\":61691},{\"attributes\":{\"id\":\"b13\"},\"end\":63494,\"start\":62763},{\"attributes\":{\"id\":\"b14\"},\"end\":63670,\"start\":63496},{\"attributes\":{\"id\":\"b15\"},\"end\":63937,\"start\":63672},{\"attributes\":{\"id\":\"b16\"},\"end\":64156,\"start\":63939},{\"attributes\":{\"id\":\"b17\"},\"end\":64293,\"start\":64158},{\"attributes\":{\"id\":\"b18\"},\"end\":64454,\"start\":64295},{\"attributes\":{\"id\":\"b19\"},\"end\":64631,\"start\":64456},{\"attributes\":{\"id\":\"b20\"},\"end\":64724,\"start\":64633},{\"attributes\":{\"id\":\"b21\"},\"end\":64842,\"start\":64726},{\"attributes\":{\"id\":\"b22\"},\"end\":65052,\"start\":64844},{\"attributes\":{\"id\":\"b23\"},\"end\":65269,\"start\":65054}]", "bib_title": "[{\"end\":56085,\"start\":56027},{\"end\":57188,\"start\":57123},{\"end\":57602,\"start\":57530},{\"end\":58078,\"start\":58012},{\"end\":58433,\"start\":58386},{\"end\":58915,\"start\":58833},{\"end\":59318,\"start\":59263}]", "bib_author": "[{\"end\":56100,\"start\":56087},{\"end\":56116,\"start\":56100},{\"end\":56129,\"start\":56116},{\"end\":56146,\"start\":56129},{\"end\":56164,\"start\":56146},{\"end\":56177,\"start\":56164},{\"end\":56193,\"start\":56177},{\"end\":56204,\"start\":56193},{\"end\":56215,\"start\":56204},{\"end\":56233,\"start\":56215},{\"end\":56246,\"start\":56233},{\"end\":56260,\"start\":56246},{\"end\":56286,\"start\":56260},{\"end\":56297,\"start\":56286},{\"end\":56313,\"start\":56297},{\"end\":56325,\"start\":56313},{\"end\":56338,\"start\":56325},{\"end\":56352,\"start\":56338},{\"end\":56368,\"start\":56352},{\"end\":56383,\"start\":56368},{\"end\":56401,\"start\":56383},{\"end\":56407,\"start\":56401},{\"end\":57201,\"start\":57190},{\"end\":57216,\"start\":57201},{\"end\":57234,\"start\":57216},{\"end\":57242,\"start\":57234},{\"end\":57246,\"start\":57242},{\"end\":57612,\"start\":57604},{\"end\":57634,\"start\":57612},{\"end\":57651,\"start\":57634},{\"end\":58094,\"start\":58080},{\"end\":58113,\"start\":58094},{\"end\":58128,\"start\":58113},{\"end\":58449,\"start\":58435},{\"end\":58465,\"start\":58449},{\"end\":58475,\"start\":58465},{\"end\":58496,\"start\":58475},{\"end\":58508,\"start\":58496},{\"end\":58930,\"start\":58917},{\"end\":58949,\"start\":58930},{\"end\":59334,\"start\":59320},{\"end\":59345,\"start\":59334}]", "bib_venue": "[{\"end\":56612,\"start\":56518},{\"end\":57794,\"start\":57731},{\"end\":58627,\"start\":58576},{\"end\":59064,\"start\":59015},{\"end\":59458,\"start\":59410},{\"end\":56516,\"start\":56407},{\"end\":57315,\"start\":57246},{\"end\":57729,\"start\":57651},{\"end\":58189,\"start\":58128},{\"end\":58574,\"start\":58508},{\"end\":59013,\"start\":58949},{\"end\":59408,\"start\":59345},{\"end\":59727,\"start\":59622},{\"end\":60379,\"start\":60185},{\"end\":60881,\"start\":60788},{\"end\":61102,\"start\":60979},{\"end\":61458,\"start\":61230},{\"end\":62186,\"start\":61691},{\"end\":63110,\"start\":62763},{\"end\":63575,\"start\":63496},{\"end\":63792,\"start\":63672},{\"end\":64046,\"start\":63939},{\"end\":64224,\"start\":64158},{\"end\":64373,\"start\":64295},{\"end\":64542,\"start\":64456},{\"end\":64677,\"start\":64633},{\"end\":64783,\"start\":64726},{\"end\":64946,\"start\":64844},{\"end\":65154,\"start\":65054}]"}}}, "year": 2023, "month": 12, "day": 17}