{"id": 250451229, "updated": "2023-10-05 12:53:22.694", "metadata": {"title": "DGPO: Discovering Multiple Strategies with Diversity-Guided Policy Optimization", "authors": "[{\"first\":\"Wenze\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Shiyu\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Yuan\",\"last\":\"Chiang\",\"middle\":[]},{\"first\":\"Tim\",\"last\":\"Pearce\",\"middle\":[]},{\"first\":\"Wei-Wei\",\"last\":\"Tu\",\"middle\":[]},{\"first\":\"Ting\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Jun\",\"last\":\"Zhu\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Most reinforcement learning algorithms seek a single optimal strategy that solves a given task. However, it can often be valuable to learn a diverse set of solutions, for instance, to make an agent's interaction with users more engaging, or improve the robustness of a policy to an unexpected perturbance. We propose Diversity-Guided Policy Optimization (DGPO), an on-policy algorithm that discovers multiple strategies for solving a given task. Unlike prior work, it achieves this with a shared policy network trained over a single run. Specifically, we design an intrinsic reward based on an information-theoretic diversity objective. Our final objective alternately constraints on the diversity of the strategies and on the extrinsic reward. We solve the constrained optimization problem by casting it as a probabilistic inference task and use policy iteration to maximize the derived lower bound. Experimental results show that our method efficiently discovers diverse strategies in a wide variety of reinforcement learning tasks. Compared to baseline methods, DGPO achieves comparable rewards, while discovering more diverse strategies, and often with better sample efficiency.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2207.05631", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/atal/ChenHCCZ23", "doi": "10.48550/arxiv.2207.05631"}}, "content": {"source": {"pdf_hash": "09093b425cd2315a874cfd57053897b1a1065a6b", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2207.05631v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "997c40fd0c6900754dbb4d6b063d34743322d107", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/09093b425cd2315a874cfd57053897b1a1065a6b.txt", "contents": "\nDGPO: Discovering Multiple Strategies with Diversity-Guided Policy Optimization\n\n\nWenze Chen \nTsinghua University\nBeijingChina\n\nShiyu Huang huangshiyu@4paradigm.com \n4Paradigm Inc\nBeijingChina\n\nYuan Chiang \nTsinghua University\nBeijingChina\n\nTim Pearce tim.pearce@microsoft.com \nMicrosoft Research London\nUnited Kingdom\n\nWei-Wei Tu tuweiwei@4paradigm.com \n4Paradigm Inc\nBeijingChina\n\nTing Chen tingchen@tsinghua.edu.cn \nTsinghua University\nBeijingChina\n\nJun Zhu \nTsinghua University\nBeijingChina\n\nDGPO: Discovering Multiple Strategies with Diversity-Guided Policy Optimization\n\nMost reinforcement learning algorithms seek a single optimal strategy that solves a given task. However, it can often be valuable to learn a diverse set of solutions, for instance, to make an agent's interaction with users more engaging, or improve the robustness of a policy to an unexpected perturbance. We propose Diversity-Guided Policy Optimization (DGPO), an on-policy algorithm that discovers multiple strategies for solving a given task. Unlike prior work, it achieves this with a shared policy network trained over a single run. Specifically, we design an intrinsic reward based on an information-theoretic diversity objective. Our final objective alternately constraints on the diversity of the strategies and on the extrinsic reward. We solve the constrained optimization problem by casting it as a probabilistic inference task and use policy iteration to maximize the derived lower bound. Experimental results show that our method efficiently discovers diverse strategies in a wide variety of reinforcement learning tasks. Compared to baseline methods, DGPO achieves comparable rewards, while discovering more diverse strategies, and often with better sample efficiency.Preprint. Under review.\n\nIntroduction\n\nReinforcement learning (RL) has achieved human-level performance in various domains, such as video games [37,2,14,13] and robotics [30,40]. However, RL algorithms suffer from several problems. One issue is that while there may exist a diverse set of possible solutions for a given task, RL algorithms seek to learn a single optimal one. This can be problematic in several circumstances. RL algorithms are notorious for \"overfitting\" the given task, i.e., while there is a diverse set of quality solutions for the given problem, RL algorithms can only obtain a single optimal one. Firstly, agents that specialize to a single pathway through the environment are brittle. If they instead can find a set of qualitatively different strategies, robustness to environment perturbations can be improved [17]. Secondly, in competitive games, an agent's weakness can more easily be identified if it follows a single strategy. Improving the diversity of agents' behavior can enhance their final performance [18].\n\nThirdly, in some practical applications, like dialogue generation, having agents include variety in their response can improve customers' experience [22,8,29,38,3]. In general, there are two main situations in which diverse strategies exist: (1) Some mistakes occur in a way that matters less for the success of the tasks. Thus, a qualified agent does not need to achieve optimal strategies. A set of sub-optimal strategies that adjust to the global optimal point is acceptable [41]. (2) Some tasks themselves have multiple solutions [27,42], e.g., there may be two optimal trajectories with equal length in a maze.\n\nDiscovering a set of high-reward diverse solutions in a sample-efficient way is non-trivial. Below, we list desiderata for such an algorithm. We also outline how the algorithm proposed in this paper, Diversity-Guided Policy Optimization (DGPO), fulfills these.\n\nStrategy Representation: An agent's design must allow it to represent different strategies efficiently. In DGPO, multiple strategies are represented by a shared deep neural network, by having it condition on a stochastic latent variable.\n\nDiversity Evaluation: Algorithms should be able to evaluate the diversity of a set of policies. DGPO utilizes a diversity metric based on mutual information between states and latent variable, implemented through a learned discriminator.\n\nDiversity Exploration: Algorithms should explore in a way that allows deviation from previously discovered solutions, while maintaining overall performance. DGPO introduces a constrained optimization approach to ensure a stable improvement in both performance and diversity.\n\nSample Efficiency: Algorithms should be sample efficient. Previous methods, such as RSPO [42] and RPG [36], used multiple training stages and multiple networks to obtain and represent the diverse strategies, resulting in poor sample efficiency. In DGPO, multiple strategies are learned simultaneously, in one training run, by a shared network, improving sample complexity.\n\nDGPO is an on-policy framework that discovers multiple strategies for a given task. It uses diversity objectives to guide a latent code conditioned policy to learn a set of diverse strategies in a single training procedure. We formalize DGPO as the combination of a diversity-constrained optimization problem and an extrinsic-reward-constrained optimization problem. We frame this as a probabilistic inference task and use policy iteration to maximize a derived lower bound.\n\nIn summary, our contributions come from the following points: (1) We formalize the process of finding high-reward diverse policies into two constrained optimization problems and develop corresponding diversity rewards to guide policy learning. (2) We propose a novel on-policy algorithm, denoted as DGPO, that can find a diverse set of quality strategies simultaneously. (3) We empirically show that our method achieves competitive performance and has better diversity or sample complexity than other baselines on various benchmarks.\n\n\nRelated Works\n\nIn this section, we summarize prior work that frames RL as a probabilistic graphical model (PGM), and also that explicitly connecting diversity learning with RL.\n\n\nReinforcement Learning as Probabilistic Graphical Model\n\nPGM's have proven to be a useful way of framing the RL problem [43,7,21]. The soft actor-critic (SAC) algorithm [10] formalizes RL as probabilistic inference and maximizes an evidence lower bound by adding an entropy term to the training objective, encouraging exploration. PGM's also serve as useful tools for studying partially observable Markov decision processes (POMDP's) [16,15,19]. Haarnoja et al. [9] use PGM's to construct a hierarchical reinforcement learning algorithm. Hausman et al. [11] optimize a multi-task policy through a variational bound, allowing for the discovery of multiple solutions with a minimum number of distinct skills. In this work, we modify the PGM of a Markov decision process (MDP) by introducing a latent variable to induce diversity into the MDP. We then derive the evidence lower bound of the new PGM, allowing us to construct a novel RL algorithm.\n\n\nDiversity in Reinforcement Learning\n\nAchieving diversity has been studied in various contexts in RL [26,24,5,27,4]. Eysenbach et al. [5] proposed DIAYN to maximize the mutual information between states and skills, which results in a maximum entropy policy. Osa et al. [27] proposed a method that can learn infinitely many solutions by training a policy conditioned on a continuous or discrete low-dimensional latent variable. Their method can learn diverse solutions in continuous control tasks via variational information maximization. There is also a growing corpus of work on diversity in multi-agent reinforcement learning [23,20,12]. Mahajan et al. [23] proposed MAVEN, a method that overcomes the detrimental effects of QMIX's [31] monotonicity constraint on exploration by maximizing the mutual information between latent variables and trajectories. However, their method can not find multiple diverse strategies for a specified task. He et al. [12] investigated multi-agent algorithms for learning diverse skills using information bottlenecks with unsupervised rewards. However, their method operates in an unsupervised manner, without external rewards. More recently, RSPO [42] was proposed to derive diverse strategies. However, it requires multiple training stages, which results in poor sample efficiency -our method trains diverse strategies simultaneously which reduces sample complexity. As introduced in [21], O t is a binary random variable, where O t = 1 denotes that the action is optimal at time t, and O t = 0 denotes that the action is not optimal. RL can be formalized as an MDP. An MDP is a tuple (S, A, P, r, \u03b3), where S and A represent state and action space respectively, P (s, a) : S \u00d7 A \u2192 S is the transition probability density, r(s, a) : S \u00d7 A \u2192 R is a reward function, and \u03b3 \u2208 [0, 1] is the discount factor. Latent conditioned policy: We consider a policy \u03c0 \u03b8 that is conditioned on latent variable z to model diverse strategies, where \u03b8 represents policy parameters. For compactness, we will omit \u03b8 in our notation. We denote the latent-conditioned policy as \u03c0(a|s, z) and a latent conditioned critic network as V \u03c0 (s, z). For each episode, a single latent variable is sampled, z \u223c p(z) from a categorical distribution with n z categories. In our work, we choose p(z) to be a uniform distribution. The agent then conditions on this latent code z, to produce a trajectory \u03c4 z .\n\n\nPreliminaries\n\nDiscounted state occupancy: The discounted state occupancy measure for policy \u03c0 is defined as\n\u03c1 \u03c0 (s) = (1 \u2212 \u03b3) \u221e t=0 \u03b3 t P \u03c0 t (s),\nwhere P \u03c0 t (s) is the probability that policy \u03c0 visit state s at time t. The goal of the RL agent is to train a policy \u03c0 to maximize the discounted accumulated reward\nJ(\u03b8) = E z\u223cp(z),s\u223c\u03c1 \u03c0 (s),a\u223c\u03c0(\u00b7|s,z) [ t \u03b3 t r(s t , a t )].\nRL as probabilistic graphical model: An MDP can be framed as a probabilistic graphical model as shown in Fig. 1(a) and the optimal control problem can be solved as a probabilistic inference task [21]. In this paper, we propose a new probabilistic graphical model, denoted as the diverse MDP, as shown in Fig. 1(b). We introduce a binary random variable O t and an integer random variable z into the model. O t = 1 denotes that action a t is optimal at time step t and O t = 0 denotes it is not. Previous work [21] has defined, p(O t = 1|s t , a t , z) \u221d exp(r(s t , a t )). The evidence lower bound (ELBO) is given by:\nlog p(O 1:T ) \u2265 E \u03c4 \u223cD\u03c0 [log p(O 1:T , a 1:T , s 1:T , z) \u2212 log \u03c0(a 1:T , s 1:T , z)] = E \u03c4 \u223cD\u03c0 [log p(O t |s t , a t , z) + log p(z|s t , a t ) \u2212 log \u03c0(a t |s t , z)],(1)\nwhere the trajectory \u03c4 = {a 1:T , s 1:T , z} is sampled from a trajectory dataset D \u03c0 . The proof of Eq. 1 can be found in Appendix C. Note that the introduction of z gives rise to the term p(z|s t , a t ), which represents how identifiable the latent code is from the current policy behavior. This will be a crucial ingredient of DGPO, guiding the policy to explore and discover a set of diverse strategies.\n\n\nMethodology\n\nIn this section, we will introduce our algorithm in detail. Our algorithm can be divided into two stages. In the first stage, we will focus on improving the agent's performance while maintaining its diversity. In the second stage, we will focus more on enhancing diverse strategies. Finally, we will introduce our final algorithm, denoted as Diversity-Guided Policy Optimization (DGPO), that unifies the two-stage training processes and also the implementation details.\n\n\nDiversity Measurement\n\nIn this section, we present a diversity score capable of evaluating the diversity of a given set of policies. We then derive a diversity objective from this score to facilitate exploration. Eysenbach et al. [5] proposed a diversity score based on mutual information between states and latent codes z,\nI(s; z) = E z\u223cp(z),s\u223c\u03c1 \u03c0 (s) [log p(z|s) \u2212 log p(z)],(2)\nwhere I(\u00b7, \u00b7) stands for mutual information. As we can not directly calculate p(z|s), we approximate it with a learned discriminator q \u03c6 (z|s) and derive the ELBO as\nE z\u223cp(z),s\u223c\u03c1 \u03c0 (s) [log q \u03c6 (z|s) \u2212 log p(z)],\nwhere \u03c6 are the parameters of the discriminator network.\n\nMutual information is equal to the KL distance between the state marginal distribution of one policy and the average state marginal distribution, i.e., I(s; z) = E z\u223cp(z) [D KL (\u03c1 \u03c0 (s|z)||\u03c1 \u03c0 (s))] [6]. This means that I(s; z) captures the diversity averaged over the whole set of policies. In DGPO, we wish to ensure that any given pair of strategies is different, rather than on average. As such, we define a novel, stricter diversity score,\nDIV(\u03c0 \u03b8 ) = E z\u223cp(z) [min z =z D KL (\u03c1 \u03c0 \u03b8 (s|z)||\u03c1 \u03c0 \u03b8 (s|z ))].(3)\nInstead of comparing policy with the average state marginal distribution, we compare it with the nearest policy in \u03c1 \u03c0 (s) space. In this way, setting DIV(\u03c0 \u03b8 ) \u2265 \u03b4 means that each pair of policies have at least \u03b4 distance in terms of expectation. In order to optimize Eq. 3, we first derive a lower bound,\nDIV(\u03c0 \u03b8 ) \u2265 E z\u223cp(z),s\u223c\u03c1 \u03c0 (s) min z =z log p(z|s) p(z|s) + p(z |s)\n.\n\nThe proof is given in Appendix D. To maximize this lower bound, we first assume we can learn a latent code discriminator, q \u03c6 (z|s), to approximate p(z|s). We then define an intrinsic reward,\nr in t = min z =z log q \u03c6 (z|s t+1 ) q \u03c6 (z|s t+1 ) + q \u03c6 (z |s t+1 ) .(5)\nThis allows us to define our final diversity objective,\nJ Div (\u03b8) = E z\u223cp(z),s\u223c\u03c1 \u03c0 (s),a\u223c\u03c0(\u00b7|s,z) [ t \u03b3 t r in t ].(6)\nA straightforward way to incorporate the diversity metric into a PGM is defining elements in Eq. 1 as,\np(O t = 1|s t , a t , z) = exp (r(s t , a t )) , p(z|s t , a t ) = exp r in t .(7)\nHowever, the simple combination of extrinsic and intrinsic rewards may lead to poor performance. In the following paragraph, we formulate the algorithm as constrained optimization problems and mask elements in Eq. 7 based on constraints to guide the policy to explore.\n\n\nStage 1: Diversity-Constrained Optimization\n\nStrategies that solve a given RL task may be very distinct. One can think of these as a set of discrete points in \u03c1 \u03c0 (s) space -if the distance between the points is large, perturbing around one single solution may not allow the discovery of all optimal strategies. Thus, we formulate the policy optimization process as a diversity-constrained optimization problem,\nmax \u03c0 \u03b8 J(\u03b8), s.t. J Div (\u03b8) \u2265 \u03b4.(8)\nUnder this objective, individual policies are constrained to keep a certain distance \u03b4 apart from each other, and systematically explore their own regions. We can introduce a Lagrange multiplier \u03bb to tackle this constrained optimization problem,\nmax \u03c0 \u03b8 min \u03bb\u22650 J(\u03b8) + \u03bb(J Div (\u03b8) \u2212 \u03b4) \u2265 max \u03c0 \u03b8 E z\u223cp(z),s\u223c\u03c1 \u03c0 (s) min \u03bb\u22650 E a\u223c\u03c0(\u00b7|s,z) [ t \u03b3 t r(s t , a t )] + \u03bb( t \u03b3 t r in t \u2212 \u03b4) .(9)\nThe proof can be found in Appendix E. Eq. 9 provides a lower bound on the Lagrange multiplier objective, which can be optimized more easily than the original problem. Eq. 9 can be interpreted as optimizing the extrinsic-rewards, J(\u03b8), when diversity is greater than some threshold. Otherwise, the intrinsic rewards objective J Div (\u03b8) are optimized. From another perspective, one can think of masking out terms in elements in Eq. 1 based on a diversity metric. The updated objective can be written,\np(O t = 1|s t , a t , z) = exp (I[J Div (\u03b8) \u2265 \u03b4]r(s t , a t )) , p(z|s t , a t ) = exp (1 \u2212 I[J Div (\u03b8) \u2265 \u03b4])r in t ,(10)\nwhere I[\u00b7] is the indicator function.\n\n\nStage 2: Extrinsic-Reward-Constrained Optimization\n\nThe objective developed in the previous section can return a set of discrete optimal points. However, sometimes two strategies may still converge to the same sub-optimal point. This can destabilize the training process since both strategies are attracted to the same optimal point but simultaneously repelled by each other. To stabilize the training process and improve diversity, we relax the definition of \"optimal\" and assume that a policy with accumulated extrinsic rewards greater than some target value R target is an optimal policy. Thus, the policy optimization process can be formulated as an extrinsic-reward-constrained optimization problem,\nmax \u03c0 \u03b8 J Div (\u03b8), s.t. J(\u03b8) \u2265 R target .(11)\nThis means that if two strategies try to converge to the same sub-optimal point. They are allowed to find their destiny in the neighborhood of the optimal point to further maximize the level of diversity.\n\nOn the other hand, for those policies that are already sufficiently distinct, the diversity objective serves as an intrinsic reward to encourage exploration. Similar to how we deal with diversity-constrained optimization. We implement Eq. 11 by injecting an extrinsic-rewards-constraint into the framework. The updated elements in Eq. 1 can be defined as:\np(O t = 1|s t , a t , z) = exp(r(s t , a t )), p(z|s t , a t ) = exp(I[J(\u03b8) \u2265 R target ]r in t ).(12)\n\nDiversity-Guided Policy Optimization\n\nIn this section, we will introduce our final algorithm which unifies the two-stage training processes into one unified algorithm and also introduce how to implement it with the on-policy RL algorithm. We develop a new variation of PPO [34] by considering policy network and critic network that are conditioned on latent variable z, i.e., \u03c0(a t |s t , z). The critic network is divided into two parts, i.e., V \u03c0 \u03c8ex (o 1:t , z) and V \u03c0 \u03c8in (o 1:t , z), where \u03c8 ex and \u03c8 in are their parameters. The parameters of critic networks can be trained by a temporal difference (TD) loss [35]:\nL(\u03c8 ex ) = M SE V \u03c0 \u03c8ex (o 1:t , z), \u221e t =t \u03b3 t \u2212t I[J Div (\u03b8) \u2265 \u03b4]r(s t , a t ) , L(\u03c8 in ) = M SE V \u03c0 \u03c8in (o 1:t , z), \u221e t =t \u03b3 t \u2212t [(1 \u2212 I[J Div (\u03b8) \u2265 \u03b4]) + I [J(\u03b8) \u2265 R target ]] r in t ,(13)\nwhere L(\u00b7) stands for loss function and M SE(\u00b7) stands for mean square error. We maintain a running average of V \u03c0 \u03c8ex (s t , z) to approximate E s\u223c\u03c1 \u03c0 (s),a\u223c\u03c0(\u00b7|s) [ t \u03b3 t r(s t , a t )]. We also construct a discriminator q \u03c6 (z|s t ) that takes the state as input and predict the probability of latent variable z, where \u03c6 is the parameter of the discriminator network. And the discriminator can be trained in a supervised manner:\nL(\u03c6) = E (st,at,z)\u223cD\u03c0 [CE(q \u03c6 (s t ), z)],(14)\nwhere CE(\u00b7, \u00b7) stands for cross entropy loss. We implement DGPO by incorporating diversityconstrained optimization and extrinsic-reward-constrained optimization into the same framework, i.e., the total value of the state and the total reward can be defined as below: V \u03c0 total (o 1:t , z) = V \u03c0 \u03c8in (o 1:t , z) + V \u03c0 \u03c8ex (o 1:t , z),\nr total t = I[J Div (\u03b8) \u2265 \u03b4]r(s t , a t ) + [(1 \u2212 I[J Div (\u03b8) \u2265 \u03b4]) + I [J(\u03b8) \u2265 R target ]] r in t .(15)\nTheoretically, we can not conduct diversity-constrained optimization and extrinsic-reward-constrained optimization simultaneously. Thus, the implementation above is an approximation to the original objective. We assume that the approximation is reasonable since the empirical results show that the two training stages do not overlap. Fig. 2 shows the overall framework of the DGPO algorithm. The detailed training process of DGPO can be found in the Appendix G.\n\n\nExperiments\n\nIn this section, we evaluate our algorithm on several RL benchmarks -Multi-agent Particle Environment (MPE) [25], StarCraft Multi-Agent Challenge (SMAC) [32], and Atari [1]. We compare our algorithm to four baseline algorithms: MAPPO [39]: MAPPO adapts the single-agent PPO [34] algorithm to the multi-agent setting by using a centralized value function with shared team-based rewards. DIAYN [5]: DIAYN trains agents with a mutual-information based intrinsic reward to discover a diverse set of skills. In our setup, these intrinsic rewards are combined with extrinsic rewards. SMERL [17]: SMERL maximizes a weighted combination of intrinsic rewards and extrinsic rewards when the return of extrinsic reward is greater than some given threshold. RSPO [42]: RSPO is an iterative algorithm for discovering a diverse set of quality strategies. It toggles between extrinsic and intrinsic rewards based on a trajectory-based novelty measurement.  and Spread (hard). In both scenarios, Agents (orange dots) aim to reach one of the destinies (blue dots). We highlight the optimal solutions with arrows of different colors.\n\n\nMulti-Agent Particle Environment\n\nWe evaluate on two scenarios shown per Fig. 3, Spread (easy) and Spread (hard). In Spread (easy), there are four landmarks and one agent. The agent starts from the center and aims to reach one of the landmarks, giving four optimal solutions. In Spread (hard), there are three agents and three landmarks. Agents cooperate to cover all the landmarks and avoid colliding with others, giving two optimal solutions. Model weights are shared across agents.\n\nSimilar to [28], we introduce a metric to quantitatively evaluate the diversity score of the given set of policies, \u03a0:\nM Div (\u03a0) = 1 nz nz i=1 nz j=i+1 ln( \u03a6(\u03c0 i ) \u2212 \u03a6(\u03c0 j ) 2 ), where \u03a6(\u03c0)\nis the behavior embedding of the policy \u03c0. For MPE, \u03a6(\u03c0) is represented by the concatenated agents' positions over an episode.\n\nWe set n z = 4 in Spread (easy) and n z = 2 in Spread (hard) to test whether an algorithm can discover all optimal solutions. The performance, diversity score, and steps required for convergence for all algorithms are given in Fig. 4. Results are averaged over five seeds.\n\nDGPO provides a favorable combination of high diversity, and high reward. It also exhibits rapid convergence. MAPPO only recovers a single solution in each setting. DIAYN only finds 2/4 optimal solutions in Spread (easy) and often only 1/2 in Spread (hard). This supports our earlier claim that a naive combination of extrinsic and intrinsic rewards is insufficient. SMERL is only able to discover new strategies by perturbing around a discovered global optimal. Thus, it is unable to find more than one optimal strategy. RSPO is the only other method to also recover all optimal strategies. However, relative to DGPO, RSPO achieves lower overall reward and slower convergence.\n\nInspired by [33], we employ an F-measurement metric to demonstrate the trade-off between diversity scores and rewards. Our approach uses the formula F score = 2\u00b7Rew\u00b7Div Rew+Div , with Rew and Div representing normalized rewards and diversity scores. For detailed information, refer to Appendix F. Our methods consistently outperformed all baseline algorithms across all scenarios, as shown in Table 1.\n\n\nAtari\n\nWe evaluate DGPO on the Atari games Pong and Boxing, to test the performance for tasks with image observations. DGPO's diversity metric is defined similarly to the one used in MPE (full details in  Appendix F). In each environment, we set n z = 2. Results are summarized in Fig. 6(a) and Table 1, averaged over five seeds. DGPO again delivers a favorable trade-off between external reward and strategy diversity. We conduct experiments on two Star-Craft II maps (2s_vs._1sc & 3m) from SMAC. We set n z = 3 and measure the mean win rates over five seeds. Fig. 6(b) and Table 1 shows that, relative to other algorithms, DGPO discovers sets of strategies that are both diverse and achieve good win rates. Fig. 7 visualizes three strategies obtained by DGPO on 3m map. In this map, we control the three green agents to combat the red built-in agents. We visualize the trajectories of our agents with green arrows. When z = 0, the policy produces an aggressive strategy, with agents moving directly forward to attack the enemies. When z = 1, the agents display a kiting strategy, alternating between attacking and moving. This allows them to attack enemies while limiting the taken damage. When z = 2, the policy produces another kiting strategy but now with a downwards, rather than upwards drift. \n\n\nAblation Study\n\nWe performed ablation studies on MPE Spread (hard) tasks, systematically removing each element of our algorithm to assess its impact on diversity. The empirical result is shown in Fig. 8(a). In this section, we set n z = 3 and the result is averaged over 5 seeds. Change-Diversity-Measurement uses mutual information as shown in Eq. 2 as diversity metric. Experimental results indicate that in the Spread (hard) scenario, the diversity score of Change-Diversity-Measurement is lower than that of DGPO. While optimizing three policies simultaneously, the limited number of optimal solutions (only two) leads to one policy behaving differently while the other two exhibit similar behavior. Consequently, the diversity score based on mutual information becomes artificially high (as it reflects the overall diversity level of the policy set), causing the policy to stop optimizing diversity, even though two policies continue to behave similarly. No-Diversity-Constrained-Optimization excludes diversity-constrained optimization. Empirical results show that it only identifies one optimal solution. This suggests that utilizing the diversity metric as a constraint, rather than blending it with extrinsic rewards during the initial stages of training, significantly enhances the algorithm's effectiveness. No-Extrinsic-Reward-Constrained-Optimization omits extrinsic-reward-constrained optimization. While DGPO continues to enhance the diversity score as the expected return surpasses R target , No-Extrinsic-Reward-Constrained-Optimization fails to do so. Consequently, it ultimately attains a lower diversity score compared to DGPO.\n\nOur algorithm can be divided into two distinct stages, as depicted in Fig. 8(b). In the initial stage, we focus on diversity-constrained optimization until policies exhibit sufficient behavioral differences. Once the expected return reaches R target , we transition to the extrinsic-reward-constrained optimization stage. Here, we maintain a fixed performance level at R target while maximizing the diversity score. These stages are clearly separated and non-overlapping.\n\n\nConclusions\n\nThis paper proposed Diversity-Guided Policy Optimization (DGPO), an on-policy algorithm that can efficiently discover sets of strategies that are diverse and achieve high reward. DGPO formulates the training process as two constrained optimization problems and solves them as a probabilistic inference task. Experiments showed that DGPO is competitive with state-of-the-art on-policy RL algorithms, achieving a favorable balance of diversity scores and reward, and with improved sample efficiency. Currently, DGPO has been tested only with discrete latent variables, and in future work, we hope to extend this to continuous latent variables. We also plan to test DGPO in increasingly complex environments, studying how it can help with the challenges of exploration, self-play, and robustness.\n\nBroader impact. Our work has focused on improving the capabilities of core RL algorithms. As such, we do not foresee any obvious negative societal impacts, though we remain cognisant of this important topic.\n\nFigure 1 :\n1(a) The graphical model of MDPs. (b) The graphical model of diverse MDPs. Grey nodes are observed, and white nodes are hidden.\n\nFigure 2 :\n2The overall framework of the DGPO algorithm. Top illustrates the way of calculating r total t , where mask r = I[J(\u03b8) \u2265 R target ] and mask d = I[J Div (\u03b8) \u2265 \u03b4]. Center shows the network structure and the data flow of the DGPO algorithm. Bottom shows the latent variable sampling process.\n\nFigure 4 :\n4As far as possible, all methods use the same hyper-parameters. However, there is some difference for RSPO, for which we use the open-source implementation. (Full hyperparameters are listed in the Experimental results in two MPE scenarios -Spread (easy) and Spread (hard) -each with multiple optimal solutions. (a) Plot showing extrinsic reward performance vs. how diverse the set of discovered strategies are. Positions in the upper-right corner are preferred -DGPO is located here. (b) Plot showing at which point in training each optimal strategy is discovered. Results show that only DGPO and RSPO can find all the solutions. But DGPO achieved over 1.7\u00d7 and 15\u00d7 speedup in convergence speed compared to RSPO in the Spread (easy) and Spread (hard) scenarios, respectively. Appendix B.) All experiments were performed on a machine with 128 GB RAM, one 32-core CPU, and one GeForce RTX 3090 GPU.\n\nFigure 3 :\n3The initial state of Spread (easy)\n\nFigure 5 :\n5Screen shots and heat maps of agents' trajectories on Pong and Boxing. (a) In the Pong game, our agent controls the paddle (green block) to hit the ball (white dot). (b) In the Boxing game, our agent (in white) has a boxing match with the opponent (in black).\n\nFig. 5 (Figure 7 :\n57a) visualizes the results of the two strategies obtained by DGPO on Pong. In this game, the agent controls the green paddle. When z = 0 the agent holds the paddle at the bottom of the screen until the ball is near, while when z = 1, the default position of the paddle is at the top of the screen.Fig. 5(b) similarly shows the different strategies obtained by DGPO on Boxing, where the agent controls the white character, and is rewarded for punching the black opponent. The heatmap shows DGPO learns to attack from different sides. Other baseline algorithms tend to remain in a single corner. Visualization results of the three strategies obtained by DGPO on 3m map. The green arrows show the trajectories of our agents (in green).\n\nFigure 6 :Figure 8 :\n68Plots showing extrinsic reward performance vs. the diversity of the set of discovered strategies. (a) In two Atari games. (b) In two SMAC scenarios. data that satisfy JDiv( ) ratio of data that satisfy J( ) Rtarget (b) (a) The impact of each component of our algorithm on diversity scores in the MPE Spread (hard) scenario. (b) DGPO can be distinctly divided into two stages: diversity-constrained optimization and extrinsic-reward-constrained optimization.\n\nTable 1 :\n1Experimental results of the F-measurement metric[33] of different algorithms.Spread(easy) Spread(hard) Pong Boxing 2s vs 1sc 3m \n\nDIAYN \n0.79 \n0.94 \n0.79 \n0.90 \n0.56 \n0.40 \nSMERL \n0.43 \n0.61 \n0.90 \n0.29 \n0.45 \n0.02 \nRSPO \n0.92 \n0.94 \n-\n-\n-\n-\nDGPO \n0.99 \n0.99 \n0.99 \n0.98 \n0.77 \n0.82 \n\n= 0 \n= 1 \n\n(a) Pong \n\n\n\nThe arcade learning environment: An evaluation platform for general agents. Yavar Marc G Bellemare, Joel Naddaf, Michael Veness, Bowling, Journal of Artificial Intelligence Research. 47Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, 47:253-279, 2013.\n\nDota 2 with large scale deep reinforcement learning. Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, arXiv:1912.06680arXiv preprintChristopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019.\n\nA mixture-of-expert approach to rl-based dialogue management. Yinlam Chow, Aza Tulepbergenov, Ofir Nachum, Moonkyung Ryu, Mohammad Ghavamzadeh, Craig Boutilier, arXiv:2206.00059arXiv preprintYinlam Chow, Aza Tulepbergenov, Ofir Nachum, MoonKyung Ryu, Mohammad Ghavamzadeh, and Craig Boutilier. A mixture-of-expert approach to rl-based dialogue management. arXiv preprint arXiv:2206.00059, 2022.\n\nAdaptable agent populations via a generative model of policies. Kenneth Derek, Phillip Isola, Advances in Neural Information Processing Systems. 34Kenneth Derek and Phillip Isola. Adaptable agent populations via a generative model of policies. Advances in Neural Information Processing Systems, 34:3902-3913, 2021.\n\nBenjamin Eysenbach, Abhishek Gupta, Julian Ibarz, Sergey Levine, arXiv:1802.06070Diversity is all you need: Learning skills without a reward function. arXiv preprintBenjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you need: Learning skills without a reward function. arXiv preprint arXiv:1802.06070, 2018.\n\nThe information geometry of unsupervised reinforcement learning. Benjamin Eysenbach, Ruslan Salakhutdinov, Sergey Levine, arXiv:2110.02719arXiv preprintBenjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine. The information geometry of unsupervised reinforcement learning. arXiv preprint arXiv:2110.02719, 2021.\n\nVariational methods for reinforcement learning. Thomas Furmston, David Barber, Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. the Thirteenth International Conference on Artificial Intelligence and StatisticsJMLR Workshop and Conference ProceedingsThomas Furmston and David Barber. Variational methods for reinforcement learning. In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pages 241-248. JMLR Workshop and Conference Proceedings, 2010.\n\nGenerating multiple diverse responses for short-text conversation. Jun Gao, Wei Bi, Xiaojiang Liu, Junhui Li, Shuming Shi, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Jun Gao, Wei Bi, Xiaojiang Liu, Junhui Li, and Shuming Shi. Generating multiple diverse responses for short-text conversation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 6383-6390, 2019.\n\nLatent space policies for hierarchical reinforcement learning. Tuomas Haarnoja, Kristian Hartikainen, Pieter Abbeel, Sergey Levine, International Conference on Machine Learning. PMLRTuomas Haarnoja, Kristian Hartikainen, Pieter Abbeel, and Sergey Levine. Latent space policies for hierarchical reinforcement learning. In International Conference on Machine Learning, pages 1851-1860. PMLR, 2018.\n\nTuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, arXiv:1812.05905Pieter Abbeel, et al. Soft actor-critic algorithms and applications. arXiv preprintTuomas Haarnoja, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, et al. Soft actor-critic algorithms and applications. arXiv preprint arXiv:1812.05905, 2018.\n\nLearning an embedding space for transferable robot skills. Karol Hausman, Jost Tobias Springenberg, Ziyu Wang, Nicolas Heess, Martin Riedmiller, International Conference on Learning Representations. Karol Hausman, Jost Tobias Springenberg, Ziyu Wang, Nicolas Heess, and Martin Riedmiller. Learning an embedding space for transferable robot skills. In International Conference on Learning Representations, 2018.\n\nSkill discovery of coordination in multi-agent reinforcement learning. Shuncheng He, Jianzhun Shao, Xiangyang Ji, arXiv:2006.04021arXiv preprintShuncheng He, Jianzhun Shao, and Xiangyang Ji. Skill discovery of coordination in multi-agent reinforcement learning. arXiv preprint arXiv:2006.04021, 2020.\n\nTikick: Towards playing multi-agent football full games from single-agent demonstrations. Shiyu Huang, Wenze Chen, Longfei Zhang, Ziyang Li, Fengming Zhu, Deheng Ye, Ting Chen, Jun Zhu, arXiv:2110.04507arXiv preprintShiyu Huang, Wenze Chen, Longfei Zhang, Ziyang Li, Fengming Zhu, Deheng Ye, Ting Chen, and Jun Zhu. Tikick: Towards playing multi-agent football full games from single-agent demonstrations. arXiv preprint arXiv:2110.04507, 2021.\n\nCombo-action: Training agent for fps game with auxiliary tasks. Shiyu Huang, Hang Su, Jun Zhu, Ting Chen, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence33Shiyu Huang, Hang Su, Jun Zhu, and Ting Chen. Combo-action: Training agent for fps game with auxiliary tasks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 954-961, 2019.\n\nSvqn: Sequential variational soft q-learning networks. Shiyu Huang, Hang Su, Jun Zhu, Ting Chen, International Conference on Learning Representations. Shiyu Huang, Hang Su, Jun Zhu, and Ting Chen. Svqn: Sequential variational soft q-learning networks. In International Conference on Learning Representations, 2019.\n\nDeep variational reinforcement learning for pomdps. Maximilian Igl, Luisa Zintgraf, Tuan Anh Le, Frank Wood, Shimon Whiteson, International Conference on Machine Learning. PMLRMaximilian Igl, Luisa Zintgraf, Tuan Anh Le, Frank Wood, and Shimon Whiteson. Deep varia- tional reinforcement learning for pomdps. In International Conference on Machine Learning, pages 2117-2126. PMLR, 2018.\n\nOne solution is not all you need: Few-shot extrapolation via structured maxent rl. Saurabh Kumar, Aviral Kumar, Sergey Levine, Chelsea Finn, Advances in Neural Information Processing Systems. 33Saurabh Kumar, Aviral Kumar, Sergey Levine, and Chelsea Finn. One solution is not all you need: Few-shot extrapolation via structured maxent rl. Advances in Neural Information Processing Systems, 33:8198-8210, 2020.\n\nA unified game-theoretic approach to multiagent reinforcement learning. Advances in neural information processing systems. Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien P\u00e9rolat, David Silver, Thore Graepel, 30Marc Lanctot, Vinicius Zambaldi, Audrunas Gruslys, Angeliki Lazaridou, Karl Tuyls, Julien P\u00e9rolat, David Silver, and Thore Graepel. A unified game-theoretic approach to multiagent reinforcement learning. Advances in neural information processing systems, 30, 2017.\n\nStochastic latent actor-critic: Deep reinforcement learning with a latent variable model. Alex X Lee, Anusha Nagabandi, Pieter Abbeel, Sergey Levine, arXiv:1907.00953arXiv preprintAlex X Lee, Anusha Nagabandi, Pieter Abbeel, and Sergey Levine. Stochastic latent actor-critic: Deep reinforcement learning with a latent variable model. arXiv preprint arXiv:1907.00953, 2019.\n\nLearning to coordinate manipulation skills via skill behavior diversification. Youngwoon Lee, Jingyun Yang, Joseph J Lim, International Conference on Learning Representations. Youngwoon Lee, Jingyun Yang, and Joseph J Lim. Learning to coordinate manipulation skills via skill behavior diversification. In International Conference on Learning Representations, 2019.\n\nSergey Levine, arXiv:1805.00909Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv preprintSergey Levine. Reinforcement learning and control as probabilistic inference: Tutorial and review. arXiv preprint arXiv:1805.00909, 2018.\n\nDeep reinforcement learning for dialogue generation. Jiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, Dan Jurafsky, arXiv:1606.01541arXiv preprintJiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jianfeng Gao, and Dan Jurafsky. Deep reinforcement learning for dialogue generation. arXiv preprint arXiv:1606.01541, 2016.\n\nAnuj Mahajan, Tabish Rashid, Mikayel Samvelyan, Shimon Whiteson Maven, arXiv:1910.07483Multi-agent variational exploration. arXiv preprintAnuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. Maven: Multi-agent variational exploration. arXiv preprint arXiv:1910.07483, 2019.\n\nVariational information maximisation for intrinsically motivated reinforcement learning. Shakir Mohamed, Danilo Jimenez Rezende, arXiv:1509.08731arXiv preprintShakir Mohamed and Danilo Jimenez Rezende. Variational information maximisation for intrinsically motivated reinforcement learning. arXiv preprint arXiv:1509.08731, 2015.\n\nEmergence of grounded compositional language in multiagent populations. Igor Mordatch, Pieter Abbeel, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence32Igor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in multi- agent populations. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.\n\nOvercoming the bootstrap problem in evolutionary robotics using behavioral diversity. Jean-Baptiste Mouret, St\u00e9phane Doncieux, IEEE Congress on Evolutionary Computation. IEEEJean-Baptiste Mouret and St\u00e9phane Doncieux. Overcoming the bootstrap problem in evolution- ary robotics using behavioral diversity. In 2009 IEEE Congress on Evolutionary Computation, pages 1161-1168. IEEE, 2009.\n\nDiscovering diverse solutions in deep reinforcement learning. Takayuki Osa, Voot Tangkaratt, Masashi Sugiyama, arXiv:2103.07084arXiv preprintTakayuki Osa, Voot Tangkaratt, and Masashi Sugiyama. Discovering diverse solutions in deep reinforcement learning. arXiv preprint arXiv:2103.07084, 2021.\n\nEffective diversity in population based reinforcement learning. Jack Parker-Holder, Aldo Pacchiano, M Krzysztof, Stephen J Choromanski, Roberts, Advances in Neural Information Processing Systems. 33Jack Parker-Holder, Aldo Pacchiano, Krzysztof M Choromanski, and Stephen J Roberts. Ef- fective diversity in population based reinforcement learning. Advances in Neural Information Processing Systems, 33:18050-18062, 2020.\n\nIncreasing diversity with deep reinforcement learning for chatbots. Cristian Pavel, Stefania Budulan, Traian Rebedea, RoCHI. Cristian Pavel, Stefania Budulan, and Traian Rebedea. Increasing diversity with deep reinforce- ment learning for chatbots. In RoCHI, pages 123-128, 2020.\n\nS-rl toolbox: Environments, datasets and evaluation metrics for state representation learning. Antonin Raffin, Ashley Hill, Ren\u00e9 Traor\u00e9, Timoth\u00e9e Lesort, Natalia D\u00edaz-Rodr\u00edguez, David Filliat, arXiv:1809.09369arXiv preprintAntonin Raffin, Ashley Hill, Ren\u00e9 Traor\u00e9, Timoth\u00e9e Lesort, Natalia D\u00edaz-Rodr\u00edguez, and David Filliat. S-rl toolbox: Environments, datasets and evaluation metrics for state representation learning. arXiv preprint arXiv:1809.09369, 2018.\n\nQmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. Tabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, Shimon Whiteson, International Conference on Machine Learning. PMLRTabish Rashid, Mikayel Samvelyan, Christian Schroeder, Gregory Farquhar, Jakob Foerster, and Shimon Whiteson. Qmix: Monotonic value function factorisation for deep multi-agent reinforcement learning. In International Conference on Machine Learning, pages 4295-4304. PMLR, 2018.\n\nMikayel Samvelyan, Tabish Rashid, Christian Schroeder De, Gregory Witt, Nantas Farquhar, Nardelli, G J Tim, Chia-Man Rudner, Hung, H S Philip, Jakob Torr, Shimon Foerster, Whiteson, arXiv:1902.04043The starcraft multi-agent challenge. arXiv preprintMikayel Samvelyan, Tabish Rashid, Christian Schroeder De Witt, Gregory Farquhar, Nan- tas Nardelli, Tim GJ Rudner, Chia-Man Hung, Philip HS Torr, Jakob Foerster, and Shimon Whiteson. The starcraft multi-agent challenge. arXiv preprint arXiv:1902.04043, 2019.\n\nThe truth of the f-measure. Teach Tutor Mater. Yutaka Sasaki, Yutaka Sasaki. The truth of the f-measure. Teach Tutor Mater, 01 2007.\n\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347Proximal policy optimization algorithms. arXiv preprintJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\n\nReinforcement learning: An introduction. S Richard, Andrew G Sutton, Barto, MIT pressRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\n\nDiscovering diverse multi-agent strategic behavior via reward randomization. Zhenggang Tang, Chao Yu, Boyuan Chen, Huazhe Xu, Xiaolong Wang, Fei Fang, Simon Du, Yu Wang, Yi Wu, arXiv:2103.04564arXiv preprintZhenggang Tang, Chao Yu, Boyuan Chen, Huazhe Xu, Xiaolong Wang, Fei Fang, Simon Du, Yu Wang, and Yi Wu. Discovering diverse multi-agent strategic behavior via reward randomization. arXiv preprint arXiv:2103.04564, 2021.\n\nGrandmaster level in starcraft ii using multi-agent reinforcement learning. Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha\u00ebl Mathieu, Andrew Dudzik, Junyoung Chung, H David, Richard Choi, Timo Powell, Petko Ewalds, Georgiev, Nature. 5757782Oriol Vinyals, Igor Babuschkin, Wojciech M Czarnecki, Micha\u00ebl Mathieu, Andrew Dudzik, Jun- young Chung, David H Choi, Richard Powell, Timo Ewalds, Petko Georgiev, et al. Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature, 575(7782):350-354, 2019.\n\nDiverse dialogue generation by fusing mutual persona-aware and self-transferrer. Fuyong Xu, Guangtao Xu, Yuanying Wang, Ru Wang, Qi Ding, Peiyu Liu, Zhenfang Zhu, Applied Intelligence. Fuyong Xu, Guangtao Xu, Yuanying Wang, Ru Wang, Qi Ding, Peiyu Liu, and Zhenfang Zhu. Diverse dialogue generation by fusing mutual persona-aware and self-transferrer. Applied Intelligence, pages 1-14, 2022.\n\nThe surprising effectiveness of mappo in cooperative. Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, Yi Wu, arXiv:2103.01955multi-agent games. arXiv preprintChao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of mappo in cooperative, multi-agent games. arXiv preprint arXiv:2103.01955, 2021.\n\nLearning efficient multi-agent cooperative visual exploration. Chao Yu, Xinyi Yang, Jiaxuan Gao, Huazhong Yang, Yu Wang, Yi Wu, arXiv:2110.05734arXiv preprintChao Yu, Xinyi Yang, Jiaxuan Gao, Huazhong Yang, Yu Wang, and Yi Wu. Learning efficient multi-agent cooperative visual exploration. arXiv preprint arXiv:2110.05734, 2021.\n\nDiscovering diverse nearly optimal policies with successor features. Tom Zahavy, O&apos; Brendan, Andre Donoghue, Sebastian Barreto, Volodymyr Flennerhag, Satinder Mnih, Singh, ICML 2021 Workshop on Unsupervised Reinforcement Learning. Tom Zahavy, Brendan O'Donoghue, Andre Barreto, Sebastian Flennerhag, Volodymyr Mnih, and Satinder Singh. Discovering diverse nearly optimal policies with successor features. In ICML 2021 Workshop on Unsupervised Reinforcement Learning, 2021.\n\nZihan Zhou, Wei Fu, Bingliang Zhang, Yi Wu, arXiv:2204.02246Continuously discovering novel strategies via reward-switching policy optimization. arXiv preprintZihan Zhou, Wei Fu, Bingliang Zhang, and Yi Wu. Continuously discovering novel strategies via reward-switching policy optimization. arXiv preprint arXiv:2204.02246, 2022.\n\nMaximum entropy inverse reinforcement learning. D Brian, Andrew L Ziebart, Andrew Maas, Anind K Bagnell, Dey, Aaai. Chicago, IL, USA8Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, and Anind K Dey. Maximum entropy inverse reinforcement learning. In Aaai, volume 8, pages 1433-1438. Chicago, IL, USA, 2008.\n", "annotations": {"author": "[{\"end\":128,\"start\":83},{\"end\":194,\"start\":129},{\"end\":241,\"start\":195},{\"end\":320,\"start\":242},{\"end\":383,\"start\":321},{\"end\":453,\"start\":384},{\"end\":496,\"start\":454}]", "publisher": null, "author_last_name": "[{\"end\":93,\"start\":89},{\"end\":140,\"start\":135},{\"end\":206,\"start\":200},{\"end\":252,\"start\":246},{\"end\":331,\"start\":329},{\"end\":393,\"start\":389},{\"end\":461,\"start\":458}]", "author_first_name": "[{\"end\":88,\"start\":83},{\"end\":134,\"start\":129},{\"end\":199,\"start\":195},{\"end\":245,\"start\":242},{\"end\":328,\"start\":321},{\"end\":388,\"start\":384},{\"end\":457,\"start\":454}]", "author_affiliation": "[{\"end\":127,\"start\":95},{\"end\":193,\"start\":167},{\"end\":240,\"start\":208},{\"end\":319,\"start\":279},{\"end\":382,\"start\":356},{\"end\":452,\"start\":420},{\"end\":495,\"start\":463}]", "title": "[{\"end\":80,\"start\":1},{\"end\":576,\"start\":497}]", "venue": null, "abstract": "[{\"end\":1783,\"start\":578}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b36\"},\"end\":1908,\"start\":1904},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1910,\"start\":1908},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1913,\"start\":1910},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":1916,\"start\":1913},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":1934,\"start\":1930},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":1937,\"start\":1934},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2598,\"start\":2594},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2799,\"start\":2795},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2955,\"start\":2951},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2957,\"start\":2955},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2960,\"start\":2957},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2963,\"start\":2960},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2965,\"start\":2963},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3284,\"start\":3280},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3340,\"start\":3336},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3343,\"start\":3340},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":4528,\"start\":4524},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4541,\"start\":4537},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6124,\"start\":6120},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6126,\"start\":6124},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6129,\"start\":6126},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6173,\"start\":6169},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6438,\"start\":6434},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6441,\"start\":6438},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6444,\"start\":6441},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6465,\"start\":6462},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6557,\"start\":6553},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7050,\"start\":7046},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7053,\"start\":7050},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7055,\"start\":7053},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7058,\"start\":7055},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7060,\"start\":7058},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7082,\"start\":7079},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7218,\"start\":7214},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7577,\"start\":7573},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7580,\"start\":7577},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7583,\"start\":7580},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7604,\"start\":7600},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7683,\"start\":7679},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7902,\"start\":7898},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8132,\"start\":8128},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8370,\"start\":8366},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9935,\"start\":9931},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10249,\"start\":10245},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11656,\"start\":11653},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":12277,\"start\":12274},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":17000,\"start\":16996},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":17343,\"start\":17339},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":19047,\"start\":19043},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":19092,\"start\":19088},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19107,\"start\":19104},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":19173,\"start\":19169},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":19213,\"start\":19209},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19330,\"start\":19327},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19523,\"start\":19519},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":19690,\"start\":19686},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":20554,\"start\":20550},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21826,\"start\":21822},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":29629,\"start\":29625}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":26796,\"start\":26657},{\"attributes\":{\"id\":\"fig_1\"},\"end\":27098,\"start\":26797},{\"attributes\":{\"id\":\"fig_2\"},\"end\":28007,\"start\":27099},{\"attributes\":{\"id\":\"fig_3\"},\"end\":28055,\"start\":28008},{\"attributes\":{\"id\":\"fig_4\"},\"end\":28328,\"start\":28056},{\"attributes\":{\"id\":\"fig_5\"},\"end\":29082,\"start\":28329},{\"attributes\":{\"id\":\"fig_6\"},\"end\":29564,\"start\":29083},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":29884,\"start\":29565}]", "paragraph": "[{\"end\":2800,\"start\":1799},{\"end\":3417,\"start\":2802},{\"end\":3679,\"start\":3419},{\"end\":3918,\"start\":3681},{\"end\":4157,\"start\":3920},{\"end\":4433,\"start\":4159},{\"end\":4807,\"start\":4435},{\"end\":5283,\"start\":4809},{\"end\":5818,\"start\":5285},{\"end\":5997,\"start\":5836},{\"end\":6943,\"start\":6057},{\"end\":9356,\"start\":6983},{\"end\":9467,\"start\":9374},{\"end\":9674,\"start\":9507},{\"end\":10354,\"start\":9736},{\"end\":10935,\"start\":10527},{\"end\":11420,\"start\":10951},{\"end\":11746,\"start\":11446},{\"end\":11969,\"start\":11804},{\"end\":12073,\"start\":12017},{\"end\":12519,\"start\":12075},{\"end\":12895,\"start\":12589},{\"end\":12965,\"start\":12964},{\"end\":13158,\"start\":12967},{\"end\":13289,\"start\":13234},{\"end\":13455,\"start\":13353},{\"end\":13807,\"start\":13539},{\"end\":14221,\"start\":13855},{\"end\":14504,\"start\":14259},{\"end\":15144,\"start\":14646},{\"end\":15304,\"start\":15267},{\"end\":16011,\"start\":15359},{\"end\":16262,\"start\":16058},{\"end\":16619,\"start\":16264},{\"end\":17344,\"start\":16761},{\"end\":17971,\"start\":17540},{\"end\":18352,\"start\":18019},{\"end\":18919,\"start\":18458},{\"end\":20050,\"start\":18935},{\"end\":20537,\"start\":20087},{\"end\":20657,\"start\":20539},{\"end\":20855,\"start\":20729},{\"end\":21129,\"start\":20857},{\"end\":21808,\"start\":21131},{\"end\":22211,\"start\":21810},{\"end\":23515,\"start\":22221},{\"end\":25165,\"start\":23534},{\"end\":25638,\"start\":25167},{\"end\":26447,\"start\":25654},{\"end\":26656,\"start\":26449}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9506,\"start\":9468},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9735,\"start\":9675},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10526,\"start\":10355},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11803,\"start\":11747},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12016,\"start\":11970},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12588,\"start\":12520},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12963,\"start\":12896},{\"attributes\":{\"id\":\"formula_8\"},\"end\":13233,\"start\":13159},{\"attributes\":{\"id\":\"formula_9\"},\"end\":13352,\"start\":13290},{\"attributes\":{\"id\":\"formula_10\"},\"end\":13538,\"start\":13456},{\"attributes\":{\"id\":\"formula_11\"},\"end\":14258,\"start\":14222},{\"attributes\":{\"id\":\"formula_12\"},\"end\":14645,\"start\":14505},{\"attributes\":{\"id\":\"formula_13\"},\"end\":15266,\"start\":15145},{\"attributes\":{\"id\":\"formula_14\"},\"end\":16057,\"start\":16012},{\"attributes\":{\"id\":\"formula_15\"},\"end\":16721,\"start\":16620},{\"attributes\":{\"id\":\"formula_16\"},\"end\":17539,\"start\":17345},{\"attributes\":{\"id\":\"formula_17\"},\"end\":18018,\"start\":17972},{\"attributes\":{\"id\":\"formula_18\"},\"end\":18457,\"start\":18353},{\"attributes\":{\"id\":\"formula_19\"},\"end\":20728,\"start\":20658}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":22210,\"start\":22203},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":22516,\"start\":22509},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":22796,\"start\":22789}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1797,\"start\":1785},{\"attributes\":{\"n\":\"2\"},\"end\":5834,\"start\":5821},{\"attributes\":{\"n\":\"2.1\"},\"end\":6055,\"start\":6000},{\"attributes\":{\"n\":\"2.2\"},\"end\":6981,\"start\":6946},{\"attributes\":{\"n\":\"3\"},\"end\":9372,\"start\":9359},{\"attributes\":{\"n\":\"4\"},\"end\":10949,\"start\":10938},{\"attributes\":{\"n\":\"4.1\"},\"end\":11444,\"start\":11423},{\"attributes\":{\"n\":\"4.2\"},\"end\":13853,\"start\":13810},{\"attributes\":{\"n\":\"4.3\"},\"end\":15357,\"start\":15307},{\"attributes\":{\"n\":\"4.4\"},\"end\":16759,\"start\":16723},{\"attributes\":{\"n\":\"5\"},\"end\":18933,\"start\":18922},{\"attributes\":{\"n\":\"5.1\"},\"end\":20085,\"start\":20053},{\"attributes\":{\"n\":\"5.2\"},\"end\":22219,\"start\":22214},{\"attributes\":{\"n\":\"5.4\"},\"end\":23532,\"start\":23518},{\"attributes\":{\"n\":\"6\"},\"end\":25652,\"start\":25641},{\"end\":26668,\"start\":26658},{\"end\":26808,\"start\":26798},{\"end\":27110,\"start\":27100},{\"end\":28019,\"start\":28009},{\"end\":28067,\"start\":28057},{\"end\":28348,\"start\":28330},{\"end\":29104,\"start\":29084},{\"end\":29575,\"start\":29566}]", "table": "[{\"end\":29884,\"start\":29654}]", "figure_caption": "[{\"end\":26796,\"start\":26670},{\"end\":27098,\"start\":26810},{\"end\":28007,\"start\":27112},{\"end\":28055,\"start\":28021},{\"end\":28328,\"start\":28069},{\"end\":29082,\"start\":28351},{\"end\":29564,\"start\":29107},{\"end\":29654,\"start\":29577}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9847,\"start\":9841},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10049,\"start\":10040},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18798,\"start\":18792},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20132,\"start\":20126},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21090,\"start\":21084},{\"end\":22501,\"start\":22495},{\"end\":22784,\"start\":22775},{\"end\":22929,\"start\":22923},{\"end\":23723,\"start\":23714},{\"end\":25246,\"start\":25237}]", "bib_author_first_name": "[{\"end\":29967,\"start\":29962},{\"end\":29990,\"start\":29986},{\"end\":30006,\"start\":29999},{\"end\":30341,\"start\":30330},{\"end\":30354,\"start\":30350},{\"end\":30371,\"start\":30365},{\"end\":30383,\"start\":30378},{\"end\":30402,\"start\":30392},{\"end\":30418,\"start\":30411},{\"end\":30434,\"start\":30429},{\"end\":30448,\"start\":30442},{\"end\":30464,\"start\":30458},{\"end\":30478,\"start\":30473},{\"end\":30840,\"start\":30834},{\"end\":30850,\"start\":30847},{\"end\":30870,\"start\":30866},{\"end\":30888,\"start\":30879},{\"end\":30902,\"start\":30894},{\"end\":30921,\"start\":30916},{\"end\":31239,\"start\":31232},{\"end\":31254,\"start\":31247},{\"end\":31492,\"start\":31484},{\"end\":31512,\"start\":31504},{\"end\":31526,\"start\":31520},{\"end\":31540,\"start\":31534},{\"end\":31901,\"start\":31893},{\"end\":31919,\"start\":31913},{\"end\":31941,\"start\":31935},{\"end\":32200,\"start\":32194},{\"end\":32216,\"start\":32211},{\"end\":32761,\"start\":32758},{\"end\":32770,\"start\":32767},{\"end\":32784,\"start\":32775},{\"end\":32796,\"start\":32790},{\"end\":32808,\"start\":32801},{\"end\":33222,\"start\":33216},{\"end\":33241,\"start\":33233},{\"end\":33261,\"start\":33255},{\"end\":33276,\"start\":33270},{\"end\":33556,\"start\":33550},{\"end\":33573,\"start\":33567},{\"end\":33588,\"start\":33580},{\"end\":33608,\"start\":33602},{\"end\":33623,\"start\":33617},{\"end\":33631,\"start\":33628},{\"end\":33643,\"start\":33637},{\"end\":33656,\"start\":33651},{\"end\":33670,\"start\":33662},{\"end\":34078,\"start\":34073},{\"end\":34092,\"start\":34088},{\"end\":34099,\"start\":34093},{\"end\":34118,\"start\":34114},{\"end\":34132,\"start\":34125},{\"end\":34146,\"start\":34140},{\"end\":34506,\"start\":34497},{\"end\":34519,\"start\":34511},{\"end\":34535,\"start\":34526},{\"end\":34823,\"start\":34818},{\"end\":34836,\"start\":34831},{\"end\":34850,\"start\":34843},{\"end\":34864,\"start\":34858},{\"end\":34877,\"start\":34869},{\"end\":34889,\"start\":34883},{\"end\":34898,\"start\":34894},{\"end\":34908,\"start\":34905},{\"end\":35243,\"start\":35238},{\"end\":35255,\"start\":35251},{\"end\":35263,\"start\":35260},{\"end\":35273,\"start\":35269},{\"end\":35660,\"start\":35655},{\"end\":35672,\"start\":35668},{\"end\":35680,\"start\":35677},{\"end\":35690,\"start\":35686},{\"end\":35978,\"start\":35968},{\"end\":35989,\"start\":35984},{\"end\":36004,\"start\":36000},{\"end\":36008,\"start\":36005},{\"end\":36018,\"start\":36013},{\"end\":36031,\"start\":36025},{\"end\":36393,\"start\":36386},{\"end\":36407,\"start\":36401},{\"end\":36421,\"start\":36415},{\"end\":36437,\"start\":36430},{\"end\":36841,\"start\":36837},{\"end\":36859,\"start\":36851},{\"end\":36878,\"start\":36870},{\"end\":36896,\"start\":36888},{\"end\":36912,\"start\":36908},{\"end\":36926,\"start\":36920},{\"end\":36941,\"start\":36936},{\"end\":36955,\"start\":36950},{\"end\":37329,\"start\":37323},{\"end\":37341,\"start\":37335},{\"end\":37359,\"start\":37353},{\"end\":37374,\"start\":37368},{\"end\":37695,\"start\":37686},{\"end\":37708,\"start\":37701},{\"end\":37723,\"start\":37715},{\"end\":37979,\"start\":37973},{\"end\":38299,\"start\":38294},{\"end\":38308,\"start\":38304},{\"end\":38321,\"start\":38317},{\"end\":38336,\"start\":38330},{\"end\":38353,\"start\":38345},{\"end\":38362,\"start\":38359},{\"end\":38583,\"start\":38579},{\"end\":38599,\"start\":38593},{\"end\":38615,\"start\":38608},{\"end\":38633,\"start\":38627},{\"end\":38642,\"start\":38634},{\"end\":38965,\"start\":38959},{\"end\":38981,\"start\":38975},{\"end\":39277,\"start\":39273},{\"end\":39294,\"start\":39288},{\"end\":39704,\"start\":39691},{\"end\":39721,\"start\":39713},{\"end\":40062,\"start\":40054},{\"end\":40072,\"start\":40068},{\"end\":40092,\"start\":40085},{\"end\":40356,\"start\":40352},{\"end\":40376,\"start\":40372},{\"end\":40389,\"start\":40388},{\"end\":40410,\"start\":40401},{\"end\":40786,\"start\":40778},{\"end\":40802,\"start\":40794},{\"end\":40818,\"start\":40812},{\"end\":41093,\"start\":41086},{\"end\":41108,\"start\":41102},{\"end\":41119,\"start\":41115},{\"end\":41136,\"start\":41128},{\"end\":41152,\"start\":41145},{\"end\":41174,\"start\":41169},{\"end\":41547,\"start\":41541},{\"end\":41563,\"start\":41556},{\"end\":41584,\"start\":41575},{\"end\":41603,\"start\":41596},{\"end\":41619,\"start\":41614},{\"end\":41636,\"start\":41630},{\"end\":41983,\"start\":41976},{\"end\":42001,\"start\":41995},{\"end\":42019,\"start\":42010},{\"end\":42041,\"start\":42034},{\"end\":42054,\"start\":42048},{\"end\":42076,\"start\":42075},{\"end\":42078,\"start\":42077},{\"end\":42092,\"start\":42084},{\"end\":42108,\"start\":42107},{\"end\":42110,\"start\":42109},{\"end\":42124,\"start\":42119},{\"end\":42137,\"start\":42131},{\"end\":42538,\"start\":42532},{\"end\":42623,\"start\":42619},{\"end\":42639,\"start\":42634},{\"end\":42656,\"start\":42648},{\"end\":42671,\"start\":42667},{\"end\":42685,\"start\":42681},{\"end\":42967,\"start\":42966},{\"end\":42983,\"start\":42977},{\"end\":42985,\"start\":42984},{\"end\":43192,\"start\":43183},{\"end\":43203,\"start\":43199},{\"end\":43214,\"start\":43208},{\"end\":43227,\"start\":43221},{\"end\":43240,\"start\":43232},{\"end\":43250,\"start\":43247},{\"end\":43262,\"start\":43257},{\"end\":43269,\"start\":43267},{\"end\":43278,\"start\":43276},{\"end\":43615,\"start\":43610},{\"end\":43629,\"start\":43625},{\"end\":43650,\"start\":43642},{\"end\":43652,\"start\":43651},{\"end\":43671,\"start\":43664},{\"end\":43687,\"start\":43681},{\"end\":43704,\"start\":43696},{\"end\":43713,\"start\":43712},{\"end\":43728,\"start\":43721},{\"end\":43739,\"start\":43735},{\"end\":43753,\"start\":43748},{\"end\":44154,\"start\":44148},{\"end\":44167,\"start\":44159},{\"end\":44180,\"start\":44172},{\"end\":44189,\"start\":44187},{\"end\":44198,\"start\":44196},{\"end\":44210,\"start\":44205},{\"end\":44224,\"start\":44216},{\"end\":44518,\"start\":44514},{\"end\":44528,\"start\":44523},{\"end\":44541,\"start\":44535},{\"end\":44554,\"start\":44552},{\"end\":44570,\"start\":44561},{\"end\":44580,\"start\":44578},{\"end\":44889,\"start\":44885},{\"end\":44899,\"start\":44894},{\"end\":44913,\"start\":44906},{\"end\":44927,\"start\":44919},{\"end\":44936,\"start\":44934},{\"end\":44945,\"start\":44943},{\"end\":45224,\"start\":45221},{\"end\":45240,\"start\":45233},{\"end\":45255,\"start\":45250},{\"end\":45275,\"start\":45266},{\"end\":45294,\"start\":45285},{\"end\":45315,\"start\":45307},{\"end\":45636,\"start\":45631},{\"end\":45646,\"start\":45643},{\"end\":45660,\"start\":45651},{\"end\":45670,\"start\":45668},{\"end\":46010,\"start\":46009},{\"end\":46024,\"start\":46018},{\"end\":46026,\"start\":46025},{\"end\":46042,\"start\":46036},{\"end\":46056,\"start\":46049}]", "bib_author_last_name": "[{\"end\":29984,\"start\":29968},{\"end\":29997,\"start\":29991},{\"end\":30013,\"start\":30007},{\"end\":30022,\"start\":30015},{\"end\":30348,\"start\":30342},{\"end\":30363,\"start\":30355},{\"end\":30376,\"start\":30372},{\"end\":30390,\"start\":30384},{\"end\":30409,\"start\":30403},{\"end\":30427,\"start\":30419},{\"end\":30440,\"start\":30435},{\"end\":30456,\"start\":30449},{\"end\":30471,\"start\":30465},{\"end\":30484,\"start\":30479},{\"end\":30845,\"start\":30841},{\"end\":30864,\"start\":30851},{\"end\":30877,\"start\":30871},{\"end\":30892,\"start\":30889},{\"end\":30914,\"start\":30903},{\"end\":30931,\"start\":30922},{\"end\":31245,\"start\":31240},{\"end\":31260,\"start\":31255},{\"end\":31502,\"start\":31493},{\"end\":31518,\"start\":31513},{\"end\":31532,\"start\":31527},{\"end\":31547,\"start\":31541},{\"end\":31911,\"start\":31902},{\"end\":31933,\"start\":31920},{\"end\":31948,\"start\":31942},{\"end\":32209,\"start\":32201},{\"end\":32223,\"start\":32217},{\"end\":32765,\"start\":32762},{\"end\":32773,\"start\":32771},{\"end\":32788,\"start\":32785},{\"end\":32799,\"start\":32797},{\"end\":32812,\"start\":32809},{\"end\":33231,\"start\":33223},{\"end\":33253,\"start\":33242},{\"end\":33268,\"start\":33262},{\"end\":33283,\"start\":33277},{\"end\":33565,\"start\":33557},{\"end\":33578,\"start\":33574},{\"end\":33600,\"start\":33589},{\"end\":33615,\"start\":33609},{\"end\":33626,\"start\":33624},{\"end\":33635,\"start\":33632},{\"end\":33649,\"start\":33644},{\"end\":33660,\"start\":33657},{\"end\":33676,\"start\":33671},{\"end\":34086,\"start\":34079},{\"end\":34112,\"start\":34100},{\"end\":34123,\"start\":34119},{\"end\":34138,\"start\":34133},{\"end\":34157,\"start\":34147},{\"end\":34509,\"start\":34507},{\"end\":34524,\"start\":34520},{\"end\":34538,\"start\":34536},{\"end\":34829,\"start\":34824},{\"end\":34841,\"start\":34837},{\"end\":34856,\"start\":34851},{\"end\":34867,\"start\":34865},{\"end\":34881,\"start\":34878},{\"end\":34892,\"start\":34890},{\"end\":34903,\"start\":34899},{\"end\":34912,\"start\":34909},{\"end\":35249,\"start\":35244},{\"end\":35258,\"start\":35256},{\"end\":35267,\"start\":35264},{\"end\":35278,\"start\":35274},{\"end\":35666,\"start\":35661},{\"end\":35675,\"start\":35673},{\"end\":35684,\"start\":35681},{\"end\":35695,\"start\":35691},{\"end\":35982,\"start\":35979},{\"end\":35998,\"start\":35990},{\"end\":36011,\"start\":36009},{\"end\":36023,\"start\":36019},{\"end\":36040,\"start\":36032},{\"end\":36399,\"start\":36394},{\"end\":36413,\"start\":36408},{\"end\":36428,\"start\":36422},{\"end\":36442,\"start\":36438},{\"end\":36849,\"start\":36842},{\"end\":36868,\"start\":36860},{\"end\":36886,\"start\":36879},{\"end\":36906,\"start\":36897},{\"end\":36918,\"start\":36913},{\"end\":36934,\"start\":36927},{\"end\":36948,\"start\":36942},{\"end\":36963,\"start\":36956},{\"end\":37333,\"start\":37330},{\"end\":37351,\"start\":37342},{\"end\":37366,\"start\":37360},{\"end\":37381,\"start\":37375},{\"end\":37699,\"start\":37696},{\"end\":37713,\"start\":37709},{\"end\":37727,\"start\":37724},{\"end\":37986,\"start\":37980},{\"end\":38302,\"start\":38300},{\"end\":38315,\"start\":38309},{\"end\":38328,\"start\":38322},{\"end\":38343,\"start\":38337},{\"end\":38357,\"start\":38354},{\"end\":38371,\"start\":38363},{\"end\":38591,\"start\":38584},{\"end\":38606,\"start\":38600},{\"end\":38625,\"start\":38616},{\"end\":38648,\"start\":38643},{\"end\":38973,\"start\":38966},{\"end\":38997,\"start\":38982},{\"end\":39286,\"start\":39278},{\"end\":39301,\"start\":39295},{\"end\":39711,\"start\":39705},{\"end\":39730,\"start\":39722},{\"end\":40066,\"start\":40063},{\"end\":40083,\"start\":40073},{\"end\":40101,\"start\":40093},{\"end\":40370,\"start\":40357},{\"end\":40386,\"start\":40377},{\"end\":40399,\"start\":40390},{\"end\":40422,\"start\":40411},{\"end\":40431,\"start\":40424},{\"end\":40792,\"start\":40787},{\"end\":40810,\"start\":40803},{\"end\":40826,\"start\":40819},{\"end\":41100,\"start\":41094},{\"end\":41113,\"start\":41109},{\"end\":41126,\"start\":41120},{\"end\":41143,\"start\":41137},{\"end\":41167,\"start\":41153},{\"end\":41182,\"start\":41175},{\"end\":41554,\"start\":41548},{\"end\":41573,\"start\":41564},{\"end\":41594,\"start\":41585},{\"end\":41612,\"start\":41604},{\"end\":41628,\"start\":41620},{\"end\":41645,\"start\":41637},{\"end\":41993,\"start\":41984},{\"end\":42008,\"start\":42002},{\"end\":42032,\"start\":42020},{\"end\":42046,\"start\":42042},{\"end\":42063,\"start\":42055},{\"end\":42073,\"start\":42065},{\"end\":42082,\"start\":42079},{\"end\":42099,\"start\":42093},{\"end\":42105,\"start\":42101},{\"end\":42117,\"start\":42111},{\"end\":42129,\"start\":42125},{\"end\":42146,\"start\":42138},{\"end\":42156,\"start\":42148},{\"end\":42545,\"start\":42539},{\"end\":42632,\"start\":42624},{\"end\":42646,\"start\":42640},{\"end\":42665,\"start\":42657},{\"end\":42679,\"start\":42672},{\"end\":42692,\"start\":42686},{\"end\":42975,\"start\":42968},{\"end\":42992,\"start\":42986},{\"end\":42999,\"start\":42994},{\"end\":43197,\"start\":43193},{\"end\":43206,\"start\":43204},{\"end\":43219,\"start\":43215},{\"end\":43230,\"start\":43228},{\"end\":43245,\"start\":43241},{\"end\":43255,\"start\":43251},{\"end\":43265,\"start\":43263},{\"end\":43274,\"start\":43270},{\"end\":43281,\"start\":43279},{\"end\":43623,\"start\":43616},{\"end\":43640,\"start\":43630},{\"end\":43662,\"start\":43653},{\"end\":43679,\"start\":43672},{\"end\":43694,\"start\":43688},{\"end\":43710,\"start\":43705},{\"end\":43719,\"start\":43714},{\"end\":43733,\"start\":43729},{\"end\":43746,\"start\":43740},{\"end\":43760,\"start\":43754},{\"end\":43770,\"start\":43762},{\"end\":44157,\"start\":44155},{\"end\":44170,\"start\":44168},{\"end\":44185,\"start\":44181},{\"end\":44194,\"start\":44190},{\"end\":44203,\"start\":44199},{\"end\":44214,\"start\":44211},{\"end\":44228,\"start\":44225},{\"end\":44521,\"start\":44519},{\"end\":44533,\"start\":44529},{\"end\":44550,\"start\":44542},{\"end\":44559,\"start\":44555},{\"end\":44576,\"start\":44571},{\"end\":44583,\"start\":44581},{\"end\":44892,\"start\":44890},{\"end\":44904,\"start\":44900},{\"end\":44917,\"start\":44914},{\"end\":44932,\"start\":44928},{\"end\":44941,\"start\":44937},{\"end\":44948,\"start\":44946},{\"end\":45231,\"start\":45225},{\"end\":45248,\"start\":45241},{\"end\":45264,\"start\":45256},{\"end\":45283,\"start\":45276},{\"end\":45305,\"start\":45295},{\"end\":45320,\"start\":45316},{\"end\":45327,\"start\":45322},{\"end\":45641,\"start\":45637},{\"end\":45649,\"start\":45647},{\"end\":45666,\"start\":45661},{\"end\":45673,\"start\":45671},{\"end\":46016,\"start\":46011},{\"end\":46034,\"start\":46027},{\"end\":46047,\"start\":46043},{\"end\":46064,\"start\":46057},{\"end\":46069,\"start\":46066}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":1552061},\"end\":30275,\"start\":29886},{\"attributes\":{\"doi\":\"arXiv:1912.06680\",\"id\":\"b1\"},\"end\":30770,\"start\":30277},{\"attributes\":{\"doi\":\"arXiv:2206.00059\",\"id\":\"b2\"},\"end\":31166,\"start\":30772},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":235899346},\"end\":31482,\"start\":31168},{\"attributes\":{\"doi\":\"arXiv:1802.06070\",\"id\":\"b4\"},\"end\":31826,\"start\":31484},{\"attributes\":{\"doi\":\"arXiv:2110.02719\",\"id\":\"b5\"},\"end\":32144,\"start\":31828},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":18661146},\"end\":32689,\"start\":32146},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":53306804},\"end\":33151,\"start\":32691},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":4718601},\"end\":33548,\"start\":33153},{\"attributes\":{\"doi\":\"arXiv:1812.05905\",\"id\":\"b9\"},\"end\":34012,\"start\":33550},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":65039738},\"end\":34424,\"start\":34014},{\"attributes\":{\"doi\":\"arXiv:2006.04021\",\"id\":\"b11\"},\"end\":34726,\"start\":34426},{\"attributes\":{\"doi\":\"arXiv:2110.04507\",\"id\":\"b12\"},\"end\":35172,\"start\":34728},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":69274303},\"end\":35598,\"start\":35174},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":210902023},\"end\":35914,\"start\":35600},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":46955236},\"end\":36301,\"start\":35916},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":221095075},\"end\":36712,\"start\":36303},{\"attributes\":{\"id\":\"b17\"},\"end\":37231,\"start\":36714},{\"attributes\":{\"doi\":\"arXiv:1907.00953\",\"id\":\"b18\"},\"end\":37605,\"start\":37233},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":213499397},\"end\":37971,\"start\":37607},{\"attributes\":{\"doi\":\"arXiv:1805.00909\",\"id\":\"b20\"},\"end\":38239,\"start\":37973},{\"attributes\":{\"doi\":\"arXiv:1606.01541\",\"id\":\"b21\"},\"end\":38577,\"start\":38241},{\"attributes\":{\"doi\":\"arXiv:1910.07483\",\"id\":\"b22\"},\"end\":38868,\"start\":38579},{\"attributes\":{\"doi\":\"arXiv:1509.08731\",\"id\":\"b23\"},\"end\":39199,\"start\":38870},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":13548281},\"end\":39603,\"start\":39201},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":3494235},\"end\":39990,\"start\":39605},{\"attributes\":{\"doi\":\"arXiv:2103.07084\",\"id\":\"b26\"},\"end\":40286,\"start\":39992},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":211010962},\"end\":40708,\"start\":40288},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":234967703},\"end\":40989,\"start\":40710},{\"attributes\":{\"doi\":\"arXiv:1809.09369\",\"id\":\"b29\"},\"end\":41449,\"start\":40991},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":4533648},\"end\":41974,\"start\":41451},{\"attributes\":{\"doi\":\"arXiv:1902.04043\",\"id\":\"b31\"},\"end\":42483,\"start\":41976},{\"attributes\":{\"id\":\"b32\"},\"end\":42617,\"start\":42485},{\"attributes\":{\"doi\":\"arXiv:1707.06347\",\"id\":\"b33\"},\"end\":42923,\"start\":42619},{\"attributes\":{\"id\":\"b34\"},\"end\":43104,\"start\":42925},{\"attributes\":{\"doi\":\"arXiv:2103.04564\",\"id\":\"b35\"},\"end\":43532,\"start\":43106},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":204972004},\"end\":44065,\"start\":43534},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":237730265},\"end\":44458,\"start\":44067},{\"attributes\":{\"doi\":\"arXiv:2103.01955\",\"id\":\"b38\"},\"end\":44820,\"start\":44460},{\"attributes\":{\"doi\":\"arXiv:2110.05734\",\"id\":\"b39\"},\"end\":45150,\"start\":44822},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":235265672},\"end\":45629,\"start\":45152},{\"attributes\":{\"doi\":\"arXiv:2204.02246\",\"id\":\"b41\"},\"end\":45959,\"start\":45631},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":336219},\"end\":46268,\"start\":45961}]", "bib_title": "[{\"end\":29960,\"start\":29886},{\"end\":31230,\"start\":31168},{\"end\":32192,\"start\":32146},{\"end\":32756,\"start\":32691},{\"end\":33214,\"start\":33153},{\"end\":34071,\"start\":34014},{\"end\":35236,\"start\":35174},{\"end\":35653,\"start\":35600},{\"end\":35966,\"start\":35916},{\"end\":36384,\"start\":36303},{\"end\":37684,\"start\":37607},{\"end\":39271,\"start\":39201},{\"end\":39689,\"start\":39605},{\"end\":40350,\"start\":40288},{\"end\":40776,\"start\":40710},{\"end\":41539,\"start\":41451},{\"end\":43608,\"start\":43534},{\"end\":44146,\"start\":44067},{\"end\":45219,\"start\":45152},{\"end\":46007,\"start\":45961}]", "bib_author": "[{\"end\":29986,\"start\":29962},{\"end\":29999,\"start\":29986},{\"end\":30015,\"start\":29999},{\"end\":30024,\"start\":30015},{\"end\":30350,\"start\":30330},{\"end\":30365,\"start\":30350},{\"end\":30378,\"start\":30365},{\"end\":30392,\"start\":30378},{\"end\":30411,\"start\":30392},{\"end\":30429,\"start\":30411},{\"end\":30442,\"start\":30429},{\"end\":30458,\"start\":30442},{\"end\":30473,\"start\":30458},{\"end\":30486,\"start\":30473},{\"end\":30847,\"start\":30834},{\"end\":30866,\"start\":30847},{\"end\":30879,\"start\":30866},{\"end\":30894,\"start\":30879},{\"end\":30916,\"start\":30894},{\"end\":30933,\"start\":30916},{\"end\":31247,\"start\":31232},{\"end\":31262,\"start\":31247},{\"end\":31504,\"start\":31484},{\"end\":31520,\"start\":31504},{\"end\":31534,\"start\":31520},{\"end\":31549,\"start\":31534},{\"end\":31913,\"start\":31893},{\"end\":31935,\"start\":31913},{\"end\":31950,\"start\":31935},{\"end\":32211,\"start\":32194},{\"end\":32225,\"start\":32211},{\"end\":32767,\"start\":32758},{\"end\":32775,\"start\":32767},{\"end\":32790,\"start\":32775},{\"end\":32801,\"start\":32790},{\"end\":32814,\"start\":32801},{\"end\":33233,\"start\":33216},{\"end\":33255,\"start\":33233},{\"end\":33270,\"start\":33255},{\"end\":33285,\"start\":33270},{\"end\":33567,\"start\":33550},{\"end\":33580,\"start\":33567},{\"end\":33602,\"start\":33580},{\"end\":33617,\"start\":33602},{\"end\":33628,\"start\":33617},{\"end\":33637,\"start\":33628},{\"end\":33651,\"start\":33637},{\"end\":33662,\"start\":33651},{\"end\":33678,\"start\":33662},{\"end\":34088,\"start\":34073},{\"end\":34114,\"start\":34088},{\"end\":34125,\"start\":34114},{\"end\":34140,\"start\":34125},{\"end\":34159,\"start\":34140},{\"end\":34511,\"start\":34497},{\"end\":34526,\"start\":34511},{\"end\":34540,\"start\":34526},{\"end\":34831,\"start\":34818},{\"end\":34843,\"start\":34831},{\"end\":34858,\"start\":34843},{\"end\":34869,\"start\":34858},{\"end\":34883,\"start\":34869},{\"end\":34894,\"start\":34883},{\"end\":34905,\"start\":34894},{\"end\":34914,\"start\":34905},{\"end\":35251,\"start\":35238},{\"end\":35260,\"start\":35251},{\"end\":35269,\"start\":35260},{\"end\":35280,\"start\":35269},{\"end\":35668,\"start\":35655},{\"end\":35677,\"start\":35668},{\"end\":35686,\"start\":35677},{\"end\":35697,\"start\":35686},{\"end\":35984,\"start\":35968},{\"end\":36000,\"start\":35984},{\"end\":36013,\"start\":36000},{\"end\":36025,\"start\":36013},{\"end\":36042,\"start\":36025},{\"end\":36401,\"start\":36386},{\"end\":36415,\"start\":36401},{\"end\":36430,\"start\":36415},{\"end\":36444,\"start\":36430},{\"end\":36851,\"start\":36837},{\"end\":36870,\"start\":36851},{\"end\":36888,\"start\":36870},{\"end\":36908,\"start\":36888},{\"end\":36920,\"start\":36908},{\"end\":36936,\"start\":36920},{\"end\":36950,\"start\":36936},{\"end\":36965,\"start\":36950},{\"end\":37335,\"start\":37323},{\"end\":37353,\"start\":37335},{\"end\":37368,\"start\":37353},{\"end\":37383,\"start\":37368},{\"end\":37701,\"start\":37686},{\"end\":37715,\"start\":37701},{\"end\":37729,\"start\":37715},{\"end\":37988,\"start\":37973},{\"end\":38304,\"start\":38294},{\"end\":38317,\"start\":38304},{\"end\":38330,\"start\":38317},{\"end\":38345,\"start\":38330},{\"end\":38359,\"start\":38345},{\"end\":38373,\"start\":38359},{\"end\":38593,\"start\":38579},{\"end\":38608,\"start\":38593},{\"end\":38627,\"start\":38608},{\"end\":38650,\"start\":38627},{\"end\":38975,\"start\":38959},{\"end\":38999,\"start\":38975},{\"end\":39288,\"start\":39273},{\"end\":39303,\"start\":39288},{\"end\":39713,\"start\":39691},{\"end\":39732,\"start\":39713},{\"end\":40068,\"start\":40054},{\"end\":40085,\"start\":40068},{\"end\":40103,\"start\":40085},{\"end\":40372,\"start\":40352},{\"end\":40388,\"start\":40372},{\"end\":40401,\"start\":40388},{\"end\":40424,\"start\":40401},{\"end\":40433,\"start\":40424},{\"end\":40794,\"start\":40778},{\"end\":40812,\"start\":40794},{\"end\":40828,\"start\":40812},{\"end\":41102,\"start\":41086},{\"end\":41115,\"start\":41102},{\"end\":41128,\"start\":41115},{\"end\":41145,\"start\":41128},{\"end\":41169,\"start\":41145},{\"end\":41184,\"start\":41169},{\"end\":41556,\"start\":41541},{\"end\":41575,\"start\":41556},{\"end\":41596,\"start\":41575},{\"end\":41614,\"start\":41596},{\"end\":41630,\"start\":41614},{\"end\":41647,\"start\":41630},{\"end\":41995,\"start\":41976},{\"end\":42010,\"start\":41995},{\"end\":42034,\"start\":42010},{\"end\":42048,\"start\":42034},{\"end\":42065,\"start\":42048},{\"end\":42075,\"start\":42065},{\"end\":42084,\"start\":42075},{\"end\":42101,\"start\":42084},{\"end\":42107,\"start\":42101},{\"end\":42119,\"start\":42107},{\"end\":42131,\"start\":42119},{\"end\":42148,\"start\":42131},{\"end\":42158,\"start\":42148},{\"end\":42547,\"start\":42532},{\"end\":42634,\"start\":42619},{\"end\":42648,\"start\":42634},{\"end\":42667,\"start\":42648},{\"end\":42681,\"start\":42667},{\"end\":42694,\"start\":42681},{\"end\":42977,\"start\":42966},{\"end\":42994,\"start\":42977},{\"end\":43001,\"start\":42994},{\"end\":43199,\"start\":43183},{\"end\":43208,\"start\":43199},{\"end\":43221,\"start\":43208},{\"end\":43232,\"start\":43221},{\"end\":43247,\"start\":43232},{\"end\":43257,\"start\":43247},{\"end\":43267,\"start\":43257},{\"end\":43276,\"start\":43267},{\"end\":43283,\"start\":43276},{\"end\":43625,\"start\":43610},{\"end\":43642,\"start\":43625},{\"end\":43664,\"start\":43642},{\"end\":43681,\"start\":43664},{\"end\":43696,\"start\":43681},{\"end\":43712,\"start\":43696},{\"end\":43721,\"start\":43712},{\"end\":43735,\"start\":43721},{\"end\":43748,\"start\":43735},{\"end\":43762,\"start\":43748},{\"end\":43772,\"start\":43762},{\"end\":44159,\"start\":44148},{\"end\":44172,\"start\":44159},{\"end\":44187,\"start\":44172},{\"end\":44196,\"start\":44187},{\"end\":44205,\"start\":44196},{\"end\":44216,\"start\":44205},{\"end\":44230,\"start\":44216},{\"end\":44523,\"start\":44514},{\"end\":44535,\"start\":44523},{\"end\":44552,\"start\":44535},{\"end\":44561,\"start\":44552},{\"end\":44578,\"start\":44561},{\"end\":44585,\"start\":44578},{\"end\":44894,\"start\":44885},{\"end\":44906,\"start\":44894},{\"end\":44919,\"start\":44906},{\"end\":44934,\"start\":44919},{\"end\":44943,\"start\":44934},{\"end\":44950,\"start\":44943},{\"end\":45233,\"start\":45221},{\"end\":45250,\"start\":45233},{\"end\":45266,\"start\":45250},{\"end\":45285,\"start\":45266},{\"end\":45307,\"start\":45285},{\"end\":45322,\"start\":45307},{\"end\":45329,\"start\":45322},{\"end\":45643,\"start\":45631},{\"end\":45651,\"start\":45643},{\"end\":45668,\"start\":45651},{\"end\":45675,\"start\":45668},{\"end\":46018,\"start\":46009},{\"end\":46036,\"start\":46018},{\"end\":46049,\"start\":46036},{\"end\":46066,\"start\":46049},{\"end\":46071,\"start\":46066}]", "bib_venue": "[{\"end\":30067,\"start\":30024},{\"end\":30328,\"start\":30277},{\"end\":30832,\"start\":30772},{\"end\":31311,\"start\":31262},{\"end\":31633,\"start\":31565},{\"end\":31891,\"start\":31828},{\"end\":32321,\"start\":32225},{\"end\":32875,\"start\":32814},{\"end\":33329,\"start\":33285},{\"end\":33761,\"start\":33694},{\"end\":34211,\"start\":34159},{\"end\":34495,\"start\":34426},{\"end\":34816,\"start\":34728},{\"end\":35341,\"start\":35280},{\"end\":35749,\"start\":35697},{\"end\":36086,\"start\":36042},{\"end\":36493,\"start\":36444},{\"end\":36835,\"start\":36714},{\"end\":37321,\"start\":37233},{\"end\":37781,\"start\":37729},{\"end\":38086,\"start\":38004},{\"end\":38292,\"start\":38241},{\"end\":38701,\"start\":38666},{\"end\":38957,\"start\":38870},{\"end\":39364,\"start\":39303},{\"end\":39773,\"start\":39732},{\"end\":40052,\"start\":39992},{\"end\":40482,\"start\":40433},{\"end\":40833,\"start\":40828},{\"end\":41084,\"start\":40991},{\"end\":41691,\"start\":41647},{\"end\":42209,\"start\":42174},{\"end\":42530,\"start\":42485},{\"end\":42749,\"start\":42710},{\"end\":42964,\"start\":42925},{\"end\":43181,\"start\":43106},{\"end\":43778,\"start\":43772},{\"end\":44250,\"start\":44230},{\"end\":44512,\"start\":44460},{\"end\":44883,\"start\":44822},{\"end\":45386,\"start\":45329},{\"end\":45773,\"start\":45691},{\"end\":46075,\"start\":46071},{\"end\":32404,\"start\":32323},{\"end\":32923,\"start\":32877},{\"end\":35389,\"start\":35343},{\"end\":39412,\"start\":39366},{\"end\":46093,\"start\":46077}]"}}}, "year": 2023, "month": 12, "day": 17}