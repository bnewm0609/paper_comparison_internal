{"id": 211678094, "updated": "2023-10-06 18:11:32.369", "metadata": {"title": "Adaptive Federated Optimization", "authors": "[{\"first\":\"Sashank\",\"last\":\"Reddi\",\"middle\":[]},{\"first\":\"Zachary\",\"last\":\"Charles\",\"middle\":[]},{\"first\":\"Manzil\",\"last\":\"Zaheer\",\"middle\":[]},{\"first\":\"Zachary\",\"last\":\"Garrett\",\"middle\":[]},{\"first\":\"Keith\",\"last\":\"Rush\",\"middle\":[]},{\"first\":\"Jakub\",\"last\":\"Konevcn'y\",\"middle\":[]},{\"first\":\"Sanjiv\",\"last\":\"Kumar\",\"middle\":[]},{\"first\":\"H.\",\"last\":\"McMahan\",\"middle\":[\"Brendan\"]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 2, "day": 29}, "abstract": "Federated learning is a distributed machine learning paradigm in which a large number of clients coordinate with a central server to learn a model without sharing their own training data. Standard federated optimization methods such as Federated Averaging (FedAvg) are often difficult to tune and exhibit unfavorable convergence behavior. In non-federated settings, adaptive optimization methods have had notable success in combating such issues. In this work, we propose federated versions of adaptive optimizers, including Adagrad, Adam, and Yogi, and analyze their convergence in the presence of heterogeneous data for general non-convex settings. Our results highlight the interplay between client heterogeneity and communication efficiency. We also perform extensive experiments on these methods and show that the use of adaptive optimizers can significantly improve the performance of federated learning.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2003.00295", "mag": "3008187686", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iclr/ReddiCZGRKKM21", "doi": null}}, "content": {"source": {"pdf_hash": "8f5d6cfcc2b62a20ba0532879dc17e65a24bc3d2", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2003.00295v5.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "83366055f664b5ef6d4a4629eb2afc0433e171eb", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/8f5d6cfcc2b62a20ba0532879dc17e65a24bc3d2.txt", "contents": "\nADAPTIVE FEDERATED OPTIMIZATION\n\n\nSashank J Reddi sashank@google.com \nGoogle Research\n\n\nZachary Charles zachcharles@google.com \nGoogle Research\n\n\nManzil Zaheer manzilzaheer@google.com \nGoogle Research\n\n\nZachary Garrett zachgarrett@google.com \nGoogle Research\n\n\nKeith Rush krush@google.com \nGoogle Research\n\n\nJakub Kone\u010dn\u00fd \nGoogle Research\n\n\nSanjiv Kumar sanjivk@google.com \nGoogle Research\n\n\nH Brendan Mcmahan mcmahan@google.com \nGoogle Research\n\n\nADAPTIVE FEDERATED OPTIMIZATION\nPublished as a conference paper at ICLR 2021\nFederated learning is a distributed machine learning paradigm in which a large number of clients coordinate with a central server to learn a model without sharing their own training data. Standard federated optimization methods such as Federated Averaging (FEDAVG) are often difficult to tune and exhibit unfavorable convergence behavior. In non-federated settings, adaptive optimization methods have had notable success in combating such issues. In this work, we propose federated versions of adaptive optimizers, including ADAGRAD, ADAM, and YOGI, and analyze their convergence in the presence of heterogeneous data for general nonconvex settings. Our results highlight the interplay between client heterogeneity and communication efficiency. We also perform extensive experiments on these methods and show that the use of adaptive optimizers can significantly improve the performance of federated learning. * Authors contributed equally to this work arXiv:2003.00295v5 [cs.LG] 8 Sep 2021Published as a conference paper at ICLR 2021 adaptivity by using adaptive optimizers as client or server optimizers. Building upon this, we develop novel adaptive optimization techniques for FL by using per-coordinate methods as server optimizers. By focusing on adaptive server optimization, we enable use of adaptive learning rates without increase in client storage or communication costs, and ensure compatibility with cross-device FL.Main contributions In light of the above, we highlight the main contributions of the paper.\u2022 We study a general framework for federated optimization using server and client optimizers. This framework generalizes many existing federated optimization methods, including FEDAVG. \u2022 We use this framework to design novel, cross-device compatible, adaptive federated optimization methods, and provide convergence analysis in general nonconvex settings. To the best of our knowledge, these are the first methods for FL using adaptive server optimization. We show an important interplay between the number of local steps and the heterogeneity among clients. \u2022 We introduce comprehensive and reproducible empirical benchmarks for comparing federated optimization methods. These benchmarks consist of seven diverse and representative FL tasks involving both image and text data, with varying amounts of heterogeneity and numbers of clients. \u2022 We demonstrate strong empirical performance of our adaptive optimizers throughout, improving upon commonly used baselines. Our results show that our methods can be easier to tune, and highlight their utility in cross-device settings.\n\nINTRODUCTION\n\nFederated learning (FL) is a machine learning paradigm in which multiple clients cooperate to learn a model under the orchestration of a central server (McMahan et al., 2017). In FL, raw client data is never shared with the server or other clients. This distinguishes FL from traditional distributed optimization, and requires contending with heterogeneous data. FL has two primary settings, crosssilo (eg. FL between large institutions) and cross-device (eg. FL across edge devices) (Kairouz et al., 2019, Table 1). In cross-silo FL, most clients participate in every round and can maintain state between rounds. In the more challenging cross-device FL, our primary focus, only a small fraction of clients participate in each round, and clients cannot maintain state across rounds. For a more in-depth discussion of FL and the challenges involved, we defer to Kairouz et al. (2019) and Li et al. (2019a).\n\nStandard optimization methods, such as distributed SGD, are often unsuitable in FL and can incur high communication costs. To remedy this, many federated optimization methods use local client updates, in which clients update their models multiple times before communicating with the server. This can greatly reduce the amount of communication required to train a model. One such method is FEDAVG (McMahan et al., 2017), in which clients perform multiple epochs of SGD on their local datasets. The clients communicate their models to the server, which averages them to form a new global model. While FEDAVG has seen great success, recent works have highlighted its convergence issues in some settings (Karimireddy et al., 2019;Hsu et al., 2019). This is due to a variety of factors including (1) client drift (Karimireddy et al., 2019), where local client models move away from globally optimal models, and (2) a lack of adaptivity. FEDAVG is similar in spirit to SGD, and may be unsuitable for settings with heavy-tail stochastic gradient noise distributions, which often arise when training language models (Zhang et al., 2019a). Such settings benefit from adaptive learning rates, which incorporate knowledge of past iterations to perform more informed optimization.\n\nIn this paper, we focus on the second issue and present a simple framework for incorporating adaptivity in FL. In particular, we propose a general optimization framework in which (1) clients perform multiple epochs of training using a client optimizer to minimize loss on their local data and (2) server updates its global model by applying a gradient-based server optimizer to the average of the clients' model updates. We show that FEDAVG is the special case where SGD is used as both client and server optimizer and server learning rate is 1. This framework can also seamlessly incorporate Adaptive methods have been the subject of significant theoretical and empirical study, in both convex (McMahan & Streeter, 2010b;Duchi et al., 2011;Kingma & Ba, 2015) and non-convex settings (Li & Orabona, 2018;Ward et al., 2018;Wu et al., 2019). Reddi et al. (2019);Zaheer et al. (2018) study convergence failures of ADAM in certain non-convex settings, and develop an adaptive optimizer, YOGI, designed to improve convergence. While most work on adaptive methods focuses on non-FL settings, Xie et al. (2019) propose ADAALTER, a method for FL using adaptive client optimization. Conceptually, our approach is also related to the LOOKAHEAD optimizer (Zhang et al., 2019b), which was designed for non-FL settings. Similar to ADAALTER, an adaptive FL variant of LOOKAHEAD entails adaptive client optimization (see Appendix B.3 for more details). We note that both ADAALTER and LOOKAHEAD are, in fact, special cases of our framework (see Algorithm 1) and the primary novelty of our work comes in focusing on adaptive server optimization. This allows us to avoid aggregating optimizer states across clients, making our methods require at most half as much communication and client memory usage per round (see Appendix B.3 for details).\n\nNotation For a, b \u2208 R d , we let \u221a a, a 2 and a/b denote the element-wise square root, square, and division of the vectors. For \u03b8 i \u2208 R d , we use both \u03b8 i,j and [\u03b8 i ] j to denote its j th coordinate.\n\n\nFEDERATED LEARNING AND FEDAVG\n\nIn federated learning, we solve an optimization problem of the form:\nmin x\u2208R d f (x) = 1 m m i=1 F i (x),(1)\nwhere F i (x) = E z\u223cDi [f i (x, z)], is the loss function of the i th client, z \u2208 Z, and D i is the data distribution for the i th client. For i = j, D i and D j may be very different. The functions F i (and therefore f ) may be nonconvex. For each i and x, we assume access to an unbiased stochastic gradient g i (x) of the client's true gradient \u2207F i (x). In addition, we make the following assumptions.\n\nAssumption 1 (Lipschitz Gradient). The function F i is L-smooth for all i \u2208 [m] i.e., \u2207F i (x) \u2212 \u2207F i (y) \u2264 L x \u2212 y , for all x, y \u2208 R d .\n\nAssumption 2 (Bounded Variance). The function F i have \u03c3 l -bounded (local) variance i.e.,\nE[ \u2207[f i (x, z)] j \u2212 [\u2207F i (x)] j 2 ] = \u03c3 2 l,j for all x \u2208 R d , j \u2208 [d] and i \u2208 [m]\n. Furthermore, we assume the (global) variance is bounded,\n( 1 /m) m i=1 \u2207[F i (x)] j \u2212 [\u2207f (x)] j 2 \u2264 \u03c3 2 g,j for all x \u2208 R d and j \u2208 [d].\nAssumption 3 (Bounded Gradients). The function f i (x, z) have G-bounded gradients i.e., for any i \u2208 [m], x \u2208 R d and z \u2208 Z we have |[\u2207f i (x, z)] j | \u2264 G for all j \u2208 [d].\n\nWith a slight abuse of notation, we use \u03c3 2 l and \u03c3 2 g to denote d j=1 \u03c3 2 l,j and d j=1 \u03c3 2 g,j . Assumptions 1 and 3 are fairly standard in nonconvex optimization literature (Reddi et al., 2016;Ward et al., 2018;Zaheer et al., 2018). We make no further assumptions regarding the similarity of clients datasets. Assumption 2 is a form of bounded variance, but between the client objective functions and the overall objective function. This assumption has been used throughout various works on federated optimization (Li et al., 2018;Wang et al., 2019). Intuitively, the parameter \u03c3 g quantifies similarity of client objective functions. Note \u03c3 g = 0 corresponds to the i.i.d. setting.\n\nA common approach to solving (1) in federated settings is FEDAVG (McMahan et al., 2017). At each round of FEDAVG, a subset of clients are selected (typically randomly) and the server broadcasts its global model to each client. In parallel, the clients run SGD on their own loss function, and send the resulting model to the server. The server then updates its global model as the average of these local models. See Algorithm 3 in the appendix for more details.\n\nSuppose that at round t, the server has model x t and samples a set S of clients. Let x t i denote the model of each client i \u2208 S after local training. We rewrite FEDAVG's update as\nx t+1 = 1 |S| i\u2208S x t i = x t \u2212 1 |S| i\u2208S x t \u2212 x t i .\nLet \u2206 t i := x t i \u2212 x t and \u2206 t := ( 1 /|S|) i\u2208S \u2206 t i . Then the server update in FEDAVG is equivalent to applying SGD to the \"pseudo-gradient\" \u2212\u2206 t with learning rate \u03b7 = 1. This formulation makes it clear that other choices of \u03b7 are possible. One could also utilize optimizers other than SGD on the clients, or use an alternative update rule on the server. This family of algorithms, which we refer to collectively as FEDOPT, is formalized in Algorithm 1.\n\nAlgorithm 1 FEDOPT FEDOPT FEDOPT 1: Input: x 0 , CLIENTOPT, SERVEROPT 2: for t = 0, \u00b7 \u00b7 \u00b7 , T \u2212 1 do 3:\n\nSample a subset S of clients 4:\n\nx t i,0 = x t\n\n\n5:\n\nfor each client i \u2208 S in parallel do 6: for k = 0, \u00b7 \u00b7 \u00b7 , K \u2212 1 do 7:\n\nCompute an unbiased estimate g t i,k of \u2207F i (x t i,k )\n\n8:\n\nx t i,k+1 = CLIENTOPT(x t i,k , g t i,k , \u03b7 l , t) 9:\n\u2206 t i = x t i,K \u2212 x t 10: \u2206 t = 1\n|S| i\u2208S \u2206 t i 11:\n\nx t+1 = SERVEROPT(x t , \u2212\u2206 t , \u03b7, t)\n\nIn Algorithm 1, CLIENTOPT and SERVEROPT are gradient-based optimizers with learning rates \u03b7 l and \u03b7 respectively. Intuitively, CLIENTOPT aims to minimize (1) based on each client's local data while SERVEROPT optimizes from a global perspective. FEDOPT naturally allows the use of adaptive optimizers (eg. ADAM, YOGI, etc.), as well as techniques such as server-side momentum (leading to FEDAVGM, proposed by Hsu et al. (2019)). In its most general form, FEDOPT uses a CLIENTOPT whose updates can depend on globally aggregated statistics (e.g. server updates in the previous iterations). We also allow \u03b7 and \u03b7 l to depend on the round t in order to encompass learning rate schedules. While we focus on specific adaptive optimizers in this work, we can in principle use any adaptive optimizer (e.g. AMSGRAD (Reddi et al., 2019), ADABOUND (Luo et al., 2019)).\n\nWhile FEDOPT has intuitive benefits over FEDAVG, it also raises a fundamental question: Can the negative of the average model difference \u2206 t be used as a pseudo-gradient in general server optimizer updates? In this paper, we provide an affirmative answer to this question by establishing a theoretical basis for FEDOPT. We will show that the use of the term SERVEROPT is justified, as we can guarantee convergence across a wide variety of server optimizers, including ADAGRAD, ADAM, and YOGI, thus developing principled adaptive optimizers for FL based on our framework.\n\n\nADAPTIVE FEDERATED OPTIMIZATION\n\nIn this section, we specialize FEDOPT to settings where SERVEROPT is an adaptive optimization method (one of ADAGRAD, YOGI or ADAM) and CLIENTOPT is SGD. By using adaptive methods (which generally require maintaining state) on the server and SGD on the clients, we ensure our methods have the same communication cost as FEDAVG and work in cross-device settings.\n\nAlgorithm 2 provides pseudo-code for our methods. An alternate version using batched data and example-based weighting (as opposed to uniform weighting) of clients is given in Algorithm 5. The parameter \u03c4 controls the algorithms' degree of adaptivity, with smaller values of \u03c4 representing higher degrees of adaptivity. Note that the server updates of our methods are invariant to fixed multiplicative changes to the client learning rate \u03b7 l for appropriately chosen \u03c4 , though as we shall see shortly, we will require \u03b7 l to be sufficiently small in our analysis.\n\nAlgorithm 2 FEDADAGRAD FEDADAGRAD FEDADAGRAD , FEDYOGI FEDYOGI FEDYOGI , and FEDADAM FEDADAM FEDADAM\n1: Initialization: x 0 , v \u22121 \u2265 \u03c4 2 , decay parameters \u03b2 1 , \u03b2 2 \u2208 [0, 1) 2: for t = 0, \u00b7 \u00b7 \u00b7 , T \u2212 1 do 3:\nSample subset S of clients 4:\n\nx t i,0 = x t\n\n\n5:\n\nfor each client i \u2208 S in parallel do 6:\n\nfor k = 0, \u00b7 \u00b7 \u00b7 , K \u2212 1 do 7:\n\nCompute an unbiased estimate g t i,k of \u2207F i (x t i,k )\n\n8:\nx t i,k+1 = x t i,k \u2212 \u03b7 l g t i,k 9: \u2206 t i = x t i,K \u2212 x t 10: \u2206 t = 1 |S| i\u2208S \u2206 t i 11: m t = \u03b2 1 m t\u22121 + (1 \u2212 \u03b2 1 )\u2206 t 12: v t = v t\u22121 + \u2206 2 t (FEDADAGRAD) (FEDADAGRAD) (FEDADAGRAD) 13: v t = v t\u22121 \u2212 (1 \u2212 \u03b2 2 )\u2206 2 t sign(v t\u22121 \u2212 \u2206 2 t ) (FEDYOGI) (FEDYOGI) (FEDYOGI) 14: v t = \u03b2 2 v t\u22121 + (1 \u2212 \u03b2 2 )\u2206 2 t (FEDADAM) (FEDADAM) (FEDADAM)\n15:\nx t+1 = x t + \u03b7 mt \u221a vt+\u03c4\nWe provide convergence analyses of these methods in general nonconvex settings, assuming full participation, i.e. S = [m]. For expository purposes, we assume \u03b2 1 = 0, though our analysis can be directly extended to \u03b2 1 > 0. Our analysis can also be extended to partial participation (i.e. |S| < m, see Appendix A.2.1 for details). Furthermore, non-uniform weighted averaging typically used in FEDAVG (McMahan et al., 2017) can also be incorporated into our analysis fairly easily. Theorem 1. Let Assumptions 1 to 3 hold, and let L, G, \u03c3 l , \u03c3 g be as defined therein. Let \u03c3 2 = \u03c3 2 l + 6K\u03c3 2 g . Consider the following conditions for \u03b7 l :\n(Condition I) \u03b7 l \u2264 1 16K min 1 L , 1 T 1/6 \u03c4 120L 2 G 1/3 , (Condition II) \u03b7 l \u2264 1 16K min \u03c4 \u03b7L 2G 2 , \u03c4 4L\u03b7 , 1 T 1/4 \u03c4 2 GL\u03b7 1/2 .\nPublished as a conference paper at ICLR 2021\n\nThen the iterates of Algorithm 2 for FEDADAGRAD satisfy under Condition I only, min\n0\u2264t\u2264T \u22121 E \u2207f (x t ) 2 \u2264 O G \u221a T + \u03c4 \u03b7 l KT (\u03a8 + \u03a8 var ) , under both Condition I & II, min 0\u2264t\u2264T \u22121 E \u2207f (x t ) 2 \u2264 O G \u221a T + \u03c4 \u03b7 l KT \u03a8 + \u03a8 var .\nHere, we define\n\u03a8 = f (x 0 ) \u2212 f (x * ) \u03b7 + 5\u03b7 3 l K 2 L 2 T 2\u03c4 \u03c3 2 , \u03a8 var = d(\u03b7 l KG 2 + \u03c4 \u03b7L) \u03c4 1 + log \u03c4 2 + \u03b7 2 l K 2 G 2 T \u03c4 2 , \u03a8 var = 2\u03b7 l KG 2 + \u03c4 \u03b7L \u03c4 2 2\u03b7 2 l KT m \u03c3 2 l + 10\u03b7 4 l K 3 L 2 T \u03c3 2 .\nAll proofs are relegated to Appendix A due to space constraints. When \u03b7 l satisfies the condition in the second part the above result, we obtain a convergence rate depending on min{\u03a8 var , \u03a8 var }. To obtain an explicit dependence on T and K, we simplify the above result for a specific choice of \u03b7, \u03b7 l and \u03c4 . Corollary 1. Suppose \u03b7 l is such that the conditions in Theorem 1 are satisfied and \u03b7 l = \u0398( 1 /(KL \u221a T ). Also suppose \u03b7 = \u0398( \u221a Km) and \u03c4 = G /L. Then, for sufficiently large T , the iterates of Algorithm 2 for FEDADAGRAD satisfy\nmin 0\u2264t\u2264T \u22121 E \u2207f (x t ) 2 = O f (x 0 ) \u2212 f (x * ) \u221a mKT + 2\u03c3 2 l L G 2 \u221a mKT + \u03c3 2 GKT + \u03c3 2 L \u221a m G 2 \u221a KT 3/2 .\nWe defer a detailed discussion about our analysis and its implication to the end of the section.\n\nAnalysis of FEDADAM Next, we provide the convergence analysis of FEDADAM. The proof of FEDYOGI is very similar and hence, we omit the details of FEDYOGI's analysis. Theorem 2. Let Assumptions 1 to 3 hold, and L, G, \u03c3 l , \u03c3 g be as defined therein. Let \u03c3 2 = \u03c3 2 l +6K\u03c3 2 g . Suppose the client learning rate satisfies \u03b7 l \u2264 1 /16LK and\n\u03b7 l \u2264 1 16K min \u03c4 120L 2 G 1/3 , \u03c4 2(2G + \u03b7L)\n.\n\nThen the iterates of Algorithm 2 for FEDADAM satisfy\nmin 0\u2264t\u2264T \u22121 E \u2207f (x t ) 2 = O \u221a \u03b2 2 \u03b7 l KG + \u03c4 \u03b7 l KT (\u03a8 + \u03a8 var ) , where \u03a8 = f (x 0 ) \u2212 f (x * ) \u03b7 + 5\u03b7 3 l K 2 L 2 T 2\u03c4 \u03c3 2 , \u03a8 var = G + \u03b7L 2 4\u03b7 2 l KT m\u03c4 2 \u03c3 2 l + 20\u03b7 4 l K 3 L 2 T \u03c4 2 \u03c3 2 .\nSimilar to the FEDADAGRAD case, we restate the above result for a specific choice of \u03b7 l , \u03b7 and \u03c4 in order to highlight the dependence of K and T . Corollary 2. Suppose \u03b7 l is chosen such that the conditions in Theorem 2 are satisfied and that \u03b7 l = \u0398( 1 /(KL \u221a T )). Also, suppose \u03b7 = \u0398( \u221a Km) and \u03c4 = G /L. Then, for sufficiently large T , the iterates of Algorithm 2 for FEDADAM satisfy\nmin 0\u2264t\u2264T \u22121 E \u2207f (x t ) 2 = O f (x 0 ) \u2212 f (x * ) \u221a mKT + 2\u03c3 2 l L G 2 \u221a mKT + \u03c3 2 GKT + \u03c3 2 L \u221a m G 2 \u221a KT 3/2 .\nRemark 1. The server learning rate \u03b7 = 1 typically used in FEDAVG does not necessarily minimize the upper bound in Theorems 1 & 2. The effect of \u03c3 g , a measure of client heterogeneity, on convergence can be reduced by choosing sufficiently \u03b7 l and a reasonably large \u03b7 (e.g. see Corollary 1). Thus, the effect of client heterogeneity can be reduced by carefully choosing client and server learning rates, but not removed entirely. Our empirical analysis (eg. Figure 1) supports this conclusion.\n\nDiscussion. We briefly discuss our theoretical analysis and its implications in the FL setting. The convergence rates for FEDADAGRAD and FEDADAM are similar, so our discussion applies to all the adaptive federated optimization algorithms (including FEDYOGI) proposed in the paper.\n\n(i) Comparison of convergence rates. When T is sufficiently large compared to K, O( 1 / \u221a mKT ) is the dominant term in Corollary 1 & 2. Thus, we effectively obtain a convergence rate of O( 1 / \u221a mKT ), which matches the best known rate for the general non-convex setting of our interest (e.g. see (Karimireddy et al., 2019)). We also note that in the i.i.d setting considered in (Wang & Joshi, 2018), which corresponds to \u03c3 g = 0, we match their convergence rates. Similar to the centralized setting, it is possible to obtain convergence rates with better dependence on constants for federated adaptive methods, compared to FEDAVG, by incorporating non-uniform bounds on gradients across coordinates (Zaheer et al., 2018).\n\n(ii) Learning rates & their decay. The client learning rate of 1 / \u221a T in our analysis requires knowledge of the number of rounds T a priori; however, it is easy to generalize our analysis to the case where \u03b7 l is decayed at a rate of 1 / \u221a t. Observe that one must decay \u03b7 l , not the server learning rate \u03b7, to obtain convergence. This is because the client drift introduced by the local updates does not vanish as T \u2192 \u221e when \u03b7 l is constant. As we show in Appendix E.6, learning rate decay can improve empirical performance. Also, note the inverse relationship between \u03b7 l and \u03b7 in Corollary 1 & 2, which we observe in our empirical analysis (see Appendix E.4).\n\n(iii) Communication efficiency & local steps. The total communication cost of the algorithms depends on the number of communication rounds T . From Corollary 1 & 2, it is clear that a larger K leads to fewer rounds of communication as long as K = O(T \u03c3 2 l /\u03c3 2 g ). Thus, the number of local iterations can be large when either the ratio \u03c3 2 l /\u03c3 2 g or T is large. In the i.i.d setting where \u03c3 g = 0, unsurprisingly, K can be very large.\n\n(iv) Client heterogeneity. While careful selection of client and server learning rates can reduce the effect of client heterogeneity (see Remark 1), it does not completely remove it. In highly heterogeneous settings, it may be necessary to use mechanisms such as control variates (Karimireddy et al., 2019). However, our empirical analysis suggest that for moderate, naturally arising heterogeneity, adaptive optimizers are quite effective, especially in cross-device settings (see Figure 1). Furthermore, our algorithms can be directly combined with such mechanisms.\n\nAs mentioned earlier, for the sake of simplicity, our analysis assumes full-participation (S = [m]). Our analysis can be directly generalized to limited participation at the cost of an additional variance term in our rates that depends on |S|/m, the fraction of clients sampled (see Section A.2.1 for details).\n\n\nEXPERIMENTAL EVALUATION: DATASETS, TASKS, AND METHODS\n\nWe evaluate our algorithms on what we believe is the most extensive and representative suite of federated datasets and modeling tasks to date. We wish to understand how server adaptivity can help improve convergence, especially in cross-device settings. To accomplish this, we conduct simulations on seven diverse and representative learning tasks across five datasets. Notably, three of the five have a naturally-arising client partitioning, highly representative of real-world FL problems. Implementation We implement all algorithms in TensorFlow Federated (Ingerman & Ostrowski, 2019), and provide an open-source implementation of all optimizers and benchmark tasks 1 . Clients are sampled uniformly at random, without replacement in a given round, but with replacement across rounds. Our implementation has two important characteristics. First, instead of doing K training steps per client, we do E epochs of training over each client's dataset. Second, to account for varying numbers of gradient steps per client, we weight the average of the client outputs \u2206 t i by each client's number of training samples. This follows the approach of (McMahan et al., 2017), and can often outperform uniform weighting (Zaheer et al., 2018). For full descriptions of the algorithms used, see Appendix B.\n\nOptimizers and hyperparameters We compare FEDADAGRAD, FEDADAM, and FEDYOGI (with adaptivity \u03c4 ) to FEDOPT where CLIENTOPT and SERVEROPT are SGD with learning rates \u03b7 l and \u03b7. For the server, we use a momentum parameter of 0 (FEDAVG), and 0.9 (FEDAVGM). We fix the client batch size on a per-task level (see Appendix D.3). For FEDADAM and FEDYOGI, we fix a momentum parameter \u03b2 1 = 0.9 and a second moment parameter \u03b2 2 = 0.99. We also compare to SCAFFOLD (see Appendix B.2 for implementation details). For SO NWP, we sample 50 clients per round, while for all other tasks we sample 10. We use E = 1 local epochs throughout.\n\nWe select \u03b7 l , \u03b7, and \u03c4 by grid-search tuning. While this is often done using validation data in centralized settings, such data is often inaccessible in FL, especially cross-device FL. Therefore, we tune by selecting the parameters that minimize the average training loss over the last 100 rounds of training. We run 1500 rounds of training on the EMNIST CR, Shakespeare, and Stack Overflow tasks, 3000 rounds for EMNIST AE, and 4000 rounds for the CIFAR tasks. For more details and a record of the best hyperparameters, see Appendix D.\n\nValidation metrics For all tasks, we measure the performance on a validation set throughout training. For Stack Overflow tasks, the validation set contains 10,000 randomly sampled test examples (due to the size of the test dataset, see Table 2). For all other tasks, we use the entire test set. Since all algorithms exchange equal-sized objects between server and clients, we use the number of communication rounds as a proxy for wall-clock training time.\n\n\nEXPERIMENTAL EVALUATION: RESULTS\n\n\nCOMPARISONS BETWEEN METHODS\n\nWe compare the convergence of our adaptive methods to non-adaptive methods: FEDAVG, FEDAVGM and SCAFFOLD. Plots of validation performances for each task/optimizer are in Figure 1, and Sparse-gradient tasks Text data often produces long-tailed feature distributions, leading to approximately-sparse gradients which adaptive optimizers can capitalize on (Zhang et al., 2019a). Both Stack Overflow tasks exhibit such behavior, though they are otherwise dramatically differentin feature representation (bag-of-words vs. variable-length token sequence), model architecture (GLM vs deep network), and optimization landscape (convex vs nonconvex). In both tasks, words that do not appear in a client's dataset produce nearzero client updates. Thus, the accumulators v t,j in Algorithm 2 remain small for parameters tied to rare words, allowing large updates to be made when they do occur. This intuition is born out in Figure 1, where adaptive optimizers dramatically outperform non-adaptive ones. For the non-convex NWP task, momentum is also critical, whereas it slightly hinders performance for the convex LR task. Dense-gradient tasks CIFAR-10/100, EMNIST AE/CR, and Shakespeare lack a sparse-gradient structure. Shakespeare is relatively easy-most optimizers perform well after enough rounds once suitably tuned, though FEDADAGRAD converges faster. For CIFAR-10/100 and EMNIST AE, adaptivity and momentum offer substantial improvements over FEDAVG. Moreover, FEDYOGI and FEDADAM have faster initial convergence than FEDAVGM on these tasks. Notably, FEDADAM and FEDYOGI perform comparably to or better than non-adaptive optimizers throughout, and close to or better than FEDADAGRAD throughout. As we discuss below, FEDADAM and FEDYOGI actually enable easier learning rate tuning than FEDAVGM in many tasks.\n\nComparison to SCAFFOLD On all tasks, SCAFFOLD performs comparably to or worse than FEDAVG and our adaptive methods. On Stack Overflow, SCAFFOLD and FEDAVG are nearly identical. This is because the number of clients (342,477) makes it unlikely we sample any client more than once. Intuitively, SCAFFOLD does not have a chance to use its client control variates. In other tasks, SCAFFOLD performs worse than other methods. We present two possible explanations: First, we only sample a small fraction of clients at each round, so most users are sampled infrequently. Intuitively, the client control variates can become stale, and may consequently degrade the performance. Second, SCAFFOLD is similar to variance reduction methods such as SVRG (Johnson & Zhang, 2013). While theoretically performant, such methods often perform worse than SGD in practice (Defazio & Bottou, 2018). As shown by Defazio et al. (2014), variance reduction often only accelerates convergence when close to a critical point. In cross-device settings (where the number of communication rounds are limited), SCAFFOLD may actually reduce empirical performance.  Obtaining optimal performance involves tuning \u03b7 l , \u03b7, and for the adaptive methods, \u03c4 . To quantify how easy it is to tune various methods, we plot their validation performance as a function of \u03b7 l and \u03b7. This leads to a natural question: Is the reduction in the need to tune \u03b7 l and \u03b7 offset by the need to tune the adaptivity \u03c4 ? In fact, while we tune \u03c4 in Figure 1, our results are relatively robust to \u03c4 . To demonstrate, we plot the best validation performance for various \u03c4 in Figure 3. For nearly all tasks and optimizers, \u03c4 = 10 \u22123 works almost as well all other values. This aligns with work by Zaheer et al. (2018), who show that moderately large \u03c4 yield better performance for centralized adaptive optimizers. FEDADAM and FEDYOGI see only small differences in performance among \u03c4 on all tasks except Stack Overflow LR (for which FEDADAGRAD is the best optimizer, and is robust to \u03c4 ).\n\n\nEASE OF TUNING\n\n\nOTHER FINDINGS\n\nWe present additional empirical analyses in Appendix E. These include EMNIST CR results (Appendix E.1), Stack Overflow results on the full test dataset (Appendix E.2), client/server learning rate heat maps for all optimizers and tasks (Appendix E.3), an analysis of the relationship between \u03b7 and \u03b7 l (Appendix E.4), and experiments with learning rate decay (Appendix E.6).\n\n\nCONCLUSION\n\nIn this paper, we demonstrated that adaptive optimizers can be powerful tools in improving the convergence of FL. By using a simple client/server optimizer framework, we can incorporate adaptivity into FL in a principled, intuitive, and theoretically-justified manner. We also developed comprehensive benchmarks for comparing federated optimization algorithms. To encourage reproducibility and breadth of comparison, we have attempted to describe our experiments as rigorously as possible, and have created an open-source framework with all models, datasets, and code. We believe our work raises many important questions about how best to perform federated optimization. Example directions for future research include understanding how the use of adaptivity affects differential privacy and fairness.\n\nKevin Hsieh, Amar Phanishayee, Onur Mutlu, and Phillip B Gibbons. The non-IID data quagmire of decentralized machine learning. arXiv preprint arXiv:1910.00189, 2019. \n\n\nA PROOF OF RESULTS\n\n\nA.1 MAIN CHALLENGES\n\nWe first recap some of the central challenges to our analysis. Theoretical analyses of optimization methods for federated learning are much different than analyses for centralized settings. The key factors complicating the analysis are:\n\n1. Clients performing multiple local updates.\n\n2. Data heterogeneity.\n\n3. Understanding the communication complexity.\n\nAs a result of (1), the updates from the clients to the server are not gradients, or even unbiased estimates of gradients, they are pseudo-gradients (see Section 2). These pseudo-gradients are challenging to analyze as they can have both high bias (their expectation is not the gradient of the empirical loss function) and high variance (due to compounding variance across client updates) and are therefore challenging to bound. This is exacerbated by (2), which we quantify by the parameter \u03c3 g in Section 2. Things are further complicated by (3), as we must obtain a good trade-off between the number of client updates taken per round (K in Algorithms 1 and 2) and the number of communication rounds T . Such trade-offs do not exist in centralized optimization.\n\n\nA.2 PROOF OF THEOREM 1\n\nProof of Theorem 1. Recall that the server update of FEDADAGRAD is the following\nx t+1,i = x t,i + \u03b7 \u2206 t,i \u221a v t,i + \u03c4 , for all i \u2208 [d].\nSince the function f is L-smooth, we have the following:\nf (x t+1 ) \u2264 f (x t ) + \u2207f (x t ), x t+1 \u2212 x t + L 2 x t+1 \u2212 x t 2 = f (x t ) + \u03b7 \u2207f (x t ), \u2206 t \u221a v t + \u03c4 + \u03b7 2 L 2 d i=1 \u2206 2 t,i ( \u221a v t,i + \u03c4 ) 2(2)\nThe second step follows simply from FEDADAGRAD's update. We take the expectation of f (x t+1 ) (over randomness at time step t) in the above inequality:\nE t [f (x t+1 )] \u2264 f (x t ) + \u03b7 \u2207f (x t ), E t \u2206 t \u221a v t + \u03c4 + \u03b7 2 L 2 d i=1 E t \u2206 2 t,i ( \u221a v t,i + \u03c4 ) 2 = f (x t ) + \u03b7 \u2207f (x t ), E t \u2206 t \u221a v t + \u03c4 \u2212 \u2206 t \u221a v t\u22121 + \u03c4 + \u2206 t \u221a v t\u22121 + \u03c4 + \u03b7 2 L 2 d j=1 E t \u2206 2 t,j ( \u221a v t,j + \u03c4 ) 2 = f (x t ) + \u03b7 \u2207f (x t ), E t \u2206 t \u221a v t\u22121 + \u03c4 T1 +\u03b7 \u2207f (x t ), E t \u2206 t \u221a v t + \u03c4 \u2212 \u2206 t \u221a v t\u22121 + \u03c4 T2 + \u03b7 2 L 2 d j=1 E t \u2206 2 t,j ( \u221a v t,j + \u03c4 ) 2(3)\nPublished as a conference paper at ICLR 2021\n\nWe will first bound T 2 in the following manner:\nT 2 = \u2207f (x t ), E t \u2206 t \u221a v t + \u03c4 \u2212 \u2206 t \u221a v t\u22121 + \u03c4 = E t d j=1 [\u2207f (x t )] j \u00d7 \u2206 t,j \u221a v t,j + \u03c4 \u2212 \u2206 t,j \u221a v t\u22121,j + \u03c4 = E t d j=1 [\u2207f (x t )] j \u00d7 \u2206 t,j \u00d7 \u221a v t\u22121,j \u2212 \u221a v t,j ( \u221a v t,j + \u03c4 )( \u221a v t\u22121,j + \u03c4 ) , and recalling v t = v t\u22121 + \u2206 2 t so \u2212\u2206 2 t,j = ( \u221a v t\u22121,j \u2212 \u221a v t,j )( \u221a v t\u22121,j + \u221a v t,j )) we have, = E t d j=1 [\u2207f (x t )] j \u00d7 \u2206 t,j \u00d7 \u2212\u2206 2 t,j ( \u221a v t,j + \u03c4 )( \u221a v t\u22121,j + \u03c4 )( \u221a v t\u22121,j + \u221a v t,j ) \u2264 E t d j=1 |\u2207f (x t )] j | \u00d7 |\u2206 t,j | \u00d7 \u2206 2 t,j ( \u221a v t,j + \u03c4 )( \u221a v t\u22121,j + \u03c4 )( \u221a v t\u22121,j + \u221a v t,j ) \u2264 E t d j=1 |\u2207f (x t )] j | \u00d7 |\u2206 t,j | \u00d7 \u2206 2 t,j (v t,j + \u03c4 2 )( \u221a v t\u22121,j + \u03c4 ) since v t\u22121,j \u2265 \u03c4 2 .\nHere v t\u22121,j \u2265 \u03c4 since v \u22121 \u2265 \u03c4 (see the initialization of Algorithm 2) and v t,j is increasing in t. The above bound can be further upper bounded in the following manner:\nT 2 \u2264 E t d j=1 \u03b7 l KG 2 \u2206 2 t,j (v t,j + \u03c4 2 )( \u221a v t\u22121,j + \u03c4 ) since [\u2207f (x t )] i \u2264 G and \u2206 t,i \u2264 \u03b7 l KG \u2264 E t d j=1 \u03b7 l KG 2 \u03c4 \u2206 2 t,j t l=0 \u2206 2 l,j + \u03c4 2 since \u221a v t\u22121,j \u2265 0.(4)\nBounding T 1 We now turn our attention to bounding the term T 1 , which we need to be sufficiently negative. We observe the following:\nT 1 = \u2207f (x t ), E t \u2206 t \u221a v t\u22121 + \u03c4 = \u2207f (x t ) \u221a v t\u22121 + \u03c4 , E t [\u2206 t \u2212 \u03b7 l K\u2207f (x t ) + \u03b7 l K\u2207f (x t )] = \u2212\u03b7 l K d j=1 [\u2207f (x t )] 2 j \u221a v t\u22121,j + \u03c4 + \u2207f (x t ) \u221a v t\u22121 + \u03c4 , E t [\u2206 t + \u03b7 l K\u2207f (x t )] T3 .(5)\nIn order to bound T 1 , we use the following upper bound on T 3 (which captures the difference between the actual update \u2206 t and an appropriate scaling of \u2212\u2207f (x t )):\nT 3 = \u2207f (x t ) \u221a v t\u22121 + \u03c4 , E t [\u2206 t + \u03b7 l K\u2207f (x t )] = \u2207f (x t ) \u221a v t\u22121 + \u03c4 , E t \u2212 1 m m i=1 K\u22121 k=0 \u03b7 l g t i,k + \u03b7 l K\u2207f (x t ) = \u2207f (x t ) \u221a v t\u22121 + \u03c4 , E t \u2212 1 m m i=1 K\u22121 k=0 \u03b7 l \u2207F i (x t i,k ) + \u03b7 l K\u2207f (x t ) . m m i=1 \u2207F i (x t )\nand g t i,k is an unbiased estimator of the gradient at x t i,k , we further bound T 3 as follows using a simple application of the fact that ab \u2264 (a 2 + b 2 )/2. :\nT 3 \u2264 \u03b7 l K 2 d j=1 [\u2207f (x t )] 2 j \u221a v t\u22121,j + \u03c4 + \u03b7 l 2K E t \uf8ee \uf8f0 1 m m i=1 K\u22121 k=0 \u2207F i (x t i,k ) \u221a v t\u22121 + \u03c4 \u2212 1 m m i=1 K\u22121 k=0 \u2207F i (x t ) \u221a v t\u22121 + \u03c4 2 \uf8f9 \uf8fb \u2264 \u03b7 l K 2 d j=1 [\u2207f (x t )] 2 j \u221a v t\u22121,j + \u03c4 + \u03b7 l 2m E t \uf8ee \uf8f0 m i=1 K\u22121 k=0 \u2207F i (x t i,k ) \u2212 \u2207F i (x t ) \u221a v t\u22121 + \u03c4 2 \uf8f9 \uf8fb \u2264 \u03b7 l K 2 d j=1 [\u2207f (x t )] 2 j \u221a v t\u22121,j + \u03c4 + \u03b7 l L 2 2m\u03c4 E t m i=1 K\u22121 k=0 x t i,k \u2212 x t 2\nusing Assumption 1 and v t\u22121 \u2265 0.\n\n(6) The second inequality follows from Lemma 6. The last inequality follows from L-Lipschitz nature of the gradient (Assumption 1). We now prove a lemma that bounds the \"drift\" of the x t i,k from x t : Lemma 3. For any step-size satisfying \u03b7 l \u2264 1 8LK , we can bound the drift for any k \u2208 {0, \u00b7 \u00b7 \u00b7 , K \u22121}\nas 1 m m i=1 E x t i,k \u2212 x t 2 \u2264 5K\u03b7 2 l E d j=1 (\u03c3 2 l,j + 2K\u03c3 2 g,j ) + 30K 2 \u03b7 2 l E[ \u2207f (x t ))) 2 .(7)\nProof. The result trivially holds for\nk = 1 since x t i,0 = x t for all i \u2208 [m]\n. We now turn our attention to the case where k \u2265 1. To prove the above result, we observe that for any client\ni \u2208 [m] and k \u2208 [K], E x t i,k \u2212 x t 2 = E x t i,k\u22121 \u2212 x t \u2212 \u03b7 l g t i,k\u22121 2 \u2264 E x t i,k\u22121 \u2212 x t \u2212 \u03b7 l (g t i,k\u22121 \u2212 \u2207F i (x t i,k\u22121 ) + \u2207F i (x t i,k\u22121 ) \u2212 \u2207F i (x t ) + \u2207F i (x t ) \u2212 \u2207f (x t ) + \u2207f (x t )) 2 \u2264 1 + 1 2K \u2212 1 E x t i,k\u22121 \u2212 x t 2 + E \u03b7 l (g t i,k\u22121 \u2212 \u2207F i (x t i,k\u22121 )) 2 + 6KE[ \u03b7 l (\u2207F i (x t i,k\u22121 ) \u2212 \u2207F i (x t )) 2 + 6KE[ \u03b7 l (\u2207F i (x t ) \u2212 \u2207f (x t )) 2 + 6KE[ \u03b7 l \u2207f (x t ))) 2\nThe first inequality uses the fact that g t k\u22121,i is an unbiased estimator of \u2207F i (x t i,k\u22121 ) and Lemma 7. The above quantity can be further bounded by the following:\nE x t i,k \u2212 x t 2 \u2264 1 + 1 2K \u2212 1 E x t i,k\u22121 \u2212 x t 2 + \u03b7 2 l E d j=1 \u03c3 2 l,j + 6K\u03b7 2 l E L(x t i,k\u22121 \u2212 x t ) 2 + 6KE[ \u03b7 l (\u2207F i (x t ) \u2212 \u2207f (x t )) 2 + 6KE[ \u03b7 l \u2207f (x t ))) 2 = 1 + 1 2K \u2212 1 + 6K\u03b7 2 l L 2 E (x t i,k\u22121 \u2212 x t ) 2 + \u03b7 2 l E d j=1 \u03c3 2 l,j + 6KE[ \u03b7 l (\u2207F i (x t ) \u2212 \u2207f (x t )) 2 + 6K\u03b7 2 l E[ \u2207f (x t ))) 2\nHere, the first inequality follows from Assumption 1 and 2. Averaging over the clients i, we obtain the following:\n1 m m i=1 E x t i,k \u2212 x t 2 \u2264 1 + 1 2K \u2212 1 + 6K\u03b7 2 l L 2 1 m m i=1 E x t i,k\u22121 \u2212 x t 2 + \u03b7 2 l E d j=1 \u03c3 2 l,j + 6K m m i=1 E[ \u03b7 l (\u2207F i (x t ) \u2212 \u2207f (x t )) 2 + 6K\u03b7 2 l E[ \u2207f (x t ))) 2 \u2264 1 + 1 2K \u2212 1 + 6K\u03b7 2 l L 2 1 m m i=1 E x t i,k\u22121 \u2212 x t 2 + \u03b7 2 l E d j=1 (\u03c3 2 l,j + 6K\u03c3 2 g,j ) + 6K\u03b7 2 l E[ \u2207f (x t ))) 2\nFrom the above, we get the following inequality:\n1 m m i=1 E x t i,k \u2212 x t 2 \u2264 1 + 1 K \u2212 1 1 m m i=1 E x t i,k\u22121 \u2212 x t 2 + \u03b7 2 l E d j=1 (\u03c3 2 l,j + 6K\u03c3 2 g,j ) + 6K\u03b7 2 l E[ \u2207f (x t ))\n) 2 Unrolling the recursion, we obtain the following:\n1 m m i=1 E x t i,k \u2212 x t 2 \u2264 k\u22121 p=0 1 + 1 K \u2212 1 p \uf8ee \uf8f0 \u03b7 2 l E d j=1 (\u03c3 2 l,j + 6K\u03c3 2 g,j ) + 6K\u03b7 2 l E[ \u2207f (x t ))) 2 \uf8f9 \uf8fb \u2264 (K \u2212 1) \u00d7 1 + 1 K \u2212 1 K \u2212 1 \u00d7 \uf8ee \uf8f0 \u03b7 2 l E d j=1 (\u03c3 2 l,j + 6K\u03c3 2 g,j ) + 6K\u03b7 2 l E[ \u2207f (x t ))) 2 \uf8f9 \uf8fb \u2264 \uf8ee \uf8f0 5K\u03b7 2 l E d j=1 (\u03c3 2 l,j + 6K\u03c3 2 g,j ) + 30K 2 \u03b7 2 l E[ \u2207f (x t ))) 2 \uf8f9 \uf8fb ,\nconcluding the proof of Lemma 3. The last inequality uses the fact that (1 + 1 K\u22121 ) K \u2264 5 for K > 1.\n\nUsing the above lemma in Equation 6 and Condition I, we can bound T 3 in the following manner:\nT 3 \u2264 \u03b7 l K 2 d j=1 [\u2207f (x t )] 2 j \u221a v t\u22121,j + \u03c4 + \u03b7 l L 2 2m\u03c4 E t \uf8ee \uf8f0 m i=1 K\u22121 k=0 d j=1 ([x t i,k ] j \u2212 [x t ] j ) 2 \uf8f9 \uf8fb \u2264 \u03b7 l K 2 d j=1 [\u2207f (x t )] 2 j \u221a v t\u22121,j + \u03c4 + \u03b7 l KL 2 2\u03c4 \uf8ee \uf8f0 5K\u03b7 2 l E d j=1 (\u03c3 2 l,j + 6K\u03c3 2 g,j ) + 30K 2 \u03b7 2 l E[ \u2207f (x t ))) 2 \uf8f9 \uf8fb \u2264 3\u03b7 l K 4 d j=1 [\u2207f (x t )] 2 j \u221a v t\u22121,j + \u03c4 + 5\u03b7 3 l K 2 L 2 2\u03c4 E d j=1 (\u03c3 2 l,j + 6K\u03c3 2 g,j )\nHere we used the fact that \u221a v t\u22121,j \u2264 \u03b7 l KG \u221a T and Condition I in Theorem 1. Using the above bound in Equation 5, we get\nT 1 \u2264 \u2212 \u03b7 l K 4 d j=1 [\u2207f (x t )] 2 j \u221a v t\u22121,j + \u03c4 + 5\u03b7 3 l K 2 L 2 2\u03c4 E d j=1 (\u03c3 2 l,j + 6K\u03c3 2 g,j )(8)\nPutting the pieces together Substituting in Equation (3), bounds T 1 in Equation (8) and bound T 2 in Equation (4), we obtain\nE t [f (x t+1 )] \u2264 f (x t ) + \u03b7 \u00d7 \uf8ee \uf8f0 \u2212 \u03b7 l K 4 d j=1 [\u2207f (x t )] 2 j \u221a v t\u22121,j + \u03c4 + 5\u03b7 3 l K 2 L 2 2\u03c4 E d j=1 (\u03c3 2 l,j + 6K\u03c3 2 g,j ) \uf8f9 \uf8fb + \u03b7 \u00d7 E d j=1 \u03b7 l KG 2 \u03c4 \u2206 2 t,j t l=0 \u2206 2 l,j + \u03c4 2 + \u03b7 2 2 d j=1 LE \u2206 2 t,j t l=0 \u2206 2 l,j + \u03c4 2 .(9)\nRearranging the above inequality and summing it from t = 0 to T \u2212 1, we get\nT \u22121 t=0 \u03b7 l K 4 d j=1 E [\u2207f (x t )] 2 j \u221a v t\u22121,j + \u03c4 \u2264 f (x 0 ) \u2212 E[f (x T )] \u03b7 + 5\u03b7 3 l K 2 L 2 T 2\u03c4 d j=1 (\u03c3 2 l,j + 6K\u03c3 2 g,j ) + T \u22121 t=0 E d j=1 \u03b7 l KG 2 \u03c4 + \u03b7L 2 \u00d7 \u2206 2 t,j t l=0 \u2206 2 l,j + \u03c4 2(10)\nThe first inequality uses simple telescoping sum. For completing the proof, we need the following result.\n\nLemma 4. The following upper bound holds for Algorithm 2 (FEDADAGRAD):\nE T \u22121 t=0 d j=1 \u2206 2 t,j t l=0 \u2206 2 l,j + \u03c4 2 \u2264 min d + d j=1 log 1 + \u03b7 2 l K 2 G 2 T \u03c4 2 , 4\u03b7 2 l KT m\u03c4 2 d j=1 \u03c3 2 l,j + 20\u03b7 4 l K 3 L 2 T d j=1 (\u03c3 2 l,j + 6K\u03c3 2 g,j ) \u03c4 2 + 40\u03b7 4 l K 2 L 2 + 2\u03b7 2 l K 2 \u03c4 2 T \u22121 t=0 E \u2207f (x t ) 2\nProof. We bound the desired quantity in the following manner:\nE T \u22121 t=0 d j=1 \u2206 2 t,j t l=0 \u2206 2 l,j + \u03c4 2 \u2264 d + E d j=1 log 1 + T \u22121 l=0 \u2206 2 l,j \u03c4 2 \u2264 d + d j=1 log 1 + \u03b7 2 l K 2 G 2 T \u03c4 2 .\nAn alternate way of the bounding this quantity is as follows:\nE T \u22121 t=0 d j=1 \u2206 2 t,j t l=0 \u2206 2 t,j + \u03c4 2 \u2264 E T \u22121 t=0 d j=1 \u2206 2 t,j \u03c4 2 \u2264 E T \u22121 t=0 \u2206 t + \u03b7 l K\u2207f (x t ) \u2212 \u03b7 l K\u2207f (x t ) \u03c4 2 \u2264 2E T \u22121 t=0 \u2206 t + \u03b7 l K\u2207f (x t ) \u03c4 2 + \u03b7 2 l K 2 \u2207f (x t ) \u03c4 2 .(11)\nThe first quantity in the above bound can be further bounded as follows:\n2E T \u22121 t=0 1 \u03c4 \u00b7 \u2212 1 m m i=1 K\u22121 k=0 \u03b7 l g t i,k + \u03b7 l K\u2207f (x t ) 2 = 2E T \u22121 t=0 1 \u03c4 \u00b7 1 m m i=1 K\u22121 k=0 \u03b7 l g t i,k \u2212 \u03b7 l \u2207F i (x t i,k ) + \u03b7 l \u2207F i (x t i,k ) \u2212 \u03b7 l \u2207F i (x t ) + \u03b7 l \u2207F i (x t ) \u2212 \u03b7 l K\u2207f (x t ) 2 = 2\u03b7 2 l m 2 T \u22121 t=0 E \uf8ee \uf8f0 m i=1 K\u22121 k=0 1 \u03c4 \u00b7 g t i,k \u2212 \u2207F i (x t i,k ) + \u2207F i (x t i,k ) \u2212 \u2207F i (x t ) 2 \uf8f9 \uf8fb \u2264 4\u03b7 2 l m 2 T \u22121 t=0 E \uf8ee \uf8f0 m i=1 K\u22121 k=0 1 \u03c4 \u00b7 g t i,k \u2212 \u2207F i (x t i,k ) 2 + m i=1 K\u22121 k=0 1 \u03c4 \u00b7 \u2207F i (x t i,k ) \u2212 \u2207F i (x t ) 2 \uf8f9 \uf8fb \u2264 4\u03b7 2 l KT m d j=1 \u03c3 2 l,j \u03c4 2 + 4\u03b7 2 l K m E m i=1 K\u22121 k=0 T \u22121 t=0 1 \u03c4 \u00b7 \u2207F i (x t i,k ) \u2212 \u2207F i (x t ) 2 \u2264 4\u03b7 2 l KT m d j=1 \u03c3 2 l,j \u03c4 2 + 4\u03b7 2 l K m E m i=1 K\u22121 k=0 T \u22121 t=0 L \u03c4 \u00b7 x t i,k \u2212 x t 2 (by Assumptions 1 and 2) \u2264 4\u03b7 2 l KT m\u03c4 2 d j=1 \u03c3 2 l,j + 20\u03b7 4 l K 3 L 2 T d j=1 (\u03c3 2 l,j + 6K\u03c3 2 g,j ) \u03c4 2 + 40\u03b7 4 l K 4 L 2 \u03c4 2 T \u22121 t=0 E \u2207f (x t ) 2 (by Lemma 3).\nHere, the first inequality follows from simple application of the fact that ab \u2264 (a 2 + b 2 )/2. The result follows.\n\nSubstituting the above bound in Equation (10), we obtain:\n\u03b7 l K 4 T \u22121 t=0 d j=1 E [\u2207f (x t )] 2 j \u221a v t\u22121,j + \u03c4 \u2264 f (x 0 ) \u2212 E[f (x T )] \u03b7 + 5\u03b7 3 l K 2 L 2 2\u03c4 E T \u22121 t=0 d j=1 (\u03c3 2 l,j + 6K\u03c3 2 g,j ) + \u03b7 l KG 2 \u03c4 + \u03b7L 2 \u00d7 min d + d log 1 + \u03b7 2 l K 2 G 2 T \u03c4 2 , 4\u03b7 2 l KT m\u03c4 2 d j=1 \u03c3 2 l,j + 20\u03b7 4 l K 3 L 2 T d j=1 (\u03c3 2 l,j + 6K\u03c3 2 g,j ) \u03c4 2 + 40\u03b7 4 l K 4 L 2 + 2\u03b7 2 l K 2 \u03c4 2 T \u22121 t=0 E \u2207f (x t ) 2(12)\nNote that under the conditions assumed, we have 40\u03b7 4 l K 4 L 2 \u2264 2\u03b7 2 l K 2 . We observe the following:\nT \u22121 t=0 d j=1 [\u2207Ef (x t )] 2 j \u221a v t\u22121,j + \u03c4 \u2265 T \u22121 t=0 d j=1 E [\u2207f (x t )] 2 j \u03b7 l KG \u221a T + \u03c4 \u2265 T \u03b7 l KG \u221a T + \u03c4 min 0\u2264t\u2264T E \u2207f (x t ) 2 .\nThe second part of Theorem 1 follows from using the above inequality in Equation (12). Note that the first part of Theorem 1 is obtain from the part of Lemma 4.\n\n\nA.2.1 LIMITED PARTICIPATION\n\nFor limited participation, the main changed in the proof is in Equation (11). The rest of the proof is similar so we mainly focus on Equation (11) here. Let S be the sampled set at the t th iteration such that |S| = s. In partial participation, we assume that the set S is sampled uniformly from all subsets of [m] with size s. In this case, for the first term in Equation (11), we have\n2E T \u22121 t=0 1 \u03c4 \u00b7 \u2212 1 |S| i\u2208S K\u22121 k=0 \u03b7 l g t i,k + \u03b7 l K\u2207f (x t ) 2 = 2E T \u22121 t=0 \uf8ee \uf8f0 1 \u03c4 \u00b7 1 s i\u2208S K\u22121 k=0 \u03b7 l g t i,k \u2212 \u03b7 l \u2207F i (x t i,k ) + \u03b7 l \u2207F i (x t i,k ) \u2212 \u03b7 l \u2207F i (x t ) + \u03b7 l \u2207F i (x t ) \u2212 \u03b7 l K\u2207f (x t ) 2 \uf8f9 \uf8fb \u2264 6\u03b7 2 l s 2 T \u22121 t=0 E i\u2208S K\u22121 k=0 1 \u03c4 \u00b7 g t i,k \u2212 \u2207F i (x t i,k ) 2 + i\u2208S K\u22121 k=0 1 \u03c4 \u00b7 \u2207F i (x t i,k ) \u2212 \u2207F i (x t ) 2 + K 2 1 \u03c4 \u00b7 i\u2208S F i (x t ) \u2212 s\u2207f (x t ) 2 \u2264 6\u03b7 2 l KT s d j=1 \u03c3 2 l,j \u03c4 2 + 6\u03b7 2 l K s E i\u2208S K\u22121 k=0 T \u22121 t=0 1 \u03c4 \u00b7 \u2207F i (x t i,k ) \u2212 \u2207F i (x t ) 2 + 6\u03b7 2 l K 2 T \u03c3 2 g \u03c4 2 1 \u2212 s m \u2264 6\u03b7 2 l KT s d j=1 \u03c3 2 l,j \u03c4 2 + 6\u03b7 2 l K s E i\u2208S K\u22121 k=0 T \u22121 t=0 L \u03c4 \u00b7 x t i,k \u2212 x t 2 + 6\u03b7 2 l K 2 T \u03c3 2 g \u03c4 2 1 \u2212 s m \u2264 6\u03b7 2 l KT \u03c4 2 s d j=1 \u03c3 2 l,j + 30\u03b7 4 l K 3 L 2 T E d j=1 (\u03c3 2 l,j + 6K\u03c3 2 g,j ) \u03c4 2 + 60\u03b7 4 l K 4 L 2 \u03c4 2 T \u22121 t=0 E \u2207f (x t ) 2 + 6\u03b7 2 l K 2 T \u03c3 2 g \u03c4 2 1 \u2212 s m .\nNote that the expectation here also includes S. The first inequality is obtained from the fact that (a + b + c) 2 \u2264 3(a 2 + b 2 + c 2 ). The second inequality is obtained from the fact that set S is uniformly sampled from all subsets of [m] with size s. The third and fourth inequalities are similar to the one used in proof of Theorem 1. Substituting the above bound in Equation (10) gives the desired convergence rate.\n\n\nA.3 PROOF OF THEOREM 2\n\nProof of Theorem 2. The proof strategy is similar to that of FEDADAGRAD except that we need to handle the exponential moving average in FEDADAM. We note that the update of FEDADAM is the following\nx t+1 = x t + \u03b7 \u2206 t \u221a v t + \u03c4 , for all i \u2208 [d].\nUsing the L-smooth nature of function f and the above update rule, we have the following:\nf (x t+1 ) \u2264 f (x t ) + \u03b7 \u2207f (x t ), \u2206 t \u221a v t + \u03c4 + \u03b7 2 L 2 d i=1 \u2206 2 t,i ( \u221a v t,i + \u03c4 ) 2(13)\nThe second step follows simply from FEDADAM's update. We take the expectation of f (x t+1 ) (over randomness at time step t) and rewrite the above inequality as:\nE t [f (x t+1 )] \u2264 f (x t ) + \u03b7 \u2207f (x t ), E t \u2206 t \u221a v t + \u03c4 \u2212 \u2206 t \u03b2 2 v t\u22121 + \u03c4 + \u2206 t \u03b2 2 v t\u22121 + \u03c4 + \u03b7 2 L 2 d j=1 E t \u2206 2 t,j ( \u221a v t,j + \u03c4 ) 2 = f (x t ) + \u03b7 \u2207f (x t ), E t \u2206 t \u03b2 2 v t\u22121 + \u03c4 R1 +\u03b7 \u2207f (x t ), E t \u2206 t \u221a v t + \u03c4 \u2212 \u2206 t \u03b2 2 v t\u22121 + \u03c4 R2 + \u03b7 2 L 2 d j=1 E t \u2206 2 t,j ( \u221a v t,j + \u03c4 ) 2(14)\nBounding R 2 . We observe the following about R 2 :\nR 2 = E t d j=1 [\u2207f (x t )] j \u00d7 \u2206 t,j \u221a v t,j + \u03c4 \u2212 \u2206 t,j \u03b2 2 v t\u22121,j + \u03c4 = E t d j=1 [\u2207f (x t )] j \u00d7 \u2206 t,j \u00d7 \u03b2 2 v t\u22121,j \u2212 \u221a v t,j ( \u221a v t,j + \u03c4 )( \u03b2 2 v t\u22121,j + \u03c4 ) = E t d j=1 [\u2207f (x t )] j \u00d7 \u2206 t,j \u00d7 \u2212(1 \u2212 \u03b2 2 )\u2206 2 t,j ( \u221a v t,j + \u03c4 )( \u03b2 2 v t\u22121,j + \u03c4 )( \u03b2 2 v t\u22121,j + \u221a v t,j ) \u2264 (1 \u2212 \u03b2 2 )E t d j=1 |\u2207f (x t )] j | \u00d7 |\u2206 t,j | \u00d7 \u2206 2 t,j ( \u221a v t,j + \u03c4 )( \u03b2 2 v t\u22121,j + \u03c4 )( \u03b2 2 v t\u22121,j + \u221a v t,j ) \u2264 1 \u2212 \u03b2 2 E t d j=1 |\u2207f (x t )] j | \u00d7 \u2206 2 t,j \u221a v t,j + \u03c4 )( \u03b2 2 v t\u22121,j + \u03c4 ) \u2264 1 \u2212 \u03b2 2 E t d j=1 G \u03c4 \u00d7 \u2206 2 t,j \u221a v t,j + \u03c4 .\nBounding R 1 . The term R 1 can be bounded as follows:\nR 1 = \u2207f (x t ), E t \u2206 t \u03b2 2 v t\u22121 + \u03c4 = \u2207f (x t ) \u03b2 2 v t\u22121 + \u03c4 , E t [\u2206 t \u2212 \u03b7 l K\u2207f (x t ) + \u03b7 l K\u2207f (x t )] = \u2212\u03b7 l K d j=1 [\u2207f (x t )] 2 j \u03b2 2 v t\u22121,j + \u03c4 + \u2207f (x t ) \u03b2 2 v t\u22121 + \u03c4 , E t [\u2206 t + \u03b7 l K\u2207f (x t )] R3 .(15)\nBounding R 3 . The term R 3 can be bounded in exactly the same way as term T 3 in proof of Theorem 1:\nR 3 \u2264 \u03b7 l K 2 d j=1 [\u2207f (x t )] 2 j \u03b2 2 v t\u22121,j + \u03c4 + \u03b7 l L 2 2m\u03c4 E t m i=1 K\u22121 k=0 x t i,k \u2212 x t 2\nSubstituting the above inequality in Equation (15), we get\nR 1 \u2264 \u2212 \u03b7 l K 2 d j=1 [\u2207f (x t )] 2 j \u03b2 2 v t\u22121,j + \u03c4 + \u03b7 l L 2 2m\u03c4 E t m i=1 K\u22121 k=0 x t i,k \u2212 x t 2\nUsing Lemma 3, we obtain the following bound on R 1 :\nR 1 \u2264 \u2212 \u03b7 l K 4 d j=1 [\u2207f (x t )] 2 j \u03b2 2 v t\u22121,j + \u03c4 + 5\u03b7 3 l K 2 L 2 2\u03c4 E t d j=1 (\u03c3 2 l,j + 6K\u03c3 2 g,j )(16)\nHere we used the fact that \u221a v t\u22121,j \u2264 \u03b7 l KG and conditions in Theorem 2.\n\nPutting pieces together. Substituting bounds R 1 and R 2 in Equation (14), we have\nE t [f (x t+1 )] \u2264 f (x t ) \u2212 \u03b7\u03b7 l K 4 d j=1 [\u2207f (x t )] 2 j \u03b2 2 v t\u22121,j + \u03c4 + 5\u03b7\u03b7 3 l K 2 L 2 2\u03c4 E d j=1 (\u03c3 2 l,j + 6K\u03c3 2 g,j ) + \u03b7 \u221a 1 \u2212 \u03b2 2 G \u03c4 d j=1 E t \u2206 2 t,j \u221a v t,j + \u03c4 + \u03b7 2 L 2 d j=1 E t \u2206 2 t,j v t,j + \u03c4 2\nSumming over t = 0 to T \u2212 1 and using telescoping sum, we have\nE[f (x T )] \u2264 f (x 0 ) \u2212 \u03b7\u03b7 l K 4 T \u22121 t=0 d j=1 E [\u2207f (x t )] 2 j \u03b2 2 v t\u22121,j + \u03c4 + 5\u03b7\u03b7 3 l K 2 L 2 T 2\u03c4 E d j=1 (\u03c3 2 l,j + 6K\u03c3 2 g,j ) \u03b7 \u221a 1 \u2212 \u03b2 2 G \u03c4 T \u22121 t=0 d j=1 E \u2206 2 t,j \u221a v t,j + \u03c4 + \u03b7 2 L 2 T \u22121 t=0 d j=1 E \u2206 2 t,j v t,j + \u03c4 2(17)\nTo bound this term further, we need the following result.\n\nLemma 5. The following upper bound holds for Algorithm 2 (FEDADAM):\nT \u22121 t=0 d j=1 E \u2206 2 t,j (v t,j + \u03c4 2 ) \u2264 4\u03b7 2 l KT m\u03c4 2 d j=1 \u03c3 2 l,j + 20\u03b7 4 l K 3 L 2 T \u03c4 2 E d j=1 (\u03c3 2 l,j + 6K\u03c3 2 g,j ) + 40\u03b7 4 l K 4 L 2 + 2\u03b7 2 l K 2 \u03c4 2 T \u22121 t=0 E \u2207f (x t ) 2 Proof. E T \u22121 t=0 d j=1 \u2206 2 t,j (1 \u2212 \u03b2 2 ) t l=0 \u03b2 t\u2212l 2 \u2206 2 t,j + \u03c4 2 \u2264 E T \u22121 t=0 d j=1 \u2206 2 t,j \u03c4 2\nThe rest of the proof follows along the lines of proof of Lemma 4. Using the same argument, we get\nd j=1 E \u2206 2 t,j (v t,j + \u03c4 2 ) \u2264 4\u03b7 2 l KT m\u03c4 2 d j=1 \u03c3 2 l,j + 20\u03b7 4 l K 3 L 2 T \u03c4 2 E d j=1 (\u03c3 2 l,j + 6K\u03c3 2 g,j ) + 40\u03b7 4 l K 4 L 2 + 2\u03b7 2 l K 2 \u03c4 2 T \u22121 t=0 E \u2207f (x t ) 2 ,\nwhich is the desired result.\n\nSubstituting the bound obtained from above lemma in Equation (17) and using a similar argument for bounding\n\u03b7 \u221a 1 \u2212 \u03b2 2 G \u03c4 T \u22121 t=0 d j=1 E \u2206 2 t,j \u221a v t,j + \u03c4 ) ,\nwe obtain\nE t [f (x T )] \u2264 f (x 0 ) \u2212 \u03b7\u03b7 l K 8 T \u22121 t=0 d j=1 [\u2207f (x t )] 2 j \u03b2 2 v t\u22121,j + \u03c4 + 5\u03b7\u03b7 3 l K 2 L 2 T 2\u03c4 E d j=1 (\u03c3 2 l,j + 6K\u03c3 2 g,j ) + \u03b7 1 \u2212 \u03b2 2 G + \u03b7 2 L 2 \u00d7 \uf8ee \uf8f0 4\u03b7 2 l KT m\u03c4 2 d j=1 \u03c3 2 l,j + 20\u03b7 4 l K 4 L 2 T \u03c4 2 E d j=1 (\u03c3 2 l,j + 6K\u03c3 2 g,j ) \uf8f9 \uf8fb\nThe above inequality is obtained due to the fact that:\n1 \u2212 \u03b2 2 G + \u03b7L 2 40\u03b7 4 l K 4 L 2 + 2\u03b7 2 l K 2 \u03c4 2 \u2264 \u03b7 l K 8 1 \u221a \u03b2 2 \u03b7 l KG + \u03c4 .\nThe above inequality is due to the condition on \u03b7 l in Theorem 2. Here, we used the fact that under the conditions assumed, we have 40\u03b7 4 l K 4 L 2 \u2264 2\u03b7 2 l K 2 . We also observe the following:\nT \u22121 t=0 d j=1 [\u2207f (x t )] 2 j \u03b2 2 v t\u22121,j + \u03c4 \u2265 T \u22121 t=0 d j=1 [\u2207f (x t )] 2 j \u221a \u03b2 2 \u03b7 l KG + \u03c4 \u2265 T \u221a \u03b2 2 \u03b7 l KG + \u03c4 min 0\u2264t\u2264T \u2207f (x t ) 2 .\nSubstituting this bound in the above equation yields the desired result.\n\nA.4 AUXILIARY LEMMATTA Lemma 6. For random variables z 1 , . . . , z r , we have\nE z 1 + ... + z r 2 \u2264 rE z 1 2 + ... + z r 2 .\nLemma 7. For independent, mean 0 random variables z 1 , . . . , z r , we have E z 1 + ... + z r 2 = E z 1 2 + ... + z r 2 .\n\n\nB FEDERATED ALGORITHMS: IMPLEMENTATIONS AND PRACTICAL CONSIDERATIONS B.1 FEDAVG AND FEDOPT\n\nIn Algorithm 3, we give a simplified version of the FEDAVG algorithm by McMahan et al. (2017), that applies to the setup given in Section 2. We write SGD K (x t , \u03b7 l , f i ) to denote K steps of SGD using Algorithm 3 Simplified FEDAVG Simplified FEDAVG Simplified FEDAVG Input: x 0 for t = 0, \u00b7 \u00b7 \u00b7 , T \u2212 1 do Sample a subset S of clients z) for z \u223c D i with (local) learning rate \u03b7 l , starting from x t . As noted in Section 2, Algorithm 3 is the special case of Algorithm 1 where CLIENTOPT is SGD, and SERVEROPT is SGD with learning rate 1.\nx t i = x t for each client i \u2208 S in parallel do x t i = SGD K (x t , \u03b7 l , f i ) for i \u2208 S (in parallel) x t+1 = 1 |S| i\u2208S x t i gradients \u2207f i (x,\nWhile Algorithms 1, 2, and 3 are useful for understanding relations between federated optimization methods, we are also interested in practical versions of these algorithms. In particular, Algorithms 1, 2, and 3 are stated in terms of a kind of 'gradient oracle', where we compute unbiased estimates of the client's gradient. In practical scenarios, we often only have access to finite data samples, the number of which may vary between clients.\n\nInstead, we assume that in (1), each client distribution D i is the uniform distribution over some finite set D i of size n i . The n i may vary significantly between clients, requiring extra care when implementing federated optimization methods. We assume the set D i is partitioned into a collection of batches B i , each of size B. For b \u2208 B i , we let f i (x; b) denote the average loss on this batch at x with corresponding gradient \u2207f i (\nx; b). Thus, if b is sampled uniformly at random from B i , \u2207f i (x; b) is an unbiased estimate of \u2207F i (x).\nWhen training, instead of uniformly using K gradient steps, as in Algorithm 1, we will instead perform E epochs of training over each client's dataset. Additionally, we will take a weighted average of the client updates, where we weight according to the number of examples n i in each client's dataset. This leads to a batched data version of FEDOPT in Algorithm 4, and a batched data version of FEDADAGRAD, FEDADAM, and FEDYOGI given in Algorithm 5.\n\n\nAlgorithm 4 FEDOPT FEDOPT FEDOPT -Batched data\n\nInput: x 0 , CLIENTOPT, SERVEROPT for t = 0, \u00b7 \u00b7 \u00b7 , T \u2212 1 do Sample a subset S of clients x t i = x t for each client i \u2208 S in parallel do for e = 1, . . . , E do\nfor b \u2208 B i do g t i = \u2207f i (x t i ; b) x t i = CLIENTOPT(x t i , g t i , \u03b7 l , t) \u2206 t i = x t i \u2212 x t n = i\u2208S n i , \u2206 t = i\u2208S ni n \u2206 t i x t+1 = SERVEROPT(x t , \u2212\u2206 t , \u03b7, t)\nIn Section 5, we use Algorithm 4 when implementing FEDAVG and FEDAVGM. In particular, FEDAVG and FEDAVGM correspond to Algorithm 4 where CLIENTOPT and SERVEROPT are SGD. FEDAVG uses vanilla SGD on the server, while FEDAVGM uses SGD with a momentum parameter of 0.9. In both methods, we tune both client learning rate \u03b7 l and server learning rate \u03b7. This means that the version of FEDAVG used in all experiments is strictly more general than that in (McMahan et al., 2017), which corresponds to FEDOPT where CLIENTOPT and SERVEROPT are SGD, and SERVEROPT has a learning rate of 1.\n\nWe use Algorithm 5 for all implementations FEDADAGRAD, FEDADAM, and FEDYOGI in Section 5. For FEDADAGRAD, we set \u03b2 1 = \u03b2 2 = 0 (as typical versions of ADAGRAD do not use momentum). For FEDADAM and FEDYOGI we set \u03b2 1 = 0.9, \u03b2 2 = 0.99. While these parameters are generally Published as a conference paper at ICLR 2021\n\n\nAlgorithm 5 FEDADAGRAD FEDADAGRAD FEDADAGRAD , FEDYOGI FEDYOGI FEDYOGI , and FEDADAM FEDADAM FEDADAM -Batched data\n\nInput: x 0 , v \u22121 \u2265 \u03c4 2 , optional \u03b2 1 , \u03b2 2 \u2208 [0, 1) for FEDYOGI and FEDADAM for t = 0, \u00b7 \u00b7 \u00b7 , T \u2212 1 do Sample a subset S of clients 1. In cross-device settings, this is not a fair comparison. In particular, SCAFFOLD does not work in settings where clients cannot maintain state across rounds, as may be the case for federated learning systems on edge devices, such as cell phones.\nx t i = x t for each client i \u2208 S in parallel do for e = 1, . . . , E do for b \u2208 B i do x t i = x t i \u2212 \u03b7 l \u2207f i (x t i ; b) \u2206 t i = x t i \u2212 x t n = i\u2208S n i , \u2206 t = i\u2208S ni n \u2206 t i m t = \u03b2 1 m t\u22121 + (1 \u2212 \u03b2 1 )\u2206 t v t = v t\u22121 + \u2206 2 t (FEDADAGRAD) (FEDADAGRAD) (FEDADAGRAD) v t = v t\u22121 \u2212 (1 \u2212 \u03b2 2 )\u2206 2 t sign(v t\u22121 \u2212 \u2206 2 t ) (FEDYOGI) (FEDYOGI) (FEDYOGI) v t = \u03b2 2 v t\u22121 + (1 \u2212 \u03b2 2 )\u2206 2 t (FEDADAM) (FEDADAM) (FEDADAM) x t+1 = x t + \u03b7 mt\n2. SCAFFOLD has two variants described by Karimireddy et al. (2019). In Option I, the control variate of a client is updated using a full gradient computation. This effectively requires performing an extra pass over each client's dataset, as compared to Algorithm 1. In order to normalize the amount of client work, we instead use Option II, in which the clients' control variates are updated using the difference between the server model and the client's learned model. This requires the same amount of client work as FEDAVG and Algorithm 2.\n\nFor practical reasons, we implement a version of SCAFFOLD mirroring Algorithm 4, in which we perform E epochs over the client's dataset, and perform weighted averaging of client models. For posterity, we give the full pseudo-code of the version of SCAFFOLD used in our experiments in Algorithm 6. This is a simple adaptiation of Option II of the SCAFFOLD algorithm in (Karimireddy et al., 2019) to the same setting as Algorithm 4. In particular, we let n i denote the number of examples in client i's local dataset.\n\nIn this algorithm, c i is the control variate of client i, and c is the running average of these control variates. In practice, we must initialize the control variates c i in some way when sampling a client i for the first time. In our implementation, we set c i = c the first time we sample a client i. This has the advantage of exactly recovering FEDAVG when each client is sampled at most once. To initialize c, we use the all zeros vector.\n\nWe compare this version of SCAFFOLD to FEDADAGRAD, FEDADAM, FEDYOGI, FEDAVGM, and FEDAVG on our tasks, tuning the learning rates in the same way (using the same grids as in Appendix D.2). In particular, \u03b7 l , \u03b7 are tuned to obtain the best training performance over the last 100 communication rounds. We use the same federated hyperparameters for SCAFFOLD as discussed in Section 4. Namely, we set E = 1, and sample 10 clients per round for all tasks except Stack Overflow NWP, where we sample 50. The results are given in Figure 1 in Section 5.\n\n\nAlgorithm 6 SCAFFOLD SCAFFOLD SCAFFOLD, Option II -Batched data\n\nInput: x 0 , c, \u03b7 l , \u03b7 for t = 0, \u00b7 \u00b7 \u00b7 , T \u2212 1 do Sample a subset S of clients\nx t i = x t for each client i \u2208 S in parallel do for e = 1, . . . , E do for b \u2208 B i do g t i = \u2207f i (x t i ; b) x t i = x t i \u2212 \u03b7 l (g t i \u2212 c i + c) c + i = c i \u2212 c + (E|B i |\u03b7 l ) \u22121 (x t i \u2212 x i ) \u2206x i = x t i \u2212 x t , \u2206c i = c + i \u2212 c i c i = c + i n = i\u2208S n i , \u2206x = i\u2208S ni n \u2206x i , \u2206c = i\u2208S ni n \u2206c i x t+1 = x t + \u03b7\u2206x, c = c + |S| N \u2206c\n\nB.3 LOOKAHEAD, ADAALTER, AND CLIENT ADAPTIVITY\n\nThe LOOKAHEAD optimizer (Zhang et al., 2019b) is primarily designed for non-FL settings. LOOKA-HEAD uses a generic optimizer in the inner loop and updates its parameters using a \"outer\" learning rate. Thus, unlike FEDOPT, LOOKAHEAD uses a single generic optimizer and is thus conceptually different. In fact, LOOKAHEAD can be seen as a special case of FEDOPT in non-FL settings which uses a generic optimizer CLIENTOPT as a client optimizer, and SGD as the server optimizer. While there are multiple ways LOOKAHEAD could be generalized to a federated setting, one straightforward version would simply use an adaptive method as the CLIENTOPT. On the other hand, ADAALTER (Xie et al., 2019) is designed specifically for distributed settings. In ADAALTER, clients use a local optimizer similar to ADAGRAD (McMahan & Streeter, 2010a; Duchi et al., 2011) to perform multiple epochs of training on their local datasets. Both LOOKAHEAD and ADAALTER use client adaptivity, which is fundamentally different from the adaptive server optimizers proposed in Algorithm 2.\n\nTo illustrate the differences, consider the client-to-server communication in ADAALTER. This requires communicating both the model weights and the client accumulators (used to perform the adaptive optimization, analogous to v t in Algorithm 2). In the case of ADAALTER, the client accumulator is the same size as the model's trainable weights. Thus, the client-to-server communication doubles for this method, relative to FEDAVG. In ADAALTER, the server averages the client accumulators and broadcasts the average to the next set of clients, who use this to initialize their adaptive optimizers. This means that the server-to-client communication also doubles relative to FEDAVG. The same would occur for any adaptive client optimizer in the distributed version of LOOKAHEAD described above. For similar reasons, we see that client adaptive methods also increase the amount of memory needed on the client (as they must store the current accumulator). By contrast, our adaptive server methods (Algorithm 2) do not require extra communication or client memory relative to FEDAVG. Thus, we see that server-side adaptive optimization benefits from lower per-round communication and client memory requirements, which are of paramount importance for FL applications (Bonawitz et al., 2019).\n\n\nC DATASET & MODELS\n\nHere we provide detailed description of the datasets and models used in the paper. We use federated versions of vision datasets EMNIST ( Table 2. We give descriptions of the datasets, models, and tasks below. Statistics on the number of clients and examples in both the training and test splits of the datasets are given in Table 2. We create a federated version of CIFAR-10 by randomly partitioning the training data among 500 clients, with each client receiving 100 examples. We use the same approach as Hsu et al. (2019), where we apply latent Dirichlet allocation (LDA) over the labels of CIFAR-10 to create a federated dataset. Each client has an associated multinomial distribution over labels from which its examples are drawn. The multinomial is drawn from a symmetric Dirichlet distribution with parameter 0.1.\n\nFor CIFAR-100, we perform a similar partitioning of 100 examples to 500 clients, but using a more sophisticated approach. We use a two step LDA process over the coarse and fine labels. We randomly partition the data to reflect the \"coarse\" and \"fine\" label structure of CIFAR-100 by using the Pachinko Allocation Method (PAM) (Li & McCallum, 2006). This creates more realistic client datasets, whose label distributions better resemble practical heterogeneous client datasets. We have made publicly available the specific federated version of CIFAR-100 we used for all experiments, though we avoid giving a link in this work in order to avoid de-anonymization. For complete details on how the dataset was created, see Appendix F.\n\nWe train a modified ResNet-18 on both datasets, where the batch normalization layers are replaced by group normalization layers (Wu & He, 2018). We use two groups in each group normalization layer. As shown by Hsieh et al. (2019), group normalization can lead to significant gains in accuracy over batch normalization in federated settings.\n\nPreprocessing CIFAR-10 and CIFAR-100 consist of images with 3 channels of 32 \u00d7 32 pixels each. Each pixel is represented by an unsigned int8. As is standard with CIFAR datasets, we perform preprocessing on both training and test images. For training images, we perform a random crop to shape (24, 24, 3), followed by a random horizontal flip. For testing images, we centrally crop the image to (24,24,3). For both training and testing images, we then normalize the pixel values according to their mean and standard deviation. Namely, given an image x, we compute (x \u2212 \u00b5)/\u03c3 where \u00b5 is the average of the pixel values in x, and \u03c3 is the standard deviation.\n\n\nC.2 EMNIST\n\nEMNIST consists of images of digits and upper and lower case English characters, with 62 total classes. The federated version of EMNIST (Caldas et al., 2018) partitions the digits by their author. The dataset has natural heterogeneity stemming from the writing style of each person. We perform two distinct tasks on EMNIST, autoencoder training (EMNIST AE) and character recognition (EMNIST CR). For EMNIST AE, we train the \"MNIST\" autoencoder (Zaheer et al., 2018). This is a densely connected autoencoder with layers of size (28 \u00d7 28) \u2212 1000 \u2212 500 \u2212 250 \u2212 30 and a symmetric decoder. A full description of the model is in Table 3. For EMNIST CR, we use a convolutional network. The network has two convolutional layers (with 3 \u00d7 3 kernels), max pooling, and dropout, followed by a 128 unit dense layer. A full description of the model is in Table 4.\n\n\nC.3 SHAKESPEARE\n\nShakespeare is a language modeling dataset built from the collective works of William Shakespeare. In this dataset, each client corresponds to a speaking role with at least two lines. The dataset consists of 715 clients. Each client's lines are partitioned into training and test sets. Here, the task is to do next character prediction. We use an RNN that first takes a series of characters as input and embeds each of them into a learned 8-dimensional space. The embedded characters are then passed through  2 LSTM layers, each with 256 nodes, followed by a densely connected softmax output layer. We split the lines of each speaking role into into sequences of 80 characters, padding if necessary. We use a vocabulary size of 90; 86 for the characters contained in the Shakespeare dataset, and 4 extra characters for padding, out-of-vocabulary, beginning of line and end of line tokens. We train our model to take a sequence of 80 characters, and predict a sequence of 80 characters formed by shifting the input sequence by one (so that its last character is the new character we are actually trying to predict). Therefore, our output dimension is 80 \u00d7 90. A full description of the model is in Table 5. For Stack Overflow NWP, we restrict each client to the first 1000 sentences in their dataset (if they contain this many, otherwise we use the full dataset). We also perform padding and truncation to ensure that sentences have 20 words. We then represent the sentence as a sequence of indices corresponding to the 10,000 frequently used words, as well as indices representing padding, out-ofvocabulary words, beginning of sentence, and end of sentence. We perform next-word-prediction on these sequences using an RNN that embeds each word in a sentence into a learned 96-dimensional space. It then feeds the embedded words into a single LSTM layer of hidden dimension 670, followed by a densely connected softmax output layer. A full description of the model is in Table 6. The metric used in the main body is the top-1 accuracy over the proper 10,000-word vocabulary; it does not include padding, out-of-vocab, or beginning or end of sentence tokens when computing the accuracy. Throughout our experiments, we compare the performance of different instantiations of Algorithm 1 that use different server optimizers. We use SGD, SGD with momentum (denoted SGDM), ADAGRAD, ADAM, and YOGI. For the client optimizer, we use mini-batch SGD throughout. For all tasks, we tune the client learning rate \u03b7 l and server learning rate \u03b7 by using a large grid search. Full descriptions of the per-task server and client learning rate grids are given in Appendix D.2.\n\nWe use the version of FEDADAGRAD, FEDADAM, and FEDYOGI in Algorithm 5. We let \u03b2 1 = 0 for FEDADAGRAD, and we let \u03b2 1 = 0.9, \u03b2 2 = 0.99 for FEDADAM, and FEDYOGI. For FEDAVG and FEDAVGM, we use Algorithm 4, where CLIENTOPT, SERVEROPT are SGD. For FEDAVGM, the server SGD optimizer uses a momentum parameter of 0.9. For FEDADAGRAD, FEDADAM, and FEDYOGI, we tune the parameter \u03c4 in Algorithm 5.\n\nWhen tuning parameters, we select the best hyperparameters (\u03b7 l , \u03b7, \u03c4 ) based on the average training loss over the last 100 communication rounds. Note that at each round, we only see a fraction of the total users (10 for each task except Stack Overflow NWP, which uses 50). Thus, the training loss at a given round is a noisy estimate of the population-level training loss, which is why we averave over a window of communication rounds.\n\n\nD.2 HYPERPARAMETER GRIDS\n\nBelow, we give the client learning rate (\u03b7 l in Algorithm 1) and server learning rate (\u03b7 in Algorithm 1) grids used for each task. These grids were chosen based on an initial evaluation over the grids \u03b7 l \u2208 {10 \u22123 , 10 \u22122.5 , 10 \u22122 , . . . , 10 0.5 } \u03b7 \u2208 {10 \u22123 , 10 \u22122.5 , 10 \u22122 , . . . , 10 1 } These grids were then refined for Stack Overflow LR and EMNIST AE in an attempt to ensure that the best client/server learning rate combinations for each optimizer was contained in the interior of the learning rate grids. We generally found that these two tasks required searching larger learning rates than the other two tasks. The final grids were as follows:\n\nCIFAR-10: \u03b7 l \u2208 {10 \u22123 , 10 \u22122.5 , . . . , 10 0.5 } \u03b7 \u2208 {10 \u22123 , 10 \u22122.5 , . . . , 10 1 } CIFAR-100: \u03b7 l \u2208 {10 \u22123 , 10 \u22122.5 , . . . , 10 0.5 } \u03b7 \u2208 {10 \u22123 , 10 \u22122.5 , . . . , 10 1 } EMNIST AE: \u03b7 l \u2208 {10 \u22121.5 , 10 \u22121 , . . . , 10 2 } \u03b7 \u2208 {10 \u22122 , 10 \u22121.5 , . . . , 10 1 } EMNIST CR: \u03b7 l \u2208 {10 \u22123 , 10 \u22122.5 , . . . , 10 0.5 } \u03b7 \u2208 {10 \u22123 , 10 \u22122.5 , . . . , 10 1 } Shakespeare: \u03b7 l \u2208 {10 \u22123 , 10 \u22122.5 , . . . , 10 0.5 } \u03b7 \u2208 {10 \u22123 , 10 \u22122.5 , . . . , 10 1 } StackOverflow LR: \u03b7 l \u2208 {10 \u22121 , 10 \u22120.5 , . . . , 10 3 } \u03b7 \u2208 {10 \u22122 , 10 \u22121.5 , . . . , 10 1.5 } StackOverflow NWP: \u03b7 l \u2208 {10 \u22123 , 10 \u22122.5 , . . . , 10 0.5 } \u03b7 \u2208 {10 \u22123 , 10 \u22122.5 , . . . , 10 1 } For all tasks, we tune \u03c4 over the grid:\n\n\u03c4 \u2208 {10 \u22125 , . . . , 10 \u22121 }.\n\n\nD.3 PER-TASK BATCH SIZES\n\nGiven the large number of hyperparameters to tune, and to avoid conflating variables, we fix the batch size at a per-task level. When comparing centralized training to federated training in Section 5, we use the same batch size in both federated and centralized training. A full summary of the batch sizes is given in Table 7. In this section, we present, for each optimizer, the best client and server learning rates and values of \u03c4 found for the tasks discussed in Section 5. Specifically, these are the hyperparameters used in Figure Figure 1 and table Table 1. The validation metrics in Table 1 are obtained using the learning rates in Table 8 and the values of \u03c4 in Table 9. As discussed in Section 4, we choose \u03b7, \u03b7 l and \u03c4 to be the parameters that minimizer the average training loss over the last 100 communication rounds. The base-10 logarithm of the client (\u03b7 l ) and server (\u03b7) learning rate combinations that achieve the accuracies from Table 1. See Appendix D.2 for a full description of the grids. Table 9: The base-10 logarithm of the parameter \u03c4 (as defined in Algorithm 2) that achieve the validation metrics in Table 1.\nFEDADAGRAD FEDADAM FEDYOGI FEDAVGM FEDAVG \u03b7 l \u03b7 \u03b7 l \u03b7 \u03b7 l \u03b7 \u03b7 l \u03b7 \u03b7 l \u03b7 CIFAR-10 -3 \u20442 -1 -3 \u20442 -2 -3 \u20442 -2 -3 \u20442 -1 \u20442 -1 \u20442 0 CIFAR-100 -1 -1 -3 \u20442 0 -3 \u20442 0 -3 \u20442 0 -1 1 \u20442 EMNIST AE 3 \u20442 -3 \u20442 1 -3 \u20442 1 -3 \u20442 1 \u20442 0 1 0 EMNIST CR -3 \u20442 -1 -3 \u20442 -5 \u20442 -3 \u20442 -5 \u20442 -3 \u20442 -1 \u20442 -1 0 SHAKESPEARE 0 -1 \u20442 0 -2 0 -2 0 -1 \u20442 0 0 STACKOVERFLOW LR 2 1 2 -1 \u20442 2 -1 \u20442 2 0 2 0 STACKOVERFLOW NWP -1 \u20442 -3 \u20442 -1 \u20442 -2 -1 \u20442 -2 -1 \u20442 0 -1 \u20442 0FEDADAGRAD FEDADAM FEDYOGI CIFAR-10 -2 -3 -3 CIFAR-100 -2 -1 -1 EMNIST AE -3 -3 -3 EMNIST CR -2 -4 -4 SHAKESPEARE -1 -3 -3 STACKOVERFLOW LR -2 -5 -5 STACKOVERFLOW NWP -4 -5 -5\n0 500 1000 1500 Number of Rounds 0.7 0.8 0.9 Accuracy EMNIST CR Figure 4: Validation accuracy on EMNIST CR using constant learning rates \u03b7, \u03b7 l , and \u03c4 tuned to achieve the best training performance on the last 100 communication rounds; see Appendix D for hyperparameter grids. We plot the validation accuracy of FEDADAGRAD, FEDADAM, FEDYOGI, FEDAVGM, FEDAVG, and SCAFFOLD on EMNIST CR. As in Figure 1, we tune the learning rates \u03b7 l , \u03b7 and adaptivity \u03c4 by selecting the parameters obtaining the smallest training loss, averaged over the last 100 training rounds. The results are given in Figure 4.\n\nWe see that all methods are roughly comparably throughout the duration of training. This reflects the fact that the dataset is quite simple, and most clients have all classes in their local datasets, reducing any heterogeneity among classes. Note that SCAFFOLD performs slightly worse than FEDAVG and all other methods here. As discussed in Section 5, this may be due to the presence of stale client control variates, and the communication-limited regime of our experiments.\n\n\nE.2 STACK OVERFLOW TEST SET PERFORMANCE\n\nAs discussed in Section 5, in order to compute a measure of performance for the Stack Overflow tasks as training progresses, we use a subsampled version of the test dataset, due to its prohibitively large number of clients and examples. In particular, at each round of training, we sample 10,000 random test samples, and use this as a measure of performance over time. However, once training is completed, we also evaluate on the full test dataset. For the Stack Overflow experiments described in Section 5, the final test accuracy is given in Table 10.\n\n\nE.3 LEARNING RATE ROBUSTNESS\n\nIn this section, we showcase what combinations of client learning rate \u03b7 l and server learning rate \u03b7 performed well for each optimizer and task. As in Figure 2, we plot, for each optimizer, task, and pair (\u03b7 l , \u03b7), the validation set performance (averaged over the last 100 rounds). As in Section 5, we fix \u03c4 = 10 \u22123 throughout. The results, for the CIFAR-10, CIFAR-100, EMNIST AE, EMNIST CR, Shakespeare, Stack Overflow LR, and Stack Overflow NWP tasks are given in Figures 5, 6 , 7, 8, 9, 10, and 11, respectively. While the general trends depend on the optimizer and task, we see that in many cases, the adaptive methods have rectangular regions that perform well. This implies a kind of robustness to fixing one of \u03b7, \u03b7 l , and varying the other. On the other hand, FEDAVGM and FEDAVG often have triangular regions that perform well, suggesting that \u03b7 and \u03b7 l should be tuned simultaneously.  1.0 0.5 0.0 -0.5 -1.0 -1.5 -2.0 Server Learning Rate (log10) 2.9 3.1 2.9 2.9 2.8 2.7 2.8 2.8 2.9 2.8 2.9 2.8 2.8 2.7 2.7 2.9 2.8 2.9 3.0 2.9 2.9 2.7 2.8 2.9 3.0 3.0 2.9 2.8 2.7 2.8 2.8 2.8 2.9 2.9 2.9 3.0 2.9 2.7 2.8 2.9 1.7 1.7 2.7 1.7 0.7 0.5 0.4 1. 3.0 2.9 2.9 2.9 2.8 2.7 2.9 2.8 2.9 2.9 3.0 2.9 2.8 2.7 2.8 2.8 2.9 2.8 2.9 2.9 2.9 2.7 2.7 3.0 1.7 2.9 3.0 3.0 3.4 2.7 2.8 3.0 1.6 1.7 1.7 1.7 1.7 2.7 2.8 2. 1.7 1.7 1.7 1.7 1.3 0.6 2.7 2.9 1.7 1.7 1.7 1.7 1.7 1.5 1.1 2.7 1.7 1.7 1.7 1.7 1.7 1.7 1.7 1.7 1.7 1.7 1.7 1.7 1.7 1.7 1.7 1.7 1.9 1.7 1.7 1.7 1.7 1.7 1.7 1.7 EMNIST AE, FedAvg   Figure 11: Validation accuracy (averaged over the last 100 rounds) of FEDADAGRAD, FEDADAM, FEDYOGI, FEDAVGM, and FEDAVG for various client/server learning rates combination on the Stack Overflow NWP task. For FEDADAGRAD, FEDADAM, and FEDYOGI, we set \u03c4 = 10 \u22123 .\n\n\nE.4 ON THE RELATION BETWEEN CLIENT AND SERVER LEARNING RATES\n\nIn order to better understand the results in Appendix E.3, we plot the relation between optimal choices of client and server learning rates. For each optimizer, task, and client learning rate \u03b7 l , we find the best corresponding server learning rate \u03b7 among the grids listed in Appendix D.2. Throughout, we fix \u03c4 = 10 \u22123 for the adaptive methods. We omit any points for which the final validation loss is within 10% of the worst-recorded validation loss over all hyperparameters. Essentially, we omit client learning rates that did not lead to any training of the model. The results are given in Figure 12.  Figure 12: The best server learning rate in our hyperparameter tuning grids for each client learning rate, optimizers, and task. We select the server learning rates based on the average validation performance over the last 100 communication rounds. For FEDADAGRAD, FEDADAM, and FEDYOGI, we fix \u03c4 = 10 \u22123 . We omit all client learning rates for which all server learning rates did not change the initial validation loss by more than 10%.\n\nIn virtually all tasks, we see a clear inverse relationship between client learning rate \u03b7 l and server learning rate \u03b7 for FEDAVG and FEDAVGM. As discussed in Section 5, this supports the observation that for the non-adaptive methods, \u03b7 l and \u03b7 must in some sense be tuned simultaneously. On the other hand, for adaptive optimizers on most tasks we see much more stability in the best server learning rate \u03b7 as the client learning rate \u03b7 l varies. This supports our observation in Section 5 that for adaptive methods, tuning the client learning rate is more important.\n\nNotably, we see a clear exception to this on the Stack Overflow LR task, where there is a definitive inverse relationship between learning rates among all optimizers. The EMNIST AE task also displays somewhat different behavior. While there are still noticeable inverse relationships between learning rates for FEDAVG and FEDAVGM, the range of good client learning rates is relatively small. We emphasize that this task is fundamentally different than the remaining tasks. As noted by Zaheer et al. (2018), the primary obstacle in training bottleneck autoencoders is escaping saddle points, not in converging to critical points. Thus, we expect there to be qualitative differences between EMNIST AE and other tasks, even EMNIST CR (which uses the same dataset).\n\n\nE.5 ROBUSTNESS OF THE ADAPTIVITY PARAMETER\n\nAs discussed in Section 5, we plot, for each adaptive optimizer and task, the validation accuracy as a function of the adaptivity parameter \u03c4 . In particular, for each value of \u03c4 (which we vary over {10 \u22125 , . . . , 10 \u22121 }, see Appendix D), we plot the best possible last-100-rounds validation set performance. Specifically, we plot the average validation performance over the last 100 rounds using the best a posteriori values of client and server learning rates. The results are given in Figure 13.  Figure 13: Validation performance of FEDADAGRAD, FEDADAM, and FEDYOGI for varying \u03c4 on various tasks. The learning rates \u03b7 and \u03b7 l are tuned for each \u03c4 to achieve the best training performance on the last 100 communication rounds.\n\n\nE.6 IMPROVING PERFORMANCE WITH LEARNING RATE DECAY\n\nDespite the success of adaptive methods, it is natural to ask if there is still more to be gained. To test this, we trained the EMNIST CR model in a centralized fashion on a shuffled version of the dataset. We trained for 100 epochs and used tuned learning rates for each (centralized) optimizer, achieving an accuracy of 88% (see Table 11, CENTRALIZED row), significantly above the best EMNIST CR results from Table 1. The theoretical results in Section 3 point to a partial explanation, as they only hold when the client learning rate is small or is decayed over time.\n\nTo validate this, we ran the same hyperparameter grids on the federated EMNIST CR task, but using a client learning rate schedule. We use a \"staircase\" exponential decay schedule (EXPDECAY) where the client learning rate \u03b7 l is decreased by a factor of 0.1 every 500 rounds. This is analogous in some sense to standard staircase learning rate schedules in centralized optimization (Goyal et al., 2017). Table 11 gives the results. EXPDECAY improves the accuracy of all optimizers, and allows most to get close to the best centralized accuracy. While we do not close the gap with centralized optimization entirely, we suspect that further tuning of the amount and frequency of decay may lead to even better accuracies. However, this may also require performing significantly more communication rounds, as the theoretical results in Section 3 are primarily asymptotic. In communication-limited settings, the added benefit of learning rate decay seems to be modest. \n\n\nF CREATING A FEDERATED CIFAR-100\n\nOverview We use the Pachinko Allocation Method (PAM) (Li & McCallum, 2006) to create a federated CIFAR-100. PAM is a topic modeling method in which the correlations between individual words in a vocabulary are represented by a rooted directed acyclic graph (DAG) whose leaves are the vocabulary words. The interior nodes are topics with Dirichlet distributions over their child nodes. To generate a document, we sample multinomial distributions from each interior node's Dirichlet distribution. To sample a word from the document, we begin at the root, and draw a child node its multinomial distribution, and continue doing this until we reach a leaf node.\n\nTo partition CIFAR-100 across clients, we use the label structure of CIFAR-100. Each image in the dataset has a fine label (often referred to as its label) which is a member of a coarse label. For example, the fine label \"seal\" is a member of the coarse label \"aquatic mammals\". There are 20 coarse labels in CIFAR-100, each with 5 fine labels. We represent this structure as a DAG G, with a root whose children are the coarse labels. Each coarse label is an interior node whose child nodes are its fine labels. The root node has a symmetric Dirichlet distribution with parameter \u03b1 over the coarse labels, and each coarse label has a symmetric Dirichlet distribution with parameter \u03b2.\n\nWe associate each client to a document. That is, we draw a multinomial distribution from the Dirichlet prior at the root (Dir(\u03b1)) and a multinomial distribution from the Dirichlet prior at each coarse label (Dir(\u03b2)). To create the client dataset, we draw leaf nodes from this DAG using Pachinko allocation, randomly sample an example with the given fine label, and assign it to the client's dataset. We do this 100 times for each of 500 distinct training clients.\n\nWhile more complex than LDA, this approach creates more realistic heterogeneity among client datasets by creating correlations between label frequencies for fine labels within the same coarse label set. Intuitively, if a client's dataset has many images of dolphins, they are likely to also have pictures of whales. By using a small \u03b1 at the root, client datasets become more likely to focus on a few coarse labels. By using a larger \u03b2 for the coarse-to-fine label distributions, clients are more likely to have multiple fine labels from the same coarse label.\n\nOne important note: Once we sample a fine label, we randomly select an element with that label without replacement. This ensures no two clients have overlapping examples. In more detail, suppose we have sample a fine label y with coarse label c for client m, and there is only one remaining such example (x, c, y). We assign (x, c, y) to client m's dataset, and remove the leaf node y from the DAG G. We also remove y from the multinomial distribution \u03b8 c that client m has associated to coarse label c, which we refer to as renormalization with respect to y (Algorithm 8). If i has no remaining children after pruning node j, we also remove node i from G and re-normalize the root multinomial \u03b8 r with respect to c. For all subsequent clients, we draw multinomials from this pruned G according to symmetric Dirichlet distributions with the same parameters as before, but with one fewer category.\n\nNotation and method Let C denote the set of coarse labels and Y the set of fine labels, and let S denote the CIFAR-100 dataset. This consists of examples (x, c, y) where x is an image vector, c \u2208 C is a coarse label set, and y \u2208 Y is a fine label with y \u2208 c. For c \u2208 C, y \u2208 Y, we let S c and S y denote the set of examples in S with coarse label c and fine label y. For v \u2208 G we let |G[v]| denote the set of children of v in G. For \u03b3 \u2208 R, let Dir(\u03b3, k) denote the symmetric Dirichlet distribution with k categories.\n\nAlgorithm 7 Creating a federated CIFAR-100 Input: N, M \u2208 Z >0 , \u03b1, \u03b2 \u2208 R \u22650 for m = 1, \u00b7 \u00b7 \u00b7 , M do Sample \u03b8 r \u223c Dir(\u03b1, |G[r]|) for c \u2208 C \u2229 G[r] do Sample \u03b8 c \u223c Dir(\u03b2, |G[c]|) D m = \u2205 for n = 1, \u00b7 \u00b7 \u00b7 N do Sample c \u223c Multinomial(\u03b8 r ) Sample y \u223c Multinomial(\u03b8 c ) Select (x, c, y) \u2208 S uniformly at random D m = D m \u222a {(x, c, y)} S = S\\{(x, c, y)} if S y = \u2205 then G = G\\{y} \u03b8 c = RENORMALIZE(\u03b8 c , y) if S c = \u2205 then G = G\\{c} \u03b8 r = RENORMALIZE(\u03b8 r , c)\n\nAlgorithm 8 RENORMALIZE Initialization: \u03b8 = (p 1 , . . . , p K ), i \u2208 [K] a = k =i p k for k \u2208 [K], k = i do p k = p k /a Return \u03b8 = (p 1 , \u00b7 \u00b7 \u00b7 p i\u22121 , p i+1 , \u00b7 \u00b7 \u00b7 , p K ) Let M denote the number of clients, N the number of examples per client, and D m the dataset for client m \u2208 {1, \u00b7 \u00b7 \u00b7 , M }. A full description of our method is given in Algorithm 7. For our experiments, we use N = 100, M = 500, \u03b1 = 0.1, \u03b2 = 10. In Figure 14, we plot the distribution of unique labels among the 500 training clients. Each client has only a fraction of the overall labels in the distribution. Moreover, there is variance in the number of unique labels, with most clients having between 20 and 30, and some having over 40. Some client datasets have very few unique labels. While this is primarily an artifact of performing without replacement sampling, this helps increase the heterogeneity of the dataset in a way that can reflect practical concerns, as in many settings, clients may only have a few types of labels in their dataset. \n\nFigure 1 :\n1Validation accuracy of adaptive and non-adaptive methods, as well as SCAFFOLD, using constant learning rates \u03b7 and \u03b7 l tuned to achieve the best training performance over the last 100 communication rounds; see Appendix D.2 for grids.\n\nFigure 2 :\n2Validation accuracy (averaged over the last 100 rounds) of FEDADAM, FEDYOGI, and FEDAVGM for various client/server learning rates combination on the SO NWP task. For FEDADAM and FEDYOGI, we set \u03c4 = 10 \u22123 .\n\nFigure 3 :\n3Validation performance of FEDADAGRAD, FEDADAM, and FEDYOGI for varying \u03c4 on various tasks. The learning rates \u03b7 and \u03b7 l are tuned for each \u03c4 to achieve the best training performance on the last 100 communication rounds.\n\nFigure 2\n2gives results for FEDADAM, FEDYOGI, and FEDAVGM on Stack Overflow NWP. Plots for all other optimizers and tasks are in Appendix E.3. For FEDAVGM, there are only a few good values of \u03b7 l for each \u03b7, while for FEDADAM and FEDYOGI, there are many good values of \u03b7 l for a range of \u03b7. Thus, FEDADAM and FEDYOGI are arguably easier to tune in this setting. Similar results hold for other tasks and optimizers (Figures 5 to 11).\n\n\n(Zaheer et al., 2018), we emphasize that better results may be obtainable by tuning these parameters.B.2 SCAFFOLDAs discussed in Section 5, we compare all five optimizers above to SCAFFOLD (Karimireddy et al., 2019) on our various tasks. There are a few important notes about the validity of this comparison.\n\n\nCohen et al., 2017),CIFAR-10 (Krizhevsky & Hinton, 2009), and CIFAR-100 (Krizhevsky & Hinton, 2009), and language modeling datasets Shakespeare(McMahan et al., 2017)  andStackOverflow (Authors, 2019). Statistics for the training datasets can be found in\n\nFigure 5 :\n5Validation accuracy (averaged over the last 100 rounds) of FEDADAGRAD, FEDADAM, FEDYOGI, FEDAVGM, and FEDAVG for various client/server learning rates combination on the CIFAR-10 task. For FEDADAGRAD, FEDADAM, and FEDYOGI, we set \u03c4 = 10 \u22123 .\n\nFigure 6 :\n6Validation accuracy (averaged over the last 100 rounds) of FEDADAGRAD, FEDADAM, FEDYOGI, FEDAVGM, and FEDAVG for various client/server learning rates combination on the CIFAR-100 task. For FEDADAGRAD, FEDADAM, and FEDYOGI, we set \u03c4 = 10 \u22123 .\n\nFigure 7 :Figure 8 :Figure 9 :Figure 10 :\n78910Validation MSE (averaged over the last 100 rounds) of FEDADAGRAD, FEDADAM, FEDYOGI, FEDAVGM, and FEDAVG for various client/server learning rates combination on the EMNIST AE task. For FEDADAGRAD, FEDADAM, and FEDYOGI, we set \u03c4 = 10 \u22123 . Validation accuracy (averaged over the last 100 rounds) of FEDADAGRAD, FEDADAM, FEDYOGI, FEDAVGM, and FEDAVG for various client/server learning rates combination on the EMNIST CR task. For FEDADAGRAD, FEDADAM, and FEDYOGI, we set \u03c4 = 10 \u22123 . Validation accuracy (averaged over the last 100 rounds) of FEDADAGRAD, FEDADAM, FEDYOGI, FEDAVGM, and FEDAVG for various client/server learning rates combination on the Shakespeare task. For FEDADAGRAD, FEDADAM, and FEDYOGI, we set \u03c4 = 10 \u22123 . Validation recall@5 (averaged over the last 100 rounds) of FEDADAGRAD, FEDADAM, FEDYOGI, FEDAVGM, and FEDAVG for various client/server learning rates combination on the Stack Overflow LR task. For FEDADAGRAD, FEDADAM, and FEDYOGI, we set \u03c4 = 10 \u22123 .\n\nFigure 14 :\n14The distribution of the number of unique labels among training client datasets in our federated CIFAR-100 dataset.\n\n\nEMNIST AE). For Shakespeare, we train an RNN for next-character-prediction. For Stack Overflow, we perform tag prediction using logistic regression on bag-of-words vectors (SO LR) and train an RNN to do next-word-prediction (SO NWP). For full details of the datasets, see Appendix C.Datasets, models, and tasks We use five datasets: CIFAR-10, CIFAR-100 (Krizhevsky & Hinton, \n2009), EMNIST (Cohen et al., 2017), Shakespeare (McMahan et al., 2017), and Stack Overflow (Au-\nthors, 2019). The first three are image datasets, the last two are text datasets. For CIFAR-10 and \nCIFAR-100, we train ResNet-18 (replacing batch norm with group norm (Hsieh et al., 2019)). For \nEMNIST, we train a CNN for character recognition (EMNIST CR) and a bottleneck autoencoder \n(\n\nTable 1\n1summarizes the last-100-round validation performance. Due to space constraints, results for EMNIST CR are in Appendix E.1, and full test set results for Stack Overflow are in Appendix E.2.FED... \nADAGRAD ADAM YOGI AVGM AVG \n\nCIFAR-10 \n72.1 \n77.4 78.0 \n77.4 72.8 \nCIFAR-100 \n47.9 \n52.5 52.4 \n52.4 44.7 \nEMNIST CR \n85.1 \n85.6 85.5 \n85.2 84.9 \nSHAKESPEARE 57.5 \n57.0 57.2 \n57.3 56.9 \nSO NWP \n23.8 \n25.2 25.2 \n23.8 19.5 \n\nSO LR \n67.1 \n65.8 65.9 \n36.9 30.0 \n\nEMNIST AE \n4.20 \n1.01 0.98 \n1.65 6.47 \n\nTable 1: Average validation performance over the last \n100 rounds: % accuracy for rows 1-5; Recall@5 (\u00d7100) \nfor Stack Overflow LR; and MSE (\u00d71000) for EMNIST \nAE. Performance within 0.5% of the best result for each \ntask are shown in bold. \n\n\n\n\nTzu-Ming Harry Hsu, Hang Qi, and Matthew Brown. Measuring the effects of non-identical data distribution for federated visual classification. arXiv preprint arXiv:1909.06335, 2019. Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems, pp. 315-323, 2013. Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. arXiv preprint arXiv:1812.06127, 2018. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Ag\u00fcera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017, 20-22 April 2017, Fort Lauderdale, FL, USA, pp. 1273-1282, 2017. URL http://proceedings.mlr. press/v54/mcmahan17a.html. H. Brendan McMahan and Matthew Streeter. Adaptive bound optimization for online convex optimization. In COLT, 2010a. H. Brendan McMahan and Matthew J. Streeter. Adaptive bound optimization for online convex optimization. In COLT The 23rd Conference on Learning Theory, 2010b. Sashank J Reddi, Satyen Kale, and Sanjiv Kumar. On the convergence of ADAM and beyond. arXiv preprint arXiv:1904.09237, 2019. Sebastian U. Stich. Local SGD converges fast and communicates little. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id= S1g2JnRcFX.Alex \nIngerman \nand \nKrzys \nOstrowski. \nIntroducing \nTensorFlow \nFed-\nerated, \n2019. \nURL \nhttps://medium.com/tensorflow/ \nintroducing-tensorflow-federated-a4147aa20041. \n\nPeter Kairouz, H Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Bennis, Arjun Nitin \nBhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode, Rachel Cummings, et al. Advances \nand open problems in federated learning. arXiv preprint arXiv:1912.04977, 2019. \n\nSai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J Reddi, Sebastian U Stich, and \nAnanda Theertha Suresh. SCAFFOLD: Stochastic controlled averaging for on-device federated \nlearning. arXiv preprint arXiv:1910.06378, 2019. \n\nAhmed Khaled, Konstantin Mishchenko, and Peter Richt\u00e1rik. First analysis of local GD on heteroge-\nneous data. arXiv preprint arXiv:1909.04715, 2019. \n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd International \nConference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, \nConference Track Proceedings, 2015. \nAlex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. \nTechnical report, Citeseer, 2009. \n\nTian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated learning: Challenges, \nmethods, and future directions. arXiv preprint arXiv:1908.07873, 2019a. \n\nWei Li and Andrew McCallum. Pachinko allocation: DAG-structured mixture models of topic \ncorrelations. In Proceedings of the 23rd international conference on Machine learning, pp. 577-\n584, 2006. \n\nXiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of \nFedAvg on non-IID data. arXiv preprint arXiv:1907.02189, 2019b. \n\nXiaoyu Li and Francesco Orabona. On the convergence of stochastic gradient descent with adaptive \nstepsizes. arXiv preprint arXiv:1805.08114, 2018. \n\nLiangchen Luo, Yuanhao Xiong, Yan Liu, and Xu Sun. Adaptive gradient methods with dynamic \nbound of learning rate. In 7th International Conference on Learning Representations, ICLR 2019, \nNew Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019. URL https://openreview. \nnet/forum?id=Bkg3g2R9FX. \n\nSashank J Reddi, Ahmed Hefny, Suvrit Sra, Barnab\u00e1s P\u00f3cz\u00f3s, and Alex Smola. Stochastic variance \nreduction for nonconvex optimization. arXiv:1603.06160, 2016. \n\nSebastian U Stich and Sai Praneeth Karimireddy. The error-feedback framework: Better rates for \nSGD with delayed gradients and compressed communication. arXiv preprint arXiv:1909.05350, \n2019. \n\nJianyu Wang and Gauri Joshi. Cooperative SGD: A unified framework for the design and analysis of \ncommunication-efficient SGD algorithms. arXiv preprint arXiv:1808.07576, 2018. \nCong Xie, Oluwasanmi Koyejo, Indranil Gupta, and Haibin Lin. Local AdaAlter: Communication-\nefficient stochastic gradient descent with adaptive learning rates. arXiv preprint arXiv:1911.09030, \n2019. \n\nHao Yu, Sen Yang, and Shenghuo Zhu. Parallel restarted SGD with faster convergence and less \ncommunication: Demystifying why model averaging works for deep learning. In Proceedings of \nthe AAAI Conference on Artificial Intelligence, volume 33, pp. 5693-5700, 2019. \n\nManzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar. Adaptive methods \nfor nonconvex optimization. In Advances in Neural Information Processing Systems, pp. 9815-\n9825, 2018. \n\nJingzhao Zhang, Sai Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank J. Reddi, \nSanjiv Kumar, and Suvrit Sra. Why ADAM beats SGD for attention models. arXiv preprint \narxiv:1912.03194, 2019a. \n\nMichael R. Zhang, James Lucas, Jimmy Ba, and Geoffrey E. Hinton. Lookahead optimizer: k \nsteps forward, 1 step back. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence \nd'Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett (eds.), Advances in Neural Information Processing \nSystems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, \n8-14 December 2019, Vancouver, BC, Canada, pp. 9593-9604, 2019b. \n\nMartin Zinkevich, Markus Weimer, Lihong Li, and Alex J Smola. Parallelized stochastic gradient \ndescent. In Advances in neural information processing systems, pp. 2595-2603, 2010. \n\n\nTable 2 :\n2Dataset statistics.DATASET \nTRAIN CLIENTS TRAIN EXAMPLES TEST CLIENTS TEST EXAMPLES \n\nCIFAR-10 \n500 \n50,000 \n100 \n10,000 \nCIFAR-100 \n500 \n50,000 \n100 \n10,000 \nEMNIST-62 \n3,400 \n671,585 \n3,400 \n77,483 \nSHAKESPEARE \n715 \n16,068 \n715 \n2,356 \nSTACKOVERFLOW \n342,477 \n135,818,730 \n204,088 \n16,586,035 \n\nC.1 CIFAR-10/CIFAR-100 \n\n\n\nTable 3 :\n3EMNIST autoencoder model architecture. We use a sigmoid activation at all dense layers.Layer Output Shape # of Trainable Parameters \n\nInput \n784 \n0 \nDense \n1000 \n785000 \nDense \n500 \n500500 \nDense \n250 \n125250 \nDense \n30 \n7530 \nDense \n250 \n7750 \nDense \n500 \n125500 \nDense \n1000 \n501000 \nDense \n784 \n784784 \n\n\n\nTable 4 :\n4EMNIST character recognition model architecture.Layer \nOutput Shape # of Trainable Parameters Activation \nHyperparameters \n\nInput \n(28, 28, 1) \n0 \nConv2d \n(26, 26, 32) \n320 \nkernel size = 3; strides=(1, 1) \nConv2d \n(24, 24, 64) \n18496 \nReLU \nkernel size = 3; strides=(1, 1) \nMaxPool2d \n(12, 12, 64) \n0 \npool size= (2, 2) \nDropout \n(12, 12, 64) \n0 \np = 0.25 \nFlatten \n9216 \n0 \nDense \n128 \n1179776 \nDropout \n128 \n0 \np = 0.5 \nDense \n62 \n7998 \nsoftmax \n\n\n\nTable 5 :\n5Shakespeare model architecture.Layer \nOutput Shape # of Trainable Parameters \n\nInput \n80 \n0 \nEmbedding \n(80, 8) \n720 \nLSTM \n(80, 256) \n271360 \nLSTM \n(80, 256) \n525312 \nDense \n(80, 90) \n23130 \n\nC.4 STACK OVERFLOW \n\nStack Overflow is a language modeling dataset consisting of question and answers from the question \nand answer site, Stack Overflow. The questions and answers also have associated metadata, including \ntags. The dataset contains 342,477 unique users which we use as clients. We perform two tasks on this \ndataset: tag prediction via logistic regression (Stack Overflow LR, SO LR for short), and next-word \nprediction (Stack Overflow NWP, SO NWP for short). For both tasks, we restrict to the 10,000 most \nfrequently used words. For Stack Overflow LR, we restrict to the 500 most frequent tags and adopt a \n\n\nTable 6 :\n6Stack Overflow next word prediction model architecture.Layer \nOutput Shape # of Trainable Parameters \n\nInput \n20 \n0 \nEmbedding \n(20, 96) \n960384 \nLSTM \n(20, 670) \n2055560 \nDense \n(20, 96) \n64416 \nDense \n(20, 10004) \n970388 \n\nD EXPERIMENT HYPERPARAMETERS \n\nD.1 HYPERPARAMETER TUNING \n\n\n\nTable 7 :\n7Client batch sizes used for each task.TASK \nBATCH SIZE \n\nCIFAR-10 \n20 \nCIFAR-100 \n20 \nEMNIST AE \n20 \nEMNIST CR \n20 \nSHAKESPEARE \n4 \nSTACKOVERFLOW LR \n100 \nSTACKOVERFLOW NWP \n16 \n\nD.4 BEST PERFORMING HYPERPARAMETERS \n\n\n\nTable 8 :\n8\n\nTable 10 :\n10Test set performance for Stack Overflow tasks after training: Accuracy for NWP and Recall@5 (\u00d7100) for LR. Performance within within 0.5% of the best result are shown in bold.FED... \nADAGRAD ADAM YOGI AVGM AVG \n\nSTACK OVERFLOW NWP 24.4 \n25.7 25.7 \n24.5 20.5 \n\nSTACK OVERFLOW LR \n66.8 \n65.2 66.5 \n46.5 40.6 \n\nE ADDITIONAL EXPERIMENTAL RESULTS \n\nE.1 RESULTS ON EMNIST CR \n\n\n\nTable 11 :\n11(Top) Test accuracy (%) of a model trained centrally with various optimizers. (Bottom) Average test accuracy (%) over the last 100 rounds of various federated optimizers on the EMNIST CR task, using constant learning rates or the EXPDECAY schedule for \u03b7 l . Accuracies (for the federated tasks) within 0.5% of the best result are shown in bold.ADAGRAD ADAM YOGI SGDM SGD \nCENTRALIZED 88.0 \n87.9 88.0 \n87.7 87.7 \n\nFED... \nADAGRAD ADAM YOGI AVGM AVG \n\nCONSTANT \u03b7 l 85.1 \n85.6 85.5 \n85.2 84.9 \nEXPDECAY \n85.3 \n86.2 86.2 \n85.8 85.2 \n\n\nhttps://github.com/google-research/federated/tree/ 780767fdf68f2f11814d41bbbfe708274eb6d8b3/optimization\n\nThe TensorFlow Federated Authors. TensorFlow Federated Stack Overflow dataset. The TensorFlow Federated Authors. TensorFlow Federated Stack Overflow dataset, 2019. URL https://www.tensorflow.org/federated/api_docs/python/tff/ simulation/datasets/stackoverflow/load_data.\n\nQsparse-local-SGD: Distributed SGD with quantization, sparsification and local computations. Debraj Basu, Deepesh Data, Can Karakus, Suhas Diggavi, Advances in Neural Information Processing Systems. Debraj Basu, Deepesh Data, Can Karakus, and Suhas Diggavi. Qsparse-local-SGD: Distributed SGD with quantization, sparsification and local computations. In Advances in Neural Information Processing Systems, pp. 14668-14679, 2019.\n\nTowards federated learning at scale: System design. Keith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chlo\u00e9 Kiddon, Jakub Kone\u010dn\u00fd, Stefano Mazzocchi, Brendan Mcmahan, Timon Van Overveldt, David Petrou, Daniel Ramage, Jason Roselander, Proceedings of Machine Learning and Systems. A. Talwalkar, V. Smith, and M. ZahariaMachine Learning and Systems1Proceedings of MLSysKeith Bonawitz, Hubert Eichner, Wolfgang Grieskamp, Dzmitry Huba, Alex Ingerman, Vladimir Ivanov, Chlo\u00e9 Kiddon, Jakub Kone\u010dn\u00fd, Stefano Mazzocchi, Brendan McMahan, Timon Van Overveldt, David Petrou, Daniel Ramage, and Jason Roselander. Towards feder- ated learning at scale: System design. In A. Talwalkar, V. Smith, and M. Zaharia (eds.), Proceedings of Machine Learning and Systems, volume 1, pp. 374-388. Proceed- ings of MLSys, 2019. URL https://proceedings.mlsys.org/paper/2019/file/ bd686fd640be98efaae0091fa301e613-Paper.pdf.\n\nLEAF: A benchmark for federated settings. Sebastian Caldas, Peter Wu, Tian Li, Jakub Kone\u010dn\u00fd, Brendan Mcmahan, Virginia Smith, Ameet Talwalkar, arXiv:1812.01097arXiv preprintSebastian Caldas, Peter Wu, Tian Li, Jakub Kone\u010dn\u00fd, H Brendan McMahan, Virginia Smith, and Ameet Talwalkar. LEAF: A benchmark for federated settings. arXiv preprint arXiv:1812.01097, 2018.\n\nEMNIST: Extending MNIST to handwritten letters. Gregory Cohen, Saeed Afshar, Jonathan Tapson, Andre Van Schaik, 2017 International Joint Conference on Neural Networks (IJCNN). IEEEGregory Cohen, Saeed Afshar, Jonathan Tapson, and Andre Van Schaik. EMNIST: Extending MNIST to handwritten letters. In 2017 International Joint Conference on Neural Networks (IJCNN), pp. 2921-2926. IEEE, 2017.\n\nOn the ineffectiveness of variance reduced optimization for deep learning. Aaron Defazio, L\u00e9on Bottou, arXiv:1812.04529arXiv preprintAaron Defazio and L\u00e9on Bottou. On the ineffectiveness of variance reduced optimization for deep learning. arXiv preprint arXiv:1812.04529, 2018.\n\nSAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. Aaron Defazio, Francis Bach, Simon Lacoste-Julien, NIPS. Aaron Defazio, Francis Bach, and Simon Lacoste-Julien. SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives. In NIPS, pp. 1646-1654, 2014.\n\nAdaptive subgradient methods for online learning and stochastic optimization. John Duchi, Elad Hazan, Yoram Singer, Journal of Machine Learning Research. 12John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121-2159, 2011.\n\nPriya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, arXiv:1706.02677Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet in 1 hour. arXiv preprintPriya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: Training ImageNet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.\n", "annotations": {"author": "[{\"end\":88,\"start\":35},{\"end\":146,\"start\":89},{\"end\":203,\"start\":147},{\"end\":261,\"start\":204},{\"end\":308,\"start\":262},{\"end\":341,\"start\":309},{\"end\":392,\"start\":342},{\"end\":448,\"start\":393}]", "publisher": null, "author_last_name": "[{\"end\":50,\"start\":45},{\"end\":104,\"start\":97},{\"end\":160,\"start\":154},{\"end\":219,\"start\":212},{\"end\":272,\"start\":268},{\"end\":322,\"start\":315},{\"end\":354,\"start\":349},{\"end\":410,\"start\":403}]", "author_first_name": "[{\"end\":42,\"start\":35},{\"end\":44,\"start\":43},{\"end\":96,\"start\":89},{\"end\":153,\"start\":147},{\"end\":211,\"start\":204},{\"end\":267,\"start\":262},{\"end\":314,\"start\":309},{\"end\":348,\"start\":342},{\"end\":394,\"start\":393},{\"end\":402,\"start\":395}]", "author_affiliation": "[{\"end\":87,\"start\":71},{\"end\":145,\"start\":129},{\"end\":202,\"start\":186},{\"end\":260,\"start\":244},{\"end\":307,\"start\":291},{\"end\":340,\"start\":324},{\"end\":391,\"start\":375},{\"end\":447,\"start\":431}]", "title": "[{\"end\":32,\"start\":1},{\"end\":480,\"start\":449}]", "venue": null, "abstract": "[{\"end\":3121,\"start\":526}]", "bib_ref": "[{\"end\":3311,\"start\":3289},{\"end\":4019,\"start\":3998},{\"end\":4041,\"start\":4024},{\"end\":4462,\"start\":4440},{\"end\":4770,\"start\":4744},{\"end\":4787,\"start\":4770},{\"end\":4878,\"start\":4852},{\"end\":5173,\"start\":5152},{\"end\":6036,\"start\":6009},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6055,\"start\":6036},{\"end\":6073,\"start\":6055},{\"end\":6118,\"start\":6098},{\"end\":6136,\"start\":6118},{\"end\":6152,\"start\":6136},{\"end\":6174,\"start\":6154},{\"end\":6194,\"start\":6174},{\"end\":6417,\"start\":6400},{\"end\":6579,\"start\":6558},{\"end\":8719,\"start\":8699},{\"end\":8737,\"start\":8719},{\"end\":8757,\"start\":8737},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9057,\"start\":9040},{\"end\":9075,\"start\":9057},{\"end\":9297,\"start\":9275},{\"end\":10568,\"start\":10566},{\"end\":11632,\"start\":11612},{\"end\":11661,\"start\":11634},{\"end\":14381,\"start\":14352},{\"end\":18220,\"start\":18194},{\"end\":18296,\"start\":18276},{\"end\":18618,\"start\":18597},{\"end\":20034,\"start\":20008},{\"end\":21252,\"start\":21224},{\"end\":21830,\"start\":21808},{\"end\":21896,\"start\":21875},{\"end\":24021,\"start\":24000},{\"end\":26216,\"start\":26193},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":26328,\"start\":26304},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26363,\"start\":26342},{\"end\":27211,\"start\":27191},{\"end\":46751,\"start\":46730},{\"end\":49664,\"start\":49642},{\"end\":51095,\"start\":51070},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":54469,\"start\":54450},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":55963,\"start\":55940},{\"end\":57155,\"start\":57134},{\"end\":57682,\"start\":57667},{\"end\":58279,\"start\":58275},{\"end\":58282,\"start\":58279},{\"end\":58284,\"start\":58282},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":58707,\"start\":58686},{\"end\":59015,\"start\":58994},{\"end\":68323,\"start\":68287},{\"end\":71743,\"start\":71723},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":73807,\"start\":73787},{\"end\":74480,\"start\":74459},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":81138,\"start\":81119}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":79919,\"start\":79673},{\"attributes\":{\"id\":\"fig_1\"},\"end\":80138,\"start\":79920},{\"attributes\":{\"id\":\"fig_2\"},\"end\":80371,\"start\":80139},{\"attributes\":{\"id\":\"fig_3\"},\"end\":80805,\"start\":80372},{\"attributes\":{\"id\":\"fig_4\"},\"end\":81116,\"start\":80806},{\"attributes\":{\"id\":\"fig_5\"},\"end\":81372,\"start\":81117},{\"attributes\":{\"id\":\"fig_6\"},\"end\":81626,\"start\":81373},{\"attributes\":{\"id\":\"fig_7\"},\"end\":81881,\"start\":81627},{\"attributes\":{\"id\":\"fig_10\"},\"end\":82902,\"start\":81882},{\"attributes\":{\"id\":\"fig_14\"},\"end\":83032,\"start\":82903},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":83795,\"start\":83033},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":84543,\"start\":83796},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":90244,\"start\":84544},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":90580,\"start\":90245},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":90900,\"start\":90581},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":91363,\"start\":90901},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":92195,\"start\":91364},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":92492,\"start\":92196},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":92722,\"start\":92493},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":92735,\"start\":92723},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":93121,\"start\":92736},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":93666,\"start\":93122}]", "paragraph": "[{\"end\":4042,\"start\":3137},{\"end\":5312,\"start\":4044},{\"end\":7139,\"start\":5314},{\"end\":7342,\"start\":7141},{\"end\":7444,\"start\":7376},{\"end\":7890,\"start\":7485},{\"end\":8030,\"start\":7892},{\"end\":8122,\"start\":8032},{\"end\":8267,\"start\":8209},{\"end\":8520,\"start\":8349},{\"end\":9208,\"start\":8522},{\"end\":9670,\"start\":9210},{\"end\":9853,\"start\":9672},{\"end\":10369,\"start\":9910},{\"end\":10474,\"start\":10371},{\"end\":10507,\"start\":10476},{\"end\":10522,\"start\":10509},{\"end\":10599,\"start\":10529},{\"end\":10656,\"start\":10601},{\"end\":10660,\"start\":10658},{\"end\":10715,\"start\":10662},{\"end\":10767,\"start\":10750},{\"end\":10805,\"start\":10769},{\"end\":11663,\"start\":10807},{\"end\":12235,\"start\":11665},{\"end\":12632,\"start\":12271},{\"end\":13197,\"start\":12634},{\"end\":13299,\"start\":13199},{\"end\":13437,\"start\":13408},{\"end\":13452,\"start\":13439},{\"end\":13498,\"start\":13459},{\"end\":13530,\"start\":13500},{\"end\":13587,\"start\":13532},{\"end\":13591,\"start\":13589},{\"end\":13932,\"start\":13929},{\"end\":14598,\"start\":13959},{\"end\":14777,\"start\":14733},{\"end\":14862,\"start\":14779},{\"end\":15026,\"start\":15011},{\"end\":15761,\"start\":15219},{\"end\":15973,\"start\":15877},{\"end\":16310,\"start\":15975},{\"end\":16358,\"start\":16357},{\"end\":16412,\"start\":16360},{\"end\":17001,\"start\":16611},{\"end\":17612,\"start\":17117},{\"end\":17894,\"start\":17614},{\"end\":18619,\"start\":17896},{\"end\":19285,\"start\":18621},{\"end\":19726,\"start\":19287},{\"end\":20295,\"start\":19728},{\"end\":20607,\"start\":20297},{\"end\":21959,\"start\":20665},{\"end\":22584,\"start\":21961},{\"end\":23124,\"start\":22586},{\"end\":23581,\"start\":23126},{\"end\":25451,\"start\":23648},{\"end\":27482,\"start\":25453},{\"end\":27891,\"start\":27518},{\"end\":28706,\"start\":27906},{\"end\":28874,\"start\":28708},{\"end\":29155,\"start\":28919},{\"end\":29202,\"start\":29157},{\"end\":29226,\"start\":29204},{\"end\":29274,\"start\":29228},{\"end\":30039,\"start\":29276},{\"end\":30146,\"start\":30066},{\"end\":30260,\"start\":30204},{\"end\":30565,\"start\":30413},{\"end\":30994,\"start\":30950},{\"end\":31044,\"start\":30996},{\"end\":31842,\"start\":31671},{\"end\":32160,\"start\":32026},{\"end\":32541,\"start\":32374},{\"end\":32951,\"start\":32787},{\"end\":33367,\"start\":33334},{\"end\":33676,\"start\":33369},{\"end\":33822,\"start\":33785},{\"end\":33975,\"start\":33865},{\"end\":34541,\"start\":34373},{\"end\":34973,\"start\":34859},{\"end\":35333,\"start\":35285},{\"end\":35522,\"start\":35469},{\"end\":35934,\"start\":35833},{\"end\":36030,\"start\":35936},{\"end\":36515,\"start\":36392},{\"end\":36747,\"start\":36622},{\"end\":37065,\"start\":36990},{\"end\":37375,\"start\":37270},{\"end\":37447,\"start\":37377},{\"end\":37740,\"start\":37679},{\"end\":37932,\"start\":37871},{\"end\":38207,\"start\":38135},{\"end\":39156,\"start\":39040},{\"end\":39215,\"start\":39158},{\"end\":39668,\"start\":39564},{\"end\":39970,\"start\":39810},{\"end\":40388,\"start\":40002},{\"end\":41626,\"start\":41206},{\"end\":41849,\"start\":41653},{\"end\":41988,\"start\":41899},{\"end\":42247,\"start\":42086},{\"end\":42602,\"start\":42551},{\"end\":43185,\"start\":43131},{\"end\":43509,\"start\":43408},{\"end\":43668,\"start\":43610},{\"end\":43824,\"start\":43771},{\"end\":44010,\"start\":43936},{\"end\":44094,\"start\":44012},{\"end\":44374,\"start\":44312},{\"end\":44673,\"start\":44616},{\"end\":44742,\"start\":44675},{\"end\":45127,\"start\":45029},{\"end\":45333,\"start\":45305},{\"end\":45442,\"start\":45335},{\"end\":45509,\"start\":45500},{\"end\":45820,\"start\":45766},{\"end\":46095,\"start\":45902},{\"end\":46310,\"start\":46238},{\"end\":46392,\"start\":46312},{\"end\":46563,\"start\":46440},{\"end\":47202,\"start\":46658},{\"end\":47797,\"start\":47352},{\"end\":48243,\"start\":47799},{\"end\":48803,\"start\":48353},{\"end\":49017,\"start\":48854},{\"end\":49772,\"start\":49193},{\"end\":50090,\"start\":49774},{\"end\":50592,\"start\":50209},{\"end\":51570,\"start\":51028},{\"end\":52087,\"start\":51572},{\"end\":52532,\"start\":52089},{\"end\":53079,\"start\":52534},{\"end\":53227,\"start\":53147},{\"end\":54678,\"start\":53620},{\"end\":55964,\"start\":54680},{\"end\":56806,\"start\":55987},{\"end\":57537,\"start\":56808},{\"end\":57879,\"start\":57539},{\"end\":58535,\"start\":57881},{\"end\":59401,\"start\":58550},{\"end\":62080,\"start\":59421},{\"end\":62472,\"start\":62082},{\"end\":62912,\"start\":62474},{\"end\":63599,\"start\":62941},{\"end\":64291,\"start\":63601},{\"end\":64322,\"start\":64293},{\"end\":65489,\"start\":64351},{\"end\":66699,\"start\":66100},{\"end\":67175,\"start\":66701},{\"end\":67772,\"start\":67219},{\"end\":69556,\"start\":67805},{\"end\":70665,\"start\":69621},{\"end\":71236,\"start\":70667},{\"end\":71999,\"start\":71238},{\"end\":72779,\"start\":72046},{\"end\":73404,\"start\":72834},{\"end\":74369,\"start\":73406},{\"end\":75062,\"start\":74406},{\"end\":75748,\"start\":75064},{\"end\":76213,\"start\":75750},{\"end\":76775,\"start\":76215},{\"end\":77673,\"start\":76777},{\"end\":78190,\"start\":77675},{\"end\":78644,\"start\":78192},{\"end\":79672,\"start\":78646}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7484,\"start\":7445},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8208,\"start\":8123},{\"attributes\":{\"id\":\"formula_2\"},\"end\":8348,\"start\":8268},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9909,\"start\":9854},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10749,\"start\":10716},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13407,\"start\":13300},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13928,\"start\":13592},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13958,\"start\":13933},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14732,\"start\":14599},{\"attributes\":{\"id\":\"formula_9\"},\"end\":15010,\"start\":14863},{\"attributes\":{\"id\":\"formula_10\"},\"end\":15218,\"start\":15027},{\"attributes\":{\"id\":\"formula_11\"},\"end\":15876,\"start\":15762},{\"attributes\":{\"id\":\"formula_12\"},\"end\":16356,\"start\":16311},{\"attributes\":{\"id\":\"formula_13\"},\"end\":16610,\"start\":16413},{\"attributes\":{\"id\":\"formula_14\"},\"end\":17116,\"start\":17002},{\"attributes\":{\"id\":\"formula_15\"},\"end\":30203,\"start\":30147},{\"attributes\":{\"id\":\"formula_16\"},\"end\":30412,\"start\":30261},{\"attributes\":{\"id\":\"formula_17\"},\"end\":30949,\"start\":30566},{\"attributes\":{\"id\":\"formula_18\"},\"end\":31670,\"start\":31045},{\"attributes\":{\"id\":\"formula_19\"},\"end\":32025,\"start\":31843},{\"attributes\":{\"id\":\"formula_20\"},\"end\":32373,\"start\":32161},{\"attributes\":{\"id\":\"formula_21\"},\"end\":32786,\"start\":32542},{\"attributes\":{\"id\":\"formula_22\"},\"end\":33333,\"start\":32952},{\"attributes\":{\"id\":\"formula_23\"},\"end\":33784,\"start\":33677},{\"attributes\":{\"id\":\"formula_24\"},\"end\":33864,\"start\":33823},{\"attributes\":{\"id\":\"formula_25\"},\"end\":34372,\"start\":33976},{\"attributes\":{\"id\":\"formula_26\"},\"end\":34858,\"start\":34542},{\"attributes\":{\"id\":\"formula_27\"},\"end\":35284,\"start\":34974},{\"attributes\":{\"id\":\"formula_28\"},\"end\":35468,\"start\":35334},{\"attributes\":{\"id\":\"formula_29\"},\"end\":35832,\"start\":35523},{\"attributes\":{\"id\":\"formula_30\"},\"end\":36391,\"start\":36031},{\"attributes\":{\"id\":\"formula_31\"},\"end\":36621,\"start\":36516},{\"attributes\":{\"id\":\"formula_32\"},\"end\":36989,\"start\":36748},{\"attributes\":{\"id\":\"formula_33\"},\"end\":37269,\"start\":37066},{\"attributes\":{\"id\":\"formula_34\"},\"end\":37678,\"start\":37448},{\"attributes\":{\"id\":\"formula_35\"},\"end\":37870,\"start\":37741},{\"attributes\":{\"id\":\"formula_36\"},\"end\":38134,\"start\":37933},{\"attributes\":{\"id\":\"formula_37\"},\"end\":39039,\"start\":38208},{\"attributes\":{\"id\":\"formula_38\"},\"end\":39563,\"start\":39216},{\"attributes\":{\"id\":\"formula_39\"},\"end\":39809,\"start\":39669},{\"attributes\":{\"id\":\"formula_40\"},\"end\":41205,\"start\":40389},{\"attributes\":{\"id\":\"formula_41\"},\"end\":41898,\"start\":41850},{\"attributes\":{\"id\":\"formula_42\"},\"end\":42085,\"start\":41989},{\"attributes\":{\"id\":\"formula_43\"},\"end\":42550,\"start\":42248},{\"attributes\":{\"id\":\"formula_44\"},\"end\":43130,\"start\":42603},{\"attributes\":{\"id\":\"formula_45\"},\"end\":43407,\"start\":43186},{\"attributes\":{\"id\":\"formula_46\"},\"end\":43609,\"start\":43510},{\"attributes\":{\"id\":\"formula_47\"},\"end\":43770,\"start\":43669},{\"attributes\":{\"id\":\"formula_48\"},\"end\":43935,\"start\":43825},{\"attributes\":{\"id\":\"formula_49\"},\"end\":44311,\"start\":44095},{\"attributes\":{\"id\":\"formula_50\"},\"end\":44615,\"start\":44375},{\"attributes\":{\"id\":\"formula_51\"},\"end\":45028,\"start\":44743},{\"attributes\":{\"id\":\"formula_52\"},\"end\":45304,\"start\":45128},{\"attributes\":{\"id\":\"formula_53\"},\"end\":45499,\"start\":45443},{\"attributes\":{\"id\":\"formula_54\"},\"end\":45765,\"start\":45510},{\"attributes\":{\"id\":\"formula_55\"},\"end\":45901,\"start\":45821},{\"attributes\":{\"id\":\"formula_56\"},\"end\":46237,\"start\":46096},{\"attributes\":{\"id\":\"formula_57\"},\"end\":46439,\"start\":46393},{\"attributes\":{\"id\":\"formula_58\"},\"end\":47351,\"start\":47203},{\"attributes\":{\"id\":\"formula_59\"},\"end\":48352,\"start\":48244},{\"attributes\":{\"id\":\"formula_60\"},\"end\":49192,\"start\":49018},{\"attributes\":{\"id\":\"formula_61\"},\"end\":51027,\"start\":50593},{\"attributes\":{\"id\":\"formula_62\"},\"end\":53570,\"start\":53228},{\"attributes\":{\"id\":\"formula_63\"},\"end\":65924,\"start\":65490},{\"attributes\":{\"id\":\"formula_64\"},\"end\":66099,\"start\":65924}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":3651,\"start\":3644},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":23369,\"start\":23362},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":56131,\"start\":56124},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":56318,\"start\":56311},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":59181,\"start\":59174},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":59400,\"start\":59393},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":60625,\"start\":60618},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":61398,\"start\":61391},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":64676,\"start\":64669},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":64914,\"start\":64901},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":64949,\"start\":64942},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":64998,\"start\":64991},{\"end\":65029,\"start\":65022},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":65308,\"start\":65301},{\"end\":65371,\"start\":65364},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":65488,\"start\":65481},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":67771,\"start\":67763},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":73173,\"start\":73165},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":73252,\"start\":73245},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":73817,\"start\":73809}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3135,\"start\":3123},{\"attributes\":{\"n\":\"2\"},\"end\":7374,\"start\":7345},{\"end\":10527,\"start\":10525},{\"attributes\":{\"n\":\"3\"},\"end\":12269,\"start\":12238},{\"end\":13457,\"start\":13455},{\"attributes\":{\"n\":\"4\"},\"end\":20663,\"start\":20610},{\"attributes\":{\"n\":\"5\"},\"end\":23616,\"start\":23584},{\"attributes\":{\"n\":\"5.1\"},\"end\":23646,\"start\":23619},{\"attributes\":{\"n\":\"5.2\"},\"end\":27499,\"start\":27485},{\"attributes\":{\"n\":\"5.3\"},\"end\":27516,\"start\":27502},{\"attributes\":{\"n\":\"6\"},\"end\":27904,\"start\":27894},{\"end\":28895,\"start\":28877},{\"end\":28917,\"start\":28898},{\"end\":30064,\"start\":30042},{\"end\":40000,\"start\":39973},{\"end\":41651,\"start\":41629},{\"end\":46656,\"start\":46566},{\"end\":48852,\"start\":48806},{\"end\":50207,\"start\":50093},{\"end\":53145,\"start\":53082},{\"end\":53618,\"start\":53572},{\"end\":55985,\"start\":55967},{\"end\":58548,\"start\":58538},{\"end\":59419,\"start\":59404},{\"end\":62939,\"start\":62915},{\"end\":64349,\"start\":64325},{\"end\":67217,\"start\":67178},{\"end\":67803,\"start\":67775},{\"end\":69619,\"start\":69559},{\"end\":72044,\"start\":72002},{\"end\":72832,\"start\":72782},{\"end\":74404,\"start\":74372},{\"end\":79684,\"start\":79674},{\"end\":79931,\"start\":79921},{\"end\":80150,\"start\":80140},{\"end\":80381,\"start\":80373},{\"end\":81384,\"start\":81374},{\"end\":81638,\"start\":81628},{\"end\":81924,\"start\":81883},{\"end\":82915,\"start\":82904},{\"end\":83804,\"start\":83797},{\"end\":90255,\"start\":90246},{\"end\":90591,\"start\":90582},{\"end\":90911,\"start\":90902},{\"end\":91374,\"start\":91365},{\"end\":92206,\"start\":92197},{\"end\":92503,\"start\":92494},{\"end\":92733,\"start\":92724},{\"end\":92747,\"start\":92737},{\"end\":93133,\"start\":93123}]", "table": "[{\"end\":83795,\"start\":83318},{\"end\":84543,\"start\":83994},{\"end\":90244,\"start\":86062},{\"end\":90580,\"start\":90276},{\"end\":90900,\"start\":90680},{\"end\":91363,\"start\":90961},{\"end\":92195,\"start\":91407},{\"end\":92492,\"start\":92263},{\"end\":92722,\"start\":92543},{\"end\":93121,\"start\":92925},{\"end\":93666,\"start\":93480}]", "figure_caption": "[{\"end\":79919,\"start\":79686},{\"end\":80138,\"start\":79933},{\"end\":80371,\"start\":80152},{\"end\":80805,\"start\":80383},{\"end\":81116,\"start\":80808},{\"end\":81372,\"start\":81119},{\"end\":81626,\"start\":81386},{\"end\":81881,\"start\":81640},{\"end\":82902,\"start\":81930},{\"end\":83032,\"start\":82918},{\"end\":83318,\"start\":83035},{\"end\":83994,\"start\":83806},{\"end\":86062,\"start\":84546},{\"end\":90276,\"start\":90257},{\"end\":90680,\"start\":90593},{\"end\":90961,\"start\":90913},{\"end\":91407,\"start\":91376},{\"end\":92263,\"start\":92208},{\"end\":92543,\"start\":92505},{\"end\":92925,\"start\":92750},{\"end\":93480,\"start\":93136}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":17585,\"start\":17577},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":20218,\"start\":20210},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":23826,\"start\":23818},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":24568,\"start\":24560},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":26954,\"start\":26946},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27078,\"start\":27070},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":35971,\"start\":35961},{\"end\":47000,\"start\":46998},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":53065,\"start\":53057},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":64896,\"start\":64881},{\"end\":66172,\"start\":66164},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":66501,\"start\":66493},{\"end\":66698,\"start\":66690},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":67965,\"start\":67957},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":68286,\"start\":68274},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":69304,\"start\":69295},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":70226,\"start\":70217},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":70238,\"start\":70229},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":72546,\"start\":72537},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":72558,\"start\":72549},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":79080,\"start\":79071}]", "bib_author_first_name": "[{\"end\":94144,\"start\":94138},{\"end\":94158,\"start\":94151},{\"end\":94168,\"start\":94165},{\"end\":94183,\"start\":94178},{\"end\":94531,\"start\":94526},{\"end\":94548,\"start\":94542},{\"end\":94566,\"start\":94558},{\"end\":94585,\"start\":94578},{\"end\":94596,\"start\":94592},{\"end\":94615,\"start\":94607},{\"end\":94629,\"start\":94624},{\"end\":94643,\"start\":94638},{\"end\":94660,\"start\":94653},{\"end\":94679,\"start\":94672},{\"end\":94694,\"start\":94689},{\"end\":94715,\"start\":94710},{\"end\":94730,\"start\":94724},{\"end\":94744,\"start\":94739},{\"end\":95473,\"start\":95464},{\"end\":95487,\"start\":95482},{\"end\":95496,\"start\":95492},{\"end\":95506,\"start\":95501},{\"end\":95523,\"start\":95516},{\"end\":95541,\"start\":95533},{\"end\":95554,\"start\":95549},{\"end\":95841,\"start\":95834},{\"end\":95854,\"start\":95849},{\"end\":95871,\"start\":95863},{\"end\":95885,\"start\":95880},{\"end\":96257,\"start\":96252},{\"end\":96271,\"start\":96267},{\"end\":96561,\"start\":96556},{\"end\":96578,\"start\":96571},{\"end\":96590,\"start\":96585},{\"end\":96881,\"start\":96877},{\"end\":96893,\"start\":96889},{\"end\":96906,\"start\":96901},{\"end\":97144,\"start\":97139},{\"end\":97157,\"start\":97152},{\"end\":97170,\"start\":97166},{\"end\":97187,\"start\":97181},{\"end\":97205,\"start\":97199},{\"end\":97222,\"start\":97218},{\"end\":97237,\"start\":97231}]", "bib_author_last_name": "[{\"end\":94149,\"start\":94145},{\"end\":94163,\"start\":94159},{\"end\":94176,\"start\":94169},{\"end\":94191,\"start\":94184},{\"end\":94540,\"start\":94532},{\"end\":94556,\"start\":94549},{\"end\":94576,\"start\":94567},{\"end\":94590,\"start\":94586},{\"end\":94605,\"start\":94597},{\"end\":94622,\"start\":94616},{\"end\":94636,\"start\":94630},{\"end\":94651,\"start\":94644},{\"end\":94670,\"start\":94661},{\"end\":94687,\"start\":94680},{\"end\":94708,\"start\":94695},{\"end\":94722,\"start\":94716},{\"end\":94737,\"start\":94731},{\"end\":94755,\"start\":94745},{\"end\":95480,\"start\":95474},{\"end\":95490,\"start\":95488},{\"end\":95499,\"start\":95497},{\"end\":95514,\"start\":95507},{\"end\":95531,\"start\":95524},{\"end\":95547,\"start\":95542},{\"end\":95564,\"start\":95555},{\"end\":95847,\"start\":95842},{\"end\":95861,\"start\":95855},{\"end\":95878,\"start\":95872},{\"end\":95896,\"start\":95886},{\"end\":96265,\"start\":96258},{\"end\":96278,\"start\":96272},{\"end\":96569,\"start\":96562},{\"end\":96583,\"start\":96579},{\"end\":96605,\"start\":96591},{\"end\":96887,\"start\":96882},{\"end\":96899,\"start\":96894},{\"end\":96913,\"start\":96907},{\"end\":97150,\"start\":97145},{\"end\":97164,\"start\":97158},{\"end\":97179,\"start\":97171},{\"end\":97197,\"start\":97188},{\"end\":97216,\"start\":97206},{\"end\":97229,\"start\":97223},{\"end\":97245,\"start\":97238}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":94043,\"start\":93773},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":174803546},\"end\":94472,\"start\":94045},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":59599820},\"end\":95420,\"start\":94474},{\"attributes\":{\"doi\":\"arXiv:1812.01097\",\"id\":\"b3\"},\"end\":95784,\"start\":95422},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":30587588},\"end\":96175,\"start\":95786},{\"attributes\":{\"doi\":\"arXiv:1812.04529\",\"id\":\"b5\"},\"end\":96454,\"start\":96177},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":218654665},\"end\":96797,\"start\":96456},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":538820},\"end\":97137,\"start\":96799},{\"attributes\":{\"doi\":\"arXiv:1706.02677\",\"id\":\"b8\"},\"end\":97603,\"start\":97139}]", "bib_title": "[{\"end\":94136,\"start\":94045},{\"end\":94524,\"start\":94474},{\"end\":95832,\"start\":95786},{\"end\":96554,\"start\":96456},{\"end\":96875,\"start\":96799}]", "bib_author": "[{\"end\":94151,\"start\":94138},{\"end\":94165,\"start\":94151},{\"end\":94178,\"start\":94165},{\"end\":94193,\"start\":94178},{\"end\":94542,\"start\":94526},{\"end\":94558,\"start\":94542},{\"end\":94578,\"start\":94558},{\"end\":94592,\"start\":94578},{\"end\":94607,\"start\":94592},{\"end\":94624,\"start\":94607},{\"end\":94638,\"start\":94624},{\"end\":94653,\"start\":94638},{\"end\":94672,\"start\":94653},{\"end\":94689,\"start\":94672},{\"end\":94710,\"start\":94689},{\"end\":94724,\"start\":94710},{\"end\":94739,\"start\":94724},{\"end\":94757,\"start\":94739},{\"end\":95482,\"start\":95464},{\"end\":95492,\"start\":95482},{\"end\":95501,\"start\":95492},{\"end\":95516,\"start\":95501},{\"end\":95533,\"start\":95516},{\"end\":95549,\"start\":95533},{\"end\":95566,\"start\":95549},{\"end\":95849,\"start\":95834},{\"end\":95863,\"start\":95849},{\"end\":95880,\"start\":95863},{\"end\":95898,\"start\":95880},{\"end\":96267,\"start\":96252},{\"end\":96280,\"start\":96267},{\"end\":96571,\"start\":96556},{\"end\":96585,\"start\":96571},{\"end\":96607,\"start\":96585},{\"end\":96889,\"start\":96877},{\"end\":96901,\"start\":96889},{\"end\":96915,\"start\":96901},{\"end\":97152,\"start\":97139},{\"end\":97166,\"start\":97152},{\"end\":97181,\"start\":97166},{\"end\":97199,\"start\":97181},{\"end\":97218,\"start\":97199},{\"end\":97231,\"start\":97218},{\"end\":97247,\"start\":97231}]", "bib_venue": "[{\"end\":94868,\"start\":94840},{\"end\":93850,\"start\":93773},{\"end\":94242,\"start\":94193},{\"end\":94800,\"start\":94757},{\"end\":95462,\"start\":95422},{\"end\":95960,\"start\":95898},{\"end\":96250,\"start\":96177},{\"end\":96611,\"start\":96607},{\"end\":96951,\"start\":96915},{\"end\":97351,\"start\":97263}]"}}}, "year": 2023, "month": 12, "day": 17}