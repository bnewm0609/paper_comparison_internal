{"id": 227210156, "updated": "2023-10-06 08:08:09.899", "metadata": {"title": "Patch-VQ: 'Patching Up' the Video Quality Problem", "authors": "[{\"first\":\"Zhenqiang\",\"last\":\"Ying\",\"middle\":[]},{\"first\":\"Maniratnam\",\"last\":\"Mandal\",\"middle\":[]},{\"first\":\"Deepti\",\"last\":\"Ghadiyaram\",\"middle\":[]},{\"first\":\"Alan\",\"last\":\"Austin\",\"middle\":[\"Bovik\",\"University\",\"of\",\"Texas\",\"at\"]},{\"first\":\"AI\",\"last\":\"Facebook\",\"middle\":[]}]", "venue": "ArXiv", "journal": "2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "No-reference (NR) perceptual video quality assessment (VQA) is a complex, unsolved, and important problem to social and streaming media applications. Efficient and accurate video quality predictors are needed to monitor and guide the processing of billions of shared, often imperfect, user-generated content (UGC). Unfortunately, current NR models are limited in their prediction capabilities on real-world,\"in-the-wild\"UGC video data. To advance progress on this problem, we created the largest (by far) subjective video quality dataset, containing 39, 000 realworld distorted videos and 117, 000 space-time localized video patches ('v-patches'), and 5.5M human perceptual quality annotations. Using this, we created two unique NR-VQA models: (a) a local-to-global region-based NR VQA architecture (called PVQ) that learns to predict global video quality and achieves state-of-the-art performance on 3 UGC datasets, and (b) a first-of-a-kind space-time video quality mapping engine (called PVQ Mapper) that helps localize and visualize perceptual distortions in space and time. We will make the new database and prediction models available immediately following the review process.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2011.13544", "mag": "3108337142", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/YingMGB21", "doi": "10.1109/cvpr46437.2021.01380"}}, "content": {"source": {"pdf_hash": "2e41d8b57af02799b4f9e89977f62c748eb995fe", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2011.13544v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2011.13544", "status": "GREEN"}}, "grobid": {"id": "cd2691400e16fd725c6c9468995b50afdd40c40f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/2e41d8b57af02799b4f9e89977f62c748eb995fe.txt", "contents": "\nPatch-VQ: 'Patching Up' the Video Quality Problem\n\n\nZhenqiang Ying \nUniversity of Texas at Austin\n\n\nManiratnam Mandal mmandal@utexas.edu \nUniversity of Texas at Austin\n\n\nDeepti Ghadiyaram deeptigp@fb.com \nFacebook AI\n\n\n\u2020 \nAlan Bovik bovik@ece.utexas.edu \nUniversity of Texas at Austin\n\n\nPatch-VQ: 'Patching Up' the Video Quality Problem\n\nNo-reference (NR) perceptual video quality assessment (VQA) is a complex, unsolved, and important problem to social and streaming media applications. Efficient and accurate video quality predictors are needed to monitor and guide the processing of billions of shared, often imperfect, user-generated content (UGC). Unfortunately, current NR models are limited in their prediction capabilities on real-world, \"in-the-wild\" UGC video data. To advance progress on this problem, we created the largest (by far) subjective video quality dataset, containing 39, 000 realworld distorted videos and 117, 000 space-time localized video patches ('v-patches'), and 5.5M human perceptual quality annotations. Using this, we created two unique NR-VQA models: (a) a local-to-global region-based NR VQA architecture (called PVQ) that learns to predict global video quality and achieves state-of-the-art performance on 3 UGC datasets, and (b) a first-of-a-kind space-time video quality mapping engine (called PVQ Mapper) that helps localize and visualize perceptual distortions in space and time. We will make the new database and prediction models available immediately following the review process.\n\nIntroduction\n\nUser-generated content (UGC) and video streaming has exploded on social media platforms such as Facebook, Instagram, YouTube, and TikTok, each supporting millions and billions of users [1]. It has been estimated that each day, about 4 billion video views occur on Facebook [2] and 1 billion hours are viewed on YouTube [3]. Given the tremendous prevalence of Internet video, it would be of great value to measure and control the quality of UGC videos, both in capture devices and at social media sites where they are uploaded, encoded, processed, and analyzed.\n\nFull-reference (FR) video quality assessment (VQA) models perceptually compare quality against pristine videos, while no-reference (NR) models involve no such comparison. Thus, NR video quality monitoring could transform the processing and interpretation of videos on * \u2020 Equal contribution \u2021 The entity that conducted all of the data collection/experimentation. smartphones, social media, telemedicine, surveillance, and vision-guided robotics, in ways that FR models are unable to. Unfortunately, measuring video quality without a pristine reference is very hard. Hence, though FR models are successfully deployed at the largest scales [4], NR video quality prediction on UGC content remains largely unsolved, for several reasons. First, UGC video distortions arise from highly diverse capture conditions, unsteady hands of content creators, imperfect camera devices, processing and editing artifacts, frame rates, compression and transmission artifacts, and the way they are perceived by viewers. Inter-mixing of distortions is common, creating complex, composite distortions that are harder to model in videos. Moreover, it is wellknown that the technical degree of distortion (e.g. amount of blur, blocking, or noise) does not correlate well with perceptual quality [5], because of neurophysiological processes that induce masking [6]. Indeed, equal amounts of distortions may very differently affect the quality of two different videos [7].\n\nSecond, most existing video quality resources are too small and unrepresentative of the complex real-world distortions [8,9,10,11,12,13,14]. While three publicly avail-able databases of authentically distorted UGC videos are available [15,16,17], they are far too small to train modern, data-hungry deep neural networks. What is needed are very large databases of videos corrupted by real-world distortions, subjectively rated by large numbers of human viewers. However, conducting large-scale psychometric studies is much harder and time-consuming (per video) than standard object or action classification tasks.\n\nFinally, although a few NR algorithms achieve reasonable performance on small databases [18,19,20,21,22,23,24], most of them fail to account for the complex spacetime distortions common to UGC videos. UGC distortions are often transient (e.g., frame drops, focus changes, and transmission glitches) and yet may significantly impact the overall perceived quality of a video [25]. Most existing models are frame-based, or use sample frame differences, and cannot capture diverse temporal impairments.\n\nWe have made recent progress towards addressing these challenges, by learning to model the relationships that exist between local and global spatio-temporal distortions and perceptual quality. We built a large-scale public UGC video dataset of unprecedented size, comprising full videos and three kinds of spatio-temporal video patches (Fig. 1), and we conducted an online visual psychometric study to gather large numbers of human subjective quality scores on them. This unique data collection allowed us to successfully learn to exploit interactions between local and global video quality perception and to create algorithms that accurately predict video quality and space-time quality maps. We summarize our contributions below:\n\n\u2022 We built the largest video quality database in existence. We sampled hundreds of thousands of open source Internet UGC digital videos to match the feature distributions of social media UGC videos. Our final collection includes 39, 000 real-world videos of diverse sizes, contents, and distortions, 26 times larger than the most recent UGC dataset [17]. We also extracted three types of v-patches from each video, yielding 117, 000 space-time video patches (\"v-patches\") in total (Sec. 3.1). \u2022 We conducted the largest subjective video quality study to date. Our final dataset consists of a total of 5.5M perceptual quality judgments on videos and v-patches from almost 6, 300 subjects, more than 9 times larger than any prior UGC video quality study (Sec. 3.2). \u2022 We created a state-of-the-art deep blind video quality predictor, using a deep neural architecture that computes 2D video features using PaQ2PiQ [29], in parallel with 3D features using ResNet3D [30]. The 2D and 3D features feed a time series regressor [31] that learns to accurately predict both global video, as well as local spacetime v-patch quality, by exploiting the relations between them. This new model, which we call Patch VQ (PVQ) achieves top performance on the new database as well as on smaller \"in-the-wild\" databases [16,15], without finetuning (Secs. 4.1 and 5.3). \u2022 We also create another unique prediction model that predicts first-of-a-kind space-time maps of video quality by learning global-to-local quality relationships. This second model, called the PVQ Mapper, helps localize, visualize, and act on video distortions (Sec. 5.2).\n\n\nRelated Work\n\nVideo Quality Datasets: Several public legacy video quality datasets [8,9,10,11,12,13,14] have been developed in the past decade. Each of these datasets comprise a small number of unique source videos (typically 10-15), which are manually distorted by one of a few synthetic impairments (e.g., Gaussian blur, compression, and transmission artifacts). Hence, these datasets are quite limited in terms of content diversity and distortion complexity, and do not capture the complex characteristics of UGC videos. Early \"inthe-wild\" datasets [27,28] included fewer than 100 unique contents, while more recent ones such as KoNViD-1k [15], LIVE-VQC [16], and YouTube-UGC [17] contain relatively more videos (500-1500 per dataset), yet insufficient to train deep models. A more recent dataset, FlickrVid-150k [32] claims to contain a large number of videos, yet, has the following notable drawbacks: (a) Only 5 quality ratings were collected on each video which, given the complexity of the task, are insufficient to compute reliable ground truth quality scores (at least 15-18 is recommended [33]). (b) the database is not publicly available, hence limiting its use for any experiments or to validate its statistical integrity. (c) the videos are all drawn from Flickr, which is largely populated by professional and advanced amateur photographers, hence is not representative of social media UGC content. Shallow NR VQA models: Early NR VQA models were distortion specific [34,35,36,37,38,39,40] and focused mostly on transmission and compression related artifacts. More recent and widely-used NR image quality prediction algorithms have been applied to frame difference statistics to create space-time video distortion models [18,21,41,42]. In all these models, handcrafted statistical features are used to train shallow regression models to predict perceptual video quality, achieving high performance on legacy datasets. Recently proposed models [22,23] use dozens or hundreds of such perceptually relevant features and achieve state-of-the-art performance on the leading UGC datasets, yet their predictive capability remains far below human performance. Deep NR VQA models: There is more progress in the development of top-performing deep models for NR image quality prediction [43,44,45,46,47,29,48,49], but relatively fewer deep NR-VQA models exist. The authors of [50] proposed a general-purpose NR VQA framework based on weakly supervised learning and a resam- pling strategy. The NR VSFA [24] model uses a CNN to extract frame-wise features followed by a gated recurrent unit to capture temporal features. These, and other attempts [51,52,50,24] mostly perform well on legacy datasets [9,11,14] and struggle on in-the-wild UGC datasets [16,17,15]. MLSP-VQA [32] reports high performance on [15], but their code is not available, and we have been unable to reproduce their reported results.\n\n\nLarge-Scale Dataset and Human Study\n\nNext, we present details of the newly constructed video quality dataset and the subjective quality study we conducted on it. The new database includes 39, 075 videos and 117, 225 \"v-patches\" extracted from them, on which we collected about 5.5M quality scores in total from around 6, 300 unique subjects. This new resource is significantly larger and more diverse than any legacy (synthetic distortion) databases [9,8,11,12] or in-the-wild crowd-sourced datasets [15,16,17] (26 times larger than [17]). We refer to the proposed dataset as the Large-Scale Social Video Quality (LSVQ) Database.\n\n\nBuilding the Dataset\n\n\nUGC-Like Data Collection and Sampling\n\nWe selected two large public UGC video repositories to source our data: the Internet Archive (IA) [53] and YFCC-100M [54], and collected a total of 400, 000 videos from them. Each video was randomly cropped to an average duration 7 seconds * using ffmpeg [55]. Sampling \"UGC-like\" videos: Our dataset distinguishes itself from other in-the-wild video datasets in several ways. First, unlike KoNViD-1k [15], we did not restrict the collected videos to have fixed resolutions or aspect ratios, making the proposed dataset much more representative of realworld content. Second, we did not apply scaling or further processing which could affect the quality of the content. Finally, to obtain \"UGC-like\" videos, we used a mixed integer programming method [56] to match a set of UGC feature histograms. Specifically, we computed the following 26 * Cropping to a fixed duration was not possible, since a video must begin with a key frame to be decoded properly.\n\nholistic spatial and temporal features on two video collections: (a) our aforementioned 400K video collection from IA and YFCC-100M and (b) 19K public, randomly selected videos from a social media website:\n\u2022 Absolute Luminance L = R + G + B.\n\u2022 Colorfulness using [57].\n\n\u2022 RMS Luminance Contrast [58].\n\n\u2022 Number of detected faces using [59].\n\n\u2022 Spatial Gaussian Derivative Filters (3 scales, 2 orientations) from Leung-Malik filter bank [60]. \u2022 Temporal Gaussian Derivatives (3 scales) first averaged along temporal dimension, followed by computing the mean and standard deviation along the spatial dimension.\n\nThe first five (spatial) features were computed on each frame, then the means and standard deviations of these features across all frames were obtained as the final features. As mentioned, we sampled and matched feature histograms and in the end, arrived at about 39,000 videos, with roughly equal amounts from IA and YFCC-100M. Fig.  2 shows 16 randomly selected video frames from LSVQ, while Fig. 3 plots the diverse sizes, aspect ratios and durations of the final set of videos. It is evident that we obtained a diverse UGC video dataset that is representative in content, resolution, aspect ratios, and distortions.  \n\n\nCropping Video-Patches\n\nTo closely study and model the relationship between global and local spatio-temporal qualities, we randomly cropped three different kinds of video patches or \"v-patches\" from each video: a spatial v-patch (sv-patch), a temporal v-patch (tv-patch), and a spatio-temporal v-patch (stvpatch). All three patches are videos obtained by cropping an original video in space, time, or both space and time, respectively ( Fig. 4). All v-patches have the same spatial aspect ratios as their source videos. Each sv-patch has the same temporal duration as their source videos, but cropped to 40% of spatial dimensions (16% of area). Each tvpatch has the same spatial size as its source, but clipped to 40% of temporal duration. Finally, each stv-patch was cropped to 40% along all three dimensions. Every v-patch is entirely contained within its source, but the volumetric overlap of each sv-patch and tv-patch with the same-source stv-patch did not exceed 25% (suppl. material). \n\n\nSubjective Quality Study\n\nAmazon Mechanical Turk (AMT) was used to collect human opinions on the videos and v-patches as in other studies [16,17,29,61,62]. We launched two separate AMT tasks -one for videos and the other for the three video patches. A total of 6, 284 subjects were allowed to participate on both tasks. On average, we collected 35 ratings on each video and v-patch. Subjects could participate in our study through desktops, laptops, or mobile devices.\n\n\nAMT Study Design\n\nThe human intelligence task (HIT) pipeline is shown in Fig.  5. Each task began with general instructions, followed by a related quiz to check subjects' comprehension of the instructions, which they had to pass to proceed further. During training, each subject rated 5 videos to become famil- iar with the interface and the task. Then, they entered the testing phase, in which they rated 90 videos. Each video was played only once, following which the subject rated the video quality on a scale of 0-100 by sliding a cursor along the rating bar (suppl. material). Subjects could report a video as inappropriate (violent or pornographic), static or incorrectly oriented. We ensured that each video was downloaded before playback to avoid rebuffering and stalling. At the end, each subject answered several survey questions about the study conditions and their demographics.\n\n\nSubject Rejection\n\nNext, we summarize the several checks we employed at various stages of the AMT task to identify and eliminate unreliable subjects [16,61] and participants with inadequate processing or network resources. During Instructions: If a participant's browser window resolution, version, zoom, and the time taken to load videos did not meet our requirements (suppl. material), they were not allowed to proceed. During Training: Although we ensured that each video was entirely downloaded prior to viewing, we also checked for any potential device-related video stalls. If the delay on any training video exceeded 2 seconds, or the total delay over the five training videos exceeded 5 seconds, the subject was not allowed to proceed (without prejudice). They were also stopped if a negative delay was detected (e.g., using plugins to speed up the video). During Task: At the middle of each subject's task, we checked for instability of the internet connection, and if more than 50% of the videos viewed until then had suffered from hardware stalls, the subject was disqualified. We also checked whether the subject had been giving similar quality scores to all videos, or was nudging the slider only slightly, both indicative of insincere ratings. Post task: In the test phase, of the 90 videos, 4, chosen at random, were repeated (seen twice at separate points), while another 4 were \"golden\" videos from KoNViD-1k [15], for which subjective ratings were available. After each task, we rejected a subject if their scores on the same repeated videos or on the gold standard videos were not similar enough.\n\nThrough all these careful checks, a total of 1,046 subjects were rejected over all sessions.\n\n\nData Cleaning\n\nFollowing subject rejection, we conducted extensive data cleaning: (1) We excluded all scores provided by the subjects who were blocked, or for whom > 50% of the videos stalled during a session.\n\n(2) We removed ratings given by people who did not wear their prescribed lenses during the study (1.13%), as uncorrected vision could affect perceived quality. (3) We applied ITU-R BT.500-14 [33] (Annex 1, Sec 2.3) standard rejection to screen the remaining subjects. This resulted in 301 subjects being rejected (about 2.6%). (4) To detect (and reject) outliers, we first calculated the kurtosis coefficient [63] of each score distribution, to determine normality. We then applied the Z-score method in [64] if the distribution deemed Gaussian-like, and the Tukey IQR method [65] otherwise (suppl. material). The total number of ratings collected after cleaning was around 5.6M (1.4M on videos and 4.1M on v-patches).\n\n\nData Analysis\n\nInter-subject consistency: On the cleaned data, we conducted an inter-subject consistency test [29,16]. Specifically, we randomly divided the subjects into two equal and disjoint sets and computed the Spearman Rank Correlation Coefficient (SRCC) [66] between the two sets of MOS over 50 such random splits. We achieved an average SRCC of 0.86 on full videos, and 0.71, 0.71 and 0.67 for sv-patches, tv-patches, and stv-patches, respectively. This indicates a high degree of agreement between the human subjects, implying a successful screening process (suppl. material). Intra-subject consistency: We computed the Linear Correlation Coefficient (LCC) [67] between subjective MOS against the original scores on the \"golden\" videos, obtaining a median PCC of 0.96 on full videos, and 0.946, 0.95, and 0.937 for sv-patches, tv-patches, and stv-patches, respectively. These high correlations further validate the efficacy of our data collection process. Relationship between patch and video quality: Fig 6 shows scatter plots of the video MOS against each type of vpatch MOS. The calculated SRCC between the video MOS and the sv-patch, tv-patch and stv-patch MOS was 0.69, 0.77, and 0.67 respectively, indicating strong relationships between global and local quality, even though the v-patches are relatively small volumes of the original video data. MOS Distributions: Fig. 7 plots the MOS distribution of the videos in the new dataset as compared to other popular \"in-the-wild\" video quality databases [15,16,17]. The new dataset has a narrower distribution than the others, which again, matches actual social media data. Such a narrow distribution makes it more challenging to create predictive models that can parse finely differing levels of quality. Fig. 7: Ground Truth MOS histograms of four \"in-the-wild\" databases. Starting from left, proposed LSVQ dataset, KoNViD-1k [15], LIVE-VQC [16], and YouTube-UGC [17].\n\n\nModeling a Blind Video Quality Predictor\n\nTaking advantage of the unique potential of the new dataset (Sec. 3), we created a deep video quality prediction model, which we refer to as Patch-VQ (PVQ), and a spatio-temporal quality mapper called PVQ-Mapper, both of which we describe next.\n\n\nOverview\n\nContrary to the way most deep image networks are trained, we did not crop, subsample, or otherwise process the input videos. Any such operation would introduce additional spatial and/or temporal artifacts, which can greatly affect video quality. Processing input videos of diverse aspect ratios, resolutions, and durations, however, makes training an end-to-end deep network impractical. To address this challenge, PVQ extracts spatial and temporal features on unprocessed original videos, and uses them to learn the local to global spatio-temporal quality relationships. As illustrated in Fig 8, PVQ involves three sequential steps: feature extraction, feature pooling, and quality regression. First, we extract features from both the 2D and 3D network streams, thereby capturing the spatial and temporal information from the whole video. Three kinds of v-patch features are also extracted from the output of both networks, using spatial and temporal pooling layers to capture local quality information. Finally, the pooled features from the video and the v-patches are processed by a time series network that effectively captures perceptual quality changes over time and predicts a single quality score per video. We provide more details of each step below.\n\n\nFeature Extraction\n\nTo capture the spatial aspects of both perceptual video quality and frame content, we extracted per frame (2D) spatial features using the PaQ-2-PiQ backbone pre-trained on the LIVE-FB Dataset [29]. To capture temporal distortions, such as flicker, stutter, and focus changes, we extracted spatio-temporal (3D) features using a 3D ResNet-18 [30] backbone, pre-trained on the Kinetics dataset [68].\n\n\nFeature Pooling\n\nSpatial and temporal pooling is applied in stages to extract features from the specified spatio-temporal regions of interest (v-patches), allowing us to model local-to-global space-time quality relationships. Spatial Pooling: The extracted 2D and 3D features are independently passed through a spatial RoIPool (region-ofinterest pooling) layer [69,70], with regions specified by the 3D v-patch coordinates. RoIPool helps compute a feature map with a fixed spatial extent of 2 \u00d7 2. The RoIPool layer generates 4 feature vectors of size 2048 per frame and video clip, for all three v-patches and the full video. Temporal Pooling: The RoIPool layer is followed by an SoIPool (segment-of-interest pooling) layer [71] that helps compute a feature map with a fixed temporal extent. Specifically, an SoIPool layer with a fixed temporal extent of 16 is applied on both 2D and 3D features of each v-patch and the full video. The SoIPool layer yields 4 feature vectors of size 16 \u00d7 2048 per all three v-patches and the full video.\n\n\nTemporal Regression\n\nThe resulting space-time quality features are fed to In-ceptionTime [31], a state-of-the-art deep model for Time Series Classification (TSC). InceptionTime consists of a series of inception modules (with intermittent residual connections) followed by a global average pooling and a fully connected layer. The inception modules learn changes in the quality features over time, which is crucial to accurately predict the global video quality. Although RNNs have been used to model temporal video quality [24,72], we have found that InceptionTime [31] is much faster and easier to train compared to RNN, does not suffer from vanishing gradients, and gives better performance.\n\n\nExperiments\n\nTrain and test splits: The entire dataset of videos, vpatches, and human annotations was divided into a training and two test sets. We first selected those videos having both of their spatial dimensions greater than 720, and reserved it for use as a secondary testing set (about 9% of the LSVQ  [22] and VIDEVAL [23], that perform very well on existing UGC video databases. We also trained the VSFA [24], which extracts frame-level ResNet-50 [74] features followed by a GRU layer to predict video quality. To study the efficacy of our local-to-global model, we trained two versions of our PVQ model, one with, and the other without the spatio-temporal v-patches. All models were trained and evaluated on the same train/test splits. Following the common practice in the field of video quality assessment, we report the performance using the correlation metrics SRCC and LCC.\n\n\nPredicting global video quality\n\nThe quality prediction performance of the compared models on the new LSVQ dataset is summarized in Table  2. As is evident, the shallow learner using traditional features (BRISQUE [73]) did not perform well on our dataset. TLVQM [22], VSFA [24], and VIDEVAL [23] performed better, indicating that they are capable of learning complex distortions. While both PVQ models (with and without patches) outperformed other models, including the v-patch data resulted in a performance boost on both test sets. Particularly on higher resolution test videos (Test-1080p), the proposed PVQ model (trained with v-patches) outperforms the strongest baseline by 3.6% on SRCC.  Performance on each v-patch: Table 3 sheds light into the capability of the compared models in predicting local quality. The two PVQ models delivered the best performance on all three types of v-patches, with the PVQ model trained on v-patches outperforming all baselines. From Tables 2 and 3, we may conclude that PVQ effectively captures global and different forms of local spatio-temporal video quality. Contribution of 2D and 3D streams: We also studied the contribution of the 2D and 3D features towards the performance of PVQ by training separate models on 2D (PVQ 2D ) and 3D (PVQ 3D ) features alone ( Table 4). As can be observed, PVQ 3D achieves higher performance than PVQ 2D on both test sets. This further asserts that 3D features are more capable of capturing complex spatio-temporal distortions, and thus more favorable for VQA. Contribution of each v-patch: To study the relative contributions of the three types of v-patches in PVQ, we trained three separate models utilizing each patch separately (Table  4). Among the three, we observe that the highest performance is achieved when trained on stv-patches. Though stv-patches have relatively least volume (Fig. 4), they contain the most localized information on video quality distortions, which could explain its better performance.\n\n\nMobile-friendly version:\n\nWe also implemented an efficient version of PVQ for mobile and embedded vision applications (PVQ Mobile ), using the 2D and 3D versions of Mo-bileNetV2 [75,76] for the two branches, and by reducing the RoIPool output size to 1 \u00d7 1. Though there is a 6% decrease in performance as compared to PVQ (w/ v-patch), our mobile model requires only 1/5 as many parameters (Table 4) compared to PVQ (w/ v-patch) and 1/2 as many parameters compared to VSFA [24] (24M parameters). Failure cases: The video in Fig 9 (a) was rated with a high score (MOS = 75.7) by human subjects, but was underrated by PVQ (predicted MOS = 47.4). We believe that an aes- thetic \"bokeh\" blur effect was interpreted as high quality content by subjects but such high levels of blur caused the model to predict low quality. The video in Fig 9 (b) was overrated by PVQ (predicted MOS = 54.7), considerably higher than the subject rating (MOS = 21). The video is of a computer generated game and does not appear very distorted. Yet, the subjects may have expected a higher resolution content for modern video games. These cases illustrate the challenges of creating models that closely align to human perception, while also highlighting the content diversity in the proposed dataset.\n\n\nPredicting perceptual quality maps\n\nWe adapted the PVQ model (Sec. 4) to compute spatial and temporal quality maps on videos. Because of its flexible network architecture, PVQ is capable of predicting quality on any number (and sizes) of local spatio-temporal patches of an input video. We exploited this to create a temporal quality series and a first of its kind video quality map predictor, dubbed PVQ Mapper. Temporal quality series: A video is uniformly divided into 16 small temporal clips of 16 (continuous) frames each \u2020 and a single quality score per clip is computed, thus capturing a temporal series of perceptual qualities across a video. Space-time quality maps: For space-time quality maps, we further divide each frame of each temporal clip defined above into a grid of 16 \u00d7 16 non-overlapping spatial blocks of the same aspect ratio as the frame and compute a local space-time video clip quality. Bi-linear interpolation was applied to spatially re-scale the spatio-temporal quality predictions to match the input dimensions. Fig. 10 depicts the temporal quality series and magma color space-time quality maps that were \u03b1-blended (\u03b1 = 0.8) with original frames picked from the center of each clip. The series shows the video quality evolving over time. As may be observed, PVQ Mapper was able to accurately capture local quality loss, distinguishing blurred and underexposed areas from high-quality regions, and high-quality stationary backgrounds from fast-moving, streaky objects. Do v-patches matter for quality maps? Fig. 11 shows spatial quality maps on two sample videos generated by PVQ Mapper, trained with and without using v-patches. In Fig. 11 (a), the object in the foreground is focus blurred, whereas in Fig. 11 (b), the dog is motion blurred and the desk is underexposed. These local quality distortions are not captured with PVQ Mapper (w/o v-patch) as indicated in the middle row, but are distinctly evident in the output of PVQ Mapper (w/ v-patch) as indicated in the bottom row. This indicates that PVQ Mapper that uses v-patches is able to better learn from both global and local video quality features and human judgments of them, and hence predict more accurate quality maps. Fig. 11: Improvement in quality maps when PVQ-Mapper is trained with patches illustrating that learning from both local space-time and global video quality yields more accurate predictions. Best viewed in color.\n\n\nCross-database comparisons\n\nTo emphasize the validity and generalizability of the PVQ model, we also tested it on the two popular, yet much smaller \"in-the-wild\" video databases: KoNViD-1k [15] and LIVE-VQC [16] (Table 1). First, we compared the performance of PVQ against other popular models when each model was separately trained and tested on both datasets. As shown in Table 5, PVQ competes very well with other models on KoNViD-1k, while improves the SRCC on LIVE-VQC by 2.8% compared to the strongest baseline.  To further study the generalizability of PVQ, we also compared the performance of all models when trained on the proposed dataset (LSVQ) but tested on the two aforementioned datasets. From Table 6, it may be seen that PVQ transferred very well to both datasets. Specifically, our model outperforms the strongest baseline by 0.7% and 3.6% boost in SRCC on KoNViD-1k and LIVE-VQC respectively. This degree of database independence, both highlights the representativeness of the new LSVQ dataset and the general efficacy of the proposed PVQ model.\n\n\nConcluding Remarks\n\nPredicting perceptual video quality is a long-standing problem in vision science, and more recently, deep learning. In recent years, it has dramatically increased in importance along with tremendous advances in video capture, sharing and streaming. Accurate and efficient video quality prediction demands the tools of large-scale data collection, visual psychometrics, and deep learning. To progress towards that goal, we built a new video quality database, which is substantially larger and diverse than previous ones. The database contains patch-level annotations that enable us (and others) to make global-to-local and local-to-global quality inferences, culminating in the accurate and generalizable PVQ model. We also created a space-time video quality mapping model, called PVQ Mapper, which utilizes learned patch quality attributes to accurately infer local space-time video quality, and is able to generate accurate spatio-temporal quality maps. We believe that the new LSVQ dataset, the PVQ model, and PVQ Mapper, can significantly advance progress on the UGC VQA problem, and enable quality-based monitoring, ingestion, and control of billions of videos streamed on social media platforms. We have mentioned the average SRCC values, representative of the inter-subject consistencies, in Sec. 3.2.4. Along with that, we present the scatter plots of the two sets of subject MOS in Fig. 2. The narrow spread of the plots shows the high agreement, and hence higher consistency, among subject ratings. We also notice that the spread is highest (or, the correlation is lowest) in the case of stv-patches. This can be attributed to the fact that they account for only 6.4% of the video pixel volume, and sometimes, distortions prominent locally, might get masked or have little impact on perceived global video quality.\n\n\nB.2 Consistency among subject demographics:\n\nWe utilized the subject data to study the effects of device parameters on MOS. The SRCC calculated between laptops and desktop computers (the most used devices in the study) was 0.7, whereas that between videos viewed on phones and other devices was 0.5. Although we collected relatively little data (3.7%) from phones, this reinforces the notion that perceptual video quality is impacted by viewing on a small device screen. We obtained the following correlations between the two major resolutions: 768 \u00d7 1366 and 640 \u00d7 360 (0.76); major viewing distances: less than 15 inches and 15-30 inches (0.76); major age groups: 20-30 and 30-40 (0.79); and genders (0.8), all of which are high, but low enough to be suggestive of further study. The consistency among the ratings from diverse subject demographics, when accumulated, result in the overall high consistency of the data (Sec. B.1), validating our data collection and cleaning methodologies.\n\n\nB.3 Effect of playback delays on video quality:\n\nDelays during playback could impact video quality [16]. We found that > 96% of the videos were viewed with delays < 1s., while 86% of the videos played without delays. By comparing the scores of the delayed videos against the \"golden\" scores, we found that device delays had negligible impact on the mean scores, and that eliminating scores associated with delays did not impact data consistency. Hence, we did not impose device delays as a rejection criteria.\n\n\nB.4 Outlier rejection:\n\nWe removed the outliers in our data in two steps as described briefly in Sec. 3.2.3 -outlier subject rejection and outlier score rejection. The former rejection was video independent, whereas the latter was subject independent. Here, we elaborate the outlier score rejection, which was executed on all videos individually. We followed the standard outlier rejection techniques, but the technique applied was dependent on the score distribution. If, for a video, the scores were (approximately) Gaussian, the modified Z-scores method [64] was applied, which is based on calculating the standard deviation of the distribution. Calculating the kurtosis helped determine the normality of the score distribution. Alternately, if the scores were deemed to be not normal, then we applied the Tukey IQR [65] detection technique, which is based on calculating the interquartile range and is a more generalized method. Tuning the outlier rejection methods based on the nature of the score distribution yielded better consistency scores.\n\n\nC. Modeling Details\n\nFor training PVQ (Sec. 4), we used the Adam optimizer with \u03b2 1 = .9 and \u03b2 2 = .99, a weight decay of .01. The initial learning rate was set to be 0.001 and we followed the 1cycle policy [78] to adjust the learning rate on the fly. We trained each model for 10 epochs and report the performance of the model on the two testing sets.\n\n\nD. Amazon Mechanical Turk (AMT) Study\n\n\nD.1 Study Requirements:\n\nEach video batch (and thus each video) was published on AMT in four phases. The first two phases targeted \"reliable\" workers (with AMT ratings > 95%, and > 10,000 HITs), who helped eliminate inappropriate (violent or pornographic) content and static videos. In the latter two phases, we reduced the numbers to 75% and 1000, respectively.\n\nAs each subject was viewing the instructions, we monitored several parameters to ensure that they could effectively participate. The following eligibility criteria were imposed - In case they failed to meet any of the above criterion, subjects were prevented from progressing and informed accordingly. Apart from these, the subjects were also required to take a quiz reflecting their understanding of the instructions, and were allowed to proceed only if they answered at least five out of the six questions.\n\n\nD.2 Interface:\n\nThe AMT interface comprised of a series of instruction pages, followed by the quiz, before they could start rating the videos. Workers were allowed to view the introductory page (Fig. 3) before accepting to participate in the study. If accepted, they had to go through the instruction pages (Fig. 4, 5, 6, 7), which were timed. During the instructions, we checked whether they satisfied the study criteria as described in Sec. D.1. Following the instruction pages, they had to pass the quiz (Fig. 8) in order to proceed to the training and testing phases. The task included rating the played video (Fig. 9) on a Likert scale [79] marked with BAD, POOR, FAIR, GOOD, and EXCELLENT, as demonstrated in Fig. 10. A similar interface was used for the v-patch sessions as well.  \n\nFig. 1 :\n1Modeling local to global perceptual quality: From each video, we extract three spatio-temporal video patches (Sec. 3.1), which along with their subjective scores, are fed to the proposed video quality model. By integrating spatial (2D) and spatio-temporal (3D) quality-sensitive features, our model learns spatial and temporal distortions, and can robustly predict both global and local quality, a temporal quality series, as well as space-time quality maps (Sec. 5.2). Best viewed in color.\n\nFig. 2 :\n2Sample video frames from the new database, each resized to fit. The actual videos are of highly diverse sizes and resolutions.\n\nFig. 3 :\n3Left: Scatter plot of video width versus video height with marker size indicating the number of videos having a given dimension in the new LSVQ database. Right: Histogram of the durations (in seconds) of the videos.\n\nFig. 4 :\n4Three kinds of video patches (v-patches) cropped from random space-time volumes from each video in the dataset. All v-patches are videos.\n\nFig. 5 :\n5Study workflow for both video and v-patch sessions.\n\nFig. 6 :\n6Scatter plots of patch-video MOS correlations Video MOS vs svpatch (left), tv-patch (middle) and stv-patch (right) MOS cropped from the same video.\n\nFig. 8 :\n8Illustrating the proposed PVQ model which involves 3 sequential steps: feature extraction, spatio-temporal pooling, and temporal regression (Sec. 4.1).\n\nFig. 9 :\n9Failure cases: Frames from video examples where predictions differed the most from the human quality judgements.\n\nFig. 10 :\n10Space-time quality maps: Space-time quality maps generated on a video using the PVQ Mapper (Sec. 5.2), and sampled in time for display. Four video frames are shown at top, with spatial quality maps (blended with the original frames using magma color) immediately under, while the bottom plots show the evolving quality of the video. Best viewed in color.\n\nFig. 1 :\n1Examples of video patch (v-patch) triplets cropped from random space-time volumes from two exemplar videos in the dataset. All v-patches are videos.B. DatasetB.1 Inter-subject consistency plots:\n\nFig. 2 :\n2Inter-subject consistency: Inter-subject scatter plot of MOS calculated between random 50% divisions of the human labels on all 39K videos (first from left) into disjoint subject sets. The same is plotted for the sv-patches (second), tv-patches (third) and stv-patches (fourth).\n\n\u2022\nBrowser Window Resolution: At least 480p for mobile devices and 720p for others. \u2022 Browser Zoom: Set to 100%. \u2022 Browsers: Latest versions of Chrome, Firefox, Edge, Safari, and Chrome. \u2022 Loading Time: Must be less than 20 secs for all the training videos.\n\nFig. 3 :\n3Introductory Page\n\nFig. 4 :Page 1 Fig. 5 :Page 2 Fig. 6 :Page 3 Fig. 7 :Page 4 Fig. 8 :Fig. 9 :\n4152637489Instruction Instruction Instruction Instruction Quiz Video PlaybackFig. 10: Rating Slider\n\nTable 1 :\n1Summary of popular public-domain video quality datasets. Legacy datasets contain singular synthetic distortions, whereas \"in-the-wild\" databases contain videos impaired by complex mixtures of diverse, real distortions.Database \n# Unique \ncontents \n\n# Video \nDuration (sec) \n\n# Distor-\ntions \n\n# Video \ncontents \n\n# V-Patch \ncontents \nDistortion type \nSubjective study \nframework \n# Annotators \n# Annotations \n\nMCL-JCV (2016) [11] \n30 \n5 \n51 \n1,560 \n0 \nCompression \nIn-lab \n150 \n78K \nVideoSet (2017) [12] \n220 \n5 \n51 \n45,760 \n0 \nCompression \nIn-lab \n800 \n-\nUGC-VIDEO (2019) [26] \n50 \n> 10 \n10 \n550 \n0 \nCompression \nIn-lab \n30 \n16.5K \nCVD-2014 (2014) [27] \n5 \n10-25 \n-\n234 \n0 \nIn-capture \nIn-lab \n210 \n-\nLIVE-Qualcomm (2016) [28] \n54 \n15 \n6 \n208 \n0 \nIn-capture \nIn-lab \n39 \n8.1K \nKoNViD-1k (2017) [15] \n1,200 \n8 \n-\n1,200 \n0 \nIn-the-wild \nCrowdsourced \n642 \n\u2248 205K \nLIVE-VQC (2018) [16] \n585 \n10 \n-\n585 \n0 \nIn-the-wild \nCrowdsourced \n4,776 \n205K \nYouTube-UGC (2019) [17] \n1,500 \n20 \n-\n1,500 \n4,500 \nIn-the-wild \nCrowdsourced \n-\n\u2248 600K \nProposed database (LSVQ) \n39, 075 \n5-12 \n-\n39, 075 \n117, 225 \nIn-the-wild \nCrowdsourced \n6, 284 \n5, 545, 594 \n\n\n\nTable 2 :\n2Performance on full-size videos in the LSVQ dataset. Higher values indicate better performance. Picture based model is italicized.Test \nTest-1080p \n\nModel \nSRCC \nLCC \nSRCC \nLCC \nBRISQUE [73] \n0.579 \n0.576 \n0.497 \n0.531 \nTLVQM [22] \n0.772 \n0.774 \n0.589 \n0.616 \nVIDEVAL [23] \n0.794 \n0.783 \n0.545 \n0.554 \nVSFA [24] \n0.801 \n0.796 \n0.675 \n0.704 \nPVQ (w/o v-patch) \n0.814 \n0.816 \n0.686 \n0.708 \nPVQ (w/ v-patch) \n0.827 \n0.828 \n0.711 \n0.739 \n\ndataset: 3.5K videos and 10.5K v-patches). About 93.2% \nof the videos in the reserved set have resolutions 1080p or \nhigher, hence we will refer to it as \"Test-1080p\". On the \nremaining videos, we applied a typical 80-20 split, yielding \nabout 28.1K videos (and 84.3K v-patches) for training, and \n7.4K videos (and 22.2K v-patches) for testing. \nInput processing and training: Each video was divided \ninto 40 clips of 16 continuous frames. For feature extrac-\ntion, we used a batch size of 8 for 3D ResNet-18 and 128 for \nPaQ-2-PiQ. For spatial and temporal pooling, we provide \nsets of spatio-temporal coordinates (x 1 , x 2 , y 1 , y 2 , t 1 , t 2 ) of \neach v-patch. When training InceptionTime, we used a \nbatch size of 128 and L1 loss to predict the output quality \nscores (details in suppl. material). \nBaselines and metrics: The model comparisons were done \non both videos and v-patches. We compared with a popu-\nlar image model BRISQUE [73], by extracting frame-level \nfeatures and training an SVR and two other shallow NR \nVQA models, TLVQM \n\nTable 3 :\n3Results on the three v-patches in the LSVQ dataset. Picture based model is italicized.sv-patch \ntv-patch \nstv-patch \n\nModel \nSRCC LCC \nSRCC LCC \nSRCC LCC \nBRISQUE [73] \n0.469 0.417 \n0.465 0.485 \n0.476 0.462 \nTLVQM [22] \n0.575 0.543 \n0.523 0.536 \n0.561 0.563 \nVIDEVAL [23] \n0.596 0.570 \n0.633 0.634 \n0.662 0.636 \nVSFA [24] \n0.654 0.609 \n0.688 0.681 \n0.685 0.670 \nPVQ (w/o v-patch) \n0.723 0.717 \n0.696 0.701 \n0.651 0.643 \nPVQ (w/ v-patch) \n0.737 0.720 \n0.701 0.700 \n0.711 0.707 \n\n\n\nTable 4 :\n4Ablation studies conducted on the Test split of the LSVQ dataset.Higher values indicate better performance.Model \nSRCC \nLCC \n# parameters \nPVQ 2D (w/ v-patch) \n0.774 \n0.774 \n16.3 M \nPVQ 3D (w/ v-patch) \n0.805 \n0.805 \n38.3 M \nPVQ (w/ sv-patch) \n0.815 \n0.815 \n54.2 M \nPVQ (w/ tv-patch) \n0.817 \n0.818 \n54.2 M \nPVQ (w/ stv-patch) \n0.824 \n0.826 \n54.2 M \nPVQ Mobile (w/ v-patch) \n0.774 \n0.779 \n10.9 M \n\n\n\nTable 5 :\n5Cross-database comparison 1: Performance when all models are separately trained and tested on KoNViD-1k[15] and LIVE-VQC[16].KoNViD-1k [15] \nLIVE-VQC [16] \nModel \nSRCC \nLCC \nSRCC \nLCC \nBRISQUE [73] \n0.657 \n0.658 \n0.592 \n0.638 \nV-BLIINDS [77] \n0.710 \n0.704 \n0.694 \n0.718 \nVSFA [24] \n0.773 \n0.775 \n0.773 \n0.795 \nTLVQM [22] \n0.773 \n0.769 \n0.799 \n0.803 \nVIDEVAL [23] \n0.783 \n0.780 \n0.752 \n0.751 \nPVQ (w/o v-patch) (Sec. 4) \n0.791 \n0.786 \n0.827 \n0.837 \n\n\n\nTable 6 :\n6Cross-database comparison 2: Performance when all models are \nseparately trained on the new LSVQ database, then evaluated on KoNViD-\n1k [15] and LIVE-VQC [16] without fine-tuning. \nKoNViD-1k [15] \nLIVE-VQC [16] \nModel \nSRCC \nLCC \nSRCC \nLCC \nBRISQUE [73] \n0.646 \n0.647 \n0.524 \n0.536 \nTLVQM [22] \n0.732 \n0.724 \n0.670 \n0.691 \nVIDEVAL [23] \n0.751 \n0.741 \n0.630 \n0.640 \nVSFA [24] \n0.784 \n0.794 \n0.734 \n0.772 \nPVQ (w/o v-patch) (Sec. 4) \n0.782 \n0.781 \n0.747 \n0.776 \nPVQ (w/ v-patch) (Sec. 4) \n0.791 \n0.795 \n0.770 \n0.807 \n\n\n\u2020 By changing the number of frames in each clip, the quality predictions can be made less or more dense.\n\nTikTok by the Numbers. Omnicore, Omnicore. TikTok by the Numbers. [Online] Available: https://www.omnicoreagency.com/\n\nFacebook Video Statistics. Facebook Video Statistics. [Online] Available: https://99firms.com/blog/ facebook-video-statistics/. 1\n\n10 Youtube Statistics Every Marketer Should Know in 2020. Maryam Mohsin, Oberlo, Maryam Mohsin, Oberlo. 10 Youtube Statistics Every Mar- keter Should Know in 2020. [Online] Available: https:// www.oberlo.com/blog/youtube-statistics. 1\n\nImage quality assessment: From error visibility to structural similarity. Z Wang, A C Bovik, H R Sheikh, E P Simoncelli, IEEE Transactions on Image Processing. 134Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli. Image quality assessment: From error visibility to structural similarity. IEEE Transactions on Image Processing, vol. 13, no. 4, pp. 600-612, April 2004. 1\n\nMean squared error: Love it or leave it? A new look at signal fidelity measures. Z Wang, A C Bovik, IEEE Signal Process. Mag. 261Z. Wang and A. C. Bovik. Mean squared error: Love it or leave it? A new look at signal fidelity measures. IEEE Signal Process. Mag., vol. 26, no. 1, pp. 98-117, Jan 2009. 1\n\nVQpooling: Video quality pooling adaptive to perceptual distortion severity. J Park, S Lee, A C Bovik, IEEE Transactions on Image Processing. 222J. Park, S. Lee, and A.C. Bovik. VQpooling: Video qual- ity pooling adaptive to perceptual distortion severity. IEEE Transactions on Image Processing, vol. 22, no. 2, pp. 610- 620, Feb. 2013. 1\n\nImage quality comparison between JPEG and JPEG2000. I. Psychophysical investigation. E , Allen S Triantaphillidou, R Jacobson, Journal of Imaging Science and Technology. 513E. Allen S. Triantaphillidou and R. Jacobson. Image quality comparison between JPEG and JPEG2000. I. Psychophysi- cal investigation. Journal of Imaging Science and Technol- ogy, vol. 51, no. 3, pp. 248-258, 2007. 1\n\nA h.264/avc video database for the evaluation of quality metrics. M , Naccari S Tubaro, F De Simone, M Tagliasacchi, T Ebrahimi, Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP). IEEE Int. Conf. Acoust., Speech, Signal ess. (ICASSP)13M. Naccari S. Tubaro F. De Simone, M. Tagliasacchi and T. Ebrahimi. A h.264/avc video database for the evaluation of quality metrics. Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP), pp. 2430-2433, 2010. 1, 2, 3\n\nStudy of subjective and objective quality assessment of video. K Seshadrinathan, R Soundararajan, A C Bovik, L K Cormack, IEEE Transactions on Image Processing. 1963K. Seshadrinathan, R. Soundararajan, A. C. Bovik, and L. K. Cormack. Study of subjective and objective quality assess- ment of video. IEEE Transactions on Image Processing, 19(6):1427-1441, 2010. 1, 2, 3\n\nThe TUM high definition video datasets. C Keimel, A Redl, K Dieopold, 1C. Keimel, A. Redl, and K. Dieopold. The TUM high defi- nition video datasets. volume pp. 97-102, 07 2012. 1, 2\n\nMcl-jcv: A JND-based h.264/avc video quality assessment dataset. H Wang, W Gan, S Hu, J Y Lin, L Jin, L Song, P Wang, I Katsavounidis, A Aaron, C . J Kuo, 2016 IEEE International Conference on Image Processing (ICIP). 13H. Wang, W. Gan, S. Hu, J. Y. Lin, L. Jin, L. Song, P. Wang, I. Katsavounidis, A. Aaron, and C. . J. Kuo. Mcl-jcv: A JND-based h.264/avc video quality assessment dataset. In 2016 IEEE International Conference on Image Processing (ICIP), pages 1509-1513, 2016. 1, 2, 3\n\nVideoset: A largescale compressed video quality dataset based on JND measurement. H Wang, I Katsavounidis, J Zhou, J Park, S Lei, Xin Zhou, M-O Pun, X Jin, R Wang, X Wang, Y Zhang, J Huang, S Kwong, C C , Jay Kuo, 13H. Wang, I. Katsavounidis, J. Zhou, J. Park, S. Lei, Xin Zhou, M-O. Pun, X. Jin, R. Wang, X. Wang, Y. Zhang, J. Huang, S. Kwong, and C. C. Jay Kuo. Videoset: A large- scale compressed video quality dataset based on JND mea- surement, 2017. 1, 2, 3\n\nVideo Quality Experts Group (VQEG). VQEG HDTV phase I databaseVideo Quality Experts Group (VQEG). VQEG HDTV phase I database. [Online] Available: https:\n\nVis3: An algorithm for video quality assessment via analysis of spatial and spatiotemporal slices. P V Vu, D M Chandler, J. Electron. Imag. 2313P. V. Vu and D. M. Chandler. Vis3: An algorithm for video quality assessment via analysis of spatial and spatiotemporal slices. J. Electron. Imag., vol. 23, no. 1, p. 013016, Feb. 2014. 1, 2, 3\n\nThe konstanz natural video database (konvid-1k). V Hosu, F Hahn, M Jenadeleh, H Lin, H Men, T Szir\u00e1nyi, S Li, D Saupe, 2017 Ninth International Conference on Quality of Multimedia Experience (QoMEX). V. Hosu, F. Hahn, M. Jenadeleh, H. Lin, H. Men, T. Szir\u00e1nyi, S. Li, and D. Saupe. The konstanz natural video database (konvid-1k). In 2017 Ninth International Conference on Quality of Multimedia Experience (QoMEX), pages 1-6.\n\n. IEEE. 58IEEE, 2017. [Online] Database: http://database. mmsp-kn.de/konvid-1k-database.html. 2, 3, 4, 5, 8\n\nLarge-scale study of perceptual video quality. Z Sinno, A C Bovik, IEEE Transactions on Image Processing. 282html. 2, 3, 4, 5, 8Z. Sinno and A.C. Bovik. Large-scale study of percep- tual video quality. IEEE Transactions on Image Process- ing, vol. 28, no. 2, pp. 612-627, Feb. 2019. [Online] LIVE VQC Database: http://live.ece.utexas. edu/research/LIVEVQC/index.html. 2, 3, 4, 5, 8,\n\nYoutube UGC dataset for video compression research. Y Wang, B Inguva, Adsumilli, Y Wang, S Inguva, and B Adsumilli. Youtube UGC dataset for video compression research. 2019. 2, 3, 4, 5\n\nBlind prediction of natural video quality. A C Bovik, M A Saad, C Charrier, IEEE Transactions on Image Processing. 233A. C. Bovik M. A. Saad and C. Charrier. Blind prediction of natural video quality. IEEE Transactions on Image Process- ing, vol. 23, no. 3, pp. 1352-1365, 2014. 2\n\nDesign of no-reference video quality metrics with multiway partial least squares regression. M Klimpke, C Keimel, J Habigt, K Diepold, Third International Workshop on Quality of Multimedia Experience. M. Klimpke C. Keimel, J. Habigt and K. Diepold. Design of no-reference video quality metrics with multiway partial least squares regression. 2011 Third International Workshop on Quality of Multimedia Experience, pp. 49-54, 2011. 2\n\nNo-reference video quality assessment based on artifact measurement and statistical analysis. V , Asari K Zhu, C Li, D Saupe, Transactions on Circuits and Systems for Video Technology. 25V. Asari K. Zhu, C. Li and D. Saupe. No-reference video quality assessment based on artifact measurement and sta- tistical analysis. Transactions on Circuits and Systems for Video Technology, vol. 25, no. 4, pp. 533-546, 2014. 2\n\nA completely blind video integrity oracle. M A Saad, A Mittal, A C Bovik, IEEE Transactions on Image Processing. 251M. A. Saad A. Mittal and A. C. Bovik. A completely blind video integrity oracle. IEEE Transactions on Image Process- ing, vol. 25, no. 1, pp. 289-300, 2015. 2\n\nTwo-level approach for no-reference consumer video quality assessment. J Korhonen, IEEE Transactions on Image Processing. 2812J. Korhonen. Two-level approach for no-reference consumer video quality assessment. IEEE Transactions on Image Pro- cessing, 28(12):5923-5938, 2019. 2, 6, 7, 8\n\nUGC-VQA: Benchmarking blind video quality assessment for user generated content. Z Tu, Y Wang, N Birkbeck, B Adsumilli, A C Bovik, 7Z. Tu, Y. Wang, N. Birkbeck, B. Adsumilli, and A. C. Bovik. UGC-VQA: Benchmarking blind video quality assessment for user generated content, 2020. 2, 6, 7, 8\n\nQuality assessment of in-thewild videos. D Li, T Jiang, M Jiang, 7D. Li, T. Jiang, and M. Jiang. Quality assessment of in-the- wild videos. 2019. 2, 3, 6, 7, 8\n\nTemporal hysteresis model of time varying subjective video quality. K Seshadrinathan, A C Bovik, 2011 IEEE international conference on acoustics, speech and signal processing (ICASSP). K. Seshadrinathan and A. C. Bovik. Temporal hysteresis model of time varying subjective video quality. In 2011 IEEE international conference on acoustics, speech and sig- nal processing (ICASSP), pages 1153-1156. IEEE, 2011. 2\n\nUGC-VIDEO: perceptual quality assessment of usergenerated videos. Y Li, S Meng, X Zhang, S Wang, Y Wang, S Ma, Y. Li, S. Meng, X. Zhang, S. Wang, Y. Wang, and S. Ma. UGC-VIDEO: perceptual quality assessment of user- generated videos, 2019. 3\n\nCVD2014-a database for evaluating no-reference video quality assessment algorithms. M Nuutinen, T Virtanen, M Vaahteranoksa, T Vuori, P Oittinen, J H\u00e4kkinen, IEEE Transactions on Image Processing. 2573M. Nuutinen, T. Virtanen, M. Vaahteranoksa, T. Vuori, P. Oittinen, and J. H\u00e4kkinen. CVD2014-a database for evaluating no-reference video quality assessment algorithms. IEEE Transactions on Image Processing, 25(7):3073-3086, 2016. 2, 3\n\nIn-capture mobile video distortions: A study of subjective behavior and objective algorithms. A C Bovik, A K Moorthy, P Panda, D Ghadiyaram, J Pan, K C Yang, IEEE Trans. Circ. and Syst. 23A. C. Bovik A. K. Moorthy P. Panda D. Ghadiyaram, J. Pan and K. C. Yang. In-capture mobile video dis- tortions: A study of subjective behavior and objective algorithms. IEEE Trans. Circ. and Syst. for Video Tech., 2017. [Online] LIVE-Qualcomm Database: http://live.ece.utexas.edu/research/ incaptureDatabase/index.html. 2, 3\n\nFrom patches to pictures (paq-2-piq): Mapping the perceptual space of picture quality. Z Ying, H Niu, P Gupta, D Mahajan, D Ghadiyaram, A C Bovik, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). 5Z. Ying, H. Niu, P. Gupta, D. Mahajan, D. Ghadiyaram, and A. C. Bovik. From patches to pictures (paq-2-piq): Mapping the perceptual space of picture quality. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 3572-3582, 2020. 2, 4, 5\n\nLearning spatio-temporal features with 3d residual networks for action recognition. K Hara, H Kataoka, Y Satoh, Proceedings of the IEEE International Conference on Computer Vision (ICCV) Workshops. the IEEE International Conference on Computer Vision (ICCV) Workshops25K. Hara, H. Kataoka, and Y. Satoh. Learning spatio-temporal features with 3d residual networks for action recognition. In Proceedings of the IEEE International Conference on Com- puter Vision (ICCV) Workshops, Oct 2017. 2, 5\n\nInceptiontime: Finding alexnet for time series classification. H I Fawaz, B Lucas, G Forestier, C Pelletier, D F Schmidt, J Weber, G I Webb, L Idoumghar, P.-A Muller, F Petitjean, 26H. I. Fawaz, B. Lucas, G. Forestier, C. Pelletier, D. F. Schmidt, J. Weber, G. I. Webb, L. Idoumghar, P.-A. Muller, and F. Petitjean. Inceptiontime: Finding alexnet for time se- ries classification, 2019. 2, 6\n\nNo-reference video quality assessment using multi-level spatially pooled features. F G Hahn, V Hosu, H Lin, D Saupe, 23F. G. Hahn, V. Hosu, H. Lin, and D. Saupe. No-reference video quality assessment using multi-level spatially pooled features, 2019. 2, 3\n\n500-14, methodologies for the subjective assessment of the quality of television images. International Telecommunication Union. ITU-R BT25International Telecommunication Union. ITU-R BT.500- 14, methodologies for the subjective assessment of the quality of television images. [Online] Available: https: //www.itu.int/dms_pubrec/itu-r/rec/bt/ R-REC-BT.500-14-201910-I!!PDF-E.pdf. 2, 5\n\nNoreference video quality assessment for sd and hd h. 264/avc sequences based on continuous estimates of packet loss visibility. M.-N S Garcia, A Argyropoulos, P Raake, List, Third International Workshop on Quality of Multimedia Experience. M.-N. Garcia S. Argyropoulos, A. Raake and P. List. No- reference video quality assessment for sd and hd h. 264/avc sequences based on continuous estimates of packet loss visi- bility. 2011 Third International Workshop on Quality of Mul- timedia Experience, pp. 31-36, 2011. 2\n\nNoreference pixel video quality monitoring of channel-induced distortion. M Tagliasacchi, G Valenzise, S Magni, S Tubaro, IEEE Transactions on Circuits and Systems for Video Technology. 22M. Tagliasacchi G. Valenzise, S. Magni and S. Tubaro. No- reference pixel video quality monitoring of channel-induced distortion. IEEE Transactions on Circuits and Systems for Video Technology, vol. 22, no. 4, pp. 605-618, 2011. 2\n\nA noreference bitstream-based perceptual model for video quality estimation of videos affected by coding artifacts and packet losses. L P Kondi, K Pandremmenou, M Shahid, B L\u00f6vstr\u00f6m, Human Vision and Electronic Imaging XX. 9394293941L. P. Kondi K. Pandremmenou, M. Shahid and B. L\u00f6vstr\u00f6m. A noreference bitstream-based perceptual model for video quality estimation of videos affected by coding artifacts and packet losses. Human Vision and Electronic Imaging XX, vol. 9394, pp.93941F, 2015. 2\n\nNo-reference video quality assessment using codec analysis. Transactions on Circuits and Systems for Video Technology. S Forchhammer, J S\u00f8gaard, J Korhonen, 25S. Forchhammer J. S\u00f8gaard and J. Korhonen. No-reference video quality assessment using codec analysis. Transactions on Circuits and Systems for Video Technology, vol. 25, no. 10, pp. 1637-1650, 2015. 2\n\nPredictive no-reference assessment of video quality. Signal Processing: Image Communication. S Stavrou, M T Vega, D C Mocanu, A Liotta, 52S. Stavrou M. T. Vega, D. C. Mocanu and A. Liotta. Predic- tive no-reference assessment of video quality. Signal Pro- cessing: Image Communication, vol. 52, pp. 20-32, 2017.\n\nNo-reference quality metric for degraded and enhanced video. Digit. Video Image Qual. Perceptual Coding. J E Caviedes, F Oberti, J. E. Caviedes and F. Oberti. No-reference quality metric for degraded and enhanced video. Digit. Video Image Qual. Perceptual Coding, pp. 305-324, 2017. 2\n\nNo-reference video quality evaluation for high-definition video. T Oelbaum, C Keimel, K Diepold, Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP). IEEE Int. Conf. Acoust., Speech, Signal ess. (ICASSP)T. Oelbaum C. Keimel and K. Diepold. No-reference video quality evaluation for high-definition video. Proc. IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP), pp. 1145-1148, 2009. 2\n\nSpatiotemporal statistics for video quality assessment. X Li, Q Guo, X Lu, IEEE Transactions on Image Processing. 257X. Li, Q. Guo, and X. Lu. Spatiotemporal statistics for video quality assessment. IEEE Transactions on Image Processing, 25(7):3329-3342, 2016. 2\n\nSpatio-temporal measures of naturalness. Z Sinno, A C Bovik, 2019 IEEE International Conference on Image Processing (ICIP). Z. Sinno and A. C. Bovik. Spatio-temporal measures of nat- uralness. In 2019 IEEE International Conference on Image Processing (ICIP), pages 1750-1754, 2019. 2\n\nBlind image quality assessment on real distorted images using deep belief nets. D Ghadiyaram, A C Bovik, IEEE Global Conference on Signal and Information processing. Atlanta, GAD. Ghadiyaram and A. C. Bovik. Blind image quality assess- ment on real distorted images using deep belief nets. In IEEE Global Conference on Signal and Information processing, volume pp. 946-950, pages 946-950, Atlanta, GA, 2014. 2\n\nDeep convolutional neural models for picture-quality prediction: Challenges and solutions to data-driven image quality assessment. J Kim, H Zeng, D Ghadiyaram, S Lee, L Zhang, A C Bovik, IEEE Signal Process. Mag. 346J. Kim, H. Zeng, D. Ghadiyaram, S. Lee, L. Zhang, and A. C. Bovik. Deep convolutional neural models for picture-quality prediction: Challenges and solutions to data-driven image quality assessment. IEEE Signal Process. Mag., vol. 34, no. 6, pp. 130-141, Nov 2017. 2\n\nA deep neural network for image quality assessment. S Bosse, D Maniry, T Wiegand, W Samek, 2016 IEEE Int'l Conf. Image Process. (ICIP). S. Bosse, D. Maniry, T. Wiegand, and W. Samek. A deep neural network for image quality assessment. In 2016 IEEE Int'l Conf. Image Process. (ICIP), pages 3773-3777, Sep. 2016. 2\n\nFully deep blind image quality predictor. J Kim, S Lee, IEEE J. of Selected Topics in Signal Process. 111J. Kim and S. Lee. Fully deep blind image quality predictor. IEEE J. of Selected Topics in Signal Process., vol. 11, no. 1, pp. 206-220, Feb 2017. 2\n\nNIMA: Neural image assessment. H Talebi, P Milanfar, IEEE Transactions on Image Processing. 278H. Talebi and P. Milanfar. NIMA: Neural image assessment. IEEE Transactions on Image Processing, vol. 27, no. 8, pp. 3998-4011, Aug 2018. 2\n\nRankIQA: Learning from rankings for no-reference image quality assessment. X Liu, J Van De Weijer, A D Bagdanov, IEEE Int'l Conf. on Comput. Vision (ICCV). X. Liu, J. van de Weijer, and A. D. Bagdanov. RankIQA: Learning from rankings for no-reference image quality as- sessment. In IEEE Int'l Conf. on Comput. Vision (ICCV), page 1040-1049, 2017. 2\n\nEnd-to-end blind image quality assessment using deep neural networks. K Ma, W Liu, K Zhang, Z Duanmu, Z Wang, W Zuo, IEEE Transactions on Image Processing. 273K. Ma, W. Liu, K. Zhang, Z. Duanmu, Z. Wang, and W. Zuo. End-to-end blind image quality assessment using deep neural networks. IEEE Transactions on Image Processing, vol. 27, no. 3, pp. 1202-1213, March 2018. 2\n\nBlind video quality assessment with weakly supervised learning and resampling strategy. Y Zhang, X Gao, L He, W Lu, R He, IEEE Transactions on Circuits and Systems for Video Technology. 2983Y. Zhang, X. Gao, L. He, W. Lu, and R. He. Blind video quality assessment with weakly supervised learning and re- sampling strategy. IEEE Transactions on Circuits and Sys- tems for Video Technology, 29(8):2244-2255, 2019. 2, 3\n\nDeep video quality assessor: From spatio-temporal visual sensitivity to a convolutional neural aggregation network. J S. Ahn, W Kim, J Kim, S Kim, Lee, Proc. Eur. Conf. Comput. Vis. (ECCV). Eur. Conf. Comput. Vis. (ECCV)S. Ahn J. Kim W. Kim, J. Kim and S. Lee. Deep video quality assessor: From spatio-temporal visual sensitivity to a convo- lutional neural aggregation network. Proc. Eur. Conf. Com- put. Vis. (ECCV), pp. 219-234, 2018. 3\n\nEnd-to-end blind quality assessment of compressed videos using deep neural networks. Z Duanmu, W Liu, Z Wang, Proc. ACM Multimedia Conf. (MM). ACM Multimedia Conf. (MM)Z. Duanmu W. Liu and Z. Wang. End-to-end blind quality as- sessment of compressed videos using deep neural networks. Proc. ACM Multimedia Conf. (MM), pp. 546-554, 2018. 3\n\nMoving Image Archive. Internet Archive, Internet Archive. Moving Image Archive. [Online] Avail- able: https://archive.org/details/movies. 3\n\nYfcc100m: The new data in multimedia research. B Thomee, D A Shamma, G Friedland, B Elizalde, K Ni, D Poland, D Borth, L.-J Li, B. Thomee, D. A. Shamma, G. Friedland, B. Elizalde, K. Ni, D. Poland, D. Borth, and L.-J. Li. Yfcc100m: The new data in multimedia research. 2015. [Online] Dataset Browser: http://projects.dfki.uni-kl.\n\n. Ffmpeg, FFmpeg. [Online] Available: https://ffmpeg.org/.\n\nA probabilistic approach to people-centric photo selection and sequencing. V Vonikakis, R Subramanian, J Arnfred, S Winkler, IEEE Transactions on Multimedia. 1911V. Vonikakis, R. Subramanian, J. Arnfred, and S. Winkler. A probabilistic approach to people-centric photo selection and sequencing. IEEE Transactions on Multimedia, vol. 19, no. 11, pp. 2609-2624, Nov 2017. 3\n\nMeasuring colorfulness in natural images. D Hasler, S E Suesstrunk, SPIE Conf. on Human Vision and Electronic Imaging VIII. D. Hasler and S. E. Suesstrunk. Measuring colorfulness in natural images. In SPIE Conf. on Human Vision and Elec- tronic Imaging VIII, 2003. 3\n\nContrast in complex images. E Peli, J. Opt. Soc. Am. A. 710E. Peli. Contrast in complex images. J. Opt. Soc. Am. A, vol. 7, no. 10, pp. 2032-2040, Oct 1990. 3\n\nFace detection using haar cascades. OpenCV-Python Tutorials. Face detection using haar cascades. OpenCV- Python Tutorials, [Online] Available: https:\n\nRepresenting and recognizing the visual appearance of materials using threedimensional textons. T Leung, J Malik, International Journal of Computer Vision. 431T. Leung and J. Malik. Representing and recog- nizing the visual appearance of materials using three- dimensional textons. International Journal of Computer Vision, vol. 43, no. 1, pp. 29-44, 2001. [Online] Fil- ter Bank: https://www.robots.ox.ac.uk/\u02dcvgg/ research/texclass/filters.html. 3\n\nMassive online crowdsourced study of subjective and objective picture quality. D Ghadiyaram, A C Bovik, IEEE Transactions on Image Processing. 251D. Ghadiyaram and A. C. Bovik. Massive online crowd- sourced study of subjective and objective picture quality. IEEE Transactions on Image Processing, vol. 25, no. 1, pp. 372-387, Jan 2016. 4\n\nKoniq-10K: Towards an ecologically valid and large-scale IQA database. H Lin, V Hosu, D Saupe, arXiv:1803.08489arXiv preprintH. Lin, V. Hosu, and D. Saupe. Koniq-10K: Towards an eco- logically valid and large-scale IQA database. arXiv preprint arXiv:1803.08489, March 2018. 4\n\n. Kurtosis Measure Of, SpringerNew York, New York, NYMeasure of Kurtosis, pages 343-343. Springer New York, New York, NY, 2008. 5\n\nB Iglewicz, D C Hoaglin, How to Detect and Handle Outliers. The ASQC Basic References in Quality Control: Statistical Techniques. 16B. Iglewicz and D. C. Hoaglin. Volume 16: How to Detect and Handle Outliers. The ASQC Basic References in Quality Control: Statistical Techniques, 1993. 5,\n\nExploratory data analysis. J Tukey, Addison-Wesley Pub. CoReading, MassJ. Tukey. Exploratory data analysis. Addison-Wesley Pub. Co, Reading, Mass, 1977. 5,\n\nRank correlation methods. Maurice George Kendall, Maurice George Kendall. Rank correlation methods. 1948. 5\n\nThirteen ways to look at the correlation coefficient. The American Statistician. Joseph Lee Rodgers, W , Alan Nicewander, 42Joseph Lee Rodgers and W. Alan Nicewander. Thirteen ways to look at the correlation coefficient. The American Statisti- cian, 42(1):59-66, 1988. 5\n\nThe kinetics human action video dataset. Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, Andrew Zisserman, Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and Andrew Zisserman. The kinetics human action video dataset, 2017. 5\n\nFast R-CNN. R Girshick, IEEE Int'l Conf. on Comput. Vision (ICCV). R. Girshick. Fast R-CNN. In IEEE Int'l Conf. on Comput. Vision (ICCV), page 1040-1049, 2015. 6\n\nFaster R-CNN: Towards real-time object detection with region proposal networks. R Girshick, Adv. Neural Info Process Syst (NIPS). R. Girshick. Faster R-CNN: Towards real-time object de- tection with region proposal networks. In Adv. Neural Info Process Syst (NIPS), 2015. 6\n\nRethinking the faster rcnn architecture for temporal action localization. Y Chao, S Vijayanarasimhan, B Seybold, D A Ross, J Deng, R Sukthankar, Y. Chao, S. Vijayanarasimhan, B. Seybold, D. A. Ross, J. Deng, and R. Sukthankar. Rethinking the faster r- cnn architecture for temporal action localization. In 2018\n\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1130-1139, 2018. 6\n\nDeep neural networks for noreference video quality assessment. J You, J Korhonen, 2019 IEEE International Conference on Image Processing (ICIP). J. You and J. Korhonen. Deep neural networks for no- reference video quality assessment. In 2019 IEEE Interna- tional Conference on Image Processing (ICIP), pages 2349- 2353, 2019. 6\n\nNo-reference image quality assessment in the spatial domain. A Mittal, A K Moorthy, A C Bovik, IEEE Transactions on Image Processing. 2112A. Mittal, A. K. Moorthy, and A. C. Bovik. No-reference image quality assessment in the spatial domain. IEEE Trans- actions on Image Processing, vol. 21, no. 12, pp. 4695-4708, 2012. 6, 7, 8\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, IEEE Conf. Comput. Vision and Pattern Recogn. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In IEEE Conf. Comput. Vision and Pattern Recogn., pages 770-778, 2016. 6\n\nMobilenetv2: Inverted residuals and linear bottlenecks. Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionMark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh- moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 4510-4520, 2018. 7\n\nResource efficient 3d convolutional neural networks. Okan Kopuklu, Neslihan Kose, Ahmet Gunduz, Gerhard Rigoll, Proceedings of the IEEE International Conference on Computer Vision Workshops. the IEEE International Conference on Computer Vision WorkshopsOkan Kopuklu, Neslihan Kose, Ahmet Gunduz, and Ger- hard Rigoll. Resource efficient 3d convolutional neural net- works. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pages 0-0, 2019. 7\n\nBlind image quality assessment: A natural scene statistics approach in the dct domain. M A Saad, A C Bovik, C Charrier, IEEE Transactions on Image Processing. 218M. A. Saad, A. C. Bovik, and C. Charrier. Blind image quality assessment: A natural scene statistics approach in the dct domain. IEEE Transactions on Image Processing, 21(8):3339-3352, 2012. 8\n\nThe 1cycle policy. [78] fastai. The 1cycle policy. [Online] Available: https://fastai1.fast.ai/callbacks.one_ cycle.html.\n\nSupplementary Material -Patch-VQ: 'Patching Up' the Video Quality Problem A. Cropping Patches Deciding number of scales for cropping v-patches: In a psychometric study, specifically based on evaluating video quality, a subject needs roughly 15-20 seconds to rate each content. This limited the number of v-patches we could collect ratings on, and thus we decided to only include one scale for each type of v-patches. Scale here defines the dimensions of the v-patches, or the proportion of the video data contained in the patches. For simplicity, we use the same scale (40% of original dimensions) for extracting the three types of v-patches. Additional examples of extracted v-patch triplets have been shown in Fig. 1. Deciding size of v-patches: Empirically, sv-patches cropped at large scales are not local enough, and do not capture the local quality features satisfactorily. Alternately, smaller scales result in tv-patches too short in duration to collect reliable judgements. Similarly, the resulting stv-patches are too small and short to rate comprehensibly and reliably. R Likert, Archives of Psychology. 140A technique for the measurement of attitudes. We determined 40% to be the most suitable scale after examining v-patch samplesR. Likert. A technique for the measurement of attitudes. Archives of Psychology, vol. 140, pp. 1-55, 1932. Supplementary Material - Patch-VQ: 'Patching Up' the Video Quality Problem A. Cropping Patches Deciding number of scales for cropping v-patches: In a psychometric study, specifically based on evaluating video quality, a subject needs roughly 15-20 seconds to rate each content. This limited the number of v-patches we could collect ratings on, and thus we decided to only include one scale for each type of v-patches. Scale here defines the dimensions of the v-patches, or the proportion of the video data contained in the patches. For simplicity, we use the same scale (40% of original dimensions) for extracting the three types of v-patches. Additional examples of extracted v-patch triplets have been shown in Fig. 1. Deciding size of v-patches: Empirically, sv-patches cropped at large scales are not local enough, and do not capture the local quality features satisfactorily. Alternately, smaller scales result in tv-patches too short in duration to collect reliable judgements. Similarly, the resulting stv-patches are too small and short to rate comprehensibly and reliably. We determined 40% to be the most suitable scale after examining v-patch samples.\n", "annotations": {"author": "[{\"end\":100,\"start\":53},{\"end\":170,\"start\":101},{\"end\":219,\"start\":171},{\"end\":222,\"start\":220},{\"end\":287,\"start\":223}]", "publisher": null, "author_last_name": "[{\"end\":67,\"start\":63},{\"end\":118,\"start\":112},{\"end\":188,\"start\":178},{\"end\":233,\"start\":228}]", "author_first_name": "[{\"end\":62,\"start\":53},{\"end\":111,\"start\":101},{\"end\":177,\"start\":171},{\"end\":221,\"start\":220},{\"end\":227,\"start\":223}]", "author_affiliation": "[{\"end\":99,\"start\":69},{\"end\":169,\"start\":139},{\"end\":218,\"start\":206},{\"end\":286,\"start\":256}]", "title": "[{\"end\":50,\"start\":1},{\"end\":337,\"start\":288}]", "venue": null, "abstract": "[{\"end\":1523,\"start\":339}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1727,\"start\":1724},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1861,\"start\":1858},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2742,\"start\":2739},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3375,\"start\":3372},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3440,\"start\":3437},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3546,\"start\":3543},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3671,\"start\":3668},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3673,\"start\":3671},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3676,\"start\":3673},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3679,\"start\":3676},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3682,\"start\":3679},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3685,\"start\":3682},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3688,\"start\":3685},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3788,\"start\":3784},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3791,\"start\":3788},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3794,\"start\":3791},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4256,\"start\":4252},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4259,\"start\":4256},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4262,\"start\":4259},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4265,\"start\":4262},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4268,\"start\":4265},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4271,\"start\":4268},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4274,\"start\":4271},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4541,\"start\":4537},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5750,\"start\":5746},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6312,\"start\":6308},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6362,\"start\":6358},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6420,\"start\":6416},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6700,\"start\":6696},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6703,\"start\":6700},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7106,\"start\":7103},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7108,\"start\":7106},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7111,\"start\":7108},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7114,\"start\":7111},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7117,\"start\":7114},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7120,\"start\":7117},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7123,\"start\":7120},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7576,\"start\":7572},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7579,\"start\":7576},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7666,\"start\":7662},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7681,\"start\":7677},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7703,\"start\":7699},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7840,\"start\":7836},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8124,\"start\":8120},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8506,\"start\":8502},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8509,\"start\":8506},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8512,\"start\":8509},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8515,\"start\":8512},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8518,\"start\":8515},{\"end\":8521,\"start\":8518},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8524,\"start\":8521},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8760,\"start\":8756},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8763,\"start\":8760},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8766,\"start\":8763},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8769,\"start\":8766},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8982,\"start\":8978},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8985,\"start\":8982},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9315,\"start\":9311},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9318,\"start\":9315},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":9321,\"start\":9318},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":9324,\"start\":9321},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9327,\"start\":9324},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9330,\"start\":9327},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":9333,\"start\":9330},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":9336,\"start\":9333},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":9404,\"start\":9400},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9530,\"start\":9526},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":9674,\"start\":9670},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":9677,\"start\":9674},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":9680,\"start\":9677},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9683,\"start\":9680},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9726,\"start\":9723},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9729,\"start\":9726},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9732,\"start\":9729},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9778,\"start\":9774},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9781,\"start\":9778},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9784,\"start\":9781},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9799,\"start\":9795},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9832,\"start\":9828},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10383,\"start\":10380},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10385,\"start\":10383},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10388,\"start\":10385},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10391,\"start\":10388},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10434,\"start\":10430},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10437,\"start\":10434},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10440,\"start\":10437},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10467,\"start\":10463},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":10726,\"start\":10722},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":10745,\"start\":10741},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":10883,\"start\":10879},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11029,\"start\":11025},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":11847,\"start\":11843},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":11879,\"start\":11875},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":11919,\"start\":11915},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":12020,\"start\":12016},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":13951,\"start\":13947},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13954,\"start\":13951},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":13957,\"start\":13954},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":13960,\"start\":13957},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":13963,\"start\":13960},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15326,\"start\":15322},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":15329,\"start\":15326},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":16603,\"start\":16599},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":17291,\"start\":17287},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":17509,\"start\":17505},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":17604,\"start\":17600},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":17676,\"start\":17672},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":17931,\"start\":17927},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":17934,\"start\":17931},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":18082,\"start\":18078},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":18487,\"start\":18483},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19336,\"start\":19332},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19339,\"start\":19336},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19342,\"start\":19339},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19710,\"start\":19706},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19725,\"start\":19721},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":19747,\"start\":19743},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21528,\"start\":21524},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":21676,\"start\":21672},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":21727,\"start\":21723},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":22096,\"start\":22092},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":22099,\"start\":22096},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":22460,\"start\":22456},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22864,\"start\":22860},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":23298,\"start\":23294},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":23301,\"start\":23298},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":23340,\"start\":23336},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23779,\"start\":23775},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":23796,\"start\":23792},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":23883,\"start\":23879},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":23926,\"start\":23922},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":24573,\"start\":24569},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":24622,\"start\":24618},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":24633,\"start\":24629},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":24651,\"start\":24647},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":26536,\"start\":26532},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":26539,\"start\":26536},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":26831,\"start\":26827},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":30252,\"start\":30248},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30270,\"start\":30266},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":34067,\"start\":34063},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":35037,\"start\":35033},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":35299,\"start\":35295},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":37444,\"start\":37440},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":44118,\"start\":44114},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":44135,\"start\":44131}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":38090,\"start\":37588},{\"attributes\":{\"id\":\"fig_1\"},\"end\":38228,\"start\":38091},{\"attributes\":{\"id\":\"fig_2\"},\"end\":38455,\"start\":38229},{\"attributes\":{\"id\":\"fig_3\"},\"end\":38604,\"start\":38456},{\"attributes\":{\"id\":\"fig_4\"},\"end\":38667,\"start\":38605},{\"attributes\":{\"id\":\"fig_5\"},\"end\":38826,\"start\":38668},{\"attributes\":{\"id\":\"fig_6\"},\"end\":38989,\"start\":38827},{\"attributes\":{\"id\":\"fig_7\"},\"end\":39113,\"start\":38990},{\"attributes\":{\"id\":\"fig_8\"},\"end\":39481,\"start\":39114},{\"attributes\":{\"id\":\"fig_9\"},\"end\":39687,\"start\":39482},{\"attributes\":{\"id\":\"fig_10\"},\"end\":39977,\"start\":39688},{\"attributes\":{\"id\":\"fig_11\"},\"end\":40235,\"start\":39978},{\"attributes\":{\"id\":\"fig_12\"},\"end\":40264,\"start\":40236},{\"attributes\":{\"id\":\"fig_13\"},\"end\":40442,\"start\":40265},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":41599,\"start\":40443},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":43097,\"start\":41600},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":43588,\"start\":43098},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":43998,\"start\":43589},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":44460,\"start\":43999},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":44989,\"start\":44461}]", "paragraph": "[{\"end\":2099,\"start\":1539},{\"end\":3547,\"start\":2101},{\"end\":4162,\"start\":3549},{\"end\":4662,\"start\":4164},{\"end\":5395,\"start\":4664},{\"end\":7017,\"start\":5397},{\"end\":9927,\"start\":7034},{\"end\":10559,\"start\":9967},{\"end\":11578,\"start\":10624},{\"end\":11785,\"start\":11580},{\"end\":11848,\"start\":11822},{\"end\":11880,\"start\":11850},{\"end\":11920,\"start\":11882},{\"end\":12188,\"start\":11922},{\"end\":12811,\"start\":12190},{\"end\":13806,\"start\":12838},{\"end\":14277,\"start\":13835},{\"end\":15170,\"start\":14298},{\"end\":16788,\"start\":15192},{\"end\":16882,\"start\":16790},{\"end\":17094,\"start\":16900},{\"end\":17814,\"start\":17096},{\"end\":19748,\"start\":17832},{\"end\":20037,\"start\":19793},{\"end\":21309,\"start\":20050},{\"end\":21728,\"start\":21332},{\"end\":22768,\"start\":21748},{\"end\":23464,\"start\":22792},{\"end\":24353,\"start\":23480},{\"end\":26351,\"start\":24389},{\"end\":27628,\"start\":26380},{\"end\":30056,\"start\":27667},{\"end\":31122,\"start\":30087},{\"end\":32968,\"start\":31145},{\"end\":33961,\"start\":33016},{\"end\":34473,\"start\":34013},{\"end\":35526,\"start\":34500},{\"end\":35881,\"start\":35550},{\"end\":36286,\"start\":35949},{\"end\":36796,\"start\":36288},{\"end\":37587,\"start\":36815}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11821,\"start\":11786}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":24496,\"start\":24488},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":25087,\"start\":25080},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":25668,\"start\":25661},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":26076,\"start\":26066},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":26752,\"start\":26744},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":30280,\"start\":30271},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":30440,\"start\":30433},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":30774,\"start\":30767}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1537,\"start\":1525},{\"attributes\":{\"n\":\"2.\"},\"end\":7032,\"start\":7020},{\"attributes\":{\"n\":\"3.\"},\"end\":9965,\"start\":9930},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10582,\"start\":10562},{\"attributes\":{\"n\":\"3.1.1\"},\"end\":10622,\"start\":10585},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":12836,\"start\":12814},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13833,\"start\":13809},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":14296,\"start\":14280},{\"attributes\":{\"n\":\"3.2.2\"},\"end\":15190,\"start\":15173},{\"attributes\":{\"n\":\"3.2.3\"},\"end\":16898,\"start\":16885},{\"attributes\":{\"n\":\"3.2.4\"},\"end\":17830,\"start\":17817},{\"attributes\":{\"n\":\"4.\"},\"end\":19791,\"start\":19751},{\"attributes\":{\"n\":\"4.1.\"},\"end\":20048,\"start\":20040},{\"attributes\":{\"n\":\"4.2.\"},\"end\":21330,\"start\":21312},{\"attributes\":{\"n\":\"4.3.\"},\"end\":21746,\"start\":21731},{\"attributes\":{\"n\":\"4.4.\"},\"end\":22790,\"start\":22771},{\"attributes\":{\"n\":\"5.\"},\"end\":23478,\"start\":23467},{\"attributes\":{\"n\":\"5.1.\"},\"end\":24387,\"start\":24356},{\"end\":26378,\"start\":26354},{\"attributes\":{\"n\":\"5.2.\"},\"end\":27665,\"start\":27631},{\"attributes\":{\"n\":\"5.3.\"},\"end\":30085,\"start\":30059},{\"attributes\":{\"n\":\"6.\"},\"end\":31143,\"start\":31125},{\"end\":33014,\"start\":32971},{\"end\":34011,\"start\":33964},{\"end\":34498,\"start\":34476},{\"end\":35548,\"start\":35529},{\"end\":35921,\"start\":35884},{\"end\":35947,\"start\":35924},{\"end\":36813,\"start\":36799},{\"end\":37597,\"start\":37589},{\"end\":38100,\"start\":38092},{\"end\":38238,\"start\":38230},{\"end\":38465,\"start\":38457},{\"end\":38614,\"start\":38606},{\"end\":38677,\"start\":38669},{\"end\":38836,\"start\":38828},{\"end\":38999,\"start\":38991},{\"end\":39124,\"start\":39115},{\"end\":39491,\"start\":39483},{\"end\":39697,\"start\":39689},{\"end\":39980,\"start\":39979},{\"end\":40245,\"start\":40237},{\"end\":40342,\"start\":40266},{\"end\":40453,\"start\":40444},{\"end\":41610,\"start\":41601},{\"end\":43108,\"start\":43099},{\"end\":43599,\"start\":43590},{\"end\":44009,\"start\":44000},{\"end\":44471,\"start\":44462}]", "table": "[{\"end\":41599,\"start\":40673},{\"end\":43097,\"start\":41742},{\"end\":43588,\"start\":43196},{\"end\":43998,\"start\":43708},{\"end\":44460,\"start\":44136},{\"end\":44989,\"start\":44478}]", "figure_caption": "[{\"end\":38090,\"start\":37599},{\"end\":38228,\"start\":38102},{\"end\":38455,\"start\":38240},{\"end\":38604,\"start\":38467},{\"end\":38667,\"start\":38616},{\"end\":38826,\"start\":38679},{\"end\":38989,\"start\":38838},{\"end\":39113,\"start\":39001},{\"end\":39481,\"start\":39127},{\"end\":39687,\"start\":39493},{\"end\":39977,\"start\":39699},{\"end\":40235,\"start\":39981},{\"end\":40264,\"start\":40247},{\"end\":40442,\"start\":40353},{\"end\":40673,\"start\":40455},{\"end\":41742,\"start\":41612},{\"end\":43196,\"start\":43110},{\"end\":43708,\"start\":43601},{\"end\":44136,\"start\":44011},{\"end\":44478,\"start\":44473}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5008,\"start\":5000},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12526,\"start\":12519},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12590,\"start\":12584},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13257,\"start\":13251},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":14360,\"start\":14353},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":18833,\"start\":18828},{\"end\":19204,\"start\":19198},{\"end\":19590,\"start\":19584},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":20646,\"start\":20640},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":24948,\"start\":24936},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26232,\"start\":26224},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":26887,\"start\":26878},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":27193,\"start\":27184},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":28680,\"start\":28673},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29175,\"start\":29168},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29305,\"start\":29294},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29376,\"start\":29365},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29852,\"start\":29845},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":32541,\"start\":32535},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":37000,\"start\":36993},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":37123,\"start\":37106},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":37313,\"start\":37306},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":37420,\"start\":37413},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37521,\"start\":37514}]", "bib_author_first_name": "[{\"end\":45410,\"start\":45404},{\"end\":45657,\"start\":45656},{\"end\":45665,\"start\":45664},{\"end\":45667,\"start\":45666},{\"end\":45676,\"start\":45675},{\"end\":45678,\"start\":45677},{\"end\":45688,\"start\":45687},{\"end\":45690,\"start\":45689},{\"end\":46042,\"start\":46041},{\"end\":46050,\"start\":46049},{\"end\":46052,\"start\":46051},{\"end\":46341,\"start\":46340},{\"end\":46349,\"start\":46348},{\"end\":46356,\"start\":46355},{\"end\":46358,\"start\":46357},{\"end\":46689,\"start\":46688},{\"end\":46697,\"start\":46692},{\"end\":46699,\"start\":46698},{\"end\":46719,\"start\":46718},{\"end\":47059,\"start\":47058},{\"end\":47069,\"start\":47062},{\"end\":47071,\"start\":47070},{\"end\":47081,\"start\":47080},{\"end\":47094,\"start\":47093},{\"end\":47110,\"start\":47109},{\"end\":47534,\"start\":47533},{\"end\":47552,\"start\":47551},{\"end\":47569,\"start\":47568},{\"end\":47571,\"start\":47570},{\"end\":47580,\"start\":47579},{\"end\":47582,\"start\":47581},{\"end\":47881,\"start\":47880},{\"end\":47891,\"start\":47890},{\"end\":47899,\"start\":47898},{\"end\":48090,\"start\":48089},{\"end\":48098,\"start\":48097},{\"end\":48105,\"start\":48104},{\"end\":48111,\"start\":48110},{\"end\":48113,\"start\":48112},{\"end\":48120,\"start\":48119},{\"end\":48127,\"start\":48126},{\"end\":48135,\"start\":48134},{\"end\":48143,\"start\":48142},{\"end\":48160,\"start\":48159},{\"end\":48169,\"start\":48168},{\"end\":48173,\"start\":48170},{\"end\":48596,\"start\":48595},{\"end\":48604,\"start\":48603},{\"end\":48621,\"start\":48620},{\"end\":48629,\"start\":48628},{\"end\":48637,\"start\":48636},{\"end\":48646,\"start\":48643},{\"end\":48656,\"start\":48653},{\"end\":48663,\"start\":48662},{\"end\":48670,\"start\":48669},{\"end\":48678,\"start\":48677},{\"end\":48686,\"start\":48685},{\"end\":48695,\"start\":48694},{\"end\":48704,\"start\":48703},{\"end\":48713,\"start\":48712},{\"end\":48715,\"start\":48714},{\"end\":48721,\"start\":48718},{\"end\":49232,\"start\":49231},{\"end\":49234,\"start\":49233},{\"end\":49240,\"start\":49239},{\"end\":49242,\"start\":49241},{\"end\":49521,\"start\":49520},{\"end\":49529,\"start\":49528},{\"end\":49537,\"start\":49536},{\"end\":49550,\"start\":49549},{\"end\":49557,\"start\":49556},{\"end\":49564,\"start\":49563},{\"end\":49576,\"start\":49575},{\"end\":49582,\"start\":49581},{\"end\":50055,\"start\":50054},{\"end\":50064,\"start\":50063},{\"end\":50066,\"start\":50065},{\"end\":50444,\"start\":50443},{\"end\":50452,\"start\":50451},{\"end\":50621,\"start\":50620},{\"end\":50623,\"start\":50622},{\"end\":50632,\"start\":50631},{\"end\":50634,\"start\":50633},{\"end\":50642,\"start\":50641},{\"end\":50953,\"start\":50952},{\"end\":50964,\"start\":50963},{\"end\":50974,\"start\":50973},{\"end\":50984,\"start\":50983},{\"end\":51387,\"start\":51386},{\"end\":51395,\"start\":51390},{\"end\":51397,\"start\":51396},{\"end\":51404,\"start\":51403},{\"end\":51410,\"start\":51409},{\"end\":51753,\"start\":51752},{\"end\":51755,\"start\":51754},{\"end\":51763,\"start\":51762},{\"end\":51773,\"start\":51772},{\"end\":51775,\"start\":51774},{\"end\":52057,\"start\":52056},{\"end\":52354,\"start\":52353},{\"end\":52360,\"start\":52359},{\"end\":52368,\"start\":52367},{\"end\":52380,\"start\":52379},{\"end\":52393,\"start\":52392},{\"end\":52395,\"start\":52394},{\"end\":52605,\"start\":52604},{\"end\":52611,\"start\":52610},{\"end\":52620,\"start\":52619},{\"end\":52793,\"start\":52792},{\"end\":52811,\"start\":52810},{\"end\":52813,\"start\":52812},{\"end\":53204,\"start\":53203},{\"end\":53210,\"start\":53209},{\"end\":53218,\"start\":53217},{\"end\":53227,\"start\":53226},{\"end\":53235,\"start\":53234},{\"end\":53243,\"start\":53242},{\"end\":53465,\"start\":53464},{\"end\":53477,\"start\":53476},{\"end\":53489,\"start\":53488},{\"end\":53506,\"start\":53505},{\"end\":53515,\"start\":53514},{\"end\":53527,\"start\":53526},{\"end\":53912,\"start\":53911},{\"end\":53914,\"start\":53913},{\"end\":53923,\"start\":53922},{\"end\":53925,\"start\":53924},{\"end\":53936,\"start\":53935},{\"end\":53945,\"start\":53944},{\"end\":53959,\"start\":53958},{\"end\":53966,\"start\":53965},{\"end\":53968,\"start\":53967},{\"end\":54419,\"start\":54418},{\"end\":54427,\"start\":54426},{\"end\":54434,\"start\":54433},{\"end\":54443,\"start\":54442},{\"end\":54454,\"start\":54453},{\"end\":54468,\"start\":54467},{\"end\":54470,\"start\":54469},{\"end\":54909,\"start\":54908},{\"end\":54917,\"start\":54916},{\"end\":54928,\"start\":54927},{\"end\":55383,\"start\":55382},{\"end\":55385,\"start\":55384},{\"end\":55394,\"start\":55393},{\"end\":55403,\"start\":55402},{\"end\":55416,\"start\":55415},{\"end\":55429,\"start\":55428},{\"end\":55431,\"start\":55430},{\"end\":55442,\"start\":55441},{\"end\":55451,\"start\":55450},{\"end\":55453,\"start\":55452},{\"end\":55461,\"start\":55460},{\"end\":55477,\"start\":55473},{\"end\":55487,\"start\":55486},{\"end\":55796,\"start\":55795},{\"end\":55798,\"start\":55797},{\"end\":55806,\"start\":55805},{\"end\":55814,\"start\":55813},{\"end\":55821,\"start\":55820},{\"end\":56487,\"start\":56483},{\"end\":56489,\"start\":56488},{\"end\":56499,\"start\":56498},{\"end\":56515,\"start\":56514},{\"end\":56948,\"start\":56947},{\"end\":56964,\"start\":56963},{\"end\":56977,\"start\":56976},{\"end\":56986,\"start\":56985},{\"end\":57428,\"start\":57427},{\"end\":57430,\"start\":57429},{\"end\":57439,\"start\":57438},{\"end\":57455,\"start\":57454},{\"end\":57465,\"start\":57464},{\"end\":57907,\"start\":57906},{\"end\":57922,\"start\":57921},{\"end\":57933,\"start\":57932},{\"end\":58243,\"start\":58242},{\"end\":58254,\"start\":58253},{\"end\":58256,\"start\":58255},{\"end\":58264,\"start\":58263},{\"end\":58266,\"start\":58265},{\"end\":58276,\"start\":58275},{\"end\":58568,\"start\":58567},{\"end\":58570,\"start\":58569},{\"end\":58582,\"start\":58581},{\"end\":58814,\"start\":58813},{\"end\":58825,\"start\":58824},{\"end\":58835,\"start\":58834},{\"end\":59211,\"start\":59210},{\"end\":59217,\"start\":59216},{\"end\":59224,\"start\":59223},{\"end\":59460,\"start\":59459},{\"end\":59469,\"start\":59468},{\"end\":59471,\"start\":59470},{\"end\":59784,\"start\":59783},{\"end\":59798,\"start\":59797},{\"end\":59800,\"start\":59799},{\"end\":60246,\"start\":60245},{\"end\":60253,\"start\":60252},{\"end\":60261,\"start\":60260},{\"end\":60275,\"start\":60274},{\"end\":60282,\"start\":60281},{\"end\":60291,\"start\":60290},{\"end\":60293,\"start\":60292},{\"end\":60650,\"start\":60649},{\"end\":60659,\"start\":60658},{\"end\":60669,\"start\":60668},{\"end\":60680,\"start\":60679},{\"end\":60954,\"start\":60953},{\"end\":60961,\"start\":60960},{\"end\":61198,\"start\":61197},{\"end\":61208,\"start\":61207},{\"end\":61478,\"start\":61477},{\"end\":61485,\"start\":61484},{\"end\":61502,\"start\":61501},{\"end\":61504,\"start\":61503},{\"end\":61823,\"start\":61822},{\"end\":61829,\"start\":61828},{\"end\":61836,\"start\":61835},{\"end\":61845,\"start\":61844},{\"end\":61855,\"start\":61854},{\"end\":61863,\"start\":61862},{\"end\":62212,\"start\":62211},{\"end\":62221,\"start\":62220},{\"end\":62228,\"start\":62227},{\"end\":62234,\"start\":62233},{\"end\":62240,\"start\":62239},{\"end\":62658,\"start\":62657},{\"end\":62668,\"start\":62667},{\"end\":62675,\"start\":62674},{\"end\":62682,\"start\":62681},{\"end\":63068,\"start\":63067},{\"end\":63078,\"start\":63077},{\"end\":63085,\"start\":63084},{\"end\":63352,\"start\":63344},{\"end\":63511,\"start\":63510},{\"end\":63521,\"start\":63520},{\"end\":63523,\"start\":63522},{\"end\":63533,\"start\":63532},{\"end\":63546,\"start\":63545},{\"end\":63558,\"start\":63557},{\"end\":63564,\"start\":63563},{\"end\":63574,\"start\":63573},{\"end\":63586,\"start\":63582},{\"end\":63930,\"start\":63929},{\"end\":63943,\"start\":63942},{\"end\":63958,\"start\":63957},{\"end\":63969,\"start\":63968},{\"end\":64270,\"start\":64269},{\"end\":64280,\"start\":64279},{\"end\":64282,\"start\":64281},{\"end\":64524,\"start\":64523},{\"end\":64903,\"start\":64902},{\"end\":64912,\"start\":64911},{\"end\":65336,\"start\":65335},{\"end\":65350,\"start\":65349},{\"end\":65352,\"start\":65351},{\"end\":65667,\"start\":65666},{\"end\":65674,\"start\":65673},{\"end\":65682,\"start\":65681},{\"end\":65882,\"start\":65874},{\"end\":66004,\"start\":66003},{\"end\":66016,\"start\":66015},{\"end\":66018,\"start\":66017},{\"end\":66320,\"start\":66319},{\"end\":66482,\"start\":66475},{\"end\":66489,\"start\":66483},{\"end\":66645,\"start\":66639},{\"end\":66649,\"start\":66646},{\"end\":66660,\"start\":66659},{\"end\":66667,\"start\":66663},{\"end\":66875,\"start\":66871},{\"end\":66885,\"start\":66881},{\"end\":66901,\"start\":66896},{\"end\":66917,\"start\":66912},{\"end\":66930,\"start\":66925},{\"end\":66950,\"start\":66940},{\"end\":66974,\"start\":66969},{\"end\":66985,\"start\":66982},{\"end\":66999,\"start\":66993},{\"end\":67010,\"start\":67006},{\"end\":67026,\"start\":67019},{\"end\":67043,\"start\":67037},{\"end\":67306,\"start\":67305},{\"end\":67537,\"start\":67536},{\"end\":67806,\"start\":67805},{\"end\":67814,\"start\":67813},{\"end\":67834,\"start\":67833},{\"end\":67845,\"start\":67844},{\"end\":67847,\"start\":67846},{\"end\":67855,\"start\":67854},{\"end\":67863,\"start\":67862},{\"end\":68261,\"start\":68260},{\"end\":68268,\"start\":68267},{\"end\":68588,\"start\":68587},{\"end\":68598,\"start\":68597},{\"end\":68600,\"start\":68599},{\"end\":68611,\"start\":68610},{\"end\":68613,\"start\":68612},{\"end\":68903,\"start\":68902},{\"end\":68909,\"start\":68908},{\"end\":68918,\"start\":68917},{\"end\":68925,\"start\":68924},{\"end\":69194,\"start\":69190},{\"end\":69210,\"start\":69204},{\"end\":69227,\"start\":69219},{\"end\":69239,\"start\":69233},{\"end\":69262,\"start\":69251},{\"end\":69718,\"start\":69714},{\"end\":69736,\"start\":69728},{\"end\":69748,\"start\":69743},{\"end\":69764,\"start\":69757},{\"end\":70224,\"start\":70223},{\"end\":70226,\"start\":70225},{\"end\":70234,\"start\":70233},{\"end\":70236,\"start\":70235},{\"end\":70245,\"start\":70244},{\"end\":71697,\"start\":71696}]", "bib_author_last_name": "[{\"end\":45127,\"start\":45119},{\"end\":45417,\"start\":45411},{\"end\":45425,\"start\":45419},{\"end\":45662,\"start\":45658},{\"end\":45673,\"start\":45668},{\"end\":45685,\"start\":45679},{\"end\":45701,\"start\":45691},{\"end\":46047,\"start\":46043},{\"end\":46058,\"start\":46053},{\"end\":46346,\"start\":46342},{\"end\":46353,\"start\":46350},{\"end\":46364,\"start\":46359},{\"end\":46716,\"start\":46700},{\"end\":46728,\"start\":46720},{\"end\":47078,\"start\":47072},{\"end\":47091,\"start\":47082},{\"end\":47107,\"start\":47095},{\"end\":47119,\"start\":47111},{\"end\":47549,\"start\":47535},{\"end\":47566,\"start\":47553},{\"end\":47577,\"start\":47572},{\"end\":47590,\"start\":47583},{\"end\":47888,\"start\":47882},{\"end\":47896,\"start\":47892},{\"end\":47908,\"start\":47900},{\"end\":48095,\"start\":48091},{\"end\":48102,\"start\":48099},{\"end\":48108,\"start\":48106},{\"end\":48117,\"start\":48114},{\"end\":48124,\"start\":48121},{\"end\":48132,\"start\":48128},{\"end\":48140,\"start\":48136},{\"end\":48157,\"start\":48144},{\"end\":48166,\"start\":48161},{\"end\":48177,\"start\":48174},{\"end\":48601,\"start\":48597},{\"end\":48618,\"start\":48605},{\"end\":48626,\"start\":48622},{\"end\":48634,\"start\":48630},{\"end\":48641,\"start\":48638},{\"end\":48651,\"start\":48647},{\"end\":48660,\"start\":48657},{\"end\":48667,\"start\":48664},{\"end\":48675,\"start\":48671},{\"end\":48683,\"start\":48679},{\"end\":48692,\"start\":48687},{\"end\":48701,\"start\":48696},{\"end\":48710,\"start\":48705},{\"end\":48725,\"start\":48722},{\"end\":49237,\"start\":49235},{\"end\":49251,\"start\":49243},{\"end\":49526,\"start\":49522},{\"end\":49534,\"start\":49530},{\"end\":49547,\"start\":49538},{\"end\":49554,\"start\":49551},{\"end\":49561,\"start\":49558},{\"end\":49573,\"start\":49565},{\"end\":49579,\"start\":49577},{\"end\":49588,\"start\":49583},{\"end\":50061,\"start\":50056},{\"end\":50072,\"start\":50067},{\"end\":50449,\"start\":50445},{\"end\":50459,\"start\":50453},{\"end\":50470,\"start\":50461},{\"end\":50629,\"start\":50624},{\"end\":50639,\"start\":50635},{\"end\":50651,\"start\":50643},{\"end\":50961,\"start\":50954},{\"end\":50971,\"start\":50965},{\"end\":50981,\"start\":50975},{\"end\":50992,\"start\":50985},{\"end\":51401,\"start\":51398},{\"end\":51407,\"start\":51405},{\"end\":51416,\"start\":51411},{\"end\":51760,\"start\":51756},{\"end\":51770,\"start\":51764},{\"end\":51781,\"start\":51776},{\"end\":52066,\"start\":52058},{\"end\":52357,\"start\":52355},{\"end\":52365,\"start\":52361},{\"end\":52377,\"start\":52369},{\"end\":52390,\"start\":52381},{\"end\":52401,\"start\":52396},{\"end\":52608,\"start\":52606},{\"end\":52617,\"start\":52612},{\"end\":52626,\"start\":52621},{\"end\":52808,\"start\":52794},{\"end\":52819,\"start\":52814},{\"end\":53207,\"start\":53205},{\"end\":53215,\"start\":53211},{\"end\":53224,\"start\":53219},{\"end\":53232,\"start\":53228},{\"end\":53240,\"start\":53236},{\"end\":53246,\"start\":53244},{\"end\":53474,\"start\":53466},{\"end\":53486,\"start\":53478},{\"end\":53503,\"start\":53490},{\"end\":53512,\"start\":53507},{\"end\":53524,\"start\":53516},{\"end\":53536,\"start\":53528},{\"end\":53920,\"start\":53915},{\"end\":53933,\"start\":53926},{\"end\":53942,\"start\":53937},{\"end\":53956,\"start\":53946},{\"end\":53963,\"start\":53960},{\"end\":53973,\"start\":53969},{\"end\":54424,\"start\":54420},{\"end\":54431,\"start\":54428},{\"end\":54440,\"start\":54435},{\"end\":54451,\"start\":54444},{\"end\":54465,\"start\":54455},{\"end\":54476,\"start\":54471},{\"end\":54914,\"start\":54910},{\"end\":54925,\"start\":54918},{\"end\":54934,\"start\":54929},{\"end\":55391,\"start\":55386},{\"end\":55400,\"start\":55395},{\"end\":55413,\"start\":55404},{\"end\":55426,\"start\":55417},{\"end\":55439,\"start\":55432},{\"end\":55448,\"start\":55443},{\"end\":55458,\"start\":55454},{\"end\":55471,\"start\":55462},{\"end\":55484,\"start\":55478},{\"end\":55497,\"start\":55488},{\"end\":55803,\"start\":55799},{\"end\":55811,\"start\":55807},{\"end\":55818,\"start\":55815},{\"end\":55827,\"start\":55822},{\"end\":56496,\"start\":56490},{\"end\":56512,\"start\":56500},{\"end\":56521,\"start\":56516},{\"end\":56527,\"start\":56523},{\"end\":56961,\"start\":56949},{\"end\":56974,\"start\":56965},{\"end\":56983,\"start\":56978},{\"end\":56993,\"start\":56987},{\"end\":57436,\"start\":57431},{\"end\":57452,\"start\":57440},{\"end\":57462,\"start\":57456},{\"end\":57474,\"start\":57466},{\"end\":57919,\"start\":57908},{\"end\":57930,\"start\":57923},{\"end\":57942,\"start\":57934},{\"end\":58251,\"start\":58244},{\"end\":58261,\"start\":58257},{\"end\":58273,\"start\":58267},{\"end\":58283,\"start\":58277},{\"end\":58579,\"start\":58571},{\"end\":58589,\"start\":58583},{\"end\":58822,\"start\":58815},{\"end\":58832,\"start\":58826},{\"end\":58843,\"start\":58836},{\"end\":59214,\"start\":59212},{\"end\":59221,\"start\":59218},{\"end\":59227,\"start\":59225},{\"end\":59466,\"start\":59461},{\"end\":59477,\"start\":59472},{\"end\":59795,\"start\":59785},{\"end\":59806,\"start\":59801},{\"end\":60250,\"start\":60247},{\"end\":60258,\"start\":60254},{\"end\":60272,\"start\":60262},{\"end\":60279,\"start\":60276},{\"end\":60288,\"start\":60283},{\"end\":60299,\"start\":60294},{\"end\":60656,\"start\":60651},{\"end\":60666,\"start\":60660},{\"end\":60677,\"start\":60670},{\"end\":60686,\"start\":60681},{\"end\":60958,\"start\":60955},{\"end\":60965,\"start\":60962},{\"end\":61205,\"start\":61199},{\"end\":61217,\"start\":61209},{\"end\":61482,\"start\":61479},{\"end\":61499,\"start\":61486},{\"end\":61513,\"start\":61505},{\"end\":61826,\"start\":61824},{\"end\":61833,\"start\":61830},{\"end\":61842,\"start\":61837},{\"end\":61852,\"start\":61846},{\"end\":61860,\"start\":61856},{\"end\":61867,\"start\":61864},{\"end\":62218,\"start\":62213},{\"end\":62225,\"start\":62222},{\"end\":62231,\"start\":62229},{\"end\":62237,\"start\":62235},{\"end\":62243,\"start\":62241},{\"end\":62665,\"start\":62659},{\"end\":62672,\"start\":62669},{\"end\":62679,\"start\":62676},{\"end\":62686,\"start\":62683},{\"end\":62691,\"start\":62688},{\"end\":63075,\"start\":63069},{\"end\":63082,\"start\":63079},{\"end\":63090,\"start\":63086},{\"end\":63360,\"start\":63353},{\"end\":63518,\"start\":63512},{\"end\":63530,\"start\":63524},{\"end\":63543,\"start\":63534},{\"end\":63555,\"start\":63547},{\"end\":63561,\"start\":63559},{\"end\":63571,\"start\":63565},{\"end\":63580,\"start\":63575},{\"end\":63589,\"start\":63587},{\"end\":63802,\"start\":63796},{\"end\":63940,\"start\":63931},{\"end\":63955,\"start\":63944},{\"end\":63966,\"start\":63959},{\"end\":63977,\"start\":63970},{\"end\":64277,\"start\":64271},{\"end\":64293,\"start\":64283},{\"end\":64529,\"start\":64525},{\"end\":64909,\"start\":64904},{\"end\":64918,\"start\":64913},{\"end\":65347,\"start\":65337},{\"end\":65358,\"start\":65353},{\"end\":65671,\"start\":65668},{\"end\":65679,\"start\":65675},{\"end\":65688,\"start\":65683},{\"end\":65893,\"start\":65883},{\"end\":66013,\"start\":66005},{\"end\":66026,\"start\":66019},{\"end\":66326,\"start\":66321},{\"end\":66497,\"start\":66490},{\"end\":66657,\"start\":66650},{\"end\":66678,\"start\":66668},{\"end\":66879,\"start\":66876},{\"end\":66894,\"start\":66886},{\"end\":66910,\"start\":66902},{\"end\":66923,\"start\":66918},{\"end\":66938,\"start\":66931},{\"end\":66967,\"start\":66951},{\"end\":66980,\"start\":66975},{\"end\":66991,\"start\":66986},{\"end\":67004,\"start\":67000},{\"end\":67017,\"start\":67011},{\"end\":67035,\"start\":67027},{\"end\":67053,\"start\":67044},{\"end\":67315,\"start\":67307},{\"end\":67546,\"start\":67538},{\"end\":67811,\"start\":67807},{\"end\":67831,\"start\":67815},{\"end\":67842,\"start\":67835},{\"end\":67852,\"start\":67848},{\"end\":67860,\"start\":67856},{\"end\":67874,\"start\":67864},{\"end\":68265,\"start\":68262},{\"end\":68277,\"start\":68269},{\"end\":68595,\"start\":68589},{\"end\":68608,\"start\":68601},{\"end\":68619,\"start\":68614},{\"end\":68906,\"start\":68904},{\"end\":68915,\"start\":68910},{\"end\":68922,\"start\":68919},{\"end\":68929,\"start\":68926},{\"end\":69202,\"start\":69195},{\"end\":69217,\"start\":69211},{\"end\":69231,\"start\":69228},{\"end\":69249,\"start\":69240},{\"end\":69267,\"start\":69263},{\"end\":69726,\"start\":69719},{\"end\":69741,\"start\":69737},{\"end\":69755,\"start\":69749},{\"end\":69771,\"start\":69765},{\"end\":70231,\"start\":70227},{\"end\":70242,\"start\":70237},{\"end\":70254,\"start\":70246},{\"end\":71704,\"start\":71698}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":45213,\"start\":45096},{\"attributes\":{\"id\":\"b1\"},\"end\":45344,\"start\":45215},{\"attributes\":{\"id\":\"b2\"},\"end\":45580,\"start\":45346},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":207761262},\"end\":45958,\"start\":45582},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":2492436},\"end\":46261,\"start\":45960},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":16071270},\"end\":46601,\"start\":46263},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":195713530},\"end\":46990,\"start\":46603},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":17728147},\"end\":47468,\"start\":46992},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":206724285},\"end\":47838,\"start\":47470},{\"attributes\":{\"id\":\"b9\"},\"end\":48022,\"start\":47840},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":11387097},\"end\":48511,\"start\":48024},{\"attributes\":{\"id\":\"b11\"},\"end\":48976,\"start\":48513},{\"attributes\":{\"id\":\"b12\"},\"end\":49130,\"start\":48978},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":6066775},\"end\":49469,\"start\":49132},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":9136312},\"end\":49896,\"start\":49471},{\"attributes\":{\"id\":\"b15\"},\"end\":50005,\"start\":49898},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":52285071},\"end\":50389,\"start\":50007},{\"attributes\":{\"id\":\"b17\"},\"end\":50575,\"start\":50391},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":14314450},\"end\":50857,\"start\":50577},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":5398303},\"end\":51290,\"start\":50859},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":8892096},\"end\":51707,\"start\":51292},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":11198468},\"end\":51983,\"start\":51709},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":195760206},\"end\":52270,\"start\":51985},{\"attributes\":{\"id\":\"b23\"},\"end\":52561,\"start\":52272},{\"attributes\":{\"id\":\"b24\"},\"end\":52722,\"start\":52563},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":2754510},\"end\":53135,\"start\":52724},{\"attributes\":{\"id\":\"b26\"},\"end\":53378,\"start\":53137},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":14124129},\"end\":53815,\"start\":53380},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":2727938},\"end\":54329,\"start\":53817},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":209444425},\"end\":54822,\"start\":54331},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":400882},\"end\":55317,\"start\":54824},{\"attributes\":{\"id\":\"b31\"},\"end\":55710,\"start\":55319},{\"attributes\":{\"id\":\"b32\"},\"end\":55967,\"start\":55712},{\"attributes\":{\"id\":\"b33\"},\"end\":56352,\"start\":55969},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":2902438},\"end\":56871,\"start\":56354},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":1188252},\"end\":57291,\"start\":56873},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":6664478},\"end\":57785,\"start\":57293},{\"attributes\":{\"id\":\"b37\"},\"end\":58147,\"start\":57787},{\"attributes\":{\"id\":\"b38\"},\"end\":58460,\"start\":58149},{\"attributes\":{\"id\":\"b39\"},\"end\":58746,\"start\":58462},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":14205119},\"end\":59152,\"start\":58748},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":16748218},\"end\":59416,\"start\":59154},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":202773173},\"end\":59701,\"start\":59418},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":15597137},\"end\":60112,\"start\":59703},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":10512190},\"end\":60595,\"start\":60114},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":15472226},\"end\":60909,\"start\":60597},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":13088241},\"end\":61164,\"start\":60911},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":13911460},\"end\":61400,\"start\":61166},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":6736352},\"end\":61750,\"start\":61402},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":4830804},\"end\":62121,\"start\":61752},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":69996769},\"end\":62539,\"start\":62123},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":52956148},\"end\":62980,\"start\":62541},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":53034536},\"end\":63320,\"start\":62982},{\"attributes\":{\"id\":\"b53\"},\"end\":63461,\"start\":63322},{\"attributes\":{\"id\":\"b54\"},\"end\":63792,\"start\":63463},{\"attributes\":{\"id\":\"b55\"},\"end\":63852,\"start\":63794},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":206753086},\"end\":64225,\"start\":63854},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":7055315},\"end\":64493,\"start\":64227},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":1434077},\"end\":64653,\"start\":64495},{\"attributes\":{\"id\":\"b59\"},\"end\":64804,\"start\":64655},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":216014943},\"end\":65254,\"start\":64806},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":8531217},\"end\":65593,\"start\":65256},{\"attributes\":{\"doi\":\"arXiv:1803.08489\",\"id\":\"b62\"},\"end\":65870,\"start\":65595},{\"attributes\":{\"id\":\"b63\"},\"end\":66001,\"start\":65872},{\"attributes\":{\"id\":\"b64\"},\"end\":66290,\"start\":66003},{\"attributes\":{\"id\":\"b65\"},\"end\":66447,\"start\":66292},{\"attributes\":{\"id\":\"b66\"},\"end\":66556,\"start\":66449},{\"attributes\":{\"id\":\"b67\"},\"end\":66828,\"start\":66558},{\"attributes\":{\"id\":\"b68\"},\"end\":67291,\"start\":66830},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":206770307},\"end\":67454,\"start\":67293},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":10328909},\"end\":67729,\"start\":67456},{\"attributes\":{\"id\":\"b71\"},\"end\":68041,\"start\":67731},{\"attributes\":{\"id\":\"b72\"},\"end\":68195,\"start\":68043},{\"attributes\":{\"id\":\"b73\"},\"end\":68524,\"start\":68197},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":2927709},\"end\":68854,\"start\":68526},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":206594692},\"end\":69132,\"start\":68856},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":4555207},\"end\":69659,\"start\":69134},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":102353321},\"end\":70134,\"start\":69661},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":5709739},\"end\":70490,\"start\":70136},{\"attributes\":{\"id\":\"b79\"},\"end\":70613,\"start\":70492},{\"attributes\":{\"id\":\"b80\"},\"end\":73127,\"start\":70615}]", "bib_title": "[{\"end\":45654,\"start\":45582},{\"end\":46039,\"start\":45960},{\"end\":46338,\"start\":46263},{\"end\":46686,\"start\":46603},{\"end\":47056,\"start\":46992},{\"end\":47531,\"start\":47470},{\"end\":48087,\"start\":48024},{\"end\":49229,\"start\":49132},{\"end\":49518,\"start\":49471},{\"end\":50052,\"start\":50007},{\"end\":50618,\"start\":50577},{\"end\":50950,\"start\":50859},{\"end\":51384,\"start\":51292},{\"end\":51750,\"start\":51709},{\"end\":52054,\"start\":51985},{\"end\":52790,\"start\":52724},{\"end\":53462,\"start\":53380},{\"end\":53909,\"start\":53817},{\"end\":54416,\"start\":54331},{\"end\":54906,\"start\":54824},{\"end\":56481,\"start\":56354},{\"end\":56945,\"start\":56873},{\"end\":57425,\"start\":57293},{\"end\":58811,\"start\":58748},{\"end\":59208,\"start\":59154},{\"end\":59457,\"start\":59418},{\"end\":59781,\"start\":59703},{\"end\":60243,\"start\":60114},{\"end\":60647,\"start\":60597},{\"end\":60951,\"start\":60911},{\"end\":61195,\"start\":61166},{\"end\":61475,\"start\":61402},{\"end\":61820,\"start\":61752},{\"end\":62209,\"start\":62123},{\"end\":62655,\"start\":62541},{\"end\":63065,\"start\":62982},{\"end\":63927,\"start\":63854},{\"end\":64267,\"start\":64227},{\"end\":64521,\"start\":64495},{\"end\":64900,\"start\":64806},{\"end\":65333,\"start\":65256},{\"end\":67303,\"start\":67293},{\"end\":67534,\"start\":67456},{\"end\":68258,\"start\":68197},{\"end\":68585,\"start\":68526},{\"end\":68900,\"start\":68856},{\"end\":69188,\"start\":69134},{\"end\":69712,\"start\":69661},{\"end\":70221,\"start\":70136},{\"end\":71694,\"start\":70615}]", "bib_author": "[{\"end\":45129,\"start\":45119},{\"end\":45419,\"start\":45404},{\"end\":45427,\"start\":45419},{\"end\":45664,\"start\":45656},{\"end\":45675,\"start\":45664},{\"end\":45687,\"start\":45675},{\"end\":45703,\"start\":45687},{\"end\":46049,\"start\":46041},{\"end\":46060,\"start\":46049},{\"end\":46348,\"start\":46340},{\"end\":46355,\"start\":46348},{\"end\":46366,\"start\":46355},{\"end\":46692,\"start\":46688},{\"end\":46718,\"start\":46692},{\"end\":46730,\"start\":46718},{\"end\":47062,\"start\":47058},{\"end\":47080,\"start\":47062},{\"end\":47093,\"start\":47080},{\"end\":47109,\"start\":47093},{\"end\":47121,\"start\":47109},{\"end\":47551,\"start\":47533},{\"end\":47568,\"start\":47551},{\"end\":47579,\"start\":47568},{\"end\":47592,\"start\":47579},{\"end\":47890,\"start\":47880},{\"end\":47898,\"start\":47890},{\"end\":47910,\"start\":47898},{\"end\":48097,\"start\":48089},{\"end\":48104,\"start\":48097},{\"end\":48110,\"start\":48104},{\"end\":48119,\"start\":48110},{\"end\":48126,\"start\":48119},{\"end\":48134,\"start\":48126},{\"end\":48142,\"start\":48134},{\"end\":48159,\"start\":48142},{\"end\":48168,\"start\":48159},{\"end\":48179,\"start\":48168},{\"end\":48603,\"start\":48595},{\"end\":48620,\"start\":48603},{\"end\":48628,\"start\":48620},{\"end\":48636,\"start\":48628},{\"end\":48643,\"start\":48636},{\"end\":48653,\"start\":48643},{\"end\":48662,\"start\":48653},{\"end\":48669,\"start\":48662},{\"end\":48677,\"start\":48669},{\"end\":48685,\"start\":48677},{\"end\":48694,\"start\":48685},{\"end\":48703,\"start\":48694},{\"end\":48712,\"start\":48703},{\"end\":48718,\"start\":48712},{\"end\":48727,\"start\":48718},{\"end\":49239,\"start\":49231},{\"end\":49253,\"start\":49239},{\"end\":49528,\"start\":49520},{\"end\":49536,\"start\":49528},{\"end\":49549,\"start\":49536},{\"end\":49556,\"start\":49549},{\"end\":49563,\"start\":49556},{\"end\":49575,\"start\":49563},{\"end\":49581,\"start\":49575},{\"end\":49590,\"start\":49581},{\"end\":50063,\"start\":50054},{\"end\":50074,\"start\":50063},{\"end\":50451,\"start\":50443},{\"end\":50461,\"start\":50451},{\"end\":50472,\"start\":50461},{\"end\":50631,\"start\":50620},{\"end\":50641,\"start\":50631},{\"end\":50653,\"start\":50641},{\"end\":50963,\"start\":50952},{\"end\":50973,\"start\":50963},{\"end\":50983,\"start\":50973},{\"end\":50994,\"start\":50983},{\"end\":51390,\"start\":51386},{\"end\":51403,\"start\":51390},{\"end\":51409,\"start\":51403},{\"end\":51418,\"start\":51409},{\"end\":51762,\"start\":51752},{\"end\":51772,\"start\":51762},{\"end\":51783,\"start\":51772},{\"end\":52068,\"start\":52056},{\"end\":52359,\"start\":52353},{\"end\":52367,\"start\":52359},{\"end\":52379,\"start\":52367},{\"end\":52392,\"start\":52379},{\"end\":52403,\"start\":52392},{\"end\":52610,\"start\":52604},{\"end\":52619,\"start\":52610},{\"end\":52628,\"start\":52619},{\"end\":52810,\"start\":52792},{\"end\":52821,\"start\":52810},{\"end\":53209,\"start\":53203},{\"end\":53217,\"start\":53209},{\"end\":53226,\"start\":53217},{\"end\":53234,\"start\":53226},{\"end\":53242,\"start\":53234},{\"end\":53248,\"start\":53242},{\"end\":53476,\"start\":53464},{\"end\":53488,\"start\":53476},{\"end\":53505,\"start\":53488},{\"end\":53514,\"start\":53505},{\"end\":53526,\"start\":53514},{\"end\":53538,\"start\":53526},{\"end\":53922,\"start\":53911},{\"end\":53935,\"start\":53922},{\"end\":53944,\"start\":53935},{\"end\":53958,\"start\":53944},{\"end\":53965,\"start\":53958},{\"end\":53975,\"start\":53965},{\"end\":54426,\"start\":54418},{\"end\":54433,\"start\":54426},{\"end\":54442,\"start\":54433},{\"end\":54453,\"start\":54442},{\"end\":54467,\"start\":54453},{\"end\":54478,\"start\":54467},{\"end\":54916,\"start\":54908},{\"end\":54927,\"start\":54916},{\"end\":54936,\"start\":54927},{\"end\":55393,\"start\":55382},{\"end\":55402,\"start\":55393},{\"end\":55415,\"start\":55402},{\"end\":55428,\"start\":55415},{\"end\":55441,\"start\":55428},{\"end\":55450,\"start\":55441},{\"end\":55460,\"start\":55450},{\"end\":55473,\"start\":55460},{\"end\":55486,\"start\":55473},{\"end\":55499,\"start\":55486},{\"end\":55805,\"start\":55795},{\"end\":55813,\"start\":55805},{\"end\":55820,\"start\":55813},{\"end\":55829,\"start\":55820},{\"end\":56498,\"start\":56483},{\"end\":56514,\"start\":56498},{\"end\":56523,\"start\":56514},{\"end\":56529,\"start\":56523},{\"end\":56963,\"start\":56947},{\"end\":56976,\"start\":56963},{\"end\":56985,\"start\":56976},{\"end\":56995,\"start\":56985},{\"end\":57438,\"start\":57427},{\"end\":57454,\"start\":57438},{\"end\":57464,\"start\":57454},{\"end\":57476,\"start\":57464},{\"end\":57921,\"start\":57906},{\"end\":57932,\"start\":57921},{\"end\":57944,\"start\":57932},{\"end\":58253,\"start\":58242},{\"end\":58263,\"start\":58253},{\"end\":58275,\"start\":58263},{\"end\":58285,\"start\":58275},{\"end\":58581,\"start\":58567},{\"end\":58591,\"start\":58581},{\"end\":58824,\"start\":58813},{\"end\":58834,\"start\":58824},{\"end\":58845,\"start\":58834},{\"end\":59216,\"start\":59210},{\"end\":59223,\"start\":59216},{\"end\":59229,\"start\":59223},{\"end\":59468,\"start\":59459},{\"end\":59479,\"start\":59468},{\"end\":59797,\"start\":59783},{\"end\":59808,\"start\":59797},{\"end\":60252,\"start\":60245},{\"end\":60260,\"start\":60252},{\"end\":60274,\"start\":60260},{\"end\":60281,\"start\":60274},{\"end\":60290,\"start\":60281},{\"end\":60301,\"start\":60290},{\"end\":60658,\"start\":60649},{\"end\":60668,\"start\":60658},{\"end\":60679,\"start\":60668},{\"end\":60688,\"start\":60679},{\"end\":60960,\"start\":60953},{\"end\":60967,\"start\":60960},{\"end\":61207,\"start\":61197},{\"end\":61219,\"start\":61207},{\"end\":61484,\"start\":61477},{\"end\":61501,\"start\":61484},{\"end\":61515,\"start\":61501},{\"end\":61828,\"start\":61822},{\"end\":61835,\"start\":61828},{\"end\":61844,\"start\":61835},{\"end\":61854,\"start\":61844},{\"end\":61862,\"start\":61854},{\"end\":61869,\"start\":61862},{\"end\":62220,\"start\":62211},{\"end\":62227,\"start\":62220},{\"end\":62233,\"start\":62227},{\"end\":62239,\"start\":62233},{\"end\":62245,\"start\":62239},{\"end\":62667,\"start\":62657},{\"end\":62674,\"start\":62667},{\"end\":62681,\"start\":62674},{\"end\":62688,\"start\":62681},{\"end\":62693,\"start\":62688},{\"end\":63077,\"start\":63067},{\"end\":63084,\"start\":63077},{\"end\":63092,\"start\":63084},{\"end\":63362,\"start\":63344},{\"end\":63520,\"start\":63510},{\"end\":63532,\"start\":63520},{\"end\":63545,\"start\":63532},{\"end\":63557,\"start\":63545},{\"end\":63563,\"start\":63557},{\"end\":63573,\"start\":63563},{\"end\":63582,\"start\":63573},{\"end\":63591,\"start\":63582},{\"end\":63804,\"start\":63796},{\"end\":63942,\"start\":63929},{\"end\":63957,\"start\":63942},{\"end\":63968,\"start\":63957},{\"end\":63979,\"start\":63968},{\"end\":64279,\"start\":64269},{\"end\":64295,\"start\":64279},{\"end\":64531,\"start\":64523},{\"end\":64911,\"start\":64902},{\"end\":64920,\"start\":64911},{\"end\":65349,\"start\":65335},{\"end\":65360,\"start\":65349},{\"end\":65673,\"start\":65666},{\"end\":65681,\"start\":65673},{\"end\":65690,\"start\":65681},{\"end\":65895,\"start\":65874},{\"end\":66015,\"start\":66003},{\"end\":66028,\"start\":66015},{\"end\":66328,\"start\":66319},{\"end\":66499,\"start\":66475},{\"end\":66659,\"start\":66639},{\"end\":66663,\"start\":66659},{\"end\":66680,\"start\":66663},{\"end\":66881,\"start\":66871},{\"end\":66896,\"start\":66881},{\"end\":66912,\"start\":66896},{\"end\":66925,\"start\":66912},{\"end\":66940,\"start\":66925},{\"end\":66969,\"start\":66940},{\"end\":66982,\"start\":66969},{\"end\":66993,\"start\":66982},{\"end\":67006,\"start\":66993},{\"end\":67019,\"start\":67006},{\"end\":67037,\"start\":67019},{\"end\":67055,\"start\":67037},{\"end\":67317,\"start\":67305},{\"end\":67548,\"start\":67536},{\"end\":67813,\"start\":67805},{\"end\":67833,\"start\":67813},{\"end\":67844,\"start\":67833},{\"end\":67854,\"start\":67844},{\"end\":67862,\"start\":67854},{\"end\":67876,\"start\":67862},{\"end\":68267,\"start\":68260},{\"end\":68279,\"start\":68267},{\"end\":68597,\"start\":68587},{\"end\":68610,\"start\":68597},{\"end\":68621,\"start\":68610},{\"end\":68908,\"start\":68902},{\"end\":68917,\"start\":68908},{\"end\":68924,\"start\":68917},{\"end\":68931,\"start\":68924},{\"end\":69204,\"start\":69190},{\"end\":69219,\"start\":69204},{\"end\":69233,\"start\":69219},{\"end\":69251,\"start\":69233},{\"end\":69269,\"start\":69251},{\"end\":69728,\"start\":69714},{\"end\":69743,\"start\":69728},{\"end\":69757,\"start\":69743},{\"end\":69773,\"start\":69757},{\"end\":70233,\"start\":70223},{\"end\":70244,\"start\":70233},{\"end\":70256,\"start\":70244},{\"end\":71706,\"start\":71696}]", "bib_venue": "[{\"end\":45117,\"start\":45096},{\"end\":45240,\"start\":45215},{\"end\":45402,\"start\":45346},{\"end\":45740,\"start\":45703},{\"end\":46084,\"start\":46060},{\"end\":46403,\"start\":46366},{\"end\":46771,\"start\":46730},{\"end\":47184,\"start\":47121},{\"end\":47629,\"start\":47592},{\"end\":47878,\"start\":47840},{\"end\":48240,\"start\":48179},{\"end\":48593,\"start\":48513},{\"end\":49012,\"start\":48978},{\"end\":49270,\"start\":49253},{\"end\":49669,\"start\":49590},{\"end\":49904,\"start\":49900},{\"end\":50111,\"start\":50074},{\"end\":50441,\"start\":50391},{\"end\":50690,\"start\":50653},{\"end\":51058,\"start\":50994},{\"end\":51475,\"start\":51418},{\"end\":51820,\"start\":51783},{\"end\":52105,\"start\":52068},{\"end\":52351,\"start\":52272},{\"end\":52602,\"start\":52563},{\"end\":52907,\"start\":52821},{\"end\":53201,\"start\":53137},{\"end\":53575,\"start\":53538},{\"end\":54001,\"start\":53975},{\"end\":54552,\"start\":54478},{\"end\":55020,\"start\":54936},{\"end\":55380,\"start\":55319},{\"end\":55793,\"start\":55712},{\"end\":56056,\"start\":55969},{\"end\":56593,\"start\":56529},{\"end\":57057,\"start\":56995},{\"end\":57514,\"start\":57476},{\"end\":57904,\"start\":57787},{\"end\":58240,\"start\":58149},{\"end\":58565,\"start\":58462},{\"end\":58908,\"start\":58845},{\"end\":59266,\"start\":59229},{\"end\":59540,\"start\":59479},{\"end\":59867,\"start\":59808},{\"end\":60325,\"start\":60301},{\"end\":60731,\"start\":60688},{\"end\":61011,\"start\":60967},{\"end\":61256,\"start\":61219},{\"end\":61556,\"start\":61515},{\"end\":61906,\"start\":61869},{\"end\":62307,\"start\":62245},{\"end\":62729,\"start\":62693},{\"end\":63123,\"start\":63092},{\"end\":63342,\"start\":63322},{\"end\":63508,\"start\":63463},{\"end\":64010,\"start\":63979},{\"end\":64349,\"start\":64295},{\"end\":64549,\"start\":64531},{\"end\":64714,\"start\":64655},{\"end\":64960,\"start\":64920},{\"end\":65397,\"start\":65360},{\"end\":65664,\"start\":65595},{\"end\":66131,\"start\":66028},{\"end\":66317,\"start\":66292},{\"end\":66473,\"start\":66449},{\"end\":66637,\"start\":66558},{\"end\":66869,\"start\":66830},{\"end\":67358,\"start\":67317},{\"end\":67584,\"start\":67548},{\"end\":67803,\"start\":67731},{\"end\":68105,\"start\":68043},{\"end\":68340,\"start\":68279},{\"end\":68658,\"start\":68621},{\"end\":68975,\"start\":68931},{\"end\":69346,\"start\":69269},{\"end\":69850,\"start\":69773},{\"end\":70293,\"start\":70256},{\"end\":70509,\"start\":70492},{\"end\":71728,\"start\":71706},{\"end\":47239,\"start\":47186},{\"end\":55091,\"start\":55022},{\"end\":58963,\"start\":58910},{\"end\":59880,\"start\":59869},{\"end\":62761,\"start\":62731},{\"end\":63150,\"start\":63125},{\"end\":69410,\"start\":69348},{\"end\":69914,\"start\":69852}]"}}}, "year": 2023, "month": 12, "day": 17}