{"id": 236460230, "updated": "2023-04-04 23:01:34.129", "metadata": {"title": "SemEval-2021 Task 5: Toxic Spans Detection", "authors": "[{\"first\":\"John\",\"last\":\"Pavlopoulos\",\"middle\":[]},{\"first\":\"Jeffrey\",\"last\":\"Sorensen\",\"middle\":[]},{\"first\":\"L\u00e9o\",\"last\":\"Laugier\",\"middle\":[]},{\"first\":\"Ion\",\"last\":\"Androutsopoulos\",\"middle\":[]}]", "venue": "SEMEVAL", "journal": "Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "The Toxic Spans Detection task of SemEval-2021 required participants to predict the spans of toxic posts that were responsible for the toxic label of the posts. The task could be addressed as supervised sequence labeling, using training data with gold toxic spans provided by the organisers. It could also be treated as rationale extraction, using classifiers trained on potentially larger external datasets of posts manually annotated as toxic or not, without toxic span annotations. For the supervised sequence labeling approach and evaluation purposes, posts previously labeled as toxic were crowd-annotated for toxic spans. Participants submitted their predicted spans for a held-out test set and were scored using character-based F1. This overview summarises the work of the 36 teams that provided system descriptions.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": "2021.semeval-1.6", "pubmed": null, "pubmedcentral": null, "dblp": "conf/semeval/PavlopoulosSLA21", "doi": "10.18653/v1/2021.semeval-1.6"}}, "content": {"source": {"pdf_hash": "4b6060911919483cc24d44cdce2618788ec477df", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclanthology.org/2021.semeval-1.6.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://aclanthology.org/2021.semeval-1.6.pdf", "status": "HYBRID"}}, "grobid": {"id": "78c84a933b35f976a6e251bfeaa3a69706817f15", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/4b6060911919483cc24d44cdce2618788ec477df.txt", "contents": "\nSemEval-2021 Task 5: Toxic Spans Detection\nAugust 5-6, 2021\n\nJohn Pavlopoulos \nDepartment of Informatics\nAthens University of Economic and Business\nGreece\n\nT\u00e9l\u00e9com Paris\nInstitut Polytechnique de ParisFrance\n\nJeffrey Sorensen \nL\u00e9o Laugier leo.laugier@telecom-paris.fr \nIon Androutsopoulos \nDepartment of Informatics\nAthens University of Economic and Business\nGreece\n\nT\u00e9l\u00e9com Paris\nInstitut Polytechnique de ParisFrance\n\n\u2021 Google Jigsaw \n\nDepartment of Computer and System Sciences\nStockholm University\nSweden\n\nSemEval-2021 Task 5: Toxic Spans Detection\n\nProceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)\nthe 15th International Workshop on Semantic Evaluation (SemEval-2021)Bangkok, ThailandAugust 5-6, 202159\nThe Toxic Spans Detection task of SemEval-2021 required participants to predict the spans of toxic posts that were responsible for the toxic label of the posts. The task could be addressed as supervised sequence labeling, using training data with gold toxic spans provided by the organisers. It could also be treated as rationale extraction, using classifiers trained on potentially larger external datasets of posts manually annotated as toxic or not, without toxic span annotations. For the supervised sequence labeling approach and evaluation purposes, posts previously labeled as toxic were crowd-annotated for toxic spans. Participants submitted their predicted spans for a held-out test set, and were scored using character-based F1. This overview summarises the work of the 36 teams that provided system descriptions.\n\nIntroduction\n\nDiscussions online often host toxic posts, meaning posts that are rude, disrespectful, or unreasonable; and which can make users want to leave the conversation (Borkan et al., 2019a). Current toxicity detection systems classify whole posts as toxic or not (Schmidt and Wiegand, 2017;Pavlopoulos et al., 2017;Zampieri et al., 2019), often to assist human moderators, who may be required to review only posts classified as toxic, when reviewing all posts is infeasible. In such cases, human moderators could be assisted even more by automatically highlighting spans of the posts that made the system classify the posts as toxic. This would allow the moderators to more quickly identify objectionable parts of the posts, especially in long posts, and more easily approve or reject the decisions of the toxicity detection systems. As a first step along this direction, Task 5 of SemEval 2021 provided the participants with posts previously rated to be toxic, and required them to identify toxic spans, i.e., spans that were responsible for the toxicity of the posts, when identifying such spans was possible. Note that a post may include no toxic span and still be marked as toxic. On the other hand, a non toxic post may comprise spans that are considered toxic in other toxic posts. We provided a dataset of English posts with gold annotations of toxic spans, and evaluated participating systems on a held-out test subset using character-based F1. The task could be addressed as supervised sequence labeling, training on the provided posts with gold toxic spans. It could also be treated as rationale extraction (Li et al., 2016;Ribeiro et al., 2016), using classifiers trained on larger external datasets of posts manually annotated as toxic or not, without toxic span annotations. There were almost 500 individual participants, and 36 out of the 92 teams that were formed submitted reports and results that we survey here. Most teams adopted the supervised sequence labeling approach. Hence, there is still scope for further work on the rationale extraction approach. We also discuss other possible improvements in the definition and data of the task.\n\n\nCompetition Dataset Creation\n\nDuring 2015, when many publications were closing down comment sections due to moderation burdens, a start up named Civil Comments launched (Finley, 2016). Using a system of peer-based review and flagging, they hoped to crowd source the moderation responsibility. When this effort shut down in 2017 (Bogdanoff, 2017), they cited the financial constraints of the competitive publishing industry and the challenges of attaining the necessary scale.\n\nThe founders of Civil Comments, in collaboration with researchers from Google Jigsaw, undertook an effort to open source the collection of more than two million comments that had been collected. After filtering the comments to remove personally identifiable information, a revised version of the annotation system of Wulczyn et al. (2017) was used on the Appen crowd rating platform to label the comments using a number of attributes including 'toxicity', 'obscene', 'threat ' Borkan et al. (2019a). The complete dataset, partitioned into training, development, and test sets, was featured in a Kaggle competition, 1 with additional material, including individual rater decisions, published (Borkan et al., 2019b) after the close of the competition.\n\nCivil Comments contains about 30k comments marked as toxic by a majority of at least three crowd raters. Toxic comments are rare, especially in fora that are not anonymous and where people have expectations that moderators will be watching and taking action. We undertook an effort to reannotate this subset of comments at the span level, using the following instructions:\n\nFor this task you will be viewing comments that a majority of annotators have already judged as toxic. We would like to know what parts of the comments are responsible for this.\n\nExtract the toxic word sequences (spans) of the comment below, by highlighting each such span and then clicking the right button. If the comment is not toxic or if the whole comment should have been annotated, check the appropriate box and do not highlight any span. and a custom JavaScript based template, 2 which allowed selection and tagging of comment spans 1 www.kaggle.com/c/jigsaw-unintendedbias-in-toxicity-classification 2 github.com/ipavlopoulos/toxic_spans ( Fig. 1). While raters were asked to categorize each span as one of five different categories, this was primarily intended as a priming exercise and all of the highlighted spans were collapsed into a single category. The lengths of the highlighted spans were decided by the raters. Seven raters were employed per post, but there were posts where fewer were eventually assigned. On the test subset (Table 1), we verified that the number of raters per post varied from three to seven; on the trial and train subsets this number varied from two to seven. All raters were warned the content might by explicit, and only raters who allowed adult content were selected. 3\n\n\nInter-annotator Agreement\n\nWe measured inter-annotator agreement, initially, on a small set of 35 posts and we found 0.61 average Cohen's Kappa. That is, we computed the mean pairwise Kappa per post, by using character offsets as instances being classified in two classes, toxic and non-toxic. And then we averaged Kappa over the 35 posts. On later experiments with larger samples (up to 1,000 posts) we observed equally moderate agreement and always higher than 0.55. Given the highly subjective nature of the task we consider this agreement to be reasonably high.\n\n\nExtracting the ground truth\n\nEach post comprises sets of annotated spans, one per rater. Each span is assigned a binary (toxic, nontoxic) label, based on whether the respective rater found the span to be insulting, threatening, identitybased attack, profane/obscene, or otherwise toxic. If the span was annotated with any of those types, the span is considered toxic according to the rater, otherwise not. For each post, we extracted the character offsets of each toxic span of each rater.\n\nIn each post, the ground truth considers a character offset as toxic if the majority of the raters included it in their toxic spans, otherwise the ground truth of the character offset is non-toxic. A toxic span (Table 1) in the ground truth of a post is a maximal sequence of contiguous toxic character-offsets.\n\n\nExploratory analysis\n\nAfter discarding duplicates and posts used as quiz questions to check the reliability of candidate annotators, we split the data into trial, train, and test (Table 1). Compared to the trial and training sets, the test set comprises posts with fewer characters and spans, but also shorter spans on average. When studying the toxicity subtypes, we find that the vast majority of posts are annotated as insulting. In the training set, more than 6,000 posts are annotated as insulting, and the same high fraction is observed in the trial and test sets. Most of the toxic spans in the training set are single-word terms. The most frequent of them, such as 'stupid' and 'idiot', occur hundreds of times and remain frequent in the trial and test sets. Multi-word terms, such as 'white trash', 'mentally ill', are less frequent and vary across the three sets.\n\nIn an analysis of the test set, Palomino et al. (2021) used an emotion classifier that returns five scores per post, one for each of the following emotions: anger, happiness, sadness, surprise, fear. 4 Fear and sadness were reported to be the emotions with the highest average scores, a finding that we verified by repeating the experiment (see Fig. 2). 5 Interestingly, the emotion with the highest average score after sadness and fear is surprise, not anger, and happiness has the lowest score. 4 pypi.org/project/text2emotion 5 A post with a high sadness score (100%) is the following: \"Such thin skin. Pathetic.\"; the toxic span shown in red. \n\n\nTask description\n\nThe objective of this task is the detection of the spans that make a post toxic, when detecting such spans is possible. Systems had to extract a list of toxic spans, or an empty list, per post. A toxic span was defined to be a sequence of words that attribute to the post's toxicity. Although we defined the task at the word level, gold labels were provided at the character level counting from zero (see Table 2). The evaluation period started on January 10, 2021 and finished on January 31, 2021. The first week, 10 submissions were allowed per day per team. The second week this number was reduced to 5, and it was reduced further to 1 during the final week. We chose to allow an extended evaluation period combined with multiple team submissions, to promote the competition. However, we also chose a decreasing submission limit to make it harder for participants to overfit the test set. As shown in Fig. 3, the number of submissions dropped over time, due to this constraint, but the interest was OFFSETS POST {} I will stand by my statement that liberalism is what smart looks like to stupid people. The letter posted yesterday states the real problem clearly enough. \"It doesn't matter if it was protected by the First Amendment.\" I have legal news flash for the University of Oregon. Because the University is public and receives government tax dollars it is indeed bound to abide by the US constitution with All OF IT'S PROTECTION INCLUDED. Which should tell everyone right off the signatories are anti-American, and perhaps even anti-America in their twisted PC nonsense.\n\n\n{}\n\nFits the despicable piece of trash like a glove.\n\n\n{}\n\nNo he shot someone so he shouldn't be free. He should be locked up like He is for the next decade. You holla like its no big deal but what if it was your little brother who was shot? Then your dumb ass wouldnrlt be screaming free ace g or whatever gay nickname he has. That is the lamest gangster name ever. Do you think that name scares people? Ace clown is what i will now call him. what a dummy, dummy has no regard for the law. you cannot brutalize a suspect. he has complete lack of respect for any law and is acting like a dictator. he is trying to emulate putin. {12, . . . , 17, 94, . . . , 102}\n\nPeople make stupid decisions and then expect the gov't to bail them out. There is no cure for stupidity.\n\n{14, . . . , 20, 29, . . . , 35} Nah, the only asshole is the asshole firing a rifle within city limits. continuous, and there were submissions until the last day. Despite the decreasing total number of submissions per day, the top daily score increased, reaching its maximum on the last day (see Fig. 4). \n\n\nParticipation overview\n\nWe received 479 individual participation requests, 92 team formations, and 1,449 submissions. 91 teams submitted valid predictions (1,385 valid submissions in total) and were scored; out of these, only 36 submitted system descriptions.\n\n\nThe HITSZ-HLT submission\n\nThe best performing team (HITSZ-HLT) formulated the problem as a combination of token label-ing and span extraction (Zhu et al., 2021).\n\nFor their token labeling approach, the team used two systems based on BERT (Devlin et al., 2019). Both systems had a Conditional Random Field (CRF) layer (Sutton and McCallum, 2006) on top, but one of the two also had an LSTM layer (Hochreiter and Schmidhuber, 1997) between BERT and the CRF layer. In both approaches, word-level BIO tags were used, i.e., words were labelled as B (beginning word of a toxic span), I (inside word of a toxic span), or O (outside of any toxic span).\n\nFor their span extraction approach, the team also used BERT. Roughly speaking, in this case BERT produces probabilities indicating how likely it is for each token to be the beginning or end of a toxic span. Then a heuristic search algorithm, originally developed for target extraction in sentiment analysis by Hu et al. (2019), selects the best combinations of candidate begin and end tokens, aiming to output the most likely set of toxic spans per post.\n\nThe character predictions of the three systems described above were combined with majority voting per character. That is, if any two systems considered a character to be part of a toxic span, then the ensemble classified the character as toxic, otherwise the ensemble classified it as non-toxic.\n\n\nThe S-NLP submission\n\nThe team with the second best performing system (S-NLP) consists of individual participants who grouped and submitted an ensemble of their sys-tems . The ensemble combines two approaches, both of which are based on a RoBERTa model (Liu et al., 2019). The latter is first fine-tuned to classify posts as toxic or nontoxic, using three Kaggle toxicity datasets. 6 For toxic span detection, RoBERTa's subword representations from three different layers (1, 6, 12) are summed to produce the corresponding word embeddings. A binary classifier on top of RoBERTa, operating on the word embeddings, predicts whether a word belongs to a toxic span or not.\n\nFor the first component of the ensemble, the word embeddings obtained from RoBERTa's subword representations are concatenated with FLAIR (Akbik et al., 2019) and FastText (Bojanowski et al., 2017) embeddings. 7 The resulting embeddings are passed on to a two-layer stacked BiLSTM with a CRF layer on top to generate a BIO tag per word.\n\nThe second component of the ensemble used the RoBERTa model as a teacher to produce silver toxic spans for 30,000 unlabelled toxic posts (Borkan et al., 2019a). RoBERTa was then retrained as a student on the augmented dataset (30k posts with silver labels and the training posts provided by the organisers) to predict toxic offsets.\n\nThe ensemble returns the intersection of the toxic spans identified by the two components.\n\n\nAdditional interesting approaches\n\nWe now discuss some of the most interesting alternative approaches tried by the participants, even if they did not lead to high scores. Rationales Some participants experimented with training toxicity classifiers on external datasets containing posts labeled as toxic or non-toxic; and then employing model-specific or model-agnostic rationale extraction mechanisms to produce toxic spans as explanations of the decisions of the classifier. The model-specific rationale mechanism of Rusert (2021) used the attention scores of an LSTM toxicity classifier to detect the toxic spans. Pluci\u0144ski and Klimczak (2021) used the same approach, but also employed an orthogonalisation technique (Mohankumar et al., 2020). The model-agnostic rationale mechanism of Rusert (2021) combined an LSTM classifier with a token-masking approach that we call Input Erasure (IE), due to its similarities to the method of Li et al. (2016). The model-agnostic approach of Pluci\u0144ski and Klimczak (2021) combined SHAP (Lundberg and Lee, 2017) with a fine-tuned BERT model. Ding and Jurgens (2021) and Benlahbib et al. (2021) also experimented with model-agnostic approaches, but they combined LIME (Ribeiro et al., 2016) with a Logistic Regression (LR) or with a linear Support Vector Machine (SVM) toxicity classifier. All the above mentioned approaches used a threshold to turn the explanation scores (e.g., attention or LIME scores) of the words into binary decisions (toxic/non-toxic words). Lexicon-based No team relied on a purely lexiconbased approach, but few experimented with lexiconbased baselines (Zhu et al., 2021;Palomino et al., 2021) or used such components in ensembles (Ranasinghe et al., 2021). Three kinds of lexiconbased methods were used. First, the lexicon was handcrafted by domain experts (Smedt et al., 2020) and it was simply employed as a list of toxic words for lookup operations (Palomino et al., 2021). Second, the lexicon was compiled using the set of tokens labeled as toxic in our span-annotated training set and it was used as a lookup table (Burtenshaw and Kestemont, 2021), possibly also storing the frequency of each lexicon token in the training set (Zhu et al., 2021). The former two were also combined (Ranasinghe et al., 2021). Third, the least supervised lexicons were built with statistical analysis on the occurrences of tokens in a training set solely annotated at the comment level (toxic/nontoxic post) (Rusert, 2021). An added value of these approaches is that easy to use resources (toxicity lexicons) are built and shared publicly, such as the one suggested by Pluci\u0144ski and Klimczak (2021). 8 Custom losses Zhen Wang and Liu (2021) experimented with a new custom loss, which weighted false toxicity predictions based on their location in the text. If a false prediction was located near a ground truth toxic span, then it would contribute less to the overall loss for that post, compared to one located further away. The loss function used by Kuyumcu et al. (2021) to train their system is the Tversky Similarity Index (Tversky, 1977), a generalisation of the S\u00f8rensen-Dice coefficient and the Jaccard index, which was adjusted by the authors to weigh up false negatives. Data augmentation The vast majority of the participating teams employed additional training data annotated at the post level. That is, either to build lexicons (Rusert, 2021), to leverage unsupervised rationale extraction methods (Rusert, 2021;Pluci\u0144ski and Klimczak, 2021;Ding and Jurgens, 2021;Benlahbib et al., 2021), or to filter posts (Luu and Nguyen, 2021) that were not labeled as toxic by a toxicity classifier. Suman and Jain (2021) astutely produced silver data from external sources to augment the initial golden annotated dataset, training their model iteratively in a semi-supervised manner.\n\n\nEvaluation\n\nThis section focuses on the evaluation framework of the task. First, the official measure that was used to evaluate the participating systems is described. Then, we discuss baseline models that were selected as benchmarks for comparison reasons. Finally, the results are presented.\n\n\nOfficial evaluation measure\n\nFollowing the work of Martino et al. (2019), systems were evaluated in terms of F1 computed on character offsets. For each system, we computed the F1 score per post, between the predicted and the ground truth character offsets. Then, we returned the macro-averaged (over test posts) score. When the ground truth set of character offsets was empty, we assigned a perfect score (F 1 = 1) to the post in question if the predicted set of character offsets was also empty, and a zero score otherwise. 9\n\n\nBenchmarks\n\nWe report the results of some baselines, developed by us or the participants, to act as benchmarks.\n\nBENCHMARK I was developed by . It is based on a RoBERTa model, fine-tuned to predict if a post is toxic or not (Section 4.2) and further fine-tuned to predict toxic spans by using a CRF layer on top.\n\nBENCHMARK II is a lexicon-based system, developed by Zhu et al. (2021), which extracts likely toxic words from the training data and simply tags them during inference. The lexicon comprises words that appear frequently inside ground truth toxic spans and not outside.\n\nBENCHMARK III is a random baseline, which assigns a random label (toxic/non-toxic) per character offset (50% chance of being toxic). 10 9 The evaluation code can be found in our GitHub repository (github.com/ipavlopoulos/toxic_spans). 10 The code of this baseline is also in the task's repository.  Table 3: Official rank and F1 score (%) of the 36 participating teams that submitted system description papers. (There were 91 teams with sumbissions in total.) The median is shown in blue and benchmarks in red. Table 3 shows the scores and ranks of all participating teams that described their approach, i.e., 36 out of 91 teams that participated. HITSZ-HLT (Section 4.1) was ranked first, followed by S-NLP (Section 4.2) that scored 0.06% lower. The rest of the teams followed with scores lower than 70%.\n\nThe score of the median is 67.58%, which is not far below the top scored team (-3.22 percent units), while it is far above the last two (+17.52 percent units). The standard deviation of system scores above the median is much lower (0.94) than that of the systems below the median (4.12). Most teams that were excluded from the table (because they did not describe their methods) score lower than the median. However, there were also top scoring teams among those that were excluded, such as a team with a RoBERTa-based token-level ensemble that was ranked 4th. 11 BENCHMARK I achieves a considerably high score and, hence, is very highly ranked. Combining BERT with a CRF or a span extraction method (two of the individual methods of the HITSZ-HLT ensemble, Section 4.1, not shown in Table 3) also performs well (Zhu et al., 2021), but these methods would be ranked two positions lower than BENCH-MARK I.  explored the benefits of further enhancing these word embeddings by concatenating them with FLAIR (Akbik et al., 2019) and FastText (Bojanowski et al., 2017) embeddings (Section 4.2). As shown in Fig. 5, the F1 score is slightly improved, reaching a maximum when both FLAIR and FastText embeddings are added. 12 We note that the same beneficial effect of enhancing the word embeddings was reported when using BERT as the base model (Sans and Farr\u00e0s, 2021). The lexicon-based BENCHMARK II and the random BENCHMARK III scored very low. The latter outperformed only one submission (MACECH), which sent the predictions in the wrong order. As noted in their report (Cech, 2021), if the predictions had been submitted in the correct order, the team's score would have been 54%, and BENCHMARK III would have been the worst system in Table 3.\n\n\nAnalysis and discussion\n\nOverall the organisers were happy to see the degree of involvement in this shared task, and the resulting diversity of approaches to this problem. We include some of our observations regarding the administration of the evaluation and what we have learned from the results.\n\n\nParticipation\n\nThe authors reached out to teams that decided not to submit a description paper and the vast majority were students who were time-limited. The fact that students participated in the task is promising and we plan to consider more ways to introduce SemEval tasks in classrooms. On the other hand, 60% of the participants chose not to describe their approach, which is problematic and should be addressed. A team could take advantage of such an option to create duplicate submissions and bypass any submission limits. More importantly, potentially interesting approaches are not discussed and properly compared to others.\n\nIt is also worth mentioning that the extended timeline allowed participants to join forces. For instance, a number of participants decided to combine their systems and form the 2nd ranked S-NLP. Their ensemble scored higher than all their standalone systems, though their best standalone system would still be ranked 2nd. In any case, we welcome the collaboration between participants, which may provide further insights regarding effective combinations of architectures.\n\n\nGeneral remarks on the approaches\n\nExcept for lexicon-based baselines, we observed that the vast majority of systems adopted the recent paradigm in NLP: fine-tuning large off-the-shelf Transformers (Vaswani et al., 2017) pre-trained on massive corpora. Non-Transformer based approaches, mostly LSTMs with pre-trained word embeddings were also used. The nature of the task, similar to the well-studied Named Entity Recognition (NER) task, led many competitors to use a CRF layer on top of the model (e.g., Transformers or LSTMs) of their choice.\n\n\nPerformance\n\nThe winning team (HITSZ-HLT) combined BERT with two approaches for their ensemble: a token labeling approach (two versions, with/without an LSTM between BERT and the CRF) and a span ex-traction approach (Section 4.1). The comparison of the two showed that span extraction is slightly better on posts with a single span, but token labeling is clearly better on multi-span posts (Zhu et al., 2021). The complementary nature of the two approaches is probably what makes even a simple majority voting ensemble better than its competitors.\n\nThe system that was ranked second (S-NLP) also employed an ensemble, using a RoBERTa model initially fine-tuned to classify posts as toxic or nontoxic as the starting point . The ensemble combined (i) the resulting RoBERTa model, now fine-tuned to predict toxic spans, with additional FLAIR and FastText embeddings, and (ii) a RoBERTa model retrained as a student to predict toxic spans (Section 4.2). Although the two standalone models achieved higher scores than the standalone models of the top-ranked team (HITSZ-HLT), the ensemble did not yield significant improvements. This may be due to the student's decisions not being that complementary to the teacher's, as the team notes .  Teams that experimented with rationale extraction mechanisms (Section 4.3) did not find this approach advantageous compared to supervised sequence labeling in terms of F1 scores. However, the reported results of the rationale-based systems show that this approach is promising, especially because it does not require any data annotated at the span-level. Hence, there is scope for future work that could explore this direction further. Table 4 shows the F1 scores of all the rationale-based systems that were reported by participants. The binary toxic post classifiers that were used were LSTM, Logistic Regression (LR), Support Vector Machines (SVM), and BERT. The attention scores of an LSTM were used with (Pluci\u0144ski and Klimczak, 2021) and without an orthogonality method (Rusert, 2021), with the latter being slightly bet-ter; these are model-specific rational extraction methods (Section 4.3). Model-agnostic approaches (Input Erasure, LIME, SHAP) were better than the model-specific ones. The best rationale-based method employed a BERT model, fine-tuned for toxic post classification, and SHAP.\n\nLexicon Name F1 (%) Report WIEGAND 1 \u2020 33.07 Zhu et al. (2021) WORD-MATCH 40.86 Ranasinghe et al. (2021) FREQ-RATIO \u2020 41.55 Rusert (2021) LOOKUP \u2021 41.61 Burtenshaw and Kestemont (2021) WIEGAND 2 \u2020 50.98 Zhu et al. (2021) ORTHRUS 61.07 Palomino et al. (2021) HITSZ-HLT \u2021 64.98 Zhu et al. (2021) +WORDNET 64.09 Zhu et al. (2021) +GLOVE 64.19 Zhu et al. (2021)  Lexicon-based approaches were only used as baselines or components in ensembles, as already noted. In principle, all lexicon-based systems are extremely efficient and interpretable. Table 5 shows they can also achieve surprisingly high scores. Recall that we used the best performing lexicon-based system, developed by Zhu et al. (2021), as BENCHMARK II. Its score is included in Table 3. Despite the fact that it is low ranked, its F1 score is less than 6 percent points lower that that of the best submission. We also note that BENCHMARK II is a high-precision classifier; it outperforms even the best system in terms of precision (Zhu et al., 2021). Attempts to expand its lexicon using WordNet and GloVe, improved recall, but eventually harmed precision and its F1 score.\n\n\nError analysis\n\nA common theme across many competitor reports was the serious challenge posed by comments with no toxic spans. It is not readily evident why this is a common occurrence in the task, and certainly the way that annotation consensus is used to combine annotations can be a contributing factor. However, many systems seemed determined to tag some spans and many authors noted that performance on posts with no tagged span was extremely poor compared to performance on posts with tagged spans.\n\nMany systems were also reluctant to tag function words like 'of' and 'and', which can be included in multi-word spans (e.g., 'piece of crap'), leading to a decline in performance as measured by the chosen F1 measure. The overwhelming presence of single word gold spans in the training set favors short spans. But the majority of the short spans comprises common cuss or clearly abusive words, which can be directly classified as toxic (Ghosh and Kumar, 2021); by contrast, the infrequent longer spans are rather context dependent and more challenging to detect. This probably also contributed to the performance of the best system (HITSZ-HLT), since one of the two components of that ensemble handled better long spans, as already discussed in Section 6.3.\n\nOther error analysis highlighted challenges intrinsic to the task. The strong dependency of toxicity on context makes it particularly difficult to solve with systems based on vocabulary. Toxicity, when expressed with subtle language, can appear through non-local text features: some comments are toxic without showing any obvious toxic span in them. Such posts made the task more difficult for participants, because systems had learnt to label the words bearing the most negative sentiment (Bansal et al., 2021). Annotation mistakes were also reported (Table 6).\n\n\nType Description\n\n\nINCONSISTENCIES\n\nNot all the occurrences of the same toxic span are annotated in the same post.\n\n\nFALSE NEGATIVES\n\nToxic words missed.\n\n\nFALSE POSITIVES\n\nNon-toxic words labelled. Participants that were notable for their effort in error analysis include Bansal et al. (2021), Hoang and Nguyen (2021), Ding and Jurgens (2021), and Ghosh and Kumar (2021), where an additional effort was made to examine their model's ability to correctly tag words in toxic and non-toxic contexts. Interestingly Sans and Farr\u00e0s (2021) also noted in their analysis that racial and ethnic terms are labeled in biased ways that reflect patterns not only in the training toxic spans, but also in external data used to pre-train underlying Transformer models.\n\n\nConclusions\n\nWe provided 10,629 posts that were annotated for toxic spans and we defined the task of toxic span detection. The task was popular, attracting almost 500 individual participants. Eventually 91 teams were formed, out of which 36 submitted a description report. This overview described the approaches of these 36 teams and discussed their results.\n\nPre-trained Transformers, fine-tuned by viewing the task as a sequence labelling one, performed well and solutions that combined these models within an ensemble were highly-rated. The performance of these models increases further with the help of pre-trained word embeddings or by using multiple Transformer layers to embed words.\n\nLong toxic spans were more likely contextdependent and less frequent in the dataset compared to single-word spans, which made their detection a challenge. The winners included in their ensemble an approach that performed better on long spans, but we note that the problem of detecting long uncommon toxic spans is far from solved.\n\nOf particular interest were approaches that employed rationale extraction mechanisms, which do not require any training data annotated at the span level. They performed much worse than sequence labeling approaches, but this is a promising direction that was considered by only a few participants.\n\nFuture similar competitions could benefit from tracks that separate supervised from unsupervised solutions. The development of datasets created with the help of crowd annotators should focus on addressing ambiguity, bias, inconsistencies, and misannotations. This could be accomplished by adding more annotators per post. Future competitions could also require participants to both classify posts as toxic or not, and detect toxic spans only when posts are classified as toxic, instead of providing the participants only with posts already classified as toxic. Finally, future competitions could require participants to distinguish toxic posts of different kinds (e.g., insult, threat, profanity, along with supporting spans), which are sometimes easier to define compared to the more general umbrella toxicity term we (and others) have used.\n\nFigure 1 :\n1Screenshot of the Appen labeling interface that was used to annotate toxic spans.\n\nFigure 2 :\n2Emotion scores of the test posts. Emotion scores were obtained using an off-the-shelf emotion classifier, followingPalomino et al. (2021).\n\nFigure 3 :\n3Number of submissions per evaluation day.\n\n\n{55, 56, 57, 58} Time to give the opposition weapons and overthrow this jerk. {31, . . . , 39} you are a paid liberal hack of stupidity shore living the liberal lie every day {9, . . . , 13} Youre an idiot. {7, 8, . . . , 11, 14, 15, . . . , 18}\n\nFigure 4 :\n4The evaluation score (character F1) of the best submission per day during the evaluation period.\n\nFigure 5 :\n5F1 of BENCHMARK I(Zhu et al., 2021) when FLAIR and FastText word embeddings are concatenated with the embeddings obtained from RoBERTa's subword representations (from layers 1, 6, 12).\n\nTable 2 :\n2Examples of toxic test posts and their ground truth toxic spans (shown in red). The left column shows the character offsets of the toxic spans. The top three posts have no toxic spans, the next three have one each, while the remaining three posts have two toxic spans each.\n\nTable 4 :\n4F1 on the evaluation set for systems employing \nrationale extraction (RE) mechanisms combined with \npost-level toxicity binary classifiers (TBC). Rationales \nare obtained via Input Erasure (IE), Attention (ATT), \nLIME, or SHAP. The binary classifier is an LSTM, Lo-\ngistic Regression (LR), SVM, or BERT. \n\n\n\nTable 5 :\n5F1 on the evaluation set for lexicon-based systems. Systems that are followed by \u2020 and \u2021 use exclusively external and internal resources respectively.\n\nTable 6 :\n6The types and descriptions of the annotation mistakes that were detected by some of the participants.\nThe full dataset and annotations for ToxicSpans is released (github.com/ipavlopoulos/toxic_spans) with a CC0 licence. The previously released Civil Comments dataset, on which the new dataset is based, was filtered to remove any potential personally identifiable information.\ngithub.com/unitaryai/detoxify 7 In the latter case, in-vocabulary word embeddings were imported to Word2Vec for efficiency, and out of vocabulary words were handled with BPEs(Sennrich et al., 2016).\ngithub.com/Orthrus-Lexicon/Toxic\nWe asked for details from participants that did not submit a description paper, but not all of them replied.12  Out of vocabulary words were tackled by using FastText embeddings of BPEs; consult.\nAcknowledgementWe thank the participants and the reviewers for their useful comments and suggestions. This research was funded in part by a Google Research Award.\nFlair: An easy-to-use framework for state-of-the-art nlp. Alan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, Roland Vollgraf, NAACL Demonstrations. Alan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, and Roland Vollgraf. 2019. Flair: An easy-to-use framework for state-of-the-art nlp. In NAACL Demonstrations, pages 54-59.\n\nIITK@Detox at SemEval-2021 Task 5: Semisupervised learning and dice loss for toxic spans detection. Archit Bansal, Abhay Kaushik, Ashutosh Modi, SemEval. Archit Bansal, Abhay Kaushik, and Ashutosh Modi. 2021. IITK@Detox at SemEval-2021 Task 5: Semi- supervised learning and dice loss for toxic spans de- tection. In SemEval.\n\nLISAC FSDM USMBA at SemEval 2021 Task 5: Tackling toxic spans detection challenge with supervised spanBERT-based model and unsupervised LIME-based model. Abdessamad Benlahbib, Hamza Alami, Ahmed Alami, In SemEvalAbdessamad Benlahbib, Hamza Alami, and Ahmed Alami. 2021. LISAC FSDM USMBA at SemEval 2021 Task 5: Tackling toxic spans detection chal- lenge with supervised spanBERT-based model and unsupervised LIME-based model. In SemEval.\n\nSaying goodbye to civil comments. Aja Bogdanoff, Aja Bogdanoff. 2017. Saying goodbye to civil com- ments. Accessed: 2021-04-15.\n\nEnriching word vectors with subword information. Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov, TACL. 5Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. TACL, 5:135-146.\n\nNuanced metrics for measuring unintended bias with real data for text classification. Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, Lucy Vasserman, WWW. San Francisco, USADaniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. 2019a. Nuanced met- rics for measuring unintended bias with real data for text classification. In WWW, pages 491-500, San Francisco, USA.\n\nExploring the role of human raters in creating nlp datasets. Daniel Borkan, Jeffrey Sorensen, Lucy Vasserman, Daniel Borkan, Jeffrey Sorensen, and Lucy Vasserman. 2019b. Exploring the role of human raters in creat- ing nlp datasets. Accessed: 2021-04-15.\n\nUAntwerp at SemEval-2021 Task 5: Spans are spans, stacking a binary word level approach to toxic span detection. Ben Burtenshaw, Mike Kestemont, SemEval. Ben Burtenshaw and Mike Kestemont. 2021. UAntwerp at SemEval-2021 Task 5: Spans are spans, stacking a binary word level approach to toxic span detection. In SemEval.\n\n2021. macech at SemEval-2021 Task 5: Toxic spans detection. Maggie Cech, In SemEvalMaggie Cech. 2021. macech at SemEval-2021 Task 5: Toxic spans detection. In SemEval.\n\nBERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, NAACL. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language under- standing. In NAACL, pages 4171-4186.\n\nHamiltonDinggg at SemEval-2021 Task 5: Investigating toxic span detection using RoBERTa pre-training. Huiyang Ding, David Jurgens, In Se-mEvalHuiyang Ding and David Jurgens. 2021. HamiltonD- inggg at SemEval-2021 Task 5: Investigating toxic span detection using RoBERTa pre-training. In Se- mEval.\n\nWant to save the comments from trolls? do it yourself. Kline Finley, Kline Finley. 2016. Want to save the comments from trolls? do it yourself. Accessed: 2021-04-15.\n\nCisco at SemEval-2021 Task 5: What's toxic?: Leveraging transformers for multiple toxic span extraction from online comments. Sreyan Ghosh, Sonal Kumar, SemEval. Sreyan Ghosh and Sonal Kumar. 2021. Cisco at SemEval-2021 Task 5: What's toxic?: Leveraging transformers for multiple toxic span extraction from online comments. In SemEval.\n\nUIT-E10dot3 at SemEval 2021 Task 5: Toxic spans detection with roberta and spacy's library base systems. Gia Phu, Luan Thanh Hoang, Nguyen, SemEval. Phu Gia Hoang and Luan Thanh Nguyen. 2021. UIT- E10dot3 at SemEval 2021 Task 5: Toxic spans de- tection with roberta and spacy's library base systems. In SemEval.\n\nLong short-term memory. Sepp Hochreiter, J\u00fcrgen Schmidhuber, Neural Computation. 98Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):1735-1780.\n\nOpen-domain targeted sentiment analysis via span-based extraction and classification. Minghao Hu, Yuxing Peng, Zhen Huang, Dongsheng Li, Yiwei Lv, ACL. Minghao Hu, Yuxing Peng, Zhen Huang, Dongsheng Li, and Yiwei Lv. 2019. Open-domain targeted sen- timent analysis via span-based extraction and classi- fication. In ACL, pages 537-546.\n\nSefamerve arge at SemEval-2021 Task 5: Toxic span detection using segmentation based 1-d convolutional neural network model. Birol Kuyumcu, Selman Delil, C\u00fcneyt Aksakall\u0131, SemEval. Birol Kuyumcu, Selman Delil, and C\u00fcneyt aksakall\u0131. 2021. Sefamerve arge at SemEval-2021 Task 5: Toxic span detection using segmentation based 1-d convolutional neural network model. In SemEval.\n\nJiwei Li, Will Monroe, Dan Jurafsky, arXiv:1612.08220Understanding neural networks through representation erasure. arXiv preprintJiwei Li, Will Monroe, and Dan Jurafsky. 2016. Un- derstanding neural networks through representation erasure. arXiv preprint arXiv:1612.08220.\n\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, Veselin Stoyanov, arXiv:1907.11692Roberta: A robustly optimized bert pretraining approach. arXiv preprintYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining ap- proach. arXiv preprint arXiv:1907.11692.\n\nA unified approach to interpreting model predictions. Scott Lundberg, Su-In Lee, arXiv:1705.07874arXiv preprintScott Lundberg and Su-In Lee. 2017. A unified ap- proach to interpreting model predictions. arXiv preprint arXiv:1705.07874.\n\nUIT-ISE-NLP at SemEval-2021 Task 5: Toxic span detection with BiLSTM -CRF and toxic BERT comment classification. T Son, Ngan Luu, Nguyen, SemEval. Son T. Luu and Ngan Nguyen. 2021. UIT-ISE-NLP at SemEval-2021 Task 5: Toxic span detection with BiLSTM -CRF and toxic BERT comment classifi- cation. In SemEval.\n\nFine-grained analysis of propaganda in news article. Giovanni Da San, Seunghak Martino, Alberto Yu, Rostislav Barr\u00f3n-Cedeno, Preslav Petrov, Nakov, EMNLP-IJCNLP. Giovanni Da San Martino, Seunghak Yu, Alberto Barr\u00f3n-Cedeno, Rostislav Petrov, and Preslav Nakov. 2019. Fine-grained analysis of propaganda in news article. In EMNLP-IJCNLP, pages 5640- 5650.\n\nTowards transparent and explainable attention models. Akash Kumar Mohankumar, Preksha Nema, Sharan Narasimhan, M Mitesh, Khapra, Balaraman Balaji Vasan Srinivasan, Ravindran, ACL. Akash Kumar Mohankumar, Preksha Nema, Sharan Narasimhan, Mitesh M Khapra, Balaji Vasan Srini- vasan, and Balaraman Ravindran. 2020. Towards transparent and explainable attention models. In ACL, pages 4206-4216.\n\nHuy Dao Quang, and Quang Huu Pham. 2021. S-NLP at semeval-2021 task 5: Toxic spans detection. Anh Viet, Tam Nguyen, Nguyen, SemEval. Viet Anh Nguyen, Tam Nguyen, Huy Dao Quang, and Quang Huu Pham. 2021. S-NLP at semeval-2021 task 5: Toxic spans detection. In SemEval.\n\nAn ensemble approach to identify toxicity in text. Marco Palomino, Dawid Grad, James Bedwell, In SemEvalMarco Palomino, Dawid Grad, and James Bedwell. 2021. An ensemble approach to identify toxicity in text. In SemEval.\n\nDeeper attention to abusive user content moderation. John Pavlopoulos, Prodromos Malakasiotis, and Ion Androutsopoulos. Copenghagen, DenmarkEMNLPJohn Pavlopoulos, Prodromos Malakasiotis, and Ion Androutsopoulos. 2017. Deeper attention to abusive user content moderation. In EMNLP, pages 1125- 1135, Copenghagen, Denmark.\n\nGHOST at SemEval-2021 Task 5: Is explanation all you need?. Kamil Pluci\u0144ski, Hanna Klimczak, In SemEvalKamil Pluci\u0144ski and Hanna Klimczak. 2021. GHOST at SemEval-2021 Task 5: Is explanation all you need? In SemEval.\n\nWLV-RIT at SemEval-2021 Task 5: A neural transformer framework for detecting toxic spans. Tharindu Ranasinghe, Diptanu Sarkar, Marcos Zampieri, Alexander Ororbia, SemEval. Tharindu Ranasinghe, Diptanu Sarkar, Marcos Zampieri, and Alexander Ororbia. 2021. WLV-RIT at SemEval-2021 Task 5: A neural transformer framework for detecting toxic spans. In SemEval.\n\nExplaining the predictions of any classifier. Marco T Ribeiro, Sameer Singh, Carlos Guestrin, SIGKDD. San Francisco, USAWhy Should I Trust You?Marco T. Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \"Why Should I Trust You?\" Explaining the predictions of any classifier. In SIGKDD, pages 1135-1144, San Francisco, USA.\n\nNLP UIOWA at Semeval-2021 Task 5: Transferring toxic sets to tag toxic spans. Jonathan Rusert, In SemEvalJonathan Rusert. 2021. NLP UIOWA at Semeval-2021 Task 5: Transferring toxic sets to tag toxic spans. In SemEval.\n\nHLE-UPC at SemEval-2021 Task 5: Multi-Depth Distil-BERT for toxic spans detection. SemEval. Rafel Palliser Sans and Albert Rial Farr\u00e0sRafel Palliser Sans and Albert Rial Farr\u00e0s. 2021. HLE- UPC at SemEval-2021 Task 5: Multi-Depth Distil- BERT for toxic spans detection. In SemEval.\n\nA survey on hate speech detection using natural language processing. Anna Schmidt, Michael Wiegand, Workshop on Natural Language Processing for Social Media. Valencia, SpainAnna Schmidt and Michael Wiegand. 2017. A survey on hate speech detection using natural language pro- cessing. In Workshop on Natural Language Process- ing for Social Media, pages 1-10, Valencia, Spain.\n\nNeural machine translation of rare words with subword units. Rico Sennrich, Barry Haddow, Alexandra Birch, ACL. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In ACL.\n\nPierre Tom De Smedt, Sylvia Vou\u00e9, Melina Jaki, Guy De R\u00f6ttcher, Pauw, Profanity & offensive words. POWTextgainTom De Smedt, Pierre Vou\u00e9, Sylvia Jaki, Melina R\u00f6ttcher, and Guy De Pauw. 2020. Profanity & of- fensive words (POW). Textgain.\n\nAS-tarTwice at SemEval-2021 Task 5: Toxic span detection using RoBERTa-CRF, domain specific pretraining and self-training. Abhinav Thakur Ashutosh Suman, Jain, SemEval. Thakur Ashutosh Suman and Abhinav Jain. 2021. AS- tarTwice at SemEval-2021 Task 5: Toxic span de- tection using RoBERTa-CRF, domain specific pre- training and self-training. In SemEval.\n\nAn Introduction to Conditional Random Fields for relational learning. Introduction to statistical relational learning. Charles Sutton, Andrew Mccallum, 2Charles Sutton and Andrew McCallum. 2006. An In- troduction to Conditional Random Fields for rela- tional learning. Introduction to statistical relational learning, 2:93-128.\n\nFeatures of similarity. Psychological review. Amos Tversky, 84327Amos Tversky. 1977. Features of similarity. Psycho- logical review, 84(4):327.\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Illia Kaiser, Polosukhin, NIPS. 30Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NIPS, volume 30.\n\nEx machina: Personal attacks seen at scale. Ellery Wulczyn, Nithum Thain, Lucas Dixon, WWW. Perth, AustraliaEllery Wulczyn, Nithum Thain, and Lucas Dixon. 2017. Ex machina: Personal attacks seen at scale. In WWW, pages 1391-1399, Perth, Australia.\n\nSemeval-2019 task 6: Identifying and categorizing offensive language in social media (offenseval). Marcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, Ritesh Kumar, In SemEvalMarcos Zampieri, Shervin Malmasi, Preslav Nakov, Sara Rosenthal, Noura Farra, and Ritesh Kumar. 2019. Semeval-2019 task 6: Identifying and cate- gorizing offensive language in social media (offen- seval). In SemEval.\n\nMedAI at SemEval-2021 Task 5: Start-to-end tagging framework for toxic spans detection. Zhen Hongjie Fan, Junfei Wang, Liu, SemEval. Hongjie Fan Zhen Wang and Junfei Liu. 2021. MedAI at SemEval-2021 Task 5: Start-to-end tagging frame- work for toxic spans detection. In SemEval.\n\nHITSZ-HLT at SemEval-2021 Task 5: Span-based ensemble model with toxic lexicon. Qinglin Zhu, Zijie Lin, Yice Zhang, Jingyi Sun, Xiang Li, Qihui Lin, Ruifeng Xu, In SemEvalQinglin Zhu, Zijie Lin, Yice Zhang, Jingyi Sun, Xi- ang Li, Qihui Lin, and Ruifeng Xu. 2021. HITSZ- HLT at SemEval-2021 Task 5: Span-based ensemble model with toxic lexicon. In SemEval.\n", "annotations": {"author": "[{\"end\":209,\"start\":62},{\"end\":227,\"start\":210},{\"end\":269,\"start\":228},{\"end\":420,\"start\":270},{\"end\":437,\"start\":421},{\"end\":510,\"start\":438}]", "publisher": null, "author_last_name": "[{\"end\":78,\"start\":67},{\"end\":226,\"start\":218},{\"end\":239,\"start\":232},{\"end\":289,\"start\":274},{\"end\":436,\"start\":430}]", "author_first_name": "[{\"end\":66,\"start\":62},{\"end\":217,\"start\":210},{\"end\":231,\"start\":228},{\"end\":273,\"start\":270},{\"end\":422,\"start\":421},{\"end\":429,\"start\":423}]", "author_affiliation": "[{\"end\":155,\"start\":80},{\"end\":208,\"start\":157},{\"end\":366,\"start\":291},{\"end\":419,\"start\":368},{\"end\":509,\"start\":439}]", "title": "[{\"end\":43,\"start\":1},{\"end\":553,\"start\":511}]", "venue": "[{\"end\":639,\"start\":555}]", "abstract": "[{\"end\":1569,\"start\":745}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1767,\"start\":1745},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":1868,\"start\":1841},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":1893,\"start\":1868},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":1915,\"start\":1893},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3212,\"start\":3195},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3233,\"start\":3212},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3921,\"start\":3908},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4084,\"start\":4067},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":4554,\"start\":4533},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4714,\"start\":4691},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4929,\"start\":4907},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8958,\"start\":8936},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":12657,\"start\":12639},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12756,\"start\":12735},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":12841,\"start\":12814},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":13469,\"start\":13453},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":14168,\"start\":14150},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":14724,\"start\":14704},{\"end\":14777,\"start\":14738},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":15063,\"start\":15041},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":15976,\"start\":15947},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16075,\"start\":16050},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16281,\"start\":16265},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":16382,\"start\":16358},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16436,\"start\":16413},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16464,\"start\":16441},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":16560,\"start\":16538},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":16967,\"start\":16949},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16989,\"start\":16967},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":17052,\"start\":17027},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":17174,\"start\":17154},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":17272,\"start\":17249},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17449,\"start\":17417},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":17547,\"start\":17529},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":17608,\"start\":17583},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":17804,\"start\":17791},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17981,\"start\":17952},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":18356,\"start\":18335},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":18426,\"start\":18411},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":18738,\"start\":18724},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":18808,\"start\":18794},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18837,\"start\":18808},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":18860,\"start\":18837},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18883,\"start\":18860},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":20380,\"start\":20363},{\"end\":20816,\"start\":20814},{\"end\":21949,\"start\":21947},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":22216,\"start\":22198},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22410,\"start\":22390},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22449,\"start\":22424},{\"end\":22603,\"start\":22601},{\"end\":22964,\"start\":22952},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":24758,\"start\":24736},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":25493,\"start\":25475},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":27060,\"start\":27030},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":27111,\"start\":27097},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":27487,\"start\":27470},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":27529,\"start\":27505},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":27562,\"start\":27549},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":27609,\"start\":27578},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":27645,\"start\":27628},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":27682,\"start\":27660},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":27718,\"start\":27701},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":27751,\"start\":27734},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":27782,\"start\":27765},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":28120,\"start\":28103},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":28435,\"start\":28417},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":29526,\"start\":29503},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":30337,\"start\":30316},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":30684,\"start\":30664},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30734,\"start\":30711},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":30762,\"start\":30740},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":33558,\"start\":33536},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":34021,\"start\":34003},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":35525,\"start\":35502}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":33407,\"start\":33313},{\"attributes\":{\"id\":\"fig_1\"},\"end\":33559,\"start\":33408},{\"attributes\":{\"id\":\"fig_2\"},\"end\":33614,\"start\":33560},{\"attributes\":{\"id\":\"fig_3\"},\"end\":33862,\"start\":33615},{\"attributes\":{\"id\":\"fig_4\"},\"end\":33972,\"start\":33863},{\"attributes\":{\"id\":\"fig_5\"},\"end\":34170,\"start\":33973},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":34456,\"start\":34171},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":34775,\"start\":34457},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":34938,\"start\":34776},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":35052,\"start\":34939}]", "paragraph": "[{\"end\":3736,\"start\":1585},{\"end\":4214,\"start\":3769},{\"end\":4965,\"start\":4216},{\"end\":5339,\"start\":4967},{\"end\":5518,\"start\":5341},{\"end\":6653,\"start\":5520},{\"end\":7221,\"start\":6683},{\"end\":7713,\"start\":7253},{\"end\":8026,\"start\":7715},{\"end\":8902,\"start\":8051},{\"end\":9551,\"start\":8904},{\"end\":11153,\"start\":9572},{\"end\":11208,\"start\":11160},{\"end\":11818,\"start\":11215},{\"end\":11924,\"start\":11820},{\"end\":12232,\"start\":11926},{\"end\":12494,\"start\":12259},{\"end\":12658,\"start\":12523},{\"end\":13141,\"start\":12660},{\"end\":13597,\"start\":13143},{\"end\":13894,\"start\":13599},{\"end\":14565,\"start\":13919},{\"end\":14902,\"start\":14567},{\"end\":15236,\"start\":14904},{\"end\":15328,\"start\":15238},{\"end\":19168,\"start\":15366},{\"end\":19464,\"start\":19183},{\"end\":19993,\"start\":19496},{\"end\":20107,\"start\":20008},{\"end\":20308,\"start\":20109},{\"end\":20577,\"start\":20310},{\"end\":21384,\"start\":20579},{\"end\":23126,\"start\":21386},{\"end\":23426,\"start\":23154},{\"end\":24062,\"start\":23444},{\"end\":24535,\"start\":24064},{\"end\":25082,\"start\":24573},{\"end\":25632,\"start\":25098},{\"end\":27423,\"start\":25634},{\"end\":28559,\"start\":27425},{\"end\":29066,\"start\":28578},{\"end\":29824,\"start\":29068},{\"end\":30388,\"start\":29826},{\"end\":30505,\"start\":30427},{\"end\":30544,\"start\":30525},{\"end\":31145,\"start\":30564},{\"end\":31506,\"start\":31161},{\"end\":31838,\"start\":31508},{\"end\":32170,\"start\":31840},{\"end\":32468,\"start\":32172},{\"end\":33312,\"start\":32470}]", "formula": null, "table_ref": "[{\"end\":6395,\"start\":6386},{\"end\":7934,\"start\":7926},{\"end\":8216,\"start\":8208},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":9984,\"start\":9977},{\"end\":20885,\"start\":20878},{\"end\":21097,\"start\":21090},{\"end\":22177,\"start\":22170},{\"end\":23125,\"start\":23118},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":27973,\"start\":27966},{\"end\":28171,\"start\":28164},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":30387,\"start\":30378}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1583,\"start\":1571},{\"attributes\":{\"n\":\"2\"},\"end\":3767,\"start\":3739},{\"attributes\":{\"n\":\"2.1\"},\"end\":6681,\"start\":6656},{\"attributes\":{\"n\":\"2.2\"},\"end\":7251,\"start\":7224},{\"attributes\":{\"n\":\"2.3\"},\"end\":8049,\"start\":8029},{\"attributes\":{\"n\":\"3\"},\"end\":9570,\"start\":9554},{\"end\":11158,\"start\":11156},{\"end\":11213,\"start\":11211},{\"attributes\":{\"n\":\"4\"},\"end\":12257,\"start\":12235},{\"attributes\":{\"n\":\"4.1\"},\"end\":12521,\"start\":12497},{\"attributes\":{\"n\":\"4.2\"},\"end\":13917,\"start\":13897},{\"attributes\":{\"n\":\"4.3\"},\"end\":15364,\"start\":15331},{\"attributes\":{\"n\":\"5\"},\"end\":19181,\"start\":19171},{\"attributes\":{\"n\":\"5.1\"},\"end\":19494,\"start\":19467},{\"attributes\":{\"n\":\"5.2\"},\"end\":20006,\"start\":19996},{\"attributes\":{\"n\":\"6\"},\"end\":23152,\"start\":23129},{\"attributes\":{\"n\":\"6.1\"},\"end\":23442,\"start\":23429},{\"attributes\":{\"n\":\"6.2\"},\"end\":24571,\"start\":24538},{\"attributes\":{\"n\":\"6.3\"},\"end\":25096,\"start\":25085},{\"attributes\":{\"n\":\"6.4\"},\"end\":28576,\"start\":28562},{\"end\":30407,\"start\":30391},{\"end\":30425,\"start\":30410},{\"end\":30523,\"start\":30508},{\"end\":30562,\"start\":30547},{\"attributes\":{\"n\":\"7\"},\"end\":31159,\"start\":31148},{\"end\":33324,\"start\":33314},{\"end\":33419,\"start\":33409},{\"end\":33571,\"start\":33561},{\"end\":33874,\"start\":33864},{\"end\":33984,\"start\":33974},{\"end\":34181,\"start\":34172},{\"end\":34467,\"start\":34458},{\"end\":34786,\"start\":34777},{\"end\":34949,\"start\":34940}]", "table": "[{\"end\":34775,\"start\":34469}]", "figure_caption": "[{\"end\":33407,\"start\":33326},{\"end\":33559,\"start\":33421},{\"end\":33614,\"start\":33573},{\"end\":33862,\"start\":33617},{\"end\":33972,\"start\":33876},{\"end\":34170,\"start\":33986},{\"end\":34456,\"start\":34183},{\"end\":34938,\"start\":34788},{\"end\":35052,\"start\":34951}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5996,\"start\":5990},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9255,\"start\":9249},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10482,\"start\":10476},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":12229,\"start\":12223},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22494,\"start\":22488}]", "bib_author_first_name": "[{\"end\":35981,\"start\":35977},{\"end\":35994,\"start\":35989},{\"end\":36011,\"start\":36005},{\"end\":36026,\"start\":36020},{\"end\":36040,\"start\":36034},{\"end\":36057,\"start\":36051},{\"end\":36394,\"start\":36388},{\"end\":36408,\"start\":36403},{\"end\":36426,\"start\":36418},{\"end\":36778,\"start\":36768},{\"end\":36795,\"start\":36790},{\"end\":36808,\"start\":36803},{\"end\":37090,\"start\":37087},{\"end\":37236,\"start\":37231},{\"end\":37256,\"start\":37249},{\"end\":37270,\"start\":37264},{\"end\":37284,\"start\":37279},{\"end\":37533,\"start\":37527},{\"end\":37547,\"start\":37542},{\"end\":37562,\"start\":37555},{\"end\":37579,\"start\":37573},{\"end\":37591,\"start\":37587},{\"end\":37912,\"start\":37906},{\"end\":37928,\"start\":37921},{\"end\":37943,\"start\":37939},{\"end\":38217,\"start\":38214},{\"end\":38234,\"start\":38230},{\"end\":38488,\"start\":38482},{\"end\":38678,\"start\":38673},{\"end\":38695,\"start\":38687},{\"end\":38709,\"start\":38703},{\"end\":38723,\"start\":38715},{\"end\":39035,\"start\":39028},{\"end\":39047,\"start\":39042},{\"end\":39285,\"start\":39280},{\"end\":39524,\"start\":39518},{\"end\":39537,\"start\":39532},{\"end\":39837,\"start\":39834},{\"end\":39853,\"start\":39843},{\"end\":40070,\"start\":40066},{\"end\":40089,\"start\":40083},{\"end\":40325,\"start\":40318},{\"end\":40336,\"start\":40330},{\"end\":40347,\"start\":40343},{\"end\":40364,\"start\":40355},{\"end\":40374,\"start\":40369},{\"end\":40699,\"start\":40694},{\"end\":40715,\"start\":40709},{\"end\":40729,\"start\":40723},{\"end\":40950,\"start\":40945},{\"end\":40959,\"start\":40955},{\"end\":40971,\"start\":40968},{\"end\":41225,\"start\":41219},{\"end\":41235,\"start\":41231},{\"end\":41246,\"start\":41241},{\"end\":41261,\"start\":41254},{\"end\":41272,\"start\":41266},{\"end\":41285,\"start\":41280},{\"end\":41296,\"start\":41292},{\"end\":41307,\"start\":41303},{\"end\":41319,\"start\":41315},{\"end\":41340,\"start\":41333},{\"end\":41734,\"start\":41729},{\"end\":41750,\"start\":41745},{\"end\":42026,\"start\":42025},{\"end\":42036,\"start\":42032},{\"end\":42282,\"start\":42274},{\"end\":42299,\"start\":42291},{\"end\":42316,\"start\":42309},{\"end\":42330,\"start\":42321},{\"end\":42353,\"start\":42346},{\"end\":42635,\"start\":42630},{\"end\":42661,\"start\":42654},{\"end\":42674,\"start\":42668},{\"end\":42688,\"start\":42687},{\"end\":42714,\"start\":42705},{\"end\":43065,\"start\":43062},{\"end\":43075,\"start\":43072},{\"end\":43293,\"start\":43288},{\"end\":43309,\"start\":43304},{\"end\":43321,\"start\":43316},{\"end\":43515,\"start\":43511},{\"end\":43845,\"start\":43840},{\"end\":43862,\"start\":43857},{\"end\":44095,\"start\":44087},{\"end\":44115,\"start\":44108},{\"end\":44130,\"start\":44124},{\"end\":44150,\"start\":44141},{\"end\":44406,\"start\":44401},{\"end\":44408,\"start\":44407},{\"end\":44424,\"start\":44418},{\"end\":44438,\"start\":44432},{\"end\":44764,\"start\":44756},{\"end\":45252,\"start\":45248},{\"end\":45269,\"start\":45262},{\"end\":45621,\"start\":45617},{\"end\":45637,\"start\":45632},{\"end\":45655,\"start\":45646},{\"end\":45800,\"start\":45794},{\"end\":45821,\"start\":45815},{\"end\":45834,\"start\":45828},{\"end\":45844,\"start\":45841},{\"end\":45847,\"start\":45845},{\"end\":46162,\"start\":46155},{\"end\":46514,\"start\":46507},{\"end\":46529,\"start\":46523},{\"end\":46767,\"start\":46763},{\"end\":46895,\"start\":46889},{\"end\":46909,\"start\":46905},{\"end\":46923,\"start\":46919},{\"end\":46937,\"start\":46932},{\"end\":46954,\"start\":46949},{\"end\":46967,\"start\":46962},{\"end\":46969,\"start\":46968},{\"end\":46982,\"start\":46977},{\"end\":47241,\"start\":47235},{\"end\":47257,\"start\":47251},{\"end\":47270,\"start\":47265},{\"end\":47545,\"start\":47539},{\"end\":47563,\"start\":47556},{\"end\":47580,\"start\":47573},{\"end\":47592,\"start\":47588},{\"end\":47609,\"start\":47604},{\"end\":47623,\"start\":47617},{\"end\":47951,\"start\":47947},{\"end\":47971,\"start\":47965},{\"end\":48226,\"start\":48219},{\"end\":48237,\"start\":48232},{\"end\":48247,\"start\":48243},{\"end\":48261,\"start\":48255},{\"end\":48272,\"start\":48267},{\"end\":48282,\"start\":48277},{\"end\":48295,\"start\":48288}]", "bib_author_last_name": "[{\"end\":35987,\"start\":35982},{\"end\":36003,\"start\":35995},{\"end\":36018,\"start\":36012},{\"end\":36032,\"start\":36027},{\"end\":36049,\"start\":36041},{\"end\":36066,\"start\":36058},{\"end\":36401,\"start\":36395},{\"end\":36416,\"start\":36409},{\"end\":36431,\"start\":36427},{\"end\":36788,\"start\":36779},{\"end\":36801,\"start\":36796},{\"end\":36814,\"start\":36809},{\"end\":37100,\"start\":37091},{\"end\":37247,\"start\":37237},{\"end\":37262,\"start\":37257},{\"end\":37277,\"start\":37271},{\"end\":37292,\"start\":37285},{\"end\":37540,\"start\":37534},{\"end\":37553,\"start\":37548},{\"end\":37571,\"start\":37563},{\"end\":37585,\"start\":37580},{\"end\":37601,\"start\":37592},{\"end\":37919,\"start\":37913},{\"end\":37937,\"start\":37929},{\"end\":37953,\"start\":37944},{\"end\":38228,\"start\":38218},{\"end\":38244,\"start\":38235},{\"end\":38493,\"start\":38489},{\"end\":38685,\"start\":38679},{\"end\":38701,\"start\":38696},{\"end\":38713,\"start\":38710},{\"end\":38733,\"start\":38724},{\"end\":39040,\"start\":39036},{\"end\":39055,\"start\":39048},{\"end\":39292,\"start\":39286},{\"end\":39530,\"start\":39525},{\"end\":39543,\"start\":39538},{\"end\":39841,\"start\":39838},{\"end\":39859,\"start\":39854},{\"end\":39867,\"start\":39861},{\"end\":40081,\"start\":40071},{\"end\":40101,\"start\":40090},{\"end\":40328,\"start\":40326},{\"end\":40341,\"start\":40337},{\"end\":40353,\"start\":40348},{\"end\":40367,\"start\":40365},{\"end\":40377,\"start\":40375},{\"end\":40707,\"start\":40700},{\"end\":40721,\"start\":40716},{\"end\":40739,\"start\":40730},{\"end\":40953,\"start\":40951},{\"end\":40966,\"start\":40960},{\"end\":40980,\"start\":40972},{\"end\":41229,\"start\":41226},{\"end\":41239,\"start\":41236},{\"end\":41252,\"start\":41247},{\"end\":41264,\"start\":41262},{\"end\":41278,\"start\":41273},{\"end\":41290,\"start\":41286},{\"end\":41301,\"start\":41297},{\"end\":41313,\"start\":41308},{\"end\":41331,\"start\":41320},{\"end\":41349,\"start\":41341},{\"end\":41743,\"start\":41735},{\"end\":41754,\"start\":41751},{\"end\":42030,\"start\":42027},{\"end\":42040,\"start\":42037},{\"end\":42048,\"start\":42042},{\"end\":42289,\"start\":42283},{\"end\":42307,\"start\":42300},{\"end\":42319,\"start\":42317},{\"end\":42344,\"start\":42331},{\"end\":42360,\"start\":42354},{\"end\":42367,\"start\":42362},{\"end\":42652,\"start\":42636},{\"end\":42666,\"start\":42662},{\"end\":42685,\"start\":42675},{\"end\":42695,\"start\":42689},{\"end\":42703,\"start\":42697},{\"end\":42738,\"start\":42715},{\"end\":42749,\"start\":42740},{\"end\":43070,\"start\":43066},{\"end\":43082,\"start\":43076},{\"end\":43090,\"start\":43084},{\"end\":43302,\"start\":43294},{\"end\":43314,\"start\":43310},{\"end\":43329,\"start\":43322},{\"end\":43527,\"start\":43516},{\"end\":43855,\"start\":43846},{\"end\":43871,\"start\":43863},{\"end\":44106,\"start\":44096},{\"end\":44122,\"start\":44116},{\"end\":44139,\"start\":44131},{\"end\":44158,\"start\":44151},{\"end\":44416,\"start\":44409},{\"end\":44430,\"start\":44425},{\"end\":44447,\"start\":44439},{\"end\":44771,\"start\":44765},{\"end\":45260,\"start\":45253},{\"end\":45277,\"start\":45270},{\"end\":45630,\"start\":45622},{\"end\":45644,\"start\":45638},{\"end\":45661,\"start\":45656},{\"end\":45813,\"start\":45801},{\"end\":45826,\"start\":45822},{\"end\":45839,\"start\":45835},{\"end\":45856,\"start\":45848},{\"end\":45862,\"start\":45858},{\"end\":46184,\"start\":46163},{\"end\":46190,\"start\":46186},{\"end\":46521,\"start\":46515},{\"end\":46538,\"start\":46530},{\"end\":46775,\"start\":46768},{\"end\":46903,\"start\":46896},{\"end\":46917,\"start\":46910},{\"end\":46930,\"start\":46924},{\"end\":46947,\"start\":46938},{\"end\":46960,\"start\":46955},{\"end\":46975,\"start\":46970},{\"end\":46989,\"start\":46983},{\"end\":47001,\"start\":46991},{\"end\":47249,\"start\":47242},{\"end\":47263,\"start\":47258},{\"end\":47276,\"start\":47271},{\"end\":47554,\"start\":47546},{\"end\":47571,\"start\":47564},{\"end\":47586,\"start\":47581},{\"end\":47602,\"start\":47593},{\"end\":47615,\"start\":47610},{\"end\":47629,\"start\":47624},{\"end\":47963,\"start\":47952},{\"end\":47976,\"start\":47972},{\"end\":47981,\"start\":47978},{\"end\":48230,\"start\":48227},{\"end\":48241,\"start\":48238},{\"end\":48253,\"start\":48248},{\"end\":48265,\"start\":48262},{\"end\":48275,\"start\":48273},{\"end\":48286,\"start\":48283},{\"end\":48298,\"start\":48296}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":181704107},\"end\":36286,\"start\":35919},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":233024836},\"end\":36612,\"start\":36288},{\"attributes\":{\"id\":\"b2\"},\"end\":37051,\"start\":36614},{\"attributes\":{\"id\":\"b3\"},\"end\":37180,\"start\":37053},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":207556454},\"end\":37439,\"start\":37182},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":75135222},\"end\":37843,\"start\":37441},{\"attributes\":{\"id\":\"b6\"},\"end\":38099,\"start\":37845},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":236460146},\"end\":38420,\"start\":38101},{\"attributes\":{\"id\":\"b8\"},\"end\":38589,\"start\":38422},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":52967399},\"end\":38924,\"start\":38591},{\"attributes\":{\"id\":\"b10\"},\"end\":39223,\"start\":38926},{\"attributes\":{\"id\":\"b11\"},\"end\":39390,\"start\":39225},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":235248423},\"end\":39727,\"start\":39392},{\"attributes\":{\"id\":\"b13\"},\"end\":40040,\"start\":39729},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1915014},\"end\":40230,\"start\":40042},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":182952458},\"end\":40567,\"start\":40232},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":236459848},\"end\":40943,\"start\":40569},{\"attributes\":{\"doi\":\"arXiv:1612.08220\",\"id\":\"b17\"},\"end\":41217,\"start\":40945},{\"attributes\":{\"doi\":\"arXiv:1907.11692\",\"id\":\"b18\"},\"end\":41673,\"start\":41219},{\"attributes\":{\"doi\":\"arXiv:1705.07874\",\"id\":\"b19\"},\"end\":41910,\"start\":41675},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":233307032},\"end\":42219,\"start\":41912},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":202788575},\"end\":42574,\"start\":42221},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":216641945},\"end\":42966,\"start\":42576},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":236460230},\"end\":43235,\"start\":42968},{\"attributes\":{\"id\":\"b24\"},\"end\":43456,\"start\":43237},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":2944650},\"end\":43778,\"start\":43458},{\"attributes\":{\"id\":\"b26\"},\"end\":43995,\"start\":43780},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":233209738},\"end\":44353,\"start\":43997},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":13029170},\"end\":44676,\"start\":44355},{\"attributes\":{\"id\":\"b29\"},\"end\":44895,\"start\":44678},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":232478589},\"end\":45177,\"start\":44897},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":9626793},\"end\":45554,\"start\":45179},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":1114678},\"end\":45792,\"start\":45556},{\"attributes\":{\"id\":\"b33\"},\"end\":46030,\"start\":45794},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":236460307},\"end\":46386,\"start\":46032},{\"attributes\":{\"id\":\"b35\"},\"end\":46715,\"start\":46388},{\"attributes\":{\"id\":\"b36\"},\"end\":46860,\"start\":46717},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":13756489},\"end\":47189,\"start\":46862},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":6060248},\"end\":47438,\"start\":47191},{\"attributes\":{\"id\":\"b39\"},\"end\":47857,\"start\":47440},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":236460136},\"end\":48137,\"start\":47859},{\"attributes\":{\"id\":\"b41\"},\"end\":48495,\"start\":48139}]", "bib_title": "[{\"end\":35975,\"start\":35919},{\"end\":36386,\"start\":36288},{\"end\":37229,\"start\":37182},{\"end\":37525,\"start\":37441},{\"end\":38212,\"start\":38101},{\"end\":38671,\"start\":38591},{\"end\":39516,\"start\":39392},{\"end\":39832,\"start\":39729},{\"end\":40064,\"start\":40042},{\"end\":40316,\"start\":40232},{\"end\":40692,\"start\":40569},{\"end\":42023,\"start\":41912},{\"end\":42272,\"start\":42221},{\"end\":42628,\"start\":42576},{\"end\":43060,\"start\":42968},{\"end\":43509,\"start\":43458},{\"end\":44085,\"start\":43997},{\"end\":44399,\"start\":44355},{\"end\":44978,\"start\":44897},{\"end\":45246,\"start\":45179},{\"end\":45615,\"start\":45556},{\"end\":46153,\"start\":46032},{\"end\":46887,\"start\":46862},{\"end\":47233,\"start\":47191},{\"end\":47945,\"start\":47859}]", "bib_author": "[{\"end\":35989,\"start\":35977},{\"end\":36005,\"start\":35989},{\"end\":36020,\"start\":36005},{\"end\":36034,\"start\":36020},{\"end\":36051,\"start\":36034},{\"end\":36068,\"start\":36051},{\"end\":36403,\"start\":36388},{\"end\":36418,\"start\":36403},{\"end\":36433,\"start\":36418},{\"end\":36790,\"start\":36768},{\"end\":36803,\"start\":36790},{\"end\":36816,\"start\":36803},{\"end\":37102,\"start\":37087},{\"end\":37249,\"start\":37231},{\"end\":37264,\"start\":37249},{\"end\":37279,\"start\":37264},{\"end\":37294,\"start\":37279},{\"end\":37542,\"start\":37527},{\"end\":37555,\"start\":37542},{\"end\":37573,\"start\":37555},{\"end\":37587,\"start\":37573},{\"end\":37603,\"start\":37587},{\"end\":37921,\"start\":37906},{\"end\":37939,\"start\":37921},{\"end\":37955,\"start\":37939},{\"end\":38230,\"start\":38214},{\"end\":38246,\"start\":38230},{\"end\":38495,\"start\":38482},{\"end\":38687,\"start\":38673},{\"end\":38703,\"start\":38687},{\"end\":38715,\"start\":38703},{\"end\":38735,\"start\":38715},{\"end\":39042,\"start\":39028},{\"end\":39057,\"start\":39042},{\"end\":39294,\"start\":39280},{\"end\":39532,\"start\":39518},{\"end\":39545,\"start\":39532},{\"end\":39843,\"start\":39834},{\"end\":39861,\"start\":39843},{\"end\":39869,\"start\":39861},{\"end\":40083,\"start\":40066},{\"end\":40103,\"start\":40083},{\"end\":40330,\"start\":40318},{\"end\":40343,\"start\":40330},{\"end\":40355,\"start\":40343},{\"end\":40369,\"start\":40355},{\"end\":40379,\"start\":40369},{\"end\":40709,\"start\":40694},{\"end\":40723,\"start\":40709},{\"end\":40741,\"start\":40723},{\"end\":40955,\"start\":40945},{\"end\":40968,\"start\":40955},{\"end\":40982,\"start\":40968},{\"end\":41231,\"start\":41219},{\"end\":41241,\"start\":41231},{\"end\":41254,\"start\":41241},{\"end\":41266,\"start\":41254},{\"end\":41280,\"start\":41266},{\"end\":41292,\"start\":41280},{\"end\":41303,\"start\":41292},{\"end\":41315,\"start\":41303},{\"end\":41333,\"start\":41315},{\"end\":41351,\"start\":41333},{\"end\":41745,\"start\":41729},{\"end\":41756,\"start\":41745},{\"end\":42032,\"start\":42025},{\"end\":42042,\"start\":42032},{\"end\":42050,\"start\":42042},{\"end\":42291,\"start\":42274},{\"end\":42309,\"start\":42291},{\"end\":42321,\"start\":42309},{\"end\":42346,\"start\":42321},{\"end\":42362,\"start\":42346},{\"end\":42369,\"start\":42362},{\"end\":42654,\"start\":42630},{\"end\":42668,\"start\":42654},{\"end\":42687,\"start\":42668},{\"end\":42697,\"start\":42687},{\"end\":42705,\"start\":42697},{\"end\":42740,\"start\":42705},{\"end\":42751,\"start\":42740},{\"end\":43072,\"start\":43062},{\"end\":43084,\"start\":43072},{\"end\":43092,\"start\":43084},{\"end\":43304,\"start\":43288},{\"end\":43316,\"start\":43304},{\"end\":43331,\"start\":43316},{\"end\":43529,\"start\":43511},{\"end\":43857,\"start\":43840},{\"end\":43873,\"start\":43857},{\"end\":44108,\"start\":44087},{\"end\":44124,\"start\":44108},{\"end\":44141,\"start\":44124},{\"end\":44160,\"start\":44141},{\"end\":44418,\"start\":44401},{\"end\":44432,\"start\":44418},{\"end\":44449,\"start\":44432},{\"end\":44773,\"start\":44756},{\"end\":45262,\"start\":45248},{\"end\":45279,\"start\":45262},{\"end\":45632,\"start\":45617},{\"end\":45646,\"start\":45632},{\"end\":45663,\"start\":45646},{\"end\":45815,\"start\":45794},{\"end\":45828,\"start\":45815},{\"end\":45841,\"start\":45828},{\"end\":45858,\"start\":45841},{\"end\":45864,\"start\":45858},{\"end\":46186,\"start\":46155},{\"end\":46192,\"start\":46186},{\"end\":46523,\"start\":46507},{\"end\":46540,\"start\":46523},{\"end\":46777,\"start\":46763},{\"end\":46905,\"start\":46889},{\"end\":46919,\"start\":46905},{\"end\":46932,\"start\":46919},{\"end\":46949,\"start\":46932},{\"end\":46962,\"start\":46949},{\"end\":46977,\"start\":46962},{\"end\":46991,\"start\":46977},{\"end\":47003,\"start\":46991},{\"end\":47251,\"start\":47235},{\"end\":47265,\"start\":47251},{\"end\":47278,\"start\":47265},{\"end\":47556,\"start\":47539},{\"end\":47573,\"start\":47556},{\"end\":47588,\"start\":47573},{\"end\":47604,\"start\":47588},{\"end\":47617,\"start\":47604},{\"end\":47631,\"start\":47617},{\"end\":47965,\"start\":47947},{\"end\":47978,\"start\":47965},{\"end\":47983,\"start\":47978},{\"end\":48232,\"start\":48219},{\"end\":48243,\"start\":48232},{\"end\":48255,\"start\":48243},{\"end\":48267,\"start\":48255},{\"end\":48277,\"start\":48267},{\"end\":48288,\"start\":48277},{\"end\":48300,\"start\":48288}]", "bib_venue": "[{\"end\":37626,\"start\":37608},{\"end\":43598,\"start\":43578},{\"end\":44475,\"start\":44457},{\"end\":45352,\"start\":45337},{\"end\":47299,\"start\":47283},{\"end\":36088,\"start\":36068},{\"end\":36440,\"start\":36433},{\"end\":36766,\"start\":36614},{\"end\":37085,\"start\":37053},{\"end\":37298,\"start\":37294},{\"end\":37606,\"start\":37603},{\"end\":37904,\"start\":37845},{\"end\":38253,\"start\":38246},{\"end\":38480,\"start\":38422},{\"end\":38740,\"start\":38735},{\"end\":39026,\"start\":38926},{\"end\":39278,\"start\":39225},{\"end\":39552,\"start\":39545},{\"end\":39876,\"start\":39869},{\"end\":40121,\"start\":40103},{\"end\":40382,\"start\":40379},{\"end\":40748,\"start\":40741},{\"end\":41058,\"start\":40998},{\"end\":41422,\"start\":41367},{\"end\":41727,\"start\":41675},{\"end\":42057,\"start\":42050},{\"end\":42381,\"start\":42369},{\"end\":42754,\"start\":42751},{\"end\":43099,\"start\":43092},{\"end\":43286,\"start\":43237},{\"end\":43576,\"start\":43529},{\"end\":43838,\"start\":43780},{\"end\":44167,\"start\":44160},{\"end\":44455,\"start\":44449},{\"end\":44754,\"start\":44678},{\"end\":44987,\"start\":44980},{\"end\":45335,\"start\":45279},{\"end\":45666,\"start\":45663},{\"end\":45891,\"start\":45864},{\"end\":46199,\"start\":46192},{\"end\":46505,\"start\":46388},{\"end\":46761,\"start\":46717},{\"end\":47007,\"start\":47003},{\"end\":47281,\"start\":47278},{\"end\":47537,\"start\":47440},{\"end\":47990,\"start\":47983},{\"end\":48217,\"start\":48139}]"}}}, "year": 2023, "month": 12, "day": 17}