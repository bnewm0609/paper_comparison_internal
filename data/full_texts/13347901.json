{"id": 13347901, "updated": "2023-11-02 04:20:35.336", "metadata": {"title": "Optimal Transport for Domain Adaptation", "authors": "[{\"first\":\"Nicolas\",\"last\":\"Courty\",\"middle\":[]},{\"first\":\"R'emi\",\"last\":\"Flamary\",\"middle\":[]},{\"first\":\"Devis\",\"last\":\"Tuia\",\"middle\":[]},{\"first\":\"Alain\",\"last\":\"Rakotomamonjy\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2015, "month": 7, "day": 2}, "abstract": "Domain adaptation from one data space (or domain) to another is one of the most challenging tasks of modern data analytics. If the adaptation is done correctly, models built on a specific data space become more robust when confronted to data depicting the same semantic concepts (the classes), but observed by another observation system with its own specificities. Among the many strategies proposed to adapt a domain to another, finding a common representation has shown excellent properties: by finding a common representation for both domains, a single classifier can be effective in both and use labelled samples from the source domain to predict the unlabelled samples of the target domain. In this paper, we propose a regularized unsupervised optimal transportation model to perform the alignment of the representations in the source and target domains. We learn a transportation plan matching both PDFs, which constrains labelled samples in the source domain to remain close during transport. This way, we exploit at the same time the few labeled information in the source and the unlabelled distributions observed in both domains. Experiments in toy and challenging real visual adaptation examples show the interest of the method, that consistently outperforms state of the art approaches.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1507.00504", "mag": "2952851926", "acl": null, "pubmed": "27723579", "pubmedcentral": null, "dblp": "journals/corr/CourtyFTR15", "doi": "10.1109/tpami.2016.2615921"}}, "content": {"source": {"pdf_hash": "8399b1bddd95c3276f3d55a3ec224fab714b679e", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1507.00504v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1507.00504", "status": "GREEN"}}, "grobid": {"id": "40d80747db5c47af5475c79e8dd34e27e416df3c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/8399b1bddd95c3276f3d55a3ec224fab714b679e.txt", "contents": "\nOptimal Transport for Domain Adaptation\n\n\nX \nNo X \nJanuary Xx \nOptimal Transport for Domain Adaptation\n\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE\n1Index Terms-Unsupervised Domain AdaptationOptimal TransportTransfer LearningVisual AdaptationClassification\nDomain adaptation is one of the most challenging tasks of modern data analytics. If the adaptation is done correctly, models built on a specific data representation become more robust when confronted to data depicting the same classes, but described by another observation system. Among the many strategies proposed, finding domain-invariant representations has shown excellent properties, in particular since it allows to train a unique classifier effective in all domains. In this paper, we propose a regularized unsupervised optimal transportation model to perform the alignment of the representations in the source and target domains. We learn a transportation plan matching both PDFs, which constrains labeled samples of the same class in the source domain to remain close during transport. This way, we exploit at the same time the labeled samples in the source and the distributions observed in both domains. Experiments on toy and challenging real visual adaptation examples show the interest of the method, that consistently outperforms state of the art approaches. In addition, numerical experiments show that our approach leads to better performances on domain invariant deep learning features and can be easily adapted to the semi-supervised case where few labeled samples are available in the target domain.\n\nINTRODUCTION\n\nM ODERN data analytics are based on the availability of large volumes of data, sensed by a variety of acquisition devices and at high temporal frequency. But this large amounts of heterogeneous data also make the task of learning semantic concepts more difficult, since the data used for learning a decision function and those used for inference tend not to follow the same distribution. Discrepancies (also known as drift) in data distribution are due to several reasons and are application-dependent. In computer vision, this problem is known as the visual adaptation domain problem, where domain drifts occur when changing lighting conditions, acquisition devices, or by considering the presence or absence of backgrounds. In speech processing, learning from one speaker and trying to deploy an application targeted to a wide public may also be hindered by the differences in background noise, tone or gender of the speaker. In remote sensing image analysis, one would like to leverage from labels defined over one city image to classify the land occupation of another city. The drifts observed in the probability density function (PDF) of remote sensing images are caused by variety of factors: different corrections for atmospheric scattering, daylight conditions at the hour of acquisition or even slight changes in the chemical composition of the materials.\n\nFor those reasons, several works have coped with these drift problems by developing learning methods able to transfer knowledge from a source domain to a target domain for which data have different PDFs. Learning in this PDF discrepancy context is denoted as the domain adaptation problem [37]. In this work, we address the most difficult variant of this problem, denoted as unsupervised domain adaptation, where data labels are only available in the source domain. We tackle this problem by assuming that the effects of the drifts can be reduced if data undergo a phase of adaptation (typically, a non-linear mapping) where both domains look more alike.\n\nSeveral theoretical works [2], [36], [22] have emphasized the role played by the divergence between the data probability distribution functions of the domains. These works have led to a principled way of solving the domain adaptation problem: transform data so as to make their distributions \"closer\", and use the label information available in the source domain to learn a classifier in the transformed domain, which can be applied to the target domain. Our work follows the same intuition and proposes a transformation of the source data that fits a least effort principle, i.e. an effect that is minimal with respect to a transformation cost or metric. In this sense, the adaptation problem boils down to: i) finding a transformation of the input data matching the source and target distributions and then ii) learning a new classifier from the transformed source samples. This process is depicted in Figure 1. In this paper, we advocate a solution for finding this transformation based on optimal transport.\n\nOptimal Transport (OT) problems have recently raised interest in several fields, in particular because OT theory can be used for computing distances between probability distributions. Those  tions without having to smoothen them using nonparametric or semi-parametric approaches; ii) By exploiting the geometry of the underlying metric space, they provide meaningful distances even when the supports of the distributions do not overlap. Leveraging from these properties, we introduce a novel framework for unsupervised domain adaptation, which consists in learning an optimal transportation based on empirical observations. In addition, we propose several regularization terms that favor learning of better transformations w.r.t. the adaptation problem. They can either encode class information contained in the source domain or promote the preservation of neighborhood structures. An efficient algorithm is proposed for solving the resulting regularized optimal transport optimization problem. Finally, this framework can also easily be extended to the semisupervised case, where few labels are available in the target domain, by a simple and elegant modification in the optimal transport optimization problem.\n\nThe remainder of this Section presents related works, while Section 2 formalizes the problem of unsupervised domain adaptation and discusses the use of optimal transport for its resolution. Section 3 introduces optimal transport and its regularized version. Section 4 presents the proposed regularization terms tailored to fit the domain adaptation constraints. Section 5 discusses algorithms for solving the regularized optimal transport problem efficiently. Section 6 evaluates the relevance of our domain adaptation framework through both synthetic and real-world examples.\n\n\nRelated works\n\nDomain adaptation. Domain adaptation strategies can be roughly divided in two families, depending on whether they assume the presence of few labels in the target domain (semi-supervised DA) or not (unsupervised DA).\n\nIn the first family, methods which have been proposed include searching for projections that are discriminative in both domains by using inner products between source samples and transformed target samples [42], [32], [29]. Learning projections, for which labeled samples of the target domain fall on the correct side of a large margin classifier trained on the source data, have also been proposed [27]. Several works based on extraction of common features under pairwise constraints have also been introduced as domain adaptation strategies [26], [52], [47].\n\nThe second family tackles the domain adaptation problem assuming, as in this paper, that no labels are available in the target domain. Besides works dealing with sample reweighting [46], many works have considered finding a common feature representation for the two (or more) domains. Since the representation, or latent space, is common to all domains, projected labeled samples from the source domain can be used to train a classifier that is general [18], [38]. A common strategy is to propose methods that aim at finding representations in which domains match in some sense. For instance, adaptation can be performed by matching the means of the domains in the feature space [38], aligning the domains by their correlations [33] or by using pairwise constraints [51]. In most of these works, feature extraction is the key tool for finding a common latent space that embeds discriminative information shared by all domains.\n\nRecently, the unsupervised domain adaptation problem has been revisited by considering strategies based on a gradual alignment of a feature representation. In [24], authors start from the hypothesis that domain adaptation can be better estimated when comparing gradual distortions. Therefore, they use intermediary projections of both domains along the Grassmannian geodesic connecting the source and target eigenvectors. In [23], [54], all sets of transformed intermediary domains are obtained by using a geodesic-flow kernel. While these methods have the advantage of providing easily computable outof-sample extensions (by projecting unseen samples onto the latent space eigenvectors), the transformation defined remains global and is applied in the same way to the whole target domain. An approach combining sample reweighting logic with representation transfer is found in [53], where authors extend the sample re-weighing to reproducing kernel Hilbert space through the use of surrogate kernels. The transformation achieved is again a global linear transformation that helps in aligning domains.\n\nOur proposition strongly differs from those reviewed above, as it defines a local transformation for each sample in the source domain. In this sense, the domain adaptation problem can be seen as a graph matching problem [35], [10], [11] as each source sample has to be mapped on target samples under the constraint of marginal distribution preservation. Optimal Transport and Machine Learning. The optimal transport problem has first been introduced by the French mathematician Gaspard Monge in the middle of the 19th century as a way to find a minimal effort solution to the transport of a given mass of dirt into a given hole. The problem reappeared in the middle of the 20th century in the work of Kantorovitch [30] and found recently surprising new developments as a polyvalent tool for several fundamental problems [49]. It was applied in a wide panel of fields, including computational fluid mechanics [3], color transfer between multiple images or morphing in the context of image processing [40], [20], [5], interpolation schemes in computer graphics [6], and economics, via matching and equilibriums problems [12].\n\nDespite the appealing properties and application success stories, the machine learning community has considered optimal transport only recently (see, for instance, works considering the computation of distances between histograms [15] or label propagation in graphs [45]); the main reason being the high computational cost induced by the computation of the optimal transportation plan. However, new computing strategies have emerged [15], [17], [5] and made possible the application of OT distances in operational settings.\n\n\nOPTIMAL TRANSPORT AND APPLICATION\n\n\nTO DOMAIN ADAPTATION\n\nIn this section, we present the general unsupervised domain adaptation problem and show how it can be addressed from an optimal transport perspective.\n\n\nProblem and theoretical motivations\n\nLet \u2126 \u2208 R d be an input measurable space of dimension d and C the set of possible labels. P(\u2126) denotes the set of all probability measures over \u2126. The standard learning paradigm assumes the existence of a set of training data X s = {x s i } Ns i=1 associated with a set of class labels Y s = {y s i } Ns i=1 , with y s i \u2208 C, and a testing set X t = {x t i } Nt i=1 with unknown labels. In order to infer the set of labels Y t associated with X t , one usually relies on an empirical estimate of the joint probability distribution P(x, y) \u2208 P(\u2126 \u00d7 C) from (X s , Y s ), and assumes that X s and X t are drawn from the same distribution P(x) \u2208 P(\u2126).\n\n\nDomain adaptation as a transportation problem\n\nIn domain adaptation problems, one assumes the existence of two distinct joint probability distributions P s (x s , y) and P t (x t , y), respectively related to a source and a target domains, noted as \u2126 s and \u2126 t . In the following, \u00b5 s and \u00b5 t are their respective marginal distributions over X. We also denote f s and f t the true labeling functions, i.e. the Bayes decision functions in each domain.\n\nAt least one of the two following assumptions is generally made by most domain adaptation methods:\n\n\u2022 Class imbalance: Label distributions are different in the two domains (P s (y) = P t (y)), but the conditional distributions of the samples with respect to the labels are the same (P s (x s |y) = P t (x t |y)); \u2022 Covariate shift: Conditional distributions of the labels with respect to the data are equal (P s (y|x s ) = P t (y|x t ), or equivalently f s = f t = f ). However, data distributions in the two domains are supposed to be different (P s (x s ) = P t (x t )). For the adaptation techniques to be effective, this difference needs to be small [2]. In real world applications, the drift occurring between the source and the target domains generally implies a change in both marginal and conditional distributions.\n\nIn our work, we assume that the domain drift is due to an unknown, possibly nonlinear transformation of the input space T : \u2126 s \u2192 \u2126 t . This transformation may have a physical interpretation (e.g. change in the acquisition conditions, sensor drifts, thermal noise, etc.). It can also be directly caused by the unknown process that generates the data. Additionnally, we also suppose that the transformation preserves the conditional distribution, i.e. P s (y|x s ) = P t (y|T(x s )).\n\nThis means that the label information is preserved by the transformation, and the Bayes decision functions are tied through the equation f t (T(x)) = f s (x).\n\nAnother insight can be provided regarding the transformation T. From a probabilistic point of view, T transforms the measure \u00b5 in its image measure, noted T#\u00b5, which is another probability measure over \u2126 t satisfying\nT#\u00b5(x) = \u00b5(T \u22121 (x)), \u2200x \u2208 \u2126 t (1)\nT is said to be a transport map or push-forward from \u00b5 s to \u00b5 t if T#\u00b5 s = \u00b5 t (as illustrated in Figure 2.a). Under this assumption, X t are drawn from the same PDF as T#\u00b5 s . This provides a principled way to solve the adaptation problem: 1) Estimate \u00b5 s and \u00b5 t from X s and X t (Equation (6)) 2) Find a transport map T from \u00b5 s to \u00b5 t 3) Use T to transport labeled samples X s and train a classifier from them. Searching for T in the space of all possible transformations is intractable, and some restrictions need to be imposed. Here, we propose that T should be chosen so as to minimize a transportation cost C(T) expressed as:\nC(T) = \u2126s c(x, T(x))d\u00b5(x),(2)\nwhere the cost function c : \u2126 s \u00d7 \u2126 t \u2192 R + is a distance function over the metric space \u2126. C(T) can be interpreted as the energy required to move a probability mass \u00b5(x) from x to T(x). The problem of finding such a transportation of minimal cost has already been investigated in the literature. For instance, the optimal transportation problem as defined by Monge is the solution of the following minimization problem:\nT 0 = argmin T \u2126s c(x, T(x))d\u00b5(x), s.t. T#\u00b5 s = \u00b5 t (\n3) The Kantorovitch formulation of the optimal transportation [30] is a convex relaxation of the above Monge problem. Indeed, let us define \u03a0 as the set of all probabilistic couplings \u2208 P(\u2126 s \u00d7\u2126 t ) with marginals \u00b5 s and \u00b5 t . The Kantorovitch problem seeks for a general coupling \u03b3 \u2208 \u03a0 between \u2126 s and \u2126 t :\n\u03b3 0 = argmin \u03b3\u2208\u03a0 \u2126s\u00d7\u2126t c(x s , x t )d\u03b3(x s , x t )(4)\nIn this formulation, \u03b3 can be understood as a joint probability measure with marginals \u00b5 s and \u00b5 t as depicted in Figure 2.b. \u03b3 0 is also known as transportation plan [43]. It allows to define the Wasserstein distance of order p between \u00b5 s and \u00b5 t . This distance is formalized as\nW p (\u00b5 s , \u00b5 t ) def = inf \u03b3\u2208\u03a0 \u2126s\u00d7\u2126t d(x s , x t ) p d\u03b3(x s , x t ) 1 p = inf \u03b3\u2208\u03a0 E x s \u223c\u00b5s,x t \u223c\u00b5t d(x s , x t ) p 1 p(5)\nwhere d is a distance and the corresponding cost function c(x s , x t ) = d(x s , x t ) p . The Wasserstein distance is also known as the Earth Mover Distance in the computer vision community [41] and it defines a metric over the space of integrable squared probability measures.\n\nIn the remainder, we consider the squared 2 Euclidean distance as a cost function, c(x, y) = x \u2212 y 2 2 for computing optimal transportation. As a consequence, we evaluate distances between measures according to the squared Wasserstein distance W 2 2 associated with the Euclidean distance d(x, y) = x \u2212 y 2 . The main rationale for this choice is that it experimentally provided the best result on average (as shown in the supplementary material). Nevertheless, other cost functions better suited to the nature of specific data can be considered, depending on the application at hand and the data representation, as discussed more in details in Section 3.4.\n\n\nREGULARIZED\n\n\nDISCRETE OPTIMAL TRANSPORT\n\nThis section discusses the problem of optimal transport for domain adaptation. In the first part, we introduce the OT optimization problem on discrete empirical distributions. Then, we discuss a regularized variant of this discrete optimal transport problem. Finally, we address the question of how the resulting probabilistic coupling can be used for mapping samples from source to target domain.\n\n\nDiscrete optimal transport\n\nWhen \u00b5 s and \u00b5 t are only accessible through discrete samples, the corresponding empirical distributions can be written as\n\u00b5 s = ns i=1 p s i \u03b4 x s i , \u00b5 t = nt i=1 p t i \u03b4 x t i (6)\nwhere \u03b4 xi is the Dirac function at location x i \u2208 R d . p s i and p t i are probability masses associated to the i-th sample and belong to the probability simplex,\ni.e. ns i=1 p s i = nt i=1 p t i = 1.\nIt is straightforward to adapt the Kantorovich formulation of optimal transport problem to the discrete case. We denote B the set of probabilistic couplings between the two empirical distributions defined as:\nB = \u03b3 \u2208 (R + ) ns\u00d7nt | \u03b31 nt = \u00b5 s , \u03b3 T 1 ns = \u00b5 t(7)\nwhere 1 d is a d-dimensional vector of ones. The Kantorovitch formulation of the optimal transport [30] reads:\n\u03b3 0 = argmin \u03b3\u2208B \u03b3, C F(8)\nwhere ., . F is the Frobenius dot product and C \u2265 0 is the cost function matrix, whose term C(i, j) = c(x s i , x t j ) denotes the cost to move a probability mass from x s i to x t j . As previously detailed, this cost was chosen as the squared Euclidean distance between the two locations, i.e. C(i, j) = ||x s i \u2212 x t j || 2 2 . Note that when n s = n t = n and \u2200i, j p s i = p t j = 1/n, \u03b3 0 is simply a permutation matrix. In this case, the optimal transport problem boils down to an optimal assignment problem. In the general case, it can be shown that \u03b3 0 is a sparse matrix with at most n s + n t \u2212 1 non zero entries, equating the rank of the constraint matrix expressing the two marginal constraints.\n\nProblem (8) is a linear program and can be solved with combinatorial algorithms such as the simplex methods and its network variants (successive shortest path algorithms, Hungarian or relaxation algorithms). Yet, the computational complexity was shown to be O((n s + n t )n s n t log(n s + n t )) [1, p. 472, Th. 12.2] at best, which dampens the utility of the method when handling large datasets. However, the regularization scheme recently proposed by Cuturi [15] presented in the next section, allows a very fast computation of a transportation plan.\n\n\nRegularized optimal transport\n\nRegularization is a classical approach used for preventing overfitting when few samples are available for learning. It can also be used for inducing some properties on the solution. In the following, we discuss a regularization term recently introduced for optimal transport problem.\n\nCuturi [15] proposed to regularize the expression of the optimal transport problem by the entropy of the probabilistic coupling. The resulting informationtheoretic regularized version of the transport \u03b3 \u03bb 0 is the solution of the minimization problem:\n\u03b3 \u03bb 0 = argmin \u03b3\u2208B \u03b3, C F + \u03bb\u2126 s (\u03b3),(9)\nwhere \u2126 s (\u03b3) = i,j \u03b3(i, j) log \u03b3(i, j) computes the negentropy of \u03b3. The intuition behind this form of regularization is the following: since most elements of \u03b3 0 should be zero with high probability, one can look for a smoother version of the transport, thus lowering its sparsity, by increasing its entropy. As a result, the optimal transport \u03b3 \u03bb 0 will have a denser coupling between the distributions. \u2126 s (\u00b7) can also be interpreted as a Kullback-Leibler divergence KL(\u03b3 \u03b3 u ) between the joint probability \u03b3 and a uniform joint probability \u03b3 u (i, j) = 1 nsnt . Indeed, by expanding this KL divergence, we have KL(\u03b3 \u03b3 u ) = log n s n t + i,j \u03b3(i, j) log \u03b3(i, j). The first term is a constant w.r.t. \u03b3, which means that we can equivalently\nuse KL(\u03b3 \u03b3 u ) or \u2126 s (\u03b3) = i,j \u03b3(i, j) log \u03b3(i, j) in Equation (9).\nHence, as the parameter \u03bb weighting the entropybased regularization increases, the sparsity of \u03b3 \u03bb 0 decreases and source points tend to distribute their probability masses toward more target points. When \u03bb becomes very large (\u03bb \u2192 \u221e), the OT solution of Equation (9) converges toward \u03b3 \u03bb 0 (i, j) \u2192 1 nsnt , \u2200i, j. Another appealing outcome of the regularized OT formulation given in Equation (9) is the derivation of a computationally efficient algorithm based on Sinkhorn-Knopp's scaling matrix approach [31]. This efficient algorithm will also be a key element in our methodology presented in Section 4.\n\n\nOT-based mapping of the samples\n\nIn the context of domain adaptation, once the probabilistic coupling \u03b3 0 has been computed, source samples have to be transported in the target domain. For this purpose, one can interpolate the two distributions \u00b5 s and \u00b5 t by following the geodesics of the Wasserstein metric [49,Chapter 7], parameterized by t \u2208 [0, 1]. This defines a new distribution\u03bc such that:\n\u00b5 = argmin \u00b5 (1 \u2212 t)W 2 (\u00b5 s , \u00b5) 2 + tW 2 (\u00b5 t , \u00b5) 2 . (10)\nStill following Villani's book, one can show that for a squared 2 cost, this distribution boils down to:\n\u00b5 = i,j \u03b3 0 (i, j)\u03b4 (1\u2212t)x s i +tx t j .(11)\nSince our goal is to transport the source samples onto the target distribution, we are mainly interested in the case t = 1. For this value of t, the novel distribution \u00b5 is a distribution with the same support of \u00b5 t , since Equation (11) \nreduces t\u00f4 \u00b5 = jp t j \u03b4 x t j .(12)\nwithp t j = i \u03b3 0 (i, j). The weightsp t j can be seen as the sum of probability mass coming from all samples {x s i } that is transferred to sample x t j . Alternatively, \u03b3 0 (i, j) also tells us how much probability mass of x s i is transferred to x t j . We can exploit this information to compute a transformation of the source samples. This transformation can be conveniently expressed with respect to the target samples as the following barycentric mapping:\nx s i = argmin x\u2208R d j \u03b3 0 (i, j)c(x, x t j ).(13)\nwhere x s i is a given source sample and x s i is its corresponding image. When the cost function is the squared 2 distance, this barycenter corresponds to a weighted average and the sample is mapped into the convex hull of the target samples. For all source samples, this barycentric mapping can therefore be expressed as:\nX s = T \u03b3 0 (X s ) = diag(\u03b3 0 1 nt ) \u22121 \u03b3 0 X t .(14)\nThe inverse mapping from the target to the source domain can also be easily computed from \u03b3 T 0 . Interestingly, one can show [17,Eq. 8] that this transformation is a first order approximation of the true n s Wasserstein barycenters of the target distributions. Also note that when marginals \u00b5 s and \u00b5 t are uniform, one can easily derive the barycentric mapping as a linear expression:\nX s = n s \u03b3 0 X t andX t = n t \u03b3 0 X s(15)\nfor the source and target samples. Finally, remark that if \u03b3 0 (i, j) = 1 nsnt , \u2200i, j, then each transported source point converges toward the center of mass of the target distribution that is 1 nt j x t j . This occurs when \u03bb \u2192 \u221e in Equation (9).\n\n\nDiscussing optimal transport for domain adaptation\n\nWe discuss here the requirements and conditions of applicability of the proposed method. Guarantees of recovery of the correct transformation. Our goal for achieving domain adaptation is to uncover the transformation that occurred between source and target distributions. While the family of transformation that an OT formulation can recover is wide, we provide a proof that, for some simple affine transformations of discrete distributions, our OT solution is able to match source and target examples exactly.\n\nTheorem 3.1: Let \u00b5 s and \u00b5 t be two discrete distributions with n Diracs as defined in Equation (6). If the following conditions hold 1) The source samples in \u00b5 s are x s i \u2208 R d , \u2200i \u2208 1, . . . , n such that x s i = x s j if i = j . 2) All weights in the source and target distributions are 1 n .\n\n3) The target samples are defined as x t i = Ax s i + b i.e. an affine tranformation of the source samples. 4) b \u2208 R d and A \u2208 S + is a strictly positive definite matrix. 5) The cost function is c(x s , x t ) = x s \u2212 x t 2 2 . then the solution T 0 of the optimal transport problem (8) is so that T 0 (x s i ) = Ax s i + b = x t i \u2200i \u2208 1, . . . , n. In this case, we retrieve the exact affine transformation on the discrete samples, which means that the label information are fully preserved during transportation. Therefore, one can train a classifier on the mapped samples with no generalization loss. We provide a simple demonstration in the supplementary material.\n\nChoosing the cost function. In this work, we have mainly considered a 2 -based cost function. Let us now discuss the implication of using a different cost function in our framework. A number of norm-based distances have been investigated by mathematicians [49, p 972]. Other types of metrics can also be considered, such as Riemannian distances over a manifold [49, Part II], or learnt metrics [16]. Concave cost functions are also of particular use in real life problems [21]. Each different cost function will lead to a different OT plan \u03b3 0 , but the cost itself does not impact the OT optimization problem, i.e. the solver is independent from the cost function. Nonetheless, since c(\u00b7, \u00b7) defines the Wasserstein geodesic, the interpolation between domains defined in Equation (10) leads to a different trajectory (potentially nonunique). Equation (11), which corresponds to c(\u00b7, \u00b7), is a squared 2 distance, so it does not hold anymore. Nevertheless, the solution of (10) for t = 1 does not depend on the cost c and one can still use the proposed barycentric mapping (13). For instance if the cost function is based on the 1 norm, the transported samples will be estimated using a component-wise weighted median. Unfortunately, for more complex cost functions, the barycentric mapping might be complex to estimate.\n\n\nCLASS-REGULARIZATION FOR DOMAIN\n\n\nADAPTATION\n\nIn this section we explore regularization terms that preserve label information and sample neighborhood during transportation. Finally, we discuss the semisupervised case and show that label information in the target domain can be effectively included in he proposed model.\n\n\nRegularizing the transport with class labels\n\nOptimal transport, as it has been presented in the previous section, does not use any class information. However, and even if our goal is unsupervised domain adaptation, class labels are available in the source domain. This information is typically used only during the decision function learning stage, which follows the adaptation step. Our proposition is to take advantage of the label information for estimating a better transport. More precisely, we aim at penalizing couplings that match source samples with different labels to same target samples.\n\nTo this end, we propose to add a new term to the regularized optimal transport, leading to the following optimization problem:\nmin \u03b3\u2208B \u03b3, C F + \u03bb\u2126 s (\u03b3) + \u03b7\u2126 c (\u03b3),(16)\nwhere \u03b7 \u2265 0 and \u2126 c (\u00b7) is a class-based regularization term.\n\nIn this work, we propose and study two choices for this regularizer \u2126 c (\u00b7). The first is based on group sparsity and promotes a probabilistic coupling \u03b3 0 where a given target sample receives masses from source samples which have same labels. The second is based on graph Laplacian regularization and promotes a locally smooth and class-regular structure in the source transported samples.\n\n\nRegularization with group-sparsity\n\nWith the first regularizer, our objective is to exploit label information in the optimal transport computation. We suppose that all samples in the source domain have labels. The main intuition underlying the use of this group-sparse regularizer is that we would like each target sample to receive masses only from source samples that have the same label. As a consequence, we expect that a given target sample will be involved in the representation of transported source samples as defined in Equation (14), but only for samples from the source domain of the same class. This behaviour can be induced by means of a group-sparse penalty on the columns of \u03b3. This approach has been introduced in our preliminary work [14]. In that paper, we proposed a p \u2212 1 regularization term with p < 1 (mainly for algorithmic reasons). When applying a majoration-minimization technique on the p \u2212 1 norm, the problem can be cast as problem (9) and can be solved using the efficient Sinkhorn-Knopp algorithm at each iteration. However, this regularization term with p < 1 is non-convex and thus the proposed algorithm is guaranteed to converge only to local stationary points.\nx s \u00b5 t (T(x s )) \u00b5 s (x s ) T(x s ) \u2326 s \u2326 t (a) \u2326 t \u2326 s (b) (c)\nIn this paper, we retain the convexity of the underlying problem and use the convex group-lasso regularizer 1 \u2212 2 instead. This regularizer is defined as\n\u2126 c (\u03b3) = j cl ||\u03b3(I cl , j)|| 2 ,(17)\nwhere || \u00b7 || 2 denotes the 2 norm and I cl contains the indices of rows in \u03b3 related to source domain samples of class cl. Hence, \u03b3(I cl , j) is a vector containing coefficients of the jth column of \u03b3 associated to class cl. Since the jth column of \u03b3 is related to the jth target sample, this regularizer will induce the desired sparse representation in the target sample. Among other benefits, the convexity of the corresponding problem allows to use an efficient generic optimization scheme, presented in Section 5. Ideally, with this regularizer we expect that the masses corresponding to each group of labels are matching samples of the source and target domains exclusively. Hence, for the domain adaptation problem to have a relevant solution, the distributions of labels are expected to be preserved in both the source and target distributions. We thus need to have P s (y) = P t (y). This assumption, which is a classical assumption in the field of learning, is nevertheless a mild requirement since, in practice, small deviations of proportions do not prevent the method from working (see reference [48] for experimental results on this particular issue).\n\n\nLaplacian regularization\n\nThis regularization term aims at preserving the data structure -approximated by a graph -during transport [20], [13]. Intuitively, we would like similar samples in the source domain to also be similar after transportation. Hence, denote asx s i the transported source sample x s i , withx s i being linearly dependent on the transportation matrix \u03b3 through Equation (14). Now, given a positive symmetric similarity matrix S s of samples in the source domain, our regularization term is defined as\n\u2126 c (\u03b3) = 1 N 2 s i,j S s (i, j) x s i \u2212x s j 2 2 ,(18)\nwhere S s (i, j) \u2265 0 are the coefficients of matrix S s \u2208 R Ns\u00d7Ns that encodes similarity between pairs of source sample. In order to further preserve class structures, we can sparsify similarities for samples of different classes. In practice, we thus impose S s (i, j) = 0 if y s i = y s j . The above equation can be simplified when the marginal distributions are uniform. In that case, transported source samples can be computed according to Equation (15). Hence, \u2126 c (\u03b3) boils down to\n\u2126 c (\u03b3) = Tr(X t \u03b3 L s \u03b3X t ),(19)\nwhere L s = diag(S s 1) \u2212 S s is the Laplacian of the graph S s . The regularizer is therefore quadratic w.r.t. \u03b3. The regularization terms (18) or (19) are defined based on the transported source samples. When a similarity information is also available in the target samples, for instance, through a similarity matrix S t , we can take advantage of this knowledge and a symmetric Laplacian regularization of the form\n\u2126 c (\u03b3) = (1 \u2212 \u03b1)Tr(X t \u03b3 L s \u03b3X t ) + \u03b1Tr(X s \u03b3L t \u03b3 X s )(20)\ncan be used instead. In the above equation L t = diag(S t 1) \u2212 S t is the Laplacian of the graph in the target domain and 0 \u2264 \u03b1 \u2264 1 is a trade-off parameter that weights the importance of each part of the regularization term. Note that, unlike the matrix S s , the similarity matrix S t cannot be sparsified according to the class structure, since labels are generally not available for the target domain. A regularization term similar to \u2126 c (\u03b3) has been proposed in [20] for histogram adaptation between images. However, the authors focused on displacements (x s i \u2212 x s i ) instead of on preserving the class structure of the transported samples.\n\n\nRegularizing for semi-supervised domain adaptation\n\nIn semi-supervised domain adaptation, few labelled samples are available in the target domain [50]. Again, such an important information can be exploited by means of a novel regularization term to be integrated in the original optimal transport formulation. This regularization term is designed such that samples in the target domain should only be matched with samples in the source domain that have the same labels. It can be expressed as:\n\u2126 semi (\u03b3) = \u03b3, M(21)\nwhere M is a n s \u00d7 n t cost matrix, with M(i, j) = 0 whenever y s i = y t j (or j is a sample with unknown label) and +\u221e otherwise. This term has the benefit to be parameter free. It boils down to changing the original cost function C, defined in Equation (8), by adding an infinite cost to undesired matches. Smooth versions of this regularization can be devised, for instance, by using a probabilistic confidence of target sample x t j to belong to class y t j . Though appealing, we have not explored this latter option in this work. It is also noticeable that the Laplacian strategy in Equation (20) can also leverage on these class labels in the target domain through the definition of matrix S t .\n\n\nGENERALIZED CONDITIONAL GRADIENT FOR SOLVING REGULARIZED OT PROBLEMS\n\nIn this section, we discuss an efficient algorithm for solving optimization problem (16), that can be used with any of the proposed regularizers.\n\nFirstly, we characterize the existence of a solution to the problem. We remark that regularizers given in Equations (17) and (18) are continuous, thus the objective function is continuous. Moreover, since the constraint set B is a convex, closed and bounded (hence compact) subset of R d , the objective function reaches its minimum on B. In addition, if the regularizer is strictly convex that minimum is unique. This occurs for instance, for the Laplacian regularization in Equation (18). Now, let us discuss algorithms for computing optimal transport solution of problem (16). For solving a similar problem with a Laplacian regularization term, Ferradans et al. [20] used a conditional gradient (CG) algorithm [4]. This approach is appealing and could be extended to our problem. It is an iterative scheme that guarantees any iterate to belong to B, meaning that any of those iterates is a transportation plan. At each of these iterations, in order to find a feasible search direction, a CG algorithm looks for a minimizer of the objective function's linear approximation . Hence, at each iteration it solves a Linear Program (LP) that is presumably easier to handle than the original regularized optimal transport problem. Nevertheless, and despite existence of efficient LP solvers such as CPLEX or MOSEK, the dimensionality of the LP problem makes this LP problem hardly tractable, since it involves n s \u00d7 n t variables.\n\nIn this work, we aim for a more scalable algorithm. To this end, we consider an approach based on a generalization of the conditional gradient algorithm [7] denoted as generalized conditional gradient (GCG).\n\nThe framework of the GCG algorithm addresses the general case of constrained minimization of composite functions defined as\nmin \u03b3\u2208B f (\u03b3) + g(\u03b3),(22)\nwhere f (\u00b7) is a differentiable and possibly non-convex function; g(\u00b7) is a convex, possibly non-differentiable function; B denotes any convex and compact subset of R n . As illustrated in Algorithm 1, all the steps of the GCG algorithm are exactly the same as those used for CG, except for the search direction part (Line 3). The difference is that GCG linearizes only part f (\u00b7) of the composite objective function, instead of the full objective function. This approach is justified when the resulting nonlinear optimization problem can be efficiently solved. The GCG algorithm has been shown by Bredies et al. [8] to converge towards a stationary point of Problem (22). In our case, since g(\u03b3) is differentiable, stronger convergence results can be provided (see supplementary material for a discussion on convergence rate and duality gap monitoring). More specifically, for problem (16) we can set f (\u03b3) = \u03b3, C F + \u03b7\u2126 c (\u03b3) and g(\u03b3) = \u03bb\u2126 s (\u03b3).\n\nSupposing now that \u2126 c (\u03b3) is differentiable, step 3 of Algorithm 1 boils down to\n\u03b3 = argmin \u03b3\u2208B \u03b3, C + \u03b7\u2207\u2126 c (\u03b3 k ) F + \u03bb\u2126 s (\u03b3)\nInterestingly, this problem is an entropy-regularized optimal transport problem similar to Problem (9) and Algorithm 1 Generalized Conditional Gradient 1: Initialize k = 0 and \u03b3 0 \u2208 P 2: repeat 3: With G \u2208 \u2207f (\u03b3 k ), solve \u03b3 = argmin \u03b3\u2208B \u03b3, G F + g(\u03b3)\n\n4:\n\nFind the optimal step \u03b1 k\n\u03b1 k = argmin 0\u2264\u03b1\u22641 f (\u03b3 k + \u03b1\u2206\u03b3) + g(\u03b3 k + \u03b1\u2206\u03b3)\nwith \u2206\u03b3 = \u03b3 * \u2212 \u03b3 k\n\n\n5:\n\n\u03b3 k+1 \u2190 \u03b3 k + \u03b1 k \u2206\u03b3, set k \u2190 k + 1 6: until Convergence can be efficiently solved using the Sinkhorn-Knopp scaling matrix approach.\n\nIn our optimal transport problem, \u2126 c (\u03b3) is instantiated by the Laplacian or the group-lasso regularization term. The former is differentiable whereas the group-lasso is not when there exists a class cl and an index j for which \u03b3(I cl , j) is a vector of 0. However, one can note that if the iterate \u03b3 k is so that \u03b3 k (I cl , j) = 0 \u2200cl, \u2200j, then the same property holds for \u03b3 k+1 . This is due to the exponentiation occurring in the Sinkhorn-Knopp algorithm used for the entropy-regularized optimal transport problem. This means that if we initialize \u03b3 0 so that \u03b3 0 (I cl , j) = 0, then \u2126 c (\u03b3 k ) is always differentiable. Hence, our GCG algorithm can also be applied to the group-lasso regularization, despite its non-differentiability in 0.\n\n\nNUMERICAL EXPERIMENTS\n\nIn this section, we study the behavior of four different versions of optimal transport applied to DA problem. In the rest of the section, OT-exact is the original transport problem (8), OT-IT the Information theoretic regularized one (9), and the two proposed class-based regularized ones are denoted OT-GL and OT-Laplace, corresponding respectively to the grouplasso (Equation (17)) and Laplacian (Equation (18)) regularization terms. We also present some results with our previous class-label based regularizer built upon an p \u2212 1 norm: OT-LpL1 [14].\n\n\nTwo moons: simulated problem with controllable complexity\n\nIn the first experiment, we consider the same toy example as in [22]. The simulated dataset consists of two domains: for the source, the standard two entangled moons data, where each moon is associated to a specific class (See Figure 3(a)). The target domain is built by applying a rotation to the two moons, which allows to consider an adaptation problem with an increasing difficulty as a function of the rotation angle. This example is notably interesting because  the corresponding problem is clearly non-linear, and because the input dimensionality is small, 2, which leads to poor performances when applying methods based on subspace alignment (e.g. [23], [34]).\n\nWe follow the same experimental protocol as in [22], thus allowing for a direct comparison with the stateof-the-art results presented therein. The source domain is composed of two moons of 150 samples each. The target domain is also sampled from these two shapes, with the same number of examples. Then, the generalization capability of our method is tested over a set of 1000 samples that follow the same distribution as the target domain. The experiments are conducted 10 times, and we consider the mean classification error as comparison criterion. As a classifier, we used a SVM with a Gaussian kernel, whose parameters were set by 5-fold cross-validation. We compare the adaptation results with two state-of-the-art methods: the DA-SVM approach [9] and the more recent PBDA [22], which has proved to provide competitive results over this dataset.\n\nResults are reported in Table 1. Our first observation is that all the methods based on optimal transport behave better than the state-of-the-art methods, in particular for low rotation angles, where results indicate that the geometrical structure is better preserved through the adaptation by optimal transport. Also, for large angle (e.g. 90 \u2022 ), the final score is also significantly better than other state-of-the-art method, but falls down to a 0.5 error rate, which is natural since in this configuration a transformation of \u221290 \u2022 , implying an inversion of labels, would have led to similar empirical distributions. This clearly shows the capacity of our method to handle large domain transformations. Adding the class-label information into the regularization also clearly helps for the mid-range angle values, where the adaptation shows nearly optimal results up to angles < 40 \u2022 . For the strongest deformation (> 70 \u2022 rotation), no clear winner among the OT methods can be found. We think that, regardless of the amount and type of regularization chosen, the classification of test samples becomes too much tributary of the training samples. These ones mostly come from the denser part of \u00b5 s and as a consequence, the less dense parts of this PDF are not satisfactorily transported. This behavior can be seen in Figure 3d. Fig. 3: Illustration of the classification decision boundary produced by OT-Laplace over the two moons example for increasing rotation angles. The source domain is represented as coloured points. The target domain is depicted as points in grey (best viewed with colors).\n(a) source domain (b) rotation=20 \u2022 (c) rotation=40 \u2022 (d) rotation=90 \u2022\n\nVisual adaptation datasets\n\nWe now evaluate our method on three challenging real world vision adaptation tasks, which have attracted a lot of interest in recent computer vision literature [39]. We start by presenting the datasets, then the experimental protocol, and finish by providing and discussing the results obtained.\n\n\nDatasets\n\nThree types of image recognition problems are considered: digits, faces and miscellaneous objects recognition. This choice of datasets was already featured in [34]. A summary of the properties of each domain considered in the three problems is provided in Table 2. An illustration of some examples of the different domains for a particular class is shown in Figure 4. Digit recognition. As source and target domains, we use the two digits datasets USPS and MNIST, that share 10 classes of digits (single digits 0 \u2212 9). We randomly sampled 1, 800 and 2, 000 images from each original dataset. The MNIST images are resized to the same resolution as that of USPS (16 \u00d7 16). The grey levels of all images are then normalized to obtain a final common feature space for both domains. . This allows to define 12 different adaptation problems with increasing difficulty (the most challenging being the adaptation from right to left poses). Let us note that each domain has a strong variability for each class due to illumination and expression variations. Object recognition. We used the Caltech-Office dataset [42], [24], [23], [54], [39].   [25], Webcam (images taken from a webcam) and DSLR (images taken from a high resolution digital SLR camera). The variability of the different domains come from several factors: presence/absence of background, lightning conditions, noise, etc. We consider two feature sets:\n\n\u2022 SURF descriptors as described in [42], used to transform each image into a 800 bins histogram. These histograms are subsequently normalized and reduced to standard scores. \u2022 two DeCAF deep learning features sets [19]: these features are extracted as the sparse activation of the neurons from the fully connected 6th and 7th layers of a convolutional network trained on imageNet and then fine tuned on the visual recognition tasks considered here. As such, they form vectors with 4096 dimensions.\n\n\nExperimental setup\n\nFollowing [23], the classification is conducted using a 1-Nearest Neighbor (1NN) classifier, which has the advantage of being parameter free. In all experiments, 1NN is trained with the adapted source data, and evaluated over the target data to provide a classification accuracy score. We compare our optimal transport solutions to the following baseline methods that are particularly well adapted for image classification:\n\n\u2022 1NN is the original classifier without adaptation and constitutes a baseline for all experiments; Fig. 4: Examples from the datasets used in the visual adaptation experiment. 5 random samples from one class are given for all the considered domains.\n\n\u2022 PCA, which consists in applying a projection on the first principal components of the joint source/target distribution (estimated from the concatenation of source and target samples); \u2022 GFK, Geodesic Flow Kernel [23]; \u2022 TSL, Transfer Subspace Learning [44], which operates by minimizing the Bregman divergence between the domains embedded in lower dimensional spaces; \u2022 JDA, Joint Distribution Adaptation [34], which extends the Transfer Component Analysis algorithm [38];\n\nIn unsupervised DA no target labels are available. As a consequence, it is impossible to consider a crossvalidation step for the hyper-parameters of the different methods. However, and in order to compare the methods fairly, we follow the following protocol. For each source domain, a random selection of 20 samples per class (with the only exception of 8 for the DSLR dataset) is adopted. Then the target domain is equivalently partitioned in a validation and test sets. The validation set is used to obtain the best accuracy in the range of the possible hyper-parameters. The accuracy, measured as the percent of correct classification over all the classes, is then evaluated on the testing set, with the best selected hyper-parameters. This strategy normally prevents overfitting on the testing set. The experimentation is conducted 10 times, and the mean accuracy over all these realizations is reported.\n\nWe considered the following parameter range : for subspace learning methods (PCA,TSL, GFK, and JDA) we considered reduced k-dimensional spaces with k \u2208 {10, 20, . . . , 70}. A linear kernel was chosen for all the methods with a kernel formulation. For the all methods requiring a regularization parameter, the best value was searched in \u03bb = {0.001, 0.01, 0.1, 1, 10, 100, 1000}. The \u03bb and \u03b7 parameters of our different regularizers (Equation (16)), are validated using the same search interval. In the case of the Laplacian regularization (OT-Laplace), S t is a binary matrix which encodes a nearest neighbors graph with a 8-connectivity. For the source domain, S s is filtered such that connections between elements of different classes are pruned. Finally, we set the \u03b1 value Equation (20) to 0.5.\n\n\nResults on unsupervised domain adaptation\n\nResults of the experiment are reported in Table 3 where the best performing method for each domain adaptation problem is highlighted in bold. On average, all the OT-based domain adaptation methods perform better than the baseline methods, except in the case of the PIE dataset, where JDA outperforms the OT-based methods in 7 out of 12 domain pairs. A possible explanation is that the dataset contains a lot of classes (68), and the EM-like step of JDA, which allows to take into account the current results of classification on the target, is clearly leading to a benefit. We notice that TSL, which is based on a similar principle of distribution divergence minimization, almost never outperforms our regularized strategies, except on pair A\u2192C. Among the different optimal transport strategies, OT-Exact leads to the lowest performances. OT-IT, the entropy regularized version of the transport, is substantially better than OT-Exact, but is still inferior to the class-based regularized strategies proposed in this paper. The best performing strategies are clearly OT-GL and OT-Laplace with a slight advantage for OT-GL. OT-LpL1, which is based on a similar regularization strategy as OT-GL, but with a different optimization scheme, has globally inferior performances, except on some pairs of domains (e.g. C\u2192A ) where it achieves better scores. On both digits and objects recognition tasks, OT-GL significantly outperforms the baseline methods.\n\nIn the next experiment (Table 4), we use the same experimental protocol on different features produced by the DeCAF deep learning architecture [19]. We report the results of the experiment conducted on the Office-Caltech dataset, with the OT-IT and OT-GL regularization strategies. For comparison purposes, JDA is also considered for this adaptation task. The results show that, even though the deep learning features yield naturally a strong improvement over the classical SURF features, the proposed OT methods are still capable of improving significantly the performances of the final classification (up to more than 20 points in some case, e.g. D\u2192A or A\u2192W). This clearly shows how OT has the capacity to handle nonstationarity in the distributions that the deep architecture has difficulty handling. We also note that using the features from the 7th layer instead of the 6th does not bring a strong improvement in the classification accuracy, suggesting that part of the work of the 7th layer is already performed by the optimal transport.\n\n\nSemi-supervised domain adaptation\n\nIn this last experiment, we assume that few labels are available in the target domain. We thus benchmark  our semi-supervised approach on SURF features extracted from the Office-Caltech dataset. We consider that only 3 labeled samples per class are at our disposal in the target domain. In order to disentangle the benefits of the labeled target samples brought by our optimal transport strategies from those brought by the classifier, we make a distinction between two cases: in the first one, denoted as \"Unsupervised + labels\", we consider that the label target samples are available only at the learning stage, after an unsupervised domain adaptation with optimal transport. In the second case, denoted as \"semi-supervised\", labels in the target domain are used to compute a new transportation plan, through the use of the proposed \n\n\nOT-IT OT-GL OT-IT\n\nOT-GL MMDT [28] \n\nFig. 1 :\n1distances, known under several names in the literature (Wasserstein, Monge-Kantorovich or Earth Mover distances) have important properties: i) They can be evaluated directly on empirical estimates of the distribu-arXiv:1507.00504v2 [cs.LG] Illustration of the proposed approach for domain adaptation. (left) dataset for training, i.e. source domain, and testing, i.e. target domain. Note that a classifier estimated on the training examples clearly does not fit the target data. (middle) a data dependent transportation map T \u03b30 is estimated and used to transport the training samples onto the target domain. Note that this transformation is usually not linear. (right) the transported labeled samples are used for estimating a classifier in the target domain.\n\nFig. 2 :\n2Illustration of the optimal transport problem. (a) Monge problem over 2D domains. T is a push-forward from \u2126 s to \u2126 t . (b) Kantorovich relaxation over 1D domains: \u03b3 can be seen as a joint probability distribution with marginals \u00b5 s and \u00b5 t . (c) Illustration of the solution of the Kantorovich relaxation computed between two ellipsoidal distributions in 2D. The grey line between two points indicate a non-zero coupling between them.\n\n\nFace recognition. In the face recognition experiment, we use the PIE (\"Pose, Illumination, Expression\") dataset, which contains 32 \u00d7 32 images of 68 individuals taken under various pose, illumination and expressions conditions. The 4 experimental domains are constructed by selecting 4 distinct poses: PIE05 (C05, left pose), PIE07 (C07, upward pose), PIE09 (C09, downward pose) and PIE29 (C29, right pose)\n\nTABLE 1 :\n1Mean error rate over 10 realizations for the two moons simulated example.\n\nTABLE 2 :\n2Summary of the domains used in the visual adaptation experiment tion\n\nTABLE 3 :\n3Overall recognition accuracies in % obtained over all domains pairs using the SURF features.Maximum values for each pair is indicated in bold font. exact OT-IT OT-Laplace OT-LpLq OT-GLDomains \n1NN \nPCA \nGFK \nTSL \nJDA \nOT-U\u2192M \n39.00 37.83 44.16 \n40.66 \n54.52 \n50.67 \n53.66 \n57.42 \n60.15 \n57.85 \nM\u2192U \n58.33 48.05 60.96 \n53.79 \n60.09 \n49.26 \n64.73 \n64.72 \n68.07 \n69.96 \nmean \n48.66 42.94 52.56 \n47.22 \n57.30 \n49.96 \n59.20 \n61.07 \n64.11 \n63.90 \nP1\u2192P2 \n23.79 32.61 22.83 \n34.29 \n67.15 \n52.27 \n57.73 \n58.92 \n59.28 \n59.41 \nP1\u2192P3 \n23.50 38.96 23.24 \n33.53 \n56.96 \n51.36 \n57.43 \n57.62 \n58.49 \n58.73 \nP1\u2192P4 \n15.69 30.82 16.73 \n26.85 \n40.44 \n40.53 \n47.21 \n47.54 \n47.29 \n48.36 \nP2\u2192P1 \n24.27 35.69 24.18 \n33.73 \n63.73 \n56.05 \n60.21 \n62.74 \n62.61 \n61.91 \nP2\u2192P3 \n44.45 40.87 44.03 \n38.35 \n68.42 \n59.15 \n63.24 \n64.29 \n62.71 \n64.36 \nP2\u2192P4 \n25.86 29.83 25.49 \n26.21 \n49.85 \n46.73 \n51.48 \n53.52 \n50.42 \n52.68 \nP3\u2192P1 \n20.95 32.01 20.79 \n39.79 \n60.88 \n54.24 \n57.50 \n57.87 \n58.96 \n57.91 \nP3\u2192P2 \n40.17 38.09 40.70 \n39.17 \n65.07 \n59.08 \n63.61 \n65.75 \n64.04 \n64.67 \nP3\u2192P4 \n26.16 36.65 25.91 \n36.88 \n52.44 \n48.25 \n52.33 \n54.02 \n52.81 \n52.83 \nP4\u2192P1 \n18.14 29.82 20.11 \n40.81 \n46.91 \n43.21 \n45.15 \n45.67 \n46.51 \n45.73 \nP4\u2192P2 \n24.37 29.47 23.34 \n37.50 \n55.12 \n46.76 \n50.71 \n52.50 \n50.90 \n51.31 \nP4\u2192P3 \n27.30 39.74 26.42 \n46.14 \n53.33 \n48.05 \n52.10 \n52.71 \n51.37 \n52.60 \nmean \n26.22 34.55 26.15 \n36.10 \n56.69 \n50.47 \n54.89 \n56.10 \n55.45 \n55.88 \nC\u2192A \n20.54 35.17 35.29 \n45.25 \n40.73 \n30.54 \n37.75 \n38.96 \n48.21 \n44.17 \nC\u2192W \n18.94 28.48 31.72 \n37.35 \n33.44 \n23.77 \n31.32 \n31.13 \n38.61 \n38.94 \nC\u2192D \n19.62 33.75 35.62 \n39.25 \n39.75 \n26.62 \n34.50 \n36.88 \n39.62 \n44.50 \nA\u2192C \n22.25 32.78 32.87 38.46 \n33.99 \n29.43 \n31.65 \n33.12 \n35.99 \n34.57 \nA\u2192W \n23.51 29.34 32.05 \n35.70 \n36.03 \n25.56 \n30.40 \n30.33 \n35.63 \n37.02 \nA\u2192D \n20.38 26.88 30.12 \n32.62 \n32.62 \n25.50 \n27.88 \n27.75 \n36.38 \n38.88 \nW\u2192C \n19.29 26.95 27.75 \n29.02 \n31.81 \n25.87 \n31.63 \n31.37 \n33.44 \n35.98 \nW\u2192A \n23.19 28.92 33.35 \n34.94 \n31.48 \n27.40 \n37.79 \n37.17 \n37.33 \n39.35 \nW\u2192D \n53.62 79.75 79.25 \n80.50 \n84.25 \n76.50 \n80.00 \n80.62 \n81.38 \n84.00 \nD\u2192C \n23.97 29.72 29.50 \n31.03 \n29.84 \n27.30 \n29.88 \n31.10 \n31.65 \n32.38 \nD\u2192A \n27.10 30.67 32.98 \n36.67 \n32.85 \n29.08 \n32.77 \n33.06 \n37.06 \n37.17 \nD\u2192W \n51.26 71.79 69.67 \n77.48 \n80.00 \n65.70 \n72.52 \n76.16 \n74.97 \n81.06 \nmean \n28.47 37.98 39.21 \n42.97 \n44.34 \n36.69 \n42.30 \n43.20 \n46.42 \n47.70 \n\n\n\nTABLE 4 :\n4Results of adaptation by optimal transport using DeCAF features.Layer 6 \nLayer 7 \n\nDomains \nDeCAF \nJDA \nOT-IT OT-GL DeCAF \nJDA \nOT-IT OT-GL \n\nC\u2192A \n79.25 \n88.04 \n88.69 \n92.08 \n85.27 \n89.63 \n91.56 \n92.15 \nC\u2192W \n48.61 \n79.60 \n75.17 \n84.17 \n65.23 \n79.80 \n82.19 \n83.84 \nC\u2192D \n62.75 \n84.12 \n83.38 \n87.25 \n75.38 \n85.00 \n85.00 \n85.38 \nA\u2192C \n64.66 \n81.28 \n81.65 \n85.51 \n72.80 \n82.59 \n84.22 \n87.16 \nA\u2192W \n51.39 \n80.33 \n78.94 \n83.05 \n63.64 \n83.05 \n81.52 \n84.50 \nA\u2192D \n60.38 \n86.25 \n85.88 \n85.00 \n75.25 \n85.50 \n86.62 \n85.25 \nW\u2192C \n58.17 \n81.97 \n74.80 \n81.45 \n69.17 \n79.84 \n81.74 \n83.71 \nW\u2192A \n61.15 \n90.19 \n80.96 \n90.62 \n72.96 \n90.94 \n88.31 \n91.98 \nW\u2192D \n97.50 \n98.88 \n95.62 \n96.25 \n98.50 \n98.88 \n98.38 \n91.38 \nD\u2192C \n52.13 \n81.13 \n77.71 \n84.11 \n65.23 \n81.21 \n82.02 \n84.93 \nD\u2192A \n60.71 \n91.31 \n87.15 \n92.31 \n75.46 \n91.92 \n92.15 \n92.92 \nD\u2192W \n85.70 \n97.48 \n93.77 \n96.29 \n92.25 \n97.02 \n96.62 \n94.17 \nmean \n65.20 \n86.72 \n83.64 \n88.18 \n75.93 \n87.11 \n87.53 \n88.11 \n\n\n\nTABLE 5 :\n5Results of semi-supervised adaptation with optimal transport using the SURF features.s \n\nUnsupervised + labels \nSemi-supervised \n\nDomains \n\nACKNOWLEDGMENTSThis work was partly funded by the Swiss National Science Foundation under the grant PP00P2-150593 and by the CNRS PEPS Fascido program under the Topase project.semi-supervised regularization term in Equation(21)). Results are reported inTable 5. They clearly show the benefits of the proposed semi-supervised regularization term in the definition of the transportation plan. A comparison with the state-of-the-art method of Hoffman and colleagues[28]is also reported, and shows the competitiveness of our approach.CONCLUSIONIn this paper, we described a new framework based on optimal transport to solve the unsupervised domain adaptation problem. We proposed two regularization schemes to encode class-structure in the source domain during the estimation of the transportation plan, thus enforcing the intuition that samples of the same class must undergo similar transformation. We extended this OT regularized framework to the semi-supervised domain adaptation case, i.e. the case where few labels are available in the target domain. Regarding the computational aspects, we suggested to use a modified version of the conditional gradient algorithm, the generalized conditional gradient splitting, which enables the method to scale up to real-world datasets. Finally, we applied the proposed methods on both synthetic and real world datasets. Results show that the optimal transportation domain adaptation schemes frequently outperform the competing state-of-the-art methods.We believe that the framework presented in this paper will lead to a paradigm shift for the domain adaptation problem. Estimating a transport is much more general than finding a common subspace, but comes with the problem of finding a proper regularization term. The proposed class-based or Laplacian regularizers show very good performances, but we believe that other types of regularizer should be investigated. Indeed, whenever the transformation is induced by a physical process, one may want the transport map to enforce physical constraints. This can be included with dedicated regularization terms. We also plan to extend our optimal transport framework to the multidomain adaptation problem, where the problem of matching several distributions can be cast as a multimarginal optimal transport problem. Nicolas Courty is associate professor within University Bretagne-Sud since October 2004. He obtained his habilitation degree (HDR) in 2013. His main research objectives are data analysis/synthesis schemes, machine learning and visualization problems, with applications in computer vision, remote sensing and computer graphics. Visit http://people.irisa.fr/Nicolas.Courty/ for more information. He obtained his Phd on Signal processing from the university of Orl\u00e9ans in 1997. His recent research activities deal with machine learning and signal processing with applications to braincomputer interfaces and audio applications. Alain serves as a regular reviewer for machine learning and signal processing journals.R\u00e9mi Flamary is Assistant\nR K Ahuja, T L Magnanti, J B Orlin, Network Flows: Theory, Algorithms, and Applications. Upper Saddle River, NJ, USAPrentice-Hall, IncR. K. Ahuja, T. L. Magnanti, and J. B. Orlin, Network Flows: Theory, Algorithms, and Applications. Upper Saddle River, NJ, USA: Prentice-Hall, Inc., 1993.\n\nImpossibility theorems for domain adaptation. S Ben-David, T Luu, T Lu, D P\u00e1l, Artificial Intelligence and Statistics Conference (AISTATS). S. Ben-David, T. Luu, T. Lu, and D. P\u00e1l, \"Impossibility the- orems for domain adaptation.\" in Artificial Intelligence and Statistics Conference (AISTATS), 2010, pp. 129-136.\n\nA computational fluid mechanics solution to the monge-kantorovich mass transfer problem. J.-D Benamou, Y Brenier, Numerische Mathematik. 843J.-D. Benamou and Y. Brenier, \"A computational fluid mechan- ics solution to the monge-kantorovich mass transfer problem,\" Numerische Mathematik, vol. 84, no. 3, pp. 375-393, 2000.\n\nNonlinear programming. Athena scientific Belmont. D P Bertsekas, D. P. Bertsekas, Nonlinear programming. Athena scientific Belmont, 1999.\n\nSliced and radon Wasserstein barycenters of measures. N Bonneel, J Rabin, G Peyr\u00e9, H Pfister, Journal of Mathematical Imaging and Vision. 51N. Bonneel, J. Rabin, G. Peyr\u00e9, and H. Pfister, \"Sliced and radon Wasserstein barycenters of measures,\" Journal of Mathe- matical Imaging and Vision, vol. 51, pp. 22-45, 2015.\n\nDisplacement interpolation using Lagrangian mass transport. N Bonneel, M Van De Panne, S Paris, W Heidrich, ACM Transaction on Graphics. 30612N. Bonneel, M. van de Panne, S. Paris, and W. Heidrich, \"Dis- placement interpolation using Lagrangian mass transport,\" ACM Transaction on Graphics, vol. 30, no. 6, pp. 158:1-158:12, 2011.\n\nA generalized conditional gradient method and its connection to an iterative shrinkage method. K Bredies, D A Lorenz, P Maass, Computational Optimization and Applications. 422K. Bredies, D. A. Lorenz, and P. Maass, \"A generalized con- ditional gradient method and its connection to an iterative shrinkage method,\" Computational Optimization and Applica- tions, vol. 42, no. 2, pp. 173-193, 2009.\n\nEquivalence of a generalized conditional gradient method and the method of surrogate functionals. Zentrum f\u00fcr Technomathematik. K Bredies, D Lorenz, P Maass, K. Bredies, D. Lorenz, and P. Maass, Equivalence of a generalized conditional gradient method and the method of surrogate functionals. Zentrum f\u00fcr Technomathematik, 2005.\n\nDomain adaptation problems: A dasvm classification technique and a circular validation strategy. L Bruzzone, M Marconcini, IEEE Transactions on Pattern Analysis and Machine Intelligence. 325L. Bruzzone and M. Marconcini, \"Domain adaptation prob- lems: A dasvm classification technique and a circular val- idation strategy,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 32, no. 5, pp. 770-787, May 2010.\n\nGrapihcal models and point pattern matching. T S Caetano, T Caelli, D Schuurmans, D Barone, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2810T. S. Caetano, T. Caelli, D. Schuurmans, and D. Barone, \"Grapi- hcal models and point pattern matching,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 28, no. 10, pp. 1646-1663, 2006.\n\nLearning graph matching. T S Caetano, J J Mcauley, L Cheng, Q V Le, A J Smola, IEEE Transactions on Pattern Analysis and Machine Intelligence. 316T. S. Caetano, J. J. McAuley, L. Cheng, Q. V. Le, and A. J. Smola, \"Learning graph matching,\" IEEE Transactions on Pattern Anal- ysis and Machine Intelligence, vol. 31, no. 6, pp. 1048-1058, 2009.\n\nNumerical methods for matching for teams and Wasserstein barycenters. G Carlier, A Oberman, E Oudet, hal-00987292Inria. Tech. Rep.G. Carlier, A. Oberman, and E. Oudet, \"Numerical methods for matching for teams and Wasserstein barycenters,\" Inria, Tech. Rep. hal-00987292, 2014.\n\nLASS: A simple assignment model with laplacian smoothing. M Carreira-Perpinan, W Wang, AAAI Conference on Artificial Intelligence. M. Carreira-Perpinan and W. Wang, \"LASS: A simple assign- ment model with laplacian smoothing,\" in AAAI Conference on Artificial Intelligence, 2014.\n\nDomain adaptation with regularized optimal transport. N Courty, R Flamary, D Tuia, European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD). N. Courty, R. Flamary, and D. Tuia, \"Domain adaptation with regularized optimal transport,\" in European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD), 2014.\n\nSinkhorn distances: Lightspeed computation of optimal transportation. M Cuturi, Neural Information Processing Systems (NIPS). M. Cuturi, \"Sinkhorn distances: Lightspeed computation of optimal transportation,\" in Neural Information Processing Sys- tems (NIPS), 2013, pp. 2292-2300.\n\nGround metric learning. M Cuturi, D Avis, Journal of Machine Learning Research. 151M. Cuturi and D. Avis, \"Ground metric learning,\" Journal of Machine Learning Research, vol. 15, no. 1, pp. 533-564, Jan. 2014.\n\nFast computation of Wasserstein barycenters. M Cuturi, A Doucet, International Conference on Machine Learning (ICML). M. Cuturi and A. Doucet, \"Fast computation of Wasserstein barycenters,\" in International Conference on Machine Learning (ICML), 2014.\n\nFrustratingly easy domain adaptation. H Daum\u00e9, Ann. Meeting of the Assoc. Computational Linguistics. H. Daum\u00e9 III, \"Frustratingly easy domain adaptation,\" in Ann. Meeting of the Assoc. Computational Linguistics, 2007.\n\nDeCAF: a deep convolutional activation feature for generic visual recognition. J Donahue, Y Jia, O Vinyals, J Hoffman, N Zhang, E Tzeng, T Darrell, International Conference on Machine Learning (ICML). J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang, E. Tzeng, and T. Darrell, \"DeCAF: a deep convolutional activation fea- ture for generic visual recognition,\" in International Conference on Machine Learning (ICML), 2014, pp. 647-655.\n\nRegularized discrete optimal transport. S Ferradans, N Papadakis, J Rabin, G Peyr\u00e9, J.-F Aujol, Scale Space and Variational Methods in Computer Vision, SSVM. S. Ferradans, N. Papadakis, J. Rabin, G. Peyr\u00e9, and J.-F. Aujol, \"Regularized discrete optimal transport,\" in Scale Space and Variational Methods in Computer Vision, SSVM, 2013, pp. 428- 439.\n\nThe geometry of optimal transportation. W Gangbo, R J Mccann, Acta Mathematica. 1772W. Gangbo and R. J. McCann, \"The geometry of optimal transportation,\" Acta Mathematica, vol. 177, no. 2, pp. 113-161, 1996.\n\nA PAC-Bayesian Approach for Domain Adaptation with Specialization to Linear Classifiers. P Germain, A Habrard, F Laviolette, E Morvant, International Conference on Machine Learning (ICML). Atlanta, USAP. Germain, A. Habrard, F. Laviolette, and E. Morvant, \"A PAC-Bayesian Approach for Domain Adaptation with Spe- cialization to Linear Classifiers,\" in International Conference on Machine Learning (ICML), Atlanta, USA, 2013, pp. 738-746.\n\nGeodesic flow kernel for unsupervised domain adaptation. B Gong, Y Shi, F Sha, K Grauman, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). B. Gong, Y. Shi, F. Sha, and K. Grauman, \"Geodesic flow kernel for unsupervised domain adaptation.\" in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012, pp. 2066- 2073.\n\nDomain adaptation for object recognition: An unsupervised approach. R Gopalan, R Li, R Chellappa, International Conference on Computer Vision (ICCV). R. Gopalan, R. Li, and R. Chellappa, \"Domain adaptation for object recognition: An unsupervised approach,\" in Inter- national Conference on Computer Vision (ICCV), 2011, pp. 999- 1006.\n\nCaltech-256 Object Category Dataset. G Griffin, A Holub, P Perona, CNS-TR-2007-001California Institute of TechnologyTech. Rep.G. Griffin, A. Holub, and P. Perona, \"Caltech-256 Object Cat- egory Dataset,\" California Institute of Technology, Tech. Rep. CNS-TR-2007-001, 2007.\n\nSemisupervised alignment of manifolds. J Ham, D Lee, L Saul, 10th International Workshop on Artificial Intelligence and Statistics. R. G. Cowell and Z. GhahramaniJ. Ham, D. Lee, and L. Saul, \"Semisupervised alignment of manifolds,\" in 10th International Workshop on Artificial Intelli- gence and Statistics, R. G. Cowell and Z. Ghahramani, Eds., 2005, pp. 120-127.\n\nEfficient learning of domain invariant image representations. J Hoffman, E Rodner, J Donahue, K Saenko, T Darrell, International Conference on Learning Representations (ICLR). J. Hoffman, E. Rodner, J. Donahue, K. Saenko, and T. Dar- rell, \"Efficient learning of domain invariant image represen- tations,\" in International Conference on Learning Representations (ICLR), 2013.\n\nEfficient learning of domain-invariant image representations. International Conference on Learning Representations (ICLR). --, \"Efficient learning of domain-invariant image represen- tations,\" in International Conference on Learning Representations (ICLR), 2013.\n\nRobust visual domain adaptation with low-rank reconstruction. I.-H Jhuo, D Liu, D T Lee, S.-F Chang, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). I.-H. Jhuo, D. Liu, D. T. Lee, and S.-F. Chang, \"Robust visual domain adaptation with low-rank reconstruction,\" in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2012, pp. 2168-2175.\n\nOn the translocation of masses. L Kantorovich, C.R. (Doklady) Acad. Sci. URSS (N.S.). 37L. Kantorovich, \"On the translocation of masses,\" C.R. (Dok- lady) Acad. Sci. URSS (N.S.), vol. 37, pp. 199-201, 1942.\n\nThe sinkhorn-knopp algorithm: Convergence and applications. P Knight, SIAM Journal on Matrix Analysis and Applications. 301P. Knight, \"The sinkhorn-knopp algorithm: Convergence and applications,\" SIAM Journal on Matrix Analysis and Applications, vol. 30, no. 1, pp. 261-275, 2008.\n\nWhat you saw is not what you get: domain adaptation using asymmetric kernel transforms. B Kulis, K Saenko, T Darrell, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Colorado Springs, COB. Kulis, K. Saenko, and T. Darrell, \"What you saw is not what you get: domain adaptation using asymmetric kernel transforms,\" in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Colorado Springs, CO, 2011.\n\nGeneralized multiview analysis: A discriminative latent space. A Kumar, H Daum\u00e9, Iii , D Jacobs, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). A. Kumar, H. Daum\u00e9 III, and D. Jacobs, \"Generalized multi- view analysis: A discriminative latent space,\" in IEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR), 2012.\n\nTransfer feature learning with joint distribution adaptation. M Long, J Wang, G Ding, J Sun, P Yu, International Conference on Computer Vision (ICCV). M. Long, J. Wang, G. Ding, J. Sun, and P. Yu, \"Transfer feature learning with joint distribution adaptation,\" in International Conference on Computer Vision (ICCV), Dec 2013, pp. 2200-2207.\n\nStructural graph matching using the em algorithm and singular value decomposition. B Luo, R Hancock, IEEE Transactions on Pattern Analysis and Machine Intelligence. 2310B. Luo and R. Hancock, \"Structural graph matching using the em algorithm and singular value decomposition,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 23, no. 10, pp. 1120-1136, 2001.\n\nDomain adaptation: Learning bounds and algorithms. Y Mansour, M Mohri, A Rostamizadeh, Conference on Learning Theory (COLT. Y. Mansour, M. Mohri, and A. Rostamizadeh, \"Domain adap- tation: Learning bounds and algorithms,\" in Conference on Learning Theory (COLT), 2009, pp. 19-30.\n\nA survey on transfer learning. S J Pan, Q Yang, IEEE Transactions on Knowledge and Data Engineering. 2210S. J. Pan and Q. Yang, \"A survey on transfer learning,\" IEEE Transactions on Knowledge and Data Engineering, vol. 22, no. 10, pp. 1345-1359, 2010.\n\nDomain adaptation via transfer component analysis. IEEE Transactions on Neural Networks. 22--, \"Domain adaptation via transfer component analysis,\" IEEE Transactions on Neural Networks, vol. 22, pp. 199-210, 2011.\n\nVisual domain adaptation: an overview of recent advances. V M Patel, R Gopalan, R Li, R Chellappa, IEEE Signal Processing Magazine. 323V. M. Patel, R. Gopalan, R. Li, and R. Chellappa, \"Visual domain adaptation: an overview of recent advances,\" IEEE Signal Processing Magazine, vol. 32, no. 3, 2015.\n\nWasserstein barycenter and its application to texture mixing,\" in Scale Space and Variational Methods in Computer Vision, ser. J Rabin, G Peyr\u00e9, J Delon, M Bernot, Lecture Notes in Computer Science. 6667J. Rabin, G. Peyr\u00e9, J. Delon, and M. Bernot, \"Wasserstein barycenter and its application to texture mixing,\" in Scale Space and Variational Methods in Computer Vision, ser. Lecture Notes in Computer Science, 2012, vol. 6667, pp. 435-446.\n\nA metric for distributions with applications to image databases. Y Rubner, C Tomasi, L Guibas, International Conference on Computer Vision (ICCV). Y. Rubner, C. Tomasi, and L. Guibas, \"A metric for distribu- tions with applications to image databases,\" in International Conference on Computer Vision (ICCV), 1998, pp. 59-66.\n\nAdapting visual category models to new domains. K Saenko, B Kulis, M Fritz, T Darrell, European Conference on Computer Vision (ECCV), ser. K. Saenko, B. Kulis, M. Fritz, and T. Darrell, \"Adapting visual category models to new domains,\" in European Conference on Computer Vision (ECCV), ser. LNCS, 2010, pp. 213-226.\n\nOptimal transport for applied mathematicians. F Santambrogio, Birk\u00e4user. F. Santambrogio, \"Optimal transport for applied mathemati- cians,\" Birk\u00e4user, NY, 2015.\n\nBregman divergence-based regularization for transfer subspace learning. S Si, D Tao, B Geng, IEEE Transactions on Knowledge and Data Engineering. 227S. Si, D. Tao, and B. Geng, \"Bregman divergence-based reg- ularization for transfer subspace learning,\" IEEE Transactions on Knowledge and Data Engineering, vol. 22, no. 7, pp. 929-942, July 2010.\n\nWasserstein propagation for semi-supervised learning. J Solomon, R Rustamov, G Leonidas, A Butscher, International Conference on Machine Learning (ICML). J. Solomon, R. Rustamov, G. Leonidas, and A. Butscher, \"Wasserstein propagation for semi-supervised learning,\" in International Conference on Machine Learning (ICML), 2014, pp. 306-314.\n\nDirect importance estimation with model selection and its application to covariate shift adaptation. M Sugiyama, S Nakajima, H Kashima, P Buenau, M Kawanabe, Neural Information Processing Systems (NIPS). M. Sugiyama, S. Nakajima, H. Kashima, P. Buenau, and M. Kawanabe, \"Direct importance estimation with model se- lection and its application to covariate shift adaptation,\" in Neural Information Processing Systems (NIPS), 2008.\n\nKernel manifold alignment for domain adaptation. D Tuia, G Camps-Valls, PLoS One. 112148655D. Tuia and G. Camps-Valls, \"Kernel manifold alignment for domain adaptation,\" PLoS One, vol. 11, no. 2, p. e0148655, 2016.\n\nMultitemporal classification without new labels: a solution with optimal transport. D Tuia, R Flamary, A Rakotomamonjy, N Courty, 8th International Workshop on the Analysis of Multitemporal Remote Sensing Images. D. Tuia, R. Flamary, A. Rakotomamonjy, and N. Courty, \"Mul- titemporal classification without new labels: a solution with optimal transport,\" in 8th International Workshop on the Analysis of Multitemporal Remote Sensing Images, 2015.\n\nOptimal transport: old and new, ser. Grundlehren der mathematischen Wissenschaften. C Villani, SpringerC. Villani, Optimal transport: old and new, ser. Grundlehren der mathematischen Wissenschaften. Springer, 2009.\n\nManifold alignment. C Wang, P Krafft, S Mahadevan, Manifold Learning: Theory and Applications. Y. Ma and Y. FuCRC PressC. Wang, P. Krafft, and S. Mahadevan, \"Manifold alignment,\" in Manifold Learning: Theory and Applications, Y. Ma and Y. Fu, Eds. CRC Press, 2011.\n\nManifold alignment without correspondence. C Wang, S Mahadevan, International Joint Conference on Artificial Intelligence (IJCAI). Pasadena, CAC. Wang and S. Mahadevan, \"Manifold alignment without correspondence,\" in International Joint Conference on Artificial Intelligence (IJCAI), Pasadena, CA, 2009.\n\nHeterogeneous domain adaptation using manifold alignment. International Joint Conference on Artificial Intelligence (IJCAI). AAAI Press--, \"Heterogeneous domain adaptation using manifold alignment,\" in International Joint Conference on Artificial Intel- ligence (IJCAI). AAAI Press, 2011, pp. 1541-1546.\n\nCovariate shift in Hilbert space: A solution via surrogate kernels. K Zhang, V W Zheng, Q Wang, J T Kwok, Q Yang, I Marsic, International Conference on Machine Learning (ICML). K. Zhang, V. W. Zheng, Q. Wang, J. T. Kwok, Q. Yang, and I. Marsic, \"Covariate shift in Hilbert space: A solution via surrogate kernels,\" in International Conference on Machine Learning (ICML), 2013.\n\nA Grassmann manifold-based domain adaptation approach. J Zheng, M.-Y Liu, R Chellappa, P Phillips, International Conference on Pattern Recognition (ICPR). J. Zheng, M.-Y. Liu, R. Chellappa, and P. Phillips, \"A Grass- mann manifold-based domain adaptation approach,\" in Inter- national Conference on Pattern Recognition (ICPR), Nov 2012, pp. 2095-2099.\n", "annotations": {"author": "[{\"end\":45,\"start\":43},{\"end\":51,\"start\":46},{\"end\":63,\"start\":52}]", "publisher": null, "author_last_name": "[{\"end\":62,\"start\":60}]", "author_first_name": "[{\"end\":44,\"start\":43},{\"end\":48,\"start\":46},{\"end\":50,\"start\":49},{\"end\":59,\"start\":52}]", "author_affiliation": null, "title": "[{\"end\":40,\"start\":1},{\"end\":103,\"start\":64}]", "venue": "[{\"end\":167,\"start\":105}]", "abstract": "[{\"end\":1597,\"start\":277}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3272,\"start\":3268},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3664,\"start\":3661},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3670,\"start\":3666},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3676,\"start\":3672},{\"end\":4837,\"start\":4832},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6882,\"start\":6878},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6888,\"start\":6884},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6894,\"start\":6890},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7075,\"start\":7071},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7219,\"start\":7215},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":7225,\"start\":7221},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7231,\"start\":7227},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7419,\"start\":7415},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7691,\"start\":7687},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7697,\"start\":7693},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7917,\"start\":7913},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7966,\"start\":7962},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8004,\"start\":8000},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8325,\"start\":8321},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8591,\"start\":8587},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":8597,\"start\":8593},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":9044,\"start\":9040},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9489,\"start\":9485},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9495,\"start\":9491},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9501,\"start\":9497},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9983,\"start\":9979},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":10089,\"start\":10085},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10176,\"start\":10173},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10268,\"start\":10264},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10274,\"start\":10270},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10279,\"start\":10276},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10327,\"start\":10324},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10387,\"start\":10383},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10624,\"start\":10620},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":10660,\"start\":10656},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10827,\"start\":10823},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10833,\"start\":10829},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":10838,\"start\":10835},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12923,\"start\":12920},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":15192,\"start\":15188},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":15661,\"start\":15657},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":16091,\"start\":16087},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":18059,\"start\":18055},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19271,\"start\":19267},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":19689,\"start\":19685},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":21296,\"start\":21292},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":21709,\"start\":21705},{\"end\":21719,\"start\":21709},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":23305,\"start\":23301},{\"end\":23310,\"start\":23305},{\"end\":25656,\"start\":25645},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":25787,\"start\":25783},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25865,\"start\":25861},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":26245,\"start\":26241},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":26465,\"start\":26461},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29015,\"start\":29011},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":29224,\"start\":29221},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":30828,\"start\":30824},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":31019,\"start\":31015},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":31025,\"start\":31021},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31279,\"start\":31275},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":31921,\"start\":31917},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":32131,\"start\":32127},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":32139,\"start\":32135},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":32941,\"start\":32937},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":33271,\"start\":33267},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":34240,\"start\":34236},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":34501,\"start\":34497},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":34689,\"start\":34685},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":35049,\"start\":35045},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":35138,\"start\":35134},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":35229,\"start\":35225},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":35276,\"start\":35273},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":36144,\"start\":36141},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":36963,\"start\":36960},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":37018,\"start\":37014},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":37237,\"start\":37233},{\"end\":37623,\"start\":37621},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":38928,\"start\":38925},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":39242,\"start\":39238},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":39373,\"start\":39369},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":39965,\"start\":39961},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":39971,\"start\":39967},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":40026,\"start\":40022},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":40728,\"start\":40725},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":40758,\"start\":40754},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":42699,\"start\":42695},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":43006,\"start\":43002},{\"end\":43512,\"start\":43503},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":43950,\"start\":43946},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":43956,\"start\":43952},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":43962,\"start\":43958},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":43968,\"start\":43964},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":43974,\"start\":43970},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":43982,\"start\":43978},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":44291,\"start\":44287},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":44470,\"start\":44466},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":44786,\"start\":44782},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":45667,\"start\":45663},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":45707,\"start\":45703},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":45860,\"start\":45856},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":45922,\"start\":45918},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":47626,\"start\":47622},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":49276,\"start\":49272},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":51083,\"start\":51079}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":51856,\"start\":51085},{\"attributes\":{\"id\":\"fig_2\"},\"end\":52303,\"start\":51857},{\"attributes\":{\"id\":\"fig_3\"},\"end\":52712,\"start\":52304},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":52798,\"start\":52713},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":52879,\"start\":52799},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":55257,\"start\":52880},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":56207,\"start\":55258},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":56359,\"start\":56208}]", "paragraph": "[{\"end\":2977,\"start\":1613},{\"end\":3633,\"start\":2979},{\"end\":4646,\"start\":3635},{\"end\":5859,\"start\":4648},{\"end\":6437,\"start\":5861},{\"end\":6670,\"start\":6455},{\"end\":7232,\"start\":6672},{\"end\":8160,\"start\":7234},{\"end\":9263,\"start\":8162},{\"end\":10388,\"start\":9265},{\"end\":10913,\"start\":10390},{\"end\":11124,\"start\":10974},{\"end\":11811,\"start\":11164},{\"end\":12264,\"start\":11861},{\"end\":12364,\"start\":12266},{\"end\":13089,\"start\":12366},{\"end\":13573,\"start\":13091},{\"end\":13733,\"start\":13575},{\"end\":13951,\"start\":13735},{\"end\":14620,\"start\":13987},{\"end\":15071,\"start\":14651},{\"end\":15435,\"start\":15126},{\"end\":15771,\"start\":15490},{\"end\":16174,\"start\":15895},{\"end\":16833,\"start\":16176},{\"end\":17275,\"start\":16878},{\"end\":17428,\"start\":17306},{\"end\":17653,\"start\":17489},{\"end\":17900,\"start\":17692},{\"end\":18066,\"start\":17956},{\"end\":18804,\"start\":18094},{\"end\":19359,\"start\":18806},{\"end\":19676,\"start\":19393},{\"end\":19929,\"start\":19678},{\"end\":20716,\"start\":19971},{\"end\":21392,\"start\":20786},{\"end\":21793,\"start\":21428},{\"end\":21960,\"start\":21856},{\"end\":22245,\"start\":22006},{\"end\":22745,\"start\":22282},{\"end\":23120,\"start\":22797},{\"end\":23561,\"start\":23175},{\"end\":23853,\"start\":23605},{\"end\":24418,\"start\":23908},{\"end\":24717,\"start\":24420},{\"end\":25387,\"start\":24719},{\"end\":26708,\"start\":25389},{\"end\":27030,\"start\":26757},{\"end\":27633,\"start\":27079},{\"end\":27761,\"start\":27635},{\"end\":27865,\"start\":27804},{\"end\":28257,\"start\":27867},{\"end\":29456,\"start\":28296},{\"end\":29675,\"start\":29522},{\"end\":30880,\"start\":29715},{\"end\":31405,\"start\":30909},{\"end\":31951,\"start\":31462},{\"end\":32404,\"start\":31987},{\"end\":33118,\"start\":32469},{\"end\":33614,\"start\":33173},{\"end\":34340,\"start\":33637},{\"end\":34558,\"start\":34413},{\"end\":35986,\"start\":34560},{\"end\":36195,\"start\":35988},{\"end\":36320,\"start\":36197},{\"end\":37295,\"start\":36347},{\"end\":37378,\"start\":37297},{\"end\":37678,\"start\":37427},{\"end\":37682,\"start\":37680},{\"end\":37709,\"start\":37684},{\"end\":37777,\"start\":37758},{\"end\":37916,\"start\":37784},{\"end\":38665,\"start\":37918},{\"end\":39243,\"start\":38691},{\"end\":39973,\"start\":39305},{\"end\":40826,\"start\":39975},{\"end\":42433,\"start\":40828},{\"end\":42830,\"start\":42535},{\"end\":44250,\"start\":42843},{\"end\":44749,\"start\":44252},{\"end\":45195,\"start\":44772},{\"end\":45447,\"start\":45197},{\"end\":45923,\"start\":45449},{\"end\":46833,\"start\":45925},{\"end\":47634,\"start\":46835},{\"end\":49127,\"start\":47680},{\"end\":50172,\"start\":49129},{\"end\":51046,\"start\":50210},{\"end\":51084,\"start\":51068}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13986,\"start\":13952},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14650,\"start\":14621},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15125,\"start\":15072},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15489,\"start\":15436},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15894,\"start\":15772},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17488,\"start\":17429},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17691,\"start\":17654},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17955,\"start\":17901},{\"attributes\":{\"id\":\"formula_8\"},\"end\":18093,\"start\":18067},{\"attributes\":{\"id\":\"formula_9\"},\"end\":19970,\"start\":19930},{\"attributes\":{\"id\":\"formula_10\"},\"end\":20785,\"start\":20717},{\"attributes\":{\"id\":\"formula_11\"},\"end\":21855,\"start\":21794},{\"attributes\":{\"id\":\"formula_12\"},\"end\":22005,\"start\":21961},{\"attributes\":{\"id\":\"formula_13\"},\"end\":22281,\"start\":22246},{\"attributes\":{\"id\":\"formula_14\"},\"end\":22796,\"start\":22746},{\"attributes\":{\"id\":\"formula_15\"},\"end\":23174,\"start\":23121},{\"attributes\":{\"id\":\"formula_16\"},\"end\":23604,\"start\":23562},{\"attributes\":{\"id\":\"formula_17\"},\"end\":27803,\"start\":27762},{\"attributes\":{\"id\":\"formula_18\"},\"end\":29521,\"start\":29457},{\"attributes\":{\"id\":\"formula_19\"},\"end\":29714,\"start\":29676},{\"attributes\":{\"id\":\"formula_20\"},\"end\":31461,\"start\":31406},{\"attributes\":{\"id\":\"formula_21\"},\"end\":31986,\"start\":31952},{\"attributes\":{\"id\":\"formula_22\"},\"end\":32468,\"start\":32405},{\"attributes\":{\"id\":\"formula_23\"},\"end\":33636,\"start\":33615},{\"attributes\":{\"id\":\"formula_24\"},\"end\":36346,\"start\":36321},{\"attributes\":{\"id\":\"formula_25\"},\"end\":37426,\"start\":37379},{\"attributes\":{\"id\":\"formula_26\"},\"end\":37757,\"start\":37710},{\"attributes\":{\"id\":\"formula_27\"},\"end\":42505,\"start\":42434}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":40859,\"start\":40852},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":47729,\"start\":47722},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":49161,\"start\":49152}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1611,\"start\":1599},{\"attributes\":{\"n\":\"1.1\"},\"end\":6453,\"start\":6440},{\"attributes\":{\"n\":\"2\"},\"end\":10949,\"start\":10916},{\"end\":10972,\"start\":10952},{\"attributes\":{\"n\":\"2.1\"},\"end\":11162,\"start\":11127},{\"attributes\":{\"n\":\"2.2\"},\"end\":11859,\"start\":11814},{\"attributes\":{\"n\":\"3\"},\"end\":16847,\"start\":16836},{\"end\":16876,\"start\":16850},{\"attributes\":{\"n\":\"3.1\"},\"end\":17304,\"start\":17278},{\"attributes\":{\"n\":\"3.2\"},\"end\":19391,\"start\":19362},{\"attributes\":{\"n\":\"3.3\"},\"end\":21426,\"start\":21395},{\"attributes\":{\"n\":\"3.4\"},\"end\":23906,\"start\":23856},{\"attributes\":{\"n\":\"4\"},\"end\":26742,\"start\":26711},{\"end\":26755,\"start\":26745},{\"attributes\":{\"n\":\"4.1\"},\"end\":27077,\"start\":27033},{\"attributes\":{\"n\":\"4.1.1\"},\"end\":28294,\"start\":28260},{\"attributes\":{\"n\":\"4.1.2\"},\"end\":30907,\"start\":30883},{\"attributes\":{\"n\":\"4.2\"},\"end\":33171,\"start\":33121},{\"attributes\":{\"n\":\"5\"},\"end\":34411,\"start\":34343},{\"end\":37782,\"start\":37780},{\"attributes\":{\"n\":\"6\"},\"end\":38689,\"start\":38668},{\"attributes\":{\"n\":\"6.1\"},\"end\":39303,\"start\":39246},{\"attributes\":{\"n\":\"6.2\"},\"end\":42533,\"start\":42507},{\"attributes\":{\"n\":\"6.2.1\"},\"end\":42841,\"start\":42833},{\"attributes\":{\"n\":\"6.2.2\"},\"end\":44770,\"start\":44752},{\"attributes\":{\"n\":\"6.2.3\"},\"end\":47678,\"start\":47637},{\"attributes\":{\"n\":\"6.2.4\"},\"end\":50208,\"start\":50175},{\"end\":51066,\"start\":51049},{\"end\":51094,\"start\":51086},{\"end\":51866,\"start\":51858},{\"end\":52723,\"start\":52714},{\"end\":52809,\"start\":52800},{\"end\":52890,\"start\":52881},{\"end\":55268,\"start\":55259},{\"end\":56218,\"start\":56209}]", "table": "[{\"end\":55257,\"start\":53076},{\"end\":56207,\"start\":55334},{\"end\":56359,\"start\":56305}]", "figure_caption": "[{\"end\":51856,\"start\":51096},{\"end\":52303,\"start\":51868},{\"end\":52712,\"start\":52306},{\"end\":52798,\"start\":52725},{\"end\":52879,\"start\":52811},{\"end\":53076,\"start\":52892},{\"end\":55334,\"start\":55270},{\"end\":56305,\"start\":56220}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":4547,\"start\":4539},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14093,\"start\":14085},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15612,\"start\":15604},{\"end\":39543,\"start\":39532},{\"end\":42161,\"start\":42152},{\"end\":42169,\"start\":42163},{\"end\":43209,\"start\":43201},{\"end\":45303,\"start\":45297}]", "bib_author_first_name": "[{\"end\":59402,\"start\":59401},{\"end\":59404,\"start\":59403},{\"end\":59413,\"start\":59412},{\"end\":59415,\"start\":59414},{\"end\":59427,\"start\":59426},{\"end\":59429,\"start\":59428},{\"end\":59738,\"start\":59737},{\"end\":59751,\"start\":59750},{\"end\":59758,\"start\":59757},{\"end\":59764,\"start\":59763},{\"end\":60099,\"start\":60095},{\"end\":60110,\"start\":60109},{\"end\":60379,\"start\":60378},{\"end\":60381,\"start\":60380},{\"end\":60522,\"start\":60521},{\"end\":60533,\"start\":60532},{\"end\":60542,\"start\":60541},{\"end\":60551,\"start\":60550},{\"end\":60845,\"start\":60844},{\"end\":60856,\"start\":60855},{\"end\":60872,\"start\":60871},{\"end\":60881,\"start\":60880},{\"end\":61212,\"start\":61211},{\"end\":61223,\"start\":61222},{\"end\":61225,\"start\":61224},{\"end\":61235,\"start\":61234},{\"end\":61642,\"start\":61641},{\"end\":61653,\"start\":61652},{\"end\":61663,\"start\":61662},{\"end\":61941,\"start\":61940},{\"end\":61953,\"start\":61952},{\"end\":62317,\"start\":62316},{\"end\":62319,\"start\":62318},{\"end\":62330,\"start\":62329},{\"end\":62340,\"start\":62339},{\"end\":62354,\"start\":62353},{\"end\":62665,\"start\":62664},{\"end\":62667,\"start\":62666},{\"end\":62678,\"start\":62677},{\"end\":62680,\"start\":62679},{\"end\":62691,\"start\":62690},{\"end\":62700,\"start\":62699},{\"end\":62702,\"start\":62701},{\"end\":62708,\"start\":62707},{\"end\":62710,\"start\":62709},{\"end\":63054,\"start\":63053},{\"end\":63065,\"start\":63064},{\"end\":63076,\"start\":63075},{\"end\":63321,\"start\":63320},{\"end\":63342,\"start\":63341},{\"end\":63598,\"start\":63597},{\"end\":63608,\"start\":63607},{\"end\":63619,\"start\":63618},{\"end\":64033,\"start\":64032},{\"end\":64269,\"start\":64268},{\"end\":64279,\"start\":64278},{\"end\":64501,\"start\":64500},{\"end\":64511,\"start\":64510},{\"end\":64747,\"start\":64746},{\"end\":65007,\"start\":65006},{\"end\":65018,\"start\":65017},{\"end\":65025,\"start\":65024},{\"end\":65036,\"start\":65035},{\"end\":65047,\"start\":65046},{\"end\":65056,\"start\":65055},{\"end\":65065,\"start\":65064},{\"end\":65408,\"start\":65407},{\"end\":65421,\"start\":65420},{\"end\":65434,\"start\":65433},{\"end\":65443,\"start\":65442},{\"end\":65455,\"start\":65451},{\"end\":65759,\"start\":65758},{\"end\":65769,\"start\":65768},{\"end\":65771,\"start\":65770},{\"end\":66017,\"start\":66016},{\"end\":66028,\"start\":66027},{\"end\":66039,\"start\":66038},{\"end\":66053,\"start\":66052},{\"end\":66424,\"start\":66423},{\"end\":66432,\"start\":66431},{\"end\":66439,\"start\":66438},{\"end\":66446,\"start\":66445},{\"end\":66785,\"start\":66784},{\"end\":66796,\"start\":66795},{\"end\":66802,\"start\":66801},{\"end\":67090,\"start\":67089},{\"end\":67101,\"start\":67100},{\"end\":67110,\"start\":67109},{\"end\":67367,\"start\":67366},{\"end\":67374,\"start\":67373},{\"end\":67381,\"start\":67380},{\"end\":67756,\"start\":67755},{\"end\":67767,\"start\":67766},{\"end\":67777,\"start\":67776},{\"end\":67788,\"start\":67787},{\"end\":67798,\"start\":67797},{\"end\":68400,\"start\":68396},{\"end\":68408,\"start\":68407},{\"end\":68415,\"start\":68414},{\"end\":68417,\"start\":68416},{\"end\":68427,\"start\":68423},{\"end\":68739,\"start\":68738},{\"end\":68975,\"start\":68974},{\"end\":69285,\"start\":69284},{\"end\":69294,\"start\":69293},{\"end\":69304,\"start\":69303},{\"end\":69691,\"start\":69690},{\"end\":69700,\"start\":69699},{\"end\":69711,\"start\":69708},{\"end\":69715,\"start\":69714},{\"end\":70039,\"start\":70038},{\"end\":70047,\"start\":70046},{\"end\":70055,\"start\":70054},{\"end\":70063,\"start\":70062},{\"end\":70070,\"start\":70069},{\"end\":70402,\"start\":70401},{\"end\":70409,\"start\":70408},{\"end\":70750,\"start\":70749},{\"end\":70761,\"start\":70760},{\"end\":70770,\"start\":70769},{\"end\":71011,\"start\":71010},{\"end\":71013,\"start\":71012},{\"end\":71020,\"start\":71019},{\"end\":71506,\"start\":71505},{\"end\":71508,\"start\":71507},{\"end\":71517,\"start\":71516},{\"end\":71528,\"start\":71527},{\"end\":71534,\"start\":71533},{\"end\":71876,\"start\":71875},{\"end\":71885,\"start\":71884},{\"end\":71894,\"start\":71893},{\"end\":71903,\"start\":71902},{\"end\":72256,\"start\":72255},{\"end\":72266,\"start\":72265},{\"end\":72276,\"start\":72275},{\"end\":72565,\"start\":72564},{\"end\":72575,\"start\":72574},{\"end\":72584,\"start\":72583},{\"end\":72593,\"start\":72592},{\"end\":72880,\"start\":72879},{\"end\":73068,\"start\":73067},{\"end\":73074,\"start\":73073},{\"end\":73081,\"start\":73080},{\"end\":73397,\"start\":73396},{\"end\":73408,\"start\":73407},{\"end\":73420,\"start\":73419},{\"end\":73432,\"start\":73431},{\"end\":73785,\"start\":73784},{\"end\":73797,\"start\":73796},{\"end\":73809,\"start\":73808},{\"end\":73820,\"start\":73819},{\"end\":73830,\"start\":73829},{\"end\":74164,\"start\":74163},{\"end\":74172,\"start\":74171},{\"end\":74415,\"start\":74414},{\"end\":74423,\"start\":74422},{\"end\":74434,\"start\":74433},{\"end\":74451,\"start\":74450},{\"end\":74863,\"start\":74862},{\"end\":75015,\"start\":75014},{\"end\":75023,\"start\":75022},{\"end\":75033,\"start\":75032},{\"end\":75304,\"start\":75303},{\"end\":75312,\"start\":75311},{\"end\":75939,\"start\":75938},{\"end\":75948,\"start\":75947},{\"end\":75950,\"start\":75949},{\"end\":75959,\"start\":75958},{\"end\":75967,\"start\":75966},{\"end\":75969,\"start\":75968},{\"end\":75977,\"start\":75976},{\"end\":75985,\"start\":75984},{\"end\":76304,\"start\":76303},{\"end\":76316,\"start\":76312},{\"end\":76323,\"start\":76322},{\"end\":76336,\"start\":76335}]", "bib_author_last_name": "[{\"end\":59410,\"start\":59405},{\"end\":59424,\"start\":59416},{\"end\":59435,\"start\":59430},{\"end\":59748,\"start\":59739},{\"end\":59755,\"start\":59752},{\"end\":59761,\"start\":59759},{\"end\":59768,\"start\":59765},{\"end\":60107,\"start\":60100},{\"end\":60118,\"start\":60111},{\"end\":60391,\"start\":60382},{\"end\":60530,\"start\":60523},{\"end\":60539,\"start\":60534},{\"end\":60548,\"start\":60543},{\"end\":60559,\"start\":60552},{\"end\":60853,\"start\":60846},{\"end\":60869,\"start\":60857},{\"end\":60878,\"start\":60873},{\"end\":60890,\"start\":60882},{\"end\":61220,\"start\":61213},{\"end\":61232,\"start\":61226},{\"end\":61241,\"start\":61236},{\"end\":61650,\"start\":61643},{\"end\":61660,\"start\":61654},{\"end\":61669,\"start\":61664},{\"end\":61950,\"start\":61942},{\"end\":61964,\"start\":61954},{\"end\":62327,\"start\":62320},{\"end\":62337,\"start\":62331},{\"end\":62351,\"start\":62341},{\"end\":62361,\"start\":62355},{\"end\":62675,\"start\":62668},{\"end\":62688,\"start\":62681},{\"end\":62697,\"start\":62692},{\"end\":62705,\"start\":62703},{\"end\":62716,\"start\":62711},{\"end\":63062,\"start\":63055},{\"end\":63073,\"start\":63066},{\"end\":63082,\"start\":63077},{\"end\":63339,\"start\":63322},{\"end\":63347,\"start\":63343},{\"end\":63605,\"start\":63599},{\"end\":63616,\"start\":63609},{\"end\":63624,\"start\":63620},{\"end\":64040,\"start\":64034},{\"end\":64276,\"start\":64270},{\"end\":64284,\"start\":64280},{\"end\":64508,\"start\":64502},{\"end\":64518,\"start\":64512},{\"end\":64753,\"start\":64748},{\"end\":65015,\"start\":65008},{\"end\":65022,\"start\":65019},{\"end\":65033,\"start\":65026},{\"end\":65044,\"start\":65037},{\"end\":65053,\"start\":65048},{\"end\":65062,\"start\":65057},{\"end\":65073,\"start\":65066},{\"end\":65418,\"start\":65409},{\"end\":65431,\"start\":65422},{\"end\":65440,\"start\":65435},{\"end\":65449,\"start\":65444},{\"end\":65461,\"start\":65456},{\"end\":65766,\"start\":65760},{\"end\":65778,\"start\":65772},{\"end\":66025,\"start\":66018},{\"end\":66036,\"start\":66029},{\"end\":66050,\"start\":66040},{\"end\":66061,\"start\":66054},{\"end\":66429,\"start\":66425},{\"end\":66436,\"start\":66433},{\"end\":66443,\"start\":66440},{\"end\":66454,\"start\":66447},{\"end\":66793,\"start\":66786},{\"end\":66799,\"start\":66797},{\"end\":66812,\"start\":66803},{\"end\":67098,\"start\":67091},{\"end\":67107,\"start\":67102},{\"end\":67117,\"start\":67111},{\"end\":67371,\"start\":67368},{\"end\":67378,\"start\":67375},{\"end\":67386,\"start\":67382},{\"end\":67764,\"start\":67757},{\"end\":67774,\"start\":67768},{\"end\":67785,\"start\":67778},{\"end\":67795,\"start\":67789},{\"end\":67806,\"start\":67799},{\"end\":68405,\"start\":68401},{\"end\":68412,\"start\":68409},{\"end\":68421,\"start\":68418},{\"end\":68433,\"start\":68428},{\"end\":68751,\"start\":68740},{\"end\":68982,\"start\":68976},{\"end\":69291,\"start\":69286},{\"end\":69301,\"start\":69295},{\"end\":69312,\"start\":69305},{\"end\":69697,\"start\":69692},{\"end\":69706,\"start\":69701},{\"end\":69722,\"start\":69716},{\"end\":70044,\"start\":70040},{\"end\":70052,\"start\":70048},{\"end\":70060,\"start\":70056},{\"end\":70067,\"start\":70064},{\"end\":70073,\"start\":70071},{\"end\":70406,\"start\":70403},{\"end\":70417,\"start\":70410},{\"end\":70758,\"start\":70751},{\"end\":70767,\"start\":70762},{\"end\":70783,\"start\":70771},{\"end\":71017,\"start\":71014},{\"end\":71025,\"start\":71021},{\"end\":71514,\"start\":71509},{\"end\":71525,\"start\":71518},{\"end\":71531,\"start\":71529},{\"end\":71544,\"start\":71535},{\"end\":71882,\"start\":71877},{\"end\":71891,\"start\":71886},{\"end\":71900,\"start\":71895},{\"end\":71910,\"start\":71904},{\"end\":72263,\"start\":72257},{\"end\":72273,\"start\":72267},{\"end\":72283,\"start\":72277},{\"end\":72572,\"start\":72566},{\"end\":72581,\"start\":72576},{\"end\":72590,\"start\":72585},{\"end\":72601,\"start\":72594},{\"end\":72893,\"start\":72881},{\"end\":73071,\"start\":73069},{\"end\":73078,\"start\":73075},{\"end\":73086,\"start\":73082},{\"end\":73405,\"start\":73398},{\"end\":73417,\"start\":73409},{\"end\":73429,\"start\":73421},{\"end\":73441,\"start\":73433},{\"end\":73794,\"start\":73786},{\"end\":73806,\"start\":73798},{\"end\":73817,\"start\":73810},{\"end\":73827,\"start\":73821},{\"end\":73839,\"start\":73831},{\"end\":74169,\"start\":74165},{\"end\":74184,\"start\":74173},{\"end\":74420,\"start\":74416},{\"end\":74431,\"start\":74424},{\"end\":74448,\"start\":74435},{\"end\":74458,\"start\":74452},{\"end\":74871,\"start\":74864},{\"end\":75020,\"start\":75016},{\"end\":75030,\"start\":75024},{\"end\":75043,\"start\":75034},{\"end\":75309,\"start\":75305},{\"end\":75322,\"start\":75313},{\"end\":75945,\"start\":75940},{\"end\":75956,\"start\":75951},{\"end\":75964,\"start\":75960},{\"end\":75974,\"start\":75970},{\"end\":75982,\"start\":75978},{\"end\":75992,\"start\":75986},{\"end\":76310,\"start\":76305},{\"end\":76320,\"start\":76317},{\"end\":76333,\"start\":76324},{\"end\":76345,\"start\":76337}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":59689,\"start\":59401},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":7505943},\"end\":60004,\"start\":59691},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":1100384},\"end\":60326,\"start\":60006},{\"attributes\":{\"id\":\"b3\"},\"end\":60465,\"start\":60328},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1907942},\"end\":60782,\"start\":60467},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":6966533},\"end\":61114,\"start\":60784},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":33129902},\"end\":61511,\"start\":61116},{\"attributes\":{\"id\":\"b7\"},\"end\":61841,\"start\":61513},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":14739721},\"end\":62269,\"start\":61843},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":15399730},\"end\":62637,\"start\":62271},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":9161996},\"end\":62981,\"start\":62639},{\"attributes\":{\"doi\":\"hal-00987292\",\"id\":\"b11\",\"matched_paper_id\":118063128},\"end\":63260,\"start\":62983},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":7026600},\"end\":63541,\"start\":63262},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":18945224},\"end\":63960,\"start\":63543},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":61675022},\"end\":64242,\"start\":63962},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":6818778},\"end\":64453,\"start\":64244},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":16786361},\"end\":64706,\"start\":64455},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":5360764},\"end\":64925,\"start\":64708},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":6161478},\"end\":65365,\"start\":64927},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":3781737},\"end\":65716,\"start\":65367},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":17286706},\"end\":65925,\"start\":65718},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":13474174},\"end\":66364,\"start\":65927},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":6742009},\"end\":66714,\"start\":66366},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":10337178},\"end\":67050,\"start\":66716},{\"attributes\":{\"doi\":\"CNS-TR-2007-001\",\"id\":\"b24\"},\"end\":67325,\"start\":67052},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":2913820},\"end\":67691,\"start\":67327},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":16496273},\"end\":68068,\"start\":67693},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":16496273},\"end\":68332,\"start\":68070},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":7176806},\"end\":68704,\"start\":68334},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":122853046},\"end\":68912,\"start\":68706},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":5852614},\"end\":69194,\"start\":68914},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":7419723},\"end\":69625,\"start\":69196},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":7166661},\"end\":69974,\"start\":69627},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":13798326},\"end\":70316,\"start\":69976},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":903453},\"end\":70696,\"start\":70318},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":6178817},\"end\":70977,\"start\":70698},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":740063},\"end\":71230,\"start\":70979},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":788838},\"end\":71445,\"start\":71232},{\"attributes\":{\"id\":\"b38\"},\"end\":71746,\"start\":71447},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":3571438},\"end\":72188,\"start\":71748},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":18648233},\"end\":72514,\"start\":72190},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":7534823},\"end\":72831,\"start\":72516},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":123894440},\"end\":72993,\"start\":72833},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":206742596},\"end\":73340,\"start\":72995},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":15087216},\"end\":73681,\"start\":73342},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":9133542},\"end\":74112,\"start\":73683},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":7927986},\"end\":74328,\"start\":74114},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":3208621},\"end\":74776,\"start\":74330},{\"attributes\":{\"id\":\"b48\"},\"end\":74992,\"start\":74778},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":13515324},\"end\":75258,\"start\":74994},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":59769929},\"end\":75563,\"start\":75260},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":10360019},\"end\":75868,\"start\":75565},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":1592117},\"end\":76246,\"start\":75870},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":5801986},\"end\":76599,\"start\":76248}]", "bib_title": "[{\"end\":59735,\"start\":59691},{\"end\":60093,\"start\":60006},{\"end\":60519,\"start\":60467},{\"end\":60842,\"start\":60784},{\"end\":61209,\"start\":61116},{\"end\":61938,\"start\":61843},{\"end\":62314,\"start\":62271},{\"end\":62662,\"start\":62639},{\"end\":63051,\"start\":62983},{\"end\":63318,\"start\":63262},{\"end\":63595,\"start\":63543},{\"end\":64030,\"start\":63962},{\"end\":64266,\"start\":64244},{\"end\":64498,\"start\":64455},{\"end\":64744,\"start\":64708},{\"end\":65004,\"start\":64927},{\"end\":65405,\"start\":65367},{\"end\":65756,\"start\":65718},{\"end\":66014,\"start\":65927},{\"end\":66421,\"start\":66366},{\"end\":66782,\"start\":66716},{\"end\":67364,\"start\":67327},{\"end\":67753,\"start\":67693},{\"end\":68130,\"start\":68070},{\"end\":68394,\"start\":68334},{\"end\":68736,\"start\":68706},{\"end\":68972,\"start\":68914},{\"end\":69282,\"start\":69196},{\"end\":69688,\"start\":69627},{\"end\":70036,\"start\":69976},{\"end\":70399,\"start\":70318},{\"end\":70747,\"start\":70698},{\"end\":71008,\"start\":70979},{\"end\":71281,\"start\":71232},{\"end\":71503,\"start\":71447},{\"end\":71873,\"start\":71748},{\"end\":72253,\"start\":72190},{\"end\":72562,\"start\":72516},{\"end\":72877,\"start\":72833},{\"end\":73065,\"start\":72995},{\"end\":73394,\"start\":73342},{\"end\":73782,\"start\":73683},{\"end\":74161,\"start\":74114},{\"end\":74412,\"start\":74330},{\"end\":75012,\"start\":74994},{\"end\":75301,\"start\":75260},{\"end\":75621,\"start\":75565},{\"end\":75936,\"start\":75870},{\"end\":76301,\"start\":76248}]", "bib_author": "[{\"end\":59412,\"start\":59401},{\"end\":59426,\"start\":59412},{\"end\":59437,\"start\":59426},{\"end\":59750,\"start\":59737},{\"end\":59757,\"start\":59750},{\"end\":59763,\"start\":59757},{\"end\":59770,\"start\":59763},{\"end\":60109,\"start\":60095},{\"end\":60120,\"start\":60109},{\"end\":60393,\"start\":60378},{\"end\":60532,\"start\":60521},{\"end\":60541,\"start\":60532},{\"end\":60550,\"start\":60541},{\"end\":60561,\"start\":60550},{\"end\":60855,\"start\":60844},{\"end\":60871,\"start\":60855},{\"end\":60880,\"start\":60871},{\"end\":60892,\"start\":60880},{\"end\":61222,\"start\":61211},{\"end\":61234,\"start\":61222},{\"end\":61243,\"start\":61234},{\"end\":61652,\"start\":61641},{\"end\":61662,\"start\":61652},{\"end\":61671,\"start\":61662},{\"end\":61952,\"start\":61940},{\"end\":61966,\"start\":61952},{\"end\":62329,\"start\":62316},{\"end\":62339,\"start\":62329},{\"end\":62353,\"start\":62339},{\"end\":62363,\"start\":62353},{\"end\":62677,\"start\":62664},{\"end\":62690,\"start\":62677},{\"end\":62699,\"start\":62690},{\"end\":62707,\"start\":62699},{\"end\":62718,\"start\":62707},{\"end\":63064,\"start\":63053},{\"end\":63075,\"start\":63064},{\"end\":63084,\"start\":63075},{\"end\":63341,\"start\":63320},{\"end\":63349,\"start\":63341},{\"end\":63607,\"start\":63597},{\"end\":63618,\"start\":63607},{\"end\":63626,\"start\":63618},{\"end\":64042,\"start\":64032},{\"end\":64278,\"start\":64268},{\"end\":64286,\"start\":64278},{\"end\":64510,\"start\":64500},{\"end\":64520,\"start\":64510},{\"end\":64755,\"start\":64746},{\"end\":65017,\"start\":65006},{\"end\":65024,\"start\":65017},{\"end\":65035,\"start\":65024},{\"end\":65046,\"start\":65035},{\"end\":65055,\"start\":65046},{\"end\":65064,\"start\":65055},{\"end\":65075,\"start\":65064},{\"end\":65420,\"start\":65407},{\"end\":65433,\"start\":65420},{\"end\":65442,\"start\":65433},{\"end\":65451,\"start\":65442},{\"end\":65463,\"start\":65451},{\"end\":65768,\"start\":65758},{\"end\":65780,\"start\":65768},{\"end\":66027,\"start\":66016},{\"end\":66038,\"start\":66027},{\"end\":66052,\"start\":66038},{\"end\":66063,\"start\":66052},{\"end\":66431,\"start\":66423},{\"end\":66438,\"start\":66431},{\"end\":66445,\"start\":66438},{\"end\":66456,\"start\":66445},{\"end\":66795,\"start\":66784},{\"end\":66801,\"start\":66795},{\"end\":66814,\"start\":66801},{\"end\":67100,\"start\":67089},{\"end\":67109,\"start\":67100},{\"end\":67119,\"start\":67109},{\"end\":67373,\"start\":67366},{\"end\":67380,\"start\":67373},{\"end\":67388,\"start\":67380},{\"end\":67766,\"start\":67755},{\"end\":67776,\"start\":67766},{\"end\":67787,\"start\":67776},{\"end\":67797,\"start\":67787},{\"end\":67808,\"start\":67797},{\"end\":68407,\"start\":68396},{\"end\":68414,\"start\":68407},{\"end\":68423,\"start\":68414},{\"end\":68435,\"start\":68423},{\"end\":68753,\"start\":68738},{\"end\":68984,\"start\":68974},{\"end\":69293,\"start\":69284},{\"end\":69303,\"start\":69293},{\"end\":69314,\"start\":69303},{\"end\":69699,\"start\":69690},{\"end\":69708,\"start\":69699},{\"end\":69714,\"start\":69708},{\"end\":69724,\"start\":69714},{\"end\":70046,\"start\":70038},{\"end\":70054,\"start\":70046},{\"end\":70062,\"start\":70054},{\"end\":70069,\"start\":70062},{\"end\":70075,\"start\":70069},{\"end\":70408,\"start\":70401},{\"end\":70419,\"start\":70408},{\"end\":70760,\"start\":70749},{\"end\":70769,\"start\":70760},{\"end\":70785,\"start\":70769},{\"end\":71019,\"start\":71010},{\"end\":71027,\"start\":71019},{\"end\":71516,\"start\":71505},{\"end\":71527,\"start\":71516},{\"end\":71533,\"start\":71527},{\"end\":71546,\"start\":71533},{\"end\":71884,\"start\":71875},{\"end\":71893,\"start\":71884},{\"end\":71902,\"start\":71893},{\"end\":71912,\"start\":71902},{\"end\":72265,\"start\":72255},{\"end\":72275,\"start\":72265},{\"end\":72285,\"start\":72275},{\"end\":72574,\"start\":72564},{\"end\":72583,\"start\":72574},{\"end\":72592,\"start\":72583},{\"end\":72603,\"start\":72592},{\"end\":72895,\"start\":72879},{\"end\":73073,\"start\":73067},{\"end\":73080,\"start\":73073},{\"end\":73088,\"start\":73080},{\"end\":73407,\"start\":73396},{\"end\":73419,\"start\":73407},{\"end\":73431,\"start\":73419},{\"end\":73443,\"start\":73431},{\"end\":73796,\"start\":73784},{\"end\":73808,\"start\":73796},{\"end\":73819,\"start\":73808},{\"end\":73829,\"start\":73819},{\"end\":73841,\"start\":73829},{\"end\":74171,\"start\":74163},{\"end\":74186,\"start\":74171},{\"end\":74422,\"start\":74414},{\"end\":74433,\"start\":74422},{\"end\":74450,\"start\":74433},{\"end\":74460,\"start\":74450},{\"end\":74873,\"start\":74862},{\"end\":75022,\"start\":75014},{\"end\":75032,\"start\":75022},{\"end\":75045,\"start\":75032},{\"end\":75311,\"start\":75303},{\"end\":75324,\"start\":75311},{\"end\":75947,\"start\":75938},{\"end\":75958,\"start\":75947},{\"end\":75966,\"start\":75958},{\"end\":75976,\"start\":75966},{\"end\":75984,\"start\":75976},{\"end\":75994,\"start\":75984},{\"end\":76312,\"start\":76303},{\"end\":76322,\"start\":76312},{\"end\":76335,\"start\":76322},{\"end\":76347,\"start\":76335}]", "bib_venue": "[{\"end\":59517,\"start\":59490},{\"end\":66128,\"start\":66116},{\"end\":69401,\"start\":69381},{\"end\":75403,\"start\":75391},{\"end\":59488,\"start\":59437},{\"end\":59829,\"start\":59770},{\"end\":60141,\"start\":60120},{\"end\":60376,\"start\":60328},{\"end\":60603,\"start\":60561},{\"end\":60919,\"start\":60892},{\"end\":61286,\"start\":61243},{\"end\":61639,\"start\":61513},{\"end\":62028,\"start\":61966},{\"end\":62425,\"start\":62363},{\"end\":62780,\"start\":62718},{\"end\":63101,\"start\":63096},{\"end\":63391,\"start\":63349},{\"end\":63741,\"start\":63626},{\"end\":64086,\"start\":64042},{\"end\":64322,\"start\":64286},{\"end\":64571,\"start\":64520},{\"end\":64807,\"start\":64755},{\"end\":65126,\"start\":65075},{\"end\":65523,\"start\":65463},{\"end\":65796,\"start\":65780},{\"end\":66114,\"start\":66063},{\"end\":66521,\"start\":66456},{\"end\":66864,\"start\":66814},{\"end\":67087,\"start\":67052},{\"end\":67457,\"start\":67388},{\"end\":67867,\"start\":67808},{\"end\":68191,\"start\":68132},{\"end\":68500,\"start\":68435},{\"end\":68790,\"start\":68753},{\"end\":69032,\"start\":68984},{\"end\":69379,\"start\":69314},{\"end\":69789,\"start\":69724},{\"end\":70125,\"start\":70075},{\"end\":70481,\"start\":70419},{\"end\":70820,\"start\":70785},{\"end\":71078,\"start\":71027},{\"end\":71319,\"start\":71283},{\"end\":71577,\"start\":71546},{\"end\":71945,\"start\":71912},{\"end\":72335,\"start\":72285},{\"end\":72653,\"start\":72603},{\"end\":72904,\"start\":72895},{\"end\":73139,\"start\":73088},{\"end\":73494,\"start\":73443},{\"end\":73885,\"start\":73841},{\"end\":74194,\"start\":74186},{\"end\":74541,\"start\":74460},{\"end\":74860,\"start\":74778},{\"end\":75087,\"start\":75045},{\"end\":75389,\"start\":75324},{\"end\":75688,\"start\":75623},{\"end\":76045,\"start\":75994},{\"end\":76401,\"start\":76347}]"}}}, "year": 2023, "month": 12, "day": 17}