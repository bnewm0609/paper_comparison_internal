{"id": 256805237, "updated": "2023-10-05 04:12:38.378", "metadata": {"title": "Self-Supervised Image Denoising for Real-World Images with Context-aware Transformer", "authors": "[{\"first\":\"Dan\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Fangfang\",\"last\":\"Zhou\",\"middle\":[]}]", "venue": "IEEE Access, vol. 11, pp. 14340-14349, 2023", "journal": "IEEE Access", "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "In recent years, the development of deep learning has been pushing image denoising to a new level. Among them, self-supervised denoising is increasingly popular because it does not require any prior knowledge. Most of the existing self-supervised methods are based on convolutional neural networks (CNN), which are restricted by the locality of the receptive field and would cause color shifts or textures loss. In this paper, we propose a novel Denoise Transformer for real-world image denoising, which is mainly constructed with Context-aware Denoise Transformer (CADT) units and Secondary Noise Extractor (SNE) block. CADT is designed as a dual-branch structure, where the global branch uses a window-based Transformer encoder to extract the global information, while the local branch focuses on the extraction of local features with small receptive field. By incorporating CADT as basic components, we build a hierarchical network to directly learn the noise distribution information through residual learning and obtain the first stage denoised output. Then, we design SNE in low computation for secondary global noise extraction. Finally the blind spots are collected from the Denoise Transformer output and reconstructed, forming the final denoised image. Extensive experiments on the real-world SIDD benchmark achieve 50.62/0.990 for PSNR/SSIM, which is competitive with the current state-of-the-art method and only 0.17/0.001 lower. Visual comparisons on public sRGB, Raw-RGB and greyscale datasets prove that our proposed Denoise Transformer has a competitive performance, especially on blurred textures and low-light images, without using additional knowledge, e.g., noise level or noise type, regarding the underlying unknown noise.", "fields_of_study": "[\"Computer Science\",\"Engineering\"]", "external_ids": {"arxiv": "2304.01627", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2304-01627", "doi": "10.1109/access.2023.3243829"}}, "content": {"source": {"pdf_hash": "c87b5ecc329cf0fdf26418c35d706af23cdc98f1", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2304.01627v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": "CCBY", "open_access_url": "https://ieeexplore.ieee.org/ielx7/6287639/10005208/10041892.pdf", "status": "GOLD"}}, "grobid": {"id": "07cc4a02fa33222dd36c594024b12c4277f79c7a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c87b5ecc329cf0fdf26418c35d706af23cdc98f1.txt", "contents": "\nSelf-Supervised Image Denoising for Real-World Images with Context-aware Transformer\n\n\nDan Zhang \nPuDong New Area\nSenslab Technology Co\n201203Ltd, ShanghaiChina\n\nFangfang Zhou \nSenslab Technology Co., Ltd, PuDong New Area\n201203ShanghaiChina\n\nDan Zhang \nSelf-Supervised Image Denoising for Real-World Images with Context-aware Transformer\n10.1109/ACCESS.2023.3243829Received 25 January 2023, accepted 3 February 2023, date of publication 10 February 2023, date of current version 15 February 2023.Corresponding author:INDEX TERMS Image denoisingself-supervisedreal-worldTransformerdual-branch\nIn recent years, the development of deep learning has been pushing image denoising to a new level. Among them, self-supervised denoising is increasingly popular because it does not require any prior knowledge. Most of the existing self-supervised methods are based on convolutional neural networks (CNN), which are restricted by the locality of the receptive field and would cause color shifts or textures loss. In this paper, we propose a novel Denoise Transformer for real-world image denoising, which is mainly constructed with Context-aware Denoise Transformer (CADT) units and Secondary Noise Extractor (SNE) block. CADT is designed as a dual-branch structure, where the global branch uses a window-based Transformer encoder to extract the global information, while the local branch focuses on the extraction of local features with small receptive field. By incorporating CADT as basic components, we build a hierarchical network to directly learn the noise distribution information through residual learning and obtain the first stage denoised output. Then, we design SNE in low computation for secondary global noise extraction. Finally the blind spots are collected from the Denoise Transformer output and reconstructed, forming the final denoised image. Extensive experiments on the real-world SIDD benchmark achieve 50.62/0.990 for PSNR/SSIM, which is competitive with the current state-of-the-art method and only 0.17/0.001 lower. Visual comparisons on public sRGB, Raw-RGB and greyscale datasets prove that our proposed Denoise Transformer has a competitive performance, especially on blurred textures and lowlight images, without using additional knowledge, e.g., noise level or noise type, regarding the underlying unknown noise.\n\nI. INTRODUCTION\n\nDuring the image acquisition process of image sensors, CCD and CMOS, various noises are introduced due to the influence of sensor material properties, working environment, electronic components and circuit structure.In addition, due to the imperfection of transmission media and recording equipment, digital images are often attacked by various noises. There are basically four types of common noise in images: Gaussian noise, Poisson noise, multiplicative noise, and salt and pepper noise. Image denoising is an inevitable step in image processing, and its denoising effect has a huge impact on the subsequent image processing process. Traditional image denoising algorithms [1], [2] are slow and less robust. With the development of deep learning, image denoising algorithms have made great progress. Although some progress has been made in traditional methods [3], supervised denoising models [4]- [7] have relatively better denoising effects on public datasets. However, supervised image denoising requires noisy-clean data pairs, which are very difficult to obtain in practical applications. The most common approach is to add Additive White Gaussian Noise (AWGN) or other simulated realworld noise to a clean image and artificially synthesise a noisy image to form noisy-clean pairs [4], [7]- [10]. However, there is an unavoidable gap between the noise synthesised by noise modelling and the real-world noise. Therefore, the denoising performance of this synthesised type of denoising model will be greatly reduced when denoising real-world images. Under these circumstances, many self-supervised training VOLUME 4, 2016 1 arXiv:2304.01627v1 [cs.CV] 4 Apr 2023 methods [11]- [16] have emerged that do not require clean images. Noise2Noise [16] trains the model with only two noisy images and achieves denoising performance comparable to other supervised algorithms. However, it requires two fully aligned noisy images, which are also difficult to obtain in practical applications. Then Noiser2noise [17], NAC [18] are proposed to add the same type of noise to the existing noisy image to form a noiser-noise pair, which means that they need to know the exact type of noise in the image as a prior. IDR [12] adopts an iterative approach, taking the noisy images as inputs to the existing denoising model trained with noiser-noise pairs and treating the trained output as the next round of optimisation targets to further refine the denoising model. In this way, the denoising model is optimised by iteration, which can easily cause the final denoised image to be unduly over-smoothened. Noise2Void [14] proposes a blind spot network (BSN) denoising method based on the assumption that pixel signals in the image are spatial correlated, while noise signals are spatially independent with zero mean. In recent years, many publications [14], [19]- [21] have proved that BSN is very effective in denoising synthetic noise. However, noise in real-world is usually spatially continuous. In order to break the spatial connection of noise, AP-BSN [22] performs 5 times pixel-shuffle downsampling (PD) on the input before training. What's more, AP-BSN [22] adoptes centre-masked convolution kernel and dilated convolution layer (DCL) to obtain the effect of blind spots during forward propagation. However, if the step of PD is too large, it will cause irreparable damage to the spatial information, but if it is too small, it will not be able to break the spatial connection of the noise [22].\n\nMost of state-of-the-art BSN denoising methods [13]- [15], [20]- [22] ultilize CNN-based networks, which are restricted by the locality of the receptive field. Therefore, they are difficult to denoise in the presence of high noise or noisy blurred textures, while repeated training leads to oversmoothing of images. In this case, we consider it is comprehensive to combine local features with the global information extracted by Transformer. The addition of global information efficiently compensates for the defects of local receptive field extraction features. In this paper, we propose the Contextaware Denoise Transformer for self-supervised denoising in two stages. For the first stage, we use a hierarchical CADTs to directly extract noise information, which will be subtracted from the original image to obtain the first denoised result. For the second stage, we use the SNE module to extract the global noise on the first denoised result and the final denoised result can be obtained by residual learning. Extensive experiments demonstrate the effectiveness and superiority of our Denoise Transformer.\n\nThe main contributions of our work are as follows: 1. We are the first to propose a Transformer-based network suitable for self-supervised image denoising.\n\n2. We propose a new structural unit CADT that can fuse global information and local features.\n\n3. We add a secondary global noise extractor to realise two stage denoising. 4. Our Denoise Transformer is competitive with current state-of-the-art methods in self-supervised real-world image denoising.\n\n\nII. RELATED WORK\n\n\nA. SUPERVISED IMAGE DENOISING BASED ON CNN\n\nZhang et al. [4] first proposed image denoising based on deep learning. They proposed DnCNN trained with generated noisy-clean pairs by artificially adding AWGN to clean images. Then, U-Net [23] became a more widely used denoising baseline with a characteristic of multi-scale features. Afterwards, many publications proposed deep learning image denoising methods based on adding AWGN to the noisy image [5], [6], [8]- [10], [24]- [27]. However, there exists a significant gap between the artificially added AWGN and the real-world noise, and these methods are not ideal for denoising in real-world applications. Several publications [28], [29] added Poisson noise corresponding to shot noise and Gaussian noise corresponding to read noise, to Raw-RGB images. After denoising in Raw-RGB space, the final denoised result image was converted back to sRGB space using ISP tools. For this denoising method, accurate noise estimation and modelling are essential for success. Although the noise obtained by statistical modelling reduced the gap between the synthetic noise and the real noise, the injected noise was not real after all. This method can mitigate the performance degradation of the denoising model in practical applications, but not enough to eliminate it. The mismatch between the training process and the application deployment limits the practical application of this method. To solve this problem perfectly, it is undoubtedly that a direct and effective method is to use the noisy-clean pairs in the real world [30], [31]. However, generating such noisy-clean pairs requires massive human labour and cost, which makes it impractical.\n\n\nB. SELF-SUPERVISED IMAGE DENOISING METHODS\n\nNoise2Noise [16] used two perfectly aligned noisy images taken in the same scene as the input and the target respectively. When training the denoising model , L2 loss was used to minimise the difference between the noise-noise pairs, making the model capable of denoising. Then, Noise2void [14], Noise2self [13], Probabilistic noise2void [15], Neig-bor2Neigbor [11], IDR [12], CVF-SID [32], Blind2Unblind [33], and AP-BSN [22] were proposed to use only noisy images for training, instead of noisy-clean pairs. Neigh-bor2Neighbor [11] directly selected two adjacent pixels in 2*2 neighbourhood within a single Raw-RGB image at random to synthesise one sub-noise image, and the remaining pixels formed another sub-noise image. Two sub-noise images obtained in this way formed a noise-noise pair, and one of them would participate in backpropagation during training. However, training with only sub-images would inevitably lose some detail. Noise2void tried to take the whole image into account. Noise2void took the noisy image with the masked pixels as input, and the complete noisy image was regarded as the target. In this way, the masked pixels would never be seen during training, which could easily lead to details loss or over-smoothing in the image. Blind2Unblind [33] used a global masker to generate interleaved blind spots and collected blind spots after model denoising to reconstruct the denoised output. The use of global masker solves this problem perfectly, but when the noise is large, the pointlike mask used in Blind2Unblind [33] will be powerless. For spatially connected noise, AP-BSN [22] used pixel-shuffle downsampling (PD) to break the spatial connection of the noise, and then used the masked convolution kernel to extract features at the very beginning of the model. Thus, the features of the masked patches were obtained from the surrounding pixels. The effect of blind spots could be achieved by combining DCLs with the subsequent steps corresponding to the size of the convolution kernel. Finally, the current pixel can be restored from the noise with the surrounding pixels through the inference process. However, if the noise is strongly spatially connected over a large area, PD will fail to break the spatial connection, and the pixels recovered from the surroundings should still be noise, resulting in color shifts or textures loss in the desoised results. [38]. These methods are based on window-based image blocks and utilize a multi-head attention mechanism, which has outstanding advantages in capturing global image information. ViT [36] showed that pure Transformers can be applied directly to sequences of image patches without overlap, and performed well on image classification tasks. Liu et al. proposed the Swin Transformer [34], which had a hierarchical structure where cross-window connections were captured by a shifted windowing scheme. Chen et al. developed IPT [38], a pre-trained Transformer model for low-level computer vision tasks. Liang et al. extended Swin Transformer for image restoration and proposed SwinIR [35], which achieved state-of-the-art performance on image super-resolution and denoising. To the best of our knowledge, the existing transformer-based image denoising methods are supervised image denoising [34], [35], [39]. Due to the global feature capturing capability of Transformer, the image features obtained based on BSNs are close to the identity transformation with revisible loss. Alternatively, the original pixel can be easily recovered from the surroundings, resulting in unsatisfactory denoising performance.\n\n\nC. TRANSFORMER DENOISING METHODS\n\n\nTransformers have achieved great success in the field of computer vision [34]-\n\nInspired by [34], [37], we propose a new CADT structure aimed at self-supervised denoising for real-world images. CADT contains two branches, a global branch and a local branch. These two branches can effectively combine the global information and the local textures. Taking CADT as the basic component, we construct a hierarchical CADT structure to extract noise features. Once the noise information is obtained, it will be subtracted from the original image by residual learning and we get the first stage denoised result. What's more, we propose to add a Secondary Noise Extractor (SNE) module based on the first denoised output and to extract the global noise information on it. SNE is designed using LN and MLP with low computational complexity. The final denoised result can be obtained by removing the SNE output as a residual from the first denoised result.\n\n\nIII. METHOD\n\n\nA. CADT\n\nSince the pure Transformer will lead to poor self-supervised image denoising effect, we propose a new dual-branch Transformer structure, Context-aware Denoise Transformer (CADT), as shown in Fig. 1. CADT includes a global feature extraction branch and a local feature extraction branch, and the framework is shown in Fig. 1a, and the workflow is shown in Fig. 1b.\n\nGlobal Transformer Encoder For the global branch, we use a window-based multi-head Transformer encoder in Swin Transformer [34] to extract the global information. The Transformer encoder contains a multi-head self-attention (MSA) module and a multi-layer perception (MLP) module. A LayerNorm (LN) layer is applied before each MSA module and each MLP, and a residual connection is applied after each module.\n\nAssuming that the token embedding input is E \u2208 R H\u00d7W \u00d7C , the global self-attention computation can be formulated as the following Eq. (1):\nE = MSA(LN(E)) + E CADT global = MLP(LN(E)) + E(1)\nWhere LN denotes the LayerNorm, CADT global denotes the global feature extracted using Transformer Encoder.\n\nLocal Feature Extractor For the local branch, we design a local feature extractor (LFE) to capture the local context CADT local , where the features of adjacent pixels and features of cross-channels are fused. The local feature extraction can be formulated as Eq. (2):\nCADT local = LFE(LN(E))(2)\nFor the token embedding vector E, we normalize it by the LN layer and reshape it into a feature vector with shape of N \u00d7H \u00d7W \u00d7C. Then we use a set of convolutions to extract and output a feature vector shaped in N \u00d7 H \u00d7 W \u00d7 C/8 for dimensionality reduction. We then use another ordinary convolution to undertake the feature after dimension reduction, followed by two convolutions with deformable kernels [40] to fuse contextual details with large changes and crosschannel features, which is a key design to preserve image textures during denoising. Now we have two vectors whose shapes are N \u00d7H \u00d7W \u00d7C/4 and N \u00d7H \u00d7W \u00d7C/2 respectively. The final shape is expanded from N \u00d7H \u00d7W \u00d7C/2 to N \u00d7 H \u00d7 W \u00d7 C by another convolution. Each convolution, except the first and the last, is followed by a LeakyReLU activation layer to improve feature selection. We summarize  The local branch is composed of several convolutions with small receptive fields. In particular, the deformable convolutions are used to extract the details of the local information mutation, which helps to preserve denoised image details. each computational step into the formulas as Eq. (3):\nf reduction = Conv(LN(E)) f local = Leaky(D(Leaky(D(f reduction ))) f expansion = Conv(f local ) CADT local = f expansion(3)\nFinally, element-wise addition is used to fuse global and local information, effectively reducing additional parameters compared to linear or convolutional layers. The local branch adds local features to the Transformer Encoder, changing the pure transformer mapping relationship, and preserving local mutation information and texture detail.\n\n\nB. SNE BLOCK\n\nWe propose a secondary noise extraction module with low computation at the end of the Denoise Transformer. We want to further distinguish true noise from textures in a relatively clean image. And this is easier to realize on a relatively clean image than on a noisy image. Therefore, we design a low computational global information extraction mechanism inspired by the multi-layer perception mechanism. First, the denoised image is reshaped into N \u00d7 C \u00d7 (H \u00d7 W ) and LayerNorm operation is performed to normalize the global information of each channel. Multi-layer perception is used to exchange information between channels. With residual learning, the output of SNE would be subtracted from the first stage denoised image. That is to say, SNE learns the second noise information mapping using the first stage denoised image and the final image. Although SNE has a relatively simple design, it achieves further noise reduction while preserving image detail. The module works as a plug-in and can be portable if required. The SNE block is calculated as follows Eq. (4):\nf SNE = MLP(LN(x))(4)\nWhere, x denotes the denoised image. LN denotes the Lay-erNorm, and MLP denotes the multi-layer perception.\n\n\nC. A CHOICE OF LOSS\n\nWith the baseline of Transformer, the local information extraction will be severely compromised if we follow [11], [33] to pass the original images through the network in a non-gradient manner. When all the noisy images and the corresponding denoised results are included in the total loss calculation, the global feature captured by Transformer encoder can easily recover the noise information of the original image. It seems that transformer can recover what it sees before, even though the pixels do not contribute to the gradient descent. To avoid this kind of the identity transformation, we do not compute the invisible loss. During the training process, only the images after the global masker are used as training objects, and the L 2 loss is performed using the blind input and the corresponding output. The loss function can be formulated as the following Eq. (5):\nL 2 = (Noisy \u2212 denoised) 2(5)\n\nD. OVERALL ARCHITECTURE OF DENOISE TRANSFORMER\n\nBased on the CADT structure and the SNE block introduced above, we propose the Denoise Transformer framework, as shown in Fig. 2. The detailed architecture is shown in Fig. 3. Firstly, we perform PD on the original image to break the spatial noise connection [22], and then we use the Globalaware Mask Mapper [33] to create the blind spot effect. The mask mapper performs global denoising on the blind spots. This mechanism constrains all pixels, promotes information exchange across whole masked regions, and improves denoising performance. In this paper, the mask width is set to 4, which means that one noisy image will be mapped into 16 blind sub-images. The blind sub-images are sent to the network as inputs. We do the feature dimension mapping of the blind input with the patch embedding dimension through a convolution layer. And then the features go through hierarchical CADTs. In this paper the number of CADT groups is set to 3, each of which contains 6 CADT units for feature extraction, and a residual connection is applied after each CADT group. Next, the output of the hierarchical CADTs goes through another convolution layer, which maps the feature dimension back to the input dimension, forming the residual noise. We subtract the noise pattern from the blind image and obtain a denoised image. The denoised image then passes through SNE, and the residual secondary noise features are extracted. Finally, the denoised image is obtained by subtracting the SNE output and is subjected to the reverse operation of the Global-aware Mask Mapper to form the final denoised result. Ablation experiments for several modules are presented in detail in Section IV-B.\n\n\nIV. EXPERIMENTS\n\n\nA. IMPLEMENTATION DETAILS\n\nTraining Details. All models are trained with most of the same settings. Although our model aimed at real-world denoising, we perform real-world deoising experiments on real-world Raw-RGB images and greyscale images, as well as synthetic sRGB with Gaussian noise, for a more comprehensive comparison with state-of-the-art models. The detailed network is shown in Fig. 2 and Fig. 3, where the token embedding dimension C is set to 60 and all convolutions used in the network have a kernel size of 3\u00d73 and a stride of 1. The batch size is set to 4 and the masker step is set to 4. We use the Adam optimizer with a weight decay of 1e-8, an initial learning rate of 0.0003 for synthetic denoising experiments in sRGB space and 0.0001 for real-world denoising experiments in Raw-RGB space and fluorescence microscopy (FM). The learning rate is multiplied by 0.25 every 20 epochs, for 100 training epochs. The images are randomly cropped into 128 \u00d7 128 patches, masked with a step size of 4, and entered into the model for training after random rotation within 90\u00b0and random horizontal or vertical flipping. All models are trained on python3.8.0, pytorch1.12.0 and Nvidia Tesla T4 GPUs. Fig. 4 shows a plotted curve for our proposed model testing on SIDD validation in different epochs, proving the effectiveness of our network structure.\n\nDatasets for Synthetic Denoising. We perform synthetic experiments with varied Gaussian noise, considering a varied Gaussian noise distribution with \u03c3 \u2208 [5,50] in sRGB space are considered. We follow the settings in [11], [33] and select 44,328 images from ImageNet [41] validation set with resolutions ranging from 256\u00d7256 to 512\u00d7512, as the clean set. For the test sets, we follow [33] to repeat Kodak [42], BSD300-test [43] and Set14 [44] by 10, 3 and 20 times, respectively, and calculate the average PSNR/SSIM.\n\nDatasets for real-world Denoising. For real-world denoising in Raw-RGB space, we take the public dataset of SIDD for experiments. The SIDD Medium dataset contains 320 pairs of noisy-clean images. We take only Raw-RGB VOLUME 4, 2016 images in SIDD Medium [45] dataset as train set, and we take Raw-RGB images in SIDD Validation and Benchmark datasets as the valid set and test set respectively. For realworld denoising on FM with greyscale images, we use FMDD [46] dataset for experiments. FMDD contains real fluorescence microscopy images and 60,000 noisy images with different noise levels, which are obtained with commercial confocal, two-photon, and widefield microscopes and representative biological samples such as cells, zebrafish, and mouse brain tissue. Following the setting in [33], we train on three datasets including Confocal Fish, Confocal Mice and Two Photon Mice, considering each set as both the training set and validation set at the same time and use the 50th view for visual testing.\n\nDetails of Experiments. We use PSNR and SSIM as evaluation metrics. PSNR/SSIM on SIDD validation set is obtained by using python toolkit based on clean images and denoised images. And PSNR/SSIM on SIDD benchmark is reported from the official website through submitting our denoised result.\n\nFor fair comparison, we follow the experimental settings of Blind2Unblind [33]. We compare our proposed Denoising Transformer method with two supervised denoising methods (N2C [23] and N2N [16]) for the baseline. We also quantitatively compare the Denosing Transformer method with a traditional approach (BM3D [1]) and several self-supervised denoising algorithms (Laine19 [19], Self2self [47], DBSN [21], N2V [14], R2R [48], Noisier2noise [17], NBR2NBR [11] and Blind2Unblind [33]).\n\nFor the estimation of synthetic denoising, we use pretrained models provided by [19] for N2C, N2N, and Laine19 [19], keeping the same network architecture as [11], [19]. And then, we use a multi-channel version of BM3D, namely CBM3D [49], to denoise Gaussian noise using the \u03c3 noise level estimation method in [50]. For Self2self, Noisier2noise, DBSN, R2R, NBR2NBR and Blind2Unblind, the official implementations are used for experiments. Since Possion noise can be transformed into Gaussian distribution [51], we will not show the denoising performance specifically for Possion noise.\n\nFor estimating of real-world denoising in Raw-RGB space, all the methods we use are based on their official implementations, trained on the SIDD Medium dataset and well shown through ISP tools [45]. For BM3D, we use BM3D-CFA [52] for Raw-RGB denoising. For the learning-based networks, we split the single channel raw image into four sub-images following the Bayer pattern. We gather four sub-images to form a four-channel image for denoising and then reconstruct the Raw-RGB space from the denoised image. For the estimation of the real-world denoising on FM greyscale set, all compared methods are retrained using the corresponding authors' implementation.\n\n\nB. ABLATION STUDY\n\nIn order to quantitatively evaluate the performance of different networks and training processes, several comprehensive ablation experiments are conducted based on the metric of PSNR/SSIM of real-world Raw-RGB images from SIDD validation set. Ablation on the network architecture. For the design of the network structure, we are inspired to introduce Transformer, which has achieved good performance in computer vision, as a baseline for self-supervised denoising. Based on this, CADT component, SNE block, and the overall Denoise Transformer are successively proposed. Therefore, in the ablation experiments, we set the following variables: -Baseline. We use a tiny version of SwinIR [35], which consists of pure transformer encoders.\n\n-CADT. This module extends the pure transformer based on the baseline, which consists of a global branch and a local branch, forming a dual-branch structure.\n\n-SNE. It is a secondary noise extractor. It is designed to be connected after the first stage denoised result, and to extract the global noise on the first stage denoised result to realize the secondary denoising.\n\n-+CADT+SNE. The overall network architecture of our proposed Denoise Transformer.\n\nThe quantitative evaluation of the SIDD validation in Raw-RGB space is shown in Table 1. It can be seen that direct use of pure Transformer method does not perform well in self-supervised denoising. Our possible reason is that overextraction of global features prevents local features from being effectively denoised. Based on the original global branch, the addition of the local branch to CADT and SNE can both improve the self-supervised denoising ability. In particular, CADTs show the surprising performance for denoising. To show the performance of SNE, we also present a set of visual intermediate results in Fig. 5. From Fig. 5 we can see that SNE can slightly contribute to denoising, which is in line with our expectations. In the ideal case, there is little noise present in the first stage denoised result and therefore we design SNE in low computation. SNE is the icing on the cake. By effectively combining it with global features and local information, the global noise estimation, denoising and local texture reconstruction can be better optimized and complement each other. Ablation on Transformer loss. Based on the overall architecture of Denoise Transformer, we conduct experiments on three different losses. Re-visible Loss1 means that the forward propagation and the total loss calculation are consistent with NBR2NBR [11]. Re-visible Loss2 means that the forward propagation and the total loss calculation are consistent with Blind2Unblind [33]. The comparison are shown in Table 2. It can be seen from Tabel 2 that when the Transformer branch is used in the self-supervised denoising model, its powerful global extraction ability is enhanced under the re-visible action, making the mapping result of the selfsupervised denoising network onto the original noisy image close to identity transformation. This makes it easier to recover the noise information of the blind spot, which greatly reduces the denoising ability. In this paper we use L 2 loss instead of the re-visible loss for Denoise Transformer.\n\n\nC. COMPARISONS WITH STATE-OF-THE-ART\n\nResults for Synthetic Denoising. For the widely used Gaussian noise, the quantitative comparison in synthetic denoising is shown in Fig. 6. To better evaluate the denosing ability of state-of-the-art models, we have varied the Gaussian noise in a wide range \u03c3 \u2208 [5,50]. Fig. 6 shows that our method outperforms the traditional denoising method BM3D and several self-supervised denoising methods, such as Self2self, Laine19-mu, DBSN and NBR2NBR. Fig. 7 shows the visual comparison of the denoised images injected with Gaussian noise with noise level \u03c3 = 25. Denoise Transformer achieves competitive visual quality among traditional and self-supervised CNN denoisers. What's more, its performance is close to the SOTA of Blind2Unblind [34] and even surpasses it on single images with low saturated textures, proving the advantage of preserving image textures of our proposed method. However, Fig. 7 also shows that Denoise Transformer does not perform as well as Blind2Unblind when processing images with high saturation. The possible reason is that in the high saturation region, the extracted global features are at a high level while the local details have relatively small changes. And the small changes in texture would be considered as noise and removed.\n\nResults for real-world Denoising. For real-world denoising, Table 3 quantitatively compares the denoising performance of different methods on Raw-RGB space of SIDD benchmark and validation, and Table 4 quantitatively compares the denoising performance on FM dataset. Our proposed Denoise Transformer outperforms the traditional method BM3D and Laine19 to a large extent, and it even outperforms Neighbor2Neighbor by 0.15dB and 0.10dB for SIDD benchmark and validation, and it outperforms NBR2NBR more on FMDD dataset.   Although the method proposed in this paper does not yet outperform the SOTA Blind2Unblind in Raw-RGB and FM, Denoise Transformer has demonstrated its potential to handle complex noise patterns in self-supervised denoising, which is comparable to the performance of SOTA. Fig. 5, Fig. 8 and Fig. 9 show the visual comparison of SIDD validation, SIDD benchmark and FMDD. There are color shifts and texture loss in NBR2NBR and Blind2Unblind, especially for the top row images in Fig. 5 has similar performance in general scene images, but has the best denoising performance in low saturated textures and low-light scenes.\n\n\nOriginal\n\nNoisy Baseline, N2C Baseline, N2N BM3D\n\nLaine19-mu(G) Laine19-mu(P) NBR2NBR Blind2Unblind Ours Computational Complexity Comparison. We compute the model size and the FLOPs of state-of-the-art self-supervised image denoising methods that have similar preprocessing and inference flows, as in the following Table 5.\n\nTransformer is known to be computationally intensive. We have made some reductions in the baseline, but it is still more computationally intensive than other models. Given the huge potential of Transformer-based methods in self-supervise denoising, we will try to find a more effective way of combining Transformers with CNN basic modules. At this stage, we can replace some of the CADT units with pure convolution modules to reduce the number of Transformers, which may reduce the amount of computation while maintaining good denoising performance. We believe that our future research on how to reduce the computational complexity of the model is of great importance in order to successfully land in the industry. \n\n\nV. CONCLUSION\n\nIn this paper, in order to improve the local limitations of CNN networks in self-supervised image denoising, we extend the Transformer-based method, inspired by its successful application in computer vision. We propose a basic network, called Denoise Transformer, which is composed of CADTs and SNE to realize self-supervised denoising in two stages. Each CADT contains a global feature extraction branch and a local feature extraction branch, and the output of the CADTs is our desired noise information. When we subtract the noise from the input through residual learning, we get the denoised output of the first stage . We then design a low computational complexity SNE to be connected after the denoised output to extract its global noise for secondary denoising. Sufficient experiments indicate that abandoning the re-visible branch is suitable for our proposed Denoise Transformer, and both CADTs and SNE contribute a better performance than traditional methods and most self-supervised training methods. Our method demonstrates the strong denoising ability of Transformer under the complex noise pattern and its potential for self-supervised image denoising. We are the first to propose a Transformer-based method for self-supervised image denoising, which fills the gap in the corresponding field and achieves state-of-the-art competitive performance on the public dataset, especially in denoising the noisy images with low-saturated textures and low-light scenes. Although we extend the Transformer based method in new application of self-supervised image denoising, its large computational load is difficult as well as the denosing of high-saturated region.\n\nWe attribute them to the effectiveness of the combination of Transformer and CNN. In the future, we will investigate the effectiveness of possible parameter reduction schemes and the balance between global and the local features. Given Transformer's strong performance in computer vision, we believe that it has a greater hidden potential for self-supervised image denoising, and hope that the method proposed in this paper can provide more inspiration to researchers.\n\nFIGURE 1 :\n1The representation of CADT. (a) shows that CADT is designed as a dual-branch structure. The left part shows the global branch of the Transformer-based method for global featrue extraction, and the right part shows the local branch, which focuses on the local feature extraction. (b) shows the workflow of CADT for feature extracting. In the global branch, token embedding is performed on image patches, and the global information is extracted by the the multi-head attention mechanism.\n\nFIGURE 2 :\n2The framework of Denoise Transformer.\n\nFIGURE 3 :\n3The architecture of Denoise Transformer.\n\nFIGURE 4 :\n4Denoise Transformer test results for SIDD validation in different epochs, with maximum PSNR and SSIM marked on the curve.\n\nFIGURE 5 :\n5Visual Comparison of different self-supervised methods denoising for Raw-RGB images in SIDD validation.\n\nFIGURE 6 :\n6The curve of quantitative comparison of several self denoising methods with Gaussian noise \u03c3 \u2208[5,50].\n\nFIGURE 7 :\n7Visual Comparison of different methods denoising for images in sRGB space with Gaussion noise \u03c3 = 25.\n\nFIGURE 9 :\n9Visual Comparison of different methods denoising for FM images.\n\nTABLE 1 :\n1Quantitative comparison of various branches and their combinations performance on the SIDD validation. (1) BL: Baseline model. CADT: Dual branch composed of global and loacal extractors. SNE: Secondary noise extractor. (2) denotes using the corresponding branch. (3) The best of PSNR/SSIM is highlighted in bold.BL \nCADT \nSNE \nSIDD Validation \n46.8/0.975 \n47.2/0.975 \n50.8/0.990 \n51.16/0.991 \n\n\n\nTABLE 2 :\n2Quantitative comparison of different losses. The best of PSNR/SSIM is highlighted in bold.Loss Type \nSIDD Validation \nRe-visible Loss [11] \n44.56/0.961 \nRe-visible Loss [36] \n48.64/0.986 \nL 2 Loss \n51.16/0.991 \n\n\n\nTABLE 3 :\n3Quantitative comparison of denoising methods on synthetic datasets in Raw-RGB space of SIDD benchmark. DT denotes our Denoised Transformer. The highest PSNR/SSIM is highlighted in bold, while the second is underlined.Methods \nNetwork \nSIDD \nBenchmark \n\nSIDD \nValidation \nBaseline, N2C [23] \nU-Net [23] \n50.60/0.991 \n51.19/0.991 \nBaseline, N2N [16] \nU-Net [23] \n50.62/0.991 \n51.21/0.991 \nBM3D [1] \n-\n48.60/0.986 \n48.92/0.986 \nN2V [14] \nU-Net [23] \n48.01/0.983 \n48.55/0.984 \nLaine19-mu (G) [19] \nU-Net [23] \n49.82/0.989 \n50.44/0.990 \nLaine19-pme (G) [19] \nU-Net [23] \n42.17/0.935 \n42.87/0.939 \nLaine19-mu (P) [19] \nU-Net [23] \n50.28/0.989 \n50.89/0.990 \nLaine19-pme (P) [19] \nU-Net [23] \n48.46/0.984 \n48.98/0.985 \nDBSN [21] \nDBSN [21] \n49.56/0.987 \n50.13/0.988 \nR2R [48] \nU-Net [23] \n46.70/0.978 \n47.20/0.980 \nNBR2NBR [11] \nU-Net [23] \n50.47/0.990 \n51.06/0.991 \nBlind2Unblind [33] \nU-Net [23] \n50.79/0.991 \n51.36/0.992 \nOurs \nDT \n50.62/0.990 \n51.16/0.991 \n\n\n\nTABLE 4 :\n4Quantitative comparison of denoising results on synthetic datasets in FMDD. DT denotes our Denoised Transformer. The highest PSNR/SSIM among unsupervised denoising methods is highlighted in bold, and the second is underlined. Laine19-mu (G) [19] U-Net [23] 31.62/0.849 37.54/0.959 32.91/0.903 Laine19-pme (G) [19] U-Net [23] 23.30/0.527 31.64/0.881 25.87/0.418 Laine19-mu (P) [19] U-Net [23] 31.59/0.854 37.30/0.956 33.09/0.907 Laine19-pme (P) [19] U-Net [23] 25.16/0.597 37.82/0.959 31.80/0.820 NBR2NBR [11] U-Net [23] 32.11/0.890 37.07/0.960 33.40/0.921 Blind2Unblind [33] U-Net [23] 32.74/0.897 38.44/0.964 34.03/0.916Methods \nNetwork \nConfocal \nFish \n\nConfocal \nMice \n\nTwo-Photon \nMice \nBaseline, N2C [23] \nU-Net [23] 32.79/0.905 38.40/0.966 34.02/0.925 \nBaseline, N2N [16] \nU-Net [23] 32.75/0.903 38.37/0.965 33.80/0.923 \nBM3D [1] \n-\n32.16/0.886 37.93/0.963 33.83/0.924 \nN2V [14] \nU-Net [23] 32.08/0.886 37.49/0.960 33.38/0.916 \nOurs \nDT \n32.52/0.895 38.21/0.962 33.64/0.914 \n\n\n\n\n. Compared to other models, our model FIGURE 8: Visual comparison of different models denoising for Raw-RGB images in SIDD benchmark. PSNR/SSIM can not be calculated since clean images are not available. The images are well shown from Raw-RGB to sRGB using the official ISP tool[45].Noisy \n\nBaseline, N2C \n\nBaseline, N2N \n\nBM3D \n\nNBR2NBR \n\nBlind2Unblind \n\nOurs \n\nNoisy \n\nBaseline, N2C \n\nBaseline, N2N \n\nBM3D \n\nNBR2NBR \n\nBlind2Unblind \n\nOurs \n\n\n\nTABLE 5 :\n5Computational Complexity Comparison of several methods.Methods \nParameters (MB) \nFLOPs (e9) \nBlind2Unblind [33] \n4.4 \n1.17 \nNBR2NBR [11] \n5.0 \n1.37 \nOurs \n14 \n3.14 \n\n\n   VOLUME 4, 2016    Author et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n   VOLUME 4, 2016    Author et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n   VOLUME 4, 2016   \n   VOLUME 4, 2016    Author et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS\n\nImage denoising by sparse 3-D transform-domain collaborative filtering. Kostadin Dabov, IEEE Transactions on image processing. 16Dabov, Kostadin, et al, \"Image denoising by sparse 3-D transform-domain collaborative filtering,\" IEEE Transactions on image processing, 16.8 (2007): 2080-2095.\n\nWeighted nuclear norm minimization with application to image denoising. Shuhang Gu, Proc. The IEEE conference on computer vision and pattern recognition. The IEEE conference on computer vision and pattern recognitionGu, Shuhang, et al, \"Weighted nuclear norm minimization with application to image denoising,\" in Proc. The IEEE conference on computer vision and pattern recognition, 2014.\n\nClustering-based natural image denoising using dictionary learning approach in wavelet domain. Asem Khmag, Abd Rahman Ramli, Noraziahtulhidayu Kamarudin, Soft computing. 23Khmag, Asem, Abd Rahman Ramli, and Noraziahtulhidayu Kamarudin. Clustering-based natural image denoising using dictionary learning ap- proach in wavelet domain. Soft computing 23.17 (2019): 8013-8027.\n\nBeyond a gaussian denoiser: Residual learning of deep cnn for image denoising. Kai Zhang, IEEE transactions on image processing. 267Zhang, Kai, et al, \"Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising,\" IEEE transactions on image processing, vol.26, no.7, pp. 3142-3155, 2017.\n\nReal image denoising with feature attention. Saeed Anwar, Nick Barnes, Proc. The IEEE/CVF international conference on computer vision. The IEEE/CVF international conference on computer visionAnwar, Saeed, and Nick Barnes, \"Real image denoising with feature attention,\" in Proc. The IEEE/CVF international conference on computer vision, pp. 3155-3164, 2019.\n\nSpatial-adaptive network for single image denoising. Meng Chang, European Conference on Computer Vision. ChamSpringerChang, Meng, et al, \"Spatial-adaptive network for single image denoising,\" European Conference on Computer Vision, Springer, Cham, pp. 171-187, Aug,2020.\n\nToward convolutional blind denoising of real photographs. Guo, Shi, Proc. The IEEE/CVF conference on computer vision and pattern recognition. The IEEE/CVF conference on computer vision and pattern recognitionGuo, Shi, et al, \"Toward convolutional blind denoising of real pho- tographs,\" in Proc. The IEEE/CVF conference on computer vision and pattern recognition, pp. 1712-1722, 2019.\n\nTransfer learning from synthetic to real-noise denoising with adaptive instance normalization. Yoonsik Kim, Proc. The IEEE/CVF Conference on Computer Vision and Pattern Recognition. The IEEE/CVF Conference on Computer Vision and Pattern RecognitionKim, Yoonsik, et al, \"Transfer learning from synthetic to real-noise de- noising with adaptive instance normalization,\" in Proc. The IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 3482-3492, 2020.\n\nMulti-level wavelet convolutional neural networks. Pengju Liu, IEEE Access. 7Liu, Pengju, et al, \"Multi-level wavelet convolutional neural networks,\" IEEE Access, 7 (2019): 74973-74985.\n\nDensely connected hierarchical network for image denoising. Park, Songhyun Bumjun, Jechang Yu, Jeong, Proc. The IEEE/CVF conference on computer vision and pattern recognition workshops. The IEEE/CVF conference on computer vision and pattern recognition workshopsPark, Bumjun, Songhyun Yu, and Jechang Jeong, \"Densely connected hier- archical network for image denoising,\" in Proc. The IEEE/CVF conference on computer vision and pattern recognition workshops, pp. 0-0, 2019.\n\nNeighbor2neighbor: Self-supervised denoising from single noisy images. Tao Huang, Proc. The IEEE/CVF conference on computer vision and pattern recognition. The IEEE/CVF conference on computer vision and pattern recognitionHuang, Tao, et al, \"Neighbor2neighbor: Self-supervised denoising from single noisy images,\" in Proc. The IEEE/CVF conference on computer vision and pattern recognition, pp. 14781-14790, 2021.\n\nIDR: Self-Supervised Image Denoising via Iterative Data Refinement. Yi Zhang, Proc. The IEEE/CVF Conference on Computer Vision and Pattern Recognition. The IEEE/CVF Conference on Computer Vision and Pattern RecognitionZhang, Yi, et al, \"IDR: Self-Supervised Image Denoising via Iterative Data Refinement,\" in Proc. The IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2098-2107, 2022.\n\nNoise2self: Blind denoising by selfsupervision. Joshua Batson, Loic Royer, PMLRInternational Conference on Machine Learning. Batson, Joshua, and Loic Royer, \"Noise2self: Blind denoising by self- supervision,\" International Conference on Machine Learning, PMLR, pp. 524-533, 2019.\n\nNoise2void-learning denoising from single noisy images. Alexander Krull, Tim-Oliver Buchholz, Florian Jug, Proc. The IEEE/CVF conference on computer vision and pattern recognition. The IEEE/CVF conference on computer vision and pattern recognitionKrull, Alexander, Tim-Oliver Buchholz, and Florian Jug, \"Noise2void- learning denoising from single noisy images,\" in Proc. The IEEE/CVF conference on computer vision and pattern recognition, pp. 2129-2137, 2019.\n\nProbabilistic noise2void: Unsupervised contentaware denoising. Alexander Krull, Frontiers in Computer Science. 25Krull, Alexander, et al, \"Probabilistic noise2void: Unsupervised content- aware denoising,\" Frontiers in Computer Science 2 (2020): 5.\n\nNoise2Noise: Learning image restoration without clean data. Jaakko Lehtinen, arXiv:1803.04189arXiv preprintLehtinen, Jaakko, et al, \"Noise2Noise: Learning image restoration without clean data,\" arXiv preprint arXiv:1803.04189 (2018).\n\nNoisier2noise: Learning to denoise from unpaired noisy data. Nick Moran, Proc. The IEEE/CVF Conference on Computer Vision and Pattern Recognition. The IEEE/CVF Conference on Computer Vision and Pattern RecognitionMoran, Nick, et al, \"Noisier2noise: Learning to denoise from unpaired noisy data,\" in Proc. The IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12064-12072, 2020.\n\nNoisy-as-clean: Learning self-supervised denoising from corrupted image. Jun Xu, IEEE Transactions on Image Processing. 29Xu, Jun, et al, \"Noisy-as-clean: Learning self-supervised denoising from corrupted image,\" IEEE Transactions on Image Processing 29 (2020): 9316-9329.\n\nHigh-quality self-supervised deep image denoising. Samuli Laine, Advances in Neural Information Processing Systems. 32Laine, Samuli, et al, \"High-quality self-supervised deep image denoising,\" Advances in Neural Information Processing Systems 32 (2019).\n\nEfficient Blind-Spot Neural Network Architecture for Image Denoising. David Honz\u00e1tko, 2020 7th Swiss Conference on Data Science (SDS). IEEEHonz\u00e1tko, David, et al, \"Efficient Blind-Spot Neural Network Architecture for Image Denoising,\" 2020 7th Swiss Conference on Data Science (SDS), IEEE, 2020.\n\nUnpaired learning of deep image denoising. Xiaohe Wu, SpringerChamWu, Xiaohe, et al, \"Unpaired learning of deep image denoising,\" European conference on computer vision,\" Springer, Cham, pp. 352-368, 2020.\n\nAP-BSN: Self-Supervised Denoising for Real-World Images via Asymmetric PD and Blind-Spot Network. Wooseok Lee, Sanghyun Son, Kyoung Mu Lee, Proc. The IEEE/CVF Conference on Computer Vision and Pattern Recognition. The IEEE/CVF Conference on Computer Vision and Pattern RecognitionLee, Wooseok, Sanghyun Son, and Kyoung Mu Lee, \"AP-BSN: Self- Supervised Denoising for Real-World Images via Asymmetric PD and Blind-Spot Network,\" in Proc. The IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 17725-17734, 2022.\n\nU-net: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, International Conference on Medical image computing and computer-assisted intervention. SpringerRonneberger, Olaf, Philipp Fischer, and Thomas Brox, \"U-net: Convolu- tional networks for biomedical image segmentation,\" International Con- ference on Medical image computing and computer-assisted intervention, Springer, Cham, 2015.\n\nSelf-guided network for fast image denoising. Shuhang Gu, Proc. The IEEE/CVF International Conference on Computer Vision. The IEEE/CVF International Conference on Computer VisionGu, Shuhang, et al, \"Self-guided network for fast image denoising,\" in Proc. The IEEE/CVF International Conference on Computer Vision, pp. 2511-2520, 2019.\n\nMultilevel edge features guided network for image denoising. Faming Fang, IEEE Transactions on Neural Networks and Learning Systems. 32Fang, Faming, et al, \"Multilevel edge features guided network for image denoising,\" IEEE Transactions on Neural Networks and Learning Systems, 32.9 (2020): 3956-3970.\n\nImage denoising via deep residual convolutional neural networks. Rushi Lan, Signal, Image and Video Processing. 15Lan, Rushi, et al, \"Image denoising via deep residual convolutional neural networks,\" Signal, Image and Video Processing, 15.1 (2021): 1-8.\n\nAdditive Gaussian noise removal based on generative adversarial network model and semi-soft thresholding approach. Asem Khmag, Multimedia Tools and Applications. Khmag, Asem. Additive Gaussian noise removal based on generative ad- versarial network model and semi-soft thresholding approach. Multimedia Tools and Applications (2022): 1-21.\n\nUnprocessing images for learned raw denoising. Tim Brooks, Proc. The IEEE/CVF Conference on Computer Vision and Pattern Recognition. The IEEE/CVF Conference on Computer Vision and Pattern RecognitionBrooks, Tim, et al, \"Unprocessing images for learned raw denoising,\" in Proc. The IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11036-11045, 2019.\n\nBurst denoising with kernel prediction networks. Ben Mildenhall, Proc. The IEEE conference on computer vision and pattern recognition. The IEEE conference on computer vision and pattern recognitionMildenhall, Ben, et al, \"Burst denoising with kernel prediction networks,\" in Proc. The IEEE conference on computer vision and pattern recognition, pp. 2502-2510, 2018.\n\nDual adversarial network: Toward real-world noise removal and noise generation. Zongsheng Yue, European Conference on Computer Vision. ChamSpringerYue, Zongsheng, et al, \"Dual adversarial network: Toward real-world noise removal and noise generation,\" European Conference on Computer Vision, Springer, Cham, pp. 41-58, 2020.\n\nLearning medical image denoising with deep dynamic residual attention network. S M A Sharif, Mithun Rizwan Ali Naqvi, Biswas, Mathematics 8. 122192Sharif, S. M. A., Rizwan Ali Naqvi, and Mithun Biswas, \"Learning medical image denoising with deep dynamic residual attention network,\" Mathematics 8.12 (2020): 2192.\n\nCVF-SID: Cyclic multi-Variate Function for Self-Supervised Image Denoising by Disentangling Noise from Image. Reyhaneh Neshatavar, Proc. The IEEE/CVF Conference on Computer Vision and Pattern Recognition. The IEEE/CVF Conference on Computer Vision and Pattern RecognitionNeshatavar, Reyhaneh, et al, \"CVF-SID: Cyclic multi-Variate Function for Self-Supervised Image Denoising by Disentangling Noise from Image,\" in Proc. The IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 17583-17591, 2022.\n\nBlind2Unblind: Self-Supervised Image Denoising with Visible Blind Spots. Zejin Wang, Proc. The IEEE/CVF Conference on Computer Vision and Pattern Recognition. The IEEE/CVF Conference on Computer Vision and Pattern RecognitionWang, Zejin, et al. \"Blind2Unblind: Self-Supervised Image Denoising with Visible Blind Spots,\" in Proc. The IEEE/CVF Conference on Com- puter Vision and Pattern Recognition, pp. 2027-2036, 2022.\n\nSwin transformer: Hierarchical vision transformer using shifted windows. Ze Liu, Proc. The IEEE/CVF International Conference on Computer Vision. The IEEE/CVF International Conference on Computer VisionLiu, Ze, et al, \"Swin transformer: Hierarchical vision transformer using shifted windows,\" in Proc. The IEEE/CVF International Conference on Computer Vision, pp. 10012-10022, 2021.\n\nSwinir: Image restoration using swin transformer. Jingyun Liang, Proc. The IEEE/CVF International Conference on Computer Vision. The IEEE/CVF International Conference on Computer VisionLiang, Jingyun, et al, \"Swinir: Image restoration using swin transformer,\" in Proc. The IEEE/CVF International Conference on Computer Vision, pp. 1833-1844, 2021.\n\nAn image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, arXiv:2010.11929arXiv preprintDosovitskiy, Alexey, et al, \"An image is worth 16x16 words: Transformers for image recognition at scale,\" arXiv preprint arXiv:2010.11929 (2020).\n\nGhost-free High Dynamic Range Imaging with Contextaware Transformer. Zhen Liu, European Conference on Computer Vision. ChamSpringerLiu, Zhen, et al, \"Ghost-free High Dynamic Range Imaging with Context- aware Transformer,\" European Conference on Computer Vision, Springer, Cham, pp. 344-360, 2022.\n\nPre-trained image processing transformer. Hanting Chen, Proc. The IEEE/CVF Conference on Computer Vision and Pattern Recognition. The IEEE/CVF Conference on Computer Vision and Pattern RecognitionChen, Hanting, et al, \"Pre-trained image processing transformer,\" in Proc. The IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12299-12310, 2021.\n\nEformer: Edge enhancement based transformer for medical image denoising. Achleshwar Luthra, arXiv:2109.08044arXiv preprintLuthra, Achleshwar, et al, \"Eformer: Edge enhancement based transformer for medical image denoising,\" arXiv preprint arXiv:2109.08044 (2021).\n\nLearning deformable kernels for image and video denoising. Xiangyu Xu, Muchen Li, Wenxiu Sun, arXiv:1904.06903arXiv preprintXu, Xiangyu, Muchen Li, and Wenxiu Sun, \"Learning deformable kernels for image and video denoising,\" arXiv preprint arXiv:1904.06903 (2019).\n\nImagenet: A large-scale hierarchical image database. Jia Deng, 2009 IEEE conference on computer vision and pattern recognition. IeeeDeng, Jia, et al, \"Imagenet: A large-scale hierarchical image database,\" 2009 IEEE conference on computer vision and pattern recognition, Ieee, pp. 248-255, 2009.\n\nWeighted nuclear norm minimization with application to image denoising. Shuhang Gu, Proc. The IEEE conference on computer vision and pattern recognition. The IEEE conference on computer vision and pattern recognitionGu, Shuhang, et al, \"Weighted nuclear norm minimization with application to image denoising,\" in Proc. The IEEE conference on computer vision and pattern recognition, pp. 2862-2869, 2014.\n\nA database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. David Martin, Proc. Eighth IEEE International Conference on Computer Vision. ICCV. Eighth IEEE International Conference on Computer Vision. ICCVIEEE2Martin, David, et al, \"A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics,\" in Proc. Eighth IEEE International Conference on Computer Vision. ICCV 2001, Vol. 2, pp. 416-423, IEEE, 2001.\n\nOn single image scaleup using sparse-representations. Roman Zeyde, Michael Elad, Matan Protter, International conference on curves and surfaces. Berlin, HeidelbergSpringerZeyde, Roman, Michael Elad, and Matan Protter, \"On single image scale- up using sparse-representations,\" International conference on curves and surfaces, Springer, Berlin, Heidelberg, pp. 711-730, 2010.\n\nA highquality denoising dataset for smartphone cameras. Abdelrahman Abdelhamed, Stephen Lin, Michael S Brown, Proc. The IEEE Conference on Computer Vision and Pattern Recognition. The IEEE Conference on Computer Vision and Pattern RecognitionAbdelhamed, Abdelrahman, Stephen Lin, and Michael S. Brown. \"A high- quality denoising dataset for smartphone cameras,\" in Proc. The IEEE Conference on Computer Vision and Pattern Recognition, pp. 1692-1700, 2018.\n\nA poisson-gaussian denoising dataset with real fluorescence microscopy images. Yide Zhang, Proc. The IEEE/CVF Conference on Computer Vision and Pattern Recognition. The IEEE/CVF Conference on Computer Vision and Pattern RecognitionZhang, Yide, et al, \"A poisson-gaussian denoising dataset with real flu- orescence microscopy images,\" in Proc. The IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 11710-11718, 2019.\n\nSelf2self with dropout: Learning self-supervised denoising from single image. Yuhui Quan, Proc. he IEEE/CVF conference on computer vision and pattern recognition. he IEEE/CVF conference on computer vision and pattern recognitionQuan, Yuhui, et al, \"Self2self with dropout: Learning self-supervised denoising from single image,\" in Proc. he IEEE/CVF conference on computer vision and pattern recognition, pp. 1890-1898, 2020.\n\nRecorrupted-to-recorrupted: unsupervised deep learning for image denoising. Tongyao Pang, Proc. The IEEE/CVF conference on computer vision and pattern recognition. The IEEE/CVF conference on computer vision and pattern recognitionPang, Tongyao, et al. \"Recorrupted-to-recorrupted: unsupervised deep learning for image denoising,\" in Proc. The IEEE/CVF conference on computer vision and pattern recognition, pp. 2043-2052, 2021.\n\nColor image denoising via sparse 3D collaborative filtering with grouping constraint in luminance-chrominance space. Kostadin Dabov, I-313-I- 3162007 IEEE International Conference on Image Processing. IEEE1Dabov, Kostadin, et al, \"Color image denoising via sparse 3D collaborative filtering with grouping constraint in luminance-chrominance space,\" 2007 IEEE International Conference on Image Processing, Vol. 1. pp. I-313-I- 316, IEEE, 2007.\n\nAn efficient statistical method for image noise level estimation. Guangyong Chen, Fengyuan Zhu, Pheng Ann Heng, Proc. The IEEE International Conference on Computer Vision. The IEEE International Conference on Computer VisionChen, Guangyong, Fengyuan Zhu, and Pheng Ann Heng, \"An efficient statistical method for image noise level estimation,\" in Proc. The IEEE International Conference on Computer Vision, pp. 477-485, 2015.\n\nOptimal inversion of the Anscombe transformation in low-count Poisson image denoising. Markku Makitalo, Alessandro Foi, IEEE transactions on Image Processing. 20Makitalo, Markku, and Alessandro Foi, \"Optimal inversion of the Anscombe transformation in low-count Poisson image denoising,\" IEEE transactions on Image Processing 20.1 (2010): 99-109.\n\n2009 international workshop on local and non-local approximation in image processing. Aram Danielyan, IEEECross-color BM3D filtering of noisy raw dataDanielyan, Aram, et al, \"Cross-color BM3D filtering of noisy raw data,\" 2009 international workshop on local and non-local approximation in image processing, pp. 125-129, IEEE, 2009.\n", "annotations": {"author": "[{\"end\":162,\"start\":88},{\"end\":243,\"start\":163},{\"end\":254,\"start\":244}]", "publisher": null, "author_last_name": "[{\"end\":97,\"start\":92},{\"end\":176,\"start\":172},{\"end\":253,\"start\":248}]", "author_first_name": "[{\"end\":91,\"start\":88},{\"end\":171,\"start\":163},{\"end\":247,\"start\":244}]", "author_affiliation": "[{\"end\":161,\"start\":99},{\"end\":242,\"start\":178}]", "title": "[{\"end\":85,\"start\":1},{\"end\":339,\"start\":255}]", "venue": null, "abstract": "[{\"end\":2337,\"start\":594}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3035,\"start\":3032},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3040,\"start\":3037},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3222,\"start\":3219},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3255,\"start\":3252},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3260,\"start\":3257},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3648,\"start\":3645},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3653,\"start\":3650},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3659,\"start\":3655},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4036,\"start\":4032},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4042,\"start\":4038},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4106,\"start\":4102},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4366,\"start\":4362},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4376,\"start\":4372},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4569,\"start\":4565},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4964,\"start\":4960},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5199,\"start\":5195},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5205,\"start\":5201},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5211,\"start\":5207},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5405,\"start\":5401},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5509,\"start\":5505},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5846,\"start\":5842},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5900,\"start\":5896},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5906,\"start\":5902},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5912,\"start\":5908},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5918,\"start\":5914},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7497,\"start\":7494},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7675,\"start\":7671},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7888,\"start\":7885},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7893,\"start\":7890},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7898,\"start\":7895},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7904,\"start\":7900},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7910,\"start\":7906},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7916,\"start\":7912},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8119,\"start\":8115},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8125,\"start\":8121},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9008,\"start\":9004},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9014,\"start\":9010},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9189,\"start\":9185},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9467,\"start\":9463},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9484,\"start\":9480},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9515,\"start\":9511},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9538,\"start\":9534},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9548,\"start\":9544},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9562,\"start\":9558},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9582,\"start\":9578},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9599,\"start\":9595},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9706,\"start\":9702},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10446,\"start\":10442},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10718,\"start\":10714},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10780,\"start\":10776},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11567,\"start\":11563},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11748,\"start\":11744},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11945,\"start\":11941},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12088,\"start\":12084},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12244,\"start\":12240},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12451,\"start\":12447},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12457,\"start\":12453},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":12463,\"start\":12459},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12897,\"start\":12893},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12903,\"start\":12899},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":14264,\"start\":14260},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":15549,\"start\":15545},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":18114,\"start\":18110},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":18120,\"start\":18116},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":19218,\"start\":19214},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":19268,\"start\":19264},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22167,\"start\":22164},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":22170,\"start\":22167},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":22231,\"start\":22227},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":22237,\"start\":22233},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":22281,\"start\":22277},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":22398,\"start\":22394},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":22419,\"start\":22415},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":22437,\"start\":22433},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":22452,\"start\":22448},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":22786,\"start\":22782},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":22991,\"start\":22987},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":23320,\"start\":23316},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":23903,\"start\":23899},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":24005,\"start\":24001},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":24018,\"start\":24014},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":24138,\"start\":24135},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24202,\"start\":24198},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":24218,\"start\":24214},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24229,\"start\":24225},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":24239,\"start\":24235},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":24249,\"start\":24245},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":24269,\"start\":24265},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":24283,\"start\":24279},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":24306,\"start\":24302},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24394,\"start\":24390},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24425,\"start\":24421},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":24472,\"start\":24468},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24478,\"start\":24474},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":24547,\"start\":24543},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":24624,\"start\":24620},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":24819,\"start\":24815},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":25094,\"start\":25090},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":25126,\"start\":25122},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":26266,\"start\":26262},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":28115,\"start\":28111},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":28238,\"start\":28234},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":29105,\"start\":29102},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":29108,\"start\":29105},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":29577,\"start\":29573},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":35403,\"start\":35400},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":35406,\"start\":35403},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":38478,\"start\":38474}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":34935,\"start\":34437},{\"attributes\":{\"id\":\"fig_2\"},\"end\":34986,\"start\":34936},{\"attributes\":{\"id\":\"fig_3\"},\"end\":35040,\"start\":34987},{\"attributes\":{\"id\":\"fig_4\"},\"end\":35175,\"start\":35041},{\"attributes\":{\"id\":\"fig_5\"},\"end\":35292,\"start\":35176},{\"attributes\":{\"id\":\"fig_6\"},\"end\":35407,\"start\":35293},{\"attributes\":{\"id\":\"fig_7\"},\"end\":35522,\"start\":35408},{\"attributes\":{\"id\":\"fig_8\"},\"end\":35599,\"start\":35523},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":36006,\"start\":35600},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":36231,\"start\":36007},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":37198,\"start\":36232},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":38193,\"start\":37199},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":38639,\"start\":38194},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":38818,\"start\":38640}]", "paragraph": "[{\"end\":5847,\"start\":2356},{\"end\":6958,\"start\":5849},{\"end\":7115,\"start\":6960},{\"end\":7210,\"start\":7117},{\"end\":7415,\"start\":7212},{\"end\":9126,\"start\":7481},{\"end\":12763,\"start\":9173},{\"end\":13746,\"start\":12881},{\"end\":14135,\"start\":13772},{\"end\":14543,\"start\":14137},{\"end\":14684,\"start\":14545},{\"end\":14843,\"start\":14736},{\"end\":15113,\"start\":14845},{\"end\":16292,\"start\":15141},{\"end\":16760,\"start\":16418},{\"end\":17847,\"start\":16777},{\"end\":17977,\"start\":17870},{\"end\":18875,\"start\":18001},{\"end\":20629,\"start\":18955},{\"end\":22009,\"start\":20677},{\"end\":22526,\"start\":22011},{\"end\":23532,\"start\":22528},{\"end\":23823,\"start\":23534},{\"end\":24308,\"start\":23825},{\"end\":24895,\"start\":24310},{\"end\":25555,\"start\":24897},{\"end\":26312,\"start\":25577},{\"end\":26471,\"start\":26314},{\"end\":26686,\"start\":26473},{\"end\":26769,\"start\":26688},{\"end\":28799,\"start\":26771},{\"end\":30098,\"start\":28840},{\"end\":31238,\"start\":30100},{\"end\":31289,\"start\":31251},{\"end\":31564,\"start\":31291},{\"end\":32281,\"start\":31566},{\"end\":33966,\"start\":32299},{\"end\":34436,\"start\":33968}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14735,\"start\":14685},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15140,\"start\":15114},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16417,\"start\":16293},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17869,\"start\":17848},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18905,\"start\":18876}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":26858,\"start\":26851},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":28275,\"start\":28268},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":30167,\"start\":30160},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":30301,\"start\":30294},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":31563,\"start\":31556}]", "section_header": "[{\"end\":2354,\"start\":2339},{\"end\":7434,\"start\":7418},{\"end\":7479,\"start\":7437},{\"end\":9171,\"start\":9129},{\"end\":12798,\"start\":12766},{\"end\":12879,\"start\":12801},{\"end\":13760,\"start\":13749},{\"end\":13770,\"start\":13763},{\"end\":16775,\"start\":16763},{\"end\":17999,\"start\":17980},{\"end\":18953,\"start\":18907},{\"end\":20647,\"start\":20632},{\"end\":20675,\"start\":20650},{\"end\":25575,\"start\":25558},{\"end\":28838,\"start\":28802},{\"end\":31249,\"start\":31241},{\"end\":32297,\"start\":32284},{\"end\":34448,\"start\":34438},{\"end\":34947,\"start\":34937},{\"end\":34998,\"start\":34988},{\"end\":35052,\"start\":35042},{\"end\":35187,\"start\":35177},{\"end\":35304,\"start\":35294},{\"end\":35419,\"start\":35409},{\"end\":35534,\"start\":35524},{\"end\":35610,\"start\":35601},{\"end\":36017,\"start\":36008},{\"end\":36242,\"start\":36233},{\"end\":37209,\"start\":37200},{\"end\":38650,\"start\":38641}]", "table": "[{\"end\":36006,\"start\":35924},{\"end\":36231,\"start\":36109},{\"end\":37198,\"start\":36461},{\"end\":38193,\"start\":37832},{\"end\":38639,\"start\":38479},{\"end\":38818,\"start\":38707}]", "figure_caption": "[{\"end\":34935,\"start\":34450},{\"end\":34986,\"start\":34949},{\"end\":35040,\"start\":35000},{\"end\":35175,\"start\":35054},{\"end\":35292,\"start\":35189},{\"end\":35407,\"start\":35306},{\"end\":35522,\"start\":35421},{\"end\":35599,\"start\":35536},{\"end\":35924,\"start\":35612},{\"end\":36109,\"start\":36019},{\"end\":36461,\"start\":36244},{\"end\":37832,\"start\":37211},{\"end\":38479,\"start\":38196},{\"end\":38707,\"start\":38652}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13969,\"start\":13963},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14096,\"start\":14089},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14134,\"start\":14127},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19083,\"start\":19077},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19129,\"start\":19123},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21057,\"start\":21040},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":21864,\"start\":21858},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27393,\"start\":27387},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27406,\"start\":27400},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":28978,\"start\":28972},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":29116,\"start\":29110},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":29291,\"start\":29285},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":29736,\"start\":29730},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":30905,\"start\":30891},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":30916,\"start\":30910},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":31102,\"start\":31096}]", "bib_author_first_name": "[{\"end\":39200,\"start\":39192},{\"end\":39490,\"start\":39483},{\"end\":39900,\"start\":39896},{\"end\":39911,\"start\":39908},{\"end\":39943,\"start\":39926},{\"end\":40257,\"start\":40254},{\"end\":40532,\"start\":40527},{\"end\":40544,\"start\":40540},{\"end\":40897,\"start\":40893},{\"end\":41600,\"start\":41593},{\"end\":42022,\"start\":42016},{\"end\":42226,\"start\":42218},{\"end\":42242,\"start\":42235},{\"end\":42701,\"start\":42698},{\"end\":43112,\"start\":43110},{\"end\":43501,\"start\":43495},{\"end\":43514,\"start\":43510},{\"end\":43793,\"start\":43784},{\"end\":43811,\"start\":43801},{\"end\":43829,\"start\":43822},{\"end\":44261,\"start\":44252},{\"end\":44504,\"start\":44498},{\"end\":44738,\"start\":44734},{\"end\":45146,\"start\":45143},{\"end\":45401,\"start\":45395},{\"end\":45674,\"start\":45669},{\"end\":45945,\"start\":45939},{\"end\":46208,\"start\":46201},{\"end\":46222,\"start\":46214},{\"end\":46237,\"start\":46228},{\"end\":46701,\"start\":46697},{\"end\":46722,\"start\":46715},{\"end\":46738,\"start\":46732},{\"end\":47129,\"start\":47122},{\"end\":47478,\"start\":47472},{\"end\":47784,\"start\":47779},{\"end\":48088,\"start\":48084},{\"end\":48360,\"start\":48357},{\"end\":48731,\"start\":48728},{\"end\":49135,\"start\":49126},{\"end\":49452,\"start\":49451},{\"end\":49456,\"start\":49453},{\"end\":49471,\"start\":49465},{\"end\":49805,\"start\":49797},{\"end\":50278,\"start\":50273},{\"end\":50696,\"start\":50694},{\"end\":51061,\"start\":51054},{\"end\":51435,\"start\":51429},{\"end\":51699,\"start\":51695},{\"end\":51973,\"start\":51966},{\"end\":52370,\"start\":52360},{\"end\":52618,\"start\":52611},{\"end\":52629,\"start\":52623},{\"end\":52640,\"start\":52634},{\"end\":52874,\"start\":52871},{\"end\":53193,\"start\":53186},{\"end\":53664,\"start\":53659},{\"end\":54142,\"start\":54137},{\"end\":54157,\"start\":54150},{\"end\":54169,\"start\":54164},{\"end\":54525,\"start\":54514},{\"end\":54545,\"start\":54538},{\"end\":54558,\"start\":54551},{\"end\":54560,\"start\":54559},{\"end\":54998,\"start\":54994},{\"end\":55433,\"start\":55428},{\"end\":55859,\"start\":55852},{\"end\":56330,\"start\":56322},{\"end\":56724,\"start\":56715},{\"end\":56739,\"start\":56731},{\"end\":56754,\"start\":56745},{\"end\":57168,\"start\":57162},{\"end\":57189,\"start\":57179},{\"end\":57513,\"start\":57509}]", "bib_author_last_name": "[{\"end\":39206,\"start\":39201},{\"end\":39493,\"start\":39491},{\"end\":39906,\"start\":39901},{\"end\":39924,\"start\":39912},{\"end\":39953,\"start\":39944},{\"end\":40263,\"start\":40258},{\"end\":40538,\"start\":40533},{\"end\":40551,\"start\":40545},{\"end\":40903,\"start\":40898},{\"end\":41173,\"start\":41170},{\"end\":41178,\"start\":41175},{\"end\":41604,\"start\":41601},{\"end\":42026,\"start\":42023},{\"end\":42216,\"start\":42212},{\"end\":42233,\"start\":42227},{\"end\":42245,\"start\":42243},{\"end\":42252,\"start\":42247},{\"end\":42707,\"start\":42702},{\"end\":43118,\"start\":43113},{\"end\":43508,\"start\":43502},{\"end\":43520,\"start\":43515},{\"end\":43799,\"start\":43794},{\"end\":43820,\"start\":43812},{\"end\":43833,\"start\":43830},{\"end\":44267,\"start\":44262},{\"end\":44513,\"start\":44505},{\"end\":44744,\"start\":44739},{\"end\":45149,\"start\":45147},{\"end\":45407,\"start\":45402},{\"end\":45683,\"start\":45675},{\"end\":45948,\"start\":45946},{\"end\":46212,\"start\":46209},{\"end\":46226,\"start\":46223},{\"end\":46241,\"start\":46238},{\"end\":46713,\"start\":46702},{\"end\":46730,\"start\":46723},{\"end\":46743,\"start\":46739},{\"end\":47132,\"start\":47130},{\"end\":47483,\"start\":47479},{\"end\":47788,\"start\":47785},{\"end\":48094,\"start\":48089},{\"end\":48367,\"start\":48361},{\"end\":48742,\"start\":48732},{\"end\":49139,\"start\":49136},{\"end\":49463,\"start\":49457},{\"end\":49488,\"start\":49472},{\"end\":49496,\"start\":49490},{\"end\":49816,\"start\":49806},{\"end\":50283,\"start\":50279},{\"end\":50700,\"start\":50697},{\"end\":51067,\"start\":51062},{\"end\":51447,\"start\":51436},{\"end\":51703,\"start\":51700},{\"end\":51978,\"start\":51974},{\"end\":52377,\"start\":52371},{\"end\":52621,\"start\":52619},{\"end\":52632,\"start\":52630},{\"end\":52644,\"start\":52641},{\"end\":52879,\"start\":52875},{\"end\":53196,\"start\":53194},{\"end\":53671,\"start\":53665},{\"end\":54148,\"start\":54143},{\"end\":54162,\"start\":54158},{\"end\":54177,\"start\":54170},{\"end\":54536,\"start\":54526},{\"end\":54549,\"start\":54546},{\"end\":54566,\"start\":54561},{\"end\":55004,\"start\":54999},{\"end\":55438,\"start\":55434},{\"end\":55864,\"start\":55860},{\"end\":56336,\"start\":56331},{\"end\":56729,\"start\":56725},{\"end\":56743,\"start\":56740},{\"end\":56759,\"start\":56755},{\"end\":57177,\"start\":57169},{\"end\":57193,\"start\":57190},{\"end\":57523,\"start\":57514}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":1475121},\"end\":39409,\"start\":39120},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1663191},\"end\":39799,\"start\":39411},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":69758710},\"end\":40173,\"start\":39801},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":996788},\"end\":40480,\"start\":40175},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":118713138},\"end\":40838,\"start\":40482},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":210932492},\"end\":41110,\"start\":40840},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":49672261},\"end\":41496,\"start\":41112},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":211506288},\"end\":41963,\"start\":41498},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":195225324},\"end\":42150,\"start\":41965},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":198922180},\"end\":42625,\"start\":42152},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":231419143},\"end\":43040,\"start\":42627},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":244714180},\"end\":43445,\"start\":43042},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b12\",\"matched_paper_id\":59523708},\"end\":43726,\"start\":43447},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":53751136},\"end\":44187,\"start\":43728},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":173990717},\"end\":44436,\"start\":44189},{\"attributes\":{\"doi\":\"arXiv:1803.04189\",\"id\":\"b15\"},\"end\":44671,\"start\":44438},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":204904999},\"end\":45068,\"start\":44673},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":222154570},\"end\":45342,\"start\":45070},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":173990648},\"end\":45597,\"start\":45344},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":220734100},\"end\":45894,\"start\":45599},{\"attributes\":{\"id\":\"b20\"},\"end\":46101,\"start\":45896},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":247596985},\"end\":46630,\"start\":46103},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":3719281},\"end\":47074,\"start\":46632},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":204955838},\"end\":47409,\"start\":47076},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":221345469},\"end\":47712,\"start\":47411},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":201258039},\"end\":47967,\"start\":47714},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":251455487},\"end\":48308,\"start\":47969},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":53770387},\"end\":48677,\"start\":48310},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":4381469},\"end\":49044,\"start\":48679},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":220495900},\"end\":49370,\"start\":49046},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":230533248},\"end\":49685,\"start\":49372},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":247627958},\"end\":50198,\"start\":49687},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":247447122},\"end\":50619,\"start\":50200},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":232352874},\"end\":51002,\"start\":50621},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":237266491},\"end\":51351,\"start\":51004},{\"attributes\":{\"doi\":\"arXiv:2010.11929\",\"id\":\"b35\"},\"end\":51624,\"start\":51353},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":251468017},\"end\":51922,\"start\":51626},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":227239228},\"end\":52285,\"start\":51924},{\"attributes\":{\"doi\":\"arXiv:2109.08044\",\"id\":\"b38\"},\"end\":52550,\"start\":52287},{\"attributes\":{\"doi\":\"arXiv:1904.06903\",\"id\":\"b39\"},\"end\":52816,\"start\":52552},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":57246310},\"end\":53112,\"start\":52818},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":1663191},\"end\":53517,\"start\":53114},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":64193},\"end\":54081,\"start\":53519},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":2356330},\"end\":54456,\"start\":54083},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":52059988},\"end\":54913,\"start\":54458},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":56895444},\"end\":55348,\"start\":54915},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":219619080},\"end\":55774,\"start\":55350},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":235719899},\"end\":56203,\"start\":55776},{\"attributes\":{\"doi\":\"I-313-I- 316\",\"id\":\"b48\",\"matched_paper_id\":1887989},\"end\":56647,\"start\":56205},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":206770723},\"end\":57073,\"start\":56649},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":10229455},\"end\":57421,\"start\":57075},{\"attributes\":{\"id\":\"b51\"},\"end\":57755,\"start\":57423}]", "bib_title": "[{\"end\":39190,\"start\":39120},{\"end\":39481,\"start\":39411},{\"end\":39894,\"start\":39801},{\"end\":40252,\"start\":40175},{\"end\":40525,\"start\":40482},{\"end\":40891,\"start\":40840},{\"end\":41168,\"start\":41112},{\"end\":41591,\"start\":41498},{\"end\":42014,\"start\":41965},{\"end\":42210,\"start\":42152},{\"end\":42696,\"start\":42627},{\"end\":43108,\"start\":43042},{\"end\":43493,\"start\":43447},{\"end\":43782,\"start\":43728},{\"end\":44250,\"start\":44189},{\"end\":44732,\"start\":44673},{\"end\":45141,\"start\":45070},{\"end\":45393,\"start\":45344},{\"end\":45667,\"start\":45599},{\"end\":46199,\"start\":46103},{\"end\":46695,\"start\":46632},{\"end\":47120,\"start\":47076},{\"end\":47470,\"start\":47411},{\"end\":47777,\"start\":47714},{\"end\":48082,\"start\":47969},{\"end\":48355,\"start\":48310},{\"end\":48726,\"start\":48679},{\"end\":49124,\"start\":49046},{\"end\":49449,\"start\":49372},{\"end\":49795,\"start\":49687},{\"end\":50271,\"start\":50200},{\"end\":50692,\"start\":50621},{\"end\":51052,\"start\":51004},{\"end\":51693,\"start\":51626},{\"end\":51964,\"start\":51924},{\"end\":52869,\"start\":52818},{\"end\":53184,\"start\":53114},{\"end\":53657,\"start\":53519},{\"end\":54135,\"start\":54083},{\"end\":54512,\"start\":54458},{\"end\":54992,\"start\":54915},{\"end\":55426,\"start\":55350},{\"end\":55850,\"start\":55776},{\"end\":56320,\"start\":56205},{\"end\":56713,\"start\":56649},{\"end\":57160,\"start\":57075}]", "bib_author": "[{\"end\":39208,\"start\":39192},{\"end\":39495,\"start\":39483},{\"end\":39908,\"start\":39896},{\"end\":39926,\"start\":39908},{\"end\":39955,\"start\":39926},{\"end\":40265,\"start\":40254},{\"end\":40540,\"start\":40527},{\"end\":40553,\"start\":40540},{\"end\":40905,\"start\":40893},{\"end\":41175,\"start\":41170},{\"end\":41180,\"start\":41175},{\"end\":41606,\"start\":41593},{\"end\":42028,\"start\":42016},{\"end\":42218,\"start\":42212},{\"end\":42235,\"start\":42218},{\"end\":42247,\"start\":42235},{\"end\":42254,\"start\":42247},{\"end\":42709,\"start\":42698},{\"end\":43120,\"start\":43110},{\"end\":43510,\"start\":43495},{\"end\":43522,\"start\":43510},{\"end\":43801,\"start\":43784},{\"end\":43822,\"start\":43801},{\"end\":43835,\"start\":43822},{\"end\":44269,\"start\":44252},{\"end\":44515,\"start\":44498},{\"end\":44746,\"start\":44734},{\"end\":45151,\"start\":45143},{\"end\":45409,\"start\":45395},{\"end\":45685,\"start\":45669},{\"end\":45950,\"start\":45939},{\"end\":46214,\"start\":46201},{\"end\":46228,\"start\":46214},{\"end\":46243,\"start\":46228},{\"end\":46715,\"start\":46697},{\"end\":46732,\"start\":46715},{\"end\":46745,\"start\":46732},{\"end\":47134,\"start\":47122},{\"end\":47485,\"start\":47472},{\"end\":47790,\"start\":47779},{\"end\":48096,\"start\":48084},{\"end\":48369,\"start\":48357},{\"end\":48744,\"start\":48728},{\"end\":49141,\"start\":49126},{\"end\":49465,\"start\":49451},{\"end\":49490,\"start\":49465},{\"end\":49498,\"start\":49490},{\"end\":49818,\"start\":49797},{\"end\":50285,\"start\":50273},{\"end\":50702,\"start\":50694},{\"end\":51069,\"start\":51054},{\"end\":51449,\"start\":51429},{\"end\":51705,\"start\":51695},{\"end\":51980,\"start\":51966},{\"end\":52379,\"start\":52360},{\"end\":52623,\"start\":52611},{\"end\":52634,\"start\":52623},{\"end\":52646,\"start\":52634},{\"end\":52881,\"start\":52871},{\"end\":53198,\"start\":53186},{\"end\":53673,\"start\":53659},{\"end\":54150,\"start\":54137},{\"end\":54164,\"start\":54150},{\"end\":54179,\"start\":54164},{\"end\":54538,\"start\":54514},{\"end\":54551,\"start\":54538},{\"end\":54568,\"start\":54551},{\"end\":55006,\"start\":54994},{\"end\":55440,\"start\":55428},{\"end\":55866,\"start\":55852},{\"end\":56338,\"start\":56322},{\"end\":56731,\"start\":56715},{\"end\":56745,\"start\":56731},{\"end\":56761,\"start\":56745},{\"end\":57179,\"start\":57162},{\"end\":57195,\"start\":57179},{\"end\":57525,\"start\":57509}]", "bib_venue": "[{\"end\":39245,\"start\":39208},{\"end\":39563,\"start\":39495},{\"end\":39969,\"start\":39955},{\"end\":40302,\"start\":40265},{\"end\":40615,\"start\":40553},{\"end\":40943,\"start\":40905},{\"end\":41252,\"start\":41180},{\"end\":41678,\"start\":41606},{\"end\":42039,\"start\":42028},{\"end\":42336,\"start\":42254},{\"end\":42781,\"start\":42709},{\"end\":43192,\"start\":43120},{\"end\":43570,\"start\":43526},{\"end\":43907,\"start\":43835},{\"end\":44298,\"start\":44269},{\"end\":44496,\"start\":44438},{\"end\":44818,\"start\":44746},{\"end\":45188,\"start\":45151},{\"end\":45458,\"start\":45409},{\"end\":45732,\"start\":45685},{\"end\":45937,\"start\":45896},{\"end\":46315,\"start\":46243},{\"end\":46831,\"start\":46745},{\"end\":47196,\"start\":47134},{\"end\":47542,\"start\":47485},{\"end\":47824,\"start\":47790},{\"end\":48129,\"start\":48096},{\"end\":48441,\"start\":48369},{\"end\":48812,\"start\":48744},{\"end\":49179,\"start\":49141},{\"end\":49511,\"start\":49498},{\"end\":49890,\"start\":49818},{\"end\":50357,\"start\":50285},{\"end\":50764,\"start\":50702},{\"end\":51131,\"start\":51069},{\"end\":51427,\"start\":51353},{\"end\":51743,\"start\":51705},{\"end\":52052,\"start\":51980},{\"end\":52358,\"start\":52287},{\"end\":52609,\"start\":52552},{\"end\":52944,\"start\":52881},{\"end\":53266,\"start\":53198},{\"end\":53740,\"start\":53673},{\"end\":54226,\"start\":54179},{\"end\":54636,\"start\":54568},{\"end\":55078,\"start\":55006},{\"end\":55511,\"start\":55440},{\"end\":55938,\"start\":55866},{\"end\":56404,\"start\":56350},{\"end\":56819,\"start\":56761},{\"end\":57232,\"start\":57195},{\"end\":57507,\"start\":57423},{\"end\":39627,\"start\":39565},{\"end\":40673,\"start\":40617},{\"end\":40949,\"start\":40945},{\"end\":41320,\"start\":41254},{\"end\":41746,\"start\":41680},{\"end\":42414,\"start\":42338},{\"end\":42849,\"start\":42783},{\"end\":43260,\"start\":43194},{\"end\":43975,\"start\":43909},{\"end\":44886,\"start\":44820},{\"end\":46383,\"start\":46317},{\"end\":47254,\"start\":47198},{\"end\":48509,\"start\":48443},{\"end\":48876,\"start\":48814},{\"end\":49185,\"start\":49181},{\"end\":49958,\"start\":49892},{\"end\":50425,\"start\":50359},{\"end\":50822,\"start\":50766},{\"end\":51189,\"start\":51133},{\"end\":51749,\"start\":51745},{\"end\":52120,\"start\":52054},{\"end\":53330,\"start\":53268},{\"end\":53803,\"start\":53742},{\"end\":54246,\"start\":54228},{\"end\":54700,\"start\":54638},{\"end\":55146,\"start\":55080},{\"end\":55578,\"start\":55513},{\"end\":56006,\"start\":55940},{\"end\":56873,\"start\":56821}]"}}}, "year": 2023, "month": 12, "day": 17}