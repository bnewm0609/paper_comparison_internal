{"id": 232380304, "updated": "2023-10-06 05:47:47.844", "metadata": {"title": "Multimodal Knowledge Expansion", "authors": "[{\"first\":\"Zihui\",\"last\":\"Xue\",\"middle\":[]},{\"first\":\"Sucheng\",\"last\":\"Ren\",\"middle\":[]},{\"first\":\"Zhengqi\",\"last\":\"Gao\",\"middle\":[]},{\"first\":\"Hang\",\"last\":\"Zhao\",\"middle\":[]}]", "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "journal": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2021, "month": 3, "day": 26}, "abstract": "The popularity of multimodal sensors and the accessibility of the Internet have brought us a massive amount of unlabeled multimodal data. Since existing datasets and well-trained models are primarily unimodal, the modality gap between a unimodal network and unlabeled multimodal data poses an interesting problem: how to transfer a pre-trained unimodal network to perform the same task on unlabeled multimodal data? In this work, we propose multimodal knowledge expansion (MKE), a knowledge distillation-based framework to effectively utilize multimodal data without requiring labels. Opposite to traditional knowledge distillation, where the student is designed to be lightweight and inferior to the teacher, we observe that a multimodal student model consistently denoises pseudo labels and generalizes better than its teacher. Extensive experiments on four tasks and different modalities verify this finding. Furthermore, we connect the mechanism of MKE to semi-supervised learning and offer both empirical and theoretical explanations to understand the denoising capability of a multimodal student.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2103.14431", "mag": "3145455064", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/XueRGZ21", "doi": "10.1109/iccv48922.2021.00089"}}, "content": {"source": {"pdf_hash": "5601fb214843399fdc6c59c70f435c7b2880ecc4", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2103.14431v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "5cab2255b7cba84d58b9ec0fadb852916bedf624", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/5601fb214843399fdc6c59c70f435c7b2880ecc4.txt", "contents": "\nMultimodal Knowledge Expansion\n\n\nZihui Xue \nSucheng Ren \nSouth China University of Technology\n\n\nZhengqi Gao \nMIT\n\n\nHang Zhao \nTsinghua University\n\n\nShanghai Qi \nZhi Institute \nU T Austin \nMultimodal Knowledge Expansion\n\nThe popularity of multimodal sensors and the accessibility of the Internet have brought us a massive amount of unlabeled multimodal data. Since existing datasets and well-trained models are primarily unimodal, the modality gap between a unimodal network and unlabeled multimodal data poses an interesting problem: how to transfer a pre-trained unimodal network to perform the same task with extra unlabeled multimodal data? In this work, we propose multimodal knowledge expansion (MKE), a knowledge distillation-based framework to effectively utilize multimodal data without requiring labels. Opposite to traditional knowledge distillation, where the student is designed to be lightweight and inferior to the teacher, we observe that a multimodal student model consistently rectifies pseudo labels and generalizes better than its teacher. Extensive experiments on four tasks and different modalities verify this finding. Furthermore, we connect the mechanism of MKE to semi-supervised learning and offer both empirical and theoretical explanations to understand the expansion capability of a multimodal student.\n\nIntroduction\n\nDeep neural networks and supervised learning have made outstanding achievements in fields like computer vision [16,21,33] and computer audition [17,47]. With the popularity of multimodal data collection devices (e.g., RGB-D cameras and video cameras) and the accessibility of the Internet, a large amount of unlabeled multimodal data has become available. A couple of examples are shown in Figure 1: (a) A unimodal dataset has been previously annotated for the data collected by an old robot; after a hardware upgrade with an additional sensor, the roboticist has access to some new unlabeled multimodal data. (b) Internet videos are abundant and easily accessible. While there are existing unimodal datasets and models for tasks such as image recognition, we further want to perform the same task on unlabeled videos. A natural question arises: how to transfer a unimodal network to the unlabeled multimodal data?\n\nOne naive solution is to directly apply the unimodal network for inference using the corresponding modality of unlabeled data. However, it overlooks information described by the other modalities. While learning with multimodal data has the advantage of facilitating information fusion and inducing more robust models compared with only using one modality, developing a multimodal network with supervised learning requires tremendous human labeling efforts.\n\nIn this work, we propose multimodal knowledge expansion (MKE), a knowledge distillation-based framework, to make the best use of unlabeled multimodal data. MKE enables a multimodal network to learn on the unlabeled data with minimum human labor (i.e., no annotation of the mul- In knowledge distillation, a cumbersome teacher network is considered as the upper bound of a lightweight student network. Contradictory to that, we introduce a unimodal teacher and a multimodal student. The multimodal student achieves knowledge expansion from the unimodal teacher. timodal data is required). As illustrated in Figure 2, a unimodal network pre-trained on the labeled dataset plays the role of a teacher and distills information to a multimodal network, termed as a student. We observe an interesting phenomenon: our multimodal student, trained only on pseudo labels provided by the unimodal teacher, consistently outperforms the teacher under our training framework. We term this observation as knowledge expansion. Namely, a multimodal student is capable of refining pseudo labels. We conduct experimental results on various tasks and different modalities to verify this observation. We further offer empirical and theoretical explanations to understand the expansion capability of a multimodal student.\n\nA closely related setting to ours is semi-supervised learning (SSL), whose goal is to improve a model's performance by leveraging unlabeled data of the same source, including modality. Different from SSL, we aim to develop an additional multimodal network on an unlabeled dataset. Despite the differences in modalities, MKE bears some similarity to SSL in terms of the mechanism. We provide a new perspective in addressing confirmation bias, a traditionally bothering problem in SSL. This bias stems from using incorrect predictions on unlabeled data for training and results in marginal performance gain over the original teacher network [3]. In SSL, various methods, i.e., data augmentation [34,43], injecting noise [44], meta-learning [29] have been proposed to address it. This work provides a novel angle orthogonal to these techniques in alleviating confirmation bias, by resorting to multimodal information. We demonstrate that multimodal inputs serve as a strong regularization, which helps correct inaccurate pseudo labels and overcome the limitation of unimodal networks.\n\n\nRelated Work\n\n\nSemi-supervised Learning\n\nPseudo labeling, also known as self-training, is a simple and powerful technique in SSL, leading to great improvements on tasks such as image classification [23,45,44,29], semantic segmentation [51,10] and domain adaptation [52,22]. One important limitation of pseudo labeling is confirmation bias [3]. Since pseudo labels are inaccurate, the student network may potentially learn these mistakes. Various works have been proposed to alleviate this bias [52,3,44,29] while their discussion is limited to unimodality. Consistency regularization is another important brand of SSL. Based on model smoothness assumption, model predictions are constrained to be invariant to small perturbations of either inputs or model hidden states. A series of works have been proposed on producing random perturbations, such as using an exponential moving average of model parameters [36], data augmentation [43,34], dropout [5,44] or adversarial perturbations [27]. Recent works [7,6,34] combine consistency regularization with pseudo labeling together and demonstrate great benefits.\n\n\nCross-modal Distillation\n\nKnowledge distillation (KD) [18] is an effective technique in transferring information from one network to another. KD has been broadly applied to model compression, where a lightweight student network learns from a cumbersome teacher network [40,18,46,31,37]. Another important application of KD is cross-modal distillation, where a teacher network transfers knowledge from one modality to a student learning from another modality. Various works have been proposed along this direction [15,19,4,28,2,50,49]. Gupta et al. [15] proposes a framework that transfers supervision from labeled RGB images to unlabeled depth and optical flow images. SoundNet [4] learns sound representations from well established visual recognition models using unlabeled videos. Zhao et al. [50] introduces an approach that estimates human poses using radio signals with crossmodal supervision signals provided by vision models.\n\n\nMultimodal Learning\n\nModels fusing data from multiple modalities has shown superior performance over unimodal models in various applications, for instance, sentiment analysis [48,26], emotion recognition [38,30], semantic segmentation [13,39,12,41] and event classification [1]. One recent work [20] provides theoretical justifications on the advantages of multimodal learning over learning with single modality.\n\nWe compare our problem setting with prior works in Table 1. SSL adopts data from the same modality. Crossmodal distillation has the same training data assumption as us while they focus on testing with unimodal data only. Su-  pervised multimodal learning does not take unlabeled data into consideration. Contrary to them, this work discusses a novel and practical scenario where only labeled unimodal and unlabeled multimodal data are available.\n\n\nApproach\n\n\nMultimodal Knowledge Expansion\n\nProblem formulation. Without loss of generality, we limit our discussion to two modalities, denoted as \u03b1 and \u03b2, respectively. We assume that a collection of labeled uni-\nmodal data D l = {(x \u03b1 i , y i )} N i=1\nis given. Each sample input x \u03b1 i has been assigned a one-hot label vector y i = {0, 1} K \u2208 R K , where K is the number of classes. Besides the labeled dataset, an unlabeled multimodal dataset\nD u = {(x \u03b1 i , x \u03b2 i )} M i=1 is available.\nOur goal is to train a network parameterized by \u03b8 (i.e., f (x; \u03b8)) that could accurately predict the label y when its feature x = (x \u03b1 , x \u03b2 ) is given.\n\nTo transfer the knowledge of a labeled unimodal dataset D l to an unlabeled multimodal dataset D u , we present a simple and efficient model-agnostic framework named multimodal knowledge expansion (MKE) in Algorithm 1. We first train a unimodal teacher network \u03b8 \u22c6 t on the labeled dataset D l . Next, the obtained teacher is employed to generate pseudo labels for the multimodal dataset D u , yielding D u . Finally, we train a multimodal student \u03b8 \u22c6 s based on the pseudo-labeledD u with the loss term described in Equation (3)- (5).\n\nIn order to prevent the student from confirming to teacher's predictions (i.e., confirmation bias [3]), the loss term in Equation (3)-(5) has been carefully designed. It combines the standard pseudo label loss (i.e., Equation (4)) and a regularization loss (i.e., Equation (5)). Intuitively speaking, pseudo label loss aims to minimize the difference between a multimodal student and the unimodal teacher, while regularization loss enforces the student to be invariant to small perturbations of input or hidden states. In the context of multimodal learning, the regularization term encourages the multimodal student to learn from the information brought by the extra modality \u03b2, and meanwhile, ensures that the student does not overfit to teacher's predictions based solely on modality \u03b1. Note that in our implementation, to avoid introducing and tuning one extra hyperparameter \u03b3 and save computation time, we train the student network with \u03b8 \u22c6 s = argmin \u03b8s\n1 M M i=1 l cls (\u1ef9 i , T (f s (x \u03b1 i , x \u03b2 i ; \u03b8 s ))\n, which is equivalent to Equation (3). The detailed proof is provided in the supplementary material.\n\nAn illustrative example. We consider a variant of the 2D-TwoMoon [3] problem shown in Figure 3a. The data located at the upper moon and lower moon have true labels 0 and 1, and are colored by red and blue, respectively. The deeply blue-or red-colored large dots compose the labeled unimodal dataset D l , and only their X coordinates are known. On the other hand, D u consists of all lightlycolored small dots, with both X and Y coordinates available. Namely, modality \u03b1 and \u03b2 are interpreted as observing from the X-axis and Y-axis, respectively.\nAlgorithm 1 multimodal knowledge expansion (MKE) (1) Train a unimodal teacher \u03b8 \u22c6 t with the labeled dataset D l = {(x \u03b1 i , y i )} N i=1 : \u03b8 \u22c6 t = argmin \u03b8t 1 N N i=1 l cls (y i , f t (x \u03b1 i ; \u03b8 t )) (1) (2) Generate pseudo labels for D u = {(x \u03b1 i , x \u03b2 i )} M i=1 by using the teacher model \u03b8 \u22c6 t , yielding the pseudo-labeled datasetD u = {(x \u03b1 i , x \u03b2 i ,\u1ef9 i )} M i=1 : y i = f t (x \u03b1 i ; \u03b8 \u22c6 t ), \u2200 (x \u03b1 i , x \u03b2 i ) \u2208 D u(2)\n(3) Train a multimodal student \u03b8 \u22c6 s withD u :\n\u03b8 \u22c6 s = argmin \u03b8s (L pl + \u03b3L reg ) (3) L pl = 1 M M i=1 l cls (\u1ef9 i , f s (x \u03b1 i , x \u03b2 i ; \u03b8 s )) (4) L reg = M i=1 l reg [f s (x \u03b1 i , x \u03b2 i ; \u03b8 s ), T (f s (x \u03b1 i , x \u03b2 i ; \u03b8 s ))] (5)\nl cls : cross entropy loss for hard\u1ef9 i and KL divergence loss for soft\u1ef9 i . l reg : a distance metric (e.g., L2 norm). \u03b3: a constant balancing the weight of L pl and L reg .\n\nT : a transformation defined on the student model, realized via input or model perturbations (i.e., augmentations, dropout).\n\nWe first train a teacher with the labeled unimodal dataset D l . The learned classification boundary is demonstrated in Figure 3b. Next, we adopt the learned teacher to generate pseudo labels for D u . As indicated in Figure 3c, pseudo labels may be inaccurate and disagree with ground truth: in our toy example, the unimodal teacher only yields 68% accuracy. As shown in Figure 3f, provided with these not-soaccurate pseudo labels, the student could still outperform the teacher by a large margin (i.e., about 13% more accurate). It presents a key finding in our work: Despite no (a) Deeply-colored large dots observed from the X-axis compose D l and lightly-colored small dots observed from the XY-plane compose Du.\n\n(b) Train a unimodal classifier on labeled data points. Since only X coordinates of the data in D l are known, it is natural that the boundary is vertical.\n\n(c) The teacher network generates pseudo labels for unlabeled data (d) Naive pseudo labeling overfits to inaccurate pseudo labels (e) Consistency regularization slightly improves a unimodal student (f) MKE greatly improves a multimodal student access to ground truth, a multimodal student is capable of correcting inaccurate labels and outperforms the teacher network. Knowledge expansion is achieved.\n\n\nRectifying Pseudo Labels\n\nThe somewhat surprising finding about knowledge expansion further motivates our thinking: where does the expansion capability of a multimodal student come from? In this section, we will answer this question with the TwoMoon example.\n\nTo start with, we consider directly adopting unimodal SSL for this problem. Namely, given a teacher network \u03b8 \u22c6 t trained with labeled data D l and an unlabeled multi-modal dataset D u , the student network takes x \u03b1 i \u2208 D u as input. Naive pseudo labeling [23] uses the following loss to minimize the disagreement between the fixed teacher \u03b8 \u22c6 t and a student network \u03b8 s :\nL \u2032 pl = E x \u03b1 i \u2208Du {l cls [f t (x \u03b1 i ; \u03b8 \u22c6 t ), f s (x \u03b1 i ; \u03b8 s )]}(6)\nHowever, due to confirmation bias [3], the student network is likely to overfit to incorrect pseudo labels pro-vided by the teacher network, yielding f s (x; \u03b8 \u22c6 s ) similar to f t (x; \u03b8 \u22c6 t ), if not identical. In the TwoMoon example, we observe that the unimodal student trained with Equation (6) achieves similar performance as its teacher. This is demonstrated in Figure 3d.\n\nTo address this bias, we follow the thought of consistency training methods in SSL [27,43,34] and introduce one general regularization loss term to enforce model smoothness:\nL \u2032 reg = E x \u03b1 i \u2208Du {l reg [f s (x \u03b1 i ; \u03b8 s ), T \u2032 (f s (x \u03b1 i ; \u03b8 s ))]} (7)\nNamely, L \u2032 reg encourages the model to output similar predictions for small perturbations of the input or the model. T \u2032 (f s (x \u03b1 i ; \u03b8 s )) denotes transformation applied to unimodal inputs or model hidden states, which can be realized via input augmentation, noise, dropout, etc. As shown in Figure 3e, the unimodal student trained with a combined loss of Equation (6)-(7) achieves about 69.50% prediction accuracy. While it indeed outperforms the teacher of 68.00% accuracy shown in Figure 3b, the unimodal student under consistency regularization fails to utilize unlabeled data effectively and only brings marginal improvement. Although confirmation bias is slightly reduced by the regularization term in Equation (7), it still heavily constrains performance of unimodal SSL methods.\n\nTherefore, we turn to multimodality as a solution and resort to the information brought by modality \u03b2. Utilizing both modalities in D u , we substitute unimodal inputs shown in Equation (6)-(7) with multimodal ones and derive the loss terms for training a multimodal student:\nL pl = E{l cls [f t (x \u03b1 i ; \u03b8 \u22c6 t ), f s (x \u03b1 i , x \u03b2 i ; \u03b8 s )]}(8)L reg = E{l reg [f s (x \u03b1 i , x \u03b2 i ; \u03b8 s ), T (f s (x \u03b1 i , x \u03b2 i ; \u03b8 s ))]} (9)\nwhere both expectations are performed with respect to (x \u03b1 i , x \u03b2 i ) \u2208 D u . In fact, Equation (8)-(9) reduces to Equation (4)-(5) when D u is a finite set containing M multimodal samples. As shown in Figure 3f, we observe substantial improvement of a multimodal student (i.e., 81.00% accuracy) over the teacher (i.e., 68.00% accuracy). It implies that a multimodal student effectively alleviates confirmation bias and leads to superior performance over the teacher.\n\nTo understand the principles behind this phenomenon, we train one unimodal student with Equation (6)-(7) and one multimodal student with Equation (8)-(9) on the TwoMoon data. Transformation T is defined on model inputs and implemented as additive Gaussian noise. Figure 4 visualizes the transformation space of one data sample A with both pseudo label and true label being \"red\". Data B is one point that the teacher predicts \"blue\" while its true label is \"red\". The pseudo label and true label of data C are \"blue\". When training a unimodal student, we only know the X coordinates of data points, and the transformation space defined by T \u2032 is given by the 1-D red line on X-axis. Under this circumstance, minimizing L \u2032 reg in Equation (7) encourages the unimodal student to predict label \"red\" for the data point located in the red line. This is the case for B, but it will also flip the teacher's prediction for C and make it wrong! The intrinsic reason is that restricted by unimodal inputs, the student network can not distinguish along the Yaxis and mistakenly assumes that C locates near A.\n\nOn the contrary, the extra modality \u03b2 helps us see the real distances among A, B, and C. Transformation space of data A in the case of a multimodal student is given by the red circle in Figure 4. A multimodal student is guided to predict \"red\" for data falling inside the circle. This time B locates in the transformation space, while C doesn't. Therefore, the multimodal student can correct the wrong pseudo label of data B due to the regularization constraint in Equation (9), and its decision boundary is pushed closer to the ground truth. This example demonstrates that multimodality serves as a strong regularization and enables the student to \"see\" something beyond the scope of its teacher, resulting in knowledge expansion.\n\n\nTheoretical Analysis\n\nIn this section, we provide a theoretical analysis of MKE. Building upon unimodal self-training [42], we prove that our multimodal student improves over pseudo labels given by the teacher.\n\nConsider a K-way classification problem, and assume that we have a teacher network pre-trained on a collection of labeled data D l . We further assume a set of unlabeled mul-\ntimodal data D u = {x i = (x \u03b1 i , x \u03b2 i ) \u2208 X } M i=1 is available, where X = X \u03b1 \u00d7 X \u03b2 . Let f \u22c6 (x; \u03b8 \u22c6 ), f t (x; \u03b8 t ), f s (x; \u03b8 s )\ndenote the ground truth classifier, a teacher classifier, and a student classifier, respectively. Error of an arbitrary classifier f (x; \u03b8) is defined as:\nErr(f (x; \u03b8)) = E x [f (x; \u03b8) \u0338 = f \u22c6 (x; \u03b8 \u22c6 )].\nLet P refer to a distribution of unlabeled samples over input space X . P i denotes the class-conditional distribution of x conditioned on f \u22c6 (x; \u03b8 \u22c6 ) = i. We use M(\u03b8 t ) \u2286 D u to denote the set of multimodal data that the teacher gives wrong predictions on, i.e., M(\u03b8 t ) =\n{(x \u03b1 , x \u03b2 )|f t (x \u03b1 ; \u03b8 t ) \u0338 = f \u22c6 (x \u03b1 ; \u03b8 \u22c6 ), (x \u03b1 , x \u03b2 ) \u2208 D u }. Let a = max i {P i (M(\u03b8 t )}\nrefer to the maximum fraction of data misclassified by the teacher network in any class.\n\nWe first require data distribution P to satisfy the following expansion assumption, which states that data distribution has good continuity in input spaces.\n\nAssumption 1 P satisfies (\u0101, c 1 ) and (\u0101, c 2 ) expansion [42] on X \u03b1 and X \u03b2 , respectively, with 1 < min(c 1 , c 2 ) \u2264 max(c 1 , c 2 ) \u2264 1 a and c 1 c 2 > 5.\nP i (N (V \u03b1 )) \u2265 min{c 1 P i (V \u03b1 ), 1}, \u2200 i \u2208 [K], \u2200 V \u03b1 \u2286 X \u03b1 with P i (V \u03b1 ) \u2264\u0101 (10) P i (N (V \u03b2 )) \u2265 min{c 2 P i (V \u03b2 ), 1}, \u2200 i \u2208 [K], \u2200 V \u03b2 \u2286 X \u03b2 with P i (V \u03b2 ) \u2264\u0101(11)\nwhere N (V ) denotes the neighborhood of a set V , following the same definition as in [42]. Furthermore, we assume conditional independence of multimodal data in Assumption 2, which is widely adopted in the literature of multimodal learning [24,8,35].\n\nAssumption 2 Conditioning on ground truth labels, X \u03b1 and X \u03b2 are independent.\nP i (V \u03b1 , V \u03b2 ) = P i (V \u03b1 ) \u00b7 P i (V \u03b2 ), \u2200 i \u2208 [K], \u2200 V \u03b1 \u2286 X \u03b1 , \u2200 V \u03b2 \u2286 X \u03b2(12)\nLemma 1 Data distribution P on X satisfies (\u0101, c 1 c 2 ) expansion.\n\nProof of Lemma 1 is provided in the supplementary material. We state below that the error of a multimodal student classifier is upper-bounded by the error of its teacher. We follow the proof in [42] to prove Theorem 1.  (13) where \u00b5 appears in Assumption 3.3 of [42] and is expected to be small or negligible. Theorem 1 helps explain the empirical finding about knowledge expansion. Training a multimodal student f (x \u03b1 , x \u03b2 ; \u03b8 s ) on pseudo labels given by a pre-trained teacher network f (x \u03b1 ; \u03b8 t ) refines pseudo labels. In addition, the error bound of a unimodal student f s (x \u03b1 ; \u03b8 s ) that only takes inputs from modality \u03b1 and pseudo labels is given by:\nErr(f s (x \u03b1 , x \u03b2 ; \u03b8 s )) \u2264 4 \u00b7 Err(f t (x \u03b1 ; \u03b8 t )) c 1 c 2 \u2212 1 + 4\u00b5Err(f s (x \u03b1 ; \u03b8 s )) \u2264 4 \u00b7 Err(f t (x \u03b1 ; \u03b8 t )) c 1 \u2212 1 + 4\u00b5(14)\nBy comparing Equation (13) and (14), we observe that the role of multimodality is to increase the expansion factor from c 1 to c 1 c 2 and to improve the accuracy bound. This observation further confirms our empirical finding and unveils the role of MKE in refining pseudo labels from a theoretical perspective.\n\n\nExperimental Results\n\nTo verify the efficiency and generalizability of the proposed method, we perform a thorough test of MKE on various tasks: (i) binary classification on the synthetic TwoMoon dataset, (ii) emotion recognition on RAVDESS [25] dataset, (iii) semantic segmentation on NYU Depth V2 [32] dataset, and (iv) event classification on AudioSet [14] and VGGsound [9] dataset. We emphasize that the above four tasks cover a broad combination of modalities. For instance, modalities \u03b1 and \u03b2 represent images and audios in (ii), where images are considered as a \"weak\" modality in classifying emotions than images. In (iii), modality \u03b1 and \u03b2 refer to RGB and depth images, respectively, where RGB images play a central role in semantic segmentation and depth images provide useful cues.\n\nBaselines. Our multimodal student (termed as MM student) trained with MKE is compared with the following baselines:\n\n\u2022 UM teacher: a unimodal teacher network trained on (x \u03b1 , y i ) \u2208 D l .\n\n\u2022 UM student: a unimodal student network trained on (x \u03b1 ,\u1ef9 i ) \u2208D u (i.e., uni-modal inputs and pseudo labels given by the UM teacher).\n\n\u2022 NOISY student [44]: a unimodal student network trained on (x \u03b1 , y i ) \u2208 D l \u222a (x \u03b1 ,\u1ef9 i ) \u2208D u with noise injected during training.\n\n\u2022 MM student (no reg): a multimodal student network trained with no regularization (i.e., Equation (5) is not applied during training).\n\n\u2022 MM student (sup): a multimodal student trained on D u with true labels provided. This supervised version can be regarded as the upper bound of our multimodal student.\n\nSince iterative training [44] can be applied to other baselines and our MM student as well, the number of iterations of a NOISY student is set as one to ensure a fair comparison. We employ different regularization techniques as T in Equation (5) for the four tasks to demonstrate the generalizability of our proposed methods. Regularization is applied to all baselines identically except for MM student (no reg).\n\nFurthermore, we present an ablation study of various components of MKE, i.e., unlabeled data size, teacher model, hard vs. soft labels, along with dataset and implementation details in the supplementary material.\n\n\nTwoMoon Experiment\n\nWe first provide results on synthetic TwoMoon data. We generate 500 samples making two interleaving half circles, each circle corresponding to one class. The dataset is randomly split as 30 labeled samples, 270 unlabeled samples and 200 test samples. X and Y coordinates of data are interpreted as modality \u03b1 and \u03b2, respectively.\n\nBaselines & Implementation. We implement both the UM teacher and the UM student networks as 3-layer MLPs with 32 hidden units, while the MM student has 16 hidden units. We design three kinds of transformations T = {T 1 , T 2 , T 3 } used in Equation (5): (i) T 1 : adding zero-mean Gaussian noise to the input with variance v 0 , (ii) T 2 : adding zeromean Gaussian noise to outputs of the first hidden layer with variance v 1 , and (iii) T 3 : adding a dropout layer with dropout rate equal to r 0 . By adjusting the values of v 0 , v 1 and r 0 , we could test all methods under no / weak / strong regularization. Specifically, higher values indicate stronger regularization.\n\n\nMethods\n\nTest Accuracy (%) UM teacher 68.00  Results. Table 2 demonstrates that a MM student under consistency regularization outperforms its unimodal counterpart in all cases of T . Specifically, a MM student under strong regularization achieves closes results with MM student (sup), as shown in the last column. The small gap between a MM student (trained on pseudo labels) and its upper bound (trained on true labels) indicates the great expansion capability of MKE. In addition, we observe better performance of both UM and MM student with increasing regularization strength, demonstrating that consistency regularization is essential in alleviating confirmation bias.\nT 1 v 0 = 0 v 0 = 1 v 0 = 2 UM\n\nEmotion Recognition\n\nWe evaluate MKE on RAVDESS [25] dataset for emotion recognition. The dataset is randomly split as 2:8 for D l and D u and 8:1:1 as train / validation / test for D u . Images and audios are considered as modality \u03b1 and \u03b2, respectively.\n\nBaselines & Implementation. For the MM student, we adopt two 3-layer CNNs to extract image and audio features, respectively. The two visual and audio features are concatenated into a vector and then passed through a 3-layer MLP. The UM teacher, UM student and NOISY student are identical to the image branch of a MM student network, also followed by a 3-layer MLP. T in Equation (5) is implemented as one dropout layer of rate 0.5. Table 3, with the assistance of labeled data and consistency regularization, NOISY student generalizes better than the UM teacher and UM student, achieving 83.09% accuracy over 80.33% and 77.79%. Still, the improvement is trivial. In contrast, our MM student network improves substantially over the original teacher network despite no access to ground truth and leads to 91.38% test accuracy. The great performance gain can be attributed to additional information brought by audio modality. It demonstrates that MKE can be plugged into existing SSL methods like NOISY student for boosting performance when multimodal data are available. Furthermore, regularization helps our MM student yield better performance than the MM student (no reg). More results are presented in the supplementary material.\n\n\nResults. As shown in\n\n\nMethods\n\nTrain  \n\n\nSemantic Segmentation\n\nWe evaluate our method on NYU Depth V2 [32]. It contains 1449 RGB-D images with 40-class labels, where 795 RGB images are adopted as D l for training the UM teacher and the rest 654 RGB-D images are for testing. Besides labeled data, NYU Depth V2 also provides unannotated video sequences, where we randomly extract 1.5K frames of RGB-D images as D u for training the student. Modality \u03b1 and \u03b2 represents RGB images and depth images.\n\n\nMethod\n\nTrain data Test mIoU (%) mod D lDu UM teacher rgb \u2713 44.15 Naive student [10] rgb \u2713 46.13 NOISY student [44] rgb \u2713 \u2713 47.68 Gupta et al. [15] rgb, d \u2713 45.65 CMKD [49] rgb   [49]. Due to different problem settings, we slightly modified cross-modal distillation methods to make them comparable. Since RGB-D images from D u are unannotated, we are unable to train a supervised version of the MM student (i.e., MM student (sup)) in this task. We adopt ResNet-101 [16] as backbone and DeepLab V3+ [11] as decoder for the UM teacher. In terms of training a MM student, depth images are first converted to HHA images and then passed to a fusion network architecture proposed in [12] along with RGB images. We design the UM student architecture as the RGB branch of a MM student network. For the regularization term, we employ input augmentation for RGB images, i.e., random horizontal flipping and scaling with scales [0.5,1.75].\n\nResults. Table 4 reports mean Intersection-over-Union (mIoU) of each method. We observe that a MM student greatly improves over the UM teacher, i.e., achieves a mIoU of 48.88 % while it is trained on pseudo labels of approximately 44.15% mIoU. Furthermore, provided with no ground truth, our MM student outperforms all baselines with a considerable performance gain. This demonstrates the effectiveness of MKE. We also arrive at the same conclusion that regularization helps improve the MM student since our MM student yields higher accuracy than a MM student (no reg). It indicates that MKE and current SSL methods that focus on designing augmentations to emphasize consistency regularization can be combined together to boost performance.\n\nVisualization results in Figure 5 demonstrate that our MM student refines pseudo labels and achieves knowledge expansion. Although it receives noisy predictions given by the UM teacher, our MM student does a good job in handling details and maintaining intra-class consistency. As shown in the third and fourth row, the MM student is robust to illumination changes while the UM teacher and NOISY student easily get confused. Depth modality helps our MM student better distinguish objects and correct wrong predictions it receives.\n\nMore qualitative examples are shown in the supplementary material.\n\n\nEvent Classification\n\nWe present experimental results on a real-world application, event classification. 3.7K audios from AudioSet [14] and 3.7K audio-video pairs from VGGSound [9] are taken as the labeled unimodal dataset D l and unlabeled multimodal dataset D u , respectively. In this task, modality \u03b1 and \u03b2 correspond to audios and videos.\n\nBaslines & Implementation. For the UM teacher, we take ResNet-18 as the backbone and a linear layer as classification layer. For the MM student, the audio backbone is identical to that of the UM teacher, and the video backbone is a ResNet-18 with 3D convolution layers. Features from the audio and video backbone are concatenated together before feeding into one classification layer. Following the same regularization term of [9], we randomly sample audio clips of 5 seconds and apply short-time Fourier Transformation for 257 \u00d7 500 spectrograms during training.\n\nResults. Table 5 reports mean Average Precision (mAP) of each method. The baseline model is the UM teacher trained on D l , which achieves a 0.345 mAP. Benefiting from the video modality, our MM student achieves best performance with a mAP of 0.427, outperforming NOSIY student [44] and cross-modal distillation methods [28] [49]. Notably, the difference between our MM student and its upper bound (i.e., MM student (sup)) is small, showing great potentials of MKE in correcting pseudo labels.\n\n\nMethod\n\nTrain data Test mAP mod D lDu UM teacher a \u2713 0.345 UM student a \u2713 0.406 NOISY student [44] a \u2713 \u2713 0.411 Owens et al. [28] a, v \u2713 0.371 CMKD [49] a, v \u2713 0.372 MM student (no reg) a, v \u2713 0.421 MM student (ours) a, v \u2713 0.427 MM student (sup) a, v \u22c6 0.434 Table 5: Results of event classification on AudioSet and VG-GSound. a and v indicate audios and videos.\n\n\nConclusion\n\nMotivated by recent progress on multimodal data collection, we propose a multimodal knowledge expansion framework to effectively utilize abundant unlabeled multimodal data. We provide theoretical analysis and conduct extensive experiments, demonstrating that a multimodal student corrects inaccurate predictions and achieves knowledge expansion from the unimodal teacher. In addition, compared with current semi-supervised learning methods, MKE offers a novel angle in addressing confirmation bias.\n\nFigure 1 :\n1The popularity of multimodal data collection devices and the Internet engenders a large amount of unlabeled multimodal data. We show two examples above: (a) after a hardware upgrade, lots of unannotated multimodal data are collected by the new sensor suite; (b) large-scale unlabeled videos can be easily obtained from the Internet.\n\nFigure 2 :\n2Framework of MKE.\n\nFigure 3 :\n3(a)-(c) problem description and illustration of MKE using the TwoMoon example; (d)-(f) comparison of naive pseudo labeling, consistency training methods, and the proposed MKE. Values in the bottom right corner denotes test accuracy (%).\n\nFigure 4 :\n4Illustration of the transformation space of one data sample A. The 1-D red line on X-axis corresponds to the transformation space of a unimodal student while the 2-D red circle corresponds to that of a multimodal student.\n\nFigure 5 :\n5Qualitative segmentation results on NYU Depth V2 test set.\n\nTable 1 :\n1Comparison of our data assumption with prior works. UM and MM denotes unimodality and multimodality respectively.\n\nTable 2 :\n2Results of TwoMoon experiment. A MM student signif-\nicantly outperforms a UM student and teacher under consistency \nregularization. \n\n\n\nTable 3 :\n3Results of emotion recognition on RAVDESS. mod, i and a denote modality, images and audios, respectively. Data used for training each method is listed. \u22c6 means that the MM student (sup) is trained on true labels instead of pseudo labels inDu.\n\nTable 4 :\n4Results of semantic segmentation on NYU Depth V2. rgb and d denote RGB images and depth images.Baselines & Implementation. We compare MKE against SSL methods[10] [44] and cross-modal distillation[15] \n\nMultimodal categorization of crisis events in social media. Mahdi Abavisani, Liwei Wu, Shengli Hu, Joel Tetreault, Alejandro Jaimes, CVPR. 2020Mahdi Abavisani, Liwei Wu, Shengli Hu, Joel Tetreault, and Alejandro Jaimes. Multimodal categorization of crisis events in social media. In CVPR, pages 14679-14689, 2020. 2\n\nLook, listen and learn. Relja Arandjelovic, Andrew Zisserman, ICCV. Relja Arandjelovic and Andrew Zisserman. Look, listen and learn. In ICCV, pages 609-617, 2017. 2\n\nPseudo-labeling and confirmation bias in deep semi-supervised learning. Eric Arazo, Diego Ortego, Paul Albert, E Noel, Kevin O&apos;connor, Mcguinness, IJCNN. 34Eric Arazo, Diego Ortego, Paul Albert, Noel E O'Connor, and Kevin McGuinness. Pseudo-labeling and confirmation bias in deep semi-supervised learning. In IJCNN, pages 1-8. IEEE, 2020. 2, 3, 4\n\nSoundnet: Learning sound representations from unlabeled video. Yusuf Aytar, Carl Vondrick, Antonio Torralba, NeurIPS. 2Yusuf Aytar, Carl Vondrick, and Antonio Torralba. Sound- net: Learning sound representations from unlabeled video. NeurIPS, 2016. 2\n\nLearning with pseudo-ensembles. Philip Bachman, Ouais Alsharif, Doina Precup, Proceedings of the 27th International Conference on Neural Information Processing Systems. the 27th International Conference on Neural Information Processing Systems2Philip Bachman, Ouais Alsharif, and Doina Precup. Learn- ing with pseudo-ensembles. In Proceedings of the 27th In- ternational Conference on Neural Information Processing Systems-Volume 2, pages 3365-3373, 2014. 2\n\nRemixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring. David Berthelot, Nicholas Carlini, D Ekin, Alex Cubuk, Kihyuk Kurakin, Han Sohn, Colin Zhang, Raffel, ICLR. 2David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Ku- rakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remix- match: Semi-supervised learning with distribution alignment and augmentation anchoring. ICLR, 2020. 2\n\nMixmatch: A holistic approach to semi-supervised learning. David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, Colin Raffel, arXiv:1905.02249arXiv preprintDavid Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin Raffel. Mixmatch: A holistic approach to semi-supervised learning. arXiv preprint arXiv:1905.02249, 2019. 2\n\nCombining labeled and unlabeled data with co-training. Avrim Blum, Tom Mitchell, Proceedings of the eleventh annual conference on Computational learning theory. the eleventh annual conference on Computational learning theoryAvrim Blum and Tom Mitchell. Combining labeled and un- labeled data with co-training. In Proceedings of the eleventh annual conference on Computational learning theory, pages 92-100, 1998. 5\n\nVggsound: A large-scale audio-visual dataset. Honglie Chen, Weidi Xie, Andrea Vedaldi, Andrew Zisserman, ICASSP. IEEE6Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zis- serman. Vggsound: A large-scale audio-visual dataset. In ICASSP, pages 721-725. IEEE, 2020. 6, 8\n\nNaive-student: Leveraging semi-supervised learning in video sequences for urban scene segmentation. Liang-Chieh Chen, Raphael Gontijo Lopes, Bowen Cheng, D Maxwell, Collins, D Ekin, Barret Cubuk, Hartwig Zoph, Jonathon Adam, Shlens, ECCV. 27Liang-Chieh Chen, Raphael Gontijo Lopes, Bowen Cheng, Maxwell D Collins, Ekin D Cubuk, Barret Zoph, Hartwig Adam, and Jonathon Shlens. Naive-student: Leveraging semi-supervised learning in video sequences for urban scene segmentation. In ECCV, pages 695-714, 2020. 2, 7\n\nRethinking atrous convolution for semantic image segmentation. Liang-Chieh Chen, George Papandreou, Florian Schroff, Hartwig Adam, arXiv:1706.05587arXiv preprintLiang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for seman- tic image segmentation. arXiv preprint arXiv:1706.05587, 2017. 7\n\nBi-directional cross-modality feature propagation with separation-andaggregation gate for rgb-d semantic segmentation. Xiaokang Chen, Kwan-Yee Lin, Jingbo Wang, Wayne Wu, Chen Qian, Hongsheng Li, Gang Zeng, ECCV. 27Xiaokang Chen, Kwan-Yee Lin, Jingbo Wang, Wayne Wu, Chen Qian, Hongsheng Li, and Gang Zeng. Bi-directional cross-modality feature propagation with separation-and- aggregation gate for rgb-d semantic segmentation. ECCV, 2020. 2, 7\n\nDeep multi-modal object detection and semantic segmentation for autonomous driving: Datasets, methods, and challenges. Di Feng, Christian Haase-Schuetz, Lars Rosenbaum, Heinz Hertlein, Claudius Glaeser, Fabian Timm, Werner Wiesbeck, Klaus Dietmayer, IEEE Transactions on Intelligent Transportation Systems. 2Di Feng, Christian Haase-Schuetz, Lars Rosenbaum, Heinz Hertlein, Claudius Glaeser, Fabian Timm, Werner Wies- beck, and Klaus Dietmayer. Deep multi-modal object de- tection and semantic segmentation for autonomous driving: Datasets, methods, and challenges. IEEE Transactions on Intelligent Transportation Systems, 2020. 2\n\nAudio set: An ontology and humanlabeled dataset for audio events. Jort F Gemmeke, P W Daniel, Dylan Ellis, Aren Freedman, Wade Jansen, Channing Lawrence, Manoj Moore, Marvin Plakal, Ritter, ICASSP. IEEE6Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human- labeled dataset for audio events. In ICASSP, pages 776-780. IEEE, 2017. 6, 8\n\nCross modal distillation for supervision transfer. Saurabh Gupta, CVPR. 27Saurabh Gupta et al. Cross modal distillation for supervision transfer. In CVPR, 2016. 2, 7\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. 17Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770-778, 2016. 1, 7\n\nCnn architectures for large-scale audio classification. Shawn Hershey, Sourish Chaudhuri, P W Daniel, Ellis, F Jort, Aren Gemmeke, Channing Jansen, Manoj Moore, Devin Plakal, Platt, A Rif, Bryan Saurous, Seybold, ICASSP. IEEEShawn Hershey, Sourish Chaudhuri, Daniel PW Ellis, Jort F Gemmeke, Aren Jansen, R Channing Moore, Manoj Plakal, Devin Platt, Rif A Saurous, Bryan Seybold, et al. Cnn ar- chitectures for large-scale audio classification. In ICASSP, pages 131-135. IEEE, 2017. 1\n\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.02531Distilling the knowledge in a neural network. arXiv preprintGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill- ing the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. 2\n\nCross-modal adaptation for rgb-d detection. Judy Hoffman, Saurabh Gupta, Jian Leong, Sergio Guadarrama, Trevor Darrell, ICRA. Judy Hoffman, Saurabh Gupta, Jian Leong, Sergio Guadar- rama, and Trevor Darrell. Cross-modal adaptation for rgb-d detection. In ICRA, pages 5032-5039. IEEE, 2016. 2\n\nWhat makes multimodal learning better than single (provably). Yu Huang, Chenzhuang Du, Zihui Xue, Xuanyao Chen, Hang Zhao, Longbo Huang, arXiv:2106.04538arXiv preprintYu Huang, Chenzhuang Du, Zihui Xue, Xuanyao Chen, Hang Zhao, and Longbo Huang. What makes multi- modal learning better than single (provably). arXiv preprint arXiv:2106.04538, 2021. 2\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, NeurIPS. 251Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural net- works. NeurIPS, 25:1097-1105, 2012. 1\n\nUnderstanding self-training for gradual domain adaptation. Ananya Kumar, ICML. Ananya Kumar et al. Understanding self-training for gradual domain adaptation. In ICML, 2020. 2\n\nPseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. Dong-Hyun Lee, Workshop on challenges in representation learning, ICML. 34Dong-Hyun Lee. Pseudo-label: The simple and effi- cient semi-supervised learning method for deep neural net- works. In Workshop on challenges in representation learn- ing, ICML, volume 3, 2013. 2, 4\n\nNaive (bayes) at forty: The independence assumption in information retrieval. D David, Lewis, ECCV. SpringerDavid D Lewis. Naive (bayes) at forty: The independence assumption in information retrieval. In ECCV, pages 4-15. Springer, 1998. 5\n\nThe ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english. R Steven, Livingstone, A Frank, Russo, PloS one. 1357Steven R Livingstone and Frank A Russo. The ryer- son audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vo- cal expressions in north american english. PloS one, 13(5):e0196391, 2018. 6, 7\n\nMultimodal sentiment analysis using hierarchical fusion with context modeling. Knowledge-based systems. Navonil Majumder, Devamanyu Hazarika, 161Alexander Gelbukh, Erik Cambria, and Soujanya PoriaNavonil Majumder, Devamanyu Hazarika, Alexander Gel- bukh, Erik Cambria, and Soujanya Poria. Multimodal senti- ment analysis using hierarchical fusion with context model- ing. Knowledge-based systems, 161:124-133, 2018. 2\n\nVirtual adversarial training: a regularization method for supervised and semi-supervised learning. Takeru Miyato, Masanori Shin-Ichi Maeda, Shin Koyama, Ishii, IEEE TPAMI. 4184Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual adversarial training: a regularization method for supervised and semi-supervised learning. IEEE TPAMI, 41(8):1979-1993, 2018. 2, 4\n\nAmbient sound provides supervision for visual learning. Andrew Owens, Jiajun Wu, Josh H Mcdermott, T William, Antonio Freeman, Torralba, ECCV. 2Andrew Owens, Jiajun Wu, Josh H McDermott, William T Freeman, and Antonio Torralba. Ambient sound provides supervision for visual learning. In ECCV, pages 801-816, 2016. 2, 8\n\n. Hieu Pham, Qizhe Xie, Zihang Dai, Quoc V Le, arXiv:2003.10580Meta pseudo labels. arXiv preprintHieu Pham, Qizhe Xie, Zihang Dai, and Quoc V Le. Meta pseudo labels. arXiv preprint arXiv:2003.10580, 2020. 2\n\nMultimodal emotion recognition using deep learning architectures. Shayok Hiranmayi Ranganathan, Sethuraman Chakraborty, Panchanathan, IEEE. 2Hiranmayi Ranganathan, Shayok Chakraborty, and Sethura- man Panchanathan. Multimodal emotion recognition using deep learning architectures. pages 1-9. IEEE, 2016. 2\n\nAdriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, Yoshua Bengio, arXiv:1412.6550Fitnets: Hints for thin deep nets. arXiv preprintAdriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550, 2014. 2\n\nIndoor segmentation and support inference from rgbd images. Nathan Silberman, Derek Hoiem, Pushmeet Kohli, Rob Fergus, ECCV. Springer67Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, pages 746-760. Springer, 2012. 6, 7\n\nVery deep convolutional networks for large-scale image recognition. Karen Simonyan, Andrew Zisserman, arXiv:1409.1556arXiv preprintKaren Simonyan and Andrew Zisserman. Very deep convo- lutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014. 1\n\nFixmatch: Simplifying semisupervised learning with consistency and confidence. Kihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, D Ekin, Alex Cubuk, Han Kurakin, Colin Zhang, Raffel, arXiv:2001.0768524arXiv preprintKihyuk Sohn, David Berthelot, Chun-Liang Li, Zizhao Zhang, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Han Zhang, and Colin Raffel. Fixmatch: Simplifying semi- supervised learning with consistency and confidence. arXiv preprint arXiv:2001.07685, 2020. 2, 4\n\nTcgm: An information-theoretic framework for semi-supervised multimodality learning. Xinwei Sun, Yilun Xu, Peng Cao, Yuqing Kong, Lingjing Hu, Shanghang Zhang, Yizhou Wang, ECCV. Xinwei Sun, Yilun Xu, Peng Cao, Yuqing Kong, Lingjing Hu, Shanghang Zhang, and Yizhou Wang. Tcgm: An information-theoretic framework for semi-supervised multi- modality learning. In ECCV, pages 171-188, 2020. 5\n\nMean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Antti Tarvainen, Harri Valpola, NeurIPS. Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In NeurIPS, 2017. 2\n\nSimilarity-preserving knowledge distillation. Frederick Tung, Greg Mori, ICCV. Frederick Tung and Greg Mori. Similarity-preserving knowl- edge distillation. In ICCV, pages 1365-1374, 2019. 2\n\nEnd-toend multimodal emotion recognition using deep neural networks. Panagiotis Tzirakis, George Trigeorgis, A Mihalis, Nicolaou, W Bj\u00f6rn, Stefanos Schuller, Zafeiriou, IEEE Journal of Selected Topics in Signal Processing. 118Panagiotis Tzirakis, George Trigeorgis, Mihalis A Nico- laou, Bj\u00f6rn W Schuller, and Stefanos Zafeiriou. End-to- end multimodal emotion recognition using deep neural net- works. IEEE Journal of Selected Topics in Signal Process- ing, 11(8):1301-1309, 2017. 2\n\nSelfsupervised model adaptation for multimodal semantic segmentation. IJCV. Abhinav Valada, Rohit Mohan, Wolfram Burgard, Abhinav Valada, Rohit Mohan, and Wolfram Burgard. Self- supervised model adaptation for multimodal semantic seg- mentation. IJCV, pages 1-47, 2019. 2\n\nKnowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks. Lin Wang, Kuk-Jin Yoon, IEEE TPAMI. 2Lin Wang and Kuk-Jin Yoon. Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks. IEEE TPAMI, 2021. 2\n\nDeep multimodal fusion by channel exchanging. Yikai Wang, Wenbing Huang, Fuchun Sun, Tingyang Xu, Yu Rong, Junzhou Huang, NeurIPS. 2Yikai Wang, Wenbing Huang, Fuchun Sun, Tingyang Xu, Yu Rong, and Junzhou Huang. Deep multimodal fusion by channel exchanging. NeurIPS, 2020. 2\n\nTheoretical analysis of self-training with deep networks on unlabeled data. Colin Wei, Kendrick Shen, Yining Chen, Tengyu Ma, arXiv:2010.03622arXiv preprintColin Wei, Kendrick Shen, Yining Chen, and Tengyu Ma. Theoretical analysis of self-training with deep networks on unlabeled data. arXiv preprint arXiv:2010.03622, 2020. 5\n\nUnsupervised data augmentation for consistency training. Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, Quoc Le, NeurIPS. 334Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. Unsupervised data augmentation for consistency training. NeurIPS, 33, 2020. 2, 4\n\nSelf-training with noisy student improves imagenet classification. Qizhe Xie, Minh-Thang Luong, Eduard Hovy, Quoc V Le, CVPR. 7Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student improves imagenet clas- sification. In CVPR, pages 10687-10698, 2020. 2, 6, 7, 8\n\nBillion-scale semi-supervised learning for image classification. Herv\u00e9 I Zeki Yalniz, Kan J\u00e9gou, Manohar Chen, Dhruv Paluri, Mahajan, arXiv:1905.00546arXiv preprintI Zeki Yalniz, Herv\u00e9 J\u00e9gou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. Billion-scale semi-supervised learning for image classification. arXiv preprint arXiv:1905.00546, 2019. 2\n\nTraining deep neural networks in generations: A more tolerant teacher educates better students. Chenglin Yang, Lingxi Xie, Siyuan Qiao, Alan L Yuille, AAAI. 33Chenglin Yang, Lingxi Xie, Siyuan Qiao, and Alan L Yuille. Training deep neural networks in generations: A more tol- erant teacher educates better students. In AAAI, volume 33, pages 5628-5635, 2019. 2\n\n. Dong Yu, Li Deng, Automatic, Recogni-Tion, Springer, Dong Yu and Li Deng. AUTOMATIC SPEECH RECOGNI- TION. Springer, 2016. 1\n\nSoujanya Poria, Erik Cambria, and Louis-Philippe Morency. Tensor fusion network for multimodal sentiment analysis. Amir Zadeh, Minghai Chen, EMNLP. 2Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Tensor fusion network for mul- timodal sentiment analysis. EMNLP, 2017. 2\n\nKnowledge as priors: Cross-modal knowledge generalization for datasets without superior knowledge. Long Zhao, CVPR. Long Zhao et al. Knowledge as priors: Cross-modal knowl- edge generalization for datasets without superior knowledge. In CVPR, 2020. 2, 7, 8\n\nThrough-wall human pose estimation using radio signals. Mingmin Zhao, Tianhong Li, Mohammad Abu Alsheikh, Yonglong Tian, Hang Zhao, Antonio Torralba, Dina Katabi, CVPR. Mingmin Zhao, Tianhong Li, Mohammad Abu Alsheikh, Yonglong Tian, Hang Zhao, Antonio Torralba, and Dina Katabi. Through-wall human pose estimation using radio signals. In CVPR, pages 7356-7365, 2018. 2\n\nRethinking pretraining and self-training. Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanxiao Liu, NeurIPS. 332Barret Zoph, Golnaz Ghiasi, Tsung-Yi Lin, Yin Cui, Hanx- iao Liu, Ekin Dogus Cubuk, and Quoc Le. Rethinking pre- training and self-training. NeurIPS, 33, 2020. 2\n\nConfidence regularized self-training. Yang Zou, Zhiding Yu, Xiaofeng Liu, Jinsong Kumar, Wang, ICCV. Yang Zou, Zhiding Yu, Xiaofeng Liu, BVK Kumar, and Jin- song Wang. Confidence regularized self-training. In ICCV, pages 5982-5991, 2019. 2\n", "annotations": {"author": "[{\"end\":44,\"start\":34},{\"end\":96,\"start\":45},{\"end\":115,\"start\":97},{\"end\":148,\"start\":116},{\"end\":161,\"start\":149},{\"end\":176,\"start\":162},{\"end\":188,\"start\":177}]", "publisher": null, "author_last_name": "[{\"end\":43,\"start\":40},{\"end\":56,\"start\":53},{\"end\":108,\"start\":105},{\"end\":125,\"start\":121},{\"end\":160,\"start\":158},{\"end\":175,\"start\":166},{\"end\":187,\"start\":181}]", "author_first_name": "[{\"end\":39,\"start\":34},{\"end\":52,\"start\":45},{\"end\":104,\"start\":97},{\"end\":120,\"start\":116},{\"end\":157,\"start\":149},{\"end\":165,\"start\":162},{\"end\":178,\"start\":177},{\"end\":180,\"start\":179}]", "author_affiliation": "[{\"end\":95,\"start\":58},{\"end\":114,\"start\":110},{\"end\":147,\"start\":127}]", "title": "[{\"end\":31,\"start\":1},{\"end\":219,\"start\":189}]", "venue": null, "abstract": "[{\"end\":1332,\"start\":221}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1463,\"start\":1459},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":1466,\"start\":1463},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":1469,\"start\":1466},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":1496,\"start\":1492},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":1499,\"start\":1496},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4665,\"start\":4662},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4720,\"start\":4716},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":4723,\"start\":4720},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4745,\"start\":4741},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4765,\"start\":4761},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5309,\"start\":5305},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":5312,\"start\":5309},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":5315,\"start\":5312},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5318,\"start\":5315},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":5346,\"start\":5342},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5349,\"start\":5346},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":5376,\"start\":5372},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5379,\"start\":5376},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5449,\"start\":5446},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":5605,\"start\":5601},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5607,\"start\":5605},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":5610,\"start\":5607},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5613,\"start\":5610},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6018,\"start\":6014},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6042,\"start\":6038},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6045,\"start\":6042},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6058,\"start\":6055},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":6061,\"start\":6058},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6095,\"start\":6091},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6113,\"start\":6110},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6115,\"start\":6113},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6118,\"start\":6115},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6276,\"start\":6272},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6491,\"start\":6487},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6494,\"start\":6491},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":6497,\"start\":6494},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6500,\"start\":6497},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6503,\"start\":6500},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6735,\"start\":6731},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6738,\"start\":6735},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6740,\"start\":6738},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6743,\"start\":6740},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6745,\"start\":6743},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":6748,\"start\":6745},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":6751,\"start\":6748},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6770,\"start\":6766},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6899,\"start\":6896},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":7017,\"start\":7013},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":7332,\"start\":7328},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7335,\"start\":7332},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7361,\"start\":7357},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7364,\"start\":7361},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7392,\"start\":7388},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7395,\"start\":7392},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7398,\"start\":7395},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7401,\"start\":7398},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7430,\"start\":7427},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7452,\"start\":7448},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9194,\"start\":9191},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9298,\"start\":9295},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10248,\"start\":10245},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10381,\"start\":10378},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13627,\"start\":13623},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13853,\"start\":13850},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14283,\"start\":14279},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":14286,\"start\":14283},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":14289,\"start\":14286},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":18097,\"start\":18093},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":19398,\"start\":19394},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":19762,\"start\":19758},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19917,\"start\":19913},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19919,\"start\":19917},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":19922,\"start\":19919},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":20356,\"start\":20352},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":20424,\"start\":20420},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21521,\"start\":21517},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":21579,\"start\":21575},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":21635,\"start\":21631},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21652,\"start\":21649},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":22420,\"start\":22416},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":22872,\"start\":22868},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":25259,\"start\":25255},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":26805,\"start\":26801},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":27282,\"start\":27278},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":27313,\"start\":27309},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":27345,\"start\":27341},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":27370,\"start\":27366},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":27381,\"start\":27377},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":27667,\"start\":27663},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27700,\"start\":27696},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":27879,\"start\":27875},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":29606,\"start\":29602},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":29651,\"start\":29648},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":30246,\"start\":30243},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":30663,\"start\":30659},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":30705,\"start\":30701},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":30710,\"start\":30706},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":30975,\"start\":30971},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":31005,\"start\":31001},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":31028,\"start\":31024},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":33388,\"start\":33384},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":33393,\"start\":33389},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":33426,\"start\":33422}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":32098,\"start\":31753},{\"attributes\":{\"id\":\"fig_1\"},\"end\":32129,\"start\":32099},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32379,\"start\":32130},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32614,\"start\":32380},{\"attributes\":{\"id\":\"fig_4\"},\"end\":32686,\"start\":32615},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":32812,\"start\":32687},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":32959,\"start\":32813},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":33214,\"start\":32960},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":33427,\"start\":33215}]", "paragraph": "[{\"end\":2262,\"start\":1348},{\"end\":2720,\"start\":2264},{\"end\":4021,\"start\":2722},{\"end\":5104,\"start\":4023},{\"end\":6215,\"start\":5148},{\"end\":7150,\"start\":6244},{\"end\":7565,\"start\":7174},{\"end\":8012,\"start\":7567},{\"end\":8227,\"start\":8058},{\"end\":8460,\"start\":8268},{\"end\":8658,\"start\":8506},{\"end\":9195,\"start\":8660},{\"end\":10156,\"start\":9197},{\"end\":10311,\"start\":10211},{\"end\":10860,\"start\":10313},{\"end\":11338,\"start\":11292},{\"end\":11698,\"start\":11525},{\"end\":11824,\"start\":11700},{\"end\":12543,\"start\":11826},{\"end\":12700,\"start\":12545},{\"end\":13103,\"start\":12702},{\"end\":13364,\"start\":13132},{\"end\":13740,\"start\":13366},{\"end\":14194,\"start\":13816},{\"end\":14369,\"start\":14196},{\"end\":15241,\"start\":14451},{\"end\":15518,\"start\":15243},{\"end\":16138,\"start\":15670},{\"end\":17239,\"start\":16140},{\"end\":17972,\"start\":17241},{\"end\":18185,\"start\":17997},{\"end\":18361,\"start\":18187},{\"end\":18655,\"start\":18501},{\"end\":18982,\"start\":18706},{\"end\":19175,\"start\":19087},{\"end\":19333,\"start\":19177},{\"end\":19495,\"start\":19335},{\"end\":19923,\"start\":19671},{\"end\":20003,\"start\":19925},{\"end\":20156,\"start\":20089},{\"end\":20823,\"start\":20158},{\"end\":21274,\"start\":20963},{\"end\":22069,\"start\":21299},{\"end\":22186,\"start\":22071},{\"end\":22260,\"start\":22188},{\"end\":22398,\"start\":22262},{\"end\":22534,\"start\":22400},{\"end\":22671,\"start\":22536},{\"end\":22841,\"start\":22673},{\"end\":23255,\"start\":22843},{\"end\":23469,\"start\":23257},{\"end\":23821,\"start\":23492},{\"end\":24499,\"start\":23823},{\"end\":25174,\"start\":24511},{\"end\":25462,\"start\":25228},{\"end\":26694,\"start\":25464},{\"end\":26736,\"start\":26729},{\"end\":27195,\"start\":26762},{\"end\":28126,\"start\":27206},{\"end\":28868,\"start\":28128},{\"end\":29400,\"start\":28870},{\"end\":29468,\"start\":29402},{\"end\":29814,\"start\":29493},{\"end\":30379,\"start\":29816},{\"end\":30874,\"start\":30381},{\"end\":31239,\"start\":30885},{\"end\":31752,\"start\":31254}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8267,\"start\":8228},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8505,\"start\":8461},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10210,\"start\":10157},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11291,\"start\":10861},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11524,\"start\":11339},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13815,\"start\":13741},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14450,\"start\":14370},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15588,\"start\":15519},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15669,\"start\":15588},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18500,\"start\":18362},{\"attributes\":{\"id\":\"formula_10\"},\"end\":18705,\"start\":18656},{\"attributes\":{\"id\":\"formula_11\"},\"end\":19086,\"start\":18983},{\"attributes\":{\"id\":\"formula_12\"},\"end\":19670,\"start\":19496},{\"attributes\":{\"id\":\"formula_13\"},\"end\":20088,\"start\":20004},{\"attributes\":{\"id\":\"formula_14\"},\"end\":20896,\"start\":20824},{\"attributes\":{\"id\":\"formula_15\"},\"end\":20962,\"start\":20896},{\"attributes\":{\"id\":\"formula_16\"},\"end\":25205,\"start\":25175}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":24563,\"start\":24556},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":25903,\"start\":25896},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":28144,\"start\":28137},{\"end\":30397,\"start\":30390},{\"end\":31143,\"start\":31136}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1346,\"start\":1334},{\"attributes\":{\"n\":\"2.\"},\"end\":5119,\"start\":5107},{\"attributes\":{\"n\":\"2.1.\"},\"end\":5146,\"start\":5122},{\"attributes\":{\"n\":\"2.2.\"},\"end\":6242,\"start\":6218},{\"attributes\":{\"n\":\"2.3.\"},\"end\":7172,\"start\":7153},{\"attributes\":{\"n\":\"3.\"},\"end\":8023,\"start\":8015},{\"attributes\":{\"n\":\"3.1.\"},\"end\":8056,\"start\":8026},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13130,\"start\":13106},{\"attributes\":{\"n\":\"3.3.\"},\"end\":17995,\"start\":17975},{\"attributes\":{\"n\":\"4.\"},\"end\":21297,\"start\":21277},{\"attributes\":{\"n\":\"4.1.\"},\"end\":23490,\"start\":23472},{\"end\":24509,\"start\":24502},{\"attributes\":{\"n\":\"4.2.\"},\"end\":25226,\"start\":25207},{\"end\":26717,\"start\":26697},{\"end\":26727,\"start\":26720},{\"attributes\":{\"n\":\"4.3.\"},\"end\":26760,\"start\":26739},{\"end\":27204,\"start\":27198},{\"attributes\":{\"n\":\"4.4.\"},\"end\":29491,\"start\":29471},{\"end\":30883,\"start\":30877},{\"attributes\":{\"n\":\"5.\"},\"end\":31252,\"start\":31242},{\"end\":31764,\"start\":31754},{\"end\":32110,\"start\":32100},{\"end\":32141,\"start\":32131},{\"end\":32391,\"start\":32381},{\"end\":32626,\"start\":32616},{\"end\":32697,\"start\":32688},{\"end\":32823,\"start\":32814},{\"end\":32970,\"start\":32961},{\"end\":33225,\"start\":33216}]", "table": "[{\"end\":32959,\"start\":32825}]", "figure_caption": "[{\"end\":32098,\"start\":31766},{\"end\":32129,\"start\":32112},{\"end\":32379,\"start\":32143},{\"end\":32614,\"start\":32393},{\"end\":32686,\"start\":32628},{\"end\":32812,\"start\":32699},{\"end\":33214,\"start\":32972},{\"end\":33427,\"start\":33227}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":1746,\"start\":1738},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":3336,\"start\":3328},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10408,\"start\":10399},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11955,\"start\":11946},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12053,\"start\":12044},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12207,\"start\":12198},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14193,\"start\":14184},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14756,\"start\":14747},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14948,\"start\":14939},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15882,\"start\":15873},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16411,\"start\":16403},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17435,\"start\":17427},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28903,\"start\":28895}]", "bib_author_first_name": "[{\"end\":33494,\"start\":33489},{\"end\":33511,\"start\":33506},{\"end\":33523,\"start\":33516},{\"end\":33532,\"start\":33528},{\"end\":33553,\"start\":33544},{\"end\":33775,\"start\":33770},{\"end\":33796,\"start\":33790},{\"end\":33988,\"start\":33984},{\"end\":34001,\"start\":33996},{\"end\":34014,\"start\":34010},{\"end\":34024,\"start\":34023},{\"end\":34036,\"start\":34031},{\"end\":34333,\"start\":34328},{\"end\":34345,\"start\":34341},{\"end\":34363,\"start\":34356},{\"end\":34555,\"start\":34549},{\"end\":34570,\"start\":34565},{\"end\":34586,\"start\":34581},{\"end\":35074,\"start\":35069},{\"end\":35094,\"start\":35086},{\"end\":35105,\"start\":35104},{\"end\":35116,\"start\":35112},{\"end\":35130,\"start\":35124},{\"end\":35143,\"start\":35140},{\"end\":35155,\"start\":35150},{\"end\":35459,\"start\":35454},{\"end\":35479,\"start\":35471},{\"end\":35492,\"start\":35489},{\"end\":35512,\"start\":35505},{\"end\":35529,\"start\":35523},{\"end\":35543,\"start\":35538},{\"end\":35845,\"start\":35840},{\"end\":35855,\"start\":35852},{\"end\":36254,\"start\":36247},{\"end\":36266,\"start\":36261},{\"end\":36278,\"start\":36272},{\"end\":36294,\"start\":36288},{\"end\":36585,\"start\":36574},{\"end\":36599,\"start\":36592},{\"end\":36607,\"start\":36600},{\"end\":36620,\"start\":36615},{\"end\":36629,\"start\":36628},{\"end\":36649,\"start\":36648},{\"end\":36662,\"start\":36656},{\"end\":36677,\"start\":36670},{\"end\":36692,\"start\":36684},{\"end\":37060,\"start\":37049},{\"end\":37073,\"start\":37067},{\"end\":37093,\"start\":37086},{\"end\":37110,\"start\":37103},{\"end\":37453,\"start\":37445},{\"end\":37468,\"start\":37460},{\"end\":37480,\"start\":37474},{\"end\":37492,\"start\":37487},{\"end\":37501,\"start\":37497},{\"end\":37517,\"start\":37508},{\"end\":37526,\"start\":37522},{\"end\":37893,\"start\":37891},{\"end\":37909,\"start\":37900},{\"end\":37929,\"start\":37925},{\"end\":37946,\"start\":37941},{\"end\":37965,\"start\":37957},{\"end\":37981,\"start\":37975},{\"end\":37994,\"start\":37988},{\"end\":38010,\"start\":38005},{\"end\":38487,\"start\":38486},{\"end\":38489,\"start\":38488},{\"end\":38503,\"start\":38498},{\"end\":38515,\"start\":38511},{\"end\":38530,\"start\":38526},{\"end\":38547,\"start\":38539},{\"end\":38563,\"start\":38558},{\"end\":38577,\"start\":38571},{\"end\":38905,\"start\":38898},{\"end\":39067,\"start\":39060},{\"end\":39079,\"start\":39072},{\"end\":39095,\"start\":39087},{\"end\":39105,\"start\":39101},{\"end\":39317,\"start\":39312},{\"end\":39334,\"start\":39327},{\"end\":39347,\"start\":39346},{\"end\":39349,\"start\":39348},{\"end\":39366,\"start\":39365},{\"end\":39377,\"start\":39373},{\"end\":39395,\"start\":39387},{\"end\":39409,\"start\":39404},{\"end\":39422,\"start\":39417},{\"end\":39439,\"start\":39438},{\"end\":39450,\"start\":39445},{\"end\":39750,\"start\":39742},{\"end\":39764,\"start\":39759},{\"end\":39778,\"start\":39774},{\"end\":40046,\"start\":40042},{\"end\":40063,\"start\":40056},{\"end\":40075,\"start\":40071},{\"end\":40089,\"start\":40083},{\"end\":40108,\"start\":40102},{\"end\":40355,\"start\":40353},{\"end\":40373,\"start\":40363},{\"end\":40383,\"start\":40378},{\"end\":40396,\"start\":40389},{\"end\":40407,\"start\":40403},{\"end\":40420,\"start\":40414},{\"end\":40712,\"start\":40708},{\"end\":40729,\"start\":40725},{\"end\":40749,\"start\":40741},{\"end\":40751,\"start\":40750},{\"end\":40992,\"start\":40986},{\"end\":41209,\"start\":41200},{\"end\":41553,\"start\":41552},{\"end\":41875,\"start\":41874},{\"end\":41898,\"start\":41897},{\"end\":42279,\"start\":42272},{\"end\":42299,\"start\":42290},{\"end\":42692,\"start\":42686},{\"end\":42709,\"start\":42701},{\"end\":42731,\"start\":42727},{\"end\":43030,\"start\":43024},{\"end\":43044,\"start\":43038},{\"end\":43053,\"start\":43049},{\"end\":43055,\"start\":43054},{\"end\":43068,\"start\":43067},{\"end\":43085,\"start\":43078},{\"end\":43294,\"start\":43290},{\"end\":43306,\"start\":43301},{\"end\":43318,\"start\":43312},{\"end\":43330,\"start\":43324},{\"end\":43568,\"start\":43562},{\"end\":43602,\"start\":43592},{\"end\":43810,\"start\":43803},{\"end\":43826,\"start\":43819},{\"end\":43841,\"start\":43835},{\"end\":43850,\"start\":43842},{\"end\":43865,\"start\":43858},{\"end\":43881,\"start\":43876},{\"end\":43895,\"start\":43889},{\"end\":44215,\"start\":44209},{\"end\":44232,\"start\":44227},{\"end\":44248,\"start\":44240},{\"end\":44259,\"start\":44256},{\"end\":44526,\"start\":44521},{\"end\":44543,\"start\":44537},{\"end\":44817,\"start\":44811},{\"end\":44829,\"start\":44824},{\"end\":44851,\"start\":44841},{\"end\":44862,\"start\":44856},{\"end\":44878,\"start\":44870},{\"end\":44889,\"start\":44888},{\"end\":44900,\"start\":44896},{\"end\":44911,\"start\":44908},{\"end\":44926,\"start\":44921},{\"end\":45325,\"start\":45319},{\"end\":45336,\"start\":45331},{\"end\":45345,\"start\":45341},{\"end\":45357,\"start\":45351},{\"end\":45372,\"start\":45364},{\"end\":45386,\"start\":45377},{\"end\":45400,\"start\":45394},{\"end\":45751,\"start\":45746},{\"end\":45768,\"start\":45763},{\"end\":46019,\"start\":46010},{\"end\":46030,\"start\":46026},{\"end\":46235,\"start\":46225},{\"end\":46252,\"start\":46246},{\"end\":46266,\"start\":46265},{\"end\":46287,\"start\":46286},{\"end\":46303,\"start\":46295},{\"end\":46724,\"start\":46717},{\"end\":46738,\"start\":46733},{\"end\":46753,\"start\":46746},{\"end\":47021,\"start\":47018},{\"end\":47035,\"start\":47028},{\"end\":47258,\"start\":47253},{\"end\":47272,\"start\":47265},{\"end\":47286,\"start\":47280},{\"end\":47300,\"start\":47292},{\"end\":47307,\"start\":47305},{\"end\":47321,\"start\":47314},{\"end\":47564,\"start\":47559},{\"end\":47578,\"start\":47570},{\"end\":47591,\"start\":47585},{\"end\":47604,\"start\":47598},{\"end\":47873,\"start\":47868},{\"end\":47885,\"start\":47879},{\"end\":47897,\"start\":47891},{\"end\":47909,\"start\":47904},{\"end\":47921,\"start\":47917},{\"end\":48154,\"start\":48149},{\"end\":48170,\"start\":48160},{\"end\":48184,\"start\":48178},{\"end\":48197,\"start\":48191},{\"end\":48451,\"start\":48446},{\"end\":48470,\"start\":48467},{\"end\":48485,\"start\":48478},{\"end\":48497,\"start\":48492},{\"end\":48829,\"start\":48821},{\"end\":48842,\"start\":48836},{\"end\":48854,\"start\":48848},{\"end\":48865,\"start\":48861},{\"end\":48867,\"start\":48866},{\"end\":49093,\"start\":49089},{\"end\":49100,\"start\":49098},{\"end\":49333,\"start\":49329},{\"end\":49348,\"start\":49341},{\"end\":49625,\"start\":49621},{\"end\":49843,\"start\":49836},{\"end\":49858,\"start\":49850},{\"end\":49871,\"start\":49863},{\"end\":49894,\"start\":49886},{\"end\":49905,\"start\":49901},{\"end\":49919,\"start\":49912},{\"end\":49934,\"start\":49930},{\"end\":50199,\"start\":50193},{\"end\":50212,\"start\":50206},{\"end\":50229,\"start\":50221},{\"end\":50238,\"start\":50235},{\"end\":50251,\"start\":50244},{\"end\":50474,\"start\":50470},{\"end\":50487,\"start\":50480},{\"end\":50500,\"start\":50492},{\"end\":50513,\"start\":50506}]", "bib_author_last_name": "[{\"end\":33504,\"start\":33495},{\"end\":33514,\"start\":33512},{\"end\":33526,\"start\":33524},{\"end\":33542,\"start\":33533},{\"end\":33560,\"start\":33554},{\"end\":33788,\"start\":33776},{\"end\":33806,\"start\":33797},{\"end\":33994,\"start\":33989},{\"end\":34008,\"start\":34002},{\"end\":34021,\"start\":34015},{\"end\":34029,\"start\":34025},{\"end\":34050,\"start\":34037},{\"end\":34062,\"start\":34052},{\"end\":34339,\"start\":34334},{\"end\":34354,\"start\":34346},{\"end\":34372,\"start\":34364},{\"end\":34563,\"start\":34556},{\"end\":34579,\"start\":34571},{\"end\":34593,\"start\":34587},{\"end\":35084,\"start\":35075},{\"end\":35102,\"start\":35095},{\"end\":35110,\"start\":35106},{\"end\":35122,\"start\":35117},{\"end\":35138,\"start\":35131},{\"end\":35148,\"start\":35144},{\"end\":35161,\"start\":35156},{\"end\":35169,\"start\":35163},{\"end\":35469,\"start\":35460},{\"end\":35487,\"start\":35480},{\"end\":35503,\"start\":35493},{\"end\":35521,\"start\":35513},{\"end\":35536,\"start\":35530},{\"end\":35550,\"start\":35544},{\"end\":35850,\"start\":35846},{\"end\":35864,\"start\":35856},{\"end\":36259,\"start\":36255},{\"end\":36270,\"start\":36267},{\"end\":36286,\"start\":36279},{\"end\":36304,\"start\":36295},{\"end\":36590,\"start\":36586},{\"end\":36613,\"start\":36608},{\"end\":36626,\"start\":36621},{\"end\":36637,\"start\":36630},{\"end\":36646,\"start\":36639},{\"end\":36654,\"start\":36650},{\"end\":36668,\"start\":36663},{\"end\":36682,\"start\":36678},{\"end\":36697,\"start\":36693},{\"end\":36705,\"start\":36699},{\"end\":37065,\"start\":37061},{\"end\":37084,\"start\":37074},{\"end\":37101,\"start\":37094},{\"end\":37115,\"start\":37111},{\"end\":37458,\"start\":37454},{\"end\":37472,\"start\":37469},{\"end\":37485,\"start\":37481},{\"end\":37495,\"start\":37493},{\"end\":37506,\"start\":37502},{\"end\":37520,\"start\":37518},{\"end\":37531,\"start\":37527},{\"end\":37898,\"start\":37894},{\"end\":37923,\"start\":37910},{\"end\":37939,\"start\":37930},{\"end\":37955,\"start\":37947},{\"end\":37973,\"start\":37966},{\"end\":37986,\"start\":37982},{\"end\":38003,\"start\":37995},{\"end\":38020,\"start\":38011},{\"end\":38484,\"start\":38470},{\"end\":38496,\"start\":38490},{\"end\":38509,\"start\":38504},{\"end\":38524,\"start\":38516},{\"end\":38537,\"start\":38531},{\"end\":38556,\"start\":38548},{\"end\":38569,\"start\":38564},{\"end\":38584,\"start\":38578},{\"end\":38592,\"start\":38586},{\"end\":38911,\"start\":38906},{\"end\":39070,\"start\":39068},{\"end\":39085,\"start\":39080},{\"end\":39099,\"start\":39096},{\"end\":39109,\"start\":39106},{\"end\":39325,\"start\":39318},{\"end\":39344,\"start\":39335},{\"end\":39356,\"start\":39350},{\"end\":39363,\"start\":39358},{\"end\":39371,\"start\":39367},{\"end\":39385,\"start\":39378},{\"end\":39402,\"start\":39396},{\"end\":39415,\"start\":39410},{\"end\":39429,\"start\":39423},{\"end\":39436,\"start\":39431},{\"end\":39443,\"start\":39440},{\"end\":39458,\"start\":39451},{\"end\":39467,\"start\":39460},{\"end\":39757,\"start\":39751},{\"end\":39772,\"start\":39765},{\"end\":39783,\"start\":39779},{\"end\":40054,\"start\":40047},{\"end\":40069,\"start\":40064},{\"end\":40081,\"start\":40076},{\"end\":40100,\"start\":40090},{\"end\":40116,\"start\":40109},{\"end\":40361,\"start\":40356},{\"end\":40376,\"start\":40374},{\"end\":40387,\"start\":40384},{\"end\":40401,\"start\":40397},{\"end\":40412,\"start\":40408},{\"end\":40426,\"start\":40421},{\"end\":40723,\"start\":40713},{\"end\":40739,\"start\":40730},{\"end\":40758,\"start\":40752},{\"end\":40998,\"start\":40993},{\"end\":41213,\"start\":41210},{\"end\":41559,\"start\":41554},{\"end\":41566,\"start\":41561},{\"end\":41882,\"start\":41876},{\"end\":41895,\"start\":41884},{\"end\":41904,\"start\":41899},{\"end\":41911,\"start\":41906},{\"end\":42288,\"start\":42280},{\"end\":42308,\"start\":42300},{\"end\":42699,\"start\":42693},{\"end\":42725,\"start\":42710},{\"end\":42738,\"start\":42732},{\"end\":42745,\"start\":42740},{\"end\":43036,\"start\":43031},{\"end\":43047,\"start\":43045},{\"end\":43065,\"start\":43056},{\"end\":43076,\"start\":43069},{\"end\":43093,\"start\":43086},{\"end\":43103,\"start\":43095},{\"end\":43299,\"start\":43295},{\"end\":43310,\"start\":43307},{\"end\":43322,\"start\":43319},{\"end\":43333,\"start\":43331},{\"end\":43590,\"start\":43569},{\"end\":43614,\"start\":43603},{\"end\":43628,\"start\":43616},{\"end\":43817,\"start\":43811},{\"end\":43833,\"start\":43827},{\"end\":43856,\"start\":43851},{\"end\":43874,\"start\":43866},{\"end\":43887,\"start\":43882},{\"end\":43902,\"start\":43896},{\"end\":44225,\"start\":44216},{\"end\":44238,\"start\":44233},{\"end\":44254,\"start\":44249},{\"end\":44266,\"start\":44260},{\"end\":44535,\"start\":44527},{\"end\":44553,\"start\":44544},{\"end\":44822,\"start\":44818},{\"end\":44839,\"start\":44830},{\"end\":44854,\"start\":44852},{\"end\":44868,\"start\":44863},{\"end\":44886,\"start\":44879},{\"end\":44894,\"start\":44890},{\"end\":44906,\"start\":44901},{\"end\":44919,\"start\":44912},{\"end\":44932,\"start\":44927},{\"end\":44940,\"start\":44934},{\"end\":45329,\"start\":45326},{\"end\":45339,\"start\":45337},{\"end\":45349,\"start\":45346},{\"end\":45362,\"start\":45358},{\"end\":45375,\"start\":45373},{\"end\":45392,\"start\":45387},{\"end\":45405,\"start\":45401},{\"end\":45761,\"start\":45752},{\"end\":45776,\"start\":45769},{\"end\":46024,\"start\":46020},{\"end\":46035,\"start\":46031},{\"end\":46244,\"start\":46236},{\"end\":46263,\"start\":46253},{\"end\":46274,\"start\":46267},{\"end\":46284,\"start\":46276},{\"end\":46293,\"start\":46288},{\"end\":46312,\"start\":46304},{\"end\":46323,\"start\":46314},{\"end\":46731,\"start\":46725},{\"end\":46744,\"start\":46739},{\"end\":46761,\"start\":46754},{\"end\":47026,\"start\":47022},{\"end\":47040,\"start\":47036},{\"end\":47263,\"start\":47259},{\"end\":47278,\"start\":47273},{\"end\":47290,\"start\":47287},{\"end\":47303,\"start\":47301},{\"end\":47312,\"start\":47308},{\"end\":47327,\"start\":47322},{\"end\":47568,\"start\":47565},{\"end\":47583,\"start\":47579},{\"end\":47596,\"start\":47592},{\"end\":47607,\"start\":47605},{\"end\":47877,\"start\":47874},{\"end\":47889,\"start\":47886},{\"end\":47902,\"start\":47898},{\"end\":47915,\"start\":47910},{\"end\":47924,\"start\":47922},{\"end\":48158,\"start\":48155},{\"end\":48176,\"start\":48171},{\"end\":48189,\"start\":48185},{\"end\":48200,\"start\":48198},{\"end\":48465,\"start\":48452},{\"end\":48476,\"start\":48471},{\"end\":48490,\"start\":48486},{\"end\":48504,\"start\":48498},{\"end\":48513,\"start\":48506},{\"end\":48834,\"start\":48830},{\"end\":48846,\"start\":48843},{\"end\":48859,\"start\":48855},{\"end\":48874,\"start\":48868},{\"end\":49096,\"start\":49094},{\"end\":49105,\"start\":49101},{\"end\":49116,\"start\":49107},{\"end\":49130,\"start\":49118},{\"end\":49140,\"start\":49132},{\"end\":49339,\"start\":49334},{\"end\":49353,\"start\":49349},{\"end\":49630,\"start\":49626},{\"end\":49848,\"start\":49844},{\"end\":49861,\"start\":49859},{\"end\":49884,\"start\":49872},{\"end\":49899,\"start\":49895},{\"end\":49910,\"start\":49906},{\"end\":49928,\"start\":49920},{\"end\":49941,\"start\":49935},{\"end\":50204,\"start\":50200},{\"end\":50219,\"start\":50213},{\"end\":50233,\"start\":50230},{\"end\":50242,\"start\":50239},{\"end\":50255,\"start\":50252},{\"end\":50478,\"start\":50475},{\"end\":50490,\"start\":50488},{\"end\":50504,\"start\":50501},{\"end\":50519,\"start\":50514},{\"end\":50525,\"start\":50521}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":215737200},\"end\":33744,\"start\":33429},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":10769575},\"end\":33910,\"start\":33746},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":199501839},\"end\":34263,\"start\":33912},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":2915490},\"end\":34515,\"start\":34265},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":8307266},\"end\":34974,\"start\":34517},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":208247904},\"end\":35393,\"start\":34976},{\"attributes\":{\"doi\":\"arXiv:1905.02249\",\"id\":\"b6\"},\"end\":35783,\"start\":35395},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":207228399},\"end\":36199,\"start\":35785},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":216522760},\"end\":36472,\"start\":36201},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":218763503},\"end\":36984,\"start\":36474},{\"attributes\":{\"doi\":\"arXiv:1706.05587\",\"id\":\"b10\"},\"end\":37324,\"start\":36986},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":220646585},\"end\":37770,\"start\":37326},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":67788034},\"end\":38402,\"start\":37772},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":21519176},\"end\":38845,\"start\":38404},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":6832420},\"end\":39012,\"start\":38847},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":206594692},\"end\":39254,\"start\":39014},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":8810481},\"end\":39740,\"start\":39256},{\"attributes\":{\"doi\":\"arXiv:1503.02531\",\"id\":\"b17\"},\"end\":39996,\"start\":39742},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":13300851},\"end\":40289,\"start\":39998},{\"attributes\":{\"doi\":\"arXiv:2106.04538\",\"id\":\"b19\"},\"end\":40641,\"start\":40291},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":195908774},\"end\":40925,\"start\":40643},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":211506808},\"end\":41101,\"start\":40927},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":18507866},\"end\":41472,\"start\":41103},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":32800624},\"end\":41713,\"start\":41474},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":21704094},\"end\":42166,\"start\":41715},{\"attributes\":{\"id\":\"b25\"},\"end\":42585,\"start\":42168},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":17504174},\"end\":42966,\"start\":42587},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":11614363},\"end\":43286,\"start\":42968},{\"attributes\":{\"doi\":\"arXiv:2003.10580\",\"id\":\"b28\"},\"end\":43494,\"start\":43288},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":6182290},\"end\":43801,\"start\":43496},{\"attributes\":{\"doi\":\"arXiv:1412.6550\",\"id\":\"b30\"},\"end\":44147,\"start\":43803},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":545361},\"end\":44451,\"start\":44149},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b32\"},\"end\":44730,\"start\":44453},{\"attributes\":{\"doi\":\"arXiv:2001.07685\",\"id\":\"b33\"},\"end\":45232,\"start\":44732},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":220514474},\"end\":45623,\"start\":45234},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":2759724},\"end\":45962,\"start\":45625},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":198179476},\"end\":46154,\"start\":45964},{\"attributes\":{\"id\":\"b37\"},\"end\":46639,\"start\":46156},{\"attributes\":{\"id\":\"b38\"},\"end\":46912,\"start\":46641},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":215745611},\"end\":47205,\"start\":46914},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":226289841},\"end\":47481,\"start\":47207},{\"attributes\":{\"doi\":\"arXiv:2010.03622\",\"id\":\"b41\"},\"end\":47809,\"start\":47483},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":195873898},\"end\":48080,\"start\":47811},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":207853355},\"end\":48379,\"start\":48082},{\"attributes\":{\"doi\":\"arXiv:1905.00546\",\"id\":\"b44\"},\"end\":48723,\"start\":48381},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":54986302},\"end\":49085,\"start\":48725},{\"attributes\":{\"id\":\"b46\"},\"end\":49212,\"start\":49087},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":950292},\"end\":49520,\"start\":49214},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":214743049},\"end\":49778,\"start\":49522},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":47020925},\"end\":50149,\"start\":49780},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":219635973},\"end\":50430,\"start\":50151},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":201646241},\"end\":50671,\"start\":50432}]", "bib_title": "[{\"end\":33487,\"start\":33429},{\"end\":33768,\"start\":33746},{\"end\":33982,\"start\":33912},{\"end\":34326,\"start\":34265},{\"end\":34547,\"start\":34517},{\"end\":35067,\"start\":34976},{\"end\":35838,\"start\":35785},{\"end\":36245,\"start\":36201},{\"end\":36572,\"start\":36474},{\"end\":37443,\"start\":37326},{\"end\":37889,\"start\":37772},{\"end\":38468,\"start\":38404},{\"end\":38896,\"start\":38847},{\"end\":39058,\"start\":39014},{\"end\":39310,\"start\":39256},{\"end\":40040,\"start\":39998},{\"end\":40706,\"start\":40643},{\"end\":40984,\"start\":40927},{\"end\":41198,\"start\":41103},{\"end\":41550,\"start\":41474},{\"end\":41872,\"start\":41715},{\"end\":42684,\"start\":42587},{\"end\":43022,\"start\":42968},{\"end\":43560,\"start\":43496},{\"end\":44207,\"start\":44149},{\"end\":45317,\"start\":45234},{\"end\":45744,\"start\":45625},{\"end\":46008,\"start\":45964},{\"end\":46223,\"start\":46156},{\"end\":47016,\"start\":46914},{\"end\":47251,\"start\":47207},{\"end\":47866,\"start\":47811},{\"end\":48147,\"start\":48082},{\"end\":48819,\"start\":48725},{\"end\":49327,\"start\":49214},{\"end\":49619,\"start\":49522},{\"end\":49834,\"start\":49780},{\"end\":50191,\"start\":50151},{\"end\":50468,\"start\":50432}]", "bib_author": "[{\"end\":33506,\"start\":33489},{\"end\":33516,\"start\":33506},{\"end\":33528,\"start\":33516},{\"end\":33544,\"start\":33528},{\"end\":33562,\"start\":33544},{\"end\":33790,\"start\":33770},{\"end\":33808,\"start\":33790},{\"end\":33996,\"start\":33984},{\"end\":34010,\"start\":33996},{\"end\":34023,\"start\":34010},{\"end\":34031,\"start\":34023},{\"end\":34052,\"start\":34031},{\"end\":34064,\"start\":34052},{\"end\":34341,\"start\":34328},{\"end\":34356,\"start\":34341},{\"end\":34374,\"start\":34356},{\"end\":34565,\"start\":34549},{\"end\":34581,\"start\":34565},{\"end\":34595,\"start\":34581},{\"end\":35086,\"start\":35069},{\"end\":35104,\"start\":35086},{\"end\":35112,\"start\":35104},{\"end\":35124,\"start\":35112},{\"end\":35140,\"start\":35124},{\"end\":35150,\"start\":35140},{\"end\":35163,\"start\":35150},{\"end\":35171,\"start\":35163},{\"end\":35471,\"start\":35454},{\"end\":35489,\"start\":35471},{\"end\":35505,\"start\":35489},{\"end\":35523,\"start\":35505},{\"end\":35538,\"start\":35523},{\"end\":35552,\"start\":35538},{\"end\":35852,\"start\":35840},{\"end\":35866,\"start\":35852},{\"end\":36261,\"start\":36247},{\"end\":36272,\"start\":36261},{\"end\":36288,\"start\":36272},{\"end\":36306,\"start\":36288},{\"end\":36592,\"start\":36574},{\"end\":36615,\"start\":36592},{\"end\":36628,\"start\":36615},{\"end\":36639,\"start\":36628},{\"end\":36648,\"start\":36639},{\"end\":36656,\"start\":36648},{\"end\":36670,\"start\":36656},{\"end\":36684,\"start\":36670},{\"end\":36699,\"start\":36684},{\"end\":36707,\"start\":36699},{\"end\":37067,\"start\":37049},{\"end\":37086,\"start\":37067},{\"end\":37103,\"start\":37086},{\"end\":37117,\"start\":37103},{\"end\":37460,\"start\":37445},{\"end\":37474,\"start\":37460},{\"end\":37487,\"start\":37474},{\"end\":37497,\"start\":37487},{\"end\":37508,\"start\":37497},{\"end\":37522,\"start\":37508},{\"end\":37533,\"start\":37522},{\"end\":37900,\"start\":37891},{\"end\":37925,\"start\":37900},{\"end\":37941,\"start\":37925},{\"end\":37957,\"start\":37941},{\"end\":37975,\"start\":37957},{\"end\":37988,\"start\":37975},{\"end\":38005,\"start\":37988},{\"end\":38022,\"start\":38005},{\"end\":38486,\"start\":38470},{\"end\":38498,\"start\":38486},{\"end\":38511,\"start\":38498},{\"end\":38526,\"start\":38511},{\"end\":38539,\"start\":38526},{\"end\":38558,\"start\":38539},{\"end\":38571,\"start\":38558},{\"end\":38586,\"start\":38571},{\"end\":38594,\"start\":38586},{\"end\":38913,\"start\":38898},{\"end\":39072,\"start\":39060},{\"end\":39087,\"start\":39072},{\"end\":39101,\"start\":39087},{\"end\":39111,\"start\":39101},{\"end\":39327,\"start\":39312},{\"end\":39346,\"start\":39327},{\"end\":39358,\"start\":39346},{\"end\":39365,\"start\":39358},{\"end\":39373,\"start\":39365},{\"end\":39387,\"start\":39373},{\"end\":39404,\"start\":39387},{\"end\":39417,\"start\":39404},{\"end\":39431,\"start\":39417},{\"end\":39438,\"start\":39431},{\"end\":39445,\"start\":39438},{\"end\":39460,\"start\":39445},{\"end\":39469,\"start\":39460},{\"end\":39759,\"start\":39742},{\"end\":39774,\"start\":39759},{\"end\":39785,\"start\":39774},{\"end\":40056,\"start\":40042},{\"end\":40071,\"start\":40056},{\"end\":40083,\"start\":40071},{\"end\":40102,\"start\":40083},{\"end\":40118,\"start\":40102},{\"end\":40363,\"start\":40353},{\"end\":40378,\"start\":40363},{\"end\":40389,\"start\":40378},{\"end\":40403,\"start\":40389},{\"end\":40414,\"start\":40403},{\"end\":40428,\"start\":40414},{\"end\":40725,\"start\":40708},{\"end\":40741,\"start\":40725},{\"end\":40760,\"start\":40741},{\"end\":41000,\"start\":40986},{\"end\":41215,\"start\":41200},{\"end\":41561,\"start\":41552},{\"end\":41568,\"start\":41561},{\"end\":41884,\"start\":41874},{\"end\":41897,\"start\":41884},{\"end\":41906,\"start\":41897},{\"end\":41913,\"start\":41906},{\"end\":42290,\"start\":42272},{\"end\":42310,\"start\":42290},{\"end\":42701,\"start\":42686},{\"end\":42727,\"start\":42701},{\"end\":42740,\"start\":42727},{\"end\":42747,\"start\":42740},{\"end\":43038,\"start\":43024},{\"end\":43049,\"start\":43038},{\"end\":43067,\"start\":43049},{\"end\":43078,\"start\":43067},{\"end\":43095,\"start\":43078},{\"end\":43105,\"start\":43095},{\"end\":43301,\"start\":43290},{\"end\":43312,\"start\":43301},{\"end\":43324,\"start\":43312},{\"end\":43335,\"start\":43324},{\"end\":43592,\"start\":43562},{\"end\":43616,\"start\":43592},{\"end\":43630,\"start\":43616},{\"end\":43819,\"start\":43803},{\"end\":43835,\"start\":43819},{\"end\":43858,\"start\":43835},{\"end\":43876,\"start\":43858},{\"end\":43889,\"start\":43876},{\"end\":43904,\"start\":43889},{\"end\":44227,\"start\":44209},{\"end\":44240,\"start\":44227},{\"end\":44256,\"start\":44240},{\"end\":44268,\"start\":44256},{\"end\":44537,\"start\":44521},{\"end\":44555,\"start\":44537},{\"end\":44824,\"start\":44811},{\"end\":44841,\"start\":44824},{\"end\":44856,\"start\":44841},{\"end\":44870,\"start\":44856},{\"end\":44888,\"start\":44870},{\"end\":44896,\"start\":44888},{\"end\":44908,\"start\":44896},{\"end\":44921,\"start\":44908},{\"end\":44934,\"start\":44921},{\"end\":44942,\"start\":44934},{\"end\":45331,\"start\":45319},{\"end\":45341,\"start\":45331},{\"end\":45351,\"start\":45341},{\"end\":45364,\"start\":45351},{\"end\":45377,\"start\":45364},{\"end\":45394,\"start\":45377},{\"end\":45407,\"start\":45394},{\"end\":45763,\"start\":45746},{\"end\":45778,\"start\":45763},{\"end\":46026,\"start\":46010},{\"end\":46037,\"start\":46026},{\"end\":46246,\"start\":46225},{\"end\":46265,\"start\":46246},{\"end\":46276,\"start\":46265},{\"end\":46286,\"start\":46276},{\"end\":46295,\"start\":46286},{\"end\":46314,\"start\":46295},{\"end\":46325,\"start\":46314},{\"end\":46733,\"start\":46717},{\"end\":46746,\"start\":46733},{\"end\":46763,\"start\":46746},{\"end\":47028,\"start\":47018},{\"end\":47042,\"start\":47028},{\"end\":47265,\"start\":47253},{\"end\":47280,\"start\":47265},{\"end\":47292,\"start\":47280},{\"end\":47305,\"start\":47292},{\"end\":47314,\"start\":47305},{\"end\":47329,\"start\":47314},{\"end\":47570,\"start\":47559},{\"end\":47585,\"start\":47570},{\"end\":47598,\"start\":47585},{\"end\":47609,\"start\":47598},{\"end\":47879,\"start\":47868},{\"end\":47891,\"start\":47879},{\"end\":47904,\"start\":47891},{\"end\":47917,\"start\":47904},{\"end\":47926,\"start\":47917},{\"end\":48160,\"start\":48149},{\"end\":48178,\"start\":48160},{\"end\":48191,\"start\":48178},{\"end\":48202,\"start\":48191},{\"end\":48467,\"start\":48446},{\"end\":48478,\"start\":48467},{\"end\":48492,\"start\":48478},{\"end\":48506,\"start\":48492},{\"end\":48515,\"start\":48506},{\"end\":48836,\"start\":48821},{\"end\":48848,\"start\":48836},{\"end\":48861,\"start\":48848},{\"end\":48876,\"start\":48861},{\"end\":49098,\"start\":49089},{\"end\":49107,\"start\":49098},{\"end\":49118,\"start\":49107},{\"end\":49132,\"start\":49118},{\"end\":49142,\"start\":49132},{\"end\":49341,\"start\":49329},{\"end\":49355,\"start\":49341},{\"end\":49632,\"start\":49621},{\"end\":49850,\"start\":49836},{\"end\":49863,\"start\":49850},{\"end\":49886,\"start\":49863},{\"end\":49901,\"start\":49886},{\"end\":49912,\"start\":49901},{\"end\":49930,\"start\":49912},{\"end\":49943,\"start\":49930},{\"end\":50206,\"start\":50193},{\"end\":50221,\"start\":50206},{\"end\":50235,\"start\":50221},{\"end\":50244,\"start\":50235},{\"end\":50257,\"start\":50244},{\"end\":50480,\"start\":50470},{\"end\":50492,\"start\":50480},{\"end\":50506,\"start\":50492},{\"end\":50521,\"start\":50506},{\"end\":50527,\"start\":50521}]", "bib_venue": "[{\"end\":34760,\"start\":34686},{\"end\":36009,\"start\":35946},{\"end\":33566,\"start\":33562},{\"end\":33812,\"start\":33808},{\"end\":34069,\"start\":34064},{\"end\":34381,\"start\":34374},{\"end\":34684,\"start\":34595},{\"end\":35175,\"start\":35171},{\"end\":35452,\"start\":35395},{\"end\":35944,\"start\":35866},{\"end\":36312,\"start\":36306},{\"end\":36711,\"start\":36707},{\"end\":37047,\"start\":36986},{\"end\":37537,\"start\":37533},{\"end\":38077,\"start\":38022},{\"end\":38600,\"start\":38594},{\"end\":38917,\"start\":38913},{\"end\":39115,\"start\":39111},{\"end\":39475,\"start\":39469},{\"end\":39845,\"start\":39801},{\"end\":40122,\"start\":40118},{\"end\":40351,\"start\":40291},{\"end\":40767,\"start\":40760},{\"end\":41004,\"start\":41000},{\"end\":41270,\"start\":41215},{\"end\":41572,\"start\":41568},{\"end\":41921,\"start\":41913},{\"end\":42270,\"start\":42168},{\"end\":42757,\"start\":42747},{\"end\":43109,\"start\":43105},{\"end\":43634,\"start\":43630},{\"end\":43952,\"start\":43919},{\"end\":44272,\"start\":44268},{\"end\":44519,\"start\":44453},{\"end\":44809,\"start\":44732},{\"end\":45411,\"start\":45407},{\"end\":45785,\"start\":45778},{\"end\":46041,\"start\":46037},{\"end\":46377,\"start\":46325},{\"end\":46715,\"start\":46641},{\"end\":47052,\"start\":47042},{\"end\":47336,\"start\":47329},{\"end\":47557,\"start\":47483},{\"end\":47933,\"start\":47926},{\"end\":48206,\"start\":48202},{\"end\":48444,\"start\":48381},{\"end\":48880,\"start\":48876},{\"end\":49360,\"start\":49355},{\"end\":49636,\"start\":49632},{\"end\":49947,\"start\":49943},{\"end\":50264,\"start\":50257},{\"end\":50531,\"start\":50527}]"}}}, "year": 2023, "month": 12, "day": 17}