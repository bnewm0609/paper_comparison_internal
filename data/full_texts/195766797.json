{"id": 195766797, "updated": "2023-10-07 01:05:39.247", "metadata": {"title": "Way Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog", "authors": "[{\"first\":\"Natasha\",\"last\":\"Jaques\",\"middle\":[]},{\"first\":\"Asma\",\"last\":\"Ghandeharioun\",\"middle\":[]},{\"first\":\"Judy\",\"last\":\"Shen\",\"middle\":[\"Hanwen\"]},{\"first\":\"Craig\",\"last\":\"Ferguson\",\"middle\":[]},{\"first\":\"Agata\",\"last\":\"Lapedriza\",\"middle\":[]},{\"first\":\"Noah\",\"last\":\"Jones\",\"middle\":[]},{\"first\":\"Shixiang\",\"last\":\"Gu\",\"middle\":[]},{\"first\":\"Rosalind\",\"last\":\"Picard\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2019, "month": 6, "day": 30}, "abstract": "Most deep reinforcement learning (RL) systems are not able to learn effectively from off-policy data, especially if they cannot explore online in the environment. These are critical shortcomings for applying RL to real-world problems where collecting data is expensive, and models must be tested offline before being deployed to interact with the environment -- e.g. systems that learn from human interaction. Thus, we develop a novel class of off-policy batch RL algorithms, which are able to effectively learn offline, without exploring, from a fixed batch of human interaction data. We leverage models pre-trained on data as a strong prior, and use KL-control to penalize divergence from this prior during RL training. We also use dropout-based uncertainty estimates to lower bound the target Q-values as a more efficient alternative to Double Q-Learning. The algorithms are tested on the problem of open-domain dialog generation -- a challenging reinforcement learning problem with a 20,000-dimensional action space. Using our Way Off-Policy algorithm, we can extract multiple different reward functions post-hoc from collected human interaction data, and learn effectively from all of these. We test the real-world generalization of these systems by deploying them live to converse with humans in an open-domain setting, and demonstrate that our algorithm achieves significant improvements over prior methods in off-policy batch RL.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1907.00456", "mag": "3017586084", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-1907-00456", "doi": null}}, "content": {"source": {"pdf_hash": "57daffd65a5d73a439903f3e50950c21c9eba687", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1907.00456v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "c127f7d06f3c5e93f68245beb8744e2d5d1d8f4d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/57daffd65a5d73a439903f3e50950c21c9eba687.txt", "contents": "\nWay Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog\n\n\nNatasha Jaques jaquesn@mit.edu \nDepartment of Media Arts and Science\nMassachusetts Institute of Technology Cambridge\n02139MA\n\nAsma Ghandeharioun \nDepartment of Media Arts and Science\nMassachusetts Institute of Technology Cambridge\n02139MA\n\nJudy Hanwen Shen \nDepartment of Media Arts and Science\nMassachusetts Institute of Technology Cambridge\n02139MA\n\nCraig Ferguson \nDepartment of Media Arts and Science\nMassachusetts Institute of Technology Cambridge\n02139MA\n\nAgata Lapedriza \nDepartment of Media Arts and Science\nMassachusetts Institute of Technology Cambridge\n02139MA\n\nNoah Jones \nDepartment of Media Arts and Science\nMassachusetts Institute of Technology Cambridge\n02139MA\n\nShixiang Gu \nDepartment of Media Arts and Science\nMassachusetts Institute of Technology Cambridge\n02139MA\n\nRosalind Picard \nDepartment of Media Arts and Science\nMassachusetts Institute of Technology Cambridge\n02139MA\n\nWay Off-Policy Batch Deep Reinforcement Learning of Implicit Human Preferences in Dialog\n\nMost deep reinforcement learning (RL) systems are not able to learn effectively from off-policy data, especially if they cannot explore online in the environment. These are critical shortcomings for applying RL to real-world problems where collecting data is expensive, and models must be tested offline before being deployed to interact with the environment -e.g. systems that learn from human interaction. Thus, we develop a novel class of off-policy batch RL algorithms, which are able to effectively learn offline, without exploring, from a fixed batch of human interaction data. We leverage models pre-trained on data as a strong prior, and use KL-control to penalize divergence from this prior during RL training. We also use dropoutbased uncertainty estimates to lower bound the target Q-values as a more efficient alternative to Double Q-Learning. The algorithms are tested on the problem of open-domain dialog generation -a challenging reinforcement learning problem with a 20,000-dimensional action space. Using our Way Off-Policy algorithm, we can extract multiple different reward functions post-hoc from collected human interaction data, and learn effectively from all of these. We test the real-world generalization of these systems by deploying them live to converse with humans in an open-domain setting, and demonstrate that our algorithm achieves significant improvements over prior methods in off-policy batch RL.Preprint. Under review.\n\nIntroduction\n\nIn order to scale deep reinforcement learning (RL) to safety-critical, real-world domains, two abilities are needed. First, since collecting real-world interaction data can be expensive and time-consuming, algorithms must be able to leverage off-policy data -collected from vastly different systems, far into the past -in order to learn. Second, it is often necessary to carefully test a policy before deploying it to the real world; for example, to ensure its behavior is safe and appropriate for humans. Thus, the algorithm must be able to learn offline first, from a static batch of data, without the ability to explore.\n\nThis off-policy, batch reinforcement learning (BRL) setting represents a challenging RL problem. Most deep RL algorithms fail to learn from data that is not heavily correlated with the current policy [16]. Even models based on off-policy algorithms like Q-learning fail to learn when the model is not able to explore during training. This is due to the fact that such algorithms are inherently optimistic in the face of uncertainty. When value estimates are noisy, taking the maximum over estimates of future reward leads to a persistent overestimation bias. In a normal RL setting, this drives the model to explore areas of the state-action space for which the value estimates have the highest variance, thus enabling it to refine them. In a batch setting where the model cannot explore, it is instead driven to value parts of the state-action space for which it has little to no data to learn a good policy.\n\nWe propose to resolve these issues by leveraging a pre-trained generative model of the state-action space trained on known sequences of interaction data. While training with RL, we penalize divergence from this prior model with different forms of KL-control. We benchmark against a discrete adaptation of Batch Constrained Q (BCQ) [16], a recently proposed BRL algorithm for continuous domains, and show that KL-control achieves superior performance. Finally, we propose using dropout to obtain uncertainty estimates of the target Q values, and use this lower bound to alleviate the Q-learning overestimation bias. This provides a more efficient alternative to Clipped Double Q-Learning [15].\n\nWe apply these algorithms to a challenging, under-explored, real-world reinforcement learning problem: using implicitly expressed human reactions in chat to improve open-domain dialog systems. When a machine learning system interacts with humans, ideally we would like to learn about the humans' preferences in order to improve the performance of the system. Yet having humans manually indicate their preferences through explicit means like pressing a button (e.g. [7]) or submitting a feedback report, does not scale. Instead, we would like to be able to use humans' implicit reactions, such as the sentiment they express, or the length of the conversation, in order to improve the policy.\n\nApplying off-policy batch RL to language generation is challenging because the number of potential combinations of words and sentences leads to a combinatorial explosion in the size of the state space. The action space -the set of frequent vocabulary words in the English language -is 20,000dimensional. This compounds the overestimation problem, making BRL even more difficult. However, when learning from human interactions in the wild, it is crucial to be able to learn offline and test the policy before deploying it, lest it learn inappropriate behaviors (e.g. [26]).\n\nTo support this work, we developed an interactive online platform that allows humans to chat with deep neural network dialog models running on GPU; the BRL models trained for this study are available live at https://neural.chat/rl. Through this platform we collected human responses to a set of over 40 different dialog models over the course of several months. Using our Way Off-Policy algorithm, we are able to effectively learn from this batch of data, in spite of the fact that it was generated with a vastly different set of model architectures, which were trained on different datasets. Further, we use the batch to learn from many different reward functions designed post-hoc to extract implicit human preferences, something that is only possible with effective off-policy BRL.\n\n\nRelated Work\n\nThe approach we propose is based on KL-control, a branch of stochastic optimal control (SOC) [58] where the Kullback-Leibler (KL) divergence from some distribution is used to regularize an RL policy (e.g. [1,33,48,61]). Well-known examples include Trust Region Policy Optimization (TRPO) [50], and use conservative, KL-regularized policy updates to restrict the RL algorithm to stay close to its own prior policy (e.g. [22,32,44,48]). KL-control can also be applied to entropy maximization (e.g. [65]); for example, G-learning penalizes KL-divergence from a simple uniform distribution in order to cope with overestimation of Q-values [14]. Soft Q-learning motivates using a Boltzmann distribution in the value function as a way of performing maximum entropy RL [21]. KL-control has also been used to improve transfer learning between maximum likelihood estimation (MLE) training on data, and training with RL [28]. To the best of our knowledge, our work is the first to propose KL-control as a way of improving off-policy learning without exploration in a BRL setting.\n\nOther strategies to improve off-policy learning have been proposed, although many focus on scenarios where the policy is able to explore and collect more data (e.g. [10,49]). In the deep RL setting, policy gradients can be corrected to account for the difference in the distribution of states visited under the original policy and the learned off-policy algorithm [40]. Covariance-shift-based methods have been adapted to the off-policy deep RL setting to deal with the issue of value divergence [19]. Normalized feature representations have been proposed as an alternative approach [4]. Batch Constrained Q-learning (BCQ) [16] tackles off-policy batch learning in continuous action domains by training a generative model of the batch, p(a|s), sampling from this model, and selecting the best action based on a Q-estimate. This approach fails to integrate information about the distribution p(a|s) directly into the policy, and cannot scale to scenarios in which the state-action space is large, and the amount of available batch data is too small to train p(a|s). Many works from off-policy policy evaluation use importance sampling or model estimation to investigate the problem of estimating the performance of a policy given a batch of off-policy data (e.g. [11,29,45,60]). Effective off-policy learning gives us the ability to learn from many different rewards post-hoc, something that could potentially improve techniques which use the relabeling trick (e.g. [30,2]).\n\nWe propose using dropout to approximate model uncertainty of the target Q-network. The idea of using dropout to estimate uncertainty in neural networks was first proposed by Gal and colleagues (2016) [17]. Different forms of uncertainty estimates have been used in RL (e.g. [31,42]); for example, Bayesian uncertainty estimates have been proposed as an alternative to double DQN [3].\n\nImproving dialog systems with RL has largely been restricted to task-oriented dialog systems, which have a limited number of task-specific actions (e.g. [12,18,38,39,59]). These approaches may incorporate human input, usually through explicit, manual feedback (e.g. [54]), but sometimes with more implicit signals, such as the user interrupting the system or starting over [55]. Attempts to expand RL to the open-domain dialog setting are less numerous. Even in this setting, authors may choose to use a highly restricted action space; for example, using RL to choose which scripted or MLE dialog model to invoke to answer a user's query [51]. Early attempts to apply deep RL to the full vocabulary-sized action space relied mainly on hand-crafted rewards that described qualities of the generated text, such as ease of answering [35]. This approach has been extended to use a discriminator trained to distinguish human from generated text as a reward function [36,37]. While some work has incorporated implicit signals such as sentiment [23] and conversation length [64] in MLE systems, the idea of using such signals as a reward for RL is relatively unexplored. Shin and colleagues uses on-policy learning in conjunction with a user-sentiment approximator to improve a seq2seq model [56], but are unable to learn directly from user feedback. To the best of our knowledge, we are the first to use batch RL to train hierarchical open-domain dialog models on implicit cues gained from real human interactions.\n\n\nMethods\n\nWe employ typical RL notation in which s t represents the environment state at time t, the agent takes action a t according to its policy \u03c0(a t |s t ), and receives a reward r(s t , a t ). The agent's goal is to maximize reward over an episode trajectory \u03c4 , with a discount factor of \u03b3 applied to future rewards. Q-learning methods learn an action-value estimate of the total expected discounted future reward, Q \u03c0 (a t , s t ) = E \u03c0 [ T t =t \u03b3 t \u2212t r(s t , a t )], through iterative updates based on the Bellman equation:\nQ \u03b8\u03c0 (s t , a t ) = r(s t , a t ) + \u03b3E st+1\u223cp(\u00b7|st,at) [max at+1 Q \u03b8 T (s t+1 , a t+1 )](1)\nIn deep Q-learning [41], a Q-network approximates Q \u03b8\u03c0 (s t , a t ) and drives the policy \u03c0. A second target Q-network approximates the expected reward from the next state, Q \u03b8 T (s t+1 , a t+1 ) -a standard practice for alleviating overestimation bias [62].\n\nTo perform batch Q-learning, we first pre-train a generative model of p(a|s) using a set of known environment trajectories. In our case, this model is then used to generate the batch data via human interaction. The weights of the Q-network and target Q-network are initialized from the pre-trained model, which helps reduce variance in the Q-estimates and works to combat overestimation bias.\n\nTo train Q \u03b8\u03c0 we sample < s t , a t , r t , s t+1 > tuples from the batch, and update the weights of the Q-network to approximate Eq. 1. This forms our baseline model, which we call Batch Q.\n\n\nDropout for uncertainty estimation of Target Q-values\n\nOverestimation of Q-values becomes particularly problematic in the batch setting. The Q estimates for state-action pairs which are not well covered in the batch will be noisy, and this variance will lead the max operator in Eq. 1 to overestimate the value of these states. This drives the model to value regions of the state-action space for which it has no data to learn a reasonable policy, and no ability to explore to refine its estimates. Clipped Double Q-learning [15] addresses the overestimation problem by maintaining two independent pairs of Q-networks, and taking the minimum of their estimates of future reward. This approach is computationally expensive and memory intensive. Further, if following a transfer learning approach where the Q-network is initialized from a pre-trained MLE model (as we do in this paper), it is not clear how to obtain multiple independent target Q-networks.\n\nInstead, we obtain a distribution over predictions from a single target Q-network trained with dropout, and take the lower bound of these to reduce overestimation bias. It has been shown that dropout approximates Bayesian uncertainty for neural networks, by assuming the weights of the network are drawn from a Gaussian prior, W \u223c N (0, I), and using variational inference to estimate the posterior distribution p(W |X, Y ) [17]. We perform dropout during both training and inference before each weight layer, and approximate the posterior such that the dropout distribution q W is a mixture of Gaussians, and D KL [q W ||p(W |X, Y )] is minimized. Given the target Q-network Q \u03b8 T , we compute Q(a t+1 , s t+1 ) using a Monte Carlo (MC) estimate of the lower-bound of Q \u03b8 T (a t+1 , s t+1 ) by running M stochastic forward passes of the network, each with a new dropout mask d i \u223c q W :\nQ(a t+1 , s t+1 ) = min i=1...M [Q \u03b8 T (a t+1 , s t+1 ; d i )](2)\nUsing the minimum operator penalizes high variance estimates, essentially leading the algorithm to be pessimistic in the face of uncertainty, rather than optimistic. Such a bias will push the model to favour actions that lead to states well covered by the batch data [16]. We evaluate the performance of this approach using a second baseline model, Batch Q MC.\n\n\nDiscrete Batch Constrained Q\n\nBatch Constrained Q-learning (BCQ) [16] proposes to address the BRL problem by constraining the actions of the Q-network to be close to the data contained within the batch. This is accomplished by learning a generative model of the batch, G w = p(a|s), and sampling from this model during learning and inference. Because BCQ is designed for continuous action domains, it applies a learned perturbation model \u03be(s, a; \u03a6) which is allowed to alter the action within the range [\u2212\u03a6, \u03a6]. BCQ learns Q-estimates that incorporate the perturbation model, Q \u03b8 (s, a + \u03be(s, a; \u03a6)). To act, n possible actions are sampled from the generative model,\n{a i \u223c G w (s)} n i=1\n, perturbed, and the action with the maximum Q-value is selected, giving the BCQ policy:\n\u03c0 BCQ (s) = arg max ai+\u03be(s,ai;\u03a6) Q \u03b8 (s, a i + \u03be(s, a i ; \u03a6))(3)\nWe focus on the scenario where a model of p(a|s) can be obtained through MLE training on data of known action sequences. This prior model provides a more robust estimate of p(a|s) than one learned from the batch data, assuming the size of the batch is small relative to unsupervised data related to the problem (i.e. when the batch comes from human interaction data). We propose an adaptation of BCQ to discrete action spaces (DBCQ) which leverages such a strong pre-trained prior model as an improved version of G w . Since BCQ relies on Double Clipped Q-learning [15], here we use dropout-based uncertainty estimates as in Eq. 2. Because the action space is discrete, we do not use a perturbation model to modify actions, but instead define the DBCQ policy as:\n\u03c0 DBCQ (s) = arg max ai\u223cp(a|s) Q \u03b8\u03c0 (s, a i )(4)\n\nKL Control from pre-trained prior\n\nRather than simply sample from the prior, we would like the Q-learning algorithm to directly incorporate the prior into the policy. Thus, we use KL-control to penalize divergence between the prior p(a|s), and the Q-network policy \u03c0 \u03b8 , while still maximizing reward. Given a trajectory of\nactions, \u03c4 = {a 1 , a 2 , ...a t\u22121 }, let q(\u03c4 ) = T t=1 \u03c0 \u03b8 (a t , s t )\nbe the policy of our Q-learning algorithm at the trajectory level. Similarly, let p(\u03c4 ) = T t=1 p(a t |s t ) be the prior distribution over the trajectory, and r(\u03c4 ) be the rewards. We seek to maximize the following KL-regularized objective:\nL(q) = E q(\u03c4 ) [r(\u03c4 )]/c \u2212 D KL [q(\u03c4 )||p(\u03c4 )](5)Since D KL [q||p] = x q(x)(log q(x) \u2212 log p(x))\n, we can see that this is equivalent to maximizing the following expected value function of the policy \u03c0 \u03b8 at the action level:\nQ \u03c0 (s t , a t ) = E \u03c0 [ T t =t r(s t , a t )/c + log p(a t |s t ) \u2212 log \u03c0(a t |s t )](6)\nThe two terms we have introduced in Eq. 6 have clear motivations. The p(a|s) term rewards the model for choosing actions that have high probability under the prior, biasing the model to state-action pairs that are realistic, and likely to be in the batch. The \u2212 log \u03c0(a|s) term is analogous to entropy regularization. Maintaining diversity in the action space through entropy regularization is important for generative models like dialog systems, which are known to collapse to an uninteresting, small number of repeated samples [34]. Re-stating Eq. 6 as an entropy-regularized Q-function, we obtain:\nQ(s t , a t ) = E \u03c0 [ T t =t r(s t , a t )/c + log p(a t |s t ) + H(\u00b7|s t )](7)\nMotivated by energy-based models of the form \u03c0(a t |s t ) \u221d exp(\u2212E(s t , a t )), one can derive a soft version of the entropy-regularized Q-function that uses a Boltzmann distribution to estimate future reward [21]. We refer to it as a \u03a8-function following previous work [28], which derived this function as a generalization of the \u03a8-learning proposed by [48]. The optimal \u03a8-function and policy are:\n\u03a8 * (s t , a t ) = r(s t , a t )/c + log p(a t |s t ) + \u03b3 log a exp(\u03a8 * (s , a ))(8)\n\u03c0 * \u03a8 (a t |s t ) = exp(\u03a8 * (s t , a t )) (9) Because it avoids taking a hard max over noisy estimates, \u03a8-learning leads to less overestimation of future reward [1,21]. This leads to more stable TD updates and aids learning. Thus, we argue it will be especially useful in the BRL setting for reducing optimism in the face of uncertainty.\n\n\nModel averaging\n\nFinally, we explore the setting where the data in the batch may be generated from a large variety of different models M with different architectures, which each learn a different estimate of p(a|s; M ). We use this diversity to create a more robust prior by computing a weighted average of these models based on a normalized score S(M ) for each model. The score could be some measure of model quality, or simply the proportion of data in the batch that was generated with that model. Thus  In this work, we employ hierarchical seq2seq dialog models [20,43,52,53], which use three recurrent networks to generate the next utterance in a conversation (see Figure 1). The encoder RNN operates on the tokens of the next input utterance u t = [w 1 , w 2 , ...w n ], and encodes them into a representation h e t = f e (u t ). This is fed into a context RNN, which forms the upper level of the hierarchy -it is updated only after each utterance, rather than each token. The context RNN outputs h c t = f c (h e t ), which is fed into the decoder RNN, which produces the output utterance u t=1 one token at a time. Note that while transformer architectures (e.g. [47]) have emerged as a powerful alternative to seq2seq models, here we choose to focus on hierarchical architectures because it gives us the flexibility to extend this work to use hierarchical control in the future, by learning to optimize rewards at both the utterance and conversation level. Although we trained and tested a variety of different architectures drawing from several works [20,43,52,53], we converged on the Variational Hierarchical Recurrent Encoder Decoder (VHRED) as the most promising model [53]. We also apply knowledge distillation to improve the model's ability to recognize and encode the sentiment and semantics of the conversation, as proposed by [20].\n\n\nRL for open-domain dialog generation\n\nApplying RL to dialog generation is challenging due to the large state-action space. The model attempts to construct a response utterance u \u03c0 t = [a 1 , a 2 , ..., a n ] by iteratively choosing an action a i as the next token. The number of tokens in the vocabulary of our pre-trained model is 20,000, making the action space very high-dimensional, potentially compounding the problem of overestimation and making batch learning excessively difficult. However, initializing the Q-networks with the weights of the pre-trained language model provides a strong prior over the appropriate word to select.\n\nHere we consider human interaction to represent the 'environment'. The response of a human to the bot's utterance is used to compute a reward signal to train the model. The state of the environment s t constitutes all of the text in the conversation uttered so far, both by the bot and the human. The state has a hierarchical structure, marking its division into utterances, which are further divided into tokens. While the bot is constructing an utterance u \u03c0 t , it is straightforward to obtain a target Q-estimate of future reward using the model's estimated Q-values over its own next token in the utterance. However, at the last token of the bot's utterance, the estimated future reward must include the human's response u h t . Therefore, we append the human response into the conversation, s t+1 = [s t\u22121 , u \u03c0 t , u h t ], feed this into the target Q-network, and use the estimated Q-values for the first token of the bot's next utterance. All of the code for our models and RL techniques is available in open-source at https://github.com/natashamjaques/neural_chat/tree/master/rl.\n\n\nLearning from implicit human preferences\n\nWe would like to improve a dialog model's ability to engage in natural conversation with a human by learning from the signals implicit in the way that the human responds. Rather than having the human manually label good performance -which we show in this work does not scale -the agent should recognize informative cues within the user's responses, like sentiment, and the amount of time they spend chatting. Essentially, we want to create an agent that is intrinsically motivated to produce positive reactions in its human conversation partner. We design several intrinsic reward functions based on the rich, interactive content of conversation, taking inspiration from the psychology of human conversation: 1) eliciting positive sentiment and transitions from negative to positive sentiment, due to the importance of emotion to creating a sense of understanding [6,63]; 2) eliciting longer conversations and more words typed, since this is a signal of engagement [57,64]; 3) eliciting laughter (counting the number of 'ha's in the user response), because of its importance in building solidarity [24]; 4) high semantic similarity (close distance in sentence embedding space [8]) between the human input and agent response, because paraphrasing and style matching are important in facilitating good conversation [27,63]; and 5) asking questions, since this is an important active listening skill [5]. The total reward given to the agent is a combination of these, with details (and coefficients) given in the supplementary material. Note that the first 4 types of rewards depend on eliciting positive responses from a human user; we call these the implicit human reward. The 5th reward is easily exploitable by the agent itself. These rewards represent only an initial foray into designing good metrics of human enjoyment, and further experimentation will be needed to improve them.\n\n\nExperiments\n\nTo collect interactive human conversation data, we built a CUDA-capable web app that can host neural network dialog models on GPU for fast, real-time inference: https:neural.chat. The code for the server is available in open-source at https://github.com/asmadotgh/neural_chat_web. We trained over 40 dialog models with different architectures (e.g. [53,52,43,20]), on different datasets (movie dialogs [9] and Reddit [20]). Note that these models varied significantly in terms of the distribution of language they learned. We collected a batch of data containing 14232 pairs of user input and agent response. This batch was used to train the RL models described in Section 3, which were then re-deployed to the website. We recruited 90 Mechanical Turk workers to provide a total of 718 7-point Likert scale ratings of the bots' quality, fluency, diversity, contingency (relatedness), and empathy, after interacting with each bot for at least 3 turns. Participants also had the option to provide explicit feedback through upvoting or downvoting a particular utterance within the interface. Note that testing these models in the wild with humans represents a more meaningful test of generalization than testing an RL model in the same limited (game) environment in which it was trained, since humans are not restricted in the text they can type to the model, and are the ultimate authority on naturalistic conversation.\n\n\nResults\n\nTo compare models, we not only look at human users' ratings and votes, but also consider the automatic signals detectable from the text itself. This implicit human reward metric aggregates the measures listed in items 1-4 in Section 4.1, and measures the ability to elicit positive responses from the human. Table 1 shows the results of the evaluation. Each of the enhancements proposed (MC estimation of target Q-values, \u03a8-learning, and MA) leads to performance gains in terms of human reward, manual votes, or ratings. However, the most notable difference in performance comes from  Table 2: Purely reward-maximizing methods like Batch Q trivially exploit the reward function by asking a question every turn, and using the maximum number of tokens in every sentence. In contrast, KL-control methods output plausible language by staying close to the prior, but shift to using polite, cheerful language to maximize implicit human rewards.\n\n[ KL-control. The KL-control models show substantial gains over the baseline 1 models across both ratings and human reward. We perform a one-way analysis of variance (ANOVA) comparing the KL-control models to the Batch Q baselines and DBCQ on the total human rating score, and find that the KL-control models are significantly better, F (x) = 4.781, p < .05. This validates the hypothesis that KL-control with a strong, pre-trained prior can be used to improve batch RL. Without KL-regularization, the baseline RL models diverge quickly and continuously from the prior, losing information about realistic sequences -as shown in Figure 2. This figure also helps explain the poor performance of DBCQ in Table 1. The underlying Q-network in DBCQ does not directly integrate the prior. As Qlearning causes the model to diverge from the prior, the Q-estimates of language generated according to the prior become unrealistic, and Eq. 4 selects unrealistic actions. This results in highly 'diverse' (random) generated utterances. Note that since we operate in discrete action space, we could not include the perturbation model originally proposed by [16], which may be critical to achieving good performance with BCQ.\n\nThe pre-trained prior may be especially important in a generative domain like dialog, where the true reward function is unknown, and so purely maximizing reward may actually lead to poorer quality conversations. Table 2 shows examples of conversations with a Batch Q and KL-control model. Because the Batch Q model has no incentive to stay close to realistic language, it learns to exploit the reward by asking a question and outputting the maximum number of tokens (30) every utterance. These sentences contain implausible phrases that do not represent realistic language (e.g. \"where did you say to me?\"). In contrast, the KL-control model uses realistic language, but shifts its distribution towards cheerful and polite speech, presumably because this is what led to positive human responses in the batch data. Rather than simply cherry-picking results, we invite the reader to check for themselves; all of the models tested in this study are available at: https://neural.chat/rl. In fact, we noticed that all models trained with the implicit human rewards described in Section 4.1 learned to use more cheerful and supportive language. Therefore, we create post-hoc metrics to measure this effect (see the supplementary material for details). Figure 3 shows how these metrics, as well as the implicit rewards, differ across models. Without KL-control, baseline methods like Batch Q exploit simple rewards like asking questions at the expense of realistic language, explaining their poor quality ratings. In contrast, KL-control models learn to rely more on realistic but polite, supportive, and cheerful dialog to elicit higher total human reward.\n\nTo understand the effect of the implicit rewards, Figure 4 shows the reward trajectory over the ten best conversations obtained with models trained with different techniques. While we see that KL-control models are able to elicit significantly higher reward than baselines, we note that KL-control Q performs best overall and in terms of words elicited, even though it had lower quality ratings in Table  1. This suggests that maximizing these rewards is not a perfect proxy for human judgments of quality. Note also that eliciting laughter is an extremely rare event, and only the KL-control models are able to do so. Finally, Figure 4 (d) shows that manual votes occur even more rarely, suggesting that explicit feedback from humans is a cumbersome and sparse reward signal. Table 3 presents the results of models trained with only a single reward function, ordered from lowest to highest quality. Notably, extracting multiple different reward functions post-hoc from a batch of data and training on these independently is only possible with an effective BRL model. Here all models are trained with KL-control, \u03a8-learning, and MC targets. Investigating which rewards presented in Section 4.1 are most critical to achieving high-quality conversations with humans, we note that maximizing positive and minimizing negative sentiment in the user turns out to lead to the highest quality bot. This underscores the importance of affective signals as cues for good conversation. Bots trained on the manual upvotes and downvotes provided by users on the utterance level fail to achieve similarly high performance. Even though users were instructed to make use of the vote feature, the task is burdensome, and users did not vote frequently enough to provide a good training signal. This validates the hypothesis that implicit signals of human enjoyment (such as sentiment) are a more scalable way to learn from human preferences.\n\n\nConclusion\n\nThis paper presents a series of techniques which improve performance when learning off-policy without the possibility to explore -i.e. batch RL (BRL). Most significantly, we are the first to propose using KL-control from a strong prior model pre-trained on data as a way to avoid overestimation and instability in BRL. Our results demonstrate that KL-control is critical to achieving good performance in this setting. In a generative domain such as dialog, the true reward function is not known, and trivially exploiting the rewards can actually lead to worse performance. Thus, KL-control may be particularly necessary to ensure samples remain realistic and close to the data distribution. We propose several reward functions that could allow an open-domain dialog generation model to learn from rich cues implicit in human interaction, where learning from expressed sentiment was most promising. While these rewards are far from perfect or complete, we see that maximizing implicit rewards leads to better performance than relying on explicit feedback. We hope that the techniques presented here will allow other researchers to leverage BRL for learning from human interaction data, and spur the development of even better rewards for capturing human preferences. To compute sentiment on short texts like conversation utterances, we leverage a state-of-the-art sentimentdetection model, which was trained on a massive amount of Twitter data to predict the emojis in tweets [13].\n\nTransfer learning from this model to other tasks showed that it was able to significantly outperform a series of sentiment, irony, and sarcasm benchmarks. This DeepMoji model outputs a probability distribution over 64 most-frequently used emojis as shown in Figure 5. After observing the performance of the model in detecting users' emotions in the domain of online chat, we define a set of weights over the emojis and calculate the weighted sum over an emotion embedding vector to derive a sentiment reward which is higher for positive sentiment and lower for negative sentiment. These weights are shown in Figure 5 (b). We also compute a sentiment-transition reward using the same score based on whether the peak positive sentiment occurred later in the conversation than the peak negative sentiment, reasoning that sentiment should improve over the course of the conversation.\n\n(a) (b) Figure 5: (a) 64-most frequent emojis as predicted by [13] used for calculating emotion embeddings. (b) Assigned weights used in producing the sentiment reward from the predicted emoji values.\n\n\nEngagement-based\n\nBased on prior work [64], we use the number of turns in the conversation as an indicator of the quality of the bot's performance. To distribute this reward over every utterance in the conversation, we take the total conversation length N , and compute the discounted reward for utterance n < N as \u03b3 N \u2212n N . We also reward each utterance with the number of words in the user's response, which we refer to as the words elicited.\n\n\nLaughter\n\nLaughter has been shown to be very important to human affiliation [46] and solidarity [24]. Therefore, we detect the number of occurrences of the string 'ha' in the user's response, and use this as a reward. Interestingly, we find that bots trained to maximize user laughter learn to be extremely supportive and cheerful compared to other bots (for definitions of supportive and cheerful, see Section 8.1.7).\n\n\nSemantic similarity\n\nLanguage style matching has been shown to be a strong predictor of relationship initiation and stability [27].\n\nWhile it would be ideal if our chatbots could intelligently adapt their conversation style to a new user, in reality most baseline dialog models struggle to maintain topic coherence, even over a few utterances (for an analysis of this effect, see [20]). Therefore we reward semantic similarity between the user's input and the bot's response, to encourage the bot to stay on topic and produce reasonable answers. This score is computing by leveraging a state-of-the-art sentence embedding model [8], and penalizing distance in embedding space.\n\n\nQuestions\n\nAsking questions is an important listening skill, and is linked to conversation management, attentiveness, and responsiveness [5]. Therefore, we give the bot a reward of 0.5 if the utterance contains a question word (how, what, where, why, when, who), and an additional 0.5 if it contains a question mark.\n\n\nTotal reward equation\n\nThe total reward used to train the bots is a combination of the above rewards, in the following proportions:\n\n0.15682657*question + 0.13837638*semantic_coherence + 0.15313653*laughter + 0.14206642*sentiment_transition + 0.14206642*sentiment + 0.14760148*words_elicited + 0.1199262*conversation_length.\n\n\nPost-hoc metrics\n\nAfter training the bots on these rewards, we noticed a shift in the distribution of their language towards more polite, cheerful, and supportive speech. Therefore, we designed post-hoc metrics to measure these qualities, which are based on counting whether a subset of phrases is present in an utterance.\n\nPoliteness phrases: if I may; may I; please; thanks; no worries; if you don't mind; have a great day; I'm sorry.\n\nSupportive phrases: you're right; you are right; you're not alone; you are not alone; congrats; that's a good idea; that is a good idea; you'll be fine; you will be fine; you'll be okay; you will be okay; it will get better; sorry you're going through; sorry you are going through; if it makes you feel better; if it makes you feel any better; keep your head up; keep it up; I'm in a similar situation; I am in a similar situation; you'll get it; you will get it; happy for you; I'm in the same boat; I am in the same boat; if you feel like you need to vent.\n\nCheerful phrases: nice to hear; happy; excited; really nice; glad; the best; great; good time; looking forward; beautiful.\n\n\nTraining details and hyperparameters\n\nRL models were trained for between 800 and 1000 batches of data, where the batch size was fixed at 32. Early stopping was used to determine the number of training iterations of the best checkpoint. All other hyperparameters were shared between RL models, and were as follows: discount \u03b3 = 0.5, weight placed on RL reward vs. KL-divergence term c = 2, number of Monte Carlo samples of the Target Q-network M = 5, target network update rate \u03b1 = .005, learning rate r = .0001. We used a smooth L1 loss function to approximate the Q-values, and clipped gradients at a value of 1.0.\n\nThe underlying parameters of the VHRED model were as follows: Context RNN hidden size = 1000, decoder hidden size = 1250, encoder hidden size = 1250, z embedding size = 600, gradient clip = 1.0, dropout d = 0.2. The maximum conversation length was fixed at 5 utterances (context from more than 5 utterances ago was discarded), and the maximum sentence length was 30 tokens.\n\nWe also added layers to the Context RNN and regularized it to be able to predict the semantic content of the input utterance using a form of knowledge distillation [25] from a state-of-the-art sentence-embedding model [8]. There were 2 additional feedforward semantic prediction prediction layers of size 128, which used ReLu activation. Figure 6 shows the normalized reward scores obtained bots trained with respect to different rewards. While some bots (such as those trained to ask questions or elicit positive sentiment) effectively generalize to new users, we see that others (e.g. words elicited) are not actually able to best elicit those responses in the wild. We hypothesize this is because the relatively small size of batch date we were able to collect (\u2248 14, 000 utterances) does not give these bots enough information about how to elicit long responses from users.   Figure 6: Normalized reward scores obtained by models trained with respect to different rewards. We see that the bot trained to ask questions is easily able to exploit this reward, and similarly the bot trained to elicit positive sentiment does so successfully. For the rest of the bots, the relationship is less clear. For example, the bot trained to elicit laughter becomes the most supportive and cheerful, while the bot trained to elicit more words is very polite.\n\n\nAdditional results\n\n\nInteractive bot platform details\n\nTo collect data from humans interacting with our bots, we built https://neural.chat, a platform for hosting deep neural network dialog models online on GPU for fast, real-time inference. Figure 7) shows an example of the interface, in which users are able to rate the bots after talking to them for at least three turns. Figure 7: Interactive evaluation ratings page available at https://neural.chat. Figure 8 is an example conversation within the platform that interactive evaluation participants see. Annotators can optionally click the up and down arrows beside each chatbot response to give feedback on the specific utterance. Once 3 or more turns of the conversation has taken place, participants may click \"Close Chat and Rate\" to get to the rating screen.\n\n\nWebsite server setup and configuration\n\nThe server was hosted on a Google Cloud Platform virtual instance with 64GB of RAM and a NVIDIA Tesla P100 graphics card. The backend was a Django program being served by NGINX and uWSGI. For simplicity, we opted to have the Django process import the chatbots into the same Python process as Django, rather than have the two connect to each other via other means such as sockets. This configuration decreased development time and increased reliability, but it would need to be revisited if the server needed to scale several orders of magnitude past what was required for this study. The current configuration was still able to support hundreds of simultaneous users and host more than 30 bots concurrently. The chatbots were kept in a separate project from the Django project and maintained separately from the server code. Each chatbot extended an abstract class that defined key methods for the Django program to use, and was registered to a globally accessible dictionary via a decorator. The Django project was provided the path to the Chatbots project in its PYTHONPATH, so it could import the dictionary in which all the chatbot objects had been registered and use that to dynamically determine which chatbots were available and to access them in its views.\n\nIt is important to note that the chatbots used PyCUDA, and PyCUDA does not work in a multiprocessing environment. Because of this, uWSGI needed to be configured to only have one python process and to disable any attempt at multiprocessing. Furthermore, the chatbots required substantial startup times, so all chatbots are kept in memory at all times in the Django process. In order to keep all the chatbots in memory concurrently, we needed a very high amount of RAM on our server and opted for a 64GB virtual instance, and a GPU with 16GB RAM. This combination of CUDA to run the chatbots on the GPU with a high amount of RAM to keep all bots in memory at the same time resulted in incredibly fast server response times, with effectively no increase in response time when using the bots in requests compared to requests that did not.\n\nFor further information and instructions on server configuration, please read the server documentation available at https://github.com/asmadotgh/neural_chat_web. We hope that this platform will allow others to host their own bots and evaluate them in an interactive setting.\n\n\n, we define p M A (a|s) as the model-averaged prior: p M A (a|s) = M S(M )p(a|s; M ).\n\nFigure 1 :\n1Simplified diagram of the variational hierarchical dialog model.\n\nFigure 2 :\n2KL-divergence of the policy from the prior is lower with KL-control throughout training. Bands show standard deviation.\n\nFigure 3 :Figure 4 :\n34Z-scored reward. Red metrics were used in training rewards, green are post-hoc. Traditional RL methods like Batch Q exploit simple action-based rewards, like asking questions. In contrast, KL-control methods shift their distribution towards polite, supportive, and cheerful conversation, allowing them to elicit higher human reward (blue). Comparison of top 10 conversation trajectories observed across deployed models, 90% CI of the rewards: (a) Implicit human feedback; (b) Words elicited; (c) Laughter; (d) Manual votes.\n\nFigure 8 :\n8Interactive evaluation chat interface.\n\nTable 1 :\n1Interactive human evaluation of techniques for off-policy batch RL. KL-control models strongly out-perform other techniques. Ratings are Likert scale, votes and human reward are z-scores. KL-control MA \u03a8 2.60 \u00b1.43 3.47 \u00b1.42 3.00\u00b1.49 2.49 \u00b1.44 2.89 \u00b1.51 14.44 \u00b11.96 .127 .042Model type \nQuality \nFluent \nDiverse \nRelated \nEmpathy Total \nVotes \nHuman \nreward \nDBCQ \n1.64 \u00b1.29 1.87 \u00b1.34 3.13 \u00b1.58 1.84 \u00b1.34 2.09 \u00b1.38 10.58 \u00b11.55 -.228 -.050 \nBatch Q \n1.87 \u00b1.30 2.36 \u00b1.42 2.20 \u00b1.41 1.91 \u00b1.32 2.58 \u00b1.47 11.91 \u00b11.58 -.163 -.005 \nBatch Q + MC \n1.85 \u00b1.39 2.46 \u00b1.44 2.46 \u00b1.52 1.98 \u00b1.39 2.34 \u00b1.49 11.07 \u00b11.82 -.068 .005 \nKL-control Q \n2.38 \u00b1.39 3.24 \u00b1.47 3.42 \u00b1.54 2.38 \u00b1.45 2.56 \u00b1.43 13.98 \u00b11.81 .016 \n.004 \nKL-control \u03a8 \n2.33 \u00b1.41 3.73 \u00b1.53 2.82 \u00b1.50 2.31 \u00b1.44 3.47 \u00b1.50 14.67 \u00b11.82 .128 \n.061 \n\n\nTable 3 :\n3Interactive human evaluation of different reward functions (models trained with KL-control)Reward \nfunction \nQuality \nFluent \nDiverse \nRelated \nEmpathy Total \nVotes \nHuman \nreward \nConv. len. \n2.20 \u00b1.40 3.61 \u00b1.53 3.02 \u00b1.52 2.25 \u00b1.46 2.48 \u00b1.45 13.57 \u00b11.84 -.035 -.003 \nSemantic sim. 1.93 \u00b1.34 3.50 \u00b1.45 2.37 \u00b1.45 2.11 \u00b1.45 2.52 \u00b1.48 12.43 \u00b11.75 -.020 .012 \nUser laughter \n1.96 \u00b1.38 3.56 \u00b1.48 2.33 \u00b1.51 1.93 \u00b1.42 3.20 \u00b1.55 12.98 \u00b11.60 -.149 -.003 \nWords elicited 2.11 \u00b1.32 3.96 \u00b1.44 3.04 \u00b1.45 2.04 \u00b1.35 2.55 \u00b1.46 13.70 \u00b11.44 .059 \n.024 \nManual votes \n2.14 \u00b1.38 3.47 \u00b1.45 2.91 \u00b1.47 2.07 \u00b1.39 2.42 \u00b1.46 13.00 \u00b11.65 -.030 .010 \nSent. trans. \n2.02 \u00b1.31 3.71 \u00b1.49 2.98 \u00b1.50 2.04 \u00b1.42 2.84 \u00b1.48 13.60 \u00b11.63 .031 \n.014 \nQuestion \n2.29 \u00b1.37 4.31 \u00b1.50 3.31 \u00b1.52 2.20 \u00b1.40 2.60 \u00b1.41 14.71 \u00b11.63 .057 \n.012 \nSentiment \n2.47 \u00b1.32 4.05 \u00b1.45 3.23 \u00b1.46 2.42 \u00b1.39 3.23 \u00b1.55 15.40 \u00b11.49 .085 \n.045 \n\n\nWe also compare the RL models to the prior, and see performance improvements in terms of the elicited human reward, but not in terms of the quality ratings. We believe this is because the rewards proposed here do not fully cover what it means to have a high quality conversation (as we will elucidate later), and hope that other researchers be able to use the techniques we propose to learn from improved rewards.\nAcknowledgmentsWe would like to thank Scott Fujimoto for insightful email correspondence on this topic, approval of the DBCQ algorithm, and suggestion to apply model averaging. We also thank Max Kleiman-Weiner, Ardavan Saeedi, Sebastian Zepf, Sara Taylor, Oliver Saunders Wilder, Kyle Kastner, and Kristy Johnson for their helpful discussions about this project, and many others for helping test-drive our bots.We thank the MIT Quest for Intelligence, and MIT Stephen A. Schwarzman College of Computing, and the Machine Learning Across Disciplines Challenge for providing computing resources, and MIT Media Lab Consortium for the support of this research.\nAbbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, arXiv:1806.06920Remi Munos, Nicolas Heess, and Martin Riedmiller. Maximum a posteriori policy optimisation. arXiv preprintAbbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos, Nicolas Heess, and Martin Riedmiller. Maximum a posteriori policy optimisation. arXiv preprint arXiv:1806.06920, 2018.\n\nOpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob Mcgrew, Josh Tobin, Advances in Neural Information Processing Systems. Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, OpenAI Pieter Abbeel, and Wojciech Zaremba. Hindsight experience replay. In Advances in Neural Information Processing Systems, pages 5048-5058, 2017.\n\nEfficient exploration through bayesian deep q-networks. Kamyar Azizzadenesheli, Emma Brunskill, Animashree Anandkumar, 2018 Information Theory and Applications Workshop (ITA). IEEEKamyar Azizzadenesheli, Emma Brunskill, and Animashree Anandkumar. Efficient exploration through bayesian deep q-networks. In 2018 Information Theory and Applications Workshop (ITA), pages 1-9. IEEE, 2018.\n\nCrossnorm: Normalization for off-policy td reinforcement learning. Aditya Bhatt, Max Argus, Artemij Amiranashvili, Thomas Brox, arXiv:1902.05605arXiv preprintAditya Bhatt, Max Argus, Artemij Amiranashvili, and Thomas Brox. Crossnorm: Normalization for off-policy td reinforcement learning. arXiv preprint arXiv:1902.05605, 2019.\n\nListening competence in initial interactions i: Distinguishing between what listening is and what listeners do. Kellie Graham D Bodie, St, Michelle Cyr, Michael Pence, James Rold, Honeycutt, International Journal of Listening. 261Graham D Bodie, Kellie St. Cyr, Michelle Pence, Michael Rold, and James Honeycutt. Listening competence in initial interactions i: Distinguishing between what listening is and what listeners do. International Journal of Listening, 26(1):1-28, 2012.\n\nThe role of \"active listening\" in informal helping conversations: Impact on perceptions of listener helpfulness, sensitivity, and supportiveness and discloser emotional improvement. Andrea J Graham D Bodie, Kaitlin Vickery, Susanne M Cannava, Jones, Western Journal of Communication. 792Graham D Bodie, Andrea J Vickery, Kaitlin Cannava, and Susanne M Jones. The role of \"active listen- ing\" in informal helping conversations: Impact on perceptions of listener helpfulness, sensitivity, and supportiveness and discloser emotional improvement. Western Journal of Communication, 79(2):151-173, 2015.\n\nDeep reinforcement learning from human preferences. Jan Paul F Christiano, Tom Leike, Miljan Brown, Shane Martic, Dario Legg, Amodei, Advances in Neural Information Processing Systems. Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforce- ment learning from human preferences. In Advances in Neural Information Processing Systems, pages 4299-4307, 2017.\n\nSupervised learning of universal sentence representations from natural language inference data. Alexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00efc Barrault, Antoine Bordes, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00efc Barrault, and Antoine Bordes. Supervised learning of universal sentence representations from natural language inference data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 670-680, 2017.\n\nChameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs. Cristian Danescu-Niculescu-Mizil, Lillian Lee, Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics. the 2nd Workshop on Cognitive Modeling and Computational LinguisticsAssociation for Computational LinguisticsCristian Danescu-Niculescu-Mizil and Lillian Lee. Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs. In Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics, pages 76-87. Association for Computational Linguistics, 2011.\n\nOff-policy actor-critic. Thomas Degris, Martha White, Richard S Sutton, Proceedings of the 29th International Coference on International Conference on Machine Learning. the 29th International Coference on International Conference on Machine LearningOmnipressThomas Degris, Martha White, and Richard S Sutton. Off-policy actor-critic. In Proceedings of the 29th International Coference on International Conference on Machine Learning, pages 179-186. Omnipress, 2012.\n\nMore robust doubly robust off-policy evaluation. Mehrdad Farajtabar, Yinlam Chow, Mohammad Ghavamzadeh, International Conference on Machine Learning. Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh. More robust doubly robust off-policy evaluation. In International Conference on Machine Learning, pages 1446-1455, 2018.\n\nPolicy networks with two-stage training for dialogue systems. Mehdi Fatemi, Layla El Asri, Hannes Schulz, Jing He, Kaheer Suleman, Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue. the 17th Annual Meeting of the Special Interest Group on Discourse and DialogueMehdi Fatemi, Layla El Asri, Hannes Schulz, Jing He, and Kaheer Suleman. Policy networks with two-stage training for dialogue systems. In Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 101-110, 2016.\n\nUsing millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm. Bjarke Felbo, Alan Mislove, Anders S\u00f8gaard, Iyad Rahwan, Sune Lehmann, 2017 Conference on Empirical Methods in Natural Language ProcessingConference on Empirical Methods in Natural Language Processing. Association for Computational LinguisticsBjarke Felbo, Alan Mislove, Anders S\u00f8gaard, Iyad Rahwan, and Sune Lehmann. Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm. In 2017 Conference on Empirical Methods in Natural Language ProcessingConference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2017.\n\nTaming the noise in reinforcement learning via soft updates. Roy Fox, Ari Pakman, Naftali Tishby, Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence. the Thirty-Second Conference on Uncertainty in Artificial IntelligenceAUAI PressRoy Fox, Ari Pakman, and Naftali Tishby. Taming the noise in reinforcement learning via soft updates. In Proceedings of the Thirty-Second Conference on Uncertainty in Artificial Intelligence, pages 202-211. AUAI Press, 2016.\n\nAddressing function approximation error in actor-critic methods. Scott Fujimoto, Herke Hoof, David Meger, International Conference on Machine Learning. Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In International Conference on Machine Learning, pages 1582-1591, 2018.\n\nOff-policy deep reinforcement learning without exploration. Scott Fujimoto, David Meger, Doina Precup, arXiv:1812.02900arXiv preprintScott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without explo- ration. arXiv preprint arXiv:1812.02900, 2018.\n\nDropout as a bayesian approximation: Representing model uncertainty in deep learning. Yarin Gal, Zoubin Ghahramani, international conference on machine learning. Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning, pages 1050-1059, 2016.\n\nOn-line policy optimisation of spoken dialogue systems via live interaction with human subjects. Milica Ga\u0161i\u0107, Filip Jur\u010d\u00ed\u010dek, Blaise Thomson, Kai Yu, Steve Young, 2011 IEEE Workshop on Automatic Speech Recognition & Understanding. IEEEMilica Ga\u0161i\u0107, Filip Jur\u010d\u00ed\u010dek, Blaise Thomson, Kai Yu, and Steve Young. On-line policy optimisation of spoken dialogue systems via live interaction with human subjects. In 2011 IEEE Workshop on Automatic Speech Recognition & Understanding, pages 312-317. IEEE, 2011.\n\nOff-policy deep reinforcement learning by bootstrapping the covariate shift. Carles Gelada, G Marc, Bellemare, arXiv:1901.09455arXiv preprintCarles Gelada and Marc G Bellemare. Off-policy deep reinforcement learning by bootstrapping the covariate shift. arXiv preprint arXiv:1901.09455, 2019.\n\nApproximating interactive human evaluation with self-play for open-domain dialog systems. Asma Ghandeharioun, Judy Shen, Natasha Jaques, Craig Ferguson, Noah Jones, Agata Lapedriza, Rosalind Picard, arXiv:1906.09308arXiv preprintAsma Ghandeharioun, Judy Shen, Natasha Jaques, Craig Ferguson, Noah Jones, Agata Lapedriza, and Rosalind Picard. Approximating interactive human evaluation with self-play for open-domain dialog systems. arXiv preprint arXiv:1906.09308, 2019.\n\nReinforcement learning with deep energy-based policies. Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, Sergey Levine, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1352-1361. JMLR. org, 2017.\n\nSoft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine, International Conference on Machine Learning. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International Conference on Machine Learning, pages 1856-1865, 2018.\n\nLearning from dialogue after deployment: Feed yourself. Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazare, Jason Weston, arXiv:1901.05415chatbot! arXiv preprintBraden Hancock, Antoine Bordes, Pierre-Emmanuel Mazare, and Jason Weston. Learning from dialogue after deployment: Feed yourself, chatbot! arXiv preprint arXiv:1901.05415, 2019.\n\nFunctions of humor in the conversations of men and women. Jennifer Hay, Journal of pragmatics. 326Jennifer Hay. Functions of humor in the conversations of men and women. Journal of pragmatics, 32(6):709-742, 2000.\n\nDistilling the knowledge in a neural network. Geoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.02531arXiv preprintGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.\n\nMicrosoft deletes 'teen girl' ai after it became a hitler-loving sex robot within 24 hours. Helena Horton, Telegraph UK. Helena Horton. Microsoft deletes 'teen girl' ai after it became a hitler-loving sex robot within 24 hours. In Telegraph UK, 2016.\n\nLanguage style matching predicts relationship initiation and stability. E Molly, Richard B Ireland, Paul W Slatcher, Lauren E Eastwick, Eli J Scissors, James W Finkel, Pennebaker, Psychological science. 221Molly E Ireland, Richard B Slatcher, Paul W Eastwick, Lauren E Scissors, Eli J Finkel, and James W Pennebaker. Language style matching predicts relationship initiation and stability. Psychological science, 22(1):39-44, 2011.\n\nSequence tutor: Conservative fine-tuning of sequence generation models with kl-control. Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Richard E Turner, Douglas Eck, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, Richard E Turner, and Douglas Eck. Sequence tutor: Conservative fine-tuning of sequence generation models with kl-control. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1645-1654. JMLR. org, 2017.\n\nDoubly robust off-policy value evaluation for reinforcement learning. Nan Jiang, Lihong Li, International Conference on Machine Learning. Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning. In International Conference on Machine Learning, pages 652-661, 2016.\n\nLearning to achieve goals. Leslie Pack, Kaelbling , IJCAI. CiteseerLeslie Pack Kaelbling. Learning to achieve goals. In IJCAI, pages 1094-1099. Citeseer, 1993.\n\nUncertainty-aware reinforcement learning for collision avoidance. Gregory Kahn, Adam Villaflor, Vitchyr Pong, Pieter Abbeel, Sergey Levine, arXiv:1702.01182arXiv preprintGregory Kahn, Adam Villaflor, Vitchyr Pong, Pieter Abbeel, and Sergey Levine. Uncertainty-aware reinforcement learning for collision avoidance. arXiv preprint arXiv:1702.01182, 2017.\n\nA natural policy gradient. M Sham, Kakade, Advances in neural information processing systems (NIPS). 14Sham M Kakade. A natural policy gradient. In Advances in neural information processing systems (NIPS), volume 14, pages 1531-1538, 2002.\n\nOptimal control as a graphical model inference problem. Vicen\u00e7 Hilbert J Kappen, Manfred G\u00f3mez, Opper, Machine learning. 872Hilbert J Kappen, Vicen\u00e7 G\u00f3mez, and Manfred Opper. Optimal control as a graphical model inference problem. Machine learning, 87(2):159-182, 2012.\n\nDialogue learning with human-in-the-loop. Jiwei Li, H Alexander, Sumit Miller, Marc&apos;aurelio Chopra, Jason Ranzato, Weston, arXiv:1611.09823arXiv preprintJiwei Li, Alexander H Miller, Sumit Chopra, Marc'Aurelio Ranzato, and Jason Weston. Dialogue learning with human-in-the-loop. arXiv preprint arXiv:1611.09823, 2016.\n\nDeep reinforcement learning for dialogue generation. Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, Jianfeng Gao, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingJiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, and Jianfeng Gao. Deep reinforcement learning for dialogue generation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1192-1202, 2016.\n\nAdversarial learning for neural dialogue generation. Jiwei Li, Will Monroe, Tianlin Shi, S\u00e9bastien Jean, Alan Ritter, Dan Jurafsky, Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. the 2017 Conference on Empirical Methods in Natural Language ProcessingJiwei Li, Will Monroe, Tianlin Shi, S\u00e9bastien Jean, Alan Ritter, and Dan Jurafsky. Adversarial learning for neural dialogue generation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2157-2169, 2017.\n\nDialogue generation: From imitation learning to inverse reinforcement learning. Ziming Li, Julia Kiseleva, Maarten De Rijke, arXiv:1812.03509arXiv preprintZiming Li, Julia Kiseleva, and Maarten de Rijke. Dialogue generation: From imitation learning to inverse reinforcement learning. arXiv preprint arXiv:1812.03509, 2018.\n\nIterative policy learning in end-to-end trainable task-oriented neural dialog models. Bing Liu, Ian Lane, 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEEBing Liu and Ian Lane. Iterative policy learning in end-to-end trainable task-oriented neural dialog models. In 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 482-489. IEEE, 2017.\n\nDialogue learning with human teaching and feedback in end-to-end trainable task-oriented dialogue systems. Bing Liu, Gokhan T\u00fcr, Dilek Hakkani-T\u00fcr, Pararth Shah, Larry Heck, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies1Long PapersBing Liu, Gokhan T\u00fcr, Dilek Hakkani-T\u00fcr, Pararth Shah, and Larry Heck. Dialogue learning with human teaching and feedback in end-to-end trainable task-oriented dialogue systems. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 2060-2069, 2018.\n\nOff-policy policy gradient with state distribution correction. Yao Liu, Adith Swaminathan, Alekh Agarwal, Emma Brunskill, arXiv:1904.08473arXiv preprintYao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill. Off-policy policy gradient with state distribution correction. arXiv preprint arXiv:1904.08473, 2019.\n\nPlaying atari with deep reinforcement learning. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller, arXiv:1312.5602arXiv preprintVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013.\n\nDeep exploration via bootstrapped dqn. Ian Osband, Charles Blundell, Alexander Pritzel, Benjamin Van Roy, Advances in neural information processing systems. Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van Roy. Deep exploration via bootstrapped dqn. In Advances in neural information processing systems, pages 4026-4034, 2016.\n\nA hierarchical latent structure for variational conversation modeling. Yookoon Park, Jaemin Cho, Gunhee Kim, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies1Long PapersYookoon Park, Jaemin Cho, and Gunhee Kim. A hierarchical latent structure for variational conversation modeling. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1792-1801, 2018.\n\nRelative entropy policy search. Jan Peters, Katharina M\u00fclling, Yasemin Altun, AAAI. Jan Peters, Katharina M\u00fclling, and Yasemin Altun. Relative entropy policy search. In AAAI, pages 1607-1612. Atlanta, 2010.\n\nEligibility traces for off-policy policy evaluation. Doina Precup, Computer Science Department Faculty Publication Series. 80Doina Precup. Eligibility traces for off-policy policy evaluation. Computer Science Department Faculty Publication Series, page 80, 2000.\n\n. Robert R Provine, Laughter, American scientist. 841Robert R Provine. Laughter. American scientist, 84(1):38-48, 1996.\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI Blog. 18Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI Blog, 1:8, 2019.\n\nOn stochastic optimal control and reinforcement learning by approximate inference. Konrad Rawlik, Marc Toussaint, Sethu Vijayakumar, Robotics: science and systems. Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar. On stochastic optimal control and reinforcement learning by approximate inference. In Robotics: science and systems, 2012.\n\nNeural fitted q iteration-first experiences with a data efficient neural reinforcement learning method. Martin Riedmiller, European Conference on Machine Learning. SpringerMartin Riedmiller. Neural fitted q iteration-first experiences with a data efficient neural reinforcement learning method. In European Conference on Machine Learning, pages 317-328. Springer, 2005.\n\nTrust region policy optimization. John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, Philipp Moritz, Proceedings of the 32nd International Conference on Machine Learning (ICML-15). the 32nd International Conference on Machine Learning (ICML-15)John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning (ICML-15), pages 1889-1897, 2015.\n\n. Chinnadhurai Iulian V Serban, Mathieu Sankar, Saizheng Germain, Zhouhan Zhang, Sandeep Lin, Taesup Subramanian, Michael Kim, Sarath Pieper, Chandar, arXiv:1709.02349Nan Rosemary KeA deep reinforcement learning chatbot. arXiv preprintIulian V Serban, Chinnadhurai Sankar, Mathieu Germain, Saizheng Zhang, Zhouhan Lin, Sandeep Subramanian, Taesup Kim, Michael Pieper, Sarath Chandar, Nan Rosemary Ke, et al. A deep reinforcement learning chatbot. arXiv preprint arXiv:1709.02349, 2017.\n\nBuilding end-to-end dialogue systems using generative hierarchical neural network models. Alessandro Iulian V Serban, Yoshua Sordoni, Aaron Bengio, Joelle Courville, Pineau, Thirtieth AAAI Conference on Artificial Intelligence. Iulian V Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville, and Joelle Pineau. Building end-to-end dialogue systems using generative hierarchical neural network models. In Thirtieth AAAI Conference on Artificial Intelligence, 2016.\n\nA hierarchical latent variable encoder-decoder model for generating dialogues. Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron Courville, Yoshua Bengio, Thirty-First AAAI Conference on Artificial Intelligence. Iulian Vlad Serban, Alessandro Sordoni, Ryan Lowe, Laurent Charlin, Joelle Pineau, Aaron Courville, and Yoshua Bengio. A hierarchical latent variable encoder-decoder model for generating dialogues. In Thirty-First AAAI Conference on Artificial Intelligence, 2017.\n\nBootstrapping a neural conversational agent with dialogue self-play, crowdsourcing and on-line reinforcement learning. Pararth Shah, Dilek Hakkani-Tur, Bing Liu, Gokhan Tur, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies3Industry PapersPararth Shah, Dilek Hakkani-Tur, Bing Liu, and Gokhan Tur. Bootstrapping a neural conversational agent with dialogue self-play, crowdsourcing and on-line reinforcement learning. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers), pages 41-51, 2018.\n\nSentiment adaptive end-to-end dialog systems. Weiyan Shi, Zhou Yu, Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics. the 56th Annual Meeting of the Association for Computational LinguisticsLong Papers1Weiyan Shi and Zhou Yu. Sentiment adaptive end-to-end dialog systems. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1509-1519, 2018.\n\nHappybot: Generating empathetic dialogue responses by improving user experience look-ahead. Jamin Shin, Peng Xu, Andrea Madotto, Pascale Fung, arXiv:1906.08487arXiv preprintJamin Shin, Peng Xu, Andrea Madotto, and Pascale Fung. Happybot: Generating empathetic dialogue responses by improving user experience look-ahead. arXiv preprint arXiv:1906.08487, 2019.\n\nWhere to look: a study of human-robot engagement. L Candace, Cory D Sidner, Christopher Kidd, Neal Lee, Lesh, Proceedings of the 9th international conference on Intelligent user interfaces. the 9th international conference on Intelligent user interfacesACMCandace L Sidner, Cory D Kidd, Christopher Lee, and Neal Lesh. Where to look: a study of human-robot engagement. In Proceedings of the 9th international conference on Intelligent user interfaces, pages 78-84. ACM, 2004.\n\nStochastic optimal control. F Robert, Stengel, John Wiley and SonsNew York, New YorkRobert F Stengel. Stochastic optimal control. John Wiley and Sons New York, New York, 1986.\n\nSample-efficient actorcritic reinforcement learning with supervised data for dialogue management. Pei-Hao Su, Pawe\u0142 Budzianowski, Stefan Ultes, Milica Gasic, Steve Young, Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue. the 18th Annual SIGdial Meeting on Discourse and DialoguePei-Hao Su, Pawe\u0142 Budzianowski, Stefan Ultes, Milica Gasic, and Steve Young. Sample-efficient actor- critic reinforcement learning with supervised data for dialogue management. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 147-157, 2017.\n\nData-efficient off-policy policy evaluation for reinforcement learning. Philip Thomas, Emma Brunskill, International Conference on Machine Learning. Philip Thomas and Emma Brunskill. Data-efficient off-policy policy evaluation for reinforcement learning. In International Conference on Machine Learning, pages 2139-2148, 2016.\n\nLinearly-solvable markov decision problems. Emanuel Todorov, Advances in neural information processing systems (NIPS). Emanuel Todorov. Linearly-solvable markov decision problems. In Advances in neural information processing systems (NIPS), pages 1369-1376, 2007.\n", "annotations": {"author": "[{\"end\":217,\"start\":92},{\"end\":331,\"start\":218},{\"end\":443,\"start\":332},{\"end\":553,\"start\":444},{\"end\":664,\"start\":554},{\"end\":770,\"start\":665},{\"end\":877,\"start\":771},{\"end\":988,\"start\":878}]", "publisher": null, "author_last_name": "[{\"end\":106,\"start\":100},{\"end\":236,\"start\":223},{\"end\":348,\"start\":344},{\"end\":458,\"start\":450},{\"end\":569,\"start\":560},{\"end\":675,\"start\":670},{\"end\":782,\"start\":780},{\"end\":893,\"start\":887}]", "author_first_name": "[{\"end\":99,\"start\":92},{\"end\":222,\"start\":218},{\"end\":336,\"start\":332},{\"end\":343,\"start\":337},{\"end\":449,\"start\":444},{\"end\":559,\"start\":554},{\"end\":669,\"start\":665},{\"end\":779,\"start\":771},{\"end\":886,\"start\":878}]", "author_affiliation": "[{\"end\":216,\"start\":124},{\"end\":330,\"start\":238},{\"end\":442,\"start\":350},{\"end\":552,\"start\":460},{\"end\":663,\"start\":571},{\"end\":769,\"start\":677},{\"end\":876,\"start\":784},{\"end\":987,\"start\":895}]", "title": "[{\"end\":89,\"start\":1},{\"end\":1077,\"start\":989}]", "venue": null, "abstract": "[{\"end\":2534,\"start\":1079}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3379,\"start\":3375},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4421,\"start\":4417},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4777,\"start\":4773},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5248,\"start\":5245},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6042,\"start\":6038},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":6944,\"start\":6940},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7055,\"start\":7052},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7058,\"start\":7055},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":7061,\"start\":7058},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":7064,\"start\":7061},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":7139,\"start\":7135},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7270,\"start\":7266},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7273,\"start\":7270},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7276,\"start\":7273},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":7279,\"start\":7276},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7486,\"start\":7482},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7613,\"start\":7609},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7761,\"start\":7757},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8087,\"start\":8083},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":8090,\"start\":8087},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8286,\"start\":8282},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8418,\"start\":8414},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8504,\"start\":8501},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8545,\"start\":8541},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":9184,\"start\":9180},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9187,\"start\":9184},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9190,\"start\":9187},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":9193,\"start\":9190},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9387,\"start\":9383},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9389,\"start\":9387},{\"end\":9592,\"start\":9567},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9597,\"start\":9593},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9671,\"start\":9667},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9674,\"start\":9671},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9775,\"start\":9772},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9935,\"start\":9931},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9938,\"start\":9935},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9941,\"start\":9938},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9944,\"start\":9941},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":9947,\"start\":9944},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":10048,\"start\":10044},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":10155,\"start\":10151},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":10420,\"start\":10416},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10612,\"start\":10608},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10743,\"start\":10739},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10746,\"start\":10743},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10820,\"start\":10816},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":11067,\"start\":11063},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11937,\"start\":11933},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13290,\"start\":13286},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":14145,\"start\":14141},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14942,\"start\":14938},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15103,\"start\":15099},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":16446,\"start\":16442},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":18177,\"start\":18173},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":18539,\"start\":18535},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":18600,\"start\":18596},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":18684,\"start\":18680},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18974,\"start\":18971},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":18977,\"start\":18974},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":19721,\"start\":19717},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":19724,\"start\":19721},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":19727,\"start\":19724},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":19730,\"start\":19727},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":20326,\"start\":20322},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":20716,\"start\":20712},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":20719,\"start\":20716},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":20722,\"start\":20719},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":20725,\"start\":20722},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":20838,\"start\":20834},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21000,\"start\":20996},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23645,\"start\":23642},{\"end\":23648,\"start\":23645},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":23747,\"start\":23743},{\"end\":23750,\"start\":23747},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":23880,\"start\":23876},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":23957,\"start\":23954},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24095,\"start\":24091},{\"end\":24098,\"start\":24095},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24178,\"start\":24175},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":25030,\"start\":25026},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":25033,\"start\":25030},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":25036,\"start\":25033},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25039,\"start\":25036},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":25082,\"start\":25079},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25098,\"start\":25094},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":28193,\"start\":28189},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":33326,\"start\":33322},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":34276,\"start\":34272},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":34941,\"start\":34937},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":34961,\"start\":34957},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":35412,\"start\":35408},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":35666,\"start\":35662},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":35913,\"start\":35910},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":36101,\"start\":36098},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":38890,\"start\":38886},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":38943,\"start\":38940}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":43397,\"start\":43310},{\"attributes\":{\"id\":\"fig_1\"},\"end\":43475,\"start\":43398},{\"attributes\":{\"id\":\"fig_2\"},\"end\":43608,\"start\":43476},{\"attributes\":{\"id\":\"fig_3\"},\"end\":44156,\"start\":43609},{\"attributes\":{\"id\":\"fig_5\"},\"end\":44208,\"start\":44157},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":45008,\"start\":44209},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":45902,\"start\":45009}]", "paragraph": "[{\"end\":3173,\"start\":2550},{\"end\":4084,\"start\":3175},{\"end\":4778,\"start\":4086},{\"end\":5470,\"start\":4780},{\"end\":6044,\"start\":5472},{\"end\":6830,\"start\":6046},{\"end\":7916,\"start\":6847},{\"end\":9391,\"start\":7918},{\"end\":9776,\"start\":9393},{\"end\":11286,\"start\":9778},{\"end\":11821,\"start\":11298},{\"end\":12172,\"start\":11914},{\"end\":12566,\"start\":12174},{\"end\":12758,\"start\":12568},{\"end\":13715,\"start\":12816},{\"end\":14604,\"start\":13717},{\"end\":15031,\"start\":14671},{\"end\":15700,\"start\":15064},{\"end\":15811,\"start\":15723},{\"end\":16639,\"start\":15877},{\"end\":17013,\"start\":16725},{\"end\":17328,\"start\":17087},{\"end\":17553,\"start\":17426},{\"end\":18244,\"start\":17644},{\"end\":18724,\"start\":18325},{\"end\":19147,\"start\":18810},{\"end\":21001,\"start\":19167},{\"end\":21642,\"start\":21042},{\"end\":22733,\"start\":21644},{\"end\":24661,\"start\":22778},{\"end\":26094,\"start\":24677},{\"end\":27044,\"start\":26106},{\"end\":28256,\"start\":27046},{\"end\":29908,\"start\":28258},{\"end\":31832,\"start\":29910},{\"end\":33327,\"start\":31847},{\"end\":34208,\"start\":33329},{\"end\":34410,\"start\":34210},{\"end\":34858,\"start\":34431},{\"end\":35279,\"start\":34871},{\"end\":35413,\"start\":35303},{\"end\":35958,\"start\":35415},{\"end\":36277,\"start\":35972},{\"end\":36411,\"start\":36303},{\"end\":36604,\"start\":36413},{\"end\":36929,\"start\":36625},{\"end\":37043,\"start\":36931},{\"end\":37603,\"start\":37045},{\"end\":37727,\"start\":37605},{\"end\":38345,\"start\":37768},{\"end\":38720,\"start\":38347},{\"end\":40070,\"start\":38722},{\"end\":40890,\"start\":40128},{\"end\":42197,\"start\":40933},{\"end\":43033,\"start\":42199},{\"end\":43309,\"start\":43035}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11913,\"start\":11822},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14670,\"start\":14605},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15722,\"start\":15701},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15876,\"start\":15812},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16688,\"start\":16640},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17086,\"start\":17014},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17378,\"start\":17329},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17425,\"start\":17378},{\"attributes\":{\"id\":\"formula_8\"},\"end\":17643,\"start\":17554},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18324,\"start\":18245},{\"attributes\":{\"id\":\"formula_10\"},\"end\":18809,\"start\":18725}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":26421,\"start\":26414},{\"end\":26698,\"start\":26691},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":27754,\"start\":27747},{\"end\":28477,\"start\":28470},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":30316,\"start\":30308},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":30694,\"start\":30687}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2548,\"start\":2536},{\"attributes\":{\"n\":\"2\"},\"end\":6845,\"start\":6833},{\"attributes\":{\"n\":\"3\"},\"end\":11296,\"start\":11289},{\"attributes\":{\"n\":\"3.1\"},\"end\":12814,\"start\":12761},{\"attributes\":{\"n\":\"3.2\"},\"end\":15062,\"start\":15034},{\"attributes\":{\"n\":\"3.3\"},\"end\":16723,\"start\":16690},{\"attributes\":{\"n\":\"3.4\"},\"end\":19165,\"start\":19150},{\"attributes\":{\"n\":\"4\"},\"end\":21040,\"start\":21004},{\"attributes\":{\"n\":\"4.1\"},\"end\":22776,\"start\":22736},{\"attributes\":{\"n\":\"5\"},\"end\":24675,\"start\":24664},{\"attributes\":{\"n\":\"6\"},\"end\":26104,\"start\":26097},{\"attributes\":{\"n\":\"7\"},\"end\":31845,\"start\":31835},{\"attributes\":{\"n\":\"8.1.2\"},\"end\":34429,\"start\":34413},{\"attributes\":{\"n\":\"8.1.3\"},\"end\":34869,\"start\":34861},{\"attributes\":{\"n\":\"8.1.4\"},\"end\":35301,\"start\":35282},{\"attributes\":{\"n\":\"8.1.5\"},\"end\":35970,\"start\":35961},{\"attributes\":{\"n\":\"8.1.6\"},\"end\":36301,\"start\":36280},{\"attributes\":{\"n\":\"8.1.7\"},\"end\":36623,\"start\":36607},{\"attributes\":{\"n\":\"8.2\"},\"end\":37766,\"start\":37730},{\"attributes\":{\"n\":\"8.3\"},\"end\":40091,\"start\":40073},{\"attributes\":{\"n\":\"8.4\"},\"end\":40126,\"start\":40094},{\"attributes\":{\"n\":\"8.4.1\"},\"end\":40931,\"start\":40893},{\"end\":43409,\"start\":43399},{\"end\":43487,\"start\":43477},{\"end\":43630,\"start\":43610},{\"end\":44168,\"start\":44158},{\"end\":44219,\"start\":44210},{\"end\":45019,\"start\":45010}]", "table": "[{\"end\":45008,\"start\":44495},{\"end\":45902,\"start\":45112}]", "figure_caption": "[{\"end\":43397,\"start\":43312},{\"end\":43475,\"start\":43411},{\"end\":43608,\"start\":43489},{\"end\":44156,\"start\":43633},{\"end\":44208,\"start\":44170},{\"end\":44495,\"start\":44221},{\"end\":45112,\"start\":45021}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19829,\"start\":19821},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27682,\"start\":27674},{\"end\":29512,\"start\":29504},{\"end\":29968,\"start\":29960},{\"end\":30546,\"start\":30538},{\"end\":33595,\"start\":33587},{\"end\":33945,\"start\":33937},{\"end\":34226,\"start\":34218},{\"end\":39068,\"start\":39060},{\"end\":39610,\"start\":39602},{\"end\":40324,\"start\":40315},{\"end\":40457,\"start\":40449},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":40537,\"start\":40529}]", "bib_author_first_name": "[{\"end\":46978,\"start\":46973},{\"end\":46996,\"start\":46992},{\"end\":47003,\"start\":46997},{\"end\":47023,\"start\":47018},{\"end\":47422,\"start\":47416},{\"end\":47442,\"start\":47437},{\"end\":47455,\"start\":47451},{\"end\":47466,\"start\":47461},{\"end\":47484,\"start\":47478},{\"end\":47496,\"start\":47491},{\"end\":47510,\"start\":47507},{\"end\":47523,\"start\":47519},{\"end\":47910,\"start\":47904},{\"end\":47932,\"start\":47928},{\"end\":47954,\"start\":47944},{\"end\":48308,\"start\":48302},{\"end\":48319,\"start\":48316},{\"end\":48334,\"start\":48327},{\"end\":48356,\"start\":48350},{\"end\":48683,\"start\":48677},{\"end\":48712,\"start\":48704},{\"end\":48725,\"start\":48718},{\"end\":48738,\"start\":48733},{\"end\":49233,\"start\":49227},{\"end\":49235,\"start\":49234},{\"end\":49259,\"start\":49252},{\"end\":49276,\"start\":49269},{\"end\":49278,\"start\":49277},{\"end\":49699,\"start\":49696},{\"end\":49722,\"start\":49719},{\"end\":49736,\"start\":49730},{\"end\":49749,\"start\":49744},{\"end\":49763,\"start\":49758},{\"end\":50149,\"start\":50143},{\"end\":50164,\"start\":50159},{\"end\":50178,\"start\":50172},{\"end\":50192,\"start\":50188},{\"end\":50210,\"start\":50203},{\"end\":50790,\"start\":50782},{\"end\":50823,\"start\":50816},{\"end\":51370,\"start\":51364},{\"end\":51385,\"start\":51379},{\"end\":51402,\"start\":51393},{\"end\":51862,\"start\":51855},{\"end\":51881,\"start\":51875},{\"end\":51896,\"start\":51888},{\"end\":52204,\"start\":52199},{\"end\":52218,\"start\":52213},{\"end\":52221,\"start\":52219},{\"end\":52234,\"start\":52228},{\"end\":52247,\"start\":52243},{\"end\":52258,\"start\":52252},{\"end\":52823,\"start\":52817},{\"end\":52835,\"start\":52831},{\"end\":52851,\"start\":52845},{\"end\":52865,\"start\":52861},{\"end\":52878,\"start\":52874},{\"end\":53501,\"start\":53498},{\"end\":53510,\"start\":53507},{\"end\":53526,\"start\":53519},{\"end\":53998,\"start\":53993},{\"end\":54014,\"start\":54009},{\"end\":54026,\"start\":54021},{\"end\":54328,\"start\":54323},{\"end\":54344,\"start\":54339},{\"end\":54357,\"start\":54352},{\"end\":54636,\"start\":54631},{\"end\":54648,\"start\":54642},{\"end\":55002,\"start\":54996},{\"end\":55015,\"start\":55010},{\"end\":55032,\"start\":55026},{\"end\":55045,\"start\":55042},{\"end\":55055,\"start\":55050},{\"end\":55485,\"start\":55479},{\"end\":55495,\"start\":55494},{\"end\":55790,\"start\":55786},{\"end\":55810,\"start\":55806},{\"end\":55824,\"start\":55817},{\"end\":55838,\"start\":55833},{\"end\":55853,\"start\":55849},{\"end\":55866,\"start\":55861},{\"end\":55886,\"start\":55878},{\"end\":56230,\"start\":56224},{\"end\":56247,\"start\":56241},{\"end\":56260,\"start\":56254},{\"end\":56275,\"start\":56269},{\"end\":56752,\"start\":56746},{\"end\":56769,\"start\":56763},{\"end\":56782,\"start\":56776},{\"end\":56797,\"start\":56791},{\"end\":57150,\"start\":57144},{\"end\":57167,\"start\":57160},{\"end\":57191,\"start\":57176},{\"end\":57205,\"start\":57200},{\"end\":57498,\"start\":57490},{\"end\":57701,\"start\":57693},{\"end\":57715,\"start\":57710},{\"end\":57729,\"start\":57725},{\"end\":57997,\"start\":57991},{\"end\":58224,\"start\":58223},{\"end\":58239,\"start\":58232},{\"end\":58241,\"start\":58240},{\"end\":58255,\"start\":58251},{\"end\":58257,\"start\":58256},{\"end\":58274,\"start\":58268},{\"end\":58276,\"start\":58275},{\"end\":58290,\"start\":58287},{\"end\":58292,\"start\":58291},{\"end\":58308,\"start\":58303},{\"end\":58310,\"start\":58309},{\"end\":58678,\"start\":58671},{\"end\":58695,\"start\":58687},{\"end\":58707,\"start\":58700},{\"end\":58722,\"start\":58718},{\"end\":58729,\"start\":58723},{\"end\":58755,\"start\":58748},{\"end\":58757,\"start\":58756},{\"end\":58773,\"start\":58766},{\"end\":59295,\"start\":59292},{\"end\":59309,\"start\":59303},{\"end\":59559,\"start\":59553},{\"end\":59575,\"start\":59566},{\"end\":59760,\"start\":59753},{\"end\":59771,\"start\":59767},{\"end\":59790,\"start\":59783},{\"end\":59803,\"start\":59797},{\"end\":59818,\"start\":59812},{\"end\":60069,\"start\":60068},{\"end\":60344,\"start\":60338},{\"end\":60370,\"start\":60363},{\"end\":60600,\"start\":60595},{\"end\":60606,\"start\":60605},{\"end\":60623,\"start\":60618},{\"end\":60649,\"start\":60632},{\"end\":60663,\"start\":60658},{\"end\":60935,\"start\":60930},{\"end\":60944,\"start\":60940},{\"end\":60957,\"start\":60953},{\"end\":60969,\"start\":60966},{\"end\":60986,\"start\":60980},{\"end\":61003,\"start\":60995},{\"end\":61477,\"start\":61472},{\"end\":61486,\"start\":61482},{\"end\":61502,\"start\":61495},{\"end\":61517,\"start\":61508},{\"end\":61528,\"start\":61524},{\"end\":61540,\"start\":61537},{\"end\":62047,\"start\":62041},{\"end\":62057,\"start\":62052},{\"end\":62075,\"start\":62068},{\"end\":62375,\"start\":62371},{\"end\":62384,\"start\":62381},{\"end\":62794,\"start\":62790},{\"end\":62806,\"start\":62800},{\"end\":62817,\"start\":62812},{\"end\":62838,\"start\":62831},{\"end\":62850,\"start\":62845},{\"end\":63579,\"start\":63576},{\"end\":63590,\"start\":63585},{\"end\":63609,\"start\":63604},{\"end\":63623,\"start\":63619},{\"end\":63888,\"start\":63879},{\"end\":63900,\"start\":63895},{\"end\":63919,\"start\":63914},{\"end\":63932,\"start\":63928},{\"end\":63948,\"start\":63941},{\"end\":63965,\"start\":63961},{\"end\":63982,\"start\":63976},{\"end\":64273,\"start\":64270},{\"end\":64289,\"start\":64282},{\"end\":64309,\"start\":64300},{\"end\":64327,\"start\":64319},{\"end\":64654,\"start\":64647},{\"end\":64667,\"start\":64661},{\"end\":64679,\"start\":64673},{\"end\":65311,\"start\":65308},{\"end\":65329,\"start\":65320},{\"end\":65346,\"start\":65339},{\"end\":65542,\"start\":65537},{\"end\":65926,\"start\":65922},{\"end\":65943,\"start\":65936},{\"end\":65953,\"start\":65948},{\"end\":65966,\"start\":65961},{\"end\":65978,\"start\":65973},{\"end\":65991,\"start\":65987},{\"end\":66270,\"start\":66264},{\"end\":66283,\"start\":66279},{\"end\":66300,\"start\":66295},{\"end\":66633,\"start\":66627},{\"end\":66932,\"start\":66928},{\"end\":66949,\"start\":66943},{\"end\":66964,\"start\":66958},{\"end\":66980,\"start\":66973},{\"end\":66996,\"start\":66989},{\"end\":67384,\"start\":67372},{\"end\":67409,\"start\":67402},{\"end\":67426,\"start\":67418},{\"end\":67443,\"start\":67436},{\"end\":67458,\"start\":67451},{\"end\":67470,\"start\":67464},{\"end\":67491,\"start\":67484},{\"end\":67503,\"start\":67497},{\"end\":67957,\"start\":67947},{\"end\":67981,\"start\":67975},{\"end\":67996,\"start\":67991},{\"end\":68011,\"start\":68005},{\"end\":68412,\"start\":68406},{\"end\":68436,\"start\":68426},{\"end\":68450,\"start\":68446},{\"end\":68464,\"start\":68457},{\"end\":68480,\"start\":68474},{\"end\":68494,\"start\":68489},{\"end\":68512,\"start\":68506},{\"end\":68969,\"start\":68962},{\"end\":68981,\"start\":68976},{\"end\":68999,\"start\":68995},{\"end\":69011,\"start\":69005},{\"end\":69729,\"start\":69723},{\"end\":69739,\"start\":69735},{\"end\":70224,\"start\":70219},{\"end\":70235,\"start\":70231},{\"end\":70246,\"start\":70240},{\"end\":70263,\"start\":70256},{\"end\":70538,\"start\":70537},{\"end\":70552,\"start\":70548},{\"end\":70554,\"start\":70553},{\"end\":70574,\"start\":70563},{\"end\":70585,\"start\":70581},{\"end\":70993,\"start\":70992},{\"end\":71246,\"start\":71239},{\"end\":71256,\"start\":71251},{\"end\":71277,\"start\":71271},{\"end\":71291,\"start\":71285},{\"end\":71304,\"start\":71299},{\"end\":71797,\"start\":71791},{\"end\":71810,\"start\":71806},{\"end\":72098,\"start\":72091}]", "bib_author_last_name": "[{\"end\":46990,\"start\":46979},{\"end\":47016,\"start\":47004},{\"end\":47029,\"start\":47024},{\"end\":47435,\"start\":47423},{\"end\":47449,\"start\":47443},{\"end\":47459,\"start\":47456},{\"end\":47476,\"start\":47467},{\"end\":47489,\"start\":47485},{\"end\":47505,\"start\":47497},{\"end\":47517,\"start\":47511},{\"end\":47529,\"start\":47524},{\"end\":47926,\"start\":47911},{\"end\":47942,\"start\":47933},{\"end\":47965,\"start\":47955},{\"end\":48314,\"start\":48309},{\"end\":48325,\"start\":48320},{\"end\":48348,\"start\":48335},{\"end\":48361,\"start\":48357},{\"end\":48698,\"start\":48684},{\"end\":48702,\"start\":48700},{\"end\":48716,\"start\":48713},{\"end\":48731,\"start\":48726},{\"end\":48743,\"start\":48739},{\"end\":48754,\"start\":48745},{\"end\":49250,\"start\":49236},{\"end\":49267,\"start\":49260},{\"end\":49286,\"start\":49279},{\"end\":49293,\"start\":49288},{\"end\":49717,\"start\":49700},{\"end\":49728,\"start\":49723},{\"end\":49742,\"start\":49737},{\"end\":49756,\"start\":49750},{\"end\":49768,\"start\":49764},{\"end\":49776,\"start\":49770},{\"end\":50157,\"start\":50150},{\"end\":50170,\"start\":50165},{\"end\":50186,\"start\":50179},{\"end\":50201,\"start\":50193},{\"end\":50217,\"start\":50211},{\"end\":50814,\"start\":50791},{\"end\":50827,\"start\":50824},{\"end\":51377,\"start\":51371},{\"end\":51391,\"start\":51386},{\"end\":51409,\"start\":51403},{\"end\":51873,\"start\":51863},{\"end\":51886,\"start\":51882},{\"end\":51908,\"start\":51897},{\"end\":52211,\"start\":52205},{\"end\":52226,\"start\":52222},{\"end\":52241,\"start\":52235},{\"end\":52250,\"start\":52248},{\"end\":52266,\"start\":52259},{\"end\":52829,\"start\":52824},{\"end\":52843,\"start\":52836},{\"end\":52859,\"start\":52852},{\"end\":52872,\"start\":52866},{\"end\":52886,\"start\":52879},{\"end\":53505,\"start\":53502},{\"end\":53517,\"start\":53511},{\"end\":53533,\"start\":53527},{\"end\":54007,\"start\":53999},{\"end\":54019,\"start\":54015},{\"end\":54032,\"start\":54027},{\"end\":54337,\"start\":54329},{\"end\":54350,\"start\":54345},{\"end\":54364,\"start\":54358},{\"end\":54640,\"start\":54637},{\"end\":54659,\"start\":54649},{\"end\":55008,\"start\":55003},{\"end\":55024,\"start\":55016},{\"end\":55040,\"start\":55033},{\"end\":55048,\"start\":55046},{\"end\":55061,\"start\":55056},{\"end\":55492,\"start\":55486},{\"end\":55500,\"start\":55496},{\"end\":55511,\"start\":55502},{\"end\":55804,\"start\":55791},{\"end\":55815,\"start\":55811},{\"end\":55831,\"start\":55825},{\"end\":55847,\"start\":55839},{\"end\":55859,\"start\":55854},{\"end\":55876,\"start\":55867},{\"end\":55893,\"start\":55887},{\"end\":56239,\"start\":56231},{\"end\":56252,\"start\":56248},{\"end\":56267,\"start\":56261},{\"end\":56282,\"start\":56276},{\"end\":56761,\"start\":56753},{\"end\":56774,\"start\":56770},{\"end\":56789,\"start\":56783},{\"end\":56804,\"start\":56798},{\"end\":57158,\"start\":57151},{\"end\":57174,\"start\":57168},{\"end\":57198,\"start\":57192},{\"end\":57212,\"start\":57206},{\"end\":57502,\"start\":57499},{\"end\":57708,\"start\":57702},{\"end\":57723,\"start\":57716},{\"end\":57734,\"start\":57730},{\"end\":58004,\"start\":57998},{\"end\":58230,\"start\":58225},{\"end\":58249,\"start\":58242},{\"end\":58266,\"start\":58258},{\"end\":58285,\"start\":58277},{\"end\":58301,\"start\":58293},{\"end\":58317,\"start\":58311},{\"end\":58329,\"start\":58319},{\"end\":58685,\"start\":58679},{\"end\":58698,\"start\":58696},{\"end\":58716,\"start\":58708},{\"end\":58746,\"start\":58730},{\"end\":58764,\"start\":58758},{\"end\":58777,\"start\":58774},{\"end\":59301,\"start\":59296},{\"end\":59312,\"start\":59310},{\"end\":59564,\"start\":59560},{\"end\":59765,\"start\":59761},{\"end\":59781,\"start\":59772},{\"end\":59795,\"start\":59791},{\"end\":59810,\"start\":59804},{\"end\":59825,\"start\":59819},{\"end\":60074,\"start\":60070},{\"end\":60082,\"start\":60076},{\"end\":60361,\"start\":60345},{\"end\":60376,\"start\":60371},{\"end\":60383,\"start\":60378},{\"end\":60603,\"start\":60601},{\"end\":60616,\"start\":60607},{\"end\":60630,\"start\":60624},{\"end\":60656,\"start\":60650},{\"end\":60671,\"start\":60664},{\"end\":60679,\"start\":60673},{\"end\":60938,\"start\":60936},{\"end\":60951,\"start\":60945},{\"end\":60964,\"start\":60958},{\"end\":60978,\"start\":60970},{\"end\":60993,\"start\":60987},{\"end\":61007,\"start\":61004},{\"end\":61480,\"start\":61478},{\"end\":61493,\"start\":61487},{\"end\":61506,\"start\":61503},{\"end\":61522,\"start\":61518},{\"end\":61535,\"start\":61529},{\"end\":61549,\"start\":61541},{\"end\":62050,\"start\":62048},{\"end\":62066,\"start\":62058},{\"end\":62084,\"start\":62076},{\"end\":62379,\"start\":62376},{\"end\":62389,\"start\":62385},{\"end\":62798,\"start\":62795},{\"end\":62810,\"start\":62807},{\"end\":62829,\"start\":62818},{\"end\":62843,\"start\":62839},{\"end\":62855,\"start\":62851},{\"end\":63583,\"start\":63580},{\"end\":63602,\"start\":63591},{\"end\":63617,\"start\":63610},{\"end\":63633,\"start\":63624},{\"end\":63893,\"start\":63889},{\"end\":63912,\"start\":63901},{\"end\":63926,\"start\":63920},{\"end\":63939,\"start\":63933},{\"end\":63959,\"start\":63949},{\"end\":63974,\"start\":63966},{\"end\":63993,\"start\":63983},{\"end\":64280,\"start\":64274},{\"end\":64298,\"start\":64290},{\"end\":64317,\"start\":64310},{\"end\":64335,\"start\":64328},{\"end\":64659,\"start\":64655},{\"end\":64671,\"start\":64668},{\"end\":64683,\"start\":64680},{\"end\":65318,\"start\":65312},{\"end\":65337,\"start\":65330},{\"end\":65352,\"start\":65347},{\"end\":65549,\"start\":65543},{\"end\":65766,\"start\":65750},{\"end\":65776,\"start\":65768},{\"end\":65934,\"start\":65927},{\"end\":65946,\"start\":65944},{\"end\":65959,\"start\":65954},{\"end\":65971,\"start\":65967},{\"end\":65985,\"start\":65979},{\"end\":66001,\"start\":65992},{\"end\":66277,\"start\":66271},{\"end\":66293,\"start\":66284},{\"end\":66312,\"start\":66301},{\"end\":66644,\"start\":66634},{\"end\":66941,\"start\":66933},{\"end\":66956,\"start\":66950},{\"end\":66971,\"start\":66965},{\"end\":66987,\"start\":66981},{\"end\":67003,\"start\":66997},{\"end\":67400,\"start\":67385},{\"end\":67416,\"start\":67410},{\"end\":67434,\"start\":67427},{\"end\":67449,\"start\":67444},{\"end\":67462,\"start\":67459},{\"end\":67482,\"start\":67471},{\"end\":67495,\"start\":67492},{\"end\":67510,\"start\":67504},{\"end\":67519,\"start\":67512},{\"end\":67973,\"start\":67958},{\"end\":67989,\"start\":67982},{\"end\":68003,\"start\":67997},{\"end\":68021,\"start\":68012},{\"end\":68029,\"start\":68023},{\"end\":68424,\"start\":68413},{\"end\":68444,\"start\":68437},{\"end\":68455,\"start\":68451},{\"end\":68472,\"start\":68465},{\"end\":68487,\"start\":68481},{\"end\":68504,\"start\":68495},{\"end\":68519,\"start\":68513},{\"end\":68974,\"start\":68970},{\"end\":68993,\"start\":68982},{\"end\":69003,\"start\":69000},{\"end\":69015,\"start\":69012},{\"end\":69733,\"start\":69730},{\"end\":69742,\"start\":69740},{\"end\":70229,\"start\":70225},{\"end\":70238,\"start\":70236},{\"end\":70254,\"start\":70247},{\"end\":70268,\"start\":70264},{\"end\":70546,\"start\":70539},{\"end\":70561,\"start\":70555},{\"end\":70579,\"start\":70575},{\"end\":70589,\"start\":70586},{\"end\":70595,\"start\":70591},{\"end\":71000,\"start\":70994},{\"end\":71009,\"start\":71002},{\"end\":71249,\"start\":71247},{\"end\":71269,\"start\":71257},{\"end\":71283,\"start\":71278},{\"end\":71297,\"start\":71292},{\"end\":71310,\"start\":71305},{\"end\":71804,\"start\":71798},{\"end\":71820,\"start\":71811},{\"end\":72106,\"start\":72099}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1806.06920\",\"id\":\"b0\"},\"end\":47341,\"start\":46973},{\"attributes\":{\"id\":\"b1\"},\"end\":47846,\"start\":47343},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3621512},\"end\":48233,\"start\":47848},{\"attributes\":{\"doi\":\"arXiv:1902.05605\",\"id\":\"b3\"},\"end\":48563,\"start\":48235},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":145680021},\"end\":49043,\"start\":48565},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":145162829},\"end\":49642,\"start\":49045},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":4787508},\"end\":50045,\"start\":49644},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":28971531},\"end\":50665,\"start\":50047},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":3101865},\"end\":51337,\"start\":50667},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":10513082},\"end\":51804,\"start\":51339},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":3634264},\"end\":52135,\"start\":51806},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":11811668},\"end\":52697,\"start\":52137},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":2493033},\"end\":53435,\"start\":52699},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2374643},\"end\":53926,\"start\":53437},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":3544558},\"end\":54261,\"start\":53928},{\"attributes\":{\"doi\":\"arXiv:1812.02900\",\"id\":\"b15\"},\"end\":54543,\"start\":54263},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":160705},\"end\":54897,\"start\":54545},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":15855483},\"end\":55400,\"start\":54899},{\"attributes\":{\"doi\":\"arXiv:1901.09455\",\"id\":\"b18\"},\"end\":55694,\"start\":55402},{\"attributes\":{\"doi\":\"arXiv:1906.09308\",\"id\":\"b19\"},\"end\":56166,\"start\":55696},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":11227891},\"end\":56645,\"start\":56168},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":28202810},\"end\":57086,\"start\":56647},{\"attributes\":{\"doi\":\"arXiv:1901.05415\",\"id\":\"b22\"},\"end\":57430,\"start\":57088},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":39424408},\"end\":57645,\"start\":57432},{\"attributes\":{\"doi\":\"arXiv:1503.02531\",\"id\":\"b24\"},\"end\":57897,\"start\":57647},{\"attributes\":{\"id\":\"b25\"},\"end\":58149,\"start\":57899},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":4949817},\"end\":58581,\"start\":58151},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":15636415},\"end\":59220,\"start\":58583},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":5806691},\"end\":59524,\"start\":59222},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":5538688},\"end\":59685,\"start\":59526},{\"attributes\":{\"doi\":\"arXiv:1702.01182\",\"id\":\"b30\"},\"end\":60039,\"start\":59687},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":14540458},\"end\":60280,\"start\":60041},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":7522435},\"end\":60551,\"start\":60282},{\"attributes\":{\"doi\":\"arXiv:1611.09823\",\"id\":\"b33\"},\"end\":60875,\"start\":60553},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":3147007},\"end\":61417,\"start\":60877},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":98180},\"end\":61959,\"start\":61419},{\"attributes\":{\"doi\":\"arXiv:1812.03509\",\"id\":\"b36\"},\"end\":62283,\"start\":61961},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":1016820},\"end\":62681,\"start\":62285},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":4938015},\"end\":63511,\"start\":62683},{\"attributes\":{\"doi\":\"arXiv:1904.08473\",\"id\":\"b39\"},\"end\":63829,\"start\":63513},{\"attributes\":{\"doi\":\"arXiv:1312.5602\",\"id\":\"b40\"},\"end\":64229,\"start\":63831},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":5865729},\"end\":64574,\"start\":64231},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":4829361},\"end\":65274,\"start\":64576},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":2984847},\"end\":65482,\"start\":65276},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":1153355},\"end\":65746,\"start\":65484},{\"attributes\":{\"id\":\"b45\"},\"end\":65867,\"start\":65748},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":160025533},\"end\":66179,\"start\":65869},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":2168740},\"end\":66521,\"start\":66181},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":6921329},\"end\":66892,\"start\":66523},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":16046818},\"end\":67368,\"start\":66894},{\"attributes\":{\"doi\":\"arXiv:1709.02349\",\"id\":\"b50\"},\"end\":67855,\"start\":67370},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":6126582},\"end\":68325,\"start\":67857},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":14857825},\"end\":68841,\"start\":68327},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":44115395},\"end\":69675,\"start\":68843},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":13754513},\"end\":70125,\"start\":69677},{\"attributes\":{\"doi\":\"arXiv:1906.08487\",\"id\":\"b55\"},\"end\":70485,\"start\":70127},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":6466830},\"end\":70962,\"start\":70487},{\"attributes\":{\"id\":\"b57\"},\"end\":71139,\"start\":70964},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":28821374},\"end\":71717,\"start\":71141},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":9311215},\"end\":72045,\"start\":71719},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":393501},\"end\":72310,\"start\":72047}]", "bib_title": "[{\"end\":47414,\"start\":47343},{\"end\":47902,\"start\":47848},{\"end\":48675,\"start\":48565},{\"end\":49225,\"start\":49045},{\"end\":49694,\"start\":49644},{\"end\":50141,\"start\":50047},{\"end\":50780,\"start\":50667},{\"end\":51362,\"start\":51339},{\"end\":51853,\"start\":51806},{\"end\":52197,\"start\":52137},{\"end\":52815,\"start\":52699},{\"end\":53496,\"start\":53437},{\"end\":53991,\"start\":53928},{\"end\":54629,\"start\":54545},{\"end\":54994,\"start\":54899},{\"end\":56222,\"start\":56168},{\"end\":56744,\"start\":56647},{\"end\":57488,\"start\":57432},{\"end\":57989,\"start\":57899},{\"end\":58221,\"start\":58151},{\"end\":58669,\"start\":58583},{\"end\":59290,\"start\":59222},{\"end\":59551,\"start\":59526},{\"end\":60066,\"start\":60041},{\"end\":60336,\"start\":60282},{\"end\":60928,\"start\":60877},{\"end\":61470,\"start\":61419},{\"end\":62369,\"start\":62285},{\"end\":62788,\"start\":62683},{\"end\":64268,\"start\":64231},{\"end\":64645,\"start\":64576},{\"end\":65306,\"start\":65276},{\"end\":65535,\"start\":65484},{\"end\":65920,\"start\":65869},{\"end\":66262,\"start\":66181},{\"end\":66625,\"start\":66523},{\"end\":66926,\"start\":66894},{\"end\":67945,\"start\":67857},{\"end\":68404,\"start\":68327},{\"end\":68960,\"start\":68843},{\"end\":69721,\"start\":69677},{\"end\":70535,\"start\":70487},{\"end\":71237,\"start\":71141},{\"end\":71789,\"start\":71719},{\"end\":72089,\"start\":72047}]", "bib_author": "[{\"end\":46992,\"start\":46973},{\"end\":47018,\"start\":46992},{\"end\":47031,\"start\":47018},{\"end\":47437,\"start\":47416},{\"end\":47451,\"start\":47437},{\"end\":47461,\"start\":47451},{\"end\":47478,\"start\":47461},{\"end\":47491,\"start\":47478},{\"end\":47507,\"start\":47491},{\"end\":47519,\"start\":47507},{\"end\":47531,\"start\":47519},{\"end\":47928,\"start\":47904},{\"end\":47944,\"start\":47928},{\"end\":47967,\"start\":47944},{\"end\":48316,\"start\":48302},{\"end\":48327,\"start\":48316},{\"end\":48350,\"start\":48327},{\"end\":48363,\"start\":48350},{\"end\":48700,\"start\":48677},{\"end\":48704,\"start\":48700},{\"end\":48718,\"start\":48704},{\"end\":48733,\"start\":48718},{\"end\":48745,\"start\":48733},{\"end\":48756,\"start\":48745},{\"end\":49252,\"start\":49227},{\"end\":49269,\"start\":49252},{\"end\":49288,\"start\":49269},{\"end\":49295,\"start\":49288},{\"end\":49719,\"start\":49696},{\"end\":49730,\"start\":49719},{\"end\":49744,\"start\":49730},{\"end\":49758,\"start\":49744},{\"end\":49770,\"start\":49758},{\"end\":49778,\"start\":49770},{\"end\":50159,\"start\":50143},{\"end\":50172,\"start\":50159},{\"end\":50188,\"start\":50172},{\"end\":50203,\"start\":50188},{\"end\":50219,\"start\":50203},{\"end\":50816,\"start\":50782},{\"end\":50829,\"start\":50816},{\"end\":51379,\"start\":51364},{\"end\":51393,\"start\":51379},{\"end\":51411,\"start\":51393},{\"end\":51875,\"start\":51855},{\"end\":51888,\"start\":51875},{\"end\":51910,\"start\":51888},{\"end\":52213,\"start\":52199},{\"end\":52228,\"start\":52213},{\"end\":52243,\"start\":52228},{\"end\":52252,\"start\":52243},{\"end\":52268,\"start\":52252},{\"end\":52831,\"start\":52817},{\"end\":52845,\"start\":52831},{\"end\":52861,\"start\":52845},{\"end\":52874,\"start\":52861},{\"end\":52888,\"start\":52874},{\"end\":53507,\"start\":53498},{\"end\":53519,\"start\":53507},{\"end\":53535,\"start\":53519},{\"end\":54009,\"start\":53993},{\"end\":54021,\"start\":54009},{\"end\":54034,\"start\":54021},{\"end\":54339,\"start\":54323},{\"end\":54352,\"start\":54339},{\"end\":54366,\"start\":54352},{\"end\":54642,\"start\":54631},{\"end\":54661,\"start\":54642},{\"end\":55010,\"start\":54996},{\"end\":55026,\"start\":55010},{\"end\":55042,\"start\":55026},{\"end\":55050,\"start\":55042},{\"end\":55063,\"start\":55050},{\"end\":55494,\"start\":55479},{\"end\":55502,\"start\":55494},{\"end\":55513,\"start\":55502},{\"end\":55806,\"start\":55786},{\"end\":55817,\"start\":55806},{\"end\":55833,\"start\":55817},{\"end\":55849,\"start\":55833},{\"end\":55861,\"start\":55849},{\"end\":55878,\"start\":55861},{\"end\":55895,\"start\":55878},{\"end\":56241,\"start\":56224},{\"end\":56254,\"start\":56241},{\"end\":56269,\"start\":56254},{\"end\":56284,\"start\":56269},{\"end\":56763,\"start\":56746},{\"end\":56776,\"start\":56763},{\"end\":56791,\"start\":56776},{\"end\":56806,\"start\":56791},{\"end\":57160,\"start\":57144},{\"end\":57176,\"start\":57160},{\"end\":57200,\"start\":57176},{\"end\":57214,\"start\":57200},{\"end\":57504,\"start\":57490},{\"end\":57710,\"start\":57693},{\"end\":57725,\"start\":57710},{\"end\":57736,\"start\":57725},{\"end\":58006,\"start\":57991},{\"end\":58232,\"start\":58223},{\"end\":58251,\"start\":58232},{\"end\":58268,\"start\":58251},{\"end\":58287,\"start\":58268},{\"end\":58303,\"start\":58287},{\"end\":58319,\"start\":58303},{\"end\":58331,\"start\":58319},{\"end\":58687,\"start\":58671},{\"end\":58700,\"start\":58687},{\"end\":58718,\"start\":58700},{\"end\":58748,\"start\":58718},{\"end\":58766,\"start\":58748},{\"end\":58779,\"start\":58766},{\"end\":59303,\"start\":59292},{\"end\":59314,\"start\":59303},{\"end\":59566,\"start\":59553},{\"end\":59578,\"start\":59566},{\"end\":59767,\"start\":59753},{\"end\":59783,\"start\":59767},{\"end\":59797,\"start\":59783},{\"end\":59812,\"start\":59797},{\"end\":59827,\"start\":59812},{\"end\":60076,\"start\":60068},{\"end\":60084,\"start\":60076},{\"end\":60363,\"start\":60338},{\"end\":60378,\"start\":60363},{\"end\":60385,\"start\":60378},{\"end\":60605,\"start\":60595},{\"end\":60618,\"start\":60605},{\"end\":60632,\"start\":60618},{\"end\":60658,\"start\":60632},{\"end\":60673,\"start\":60658},{\"end\":60681,\"start\":60673},{\"end\":60940,\"start\":60930},{\"end\":60953,\"start\":60940},{\"end\":60966,\"start\":60953},{\"end\":60980,\"start\":60966},{\"end\":60995,\"start\":60980},{\"end\":61009,\"start\":60995},{\"end\":61482,\"start\":61472},{\"end\":61495,\"start\":61482},{\"end\":61508,\"start\":61495},{\"end\":61524,\"start\":61508},{\"end\":61537,\"start\":61524},{\"end\":61551,\"start\":61537},{\"end\":62052,\"start\":62041},{\"end\":62068,\"start\":62052},{\"end\":62086,\"start\":62068},{\"end\":62381,\"start\":62371},{\"end\":62391,\"start\":62381},{\"end\":62800,\"start\":62790},{\"end\":62812,\"start\":62800},{\"end\":62831,\"start\":62812},{\"end\":62845,\"start\":62831},{\"end\":62857,\"start\":62845},{\"end\":63585,\"start\":63576},{\"end\":63604,\"start\":63585},{\"end\":63619,\"start\":63604},{\"end\":63635,\"start\":63619},{\"end\":63895,\"start\":63879},{\"end\":63914,\"start\":63895},{\"end\":63928,\"start\":63914},{\"end\":63941,\"start\":63928},{\"end\":63961,\"start\":63941},{\"end\":63976,\"start\":63961},{\"end\":63995,\"start\":63976},{\"end\":64282,\"start\":64270},{\"end\":64300,\"start\":64282},{\"end\":64319,\"start\":64300},{\"end\":64337,\"start\":64319},{\"end\":64661,\"start\":64647},{\"end\":64673,\"start\":64661},{\"end\":64685,\"start\":64673},{\"end\":65320,\"start\":65308},{\"end\":65339,\"start\":65320},{\"end\":65354,\"start\":65339},{\"end\":65551,\"start\":65537},{\"end\":65768,\"start\":65750},{\"end\":65778,\"start\":65768},{\"end\":65936,\"start\":65922},{\"end\":65948,\"start\":65936},{\"end\":65961,\"start\":65948},{\"end\":65973,\"start\":65961},{\"end\":65987,\"start\":65973},{\"end\":66003,\"start\":65987},{\"end\":66279,\"start\":66264},{\"end\":66295,\"start\":66279},{\"end\":66314,\"start\":66295},{\"end\":66646,\"start\":66627},{\"end\":66943,\"start\":66928},{\"end\":66958,\"start\":66943},{\"end\":66973,\"start\":66958},{\"end\":66989,\"start\":66973},{\"end\":67005,\"start\":66989},{\"end\":67402,\"start\":67372},{\"end\":67418,\"start\":67402},{\"end\":67436,\"start\":67418},{\"end\":67451,\"start\":67436},{\"end\":67464,\"start\":67451},{\"end\":67484,\"start\":67464},{\"end\":67497,\"start\":67484},{\"end\":67512,\"start\":67497},{\"end\":67521,\"start\":67512},{\"end\":67975,\"start\":67947},{\"end\":67991,\"start\":67975},{\"end\":68005,\"start\":67991},{\"end\":68023,\"start\":68005},{\"end\":68031,\"start\":68023},{\"end\":68426,\"start\":68406},{\"end\":68446,\"start\":68426},{\"end\":68457,\"start\":68446},{\"end\":68474,\"start\":68457},{\"end\":68489,\"start\":68474},{\"end\":68506,\"start\":68489},{\"end\":68521,\"start\":68506},{\"end\":68976,\"start\":68962},{\"end\":68995,\"start\":68976},{\"end\":69005,\"start\":68995},{\"end\":69017,\"start\":69005},{\"end\":69735,\"start\":69723},{\"end\":69744,\"start\":69735},{\"end\":70231,\"start\":70219},{\"end\":70240,\"start\":70231},{\"end\":70256,\"start\":70240},{\"end\":70270,\"start\":70256},{\"end\":70548,\"start\":70537},{\"end\":70563,\"start\":70548},{\"end\":70581,\"start\":70563},{\"end\":70591,\"start\":70581},{\"end\":70597,\"start\":70591},{\"end\":71002,\"start\":70992},{\"end\":71011,\"start\":71002},{\"end\":71251,\"start\":71239},{\"end\":71271,\"start\":71251},{\"end\":71285,\"start\":71271},{\"end\":71299,\"start\":71285},{\"end\":71312,\"start\":71299},{\"end\":71806,\"start\":71791},{\"end\":71822,\"start\":71806},{\"end\":72108,\"start\":72091}]", "bib_venue": "[{\"end\":50378,\"start\":50307},{\"end\":50982,\"start\":50914},{\"end\":51588,\"start\":51508},{\"end\":52443,\"start\":52364},{\"end\":53692,\"start\":53622},{\"end\":56407,\"start\":56354},{\"end\":58902,\"start\":58849},{\"end\":61168,\"start\":61097},{\"end\":61710,\"start\":61639},{\"end\":63128,\"start\":63001},{\"end\":64956,\"start\":64829},{\"end\":67148,\"start\":67085},{\"end\":69288,\"start\":69161},{\"end\":69905,\"start\":69833},{\"end\":70740,\"start\":70677},{\"end\":71443,\"start\":71386},{\"end\":47137,\"start\":47047},{\"end\":47580,\"start\":47531},{\"end\":48022,\"start\":47967},{\"end\":48300,\"start\":48235},{\"end\":48790,\"start\":48756},{\"end\":49327,\"start\":49295},{\"end\":49827,\"start\":49778},{\"end\":50305,\"start\":50219},{\"end\":50912,\"start\":50829},{\"end\":51506,\"start\":51411},{\"end\":51954,\"start\":51910},{\"end\":52362,\"start\":52268},{\"end\":53017,\"start\":52888},{\"end\":53620,\"start\":53535},{\"end\":54078,\"start\":54034},{\"end\":54321,\"start\":54263},{\"end\":54705,\"start\":54661},{\"end\":55129,\"start\":55063},{\"end\":55477,\"start\":55402},{\"end\":55784,\"start\":55696},{\"end\":56352,\"start\":56284},{\"end\":56850,\"start\":56806},{\"end\":57142,\"start\":57088},{\"end\":57525,\"start\":57504},{\"end\":57691,\"start\":57647},{\"end\":58018,\"start\":58006},{\"end\":58352,\"start\":58331},{\"end\":58847,\"start\":58779},{\"end\":59358,\"start\":59314},{\"end\":59583,\"start\":59578},{\"end\":59751,\"start\":59687},{\"end\":60140,\"start\":60084},{\"end\":60401,\"start\":60385},{\"end\":60593,\"start\":60553},{\"end\":61095,\"start\":61009},{\"end\":61637,\"start\":61551},{\"end\":62039,\"start\":61961},{\"end\":62463,\"start\":62391},{\"end\":62999,\"start\":62857},{\"end\":63574,\"start\":63513},{\"end\":63877,\"start\":63831},{\"end\":64386,\"start\":64337},{\"end\":64827,\"start\":64685},{\"end\":65358,\"start\":65354},{\"end\":65605,\"start\":65551},{\"end\":65796,\"start\":65778},{\"end\":66014,\"start\":66003},{\"end\":66343,\"start\":66314},{\"end\":66685,\"start\":66646},{\"end\":67083,\"start\":67005},{\"end\":68083,\"start\":68031},{\"end\":68576,\"start\":68521},{\"end\":69159,\"start\":69017},{\"end\":69831,\"start\":69744},{\"end\":70217,\"start\":70127},{\"end\":70675,\"start\":70597},{\"end\":70990,\"start\":70964},{\"end\":71384,\"start\":71312},{\"end\":71866,\"start\":71822},{\"end\":72164,\"start\":72108}]"}}}, "year": 2023, "month": 12, "day": 17}