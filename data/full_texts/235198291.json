{"id": 235198291, "updated": "2022-10-13 14:16:21.497", "metadata": {"title": "Recurrent Multi-Frame Deraining: Combining Physics Guidance and Adversarial Learning", "authors": "[{\"first\":\"Wenhan\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Robby\",\"last\":\"Tan\",\"middle\":[\"T.\"]},{\"first\":\"Jiashi\",\"last\":\"Feng\",\"middle\":[]},{\"first\":\"Shiqi\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Bin\",\"last\":\"Cheng\",\"middle\":[]},{\"first\":\"Jiaying\",\"last\":\"Liu\",\"middle\":[]}]", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Existing video rain removal methods mainly focus on rain streak removal and are solely trained based on the synthetic data, which neglect more complex degradation factors, e.g., rain accumulation, and the prior knowledge in real rain data. Thus, in this paper, we build a more comprehensive rain model with several degradation factors and construct a novel two-stage video rain removal method that combines the power of synthetic videos and real data. Specifically, a novel two-stage progressive network is proposed: recovery guided by a physics model, and further restoration by adversarial learning. The first stage performs an inverse recovery process guided by our proposed rain model. An initially estimated background frame is obtained based on the input rain frame. The second stage employs adversarial learning to refine the result, i.e., recovering the overall color and illumination distributions of the frame, the background details that are failed to be recovered in the first stage, and removing the artifacts generated in the first stage. Furthermore, we also introduce a more comprehensive rain model that includes degradation factors, e.g., occlusion and rain accumulation, which appear in real scenes yet ignored by existing methods. This model, which generates more realistic rain images, will train and evaluate our models better. Extensive evaluations on synthetic and real videos show the effectiveness of our method in comparisons to the state-of-the-art methods. Our datasets, results and code are available at: https://github.com/flyywh/Recurrent-Multi-Frame-Deraining.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": "34029186", "pubmedcentral": null, "dblp": "journals/pami/YangTFWCL22", "doi": "10.1109/tpami.2021.3083076"}}, "content": {"source": {"pdf_hash": "033ca5136cc457d2fee6bfd2ebea9159c4d166c4", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "019b51a8d70b62d2b1802e4ec272e401f1f2104a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/033ca5136cc457d2fee6bfd2ebea9159c4d166c4.txt", "contents": "\nRecurrent Multi-Frame Deraining: Combining Physics Guidance and Adversarial Learning\n\n\nMember, IEEEWenhan Yang \nMember, IEEERobby T Tan \nMember, IEEEJiashi Feng \nMember, IEEEShiqi Wang \nBin Cheng \nSenior Member, IEEEJiaying Liu \nRecurrent Multi-Frame Deraining: Combining Physics Guidance and Adversarial Learning\n10.1109/TPAMI.2021.3083076Index Terms-Multi-framevideo rain removalphysics recovery guidanceadversarial learning\nExisting video rain removal methods mainly focus on rain streak removal and are solely trained based on the synthetic data, which neglect more complex degradation factors, e.g., rain accumulation, and the prior knowledge in real rain data. Thus, in this paper, we build a more comprehensive rain model with several degradation factors and construct a novel two-stage video rain removal method that combines the power of synthetic videos and real data. Specifically, a novel two-stage progressive network is proposed: recovery guided by a physics model, and further restoration by adversarial learning. The first stage performs an inverse recovery process guided by our proposed rain model. An initially estimated background frame is obtained based on the input rain frame. The second stage employs adversarial learning to refine the result, i.e., recovering the overall color and illumination distributions of the frame, the background details that are failed to be recovered in the first stage, and removing the artifacts generated in the first stage. Furthermore, we also introduce a more comprehensive rain model that includes degradation factors, e.g., occlusion and rain accumulation, which appear in real scenes yet ignored by existing methods. This model, which generates more realistic rain images, will train and evaluate our models better. Extensive evaluations on synthetic and real videos show the effectiveness of our method in comparisons to the state-of-the-art methods. Our datasets, results and code are available at: https://github.com/flyywh/Recurrent-Multi-Frame-Deraining.\n\n\u00c7\n\n\nINTRODUCTION\n\nR AIN degrades videos, causing outdoor computer vision systems to be erroneous, as most of them assume clear input videos. There are a few factors of rain degradation. Rain streaks lead to intensity changes in image content, obscuring the background and blurring the scene. Rain streaks can also completely occlude some background signals, where no background signals go through, a phenomenon we call rain occlusion. Rain accumulation (also known as rain veiling effect), where individual rain streaks and water particles accumulate forming visual effects similar to mist or fog, impair the background contrast, reducing the distant scenes' visibility significantly. When rainfall intensity in some period of time changes rapidly, rain accumulation can fluctuate over the period of time, which is visually like a flowing transparent veil covering the background. We call this phenomenon accumulation flow.\n\nMany methods have been proposed to derain either images or videos. Single-image-based methods, e.g., [15], [20], [28], [34] employ some techniques, such as a frequency-domain representation [20], sparse representation [28], Gaussian mixture model [25] and deep networks [8], [42]. Video-based methods, e.g., [1], [2], [11], [46] make full use of both temporal and spatial information. Garg and Nayar [11] utilize the physics properties of rain, e.g., chromatic and direction. Kim et al. and Jiang et al. [19], [21] further exploit temporal dynamics, i.e., background motion's continuity, rain streaks' random occurrence, and motion cues.\n\nRecently, deep-learning based methods have been proposed to tackle the video deraining problem. In [4], a rain image is first segmented into superpixels, then a consistency constraint is imposed on these aligned superpixels. Li et al. [23] propose a multiscale convolutional sparse coding-based video rain streak removal method. Liu et al. [26], [27] build a recurrent network to jointly integrate the tasks of rain degradation detection, background reconstruction and rain removal. In [18], [19], a tensor decomposition based deraining methods is proposed to fully consider the discriminative characteristics of clean backgrounds and rain streaks in the gradient domain. While these video deraining methods can be effective in some cases, they all are designed to handle only rain streak removal. Little attention is given to other factors of rain degradation, such as rain accumulation, despite their degradation in many cases is obviously visible. Moreover, how to make a full use of inter-frame and intra-frame contexts to promote joint estimation of multiple rainrelated factors has not been fully explored.\n\nOur goal in this paper is to handle video deraining in a more comprehensive way by fully considering the rainrelated factors: rain streak, rain accumulation, accumulation flow, and rain occlusion as illustrated in Fig. 1. To achieve the goal, we introduce a new rain model to synthesize more visually realistic effects of various factors, i.e., rain streak, rain occlusion, rain accumulation, and accumulation flow. We also design a two-stage progressive network, which combines the rain model as well as both physics and natural video priors. In the first stage, a rain-free frame is recovered, which is followed by the inverse process based on our rain model. Subsequently, with the help of the previously recovered clean frames and the initial estimation, in the second stage, a more accurate estimation is inferred using adversarial learning.\n\nOur contributions can be summarized as follows:\n\nA new rain model is proposed. Beyond existing video rain models, it captures rain degradation factors comprehensively, i.e., rain accumulation, accumulation flow, rain streaks, and rain occlusion, providing more realistic modeling of rain scenes. Based on the model, a novel rain video dataset is synthesized to support the development and evaluation of learning-based video rain removal methods in heavy rain.\n\nTo make full use of the spatial and temporal contexts in rain scenes, a convolutional LSTM network is introduced to our deraining network. In the network, the inverse recovery module (physics network) is embedded. The rain-related variables are predicted. Then, the physics network estimates the rain-free frame based on the rain-related variables. This design takes advantages of the prior of the rain model and brings in a more effective architecture. Our proposed LSTM network has a two-stage design, which makes the first attempt to utilize the knowledge of both rain model and adversarial learning for video deraining. The first stage provides the physics accurate results and the second stage, where the results are further processed by the generator trained via the adversarial learning, adjusts the color and contrast distributions, correct details and remove artifacts. This paper is an extension of [41], where we make further significant improvements: 1) In [41], for our synthetic data, the transmission used to generate the accumulation is constant within a frame. In this work, we change it to pixelwise adaptive. The detail is illustrated in Section 5-Datasets.\n\n2) We introduce the information of multiple frames in our two-stage progressive learning framework. In our current version, the models take five successive frames as their input in each stage, which is demonstrated to largely outperform the previous conference version. 3) To further utilize the prior knowledge of natural images, we apply the adversarial learning in the second stage of refinement network, to adjust the color and contrast as well as to correct the details and remove the artifacts generated in the physics recovery process. Extensive experiments demonstrate that, with the above-mentioned contributions, our model outperforms previous methods (including our conference version) quantitatively and qualitatively.\n\nThe rest of our paper is organized as follows. Section 2 illustrates the related work briefly. Section 3 presents our proposed comprehensive rain synthesis model. Section 4 proposes our recurrent video deraining network in details. In Section 5, experimental configurations and results are presented. The concluding remarks are provided in Section 6.\n\n\nRELATED WORK\n\n\nSingle-Image Rain Removal\n\nSingle image rain removal is an ill-posed task. To handle the ill-possessedness, different models and priors are utilized to separate the normal texture and rain signal from rain images. These models consist of sparse coding [20], Gaussian mixture model [25], discriminative sparse coding [28], rain direction prior [45], bilayer optimization [47], joint convolution analysis and synthesis sparse representation model [12]. The advent of deep networks promote the fast evolution of the rain removal from single images. In [7], [8], deep detail networks are constructed to infer the negative residue according to the information of the extracted highfrequency details of the rain images.\n\nYang et al. [42] developed deep networks to detect and remove rain streak in a joint manner, and to recurrent remove the rain streaks and accumulation. In [24], in order to handle the rain streaks having different sizes, Li   rain density and remove rain streaks sequentially. In [30], a progressive recurrent network is constructed, which is incorporated with gate functions and recurrent units to capture the deep features' inter-stage dependencies to remove rain streak. In [9], inspired by Gaussian Laplacian pyramid decomposition, the network is designed to perform operations on the decomposition result, which makes the deraining process more efficiently and the model learning easier.\n\nYasarla et al. [44] proposed a network to extract rainrelated content first at different scales and the corresponding confidence measure, which later on guides the successive rain removal process. In [13], Hu et al. developed a deep network for obtaining the depth-attentional features to estimate a residual signal and restore a clean background one. In [37], a spatial attentive network is built to remove rain streaks with the local-to-global attention guidance. Compared to the above-mentioned single-image rain removal work, only relying on exploiting spatial correlation, in our work, video rain removal is focused on, where we exploit temporal and spatial correlation jointly for removing rain from videos.\n\n\nMulti-Frame Rain/Haze Removal\n\nVideo rain removal can exploit the temporal information and motion context additionally. Garg and Nayar make the first attempt to build the rain model [11] and deal with rain removal problem [10]. The later works address the problem with more flexible and intrinsic modeling of rain streaks and backgrounds, i.e., the shape, orientation, and size of rain streaks [2], Fourier domain feature [1], temporal and chromatic properties [46], phase congruency features [32], and rain streaks' directional tendency [19]. Later on, datadriven methods emerge and brings new progress as well as improved modeling capacity.\n\nIn [35], [36], with the help of the temporal and spatial features, a Bayesian rain detector is developed. Wei et al. [39] made attempt to encode rain streaks as mixtures of Gaussian. The model can finely adapt to a wide kind of rain variations. Kim et al. [21] trained an SVM. The SVM can be used to re-estimate the roughly detected rain streak maps. In [31], a matrix decomposition model is designed. The model is utilized to classify rain streaks into dense and sparse streaks. In [4], a rain image is first segmented into superpixels. Then, the aligned superpixels are enforced by the consistency constraints. After that, the aligned superpixels are compensated for the the lost details. In [23], a multiscale convolutional sparse coding approach is designed for video deraining. In [26], Liu et al. built a recurrent network to seamlessly integrate the multi-task of rain degradation detection, rain removal and background reconstruction. However, all of these previous methods do not pay attention to dealing with rain accumulation.\n\nA series of works that focus on video haze removal provide meaningful insights to handle rain accumulation. Zhang et al. [51] estimated the scene depth jointly with the clear latent image, where the formulation models the depth cues from stereo matching and fog information in a mutually beneficial way. Cai et al. [52] built a Markov random field injected with intensity value prior to improve spatial consistency and temporal coherence for video dehazing. In [22], Li et al. conducted a thorough study over a number of network structure choices for the temporal fusion in the end-to-end learning context. Besides, the video dehazing and object detection are optimized jointly. In [50], Ren et al. build an end-to-end learnable deep network to gather information among adjacent frames to estimate the transmission.\n\nIn our work, we target at handling more kinds of visibility degradation based on the rain synthesis model we propose, i.e., rain streaks, accumulation, accumulation flow, occlusion. To better utilize inter-frame correlation, a twostep RNN is designed to fully make use of the knowledge of physics guidance and adversarial learning. The first stage provides the physics accurate results and then in the second stage, the results are further processed by the generator trained via the adversarial learning, to adjust the color and contrast distributions as well as to correct details and remove artifacts.\n\n\nCOMPREHENSIVE RAIN MODEL\n\nTo handle video deraining issue, we develop a new comprehensive rain model. Using the model, we synthesize rain images from clean ones with the four degradation factors: rain streaks, rain accumulation, accumulation flow and rain occlusion. Rain streaks are the falling raindrops that form whitish streaks due to raindrops' rapid speed relative to the camera's exposure time. Their appearance occludes the background, as illustrated in Fig. 1a. In our rain-streak rendering, rain streaks are fused linearly with the clean background frames [8], [25], [42]. Rain accumulation occurs when the distant rain streaks together with water particles interweave, generating an atmospheric veiling effect [24], [42] where individual rain-streaks cannot be seen individually any more, as illustrated in Fig. 1b. In our rendering, we follow the physics model commonly used to generate fog [42].\n\nIn videos, rain accumulation can be dynamic due to wind or other atmospheric conditions. This dynamic rain accumulation over time form accumulation flow, which shown in Fig. 1c. Its transparency is independent from the depth of the background. It takes any shape, and produces a semitransparent covering veil effect. Its existence is continuous temporally. In the synthesis process, we sample a nature gray image, blur it, and then adjust its intensity range globally to simulate the accumulation flow. At a given temporal step, we will randomly generate the motion vector of the accumulation flow at the moment and the flow is then set based on the vector to move in different temporal steps. The light transmittance of raindrops turns to be low in the heavy rain case. In this case, the additive rain model is not obeyed anymore, and the rain region is identical in intensity [26]. As illustrated in Fig. 1d, the background information is totally occluded. The occlusion image is rendered through an alpha matting process. Its generation is guided a binary mask, with a rain-contaminated image as well as the given intensity map to fuse.\n\nIn its basic form, our rain model follows the commonly used rain model for a single image [16], [25], [28]:\nO \u00bc B \u00fe S;(1)\nwhere S represents rain streaks, B represent the rain-free frame, and O is the image degraded by rain streaks. For video, we add a temporal indicator t:\nO t \u00bc B t \u00fe S t ; t \u00bc 1; 2; . . . ; N;(2)\nwhere N denotes the number of the video frames. S t , the rain streaks, are assumed to be independent and identically distributed [33]. Taking into account rain accumulation and accumulation flow, our model is expressed as:\nO t \u00bc b t B t \u00fe \u00f01 \u00c0 b t \u00deA t \u00fe U t \u00fe S t ; t \u00bc 1; 2; . . . ; N:(3)\nwhere A t represents the global atmospheric light, b t represents atmospheric transmission that dependent on the depth of scene , and U t denotes the rain accumulation flow layer that dependent on the atmospheric flow and local raindrop density. All these factors are temporally continuous. For a given fixed scene, A t f g and a t f g are affected only by the camera motions. U t f g has its motion trajectory. Finally, similar to modeling rain occlusions in [26], [27]:\nO 0 t \u00bc 1 \u00c0 a t \u00f0 \u00de B t \u00fe S t \u00f0 \u00de\u00fea t M t ;(4)\nwhere a t signifies an alpha matting map, and M t is the rain reliance map, the rain model starting from Eq. (3) to describe rain occlusions is expressed as:\ne O t \u00bc 1 \u00c0 a t \u00f0 \u00deO t \u00fe a t M t :(5)\nTherefore, we obtain a rain model that captures rain streaks, accumulation, accumulation flow, and occlusions in a comprehensive way.\n\nWith the guidance of our rain model (5), we can synthesize more realistic-looking rain videos compared to existing methods. Two rendered examples by our model are shown in Fig. 2. Based on the rain synthesis model, we build a novel video rain dataset. More details are discussed in Section 5. A summary of commonly used datasets in recent video rain removal works, including their included degradation factors, rain models, main features, and code links, are provided in Table 1. Most of previously adopted datasets (TCLRM, Stochastic, MS-CSC, DIP, FastDeRain, MRF, and NTURain) only consider rain streak degradation and takes the rain model in Eq. (2). RainSynLight25 and RainSynComplex25 additionally model rain occlusions and their related rain models turn Eq. (4). Our dataset is build based on Eq. (5) and four kinds of degradation factors are included.\n\n\nRECURRENT VIDEO DERAINING NETWORK\n\nOur method is based on a two-stage network that utilizes multi-frames to derain the input video progressively. The initially derained estimations are used as guidance on the refined deraining network, which extracts more effective features. The first stage of our method follows the inverse recovery process in Eq. (3) and Eq. (5). Our method makes use of both the prior knowledge of rain model via injecting physics network and nature image distributions by employing the adversarial learning. As rain models cannot totally simulate complex rain scenes, i.e., the complex real rain accumulation and illumination change after the degradation, therefore, we introduce an enhancement network that applies adversarial learning to adjust the derained results generated by the inverse recovery of a rain model.\n\n\nNetwork Architecture\n\nOur method consists of 3 main networks: the initial deraining network (Initial-DerainNet), inverse recovery network (Phys-icsNet) refined deraining network (Refined-DerainNet), as illustrated in Fig. 3. In the first stage, Initial-DerainNet takes successive rain frames O t\u00c02 ; O t\u00c01 ; . . . ; O t\u00fe2 as its input and estimates the rain-related variables of the frame t (\u0125 t ;b i t ; and\u00c2 t ), where\u0125 t aims to regress O t \u00c0 U t \u00c0 S t . PhysicsNet utilizes the predicted rain-related variables to estimate the initial background (rain-free) frame b B i t with the help of Eqs. (3) and (5).\n\nIn the second stage, Refined-DerainNet takes the existing clean frames ( b B f t\u00c02 , b B f t\u00c01 and the initially estimated clean frame B i t from the first stage) as well as their corresponding rain frames O t\u00fe1 and O t\u00fe2 as input to predict the refined background frame b B f t . The adversarial learning is used to constrain the training of the Refined-DerainNet, i.e., the generation process of b B f t . We utilize multiples losses to jointly regularize the recovery of b B f t to accurately predict background frames while keeping the generalization capacity of the models. Note that, compared to [41] we do not align the frames. We observed that, alignment cannot lead to performance gains in our new framework, which takes successive five frames as input. The exclusion of the alignment process reduces the network's complexity.\n\n\nInitial Deraining Network\n\nInitial-DerainNet's architecture is a U-Net [49] like network, as illustrated in Fig. 3. A few frames are fed into convolutional layers concurrently and transformed into features via multiple convolutional layers. For the intermediate layers, we down-sample the spatial resolutions of features (at the encoder side) and then up-sample them (at the decoder side). In the encoder part, 3D convolutions are used to change the resolution sizes and shrink the temporal step of the tensor stacked by the features of the input frames. The specific structure of the encoder is depicted in Fig. 4a. The input frames are rearranged into two sequences: \ns 1 \u00bc s 2 \u00bc O t\u00c02 ; O t\u00c01 ; O t ; O t\u00fe1 ; O t\u00fe2 \u00bd ;(6)\nwhich are processed by two sub-encoder as illustrated in Fig. 4b and the extracted features are summed together after the process as illustrated in Fig. 4a. As shown in Fig. 3, we use the skip connections (red lines), which help the features produced by the shallow layers reach the decoder's counterpart layers. Initial-Derain-Net generates three rain-related variables:\nv i t \u00bc\u0125 i t ;\u00c2 i t ;b i t h i \u00bc G I s 1 ; s 2 \u00f0 \u00de;(7)\nwhere\u0125 i t ,\u00c2 i t , andb i t are rain streak-free image (might including rain accumulation), atmospheric air light and transmission of the rain accumulation estimated by the first stage Initial-DerainNet. G I \u00f0\u00c1\u00de denotes the Initial-DerainNet process.\n\nThere are three decoders to decode the feature generated by the encoder to output\u0125 i Eq. (4) The dataset is synthesized by non-rain sequences with the rain streaks generated by the probabilistic model [11], sharp line streaks [42] and sparkle noises.\n\nLiu et al.\n\n\n[26]\n\nRainSynAll100 900 (Synthetic Train) 100 (Synthetic Test) Streak, Occlusion, Rain Accumulation, Accumulation Flow\n\nEq. (5) The dataset is generated by 1,000 clean sequences from the Vimeo-90K dataset [40] with the mentioned four kinds of degradations.\n\nOur work \nB i t \u00bc\u0125 t \u00c0 1 \u00c0b i t \u00c2\u00c2 t maxb i t ; ;(8)\nwhere signifies the threshold that helps guarantee the numerical stability, which in our experiments is set to 0.1.\u0125 t aims to regress O t \u00c0 U t \u00c0 S t . This module, which is injected to the whole network for an end-to-end training, makes full use of the prior of the physics model and brings in a more effective architecture.\n\n\nRefined Deraining Network\n\nHaving estimated the \u00f0t \u00c0 1\u00deth and \u00f0t \u00c0 2\u00deth rain-free background framesB f t\u00c01 andB f t\u00c02 as well as the initially estimated background ofB i t at time-step t, Refined-DerainNet takes them as input:\ns 0 1 \u00bcB f t\u00c02 ;B f t\u00c01 ;B i t ; O t\u00c01 ; O t\u00fe1 h i (9) s 0 2 \u00bcB f t\u00c02 ;B f t\u00c01 ; O t ; O t\u00fe1 ; O t\u00fe2 ; h i ;(10)\nand directly predicts more refined rain-free background frames.\n\nRefined-DerainNet has the same architecture as Initial-DerainNet. In the network, features are extracted from s 0 1 and s 0 2 , respectively, and summed together at the bottleneck  of the encoder and decoder, which bridges the encoder and decoder and has the smallest spatial size. The network employs the skip connections as well as a convolutional LSTM. The convolutional LSTM is used to propagate the information at the feature level across frames, as denoted in Fig. 3, at the beginning of the decoder. We re-estimate the rain-free frames with a refinement network:\nB f t \u00bc G R s 0 1 ; s 0 2 ;(11)\nwhereB f t is the refined rain-free frame. G R \u00f0\u00c1\u00de denote the process of Refined-DerainNet.\n\n\nAdversarial Learning\n\nTo check whether the output (the clean background) looks realistic and indeed clean, we employ a discriminator, using the following loss functions:\nL f Rect \u00bc \u00c0SSIMB f t ; B t ; (12) L f Dis \u00bc \u00c0log G D \u00f0B t \u00de \u00f0 \u00de\u00c0log 1 \u00c0 G DB f t ;(13)\nwhere G D \u00f0\u00c1\u00de is the discriminator, L f Dis is the adversarial loss, and B t is the corresponding ground-truth. L f Rect is the reconstruction loss to keep the fidelity, and SSIM \u00c1 \u00f0 \u00de is the SSIM function.\n\nWe also intend to keep the refined images free from rain accumulation and continuous temporally. For this, we apply constraints based on the dark channels [48] of the refined frames:\nL f Dark \u00bc DarkB f t 2 2 ; (14) L f Dark TV \u00bc DarkB f t \u00c0 DarkB f t\u00c01 2 2 ;(15)\nwhere Dark \u00c1 \u00f0 \u00de is the function to calculate the dark channel of the input. The first term, the dark channel minimization, makes the refined frames more rain-accumulation free while the second term, the dark channel temporal total variation, makes the refined frame more temporally continuous.\n\n\nLoss Functions\n\nOur network is trained in an end-to-end manner. The loss functions are expressed as:\nL all \u00bc L i Var \u00fe Rect L f Rect \u00fe Dis L f Dis \u00fe Dark L f Dark(16)\n\u00fe Dark TV L f Dark TV ;\nL i Var \u00bc \u00c0SSIM\u0125 i t ; h t \u00c0 SSIMb i t ; b t \u00c0 \u00c1 \u00c0 SSIM\u00c2 i t ; A t \u00c0 \u00c1 ;(17)\nwhere h t , b t and A t are the corresponding ground truths, and Rect , Dis , Dark and Dark TV are the weighting parameters.\n\n\nEXPERIMENTAL RESULTS\n\n\nDatasets\n\nOur Dataset Synthesis. Our dataset is synthesized based on Eq. (5). The synthesized videos come from two kinds of resources: 1) single images from OTS [54] with the depth information and sampled uniform motions; 2) real videos without the depth information.\n\nIn the first case, the transmission b t is generated by:\nb t \u00bc exp \u00c0s b t d t =C b t ;(18)\nwhere s b t is sampled from a uniform distribution between [2.6, 4.6], d t is the scene depth, and C b t is set to 10 empirically. The global atmospheric light A t is sampled from a uniform distribution between [0.25, 0.95]. The motion vector of the frame is sampled from a uniform distribution between \u00c0B MV t ;\nB MV t \u00c2 \u00c3 , where B MV t\nis a quarter of the minimum of the frame's weight and height. The synthesized motions will be applied to crop different image regions to form a simulated video. The accumulation flow U t is generated via the following process: we sample a nature gray image from BSD500 dataset [56], resize it to a very large one, crop and blur it, and then adjust its intensity range (with a random value sampled from the distribution between [0.1, 0.5]) globally to simulate the accumulation flow; the motion of the accumulation flow is selected from KITTI dataset [55] with a guided blurring operation; after that, this motion guides the accumulation flow to move among different time-steps. The rain streak S t and occlusion-related variables a t ; M t are generated following the same procedure in [26], [27].\n\nIn the second case, the procedures are the same in most aspects. The differences lie in: 1) b t is global variable sampled from [0.5, 1] instead of a pixel-wise one; 2) the background sequences are selected from the Vimeo-90K Dataset [40]; 3) the videos are ready-made, which are not synthesized from single images. More details about our dataset synthesis can be found on our website.\n\nFinally, our dataset includes 900 training sequences and 100 validation sequences, where the sequence lengths of half of these sequences are 7 while those of the other half are 9.\n\nEvaluation Datasets. The proposed method is compared with SOTA on widely used datasets. In [26], Liu et al. propose two datasets RainSynComplex25 and RainSynLight25 with respective heavy and light rain streaks. In [4], Chen et al. propose NTURain, which consists of two groups. One is captured by panning and unstable camera that has slow movements, while the other is taken from a car-mount fast moving camera. In this paper, we additionally propose RainSynAll100, which is generated by 500 clean sequences and 500 clean images with the mentioned four kinds of degradation factors. The whole dataset consists of testing and training datasets, including the respective 100 and 900 video sequences. We use practical rain video sequences selected from of movie clips and videos of Youtube website.\n\n\nEvaluations\n\nBaselines. Our MFGAN is compared with the following stateof-the-art (SOTA) methods: DetailNet [8], Discriminative Sparse Coding (DSC) [28], Joint Rain Detection and Removal (JORDER) [42], Progressive Recurrent Network (PReNet) [30], Uncertainty guided Multi-scale Residual Learning (UMRL) [44], Stochastic Encoding (SE) [39], Temporal Correlation and Low-Rank Matrix completion (TCLRM) [21], Discriminative Intrinsic Priors (DIP) [19], Joint Recurrent Rain Removal and Reconstruction (J4RNet) [26], FastDeRain [18], MultiScale Convolutional Sparse Coding (MS-CSC) [23], SuperPixel Alignment and Compensation CNN (SpacCNN) [4]. The code links of all compared methods are provided in Table 4. J4RNet, DetailNet, JORDER, MS-CSC, PReNet, and SpacCNN are built based on deep-learning. SE, FastDerain, TCLRM, MS-CSC, J4RNet, DIP, and SpacCNN are video rain removal methods. DSC, DetailNet, PReNet, JORDER, and UMRL are single image deraining methods. When we conduct evaluations on RainSynAll100, for the methods that do not handle rain accumulation, spatio-temporal MRF dehazing (MRF) [3] and Endto-end united Video Dehazing and detection Network (EVD-Net) [22] are taken for pre-processing or post-processing.\n\nImplementation Details. For quantitative evaluation, we use RainSynLight25, RainSynComplex25, NTURain, our proposed RainSynAll100, and our collected real rain videos for evaluation. As demonstrated in Table 1, NTURain includes 25 paired videos for training and 8 for testing. Both RainSyn-Light25 and RainSynHeavy25 include 190 paired videos for training and 25 for testing, respectively. Our proposed Rain-SynAll100 includes 900 paired videos for training and 100 for testing. For qualitative evaluation, we use the collected real rain videos that do not have the paired clean version. Our MFGAN is trained via two steps. In the first step, our model is first trained without using adversarial loss L f Dis , as well as dark channel prior related losses L f\n\nDark and L f Dark TV . In the second step, all losses are used for training. The weighting parameters are set as follows: Rect \u00bc 1, Dis \u00bc 0:001, Dark \u00bc 0:01 and Dark TV \u00bc 0:1. Adam optimizer is used in the whole training process with the learning rate 1e-4 for both generator and discriminator of our MFGAN. The model is initialized by Kaiming Initialization [53]. Empirically, the initialized weight of the convolutional layer takes only 0.5 of the default value. All training videos are sampled and cropped into 128 \u00c2 128 \u00c2 5 cubics with a batch size of 2. For the non-deep learning-based methods, including TCLRM, SE, DSC, UMRL, FastDeRain, JCAS, and DIP, the evaluation is directly performed based on the codes released by the authors. For DetailNet and SpacCNN, we use their released models. JORDER and J4RNet are retrained based on the gray version of the respective training set, following their original settings. J4RNet-E, J4RNet-P, CVPR-2019, and our proposed method are retrained with the respective training set when the evaluation is performed on different datasets. MS-CSC does not need training, as it is an optimization-based deep-learning method. In the quantitative evaluation, Structure Similarity Index (SSIM) [38] and Peak Signal-to-Noise Ratio (PSNR) [17] are used as the quality measures. We follow previous methods to compare the quantitative results in the luminance channel only, since the human visual system is more sensitive to the luminance channel compared to the chrominance ones.\n\nQuantitative Evaluation. In Table 2, our method is compared on the datasets with rain streak degradation only. Our method achieves better performance compared with previous methods. The proposed method obtains more than 7.5 dB and 4.0 dB PSNR gains on RainSynComplex25 and RainSynLight25 compared with J4RNet and SpacCNN. Compared to our CVPR-2019 results [41], Our method obtains almost 5.0 dB and 1.1dB PSNR gains on RainSyn-Complex25 and RainSynLight25.\n\nAll methods are also evaluated on the proposed synthesized rain dataset RainSynAll100. SE, FastDerain, DIP, Best results are denoted in red and the second best results are denoted in blue.\n\n\nTABLE 3 Quantitative Evaluation on RainSynAll100\n\nST-MRF and EVD-Net as used as pre/post-processing. Best results are denoted in red and the second best results are denoted in blue.\n\nSpacCNN and MS-CSC are combined with two SOTA video defogging methods: EVD-Net [22] and ST-MRF [3] (post/preprocessing of rain accumulation removal) for a fair comparison. In Table 3, MRF + DIP uses the sequential combination of ST-MRF and DIP. DIP + MRF takes ST-MRF as post-processing. EVD-Net + DIP and DIP+ EVD-Net employ EVD-Net as pre/post-processing, respectively. The same applies to other SOTA methods. J4RNet-E uses the method in [26] to predict the background frame based on the input rain frame directly. J4RNet-P injects the inverse recovery module to predict the rain-related variables first and then estimate the background frame accordingly based on the predicted variables. As is illustrated in Table 3, our method rank the first among all methods. The performance gain is almost 6.0 dB in PSNR and 0.3 in SSIM. Compared to our previous CVPR-2019 results [41], our method obtains more than 0.170 and 4dB gains in SSIM and PSNR, respectively.\n\nQualitative Evaluation. Visual results of different deraining methods are also compared in Figs. 5,6,7,8,9,10,11,12,13,14,15,and 16. Figs. 5,6,and 7 show the results on synthetic rain videos. It is illustrated that, our method performs better than other SOTA methods on the synthetic data. Our method also obtains better results than other SOTA methods on real images (Figs. 9-7). For Figs. 9, 10, 11, 12, 13, and 14, all methods only apply rain streak removal. It is shown that, the proposed method method is better to remove most large (Figs. 8 and 9) and small rain streaks (Fig. 10). For accumulation, our method restores the best results in Figs. 14, 15, and 16. Note that, For Fig. 15, other methods apply EVD-Net as post-processing. Comparatively, our method is more successful to remove rain accumulation. For Fig. 16, other methods apply ST-MRF as pre-processing. The results of other methods are over-exposed. Comparatively, our method obtains naturally looking results.  Complexity Comparison. In Table 5, we compare the runtime of several SOTA methods. The testing video's resolution is 832 \u00c2 512. The proposed method, J4RNet-E, and J4RNet-P method are implemented by Pytorch. Other SOTA methods are implemented by MATLAB. DetailNet and SpacCNN are built based on MatConv-Net. 1 JORDER is built on the Caffe's Matlab wrapper. 2 TCLRM is built based on CPU while other methods are GPU-based methods. Generally, the running speed of the proposed method is on par with other SOTA methods. Note that, compared with our previous work [41], our this work only needs almost a half running time. We also compare the parameters of different deraining methods in Table 6. In general, the parameters of our lightweighted model (Ours-S used for only rain streaks in Table 2) are on par with those of JORDER, CVPR-2019 and SpacCNN. However, Ours-S achieves much better quantitative results on the three datasets in Table 2. Our full model (Ours-L) includes many more parameters and leads to significantly superior performance as demonstrated in the quantitative results of Table 3 and the visual results in Figs. 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, and 17. It is noted that, Ours-S and Ours-L have more parameters as they introduce 3D convolutions to aggregate temporal information at the feature level. The Fig. 7. Results of rain removal methods on a video frame from the synthesized dataset RainSynAll100. Except for our method, other methods apply EVD-Net as pre-processing.   increased parameters do not certainly lead to an increased computational complexity as the 2D convolutions in other models will be reused many times when dealing with multiple input frames at a certain time-step.\n\n\nAblation Studies\n\nAblation Study on Network Architecture. In Table 7, our methods with different components are evaluated. Table 7 shows that, the LSTM and SF-DerainNet improve the quantitative results significantly (v 1 versus v 4 and v 2 versus v 4 ). The rain streak removal can be benefited from the physics network, resulting a higher SSIM (v 3 versus v 4 ). Adding flow estimation and alignment cannot further improve the performance in our case (v 4 versus v 5 ). Therefore, v 4 is selected as our final version.  Visual Comparisons of Different Versions. We also compare our results with our previous results (CVPR-2019) [41] in Fig. 17. The figure shows our method is more successful in removing the rain streaks, as illustrated in the blue boxes of the top two panels in Fig. 17, and in removing rain accumulation, as illustrated in the blue boxed of the bottom two panels in Fig. 17.\n\nResult of Versions with Different Parameters. We also show the performance of our methods with different parameters in Fig. 18. The figure shows that more parameters lead to higher performance, and with more parameters, the marginal performance gain is small.\n\n\nCONCLUSION\n\nWe introduced a video deraining method that consider more comprehensive degradation factors, i.e., accumulation, rain streak, accumulation flow and occlusion. To accomplish this, a new rain model is proposed, which can capture the factors, i.e., rain accumulation, accumulation flow, rain streaks, and rain occlusion. Based on the model, a novel rain video dataset is synthesized to support the development and evaluation of our deraining method. A recurrent neural network (RNN) is constructed, where the inverse recovery module can be injected. Our proposed two-stage RNN exploits the  knowledge of adversarial learning and physics model. The first stage provides the physics accurate results and then in the second stage, the results are further processed by the generator trained via the adversarial learning, to adjust the color and contrast distributions as well as to correct details.       Robby T. Tan (Member, IEEE) received the PhD degree in computer science from the University of Tokyo. He is currently an associate professor at Yale-NUS College and ECE (Electrical and Computing Engineering), National University of Singapore. Previously, he was an assistant professor with Utrecht University. His research interests include machine learning and computer vision, particularly in dealing with bad weather, physics-based, and motion analysis.\nBaseline v 1 v 2 v 3 v 4 v 5 Initial-DerainNet \u00c2 @ @ @ @ Inverse Recovery @ @ \u00c2 @ @ LSTM @ \u00c2 @ @ @ Alignment \u00c2 \u00c2 \u00c2 \u00c2 @PSNR\nJiashi Feng (Member, IEEE) received the PhD degree from the National University of Singapore, in 2014. He is currently an assistant professor at the Department of Electrical and Computer Engineering, National University of Singapore. Before joining NUS as a faculty, he was a postdoc research follow at UC Berkeley. His research interests include computer vision and machine learning. In particular, he is interested in object recognition, detection, segmentation, robust learning and deep learning. Bin Cheng received the BE degree from the University of Science and Technology of China, and the PhD degree from the National University of Singapore. He is currently a research director at the Beijing Academy of Artificial Intelligence (BAAI). His research interests include computer vision, machine learning and the related applications. And he also successfully shipped a dozen of technologies to incubate industry products, including smart devices, search, live stream, short video, and financial risk-control.\n\nJiaying Liu (Senior Member, IEEE) received the PhD degree (Hons.) in computer science from Peking University, Beijing China, in 2010. She is currently an associate professor with Peking University Boya Young fellow with the Wangxuan Institute of Computer Technology, Peking University. She has authored more than 100 technical articles in refereed journals and proceedings, and holds 50 granted patents. Her current research interests include multimedia signal processing, compression, and computer vision. She is a senior member of CSIG and CCF. She was a visiting scholar with the \n\nFig. 1 .\n1Visibility degradation caused by rain. (a) Rain streaks. (b) Rain accumulation. (c) Rain accumulation flow. The atmosphere flow makes veiling layers' densities at the same pixel of two frames different. (d) Rain occlusion. There is an identical intensity in the occlusion regions.\n\nFig. 2 .\n2Several example data based on our synthesis data produced by Eq. (5).\n\nFig. 3 .\n3Our two-stage progressive network framework for video rain removal. In the first stage, Initial-DerainNet uses successive rain frames O t\u00c02 ; O t\u00c01 ; . . . ; O t\u00fe2 as its input and outputs the estimation of the rain-related variables of the frame t. Physics recovery module translates these predicted rain-related variables into the initially estimated background frame B i t with the guidance of the inverse recovery in Eq. (3) and Eq. (5). In the second stage, Refined-DerainNet takes the existing clean frames ( b B f t\u00c02 , b B f t\u00c01 and the initially estimated clean frame b B i t from the first stage) as well as their corresponding rainy frames O t\u00fe1 , O t\u00fe2 as the network's input to directly predict the rain-free frames. We train the whole model in an end-to-end manner with the loss functions for variable estimation L f Var , and background frame refinement (reconstruction constraint L f Rect , adversarial learning L f Dis , and dark channel prior constraint L f Dark ; L f Dark TV ).\n\nFig. 4 .\n4(a) The architecture of our encoder inFig. 3. (b)The sub-encoder architecture that constitutes the encoder in (a).\n\nFig. 5 .\n5Results of rain removal methods on a video frame from the synthesized dataset RainSynComplex25.\n\nFig. 6 .\n6Results of rain removal methods on a video frame from the synthesized dataset RainSynAll100. Except for our method, other methods apply ST-MRF as post-processing.\n\nFig. 8 .\n8Results of rain streak removal by different methods on a real video frame. The results of SE, PreNet, FastDeRain, UMRL, and MS-CSC have remaining rain streaks, as denoted in blue boxes. Meanwhile, SE, PreNet and FastDeRain also falsely remove some details, as denoted by red boxes. Comparatively, our method can well handle the rain streaks.1. http://www.vlfeat.org/matconvnet/ 2. http://caffe.berkeleyvision.org/Fig. 9. Results of rain streak removal by different methods on a real video frame. The results of SE, DIP, FastDeRain, UMRL, and MS-CSC have obvious remaining rain streaks. For SpacCNN, there are small rain streaks in the regions denoted by blue boxes and details are falsely removed as denoted in the red box. Comparatively, our method can well handle the rain streaks.\n\nFig. 10 .\n10Results of rain streak removal by different methods on a real video frame. The results of JCAS, UGSM, MS-CSC, SE, and FastDeRain have obvious remaining rain streaks. For SpacCNN, there are small rain streaks in the regions denoted by blue boxes. Comparatively, our method can well handle the rain streaks.\n\nFig. 11 .\n11Results of rain streak removal by different methods on a real video frame. The results of JCAS, UGSM, and FastDeRain have obvious remaining rain streaks, as denoted by blue arrows. As the video clip includes large camera motions, the results of MS-CSC and SE are totally damaged. For DIP, there are remaining rain streaks denoted by blue arrows and the details are blurred in the regions denoted by red arrows. Comparatively, our method can well handle the rain streaks and preserve structure details.\n\nFig. 12 .\n12Results of rain streak removal by different methods on a real video frame. The results of JCAS, UGSM, MS-CSC, SE, and FastDeRain have obvious remaining rain streaks as the rain streaks are too dense in the frame. SE additionally suffers from the color cast. SpacCNN still has remaining rain streaks as denoted by the blue box. Comparatively, our method can successfully remove most rain streaks.\n\nFig. 13 .\n13Results of rain streak removal by different methods on a real video frame. It is clearly observed that, the results of JCAS, UGSM, MS-CSC, SE, and FastDeRain fail to remove the intensive rain streaks. J4RNet still has remaining rain streaks as denoted by blue boxes. Comparatively, our method can successfully remove most rain streaks.\n\nFig. 14 .\n14Results of rain removal (rain streak and accumulation removal) by different methods on a real video frame. No method applies pre-processing or post-processing. The remaining rain streaks are denoted in blue boxes.\n\nFig. 15 .\n15Results of rain removal (rain streak and accumulation removal) by different methods on a real video frame. Except for our method, other methods apply EVD-Net as post-processing. The remaining rain streaks are denoted in blue boxes.\n\nFig. 16 .\n16Results of rain removal (rain streak and accumulation removal) by different methods on a real video frame. Except for our method, other methods apply ST-MRF as pre-processing. The remaining rain streaks are denoted in blue boxes.\n\nFig. 17 .\n17Visual comparisons of our method and our previous version (CVPR-2019). YANG ET AL.: RECURRENT MULTI-FRAME DERAINING: COMBINING PHYSICS GUIDANCE AND ADVERSARIAL LEARNING 8583\n\nShiqi\nWang (Member, IEEE) received the BS degree in computer science from the Harbin Institute of Technology, in 2008, and the PhD degree in computer application technology from Peking University, in 2014. From 2014 to 2016, he was a post-doctoral fellow with the Department of Electrical and Computer Engineering, University of Waterloo, Waterloo, ON, Canada. From 2016 to 2017, he was with the Rapid-Rich Object Search Laboratory, Nanyang Technological University, Singapore, as a research fellow. He is currently an assistant professor with the Department of Computer Science, City University of Hong Kong. He has proposed more than 40 technical proposals to ISO/MPEG, ITU-T, and AVS standards, and authored/coauthored more than 150 refereed journal/conference papers. His research interests include video compression, image/video quality assessment, and image/video search and analysis. He received the Best Paper Award from IEEE ICME 2019, IEEE Multimedia 2018, PCM 2017, and is the coauthor of a paper that received the Best Student Paper Award in the IEEE ICIP 2018.\n\n\nUniversity of Southern California, Los Angeles, from 2007 to 2008. She was a visiting researcher with the Microsoft Research Asia in 2015 supported by the Star Track Young Faculties Award. She has served as a member of Multimedia Systems and Applications Technical Committee (MSA TC), and Visual Signal Processing and Communications Technical Committee (VSPC TC) in IEEE Circuits and Systems Society. She received the IEEE ICME-2020 Best Paper Award and IEEE MMSP-2015 Top10 percent Paper Award. She has also served as the associate editor of the IEEE Trans. on Image Processing, the IEEE Trans. on Circuit System for Video Technology and Elsevier JVCI, the technical program chair of IEEE ICME-2021/ACM ICMR-2021, the publicity chair of IEEE ICME-2020/ICIP-2019, and the area chair of CVPR-2021/ECCV-2020/ICCV-2019. She was the APSIPA distinguished lecturer (2016-2017).\n\n\net al. built several parallel sub-networks to generate the intermediate results, and after that, intermediate results are integrated into the final result. In [45], a new multi-stream density-aware densely connected CNN is built to estimate\n\nTABLE 1\n1Summary of Rain Synthetic Models in the LiteratureName \n# Sequence \nDegradation \nModel \nMain Features \nPublication \n\nTCLRM 1 \n9 (Synthetic Test) 6 \n(Real Test) \n\nStreak \nEq. (2) The real testing sequences include 1 captured \none and 5 movie clips. 3 of 9 synthetic \nsequences are captured with moving cameras, \nwhereas 6 of 9 are captured with stationary \nones. \n\nKim et al. \n2015 [21] \n\nStochastic 2 \n4 (Synthetic Test) 2 \n(Real Test) \n\nStreak \nEq. (2) Rain streaks, varied from tiny drizzling to heavy \nrainstorms, are added to four videos with static \nbackgrounds. \n\nWei et al. \n2017 [39] \n\nMS-CSC 3 \n3 (Synthetic Test) 3 \n(Real Test) \n\nStreak \nEq. (2) Different types of rain streaks are added to these \nvideos, varying from tiny drizzling to heavy \nrainstorms and vertical rain to slash lines. \n\nLi et al. \n2018 [23] \n\nDIP \n6 (Synthetic Test) 2 \n(Real Test) \n\nStreak \nEq. (2) The synthesized rain videos include heavy and \nlight synthetic rain. \n\nJiang et al. \n2017 [19] \nFastDeRain 4 \n12 (Synthetic Test) \n4 (Real Test) \n\nStreak \nEq. (2) 12 video sequences are synthesized with 4 clean \nvideos and 3 types of rain streaks. \n\nJiang et al. \n2019 [18] \nMRF \n5 (Synthetic Test) 1 \n(Real Test) \n\nStreak \nEq. (2) Various rain and snow video sequences include \nillumination variations, camera motions, moving \nobjects, etc. \n\nRen et al. \n2017 [31] \n\nNTURain 5 \n25 (Synthetic \nTrain) 8 (Synthetic \nTest) 7 (Real Test) \n\nStreak \nEq. (2) \nThree to four different rain appearances are \nsynthesized over each video clip to provide \nus 25 rainy scenes. 8 testing scenes can be \ndivided into two groups: one shot from a \npanning and unstable camera and the other from \na fast-moving camera. \n\nLiu et al. \n2018 [4] \n\nRainSynLight25 6 \n190 (Synthetic \nTrain) 25 \n(Synthetic Test) \n\nStreak, Occlusion \nEq. (4) The dataset is synthesized by non-rain sequences \nwith the rain streaks generated by the \nprobabilistic model [11]. \n\nLiu et al. \n2018 [26] \n\nRainSynComplex25 6 190 (Synthetic \nTrain) 25 \n(Synthetic Test) \n\nStreak, Occlusion \n\n\nTABLE 2 Quantitative\n2Evaluation of Different Rain Streak Removal Methods on RainSynLight25, RainSynComplex25, and NTURain\n\nTABLE 5 Running\n5Time Comparison (in Section) of Different Rain Removal Methods on a Video with the Spatial Resolution 832 \u00c2 512Methods JORDER DetailNet FastDeRain SpacCNN \nTCLRM \n\nTime \n0.6329 \n1.4698 \n0.3962 \n9.5075 \n192.7007 \n\nMethods MS-CSC \nSE \nCVPR-2019 \nJ4RNet \nProposed \n\nTime \n15.7957 \n19.8516 \n0.8974 \n0.8401 \n0.5146 \n\n\n\nTABLE 6 Parameter\n6Comparison of Different Deep-Learning Based Rain Removal MethodsMethods DetailNet \nPreNet \nJORDER \nSpacCNN \n\n#Para. \n58,175 \n168,963 \n4,169,024 1,430,403 \n\nMethod \n-\nCVPR-2019 \nOurs-S \nOurs-L \n\n#Para. \n-\n4,466,694 \n6,964,803 29,472,018 \n\n\n\nTABLE 7 Ablation\n7Analysis for Network Architecture\n\n\nBest results are denoted in red and the second best results are denoted in blue.23.79 \n24.82 \n24.93 \n25.14 \n24.76 \n\nSSIM \n0.9029 0.9166 0.9090 0.9172 0.9160 \n\n\n\nTABLE 4\n4Summary of Code Links for All MethodsMethodsProject PageDetailNet https://xueyangfu.github.io/projects/tip2017.html DSC http://www.math.nus.edu.sg/ matih/download/imaee_derainine/rain_removal_v.l.l.zin PReNet https://github.com/csdwren/PReNet UMRL https://github.com/raieevyasarla/UMRL&ndash;using-Cycle-Spinning SE https://github.com/wwzier/RainRemoval_ICCV2017 TCLRM http://mcl.korea.ac.kr/derainina/ DIP https://github.com/TaiXiangJiang/FastDeRain FastDeRain -J4RNet https://github.com/flyywh/J4RNet-Deep-Video-Derainina-CVPR-2018 MS-CSC https://github.com/MinehanLi/MS-CSC-Rain-Streak-Removal SpacCNN https://github.com/hotndv/SPAC-SupplementarvMaterials MRF https://caibolun.aithub.io/st-mrf/ EVD-Net https://github.com/Boyiliee/EVD-Net\nt ,\u00c2 i t , andb i t , respectively. Note that, the estimations of\u00c2 i t andb i t will influence each other, hence we make them share the same encoder. Due to\u00c2 i t is a global variable, there are no skip connections that bypassing the features from the encoder to the corresponding decoder side. A convolutional LSTM is used to feed-forward the information at the feature level across frames at the end of the convolutional layers at the beginning of the decoder side, as denoted inFig. 3.4.3 Physics ModuleGiven\u0125 i t ,b i t , and\u00c2 i t ,we employ Eq. (3) to estimate the clean background frameB i t with the guidance of a single frame rain input:\n\" For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/csdl.\nACKNOWLEDGMENTS\nAnalysis of rain and snow in frequency space. P C Barnum, S Narasimhan, T Kanade, Int. J. Comput. Vis. 862/3P. C. Barnum, S. Narasimhan, and T. Kanade, \"Analysis of rain and snow in frequency space,\" Int. J. Comput. Vis., vol. 86, no. 2/3, 2010, Art. no. 256.\n\nRain or snow detection in image sequences through use of a histogram of orientation of streaks. J Bossu, N Hauti, J.-P Tarel, Int. J. Comput. Vis. 933J. Bossu, N. Hauti ere, and J.-P. Tarel, \"Rain or snow detection in image sequences through use of a histogram of orientation of streaks,\" Int. J. Comput. Vis., vol. 93, no. 3, pp. 348-367, 2011.\n\nReal-time video dehazing based on spatio-temporal MRF. B Cai, X Xu, D Tao, Proc. Pacific Rim Conf. Multimedia. Pacific Rim Conf. MultimediaB. Cai, X. Xu, and D. Tao, \"Real-time video dehazing based on spatio-temporal MRF,\" in Proc. Pacific Rim Conf. Multimedia, 2016, pp. 315-325.\n\nRobust video content alignment and compensation for rain removal in a CNN framework. J Chen, C.-H Tan, J Hou, L.-P Chau, H Li, Proc. Conf. Comput. Vis. Conf. Comput. VisJ. Chen, C.-H. Tan, J. Hou, L.-P. Chau, and H. Li, \"Robust video content alignment and compensation for rain removal in a CNN framework,\" in Proc. Conf. Comput. Vis. Pattern Recognit., 2018, pp. 6286-6295.\n\nCompression artifacts reduction by a deep convolutional network. C Dong, Y Deng, C C Loy, X Tang, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisC. Dong, Y. Deng, C. C. Loy, and X. Tang, \"Compression artifacts reduction by a deep convolutional network,\" in Proc. IEEE Int. Conf. Comput. Vis., 2015, pp. 576-584.\n\nImage super-resolution using deep convolutional networks. C Dong, C C Loy, K He, X Tang, IEEE Trans. Pattern Anal. Mach. Intell. 382C. Dong, C. C. Loy, K. He, and X. Tang, \"Image super-resolution using deep convolutional networks,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 38, no. 2, pp. 295-307, Feb. 2016.\n\nClearing the skies: A deep network architecture for single-image rain removal. X Fu, J Huang, X Ding, Y Liao, J Paisley, IEEE Trans. Image Process. 266X. Fu, J. Huang, X. Ding, Y. Liao, and J. Paisley, \"Clearing the skies: A deep network architecture for single-image rain remov- al,\" IEEE Trans. Image Process., vol. 26, no. 6, pp. 2944-2956, Jun. 2017.\n\nRemoving rain from single images via a deep detail network. X Fu, J Huang, D Zeng, Y Huang, X Ding, J Paisley, Proc. Conf. Comput. Vis. Pattern Recognit. Conf. Comput. Vis. Pattern RecognitX. Fu, J. Huang, D. Zeng, Y. Huang, X. Ding, and J. Paisley, \"Removing rain from single images via a deep detail network,\" in Proc. Conf. Comput. Vis. Pattern Recognit., 2017, pp. 3855-3863.\n\nLightweight pyramid networks for image deraining. X Fu, B Liang, Y Huang, X Ding, J Paisley, IEEE Trans. Neural Netw. Learn. Syst. 316X. Fu, B. Liang, Y. Huang, X. Ding, and J. Paisley, \"Lightweight pyramid networks for image deraining,\" IEEE Trans. Neural Netw. Learn. Syst., vol. 31, no. 6, pp. 1794-1807, Jun. 2020.\n\nWhen does a camera see rain?. K Garg, S K Nayar, Proc. 10th IEEE Int. Conf. Comput. Vis. 10th IEEE Int. Conf. Comput. VisK. Garg and S. K. Nayar, \"When does a camera see rain?,\" in Proc. 10th IEEE Int. Conf. Comput. Vis., 2005, 1067-1074.\n\nPhotorealistic rendering of rain streaks. K Garg, S K Nayar, ACM Trans. Graph. 253K. Garg and S. K. Nayar, \"Photorealistic rendering of rain streaks,\" ACM Trans. Graph., vol. 25, no. 3, pp. 996-1002, 2006.\n\nJoint convolutional analysis and synthesis sparse representation for single image layer separation. S Gu, D Meng, W Zuo, L Zhang, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisS. Gu, D. Meng, W. Zuo, and L. Zhang, \"Joint convolutional anal- ysis and synthesis sparse representation for single image layer separation,\" in Proc. IEEE Int. Conf. Comput. Vis., 2017, pp. 1708- 1716.\n\nDepth-attentional features for single-image rain removal. X Hu, C.-W Fu, L Zhu, P.-A Heng, Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. IEEE/CVF Conf. Comput. Vis. Pattern RecognitX. Hu, C.-W. Fu, L. Zhu, and P.-A. Heng, \"Depth-attentional fea- tures for single-image rain removal,\" in Proc. IEEE/CVF Conf. Com- put. Vis. Pattern Recognit., 2019, pp. 8022-8031.\n\nPerformance of versions having different channels and parameters. Fig, 18Fig. 18. Performance of versions having different channels and parameters.\n\nEnhanced intra prediction with recurrent neural network in video coding. Y Hu, W Yang, S Xia, W Cheng, J Liu, Proc. Data Compression Conf. Data Compression ConfY. Hu, W. Yang, S. Xia, W. Cheng, and J. Liu, \"Enhanced intra pre- diction with recurrent neural network in video coding,\" in Proc. Data Compression Conf., 2018, pp. 413-413.\n\nSelf-learning based image decomposition with applications to single image denoising. D.-A Huang, L.-W Kang, Y.-C F Wang, C.-W Lin, IEEE Trans. Multimedia. 161D.-A. Huang, L.-W. Kang, Y.-C. F. Wang, and C.-W. Lin, \"Self-learn- ing based image decomposition with applications to single image denoising,\" IEEE Trans. Multimedia, vol. 16, no. 1, pp. 83-93, Jan. 2014.\n\nContext-aware single image rain removal. D.-A Huang, L.-W Kang, M.-C Yang, C.-W Lin, Y.-C F Wang, Proc. IEEE Int. Conf. Multimedia Expo. IEEE Int. Conf. Multimedia ExpoD.-A. Huang, L.-W. Kang, M.-C. Yang, C.-W. Lin, and Y.-C. F. Wang, \"Context-aware single image rain removal,\" in Proc. IEEE Int. Conf. Multimedia Expo, 2012, pp. 164-169.\n\nScope of validity of PSNR in image/video quality assessment. Q Huynh-Thu, M Ghanbari, Electron. Lett. 4413Q. Huynh-Thu and M. Ghanbari, \"Scope of validity of PSNR in image/video quality assessment,\" Electron. Lett., vol. 44, no. 13, pp. 800-801, 2008.\n\nFastderain: A novel video rain streak removal method using directional gradient priors. T Jiang, T Huang, X Zhao, L Deng, Y Wang, IEEE Trans. Image Process. 284T. Jiang, T. Huang, X. Zhao, L. Deng, and Y. Wang, \"Fastderain: A novel video rain streak removal method using directional gradi- ent priors,\" IEEE Trans. Image Process., vol. 28, no. 4, pp. 2089- 2102, Apr. 2019.\n\nA novel tensor-based video rain streaks removal approach via utilizing discriminatively intrinsic priors. T.-X Jiang, T.-Z Huang, X.-L Zhao, L.-J Deng, Y Wang, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitT.-X. Jiang, T.-Z. Huang, X.-L. Zhao, L.-J. Deng, and Y. Wang, \"A novel tensor-based video rain streaks removal approach via utiliz- ing discriminatively intrinsic priors,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp. 4057-4066.\n\nAutomatic single-imagebased rain streaks removal via image decomposition. L W Kang, C W Lin, Y H Fu, IEEE Trans. Image Process. 214L. W. Kang, C. W. Lin, and Y. H. Fu, \"Automatic single-image- based rain streaks removal via image decomposition,\" IEEE Trans. Image Process., vol. 21, no. 4, pp. 1742-1755, Apr. 2012.\n\nVideo deraining and desnowing using temporal correlation and low-rank matrix completion. J H Kim, J Y Sim, C S Kim, IEEE Trans. Image Process. 249J. H. Kim, J. Y. Sim, and C. S. Kim, \"Video deraining and des- nowing using temporal correlation and low-rank matrix com- pletion,\" IEEE Trans. Image Process., vol. 24, no. 9, pp. 2658- 2670, Sep. 2015.\n\nEnd-to-end united video dehazing and detection. B Li, X Peng, Z Wang, J Xu, D Feng, Proc. AAAI Conf. AAAI ConfB. Li, X. Peng, Z. Wang, J. Xu, and D. Feng, \"End-to-end united video dehazing and detection,\" in Proc. AAAI Conf. Artif. Intell., 2018, pp. 7016-7023.\n\nVideo rain streak removal by multiscale convolutional sparse coding. M Li, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitM. Li et al., \"Video rain streak removal by multiscale convolu- tional sparse coding,\" in Proc. IEEE Conf. Comput. Vis. Pattern Rec- ognit., 2018, pp. 6644-6653.\n\nSingle Image deraining using scale-aware multi-stage recurrent network. R Li, L.-F Cheong, R T Tan, arXiv:1712.06830R. Li, L.-F. Cheong, and R. T. Tan, \"Single Image deraining using scale-aware multi-stage recurrent network,\" 2017, arXiv: 1712.06830.\n\nRain streak removal using layer priors. Y Li, R T Tan, X Guo, J Lu, M S Brown, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitY. Li, R. T. Tan, X. Guo, J. Lu, and M. S. Brown, \"Rain streak removal using layer priors,\" in Proc. IEEE Conf. Comput. Vis. Pat- tern Recognit., 2016, pp. 2736-2744.\n\nErase or fill? Deep joint recurrent rain removal and reconstruction in videos. J Liu, W Yang, S Yang, Z Guo, Proc. IEEE Conf. Comput. Vis. IEEE Conf. Comput. VisJ. Liu, W. Yang, S. Yang, and Z. Guo, \"Erase or fill? Deep joint recurrent rain removal and reconstruction in videos,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 3233-3242.\n\nD3R-Net: Dynamic routing residue recurrent network for video rain removal. J Liu, W Yang, S Yang, Z Guo, IEEE Trans. Image Process. 282J. Liu, W. Yang, S. Yang, and Z. Guo, \"D3R-Net: Dynamic routing residue recurrent network for video rain removal,\" IEEE Trans. Image Process., vol. 28, no. 2, pp. 699-712, Feb. 2019.\n\nRemoving rain from a single image via discriminative sparse coding. Y Luo, Y Xu, H Ji, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisY. Luo, Y. Xu, and H. Ji, \"Removing rain from a single image via discriminative sparse coding,\" in Proc. IEEE Int. Conf. Comput. Vis., 2015, pp. 3397-3405.\n\nAttentive generative adversarial network for raindrop removal from a single image. R Qian, R T Tan, W Yang, J Su, J Liu, Proc. IEEE Conf. Comput. Vis. IEEE Conf. Comput. VisR. Qian, R. T. Tan, W. Yang, J. Su, and J. Liu, \"Attentive generative adversarial network for raindrop removal from a single image,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2018, pp. 2482- 2491.\n\nProgressive image deraining networks: A better and simpler baseline. D Ren, W Zuo, Q Hu, P Zhu, D Meng, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitD. Ren, W. Zuo, Q. Hu, P. Zhu, and D. Meng, \"Progressive image deraining networks: A better and simpler baseline,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 3937-3946.\n\nVideo desnowing and deraining based on matrix decomposition. W Ren, J Tian, Z Han, A Chan, Y Tang, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitW. Ren, J. Tian, Z. Han, A. Chan, and Y. Tang, \"Video desnowing and deraining based on matrix decomposition,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp. 4210-4219.\n\nUtilizing local phase information to remove rain from video. V Santhaseelan, V K Asari, Int. J. Comput. Vis. 1121V. Santhaseelan and V. K. Asari, \"Utilizing local phase informa- tion to remove rain from video,\" Int. J. Comput. Vis., vol. 112, no. 1, pp. 71-89, 2015.\n\nSimulation of rain in videos. S Starik, M Werman, Proc. IEEE/CVF Int. Conf. Comput. Vis. Workshops. IEEE/CVF Int. Conf. Comput. Vis. WorkshopsS. Starik and M. Werman, \"Simulation of rain in videos,\" in Proc. IEEE/CVF Int. Conf. Comput. Vis. Workshops, 2003, pp. 406-409.\n\nExploiting image structural similarity for single image rain removal. S.-H Sun, S.-P Fan, Y.-C F Wang, Proc. IEEE Int. Conf. Image Process. IEEE Int. Conf. Image essS.-H. Sun, S.-P. Fan, and Y.-C. F. Wang, \"Exploiting image struc- tural similarity for single image rain removal,\" in Proc. IEEE Int. Conf. Image Process., 2014, pp. 4482-4486.\n\nA probabilistic approach for detection and removal of rain from videos. A K Tripathi, S Mukhopadhyay, IETE J. Res. 571A. K. Tripathi and S. Mukhopadhyay, \"A probabilistic approach for detection and removal of rain from videos,\" IETE J. Res., vol. 57, no. 1, pp. 82-91, 2011.\n\nVideo post-processing: Low-latency spatio-temporal approach for detection and removal of rain. A K Tripathi, S Mukhopadhyay, IET Image Process. 62A. K. Tripathi and S. Mukhopadhyay, \"Video post-processing: Low-latency spatio-temporal approach for detection and removal of rain,\" IET Image Process., vol. 6, no. 2, pp. 181-196, 2012.\n\nSpatial attentive single-image deraining with a high quality real rain dataset. T Wang, X Yang, K Xu, S Chen, Q Zhang, R W H Lau, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitT. Wang, X. Yang, K. Xu, S. Chen, Q. Zhang, and R. W. H. Lau, \"Spatial attentive single-image deraining with a high quality real rain dataset,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 12270-12279.\n\nImage quality assessment: From error visibility to structural similarity. Z Wang, A C Bovik, H R Sheikh, E P Simoncelli, IEEE Trans. Image Process. 134Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, \"Image quality assessment: From error visibility to structural similarity,\" IEEE Trans. Image Process., vol. 13, no. 4, pp. 600-612, Apr. 2004.\n\nShould we encode rain streaks in video as deterministic or stochastic?. W Wei, L Yi, Q Xie, Q Zhao, D Meng, Z Xu, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisW. Wei, L. Yi, Q. Xie, Q. Zhao, D. Meng, and Z. Xu, \"Should we encode rain streaks in video as deterministic or stochastic?,\" in Proc. IEEE Int. Conf. Comput. Vis., 2017, pp. 2516-2525.\n\nVideo enhancement with task-oriented flow. T Xue, B Chen, J Wu, D Wei, W T Freeman, Int. J. Comput. Vis. 1278T. Xue, B. Chen, J. Wu, D. Wei, and W. T. Freeman, \"Video enhancement with task-oriented flow,\" Int. J. Comput. Vis., vol. 127, no. 8, pp. 1106-1125, 2019.\n\nFrame-consistent recurrent video deraining with dual-level flow. W Yang, J Liu, J Feng, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitW. Yang, J. Liu, and J. Feng, \"Frame-consistent recurrent video deraining with dual-level flow,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 1661-1670.\n\nDeep joint rain detection and removal from a single image. W Yang, R T Tan, J Feng, J Liu, Z Guo, S Yan, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitW. Yang, R. T. Tan, J. Feng, J. Liu, Z. Guo, and S. Yan, \"Deep joint rain detection and removal from a single image,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2017, pp. 1357-1366.\n\nJoint rain detection and removal from a single image with contextualized deep networks. W Yang, R T Tan, J Feng, J Liu, S Yan, Z Guo, IEEE Trans. Pattern Anal. Mach. Intell. 426W. Yang, R. T. Tan, J. Feng, J. Liu, S. Yan, and Z. Guo, \"Joint rain detection and removal from a single image with contextualized deep networks,\" IEEE Trans. Pattern Anal. Mach. Intell., Vol. 42, no. 6, pp. 1377-1393, Jun. 2020.\n\nUncertainty guided multi-scale residual learning-using a cycle spinning CNN for single image de-raining. R Yasarla, V M Patel, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitR. Yasarla and V. M. Patel, \"Uncertainty guided multi-scale resid- ual learning-using a cycle spinning CNN for single image de-rain- ing,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2019, pp. 8405-8414.\n\nDensity-aware single image de-raining using a multi-stream dense network. H Zhang, V M Patel, Proc. IEEE Conf. Comput. Vis. IEEE Conf. Comput. VisH. Zhang and V. M. Patel, \"Density-aware single image de-rain- ing using a multi-stream dense network,\" in Proc. IEEE Conf. Com- put. Vis. Pattern Recognit., 2018, pp. 695-704.\n\nRain removal in video by combining temporal and chromatic properties. X Zhang, H Li, Y Qi, W K Leow, T K Ng, Proc. IEEE Int. Conf. Multimedia Expo. IEEE Int. Conf. Multimedia ExpoX. Zhang, H. Li, Y. Qi, W. K. Leow, and T. K. Ng, \"Rain removal in video by combining temporal and chromatic properties,\" in Proc. IEEE Int. Conf. Multimedia Expo, 2006, pp. 461-464.\n\nJoint bi-layer optimization for single-image rain streak removal. L Zhu, C Fu, D Lischinski, P Heng, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisL. Zhu, C. Fu, D. Lischinski, and P. Heng, \"Joint bi-layer optimiza- tion for single-image rain streak removal,\" in Proc. IEEE Int. Conf. Comput. Vis., 2017, pp. 2526-2534.\n\nSingle image haze removal using dark channel prior. K He, J Sun, X Tang, IEEE Trans. Pattern Anal. Mach. Intell. 3312K. He, J. Sun and X. Tang, \"Single image haze removal using dark channel prior,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 12, pp. 2341-2353, Dec. 2011.\n\nU-Net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, Proc. Int. Conf. Med. Image Comput. Comput.-Assisted Intervention. Int. Conf. Med. Image Comput. Comput.-Assisted InterventionO. Ronneberger, P. Fischer, and T. Brox, \"U-Net: Convolu- tional networks for biomedical image segmentation,\" in Proc. Int. Conf. Med. Image Comput. Comput.-Assisted Intervention, 2015, pp. 234-241.\n\nDeep video dehazing with semantic segmentation. W Ren, IEEE Trans. Image Process. 284W. Ren et al., \"Deep video dehazing with semantic segmentation,\" IEEE Trans. Image Process., vol. 28, no. 4, pp. 1895-1908, Apr. 2019.\n\nSimultaneous video defogging and stereo reconstruction. Z Li, P Tan, R Tan, D Zou, S Z Zhou, L Cheong, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitZ. Li, P. Tan, R. Tan, D. Zou, S. Z. Zhou, and L. Cheong, \"Simultaneous video defogging and stereo reconstruction,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2015, pp. 4988-4997.\n\nReal-time video dehazing based on spatiotemporal MRF. B Cai, X Xu, D Tao, Proc. Pacific Rim Conf. Multimedia. Pacific Rim Conf. MultimediaB. Cai, X. Xu, D. Tao, \"Real-time video dehazing based on spatio- temporal MRF,\" in Proc. Pacific Rim Conf. Multimedia, 2016, pp. 315-325.\n\nDelving deep into rectifiers: Surpassing human-level performance on ImageNet classification. K He, X Zhang, S Ren, J Sun, Proc. IEEE Int. Conf. Comput. Vis. IEEE Int. Conf. Comput. VisK. He, X. Zhang, S. Ren, and J. Sun, \"Delving deep into rectifiers: Surpassing human-level performance on ImageNet classi- fication,\" in Proc. IEEE Int. Conf. Comput. Vis., 2015, pp. 1026-1034.\n\nBenchmarking single-image dehazing and beyond. B Li, IEEE Trans. Image Process. 281B. Li et al., \"Benchmarking single-image dehazing and beyond,\" IEEE Trans. Image Process., vol. 28, no. 1, pp. 492-505, Jan. 2019.\n\nAre we ready for autonomous driving? The KITTI vision benchmark suite. A Geiger, P Lenz, R Urtasun, Proc. IEEE Conf. Comput. Vis. Pattern Recognit. IEEE Conf. Comput. Vis. Pattern RecognitA. Geiger, P. Lenz, and R. Urtasun, \"Are we ready for autono- mous driving? The KITTI vision benchmark suite,\" in Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2012, pp. 3354-3361.\n\nContour detection and hierarchical image segmentation. P Arbelaez, M Maire, C Fowlkes, J Malik, IEEE Trans. Pattern Anal. Mach. Intell. 335P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik, \"Contour detec- tion and hierarchical image segmentation,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 33, no. 5, 898-916, May 2011.\n\nHe is currently a postdoctoral research fellow at the Department of Computer Science, City University of Hong Kong. His current research interests include image/video processing/restoration, bad weather restoration and human-machine collaborative coding. Wenhan Yang, He has authored more than 100 technical articles in refereed journals and proceedings, and holds 9 granted patents. He received the IEEE ICME-2020 Best Paper Award, the IFTC 2017 Best Paper Award, and the IEEE CVPR. Beijing, ChinaUG2 Challenge First Runner-up Award. He was the Candidate of CSIG Best Doctoral Dissertation Award in 2019. He served as the area and session chair of IEEE. and the organizer of the IEEE CVPR-2019/2020/2021 UG2+ Challenge and WorkshopWenhan Yang (Member, IEEE) received the BS and PhD degrees (Hons.) in computer science from Peking University, Beijing, China, in 2012 and 2018. He is currently a postdoctoral research fellow at the Department of Computer Science, City University of Hong Kong. His current research interests include image/video processing/restoration, bad weather restoration and human-machine collaborative cod- ing. He has authored more than 100 technical articles in refereed journals and proceedings, and holds 9 granted patents. He received the IEEE ICME-2020 Best Paper Award, the IFTC 2017 Best Paper Award, and the IEEE CVPR-2018 UG2 Challenge First Runner-up Award. He was the Can- didate of CSIG Best Doctoral Dissertation Award in 2019. He served as the area and session chair of IEEE ICME-2021, and the organizer of the IEEE CVPR-2019/2020/2021 UG2+ Challenge and Workshop.\n", "annotations": {"author": "[{\"end\":112,\"start\":88},{\"end\":137,\"start\":113},{\"end\":162,\"start\":138},{\"end\":186,\"start\":163},{\"end\":197,\"start\":187},{\"end\":229,\"start\":198}]", "publisher": null, "author_last_name": "[{\"end\":111,\"start\":107},{\"end\":136,\"start\":133},{\"end\":161,\"start\":157},{\"end\":185,\"start\":181},{\"end\":196,\"start\":191},{\"end\":228,\"start\":225}]", "author_first_name": "[{\"end\":106,\"start\":100},{\"end\":130,\"start\":125},{\"end\":132,\"start\":131},{\"end\":156,\"start\":150},{\"end\":180,\"start\":175},{\"end\":190,\"start\":187},{\"end\":224,\"start\":217}]", "author_affiliation": null, "title": "[{\"end\":85,\"start\":1},{\"end\":314,\"start\":230}]", "venue": null, "abstract": "[{\"end\":2021,\"start\":428}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3053,\"start\":3049},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3059,\"start\":3055},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3065,\"start\":3061},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3071,\"start\":3067},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3142,\"start\":3138},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3170,\"start\":3166},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3199,\"start\":3195},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3221,\"start\":3218},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3227,\"start\":3223},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3259,\"start\":3256},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3264,\"start\":3261},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3270,\"start\":3266},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":3276,\"start\":3272},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3352,\"start\":3348},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3456,\"start\":3452},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3462,\"start\":3458},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3689,\"start\":3686},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3826,\"start\":3822},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3931,\"start\":3927},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3937,\"start\":3933},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4077,\"start\":4073},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4083,\"start\":4079},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6923,\"start\":6919},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6983,\"start\":6979},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7459,\"start\":7458},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8544,\"start\":8540},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8573,\"start\":8569},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8608,\"start\":8604},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8635,\"start\":8631},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8662,\"start\":8658},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8737,\"start\":8733},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8840,\"start\":8837},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8845,\"start\":8842},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9019,\"start\":9015},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9162,\"start\":9158},{\"end\":9226,\"start\":9224},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9287,\"start\":9283},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9483,\"start\":9480},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9716,\"start\":9712},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9901,\"start\":9897},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10056,\"start\":10052},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10599,\"start\":10595},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10639,\"start\":10635},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10810,\"start\":10807},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10838,\"start\":10835},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":10878,\"start\":10874},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10910,\"start\":10906},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10955,\"start\":10951},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11064,\"start\":11060},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11070,\"start\":11066},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11178,\"start\":11174},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11317,\"start\":11313},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11415,\"start\":11411},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11543,\"start\":11540},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11755,\"start\":11751},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11847,\"start\":11843},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":12221,\"start\":12217},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":12415,\"start\":12411},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12561,\"start\":12557},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":12782,\"start\":12778},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":14088,\"start\":14085},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":14094,\"start\":14090},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":14100,\"start\":14096},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14244,\"start\":14240},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":14250,\"start\":14246},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":14426,\"start\":14422},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":15311,\"start\":15307},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15664,\"start\":15660},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":15670,\"start\":15666},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":15676,\"start\":15672},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":16021,\"start\":16017},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":16643,\"start\":16639},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16649,\"start\":16645},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17681,\"start\":17678},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":19951,\"start\":19947},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":20258,\"start\":20254},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":21793,\"start\":21789},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":21818,\"start\":21814},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":22062,\"start\":22058},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":24219,\"start\":24215},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":25203,\"start\":25199},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":26018,\"start\":26014},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":26291,\"start\":26287},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":26527,\"start\":26523},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26533,\"start\":26529},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":26774,\"start\":26770},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":27199,\"start\":27195},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27321,\"start\":27318},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":28012,\"start\":28009},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":28053,\"start\":28049},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":28101,\"start\":28097},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":28146,\"start\":28142},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":28208,\"start\":28204},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":28239,\"start\":28235},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":28305,\"start\":28301},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":28349,\"start\":28345},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":28412,\"start\":28408},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":28429,\"start\":28425},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":28483,\"start\":28479},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":28540,\"start\":28537},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":28998,\"start\":28995},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":29071,\"start\":29067},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":30245,\"start\":30241},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":31116,\"start\":31112},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":31159,\"start\":31155},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":31756,\"start\":31752},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":32311,\"start\":32307},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":32326,\"start\":32323},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":32672,\"start\":32668},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":33104,\"start\":33100},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":33287,\"start\":33285},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":33289,\"start\":33287},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":33291,\"start\":33289},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":33293,\"start\":33291},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":33295,\"start\":33293},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":33298,\"start\":33295},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":33301,\"start\":33298},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":33304,\"start\":33301},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":33307,\"start\":33304},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":33310,\"start\":33307},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":33313,\"start\":33310},{\"end\":33329,\"start\":33313},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":33331,\"start\":33329},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":33336,\"start\":33331},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":34478,\"start\":34477},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":34527,\"start\":34526},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":34733,\"start\":34729},{\"end\":35349,\"start\":35300},{\"end\":35508,\"start\":35502},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":36523,\"start\":36519}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":40429,\"start\":40138},{\"attributes\":{\"id\":\"fig_1\"},\"end\":40510,\"start\":40430},{\"attributes\":{\"id\":\"fig_2\"},\"end\":41519,\"start\":40511},{\"attributes\":{\"id\":\"fig_3\"},\"end\":41645,\"start\":41520},{\"attributes\":{\"id\":\"fig_4\"},\"end\":41752,\"start\":41646},{\"attributes\":{\"id\":\"fig_5\"},\"end\":41926,\"start\":41753},{\"attributes\":{\"id\":\"fig_6\"},\"end\":42721,\"start\":41927},{\"attributes\":{\"id\":\"fig_7\"},\"end\":43040,\"start\":42722},{\"attributes\":{\"id\":\"fig_8\"},\"end\":43555,\"start\":43041},{\"attributes\":{\"id\":\"fig_9\"},\"end\":43964,\"start\":43556},{\"attributes\":{\"id\":\"fig_10\"},\"end\":44313,\"start\":43965},{\"attributes\":{\"id\":\"fig_11\"},\"end\":44540,\"start\":44314},{\"attributes\":{\"id\":\"fig_12\"},\"end\":44785,\"start\":44541},{\"attributes\":{\"id\":\"fig_13\"},\"end\":45028,\"start\":44786},{\"attributes\":{\"id\":\"fig_14\"},\"end\":45215,\"start\":45029},{\"attributes\":{\"id\":\"fig_15\"},\"end\":46290,\"start\":45216},{\"attributes\":{\"id\":\"fig_16\"},\"end\":47164,\"start\":46291},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":47407,\"start\":47165},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":49443,\"start\":47408},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":49567,\"start\":49444},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":49898,\"start\":49568},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":50157,\"start\":49899},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":50210,\"start\":50158},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":50372,\"start\":50211},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":51124,\"start\":50373}]", "paragraph": "[{\"end\":2946,\"start\":2041},{\"end\":3585,\"start\":2948},{\"end\":4699,\"start\":3587},{\"end\":5547,\"start\":4701},{\"end\":5596,\"start\":5549},{\"end\":6008,\"start\":5598},{\"end\":7186,\"start\":6010},{\"end\":7918,\"start\":7188},{\"end\":8270,\"start\":7920},{\"end\":9001,\"start\":8315},{\"end\":9695,\"start\":9003},{\"end\":10410,\"start\":9697},{\"end\":11055,\"start\":10444},{\"end\":12094,\"start\":11057},{\"end\":12911,\"start\":12096},{\"end\":13516,\"start\":12913},{\"end\":14427,\"start\":13545},{\"end\":15568,\"start\":14429},{\"end\":15677,\"start\":15570},{\"end\":15844,\"start\":15692},{\"end\":16110,\"start\":15887},{\"end\":16650,\"start\":16179},{\"end\":16855,\"start\":16698},{\"end\":17027,\"start\":16894},{\"end\":17887,\"start\":17029},{\"end\":18730,\"start\":17925},{\"end\":19343,\"start\":18755},{\"end\":20180,\"start\":19345},{\"end\":20852,\"start\":20210},{\"end\":21279,\"start\":20908},{\"end\":21586,\"start\":21335},{\"end\":21838,\"start\":21588},{\"end\":21850,\"start\":21840},{\"end\":21971,\"start\":21859},{\"end\":22109,\"start\":21973},{\"end\":22120,\"start\":22111},{\"end\":22490,\"start\":22164},{\"end\":22719,\"start\":22520},{\"end\":22896,\"start\":22833},{\"end\":23467,\"start\":22898},{\"end\":23591,\"start\":23500},{\"end\":23763,\"start\":23616},{\"end\":24058,\"start\":23852},{\"end\":24242,\"start\":24060},{\"end\":24617,\"start\":24323},{\"end\":24720,\"start\":24636},{\"end\":24810,\"start\":24787},{\"end\":25012,\"start\":24888},{\"end\":25305,\"start\":25048},{\"end\":25363,\"start\":25307},{\"end\":25710,\"start\":25398},{\"end\":26534,\"start\":25737},{\"end\":26921,\"start\":26536},{\"end\":27102,\"start\":26923},{\"end\":27899,\"start\":27104},{\"end\":29120,\"start\":27915},{\"end\":29880,\"start\":29122},{\"end\":31394,\"start\":29882},{\"end\":31852,\"start\":31396},{\"end\":32042,\"start\":31854},{\"end\":32226,\"start\":32095},{\"end\":33186,\"start\":32228},{\"end\":35887,\"start\":33188},{\"end\":36784,\"start\":35908},{\"end\":37045,\"start\":36786},{\"end\":38414,\"start\":37060},{\"end\":39552,\"start\":38538},{\"end\":40137,\"start\":39554}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15691,\"start\":15678},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15886,\"start\":15845},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16178,\"start\":16111},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16697,\"start\":16651},{\"attributes\":{\"id\":\"formula_4\"},\"end\":16893,\"start\":16856},{\"attributes\":{\"id\":\"formula_5\"},\"end\":20907,\"start\":20853},{\"attributes\":{\"id\":\"formula_6\"},\"end\":21334,\"start\":21280},{\"attributes\":{\"id\":\"formula_7\"},\"end\":22163,\"start\":22121},{\"attributes\":{\"id\":\"formula_8\"},\"end\":22832,\"start\":22720},{\"attributes\":{\"id\":\"formula_9\"},\"end\":23499,\"start\":23468},{\"attributes\":{\"id\":\"formula_10\"},\"end\":23851,\"start\":23764},{\"attributes\":{\"id\":\"formula_11\"},\"end\":24322,\"start\":24243},{\"attributes\":{\"id\":\"formula_12\"},\"end\":24786,\"start\":24721},{\"attributes\":{\"id\":\"formula_13\"},\"end\":24887,\"start\":24811},{\"attributes\":{\"id\":\"formula_14\"},\"end\":25397,\"start\":25364},{\"attributes\":{\"id\":\"formula_15\"},\"end\":25736,\"start\":25711},{\"attributes\":{\"id\":\"formula_16\"},\"end\":38537,\"start\":38415}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":17507,\"start\":17500},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":28604,\"start\":28597},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":29330,\"start\":29323},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":31431,\"start\":31424},{\"end\":32410,\"start\":32403},{\"end\":32947,\"start\":32940},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":34203,\"start\":34196},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":34860,\"start\":34853},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":34961,\"start\":34954},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":35109,\"start\":35102},{\"end\":35267,\"start\":35260},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":35958,\"start\":35951},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":36020,\"start\":36013}]", "section_header": "[{\"end\":2024,\"start\":2023},{\"attributes\":{\"n\":\"1\"},\"end\":2039,\"start\":2027},{\"attributes\":{\"n\":\"2\"},\"end\":8285,\"start\":8273},{\"attributes\":{\"n\":\"2.1\"},\"end\":8313,\"start\":8288},{\"attributes\":{\"n\":\"2.2\"},\"end\":10442,\"start\":10413},{\"attributes\":{\"n\":\"3\"},\"end\":13543,\"start\":13519},{\"attributes\":{\"n\":\"4\"},\"end\":17923,\"start\":17890},{\"attributes\":{\"n\":\"4.1\"},\"end\":18753,\"start\":18733},{\"attributes\":{\"n\":\"4.2\"},\"end\":20208,\"start\":20183},{\"attributes\":{\"n\":\"2018\"},\"end\":21857,\"start\":21853},{\"attributes\":{\"n\":\"4.4\"},\"end\":22518,\"start\":22493},{\"attributes\":{\"n\":\"4.4.1\"},\"end\":23614,\"start\":23594},{\"attributes\":{\"n\":\"4.4.2\"},\"end\":24634,\"start\":24620},{\"attributes\":{\"n\":\"5\"},\"end\":25035,\"start\":25015},{\"attributes\":{\"n\":\"5.1\"},\"end\":25046,\"start\":25038},{\"attributes\":{\"n\":\"5.2\"},\"end\":27913,\"start\":27902},{\"end\":32093,\"start\":32045},{\"attributes\":{\"n\":\"5.3\"},\"end\":35906,\"start\":35890},{\"attributes\":{\"n\":\"6\"},\"end\":37058,\"start\":37048},{\"end\":40147,\"start\":40139},{\"end\":40439,\"start\":40431},{\"end\":40520,\"start\":40512},{\"end\":41529,\"start\":41521},{\"end\":41655,\"start\":41647},{\"end\":41762,\"start\":41754},{\"end\":41936,\"start\":41928},{\"end\":42732,\"start\":42723},{\"end\":43051,\"start\":43042},{\"end\":43566,\"start\":43557},{\"end\":43975,\"start\":43966},{\"end\":44324,\"start\":44315},{\"end\":44551,\"start\":44542},{\"end\":44796,\"start\":44787},{\"end\":45039,\"start\":45030},{\"end\":45222,\"start\":45217},{\"end\":47416,\"start\":47409},{\"end\":49465,\"start\":49445},{\"end\":49584,\"start\":49569},{\"end\":49917,\"start\":49900},{\"end\":50175,\"start\":50159},{\"end\":50381,\"start\":50374}]", "table": "[{\"end\":49443,\"start\":47468},{\"end\":49898,\"start\":49697},{\"end\":50157,\"start\":49983},{\"end\":50372,\"start\":50293}]", "figure_caption": "[{\"end\":40429,\"start\":40149},{\"end\":40510,\"start\":40441},{\"end\":41519,\"start\":40522},{\"end\":41645,\"start\":41531},{\"end\":41752,\"start\":41657},{\"end\":41926,\"start\":41764},{\"end\":42721,\"start\":41938},{\"end\":43040,\"start\":42735},{\"end\":43555,\"start\":43054},{\"end\":43964,\"start\":43569},{\"end\":44313,\"start\":43978},{\"end\":44540,\"start\":44327},{\"end\":44785,\"start\":44554},{\"end\":45028,\"start\":44799},{\"end\":45215,\"start\":45042},{\"end\":46290,\"start\":45223},{\"end\":47164,\"start\":46293},{\"end\":47407,\"start\":47167},{\"end\":47468,\"start\":47418},{\"end\":49567,\"start\":49467},{\"end\":49697,\"start\":49586},{\"end\":49983,\"start\":49919},{\"end\":50210,\"start\":50177},{\"end\":50293,\"start\":50213},{\"end\":51124,\"start\":50383}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4921,\"start\":4915},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13988,\"start\":13981},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14344,\"start\":14337},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14605,\"start\":14598},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15338,\"start\":15331},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17207,\"start\":17201},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18956,\"start\":18950},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20297,\"start\":20291},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20798,\"start\":20791},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20972,\"start\":20965},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21063,\"start\":21056},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21083,\"start\":21077},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23370,\"start\":23364},{\"end\":33567,\"start\":33556},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33773,\"start\":33765},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33878,\"start\":33867},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34013,\"start\":34002},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36534,\"start\":36527},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36678,\"start\":36671},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36783,\"start\":36776},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36912,\"start\":36905}]", "bib_author_first_name": "[{\"end\":51953,\"start\":51952},{\"end\":51955,\"start\":51954},{\"end\":51965,\"start\":51964},{\"end\":51979,\"start\":51978},{\"end\":52264,\"start\":52263},{\"end\":52273,\"start\":52272},{\"end\":52285,\"start\":52281},{\"end\":52570,\"start\":52569},{\"end\":52577,\"start\":52576},{\"end\":52583,\"start\":52582},{\"end\":52882,\"start\":52881},{\"end\":52893,\"start\":52889},{\"end\":52900,\"start\":52899},{\"end\":52910,\"start\":52906},{\"end\":52918,\"start\":52917},{\"end\":53238,\"start\":53237},{\"end\":53246,\"start\":53245},{\"end\":53254,\"start\":53253},{\"end\":53256,\"start\":53255},{\"end\":53263,\"start\":53262},{\"end\":53559,\"start\":53558},{\"end\":53567,\"start\":53566},{\"end\":53569,\"start\":53568},{\"end\":53576,\"start\":53575},{\"end\":53582,\"start\":53581},{\"end\":53894,\"start\":53893},{\"end\":53900,\"start\":53899},{\"end\":53909,\"start\":53908},{\"end\":53917,\"start\":53916},{\"end\":53925,\"start\":53924},{\"end\":54231,\"start\":54230},{\"end\":54237,\"start\":54236},{\"end\":54246,\"start\":54245},{\"end\":54254,\"start\":54253},{\"end\":54263,\"start\":54262},{\"end\":54271,\"start\":54270},{\"end\":54602,\"start\":54601},{\"end\":54608,\"start\":54607},{\"end\":54617,\"start\":54616},{\"end\":54626,\"start\":54625},{\"end\":54634,\"start\":54633},{\"end\":54902,\"start\":54901},{\"end\":54910,\"start\":54909},{\"end\":54912,\"start\":54911},{\"end\":55154,\"start\":55153},{\"end\":55162,\"start\":55161},{\"end\":55164,\"start\":55163},{\"end\":55419,\"start\":55418},{\"end\":55425,\"start\":55424},{\"end\":55433,\"start\":55432},{\"end\":55440,\"start\":55439},{\"end\":55773,\"start\":55772},{\"end\":55782,\"start\":55778},{\"end\":55788,\"start\":55787},{\"end\":55798,\"start\":55794},{\"end\":56307,\"start\":56306},{\"end\":56313,\"start\":56312},{\"end\":56321,\"start\":56320},{\"end\":56328,\"start\":56327},{\"end\":56337,\"start\":56336},{\"end\":56658,\"start\":56654},{\"end\":56670,\"start\":56666},{\"end\":56681,\"start\":56677},{\"end\":56683,\"start\":56682},{\"end\":56694,\"start\":56690},{\"end\":56979,\"start\":56975},{\"end\":56991,\"start\":56987},{\"end\":57002,\"start\":56998},{\"end\":57013,\"start\":57009},{\"end\":57023,\"start\":57019},{\"end\":57025,\"start\":57024},{\"end\":57336,\"start\":57335},{\"end\":57349,\"start\":57348},{\"end\":57616,\"start\":57615},{\"end\":57625,\"start\":57624},{\"end\":57634,\"start\":57633},{\"end\":57642,\"start\":57641},{\"end\":57650,\"start\":57649},{\"end\":58012,\"start\":58008},{\"end\":58024,\"start\":58020},{\"end\":58036,\"start\":58032},{\"end\":58047,\"start\":58043},{\"end\":58055,\"start\":58054},{\"end\":58472,\"start\":58471},{\"end\":58474,\"start\":58473},{\"end\":58482,\"start\":58481},{\"end\":58484,\"start\":58483},{\"end\":58491,\"start\":58490},{\"end\":58493,\"start\":58492},{\"end\":58804,\"start\":58803},{\"end\":58806,\"start\":58805},{\"end\":58813,\"start\":58812},{\"end\":58815,\"start\":58814},{\"end\":58822,\"start\":58821},{\"end\":58824,\"start\":58823},{\"end\":59113,\"start\":59112},{\"end\":59119,\"start\":59118},{\"end\":59127,\"start\":59126},{\"end\":59135,\"start\":59134},{\"end\":59141,\"start\":59140},{\"end\":59397,\"start\":59396},{\"end\":59726,\"start\":59725},{\"end\":59735,\"start\":59731},{\"end\":59745,\"start\":59744},{\"end\":59747,\"start\":59746},{\"end\":59946,\"start\":59945},{\"end\":59952,\"start\":59951},{\"end\":59954,\"start\":59953},{\"end\":59961,\"start\":59960},{\"end\":59968,\"start\":59967},{\"end\":59974,\"start\":59973},{\"end\":59976,\"start\":59975},{\"end\":60320,\"start\":60319},{\"end\":60327,\"start\":60326},{\"end\":60335,\"start\":60334},{\"end\":60343,\"start\":60342},{\"end\":60670,\"start\":60669},{\"end\":60677,\"start\":60676},{\"end\":60685,\"start\":60684},{\"end\":60693,\"start\":60692},{\"end\":60982,\"start\":60981},{\"end\":60989,\"start\":60988},{\"end\":60995,\"start\":60994},{\"end\":61303,\"start\":61302},{\"end\":61311,\"start\":61310},{\"end\":61313,\"start\":61312},{\"end\":61320,\"start\":61319},{\"end\":61328,\"start\":61327},{\"end\":61334,\"start\":61333},{\"end\":61670,\"start\":61669},{\"end\":61677,\"start\":61676},{\"end\":61684,\"start\":61683},{\"end\":61690,\"start\":61689},{\"end\":61697,\"start\":61696},{\"end\":62043,\"start\":62042},{\"end\":62050,\"start\":62049},{\"end\":62058,\"start\":62057},{\"end\":62065,\"start\":62064},{\"end\":62073,\"start\":62072},{\"end\":62414,\"start\":62413},{\"end\":62430,\"start\":62429},{\"end\":62432,\"start\":62431},{\"end\":62651,\"start\":62650},{\"end\":62661,\"start\":62660},{\"end\":62966,\"start\":62962},{\"end\":62976,\"start\":62972},{\"end\":62986,\"start\":62982},{\"end\":62988,\"start\":62987},{\"end\":63308,\"start\":63307},{\"end\":63310,\"start\":63309},{\"end\":63322,\"start\":63321},{\"end\":63607,\"start\":63606},{\"end\":63609,\"start\":63608},{\"end\":63621,\"start\":63620},{\"end\":63926,\"start\":63925},{\"end\":63934,\"start\":63933},{\"end\":63942,\"start\":63941},{\"end\":63948,\"start\":63947},{\"end\":63956,\"start\":63955},{\"end\":63965,\"start\":63964},{\"end\":63969,\"start\":63966},{\"end\":64358,\"start\":64357},{\"end\":64366,\"start\":64365},{\"end\":64368,\"start\":64367},{\"end\":64377,\"start\":64376},{\"end\":64379,\"start\":64378},{\"end\":64389,\"start\":64388},{\"end\":64391,\"start\":64390},{\"end\":64710,\"start\":64709},{\"end\":64717,\"start\":64716},{\"end\":64723,\"start\":64722},{\"end\":64730,\"start\":64729},{\"end\":64738,\"start\":64737},{\"end\":64746,\"start\":64745},{\"end\":65044,\"start\":65043},{\"end\":65051,\"start\":65050},{\"end\":65059,\"start\":65058},{\"end\":65065,\"start\":65064},{\"end\":65072,\"start\":65071},{\"end\":65074,\"start\":65073},{\"end\":65332,\"start\":65331},{\"end\":65340,\"start\":65339},{\"end\":65347,\"start\":65346},{\"end\":65673,\"start\":65672},{\"end\":65681,\"start\":65680},{\"end\":65683,\"start\":65682},{\"end\":65690,\"start\":65689},{\"end\":65698,\"start\":65697},{\"end\":65705,\"start\":65704},{\"end\":65712,\"start\":65711},{\"end\":66087,\"start\":66086},{\"end\":66095,\"start\":66094},{\"end\":66097,\"start\":66096},{\"end\":66104,\"start\":66103},{\"end\":66112,\"start\":66111},{\"end\":66119,\"start\":66118},{\"end\":66126,\"start\":66125},{\"end\":66512,\"start\":66511},{\"end\":66523,\"start\":66522},{\"end\":66525,\"start\":66524},{\"end\":66909,\"start\":66908},{\"end\":66918,\"start\":66917},{\"end\":66920,\"start\":66919},{\"end\":67229,\"start\":67228},{\"end\":67238,\"start\":67237},{\"end\":67244,\"start\":67243},{\"end\":67250,\"start\":67249},{\"end\":67252,\"start\":67251},{\"end\":67260,\"start\":67259},{\"end\":67262,\"start\":67261},{\"end\":67588,\"start\":67587},{\"end\":67595,\"start\":67594},{\"end\":67601,\"start\":67600},{\"end\":67615,\"start\":67614},{\"end\":67911,\"start\":67910},{\"end\":67917,\"start\":67916},{\"end\":67924,\"start\":67923},{\"end\":68207,\"start\":68206},{\"end\":68222,\"start\":68221},{\"end\":68233,\"start\":68232},{\"end\":68615,\"start\":68614},{\"end\":68844,\"start\":68843},{\"end\":68850,\"start\":68849},{\"end\":68857,\"start\":68856},{\"end\":68864,\"start\":68863},{\"end\":68871,\"start\":68870},{\"end\":68873,\"start\":68872},{\"end\":68881,\"start\":68880},{\"end\":69223,\"start\":69222},{\"end\":69230,\"start\":69229},{\"end\":69236,\"start\":69235},{\"end\":69540,\"start\":69539},{\"end\":69546,\"start\":69545},{\"end\":69555,\"start\":69554},{\"end\":69562,\"start\":69561},{\"end\":69873,\"start\":69872},{\"end\":70112,\"start\":70111},{\"end\":70122,\"start\":70121},{\"end\":70130,\"start\":70129},{\"end\":70469,\"start\":70468},{\"end\":70481,\"start\":70480},{\"end\":70490,\"start\":70489},{\"end\":70501,\"start\":70500},{\"end\":70998,\"start\":70992}]", "bib_author_last_name": "[{\"end\":51962,\"start\":51956},{\"end\":51976,\"start\":51966},{\"end\":51986,\"start\":51980},{\"end\":52270,\"start\":52265},{\"end\":52279,\"start\":52274},{\"end\":52291,\"start\":52286},{\"end\":52574,\"start\":52571},{\"end\":52580,\"start\":52578},{\"end\":52587,\"start\":52584},{\"end\":52887,\"start\":52883},{\"end\":52897,\"start\":52894},{\"end\":52904,\"start\":52901},{\"end\":52915,\"start\":52911},{\"end\":52921,\"start\":52919},{\"end\":53243,\"start\":53239},{\"end\":53251,\"start\":53247},{\"end\":53260,\"start\":53257},{\"end\":53268,\"start\":53264},{\"end\":53564,\"start\":53560},{\"end\":53573,\"start\":53570},{\"end\":53579,\"start\":53577},{\"end\":53587,\"start\":53583},{\"end\":53897,\"start\":53895},{\"end\":53906,\"start\":53901},{\"end\":53914,\"start\":53910},{\"end\":53922,\"start\":53918},{\"end\":53933,\"start\":53926},{\"end\":54234,\"start\":54232},{\"end\":54243,\"start\":54238},{\"end\":54251,\"start\":54247},{\"end\":54260,\"start\":54255},{\"end\":54268,\"start\":54264},{\"end\":54279,\"start\":54272},{\"end\":54605,\"start\":54603},{\"end\":54614,\"start\":54609},{\"end\":54623,\"start\":54618},{\"end\":54631,\"start\":54627},{\"end\":54642,\"start\":54635},{\"end\":54907,\"start\":54903},{\"end\":54918,\"start\":54913},{\"end\":55159,\"start\":55155},{\"end\":55170,\"start\":55165},{\"end\":55422,\"start\":55420},{\"end\":55430,\"start\":55426},{\"end\":55437,\"start\":55434},{\"end\":55446,\"start\":55441},{\"end\":55776,\"start\":55774},{\"end\":55785,\"start\":55783},{\"end\":55792,\"start\":55789},{\"end\":55803,\"start\":55799},{\"end\":56153,\"start\":56150},{\"end\":56310,\"start\":56308},{\"end\":56318,\"start\":56314},{\"end\":56325,\"start\":56322},{\"end\":56334,\"start\":56329},{\"end\":56341,\"start\":56338},{\"end\":56664,\"start\":56659},{\"end\":56675,\"start\":56671},{\"end\":56688,\"start\":56684},{\"end\":56698,\"start\":56695},{\"end\":56985,\"start\":56980},{\"end\":56996,\"start\":56992},{\"end\":57007,\"start\":57003},{\"end\":57017,\"start\":57014},{\"end\":57030,\"start\":57026},{\"end\":57346,\"start\":57337},{\"end\":57358,\"start\":57350},{\"end\":57622,\"start\":57617},{\"end\":57631,\"start\":57626},{\"end\":57639,\"start\":57635},{\"end\":57647,\"start\":57643},{\"end\":57655,\"start\":57651},{\"end\":58018,\"start\":58013},{\"end\":58030,\"start\":58025},{\"end\":58041,\"start\":58037},{\"end\":58052,\"start\":58048},{\"end\":58060,\"start\":58056},{\"end\":58479,\"start\":58475},{\"end\":58488,\"start\":58485},{\"end\":58496,\"start\":58494},{\"end\":58810,\"start\":58807},{\"end\":58819,\"start\":58816},{\"end\":58828,\"start\":58825},{\"end\":59116,\"start\":59114},{\"end\":59124,\"start\":59120},{\"end\":59132,\"start\":59128},{\"end\":59138,\"start\":59136},{\"end\":59146,\"start\":59142},{\"end\":59400,\"start\":59398},{\"end\":59729,\"start\":59727},{\"end\":59742,\"start\":59736},{\"end\":59751,\"start\":59748},{\"end\":59949,\"start\":59947},{\"end\":59958,\"start\":59955},{\"end\":59965,\"start\":59962},{\"end\":59971,\"start\":59969},{\"end\":59982,\"start\":59977},{\"end\":60324,\"start\":60321},{\"end\":60332,\"start\":60328},{\"end\":60340,\"start\":60336},{\"end\":60347,\"start\":60344},{\"end\":60674,\"start\":60671},{\"end\":60682,\"start\":60678},{\"end\":60690,\"start\":60686},{\"end\":60697,\"start\":60694},{\"end\":60986,\"start\":60983},{\"end\":60992,\"start\":60990},{\"end\":60998,\"start\":60996},{\"end\":61308,\"start\":61304},{\"end\":61317,\"start\":61314},{\"end\":61325,\"start\":61321},{\"end\":61331,\"start\":61329},{\"end\":61338,\"start\":61335},{\"end\":61674,\"start\":61671},{\"end\":61681,\"start\":61678},{\"end\":61687,\"start\":61685},{\"end\":61694,\"start\":61691},{\"end\":61702,\"start\":61698},{\"end\":62047,\"start\":62044},{\"end\":62055,\"start\":62051},{\"end\":62062,\"start\":62059},{\"end\":62070,\"start\":62066},{\"end\":62078,\"start\":62074},{\"end\":62427,\"start\":62415},{\"end\":62438,\"start\":62433},{\"end\":62658,\"start\":62652},{\"end\":62668,\"start\":62662},{\"end\":62970,\"start\":62967},{\"end\":62980,\"start\":62977},{\"end\":62993,\"start\":62989},{\"end\":63319,\"start\":63311},{\"end\":63335,\"start\":63323},{\"end\":63618,\"start\":63610},{\"end\":63634,\"start\":63622},{\"end\":63931,\"start\":63927},{\"end\":63939,\"start\":63935},{\"end\":63945,\"start\":63943},{\"end\":63953,\"start\":63949},{\"end\":63962,\"start\":63957},{\"end\":63973,\"start\":63970},{\"end\":64363,\"start\":64359},{\"end\":64374,\"start\":64369},{\"end\":64386,\"start\":64380},{\"end\":64402,\"start\":64392},{\"end\":64714,\"start\":64711},{\"end\":64720,\"start\":64718},{\"end\":64727,\"start\":64724},{\"end\":64735,\"start\":64731},{\"end\":64743,\"start\":64739},{\"end\":64749,\"start\":64747},{\"end\":65048,\"start\":65045},{\"end\":65056,\"start\":65052},{\"end\":65062,\"start\":65060},{\"end\":65069,\"start\":65066},{\"end\":65082,\"start\":65075},{\"end\":65337,\"start\":65333},{\"end\":65344,\"start\":65341},{\"end\":65352,\"start\":65348},{\"end\":65678,\"start\":65674},{\"end\":65687,\"start\":65684},{\"end\":65695,\"start\":65691},{\"end\":65702,\"start\":65699},{\"end\":65709,\"start\":65706},{\"end\":65716,\"start\":65713},{\"end\":66092,\"start\":66088},{\"end\":66101,\"start\":66098},{\"end\":66109,\"start\":66105},{\"end\":66116,\"start\":66113},{\"end\":66123,\"start\":66120},{\"end\":66130,\"start\":66127},{\"end\":66520,\"start\":66513},{\"end\":66531,\"start\":66526},{\"end\":66915,\"start\":66910},{\"end\":66926,\"start\":66921},{\"end\":67235,\"start\":67230},{\"end\":67241,\"start\":67239},{\"end\":67247,\"start\":67245},{\"end\":67257,\"start\":67253},{\"end\":67265,\"start\":67263},{\"end\":67592,\"start\":67589},{\"end\":67598,\"start\":67596},{\"end\":67612,\"start\":67602},{\"end\":67620,\"start\":67616},{\"end\":67914,\"start\":67912},{\"end\":67921,\"start\":67918},{\"end\":67929,\"start\":67925},{\"end\":68219,\"start\":68208},{\"end\":68230,\"start\":68223},{\"end\":68238,\"start\":68234},{\"end\":68619,\"start\":68616},{\"end\":68847,\"start\":68845},{\"end\":68854,\"start\":68851},{\"end\":68861,\"start\":68858},{\"end\":68868,\"start\":68865},{\"end\":68878,\"start\":68874},{\"end\":68888,\"start\":68882},{\"end\":69227,\"start\":69224},{\"end\":69233,\"start\":69231},{\"end\":69240,\"start\":69237},{\"end\":69543,\"start\":69541},{\"end\":69552,\"start\":69547},{\"end\":69559,\"start\":69556},{\"end\":69566,\"start\":69563},{\"end\":69876,\"start\":69874},{\"end\":70119,\"start\":70113},{\"end\":70127,\"start\":70123},{\"end\":70138,\"start\":70131},{\"end\":70478,\"start\":70470},{\"end\":70487,\"start\":70482},{\"end\":70498,\"start\":70491},{\"end\":70507,\"start\":70502},{\"end\":71003,\"start\":70999}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":18178173},\"end\":52165,\"start\":51906},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":207252112},\"end\":52512,\"start\":52167},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":25510878},\"end\":52794,\"start\":52514},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":4395980},\"end\":53170,\"start\":52796},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":9569924},\"end\":53498,\"start\":53172},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":6593498},\"end\":53812,\"start\":53500},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":1372266},\"end\":54168,\"start\":53814},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":17115407},\"end\":54549,\"start\":54170},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":21712570},\"end\":54869,\"start\":54551},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":206769439},\"end\":55109,\"start\":54871},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":13938928},\"end\":55316,\"start\":55111},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":21794673},\"end\":55712,\"start\":55318},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":195654199},\"end\":56082,\"start\":55714},{\"attributes\":{\"id\":\"b13\"},\"end\":56231,\"start\":56084},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":51715023},\"end\":56567,\"start\":56233},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":6026136},\"end\":56932,\"start\":56569},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":18720628},\"end\":57272,\"start\":56934},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":62732555},\"end\":57525,\"start\":57274},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":4000182},\"end\":57900,\"start\":57527},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":4910523},\"end\":58395,\"start\":57902},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":248403522},\"end\":58712,\"start\":58397},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":11946697},\"end\":59062,\"start\":58714},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":19184597},\"end\":59325,\"start\":59064},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":52838862},\"end\":59651,\"start\":59327},{\"attributes\":{\"doi\":\"arXiv:1712.06830\",\"id\":\"b24\"},\"end\":59903,\"start\":59653},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":9007541},\"end\":60238,\"start\":59905},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":52839214},\"end\":60592,\"start\":60240},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":52284167},\"end\":60911,\"start\":60594},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":14460720},\"end\":61217,\"start\":60913},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":4539586},\"end\":61598,\"start\":61219},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":59316941},\"end\":61979,\"start\":61600},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":22540825},\"end\":62350,\"start\":61981},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":15886012},\"end\":62618,\"start\":62352},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":1305628},\"end\":62890,\"start\":62620},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":2242797},\"end\":63233,\"start\":62892},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":124393911},\"end\":63509,\"start\":63235},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":120997118},\"end\":63843,\"start\":63511},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":91184545},\"end\":64281,\"start\":63845},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":207761262},\"end\":64635,\"start\":64283},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":24803154},\"end\":64998,\"start\":64637},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":40412298},\"end\":65264,\"start\":65000},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":195493489},\"end\":65611,\"start\":65266},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":15443600},\"end\":65996,\"start\":65613},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":73439498},\"end\":66404,\"start\":65998},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":195657934},\"end\":66832,\"start\":66406},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":3406592},\"end\":67156,\"start\":66834},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":206818050},\"end\":67519,\"start\":67158},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":5874323},\"end\":67856,\"start\":67521},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":49333383},\"end\":68139,\"start\":67858},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":3719281},\"end\":68564,\"start\":68141},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":52988330},\"end\":68785,\"start\":68566},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":10360815},\"end\":69166,\"start\":68787},{\"attributes\":{\"id\":\"b52\"},\"end\":69444,\"start\":69168},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":13740328},\"end\":69823,\"start\":69446},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":39760169},\"end\":70038,\"start\":69825},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":6724907},\"end\":70411,\"start\":70040},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":206764694},\"end\":70735,\"start\":70413},{\"attributes\":{\"id\":\"b57\"},\"end\":72338,\"start\":70737}]", "bib_title": "[{\"end\":51950,\"start\":51906},{\"end\":52261,\"start\":52167},{\"end\":52567,\"start\":52514},{\"end\":52879,\"start\":52796},{\"end\":53235,\"start\":53172},{\"end\":53556,\"start\":53500},{\"end\":53891,\"start\":53814},{\"end\":54228,\"start\":54170},{\"end\":54599,\"start\":54551},{\"end\":54899,\"start\":54871},{\"end\":55151,\"start\":55111},{\"end\":55416,\"start\":55318},{\"end\":55770,\"start\":55714},{\"end\":56304,\"start\":56233},{\"end\":56652,\"start\":56569},{\"end\":56973,\"start\":56934},{\"end\":57333,\"start\":57274},{\"end\":57613,\"start\":57527},{\"end\":58006,\"start\":57902},{\"end\":58469,\"start\":58397},{\"end\":58801,\"start\":58714},{\"end\":59110,\"start\":59064},{\"end\":59394,\"start\":59327},{\"end\":59943,\"start\":59905},{\"end\":60317,\"start\":60240},{\"end\":60667,\"start\":60594},{\"end\":60979,\"start\":60913},{\"end\":61300,\"start\":61219},{\"end\":61667,\"start\":61600},{\"end\":62040,\"start\":61981},{\"end\":62411,\"start\":62352},{\"end\":62648,\"start\":62620},{\"end\":62960,\"start\":62892},{\"end\":63305,\"start\":63235},{\"end\":63604,\"start\":63511},{\"end\":63923,\"start\":63845},{\"end\":64355,\"start\":64283},{\"end\":64707,\"start\":64637},{\"end\":65041,\"start\":65000},{\"end\":65329,\"start\":65266},{\"end\":65670,\"start\":65613},{\"end\":66084,\"start\":65998},{\"end\":66509,\"start\":66406},{\"end\":66906,\"start\":66834},{\"end\":67226,\"start\":67158},{\"end\":67585,\"start\":67521},{\"end\":67908,\"start\":67858},{\"end\":68204,\"start\":68141},{\"end\":68612,\"start\":68566},{\"end\":68841,\"start\":68787},{\"end\":69220,\"start\":69168},{\"end\":69537,\"start\":69446},{\"end\":69870,\"start\":69825},{\"end\":70109,\"start\":70040},{\"end\":70466,\"start\":70413},{\"end\":70990,\"start\":70737}]", "bib_author": "[{\"end\":51964,\"start\":51952},{\"end\":51978,\"start\":51964},{\"end\":51988,\"start\":51978},{\"end\":52272,\"start\":52263},{\"end\":52281,\"start\":52272},{\"end\":52293,\"start\":52281},{\"end\":52576,\"start\":52569},{\"end\":52582,\"start\":52576},{\"end\":52589,\"start\":52582},{\"end\":52889,\"start\":52881},{\"end\":52899,\"start\":52889},{\"end\":52906,\"start\":52899},{\"end\":52917,\"start\":52906},{\"end\":52923,\"start\":52917},{\"end\":53245,\"start\":53237},{\"end\":53253,\"start\":53245},{\"end\":53262,\"start\":53253},{\"end\":53270,\"start\":53262},{\"end\":53566,\"start\":53558},{\"end\":53575,\"start\":53566},{\"end\":53581,\"start\":53575},{\"end\":53589,\"start\":53581},{\"end\":53899,\"start\":53893},{\"end\":53908,\"start\":53899},{\"end\":53916,\"start\":53908},{\"end\":53924,\"start\":53916},{\"end\":53935,\"start\":53924},{\"end\":54236,\"start\":54230},{\"end\":54245,\"start\":54236},{\"end\":54253,\"start\":54245},{\"end\":54262,\"start\":54253},{\"end\":54270,\"start\":54262},{\"end\":54281,\"start\":54270},{\"end\":54607,\"start\":54601},{\"end\":54616,\"start\":54607},{\"end\":54625,\"start\":54616},{\"end\":54633,\"start\":54625},{\"end\":54644,\"start\":54633},{\"end\":54909,\"start\":54901},{\"end\":54920,\"start\":54909},{\"end\":55161,\"start\":55153},{\"end\":55172,\"start\":55161},{\"end\":55424,\"start\":55418},{\"end\":55432,\"start\":55424},{\"end\":55439,\"start\":55432},{\"end\":55448,\"start\":55439},{\"end\":55778,\"start\":55772},{\"end\":55787,\"start\":55778},{\"end\":55794,\"start\":55787},{\"end\":55805,\"start\":55794},{\"end\":56155,\"start\":56150},{\"end\":56312,\"start\":56306},{\"end\":56320,\"start\":56312},{\"end\":56327,\"start\":56320},{\"end\":56336,\"start\":56327},{\"end\":56343,\"start\":56336},{\"end\":56666,\"start\":56654},{\"end\":56677,\"start\":56666},{\"end\":56690,\"start\":56677},{\"end\":56700,\"start\":56690},{\"end\":56987,\"start\":56975},{\"end\":56998,\"start\":56987},{\"end\":57009,\"start\":56998},{\"end\":57019,\"start\":57009},{\"end\":57032,\"start\":57019},{\"end\":57348,\"start\":57335},{\"end\":57360,\"start\":57348},{\"end\":57624,\"start\":57615},{\"end\":57633,\"start\":57624},{\"end\":57641,\"start\":57633},{\"end\":57649,\"start\":57641},{\"end\":57657,\"start\":57649},{\"end\":58020,\"start\":58008},{\"end\":58032,\"start\":58020},{\"end\":58043,\"start\":58032},{\"end\":58054,\"start\":58043},{\"end\":58062,\"start\":58054},{\"end\":58481,\"start\":58471},{\"end\":58490,\"start\":58481},{\"end\":58498,\"start\":58490},{\"end\":58812,\"start\":58803},{\"end\":58821,\"start\":58812},{\"end\":58830,\"start\":58821},{\"end\":59118,\"start\":59112},{\"end\":59126,\"start\":59118},{\"end\":59134,\"start\":59126},{\"end\":59140,\"start\":59134},{\"end\":59148,\"start\":59140},{\"end\":59402,\"start\":59396},{\"end\":59731,\"start\":59725},{\"end\":59744,\"start\":59731},{\"end\":59753,\"start\":59744},{\"end\":59951,\"start\":59945},{\"end\":59960,\"start\":59951},{\"end\":59967,\"start\":59960},{\"end\":59973,\"start\":59967},{\"end\":59984,\"start\":59973},{\"end\":60326,\"start\":60319},{\"end\":60334,\"start\":60326},{\"end\":60342,\"start\":60334},{\"end\":60349,\"start\":60342},{\"end\":60676,\"start\":60669},{\"end\":60684,\"start\":60676},{\"end\":60692,\"start\":60684},{\"end\":60699,\"start\":60692},{\"end\":60988,\"start\":60981},{\"end\":60994,\"start\":60988},{\"end\":61000,\"start\":60994},{\"end\":61310,\"start\":61302},{\"end\":61319,\"start\":61310},{\"end\":61327,\"start\":61319},{\"end\":61333,\"start\":61327},{\"end\":61340,\"start\":61333},{\"end\":61676,\"start\":61669},{\"end\":61683,\"start\":61676},{\"end\":61689,\"start\":61683},{\"end\":61696,\"start\":61689},{\"end\":61704,\"start\":61696},{\"end\":62049,\"start\":62042},{\"end\":62057,\"start\":62049},{\"end\":62064,\"start\":62057},{\"end\":62072,\"start\":62064},{\"end\":62080,\"start\":62072},{\"end\":62429,\"start\":62413},{\"end\":62440,\"start\":62429},{\"end\":62660,\"start\":62650},{\"end\":62670,\"start\":62660},{\"end\":62972,\"start\":62962},{\"end\":62982,\"start\":62972},{\"end\":62995,\"start\":62982},{\"end\":63321,\"start\":63307},{\"end\":63337,\"start\":63321},{\"end\":63620,\"start\":63606},{\"end\":63636,\"start\":63620},{\"end\":63933,\"start\":63925},{\"end\":63941,\"start\":63933},{\"end\":63947,\"start\":63941},{\"end\":63955,\"start\":63947},{\"end\":63964,\"start\":63955},{\"end\":63975,\"start\":63964},{\"end\":64365,\"start\":64357},{\"end\":64376,\"start\":64365},{\"end\":64388,\"start\":64376},{\"end\":64404,\"start\":64388},{\"end\":64716,\"start\":64709},{\"end\":64722,\"start\":64716},{\"end\":64729,\"start\":64722},{\"end\":64737,\"start\":64729},{\"end\":64745,\"start\":64737},{\"end\":64751,\"start\":64745},{\"end\":65050,\"start\":65043},{\"end\":65058,\"start\":65050},{\"end\":65064,\"start\":65058},{\"end\":65071,\"start\":65064},{\"end\":65084,\"start\":65071},{\"end\":65339,\"start\":65331},{\"end\":65346,\"start\":65339},{\"end\":65354,\"start\":65346},{\"end\":65680,\"start\":65672},{\"end\":65689,\"start\":65680},{\"end\":65697,\"start\":65689},{\"end\":65704,\"start\":65697},{\"end\":65711,\"start\":65704},{\"end\":65718,\"start\":65711},{\"end\":66094,\"start\":66086},{\"end\":66103,\"start\":66094},{\"end\":66111,\"start\":66103},{\"end\":66118,\"start\":66111},{\"end\":66125,\"start\":66118},{\"end\":66132,\"start\":66125},{\"end\":66522,\"start\":66511},{\"end\":66533,\"start\":66522},{\"end\":66917,\"start\":66908},{\"end\":66928,\"start\":66917},{\"end\":67237,\"start\":67228},{\"end\":67243,\"start\":67237},{\"end\":67249,\"start\":67243},{\"end\":67259,\"start\":67249},{\"end\":67267,\"start\":67259},{\"end\":67594,\"start\":67587},{\"end\":67600,\"start\":67594},{\"end\":67614,\"start\":67600},{\"end\":67622,\"start\":67614},{\"end\":67916,\"start\":67910},{\"end\":67923,\"start\":67916},{\"end\":67931,\"start\":67923},{\"end\":68221,\"start\":68206},{\"end\":68232,\"start\":68221},{\"end\":68240,\"start\":68232},{\"end\":68621,\"start\":68614},{\"end\":68849,\"start\":68843},{\"end\":68856,\"start\":68849},{\"end\":68863,\"start\":68856},{\"end\":68870,\"start\":68863},{\"end\":68880,\"start\":68870},{\"end\":68890,\"start\":68880},{\"end\":69229,\"start\":69222},{\"end\":69235,\"start\":69229},{\"end\":69242,\"start\":69235},{\"end\":69545,\"start\":69539},{\"end\":69554,\"start\":69545},{\"end\":69561,\"start\":69554},{\"end\":69568,\"start\":69561},{\"end\":69878,\"start\":69872},{\"end\":70121,\"start\":70111},{\"end\":70129,\"start\":70121},{\"end\":70140,\"start\":70129},{\"end\":70480,\"start\":70468},{\"end\":70489,\"start\":70480},{\"end\":70500,\"start\":70489},{\"end\":70509,\"start\":70500},{\"end\":71005,\"start\":70992}]", "bib_venue": "[{\"end\":52007,\"start\":51988},{\"end\":52312,\"start\":52293},{\"end\":52623,\"start\":52589},{\"end\":52946,\"start\":52923},{\"end\":53303,\"start\":53270},{\"end\":53627,\"start\":53589},{\"end\":53960,\"start\":53935},{\"end\":54322,\"start\":54281},{\"end\":54680,\"start\":54644},{\"end\":54958,\"start\":54920},{\"end\":55188,\"start\":55172},{\"end\":55481,\"start\":55448},{\"end\":55855,\"start\":55805},{\"end\":56148,\"start\":56084},{\"end\":56370,\"start\":56343},{\"end\":56722,\"start\":56700},{\"end\":57069,\"start\":57032},{\"end\":57374,\"start\":57360},{\"end\":57682,\"start\":57657},{\"end\":58108,\"start\":58062},{\"end\":58523,\"start\":58498},{\"end\":58855,\"start\":58830},{\"end\":59163,\"start\":59148},{\"end\":59448,\"start\":59402},{\"end\":59723,\"start\":59653},{\"end\":60030,\"start\":59984},{\"end\":60377,\"start\":60349},{\"end\":60724,\"start\":60699},{\"end\":61033,\"start\":61000},{\"end\":61368,\"start\":61340},{\"end\":61750,\"start\":61704},{\"end\":62126,\"start\":62080},{\"end\":62459,\"start\":62440},{\"end\":62718,\"start\":62670},{\"end\":63030,\"start\":62995},{\"end\":63348,\"start\":63337},{\"end\":63653,\"start\":63636},{\"end\":64021,\"start\":63975},{\"end\":64429,\"start\":64404},{\"end\":64784,\"start\":64751},{\"end\":65103,\"start\":65084},{\"end\":65400,\"start\":65354},{\"end\":65764,\"start\":65718},{\"end\":66170,\"start\":66132},{\"end\":66579,\"start\":66533},{\"end\":66956,\"start\":66928},{\"end\":67304,\"start\":67267},{\"end\":67655,\"start\":67622},{\"end\":67969,\"start\":67931},{\"end\":68305,\"start\":68240},{\"end\":68646,\"start\":68621},{\"end\":68936,\"start\":68890},{\"end\":69276,\"start\":69242},{\"end\":69601,\"start\":69568},{\"end\":69903,\"start\":69878},{\"end\":70186,\"start\":70140},{\"end\":70547,\"start\":70509},{\"end\":71219,\"start\":71005},{\"end\":52653,\"start\":52625},{\"end\":52965,\"start\":52948},{\"end\":53332,\"start\":53305},{\"end\":54359,\"start\":54324},{\"end\":54992,\"start\":54960},{\"end\":55510,\"start\":55483},{\"end\":55901,\"start\":55857},{\"end\":56393,\"start\":56372},{\"end\":57102,\"start\":57071},{\"end\":58150,\"start\":58110},{\"end\":59174,\"start\":59165},{\"end\":59490,\"start\":59450},{\"end\":60072,\"start\":60032},{\"end\":60401,\"start\":60379},{\"end\":61062,\"start\":61035},{\"end\":61392,\"start\":61370},{\"end\":61792,\"start\":61752},{\"end\":62168,\"start\":62128},{\"end\":62762,\"start\":62720},{\"end\":63057,\"start\":63032},{\"end\":64063,\"start\":64023},{\"end\":64813,\"start\":64786},{\"end\":65442,\"start\":65402},{\"end\":65806,\"start\":65766},{\"end\":66621,\"start\":66581},{\"end\":66980,\"start\":66958},{\"end\":67337,\"start\":67306},{\"end\":67684,\"start\":67657},{\"end\":68366,\"start\":68307},{\"end\":68978,\"start\":68938},{\"end\":69306,\"start\":69278},{\"end\":69630,\"start\":69603},{\"end\":70228,\"start\":70188},{\"end\":71235,\"start\":71221}]"}}}, "year": 2023, "month": 12, "day": 17}