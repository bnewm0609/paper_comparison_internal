{"id": 245537466, "updated": "2023-10-05 18:19:56.953", "metadata": {"title": "Fast Changeset-based Bug Localization with BERT", "authors": "[{\"first\":\"Agnieszka\",\"last\":\"Ciborowska\",\"middle\":[]},{\"first\":\"Kostadin\",\"last\":\"Damevski\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Automatically localizing software bugs to the changesets that induced them has the potential to improve software developer efficiency and to positively affect software quality. To facilitate this automation, a bug report has to be effectively matched with source code changes, even when a significant lexical gap exists between natural language used to describe the bug and identifier naming practices used by developers. To bridge this gap, we need techniques that are able to capture software engineering-specific and project-specific semantics in order to detect relatedness between the two types of documents that goes beyond exact term matching. Popular transformer-based deep learning architectures, such as BERT, excel at leveraging contextual information, hence appear to be a suitable candidate for the task. However, BERT-like models are computationally expensive, which precludes them from being used in an environment where response time is important. In this paper, we describe how BERT can be made fast enough to be applicable to changeset-based bug localization. We also explore several design decisions in using BERT for this purpose, including how best to encode changesets and how to match bug reports to individual changes for improved accuracy. We compare the accuracy and performance of our model to a non-contextual baseline (i.e., vector space model) and BERT-based architectures previously used in software engineering. Our evaluation results demonstrate advantages in using the proposed BERT model compared to the baselines, especially for bug reports that lack any hints about related code elements.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2112.14169", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icse/CiborowskaD22", "doi": "10.1145/3510003.3510042"}}, "content": {"source": {"pdf_hash": "bef522d3533a378725f0b5bdf664fd3cd6836fed", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2112.14169v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "c5e71eccc3f49f9f28eff4d836d5be6d1ed6821b", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/bef522d3533a378725f0b5bdf664fd3cd6836fed.txt", "contents": "\nFast Changeset-based Bug Localization with BERT\n\n\nAgnieszka Ciborowska ciborowskaa@vcu.edu \nDepartment of Computer Science Richmond\nVirginia Commonwealth University\nVAUSA\n\nKostadin Damevski kdamevski@vcu.edu \nDepartment of Computer Science Richmond\nVirginia Commonwealth University\nVAUSA\n\nFast Changeset-based Bug Localization with BERT\n10.1145/1122445.1122456bug localizationchangesetsinformation retrievalBERT\nAutomatically localizing software bugs to the changesets that induced them has the potential to improve software developer efficiency and to positively affect software quality. To facilitate this automation, a bug report has to be effectively matched with source code changes, even when a significant lexical gap exists between natural language used to describe the bug and identifier naming practices used by developers. To bridge this gap, we need techniques that are able to capture software engineering-specific and project-specific semantics in order to detect relatedness between the two types of documents that goes beyond exact term matching. Popular transformer-based deep learning architectures, such as BERT, excel at leveraging contextual information, hence appear to be a suitable candidate for the task. However, BERT-like models are computationally expensive, which precludes them from being used in an environment where response time is important.In this paper, we describe how BERT can be made fast enough to be applicable to changeset-based bug localization. We also explore several design decisions in using BERT for this purpose, including how best to encode changesets and how to match bug reports to individual changes for improved accuracy. We compare the accuracy and performance of our model to a non-contextual baseline (i.e., vector space model) and BERT-based architectures previously used in software engineering. Our evaluation results demonstrate advantages in using the proposed BERT model compared to the baselines, especially for bug reports that lack any hints about related code elements.\n\nINTRODUCTION\n\nTwo of the most prevalent tools used today by software engineers are repositories to store project files (e.g., git) and bug trackers to Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. report and monitor bug fixing activity (e.g., JIRA, BugZilla). Automatically linking a bug report in a bug tracker and related software artifacts from a repository is one of the long-standing goals in the software engineering research community, due to its potential to improve practice by reducing the time developers spend examining code when addressing a newly reported bug, i.e., bug localization [35,62]. However, despite numerous efforts, the accuracy of bug localization approaches is not yet high enough for widespread use, especially as it applies to different software projects that vary in bug report and code style [32]. In examining the trends from interviews conducted with a large cohort of software developers from industry and open-source software, Zou et al. report that developers do not trust bug localization tools due to their inability to adapt to different types of bug reports, specifically noting that existing techniques only work on the most simple cases, with straightforward textual similarity between the bug report and code base [64]. More work is needed to improve the retrieval quality of bug localization techniques.\n\nAt the same time, as industry is increasingly attempting to use bug localization to aid developers in their daily work, specific requirements of the problem for modern use are coming to the forefront [33]. One key characteristic found beneficial in modern software projects is bug-inducing changeset-(or commit-) level retrieval. A bug-inducing changeset is one where the bug was initially introduced into the repository. Retrieving such changesets leads to faster bug repair, as they contain related parts of the code that were changed together, which makes fixing the bug easier. However, retrieving bug-inducing changesets with high accuracy is more challenging than retrieving buggy source code elements due to the potentially large number of commits in the corpus.\n\nIn recent years, numerous popular natural language processing tasks (e.g., question answering, machine translation) have all observed improved performance when using neural network architectures based on transformers. These transformer-based models are typically applied via transfer learning, by first pre-training them on a very large corpus and then fine tuning on a much smaller dataset towards the specific task they are to be used for. Transformer-based models pre-trained on large software engineering corpora (e.g., StackOverflow) are now becoming available [48], with the potential to improve software engineering tasks like bug localization. In this paper, we use the BERT (Bidirectional Encoder Representations from Transformers) transformer-based architecture, which is a highly popular model introduced by Devlin et al. [9].\n\nBug localization is usually framed as an Information Retrieval (IR) task, where a document (i.e., a software artifact) is retrieved from a corpus-based on a query (i.e., the bug report text). A measure of semantic relatedness between the bug report and the software arXiv:2112.14169v2 [cs.SE] 11 Apr 2022 artifact is necessary to rank the results retrieved from the corpus. Given the fact that transformer-based models consist of many neural layers and require heavy computation for each sentence, measuring relatedness between the query and the corpus quickly becomes expensive.\n\nThis paper applies BERT to the problem of changeset-based bug localization with the goal of improved retrieval quality, especially on bug reports where straightforward textual similarity would not suffice. We describe an architecture for IR that leverages BERT without compromising retrieval speed and response time. In addition, we examine a number of design decisions that can be beneficial in leveraging BERT-like models for bug localization, including how best to encode changesets and their unique structure.\n\nOur experimental results indicate that the proposed approach improves upon popular bug localization techniques by, e.g., increasing the retrieval accuracy between 5.5% and 20.6% for bug reports with no or a limited number of localization hints. We note that using entire changesets as input granularity significantly hinders the models performance, while leveraging more fine grained input data, such as hunks, results in the highest retrieval quality. We also observe that the size of search space (i.e., the number of changesets in a project) significantly impacts the retrieval delay of different BERT-based models, though less in the case of the proposed model.\n\nThe main contributions of this paper are:\n\n\u2022 approach that applies BERT to the bug localization problem (specifically, localizing bug-inducing changesets) that is more accurate than the state-of-the-art, \u2022 improvement over other recent BERT-based architectures proposed towards changeset retrieval, showing significant advantages with respect to retrieval speed, \u2022 evaluation and recommendations for key design choices in applying BERT to changesets (i.e., code change encoding, data granularity).\n\nSignificance of contribution. The BERT-based technique proposed in this paper enables semantic retrieval of software artifacts (specifically, changesets) for bug localization that goes beyond (and can complement) the exact term matching in the current popular state-of-the-art techniques (e.g., [45,57]). Relative to a similar, recent BERT-based technique [27], we offer an approach that improves retrieval speed significantly, in a way that supports realworld use, while also enhancing retrieval quality.\n\n\nPROBLEM DESCRIPTION\n\nIn this section, we list and discuss the specific constraints of the bug localization problem that we aim to address, which are based on a recent survey of industry practitioners and the problem requirements observed at a large software enterprise [33,64]. Our focus is a bug localization technique that: 1) focuses on retrieving changesets; 2) aims to capture semantics and can be applied to bug reports that do not share terms with the relevant parts of the code base; and 3) quickly retrieves results for a newly created bug report.\n\n1. Localizing changesets [33]. Over the years, a large body of research has been dedicated to locating source code files (or classes) relevant to a bug report [7,22,36,45,55,57]. However, recent studies have pointed out that bug localization at the level of source code files still requires significant effort by software developers in order to locate relevant code within large files [33,56,64]. Adjusting for this finding, researchers shifted their efforts towards more fine grained code elements, such as file segments [57] and methods [50,59,61], which introduce new sets of challenges such as difficulty in selecting optimal segment size and large methods that still require effort to examine. More recently, there has been a growing interest in changeset retrieval [7,27,56,58] for bug localization because changesets have several unique properties that make them convenient to developers aiming to fix a bug. First, they inherently capture lines of code that are related to each other within the context of a modification. Second, when locating changesets, we can retrieve not only the modified portion of the code, but identify a software developer that committed the modification in the first place, therefore easing the bug triaging process. Finally, changesets allow for straightforward context-aware division into a set of hunks, i.e., a set of changes in one area of the file. Hunks are usually convenient to read for developers and allow for easy detection of changes with no semantic value (e.g., changes only in white spaces). [33,48]. As software evolves rapidly and is actively maintained by multiple developers, different portions of the code base become affected by distinctive identifier naming patterns and conventions, which exacerbate the already existing semantic gap between bug reports and related code elements, posing a significant challenge to traditional IR systems based solely on token similarity [12]. Surveys of practitioners have also indicated that bug reports that explicitly mention the names of classes or methods relevant to the bug fix do not require automated bug localization, while assisting in bug reports with large semantic gaps with the code base is likely more valuable to developers. For instance, one surveyed developer in the study by Zou et al. [48] stated the following about current bug localization, \"It seems that existing techniques mainly make use of the textual similarity between bug reports and source code files to perform bug localization. However, I encountered many bugs that have very little similarity between their bug reports and code files. I wonder what kind of bugs such techniques can localize? Maybe only simple bugs?\". To bridge this gap, researchers have recently proposed to use deep learning models capable of building semantically rich document representations [4,6,12,16,24,27,33]. Transformerbased models, and BERT in particular, are currently one of the most exciting deep learning techniques achieving broad improvements across a variety of text-based tasks. The main strength of BERT-like models is in building a token representation based on bidirectional contextual information encoded in the preceding and succeeding tokens, which leads to richer semantics that is more likely to detect related pairs of bug reports and changesets that do not share terms. Prior generations of word embeddings, e.g., word2vec [31] and GloVe [38], which have been frequently applied on software engineering tasks [5], do not use word context at inference time, i.e., each token maps to a vector regardless of the surrounding text.\n\n\nLeveraging semantics of input documents\n\n3. Fast retrieval in a large search space [40]. Retrieving buginducing changesets requires computing similarity between a bug report of interest and all changesets committed to a repository up to the present point in time. Given that modern software evolves rapidly, resulting in large source code repositories with numerous commits [43,46], it is impractical to compute pair-wise similarity due to the large search space. This is especially the case if computing the similarity measure itself is expensive. Though deep learning models provide state-of-the-art accuracy, they typically require more computational resources than token-based techniques, which emphasizes the need for a bug localization technique to limit the search space in order to improve performance without compromising accuracy.\n\n\nAPPROACH\n\nIn order to address the above problem constraints, in this paper we investigate the use of a BERT model towards bug localization with changesets as a primary data granularity, which is also preferred by practitioners' ( \u00a72.1) [52]. We specifically selected BERT as it is the state-of-the-art in semantics modeling and extracting contextual information ( \u00a72.2). Finally, to ensure that our approach is applicable to large, industry scale repositories ( \u00a72.3), we introduce Fast Bug Localization BERT (FBL-BERT), which reduces the search space, such that only promising candidate changesets are considered for neural re-ranking with BERT. In addition, FBL-BERT encodes a bug report and a changeset separately, allowing to compute changeset representations offline and reduce the computational effort per bug report at retrieval time. Replication package is available at [1].\n\n\nBERT for bug localization\n\nThe architecture of BERT consists of multiple layers of transformerencoders, which are an abstraction aimed at modeling sequential data that utilizes self-attention; the notion of attention is to weight specific terms in the sequence differently, i.e., encoding a stronger relationship from each term in the sequence to the remaining most semantically relevant terms. As pointed out by Mills et al. [32], retrieval techniques for bug localization can be significantly improved with intelligent query construction, i.e., by carefully choosing which parts of the bug report to use for comparison. Therefore, leveraging a model that uses attention to emphasize certain word relationships has the potential to significantly improve upon prior state-of-the-art bug localization techniques. Using a BERT model for bug localization (or other similar purposes) involves three essential steps: (1) pre-training the model with a large corpus of general software engineering-related data, (2) fine tuning the BERT model for bug localization, and finally, after BERT has been completely trained, (3) retrieving relevant bug-inducing changesets for a newly reported bug.\n\nDuring pre-training, BERT uses massive corpora of relevant text to build a language model for a specific domain, e.g., software development. Given that this step requires a significant amount of data and computational resources, a common choice is to reuse a pre-trained BERT model, when available. In the fine tuning step, BERT updates the general data representation with respect to a specific downstream task (e.g., bug localization) given a much smaller, task-specific dataset. More precisely, fine tuning a BERT model occurs by adding an additional layer (e.g., a classification layer) to the pre-trained BERT model. This task-specific layer takes the output of BERT as input and represents the part of the model that is primarily trained during fine tuning, though BERT's internal weights are also updated in the process. In most scenarios, fine tuning can be completed faster and with much less computational resources than pre-training. Since our goal is locating bug-inducing changesets, a natural choice for a task-specific dataset consists of bug reports and their inducing changesets. A key design choice at this stage is how to connect BERT with the additional task-specific neural network layer. Given an input document, BERT encodes each word in the document with a vector, i.e., for each input document, the output of the BERT model is an embedding matrix of size | | by , where | | represents the number of words in the document and the length of a BERT vector; typically = 728. The most common approach when retrieving BERT-encoded documents is to aggregate the embedding matrix across words through average or summation, which produces a single vector as output. Using such an aggregate representation of a document allows for faster processing and easier comparison between pairs of documents. However, as pointed by Sachdev et al. [44], this simple aggregation strategy leads to a dissipative data representation that has the potential to negatively affect retrieval performance. In the next section, we describe an alternative strategy that takes advantage of the full matrix to encode input data. In the simplest changeset retrieval scenario, presented in Fig. 1a, each newly arriving bug report is concatenated with every changeset in the project history. Subsequently, they are processed by BERT, producing an embedding matrix, which is transformed to a vector by an aggregation layer. Finally, the vector is passed into a classification layer that produces a relevancy score between a bug report and a changeset. Changesets are ordered based on their scores to produce a ranked result set. This type of BERT architecture for information retrieval is often referred to as Single BERT [8,27,37]. In an alternative retrieval architecture, called Siamese BERT [27,41] and depicted in Fig. 1b, the bug report and the changeset are processed separately, first through BERT and then through an aggregation layer. As a result, bug reports and changesets are transformed into independent vectors that are subsequently concatenated and fed into the classification layer to produce a relevance score. The advantage of Siamese BERT over Single BERT is that Siamese BERT enables pre-computing changeset representations offline since changesets are not required to be concatenated with a bug report for retrieval. However, Siamese BERT still requires comparing a bug report to each changeset, which incurs significant retrieval delay in the case of large number of changesets.\n\n\nFast Bug Localization BERT\n\nThe FBL-BERT architecture, based on ColBERT by Khattab et al. [21], eschews aggregation of the embedding matrix, and instead builds a relevance score by leveraging the whole matrix, resulting in a more complete, fine grained comparison. More specifically, a bug report and a changeset are separately processed by BERT creating embedding matrices and , respectively. To compute the relevance score between and , for each word embedding in the bug report \u2208 , we find the maximum cosine similarity across word embeddings of the changeset \u2208 , and combine the maximum cosine similarities via summation as illustrated in Fig. 1c. As a result, the model learns how to associate words from a bug report with tokens in a changeset, taking into account the context in which they appear. To account for the two different types of data we process, i.e., bug reports and changesets, we modify ColBERT by increasing the numbers of BERT encoder layers taken to the linear layer. More specifically, while ColBERT uses the output of the last BERT encoder, we take the output of the last 4 encoders (as recommended by [9]). This modification is dictated by prior studies observing that that different layers of BERT encode different granularity of semantic information [9,39,51]. Note that the linear layer in FBL-BERT is not equivalent to the aggregation layer discussed before, but is used to reduce the size of word embeddings produced by BERT, retaining all word embeddings in a compressed form for faster downstream processing.\n\nThere are several benefits that make the FBL-BERT architecture particularly applicable to our problem. First, the model purposely avoids joint document encoding, as in Single BERT, delaying interaction between a bug report and a changeset to facilitate off-line encoding of changesets. Moreover, by using computationally cheap, yet efficient, maximum similarity summation as a scoring operator instead of a more complex strategy, such as the classification layer in Siamese BERT, the processing time for a query is reduced. Finally, given that the relevance score computation is isolated and relies solely on maximum similarity, it is possible to utilize efficient vector similarity algorithms to reduce the search space of all changesets by identifying top-changesets, << , that are similar to a new bug report, and subsequently re-rank only the top-subset.\n\nTo clarify how FBL-BERT operates for changeset-based bug localization, consider the pipeline depicted in Fig. 2. First, as shown in the Model Training section of Fig. 2, the FBL-BERT model is fine tuned on a project-specific dataset consisting of bug reports and bug-inducing changesets. In the next step (Offline Indexing), all changesets in the project repository are encoded via FBL-BERT and stored in an index supporting efficient vector-similarity search. For this purpose, we use an IVFPQ (InVerted File with Product Quantization) index, implemented in the Faiss library [19]. The IVFPQ index uses the k-means algorithm to partition the embedding space into (e.g., = 300) partitions, and subsequently assigns each word embedding to its nearest cluster. To facilitate efficient search, when a query is issued, the query is first compared against the partitions' centroids to locate the nearest partitions, and then the search continues to the instance-level only within those. Note that the Faiss index contains word embeddings across all changesets. After completion of this step, the retrieval system is ready to be deployed. When a new bug report arrives, it is first encoded via FBL-BERT producing an embedding matrix. Next, for each word embedding in the embedding matrix, we query the Faiss index to identify the \u2032 most similar embeddings across all changesets embeddings stored in the Faiss index. Since among \u2032 most similar embeddings some may point to the same changeset, in the end we obtain a total of unique candidate changesets. Finally, we use FBL-BERT to re-rank the candidate changesets and produce the final ranking.\n\n\nChangesets encoding strategies\n\nSoftware evolution over time is recorded in a repository as a timeordered sequence of changesets. Each changeset consists of a log message, providing a short rationale explaining the goal of the modification, and a set of source code changes. Depending on the version control system and diff algorithm used in the software project, the representation of source code changes can vary. In this paper, we focus on the format that is the output of the git diff command, in which added lines of code are annotated with +, removed lines with -, and all modified lines are surrounded by 3 lines of contextual, unchanged lines. While there exist more advanced tree-based code differencing algorithms (e.g., GumTreeDiff [10]), providing detailed code-change information to a machine learning model may affect the model negatively [60], hence we opt for a textbased approach. Changesets can encapsulate code changes across one or multiple source code files, and modifications to each file can be divided into hunks -groups of modified (added or removed) lines surrounded by unchanged (context) lines. Given this specific formatting, we explore how best to utilize changesets' properties to construct BERT input from two perspectives: (1) encoding characteristics of code modifications, such as additions or removals; and (2) levels of granularity in a changeset.\n\nInput provided to BERT models is required to follow certain rules. First, a document (e.g., a changeset or a bug report) needs to be tokenized and each token replaced by its unique token id. Pre-trained BERT models supply their own BERT tokenizers, that are optimized towards the corpus on which the model is pre-trained. BERT tokenizers are trained using the WordPiece algorithm [47]. The main advantage of BERT tokenizers is in avoiding out-of-vocabulary words by dividing unknown words to their largest subwords present in the vocabulary, which is likely to be beneficial in our setting, as software projects can have very specific vocabularies unlikely to be observed elsewhere [48]. Secondly, BERT uses a pre-defined set of special tokens. In general, due to how BERT is trained (more details in [9]), the model requires that each token sequence starts with special classification token [CLS] and ends with separator token [SEP], while other special tokens, such as padding [PAD] are used if and when necessary. Special tokens can convey information about the structure of data allowing BERT to differentiate between parts of the input, hence we explore how special tokens can be best utilized to encode changesets. To this end, we propose the following encoding strategies, depicted in Fig. 3.\n\n\nD:\n\nA changeset is considered a single document that is feed into the model. To inform the model that a changeset sequence begins, we define and pre-append the special token [D] at the beginning of the code sequence. Since this strategy does not utilize specific characteristics of a code change, it serves as a baseline to compare against other strategies.\n\nARC: In this encoding, a changeset is split into lines, and the lines are subsequently grouped based on whether they are added, removed or provide context, as indicated by their initial character: + , or [C] are pre-appended wherever type of modification changes. While this strategy results in more accurate data representation, compared to ARC, ARC L is also more challenging for the model, since the special tokens occur multiple times and in several places.\n\nGiven that a bug report and a changeset are encoded separately, the model has to differentiate between these two types of documents. To this end, when encoding a bug report, we define a special token [Q] that is pre-appended to the query, i.e., the bug report.\n\nAnother dimension in choosing how to best encode changesets is related to their granularity, i.e., using entire changesets or separating a changeset to a file-or hunk-level. Leveraging hunks as the primary data dimension in an IR model brings several advantages. First, bugs have been observed to be typically caused by small pieces of code [57,59], thus the inherent fine granularity of hunks makes them less susceptible to noise when compared to whole source code files [56]. Second, dividing changesets into hunks alleviates issues caused by tangled commits [13]. Given the fact that hunks are typically small and concentrate on an enclosed portion of the code, BERT is not affected by long-range token dependencies, which is a problem typically affecting source code [48]. Finally, shorter input documents are less likely to exceed the maximum sequence length accepted by BERT, while longer documents have to be truncated, which may negatively affect the results. However, despite easily accessible smaller data granularity within a changeset, to date, most of the efforts are focused on leveraging entire changesets [3,27,33]. The main opportunity in using FBL-BERT is in incorporating additional context and semantics when retrieving bug-inducing changesets, which should provide improvements in accuracy over the state-of-the-art, especially for bug reports that provide high level bug descriptions and lack explicit localization hints. Researchers have identified that a non-trivial amount of bug reports already contain localization hints, i.e., they mention the class or method names relevant to fixing the bug, and some recent approaches for bug localization argue that only bug reports that lack extensive localization hints should be considered in evaluation [17]. We follow the methodology proposed by Kochhar et al. [23] to categorize bug reports into 3 groups based on the completeness of localization hints they provide and evaluate the performance for each bug report group separately. We also investigate how the runtime performance of FBL-BERT, which utilizes fine grained matching, compares to other BERT-based architectures that rely on embedding aggregation and perform retrieval across the entire search space. As baselines, we use (1) Locus [56], a state-of-the-art approach based on VSM that locates bug-inducing changesets, and (2) TBERT-Single and TBERT-Siamese [27] approaches that utilize aggregated BERT-based representations that have recently been proposed for software engineering.\n\nRQ2: Which changeset encoding strategy is the most profitable? Are there advantages to using hunks, changeset-files or entire changesets as the primary data dimension? In this RQ, we first investigate whether encoding information about the type of modification in each line of a changeset can increase the performance of the FBL-BERT model. We evaluate two alternatives to encode changesets semantics, ARC, ARC L , and a baseline approach, D, which disregards change-related information. Second, we investigate how granularity of the input data affects the model performance and what are the benefits and challenges of leveraging changesets, changeset-files, or hunks in our model. To answer this RQ, we fine tune FBL-BERT separately for each of the encoding strategies and with each input data granularity, resulting in 9 evaluation configurations per software project, measuring the model's performance in retrieving relevant changesets.\n\n\nDataset and baselines\n\nTo answer the RQs, we leverage the dataset of bugs and their inducing changesets collected and manually validated by Wen et al. [56]; manually validated datasets remove the error that can be introduced by the SZZ algorithm that maps the bug fixing to the inducing commit [34]. This dataset includes 6 software projects, namely AspectJ, JDT, PDE, SWT, Tomcat and ZXing (descriptive statistics are presented in Table 1). To create a training set for each project, we selected the first half of project's pairs of bug reports and bug-inducing changesets, ordered by bug opening date, as a training set, and left the remaining half as a test set. For each pair in the training sets, we also create a negative sample by randomly choosing a code change which does not belong to the inducing changeset, essentially forming triplets of bug report, bug-inducing changeset, not bug-inducing changeset. We experimented with choosing negative samples by selecting a syntactically similar changeset that was not bug-inducing but we did not observe a significant change in retrieval accuracy. As this type of generating negative samples incurred substantial computational cost to gather, we opted to use random sampling. Finally, for each project we obtained a balanced training set with equal number of positive and negative examples. Note that although training sets do not include all available code changes, during bug localization the model performs retrieval across all code changes available for a specific project (as explained in Section 3.2). To study the impact of different changeset data granularity on the BERT-based models, we created a separate dataset for each type of granularity, i.e., changesets, changeset-files and hunks. To this end, for changeset-file and hunk granularity, we divide the bug-inducing changeset to file-or hunk-level code changes, such that one bug report creates multiple pairs with files or hunks from its respective inducing changeset.\n\nWe compare the performance of the proposed model with Locus [56], which is an unsupervised model that utilizes hunk-level granularity and the VSM to locate relevant changesets based on the maximum similarity score obtained between a bug report, a hunk, and a log message. Note that FBL-BERT does not use log messages as our goal is to explore mapping from natural language in a bug report to code changes. While well written log messages can have a positive impact on the results by boosting the scores for some changesets, not all relevant code changes are accompanied by logs of good quality [18,29]. As a second set of baselines, we employ TBERT architectures for software artifacts retrieval recently proposed by Lin et al. [27]. Out of the three architectures investigated by Lin et al., we selected TBERT-Single and TBERT-Siamese as our baselines, rejecting TBERT-Twin, since its performance in terms of accuracy and time was significantly surpassed by the two others. In general, both of these architectures are fairly similar to those presented in Fig. 1 with an exception of using more advanced embedding aggregation operators [27].\n\n\nMetrics\n\nTo evaluate the performance of the model, we employ a set metrics commonly used to evaluate performance of IR systems. Mean Reciprocal Rank: MRR quantifies the ability of a model to locate the first relevant changeset to a bug report. The metric is calculated as an average of reciprocal ranks across bug reports, while a reciprocal rank for a bug report is equal to an inverted rank of the first relevant changeset in the ranking:\n= 1 | | | | \u2211\ufe01 =1 1 1 .\nMean Average Precision: MAP measures how well a model can locate all changesets relevant to a bug report. MAP is calculated as the mean of average precision values ( ) for bug reports, while average precision for a bug report , , is computed based on the positions of all relevant changesets in the ranking:\n= 1 | | | | \u2211\ufe01 =1 1 .\nPrecision@K: P@K evaluates how many of the top-changeset in a ranking are relevant to a bug report. The value of P@K is equal to the number of relevant changesets | | located in the topposition in the ranking averaged across bug reports:\n@ = 1 | | | | \u2211\ufe01 =1 | | .\n\nExperiment setup\n\nThe experiments were conducted on a server with Dual 12-core 3.2GHz Intel Xeon and utilized 1 NVIDIA Tesla V100 with 32GB \n\n\n2, and\n\nFaiss v.1.6.5 with GPU support. Since pre-training is a computationally expensive task and requires a huge dataset, we decided to use an available pre-trained BERT model, BERTOverflow [49]. BERTOverflow is trained on StackOverflow data, hence it contains a mixture of code snippets and natural language descriptions, which is logical for the bug localization task that operates on both code and natural language. We fine tuned our BERT model and TBERT baselines for 4 epochs with batches of size 16 and a learning rate of 3E-06 [9]. Based on the average number of tokens in bug reports, hunks, changeset-files and changesets across the evaluation projects, we set the maximum length limit to 256, 256, 512, and 512 respectively. All input documents are truncated or padded to their respective length limit. For the Faiss index, we set the number of partitions to 320 and retrieved a total of 1000 changesets for re-ranking with FBL-BERT [21]. In the case of Locus, we set the model parameters to = 5 and 2 = 0.2, indicated by the authors to provide the highest performance.\n\n\nRESULTS\n\n\nRQ1: Retrieval performance\n\nRetrieval accuracy. Table 2 contrasts the retrieval performance of the FBL-BERT model against the baseline approaches for three different types of bug reports: not localized, partially localized, or fully localized. If a bug report has no mentions of relevant classes, it is classified as not localized (BR NL ); when some of the relevant classes appear in the report, the bug is categorized as partially localized (BR PL ); and if all relevant class names are provided, the bug report is fully localized (BR FL ) [23]. Note that in the case of FBL-BERT, we use the results of the model trained with ARC L encoding since, on average, it provides the best performance across the evaluation projects, as shown in Section 5.2. FBL-BERT outperforms Locus for BR NL and BR PL by 5.5% and 20.6% respectively, while in the case of BR FL , Locus surpasses our approach by 11.4%. Given that Locus relies on more direct term matching between a bug report and a changeset, it makes intuitive sense that such a model performs best when localization hints are present in a bug report, and struggles in their absence (as indicated by lower MRR values for BR NL and BR PL ). On the other hand, FBL-BERT utilizes higher-level association between bug reports and bug-introducing changesets, which can result in exact matches getting less emphasis. Interestingly, the highest improvement in retrieval accuracy is observed for BR PL indicating that the model can effectively retrieve changesets based on partial clues by associating them with patterns learned from historical data.\n\nThe performance of both TBERT models and FBL-BERT improves when the models are trained and evaluated on hunks or changesetfiles. Compared to leveraging changesets, across all bug reports FBL-BERT improves between 23.5%-25.9%, while the retrieval accuracy of TBERT-Single and TBERT-Siamese increases by 16%-18% and 9.8%-13.9% respectively. While this results indicate that leveraging fine grained data affects retrieval performance positively, it is important to note that the poor performance observed for changesets can be partially attributed to the input size limit of the BERT model (i.e., 512 tokens), which is more often exceed by changesets than hunks or changeset-files. More specifically, in our dataset truncation affects about 8% of hunks and 25% of changeset-files compared to 45% of changesets.\n\nIn general, FBL-BERT outperforms TBERT-Single and TBERT-Siamese by 4.9% and 7.6% respectively across all types of bug reports. Comparing the results of FBL-BERT trained on hunks to TBERT models trained on changeset-files, given that changeset-files provide on average the best performance for TBERT models, we note varying difference in retrieval accuracy depending on the bug report type. In the case of BR NL , FBL-BERT improves MRR score by only about 2% over TBERT models. For BR PL , FBL-BERT improves by 4% and 8.5% over TBERT-Single and TBERT-Siamese, while for BR FL the improvement is equal to 3.9% and 13.8% respectively. The larger gap in retrieval accuracy for BR PL and BR FL between FBL-BERT and TBERT models indicates the importance of token-level embedding matching, i.e., while TBERT uses aggregated embedding to represent and compare documents, the token-level embedding matching performed by FBL-BERT allows this model to better recognize the key code names presented in the bug report, which, in turn, translates to higher retrieval accuracy. Retrieval time. One of the key desirable characteristics of FBL-BERT is to perform efficient retrieval across a large corpus. This would allow it to leverage fine grained data, such as changesetsfiles or hunks which were observed to provide the best retrieval accuracy, while maintaining reasonable retrieval delay. In Fig. 4, we compare the average retrieval time per bug report with respect to the increasing number of documents in the search space, i.e., changesets, changesets-files and hunks. In general, FBL-BERT retrieves relevant documents faster than both TBERT models with the retrieval time gap increasing as the search space grows. More specifically, TBERT-Single is the slowest model and requires about 50s to perform retrieval over a small number of documents (e.g., ZXing), and nearly 1000s(!) for a large project (e.g., JDT). TBERT-Siamese is significantly faster than TBERT-Single, and up to the search space of about 15K documents, it performs on-pair with FBL-BERT. However, after that point, retrieval time for TBERT-Siamese rises steadily to reach about 70s for the largest search space, while in the case of FBL-BERT the retrieval time is still just above 1s. By comparing the performance of FBL-BERT against TBERT models, it becomes evident that plain BERT-based models can quickly hit a retrieval delay wall which makes them impractical to use. On the other hand, FBL-BERT scales up with respect to the search space size allowing to leverage fine grained data to increase retrieval accuracy without sacrificing model responsiveness.\n\nNote that the observed speed improvement is the result of both FBL-BERT and FAISS. More specifically, the training objective of FBL-BERT (i.e., finding most similar embedding vectors) enables using vector similarity search (e.g., FAISS). As a consequence, FAISS can be used to retrieve the best candidates ( << , where is #documents) with similar word-level embedding representations that are then re-ranked by FBL-BERT. By re-ranking only documents, the search space becomes significantly reduced, hence decreasing the retrieval time. On the other hand, typical BERT-based pipelines (e.g., TBERT) concatenate bug reports and changesets, and use neural network layers to estimate a relevancy score. This approach precludes pruning the search space via FAISS, therefore, during retrieval a bug report has to be compared to all documents, which in turn increases retrieval delay. Error analysis. To gain more insight into factors that negatively affect the retrieval accuracy of FBL-BERT, we manually analyzed the bug reports for which the model struggles the most. More specifically, we selected all bug reports where the bug-inducing hunk was ranked 50 or worse by FBL-BERT. This resulted in 20 bug reports ( = 8, = 3, = 9) that the authors independently analyzed, contrasting the retrieved hunks to the true bug-inducing hunks in order to devise a set of common issues causing low retrieval accuracy. The authors also examined the most similar terms (and their weights) for both the retrieved and gold set hunks, focusing specifically on the sources of largest differences between the two. Finally, the authors discussed their independent observations and agreed on three common error categories: stack trace/code snippets, comments, and code tokens splitting, where a single bug report can belong to more than one error category. We discuss each of these, in turn.\n\nIn 11 out of 20 bug reports, the difficulty to retrieve the correct hunk was caused by the presence of a code snippet or a stack trace in the bug report. Since code snippets and stack traces typically consists of multiple class names or code tokens, they have a potential to introduce noise through unrelated code names, which, in turn, can lead the model astray [53]. For 7 out of 20 bug reports,  we noted that the model was misguided by source code comments present in the top-1 retrieved hunk. Since source code comments are formulated in natural language, a highly-contextual model like BERT tends to emphasize their similarity with the bug report as it is also expressed in natural language. For both of the above error categories, we believe that the wholesale removal of the problematic text (i.e, comments from code and code snippets and stack traces from bug reports) would negatively affect the model as it removes both relevant and irrelevant information. Hence, researchers should explore strategies to treat this data separately, perhaps by encoding their content within BERT with special tokens akin to the ARC and ARC L strategies we discuss in this paper. Finally, for 5 of the bug reports, FBL-BERT failed due to spurious matches in code tokens that were split into sub-tokens during preprocessing. One of the previously observed strengths of BERT is in using the WordPiece algorithm to avoid the out-of-vocabulary problem by splitting unseen tokens into the largest sub-tokens that are part of the BERT vocabulary [20]. Since source code identifier names are typically project-specific words, they do not occur in the pre-trained vocabulary, hence they are often split by WordPiece (e.g., ManagerServlet \u2192 manager, ##servlet). The sub-tokens can then spuriously match other terms, including sub-tokens from other split identifiers, but not the whole, unsplit term. Researchers in the biomedical domain recognized the same issue affecting medical terms and proposed domain-specific BERT adaptations [2,11,26]. Table 3 shows retrieval performance of FBL-BERT trained and evaluated with different changeset encoding strategies and input data granularities. For each project, the three best performing configurations are highlighted, such that dark green marks a configuration with the highest retrieval performance, while green and yellow correspond to the second and third best configurations. Overall, we notice that using entire changesets as the granularity of input results in, by far, the worst performance across all of the investigated configurations for all evaluation projects. We can attribute this result to: (1) truncation of changesets due to input length limitation of the BERT model; and (2) tangled changes within a single changeset [14], which are likely to affect the model by introducing noise via unrelated code modifications. On the other hand, while the model based on hunks or changeset-files is not free of these problems, the finer data granularity allows it to partially overcome them. For instance, in case of tangled changes, dividing the entire changeset into hunks or changeset-files creates multiple new data points, which limits the noise introduced by instances that are poorly related to the bug. The difference in retrieval accuracy across all the metrics between using hunks and changeset-files as the input data is minor and differs from 1% to 12.2% per project. This results is indicative of the observation that leveraging hunks and changeset-files perform similarly and are both resilient to the problems affecting changesets.\n\n\nRQ2: Changeset encoding strategy\n\nExamining the results for different changest encoding strategies, we observe that ARC L performs universally best across hunks and changeset-files. Interestingly, at the level of changeset-files, the baseline encoding D, which does not encode modification type, does surprisingly well and outperforms ARC encoding. We attribute this result to the specifics of ARC encoding, which groups lines based on the performed modification, hence in the case of larger documents the grouping may affect the semantics of the documents. On the other hand, ARC encoding for hunks is less likely to be susceptible to that problem since hunks are typically much shorter. Analyzing the results for different projects, we observe that ARC L performs best for AspectJ, JDT and Tomcat, with an improvement in MRR scores of 0.7%, 6.7% and 4.1% over their second best configurations respectively, while ARC is the most beneficial strategy for the PDE project. In the case of SWT, we observe the highest retrieval accuracy with ARC L , while ZXing performs best with D encoding; however, both of these observations are likely negligible given the low difference between ARC L and other encodings for SWT, and the relatively fewer bug reports in the ZXing project. Overall, we conclude that leveraging changesets semantics via encoding modification with either ARC and ARC L increases retrieval accuracy over the D configuration which does not provide the model with additional information about the change. However, based on these results, the difference between ARC and ARC L is not significant enough to clearly indicate which strategy is superior on average.\n\n\nThreats to validity\n\nThe conclusions of this paper suffer from several threats to validity.\n\nA key threat to the internal validity of our study are the specific parameter choices we used to build our FBL-BERT model. A mitigating factor is that all parameters were either studied by us or were reported in other prior reputable papers as recommended or optimal [9,21]. Another threat is our automated separation of bug reports based on localization hints into, not localized, partially localized, and fully localized, which may result in mistaken categorization, even though we used a well-known and frequently followed procedure [23].\n\nLeveraging changesets for bug localization poses another threat due to possible noise that can be introduced by SZZ [42], which could result in poor quality mapping between bug reports and bug-inducing changesets. However, the dataset was validated manually [56,62], and therefore such mistakes, if they still exist, should not significantly affect our conclusions. Errors due to tangled changes [14,30] are still possible in the dataset as such changes are difficult to remove manually. We believe tangled commits to have affected our final presented results (as discussed in RQ2), however, since tangled commits are a part of software development removing them completely may arguably result in unrealistic evaluation.\n\nA threat to external validity, which concerns the ability to generalize our evaluation results, is that we applied the bug localization technique only on a limited number of bugs collected from a selection of popular open source Java projects. A mitigating factor is that the projects have a variety of purposes and development styles and the benchmark we used has also been applied to prior changeset-based bug localization studies [45,56,57]. Another threat to external validity is in the chosen evaluation metrics, which may not directly gauge user satisfaction with our bug localization technique [54], impacting the validity of the reported results. The threat is mitigated by the fact that the selected metrics are well-known and widely accepted as best available to measure and compare the performance of IR techniques.\n\n\nRELATED WORK\n\nBug localization has generated significant research interest over the years. In this section, first, we survey related code elementbased bug localization techniques, followed by approaches towards bug-inducing changeset retrieval. Finally, we review methods for encoding changesets characteristics.\n\n\nCode element-based bug localization\n\nBug localization techniques predominantly utilize information retrieval where the bug report text is used to formulate a query that is matched to a corpus of code elements, i.e., classes or methods. To compute similarity between bug reports and source code, the Vector Space Model (VSM) is often used as one of the simplest and effective information retrieval algorithms, which is leveraged by many bug localization techniques. For instance, BugLocator [22] combines two rankings, one produced by similarity between the bug report and code elements and another based on similarity of the bug report to prior fixed bug reports. BLUiR [45] uses code and bug report structure to create groups of terms and computes similarity between different groups separately, while AmaLgam [55] creates an ensemble consisting of BugLocator, BLUiR and a defect predictor leveraging the development history of a project. BRTracer [57] focuses on analyzing and prioritizing stack traces when they are included in bug reports. Kochhar et al. were among the first to report that evaluation of bug localization was biased by explicit localization hints in a significant subset of the included bug reports [23]. VSM-based techniques are likely to perform well on such bug reports, though localizing them may not be as useful to developers [48]. Mills et al. refute the idea that VSM-based bug localization are significantly aided by hints, and note that VSM can perform well for bug localization if more attention is paid to how the query is constructed from the bug report text [32]. However, their findings do not preclude additional accuracy improvements by using more complex, semantic models, such as BERT.\n\nMore recently, software engineering researchers have been interested in the applying deep learning techniques towards bug localization. For instance, TRANP-CNN [17] is a recent technique that combines cross-project transfer learning and convolutional neural networks to achieve state-of-the-art performance on file-level bug localization. CooBa improves on TRANP-CNN by combining a shared encoder to capture cross-project with per-project features and using adversarial training to ensure that the per-project information remains unaffected by noise [63]. Lam et al.'s technique, DNNLOC, combines a deep neural network with the VSM in order to be effective across different types of similarity [25]. While we also leverage a deep learning model, BERT is significantly different from these prior techniques. Recent work in bug localization also includes reports on the value of retrieving changesets instead of source code elements [33,56].\n\n\nChangeset-based bug localization\n\nThe earliest work on changeset-based bug localization is Locus [56], which is based on VSM matching of bug reports to hunks. To adjust for localization hints, Locus adapts its similarity scores based on the proportion of code element mentions in a bug report. Bhagwan et al. [3] introduced Orca, a tool that uses a provenance graph to identify commits leading to faulty builds. ChangeLocator [58] uses historical data on software crashes to build a model identifying relevant changesets based on collection on crash reports. Although this approach allows to retrieve changesets, it requires sufficient amount of historical data to train the model, and a stack trace as an input. One of the benefits of VSM is that it is an unsupervised approach, hence a training corpus of bug reports and their inducing commits is not required. However, as VSM fundamentally requires at least partial token overlap, while it ignores the context in which tokens appear in documents, all bug localization technique based on it have a limited accuracy ceiling [33].\n\nRecently, researchers have also shifted their attention to deep learning models for changeset-based localization. For instance, Murali et al. [33] proposed Bug2Commit, an unsupervised model leveraging multiple dimension of data associated with bug reports and commits, such as metrics, stack traces or commit meta data. They observed that using embeddings can lead to improvement in model accuracy when compared to BM25. Lin et al. [27] studied the tradeoffs between different BERT architectures for the purpose of changeset retrieval, and observed the accuracy of Siamese architecture is on pair with Single-BERT architecture, while being significantly faster. However, the speed and interactivity of these models is not on par with the BERT technique described in this paper.\n\n\nChangeset representation\n\nBuilding a semantically rich representations of changesets is relevant to other software engineering applications beyond bug localization, i.e., just-in-time defect prediction, recommendation of a code reviewer for a patch, tangled change prediction. Approaches that define novel changeset embeddings (vector representations of changeset), including CC2Vec [15] and Commit2Vec [28], leverage the difference between added and removed lines of code, among other changeset characteristics. Corley et al. [7] studied how including different types of lines from a changeset affects the performance of Latent Dirichlet Allocation-based feature location, observing that including context, additions, and log messages, but excluding removed lines, achieves the best performance. However, these studies did not utilize a transfer learning technique, like BERT, which requires compatibility with a pre-trained model, and also prior work did not extensively explore hunks as a primary data dimension.\n\n\nCONCLUSION\n\nThis paper presents an approach for automatically retrieving buginducing changesets for a newly reported bug. The approach uses the popular BERT model to more accurately match the semantics in the bug report text to the inducing changeset. More specifically, we describe the FBL-BERT model, based on the prior work by Khattab et al. [21], which speeds up the retrieval of results while performing fine grained matching across all embeddings in the two documents. The results show an improvement in retrieval accuracy for bug reports that lack localization hints or have only partial hints. We also evaluate different approaches for utilizing changesets in BERT-like models, producing recommendations on the input data granularity and the use of special tokens for the purpose of capturing changeset semantics.\n\n\nICSE '22, May 21-29, 2022, Pittsburgh, PA, USA \u00a9 2022 Association for Computing Machinery. ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00 https://doi.org/10.1145/1122445.1122456\n\nFigure 1 :\n1BERT-based architectures for changesets retrieval\n\nFigure 2 :\n2FBL-BERT for changeset-based bug localization pipeline.\n\n\nHow effective is FBL-BERT when compared to (1) state-of-the-art techniques based on the VSM, and (2) related BERT-based architectures?\n\nFigure 4 :\n4Average retrieval time per a bug report with different sizes of search space ( TBERT-Single, TBERT-Siamese, FBL-BERT).\n\n\nBug 83699 Summary: Font reset to default after screen saver Description: All editors and views using a StyledText widget have the font reset to default after coming back from my screen saver. [..]. This breakpoint gets hit when I return from the screen saver: [..] StyledText(Control).updateFont(Font, Font) line: 2913BERT Tokenizer \n\nfont \n[CLS] [Q] \nreset \n\nD -encoding \n\nARC -encoding \n\nARC L -encoding \n\ngit diff 68f73a31d3bd23bb9be3de8de4cfa69258483b46 \n+++ b/[..]/org/eclipse/swt/widgets/Composite.java \n[empty lines omitted] \n-void updateFont (Font oldFont, Font newFont) { \n+ boolean updateFont (Font oldFont, Font newFont) { \nControl [] children = _getChildren (); \nfor (int i=0; i<children.length; i++) { \nControl control = children [i]; \n\n+++ b/[..]/org/eclipse/swt/widgets/Control.java \n[empty lines omitted] \n-void updateFont (Font oldFont, Font newFont) { \n-\nFont font = getFont (); \n-\nif (font.equals (oldFont)) setFont (newFont); \n+ boolean updateFont (Font oldFont, Font newFont) { \n+ \nboolean sameFont = getFont ().equals (oldFont); \n+ \nif (!sameFont) setFont (newFont); \n+ \nreturn !sameFont; \n} \nvoid updateLayout (boolean resize, boolean all) { \n\nfont \nline [SEP] \n\nvoid \n[CLS] [D] \nupdate \nbool-\nean \nall [SEP] \n\nbool-\nean \n[CLS] [A] \nupdate \nsame font \n\nvoid \n[R] \nupdate \nnew font \n\ncont-\nrol \n[C] \nchild-\nren \n\nbool-\nean \nall [SEP] \n\nbool-\nean \n\n[CLS] \n\n[A] \nupdate \nnew font \n\nvoid \n[R] \nupdate \nnew font \n\ncont-\nrol \n[C] \nchild-\nren \n\nchild-\nren \ni \n\nvoid \n[R] \nupdate \nnew font \n\nbool-\nean \n[A] \nupdate \nsame font \n\nvoid \n[C] \nupdate \nbool-\nean \nall [SEP] \n\nFigure 3: Changeset encoding strategies. \n\nfor added, -for removed, and an empty space for context lines. The \nlines in each group are concatenated to create a sequence to which \nwe pre-append a special token: [A] for the sequence of added lines, \n[R] for the sequence of removed lines and [C] for the sequence of \ncontext lines. Finally, all the sequences are concatenated together \nto create an input for the model. By grouping different parts of \nchangesets based on their characteristics, we aim to investigate \nwhether any particular type of modification is more beneficial than \nthe other. With the ARC strategy the model is given an opportu-\nnity to learn how to combine information of different types and, \nif necessary, decide to disregard a portion of it if it poorly affects \nperformance. \nARC L : Similarly as in ARC, a changeset is divided into lines, how-\never ARC L encoding does not group the lines. Instead, it preserves \nthe ordering of lines within a changeset, such that special tokens \n[A], [R]\n\nTable 1 :\n1Projects in evaluation dataset.#Bugs #Changesets #Changeset-files #Hunks \n\nAspectJ \n200 \n2,939 \n14,030 \n23,446 \nJDT \n94 \n13,860 \n58,619 \n150,630 \nPDE \n60 \n9,419 \n42,303 \n100,373 \nSWT \n90 \n10,206 \n25,666 \n69,833 \nTomcat \n193 \n10,034 \n30,866 \n72,134 \nZXing \n20 \n843 \n2,846 \n6,165 \n\n\n\nTable 2 :\n2Mean Reciprocal Rank (MRR) of changeset-based BL techniques for different types of bug reports.Bug report type \n\n\n\nTable 3 :\n3Retrieval performance for different configurations of FBL-BERT.#Bugs \nMRR MAP P@1 \nP@3 \nP@5 \nMRR \nMAP P@1 \nP@3 \nP@5 \nMRR MAP P@1 \nP@3 \nP@5 \n\n\n\nSciBERT: Pretrained Language Model for Scientific Text. Iz Beltagy, Kyle Lo, Arman Cohan, arXiv:arXiv:1903.10676EMNLP. Iz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: Pretrained Language Model for Scientific Text. In EMNLP. arXiv:arXiv:1903.10676\n\nOrca: Differential Bug Localization in Large-scale Services. Ranjita Bhagwan, Rahul Kumar, Chandra Sekhar Maddila, Adithya Abraham Philip, Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation. the 12th USENIX Conference on Operating Systems Design and ImplementationCarlsbad, CA, USARanjita Bhagwan, Rahul Kumar, Chandra Sekhar Maddila, and Adithya Abraham Philip. 2018. Orca: Differential Bug Localization in Large-scale Services. In Proceedings of the 12th USENIX Conference on Operating Systems Design and Implementation (Carlsbad, CA, USA) (OSDI'18). 493-509.\n\nBugPecker: Locating Faulty Methods with Deep Learning on Revision Graphs. J Cao, S Yang, W Jiang, H Zeng, B Shen, H Zhong, 35th IEEE/ACM International Conference on Automated Software Engineering (ASE). J. Cao, S. Yang, W. Jiang, H. Zeng, B. Shen, and H. Zhong. 2020. BugPecker: Locat- ing Faulty Methods with Deep Learning on Revision Graphs. In 35th IEEE/ACM International Conference on Automated Software Engineering (ASE).\n\nZimin Chen, Martin Monperrus, arXiv:1904.03061A literature study of embeddings on source code. arXiv preprintZimin Chen and Martin Monperrus. 2019. A literature study of embeddings on source code. arXiv preprint arXiv:1904.03061 (2019).\n\nA Similarity Integration Method based Information Retrieval and Word Embedding in Bug Localization. S Cheng, X Yan, A A Khan, 2020 IEEE 20th International Conference on Software Quality, Reliability and Security (QRS). S. Cheng, X. Yan, and A. A. Khan. 2020. A Similarity Integration Method based Information Retrieval and Word Embedding in Bug Localization. In 2020 IEEE 20th International Conference on Software Quality, Reliability and Security (QRS).\n\nChangeset-Based Topic Modeling of Software Repositories. C S Corley, K Damevski, N A Kraft, IEEE Transactions on Software Engineering. C. S. Corley, K. Damevski, and N. A. Kraft. 2018. Changeset-Based Topic Modeling of Software Repositories. IEEE Transactions on Software Engineering (2018).\n\nDeeper Text Understanding for IR with Contextual Neural Language Modeling. Zhuyun Dai, Jamie Callan, Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'19). the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'19)Zhuyun Dai and Jamie Callan. 2019. Deeper Text Understanding for IR with Contextual Neural Language Modeling. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR'19).\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies.\n\nFine-grained and accurate source code differencing. Jean-R\u00e9my Falleri, Flor\u00e9al Morandat, Xavier Blanc, Matias Martinez, 10.1145/2642937.2642982ACM/IEEE International Conference on Automated Software Engineering, ASE '14. Vasteras, SwedenJean-R\u00e9my Falleri, Flor\u00e9al Morandat, Xavier Blanc, Matias Martinez, and Mar- tin Monperrus. 2014. Fine-grained and accurate source code differencing. In ACM/IEEE International Conference on Automated Software Engineering, ASE '14, Vasteras, Sweden -September 15 -19, 2014. 313-324. https://doi.org/10.1145/ 2642937.2642982\n\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, arXiv:2007.15779Jianfeng Gao, and Hoifung Poon. 2021. Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing. cs.CLYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2021. Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing. arXiv:2007.15779 [cs.CL]\n\nSemantically Enhanced Software Traceability Using Deep Learning Techniques. Jin Guo, Jinghui Cheng, Jane Cleland-Huang, Proceedings of the 39th International Conference on Software Engineering (ICSE '17). the 39th International Conference on Software Engineering (ICSE '17)Jin Guo, Jinghui Cheng, and Jane Cleland-Huang. 2017. Semantically Enhanced Software Traceability Using Deep Learning Techniques. In Proceedings of the 39th International Conference on Software Engineering (ICSE '17).\n\nThe impact of tangled code changes. Kim Herzig, Andreas Zeller, 10th Working Conference on Mining Software Repositories (MSR). Kim Herzig and Andreas Zeller. 2013. The impact of tangled code changes. In 2013 10th Working Conference on Mining Software Repositories (MSR).\n\nThe Impact of Tangled Code Changes. Kim Herzig, Andreas Zeller, MSR '13Proceedings of the 10th Working Conference on Mining Software Repositories. the 10th Working Conference on Mining Software RepositoriesSan Francisco, CA, USAKim Herzig and Andreas Zeller. 2013. The Impact of Tangled Code Changes. In Proceedings of the 10th Working Conference on Mining Software Repositories (San Francisco, CA, USA) (MSR '13). 121-130.\n\nCC2Vec: Distributed representations of code changes. Thong Hoang, Hong Jin Kang, David Lo, Julia Lawall, Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering. the ACM/IEEE 42nd International Conference on Software EngineeringThong Hoang, Hong Jin Kang, David Lo, and Julia Lawall. 2020. CC2Vec: Dis- tributed representations of code changes. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering.\n\nDeep Transfer Bug Localization. X Huo, F Thung, M Li, D Lo, S Shi, IEEE Transactions on Software Engineering. X. Huo, F. Thung, M. Li, D. Lo, and S. Shi. 2019. Deep Transfer Bug Localization. IEEE Transactions on Software Engineering (2019).\n\nDeep Transfer Bug Localization. X Huo, F Thung, M Li, D Lo, S Shi, 10.1109/TSE.2019.2920771IEEE Transactions on Software Engineering. X. Huo, F. Thung, M. Li, D. Lo, and S. Shi. 2019. Deep Transfer Bug Localization. IEEE Transactions on Software Engineering (2019), 1-1. https://doi.org/10.1109/ TSE.2019.2920771\n\nAutomatically generating commit messages from diffs using neural machine translation. Siyuan Jiang, Ameer Armaly, Collin Mcmillan, 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE). Siyuan Jiang, Ameer Armaly, and Collin McMillan. 2017. Automatically generat- ing commit messages from diffs using neural machine translation. In 2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE).\n\nJeff Johnson, Matthijs Douze, Herv\u00e9 J\u00e9gou, arXiv:1702.08734Billion-scale similarity search with GPUs. arXiv preprintJeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. 2017. Billion-scale similarity search with GPUs. arXiv preprint arXiv:1702.08734 (2017).\n\nOpen-Vocabulary Models for Source Code. Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, Andrea Janes, Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings. the ACM/IEEE 42nd International Conference on Software Engineering: Companion ProceedingsSeoul, South KoreaICSE '20Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, and Andrea Janes. 2020. Open-Vocabulary Models for Source Code. In Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings (Seoul, South Korea) (ICSE '20). 294-295.\n\nColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT (SIGIR '20). Omar Khattab, Matei Zaharia, Omar Khattab and Matei Zaharia. 2020. ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT (SIGIR '20).\n\nWhere Should We Fix This Bug? A Two-Phase Recommendation Model. D Kim, Y Tao, S Kim, A Zeller, IEEE Transactions on Software Engineering. 3911D. Kim, Y. Tao, S. Kim, and A. Zeller. 2013. Where Should We Fix This Bug? A Two-Phase Recommendation Model. IEEE Transactions on Software Engineering 39, 11 (Nov 2013), 1597-1610.\n\nPotential Biases in Bug Localization: Do They Matter. Pavneet Singh Kochhar, Yuan Tian, David Lo, Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering. the 29th ACM/IEEE International Conference on Automated Software EngineeringVasteras, SwedenPavneet Singh Kochhar, Yuan Tian, and David Lo. 2014. Potential Biases in Bug Localization: Do They Matter?. In Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering (Vasteras, Sweden) (ASE '14). 803-814.\n\nCombining Deep Learning with Information Retrieval to Localize Buggy Files for Bug Reports (N). A N Lam, A T Nguyen, H A Nguyen, T N Nguyen, 30th IEEE/ACM International Conference on Automated Software Engineering (ASE. A. N. Lam, A. T. Nguyen, H. A. Nguyen, and T. N. Nguyen. 2015. Combining Deep Learning with Information Retrieval to Localize Buggy Files for Bug Reports (N). In 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE). 476-481.\n\nBug Localization with Combination of Deep Learning and Information Retrieval. An Ngoc Lam, Anh Tuan Nguyen, Anh Hoan, Tien N Nguyen, Nguyen, 10.1109/ICPC.2017.24Proceedings of the 25th International Conference on Program Comprehension. the 25th International Conference on Program ComprehensionBuenos Aires, ArgentinaIEEE PressICPC '17)An Ngoc Lam, Anh Tuan Nguyen, Hoan Anh Nguyen, and Tien N. Nguyen. 2017. Bug Localization with Combination of Deep Learning and Information Retrieval. In Proceedings of the 25th International Conference on Program Comprehension (Buenos Aires, Argentina) (ICPC '17). IEEE Press, 218-229. https://doi.org/10. 1109/ICPC.2017.24\n\nBioBERT: a pre-trained biomedical language representation model for biomedical text mining. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, Jaewoo Kang, 10.1093/bioinformatics/btz682Bioinformatics. Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2019. BioBERT: a pre-trained biomedical lan- guage representation model for biomedical text mining. Bioinformatics (Sep 2019). https://doi.org/10.1093/bioinformatics/btz682\n\nJinfeng Lin, Yalin Liu, Qingkai Zeng, Meng Jiang, Jane Cleland-Huang, arXiv:2102.04411Traceability Transformed: Generating more Accurate Links with Pre-Trained BERT Models. cs.SEJinfeng Lin, Yalin Liu, Qingkai Zeng, Meng Jiang, and Jane Cleland-Huang. 2021. Traceability Transformed: Generating more Accurate Links with Pre-Trained BERT Models. arXiv:2102.04411 [cs.SE]\n\nRoc\u00eco Cabrera Lozoya, Arnaud Baumann, Antonino Sabetta, Michele Bezzi, arXiv:1911.07605Commit2Vec: Learning Distributed Representations of Code Changes. Roc\u00eco Cabrera Lozoya, Arnaud Baumann, Antonino Sabetta, and Michele Bezzi. 2019. Commit2Vec: Learning Distributed Representations of Code Changes. arXiv:1911.07605\n\nCan development work describe itself. Walid Maalej, Hans-J\u00f6rg Happel, 7th IEEE Working Conference on Mining Software Repositories. Walid Maalej and Hans-J\u00f6rg Happel. 2010. Can development work describe itself?. In 2010 7th IEEE Working Conference on Mining Software Repositories (MSR 2010).\n\nAn Empirical Study of Build Maintenance Effort. Shane Mcintosh, Bram Adams, H D Thanh, Yasutaka Nguyen, Ahmed E Kamei, Hassan, Proceedings of the 33rd International Conference on Software Engineering. the 33rd International Conference on Software EngineeringWaikiki, Honolulu, HI, USAShane McIntosh, Bram Adams, Thanh H.D. Nguyen, Yasutaka Kamei, and Ahmed E. Hassan. 2011. An Empirical Study of Build Maintenance Effort. In Proceedings of the 33rd International Conference on Software Engineering (Waikiki, Honolulu, HI, USA) (ICSE '11). 141-150.\n\nDistributed Representations of Words and Phrases and their Compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, Jeff Dean, Advances in Neural Information Processing Systems. C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. WeinbergerTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed Representations of Words and Phrases and their Compositionality. In Advances in Neural Information Processing Systems, C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger (Eds.).\n\nOn the relationship between bug reports and queries for text retrieval-based bug localization. Chris Mills, Esteban Parra, Jevgenija Pantiuchina, Gabriele Bavota, Sonia Haiduc, Empirical Software Engineering. 25Chris Mills, Esteban Parra, Jevgenija Pantiuchina, Gabriele Bavota, and Sonia Haiduc. 2020. On the relationship between bug reports and queries for text retrieval-based bug localization. Empirical Software Engineering 25 (2020).\n\nIndustry-scale IR-based Bug Localization: A Perspective from Facebook. Vijayaraghavan Murali, Lee Gross, Rebecca Qian, Satish Chandra, Proceedings of the 42nd International Conference on Software Engineering (ICSE '20). the 42nd International Conference on Software Engineering (ICSE '20)Vijayaraghavan Murali, Lee Gross, Rebecca Qian, and Satish Chandra. 2020. Industry-scale IR-based Bug Localization: A Perspective from Facebook. In Pro- ceedings of the 42nd International Conference on Software Engineering (ICSE '20).\n\nThe impact of refactoring changes on the SZZ algorithm: An empirical study. E C Neto, D A Da Costa, U Kulesza, IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER. E. C. Neto, D. A. da Costa, and U. Kulesza. 2018. The impact of refactoring changes on the SZZ algorithm: An empirical study. In IEEE 25th International Conference on Software Analysis, Evolution and Reengineering (SANER) (SANER 2018).\n\nA Topic-based Approach for Narrowing the Search Space of Buggy Files from a Bug Report. A T Nguyen, T T Nguyen, J Al-Kofahi, H V Nguyen, T N Nguyen, 10.1109/ASE.2011.6100062Proceedings of the 26th IEEE/ACM International Conference on Automated Software Engineering. the 26th IEEE/ACM International Conference on Automated Software EngineeringA. T. Nguyen, T. T. Nguyen, J. Al-Kofahi, H. V. Nguyen, and T. N. Nguyen. 2011. A Topic-based Approach for Narrowing the Search Space of Buggy Files from a Bug Report. In Proceedings of the 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011). 263-272. https://doi.org/10.1109/ ASE.2011.6100062\n\nA topic-based approach for narrowing the search space of buggy files from a bug report. A T Nguyen, T T Nguyen, J Al-Kofahi, H V Nguyen, T N Nguyen, 26th IEEE/ACM International Conference on Automated Software Engineering. A. T. Nguyen, T. T. Nguyen, J. Al-Kofahi, H. V. Nguyen, and T. N. Nguyen. 2011. A topic-based approach for narrowing the search space of buggy files from a bug report. In 2011 26th IEEE/ACM International Conference on Automated Software Engineering (ASE 2011). 263-272.\n\nRodrigo Nogueira, Kyunghyun Cho, arXiv:1901.04085Passage Re-ranking with BERT. cs.IRRodrigo Nogueira and Kyunghyun Cho. 2020. Passage Re-ranking with BERT. arXiv:1901.04085 [cs.IR]\n\nGloVe: Global Vectors for Word Representation. Jeffrey Pennington, Richard Socher, Christopher D Manning, Empirical Methods in Natural Language Processing. EMNLPJeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe: Global Vectors for Word Representation. In Empirical Methods in Natural Lan- guage Processing (EMNLP).\n\nDissecting Contextual Word Embeddings: Architecture and Representation. Matthew Peters, Mark Neumann, Luke Zettlemoyer, Wen-Tau Yih, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingMatthew Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. 2018. Dissecting Contextual Word Embeddings: Architecture and Representation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing.\n\nScaffle: Bug Localization on Millions of Files. Michael Pradel, Vijayaraghavan Murali, Rebecca Qian, Mateusz Machalica, Erik Meijer, Satish Chandra, Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis. the 29th ACM SIGSOFT International Symposium on Software Testing and AnalysisISSTA 2020Michael Pradel, Vijayaraghavan Murali, Rebecca Qian, Mateusz Machalica, Erik Meijer, and Satish Chandra. 2020. Scaffle: Bug Localization on Millions of Files. In Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA 2020).\n\nNils Reimers, Iryna Gurevych, arXiv:1908.10084Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. cs.CLNils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. arXiv:1908.10084 [cs.CL]\n\nGiovanni Rosa, Luca Pascarella, Simone Scalabrino, Rosalia Tufano, Gabriele Bavota, Michele Lanza, Rocco Oliveto, arXiv:2102.03300Evaluating SZZ Implementations Through a Developer-informed Oracle. cs.SEGiovanni Rosa, Luca Pascarella, Simone Scalabrino, Rosalia Tufano, Gabriele Bavota, Michele Lanza, and Rocco Oliveto. 2021. Evaluating SZZ Implementations Through a Developer-informed Oracle. arXiv:2102.03300 [cs.SE]\n\nCommit Guru: Analytics and Risk Prediction of Software Commits. Christoffer Rosen, Ben Grawi, Emad Shihab, Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering. the 2015 10th Joint Meeting on Foundations of Software EngineeringBergamo, Italy; New York, NY, USAAssociation for Computing MachineryESEC/FSE 2015)Christoffer Rosen, Ben Grawi, and Emad Shihab. 2015. Commit Guru: Analytics and Risk Prediction of Software Commits. In Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering (Bergamo, Italy) (ESEC/FSE 2015). Association for Computing Machinery, New York, NY, USA, 966-969.\n\nRetrieval on Source Code: A Neural Code Search. Saksham Sachdev, Hongyu Li, Sifei Luan, Seohyun Kim, Koushik Sen, Satish Chandra, Proceedings of the 2nd Workshop on Machine Learning and Programming Language. the 2nd Workshop on Machine Learning and Programming LanguageSaksham Sachdev, Hongyu Li, Sifei Luan, Seohyun Kim, Koushik Sen, and Satish Chandra. 2018. Retrieval on Source Code: A Neural Code Search. In Proceedings of the 2nd Workshop on Machine Learning and Programming Language (MAPL 2018).\n\nImproving Bug Localization Using Structured Information Retrieval. K Ripon, Matthew Saha, Sarfraz Lease, Dewayne E Khurshid, Perry, Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering. the 28th IEEE/ACM International Conference on Automated Software EngineeringSilicon Valley, CA, USARipon K. Saha, Matthew Lease, Sarfraz Khurshid, and Dewayne E. Perry. 2013. Im- proving Bug Localization Using Structured Information Retrieval. In Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering (Silicon Valley, CA, USA) (ASE'13). 345-355.\n\nContinuous Deployment at Facebook and OANDA. T Savor, M Douglas, M Gentili, L Williams, K Beck, M Stumm, 2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C). T. Savor, M. Douglas, M. Gentili, L. Williams, K. Beck, and M. Stumm. 2016. Continuous Deployment at Facebook and OANDA. In 2016 IEEE/ACM 38th Inter- national Conference on Software Engineering Companion (ICSE-C). 21-30.\n\nJapanese and Korean voice search. M Schuster, K Nakajima, 2012 IEEE International Conference on Acoustics, Speech and Signal Processing. ICASSPM. Schuster and K. Nakajima. 2012. Japanese and Korean voice search. In 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP).\n\nCode and Named Entity Recognition in StackOverflow. Jeniya Tabassum, Mounica Maddela, Wei Xu, Alan Ritter, 10.18653/v1/2020.acl-main.443Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Online. the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, OnlineJeniya Tabassum, Mounica Maddela, Wei Xu, and Alan Ritter. 2020. Code and Named Entity Recognition in StackOverflow. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Com- putational Linguistics, Online, 4913-4926. https://doi.org/10.18653/v1/2020.acl- main.443\n\nCode and Named Entity Recognition in StackOverflow. Jeniya Tabassum, Mounica Maddela, Wei Xu, Alan Ritter, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsJeniya Tabassum, Mounica Maddela, Wei Xu, and Alan Ritter. 2020. Code and Named Entity Recognition in StackOverflow. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.\n\nThe impact of IR-based classifier configuration on the performance and the effort of method-level bug localization. Chakkrit Tantithamthavorn, Ahmed E Surafel Lemma Abebe, Akinori Hassan, Kenichi Ihara, Matsumoto, Information and Software Technology. Chakkrit Tantithamthavorn, Surafel Lemma Abebe, Ahmed E. Hassan, Akinori Ihara, and Kenichi Matsumoto. 2018. The impact of IR-based classifier con- figuration on the performance and the effort of method-level bug localization. Information and Software Technology (2018).\n\nBERT Rediscovers the Classical NLP Pipeline. Ian Tenney, Dipanjan Das, Ellie Pavlick, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational LinguisticsIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT Rediscovers the Classical NLP Pipeline. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.\n\n2020. Perceptions, Expectations, and Challenges in Defect Prediction. Z Wan, X Xia, A E Hassan, D Lo, J Yin, X Yang, IEEE Transactions on Software Engineering. 4611Z. Wan, X. Xia, A. E. Hassan, D. Lo, J. Yin, and X. Yang. 2020. Perceptions, Expectations, and Challenges in Defect Prediction. IEEE Transactions on Software Engineering 46, 11 (2020).\n\nEvaluating the Usefulness of IR-Based Fault Localization Techniques. Qianqian Wang, Chris Parnin, Alessandro Orso, Proceedings of the 2015 International Symposium on Software Testing and Analysis. the 2015 International Symposium on Software Testing and AnalysisBaltimore, MD, USAQianqian Wang, Chris Parnin, and Alessandro Orso. 2015. Evaluating the Use- fulness of IR-Based Fault Localization Techniques. In Proceedings of the 2015 International Symposium on Software Testing and Analysis (Baltimore, MD, USA) (ISSTA 2015). 1-11.\n\nEvaluating the Usefulness of IR-Based Fault Localization Techniques. Qianqian Wang, Chris Parnin, Alessandro Orso, Proceedings of the 2015 International Symposium on Software Testing and Analysis. the 2015 International Symposium on Software Testing and AnalysisBaltimore, MD, USAQianqian Wang, Chris Parnin, and Alessandro Orso. 2015. Evaluating the Use- fulness of IR-Based Fault Localization Techniques. In Proceedings of the 2015 International Symposium on Software Testing and Analysis (ISSTA 2015) (Balti- more, MD, USA). 1-11.\n\nVersion History, Similar Report, and Structure: Putting Them Together for Improved Bug Localization. Shaowei Wang, David Lo, Proceedings of the 22Nd International Conference on Program Comprehension. the 22Nd International Conference on Program ComprehensionHyderabad, IndiaICPCShaowei Wang and David Lo. 2014. Version History, Similar Report, and Structure: Putting Them Together for Improved Bug Localization. In Proceedings of the 22Nd International Conference on Program Comprehension (Hyderabad, India) (ICPC 2014). 53-63.\n\nLocus: Locating Bugs from Software Changes. Ming Wen, Rongxin Wu, Shing-Chi Cheung, Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering. the 31st IEEE/ACM International Conference on Automated Software EngineeringSingapore, SingaporeMing Wen, Rongxin Wu, and Shing-Chi Cheung. 2016. Locus: Locating Bugs from Software Changes. In Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering (Singapore, Singapore) (ASE 2016). 262-273.\n\nBoosting Bug-Report-Oriented Fault Localization with Segmentation and Stack-Trace Analysis. Chu-Pan Wong, Yingfei Xiong, Hongyu Zhang, Dan Hao, Lu Zhang, Hong Mei, Proceedings of the 2014 IEEE International Conference on Software Maintenance and Evolution (ICSME '14). the 2014 IEEE International Conference on Software Maintenance and Evolution (ICSME '14)Chu-Pan Wong, Yingfei Xiong, Hongyu Zhang, Dan Hao, Lu Zhang, and Hong Mei. 2014. Boosting Bug-Report-Oriented Fault Localization with Segmentation and Stack-Trace Analysis. In Proceedings of the 2014 IEEE International Conference on Software Maintenance and Evolution (ICSME '14). 181-190.\n\nChange-Locator: Locate Crash-Inducing Changes Based on Crash Reports. Rongxin Wu, Ming Wen, Shing-Chi Cheung, Hongyu Zhang, Proceedings of the 40th International Conference on Software Engineering (ICSE '18). the 40th International Conference on Software Engineering (ICSE '18)Rongxin Wu, Ming Wen, Shing-Chi Cheung, and Hongyu Zhang. 2018. Change- Locator: Locate Crash-Inducing Changes Based on Crash Reports. In Proceedings of the 40th International Conference on Software Engineering (ICSE '18).\n\nLearning to Rank Relevant Files for Bug Reports Using Domain Knowledge. Xin Ye, Razvan Bunescu, Chang Liu, Proceedings of the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering. the 22Nd ACM SIGSOFT International Symposium on Foundations of Software EngineeringHong Kong, ChinaXin Ye, Razvan Bunescu, and Chang Liu. 2014. Learning to Rank Relevant Files for Bug Reports Using Domain Knowledge. In Proceedings of the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering (Hong Kong, China) (FSE 2014). 689-699.\n\nDeep Just-in-Time Defect Prediction: How Far Are We?. Zhengran Zeng, Yuqun Zhang, Haotian Zhang, Lingming Zhang, ISSTA 2021Zhengran Zeng, Yuqun Zhang, Haotian Zhang, and Lingming Zhang. 2021. Deep Just-in-Time Defect Prediction: How Far Are We? (ISSTA 2021).\n\nFineLocator: A novel approach to method-level fine-grained bug localization by query expansion. Wen Zhang, Ziqiang Li, Qing Wang, Juan Li, Information and Software Technology. 110Wen Zhang, Ziqiang Li, Qing Wang, and Juan Li. 2019. FineLocator: A novel approach to method-level fine-grained bug localization by query expansion. Information and Software Technology 110 (2019), 121-135.\n\nWhere should the bugs be fixed? More accurate information retrieval-based bug localization based on bug reports. J Zhou, H Zhang, D Lo, 10.1109/ICSE.2012.6227210Proceedings of the 34th International Conference on Software Engineering (ICSE). the 34th International Conference on Software Engineering (ICSE)J. Zhou, H. Zhang, and D. Lo. 2012. Where should the bugs be fixed? More accurate information retrieval-based bug localization based on bug reports. In Proceedings of the 34th International Conference on Software Engineering (ICSE). 14-24. https://doi.org/10.1109/ICSE.2012.6227210\n\nCooBa: Cross-project Bug Localization via Adversarial Transfer Learning. Ziye Zhu, Y Li, Hanghang Tong, Yu Wang, IJCAI. Ziye Zhu, Y. Li, Hanghang Tong, and Yu Wang. 2020. CooBa: Cross-project Bug Localization via Adversarial Transfer Learning. In IJCAI.\n\nHow Practitioners Perceive Automated Bug Report Management Techniques. W Zou, D Lo, Z Chen, X Xia, Y Feng, B Xu, IEEE Transactions on Software Engineering. 46W. Zou, D. Lo, Z. Chen, X. Xia, Y. Feng, and B. Xu. 2020. How Practitioners Perceive Automated Bug Report Management Techniques. IEEE Transactions on Software Engineering 46, 8 (2020).\n", "annotations": {"author": "[{\"end\":172,\"start\":51},{\"end\":289,\"start\":173}]", "publisher": null, "author_last_name": "[{\"end\":71,\"start\":61},{\"end\":190,\"start\":182}]", "author_first_name": "[{\"end\":60,\"start\":51},{\"end\":181,\"start\":173}]", "author_affiliation": "[{\"end\":171,\"start\":93},{\"end\":288,\"start\":210}]", "title": "[{\"end\":48,\"start\":1},{\"end\":337,\"start\":290}]", "venue": null, "abstract": "[{\"end\":2037,\"start\":413}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3171,\"start\":3167},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":3174,\"start\":3171},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3397,\"start\":3393},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":3831,\"start\":3827},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4123,\"start\":4119},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":5260,\"start\":5256},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5526,\"start\":5523},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8090,\"start\":8086},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":8093,\"start\":8090},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8151,\"start\":8147},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8572,\"start\":8568},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":8575,\"start\":8572},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8886,\"start\":8882},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9019,\"start\":9016},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9022,\"start\":9019},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9025,\"start\":9022},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9028,\"start\":9025},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":9031,\"start\":9028},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":9034,\"start\":9031},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9246,\"start\":9242},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":9249,\"start\":9246},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":9252,\"start\":9249},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":9383,\"start\":9379},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":9400,\"start\":9396},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":9403,\"start\":9400},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":9406,\"start\":9403},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9631,\"start\":9628},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9634,\"start\":9631},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":9637,\"start\":9634},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":9640,\"start\":9637},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":10404,\"start\":10400},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":10407,\"start\":10404},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10791,\"start\":10787},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":11160,\"start\":11156},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11702,\"start\":11699},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11704,\"start\":11702},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11707,\"start\":11704},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11710,\"start\":11707},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11713,\"start\":11710},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11716,\"start\":11713},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11719,\"start\":11716},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12259,\"start\":12255},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12274,\"start\":12270},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12344,\"start\":12341},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":12548,\"start\":12544},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":12839,\"start\":12835},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":12842,\"start\":12839},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":13544,\"start\":13540},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14619,\"start\":14615},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":17231,\"start\":17227},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":18087,\"start\":18084},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18090,\"start\":18087},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":18093,\"start\":18090},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18161,\"start\":18157},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":18164,\"start\":18161},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":18960,\"start\":18956},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19997,\"start\":19994},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":20148,\"start\":20145},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":20151,\"start\":20148},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":20154,\"start\":20151},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":21851,\"start\":21847},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":23658,\"start\":23654},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":23768,\"start\":23764},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":24681,\"start\":24677},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":24983,\"start\":24979},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":25101,\"start\":25098},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":27028,\"start\":27024},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":27031,\"start\":27028},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":27159,\"start\":27155},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":27248,\"start\":27244},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":27458,\"start\":27454},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":27807,\"start\":27804},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":27810,\"start\":27807},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":27813,\"start\":27810},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":28459,\"start\":28455},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":28518,\"start\":28514},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":28953,\"start\":28949},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":29077,\"start\":29073},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":30297,\"start\":30293},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":30440,\"start\":30436},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":32195,\"start\":32191},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":32729,\"start\":32725},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":32732,\"start\":32729},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":32863,\"start\":32859},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":33271,\"start\":33267},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":34674,\"start\":34670},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":35017,\"start\":35014},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":35427,\"start\":35423},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":36118,\"start\":36114},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":42829,\"start\":42825},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":43999,\"start\":43995},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":44482,\"start\":44479},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":44485,\"start\":44482},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":44488,\"start\":44485},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":45232,\"start\":45228},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":48086,\"start\":48083},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":48089,\"start\":48086},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":48356,\"start\":48352},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":48479,\"start\":48475},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":48621,\"start\":48617},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":48624,\"start\":48621},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":48759,\"start\":48755},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":48762,\"start\":48759},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":49518,\"start\":49514},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":49521,\"start\":49518},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":49524,\"start\":49521},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":49686,\"start\":49682},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":50719,\"start\":50715},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":50899,\"start\":50895},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":51040,\"start\":51036},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":51178,\"start\":51174},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":51449,\"start\":51445},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":51582,\"start\":51578},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":51822,\"start\":51818},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":52116,\"start\":52112},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":52506,\"start\":52502},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":52650,\"start\":52646},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":52887,\"start\":52883},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":52890,\"start\":52887},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":52995,\"start\":52991},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":53206,\"start\":53203},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":53324,\"start\":53320},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":53973,\"start\":53969},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":54122,\"start\":54118},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":54412,\"start\":54408},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":55143,\"start\":55139},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":55163,\"start\":55159},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":55286,\"start\":55283},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":56123,\"start\":56119}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":56773,\"start\":56596},{\"attributes\":{\"id\":\"fig_1\"},\"end\":56836,\"start\":56774},{\"attributes\":{\"id\":\"fig_2\"},\"end\":56905,\"start\":56837},{\"attributes\":{\"id\":\"fig_3\"},\"end\":57042,\"start\":56906},{\"attributes\":{\"id\":\"fig_4\"},\"end\":57174,\"start\":57043},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":59777,\"start\":57175},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":60070,\"start\":59778},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":60196,\"start\":60071},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":60350,\"start\":60197}]", "paragraph": "[{\"end\":3917,\"start\":2053},{\"end\":4688,\"start\":3919},{\"end\":5527,\"start\":4690},{\"end\":6108,\"start\":5529},{\"end\":6623,\"start\":6110},{\"end\":7290,\"start\":6625},{\"end\":7333,\"start\":7292},{\"end\":7789,\"start\":7335},{\"end\":8296,\"start\":7791},{\"end\":8855,\"start\":8320},{\"end\":12458,\"start\":8857},{\"end\":13301,\"start\":12502},{\"end\":14186,\"start\":13314},{\"end\":15373,\"start\":14216},{\"end\":18863,\"start\":15375},{\"end\":20408,\"start\":18894},{\"end\":21268,\"start\":20410},{\"end\":22908,\"start\":21270},{\"end\":24295,\"start\":22943},{\"end\":25596,\"start\":24297},{\"end\":25956,\"start\":25603},{\"end\":26419,\"start\":25958},{\"end\":26681,\"start\":26421},{\"end\":29198,\"start\":26683},{\"end\":30139,\"start\":29200},{\"end\":32129,\"start\":30165},{\"end\":33272,\"start\":32131},{\"end\":33715,\"start\":33284},{\"end\":34047,\"start\":33740},{\"end\":34307,\"start\":34070},{\"end\":34475,\"start\":34353},{\"end\":35559,\"start\":34486},{\"end\":37162,\"start\":35600},{\"end\":37971,\"start\":37164},{\"end\":40592,\"start\":37973},{\"end\":42460,\"start\":40594},{\"end\":46045,\"start\":42462},{\"end\":47720,\"start\":46082},{\"end\":47814,\"start\":47744},{\"end\":48357,\"start\":47816},{\"end\":49079,\"start\":48359},{\"end\":49907,\"start\":49081},{\"end\":50222,\"start\":49924},{\"end\":51950,\"start\":50262},{\"end\":52891,\"start\":51952},{\"end\":53974,\"start\":52928},{\"end\":54753,\"start\":53976},{\"end\":55771,\"start\":54782},{\"end\":56595,\"start\":55786}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":33739,\"start\":33716},{\"attributes\":{\"id\":\"formula_1\"},\"end\":34069,\"start\":34048},{\"attributes\":{\"id\":\"formula_2\"},\"end\":34333,\"start\":34308}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":30581,\"start\":30574},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":35627,\"start\":35620},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":44497,\"start\":44490}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2051,\"start\":2039},{\"attributes\":{\"n\":\"2\"},\"end\":8318,\"start\":8299},{\"attributes\":{\"n\":\"2.\"},\"end\":12500,\"start\":12461},{\"attributes\":{\"n\":\"3\"},\"end\":13312,\"start\":13304},{\"attributes\":{\"n\":\"3.1\"},\"end\":14214,\"start\":14189},{\"attributes\":{\"n\":\"3.2\"},\"end\":18892,\"start\":18866},{\"attributes\":{\"n\":\"3.3\"},\"end\":22941,\"start\":22911},{\"end\":25601,\"start\":25599},{\"attributes\":{\"n\":\"4.2\"},\"end\":30163,\"start\":30142},{\"attributes\":{\"n\":\"4.3\"},\"end\":33282,\"start\":33275},{\"attributes\":{\"n\":\"4.4\"},\"end\":34351,\"start\":34335},{\"end\":34484,\"start\":34478},{\"attributes\":{\"n\":\"5\"},\"end\":35569,\"start\":35562},{\"attributes\":{\"n\":\"5.1\"},\"end\":35598,\"start\":35572},{\"attributes\":{\"n\":\"5.2\"},\"end\":46080,\"start\":46048},{\"attributes\":{\"n\":\"5.3\"},\"end\":47742,\"start\":47723},{\"attributes\":{\"n\":\"6\"},\"end\":49922,\"start\":49910},{\"attributes\":{\"n\":\"6.1\"},\"end\":50260,\"start\":50225},{\"attributes\":{\"n\":\"6.2\"},\"end\":52926,\"start\":52894},{\"attributes\":{\"n\":\"6.3\"},\"end\":54780,\"start\":54756},{\"attributes\":{\"n\":\"7\"},\"end\":55784,\"start\":55774},{\"end\":56785,\"start\":56775},{\"end\":56848,\"start\":56838},{\"end\":57054,\"start\":57044},{\"end\":59788,\"start\":59779},{\"end\":60081,\"start\":60072},{\"end\":60207,\"start\":60198}]", "table": "[{\"end\":59777,\"start\":57495},{\"end\":60070,\"start\":59821},{\"end\":60196,\"start\":60178},{\"end\":60350,\"start\":60272}]", "figure_caption": "[{\"end\":56773,\"start\":56598},{\"end\":56836,\"start\":56787},{\"end\":56905,\"start\":56850},{\"end\":57042,\"start\":56908},{\"end\":57174,\"start\":57056},{\"end\":57495,\"start\":57177},{\"end\":59821,\"start\":59790},{\"end\":60178,\"start\":60083},{\"end\":60272,\"start\":60209}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17561,\"start\":17554},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18188,\"start\":18181},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19516,\"start\":19509},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21381,\"start\":21375},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21438,\"start\":21432},{\"end\":25595,\"start\":25589},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":33193,\"start\":33187},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":39361,\"start\":39355}]", "bib_author_first_name": "[{\"end\":60410,\"start\":60408},{\"end\":60424,\"start\":60420},{\"end\":60434,\"start\":60429},{\"end\":60673,\"start\":60666},{\"end\":60688,\"start\":60683},{\"end\":60703,\"start\":60696},{\"end\":60735,\"start\":60720},{\"end\":61281,\"start\":61280},{\"end\":61288,\"start\":61287},{\"end\":61296,\"start\":61295},{\"end\":61305,\"start\":61304},{\"end\":61313,\"start\":61312},{\"end\":61321,\"start\":61320},{\"end\":61639,\"start\":61634},{\"end\":61652,\"start\":61646},{\"end\":61973,\"start\":61972},{\"end\":61982,\"start\":61981},{\"end\":61989,\"start\":61988},{\"end\":61991,\"start\":61990},{\"end\":62386,\"start\":62385},{\"end\":62388,\"start\":62387},{\"end\":62398,\"start\":62397},{\"end\":62410,\"start\":62409},{\"end\":62412,\"start\":62411},{\"end\":62702,\"start\":62696},{\"end\":62713,\"start\":62708},{\"end\":63278,\"start\":63273},{\"end\":63295,\"start\":63287},{\"end\":63309,\"start\":63303},{\"end\":63323,\"start\":63315},{\"end\":63969,\"start\":63960},{\"end\":63986,\"start\":63979},{\"end\":64003,\"start\":63997},{\"end\":64017,\"start\":64011},{\"end\":64471,\"start\":64469},{\"end\":64482,\"start\":64476},{\"end\":64492,\"start\":64489},{\"end\":64507,\"start\":64500},{\"end\":64520,\"start\":64515},{\"end\":64538,\"start\":64530},{\"end\":64551,\"start\":64544},{\"end\":65029,\"start\":65026},{\"end\":65042,\"start\":65035},{\"end\":65054,\"start\":65050},{\"end\":65481,\"start\":65478},{\"end\":65497,\"start\":65490},{\"end\":65753,\"start\":65750},{\"end\":65769,\"start\":65762},{\"end\":66197,\"start\":66192},{\"end\":66209,\"start\":66205},{\"end\":66213,\"start\":66210},{\"end\":66225,\"start\":66220},{\"end\":66235,\"start\":66230},{\"end\":66630,\"start\":66629},{\"end\":66637,\"start\":66636},{\"end\":66646,\"start\":66645},{\"end\":66652,\"start\":66651},{\"end\":66658,\"start\":66657},{\"end\":66873,\"start\":66872},{\"end\":66880,\"start\":66879},{\"end\":66889,\"start\":66888},{\"end\":66895,\"start\":66894},{\"end\":66901,\"start\":66900},{\"end\":67246,\"start\":67240},{\"end\":67259,\"start\":67254},{\"end\":67274,\"start\":67268},{\"end\":67601,\"start\":67597},{\"end\":67619,\"start\":67611},{\"end\":67632,\"start\":67627},{\"end\":67904,\"start\":67890},{\"end\":67922,\"start\":67918},{\"end\":67936,\"start\":67930},{\"end\":67952,\"start\":67945},{\"end\":67967,\"start\":67961},{\"end\":68593,\"start\":68589},{\"end\":68608,\"start\":68603},{\"end\":68829,\"start\":68828},{\"end\":68836,\"start\":68835},{\"end\":68843,\"start\":68842},{\"end\":68850,\"start\":68849},{\"end\":69149,\"start\":69142},{\"end\":69169,\"start\":69165},{\"end\":69181,\"start\":69176},{\"end\":69712,\"start\":69711},{\"end\":69714,\"start\":69713},{\"end\":69721,\"start\":69720},{\"end\":69723,\"start\":69722},{\"end\":69733,\"start\":69732},{\"end\":69735,\"start\":69734},{\"end\":69745,\"start\":69744},{\"end\":69747,\"start\":69746},{\"end\":70172,\"start\":70170},{\"end\":70177,\"start\":70173},{\"end\":70186,\"start\":70183},{\"end\":70191,\"start\":70187},{\"end\":70203,\"start\":70200},{\"end\":70214,\"start\":70210},{\"end\":70216,\"start\":70215},{\"end\":70853,\"start\":70846},{\"end\":70865,\"start\":70859},{\"end\":70880,\"start\":70872},{\"end\":70895,\"start\":70886},{\"end\":70907,\"start\":70901},{\"end\":70917,\"start\":70913},{\"end\":70931,\"start\":70925},{\"end\":71260,\"start\":71253},{\"end\":71271,\"start\":71266},{\"end\":71284,\"start\":71277},{\"end\":71295,\"start\":71291},{\"end\":71307,\"start\":71303},{\"end\":71629,\"start\":71624},{\"end\":71652,\"start\":71646},{\"end\":71670,\"start\":71662},{\"end\":71687,\"start\":71680},{\"end\":71985,\"start\":71980},{\"end\":72003,\"start\":71994},{\"end\":72287,\"start\":72282},{\"end\":72302,\"start\":72298},{\"end\":72311,\"start\":72310},{\"end\":72313,\"start\":72312},{\"end\":72329,\"start\":72321},{\"end\":72343,\"start\":72338},{\"end\":72345,\"start\":72344},{\"end\":72865,\"start\":72860},{\"end\":72879,\"start\":72875},{\"end\":72894,\"start\":72891},{\"end\":72905,\"start\":72901},{\"end\":72907,\"start\":72906},{\"end\":72921,\"start\":72917},{\"end\":73448,\"start\":73443},{\"end\":73463,\"start\":73456},{\"end\":73480,\"start\":73471},{\"end\":73502,\"start\":73494},{\"end\":73516,\"start\":73511},{\"end\":73874,\"start\":73860},{\"end\":73886,\"start\":73883},{\"end\":73901,\"start\":73894},{\"end\":73914,\"start\":73908},{\"end\":74390,\"start\":74389},{\"end\":74392,\"start\":74391},{\"end\":74400,\"start\":74399},{\"end\":74402,\"start\":74401},{\"end\":74414,\"start\":74413},{\"end\":74843,\"start\":74842},{\"end\":74845,\"start\":74844},{\"end\":74855,\"start\":74854},{\"end\":74857,\"start\":74856},{\"end\":74867,\"start\":74866},{\"end\":74880,\"start\":74879},{\"end\":74882,\"start\":74881},{\"end\":74892,\"start\":74891},{\"end\":74894,\"start\":74893},{\"end\":75512,\"start\":75511},{\"end\":75514,\"start\":75513},{\"end\":75524,\"start\":75523},{\"end\":75526,\"start\":75525},{\"end\":75536,\"start\":75535},{\"end\":75549,\"start\":75548},{\"end\":75551,\"start\":75550},{\"end\":75561,\"start\":75560},{\"end\":75563,\"start\":75562},{\"end\":75924,\"start\":75917},{\"end\":75944,\"start\":75935},{\"end\":76153,\"start\":76146},{\"end\":76173,\"start\":76166},{\"end\":76193,\"start\":76182},{\"end\":76195,\"start\":76194},{\"end\":76520,\"start\":76513},{\"end\":76533,\"start\":76529},{\"end\":76547,\"start\":76543},{\"end\":76568,\"start\":76561},{\"end\":77023,\"start\":77016},{\"end\":77046,\"start\":77032},{\"end\":77062,\"start\":77055},{\"end\":77076,\"start\":77069},{\"end\":77092,\"start\":77088},{\"end\":77107,\"start\":77101},{\"end\":77572,\"start\":77568},{\"end\":77587,\"start\":77582},{\"end\":77820,\"start\":77812},{\"end\":77831,\"start\":77827},{\"end\":77850,\"start\":77844},{\"end\":77870,\"start\":77863},{\"end\":77887,\"start\":77879},{\"end\":77903,\"start\":77896},{\"end\":77916,\"start\":77911},{\"end\":78308,\"start\":78297},{\"end\":78319,\"start\":78316},{\"end\":78331,\"start\":78327},{\"end\":78928,\"start\":78921},{\"end\":78944,\"start\":78938},{\"end\":78954,\"start\":78949},{\"end\":78968,\"start\":78961},{\"end\":78981,\"start\":78974},{\"end\":78993,\"start\":78987},{\"end\":79444,\"start\":79443},{\"end\":79459,\"start\":79452},{\"end\":79473,\"start\":79466},{\"end\":79488,\"start\":79481},{\"end\":79490,\"start\":79489},{\"end\":80032,\"start\":80031},{\"end\":80041,\"start\":80040},{\"end\":80052,\"start\":80051},{\"end\":80063,\"start\":80062},{\"end\":80075,\"start\":80074},{\"end\":80083,\"start\":80082},{\"end\":80436,\"start\":80435},{\"end\":80448,\"start\":80447},{\"end\":80763,\"start\":80757},{\"end\":80781,\"start\":80774},{\"end\":80794,\"start\":80791},{\"end\":80803,\"start\":80799},{\"end\":81483,\"start\":81477},{\"end\":81501,\"start\":81494},{\"end\":81514,\"start\":81511},{\"end\":81523,\"start\":81519},{\"end\":82027,\"start\":82019},{\"end\":82051,\"start\":82046},{\"end\":82053,\"start\":82052},{\"end\":82082,\"start\":82075},{\"end\":82098,\"start\":82091},{\"end\":82474,\"start\":82471},{\"end\":82491,\"start\":82483},{\"end\":82502,\"start\":82497},{\"end\":82933,\"start\":82932},{\"end\":82940,\"start\":82939},{\"end\":82947,\"start\":82946},{\"end\":82949,\"start\":82948},{\"end\":82959,\"start\":82958},{\"end\":82965,\"start\":82964},{\"end\":82972,\"start\":82971},{\"end\":83289,\"start\":83281},{\"end\":83301,\"start\":83296},{\"end\":83320,\"start\":83310},{\"end\":83822,\"start\":83814},{\"end\":83834,\"start\":83829},{\"end\":83853,\"start\":83843},{\"end\":84388,\"start\":84381},{\"end\":84400,\"start\":84395},{\"end\":84857,\"start\":84853},{\"end\":84870,\"start\":84863},{\"end\":84884,\"start\":84875},{\"end\":85415,\"start\":85408},{\"end\":85429,\"start\":85422},{\"end\":85443,\"start\":85437},{\"end\":85454,\"start\":85451},{\"end\":85462,\"start\":85460},{\"end\":85474,\"start\":85470},{\"end\":86042,\"start\":86035},{\"end\":86051,\"start\":86047},{\"end\":86066,\"start\":86057},{\"end\":86081,\"start\":86075},{\"end\":86541,\"start\":86538},{\"end\":86552,\"start\":86546},{\"end\":86567,\"start\":86562},{\"end\":87094,\"start\":87086},{\"end\":87106,\"start\":87101},{\"end\":87121,\"start\":87114},{\"end\":87137,\"start\":87129},{\"end\":87391,\"start\":87388},{\"end\":87406,\"start\":87399},{\"end\":87415,\"start\":87411},{\"end\":87426,\"start\":87422},{\"end\":87792,\"start\":87791},{\"end\":87800,\"start\":87799},{\"end\":87809,\"start\":87808},{\"end\":88344,\"start\":88340},{\"end\":88351,\"start\":88350},{\"end\":88364,\"start\":88356},{\"end\":88373,\"start\":88371},{\"end\":88594,\"start\":88593},{\"end\":88601,\"start\":88600},{\"end\":88607,\"start\":88606},{\"end\":88615,\"start\":88614},{\"end\":88622,\"start\":88621},{\"end\":88630,\"start\":88629}]", "bib_author_last_name": "[{\"end\":60418,\"start\":60411},{\"end\":60427,\"start\":60425},{\"end\":60440,\"start\":60435},{\"end\":60681,\"start\":60674},{\"end\":60694,\"start\":60689},{\"end\":60718,\"start\":60704},{\"end\":60742,\"start\":60736},{\"end\":61285,\"start\":61282},{\"end\":61293,\"start\":61289},{\"end\":61302,\"start\":61297},{\"end\":61310,\"start\":61306},{\"end\":61318,\"start\":61314},{\"end\":61327,\"start\":61322},{\"end\":61644,\"start\":61640},{\"end\":61662,\"start\":61653},{\"end\":61979,\"start\":61974},{\"end\":61986,\"start\":61983},{\"end\":61996,\"start\":61992},{\"end\":62395,\"start\":62389},{\"end\":62407,\"start\":62399},{\"end\":62418,\"start\":62413},{\"end\":62706,\"start\":62703},{\"end\":62720,\"start\":62714},{\"end\":63285,\"start\":63279},{\"end\":63301,\"start\":63296},{\"end\":63313,\"start\":63310},{\"end\":63333,\"start\":63324},{\"end\":63977,\"start\":63970},{\"end\":63995,\"start\":63987},{\"end\":64009,\"start\":64004},{\"end\":64026,\"start\":64018},{\"end\":64474,\"start\":64472},{\"end\":64487,\"start\":64483},{\"end\":64498,\"start\":64493},{\"end\":64513,\"start\":64508},{\"end\":64528,\"start\":64521},{\"end\":64542,\"start\":64539},{\"end\":64559,\"start\":64552},{\"end\":65033,\"start\":65030},{\"end\":65048,\"start\":65043},{\"end\":65068,\"start\":65055},{\"end\":65488,\"start\":65482},{\"end\":65504,\"start\":65498},{\"end\":65760,\"start\":65754},{\"end\":65776,\"start\":65770},{\"end\":66203,\"start\":66198},{\"end\":66218,\"start\":66214},{\"end\":66228,\"start\":66226},{\"end\":66242,\"start\":66236},{\"end\":66634,\"start\":66631},{\"end\":66643,\"start\":66638},{\"end\":66649,\"start\":66647},{\"end\":66655,\"start\":66653},{\"end\":66662,\"start\":66659},{\"end\":66877,\"start\":66874},{\"end\":66886,\"start\":66881},{\"end\":66892,\"start\":66890},{\"end\":66898,\"start\":66896},{\"end\":66905,\"start\":66902},{\"end\":67252,\"start\":67247},{\"end\":67266,\"start\":67260},{\"end\":67283,\"start\":67275},{\"end\":67609,\"start\":67602},{\"end\":67625,\"start\":67620},{\"end\":67638,\"start\":67633},{\"end\":67916,\"start\":67905},{\"end\":67928,\"start\":67923},{\"end\":67943,\"start\":67937},{\"end\":67959,\"start\":67953},{\"end\":67973,\"start\":67968},{\"end\":68601,\"start\":68594},{\"end\":68616,\"start\":68609},{\"end\":68833,\"start\":68830},{\"end\":68840,\"start\":68837},{\"end\":68847,\"start\":68844},{\"end\":68857,\"start\":68851},{\"end\":69163,\"start\":69150},{\"end\":69174,\"start\":69170},{\"end\":69184,\"start\":69182},{\"end\":69718,\"start\":69715},{\"end\":69730,\"start\":69724},{\"end\":69742,\"start\":69736},{\"end\":69754,\"start\":69748},{\"end\":70181,\"start\":70178},{\"end\":70198,\"start\":70192},{\"end\":70208,\"start\":70204},{\"end\":70223,\"start\":70217},{\"end\":70231,\"start\":70225},{\"end\":70857,\"start\":70854},{\"end\":70870,\"start\":70866},{\"end\":70884,\"start\":70881},{\"end\":70899,\"start\":70896},{\"end\":70911,\"start\":70908},{\"end\":70923,\"start\":70918},{\"end\":70936,\"start\":70932},{\"end\":71264,\"start\":71261},{\"end\":71275,\"start\":71272},{\"end\":71289,\"start\":71285},{\"end\":71301,\"start\":71296},{\"end\":71321,\"start\":71308},{\"end\":71644,\"start\":71630},{\"end\":71660,\"start\":71653},{\"end\":71678,\"start\":71671},{\"end\":71693,\"start\":71688},{\"end\":71992,\"start\":71986},{\"end\":72010,\"start\":72004},{\"end\":72296,\"start\":72288},{\"end\":72308,\"start\":72303},{\"end\":72319,\"start\":72314},{\"end\":72336,\"start\":72330},{\"end\":72351,\"start\":72346},{\"end\":72359,\"start\":72353},{\"end\":72873,\"start\":72866},{\"end\":72889,\"start\":72880},{\"end\":72899,\"start\":72895},{\"end\":72915,\"start\":72908},{\"end\":72926,\"start\":72922},{\"end\":73454,\"start\":73449},{\"end\":73469,\"start\":73464},{\"end\":73492,\"start\":73481},{\"end\":73509,\"start\":73503},{\"end\":73523,\"start\":73517},{\"end\":73881,\"start\":73875},{\"end\":73892,\"start\":73887},{\"end\":73906,\"start\":73902},{\"end\":73922,\"start\":73915},{\"end\":74397,\"start\":74393},{\"end\":74411,\"start\":74403},{\"end\":74422,\"start\":74415},{\"end\":74852,\"start\":74846},{\"end\":74864,\"start\":74858},{\"end\":74877,\"start\":74868},{\"end\":74889,\"start\":74883},{\"end\":74901,\"start\":74895},{\"end\":75521,\"start\":75515},{\"end\":75533,\"start\":75527},{\"end\":75546,\"start\":75537},{\"end\":75558,\"start\":75552},{\"end\":75570,\"start\":75564},{\"end\":75933,\"start\":75925},{\"end\":75948,\"start\":75945},{\"end\":76164,\"start\":76154},{\"end\":76180,\"start\":76174},{\"end\":76203,\"start\":76196},{\"end\":76527,\"start\":76521},{\"end\":76541,\"start\":76534},{\"end\":76559,\"start\":76548},{\"end\":76572,\"start\":76569},{\"end\":77030,\"start\":77024},{\"end\":77053,\"start\":77047},{\"end\":77067,\"start\":77063},{\"end\":77086,\"start\":77077},{\"end\":77099,\"start\":77093},{\"end\":77115,\"start\":77108},{\"end\":77580,\"start\":77573},{\"end\":77596,\"start\":77588},{\"end\":77825,\"start\":77821},{\"end\":77842,\"start\":77832},{\"end\":77861,\"start\":77851},{\"end\":77877,\"start\":77871},{\"end\":77894,\"start\":77888},{\"end\":77909,\"start\":77904},{\"end\":77924,\"start\":77917},{\"end\":78314,\"start\":78309},{\"end\":78325,\"start\":78320},{\"end\":78338,\"start\":78332},{\"end\":78936,\"start\":78929},{\"end\":78947,\"start\":78945},{\"end\":78959,\"start\":78955},{\"end\":78972,\"start\":78969},{\"end\":78985,\"start\":78982},{\"end\":79001,\"start\":78994},{\"end\":79450,\"start\":79445},{\"end\":79464,\"start\":79460},{\"end\":79479,\"start\":79474},{\"end\":79499,\"start\":79491},{\"end\":79506,\"start\":79501},{\"end\":80038,\"start\":80033},{\"end\":80049,\"start\":80042},{\"end\":80060,\"start\":80053},{\"end\":80072,\"start\":80064},{\"end\":80080,\"start\":80076},{\"end\":80089,\"start\":80084},{\"end\":80445,\"start\":80437},{\"end\":80457,\"start\":80449},{\"end\":80772,\"start\":80764},{\"end\":80789,\"start\":80782},{\"end\":80797,\"start\":80795},{\"end\":80810,\"start\":80804},{\"end\":81492,\"start\":81484},{\"end\":81509,\"start\":81502},{\"end\":81517,\"start\":81515},{\"end\":81530,\"start\":81524},{\"end\":82044,\"start\":82028},{\"end\":82073,\"start\":82054},{\"end\":82089,\"start\":82083},{\"end\":82104,\"start\":82099},{\"end\":82115,\"start\":82106},{\"end\":82481,\"start\":82475},{\"end\":82495,\"start\":82492},{\"end\":82510,\"start\":82503},{\"end\":82937,\"start\":82934},{\"end\":82944,\"start\":82941},{\"end\":82956,\"start\":82950},{\"end\":82962,\"start\":82960},{\"end\":82969,\"start\":82966},{\"end\":82977,\"start\":82973},{\"end\":83294,\"start\":83290},{\"end\":83308,\"start\":83302},{\"end\":83325,\"start\":83321},{\"end\":83827,\"start\":83823},{\"end\":83841,\"start\":83835},{\"end\":83858,\"start\":83854},{\"end\":84393,\"start\":84389},{\"end\":84403,\"start\":84401},{\"end\":84861,\"start\":84858},{\"end\":84873,\"start\":84871},{\"end\":84891,\"start\":84885},{\"end\":85420,\"start\":85416},{\"end\":85435,\"start\":85430},{\"end\":85449,\"start\":85444},{\"end\":85458,\"start\":85455},{\"end\":85468,\"start\":85463},{\"end\":85478,\"start\":85475},{\"end\":86045,\"start\":86043},{\"end\":86055,\"start\":86052},{\"end\":86073,\"start\":86067},{\"end\":86087,\"start\":86082},{\"end\":86544,\"start\":86542},{\"end\":86560,\"start\":86553},{\"end\":86571,\"start\":86568},{\"end\":87099,\"start\":87095},{\"end\":87112,\"start\":87107},{\"end\":87127,\"start\":87122},{\"end\":87143,\"start\":87138},{\"end\":87397,\"start\":87392},{\"end\":87409,\"start\":87407},{\"end\":87420,\"start\":87416},{\"end\":87429,\"start\":87427},{\"end\":87797,\"start\":87793},{\"end\":87806,\"start\":87801},{\"end\":87812,\"start\":87810},{\"end\":88348,\"start\":88345},{\"end\":88354,\"start\":88352},{\"end\":88369,\"start\":88365},{\"end\":88378,\"start\":88374},{\"end\":88598,\"start\":88595},{\"end\":88604,\"start\":88602},{\"end\":88612,\"start\":88608},{\"end\":88619,\"start\":88616},{\"end\":88627,\"start\":88623},{\"end\":88633,\"start\":88631}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:arXiv:1903.10676\",\"id\":\"b0\",\"matched_paper_id\":202558505},\"end\":60603,\"start\":60352},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":52986197},\"end\":61204,\"start\":60605},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":222126736},\"end\":61632,\"start\":61206},{\"attributes\":{\"doi\":\"arXiv:1904.03061\",\"id\":\"b3\"},\"end\":61870,\"start\":61634},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":228098443},\"end\":62326,\"start\":61872},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":69770239},\"end\":62619,\"start\":62328},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":162168864},\"end\":63189,\"start\":62621},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":52967399},\"end\":63906,\"start\":63191},{\"attributes\":{\"doi\":\"10.1145/2642937.2642982\",\"id\":\"b8\",\"matched_paper_id\":218737160},\"end\":64467,\"start\":63908},{\"attributes\":{\"doi\":\"arXiv:2007.15779\",\"id\":\"b9\"},\"end\":64948,\"start\":64469},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":4708148},\"end\":65440,\"start\":64950},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1746574},\"end\":65712,\"start\":65442},{\"attributes\":{\"doi\":\"MSR '13\",\"id\":\"b12\",\"matched_paper_id\":1746574},\"end\":66137,\"start\":65714},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":212675399},\"end\":66595,\"start\":66139},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":196201492},\"end\":66838,\"start\":66597},{\"attributes\":{\"doi\":\"10.1109/TSE.2019.2920771\",\"id\":\"b15\",\"matched_paper_id\":196201492},\"end\":67152,\"start\":66840},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":9237290},\"end\":67595,\"start\":67154},{\"attributes\":{\"doi\":\"arXiv:1702.08734\",\"id\":\"b17\"},\"end\":67848,\"start\":67597},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":222080891},\"end\":68480,\"start\":67850},{\"attributes\":{\"id\":\"b19\"},\"end\":68762,\"start\":68482},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":10429378},\"end\":69086,\"start\":68764},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":14698227},\"end\":69613,\"start\":69088},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":206511804},\"end\":70090,\"start\":69615},{\"attributes\":{\"doi\":\"10.1109/ICPC.2017.24\",\"id\":\"b23\",\"matched_paper_id\":9203280},\"end\":70752,\"start\":70092},{\"attributes\":{\"doi\":\"10.1093/bioinformatics/btz682\",\"id\":\"b24\",\"matched_paper_id\":59291975},\"end\":71251,\"start\":70754},{\"attributes\":{\"doi\":\"arXiv:2102.04411\",\"id\":\"b25\"},\"end\":71622,\"start\":71253},{\"attributes\":{\"doi\":\"arXiv:1911.07605\",\"id\":\"b26\"},\"end\":71940,\"start\":71624},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":7227935},\"end\":72232,\"start\":71942},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":14113022},\"end\":72781,\"start\":72234},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":16447573},\"end\":73346,\"start\":72783},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":220506957},\"end\":73787,\"start\":73348},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":224803856},\"end\":74311,\"start\":73789},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":4614056},\"end\":74752,\"start\":74313},{\"attributes\":{\"doi\":\"10.1109/ASE.2011.6100062\",\"id\":\"b33\",\"matched_paper_id\":14795266},\"end\":75421,\"start\":74754},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":14795266},\"end\":75915,\"start\":75423},{\"attributes\":{\"doi\":\"arXiv:1901.04085\",\"id\":\"b35\"},\"end\":76097,\"start\":75917},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":1957433},\"end\":76439,\"start\":76099},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":52098907},\"end\":76966,\"start\":76441},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":219627145},\"end\":77566,\"start\":76968},{\"attributes\":{\"doi\":\"arXiv:1908.10084\",\"id\":\"b39\"},\"end\":77810,\"start\":77568},{\"attributes\":{\"doi\":\"arXiv:2102.03300\",\"id\":\"b40\"},\"end\":78231,\"start\":77812},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":17345191},\"end\":78871,\"start\":78233},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":47019554},\"end\":79374,\"start\":78873},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":2134475},\"end\":79984,\"start\":79376},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":3264034},\"end\":80399,\"start\":79986},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":22320655},\"end\":80703,\"start\":80401},{\"attributes\":{\"doi\":\"10.18653/v1/2020.acl-main.443\",\"id\":\"b46\",\"matched_paper_id\":218487168},\"end\":81423,\"start\":80705},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":218487168},\"end\":81901,\"start\":81425},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":49319708},\"end\":82424,\"start\":81903},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":155092004},\"end\":82860,\"start\":82426},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":53410564},\"end\":83210,\"start\":82862},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":14469144},\"end\":83743,\"start\":83212},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":14469144},\"end\":84278,\"start\":83745},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":2087154},\"end\":84807,\"start\":84280},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":13048843},\"end\":85314,\"start\":84809},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":12319675},\"end\":85963,\"start\":85316},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":22766834},\"end\":86464,\"start\":85965},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":2361519},\"end\":87030,\"start\":86466},{\"attributes\":{\"id\":\"b58\"},\"end\":87290,\"start\":87032},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":86517127},\"end\":87676,\"start\":87292},{\"attributes\":{\"doi\":\"10.1109/ICSE.2012.6227210\",\"id\":\"b60\",\"matched_paper_id\":8037297},\"end\":88265,\"start\":87678},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":220480942},\"end\":88520,\"start\":88267},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":53373051},\"end\":88864,\"start\":88522}]", "bib_title": "[{\"end\":60406,\"start\":60352},{\"end\":60664,\"start\":60605},{\"end\":61278,\"start\":61206},{\"end\":61970,\"start\":61872},{\"end\":62383,\"start\":62328},{\"end\":62694,\"start\":62621},{\"end\":63271,\"start\":63191},{\"end\":63958,\"start\":63908},{\"end\":65024,\"start\":64950},{\"end\":65476,\"start\":65442},{\"end\":65748,\"start\":65714},{\"end\":66190,\"start\":66139},{\"end\":66627,\"start\":66597},{\"end\":66870,\"start\":66840},{\"end\":67238,\"start\":67154},{\"end\":67888,\"start\":67850},{\"end\":68826,\"start\":68764},{\"end\":69140,\"start\":69088},{\"end\":69709,\"start\":69615},{\"end\":70168,\"start\":70092},{\"end\":70844,\"start\":70754},{\"end\":71978,\"start\":71942},{\"end\":72280,\"start\":72234},{\"end\":72858,\"start\":72783},{\"end\":73441,\"start\":73348},{\"end\":73858,\"start\":73789},{\"end\":74387,\"start\":74313},{\"end\":74840,\"start\":74754},{\"end\":75509,\"start\":75423},{\"end\":76144,\"start\":76099},{\"end\":76511,\"start\":76441},{\"end\":77014,\"start\":76968},{\"end\":78295,\"start\":78233},{\"end\":78919,\"start\":78873},{\"end\":79441,\"start\":79376},{\"end\":80029,\"start\":79986},{\"end\":80433,\"start\":80401},{\"end\":80755,\"start\":80705},{\"end\":81475,\"start\":81425},{\"end\":82017,\"start\":81903},{\"end\":82469,\"start\":82426},{\"end\":82930,\"start\":82862},{\"end\":83279,\"start\":83212},{\"end\":83812,\"start\":83745},{\"end\":84379,\"start\":84280},{\"end\":84851,\"start\":84809},{\"end\":85406,\"start\":85316},{\"end\":86033,\"start\":85965},{\"end\":86536,\"start\":86466},{\"end\":87386,\"start\":87292},{\"end\":87789,\"start\":87678},{\"end\":88338,\"start\":88267},{\"end\":88591,\"start\":88522}]", "bib_author": "[{\"end\":60420,\"start\":60408},{\"end\":60429,\"start\":60420},{\"end\":60442,\"start\":60429},{\"end\":60683,\"start\":60666},{\"end\":60696,\"start\":60683},{\"end\":60720,\"start\":60696},{\"end\":60744,\"start\":60720},{\"end\":61287,\"start\":61280},{\"end\":61295,\"start\":61287},{\"end\":61304,\"start\":61295},{\"end\":61312,\"start\":61304},{\"end\":61320,\"start\":61312},{\"end\":61329,\"start\":61320},{\"end\":61646,\"start\":61634},{\"end\":61664,\"start\":61646},{\"end\":61981,\"start\":61972},{\"end\":61988,\"start\":61981},{\"end\":61998,\"start\":61988},{\"end\":62397,\"start\":62385},{\"end\":62409,\"start\":62397},{\"end\":62420,\"start\":62409},{\"end\":62708,\"start\":62696},{\"end\":62722,\"start\":62708},{\"end\":63287,\"start\":63273},{\"end\":63303,\"start\":63287},{\"end\":63315,\"start\":63303},{\"end\":63335,\"start\":63315},{\"end\":63979,\"start\":63960},{\"end\":63997,\"start\":63979},{\"end\":64011,\"start\":63997},{\"end\":64028,\"start\":64011},{\"end\":64476,\"start\":64469},{\"end\":64489,\"start\":64476},{\"end\":64500,\"start\":64489},{\"end\":64515,\"start\":64500},{\"end\":64530,\"start\":64515},{\"end\":64544,\"start\":64530},{\"end\":64561,\"start\":64544},{\"end\":65035,\"start\":65026},{\"end\":65050,\"start\":65035},{\"end\":65070,\"start\":65050},{\"end\":65490,\"start\":65478},{\"end\":65506,\"start\":65490},{\"end\":65762,\"start\":65750},{\"end\":65778,\"start\":65762},{\"end\":66205,\"start\":66192},{\"end\":66220,\"start\":66205},{\"end\":66230,\"start\":66220},{\"end\":66244,\"start\":66230},{\"end\":66636,\"start\":66629},{\"end\":66645,\"start\":66636},{\"end\":66651,\"start\":66645},{\"end\":66657,\"start\":66651},{\"end\":66664,\"start\":66657},{\"end\":66879,\"start\":66872},{\"end\":66888,\"start\":66879},{\"end\":66894,\"start\":66888},{\"end\":66900,\"start\":66894},{\"end\":66907,\"start\":66900},{\"end\":67254,\"start\":67240},{\"end\":67268,\"start\":67254},{\"end\":67285,\"start\":67268},{\"end\":67611,\"start\":67597},{\"end\":67627,\"start\":67611},{\"end\":67640,\"start\":67627},{\"end\":67918,\"start\":67890},{\"end\":67930,\"start\":67918},{\"end\":67945,\"start\":67930},{\"end\":67961,\"start\":67945},{\"end\":67975,\"start\":67961},{\"end\":68603,\"start\":68589},{\"end\":68618,\"start\":68603},{\"end\":68835,\"start\":68828},{\"end\":68842,\"start\":68835},{\"end\":68849,\"start\":68842},{\"end\":68859,\"start\":68849},{\"end\":69165,\"start\":69142},{\"end\":69176,\"start\":69165},{\"end\":69186,\"start\":69176},{\"end\":69720,\"start\":69711},{\"end\":69732,\"start\":69720},{\"end\":69744,\"start\":69732},{\"end\":69756,\"start\":69744},{\"end\":70183,\"start\":70170},{\"end\":70200,\"start\":70183},{\"end\":70210,\"start\":70200},{\"end\":70225,\"start\":70210},{\"end\":70233,\"start\":70225},{\"end\":70859,\"start\":70846},{\"end\":70872,\"start\":70859},{\"end\":70886,\"start\":70872},{\"end\":70901,\"start\":70886},{\"end\":70913,\"start\":70901},{\"end\":70925,\"start\":70913},{\"end\":70938,\"start\":70925},{\"end\":71266,\"start\":71253},{\"end\":71277,\"start\":71266},{\"end\":71291,\"start\":71277},{\"end\":71303,\"start\":71291},{\"end\":71323,\"start\":71303},{\"end\":71646,\"start\":71624},{\"end\":71662,\"start\":71646},{\"end\":71680,\"start\":71662},{\"end\":71695,\"start\":71680},{\"end\":71994,\"start\":71980},{\"end\":72012,\"start\":71994},{\"end\":72298,\"start\":72282},{\"end\":72310,\"start\":72298},{\"end\":72321,\"start\":72310},{\"end\":72338,\"start\":72321},{\"end\":72353,\"start\":72338},{\"end\":72361,\"start\":72353},{\"end\":72875,\"start\":72860},{\"end\":72891,\"start\":72875},{\"end\":72901,\"start\":72891},{\"end\":72917,\"start\":72901},{\"end\":72928,\"start\":72917},{\"end\":73456,\"start\":73443},{\"end\":73471,\"start\":73456},{\"end\":73494,\"start\":73471},{\"end\":73511,\"start\":73494},{\"end\":73525,\"start\":73511},{\"end\":73883,\"start\":73860},{\"end\":73894,\"start\":73883},{\"end\":73908,\"start\":73894},{\"end\":73924,\"start\":73908},{\"end\":74399,\"start\":74389},{\"end\":74413,\"start\":74399},{\"end\":74424,\"start\":74413},{\"end\":74854,\"start\":74842},{\"end\":74866,\"start\":74854},{\"end\":74879,\"start\":74866},{\"end\":74891,\"start\":74879},{\"end\":74903,\"start\":74891},{\"end\":75523,\"start\":75511},{\"end\":75535,\"start\":75523},{\"end\":75548,\"start\":75535},{\"end\":75560,\"start\":75548},{\"end\":75572,\"start\":75560},{\"end\":75935,\"start\":75917},{\"end\":75950,\"start\":75935},{\"end\":76166,\"start\":76146},{\"end\":76182,\"start\":76166},{\"end\":76205,\"start\":76182},{\"end\":76529,\"start\":76513},{\"end\":76543,\"start\":76529},{\"end\":76561,\"start\":76543},{\"end\":76574,\"start\":76561},{\"end\":77032,\"start\":77016},{\"end\":77055,\"start\":77032},{\"end\":77069,\"start\":77055},{\"end\":77088,\"start\":77069},{\"end\":77101,\"start\":77088},{\"end\":77117,\"start\":77101},{\"end\":77582,\"start\":77568},{\"end\":77598,\"start\":77582},{\"end\":77827,\"start\":77812},{\"end\":77844,\"start\":77827},{\"end\":77863,\"start\":77844},{\"end\":77879,\"start\":77863},{\"end\":77896,\"start\":77879},{\"end\":77911,\"start\":77896},{\"end\":77926,\"start\":77911},{\"end\":78316,\"start\":78297},{\"end\":78327,\"start\":78316},{\"end\":78340,\"start\":78327},{\"end\":78938,\"start\":78921},{\"end\":78949,\"start\":78938},{\"end\":78961,\"start\":78949},{\"end\":78974,\"start\":78961},{\"end\":78987,\"start\":78974},{\"end\":79003,\"start\":78987},{\"end\":79452,\"start\":79443},{\"end\":79466,\"start\":79452},{\"end\":79481,\"start\":79466},{\"end\":79501,\"start\":79481},{\"end\":79508,\"start\":79501},{\"end\":80040,\"start\":80031},{\"end\":80051,\"start\":80040},{\"end\":80062,\"start\":80051},{\"end\":80074,\"start\":80062},{\"end\":80082,\"start\":80074},{\"end\":80091,\"start\":80082},{\"end\":80447,\"start\":80435},{\"end\":80459,\"start\":80447},{\"end\":80774,\"start\":80757},{\"end\":80791,\"start\":80774},{\"end\":80799,\"start\":80791},{\"end\":80812,\"start\":80799},{\"end\":81494,\"start\":81477},{\"end\":81511,\"start\":81494},{\"end\":81519,\"start\":81511},{\"end\":81532,\"start\":81519},{\"end\":82046,\"start\":82019},{\"end\":82075,\"start\":82046},{\"end\":82091,\"start\":82075},{\"end\":82106,\"start\":82091},{\"end\":82117,\"start\":82106},{\"end\":82483,\"start\":82471},{\"end\":82497,\"start\":82483},{\"end\":82512,\"start\":82497},{\"end\":82939,\"start\":82932},{\"end\":82946,\"start\":82939},{\"end\":82958,\"start\":82946},{\"end\":82964,\"start\":82958},{\"end\":82971,\"start\":82964},{\"end\":82979,\"start\":82971},{\"end\":83296,\"start\":83281},{\"end\":83310,\"start\":83296},{\"end\":83327,\"start\":83310},{\"end\":83829,\"start\":83814},{\"end\":83843,\"start\":83829},{\"end\":83860,\"start\":83843},{\"end\":84395,\"start\":84381},{\"end\":84405,\"start\":84395},{\"end\":84863,\"start\":84853},{\"end\":84875,\"start\":84863},{\"end\":84893,\"start\":84875},{\"end\":85422,\"start\":85408},{\"end\":85437,\"start\":85422},{\"end\":85451,\"start\":85437},{\"end\":85460,\"start\":85451},{\"end\":85470,\"start\":85460},{\"end\":85480,\"start\":85470},{\"end\":86047,\"start\":86035},{\"end\":86057,\"start\":86047},{\"end\":86075,\"start\":86057},{\"end\":86089,\"start\":86075},{\"end\":86546,\"start\":86538},{\"end\":86562,\"start\":86546},{\"end\":86573,\"start\":86562},{\"end\":87101,\"start\":87086},{\"end\":87114,\"start\":87101},{\"end\":87129,\"start\":87114},{\"end\":87145,\"start\":87129},{\"end\":87399,\"start\":87388},{\"end\":87411,\"start\":87399},{\"end\":87422,\"start\":87411},{\"end\":87431,\"start\":87422},{\"end\":87799,\"start\":87791},{\"end\":87808,\"start\":87799},{\"end\":87814,\"start\":87808},{\"end\":88350,\"start\":88340},{\"end\":88356,\"start\":88350},{\"end\":88371,\"start\":88356},{\"end\":88380,\"start\":88371},{\"end\":88600,\"start\":88593},{\"end\":88606,\"start\":88600},{\"end\":88614,\"start\":88606},{\"end\":88621,\"start\":88614},{\"end\":88629,\"start\":88621},{\"end\":88635,\"start\":88629}]", "bib_venue": "[{\"end\":60469,\"start\":60464},{\"end\":60832,\"start\":60744},{\"end\":61407,\"start\":61329},{\"end\":61727,\"start\":61680},{\"end\":62089,\"start\":61998},{\"end\":62461,\"start\":62420},{\"end\":62844,\"start\":62722},{\"end\":63477,\"start\":63335},{\"end\":64127,\"start\":64051},{\"end\":64700,\"start\":64577},{\"end\":65153,\"start\":65070},{\"end\":65567,\"start\":65506},{\"end\":65859,\"start\":65785},{\"end\":66325,\"start\":66244},{\"end\":66705,\"start\":66664},{\"end\":66972,\"start\":66931},{\"end\":67363,\"start\":67285},{\"end\":67697,\"start\":67656},{\"end\":68079,\"start\":67975},{\"end\":68587,\"start\":68482},{\"end\":68900,\"start\":68859},{\"end\":69277,\"start\":69186},{\"end\":69833,\"start\":69756},{\"end\":70326,\"start\":70253},{\"end\":70981,\"start\":70967},{\"end\":71424,\"start\":71339},{\"end\":71775,\"start\":71711},{\"end\":72071,\"start\":72012},{\"end\":72433,\"start\":72361},{\"end\":72977,\"start\":72928},{\"end\":73555,\"start\":73525},{\"end\":74007,\"start\":73924},{\"end\":74515,\"start\":74424},{\"end\":75018,\"start\":74927},{\"end\":75644,\"start\":75572},{\"end\":75994,\"start\":75966},{\"end\":76253,\"start\":76205},{\"end\":76660,\"start\":76574},{\"end\":77209,\"start\":77117},{\"end\":77676,\"start\":77614},{\"end\":78008,\"start\":77942},{\"end\":78421,\"start\":78340},{\"end\":79079,\"start\":79003},{\"end\":79599,\"start\":79508},{\"end\":80177,\"start\":80091},{\"end\":80536,\"start\":80459},{\"end\":80979,\"start\":80841},{\"end\":81619,\"start\":81532},{\"end\":82152,\"start\":82117},{\"end\":82599,\"start\":82512},{\"end\":83020,\"start\":82979},{\"end\":83407,\"start\":83327},{\"end\":83940,\"start\":83860},{\"end\":84478,\"start\":84405},{\"end\":84984,\"start\":84893},{\"end\":85583,\"start\":85480},{\"end\":86172,\"start\":86089},{\"end\":86671,\"start\":86573},{\"end\":87084,\"start\":87032},{\"end\":87466,\"start\":87431},{\"end\":87918,\"start\":87839},{\"end\":88385,\"start\":88380},{\"end\":88676,\"start\":88635},{\"end\":60924,\"start\":60834},{\"end\":62953,\"start\":62846},{\"end\":63606,\"start\":63479},{\"end\":64145,\"start\":64129},{\"end\":65223,\"start\":65155},{\"end\":65942,\"start\":65861},{\"end\":66393,\"start\":66327},{\"end\":68188,\"start\":68081},{\"end\":69371,\"start\":69279},{\"end\":70409,\"start\":70328},{\"end\":72518,\"start\":72435},{\"end\":74077,\"start\":74009},{\"end\":75096,\"start\":75020},{\"end\":76733,\"start\":76662},{\"end\":77288,\"start\":77211},{\"end\":78522,\"start\":78423},{\"end\":79142,\"start\":79081},{\"end\":79700,\"start\":79601},{\"end\":81104,\"start\":80981},{\"end\":81693,\"start\":81621},{\"end\":82673,\"start\":82601},{\"end\":83492,\"start\":83409},{\"end\":84025,\"start\":83942},{\"end\":84554,\"start\":84480},{\"end\":85082,\"start\":84986},{\"end\":85673,\"start\":85585},{\"end\":86242,\"start\":86174},{\"end\":86772,\"start\":86673},{\"end\":87984,\"start\":87920}]"}}}, "year": 2023, "month": 12, "day": 17}