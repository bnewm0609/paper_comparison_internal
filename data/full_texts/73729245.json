{"id": 73729245, "updated": "2023-10-04 03:43:44.648", "metadata": {"title": "Fall of Empires: Breaking Byzantine-tolerant SGD by Inner Product Manipulation", "authors": "[{\"first\":\"Cong\",\"last\":\"Xie\",\"middle\":[]},{\"first\":\"Sanmi\",\"last\":\"Koyejo\",\"middle\":[]},{\"first\":\"Indranil\",\"last\":\"Gupta\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2019, "month": 3, "day": 10}, "abstract": "Recently, new defense techniques have been developed to tolerate Byzantine failures for distributed machine learning. The Byzantine model captures workers that behave arbitrarily, including malicious and compromised workers. In this paper, we break two prevailing Byzantine-tolerant techniques. Specifically we show robust aggregation methods for synchronous SGD -- coordinate-wise median and Krum -- can be broken using new attack strategies based on inner product manipulation. We prove our results theoretically, as well as show empirical validation.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1903.03936", "mag": "2966757932", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/uai/XieKG19", "doi": null}}, "content": {"source": {"pdf_hash": "c09ba5253ff59364e1e31c64f5d92af13e3cd8a4", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1903.03936v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "f18a7950cb0ee25b39f1f3e88e17403c034cfc25", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c09ba5253ff59364e1e31c64f5d92af13e3cd8a4.txt", "contents": "\nFall of Empires: Breaking Byzantine-tolerant SGD by Inner Product Manipulation\n\n\nCong Xie \nComputer Science Dept. UIUC\nComputer Science Dept. UIUC\nComputer Science Dept. UIUC\n\n\nSanmi Koyejo \nComputer Science Dept. UIUC\nComputer Science Dept. UIUC\nComputer Science Dept. UIUC\n\n\nIndranil Gupta \nComputer Science Dept. UIUC\nComputer Science Dept. UIUC\nComputer Science Dept. UIUC\n\n\nFall of Empires: Breaking Byzantine-tolerant SGD by Inner Product Manipulation\n\nRecently, new defense techniques have been developed to tolerate Byzantine failures for distributed machine learning. The Byzantine model captures workers that behave arbitrarily, including malicious and compromised workers. In this paper, we break two prevailing Byzantine-tolerant techniques. Specifically we show robust aggregation methods for synchronous SGD -coordinate-wise median and Krum -can be broken using new attack strategies based on inner product manipulation. We prove our results theoretically, as well as show empirical validation.\n\nINTRODUCTION\n\nThe security of distributed machine learning has drawn increasing attention in recent years. Among the threat models, Byzantine failures (Lamport et al., 1982) are perhaps the most well-studied. In the Byzantine model, workers can behave arbitrarily and maliciously. In addition, Byzantine workers are omniscient and can conspire. Most of the existing Byzantine-tolerant machine-learning algorithms (Blanchard et al., 2017;Chen et al., 2017;Yin et al., 2018;Feng et al., 2014;Su & Vaidya, 2016a;Alistarh et al., 2018) focus on the protection of distributed Stochastic Gradient Descent (SGD).\n\nIn this paper, we consider Byzantine-tolerant SGD in a server-worker architecture (also known as the parameter server architecture (Li et al., 2014a;), depicted in Figure 3. The system is composed of server nodes and worker nodes. In each epoch, the workers pull the latest model from the servers, estimate the gradients using the locally sampled training data, and then push the gradient estimators to the servers. The servers aggregate the gradient estimators, and update the model by using the We execute distributed synchronous SGD on CIFAR-10 image classification, with 25 workers. Beginning from the 100th epoch, we attack the a system by replacing some workers with Byzantine workers. During the attack, 12 workers are Byzantine in the case of coordinate-wise median, and 11 workers are Byzantine in the case of Krum. The Byzantine workers push \u2212 g to the server, where g is the true gradient. aggregated gradients.\n\nWe consider Byzantine failures at a subset of the worker nodes. Byzantine workers send arbitrary values instead of the gradient estimators to the server. Such Byzantine gradients are potentially adversarial, and this can result in convergence to sub-optimum models, or even lead to divergence. To make things worse, the Byzantine workers can spy on the information at any server or at any honest worker (omniscience). Byzantine gradients can thus be tailored to have similar variance and magnitude as the correct gradients, which makes them hard to distinguish.\n\nAdditionally, in different iterations, different subsets of workers can behave in a Byzantine manner, evading detection. Existing literature assumes that less than half of the workers are Byzantine in any iteration.\n\nCompared to traditional Byzantine tolerance in distributed systems (Lynch, 1996;Avizienis et al., 2004;Tanenbaum & Van Steen, 2007;Fischer et al., 1982), Byzantine tolerance in distributed machine learning has unique properties and challenges. Traditional Byzantine tolerance attempts to reach consensus on correct values. However, machine learning algorithms do not need to reach consensus. Further, even non-Byzantine-tolerant machine learning algorithms can naturally tolerate some noise in the input and during execution (Xing et al., 2016). Thus for distributed SGD, existing techniques for Byzantine-tolerant execution guarantee an upper-bound on the distance between the aggregated approximate gradient (under Byzantine workers) and the true gradient (Blanchard et al., 2017;Yin et al., 2018).\n\nA deeper introspection reveals, however, that what really matters for gradient descent algorithms is the direction of the descent. As shown in Figure 2, to let the gradient descent algorithm make progress, we need to guarantee that the direction of the aggregated vector agrees with the true gradient, i.e., the inner product between the aggregated vector and the true gradient must be non-negative. This can be violated by an attack that makes the inner product negative. We call this class of new attacks \"inner product manipulation attacks\".\n\nFigure 2: Descent Direction. The blue arrows represent the directions which agree with the steepest descent direction (negative gradient). The red arrows represent directions which agree with the steepest ascent direction (gradient).\n\nWe observe that the bounded distance between the aggregated value and the true gradient guaranteed by existing techniques is not enough to defend distributed synchronous SGD against inner product manipulation attacks. For example, for the coordinate-wise median, if we put all the Byzantine values on the opposite side of the true gradient, the inner product between the aggregated vector and the true gradient can be manipulated to be negative.\n\nIn this paper, we study how inner product manipulation makes Byzantine-tolerant SGD vulnerable. We conduct case studies on coordinate-wise median (Yin et al., 2018) and Krum (Blanchard et al., 2017). Figure 1 gives a glimpse of how bad the effect of the attack can be. In a nutshell, creating gradients in the opposite direction with large magnitude crashes coordinate-wise median, while creating gradients in the opposite direction with small magnitude crashes Krum. We provide theoretical analysis as well as empirical results to validate these findings.\n\nBased on these results, we argue that there is a need to revise the definition of Byzantine tolerance in distributed SGD. We provide a new definition, and study its satisfaction on two prevailing robust distributed SGD algorithms, theoretically and empirically. In summary, our contributions are:\n\n\u2022 We break two prevailing Byzantine tolerant SGD algorithms -coordinate-wise median (Yin et al., 2018) and Krum (Blanchard et al., 2017) -using a new class of attacks called inner production manipulation attacks. We theoretically prove that under certain conditions, we can backdoor these two algorithms, even when the assumptions and theorems presented in these papers are valid.\n\n\u2022 We show how to design Byzantine gradients to compromise the robust aggregation rules. We conduct experiments to validate further.\n\n\u2022 Following our theoretical and empirical analysis, we propose a revised definition of Byzantine tolerance for distributed SGD.\n\n\nRELATED WORK\n\nRobust estimators such as the median are well studied, and can naturally be applied to Byzantine tolerance. Coordinate-wise median is one approach that generalizes the median to high-dimensional vectors. In Yin et al. (2018), statistical error rates are studied for using coordinate-wise median in distributed SGD. Blanchard et al. (2017) propose Krum, which is not based on robust statistics. For each candidate gradient, Krum computes the local sum of squared Euclidean distances to the other candidates, and outputs the one with minimal sum.\n\nIn this paper, we focus on coordinate-wise median and Krum. There are other Byzantine-tolerant SGD algorithms. For example, Bulyan (Guerraoui et al., 2018) is built based on Krum, which potentially shares the same flaws. DRACO  uses coding theory to ensure robustness, and is different from the other Byzantine-tolerant SGD algorithms.\n\nRecently, an increasing number of papers propose attack mechanisms to break the defense of machine learning in various scenarios. For example, Athalye et al. (2018) propose attack techniques using adversarial training data. Bhagoji et al. (2018); Bagdasaryan et al. (2018) break the defense of federated learning (McMahan et al., 2016). In this paper, we focus on attacking distributed synchronous SGD using adversarial gradients sent by Byzantine workers.\n\n\nPRELIMINARIES\n\nIn this paper, we focus on distributed synchronous Stochastic Gradient Descent (SGD) with Parameter Server (PS). In this section, we formally introduce distributed synchronous SGD and the threat model of Byzantine failures. \n\n\nSTOCHASTIC GRADIENT DESCENT\n\nWe consider the following optimization problem:\nmin x\u2208R d F (x), where F (x) = E z\u223cD [f (x; z)\n] is a differentiable function, z is sampled from some unknown distribution D, d is the number of dimensions. We assume that there exists at least one minimizer of F (x), which is denoted by\nx * , where \u2207F (x * ) = 0.\nThis problem is solved in a distributed manner with m workers. In each iteration, each worker will sample n independent and identically distributed (i.i.d.) data points from the distribution D, and compute the gradient of the local empirical loss\nF i (x) = 1 n n j=1 f (x; z i,j ), \u2200i \u2208 [m]\n, where z i,j is the jth sampled data on the ith worker. The servers will collect and aggregate the gradients sent by the workers, and update the model as follows:\nx t+1 = x t \u2212 \u03b3 t Aggr({\u1e7d t i : i \u2208 [m]}),\nwhere Aggr(\u00b7) is an aggregation rule (e.g., averaging), and\u1e7d t i is the gradient sent by the ith worker, \u03b3 t is the learning rate in the t th iteration. For an honest worker,\n\u1e7d t i = \u2207F i (x t ) is an unbiased estimator such that E[\u2207F i (x t )] = \u2207F (x t ).\nWhen all the workers are honest, the most common choice of the aggregation rule Aggr(\u00b7) is averaging:\nAggr({\u1e7d t i : i \u2208 [m]}) = 1 m i\u2208[m]\u1e7d t i .\nThe detailed algorithm of distributed synchronous SGD with aggregation rule Aggr(\u00b7) is shown in Algorithm 1.\n\n\nAlgorithm 1 Distributed Synchronous SGD with Robust Aggregation\nServer x 0 \u2190 rand() {Initialization} for t = 0, . . . , T do Broadcast x t to all the workers Wait until all the gradients {\u1e7d t i : i \u2208 [m]} arrive Computev t = Aggr({\u1e7d t i : i \u2208 [m]}) Update the parameter x t+1 \u2190 x t \u2212 \u03b3 tvt end for Worker i = 1, . . . , m for t = 0, . . . , T do\nReceive x t from the server Draw the samples, compute, and send the gradient v t i = \u2207F t i (x t ) to the server end for\n\n\nTHREAT MODEL\n\nIn the Byzantine failure model, the gradients sent by malicious workers can take an arbitrary value:\nv t i = * , if ith worker is Byzantine, \u2207F i (x t ), otherwise,(1)\nwhere \" * \" represents arbitrary values.\n\nFormally, we define the threat model of Byzantine failure as follows.\n\nDefinition 1. (Threat Model (Blanchard et al., 2017;Chen et al., 2017;Yin et al., 2018)). In the t th iteration,  (1). In other words, a correct gradient is \u2207F i (x t ), while a Byzantine gradient, marked as \" * \", is assigned arbitrary value. We assume that q out of m vectors are Byzantine, where 2q < m.\nlet {v t i : i \u2208 [m]} be i.i.d. random vectors in R d , where v t i = \u2207F i (x t ).\nFurthermore, the indices of faulty workers can change across different iterations. If the failures are caused by attackers, the threat model includes the case where the attackers can collude.\n\nThe notations used in this paper is summarized in Table 1. Learning rate x\n\nModel parameters v t i Gradient sent by the ith worker in the t th iteration, potentially Byzantine v t i Correct gradient produced by the ith worker in the t th iteration \u00b7 All the norms in this paper are l2-norms a, b\n\nInner product between a and b\n\n\nDEFENSE TECHNIQUES\n\nIn this section, we introduce two prevailing robust aggregation rules against Byzantine failures in distributed synchronous SGD: coordinate-wise median and Krum.\n\nFor the remainder of this paper, we ignore the iteration superscript t in\u1e7d t i and v t i for convenience.\n\n\nCOORDINATE-WISE MEDIAN\n\nDefinition 2. (Coordinate-wise Median (Yin et al., 2018)) We define the coordinate-wise median aggregation rule Median(\u00b7) as\nmed = Median({\u1e7d i : i \u2208 [m]}), where for any j \u2208 [d], the jth dimension of med is med j = median ({(\u1e7d 1 ) j , . . . , (\u1e7d m ) j }), (\u1e7d i ) j is the jth dimension of the vector\u1e7d i , median(\u00b7) is the one- dimensional median.\n\nKRUM\n\nDefinition 3. (Krum (Blanchard et al., 2017))\nKrum({\u1e7d i : i \u2208 [m]}) =\u1e7d k , k = argmin i\u2208[m] KR(\u1e7d i ), KR(\u1e7d i ) = i\u2192j \u1e7d i \u2212\u1e7d j 2 ,\nwhere i \u2192 j are the indices of the m \u2212 q \u2212 2 nearest neighbours of\u1e7d i in {\u1e7d j : j \u2208 [m], i = j} as measured by squared Euclidean distance.\n\nFor convenience, we refer to the coordinate-wise median and Krum as Median and Krum.\n\n\nATTACK TECHNIQUES\n\nIn this section, we revise the definition of Byzantine tolerance in distributed synchronous SGD. Then, we theoretically analyze the Byzantine tolerance of coordinate-wise median and Krum, and show that under certain conditions, these two robust aggregation rules are no longer Byzantine-tolerant.\n\n\nINNER PRODUCT MANIPULATION\n\nIn the previous work on Byzantine-tolerant SGD algorithms, most of the robust aggregation rules only guarantee that the robust estimator is not arbitrarily far away from the mean of the correct gradients. In other words, the distance between the robust estimator and the correct mean is upper-bounded. However, for gradient descent algorithms, to guarantee the descent of the loss, the inner product between the true gradient and the robust estimator must be non-negative:\n\u2207F (x), Aggr({\u1e7d i : i \u2208 [m]}) \u2265 0,\nso that at least the loss will not increase in expectation.\n\nIn particular, bounded distance is not enough to guarantee robustness, if the attackers manipulate the Byzantine gradients and make the inner product negative.\n\nThe intuition underlying the inner product manipulation attack is that, when gradient descent algorithm converges, the gradient \u2207F (x t ) approaches 0. Thus, even if the distance between the robust estimator and the correct mean is bounded, it is still possible to manipulate their inner product to be negative, especially when the upper bound of such distance is large.\n\nWe formally define a revised version of Byzantine tolerance for distributed synchronous SGD (DSSGD-Byzantine tolerance): Definition 4. (DSSGD-Byzantine Tolerance) Without loss of generality, suppose that in a specific iteration, the server receives (m \u2212 q) correct gradients V = {v 1 , . . . , v m\u2212q } and q Byzantine gradients U = {u 1 , . . . , u q }. We assume that the correct gradients have the same expectation E[v i ] = g, \u2200i \u2208 [m\u2212q]. An aggregation rule Aggr(\u00b7) is said to be DSSGD-Byzantine-tolerant if\ng, E [Aggr(V \u222a U)] \u2265 0.\nWith the revised definition, now we theoretically analyze the DSSGD-Byzantine tolerance of coordinate-wise median and Krum.\n\nRemark 1. Note that we do not argue that the theoretical guarantees in the previous work are wrong. Instead, our claim is that the theoretical guarantees on the bounded distances are not enough to secure distributed synchronous SGD. In particular, DSSGD-Byzantine tolerance is different from the Byzantine tolerance proposed in previous work.\n\n\nCOORDINATE-WISE MEDIAN\n\nThe following theorem shows that under certain conditions, Median is not DSSGD-Byzantine-tolerant. Theorem 1. We consider the worst case where m \u2212 2q = 1. The server receives (m \u2212 q) correct gradients V = {v 1 , . . . , v m\u2212q } and q Byzantine gradients U = {u 1 , . . . , u q }. We assume that the stochastic gradients\nhave identical expectation E[v i ] = g, \u2200i \u2208 [m \u2212 q], and non-zero coordinate-wise variance E[((v i ) j \u2212 g j ) 2 ] \u2265 \u03c3 2 , \u2200i \u2208 [m \u2212 q], j \u2208 [d], where (v i ) j is the jth coor- dinate of v i , and g j is the jth coordinate of g. When max j\u2208[d] |g j | < \u03c3 \u221a m\u2212q\u22121 , there exist Byzantine gradi- ents U = {u 1 , . . . , u q } such that g, E [Median(V \u222a U)] < 0.\nProof. (sketch) Since median is independently taken in each coordinate, it is sufficient to prove Byzantine vulnerability for one coordinate or scalars. Thus, for convenience, with a little bit abuse of notation, we suppose that the correct gradients V = {v 1 , . . . , v m\u2212q } and q Byzantine gradients U = {u 1 , . . . , u q } are all scalars. We only need to show that under certain attacks, the aggregated value Median(V \u222a U) has a different sign than i\u2208[m\u2212q] v i .\n\nWithout loss of generality, we assume that g = 1 m\u22121 i\u2208[m\u2212q] E[v i ] > 0 (the mirror case can be easily proved with a similar procedure). The Byzantine gradients are all assigned negative value: u i < 0, \u2200i \u2208 [q]. Furthermore, we make the Byzantine gradients small enough such that u i < min(V), \u2200i \u2208 [q].\n\nBy sorting the correct gradients, we can define the se-\nquence {v 1:m\u2212q , . . . , v m\u2212q:m\u2212q }, where v i:m\u2212q is the ith smallest element in {v 1 , . . . , v m\u2212q }: v 1:m\u2212q \u2264 v 2:m\u2212q \u2264 \u00b7 \u00b7 \u00b7 \u2264 v m\u2212q:m\u2212q .\nWe also define the expectation of the ith smallest element:\n\u00b5 i:m\u2212q = E[v i:m\u2212q ].\nThen, it is easy to check that Median(V \u222a U) = v 1:m\u2212q , and E [Median(V \u222a U)] = \u00b5 1:m\u2212q .\n\nUsing Theorem 1(b) from Hawkins (1971) (equiv. 9(a) from Arnold et al. (1979)), we have\n\u00b5 1:m\u2212q \u2264 g \u2212 \u03c3 \u221a m \u2212 q \u2212 1 . Thus, when g < \u03c3 \u221a m\u2212q\u22121 , E [Median(V \u222a U)] is nega- tive.\nRemark 2. When gradient descent converges, the expectation of the gradient g approaches 0. Furthermore, since the gradient produced by the correct workers are stochastic, the variance always exists. Thus, eventually, the condition max j\u2208[d] |g j | < \u03c3 \u221a m\u2212q\u22121 will be satisfied. To make things worse, the closer SGD approaches a critical point, the less likely the coordinate-wise median is DSSGD-Byzantine-tolerant.\n\nRemark 3. The proof of Theorem 1 provides the intuition of constructing adversarial gradients for the attackers. In practice, in each coordinate, the attackers only need to guarantee that all the Byzantine values are much smaller than the smallest correct value if the correct expectation is positive, or much larger than the largest correct value if the correct expectation is negative. Then, hopefully, if the variance is large enough, the smallest/largest value has the opposite sign to the correct expectation. Then, the attackers can successfully manipulate the aggregated value into the opposite direction to the correct expectation.\n\n\nToy Example\n\nWe provide an 1-dimensional toy example to illustrate how easily Median can fail. Suppose there are 3 correct gradients V = {\u22120.1, 0.1, 0.3} with the mean 0.1, and 2 Byzantine gradient U = {\u22124, \u22122} with the negative mean \u22123. According to Definition 2, it is easy to check that Median(U \u2229 V) = \u22120.1, which means that Median produces a value with the opposite sign of the mean of the correct gradients.\n\n\nKRUM\n\nThe following theorem proves that under certain conditions, Krum is not DSSGD-Byzantine-tolerant. Note that Krum requires that m \u2212 2q > 2.\n\nTheorem 2. We consider the worst case where m \u2212 2q = 3. The server receives (m \u2212 q) correct gradients V = {v 1 , . . . , v m\u2212q } and q Byzantine gradients U = {u 1 , . . . , u q }. We assume that the stochastic gradients have identical expectation E[v i ] = g, \u2200i \u2208 [m \u2212 q]. We define the mean of the correct gradients v = 1 m\u2212q i\u2208[m\u2212q] v i . We assume that the correct gradients are bounded by v i \u2212v 2 \u2264 v 2 , \u2200i \u2208 [m\u2212q]. Furthermore, we assume that v i = v j , \u2200i = j, i, j \u2208 [m \u2212 q], and \u2203\u03b2 such that v i \u2212 v j 2 \u2265 \u03b2 2 , \u2200i = j, i, j \u2208 [m \u2212 q]. We take u 1 = u 2 = \u00b7 \u00b7 \u00b7 = u q = \u2212 v, where is a small positive constant value such that 2 v 2 \u2264 \u03b2 2 . When (m \u2212 q) is large enough: m \u2212 q > 2( +2) 2 2 + 2, we have\ng, E [Krum(V \u222a U)] < 0. Proof. (sketch) For \u2200u \u2208 U, u = \u2212 v, wherev = 1 m\u22121 i\u2208[m\u2212q] v i .\nSince any u \u2208 U is identical, the nearest (m \u2212 q \u2212 4) neighbours of u must belong to U. The remaining (m \u2212 q \u2212 2) \u2212 (m \u2212 q \u2212 4) = 2 nearest neighbours must belong to the set of correct gradients V. Thus, we have\nKR(u) \u2264 2 v +v + v 2 = 2( + 2) 2 v 2 .\nFor the correct gradients \u2200v \u2208 V, there are two cases:\n\n\u2022 Case 1: There are some u \u2208 U which belong to the (m \u2212 q \u2212 2) nearest neighbours of v.\n\nSuppose there are a 1 nearest neighbours in V and a 2 nearest neighbours in U, where a 1 + a 2 = m \u2212 q \u2212 2.\n\nSince the correct gradients are bounded by\nv i \u2212v 2 \u2264 v 2 , \u2200i \u2208 [m \u2212 q], it is easy to check that v \u2212 u 2 \u2265 2 v 2 . Thus, we have KR(v) \u2265 a 1 \u03b2 2 + a 2 v \u2212 u 2 \u2265 (m \u2212 q \u2212 2) 2 v 2 .\n\u2022 Case 2: There are no u \u2208 U which belong to the (m \u2212 q \u2212 2) nearest neighbours of v. Thus, we have\nKR(v) \u2265 (m \u2212 q \u2212 2)\u03b2 2 \u2265 (m \u2212 q \u2212 2) 2 v 2 .\nIn both cases, we have\nKR(v) \u2265 (m \u2212 q \u2212 2) 2 v 2 .\nThus, when (m\u2212q) is large enough: m\u2212q > 2( +2) 2 2 +2, we have\nKR(u) \u2264 2( + 2) 2 v 2 < (m \u2212 q \u2212 2) 2 v 2 \u2264 KR(v). As a result, Krum(V \u2229 U) = u = \u2212 v. Thus, E [Krum(V \u2229 U)] = \u2212 g.\nRemark 4. In the theorem above, we assume that all the correct gradients are inside a Euclidean ball centered at their mean:\nv i \u2212v 2 \u2264 v 2 , \u2200i \u2208 [m \u2212 q]\n. Such assumption can not always be satisfied, but it is reasonable that the random samples are sometimes inside such a Euclidean ball, if the variance is not too large. On the other hand, we assume that the pair-wise distances between the correct gradients are lower-bounded by \u03b2 > 0. Almost surely, such \u03b2 exists, no matter how small it is. Note that the Byzantine attackers are supposed to be omniscient. Thus, the attackers can spy on the honest workers, and obtain V and \u03b2. Then, the attackers can choose an such that 2 v 2 \u2264 \u03b2 2 . Finally, we only need the number of workers to be large enough, so that m \u2212 q > 2( +2) 2 2 + 2.\n\nRemark 5. The proof of Theorem 2 provides the intuition of constructing adversarial gradients for the attackers. In practice, the attackers only need to assign m\u2212q i\u2208 [m\u2212q] v i to all the Byzantine gradients, with an > 0 small enough.\n\nRemark 6. Note that in Blanchard et al. (2017), Krum requires the assumption that c\u03c3 < g for convergence, where c is a general constant, \u03c3 is the maximal variance of the gradients, and g is the gradient in expectation. Note that g \u2192 0 when SGD converges to a critical point. Thus, such an assumption is never guaranteed to be satisfied, if the variance is non-zero. Furthermore, the better SGD converges, the less likely such an assumption can be satisfied.\n\n\nToy Example\n\nNote that the assumptions made in Theorem 2 are sufficient but not necessary conditions of the DSSGD-Byzantine vulnerability of Krum. In practice, it can be easier to find an that crashes Krum, especially for 1dimensional cases.\n\nWe \n\n\nCASE STUDY\n\nIn this section, we implement special attack strategies for Median and Krum, and evaluate our attack strategies on a real-world application. The attack strategies are designed by using the intuitions underlying Theorem 1 and Theorem 2, which are mentioned in Remark 3 and Remark 5.\n\n\nDATASETS AND EVALUATION METRICS\n\nWe conduct experiments on the benchmark CIFAR-10 image classification dataset (Krizhevsky & Hinton, 2009), which is composed of 50k images for training and 10k images for testing. We use a convolutional neural network (CNN) with 4 convolutional layers followed by 1 fully connected layer. The detailed network architecture can be found in the appendix. For any worker, the mini-     batch size for SGD is 50.\n\nIn each experiment, we launch 25 worker processes. We repeat each experiment 10 times and take the average. We use top-1 accuracy on the testing set and the cross-entropy loss function on the training set as the evaluation metrics.\n\nWe use the averaging, Median, and Krum without attacks as the gold standard, which are referred to as Mean without attack, Median without attack, and Krum without attack. We start the attack at different epochs, so that SGD can warm up and make some progress first. We include some additional experiments in the appendix.\n\n\nMEDIAN\n\nIn each iteration, the server receives m = 25 gradients. A randomly selected subset of q = 12 correct gradients are replaced by Byzantine gradients. We define the set of Byzantine gradients as U = {u 1 , . . . , u 12 }, and the set of the remaining correct gradients as V = {v 1 , . . . , v 13 }.\n\nOur attack strategy is as follows:\nu 1 = u 2 = \u00b7 \u00b7 \u00b7 = u 12 = \u2212 13 13 i=1 v i .\nAccording to Theorem 1 and Remark 3, Median is vulnerable to positive with large magnitude | |.\n\nWe test the above attack strategy with different . The results are shown in Figure 4. Median fails when > 0. When = 0, Median gets stuck and stops making progress. When < 0, Median successfully defends against the attack.\n\n\nKRUM\n\nIn each iteration, the server receives m = 25 gradients. A randomly selected subset of q = 11 correct gradients are replaced by Byzantine gradients. We define the set of Byzantine gradients as U = {u 1 , . . . , u 11 }, and the set of the remaining correct gradients as V = {v 1 , . . . , v 14 }. Our attack strategy is as follows:\nu 1 = u 2 = \u00b7 \u00b7 \u00b7 = u 11 = \u2212 14 14 i=1 v i .\nAccording to Theorem 2 and Remark 5, Krum is vulnerable to positive with small magnitude | |.\n\nWe test the above attack strategy with different . The results are shown in Figure 5. Krum fails when > 0 is small. When is large enough, Krum successfully defends against the attack.\n\n\nDISCUSSION\n\nSurprisingly, both Median and Krum are more vulnerable than we expected. Note that our theorems only analyze the worst cases. There are other cases where Median and Krum can fail.\n\nFor Median, even if we take = 0, SGD still performs badly. Theoretically, even if we do not use positive , small can still enlarge the variance of SGD, which can be potentially harmful to the convergence. We can see that with large negative , the defense of Median is successful. In our experiment, we reveal certain new vulnerabilities of Median in distributed synchronous SGD. The experiments conducted by Yin et al. (2018) do not fail because the attacker only changes the labels of the poisoned training data by flipping a label \u2208 {0, . . . , 9} to 9 \u2212 label. It is very likely that such an attack produces Byzantine gradients surrounding the correct gradients coordinate-wisely on both sides. However, according to Theorem 1 and Remark 3, an effective attack should place the Byzantine gradient on one and only one side of the correct gradients, which is the side opposite to the mean of the correct gradients, coordinate-wisely.\n\nFor Krum, small positive makes SGD vulnerable. Furthermore, even if we take = 1, Krum still fails. In our experiment, we reveal certain new vulnerability of Krum in distributed synchronous SGD. The experiments conducted by Blanchard et al. (2017) do not fail even though a similar attack strategy called \"omniscient\" is conducted. The reason is that, in the paper of Blanchard et al. (2017), the attacker \"proposes the opposite vector, scaled to a large length\", which is similar to our attack strategy with a large .\n\nGuided by our theoretical analysis, we designed efficient attack strategies for both Median and Krum. Our results show that the definition of Byzantine tolerance for distributed synchronous SGD should be revised. Using our definition of DSSGD-Byzantine tolerance, research can be conducted to design better defense techniques.\n\n\nCONCLUSION\n\nWe propose a revised definition of Byzantine tolerance for distributed synchronous SGD. With the new definition, we theoretically and empirically examine the Byzantine tolerance of two prevailing robust aggregation rules. Guided by our theoretical analysis, attack techniques can be designed to fail the aggregation rules. In the future, we hope new defense techniques can be designed using our revised definition of Byzantine tolerance.\n\nFigure 1 :\n1Illustration of failed Byzantine-tolerant SGD.\n\nFigure 3 :\n3Worker-Server Architecture\n\n.\nprovide an 1-dimensional toy example to show how easily Krum can fail. Suppose there are 6 correct gradients V = {0, 0.02, 0.14, 0.26, 0.38, 0.5} with the mean 0.2167, and 3 Byzantine gradient U = {\u22120.1, \u22120.1, \u22120.1} with the negative mean \u22120.1. According to Definition 3, the corresponding function values KR(\u00b7) of U \u2229 Thus, Krum(U \u2229 V) = \u22120.1, which means that Krum chooses the Byzantine gradient with the opposite sign of the mean of the correct gradients.\n\n10 Figure 4 :\n104Median, attack starts at 10th epoch, =-10 Median, attack starts at 50th epoch, =-10 Median, attack starts at 100th epoch, =-10 Median, attack starts at 150th epoch, Median, attack starts at 10th epoch, =-10 Median, attack starts at 50th epoch, =-10 Median, attack starts at 100th epoch, =-10 Median, attack starts at 150th epoch, =-10 (h) Cross Entropy on Training Set, = \u2212Convergence on training set, using Median as aggregation rule. \u2208 {10, 0.1, 0, \u221210}.\n\nFigure 5 :\n5Convergence on training set, using Krum as aggregation rule. \u2208 {0.1, 0.5, 1, 10}.\n\nTable 1 :\n1NotationsNotation Description \nm \nNumber of workers \nn \nMinibatch size on each worker \nT \nNumber of iterations \n[m] \nSet of integers {1, . . . , m} \nq \nNumber of Byzantine workers \n\u03b3 \n\n\n\nattack starts at 10th iteration, =1 Krum, attack starts at 50th iteration, =1 Krum, attack starts at 100th iteration, =1 Krum, attack starts at 150th iteration, =1 (f) Cross Entropy on Training Set, = 1Top-1 accuracy \n\nMean without attack \nKrum without attack \nKrum, attack starts at 10th iteration, =0.1 \nKrum, attack starts at 50th iteration, =0.1 \nKrum, attack starts at 100th iteration, =0.1 \nKrum, attack starts at 150th iteration, =0.1 \n\n(a) Top-1 Accuracy on Testing Set, = 0.1 \n\n0 \n20 \n40 \n60 \n80 \n100 \n120 \n140 \n160 \n180 \n\nEpoch \n\n0 \n\n0.5 \n\n1 \n\n1.5 \n\n2 \n\n2.5 \n\nLoss \n\nMean without attack \nKrum without attack \nKrum, attack starts at 10th iteration, =0.1 \nKrum, attack starts at 50th iteration, =0.1 \nKrum, attack starts at 100th iteration, =0.1 \nKrum, attack starts at 150th iteration, =0.1 \n\n(b) Cross Entropy on Training Set, = 0.1 \n\n0 \n20 \n40 \n60 \n80 \n100 \n120 \n140 \n160 \n180 \n\nEpoch \n\n0 \n\n0.1 \n\n0.2 \n\n0.3 \n\n0.4 \n\n0.5 \n\n0.6 \n\n0.7 \n\n0.8 \n\n0.9 \n\nTop-1 accuracy \n\nMean without attack \nKrum without attack \nKrum, attack starts at 10th iteration, =0.5 \nKrum, attack starts at 50th iteration, =0.5 \nKrum, attack starts at 100th iteration, =0.5 \nKrum, attack starts at 150th iteration, =0.5 \n\n(c) Top-1 Accuracy on Testing Set, = 0.5 \n\n0 \n20 \n40 \n60 \n80 \n100 \n120 \n140 \n160 \n180 \n\nEpoch \n\n0 \n\n0.5 \n\n1 \n\n1.5 \n\n2 \n\n2.5 \n\nLoss \n\nMean without attack \nKrum without attack \nKrum, attack starts at 10th iteration, =0.5 \nKrum, attack starts at 50th iteration, =0.5 \nKrum, attack starts at 100th iteration, =0.5 \nKrum, attack starts at 150th iteration, =0.5 \n\n(d) Cross Entropy on Training Set, = 0.5 \n\n0 \n20 \n40 \n60 \n80 \n100 \n120 \n140 \n160 \n180 \n\nEpoch \n\n0 \n\n0.1 \n\n0.2 \n\n0.3 \n\n0.4 \n\n0.5 \n\n0.6 \n\n0.7 \n\n0.8 \n\n0.9 \n\nTop-1 accuracy \n\nMean without attack \nKrum without attack \nKrum, attack starts at 10th iteration, =1 \nKrum, attack starts at 50th iteration, =1 \nKrum, attack starts at 100th iteration, =1 \nKrum, attack starts at 150th iteration, =1 \n\n(e) Top-1 Accuracy on Testing Set, = 1 \n\n0 \n20 \n40 \n60 \n80 \n100 \n120 \n140 \n160 \n180 \n\nEpoch \n\n0 \n\n0.5 \n\n1 \n\n1.5 \n\n2 \n\n2.5 \n\nLoss \n\nMean without attack \nKrum without attack \nKrum, 0 \n20 \n40 \n60 \n80 \n100 \n120 \n140 \n160 \n180 \n\nEpoch \n\n0 \n\n0.1 \n\n0.2 \n\n0.3 \n\n0.4 \n\n0.5 \n\n0.6 \n\n0.7 \n\n0.8 \n\n0.9 \n\nTop-1 accuracy \n\nMean without attack \nKrum without attack \nKrum, attack starts at 10th iteration, =10 \nKrum, attack starts at 50th iteration, =10 \nKrum, attack starts at 100th iteration, =10 \nKrum, attack starts at 150th iteration, =10 \n\n(g) Top-1 Accuracy on Testing Set, = 10 \n\n0 \n20 \n40 \n60 \n80 \n100 \n120 \n140 \n160 \n180 \n\nEpoch \n\n0 \n\n0.5 \n\n1 \n\n1.5 \n\n2 \n\n2.5 \n\nLoss \n\nMean without attack \nKrum without attack \nKrum, attack starts at 10th iteration, =10 \nKrum, attack starts at 50th iteration, =10 \nKrum, attack starts at 100th iteration, =10 \nKrum, attack starts at 150th iteration, =10 \n\n\n\nByzantine stochastic gradient descent. D Alistarh, Z Allen-Zhu, Li , J , arXiv:1803.08917arXiv preprintAlistarh, D., Allen-Zhu, Z., and Li, J. Byzantine stochas- tic gradient descent. arXiv preprint arXiv:1803.08917, 2018.\n\nBounds on expectations of linear systematic statistics based on dependent samples. B C Arnold, R A Groeneveld, The Annals of Statistics. 71Arnold, B. C., Groeneveld, R. A., et al. Bounds on expec- tations of linear systematic statistics based on depen- dent samples. The Annals of Statistics, 7(1):220-223, 1979.\n\nObfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. A Athalye, N Carlini, D Wagner, International Conference on Machine Learning. Athalye, A., Carlini, N., and Wagner, D. Obfuscated gradients give a false sense of security: Circumvent- ing defenses to adversarial examples. In International Conference on Machine Learning, pp. 274-283, 2018.\n\nBasic concepts and taxonomy of dependable and secure computing. A Avizienis, J.-C Laprie, B Randell, C Landwehr, IEEE transactions on dependable and secure computing. 11Avizienis, A., Laprie, J.-C., Randell, B., and Landwehr, C. Basic concepts and taxonomy of dependable and secure computing. IEEE transactions on dependable and secure computing, 1(1):11-33, 2004.\n\nE Bagdasaryan, A Veit, Y Hua, D Estrin, V Shmatikov, arXiv:1807.00459How to backdoor federated learning. arXiv preprintBagdasaryan, E., Veit, A., Hua, Y., Estrin, D., and Shmatikov, V. How to backdoor federated learning. arXiv preprint arXiv:1807.00459, 2018.\n\nAnalyzing federated learning through an adversarial lens. A N Bhagoji, S Chakraborty, P Mittal, S Calo, arXiv:1811.12470arXiv preprintBhagoji, A. N., Chakraborty, S., Mittal, P., and Calo, S. Analyzing federated learning through an adversarial lens. arXiv preprint arXiv:1811.12470, 2018.\n\nMachine learning with adversaries: Byzantine tolerant gradient descent. P Blanchard, R Guerraoui, J Stainer, Advances in Neural Information Processing Systems. Blanchard, P., Guerraoui, R., Stainer, J., et al. Machine learning with adversaries: Byzantine tolerant gradient descent. In Advances in Neural Information Processing Systems, pp. 118-128, 2017.\n\nByzantine-resilient distributed training via redundant gradients. L Chen, H Wang, Z Charles, D Papailiopoulos, Draco, International Conference on Machine Learning. Chen, L., Wang, H., Charles, Z., and Papailiopoulos, D. Draco: Byzantine-resilient distributed training via re- dundant gradients. In International Conference on Machine Learning, pp. 902-911, 2018.\n\nDistributed statistical machine learning in adversarial settings: Byzantine gradient descent. Y Chen, L Su, J Xu, 1:44:1-44:25POMACS. Chen, Y., Su, L., and Xu, J. Distributed statistical machine learning in adversarial settings: Byzantine gradient descent. POMACS, 1:44:1-44:25, 2017.\n\n. J Feng, H Xu, S Mannor, arXiv:1409.5937Distributed robust learning. arXiv preprintFeng, J., Xu, H., and Mannor, S. Distributed robust learning. arXiv preprint arXiv:1409.5937, 2014.\n\nImpossibility of distributed consensus with one faulty process. M J Fischer, N A Lynch, M S Paterson, MASSACHUSETTS INST OF TECH CAMBRIDGE LAB FOR COMPUTER SCIENCE. Technical reportFischer, M. J., Lynch, N. A., and Paterson, M. S. Im- possibility of distributed consensus with one faulty process. Technical report, MASSACHUSETTS INST OF TECH CAMBRIDGE LAB FOR COMPUTER SCIENCE, 1982.\n\nThe hidden vulnerability of distributed learning in byzantium. R Guerraoui, S Rouault, International Conference on Machine Learning. Guerraoui, R., Rouault, S., et al. The hidden vulnera- bility of distributed learning in byzantium. In Inter- national Conference on Machine Learning, pp. 3518- 3527, 2018.\n\nOn the bounds of the range of order statistics. D M Hawkins, Journal of the American Statistical Association. 66335Hawkins, D. M. On the bounds of the range of order statis- tics. Journal of the American Statistical Association, 66(335):644-645, 1971.\n\nLearning multiple layers of features from tiny images. A Krizhevsky, G Hinton, Krizhevsky, A. and Hinton, G. Learning multiple layers of features from tiny images. 2009.\n\nThe byzantine generals problem. L Lamport, R E Shostak, M C Pease, ACM Trans. Program. Lang. Syst. 4Lamport, L., Shostak, R. E., and Pease, M. C. The byzan- tine generals problem. ACM Trans. Program. Lang. Syst., 4:382-401, 1982.\n\nScaling distributed machine learning with the parameter server. M Li, D G Andersen, J W Park, A J Smola, A Ahmed, V Josifovski, J Long, E J Shekita, B.-Y Su, OSDI. 14Li, M., Andersen, D. G., Park, J. W., Smola, A. J., Ahmed, A., Josifovski, V., Long, J., Shekita, E. J., and Su, B.-Y. Scaling distributed machine learning with the parameter server. In OSDI, volume 14, pp. 583-598, 2014a.\n\nCommunication efficient distributed machine learning with the parameter server. M Li, D G Andersen, A J Smola, Yu , K , Advances in Neural Information Processing Systems. Li, M., Andersen, D. G., Smola, A. J., and Yu, K. Commu- nication efficient distributed machine learning with the parameter server. In Advances in Neural Information Processing Systems, pp. 19-27, 2014b.\n\nDistributed algorithms. N A Lynch, ElsevierLynch, N. A. Distributed algorithms. Elsevier, 1996.\n\nCommunication-efficient learning of deep networks from decentralized data. H B Mcmahan, E Moore, D Ramage, S Hampson, arXiv:1602.05629arXiv preprintMcMahan, H. B., Moore, E., Ramage, D., Hampson, S., et al. Communication-efficient learning of deep networks from decentralized data. arXiv preprint arXiv:1602.05629, 2016.\n\nFault-tolerant multi-agent optimization: Optimal iterative distributed algorithms. L Su, N H Vaidya, PODC. Su, L. and Vaidya, N. H. Fault-tolerant multi-agent opti- mization: Optimal iterative distributed algorithms. In PODC, 2016a.\n\nDefending non-bayesian learning against adversarial attacks. L Su, N H Vaidya, arXiv:1606.08883arXiv preprintSu, L. and Vaidya, N. H. Defending non-bayesian learning against adversarial attacks. arXiv preprint arXiv:1606.08883, 2016b.\n\nDistributed systems: principles and paradigms. A S Tanenbaum, M Van Steen, Prentice-HallTanenbaum, A. S. and Van Steen, M. Distributed systems: principles and paradigms. Prentice-Hall, 2007.\n\nStrategies and principles of distributed machine learning on big data. E P Xing, Q Ho, P Xie, Wei , D , Engineering. 22Xing, E. P., Ho, Q., Xie, P., and Wei, D. Strategies and principles of distributed machine learning on big data. Engineering, 2(2):179-195, 2016.\n\nD Yin, Y Chen, K Ramchandran, P Bartlett, arXiv:1803.01498Byzantine-robust distributed learning: Towards optimal statistical rates. arXiv preprintYin, D., Chen, Y., Ramchandran, K., and Bartlett, P. Byzantine-robust distributed learning: Towards opti- mal statistical rates. arXiv preprint arXiv:1803.01498, 2018.\n", "annotations": {"author": "[{\"end\":177,\"start\":82},{\"end\":277,\"start\":178},{\"end\":379,\"start\":278}]", "publisher": null, "author_last_name": "[{\"end\":90,\"start\":87},{\"end\":190,\"start\":184},{\"end\":292,\"start\":287}]", "author_first_name": "[{\"end\":86,\"start\":82},{\"end\":183,\"start\":178},{\"end\":286,\"start\":278}]", "author_affiliation": "[{\"end\":176,\"start\":92},{\"end\":276,\"start\":192},{\"end\":378,\"start\":294}]", "title": "[{\"end\":79,\"start\":1},{\"end\":458,\"start\":380}]", "venue": null, "abstract": "[{\"end\":1009,\"start\":460}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1184,\"start\":1162},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1448,\"start\":1424},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1466,\"start\":1448},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1483,\"start\":1466},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1501,\"start\":1483},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":1520,\"start\":1501},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1542,\"start\":1520},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":1767,\"start\":1749},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3402,\"start\":3389},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3425,\"start\":3402},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3453,\"start\":3425},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3474,\"start\":3453},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3866,\"start\":3847},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4104,\"start\":4080},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4121,\"start\":4104},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5516,\"start\":5498},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5550,\"start\":5526},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6310,\"start\":6292},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6343,\"start\":6320},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7091,\"start\":7074},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7205,\"start\":7182},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7568,\"start\":7544},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7914,\"start\":7893},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7995,\"start\":7974},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8022,\"start\":7997},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8085,\"start\":8063},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10621,\"start\":10597},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10639,\"start\":10621},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10656,\"start\":10639},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11852,\"start\":11834},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":12194,\"start\":12170},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16901,\"start\":16881},{\"end\":21453,\"start\":21448},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21563,\"start\":21540},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":22660,\"start\":22633},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":25511,\"start\":25494},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26268,\"start\":26245},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26412,\"start\":26389}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27379,\"start\":27320},{\"attributes\":{\"id\":\"fig_1\"},\"end\":27419,\"start\":27380},{\"attributes\":{\"id\":\"fig_2\"},\"end\":27881,\"start\":27420},{\"attributes\":{\"id\":\"fig_4\"},\"end\":28356,\"start\":27882},{\"attributes\":{\"id\":\"fig_5\"},\"end\":28451,\"start\":28357},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":28648,\"start\":28452},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":31476,\"start\":28649}]", "paragraph": "[{\"end\":1616,\"start\":1025},{\"end\":2540,\"start\":1618},{\"end\":3103,\"start\":2542},{\"end\":3320,\"start\":3105},{\"end\":4122,\"start\":3322},{\"end\":4668,\"start\":4124},{\"end\":4903,\"start\":4670},{\"end\":5350,\"start\":4905},{\"end\":5908,\"start\":5352},{\"end\":6206,\"start\":5910},{\"end\":6588,\"start\":6208},{\"end\":6721,\"start\":6590},{\"end\":6850,\"start\":6723},{\"end\":7411,\"start\":6867},{\"end\":7748,\"start\":7413},{\"end\":8206,\"start\":7750},{\"end\":8448,\"start\":8224},{\"end\":8527,\"start\":8480},{\"end\":8765,\"start\":8575},{\"end\":9039,\"start\":8793},{\"end\":9247,\"start\":9084},{\"end\":9465,\"start\":9291},{\"end\":9650,\"start\":9549},{\"end\":9802,\"start\":9694},{\"end\":10271,\"start\":10151},{\"end\":10388,\"start\":10288},{\"end\":10496,\"start\":10456},{\"end\":10567,\"start\":10498},{\"end\":10875,\"start\":10569},{\"end\":11150,\"start\":10959},{\"end\":11226,\"start\":11152},{\"end\":11447,\"start\":11228},{\"end\":11478,\"start\":11449},{\"end\":11662,\"start\":11501},{\"end\":11769,\"start\":11664},{\"end\":11920,\"start\":11796},{\"end\":12195,\"start\":12150},{\"end\":12418,\"start\":12280},{\"end\":12504,\"start\":12420},{\"end\":12822,\"start\":12526},{\"end\":13325,\"start\":12853},{\"end\":13420,\"start\":13361},{\"end\":13581,\"start\":13422},{\"end\":13953,\"start\":13583},{\"end\":14466,\"start\":13955},{\"end\":14614,\"start\":14491},{\"end\":14958,\"start\":14616},{\"end\":15304,\"start\":14985},{\"end\":16136,\"start\":15667},{\"end\":16443,\"start\":16138},{\"end\":16500,\"start\":16445},{\"end\":16708,\"start\":16649},{\"end\":16822,\"start\":16732},{\"end\":16911,\"start\":16824},{\"end\":17418,\"start\":17002},{\"end\":18059,\"start\":17420},{\"end\":18475,\"start\":18075},{\"end\":18622,\"start\":18484},{\"end\":19338,\"start\":18624},{\"end\":19640,\"start\":19429},{\"end\":19734,\"start\":19680},{\"end\":19823,\"start\":19736},{\"end\":19932,\"start\":19825},{\"end\":19976,\"start\":19934},{\"end\":20216,\"start\":20117},{\"end\":20284,\"start\":20262},{\"end\":20375,\"start\":20313},{\"end\":20616,\"start\":20492},{\"end\":21279,\"start\":20647},{\"end\":21515,\"start\":21281},{\"end\":21974,\"start\":21517},{\"end\":22218,\"start\":21990},{\"end\":22223,\"start\":22220},{\"end\":22519,\"start\":22238},{\"end\":22963,\"start\":22555},{\"end\":23196,\"start\":22965},{\"end\":23519,\"start\":23198},{\"end\":23826,\"start\":23530},{\"end\":23862,\"start\":23828},{\"end\":24003,\"start\":23908},{\"end\":24226,\"start\":24005},{\"end\":24566,\"start\":24235},{\"end\":24705,\"start\":24612},{\"end\":24890,\"start\":24707},{\"end\":25084,\"start\":24905},{\"end\":26020,\"start\":25086},{\"end\":26539,\"start\":26022},{\"end\":26867,\"start\":26541},{\"end\":27319,\"start\":26882}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8574,\"start\":8528},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8792,\"start\":8766},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9083,\"start\":9040},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9290,\"start\":9248},{\"attributes\":{\"id\":\"formula_4\"},\"end\":9548,\"start\":9466},{\"attributes\":{\"id\":\"formula_5\"},\"end\":9693,\"start\":9651},{\"attributes\":{\"id\":\"formula_6\"},\"end\":10150,\"start\":9869},{\"attributes\":{\"id\":\"formula_7\"},\"end\":10455,\"start\":10389},{\"attributes\":{\"id\":\"formula_8\"},\"end\":10958,\"start\":10876},{\"attributes\":{\"id\":\"formula_9\"},\"end\":12142,\"start\":11921},{\"attributes\":{\"id\":\"formula_10\"},\"end\":12279,\"start\":12196},{\"attributes\":{\"id\":\"formula_11\"},\"end\":13360,\"start\":13326},{\"attributes\":{\"id\":\"formula_12\"},\"end\":14490,\"start\":14467},{\"attributes\":{\"id\":\"formula_13\"},\"end\":15666,\"start\":15305},{\"attributes\":{\"id\":\"formula_14\"},\"end\":16648,\"start\":16501},{\"attributes\":{\"id\":\"formula_15\"},\"end\":16731,\"start\":16709},{\"attributes\":{\"id\":\"formula_16\"},\"end\":17001,\"start\":16912},{\"attributes\":{\"id\":\"formula_17\"},\"end\":19428,\"start\":19339},{\"attributes\":{\"id\":\"formula_18\"},\"end\":19679,\"start\":19641},{\"attributes\":{\"id\":\"formula_19\"},\"end\":20116,\"start\":19977},{\"attributes\":{\"id\":\"formula_20\"},\"end\":20261,\"start\":20217},{\"attributes\":{\"id\":\"formula_21\"},\"end\":20312,\"start\":20285},{\"attributes\":{\"id\":\"formula_22\"},\"end\":20491,\"start\":20376},{\"attributes\":{\"id\":\"formula_23\"},\"end\":20646,\"start\":20617},{\"attributes\":{\"id\":\"formula_24\"},\"end\":23907,\"start\":23863},{\"attributes\":{\"id\":\"formula_25\"},\"end\":24611,\"start\":24567}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":11209,\"start\":11202}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1023,\"start\":1011},{\"attributes\":{\"n\":\"2\"},\"end\":6865,\"start\":6853},{\"attributes\":{\"n\":\"3\"},\"end\":8222,\"start\":8209},{\"attributes\":{\"n\":\"3.1\"},\"end\":8478,\"start\":8451},{\"end\":9868,\"start\":9805},{\"attributes\":{\"n\":\"3.2\"},\"end\":10286,\"start\":10274},{\"attributes\":{\"n\":\"4\"},\"end\":11499,\"start\":11481},{\"attributes\":{\"n\":\"4.1\"},\"end\":11794,\"start\":11772},{\"attributes\":{\"n\":\"4.2\"},\"end\":12148,\"start\":12144},{\"attributes\":{\"n\":\"5\"},\"end\":12524,\"start\":12507},{\"attributes\":{\"n\":\"5.1\"},\"end\":12851,\"start\":12825},{\"attributes\":{\"n\":\"5.2\"},\"end\":14983,\"start\":14961},{\"attributes\":{\"n\":\"5.2.1\"},\"end\":18073,\"start\":18062},{\"attributes\":{\"n\":\"5.3\"},\"end\":18482,\"start\":18478},{\"attributes\":{\"n\":\"5.3.1\"},\"end\":21988,\"start\":21977},{\"attributes\":{\"n\":\"6\"},\"end\":22236,\"start\":22226},{\"attributes\":{\"n\":\"6.1\"},\"end\":22553,\"start\":22522},{\"attributes\":{\"n\":\"6.2\"},\"end\":23528,\"start\":23522},{\"attributes\":{\"n\":\"6.3\"},\"end\":24233,\"start\":24229},{\"attributes\":{\"n\":\"6.4\"},\"end\":24903,\"start\":24893},{\"attributes\":{\"n\":\"7\"},\"end\":26880,\"start\":26870},{\"end\":27331,\"start\":27321},{\"end\":27391,\"start\":27381},{\"end\":27422,\"start\":27421},{\"end\":27896,\"start\":27883},{\"end\":28368,\"start\":28358},{\"end\":28462,\"start\":28453}]", "table": "[{\"end\":28648,\"start\":28473},{\"end\":31476,\"start\":28853}]", "figure_caption": "[{\"end\":27379,\"start\":27333},{\"end\":27419,\"start\":27393},{\"end\":27881,\"start\":27423},{\"end\":28356,\"start\":27900},{\"end\":28451,\"start\":28370},{\"end\":28473,\"start\":28464},{\"end\":28853,\"start\":28651}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":1790,\"start\":1782},{\"end\":4275,\"start\":4267},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5560,\"start\":5552},{\"end\":24089,\"start\":24081},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":24791,\"start\":24783}]", "bib_author_first_name": "[{\"end\":31518,\"start\":31517},{\"end\":31530,\"start\":31529},{\"end\":31544,\"start\":31542},{\"end\":31548,\"start\":31547},{\"end\":31786,\"start\":31785},{\"end\":31788,\"start\":31787},{\"end\":31798,\"start\":31797},{\"end\":31800,\"start\":31799},{\"end\":32118,\"start\":32117},{\"end\":32129,\"start\":32128},{\"end\":32140,\"start\":32139},{\"end\":32473,\"start\":32472},{\"end\":32489,\"start\":32485},{\"end\":32499,\"start\":32498},{\"end\":32510,\"start\":32509},{\"end\":32775,\"start\":32774},{\"end\":32790,\"start\":32789},{\"end\":32798,\"start\":32797},{\"end\":32805,\"start\":32804},{\"end\":32815,\"start\":32814},{\"end\":33094,\"start\":33093},{\"end\":33096,\"start\":33095},{\"end\":33107,\"start\":33106},{\"end\":33122,\"start\":33121},{\"end\":33132,\"start\":33131},{\"end\":33398,\"start\":33397},{\"end\":33411,\"start\":33410},{\"end\":33424,\"start\":33423},{\"end\":33748,\"start\":33747},{\"end\":33756,\"start\":33755},{\"end\":33764,\"start\":33763},{\"end\":33775,\"start\":33774},{\"end\":34140,\"start\":34139},{\"end\":34148,\"start\":34147},{\"end\":34154,\"start\":34153},{\"end\":34334,\"start\":34333},{\"end\":34342,\"start\":34341},{\"end\":34348,\"start\":34347},{\"end\":34581,\"start\":34580},{\"end\":34583,\"start\":34582},{\"end\":34594,\"start\":34593},{\"end\":34596,\"start\":34595},{\"end\":34605,\"start\":34604},{\"end\":34607,\"start\":34606},{\"end\":34965,\"start\":34964},{\"end\":34978,\"start\":34977},{\"end\":35257,\"start\":35256},{\"end\":35259,\"start\":35258},{\"end\":35517,\"start\":35516},{\"end\":35531,\"start\":35530},{\"end\":35665,\"start\":35664},{\"end\":35676,\"start\":35675},{\"end\":35678,\"start\":35677},{\"end\":35689,\"start\":35688},{\"end\":35691,\"start\":35690},{\"end\":35928,\"start\":35927},{\"end\":35934,\"start\":35933},{\"end\":35936,\"start\":35935},{\"end\":35948,\"start\":35947},{\"end\":35950,\"start\":35949},{\"end\":35958,\"start\":35957},{\"end\":35960,\"start\":35959},{\"end\":35969,\"start\":35968},{\"end\":35978,\"start\":35977},{\"end\":35992,\"start\":35991},{\"end\":36000,\"start\":35999},{\"end\":36002,\"start\":36001},{\"end\":36016,\"start\":36012},{\"end\":36334,\"start\":36333},{\"end\":36340,\"start\":36339},{\"end\":36342,\"start\":36341},{\"end\":36354,\"start\":36353},{\"end\":36356,\"start\":36355},{\"end\":36366,\"start\":36364},{\"end\":36370,\"start\":36369},{\"end\":36654,\"start\":36653},{\"end\":36656,\"start\":36655},{\"end\":36802,\"start\":36801},{\"end\":36804,\"start\":36803},{\"end\":36815,\"start\":36814},{\"end\":36824,\"start\":36823},{\"end\":36834,\"start\":36833},{\"end\":37132,\"start\":37131},{\"end\":37138,\"start\":37137},{\"end\":37140,\"start\":37139},{\"end\":37344,\"start\":37343},{\"end\":37350,\"start\":37349},{\"end\":37352,\"start\":37351},{\"end\":37566,\"start\":37565},{\"end\":37568,\"start\":37567},{\"end\":37581,\"start\":37580},{\"end\":37782,\"start\":37781},{\"end\":37784,\"start\":37783},{\"end\":37792,\"start\":37791},{\"end\":37798,\"start\":37797},{\"end\":37807,\"start\":37804},{\"end\":37811,\"start\":37810},{\"end\":37977,\"start\":37976},{\"end\":37984,\"start\":37983},{\"end\":37992,\"start\":37991},{\"end\":38007,\"start\":38006}]", "bib_author_last_name": "[{\"end\":31527,\"start\":31519},{\"end\":31540,\"start\":31531},{\"end\":31795,\"start\":31789},{\"end\":31811,\"start\":31801},{\"end\":32126,\"start\":32119},{\"end\":32137,\"start\":32130},{\"end\":32147,\"start\":32141},{\"end\":32483,\"start\":32474},{\"end\":32496,\"start\":32490},{\"end\":32507,\"start\":32500},{\"end\":32519,\"start\":32511},{\"end\":32787,\"start\":32776},{\"end\":32795,\"start\":32791},{\"end\":32802,\"start\":32799},{\"end\":32812,\"start\":32806},{\"end\":32825,\"start\":32816},{\"end\":33104,\"start\":33097},{\"end\":33119,\"start\":33108},{\"end\":33129,\"start\":33123},{\"end\":33137,\"start\":33133},{\"end\":33408,\"start\":33399},{\"end\":33421,\"start\":33412},{\"end\":33432,\"start\":33425},{\"end\":33753,\"start\":33749},{\"end\":33761,\"start\":33757},{\"end\":33772,\"start\":33765},{\"end\":33790,\"start\":33776},{\"end\":33797,\"start\":33792},{\"end\":34145,\"start\":34141},{\"end\":34151,\"start\":34149},{\"end\":34157,\"start\":34155},{\"end\":34339,\"start\":34335},{\"end\":34345,\"start\":34343},{\"end\":34355,\"start\":34349},{\"end\":34591,\"start\":34584},{\"end\":34602,\"start\":34597},{\"end\":34616,\"start\":34608},{\"end\":34975,\"start\":34966},{\"end\":34986,\"start\":34979},{\"end\":35267,\"start\":35260},{\"end\":35528,\"start\":35518},{\"end\":35538,\"start\":35532},{\"end\":35673,\"start\":35666},{\"end\":35686,\"start\":35679},{\"end\":35697,\"start\":35692},{\"end\":35931,\"start\":35929},{\"end\":35945,\"start\":35937},{\"end\":35955,\"start\":35951},{\"end\":35966,\"start\":35961},{\"end\":35975,\"start\":35970},{\"end\":35989,\"start\":35979},{\"end\":35997,\"start\":35993},{\"end\":36010,\"start\":36003},{\"end\":36019,\"start\":36017},{\"end\":36337,\"start\":36335},{\"end\":36351,\"start\":36343},{\"end\":36362,\"start\":36357},{\"end\":36662,\"start\":36657},{\"end\":36812,\"start\":36805},{\"end\":36821,\"start\":36816},{\"end\":36831,\"start\":36825},{\"end\":36842,\"start\":36835},{\"end\":37135,\"start\":37133},{\"end\":37147,\"start\":37141},{\"end\":37347,\"start\":37345},{\"end\":37359,\"start\":37353},{\"end\":37578,\"start\":37569},{\"end\":37591,\"start\":37582},{\"end\":37789,\"start\":37785},{\"end\":37795,\"start\":37793},{\"end\":37802,\"start\":37799},{\"end\":37981,\"start\":37978},{\"end\":37989,\"start\":37985},{\"end\":38004,\"start\":37993},{\"end\":38016,\"start\":38008}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1803.08917\",\"id\":\"b0\"},\"end\":31700,\"start\":31478},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":121332291},\"end\":32014,\"start\":31702},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3310672},\"end\":32406,\"start\":32016},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":215753451},\"end\":32772,\"start\":32408},{\"attributes\":{\"doi\":\"arXiv:1807.00459\",\"id\":\"b4\"},\"end\":33033,\"start\":32774},{\"attributes\":{\"doi\":\"arXiv:1811.12470\",\"id\":\"b5\"},\"end\":33323,\"start\":33035},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":28527385},\"end\":33679,\"start\":33325},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":49397275},\"end\":34043,\"start\":33681},{\"attributes\":{\"doi\":\"1:44:1-44:25\",\"id\":\"b8\",\"matched_paper_id\":58534983},\"end\":34329,\"start\":34045},{\"attributes\":{\"doi\":\"arXiv:1409.5937\",\"id\":\"b9\"},\"end\":34514,\"start\":34331},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2458859},\"end\":34899,\"start\":34516},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":3473997},\"end\":35206,\"start\":34901},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":123558462},\"end\":35459,\"start\":35208},{\"attributes\":{\"id\":\"b13\"},\"end\":35630,\"start\":35461},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":55899582},\"end\":35861,\"start\":35632},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":4614646},\"end\":36251,\"start\":35863},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":2235165},\"end\":36627,\"start\":36253},{\"attributes\":{\"id\":\"b17\"},\"end\":36724,\"start\":36629},{\"attributes\":{\"doi\":\"arXiv:1602.05629\",\"id\":\"b18\"},\"end\":37046,\"start\":36726},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":15981366},\"end\":37280,\"start\":37048},{\"attributes\":{\"doi\":\"arXiv:1606.08883\",\"id\":\"b20\"},\"end\":37516,\"start\":37282},{\"attributes\":{\"id\":\"b21\"},\"end\":37708,\"start\":37518},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":17010385},\"end\":37974,\"start\":37710},{\"attributes\":{\"doi\":\"arXiv:1803.01498\",\"id\":\"b23\"},\"end\":38289,\"start\":37976}]", "bib_title": "[{\"end\":31783,\"start\":31702},{\"end\":32115,\"start\":32016},{\"end\":32470,\"start\":32408},{\"end\":33395,\"start\":33325},{\"end\":33745,\"start\":33681},{\"end\":34137,\"start\":34045},{\"end\":34578,\"start\":34516},{\"end\":34962,\"start\":34901},{\"end\":35254,\"start\":35208},{\"end\":35662,\"start\":35632},{\"end\":35925,\"start\":35863},{\"end\":36331,\"start\":36253},{\"end\":37129,\"start\":37048},{\"end\":37779,\"start\":37710}]", "bib_author": "[{\"end\":31529,\"start\":31517},{\"end\":31542,\"start\":31529},{\"end\":31547,\"start\":31542},{\"end\":31551,\"start\":31547},{\"end\":31797,\"start\":31785},{\"end\":31813,\"start\":31797},{\"end\":32128,\"start\":32117},{\"end\":32139,\"start\":32128},{\"end\":32149,\"start\":32139},{\"end\":32485,\"start\":32472},{\"end\":32498,\"start\":32485},{\"end\":32509,\"start\":32498},{\"end\":32521,\"start\":32509},{\"end\":32789,\"start\":32774},{\"end\":32797,\"start\":32789},{\"end\":32804,\"start\":32797},{\"end\":32814,\"start\":32804},{\"end\":32827,\"start\":32814},{\"end\":33106,\"start\":33093},{\"end\":33121,\"start\":33106},{\"end\":33131,\"start\":33121},{\"end\":33139,\"start\":33131},{\"end\":33410,\"start\":33397},{\"end\":33423,\"start\":33410},{\"end\":33434,\"start\":33423},{\"end\":33755,\"start\":33747},{\"end\":33763,\"start\":33755},{\"end\":33774,\"start\":33763},{\"end\":33792,\"start\":33774},{\"end\":33799,\"start\":33792},{\"end\":34147,\"start\":34139},{\"end\":34153,\"start\":34147},{\"end\":34159,\"start\":34153},{\"end\":34341,\"start\":34333},{\"end\":34347,\"start\":34341},{\"end\":34357,\"start\":34347},{\"end\":34593,\"start\":34580},{\"end\":34604,\"start\":34593},{\"end\":34618,\"start\":34604},{\"end\":34977,\"start\":34964},{\"end\":34988,\"start\":34977},{\"end\":35269,\"start\":35256},{\"end\":35530,\"start\":35516},{\"end\":35540,\"start\":35530},{\"end\":35675,\"start\":35664},{\"end\":35688,\"start\":35675},{\"end\":35699,\"start\":35688},{\"end\":35933,\"start\":35927},{\"end\":35947,\"start\":35933},{\"end\":35957,\"start\":35947},{\"end\":35968,\"start\":35957},{\"end\":35977,\"start\":35968},{\"end\":35991,\"start\":35977},{\"end\":35999,\"start\":35991},{\"end\":36012,\"start\":35999},{\"end\":36021,\"start\":36012},{\"end\":36339,\"start\":36333},{\"end\":36353,\"start\":36339},{\"end\":36364,\"start\":36353},{\"end\":36369,\"start\":36364},{\"end\":36373,\"start\":36369},{\"end\":36664,\"start\":36653},{\"end\":36814,\"start\":36801},{\"end\":36823,\"start\":36814},{\"end\":36833,\"start\":36823},{\"end\":36844,\"start\":36833},{\"end\":37137,\"start\":37131},{\"end\":37149,\"start\":37137},{\"end\":37349,\"start\":37343},{\"end\":37361,\"start\":37349},{\"end\":37580,\"start\":37565},{\"end\":37593,\"start\":37580},{\"end\":37791,\"start\":37781},{\"end\":37797,\"start\":37791},{\"end\":37804,\"start\":37797},{\"end\":37810,\"start\":37804},{\"end\":37814,\"start\":37810},{\"end\":37983,\"start\":37976},{\"end\":37991,\"start\":37983},{\"end\":38006,\"start\":37991},{\"end\":38018,\"start\":38006}]", "bib_venue": "[{\"end\":31515,\"start\":31478},{\"end\":31837,\"start\":31813},{\"end\":32193,\"start\":32149},{\"end\":32573,\"start\":32521},{\"end\":32877,\"start\":32843},{\"end\":33091,\"start\":33035},{\"end\":33483,\"start\":33434},{\"end\":33843,\"start\":33799},{\"end\":34177,\"start\":34171},{\"end\":34679,\"start\":34618},{\"end\":35032,\"start\":34988},{\"end\":35316,\"start\":35269},{\"end\":35514,\"start\":35461},{\"end\":35729,\"start\":35699},{\"end\":36025,\"start\":36021},{\"end\":36422,\"start\":36373},{\"end\":36651,\"start\":36629},{\"end\":36799,\"start\":36726},{\"end\":37153,\"start\":37149},{\"end\":37341,\"start\":37282},{\"end\":37563,\"start\":37518},{\"end\":37825,\"start\":37814},{\"end\":38106,\"start\":38034}]"}}}, "year": 2023, "month": 12, "day": 17}