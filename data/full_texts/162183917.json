{"id": 162183917, "updated": "2023-10-07 02:09:48.437", "metadata": {"title": "Speech2Face: Learning the Face Behind a Voice", "authors": "[{\"first\":\"Tae-Hyun\",\"last\":\"Oh\",\"middle\":[]},{\"first\":\"Tali\",\"last\":\"Dekel\",\"middle\":[]},{\"first\":\"Changil\",\"last\":\"Kim\",\"middle\":[]},{\"first\":\"Inbar\",\"last\":\"Mosseri\",\"middle\":[]},{\"first\":\"William\",\"last\":\"Freeman\",\"middle\":[\"T.\"]},{\"first\":\"Michael\",\"last\":\"Rubinstein\",\"middle\":[]},{\"first\":\"Wojciech\",\"last\":\"Matusik\",\"middle\":[]}]", "venue": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "How much can we infer about a person's looks from the way they speak? In this paper, we study the task of reconstructing a facial image of a person from a short audio recording of that person speaking. We design and train a deep neural network to perform this task using millions of natural Internet/YouTube videos of people speaking. During training, our model learns voice-face correlations that allow it to produce images that capture various physical attributes of the speakers such as age, gender and ethnicity. This is done in a self-supervised manner, by utilizing the natural co-occurrence of faces and speech in Internet videos, without the need to model attributes explicitly. We evaluate and numerically quantify how--and in what manner--our Speech2Face reconstructions, obtained directly from audio, resemble the true face images of the speakers.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1905.09773", "mag": "2979157532", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/OhDKMFRM19", "doi": "10.1109/cvpr.2019.00772"}}, "content": {"source": {"pdf_hash": "3a3def064c091368d9da89512fd1e7a3e39a2b10", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": "CCBYNCSA", "open_access_url": "https://dspace.mit.edu/bitstream/1721.1/130277/2/1905.09773.pdf", "status": "GREEN"}}, "grobid": {"id": "8bafaa6cfde46b7daea4cbd4061cfbcb0faae9c2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/3a3def064c091368d9da89512fd1e7a3e39a2b10.txt", "contents": "\nSpeech2Face: Learning the Face Behind a Voice\n\n\nTae-Hyun Oh \nMIT CSAIL\n\n\nTali Dekel \nMIT CSAIL\n\n\nChangil Kim \nMIT CSAIL\n\n\nInbar Mosseri \nMIT CSAIL\n\n\nWilliam T Freeman \nMIT CSAIL\n\n\nMichael Rubinstein \nMIT CSAIL\n\n\nWojciech Matusik \nMIT CSAIL\n\n\nSpeech2Face: Learning the Face Behind a Voice\n10.1109/CVPR.2019.00772\nHow much can we infer about a person's looks from the way they speak? In this paper, we study the task of reconstructing a facial image of a person from a short audio recording of that person speaking. We design and train a deep neural network to perform this task using millions of natural Internet/YouTube videos of people speaking. During training, our model learns voice-face correlations that allow it to produce images that capture various physical attributes of the speakers such as age, gender and ethnicity. This is done in a self-supervised manner, by utilizing the natural co-occurrence of faces and speech in Internet videos, without the need to model attributes explicitly. We evaluate and numerically quantify how-and in what manner-our Speech2Face reconstructions, obtained directly from audio, resemble the true face images of the speakers. Supplementary material (SM): https://speech2face.github.io \u2020 Disclaimer: Our project intended purely for academic investigation, however we reckon that it is important to explicitly discuss several ethical considerations due to the potential sensitivity of facial information. See the supplementary material or extended arXiv version for elaborated discussion.\n\nIntroduction\n\nWhen we listen to a person speaking without seeing his/her face, on the phone, or on the radio, we often build a mental model for the way the person looks [23,44]. There is a strong connection between speech and appearance, part of which is a direct result of the mechanics of speech production: age, gender (which affects the pitch of our voice), the shape of the mouth, facial bone structure, thin or full lips-all can affect the sound we generate. In addition, other voice-appearance correlations stem from the way in which we talk: language, accent, speed, pronunciations-such properties of speech are often shared among nationalities and cultures, which can in turn translate to common physical features [12].\n\nOur goal in this work is to study to what extent we can infer how a person looks from the way they talk. Specifically, from a short input audio segment of a person speaking, our method directly reconstructs an image of the person's face in a canonical form (i.e., frontal-facing, neutral faces). Fig. 1 shows sample results of our method. Note that our goal is not to predict a recognizable image of the exact face (nor do Figure 1. Top: We consider the task of reconstructing an image of a person's face from a short input audio segment of speech. Bottom: Several results produced by our Speech2Face model, which takes only an audio waveform as input; the true faces are shown just for reference. Note that our goal is not to reconstruct an accurate image of the person, but rather to recover characteristic physical features that are correlated with the input speech. All our results including the input audio, are available in the supplementary material (SM).\n\nwe believe it is possible), but rather capture dominant facial traits of the person that are correlated with the input speech.\n\nWe design a neural network model that takes the complex spectrogram of a short speech segment as input and predicts a feature vector representing the face. More specifically, the predicted face feature represents a 4096-D face feature that is extracted from the penultimate layer (i.e., one layer prior to the classification layer) of a pre-trained face recognition network [39]. We decode the predicted face feature into a canonical image of the person's face using a separately-trained reconstruction model [10]. To train our model, we use the AVSpeech dataset [13], comprised of millions of video segments from YouTube with more than 100,000 different people speaking. Our method is trained in a self-supervised manner, i.e., it simply uses the natural  Figure 2. Speech2Face model and training pipeline. The input to our network is a complex spectrogram computed from the short audio segment of a person speaking. The output is a 4096-D face feature that is then decoded into a canonical image of the face using a pre-trained face decoder network [10]. The module we train is marked by the orange-tinted box. We train the network to regress to the true face feature computed by feeding an image of the person (representative frame from the video) into a face recognition network [39] and extracting the feature from its penultimate layer. Our model is trained on millions of speech-face embedding pairs from the AVSpeech dataset [13].\n\nco-occurrence of speech and faces in videos, not requiring additional information, e.g., human annotations.\n\nWe are certainly not the first to attempt to infer information about people from their voices. For example, predicting age and gender from speech has been widely explored [52,16,14,7,49]. Indeed, one can consider an alternative approach to attaching a face image to an input voice by first predicting some attributes from the person's voice (e.g., their age, gender, etc [52]), and then either fetching an image from a database that best fits the predicted set of attributes, or using the attributes to generate an image [51]. However, this approach has several limitations. First, predicting attributes from an input signal requires the existence of robust and accurate classifiers and often requires ground truth labels for supervision. For example, predicting age, gender, or ethnicity, from speech requires building classifiers specifically trained to capture those properties. More importantly, this approach limits the predicted face to resemble only an a priori specific set of attributes.\n\nWe aim at studying a more general, open question: what kind of facial information can be extracted from speech? Our approach of predicting full visual appearance (e.g., a face image) directly from speech allows us to explore it without being shackled to predefined facial traits. Specifically, we show that our reconstructed face images can be used as a proxy to convey the visual properties of the person. These visual features include age, gender and ethnicity. Beyond these dominant features, our reconstructions reveal non-negligible correlations between craniofacial features (e.g., nose structure) and voice. This is achieved with no prior information or the existence of accurate classifiers for these types of fine geometric features.\n\nFinally, we believe the face images we recover from speech may also support useful applications. These can include attaching a representative face to a person in phone/video calls where the person may not wish to share their picture or video feed. Other uses may include forensic applications, helping to establish visual profiles of people from phone recordings. In all these cases, our technique can be applied directly to the input speech in a single shot, without the need to first predict a discrete set of attributes, then translate them to a face image.\n\nTo our knowledge, our paper is the first to explore learning to reconstruct face images directly from speech. We test our model on various speakers and present qualitative results. In addition, we numerically evaluate different aspects of our reconstructions including how well a true face image of can be retrieved based solely on an audio query; and how well our reconstructed face images agree with the true face images (unknown to the method, only used for evaluation) in terms of age, gender, ethnicity, and craniofacial features.\n\n\nRelated Work\n\nAudio-visual cross-modal learning. In the recent surge of deep audio-visual multi-modal learning, the co-occurrence of audio and visual signals often provides rich supervision signal without exhaustive labeling, often called selfsupervision [11] or natural supervision [22].\n\nArandjelovi\u0107 and Zisserman leveraged this to jointly learn generic visual and audio representations directly from video data [4]. Aytar et al. [6] and Castrejon et al. [8] use a pretrained network to directionally transfer its knowledge, so that the other modal network can be trained to align a shared representation space. Aytar et al. demonstrated the recognition of objects and places from sound by transferring the knowledge of the pre-trained visual network to the other. Castrejon et al. induced alignment of a shared cross-modal representation by sharing higher layers across networks of each modality. The learned correlation between the vision and the sound are used for cross-modal retrieval [37,38,45], sound source localization in the scenes [41,5,36], or sound source separation [53,13]. Our work uses the pre-trained face recognition network to transfer its face trait knowledge to the voice network.\n\nSpeech-face association learning. The associations between human faces and voices have been studied extensively in many scientific disciplines. These associations are  Channels  2  64  64  128  -128  -128  -256  -512  512  512  -4096  4096  Stride  -1  1  learned by different cross-modal matching approaches: a binary or multi-way classification task [33,32,43]; metric learning [25,19]; and using the multi-task classification loss [49]. The cross-modal signals from face and audio are further used to disambiguate voiced and unvoiced consonants [35,9]; to identify active speakers of a video from non-speakers therein [18,15]; to separate mixed speech signals of multiple speakers [13]. Lip motions from speech enjoyed particular interest due to the direct relationship with the voice as a vocal organ [35,3]. Beyond this, Albanie et al. [2] learned the correlation between speech and emotion. Our work extends this association learning to synthesizing the face resembling original facial attributes of that face from the voice, which has not been explored yet.\n1 2 \u00d7 1 1 2 \u00d7 1 1 2 \u00d7 1 1 2 \u00d7 1 1 2 2 1 1 1 Kernel size - 4 \u00d7 4 4\u00d7 4 4 \u00d7 4 2\u00d7 1 4 \u00d7 4 2\u00d7 1 4 \u00d7 4 2\u00d7 1 4 \u00d7 4 2\u00d7 1 4 \u00d7 4 4\u00d7 4 4 \u00d7 4 \u221e \u00d7 1 1 \u00d7 1 1\u00d7 1\nCross-modal synthesis. Various methods have been recently proposed to reconstruct visual appearances from diverse types of audio signals. In a more graphics-oriented application, automatic generation of facial or body animations from music or speech is of great importance [47,24,46,42]. However, the subject to be reconstructed is generally parameterized a priori, and its texture is typically manually created or mined from a collection of textures. On the contrary, our goal is to reconstruct the face in a pixel level directly from speech, such that the reconstruction embodies inferrable attributes from speech. In the context of pixel-level generative approaches, Sadoughi and Busso [40] propose to synthesize lip motions from speech, and Wiles et al. [50] retarget a reference face image conditionally according to given the facial expression, pose of another face, or audio, respectively. While it is not directly related to audio signal, Yan et al. [51] and Liu and Tuzel [29] address the face image synthesis from given facial attributes as input.\n\n\nSpeech2Face (S2F) Model\n\nThe large variability in facial expressions, head poses, occlusions, and lighting conditions in natural face images make the design and training of a Speech2Face model non-trivial. For example, a straightforward approach of regressing from input speech to image pixels does not work; such a model have to learn to factor out many irrelevant variations in the data and to implicitly extract some meaningful internal representation of faces-a challenging task by itself.\n\nTo sidestep these challenges, we train our model to regress to a low-dimensional intermediate representation of the face. More specifically, we utilize the VGG-Face model, a pretrained face recognition model trained on a large-scale face dataset [39], and extract a 4096-D face feature from the penultimate layer (fc7) of the network. These face features were shown to contain enough information to reconstruct the corresponding face images while being robust to many of the aforementioned variations [10].\n\nOur Speech2Face pipeline, illustrated in Fig. 2, consists of two main components: 1) a voice encoder, which takes a complex spectrogram of speech as input, and predicts a low-dimensional face feature that would correspond to the associated face; and 2) a face decoder, which takes as input the face feature and produces an image of the face in a canonical form (frontal-facing and with neutral expression). During training, the face decoder is fixed, and we train only the voice encoder that predicts the face feature. The voice encoder is a model we designed and trained, while for the face decoder we used the previously proposed model by [10]. We now describe both models in detail. Voice encoder network. Our voice encoder module is a convolutional neural network that turns the spectrogram of a short input speech into a pseudo face feature, which is subsequently fed into the face decoder to reconstruct the face image (Fig. 2). The architecture of the voice encoder is summarized in Table 1. The blocks of a convolution layer, ReLU, and batch normalization [21] alternate with max-pooling layers, which pool along only the temporal dimension of the spectrograms, while leaving the frequency information carried over. This is intended to preserve more of the vocal characteristics, since they are better contained in the frequency content whereas linguistic information usually spans longer time duration [20]. At the end of these blocks, we apply average pooling along the temporal dimension. This allows us to efficiently aggregate information over time and makes the model applicable to input speech of varying duration. The pooled features are then fed into two fully-connected layers to produce a 4096-D face feature. Face decoder network. The goal of the face decoder is to reconstruct the image of a face from a low-dimensional face feature. We opt to factor out any irrelevant variations (pose, lighting, etc.), while preserving the facial attributes.\n\nTo do so, we use the face decoder model of Cole et al. [10] to reconstruct a canonical face image that only contains neutral expression and frontalized face. We train this model using the same face features extracted from VGG-Face model as input to the face decoder. This model is trained separately and kept fixed during the voice encoder training. Training. Our voice encoder is trained in a self-supervised manner, using the natural co-occurrence of a speaker's speech and facial images in videos. To this end, we use the AVSpeech dataset [13], a large-scale \"in-the-wild\" audiovisual dataset of people speaking. A single frame containing  the speaker's face is extracted from each video clip and fed to VGG-Face to extract the 4096-D feature vector, v f . This serves as the supervision signal for our voice encoder-the feature, v s , of our voice encoder is trained to predict v f . A natural choice for the loss function would be the L 1 distance between the features: v f \u2212 v s 1 . However, we found that the training undergoes slow and unstable progression with this loss alone. To stabilize the training, we introduce additional loss terms, motivated by Castrejon et al. [8]. Specifically, we additionally penalize the difference in the activation of the last layer of the face encoder, f VGG : R 4096 \u2192 R 2622 , i.e., fc8 of VGG-Face, and that of the first layer of the face decoder, f dec : R 4096 \u2192R 1000 , which are pre-trained and fixed during training the voice encoder. We feed both our predictions and the ground truth face features to these layers to calculate the losses. The final loss is:\nL total = f dec (v f ) \u2212 f dec (v s ) 1 + \u03bb 1 v f v f \u2212 vs vs 2 2 +\u03bb 2 L distill (f VGG (v f ), f VGG (v s )) ,(1)\nwhere \u03bb 1 =0.025 and \u03bb 2 =200. \u03bb 1 and \u03bb 2 are tuned such that the gradient magnitude of each term with respect to v s are within a similar scale at an early iteration (we measured at the 1000th iteration). The knowledge distil-\nlation loss L distill (a, b) = \u2212 i p (i) (a) log p (i) (b),\nwhere p (i) (a) = exp(ai/T ) j exp(aj /T ) , is used as an alternative of the cross entropy loss, which encourages the output of a network to approximate the output of another [17]. T =2 is used as recommended by the authors, which makes the activation smoother. We found that enforcing similarity over these additional layers stabilized and sped up the training process, in addition to a slight improvement in the resulting quality. Implementation details. We use up to 6 seconds of audio taken from the beginning of each video clip in AVSpeech. If the video clip is shorter than 6 seconds, we repeat the audio such that it becomes at least 6-seconds long. The audio waveform is resampled at 16 kHz and only a single channel is used.\n\nSpectrograms are computed similarly to Ephrat et al. [13] by taking STFT with a Hann window of 25 mm, the hop length of 10 ms, and 512 FFT frequency bands. Each complex spectrogram S subsequently goes through the power-law compression, resulting sgn(S)|S| 0.3 for real and imaginary independently, where sgn(\u00b7) denotes the signum. We run the CNN-based face detector from Dlib [26], crop the face regions from the frames, and resize them to 224 \u00d7 224 pixels. The VGG-Face features are computed from the resized face images. The computed spectrogram and VGG-Face feature of each segment are collected and used for training. The resulting training and test sets include 1.7 and 0.15 million spectra-face feature pairs, respectively. Our network is implemented in TensorFlow and optimized by ADAM [27] with \u03b2 1 = 0.5, = 10 \u22124 , the learning rate of 0.001 with the exponentially decay rate of 0.95 at every 10,000 iterations, and the batch size of 8 for 3 epochs.\n\n\nResults\n\nWe test our model both qualitatively and quantitatively on the AVSpeech dataset [13] and the VoxCeleb dataset [34]. Our goal is to gain insights and to quantify how-and in which manner-our Speech2Face reconstructions resemble the true face images.\n\nQualitative results on the AVSpeech test set are shown in Fig. 3. For each example, we show the true image of the speaker for reference (unknown to our model), the face reconstructed from the face feature (computed from the true image) by the face decoder (Sec. 3), and the face reconstructed from a 6-seconds audio segment of the person's speech, which is our Speech2Face result. While looking somewhat like average faces, our Speech2Face reconstructions nonetheless capture rich physical information about the speaker, such as their age, gender, and ethnicity. The predicted images also capture additional properties like the shape of the face or head (e.g., elongated vs. round), which we often find consistent with the true appearance of the    Figure 5. Craniofacial features. We measure the correlation between craniofacial features extracted from (a) face decoder reconstructions from the original image (F2F), and (b) features extracted from our corresponding Speech2Face reconstructions (S2F); the features are computed from detected facial landmarks, as described in [30]. The table reports Pearson correlation coefficient and statistical significance computed over 1,000 test images for each feature. Random baseline is computed for \"Nasal index\" by comparing random pairs of F2F reconstruction (a) and S2F reconstruction (b).\n\nspeaker; see the last two rows in Fig. 3 for instance.\n\n\nFacial Features Evaluation\n\nWe quantify how well different facial attributes are being captured in our Speech2Face reconstructions and test different aspects of our model. Demographic attributes. We use Face++ [28], a leading commercial service for computing facial attributes. Specifically, we evaluate and compare age, gender, and ethnicity, by running the Face++ classifiers on the original images and our Speech2Face reconstructions. The Face++ classifiers return either \"male\" or \"female\" for gender, a continuous number for age, and one of the four values, \"Asian\", \"black\", \"India\", or \"white\", for ethnicity. 1 Fig. 4(a) shows confusion matrices for each of the attributes, comparing the attributes inferred from the original images with those inferred from our Speech2Face reconstructions (S2F). See the supplementary material for similar evaluations of our faced-decoder reconstructions from the images (F2F). As can be seen, for age and gender the classification results are highly correlated. For gender, there is  Table 2. Feature similarity. We measure the similarity between our features predicted from speech and the corresponding face features computed on the true images of the speakers. We report average cosine, L2 and L1 distances over 5000 random samples from the AVSpeech test set, using 3-and 6-second audio segments. Figure 6. The effect of input audio duration. We compare our face reconstructions when using 3-second (middle row) and 6second (bottom row) input voice segments at test time (in both cases we use the same model, trained on 6-second segments). The top row shows representative frames from the videos for reference. With longer speech duration the reconstructed faces capture the facial attributes better.\n\nan agreement of 94% in male/female labels between the true images and our reconstructions from speech. For ethnicity, there is a good correlation on the \"white\" and \"Asian\", but we observe less agreement on \"India\" and \"black\". We believe this is because those classes have a smaller representation in the data (see statistics we computed on AVSpeech in Fig. 4(b)). The performance can potentially be improved by leveraging the statistics to balance the training data for the voice encoder model, which we leave for future work. Craniofacial attributes. We evaluated craniofacial measurements commonly used in the literature for capturing ratios and distances in the face [30]. For each such measurement, we computed the correlation between F2F (Fig. 5(a)), and our corresponding S2F reconstructions ( Fig. 5(b)). Face landmarks were computed using the DEST library [1]. Note that this evaluation is made possible because we are working with normalized faces (neutral expression, fronto-parallel), thus differences between the facial landmarks' positions reflect geometric craniofacial changes. Fig. 5(c) shows the Pearson correlation coefficient for several measures, computed over 1,000 random samples from the AVSpeech test set. As can be seen, there is statistically significant (i.e., p < 0.001) positive correlation for several measurements. In particular, the highest correlation is measured for nasal index (0.38) and nose width (0.35), features indicative of nose structure that may affect a speaker's voice.\n\nFeature similarity. We test how well a person can be recognized from on the face features predicted from speech. We first directly measure the cosine distance between our predicted features and the true ones obtained from the original  Table 3. S2F\u2192Face retrieval performance. We measure retrieval performance by recall at K (R@K, in %), which indicates the chance of retrieving the true image of a speaker within the top-K results. We used a database of 5,000 images for this experiment; see Fig. 7 for qualitative results. The higher the better. Random chance is presented as a baseline. face image of the speaker. Table 2 shows the average error over 5,000 test images, for the predictions using 3s and 6s audio segments. The use of longer audio clips exhibits consistent improvement in all error metrics; this further evidences the qualitative improvement we observe in Fig. 6. We further evaluated how accurately we can retrieve the true speaker from a database of face images. To do so, we take the speech of a person to predict the feature using our Speech2Face model, and query it by computing its distances to the face features of all face images in the database. We report the retrieval performance by measuring the recall at K, i.e., the percentage of time the true face is retrieved within the rank of K. Table 3 shows the computed recalls for varying configurations. In all cases, the cross-modal retrieval using our model shows a significant performance gain compared to the random chance. It also shows that a longer duration of the  input speech noticeably improves the performance. In Fig. 7, we show several examples of 5 nearest faces such retrieved, which demonstrate the consistent facial characteristics that are being captured by our predicted face features. t-SNE visualization for learned feature analysis. To gain more insights on our predicted features, we present 2-D t-SNE plot [48] of the features in the SM.\n\n\nAblation Studies\n\nThe effect of audio duration and batch normalization. We tested the effect of the duration of the input audio during both the train and test stages. Specifically, we trained two models with 3-and 6-second speech segments. We found that during the training time, the audio duration has an only subtle effect on the convergence speed, without much effect on the overall loss and quality of the reconstructions (Fig. 8). However, we found that feeding longer speech as input at test time leads to improvement in reconstruction quality, that is, reconstructed faces capture the personal attributes better, regardless of which of the two models are used. Fig. 6 shows several qualitative comparisons, which are also consistent with the quantitative evaluations in Tables 2 and 3. Fig. 8 also shows the training curves w/ and w/o Batch Normalization (BN). As can be seen, without BN the reconstructed faces converge to an average face. With BN the results contain much richer facial information. Additional observations and limitations. In Fig. 9, we infer faces from different speech segments of the same person, taken from different parts within the same video, and from a different video, in order to test the stability of our Speech2Face reconstruction. The reconstructed face images are consistent within and between the videos. We show more such results in the SM.\n\nTo qualitatively test the effect of language and accent, we probe the model with an Asian male example, saying the same sentence in English and Chinese ( Fig. 10(a)). While having the same reconstructed face in both cases would be ideal, the model inferred different faces based on the spoken language. However, in other examples, e.g., Fig. 10(b), the model was able to successfully factor out the language, reconstructing a face with Asian features even though the girl was speaking in English with no apparent accent (the audio is available in the SM). In general, we observed mixed behavior and a more thorough examination is needed to determine to which extent the model relies on language.\n\nMore generally, the ability of speech to capture the latent attributes, such as age, gender, and ethnicity, depends on several factors such as accent, spoken language, or voice pitch. Clearly, in some cases, these vocal attributes would not match the person's appearance. Several such typical speech-face mismatch examples are shown in Fig. 11. \n\n\nSpeech2cartoon\n\nOur face images reconstructed from speech may be used for generating personalized cartoons of speakers from their voices, as shown in Fig. 12. We use Gboard, the keyboard app available on Android phones, which is also capable of analyzing a selfie image to produce a cartoon-like version of the face [31]. As can be seen, our reconstructions capture the facial attributes well enough for the app to work. Such cartoon re-rendering of the face may be useful as a visual representation of a person during a phone or a video-conferencing call, when the person identity is unknown or the person prefers not to share his/her picture. Our reconstructed faces may also be directly, to assign faces to machine-generated voices used in home devices and virtual assistants.\n\n\nConclusion\n\nWe have presented a novel study of face reconstruction directly from the audio recording of a person speaking. We address this problem by learning to align the feature space of speech with that of a pre-trained face decoder using millions of natural videos of people speaking. We have demonstrated that our method can predict plausible faces with the facial attributes consistent with those of real images. By reconstructing faces directly from this cross-modal feature space, we validate visually the existence of cross-modal biometric information postulated in previous studies [25,33]. We believe that generating faces, as opposed to predicting specific attributes, may provide a more comprehensive view of voiceface correlations and can open up new research opportunities and applications.\n\nFigure 3 .\n3Qualitative results on the AVSpeech test set. For every example (triplet of images) we show: (left) the original image, i.e., a representative frame from the video cropped around the person's face; (middle) the frontalized, lighting-normalized face decoder reconstruction from the VGG-Face feature extracted from the original image; (right) our Speech2Face reconstruction, computed by decoding the predicted VGG-Face feature from the audio. In this figure, we highlight successful results in our method. Some failure cases are shown in Fig. 11, and more results (including the input audio for all the examples) can be found in the SM. Age (a) Confusion matrices for the attributes (b) AVSpeech dataset statistics\n\nFigure 4 .\n4Facial attribute evaluation. (a) confusion matrices (with row-wise normalization) comparing the classification results on our Speech2Face image reconstructions and those obtained from the original images for gender, age, and ethnicity; the stronger diagonal tendency the better performance. Ethnicity performance in (a) appears to be biased due to uneven distribution of the training set shown in (b).\n\n\nmarked on our corresponding reconstructions from speech\n\nFace\n\n\nFigure 7 .\n7S2F\u2192Face retrieval examples. We query a database of 5,000 face images by comparing our Speech2Face prediction of input audio to all VGG-Face face features in the database (computed directly from the original faces). For each query, we show the top-5 retrieved samples. The last row is an example where the true face was not among the top results, but still shows visually close results to the query. More results are available in the SM.\n\nFigure 8 .\n8Training convergence patterns. BN denotes batch normalization. The red and green curves are obtained by using 3-and 6-second audio clips as input during training, respectively (dashed line: training loss; solid line: validation loss). The face thumbnails show reconstructions from models trained with and without BN.\n\nFigure 9 .Figure 10 .\n910Temporal and cross-video consistency. Face reconstruction from different speech segments of the same person taken from different parts within (a) the same or from (b) a different video. An Asian male speaking in English (left) & Chinese (right) An Asian girl speaking in English The effect of language. We notice mixed performance in terms of the ability of the model to handle languages and accents. (a) A sample case of language-dependent face reconstructions. (b) A sample case that successfully factors out the language.\n\nFigure 11 .\n11Example failure cases. (a) High-pitch male voice, e.g., of kids, may lead to a face image with female features. (b) Spoken language does not match ethnicity. (c-d) Age mismatches.\n\nTable 1 .\n1Voice encoder architecture. The input spectrogram dimensions are 598 \u00d7 257 (time \u00d7 frequency) for a 6-second audio segment (which can be arbitrarily long), with the two input channels in the table corresponding to the spectrogram's real and imaginary components.\nWe directly refer to the Face++ ethnicity labels, which are not our terminology. We would recommend using African Americans, American Indian and Caucasian.\nAcknowledgment The authors would like to thank Suwon Shon, James Glass, Forrester Cole and Dilip Krishnan for helpful discussion. T.-H. Oh and C. Kim were supported by QCRI-CSAIL Computer Science Research Program at MIT.Figure 12. Speech-to-cartoon. Our reconstructed faces from audio (b) can be re-rendered as cartoons (c) using existing tools, such as the personalized emoji app available in Gboard, the keyboard app in Android phones[31]. (a) The true images of the person are shown for reference.\nOne Millisecond Deformable Shape Tracking Library (DEST). One Millisecond Deformable Shape Tracking Library (DEST). https://github.com/cheind/dest. 6\n\nEmotion recognition in speech using cross-modal transfer in the wild. S Albanie, A Nagrani, A Vedaldi, A Zisserman, ACM Multimedia Conference (MM). S. Albanie, A. Nagrani, A. Vedaldi, and A. Zisserman. Emo- tion recognition in speech using cross-modal transfer in the wild. In ACM Multimedia Conference (MM), 2018. 3\n\nDeep canonical correlation analysis. G Andrew, R Arora, J Bilmes, K Livescu, International Conference on Machine Learning (ICML). G. Andrew, R. Arora, J. Bilmes, and K. Livescu. Deep canon- ical correlation analysis. In International Conference on Machine Learning (ICML), 2013. 3\n\nLook, listen and learn. R Arandjelovic, A Zisserman, IEEE International Conference on Computer Vision (ICCV). R. Arandjelovic and A. Zisserman. Look, listen and learn. In IEEE International Conference on Computer Vision (ICCV), 2017. 2\n\nObjects that sound. R Arandjelovic, A Zisserman, European Conference on Computer Vision (ECCV). SpringerR. Arandjelovic and A. Zisserman. Objects that sound. In European Conference on Computer Vision (ECCV), Springer, 2018. 2\n\nSoundnet: Learning sound representations from unlabeled video. Y Aytar, C Vondrick, A Torralba, Advances in Neural Information Processing Systems (NIPS). Y. Aytar, C. Vondrick, and A. Torralba. Soundnet: Learning sound representations from unlabeled video. In Advances in Neural Information Processing Systems (NIPS), 2016. 2\n\nSpeaker age estimation and gender detection based on supervised non-negative matrix factorization. M H Bahari, H Van Hamme, IEEE Workshop on Biometric Measurements and Systems for Security and Medical Applications. M. H. Bahari and H. Van Hamme. Speaker age estimation and gender detection based on supervised non-negative matrix factorization. In IEEE Workshop on Biometric Measurements and Systems for Security and Medical Applications, 2011. 2\n\nLearning aligned cross-modal representations from weakly aligned data. L Castrejon, Y Aytar, C Vondrick, H Pirsiavash, A Torralba, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 25L. Castrejon, Y. Aytar, C. Vondrick, H. Pirsiavash, and A. Tor- ralba. Learning aligned cross-modal representations from weakly aligned data. In IEEE Conference on Computer Vi- sion and Pattern Recognition (CVPR), 2016. 2, 5\n\nLip reading sentences in the wild. J S Chung, A W Senior, O Vinyals, A Zisserman, 2017. 3IEEE Conference on Computer Vision and Pattern Recognition. J. S. Chung, A. W. Senior, O. Vinyals, and A. Zisserman. Lip reading sentences in the wild. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 3\n\nSynthesizing normalized faces from facial identity features. F Cole, D Belanger, D Krishnan, A Sarna, I Mosseri, W T Freeman, IEEE Conference on Computer Vision and Pattern Recognition. 23F. Cole, D. Belanger, D. Krishnan, A. Sarna, I. Mosseri, and W. T. Freeman. Synthesizing normalized faces from facial identity features. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017. 1, 2, 3\n\nMinimizing disagreement for self-supervised classification. V R De Sa, Proceedings of the 1993 Connectionist Models Summer School. the 1993 Connectionist Models Summer SchoolPsychology Press300V. R. de Sa. Minimizing disagreement for self-supervised clas- sification. In Proceedings of the 1993 Connectionist Models Summer School, page 300. Psychology Press, 1994. 2\n\nThe speech chain. P B Denes, P Denes, E Pinson, MacmillanP. B. Denes, P. Denes, and E. Pinson. The speech chain. Macmillan, 1993. 1\n\nLooking to listen at the cocktail party: a speaker-independent audio-visual model for speech separation. A Ephrat, I Mosseri, O Lang, T Dekel, K Wilson, A Hassidim, W T Freeman, M Rubinstein, 112:1-112:11ACM Transactions on Graphics (SIG-GRAPH). 374A. Ephrat, I. Mosseri, O. Lang, T. Dekel, K. Wilson, A. Has- sidim, W. T. Freeman, and M. Rubinstein. Looking to listen at the cocktail party: a speaker-independent audio-visual model for speech separation. ACM Transactions on Graphics (SIG- GRAPH), 37(4):112:1-112:11, 2018. 1, 2, 3, 5\n\nAutomatic speaker age and gender recognition in the car for tailoring dialog and mobile services. M Feld, F Burkhardt, C M\u00fcller, InterspeechM. Feld, F. Burkhardt, and C. M\u00fcller. Automatic speaker age and gender recognition in the car for tailoring dialog and mobile services. In Interspeech, 2010. 2\n\nTracking the active speaker based on a joint audio-visual observation model. I D Gebru, S Ba, G Evangelidis, R Horaud, IEEE International Conference on Computer Vision Workshops. I. D. Gebru, S. Ba, G. Evangelidis, and R. Horaud. Tracking the active speaker based on a joint audio-visual observation model. In IEEE International Conference on Computer Vision Workshops, 2015. 3\n\nSpeaker height estimation from speech: Fusing spectral regression and statistical acoustic models. J H Hansen, K Williams, H Bo\u0159il, The Journal of the Acoustical Society of America. 1382J. H. Hansen, K. Williams, and H. Bo\u0159il. Speaker height esti- mation from speech: Fusing spectral regression and statistical acoustic models. The Journal of the Acoustical Society of America, 138(2):1052-1067, 2015. 2\n\nDistilling the knowledge in a neural network. G Hinton, O Vinyals, J Dean, abs/1503.02531CoRRG. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge in a neural network. CoRR, abs/1503.02531, 2015. 5\n\nPutting a face to the voice: Fusing audio and visual signals across a video to determine speakers. K Hoover, S Chaudhuri, C Pantofaru, M Slaney, I Sturdy, abs/1706.00079CoRRK. Hoover, S. Chaudhuri, C. Pantofaru, M. Slaney, and I. Sturdy. Putting a face to the voice: Fusing audio and vi- sual signals across a video to determine speakers. CoRR, abs/1706.00079, 2017. 3\n\nFace-voice matching using cross-modal embeddings. S Horiguchi, N Kanda, K Nagamatsu, ACM Multimedia Conference (MM). S. Horiguchi, N. Kanda, and K. Nagamatsu. Face-voice matching using cross-modal embeddings. In ACM Multi- media Conference (MM), 2018. 3\n\nUnsupervised learning of disentangled and interpretable representations from sequential data. W.-N Hsu, Y Zhang, J Glass, 2017. 3Advances in Neural Information Processing Systems (NIPS. W.-N. Hsu, Y. Zhang, and J. Glass. Unsupervised learning of disentangled and interpretable representations from sequential data. In Advances in Neural Information Processing Systems (NIPS), 2017. 3\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, International Conference on Machine Learning (ICML). S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning (ICML), 2015. 3\n\nThe Discovery of perceptual structure from visual co-occurrences in space and time. P J Isola, Massachusetts Institute of TechnologyPhD thesisP. J. Isola. The Discovery of perceptual structure from visual co-occurrences in space and time. PhD thesis, Massachusetts Institute of Technology, 2015. 2\n\nPutting the face to the voice': Matching identity across modality. M Kamachi, H Hill, K Lander, E Vatikiotis-Bateson, Current Biology. 1319M. Kamachi, H. Hill, K. Lander, and E. Vatikiotis-Bateson. Putting the face to the voice': Matching identity across modal- ity. Current Biology, 13(19):1709-1714, 2003. 1\n\nAudiodriven facial animation by joint end-to-end learning of pose and emotion. T Karras, T Aila, S Laine, A Herva, J Lehtinen, ACM Transactions on Graphics (SIGGRAPH). 36494T. Karras, T. Aila, S. Laine, A. Herva, and J. Lehtinen. Audio- driven facial animation by joint end-to-end learning of pose and emotion. ACM Transactions on Graphics (SIGGRAPH), 36(4):94, 2017. 3\n\nOn learning associations of faces and voices. C Kim, H V Shin, T.-H Oh, A Kaspar, M Elgharib, W Matusik, Asian Conference on Computer Vision (ACCV). Springer3C. Kim, H. V. Shin, T.-H. Oh, A. Kaspar, M. Elgharib, and W. Matusik. On learning associations of faces and voices. In Asian Conference on Computer Vision (ACCV), Springer, 2018. 3, 8\n\nDlib-ml: A machine learning toolkit. D E King, Journal of Machine Learning Research (JMLR). 105D. E. King. Dlib-ml: A machine learning toolkit. Journal of Machine Learning Research (JMLR), 10:1755-1758, 2009. 5\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, International Conference for Learning Representations (ICLR). D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In International Conference for Learning Rep- resentations (ICLR), 2015. 5\n\nLeading face-based identity verification service. Leading face-based identity verification service. Face++. https://www.faceplusplus.com/attributes/.\n\nCoupled generative adversarial networks. M.-Y Liu, O Tuzel, Advances in Neural Information Processing Systems (NIPS). M.-Y. Liu and O. Tuzel. Coupled generative adversarial networks. In Advances in Neural Information Processing Systems (NIPS), 2016. 3\n\n. M Merler, N Ratha, R S Feris, J R Smith, Diversity in faces. CoRR, abs/1901.10436M. Merler, N. Ratha, R. S. Feris, and J. R. Smith. Diversity in faces. CoRR, abs/1901.10436, 2019. 6\n\nMini stickers for Gboard. Mini stickers for Gboard. Google Inc. https://goo.gl/ hu5DsR. 8\n\nLearnable pins: Cross-modal embeddings for person identity. A Nagrani, S Albanie, A Zisserman, European Conference on Computer Vision (ECCV). SpringerA. Nagrani, S. Albanie, and A. Zisserman. Learnable pins: Cross-modal embeddings for person identity. In European Conference on Computer Vision (ECCV), Springer, 2018. 3\n\nSeeing voices and hearing faces: Cross-modal biometric matching. A Nagrani, S Albanie, A Zisserman, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 3A. Nagrani, S. Albanie, and A. Zisserman. Seeing voices and hearing faces: Cross-modal biometric matching. In IEEE Con- ference on Computer Vision and Pattern Recognition (CVPR), 2018. 3, 8\n\nVoxceleb: a large-scale speaker identification dataset. A Nagrani, J S Chung, A Zisserman, A. Nagrani, J. S. Chung, and A. Zisserman. Voxceleb: a large-scale speaker identification dataset. Interspeech, 2017. 5\n\nMultimodal deep learning. J Ngiam, A Khosla, M Kim, J Nam, H Lee, A Y Ng, International Conference on Machine Learning (ICML). J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng. Multimodal deep learning. In International Conference on Machine Learning (ICML), 2011. 3\n\nAudio-visual scene analysis with self-supervised multisensory features. A Owens, A A Efros, European Conference on Computer Vision (ECCV). SpringerA. Owens and A. A. Efros. Audio-visual scene analysis with self-supervised multisensory features. In European Confer- ence on Computer Vision (ECCV), Springer, 2018. 2\n\nVisually indicated sounds. A Owens, P Isola, J H Mcdermott, A Torralba, E H Adelson, W T Freeman, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). A. Owens, P. Isola, J. H. McDermott, A. Torralba, E. H. Adel- son, and W. T. Freeman. Visually indicated sounds. In IEEE Conference on Computer Vision and Pattern Recog- nition (CVPR), 2016. 2\n\nLearning sight from sound: Ambient sound provides supervision for visual learning. A Owens, J Wu, J H Mcdermott, W T Freeman, A Torralba, International Journal of Computer Vision (IJCV). 12610A. Owens, J. Wu, J. H. McDermott, W. T. Freeman, and A. Torralba. Learning sight from sound: Ambient sound pro- vides supervision for visual learning. International Journal of Computer Vision (IJCV), 126(10):1120-1137, 2018. 2\n\nDeep face recognition. O M Parkhi, A Vedaldi, A Zisserman, British Machine Vision Conference (BMVC). 23O. M. Parkhi, A. Vedaldi, and A. Zisserman. Deep face recognition. In British Machine Vision Conference (BMVC), 2015. 1, 2, 3\n\nSpeech-driven expressive talking lips with conditional sequential generative adversarial networks. N Sadoughi, C Busso, abs/1806.00154CoRRN. Sadoughi and C. Busso. Speech-driven expressive talk- ing lips with conditional sequential generative adversarial networks. CoRR, abs/1806.00154, 2018. 3\n\nLearning to localize sound source in visual scenes. A Senocak, T.-H Oh, J Kim, M.-H Yang, I S Kweon, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). A. Senocak, T.-H. Oh, J. Kim, M.-H. Yang, and I. S. Kweon. Learning to localize sound source in visual scenes. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 2\n\nAudio to body dynamics. E Shlizerman, L Dery, H Schoen, I Kemelmacher-Shlizerman, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). E. Shlizerman, L. Dery, H. Schoen, and I. Kemelmacher- Shlizerman. Audio to body dynamics. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018. 3\n\nNoise-tolerant audio-visual online person verification using an attention-based neural network fusion. S Shon, T.-H Oh, J Glass, abs/1811.10813CoRRS. Shon, T.-H. Oh, and J. Glass. Noise-tolerant audio-visual online person verification using an attention-based neural network fusion. CoRR, abs/1811.10813, 2018. 3\n\nMatching novel face and voice identity using static and dynamic facial images. H M Smith, A K Dunn, T Baguley, P C Stacey, Perception, & Psychophysics. 783AttentionH. M. Smith, A. K. Dunn, T. Baguley, and P. C. Stacey. Matching novel face and voice identity using static and dy- namic facial images. Attention, Perception, & Psychophysics, 78(3):868-879, 2016. 1\n\nSuggesting sounds for images from video collections. M Sol\u00e8r, J C Bazin, O Wang, A Krause, A Sorkine-Hornung, European Conference on Computer Vision Workshops. M. Sol\u00e8r, J. C. Bazin, O. Wang, A. Krause, and A. Sorkine- Hornung. Suggesting sounds for images from video collec- tions. In European Conference on Computer Vision Work- shops, 2016. 2\n\nSynthesizing obama: learning lip sync from audio. S Suwajanakorn, S M Seitz, I Kemelmacher-Shlizerman, ACM Transactions on Graphics (SIGGRAPH). 36495S. Suwajanakorn, S. M. Seitz, and I. Kemelmacher- Shlizerman. Synthesizing obama: learning lip sync from au- dio. ACM Transactions on Graphics (SIGGRAPH), 36(4):95, 2017. 3\n\nA deep learning approach for generalized speech animation. S L Taylor, T Kim, Y Yue, M Mahler, J Krahe, A G Rodriguez, J K Hodgins, I A Matthews, 93:1-93:11ACM Transactions on Graphics (SIGGRAPH). 364S. L. Taylor, T. Kim, Y. Yue, M. Mahler, J. Krahe, A. G. Ro- driguez, J. K. Hodgins, and I. A. Matthews. A deep learning approach for generalized speech animation. ACM Transac- tions on Graphics (SIGGRAPH), 36(4):93:1-93:11, 2017. 3\n\nVisualizing data using t-sne. L Van Der Maaten, G Hinton, Journal of Machine Learning Research. 97JMLRL. van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of Machine Learning Research (JMLR), 9(Nov):2579- 2605, 2008. 7\n\nDisjoint mapping network for cross-modal matching of voices and faces. Y Wen, M A Ismail, W Liu, B Raj, R Singh, International Conference on Learning Representations (ICLR). 23Y. Wen, M. A. Ismail, W. Liu, B. Raj, and R. Singh. Disjoint mapping network for cross-modal matching of voices and faces. In International Conference on Learning Representa- tions (ICLR), 2019. 2, 3\n\nX2face: A network for controlling face generation using images, audio, and pose codes. O Wiles, A S Koepke, A Zisserman, European Conference on Computer Vision (ECCV). SpringerO. Wiles, A. S. Koepke, and A. Zisserman. X2face: A network for controlling face generation using images, audio, and pose codes. In European Conference on Computer Vision (ECCV), Springer, 2018. 3\n\nAttribute2image: Conditional image generation from visual attributes. X Yan, J Yang, K Sohn, H Lee, European Conference on Computer Vision (ECCV). Springer23X. Yan, J. Yang, K. Sohn, and H. Lee. Attribute2image: Con- ditional image generation from visual attributes. In European Conference on Computer Vision (ECCV), Springer, 2016. 2, 3\n\nAge estimation in short speech utterances based on lstm recurrent neural networks. R Zazo, P S Nidadavolu, N Chen, J Gonzalez-Rodriguez, N Dehak, IEEE Access. 62R. Zazo, P. S. Nidadavolu, N. Chen, J. Gonzalez-Rodriguez, and N. Dehak. Age estimation in short speech utterances based on lstm recurrent neural networks. IEEE Access, 6:22524-22530, 2018. 2\n\nThe sound of pixels. H Zhao, C Gan, A Rouditchenko, C Vondrick, J H Mc-Dermott, A Torralba, European Conference on Computer Vision (ECCV). SpringerH. Zhao, C. Gan, A. Rouditchenko, C. Vondrick, J. H. Mc- Dermott, and A. Torralba. The sound of pixels. In European Conference on Computer Vision (ECCV), Springer, 2018. 2\n", "annotations": {"author": "[{\"end\":73,\"start\":49},{\"end\":97,\"start\":74},{\"end\":122,\"start\":98},{\"end\":149,\"start\":123},{\"end\":180,\"start\":150},{\"end\":212,\"start\":181},{\"end\":242,\"start\":213}]", "publisher": null, "author_last_name": "[{\"end\":60,\"start\":58},{\"end\":84,\"start\":79},{\"end\":109,\"start\":106},{\"end\":136,\"start\":129},{\"end\":167,\"start\":160},{\"end\":199,\"start\":189},{\"end\":229,\"start\":222}]", "author_first_name": "[{\"end\":57,\"start\":49},{\"end\":78,\"start\":74},{\"end\":105,\"start\":98},{\"end\":128,\"start\":123},{\"end\":157,\"start\":150},{\"end\":159,\"start\":158},{\"end\":188,\"start\":181},{\"end\":221,\"start\":213}]", "author_affiliation": "[{\"end\":72,\"start\":62},{\"end\":96,\"start\":86},{\"end\":121,\"start\":111},{\"end\":148,\"start\":138},{\"end\":179,\"start\":169},{\"end\":211,\"start\":201},{\"end\":241,\"start\":231}]", "title": "[{\"end\":46,\"start\":1},{\"end\":288,\"start\":243}]", "venue": null, "abstract": "[{\"end\":1530,\"start\":313}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b22\"},\"end\":1705,\"start\":1701},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":1708,\"start\":1705},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2259,\"start\":2255},{\"end\":2693,\"start\":2685},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3732,\"start\":3728},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3867,\"start\":3863},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3921,\"start\":3917},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4409,\"start\":4405},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":4641,\"start\":4637},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4791,\"start\":4787},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":5078,\"start\":5074},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5081,\"start\":5078},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5084,\"start\":5081},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5086,\"start\":5084},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":5089,\"start\":5086},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":5278,\"start\":5274},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":5428,\"start\":5424},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8004,\"start\":8000},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8032,\"start\":8028},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8163,\"start\":8160},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8181,\"start\":8178},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8206,\"start\":8203},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8742,\"start\":8738},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8745,\"start\":8742},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8748,\"start\":8745},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8794,\"start\":8790},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8796,\"start\":8794},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8799,\"start\":8796},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":8832,\"start\":8828},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8835,\"start\":8832},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9308,\"start\":9304},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9311,\"start\":9308},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9314,\"start\":9311},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9336,\"start\":9332},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9339,\"start\":9336},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":9390,\"start\":9386},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9504,\"start\":9500},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9506,\"start\":9504},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9577,\"start\":9573},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9580,\"start\":9577},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9640,\"start\":9636},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9761,\"start\":9757},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9763,\"start\":9761},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9796,\"start\":9793},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":10441,\"start\":10437},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":10444,\"start\":10441},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":10447,\"start\":10444},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10450,\"start\":10447},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10857,\"start\":10853},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":10926,\"start\":10922},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":11126,\"start\":11122},{\"end\":11149,\"start\":11131},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":11969,\"start\":11965},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12224,\"start\":12220},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12872,\"start\":12868},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":13295,\"start\":13291},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13642,\"start\":13638},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14253,\"start\":14249},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":14740,\"start\":14736},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15377,\"start\":15374},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":16388,\"start\":16384},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":17001,\"start\":16997},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":17324,\"start\":17320},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17741,\"start\":17737},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":17998,\"start\":17994},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":18028,\"start\":18024},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":19244,\"start\":19240},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19773,\"start\":19769},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21982,\"start\":21978},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":22175,\"start\":22172},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":24736,\"start\":24732},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":27515,\"start\":27511},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":28573,\"start\":28569},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":28576,\"start\":28573}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29508,\"start\":28783},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29923,\"start\":29509},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29981,\"start\":29924},{\"attributes\":{\"id\":\"fig_3\"},\"end\":29988,\"start\":29982},{\"attributes\":{\"id\":\"fig_4\"},\"end\":30439,\"start\":29989},{\"attributes\":{\"id\":\"fig_6\"},\"end\":30769,\"start\":30440},{\"attributes\":{\"id\":\"fig_7\"},\"end\":31320,\"start\":30770},{\"attributes\":{\"id\":\"fig_8\"},\"end\":31515,\"start\":31321},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":31790,\"start\":31516}]", "paragraph": "[{\"end\":2260,\"start\":1546},{\"end\":3224,\"start\":2262},{\"end\":3352,\"start\":3226},{\"end\":4792,\"start\":3354},{\"end\":4901,\"start\":4794},{\"end\":5899,\"start\":4903},{\"end\":6643,\"start\":5901},{\"end\":7205,\"start\":6645},{\"end\":7742,\"start\":7207},{\"end\":8033,\"start\":7759},{\"end\":8950,\"start\":8035},{\"end\":10016,\"start\":8952},{\"end\":11221,\"start\":10164},{\"end\":11717,\"start\":11249},{\"end\":12225,\"start\":11719},{\"end\":14192,\"start\":12227},{\"end\":15803,\"start\":14194},{\"end\":16147,\"start\":15919},{\"end\":16942,\"start\":16208},{\"end\":17902,\"start\":16944},{\"end\":18161,\"start\":17914},{\"end\":19500,\"start\":18163},{\"end\":19556,\"start\":19502},{\"end\":21304,\"start\":19587},{\"end\":22823,\"start\":21306},{\"end\":24763,\"start\":22825},{\"end\":26148,\"start\":24784},{\"end\":26845,\"start\":26150},{\"end\":27192,\"start\":26847},{\"end\":27974,\"start\":27211},{\"end\":28782,\"start\":27989}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10163,\"start\":10017},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15918,\"start\":15804},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16207,\"start\":16148}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":9206,\"start\":9120},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":13224,\"start\":13217},{\"end\":20593,\"start\":20586},{\"end\":23068,\"start\":23061},{\"end\":23449,\"start\":23442},{\"end\":24149,\"start\":24142},{\"end\":25557,\"start\":25543}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1544,\"start\":1532},{\"attributes\":{\"n\":\"2.\"},\"end\":7757,\"start\":7745},{\"attributes\":{\"n\":\"3.\"},\"end\":11247,\"start\":11224},{\"attributes\":{\"n\":\"4.\"},\"end\":17912,\"start\":17905},{\"attributes\":{\"n\":\"4.1.\"},\"end\":19585,\"start\":19559},{\"attributes\":{\"n\":\"4.2.\"},\"end\":24782,\"start\":24766},{\"attributes\":{\"n\":\"4.3.\"},\"end\":27209,\"start\":27195},{\"attributes\":{\"n\":\"5.\"},\"end\":27987,\"start\":27977},{\"end\":28794,\"start\":28784},{\"end\":29520,\"start\":29510},{\"end\":29987,\"start\":29983},{\"end\":30000,\"start\":29990},{\"end\":30451,\"start\":30441},{\"end\":30792,\"start\":30771},{\"end\":31333,\"start\":31322},{\"end\":31526,\"start\":31517}]", "table": null, "figure_caption": "[{\"end\":29508,\"start\":28796},{\"end\":29923,\"start\":29522},{\"end\":29981,\"start\":29926},{\"end\":30439,\"start\":30002},{\"end\":30769,\"start\":30453},{\"end\":31320,\"start\":30796},{\"end\":31515,\"start\":31336},{\"end\":31790,\"start\":31528}]", "figure_ref": "[{\"end\":2564,\"start\":2558},{\"end\":4119,\"start\":4111},{\"end\":12274,\"start\":12268},{\"end\":13160,\"start\":13152},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":18227,\"start\":18221},{\"end\":18920,\"start\":18912},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":19542,\"start\":19536},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20187,\"start\":20178},{\"end\":20909,\"start\":20901},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21669,\"start\":21660},{\"end\":22061,\"start\":22051},{\"end\":22117,\"start\":22108},{\"end\":22410,\"start\":22401},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":23324,\"start\":23318},{\"end\":23705,\"start\":23699},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":24433,\"start\":24427},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":25200,\"start\":25192},{\"end\":25440,\"start\":25434},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":25565,\"start\":25559},{\"end\":25824,\"start\":25818},{\"end\":26314,\"start\":26304},{\"end\":26497,\"start\":26487},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":27190,\"start\":27183},{\"end\":27352,\"start\":27345}]", "bib_author_first_name": "[{\"end\":32670,\"start\":32669},{\"end\":32681,\"start\":32680},{\"end\":32692,\"start\":32691},{\"end\":32703,\"start\":32702},{\"end\":32955,\"start\":32954},{\"end\":32965,\"start\":32964},{\"end\":32974,\"start\":32973},{\"end\":32984,\"start\":32983},{\"end\":33224,\"start\":33223},{\"end\":33240,\"start\":33239},{\"end\":33457,\"start\":33456},{\"end\":33473,\"start\":33472},{\"end\":33727,\"start\":33726},{\"end\":33736,\"start\":33735},{\"end\":33748,\"start\":33747},{\"end\":34090,\"start\":34089},{\"end\":34092,\"start\":34091},{\"end\":34102,\"start\":34101},{\"end\":34510,\"start\":34509},{\"end\":34523,\"start\":34522},{\"end\":34532,\"start\":34531},{\"end\":34544,\"start\":34543},{\"end\":34558,\"start\":34557},{\"end\":34900,\"start\":34899},{\"end\":34902,\"start\":34901},{\"end\":34911,\"start\":34910},{\"end\":34913,\"start\":34912},{\"end\":34923,\"start\":34922},{\"end\":34934,\"start\":34933},{\"end\":35246,\"start\":35245},{\"end\":35254,\"start\":35253},{\"end\":35266,\"start\":35265},{\"end\":35278,\"start\":35277},{\"end\":35287,\"start\":35286},{\"end\":35298,\"start\":35297},{\"end\":35300,\"start\":35299},{\"end\":35655,\"start\":35654},{\"end\":35657,\"start\":35656},{\"end\":35981,\"start\":35980},{\"end\":35983,\"start\":35982},{\"end\":35992,\"start\":35991},{\"end\":36001,\"start\":36000},{\"end\":36201,\"start\":36200},{\"end\":36211,\"start\":36210},{\"end\":36222,\"start\":36221},{\"end\":36230,\"start\":36229},{\"end\":36239,\"start\":36238},{\"end\":36249,\"start\":36248},{\"end\":36261,\"start\":36260},{\"end\":36263,\"start\":36262},{\"end\":36274,\"start\":36273},{\"end\":36731,\"start\":36730},{\"end\":36739,\"start\":36738},{\"end\":36752,\"start\":36751},{\"end\":37011,\"start\":37010},{\"end\":37013,\"start\":37012},{\"end\":37022,\"start\":37021},{\"end\":37028,\"start\":37027},{\"end\":37043,\"start\":37042},{\"end\":37412,\"start\":37411},{\"end\":37414,\"start\":37413},{\"end\":37424,\"start\":37423},{\"end\":37436,\"start\":37435},{\"end\":37764,\"start\":37763},{\"end\":37774,\"start\":37773},{\"end\":37785,\"start\":37784},{\"end\":38023,\"start\":38022},{\"end\":38033,\"start\":38032},{\"end\":38046,\"start\":38045},{\"end\":38059,\"start\":38058},{\"end\":38069,\"start\":38068},{\"end\":38344,\"start\":38343},{\"end\":38357,\"start\":38356},{\"end\":38366,\"start\":38365},{\"end\":38646,\"start\":38642},{\"end\":38653,\"start\":38652},{\"end\":38662,\"start\":38661},{\"end\":39028,\"start\":39027},{\"end\":39037,\"start\":39036},{\"end\":39369,\"start\":39368},{\"end\":39371,\"start\":39370},{\"end\":39651,\"start\":39650},{\"end\":39662,\"start\":39661},{\"end\":39670,\"start\":39669},{\"end\":39680,\"start\":39679},{\"end\":39974,\"start\":39973},{\"end\":39984,\"start\":39983},{\"end\":39992,\"start\":39991},{\"end\":40001,\"start\":40000},{\"end\":40010,\"start\":40009},{\"end\":40312,\"start\":40311},{\"end\":40319,\"start\":40318},{\"end\":40321,\"start\":40320},{\"end\":40332,\"start\":40328},{\"end\":40338,\"start\":40337},{\"end\":40348,\"start\":40347},{\"end\":40360,\"start\":40359},{\"end\":40646,\"start\":40645},{\"end\":40648,\"start\":40647},{\"end\":40865,\"start\":40864},{\"end\":40867,\"start\":40866},{\"end\":40877,\"start\":40876},{\"end\":41284,\"start\":41280},{\"end\":41291,\"start\":41290},{\"end\":41495,\"start\":41494},{\"end\":41505,\"start\":41504},{\"end\":41514,\"start\":41513},{\"end\":41516,\"start\":41515},{\"end\":41525,\"start\":41524},{\"end\":41527,\"start\":41526},{\"end\":41829,\"start\":41828},{\"end\":41840,\"start\":41839},{\"end\":41851,\"start\":41850},{\"end\":42155,\"start\":42154},{\"end\":42166,\"start\":42165},{\"end\":42177,\"start\":42176},{\"end\":42505,\"start\":42504},{\"end\":42516,\"start\":42515},{\"end\":42518,\"start\":42517},{\"end\":42527,\"start\":42526},{\"end\":42687,\"start\":42686},{\"end\":42696,\"start\":42695},{\"end\":42706,\"start\":42705},{\"end\":42713,\"start\":42712},{\"end\":42720,\"start\":42719},{\"end\":42727,\"start\":42726},{\"end\":42729,\"start\":42728},{\"end\":43010,\"start\":43009},{\"end\":43019,\"start\":43018},{\"end\":43021,\"start\":43020},{\"end\":43281,\"start\":43280},{\"end\":43290,\"start\":43289},{\"end\":43299,\"start\":43298},{\"end\":43301,\"start\":43300},{\"end\":43314,\"start\":43313},{\"end\":43326,\"start\":43325},{\"end\":43328,\"start\":43327},{\"end\":43339,\"start\":43338},{\"end\":43341,\"start\":43340},{\"end\":43696,\"start\":43695},{\"end\":43705,\"start\":43704},{\"end\":43711,\"start\":43710},{\"end\":43713,\"start\":43712},{\"end\":43726,\"start\":43725},{\"end\":43728,\"start\":43727},{\"end\":43739,\"start\":43738},{\"end\":44056,\"start\":44055},{\"end\":44058,\"start\":44057},{\"end\":44068,\"start\":44067},{\"end\":44079,\"start\":44078},{\"end\":44362,\"start\":44361},{\"end\":44374,\"start\":44373},{\"end\":44611,\"start\":44610},{\"end\":44625,\"start\":44621},{\"end\":44631,\"start\":44630},{\"end\":44641,\"start\":44637},{\"end\":44649,\"start\":44648},{\"end\":44651,\"start\":44650},{\"end\":44941,\"start\":44940},{\"end\":44955,\"start\":44954},{\"end\":44963,\"start\":44962},{\"end\":44973,\"start\":44972},{\"end\":45339,\"start\":45338},{\"end\":45350,\"start\":45346},{\"end\":45356,\"start\":45355},{\"end\":45629,\"start\":45628},{\"end\":45631,\"start\":45630},{\"end\":45640,\"start\":45639},{\"end\":45642,\"start\":45641},{\"end\":45650,\"start\":45649},{\"end\":45661,\"start\":45660},{\"end\":45663,\"start\":45662},{\"end\":45967,\"start\":45966},{\"end\":45976,\"start\":45975},{\"end\":45978,\"start\":45977},{\"end\":45987,\"start\":45986},{\"end\":45995,\"start\":45994},{\"end\":46005,\"start\":46004},{\"end\":46311,\"start\":46310},{\"end\":46327,\"start\":46326},{\"end\":46329,\"start\":46328},{\"end\":46338,\"start\":46337},{\"end\":46643,\"start\":46642},{\"end\":46645,\"start\":46644},{\"end\":46655,\"start\":46654},{\"end\":46662,\"start\":46661},{\"end\":46669,\"start\":46668},{\"end\":46679,\"start\":46678},{\"end\":46688,\"start\":46687},{\"end\":46690,\"start\":46689},{\"end\":46703,\"start\":46702},{\"end\":46705,\"start\":46704},{\"end\":46716,\"start\":46715},{\"end\":46718,\"start\":46717},{\"end\":47048,\"start\":47047},{\"end\":47066,\"start\":47065},{\"end\":47327,\"start\":47326},{\"end\":47334,\"start\":47333},{\"end\":47336,\"start\":47335},{\"end\":47346,\"start\":47345},{\"end\":47353,\"start\":47352},{\"end\":47360,\"start\":47359},{\"end\":47720,\"start\":47719},{\"end\":47729,\"start\":47728},{\"end\":47731,\"start\":47730},{\"end\":47741,\"start\":47740},{\"end\":48077,\"start\":48076},{\"end\":48084,\"start\":48083},{\"end\":48092,\"start\":48091},{\"end\":48100,\"start\":48099},{\"end\":48429,\"start\":48428},{\"end\":48437,\"start\":48436},{\"end\":48439,\"start\":48438},{\"end\":48453,\"start\":48452},{\"end\":48461,\"start\":48460},{\"end\":48483,\"start\":48482},{\"end\":48721,\"start\":48720},{\"end\":48729,\"start\":48728},{\"end\":48736,\"start\":48735},{\"end\":48752,\"start\":48751},{\"end\":48764,\"start\":48763},{\"end\":48766,\"start\":48765},{\"end\":48780,\"start\":48779}]", "bib_author_last_name": "[{\"end\":32678,\"start\":32671},{\"end\":32689,\"start\":32682},{\"end\":32700,\"start\":32693},{\"end\":32713,\"start\":32704},{\"end\":32962,\"start\":32956},{\"end\":32971,\"start\":32966},{\"end\":32981,\"start\":32975},{\"end\":32992,\"start\":32985},{\"end\":33237,\"start\":33225},{\"end\":33250,\"start\":33241},{\"end\":33470,\"start\":33458},{\"end\":33483,\"start\":33474},{\"end\":33733,\"start\":33728},{\"end\":33745,\"start\":33737},{\"end\":33757,\"start\":33749},{\"end\":34099,\"start\":34093},{\"end\":34112,\"start\":34103},{\"end\":34520,\"start\":34511},{\"end\":34529,\"start\":34524},{\"end\":34541,\"start\":34533},{\"end\":34555,\"start\":34545},{\"end\":34567,\"start\":34559},{\"end\":34908,\"start\":34903},{\"end\":34920,\"start\":34914},{\"end\":34931,\"start\":34924},{\"end\":34944,\"start\":34935},{\"end\":35251,\"start\":35247},{\"end\":35263,\"start\":35255},{\"end\":35275,\"start\":35267},{\"end\":35284,\"start\":35279},{\"end\":35295,\"start\":35288},{\"end\":35308,\"start\":35301},{\"end\":35663,\"start\":35658},{\"end\":35989,\"start\":35984},{\"end\":35998,\"start\":35993},{\"end\":36008,\"start\":36002},{\"end\":36208,\"start\":36202},{\"end\":36219,\"start\":36212},{\"end\":36227,\"start\":36223},{\"end\":36236,\"start\":36231},{\"end\":36246,\"start\":36240},{\"end\":36258,\"start\":36250},{\"end\":36271,\"start\":36264},{\"end\":36285,\"start\":36275},{\"end\":36736,\"start\":36732},{\"end\":36749,\"start\":36740},{\"end\":36759,\"start\":36753},{\"end\":37019,\"start\":37014},{\"end\":37025,\"start\":37023},{\"end\":37040,\"start\":37029},{\"end\":37050,\"start\":37044},{\"end\":37421,\"start\":37415},{\"end\":37433,\"start\":37425},{\"end\":37442,\"start\":37437},{\"end\":37771,\"start\":37765},{\"end\":37782,\"start\":37775},{\"end\":37790,\"start\":37786},{\"end\":38030,\"start\":38024},{\"end\":38043,\"start\":38034},{\"end\":38056,\"start\":38047},{\"end\":38066,\"start\":38060},{\"end\":38076,\"start\":38070},{\"end\":38354,\"start\":38345},{\"end\":38363,\"start\":38358},{\"end\":38376,\"start\":38367},{\"end\":38650,\"start\":38647},{\"end\":38659,\"start\":38654},{\"end\":38668,\"start\":38663},{\"end\":39034,\"start\":39029},{\"end\":39045,\"start\":39038},{\"end\":39377,\"start\":39372},{\"end\":39659,\"start\":39652},{\"end\":39667,\"start\":39663},{\"end\":39677,\"start\":39671},{\"end\":39699,\"start\":39681},{\"end\":39981,\"start\":39975},{\"end\":39989,\"start\":39985},{\"end\":39998,\"start\":39993},{\"end\":40007,\"start\":40002},{\"end\":40019,\"start\":40011},{\"end\":40316,\"start\":40313},{\"end\":40326,\"start\":40322},{\"end\":40335,\"start\":40333},{\"end\":40345,\"start\":40339},{\"end\":40357,\"start\":40349},{\"end\":40368,\"start\":40361},{\"end\":40653,\"start\":40649},{\"end\":40874,\"start\":40868},{\"end\":40880,\"start\":40878},{\"end\":41288,\"start\":41285},{\"end\":41297,\"start\":41292},{\"end\":41502,\"start\":41496},{\"end\":41511,\"start\":41506},{\"end\":41522,\"start\":41517},{\"end\":41533,\"start\":41528},{\"end\":41837,\"start\":41830},{\"end\":41848,\"start\":41841},{\"end\":41861,\"start\":41852},{\"end\":42163,\"start\":42156},{\"end\":42174,\"start\":42167},{\"end\":42187,\"start\":42178},{\"end\":42513,\"start\":42506},{\"end\":42524,\"start\":42519},{\"end\":42537,\"start\":42528},{\"end\":42693,\"start\":42688},{\"end\":42703,\"start\":42697},{\"end\":42710,\"start\":42707},{\"end\":42717,\"start\":42714},{\"end\":42724,\"start\":42721},{\"end\":42732,\"start\":42730},{\"end\":43016,\"start\":43011},{\"end\":43027,\"start\":43022},{\"end\":43287,\"start\":43282},{\"end\":43296,\"start\":43291},{\"end\":43311,\"start\":43302},{\"end\":43323,\"start\":43315},{\"end\":43336,\"start\":43329},{\"end\":43349,\"start\":43342},{\"end\":43702,\"start\":43697},{\"end\":43708,\"start\":43706},{\"end\":43723,\"start\":43714},{\"end\":43736,\"start\":43729},{\"end\":43748,\"start\":43740},{\"end\":44065,\"start\":44059},{\"end\":44076,\"start\":44069},{\"end\":44089,\"start\":44080},{\"end\":44371,\"start\":44363},{\"end\":44380,\"start\":44375},{\"end\":44619,\"start\":44612},{\"end\":44628,\"start\":44626},{\"end\":44635,\"start\":44632},{\"end\":44646,\"start\":44642},{\"end\":44657,\"start\":44652},{\"end\":44952,\"start\":44942},{\"end\":44960,\"start\":44956},{\"end\":44970,\"start\":44964},{\"end\":44996,\"start\":44974},{\"end\":45344,\"start\":45340},{\"end\":45353,\"start\":45351},{\"end\":45362,\"start\":45357},{\"end\":45637,\"start\":45632},{\"end\":45647,\"start\":45643},{\"end\":45658,\"start\":45651},{\"end\":45670,\"start\":45664},{\"end\":45973,\"start\":45968},{\"end\":45984,\"start\":45979},{\"end\":45992,\"start\":45988},{\"end\":46002,\"start\":45996},{\"end\":46021,\"start\":46006},{\"end\":46324,\"start\":46312},{\"end\":46335,\"start\":46330},{\"end\":46361,\"start\":46339},{\"end\":46652,\"start\":46646},{\"end\":46659,\"start\":46656},{\"end\":46666,\"start\":46663},{\"end\":46676,\"start\":46670},{\"end\":46685,\"start\":46680},{\"end\":46700,\"start\":46691},{\"end\":46713,\"start\":46706},{\"end\":46727,\"start\":46719},{\"end\":47063,\"start\":47049},{\"end\":47073,\"start\":47067},{\"end\":47331,\"start\":47328},{\"end\":47343,\"start\":47337},{\"end\":47350,\"start\":47347},{\"end\":47357,\"start\":47354},{\"end\":47366,\"start\":47361},{\"end\":47726,\"start\":47721},{\"end\":47738,\"start\":47732},{\"end\":47751,\"start\":47742},{\"end\":48081,\"start\":48078},{\"end\":48089,\"start\":48085},{\"end\":48097,\"start\":48093},{\"end\":48104,\"start\":48101},{\"end\":48434,\"start\":48430},{\"end\":48450,\"start\":48440},{\"end\":48458,\"start\":48454},{\"end\":48480,\"start\":48462},{\"end\":48489,\"start\":48484},{\"end\":48726,\"start\":48722},{\"end\":48733,\"start\":48730},{\"end\":48749,\"start\":48737},{\"end\":48761,\"start\":48753},{\"end\":48777,\"start\":48767},{\"end\":48789,\"start\":48781}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":32597,\"start\":32448},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":52018690},\"end\":32915,\"start\":32599},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":2495132},\"end\":33197,\"start\":32917},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":10769575},\"end\":33434,\"start\":33199},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":36022762},\"end\":33661,\"start\":33436},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2915490},\"end\":33988,\"start\":33663},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":14052925},\"end\":34436,\"start\":33990},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":2560991},\"end\":34862,\"start\":34438},{\"attributes\":{\"doi\":\"2017. 3\",\"id\":\"b8\",\"matched_paper_id\":1662180},\"end\":35182,\"start\":34864},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":26647143},\"end\":35592,\"start\":35184},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":53001577},\"end\":35960,\"start\":35594},{\"attributes\":{\"id\":\"b11\"},\"end\":36093,\"start\":35962},{\"attributes\":{\"doi\":\"112:1-112:11\",\"id\":\"b12\",\"matched_paper_id\":215808493},\"end\":36630,\"start\":36095},{\"attributes\":{\"id\":\"b13\"},\"end\":36931,\"start\":36632},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":216093683},\"end\":37310,\"start\":36933},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":14276584},\"end\":37715,\"start\":37312},{\"attributes\":{\"doi\":\"abs/1503.02531\",\"id\":\"b16\"},\"end\":37921,\"start\":37717},{\"attributes\":{\"doi\":\"abs/1706.00079\",\"id\":\"b17\"},\"end\":38291,\"start\":37923},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":53039243},\"end\":38546,\"start\":38293},{\"attributes\":{\"doi\":\"2017. 3\",\"id\":\"b19\",\"matched_paper_id\":39395448},\"end\":38931,\"start\":38548},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":5808102},\"end\":39282,\"start\":38933},{\"attributes\":{\"id\":\"b21\"},\"end\":39581,\"start\":39284},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":10147653},\"end\":39892,\"start\":39583},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":13515193},\"end\":40263,\"start\":39894},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":21686127},\"end\":40606,\"start\":40265},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":6155330},\"end\":40818,\"start\":40608},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":6628106},\"end\":41086,\"start\":40820},{\"attributes\":{\"id\":\"b27\"},\"end\":41237,\"start\":41088},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":10627900},\"end\":41490,\"start\":41239},{\"attributes\":{\"id\":\"b29\"},\"end\":41675,\"start\":41492},{\"attributes\":{\"id\":\"b30\"},\"end\":41766,\"start\":41677},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":13746939},\"end\":42087,\"start\":41768},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":4559198},\"end\":42446,\"start\":42089},{\"attributes\":{\"id\":\"b33\"},\"end\":42658,\"start\":42448},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":352650},\"end\":42935,\"start\":42660},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":4724792},\"end\":43251,\"start\":42937},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":1697911},\"end\":43610,\"start\":43253},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":37241925},\"end\":44030,\"start\":43612},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":4637184},\"end\":44260,\"start\":44032},{\"attributes\":{\"doi\":\"abs/1806.00154\",\"id\":\"b39\"},\"end\":44556,\"start\":44262},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":3841418},\"end\":44914,\"start\":44558},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":4696924},\"end\":45233,\"start\":44916},{\"attributes\":{\"doi\":\"abs/1811.10813\",\"id\":\"b42\"},\"end\":45547,\"start\":45235},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":14592574},\"end\":45911,\"start\":45549},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":154440},\"end\":46258,\"start\":45913},{\"attributes\":{\"id\":\"b45\"},\"end\":46581,\"start\":46260},{\"attributes\":{\"doi\":\"93:1-93:11\",\"id\":\"b46\",\"matched_paper_id\":32124381},\"end\":47015,\"start\":46583},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":5855042},\"end\":47253,\"start\":47017},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":49745096},\"end\":47630,\"start\":47255},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":51866642},\"end\":48004,\"start\":47632},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":7577075},\"end\":48343,\"start\":48006},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":13722444},\"end\":48697,\"start\":48345},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":4748509},\"end\":49017,\"start\":48699}]", "bib_title": "[{\"end\":32667,\"start\":32599},{\"end\":32952,\"start\":32917},{\"end\":33221,\"start\":33199},{\"end\":33454,\"start\":33436},{\"end\":33724,\"start\":33663},{\"end\":34087,\"start\":33990},{\"end\":34507,\"start\":34438},{\"end\":34897,\"start\":34864},{\"end\":35243,\"start\":35184},{\"end\":35652,\"start\":35594},{\"end\":36198,\"start\":36095},{\"end\":37008,\"start\":36933},{\"end\":37409,\"start\":37312},{\"end\":38341,\"start\":38293},{\"end\":38640,\"start\":38548},{\"end\":39025,\"start\":38933},{\"end\":39648,\"start\":39583},{\"end\":39971,\"start\":39894},{\"end\":40309,\"start\":40265},{\"end\":40643,\"start\":40608},{\"end\":40862,\"start\":40820},{\"end\":41278,\"start\":41239},{\"end\":41826,\"start\":41768},{\"end\":42152,\"start\":42089},{\"end\":42684,\"start\":42660},{\"end\":43007,\"start\":42937},{\"end\":43278,\"start\":43253},{\"end\":43693,\"start\":43612},{\"end\":44053,\"start\":44032},{\"end\":44608,\"start\":44558},{\"end\":44938,\"start\":44916},{\"end\":45626,\"start\":45549},{\"end\":45964,\"start\":45913},{\"end\":46308,\"start\":46260},{\"end\":46640,\"start\":46583},{\"end\":47045,\"start\":47017},{\"end\":47324,\"start\":47255},{\"end\":47717,\"start\":47632},{\"end\":48074,\"start\":48006},{\"end\":48426,\"start\":48345},{\"end\":48718,\"start\":48699}]", "bib_author": "[{\"end\":32680,\"start\":32669},{\"end\":32691,\"start\":32680},{\"end\":32702,\"start\":32691},{\"end\":32715,\"start\":32702},{\"end\":32964,\"start\":32954},{\"end\":32973,\"start\":32964},{\"end\":32983,\"start\":32973},{\"end\":32994,\"start\":32983},{\"end\":33239,\"start\":33223},{\"end\":33252,\"start\":33239},{\"end\":33472,\"start\":33456},{\"end\":33485,\"start\":33472},{\"end\":33735,\"start\":33726},{\"end\":33747,\"start\":33735},{\"end\":33759,\"start\":33747},{\"end\":34101,\"start\":34089},{\"end\":34114,\"start\":34101},{\"end\":34522,\"start\":34509},{\"end\":34531,\"start\":34522},{\"end\":34543,\"start\":34531},{\"end\":34557,\"start\":34543},{\"end\":34569,\"start\":34557},{\"end\":34910,\"start\":34899},{\"end\":34922,\"start\":34910},{\"end\":34933,\"start\":34922},{\"end\":34946,\"start\":34933},{\"end\":35253,\"start\":35245},{\"end\":35265,\"start\":35253},{\"end\":35277,\"start\":35265},{\"end\":35286,\"start\":35277},{\"end\":35297,\"start\":35286},{\"end\":35310,\"start\":35297},{\"end\":35665,\"start\":35654},{\"end\":35991,\"start\":35980},{\"end\":36000,\"start\":35991},{\"end\":36010,\"start\":36000},{\"end\":36210,\"start\":36200},{\"end\":36221,\"start\":36210},{\"end\":36229,\"start\":36221},{\"end\":36238,\"start\":36229},{\"end\":36248,\"start\":36238},{\"end\":36260,\"start\":36248},{\"end\":36273,\"start\":36260},{\"end\":36287,\"start\":36273},{\"end\":36738,\"start\":36730},{\"end\":36751,\"start\":36738},{\"end\":36761,\"start\":36751},{\"end\":37021,\"start\":37010},{\"end\":37027,\"start\":37021},{\"end\":37042,\"start\":37027},{\"end\":37052,\"start\":37042},{\"end\":37423,\"start\":37411},{\"end\":37435,\"start\":37423},{\"end\":37444,\"start\":37435},{\"end\":37773,\"start\":37763},{\"end\":37784,\"start\":37773},{\"end\":37792,\"start\":37784},{\"end\":38032,\"start\":38022},{\"end\":38045,\"start\":38032},{\"end\":38058,\"start\":38045},{\"end\":38068,\"start\":38058},{\"end\":38078,\"start\":38068},{\"end\":38356,\"start\":38343},{\"end\":38365,\"start\":38356},{\"end\":38378,\"start\":38365},{\"end\":38652,\"start\":38642},{\"end\":38661,\"start\":38652},{\"end\":38670,\"start\":38661},{\"end\":39036,\"start\":39027},{\"end\":39047,\"start\":39036},{\"end\":39379,\"start\":39368},{\"end\":39661,\"start\":39650},{\"end\":39669,\"start\":39661},{\"end\":39679,\"start\":39669},{\"end\":39701,\"start\":39679},{\"end\":39983,\"start\":39973},{\"end\":39991,\"start\":39983},{\"end\":40000,\"start\":39991},{\"end\":40009,\"start\":40000},{\"end\":40021,\"start\":40009},{\"end\":40318,\"start\":40311},{\"end\":40328,\"start\":40318},{\"end\":40337,\"start\":40328},{\"end\":40347,\"start\":40337},{\"end\":40359,\"start\":40347},{\"end\":40370,\"start\":40359},{\"end\":40655,\"start\":40645},{\"end\":40876,\"start\":40864},{\"end\":40882,\"start\":40876},{\"end\":41290,\"start\":41280},{\"end\":41299,\"start\":41290},{\"end\":41504,\"start\":41494},{\"end\":41513,\"start\":41504},{\"end\":41524,\"start\":41513},{\"end\":41535,\"start\":41524},{\"end\":41839,\"start\":41828},{\"end\":41850,\"start\":41839},{\"end\":41863,\"start\":41850},{\"end\":42165,\"start\":42154},{\"end\":42176,\"start\":42165},{\"end\":42189,\"start\":42176},{\"end\":42515,\"start\":42504},{\"end\":42526,\"start\":42515},{\"end\":42539,\"start\":42526},{\"end\":42695,\"start\":42686},{\"end\":42705,\"start\":42695},{\"end\":42712,\"start\":42705},{\"end\":42719,\"start\":42712},{\"end\":42726,\"start\":42719},{\"end\":42734,\"start\":42726},{\"end\":43018,\"start\":43009},{\"end\":43029,\"start\":43018},{\"end\":43289,\"start\":43280},{\"end\":43298,\"start\":43289},{\"end\":43313,\"start\":43298},{\"end\":43325,\"start\":43313},{\"end\":43338,\"start\":43325},{\"end\":43351,\"start\":43338},{\"end\":43704,\"start\":43695},{\"end\":43710,\"start\":43704},{\"end\":43725,\"start\":43710},{\"end\":43738,\"start\":43725},{\"end\":43750,\"start\":43738},{\"end\":44067,\"start\":44055},{\"end\":44078,\"start\":44067},{\"end\":44091,\"start\":44078},{\"end\":44373,\"start\":44361},{\"end\":44382,\"start\":44373},{\"end\":44621,\"start\":44610},{\"end\":44630,\"start\":44621},{\"end\":44637,\"start\":44630},{\"end\":44648,\"start\":44637},{\"end\":44659,\"start\":44648},{\"end\":44954,\"start\":44940},{\"end\":44962,\"start\":44954},{\"end\":44972,\"start\":44962},{\"end\":44998,\"start\":44972},{\"end\":45346,\"start\":45338},{\"end\":45355,\"start\":45346},{\"end\":45364,\"start\":45355},{\"end\":45639,\"start\":45628},{\"end\":45649,\"start\":45639},{\"end\":45660,\"start\":45649},{\"end\":45672,\"start\":45660},{\"end\":45975,\"start\":45966},{\"end\":45986,\"start\":45975},{\"end\":45994,\"start\":45986},{\"end\":46004,\"start\":45994},{\"end\":46023,\"start\":46004},{\"end\":46326,\"start\":46310},{\"end\":46337,\"start\":46326},{\"end\":46363,\"start\":46337},{\"end\":46654,\"start\":46642},{\"end\":46661,\"start\":46654},{\"end\":46668,\"start\":46661},{\"end\":46678,\"start\":46668},{\"end\":46687,\"start\":46678},{\"end\":46702,\"start\":46687},{\"end\":46715,\"start\":46702},{\"end\":46729,\"start\":46715},{\"end\":47065,\"start\":47047},{\"end\":47075,\"start\":47065},{\"end\":47333,\"start\":47326},{\"end\":47345,\"start\":47333},{\"end\":47352,\"start\":47345},{\"end\":47359,\"start\":47352},{\"end\":47368,\"start\":47359},{\"end\":47728,\"start\":47719},{\"end\":47740,\"start\":47728},{\"end\":47753,\"start\":47740},{\"end\":48083,\"start\":48076},{\"end\":48091,\"start\":48083},{\"end\":48099,\"start\":48091},{\"end\":48106,\"start\":48099},{\"end\":48436,\"start\":48428},{\"end\":48452,\"start\":48436},{\"end\":48460,\"start\":48452},{\"end\":48482,\"start\":48460},{\"end\":48491,\"start\":48482},{\"end\":48728,\"start\":48720},{\"end\":48735,\"start\":48728},{\"end\":48751,\"start\":48735},{\"end\":48763,\"start\":48751},{\"end\":48779,\"start\":48763},{\"end\":48791,\"start\":48779}]", "bib_venue": "[{\"end\":35768,\"start\":35725},{\"end\":32504,\"start\":32448},{\"end\":32745,\"start\":32715},{\"end\":33045,\"start\":32994},{\"end\":33307,\"start\":33252},{\"end\":33530,\"start\":33485},{\"end\":33815,\"start\":33759},{\"end\":34203,\"start\":34114},{\"end\":34634,\"start\":34569},{\"end\":35011,\"start\":34953},{\"end\":35368,\"start\":35310},{\"end\":35723,\"start\":35665},{\"end\":35978,\"start\":35962},{\"end\":36339,\"start\":36299},{\"end\":36728,\"start\":36632},{\"end\":37110,\"start\":37052},{\"end\":37492,\"start\":37444},{\"end\":37761,\"start\":37717},{\"end\":38020,\"start\":37923},{\"end\":38408,\"start\":38378},{\"end\":38732,\"start\":38677},{\"end\":39098,\"start\":39047},{\"end\":39366,\"start\":39284},{\"end\":39716,\"start\":39701},{\"end\":40060,\"start\":40021},{\"end\":40412,\"start\":40370},{\"end\":40698,\"start\":40655},{\"end\":40942,\"start\":40882},{\"end\":41136,\"start\":41088},{\"end\":41355,\"start\":41299},{\"end\":41701,\"start\":41677},{\"end\":41908,\"start\":41863},{\"end\":42254,\"start\":42189},{\"end\":42502,\"start\":42448},{\"end\":42785,\"start\":42734},{\"end\":43074,\"start\":43029},{\"end\":43416,\"start\":43351},{\"end\":43797,\"start\":43750},{\"end\":44131,\"start\":44091},{\"end\":44359,\"start\":44262},{\"end\":44724,\"start\":44659},{\"end\":45063,\"start\":44998},{\"end\":45336,\"start\":45235},{\"end\":45699,\"start\":45672},{\"end\":46071,\"start\":46023},{\"end\":46402,\"start\":46363},{\"end\":46778,\"start\":46739},{\"end\":47111,\"start\":47075},{\"end\":47427,\"start\":47368},{\"end\":47798,\"start\":47753},{\"end\":48151,\"start\":48106},{\"end\":48502,\"start\":48491},{\"end\":48836,\"start\":48791}]"}}}, "year": 2023, "month": 12, "day": 17}