{"id": 8535487, "updated": "2023-11-08 08:06:18.451", "metadata": {"title": "Knowledge Graph Embedding with Iterative Guidance from Soft Rules", "authors": "[{\"first\":\"Shu\",\"last\":\"Guo\",\"middle\":[]},{\"first\":\"Quan\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Lihong\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Bin\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Li\",\"last\":\"Guo\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2017, "month": 11, "day": 30}, "abstract": "Embedding knowledge graphs (KGs) into continuous vector spaces is a focus of current research. Combining such an embedding model with logic rules has recently attracted increasing attention. Most previous attempts made a one-time injection of logic rules, ignoring the interactive nature between embedding learning and logical inference. And they focused only on hard rules, which always hold with no exception and usually require extensive manual effort to create or validate. In this paper, we propose Rule-Guided Embedding (RUGE), a novel paradigm of KG embedding with iterative guidance from soft rules. RUGE enables an embedding model to learn simultaneously from 1) labeled triples that have been directly observed in a given KG, 2) unlabeled triples whose labels are going to be predicted iteratively, and 3) soft rules with various confidence levels extracted automatically from the KG. In the learning process, RUGE iteratively queries rules to obtain soft labels for unlabeled triples, and integrates such newly labeled triples to update the embedding model. Through this iterative procedure, knowledge embodied in logic rules may be better transferred into the learned embeddings. We evaluate RUGE in link prediction on Freebase and YAGO. Experimental results show that: 1) with rule knowledge injected iteratively, RUGE achieves significant and consistent improvements over state-of-the-art baselines; and 2) despite their uncertainties, automatically extracted soft rules are highly beneficial to KG embedding, even those with moderate confidence levels. The code and data used for this paper can be obtained from https://github.com/iieir-km/RUGE.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1711.11231", "mag": "2963359213", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/aaai/GuoWWWG18", "doi": "10.1609/aaai.v32i1.11918"}}, "content": {"source": {"pdf_hash": "c7b09340ceacbdd482be31e2cbe4efac0929acdf", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1711.11231v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "5f9fec70146472b484edb9a0e83eb664c3acd9cb", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c7b09340ceacbdd482be31e2cbe4efac0929acdf.txt", "contents": "\nKnowledge Graph Embedding with Iterative Guidance from Soft Rules\n\n\nShu Guo \nInstitute of Information Engineering\nChinese Academy of Sciences\n\n\nSchool of Cyber Security\nUniversity of Chinese Academy of Sciences\n\n\nQuan Wang \nInstitute of Information Engineering\nChinese Academy of Sciences\n\n\nSchool of Cyber Security\nUniversity of Chinese Academy of Sciences\n\n\nState Key Laboratory of Information Security\nChinese Academy of Sciences\n\n\nLihong Wang \nNational Computer Network Emergency Response Technical Team & Coordination Center of China\n\n\nBin Wang \nInstitute of Information Engineering\nChinese Academy of Sciences\n\n\nSchool of Cyber Security\nUniversity of Chinese Academy of Sciences\n\n\nLi Guo \nInstitute of Information Engineering\nChinese Academy of Sciences\n\n\nSchool of Cyber Security\nUniversity of Chinese Academy of Sciences\n\n\nKnowledge Graph Embedding with Iterative Guidance from Soft Rules\n\nEmbedding knowledge graphs (KGs) into continuous vector spaces is a focus of current research. Combining such an embedding model with logic rules has recently attracted increasing attention. Most previous attempts made a one-time injection of logic rules, ignoring the interactive nature between embedding learning and logical inference. And they focused only on hard rules, which always hold with no exception and usually require extensive manual effort to create or validate. In this paper, we propose Rule-Guided Embedding (RUGE), a novel paradigm of KG embedding with iterative guidance from soft rules. RUGE enables an embedding model to learn simultaneously from 1) labeled triples that have been directly observed in a given KG, 2) unlabeled triples whose labels are going to be predicted iteratively, and 3) soft rules with various confidence levels extracted automatically from the KG. In the learning process, RUGE iteratively queries rules to obtain soft labels for unlabeled triples, and integrates such newly labeled triples to update the embedding model. Through this iterative procedure, knowledge embodied in logic rules may be better transferred into the learned embeddings. We evaluate RUGE in link prediction on Freebase and YAGO. Experimental results show that: 1) with rule knowledge injected iteratively, RUGE achieves significant and consistent improvements over state-of-the-art baselines; and 2) despite their uncertainties, automatically extracted soft rules are highly beneficial to KG embedding, even those with moderate confidence levels. The code and data used for this paper can be obtained from https://github.com/iieir-km/RUGE.\n\nIntroduction\n\nKnowledge graphs (KGs) such as WordNet (Miller 1995), Freebase (Bollacker et al. 2008), YAGO (Suchanek, Kasneci, and Weikum 2007), and NELL (Carlson et al. 2010) are extremely useful resources for many AI related applications. A KG is a multi-relational graph composed of entities as nodes and relations as different types of edges. Each edge is represented as a triple (head entity, relation, tail entity), indicating that there is a specific relation between two entities, e.g., (Paris, CapitalOf, France). Although effective in representing structured data, the underlying symbolic nature of such triples often makes KGs hard to manipulate.\n\nRecently, a new research direction termed as knowledge graph embedding has been proposed and quickly received massive attention (Nickel, Tresp, and Kriegel 2011;Wang et al. 2014;Lin et al. 2015b;Yang et al. 2015;Nickel, Rosasco, and Poggio 2016;Trouillon et al. 2016). The key idea is to embed entities and relations in a KG into a low-dimensional continuous vector space, so as to simplify the manipulation while preserving the inherent structure of the KG. Such embeddings contain rich semantic information, and can benefit a broad range of downstream applications Bordes et al. 2014;Zhang et al. 2016;Xiong, Power, and Callan 2017).\n\nTraditional methods performed embedding based solely on triples observed in a KG. But considering the power of logic rules in knowledge acquisition and inference, combining embedding models with logic rules has become a focus of current research Vendrov et al. 2015; Wang and Cohen 2016;.  and  tried to use embedding models and logic rules for KG completion. But in their work, rules are modeled separately from embedding models, and would not help to learn more predictive embeddings. Rockt\u00e4schel et al. (2015) and Guo et al. (2016) then devised joint learning paradigms which can inject first-order logic (FOL) into KG embedding. Demeester et al. (2016) further proposed lifted rule injection to avoid the costly propositionalization of FOL rules. Although these joint models are able to learn better embeddings after integrating logic rules, they still have their drawbacks and restrictions.\n\nFirst of all, these joint models made a one-time injection of logic rules, taking them as additional rule-based training instances (Rockt\u00e4schel, Singh, and Riedel 2015) or regularization terms (Demeester, Rockt\u00e4schel, and Riedel 2016). We argue that rules can better enhance KG embedding, however, in an iterative manner. Given the learned embeddings and their rough predictions, rules can be used to refine the predictions and infer new facts. The newly inferred facts, in turn, will help to learn better embeddings and more accurate logical inference. Previous methods fail to model such interactions between embedding models and logic rules. Furthermore, they focused only on hard rules which always hold with no exception. Such rules usually require extensive manual effort to create or validate. Actually, besides hard rules, a significant amount of background information can be en-  Figure 1: Framework overview. RUGE enables an embedding model to learn simultaneously from labeled triples, unlabeled triples, and soft rules in an iterative manner, where each iteration alternates between a soft label prediction stage and an embedding rectification stage.\n\ncoded as soft rules, e.g., \"a person is very likely (but not necessarily) to have a nationality of the country where he/she was born\". Soft rules can be extracted automatically and efficiently via modern rule mining systems (Gal\u00e1rraga et al. 2013;Gal\u00e1rraga et al. 2015). Yet, despite this merit, soft rules have not been well studied in previous methods.\n\nThis paper proposes RUle-Guided Embedding (RUGE), a novel paradigm of KG embedding with iterative guidance from soft rules. As sketched in Fig. 1, it enables an embedding model to learn simultaneously from 1) labeled triples that have been directly observed in a given KG, 2) unlabeled triples whose labels are going to be predicted iteratively, and 3) soft rules with different confidence levels extracted automatically from the KG. During each iteration of the learning process, the model alternates between a soft label prediction stage and an embedding rectification stage. The former uses currently learned embeddings and soft rules to predict soft labels for unlabeled triples, and the latter further integrates both labeled and unlabeled triples (with hard and soft labels respectively) to update current embeddings. Through this iterative procedure, knowledge embodied in logic rules may be better transferred into the learned embeddings.\n\nWe empirically evaluate RUGE on large scale public KGs, namely Freebase and YAGO. Experimental results reveal that: 1) by incorporating logic rules, RUGE significantly and consistently improves over state-of-the-art basic embedding models (without rules); 2) compared to those one-time injection schemes studied before, the iterative injection strategy maximizes the utility of logic rules for KG embedding, and indeed achieves substantially better performance; 3) despite the uncertainties, automatically extracted soft rules are highly beneficial to KG embedding, even those with moderate confidence levels.\n\nThe contributions of this paper are threefold. 1) We devise a novel paradigm of KG embedding which iteratively injects logic rules into the learned embeddings. To our knowledge, this is the first work that models interactions between embedding learning and logical inference in a principled framework. 2) We demonstrate the usefulness of automatically extracted soft rules in KG embedding, thereby eliminating the requirement of laborious manual rule creation. 3) Our ap-proach is quite generic and flexible. It can integrate various types of rules with different confidence levels to enhance a good variety of KG embedding models.\n\n\nRelated Work\n\nRecent years have witnessed increasing interest in learning distributed representations for entities and relations in KGs, a.k.a. KG embedding. Various techniques have been devised for this task, e.g., translation-based models which take relations as translating operations between head and tail entities Wang et al. 2014;Lin et al. 2015b), simple compositional models which match compositions of head-tail entity pairs with their relations (Nickel, Tresp, and Kriegel 2011;Yang et al. 2015;Nickel, Rosasco, and Poggio 2016;Trouillon et al. 2016), and neural networks which further introduce non-linear layers and deep architectures (Socher et al. 2013;Bordes et al. 2014;Dong et al. 2014;). Among these techniques, ComplEx (Trouillon et al. 2016), a compositional model which represents entities and relations as complex-valued vectors, achieves a very good trade-off between accuracy and efficiency. Most of the currently available techniques perform the embedding task based solely on triples observed in a KG. Some recent work further tried to use other information, e.g., entity types Xie, Liu, and Sun 2016) and textual descriptions Xiao, Huang, and Zhu 2017), to learn more predictive embeddings. See (Wang et al. 2017) for a thorough review of KG embedding techniques.\n\nGiven the power of logic rules in knowledge acquisition and inference, combining KG embedding with logic rules becomes a focus of current research.  and  devised pipelined frameworks which use logic rules to further refine predictions made by embedding models. In their work, rules will not help to learn better embeddings. Rockt\u00e4schel et al. (2015) and Guo et al. (2016) then tried to learn KG embeddings jointly from triples and propositionalized FOL rules. Demeester et al. (2016) further proposed lifted rule injection to avoid the costly propositionalization. These joint models, however, made a one-time injection of logic rules, ignoring the interactive nature between embedding learning and logical inference. Moreover, they can only handle hard rules which are usually manually created or validated.\n\nBesides logic rules, relation paths which can be regarded as Horn clauses and get a strong connection to logical inference (Gardner, Talukdar, and Mitchell 2015), have also been studied in KG embedding (Neelakantan, Roth, and McCallum 2015;Lin et al. 2015a;Guu, Miller, and Liang 2015). But in these methods, relation paths are incorporated, again, in a one-time manner. Our approach, in contrast, iteratively injects knowledge contained in logic rules into KG embedding, and is able to handle soft rules with various confidence levels extracted automatically from KGs.\n\nCombining logic rules with distributed representations is also an active research topic in other contexts outside KGs. Faruqui et al. (2014) tried to inject ontological knowledge from WordNet into word embeddings. Vendrov et al. (2015) introduced order-embedding to model the partial order structure of hypernymy, textual entailment, and image caption-ing.  proposed to enhance various types of neural networks with FOL rules. All these studies demonstrate the capability of logic rules to enhance distributed representation learning.\n\n\nRule-Guided Knowledge Graph Embedding\n\nThis section introduces RUle-Guided Embedding (RUGE), a novel paradigm of KG embedding with iterative guidance from soft rules. RUGE enables an embedding model to learn simultaneously from labeled triples, unlabeled triples, and soft rules in an iterative manner. During each iteration, the model alternates between a soft label prediction stage and an embedding rectification stage. Fig. 1 sketches this overall framework. In what follows, we first describe our learning resources, and then detail the two alternating stages.\n\n\nLearning Resources\n\nSuppose we are given a KG with a set of triples observed, i.e., O = {(e i , r k , e j )}. Each triple is composed of two entities e i , e j \u2208 E and their relation r k \u2208 R, where E and R are the sets of entities and relations respectively. We obtain our learning resources (i.e., labeled triples, unlabeled triples, and soft rules) and model them as follows. Labeled Triples. We take the triples observed in O as positive ones. For each positive triple (e i , r k , e j ), we randomly corrupt the head e i or the tail e j , to form a negative triple\n(e i , r k , e j ) or (e i , r k , e j ), where e i \u2208 E \\ {e i } and e j \u2208 E \\ {e j }.\nWe denote a labeled triple as x , and associate with it a label y = 1 if x is positive, and y = 0 otherwise. Let L = {(x , y )} denote the set of these labeled triples (along with their labels). Unlabeled Triples. Besides the labeled triples, we collect a set of unlabeled triples U = {x u }, where x u = (e i , r k , e j ) indicates an unlabeled triple. In fact, all the triples that have not been observed in O can be taken as unlabeled ones. But in this paper, we consider only those encoded in the conclusion of a soft rule, as detailed below. Soft Rules. We also consider a set of FOL rules with different confidence levels, denoted as F = {(f p , \u03bb p )} P p=1 . Here, f p is the p-th logic rule defined over the given KG, represented, e.g., in the form of \u2200x, y : (x, r s , y) \u21d2 (x, r t , y), stating that two entities linked by relation r s might also be linked by relation r t . The left-hand side of the implication \"\u21d2\" is called the premise, and the right-hand side the conclusion. In this paper, we restrict f p to be a Horn clause rule, where the conclusion contains only a single atom and the premise is a conjunction of several atoms. The confidence level of rule f p is denoted as \u03bb p \u2208 [0, 1]. Rules with higher confidence levels are more likely to hold, and a confidence level of \u03bb p = 1 indicates a hard rule which always holds with no exception. Such rules as well as their confidence levels can be extracted automatically from the KG (with the observed triple set O as input), by using modern rule mining systems like AMIE and AMIE+ (Gal\u00e1rraga et al. 2013;Gal\u00e1rraga et al. 2015).\n\nWe then propositionalize these rules to get their groundings. Here a grounding is the logical expression with all variables instantiated with concrete entities in E. For instance, a universally quantified rule \u2200x, y : (x, BornInCountry, y) \u21d2 (x, Nationality, y) could be instantiated with two entities EmmanuelMacron and France, and gives a resultant grounding (EmmanuelMacron, BornInCountry, France) \u21d2 (EmmanuelMacron, Nationality, France). Obviously, there could be a huge number of groundings, especially given a large entity vocabulary E. In this paper, to maximize the utility for knowledge acquisition and inference, we take as valid groundings only those where premise triples are observed in O while conclusion triples are not. That means the aforementioned grounding will be considered as valid if the triple\n(EmmanuelMacron, BornInCountry, France) \u2208 O but (EmmanuelMacron, Nationality, France) / \u2208 O. For each FOL rule f p , let G p = {g pq } Qp q=1\ndenote the set of its valid groundings. All the premise triples of g pq are contained in O, but the single conclusion triple is not. These conclusion triples are further used to construct our unlabeled triple set U. That means, our unlabeled triples are those which are not directly observed in the KG but could be inferred by the rules with high probabilities. Modeling Triples and Rules. Given the labeled triples L, unlabeled triples U, and the valid groundings of FOL rules G = {G p } P p=1 , we discuss how to model these triples and rules in the context of KG embedding. To model triples, we follow ComplEx (Trouillon et al. 2016), a recently proposed method which is simple and efficient while achieving stateof-the-art predictive performance. Specifically, we assume entities and relations to have complex-valued vector embeddings. Given a triple (e i , r k , e j ) \u2208 E \u00d7R\u00d7E, we score it by a multi-linear dot product:\n\u03b7 ijk = Re( e i , r k ,\u0113 j ) = Re( m [e i ] m [r k ] m [\u0113 j ] m ), (1)\nwhere e i , e j , r k \u2208 C d are the complex-valued vector embeddings associated with e i , e j , and r k , respectively;\u0113 j is the conjugate of e j ; [\u00b7] m is the m-th entry of a vector; and Re(\u00b7) means taking the real part of a complex value. We further introduce a mapping function \u03c6 : E\u00d7R\u00d7E \u2192 (0, 1), so as to map the score \u03b7 ijk to a continuous truth value which lies in the range of (0, 1), i.e., \u03c6(e i , r k , e j ) = \u03c3(\u03b7 ijk ) = \u03c3 Re( e i , r k ,\u0113 j ) , (2) where \u03c3(x) = 1/(1 + exp(\u2212x)) denotes the sigmoid function. Triples with higher truth values are more likely to hold.\n\nTo model propositionalized rules (i.e. groundings), we use t-norm based fuzzy logics (H\u00e1jek 1998). The key idea is to model the truth value of a propositionalized rule as a composition of the truth values of its constituent triples, through specific logical connectives (e.g. \u2227 and \u21d2). For instance, the truth value of a grounded rule (e u , r s , e v ) \u21d2 (e u , r t , e v ) will be determined by the truth values of the two triples (e u , r s , e v ) and (e u , r t , e v ), via a composition defined by logical implication. We follow (Guo et al. 2016) and define the compositions associated with logical conjunction (\u2227), disjunction (\u00ac), and negation (\u00ac) as:\n\u03c0(a \u2227 b) = \u03c0(a) \u00b7 \u03c0(b),(3)\u03c0(a \u2228 b) = \u03c0(a) + \u03c0(b) \u2212 \u03c0(a) \u00b7 \u03c0(b),(4)\n\u03c0(\u00aca) = 1 \u2212 \u03c0(a).\n\nHere, a and b are two logical expressions, which can either be single triples or be constructed by combining triples with logical connectives; and \u03c0(a) is the truth value of a, indicating to what degree the logical expression is true. If a is a single triple, say (e i , r k , e j ), we have \u03c0(a) = \u03c6(e i , r k , e j ), as defined in Eq.\n\n(2). Given these compositions, the truth value of any logical expression can be calculated recursively (Guo et al. 2016), e.g., \u03c0(a \u21d2 b) = \u03c0(\u00aca \u2228 b) = \u03c0(a) \u00b7 \u03c0(b) \u2212 \u03c0(a) + 1. (6) Logical expressions with higher truth values have greater degrees to be true. Let \u0398 = {e} e\u2208E \u222a {r} r\u2208R denote the set of all entity and relation embeddings. The proposed approach, RUGE, then aims to learn these embeddings by using the labeled triples L, unlabeled triples U, and valid groundings {G p } P p=1 in an iterative manner, where each iteration alternates between a soft label prediction stage and an embedding rectification stage.\n\n\nSoft Label Prediction\n\nThis stage is to use currently learned embeddings and propositionalized rules to predict soft labels for unlabeled triples. Specifically, let n be the iteration index, and \u0398 (n\u22121) the set of current embeddings learned from the previous iteration. Recall that we are given a set of P FOL rules with their confidence levels F = {(f p , \u03bb p )} P p=1 , and each FOL rule f p has Q p valid groundings G p = {g pq } Qp q=1 . Our aim is to predict a soft label s(x u ) \u2208 [0, 1] for each unlabeled triple x u \u2208 U, by using the current embeddings \u0398 (n\u22121) and all the groundings G = {G p } P p=1 . To do so, we solve a rule-constrained optimization problem, which projects truth values of unlabeled triples computed by the current embeddings into a subspace constrained by the rules. The key idea here is to find optimal soft labels that stay close to these truth values, while at the same time fitting the rules. For the first property, given each unlabeled triple x u \u2208 U, we calculate its truth value \u03c6(x u ) using the current embeddings via Eq. (2), and require the soft label s(x u ) to stay close to this truth value. We measure the closeness between s(x u ) and \u03c6(x u ) with a squared loss, and try to minimize it. For the second property, we further impose rule constraints onto the soft labels S = {s(x u )}. Specifically, for each FOL rule f p and each of its groundings g pq , we expect g pq to be true, i.e., \u03c0(g pq |S) = 1 with confidence \u03bb p . Here, \u03c0(g pq |S) is the conditional truth value of g pq given the soft labels, which can be calculated recursively with the logical compositions defined in Eq. (3) to Eq. (5). Take g pq := (e u , r s , e v ) \u21d2 (e u , r t , e v ) as an example, where the premise (e u , r s , e v ) is directly observed in O, and the conclusion (e u , r t , e v ) is an unlabeled triple included in U. The conditional truth value of g pq can then be calculated as:\n\n\u03c0(g pq |S) = \u03c6(e u , r s , e v )\u00b7s(e u , r t , e v )\u2212\u03c6(e u , r s , e v )+1, (7) where \u03c6(e u , r s , e v ) is a truth value defined by Eq. (2) with the current embeddings; and s(e u , r t , e v ) is a soft label to be predicted. Comparing Eq. (7) with Eq. (6), we can see that during the calculation of \u03c0(g pq |S), for any unlabeled triple, we use the soft label s(\u00b7) rather than the truth value \u03c6(\u00b7), so as to better impose rule constraints onto the soft labels S.\n\nCombining the two properties together and further allowing slackness for rule constraints, we finally get the following optimization problem:\nmin S,\u03be 1 2 xu\u2208U (s(x u ) \u2212 \u03c6(x u )) 2 + C p,q \u03be pq ,\ns.t. \u03bb p (1 \u2212 \u03c0(g pq |S)) \u2264 \u03be pq , q = 1,\u00b7 \u00b7 \u00b7, Q p , p = 1,\u00b7 \u00b7 \u00b7, P, \u03be pq \u2265 0, q = 1,\u00b7 \u00b7 \u00b7, Q p , p = 1,\u00b7 \u00b7 \u00b7, P, 0 \u2264 s(x u ) \u2264 1, \u2200s(x u ) \u2208 S, (8) where \u03be pq is a slack variable and C the penalty coefficient. Note that confidence levels of rules (i.e. \u03bb p 's) are encoded in the constraints, making our approach capable of handling soft rules. Rules with higher confidence levels show less tolerance for violating the constraints. This optimization problem is convex, and can be solved efficiently with its closedform solution:\ns(x u ) = \u03c6(x u ) + C p,q \u03bb p \u2207 s(xu) \u03c0(g pq |S) 1 0 (9)\nfor each x u \u2208 U. Here, \u2207 s(xu) \u03c0(g pq |S) means the gradient of \u03c0(g pq |S) w.r.t s(x u ), which is a constant w.r.t. S, 1 and [x] 1 0 = min(max(x, 0), 1) is a truncation function enforcing the solutions to stay within [0, 1]. We provide the proof of convexity and detailed derivation as supplementary materials. Soft labels obtained in this way shall 1) stay close to the predictions made by the current embedding model, and 2) fit the rules as well as possible.\n\n\nEmbedding Rectification\n\nThis stage is to integrate both labeled and unlabeled triples (with hard and soft labels respectively) to update current embeddings. Specifically, we are given a set of labeled triples with their hard labels specified in {0, 1}, i.e., L = {(x , y )}, and also a set of unlabeled triples encoded in propositionalized rules, i.e., U = {x u }. Each unlabeled triple x u has a soft label s(x u ) \u2208 [0, 1], predicted by Eq. (9). We would like to use these labeled and unlabeled triples to learn the updated embeddings \u0398 (n) . Here n is the iteration index.\n\nTo this end, we minimize a global loss over L and U, so as to find embeddings which can predict the true hard labels for triples contained in L, while imitating the soft labels for those contained in U. The optimization problem is:\nmin \u0398 1 |L| L (\u03c6(x ), y ) + 1 |U| U (\u03c6(x u ), s(x u )),(10)\nwhere (x, y) = \u2212y log x \u2212 (1 \u2212 y) log(1 \u2212 x) is the cross entropy; and \u03c6(\u00b7) is a function w.r.t. \u0398 defined by Eq. (2). We further impose L 2 regularization on the parameters \u0398 to avoid overfitting. Gradient descent algorithms can be used to solve this problem. Embeddings learned in this way will 1) be compatible with all the labeled triples, and 2) absorb rule knowledge carried by the unlabeled triples.\n\n\nWhole Procedure\n\nAlgorithm 1 summarizes the iterative learning procedure of our approach. To enable efficient learning, we use an online scheme in mini-batch mode. At each iteration, we sample a mini-batch L b , U b , and G b from the labeled triples L, unlabeled triples U, and propositionalized rules G, respectively (line 3). 2 Soft label prediction and embedding rectification are then conducted locally on these mini-batches (line 4 and line 5 respectively). This iterative procedure captures the interactive nature between embedding learning and logical inference: given current embeddings, logic rules can be used to perform approximate inference and predict soft labels for unlabeled triples; these newly labeled triples carry rich rule knowledge and will in turn help to learn better embeddings. In this way, knowledge contained in logic rules can be fully transferred into the learned embeddings. Note also that our approach is flexible enough to handle soft rules with various confidence levels extracted automatically from the KG.\n\n\nDiscussions\n\nWe further analyze the space and time complexity, and discuss possible extensions of our approach.\n\nComplexity. RUGE follows ComplEx to represent entities and relations as complex-valued vectors, hence has a space complexity of O(n e d + n r d) which scales linearly w.r.t. n e , n r , and d. Here, n e is the number of entities, n r the number of relations, and d the dimensionality of the embedding space. During the learning procedure, each iteration requires a time complexity of O(\u03c4 (n d + n u d)), where n /n u is the average number of labeled/unlabeled triples in a mini-batch, and \u03c4 the number of inner epochs used for embedding rectification (cf. Eq. (10)). In practice, we usually have n u n (see Table 2 for the number of labeled and unlabeled triples used on our datasets), and we can also set \u03c4 to a very small value, e.g., \u03c4 = 1. That means, RUGE has almost the same time complexity as those most efficient KG embedding techniques (e.g. ComplEx) which require O(n d) per iteration during training. 3 In addition, RUGE further requires preprocessing steps before training, i.e., rule mining and propositionalization. But these steps are performed only once, and not required during the iterations.\n\nExtensions. Our approach is quite generic and flexible. 1) The idea of iteratively injecting logic rules can be applied to enhance a wide variety of embedding models, as long as an appropriate scoring function is accordingly designed, e.g., the one defined in Eq. (1) by ComplEx. 2) Various types of rules can be incorporated as long as they can be modeled by the logical compositions defined in Eq. (3) to Eq. (5), and we can even use other types of t-norm fuzzy logics to define such compositions. 3) Rules with different confidence levels can be handled in a unified manner. \nb / U b / G b from L / U / G 4: S b \u2190 SoftLabelPrediction (U b , G b , \u0398 (n\u22121) ) cf. Eq. (9) 5: \u0398 (n) \u2190 EmbeddingRectification (L b , U b , S b ) cf.\nEq. (10) 6: end for Ensure: \u0398 (N )\n\n\nExperiments\n\nWe evaluate RUGE in the link prediction task. This task is to complete a triple (e i , r k , e j ) with e i or e j missing, i.e., to predict e i given (r k , e j ) or e j given (e i , r k ). Datasets. We use two datasets: FB15K and YAGO37. The former is a subgraph of Freebase containing 1,345 relations and 14,951 entities, released by . 4 The latter is extracted from the core facts of YAGO3. 5 During the extraction, entities appearing less than 10 times are discarded. The final dataset consists of 37 relations and 123,189 entities. Triples on both datasets are split into training, validation, and test sets, used for model training, hyperparameter tuning, and evaluation, respectively. We use the original split for FB15K, and draw a split of 989,132/50,000/50,000 triples for YAGO37.\n\nNote that on both datasets, the training sets contain only positive triples. Negative triples are generated using the local closed world assumption (Dong et al. 2014). This negative sampling procedure is performed at runtime for each batch of training positive triples. Such positive and negative triples (along with their hard labels) form our labeled triple set.\n\nWe further employ AMIE+ (Gal\u00e1rraga et al. 2015) 6 to automatically extract Horn clause rules from each dataset, with the training set as input. To enable efficient extraction, we consider rules with length not longer than 2 and confidence levels not less than 0.8. 7 The length of a Horn clause rule is the number of atoms appearing in its premise, e.g., \u2200x, y : (x, BornInCountry, y) \u21d2 (x, Nationality, y) has the length of 1. And the confidence threshold of 0.8 leads to the best performance on both datasets (detailed later). Using this setting, we extract 454 (universally quantified) Horn clause rules from FB15K, and 16 such rules from YAGO37. Table 1 shows some examples with their confidence levels.\n\nThen, we instantiate these rules with concrete entities, i.e., propositionalization. Propositionalized rules whose premise triples are all contained in the training set (while conclusion Table 1: Horn clause rules with confidence levels extracted by AMIE+ from FB15K (top) and YAGO37 (bottom). /location/people born here(x,y)\u21d2/people/place of birth(y,x) 1.00 /director/film(x,y)\u21d2/film/directed by (y,x) 0.99 /film/directed by(x,y)\u2227/person/language(y,z)\u21d2/film/language(x,z) 0.88\n\nisMarriedTo(x,y)\u21d2isMarriedTo(y,x) 0.97 hasChild(x,y)\u2227isCitizenOf(y,z)\u21d2isCitizenOf(x,z) 0.94 playsFor(x,y)\u21d2isAffiliatedTo(x,y) 0.86 Table 2: Statistics of datasets, where n e /n r denotes the number of entities/relations, n /n u /n g is the number of labeled triples/unlabeled triples/valid groundings used for training, and n v /n t denotes the number of validation/test triples. triples are not) are taken as valid groundings and used during embedding learning. We obtain 96,724 valid groundings on FB15K and 72,670 on YAGO37. Conclusion triples of these valid groundings are further collected to form our unlabeled triple set. We finally get 74,707 unlabeled triples on FB15K and 69,680 on YAGO37. Table 2 provides some statistics of the two datasets. Evaluation Protocol. To evaluate the performance in link prediction, we follow the standard protocol used in . For each test triple (e i , r k , e j ), we replace the head entity e i with each entity e i \u2208 E, and calculate the score for (e i , r k , e j ). Ranking these scores in descending order, we get the rank of the correct entity e i . Similarly, we can get another rank by replacing the tail entity. Aggregated over all test triples, we report three metrics: 1) the mean reciprocal rank (MRR), 2) the median of the ranks (MED), and 3) the proportion of ranks no larger than n (HITS@N). During this ranking process, we remove corrupted triples which already exist in either the training, validation, or test set, since they themselves are true triples. This corresponds to the \"filtered\" setting in . Comparison Settings. We compare RUGE with four stateof-the-art basic embedding models, including TransE ), DistMult (Yang et al. 2015, HolE (Nickel, Rosasco, and Poggio 2016), and ComplEx (Trouillon et al. 2016). These basic models rely only on triples observed in a KG and use no rules. We further take PTransE (Lin et al. 2015a) and KALE (Guo et al. 2016) as additional baselines. Both of them are extensions of TransE, with the former integrating relation paths (Horn clauses), and the latter FOL rules (hard rules) in a one-time injection manner. In contrast, RUGE incorporates soft rules and transfers rule knowledge into KG embedding in an iterative manner. We use the code provided by Trouillon et al. (2016) TransE, DistMult, and ComplEx, and reimplement HolE so that all these four basic models share the identical mode of optimization, i.e., SGD with AdaGrad (Duchi, Hazan, and Singer 2011) and gradient normalization. As such, we reproduce the results of TransE, DistMult, and ComplEx reported on FB15K (Trouillon et al. 2016), and improve the results of HolE substantially compared to those reported in the original paper (Nickel, Rosasco, and Poggio 2016). 9 The code for PTransE is provided by its authors. 10 We implement KALE and RUGE in Java, both using SGD with AdaGrad and gradient normalization to facilitate a fair comparison. There are two types of loss functions that could be used for these baselines, i.e., the logistic loss or the pairwise ranking loss (Nickel, Rosasco, and Poggio 2016). Trouillon et al. (2016) have recently demonstrated that the logistic loss generally performs better than the pairwise ranking loss, except for TransE. So, for TransE and its extensions (PTransE and KALE) we use the pairwise ranking loss, and for all the other baselines we use the logistic loss. To extract relation paths for PTransE, we follow the optimal configuration reported in (Lin et al. 2015a), where paths constituted by at most 3 relations are included. For KALE and RUGE, we use the same set of propositionalized rules to make it a fair comparison. 11 For all the methods, we create 100 mini-batches on each dataset, and tune the embedding dimensionality d in {50, 100, 150, 200}, the number of negatives per positive triple \u03b1 in {1, 2, 5, 10}, the initial learning rate \u03b3 in {0.01, 0.05, 0.1, 0.5, 1.0}, and the L 2 regularization coefficient \u03bb in {0.001, 0.003, 0.01, 0.03, 0.1}. For TransE and its extensions which use the pairwise ranking loss, we further tune the margin \u03b4 in {0.1, 0.2, 0.5, 1, 2, 5, 10}. The slackness penalty C in RUGE (cf. Eq. (8)) is selected from {0.001, 0.01, 0.1, 1}, and the number of inner iterations (cf. Eq. (10)) is fixed to \u03c4 = 1. Best models are selected by early stopping on the validation set (monitoring MRR), with at most 1000 iterations over the training set. The optimal configurations for RUGE are: d = 200, \u03b1 = 10, \u03b3 = 0.5, \u03bb = 0.01, C = 0.01 on FB15K; and d = 150, \u03b1 = 10, \u03b3 = 1.0, \u03bb = 0.003, C = 0.01 on YAGO37. Link Prediction Results. Table 3 shows the results of these methods on the test sets of FB15K and YAGO37. The results indicate that RUGE significantly and consistently outperforms all the baselines on both datasets and in all metrics. It beats not only the four basic models which use triples alone (TransE, DistMult, HolE, and ComplEx), but also PTransE and KALE which further incorporate logic rules (or relation paths) in a one-time injection manner. This demonstrates the superiority of injecting logic rules into KG embedding, particularly in an iterative manner. Compared to the best performing baseline ComplEx (this is also the model based on which RUGE is designed), RUGE achieves an improvement of 11%/18% in MRR/HITS@1 on FB15K, and an improvement of 3%/6% on YAGO37. The improvements on FB15K Table 3: Link prediction results on the test sets of FB15K and YAGO37. As baselines, rows 1-4 are the four basic models which use triples alone, and rows 5-6 further integrate logic rules (or relation paths) in a one-time injection manner. are more substantial than those on YAGO37. The reason is probably that FB15K contains more relations from which a good range of rules can be extracted (454 universally quantified rules from FB15K, and 16 from YAGO37). Influence of Confidence Levels. We further investigate the influence of the threshold of rules' confidence levels used in RUGE. To do so, we fix all the hyperparameters to the optimal configurations determined by the previous experiment, and vary the confidence threshold in [0.1, 1] with a step 0.05. Fig. 2 shows MRR achieved by RUGE with various thresholds on the test set of FB15K. We can see that the threshold of 0.8 is a good tradeoff and indeed performs best. A threshold higher than that will reduce the number of rules that can be extracted, while a one lower than that might introduce too many less credible rules. Both hurt the performance. However, even so, RUGE outperforms ComplEx by a large margin, with the threshold set in a broad range of [0.35, 0.9]. This observation indicates that soft rules, even those with moderate confidence levels, are highly beneficial to KG embedding despite their uncertainties. Comparison of Runtime. Finally, we compare RUGE with ComplEx, PTransE, and KALE in their runtime. 12 ComplEx is a basic model which only requires model training. RUGE as well as the other two baselines further require preprocessing of rule/path extraction and propositionalization. Table 4 lists the runtime of these methods required for each step on FB15K and YAGO37. Here, to facilitate a fair comparison, 12 The other three baselines are implemented in Python and much slower. So they are not considered here. we set d = 200 (embedding dimensionality) and \u03b1 = 2 (number of negatives per positive triple) for all the methods. Other hyperparameters are fixed to their optimal configurations determined in link prediction. We can see that RUGE is still quite efficient despite integrating additional rules. The average training time per iteration increases from 11.4 to 14.1 on FB15K, and from 49.5 to 55.2 on YAGO37. The preprocessing steps, although performed only once, are also highly efficient, requiring much less time compared to PTransE.\n\n\nConclusion\n\nThis paper proposes a novel paradigm that learns entity and relation embeddings with iterative guidance from soft rules, referred to as RUGE. It enables an embedding model to learn simultaneously from labeled triples, unlabeled triples, and soft rules in an iterative manner. Each iteration alternates between 1) a soft label prediction stage which predicts soft labels for unlabeled triples using currently learned embeddings and soft rules, and 2) an embedding rectification stage which further integrates both labeled and unlabeled triples to update current embeddings. This iterative procedure may better transfer the knowledge contained in logic rules into the learned embeddings. Link prediction results on Freebase and YAGO show that RUGE achieves significant and consistent improvements over state-of-the-art baselines. Moreover, RUGE demonstrates the usefulness of automatically extracted soft rules. Even those with moderate confidence levels can be highly beneficial to KG embedding.\n\n\nLabeled triples L = {(x , y )} Unlabeled triples U = {xu} FOL rules F={(fp, \u03bbp)} and their groundings G={gpq} 1: Randomly initialize entity and relation embeddings \u0398 (0) 2: for n = 1 : N do 3: Sample a mini-batch L\n\n\nFigure 2: MRR achieved by RUGE with different confidence thresholds on the test set of FB15K.FB15K \nYAGO37 \n\nHITS@N \nHITS@N \n\nMethod \nMRR \nMED \n1 \n3 \n5 \n10 \nMRR \nMED \n1 \n3 \n5 \n10 \n\nTransE \n0.400 \n4.0 \n0.246 \n0.495 \n0.576 \n0.662 \n0.303 \n13.0 \n0.218 \n0.336 \n0.387 \n0.475 \nDistMult \n0.644 \n1.0 \n0.532 \n0.730 \n0.769 \n0.812 \n0.365 \n6.0 \n0.262 \n0.411 \n0.493 \n0.575 \nHolE \n0.600 \n2.0 \n0.485 \n0.673 \n0.722 \n0.779 \n0.380 \n7.0 \n0.288 \n0.420 \n0.479 \n0.551 \nComplEx \n0.690 \n1.0 \n0.598 \n0.756 \n0.793 \n0.837 \n0.417 \n4.0 \n0.320 \n0.471 \n0.533 \n0.603 \nPTransE \n0.679 \n1.0 \n0.565 \n0.768 \n0.810 \n0.855 \n0.403 \n9.0 \n0.339 \n0.444 \n0.473 \n0.506 \nKALE \n0.523 \n2.0 \n0.383 \n0.616 \n0.683 \n0.762 \n0.321 \n9.0 \n0.215 \n0.372 \n0.438 \n0.522 \n\nRUGE \n0.768 \n1.0 \n0.703 \n0.815 \n0.836 \n0.865 \n0.431 \n4.0 \n0.340 \n0.482 \n0.541 \n0.603 \n\n\n\nTable 4 :\n4Runtime (in sec.) on FB15K and YAGO37. Extr. is the time required for rule/path extraction, Prop. for propositionalization, and Lean. for training per iteration.FB15K \nYAGO37 \n\nMethod \nExtr. Prop. Learn. \nExtr. Prop. Learn. \n\nComplEx \n-\n-\n11.4 \n-\n-\n49.5 \nPTransE \n868.4 \n-\n46.5 \n13939.5 \n-\n23.8 \nKALE \n43.1 \n4.0 \n4.8 \n337.5 13.8 \n27.5 \nRUGE \n43.1 \n4.0 \n14.1 \n337.5 13.8 \n55.2 \n\n\nNote that each gpq contains only a single unlabeled triple, i.e., the conclusion triple. Take \u03c0(gpq|S) defined in Eq. (7) for example. In this case, s(xu) = s(eu, rt, ev) is the soft label to be predicted and \u2207 s(xu) \u03c0(gpq|S) = \u03c6(eu, rs, ev) is a constant w.r.t. S.\nWe first sample L b from L. G b is then constructed by those whose premise triples are all contained in L b but conclusion triples are not. These conclusion triples are further used to construct U b .3 Such techniques often use SGD in mini-batch mode for training, and sample a mini-batch of n labeled triples at each iteration.\nhttps://everest.hds.utc.fr/doku.php?id=en:smemlj12 5 http://www.mpi-inf.mpg.de/departments/databases-andinformation-systems/research/yago-naga/yago/downloads/ 6 https://www.mpi-inf.mpg.de/departments/databases-andinformation-systems/research/yago-naga/amie/ 7 AMIE+ provides two types of confidence, i.e. standard confidence and PCA confidence. This paper uses PCA confidence.\nHolE in its original implementation uses SGD with AdaGrad, but no gradient normalization. 10 https://github.com/thunlp/KB2E 11 KALE takes all these groundings as hard rules. This approximation works quite well with an appropriate confidence threshold.\n= min(max(x, 0), 1) is a truncation function enforcing the solutions to stay within [0, 1].\nAcknowledgmentsThe authors would like to thank all the reviewers for their insightful and valuable suggestions, which significantly improve the quality of this paper. This work is supported by the NationalA Solving the Soft Label Prediction ProblemWe discuss how to solve the soft label prediction problem, i.e., Eq. (8) in the submission. We first review the problem. Recall that we are given a set of P FOL rules with their confidence levels F = {(f p , \u03bb p )} P p=1 , and each FOL rule f p has Q p valid groundings G p = {g pq } Qp q=1 . The aim is to predict a soft label s(x u ) \u2208 [0, 1] for each unlabeled triple x u \u2208 U, by using the current embeddings \u0398 (n\u22121) and all the groundings G = {G p } P p=1 . The optimization problem is:where S = {s(x u )} is the set of soft labels; \u03be = {\u03be pq } the set of slack variables; \u03c6(x u ) the truth value of x u calculated by the current embeddings; \u03c0(g pq |S) the conditional truth value of g pq given the soft labels; and C the penalty coefficient. Then, we prove this optimization problem is convex and has closed-form solutions.Proof of ConvexityIt is easy to see that the objective function is convex, as it is quadratic w.r.t. each s(x u ) and linear w.r.t. each \u03be pq . As for the constraints, note that each valid grounding g pq contains only a single unlabeled triple, i.e., the triple in the conclusion. So the conditional truth value \u03c0(g pq |S) is a linear function w.r.t. a single s(x u ). Take g pq := (e u , r s , e v ) \u21d2 (e u , r t , e v ) as an example, where the premise (e u , r s , e v ) is directly observed, and the conclusion (e u , r t , e v ) is an unlabeled triple with its soft label s(e u , r t , e v ) to be predicted. The conditional truth value can be calculated as:which is a linear function w.r.t. s(e u , r t , e v ). In this case, all the constraints are linear functions. So the whole optimization problem is convex. * Corresponding author: Quan Wang (wangquan@iie.ac.cn).Closed-Form SolutionsThe optimization problem of Eq. (A-1) can be equivalently written as:where [x] + = max(x, 0). Given the logical compositions defined in Eq.(3) to Eq. (5) in the submission, and the constraints \u03c6(\u00b7) \u2208 [0, 1], s(\u00b7) \u2208 [0, 1] as well, we will have \u03c0(g pq |S) \u2264 1 for any g pq . The objective function of Eq. (A-2) then becomes:Recall that each \u03c0(g pq |S) is a linear function w.r.t. a single s(x u ). So P is an upward parabola w.r.t. each s(x u ), with the axis of symmetry given as:Here, \u2207 s(xu) \u03c0(g pq |S) means the gradient of \u03c0(g pq |S) w.r.t s(x u ), which is a constant w.r.t. S. By further considering the constraint 0 \u2264 s(x u ) \u2264 1 for each s(x u ) \u2208 S, we get the optimal solutions:where [x] 1\nFreebase: A collaboratively created graph database for structuring human knowledge. [ References, Bollacker, SIGMOD. NIPSReferences [Bollacker et al. 2008] Bollacker, K.; Evans, C.; Paritosh, P.; Sturge, T.; and Taylor, J. 2008. Freebase: A collaboratively created graph database for structuring human knowledge. In SIGMOD, 1247-1250. [Bordes et al. 2013] Bordes, A.; Usunier, N.; Garc\u00eda-Dur\u00e1n, A.; Weston, J.; and Yakhnenko, O. 2013. Translating embeddings for modeling multi-relational data. In NIPS, 2787-2795.\n\nA semantic matching energy function for learning with multi-relational data. Bordes, EMNLP. 94AAAI[Bordes et al. 2014] Bordes, A.; Glorot, X.; Weston, J.; and Bengio, Y. 2014. A semantic matching energy function for learning with multi-relational data. MACH LEARN 94(2):233-259. [Carlson et al. 2010] Carlson, A.; Betteridge, J.; Kisiel, B.; Settles, B.; Hruschka Jr, E. R.; and Mitchell, T. M. 2010. Toward an archi- tecture for never-ending language learning. In AAAI, 1306-1313. [Demeester, Rockt\u00e4schel, and Riedel 2016] Demeester, T.; Rockt\u00e4schel, T.; and Riedel, S. 2016. Lifted rule injection for relation embeddings. In EMNLP, 1389-1399.\n\nKnowledge vault: A web-scale approach to probabilistic knowledge fusion. Dong, SIGKDD. [Dong et al. 2014] Dong, X.; Gabrilovich, E.; Heitz, G.; Horn, W.; Lao, N.; Murphy, K.; Strohmann, T.; Sun, S.; and Zhang, W. 2014. Knowledge vault: A web-scale approach to probabilistic knowl- edge fusion. In SIGKDD, 601-610.\n\nAMIE: Association rule mining under incomplete evidence in ontological knowledge bases. Hazan Duchi, J Singer ; Duchi, E Hazan, Y Singer, M Faruqui, J Dodge, S K Jauhar, C Dyer, E Hovy, N A Smith, L A Gal\u00e1rraga, C Teflioudi, K Hose, F M Suchanek, arXiv:1411.4166WWW. 12Adaptive subgradient methods for online learning and stochastic optimizationDuchi, Hazan, and Singer 2011] Duchi, J.; Hazan, E.; and Singer, Y. 2011. Adaptive subgradient methods for online learning and stochastic optimization. J MACH LEARN RES 12(Jul):2121-2159. [Faruqui et al. 2014] Faruqui, M.; Dodge, J.; Jauhar, S. K.; Dyer, C.; Hovy, E.; and Smith, N. A. 2014. Retrofitting word vectors to semantic lexicons. arXiv:1411.4166. [Gal\u00e1rraga et al. 2013] Gal\u00e1rraga, L. A.; Teflioudi, C.; Hose, K.; and Suchanek, F. M. 2013. AMIE: Association rule mining under incomplete evidence in ontological knowledge bases. In WWW, 413-422.\n\nFast rule mining in ontological knowledge bases with AMIE+. [ Gal\u00e1rraga, VLDB J. 246[Gal\u00e1rraga et al. 2015] Gal\u00e1rraga, L. A.; Teflioudi, C.; Hose, K.; and Suchanek, F. M. 2015. Fast rule mining in ontological knowl- edge bases with AMIE+. VLDB J 24(6):707-730.\n\nCombining vector space embeddings with symbolic logical inference over open-domain text. Talukdar Gardner, M Gardner, P Talukdar, T Mitchell, AAAI Spring Symposium Series. Gardner, Talukdar, and Mitchell 2015] Gardner, M.; Talukdar, P.; and Mitchell, T. 2015. Combining vector space embeddings with symbolic logical inference over open-domain text. In AAAI Spring Symposium Series, 61-65.\n\nSemantically smooth knowledge graph embedding. [ Guo, ACL. [Guo et al. 2015] Guo, S.; Wang, Q.; Wang, L.; Wang, B.; and Guo, L. 2015. Semantically smooth knowledge graph embedding. In ACL, 84-94.\n\nJointly embedding knowledge graphs and logical rules. [ Guo, EMNLP. [Guo et al. 2016] Guo, S.; Wang, Q.; Wang, L.; Wang, B.; and Guo, L. 2016. Jointly embedding knowledge graphs and logical rules. In EMNLP, 192-202.\n\nTraversing knowledge graphs in vector space. Miller Guu, K Liang ; Guu, J Miller, P Liang, EMNLP. Guu, Miller, and Liang 2015] Guu, K.; Miller, J.; and Liang, P. 2015. Traversing knowledge graphs in vector space. In EMNLP, 318-327.\n\nHarnessing deep neural networks with logic rules. P H\u00e1jek, Hu, ACL. H\u00e1jekThe metamathematics of fuzzy logic. Kluwer[H\u00e1jek 1998] H\u00e1jek, P. 1998. The metamathematics of fuzzy logic. Kluwer. [Hu et al. 2016] Hu, Z.; Ma, X.; Liu, Z.; Hovy, E.; and Xing, E. 2016. Harnessing deep neural networks with logic rules. In ACL, 2410-2420.\n\nModeling relation paths for representation learning of knowledge bases. Lin, EMNLP. [Lin et al. 2015a] Lin, Y.; Liu, Z.; Luan, H.; Sun, M.; Rao, S.; and Liu, S. 2015a. Modeling relation paths for representation learning of knowledge bases. In EMNLP, 705-714.\n\nLearning entity and relation embeddings for knowledge graph completion. Lin, AAAI. [Lin et al. 2015b] Lin, Y.; Liu, Z.; Sun, M.; Liu, Y.; and Zhu, X. 2015b. Learning entity and relation embeddings for knowledge graph completion. In AAAI, 2181-2187.\n\nProbabilistic reasoning via deep learning: Neural association models. [ Liu, AAAI. Neelakantan, Roth, and McCallum38ACL[Liu et al. 2016] Liu, Q.; Jiang, H.; Evdokimov, A.; Ling, Z.-H.; Zhu, X.; Wei, S.; and Hu, Y. 2016. Probabilistic reasoning via deep learning: Neural association models. [Miller 1995] Miller, G. A. 1995. WordNet: A lexical database for English. COMMUN ACM 38(11):39-41. [Neelakantan, Roth, and McCallum 2015] Neelakantan, A.; Roth, B.; and McCallum, A. 2015. Compositional vector space mod- els for knowledge base completion. In ACL, 156-166. [Nickel, Rosasco, and Poggio 2016] Nickel, M.; Rosasco, L.; and Poggio, T. 2016. Holographic embeddings of knowledge graphs. In AAAI, 1955-1961.\n\nA three-way model for collective learning on multi-relational data. Tresp Nickel, M Tresp, V Kriegel, H.-P , ICML. , Tresp, and Kriegel 2011] Nickel, M.; Tresp, V.; and Kriegel, H.-P. 2011. A three-way model for collective learning on multi-relational data. In ICML, 809-816.\n\nLow-dimensional embeddings of logic. T Rockt\u00e4schel, M Bo\u0161njak, S Singh, S Riedel, ACL Workshop on Semantic Parsing. Rockt\u00e4schel et al. 2014[Rockt\u00e4schel et al. 2014] Rockt\u00e4schel, T.; Bo\u0161njak, M.; Singh, S.; and Riedel, S. 2014. Low-dimensional embeddings of logic. In ACL Workshop on Semantic Parsing, 45-49.\n\nInjecting logical background knowledge into embeddings for relation extraction. Singh Rockt\u00e4schel, T Rockt\u00e4schel, S Singh, S Riedel, NAACL. Rockt\u00e4schel, Singh, and Riedel 2015] Rockt\u00e4schel, T.; Singh, S.; and Riedel, S. 2015. Injecting logical background knowledge into embeddings for relation extraction. In NAACL, 1119-1129.\n\nReasoning with neural tensor networks for knowledge base completion. Socher, NIPS. [Socher et al. 2013] Socher, R.; Chen, D.; Manning, C. D.; and Ng, A. Y. 2013. Reasoning with neural tensor networks for knowledge base completion. In NIPS, 926-934.\n\nYAGO: A core of semantic knowledge. Kasneci Suchanek, F M Suchanek, G Kasneci, G Weikum, WWW. Suchanek, Kasneci, and Weikum 2007] Suchanek, F. M.; Kasneci, G.; and Weikum, G. 2007. YAGO: A core of semantic knowledge. In WWW, 697-706.\n\nComplex embeddings for simple link prediction. ICML. et al. 2016] Trouillon, T.; Welbl, J.; Riedel, S.; Gaussier, E.; and Bouchard, G. 2016. Complex embeddings for simple link prediction. In ICML, 2071-2080.\n\n[ Vendrov, arXiv:1511.06361Order-embeddings of images and language. [Vendrov et al. 2015] Vendrov, I.; Kiros, R.; Fidler, S.; and Ur- tasun, R. 2015. Order-embeddings of images and language. arXiv:1511.06361.\n\nLearning first-order logic embeddings via matrix factorization. W Y Wang, W W Cohen, IJCAI. and Cohenand Cohen 2016] Wang, W. Y., and Cohen, W. W. 2016. Learning first-order logic embeddings via matrix factorization. In IJCAI, 2132-2138.\n\nKnowledge graph embedding by translating on hyperplanes. AAAI. et al. 2014] Wang, Z.; Zhang, J.; Feng, J.; and Chen, Z. 2014. Knowledge graph embedding by translating on hyperplanes. In AAAI, 1112-1119.\n\nKnowledge graph embedding: A survey of approaches and applications. IEEE TRANS KNOWL DATA ENG. 2912et al. 2017] Wang, Q.; Mao, Z.; Wang, B.; and Guo, L. 2017. Knowledge graph embedding: A survey of approaches and appli- cations. IEEE TRANS KNOWL DATA ENG 29(12):2724-2743.\n\nKnowledge base completion using embeddings and rules. Wang Wang, Q Wang, B Guo, L , IJCAI. , Wang, and Guo 2015] Wang, Q.; Wang, B.; and Guo, L. 2015. Knowledge base completion using embeddings and rules. In IJCAI, 1859-1865.\n\nLarge-scale knowledge base completion: Inferring via grounding network sampling over selected instances. [ Wei, CIKM. [Wei et al. 2015] Wei, Z.; Zhao, J.; Liu, K.; Qi, Z.; Sun, Z.; and Tian, G. 2015. Large-scale knowledge base completion: Infer- ring via grounding network sampling over selected instances. In CIKM, 1331-1340.\n\nConnecting language and knowledge bases with embedding models for relation extraction. [ Weston, EMNLP. AAAI[Weston et al. 2013] Weston, J.; Bordes, A.; Yakhnenko, O.; and Usunier, N. 2013. Connecting language and knowledge bases with embedding models for relation extraction. In EMNLP, 1366-1371. [Xiao, Huang, and Zhu 2017] Xiao, H.; Huang, M.; and Zhu, X. 2017. SSP: Semantic space projection for knowledge graph em- bedding with text descriptions. In AAAI, 3104-3110.\n\nRepresentation learning of knowledge graphs with entity descriptions. AAAI. et al. 2016] Xie, R.; Liu, Z.; Jia, J.; Luan, H.; and Sun, M. 2016. Representation learning of knowledge graphs with entity descriptions. In AAAI, 2659-2665.\n\nRepresentation learning of knowledge graphs with hierarchical types. Liu Xie, R Sun ; Xie, Z Liu, M Sun, IJCAI. Xie, Liu, and Sun 2016] Xie, R.; Liu, Z.; and Sun, M. 2016. Rep- resentation learning of knowledge graphs with hierarchical types. In IJCAI, 2965-2971.\n\nExplicit semantic ranking for academic search via knowledge graph embedding. Power Xiong, C Xiong, R Power, J Callan, WWW. Xiong, Power, and Callan 2017] Xiong, C.; Power, R.; and Callan, J. 2017. Explicit semantic ranking for academic search via knowl- edge graph embedding. In WWW, 1271-1279.\n\nEmbedding entities and relations for learning and inference in knowledge bases. Yang , ICLR. [Yang et al. 2015] Yang, B.; Yih, W.-t.; He, X.; Gao, J.; and Deng, L. 2015. Embedding entities and relations for learning and infer- ence in knowledge bases. In ICLR.\n\nCollaborative knowledge base embedding for recommender systems. SIGKDD. et al. 2016] Zhang, F.; Yuan, N. J.; Lian, D.; Xie, X.; and Ma, W.-Y. 2016. Collaborative knowledge base embedding for recommender systems. In SIGKDD, 353-362.\n", "annotations": {"author": "[{\"end\":213,\"start\":69},{\"end\":435,\"start\":214},{\"end\":541,\"start\":436},{\"end\":687,\"start\":542},{\"end\":831,\"start\":688}]", "publisher": null, "author_last_name": "[{\"end\":76,\"start\":73},{\"end\":223,\"start\":219},{\"end\":447,\"start\":443},{\"end\":550,\"start\":546},{\"end\":694,\"start\":691}]", "author_first_name": "[{\"end\":72,\"start\":69},{\"end\":218,\"start\":214},{\"end\":442,\"start\":436},{\"end\":545,\"start\":542},{\"end\":690,\"start\":688}]", "author_affiliation": "[{\"end\":143,\"start\":78},{\"end\":212,\"start\":145},{\"end\":290,\"start\":225},{\"end\":359,\"start\":292},{\"end\":434,\"start\":361},{\"end\":540,\"start\":449},{\"end\":617,\"start\":552},{\"end\":686,\"start\":619},{\"end\":761,\"start\":696},{\"end\":830,\"start\":763}]", "title": "[{\"end\":66,\"start\":1},{\"end\":897,\"start\":832}]", "venue": null, "abstract": "[{\"end\":2559,\"start\":899}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2627,\"start\":2614},{\"end\":2661,\"start\":2629},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2704,\"start\":2668},{\"end\":2736,\"start\":2710},{\"end\":3082,\"start\":3056},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3381,\"start\":3348},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3398,\"start\":3381},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3415,\"start\":3398},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3432,\"start\":3415},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3465,\"start\":3432},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3487,\"start\":3465},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3806,\"start\":3787},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3824,\"start\":3806},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3854,\"start\":3824},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4122,\"start\":4103},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4144,\"start\":4124},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4369,\"start\":4344},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4391,\"start\":4374},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4513,\"start\":4490},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4922,\"start\":4885},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4988,\"start\":4959},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6166,\"start\":6143},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6187,\"start\":6166},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8804,\"start\":8787},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8820,\"start\":8804},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8956,\"start\":8923},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":8973,\"start\":8956},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9006,\"start\":8973},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9028,\"start\":9006},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9135,\"start\":9115},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9154,\"start\":9135},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9171,\"start\":9154},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9228,\"start\":9206},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9595,\"start\":9572},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9647,\"start\":9621},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9708,\"start\":9690},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10109,\"start\":10084},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10131,\"start\":10114},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10731,\"start\":10693},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10810,\"start\":10772},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":10827,\"start\":10810},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":10855,\"start\":10827},{\"end\":11281,\"start\":11255},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11376,\"start\":11355},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14478,\"start\":14455},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14500,\"start\":14478},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16098,\"start\":16076},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17141,\"start\":17129},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17596,\"start\":17580},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18249,\"start\":18233},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":27674,\"start\":27657},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":27922,\"start\":27899},{\"end\":28986,\"start\":28981},{\"end\":30758,\"start\":30729},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":30799,\"start\":30765},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":30836,\"start\":30813},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30954,\"start\":30937},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30981,\"start\":30965},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":31340,\"start\":31317},{\"end\":31387,\"start\":31341},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31525,\"start\":31494},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":31662,\"start\":31639},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":31793,\"start\":31759},{\"end\":31796,\"start\":31795},{\"end\":31848,\"start\":31846},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":32138,\"start\":32104},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":32163,\"start\":32140},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":32541,\"start\":32523},{\"end\":32702,\"start\":32688},{\"end\":35898,\"start\":35896},{\"end\":36208,\"start\":36206}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":38069,\"start\":37853},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":38870,\"start\":38070},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":39261,\"start\":38871}]", "paragraph": "[{\"end\":3218,\"start\":2575},{\"end\":3855,\"start\":3220},{\"end\":4752,\"start\":3857},{\"end\":5917,\"start\":4754},{\"end\":6273,\"start\":5919},{\"end\":7221,\"start\":6275},{\"end\":7832,\"start\":7223},{\"end\":8465,\"start\":7834},{\"end\":9758,\"start\":8482},{\"end\":10568,\"start\":9760},{\"end\":11139,\"start\":10570},{\"end\":11675,\"start\":11141},{\"end\":12243,\"start\":11717},{\"end\":12814,\"start\":12266},{\"end\":14501,\"start\":12902},{\"end\":15320,\"start\":14503},{\"end\":16389,\"start\":15463},{\"end\":17042,\"start\":16461},{\"end\":17704,\"start\":17044},{\"end\":17789,\"start\":17772},{\"end\":18128,\"start\":17791},{\"end\":18750,\"start\":18130},{\"end\":20670,\"start\":18776},{\"end\":21136,\"start\":20672},{\"end\":21279,\"start\":21138},{\"end\":21864,\"start\":21334},{\"end\":22385,\"start\":21922},{\"end\":22964,\"start\":22413},{\"end\":23197,\"start\":22966},{\"end\":23664,\"start\":23258},{\"end\":24709,\"start\":23684},{\"end\":24823,\"start\":24725},{\"end\":25935,\"start\":24825},{\"end\":26515,\"start\":25937},{\"end\":26700,\"start\":26666},{\"end\":27507,\"start\":26716},{\"end\":27873,\"start\":27509},{\"end\":28582,\"start\":27875},{\"end\":29061,\"start\":28584},{\"end\":36843,\"start\":29063},{\"end\":37852,\"start\":36858}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12901,\"start\":12815},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15462,\"start\":15321},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16460,\"start\":16390},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17731,\"start\":17705},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17771,\"start\":17731},{\"attributes\":{\"id\":\"formula_6\"},\"end\":21333,\"start\":21280},{\"attributes\":{\"id\":\"formula_7\"},\"end\":21921,\"start\":21865},{\"attributes\":{\"id\":\"formula_8\"},\"end\":23257,\"start\":23198},{\"attributes\":{\"id\":\"formula_9\"},\"end\":26665,\"start\":26516}]", "table_ref": "[{\"end\":25439,\"start\":25432},{\"end\":28532,\"start\":28525},{\"end\":28778,\"start\":28771},{\"end\":29201,\"start\":29194},{\"end\":29770,\"start\":29763},{\"end\":33641,\"start\":33634},{\"end\":34421,\"start\":34414},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":36087,\"start\":36080}]", "section_header": "[{\"end\":2573,\"start\":2561},{\"end\":8480,\"start\":8468},{\"end\":11715,\"start\":11678},{\"end\":12264,\"start\":12246},{\"end\":18774,\"start\":18753},{\"end\":22411,\"start\":22388},{\"end\":23682,\"start\":23667},{\"end\":24723,\"start\":24712},{\"end\":26714,\"start\":26703},{\"end\":36856,\"start\":36846},{\"end\":38881,\"start\":38872}]", "table": "[{\"end\":38870,\"start\":38165},{\"end\":39261,\"start\":39044}]", "figure_caption": "[{\"end\":38069,\"start\":37855},{\"end\":38165,\"start\":38072},{\"end\":39044,\"start\":38883}]", "figure_ref": "[{\"end\":5652,\"start\":5644},{\"end\":6420,\"start\":6414},{\"end\":12107,\"start\":12101},{\"end\":35180,\"start\":35174}]", "bib_author_first_name": "[{\"end\":43334,\"start\":43333},{\"end\":44818,\"start\":44813},{\"end\":44827,\"start\":44826},{\"end\":44845,\"start\":44844},{\"end\":44854,\"start\":44853},{\"end\":44864,\"start\":44863},{\"end\":44875,\"start\":44874},{\"end\":44884,\"start\":44883},{\"end\":44886,\"start\":44885},{\"end\":44896,\"start\":44895},{\"end\":44904,\"start\":44903},{\"end\":44912,\"start\":44911},{\"end\":44914,\"start\":44913},{\"end\":44923,\"start\":44922},{\"end\":44925,\"start\":44924},{\"end\":44938,\"start\":44937},{\"end\":44951,\"start\":44950},{\"end\":44959,\"start\":44958},{\"end\":44961,\"start\":44960},{\"end\":45687,\"start\":45686},{\"end\":45985,\"start\":45977},{\"end\":45996,\"start\":45995},{\"end\":46007,\"start\":46006},{\"end\":46019,\"start\":46018},{\"end\":46326,\"start\":46325},{\"end\":46530,\"start\":46529},{\"end\":46743,\"start\":46737},{\"end\":46750,\"start\":46749},{\"end\":46765,\"start\":46764},{\"end\":46775,\"start\":46774},{\"end\":46976,\"start\":46975},{\"end\":47835,\"start\":47834},{\"end\":48546,\"start\":48541},{\"end\":48556,\"start\":48555},{\"end\":48565,\"start\":48564},{\"end\":48579,\"start\":48575},{\"end\":48788,\"start\":48787},{\"end\":48803,\"start\":48802},{\"end\":48814,\"start\":48813},{\"end\":48823,\"start\":48822},{\"end\":49144,\"start\":49139},{\"end\":49159,\"start\":49158},{\"end\":49174,\"start\":49173},{\"end\":49183,\"start\":49182},{\"end\":49680,\"start\":49673},{\"end\":49692,\"start\":49691},{\"end\":49694,\"start\":49693},{\"end\":49706,\"start\":49705},{\"end\":49717,\"start\":49716},{\"end\":50082,\"start\":50081},{\"end\":50356,\"start\":50355},{\"end\":50358,\"start\":50357},{\"end\":50366,\"start\":50365},{\"end\":50368,\"start\":50367},{\"end\":51066,\"start\":51062},{\"end\":51074,\"start\":51073},{\"end\":51082,\"start\":51081},{\"end\":51089,\"start\":51088},{\"end\":51341,\"start\":51340},{\"end\":51651,\"start\":51650},{\"end\":52343,\"start\":52340},{\"end\":52350,\"start\":52349},{\"end\":52363,\"start\":52362},{\"end\":52370,\"start\":52369},{\"end\":52618,\"start\":52613},{\"end\":52627,\"start\":52626},{\"end\":52636,\"start\":52635},{\"end\":52645,\"start\":52644},{\"end\":52916,\"start\":52912}]", "bib_author_last_name": "[{\"end\":43345,\"start\":43335},{\"end\":43356,\"start\":43347},{\"end\":43847,\"start\":43841},{\"end\":44487,\"start\":44483},{\"end\":44824,\"start\":44819},{\"end\":44842,\"start\":44828},{\"end\":44851,\"start\":44846},{\"end\":44861,\"start\":44855},{\"end\":44872,\"start\":44865},{\"end\":44881,\"start\":44876},{\"end\":44893,\"start\":44887},{\"end\":44901,\"start\":44897},{\"end\":44909,\"start\":44905},{\"end\":44920,\"start\":44915},{\"end\":44935,\"start\":44926},{\"end\":44948,\"start\":44939},{\"end\":44956,\"start\":44952},{\"end\":44970,\"start\":44962},{\"end\":45697,\"start\":45688},{\"end\":45993,\"start\":45986},{\"end\":46004,\"start\":45997},{\"end\":46016,\"start\":46008},{\"end\":46028,\"start\":46020},{\"end\":46330,\"start\":46327},{\"end\":46534,\"start\":46531},{\"end\":46747,\"start\":46744},{\"end\":46762,\"start\":46751},{\"end\":46772,\"start\":46766},{\"end\":46781,\"start\":46776},{\"end\":46982,\"start\":46977},{\"end\":46986,\"start\":46984},{\"end\":47329,\"start\":47326},{\"end\":47589,\"start\":47586},{\"end\":47839,\"start\":47836},{\"end\":48553,\"start\":48547},{\"end\":48562,\"start\":48557},{\"end\":48573,\"start\":48566},{\"end\":48800,\"start\":48789},{\"end\":48811,\"start\":48804},{\"end\":48820,\"start\":48815},{\"end\":48830,\"start\":48824},{\"end\":49156,\"start\":49145},{\"end\":49171,\"start\":49160},{\"end\":49180,\"start\":49175},{\"end\":49190,\"start\":49184},{\"end\":49462,\"start\":49456},{\"end\":49689,\"start\":49681},{\"end\":49703,\"start\":49695},{\"end\":49714,\"start\":49707},{\"end\":49724,\"start\":49718},{\"end\":50090,\"start\":50083},{\"end\":50363,\"start\":50359},{\"end\":50374,\"start\":50369},{\"end\":51071,\"start\":51067},{\"end\":51079,\"start\":51075},{\"end\":51086,\"start\":51083},{\"end\":51345,\"start\":51342},{\"end\":51658,\"start\":51652},{\"end\":52347,\"start\":52344},{\"end\":52360,\"start\":52351},{\"end\":52367,\"start\":52364},{\"end\":52374,\"start\":52371},{\"end\":52624,\"start\":52619},{\"end\":52633,\"start\":52628},{\"end\":52642,\"start\":52637},{\"end\":52652,\"start\":52646}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":207167677},\"end\":43762,\"start\":43249},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":9095914},\"end\":44408,\"start\":43764},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":4557963},\"end\":44723,\"start\":44410},{\"attributes\":{\"doi\":\"arXiv:1411.4166\",\"id\":\"b3\",\"matched_paper_id\":4090850},\"end\":45624,\"start\":44725},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":207032678},\"end\":45886,\"start\":45626},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":18985848},\"end\":46276,\"start\":45888},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":205692},\"end\":46473,\"start\":46278},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":7958862},\"end\":46690,\"start\":46475},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":14170854},\"end\":46923,\"start\":46692},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":7663461},\"end\":47252,\"start\":46925},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1969092},\"end\":47512,\"start\":47254},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":2949428},\"end\":47762,\"start\":47514},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":638838},\"end\":48471,\"start\":47764},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":1157792},\"end\":48748,\"start\":48473},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":17419203},\"end\":49057,\"start\":48750},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":6488690},\"end\":49385,\"start\":49059},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":8429835},\"end\":49635,\"start\":49387},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":207163173},\"end\":49870,\"start\":49637},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":15150247},\"end\":50079,\"start\":49872},{\"attributes\":{\"doi\":\"arXiv:1511.06361\",\"id\":\"b19\"},\"end\":50289,\"start\":50081},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":12071886},\"end\":50528,\"start\":50291},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":15027084},\"end\":50732,\"start\":50530},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":19135805},\"end\":51006,\"start\":50734},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":3143712},\"end\":51233,\"start\":51008},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":213169},\"end\":51561,\"start\":51235},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":89639},\"end\":52034,\"start\":51563},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":31606602},\"end\":52269,\"start\":52036},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":1743664},\"end\":52534,\"start\":52271},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":1644335},\"end\":52830,\"start\":52536},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":2768038},\"end\":53092,\"start\":52832},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":7062707},\"end\":53325,\"start\":53094}]", "bib_title": "[{\"end\":43331,\"start\":43249},{\"end\":43839,\"start\":43764},{\"end\":44481,\"start\":44410},{\"end\":44811,\"start\":44725},{\"end\":45684,\"start\":45626},{\"end\":45975,\"start\":45888},{\"end\":46323,\"start\":46278},{\"end\":46527,\"start\":46475},{\"end\":46735,\"start\":46692},{\"end\":46973,\"start\":46925},{\"end\":47324,\"start\":47254},{\"end\":47584,\"start\":47514},{\"end\":47832,\"start\":47764},{\"end\":48539,\"start\":48473},{\"end\":48785,\"start\":48750},{\"end\":49137,\"start\":49059},{\"end\":49454,\"start\":49387},{\"end\":49671,\"start\":49637},{\"end\":49917,\"start\":49872},{\"end\":50353,\"start\":50291},{\"end\":50585,\"start\":50530},{\"end\":50800,\"start\":50734},{\"end\":51060,\"start\":51008},{\"end\":51338,\"start\":51235},{\"end\":51648,\"start\":51563},{\"end\":52104,\"start\":52036},{\"end\":52338,\"start\":52271},{\"end\":52611,\"start\":52536},{\"end\":52910,\"start\":52832},{\"end\":53156,\"start\":53094}]", "bib_author": "[{\"end\":43347,\"start\":43333},{\"end\":43358,\"start\":43347},{\"end\":43849,\"start\":43841},{\"end\":44489,\"start\":44483},{\"end\":44826,\"start\":44813},{\"end\":44844,\"start\":44826},{\"end\":44853,\"start\":44844},{\"end\":44863,\"start\":44853},{\"end\":44874,\"start\":44863},{\"end\":44883,\"start\":44874},{\"end\":44895,\"start\":44883},{\"end\":44903,\"start\":44895},{\"end\":44911,\"start\":44903},{\"end\":44922,\"start\":44911},{\"end\":44937,\"start\":44922},{\"end\":44950,\"start\":44937},{\"end\":44958,\"start\":44950},{\"end\":44972,\"start\":44958},{\"end\":45699,\"start\":45686},{\"end\":45995,\"start\":45977},{\"end\":46006,\"start\":45995},{\"end\":46018,\"start\":46006},{\"end\":46030,\"start\":46018},{\"end\":46332,\"start\":46325},{\"end\":46536,\"start\":46529},{\"end\":46749,\"start\":46737},{\"end\":46764,\"start\":46749},{\"end\":46774,\"start\":46764},{\"end\":46783,\"start\":46774},{\"end\":46984,\"start\":46975},{\"end\":46988,\"start\":46984},{\"end\":47331,\"start\":47326},{\"end\":47591,\"start\":47586},{\"end\":47841,\"start\":47834},{\"end\":48555,\"start\":48541},{\"end\":48564,\"start\":48555},{\"end\":48575,\"start\":48564},{\"end\":48582,\"start\":48575},{\"end\":48802,\"start\":48787},{\"end\":48813,\"start\":48802},{\"end\":48822,\"start\":48813},{\"end\":48832,\"start\":48822},{\"end\":49158,\"start\":49139},{\"end\":49173,\"start\":49158},{\"end\":49182,\"start\":49173},{\"end\":49192,\"start\":49182},{\"end\":49464,\"start\":49456},{\"end\":49691,\"start\":49673},{\"end\":49705,\"start\":49691},{\"end\":49716,\"start\":49705},{\"end\":49726,\"start\":49716},{\"end\":50092,\"start\":50081},{\"end\":50365,\"start\":50355},{\"end\":50376,\"start\":50365},{\"end\":51073,\"start\":51062},{\"end\":51081,\"start\":51073},{\"end\":51088,\"start\":51081},{\"end\":51092,\"start\":51088},{\"end\":51347,\"start\":51340},{\"end\":51660,\"start\":51650},{\"end\":52349,\"start\":52340},{\"end\":52362,\"start\":52349},{\"end\":52369,\"start\":52362},{\"end\":52376,\"start\":52369},{\"end\":52626,\"start\":52613},{\"end\":52635,\"start\":52626},{\"end\":52644,\"start\":52635},{\"end\":52654,\"start\":52644},{\"end\":52919,\"start\":52912}]", "bib_venue": "[{\"end\":46998,\"start\":46993},{\"end\":47878,\"start\":47847},{\"end\":43364,\"start\":43358},{\"end\":43854,\"start\":43849},{\"end\":44495,\"start\":44489},{\"end\":44990,\"start\":44987},{\"end\":45705,\"start\":45699},{\"end\":46058,\"start\":46030},{\"end\":46335,\"start\":46332},{\"end\":46541,\"start\":46536},{\"end\":46788,\"start\":46783},{\"end\":46991,\"start\":46988},{\"end\":47336,\"start\":47331},{\"end\":47595,\"start\":47591},{\"end\":47845,\"start\":47841},{\"end\":48586,\"start\":48582},{\"end\":48864,\"start\":48832},{\"end\":49197,\"start\":49192},{\"end\":49468,\"start\":49464},{\"end\":49729,\"start\":49726},{\"end\":49923,\"start\":49919},{\"end\":50147,\"start\":50108},{\"end\":50381,\"start\":50376},{\"end\":50591,\"start\":50587},{\"end\":50827,\"start\":50802},{\"end\":51097,\"start\":51092},{\"end\":51351,\"start\":51347},{\"end\":51665,\"start\":51660},{\"end\":52110,\"start\":52106},{\"end\":52381,\"start\":52376},{\"end\":52657,\"start\":52654},{\"end\":52923,\"start\":52919},{\"end\":53164,\"start\":53158}]"}}}, "year": 2023, "month": 12, "day": 17}