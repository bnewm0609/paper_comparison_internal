{"id": 152282661, "updated": "2023-10-07 05:21:12.698", "metadata": {"title": "CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features", "authors": "[{\"first\":\"Sangdoo\",\"last\":\"Yun\",\"middle\":[]},{\"first\":\"Dongyoon\",\"last\":\"Han\",\"middle\":[]},{\"first\":\"Seong\",\"last\":\"Oh\",\"middle\":[\"Joon\"]},{\"first\":\"Sanghyuk\",\"last\":\"Chun\",\"middle\":[]},{\"first\":\"Junsuk\",\"last\":\"Choe\",\"middle\":[]},{\"first\":\"Youngjoon\",\"last\":\"Yoo\",\"middle\":[]}]", "venue": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)", "journal": "2019 IEEE/CVF International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2019, "month": 5, "day": 13}, "abstract": "Regional dropout strategies have been proposed to enhance the performance of convolutional neural network classifiers. They have proved to be effective for guiding the model to attend on less discriminative parts of objects (\\eg leg as opposed to head of a person), thereby letting the network generalize better and have better object localization capabilities. On the other hand, current methods for regional dropout removes informative pixels on training images by overlaying a patch of either black pixels or random noise. {Such removal is not desirable because it leads to information loss and inefficiency during training.} We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efficient use of training pixels and \\mbox{retaining} the regularization effect of regional dropout, CutMix consistently outperforms the state-of-the-art augmentation strategies on CIFAR and ImageNet classification tasks, as well as on the ImageNet weakly-supervised localization task. Moreover, unlike previous augmentation methods, our CutMix-trained ImageNet classifier, when used as a pretrained model, results in consistent performance gains in Pascal detection and MS-COCO image captioning benchmarks. We also show that CutMix improves the model robustness against input corruptions and its out-of-distribution detection performances.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1905.04899", "mag": "2992308087", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/YunHCOYC19", "doi": "10.1109/iccv.2019.00612"}}, "content": {"source": {"pdf_hash": "702c241614fbea136fe3b727b26406f7a081d8b9", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1905.04899v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1905.04899", "status": "GREEN"}}, "grobid": {"id": "222b3307a509617bbe1e35ff40cbdff407344187", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/702c241614fbea136fe3b727b26406f7a081d8b9.txt", "contents": "\nCutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features\n\n\nSangdoo Yun \nClova AI Research\nNAVER Corp\n\n\nDongyoon Han \nClova AI Research\nNAVER Corp\n\n\nSeong Joon Oh \nClova AI Research\nLINE Plus Corp\n\n\nSanghyuk Chun \nClova AI Research\nNAVER Corp\n\n\nJunsuk Choe \nYonsei University\n\n\nYoungjoon Yoo \nClova AI Research\nNAVER Corp\n\n\nCutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features\n\nRegional dropout strategies have been proposed to enhance the performance of convolutional neural network classifiers. They have proved to be effective for guiding the model to attend on less discriminative parts of objects (e.g. leg as opposed to head of a person), thereby letting the network generalize better and have better object localization capabilities. On the other hand, current methods for regional dropout removes informative pixels on training images by overlaying a patch of either black pixels or random noise. Such removal is not desirable because it leads to information loss and inefficiency during training. We therefore propose the CutMix augmentation strategy: patches are cut and pasted among training images where the ground truth labels are also mixed proportionally to the area of the patches. By making efficient use of training pixels and retaining the regularization effect of regional dropout, CutMix consistently outperforms the stateof-the-art augmentation strategies on CIFAR and ImageNet classification tasks, as well as on the ImageNet weaklysupervised localization task. Moreover, unlike previous augmentation methods, our CutMix-trained ImageNet classifier, when used as a pretrained model, results in consistent performance gains in Pascal detection and MS-COCO image captioning benchmarks. We also show that CutMix improves the model robustness against input corruptions and its out-of-distribution detection performances.\n\nIntroduction\n\nDeep convolutional neural networks (CNNs) have shown promising performances on various computer vision problems such as image classification [30,19,11], object de- * Correspondence to sangdoo.yun@navercorp.com  tection [29,23], semantic segmentation [1,24], and video analysis [27,31]. To further improve the training efficiency and performance, a number of training strategies have been proposed, including data augmentation [19] and regularization techniques [33,16,37]. In particular, to prevent a CNN from focusing too much on a small set of intermediate activations or on a small region on input images, random feature removal regularizations have been proposed. Examples include dropout [33] for randomly dropping hidden activations and regional dropout [2,49,32,7] for erasing random regions on the input. Researchers have shown that the feature removal strategies improve generalization and localization by letting a model attend not only to the most discriminative parts of objects, but rather to the entire object region [32,7]. While regional dropout strategies have shown improvements of classification and localization performances to a certain degree, deleted regions are usually zeroed-out [2,32] or filled with random noise [49], greatly reducing the proportion of informative pixels on training images. We recognize this as a severe conceptual limitation as CNNs are generally data hungry [26]. How can we maximally utilize the deleted regions, while taking advantage of better generalization and localization using regional dropout?\n\nWe address the above question by introducing an augmentation strategy CutMix. Instead of simply removing pixels, we replace the removed regions with a patch from another image (See Table 1). The ground truth labels are also mixed proportionally to the number of pixels of combined images. CutMix now enjoys the property that there is no uninformative pixel during training, making training efficient, while retaining the advantages of regional dropout to attend to non-discriminative parts of objects. The added patches further enhance localization ability by requiring the model to identify the object from a partial view. The training and inference budgets remain the same.\n\nCutMix shares similarity with Mixup [46] which mixes two samples by interpolating both the images and labels. Mixup has been found to improve classification, but the interpolated sample tends to be unnatural (See the mixed image in Table 1). On the other hand, CutMix overcomes the problem by replacing the image region with a image patch from another training image. Table 1 gives an overview of Mixup [46], Cutout [2], and CutMix on image classification, weakly supervised localization, and transfer learning to object detection methods. Although Mixup and Cutout enhance the ImageNet classification accuracies, they suffer from performance degradation on ImageNet localization and object detection tasks. On the other hand, CutMix consistently achieves significant enhancements across three tasks, proving its superior classification and localization ability beyond the baseline and other augmentation methods.\n\nWe present extensive evaluations of our CutMix on various CNN architectures and on various datasets and tasks. Summarizing the key results, CutMix has significantly improved the accuracy of a baseline classifier on CIFAR-100 and has obtained the state-of-the-art top-1 error 14.23%. On ImageNet [30], applying CutMix to ResNet-50 and ResNet-101 [11] has improved the classification accuracy by +2.08% and +1.70%, respectively. On the localization front, CutMix improves the performance of the weaklysupervised object localization (WSOL) task on CUB200-2011 [42] and ImageNet [30] by +5.4% and +0.9% gains, respectively. The superior localization capability is further evidenced by fine-tuning a detector and an image caption generator on CutMix-ImageNet-pretrained models; the Cut-Mix pretraining has improved the overall detection perfor-mances on Pascal VOC [5] by +1% mAP and image captioning performance on MS-COCO [22] by 2 BLEU score. CutMix also enhances the model robustness and dramatically alleviates the over-confident issue [12,21] of deep networks.\n\n\nRelated Works\n\nRegional dropout: Methods [2,49,32] removing random regions in images have been proposed to enhance the generalization and localization performances of CNNs. CutMix is similar to those methods, while the critical difference is that the removed regions are filled with patches from other images. On the feature level, DropBlock [7] has generalized the regional dropout to the feature space and have shown enhanced generalizability as well. CutMix can also be performed on the feature space, as we will see in the experiments. Synthesizing training data: Some works have explored synthesizing training data for further generalizability. Generating [6] new training samples by Stylizing ImageNet [30] has guided the model to focus more on shape than texture, leading to better classification and object detection performances. CutMix also generates new samples by cutting and pasting patches within mini-batches, leading to performance boosts in many computer vision tasks; the main advantage of CutMix is that the additional cost for sample generation is negligible. For object detection, object insertion methods [4,3] have been proposed as a way to synthesize objects in the background. These methods are different from CutMix because they aim to represent a single object well while CutMix generates combined samples which may contain multiple objects. Mixup: CutMix shares similarity with Mixup [46,39] in that both combines two samples, where the ground truth label of the new sample is given by the mixture of one-hot labels. As we will see in the experiments, Mixup samples suffer from the fact that they are locally ambiguous and unnatural, and therefore confuses the model, especially for localization. Recently, Mixup variants [40,34,9] have been proposed; they perform feature-level interpolation and other types of transformations. Above works, however, generally lack a deep analysis in particular on the localization ability and transfer-learned performances. Tricks for training deep networks: Efficient training of deep networks is one of the most important problems in research community, as they require great amount of compute and data. Methods such as weight decay, dropout [33], and Batch Normalization [17] are widely used to train more generalizable deep networks. Recently, methods adding noises to internal features [16,7,44] or adding extra path to the architecture [14,13] have been proposed. CutMix is complementary to the above methods because it operates on the data level, without changing internal representations or ar-chitecture.\n\n\nCutMix\n\nWe describe the CutMix algorithm in detail.\n\n\nAlgorithm\n\nLet x \u2208 R W \u00d7H\u00d7C and y denote a training image and its label, respectively. The goal of CutMix is to generate a new training sample (x,\u1ef9) by combining two training samples (x A , y A ) and (x B , y B ). Then, the new generated training sample (x,\u1ef9) is used to train the model with its original loss function. To this end, we define the combining operation as\nx = M x A + (1 \u2212 M) x B y = \u03bby A + (1 \u2212 \u03bb)y B ,(1)\nwhere M \u2208 {0, 1} W \u00d7H denotes a binary mask indicating where to drop out and fill in from two images, 1 is a binary mask filled with ones, and is element-wise multiplication. Like Mixup [46], the combination ratio \u03bb between two data points is sampled from the beta distribution Beta(\u03b1, \u03b1). In our all experiments, we set \u03b1 to 1, that is \u03bb is sampled from the uniform distribution (0, 1). Note that the major difference is that CutMix replaces image region with a patch from another training image and can generate more locally natural image than Mixup.\n\nTo sample the binary mask M, we first sample the bounding box coordinates B = (r x , r y , r w , r h ) indicating the cropping regions on x A and x B . The region B in x A is dropped out and filled with the patch cropped at B of x B .\n\nIn our experiments, we sample rectangular masks M whose aspect ratio is proportional to the original image. The box coordinates are uniformly sampled according to:\nr x \u223c Unif (0, W ) , r w = W \u221a 1 \u2212 \u03bb, r y \u223c Unif (0, H) , r h = H \u221a 1 \u2212 \u03bb(2)\nmaking the cropped area ratio rwr h W H = 1 \u2212 \u03bb. With the cropping region, the binary mask M \u2208 {0, 1} W \u00d7H is decided by filling with 0 within the bounding box B, otherwise 1.\n\nSince implementing CutMix is simple and has negligible computational overheads as existing data augmentation techniques as used in [35,15], we can efficiently utilize it to train any network architectures. In each training iteration, a CutMix-ed sample (x,\u1ef9) is generated by combining randomly selected two training samples in a mini-batch according to Equation (1). Code-level details are presented in Appendix A.\n\n\nDiscussion\n\nWhat does model learn with CutMix? We have motivated CutMix such that full object regions are considered for classification, as Cutout is designed for, while ensuring  [50] visualizations on 'Saint Bernard' and 'Miniature Poodle' samples using various augmentation techniques. From top to bottom rows, we show the original images, input augmented image, CAM for class 'Saint Bernard', and CAM for class 'Miniature Poodle', respectively. Note that CutMix can take advantage of the mixed region on image, but Cutout cannot.\n\n\nMixup Cutout CutMix\n\nUsage of full image region Regional dropout Mixed image & label Table 2: Comparison among Mixup, Cutout, and CutMix. two objects are recognized from partial views from a single image to increase training efficiency. To verify that Cut-Mix is indeed learning to recognize two objects from the augmented samples from their respective partial views, we visually compare the activation maps for CutMix against Cutout [2] and Mixup [46]. Figure 1 shows example augmentation inputs as well as corresponding class activation maps (CAM) [50] for two classes present, Saint Bernard and Miniature Poodle. We use vanilla ResNet-50 model 1 for obtaining the CAMs to clearly see the effect of augmentation method only.\n\nWe can observe that Cutout successfully lets a model focus on less discriminative parts of the object. For example, the model focuses on the belly of Saint Bernard on the Cutout-ed sample. We also observe, however, that they make less efficient use of training data due to uninformative pixels. Mixup, on the other hand, makes full use of pixels, but introduces unnatural artifacts. The CAM for Mixup, as a result, shows the confusion of model in choosing cues for recognition. We hypothesize that such confusion leads to its suboptimal performance in classification and localization as we will see in Section 4. CutMix efficiently improves upon Cutout by being able to localize the two object classes accurately, as Cutout can only deal with one object on a single image. We summarize the comparison among Mixup, Cutout, and CutMix as in Table 2. Analysis on validation error: We analyze the effect of CutMix on stabilizing the training of deep networks. We compare the top-1 validation error during the training with CutMix against the baseline. We train ResNet-50 [11] for ImageNet Classification, and PyramidNet-200 [10] for CIFAR-100 Classification. Figure 2 shows the results.\n\nWe observe, first of all, that CutMix achieves lower validation errors than the baseline at the end of training. After the half of the epochs where learning rates are further reduced, we observe that the baseline suffers from overfitting with increasing validation error. CutMix, on the other hand, shows a steady decrease in validation error, proving its ability to reduce overfitting by guiding the training with diverse samples.\n\n\nExperiments\n\nIn this section, we evaluate CutMix for its capability to improve localizability as well as generalizability of a trained model on multiple tasks. We first study the effect of Cut-Mix on image classification (Section 4.1) and weakly supervised object localization (Section 4.2). Next, we show the transferability of a pretrained model using CutMix when it is fine-tuned for object detection and image captioning tasks (Section 4.3). We also show that CutMix can improve the model robustness and alleviate the over-confident issue in Section 4.4. Throughout the experiments, we verify that CutMix outperforms other state-of-the-art regularization methods in above tasks and we further analyze the inner mechanisms behind such superiority.\n\nAll experiments were implemented and evaluated on NAVER Smart Machine Learning (NSML) [18] platform with PyTorch [28]. Codes will be released in near future.\n\n\nImage Classification\n\n\nImageNet Classification\n\nWe evaluate on ImageNet-1K benchmark [30] a dataset containing over 1M training images and 50K validation images of 1K categories. For fair comparison, we use the standard augmentation setting for ImageNet dataset such as resizing, cropping, and flipping, as also done in [10,7,15,36]. We found that such regularization methods including Stochastic Depth [16], Cutout [2], Mixup [46], and our Cut-Mix, require a greater number of training epochs till convergence. Therefore, we have trained all the models with   Table 3: ImageNet classification results based on ResNet-50 model. '*' denotes results reported in the original papers.\n\n300 epochs initial learning rate 0.1, decayed by factor 0.1 at epochs 75, 150, and 225. The batch size is set to 256. The mixture hyper-parameter for CutMix \u03b1 is set to 1.\n\nWe briefly describe the settings for baseline augmentation schemes. We set the dropping rate of residual blocks to 0.25 for the best performance of Stochastic Depth [16]. The mask size for Cutout [2] is set to 112 \u00d7 112 and the location for dropping out is uniformly sampled. The performance of DropBlock [7] is from the original paper and the difference from our setting is the training epochs which is set to 270. Manifold Mixup [40] applies Mixup operation on the randomly chosen internal feature map. Hyper-parameter \u03b1 for Mixup and Manifold Mixup was tested with 0.5 and 1.0 and we selected 1.0 which shows better performance. Conceptually it is also possible to extend CutMix to feature-level augmentation. To test this, we propose \"Feature CutMix\" scheme, which applies the CutMix operation at a randomly chosen layer per minibatch as Manifold Mixup does. Comparison against baseline augmentations: Results are given in Table 3. We observe that our CutMix method   [14] and GE [13] boosts +1.56% and +1.80%, respectively. The improvement due to CutMix is more impressive, since it does not require additional parameters or more computation per SGD update (as architectural changes do). CutMix is a competitive data augmentation scheme that requires minimal additional cost. CutMix for Deeper Models: We have explored the performance of CutMix for the deeper networks, ResNet-101 [11] and ResNeXt-101 (32\u00d74d) [43], on ImageNet. As seen in Table 4, we observe +1.60% and +1.71% improvements in top-1 errors due to CutMix, respectively.\n\n\nCIFAR Classification\n\nHere we describe the results on CIFAR classification. We set mini-batch size to 64 and training epochs to 300 for CI-FAR classification. The learning rate was initially set to 0.25 and decayed by the factor of 0.1 at 150 and 225 epoch. To ensure the effectiveness of the proposed method, we used very strong baseline, PyramidNet-200 [10], the widening factor\u03b1 = 240 and the number of parameters is 26.8 M, which shows state-of-the-art performance on CIFAR-100 (top-1 error is 16.45). Table 5 shows the performance comparison with other state-of-the-art data augmentation and regularization methods. All experiments were conducted three times and the averaged performance were reported.\n\nHyper-parameter settings: We set the hole size of Cutout [2] to 16 \u00d7 16. For DropBlock [7], keep prob and block size are set to 0.9 and 4, respectively. The drop    Combination of regularization methods: One step further for validating each regularization methods, we also tested the combination of the various methods. We found that both Cutout [2] and label smoothing [37] could not improve the accuracy when independently adopted to the training, but it was effective when we used the two methods simultaneously. Dropblock [7], the generalized version of Cutout to feature-maps, was also more effective when label smoothing was attached. We observe that Mixup [46] and Manifold Mixup [40] both achieved higher accuracy when the image is applied by Cutout. The combination of Cutout and Mixup tends to generate locally separated and mixed samples since the Cutout-ed region has less ambiguity than the vanilla Mixup. Thus, the success of combining Cutout and Mixup shows that mixing via cut-and-paste manner is better than interpolation, as we conjectured. Consequently, we achieved 14.23% error in CIFAR-100 classification, which is +2.22% higher than baseline 16.45% error-rate. Also, we note that we achieved a new state-of-the-art performance 13.81% when adding CutMix and ShakeDrop [44], which is a regularization technique by adding noise to feature space. CutMix for various models: Table 6 shows CutMix can also significantly improve over the weaker baselines, such as PyramidNet-110 [10] and ResNet-110. CutMix for CIFAR-10: We evaluated CutMix on CIFAR-10 dataset using the same baseline and training setting for CIFAR-100. The results are given in Table 7. CutMix can also enhance the performance over the baseline by +0.97% and outperforms Mixup and Cutout.\n\n\nAblation Studies\n\nWe conducted ablation study in CIFAR-100 dataset using the same experimental settings in Section 4.1.2. We evaluated CutMix with varing the parameters \u03b1 to 0.1, 0.25, 0.5, 1.0, 2.0 and 4.0 and the results are given in the left graph of Figure 3. From all the different values of \u03b1, we achieved better results than the baseline (16.45%), and the best performance was achieved when \u03b1 = 1.0.\n\nThe performance of feature-level CutMix is given in the right graph of Figure 3. We changed the location where to apply CutMix from image level to feature level. We denote the index as (0=image level, 1=after first conv-bn, 2=after layer1, 3=after layer2, 4=after layer3). CutMix achieved the best performance when it was applied to input. Still, feature-level CutMix except the layer3 case can improve the accuracy over the baseline (16.45%).   Table 9: Weakly supervised object localization results on CUB200-2011 and ImageNet.\n\nconfigurations. 'Center Gaussian CutMix' denotes the experiment adapting Gaussian distribution whose mean is the center of image instead of uniform distribution when sampling r x , r y of Equation (2). 'Fixed-size CutMix' fixes the size of cropping region (r w , r h ) as 16 \u00d7 16, thus \u03bb is always 0.75. 'Scheduled CutMix' changes the probability to apply CutMix or not during training as [7,16] do. 'Onehot CutMix' denotes the case where the label is not combined as Equation (1), but decided to a single label which has larger portion in the image. We scheduled the probability from 0 to 1 linearly as increasing training epoch. The results show that adding the priors in center for CutMix (Center Gaussian CutMix) and fixing the size of cropping region (Fixed-size CutMix) lead performance degradation. Scheduled CutMix shows slightly worse performance than CutMix. One-hot CutMix shows much worse performance than CutMix, highlighting the effect of combined label.\n\n\nWeakly Supervised Object Localization\n\nWeakly supervised object localization (WSOL) task aims to train the classifier to localize target objects by using only the class label. To localize the target well, it is important to make CNNs look at the full object region not to focus on a discriminant part of the target. That is, learning spatially distributed representation is the key for improving performance on WSOL task. Thus, here we measure how CutMix learns spatially distributed representation beyond other baselines by conducting WSOL task. We followed the training and evaluation strategy of the existing WSOL  methods [32,47,48]. ResNet-50 is used as the base model. The model is initialized using ImageNet Pre-trained model before training, and is modified to enlarge the final convolutional feature map size to 14 \u00d7 14 from 7 \u00d7 7. Then, the model is fine-tuned on CUB200-2011 [42] and ImageNet-1K [30] dataset only using class labels. At evaluation, we utilized class activation mappings (CAM) [50] to estimate the bounding box of an object. The quantitative and qualitative results are given in Table 9 and Figure 4, respectively. The implementation details are in Appendix B.\n\nComparison against Mixup and Cutout: CutMix outperforms Mixup [46] by +5.51% and +1.41% on CUB200-2011 and ImageNet dataset, respectively. We observe that Mixup degraded the localization accuracy over baseline and tends to focus on a small region of image as shown in Figure 4. As we hypothesized in Section 3.2, the Mixuped sample has the ambiguity, so the CNN trained with those samples tends to focus on the most discriminative part for classification, which leads the degradation of localization ability. Although Cutout [2] can improve the localization accuracy over the baseline, CutMix still outperforms Cutout by +2.03% and +0.56% on CUB200-2011 and ImageNet dataset, respectively. Furthermore, CutMix achieved comparable localization accuracy on CUB200-2011 and ImageNet dataset compared with state-of-the-art methods [50,47,48] that focus on learning spatially distributed representations.\n\n\nTransfer Learning of Pretrained Model\n\nIn this section, we check the generalization ability of CutMix by transferring task from image classification to other computer vision tasks such as object detection and image captioning, which require the localization ability of the learned CNN feature. For each task, we replace the backbone network of the original model with various ImageNetpretrained models using Mixup [46], Cutout [2], and our CutMix. Then the model is finetuned for each task and we validate the performance improvement of CutMix over the original backbone and other baselines. ResNet-50 model is used as a baseline. Transferring to Pascal VOC object detection: Two popular detection models, SSD [23] and Faster RCNN [29], are used for the experiment. Originally the two methods utilized VGG-16 as a backbone network, but we changed it to ResNet-50. The ResNet-50 backbone is initialized with various ImageNet-pretrained models and finetuned using Pascal VOC 2007 and 2012 [5] trainval data, and evaluated with VOC Pascal 2007 test data using mAP metric. We follow the finetuning strategy as the original methods [23,29] do and the implementation details are in Appendix C. The performance result is represented in Table 10. The pretrained backbone models of Cutout and Mixup failed to improve the performance on object detection task over the original model. However, the backbone pretrained by CutMix can clearly improve the performance of both SSD and Faster-RCNN. This shows that our method can train CNNs to have generalization ability to be applied to object detection. Transferring to MS-COCO image captioning: We used Neural Image Caption (NIC) [41] as the base model for image captioning experiment. We change the backbone network of encoder from GoogLeNet [41] to ResNet-50. The backbone network is initialized with ImageNet-pretrained models, and then we trained and evaluated NIC on MS-COCO dataset [22]. The implementation details and other evaluation metrics (METEOR, CIDER, etc.) are in Ap- pendix D. \n\n\nRobustness and Uncertainty\n\nMany researches have shown that deep models are easily fooled by small and unrecognizable perturbations to the input image, which is called adversarial attacks [8,38]. One straightforward way to enhance robustness and uncertainty is an input augmentation by generating unseen samples [25]. We evaluate robustness and uncertainty improvements by input augmentation methods including Mixup, Cutout and CutMix comparing to the baseline. Robustness: We evaluate the robustness of the trained models to adversarial samples, occluded samples and in-between class samples. We use ImageNet-pretrained ResNet-50 models with same setting in Section 4.1.1.\n\nFast Gradient Sign Method (FGSM) [8] is used to generate adversarial perturbations and we assume that the adversary has full information of the models, which is called white-box attack. We report top-1 accuracy after attack on ImageNet validation set in Table 11. CutMix significantly improves the robustness to adversarial attacks compared to other augmentation methods.\n\nFor occlusion experiments, we generate occluded samples in two ways: center occlusion by filling zeros in a center hole and boundary occlusion by filling zeros outside of the hole. In Figure 5a, we measure the top-1 error by varying the hole size from 0 to 224. For both employed occlusion scenario, Cutout and CutMix achieve significant improvements of robustness while Mixup nearly improves robustness to occlusion. Interestingly, CutMix almost achieves compatible performance compared to Cutout even though CutMix did not obseverve occluded samples during the training stage unlike Cutout.    Table 12: Out-of-distribution (OOD) detection results with CIFAR-100 trained models. Results are averaged on seven datasets. All numbers are in percents; higher is better.\n\ntwo classes by varying the combination ratio \u03bb is illustrated in Figure 5b. We randomly select 50, 000 in-between samples in ImageNet validation set. In the both in-between class sample experiments, Mixup and CutMix improves the performance of the network while improvements by Cutout is almost neglectable. Similarly to the previous occlusion experiments, CutMix even improves the robustness to the unseen Mixup in-between class samples. Uncertainty: We measure the performance of the out-ofdistribution (OOD) detectors proposed by [12] which determines whether the sample is in-or out-of-distribution by score thresholding. We use PyramidNet-200 trained on CIFAR-100 datasets with same setting in Section 4.1.2. In Table 12, we report averaged OOD detection performances against seven out-of-distribution samples from [12,21], including TinyImageNet, LSUN [45], uniform noise, Gaussian noise, etc. More results are illustrated in Appendix E. Note that Mixup and Cutout seriously impair the baseline performance, in other words, Mixup and Cutout augmentations aggravate the overconfidence issue of the base network. Meanwhile, our proposed CutMix significantly outperforms the baseline performance.\n\n\nConclusion\n\nIn this work, we introduced CutMix for training CNNs to have strong classification and localization ability. Cut-Mix is simple, easy to implement, and has no computational overheads, but surprisingly effective on various tasks. On ImageNet classification, applying CutMix to ResNet-50 and ResNet-101 brings +2.08% and +1.70% top-1 accuracy improvements. On CIFAR classification, CutMix also can significantly improve the performance of baseline by +2.22% and achieved the state-of-the-art top-1 error 14.23% performance. On weakly supervised object localization (WSOL), CutMix can enhance the localization accuracy and achieved comparable localization performance to state-of-the-art WSOL methods without any WSOL techniques. Furthermore, simply using CutMix-ImageNet-pretrained model as the initialized backbone of the object detection and image captioning brings overall performance improvements. Last, CutMix achieves performance improvements in robustness and uncertainty benchmarks compared to the other augmentation methods.\n\n\nA. CutMix Algorithm\n\nWe present the code-level description of CutMix algorithm in Algorithm A1. N, C, and K denote the size of minibatch, channel size of input image, and the number of classes. First, CutMix shuffles the order of the minibatch input and target along the first axis of the tensors. And the lambda and the cropping region (x1,x2,y1,y2) are sampled. Then, we mix the input and input s by replacing the cropping region of input to the region of input s. The target label is also mixed by interpolating method.\n\nNote that CutMix is easy to implement with few lines (from line 4 to line 15), so is very practical algorithm giving significant impact on a wide range of tasks.\n\n\nB. Weakly-supervised Object Localization\n\nWe describe the training and evaluation procedure in detail. Network modification: Basically weakly-supervised object localization (WSOL) has the same training strategy as image classification does. Training WSOL is starting from ImageNet-pretrained model. From the base network structure (ResNet-50 [11]), WSOL takes larger spatial size of feature map 14 \u00d7 14 whereas the original ResNet-50 has 7 \u00d7 7. To enlarge the spatial size, we modify the base network's final residual block (layer4) to have no stride, which originally has stride 2.\n\nSince the network is modified and the target dataset could be different from ImageNet [30], the last fullyconnected layer is randomly initialized with the final output dimension of 200 and 1000 for CUB200-2011 [42] and ImageNet, respectively. Input image transformation: For fair comparison, we used the same data augmentation strategy except Mixup, Cutout, and CutMix as the state-of-the-art WSOL methods do [32,47]. In training, the input image is resized to 256 \u00d7 256 size and randomly cropped 224 \u00d7 224 size images are used to train network. In testing, the input image is resized to 256 \u00d7 256, cropped at center with 224 \u00d7 224 size and used to validate the network, which called single crop strategy. Estimating bounding box: We utilize class activation mapping (CAM) [50] to estimate the bounding box of an object. First we compute CAM of an image, and next, we decide the foreground region of the image by binarizing the CAM with a specific threshold. The region with intensity over the threshold is set to 1, otherwise to 0. We use the threshold as a specific rate \u03c3 of the maximum intensity of the CAM. We set \u03c3 to 0.15 for all our experiments. From the binarized foreground map, the tightest box which can cover the largest connected region in the foreground map is selected to the bounding box for WSOL. of models, we report top-1 localization accuracy (Loc), which is used for ImageNet localization challenge [30]. For top-1 localization accuracy, intersection-over-union (IoU) between the estimated bounding box and ground truth position is larger than 0.5, and, at the same time, the estimated class label should be correct. Otherwise, top-1 localization accuracy treats the estimation was wrong.\n\n\nB.1. CUB200-2011\n\nCUB-200-2011 dataset [42] contains over 11 K images with 200 categories of birds. We set the number of training epochs to 600. The learning rate for the last fullyconnected layer and the other were set to 0.01 and 0.001, respectively. The learning rate is decaying by the factor of 0.1 at every 150 epochs. We used SGD optimizer, and the minibatch size, momentum, weight decay were set to 32, 0.9, and 0.0001. Table A1 shows that our model, ResNet-50 + CutMix, achieves 54.81% localization accuracy on CUB200 dataset which outperforms other state-of-the-art WSOL methods [50,47,48].\n\n\nB.2. ImageNet dataset\n\nImageNet-1K [30] is a large-scale dataset for general objects consisting of 13 M training samples and 50 K validation samples. We set the number of training epochs to 20. The learning rate for the last fully-connected layer and the other were set to 0.1 and 0.01, respectively. The learning rate is decaying by the factor of 0.1 at every 6 epochs. We used SGD optimizer, and the minibatch size, momentum, weight decay were set to 256, 0.9, and 0.0001. In Table A1, our model, ResNet-50 + CutMix, also achieves 47.25% localization accuracy on ImageNet-1K dataset, which shows comparable performance compared with other state-of-theart WSOL methods [50,47,48].\n\nAlgorithm A1 Pseudo-code of CutMix  [23]: The input image is resized to 300 \u00d7 300 (SSD300) and we used the basic training strategy of the original paper such as data augmentation, prior boxes, and extra layers. Since the backbone network is changed from VGG16 to ResNet-50, the pooling location conv4 3 of VGG16 is modified to the output of layer2 of ResNet-50. For training, we set the batch size, learning rate, and training iterations to 32, 0.001, and 120 K, respectively. The learning rate is decayed by the factor of 0.1 at 80 K and 100 K iterations. Finetuning on Faster-RCNN 3 [29]: Faster-RCNN takes fully-convolutional structure, so we only modify the backbone from VGG16 to ResNet-50. The batch size, learning rate, training iterations are set to 8, 0.01, and 120 K. The learning rate is decayed by the factor of 0.1 at 100 K iterations.\n\n\nD. Transfer Learning to Image Captioning\n\nMS-COCO dataset [22] contains 120 K trainval images and 40 K test images. From the base model NIC 4 [41], the backbone model is changed from\n\n\nE. Robustness and Uncertainty\n\nIn this section, we describe the details of the experimental setting and evaluation methods.\n\n\nE.1. Robustness\n\nWe evaluate the model robustness to adversarial perturbations, occlusion and in-between samples using Ima-geNet trained models. For the base models, we use ResNet-50 structure and follow the settings in Section 4.  [8] to generate adversarial samples. For the given image x, the ground truth label y and the noise size , FGSM generates an adversarial sample as the followingx = x + sign (\u2207 x L(\u03b8, x, y)) ,\n\nwhere L(\u03b8, x, y) denotes a loss function, for example, cross entropy function. In our experiments, we set the noise scale = 8/255.  Occlusion: For the given hole size s, we make a hole with width and height equals to s in the center of the image. For center occluded samples, we zeroed-out inside of the hole and for boundary occluded samples, we zeroed-out outside of the hole. In our experiments, we test the top-1 ImageNet validation accuracy of the models with varying hole size from 0 to 224.\n\nIn-between class samples: To generate in-between class samples, we first sample 50, 000 pairs of images from the ImageNet validation set. For generating Mixup samples, we generate a sample x from the selected pair x A and x B by x = \u03bbx A + (1 \u2212 \u03bb)x B . We report the top-1 accuracy on the Mixup samples by varying \u03bb from 0 to 1. To generate CutMix in-between samples, we employ the center mask instead of the random mask. We follow the hole generation process used in the occlusion experiments. We evaluate the top-1 accuracy on the CutMix samples by varing hole size s from 0 to 224.\n\n\nE.2. Uncertainty\n\nDeep neural networks are often overconfident in their predictions. For example, deep neural networks produce high confidence number even for random noise [12]. One standard benchmark to evaluate the overconfidence of the network is Out-of-distribution (OOD) detection proposed by [12]. The authors proposed a threshold-baed detector which solves the binary classification task by classifying in-distribution and out-of-distribution using the prediction of the given network. Recently, a number of reserchs are proposed to enhance the performance of the baseline detector [21,20] but in this paper, we follow only the baseline detector algorithm without any input enhancement and temperature scaling [21]. Setup: We compare the OOD detector performance using CIFAR-100 trained models described in Section 4. Evaluation Metrics and Out-of-distributions: In this work, we follow the experimental setting used in [12,21]. To measure the performance of the OOD detector, we report the true negative rate (TNR) at 95% true positive rate (TPR), the area under the receiver operating characteristic curve (AUROC) and detection accuracy of each OOD detector. We use seven datasets for out-of-distribution: TinyIma-geNet (crop), TinyImageNet (resize), LSUN [45] (crop), LSUN (resize), iSUN, Uniform noise and Gaussian noise. Results: We report OOD detector performance to seven OODs in Table A3. Overall, CutMix outperforms baseline, Mixup and Cutout. Moreover, we find that even though Mixup and Cutout outperform the classification performance, Mixup and Cutout largely degenerate the baseline detector performance. Especially, for Uniform noise and Gaussian noise, Mixup and Cutout seriously impair the baseline performance while CutMix dramatically improves the performance. From the experiments, we observe that our proposed CutMix enhances the OOD detector performance while Mixup and Cutout produce more overconfident predictions to OOD samples than the baseline.\n\n\nMethod\n\nTNR at TPR 95% AUROC Detection Acc. TNR at TPR 95% AUROC Detection Acc.\n\nTinyImageNet TinyImageNet (resize)\n\nFigure 1 :\n1Class activation mapping (CAM)\n\nFigure 2 :\n2Top-1 test error plot for CIFAR100 (left) and Im-ageNet (right) classification. Cutmix can avoid overfitting problem and achieves lower test errors than the baseline at the end of training.\n\nFigure 3 :\n3Impact of \u03b1 and CutMix layer depth on top-1 error on CIFAR-100.\n\nFigure 4 :\n4Qualitative comparison of the baseline (ResNet-50), Mixup, Cutout, and CutMix for weakly supervised object localization task on CUB-200-2011 dataset. Ground truth and the estimated results are denoted as red and green, respectively.\n\nFigure 5 :\n5Robustness experiments on ImageNet validation set.\n\n\nFinally, we evaluate the top-1 error of Mixup and Cut-Mix in-between samples. The probability to predict neither\n\n\n1.2. For comparison, we use PyramidNet-200 model without any regularization method, PyramidNet-200 model with Mixup, PyramidNet-200 model with Cutout and PyramidNet-200 model with our proposed CutMix.\n\nTable 1 :\n1Overview of the results of Mixup, Cutout, and our CutMix on ImageNet classification, ImageNet localization, and Pascal VOC 07 detection (transfer learning with SSD[23] finetuning) tasks. Note that CutMix improves the performance on various tasks.\n\nTable 4 :\n4Impact of CutMix on ImageNet classification for ResNet-101 and ResNext-101.achieves the best result, 21.60% top-1 error, among the considered augmentation strategies. CutMix outperforms Cutout and Mixup, the two closest approaches to ours, by +1.33% and +0.98%, respectively. On the feature level as well, we find CutMix preferable to Mixup, with top-1 errors 21.78% and 22.50%, respectively. Comparison against architectural improvements: We have also compared improvements due to CutMix against the improvements due to architectural improvements (e.g. greater depth or additional modules). We observe that Cut-Mix improves the performance by +2.08% while increased depth (ResNet-50 \u2192 ResNet-152) boosts +1.99% and SE\n\nTable 5 :\n5Comparison of state-of-the-art regularization meth-\nods on CIFAR-100. \n\nModel \n# Params \nTop-1 \nErr (%) \n\nTop-5 \nErr (%) \n\nPyramidNet-110 (\u03b1 = 64) [10] \n1.7 M \n19.85 \n4.66 \nPyramidNet-110 + CutMix \n1.7 M \n17.97 \n3.83 \n\nResNet-110 [11] \n1.1 M \n23.14 \n5.95 \nResNet-110 + CutMix \n1.1 M \n20.11 \n4.43 \n\n\n\nTable 6 :\n6Impact of CutMix on lighter architectures on \nCIFAR-100. \n\nPyramidNet-200 (\u03b1=240) \nTop-1 Error (%) \n\nBaseline \n3.85 \n+ Cutout \n3.10 \n+ Mixup (\u03b1=1.0) \n3.09 \n+ Manifold Mixup (\u03b1=1.0) \n3.15 \n+ CutMix \n2.88 \n\n\n\nTable 7 :\n7Impact of CutMix on CIFAR-10.rate for Stochastic Depth [16] is set to 0.25. For Mixup [46], \nwe tested the hyper-parameter \u03b1 with 0.5 and 1.0. For Mani-\nfold Mixup [40], we applied Mixup operation at a randomly \nchosen layer per minibatch. \n\n\nTable 8 :\n8Performance of CutMix variants on CIFAR-100.Method \nCUB200-2011 \nLoc Acc (%) \n\nImageNet \nLoc Acc (%) \n\nResNet-50 \n49.41 \n46.30 \nMixup [46] \n49.30 \n45.84 \nCutout [2] \n52.78 \n46.69 \nCutMix \n54.81 \n47.25 \n\n\n\nTable 10 :\n10Impact of CutMix on transfer learning of pretrained model to other tasks, object detection and image captioning.\n\nTable 10\n10shows the result performance when ap-\nplying each pretrained model. CutMix outperforms Mixup \nand Cutout in both BLEU1 and BLEU4 metrics. \nNote that simply replacing backbone network with our \nCutMix-pretrained model gives clear performance gains for \nobject detection and image captioning tasks. \n\n\n\nTable 11 :\n11Top-1 accuracy after FGSM white-box attack on \nImageNet validation set. \n\nMethod TNR at TPR 95% AUROC Detection Acc. \n\n\n\n\nEvaluation metric: To measure the localization accuracyMethod \nCUB200-2011 ImageNet-1K \n\nTop-1 Loc (%) Top-1 Loc (%) \n\nVGG16 + CAM* [50] \n-\n42.80 \nVGG16 + ACoL* [47] \n45.92 \n45.83 \nInceptionV3 + SPG* [48] \n46.64 \n48.60 \n\nResNet-50 \n49.41 \n46.30 \nResNet-50 + Mixup \n49.30 \n45.84 \nResNet-50 + Cutout \n52.78 \n46.69 \nResNet-50 + CutMix \n54.81 \n47.25 \n\nTable A1: Weakly supervised object localization results on \nCUB200-2011 and ImageNet-1K dataset. * denotes results \nreported in the original papers. \n\n\n\n1 :\n1for each iteration do2:    input, target = get minibatch(dataset) input is N\u00d7C\u00d7W\u00d7H size tensor, target is N\u00d7K size tensor.C. Transfer Learning to Object DetectionWe evaluate the models on the Pascal VOC 2007 detection benchmark[5] with 5 K test images over 20 object categories. For training, we use both VOC2007 and VOC2012 trainval (VOC07+12). Finetuning on SSD 23: \n\nif mode == training then \n\n4: \n\ninput s, target s = shuffle minibatch(input, target) \nCutMix starts here. \n\n5: \n\nlambda = Unif(0,1) \n\n6: \n\nr x = Unif(0,W) \n\n7: \n\nr y = Unif(0,H) \n\n8: \n\nr w = Sqrt(1 -lambda) \n\n9: \n\nr h = Sqrt(1 -lambda) \n\n10: \n\nx1 = Round(Clip(r x -r w / 2, min=0)) \n\n11: \n\nx2 = Round(Clip(r x + r w / 2, max=W)) \n\n12: \n\ny1 = Round(Clip(r y -r h / 2, min=0)) \n\n13: \n\ny2 = Round(Clip(r y + r h / 2, min=H)) \n\n14: \n\ninput[:, :, x1:x2, y1:y2] = input s[:, :, x1:x2, y1:y2] \n\n15: \n\ntarget = lambda * target + (1 -lambda) * target s \nCutMix ends. \n\n16: \n\nend if \n\n17: \n\noutput = model forward(input) \n\n18: \n\nloss = compute loss(output, target) \n\n19: \n\nmodel update() \n20: end for \n\n\n\n\n1.1. For comparison, we use ResNet-50 trained without any additional regularization or augmentation techniques, ResNet-50 trained by Mixup strategy, ResNet-50 trained by Cutout strategy and ResNet-50 trained by our proposed CutMix strategy. Fast Gradient Sign Method (FGSM): We employ Fast Gradient Sign Method (FGSM)\n\n\nBLEU1 BLEU2 BLEU3 BLEU4 METEOR ROUGE CIDERResNet-50 (Baseline) \n61.4 \n43.8 \n31.4 \n22.9 \n22.8 \n44.7 \n71.2 \nResNet-50 + Mixup \n61.6 \n44.1 \n31.6 \n23.2 \n22.9 \n47.9 \n72.2 \nResNet-50 + Cutout \n63.0 \n45.3 \n32.6 \n24.0 \n22.6 \n48.2 \n74.1 \nResNet-50 + CutMix \n64.2 \n46.3 \n33.6 \n24.9 \n23.1 \n49.0 \n77.6 \n\n\n\nTable A2 :\nA2Image captioning results on MS-COCO dataset.\nWe use ImageNet-pretrained ResNet-50 provided by PyTorch[28].\nhttps://github.com/amdegroot/ssd.pytorch 3 https://github.com/jwyang/faster-rcnn.pytorch 4 https://github.com/stevehuanghe/image captioning GoogLeNet to ResNet-50. For training, we set batch size, learning rate, and training epochs to 20, 0.001, and 100, respectively. For evaluation, the beam size is set to 20 for all the experiments. Image captioning results with various metrics are shown inTable A2.\nAcknowledgementWe are grateful to Clova AI members with valuable discussions, and to Ziad Al-Halah for proofreading the manuscript.\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. L.-C Chen, G Papandreou, I Kokkinos, K Murphy, A L Yuille, IEEE transactions on pattern analysis and machine intelligence. 40L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully con- nected crfs. IEEE transactions on pattern analysis and ma- chine intelligence, 40(4):834-848, 2018.\n\nT Devries, G W Taylor, arXiv:1708.04552Improved regularization of convolutional neural networks with cutout. arXiv preprintT. DeVries and G. W. Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.\n\nModeling visual context is key to augmenting object detection datasets. N Dvornik, J Mairal, C Schmid, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)N. Dvornik, J. Mairal, and C. Schmid. Modeling visual con- text is key to augmenting object detection datasets. In Pro- ceedings of the European Conference on Computer Vision (ECCV), pages 364-380, 2018.\n\nCut, paste and learn: Surprisingly easy synthesis for instance detection. D Dwibedi, I Misra, M Hebert, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionD. Dwibedi, I. Misra, and M. Hebert. Cut, paste and learn: Surprisingly easy synthesis for instance detection. In Pro- ceedings of the IEEE International Conference on Computer Vision, pages 1301-1310, 2017.\n\nThe pascal visual object classes (voc) challenge. M Everingham, L Van Gool, C K I Williams, J Winn, A Zisserman, International Journal of Computer Vision. 882M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) chal- lenge. International Journal of Computer Vision, 88(2):303- 338, June 2010.\n\nImagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. R Geirhos, P Rubisch, C Michaelis, M Bethge, F A Wichmann, W Brendel, arXiv:1811.12231arXiv preprintR. Geirhos, P. Rubisch, C. Michaelis, M. Bethge, F. A. Wich- mann, and W. Brendel. Imagenet-trained cnns are biased to- wards texture; increasing shape bias improves accuracy and robustness. arXiv preprint arXiv:1811.12231, 2018.\n\nDropblock: A regularization method for convolutional networks. G Ghiasi, T.-Y. Lin, Q V Le, Advances in Neural Information Processing Systems. G. Ghiasi, T.-Y. Lin, and Q. V. Le. Dropblock: A regular- ization method for convolutional networks. In Advances in Neural Information Processing Systems, pages 10750- 10760, 2018.\n\nExplaining and harnessing adversarial examples. I J Goodfellow, J Shlens, C Szegedy, International Conference on Learning Representations. I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In International Confer- ence on Learning Representations, 2015.\n\nH Guo, Y Mao, R Zhang, arXiv:1809.02499Mixup as locally linear out-of-manifold regularization. arXiv preprintH. Guo, Y. Mao, and R. Zhang. Mixup as locally linear out-of-manifold regularization. arXiv preprint arXiv:1809.02499, 2018.\n\nDeep pyramidal residual networks. D Han, J Kim, J Kim, CVPR. D. Han, J. Kim, and J. Kim. Deep pyramidal residual net- works. In CVPR, 2017.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR. K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.\n\nA baseline for detecting misclassified and out-of-distribution examples in neural networks. D Hendrycks, K Gimpel, International Conference on Learning Representations. D. Hendrycks and K. Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural net- works. In International Conference on Learning Represen- tations, 2017.\n\nGatherexcite: Exploiting feature context in convolutional neural networks. J Hu, L Shen, S Albanie, G Sun, A Vedaldi, Advances in Neural Information Processing Systems. J. Hu, L. Shen, S. Albanie, G. Sun, and A. Vedaldi. Gather- excite: Exploiting feature context in convolutional neural networks. In Advances in Neural Information Processing Systems, pages 9423-9433, 2018.\n\nSqueeze-and-excitation networks. J Hu, L Shen, G Sun, arXiv:1709.01507J. Hu, L. Shen, and G. Sun. Squeeze-and-excitation net- works. In arXiv:1709.01507, 2017.\n\nDensely connected convolutional networks. G Huang, Z Liu, K Q Weinberger, CVPR. G. Huang, Z. Liu, and K. Q. Weinberger. Densely connected convolutional networks. In CVPR, 2017.\n\nDeep networks with stochastic depth. G Huang, Y Sun, Z Liu, D Sedra, K Weinberger, ECCV. G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Weinberger. Deep networks with stochastic depth. In ECCV, 2016.\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, ICML. S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015.\n\nNSML: meet the mlaas platform with a real-world case study. H Kim, M Kim, D Seo, J Kim, H Park, S Park, H Jo, K Kim, Y Yang, Y Kim, N Sung, J Ha, abs/1810.09957CoRRH. Kim, M. Kim, D. Seo, J. Kim, H. Park, S. Park, H. Jo, K. Kim, Y. Yang, Y. Kim, N. Sung, and J. Ha. NSML: meet the mlaas platform with a real-world case study. CoRR, abs/1810.09957, 2018.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, NIPS. A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet clas- sification with deep convolutional neural networks. In NIPS, 2012.\n\nA simple unified framework for detecting out-of-distribution samples and adversarial attacks. K Lee, K Lee, H Lee, J Shin, Advances in Neural Information Processing Systems. K. Lee, K. Lee, H. Lee, and J. Shin. A simple unified frame- work for detecting out-of-distribution samples and adversar- ial attacks. In Advances in Neural Information Processing Systems, pages 7167-7177, 2018.\n\nEnhancing the reliability of out-of-distribution image detection in neural networks. S Liang, Y Li, R Srikant, In International Conference on Learning Representations. S. Liang, Y. Li, and R. Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. In In- ternational Conference on Learning Representations, 2018.\n\nMicrosoft coco: Common objects in context. T.-Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Doll\u00e1r, C L Zitnick, European conference on computer vision. SpringerT.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra- manan, P. Doll\u00e1r, and C. L. Zitnick. Microsoft coco: Com- mon objects in context. In European conference on computer vision, pages 740-755. Springer, 2014.\n\nSsd: Single shot multibox detector. W Liu, D Anguelov, D Erhan, C Szegedy, S Reed, C.-Y Fu, A C Berg, European conference on computer vision. W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.- Y. Fu, and A. C. Berg. Ssd: Single shot multibox detector. In European conference on computer vision, pages 21-37.\n\n. Springer, Springer, 2016.\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). J. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In The IEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR), June 2015.\n\nTowards deep learning models resistant to adversarial attacks. A Madry, A Makelov, L Schmidt, D Tsipras, A Vladu, arXiv:1706.06083arXiv preprintA. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adver- sarial attacks. arXiv preprint arXiv:1706.06083, 2017.\n\nExploring the limits of weakly supervised pretraining. D Mahajan, R Girshick, V Ramanathan, K He, M Paluri, Y Li, A Bharambe, L Van Der Maaten, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)D. Mahajan, R. Girshick, V. Ramanathan, K. He, M. Paluri, Y. Li, A. Bharambe, and L. van der Maaten. Exploring the limits of weakly supervised pretraining. In Proceedings of the European Conference on Computer Vision (ECCV), pages 181-196, 2018.\n\nLearning multi-domain convolutional neural networks for visual tracking. H Nam, B Han, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionH. Nam and B. Han. Learning multi-domain convolutional neural networks for visual tracking. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 4293-4302, 2016.\n\nAutomatic differentiation in pytorch. A Paszke, S Gross, S Chintala, G Chanan, E Yang, Z De-Vito, Z Lin, A Desmaison, L Antiga, A Lerer, A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. De- Vito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. Auto- matic differentiation in pytorch. 2017.\n\nFaster r-cnn: Towards real-time object detection with region proposal networks. S Ren, K He, R Girshick, J Sun, NIPS. S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In NIPS, 2015.\n\nImagenet large scale visual recognition challenge. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, A C Berg, L Fei-Fei, International Journal of Computer Vision. 1153O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. Imagenet large scale visual recog- nition challenge. International Journal of Computer Vision, 115(3):211-252, 2015.\n\nTwo-stream convolutional networks for action recognition in videos. K Simonyan, A Zisserman, Advances in neural information processing systems. K. Simonyan and A. Zisserman. Two-stream convolutional networks for action recognition in videos. In Advances in neural information processing systems, pages 568-576, 2014.\n\nHide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization. K K Singh, Y J Lee, 2017 IEEE International Conference on Computer Vision (ICCV). IEEEK. K. Singh and Y. J. Lee. Hide-and-seek: Forcing a net- work to be meticulous for weakly-supervised object and ac- tion localization. In 2017 IEEE International Conference on Computer Vision (ICCV), pages 3544-3553. IEEE, 2017.\n\nDropout: A simple way to prevent neural networks from overfitting. N Srivastava, G Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov, Journal of Machine Learning Research. 15N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: A simple way to prevent neu- ral networks from overfitting. Journal of Machine Learning Research, 15:1929-1958, 2014.\n\nImproved mixed-example data augmentation. C Summers, M J Dinneen, 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). C. Summers and M. J. Dinneen. Improved mixed-example data augmentation. In 2019 IEEE Winter Conference on Ap- plications of Computer Vision (WACV), pages 1262-1270.\n\n. IEEE. IEEE, 2019.\n\nInception-v4, inception-resnet and the impact of residual connections on learning. C Szegedy, S Ioffe, V Vanhoucke, ICLR Workshop. C. Szegedy, S. Ioffe, and V. Vanhoucke. Inception-v4, inception-resnet and the impact of residual connections on learning. In ICLR Workshop, 2016.\n\nGoing deeper with convolutions. C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, D Anguelov, D Erhan, V Vanhoucke, A Rabinovich, CVPR. C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In CVPR, 2015.\n\nRethinking the inception architecture for computer vision. C Szegedy, V Vanhoucke, S Ioffe, J Shlens, Z Wojna, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionC. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2818-2826, 2016.\n\nC Szegedy, W Zaremba, I Sutskever, J Bruna, D Erhan, I Goodfellow, R Fergus, arXiv:1312.6199Intriguing properties of neural networks. arXiv preprintC. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.\n\nBetween-class learning for image classification. Y Tokozume, Y Ushiku, T Harada, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionY. Tokozume, Y. Ushiku, and T. Harada. Between-class learning for image classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5486-5494, 2018.\n\nV Verma, A Lamb, C Beckham, A Courville, I Mitliagkis, Y Bengio, arXiv:1806.05236Manifold mixup: Encouraging meaningful on-manifold interpolation as a regularizer. arXiv preprintV. Verma, A. Lamb, C. Beckham, A. Courville, I. Mitliagkis, and Y. Bengio. Manifold mixup: Encouraging meaningful on-manifold interpolation as a regularizer. arXiv preprint arXiv:1806.05236, 2018.\n\nShow and tell: A neural image caption generator. O Vinyals, A Toshev, S Bengio, D Erhan, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionO. Vinyals, A. Toshev, S. Bengio, and D. Erhan. Show and tell: A neural image caption generator. In Proceedings of the IEEE conference on computer vision and pattern recog- nition, pages 3156-3164, 2015.\n\nThe Caltech-UCSD Birds-200-2011 Dataset. C Wah, S Branson, P Welinder, P Perona, S Belongie, - port CNS-TR-2011-001California Institute of TechnologyTechnical ReC. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie. The Caltech-UCSD Birds-200-2011 Dataset. Technical Re- port CNS-TR-2011-001, California Institute of Technology, 2011.\n\nAggregated residual transformations for deep neural networks. S Xie, R Girshick, P Doll\u00e1r, Z Tu, K He, CVPR. S. Xie, R. Girshick, P. Doll\u00e1r, Z. Tu, and K. He. Aggregated residual transformations for deep neural networks. In CVPR, 2017.\n\nShakedrop regularization for deep residual learning. Y Yamada, M Iwamura, T Akiba, K Kise, arXiv:1802.02375arXiv preprintY. Yamada, M. Iwamura, T. Akiba, and K. Kise. Shake- drop regularization for deep residual learning. arXiv preprint arXiv:1802.02375, 2018.\n\nF Yu, A Seff, Y Zhang, S Song, T Funkhouser, J Xiao, Lsun, arXiv:1506.03365Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprintF. Yu, A. Seff, Y. Zhang, S. Song, T. Funkhouser, and J. Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.\n\nH Zhang, M Cisse, Y N Dauphin, D Lopez-Paz, arXiv:1710.09412mixup: Beyond empirical risk minimization. arXiv preprintH. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.\n\nAdversarial complementary learning for weakly supervised object localization. X Zhang, Y Wei, J Feng, Y Yang, T S Huang, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionX. Zhang, Y. Wei, J. Feng, Y. Yang, and T. S. Huang. Ad- versarial complementary learning for weakly supervised ob- ject localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1325- 1334, 2018.\n\nSelfproduced guidance for weakly-supervised object localization. X Zhang, Y Wei, G Kang, Y Yang, T Huang, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)X. Zhang, Y. Wei, G. Kang, Y. Yang, and T. Huang. Self- produced guidance for weakly-supervised object localiza- tion. In Proceedings of the European Conference on Com- puter Vision (ECCV), pages 597-613, 2018.\n\nZ Zhong, L Zheng, G Kang, S Li, Y Yang, arXiv:1708.04896Random erasing data augmentation. arXiv preprintZ. Zhong, L. Zheng, G. Kang, S. Li, and Y. Yang. Random erasing data augmentation. arXiv preprint arXiv:1708.04896, 2017.\n\nLearning deep features for discriminative localization. B Zhou, A Khosla, A Lapedriza, A Oliva, A Torralba, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionB. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning deep features for discriminative localization. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2921-2929, 2016.\n\nOut-of-distribution (OOD) detection results on TinyImageNet, LSUN, iSUN, Gaussian noise and Uniform noise using CIFAR-100 trained models. All numbers are in percents. A3 Table, higher is betterTable A3: Out-of-distribution (OOD) detection results on TinyImageNet, LSUN, iSUN, Gaussian noise and Uniform noise using CIFAR-100 trained models. All numbers are in percents; higher is better.\n", "annotations": {"author": "[{\"end\":132,\"start\":89},{\"end\":177,\"start\":133},{\"end\":227,\"start\":178},{\"end\":273,\"start\":228},{\"end\":306,\"start\":274},{\"end\":352,\"start\":307}]", "publisher": null, "author_last_name": "[{\"end\":100,\"start\":97},{\"end\":145,\"start\":142},{\"end\":191,\"start\":189},{\"end\":241,\"start\":237},{\"end\":285,\"start\":281},{\"end\":320,\"start\":317}]", "author_first_name": "[{\"end\":96,\"start\":89},{\"end\":141,\"start\":133},{\"end\":183,\"start\":178},{\"end\":188,\"start\":184},{\"end\":236,\"start\":228},{\"end\":280,\"start\":274},{\"end\":316,\"start\":307}]", "author_affiliation": "[{\"end\":131,\"start\":102},{\"end\":176,\"start\":147},{\"end\":226,\"start\":193},{\"end\":272,\"start\":243},{\"end\":305,\"start\":287},{\"end\":351,\"start\":322}]", "title": "[{\"end\":86,\"start\":1},{\"end\":438,\"start\":353}]", "venue": null, "abstract": "[{\"end\":1901,\"start\":440}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2062,\"start\":2058},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2065,\"start\":2062},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2068,\"start\":2065},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2140,\"start\":2136},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2143,\"start\":2140},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2170,\"start\":2167},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2173,\"start\":2170},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2198,\"start\":2194},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2201,\"start\":2198},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2347,\"start\":2343},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2382,\"start\":2378},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2385,\"start\":2382},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2388,\"start\":2385},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2614,\"start\":2610},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2680,\"start\":2677},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":2683,\"start\":2680},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2686,\"start\":2683},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2688,\"start\":2686},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2952,\"start\":2948},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2954,\"start\":2952},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3125,\"start\":3122},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3128,\"start\":3125},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":3161,\"start\":3157},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3327,\"start\":3323},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":4186,\"start\":4182},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":4553,\"start\":4549},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4565,\"start\":4562},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5360,\"start\":5356},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5410,\"start\":5406},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":5622,\"start\":5618},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5640,\"start\":5636},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5924,\"start\":5921},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5984,\"start\":5980},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6101,\"start\":6097},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6104,\"start\":6101},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6169,\"start\":6166},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":6172,\"start\":6169},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6175,\"start\":6172},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6470,\"start\":6467},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6789,\"start\":6786},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6837,\"start\":6833},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7255,\"start\":7252},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7257,\"start\":7255},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":7541,\"start\":7537},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7544,\"start\":7541},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7879,\"start\":7875},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7882,\"start\":7879},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7884,\"start\":7882},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8336,\"start\":8332},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8366,\"start\":8362},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8483,\"start\":8479},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8485,\"start\":8483},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8488,\"start\":8485},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8534,\"start\":8530},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8537,\"start\":8534},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9369,\"start\":9365},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10522,\"start\":10518},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10525,\"start\":10522},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":10988,\"start\":10984},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11777,\"start\":11774},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":11792,\"start\":11788},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":11894,\"start\":11890},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":13139,\"start\":13135},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13192,\"start\":13188},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14528,\"start\":14524},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14555,\"start\":14551},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14687,\"start\":14683},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14922,\"start\":14918},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":14924,\"start\":14922},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14927,\"start\":14924},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":14930,\"start\":14927},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15005,\"start\":15001},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15017,\"start\":15014},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":15029,\"start\":15025},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15622,\"start\":15618},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15652,\"start\":15649},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15761,\"start\":15758},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":15888,\"start\":15884},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":16429,\"start\":16425},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":16441,\"start\":16437},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16843,\"start\":16839},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":16872,\"start\":16868},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17355,\"start\":17351},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17765,\"start\":17762},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17795,\"start\":17792},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18054,\"start\":18051},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":18079,\"start\":18075},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":18234,\"start\":18231},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":18372,\"start\":18368},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":18396,\"start\":18392},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":18998,\"start\":18994},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":19203,\"start\":19199},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20618,\"start\":20615},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20810,\"start\":20807},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20813,\"start\":20810},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":22019,\"start\":22015},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":22022,\"start\":22019},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":22025,\"start\":22022},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":22279,\"start\":22275},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":22300,\"start\":22296},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":22397,\"start\":22393},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":22644,\"start\":22640},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23106,\"start\":23103},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":23409,\"start\":23405},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":23412,\"start\":23409},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":23415,\"start\":23412},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":23898,\"start\":23894},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":23910,\"start\":23907},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":24194,\"start\":24190},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":24215,\"start\":24211},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":24470,\"start\":24467},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":24611,\"start\":24607},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":24614,\"start\":24611},{\"end\":24717,\"start\":24709},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":25151,\"start\":25147},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":25264,\"start\":25260},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":25409,\"start\":25405},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":25704,\"start\":25701},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":25707,\"start\":25704},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25829,\"start\":25825},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":26224,\"start\":26221},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":27867,\"start\":27863},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":28154,\"start\":28150},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":28157,\"start\":28154},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":28192,\"start\":28188},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30611,\"start\":30607},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":30939,\"start\":30935},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":31063,\"start\":31059},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":31262,\"start\":31258},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":31265,\"start\":31262},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":31626,\"start\":31622},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":32274,\"start\":32270},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":32605,\"start\":32601},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":33155,\"start\":33151},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":33158,\"start\":33155},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":33161,\"start\":33158},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":33204,\"start\":33200},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":33839,\"start\":33835},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":33842,\"start\":33839},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":33845,\"start\":33842},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":33888,\"start\":33884},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":34437,\"start\":34433},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":34761,\"start\":34757},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":34845,\"start\":34841},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":35245,\"start\":35242},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":36696,\"start\":36692},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":36822,\"start\":36818},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":37113,\"start\":37109},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":37116,\"start\":37113},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":37241,\"start\":37237},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":37451,\"start\":37447},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":37454,\"start\":37451},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":37789,\"start\":37785},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":39748,\"start\":39744},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":42869,\"start\":42866},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":44437,\"start\":44433}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":38660,\"start\":38617},{\"attributes\":{\"id\":\"fig_2\"},\"end\":38863,\"start\":38661},{\"attributes\":{\"id\":\"fig_3\"},\"end\":38940,\"start\":38864},{\"attributes\":{\"id\":\"fig_4\"},\"end\":39186,\"start\":38941},{\"attributes\":{\"id\":\"fig_5\"},\"end\":39250,\"start\":39187},{\"attributes\":{\"id\":\"fig_6\"},\"end\":39365,\"start\":39251},{\"attributes\":{\"id\":\"fig_8\"},\"end\":39568,\"start\":39366},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":39827,\"start\":39569},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":40558,\"start\":39828},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":40869,\"start\":40559},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":41087,\"start\":40870},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":41341,\"start\":41088},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":41557,\"start\":41342},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":41684,\"start\":41558},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":41996,\"start\":41685},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":42130,\"start\":41997},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":42632,\"start\":42131},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":43702,\"start\":42633},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":44022,\"start\":43703},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":44317,\"start\":44023},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":44376,\"start\":44318}]", "paragraph": "[{\"end\":3467,\"start\":1917},{\"end\":4144,\"start\":3469},{\"end\":5059,\"start\":4146},{\"end\":6122,\"start\":5061},{\"end\":8701,\"start\":6140},{\"end\":8755,\"start\":8712},{\"end\":9127,\"start\":8769},{\"end\":9731,\"start\":9179},{\"end\":9967,\"start\":9733},{\"end\":10132,\"start\":9969},{\"end\":10385,\"start\":10210},{\"end\":10801,\"start\":10387},{\"end\":11337,\"start\":10816},{\"end\":12066,\"start\":11361},{\"end\":13250,\"start\":12068},{\"end\":13683,\"start\":13252},{\"end\":14436,\"start\":13699},{\"end\":14595,\"start\":14438},{\"end\":15278,\"start\":14646},{\"end\":15451,\"start\":15280},{\"end\":16993,\"start\":15453},{\"end\":17703,\"start\":17018},{\"end\":19476,\"start\":17705},{\"end\":19885,\"start\":19497},{\"end\":20416,\"start\":19887},{\"end\":21386,\"start\":20418},{\"end\":22576,\"start\":21428},{\"end\":23477,\"start\":22578},{\"end\":25510,\"start\":23519},{\"end\":26186,\"start\":25541},{\"end\":26559,\"start\":26188},{\"end\":27328,\"start\":26561},{\"end\":28529,\"start\":27330},{\"end\":29574,\"start\":28544},{\"end\":30099,\"start\":29598},{\"end\":30262,\"start\":30101},{\"end\":30847,\"start\":30307},{\"end\":32559,\"start\":30849},{\"end\":33162,\"start\":32580},{\"end\":33846,\"start\":33188},{\"end\":34696,\"start\":33848},{\"end\":34881,\"start\":34741},{\"end\":35007,\"start\":34915},{\"end\":35432,\"start\":35027},{\"end\":35931,\"start\":35434},{\"end\":36517,\"start\":35933},{\"end\":38498,\"start\":36538},{\"end\":38580,\"start\":38509},{\"end\":38616,\"start\":38582}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9178,\"start\":9128},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10209,\"start\":10133}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":3657,\"start\":3650},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":4385,\"start\":4378},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":4521,\"start\":4514},{\"end\":11432,\"start\":11425},{\"end\":12914,\"start\":12907},{\"end\":15166,\"start\":15159},{\"end\":16387,\"start\":16380},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":16905,\"start\":16898},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":17509,\"start\":17502},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":19104,\"start\":19097},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":19373,\"start\":19366},{\"end\":20340,\"start\":20333},{\"end\":22502,\"start\":22495},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":26450,\"start\":26442},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":27165,\"start\":27157},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":28055,\"start\":28047},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":32998,\"start\":32990},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":33651,\"start\":33643},{\"end\":37922,\"start\":37914}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1915,\"start\":1903},{\"attributes\":{\"n\":\"2.\"},\"end\":6138,\"start\":6125},{\"attributes\":{\"n\":\"3.\"},\"end\":8710,\"start\":8704},{\"attributes\":{\"n\":\"3.1.\"},\"end\":8767,\"start\":8758},{\"attributes\":{\"n\":\"3.2.\"},\"end\":10814,\"start\":10804},{\"end\":11359,\"start\":11340},{\"attributes\":{\"n\":\"4.\"},\"end\":13697,\"start\":13686},{\"attributes\":{\"n\":\"4.1.\"},\"end\":14618,\"start\":14598},{\"attributes\":{\"n\":\"4.1.1\"},\"end\":14644,\"start\":14621},{\"attributes\":{\"n\":\"4.1.2\"},\"end\":17016,\"start\":16996},{\"attributes\":{\"n\":\"4.1.3\"},\"end\":19495,\"start\":19479},{\"attributes\":{\"n\":\"4.2.\"},\"end\":21426,\"start\":21389},{\"attributes\":{\"n\":\"4.3.\"},\"end\":23517,\"start\":23480},{\"attributes\":{\"n\":\"4.4.\"},\"end\":25539,\"start\":25513},{\"attributes\":{\"n\":\"5.\"},\"end\":28542,\"start\":28532},{\"end\":29596,\"start\":29577},{\"end\":30305,\"start\":30265},{\"end\":32578,\"start\":32562},{\"end\":33186,\"start\":33165},{\"end\":34739,\"start\":34699},{\"end\":34913,\"start\":34884},{\"end\":35025,\"start\":35010},{\"end\":36536,\"start\":36520},{\"end\":38507,\"start\":38501},{\"end\":38628,\"start\":38618},{\"end\":38672,\"start\":38662},{\"end\":38875,\"start\":38865},{\"end\":38952,\"start\":38942},{\"end\":39198,\"start\":39188},{\"end\":39579,\"start\":39570},{\"end\":39838,\"start\":39829},{\"end\":40569,\"start\":40560},{\"end\":40880,\"start\":40871},{\"end\":41098,\"start\":41089},{\"end\":41352,\"start\":41343},{\"end\":41569,\"start\":41559},{\"end\":41694,\"start\":41686},{\"end\":42008,\"start\":41998},{\"end\":42637,\"start\":42634},{\"end\":44329,\"start\":44319}]", "table": "[{\"end\":40869,\"start\":40571},{\"end\":41087,\"start\":40882},{\"end\":41341,\"start\":41129},{\"end\":41557,\"start\":41398},{\"end\":41996,\"start\":41697},{\"end\":42130,\"start\":42011},{\"end\":42632,\"start\":42188},{\"end\":43702,\"start\":43004},{\"end\":44317,\"start\":44067}]", "figure_caption": "[{\"end\":38660,\"start\":38630},{\"end\":38863,\"start\":38674},{\"end\":38940,\"start\":38877},{\"end\":39186,\"start\":38954},{\"end\":39250,\"start\":39200},{\"end\":39365,\"start\":39253},{\"end\":39568,\"start\":39368},{\"end\":39827,\"start\":39581},{\"end\":40558,\"start\":39840},{\"end\":41129,\"start\":41100},{\"end\":41398,\"start\":41354},{\"end\":41684,\"start\":41572},{\"end\":42188,\"start\":42133},{\"end\":43004,\"start\":42639},{\"end\":44022,\"start\":43705},{\"end\":44067,\"start\":44025},{\"end\":44376,\"start\":44332}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11802,\"start\":11794},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13231,\"start\":13223},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19741,\"start\":19733},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19966,\"start\":19958},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22515,\"start\":22507},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22854,\"start\":22846},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":26754,\"start\":26745},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":27404,\"start\":27395}]", "bib_author_first_name": "[{\"end\":45093,\"start\":45089},{\"end\":45101,\"start\":45100},{\"end\":45115,\"start\":45114},{\"end\":45127,\"start\":45126},{\"end\":45137,\"start\":45136},{\"end\":45139,\"start\":45138},{\"end\":45487,\"start\":45486},{\"end\":45498,\"start\":45497},{\"end\":45500,\"start\":45499},{\"end\":45821,\"start\":45820},{\"end\":45832,\"start\":45831},{\"end\":45842,\"start\":45841},{\"end\":46246,\"start\":46245},{\"end\":46257,\"start\":46256},{\"end\":46266,\"start\":46265},{\"end\":46656,\"start\":46655},{\"end\":46670,\"start\":46669},{\"end\":46682,\"start\":46681},{\"end\":46686,\"start\":46683},{\"end\":46698,\"start\":46697},{\"end\":46706,\"start\":46705},{\"end\":47066,\"start\":47065},{\"end\":47077,\"start\":47076},{\"end\":47088,\"start\":47087},{\"end\":47101,\"start\":47100},{\"end\":47111,\"start\":47110},{\"end\":47113,\"start\":47112},{\"end\":47125,\"start\":47124},{\"end\":47460,\"start\":47459},{\"end\":47474,\"start\":47469},{\"end\":47481,\"start\":47480},{\"end\":47483,\"start\":47482},{\"end\":47770,\"start\":47769},{\"end\":47772,\"start\":47771},{\"end\":47786,\"start\":47785},{\"end\":47796,\"start\":47795},{\"end\":48020,\"start\":48019},{\"end\":48027,\"start\":48026},{\"end\":48034,\"start\":48033},{\"end\":48289,\"start\":48288},{\"end\":48296,\"start\":48295},{\"end\":48303,\"start\":48302},{\"end\":48442,\"start\":48441},{\"end\":48448,\"start\":48447},{\"end\":48457,\"start\":48456},{\"end\":48464,\"start\":48463},{\"end\":48668,\"start\":48667},{\"end\":48681,\"start\":48680},{\"end\":49008,\"start\":49007},{\"end\":49014,\"start\":49013},{\"end\":49022,\"start\":49021},{\"end\":49033,\"start\":49032},{\"end\":49040,\"start\":49039},{\"end\":49342,\"start\":49341},{\"end\":49348,\"start\":49347},{\"end\":49356,\"start\":49355},{\"end\":49512,\"start\":49511},{\"end\":49521,\"start\":49520},{\"end\":49528,\"start\":49527},{\"end\":49530,\"start\":49529},{\"end\":49685,\"start\":49684},{\"end\":49694,\"start\":49693},{\"end\":49701,\"start\":49700},{\"end\":49708,\"start\":49707},{\"end\":49717,\"start\":49716},{\"end\":49939,\"start\":49938},{\"end\":49948,\"start\":49947},{\"end\":50160,\"start\":50159},{\"end\":50167,\"start\":50166},{\"end\":50174,\"start\":50173},{\"end\":50181,\"start\":50180},{\"end\":50188,\"start\":50187},{\"end\":50196,\"start\":50195},{\"end\":50204,\"start\":50203},{\"end\":50210,\"start\":50209},{\"end\":50217,\"start\":50216},{\"end\":50225,\"start\":50224},{\"end\":50232,\"start\":50231},{\"end\":50240,\"start\":50239},{\"end\":50520,\"start\":50519},{\"end\":50534,\"start\":50533},{\"end\":50547,\"start\":50546},{\"end\":50549,\"start\":50548},{\"end\":50789,\"start\":50788},{\"end\":50796,\"start\":50795},{\"end\":50803,\"start\":50802},{\"end\":50810,\"start\":50809},{\"end\":51167,\"start\":51166},{\"end\":51176,\"start\":51175},{\"end\":51182,\"start\":51181},{\"end\":51480,\"start\":51476},{\"end\":51487,\"start\":51486},{\"end\":51496,\"start\":51495},{\"end\":51508,\"start\":51507},{\"end\":51516,\"start\":51515},{\"end\":51526,\"start\":51525},{\"end\":51537,\"start\":51536},{\"end\":51547,\"start\":51546},{\"end\":51549,\"start\":51548},{\"end\":51862,\"start\":51861},{\"end\":51869,\"start\":51868},{\"end\":51881,\"start\":51880},{\"end\":51890,\"start\":51889},{\"end\":51901,\"start\":51900},{\"end\":51912,\"start\":51908},{\"end\":51918,\"start\":51917},{\"end\":51920,\"start\":51919},{\"end\":52225,\"start\":52224},{\"end\":52233,\"start\":52232},{\"end\":52246,\"start\":52245},{\"end\":52574,\"start\":52573},{\"end\":52583,\"start\":52582},{\"end\":52594,\"start\":52593},{\"end\":52605,\"start\":52604},{\"end\":52616,\"start\":52615},{\"end\":52875,\"start\":52874},{\"end\":52886,\"start\":52885},{\"end\":52898,\"start\":52897},{\"end\":52912,\"start\":52911},{\"end\":52918,\"start\":52917},{\"end\":52928,\"start\":52927},{\"end\":52934,\"start\":52933},{\"end\":52946,\"start\":52945},{\"end\":53399,\"start\":53398},{\"end\":53406,\"start\":53405},{\"end\":53792,\"start\":53791},{\"end\":53802,\"start\":53801},{\"end\":53811,\"start\":53810},{\"end\":53823,\"start\":53822},{\"end\":53833,\"start\":53832},{\"end\":53841,\"start\":53840},{\"end\":53852,\"start\":53851},{\"end\":53859,\"start\":53858},{\"end\":53872,\"start\":53871},{\"end\":53882,\"start\":53881},{\"end\":54132,\"start\":54131},{\"end\":54139,\"start\":54138},{\"end\":54145,\"start\":54144},{\"end\":54157,\"start\":54156},{\"end\":54357,\"start\":54356},{\"end\":54372,\"start\":54371},{\"end\":54380,\"start\":54379},{\"end\":54386,\"start\":54385},{\"end\":54396,\"start\":54395},{\"end\":54408,\"start\":54407},{\"end\":54414,\"start\":54413},{\"end\":54423,\"start\":54422},{\"end\":54435,\"start\":54434},{\"end\":54445,\"start\":54444},{\"end\":54458,\"start\":54457},{\"end\":54460,\"start\":54459},{\"end\":54468,\"start\":54467},{\"end\":54850,\"start\":54849},{\"end\":54862,\"start\":54861},{\"end\":55204,\"start\":55203},{\"end\":55206,\"start\":55205},{\"end\":55215,\"start\":55214},{\"end\":55217,\"start\":55216},{\"end\":55587,\"start\":55586},{\"end\":55601,\"start\":55600},{\"end\":55611,\"start\":55610},{\"end\":55625,\"start\":55624},{\"end\":55638,\"start\":55637},{\"end\":55942,\"start\":55941},{\"end\":55953,\"start\":55952},{\"end\":55955,\"start\":55954},{\"end\":56307,\"start\":56306},{\"end\":56318,\"start\":56317},{\"end\":56327,\"start\":56326},{\"end\":56535,\"start\":56534},{\"end\":56546,\"start\":56545},{\"end\":56553,\"start\":56552},{\"end\":56560,\"start\":56559},{\"end\":56572,\"start\":56571},{\"end\":56580,\"start\":56579},{\"end\":56592,\"start\":56591},{\"end\":56601,\"start\":56600},{\"end\":56614,\"start\":56613},{\"end\":56847,\"start\":56846},{\"end\":56858,\"start\":56857},{\"end\":56871,\"start\":56870},{\"end\":56880,\"start\":56879},{\"end\":56890,\"start\":56889},{\"end\":57266,\"start\":57265},{\"end\":57277,\"start\":57276},{\"end\":57288,\"start\":57287},{\"end\":57301,\"start\":57300},{\"end\":57310,\"start\":57309},{\"end\":57319,\"start\":57318},{\"end\":57333,\"start\":57332},{\"end\":57632,\"start\":57631},{\"end\":57644,\"start\":57643},{\"end\":57654,\"start\":57653},{\"end\":57999,\"start\":57998},{\"end\":58008,\"start\":58007},{\"end\":58016,\"start\":58015},{\"end\":58027,\"start\":58026},{\"end\":58040,\"start\":58039},{\"end\":58054,\"start\":58053},{\"end\":58424,\"start\":58423},{\"end\":58435,\"start\":58434},{\"end\":58445,\"start\":58444},{\"end\":58455,\"start\":58454},{\"end\":58851,\"start\":58850},{\"end\":58858,\"start\":58857},{\"end\":58869,\"start\":58868},{\"end\":58881,\"start\":58880},{\"end\":58891,\"start\":58890},{\"end\":59214,\"start\":59213},{\"end\":59221,\"start\":59220},{\"end\":59233,\"start\":59232},{\"end\":59243,\"start\":59242},{\"end\":59249,\"start\":59248},{\"end\":59442,\"start\":59441},{\"end\":59452,\"start\":59451},{\"end\":59463,\"start\":59462},{\"end\":59472,\"start\":59471},{\"end\":59651,\"start\":59650},{\"end\":59657,\"start\":59656},{\"end\":59665,\"start\":59664},{\"end\":59674,\"start\":59673},{\"end\":59682,\"start\":59681},{\"end\":59696,\"start\":59695},{\"end\":60027,\"start\":60026},{\"end\":60036,\"start\":60035},{\"end\":60045,\"start\":60044},{\"end\":60047,\"start\":60046},{\"end\":60058,\"start\":60057},{\"end\":60358,\"start\":60357},{\"end\":60367,\"start\":60366},{\"end\":60374,\"start\":60373},{\"end\":60382,\"start\":60381},{\"end\":60390,\"start\":60389},{\"end\":60392,\"start\":60391},{\"end\":60849,\"start\":60848},{\"end\":60858,\"start\":60857},{\"end\":60865,\"start\":60864},{\"end\":60873,\"start\":60872},{\"end\":60881,\"start\":60880},{\"end\":61217,\"start\":61216},{\"end\":61226,\"start\":61225},{\"end\":61235,\"start\":61234},{\"end\":61243,\"start\":61242},{\"end\":61249,\"start\":61248},{\"end\":61500,\"start\":61499},{\"end\":61508,\"start\":61507},{\"end\":61518,\"start\":61517},{\"end\":61531,\"start\":61530},{\"end\":61540,\"start\":61539},{\"end\":62084,\"start\":62082}]", "bib_author_last_name": "[{\"end\":45098,\"start\":45094},{\"end\":45112,\"start\":45102},{\"end\":45124,\"start\":45116},{\"end\":45134,\"start\":45128},{\"end\":45146,\"start\":45140},{\"end\":45495,\"start\":45488},{\"end\":45507,\"start\":45501},{\"end\":45829,\"start\":45822},{\"end\":45839,\"start\":45833},{\"end\":45849,\"start\":45843},{\"end\":46254,\"start\":46247},{\"end\":46263,\"start\":46258},{\"end\":46273,\"start\":46267},{\"end\":46667,\"start\":46657},{\"end\":46679,\"start\":46671},{\"end\":46695,\"start\":46687},{\"end\":46703,\"start\":46699},{\"end\":46716,\"start\":46707},{\"end\":47074,\"start\":47067},{\"end\":47085,\"start\":47078},{\"end\":47098,\"start\":47089},{\"end\":47108,\"start\":47102},{\"end\":47122,\"start\":47114},{\"end\":47133,\"start\":47126},{\"end\":47467,\"start\":47461},{\"end\":47478,\"start\":47475},{\"end\":47486,\"start\":47484},{\"end\":47783,\"start\":47773},{\"end\":47793,\"start\":47787},{\"end\":47804,\"start\":47797},{\"end\":48024,\"start\":48021},{\"end\":48031,\"start\":48028},{\"end\":48040,\"start\":48035},{\"end\":48293,\"start\":48290},{\"end\":48300,\"start\":48297},{\"end\":48307,\"start\":48304},{\"end\":48445,\"start\":48443},{\"end\":48454,\"start\":48449},{\"end\":48461,\"start\":48458},{\"end\":48468,\"start\":48465},{\"end\":48678,\"start\":48669},{\"end\":48688,\"start\":48682},{\"end\":49011,\"start\":49009},{\"end\":49019,\"start\":49015},{\"end\":49030,\"start\":49023},{\"end\":49037,\"start\":49034},{\"end\":49048,\"start\":49041},{\"end\":49345,\"start\":49343},{\"end\":49353,\"start\":49349},{\"end\":49360,\"start\":49357},{\"end\":49518,\"start\":49513},{\"end\":49525,\"start\":49522},{\"end\":49541,\"start\":49531},{\"end\":49691,\"start\":49686},{\"end\":49698,\"start\":49695},{\"end\":49705,\"start\":49702},{\"end\":49714,\"start\":49709},{\"end\":49728,\"start\":49718},{\"end\":49945,\"start\":49940},{\"end\":49956,\"start\":49949},{\"end\":50164,\"start\":50161},{\"end\":50171,\"start\":50168},{\"end\":50178,\"start\":50175},{\"end\":50185,\"start\":50182},{\"end\":50193,\"start\":50189},{\"end\":50201,\"start\":50197},{\"end\":50207,\"start\":50205},{\"end\":50214,\"start\":50211},{\"end\":50222,\"start\":50218},{\"end\":50229,\"start\":50226},{\"end\":50237,\"start\":50233},{\"end\":50243,\"start\":50241},{\"end\":50531,\"start\":50521},{\"end\":50544,\"start\":50535},{\"end\":50556,\"start\":50550},{\"end\":50793,\"start\":50790},{\"end\":50800,\"start\":50797},{\"end\":50807,\"start\":50804},{\"end\":50815,\"start\":50811},{\"end\":51173,\"start\":51168},{\"end\":51179,\"start\":51177},{\"end\":51190,\"start\":51183},{\"end\":51484,\"start\":51481},{\"end\":51493,\"start\":51488},{\"end\":51505,\"start\":51497},{\"end\":51513,\"start\":51509},{\"end\":51523,\"start\":51517},{\"end\":51534,\"start\":51527},{\"end\":51544,\"start\":51538},{\"end\":51557,\"start\":51550},{\"end\":51866,\"start\":51863},{\"end\":51878,\"start\":51870},{\"end\":51887,\"start\":51882},{\"end\":51898,\"start\":51891},{\"end\":51906,\"start\":51902},{\"end\":51915,\"start\":51913},{\"end\":51925,\"start\":51921},{\"end\":52149,\"start\":52141},{\"end\":52230,\"start\":52226},{\"end\":52243,\"start\":52234},{\"end\":52254,\"start\":52247},{\"end\":52580,\"start\":52575},{\"end\":52591,\"start\":52584},{\"end\":52602,\"start\":52595},{\"end\":52613,\"start\":52606},{\"end\":52622,\"start\":52617},{\"end\":52883,\"start\":52876},{\"end\":52895,\"start\":52887},{\"end\":52909,\"start\":52899},{\"end\":52915,\"start\":52913},{\"end\":52925,\"start\":52919},{\"end\":52931,\"start\":52929},{\"end\":52943,\"start\":52935},{\"end\":52961,\"start\":52947},{\"end\":53403,\"start\":53400},{\"end\":53410,\"start\":53407},{\"end\":53799,\"start\":53793},{\"end\":53808,\"start\":53803},{\"end\":53820,\"start\":53812},{\"end\":53830,\"start\":53824},{\"end\":53838,\"start\":53834},{\"end\":53849,\"start\":53842},{\"end\":53856,\"start\":53853},{\"end\":53869,\"start\":53860},{\"end\":53879,\"start\":53873},{\"end\":53888,\"start\":53883},{\"end\":54136,\"start\":54133},{\"end\":54142,\"start\":54140},{\"end\":54154,\"start\":54146},{\"end\":54161,\"start\":54158},{\"end\":54369,\"start\":54358},{\"end\":54377,\"start\":54373},{\"end\":54383,\"start\":54381},{\"end\":54393,\"start\":54387},{\"end\":54405,\"start\":54397},{\"end\":54411,\"start\":54409},{\"end\":54420,\"start\":54415},{\"end\":54432,\"start\":54424},{\"end\":54442,\"start\":54436},{\"end\":54455,\"start\":54446},{\"end\":54465,\"start\":54461},{\"end\":54476,\"start\":54469},{\"end\":54859,\"start\":54851},{\"end\":54872,\"start\":54863},{\"end\":55212,\"start\":55207},{\"end\":55221,\"start\":55218},{\"end\":55598,\"start\":55588},{\"end\":55608,\"start\":55602},{\"end\":55622,\"start\":55612},{\"end\":55635,\"start\":55626},{\"end\":55652,\"start\":55639},{\"end\":55950,\"start\":55943},{\"end\":55963,\"start\":55956},{\"end\":56315,\"start\":56308},{\"end\":56324,\"start\":56319},{\"end\":56337,\"start\":56328},{\"end\":56543,\"start\":56536},{\"end\":56550,\"start\":56547},{\"end\":56557,\"start\":56554},{\"end\":56569,\"start\":56561},{\"end\":56577,\"start\":56573},{\"end\":56589,\"start\":56581},{\"end\":56598,\"start\":56593},{\"end\":56611,\"start\":56602},{\"end\":56625,\"start\":56615},{\"end\":56855,\"start\":56848},{\"end\":56868,\"start\":56859},{\"end\":56877,\"start\":56872},{\"end\":56887,\"start\":56881},{\"end\":56896,\"start\":56891},{\"end\":57274,\"start\":57267},{\"end\":57285,\"start\":57278},{\"end\":57298,\"start\":57289},{\"end\":57307,\"start\":57302},{\"end\":57316,\"start\":57311},{\"end\":57330,\"start\":57320},{\"end\":57340,\"start\":57334},{\"end\":57641,\"start\":57633},{\"end\":57651,\"start\":57645},{\"end\":57661,\"start\":57655},{\"end\":58005,\"start\":58000},{\"end\":58013,\"start\":58009},{\"end\":58024,\"start\":58017},{\"end\":58037,\"start\":58028},{\"end\":58051,\"start\":58041},{\"end\":58061,\"start\":58055},{\"end\":58432,\"start\":58425},{\"end\":58442,\"start\":58436},{\"end\":58452,\"start\":58446},{\"end\":58461,\"start\":58456},{\"end\":58855,\"start\":58852},{\"end\":58866,\"start\":58859},{\"end\":58878,\"start\":58870},{\"end\":58888,\"start\":58882},{\"end\":58900,\"start\":58892},{\"end\":59218,\"start\":59215},{\"end\":59230,\"start\":59222},{\"end\":59240,\"start\":59234},{\"end\":59246,\"start\":59244},{\"end\":59252,\"start\":59250},{\"end\":59449,\"start\":59443},{\"end\":59460,\"start\":59453},{\"end\":59469,\"start\":59464},{\"end\":59477,\"start\":59473},{\"end\":59654,\"start\":59652},{\"end\":59662,\"start\":59658},{\"end\":59671,\"start\":59666},{\"end\":59679,\"start\":59675},{\"end\":59693,\"start\":59683},{\"end\":59701,\"start\":59697},{\"end\":59707,\"start\":59703},{\"end\":60033,\"start\":60028},{\"end\":60042,\"start\":60037},{\"end\":60055,\"start\":60048},{\"end\":60068,\"start\":60059},{\"end\":60364,\"start\":60359},{\"end\":60371,\"start\":60368},{\"end\":60379,\"start\":60375},{\"end\":60387,\"start\":60383},{\"end\":60398,\"start\":60393},{\"end\":60855,\"start\":60850},{\"end\":60862,\"start\":60859},{\"end\":60870,\"start\":60866},{\"end\":60878,\"start\":60874},{\"end\":60887,\"start\":60882},{\"end\":61223,\"start\":61218},{\"end\":61232,\"start\":61227},{\"end\":61240,\"start\":61236},{\"end\":61246,\"start\":61244},{\"end\":61254,\"start\":61250},{\"end\":61505,\"start\":61501},{\"end\":61515,\"start\":61509},{\"end\":61528,\"start\":61519},{\"end\":61537,\"start\":61532},{\"end\":61549,\"start\":61541},{\"end\":62090,\"start\":62085}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":3429309},\"end\":45484,\"start\":44976},{\"attributes\":{\"doi\":\"arXiv:1708.04552\",\"id\":\"b1\"},\"end\":45746,\"start\":45486},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":49882717},\"end\":46169,\"start\":45748},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":2229234},\"end\":46603,\"start\":46171},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":4246903},\"end\":46957,\"start\":46605},{\"attributes\":{\"doi\":\"arXiv:1811.12231\",\"id\":\"b5\"},\"end\":47394,\"start\":46959},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":53111490},\"end\":47719,\"start\":47396},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":6706414},\"end\":48017,\"start\":47721},{\"attributes\":{\"doi\":\"arXiv:1809.02499\",\"id\":\"b8\"},\"end\":48252,\"start\":48019},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":5398883},\"end\":48393,\"start\":48254},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":206594692},\"end\":48573,\"start\":48395},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":13046179},\"end\":48930,\"start\":48575},{\"attributes\":{\"id\":\"b12\"},\"end\":49306,\"start\":48932},{\"attributes\":{\"doi\":\"arXiv:1709.01507\",\"id\":\"b13\"},\"end\":49467,\"start\":49308},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":9433631},\"end\":49645,\"start\":49469},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":6773885},\"end\":49842,\"start\":49647},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":5808102},\"end\":50097,\"start\":49844},{\"attributes\":{\"doi\":\"abs/1810.09957\",\"id\":\"b17\"},\"end\":50452,\"start\":50099},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":195908774},\"end\":50692,\"start\":50454},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":49667948},\"end\":51079,\"start\":50694},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":3526391},\"end\":51431,\"start\":51081},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":14113767},\"end\":51823,\"start\":51433},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":2141740},\"end\":52137,\"start\":51825},{\"attributes\":{\"id\":\"b23\"},\"end\":52166,\"start\":52139},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":1629541},\"end\":52508,\"start\":52168},{\"attributes\":{\"doi\":\"arXiv:1706.06083\",\"id\":\"b25\"},\"end\":52817,\"start\":52510},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":13751202},\"end\":53323,\"start\":52819},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":973101},\"end\":53751,\"start\":53325},{\"attributes\":{\"id\":\"b28\"},\"end\":54049,\"start\":53753},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":10328909},\"end\":54303,\"start\":54051},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":2930547},\"end\":54779,\"start\":54305},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":11797475},\"end\":55097,\"start\":54781},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":976598},\"end\":55517,\"start\":55099},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":6844431},\"end\":55897,\"start\":55519},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":44150117},\"end\":56200,\"start\":55899},{\"attributes\":{\"id\":\"b35\"},\"end\":56221,\"start\":56202},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":1023605},\"end\":56500,\"start\":56223},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":206592484},\"end\":56785,\"start\":56502},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":206593880},\"end\":57263,\"start\":56787},{\"attributes\":{\"doi\":\"arXiv:1312.6199\",\"id\":\"b39\"},\"end\":57580,\"start\":57265},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":4716502},\"end\":57996,\"start\":57582},{\"attributes\":{\"doi\":\"arXiv:1806.05236\",\"id\":\"b41\"},\"end\":58372,\"start\":57998},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":1169492},\"end\":58807,\"start\":58374},{\"attributes\":{\"doi\":\"- port CNS-TR-2011-001\",\"id\":\"b43\"},\"end\":59149,\"start\":58809},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":8485068},\"end\":59386,\"start\":59151},{\"attributes\":{\"doi\":\"arXiv:1802.02375\",\"id\":\"b45\"},\"end\":59648,\"start\":59388},{\"attributes\":{\"doi\":\"arXiv:1506.03365\",\"id\":\"b46\"},\"end\":60024,\"start\":59650},{\"attributes\":{\"doi\":\"arXiv:1710.09412\",\"id\":\"b47\"},\"end\":60277,\"start\":60026},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":4940580},\"end\":60781,\"start\":60279},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":50769451},\"end\":61214,\"start\":60783},{\"attributes\":{\"doi\":\"arXiv:1708.04896\",\"id\":\"b50\"},\"end\":61441,\"start\":61216},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":6789015},\"end\":61913,\"start\":61443},{\"attributes\":{\"id\":\"b52\"},\"end\":62302,\"start\":61915}]", "bib_title": "[{\"end\":45087,\"start\":44976},{\"end\":45818,\"start\":45748},{\"end\":46243,\"start\":46171},{\"end\":46653,\"start\":46605},{\"end\":47457,\"start\":47396},{\"end\":47767,\"start\":47721},{\"end\":48286,\"start\":48254},{\"end\":48439,\"start\":48395},{\"end\":48665,\"start\":48575},{\"end\":49005,\"start\":48932},{\"end\":49509,\"start\":49469},{\"end\":49682,\"start\":49647},{\"end\":49936,\"start\":49844},{\"end\":50517,\"start\":50454},{\"end\":50786,\"start\":50694},{\"end\":51164,\"start\":51081},{\"end\":51474,\"start\":51433},{\"end\":51859,\"start\":51825},{\"end\":52222,\"start\":52168},{\"end\":52872,\"start\":52819},{\"end\":53396,\"start\":53325},{\"end\":54129,\"start\":54051},{\"end\":54354,\"start\":54305},{\"end\":54847,\"start\":54781},{\"end\":55201,\"start\":55099},{\"end\":55584,\"start\":55519},{\"end\":55939,\"start\":55899},{\"end\":56304,\"start\":56223},{\"end\":56532,\"start\":56502},{\"end\":56844,\"start\":56787},{\"end\":57629,\"start\":57582},{\"end\":58421,\"start\":58374},{\"end\":59211,\"start\":59151},{\"end\":60355,\"start\":60279},{\"end\":60846,\"start\":60783},{\"end\":61497,\"start\":61443}]", "bib_author": "[{\"end\":45100,\"start\":45089},{\"end\":45114,\"start\":45100},{\"end\":45126,\"start\":45114},{\"end\":45136,\"start\":45126},{\"end\":45148,\"start\":45136},{\"end\":45497,\"start\":45486},{\"end\":45509,\"start\":45497},{\"end\":45831,\"start\":45820},{\"end\":45841,\"start\":45831},{\"end\":45851,\"start\":45841},{\"end\":46256,\"start\":46245},{\"end\":46265,\"start\":46256},{\"end\":46275,\"start\":46265},{\"end\":46669,\"start\":46655},{\"end\":46681,\"start\":46669},{\"end\":46697,\"start\":46681},{\"end\":46705,\"start\":46697},{\"end\":46718,\"start\":46705},{\"end\":47076,\"start\":47065},{\"end\":47087,\"start\":47076},{\"end\":47100,\"start\":47087},{\"end\":47110,\"start\":47100},{\"end\":47124,\"start\":47110},{\"end\":47135,\"start\":47124},{\"end\":47469,\"start\":47459},{\"end\":47480,\"start\":47469},{\"end\":47488,\"start\":47480},{\"end\":47785,\"start\":47769},{\"end\":47795,\"start\":47785},{\"end\":47806,\"start\":47795},{\"end\":48026,\"start\":48019},{\"end\":48033,\"start\":48026},{\"end\":48042,\"start\":48033},{\"end\":48295,\"start\":48288},{\"end\":48302,\"start\":48295},{\"end\":48309,\"start\":48302},{\"end\":48447,\"start\":48441},{\"end\":48456,\"start\":48447},{\"end\":48463,\"start\":48456},{\"end\":48470,\"start\":48463},{\"end\":48680,\"start\":48667},{\"end\":48690,\"start\":48680},{\"end\":49013,\"start\":49007},{\"end\":49021,\"start\":49013},{\"end\":49032,\"start\":49021},{\"end\":49039,\"start\":49032},{\"end\":49050,\"start\":49039},{\"end\":49347,\"start\":49341},{\"end\":49355,\"start\":49347},{\"end\":49362,\"start\":49355},{\"end\":49520,\"start\":49511},{\"end\":49527,\"start\":49520},{\"end\":49543,\"start\":49527},{\"end\":49693,\"start\":49684},{\"end\":49700,\"start\":49693},{\"end\":49707,\"start\":49700},{\"end\":49716,\"start\":49707},{\"end\":49730,\"start\":49716},{\"end\":49947,\"start\":49938},{\"end\":49958,\"start\":49947},{\"end\":50166,\"start\":50159},{\"end\":50173,\"start\":50166},{\"end\":50180,\"start\":50173},{\"end\":50187,\"start\":50180},{\"end\":50195,\"start\":50187},{\"end\":50203,\"start\":50195},{\"end\":50209,\"start\":50203},{\"end\":50216,\"start\":50209},{\"end\":50224,\"start\":50216},{\"end\":50231,\"start\":50224},{\"end\":50239,\"start\":50231},{\"end\":50245,\"start\":50239},{\"end\":50533,\"start\":50519},{\"end\":50546,\"start\":50533},{\"end\":50558,\"start\":50546},{\"end\":50795,\"start\":50788},{\"end\":50802,\"start\":50795},{\"end\":50809,\"start\":50802},{\"end\":50817,\"start\":50809},{\"end\":51175,\"start\":51166},{\"end\":51181,\"start\":51175},{\"end\":51192,\"start\":51181},{\"end\":51486,\"start\":51476},{\"end\":51495,\"start\":51486},{\"end\":51507,\"start\":51495},{\"end\":51515,\"start\":51507},{\"end\":51525,\"start\":51515},{\"end\":51536,\"start\":51525},{\"end\":51546,\"start\":51536},{\"end\":51559,\"start\":51546},{\"end\":51868,\"start\":51861},{\"end\":51880,\"start\":51868},{\"end\":51889,\"start\":51880},{\"end\":51900,\"start\":51889},{\"end\":51908,\"start\":51900},{\"end\":51917,\"start\":51908},{\"end\":51927,\"start\":51917},{\"end\":52151,\"start\":52141},{\"end\":52232,\"start\":52224},{\"end\":52245,\"start\":52232},{\"end\":52256,\"start\":52245},{\"end\":52582,\"start\":52573},{\"end\":52593,\"start\":52582},{\"end\":52604,\"start\":52593},{\"end\":52615,\"start\":52604},{\"end\":52624,\"start\":52615},{\"end\":52885,\"start\":52874},{\"end\":52897,\"start\":52885},{\"end\":52911,\"start\":52897},{\"end\":52917,\"start\":52911},{\"end\":52927,\"start\":52917},{\"end\":52933,\"start\":52927},{\"end\":52945,\"start\":52933},{\"end\":52963,\"start\":52945},{\"end\":53405,\"start\":53398},{\"end\":53412,\"start\":53405},{\"end\":53801,\"start\":53791},{\"end\":53810,\"start\":53801},{\"end\":53822,\"start\":53810},{\"end\":53832,\"start\":53822},{\"end\":53840,\"start\":53832},{\"end\":53851,\"start\":53840},{\"end\":53858,\"start\":53851},{\"end\":53871,\"start\":53858},{\"end\":53881,\"start\":53871},{\"end\":53890,\"start\":53881},{\"end\":54138,\"start\":54131},{\"end\":54144,\"start\":54138},{\"end\":54156,\"start\":54144},{\"end\":54163,\"start\":54156},{\"end\":54371,\"start\":54356},{\"end\":54379,\"start\":54371},{\"end\":54385,\"start\":54379},{\"end\":54395,\"start\":54385},{\"end\":54407,\"start\":54395},{\"end\":54413,\"start\":54407},{\"end\":54422,\"start\":54413},{\"end\":54434,\"start\":54422},{\"end\":54444,\"start\":54434},{\"end\":54457,\"start\":54444},{\"end\":54467,\"start\":54457},{\"end\":54478,\"start\":54467},{\"end\":54861,\"start\":54849},{\"end\":54874,\"start\":54861},{\"end\":55214,\"start\":55203},{\"end\":55223,\"start\":55214},{\"end\":55600,\"start\":55586},{\"end\":55610,\"start\":55600},{\"end\":55624,\"start\":55610},{\"end\":55637,\"start\":55624},{\"end\":55654,\"start\":55637},{\"end\":55952,\"start\":55941},{\"end\":55965,\"start\":55952},{\"end\":56317,\"start\":56306},{\"end\":56326,\"start\":56317},{\"end\":56339,\"start\":56326},{\"end\":56545,\"start\":56534},{\"end\":56552,\"start\":56545},{\"end\":56559,\"start\":56552},{\"end\":56571,\"start\":56559},{\"end\":56579,\"start\":56571},{\"end\":56591,\"start\":56579},{\"end\":56600,\"start\":56591},{\"end\":56613,\"start\":56600},{\"end\":56627,\"start\":56613},{\"end\":56857,\"start\":56846},{\"end\":56870,\"start\":56857},{\"end\":56879,\"start\":56870},{\"end\":56889,\"start\":56879},{\"end\":56898,\"start\":56889},{\"end\":57276,\"start\":57265},{\"end\":57287,\"start\":57276},{\"end\":57300,\"start\":57287},{\"end\":57309,\"start\":57300},{\"end\":57318,\"start\":57309},{\"end\":57332,\"start\":57318},{\"end\":57342,\"start\":57332},{\"end\":57643,\"start\":57631},{\"end\":57653,\"start\":57643},{\"end\":57663,\"start\":57653},{\"end\":58007,\"start\":57998},{\"end\":58015,\"start\":58007},{\"end\":58026,\"start\":58015},{\"end\":58039,\"start\":58026},{\"end\":58053,\"start\":58039},{\"end\":58063,\"start\":58053},{\"end\":58434,\"start\":58423},{\"end\":58444,\"start\":58434},{\"end\":58454,\"start\":58444},{\"end\":58463,\"start\":58454},{\"end\":58857,\"start\":58850},{\"end\":58868,\"start\":58857},{\"end\":58880,\"start\":58868},{\"end\":58890,\"start\":58880},{\"end\":58902,\"start\":58890},{\"end\":59220,\"start\":59213},{\"end\":59232,\"start\":59220},{\"end\":59242,\"start\":59232},{\"end\":59248,\"start\":59242},{\"end\":59254,\"start\":59248},{\"end\":59451,\"start\":59441},{\"end\":59462,\"start\":59451},{\"end\":59471,\"start\":59462},{\"end\":59479,\"start\":59471},{\"end\":59656,\"start\":59650},{\"end\":59664,\"start\":59656},{\"end\":59673,\"start\":59664},{\"end\":59681,\"start\":59673},{\"end\":59695,\"start\":59681},{\"end\":59703,\"start\":59695},{\"end\":59709,\"start\":59703},{\"end\":60035,\"start\":60026},{\"end\":60044,\"start\":60035},{\"end\":60057,\"start\":60044},{\"end\":60070,\"start\":60057},{\"end\":60366,\"start\":60357},{\"end\":60373,\"start\":60366},{\"end\":60381,\"start\":60373},{\"end\":60389,\"start\":60381},{\"end\":60400,\"start\":60389},{\"end\":60857,\"start\":60848},{\"end\":60864,\"start\":60857},{\"end\":60872,\"start\":60864},{\"end\":60880,\"start\":60872},{\"end\":60889,\"start\":60880},{\"end\":61225,\"start\":61216},{\"end\":61234,\"start\":61225},{\"end\":61242,\"start\":61234},{\"end\":61248,\"start\":61242},{\"end\":61256,\"start\":61248},{\"end\":61507,\"start\":61499},{\"end\":61517,\"start\":61507},{\"end\":61530,\"start\":61517},{\"end\":61539,\"start\":61530},{\"end\":61551,\"start\":61539},{\"end\":62092,\"start\":62082}]", "bib_venue": "[{\"end\":45966,\"start\":45917},{\"end\":46396,\"start\":46344},{\"end\":53078,\"start\":53029},{\"end\":53553,\"start\":53491},{\"end\":57039,\"start\":56977},{\"end\":57804,\"start\":57742},{\"end\":58604,\"start\":58542},{\"end\":60541,\"start\":60479},{\"end\":61004,\"start\":60955},{\"end\":61692,\"start\":61630},{\"end\":45210,\"start\":45148},{\"end\":45593,\"start\":45525},{\"end\":45915,\"start\":45851},{\"end\":46342,\"start\":46275},{\"end\":46758,\"start\":46718},{\"end\":47063,\"start\":46959},{\"end\":47537,\"start\":47488},{\"end\":47858,\"start\":47806},{\"end\":48112,\"start\":48058},{\"end\":48313,\"start\":48309},{\"end\":48474,\"start\":48470},{\"end\":48742,\"start\":48690},{\"end\":49099,\"start\":49050},{\"end\":49339,\"start\":49308},{\"end\":49547,\"start\":49543},{\"end\":49734,\"start\":49730},{\"end\":49962,\"start\":49958},{\"end\":50157,\"start\":50099},{\"end\":50562,\"start\":50558},{\"end\":50866,\"start\":50817},{\"end\":51247,\"start\":51192},{\"end\":51597,\"start\":51559},{\"end\":51965,\"start\":51927},{\"end\":52325,\"start\":52256},{\"end\":52571,\"start\":52510},{\"end\":53027,\"start\":52963},{\"end\":53489,\"start\":53412},{\"end\":53789,\"start\":53753},{\"end\":54167,\"start\":54163},{\"end\":54518,\"start\":54478},{\"end\":54923,\"start\":54874},{\"end\":55283,\"start\":55223},{\"end\":55690,\"start\":55654},{\"end\":56034,\"start\":55965},{\"end\":56208,\"start\":56204},{\"end\":56352,\"start\":56339},{\"end\":56631,\"start\":56627},{\"end\":56975,\"start\":56898},{\"end\":57397,\"start\":57357},{\"end\":57740,\"start\":57663},{\"end\":58160,\"start\":58079},{\"end\":58540,\"start\":58463},{\"end\":58848,\"start\":58809},{\"end\":59258,\"start\":59254},{\"end\":59439,\"start\":59388},{\"end\":59812,\"start\":59725},{\"end\":60127,\"start\":60086},{\"end\":60477,\"start\":60400},{\"end\":60953,\"start\":60889},{\"end\":61304,\"start\":61272},{\"end\":61628,\"start\":61551},{\"end\":62080,\"start\":61915}]"}}}, "year": 2023, "month": 12, "day": 17}