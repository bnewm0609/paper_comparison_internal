{"id": 25648541, "updated": "2023-10-02 01:38:27.089", "metadata": {"title": "Learning Tree-based Deep Model for Recommender Systems", "authors": "[{\"first\":\"Han\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Xiang\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Pengye\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Guozheng\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Jie\",\"last\":\"He\",\"middle\":[]},{\"first\":\"Han\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Kun\",\"last\":\"Gai\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining", "publication_date": {"year": 2018, "month": 1, "day": 8}, "abstract": "Model-based methods for recommender systems have been studied extensively in recent years. In systems with large corpus, however, the calculation cost for the learnt model to predict all user-item preferences is tremendous, which makes full corpus retrieval extremely difficult. To overcome the calculation barriers, models such as matrix factorization resort to inner product form (i.e., model user-item preference as the inner product of user, item latent factors) and indexes to facilitate efficient approximate k-nearest neighbor searches. However, it still remains challenging to incorporate more expressive interaction forms between user and item features, e.g., interactions through deep neural networks, because of the calculation cost. In this paper, we focus on the problem of introducing arbitrary advanced models to recommender systems with large corpus. We propose a novel tree-based method which can provide logarithmic complexity w.r.t. corpus size even with more expressive models such as deep neural networks. Our main idea is to predict user interests from coarse to fine by traversing tree nodes in a top-down fashion and making decisions for each user-node pair. We also show that the tree structure can be jointly learnt towards better compatibility with users' interest distribution and hence facilitate both training and prediction. Experimental evaluations with two large-scale real-world datasets show that the proposed method significantly outperforms traditional methods. Online A/B test results in Taobao display advertising platform also demonstrate the effectiveness of the proposed method in production environments.", "fields_of_study": "[\"Mathematics\",\"Computer Science\"]", "external_ids": {"arxiv": "1801.02294", "mag": "3106181667", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/kdd/ZhuLZLHLG18", "doi": "10.1145/3219819.3219826"}}, "content": {"source": {"pdf_hash": "39667784085b83191b58c361d60d518a9e9f373d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1801.02294v5.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1801.02294", "status": "GREEN"}}, "grobid": {"id": "401b4eb1921e782884b807a9cc0b993fa3b95fc9", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/39667784085b83191b58c361d60d518a9e9f373d.txt", "contents": "\nLearning Tree-based Deep Model for Recommender Systems\n21 Dec 2018\n\nHan Zhu \nAlibaba Group\n\n\nXiang Li \nAlibaba Group\n\n\nPengye Zhang pengye.zpy@alibaba-inc.com \nAlibaba Group\n\n\nGuozheng Li guozheng.lgz@alibaba-inc.com \nAlibaba Group\n\n\nJie He \nAlibaba Group\n\n\nHan Li \nAlibaba Group\n\n\nKun Gai \nAlibaba Group\n\n\nLearning Tree-based Deep Model for Recommender Systems\n21 Dec 2018CCS CONCEPTS \u2022 Computing methodologies \u2192 Classification and regression treesNeural networks\u2022 Information systems \u2192 Recom- mender systemsKEYWORDS Tree-based Learning, Recommender Systems, Implicit Feedback\nModel-based methods for recommender systems have been studied extensively in recent years. In systems with large corpus, however, the calculation cost for the learnt model to predict all useritem preferences is tremendous, which makes full corpus retrieval extremely difficult. To overcome the calculation barriers, models such as matrix factorization resort to inner product form (i.e., model user-item preference as the inner product of user, item latent factors) and indexes to facilitate efficient approximate k-nearest neighbor searches. However, it still remains challenging to incorporate more expressive interaction forms between user and item features, e.g., interactions through deep neural networks, because of the calculation cost.In this paper, we focus on the problem of introducing arbitrary advanced models to recommender systems with large corpus. We propose a novel tree-based method which can provide logarithmic complexity w.r.t. corpus size even with more expressive models such as deep neural networks. Our main idea is to predict user interests from coarse to fine by traversing tree nodes in a top-down fashion and making decisions for each user-node pair. We also show that the tree structure can be jointly learnt towards better compatibility with users' interest distribution and hence facilitate both training and prediction. Experimental evaluations with two large-scale real-world datasets show that the proposed method significantly outperforms traditional methods. Online A/B test results in Taobao display advertising platform also demonstrate the effectiveness of the proposed method in production environments.\n\nINTRODUCTION\n\nRecommendation has been widely used by various kinds of content providers. Personalized recommendation method, based on the intuition that users' interests can be inferred from their historical behaviors or other users with similar preference, has been proven to be effective in YouTube [7] and Amazon [22].\n\nDesigning such a recommendation model to predict the best candidate set from the entire corpus for each user has many challenges. In systems with enormous corpus, some well-performed recommendation algorithms may fail to predict from the entire corpus. The linear prediction complexity w.r.t. the corpus size is unacceptable. Deploying such large-scale recommender system requires the amount of calculation to predict for each single user be limited. And besides preciseness, the novelty of recommended items should also be responsible for user experience. Results that only contain homogeneous items with user's historical behaviors are not expected.\n\nTo reduce the amount of calculation and handle enormous corpus, memory-based collaborative filtering methods are widely deployed in industry [22]. As a representative method in collaborative filtering family, item-based collaborative filtering [31] can recommend from very large corpus with relatively much fewer computations, depending on the pre-calculated similarity between item pairs and using user's historical behaviors as triggers to recall those most similar items. However, there exists restriction on the scope of candidate set, i.e., not all items but only items similar to the triggers can be ultimately recommended. This intuition prevents the recommender system from jumping out of historical behavior to explore potential user interests, which limits the accuracy of recalled results. And in practice the recommendation novelty is also criticized. Another way to reduce calculation is making coarsegrained recommendation. For example, the system recommends a small number of item categories for users and picks out all corresponding items, with a following ranking stage. However, for large corpus, the calculation problem is still not solved. If the category number is large, the category recommendation itself also meets the calculation barrier. If not, some categories will inevitably include too many items, making the following ranking calculation impracticable. Besides, the used categories are usually not designed for recommendation problem, which can seriously harm the recommendation accuracy.\n\nIn the literatures of recommender systems, model-based methods are an active topic. Models such as matrix factorization (MF) [19,30] try to decompose pairwise user-item preferences (e.g., ratings) into user and item factors, then recommend to each user its most preferred items. Factorization machine (FM) [28] further proposes a unified model that can mimic different factorization models with any kind of input data. In some real-world scenarios that have no explicit preference but only implicit user feedback (e.g., user behaviors like clicks or purchases), Bayesian personalized ranking [29] gives a solution that formulates the preference in triplets with partial order, and applies it to MF models. In industry, YouTube uses deep neural network [7] to learn both user and item's embeddings, where two kinds of embeddings are generated from their corresponding features separately. In all the above kinds of methods, the preference of user-item pair can be formulated as the inner product of user and item's vector representations. The prediction stage thus is equivalent to retrieve user vector's nearest neighbors in inner product space. For vector search problem, indices like hashing or quantization [18] for approximate k-nearest neighbor (kNN) search can ensure the efficiency of retrieval.\n\nHowever, the inner product interaction form between user and item's vector representations severely limits model's capability. There exist many other kinds of more expressive interaction forms, for example, cross-product features between user's historical behaviors and candidate items are widely used in click-through rate prediction [5]. Recent work [13] proposes a neural collaborative filtering method, where a neural network instead of inner product is used to model the interaction between user and item's vector representations. The work's experimental results prove that a multi-layer feed-forward neural network performs better than the fixed inner product manner. Deep interest network [34] points out that user interests are diverse, and an attention like network structure can generate varying user vectors according to different candidate items. Beyond the above works, other methods like product neural network [27] have also proven the effectiveness of advanced neural networks. However, as these kinds of models can not be regulated to inner product form between user and item vectors to utilize efficient approximate kNN search, they can not be used to recall candidates in large-scale recommender systems. How to overcome the calculation barrier to make arbitrary advanced neural networks feasible in large-scale recommendation is a problem.\n\nTo address the challenges above, we propose a novel tree-based deep recommendation model (TDM) in this paper. Tree and treebased methods are researched in multiclass classification problem [1-3, 6, 15, 26, 32], where tree is usually used to partition the sample or label space to reduce calculation cost. However, researchers seldom set foot in the context of recommender systems using tree structure as an index for retrieval. Actually, hierarchical structure of information ubiquitously exists in many domains. For example, in E-commerce scenario, iPhone is the fine-grained item while smartphone is the coarse-grained concept to which iPhone belongs. The proposed TDM method leverages this hierarchy of information and turns recommendation problem into a series of hierarchical classification problems. By solving the problem from easy to difficult, TDM can improve both accuracy and efficiency. The main contributions of our paper are summarized as follows:\n\n\u2022 To our best knowledge, TDM is the first method that makes arbitrary advanced models possible in generating recommendations from large corpus. Benefiting from hierarchical tree search, TDM achieves logarithmic amount of calculation w.r.t. corpus size when making prediction. \u2022 TDM can help find novel but effective recommendation results more precisely, because the entire corpus is explored and more effective deep models also can help find potential interests. \u2022 Besides more advanced models, TDM also promotes recommendation accuracy by hierarchical search, which divides a large problem into smaller ones and solves them successively from easy to difficult. \u2022 As a kind of index, the tree structure can also be learnt towards optimal hierarchy of items and concepts for more effective retrieval, which in turn facilitates the model training.\n\nWe employ a tree learning method that allows joint training of neural network and the tree structure. \u2022 We conduct extensive experiments on two large-scale realworld datasets, which show that TDM outperforms existing methods significantly.\n\nIt's worth mentioning that tree-based approach is also researched in language model work hierarchical softmax [24], but it's different from the proposed TDM not only in motivation but also in formulation. In next-word prediction problem, conventional softmax has to calculate the normalization term to get any single word's probability, which is very time-consuming. Hierarchical softmax uses tree structure, and next-word's probability is converted to the product of node probabilities along the tree path. Such formulation reduces the computation complexity of next-word's probability to logarithmic magnitude w.r.t. the corpus size. However, in recommendation problem, the goal is to search the entire corpus for those most preferred items, which is a retrieval problem. In hierarchical softmax tree, the optimum of parent nodes can not guarantee that the optimal low level nodes are in their descendants, and all items still need to be traversed to find the optimal one. Thus, it's not suitable for such a retrieval problem. To address the retrieval problem, we propose a max-heap like tree formulation and introduce deep neural networks to model the tree, which forms an efficient method for large-scale recommendation. The following sections will show its difference in formulation and its superiority in performance. In addition, hierarchical softmax adopts a single hidden layer network for a specific natural language processing problem, while the proposed TDM method is practicable to engage any neural network structures.\n\nThe proposed tree-based model is a universal solution for all kinds of online content providers. The remainder of this paper is organized as follows: In Section 2, we'll introduce the system architecture of Taobao display advertising to show the position of the proposed method. Section 3 will give a detailed introduction and formalization of the proposed tree-based deep model. And the following Section 4 will describe how the tree-based model serves online. Experimental results on large-scale benchmark dataset and Taobao advertising dataset are shown in Section 5. At last, Section 6 gives our work a conclusion.\n\n\nSYSTEM ARCHITECTURE\n\nIn this section, we introduce the architecture of Taobao display advertising recommender system as Figure 1. After receiving page view request from a user, the system uses user features, context features and item features as input to generate a relatively much smaller set (usually hundreds) of candidate items from the entire corpus (hundreds of millions) in the matching server. The treebased recommendation model takes effort in this stage and shrinks the size of candidate set by several orders of magnitude.\n\nWith hundreds of candidate items, the real-time prediction server uses more expressive but also more time consuming models [11,34] to predict indicators like click-through rate or conversion rate. And after ranking by strategy [17,35], several items are ultimately impressed to user.\n\nAs aforementioned, the proposed recommendation model aims to construct a candidate set with hundreds of items. This stage is essential and also difficult. Whether the user is interested in the generated candidates gives an upper bound of the impression quality. How to draw candidates from the entire corpus weighing efficiency and effectiveness is a problem.  Figure 1: The system architecture of Taobao display advertising recommender system.\n\n\nTREE-BASED DEEP MODEL\n\nIn this part, we first introduce the tree structure used in our treebased model to give an overall conception. Secondly, we introduce hierarchical softmax [24] to show why its formulation is not suitable for recommendation. After that, we give a novel max-heap like tree formulation and show how to train the tree-based model. Then, the deep neural network architecture is introduced. At last, we show how to construct and learn the tree used in the tree-based model.\n\n\nTree for Recommendation\n\nA recommendation tree consists of a set of nodes N , where N = {n 1 , n 2 , \u00b7 \u00b7 \u00b7 n |N | } represents |N | individual non-leaf or leaf nodes. Each node in N except the root node has one parent and an arbitrary number of children. Specifically, each item c i in the corpus C corresponds to one and only one leaf node in the tree, and those non-leaf nodes are coarse-grained concepts. Without loss of generality, we suppose that node n 1 is always the root node. An example tree is illustrated in the right bottom corner of Figure 2, in which each circle represents a node and the number of node is its index in tree. The tree has 8 leaf nodes in total, each of which corresponds to an item in the corpus. It's worth mentioning that though the given example is a complete binary tree, we don't impose complete and binary as restrictions on the type of the tree in our model.\n\n\nRelated Work\n\nWith the tree structure, we firstly introduce the related work hierarchical softmax to help understand its difference with our TDM. In hierarchical softmax, each leaf node n in tree has its unique encoding from the root to the node. For example, if we encode 1 as choosing the left branch and 0 as choosing the right branch, n 9 's encoding in tree in Figure 2 is 110 and n 15 's encoding is 000. Denote b j (n) as the encoding of node n in level j. In hierarchical softmax's formulation, the next-word's probability given the context is derived as\nP n|context = w j=1 P b = b j (n) l j (n), context ,(1)\nwhere w is the length of leaf node n's encoding, and l j (n) is n's ancestor node in level j.\n\nIn such a way, hierarchical softmax solves the probability calculation problem by avoiding the normalization term (each word in the corpus needs to be traversed) in conventional softmax. However, to find the most possible leaf, the model still has to traverse the entire corpus. Traversing each level's most possible node topdown along the tree path can not guarantee to successfully retrieve the optimal leaf. Therefore, hierarchical softmax's formulation is not suitable for large-scale retrieval problem. In addition, according to Equation 1, each non-leaf node in tree is trained as a binary classifier to discriminate between its two children nodes. But if two nodes are neighbors in the tree, they are probably to be similar. In recommendation scenario, it's likely that user is interested in both two children. Hierarchical softmax's model focuses on distinguishing optimal and suboptimal choices, which may lose the capability of discriminating from a global view. If greedy beam search is used to retrieve those most possible leaf nodes, once bad decisions are made in upper levels of the tree, the model may fail to find relatively better results among those low quality candidates in lower levels. YouTube's work [7] also reports that they have tried hierarchical softmax to learn user and item embeddings, while it performs worse than sampled-softmax [16] manner.\n\nGiven that hierarchical softmax's formulation is not suitable for large-scale recommendation, we propose a new tree model formulation in the following section.\n\n\nTree-based Model Formulation\n\nTo address the problem of efficient top-k retrieval of most preferred items, we propose a max-heap like tree probability formulation. Max-heap like tree is a tree structure where every non-leaf node n in level j satisfies the following equation for each user u:\nP (j) (n|u) = max n c \u2208{n's children nodes in level j+1} P (j+1) (n c |u) \u03b1 (j) ,(2)\nwhere P (j) (n|u) is the ground truth probability that user u is interested in n. \u03b1 (j) is the layer-specific normalization term of level j to ensure that the probability sum in the level equals to 1. Equation 2 says that a parent node's ground truth preference equals to the maximum preference of its children nodes, divided by the normalization term. Note that we slightly abuse the notation and let u denote a specific user state. In other words, a specific user state u may transfer to another state u \u2032 once the user has a new behavior. The goal is to find k leaf nodes with largest preference probabilities. Suppose that we have each node n's ground truth P (j) (n|u) in the tree, we can retrieve k nodes with largest preference probabilities layer-wise, and only those children nodes of each level's top k need to be explored. In this way, top k leaf nodes can be ultimately retrieved. Actually, we don't need to know each tree node's exact ground truth probability in the above retrieval process. What we need is the order of the probabilities in each level to help find the top k nodes in the level. Based on this observation, we use user's implicit feedback data and neural network to train each level's discriminator that can tell the order of preference probabilities.\n\nSuppose that user u has an interaction with leaf node n d , i.e., n d is a positive sample node for u. It means an order P (m) (n d |u) > P (m) (n t |u), where m is the level of leaves and n t is any other leaf node. In any level j, denote l j (n d ) as n d 's ancestor in level j. According to the formulation of tree in Equation 2, we can derive that P (j) (l j (n d )|u) > P (j) (n q |u), where n q is any node in level j except l j (n d ). In basis of the above analysis, we can use negative sampling [23] to train each level's order discriminator. In detail, leaf node that have interaction with u, and its ancestor nodes constitute the set of positive samples in each level for u. And randomly selected nodes except positive ones in each level constitute the set of negative samples. Those green and red nodes in Figure 2 give examples for sampling. Suppose that given a user and its state, the target node is n 13 . Then, n 13 's ancestors are positive samples, and those randomly sampled red nodes in each level are negative samples. These samples are then fed into binary probability models to get levels' order discriminators. We use one global deep neural network binary model with different input for all levels' order discriminators. Arbitrary advanced neural network can be adopted to improve model capability.\n\nDenote Y + u and Y \u2212 u as the set of positive and negative samples for u. The likelihood function is then derived as:\nu n \u2208Y + u P \u02c6 u (n) = 1|n,u n \u2208Y \u2212 u P \u02c6 u (n) = 0|n,u ,(3)\nwhere\u02c6 u (n) is the predicted label of node n given u. P \u02c6 u (n)|n,u is the output of binary probability model, taking user state u and the sampled node n as input. The corresponding loss function is\n\u2212 u n \u2208Y + u \u222aY \u2212 u u (n) log P \u02c6 u (n) = 1|n,u + 1\u2212 u (n) log P \u02c6 u (n) = 0|n,u , (4) where u (n)\nis the ground truth label of node n given u. Details about how to train the model according to the loss function are in Section 3.4.\n\nNote that the proposed sampling method is quite different from the underlying one in hierarchical softmax. Compared to the method used in hierarchical softmax which leads the model to distinguish optimal and suboptimal results, we randomly select negative samples in the same level for each positive node. Such method makes each level's discriminator be an intra-level global one. Each level's global discriminator can make precise decisions independently, without depending on the goodness of upper levels' decisions. The global discriminating capability is very important for hierarchical recommendation approaches. It ensures that even if the model makes bad decision and low quality nodes leak into the candidate set in an upper-level, those relatively better nodes rather than very bad ones can be chosen by the model in the following levels.\n\nGiven a recommendation tree and an optimized model, the detailed hierarchical prediction algorithm is described in Algorithm 1. The retrieval process is layer-wise and top-down. Suppose that the desired candidate item number is k. For corpus C with size |C |, traversing at most 2 * k * log |C | nodes can get the final recommendation set in a complete binary tree. The number of nodes need to be traversed is in a logarithmic relation w.r.t. corpus size, which makes advanced binary probability models possible to be employed.\n\nOur proposed TDM method not only reduces the amount of calculation when making prediction, it also has potential to improve recommendation quality compared with brute-force search in all leaf nodes. Without the tree, training a model to find optimal items directly is a difficult problem because of the corpus size. Employing the tree hierarchy, a large-scale recommendation problem is divided into many smaller problems. There only exist a few nodes in high levels of the tree, thus the discrimination problem is easier. And decisions made by high levels refine the candidate set, which may help lower levels make better judgments. Experimental results in Section 5.4 will show that the proposed hierarchical retrieval approach performs better than direct brute-force search.\n\n\nThe Deep Model\n\nIn the following part, we introduce the deep model we use. The entire model is illustrated in Figure 2. Inspired by the click-through rate prediction work [34], we learn low dimensional embeddings for each node in the tree, and use attention module to softly searching for related behaviors for better user representation. To exploit user behavior that contains timestamp information, we design the block-wise input layer to distinguish behaviors that lie in different time windows. The historical behaviors can be divided into different time windows along the timeline, and item embeddings in each time window is weighted averaged. Attention module and the following network greatly strengthen the model capability, and also make user's preferences over candidate items can not be regulated to inner product form. The embeddings of tree nodes and the tree structure itself are also parts of the model. To minimize Loss 4, the sampled nodes and the corresponding features are used to train the network. Note that we only illustrate the usage of user behavior feature in Figure 2 for briefness, while other features like user profile or contextual feature can be used with no obstacles in practice.\n\n\nTree Construction and Learning\n\nThe recommendation tree is a fundamental part of the tree-based deep recommendation model. Unlike multiclass and multi-label classification works [26,32] where tree is used to partition samples or labels, our recommendation tree indexes items for retrieval. In hierarchical softmax [24], the word hierarchy is built according to  In each time window, item embeddings are weighted averaged, and the weights come from activation units. Each time window's output along with the candidate node's embedding are concatenated as the neural network input. After three fully connected layers with PReLU [33] activation and batch normalization [14], a binary softmax is used to yield the probability whether the user is interested in the candidate node. Each item and its corresponding leaf node share the same embedding. All embeddings are randomly initialized.\n\nexpert knowledge from WordNet [21]. In the scenario of recommendation, not every corpus can provide specific expert knowledge. An intuitive alternation is to construct the tree using hierarchical clustering methods in basis of item concurrence or similarity drawn from the dataset. But the clustered tree may be quite imbalanced, which is detrimental for training and retrieval. Given pairwise item similarity, algorithm in [2] gives a way to split items into subsets recursively by spectral clustering [25]. However, spectral clustering is not scalable enough (cubic time complexity w.r.t. corpus size) for large-scale corpus. In this section, we focus on reasonable and feasible tree construction and learning approaches.\n\nTree initialization. Since we suppose the tree to represent user interests' hierarchical information, it's natural to build the tree in a way that similar items are organized in close positions. Given that category information is extensive available in many domains, we intuitively come up with a method leveraging item's category information to build the initial tree. Without loss of generality, we take binary tree as an example in this section. Firstly, we sort all categories randomly, and place items belonging to the same category together in an intra-category random order. If an item belongs to more than one category, the item is assigned to a random one for uniqueness. In such way, we can get a list of ranked items.\n\nSecondly, those ranked items are halved to two equal parts recursively until the current set contains only one item, which could construct a near-complete binary tree top-down. The above kind of category-based initialization can get better hierarchy and results in our experiments than a complete random tree.\n\nTree learning. As a part of the model, each leaf node's embedding can be learnt after model training. Then we use the learnt leaf nodes' embedding vectors to cluster a new tree. Considering the corpus size, we use k-means clustering algorithm for its good scalability. At each step, items are clustered into two subsets according to their embedding vectors. Note that the two subsets are adjusted to equal for a more balanced tree. The recursion stops when only one item is left, and a binary tree could be constructed in such a top-down way. In our experiments, it takes about an hour to construct such a cluster tree when the corpus size is about 4 millions, using a single machine. Experimental results in Section 5 will show the effectiveness of the given tree learning algorithm.\n\nThe deep model and tree structure are learnt jointly in an alternative way: 1) Construct an initial tree and train the model till converging; 2) Learn to get a new tree structure in basis of trained leaf nodes' embeddings; 3) Train the model again with the learnt new tree structure. Figure 3 illustrates the online serving system of the proposed method. Input feature assembling and item retrieval are split into two asynchronous stages. Each user behavior including click, purchase and adding item into shopping cart will strike the real-time feature server to assemble new input features. And once receiving page view request, the user targeting server will use the pre-assembled features to retrieve candidates from the tree. As described in Algorithm 1, the retrieval is layer-wise and the trained neural network is used to calculate the probability that whether a node is preferred given the input features.\n\n\nONLINE SERVING\n\n\nReal-time feature server\n\n\nFeatures\n\nNeural network and tree\n\n\nUser targeting server\n\nPage view request User behavior\n\n\nUser features\n\n\nUser-candidates matching\n\nCandidates Figure 3: The online serving system of the tree-based model, where user feature is assembled asynchronously.\n\n\nEXPERIMENTAL STUDY\n\nWe study the performance of the proposed tree-based model in this section. Experimental results in MovieLens-20M [12] and Taobao advertising dataset called UserBehavior are presented. In the experiments, we compare the proposed method to other existing methods to show the effectiveness of the model, and empirical study results show how the tree-based model and tree learning algorithm work.\n\n\nDatasets\n\nThe experiments are conducted in two large-scale real-world datasets with timestamps: 1) users' movie viewing data from MovieLens [12]; 2) a user-item behavior dataset from Taobao called UserBehavior. In more details:\n\nMovieLens-20M: It contains user-movie ratings with timestamps in this dataset. As we deal with implicit feedback problem, the ratings are binarized by keeping the ratings of four or higher, which is a common way in other works [8,20]. Besides, only the users who have watched at least 10 movies are kept. To create training, validation and testing sets, we randomly sample 1, 000 users as testing set and another 1, 000 users as validation set, while the rest users constitute the training set [8]. For validation and testing sets, the first half of user-movie views along the timeline is regarded as known behaviors to predict the latter half.\n\nUserBehavior 1 : This dataset is a subset of Taobao user behavior data. We randomly select about 1 million users who have behaviors including click, purchase, adding item to shopping cart and item favoring during November 25 to December 03, 2017. The data 1 https://tianchi.aliyun.com/datalab/dataSet.html?spm=5176.100073.0.0.614435eeJVooEG&dataId=649 is organized in a very similar form to MovieLens-20M, i.e., a useritem behavior consists of user ID, item ID, item's category ID, behavior type and timestamp. As we do in MovieLens-20M, only the users who have at least 10 behaviors are kept. 10, 000 users are randomly selected as testing set and another randomly selected 10, 000 users are validation set. Items' categories are from the bottom level of Taobao's current commodity taxonomy. Table 1 summarizes the major dimensions of the above two datasets after preprocessing.  \n\n\nMetrics and Comparison Methods\n\nTo evaluate the effectiveness of different methods, we use Preci-sion@M, Recall@M and F-Measure@M metrics [20]. Derive the recalled set of items for a user u as P u (|P u | = M) and the user's ground truth set as G u . Precision@M and Recall@M are\nPrecision@M(u) = |P u \u2229 G u | M , Recall@M(u) = |P u \u2229 G u | |G u | ,(5)\nand F-Measure@M is\nF -Measure@M(u) = 2 * Precision@M(u) * Recall@M(u) Precision@M(u) + Recall@M(u) .(6)\nAs we emphasize, recommendation results' novelty is responsible for user experience. Existing work [4] gives several approaches to measure the novelty of recommended list of items. Following one of its definition, the Novelty@M is defined as\nN o elt @M(u) = |P u \\ S u | M ,(7)\nwhere S u is the set of items that have interactions with user u before recommending. User average of the above four metrics in testing set are used to compare the following methods:\n\n\u2022 FM [28]. FM is a framework for factorization tasks. We use the implementation of FM provided by xLearn 2 project. \u2022 BPR-MF [29]. We use its matrix factorization form for implicit feedback recommendation. Implementation of BPR-MF provided by [10] is used. \u2022 Item-CF [31]. Item-based collaborative filtering is one of the most widely used personalized recommendation method in production with large-scale corpus [22]. It's also one of the major candidate generation approaches in Taobao. We use the implementation of item-CF provided by Alibaba machine learning platform. \u2022 YouTube product-DNN [7] is the deep recommendation approach proposed by YouTube. Sampled-softmax [16] is employed in training, and the inner product of user and item's embeddings reflects the preference. We implement YouTube product-DNN in Alibaba deep learning platform with the same input features with our proposed model. Exact kNN search in inner product space is adopted in prediction. \u2022 TDM attention-DNN (tree-based deep model using attention network) is our proposed method in Figure 2. The tree is initialized in the way described in Section 3.5 and keeps unchanged during the experiments. The implementation is available in GitHub 3 .\n\nFor FM, BPR-MF and item-CF, we tune several most important hyper-parameters based on the validation set, i.e., the number of factors and iterations in FM and BPR-MF, the number of neighbors in item-CF. FM and BPR-MF require that the users in testing or validation set also have feedback in training set. Therefore, we add the first half of user-item interactions along the timeline in testing and validation set into the training set in both datasets. For YouTube product-DNN and TDM attention-DNN, the node embeddings' dimension is set to 24, because a higher dimension doesn't perform significantly better in our experiments. The hidden unit numbers of three fully connected layers are 128, 64 and 24 respectively. According to the timestamp, user behaviors are divided into 10 time windows. In YouTube product-DNN and TDM attention-DNN, for each implicit feedback we randomly select 100 negative samples in MovieLens-20M and 600 negative samples in UserBehavior. Note that the negative sample number of TDM is the sum of all levels. And we sample more negatives for levels near to leaf.\n\n\nComparison Results\n\nThe comparison results of different methods are shown in Table 2 above the dash line. Each metric is the average across all the users in testing set, and the presented values are the average across five different runs for methods with variance.\n\nFirst, the results indicate that the proposed TDM attention-DNN outperforms all the baselines significantly in both datasets on most of the metrics. Comparing to the second best YouTube product-DNN approach, TDM attention-DNN achieves 21.1% and 42.6% improvements on recall metric in two datasets respectively without filtering. This result proves the effectiveness of advanced neural network and hierarchical tree search adopted by TDM attention-DNN. Among the methods that model user preference over items in inner product form, YouTube product-DNN outperforms BPR-MF and FM because of the usage of neural network. The widely used item-CF method gets worst novelty results, since it has strong memories about what the user has already interacted.\n\nTo improve the novelty, a common way in practice is to filter those interacted items in recommendation set [8,20], i.e., only those novel items could be ultimately recommended. Thus, it's more important to compare accuracy in a complete novel result set. In this experiment, the result set size will be complemented to required number M if its size is smaller than M after filtering. The bottom half of Table 2 shows that TDM attention-DNN outperforms all baselines in large margin as well after filtering interacted items.\n\nTo further evaluate the exploration ability of different methods, we do experiments by excluding those interacted categories from 3 https://github.com/alibaba/x-deeplearning/tree/master/xdl-algorithm-solution/TDM recommendation results. Results of each method are also complemented to satisfy the size requirement. Indeed, category-level novelty is currently the most important novelty metric in Taobao recommender system, as we want to reduce the amount of recommendations similar to user's interacted items. Since MovieLens-20M has only 20 categories in total, these experiments are only conducted in UserBehavior dataset and results are shown in Table 3. Take the recall metric for example. We can observe that item-CF's recall is only 1.06%, because its recommendation results can hardly jump out of user's historical behaviors. YouTube product-DNN gets much better results compared to item-CF, since it can explore user's potential interests from the entire corpus. The proposed TDM attention-DNN performs 34.3% better in recall than YouTube's inner product manner. Such huge improvement is very meaningful for recommender systems, and it proves that more advanced model is an enormous difference for recommendation problem.\n\n\nEmpirical Analysis\n\nVariants of TDM. To comprehend the proposed TDM method itself, we derive and evaluate several variants of TDM:\n\n\u2022 TDM product-DNN. To find out whether advanced neural network can benefit the results in TDM, we test the variant TDM product-DNN. TDM product-DNN uses the same inner product manner as YouTube product-DNN. Specifically, the attention module in Figure 2 is removed, and the node embedding term is also removed from the network input. The inner product of node embedding and the third fully connected layer's output (without PReLU and BN) along with a sigmoid activation constitute the new binary classifier. \u2022 TDM DNN. To further verify the improvements brought by attention module in TDM attention-DNN, we test the variant TDM DNN that only removes the activation unit, i.e., all items' weights are 1.0 in Figure 2. \u2022 TDM attention-DNN-HS. As mentioned in Section 3, hierarchical softmax (HS) method [24] is not suitable for recommendation. We test the TDM attention-DNN-HS variant, i.e., use positive nodes' neighbors as negative samples instead of randomly selected ones. Correspondingly, in retrieval of Algorithm 1, the ranking indicator changes from a single node's P \u02c6 u (n) = 1|n,u to n \u2032 \u2208 n's ancestors P \u02c6 u (n \u2032 ) = 1|n \u2032 , u . Attention-DNN is used as the network structure.\n\nThe experimental results of the above variants in both datasets are shown in Table 2 under the dash line. Comparing TDM attention-DNN to TDM DNN, the near 10% recall improvement in UserBehavior dataset indicates that the attention module takes impressive efforts. TDM product-DNN performs worse than TDM DNN and TDM attention-DNN, since the inner product manner is much less powerful than the neural network interaction form. These results prove that introducing advanced models in TDM can significantly improve the recommendation performance. Note that TDM attention-DNN-HS gets much worse results compared to TDM attention-DNN, since hierarchical softmax's formulation doesn't fit for recommendation problem.   Table 3: Results in UserBehavior dataset. Items belong to interacted categories are excluded from recommendation results and ground truth.\n\nRole of the tree. Tree is the key component of the proposed TDM method. It not only acts as an index used in retrieval, but also models the corpus in coarse-to-fine hierarchy. Section 3.3 mentioned that directly making fine-grained recommendation is more difficult than a hierarchical way. We conduct experiments to prove the point of view. Figure 4 illustrates the layer-wise Recall@200 of hierarchical tree search (Algorithm 1) and brute-force search (traverse all nodes in the corresponding level). The experiments are conducted in UserBehavior dataset with TDM product-DNN model, because it's the only variant that is possible to employ bruteforce search. Brute-force search slightly outperforms tree search in high levels (level 8,9), since the node numbers there are small. Once the node number in a level grows, tree search gets better recall results compared to brute-force search, because the tree search can exclude those low quality results in high levels, which reduces the difficulty of the problems in low levels. This result indicates that the hierarchy information contained in the tree structure can help improve recommendation preciseness.\n\nTree learning. In Section 3.5, we propose the tree initialization and learning algorithms. Table 4 gives the comparison results between initial tree and learnt tree. From the results, we can observe that the trained model with learnt tree structure significantly outperforms the initial one. For example, the recall metric of learnt  tree increases from 4.15% to 4.82% compared to initial tree in experiments of filtering interacted categories, which surpasses YouTube product-DNN's 3.09% and item-CF's 1.06% in very large margin. To further compare these two trees, we illustrate the test loss and recall curve of TDM attention-DNN method w.r.t. training iterations in Figure 5. From Figure 5(a), we can see that the learnt tree structure gets smaller test loss. And both Figure 5(a) and 5(b) indicate that the model converges to better results with learnt tree. The above results prove that the tree learning algorithm can improve the hierarchy of items, further to facilitate training and prediction.\n\n\nOnline Results\n\nWe evaluate the proposed TDM method in Taobao    In our advertising system, advertisers bid on some given ad clusters. There are about 1.4 million clusters and each ad cluster contains hundreds or thousands of similar ads. The experiments are conducted in the granularity of ad cluster to keep consistent with the existing system. The comparison method is mixture of logistic regression [9] that used to pick out superior results only from those interacted clusters, which is a strong baseline. Since there are many stages in the system like CTR prediction [11,34] and ranking [35] as illustrated in Figure 1, deploying and evaluating the proposed TDM method online is a huge project, which involves the linkage and optimization of the whole system. We have finished the deployment of the first TDM DNN version so far and evaluated its improvements online. Each of the comparison buckets has 5% of all online traffic. It's worth mentioning that there are several online simultaneously running recommendation methods. They take efforts in different point of views, and their recommendation results are merged together for the following stages. TDM only replaces the most effective one of them while keeping other modules unchanged. The average metric lift rates of the testing bucket with TDM are listed in Table 5.\n\nAs shown in Table 5, the CTR of TDM method increases 2.1%. This improvement indicates that the proposed method can recall more accurate results for users. And on the other hand the RPM metric increases 6.4%, which means the TDM method can also bring more revenue for Taobao advertising platform. TDM has been Metric CTR RPM Lift Rate 2.1% 6.4% deployed to serve major online traffic, we believe that the above improvement is only a preliminary result in a huge project, and there has room for further improvements.\n\nPrediction efficiency. TDM makes advanced neural network feasible to interact user and items in large-scale recommendation, which opens a new perspective of view in recommender systems. It's worth mentioning that though advanced neural networks need more calculation when inferring, but the complexity of a whole prediction process is no larger than O(k * log |C | * t), where k is the required results size, |C | is the corpus size and t is the complexity of network's single feed-forward pass. This complexity upper bound is acceptable under current CPU/GPU hardware conditions, and user side's features are shared across different nodes in one retrieval and some calculation could be shared according to model designs. In Taobao display advertising system, it actually takes the deployed TDM DNN model about 6 milliseconds to recommend once in average. Such running time is shorter than the following click-through rate prediction module, and is not the system's bottleneck.\n\n\nCONCLUSION\n\nWe figure out the main challenge for model-based methods to generate recommendations from large-scale corpus, i.e., the amount of calculation problem when making prediction. A tree-based approach is proposed, where arbitrary advanced models can be employed in large-scale recommendation to infer user interests coarseto-fine along the tree. Besides training the model, a tree structure learning approach is used, which proves that a better tree structure can lead to significantly better results. A possible future direction is to design more elaborate tree learning approaches. We conduct extensive experiments which validate the effectiveness of the proposed method, both in recommendation accuracy and novelty. In addition, empirical analysis showcases how and why the proposed method works. In Taobao display advertising platform, the proposed TDM method has been deployed in production, which improves both business benefits and user experience.\n\nAlgorithm 1 :\n1Layer-wise Retrieval Algorithm in PredictionInput: User state u, the recommendation tree, the desired item number k, the learnt model Output: The set of recommended leaf nodes 1 Result set A = \u2205, candidate set Q = {the root node n 1 }; 2 repeat3 If there are leaf nodes in Q, remove them from Q and insert them into A;4 Calculate P \u02c6 u (n) = 1|n,u for each remaining node n \u2208Q;5 Sort nodes in Q in descending order of P \u02c6 u (n) = 1|n,u and derive the set of top k nodes as I ; 6 Q = children nodes of n|n \u2208 I ; 7 until |Q | == 0; 8 Return the top k items in set A, according to P \u02c6 u (n) = 1|n,u ;\n\nFigure 2 :\n2The tree-based deep model architecture. User behaviors are divided into different time windows according to the timestamp.\n\nFigure 4 :\n4The results of layer-wise Recall@200 in UserBehavior dataset. The ground truth in testing set is traced back to each node's ancestors, till the root node.\n\nFigure 5 :\n5The test loss and test Recall@200 on UserBehavior dataset for initial and learnt tree.rate (CTR) and revenue per mille (RPM). Details are as follows: CT R = # of clicks # of impressions , RPM = Ad revenue # of impressions * 1000. (8)\n\nTable 1 :\n1Dimensions of the two datasets after preprocessing. One record is a user-item pair that represents user feedback.\n\nTable 2 :\n2The comparison results of different methods in MovieLens-20M and UserBehavior datasets. According to the different corpus size, metrics are evaluated @10 in MovieLens-20 and @200 in UserBehavior. In experiments of filtering interacted items, the recommendation results and ground truth only contain items that the user has not yet interacted with before.Method (@200) \nPrecision Recall F-Measure \nItem-CF \n0.07% \n1.06% \n0.13% \nYouTube product-DNN \n0.26% \n3.09% \n0.45% \nTDM attention-DNN \n0.35% \n4.15% \n0.60% \n\n\n\n\ndisplay advertising platform with real traffic. The experiments are conducted in Guess What You Like column of Taobao App Homepage. Two online metrics are used to measure the performance: click-through8 \n\nFiltering \n\nTree \nPrecision \nRecall F-Measure Novelty \n\nNone \nInitial \n2.00% \n10.81% \n3.03% \n97.30% \nLearnt \n2.34% \n12.37% \n3.54% \n96.68% \nInteracted \nitems \n\nInitial \n1.16% \n7.50% \n1.81% \n100.00% \nLearnt \n1.33% \n8.38% \n2.09% \n100.00% \nInteracted \ncategories \n\nInitial \n0.35% \n4.15% \n0.60% \n100.00% \nLearnt \n0.40% \n4.82% \n0.69% \n100.00% \n\n\n\nTable 4 :\n4Comparison results of different tree structures in UserBehavior dataset using TDM attention-DNN model (@200). Tree is initialized and learnt according to the algorithm described in Section 3.5.0.5 \n1 \n1.5 \n2 \n2.5 \n3 \n3.5 \n4 \n\nEpoch \n\n0.05 \n\n0.06 \n\n0.07 \n\n0.08 \n\n0.09 \n\n0.1 \n\nTest Loss \n\nLearnt Tree \nInitial Tree \n\n(a) Test Loss \n\n0.5 \n1 \n1.5 \n2 \n2.5 \n3 \n3.5 \n4 \n\nEpoch \n\n0 \n\n0.03 \n\n0.06 \n\n0.09 \n\n0.12 \n\n0.15 \n\n0.18 \n\nRecall@200 \n\nLearnt Tree \nInitial Tree \n\n(b) Test Recall \n\n\n\nTable 5 :\n5Online results from Jan 22 to Jan 28, 2018 in Guess What You Like column of Taobao App Homepage.\nhttps://github.com/aksnzhy/xlearn\nACKNOWLEDGEMENTSWe deeply appreciate Jian Xu, Chengru Song, Chuan Yu, Guorui Zhou and Yongliang Wang for their helpful suggestions and discussions. Thank Huimin Yi, Yang Zheng, Zelin Hu, Sui Huang, Yin Yang and Bochao Liu for implementing the key components of the training and serving infrastructure. Thank Haiyang He, Yangyang Fu and Yang Wang for necessary engineering supports.\nMultilabel learning with millions of labels: Recommending advertiser bid phrases for 9 web pages. Rahul Agrawal, Archit Gupta, Yashoteja Prabhu, Manik Varma, Proceedings of the 22nd international conference on World Wide Web. the 22nd international conference on World Wide WebACMRahul Agrawal, Archit Gupta, Yashoteja Prabhu, and Manik Varma. 2013. Multi- label learning with millions of labels: Recommending advertiser bid phrases for 9 web pages. In Proceedings of the 22nd international conference on World Wide Web. ACM, 13-24.\n\nLabel embedding trees for large multi-class tasks. Samy Bengio, Jason Weston, David Grangier, International Conference on Neural Information Processing Systems. Samy Bengio, Jason Weston, and David Grangier. 2010. Label embedding trees for large multi-class tasks. In International Conference on Neural Information Pro- cessing Systems. 163-171.\n\nMulticlass classification with filter trees. Alina Beygelzimer, John Langford, Pradeep Ravikumar, Gynecologic Oncology. 105Alina Beygelzimer, John Langford, and Pradeep Ravikumar. 2007. Multiclass classification with filter trees. Gynecologic Oncology 105, 2 (2007), 312-320.\n\nNovelty and Diversity Metrics for Recommender Systems: Choice, Discovery and Relevance. Pablo Castells, Sa\u00e3\u017el Vargas, Jun Wang, Proceedings of International Workshop on Diversity in Document Retrieval. International Workshop on Diversity in Document RetrievalPablo Castells, Sa\u00c3\u017el Vargas, and Jun Wang. 2011. Novelty and Diversity Metrics for Recommender Systems: Choice, Discovery and Relevance. In Proceedings of International Workshop on Diversity in Document Retrieval (2011), 29-37.\n\n. Heng-Tze, Levent Cheng, Jeremiah Koc, Tal Harmsen, Tushar Shaked, Hrishi Chandra, Glen Aradhye, Greg Anderson, Wei Corrado, Mustafa Chai, Ispir, Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al.\n\nWide & deep learning for recommender systems. Proceedings of the 1st Workshop on Deep Learning for Recommender Systems. the 1st Workshop on Deep Learning for Recommender SystemsACMWide & deep learning for recommender systems. In Proceedings of the 1st Workshop on Deep Learning for Recommender Systems. ACM, 7-10.\n\nLogarithmic time online multiclass prediction. E Anna, John Choromanska, Langford, Advances in Neural Information Processing Systems. Anna E Choromanska and John Langford. 2015. Logarithmic time online multi- class prediction. In Advances in Neural Information Processing Systems. 55-63.\n\nDeep Neural Networks for YouTube Recommendations. Paul Covington, Jay Adams, Emre Sargin, ACM Conference on Recommender Systems. Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep Neural Networks for YouTube Recommendations. In ACM Conference on Recommender Systems. 191- 198.\n\nRobin Devooght, Hugues Bersini, arXiv:1608.07400Collaborative filtering with recurrent neural networks. arXiv preprintRobin Devooght and Hugues Bersini. 2016. Collaborative filtering with recurrent neural networks. arXiv preprint arXiv:1608.07400 (2016).\n\nLearning Piecewise Linear Models from Large Scale Data for Ad Click Prediction. Kun Gai, Xiaoqiang Zhu, Han Li, Kai Liu, Zhe Wang, arXiv:1704.05194arXiv preprintKun Gai, Xiaoqiang Zhu, Han Li, Kai Liu, and Zhe Wang. 2017. Learning Piece- wise Linear Models from Large Scale Data for Ad Click Prediction. arXiv preprint arXiv:1704.05194 (2017).\n\nMyMediaLite: A free recommender system library. Zeno Gantner, Steffen Rendle, Christoph Freudenthaler, Lars Schmidt-Thieme, Proceedings of the fifth ACM conference on Recommender systems. the fifth ACM conference on Recommender systemsACMZeno Gantner, Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt- Thieme. 2011. MyMediaLite: A free recommender system library. In Proceedings of the fifth ACM conference on Recommender systems. ACM, 305-308.\n\nTiezheng Ge, Liqin Zhao, Guorui Zhou, Keyu Chen, Shuying Liu, Huiming Yi, Zelin Hu, Bochao Liu, Peng Sun, Haoyu Liu, arXiv:1711.06505Image Matters: Jointly Train Advertising CTR Model with Image Representation of Ad and User Behavior. arXiv preprintTiezheng Ge, Liqin Zhao, Guorui Zhou, Keyu Chen, Shuying Liu, Huiming Yi, Zelin Hu, Bochao Liu, Peng Sun, Haoyu Liu, et al. 2017. Image Matters: Jointly Train Advertising CTR Model with Image Representation of Ad and User Behav- ior. arXiv preprint arXiv:1711.06505 (2017).\n\nThe movielens datasets: History and context. Maxwell Harper, Joseph A Konstan, ACM Transactions on Interactive Intelligent Systems. 519F Maxwell Harper and Joseph A Konstan. 2016. The movielens datasets: History and context. ACM Transactions on Interactive Intelligent Systems 5, 4 (2016), 19.\n\nNeural collaborative filtering. Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, Tat-Seng Chua, Proceedings of the 26th International Conference on World Wide Web. the 26th International Conference on World Wide WebXiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th International Conference on World Wide Web. 173-182.\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. Sergey Ioffe, Christian Szegedy, International conference on machine learning. Sergey Ioffe and Christian Szegedy. 2015. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International con- ference on machine learning. 448-456.\n\nExtreme multi-label loss functions for recommendation, tagging, ranking & other missing label applications. Himanshu Jain, Yashoteja Prabhu, Manik Varma, Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data MiningACMHimanshu Jain, Yashoteja Prabhu, and Manik Varma. 2016. Extreme multi-label loss functions for recommendation, tagging, ranking & other missing label ap- plications. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 935-944.\n\nOn using very large target vocabulary for neural machine translation. S\u00e9bastien Jean, Kyunghyun Cho, Roland Memisevic, Yoshua Bengio, arXiv:1412.2007arXiv preprintS\u00e9bastien Jean, Kyunghyun Cho, Roland Memisevic, and Yoshua Bengio. 2014. On using very large target vocabulary for neural machine translation. arXiv preprint arXiv:1412.2007 (2014).\n\nJunqi Jin, Chengru Song, Han Li, Kun Gai, Jun Wang, Weinan Zhang, arXiv:1802.09756Real-Time Bidding with Multi-Agent Reinforcement Learning in Display Advertising. arXiv preprintJunqi Jin, Chengru Song, Han Li, Kun Gai, Jun Wang, and Weinan Zhang. 2018. Real-Time Bidding with Multi-Agent Reinforcement Learning in Display Adver- tising. arXiv preprint arXiv:1802.09756 (2018).\n\nJeff Johnson, Matthijs Douze, Herv\u00e9 J\u00e9gou, arXiv:1702.08734Billion-scale similarity search with GPUs. arXiv preprintJeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. 2017. Billion-scale similarity search with GPUs. arXiv preprint arXiv:1702.08734 (2017).\n\nMatrix Factorization Techniques for Recommender Systems. Yehuda Koren, Robert Bell, Chris Volinsky, Computer. 42Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix Factorization Tech- niques for Recommender Systems. Computer 42, 8 (2009), 30-37.\n\nFactorization Meets the Item Embedding:Regularizing Matrix Factorization with Item Co-occurrence. Dawen Liang, Jaan Altosaar, Laurent Charlin, David M Blei, ACM Conference on Recommender Systems. Dawen Liang, Jaan Altosaar, Laurent Charlin, and David M. Blei. 2016. Factor- ization Meets the Item Embedding:Regularizing Matrix Factorization with Item Co-occurrence. In ACM Conference on Recommender Systems. 59-66.\n\nWordNet: An Electronic Lexical Database. D Lin, Computational Linguistics. 25D. Lin. 1999. WordNet: An Electronic Lexical Database. Computational Linguis- tics 25, 2 (1999), 292-296.\n\nAmazon.com recommendations: Item-to-item collaborative filtering. Greg Linden, Brent Smith, Jeremy York, IEEE Internet computing. 7Greg Linden, Brent Smith, and Jeremy York. 2003. Amazon.com recommenda- tions: Item-to-item collaborative filtering. IEEE Internet computing 7, 1 (2003), 76-80.\n\nDistributed representations of words and phrases and their compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, Jeffrey Dean, International Conference on Neural Information Processing Systems. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In International Conference on Neural Information Processing Systems. 3111-3119.\n\nHierarchical probabilistic neural network language model. Frederic Morin, Yoshua Bengio, Aistats. Frederic Morin and Yoshua Bengio. 2005. Hierarchical probabilistic neural net- work language model. Aistats (2005).\n\nOn spectral clustering: analysis and an algorithm. Andrew Y Ng, Michael I Jordan, Yair Weiss, International Conference on Neural Information Processing Systems: Natural and Synthetic. Andrew Y. Ng, Michael I. Jordan, and Yair Weiss. 2001. On spectral clustering: analysis and an algorithm. In International Conference on Neural Information Pro- cessing Systems: Natural and Synthetic. 849-856.\n\nFastxml: A fast, accurate and stable tree-classifier for extreme multi-label learning. Yashoteja Prabhu, Manik Varma, Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. the 20th ACM SIGKDD international conference on Knowledge discovery and data miningACMYashoteja Prabhu and Manik Varma. 2014. Fastxml: A fast, accurate and stable tree-classifier for extreme multi-label learning. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 263-272.\n\nProduct-based neural networks for user response prediction. Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, Jun Wang, IEEE 16th International Conference on Data Mining. IEEEYanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang. 2016. Product-based neural networks for user response prediction. In IEEE 16th International Conference on Data Mining. IEEE, 1149-1154.\n\nFactorization Machines. Steffen Rendle, IEEE International Conference on Data Mining. Steffen Rendle. 2010. Factorization Machines. In IEEE International Conference on Data Mining. 995-1000.\n\nBPR: Bayesian personalized ranking from implicit feedback. Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, Lars Schmidt-Thieme, Proceedings of the 25th conference on uncertainty in artificial intelligence. the 25th conference on uncertainty in artificial intelligenceAUAI PressSteffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt- Thieme. 2009. BPR: Bayesian personalized ranking from implicit feedback. In Proceedings of the 25th conference on uncertainty in artificial intelligence. AUAI Press, 452-461.\n\nProbabilistic Matrix Factorization. Ruslan Salakhutdinov, Andriy Mnih, International Conference on Neural Information Processing Systems. Ruslan Salakhutdinov and Andriy Mnih. 2007. Probabilistic Matrix Factorization. In International Conference on Neural Information Processing Systems. 1257-1264.\n\nItembased collaborative filtering recommendation algorithms. Badrul Sarwar, George Karypis, Joseph Konstan, John Riedl, International Conference on World Wide Web. Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2001. Item- based collaborative filtering recommendation algorithms. In International Con- ference on World Wide Web. 285-295.\n\nLabel partitioning for sublinear ranking. J Weston, A Makadia, H Yee, International Conference on Machine Learning. J. Weston, A. Makadia, and H. Yee. 2013. Label partitioning for sublinear ranking. In International Conference on Machine Learning. 181-189.\n\nBing Xu, Naiyan Wang, Tianqi Chen, Mu Li, arXiv:1505.00853Empirical evaluation of rectified activations in convolutional network. Bing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. 2015. Empirical evaluation of rectified activations in convolutional network. arXiv:1505.00853 (2015).\n\nDeep interest network for click-through rate prediction. Guorui Zhou, Chengru Song, Xiaoqiang Zhu, Xiao Ma, Yanghui Yan, Xingya Dai, Han Zhu, Junqi Jin, Han Li, Kun Gai, Proceedings of the 24th ACM SIGKDD Conference. the 24th ACM SIGKDD ConferenceACMGuorui Zhou, Chengru Song, Xiaoqiang Zhu, Xiao Ma, Yanghui Yan, Xingya Dai, Han Zhu, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through rate prediction. In Proceedings of the 24th ACM SIGKDD Conference. ACM.\n\nOptimized Cost Per Click in Taobao Display Advertising. Han Zhu, Junqi Jin, Chang Tan, Fei Pan, Yifan Zeng, Han Li, Kun Gai, Proceedings of the 23rd ACM SIGKDD Conference. the 23rd ACM SIGKDD ConferenceACMHan Zhu, Junqi Jin, Chang Tan, Fei Pan, Yifan Zeng, Han Li, and Kun Gai. 2017. Optimized Cost Per Click in Taobao Display Advertising. In Proceedings of the 23rd ACM SIGKDD Conference. ACM, 2191-2200.\n", "annotations": {"author": "[{\"end\":93,\"start\":69},{\"end\":119,\"start\":94},{\"end\":176,\"start\":120},{\"end\":234,\"start\":177},{\"end\":258,\"start\":235},{\"end\":282,\"start\":259},{\"end\":307,\"start\":283}]", "publisher": null, "author_last_name": "[{\"end\":76,\"start\":73},{\"end\":102,\"start\":100},{\"end\":132,\"start\":127},{\"end\":188,\"start\":186},{\"end\":241,\"start\":239},{\"end\":265,\"start\":263},{\"end\":290,\"start\":287}]", "author_first_name": "[{\"end\":72,\"start\":69},{\"end\":99,\"start\":94},{\"end\":126,\"start\":120},{\"end\":185,\"start\":177},{\"end\":238,\"start\":235},{\"end\":262,\"start\":259},{\"end\":286,\"start\":283}]", "author_affiliation": "[{\"end\":92,\"start\":78},{\"end\":118,\"start\":104},{\"end\":175,\"start\":161},{\"end\":233,\"start\":219},{\"end\":257,\"start\":243},{\"end\":281,\"start\":267},{\"end\":306,\"start\":292}]", "title": "[{\"end\":55,\"start\":1},{\"end\":362,\"start\":308}]", "venue": null, "abstract": "[{\"end\":2224,\"start\":579}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2530,\"start\":2527},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2546,\"start\":2542},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3347,\"start\":3343},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3450,\"start\":3446},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4852,\"start\":4848},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4855,\"start\":4852},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5033,\"start\":5029},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5319,\"start\":5315},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5478,\"start\":5475},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5937,\"start\":5933},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6365,\"start\":6362},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6383,\"start\":6379},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6727,\"start\":6723},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6956,\"start\":6952},{\"end\":7597,\"start\":7577},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9554,\"start\":9550},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12257,\"start\":12253},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12260,\"start\":12257},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12361,\"start\":12357},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":12364,\"start\":12361},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":13044,\"start\":13040},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":16196,\"start\":16193},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":16336,\"start\":16332},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":18676,\"start\":18672},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":22437,\"start\":22433},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":23660,\"start\":23656},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":23663,\"start\":23660},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":23796,\"start\":23792},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24108,\"start\":24104},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24148,\"start\":24144},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24398,\"start\":24394},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24791,\"start\":24788},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":24871,\"start\":24867},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":28270,\"start\":28266},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":28692,\"start\":28688},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":29007,\"start\":29004},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":29010,\"start\":29007},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":29274,\"start\":29271},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":30449,\"start\":30445},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":30866,\"start\":30863},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":31235,\"start\":31231},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":31355,\"start\":31351},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":31473,\"start\":31469},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":31497,\"start\":31493},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":31642,\"start\":31638},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":31823,\"start\":31820},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":31901,\"start\":31897},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":34664,\"start\":34661},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":34667,\"start\":34664},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":37248,\"start\":37244},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":39221,\"start\":39219},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":39223,\"start\":39221},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":41056,\"start\":41053},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":41227,\"start\":41223},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":41230,\"start\":41227},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":41247,\"start\":41243},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":44702,\"start\":44701},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":44776,\"start\":44775},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":44835,\"start\":44834}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":45054,\"start\":44441},{\"attributes\":{\"id\":\"fig_1\"},\"end\":45190,\"start\":45055},{\"attributes\":{\"id\":\"fig_3\"},\"end\":45358,\"start\":45191},{\"attributes\":{\"id\":\"fig_4\"},\"end\":45605,\"start\":45359},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":45731,\"start\":45606},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":46254,\"start\":45732},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":46801,\"start\":46255},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":47291,\"start\":46802},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":47400,\"start\":47292}]", "paragraph": "[{\"end\":2547,\"start\":2240},{\"end\":3200,\"start\":2549},{\"end\":4721,\"start\":3202},{\"end\":6025,\"start\":4723},{\"end\":7386,\"start\":6027},{\"end\":8349,\"start\":7388},{\"end\":9197,\"start\":8351},{\"end\":9438,\"start\":9199},{\"end\":10972,\"start\":9440},{\"end\":11592,\"start\":10974},{\"end\":12128,\"start\":11616},{\"end\":12413,\"start\":12130},{\"end\":12859,\"start\":12415},{\"end\":13352,\"start\":12885},{\"end\":14252,\"start\":13380},{\"end\":14817,\"start\":14269},{\"end\":14967,\"start\":14874},{\"end\":16344,\"start\":14969},{\"end\":16505,\"start\":16346},{\"end\":16799,\"start\":16538},{\"end\":18165,\"start\":16885},{\"end\":19491,\"start\":18167},{\"end\":19610,\"start\":19493},{\"end\":19871,\"start\":19672},{\"end\":20103,\"start\":19971},{\"end\":20952,\"start\":20105},{\"end\":21481,\"start\":20954},{\"end\":22259,\"start\":21483},{\"end\":23475,\"start\":22278},{\"end\":24362,\"start\":23510},{\"end\":25087,\"start\":24364},{\"end\":25817,\"start\":25089},{\"end\":26128,\"start\":25819},{\"end\":26914,\"start\":26130},{\"end\":27829,\"start\":26916},{\"end\":27909,\"start\":27886},{\"end\":27966,\"start\":27935},{\"end\":28130,\"start\":28011},{\"end\":28545,\"start\":28153},{\"end\":28775,\"start\":28558},{\"end\":29421,\"start\":28777},{\"end\":30304,\"start\":29423},{\"end\":30586,\"start\":30339},{\"end\":30678,\"start\":30660},{\"end\":31005,\"start\":30764},{\"end\":31224,\"start\":31042},{\"end\":32444,\"start\":31226},{\"end\":33535,\"start\":32446},{\"end\":33802,\"start\":33558},{\"end\":34552,\"start\":33804},{\"end\":35077,\"start\":34554},{\"end\":36308,\"start\":35079},{\"end\":36441,\"start\":36331},{\"end\":37630,\"start\":36443},{\"end\":38483,\"start\":37632},{\"end\":39642,\"start\":38485},{\"end\":40647,\"start\":39644},{\"end\":41980,\"start\":40666},{\"end\":42496,\"start\":41982},{\"end\":43475,\"start\":42498},{\"end\":44440,\"start\":43490}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14873,\"start\":14818},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16884,\"start\":16800},{\"attributes\":{\"id\":\"formula_2\"},\"end\":19671,\"start\":19611},{\"attributes\":{\"id\":\"formula_3\"},\"end\":19970,\"start\":19872},{\"attributes\":{\"id\":\"formula_4\"},\"end\":30659,\"start\":30587},{\"attributes\":{\"id\":\"formula_5\"},\"end\":30763,\"start\":30679},{\"attributes\":{\"id\":\"formula_6\"},\"end\":31041,\"start\":31006}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":30223,\"start\":30216},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":33622,\"start\":33615},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":34964,\"start\":34957},{\"end\":35735,\"start\":35728},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":37716,\"start\":37709},{\"end\":38352,\"start\":38345},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":39742,\"start\":39735},{\"end\":40711,\"start\":40705},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":41979,\"start\":41972},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":42001,\"start\":41994}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2238,\"start\":2226},{\"attributes\":{\"n\":\"2\"},\"end\":11614,\"start\":11595},{\"attributes\":{\"n\":\"3\"},\"end\":12883,\"start\":12862},{\"attributes\":{\"n\":\"3.1\"},\"end\":13378,\"start\":13355},{\"attributes\":{\"n\":\"3.2\"},\"end\":14267,\"start\":14255},{\"attributes\":{\"n\":\"3.3\"},\"end\":16536,\"start\":16508},{\"attributes\":{\"n\":\"3.4\"},\"end\":22276,\"start\":22262},{\"attributes\":{\"n\":\"3.5\"},\"end\":23508,\"start\":23478},{\"attributes\":{\"n\":\"4\"},\"end\":27846,\"start\":27832},{\"end\":27873,\"start\":27849},{\"end\":27884,\"start\":27876},{\"end\":27933,\"start\":27912},{\"end\":27982,\"start\":27969},{\"end\":28009,\"start\":27985},{\"attributes\":{\"n\":\"5\"},\"end\":28151,\"start\":28133},{\"attributes\":{\"n\":\"5.1\"},\"end\":28556,\"start\":28548},{\"attributes\":{\"n\":\"5.2\"},\"end\":30337,\"start\":30307},{\"attributes\":{\"n\":\"5.3\"},\"end\":33556,\"start\":33538},{\"attributes\":{\"n\":\"5.4\"},\"end\":36329,\"start\":36311},{\"attributes\":{\"n\":\"5.5\"},\"end\":40664,\"start\":40650},{\"attributes\":{\"n\":\"6\"},\"end\":43488,\"start\":43478},{\"end\":44455,\"start\":44442},{\"end\":45066,\"start\":45056},{\"end\":45202,\"start\":45192},{\"end\":45370,\"start\":45360},{\"end\":45616,\"start\":45607},{\"end\":45742,\"start\":45733},{\"end\":46812,\"start\":46803},{\"end\":47302,\"start\":47293}]", "table": "[{\"end\":46254,\"start\":46098},{\"end\":46801,\"start\":46458},{\"end\":47291,\"start\":47007}]", "figure_caption": "[{\"end\":45054,\"start\":44457},{\"end\":45190,\"start\":45068},{\"end\":45358,\"start\":45204},{\"end\":45605,\"start\":45372},{\"end\":45731,\"start\":45618},{\"end\":46098,\"start\":45744},{\"end\":46458,\"start\":46257},{\"end\":47007,\"start\":46814},{\"end\":47400,\"start\":47304}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11723,\"start\":11715},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12784,\"start\":12776},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13910,\"start\":13902},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":14629,\"start\":14621},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18994,\"start\":18986},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22380,\"start\":22372},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":23356,\"start\":23348},{\"end\":27208,\"start\":27200},{\"end\":28030,\"start\":28022},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":32293,\"start\":32285},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":36696,\"start\":36688},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":37158,\"start\":37150},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":38834,\"start\":38826},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":40322,\"start\":40314},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":40337,\"start\":40329},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":40425,\"start\":40417},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":41274,\"start\":41266}]", "bib_author_first_name": "[{\"end\":47920,\"start\":47915},{\"end\":47936,\"start\":47930},{\"end\":47953,\"start\":47944},{\"end\":47967,\"start\":47962},{\"end\":48406,\"start\":48402},{\"end\":48420,\"start\":48415},{\"end\":48434,\"start\":48429},{\"end\":48748,\"start\":48743},{\"end\":48766,\"start\":48762},{\"end\":48784,\"start\":48777},{\"end\":49068,\"start\":49063},{\"end\":49084,\"start\":49079},{\"end\":49096,\"start\":49093},{\"end\":49482,\"start\":49476},{\"end\":49498,\"start\":49490},{\"end\":49507,\"start\":49504},{\"end\":49523,\"start\":49517},{\"end\":49538,\"start\":49532},{\"end\":49552,\"start\":49548},{\"end\":49566,\"start\":49562},{\"end\":49580,\"start\":49577},{\"end\":49597,\"start\":49590},{\"end\":50126,\"start\":50125},{\"end\":50137,\"start\":50133},{\"end\":50421,\"start\":50417},{\"end\":50436,\"start\":50433},{\"end\":50448,\"start\":50444},{\"end\":50654,\"start\":50649},{\"end\":50671,\"start\":50665},{\"end\":50988,\"start\":50985},{\"end\":51003,\"start\":50994},{\"end\":51012,\"start\":51009},{\"end\":51020,\"start\":51017},{\"end\":51029,\"start\":51026},{\"end\":51302,\"start\":51298},{\"end\":51319,\"start\":51312},{\"end\":51337,\"start\":51328},{\"end\":51357,\"start\":51353},{\"end\":51713,\"start\":51705},{\"end\":51723,\"start\":51718},{\"end\":51736,\"start\":51730},{\"end\":51747,\"start\":51743},{\"end\":51761,\"start\":51754},{\"end\":51774,\"start\":51767},{\"end\":51784,\"start\":51779},{\"end\":51795,\"start\":51789},{\"end\":51805,\"start\":51801},{\"end\":51816,\"start\":51811},{\"end\":52281,\"start\":52274},{\"end\":52296,\"start\":52290},{\"end\":52298,\"start\":52297},{\"end\":52564,\"start\":52556},{\"end\":52573,\"start\":52569},{\"end\":52587,\"start\":52580},{\"end\":52602,\"start\":52595},{\"end\":52611,\"start\":52608},{\"end\":52624,\"start\":52616},{\"end\":53048,\"start\":53042},{\"end\":53065,\"start\":53056},{\"end\":53434,\"start\":53426},{\"end\":53450,\"start\":53441},{\"end\":53464,\"start\":53459},{\"end\":54021,\"start\":54012},{\"end\":54037,\"start\":54028},{\"end\":54049,\"start\":54043},{\"end\":54067,\"start\":54061},{\"end\":54294,\"start\":54289},{\"end\":54307,\"start\":54300},{\"end\":54317,\"start\":54314},{\"end\":54325,\"start\":54322},{\"end\":54334,\"start\":54331},{\"end\":54347,\"start\":54341},{\"end\":54672,\"start\":54668},{\"end\":54690,\"start\":54682},{\"end\":54703,\"start\":54698},{\"end\":54984,\"start\":54978},{\"end\":54998,\"start\":54992},{\"end\":55010,\"start\":55005},{\"end\":55279,\"start\":55274},{\"end\":55291,\"start\":55287},{\"end\":55309,\"start\":55302},{\"end\":55324,\"start\":55319},{\"end\":55326,\"start\":55325},{\"end\":55634,\"start\":55633},{\"end\":55846,\"start\":55842},{\"end\":55860,\"start\":55855},{\"end\":55874,\"start\":55868},{\"end\":56151,\"start\":56146},{\"end\":56165,\"start\":56161},{\"end\":56180,\"start\":56177},{\"end\":56191,\"start\":56187},{\"end\":56208,\"start\":56201},{\"end\":56586,\"start\":56578},{\"end\":56600,\"start\":56594},{\"end\":56792,\"start\":56786},{\"end\":56794,\"start\":56793},{\"end\":56806,\"start\":56799},{\"end\":56808,\"start\":56807},{\"end\":56821,\"start\":56817},{\"end\":57226,\"start\":57217},{\"end\":57240,\"start\":57235},{\"end\":57744,\"start\":57739},{\"end\":57752,\"start\":57749},{\"end\":57761,\"start\":57758},{\"end\":57773,\"start\":57767},{\"end\":57785,\"start\":57781},{\"end\":57794,\"start\":57790},{\"end\":57803,\"start\":57800},{\"end\":58336,\"start\":58329},{\"end\":58354,\"start\":58345},{\"end\":58374,\"start\":58370},{\"end\":58388,\"start\":58384},{\"end\":58845,\"start\":58839},{\"end\":58867,\"start\":58861},{\"end\":59170,\"start\":59164},{\"end\":59185,\"start\":59179},{\"end\":59201,\"start\":59195},{\"end\":59215,\"start\":59211},{\"end\":59501,\"start\":59500},{\"end\":59511,\"start\":59510},{\"end\":59522,\"start\":59521},{\"end\":59720,\"start\":59716},{\"end\":59731,\"start\":59725},{\"end\":59744,\"start\":59738},{\"end\":59753,\"start\":59751},{\"end\":60059,\"start\":60053},{\"end\":60073,\"start\":60066},{\"end\":60089,\"start\":60080},{\"end\":60099,\"start\":60095},{\"end\":60111,\"start\":60104},{\"end\":60123,\"start\":60117},{\"end\":60132,\"start\":60129},{\"end\":60143,\"start\":60138},{\"end\":60152,\"start\":60149},{\"end\":60160,\"start\":60157},{\"end\":60541,\"start\":60538},{\"end\":60552,\"start\":60547},{\"end\":60563,\"start\":60558},{\"end\":60572,\"start\":60569},{\"end\":60583,\"start\":60578},{\"end\":60593,\"start\":60590},{\"end\":60601,\"start\":60598}]", "bib_author_last_name": "[{\"end\":47928,\"start\":47921},{\"end\":47942,\"start\":47937},{\"end\":47960,\"start\":47954},{\"end\":47973,\"start\":47968},{\"end\":48413,\"start\":48407},{\"end\":48427,\"start\":48421},{\"end\":48443,\"start\":48435},{\"end\":48760,\"start\":48749},{\"end\":48775,\"start\":48767},{\"end\":48794,\"start\":48785},{\"end\":49077,\"start\":49069},{\"end\":49091,\"start\":49085},{\"end\":49101,\"start\":49097},{\"end\":49474,\"start\":49466},{\"end\":49488,\"start\":49483},{\"end\":49502,\"start\":49499},{\"end\":49515,\"start\":49508},{\"end\":49530,\"start\":49524},{\"end\":49546,\"start\":49539},{\"end\":49560,\"start\":49553},{\"end\":49575,\"start\":49567},{\"end\":49588,\"start\":49581},{\"end\":49602,\"start\":49598},{\"end\":49609,\"start\":49604},{\"end\":50131,\"start\":50127},{\"end\":50149,\"start\":50138},{\"end\":50159,\"start\":50151},{\"end\":50431,\"start\":50422},{\"end\":50442,\"start\":50437},{\"end\":50455,\"start\":50449},{\"end\":50663,\"start\":50655},{\"end\":50679,\"start\":50672},{\"end\":50992,\"start\":50989},{\"end\":51007,\"start\":51004},{\"end\":51015,\"start\":51013},{\"end\":51024,\"start\":51021},{\"end\":51034,\"start\":51030},{\"end\":51310,\"start\":51303},{\"end\":51326,\"start\":51320},{\"end\":51351,\"start\":51338},{\"end\":51372,\"start\":51358},{\"end\":51716,\"start\":51714},{\"end\":51728,\"start\":51724},{\"end\":51741,\"start\":51737},{\"end\":51752,\"start\":51748},{\"end\":51765,\"start\":51762},{\"end\":51777,\"start\":51775},{\"end\":51787,\"start\":51785},{\"end\":51799,\"start\":51796},{\"end\":51809,\"start\":51806},{\"end\":51820,\"start\":51817},{\"end\":52288,\"start\":52282},{\"end\":52306,\"start\":52299},{\"end\":52567,\"start\":52565},{\"end\":52578,\"start\":52574},{\"end\":52593,\"start\":52588},{\"end\":52606,\"start\":52603},{\"end\":52614,\"start\":52612},{\"end\":52629,\"start\":52625},{\"end\":53054,\"start\":53049},{\"end\":53073,\"start\":53066},{\"end\":53439,\"start\":53435},{\"end\":53457,\"start\":53451},{\"end\":53470,\"start\":53465},{\"end\":54026,\"start\":54022},{\"end\":54041,\"start\":54038},{\"end\":54059,\"start\":54050},{\"end\":54074,\"start\":54068},{\"end\":54298,\"start\":54295},{\"end\":54312,\"start\":54308},{\"end\":54320,\"start\":54318},{\"end\":54329,\"start\":54326},{\"end\":54339,\"start\":54335},{\"end\":54353,\"start\":54348},{\"end\":54680,\"start\":54673},{\"end\":54696,\"start\":54691},{\"end\":54709,\"start\":54704},{\"end\":54990,\"start\":54985},{\"end\":55003,\"start\":54999},{\"end\":55019,\"start\":55011},{\"end\":55285,\"start\":55280},{\"end\":55300,\"start\":55292},{\"end\":55317,\"start\":55310},{\"end\":55331,\"start\":55327},{\"end\":55638,\"start\":55635},{\"end\":55853,\"start\":55847},{\"end\":55866,\"start\":55861},{\"end\":55879,\"start\":55875},{\"end\":56159,\"start\":56152},{\"end\":56175,\"start\":56166},{\"end\":56185,\"start\":56181},{\"end\":56199,\"start\":56192},{\"end\":56213,\"start\":56209},{\"end\":56592,\"start\":56587},{\"end\":56607,\"start\":56601},{\"end\":56797,\"start\":56795},{\"end\":56815,\"start\":56809},{\"end\":56827,\"start\":56822},{\"end\":57233,\"start\":57227},{\"end\":57246,\"start\":57241},{\"end\":57747,\"start\":57745},{\"end\":57756,\"start\":57753},{\"end\":57765,\"start\":57762},{\"end\":57779,\"start\":57774},{\"end\":57788,\"start\":57786},{\"end\":57798,\"start\":57795},{\"end\":57808,\"start\":57804},{\"end\":58116,\"start\":58102},{\"end\":58343,\"start\":58337},{\"end\":58368,\"start\":58355},{\"end\":58382,\"start\":58375},{\"end\":58403,\"start\":58389},{\"end\":58859,\"start\":58846},{\"end\":58872,\"start\":58868},{\"end\":59177,\"start\":59171},{\"end\":59193,\"start\":59186},{\"end\":59209,\"start\":59202},{\"end\":59221,\"start\":59216},{\"end\":59508,\"start\":59502},{\"end\":59519,\"start\":59512},{\"end\":59526,\"start\":59523},{\"end\":59723,\"start\":59721},{\"end\":59736,\"start\":59732},{\"end\":59749,\"start\":59745},{\"end\":59756,\"start\":59754},{\"end\":60064,\"start\":60060},{\"end\":60078,\"start\":60074},{\"end\":60093,\"start\":60090},{\"end\":60102,\"start\":60100},{\"end\":60115,\"start\":60112},{\"end\":60127,\"start\":60124},{\"end\":60136,\"start\":60133},{\"end\":60147,\"start\":60144},{\"end\":60155,\"start\":60153},{\"end\":60164,\"start\":60161},{\"end\":60545,\"start\":60542},{\"end\":60556,\"start\":60553},{\"end\":60567,\"start\":60564},{\"end\":60576,\"start\":60573},{\"end\":60588,\"start\":60584},{\"end\":60596,\"start\":60594},{\"end\":60605,\"start\":60602}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":48349,\"start\":47817},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":312693},\"end\":48696,\"start\":48351},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":7510104},\"end\":48973,\"start\":48698},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":16094127},\"end\":49462,\"start\":48975},{\"attributes\":{\"id\":\"b4\"},\"end\":49761,\"start\":49464},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":3352400},\"end\":50076,\"start\":49763},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":9154006},\"end\":50365,\"start\":50078},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":207240067},\"end\":50647,\"start\":50367},{\"attributes\":{\"doi\":\"arXiv:1608.07400\",\"id\":\"b8\"},\"end\":50903,\"start\":50649},{\"attributes\":{\"doi\":\"arXiv:1704.05194\",\"id\":\"b9\"},\"end\":51248,\"start\":50905},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":5826952},\"end\":51703,\"start\":51250},{\"attributes\":{\"doi\":\"arXiv:1711.06505\",\"id\":\"b11\"},\"end\":52227,\"start\":51705},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":16619709},\"end\":52522,\"start\":52229},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":13907106},\"end\":52946,\"start\":52524},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":5808102},\"end\":53316,\"start\":52948},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":7697669},\"end\":53940,\"start\":53318},{\"attributes\":{\"doi\":\"arXiv:1412.2007\",\"id\":\"b16\"},\"end\":54287,\"start\":53942},{\"attributes\":{\"doi\":\"arXiv:1802.09756\",\"id\":\"b17\"},\"end\":54666,\"start\":54289},{\"attributes\":{\"doi\":\"arXiv:1702.08734\",\"id\":\"b18\"},\"end\":54919,\"start\":54668},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":58370896},\"end\":55174,\"start\":54921},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":1196797},\"end\":55590,\"start\":55176},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":5958691},\"end\":55774,\"start\":55592},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":14604122},\"end\":56067,\"start\":55776},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":16447573},\"end\":56518,\"start\":56069},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":1326925},\"end\":56733,\"start\":56520},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":18764978},\"end\":57128,\"start\":56735},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":387208},\"end\":57677,\"start\":57130},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":3424778},\"end\":58076,\"start\":57679},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":17265929},\"end\":58268,\"start\":58078},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":10795036},\"end\":58801,\"start\":58270},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":467086},\"end\":59101,\"start\":58803},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":8047550},\"end\":59456,\"start\":59103},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":10807390},\"end\":59714,\"start\":59458},{\"attributes\":{\"doi\":\"arXiv:1505.00853\",\"id\":\"b33\"},\"end\":59994,\"start\":59716},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":1637394},\"end\":60480,\"start\":59996},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":8879805},\"end\":60887,\"start\":60482}]", "bib_title": "[{\"end\":47913,\"start\":47817},{\"end\":48400,\"start\":48351},{\"end\":48741,\"start\":48698},{\"end\":49061,\"start\":48975},{\"end\":49807,\"start\":49763},{\"end\":50123,\"start\":50078},{\"end\":50415,\"start\":50367},{\"end\":51296,\"start\":51250},{\"end\":52272,\"start\":52229},{\"end\":52554,\"start\":52524},{\"end\":53040,\"start\":52948},{\"end\":53424,\"start\":53318},{\"end\":54976,\"start\":54921},{\"end\":55272,\"start\":55176},{\"end\":55631,\"start\":55592},{\"end\":55840,\"start\":55776},{\"end\":56144,\"start\":56069},{\"end\":56576,\"start\":56520},{\"end\":56784,\"start\":56735},{\"end\":57215,\"start\":57130},{\"end\":57737,\"start\":57679},{\"end\":58100,\"start\":58078},{\"end\":58327,\"start\":58270},{\"end\":58837,\"start\":58803},{\"end\":59162,\"start\":59103},{\"end\":59498,\"start\":59458},{\"end\":60051,\"start\":59996},{\"end\":60536,\"start\":60482}]", "bib_author": "[{\"end\":47930,\"start\":47915},{\"end\":47944,\"start\":47930},{\"end\":47962,\"start\":47944},{\"end\":47975,\"start\":47962},{\"end\":48415,\"start\":48402},{\"end\":48429,\"start\":48415},{\"end\":48445,\"start\":48429},{\"end\":48762,\"start\":48743},{\"end\":48777,\"start\":48762},{\"end\":48796,\"start\":48777},{\"end\":49079,\"start\":49063},{\"end\":49093,\"start\":49079},{\"end\":49103,\"start\":49093},{\"end\":49476,\"start\":49466},{\"end\":49490,\"start\":49476},{\"end\":49504,\"start\":49490},{\"end\":49517,\"start\":49504},{\"end\":49532,\"start\":49517},{\"end\":49548,\"start\":49532},{\"end\":49562,\"start\":49548},{\"end\":49577,\"start\":49562},{\"end\":49590,\"start\":49577},{\"end\":49604,\"start\":49590},{\"end\":49611,\"start\":49604},{\"end\":50133,\"start\":50125},{\"end\":50151,\"start\":50133},{\"end\":50161,\"start\":50151},{\"end\":50433,\"start\":50417},{\"end\":50444,\"start\":50433},{\"end\":50457,\"start\":50444},{\"end\":50665,\"start\":50649},{\"end\":50681,\"start\":50665},{\"end\":50994,\"start\":50985},{\"end\":51009,\"start\":50994},{\"end\":51017,\"start\":51009},{\"end\":51026,\"start\":51017},{\"end\":51036,\"start\":51026},{\"end\":51312,\"start\":51298},{\"end\":51328,\"start\":51312},{\"end\":51353,\"start\":51328},{\"end\":51374,\"start\":51353},{\"end\":51718,\"start\":51705},{\"end\":51730,\"start\":51718},{\"end\":51743,\"start\":51730},{\"end\":51754,\"start\":51743},{\"end\":51767,\"start\":51754},{\"end\":51779,\"start\":51767},{\"end\":51789,\"start\":51779},{\"end\":51801,\"start\":51789},{\"end\":51811,\"start\":51801},{\"end\":51822,\"start\":51811},{\"end\":52290,\"start\":52274},{\"end\":52308,\"start\":52290},{\"end\":52569,\"start\":52556},{\"end\":52580,\"start\":52569},{\"end\":52595,\"start\":52580},{\"end\":52608,\"start\":52595},{\"end\":52616,\"start\":52608},{\"end\":52631,\"start\":52616},{\"end\":53056,\"start\":53042},{\"end\":53075,\"start\":53056},{\"end\":53441,\"start\":53426},{\"end\":53459,\"start\":53441},{\"end\":53472,\"start\":53459},{\"end\":54028,\"start\":54012},{\"end\":54043,\"start\":54028},{\"end\":54061,\"start\":54043},{\"end\":54076,\"start\":54061},{\"end\":54300,\"start\":54289},{\"end\":54314,\"start\":54300},{\"end\":54322,\"start\":54314},{\"end\":54331,\"start\":54322},{\"end\":54341,\"start\":54331},{\"end\":54355,\"start\":54341},{\"end\":54682,\"start\":54668},{\"end\":54698,\"start\":54682},{\"end\":54711,\"start\":54698},{\"end\":54992,\"start\":54978},{\"end\":55005,\"start\":54992},{\"end\":55021,\"start\":55005},{\"end\":55287,\"start\":55274},{\"end\":55302,\"start\":55287},{\"end\":55319,\"start\":55302},{\"end\":55333,\"start\":55319},{\"end\":55640,\"start\":55633},{\"end\":55855,\"start\":55842},{\"end\":55868,\"start\":55855},{\"end\":55881,\"start\":55868},{\"end\":56161,\"start\":56146},{\"end\":56177,\"start\":56161},{\"end\":56187,\"start\":56177},{\"end\":56201,\"start\":56187},{\"end\":56215,\"start\":56201},{\"end\":56594,\"start\":56578},{\"end\":56609,\"start\":56594},{\"end\":56799,\"start\":56786},{\"end\":56817,\"start\":56799},{\"end\":56829,\"start\":56817},{\"end\":57235,\"start\":57217},{\"end\":57248,\"start\":57235},{\"end\":57749,\"start\":57739},{\"end\":57758,\"start\":57749},{\"end\":57767,\"start\":57758},{\"end\":57781,\"start\":57767},{\"end\":57790,\"start\":57781},{\"end\":57800,\"start\":57790},{\"end\":57810,\"start\":57800},{\"end\":58118,\"start\":58102},{\"end\":58345,\"start\":58329},{\"end\":58370,\"start\":58345},{\"end\":58384,\"start\":58370},{\"end\":58405,\"start\":58384},{\"end\":58861,\"start\":58839},{\"end\":58874,\"start\":58861},{\"end\":59179,\"start\":59164},{\"end\":59195,\"start\":59179},{\"end\":59211,\"start\":59195},{\"end\":59223,\"start\":59211},{\"end\":59510,\"start\":59500},{\"end\":59521,\"start\":59510},{\"end\":59528,\"start\":59521},{\"end\":59725,\"start\":59716},{\"end\":59738,\"start\":59725},{\"end\":59751,\"start\":59738},{\"end\":59758,\"start\":59751},{\"end\":60066,\"start\":60053},{\"end\":60080,\"start\":60066},{\"end\":60095,\"start\":60080},{\"end\":60104,\"start\":60095},{\"end\":60117,\"start\":60104},{\"end\":60129,\"start\":60117},{\"end\":60138,\"start\":60129},{\"end\":60149,\"start\":60138},{\"end\":60157,\"start\":60149},{\"end\":60166,\"start\":60157},{\"end\":60547,\"start\":60538},{\"end\":60558,\"start\":60547},{\"end\":60569,\"start\":60558},{\"end\":60578,\"start\":60569},{\"end\":60590,\"start\":60578},{\"end\":60598,\"start\":60590},{\"end\":60607,\"start\":60598}]", "bib_venue": "[{\"end\":48041,\"start\":47975},{\"end\":48510,\"start\":48445},{\"end\":48816,\"start\":48796},{\"end\":49175,\"start\":49103},{\"end\":49881,\"start\":49809},{\"end\":50210,\"start\":50161},{\"end\":50494,\"start\":50457},{\"end\":50751,\"start\":50697},{\"end\":50983,\"start\":50905},{\"end\":51436,\"start\":51374},{\"end\":51938,\"start\":51838},{\"end\":52359,\"start\":52308},{\"end\":52697,\"start\":52631},{\"end\":53119,\"start\":53075},{\"end\":53570,\"start\":53472},{\"end\":54010,\"start\":53942},{\"end\":54451,\"start\":54371},{\"end\":54768,\"start\":54727},{\"end\":55029,\"start\":55021},{\"end\":55370,\"start\":55333},{\"end\":55665,\"start\":55640},{\"end\":55904,\"start\":55881},{\"end\":56280,\"start\":56215},{\"end\":56616,\"start\":56609},{\"end\":56917,\"start\":56829},{\"end\":57346,\"start\":57248},{\"end\":57859,\"start\":57810},{\"end\":58162,\"start\":58118},{\"end\":58481,\"start\":58405},{\"end\":58939,\"start\":58874},{\"end\":59265,\"start\":59223},{\"end\":59572,\"start\":59528},{\"end\":59844,\"start\":59774},{\"end\":60211,\"start\":60166},{\"end\":60652,\"start\":60607},{\"end\":48094,\"start\":48043},{\"end\":49234,\"start\":49177},{\"end\":49940,\"start\":49883},{\"end\":51485,\"start\":51438},{\"end\":52750,\"start\":52699},{\"end\":53655,\"start\":53572},{\"end\":57431,\"start\":57348},{\"end\":58544,\"start\":58483},{\"end\":60243,\"start\":60213},{\"end\":60684,\"start\":60654}]"}}}, "year": 2023, "month": 12, "day": 17}