{"id": 258179044, "updated": "2023-10-05 02:01:11.631", "metadata": {"title": "Chinese Open Instruction Generalist: A Preliminary Release", "authors": "[{\"first\":\"Ge\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Yemin\",\"last\":\"Shi\",\"middle\":[]},{\"first\":\"Ruibo\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Ruibin\",\"last\":\"Yuan\",\"middle\":[]},{\"first\":\"Yizhi\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Siwei\",\"last\":\"Dong\",\"middle\":[]},{\"first\":\"Yu\",\"last\":\"Shu\",\"middle\":[]},{\"first\":\"Zhaoqun\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Zekun\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Chenghua\",\"last\":\"Lin\",\"middle\":[]},{\"first\":\"Wenhao\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Jie\",\"last\":\"Fu\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Instruction tuning is widely recognized as a key technique for building generalist language models, which has attracted the attention of researchers and the public with the release of InstructGPT~\\citep{ouyang2022training} and ChatGPT\\footnote{\\url{https://chat.openai.com/}}. Despite impressive progress in English-oriented large-scale language models (LLMs), it is still under-explored whether English-based foundation LLMs can perform similarly on multilingual tasks compared to English tasks with well-designed instruction tuning and how we can construct the corpora needed for the tuning. To remedy this gap, we propose the project as an attempt to create a Chinese instruction dataset by various methods adapted to the intrinsic characteristics of 4 sub-tasks. We collect around 200k Chinese instruction tuning samples, which have been manually checked to guarantee high quality. We also summarize the existing English and Chinese instruction corpora and briefly describe some potential applications of the newly constructed Chinese instruction corpora. The resulting \\textbf{C}hinese \\textbf{O}pen \\textbf{I}nstruction \\textbf{G}eneralist (\\textbf{COIG}) corpora are available in Huggingface\\footnote{\\url{https://huggingface.co/datasets/BAAI/COIG}} and Github\\footnote{\\url{https://github.com/BAAI-Zlab/COIG}}, and will be continuously updated.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2304.07987", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2304-07987", "doi": "10.48550/arxiv.2304.07987"}}, "content": {"source": {"pdf_hash": "c01e43c65a04d766e429863bdf7cf65b895df20e", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2304.07987v4.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "0477786afb8c03151a72d2f6f9ba10f83fae07f8", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c01e43c65a04d766e429863bdf7cf65b895df20e.txt", "contents": "\nCHINESE OPEN INSTRUCTION GENERALIST: A PRELIMINARY RELEASE\n\n\nGe Zhang gezhang@umich.edu1 \nBeijing Academy of Artificial Intelligence\nChina\n\nUniversity of Michigan Ann Arbor\nUSA\n\nYemin Shi \nBeijing Academy of Artificial Intelligence\nChina\n\nRuibo Liu \nBeijing Academy of Artificial Intelligence\nChina\n\nDartmouth University\nUSA\n\nRuibin Yuan \nBeijing Academy of Artificial Intelligence\nChina\n\nCarnegie Mellon University\nUSA\n\nYizhi Li \nDepartment of Computer Science\nThe University of Sheffield\nUK\n\nSiwei Dong \nBeijing Academy of Artificial Intelligence\nChina\n\nYu Shu \nBeijing Academy of Artificial Intelligence\nChina\n\nZhaoqun Li \nBeijing Academy of Artificial Intelligence\nChina\n\nZhejiang University\nChina\n\nZekun Wang \nBeijing Academy of Artificial Intelligence\nChina\n\nBeihang University\nChina\n\nChenghua Lin \nBeijing Academy of Artificial Intelligence\nChina\n\nDepartment of Computer Science\nThe University of Sheffield\nUK\n\nWenhao Huang \nBeijing Academy of Artificial Intelligence\nChina\n\n\u2020 \nJie Fu fujie@baai.ac.cn1 \nBeijing Academy of Artificial Intelligence\nChina\n\nCHINESE OPEN INSTRUCTION GENERALIST: A PRELIMINARY RELEASE\n\nInstruction tuning is widely recognized as a key technique for building generalist language models, which has attracted the attention of researchers and the public with the release of InstructGPT (Ouyang  et al., 2022)  and ChatGPT 3 . Despite impressive progress in English-oriented large-scale language models (LLMs), it is still under-explored whether English-based foundation LLMs can perform similarly on multilingual tasks compared to English tasks with well-designed instruction tuning and how we can construct the corpora needed for the tuning. To remedy this gap, we propose the project as an attempt to create a Chinese instruction dataset by various methods adapted to the intrinsic characteristics of 4 sub-tasks. We collect around 200k Chinese instruction tuning samples, which have been manually checked to guarantee high quality. We also summarize the existing English and Chinese instruction corpora and briefly describe some potential applications of the newly constructed Chinese instruction corpora. The resulting Chinese Open Instruction Generalist (COIG) corpora are available in Huggingface 4 and Github 5 , and will be continuously updated. * The two authors contributed equally to this work. \u2020 Corresponding authors.\n\nIntroduction\n\nPre-trained large-scale language models (LLMs) have shown revolutionary performance in many downstream tasks Wei et al., 2021). One crucial ability of LLMs is called instruction following. That is, models can complete the tasks described by instructions given as input. This ability is based on a specialized training stage called instruction tuning. Compared to unlabeled data used for pre-training, the data for instruction tuning is typically more goal-oriented, and it should explicitly demonstrate how a response follows its corresponding instruction with a given input.\n\nThere are many instruction tuning datasets in English. For example, the FLAN collection (Longpre et al., 2023) contains 15M examples covering 1836 tasks, and OPT-IML (Iyer et al., 2022b) claims to have 18M examples for more than 2000 tasks (although it is still not publicly available). In contrast, existing data resources for Chinese instruction tuning are either small in scale or have questionable quality. For example, Ziang Leng and  directly translate English instruction tuning data into Chinese, but do not consider mitigating translation errors or potential cultural gaps, e.g. Chinese prefer self-sacrifice spirit while most Western countries prefer self-expression and individualistic heroism, Table 1: The four dimensions we consider when constructing COIG instruction following data. Verification: whether the response can be verified. Format: whether the format is crucial. Culture: whether the response will depend on a certain culture. Scale: whether scaling is important. The number of filled stars presents the importance of a certain factor. between languages. For datasets that are mostly constructed through generations using Chinese LLMs (Yunjie et al., 2023;Xu et al., 2023;Chenghao Fan and Tian, 2023), they typically lacks a thorough data verification process for quality assurance. Therefore, we are motivated to develop new instruction-tuning corpora, Chinese Open Instruction Generalist (COIG), that is larger, more diverse, and manually verified by humans. This ensures its quality, which echoes the findings of Alpaca (Taori et al., 2023) that high-quality and diverse demonstrations are crucial for good instruction-following performance.\n\nWe highlight some unique features of COIG:\n\n\u2022 Domain Adaption: As shown in Tab. 1, we consider four dimensions of instruction-tuning datasets (Verification, Format, Culture, Scaling). For each domain, we adapt our data collection pipeline to better reflect the domain specialty.\n\n\u2022 Diversity: We consider a variety of tasks, including common sense reasoning, human value alignment, code generation, and hallucination correction, while very few Chinese instruction tuning data is deliberately designed for such a complete spectrum.\n\n\u2022 Quality Check by Humans: Compared to existing model-generated Chinese instruction corpora, including (Ziang Leng and Li, 2023;Yunjie et al., 2023;Xue et al., 2023;JosephusCheung, 2021), COIG translated corpus is carefully verified by human annotators. Moreover, since COIG translated corpus is translated from English instruction corpora Honovich et al., 2022;Wang et al., 2022a) with diverse tasks, it is much more diverse than Chinese instruction corpora built by adapting prompt engineering on existing Chinese datasets, e.g. (Zeng et al., 2023;Yang, 2023;Guo et al., 2023).\n\nThe main portion of COIG data is actual data that already exists on the Web, and we convert it into the proper instruction-following manner in terms of their characteristics. For example, for the academic exams domain, we crawled and manually annotated 63.5k instructions from the Chinese National College Entrance Examination, Civil Servant Examination, etc. COIG also features in including data on human value alignment in the Chinese-speaking world, and leetcode-based instruction following samples for programming. To ensure the final data quality, we hired 223 Chinese college students as quality checkers, to help us with data filtering, correction, and ratings. The resulting COIG corpus is a comprehensive set that can equip Chinese LLMs with strong instruction-following abilities in many domains.\n\nIn addition, we provide insights into the data construction pipeline based on empirical observations. We demonstrate that selecting the proper pipeline for different domains is crucial, and we have suggested the best practice for constructing instruction-tuning data in the domains COIG covers ( \u00a7 3), which can be used as a reference for future instruction corpus construction workflow design.\n\nThe paper's contributions are as follows:\n\n\u2022 To the best of our knowledge, this is one of the very first research works specifically summarizing the existing Chinese instruction tuning corpora and providing insights about how future Chinese instruction tuning corpora can be constructed.\n\n\u2022 We construct 5 open-source high-quality Chinese instruction corpora, including a 68k general Chinese instruction corpus, a 62k Chinese exam instruction corpus, a 3k Chinese human-value alignment corpus, and a 13k Chinese Counterfactual Correction Multi-round Chat corpus, as samples of constructing new Chinese instruction corpora along the research directions pointed out.\n\n\u2022 We construct a manually verified general high-quality Chinese instruction tuning corpus which can be directly used for Chinese LLMs' instruction tuning, both commercial and non-commercial. Large language models (LLMs) fine-tuned to respond to specific instructions have demonstrated a remarkable zero-shot ability to generalize to new tasks. One key ingredient is the curation of the instruction data, for which the research community has developed various strategies for dataset construction. In this section, we provide a comprehensive summary of the English and Chinese instruction corpora in Tab. 2, 3 and Tab. 4, respectively. We also describe the mainstream approaches for constructing the instruction-tuning datasets below.\n\n\nExisting Instruction Corpora\n\nHuman annotation. Early attempts to construct instruction data is typically through human annotation (Mishra et al., 2022;Wang et al., 2022b;Databricks, 2022). Representative works include PromptSource (Bach et al., 2022) and Super-Natural-Instructions , both instructional datasets that require extensive manual/expert annotations to collect instructions that can train models to follow various in-context instructions. Although human-annotated instruction data is generally of high quality, they are also limited in quantity, diversity, and creativity. This limitation has a significant impact. Previous research has demonstrated a direct correlation between the size and diversity of instructional data and the generalizability of the resultant models to previously unseen tasks .\n\nSemi-and automatic construction. To address this bottleneck and reduce dependence on human annotators, researchers have proposed various methods, ranging from semi-automatic , to fully automatic instruction generation (Honovich et al., 2022). Self-Instruct ) is a bootstrapping framework that utilizes an initial set of manually-written instructions to guide the expansion of the instructions. The framework generates its instructions and aligns its outputs with them, resulting in enhanced instruction-following abilities of LLMs. Motivated by recent research on leveraging language models for data generation, Honovich et al. (2022) propose to collect instructions by prompting an LLM. This involves eliciting additional instruction examples using a limited number of seed instructions, and further expanding the dataset by soliciting the model to rephrase each instruction. To promote creativity, stochastic decoding is utilized to generate diverse example inputs, while deterministic decoding is employed for output generation to ensure accuracy. Although automated or semi-automated methods for data construction significantly reduce the need for human labor, they may also result in a substantial amount of noise in the generated samples. For instance, Unnatural Instructions can exhibit up to 50% noisy samples. Therefore, it is crucial to implement mechanisms (e.g., data pruning) that can mitigate this challenge and improve the usability of the data.\n\nLLM Society. Given a set of manual configuration, communicative or generative agents represent a promising alternative approach to generate instruction or chat corpora (Guohao et al., 2023;Xu et al., 2023;Park et al., 2023). Guohao et al. (2023) claim that the instruction and chat corpora generated by communicative agents can retain many useful characteristics of CoTs and self-refinement. Xu et al. (2023) validate that self-chat corpus generated by only giving ChatGPT manual configurations is helpful for aligning LLM with human preferences.\n\nTranslation. In addition to the aforementioned methods, translation is also a primary method for constructing Chinese instruction tuning corpora, that is, translating English corpora into Chinese. Representative datasets constructed through translation include Luotuo (Ziang Leng and Li, 2023), BELLE (Yunjie et al., 2023), and Alpaca , which are constructed by machine translation engines with little or no manual verification. Furthermore, the popular Alpaca (Taori et al., 2023;Liu et al., 2023) instruction corpus is limited to noncommercial use only. Nevertheless, the Chinese instruction corpora are much scarcer, compared to the English corpora available. A Preliminary Release \n\n\nCOIG: Chinese Open Instruction Generalist\n\nTo address the scarcity of instruction corpora, we propose the Chinese Open Instruction Generalist (COIG) project to maintain a harmless, helpful, and diverse set of Chinese instruction corpora. We welcome all researchers in the community to contribute to the corpus set and collaborate with us. We only release the first chip of COIG to help the Chinese LLMs' development in the exploration stage and appeal to more researchers joining us in building COIG. We separately introduce a manually verified translated general instruction corpus in \u00a7 3.1, a manually annotated exam instruction corpus in \u00a7 3.2, a human value alignment instruction corpus in \u00a7 3.3, a multi-round counterfactual correction chat corpus in \u00a7 3.3, and a leetcode instruction corpus in \u00a7 3.5. We provide these new instruction corpora to assist the community with instruction tuning on Chinese LLMs. These instruction corpora are also template workflows for how new Chinese instruction corpora can be built and expanded effectively.\n\n\nTranslation-based General Instruction Corpus\n\nTo enable the corpus for commercial and non-commercial use, we carefully select the core data of unnatural instructions (Honovich et al., 2022), the seed instruction set of self-instruct , and task descriptions of supernatural instructions ) as the English instruction source. These source instructions are not generated by any OpenAI API, and are therefore available for commercial and non-commercial use Honovich et al., 2022;Wang et al., 2022b). There are 67798 instructions in total, which are composed of 1616 task descriptions in  along with a single instance for each of them, 175 seed tasks in , and 66007 instructions from (Honovich et al., 2022).\n\nTo reduce the cost and further improve the quality of the instruction corpus, we separate the translation procedure into three phases: automatic translation, manual verification, and manual correction. First, during the automatic translation phase, we concatenate the instruction with the input and output of the instances and feed them into DeepL 6 for translation.\n\nSecond, during the manual verification phase, we define 4 labels for the annotators to select for each instruction. Each instruction is (i) directly usable; (ii) usable but with the source input and output of the instance; (iii) usable but with manual correction, and (iv) not usable. There are fewer than twenty cases where it is not usable. We adopt a two-phase quality verification for the manual verification phase: In the first phase, each case is verified by an industrial experienced quality inspector with more than 5 years of work experience after being annotated by the annotator. The entire corpus can be passed into the second quality verification phase if and only if the correctness rate exceeds 95%. The corpus got a 96.63% correctness rate in the first quality verification phase in the final. Our expert quality inspectors (namely, our coauthors) are in charge of the second quality verification phase and only randomly sample 200 cases from the total corpus for quality verification. If and only if all sampled cases are classified correctly, the corpus is able to be passed into the manual correction phase.\n\nThird, during the manual correction phase, the annotators are asked to correct translated instructions and instances into correct Chinese {instruction, input, output} triplets instead of just keeping the translation correct. The annotators are asked to do it because there exist factual errors in source unnatural instructions which might lead to LLMs' hallucinations. There are 18074 instructions fed into the manual correction phase in total. We use the same two-phase quality verification procedure as the manual verification phase. The corpus got a 97.24% correctness rate in the first quality verification phase of the manual correction phase.\n\nThese strict quality verification procedures assure the reliability of the translated corpus.\n\n\nExam Instructions\n\nThe Chinese National College Entrance Examination, Middle School Entrance Examinations, and Civil Servant Examination are the main Chinese commonsense tests. These exams contain various question formats and detailed analysis that can be used as the Chain-of-Thought (CoT) corpus. We use potato (Pei et al., 2022), an active learning powered open-source annotation website template, for manual annotation, which extracts six informative elements from original exam questions, including instruction, question context, question, answer, answer analysis, and coarse-grained subject. There are many reading comprehension questions in these exams, and the question context means the reading material of these reading comprehension questions. There are six main coarse-grained subjects: Chinese, English, Politics, Biology, History, and Geology. There are very few Math, Physics, and Chemistry questions in the corpus because these questions are often with complex symbols which are hard to annotate. We illustrate the question format percentage in Fig. 1   researchers utilize this corpus to further post-process it using prompts or post-process it to blank-filling questions to increase the instructions' diversity further.\n\n\nHuman Value Alignment Instructions\n\nMany existing human value alignment datasets can serve instruction tuning (Anthropic, 2022;Forbes et al., 2020;Emelin et al., 2021); however, these datasets are in English, and we find that simply translating them into Chinese cannot produce high-quality alignment data that matches the unique culture rooted in the Chinese-speaking world. For example, in Western English-speaking countries, people are often encouraged to move out when they hit adulthood, while in many Chinese-speaking communities or broader East Asian countries, it is acceptable or even encouraged for the youth to still live with and accompany their family even though they have grown up.\n\nTo respect and reflect this major difference caused by different cultural backgrounds, different from other tasks in COIG that leverage one unified collection of instruction-following samples, we categorize the value alignment data into two separate sets: 1) a set of samples that present shared human values in the Chinese-speaking world, and 2) some additional sets of samples that present regional-culture or country-specific human values. For the first shared set, we choose self-instruct  as the main method to augment a set of seed instruction-following samples. For the additional sets, to guarantee that the data genuinely reflect the local values, we mainly rely on web crawlers to collect the data in original forms.\n\nThe seed instructions for shared human values are manually picked from Chinese textbooks and exams on ethics education, since we believe most of the content in these materials has already considered the common ground of different communities (e.g., there are 56 minorities in China). We deliberately consider the following three principles when filtering the data:\n\n\u2022 It should present shared human values widely accepted in the Chinese-speaking world, rather than regional ones.\n\n\u2022 It should not include political propaganda or religious beliefs and should not be related to disputed claims.\n\n\u2022 It should not just explain proverbs or quotes, since they will likely be covered in the knowledge retrieval instructions-following data.\n\nIn total, we choose 50 instructions as the augmentation seeds, and produce 3k resulting instructions following samples for general-purpose value alignment in the Chinese-speaking world. Meanwhile, we also collect 19,470 samples as a regional addition, which is specific to users in China (including many terms that are only used in the Chinese community). See A.1 for examples.\n\n\nChinese Open Instruction Generalist: A Preliminary Release\n\n\nCounterfactual Correction Multi-round Chat\n\nLLMs have become ubiquitous in a variety of NLP applications. However, these models often generate responses that are not truthful, and in some cases can even propagate misinformation or hallucination. The models may falsely and repeatedly insist a claim with no sign of internal awareness that the claim was a product of their own imagination.\n\nTo mitigate above issues, and enhance the truthfulness of the model's responses, we build the Counterfactual Correction Multi-round Chat dataset (CCMC). It is constructed based on the CN-DBpedia knowledge base (Xu et al., 2017) with the aim of alleviating and resolving the pain points of hallucination and factual inconsistency in current LLMs. The original knowledge base consists of 5634k entities with their corresponding attribute-value pairs and original text.\n\nThe CCMC dataset includes 5 rounds of role-playing chat between a student and a teacher, and the corresponding knowledge they refer to. The teacher generates responses based on ground-truth knowledge and corrects factual errors or inconsistencies in the student's questions or statements in each round. In the final round, the teacher will summarize the chat and review the confusing terms, i.e. the factual errors or inconsistencies in the student's questions or statements. The dataset contains 13,653 dialogues and resulting in 68,265 rounds of chat. See A.2 for example.\n\nWe outline the workflow for creating the CCMC dataset. The workflow consists of three main parts: entity selection, information extraction, and chat generation.\n\nWe first perform entity selection by ranking entities based on entity tag frequency and choosing the top 200. We prioritize entities with summaries and aim to retain factual/knowledge-based content, such as well-established, historically tested concepts, and entities related to various academic disciplines, historical events, and social events. Tags like organizations, companies, foods, and games are excluded.\n\nNext, we extract information from the knowledge base using a chat LLM. We first obtain a source entity by randomly sampling an entity from the high-priority categories, returning triplets, content summaries, and content section titles. Then we ask a chat LLM to summarize all the information into a better summary and also extract attribute-value pairs from the input. This can filter out some of the false tags in the Baidu tags, and also take information in the unstructured content into consideration. For the confusion entity, we use a prompt-based method to extract a list of confusing terms based on the input information. Then we match the terms with the knowledge base. If the term exists in the base, we keep the term and use the same method to extract better summary and attribute-value pairs.\n\nWe employ a teacher-student question-and-answer approach for chat generation to generate attack and defense scenarios gradually. We provide the extracted original entity summary and confusing entity summary. Then, we let the student ask the teacher about the original concept while mistakenly mixing it up with the confusing one. The teacher would then clarify and differentiate the concepts in a JSON format. The conversation would continue for multiple rounds, each time with the student challenging the teacher based on previous dialogues, and the teacher providing clarifications and distinctions. In the final round, the teacher would reintroduce the original concept and summarize the concepts that were easily confused, emphasizing and differentiating the concepts the student had previously mixed up. All chats are generated by prompting a chat LLM. Given that the code-related tasks potentially contribute to the ability emergence of LLMs (Ouyang et al., 2022), we argue that code-related tasks aligned with the Chinese natural language should be considered in our datasets. Therefore, we build the Leetcode instructions from a CC-BY-SA-4.0 license collection 7 of 2, 589 programming questions. The questions contain problem descriptions, multiple programming languages, and explanations 8 .\n\n\nLeetcode Instructions\n\nWe categorize the instruction tasks into two classes considering the input and output: code-to-text and text-to-code. The code-to-text task requires producing function descriptions given programming codes, whereas the text-to-code task requires output codes from the question. Depending on whether the program question has a corresponding explanation, the task instruction will be distinguished by with/without explanation. We prepare 38 types of descriptions to generate the Leetcode instructions. We iterate through the available programming language implementations for each programming question, randomly sample the task as code-to-text or text-to-code, and then randomly select a corresponding instruction description.\n\nThe statistics of the Leetcode instructions are shown in Tab. 5. We derive 11, 737 code-related instructions in four types of tasks in more than 10 programming languages from the collection. The statistics show that the constructed dataset is diversified and may benefit LLM instruction tuning.\n\n\nEmpirical Validation of Instruction Corpora Construction Workflow\n\nThis section summarizes reasonable empirical conclusions and lessons about the Chinese instruction corpora construction workflow.\n\nFirst, adopting In-Context-Learning (ICL) for generating new instructions Honovich et al., 2022) is a key contributing factor when we want to expand the size of the instruction corpus. Taking the general-purpose instruction corpora (Yunjie et al., 2023;Taori et al., 2023) in Tab. 1 as examples, generating these instructions is more realistic using the ICL ability of existing LLMs instead of relying on manual annotation or other methods 9 . LLM developers should carefully decide which LLMs and seed instruction corpora they prefer based on the license of the source, the relationship of the source with OpenAI 10 , and their needs.\n\nSecond, human annotation or verification is needed when there is a cultural difference between the targeted language and the language of source instruction corpora. As in \u00a7 3.3, we must carefully select the seed in manual instruction to ensure that the seed instructions align well with Chinese culture and do not include political propaganda or regional beliefs. We also recommend using existing corpora, such as the method introduced in (Ethayarajh et al., 2023) when building human value alignment instructions, where one crawls corpus from forums and post-processes it to make it harmless.\n\nThird, model-generated corpora need more detailed manual quality verification, especially in cases where the output format is crucial. During the translation and verification procedure of the unnatural instructions (Honovich et al., 2022) explained in \u00a7 3.1, we notice many instances that do not follow the model-generated instructions and a considerable number of imperfect model-generated instructions. Another concern is that the diversity and distribution of modelgenerated instructions are highly dependent on seed instructions. Manual selection and verification may help sample an instruction corpus from a large raw instruction corpus with a more balanced distribution and better diversity than the large raw instruction corpus itself, as indicated in (Geng et al., 2023).\n\n\nConclusion and Discussion\n\nWe have described how we build the most comprehensive Chinese instruction dataset with careful human verification. Since our aim is to build a community based on the continuous update philosophy, this early-phase release is a solid foundation and momentum for future evolution and improvement. Note that \"early-phase\" does not imply that the current version is highly incomplete, but emphasizes that we commit to updating the corpora and welcome contributions from the community. Our next major release will focus on building better instructions for collective LLMs (Park et al., 2023;Guohao et al., 2023;Xu et al., 2023) and how to use them to improve the training of LLMs in return. 7 https://github.com/doocs/leetcode 8 834 questions do not have explanations. 9 GLM-130B (Zeng et al., 2023), T5 (Raffel et al., 2020), and various other LLMs are also capable of performing the ICL procedure needed to generate new instructions (Honovich et al., 2022;Wang et al., 2022a) but with a relatively higher rate of error compared to ChatGPT. 10 OpenAI does not allow the content generated from their applications to be used to improve model performance without permission. For details, see https://openai.com/policies/terms-of-use.\n\n\nChinese Open Instruction Generalist: A Preliminary Release\n\nIn addition to our efforts in constructing the instruction corpora, we also want to share our thoughts on several potential algorithmic improvements that warrant investigation. For example, it is well-known that not every training sample is equally useful when training machine learning models. Thus, it is reasonable to conjecture that some instructions may be more important than others when fine-tuning LLMs. In practice, we are always constrained by the limited resources for a specific training task. Fine-tuning LLMs with redundant instructions may give little benefit, and constructing meaningful and high-quality instructions is difficult and costly.\n\nFurthermore, inspired by the detrimental gradient interference phenomena studied in (Yu et al., 2020), we hypothesize that the diverse nature of instructions could make fine-tuning challenging as the gradients 11 obtained by various instructions might even conflict with each other, making the optimization process difficult to converge to good solutions. One solution is to adopt active learning approaches to proactively and iteratively construct the most informative instructions that benefit LLM the most, rather than creating them simultaneously and treating them equally important. This active learning-based approach requires a set of comprehensive evaluation and diagnosis toolkits to guide the active construction procedure of instructions. Additionally, humans learn much better and faster if the training samples are presented in a meaningful order which usually exposes the learners to gradually increasing complexity (Bengio et al., 2009). It makes sense to incorporate this curriculum learning principle  into the active learning framework (Jafarpour et al., 2021). Alternatively, after generating these instructions, we can learn to reweight the instructions within a mini-batch or across multiple mini-batches, using meta learning (Ren et al., 2018) to mitigate the issue of conflicting gradients.\n\nFigure 2 :\n2The percentage of instructions in different question formats.\n\nTable 2 :\n2Note that the number of tasks, task types, instructions, and samples are not equivalent to one another. Only the number of tasks and instructions are reported. If the instructional data was obtained from existing public datasets and the data processing pipeline is publicly available, it is considered open-sourced. The field \"Verified?\" refers to whether the data has been manually verified.Dataset \n# Tasks # Instructions Lan \nCollection \nMethod \n\nUsage \nAccess Verified? \n\nPromptSource (Bach et al., 2022) \n180 \n2,085 \nEnglish \nMixed \nInstruct. \nTuning \n\nOpen \nYes \n\nP3 (Sanh et al., 2021) \n270 \n2,073 \nEnglish \nMixed \nInstruct. \nTuning \n\nOpen \nYes \n\nxP3 (Muennighoff et al., 2022) \n83 \n-\nMultilingual Mixed \nInstruct. \nTuning \n\nOpen \nNo \n\nNatural Instruct v1 (Mishra et al., \n2022) \n\n61 \n61 \nEnglish \nExisting \nInstruct. \nTuning \n\nOpen \nYes \n\nSuper-Natural-Instruct v2 (Wang \net al., 2022b) \n\n1,616 \n1,616 \nMultilingual Mixed \nInstruct. \nTuning \n\nOpen \nYes \n\nCrossFit (Ye et al., 2021) \n160 \n-\nEnglish \nExisting \nInstruct. \nTuning \n\nOpen \nYes \n\nFLAN (Wei et al., 2021) 2021 \n62 \n620 \nEnglish \nExisting \nInstruct. \nTuning \n\nOpen \nYes \n\nExMix (Aribandi et al., 2021) \n107 \n107 \nEnglish \nExisting \nInstruct. \nTuning \n\nOpen \n-\n\nUnifiedSKG (Xie et al., 2022) \n21 \n21 \nEnglish \nExisting \nInstruct. \nTuning \n\nOpen \nYes \n\nMetaICL (Min et al., 2021) \n142 \n-\nEnglish \nExisting \nInstruct. \nTuning \n\nOpen \nYes \n\nInstructGPT (Ouyang et al., 2022) \n-\n112,801 \nEnglish \nHuman \nAnnotated \n\nRLHF, \nIn-\nstruct. \nTuning \n\nClosed Yes \n\nFLAN Collection 2022 (Chung \net al., 2022; Longpre et al., 2023) \n\n1,836 \n18,360 \nEnglish \nExisting \nInstruct. \nTuning \n\nOpen \nNo \n\nOPT-IML Bench (Iyer et al., 2022a) 1,667 \n3,128 \nEnglish \nExisting \nInstruct. \nTuning \n\nOpen \nYes \n\nGLM-130B (Zeng et al., 2023) \n74 \n-\nMultilingual Existing \nInstruct. \nTuning \n\nOpen \nYes \n\nSelf-Instruct (Wang et al., 2022a) \n175 \n52,445 \nEnglish \nModel \nGenerated \n\nInstruct. \nTuning \n\nOpen \nNo \n\nUnnatural Instructions (Honovich \net al., 2022) \n\n-\n240,000 \nEnglish \nModel \nGenerated \n\nInstruct. \nTuning \n\nOpen \nNo \n\nAlpaca (Taori et al., 2023) \n175 \n51,942 \nEnglish \nModel \nGenerated \n\nInstruct. \nTuning \n\nOpen \nNo \n\n\n\nTable 3 :\n3English Instruction Data (Continued from Table 2)Dataset \n# Tasks # Instructions Lan \nCollection \nMethod \n\nUsage \nAccess Human \nVeri-\nfied? \nOIG (AI, 2021) \n30 \n43M \nEnglish \nMixed \nInstruct. \nTuning \n\nOpen \nNo \n\nBaize (Xu et al., 2023) \n3 \n100K+ \nEnglish \nModel \nGenerated \n\nChat \nOpen \nNo \n\nCamel (Guohao et al., 2023) \n-\n115K \nEnglish \nModel \nGenerated \n\nInstruct. \nTuning, \nChat \n\nOpen \nNo \n\nUltraChat (Ding et al., 2023) \n-\n675K \nEnglish \nModel \nGenerated \n\nChat \nOpen \nNo \n\nDolly (Databricks, 2022) \n7 \n15,000 \nEnglish \nHuman \nAnnotated \n\nInstruct. \nTuning \n\nOpen \nYes \n\nGuanaco-Dataset (JosephusCheung, \n2021) \n\n175 \n534,530 \nMultilingual Mixed \nInstruct. \nTuning \n\nOpen \nNo \n\nChatLLaMA Chinese-ChatLLaMA \n(YDli-ai, 2021) \n\n-\n-\nMultilingual Mixed \nInstruct. \nTuning \n\nOpen \nNo \n\nGPT-4-LLM (Peng et al., 2023) \n175 \n165K \nMultilingual Model \nGenerated \n\nRLHF, \nInstruct. \nTuning \n\nOpen \nNo \n\nShareGPT (ShareGPT, 2021) \n-\n-\nMultilingual Model \nGenerated \n\nInstruct. \nTuning, \nChat \n\nClosed Yes \n\nSHP (Ethayarajh et al., 2023) \n18 \n385K \nEnglish \nExisting, \nHuman \nAnnotated \n\nRLHF, \nInstruct. \nTuning \n\nOpen \nYes \n\nHH-RLHF (Bai et al., 2022; An-\nthropic, 2022; Ganguli et al., 2022) \n\n-\n169,550 \nEnglish \nMixed \nRLHF, \nInstruct. \nTuning \n\nOpen \nYes \n\nHC3 (Guo et al., 2023) \n12 \n37,175 \nMultilingual Mixed \nInstruct. \nTuning \n\nOpen \nYes \n\nStack-Exchange-Preferences (Lam-\nbert et al., 2023) \n\n-\n10M \nEnglish \nExisting \nRLHF, \nInstruct. \nTuning \n\nOpen \nYes \n\nInstructWild (Xue et al., 2023) \n429 \n104K \nMultilingual Model \nGenerated \n\nInstruct. \nTuning \n\nOpen \nNo \n\n\n\nTable 4 :\n4Chinese Instruction Data (Continued fromTable 3)Dataset \n#Tasks \n# Instructions Language \nCollection \nMethod \n\nUsage \nAccess Human \nVerified? \nxP3 (Muennighoff \net al., 2022) \n\n83 \n-\nMultilingual \nMixed \nInstruct. \nTuning \n\nOpen \nNo \n\nSuper-Natural-\nInstructions \n(v2) (Wang et al., \n2022b) \n\n1,616 \n1,616 \nMultilingual \nMixed \nInstruct. \nTuning \n\nOpen \nYes \n\nZeroPrompt (Xu \net al., 2022) \n\n1,110 \n-\nChinese \nHuman \nAnnotated, \nExisting \nDataset \n\nInstruct. \nTuning \n\nClosed Yes \n\nGLM-130B (Zeng \net al., 2023) \n\n74 \n-\nMultilingual \n(eng, zh) \n\nExisting \nDataset \n\nInstruct. \nTuning \n\nOpen \nYes \n\npCLUE (CLUEbench-\nmark, 2021) \n\n9 \n73 \nChinese \nExisting \nDataset \n\nInstruct. \nTuning \n\nOpen \nYes \n\nBelle-1.5M (Yunjie \net al., 2023) \n\n175 \n1.5M \nChinese \nModel \nGenerated \n\nInstruct. \nTuning \n\nOpen \nNo \n\nGuanaco-Dataset \n(JosephusCheung, \n2021) \n\n175 \n534,530 \nMultilingual \nMixed \nInstruct. \nTuning \n\nOpen \nNo \n\nCSL (Li et al., \n2022) \n\n4 \n396,209 \nChinese \nExisting \nDataset \n\nInstruct. \nTuning \n\nOpen \nYes \n\nChinese-\nChatLLaMA \n(YDli-ai, 2021) \n\n-\n-\nMultilingual \nMixed \nInstruct. \nTuning \n\nOpen \nNo \n\nFirefly (Yang, 2023) 23 \n1.1M \nChinese \nMixed \nInstruct. \nTuning \n\nOpen \nYes \n\nLuotuo (Ziang Leng \nand Li, 2023) \n\n175 \n51,672 \nChinese \nMixed \nInstruct. \nTuning \n\nOpen \nNo \n\nChinese-Alpaca \n(Liu et al., 2023) \n\n-\n-\nChinese \nExisting \nDataset, \nHuman \nAnnotated \n\nInstruct. \nTuning \n\nOpen \nNo \n\nGPT-4-LLM (Peng \net al., 2023) \n\n175 \n165K \nMultilingual \n(eng, zh) \n\nModel \nGenerated \n\nRLHF, \nInstruct. \nTuning \n\nOpen \nNo \n\nShareGPT \n(ShareGPT, 2021) \n\n-\n-\nMultilingual \nModel \nGenerated \n\nInstruct. \nTuning, \nChat \n\nClosed Yes \n\nChinese-Vicuna \n(Chenghao Fan and \nTian, 2023) \n\n-\n1M \nChinese \nModel \nGenerated, \nExisting \nDataset \n\nInstruct. \nTuning \n\nOpen \nNo \n\nCUGE (Yao et al., \n2021) \n\n18 \n-\nChinese \nExisting \nDataset \n\nInstruct. \nTuning \n\nOpen \nYes \n\nHC3 (Guo et al., \n2023) \n\n12 \n37,175 \nMultilingual \n(eng, zh) \n\nMixed \nInstruct. \nTuning \n\nOpen \nYes \n\nInstructWild (Xue \net al., 2023) \n\n429 \n104K \nMultilingual \n(eng, zh) \n\nModel \nGenerated \n\nInstruct. \nTuning \n\nOpen \nNo \n\nOur Translated Cor-\npus \n\n2k \n67,798 \nChinese \nMixed \nInstruct. \nTuning \n\nOpen \nYes \n\n\n\nand the major subject percentage inFig. 2. For many choice questions, we recommend that theChinese Open Instruction Generalist: \nA Preliminary Release \n\nGeograhpy \n1.6% \n\nHistory \n39.6% \n\nBiology \n4.5% \n\nChinese \n15.2% \n\nEnglish \n9.7% \n\nPolitics \n29.4% \n\nFigure 1: The percentage of instructions of different coarse-grained subjects. \n\nBlank-Filling \n4.4% \n\nChoice \n63.1% \n\nMaterial Analysis \n19.5% \nTrue or False \n1.6% \nShort Answer \n0.9% \nReading \n10.5% \n\n\n\nTable 5 :\n5Statistics of Leetcode Instructions. Task types C2T and T2C refer to code-to-text and text-to-code, respectively. \nAnd e. is the abbreviation for \"explanation\". Programming languages with less than 50 instructions are merged into the \n\"Others\" class. \n\nProgramming \nLanguage \n\nTask Type \nC2T w/o e. C2T w/ e. T2C w/o e. T2C w/ e. All \n\nC \n8 \n76 \n12 \n89 \n185 \nC# \n8 \n56 \n6 \n58 \n128 \nC++ \n168 \n943 \n180 \n963 \n2254 \nGo \n175 \n1008 \n164 \n899 \n2246 \nJava \n213 \n989 \n193 \n983 \n2378 \nJavaScript \n16 \n172 \n29 \n153 \n370 \nPython3 \n198 \n995 \n208 \n981 \n2382 \nRust \n46 \n252 \n39 \n252 \n589 \nSQL \n35 \n6 \n30 \n5 \n76 \nTypeScript \n98 \n454 \n82 \n450 \n1084 \nOthers \n2 \n20 \n3 \n20 \n45 \n\nAll \n967 \n4971 \n946 \n4853 11737 \n\n8 \n\n\nhttps://www.deepl.com/translator\nIt becomes even more tricky when we pack these instructions into a mini-batch, usually constrained by the on-device memory capacity. One remedy is to design the corresponding gradient accumulation scheme with care.\nAcknowledgementWe would like to express our gratitude to the student volunteers from Zhejiang University, China University of Geosciences (Beijing), Beijing Language and Culture University, Zhengzhou University, Beijing University of Posts and Telecommunications, Beihang University, and Hangzhou Dianzi University for their manual annotation and quality control efforts. We also appreciate the non-profit annotation services provided by Stardust.ai in the construction of the translation dataset.A Appendix\nOig dataset. A I , L , Online; accessed 14AI, L. (2021). Oig dataset. https://laion.ai/blog/oig-dataset/. [Online; accessed 14-April-2023].\n\nHuggingface datasets: hh-rlhf. Anthropic, Online; accessed 14Anthropic (2022). Huggingface datasets: hh-rlhf. https://huggingface.co/datasets/Anthropic/ hh-rlhf/tree/main. [Online; accessed 14-April-2023].\n\nV Aribandi, Y Tay, T Schuster, J Rao, H Zheng, S V Mehta, H Zhuang, V Tran, D Bahri, J Ni, J Gupta, K Hui, S Ruder, D Metzler, Ext5: Towards extreme multi-task scaling for transfer learning. International Conference On Learning Representations. Aribandi, V., Tay, Y., Schuster, T., Rao, J., Zheng, H., Mehta, S. V., Zhuang, H., Tran, V., Bahri, D., Ni, J., Gupta, J., Hui, K., Ruder, S., and Metzler, D. (2021). Ext5: Towards extreme multi-task scaling for transfer learning. International Conference On Learning Representations.\n\nPromptsource: An integrated development environment and repository for natural language prompts. S H Bach, V Sanh, Z X Yong, A Webson, C Raffel, N V Nayak, A Sharma, T Kim, M S Bari, T F\u00e9vry, Z Alyafeai, M Dey, A Santilli, Z Sun, S Ben-David, C Xu, G Chhablani, H Wang, J A Fries, M S Al-Shaibani, S Sharma, U Thakker, K Almubarak, X Tang, M T Jiang, .-J Rush, A M , Annual Meeting Of The Association For Computational Linguistics. Bach, S. H., Sanh, V., Yong, Z. X., Webson, A., Raffel, C., Nayak, N. V., Sharma, A., Kim, T., Bari, M. S., F\u00e9vry, T., Alyafeai, Z., Dey, M., Santilli, A., Sun, Z., Ben-David, S., Xu, C., Chhablani, G., Wang, H., Fries, J. A., Al-shaibani, M. S., Sharma, S., Thakker, U., Almubarak, K., Tang, X., Jiang, M. T.-J., and Rush, A. M. (2022). Promptsource: An integrated development environment and repository for natural language prompts. Annual Meeting Of The Association For Computational Linguistics.\n\n. Y Bai, A Jones, K Ndousse, A Askell, A Chen, N Dassarma, D Drain, S Fort, D Ganguli, T Henighan, N Joseph, S Kadavath, J Kernion, T Conerly, S El-Showk, N Elhage, Z Hatfield-Dodds, D Hernandez, T Hume, S Johnston, S Kravec, L Lovitt, N Nanda, C Olsson, D Amodei, T B Brown, J Clark, S Mccandlish, C Olah, B Mann, J Kaplan, Training a helpful and harmless assistant with reinforcement learning from human feedback. ARXIV.ORGBai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli, D., Henighan, T., Joseph, N., Kadavath, S., Kernion, J., Conerly, T., El-Showk, S., Elhage, N., Hatfield-Dodds, Z., Hernandez, D., Hume, T., Johnston, S., Kravec, S., Lovitt, L., Nanda, N., Olsson, C., Amodei, D., Brown, T. B., Clark, J., McCandlish, S., Olah, C., Mann, B., and Kaplan, J. (2022). Training a helpful and harmless assistant with reinforcement learning from human feedback. ARXIV.ORG.\n\nCurriculum learning. Y Bengio, J Louradour, R Collobert, Weston , J , Proceedings of the 26th annual international conference on machine learning. the 26th annual international conference on machine learningBengio, Y., Louradour, J., Collobert, R., and Weston, J. (2009). Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 41-48.\n\nChinese-vicuna: A chinese instruction-following llama-based model. Z L Chenghao Fan, J Tian, Chenghao Fan, Z. L. and Tian, J. (2023). Chinese-vicuna: A chinese instruction-following llama-based model.\n\n. H W Chung, L Hou, S Longpre, B Zoph, Y Tay, W Fedus, E Li, X Wang, M Dehghani, S Brahma, A Webson, S Gu, Z Dai, M Suzgun, X Chen, A Chowdhery, D Valter, S Narang, G Mishra, A Yu, V Zhao, Chinese Open Instruction Generalist: A Preliminary ReleaseChung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., Webson, A., Gu, S., Dai, Z., Suzgun, M., Chen, X., Chowdhery, A., Valter, D., Narang, S., Mishra, G., Yu, A., Zhao, V., Chinese Open Instruction Generalist: A Preliminary Release\n\nScaling instruction-finetuned language models. Y Huang, A M Dai, H Yu, S Petrov, E Chi, J Dean, J Devlin, A Roberts, D Zhou, Q V Le, Wei , J , ARXIV.ORGHuang, Y., Dai, A. M., Yu, H., Petrov, S., Chi, E., Dean, J., Devlin, J., Roberts, A., Zhou, D., Le, Q. V., and Wei, J. (2022). Scaling instruction-finetuned language models. ARXIV.ORG.\n\npclue: A parallel training framework for chinese language understanding evaluation. Cluebenchmark, Online; accessed 14CLUEbenchmark (2021). pclue: A parallel training framework for chinese language understanding evaluation. https://github.com/CLUEbenchmark/pCLUE. [Online; accessed 14-April-2023].\n\nDolly: A high-performance deep learning library for large-scale nlp. Databricks, Online; accessed 14Databricks (2022). Dolly: A high-performance deep learning library for large-scale nlp. https://github.com/ databrickslabs/dolly. [Online; accessed 14-April-2023].\n\nUltrachat: A large-scale auto-generated multi-round dialogue data. N Ding, Y Chen, B Xu, S Hu, Y Qin, Z Liu, M Sun, B Zhou, Ding, N., Chen, Y., Xu, B., Hu, S., Qin, Y., Liu, Z., Sun, M., and Zhou, B. (2023). Ultrachat: A large-scale auto-generated multi-round dialogue data. https://github.com/thunlp/ultrachat.\n\nMoral stories: Situated reasoning about norms, intents, actions, and their consequences. D Emelin, R Le Bras, J D Hwang, M Forbes, Y Choi, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingOnline and Punta Cana, Dominican RepublicAssociation for Computational LinguisticsEmelin, D., Le Bras, R., Hwang, J. D., Forbes, M., and Choi, Y. (2021). Moral stories: Situated reasoning about norms, intents, actions, and their consequences. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 698-718, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\n. K Ethayarajh, H Zhang, Y Wang, Jurafsky , D , Stanford human preferences datasetEthayarajh, K., Zhang, H., Wang, Y., and Jurafsky, D. (2023). Stanford human preferences dataset.\n\nSocial chemistry 101: Learning to reason about social and moral norms. M Forbes, J D Hwang, V Shwartz, M Sap, Y Choi, arXiv:2011.00620arXiv preprintForbes, M., Hwang, J. D., Shwartz, V., Sap, M., and Choi, Y. (2020). Social chemistry 101: Learning to reason about social and moral norms. arXiv preprint arXiv:2011.00620.\n\n. D Ganguli, L Lovitt, J Kernion, A Askell, Y Bai, S Kadavath, B Mann, E Perez, N Schiefer, K Ndousse, A Jones, S Bowman, A Chen, T Conerly, N Dassarma, D Drain, N Elhage, S El-Showk, S Fort, Z Hatfield-Dodds, T Henighan, D Hernandez, T Hume, J Jacobson, S Johnston, S Kravec, C Olsson, S Ringer, E Tran-Johnson, D Amodei, T Brown, N Joseph, S Mccandlish, C Olah, J Kaplan, Clark , J , arXiv: Arxiv-2209.07858arXiv preprintRed teaming language models to reduce harms: Methods, scaling behaviors, and lessons learnedGanguli, D., Lovitt, L., Kernion, J., Askell, A., Bai, Y., Kadavath, S., Mann, B., Perez, E., Schiefer, N., Ndousse, K., Jones, A., Bowman, S., Chen, A., Conerly, T., DasSarma, N., Drain, D., Elhage, N., El-Showk, S., Fort, S., Hatfield-Dodds, Z., Henighan, T., Hernandez, D., Hume, T., Jacobson, J., Johnston, S., Kravec, S., Olsson, C., Ringer, S., Tran-Johnson, E., Amodei, D., Brown, T., Joseph, N., McCandlish, S., Olah, C., Kaplan, J., and Clark, J. (2022). Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. arXiv preprint arXiv: Arxiv-2209.07858.\n\nKoala: A dialogue model for academic research. X Geng, A Gudibande, H Liu, E Wallace, P Abbeel, S Levine, D Song, Blog postGeng, X., Gudibande, A., Liu, H., Wallace, E., Abbeel, P., Levine, S., and Song, D. (2023). Koala: A dialogue model for academic research. Blog post.\n\nHow close is chatgpt to human experts? comparison corpus, evaluation, and detection. B Guo, X Zhang, Z Wang, M Jiang, J Nie, Y Ding, J Yue, Y Wu, arxiv:2301.07597arXiv preprintGuo, B., Zhang, X., Wang, Z., Jiang, M., Nie, J., Ding, Y., Yue, J., and Wu, Y. (2023). How close is chatgpt to human experts? comparison corpus, evaluation, and detection. arXiv preprint arxiv:2301.07597.\n\nCamel: Communicative agents for \"mind\" exploration of large scale language model society. L Guohao, K H Hasan Abed Al, I Hani, K Dmitrii, B Ghanem, Guohao, L., Hasan Abed Al, K. H., Hani, I., Dmitrii, K., and Ghanem, B. (2023). Camel: Communicative agents for \"mind\" exploration of large scale language model society.\n\nUnnatural instructions: Tuning language models with (almost) no human labor. O Honovich, T Scialom, O Levy, T Schick, Honovich, O., Scialom, T., Levy, O., and Schick, T. (2022). Unnatural instructions: Tuning language models with (almost) no human labor.\n\nOpt-iml: Scaling language model instruction meta learning through the lens of generalization. S Iyer, X Lin, R Pasunuru, T Mihaylov, D Simig, P Yu, K Shuster, T Wang, Q Liu, P S Koura, X Li, B O&apos;horo, G Pereyra, J Wang, C Dewan, A Celikyilmaz, L Zettlemoyer, V Stoyanov, ARXIV.ORGIyer, S., Lin, X., Pasunuru, R., Mihaylov, T., Simig, D., Yu, P., Shuster, K., Wang, T., Liu, Q., Koura, P. S., Li, X., O'Horo, B., Pereyra, G., Wang, J., Dewan, C., Celikyilmaz, A., Zettlemoyer, L., and Stoyanov, V. (2022a). Opt-iml: Scaling language model instruction meta learning through the lens of generalization. ARXIV.ORG.\n\nOpt-iml: Scaling language model instruction meta learning through the lens of generalization. S Iyer, X V Lin, R Pasunuru, T Mihaylov, D Simig, P Yu, K Shuster, T Wang, Q Liu, P S Koura, arXiv:2212.12017arXiv preprintIyer, S., Lin, X. V., Pasunuru, R., Mihaylov, T., Simig, D., Yu, P., Shuster, K., Wang, T., Liu, Q., Koura, P. S., et al. (2022b). Opt-iml: Scaling language model instruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017.\n\nActive curriculum learning. B Jafarpour, D Sepehr, N Pogrebnyakov, Proceedings of the First Workshop on Interactive Learning for Natural Language Processing. the First Workshop on Interactive Learning for Natural Language ProcessingJafarpour, B., Sepehr, D., and Pogrebnyakov, N. (2021). Active curriculum learning. In Proceedings of the First Workshop on Interactive Learning for Natural Language Processing, pages 40-45.\n\n. Josephuscheung, Online; accessed 14JosephusCheung (2021). Guanacodataset. https://huggingface.co/datasets/JosephusCheung/ GuanacoDataset. [Online; accessed 14-April-2023].\n\nHuggingface h4 stack exchange preference dataset. N Lambert, L Tunstall, N Rajani, T Thrush, Lambert, N., Tunstall, L., Rajani, N., and Thrush, T. (2023). Huggingface h4 stack exchange preference dataset.\n\nCSL: A large-scale Chinese scientific literature dataset. Y Li, Y Zhang, Z Zhao, L Shen, W Liu, W Mao, H Zhang, Proceedings of the 29th International Conference on Computational Linguistics. the 29th International Conference on Computational LinguisticsGyeongju, Republic of KoreaInternational Committee on Computational LinguisticsLi, Y., Zhang, Y., Zhao, Z., Shen, L., Liu, W., Mao, W., and Zhang, H. (2022). CSL: A large-scale Chinese scientific literature dataset. In Proceedings of the 29th International Conference on Computational Linguistics, pages 3917- 3923, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.\n\nB Liu, K Huang, L Jiao, Y He, R Zhang, Y Liang, Wang , Y , Chinese alpaca dataset. Liu, B., Huang, K., Jiao, L., He, Y., Zhang, R., Liang, Y., and Wang, Y. (2023). Chinese alpaca dataset. https: //github.com/hikariming/alpaca_chinese_dataset.\n\nChinese Open Instruction Generalist: A Preliminary Release. Chinese Open Instruction Generalist: A Preliminary Release\n\nThe flan collection: Designing data and methods for effective instruction tuning. S Longpre, L Hou, T Vu, A Webson, H W Chung, Y Tay, D Zhou, Q V Le, B Zoph, J Wei, A Roberts, ARXIV.ORGLongpre, S., Hou, L., Vu, T., Webson, A., Chung, H. W., Tay, Y., Zhou, D., Le, Q. V., Zoph, B., Wei, J., and Roberts, A. (2023). The flan collection: Designing data and methods for effective instruction tuning. ARXIV.ORG.\n\nMetaicl: Learning to learn in context. S Min, M Lewis, L Zettlemoyer, H Hajishirzi, North American Chapter Of The Association For Computational LinguisticsMin, S., Lewis, M., Zettlemoyer, L., and Hajishirzi, H. (2021). Metaicl: Learning to learn in context. North American Chapter Of The Association For Computational Linguistics.\n\nCross-task generalization via natural language crowdsourcing instructions. S Mishra, D Khashabi, C Baral, H Hajishirzi, ACL. Mishra, S., Khashabi, D., Baral, C., and Hajishirzi, H. (2022). Cross-task generalization via natural language crowdsourcing instructions. In ACL.\n\nCrosslingual generalization through multitask finetuning. N Muennighoff, T Wang, L Sutawika, A Roberts, S Biderman, T L Scao, M S Bari, S Shen, Z.-X Yong, H Schoelkopf, X Tang, D Radev, A F Aji, K Almubarak, S Albanie, Z Alyafeai, A Webson, E Raff, C Raffel, Muennighoff, N., Wang, T., Sutawika, L., Roberts, A., Biderman, S., Scao, T. L., Bari, M. S., Shen, S., Yong, Z.-X., Schoelkopf, H., Tang, X., Radev, D., Aji, A. F., Almubarak, K., Albanie, S., Alyafeai, Z., Webson, A., Raff, E., and Raffel, C. (2022). Crosslingual generalization through multitask finetuning.\n\nTraining language models to follow instructions with human feedback. L Ouyang, J Wu, X Jiang, D Almeida, C Wainwright, P Mishkin, C Zhang, S Agarwal, K Slama, A Ray, Advances in Neural Information Processing Systems. 35Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. (2022). Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730-27744.\n\nJ S Park, J C O&apos;brien, C J Cai, M R Morris, P Liang, M S Bernstein, arXiv:2304.03442Generative agents: Interactive simulacra of human behavior. arXiv preprintPark, J. S., O'Brien, J. C., Cai, C. J., Morris, M. R., Liang, P., and Bernstein, M. S. (2023). Generative agents: Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442.\n\nPotato: The portable text annotation tool. J Pei, A Ananthasubramaniam, X Wang, N Zhou, A Dedeloudis, J Sargent, D Jurgens, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. the 2022 Conference on Empirical Methods in Natural Language Processing: System DemonstrationsPei, J., Ananthasubramaniam, A., Wang, X., Zhou, N., Dedeloudis, A., Sargent, J., and Jurgens, D. (2022). Potato: The portable text annotation tool. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations.\n\nB Peng, C Li, P He, M Galley, J Gao, arXiv:2304.03277Instruction tuning with gpt-4. arXiv preprintPeng, B., Li, C., He, P., Galley, M., and Gao, J. (2023). Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. C Raffel, N Shazeer, A Roberts, K Lee, S Narang, M Matena, Y Zhou, W Li, P J Liu, The Journal of Machine Learning Research. 211Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551.\n\nLearning to reweight examples for robust deep learning. M Ren, W Zeng, B Yang, R Urtasun, PMLRInternational conference on machine learning. Ren, M., Zeng, W., Yang, B., and Urtasun, R. (2018). Learning to reweight examples for robust deep learning. In International conference on machine learning, pages 4334-4343. PMLR.\n\n. V Sanh, A Webson, C Raffel, S H Bach, L Sutawika, Z Alyafeai, A Chaffin, A Stiegler, T L Scao, A Raja, M Dey, M S Bari, C Xu, U Thakker, S S Sharma, E Szczechla, T Kim, G Chhablani, N Nayak, D Datta, J Chang, M T Jiang, .-J Wang, H Manica, M Shen, S Yong, Z X Pandey, H Bawden, R Wang, T Neeraj, T Rozen, J Sharma, A Santilli, A Fevry, T Fries, J A Teehan, R Biderman, S Gao, L Bers, T Wolf, T Rush, A M , Multitask prompted training enables zero-shot task generalizationSanh, V., Webson, A., Raffel, C., Bach, S. H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Scao, T. L., Raja, A., Dey, M., Bari, M. S., Xu, C., Thakker, U., Sharma, S. S., Szczechla, E., Kim, T., Chhablani, G., Nayak, N., Datta, D., Chang, J., Jiang, M. T.-J., Wang, H., Manica, M., Shen, S., Yong, Z. X., Pandey, H., Bawden, R., Wang, T., Neeraj, T., Rozen, J., Sharma, A., Santilli, A., Fevry, T., Fries, J. A., Teehan, R., Biderman, S., Gao, L., Bers, T., Wolf, T., and Rush, A. M. (2021). Multitask prompted training enables zero-shot task generalization.\n\n. Sharegpt, 14ShareGPT (2021). Sharegpt. https://sharegpt.com/. [Online; accessed 14-April-2023].\n\nStanford alpaca: An instruction-following llama model. R Taori, I Gulrajani, T Zhang, Y Dubois, X Li, C Guestrin, P Liang, T B Hashimoto, Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T. B. (2023). Stanford alpaca: An instruction-following llama model. https://github.com/tatsu-lab/stanford_alpaca.\n\nA survey on curriculum learning. X Wang, Y Chen, W Zhu, IEEE Transactions on Pattern Analysis and Machine Intelligence. 449Wang, X., Chen, Y., and Zhu, W. (2021). A survey on curriculum learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(9):4555-4576.\n\nSelf-instruct: Aligning language model with self generated instructions. Y Wang, Y Kordi, S Mishra, A Liu, N A Smith, D Khashabi, H Hajishirzi, Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. (2022a). Self-instruct: Aligning language model with self generated instructions.\n\nSuper-naturalinstructions:generalization via declarative instructions on 1600+ tasks. Y Wang, S Mishra, P Alipoormolabashi, Y Kordi, A Mirzaei, A Arunkumar, A Ashok, A S Dhanasekaran, A Naik, D Stap, EMNLP. Wang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., Mirzaei, A., Arunkumar, A., Ashok, A., Dhanasekaran, A. S., Naik, A., Stap, D., et al. (2022b). Super-naturalinstructions:generalization via declarative instructions on 1600+ tasks. In EMNLP.\n\nFinetuned language models are zero-shot learners. J Wei, M Bosma, V Zhao, K Guu, A W Yu, B Lester, N Du, A M Dai, Q V Le, International Conference on Learning Representations. Wei, J., Bosma, M., Zhao, V., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. (2021). Finetuned language models are zero-shot learners. In International Conference on Learning Representations.\n\nUnifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models. T Xie, C H Wu, P Shi, R Zhong, T Scholak, M Yasunaga, C Wu, M Zhong, P Yin, S I Wang, V Zhong, B Wang, C Li, C Boyle, A Ni, Z Yao, D Radev, C Xiong, L Kong, R Zhang, N A Smith, L Zettlemoyer, Yu , T , Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Goldberg, Y., Kozareva, Z., and Zhang, Y.the 2022 Conference on Empirical Methods in Natural Language ProcessingAbu Dhabi; United Arab EmiratesAssociation for Computational Linguistics2022Xie, T., Wu, C. H., Shi, P., Zhong, R., Scholak, T., Yasunaga, M., Wu, C., Zhong, M., Yin, P., Wang, S. I., Zhong, V., Wang, B., Li, C., Boyle, C., Ni, A., Yao, Z., Radev, D., Xiong, C., Kong, L., Zhang, R., Smith, N. A., Zettlemoyer, L., and Yu, T. (2022). Unifiedskg: Unifying and multi-tasking structured knowledge grounding with text-to-text language models. In Goldberg, Y., Kozareva, Z., and Zhang, Y., editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 602-631. Association for Computational Linguistics.\n\nChinese Open Instruction Generalist: A Preliminary Release. Chinese Open Instruction Generalist: A Preliminary Release\n\nCn-dbpedia: A never-ending chinese knowledge extraction system. B Xu, Y Xu, J Liang, C Xie, B Liang, W Cui, Xiao , Y , Advances in Artificial Intelligence: From Theory to Practice: 30th International Conference on Industrial Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2017. Arras, FranceSpringerXu, B., Xu, Y., Liang, J., Xie, C., Liang, B., Cui, W., and Xiao, Y. (2017). Cn-dbpedia: A never-ending chinese knowledge extraction system. In Advances in Artificial Intelligence: From Theory to Practice: 30th International Conference on Industrial Engineering and Other Applications of Applied Intelligent Systems, IEA/AIE 2017, Arras, France, June 27-30, 2017, Proceedings, Part II, pages 428-438. Springer.\n\nBaize: An open-source chat model with parameter-efficient tuning on self-chat data. C Xu, D Guo, N Duan, J Mcauley, arXiv: Arxiv-2304.01196arXiv preprintXu, C., Guo, D., Duan, N., and McAuley, J. (2023). Baize: An open-source chat model with parameter-efficient tuning on self-chat data. arXiv preprint arXiv: Arxiv-2304.01196.\n\nZeroPrompt: Scaling prompt-based pretraining to 1,000 tasks improves zero-shot generalization. H Xu, Y Chen, Y Du, N Shao, W Yanggang, H Li, Yang , Z , Findings of the Association for Computational Linguistics: EMNLP 2022. Abu Dhabi, United Arab EmiratesAssociation for Computational LinguisticsXu, H., Chen, Y., Du, Y., Shao, N., Yanggang, W., Li, H., and Yang, Z. (2022). ZeroPrompt: Scaling prompt-based pretraining to 1,000 tasks improves zero-shot generalization. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 4235-4252, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\n\nInstruction in the wild: A user-based instruction dataset. F Xue, Z Zheng, Y You, Xue, F., Zheng, Z., and You, Y. (2023). Instruction in the wild: A user-based instruction dataset. https://github. com/XueFuzhao/InstructionWild.\n\n. J Yang, Yang, J. (2023). Firefly. https://github.com/yangjianxin1/Firefly.\n\n. Y Yao, Q Dong, J Guan, B Cao, Z Zhang, C Xiao, X Wang, F Qi, J Bao, J Nie, Z Zeng, Y Gu, K Zhou, X Huang, W Li, S Ren, J Lu, C Xu, H Wang, G Zeng, Z Zhou, J Zhang, J Li, M Huang, R Yan, X He, X Wan, X Zhao, X Sun, Y Liu, Z Liu, X Han, E Yang, Z Sui, M Sun, Cuge: A chinese language understanding and generation evaluation benchmark. ARXIV.ORGYao, Y., Dong, Q., Guan, J., Cao, B., Zhang, Z., Xiao, C., Wang, X., Qi, F., Bao, J., Nie, J., Zeng, Z., Gu, Y., Zhou, K., Huang, X., Li, W., Ren, S., Lu, J., Xu, C., Wang, H., Zeng, G., Zhou, Z., Zhang, J., Li, J., Huang, M., Yan, R., He, X., Wan, X., Zhao, X., Sun, X., Liu, Y., Liu, Z., Han, X., Yang, E., Sui, Z., and Sun, M. (2021). Cuge: A chinese language understanding and generation evaluation benchmark. ARXIV.ORG.\n\nChinese-chatllama. Ydli-Ai, Online; accessed. 14YDli-ai (2021). Chinese-chatllama. https://github.com/ydli-ai/Chinese-ChatLLaMA. [Online; ac- cessed 14-April-2023].\n\nCrossFit: A few-shot learning challenge for cross-task generalization in NLP. Q Ye, B Y Lin, X Ren, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. the 2021 Conference on Empirical Methods in Natural Language ProcessingOnline and Punta Cana, Dominican RepublicAssociation for Computational LinguisticsYe, Q., Lin, B. Y., and Ren, X. (2021). CrossFit: A few-shot learning challenge for cross-task generalization in NLP. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7163-7189, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\n\nGradient surgery for multi-task learning. T Yu, S Kumar, A Gupta, S Levine, K Hausman, C Finn, Advances in Neural Information Processing Systems. 33Yu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., and Finn, C. (2020). Gradient surgery for multi-task learning. Advances in Neural Information Processing Systems, 33:5824-5836.\n\nBelle: Be everyone's large language model engine. J Yunjie, D Yong, G Yan, P Yiping, N Qiang, M Baochang, L Xiangang, Yunjie, J., Yong, D., Yan, G., Yiping, P., Qiang, N., Baochang, M., and Xiangang, L. (2023). Belle: Be everyone's large language model engine. https://github.com/LianjiaTech/BELLE.\n\nGLM-130b: An open bilingual pre-trained model. A Zeng, X Liu, Z Du, Z Wang, H Lai, M Ding, Z Yang, Y Xu, W Zheng, X Xia, W L Tam, Z Ma, Y Xue, J Zhai, W Chen, Z Liu, P Zhang, Y Dong, J Tang, The Eleventh International Conference on Learning Representations. ICLRZeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M., Yang, Z., Xu, Y., Zheng, W., Xia, X., Tam, W. L., Ma, Z., Xue, Y., Zhai, J., Chen, W., Liu, Z., Zhang, P., Dong, Y., and Tang, J. (2023). GLM-130b: An open bilingual pre-trained model. In The Eleventh International Conference on Learning Representations (ICLR).\n\nLuotuo: An instruction-following chinese language model, lora tuning on llama. Q C Ziang Leng, C Li, Ziang Leng, Q. C. and Li, C. (2023). Luotuo: An instruction-following chinese language model, lora tuning on llama. https://github.com/LC1332/Chinese-alpaca-lora.\n", "annotations": {"author": "[{\"end\":178,\"start\":62},{\"end\":239,\"start\":179},{\"end\":326,\"start\":240},{\"end\":421,\"start\":327},{\"end\":494,\"start\":422},{\"end\":556,\"start\":495},{\"end\":614,\"start\":557},{\"end\":703,\"start\":615},{\"end\":791,\"start\":704},{\"end\":918,\"start\":792},{\"end\":982,\"start\":919},{\"end\":985,\"start\":983},{\"end\":1061,\"start\":986}]", "publisher": null, "author_last_name": "[{\"end\":70,\"start\":65},{\"end\":188,\"start\":185},{\"end\":249,\"start\":246},{\"end\":338,\"start\":334},{\"end\":430,\"start\":428},{\"end\":505,\"start\":501},{\"end\":563,\"start\":560},{\"end\":625,\"start\":623},{\"end\":714,\"start\":710},{\"end\":804,\"start\":801},{\"end\":931,\"start\":926},{\"end\":992,\"start\":990}]", "author_first_name": "[{\"end\":64,\"start\":62},{\"end\":184,\"start\":179},{\"end\":245,\"start\":240},{\"end\":333,\"start\":327},{\"end\":427,\"start\":422},{\"end\":500,\"start\":495},{\"end\":559,\"start\":557},{\"end\":622,\"start\":615},{\"end\":709,\"start\":704},{\"end\":800,\"start\":792},{\"end\":925,\"start\":919},{\"end\":984,\"start\":983},{\"end\":989,\"start\":986}]", "author_affiliation": "[{\"end\":139,\"start\":91},{\"end\":177,\"start\":141},{\"end\":238,\"start\":190},{\"end\":299,\"start\":251},{\"end\":325,\"start\":301},{\"end\":388,\"start\":340},{\"end\":420,\"start\":390},{\"end\":493,\"start\":432},{\"end\":555,\"start\":507},{\"end\":613,\"start\":565},{\"end\":675,\"start\":627},{\"end\":702,\"start\":677},{\"end\":764,\"start\":716},{\"end\":790,\"start\":766},{\"end\":854,\"start\":806},{\"end\":917,\"start\":856},{\"end\":981,\"start\":933},{\"end\":1060,\"start\":1012}]", "title": "[{\"end\":59,\"start\":1},{\"end\":1120,\"start\":1062}]", "venue": null, "abstract": "[{\"end\":2362,\"start\":1122}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b44\"},\"end\":2504,\"start\":2487},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3065,\"start\":3043},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3141,\"start\":3121},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":4137,\"start\":4116},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":4153,\"start\":4137},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4181,\"start\":4153},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":4524,\"start\":4504},{\"end\":4809,\"start\":4769},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":5287,\"start\":5262},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":5307,\"start\":5287},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":5324,\"start\":5307},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5345,\"start\":5324},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5521,\"start\":5499},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":5540,\"start\":5521},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":5709,\"start\":5690},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":5720,\"start\":5709},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5737,\"start\":5720},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8497,\"start\":8476},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8516,\"start\":8497},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8533,\"start\":8516},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8596,\"start\":8577},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9401,\"start\":9378},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9794,\"start\":9772},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10811,\"start\":10790},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":10827,\"start\":10811},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10845,\"start\":10827},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10867,\"start\":10847},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":11030,\"start\":11014},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":11492,\"start\":11471},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11651,\"start\":11631},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11668,\"start\":11651},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13095,\"start\":13072},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13380,\"start\":13358},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":13399,\"start\":13380},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":13607,\"start\":13584},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":16183,\"start\":16165},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":17219,\"start\":17202},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":17239,\"start\":17219},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":17259,\"start\":17239},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":20310,\"start\":20293},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":23478,\"start\":23457},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25151,\"start\":25129},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":25308,\"start\":25287},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":25327,\"start\":25308},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":26156,\"start\":26131},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":26525,\"start\":26502},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":27065,\"start\":27046},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":27681,\"start\":27662},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":27701,\"start\":27681},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":27717,\"start\":27701},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":27889,\"start\":27870},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":27915,\"start\":27894},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":28048,\"start\":28025},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":28067,\"start\":28048},{\"end\":28134,\"start\":28132},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":29145,\"start\":29128},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":29995,\"start\":29974},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":30122,\"start\":30098},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":30309,\"start\":30291}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30432,\"start\":30358},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":32617,\"start\":30433},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":34200,\"start\":32618},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":36384,\"start\":34201},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":36845,\"start\":36385},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":37557,\"start\":36846}]", "paragraph": "[{\"end\":2953,\"start\":2378},{\"end\":4625,\"start\":2955},{\"end\":4669,\"start\":4627},{\"end\":4905,\"start\":4671},{\"end\":5157,\"start\":4907},{\"end\":5738,\"start\":5159},{\"end\":6546,\"start\":5740},{\"end\":6942,\"start\":6548},{\"end\":6985,\"start\":6944},{\"end\":7231,\"start\":6987},{\"end\":7608,\"start\":7233},{\"end\":8342,\"start\":7610},{\"end\":9158,\"start\":8375},{\"end\":10620,\"start\":9160},{\"end\":11168,\"start\":10622},{\"end\":11855,\"start\":11170},{\"end\":12903,\"start\":11901},{\"end\":13608,\"start\":12952},{\"end\":13976,\"start\":13610},{\"end\":15104,\"start\":13978},{\"end\":15754,\"start\":15106},{\"end\":15849,\"start\":15756},{\"end\":17089,\"start\":15871},{\"end\":17788,\"start\":17128},{\"end\":18516,\"start\":17790},{\"end\":18882,\"start\":18518},{\"end\":18997,\"start\":18884},{\"end\":19110,\"start\":18999},{\"end\":19250,\"start\":19112},{\"end\":19629,\"start\":19252},{\"end\":20081,\"start\":19737},{\"end\":20549,\"start\":20083},{\"end\":21125,\"start\":20551},{\"end\":21287,\"start\":21127},{\"end\":21702,\"start\":21289},{\"end\":22507,\"start\":21704},{\"end\":23809,\"start\":22509},{\"end\":24558,\"start\":23835},{\"end\":24854,\"start\":24560},{\"end\":25053,\"start\":24924},{\"end\":25690,\"start\":25055},{\"end\":26285,\"start\":25692},{\"end\":27066,\"start\":26287},{\"end\":28321,\"start\":27096},{\"end\":29042,\"start\":28384},{\"end\":30357,\"start\":29044}]", "formula": null, "table_ref": "[{\"end\":3668,\"start\":3661}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2376,\"start\":2364},{\"attributes\":{\"n\":\"2\"},\"end\":8373,\"start\":8345},{\"attributes\":{\"n\":\"3\"},\"end\":11899,\"start\":11858},{\"attributes\":{\"n\":\"3.1\"},\"end\":12950,\"start\":12906},{\"attributes\":{\"n\":\"3.2\"},\"end\":15869,\"start\":15852},{\"attributes\":{\"n\":\"3.3\"},\"end\":17126,\"start\":17092},{\"end\":19690,\"start\":19632},{\"attributes\":{\"n\":\"3.4\"},\"end\":19735,\"start\":19693},{\"attributes\":{\"n\":\"3.5\"},\"end\":23833,\"start\":23812},{\"attributes\":{\"n\":\"3.6\"},\"end\":24922,\"start\":24857},{\"attributes\":{\"n\":\"4\"},\"end\":27094,\"start\":27069},{\"end\":28382,\"start\":28324},{\"end\":30369,\"start\":30359},{\"end\":30443,\"start\":30434},{\"end\":32628,\"start\":32619},{\"end\":34211,\"start\":34202},{\"end\":36856,\"start\":36847}]", "table": "[{\"end\":32617,\"start\":30837},{\"end\":34200,\"start\":32679},{\"end\":36384,\"start\":34261},{\"end\":36845,\"start\":36478},{\"end\":37557,\"start\":36858}]", "figure_caption": "[{\"end\":30432,\"start\":30371},{\"end\":30837,\"start\":30445},{\"end\":32679,\"start\":32630},{\"end\":34261,\"start\":34213},{\"end\":36478,\"start\":36387}]", "figure_ref": "[{\"end\":16919,\"start\":16913}]", "bib_author_first_name": "[{\"end\":38328,\"start\":38327},{\"end\":38330,\"start\":38329},{\"end\":38334,\"start\":38333},{\"end\":38663,\"start\":38662},{\"end\":38675,\"start\":38674},{\"end\":38682,\"start\":38681},{\"end\":38694,\"start\":38693},{\"end\":38701,\"start\":38700},{\"end\":38710,\"start\":38709},{\"end\":38712,\"start\":38711},{\"end\":38721,\"start\":38720},{\"end\":38731,\"start\":38730},{\"end\":38739,\"start\":38738},{\"end\":38748,\"start\":38747},{\"end\":38754,\"start\":38753},{\"end\":38763,\"start\":38762},{\"end\":38770,\"start\":38769},{\"end\":38779,\"start\":38778},{\"end\":39291,\"start\":39290},{\"end\":39293,\"start\":39292},{\"end\":39301,\"start\":39300},{\"end\":39309,\"start\":39308},{\"end\":39311,\"start\":39310},{\"end\":39319,\"start\":39318},{\"end\":39329,\"start\":39328},{\"end\":39339,\"start\":39338},{\"end\":39341,\"start\":39340},{\"end\":39350,\"start\":39349},{\"end\":39360,\"start\":39359},{\"end\":39367,\"start\":39366},{\"end\":39369,\"start\":39368},{\"end\":39377,\"start\":39376},{\"end\":39386,\"start\":39385},{\"end\":39398,\"start\":39397},{\"end\":39405,\"start\":39404},{\"end\":39417,\"start\":39416},{\"end\":39424,\"start\":39423},{\"end\":39437,\"start\":39436},{\"end\":39443,\"start\":39442},{\"end\":39456,\"start\":39455},{\"end\":39464,\"start\":39463},{\"end\":39466,\"start\":39465},{\"end\":39475,\"start\":39474},{\"end\":39477,\"start\":39476},{\"end\":39492,\"start\":39491},{\"end\":39502,\"start\":39501},{\"end\":39513,\"start\":39512},{\"end\":39526,\"start\":39525},{\"end\":39534,\"start\":39533},{\"end\":39536,\"start\":39535},{\"end\":39547,\"start\":39544},{\"end\":39555,\"start\":39554},{\"end\":39557,\"start\":39556},{\"end\":40129,\"start\":40128},{\"end\":40136,\"start\":40135},{\"end\":40145,\"start\":40144},{\"end\":40156,\"start\":40155},{\"end\":40166,\"start\":40165},{\"end\":40174,\"start\":40173},{\"end\":40186,\"start\":40185},{\"end\":40195,\"start\":40194},{\"end\":40203,\"start\":40202},{\"end\":40214,\"start\":40213},{\"end\":40226,\"start\":40225},{\"end\":40236,\"start\":40235},{\"end\":40248,\"start\":40247},{\"end\":40259,\"start\":40258},{\"end\":40270,\"start\":40269},{\"end\":40282,\"start\":40281},{\"end\":40292,\"start\":40291},{\"end\":40310,\"start\":40309},{\"end\":40323,\"start\":40322},{\"end\":40331,\"start\":40330},{\"end\":40343,\"start\":40342},{\"end\":40353,\"start\":40352},{\"end\":40363,\"start\":40362},{\"end\":40372,\"start\":40371},{\"end\":40382,\"start\":40381},{\"end\":40392,\"start\":40391},{\"end\":40394,\"start\":40393},{\"end\":40403,\"start\":40402},{\"end\":40412,\"start\":40411},{\"end\":40426,\"start\":40425},{\"end\":40434,\"start\":40433},{\"end\":40442,\"start\":40441},{\"end\":41073,\"start\":41072},{\"end\":41083,\"start\":41082},{\"end\":41096,\"start\":41095},{\"end\":41114,\"start\":41108},{\"end\":41118,\"start\":41117},{\"end\":41506,\"start\":41505},{\"end\":41508,\"start\":41507},{\"end\":41524,\"start\":41523},{\"end\":41643,\"start\":41642},{\"end\":41645,\"start\":41644},{\"end\":41654,\"start\":41653},{\"end\":41661,\"start\":41660},{\"end\":41672,\"start\":41671},{\"end\":41680,\"start\":41679},{\"end\":41687,\"start\":41686},{\"end\":41696,\"start\":41695},{\"end\":41702,\"start\":41701},{\"end\":41710,\"start\":41709},{\"end\":41722,\"start\":41721},{\"end\":41732,\"start\":41731},{\"end\":41742,\"start\":41741},{\"end\":41748,\"start\":41747},{\"end\":41755,\"start\":41754},{\"end\":41765,\"start\":41764},{\"end\":41773,\"start\":41772},{\"end\":41786,\"start\":41785},{\"end\":41796,\"start\":41795},{\"end\":41806,\"start\":41805},{\"end\":41816,\"start\":41815},{\"end\":41822,\"start\":41821},{\"end\":42225,\"start\":42224},{\"end\":42234,\"start\":42233},{\"end\":42236,\"start\":42235},{\"end\":42243,\"start\":42242},{\"end\":42249,\"start\":42248},{\"end\":42259,\"start\":42258},{\"end\":42266,\"start\":42265},{\"end\":42274,\"start\":42273},{\"end\":42284,\"start\":42283},{\"end\":42295,\"start\":42294},{\"end\":42303,\"start\":42302},{\"end\":42305,\"start\":42304},{\"end\":42313,\"start\":42310},{\"end\":42317,\"start\":42316},{\"end\":43148,\"start\":43147},{\"end\":43156,\"start\":43155},{\"end\":43164,\"start\":43163},{\"end\":43170,\"start\":43169},{\"end\":43176,\"start\":43175},{\"end\":43183,\"start\":43182},{\"end\":43190,\"start\":43189},{\"end\":43197,\"start\":43196},{\"end\":43483,\"start\":43482},{\"end\":43493,\"start\":43492},{\"end\":43504,\"start\":43503},{\"end\":43506,\"start\":43505},{\"end\":43515,\"start\":43514},{\"end\":43525,\"start\":43524},{\"end\":44130,\"start\":44129},{\"end\":44144,\"start\":44143},{\"end\":44153,\"start\":44152},{\"end\":44168,\"start\":44160},{\"end\":44172,\"start\":44171},{\"end\":44380,\"start\":44379},{\"end\":44390,\"start\":44389},{\"end\":44392,\"start\":44391},{\"end\":44401,\"start\":44400},{\"end\":44412,\"start\":44411},{\"end\":44419,\"start\":44418},{\"end\":44633,\"start\":44632},{\"end\":44644,\"start\":44643},{\"end\":44654,\"start\":44653},{\"end\":44665,\"start\":44664},{\"end\":44675,\"start\":44674},{\"end\":44682,\"start\":44681},{\"end\":44694,\"start\":44693},{\"end\":44702,\"start\":44701},{\"end\":44711,\"start\":44710},{\"end\":44723,\"start\":44722},{\"end\":44734,\"start\":44733},{\"end\":44743,\"start\":44742},{\"end\":44753,\"start\":44752},{\"end\":44761,\"start\":44760},{\"end\":44772,\"start\":44771},{\"end\":44784,\"start\":44783},{\"end\":44793,\"start\":44792},{\"end\":44803,\"start\":44802},{\"end\":44815,\"start\":44814},{\"end\":44823,\"start\":44822},{\"end\":44841,\"start\":44840},{\"end\":44853,\"start\":44852},{\"end\":44866,\"start\":44865},{\"end\":44874,\"start\":44873},{\"end\":44886,\"start\":44885},{\"end\":44898,\"start\":44897},{\"end\":44908,\"start\":44907},{\"end\":44918,\"start\":44917},{\"end\":44928,\"start\":44927},{\"end\":44944,\"start\":44943},{\"end\":44954,\"start\":44953},{\"end\":44963,\"start\":44962},{\"end\":44973,\"start\":44972},{\"end\":44987,\"start\":44986},{\"end\":44995,\"start\":44994},{\"end\":45009,\"start\":45004},{\"end\":45013,\"start\":45012},{\"end\":45792,\"start\":45791},{\"end\":45800,\"start\":45799},{\"end\":45813,\"start\":45812},{\"end\":45820,\"start\":45819},{\"end\":45831,\"start\":45830},{\"end\":45841,\"start\":45840},{\"end\":45851,\"start\":45850},{\"end\":46104,\"start\":46103},{\"end\":46111,\"start\":46110},{\"end\":46120,\"start\":46119},{\"end\":46128,\"start\":46127},{\"end\":46137,\"start\":46136},{\"end\":46144,\"start\":46143},{\"end\":46152,\"start\":46151},{\"end\":46159,\"start\":46158},{\"end\":46492,\"start\":46491},{\"end\":46502,\"start\":46501},{\"end\":46504,\"start\":46503},{\"end\":46521,\"start\":46520},{\"end\":46529,\"start\":46528},{\"end\":46540,\"start\":46539},{\"end\":46798,\"start\":46797},{\"end\":46810,\"start\":46809},{\"end\":46821,\"start\":46820},{\"end\":46829,\"start\":46828},{\"end\":47071,\"start\":47070},{\"end\":47079,\"start\":47078},{\"end\":47086,\"start\":47085},{\"end\":47098,\"start\":47097},{\"end\":47110,\"start\":47109},{\"end\":47119,\"start\":47118},{\"end\":47125,\"start\":47124},{\"end\":47136,\"start\":47135},{\"end\":47144,\"start\":47143},{\"end\":47151,\"start\":47150},{\"end\":47153,\"start\":47152},{\"end\":47162,\"start\":47161},{\"end\":47168,\"start\":47167},{\"end\":47183,\"start\":47182},{\"end\":47194,\"start\":47193},{\"end\":47202,\"start\":47201},{\"end\":47211,\"start\":47210},{\"end\":47226,\"start\":47225},{\"end\":47241,\"start\":47240},{\"end\":47688,\"start\":47687},{\"end\":47696,\"start\":47695},{\"end\":47698,\"start\":47697},{\"end\":47705,\"start\":47704},{\"end\":47717,\"start\":47716},{\"end\":47729,\"start\":47728},{\"end\":47738,\"start\":47737},{\"end\":47744,\"start\":47743},{\"end\":47755,\"start\":47754},{\"end\":47763,\"start\":47762},{\"end\":47770,\"start\":47769},{\"end\":47772,\"start\":47771},{\"end\":48098,\"start\":48097},{\"end\":48111,\"start\":48110},{\"end\":48121,\"start\":48120},{\"end\":48719,\"start\":48718},{\"end\":48730,\"start\":48729},{\"end\":48742,\"start\":48741},{\"end\":48752,\"start\":48751},{\"end\":48933,\"start\":48932},{\"end\":48939,\"start\":48938},{\"end\":48948,\"start\":48947},{\"end\":48956,\"start\":48955},{\"end\":48964,\"start\":48963},{\"end\":48971,\"start\":48970},{\"end\":48978,\"start\":48977},{\"end\":49528,\"start\":49527},{\"end\":49535,\"start\":49534},{\"end\":49544,\"start\":49543},{\"end\":49552,\"start\":49551},{\"end\":49558,\"start\":49557},{\"end\":49567,\"start\":49566},{\"end\":49579,\"start\":49575},{\"end\":49583,\"start\":49582},{\"end\":49974,\"start\":49973},{\"end\":49985,\"start\":49984},{\"end\":49992,\"start\":49991},{\"end\":49998,\"start\":49997},{\"end\":50008,\"start\":50007},{\"end\":50010,\"start\":50009},{\"end\":50019,\"start\":50018},{\"end\":50026,\"start\":50025},{\"end\":50034,\"start\":50033},{\"end\":50036,\"start\":50035},{\"end\":50042,\"start\":50041},{\"end\":50050,\"start\":50049},{\"end\":50057,\"start\":50056},{\"end\":50339,\"start\":50338},{\"end\":50346,\"start\":50345},{\"end\":50355,\"start\":50354},{\"end\":50370,\"start\":50369},{\"end\":50707,\"start\":50706},{\"end\":50717,\"start\":50716},{\"end\":50729,\"start\":50728},{\"end\":50738,\"start\":50737},{\"end\":50963,\"start\":50962},{\"end\":50978,\"start\":50977},{\"end\":50986,\"start\":50985},{\"end\":50998,\"start\":50997},{\"end\":51009,\"start\":51008},{\"end\":51021,\"start\":51020},{\"end\":51023,\"start\":51022},{\"end\":51031,\"start\":51030},{\"end\":51033,\"start\":51032},{\"end\":51041,\"start\":51040},{\"end\":51052,\"start\":51048},{\"end\":51060,\"start\":51059},{\"end\":51074,\"start\":51073},{\"end\":51082,\"start\":51081},{\"end\":51091,\"start\":51090},{\"end\":51093,\"start\":51092},{\"end\":51100,\"start\":51099},{\"end\":51113,\"start\":51112},{\"end\":51124,\"start\":51123},{\"end\":51136,\"start\":51135},{\"end\":51146,\"start\":51145},{\"end\":51154,\"start\":51153},{\"end\":51545,\"start\":51544},{\"end\":51555,\"start\":51554},{\"end\":51561,\"start\":51560},{\"end\":51570,\"start\":51569},{\"end\":51581,\"start\":51580},{\"end\":51595,\"start\":51594},{\"end\":51606,\"start\":51605},{\"end\":51615,\"start\":51614},{\"end\":51626,\"start\":51625},{\"end\":51635,\"start\":51634},{\"end\":51964,\"start\":51963},{\"end\":51966,\"start\":51965},{\"end\":51974,\"start\":51973},{\"end\":51976,\"start\":51975},{\"end\":51992,\"start\":51991},{\"end\":51994,\"start\":51993},{\"end\":52001,\"start\":52000},{\"end\":52003,\"start\":52002},{\"end\":52013,\"start\":52012},{\"end\":52022,\"start\":52021},{\"end\":52024,\"start\":52023},{\"end\":52360,\"start\":52359},{\"end\":52367,\"start\":52366},{\"end\":52389,\"start\":52388},{\"end\":52397,\"start\":52396},{\"end\":52405,\"start\":52404},{\"end\":52419,\"start\":52418},{\"end\":52430,\"start\":52429},{\"end\":52910,\"start\":52909},{\"end\":52918,\"start\":52917},{\"end\":52924,\"start\":52923},{\"end\":52930,\"start\":52929},{\"end\":52940,\"start\":52939},{\"end\":53214,\"start\":53213},{\"end\":53224,\"start\":53223},{\"end\":53235,\"start\":53234},{\"end\":53246,\"start\":53245},{\"end\":53253,\"start\":53252},{\"end\":53263,\"start\":53262},{\"end\":53273,\"start\":53272},{\"end\":53281,\"start\":53280},{\"end\":53287,\"start\":53286},{\"end\":53289,\"start\":53288},{\"end\":53652,\"start\":53651},{\"end\":53659,\"start\":53658},{\"end\":53667,\"start\":53666},{\"end\":53675,\"start\":53674},{\"end\":53920,\"start\":53919},{\"end\":53928,\"start\":53927},{\"end\":53938,\"start\":53937},{\"end\":53948,\"start\":53947},{\"end\":53950,\"start\":53949},{\"end\":53958,\"start\":53957},{\"end\":53970,\"start\":53969},{\"end\":53982,\"start\":53981},{\"end\":53993,\"start\":53992},{\"end\":54005,\"start\":54004},{\"end\":54007,\"start\":54006},{\"end\":54015,\"start\":54014},{\"end\":54023,\"start\":54022},{\"end\":54030,\"start\":54029},{\"end\":54032,\"start\":54031},{\"end\":54040,\"start\":54039},{\"end\":54046,\"start\":54045},{\"end\":54057,\"start\":54056},{\"end\":54059,\"start\":54058},{\"end\":54069,\"start\":54068},{\"end\":54082,\"start\":54081},{\"end\":54089,\"start\":54088},{\"end\":54102,\"start\":54101},{\"end\":54111,\"start\":54110},{\"end\":54120,\"start\":54119},{\"end\":54129,\"start\":54128},{\"end\":54131,\"start\":54130},{\"end\":54142,\"start\":54139},{\"end\":54150,\"start\":54149},{\"end\":54160,\"start\":54159},{\"end\":54168,\"start\":54167},{\"end\":54176,\"start\":54175},{\"end\":54178,\"start\":54177},{\"end\":54188,\"start\":54187},{\"end\":54198,\"start\":54197},{\"end\":54206,\"start\":54205},{\"end\":54216,\"start\":54215},{\"end\":54225,\"start\":54224},{\"end\":54235,\"start\":54234},{\"end\":54247,\"start\":54246},{\"end\":54256,\"start\":54255},{\"end\":54265,\"start\":54264},{\"end\":54267,\"start\":54266},{\"end\":54277,\"start\":54276},{\"end\":54289,\"start\":54288},{\"end\":54296,\"start\":54295},{\"end\":54304,\"start\":54303},{\"end\":54312,\"start\":54311},{\"end\":54320,\"start\":54319},{\"end\":54322,\"start\":54321},{\"end\":55117,\"start\":55116},{\"end\":55126,\"start\":55125},{\"end\":55139,\"start\":55138},{\"end\":55148,\"start\":55147},{\"end\":55158,\"start\":55157},{\"end\":55164,\"start\":55163},{\"end\":55176,\"start\":55175},{\"end\":55185,\"start\":55184},{\"end\":55187,\"start\":55186},{\"end\":55446,\"start\":55445},{\"end\":55454,\"start\":55453},{\"end\":55462,\"start\":55461},{\"end\":55764,\"start\":55763},{\"end\":55772,\"start\":55771},{\"end\":55781,\"start\":55780},{\"end\":55791,\"start\":55790},{\"end\":55798,\"start\":55797},{\"end\":55800,\"start\":55799},{\"end\":55809,\"start\":55808},{\"end\":55821,\"start\":55820},{\"end\":56093,\"start\":56092},{\"end\":56101,\"start\":56100},{\"end\":56111,\"start\":56110},{\"end\":56131,\"start\":56130},{\"end\":56140,\"start\":56139},{\"end\":56151,\"start\":56150},{\"end\":56164,\"start\":56163},{\"end\":56173,\"start\":56172},{\"end\":56175,\"start\":56174},{\"end\":56191,\"start\":56190},{\"end\":56199,\"start\":56198},{\"end\":56512,\"start\":56511},{\"end\":56519,\"start\":56518},{\"end\":56528,\"start\":56527},{\"end\":56536,\"start\":56535},{\"end\":56543,\"start\":56542},{\"end\":56545,\"start\":56544},{\"end\":56551,\"start\":56550},{\"end\":56561,\"start\":56560},{\"end\":56567,\"start\":56566},{\"end\":56569,\"start\":56568},{\"end\":56576,\"start\":56575},{\"end\":56578,\"start\":56577},{\"end\":56955,\"start\":56954},{\"end\":56962,\"start\":56961},{\"end\":56964,\"start\":56963},{\"end\":56970,\"start\":56969},{\"end\":56977,\"start\":56976},{\"end\":56986,\"start\":56985},{\"end\":56997,\"start\":56996},{\"end\":57009,\"start\":57008},{\"end\":57015,\"start\":57014},{\"end\":57024,\"start\":57023},{\"end\":57031,\"start\":57030},{\"end\":57033,\"start\":57032},{\"end\":57041,\"start\":57040},{\"end\":57050,\"start\":57049},{\"end\":57058,\"start\":57057},{\"end\":57064,\"start\":57063},{\"end\":57073,\"start\":57072},{\"end\":57079,\"start\":57078},{\"end\":57086,\"start\":57085},{\"end\":57095,\"start\":57094},{\"end\":57104,\"start\":57103},{\"end\":57112,\"start\":57111},{\"end\":57121,\"start\":57120},{\"end\":57123,\"start\":57122},{\"end\":57132,\"start\":57131},{\"end\":57148,\"start\":57146},{\"end\":57152,\"start\":57151},{\"end\":58247,\"start\":58246},{\"end\":58253,\"start\":58252},{\"end\":58259,\"start\":58258},{\"end\":58268,\"start\":58267},{\"end\":58275,\"start\":58274},{\"end\":58284,\"start\":58283},{\"end\":58294,\"start\":58290},{\"end\":58298,\"start\":58297},{\"end\":59006,\"start\":59005},{\"end\":59012,\"start\":59011},{\"end\":59019,\"start\":59018},{\"end\":59027,\"start\":59026},{\"end\":59346,\"start\":59345},{\"end\":59352,\"start\":59351},{\"end\":59360,\"start\":59359},{\"end\":59366,\"start\":59365},{\"end\":59374,\"start\":59373},{\"end\":59386,\"start\":59385},{\"end\":59395,\"start\":59391},{\"end\":59399,\"start\":59398},{\"end\":59947,\"start\":59946},{\"end\":59954,\"start\":59953},{\"end\":59963,\"start\":59962},{\"end\":60119,\"start\":60118},{\"end\":60197,\"start\":60196},{\"end\":60204,\"start\":60203},{\"end\":60212,\"start\":60211},{\"end\":60220,\"start\":60219},{\"end\":60227,\"start\":60226},{\"end\":60236,\"start\":60235},{\"end\":60244,\"start\":60243},{\"end\":60252,\"start\":60251},{\"end\":60258,\"start\":60257},{\"end\":60265,\"start\":60264},{\"end\":60272,\"start\":60271},{\"end\":60280,\"start\":60279},{\"end\":60286,\"start\":60285},{\"end\":60294,\"start\":60293},{\"end\":60303,\"start\":60302},{\"end\":60309,\"start\":60308},{\"end\":60316,\"start\":60315},{\"end\":60322,\"start\":60321},{\"end\":60328,\"start\":60327},{\"end\":60336,\"start\":60335},{\"end\":60344,\"start\":60343},{\"end\":60352,\"start\":60351},{\"end\":60361,\"start\":60360},{\"end\":60367,\"start\":60366},{\"end\":60376,\"start\":60375},{\"end\":60383,\"start\":60382},{\"end\":60389,\"start\":60388},{\"end\":60396,\"start\":60395},{\"end\":60404,\"start\":60403},{\"end\":60411,\"start\":60410},{\"end\":60418,\"start\":60417},{\"end\":60425,\"start\":60424},{\"end\":60432,\"start\":60431},{\"end\":60440,\"start\":60439},{\"end\":60447,\"start\":60446},{\"end\":61209,\"start\":61208},{\"end\":61215,\"start\":61214},{\"end\":61217,\"start\":61216},{\"end\":61224,\"start\":61223},{\"end\":61827,\"start\":61826},{\"end\":61833,\"start\":61832},{\"end\":61842,\"start\":61841},{\"end\":61851,\"start\":61850},{\"end\":61861,\"start\":61860},{\"end\":61872,\"start\":61871},{\"end\":62167,\"start\":62166},{\"end\":62177,\"start\":62176},{\"end\":62185,\"start\":62184},{\"end\":62192,\"start\":62191},{\"end\":62202,\"start\":62201},{\"end\":62211,\"start\":62210},{\"end\":62223,\"start\":62222},{\"end\":62464,\"start\":62463},{\"end\":62472,\"start\":62471},{\"end\":62479,\"start\":62478},{\"end\":62485,\"start\":62484},{\"end\":62493,\"start\":62492},{\"end\":62500,\"start\":62499},{\"end\":62508,\"start\":62507},{\"end\":62516,\"start\":62515},{\"end\":62522,\"start\":62521},{\"end\":62531,\"start\":62530},{\"end\":62538,\"start\":62537},{\"end\":62540,\"start\":62539},{\"end\":62547,\"start\":62546},{\"end\":62553,\"start\":62552},{\"end\":62560,\"start\":62559},{\"end\":62568,\"start\":62567},{\"end\":62576,\"start\":62575},{\"end\":62583,\"start\":62582},{\"end\":62592,\"start\":62591},{\"end\":62600,\"start\":62599},{\"end\":63077,\"start\":63076},{\"end\":63079,\"start\":63078},{\"end\":63093,\"start\":63092}]", "bib_author_last_name": "[{\"end\":38495,\"start\":38486},{\"end\":38672,\"start\":38664},{\"end\":38679,\"start\":38676},{\"end\":38691,\"start\":38683},{\"end\":38698,\"start\":38695},{\"end\":38707,\"start\":38702},{\"end\":38718,\"start\":38713},{\"end\":38728,\"start\":38722},{\"end\":38736,\"start\":38732},{\"end\":38745,\"start\":38740},{\"end\":38751,\"start\":38749},{\"end\":38760,\"start\":38755},{\"end\":38767,\"start\":38764},{\"end\":38776,\"start\":38771},{\"end\":38787,\"start\":38780},{\"end\":39298,\"start\":39294},{\"end\":39306,\"start\":39302},{\"end\":39316,\"start\":39312},{\"end\":39326,\"start\":39320},{\"end\":39336,\"start\":39330},{\"end\":39347,\"start\":39342},{\"end\":39357,\"start\":39351},{\"end\":39364,\"start\":39361},{\"end\":39374,\"start\":39370},{\"end\":39383,\"start\":39378},{\"end\":39395,\"start\":39387},{\"end\":39402,\"start\":39399},{\"end\":39414,\"start\":39406},{\"end\":39421,\"start\":39418},{\"end\":39434,\"start\":39425},{\"end\":39440,\"start\":39438},{\"end\":39453,\"start\":39444},{\"end\":39461,\"start\":39457},{\"end\":39472,\"start\":39467},{\"end\":39489,\"start\":39478},{\"end\":39499,\"start\":39493},{\"end\":39510,\"start\":39503},{\"end\":39523,\"start\":39514},{\"end\":39531,\"start\":39527},{\"end\":39542,\"start\":39537},{\"end\":39552,\"start\":39548},{\"end\":40133,\"start\":40130},{\"end\":40142,\"start\":40137},{\"end\":40153,\"start\":40146},{\"end\":40163,\"start\":40157},{\"end\":40171,\"start\":40167},{\"end\":40183,\"start\":40175},{\"end\":40192,\"start\":40187},{\"end\":40200,\"start\":40196},{\"end\":40211,\"start\":40204},{\"end\":40223,\"start\":40215},{\"end\":40233,\"start\":40227},{\"end\":40245,\"start\":40237},{\"end\":40256,\"start\":40249},{\"end\":40267,\"start\":40260},{\"end\":40279,\"start\":40271},{\"end\":40289,\"start\":40283},{\"end\":40307,\"start\":40293},{\"end\":40320,\"start\":40311},{\"end\":40328,\"start\":40324},{\"end\":40340,\"start\":40332},{\"end\":40350,\"start\":40344},{\"end\":40360,\"start\":40354},{\"end\":40369,\"start\":40364},{\"end\":40379,\"start\":40373},{\"end\":40389,\"start\":40383},{\"end\":40400,\"start\":40395},{\"end\":40409,\"start\":40404},{\"end\":40423,\"start\":40413},{\"end\":40431,\"start\":40427},{\"end\":40439,\"start\":40435},{\"end\":40449,\"start\":40443},{\"end\":41080,\"start\":41074},{\"end\":41093,\"start\":41084},{\"end\":41106,\"start\":41097},{\"end\":41521,\"start\":41509},{\"end\":41529,\"start\":41525},{\"end\":41651,\"start\":41646},{\"end\":41658,\"start\":41655},{\"end\":41669,\"start\":41662},{\"end\":41677,\"start\":41673},{\"end\":41684,\"start\":41681},{\"end\":41693,\"start\":41688},{\"end\":41699,\"start\":41697},{\"end\":41707,\"start\":41703},{\"end\":41719,\"start\":41711},{\"end\":41729,\"start\":41723},{\"end\":41739,\"start\":41733},{\"end\":41745,\"start\":41743},{\"end\":41752,\"start\":41749},{\"end\":41762,\"start\":41756},{\"end\":41770,\"start\":41766},{\"end\":41783,\"start\":41774},{\"end\":41793,\"start\":41787},{\"end\":41803,\"start\":41797},{\"end\":41813,\"start\":41807},{\"end\":41819,\"start\":41817},{\"end\":41827,\"start\":41823},{\"end\":42231,\"start\":42226},{\"end\":42240,\"start\":42237},{\"end\":42246,\"start\":42244},{\"end\":42256,\"start\":42250},{\"end\":42263,\"start\":42260},{\"end\":42271,\"start\":42267},{\"end\":42281,\"start\":42275},{\"end\":42292,\"start\":42285},{\"end\":42300,\"start\":42296},{\"end\":42308,\"start\":42306},{\"end\":42613,\"start\":42600},{\"end\":42894,\"start\":42884},{\"end\":43153,\"start\":43149},{\"end\":43161,\"start\":43157},{\"end\":43167,\"start\":43165},{\"end\":43173,\"start\":43171},{\"end\":43180,\"start\":43177},{\"end\":43187,\"start\":43184},{\"end\":43194,\"start\":43191},{\"end\":43202,\"start\":43198},{\"end\":43490,\"start\":43484},{\"end\":43501,\"start\":43494},{\"end\":43512,\"start\":43507},{\"end\":43522,\"start\":43516},{\"end\":43530,\"start\":43526},{\"end\":44141,\"start\":44131},{\"end\":44150,\"start\":44145},{\"end\":44158,\"start\":44154},{\"end\":44387,\"start\":44381},{\"end\":44398,\"start\":44393},{\"end\":44409,\"start\":44402},{\"end\":44416,\"start\":44413},{\"end\":44424,\"start\":44420},{\"end\":44641,\"start\":44634},{\"end\":44651,\"start\":44645},{\"end\":44662,\"start\":44655},{\"end\":44672,\"start\":44666},{\"end\":44679,\"start\":44676},{\"end\":44691,\"start\":44683},{\"end\":44699,\"start\":44695},{\"end\":44708,\"start\":44703},{\"end\":44720,\"start\":44712},{\"end\":44731,\"start\":44724},{\"end\":44740,\"start\":44735},{\"end\":44750,\"start\":44744},{\"end\":44758,\"start\":44754},{\"end\":44769,\"start\":44762},{\"end\":44781,\"start\":44773},{\"end\":44790,\"start\":44785},{\"end\":44800,\"start\":44794},{\"end\":44812,\"start\":44804},{\"end\":44820,\"start\":44816},{\"end\":44838,\"start\":44824},{\"end\":44850,\"start\":44842},{\"end\":44863,\"start\":44854},{\"end\":44871,\"start\":44867},{\"end\":44883,\"start\":44875},{\"end\":44895,\"start\":44887},{\"end\":44905,\"start\":44899},{\"end\":44915,\"start\":44909},{\"end\":44925,\"start\":44919},{\"end\":44941,\"start\":44929},{\"end\":44951,\"start\":44945},{\"end\":44960,\"start\":44955},{\"end\":44970,\"start\":44964},{\"end\":44984,\"start\":44974},{\"end\":44992,\"start\":44988},{\"end\":45002,\"start\":44996},{\"end\":45797,\"start\":45793},{\"end\":45810,\"start\":45801},{\"end\":45817,\"start\":45814},{\"end\":45828,\"start\":45821},{\"end\":45838,\"start\":45832},{\"end\":45848,\"start\":45842},{\"end\":45856,\"start\":45852},{\"end\":46108,\"start\":46105},{\"end\":46117,\"start\":46112},{\"end\":46125,\"start\":46121},{\"end\":46134,\"start\":46129},{\"end\":46141,\"start\":46138},{\"end\":46149,\"start\":46145},{\"end\":46156,\"start\":46153},{\"end\":46162,\"start\":46160},{\"end\":46499,\"start\":46493},{\"end\":46518,\"start\":46505},{\"end\":46526,\"start\":46522},{\"end\":46537,\"start\":46530},{\"end\":46547,\"start\":46541},{\"end\":46807,\"start\":46799},{\"end\":46818,\"start\":46811},{\"end\":46826,\"start\":46822},{\"end\":46836,\"start\":46830},{\"end\":47076,\"start\":47072},{\"end\":47083,\"start\":47080},{\"end\":47095,\"start\":47087},{\"end\":47107,\"start\":47099},{\"end\":47116,\"start\":47111},{\"end\":47122,\"start\":47120},{\"end\":47133,\"start\":47126},{\"end\":47141,\"start\":47137},{\"end\":47148,\"start\":47145},{\"end\":47159,\"start\":47154},{\"end\":47165,\"start\":47163},{\"end\":47180,\"start\":47169},{\"end\":47191,\"start\":47184},{\"end\":47199,\"start\":47195},{\"end\":47208,\"start\":47203},{\"end\":47223,\"start\":47212},{\"end\":47238,\"start\":47227},{\"end\":47250,\"start\":47242},{\"end\":47693,\"start\":47689},{\"end\":47702,\"start\":47699},{\"end\":47714,\"start\":47706},{\"end\":47726,\"start\":47718},{\"end\":47735,\"start\":47730},{\"end\":47741,\"start\":47739},{\"end\":47752,\"start\":47745},{\"end\":47760,\"start\":47756},{\"end\":47767,\"start\":47764},{\"end\":47778,\"start\":47773},{\"end\":48108,\"start\":48099},{\"end\":48118,\"start\":48112},{\"end\":48134,\"start\":48122},{\"end\":48509,\"start\":48495},{\"end\":48727,\"start\":48720},{\"end\":48739,\"start\":48731},{\"end\":48749,\"start\":48743},{\"end\":48759,\"start\":48753},{\"end\":48936,\"start\":48934},{\"end\":48945,\"start\":48940},{\"end\":48953,\"start\":48949},{\"end\":48961,\"start\":48957},{\"end\":48968,\"start\":48965},{\"end\":48975,\"start\":48972},{\"end\":48984,\"start\":48979},{\"end\":49532,\"start\":49529},{\"end\":49541,\"start\":49536},{\"end\":49549,\"start\":49545},{\"end\":49555,\"start\":49553},{\"end\":49564,\"start\":49559},{\"end\":49573,\"start\":49568},{\"end\":49982,\"start\":49975},{\"end\":49989,\"start\":49986},{\"end\":49995,\"start\":49993},{\"end\":50005,\"start\":49999},{\"end\":50016,\"start\":50011},{\"end\":50023,\"start\":50020},{\"end\":50031,\"start\":50027},{\"end\":50039,\"start\":50037},{\"end\":50047,\"start\":50043},{\"end\":50054,\"start\":50051},{\"end\":50065,\"start\":50058},{\"end\":50343,\"start\":50340},{\"end\":50352,\"start\":50347},{\"end\":50367,\"start\":50356},{\"end\":50381,\"start\":50371},{\"end\":50714,\"start\":50708},{\"end\":50726,\"start\":50718},{\"end\":50735,\"start\":50730},{\"end\":50749,\"start\":50739},{\"end\":50975,\"start\":50964},{\"end\":50983,\"start\":50979},{\"end\":50995,\"start\":50987},{\"end\":51006,\"start\":50999},{\"end\":51018,\"start\":51010},{\"end\":51028,\"start\":51024},{\"end\":51038,\"start\":51034},{\"end\":51046,\"start\":51042},{\"end\":51057,\"start\":51053},{\"end\":51071,\"start\":51061},{\"end\":51079,\"start\":51075},{\"end\":51088,\"start\":51083},{\"end\":51097,\"start\":51094},{\"end\":51110,\"start\":51101},{\"end\":51121,\"start\":51114},{\"end\":51133,\"start\":51125},{\"end\":51143,\"start\":51137},{\"end\":51151,\"start\":51147},{\"end\":51161,\"start\":51155},{\"end\":51552,\"start\":51546},{\"end\":51558,\"start\":51556},{\"end\":51567,\"start\":51562},{\"end\":51578,\"start\":51571},{\"end\":51592,\"start\":51582},{\"end\":51603,\"start\":51596},{\"end\":51612,\"start\":51607},{\"end\":51623,\"start\":51616},{\"end\":51632,\"start\":51627},{\"end\":51639,\"start\":51636},{\"end\":51971,\"start\":51967},{\"end\":51989,\"start\":51977},{\"end\":51998,\"start\":51995},{\"end\":52010,\"start\":52004},{\"end\":52019,\"start\":52014},{\"end\":52034,\"start\":52025},{\"end\":52364,\"start\":52361},{\"end\":52386,\"start\":52368},{\"end\":52394,\"start\":52390},{\"end\":52402,\"start\":52398},{\"end\":52416,\"start\":52406},{\"end\":52427,\"start\":52420},{\"end\":52438,\"start\":52431},{\"end\":52915,\"start\":52911},{\"end\":52921,\"start\":52919},{\"end\":52927,\"start\":52925},{\"end\":52937,\"start\":52931},{\"end\":52944,\"start\":52941},{\"end\":53221,\"start\":53215},{\"end\":53232,\"start\":53225},{\"end\":53243,\"start\":53236},{\"end\":53250,\"start\":53247},{\"end\":53260,\"start\":53254},{\"end\":53270,\"start\":53264},{\"end\":53278,\"start\":53274},{\"end\":53284,\"start\":53282},{\"end\":53293,\"start\":53290},{\"end\":53656,\"start\":53653},{\"end\":53664,\"start\":53660},{\"end\":53672,\"start\":53668},{\"end\":53683,\"start\":53676},{\"end\":53925,\"start\":53921},{\"end\":53935,\"start\":53929},{\"end\":53945,\"start\":53939},{\"end\":53955,\"start\":53951},{\"end\":53967,\"start\":53959},{\"end\":53979,\"start\":53971},{\"end\":53990,\"start\":53983},{\"end\":54002,\"start\":53994},{\"end\":54012,\"start\":54008},{\"end\":54020,\"start\":54016},{\"end\":54027,\"start\":54024},{\"end\":54037,\"start\":54033},{\"end\":54043,\"start\":54041},{\"end\":54054,\"start\":54047},{\"end\":54066,\"start\":54060},{\"end\":54079,\"start\":54070},{\"end\":54086,\"start\":54083},{\"end\":54099,\"start\":54090},{\"end\":54108,\"start\":54103},{\"end\":54117,\"start\":54112},{\"end\":54126,\"start\":54121},{\"end\":54137,\"start\":54132},{\"end\":54147,\"start\":54143},{\"end\":54157,\"start\":54151},{\"end\":54165,\"start\":54161},{\"end\":54173,\"start\":54169},{\"end\":54185,\"start\":54179},{\"end\":54195,\"start\":54189},{\"end\":54203,\"start\":54199},{\"end\":54213,\"start\":54207},{\"end\":54222,\"start\":54217},{\"end\":54232,\"start\":54226},{\"end\":54244,\"start\":54236},{\"end\":54253,\"start\":54248},{\"end\":54262,\"start\":54257},{\"end\":54274,\"start\":54268},{\"end\":54286,\"start\":54278},{\"end\":54293,\"start\":54290},{\"end\":54301,\"start\":54297},{\"end\":54309,\"start\":54305},{\"end\":54317,\"start\":54313},{\"end\":54972,\"start\":54964},{\"end\":55123,\"start\":55118},{\"end\":55136,\"start\":55127},{\"end\":55145,\"start\":55140},{\"end\":55155,\"start\":55149},{\"end\":55161,\"start\":55159},{\"end\":55173,\"start\":55165},{\"end\":55182,\"start\":55177},{\"end\":55197,\"start\":55188},{\"end\":55451,\"start\":55447},{\"end\":55459,\"start\":55455},{\"end\":55466,\"start\":55463},{\"end\":55769,\"start\":55765},{\"end\":55778,\"start\":55773},{\"end\":55788,\"start\":55782},{\"end\":55795,\"start\":55792},{\"end\":55806,\"start\":55801},{\"end\":55818,\"start\":55810},{\"end\":55832,\"start\":55822},{\"end\":56098,\"start\":56094},{\"end\":56108,\"start\":56102},{\"end\":56128,\"start\":56112},{\"end\":56137,\"start\":56132},{\"end\":56148,\"start\":56141},{\"end\":56161,\"start\":56152},{\"end\":56170,\"start\":56165},{\"end\":56188,\"start\":56176},{\"end\":56196,\"start\":56192},{\"end\":56204,\"start\":56200},{\"end\":56516,\"start\":56513},{\"end\":56525,\"start\":56520},{\"end\":56533,\"start\":56529},{\"end\":56540,\"start\":56537},{\"end\":56548,\"start\":56546},{\"end\":56558,\"start\":56552},{\"end\":56564,\"start\":56562},{\"end\":56573,\"start\":56570},{\"end\":56581,\"start\":56579},{\"end\":56959,\"start\":56956},{\"end\":56967,\"start\":56965},{\"end\":56974,\"start\":56971},{\"end\":56983,\"start\":56978},{\"end\":56994,\"start\":56987},{\"end\":57006,\"start\":56998},{\"end\":57012,\"start\":57010},{\"end\":57021,\"start\":57016},{\"end\":57028,\"start\":57025},{\"end\":57038,\"start\":57034},{\"end\":57047,\"start\":57042},{\"end\":57055,\"start\":57051},{\"end\":57061,\"start\":57059},{\"end\":57070,\"start\":57065},{\"end\":57076,\"start\":57074},{\"end\":57083,\"start\":57080},{\"end\":57092,\"start\":57087},{\"end\":57101,\"start\":57096},{\"end\":57109,\"start\":57105},{\"end\":57118,\"start\":57113},{\"end\":57129,\"start\":57124},{\"end\":57144,\"start\":57133},{\"end\":58250,\"start\":58248},{\"end\":58256,\"start\":58254},{\"end\":58265,\"start\":58260},{\"end\":58272,\"start\":58269},{\"end\":58281,\"start\":58276},{\"end\":58288,\"start\":58285},{\"end\":59009,\"start\":59007},{\"end\":59016,\"start\":59013},{\"end\":59024,\"start\":59020},{\"end\":59035,\"start\":59028},{\"end\":59349,\"start\":59347},{\"end\":59357,\"start\":59353},{\"end\":59363,\"start\":59361},{\"end\":59371,\"start\":59367},{\"end\":59383,\"start\":59375},{\"end\":59389,\"start\":59387},{\"end\":59951,\"start\":59948},{\"end\":59960,\"start\":59955},{\"end\":59967,\"start\":59964},{\"end\":60124,\"start\":60120},{\"end\":60201,\"start\":60198},{\"end\":60209,\"start\":60205},{\"end\":60217,\"start\":60213},{\"end\":60224,\"start\":60221},{\"end\":60233,\"start\":60228},{\"end\":60241,\"start\":60237},{\"end\":60249,\"start\":60245},{\"end\":60255,\"start\":60253},{\"end\":60262,\"start\":60259},{\"end\":60269,\"start\":60266},{\"end\":60277,\"start\":60273},{\"end\":60283,\"start\":60281},{\"end\":60291,\"start\":60287},{\"end\":60300,\"start\":60295},{\"end\":60306,\"start\":60304},{\"end\":60313,\"start\":60310},{\"end\":60319,\"start\":60317},{\"end\":60325,\"start\":60323},{\"end\":60333,\"start\":60329},{\"end\":60341,\"start\":60337},{\"end\":60349,\"start\":60345},{\"end\":60358,\"start\":60353},{\"end\":60364,\"start\":60362},{\"end\":60373,\"start\":60368},{\"end\":60380,\"start\":60377},{\"end\":60386,\"start\":60384},{\"end\":60393,\"start\":60390},{\"end\":60401,\"start\":60397},{\"end\":60408,\"start\":60405},{\"end\":60415,\"start\":60412},{\"end\":60422,\"start\":60419},{\"end\":60429,\"start\":60426},{\"end\":60437,\"start\":60433},{\"end\":60444,\"start\":60441},{\"end\":60451,\"start\":60448},{\"end\":60990,\"start\":60983},{\"end\":61212,\"start\":61210},{\"end\":61221,\"start\":61218},{\"end\":61228,\"start\":61225},{\"end\":61830,\"start\":61828},{\"end\":61839,\"start\":61834},{\"end\":61848,\"start\":61843},{\"end\":61858,\"start\":61852},{\"end\":61869,\"start\":61862},{\"end\":61877,\"start\":61873},{\"end\":62174,\"start\":62168},{\"end\":62182,\"start\":62178},{\"end\":62189,\"start\":62186},{\"end\":62199,\"start\":62193},{\"end\":62208,\"start\":62203},{\"end\":62220,\"start\":62212},{\"end\":62232,\"start\":62224},{\"end\":62469,\"start\":62465},{\"end\":62476,\"start\":62473},{\"end\":62482,\"start\":62480},{\"end\":62490,\"start\":62486},{\"end\":62497,\"start\":62494},{\"end\":62505,\"start\":62501},{\"end\":62513,\"start\":62509},{\"end\":62519,\"start\":62517},{\"end\":62528,\"start\":62523},{\"end\":62535,\"start\":62532},{\"end\":62544,\"start\":62541},{\"end\":62550,\"start\":62548},{\"end\":62557,\"start\":62554},{\"end\":62565,\"start\":62561},{\"end\":62573,\"start\":62569},{\"end\":62580,\"start\":62577},{\"end\":62589,\"start\":62584},{\"end\":62597,\"start\":62593},{\"end\":62605,\"start\":62601},{\"end\":63090,\"start\":63080},{\"end\":63096,\"start\":63094}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":38453,\"start\":38314},{\"attributes\":{\"id\":\"b1\"},\"end\":38660,\"start\":38455},{\"attributes\":{\"id\":\"b2\"},\"end\":39191,\"start\":38662},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":246485605},\"end\":40124,\"start\":39193},{\"attributes\":{\"id\":\"b4\"},\"end\":41049,\"start\":40126},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":873046},\"end\":41436,\"start\":41051},{\"attributes\":{\"id\":\"b6\"},\"end\":41638,\"start\":41438},{\"attributes\":{\"id\":\"b7\"},\"end\":42175,\"start\":41640},{\"attributes\":{\"id\":\"b8\"},\"end\":42514,\"start\":42177},{\"attributes\":{\"id\":\"b9\"},\"end\":42813,\"start\":42516},{\"attributes\":{\"id\":\"b10\"},\"end\":43078,\"start\":42815},{\"attributes\":{\"id\":\"b11\"},\"end\":43391,\"start\":43080},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":229923749},\"end\":44125,\"start\":43393},{\"attributes\":{\"id\":\"b13\"},\"end\":44306,\"start\":44127},{\"attributes\":{\"doi\":\"arXiv:2011.00620\",\"id\":\"b14\"},\"end\":44628,\"start\":44308},{\"attributes\":{\"doi\":\"arXiv: Arxiv-2209.07858\",\"id\":\"b15\"},\"end\":45742,\"start\":44630},{\"attributes\":{\"id\":\"b16\"},\"end\":46016,\"start\":45744},{\"attributes\":{\"doi\":\"arxiv:2301.07597\",\"id\":\"b17\"},\"end\":46399,\"start\":46018},{\"attributes\":{\"id\":\"b18\"},\"end\":46718,\"start\":46401},{\"attributes\":{\"id\":\"b19\"},\"end\":46974,\"start\":46720},{\"attributes\":{\"id\":\"b20\"},\"end\":47591,\"start\":46976},{\"attributes\":{\"doi\":\"arXiv:2212.12017\",\"id\":\"b21\"},\"end\":48067,\"start\":47593},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":236486249},\"end\":48491,\"start\":48069},{\"attributes\":{\"id\":\"b23\"},\"end\":48666,\"start\":48493},{\"attributes\":{\"id\":\"b24\"},\"end\":48872,\"start\":48668},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":252199605},\"end\":49525,\"start\":48874},{\"attributes\":{\"id\":\"b26\"},\"end\":49769,\"start\":49527},{\"attributes\":{\"id\":\"b27\"},\"end\":49889,\"start\":49771},{\"attributes\":{\"id\":\"b28\"},\"end\":50297,\"start\":49891},{\"attributes\":{\"id\":\"b29\"},\"end\":50629,\"start\":50299},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":237421373},\"end\":50902,\"start\":50631},{\"attributes\":{\"id\":\"b31\"},\"end\":51473,\"start\":50904},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":246426909},\"end\":51961,\"start\":51475},{\"attributes\":{\"doi\":\"arXiv:2304.03442\",\"id\":\"b33\"},\"end\":52314,\"start\":51963},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":254823456},\"end\":52907,\"start\":52316},{\"attributes\":{\"doi\":\"arXiv:2304.03277\",\"id\":\"b35\"},\"end\":53128,\"start\":52909},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":204838007},\"end\":53593,\"start\":53130},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b37\",\"matched_paper_id\":4321928},\"end\":53915,\"start\":53595},{\"attributes\":{\"id\":\"b38\"},\"end\":54960,\"start\":53917},{\"attributes\":{\"id\":\"b39\"},\"end\":55059,\"start\":54962},{\"attributes\":{\"id\":\"b40\"},\"end\":55410,\"start\":55061},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":232362223},\"end\":55688,\"start\":55412},{\"attributes\":{\"id\":\"b42\"},\"end\":56004,\"start\":55690},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":253098274},\"end\":56459,\"start\":56006},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":237416585},\"end\":56847,\"start\":56461},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":246016124},\"end\":58060,\"start\":56849},{\"attributes\":{\"id\":\"b46\"},\"end\":58180,\"start\":58062},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":1627142},\"end\":58919,\"start\":58182},{\"attributes\":{\"doi\":\"arXiv: Arxiv-2304.01196\",\"id\":\"b48\"},\"end\":59248,\"start\":58921},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":246035184},\"end\":59885,\"start\":59250},{\"attributes\":{\"id\":\"b50\"},\"end\":60114,\"start\":59887},{\"attributes\":{\"id\":\"b51\"},\"end\":60192,\"start\":60116},{\"attributes\":{\"id\":\"b52\"},\"end\":60962,\"start\":60194},{\"attributes\":{\"id\":\"b53\"},\"end\":61128,\"start\":60964},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":233296709},\"end\":61782,\"start\":61130},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":210839011},\"end\":62114,\"start\":61784},{\"attributes\":{\"id\":\"b56\"},\"end\":62414,\"start\":62116},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":252715691},\"end\":62995,\"start\":62416},{\"attributes\":{\"id\":\"b58\"},\"end\":63260,\"start\":62997}]", "bib_title": "[{\"end\":39288,\"start\":39193},{\"end\":41070,\"start\":41051},{\"end\":43480,\"start\":43393},{\"end\":48095,\"start\":48069},{\"end\":48930,\"start\":48874},{\"end\":50704,\"start\":50631},{\"end\":51542,\"start\":51475},{\"end\":52357,\"start\":52316},{\"end\":53211,\"start\":53130},{\"end\":53649,\"start\":53595},{\"end\":55443,\"start\":55412},{\"end\":56090,\"start\":56006},{\"end\":56509,\"start\":56461},{\"end\":56952,\"start\":56849},{\"end\":58244,\"start\":58182},{\"end\":59343,\"start\":59250},{\"end\":60981,\"start\":60964},{\"end\":61206,\"start\":61130},{\"end\":61824,\"start\":61784},{\"end\":62461,\"start\":62416}]", "bib_author": "[{\"end\":38333,\"start\":38327},{\"end\":38337,\"start\":38333},{\"end\":38497,\"start\":38486},{\"end\":38674,\"start\":38662},{\"end\":38681,\"start\":38674},{\"end\":38693,\"start\":38681},{\"end\":38700,\"start\":38693},{\"end\":38709,\"start\":38700},{\"end\":38720,\"start\":38709},{\"end\":38730,\"start\":38720},{\"end\":38738,\"start\":38730},{\"end\":38747,\"start\":38738},{\"end\":38753,\"start\":38747},{\"end\":38762,\"start\":38753},{\"end\":38769,\"start\":38762},{\"end\":38778,\"start\":38769},{\"end\":38789,\"start\":38778},{\"end\":39300,\"start\":39290},{\"end\":39308,\"start\":39300},{\"end\":39318,\"start\":39308},{\"end\":39328,\"start\":39318},{\"end\":39338,\"start\":39328},{\"end\":39349,\"start\":39338},{\"end\":39359,\"start\":39349},{\"end\":39366,\"start\":39359},{\"end\":39376,\"start\":39366},{\"end\":39385,\"start\":39376},{\"end\":39397,\"start\":39385},{\"end\":39404,\"start\":39397},{\"end\":39416,\"start\":39404},{\"end\":39423,\"start\":39416},{\"end\":39436,\"start\":39423},{\"end\":39442,\"start\":39436},{\"end\":39455,\"start\":39442},{\"end\":39463,\"start\":39455},{\"end\":39474,\"start\":39463},{\"end\":39491,\"start\":39474},{\"end\":39501,\"start\":39491},{\"end\":39512,\"start\":39501},{\"end\":39525,\"start\":39512},{\"end\":39533,\"start\":39525},{\"end\":39544,\"start\":39533},{\"end\":39554,\"start\":39544},{\"end\":39560,\"start\":39554},{\"end\":40135,\"start\":40128},{\"end\":40144,\"start\":40135},{\"end\":40155,\"start\":40144},{\"end\":40165,\"start\":40155},{\"end\":40173,\"start\":40165},{\"end\":40185,\"start\":40173},{\"end\":40194,\"start\":40185},{\"end\":40202,\"start\":40194},{\"end\":40213,\"start\":40202},{\"end\":40225,\"start\":40213},{\"end\":40235,\"start\":40225},{\"end\":40247,\"start\":40235},{\"end\":40258,\"start\":40247},{\"end\":40269,\"start\":40258},{\"end\":40281,\"start\":40269},{\"end\":40291,\"start\":40281},{\"end\":40309,\"start\":40291},{\"end\":40322,\"start\":40309},{\"end\":40330,\"start\":40322},{\"end\":40342,\"start\":40330},{\"end\":40352,\"start\":40342},{\"end\":40362,\"start\":40352},{\"end\":40371,\"start\":40362},{\"end\":40381,\"start\":40371},{\"end\":40391,\"start\":40381},{\"end\":40402,\"start\":40391},{\"end\":40411,\"start\":40402},{\"end\":40425,\"start\":40411},{\"end\":40433,\"start\":40425},{\"end\":40441,\"start\":40433},{\"end\":40451,\"start\":40441},{\"end\":41082,\"start\":41072},{\"end\":41095,\"start\":41082},{\"end\":41108,\"start\":41095},{\"end\":41117,\"start\":41108},{\"end\":41121,\"start\":41117},{\"end\":41523,\"start\":41505},{\"end\":41531,\"start\":41523},{\"end\":41653,\"start\":41642},{\"end\":41660,\"start\":41653},{\"end\":41671,\"start\":41660},{\"end\":41679,\"start\":41671},{\"end\":41686,\"start\":41679},{\"end\":41695,\"start\":41686},{\"end\":41701,\"start\":41695},{\"end\":41709,\"start\":41701},{\"end\":41721,\"start\":41709},{\"end\":41731,\"start\":41721},{\"end\":41741,\"start\":41731},{\"end\":41747,\"start\":41741},{\"end\":41754,\"start\":41747},{\"end\":41764,\"start\":41754},{\"end\":41772,\"start\":41764},{\"end\":41785,\"start\":41772},{\"end\":41795,\"start\":41785},{\"end\":41805,\"start\":41795},{\"end\":41815,\"start\":41805},{\"end\":41821,\"start\":41815},{\"end\":41829,\"start\":41821},{\"end\":42233,\"start\":42224},{\"end\":42242,\"start\":42233},{\"end\":42248,\"start\":42242},{\"end\":42258,\"start\":42248},{\"end\":42265,\"start\":42258},{\"end\":42273,\"start\":42265},{\"end\":42283,\"start\":42273},{\"end\":42294,\"start\":42283},{\"end\":42302,\"start\":42294},{\"end\":42310,\"start\":42302},{\"end\":42316,\"start\":42310},{\"end\":42320,\"start\":42316},{\"end\":42615,\"start\":42600},{\"end\":42896,\"start\":42884},{\"end\":43155,\"start\":43147},{\"end\":43163,\"start\":43155},{\"end\":43169,\"start\":43163},{\"end\":43175,\"start\":43169},{\"end\":43182,\"start\":43175},{\"end\":43189,\"start\":43182},{\"end\":43196,\"start\":43189},{\"end\":43204,\"start\":43196},{\"end\":43492,\"start\":43482},{\"end\":43503,\"start\":43492},{\"end\":43514,\"start\":43503},{\"end\":43524,\"start\":43514},{\"end\":43532,\"start\":43524},{\"end\":44143,\"start\":44129},{\"end\":44152,\"start\":44143},{\"end\":44160,\"start\":44152},{\"end\":44171,\"start\":44160},{\"end\":44175,\"start\":44171},{\"end\":44389,\"start\":44379},{\"end\":44400,\"start\":44389},{\"end\":44411,\"start\":44400},{\"end\":44418,\"start\":44411},{\"end\":44426,\"start\":44418},{\"end\":44643,\"start\":44632},{\"end\":44653,\"start\":44643},{\"end\":44664,\"start\":44653},{\"end\":44674,\"start\":44664},{\"end\":44681,\"start\":44674},{\"end\":44693,\"start\":44681},{\"end\":44701,\"start\":44693},{\"end\":44710,\"start\":44701},{\"end\":44722,\"start\":44710},{\"end\":44733,\"start\":44722},{\"end\":44742,\"start\":44733},{\"end\":44752,\"start\":44742},{\"end\":44760,\"start\":44752},{\"end\":44771,\"start\":44760},{\"end\":44783,\"start\":44771},{\"end\":44792,\"start\":44783},{\"end\":44802,\"start\":44792},{\"end\":44814,\"start\":44802},{\"end\":44822,\"start\":44814},{\"end\":44840,\"start\":44822},{\"end\":44852,\"start\":44840},{\"end\":44865,\"start\":44852},{\"end\":44873,\"start\":44865},{\"end\":44885,\"start\":44873},{\"end\":44897,\"start\":44885},{\"end\":44907,\"start\":44897},{\"end\":44917,\"start\":44907},{\"end\":44927,\"start\":44917},{\"end\":44943,\"start\":44927},{\"end\":44953,\"start\":44943},{\"end\":44962,\"start\":44953},{\"end\":44972,\"start\":44962},{\"end\":44986,\"start\":44972},{\"end\":44994,\"start\":44986},{\"end\":45004,\"start\":44994},{\"end\":45012,\"start\":45004},{\"end\":45016,\"start\":45012},{\"end\":45799,\"start\":45791},{\"end\":45812,\"start\":45799},{\"end\":45819,\"start\":45812},{\"end\":45830,\"start\":45819},{\"end\":45840,\"start\":45830},{\"end\":45850,\"start\":45840},{\"end\":45858,\"start\":45850},{\"end\":46110,\"start\":46103},{\"end\":46119,\"start\":46110},{\"end\":46127,\"start\":46119},{\"end\":46136,\"start\":46127},{\"end\":46143,\"start\":46136},{\"end\":46151,\"start\":46143},{\"end\":46158,\"start\":46151},{\"end\":46164,\"start\":46158},{\"end\":46501,\"start\":46491},{\"end\":46520,\"start\":46501},{\"end\":46528,\"start\":46520},{\"end\":46539,\"start\":46528},{\"end\":46549,\"start\":46539},{\"end\":46809,\"start\":46797},{\"end\":46820,\"start\":46809},{\"end\":46828,\"start\":46820},{\"end\":46838,\"start\":46828},{\"end\":47078,\"start\":47070},{\"end\":47085,\"start\":47078},{\"end\":47097,\"start\":47085},{\"end\":47109,\"start\":47097},{\"end\":47118,\"start\":47109},{\"end\":47124,\"start\":47118},{\"end\":47135,\"start\":47124},{\"end\":47143,\"start\":47135},{\"end\":47150,\"start\":47143},{\"end\":47161,\"start\":47150},{\"end\":47167,\"start\":47161},{\"end\":47182,\"start\":47167},{\"end\":47193,\"start\":47182},{\"end\":47201,\"start\":47193},{\"end\":47210,\"start\":47201},{\"end\":47225,\"start\":47210},{\"end\":47240,\"start\":47225},{\"end\":47252,\"start\":47240},{\"end\":47695,\"start\":47687},{\"end\":47704,\"start\":47695},{\"end\":47716,\"start\":47704},{\"end\":47728,\"start\":47716},{\"end\":47737,\"start\":47728},{\"end\":47743,\"start\":47737},{\"end\":47754,\"start\":47743},{\"end\":47762,\"start\":47754},{\"end\":47769,\"start\":47762},{\"end\":47780,\"start\":47769},{\"end\":48110,\"start\":48097},{\"end\":48120,\"start\":48110},{\"end\":48136,\"start\":48120},{\"end\":48511,\"start\":48495},{\"end\":48729,\"start\":48718},{\"end\":48741,\"start\":48729},{\"end\":48751,\"start\":48741},{\"end\":48761,\"start\":48751},{\"end\":48938,\"start\":48932},{\"end\":48947,\"start\":48938},{\"end\":48955,\"start\":48947},{\"end\":48963,\"start\":48955},{\"end\":48970,\"start\":48963},{\"end\":48977,\"start\":48970},{\"end\":48986,\"start\":48977},{\"end\":49534,\"start\":49527},{\"end\":49543,\"start\":49534},{\"end\":49551,\"start\":49543},{\"end\":49557,\"start\":49551},{\"end\":49566,\"start\":49557},{\"end\":49575,\"start\":49566},{\"end\":49582,\"start\":49575},{\"end\":49586,\"start\":49582},{\"end\":49984,\"start\":49973},{\"end\":49991,\"start\":49984},{\"end\":49997,\"start\":49991},{\"end\":50007,\"start\":49997},{\"end\":50018,\"start\":50007},{\"end\":50025,\"start\":50018},{\"end\":50033,\"start\":50025},{\"end\":50041,\"start\":50033},{\"end\":50049,\"start\":50041},{\"end\":50056,\"start\":50049},{\"end\":50067,\"start\":50056},{\"end\":50345,\"start\":50338},{\"end\":50354,\"start\":50345},{\"end\":50369,\"start\":50354},{\"end\":50383,\"start\":50369},{\"end\":50716,\"start\":50706},{\"end\":50728,\"start\":50716},{\"end\":50737,\"start\":50728},{\"end\":50751,\"start\":50737},{\"end\":50977,\"start\":50962},{\"end\":50985,\"start\":50977},{\"end\":50997,\"start\":50985},{\"end\":51008,\"start\":50997},{\"end\":51020,\"start\":51008},{\"end\":51030,\"start\":51020},{\"end\":51040,\"start\":51030},{\"end\":51048,\"start\":51040},{\"end\":51059,\"start\":51048},{\"end\":51073,\"start\":51059},{\"end\":51081,\"start\":51073},{\"end\":51090,\"start\":51081},{\"end\":51099,\"start\":51090},{\"end\":51112,\"start\":51099},{\"end\":51123,\"start\":51112},{\"end\":51135,\"start\":51123},{\"end\":51145,\"start\":51135},{\"end\":51153,\"start\":51145},{\"end\":51163,\"start\":51153},{\"end\":51554,\"start\":51544},{\"end\":51560,\"start\":51554},{\"end\":51569,\"start\":51560},{\"end\":51580,\"start\":51569},{\"end\":51594,\"start\":51580},{\"end\":51605,\"start\":51594},{\"end\":51614,\"start\":51605},{\"end\":51625,\"start\":51614},{\"end\":51634,\"start\":51625},{\"end\":51641,\"start\":51634},{\"end\":51973,\"start\":51963},{\"end\":51991,\"start\":51973},{\"end\":52000,\"start\":51991},{\"end\":52012,\"start\":52000},{\"end\":52021,\"start\":52012},{\"end\":52036,\"start\":52021},{\"end\":52366,\"start\":52359},{\"end\":52388,\"start\":52366},{\"end\":52396,\"start\":52388},{\"end\":52404,\"start\":52396},{\"end\":52418,\"start\":52404},{\"end\":52429,\"start\":52418},{\"end\":52440,\"start\":52429},{\"end\":52917,\"start\":52909},{\"end\":52923,\"start\":52917},{\"end\":52929,\"start\":52923},{\"end\":52939,\"start\":52929},{\"end\":52946,\"start\":52939},{\"end\":53223,\"start\":53213},{\"end\":53234,\"start\":53223},{\"end\":53245,\"start\":53234},{\"end\":53252,\"start\":53245},{\"end\":53262,\"start\":53252},{\"end\":53272,\"start\":53262},{\"end\":53280,\"start\":53272},{\"end\":53286,\"start\":53280},{\"end\":53295,\"start\":53286},{\"end\":53658,\"start\":53651},{\"end\":53666,\"start\":53658},{\"end\":53674,\"start\":53666},{\"end\":53685,\"start\":53674},{\"end\":53927,\"start\":53919},{\"end\":53937,\"start\":53927},{\"end\":53947,\"start\":53937},{\"end\":53957,\"start\":53947},{\"end\":53969,\"start\":53957},{\"end\":53981,\"start\":53969},{\"end\":53992,\"start\":53981},{\"end\":54004,\"start\":53992},{\"end\":54014,\"start\":54004},{\"end\":54022,\"start\":54014},{\"end\":54029,\"start\":54022},{\"end\":54039,\"start\":54029},{\"end\":54045,\"start\":54039},{\"end\":54056,\"start\":54045},{\"end\":54068,\"start\":54056},{\"end\":54081,\"start\":54068},{\"end\":54088,\"start\":54081},{\"end\":54101,\"start\":54088},{\"end\":54110,\"start\":54101},{\"end\":54119,\"start\":54110},{\"end\":54128,\"start\":54119},{\"end\":54139,\"start\":54128},{\"end\":54149,\"start\":54139},{\"end\":54159,\"start\":54149},{\"end\":54167,\"start\":54159},{\"end\":54175,\"start\":54167},{\"end\":54187,\"start\":54175},{\"end\":54197,\"start\":54187},{\"end\":54205,\"start\":54197},{\"end\":54215,\"start\":54205},{\"end\":54224,\"start\":54215},{\"end\":54234,\"start\":54224},{\"end\":54246,\"start\":54234},{\"end\":54255,\"start\":54246},{\"end\":54264,\"start\":54255},{\"end\":54276,\"start\":54264},{\"end\":54288,\"start\":54276},{\"end\":54295,\"start\":54288},{\"end\":54303,\"start\":54295},{\"end\":54311,\"start\":54303},{\"end\":54319,\"start\":54311},{\"end\":54325,\"start\":54319},{\"end\":54974,\"start\":54964},{\"end\":55125,\"start\":55116},{\"end\":55138,\"start\":55125},{\"end\":55147,\"start\":55138},{\"end\":55157,\"start\":55147},{\"end\":55163,\"start\":55157},{\"end\":55175,\"start\":55163},{\"end\":55184,\"start\":55175},{\"end\":55199,\"start\":55184},{\"end\":55453,\"start\":55445},{\"end\":55461,\"start\":55453},{\"end\":55468,\"start\":55461},{\"end\":55771,\"start\":55763},{\"end\":55780,\"start\":55771},{\"end\":55790,\"start\":55780},{\"end\":55797,\"start\":55790},{\"end\":55808,\"start\":55797},{\"end\":55820,\"start\":55808},{\"end\":55834,\"start\":55820},{\"end\":56100,\"start\":56092},{\"end\":56110,\"start\":56100},{\"end\":56130,\"start\":56110},{\"end\":56139,\"start\":56130},{\"end\":56150,\"start\":56139},{\"end\":56163,\"start\":56150},{\"end\":56172,\"start\":56163},{\"end\":56190,\"start\":56172},{\"end\":56198,\"start\":56190},{\"end\":56206,\"start\":56198},{\"end\":56518,\"start\":56511},{\"end\":56527,\"start\":56518},{\"end\":56535,\"start\":56527},{\"end\":56542,\"start\":56535},{\"end\":56550,\"start\":56542},{\"end\":56560,\"start\":56550},{\"end\":56566,\"start\":56560},{\"end\":56575,\"start\":56566},{\"end\":56583,\"start\":56575},{\"end\":56961,\"start\":56954},{\"end\":56969,\"start\":56961},{\"end\":56976,\"start\":56969},{\"end\":56985,\"start\":56976},{\"end\":56996,\"start\":56985},{\"end\":57008,\"start\":56996},{\"end\":57014,\"start\":57008},{\"end\":57023,\"start\":57014},{\"end\":57030,\"start\":57023},{\"end\":57040,\"start\":57030},{\"end\":57049,\"start\":57040},{\"end\":57057,\"start\":57049},{\"end\":57063,\"start\":57057},{\"end\":57072,\"start\":57063},{\"end\":57078,\"start\":57072},{\"end\":57085,\"start\":57078},{\"end\":57094,\"start\":57085},{\"end\":57103,\"start\":57094},{\"end\":57111,\"start\":57103},{\"end\":57120,\"start\":57111},{\"end\":57131,\"start\":57120},{\"end\":57146,\"start\":57131},{\"end\":57151,\"start\":57146},{\"end\":57155,\"start\":57151},{\"end\":58252,\"start\":58246},{\"end\":58258,\"start\":58252},{\"end\":58267,\"start\":58258},{\"end\":58274,\"start\":58267},{\"end\":58283,\"start\":58274},{\"end\":58290,\"start\":58283},{\"end\":58297,\"start\":58290},{\"end\":58301,\"start\":58297},{\"end\":59011,\"start\":59005},{\"end\":59018,\"start\":59011},{\"end\":59026,\"start\":59018},{\"end\":59037,\"start\":59026},{\"end\":59351,\"start\":59345},{\"end\":59359,\"start\":59351},{\"end\":59365,\"start\":59359},{\"end\":59373,\"start\":59365},{\"end\":59385,\"start\":59373},{\"end\":59391,\"start\":59385},{\"end\":59398,\"start\":59391},{\"end\":59402,\"start\":59398},{\"end\":59953,\"start\":59946},{\"end\":59962,\"start\":59953},{\"end\":59969,\"start\":59962},{\"end\":60126,\"start\":60118},{\"end\":60203,\"start\":60196},{\"end\":60211,\"start\":60203},{\"end\":60219,\"start\":60211},{\"end\":60226,\"start\":60219},{\"end\":60235,\"start\":60226},{\"end\":60243,\"start\":60235},{\"end\":60251,\"start\":60243},{\"end\":60257,\"start\":60251},{\"end\":60264,\"start\":60257},{\"end\":60271,\"start\":60264},{\"end\":60279,\"start\":60271},{\"end\":60285,\"start\":60279},{\"end\":60293,\"start\":60285},{\"end\":60302,\"start\":60293},{\"end\":60308,\"start\":60302},{\"end\":60315,\"start\":60308},{\"end\":60321,\"start\":60315},{\"end\":60327,\"start\":60321},{\"end\":60335,\"start\":60327},{\"end\":60343,\"start\":60335},{\"end\":60351,\"start\":60343},{\"end\":60360,\"start\":60351},{\"end\":60366,\"start\":60360},{\"end\":60375,\"start\":60366},{\"end\":60382,\"start\":60375},{\"end\":60388,\"start\":60382},{\"end\":60395,\"start\":60388},{\"end\":60403,\"start\":60395},{\"end\":60410,\"start\":60403},{\"end\":60417,\"start\":60410},{\"end\":60424,\"start\":60417},{\"end\":60431,\"start\":60424},{\"end\":60439,\"start\":60431},{\"end\":60446,\"start\":60439},{\"end\":60453,\"start\":60446},{\"end\":60992,\"start\":60983},{\"end\":61214,\"start\":61208},{\"end\":61223,\"start\":61214},{\"end\":61230,\"start\":61223},{\"end\":61832,\"start\":61826},{\"end\":61841,\"start\":61832},{\"end\":61850,\"start\":61841},{\"end\":61860,\"start\":61850},{\"end\":61871,\"start\":61860},{\"end\":61879,\"start\":61871},{\"end\":62176,\"start\":62166},{\"end\":62184,\"start\":62176},{\"end\":62191,\"start\":62184},{\"end\":62201,\"start\":62191},{\"end\":62210,\"start\":62201},{\"end\":62222,\"start\":62210},{\"end\":62234,\"start\":62222},{\"end\":62471,\"start\":62463},{\"end\":62478,\"start\":62471},{\"end\":62484,\"start\":62478},{\"end\":62492,\"start\":62484},{\"end\":62499,\"start\":62492},{\"end\":62507,\"start\":62499},{\"end\":62515,\"start\":62507},{\"end\":62521,\"start\":62515},{\"end\":62530,\"start\":62521},{\"end\":62537,\"start\":62530},{\"end\":62546,\"start\":62537},{\"end\":62552,\"start\":62546},{\"end\":62559,\"start\":62552},{\"end\":62567,\"start\":62559},{\"end\":62575,\"start\":62567},{\"end\":62582,\"start\":62575},{\"end\":62591,\"start\":62582},{\"end\":62599,\"start\":62591},{\"end\":62607,\"start\":62599},{\"end\":63092,\"start\":63076},{\"end\":63098,\"start\":63092}]", "bib_venue": "[{\"end\":41258,\"start\":41198},{\"end\":43732,\"start\":43620},{\"end\":48301,\"start\":48227},{\"end\":49154,\"start\":49065},{\"end\":52645,\"start\":52551},{\"end\":57386,\"start\":57284},{\"end\":58501,\"start\":58488},{\"end\":59504,\"start\":59473},{\"end\":61430,\"start\":61318},{\"end\":38325,\"start\":38314},{\"end\":38484,\"start\":38455},{\"end\":38905,\"start\":38789},{\"end\":39623,\"start\":39560},{\"end\":41196,\"start\":41121},{\"end\":41503,\"start\":41438},{\"end\":42222,\"start\":42177},{\"end\":42598,\"start\":42516},{\"end\":42882,\"start\":42815},{\"end\":43145,\"start\":43080},{\"end\":43618,\"start\":43532},{\"end\":44377,\"start\":44308},{\"end\":45789,\"start\":45744},{\"end\":46101,\"start\":46018},{\"end\":46489,\"start\":46401},{\"end\":46795,\"start\":46720},{\"end\":47068,\"start\":46976},{\"end\":47685,\"start\":47593},{\"end\":48225,\"start\":48136},{\"end\":48716,\"start\":48668},{\"end\":49063,\"start\":48986},{\"end\":49608,\"start\":49586},{\"end\":49829,\"start\":49771},{\"end\":49971,\"start\":49891},{\"end\":50336,\"start\":50299},{\"end\":50754,\"start\":50751},{\"end\":50960,\"start\":50904},{\"end\":51690,\"start\":51641},{\"end\":52110,\"start\":52052},{\"end\":52549,\"start\":52440},{\"end\":52991,\"start\":52962},{\"end\":53335,\"start\":53295},{\"end\":53733,\"start\":53689},{\"end\":55114,\"start\":55061},{\"end\":55530,\"start\":55468},{\"end\":55761,\"start\":55690},{\"end\":56211,\"start\":56206},{\"end\":56635,\"start\":56583},{\"end\":57241,\"start\":57155},{\"end\":58120,\"start\":58062},{\"end\":58486,\"start\":58301},{\"end\":59003,\"start\":58921},{\"end\":59471,\"start\":59402},{\"end\":59944,\"start\":59887},{\"end\":61008,\"start\":60992},{\"end\":61316,\"start\":61230},{\"end\":61928,\"start\":61879},{\"end\":62164,\"start\":62116},{\"end\":62672,\"start\":62607},{\"end\":63074,\"start\":62997}]"}}}, "year": 2023, "month": 12, "day": 17}