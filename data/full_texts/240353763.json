{"id": 240353763, "updated": "2023-10-05 20:07:53.977", "metadata": {"title": "Egocentric Human Trajectory Forecasting with a Wearable Camera and Multi-Modal Fusion", "authors": "[{\"first\":\"Jianing\",\"last\":\"Qiu\",\"middle\":[]},{\"first\":\"Lipeng\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Xiao\",\"last\":\"Gu\",\"middle\":[]},{\"first\":\"Frank\",\"last\":\"Lo\",\"middle\":[\"P.-W.\"]},{\"first\":\"Ya-Yen\",\"last\":\"Tsai\",\"middle\":[]},{\"first\":\"Jiankai\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Jiaqi\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Benny\",\"last\":\"Lo\",\"middle\":[]}]", "venue": "IEEE Robotics and Automation Letters, June, 2022", "journal": "IEEE Robotics and Automation Letters", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "In this paper, we address the problem of forecasting the trajectory of an egocentric camera wearer (ego-person) in crowded spaces. The trajectory forecasting ability learned from the data of different camera wearers walking around in the real world can be transferred to assist visually impaired people in navigation, as well as to instill human navigation behaviours in mobile robots, enabling better human-robot interactions. To this end, a novel egocentric human trajectory forecasting dataset was constructed, containing real trajectories of people navigating in crowded spaces wearing a camera, as well as extracted rich contextual data. We extract and utilize three different modalities to forecast the trajectory of the camera wearer, i.e., his/her past trajectory, the past trajectories of nearby people, and the environment such as the scene semantics or the depth of the scene. A Transformer-based encoder-decoder neural network model, integrated with a novel cascaded cross-attention mechanism that fuses multiple modalities, has been designed to predict the future trajectory of the camera wearer. Extensive experiments have been conducted, with results showing that our model outperforms the state-of-the-art methods in egocentric human trajectory forecasting.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2111.00993", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/ral/QiuCGLTSLL22", "doi": "10.1109/lra.2022.3188101"}}, "content": {"source": {"pdf_hash": "e3a8ea14fd7c7355ac4af43fcac99961dcfe2bfa", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2111.00993v3.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2111.00993", "status": "GREEN"}}, "grobid": {"id": "e10a7fb218d41a3c82d5722849c5ad785b1628f7", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e3a8ea14fd7c7355ac4af43fcac99961dcfe2bfa.txt", "contents": "\nEgocentric Human Trajectory Forecasting with a Wearable Camera and Multi-Modal Fusion\n\n\nJianing Qiu \nLipeng Chen \nXiao Gu \nFrank \nP.-W Lo \nYa-Yen Tsai \nJiankai Sun \nJiaqi Liu \nBenny Lo \nEgocentric Human Trajectory Forecasting with a Wearable Camera and Multi-Modal Fusion\nIEEE ROBOTICS AND AUTOMATION LETTERS. PREPRINT VERSION. ACCEPTED JUNE, 2022 1Index Terms-Human trajectory forecastingegocentric visionmulti-modal learning\nIn this paper, we address the problem of forecasting the trajectory of an egocentric camera wearer (ego-person) in crowded spaces. The trajectory forecasting ability learned from the data of different camera wearers walking around in the real world can be transferred to assist visually impaired people in navigation, as well as to instill human navigation behaviours in mobile robots, enabling better human-robot interactions. To this end, a novel egocentric human trajectory forecasting dataset was constructed, containing real trajectories of people navigating in crowded spaces wearing a camera, as well as extracted rich contextual data. We extract and utilize three different modalities to forecast the trajectory of the camera wearer, i.e., his/her past trajectory, the past trajectories of nearby people, and the environment such as the scene semantics or the depth of the scene. A Transformer-based encoder-decoder neural network model, integrated with a novel cascaded cross-attention mechanism that fuses multiple modalities, has been designed to predict the future trajectory of the camera wearer. Extensive experiments have been conducted, with results showing that our model outperforms the state-of-the-art methods in egocentric human trajectory forecasting.\n\nTransfer to Potential Applications\n(a) (b) (c) (d)\nSocially Compliant Robot Navigation Fig. 1: Illustration of the egocentric human trajectory forecasting. (a) Global view of the ego-person's trajectory: an ego-person (a person with a wearable camera) is walking around in a crowded space (e.g., in a shopping mall). The wearable camera, which has an egocentric view as shown in (b), continuously captures close-up details of the wearer's surroundings. In this work, we aim to predict the ego-person's trajectory by only using the contextual cues captured by the wearable camera and the ego-person's past trajectory. The learned egocentric trajectory forecasting ability can be transferred to downstream applications such as assisting blind navigation (c) and teaching robots to navigate in a socially compliant way (d).\n\n(learned from sighted people walking with a wearable camera) forecasting the optimal trajectory for navigation in the next few seconds, a blind or visually impaired person can navigate through a space independently in a safer and more natural way (Fig. 1c). With a set of synchronized sensors, the predicted optimal trajectory that the user shall follow can be communicated to him/her via vibration [4] or audio [7]. On the other hand, humans tend to unconsciously follow certain social rules, such as maintaining a comfortable distance with others, and keeping following the people in front of them if not in a rush while walking in crowded spaces. These socially polite ways of navigation are common in human navigation. For a mobile robot, if it can imitate such human navigation behaviours, the robot could better blend itself into humancentric environments while moving in shared spaces with humans (Fig. 1d). The human navigation behaviors distilled from the egocentric perception data of humans navigating in the real world therefore has the potential to be transferred to mobile robots, enabling socially compliant robot navigation [8]. In the context of navigation, egocentric trajectory forecasting is similar to motion planning. However, forecasting a camera wearer's trajectory can also find itself useful in other contexts such as AR/VR, by providing a user with a better interactive and immersive experience. However, such data is currently lacking and limited research has been carried out in forecasting the trajectory of the camera wearer [9], [10], [11], [12]. Most human trajectory forecasting methods are targeted for scenarios captured from a bird-eye view or by a static camera [13], [14], [15], [16], [17], [18], [19], [20], [21], [22]. Different from these views, egocentric view brings up the opportunity of understanding how humans perceive the surroundings, and initiate/execute the responsive actions. Such embodied knowledge fully considers the relationship between the agent itself and the close-up contextual information, such as the poses of nearby people and their trajectories, and the sizes of obstacles. These close-up details are crucial for forecasting a safe trajectory. Nevertheless, to the best of our knowledge, there is currently no effort that has been made to forecast the camera wearer's trajectory with these close-up contextual cues. From the perspective of applicability, it is also more practical to implement visual navigation assistance using an egocentric view than using a bird-eye or static camera view.\n\nIn light of these, in this work, we propose and address the task of forecasting the trajectory of an egocentric camera wearer in crowded spaces. Fig. 1a and Fig. 1b provide an illustration of this task. Specifically, given a period of observation, the task is to forecast the camera wearer's trajectory (expressed in the 3D world coordinates with both position and orientation) using cues available in the observation period. We adopt the past trajectory of the camera wearer as the basic input, and meanwhile, utilize the contextual information (as shown in Fig. 2), such as the trajectories of nearby people, and the scene semantics or the depth of the scene, to forecast the trajectory of the camera wearer. To this end, an encoder-decoder structure is proposed, in which multiple encoder streams are used to encode different input modalities, and a novel cascaded cross-attention mechanism is then used to fuse the resulting encodings. The decoder receives the fused encodings, and then decodes the future trajectory in an autoregressive manner. The effectiveness of using different modalities has been thoroughly studied in this work. We also implemented socially compliant navigation on a real robot to demonstrate the generalization capability of our model.\n\nIn a nutshell, the contributions of this work include:\n\n\u2022 A novel in-the-wild egocentric human trajectory forecasting dataset has been constructed. The dataset can be downloaded from https://github.com/Jianing-Qiu/TISS. \u2022 Four different types of modalities are proposed and their effectiveness has been evaluated for egocentric human trajectory forecasting. \u2022 A Transformer-based encoder-decoder framework integrated with a novel cascaded cross-attention mechanism has been designed to forecast the trajectory of the camera wearer in the egocentric setting.\n\n\nII. RELATED WORK\n\nIn this section, we review prior work in human trajectory prediction, including in both non-egocentric and egocentric scenarios.\n\n\nA. Non-Egocentric Human Trajectory Prediction\n\nMost human trajectory prediction studies are focused on scenes captured from a bird-eye view or by a static camera, i.e., non-egocentric scenarios, with targeted applications, such as intelligent surveillance systems and crowd behavior monitoring. The majority of the work considers modelling pedestrian walking dynamics to approach this task. Early work resorted to hand-craft features such as social force [23]. Recent studies have shifted to neural network-based approaches. Typically, these approaches consider modeling the social interactions between pedestrians to predict a pedestrian's trajectory [13], [14], [15], [16], or jointly modeling future trajectory with future activities [17] or with destinations [19]. Liang et al. [18] used multi-scale location decoders to predict multiple future trajectories. Shafiee et al. [20] proposed a conditional 3D attention mechanism for pedestrian trajectory prediction.\n\nTransformer [24] recently has established new state-of-thearts in a series of vision and language tasks. Few studies have also exploited the transformer-based architecture for pedestrian trajectory prediction. STAR [21] predicts pedestrian trajectories with only the attention mechanism, which is achieved by a graph-based spatial transformer and a temporal transformer. AgentFormer [22], a transformer-based framework, jointly models temporal and social dimensions in human motion dynamics to predict future trajectories. Our model is also based on transformer, but we differ from them in that 1) we target for egocentric scenarios, and 2) our model encodes multiple modalities with a novel cascaded cross-attention mechanism, whereas their models are designed to use the past trajectories as the only cue for the future trajectory prediction.\n\n\nB. Egocentric Human Trajectory Prediction\n\nIn egocentric scenarios, based on the types of used visual sensors, human trajectory prediction can be categorized into the vehicle-mounted camera scenario and the wearable camera scenario.\n\n1) Vehicle-Mounted Camera Scenario: In the field of autonomous driving, pedestrian trajectory prediction has been actively researched [25], [26], [27], [28], [29], [30]. Some research has also studied the prediction of a pedestrian's intent of crossing or not crossing the road [31], [32], and the detection of traffic accident by predicting pedestrians' future locations [33]. As the vehicles are mainly on the road, in the images/videos captured by a vehicle-mounted camera, the pedestrians often appear on the sidewalks, and therefore are mainly in the left or right section of the images/videos, or in the middle when the vehicle is at an intersection. These scenes hence differ from those captured by a wearable camera, as the places the camera wearer can visit and enter are different from vehicles and the locations of nearby people can be in close proximity to the camera wearer. For example, the camera wearer can walk in a long distance following the person in front, which can rarely be found in vehicle-mounted camera scenarios.\n\n2) Wearable Camera Scenario: Among all the works using wearable cameras, Yagi et al. [34] first proposed the task of future person localization in egocentric videos. In their work, a multi-stream 1D convolution-deconvolution (1D Conv-DeConv) network was designed to utilize a pedestrian's past poses, locations and scales, as well as ego-motion to infer his/her future locations. In [35], a GRU-CNN-based framework was proposed to conduct future person localization. An LSTM-based encoder-decoder framework was utilized in [36] to predict human trajectory in indoor environments. The above work, similar to those in the vehicle-mounted camera scenario, is focused on predicting the trajectories of the pedestrians captured by egocentric cameras, which is different from our work. Our work is to predict the trajectory of the camera wearer rather than those of pedestrians, which could benefit the blind navigation as well as the socially compliant robot navigation.\n\nNevertheless, little research has been carried out in predicting the camera wearer's trajectory. An early attempt was made by Park et al. [9], who used stereo images and an EgoRetinal map to predict the camera wearer's future path. Given a single egocentric image, a nearest-neighbor based search approach was used in [10] to predict the future trajectory. The search was based on the measured deep-feature similarity between images. Camera wearer trajectory prediction has also been studied in the context of playing basketball. Bertasius et al. [11] proposed to use a future CNN and a goal verifier network followed by an inverse synthesis procedure to estimate a basketball player's motion. In [12], egocentric videos were used and basketball players' trajectories were predicted by a retrievalbased method. Although our work also predicts the trajectory (a) Transformer-based encoderdecoder structure of the camera wearer, we differ from them in that 1) we model the interactions between the camera wearer and the people in his/her close vicinity in both crowded indoor and outdoor spaces; 2) rich visual features, such as scene semantics and depth information, are leveraged to aid trajectory prediction; 3) and without bells and whistles, our method, based on a simple encoder-decoder structure and a novel cascaded cross-attention mechanism, has shown to be effective in fusing multiple contextual cues and accurately predicting the trajectory of the camera wearer.\n!\"#$% & k v Sum \u00d7 \u00d7 q n k v !\"#$% & 1 1 1 1 1 k v Sum \u00d7 \u00d7 q n k v 1 1 1 1 1 Dot Product \u00d7 Multiplication !\"#$% && (b) Cascaded cross-attention\n\nIII. METHOD\n\n\nA. Problem Formulation\n\nWe formulate the task of predicting the trajectory of an egocentric camera wearer as to learn a function \u03c6 \u03b8 : (Y, Z) \u2192 Y, which takes the past trajectory Y of the camera wearer and some contextual cues Z as inputs, and outputs the camera wearer's future trajectory\u0176.\n\n\nB. Ego-Trajectory Forecasting with Multiple Modalities\n\nPeople navigate in crowded spaces with social etiquette, and the movement of each individual is influenced by those of the nearby people. Therefore, in predicting the trajectory of a camera wearer, the trajectories of people in his/her close vicinity can play an important role. Hence, we use the nearby people's trajectories J as one of the cues for forecasting the camera wearer's trajectory. In addition, understanding the scene semantics (e.g., walkable areas such as the floor, and obstacles such as walls) can also aid trajectory forecasting. We therefore use scene semantic segmentation S as another cue for camera wearer trajectory forecasting. Hence, we have\nZ = [J , S].\nWe have also experimented on the depth information, and used depth estimation D as an alternative to the scene semantic segmentation, i.e., Z = [J , D]. As the methodologies of processing and fusing scene segmentation and depth estimation with other two modalities are similar, in this section, we mainly focus on the use of the past trajectory Y of the \n1/ \u221a d k is the scaling factor. * / 1 E fused \u2190 E Y ; 2 E others \u2190 [E J , E S ]; 3 for E in E others do 4 E fused \u2190 LN(E fused ); 5 E fused \u2190 softmax(W q E fused (W k E) T / \u221a d k )W v E +E fused ; 6 end\ncamera wearer, those of nearby people J , and the scene semantic segmentation S. Experimental results of using the depth estimation will be present in Section IV-E2.\n\nConcretely, given a period of observation (past) T obs , the camera wearer's past trajectory Y \u2208 R Tobs\u00d77 is formed chronologically using the camera position (p \u2208 R Tobs\u00d73 ) and orientation (o \u2208 R Tobs\u00d74 , as quaternion) at each time point t \u2208 (t 0 \u2212 T obs + 1, ..., t 0 ) where t 0 is the end of the observation period. The past trajectories of nearby people J = N n=1 J n where N is the number of nearby people and J n \u2208 R Tobs\u00d7d is each nearby person's trajectory projected in the 2D pixel space (the viewpoint of the camera wearer) during the time period t \u2208 (t 0 \u2212 T obs + 1, ..., t 0 ), and d is the dimension of the cue we use to represent the nearby people's trajectories. We mainly use a nearby person's pose to form his/her trajectory, but the use of his/her center body point, as well as his/her bounding box has also been investigated. For scene semantic segmentation, we encode the corresponding segmentation mask at each time point in the observation period into a k-dimensional vector, i.e., S \u2208 R Tobs\u00d7k . Similarly, if the depth images are used, we have D \u2208 R Tobs\u00d7k .\n\nFinally, given a prediction period T pred , the future trajectory of the camera wearer\u0176 \u2208 R Tpred\u00d77 can be predicted using\n\u03c6 \u03b8 (Y, Z) where Z = [J , S] or Z = [J , D].\n\nC. Model Structure\n\nWe implement \u03c6 \u03b8 as a Transformer based encoder-decoder neural network model containing a novel cascaded crossattention mechanism to fuse encodings of different modalities. Fig. 3 shows the model structure. Each modality is encoded by a separate encoder stream, and each stream contains three Pre-LN Transformer layers [37]. After encoding, the inputs Y, J and S are transformed into E Y , E J , and E S , respectively. We then utilize a cascaded cross-attention mechanism (see Algorithm 1) to fuse them. The fused encoding E fused is then fed into the decoder. In inference, we use greedy decoding to decode the future trajectory, and based on the experiments, using greedy decoding in training is able to increase the trajectory prediction accuracy during inference. This increase is also observed in [22]. Therefore, in both training and inference, the decoder decodes the future trajectory of the camera wearer in a greedy autoregressive manner. L 2 loss is used during model training. We name our model as CXA-Transformer (Cascaded Cross-Attention Transformer).\n\n\nIV. EXPERIMENT\n\n\nA. Dataset\n\nA novel egocentric human trajectory forecasting dataset has been constructed. To the best of our knowledge, this new dataset is the first of its kind in the community.\n\n1) Data Collection: Camera wearers were wearing a GoPro Hero 9 Black camera while walking around in crowded spaces, such as a large shopping mall. Six unique areas were chosen to collect the data. Both indoor and outdoor scenarios were covered. For outdoor scenarios, data was collected in busy city areas where vehicles are not allowed to enter. The camera was mounted on the chest of the wearer and facing forward. The camera recorded egocentric videos with a resolution of 1920 \u00d7 1080 at 30 fps.\n\n2) Data Pre-Processing: As our data was collected in the real world, including both indoor and outdoor environments, it is not feasible to use GPS (considering it is not available in the indoor environment) and also not feasible to use motion capture systems such as Vicon, which are installed in a studio or a laboratory, to obtain the camera wearer's trajectories. Using SLAM becomes the option to obtain the camera wearer's trajectories under our data collection setting. ORB-SLAM3 [38] is one of cutting-edge SLAM algorithms that can precisely output an camera's locations in the 3D world. Its average absolute trajectory error is 0.041m tested on the EuRoC dataset [39]. We therefore ran ORB-SLAM3 [38] to obtain the ground truth trajectories of the camera wearer from the recorded egocentric monocular RGB videos. Note that the raw ground truth trajectories are only measured with reference to the map initialized by the ORB-SLAM3 algorithm. Their length is not in the real-world scale, but their shape is the same as in the real world. We then used ffmpeg to extract frames from the videos and set the resolution of each frame to 480 \u00d7 270. AlphaPose [40] was then used to detect and track the nearby people appearing in each frame. For each tracked nearby person, we used his/her detected pose in each frame to form his/her trajectory in the 2D pixel space. . It can be observed that the encodings learned from the designed autoencoder are discriminative between different trajectories, and the encodings of scene semantic segmentation are more discriminative than those of depth images, which is also confirmed by the experiments discussed in Section IV-E2\n\nWe also used the center of his/her neck and hip keypoints to represent his/her center body point in each frame. The center body point and the detected bounding box of a nearby person are used as alternatives to the pose to form his/her trajectory. PSPNet [41] was used to segment scene semantics. The parsed scene semantics of each frame is represented by a segmentation mask with the same size as the RGB frame. After segmentation masks were obtained, we trained an autoencoder to encode each mask into a k-dimensional vector. Monodepth2 [42] was used to estimate the depth from the monocular RGB frames. Similar to the segmentation mask, we trained a separate autoencoder to encode each depth image into a k-dimensional vector. The timestamps of the camera wearer's trajectory are aligned with those of video frames, and we reduce the frame rate to 2fps. Each trajectory of the camera wearer is 5 seconds long (i.e., 10 time points), and records a sequence of camera poses expressed in relative to the initial pose of that sequence.\n\nIn total, the dataset contains 12,492 unique trajectories, and we used 9,992 for training and the rest 2,500 for testing. We limited the maximum number of tracked nearby people to 5 in the dataset. Note that this is not the actual number of people in the environment. Fig. 4a shows the number of data samples with respect to the different number of tracked nearby people. We use t-SNE [43] to visualize the encoded vectors (by the respective autoencoder) of segmentation masks and depth images in Fig 5. We name our dataset as TISS (the initial of four institutions with which the authors are affiliated).\n\n\nB. Implementation Details\n\nOur model was implemented using PyTorch. We used 4 attention heads in the Pre-LN Transformer layers and the cascaded cross-attention module. Hidden layer dimension of the Pre-LN Transformer's feed forward sub-layers was set to 2,048. The embedding layers transform modalities of different dimensions to the same dimension of 512. The implementation of PSPNet and its pre-trained model were adopted from [44]. We set the observation time T obs to 3 time points (at 2fps, it is equal to 1.5 s) and the prediction time T pred to 7 time points (at 2fps, it is equal to 3.5 s). We used 26 body keypoints to represent a nearby person's pose. We used zero padding to pad those data samples which have the number of tracked nearby people less than 5. Therefore, d was set to 260 when body pose was used, and d was 10 and 20 respectively, if the center body point and the bounding box were used. k was set to 648. Each modality was normalized in order to accelerate the convergence of the model. We trained our model for 300 epochs using Adam optimization [45]. The batch size was set to 1,024 with a learning rate of 5e-5.\n\n\nC. Baselines\n\nWe compared the performance of egocentric human trajectory prediction of our CXA-Transformer model with the following baseline methods, all of which are able to take multiple modalities simultaneously as their inputs:\n\n\u2022 1D Conv-DeConv [34]: A multi-stream 1D convolutiondeconvolution framework originally proposed for future pedestrian localization in egocentric videos. We adapt it to be compatible with different modalities and their different dimensions in our dataset. \u2022 Triple-Stream LSTM: An LSTM-based encoderdecoder framework with three encoder streams and one decoder stream. Different input modalities are merged using two fully connected layers (one for hidden states, and the other for cell) before being fed into the decoder. \u2022 LIP-LSTM [36]: A single stream LSTM-based encoderdecoder framework originally proposed for predicting the location and movement trajectory of pedestrians captured in egocentric videos. We concatenate input modalities and then feed the resulting vector to its encoder. Future trajectory is then predicted by its decoder. The above neural network-based baselines were also trained with L 2 loss. Apart from them, we also implemented two simple trajectory predictors that take only the past trajectory as the input to predict the future trajectory. The first one is based on random sampling. We randomly sample the position difference and orientation difference between the time point t+1 and the time point t in the future based on the differences measured between the time points in the observation period T obs . The minimum and maximum values of those differences in the past are used as the minimum and maximum range for the sampling. The future trajectory is then obtained by iteratively adding the randomly sampled position and orientation differences to the end of the observed trajectory. The second one is a Gaussian Process Regression-based method (GPR), which was trained using the trajectories in the training set.\n\n\nD. Evaluation Metrics\n\nWe used MSE to measure the future trajectory prediction error of each method. We first measure the MSE error over  the entire predicted trajectory, denoted as E-MSE (i.e., 7 time points in the case of predicting 3.5s into the future). We also separately report the MSE error of position and orientation prediction, denoted as P-MSE and O-MSE, respectively. In addition, we also compared the performance of each method at different prediction horizons, i.e., 1.0s, 1.5s, 2.0s, 2.5s, and 3.0s. Finally, we also report the MSE error of the end point, i.e, the final prediction time point, which is denoted as F-MSE.\n\n\nE. Results\n\nWe first report the E-MSE error of the two simple trajectory predictors. We executed the random sampling one for 10 times, and its error is 5.93e-2\u00b12.5e-4. For the GPR model, its E-MSE error is 4.87e-2. We then compare our proposed method with the neural-network based baselines in the following section.\n\n1) Comparison with the State-of-the-Art: Table I summarizes the results of our method and the state-of-the-art neural network-based baselines on the task of egocentric human trajectory forecasting. Our CXA-Transformer achieves the lowest prediction error in terms of the estimated future position and orientation of the camera wearer. Its E-MSE error is nearly the half of the second best method's error. The F-MSE achieved by our model is also lower than those of baselines. Fig. 6 shows three examples of the predicted trajectories of different methods as well as the ground truth and the observed trajectory. It can be seen that the predicted trajectory of our model aligns better with the ground truth than those of baselines. Fig. 4b shows the results of predicting future trajectory at various horizons. The proposed CXA-Transformer shows better performance than the baselines at all different prediction horizons. It is worth noting that at very short prediction horizons, e.g., 1.0s, and 1.5s, the prediction error of the CXA-Transformer is not significantly lower than those of the baselines. However, as the prediction horizon increases, the  error of our method grows at a slower rate than those of the baselines.\n\n2) Ablation Studies: Different Fusion Mechanisms: We replaced the CXA fusion module in our model with average pooling, max pooling, or a linear layer followed by relu activation to fuse the encodings of different input modalities. As shown in Table II, our proposed CXA fusion mechanism leads to a better result compared to other traditional fusion mechanisms.\n\nDifferent Modalities: We further studied the effectiveness of each input modality in predicting egocentric human trajectory. Table III summarizes the results of using different number of modalities as the input to the CXA-Transformer. Compared to using the camera wearer's past trajectory Y as the only cue for prediction, adding the trajectories of nearby people (whether using their center body point C, bounding box B, or pose P), or the scene semantics S, or the depth information D can reduce the prediction error. When adding the cues of both the nearby people and the environment, the prediction error can be further reduced. The CXA-Transformer achieved the lowest prediction error with the Y, P, and S as its input. The prediction error of using the depth information is higher than that of using the scene semantics as the depth is less discriminative than the scene segmentation as shown in Fig. 5. We show in Fig. 7a the accuracy of the CXA-Transformer with different structure settings, i.e., varying the number of Pre-LN layers, the dimension of the output of the embedding layers and that of the feed-forward sub-layers, and also the number of attention heads.\n\n\nF. Transfer to Robot Navigation\n\nTo demonstrate that the egocentric trajectory forecasting ability learned from the sighted people navigating in the real world can be transferred to robots, enabling them to navigate in a socially compliant way, we conducted experiments on a real robot deployed in the crowd. The camera was mounted on top of the robot, which is about human chest height. Fig. 7 shows two examples. In Fig. 7b, although there existed a chance for the robot to overtake the two people it was following, it still chose to keep following the people for a few seconds, as the model for predicting trajectories was learned from sighted people navigating in the real world (we used the one trained with Y + P + S). As mentioned earlier, humans tend to follow certain social conventions while walking in the crowd. For example, in a scenario like the one shown in Fig 7b, instead of overtaking abruptly using the available space, which was in practice feasible, humans tended to keep following and not to disrupt the walking route of the people walking towards them. In this scenario, the robot well imitated such human behaviors and navigated in a socially polite way. In Fig 7c, the robot was moving in the opposite direction of the two people in its front. Our model predicted a safe trajectory for the robot to avoid collision with the people walking towards it.\n\n\nV. DISCUSSION\n\nWith a person navigating in crowded spaces wearing a camera for data collection, the nearby people's navigation behaviors are less likely to be affected. Therefore, our dataset can be used to understand and model human navigation behaviours more precisely in egocentric settings compared to the data collected by a mobile robot. Our proposed endto-end approach that utilizes multiple input modalities has also shown to be effective in forecasting the trajectory of the camera wearer. Although our experiments have shown that it is feasible to transfer human navigation behaviours to a mobile robot to enable socially compliant robot navigation, a successful transfer requires the robot to have a similar viewing angle and moving speed to the camera wearers who collected our dataset. Developing an effective adaptive method is needed especially if the robots have a different viewing angle. This is also applicable to assisting blind navigation, as a model learned from the sighted people data has to be adaptive to a different user, as different people have different heights, gaits, walking speeds, and reaction speeds. In addition, as our system directly forecasts the future trajectory that a blind person shall follow (this ability is learned from sighted people's walking data), the system has to be initialized well (i.e., the blind person should be guided walking like a sighted person at the very first few minutes in order to allow the system to forecast meaningful trajectories). We leave implementing a complete blind navigation system as our future work. Integrating egocentric human trajectory forecasting with AR/VR applications is also practically useful. Future research can also be carried out in this direction.\n\n\nVI. CONCLUSION\n\nIn this work, we address the problem of forecasting the trajectory of camera wearers in the egocentric setting. To this end, a novel in-the-wild egocentric human trajectory forecasting dataset has been constructed, containing trajectories of the camera wearer in the 3D space and trajectories of nearby people in the 2D pixel space, as well as the segmented scene semantics and the estimated depth of the environment captured by the wearable camera. A transformer-based encoder-decoder model has been designed with a novel cascaded cross-attention mechanism to fuse features of different modalities for egocentric trajectory forecasting. We have also demonstrated that the trajectory forecasting ability learned from sighted people's navigation data can be transferred to a mobile robot, enabling socially compliant robot navigation.\n\nFig. 2 :\n2Visual cues for forecasting the trajectory of the camera wearer. (Left) the past trajectories of nearby people (formed by using their poses at each time point. We have also investigated the use of their center body points or bounding boxes to form their trajectories in the 2D pixel space). (Middle) the segmented semantics of the scene at each time point in the observation period. (Right) the estimated depth of the scene at each time point in the observation period.\n\nFig. 3 :\n3(a) Overview of the model structure. The model is composed of three encoder streams, each with three Pre-LN Transformer layers to encode an input modality, and a single decoder stream to decode the future trajectory of the camera wearer in an autoregressive manner. A cascaded crossattention mechanism is designed to fuse the encodings of multiple modalities. (b) Illustration of the cascaded crossattention mechanism.\n\nFig. 4 :\n4(a) The number of data samples with respect to the different number of tracked nearby people. (b) Comparison between different methods on predicting the trajectory at different time horizons.\n\nFig. 5 :\n5t-SNE visualization of the encodings of scene semantic segmentation masks (a) and depth images (b). Each cluster is the encodings of the segmentation masks or the depth images of one trajectory in the observation period (best viewed in zoom in). Each encoding is represented by a dot. Same color means they are from the same trajectory. For visualization purpose, 1) we use 15 frames for each trajectory, i.e, 10fps, and the used frames of 2fps are within these 15 frames; 2) 200 samples are randomly chosen from the test set, as shown in (a) and (b)\n\nFig. 6 :\n6Visualization of the predicted trajectories of different methods, the ground truth, and the observed trajectory. The predicted trajectory of our CXA-Transformer better aligns with the ground truth compared to those of baselines. We show two views for each of the three examples. Left: x-z horizontal plane. Right: z-y vertical plane.\n\nFig. 7 :\n7(a) Accuracy of the CXA-Transformer with the different number of Pre-LN layers (NP), different embedding dimensions (DE), different dimensions of the feed-forward sub-layer (DF), and different number of attention heads (NA). Accuracy is reported with the use of Y + P + S as the input. X label is represented in the format of NP DE DF NA. (b)-(c) Our model predicts socially compliant trajectories for robot navigation. (b) Robot follows people in front instead of overtaking them. (c) Robot turns left to avoid collision with the people walking towards it.\n\n\nAlgorithm 1: Cascaded Cross-Attention Mechanism Data: E Y , E J , and E S Result: E fused : the fused encodings of all input modalities. / * LN is Layer Normalization. W q , W k , and W v are learnable matrices.\n\nTABLE I :\nIComparison with the State-of-the-Art Methods \n\nMethod \nP-MSE \nO-MSE \nE-MSE \nF-MSE \n\n1D Conv-DeConv [34] \n7.09e-2 \n1.77e-3 \n3.14e-2 8.46e-2 \nTriple-Stream LSTM \n6.40e-2 \n7.37e-4 \n2.78e-2 6.59e-2 \nLIP-LSTM [36] \n6.67e-2 \n4.54e-4 \n2.89e-2 6.69e-2 \n\nCXA-Transformer (Ours) 3.54e-2 \n4.27e-4 \n1.54e-2 4.20e-2 \n\n\n\nTABLE II :\nIIComparison between the Proposed Cascaded Cross-Attention (CXA) and Other Fusion MechanismsFusion Mechanism \nP-MSE \nO-MSE \nE-MSE \n\nAverage Pooling \n5.79e-2 \n5.83e-4 \n2.51e-2 \nMax Pooling \n4.85e-2 \n5.86e-4 \n2.11e-2 \nLinear Layer \n4.61e-2 \n5.12e-4 \n2.00e-2 \n\nCXA (ours) \n3.54e-2 \n4.27e-4 \n1.54e-2 \n\n\n\nTABLE III :\nIIIAblation Studies of Using Different Types of Modalities as the Input to CXA-TransformerModality \nP-MSE \nO-MSE \nE-MSE \n\nY \n1.06e-1 \n7.10e-4 \n4.60e-2 \n\nY + C \n1.01e-1 \n6.56e-4 \n4.36e-2 \nY + B \n9.28e-2 \n6.78e-4 \n4.01e-2 \nY + P \n9.06e-2 \n6.31e-4 \n3.92e-2 \nY + S \n6.23e-2 \n6.00e-4 \n2.70e-2 \nY + D \n7.02e-2 \n6.66e-4 \n3.05e-2 \n\nY + C + S \n3.73e-2 \n4.57e-4 \n1.63e-2 \nY + B + S \n3.61e-2 \n4.38e-4 \n1.57e-2 \nY + P + S \n3.54e-2 \n4.27e-4 \n1.54e-2 \nY + C + D \n4.93e-2 \n5.26e-4 \n2.14e-2 \nY + B + D \n4.76e-2 \n5.36e-4 \n2.07e-2 \nY + P + D \n4.81e-2 \n5.37e-4 \n2.09e-2 \n\n\n\nMotion planning among dynamic, decision-making agents with deep reinforcement learning. M Everett, Y F Chen, J P How, 2018M. Everett, Y. F. Chen, and J. P. How, \"Motion planning among dynamic, decision-making agents with deep reinforcement learning,\" in 2018\n\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEEIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEE, 2018, pp. 3052-3059.\n\nCrowd-robot interaction: Crowd-aware robot navigation with attention-based deep reinforcement learning. C Chen, Y Liu, S Kreiss, A Alahi, 2019 International Conference on Robotics and Automation (ICRA). IEEEC. Chen, Y. Liu, S. Kreiss, and A. Alahi, \"Crowd-robot interaction: Crowd-aware robot navigation with attention-based deep reinforcement learning,\" in 2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019, pp. 6015-6022.\n\nMultiple trajectory prediction of moving agents with memory augmented networks. F Marchetti, F Becattini, L Seidenari, A Del Bimbo, IEEE Transactions on Pattern Analysis and Machine Intelligence. F. Marchetti, F. Becattini, L. Seidenari, and A. Del Bimbo, \"Multiple tra- jectory prediction of moving agents with memory augmented networks,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.\n\nEnabling independent navigation for visually impaired people through a wearable vision-based feedback system. H.-C Wang, R K Katzschmann, S Teng, B Araki, L Giarr\u00e9, D Rus, 2017 IEEE international conference on robotics and automation (ICRA). IEEEH.-C. Wang, R. K. Katzschmann, S. Teng, B. Araki, L. Giarr\u00e9, and D. Rus, \"Enabling independent navigation for visually impaired people through a wearable vision-based feedback system,\" in 2017 IEEE international conference on robotics and automation (ICRA). IEEE, 2017, pp. 6533-6540.\n\nPersonalized dynamics models for adaptive assistive navigation systems. E Ohnbar, K Kitani, C Asakawa, Conference on Robot Learning. PMLR. E. OhnBar, K. Kitani, and C. Asakawa, \"Personalized dynamics models for adaptive assistive navigation systems,\" in Conference on Robot Learning. PMLR, 2018, pp. 16-39.\n\nDesigning ar visualizations to facilitate stair navigation for people with low vision. Y Zhao, E Kupferstein, B V Castro, S Feiner, S Azenkot, Proceedings of the 32nd annual ACM symposium on user interface software and technology. the 32nd annual ACM symposium on user interface software and technologyY. Zhao, E. Kupferstein, B. V. Castro, S. Feiner, and S. Azenkot, \"Designing ar visualizations to facilitate stair navigation for people with low vision,\" in Proceedings of the 32nd annual ACM symposium on user interface software and technology, 2019, pp. 387-402.\n\nProject guideline: Enabling those with low vision to run independently. google, \"Project guideline: Enabling those with low vision to run independently,\" 2021.\n\nSocially compliant mobile robot navigation via inverse reinforcement learning. H Kretzschmar, M Spies, C Sprunk, W Burgard, The International Journal of Robotics Research. 3511H. Kretzschmar, M. Spies, C. Sprunk, and W. Burgard, \"Socially compliant mobile robot navigation via inverse reinforcement learning,\" The International Journal of Robotics Research, vol. 35, no. 11, pp. 1289-1307, 2016.\n\nEgocentric future localization. H S Park, J.-J Hwang, Y Niu, J Shi, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionH. S. Park, J.-J. Hwang, Y. Niu, and J. Shi, \"Egocentric future localiza- tion,\" in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 4697-4705.\n\nKrishnacam: Using a longitudinal, single-person, egocentric dataset for scene understanding tasks. K K Singh, K Fatahalian, A A Efros, 2016 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEEK. K. Singh, K. Fatahalian, and A. A. Efros, \"Krishnacam: Using a longitudinal, single-person, egocentric dataset for scene understanding tasks,\" in 2016 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2016, pp. 1-9.\n\nEgocentric basketball motion planning from a single first-person image. G Bertasius, A Chan, J Shi, CVPR. G. Bertasius, A. Chan, and J. Shi, \"Egocentric basketball motion planning from a single first-person image,\" in CVPR, 2018, pp. 5889- 5898.\n\nPredicting behaviors of basketball players from first person videos. S Su, J Hong, J Shi, H. Soo Park, S. Su, J. Pyo Hong, J. Shi, and H. Soo Park, \"Predicting behaviors of basketball players from first person videos,\" in CVPR, 2017, pp. 1501- 1510.\n\nSocial lstm: Human trajectory prediction in crowded spaces. A Alahi, K Goel, V Ramanathan, A Robicquet, L Fei-Fei, S Savarese, CVPR. A. Alahi, K. Goel, V. Ramanathan, A. Robicquet, L. Fei-Fei, and S. Savarese, \"Social lstm: Human trajectory prediction in crowded spaces,\" in CVPR, 2016, pp. 961-971.\n\nSocial gan: Socially acceptable trajectories with generative adversarial networks. A Gupta, J Johnson, L Fei-Fei, S Savarese, A Alahi, CVPR. A. Gupta, J. Johnson, L. Fei-Fei, S. Savarese, and A. Alahi, \"Social gan: Socially acceptable trajectories with generative adversarial networks,\" in CVPR, 2018, pp. 2255-2264.\n\nSophie: An attentive gan for predicting paths compliant to social and physical constraints. A Sadeghian, V Kosaraju, A Sadeghian, N Hirose, H Rezatofighi, S Savarese, CVPR. A. Sadeghian, V. Kosaraju, A. Sadeghian, N. Hirose, H. Rezatofighi, and S. Savarese, \"Sophie: An attentive gan for predicting paths compliant to social and physical constraints,\" in CVPR, 2019, pp. 1349-1358.\n\nSocial-bigat: Multimodal trajectory forecasting using bicycle-gan and graph attention networks. V Kosaraju, A Sadeghian, R Mart\u00edn-Mart\u00edn, I Reid, S H Rezatofighi, S Savarese, arXiv:1907.03395arXiv preprintV. Kosaraju, A. Sadeghian, R. Mart\u00edn-Mart\u00edn, I. Reid, S. H. Rezatofighi, and S. Savarese, \"Social-bigat: Multimodal trajectory forecasting using bicycle-gan and graph attention networks,\" arXiv preprint arXiv:1907.03395, 2019.\n\nPeeking into the future: Predicting future person activities and locations in videos. J Liang, L Jiang, J C Niebles, A G Hauptmann, L Fei-Fei, CVPR. J. Liang, L. Jiang, J. C. Niebles, A. G. Hauptmann, and L. Fei-Fei, \"Peeking into the future: Predicting future person activities and locations in videos,\" in CVPR, 2019, pp. 5725-5734.\n\nThe garden of forking paths: Towards multi-future trajectory prediction. J Liang, L Jiang, K Murphy, T Yu, A Hauptmann, CVPR. J. Liang, L. Jiang, K. Murphy, T. Yu, and A. Hauptmann, \"The garden of forking paths: Towards multi-future trajectory prediction,\" in CVPR, 2020, pp. 10 508-10 518.\n\nIt is not the journey but the destination: Endpoint conditioned trajectory prediction. K Mangalam, H Girase, S Agarwal, K.-H Lee, E Adeli, J Malik, A Gaidon, ECCV. SpringerK. Mangalam, H. Girase, S. Agarwal, K.-H. Lee, E. Adeli, J. Malik, and A. Gaidon, \"It is not the journey but the destination: Endpoint conditioned trajectory prediction,\" in ECCV. Springer, 2020, pp. 759- 776.\n\nIntrovert: Human trajectory prediction via conditional 3d attention. N Shafiee, T Padir, E Elhamifar, CVPR. N. Shafiee, T. Padir, and E. Elhamifar, \"Introvert: Human trajectory prediction via conditional 3d attention,\" in CVPR, 2021, pp. 16 815- 16 825.\n\nSpatio-temporal graph transformer networks for pedestrian trajectory prediction. C Yu, X Ma, J Ren, H Zhao, S Yi, ECCV. SpringerC. Yu, X. Ma, J. Ren, H. Zhao, and S. Yi, \"Spatio-temporal graph transformer networks for pedestrian trajectory prediction,\" in ECCV. Springer, 2020, pp. 507-523.\n\nAgentformer: Agent-aware transformers for socio-temporal multi-agent forecasting. Y Yuan, X Weng, Y Ou, K Kitani, arXiv:2103.14023arXiv preprintY. Yuan, X. Weng, Y. Ou, and K. Kitani, \"Agentformer: Agent-aware transformers for socio-temporal multi-agent forecasting,\" arXiv preprint arXiv:2103.14023, 2021.\n\nSocial force model for pedestrian dynamics. D Helbing, P Molnar, Physical review E. 5154282D. Helbing and P. Molnar, \"Social force model for pedestrian dynamics,\" Physical review E, vol. 51, no. 5, p. 4282, 1995.\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, \u0141 Kaiser, I Polosukhin, Advances in neural information processing systems. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \"Attention is all you need,\" in Advances in neural information processing systems, 2017, pp. 5998-6008.\n\nContext-based pedestrian path prediction. J F P Kooij, N Schneider, F Flohr, D M Gavrila, ECCV. SpringerJ. F. P. Kooij, N. Schneider, F. Flohr, and D. M. Gavrila, \"Context-based pedestrian path prediction,\" in ECCV. Springer, 2014, pp. 618-633.\n\nLong-term on-board prediction of people in traffic scenes under uncertainty. A Bhattacharyya, M Fritz, B Schiele, CVPR. A. Bhattacharyya, M. Fritz, and B. Schiele, \"Long-term on-board prediction of people in traffic scenes under uncertainty,\" in CVPR, 2018, pp. 4194-4202.\n\nForecasting pedestrian trajectory with machine-annotated training data. O Styles, A Ross, V Sanchez, 2019 IEEE Intelligent Vehicles Symposium (IV). IEEEO. Styles, A. Ross, and V. Sanchez, \"Forecasting pedestrian trajectory with machine-annotated training data,\" in 2019 IEEE Intelligent Vehicles Symposium (IV). IEEE, 2019, pp. 716-721.\n\nTraphic: Trajectory prediction in dense and heterogeneous traffic using weighted interactions. R Chandra, U Bhattacharya, A Bera, D Manocha, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionR. Chandra, U. Bhattacharya, A. Bera, and D. Manocha, \"Traphic: Trajectory prediction in dense and heterogeneous traffic using weighted interactions,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 8483-8492.\n\nMultimodal future localization and emergence prediction for objects in egocentric view with a reachability prior. O Makansi, O Cicek, K Buchicchio, T Brox, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionO. Makansi, O. Cicek, K. Buchicchio, and T. Brox, \"Multimodal future localization and emergence prediction for objects in egocentric view with a reachability prior,\" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 4354-4363.\n\nDisentangling human dynamics for pedestrian locomotion forecasting with noisy supervision. K Mangalam, E Adeli, K.-H Lee, A Gaidon, J C Niebles, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer VisionK. Mangalam, E. Adeli, K.-H. Lee, A. Gaidon, and J. C. Niebles, \"Dis- entangling human dynamics for pedestrian locomotion forecasting with noisy supervision,\" in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2020, pp. 2784-2793.\n\nClassifying pedestrian actions in advance using predicted video of urban driving scenes. P Gujjar, R Vaughan, 2019 International Conference on Robotics and Automation (ICRA). IEEEP. Gujjar and R. Vaughan, \"Classifying pedestrian actions in advance using predicted video of urban driving scenes,\" in 2019 International Conference on Robotics and Automation (ICRA). IEEE, 2019, pp. 2097-2103.\n\nSpatiotemporal relationship reasoning for pedestrian intent prediction. B Liu, E Adeli, Z Cao, K.-H Lee, A Shenoi, A Gaidon, J C Niebles, IEEE Robotics and Automation Letters. 52B. Liu, E. Adeli, Z. Cao, K.-H. Lee, A. Shenoi, A. Gaidon, and J. C. Niebles, \"Spatiotemporal relationship reasoning for pedestrian intent prediction,\" IEEE Robotics and Automation Letters, vol. 5, no. 2, pp. 3485-3492, 2020.\n\nUnsupervised traffic accident detection in first-person videos. Y Yao, M Xu, Y Wang, D J Crandall, E M Atkins, 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). Y. Yao, M. Xu, Y. Wang, D. J. Crandall, and E. M. Atkins, \"Unsuper- vised traffic accident detection in first-person videos,\" in 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS).\n\n. IEEE. IEEE, 2019, pp. 273-280.\n\nFuture person localization in first-person videos. T Yagi, K Mangalam, R Yonetani, Y Sato, CVPR. T. Yagi, K. Mangalam, R. Yonetani, and Y. Sato, \"Future person localization in first-person videos,\" in CVPR, June 2018.\n\nMultiple object forecasting: Predicting future object locations in diverse environments. O Styles, V Sanchez, T Guha, Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. the IEEE/CVF Winter Conference on Applications of Computer VisionO. Styles, V. Sanchez, and T. Guha, \"Multiple object forecasting: Pre- dicting future object locations in diverse environments,\" in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2020, pp. 690-699.\n\nIndoor future person localization from an egocentric wearable camera. J Qiu, F P Lo, X Gu, Y Sun, S Jiang, B Lo, IROS. J. Qiu, F. P.-W. Lo, X. Gu, Y. Sun, S. Jiang, and B. Lo, \"Indoor future person localization from an egocentric wearable camera,\" IROS, 2021.\n\nOn layer normalization in the transformer architecture. R Xiong, Y Yang, D He, K Zheng, S Zheng, C Xing, H Zhang, Y Lan, L Wang, T Liu, International Conference on Machine Learning. PMLR, 2020. R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L. Wang, and T. Liu, \"On layer normalization in the trans- former architecture,\" in International Conference on Machine Learning. PMLR, 2020, pp. 10 524-10 533.\n\nOrb-slam3: An accurate open-source library for visual, visual-inertial, and multimap slam. C Campos, R Elvira, J J G Rodr\u00edguez, J M Montiel, J D Tard\u00f3s, IEEE Transactions on Robotics. C. Campos, R. Elvira, J. J. G. Rodr\u00edguez, J. M. Montiel, and J. D. Tard\u00f3s, \"Orb-slam3: An accurate open-source library for visual, visual-inertial, and multimap slam,\" IEEE Transactions on Robotics, 2021.\n\nThe euroc micro aerial vehicle datasets. M Burri, J Nikolic, P Gohl, T Schneider, J Rehder, S Omari, M W Achtelik, R Siegwart, The International Journal of Robotics Research. 3510M. Burri, J. Nikolic, P. Gohl, T. Schneider, J. Rehder, S. Omari, M. W. Achtelik, and R. Siegwart, \"The euroc micro aerial vehicle datasets,\" The International Journal of Robotics Research, vol. 35, no. 10, pp. 1157-1163, 2016.\n\nPose Flow: Efficient online pose tracking. Y Xiu, J Li, H Wang, Y Fang, C Lu, BMVC. Y. Xiu, J. Li, H. Wang, Y. Fang, and C. Lu, \"Pose Flow: Efficient online pose tracking,\" in BMVC, 2018.\n\nPyramid scene parsing network. H Zhao, J Shi, X Qi, X Wang, J Jia, H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, \"Pyramid scene parsing network,\" in CVPR, 2017, pp. 2881-2890.\n\nDigging into self-supervised monocular depth prediction. C Godard, O Mac Aodha, M Firman, G J Brostow, C. Godard, O. Mac Aodha, M. Firman, and G. J. Brostow, \"Digging into self-supervised monocular depth prediction,\" October 2019.\n\nVisualizing data using t-sne. L Van Der Maaten, G Hinton, Journal of machine learning research. 911L. Van der Maaten and G. Hinton, \"Visualizing data using t-sne.\" Journal of machine learning research, vol. 9, no. 11, 2008.\n\nSemantic understanding of scenes through the ade20k dataset. B Zhou, H Zhao, X Puig, T Xiao, S Fidler, A Barriuso, A Torralba, International Journal on Computer Vision. B. Zhou, H. Zhao, X. Puig, T. Xiao, S. Fidler, A. Barriuso, and A. Torralba, \"Semantic understanding of scenes through the ade20k dataset,\" International Journal on Computer Vision, 2018.\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980arXiv preprintD. P. Kingma and J. Ba, \"Adam: A method for stochastic optimization,\" arXiv preprint arXiv:1412.6980, 2014.\n", "annotations": {"author": "[{\"end\":101,\"start\":89},{\"end\":114,\"start\":102},{\"end\":123,\"start\":115},{\"end\":130,\"start\":124},{\"end\":139,\"start\":131},{\"end\":152,\"start\":140},{\"end\":165,\"start\":153},{\"end\":176,\"start\":166},{\"end\":186,\"start\":177}]", "publisher": null, "author_last_name": "[{\"end\":100,\"start\":97},{\"end\":113,\"start\":109},{\"end\":122,\"start\":120},{\"end\":138,\"start\":136},{\"end\":151,\"start\":147},{\"end\":164,\"start\":161},{\"end\":175,\"start\":172},{\"end\":185,\"start\":183}]", "author_first_name": "[{\"end\":96,\"start\":89},{\"end\":108,\"start\":102},{\"end\":119,\"start\":115},{\"end\":129,\"start\":124},{\"end\":135,\"start\":131},{\"end\":146,\"start\":140},{\"end\":160,\"start\":153},{\"end\":171,\"start\":166},{\"end\":182,\"start\":177}]", "author_affiliation": null, "title": "[{\"end\":86,\"start\":1},{\"end\":272,\"start\":187}]", "venue": null, "abstract": "[{\"end\":1701,\"start\":428}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2927,\"start\":2924},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2940,\"start\":2937},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3668,\"start\":3665},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4084,\"start\":4081},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4090,\"start\":4086},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4096,\"start\":4092},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4102,\"start\":4098},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4229,\"start\":4225},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4235,\"start\":4231},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4241,\"start\":4237},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4247,\"start\":4243},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4253,\"start\":4249},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4259,\"start\":4255},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4265,\"start\":4261},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4271,\"start\":4267},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4277,\"start\":4273},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4283,\"start\":4279},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7519,\"start\":7515},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7716,\"start\":7712},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7722,\"start\":7718},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7728,\"start\":7724},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7734,\"start\":7730},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7801,\"start\":7797},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7827,\"start\":7823},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7846,\"start\":7842},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7942,\"start\":7938},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8044,\"start\":8040},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8247,\"start\":8243},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8415,\"start\":8411},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9247,\"start\":9243},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9253,\"start\":9249},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9259,\"start\":9255},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9265,\"start\":9261},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9271,\"start\":9267},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9277,\"start\":9273},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9391,\"start\":9387},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9397,\"start\":9393},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9485,\"start\":9481},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":10240,\"start\":10236},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10538,\"start\":10534},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10678,\"start\":10674},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11259,\"start\":11256},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11440,\"start\":11436},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11669,\"start\":11665},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11819,\"start\":11815},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":16105,\"start\":16101},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":16589,\"start\":16585},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":18038,\"start\":18034},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":18223,\"start\":18219},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":18256,\"start\":18252},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":18711,\"start\":18707},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":19475,\"start\":19471},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":19759,\"start\":19755},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":20641,\"start\":20637},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":21294,\"start\":21290},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":21938,\"start\":21934},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":22258,\"start\":22254},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":22773,\"start\":22769}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":32165,\"start\":31685},{\"attributes\":{\"id\":\"fig_2\"},\"end\":32595,\"start\":32166},{\"attributes\":{\"id\":\"fig_3\"},\"end\":32798,\"start\":32596},{\"attributes\":{\"id\":\"fig_4\"},\"end\":33360,\"start\":32799},{\"attributes\":{\"id\":\"fig_5\"},\"end\":33705,\"start\":33361},{\"attributes\":{\"id\":\"fig_6\"},\"end\":34274,\"start\":33706},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":34488,\"start\":34275},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":34806,\"start\":34489},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":35117,\"start\":34807},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":35684,\"start\":35118}]", "paragraph": "[{\"end\":2523,\"start\":1754},{\"end\":5083,\"start\":2525},{\"end\":6349,\"start\":5085},{\"end\":6405,\"start\":6351},{\"end\":6908,\"start\":6407},{\"end\":7057,\"start\":6929},{\"end\":8026,\"start\":7107},{\"end\":8872,\"start\":8028},{\"end\":9107,\"start\":8918},{\"end\":10149,\"start\":9109},{\"end\":11116,\"start\":10151},{\"end\":12590,\"start\":11118},{\"end\":13040,\"start\":12773},{\"end\":13766,\"start\":13099},{\"end\":14134,\"start\":13780},{\"end\":14504,\"start\":14339},{\"end\":15591,\"start\":14506},{\"end\":15715,\"start\":15593},{\"end\":16848,\"start\":15782},{\"end\":17047,\"start\":16880},{\"end\":17547,\"start\":17049},{\"end\":19214,\"start\":17549},{\"end\":20250,\"start\":19216},{\"end\":20857,\"start\":20252},{\"end\":22001,\"start\":20887},{\"end\":22235,\"start\":22018},{\"end\":23984,\"start\":22237},{\"end\":24622,\"start\":24010},{\"end\":24941,\"start\":24637},{\"end\":26167,\"start\":24943},{\"end\":26529,\"start\":26169},{\"end\":27706,\"start\":26531},{\"end\":29084,\"start\":27742},{\"end\":30832,\"start\":29102},{\"end\":31684,\"start\":30851}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":1753,\"start\":1738},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12733,\"start\":12591},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13779,\"start\":13767},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14338,\"start\":14135},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15760,\"start\":15716}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":24991,\"start\":24984},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26420,\"start\":26412},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26665,\"start\":26656}]", "section_header": "[{\"end\":1737,\"start\":1703},{\"end\":6927,\"start\":6911},{\"end\":7105,\"start\":7060},{\"end\":8916,\"start\":8875},{\"end\":12746,\"start\":12735},{\"end\":12771,\"start\":12749},{\"end\":13097,\"start\":13043},{\"end\":15780,\"start\":15762},{\"end\":16865,\"start\":16851},{\"end\":16878,\"start\":16868},{\"end\":20885,\"start\":20860},{\"end\":22016,\"start\":22004},{\"end\":24008,\"start\":23987},{\"end\":24635,\"start\":24625},{\"end\":27740,\"start\":27709},{\"end\":29100,\"start\":29087},{\"end\":30849,\"start\":30835},{\"end\":31694,\"start\":31686},{\"end\":32175,\"start\":32167},{\"end\":32605,\"start\":32597},{\"end\":32808,\"start\":32800},{\"end\":33370,\"start\":33362},{\"end\":33715,\"start\":33707},{\"end\":34499,\"start\":34490},{\"end\":34818,\"start\":34808},{\"end\":35130,\"start\":35119}]", "table": "[{\"end\":34806,\"start\":34501},{\"end\":35117,\"start\":34911},{\"end\":35684,\"start\":35221}]", "figure_caption": "[{\"end\":32165,\"start\":31696},{\"end\":32595,\"start\":32177},{\"end\":32798,\"start\":32607},{\"end\":33360,\"start\":32810},{\"end\":33705,\"start\":33372},{\"end\":34274,\"start\":33717},{\"end\":34488,\"start\":34277},{\"end\":34911,\"start\":34821},{\"end\":35221,\"start\":35134}]", "figure_ref": "[{\"end\":1796,\"start\":1790},{\"end\":2781,\"start\":2772},{\"end\":3438,\"start\":3429},{\"end\":5237,\"start\":5230},{\"end\":5249,\"start\":5242},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5651,\"start\":5644},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15961,\"start\":15955},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20527,\"start\":20520},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20755,\"start\":20749},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":25425,\"start\":25419},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25681,\"start\":25674},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":27439,\"start\":27433},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":27459,\"start\":27452},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":28103,\"start\":28097},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":28134,\"start\":28127},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":28589,\"start\":28582},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":28898,\"start\":28891}]", "bib_author_first_name": "[{\"end\":35775,\"start\":35774},{\"end\":35786,\"start\":35785},{\"end\":35788,\"start\":35787},{\"end\":35796,\"start\":35795},{\"end\":35798,\"start\":35797},{\"end\":36235,\"start\":36234},{\"end\":36243,\"start\":36242},{\"end\":36250,\"start\":36249},{\"end\":36260,\"start\":36259},{\"end\":36662,\"start\":36661},{\"end\":36675,\"start\":36674},{\"end\":36688,\"start\":36687},{\"end\":36701,\"start\":36700},{\"end\":36705,\"start\":36702},{\"end\":37106,\"start\":37102},{\"end\":37114,\"start\":37113},{\"end\":37116,\"start\":37115},{\"end\":37131,\"start\":37130},{\"end\":37139,\"start\":37138},{\"end\":37148,\"start\":37147},{\"end\":37158,\"start\":37157},{\"end\":37597,\"start\":37596},{\"end\":37607,\"start\":37606},{\"end\":37617,\"start\":37616},{\"end\":37920,\"start\":37919},{\"end\":37928,\"start\":37927},{\"end\":37943,\"start\":37942},{\"end\":37945,\"start\":37944},{\"end\":37955,\"start\":37954},{\"end\":37965,\"start\":37964},{\"end\":38641,\"start\":38640},{\"end\":38656,\"start\":38655},{\"end\":38665,\"start\":38664},{\"end\":38675,\"start\":38674},{\"end\":38991,\"start\":38990},{\"end\":38993,\"start\":38992},{\"end\":39004,\"start\":39000},{\"end\":39013,\"start\":39012},{\"end\":39020,\"start\":39019},{\"end\":39452,\"start\":39451},{\"end\":39454,\"start\":39453},{\"end\":39463,\"start\":39462},{\"end\":39477,\"start\":39476},{\"end\":39479,\"start\":39478},{\"end\":39877,\"start\":39876},{\"end\":39890,\"start\":39889},{\"end\":39898,\"start\":39897},{\"end\":40121,\"start\":40120},{\"end\":40127,\"start\":40126},{\"end\":40135,\"start\":40134},{\"end\":40147,\"start\":40141},{\"end\":40363,\"start\":40362},{\"end\":40372,\"start\":40371},{\"end\":40380,\"start\":40379},{\"end\":40394,\"start\":40393},{\"end\":40407,\"start\":40406},{\"end\":40418,\"start\":40417},{\"end\":40687,\"start\":40686},{\"end\":40696,\"start\":40695},{\"end\":40707,\"start\":40706},{\"end\":40718,\"start\":40717},{\"end\":40730,\"start\":40729},{\"end\":41014,\"start\":41013},{\"end\":41027,\"start\":41026},{\"end\":41039,\"start\":41038},{\"end\":41052,\"start\":41051},{\"end\":41062,\"start\":41061},{\"end\":41077,\"start\":41076},{\"end\":41401,\"start\":41400},{\"end\":41413,\"start\":41412},{\"end\":41426,\"start\":41425},{\"end\":41443,\"start\":41442},{\"end\":41451,\"start\":41450},{\"end\":41453,\"start\":41452},{\"end\":41468,\"start\":41467},{\"end\":41824,\"start\":41823},{\"end\":41833,\"start\":41832},{\"end\":41842,\"start\":41841},{\"end\":41844,\"start\":41843},{\"end\":41855,\"start\":41854},{\"end\":41857,\"start\":41856},{\"end\":41870,\"start\":41869},{\"end\":42147,\"start\":42146},{\"end\":42156,\"start\":42155},{\"end\":42165,\"start\":42164},{\"end\":42175,\"start\":42174},{\"end\":42181,\"start\":42180},{\"end\":42453,\"start\":42452},{\"end\":42465,\"start\":42464},{\"end\":42475,\"start\":42474},{\"end\":42489,\"start\":42485},{\"end\":42496,\"start\":42495},{\"end\":42505,\"start\":42504},{\"end\":42514,\"start\":42513},{\"end\":42818,\"start\":42817},{\"end\":42829,\"start\":42828},{\"end\":42838,\"start\":42837},{\"end\":43085,\"start\":43084},{\"end\":43091,\"start\":43090},{\"end\":43097,\"start\":43096},{\"end\":43104,\"start\":43103},{\"end\":43112,\"start\":43111},{\"end\":43378,\"start\":43377},{\"end\":43386,\"start\":43385},{\"end\":43394,\"start\":43393},{\"end\":43400,\"start\":43399},{\"end\":43648,\"start\":43647},{\"end\":43659,\"start\":43658},{\"end\":43845,\"start\":43844},{\"end\":43856,\"start\":43855},{\"end\":43867,\"start\":43866},{\"end\":43877,\"start\":43876},{\"end\":43890,\"start\":43889},{\"end\":43899,\"start\":43898},{\"end\":43901,\"start\":43900},{\"end\":43910,\"start\":43909},{\"end\":43920,\"start\":43919},{\"end\":44234,\"start\":44233},{\"end\":44238,\"start\":44235},{\"end\":44247,\"start\":44246},{\"end\":44260,\"start\":44259},{\"end\":44269,\"start\":44268},{\"end\":44271,\"start\":44270},{\"end\":44515,\"start\":44514},{\"end\":44532,\"start\":44531},{\"end\":44541,\"start\":44540},{\"end\":44784,\"start\":44783},{\"end\":44794,\"start\":44793},{\"end\":44802,\"start\":44801},{\"end\":45145,\"start\":45144},{\"end\":45156,\"start\":45155},{\"end\":45172,\"start\":45171},{\"end\":45180,\"start\":45179},{\"end\":45713,\"start\":45712},{\"end\":45724,\"start\":45723},{\"end\":45733,\"start\":45732},{\"end\":45747,\"start\":45746},{\"end\":46269,\"start\":46268},{\"end\":46281,\"start\":46280},{\"end\":46293,\"start\":46289},{\"end\":46300,\"start\":46299},{\"end\":46310,\"start\":46309},{\"end\":46312,\"start\":46311},{\"end\":46825,\"start\":46824},{\"end\":46835,\"start\":46834},{\"end\":47200,\"start\":47199},{\"end\":47207,\"start\":47206},{\"end\":47216,\"start\":47215},{\"end\":47226,\"start\":47222},{\"end\":47233,\"start\":47232},{\"end\":47243,\"start\":47242},{\"end\":47253,\"start\":47252},{\"end\":47255,\"start\":47254},{\"end\":47597,\"start\":47596},{\"end\":47604,\"start\":47603},{\"end\":47610,\"start\":47609},{\"end\":47618,\"start\":47617},{\"end\":47620,\"start\":47619},{\"end\":47632,\"start\":47631},{\"end\":47634,\"start\":47633},{\"end\":48021,\"start\":48020},{\"end\":48029,\"start\":48028},{\"end\":48041,\"start\":48040},{\"end\":48053,\"start\":48052},{\"end\":48278,\"start\":48277},{\"end\":48288,\"start\":48287},{\"end\":48299,\"start\":48298},{\"end\":48758,\"start\":48757},{\"end\":48765,\"start\":48764},{\"end\":48767,\"start\":48766},{\"end\":48773,\"start\":48772},{\"end\":48779,\"start\":48778},{\"end\":48786,\"start\":48785},{\"end\":48795,\"start\":48794},{\"end\":49005,\"start\":49004},{\"end\":49014,\"start\":49013},{\"end\":49022,\"start\":49021},{\"end\":49028,\"start\":49027},{\"end\":49037,\"start\":49036},{\"end\":49046,\"start\":49045},{\"end\":49054,\"start\":49053},{\"end\":49063,\"start\":49062},{\"end\":49070,\"start\":49069},{\"end\":49078,\"start\":49077},{\"end\":49469,\"start\":49468},{\"end\":49479,\"start\":49478},{\"end\":49489,\"start\":49488},{\"end\":49493,\"start\":49490},{\"end\":49506,\"start\":49505},{\"end\":49508,\"start\":49507},{\"end\":49519,\"start\":49518},{\"end\":49521,\"start\":49520},{\"end\":49809,\"start\":49808},{\"end\":49818,\"start\":49817},{\"end\":49829,\"start\":49828},{\"end\":49837,\"start\":49836},{\"end\":49850,\"start\":49849},{\"end\":49860,\"start\":49859},{\"end\":49869,\"start\":49868},{\"end\":49871,\"start\":49870},{\"end\":49883,\"start\":49882},{\"end\":50219,\"start\":50218},{\"end\":50226,\"start\":50225},{\"end\":50232,\"start\":50231},{\"end\":50240,\"start\":50239},{\"end\":50248,\"start\":50247},{\"end\":50396,\"start\":50395},{\"end\":50404,\"start\":50403},{\"end\":50411,\"start\":50410},{\"end\":50417,\"start\":50416},{\"end\":50425,\"start\":50424},{\"end\":50598,\"start\":50597},{\"end\":50608,\"start\":50607},{\"end\":50612,\"start\":50609},{\"end\":50621,\"start\":50620},{\"end\":50631,\"start\":50630},{\"end\":50633,\"start\":50632},{\"end\":50803,\"start\":50802},{\"end\":50821,\"start\":50820},{\"end\":51059,\"start\":51058},{\"end\":51067,\"start\":51066},{\"end\":51075,\"start\":51074},{\"end\":51083,\"start\":51082},{\"end\":51091,\"start\":51090},{\"end\":51101,\"start\":51100},{\"end\":51113,\"start\":51112},{\"end\":51400,\"start\":51399},{\"end\":51402,\"start\":51401},{\"end\":51412,\"start\":51411}]", "bib_author_last_name": "[{\"end\":35783,\"start\":35776},{\"end\":35793,\"start\":35789},{\"end\":35802,\"start\":35799},{\"end\":36240,\"start\":36236},{\"end\":36247,\"start\":36244},{\"end\":36257,\"start\":36251},{\"end\":36266,\"start\":36261},{\"end\":36672,\"start\":36663},{\"end\":36685,\"start\":36676},{\"end\":36698,\"start\":36689},{\"end\":36711,\"start\":36706},{\"end\":37111,\"start\":37107},{\"end\":37128,\"start\":37117},{\"end\":37136,\"start\":37132},{\"end\":37145,\"start\":37140},{\"end\":37155,\"start\":37149},{\"end\":37162,\"start\":37159},{\"end\":37604,\"start\":37598},{\"end\":37614,\"start\":37608},{\"end\":37625,\"start\":37618},{\"end\":37925,\"start\":37921},{\"end\":37940,\"start\":37929},{\"end\":37952,\"start\":37946},{\"end\":37962,\"start\":37956},{\"end\":37973,\"start\":37966},{\"end\":38653,\"start\":38642},{\"end\":38662,\"start\":38657},{\"end\":38672,\"start\":38666},{\"end\":38683,\"start\":38676},{\"end\":38998,\"start\":38994},{\"end\":39010,\"start\":39005},{\"end\":39017,\"start\":39014},{\"end\":39024,\"start\":39021},{\"end\":39460,\"start\":39455},{\"end\":39474,\"start\":39464},{\"end\":39485,\"start\":39480},{\"end\":39887,\"start\":39878},{\"end\":39895,\"start\":39891},{\"end\":39902,\"start\":39899},{\"end\":40124,\"start\":40122},{\"end\":40132,\"start\":40128},{\"end\":40139,\"start\":40136},{\"end\":40152,\"start\":40148},{\"end\":40369,\"start\":40364},{\"end\":40377,\"start\":40373},{\"end\":40391,\"start\":40381},{\"end\":40404,\"start\":40395},{\"end\":40415,\"start\":40408},{\"end\":40427,\"start\":40419},{\"end\":40693,\"start\":40688},{\"end\":40704,\"start\":40697},{\"end\":40715,\"start\":40708},{\"end\":40727,\"start\":40719},{\"end\":40736,\"start\":40731},{\"end\":41024,\"start\":41015},{\"end\":41036,\"start\":41028},{\"end\":41049,\"start\":41040},{\"end\":41059,\"start\":41053},{\"end\":41074,\"start\":41063},{\"end\":41086,\"start\":41078},{\"end\":41410,\"start\":41402},{\"end\":41423,\"start\":41414},{\"end\":41440,\"start\":41427},{\"end\":41448,\"start\":41444},{\"end\":41465,\"start\":41454},{\"end\":41477,\"start\":41469},{\"end\":41830,\"start\":41825},{\"end\":41839,\"start\":41834},{\"end\":41852,\"start\":41845},{\"end\":41867,\"start\":41858},{\"end\":41878,\"start\":41871},{\"end\":42153,\"start\":42148},{\"end\":42162,\"start\":42157},{\"end\":42172,\"start\":42166},{\"end\":42178,\"start\":42176},{\"end\":42191,\"start\":42182},{\"end\":42462,\"start\":42454},{\"end\":42472,\"start\":42466},{\"end\":42483,\"start\":42476},{\"end\":42493,\"start\":42490},{\"end\":42502,\"start\":42497},{\"end\":42511,\"start\":42506},{\"end\":42521,\"start\":42515},{\"end\":42826,\"start\":42819},{\"end\":42835,\"start\":42830},{\"end\":42848,\"start\":42839},{\"end\":43088,\"start\":43086},{\"end\":43094,\"start\":43092},{\"end\":43101,\"start\":43098},{\"end\":43109,\"start\":43105},{\"end\":43115,\"start\":43113},{\"end\":43383,\"start\":43379},{\"end\":43391,\"start\":43387},{\"end\":43397,\"start\":43395},{\"end\":43407,\"start\":43401},{\"end\":43656,\"start\":43649},{\"end\":43666,\"start\":43660},{\"end\":43853,\"start\":43846},{\"end\":43864,\"start\":43857},{\"end\":43874,\"start\":43868},{\"end\":43887,\"start\":43878},{\"end\":43896,\"start\":43891},{\"end\":43907,\"start\":43902},{\"end\":43917,\"start\":43911},{\"end\":43931,\"start\":43921},{\"end\":44244,\"start\":44239},{\"end\":44257,\"start\":44248},{\"end\":44266,\"start\":44261},{\"end\":44279,\"start\":44272},{\"end\":44529,\"start\":44516},{\"end\":44538,\"start\":44533},{\"end\":44549,\"start\":44542},{\"end\":44791,\"start\":44785},{\"end\":44799,\"start\":44795},{\"end\":44810,\"start\":44803},{\"end\":45153,\"start\":45146},{\"end\":45169,\"start\":45157},{\"end\":45177,\"start\":45173},{\"end\":45188,\"start\":45181},{\"end\":45721,\"start\":45714},{\"end\":45730,\"start\":45725},{\"end\":45744,\"start\":45734},{\"end\":45752,\"start\":45748},{\"end\":46278,\"start\":46270},{\"end\":46287,\"start\":46282},{\"end\":46297,\"start\":46294},{\"end\":46307,\"start\":46301},{\"end\":46320,\"start\":46313},{\"end\":46832,\"start\":46826},{\"end\":46843,\"start\":46836},{\"end\":47204,\"start\":47201},{\"end\":47213,\"start\":47208},{\"end\":47220,\"start\":47217},{\"end\":47230,\"start\":47227},{\"end\":47240,\"start\":47234},{\"end\":47250,\"start\":47244},{\"end\":47263,\"start\":47256},{\"end\":47601,\"start\":47598},{\"end\":47607,\"start\":47605},{\"end\":47615,\"start\":47611},{\"end\":47629,\"start\":47621},{\"end\":47641,\"start\":47635},{\"end\":48026,\"start\":48022},{\"end\":48038,\"start\":48030},{\"end\":48050,\"start\":48042},{\"end\":48058,\"start\":48054},{\"end\":48285,\"start\":48279},{\"end\":48296,\"start\":48289},{\"end\":48304,\"start\":48300},{\"end\":48762,\"start\":48759},{\"end\":48770,\"start\":48768},{\"end\":48776,\"start\":48774},{\"end\":48783,\"start\":48780},{\"end\":48792,\"start\":48787},{\"end\":48798,\"start\":48796},{\"end\":49011,\"start\":49006},{\"end\":49019,\"start\":49015},{\"end\":49025,\"start\":49023},{\"end\":49034,\"start\":49029},{\"end\":49043,\"start\":49038},{\"end\":49051,\"start\":49047},{\"end\":49060,\"start\":49055},{\"end\":49067,\"start\":49064},{\"end\":49075,\"start\":49071},{\"end\":49082,\"start\":49079},{\"end\":49476,\"start\":49470},{\"end\":49486,\"start\":49480},{\"end\":49503,\"start\":49494},{\"end\":49516,\"start\":49509},{\"end\":49528,\"start\":49522},{\"end\":49815,\"start\":49810},{\"end\":49826,\"start\":49819},{\"end\":49834,\"start\":49830},{\"end\":49847,\"start\":49838},{\"end\":49857,\"start\":49851},{\"end\":49866,\"start\":49861},{\"end\":49880,\"start\":49872},{\"end\":49892,\"start\":49884},{\"end\":50223,\"start\":50220},{\"end\":50229,\"start\":50227},{\"end\":50237,\"start\":50233},{\"end\":50245,\"start\":50241},{\"end\":50251,\"start\":50249},{\"end\":50401,\"start\":50397},{\"end\":50408,\"start\":50405},{\"end\":50414,\"start\":50412},{\"end\":50422,\"start\":50418},{\"end\":50429,\"start\":50426},{\"end\":50605,\"start\":50599},{\"end\":50618,\"start\":50613},{\"end\":50628,\"start\":50622},{\"end\":50641,\"start\":50634},{\"end\":50818,\"start\":50804},{\"end\":50828,\"start\":50822},{\"end\":51064,\"start\":51060},{\"end\":51072,\"start\":51068},{\"end\":51080,\"start\":51076},{\"end\":51088,\"start\":51084},{\"end\":51098,\"start\":51092},{\"end\":51110,\"start\":51102},{\"end\":51122,\"start\":51114},{\"end\":51409,\"start\":51403},{\"end\":51415,\"start\":51413}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":35944,\"start\":35686},{\"attributes\":{\"id\":\"b1\"},\"end\":36128,\"start\":35946},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":52812470},\"end\":36579,\"start\":36130},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":220977460},\"end\":36990,\"start\":36581},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":3913693},\"end\":37522,\"start\":36992},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":52937320},\"end\":37830,\"start\":37524},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":204811963},\"end\":38398,\"start\":37832},{\"attributes\":{\"id\":\"b7\"},\"end\":38559,\"start\":38400},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":14275694},\"end\":38956,\"start\":38561},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":2843608},\"end\":39350,\"start\":38958},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":7938367},\"end\":39802,\"start\":39352},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":3665577},\"end\":40049,\"start\":39804},{\"attributes\":{\"id\":\"b12\"},\"end\":40300,\"start\":40051},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":9854676},\"end\":40601,\"start\":40302},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":4461350},\"end\":40919,\"start\":40603},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":46938830},\"end\":41302,\"start\":40921},{\"attributes\":{\"doi\":\"arXiv:1907.03395\",\"id\":\"b16\"},\"end\":41735,\"start\":41304},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":60440525},\"end\":42071,\"start\":41737},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":209370535},\"end\":42363,\"start\":42073},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":214802170},\"end\":42746,\"start\":42365},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":235692915},\"end\":43001,\"start\":42748},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":218673505},\"end\":43293,\"start\":43003},{\"attributes\":{\"doi\":\"arXiv:2103.14023\",\"id\":\"b22\"},\"end\":43601,\"start\":43295},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":29333691},\"end\":43815,\"start\":43603},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":13756489},\"end\":44189,\"start\":43817},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":243737},\"end\":44435,\"start\":44191},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":19515346},\"end\":44709,\"start\":44437},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":149445703},\"end\":45047,\"start\":44711},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":54481852},\"end\":45596,\"start\":45049},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":219531853},\"end\":46175,\"start\":45598},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":207870260},\"end\":46733,\"start\":46177},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":199541048},\"end\":47125,\"start\":46735},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":211205148},\"end\":47530,\"start\":47127},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":67855477},\"end\":47933,\"start\":47532},{\"attributes\":{\"id\":\"b34\"},\"end\":47967,\"start\":47935},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":4406882},\"end\":48186,\"start\":47969},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":202889302},\"end\":48685,\"start\":48188},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":232147799},\"end\":48946,\"start\":48687},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":211082816},\"end\":49375,\"start\":48948},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":220713377},\"end\":49765,\"start\":49377},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":9999787},\"end\":50173,\"start\":49767},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":3654694},\"end\":50362,\"start\":50175},{\"attributes\":{\"id\":\"b42\"},\"end\":50538,\"start\":50364},{\"attributes\":{\"id\":\"b43\"},\"end\":50770,\"start\":50540},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":5855042},\"end\":50995,\"start\":50772},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":11371972},\"end\":51353,\"start\":50997},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b46\"},\"end\":51553,\"start\":51355}]", "bib_title": "[{\"end\":36232,\"start\":36130},{\"end\":36659,\"start\":36581},{\"end\":37100,\"start\":36992},{\"end\":37594,\"start\":37524},{\"end\":37917,\"start\":37832},{\"end\":38638,\"start\":38561},{\"end\":38988,\"start\":38958},{\"end\":39449,\"start\":39352},{\"end\":39874,\"start\":39804},{\"end\":40360,\"start\":40302},{\"end\":40684,\"start\":40603},{\"end\":41011,\"start\":40921},{\"end\":41821,\"start\":41737},{\"end\":42144,\"start\":42073},{\"end\":42450,\"start\":42365},{\"end\":42815,\"start\":42748},{\"end\":43082,\"start\":43003},{\"end\":43645,\"start\":43603},{\"end\":43842,\"start\":43817},{\"end\":44231,\"start\":44191},{\"end\":44512,\"start\":44437},{\"end\":44781,\"start\":44711},{\"end\":45142,\"start\":45049},{\"end\":45710,\"start\":45598},{\"end\":46266,\"start\":46177},{\"end\":46822,\"start\":46735},{\"end\":47197,\"start\":47127},{\"end\":47594,\"start\":47532},{\"end\":48018,\"start\":47969},{\"end\":48275,\"start\":48188},{\"end\":48755,\"start\":48687},{\"end\":49002,\"start\":48948},{\"end\":49466,\"start\":49377},{\"end\":49806,\"start\":49767},{\"end\":50216,\"start\":50175},{\"end\":50800,\"start\":50772},{\"end\":51056,\"start\":50997}]", "bib_author": "[{\"end\":35785,\"start\":35774},{\"end\":35795,\"start\":35785},{\"end\":35804,\"start\":35795},{\"end\":36242,\"start\":36234},{\"end\":36249,\"start\":36242},{\"end\":36259,\"start\":36249},{\"end\":36268,\"start\":36259},{\"end\":36674,\"start\":36661},{\"end\":36687,\"start\":36674},{\"end\":36700,\"start\":36687},{\"end\":36713,\"start\":36700},{\"end\":37113,\"start\":37102},{\"end\":37130,\"start\":37113},{\"end\":37138,\"start\":37130},{\"end\":37147,\"start\":37138},{\"end\":37157,\"start\":37147},{\"end\":37164,\"start\":37157},{\"end\":37606,\"start\":37596},{\"end\":37616,\"start\":37606},{\"end\":37627,\"start\":37616},{\"end\":37927,\"start\":37919},{\"end\":37942,\"start\":37927},{\"end\":37954,\"start\":37942},{\"end\":37964,\"start\":37954},{\"end\":37975,\"start\":37964},{\"end\":38655,\"start\":38640},{\"end\":38664,\"start\":38655},{\"end\":38674,\"start\":38664},{\"end\":38685,\"start\":38674},{\"end\":39000,\"start\":38990},{\"end\":39012,\"start\":39000},{\"end\":39019,\"start\":39012},{\"end\":39026,\"start\":39019},{\"end\":39462,\"start\":39451},{\"end\":39476,\"start\":39462},{\"end\":39487,\"start\":39476},{\"end\":39889,\"start\":39876},{\"end\":39897,\"start\":39889},{\"end\":39904,\"start\":39897},{\"end\":40126,\"start\":40120},{\"end\":40134,\"start\":40126},{\"end\":40141,\"start\":40134},{\"end\":40154,\"start\":40141},{\"end\":40371,\"start\":40362},{\"end\":40379,\"start\":40371},{\"end\":40393,\"start\":40379},{\"end\":40406,\"start\":40393},{\"end\":40417,\"start\":40406},{\"end\":40429,\"start\":40417},{\"end\":40695,\"start\":40686},{\"end\":40706,\"start\":40695},{\"end\":40717,\"start\":40706},{\"end\":40729,\"start\":40717},{\"end\":40738,\"start\":40729},{\"end\":41026,\"start\":41013},{\"end\":41038,\"start\":41026},{\"end\":41051,\"start\":41038},{\"end\":41061,\"start\":41051},{\"end\":41076,\"start\":41061},{\"end\":41088,\"start\":41076},{\"end\":41412,\"start\":41400},{\"end\":41425,\"start\":41412},{\"end\":41442,\"start\":41425},{\"end\":41450,\"start\":41442},{\"end\":41467,\"start\":41450},{\"end\":41479,\"start\":41467},{\"end\":41832,\"start\":41823},{\"end\":41841,\"start\":41832},{\"end\":41854,\"start\":41841},{\"end\":41869,\"start\":41854},{\"end\":41880,\"start\":41869},{\"end\":42155,\"start\":42146},{\"end\":42164,\"start\":42155},{\"end\":42174,\"start\":42164},{\"end\":42180,\"start\":42174},{\"end\":42193,\"start\":42180},{\"end\":42464,\"start\":42452},{\"end\":42474,\"start\":42464},{\"end\":42485,\"start\":42474},{\"end\":42495,\"start\":42485},{\"end\":42504,\"start\":42495},{\"end\":42513,\"start\":42504},{\"end\":42523,\"start\":42513},{\"end\":42828,\"start\":42817},{\"end\":42837,\"start\":42828},{\"end\":42850,\"start\":42837},{\"end\":43090,\"start\":43084},{\"end\":43096,\"start\":43090},{\"end\":43103,\"start\":43096},{\"end\":43111,\"start\":43103},{\"end\":43117,\"start\":43111},{\"end\":43385,\"start\":43377},{\"end\":43393,\"start\":43385},{\"end\":43399,\"start\":43393},{\"end\":43409,\"start\":43399},{\"end\":43658,\"start\":43647},{\"end\":43668,\"start\":43658},{\"end\":43855,\"start\":43844},{\"end\":43866,\"start\":43855},{\"end\":43876,\"start\":43866},{\"end\":43889,\"start\":43876},{\"end\":43898,\"start\":43889},{\"end\":43909,\"start\":43898},{\"end\":43919,\"start\":43909},{\"end\":43933,\"start\":43919},{\"end\":44246,\"start\":44233},{\"end\":44259,\"start\":44246},{\"end\":44268,\"start\":44259},{\"end\":44281,\"start\":44268},{\"end\":44531,\"start\":44514},{\"end\":44540,\"start\":44531},{\"end\":44551,\"start\":44540},{\"end\":44793,\"start\":44783},{\"end\":44801,\"start\":44793},{\"end\":44812,\"start\":44801},{\"end\":45155,\"start\":45144},{\"end\":45171,\"start\":45155},{\"end\":45179,\"start\":45171},{\"end\":45190,\"start\":45179},{\"end\":45723,\"start\":45712},{\"end\":45732,\"start\":45723},{\"end\":45746,\"start\":45732},{\"end\":45754,\"start\":45746},{\"end\":46280,\"start\":46268},{\"end\":46289,\"start\":46280},{\"end\":46299,\"start\":46289},{\"end\":46309,\"start\":46299},{\"end\":46322,\"start\":46309},{\"end\":46834,\"start\":46824},{\"end\":46845,\"start\":46834},{\"end\":47206,\"start\":47199},{\"end\":47215,\"start\":47206},{\"end\":47222,\"start\":47215},{\"end\":47232,\"start\":47222},{\"end\":47242,\"start\":47232},{\"end\":47252,\"start\":47242},{\"end\":47265,\"start\":47252},{\"end\":47603,\"start\":47596},{\"end\":47609,\"start\":47603},{\"end\":47617,\"start\":47609},{\"end\":47631,\"start\":47617},{\"end\":47643,\"start\":47631},{\"end\":48028,\"start\":48020},{\"end\":48040,\"start\":48028},{\"end\":48052,\"start\":48040},{\"end\":48060,\"start\":48052},{\"end\":48287,\"start\":48277},{\"end\":48298,\"start\":48287},{\"end\":48306,\"start\":48298},{\"end\":48764,\"start\":48757},{\"end\":48772,\"start\":48764},{\"end\":48778,\"start\":48772},{\"end\":48785,\"start\":48778},{\"end\":48794,\"start\":48785},{\"end\":48800,\"start\":48794},{\"end\":49013,\"start\":49004},{\"end\":49021,\"start\":49013},{\"end\":49027,\"start\":49021},{\"end\":49036,\"start\":49027},{\"end\":49045,\"start\":49036},{\"end\":49053,\"start\":49045},{\"end\":49062,\"start\":49053},{\"end\":49069,\"start\":49062},{\"end\":49077,\"start\":49069},{\"end\":49084,\"start\":49077},{\"end\":49478,\"start\":49468},{\"end\":49488,\"start\":49478},{\"end\":49505,\"start\":49488},{\"end\":49518,\"start\":49505},{\"end\":49530,\"start\":49518},{\"end\":49817,\"start\":49808},{\"end\":49828,\"start\":49817},{\"end\":49836,\"start\":49828},{\"end\":49849,\"start\":49836},{\"end\":49859,\"start\":49849},{\"end\":49868,\"start\":49859},{\"end\":49882,\"start\":49868},{\"end\":49894,\"start\":49882},{\"end\":50225,\"start\":50218},{\"end\":50231,\"start\":50225},{\"end\":50239,\"start\":50231},{\"end\":50247,\"start\":50239},{\"end\":50253,\"start\":50247},{\"end\":50403,\"start\":50395},{\"end\":50410,\"start\":50403},{\"end\":50416,\"start\":50410},{\"end\":50424,\"start\":50416},{\"end\":50431,\"start\":50424},{\"end\":50607,\"start\":50597},{\"end\":50620,\"start\":50607},{\"end\":50630,\"start\":50620},{\"end\":50643,\"start\":50630},{\"end\":50820,\"start\":50802},{\"end\":50830,\"start\":50820},{\"end\":51066,\"start\":51058},{\"end\":51074,\"start\":51066},{\"end\":51082,\"start\":51074},{\"end\":51090,\"start\":51082},{\"end\":51100,\"start\":51090},{\"end\":51112,\"start\":51100},{\"end\":51124,\"start\":51112},{\"end\":51411,\"start\":51399},{\"end\":51417,\"start\":51411}]", "bib_venue": "[{\"end\":35772,\"start\":35686},{\"end\":36020,\"start\":35946},{\"end\":36331,\"start\":36268},{\"end\":36775,\"start\":36713},{\"end\":37232,\"start\":37164},{\"end\":37661,\"start\":37627},{\"end\":38061,\"start\":37975},{\"end\":38470,\"start\":38400},{\"end\":38731,\"start\":38685},{\"end\":39103,\"start\":39026},{\"end\":39556,\"start\":39487},{\"end\":39908,\"start\":39904},{\"end\":40118,\"start\":40051},{\"end\":40433,\"start\":40429},{\"end\":40742,\"start\":40738},{\"end\":41092,\"start\":41088},{\"end\":41398,\"start\":41304},{\"end\":41884,\"start\":41880},{\"end\":42197,\"start\":42193},{\"end\":42527,\"start\":42523},{\"end\":42854,\"start\":42850},{\"end\":43121,\"start\":43117},{\"end\":43375,\"start\":43295},{\"end\":43685,\"start\":43668},{\"end\":43982,\"start\":43933},{\"end\":44285,\"start\":44281},{\"end\":44555,\"start\":44551},{\"end\":44857,\"start\":44812},{\"end\":45271,\"start\":45190},{\"end\":45835,\"start\":45754},{\"end\":46402,\"start\":46322},{\"end\":46908,\"start\":46845},{\"end\":47301,\"start\":47265},{\"end\":47722,\"start\":47643},{\"end\":47941,\"start\":47937},{\"end\":48064,\"start\":48060},{\"end\":48386,\"start\":48306},{\"end\":48804,\"start\":48800},{\"end\":49140,\"start\":49084},{\"end\":49559,\"start\":49530},{\"end\":49940,\"start\":49894},{\"end\":50257,\"start\":50253},{\"end\":50393,\"start\":50364},{\"end\":50595,\"start\":50540},{\"end\":50866,\"start\":50830},{\"end\":51164,\"start\":51124},{\"end\":51397,\"start\":51355},{\"end\":38134,\"start\":38063},{\"end\":39167,\"start\":39105},{\"end\":45339,\"start\":45273},{\"end\":45903,\"start\":45837},{\"end\":46469,\"start\":46404},{\"end\":48453,\"start\":48388}]"}}}, "year": 2023, "month": 12, "day": 17}