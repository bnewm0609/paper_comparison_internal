{"id": 209450900, "updated": "2022-12-23 14:22:46.046", "metadata": {"title": "Abstractive Text Summarization Using Pointer-Generator Networks With Pre-trained Word Embedding", "authors": "[{\"first\":\"Dang\",\"last\":\"Anh\",\"middle\":[\"Trung\"]},{\"first\":\"Nguyen\",\"last\":\"Trang\",\"middle\":[\"Thi Thu\"]}]", "venue": "SoICT 2019", "journal": "Proceedings of the 10th International Symposium on Information and Communication Technology", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Abstractive text summarization is the task of generating a summary that captures the main content of a text document. As a state-of-the-art method for abstractive summarization, the pointer-generator network produces more fluent summaries and solves two shortcomings of reproducing factual details inaccurately and phrase repetition. Though this network can generate Out-Of-Vocabulary (OOV) words, it cannot completely represent them in the context and may face the information loss problem. This paper aims to improve the quality of abstractive summarization with an extra pretrained layer of word embedding for the pointer-generator network. This mechanism helps to maintain the meaning of words in more various contexts. This assures that every word has its own representation, even though it does not exist in the vocabulary. We modify the network with the two latest word embedding mechanisms, i.e. Word2vec and Fasttext, to represent the semantic information of words more accurately. Some OOV words which are marked as unknown tokens now can have their right embeddings and be well considered in summary generation. The experiments on the CNN/Daily Mail corpus shows that the new mechanism outperforms the only pointer-generator network in all 3 ROUGE scores (R1, R2, RL).", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2995934967", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/soict/AnhT19", "doi": "10.1145/3368926.3369728"}}, "content": {"source": {"pdf_hash": "86b73d545cb52fc4470c34c6ffdff686e9ad4f6d", "pdf_src": "ACM", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "c76cfae55e82cd6746b336a2d7b9cc856e6a4ba5", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/86b73d545cb52fc4470c34c6ffdff686e9ad4f6d.txt", "contents": "\nAbstractive Text Summarization Using Pointer-Generator Networks With Pre-trained Word Embedding\nACMCopyright ACM2019. December 4-6, 2019\n\nTrung Dang \nAnh \nThi Thu Nguyen \nTrang trangntt@soict.hust.edu.vn \nTrung Dang \nNguyen Anh \nThu Thi \nTrang \n\nHanoi University of Science and Technology\n1 Dai Co Viet, Hai Ba TrungHanoi Vietnam\n\n\nHanoi University of Science and Technology\n1 Dai Co Viet, Hai Ba TrungHanoi Vietnam\n\nAbstractive Text Summarization Using Pointer-Generator Networks With Pre-trained Word Embedding\n\nViet Nam\nHanoi -Ha Long Bay; New York, NY, USA, 6 pagesACM2019. December 4-6, 201910.1145/3368926.3369728\nAbstractive text summarization is the task of generating a summary that captures the main content of a text document. As a state-of-the-art method for abstractive summarization, the pointergenerator network produces more fluent summaries and solves two shortcomings of reproducing factual details inaccurately and phrase repetition. Though this network can generate Out-Of-Vocabulary (OOV) words, it cannot completely represent them in the context and may face the information loss problem. This paper aims to improve the quality of abstractive summarization with an extra pre-trained layer of word embedding for the pointergenerator network. This mechanism helps to maintain the meaning of words in more various contexts. This assures that every word has its own representation, even though it does not exist in the vocabulary. We modify the network with the two latest word embedding mechanisms, i.e. Word2vec and Fasttext, to represent the semantic information of words more accurately. Some OOV words which are marked as unknown tokens now can have their right embeddings and be well considered in summary generation. The experiments on the CNN/Daily Mail corpus shows that the new mechanism outperforms the only pointergenerator network in all 3 ROUGE scores (R1, R2, RL).\n\nINTRODUCTION\n\nText summarization is the process of identifying the most important meaningful information in a document or set of related documents and compressing them into a shorter version preserving its overall meanings [1]. The goal of automatic text summarization is presenting the source text into a shorter version with semantics. The most important advantage of using a summary is reducing the reading time. Text summarization methods can be classified into extractive and abstractive summarization. An extractive summarization method consists of selecting important sentences, paragraphs etc. from the original document and concatenating them into shorter form. An abstractive summarization is an understanding of the main concepts in a document and then express those concepts in clear natural language. This approach is more complex than extractive approach since it conveys in information in concise way, and usually requires advanced language generation. Due to the difficulty and complexity of the abstractive approach, in the past, most of the work has been extractive [2] [3] [4] [5]. On the other hand, the previous abstractive summarization methods are Tree-based method [6], Semantic Graph based method [7] [8] [9] [10], which depend a lot on the complication of the language and the researcher's knowledge. Recently, with the help of high performance computing, summarization system using neural networks, especially sequence-to-sequence models [11] are very promising. Sequence-to-sequence learning is about training models to convert sequences from one domain to sequences in another domain. The current state-of-the-art neural model for the abstractive summarization problem combines extractive and abstractive mechanism, using pointer-generator networks [12] which can copy words from the source text via pointing [13]. This end-to-end model produces more fluent abstractive summaries, but it represents incorrectly Out-Of-Vocabulary (OOV) words in the input layer. Particularly, all words that are not found in vocabulary will be represented as unknown tokens before being fed to the embedding layer. This makes embeddings of OOV words are incorrectly learned; therefore, this causes the loss information in calculating attention distribution. Moreover, the embedding layer is trained together with the pointer-generator network on the same corpus, which will make the embedding representation may be overfit with the training data set. In this SoICT 2019, December 2019, Hanoi -Ha Long Bay, Vietnam D. Anh et al.\n\npaper, we propose a mechanism using an extra pre-trained layer of word embedding for the pointer-generator network. This helps more words to have their representation from pre-trained word embedding models. The rest of this paper is organized as follows. Section 2 introduces the baseline abstractive summarization model, i.e. pointergenerator with coverage, and our proposal with a pre-trained word embedding layer. Section 3 gives some discussions on related works of the baseline model and the word embedding mechanism. The experiments are shown in Section 4. Finally, Section 5 concludes the paper and gives some perspectives for the work.\n\n\nPROPOSED MODEL\n\n\nBaseline model\n\nThe baseline model of our work is the Pointer-Generator network with a coverage mechanism, the state-of-the-art one for abstractive summarization.\n\n\nPointer-Generator network\n\nThis network is hybrid between the attentional sequence to sequence baseline and a pointer network, as it allows both copying words via pointing, and generating words from a fixed vocabulary. The baseline model of pointer-generator network is similar to that of Nallapati et al. [14], and is depicted in Figure 1. The tokens of the article w \" are fed one-by-one into the encoder (i.e. a single layer bidirectional LSTM), producing a sequence of encoder hidden states h \" . On each step t, the decoder (i.e. a single layer unidirectional LSTM) receives the word embedding of the previous word, and has decoder state s % . During the time of training, the previous word is found in the reference summary; at test time the previous word is emitted by the decoder. The attention distribution at is calculated as in Bahdanau et al. [15]: ' ( = + tanh( 1 \u210e ' + 4 ( + 7( (8 ) (1)\n( = softmax( ( )(2)\nWhere v, W A , W B and b D%%E are learnable parameters.\n\nThe attention distribution can be viewed as a probability distribution over the source words, which tells the decoder where to look to produce the next word. Next, the attention distribution is used to produce a weighted sum of the encoder hidden states, known as the context vector h % * :\n\u210e ( * = \u2211 ( ' \u210e ' '(3)\nThe context vector, which can be seen as a fixed-size representation of what has been read from the source for this step, is concatenated with the decoder state s % and fed through two linear layers to produce the vocabulary distribution P KLMDN :\nPQR7S = softmax( U ( [ ( , \u210e * ( ] + ) + U ) (4)\nwhere V, V U , b and b U are learnable parameters, P KLMDN is a probability distribution over all words in the vocabulary.\n\nIn the pointer-generator model, the attention distribution a % and context vector h % * are calculated as in the formula (2) and (3). In addition, the generation probability p \\]E \u2208 [0, 1] for timestep t is calculated from the context vector h % * , the decoder state s % and the decoder input x % : bc8 = e 1 * + \u210e * ( + 4\n+ ( + g + ( + i(j k (5)\nwhere vectors w A * , w B , w m and scalar b n%o are learnable parameters and \u03c3 is the sigmoid function. Next, p \\]E is used as a soft switch to choose between generating a word from the vocabulary by sampling from P KLMDN , or copying a word from the input sequence by sampling from the attention distribution at. For each document let the extended vocabulary denote the union of the vocabulary, and all words appearing in the source document. We obtain the following probability distribution over the extended vocabulary:\n\n( ) = bc8 PQR7S ( ) + e1 \u2212 bc8 k \u2211 ( ' ':s t us (6) Note that if w is an out-of-vocabulary (OOV) word, then P KLMDN (w) is zero; similarly if w does not appear in the source document, then \u2211 a % \" \":w x uw is zero. The ability to produce OOV words is one of the primary advantages of pointer-generator models; by contrast models such as our baseline are restricted to their preset vocabulary.\n\n\nCoverage mechanism\n\nTo solve the repetition problem, the coverage mechanism keeps a coverage vector c % , which is the sum of attention distributions over all previous decoder timesteps:\n( = \u2211 ( { (|} ( { u~ (7)\nIntuitively, c % is a (unnormalized) distribution over the source document words that represents the degree of coverage that those words have received from the attention mechanism so far. Note that c~ is a zero vector, because on the first timestep, none of the source document has been covered. The coverage vector is used as extra input to the attention mechanism to:\n' ( = + tanh( 1 \u210e ' + 4 ( + R ' ( + 7((8 )(8)\nwhere w M is a learnable parameter vector of same length as . This ensures that the attention mechanism's current decision (choosing where to attend next) is informed by a reminder of its previous decisions (summarized in c % ). This should make it easier for the attention mechanism to avoid repeatedly attending to the same locations, and thus avoid generating repetitive text.\n\n\nWord embedding pre-trained layer\n\nThe proposed architecture of our abstractive summarization approach is represented in Figure 1. Based on the pointer-generator network, we put a pre-trained model to the architecture to represent the input words. With this mechanism, every word in the encoder and decoder layer will be presented as an embedded vector (300 embedding dimensions in our experiment) before being fed to the next layers. Embedded vector ( of each word w now becomes the formula (9):\n( = ( )(9)\nWhere g is a function that transforms each input word into its corresponding embedding vector in the pre-trained model. This makes the network better to calculate hidden states, because the inputs at each time step have been accurately and fully represented, thereby contributing to the improvement of the attention distribution. Moreover, the bc8 value will be calculated more correctly, because its input component ( has been learned with full information. The formula (5) now becomes the formulua (10):\n\nbc8 = e 1 * + \u210e * ( + 4\n+ ( + g + ( ) + i(j k(10)\nTo compare with the original network proposed by Abigail See [13] in previous section, the input is no longer the word indices in the vocabulary in the source text but their embeddings trained by a separate pre-trained model. The new embedding layer is applied in both encoder and decoder of the network. While the encoder receives the embedding from the source text as their input for time steps; every decoder input at every time step of the decoder will be embedded before being fed. This confirms that all words in the source text and the summary have their own representation in the network.\n\n\nRELATED WORKS\n\n\nSequence-to-sequence attentional model\n\nSequence-to-sequence model with attention mechanism was first used by Rush et al. [16], tested on the DUC-2004 and Gigaword datasets, yields high results and became state-of-the-art at the time.\n\nTheir approach, which is centered on the attention mechanism, has been augmented with recurrent decoders [17], Abstract Meaning Representations [18], hierarchical networks [14], variational autoencoders [19] and direct optimization of the performance metric [20], further improving performance on those datasets. Although the results are good, the generated abstracts contain the words that are limited to the vocabulary which has been originally defined. Moreover, these models have a problem when generating repeated phrases in the summary. Recently, a new approach has been proposed by See [13], which uses the Pointer-generator network to generate both new words in the source text, and combines coverage mechanism to solve the problem of repetition in the previous approaches.\n\n\nPointer-generator with coverage\n\nThe Pointer-generator is a hybrid network between the baseline model [14] and the Pointer network [12], which both allows to copy words from document via pointing and allows to generate words from a fixed vocabulary. As a result, it can generate OOV words, overcoming the disadvantage in previous models. Moreover, with the addition of the Coverage mechanism, See et al. solved the repetition problem, which helps to generate better summary results. Particularly, every word before being fed into the network is passed through an embedding layer, which initiates a random embedding vector representing the word respectively.\n\nSince it is an end-to-end network, this embedding vector is updated through each step. Therefore, the model encounter two problems, firstly the new input words at both the decoder and encoder layer which are not in the extended dictionary are represented by the unknown token before being fed to embedding layer of the network, this can result in loss of information The second problem is that due to the end-to-end network, the embedding layer is learned through training along with the model, so it may be overfitting with the training data set.\n\n\nWord embedding\n\nWord embedding is the most popular representation method for words in a document, it captures the context of words, semantic and syntactic similarity, relation with other words, etc. Using word embedding makes it easier to represent words with less memory than using one-hot vector, while also showing the relationship between words. One of the most typical word embedding methods is Word2Vec and recently is Fasttext which was proposed in [21]. With its advantages, word embedding has been used in many different problems of natural language processing (NLP). In medicine problems, word embedding has been used to make a diagnosis prediction model from orthopedic clinical note [22]. Moreover, it also is used to learn sentiment specific word embedding via global sentiment representation [23], learn hybrid word embeddings for text classification [24] and improve the WordNet in [25]. For summary problem, word embedding has been used extensively in both extractive and abstractive summarization. In extractive summarization, recent approaches that use Word2Vec like [26] [27] have yielded high scores in ROUGE points and it is promising. On the other hand, in abstractive summarization, the pointer-generator combined with Coverage mechanism and proposed by See et al. in [13] also have its own embedding layer. This makes pointer-generator network become an end-to-end model which is known as the current state-of-the-art abstractive summarization. Despite these advantages, these word embedding methods do not capture the context of the whole sentence in the document, they only show the similarity of the current sentence with the other sentences. To solve this problem, methods of deep contextualized word representation have been proposed, including ELMo [28] and Google BERT [29], which bring certain promises in improving results for NLP problems. However, the existing pretrained sets of these methods are very large, and trained with different domains from the data set of Daily mail/CNN. Besides, the encoder layer of our model is a bidirectional LSTM network, it already captures the context of the whole document. Therefore, using BERT or ELMo for this work might not be necessary.\n\n\nEXPERIMENT\n\n\nDataset\n\nWe use the CNN/Daily Mail dataset [14] [30], which contains online news articles (781 tokens on average) paired with multisentence summaries (3.75 sentences or 56 tokens on average). We use scripts supplied by Nallapati et al. [14] to obtain the same version of the data, which has 287,226 training pairs, 13,368 validation pairs and 11,490 test pairs. Both the dataset's published results [14] use the anonymized version of the data, which has been pre-processed to replace each named entity.\n\nIn this paper, we experiment with two pre-trained models: Word2vec from Google and Fasttext from Facebook. The Word2vec was trained on the Google News dataset consisting nearly 100 billion words. This model includes word vectors for a vocabulary of 3 million words. On the other hand, the Fasttext was trained on the Wikipedia dataset with character n-grams of length 5. Both the Word2Vec and Fasttext models is trained by CBOW method. They also have 300-dimensional word embeddings, which is the same as our model.\n\n\nTraining\n\nWe train on a single GTX-1080Ti GPU with a batch size of 8. At test time, our summaries are produced using beam search with beam size 4. We trained both the baseline model and our proposal model for about 600,000 iterations (33 epochs) -this is similar to the 33 epochs required by Abigail See' best model [13]. The training took 5 days and 10 hours for the 50k vocabulary model. For all experiments, our model has 256-dimensional hidden states and 300-dimensional word embeddings.\n\n\nResult\n\nOur experiments are performed on coverage and non-coverage models. We re-run the baseline model, and then compare with the results obtained from our proposal models which are using the Word2vec and Fasttext for the word embedding layer. The experiment results are given in Table 1. We evaluate our models Abstractive Text Summarization Using Pointer-Generator Networks With Pre-trained Word Embedding SoICT 2019, December 2019, Hanoi -Ha Long Bay, Vietnam with the standard ROUGE metric, reporting the F1 scores for ROUGE-1, ROUGE-2 and ROUGE-L (which respectively measure the word-overlap, bigram-overlap, and longest common sequence between the reference summary and the summary to be evaluated). We obtain our ROUGE scores using the pyrouge package. Experimental results in show that in both cases of coverage and non-use, the addition of Word2Vec pre-trained and Fasttext pretrained also help to improved results, only with Coverage mechanism, the results of model with Word2Vec is lower than the baseline model at ROUGE-1 and ROUGE-2 points. Especially, when using Fasttext pre-trained, in case of not using Coverage, ROUGE-1 points increased by 0.51 points, ROUGE-2 points increased by 0.39 points and ROUGE-L points increased by 0.5 points with baseline model. We worked more on observing some documents containing words that are not found in the vocabulary (and the training corpus). For instance, we found a sentence in such a document: \"Still , his campaign believes that if it can tap into the group of evangelicals who have been staying home and get the demographic as a whole to overperform, then that could mean the difference of millions more at the polls.\". The word \"overperform\" does not appear in the vocabulary, hence without a separate word embedding layer as the proposal of this paper, this word is represented by the embedding of the unknown token. This makes an inaccurate embedding representation hence loss information for this word. However, this word has its own vector embedding in not only Word2Vec but also Fasttext pre-trained model. With a huge training corpus (e.g. a total of 100 billion words with a 3-millionword vocab in Google News), the pre-trained model can cover much more context for word embeddings than the auto-updating mechanism of the word embedding in the end-to-end abstractive summarization model with its training corpus (e.g. a total of 240 million words with a 50k-word vocab in Daily Mail/CNN). As the experimental results, the proposed model work better than the only pointer-generator network by all 3 ROUGE points. The distance is around 0.5 points with the Daily Mail/CNN. We find that there are not so many documents in the test set that contains words not in the vocab of the model. However, we believe that with a more varied test set, the proposed model will prove its advantage much better.\n\n\nCONCLUSIONS\n\nThis paper aims at improving the quality of abstractive text summarization with the pointer-generator network by using a pretrained word embedding layer. With this mechanism, the meaning of the input words has been better captured, with more context from pre-trained embedding models. The experimental results on Daily Mail/CNN dataset shows that when the pointer-generator network is combined with Fasttext pre-trained, the ROUGE-1, ROUGE-2 and ROUGE-L measures of the proposed model increase from 0.4 to 0.5 points compared to the baseline one.\n\nIn the future, we plan to study on sentence features [5] to give the attentions more knowledge at the sentence level. On the other hand, Rouge measures are based on character matching, thus two sentences that are similar in meaning may be considered as different. Another possible work is to improve Rouge measures so that these measures can deal with semantic aspects.\n\n\nACKNOWLEDGEMENT\n\nThis research is supported by VinTech Fund, a grant for applied research managed by VinTech City.\n\nFigure 1 .\n1Pointer-generator network with word embedding pre-trained layer.\n\nTable 1 :\n1ROUGE scores on the Daily Mail/CNN test setModel \nROUGE (F1) \nR1 (%) R2 (%) RL (%) \nPointer-generator \n35.77 \n15.22 32.60 \nPointer-generator + word2vec pre-\ntrained (ours) \n36.14 \n15.31 33.06 \n\nPointer-generator + Fasttext pre-\ntrained (ours) \n36.28 \n15.61 33.10 \n\nPointer-generator + coverage \n38.56 \n16.75 35.10 \nPointer-generator + coverage + \nword2vec pre-trained (ours) \n38.33 \n16.55 35.16 \n\nPointer-generator + coverage + \nFasttext pre-trained (ours) \n39.06 \n17.05 35.85 \n\n\n\nText Summarization:An Overview. S Babar, M Tech-Cse, Rit , S. Babar, M. Tech-Cse, and Rit, \"Text Summarization:An Overview,\" Oct. 2013.\n\nA trainable document summarizer. J Kupiec, J Pedersen, F Chen, Proceedings of the 18th annual international ACM SIGIR conference on Research and development in information retrieval -SIGIR '95. the 18th annual international ACM SIGIR conference on Research and development in information retrieval -SIGIR '95Seattle, Washington, United StatesJ. Kupiec, J. Pedersen, and F. Chen, \"A trainable document summarizer,\" in Proceedings of the 18th annual international ACM SIGIR conference on Research and development in information retrieval -SIGIR '95, Seattle, Washington, United States, 1995, pp. 68-73.\n\nAutomatic Text Summarization: Past, Present and Future,\" in Multi-source, Multilingual Information Extraction and Summarization. H Saggion, T Poibeau, R. Y. T. PoibeauH. Saggion. J. PiskorskiSpringerH. Saggion and T. Poibeau, \"Automatic Text Summarization: Past, Present and Future,\" in Multi-source, Multilingual Information Extraction and Summarization, R. Y. T. Poibeau; H. Saggion. J. Piskorski, Ed. Springer, 2012, pp. 3-13.\n\nSummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents. R Nallapati, F Zhai, B Zhou, R. Nallapati, F. Zhai, and B. Zhou, \"SummaRuNNer: A Recurrent Neural Network based Sequence Model for Extractive Summarization of Documents,\"\n\n. Arxiv161104230 Cs, ArXiv161104230 Cs, Nov. 2016.\n\nExtractive Summarization Using Supervised and Semi-Supervised Learning. K.-F Wong, M Wu, W Li, COLING. K.-F. Wong, M. Wu, and W. Li, \"Extractive Summarization Using Supervised and Semi-Supervised Learning,\" in COLING, 2008.\n\nSentence Fusion for Multidocument News Summarization. R Barzilay, K R Mckeown, Comput. Linguist. 313R. Barzilay and K. R. McKeown, \"Sentence Fusion for Multidocument News Summarization,\" Comput. Linguist., vol. 31, no. 3, pp. 297-328, 2005.\n\nSemantic graph reduction approach for abstractive Text Summarization. I Moawad, M Aref, I. Moawad and M. Aref, Semantic graph reduction approach for abstractive Text Summarization. 2012.\n\nA Novel Concept-level Approach for Ultra-concise Opinion Summarization. E Lloret, E Boldrini, T Vodolazova, P Mart\u00ednez-Barco, R Mu\u00f1oz, M Palomar, Expert Syst Appl. 4220E. Lloret, E. Boldrini, T. Vodolazova, P. Mart\u00ednez-Barco, R. Mu\u00f1oz, and M. Palomar, \"A Novel Concept-level Approach for Ultra-concise Opinion Summarization,\" Expert Syst Appl, vol. 42, no. 20, pp. 7148-7156, Nov. 2015.\n\nConceptual Framework for Abstractive Text Summarization. N Munot, S Govilkar, Int. J. Nat. Lang. Comput. 4N. Munot and S. Govilkar, \"Conceptual Framework for Abstractive Text Summarization,\" Int. J. Nat. Lang. Comput., vol. 4, pp. 39-50, Feb. 2015.\n\n. Hanoi -Ha Long Bay, D Vietnam, Anh, SoICT. SoICT 2019, December 2019, Hanoi -Ha Long Bay, Vietnam D. Anh et al.\n\nLearning Sub-structures of Document Semantic Graphs for Document Summarization. J Leskovec, M Grobelnik, N Milic-Frayling, 9J. Leskovec, M. Grobelnik, and N. Milic-Frayling, \"Learning Sub-structures of Document Semantic Graphs for Document Summarization,\" p. 9.\n\nSequence to Sequence Learning with Neural Networks. I Sutskever, O Vinyals, Q V Le, Advances in Neural Information Processing Systems. Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. WeinbergerCurran Associates, Inc27I. Sutskever, O. Vinyals, and Q. V. Le, \"Sequence to Sequence Learning with Neural Networks,\" in Advances in Neural Information Processing Systems 27, Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, Eds. Curran Associates, Inc., 2014, pp. 3104-3112.\n\nGet To The Point: Summarization with Pointer-Generator Networks. A See, P J Liu, C D Manning, A. See, P. J. Liu, and C. D. Manning, \"Get To The Point: Summarization with Pointer-Generator Networks,\" ArXiv170404368 Cs, Apr. 2017.\n\nPointer Networks. O Vinyals, M Fortunato, N Jaitly, ArXiv150603134 Cs Stat. O. Vinyals, M. Fortunato, and N. Jaitly, \"Pointer Networks,\" ArXiv150603134 Cs Stat, Jun. 2015.\n\nAbstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond. R Nallapati, B Zhou, C N Santos, C Gulcehre, B Xiang, R. Nallapati, B. Zhou, C. N. dos santos, C. Gulcehre, and B. Xiang, \"Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond,\" ArXiv160206023 Cs, Feb. 2016.\n\nNeural Machine Translation by Jointly Learning to Align and Translate. D Bahdanau, K Cho, Y Bengio, ArXiv14090473 Cs Stat. D. Bahdanau, K. Cho, and Y. Bengio, \"Neural Machine Translation by Jointly Learning to Align and Translate,\" ArXiv14090473 Cs Stat, Sep. 2014.\n\nA Neural Attention Model for Abstractive Sentence Summarization. A M Rush, S Chopra, J Weston, Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. the 2015 Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalA. M. Rush, S. Chopra, and J. Weston, \"A Neural Attention Model for Abstractive Sentence Summarization,\" in Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, Lisbon, Portugal, 2015, pp. 379-389.\n\nAbstractive Sentence Summarization with Attentive Recurrent Neural Networks. S Chopra, M Auli, A M Rush, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesSan Diego, CaliforniaS. Chopra, M. Auli, and A. M. Rush, \"Abstractive Sentence Summarization with Attentive Recurrent Neural Networks,\" in Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, San Diego, California, 2016, pp. 93-98.\n\nNeural Headline Generation on Abstract Meaning Representation. S Takase, J Suzuki, N Okazaki, T Hirao, M Nagata, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasS. Takase, J. Suzuki, N. Okazaki, T. Hirao, and M. Nagata, \"Neural Headline Generation on Abstract Meaning Representation,\" in Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Austin, Texas, 2016, pp. 1054-1059.\n\nLanguage as a Latent Variable: Discrete Generative Models for Sentence Compression. Y Miao, P Blunsom, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingAustin, TexasY. Miao and P. Blunsom, \"Language as a Latent Variable: Discrete Generative Models for Sentence Compression,\" in Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, Austin, Texas, 2016, pp. 319-328.\n\nM Ranzato, S Chopra, M Auli, W Zaremba, Sequence Level Training with Recurrent Neural Networks. M. Ranzato, S. Chopra, M. Auli, and W. Zaremba, \"Sequence Level Training with Recurrent Neural Networks,\" ArXiv151106732 Cs, Nov. 2015.\n\nEnriching Word Vectors with Subword Information. P Bojanowski, E Grave, A Joulin, T Mikolov, P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, \"Enriching Word Vectors with Subword Information,\" ArXiv160704606 Cs, Jul. 2016.\n\nApplying Deep Learning in Word Embedding for Making a Diagnosis Prediction Model from Orthopedic Clinical Note. T Rattanajariya, K Piromsopa, Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval -NLPIR 2019. the 2019 3rd International Conference on Natural Language Processing and Information Retrieval -NLPIR 2019Tokushima, JapanT. Rattanajariya and K. Piromsopa, \"Applying Deep Learning in Word Embedding for Making a Diagnosis Prediction Model from Orthopedic Clinical Note,\" in Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval - NLPIR 2019, Tokushima, Japan, 2019, pp. 44-48.\n\nLearning Sentiment-Specific Word Embedding via Global Sentiment Representation. P Fu, Z Lin, F Yuan, W Wang, D Meng, AAAI. P. Fu, Z. Lin, F. Yuan, W. Wang, and D. Meng, \"Learning Sentiment-Specific Word Embedding via Global Sentiment Representation,\" in AAAI, 2018.\n\nHWE: Hybrid Word Embeddings For Text Classification. X Song, P K Srimani, J Z Wang, Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval -NLPIR 2019. the 2019 3rd International Conference on Natural Language Processing and Information Retrieval -NLPIR 2019Tokushima, JapanX. Song, P. K. Srimani, and J. Z. Wang, \"HWE: Hybrid Word Embeddings For Text Classification,\" in Proceedings of the 2019 3rd International Conference on Natural Language Processing and Information Retrieval - NLPIR 2019, Tokushima, Japan, 2019, pp. 25-29.\n\nImproving Vietnamese WordNet using word embedding. K N Lam, T H To, T T Tran, J Kalita, 6K. N. Lam, T. H. To, T. T. Tran, and J. Kalita, \"Improving Vietnamese WordNet using word embedding,\" p. 6.\n\nExtractive summarization using sentence embeddings. L Haas, 77L. de Haas, \"Extractive summarization using sentence embeddings,\" p. 77.\n\nEnhancing extractive summarization using non-negative matrix factorization with semantic aspects and sentence features. N T T Trang, L T Huong, D V Hung, Proceedings of the Eighth International Symposium on Information and Communication Technology -SoICT. the Eighth International Symposium on Information and Communication Technology -SoICTN. T. T. Trang, L. T. Huong, and D. V. Hung, \"Enhancing extractive summarization using non-negative matrix factorization with semantic aspects and sentence features,\" in Proceedings of the Eighth International Symposium on Information and Communication Technology -SoICT 2017, Nha Trang City, Viet Nam, 2017, pp. 78-83.\n\nDeep Contextualized Word Representations. M Peters, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesNew Orleans, LouisianaLong Papers1M. Peters et al., \"Deep Contextualized Word Representations,\" in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), New Orleans, Louisiana, 2018, pp. 2227-2237.\n\nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,\" ArXiv181004805 Cs, Oct. 2018.\n\nTeaching Machines to Read and Comprehend. K M Hermann, K. M. Hermann et al., \"Teaching Machines to Read and Comprehend,\" ArXiv150603340 Cs, Jun. 2015.\n", "annotations": {"author": "[{\"end\":150,\"start\":139},{\"end\":155,\"start\":151},{\"end\":171,\"start\":156},{\"end\":205,\"start\":172},{\"end\":217,\"start\":206},{\"end\":229,\"start\":218},{\"end\":238,\"start\":230},{\"end\":245,\"start\":239},{\"end\":331,\"start\":246},{\"end\":417,\"start\":332}]", "publisher": "[{\"end\":100,\"start\":97},{\"end\":573,\"start\":570}]", "author_last_name": "[{\"end\":149,\"start\":145},{\"end\":154,\"start\":151},{\"end\":170,\"start\":164},{\"end\":177,\"start\":172},{\"end\":216,\"start\":212},{\"end\":228,\"start\":225},{\"end\":237,\"start\":234},{\"end\":244,\"start\":239}]", "author_first_name": "[{\"end\":144,\"start\":139},{\"end\":159,\"start\":156},{\"end\":163,\"start\":160},{\"end\":211,\"start\":206},{\"end\":224,\"start\":218},{\"end\":233,\"start\":230}]", "author_affiliation": "[{\"end\":330,\"start\":247},{\"end\":416,\"start\":333}]", "title": "[{\"end\":96,\"start\":1},{\"end\":513,\"start\":418}]", "venue": "[{\"end\":523,\"start\":515}]", "abstract": "[{\"end\":1898,\"start\":621}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2126,\"start\":2123},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2987,\"start\":2984},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2991,\"start\":2988},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2999,\"start\":2996},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3092,\"start\":3089},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3125,\"start\":3122},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3133,\"start\":3130},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3138,\"start\":3134},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3369,\"start\":3365},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3682,\"start\":3678},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3742,\"start\":3738},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5578,\"start\":5574},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6127,\"start\":6123},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7905,\"start\":7902},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10388,\"start\":10384},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11064,\"start\":11060},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11283,\"start\":11279},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11322,\"start\":11318},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11350,\"start\":11346},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11381,\"start\":11377},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11436,\"start\":11432},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11771,\"start\":11767},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12064,\"start\":12060},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":12093,\"start\":12089},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13627,\"start\":13623},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13866,\"start\":13862},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":13977,\"start\":13973},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":14036,\"start\":14032},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14068,\"start\":14064},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":14256,\"start\":14252},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14261,\"start\":14257},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14462,\"start\":14458},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":14950,\"start\":14946},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":14971,\"start\":14967},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15442,\"start\":15438},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15635,\"start\":15631},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":15798,\"start\":15794},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":16737,\"start\":16733},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":20394,\"start\":20391}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":20902,\"start\":20825},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":21394,\"start\":20903}]", "paragraph": "[{\"end\":4438,\"start\":1914},{\"end\":5083,\"start\":4440},{\"end\":5265,\"start\":5119},{\"end\":6168,\"start\":5295},{\"end\":6244,\"start\":6189},{\"end\":6536,\"start\":6246},{\"end\":6807,\"start\":6560},{\"end\":6979,\"start\":6857},{\"end\":7304,\"start\":6981},{\"end\":7852,\"start\":7329},{\"end\":8246,\"start\":7854},{\"end\":8435,\"start\":8269},{\"end\":8830,\"start\":8461},{\"end\":9256,\"start\":8877},{\"end\":9754,\"start\":9293},{\"end\":10271,\"start\":9766},{\"end\":10296,\"start\":10273},{\"end\":10919,\"start\":10323},{\"end\":11172,\"start\":10978},{\"end\":11955,\"start\":11174},{\"end\":12615,\"start\":11991},{\"end\":13164,\"start\":12617},{\"end\":15379,\"start\":13183},{\"end\":15897,\"start\":15404},{\"end\":16414,\"start\":15899},{\"end\":16908,\"start\":16427},{\"end\":19774,\"start\":16919},{\"end\":20336,\"start\":19790},{\"end\":20707,\"start\":20338},{\"end\":20824,\"start\":20727}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6188,\"start\":6169},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6559,\"start\":6537},{\"attributes\":{\"id\":\"formula_2\"},\"end\":6856,\"start\":6808},{\"attributes\":{\"id\":\"formula_3\"},\"end\":7328,\"start\":7305},{\"attributes\":{\"id\":\"formula_4\"},\"end\":8460,\"start\":8436},{\"attributes\":{\"id\":\"formula_5\"},\"end\":8876,\"start\":8831},{\"attributes\":{\"id\":\"formula_6\"},\"end\":9765,\"start\":9755},{\"attributes\":{\"id\":\"formula_7\"},\"end\":10322,\"start\":10297}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":17199,\"start\":17192}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1912,\"start\":1900},{\"attributes\":{\"n\":\"2\"},\"end\":5100,\"start\":5086},{\"attributes\":{\"n\":\"2.1\"},\"end\":5117,\"start\":5103},{\"end\":5293,\"start\":5268},{\"end\":8267,\"start\":8249},{\"attributes\":{\"n\":\"2.2\"},\"end\":9291,\"start\":9259},{\"attributes\":{\"n\":\"3\"},\"end\":10935,\"start\":10922},{\"end\":10976,\"start\":10938},{\"end\":11989,\"start\":11958},{\"end\":13181,\"start\":13167},{\"attributes\":{\"n\":\"4\"},\"end\":15392,\"start\":15382},{\"attributes\":{\"n\":\"4.1\"},\"end\":15402,\"start\":15395},{\"attributes\":{\"n\":\"4.2.\"},\"end\":16425,\"start\":16417},{\"attributes\":{\"n\":\"4.3\"},\"end\":16917,\"start\":16911},{\"attributes\":{\"n\":\"5\"},\"end\":19788,\"start\":19777},{\"attributes\":{\"n\":\"6\"},\"end\":20725,\"start\":20710},{\"end\":20836,\"start\":20826},{\"end\":20913,\"start\":20904}]", "table": "[{\"end\":21394,\"start\":20958}]", "figure_caption": "[{\"end\":20902,\"start\":20838},{\"end\":20958,\"start\":20915}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5607,\"start\":5599},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9387,\"start\":9379}]", "bib_author_first_name": "[{\"end\":21429,\"start\":21428},{\"end\":21438,\"start\":21437},{\"end\":21452,\"start\":21449},{\"end\":21567,\"start\":21566},{\"end\":21577,\"start\":21576},{\"end\":21589,\"start\":21588},{\"end\":22265,\"start\":22264},{\"end\":22276,\"start\":22275},{\"end\":22671,\"start\":22670},{\"end\":22684,\"start\":22683},{\"end\":22692,\"start\":22691},{\"end\":22858,\"start\":22844},{\"end\":22970,\"start\":22966},{\"end\":22978,\"start\":22977},{\"end\":22984,\"start\":22983},{\"end\":23174,\"start\":23173},{\"end\":23186,\"start\":23185},{\"end\":23188,\"start\":23187},{\"end\":23432,\"start\":23431},{\"end\":23442,\"start\":23441},{\"end\":23622,\"start\":23621},{\"end\":23632,\"start\":23631},{\"end\":23644,\"start\":23643},{\"end\":23658,\"start\":23657},{\"end\":23676,\"start\":23675},{\"end\":23685,\"start\":23684},{\"end\":23995,\"start\":23994},{\"end\":24004,\"start\":24003},{\"end\":24203,\"start\":24189},{\"end\":24210,\"start\":24209},{\"end\":24383,\"start\":24382},{\"end\":24395,\"start\":24394},{\"end\":24408,\"start\":24407},{\"end\":24618,\"start\":24617},{\"end\":24631,\"start\":24630},{\"end\":24642,\"start\":24641},{\"end\":24644,\"start\":24643},{\"end\":25143,\"start\":25142},{\"end\":25150,\"start\":25149},{\"end\":25152,\"start\":25151},{\"end\":25159,\"start\":25158},{\"end\":25161,\"start\":25160},{\"end\":25326,\"start\":25325},{\"end\":25337,\"start\":25336},{\"end\":25350,\"start\":25349},{\"end\":25556,\"start\":25555},{\"end\":25569,\"start\":25568},{\"end\":25577,\"start\":25576},{\"end\":25579,\"start\":25578},{\"end\":25589,\"start\":25588},{\"end\":25601,\"start\":25600},{\"end\":25857,\"start\":25856},{\"end\":25869,\"start\":25868},{\"end\":25876,\"start\":25875},{\"end\":26118,\"start\":26117},{\"end\":26120,\"start\":26119},{\"end\":26128,\"start\":26127},{\"end\":26138,\"start\":26137},{\"end\":26634,\"start\":26633},{\"end\":26644,\"start\":26643},{\"end\":26652,\"start\":26651},{\"end\":26654,\"start\":26653},{\"end\":27320,\"start\":27319},{\"end\":27330,\"start\":27329},{\"end\":27340,\"start\":27339},{\"end\":27351,\"start\":27350},{\"end\":27360,\"start\":27359},{\"end\":27878,\"start\":27877},{\"end\":27886,\"start\":27885},{\"end\":28305,\"start\":28304},{\"end\":28316,\"start\":28315},{\"end\":28326,\"start\":28325},{\"end\":28334,\"start\":28333},{\"end\":28587,\"start\":28586},{\"end\":28601,\"start\":28600},{\"end\":28610,\"start\":28609},{\"end\":28620,\"start\":28619},{\"end\":28877,\"start\":28876},{\"end\":28894,\"start\":28893},{\"end\":29544,\"start\":29543},{\"end\":29550,\"start\":29549},{\"end\":29557,\"start\":29556},{\"end\":29565,\"start\":29564},{\"end\":29573,\"start\":29572},{\"end\":29784,\"start\":29783},{\"end\":29792,\"start\":29791},{\"end\":29794,\"start\":29793},{\"end\":29805,\"start\":29804},{\"end\":29807,\"start\":29806},{\"end\":30369,\"start\":30368},{\"end\":30371,\"start\":30370},{\"end\":30378,\"start\":30377},{\"end\":30380,\"start\":30379},{\"end\":30386,\"start\":30385},{\"end\":30388,\"start\":30387},{\"end\":30396,\"start\":30395},{\"end\":30567,\"start\":30566},{\"end\":30771,\"start\":30770},{\"end\":30775,\"start\":30772},{\"end\":30784,\"start\":30783},{\"end\":30786,\"start\":30785},{\"end\":30795,\"start\":30794},{\"end\":30797,\"start\":30796},{\"end\":31355,\"start\":31354},{\"end\":32031,\"start\":32030},{\"end\":32044,\"start\":32040},{\"end\":32053,\"start\":32052},{\"end\":32060,\"start\":32059},{\"end\":32280,\"start\":32279},{\"end\":32282,\"start\":32281}]", "bib_author_last_name": "[{\"end\":21435,\"start\":21430},{\"end\":21447,\"start\":21439},{\"end\":21574,\"start\":21568},{\"end\":21586,\"start\":21578},{\"end\":21594,\"start\":21590},{\"end\":22273,\"start\":22266},{\"end\":22284,\"start\":22277},{\"end\":22681,\"start\":22672},{\"end\":22689,\"start\":22685},{\"end\":22697,\"start\":22693},{\"end\":22861,\"start\":22859},{\"end\":22975,\"start\":22971},{\"end\":22981,\"start\":22979},{\"end\":22987,\"start\":22985},{\"end\":23183,\"start\":23175},{\"end\":23196,\"start\":23189},{\"end\":23439,\"start\":23433},{\"end\":23447,\"start\":23443},{\"end\":23629,\"start\":23623},{\"end\":23641,\"start\":23633},{\"end\":23655,\"start\":23645},{\"end\":23673,\"start\":23659},{\"end\":23682,\"start\":23677},{\"end\":23693,\"start\":23686},{\"end\":24001,\"start\":23996},{\"end\":24013,\"start\":24005},{\"end\":24207,\"start\":24204},{\"end\":24218,\"start\":24211},{\"end\":24223,\"start\":24220},{\"end\":24392,\"start\":24384},{\"end\":24405,\"start\":24396},{\"end\":24423,\"start\":24409},{\"end\":24628,\"start\":24619},{\"end\":24639,\"start\":24632},{\"end\":24647,\"start\":24645},{\"end\":25147,\"start\":25144},{\"end\":25156,\"start\":25153},{\"end\":25169,\"start\":25162},{\"end\":25334,\"start\":25327},{\"end\":25347,\"start\":25338},{\"end\":25357,\"start\":25351},{\"end\":25566,\"start\":25557},{\"end\":25574,\"start\":25570},{\"end\":25586,\"start\":25580},{\"end\":25598,\"start\":25590},{\"end\":25607,\"start\":25602},{\"end\":25866,\"start\":25858},{\"end\":25873,\"start\":25870},{\"end\":25883,\"start\":25877},{\"end\":26125,\"start\":26121},{\"end\":26135,\"start\":26129},{\"end\":26145,\"start\":26139},{\"end\":26641,\"start\":26635},{\"end\":26649,\"start\":26645},{\"end\":26659,\"start\":26655},{\"end\":27327,\"start\":27321},{\"end\":27337,\"start\":27331},{\"end\":27348,\"start\":27341},{\"end\":27357,\"start\":27352},{\"end\":27367,\"start\":27361},{\"end\":27883,\"start\":27879},{\"end\":27894,\"start\":27887},{\"end\":28313,\"start\":28306},{\"end\":28323,\"start\":28317},{\"end\":28331,\"start\":28327},{\"end\":28342,\"start\":28335},{\"end\":28598,\"start\":28588},{\"end\":28607,\"start\":28602},{\"end\":28617,\"start\":28611},{\"end\":28628,\"start\":28621},{\"end\":28891,\"start\":28878},{\"end\":28904,\"start\":28895},{\"end\":29547,\"start\":29545},{\"end\":29554,\"start\":29551},{\"end\":29562,\"start\":29558},{\"end\":29570,\"start\":29566},{\"end\":29578,\"start\":29574},{\"end\":29789,\"start\":29785},{\"end\":29802,\"start\":29795},{\"end\":29812,\"start\":29808},{\"end\":30375,\"start\":30372},{\"end\":30383,\"start\":30381},{\"end\":30393,\"start\":30389},{\"end\":30403,\"start\":30397},{\"end\":30572,\"start\":30568},{\"end\":30781,\"start\":30776},{\"end\":30792,\"start\":30787},{\"end\":30802,\"start\":30798},{\"end\":31362,\"start\":31356},{\"end\":32038,\"start\":32032},{\"end\":32050,\"start\":32045},{\"end\":32057,\"start\":32054},{\"end\":32070,\"start\":32061},{\"end\":32290,\"start\":32283}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":21531,\"start\":21396},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":5775833},\"end\":22133,\"start\":21533},{\"attributes\":{\"id\":\"b2\"},\"end\":22564,\"start\":22135},{\"attributes\":{\"id\":\"b3\"},\"end\":22840,\"start\":22566},{\"attributes\":{\"id\":\"b4\"},\"end\":22892,\"start\":22842},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":18517541},\"end\":23117,\"start\":22894},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":16188305},\"end\":23359,\"start\":23119},{\"attributes\":{\"id\":\"b7\"},\"end\":23547,\"start\":23361},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":28098931},\"end\":23935,\"start\":23549},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":61905993},\"end\":24185,\"start\":23937},{\"attributes\":{\"id\":\"b10\"},\"end\":24300,\"start\":24187},{\"attributes\":{\"id\":\"b11\"},\"end\":24563,\"start\":24302},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":7961699},\"end\":25075,\"start\":24565},{\"attributes\":{\"id\":\"b13\"},\"end\":25305,\"start\":25077},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":5692837},\"end\":25478,\"start\":25307},{\"attributes\":{\"id\":\"b15\"},\"end\":25783,\"start\":25480},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":11212020},\"end\":26050,\"start\":25785},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1918428},\"end\":26554,\"start\":26052},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":133195},\"end\":27254,\"start\":26556},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":5450302},\"end\":27791,\"start\":27256},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":10480989},\"end\":28302,\"start\":27793},{\"attributes\":{\"id\":\"b21\"},\"end\":28535,\"start\":28304},{\"attributes\":{\"id\":\"b22\"},\"end\":28762,\"start\":28537},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":201102749},\"end\":29461,\"start\":28764},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":19231718},\"end\":29728,\"start\":29463},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":201102894},\"end\":30315,\"start\":29730},{\"attributes\":{\"id\":\"b26\"},\"end\":30512,\"start\":30317},{\"attributes\":{\"id\":\"b27\"},\"end\":30648,\"start\":30514},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":195625},\"end\":31310,\"start\":30650},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":3626819},\"end\":31946,\"start\":31312},{\"attributes\":{\"id\":\"b30\"},\"end\":32235,\"start\":31948},{\"attributes\":{\"id\":\"b31\"},\"end\":32387,\"start\":32237}]", "bib_title": "[{\"end\":21564,\"start\":21533},{\"end\":22964,\"start\":22894},{\"end\":23171,\"start\":23119},{\"end\":23619,\"start\":23549},{\"end\":23992,\"start\":23937},{\"end\":24615,\"start\":24565},{\"end\":25323,\"start\":25307},{\"end\":25854,\"start\":25785},{\"end\":26115,\"start\":26052},{\"end\":26631,\"start\":26556},{\"end\":27317,\"start\":27256},{\"end\":27875,\"start\":27793},{\"end\":28874,\"start\":28764},{\"end\":29541,\"start\":29463},{\"end\":29781,\"start\":29730},{\"end\":30768,\"start\":30650},{\"end\":31352,\"start\":31312}]", "bib_author": "[{\"end\":21437,\"start\":21428},{\"end\":21449,\"start\":21437},{\"end\":21455,\"start\":21449},{\"end\":21576,\"start\":21566},{\"end\":21588,\"start\":21576},{\"end\":21596,\"start\":21588},{\"end\":22275,\"start\":22264},{\"end\":22286,\"start\":22275},{\"end\":22683,\"start\":22670},{\"end\":22691,\"start\":22683},{\"end\":22699,\"start\":22691},{\"end\":22863,\"start\":22844},{\"end\":22977,\"start\":22966},{\"end\":22983,\"start\":22977},{\"end\":22989,\"start\":22983},{\"end\":23185,\"start\":23173},{\"end\":23198,\"start\":23185},{\"end\":23441,\"start\":23431},{\"end\":23449,\"start\":23441},{\"end\":23631,\"start\":23621},{\"end\":23643,\"start\":23631},{\"end\":23657,\"start\":23643},{\"end\":23675,\"start\":23657},{\"end\":23684,\"start\":23675},{\"end\":23695,\"start\":23684},{\"end\":24003,\"start\":23994},{\"end\":24015,\"start\":24003},{\"end\":24209,\"start\":24189},{\"end\":24220,\"start\":24209},{\"end\":24225,\"start\":24220},{\"end\":24394,\"start\":24382},{\"end\":24407,\"start\":24394},{\"end\":24425,\"start\":24407},{\"end\":24630,\"start\":24617},{\"end\":24641,\"start\":24630},{\"end\":24649,\"start\":24641},{\"end\":25149,\"start\":25142},{\"end\":25158,\"start\":25149},{\"end\":25171,\"start\":25158},{\"end\":25336,\"start\":25325},{\"end\":25349,\"start\":25336},{\"end\":25359,\"start\":25349},{\"end\":25568,\"start\":25555},{\"end\":25576,\"start\":25568},{\"end\":25588,\"start\":25576},{\"end\":25600,\"start\":25588},{\"end\":25609,\"start\":25600},{\"end\":25868,\"start\":25856},{\"end\":25875,\"start\":25868},{\"end\":25885,\"start\":25875},{\"end\":26127,\"start\":26117},{\"end\":26137,\"start\":26127},{\"end\":26147,\"start\":26137},{\"end\":26643,\"start\":26633},{\"end\":26651,\"start\":26643},{\"end\":26661,\"start\":26651},{\"end\":27329,\"start\":27319},{\"end\":27339,\"start\":27329},{\"end\":27350,\"start\":27339},{\"end\":27359,\"start\":27350},{\"end\":27369,\"start\":27359},{\"end\":27885,\"start\":27877},{\"end\":27896,\"start\":27885},{\"end\":28315,\"start\":28304},{\"end\":28325,\"start\":28315},{\"end\":28333,\"start\":28325},{\"end\":28344,\"start\":28333},{\"end\":28600,\"start\":28586},{\"end\":28609,\"start\":28600},{\"end\":28619,\"start\":28609},{\"end\":28630,\"start\":28619},{\"end\":28893,\"start\":28876},{\"end\":28906,\"start\":28893},{\"end\":29549,\"start\":29543},{\"end\":29556,\"start\":29549},{\"end\":29564,\"start\":29556},{\"end\":29572,\"start\":29564},{\"end\":29580,\"start\":29572},{\"end\":29791,\"start\":29783},{\"end\":29804,\"start\":29791},{\"end\":29814,\"start\":29804},{\"end\":30377,\"start\":30368},{\"end\":30385,\"start\":30377},{\"end\":30395,\"start\":30385},{\"end\":30405,\"start\":30395},{\"end\":30574,\"start\":30566},{\"end\":30783,\"start\":30770},{\"end\":30794,\"start\":30783},{\"end\":30804,\"start\":30794},{\"end\":31364,\"start\":31354},{\"end\":32040,\"start\":32030},{\"end\":32052,\"start\":32040},{\"end\":32059,\"start\":32052},{\"end\":32072,\"start\":32059},{\"end\":32292,\"start\":32279}]", "bib_venue": "[{\"end\":21875,\"start\":21727},{\"end\":26322,\"start\":26235},{\"end\":26953,\"start\":26805},{\"end\":27541,\"start\":27457},{\"end\":28068,\"start\":27984},{\"end\":29151,\"start\":29029},{\"end\":30059,\"start\":29937},{\"end\":30991,\"start\":30906},{\"end\":31657,\"start\":31508},{\"end\":21426,\"start\":21396},{\"end\":21725,\"start\":21596},{\"end\":22262,\"start\":22135},{\"end\":22668,\"start\":22566},{\"end\":22995,\"start\":22989},{\"end\":23214,\"start\":23198},{\"end\":23429,\"start\":23361},{\"end\":23711,\"start\":23695},{\"end\":24040,\"start\":24015},{\"end\":24230,\"start\":24225},{\"end\":24380,\"start\":24302},{\"end\":24698,\"start\":24649},{\"end\":25140,\"start\":25077},{\"end\":25381,\"start\":25359},{\"end\":25553,\"start\":25480},{\"end\":25906,\"start\":25885},{\"end\":26233,\"start\":26147},{\"end\":26803,\"start\":26661},{\"end\":27455,\"start\":27369},{\"end\":27982,\"start\":27896},{\"end\":28398,\"start\":28344},{\"end\":28584,\"start\":28537},{\"end\":29027,\"start\":28906},{\"end\":29584,\"start\":29580},{\"end\":29935,\"start\":29814},{\"end\":30366,\"start\":30317},{\"end\":30564,\"start\":30514},{\"end\":30904,\"start\":30804},{\"end\":31506,\"start\":31364},{\"end\":32028,\"start\":31948},{\"end\":32277,\"start\":32237}]"}}}, "year": 2023, "month": 12, "day": 17}