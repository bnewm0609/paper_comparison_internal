{"id": 247958018, "updated": "2023-10-05 15:49:10.452", "metadata": {"title": "SNUG: Self-Supervised Neural Dynamic Garments", "authors": "[{\"first\":\"Igor\",\"last\":\"Santesteban\",\"middle\":[]},{\"first\":\"Miguel\",\"last\":\"Otaduy\",\"middle\":[\"A.\"]},{\"first\":\"Dan\",\"last\":\"Casas\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "We present a self-supervised method to learn dynamic 3D deformations of garments worn by parametric human bodies. State-of-the-art data-driven approaches to model 3D garment deformations are trained using supervised strategies that require large datasets, usually obtained by expensive physics-based simulation methods or professional multi-camera capture setups. In contrast, we propose a new training scheme that removes the need for ground-truth samples, enabling self-supervised training of dynamic 3D garment deformations. Our key contribution is to realize that physics-based deformation models, traditionally solved in a frame-by-frame basis by implicit integrators, can be recasted as an optimization problem. We leverage such optimization-based scheme to formulate a set of physics-based loss terms that can be used to train neural networks without precomputing ground-truth data. This allows us to learn models for interactive garments, including dynamic deformations and fine wrinkles, with two orders of magnitude speed up in training time compared to state-of-the-art supervised methods", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2204.02219", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/SantestebanOC22", "doi": "10.1109/cvpr52688.2022.00797"}}, "content": {"source": {"pdf_hash": "794407eef1c042a9d52d9dc32ea94759ecfd4fca", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2204.02219v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "2993be71ee26bff13208b64c0fc668bf563695f8", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/794407eef1c042a9d52d9dc32ea94759ecfd4fca.txt", "contents": "\nSNUG: Self-Supervised Neural Dynamic Garments STATE OF THE ART Supervised training ~17h L reconstruction Neural Network Data generation ~180h Self-supervised training ~2h\n\n\nIgor Santesteban \nL physics Input Input\nUniversidad Rey Juan Carlos\nSpain\n\nMiguel A Otaduy \nL physics Input Input\nUniversidad Rey Juan Carlos\nSpain\n\nDan Casas \nL physics Input Input\nUniversidad Rey Juan Carlos\nSpain\n\nNeural Network \nL physics Input Input\nUniversidad Rey Juan Carlos\nSpain\n\nSNUG: Self-Supervised Neural Dynamic Garments STATE OF THE ART Supervised training ~17h L reconstruction Neural Network Data generation ~180h Self-supervised training ~2h\n\nInference 2.2ms( SNUG OURS) Inference 2.5msFigure 1. Existing learning-based methods for garment deformations (left) use supervised training schemes that require the expensive computation of large datasets. In contrast, our approach SNUG (right) is a learning-based method that enables the self-supervised training of dynamic neural 3D garments, without requiring any ground-truth data.AbstractWe present a self-supervised method to learn dynamic 3D deformations of garments worn by parametric human bodies. State-of-the-art data-driven approaches to model 3D garment deformations are trained using supervised strategies that require large datasets, usually obtained by expensive physics-based simulation methods or professional multi-camera capture setups. In contrast, we propose a new training scheme that removes the need for groundtruth samples, enabling self-supervised training of dynamic 3D garment deformations. Our key contribution is to realize that physics-based deformation models, traditionally solved in a frame-by-frame basis by implicit integrators, can be recasted as an optimization problem. We leverage such optimization-based scheme to formulate a set of physicsbased loss terms that can be used to train neural networks without precomputing ground-truth data. This allows us to learn models for interactive garments, including dynamic deformations and fine wrinkles, with a two orders of magnitude speed up in training time compared to state-of-the-art supervised methods.\n\nIntroduction\n\nThe efficient modeling of digital garments is an active area of research due to the large number of applications, including fashion design, e-commerce, virtual try-on, and video games. The traditional approach to this problem is through physics-based simulation [38], but the high computational cost required at run time hinders the deployment of these techniques to real-world applications. Recently, learning-based methods [17,31,39,45,46,50,53,55] have demonstrated that it is possible to closely approximate the accuracy of physics-based solutions. These methods use supervised learning strategies to find a function that outputs a deformed garment given an input body descriptor. During the training phase, the supervision is enforced by directly minimizing at a vertex level the difference between the predicted garment and ground-truth 3D meshes. Despite requiring hours of training, learning-based methods are highly-efficient to evaluate at run time, therefore they potentially offer an attractive alternative to traditional physics-based solutions.\n\nHowever, the need for large datasets in current supervised methods is far from ideal. Ground-truth meshes must be obtained -for each combination of garment, body shape, and pose-via computationally-expensive simulations [37] or complex 3D scanning setups [40], which heavily hinders the scalability of current learning-based methods. We observe that for similar image-based problems, self-supervised strategies have shown that it is possible to learn complex tasks without requiring ground-truth data [41,57]. Unfortunately, self-supervision for dynamic 3D clothing has not been explored.\n\nIn this work, we present a self-supervised method to learn dynamic deformations of 3D garments worn by para-metric human bodies. The key to our success is realizing that the solution to the equations of motion used in current physics-based methods can also be formulated as an optimization problem [34]. More specifically, we show that the per-time-step numerical integration scheme used to update the vertex position (e.g., backward Euler) in physicsbased simulators, can be recast as an optimization problem, and demonstrate that the function for this minimization can become the central ingredient of a self-supervised learning scheme. Since this objective function includes both an inertial term and static term directly derived from the equations of motion, we are able to learn time-dependent and posedependent deformations without any ground-truth data.\n\nThe advantages of self-supervision go beyond removing the need for ground-truth data. By reformulating the learning tasks in terms of physics-based intrinsic properties instead of explicit 3D surface similarity, we also mitigate the smoothing artifacts common in supervised methods where L2 losses are used directly at the vertex level [39]. Additionally, self-supervised approaches also generalize better to test sequences outside the distribution of the training set. Finally, we also show how different material models can be easily formulated in our self-supervised framework, bringing the generalization capabilities of physics-based solutions (i.e., deform any material) to learning-based methods, without requiring any precalculation or offline step.\n\nAll in all, our main contribution is a novel learningbased method capable of learning to dynamically deform garments using a self-supervised strategy. We demonstrate the superiority of our approach in terms of data requirements, training time, and inference time, and we quantitatively and qualitatively compare our results with state-ofthe-art supervised methods.\n\n\nRelated Work\n\nExisting methods that model how cloth and garment deform can be categorized into two groups: physics-based models and learning-based models. Physics-Based Methods. Physics-based simulation of cloth is to date a very mature field. Over the years, many methods have been developed to solve the most relevant challenges. These include the design of deformation models such as in-plane and bending energies [15,22], robust implicit solvers [4], rich and efficient contact handling [9,49], or adaptive discretizations [37]. Recent efforts also include the design of differentiable physics simulators [18], including specific problems of cloth simulation such as continuous collision detection and constraint-based solvers [28], and physics-based objectives for tracking and reconstruction of garments [27,59]. While the majority of the cloth simulation models represent the fabric as a continuum, a recent line of research uses yarn-level representations for highresolution detail [10,21]. Some works also show two-way coupling between garments and soft-body avatars [36,42]. While we do not tackle this level of detail in our paper, more accurate methods could be used to replace our cloth and body models.\n\nLearning-Based Methods. In contrast to physics-based models, which typically require solving large systems of nonlinear equations at each time step, learning-based methods aim at estimating a single function that directly outputs the desired deformation for any input. Inspired by early works on Pose Space Deformation [24], a common strategy is to learn parametric garment deformations, which are added to a mesh template, as a function of pose [16,56], shape [53], pose-and-shape [5,45], design [31,39,55], or garment size [50].\n\nTo this end, state-of-the-art methods for garments use supervised strategies that require large datasets of groundtruth data of the specific task to be learned. This methodology has been recently explored for many use cases, including 3D reconstruction [1,2,43,62], garment design [46,53,55], animation [7,17,19,31,39,56], and virtual tryon [8,16,45,61]. To efficiently tackle the learning task, and depending on the goal of each method, different supervision terms and domains have been used. Most methods use direct 3D supervision at the vertex level [17,39,45,53], but image-based 2D supervision in form of UV maps [20,23,46], point clouds [32,44], or sketches [55] also exist. Very recently, implicit representations have shown impressive results on learning to deform humans [3,12,35] and dress avatars [11,44,51,54].\n\nDatasets are a fundamental piece to enable supervision, and most methods [5,39,45,55] opt for synthetic data generated with physics-based simulators such as ARCSim [37] or Argus [25]. Alternatively, other methods [23,31,44,50] use high-quality 3D scans obtained in expensive multi-camera setups [40,60]. Despite the success of all these supervised methods for learning-based garments, relaying on groundtruth data to train the models is a major limitation due to the associated costs and hinders to create datasets.\n\nSelf-supervised strategies are the ideal alternative to circumvent the need for ground-truth data in learning-based methods [48]. Instead of relying on losses that evaluate prediction error based on the difference with respect to groundtruth samples, self-supervised methods use implicit properties of the training data (or domain) as a supervision signal [64]. This strategy is nowadays very popular in datadriven methods for image-based problems [26,41,63], however, almost all state-of-the-art approaches to learn 3D garment deformations rely on ground-truth data [17,39,45]. For 3D deformations tasks not related to garments, many works use physics laws or constraints as a supervision signal [52,58,64]. For example, Tompson et al. [52] enforce incompressibility constraints to learn to solve the system of equations required in physics-based fluid simulation, Xie et al. [58] enforce temporal coherence of consecutive frames in fluid simulations to enhance detail, and Zhu et al. [64] incorporates the governing equations of the physical model (i.e., Partial Differential Equations, PDEs) in the loss to learn image-based flow simulations. Despite the significant progress in self-supervised learning, no previous works addresses the learning of 3D garments in self-supervised strategy, with just the notable and very recent exception of PBNS [6]. PBNS proposes to learn pose space deformations for garments by enforcing static physical consistency during the training of the model. We follow a similar underlying idea, but propose to use a full physics-based deformation scheme recast as an optimization problem to learn, for first time, a model for dynamic garment deformations with self-supervision only. Additionally, our approach learns shape-dependent effects and is able to cope with a material model that produces highlyrealistic and finer wrinkles.\n\n\nMethod\n\nOur goal is to find a function M () that deforms a 3D garment given the underlying body parameters and motion. To this end, in Sec. 3.1, we first describe our garment model used to implement M (), which is based on per-vertex dynamic 3D displacements that are added to a rigged template mesh. Then, in Sec. 3.2, we direct our attention to an optimization-based formulation of dynamic deformations. Based on this formulation, in Sec. 3.3, we introduce our main contribution and describe a physics-based deformation model that allows us to train a regressor R() for 3D garment displacements. Importantly, our loss is driven by fundamental physical properties of deformable objects, not by the reconstruction of ground-truth garments, and therefore it enables self-supervised learning. In Sec. 3.4 we specify the material model used in the different terms of our loss, and define the relevant energies such as the strain, and bending energies. Finally, in Sec. 3.5 we describe the recurrent architecture used to implement the regressor R(). \n\n\nGarment Model\n\nSimilar to state-of-the-art methods for data-driven garments [5,17,39,45,53], we leverage and extend existing human body models [13,30] to encode garment deformations. More specifically, we build our representation on top of the popular SMPL human model [30]. SMPL encodes bodies by deforming a rigged human template according to shape and pose-dependent deformations that are learned from data. Following this idea, we define our garment model as\nM (\u03b2, \u03c6) = W (T (\u03b2, \u03c6), J(\u03b2), \u03b8, W G ) (1) T (\u03b2, \u03c6) = T + R(\u03b2, \u03c6)(2)\nwhere W is a skinning function (e.g., linear blend skinning or dual quaternion) with skinning weights W G , joint locations J(\u03b2), and motion parameters \u03c6 that articulate an unposed deformed garment mesh T (\u03b2, \u03c6). The latter is computed from a garment template mesh T deformed by a function R(\u03b2, \u03c6) that outputs per-vertex 3D displacements to encode dynamic deformations conditioned to the underlying body shape \u03b2 and body motion \u03c6. The body motion \u03c6 contains the current body pose \u03b8 as well as the global velocity of the root joint.\n\nAssuming that the garment template T is correctly located on top of the mean SMPL body mesh [30], we define W G by borrowing the SMPL skinning weights of closest body vertex in rest pose. In the remainder of this section we introduce our novel strategy to learn the 3D displacement regressor R(\u03b2, \u03c6).\n\n\nOptimization-Based Dynamic Deformation\n\nOur goal is to learn the 3D displacement regressor R(\u03b2, \u03c6) in Equation 2 using a self-supervised strategy. To this end, our first task is to find a set of physics-based properties that describe how cloth behaves. Physics-based simulators traditionally solve dynamics by applying a numerical integration scheme, e.g., backward Euler, to the differential equations of motion, and finding the roots of the resulting nonlinear discrete equations [38]. This formulation is applied independently at each simulation frame, to iteratively update the positions and velocities of garment vertices. Our key observation is to realize that the solution to the equations of motion discretized with backward Euler can also be formulated as an optimization problem [34], and the objective function for this minimization can become the central ingredient of a self-supervised learning scheme. Optimization-based dynamics have been used in the Computer Graphics literature to increase the efficiency and robustness of dynamics solvers, through quasi-Newton schemes and step-size selection [14,29]. Instead, we propose to leverage such optimization-based formulation to define a loss for training a neural network that generalizes well to any input (i.e., any body shape and motion).\n\nThe equations of motion can be discretized with backward Euler as\nM x t+1 \u2212 x t \u2212 \u2206tv t \u2206t 2 = f x t+1 , x t+1 \u2212 x t \u2206t ,(3)\nwhere M is the mass matrix, f are forces, and x and v are the positions and velocities of garment nodes. The solution to these equations can be recast as an optimization [14,34]:\n\nx t+1 = arg min\nx 1 2\u2206t 2 (x \u2212x) M(x \u2212x) + \u03a6,(4)\nwherex = x t + \u2206tv t is a tentative (explicit) position update, and \u03a6 is the potential energy due to internal and external forces f of the system. Reshape Figure 2. Overview of our method. First, the recurrent regressor predicts per-vertex offsets as a function of body shape and motion. These offsets are added to the garment template which is then skinned to produce the final result. We train the network by optimizing a set of physical properties of the predicted garments, removing need for ground-truth data.\n+ T h t-1 h t-1 h t-1 h t h t-1 h t h t h t W h 0~ N(\u03bc, \u03c3) ... h 0~ N(\u03bc, \u03c3) ... h 0~ N(\u03bc, \u03c3) ... h 0~ N(\u03bc, \u03c3) ... L static L inertia Garment Model -Section 3.1 M(\u03b2, \u03d5 t-1 ) M(\u03b2, \u03d5 t-2 ) M(\u03b2, \u03d5 t ) Section 3.3 T(\u03b2, \u03d5 t )\n\nTurning Dynamics into Self-Supervision\n\nThe key to our method is to define a set of losses based on Equation 4 to train the regressor R(). To this end, we propose a loss with two terms\nL = L inertia + L static ,(5)\nwhere L inertia models the inertia of the garment and it is defined analogous to the first term of Equation 4\nL inertia = 1 2\u2206t 2 (x \u2212x) M(x \u2212x).(6)\nIntuitively, this term prevents the change of garment velocities over time, but garment velocities will change anyway due to the underlying body motion, which makes dynamics and wrinkle effects appear. L static , the second term of our loss L, models the potential energy \u03a6 of Equation 4 which represents the internal and external forces that affect the garment. Inspired by works from cloth simulation literature [37,47], we define L static as the sum of different physics-based terms that model the energies that emerge on deformable solids, including strain, bending, gravity, and collisions L static = L strain + L bending + L gravity + L collision . (7) This formulation of L static is general, and the definition of each term depends on the material model used, which we detail in the next section.\n\n\nMaterial Model\n\nThe literature of simulation of elastic solids characterizes materials using equations that relate stimuli (e.g., deformations) to material response (e.g., energies) [47]. Inspired by this, and with the goal of learning physically-correct garment behaviors, we define the terms of our static loss L static based on equations of state-of-the-art cloth simulators [37] to model the following energies:\n\nMembrane Strain Energy. The membrane strain term models the response of the material to in-plane deformation. Given a deformed position x \u2208 R 3 and an undeformed position X \u2208 R 2 (i.e., the garment template), it defines an internal energy based on a first-order deformation metric, typically the deformation gradient F = \u2202x \u2202X . In our loss we implement it using the Saint Venant Kirchhoff (StVK) elastic material model that defines membrane strain energy as\n\u03a8 S = \u03bb 2 tr(G) 2 + \u00b5tr(G 2 ),(8)\nwhere \u03bb and \u00b5 are the Lam\u00e9 constants, and G = 1 2 (F F \u2212 I) is the Green strain tensor. The membrane strain energy of the mesh is computed as\nL strain = triangles V\u03a8 S ,(9)\nwhere V is the volume of each triangle (i.e., area \u00d7 thickness).\n\nBending Energy. The bending term models the energy due to the angle of two adjacent faces and we model it as\nL bending = edges k bending 2 \u03b8 2(10)\nwhere \u03b8 is the dihedral angle between the faces and k bending is a bending stiffness.\n\nGravity. To model the effect of gravity in the learned deformations, we add a loss term with the potential energy of each cloth vertex\nL gravity = vertices \u2212m g x(11)\nwhere m is the vertex mass, and g is the gravitational acceleration. Collision Penalty. This term is crucial to learn plausible deformations, enforcing the garment to follow the underlying body motion. We implement it as\nL collision = vertices k collision max( \u2212 d(x), 0) 3(12)\nwhere d(x) is a function that computes the distance to the body, k collision is a collision stiffness, and is a safety margin to prevent the garment from overlapping with the body surface.\n\nTo highlight the realism of the proposed material, in Figure 3 we show a ground-truth simulation of our model, and the simpler material model used in PBNS [6] based on a traditional mass-spring formulation. Overall, our model is capable of reproducing more complex behaviors typically present in garments, including wrinkles and folds at different scales.\n\n\nRegressing Garment Deformations\n\nWith our novel self-supervised loss L defined in Section 3.3, we are ready to train the garment displacement regressor R() from Equation 2 without requiring ground-truth data. To this end, in order to model the time dependencies of the inertial term L inertia , we implement the regressor using 4 Gated Recurrent Units (GRU), each with an output of size 256, and tanh as the activation function (see Figure  2). However, the recurrent nature of GRUs combined with the lack of ground-truth values to guide the training process make the regressor converge to bad solutions if a naive recurrent training protocol is used. We need to take special care into how the hidden states of the GRUs are initialized and updated. Intuitively, the model should be able to learn dynamics from just 3 frames, since L inertia from Equation 6 depends only on the vertex positions and velocities of the previous step. Therefore, we train our network using subsequences of 3 frames. Interestingly, we found that training on longer sub-sequences also minimizes L inertia correctly, but the learned deformations do not model true dynamics.\n\nAt runtime, the network supports sequences of arbitrary length, but results can degrade noticeably for sequences longer than those used in training if initialization of the GRU hidden states is not well handled. More specifically, we observe that for each training sub-sequence, setting the initial hidden states h 0 = 0 hinders the network to generalize to sequences longer than 3 frames. We address this issue by sampling the initial state h 0 of each GRU from N (\u00b5, \u03c3) (empirically, \u00b5 = 0 and \u03c3 = 0.1), which allows the model to generalize well even for sequences with thousands of frames. Notice that at runtime the state h t depends on an arbitrarily large number of previous frames, not just the last 3, hence the use of noise to initialize states on train sub-sequences is fundamental to augment variance in states.\n\n\nEvaluation\n\n\nTraining\n\nTo self-supervise the training process of our regressor R() we need to feed it with human motions and shapes. To this end, we use a set of 52 sequences from the AMASS dataset [33], totaling 6,519 frames, which we split into subsequences of 3 frames as described in Section 3.5. We set aside 4 full sequences for validation purposes. To provide body shape variety at train time, each of the sub-sequences is assigned a different body shape \u03b2 sampled from U(\u22123, 3) at each epoch. Notice that, enabled by our self-supervised approach, this strategy allows us to train using thousands of different body shapes, while competitive supervised methods are limited to a dramatically smaller shape sample ( [39] uses 9 shapes, [45] uses 17) due to the computational restrictions caused by the need for a ground-truth database.\n\nRegarding the network hyper-parameters, we use a batch size of 16, initially train for 10 epochs using a learning rate of 0.001, and then resume the learning with a learning rate of 0.0001 until it converges. This approach is fast, works for all garments, and avoids erroneous states. The rest of the material and training parameters do not affect stability. Larger learning rates can introduce instabilities due to energy spikes that make the training struggle to recover (i.e. the predicted mesh has collisions that are too large to be resolved). Small body-garment collisions are not a probleme.g., we can handle pants despite self-collisions in the legs on some poses.\n\nOur approach does not require balancing loss terms, we just need to set the material properties of the garment. To this end, we tune material parameters to produce a desired fabric behavior, hence the parameters of the loss have a physical meaning -they are not arbitrary hyperparame-   Figure 4. Quantitative evaluation of our approach. We evaluate the minimization error in different the physics-based terms used in our loss, in the test sequence 01 01 of AMASS [33]. Sudden motion changes (e.g., jumps) naturally produce peaks in the inertial term, due to drastic changes in the velocity of the garment. Intuitively, cloth dynamics arise when the garment resists to those changes enforced by the body, therefore lower inertial values indicates that our model better learns time-dependent effects than PBNS [6]. See the supplementary video for qualitative results of this evaluation. The significantly improved realism of our method is better appreciated looking into sequences of deformed garments.   Table 2. Quantitative ablation study. Each term of our loss contributes to the accuracy of the final result.\n\nters. To compute the mass matrix M we use real measurements of the thickness and density of 100% cotton fabric (0.47 mm and 426 kg/m 3 respectively). The rest of the material parameters have the following values: the Lam\u00e9 constants are set to \u03bb = 20.9 and \u00b5 = 11.1, the bending stiffness k bending = 3.96e\u22125, the collision stiffness k collision = 250, and the collision margin = 2 mm. We use the same parameters for all our garments.\n\nTo thoroughly validate our model, in addition to comparisons to SOTA methods, in this section we also include ablations and comparisons that use a ground-truth simulated dataset. For as fair as possible evaluations, such dataset is created using the same motions, and the same train-test split, that we use to train SNUG.\n\nWe implement our method in a regular desktop PC equipped with an AMD Ryzen 7 2700 CPU, an Nvidia GTX 1080 Ti GPU, and 32GB of RAM.\n\n\nData\n\nTrain Runtime Memory generation TailorNet [39] 29 h 6.5 h 10.1 ms 2114 MB Santesteban [45] 180 h 17 h 2.5 ms 109 MB SNUG (Ours) 0 h 2 h 2.2 ms 19 MB Table 3. Timings, memory requirements, and performance of state-of-the-art methods. Our self-supervised approach avoids the expensive cost of data generation, while also achieving significantly lower training times.\n\n\nQuantitative Evaluation\n\nTo quantitatively evaluate our approach, we measure the physics-based terms of our loss L in test motions and compare it with the predictions of PBNS [6]. Notice that the original PBNS method uses a different (and simpler) material model but, in order to get a meaningful quantitative comparison, we extended and re-trained the publicly available PBNS implementation with our material model defined in Section 3.4. Also, notice that we cannot provide this comparison for supervised state-of-the-art methods (e.g., [17,39,45]) because the simulation schemes, material models, and parameters used to build their datasets are different and, therefore, the ground-truth physics properties (i.e., our loss terms) might differ significantly. Figure 4 shows the quantitative evaluation for the most important terms of our loss, and compares it with the extended implementation of PBNS [6] using our material, in the test sequence 01 01 of AMASS [33]. Notice how our method consistently produces lower error values across all terms (strain, bending, and inertia), indicating that test samples processed with SNUG better match the behavior of physics-based solutions (i.e., the minimization of the terms). Table 1 presents a quantitative evaluation of both methods in our full test set (4 sequences, 598 frames unseen . When trained using same motions and same architecture, direct supervision at the vertex level leads to smoothing artifacts (a). In contrast, our physics-based loss is able to learn more realistic details (b), as shown in this frame from a test sequence.\n\nat train time), which further demonstrates that our approach improves upon the method of PBNS.\n\nTo validate each term of our formulation, in Table 2 we show an ablation of the mean-curvature error, evaluated in the test set of our ground truth simulated dataset, when leaving out some of the terms.\n\nFinally, in Table 3 we also evaluate the memory requirements, training time, and runtime performance of our approach and compare to existing state-of-the-art supervised methods. Even if these methods do no address exactly the same problem (e.g. TailorNet [39] models garment variations and SNUG does not, but the latter models dynamics), SNUG outperforms supervised methods by a large margin in all metrics, resulting in a compact model, only 19MB, trained in just 2h, which opens the door to scalable learningbased garment models.\n\n\nQualitative Evaluation\n\nWe qualitatively evaluate our method in Figure 7 and, more extensively, in the supplementary video. To this end, notice that we always use body shapes and motions unseen during training. Additionally, we provide comparisons to the state-of-the-art supervised methods of Santesteban et al. [45] and TailorNet [39], as well as to the recent work PBNS [6] that uses physics-constraints as supervision. To ease the assessment of the realism of each method, we also show results computed with a physics-based simulator [37], but notice that this is a traditional offline method, several orders of magnitude slower. These results demonstrate that our self-supervised method SNUG produces garment deformations that are, at least, on par with the state-of-the-art supervised methods [39,45], while we do not require any ground-truth dataset. For PBNS [6], we use a mean body shape because it does not generalize to different bodies. Because PBNS does not model an inertial term and it is limited to a simpler material model, the garment deformations are generally more stiff, less realistic, and do not change naturally as a function of body pose. This is visible in rows 1 and 3 for PBNS in Figure 7, where the overall wrinkles are the same despite the significant change in body pose.\n\nTo further validate our model, we use the ground-truth simulated dataset (described in Section 4.1, used for validation purposes only) to retrain our neural network in a per-vertex supervised manner. In Figure 5 and in the supplementary we qualitatively demonstrate that the selfsupervised method learns more detailed wrinkles than the supervised counterpart trained with exactly the same motions.\n\nAdditionally, in Figure 6 we show more results for a variety of garments learned with our approach, including t-shirts, tops, sleeveless shirts, pants, and shorts, worn by different body shapes. Notice how our approach produces different wrinkles for each garment type, pose, and shape combination, demonstrating the generalization capabilities of our self-supervised approach. For this figure, we trained one regressor for each garment type. In the supplementary video you can see animated results of these garments, showcasing for the first time realistic dynamic deformations of self-supervised learning-based garments.\n\nFinally, in the supplementary video we also show an ablation study of the influence of each term of our loss function. We demonstrate that all terms of Equation 5 contribute to improve the realism of our predicted garments.\n\n\nLimitations and Conclusion\n\nWe believe SNUG makes an important step towards efficient learning-based models for 3D garments. To improve the state-of-the-art, instead of following the standard route of training with more data, adding more explicit supervision, or designing more complex architectures, we show that self-supervision based on physical properties of deformable solids leads to simpler and smaller yet highlyrealistic models.\n\nWhile our physics-based loss terms are the fundamental key to self-supervision, we also want to point out that our strategy of exploiting optimization-based schemes (originally derived for simulation problems) to train a neural network carries a few weaknesses and important considerations to take into account.\n\nSpecifically, we notice that the self-supervised network tends to converge to simpler solutions than a traditional simulator. For example, although our approach is capable of learning pose-and shape-dependent wrinkles and overall dynamics, we struggle to predict fine-level dynamics. We hypothesize that this limitation arises from a fundamental difference in how our method works: while standard simulators solve physics for one frame at a time, our model optimizes thousands of frames simultaneously during training. This makes our approach more prone to converge to simpler local minima. Nevertheless, we want to highlight that, despite this limitation, the cloth dynamics learned by our method are on par with other data-driven approaches.\n\nAnother aspect open to future research is the collision Figure 6. Qualitative results of our self-supervised method, in validation body shapes and poses unseen during training. SNUG successfully learns highly-realistic garment deformations, including fine wrinkles, as a function of body shape and motion.\n\n\nSNUG (Ours)\n\nSantesteban et al. [45] TailorNet [39] PBNS [6] Simulation [37] Figure 7. Qualitative comparison with state-of-the-art methods. SNUG generalizes well to unseen body shapes and motions, and produces detailed folds and wrinkles. The results of SNUG are, at least, on par with the realism of supervised methods that require large datasets [39,45], and close to state-of-the-art offline physics-based simulation [37].\n\nhandling. Although our loss penalizes collisions between the garment and the body in train samples, we found noticeable collisions in test motions. Although these collisions can be efficiently solved with a postprocessing step, we believe it would be valuable to explore ways to enforce this constraint on the network. Addressing self-collisions of the garment is another aspect that would be worth taking into consideration.\n\nTo foster future research on the field, our trained models and the code to run them are available on http:// mslab.es/projects/SNUG.\n\n\nSee Figure 2 for an overview of our method.\n\nFigure 3 .\n3The material model used is crucial to obtain realistic garment behaviors. We formulate our losses using the Saint Venant Kirchoff (StVK) model, in contrast to simpler alternatives that lead to less expressive deformations.\n\nFigure 5\n5Figure 5. When trained using same motions and same architecture, direct supervision at the vertex level leads to smoothing artifacts (a). In contrast, our physics-based loss is able to learn more realistic details (b), as shown in this frame from a test sequence.\nAcknowledgments. The work was funded in part by the European Research Council (ERC Consolidator Grant no. 772738 TouchDesign) and Spanish Ministry of Science (RTI2018-098694-B-I00 VizLearning).\nLearning to Reconstruct People in Clothing from a Single RGB Camera. Thiemo Alldieck, Marcus Magnor, Bharat Lal Bhatnagar, Christian Theobalt, Gerard Pons-Moll, Proc. of Computer Vision and Pattern Recognition (CVPR). of Computer Vision and Pattern Recognition (CVPR)Thiemo Alldieck, Marcus Magnor, Bharat Lal Bhatnagar, Christian Theobalt, and Gerard Pons-Moll. Learning to Re- construct People in Clothing from a Single RGB Camera. In Proc. of Computer Vision and Pattern Recognition (CVPR), 2019. 2\n\nDetailed Human Avatars from Monocular Video. Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian Theobalt, Gerard Pons-Moll, Proc. of International Conference on 3D Vision (3DV). of International Conference on 3D Vision (3DV)Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian Theobalt, and Gerard Pons-Moll. Detailed Human Avatars from Monocular Video. In Proc. of International Conference on 3D Vision (3DV), pages 98-109. IEEE, 2018. 2\n\nimGHUM: Implicit Generative Models of 3D Human Shape and Articulated Pose. Thiemo Alldieck, Hongyi Xu, Cristian Sminchisescu, Proc. of IEEE International Conference on Computer Vision (ICCV). of IEEE International Conference on Computer Vision (ICCV)2021Thiemo Alldieck, Hongyi Xu, and Cristian Sminchisescu. imGHUM: Implicit Generative Models of 3D Human Shape and Articulated Pose. In Proc. of IEEE International Con- ference on Computer Vision (ICCV), 2021. 2\n\nLarge Steps in Cloth Simulation. David Baraff, Andrew Witkin, Proc. of Conference on Computer Graphics and Interactive Techniques (SIGGRAPH). of Conference on Computer Graphics and Interactive Techniques (SIGGRAPH)David Baraff and Andrew Witkin. Large Steps in Cloth Sim- ulation. In Proc. of Conference on Computer Graphics and Interactive Techniques (SIGGRAPH), page 43-54, 1998. 2\n\nCLOTH3D: Clothed 3D Humans. Hugo Bertiche, Meysam Madadi, Sergio Escalera, Proc. of European Conference on Computer Vision (ECCV). of European Conference on Computer Vision (ECCV)23Hugo Bertiche, Meysam Madadi, and Sergio Escalera. CLOTH3D: Clothed 3D Humans. In Proc. of European Con- ference on Computer Vision (ECCV), 2020. 2, 3\n\nPBNS: Physically Based Neural Simulation for Unsupervised Garment Pose Space Deformation. Hugo Bertiche, Meysam Madadi, Sergio Escalera, Proc. SIGGRAPH Asia). SIGGRAPH Asia)40Hugo Bertiche, Meysam Madadi, and Sergio Escalera. PBNS: Physically Based Neural Simulation for Unsuper- vised Garment Pose Space Deformation. ACM Transactions on Graphics (Proc. SIGGRAPH Asia), 40(6), dec 2021. 3, 5, 6, 7, 8\n\nDeePSD: Automatic Deep Skinning and Pose Space Deformation for 3D Garment Animation. Hugo Bertiche, Meysam Madadi, Emilio Tylson, Sergio Escalera, Proc. of IEEE International Conference on Computer Vision (ICCV). of IEEE International Conference on Computer Vision (ICCV)2021Hugo Bertiche, Meysam Madadi, Emilio Tylson, and Ser- gio Escalera. DeePSD: Automatic Deep Skinning and Pose Space Deformation for 3D Garment Animation. In Proc. of IEEE International Conference on Computer Vision (ICCV), 2021. 2\n\nMulti-garment net: Learning to Dress 3D People from Images. Garvita Bharat Lal Bhatnagar, Christian Tiwari, Gerard Theobalt, Pons-Moll, Proc. of IEEE International Conference on Computer Vision (ICCV). of IEEE International Conference on Computer Vision (ICCV)Bharat Lal Bhatnagar, Garvita Tiwari, Christian Theobalt, and Gerard Pons-Moll. Multi-garment net: Learning to Dress 3D People from Images. In Proc. of IEEE Interna- tional Conference on Computer Vision (ICCV), pages 5420- 5430, 2019. 2\n\nRobust treatment of collisions, contact and friction for cloth animation. Robert Bridson, Ronald Fedkiw, John Anderson, ACM Trans. Graph. 213Robert Bridson, Ronald Fedkiw, and John Anderson. Robust treatment of collisions, contact and friction for cloth anima- tion. ACM Trans. Graph., 21(3):594-603, 2002. 2\n\nYarn-level simulation of woven cloth. Gabriel Cirio, Jorge Lopez-Moreno, David Miraut, Miguel A Otaduy, Proc. SIGGRAPH Asia). SIGGRAPH Asia)33Gabriel Cirio, Jorge Lopez-Moreno, David Miraut, and Miguel A Otaduy. Yarn-level simulation of woven cloth. ACM Transactions on Graphics (Proc. SIGGRAPH Asia), 33(6):1-11, 2014. 2\n\nSMPLicit: Topology-aware Generative Model for Clothed People. Enric Corona, Albert Pumarola, Guillem Aleny\u00e0, Gerard Pons-Moll, Francesc Moreno-Noguer, Proc. of Computer Vision and Pattern Recognition (CVPR). of Computer Vision and Pattern Recognition (CVPR)2021Enric Corona, Albert Pumarola, Guillem Aleny\u00e0, Ger- ard Pons-Moll, and Francesc Moreno-Noguer. SMPLicit: Topology-aware Generative Model for Clothed People. In Proc. of Computer Vision and Pattern Recognition (CVPR), 2021. 2\n\nNASA: Neural Articulated Ahape Approximation. Boyang Deng, P John, Timothy Lewis, Gerard Jeruzalski, Geoffrey Pons-Moll, Mohammad Hinton, Andrea Norouzi, Tagliasacchi, Proc. of European Conference on Computer Vision (ECCV). of European Conference on Computer Vision (ECCV)2020Boyang Deng, John P Lewis, Timothy Jeruzalski, Gerard Pons-Moll, Geoffrey Hinton, Mohammad Norouzi, and An- drea Tagliasacchi. NASA: Neural Articulated Ahape Ap- proximation. In Proc. of European Conference on Computer Vision (ECCV), 2020. 2\n\nAvatar Reshaping and Automatic Rigging Using a Deformable Model. Andrew Feng, Dan Casas, Ari Shapiro, Proc. of ACM SIGGRAPH Conference on Motion in Games (MIG). of ACM SIGGRAPH Conference on Motion in Games (MIG)Andrew Feng, Dan Casas, and Ari Shapiro. Avatar Reshap- ing and Automatic Rigging Using a Deformable Model. In Proc. of ACM SIGGRAPH Conference on Motion in Games (MIG), pages 57-64, 2015. 3\n\nOptimization Integrator for Large Time Steps. F Theodore, Craig Gast, Alexey Schroeder, Chenfanfu Stomakhin, Joseph M Jiang, Teran, IEEE Transactions on Visualization and Computer Graphics (TVCG). 2110Theodore F Gast, Craig Schroeder, Alexey Stomakhin, Chenfanfu Jiang, and Joseph M Teran. Optimization Inte- grator for Large Time Steps. IEEE Transactions on Visual- ization and Computer Graphics (TVCG), 21(10):1103-1115, 2015. 3\n\nDiscrete Shells. Eitan Grinspun, Anil N Hirani, Mathieu Desbrun, Peter Schr\u00f6der, Proc. of ACM SIG-GRAPH/Eurographics Symposium on Computer Animation (SCA). of ACM SIG-GRAPH/Eurographics Symposium on Computer Animation (SCA)Eitan Grinspun, Anil N. Hirani, Mathieu Desbrun, and Pe- ter Schr\u00f6der. Discrete Shells. In Proc. of ACM SIG- GRAPH/Eurographics Symposium on Computer Animation (SCA), page 62-67, 2003. 2\n\nDRAPE: DRessing Any PErson. Loretta Peng Guan, Reiss, A David, Alexander Hirshberg, Michael J Weiss, Black, Proc. SIGGRAPH). SIGGRAPH)31Peng Guan, Loretta Reiss, David A Hirshberg, Alexander Weiss, and Michael J Black. DRAPE: DRessing Any PEr- son. ACM Transactions on Graphics (Proc. SIGGRAPH), 31(4), 2012. 2\n\nGarNet: A two-stream network for fast and accurate 3D cloth draping. Erhan Gundogdu, Victor Constantin, Amrollah Seifoddini, Minh Dang, Mathieu Salzmann, Pascal Fua, Proc. of IEEE International Conference on Computer Vision (ICCV). of IEEE International Conference on Computer Vision (ICCV)6Erhan Gundogdu, Victor Constantin, Amrollah Seifoddini, Minh Dang, Mathieu Salzmann, and Pascal Fua. GarNet: A two-stream network for fast and accurate 3D cloth draping. In Proc. of IEEE International Conference on Computer Vi- sion (ICCV), 2019. 1, 2, 3, 6\n\nDiffTaichi: Differentiable Programming for Physical Simulation. Yuanming Hu, Luke Anderson, Tzu-Mao Li, Qi Sun, Nathan Carr, Jonathan Ragan-Kelley, Fr\u00e9do Durand, ICLR. 2Yuanming Hu, Luke Anderson, Tzu-Mao Li, Qi Sun, Nathan Carr, Jonathan Ragan-Kelley, and Fr\u00e9do Durand. DiffTaichi: Differentiable Programming for Physical Simulation. ICLR, 2020. 2\n\nARCH: Animatable Reconstruction of Clothed Humans. Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, Tony Tung, Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition. of IEEE/CVF Conference on Computer Vision and Pattern RecognitionZeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, and Tony Tung. ARCH: Animatable Reconstruction of Clothed Humans. In Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3093-3102, 2020. 2\n\nA Pixel-Based Framework for Data-Driven Clothing. Yilin Ning Jin, Zhenglin Zhu, Ron Geng, Fedkiw, Computer Graphics Forum (Proc. of SCA). 2020Ning Jin, Yilin Zhu, Zhenglin Geng, and Ron Fedkiw. A Pixel-Based Framework for Data-Driven Clothing. Com- puter Graphics Forum (Proc. of SCA), 2020. 2\n\nSimulating knitted cloth at the yarn level. Jonathan M Kaldor, Doug L James, Steve Marschner, ACM Trans. Graph. 273Jonathan M. Kaldor, Doug L. James, and Steve Marschner. Simulating knitted cloth at the yarn level. ACM Trans. Graph., 27(3):1-9, 2008. 2\n\nA finite element formulation of baraff-witkin cloth. Theodore Kim, Computer Graphics Forum. 398Theodore Kim. A finite element formulation of baraff-witkin cloth. Computer Graphics Forum, 39(8):171-179, 2020. 2\n\nDeepWrinkles: Accurate and Realistic Clothing Modeling. Zorah Lahner, Daniel Cremers, Tony Tung, Proc. of European Conference on Computer Vision (ECCV). of European Conference on Computer Vision (ECCV)Zorah Lahner, Daniel Cremers, and Tony Tung. DeepWrin- kles: Accurate and Realistic Clothing Modeling. In Proc. of European Conference on Computer Vision (ECCV), 2018. 2\n\nPose Space Deformation: A Unified Approach to Shape Interpolation and Skeleton-Driven Deformation. P John, Matt Lewis, Nickson Cordner, Fong, Proc. of the Annual Conference on Computer Graphics and Interactive Techniques. of the Annual Conference on Computer Graphics and Interactive TechniquesJohn P Lewis, Matt Cordner, and Nickson Fong. Pose Space Deformation: A Unified Approach to Shape Interpolation and Skeleton-Driven Deformation. In Proc. of the Annual Conference on Computer Graphics and Interactive Tech- niques, pages 165-172, 2000. 2\n\nAn Implicit Frictional Contact Solver for Adaptive Cloth Simulation. Jie Li, Gilles Daviet, Rahul Narain, Florence Bertails-Descoubes, Matthew Overby, George E Brown, Laurence Boissieux, Proc. SIGGRAPH). SIGGRAPH)37Jie Li, Gilles Daviet, Rahul Narain, Florence Bertails- Descoubes, Matthew Overby, George E Brown, and Lau- rence Boissieux. An Implicit Frictional Contact Solver for Adaptive Cloth Simulation. ACM Transactions on Graphics (Proc. SIGGRAPH), 37(4):1-15, 2018. 2\n\nSelf-supervised single-view 3d reconstruction via semantic consistency. Xueting Li, Sifei Liu, Kihwan Kim, Varun Shalini De Mello, Ming-Hsuan Jampani, Jan Yang, Kautz, Proc. of European Conference on Computer Vision (ECCV). of European Conference on Computer Vision (ECCV)2020Xueting Li, Sifei Liu, Kihwan Kim, Shalini De Mello, Varun Jampani, Ming-Hsuan Yang, and Jan Kautz. Self-supervised single-view 3d reconstruction via semantic consistency. In Proc. of European Conference on Computer Vision (ECCV), 2020. 2\n\nDeep Physicsaware Inference of Cloth Deformation for Monocular Human Performance Capture. Yue Li, Marc Habermann, Bernhard Thomaszewski, Stelian Coros, Thabo Beeler, Christian Theobalt, Proc. of International Conference on 3D Vision (3DV). of International Conference on 3D Vision (3DV)2021Yue Li, Marc Habermann, Bernhard Thomaszewski, Stelian Coros, Thabo Beeler, and Christian Theobalt. Deep Physics- aware Inference of Cloth Deformation for Monocular Hu- man Performance Capture. In Proc. of International Confer- ence on 3D Vision (3DV), 2021. 2\n\nDifferentiable Cloth Simulation for Inverse Problems. Junbang Liang, Ming Lin, Vladlen Koltun, Advances in Neural Information Processing Systems (NeurIPS). Junbang Liang, Ming Lin, and Vladlen Koltun. Differen- tiable Cloth Simulation for Inverse Problems. In Advances in Neural Information Processing Systems (NeurIPS), pages 771-780, 2019. 2\n\nQuasi-Newton Methods for Real-Time Simulation of Hyperelastic Materials. Tiantian Liu, Sofien Bouaziz, Ladislav Kavan, 2017. 3ACM Trans. Graph. 364Tiantian Liu, Sofien Bouaziz, and Ladislav Kavan. Quasi- Newton Methods for Real-Time Simulation of Hyperelastic Materials. ACM Trans. Graph., 36(4), 2017. 3\n\nSmpl: A skinned multiperson linear model. Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, Michael J Black, Proc. SIGGRAPH Asia). SIGGRAPH Asia)34Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J Black. Smpl: A skinned multi- person linear model. ACM Transactions on Graphics (Proc. SIGGRAPH Asia), 34(6):1-16, 2015. 3\n\nLearning to Dress 3D People in Generative Clothing. Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades, Gerard Pons-Moll, Siyu Tang, Michael J Black, Proc. of Computer Vision and Pattern Recognition (CVPR). of Computer Vision and Pattern Recognition (CVPR)Qianli Ma, Jinlong Yang, Anurag Ranjan, Sergi Pujades, Gerard Pons-Moll, Siyu Tang, and Michael J. Black. Learn- ing to Dress 3D People in Generative Clothing. In Proc. of Computer Vision and Pattern Recognition (CVPR), 2020. 1, 2\n\nThe power of points for modeling humans in clothing. Qianli Ma, Jinlong Yang, Siyu Tang, Michael J Black, Proc. of IEEE International Conference on Computer Vision (ICCV). of IEEE International Conference on Computer Vision (ICCV)2021Qianli Ma, Jinlong Yang, Siyu Tang, and Michael J Black. The power of points for modeling humans in clothing. In Proc. of IEEE International Conference on Computer Vision (ICCV), 2021. 2\n\nAMASS: Archive of Motion Capture as Surface Shapes. Naureen Mahmood, Nima Ghorbani, F Nikolaus, Gerard Troje, Michael J Pons-Moll, Black, Proc. of IEEE International Conference on Computer Vision (ICCV). of IEEE International Conference on Computer Vision (ICCV)56Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Ger- ard Pons-Moll, and Michael J. Black. AMASS: Archive of Motion Capture as Surface Shapes. In Proc. of IEEE In- ternational Conference on Computer Vision (ICCV), pages 5442-5451, Oct. 2019. 5, 6\n\nExample-Based Elastic Materials. Sebastian Martin, Bernhard Thomaszewski, Eitan Grinspun, Markus Gross, ACM Trans. Graph. 3043Sebastian Martin, Bernhard Thomaszewski, Eitan Grinspun, and Markus Gross. Example-Based Elastic Materials. ACM Trans. Graph., 30(4), 2011. 2, 3\n\nLEAP: Learning Articulated Occupancy of People. Marko Mihajlovic, Yan Zhang, J Michael, Siyu Black, Tang, Proc. of Computer Vision and Pattern Recognition (CVPR). of Computer Vision and Pattern Recognition (CVPR)2021Marko Mihajlovic, Yan Zhang, Michael J Black, and Siyu Tang. LEAP: Learning Articulated Occupancy of People. In Proc. of Computer Vision and Pattern Recognition (CVPR), 2021. 2\n\nComputational Design of Skintight Clothing. Juan Montes, Bernhard Thomaszewski, Sudhir Mudur, Tiberiu Popa, ACM Trans. Graph. 394Juan Montes, Bernhard Thomaszewski, Sudhir Mudur, and Tiberiu Popa. Computational Design of Skintight Clothing. ACM Trans. Graph., 39(4), 2020. 2\n\nAdaptive Anisotropic Remeshing for Cloth Simulation. Rahul Narain, Armin Samii, James F O&apos; Brien, Proc. SIGGRAPH Asia). SIGGRAPH Asia)31Rahul Narain, Armin Samii, and James F O'brien. Adaptive Anisotropic Remeshing for Cloth Simulation. ACM Trans- actions on Graphics (Proc. SIGGRAPH Asia), 31(6):1-10, 2012. 1, 2, 4, 7, 8\n\nPhysically Based Deformable Models in Computer Graphics. Andrew Nealen, Matthias M\u00fcller, Richard Keiser, Eddy Boxerman, Mark Carlson, Computer Graphics Forum. 2543Andrew Nealen, Matthias M\u00fcller, Richard Keiser, Eddy Boxerman, and Mark Carlson. Physically Based Deformable Models in Computer Graphics. Computer Graphics Forum, 25(4):809-836, 2006. 1, 3\n\nThe Virtual Tailor: Predicting Clothing in 3D as a Function of Human Pose, Shape and Garment Style. Chaitanya Patel, Zhouyingcheng Liao, Gerard Pons-Moll, Proc. of Computer Vision and Pattern Recognition (CVPR). of Computer Vision and Pattern Recognition (CVPR)7Chaitanya Patel, Zhouyingcheng Liao, and Gerard Pons- Moll. The Virtual Tailor: Predicting Clothing in 3D as a Function of Human Pose, Shape and Garment Style. In Proc. of Computer Vision and Pattern Recognition (CVPR), 2020. 1, 2, 3, 5, 6, 7, 8\n\nClothCap: Seamless 4D clothing capture and retargeting. Gerard Pons-Moll, Sergi Pujades, Sonny Hu, Michael J Black, Proc. SIGGRAPH). SIGGRAPH)36Gerard Pons-Moll, Sergi Pujades, Sonny Hu, and Michael J. Black. ClothCap: Seamless 4D clothing capture and retar- geting. ACM Transactions on Graphics (Proc. SIGGRAPH), 36(4), 2017. 1, 2\n\nSwapNet: Image Based Garment Transfer. Amit Raj, Patsorn Sangkloy, Huiwen Chang, Jingwan Lu, Duygu Ceylan, James Hays, Proc. of European Conference on Computer Vision (ECCV). of European Conference on Computer Vision (ECCV)1Amit Raj, Patsorn Sangkloy, Huiwen Chang, Jingwan Lu, Duygu Ceylan, and James Hays. SwapNet: Image Based Garment Transfer. In Proc. of European Conference on Com- puter Vision (ECCV), pages 666-682, 2018. 1, 2\n\nModeling and Estimation of Nonlinear Skin Mechanics for Animated Avatars. Cristian Romero, Miguel A Otaduy, Dan Casas, Jesus Perez, 2020. 2Proc. Eurographics). Eurographics)39Cristian Romero, Miguel A. Otaduy, Dan Casas, and Jesus Perez. Modeling and Estimation of Nonlinear Skin Mechan- ics for Animated Avatars. Computer Graphics Forum (Proc. Eurographics), 39(2), 2020. 2\n\nPIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization. Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, Hao Li, Proc. of IEEE International Conference on Computer Vision (ICCV). of IEEE International Conference on Computer Vision (ICCV)Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Mor- ishima, Angjoo Kanazawa, and Hao Li. PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digi- tization. In Proc. of IEEE International Conference on Com- puter Vision (ICCV), 2019. 2\n\nSCANimate: Weakly supervised learning of skinned clothed avatar networks. Shunsuke Saito, Jinlong Yang, Qianli Ma, Michael J Black, Proc. of Computer Vision and Pattern Recognition (CVPR). of Computer Vision and Pattern Recognition (CVPR)2021Shunsuke Saito, Jinlong Yang, Qianli Ma, and Michael J. Black. SCANimate: Weakly supervised learning of skinned clothed avatar networks. In Proc. of Computer Vision and Pattern Recognition (CVPR), 2021. 2\n\nLearning-Based Animation of Clothing for Virtual Try-On. Igor Santesteban, Miguel A Otaduy, Dan Casas, Computer Graphics Forum (Proc. Eurographics). 38Igor Santesteban, Miguel A. Otaduy, and Dan Casas. Learning-Based Animation of Clothing for Virtual Try-On. Computer Graphics Forum (Proc. Eurographics), 38(2), 2019. 1, 2, 3, 5, 6, 7, 8\n\nGAN-based Garment Generation Using Sewing Pattern Images. Yu Shen, Junbang Liang, Ming C Lin, Proc. of European Conference on Computer Vision (ECCV). of European Conference on Computer Vision (ECCV)1Yu Shen, Junbang Liang, and Ming C. Lin. GAN-based Gar- ment Generation Using Sewing Pattern Images. In Proc. of European Conference on Computer Vision (ECCV), 2020. 1, 2\n\nFEM simulation of 3D deformable solids: a practitioner's guide to theory, discretization and model reduction. Eftychios Sifakis, Jernej Barbic, SIGGRAPH 2012 Courses. ACMEftychios Sifakis and Jernej Barbic. FEM simulation of 3D deformable solids: a practitioner's guide to theory, discretization and model reduction. In SIGGRAPH 2012 Courses, pages 1-50. ACM, 2012. 4\n\nLabel-Free Supervision of Neural Networks with Physics and Domain Knowledge. Russell Stewart, Stefano Ermon, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceRussell Stewart and Stefano Ermon. Label-Free Supervision of Neural Networks with Physics and Domain Knowledge. In Proceedings of the AAAI Conference on Artificial Intelli- gence, page 2576-2582, 2017. 2\n\nI-Cloth: Incremental Collision Handling for GPU-Based Interactive Cloth Simulation. Min Tang, Tongtong Wang, Zhongyuan Liu, Ruofeng Tong, Dinesh Manocha, Proc. SIGGRAPH Asia). SIGGRAPH Asia)37Min Tang, Tongtong Wang, Zhongyuan Liu, Ruofeng Tong, and Dinesh Manocha. I-Cloth: Incremental Collision Han- dling for GPU-Based Interactive Cloth Simulation. ACM Transactions on Graphics (Proc. SIGGRAPH Asia), 37(6), 2018. 2\n\nSIZER: A Dataset and Model for Parsing 3D Clothing and Learning Size Sensitive 3D Clothing. Garvita Tiwari, Bharat Lal Bhatnagar, Tony Tung, Gerard Pons-Moll, Proc. of European Conference on Computer Vision (ECCV). of European Conference on Computer Vision (ECCV)Garvita Tiwari, Bharat Lal Bhatnagar, Tony Tung, and Ger- ard Pons-Moll. SIZER: A Dataset and Model for Parsing 3D Clothing and Learning Size Sensitive 3D Clothing. In Proc. of European Conference on Computer Vision (ECCV), 2020. 1, 2\n\nNeural-GIF: Neural Generalized Implicit Functions for Animating People in Clothing. Garvita Tiwari, Nikolaos Sarafianos, Tony Tung, Gerard Pons-Moll, Proc. of IEEE International Conference on Computer Vision (ICCV). of IEEE International Conference on Computer Vision (ICCV)2021Garvita Tiwari, Nikolaos Sarafianos, Tony Tung, and Gerard Pons-Moll. Neural-GIF: Neural Generalized Implicit Func- tions for Animating People in Clothing. In Proc. of IEEE International Conference on Computer Vision (ICCV), 2021. 2\n\nAccelerating Eulerian Fluid Simulation With Convolutional Networks. Jonathan Tompson, Kristofer Schlachter, Pablo Sprechmann, Ken Perlin, International Conference on Machine Learning (ICML). Jonathan Tompson, Kristofer Schlachter, Pablo Sprechmann, and Ken Perlin. Accelerating Eulerian Fluid Simulation With Convolutional Networks. In International Conference on Machine Learning (ICML), pages 3424-3433, 2017. 2\n\nFully Convolutional Graph Neural Networks for Parametric Virtual Try-On. Raquel Vidaurre, Igor Santesteban, Elena Garces, Dan Casas, 2020. 1Computer Graphics Forum (Proc. SCA). 23Raquel Vidaurre, Igor Santesteban, Elena Garces, and Dan Casas. Fully Convolutional Graph Neural Networks for Para- metric Virtual Try-On. Computer Graphics Forum (Proc. SCA), 39(8), 2020. 1, 2, 3\n\nMetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images. Shaofei Wang, Marko Mihajlovic, Qianli Ma, Andreas Geiger, Siyu Tang, Advances in Neural Information Processing Systems (NeurIPS). 2021Shaofei Wang, Marko Mihajlovic, Qianli Ma, Andreas Geiger, and Siyu Tang. MetaAvatar: Learning Animatable Clothed Human Models from Few Depth Images. In Ad- vances in Neural Information Processing Systems (NeurIPS), 2021. 2\n\nLearning a Shared Shape Space for Multimodal Garment Design. Y Tuanfeng, Duygu Wang, Jovan Ceylan, Niloy J Popovi\u0107, Mitra, Proc. SIGGRAPH Asia). SIGGRAPH Asia)37Tuanfeng Y Wang, Duygu Ceylan, Jovan Popovi\u0107, and Niloy J Mitra. Learning a Shared Shape Space for Mul- timodal Garment Design. ACM Transactions on Graphics (Proc. SIGGRAPH Asia), 37(6), 2018. 1, 2\n\nLearning an Intrinsic Garment Space for Interactive Authoring of Garment Animation. Y Tuanfeng, Tianjia Wang, Kai Shao, Niloy J Fu, Mitra, Proc. SIGGRAPH Asia). SIGGRAPH Asia)38Tuanfeng Y Wang, Tianjia Shao, Kai Fu, and Niloy J Mitra. Learning an Intrinsic Garment Space for Interactive Author- ing of Garment Animation. ACM Transactions on Graphics (Proc. SIGGRAPH Asia), 38(6), 2019. 2\n\nM2E-Try On Net: Fashion from Model to Everyone. Zhonghua Wu, Guosheng Lin, Qingyi Tao, Jianfei Cai, Proc. of ACM International Conference on Multimedia. of ACM International Conference on MultimediaZhonghua Wu, Guosheng Lin, Qingyi Tao, and Jianfei Cai. M2E-Try On Net: Fashion from Model to Everyone. In Proc. of ACM International Conference on Multimedia, 2019. 1\n\ntempoGAN: A Temporally Coherent, Volumetric GAN for Super-resolution Fluid Flow. You Xie, Erik Franz, Mengyu Chu, Nils Thuerey, Proc. SIGGRAPH). SIGGRAPH)373You Xie, Erik Franz, Mengyu Chu, and Nils Thuerey. tempoGAN: A Temporally Coherent, Volumetric GAN for Super-resolution Fluid Flow. ACM Transactions on Graph- ics (Proc. SIGGRAPH), 37(4), 2018. 2, 3\n\nSimulcap: Singleview human performance capture with cloth simulation. Tao Yu, Zerong Zheng, Yuan Zhong, Jianhui Zhao, Qionghai Dai, Gerard Pons-Moll, Yebin Liu, Proc. of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). of IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)Tao Yu, Zerong Zheng, Yuan Zhong, Jianhui Zhao, Qionghai Dai, Gerard Pons-Moll, and Yebin Liu. Simulcap: Single- view human performance capture with cloth simulation. In Proc. of IEEE/CVF Conference on Computer Vision and Pat- tern Recognition (CVPR), 2019. 2\n\nDetailed, accurate, human shape estimation from clothed 3d scan sequences. Chao Zhang, Sergi Pujades, J Michael, Gerard Black, Pons-Moll, Proc. of Computer Vision and Pattern Recognition (CVPR). of Computer Vision and Pattern Recognition (CVPR)Chao Zhang, Sergi Pujades, Michael J Black, and Gerard Pons-Moll. Detailed, accurate, human shape estimation from clothed 3d scan sequences. In Proc. of Computer Vision and Pattern Recognition (CVPR), pages 4191-4200, 2017. 2\n\nM3D-VTON: A Monocular-to-3D Virtual Try-On Network. Fuwei Zhao, Zhenyu Xie, Michael C Kampffmeyer, Haoye Dong, Songfang Han, Tianxiang Zheng, Tao Zhang, Xiaodan Liang, Proc. of IEEE International Conference on Computer Vision (ICCV). of IEEE International Conference on Computer Vision (ICCV)2021Fuwei Zhao, Zhenyu Xie, Michael C. Kampffmeyer, Haoye Dong, Songfang Han, Tianxiang Zheng, Tao Zhang, and Xi- aodan Liang. M3D-VTON: A Monocular-to-3D Virtual Try- On Network. In Proc. of IEEE International Conference on Computer Vision (ICCV), 2021. 2\n\nDeep Fashion3D: A Dataset and Benchmark for 3D Garment Reconstruction from Single Images. Heming Zhu, Yu Cao, Hang Jin, Weikai Chen, Dong Du, Zhangye Wang, Shuguang Cui, Xiaoguang Han, Proc. of European Conference on Computer Vision (ECCV). of European Conference on Computer Vision (ECCV)2020Heming Zhu, Yu Cao, Hang Jin, Weikai Chen, Dong Du, Zhangye Wang, Shuguang Cui, and Xiaoguang Han. Deep Fashion3D: A Dataset and Benchmark for 3D Garment Re- construction from Single Images. In Proc. of European Con- ference on Computer Vision (ECCV), 2020. 2\n\nUnpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A Efros, Proc. of IEEE International Conference on Computer Vision (ICCV). of IEEE International Conference on Computer Vision (ICCV)Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired Image-to-Image Translation using Cycle- Consistent Adversarial Networks . In Proc. of IEEE Interna- tional Conference on Computer Vision (ICCV), pages 2223- 2232, 2017. 2\n\nPhysics-constrained deep learning for high-dimensional surrogate modeling and uncertainty quantification without labeled data. Yinhao Zhu, Nicholas Zabaras, Journal of Computational Physics. 3943Phaedon-Stelios Koutsourelakis, and Paris PerdikarisYinhao Zhu, Nicholas Zabaras, Phaedon-Stelios Koutsoure- lakis, and Paris Perdikaris. Physics-constrained deep learn- ing for high-dimensional surrogate modeling and uncertainty quantification without labeled data. Journal of Computa- tional Physics, 394:56-81, 2019. 2, 3\n", "annotations": {"author": "[{\"end\":248,\"start\":174},{\"end\":322,\"start\":249},{\"end\":390,\"start\":323},{\"end\":463,\"start\":391}]", "publisher": null, "author_last_name": "[{\"end\":190,\"start\":179},{\"end\":264,\"start\":258},{\"end\":332,\"start\":327},{\"end\":405,\"start\":398}]", "author_first_name": "[{\"end\":178,\"start\":174},{\"end\":255,\"start\":249},{\"end\":257,\"start\":256},{\"end\":326,\"start\":323},{\"end\":397,\"start\":391}]", "author_affiliation": "[{\"end\":247,\"start\":192},{\"end\":321,\"start\":266},{\"end\":389,\"start\":334},{\"end\":462,\"start\":407}]", "title": "[{\"end\":171,\"start\":1},{\"end\":634,\"start\":464}]", "venue": null, "abstract": "[{\"end\":2130,\"start\":636}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2412,\"start\":2408},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2575,\"start\":2571},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2578,\"start\":2575},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2581,\"start\":2578},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":2584,\"start\":2581},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":2587,\"start\":2584},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":2590,\"start\":2587},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":2593,\"start\":2590},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":2596,\"start\":2593},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3430,\"start\":3426},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3465,\"start\":3461},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3711,\"start\":3707},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":3714,\"start\":3711},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4098,\"start\":4094},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":4998,\"start\":4994},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6205,\"start\":6201},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6208,\"start\":6205},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6237,\"start\":6234},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6278,\"start\":6275},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":6281,\"start\":6278},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":6315,\"start\":6311},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6397,\"start\":6393},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6519,\"start\":6515},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6598,\"start\":6594},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":6601,\"start\":6598},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6778,\"start\":6774},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6781,\"start\":6778},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6864,\"start\":6860},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6867,\"start\":6864},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7325,\"start\":7321},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7452,\"start\":7448},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":7455,\"start\":7452},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":7467,\"start\":7463},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7487,\"start\":7484},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":7490,\"start\":7487},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7503,\"start\":7499},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7506,\"start\":7503},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":7509,\"start\":7506},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":7531,\"start\":7527},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7790,\"start\":7787},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7792,\"start\":7790},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7795,\"start\":7792},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":7798,\"start\":7795},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7819,\"start\":7815},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":7822,\"start\":7819},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":7825,\"start\":7822},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7840,\"start\":7837},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7843,\"start\":7840},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7846,\"start\":7843},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7849,\"start\":7846},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7852,\"start\":7849},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":7855,\"start\":7852},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7878,\"start\":7875},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7881,\"start\":7878},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":7884,\"start\":7881},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":7887,\"start\":7884},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8091,\"start\":8087},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8094,\"start\":8091},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8097,\"start\":8094},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":8100,\"start\":8097},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8156,\"start\":8152},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8159,\"start\":8156},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8162,\"start\":8159},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8181,\"start\":8177},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8184,\"start\":8181},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":8202,\"start\":8198},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8317,\"start\":8314},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8320,\"start\":8317},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8323,\"start\":8320},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8346,\"start\":8342},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8349,\"start\":8346},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":8352,\"start\":8349},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":8355,\"start\":8352},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8434,\"start\":8431},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8437,\"start\":8434},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8440,\"start\":8437},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":8443,\"start\":8440},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8526,\"start\":8522},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8540,\"start\":8536},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8575,\"start\":8571},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8578,\"start\":8575},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8581,\"start\":8578},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":8584,\"start\":8581},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8657,\"start\":8653},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":8660,\"start\":8657},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9003,\"start\":8999},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":9235,\"start\":9231},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":9327,\"start\":9323},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9330,\"start\":9327},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":9333,\"start\":9330},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9446,\"start\":9442},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9449,\"start\":9446},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9452,\"start\":9449},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":9576,\"start\":9572},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":9579,\"start\":9576},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":9582,\"start\":9579},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":9616,\"start\":9612},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":9756,\"start\":9752},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":9865,\"start\":9861},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10227,\"start\":10224},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11869,\"start\":11866},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11872,\"start\":11869},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":11875,\"start\":11872},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":11878,\"start\":11875},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":11881,\"start\":11878},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11937,\"start\":11933},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11940,\"start\":11937},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12063,\"start\":12059},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12952,\"start\":12948},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":13645,\"start\":13641},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":13952,\"start\":13948},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14274,\"start\":14270},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14277,\"start\":14274},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14764,\"start\":14760},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":14767,\"start\":14764},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":16337,\"start\":16333},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":16340,\"start\":16337},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":16912,\"start\":16908},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":17108,\"start\":17104},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18902,\"start\":18899},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":19958,\"start\":19957},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21280,\"start\":21276},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":21802,\"start\":21798},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":21822,\"start\":21818},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":23061,\"start\":23057},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23405,\"start\":23402},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":24650,\"start\":24646},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":24694,\"start\":24690},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25149,\"start\":25146},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":25514,\"start\":25510},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":25517,\"start\":25514},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":25520,\"start\":25517},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":25877,\"start\":25874},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":25938,\"start\":25934},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":27121,\"start\":27117},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":27713,\"start\":27709},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":27732,\"start\":27728},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27772,\"start\":27769},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":27938,\"start\":27934},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":28199,\"start\":28195},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":28202,\"start\":28199},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":28266,\"start\":28263},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":31790,\"start\":31786},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":31805,\"start\":31801},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":31814,\"start\":31811},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":31830,\"start\":31826},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":32107,\"start\":32103},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":32110,\"start\":32107},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":32179,\"start\":32175}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":32787,\"start\":32742},{\"attributes\":{\"id\":\"fig_2\"},\"end\":33023,\"start\":32788},{\"attributes\":{\"id\":\"fig_5\"},\"end\":33298,\"start\":33024}]", "paragraph": "[{\"end\":3204,\"start\":2146},{\"end\":3794,\"start\":3206},{\"end\":4656,\"start\":3796},{\"end\":5415,\"start\":4658},{\"end\":5781,\"start\":5417},{\"end\":7000,\"start\":5798},{\"end\":7532,\"start\":7002},{\"end\":8356,\"start\":7534},{\"end\":8873,\"start\":8358},{\"end\":10738,\"start\":8875},{\"end\":11787,\"start\":10749},{\"end\":12252,\"start\":11805},{\"end\":12854,\"start\":12322},{\"end\":13156,\"start\":12856},{\"end\":14463,\"start\":13199},{\"end\":14530,\"start\":14465},{\"end\":14768,\"start\":14590},{\"end\":14785,\"start\":14770},{\"end\":15333,\"start\":14819},{\"end\":15739,\"start\":15595},{\"end\":15879,\"start\":15770},{\"end\":16723,\"start\":15919},{\"end\":17141,\"start\":16742},{\"end\":17601,\"start\":17143},{\"end\":17777,\"start\":17636},{\"end\":17873,\"start\":17809},{\"end\":17983,\"start\":17875},{\"end\":18107,\"start\":18022},{\"end\":18243,\"start\":18109},{\"end\":18496,\"start\":18276},{\"end\":18742,\"start\":18554},{\"end\":19099,\"start\":18744},{\"end\":20251,\"start\":19135},{\"end\":21075,\"start\":20253},{\"end\":21917,\"start\":21101},{\"end\":22591,\"start\":21919},{\"end\":23705,\"start\":22593},{\"end\":24140,\"start\":23707},{\"end\":24463,\"start\":24142},{\"end\":24595,\"start\":24465},{\"end\":24968,\"start\":24604},{\"end\":26560,\"start\":24996},{\"end\":26656,\"start\":26562},{\"end\":26860,\"start\":26658},{\"end\":27393,\"start\":26862},{\"end\":28698,\"start\":27420},{\"end\":29097,\"start\":28700},{\"end\":29721,\"start\":29099},{\"end\":29946,\"start\":29723},{\"end\":30386,\"start\":29977},{\"end\":30699,\"start\":30388},{\"end\":31444,\"start\":30701},{\"end\":31751,\"start\":31446},{\"end\":32180,\"start\":31767},{\"end\":32607,\"start\":32182},{\"end\":32741,\"start\":32609}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12321,\"start\":12253},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14589,\"start\":14531},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14818,\"start\":14786},{\"attributes\":{\"id\":\"formula_3\"},\"end\":15553,\"start\":15334},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15769,\"start\":15740},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15918,\"start\":15880},{\"attributes\":{\"id\":\"formula_6\"},\"end\":17635,\"start\":17602},{\"attributes\":{\"id\":\"formula_7\"},\"end\":17808,\"start\":17778},{\"attributes\":{\"id\":\"formula_8\"},\"end\":18021,\"start\":17984},{\"attributes\":{\"id\":\"formula_9\"},\"end\":18275,\"start\":18244},{\"attributes\":{\"id\":\"formula_10\"},\"end\":18553,\"start\":18497}]", "table_ref": "[{\"end\":23604,\"start\":23597},{\"end\":24760,\"start\":24753},{\"end\":26200,\"start\":26193},{\"end\":26710,\"start\":26703},{\"end\":26881,\"start\":26874}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2144,\"start\":2132},{\"attributes\":{\"n\":\"2.\"},\"end\":5796,\"start\":5784},{\"attributes\":{\"n\":\"3.\"},\"end\":10747,\"start\":10741},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11803,\"start\":11790},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13197,\"start\":13159},{\"attributes\":{\"n\":\"3.3.\"},\"end\":15593,\"start\":15555},{\"attributes\":{\"n\":\"3.4.\"},\"end\":16740,\"start\":16726},{\"attributes\":{\"n\":\"3.5.\"},\"end\":19133,\"start\":19102},{\"attributes\":{\"n\":\"4.\"},\"end\":21088,\"start\":21078},{\"attributes\":{\"n\":\"4.1.\"},\"end\":21099,\"start\":21091},{\"end\":24602,\"start\":24598},{\"attributes\":{\"n\":\"4.2.\"},\"end\":24994,\"start\":24971},{\"attributes\":{\"n\":\"4.3.\"},\"end\":27418,\"start\":27396},{\"attributes\":{\"n\":\"5.\"},\"end\":29975,\"start\":29949},{\"end\":31765,\"start\":31754},{\"end\":32799,\"start\":32789},{\"end\":33033,\"start\":33025}]", "table": null, "figure_caption": "[{\"end\":32787,\"start\":32744},{\"end\":33023,\"start\":32801},{\"end\":33298,\"start\":33035}]", "figure_ref": "[{\"end\":14982,\"start\":14974},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18806,\"start\":18798},{\"end\":19544,\"start\":19535},{\"end\":22888,\"start\":22880},{\"end\":25740,\"start\":25732},{\"end\":27468,\"start\":27460},{\"end\":28612,\"start\":28604},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":28911,\"start\":28903},{\"end\":29124,\"start\":29116},{\"end\":31510,\"start\":31502}]", "bib_author_first_name": "[{\"end\":33568,\"start\":33562},{\"end\":33585,\"start\":33579},{\"end\":33600,\"start\":33594},{\"end\":33604,\"start\":33601},{\"end\":33625,\"start\":33616},{\"end\":33642,\"start\":33636},{\"end\":34047,\"start\":34041},{\"end\":34064,\"start\":34058},{\"end\":34080,\"start\":34073},{\"end\":34094,\"start\":34085},{\"end\":34111,\"start\":34105},{\"end\":34521,\"start\":34515},{\"end\":34538,\"start\":34532},{\"end\":34551,\"start\":34543},{\"end\":34942,\"start\":34937},{\"end\":34957,\"start\":34951},{\"end\":35321,\"start\":35317},{\"end\":35338,\"start\":35332},{\"end\":35353,\"start\":35347},{\"end\":35716,\"start\":35712},{\"end\":35733,\"start\":35727},{\"end\":35748,\"start\":35742},{\"end\":36113,\"start\":36109},{\"end\":36130,\"start\":36124},{\"end\":36145,\"start\":36139},{\"end\":36160,\"start\":36154},{\"end\":36597,\"start\":36590},{\"end\":36629,\"start\":36620},{\"end\":36644,\"start\":36638},{\"end\":37108,\"start\":37102},{\"end\":37124,\"start\":37118},{\"end\":37137,\"start\":37133},{\"end\":37383,\"start\":37376},{\"end\":37396,\"start\":37391},{\"end\":37416,\"start\":37411},{\"end\":37431,\"start\":37425},{\"end\":37433,\"start\":37432},{\"end\":37728,\"start\":37723},{\"end\":37743,\"start\":37737},{\"end\":37761,\"start\":37754},{\"end\":37776,\"start\":37770},{\"end\":37796,\"start\":37788},{\"end\":38200,\"start\":38194},{\"end\":38208,\"start\":38207},{\"end\":38222,\"start\":38215},{\"end\":38236,\"start\":38230},{\"end\":38257,\"start\":38249},{\"end\":38277,\"start\":38269},{\"end\":38292,\"start\":38286},{\"end\":38738,\"start\":38732},{\"end\":38748,\"start\":38745},{\"end\":38759,\"start\":38756},{\"end\":39118,\"start\":39117},{\"end\":39134,\"start\":39129},{\"end\":39147,\"start\":39141},{\"end\":39168,\"start\":39159},{\"end\":39188,\"start\":39180},{\"end\":39525,\"start\":39520},{\"end\":39540,\"start\":39536},{\"end\":39542,\"start\":39541},{\"end\":39558,\"start\":39551},{\"end\":39573,\"start\":39568},{\"end\":39949,\"start\":39942},{\"end\":39969,\"start\":39968},{\"end\":39986,\"start\":39977},{\"end\":40007,\"start\":39998},{\"end\":40300,\"start\":40295},{\"end\":40317,\"start\":40311},{\"end\":40338,\"start\":40330},{\"end\":40355,\"start\":40351},{\"end\":40369,\"start\":40362},{\"end\":40386,\"start\":40380},{\"end\":40848,\"start\":40840},{\"end\":40857,\"start\":40853},{\"end\":40875,\"start\":40868},{\"end\":40882,\"start\":40880},{\"end\":40894,\"start\":40888},{\"end\":40909,\"start\":40901},{\"end\":40929,\"start\":40924},{\"end\":41181,\"start\":41177},{\"end\":41195,\"start\":41189},{\"end\":41209,\"start\":41200},{\"end\":41222,\"start\":41219},{\"end\":41231,\"start\":41227},{\"end\":41649,\"start\":41644},{\"end\":41668,\"start\":41660},{\"end\":41677,\"start\":41674},{\"end\":41941,\"start\":41933},{\"end\":41943,\"start\":41942},{\"end\":41956,\"start\":41952},{\"end\":41958,\"start\":41957},{\"end\":41971,\"start\":41966},{\"end\":42204,\"start\":42196},{\"end\":42415,\"start\":42410},{\"end\":42430,\"start\":42424},{\"end\":42444,\"start\":42440},{\"end\":42826,\"start\":42825},{\"end\":42837,\"start\":42833},{\"end\":42852,\"start\":42845},{\"end\":43346,\"start\":43343},{\"end\":43357,\"start\":43351},{\"end\":43371,\"start\":43366},{\"end\":43388,\"start\":43380},{\"end\":43416,\"start\":43409},{\"end\":43431,\"start\":43425},{\"end\":43433,\"start\":43432},{\"end\":43449,\"start\":43441},{\"end\":43830,\"start\":43823},{\"end\":43840,\"start\":43835},{\"end\":43852,\"start\":43846},{\"end\":43863,\"start\":43858},{\"end\":43892,\"start\":43882},{\"end\":43905,\"start\":43902},{\"end\":44360,\"start\":44357},{\"end\":44369,\"start\":44365},{\"end\":44389,\"start\":44381},{\"end\":44411,\"start\":44404},{\"end\":44424,\"start\":44419},{\"end\":44442,\"start\":44433},{\"end\":44880,\"start\":44873},{\"end\":44892,\"start\":44888},{\"end\":44905,\"start\":44898},{\"end\":45245,\"start\":45237},{\"end\":45257,\"start\":45251},{\"end\":45275,\"start\":45267},{\"end\":45519,\"start\":45512},{\"end\":45534,\"start\":45527},{\"end\":45550,\"start\":45544},{\"end\":45565,\"start\":45559},{\"end\":45586,\"start\":45577},{\"end\":45893,\"start\":45887},{\"end\":45905,\"start\":45898},{\"end\":45918,\"start\":45912},{\"end\":45932,\"start\":45927},{\"end\":45948,\"start\":45942},{\"end\":45964,\"start\":45960},{\"end\":45978,\"start\":45971},{\"end\":45980,\"start\":45979},{\"end\":46385,\"start\":46379},{\"end\":46397,\"start\":46390},{\"end\":46408,\"start\":46404},{\"end\":46424,\"start\":46415},{\"end\":46807,\"start\":46800},{\"end\":46821,\"start\":46817},{\"end\":46833,\"start\":46832},{\"end\":46850,\"start\":46844},{\"end\":46865,\"start\":46858},{\"end\":46867,\"start\":46866},{\"end\":47304,\"start\":47295},{\"end\":47321,\"start\":47313},{\"end\":47341,\"start\":47336},{\"end\":47358,\"start\":47352},{\"end\":47587,\"start\":47582},{\"end\":47603,\"start\":47600},{\"end\":47612,\"start\":47611},{\"end\":47626,\"start\":47622},{\"end\":47976,\"start\":47972},{\"end\":47993,\"start\":47985},{\"end\":48014,\"start\":48008},{\"end\":48029,\"start\":48022},{\"end\":48262,\"start\":48257},{\"end\":48276,\"start\":48271},{\"end\":48299,\"start\":48284},{\"end\":48596,\"start\":48590},{\"end\":48613,\"start\":48605},{\"end\":48629,\"start\":48622},{\"end\":48642,\"start\":48638},{\"end\":48657,\"start\":48653},{\"end\":48995,\"start\":48986},{\"end\":49016,\"start\":49003},{\"end\":49029,\"start\":49023},{\"end\":49457,\"start\":49451},{\"end\":49474,\"start\":49469},{\"end\":49489,\"start\":49484},{\"end\":49501,\"start\":49494},{\"end\":49503,\"start\":49502},{\"end\":49771,\"start\":49767},{\"end\":49784,\"start\":49777},{\"end\":49801,\"start\":49795},{\"end\":49816,\"start\":49809},{\"end\":49826,\"start\":49821},{\"end\":49840,\"start\":49835},{\"end\":50245,\"start\":50237},{\"end\":50260,\"start\":50254},{\"end\":50262,\"start\":50261},{\"end\":50274,\"start\":50271},{\"end\":50287,\"start\":50282},{\"end\":50633,\"start\":50625},{\"end\":50645,\"start\":50641},{\"end\":50658,\"start\":50653},{\"end\":50674,\"start\":50668},{\"end\":50692,\"start\":50686},{\"end\":50706,\"start\":50703},{\"end\":51177,\"start\":51169},{\"end\":51192,\"start\":51185},{\"end\":51205,\"start\":51199},{\"end\":51217,\"start\":51210},{\"end\":51219,\"start\":51218},{\"end\":51604,\"start\":51600},{\"end\":51624,\"start\":51618},{\"end\":51626,\"start\":51625},{\"end\":51638,\"start\":51635},{\"end\":51942,\"start\":51940},{\"end\":51956,\"start\":51949},{\"end\":51968,\"start\":51964},{\"end\":51970,\"start\":51969},{\"end\":52372,\"start\":52363},{\"end\":52388,\"start\":52382},{\"end\":52706,\"start\":52699},{\"end\":52723,\"start\":52716},{\"end\":53132,\"start\":53129},{\"end\":53147,\"start\":53139},{\"end\":53163,\"start\":53154},{\"end\":53176,\"start\":53169},{\"end\":53189,\"start\":53183},{\"end\":53564,\"start\":53557},{\"end\":53579,\"start\":53573},{\"end\":53583,\"start\":53580},{\"end\":53599,\"start\":53595},{\"end\":53612,\"start\":53606},{\"end\":54055,\"start\":54048},{\"end\":54072,\"start\":54064},{\"end\":54089,\"start\":54085},{\"end\":54102,\"start\":54096},{\"end\":54552,\"start\":54544},{\"end\":54571,\"start\":54562},{\"end\":54589,\"start\":54584},{\"end\":54605,\"start\":54602},{\"end\":54970,\"start\":54964},{\"end\":54985,\"start\":54981},{\"end\":55004,\"start\":54999},{\"end\":55016,\"start\":55013},{\"end\":55351,\"start\":55344},{\"end\":55363,\"start\":55358},{\"end\":55382,\"start\":55376},{\"end\":55394,\"start\":55387},{\"end\":55407,\"start\":55403},{\"end\":55766,\"start\":55765},{\"end\":55782,\"start\":55777},{\"end\":55794,\"start\":55789},{\"end\":55810,\"start\":55803},{\"end\":56149,\"start\":56148},{\"end\":56167,\"start\":56160},{\"end\":56177,\"start\":56174},{\"end\":56191,\"start\":56184},{\"end\":56509,\"start\":56501},{\"end\":56522,\"start\":56514},{\"end\":56534,\"start\":56528},{\"end\":56547,\"start\":56540},{\"end\":56904,\"start\":56901},{\"end\":56914,\"start\":56910},{\"end\":56928,\"start\":56922},{\"end\":56938,\"start\":56934},{\"end\":57250,\"start\":57247},{\"end\":57261,\"start\":57255},{\"end\":57273,\"start\":57269},{\"end\":57288,\"start\":57281},{\"end\":57303,\"start\":57295},{\"end\":57315,\"start\":57309},{\"end\":57332,\"start\":57327},{\"end\":57830,\"start\":57826},{\"end\":57843,\"start\":57838},{\"end\":57854,\"start\":57853},{\"end\":57870,\"start\":57864},{\"end\":58279,\"start\":58274},{\"end\":58292,\"start\":58286},{\"end\":58305,\"start\":58298},{\"end\":58307,\"start\":58306},{\"end\":58326,\"start\":58321},{\"end\":58341,\"start\":58333},{\"end\":58356,\"start\":58347},{\"end\":58367,\"start\":58364},{\"end\":58382,\"start\":58375},{\"end\":58868,\"start\":58862},{\"end\":58876,\"start\":58874},{\"end\":58886,\"start\":58882},{\"end\":58898,\"start\":58892},{\"end\":58909,\"start\":58905},{\"end\":58921,\"start\":58914},{\"end\":58936,\"start\":58928},{\"end\":58951,\"start\":58942},{\"end\":59414,\"start\":59407},{\"end\":59427,\"start\":59420},{\"end\":59441,\"start\":59434},{\"end\":59455,\"start\":59449},{\"end\":59457,\"start\":59456},{\"end\":59965,\"start\":59959},{\"end\":59979,\"start\":59971}]", "bib_author_last_name": "[{\"end\":33577,\"start\":33569},{\"end\":33592,\"start\":33586},{\"end\":33614,\"start\":33605},{\"end\":33634,\"start\":33626},{\"end\":33652,\"start\":33643},{\"end\":34056,\"start\":34048},{\"end\":34071,\"start\":34065},{\"end\":34083,\"start\":34081},{\"end\":34103,\"start\":34095},{\"end\":34121,\"start\":34112},{\"end\":34530,\"start\":34522},{\"end\":34541,\"start\":34539},{\"end\":34564,\"start\":34552},{\"end\":34949,\"start\":34943},{\"end\":34964,\"start\":34958},{\"end\":35330,\"start\":35322},{\"end\":35345,\"start\":35339},{\"end\":35362,\"start\":35354},{\"end\":35725,\"start\":35717},{\"end\":35740,\"start\":35734},{\"end\":35757,\"start\":35749},{\"end\":36122,\"start\":36114},{\"end\":36137,\"start\":36131},{\"end\":36152,\"start\":36146},{\"end\":36169,\"start\":36161},{\"end\":36618,\"start\":36598},{\"end\":36636,\"start\":36630},{\"end\":36653,\"start\":36645},{\"end\":36664,\"start\":36655},{\"end\":37116,\"start\":37109},{\"end\":37131,\"start\":37125},{\"end\":37146,\"start\":37138},{\"end\":37389,\"start\":37384},{\"end\":37409,\"start\":37397},{\"end\":37423,\"start\":37417},{\"end\":37440,\"start\":37434},{\"end\":37735,\"start\":37729},{\"end\":37752,\"start\":37744},{\"end\":37768,\"start\":37762},{\"end\":37786,\"start\":37777},{\"end\":37810,\"start\":37797},{\"end\":38205,\"start\":38201},{\"end\":38213,\"start\":38209},{\"end\":38228,\"start\":38223},{\"end\":38247,\"start\":38237},{\"end\":38267,\"start\":38258},{\"end\":38284,\"start\":38278},{\"end\":38300,\"start\":38293},{\"end\":38314,\"start\":38302},{\"end\":38743,\"start\":38739},{\"end\":38754,\"start\":38749},{\"end\":38767,\"start\":38760},{\"end\":39127,\"start\":39119},{\"end\":39139,\"start\":39135},{\"end\":39157,\"start\":39148},{\"end\":39178,\"start\":39169},{\"end\":39194,\"start\":39189},{\"end\":39201,\"start\":39196},{\"end\":39534,\"start\":39526},{\"end\":39549,\"start\":39543},{\"end\":39566,\"start\":39559},{\"end\":39582,\"start\":39574},{\"end\":39959,\"start\":39950},{\"end\":39966,\"start\":39961},{\"end\":39975,\"start\":39970},{\"end\":39996,\"start\":39987},{\"end\":40013,\"start\":40008},{\"end\":40020,\"start\":40015},{\"end\":40309,\"start\":40301},{\"end\":40328,\"start\":40318},{\"end\":40349,\"start\":40339},{\"end\":40360,\"start\":40356},{\"end\":40378,\"start\":40370},{\"end\":40390,\"start\":40387},{\"end\":40851,\"start\":40849},{\"end\":40866,\"start\":40858},{\"end\":40878,\"start\":40876},{\"end\":40886,\"start\":40883},{\"end\":40899,\"start\":40895},{\"end\":40922,\"start\":40910},{\"end\":40936,\"start\":40930},{\"end\":41187,\"start\":41182},{\"end\":41198,\"start\":41196},{\"end\":41217,\"start\":41210},{\"end\":41225,\"start\":41223},{\"end\":41236,\"start\":41232},{\"end\":41658,\"start\":41650},{\"end\":41672,\"start\":41669},{\"end\":41682,\"start\":41678},{\"end\":41690,\"start\":41684},{\"end\":41950,\"start\":41944},{\"end\":41964,\"start\":41959},{\"end\":41981,\"start\":41972},{\"end\":42208,\"start\":42205},{\"end\":42422,\"start\":42416},{\"end\":42438,\"start\":42431},{\"end\":42449,\"start\":42445},{\"end\":42831,\"start\":42827},{\"end\":42843,\"start\":42838},{\"end\":42860,\"start\":42853},{\"end\":42866,\"start\":42862},{\"end\":43349,\"start\":43347},{\"end\":43364,\"start\":43358},{\"end\":43378,\"start\":43372},{\"end\":43407,\"start\":43389},{\"end\":43423,\"start\":43417},{\"end\":43439,\"start\":43434},{\"end\":43459,\"start\":43450},{\"end\":43833,\"start\":43831},{\"end\":43844,\"start\":43841},{\"end\":43856,\"start\":43853},{\"end\":43880,\"start\":43864},{\"end\":43900,\"start\":43893},{\"end\":43910,\"start\":43906},{\"end\":43917,\"start\":43912},{\"end\":44363,\"start\":44361},{\"end\":44379,\"start\":44370},{\"end\":44402,\"start\":44390},{\"end\":44417,\"start\":44412},{\"end\":44431,\"start\":44425},{\"end\":44451,\"start\":44443},{\"end\":44886,\"start\":44881},{\"end\":44896,\"start\":44893},{\"end\":44912,\"start\":44906},{\"end\":45249,\"start\":45246},{\"end\":45265,\"start\":45258},{\"end\":45281,\"start\":45276},{\"end\":45525,\"start\":45520},{\"end\":45542,\"start\":45535},{\"end\":45557,\"start\":45551},{\"end\":45575,\"start\":45566},{\"end\":45592,\"start\":45587},{\"end\":45896,\"start\":45894},{\"end\":45910,\"start\":45906},{\"end\":45925,\"start\":45919},{\"end\":45940,\"start\":45933},{\"end\":45958,\"start\":45949},{\"end\":45969,\"start\":45965},{\"end\":45986,\"start\":45981},{\"end\":46388,\"start\":46386},{\"end\":46402,\"start\":46398},{\"end\":46413,\"start\":46409},{\"end\":46430,\"start\":46425},{\"end\":46815,\"start\":46808},{\"end\":46830,\"start\":46822},{\"end\":46842,\"start\":46834},{\"end\":46856,\"start\":46851},{\"end\":46877,\"start\":46868},{\"end\":46884,\"start\":46879},{\"end\":47311,\"start\":47305},{\"end\":47334,\"start\":47322},{\"end\":47350,\"start\":47342},{\"end\":47364,\"start\":47359},{\"end\":47598,\"start\":47588},{\"end\":47609,\"start\":47604},{\"end\":47620,\"start\":47613},{\"end\":47632,\"start\":47627},{\"end\":47638,\"start\":47634},{\"end\":47983,\"start\":47977},{\"end\":48006,\"start\":47994},{\"end\":48020,\"start\":48015},{\"end\":48034,\"start\":48030},{\"end\":48269,\"start\":48263},{\"end\":48282,\"start\":48277},{\"end\":48305,\"start\":48300},{\"end\":48603,\"start\":48597},{\"end\":48620,\"start\":48614},{\"end\":48636,\"start\":48630},{\"end\":48651,\"start\":48643},{\"end\":48665,\"start\":48658},{\"end\":49001,\"start\":48996},{\"end\":49021,\"start\":49017},{\"end\":49039,\"start\":49030},{\"end\":49467,\"start\":49458},{\"end\":49482,\"start\":49475},{\"end\":49492,\"start\":49490},{\"end\":49509,\"start\":49504},{\"end\":49775,\"start\":49772},{\"end\":49793,\"start\":49785},{\"end\":49807,\"start\":49802},{\"end\":49819,\"start\":49817},{\"end\":49833,\"start\":49827},{\"end\":49845,\"start\":49841},{\"end\":50252,\"start\":50246},{\"end\":50269,\"start\":50263},{\"end\":50280,\"start\":50275},{\"end\":50293,\"start\":50288},{\"end\":50639,\"start\":50634},{\"end\":50651,\"start\":50646},{\"end\":50666,\"start\":50659},{\"end\":50684,\"start\":50675},{\"end\":50701,\"start\":50693},{\"end\":50709,\"start\":50707},{\"end\":51183,\"start\":51178},{\"end\":51197,\"start\":51193},{\"end\":51208,\"start\":51206},{\"end\":51225,\"start\":51220},{\"end\":51616,\"start\":51605},{\"end\":51633,\"start\":51627},{\"end\":51644,\"start\":51639},{\"end\":51947,\"start\":51943},{\"end\":51962,\"start\":51957},{\"end\":51974,\"start\":51971},{\"end\":52380,\"start\":52373},{\"end\":52395,\"start\":52389},{\"end\":52714,\"start\":52707},{\"end\":52729,\"start\":52724},{\"end\":53137,\"start\":53133},{\"end\":53152,\"start\":53148},{\"end\":53167,\"start\":53164},{\"end\":53181,\"start\":53177},{\"end\":53197,\"start\":53190},{\"end\":53571,\"start\":53565},{\"end\":53593,\"start\":53584},{\"end\":53604,\"start\":53600},{\"end\":53622,\"start\":53613},{\"end\":54062,\"start\":54056},{\"end\":54083,\"start\":54073},{\"end\":54094,\"start\":54090},{\"end\":54112,\"start\":54103},{\"end\":54560,\"start\":54553},{\"end\":54582,\"start\":54572},{\"end\":54600,\"start\":54590},{\"end\":54612,\"start\":54606},{\"end\":54979,\"start\":54971},{\"end\":54997,\"start\":54986},{\"end\":55011,\"start\":55005},{\"end\":55022,\"start\":55017},{\"end\":55356,\"start\":55352},{\"end\":55374,\"start\":55364},{\"end\":55385,\"start\":55383},{\"end\":55401,\"start\":55395},{\"end\":55412,\"start\":55408},{\"end\":55775,\"start\":55767},{\"end\":55787,\"start\":55783},{\"end\":55801,\"start\":55795},{\"end\":55818,\"start\":55811},{\"end\":55825,\"start\":55820},{\"end\":56158,\"start\":56150},{\"end\":56172,\"start\":56168},{\"end\":56182,\"start\":56178},{\"end\":56194,\"start\":56192},{\"end\":56201,\"start\":56196},{\"end\":56512,\"start\":56510},{\"end\":56526,\"start\":56523},{\"end\":56538,\"start\":56535},{\"end\":56551,\"start\":56548},{\"end\":56908,\"start\":56905},{\"end\":56920,\"start\":56915},{\"end\":56932,\"start\":56929},{\"end\":56946,\"start\":56939},{\"end\":57253,\"start\":57251},{\"end\":57267,\"start\":57262},{\"end\":57279,\"start\":57274},{\"end\":57293,\"start\":57289},{\"end\":57307,\"start\":57304},{\"end\":57325,\"start\":57316},{\"end\":57336,\"start\":57333},{\"end\":57836,\"start\":57831},{\"end\":57851,\"start\":57844},{\"end\":57862,\"start\":57855},{\"end\":57876,\"start\":57871},{\"end\":57887,\"start\":57878},{\"end\":58284,\"start\":58280},{\"end\":58296,\"start\":58293},{\"end\":58319,\"start\":58308},{\"end\":58331,\"start\":58327},{\"end\":58345,\"start\":58342},{\"end\":58362,\"start\":58357},{\"end\":58373,\"start\":58368},{\"end\":58388,\"start\":58383},{\"end\":58872,\"start\":58869},{\"end\":58880,\"start\":58877},{\"end\":58890,\"start\":58887},{\"end\":58903,\"start\":58899},{\"end\":58912,\"start\":58910},{\"end\":58926,\"start\":58922},{\"end\":58940,\"start\":58937},{\"end\":58955,\"start\":58952},{\"end\":59418,\"start\":59415},{\"end\":59432,\"start\":59428},{\"end\":59447,\"start\":59442},{\"end\":59463,\"start\":59458},{\"end\":59969,\"start\":59966},{\"end\":59987,\"start\":59980}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":76666456},\"end\":33994,\"start\":33493},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":51929974},\"end\":34438,\"start\":33996},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":237278058},\"end\":34902,\"start\":34440},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":326544},\"end\":35287,\"start\":34904},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":208637139},\"end\":35620,\"start\":35289},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":235185437},\"end\":36022,\"start\":35622},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":221516172},\"end\":36528,\"start\":36024},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":201070729},\"end\":37026,\"start\":36530},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":5632716},\"end\":37336,\"start\":37028},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1393088},\"end\":37659,\"start\":37338},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":232185555},\"end\":38146,\"start\":37661},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":208857381},\"end\":38665,\"start\":38148},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":6094650},\"end\":39069,\"start\":38667},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2349138},\"end\":39501,\"start\":39071},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":6425185},\"end\":39912,\"start\":39503},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":207194165},\"end\":40224,\"start\":39914},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":53763742},\"end\":40774,\"start\":40226},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":203626832},\"end\":41124,\"start\":40776},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":215548941},\"end\":41592,\"start\":41126},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":54458149},\"end\":41887,\"start\":41594},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":1921881},\"end\":42141,\"start\":41889},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":221737757},\"end\":42352,\"start\":42143},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":51967305},\"end\":42724,\"start\":42354},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":12672235},\"end\":43272,\"start\":42726},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":51875045},\"end\":43749,\"start\":43274},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":212725830},\"end\":44265,\"start\":43751},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":227162167},\"end\":44817,\"start\":44267},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":202780283},\"end\":45162,\"start\":44819},{\"attributes\":{\"doi\":\"2017. 3\",\"id\":\"b28\",\"matched_paper_id\":19463977},\"end\":45468,\"start\":45164},{\"attributes\":{\"id\":\"b29\"},\"end\":45833,\"start\":45470},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":209386419},\"end\":46324,\"start\":45835},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":237382270},\"end\":46746,\"start\":46326},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":102351100},\"end\":47260,\"start\":46748},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":12590968},\"end\":47532,\"start\":47262},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":233231226},\"end\":47926,\"start\":47534},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":221105077},\"end\":48202,\"start\":47928},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":2749951},\"end\":48531,\"start\":48204},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":15244087},\"end\":48884,\"start\":48533},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":212412719},\"end\":49393,\"start\":48886},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":12569740},\"end\":49726,\"start\":49395},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":52261401},\"end\":50161,\"start\":49728},{\"attributes\":{\"doi\":\"2020. 2\",\"id\":\"b41\",\"matched_paper_id\":218501576},\"end\":50537,\"start\":50163},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":152282359},\"end\":51093,\"start\":50539},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":233169009},\"end\":51541,\"start\":51095},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":81979618},\"end\":51880,\"start\":51543},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":221038849},\"end\":52251,\"start\":51882},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":15325137},\"end\":52620,\"start\":52253},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":2952130},\"end\":53043,\"start\":52622},{\"attributes\":{\"id\":\"b48\"},\"end\":53463,\"start\":53045},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":220713845},\"end\":53962,\"start\":53465},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":237213375},\"end\":54474,\"start\":53964},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":41947},\"end\":54889,\"start\":54476},{\"attributes\":{\"doi\":\"2020. 1\",\"id\":\"b52\",\"matched_paper_id\":221586450},\"end\":55266,\"start\":54891},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":235593022},\"end\":55702,\"start\":55268},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":49551925},\"end\":56062,\"start\":55704},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":203559691},\"end\":56451,\"start\":56064},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":53725610},\"end\":56818,\"start\":56453},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":28259635},\"end\":57175,\"start\":56820},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":80628373},\"end\":57749,\"start\":57177},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":9314967},\"end\":58220,\"start\":57751},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":236976082},\"end\":58770,\"start\":58222},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":214714400},\"end\":59324,\"start\":58772},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":195944196},\"end\":59830,\"start\":59326},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":58028780},\"end\":60351,\"start\":59832}]", "bib_title": "[{\"end\":33560,\"start\":33493},{\"end\":34039,\"start\":33996},{\"end\":34513,\"start\":34440},{\"end\":34935,\"start\":34904},{\"end\":35315,\"start\":35289},{\"end\":35710,\"start\":35622},{\"end\":36107,\"start\":36024},{\"end\":36588,\"start\":36530},{\"end\":37100,\"start\":37028},{\"end\":37374,\"start\":37338},{\"end\":37721,\"start\":37661},{\"end\":38192,\"start\":38148},{\"end\":38730,\"start\":38667},{\"end\":39115,\"start\":39071},{\"end\":39518,\"start\":39503},{\"end\":39940,\"start\":39914},{\"end\":40293,\"start\":40226},{\"end\":40838,\"start\":40776},{\"end\":41175,\"start\":41126},{\"end\":41642,\"start\":41594},{\"end\":41931,\"start\":41889},{\"end\":42194,\"start\":42143},{\"end\":42408,\"start\":42354},{\"end\":42823,\"start\":42726},{\"end\":43341,\"start\":43274},{\"end\":43821,\"start\":43751},{\"end\":44355,\"start\":44267},{\"end\":44871,\"start\":44819},{\"end\":45235,\"start\":45164},{\"end\":45510,\"start\":45470},{\"end\":45885,\"start\":45835},{\"end\":46377,\"start\":46326},{\"end\":46798,\"start\":46748},{\"end\":47293,\"start\":47262},{\"end\":47580,\"start\":47534},{\"end\":47970,\"start\":47928},{\"end\":48255,\"start\":48204},{\"end\":48588,\"start\":48533},{\"end\":48984,\"start\":48886},{\"end\":49449,\"start\":49395},{\"end\":49765,\"start\":49728},{\"end\":50235,\"start\":50163},{\"end\":50623,\"start\":50539},{\"end\":51167,\"start\":51095},{\"end\":51598,\"start\":51543},{\"end\":51938,\"start\":51882},{\"end\":52361,\"start\":52253},{\"end\":52697,\"start\":52622},{\"end\":53127,\"start\":53045},{\"end\":53555,\"start\":53465},{\"end\":54046,\"start\":53964},{\"end\":54542,\"start\":54476},{\"end\":54962,\"start\":54891},{\"end\":55342,\"start\":55268},{\"end\":55763,\"start\":55704},{\"end\":56146,\"start\":56064},{\"end\":56499,\"start\":56453},{\"end\":56899,\"start\":56820},{\"end\":57245,\"start\":57177},{\"end\":57824,\"start\":57751},{\"end\":58272,\"start\":58222},{\"end\":58860,\"start\":58772},{\"end\":59405,\"start\":59326},{\"end\":59957,\"start\":59832}]", "bib_author": "[{\"end\":33579,\"start\":33562},{\"end\":33594,\"start\":33579},{\"end\":33616,\"start\":33594},{\"end\":33636,\"start\":33616},{\"end\":33654,\"start\":33636},{\"end\":34058,\"start\":34041},{\"end\":34073,\"start\":34058},{\"end\":34085,\"start\":34073},{\"end\":34105,\"start\":34085},{\"end\":34123,\"start\":34105},{\"end\":34532,\"start\":34515},{\"end\":34543,\"start\":34532},{\"end\":34566,\"start\":34543},{\"end\":34951,\"start\":34937},{\"end\":34966,\"start\":34951},{\"end\":35332,\"start\":35317},{\"end\":35347,\"start\":35332},{\"end\":35364,\"start\":35347},{\"end\":35727,\"start\":35712},{\"end\":35742,\"start\":35727},{\"end\":35759,\"start\":35742},{\"end\":36124,\"start\":36109},{\"end\":36139,\"start\":36124},{\"end\":36154,\"start\":36139},{\"end\":36171,\"start\":36154},{\"end\":36620,\"start\":36590},{\"end\":36638,\"start\":36620},{\"end\":36655,\"start\":36638},{\"end\":36666,\"start\":36655},{\"end\":37118,\"start\":37102},{\"end\":37133,\"start\":37118},{\"end\":37148,\"start\":37133},{\"end\":37391,\"start\":37376},{\"end\":37411,\"start\":37391},{\"end\":37425,\"start\":37411},{\"end\":37442,\"start\":37425},{\"end\":37737,\"start\":37723},{\"end\":37754,\"start\":37737},{\"end\":37770,\"start\":37754},{\"end\":37788,\"start\":37770},{\"end\":37812,\"start\":37788},{\"end\":38207,\"start\":38194},{\"end\":38215,\"start\":38207},{\"end\":38230,\"start\":38215},{\"end\":38249,\"start\":38230},{\"end\":38269,\"start\":38249},{\"end\":38286,\"start\":38269},{\"end\":38302,\"start\":38286},{\"end\":38316,\"start\":38302},{\"end\":38745,\"start\":38732},{\"end\":38756,\"start\":38745},{\"end\":38769,\"start\":38756},{\"end\":39129,\"start\":39117},{\"end\":39141,\"start\":39129},{\"end\":39159,\"start\":39141},{\"end\":39180,\"start\":39159},{\"end\":39196,\"start\":39180},{\"end\":39203,\"start\":39196},{\"end\":39536,\"start\":39520},{\"end\":39551,\"start\":39536},{\"end\":39568,\"start\":39551},{\"end\":39584,\"start\":39568},{\"end\":39961,\"start\":39942},{\"end\":39968,\"start\":39961},{\"end\":39977,\"start\":39968},{\"end\":39998,\"start\":39977},{\"end\":40015,\"start\":39998},{\"end\":40022,\"start\":40015},{\"end\":40311,\"start\":40295},{\"end\":40330,\"start\":40311},{\"end\":40351,\"start\":40330},{\"end\":40362,\"start\":40351},{\"end\":40380,\"start\":40362},{\"end\":40392,\"start\":40380},{\"end\":40853,\"start\":40840},{\"end\":40868,\"start\":40853},{\"end\":40880,\"start\":40868},{\"end\":40888,\"start\":40880},{\"end\":40901,\"start\":40888},{\"end\":40924,\"start\":40901},{\"end\":40938,\"start\":40924},{\"end\":41189,\"start\":41177},{\"end\":41200,\"start\":41189},{\"end\":41219,\"start\":41200},{\"end\":41227,\"start\":41219},{\"end\":41238,\"start\":41227},{\"end\":41660,\"start\":41644},{\"end\":41674,\"start\":41660},{\"end\":41684,\"start\":41674},{\"end\":41692,\"start\":41684},{\"end\":41952,\"start\":41933},{\"end\":41966,\"start\":41952},{\"end\":41983,\"start\":41966},{\"end\":42210,\"start\":42196},{\"end\":42424,\"start\":42410},{\"end\":42440,\"start\":42424},{\"end\":42451,\"start\":42440},{\"end\":42833,\"start\":42825},{\"end\":42845,\"start\":42833},{\"end\":42862,\"start\":42845},{\"end\":42868,\"start\":42862},{\"end\":43351,\"start\":43343},{\"end\":43366,\"start\":43351},{\"end\":43380,\"start\":43366},{\"end\":43409,\"start\":43380},{\"end\":43425,\"start\":43409},{\"end\":43441,\"start\":43425},{\"end\":43461,\"start\":43441},{\"end\":43835,\"start\":43823},{\"end\":43846,\"start\":43835},{\"end\":43858,\"start\":43846},{\"end\":43882,\"start\":43858},{\"end\":43902,\"start\":43882},{\"end\":43912,\"start\":43902},{\"end\":43919,\"start\":43912},{\"end\":44365,\"start\":44357},{\"end\":44381,\"start\":44365},{\"end\":44404,\"start\":44381},{\"end\":44419,\"start\":44404},{\"end\":44433,\"start\":44419},{\"end\":44453,\"start\":44433},{\"end\":44888,\"start\":44873},{\"end\":44898,\"start\":44888},{\"end\":44914,\"start\":44898},{\"end\":45251,\"start\":45237},{\"end\":45267,\"start\":45251},{\"end\":45283,\"start\":45267},{\"end\":45527,\"start\":45512},{\"end\":45544,\"start\":45527},{\"end\":45559,\"start\":45544},{\"end\":45577,\"start\":45559},{\"end\":45594,\"start\":45577},{\"end\":45898,\"start\":45887},{\"end\":45912,\"start\":45898},{\"end\":45927,\"start\":45912},{\"end\":45942,\"start\":45927},{\"end\":45960,\"start\":45942},{\"end\":45971,\"start\":45960},{\"end\":45988,\"start\":45971},{\"end\":46390,\"start\":46379},{\"end\":46404,\"start\":46390},{\"end\":46415,\"start\":46404},{\"end\":46432,\"start\":46415},{\"end\":46817,\"start\":46800},{\"end\":46832,\"start\":46817},{\"end\":46844,\"start\":46832},{\"end\":46858,\"start\":46844},{\"end\":46879,\"start\":46858},{\"end\":46886,\"start\":46879},{\"end\":47313,\"start\":47295},{\"end\":47336,\"start\":47313},{\"end\":47352,\"start\":47336},{\"end\":47366,\"start\":47352},{\"end\":47600,\"start\":47582},{\"end\":47611,\"start\":47600},{\"end\":47622,\"start\":47611},{\"end\":47634,\"start\":47622},{\"end\":47640,\"start\":47634},{\"end\":47985,\"start\":47972},{\"end\":48008,\"start\":47985},{\"end\":48022,\"start\":48008},{\"end\":48036,\"start\":48022},{\"end\":48271,\"start\":48257},{\"end\":48284,\"start\":48271},{\"end\":48307,\"start\":48284},{\"end\":48605,\"start\":48590},{\"end\":48622,\"start\":48605},{\"end\":48638,\"start\":48622},{\"end\":48653,\"start\":48638},{\"end\":48667,\"start\":48653},{\"end\":49003,\"start\":48986},{\"end\":49023,\"start\":49003},{\"end\":49041,\"start\":49023},{\"end\":49469,\"start\":49451},{\"end\":49484,\"start\":49469},{\"end\":49494,\"start\":49484},{\"end\":49511,\"start\":49494},{\"end\":49777,\"start\":49767},{\"end\":49795,\"start\":49777},{\"end\":49809,\"start\":49795},{\"end\":49821,\"start\":49809},{\"end\":49835,\"start\":49821},{\"end\":49847,\"start\":49835},{\"end\":50254,\"start\":50237},{\"end\":50271,\"start\":50254},{\"end\":50282,\"start\":50271},{\"end\":50295,\"start\":50282},{\"end\":50641,\"start\":50625},{\"end\":50653,\"start\":50641},{\"end\":50668,\"start\":50653},{\"end\":50686,\"start\":50668},{\"end\":50703,\"start\":50686},{\"end\":50711,\"start\":50703},{\"end\":51185,\"start\":51169},{\"end\":51199,\"start\":51185},{\"end\":51210,\"start\":51199},{\"end\":51227,\"start\":51210},{\"end\":51618,\"start\":51600},{\"end\":51635,\"start\":51618},{\"end\":51646,\"start\":51635},{\"end\":51949,\"start\":51940},{\"end\":51964,\"start\":51949},{\"end\":51976,\"start\":51964},{\"end\":52382,\"start\":52363},{\"end\":52397,\"start\":52382},{\"end\":52716,\"start\":52699},{\"end\":52731,\"start\":52716},{\"end\":53139,\"start\":53129},{\"end\":53154,\"start\":53139},{\"end\":53169,\"start\":53154},{\"end\":53183,\"start\":53169},{\"end\":53199,\"start\":53183},{\"end\":53573,\"start\":53557},{\"end\":53595,\"start\":53573},{\"end\":53606,\"start\":53595},{\"end\":53624,\"start\":53606},{\"end\":54064,\"start\":54048},{\"end\":54085,\"start\":54064},{\"end\":54096,\"start\":54085},{\"end\":54114,\"start\":54096},{\"end\":54562,\"start\":54544},{\"end\":54584,\"start\":54562},{\"end\":54602,\"start\":54584},{\"end\":54614,\"start\":54602},{\"end\":54981,\"start\":54964},{\"end\":54999,\"start\":54981},{\"end\":55013,\"start\":54999},{\"end\":55024,\"start\":55013},{\"end\":55358,\"start\":55344},{\"end\":55376,\"start\":55358},{\"end\":55387,\"start\":55376},{\"end\":55403,\"start\":55387},{\"end\":55414,\"start\":55403},{\"end\":55777,\"start\":55765},{\"end\":55789,\"start\":55777},{\"end\":55803,\"start\":55789},{\"end\":55820,\"start\":55803},{\"end\":55827,\"start\":55820},{\"end\":56160,\"start\":56148},{\"end\":56174,\"start\":56160},{\"end\":56184,\"start\":56174},{\"end\":56196,\"start\":56184},{\"end\":56203,\"start\":56196},{\"end\":56514,\"start\":56501},{\"end\":56528,\"start\":56514},{\"end\":56540,\"start\":56528},{\"end\":56553,\"start\":56540},{\"end\":56910,\"start\":56901},{\"end\":56922,\"start\":56910},{\"end\":56934,\"start\":56922},{\"end\":56948,\"start\":56934},{\"end\":57255,\"start\":57247},{\"end\":57269,\"start\":57255},{\"end\":57281,\"start\":57269},{\"end\":57295,\"start\":57281},{\"end\":57309,\"start\":57295},{\"end\":57327,\"start\":57309},{\"end\":57338,\"start\":57327},{\"end\":57838,\"start\":57826},{\"end\":57853,\"start\":57838},{\"end\":57864,\"start\":57853},{\"end\":57878,\"start\":57864},{\"end\":57889,\"start\":57878},{\"end\":58286,\"start\":58274},{\"end\":58298,\"start\":58286},{\"end\":58321,\"start\":58298},{\"end\":58333,\"start\":58321},{\"end\":58347,\"start\":58333},{\"end\":58364,\"start\":58347},{\"end\":58375,\"start\":58364},{\"end\":58390,\"start\":58375},{\"end\":58874,\"start\":58862},{\"end\":58882,\"start\":58874},{\"end\":58892,\"start\":58882},{\"end\":58905,\"start\":58892},{\"end\":58914,\"start\":58905},{\"end\":58928,\"start\":58914},{\"end\":58942,\"start\":58928},{\"end\":58957,\"start\":58942},{\"end\":59420,\"start\":59407},{\"end\":59434,\"start\":59420},{\"end\":59449,\"start\":59434},{\"end\":59465,\"start\":59449},{\"end\":59971,\"start\":59959},{\"end\":59989,\"start\":59971}]", "bib_venue": "[{\"end\":33709,\"start\":33654},{\"end\":34175,\"start\":34123},{\"end\":34630,\"start\":34566},{\"end\":35044,\"start\":34966},{\"end\":35418,\"start\":35364},{\"end\":35779,\"start\":35759},{\"end\":36235,\"start\":36171},{\"end\":36730,\"start\":36666},{\"end\":37164,\"start\":37148},{\"end\":37462,\"start\":37442},{\"end\":37867,\"start\":37812},{\"end\":38370,\"start\":38316},{\"end\":38826,\"start\":38769},{\"end\":39266,\"start\":39203},{\"end\":39657,\"start\":39584},{\"end\":40037,\"start\":40022},{\"end\":40456,\"start\":40392},{\"end\":40942,\"start\":40938},{\"end\":41309,\"start\":41238},{\"end\":41730,\"start\":41692},{\"end\":41999,\"start\":41983},{\"end\":42233,\"start\":42210},{\"end\":42505,\"start\":42451},{\"end\":42946,\"start\":42868},{\"end\":43476,\"start\":43461},{\"end\":43973,\"start\":43919},{\"end\":44505,\"start\":44453},{\"end\":44973,\"start\":44914},{\"end\":45306,\"start\":45290},{\"end\":45614,\"start\":45594},{\"end\":46043,\"start\":45988},{\"end\":46496,\"start\":46432},{\"end\":46950,\"start\":46886},{\"end\":47382,\"start\":47366},{\"end\":47695,\"start\":47640},{\"end\":48052,\"start\":48036},{\"end\":48327,\"start\":48307},{\"end\":48690,\"start\":48667},{\"end\":49096,\"start\":49041},{\"end\":49526,\"start\":49511},{\"end\":49901,\"start\":49847},{\"end\":50321,\"start\":50302},{\"end\":50775,\"start\":50711},{\"end\":51282,\"start\":51227},{\"end\":51690,\"start\":51646},{\"end\":52030,\"start\":51976},{\"end\":52418,\"start\":52397},{\"end\":52792,\"start\":52731},{\"end\":53219,\"start\":53199},{\"end\":53678,\"start\":53624},{\"end\":54178,\"start\":54114},{\"end\":54665,\"start\":54614},{\"end\":55066,\"start\":55031},{\"end\":55473,\"start\":55414},{\"end\":55847,\"start\":55827},{\"end\":56223,\"start\":56203},{\"end\":56604,\"start\":56553},{\"end\":56963,\"start\":56948},{\"end\":57416,\"start\":57338},{\"end\":57944,\"start\":57889},{\"end\":58454,\"start\":58390},{\"end\":59011,\"start\":58957},{\"end\":59529,\"start\":59465},{\"end\":60021,\"start\":59989},{\"end\":33760,\"start\":33711},{\"end\":34223,\"start\":34177},{\"end\":34690,\"start\":34632},{\"end\":35118,\"start\":35046},{\"end\":35468,\"start\":35420},{\"end\":35795,\"start\":35781},{\"end\":36295,\"start\":36237},{\"end\":36790,\"start\":36732},{\"end\":37478,\"start\":37464},{\"end\":37918,\"start\":37869},{\"end\":38420,\"start\":38372},{\"end\":38879,\"start\":38828},{\"end\":39726,\"start\":39659},{\"end\":40048,\"start\":40039},{\"end\":40516,\"start\":40458},{\"end\":41376,\"start\":41311},{\"end\":42555,\"start\":42507},{\"end\":43020,\"start\":42948},{\"end\":43487,\"start\":43478},{\"end\":44023,\"start\":43975},{\"end\":44553,\"start\":44507},{\"end\":45630,\"start\":45616},{\"end\":46094,\"start\":46045},{\"end\":46556,\"start\":46498},{\"end\":47010,\"start\":46952},{\"end\":47746,\"start\":47697},{\"end\":48343,\"start\":48329},{\"end\":49147,\"start\":49098},{\"end\":49537,\"start\":49528},{\"end\":49951,\"start\":49903},{\"end\":50336,\"start\":50323},{\"end\":50835,\"start\":50777},{\"end\":51333,\"start\":51284},{\"end\":52080,\"start\":52032},{\"end\":52840,\"start\":52794},{\"end\":53235,\"start\":53221},{\"end\":53728,\"start\":53680},{\"end\":54238,\"start\":54180},{\"end\":55863,\"start\":55849},{\"end\":56239,\"start\":56225},{\"end\":56651,\"start\":56606},{\"end\":56974,\"start\":56965},{\"end\":57490,\"start\":57418},{\"end\":57995,\"start\":57946},{\"end\":58514,\"start\":58456},{\"end\":59061,\"start\":59013},{\"end\":59589,\"start\":59531}]"}}}, "year": 2023, "month": 12, "day": 17}