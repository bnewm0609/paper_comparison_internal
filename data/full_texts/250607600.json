{"id": 250607600, "updated": "2023-10-05 12:37:40.811", "metadata": {"title": "QSAN: A Near-term Achievable Quantum Self-Attention Network", "authors": "[{\"first\":\"Jinjing\",\"last\":\"Shi\",\"middle\":[]},{\"first\":\"Ren-Xin\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Wenxuan\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Shichao\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Xuelong\",\"last\":\"Li\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Self-Attention Mechanism (SAM) is good at capturing the internal connections of features and greatly improves the performance of machine learning models, espeacially requiring efficient characterization and feature extraction of high-dimensional data. A novel Quantum Self-Attention Network (QSAN) is proposed for image classification tasks on near-term quantum devices. First, a Quantum Self-Attention Mechanism (QSAM) including Quantum Logic Similarity (QLS) and Quantum Bit Self-Attention Score Matrix (QBSASM) is explored as the theoretical basis of QSAN to enhance the data representation of SAM. QLS is employed to prevent measurements from obtaining inner products to allow QSAN to be fully implemented on quantum computers, and QBSASM as a result of the evolution of QSAN to produce a density matrix that effectively reflects the attention distribution of the output. Then, the framework for one-step realization and quantum circuits of QSAN are designed for fully considering the compression of the measurement times to acquire QBSASM in the intermediate process, in which a quantum coordinate prototype is introduced as well in the quantum circuit for describing the mathematical relation between the output and control bits to facilitate programming. Ultimately, the method comparision and binary classification experiments on MNIST with the pennylane platform demonstrate that QSAN converges about 1.7x and 2.3x faster than hardware-efficient ansatz and QAOA ansatz respevtively with similar parameter configurations and 100% prediction accuracy, which indicates it has a better learning capability. QSAN is quite suitable for fast and in-depth analysis of the primary and secondary relationships of image and other data, which has great potential for applications of quantum computer vision from the perspective of enhancing the information extraction ability of models.", "fields_of_study": "[\"Physics\",\"Computer Science\"]", "external_ids": {"arxiv": "2207.07563", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2207-07563", "doi": "10.48550/arxiv.2207.07563"}}, "content": {"source": {"pdf_hash": "9ee4836c974f02f3cfe4cdb4fefa748fd6bb021c", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2207.07563v4.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "3a64923432c0b771efd8894828192a24b4679d23", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9ee4836c974f02f3cfe4cdb4fefa748fd6bb021c.txt", "contents": "\nQSAN: A Near-term Achievable Quantum Self-Attention Network\n5 Aug 2023\n\nMember, IEEEJinjing Shi \nMember, IEEERen-Xin Zhao \nStudent Member, IEEEWenxuan Wang \nShichao Zhang \nSenior Member, IEEEXuelong Li \nQSAN: A Near-term Achievable Quantum Self-Attention Network\n5 Aug 20231Index Terms-Machine learningQuantum machine learningImage classificationSelf-attention mechanismQuantum self-attention mechanismQuantum neural networkQuantum circuit\nSelf-Attention Mechanism (SAM) is good at capturing the internal connections of features and greatly improves the performance of machine learning models, espeacially requiring efficient characterization and feature extraction of high-dimensional data. A novel Quantum Self-Attention Network (QSAN) is proposed for image classification tasks on near-term quantum devices. First, a Quantum Self-Attention Mechanism (QSAM) including Quantum Logic Similarity (QLS) and Quantum Bit Self-Attention Score Matrix (QBSASM) is explored as the theoretical basis of QSAN to enhance the data representation of SAM. QLS is employed to prevent measurements from obtaining inner products to allow QSAN to be fully implemented on quantum computers, and QBSASM as a result of the evolution of QSAN to produce a density matrix that effectively reflects the attention distribution of the output. Then, the framework for one-step realization and quantum circuits of QSAN are designed for fully considering the compression of the measurement times to acquire QBSASM in the intermediate process, in which a quantum coordinate prototype is introduced as well in the quantum circuit for describing the mathematical relation between the output and control bits to facilitate programming. Ultimately, the method comparision and binary classification experiments on MNIST with the pennylane platform demonstrate that QSAN converges about 1.7x and 2.3x faster than hardware-efficient ansatz and QAOA ansatz respevtively with similar parameter configurations and 100% prediction accuracy, which indicates it has a better learning capability. QSAN is quite suitable for fast and in-depth analysis of the primary and secondary relationships of image and other data, which has great potential for applications of quantum computer vision from the perspective of enhancing the information extraction ability of models.\n\nINTRODUCTION\n\nI N recent years, tremendous progress has been achieved in the field of machine learning [1], where SAM is an important machine learning operator that produces attention scores from individual sequence itself to calculate sequence. It was originally introduced by a deep learning framework for machine translation called Transformer to overcome the problem of long-range dependencies in previous neural networks such as RNNs [2]. Corresponding experimental results demonstrate that SAM can reduce the dependence on external information and better capture the intrinsic relevance of features [2]. This importance has been repeatedly evidenced in many fields such as computer vision [3][4][5][6], natural language processing [7][8][9], speech [10] and emotion analysis [11]. For example, in 2021, 84.7% first accuracy on the ImageNet benchmark was realized by a BoTNet with SAM [12]. In the same year, this accuracy was boosted to 87.3% by a Swin Transformer with shifted window SAM  [13]. It is a remarkable progress compared to the models before the advent of SAM, while it seems to be not efficient enough for high-dimensional data representation and feature extraction. Thus it leads us to think whether there is a new platform that can enhance capabilities of SAM in data characterization and how effective it is. Fortunately, a feasible solution to this problem can be supplied by quantum computer. Quantum computer is considered as a new paradigm that can achieve quadratic speedup of algorithms, which has made significant breakthroughs in recent years [14][15][16][17][18]. The superiority offered by quantum computers, also known as quantum supremacy [19,20], specifically refers to the exponential storage and secondary computational acceleration arising from the effects of quantum properties, as reflected in Quantum Machine Learning (QML) as well as quantum simulations [21,22]. In particular, the talent of quantum computers in data representation [23,24] led us to ponder whether it can efficiently express SAM for sequences and how to embody quantum advantages in SAM, which has already inspired some exploratory works. For instance, in 2017, Niu et al. exploited the idea of weak measurement in quantum mechanics to construct a parameter-free, more efficient quantum attention [25], which is used in the LSTM framework and found to have better sentence modeling performance. In 2021, Zhao et al. considered the quantum attention mechanism as a density matrix to construct more powerful sentence representations [26]. Unfortunately, the above two approaches only involve certain physical concepts in quantum mechanics without providing specific quantum circuits. A recent meaningful effort was contributed by the Baidu group, where a Gaussian projection-based neural network using Variational Quantum Algorithm (VQA) [29] to build ansatz [1,[30][31][32] on Noisy Intermediate-Scale Quantum (NISQ) [56] devices was applied to text classification [27]. In the current mainstream quantum-based SAM [25][26][27], only some parts of the SAM task are undertaken by the quantum computer, while it is more ideal for quantum computers to accomplish all the tasks of SAM, including calculating the attention scores and deriving outcomes in one step. Furthermore, the compression of the number of measurements is not sufficiently considered, where the more measurements are made, the more quantum data should be converted into classical data, resulting in more classical storage consumption.\n\nA novel complete QSAN is proposed in order to enhance data characterization by exploring quantum advantages in SAM and more efficiently acquire attention scores and outcomes in one step, where QSAM including QLS and QBSASM is explored as the theoretical basis of QSAN. Compared to SAM, QSAM demands exponentially less storage with the assistance of quantum representation for the same input sequence. In contrast to Refs. [25][26][27], QSAN can be potentially fully deployed and realized on quantum devices with fewer measurements. Moreover, the most encouraging thing is that we have exploringly discovered that young quantum computers may have quantum characteristic attention and can depict the distribution of outputs in a quantum language, but not to replace SAM or to beat all the schemes in the Refs. [25][26][27]. Quantum characteristic here could be understood as probability, linearity and reversibility, while the quantum language refers to specialized terms in quantum mechanics, such as density matrix, Hamiltonian operators, etc. The main contributions of this paper are summarized as follows.\n\n\u2022 QLS and QBSASM are defined to constitute the QSAM as the theoretical basis of QSAN to enhance the data representation, where QLS obtain the similarity directly by replacing the implicit inner product similarity with logical operations and QBSASM as a by-product generated from the evolution of QSAN can reflect the output attention distribution effectively in the form of a density matrix. \u2022 The overall framework of QSAN and its quantum circuits are designed based on QSAM, in which quantum coordinates are proposed as an action criteria to simplify the design of QSAN to realize deriving the QBSASM and solution of task simultaneously in one step by compressing the number of measurements. \u2022 Classification experiments for the MNIST are impletemented on the pennylane platform, which prove QSAN converges approximately 1.7x and 2.3x faster than hardware-efficient ansatz and QAOA ansatz respectively with 100% prediction accuracy based on the same experimental setup and similar number of parameters, implying that QSAN has better learning capability.\n\nThe rest of this paper is organized as follows. Basic theory and fundamental principles are summarized in section 2. QSAM with QLS, QBSASM are proposed in section 3. QSAN and its corresponding quantum circuits are designed in Section 4. The experiments and discussions are conducted in Section 5. Finally, the conclusion is drawn in Section 6.\n\n\nPRELIMINARIES\n\nQML, SAM, VQA and quantum operators are briefly outlined in this section.\n\n\nQuantum Machine Learning\n\nQML is a new computational paradigm rooted in quantum mechanics. Various QML models with variational quantum algorithmic structures [33], such as quantum convolutional neural networks [34], quantum recurrent neural networks [35] and quantum generative adversarial networks [36], have been introduced and applied in areas such as chemical analysis [50] and physical dynamics simulation [37]. QML has the ability to generate atypical data patterns via quantum mechanics to demonstrate potential quantum advantages [38], as verified by quantum kernel methods [39,40]. In addition, some QML models exhibit quadratic acceleration with rigorous mathematical proofs [41,42]. However, the development of QML is in the early stages, the situation of quantum resource constraints [43] and quantum noise affecting accurate predictions [44] make the performance of quantum advantages still elusive. Therefore, it is recommended to focus on enriching the use cases of QML to lay the foundation for the industrialization of future quantum computers, rather than pursuing quantum advantages alone [45].\n\n\nSelf-Attention Mechanism\n\nThe input In = {w i } n\u22121 i=0 , w i \u2208 R 1\u00d7l and the output Out = {new w j } n\u22121 j=0 , new w j \u2208 R 1\u00d7l are defined, where n is the total number of outputs, l is the dimension of the element. w i (new w j ) indicates the i-th (j-th) element of the sequence In (Out). For conciseness, it is specified that all subscripts in the text are used to indicate the location of the variable in the sequence, except for special instructions. Then SAM [2] can be stated as\nnew w i = j w i,j V j .(1)\nIn Eq. (1), new w i represents new output after the weighting operation. The weights\nw i,j = softmax Q i K T j \u221a d ,(2)\nalso called attention scores, are obtained by normalizing the inner product Q i K T j . \u221a d is a scaling factor. In Eq. (1) and Eq. (2),\nQ i = w i \u00b7 U Q ,(3)V j = w j \u00b7 U V ,(4)\nand K T j are the query vector, the value vector and the transpose of the key vector\nK j = w j \u00b7 U K ,(5)\nwhere w i and w j are inputs. In Eq. (3) to Eq. (5), U Q , U K and U V are three trainable parameter matrices named as query conversion matrix, key conversion matrix and value conversion matrix respectively.\n\n\nVariational Quantum Algorithm\n\nIn the NISQ era, it is very difficult to fully deploy deep networks for deep learning on quantum computers with limited qubits. Firstly, the dimensionality of the model grows exponentially as the size of the quantum circuit gets larger [43]. Secondly, noise imposes many unknowns on the training results [44,46]. Therefore, quantum-classical hybrid model is deemed as an efficient path. VQA is one such class of algorithms. The framework of VQA is exhibited in Fig. 1, which can be divided into two parts. The blue box designates the range of the classical computer. This stage focuses on the calculation of the loss function and the optimization of the parameters, as shown in the two purple curves in Fig. 1. The general formulation of the loss function is\nC(\u03b8)= k F k (Tr[O k U(\u03b8) \u03c1 k U \u2020 (\u03b8)]),(6)\nwhere F k is a set of certain functions determined by specific tasks. Tr [\u00b7] indicates the trace of O k U(\u03b8) \u03c1 k U \u2020 (\u03b8). O k is a set of observables. U(\u03b8) = \u2297 i U i (\u03b8 i ) denotes the product of a series of unitary operators, where \u03b8 comprises a series of continuous or discrete hyperparameters. U \u2020 (\u03b8) is the conjugate transpose matrix of U(\u03b8). {\u03c1 k } is the input state of the training set. Currently, methods for optimizing Eq. (6) include iCANS [47], stochastic gradient descent [48], quantum natural gradient [49], etc. The green box stands for the quantum computer domain. In this box, a ansatz model is drawn. The black dashed box is the centerpiece of this model, the ansatz, which is a circuit with a specific structure and function. Common examples of ansatz contain hardware-efficient ansatz [50,51], QAOA ansatz [52][53][54], etc.\n\nThe arrows in Fig. 1 illustrate the interaction of information of quantum computers and classical counterparts. The quantum computer provides the classical computer with quantum circuit measurements and loss function forms to be used for prediction. After the classical computer is trained, a new round of hyperparameters is uploaded and updated into the quantum circuit.\n\n\nQubit and Operators\n\nIn a quantum computer, the smallest element of information is a qubit |\u03c8 which is usually expressed as a linear superposition of two eigenstates |0 and |1 [55], namely\n|\u03c8 = \u03b1|0 + \u03b2|1 ,(7)\nwhere \u03b1 and \u03b2 as probability amplitudes, satisfy\n|\u03b1| 2 + |\u03b2| 2 = 1.(8)\nThese qubits evolve through unitary operators U which are also called quantum gates and refer to matrices that satisfy\nU \u22121 = U \u2020 , U U \u2020 = I,(9)\nwhere I is the identity matrix, U \u2020 is the complex conjugate of U . In this paper, rotating Pauli Y gate, Hadamard gate, SWAP gate, CNOT gate, Toffoli gate and multi-controlled Toffoli gate are mainly used, as shown in Tab. A1.\n\n\nQUANTUM SELF-ATTENTION MECHANISM\n\nThis section presents a QSAM framework, in which QLS is proposed to measure logical similarity and enables QSAM to be freed from numerical operations such as addition, thus conserving more qubits. More importantly, QLS replaces the inner product similarity that needs to be implemented by measurement, which ensures that the task is always executed on the quantum computer without interruption. QBSASM derived from QLS expresses the weight distribution of the quantum computer on outputs in the form of a density matrix.\n\nFirstly In and Out are re-expressed in quantum states\nas Q in = {|w i } n\u22121 i=0 , |w i \u2208 R 2 m \u00d71 and Q out = {|new w j } n\u22121 j=0 , |new w j \u2208 R 2 m+1 \u00d71\nrespectively, where m = \u2308log 2 l\u2309 denotes both the number of qubits and the length of the quantum string. l is the feature dimension of the classical output. Then QSAM is described as\n|new w i := \u2299 j Q i |K j \u2297 |V j .(10)\nwhere Q i |K j is QLS in subsection 3.1. The symbol \u2297 signifies a tensor operation. The symbol \u2299 encompasses two operations. One is to apply a multi-controlled Toffoli gate to several specific QLS elements. The method of selecting these specific QLS is called slicing operation in subsection 3.2. The other is to use CNOT gates to |V j to perform dimensional compression. In Eq. (10),\n|Q i = U q |W i ,(11)|K j = U k |W j ,(12)\nand\n|V j = U v |W j ,(13)\nwhere |Q i , |K j and |V j are the elements of the query quantum state |Q , the key quantum state |K and the value quantum state |V , respectively. They can all be written in the form of quantum strings\n|Q i = m\u22121 \u2297 a=0 |Q i,a ,(14)|K j = m\u22121 \u2297 b=0 |K j,b ,(15)\nand\n|V j = m\u22121 \u2297 c=0 |V j,c ,(16)\nwhere |Q i,a , |K j,b and |V j,c indicate quantum superposition states at different positions. These representations are indicated in Fig. 6 and Fig. 8. In Eq. (11), Eq. (12) and Eq. (13), U q , U k and U v are specified as three composite unitary operators with the identical structure but distinct parameters. The same composition means that all three matrices above are composed of (m\u22121) Hadamard gates, m rotating Pauli Y gates, and m CNOT gates, and are arranged in order\nU M\u2208{q,k,v} = CN OT \u2297(m\u22121) R y (\u03b8 M ) \u2297m H \u2297m .(17)\nThe benefit of this design is to maintain that the probability amplitudes are all real numbers [57]. Furthermore, |w j (or |w i ) is the input quantum state that can be split into the form of the quantum string\n|W j = m\u22121 \u2297 i=0 |W j,i(18)\nand this denotation is presented in Fig. 4, Fig. 5 and Fig. 7. Formally, Eq. (10) is very similar to Eq. (1), but there are essential changes. Comparing Eq. (1) and Eq. (10), Eq. (1) is an attention mechanism with nonlinear operations, while Eq. (10) has a linearized, logical character, which makes it easier to be implemented on quantum computers across the board. Futhermore, in Eq. (1), a large number of numerical operations are required, such as solving the inner product as well as weighted summation, which requires a large number of qubits to implement a network of numerical operations on existing quantum computers. In contrast, Eq. (10) reduces the implementation cost with QLS and saves even more qubits. Finally, the relationship between QLS and |V j in Eq. (10) is a tensor product \u2297, not a product in Eq. (1), hence |w i and |new w j differ in dimension.\n\n\nQuantum Logical Similarity\n\nInspired by CRC checksum and Frobenius inner product, QLS in the Definition 1 is proposed in order to obtain the similarity directly as well as to make QSAN avoid measurements in the intermediate process to ensure that quantum data are maintained during processing, which is quite different from the common way with SWAP test [58] or Hadamard test [59] to characterize the similarity between two quantum states |Q i and |K j . Definition 1 (QLS): For any quantum state |Q i and |K j with i, j \u2208 {0, \u00b7 \u00b7 \u00b7 , n \u2212 1}, QLS is redefined as\nQ i |K j := \u2295 h (Q i,h \u2227 K j,h ),(19)\nwhere |Q i,h and |K j,h , h \u2208 {0, \u00b7 \u00b7 \u00b7 , m \u2212 1} stand for the h-th qubit of |Q i and |K j , respectively. The symbol \u2295 indicates modulo-two addition and the symbol \u2227 is logical AND operation. From the implementation point of view, AND operation and modulo-two addition can be realized with Toffoli gates and CNOT gates, respectively. Then Eq. (19) can be explained in terms of quantum gates:\nT of f oli|Q i,h , K j,h , 0 = |Q i,h , K j,h , Q i,h \u2227 K j,h , CN OT |Q i,h \u2227 K j,h , Q i,h \u2227 K j,h = |Q i,h \u2227 K j,h , (Q i,h \u2227 K j,h ) \u2295 (Q i,h \u2227 K j,h ) .(20)\nAfter quantum logic calculations, the two lengthy qubit strings |Q i and |K j are compressed into a superposition state with QLS in Eq. (19), which forms the basic unit in the next subsection QBSASM.\n\n\nQuantum Bit Self-Attention Score Matrix\n\nQBSASM, mathematically a matrix with QLS as elements, is a byproduct of the QSAN execution process to mirror the change in attention distribution before and after training.\n\nIt is known that the solution procedure for a single new output only is given by Eq. (10). By analogy, the solution procedure for all outputs is depicted by the matrix form as follows:\n\uf8ee \uf8ef \uf8ef \uf8ef \uf8f0 ( Q 0 |K 0 \u2297 |V 0 ) \u2299 \u00b7 \u00b7 \u00b7 \u2299 ( Q 0 |K n\u22121 \u2297 |V n\u22121 ) ( Q 1 |K 0 \u2297 |V 0 ) \u2299 \u00b7 \u00b7 \u00b7 \u2299 ( Q 1 |K n\u22121 \u2297 |V n\u22121 )\n. . . \n( Q n\u22121 |K 0 \u2297 |V 0 ) \u2299 \u00b7 \u00b7 \u00b7 \u2299 ( Q n\u22121 |K n\u22121 \u2297 |V n\u22121 ) \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fb = \uf8ee \uf8ef \uf8ef \uf8ef \uf8f0 | new w 0 | new w 1 . . . | new w n\u22121 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fb .(21)\uf8ee \uf8ef \uf8ef \uf8ef \uf8f0 Q 0 |K 0 Q 0 |K 1 \u00b7 \u00b7 \u00b7 Q 0 |K n\u22121 Q 1 |K 0 Q 1 |K 1 \u00b7 \u00b7 \u00b7 Q 1 |K n\u22121 . . . . . . Q n\u22121 |K 0 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 Q n\u22121 |K n\u22121 \uf8f9 \uf8fa \uf8fa \uf8fa \uf8fb ,(22)\nis extracted from Eq. (21) as QBSASM to depict the distribution of attention scores, where each element is computed by QLS. The slicing operation mentioned previously comes into play here. Specifically, the slicing operation takes the elements QLS in each row of Eq. (22) (e.g., element Q 0 |K 0 to element Q 0 |K n\u22121 ) as control bits of the multi-controlled quantum gate. The results of their operations are employed as new weights, thus reflecting the weighting operations of QSAN. In practice, QBSASM can be obtained by pennylane intercepting the density matrix of QLS. In summary, in Eq. (22), since each element QLS from Eq. (19) is a superposition state rather than a scalar, QB-SASM has higher dimensionality than the classical attention score matrix and can represent more state information. Particularly, as the dimensionality increases, QBSASM is more difficult to be performed classical simulation, which is manifesting the storage advantage of quantum computers.\n\n\nQUANTUM SELF-ATTENTION NETWORK\n\nIn this section, the overall framework of QSAN based on QSAM theory in the previous section and the corresponding quantum circuits are illustrated. Especially, a prototype of quantum coordinates is presented, which is a design guideline for quantum circuits with regular layout. The functional link between control bits and output bits can be established with the guidance of quantum coordinates to facilitate programming. It is also worth exploring in quantum circuit optimization. Step 1, 3 and 6 are dedicated to calculate the query quantum state |Q , the key quantum state |K and the value quantum state |V , respectively. Steps 2 and 4 are barbell operations designed to swap with the corresponding garbage registers.\n\nStep 5 is the QLS module to compute the QLS elements, which produces the by-product QBSASM.\n\nStep 7 is the entanglement compression operation, which reduces the measurements.\n\nStep 8 is the slicing operation for calculating the final weights. The final step is measurement.\n\n\nFramework of Quantum Self-Attention Network\n\nTo implement the QSAM theory in Eq. (21), the general framework of QSAN is designed in the style of Fig. 2.\n\nVertically, QSAN in Fig. 2 consists of one input register and three garbage registers, where the three garbage registers are used to compute the query quantum state |Q , the key quantum state |K and QLS, respectively. Moreover, the input register corresponds to Input reg in Fig. 2, Fig. 4, Fig. 5 and Fig. 7, and the three garbage registers contrast to garbage reg for |Q , garbage reg for |K and garbage reg for QLS in Fig. 2, Fig. 5, Fig. 6 and Fig. 8, respectively. In terms of resource consumption for QSAN in Fig. 2, the first, second and third register takes n \u00d7 m qubits each, while the fourth register needs m n i=1 i qubits, for a total of 3m \u00d7 n + m n i=1 i qubits. Horizontally, QSAN in Fig. 2 is divided into 9 steps in terms of workflow, including Step 1 to 8 and the final measurements. Functionally it is split into 5 special modules, mainly comprising barbell operation, QLS module and entanglement compression operation. In particular, the 5 special modules are marked with 5 different colors in Fig.  2, where the same color means the same operation. Specifically, in Fig. 2, Step 1, 3 and 6 are dedicated to calculate the query quantum state |Q , the key quantum state |K and the value quantum state |V , respectively. Steps 2 and 4 are barbell operations designed to swap with the corresponding garbage registers.\n\nStep 5 is the QLS module to compute the QLS elements, which produces the by-product QBSASM.\n\nStep 7 is the entanglement compression operation, which reduces the measurements.\n\nStep 8 is the slicing operation for calculating the final weights. The final step is measurement.\n\nAdditionally, for the successful execution of Eq. (21), for the first three registers in Fig. 2 (i.e., Input reg, garbage reg for |Q and garbage reg for |K ), the same quantum coding [24] is used to prepare the same initial quantum states\n|In = n \u2297 j=0 |w j .(23)\n\nQuantum Circuit\n\nIn this subsection, the quantum circuit of QSAN is designed in detail following the steps in Fig. 2.\n\n\nQuantum Coordinates\n\nIn the design of quantum circuit for QSAN, a quantum coordinate is proposed for guiding the specific steps to implement a quantum circuit. Definition 3 (Quantum Coordinates): For a regularly arranged quantum circuit, the intersection of the number of layers and the circuit line number is the quantum coordinate.\n\nQuantum coordinates are used to dig the mathematical general term between control or output bits to quickly model the network. A simple case is exhibited in Fig. 3 to delineate how a quantum circuit diagram with a regular layout can be converted into a mathematical representation.\n\nIntuitively, Fig. 3 reveals that there is a special mathematical pattern between the variable line and the variable layer, and between the control and output bits of the multicontrolled CNOT gate. Then the variables layer and line can be chosen to jointly describe this network, which transforms the graphical language into a mathematical expression.\nt = f (layer, line) MultiControlledCNOT[t, t + 2, t + 4](24)\nin Fig. 3 is the mathematical law of this particular network, where the symbol MultiControlledCNOT represents the logical relationship between the control and output bits, and the contents of the bracket [t, t + 2, t + 4] (i.e., quantum coordinates) indicates the specific locations of the control and output bits in this network. Moreover, the mathematical formula in Eq. (24) is obtained by mathematical induction. Based on the above definition, it is even possible to derive the coordinates of the entire network. Then the whole quantum network can be displayed in the form of coordinate points or can be generalized in a generalized term formula, which enhances the interpretability of the network. The induction by means of coordinate points or generalized terms may provide a feasible solution for quantum circuit optimization. Later on, the charm of quantum coordinates is exhibited. Here, a CNOT gate coordinate law applicable to this project is extracted, which performs a crucial role. In the same register, the quantum coordinate of the CNOT gate is\nCN OT [s(r), s(r) + 1],(25)\nwhere\ns(r) = m \u00d7 r \u2212 r mod (m \u2212 1) m \u2212 1 + r mod (m \u2212 1) (26)\nis a general term formula with respect to r. This expression is more concise. The logical function it implies is to XOR the s(r)-th and (s(r) + 1)-th in the same register. The value range of r depends on the situation.\n\n\nImplementation Steps\n\n9 calculation steps (Step 1-8 and measurements) and the 5 functional modules previously mentioned in the framework are explained here.\n\nStep 1: calculate the query quantum state |Q according to Eq. (11). The procedure is as follows. \n\nwhere U q \u2297n is shown in Fig. 4 in the order provided by Eq.\n\n(17).\n\nStep 2: perform a barbell operation. The barbell operation, which gets its name from the module's form factor, actually swaps the input value of the second garbage register with the current value of the input register. This operation causes the input register to be reset and the result |Q to be saved in the second garbage register. The exact procedure is explained by the following equation: where \u2297 r SW AP [r, r + m \u00d7 n] as shown in Fig. 5 indicates that SWAP gates should be used for each dimension of each  \nSW AP|W j,i , Q j,i = |Q j,i , W j,i ,(29)\nwhen W j,i is at line i and Q j,i is at line (i + m \u00d7 n).\n\nStep 3: calculate the key quantum state |K according to Eq. (12). The details are shown in Fig. 4. The mathematical equation is expressed as |U k \u2297n In, Q, In, 0 = |K, Q, In, 0 .\n\nStep 4: perform a barbell operation. This time the present data of the input register is exchanged with the content of the third register: Step 5: calculate the QLS according to Eq. (19). The details are drawn in Fig. 6. Using the coordinates, Q \u2227 K is defined as\nQ \u2227 K := \u2297 r,j T of f oli[p 1 (r), p 2 (r), p 3 (r, j)],(33)\nwhere with r \u2208 {0, \u00b7 \u00b7 \u00b7 , m\u00d7n\u22121} and j \u2208 {0, \u00b7 \u00b7 \u00b7 , n\u2212\u230ar/m\u230b\u22121}. m \u00d7 n, 2m \u00d7 n and 3m \u00d7 n are biases for locating which register the current control or output bit is in, e.g. m \u00d7 n means it is in the first garbage register and 2m\u00d7n represents it is in the second garbage register. Secondly, the CNOT gates are applied to the fourth garbage register to acquire the eventual result of QLS. According to the law summarized in Eq. (25), the process of applying a CNOT gate at this point is defined as\np 1 (r) = r + m \u00d7 n,(34)p 2 (r) = r + 2m \u00d7 n,(35)\u2297 r CN OT [s(r) + 3m \u00d7 n, s(r) + 3m \u00d7 n + 1],(37)\nwith r \u2208 {0, \u00b7 \u00b7 \u00b7 , (m \u2212 1) n i=1 i}. The fourth register can be located by adding bias 3m \u00d7 n.\n\nThe above two steps complete the whole operation steps of QLS:\n\n|In, Q, K, Q|K .\n\nBut the fact is that the outputs (s(r) + 3m \u00d7 n + 1) in Eq. (37) of QLS do not all need to be concerned. That is, one needs to filter the entire range of values r \u2208 {0, \u00b7 \u00b7 \u00b7 , (m \u2212 1) n i=1 i} from Eq. (37) to find the part that the project wants. Here,\ng(o) = m \u00d7 o \u2212 1 + 3m \u00d7 n \u2208 (s(r) + 3m \u00d7 n + 1) (39) with o \u2208 {1, \u00b7 \u00b7 \u00b7 , n i=1 i} is picked from r \u2208 {0, \u00b7 \u00b7 \u00b7 , (m \u2212 1) n i=1\ni} as the true QLS output. Once the effective outputs g(o) of QLS are available, the distribution of the outputs can be accessed by programmatically querying the density matrix of g(o), i.e., the by-product QBSASM.\n\nStep 6: calculate the key quantum state |V according to Eq. (13) and Fig. 4:\n|U v \u2297n In, Q, K, Q|K = |V, Q, K, Q|K .(40)\nStep 7: perform the entanglement compression operation in Fig. 7. This means the output is compressed to the last output of the input register after entanglement by CNOT gates to reduce the number of measurements. CNOT gates  (41) if |V i is written as Eq. (16).\nI 2 i \u2297 CN OT \u2297 I 2 n\u2212i |V = m\u22121 \u2297 i=0 n\u22121 \u2295 j=0 V i,i+m\u00d7j\nStep 8: execute the slicing operation as shown in Fig. 8 \no = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 1 + j 2 + n i=1 i \u2212 n\u2212j1 j=1 j j 1 \u2264 j 2 1 + j 1 + n\u22121 i=1 i \u2212 n\u22121\u2212j2 j=1 j else .(42)\nWhen j 1 \u2264 j 2 , j 1 \u2208 {0, \u00b7 \u00b7 \u00b7 , n \u2212 1}, j 2 \u2208 {0, \u00b7 \u00b7 \u00b7 , n \u2212 j 1 }; otherwise j 1 \u2208 {1, \u00b7 \u00b7 \u00b7 , n \u2212 1}, j 2 \u2208 {0, \u00b7 \u00b7 \u00b7 , j 1 }. In this way, the equivalence between the coordinates of the quantum gate and the positions of the elements of the weight matrix is established, then the coordinates of the quantum gate can be confirmed by retrieving the positions of the corresponding elements.\n\nStep 9: combined measurements. This step is measured with skill. Choosing the full output qubit of Eq. (41) and one of the qubits in Eq. (39), the corresponding output can be formed, which also conforms to the reality that the output has 1 more qubit than the input. If the dimensionality is to be guaranteed to be the same, a layer of neural network can be used.\n\nIn general, with the detailed design of the 9 steps, QSAN can not only prevent the measurement in the middle process to ensure the calculation with quantum data until the advent of Step 9, but also compress the number of measurements when Step 9 arrives.\n\n\nMETHOD COMPARISON AND EXPERIMENT\n\nIn this section, the features of QSAN are highlighted by theoretical comparisons. In addition, the feasibility of QSAN with two cores of this paper (QLS and quantum coordinates) is verified by conducting the following experiments on the IBM Qiskit and pennylane platforms. Specifically, the following analysis and experiments are performed:\n\n\u2022 Method Comparison: A theoretical comparison between QSAN and Refs. [2,[25][26][27][28]. \u2022 Experiment 1: The differences between QLS and common quantum similarity ways (i.e. Hadamard test and Swap test) are compared, highlighting the straightforwardness of QLS to solve for similarity. \u2022 Experiment 2: A simple quantum circuit network is constructed, demonstrating that quantum coordinates can convert graphical language to mathematical language, enhancing modeling capabilities and the ability to screen output signals, as well as facilitating programming. \u2022 Experiment 3: QSAN is compared with the mainstream hardware-efficient ansatz and QAOA ansatz to demonstrate that QSAN is faster in classifying the MNIST dataset while obtaining the same prediction accuracy.\n\n\nMethod Comparison\n\nPresently, freely available quantum computers grapple with severe hardware resource constraints. Furthermore, when QSAN is simulated with a classical system, memory requirements grow exponentially as the number of simulated qubits increases. Thus QSAN is theoretically compared with Refs. [2,[25][26][27][28] to highlight its characteristics in terms of four aspects: implementability on quantum computers, methods for realizing attention scores, data properties of attention scores, and measurement compression. The specific comparison results are shown in Tab. 1. From a QML perspective, Tab. 1 lists several important indicators. Firstly, both QSAN and Li's method [27] are implementable on current quantum computing platforms, underscoring the positive effect of QSAN in the QML area. The difference is that QSAN can be completely on a quantum computer, i.e., the self-attention score and the output can be derived from its quantum circuit simultaneously. In contrast, there are additional classical processing steps involved in Li's method. Refs. [25,26] utilize quantum physics concepts to improve classical model performance without designing specific quantum circuits. Refs. [2,28] belong to the classical machine learning domain. QSAN assimilates numerous concepts from Ref. [2], including Key-Query-Value and self-attention score, whlie leverages these concepts to derive a novel operational mechanism without strictly adhering to the framework of Ref. [2]. Meanwhile, Ref. [28] employs quantization to logicize a portion of the operation of SAM, but distinctions persist between digital and quantum logic. For example, quantum logic, founded on tensor operations in Hilbert space, provides a more precise characterization of the physical properties of particles. Additionally, rotating quantum gates with parameters are more flexible than digital logic, boosting applicability.\n\nAnother discrepancy arises in the approach to solving the self-attention scores and theutilized data type. Despite the diverse array of methods for solving self-attention scores, Refs. [2,[25][26][27][28] conceptualize the self-attention score as a scalar, while QSAN consistently upholds it as a tensor in the Hilbert space. In other words, QSAN always maintains the self-attention score as a quantum state by virtue of QLS. This endows QSAN with probabilistic self-attention scores, significantly expands the data representation space of QBSASM beyond the classical self-attention score matrix, and establishes the groundwork for QSAN to obtain outputs and self-attention scores concurrently in a single step.\n\nFinally, QSAN takes into account the compression for the number of measurements. Drawing from the VQA framework in subsection 2.3, outcomes of QSAN are ultimately extracted via quantum measurements. The combined measurements at step 9 in Fig. 2 can compress the storage of the results of quantum measurements in a classical computer. Reducing the number of measurements can somehow minimize the effect of quantum channel noise and save classical storage at the same time.\n\n\nExperiment 1: Comparison of Quantum Logical Similarity and Quantum State Overlap Circuits\n\nInspired by CRC checksums and Frobenius inner products, QLS is an uninterrupted unsigned sum that aims to compress information into a qubit.\n\nSuppose a pair of non-orthogonal quantum states \n\non the Hadamard test circuit and\nP (|0 ) = 1 + | state 1 |state 2 | 2 2(44)\non the Swap test, where P (|0 ) is the measured probability on the ground state |0 . Re( state 1 |state 2 ) and | state 1 |state 2 | 2 are the similarities, especially Re( state 1 |state 2 ) is also the signed similarity. Then these two pairs of quantum states are taken as inputs to Hadamard test circuit, Swap test circuit and QLS module, respectively. The outcomes of comparing these three quantum circuits in solving the quantum state similarity are depicted in Fig. 9. The horizontal coordinates 0 and 1 indicate ground state |0 and excited state |1 . The vertical coordinates indicate the number of statistics, which can be also equivalently viewed as the corresponding probability generated by measuring 1000 times. According to Fig. 9 15|1 for non-orthogonal quantum states and |0 for orthogonal quantum states. Mathematically, QLS is a tensor applicable to quantum linear systems. On the contrary, the similarity of Hadamard (or Swap) test is a scalar, since it has to be gained by collapsing the quantum state in measurement. During processing, its similarity is often determined by max[P (|0 ), P (|1 )].\n\nIn conclusion, compared to the other two similarity methods, QLS is a tensor in mathematical expression and more straightforward in principle, which lays the groundwork for the one-step generation for output and QBSASM of QSAN.\n\n\nExperiment 2: Modeling and Screening Capabilities of Quantum Coordinates\n\nQuantum coordinates, as the design guideline for QSAN, bring convenience to modeling and filtering output. Here, the structure of Fig. 3 is extended, as shown in Fig. 10, to illustrate this advantage.\n\nAs shown in Fig. 10, the Toffoli gates form a regular arrangement. The control and output bits of the Toffoli gates located at different layers both intersect the line to produce Cartesian coordinate points, which is the original idea of quantum coordinates.\n\nModeling: Following the Cartesian coordinates of Fig. 10, we can know that the control and output bits of the Toffoli gates are an arithmetic progression. Therefore, the formula \n\ncan be obtained by mathematical induction to summarize the laws of the Modeling part of Fig. 10. In Eq. (45), layer (or layer + 2) denotes the position of the first (or second) control bit. layer + 4 is the position of the output bit, which is also the position we are interested in.  Screening: Once the coordinates of the output bits are known, the results can be screened. Since in many cases some of the information in the output would be processed again, this makes it necessary to pick out the signals we care about. For example, in this experiment, the output bits with odd serial numbers are picked and then a CNOT gate is applied between them.\n\nAccording to Eq. (45), the sequence\n{layer + 4} n\u22121 layer=0(46)\ncontaining odd and even numbers is obtained. Here the selection operator g(o) is imposed so that the sequence (46) contains only odd numbers. By observation, when g(o) takes an odd number, it makes the condition hold. That is\ng(o) = 2o + 1, o \u2208 N, g(o) \u2208 layer.(47)\nWhen the filtering operator is applied, the layers of the function are deepened and the independent variable becomes the o of the filtering operator instead of the layer, because g(o) is a layer value in the sense. In the end, the screened model\n\u2297 o CN OT [g(o), g(o + 1)](48)\nis derived. Thus in summary, the final mathematical model of this network structure is as follows. \n\nThis example confirms that quantum coordinates can facilitate programming by converting graphical structures into a mathematical language and providing an interface for modeling the next layer by screening operators.\n\n\nExperiment 3: Classification of MNIST dataset based on QSAN\n\nThere is evidence in the classical domain that SAM can be used for image classification [12,13]. In this experiment, QSAN, hardware-efficient ansatz and QAOA ansatz are used on the pennylane platform to simultaneously biclassify images 0 and 1 from the MNIST dataset to demonstrate the power of quantum classifiers.\n\n\nPre-processing of the MNIST dataset\n\nMNIST is a well-known database of handwritten numbers covering 10,000 test images and 60,000 training images [61]. Each image has 28 \u00d7 28 pixel points. However, due to the limitation of the number of qubits in the NISQ era, this dataset must be preprocessed. Preprocessing here refers to some dimensionality reduction operations.\n\nAfter the size of the original image to 4 \u00d7 4, these data are visualized in Fig. 11. The labels in Fig. 11 indicate the numeric type of handwriting. In this experiment, only the data with labels 0 and 1 are applied. Further, the first 50 (or 30) data of label 0 and 1 are selected as the training (or test) set, respectively.   \n\n\nExperimental settings\n\nIn order to verify the effectiveness of QSAN, two typical models in QML, hardware-efficient ansatz [50] and QAOA ansatz [52] are chosen for comparison. They process the same MNIST dataset. The specific experimental parameter settings are shown in Tab. 2, where the training parameters, such as learning rate, optimizer, etc. are kept consistent. However, the parameters, number of layers and data encoding methods vary due to the different quantum models.\n\n\nExperimental analysis\n\nTraining results The classification results of the three models for MNIST data are shown in Fig. 12. The blue line, orange line, and green line indicate hardware-efficient ansatz, QAOA ansatz, and QSAN, respectively. The first, second and third plots in Fig. 12 represent the variation of loss function, test accuracy and training accuracy, respectively. Combining all the plots of experimental results in Fig. 12, the following conclusions are drawn:\n\n\u2022 Quantum circuit model for MNIST dataset classification has 100% test and training accuracy.\n\n\u2022 With the same experimental configuration and similar number of model parameters, QSAN begins to converge at step 130, relative to step 220 for hardwareefficient ansatz and step 300 for QAOA ansatz. QSAN converges about 1.7x and 2.3x faster than hardwareefficient ansatz and QAOA ansatz. Additionally, QSAN achieves a lower loss function value, indicating stronger learning capability. \u2022 The convergence speed of QSAN is relatively sensitive to the parameter batch size within a certain range of variations. As can be seen from the experimental results in Fig. 12, when the batch size changes from 15 to 20, the convergence of the cost function, training accuracy and test accuracy curves of QSAN becomes significantly slower.\n\nQBSASM These heat maps in Fig. 13 are the density matrix QBSASM intercepted by pennylane. Their axes represent the quantum states |Q i , |K j and attention score, respectively. By intercepting the results of the first iteration and the final iteration, it is found that the QBSASM of the output has changed.\n\n\u2022 The high-dimensional property of the QBSASM in Eq. (22) under Hilbert space and the exponential information characterization ability are exhibited. \u2022 As can be expected, this matrix is difficult to simulate for classical computers because it grows exponentially with increasing input.\n\nIn conclusion, the above two points demonstrate that QSAN has faster training speed and higher dimensionality of QBSASM. As the dimensionality of outputs rises, QB-SASM is difficult to be simulated by classical computers, reflecting the advantage of state space representation of quantum computers.\n\n\nCONCLUSION\n\nA novel QSAM framework and its practical model QSAN are proposed. QSAM consists of two major parts, QLS and QBSASM. QLS replaces the practice of inner product similarity and avoids the construction of large quantum numerical operation networks, thus saving more qubits and making QSAN fully deployable on quantum computers in one step. QBSASM can be obtained during the evolution of QSAN to present quantum attention distribution in the form of density matrix. QSAN is a practical model of QSAM, divided into 9 specific steps and 5 special functional modules, which allow to obtain QBSASM during the evolution, as well as to compress the number of measurements. It is worth mentioning that QSAN belongs to a network structure with regular layout. Therefore, quantum coordinates are able to obtain mathematical connections between quantum gate control bits and output bits by induction, which enables to describe the network with mathematical formulas, facilitating the programming implementation and possibly laying the foundation for optimization. In the end, MNIST data binary classification experiments demonstrate that QSAN converges about 1.7x and 2.3x faster than hardware-efficient ansatz and QAOA ansatz with similar configurations, which predicts that the QSAN model has better learning capability.\n\nIn addition, QSAN, as an extensible module, can be embedded into classical or QML architectures, facilitating the construction of a quantum version of Transformer and laying the foundation for quantum-enhanced machine learning.\n\nFig. 1 .\n1Framework of VQA\n\n\nThis led to the following definition of QBSASM. Definition 2 (QBSASM): The weight coefficient matrix\n\nFig. 2 .\n2Circuit Model of QSAN to implement the QSAM theory in Eq. (21).\n\n\n|U q \u2297n In, In, In, 0 = |Q, In, In, 0 ,\n\n\n[r, r + m \u00d7 n]|Q, In, In, 0 = |In, Q, In, 0 ,(28) \n\nFig. 3 .\n3An example of a multi-controlled CNOT gate network to illustrate quantum coordinates.\n\nFig. 4 .\n4Circuit for Uq or U k or Uv output. SW AP [r, r + m \u00d7 n] denotes the exchange of the quantum states between the r-th and (r + m \u00d7 n)-th lines. TakeFig. 5as an example, i.e.\n\n\n[r, r + 2m \u00d7 n]|K, Q, In, 0 = |In, Q, K, 0 .(31) \n\nFig. 5 .\n5Circuit for barbell operation Fig. 6. Circuit for QLS module Firstly, the AND operation is conducted on the qubits in the same position of |Q and |K , and the result is stored in the last garbage register: Q, K, 0 = |In, Q, K, Q \u2227 K . (32)\n\np 3\n3(r, j) = r +m\u00d7j +m(\n\nFig. 7 .\n7Circuit for entanglement compression operation are added for |V . The specific way of adding CNOT is executed according to Eq. (10) and Eq. (21). Specifically, m\u00d7n\n\nFig. 8 .\n8Circuit for slicing operationFirst of all, forFig. 8, the Reset operation[60] must be performed before applying the multi-controlled quantum gates, as the original output is not allowed to have any further effect on the present result. Secondly, the relationship between the element Q j1 |K j2 of QBSASM and the coordinate g(o) is explored, where j 1 is the row number and j 2 is the column number. Observing Eq.(22) and Eq. (39), the parameter o of Eq. (39) has the following relationship with the positions of the matrix elements:\n\n\ngenerated. It is also known that the mathematical relation between similarity and measured probability is demonstrated as P (|0 ) = 1 + Re( state 1 |state 2 ) 2\n\n\n\u2297 layer T of f oli[layer, layer + 2, layer + 4]\n\nFig. 9 .\n9Comparison of results for QLS and quantum state overlapping circuits.\n\nFig. 10 .\n10Extension ofFig. 3to explain the modeling and screening capabilities of quantum coordinates.Fig. 11. Visualization of MNIST dataset\n\n\nf oli[layer, layer + 2, layer + 4] Screening : \u2297 o CN OT [g(o), g(o + 1)].\n\nFig. 12 .\n12Classification results of MNIST data by QSAN ansatz, hardware-efficient ansatz and QAOA ansatz.\n\nFig. 13 .\n13Quantum attention score matrix\n\n\nJinjing Shi and Ren-Xin Zhao contributed equally to this work. Shichao Zhang is corresponding author. \u2022 E-mails: shijinjing@csu.edu.cn, 13061508@alu.hdu.edu.cn, zhangsc@csu.edu.cn, wangwenxuan0524@csu.edu.cn, li@nwpu.edu.cn.\u2022 Jinjing Shi, Ren-Xin Zhao, Shichao Zhang and Wenxuan Wang are \nwith the School of Computer Science and Engineering, Central South \nUniverisity, China, Changsha, 410083. \n\u2022 Xuelong Li is with the School of Artificial Intelligence, OPtics and Elec-\ntroNics (iOPEN), Northwestern Polytechnical University, Xi'an 710072, \nP. R. China. Xuelong Li is also with the Key Laboratory of Intelligent \nInteraction and Applications (Northwestern Polytechnical University), \nMinistry of Industry and Information Technology, Xi'an 710072, P. R. \nChina. \n\u2022 Manuscript received XXXX; revised XXXX. \n\n\n\n\nand select the control bits in accordance with Eq.(22).Tab. 1 \nMethod Comparison \n\nIndicators \nModels \n\nQSAN \nVaswani's [2] \nNiu's [25] \nZhao's [26] \nLi's [27] \nQin's [28] \n\nrealizability on quantum computers \ncompletely \n\u00d7 \n\u00d7 \n\u00d7 \npartially \n\u00d7 \n\nmethods for solving the attention score \nQLS \ndot product \nweak measurement \ndensity matrix Gaussian projection \nquantization \n\ndata types for attention scores \ntensor \nscalar \nscalar \nscalar \nscalar \nscalar \n\ncompressing the number of measurements \n-\n-\n-\n\u00d7 \n-\n\n\n\n\nSwap test. The reason is that the output probability of the QLS directly corresponds to the similarity, while for the Hadamard test and the Swap test, the similarity should be derived with an inverse solving process. Secondly, QLS is designed to preserve quantum states without the requirement for measurement during QSAN computation. Consequently, in this experiment, QLS behaves as, the \nmeasurement probabilities of Hadamard test, Swap test and \nQLS for |0 are 0.722, 0.79 and 0.85, respectively, when the \ninputs are |A and |B . Conversely, the probabilities are 0.52, \n0.499 and 1 for inputs |C and |D , respectively. \nFirstly, comparing Eq. (19), Eq. (43), and Eq. (44), QLS \nis more straightforward relative to the Hadamard test and \nthe \u221a \n0.85|0 + \n\u221a \n0.\n\nParameterized Hamiltonian learning with quantum circuit. J Shi, W Wang, IEEE Transactions on Pattern Analysis and Machine Intelligence. J. Shi, W. Wang et al., \"Parameterized Hamiltonian learn- ing with quantum circuit,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1-10, 2022.\n\nAttention is all you need. A Vaswani, N Shazeer, Advances in Neural Information Processing Systems. 30A. Vaswani, N. Shazeer et al., \"Attention is all you need,\" Advances in Neural Information Processing Sys- tems, vol. 30, 2017.\n\nASTER: An attentional scene text recognizer with flexible rectification. B Shi, M Yang, X Wang, P Lyu, C Yao, X Bai, IEEE Transactions on Pattern Analysis and Machine Intelligence. 419B. Shi, M. Yang, X. Wang, P. Lyu, C. Yao, and X. Bai, \"ASTER: An attentional scene text recognizer with flexi- ble rectification,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 41, no. 9, pp. 2035-2048, 2019.\n\nProgressive and aligned pose attention transfer for person image generation. Z Zhu, T Huang, M Xu, B Shi, W Cheng, X Bai, IEEE Transactions on Pattern Analysis and Machine Intelligence. 448Z. Zhu, T. Huang, M. Xu, B. Shi, W. Cheng, and X. Bai, \"Progressive and aligned pose attention transfer for person image generation,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 8, pp. 4306-4320, 2022.\n\nRealtime scene text detection with differentiable binarization and adaptive scale fusion. M Liao, Z Zou, Z Wan, C Yao, X Bai, IEEE Transactions on Pattern Analysis and Machine Intelligence. M. Liao, Z. Zou, Z. Wan, C. Yao, and X. Bai, \"Real- time scene text detection with differentiable binarization and adaptive scale fusion,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, pp. 1-1, 2022.\n\nDescribing video with attention-based bidirectional LSTM. Y Bin, Y Yang, F Shen, N Xie, H T Shen, X Li, IEEE Transactions on Cybernetics. 497Y. Bin, Y. Yang, F. Shen, N. Xie, H. T. Shen, and X. Li, \"Describing video with attention-based bidirectional LSTM,\" IEEE Transactions on Cybernetics, vol. 49, no. 7, pp. 2631-2641, 2019.\n\nGLCM: Global-local captioning model for remote sensing image captioning. Q Wang, W Huang, X Zhang, X Li, IEEE Transactions on Cybernetics. Q. Wang, W. Huang, X. Zhang, and X. Li, \"GLCM: Global-local captioning model for remote sensing image captioning,\" IEEE Transactions on Cybernetics, pp. 1-13, 2022.\n\nTwo end-to-end quantum-inspired deep neural networks for text classification. J Shi, Z Li, IEEE Transactions on Knowledge and Data Engineering. J. Shi, Z. Li et al., \"Two end-to-end quantum-inspired deep neural networks for text classification,\" IEEE Transactions on Knowledge and Data Engineering, pp. 1-1, 2021.\n\nSG-Net: Syntax guided Transformer for language representation. Z Zhang, Y Wu, IEEE Transactions on Pattern Analysis and Machine Intelligence. 446Z. Zhang, Y. Wu et al., \"SG-Net: Syntax guided Trans- former for language representation,\" IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 6, pp. 3285-3299, 2022.\n\nNeural speech synthesis with Transformer network. N Li, S Liu, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceN. Li, S. Liu et al., \"Neural speech synthesis with Transformer network,\" in Proceedings of the AAAI Con- ference on Artificial Intelligence, pp. 6706-6713, 2019.\n\nAttention-emotion-enhanced convolutional LSTM for sentiment analysis. F Huang, X Li, IEEE Transactions on Neural Networks and Learning Systems. F. Huang, X. Li et al., \"Attention-emotion-enhanced convolutional LSTM for sentiment analysis,\" IEEE Transactions on Neural Networks and Learning Sys- tems, pp. 1-14, 2021.\n\nBottleneck Transformers for visual recognition. A Srinivas, T Y Lin, 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). A. Srinivas, T. Y. Lin et al., \"Bottleneck Transformers for visual recognition,\" in 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 16514-16524, 2021.\n\nSwin Transformer: Hierarchical vision Transformer using shifted windows. Z Liu, Y Lin, 2021 IEEE/CVF International Conference on Computer Vision (ICCV). Z. Liu, Y. Lin et al., \"Swin Transformer: Hierarchical vision Transformer using shifted windows,\" in 2021 IEEE/CVF International Conference on Computer Vi- sion (ICCV), pp. 9992-10002, 2021.\n\nArchitectures for quantum simulation showing a quantum speedup. J Bermejo-Vega, D Hangleiter, Physical Review X. 8221010J. Bermejo-Vega, D. Hangleiter et al., \"Architectures for quantum simulation showing a quantum speedup,\" Physical Review X, vol. 8, no. 2, pp. 021010, 2018.\n\nQuadratic speedup for spatial search by continuous-time quantum walk. S Apers, S Chakraborty, Physical Review Letters. 12916160502S. Apers, S. Chakraborty et al., \"Quadratic speedup for spatial search by continuous-time quantum walk,\" Physical Review Letters, vol. 129, no. 16, pp. 160502, 2022.\n\nQuantum supremacy using a programmable superconducting processor. F Arute, K Arya, Nature. 5747779F. Arute, K. Arya et al., \"Quantum supremacy using a programmable superconducting processor,\" Nature, vol. 574, no. 7779, pp. 505-510, 2019.\n\nQuantum computational advantage using photons. H S Zhong, H Wang, Science. 3706523H. S. Zhong, H. Wang et al., \"Quantum computational advantage using photons,\" Science, vol. 370, no. 6523, pp. 1460-1463, 2020.\n\nQuantum computational advantage with a programmable photonic processor. L S Madsen, F Laudenbach, Nature. 6067912L. S. Madsen, F. Laudenbach et al. \"Quantum com- putational advantage with a programmable photonic processor,\" Nature, vol. 606, no. 7912, pp. 75-81, 2022.\n\nQuantum supremacy: Some fundamental concepts. M H Yung, National Science Review. 61M. H. Yung, \"Quantum supremacy: Some fundamental concepts,\" National Science Review, vol. 6, no. 1, pp. 22- 23, 2019.\n\nQuantum computational supremacy. A W Harrow, A Montanaro, Nature. 5497671A. W. Harrow, and A. Montanaro, \"Quantum compu- tational supremacy,\" Nature, vol. 549, no. 7671, pp. 203- 209, 2017.\n\nPractical quantum advantage in quantum simulation. A J Daley, I Bloch, Nature. 6077920A. J. Daley, I. Bloch et al., \"Practical quantum advan- tage in quantum simulation,\" Nature, vol. 607, no. 7920, pp. 667-676, 2022.\n\nChallenges and opportunities in quantum machine learning. M Cerezo, G Verdon, Nature Computational Science. M. Cerezo, G. Verdon et al., \"Challenges and opportu- nities in quantum machine learning,\" Nature Computa- tional Science, 2022.\n\nEncoding-dependent generalization bounds for parametrized quantum circuits. M C Caro, E Gil-Fuster, 5582QuantumM. C. Caro, E. Gil-Fuster et al., \"Encoding-dependent generalization bounds for parametrized quantum cir- cuits,\" Quantum, vol. 5, pp. 582, 2021.\n\nExpanding data encoding patterns for quantum algorithms. M Weigold, J Barzen, 2021 IEEE 18th International Conference on Software Architecture Companion (ICSA-C). M. Weigold, J. Barzen et al., \"Expanding data encoding patterns for quantum algorithms,\" in 2021 IEEE 18th International Conference on Software Architecture Com- panion (ICSA-C), pp. 95-101, 2021.\n\nBi-directional LSTM with quantum attention mechanism for sentence modeling. X Niu, Y Hou, Neural Information Processing. X. Niu, Y. Hou et al., \"Bi-directional LSTM with quan- tum attention mechanism for sentence modeling,\" in Neural Information Processing, pp. 178-188, 2017.\n\nQuantum attention based language model for answer selection. Q Zhao, C Hou, Artificial Intelligence and Mobile Services -Aims 2021. Q. Zhao, C. Hou et al., \"Quantum attention based language model for answer selection,\" in Artificial In- telligence and Mobile Services -Aims 2021, pp. 47-57, 2022.\n\nQuantum self-attention neural networks for text classification. G Li, X Zhao, arxiv:2205.05625arxiv PreprintG. Li, X. Zhao et al., \"Quantum self-attention neu- ral networks for text classification,\" arxiv Preprint arxiv:2205.05625, 2022.\n\nBiBERT: Accurate fully binarized BERT. H Qin, Y Ding, International Conference on Learning Representations. H. Qin, Y. Ding et al., \"BiBERT: Accurate fully bina- rized BERT,\" in International Conference on Learning Representations, 2021.\n\nA variational eigenvalue solver on a photonic quantum processor. A Peruzzo, J Mcclean, Nature Communications. 514213A. Peruzzo, J. Mcclean et al., \"A variational eigenvalue solver on a photonic quantum processor,\" Nature Com- munications, vol. 5, no. 1, pp. 4213, 2014.\n\nQuantum siamese neural network based on binary quantum flexible representation. J Shi, Y Lu, IEEE Transactions on Pattern Analysis and Machine Intelligence (Under Review. 2022J. Shi, Y. Lu et al., \"Quantum siamese neural network based on binary quantum flexible representation,\" IEEE Transactions on Pattern Analysis and Machine Intelli- gence (Under Review), 2022.\n\nQuantum circuit learning with parameterized boson sampling. J Shi, Y Tang, IEEE Transactions on Knowledge and Data Engineering. J. Shi, Y. Tang et al., \"Quantum circuit learning with parameterized boson sampling,\" IEEE Transactions on Knowledge and Data Engineering, pp. 1-1, 2021.\n\nRobust quantum feature selection algorithm. S Zhang, J Li, IEEE Transactions on Pattern Analysis and Machine Intelligence (Under Review. 2022S. Zhang, J. Li et al., \"Robust quantum feature selection algorithm,\" IEEE Transactions on Pattern Analysis and Machine Intelligence (Under Review), 2022.\n\nR Zhao, S Wang, arXiv:2109.01840A review of quantum neural networks: Methods, models, dilemma. arXiv preprintR. Zhao, and S. Wang, \"A review of quantum neural networks: Methods, models, dilemma,\" arXiv preprint arXiv:2109.01840, 2021.\n\nQuantum convolutional neural networks. I Cong, S Choi, Nature Physics. 1512I. Cong, S. Choi et al., \"Quantum convolutional neural networks,\" Nature Physics, vol. 15, no. 12, pp. 1273-1278, 2019.\n\nRecurrent quantum neural networks. J Bausch, Advances in Neural Information Processing Systems. 33J. Bausch, \"Recurrent quantum neural networks,\" Ad- vances in Neural Information Processing Systems, vol. 33, pp. 1368-1379, 2020.\n\nExperimental quantum adversarial learning with programmable superconducting qubits. W Ren, W Li, Nature Computational Science. 211W. Ren, W. Li et al., \"Experimental quantum ad- versarial learning with programmable superconducting qubits,\" Nature Computational Science, vol. 2, no. 11, pp. 711-717, 2022.\n\nQuantum machine learning for electronic structure calculations. R Xia, S Kais, Nature Communications. 914195R. Xia, and S. Kais, \"Quantum machine learning for electronic structure calculations,\" Nature Communica- tions, vol. 9, no. 1, pp. 4195, 2018.\n\nQuantum machine learning. J Biamonte, P Wittek, Nature. 5497671J. Biamonte, P. Wittek et al., \"Quantum machine learn- ing,\" Nature, vol. 549, no. 7671, pp. 195-202, 2017.\n\nSupervised learning with quantum-enhanced feature spaces. V Havl\u00ed\u010dek, A D C\u00f3rcoles, Nature. 5677747V. Havl\u00ed\u010dek, A. D. C\u00f3rcoles et al., \"Supervised learning with quantum-enhanced feature spaces,\" Nature, vol. 567, no. 7747, pp. 209-212, 2019.\n\nQuantum machine learning in feature Hilbert spaces. M Schuld, N Killoran, Physical Review Letters. 122440504M. Schuld, and N. Killoran, \"Quantum machine learn- ing in feature Hilbert spaces,\" Physical Review Letters, vol. 122, no. 4, pp. 040504, 2019.\n\nQuantum principal component analysis. S Lloyd, M Mohseni, Nature Physics. 109S. Lloyd, M. Mohseni et al., \"Quantum principal com- ponent analysis,\" Nature Physics, vol. 10, no. 9, pp. 631- 633, 2014.\n\nQuantum Boltzmann machine. M H Amin, E Andriyash, Physical Review X. 8221050M. H. Amin, E. Andriyash et al., \"Quantum Boltzmann machine,\" Physical Review X, vol. 8, no. 2, pp. 021050, 2018.\n\nParameterized quantum circuits as machine learning models. M Benedetti, E Lloyd, Quantum Science and Technology. 4443001M. Benedetti, E. Lloyd et al., \"Parameterized quantum circuits as machine learning models,\" Quantum Science and Technology, vol. 4, no. 4, pp. 043001, 2019.\n\nNoise-induced barren plateaus in variational quantum algorithms. S Wang, E Fontana, Nature Communications. 1216961S. Wang, E. Fontana et al., \"Noise-induced barren plateaus in variational quantum algorithms,\" Nature Communications, vol. 12, no. 1, pp. 6961, 2021.\n\nIs quantum advantage the right goal for quantum machine learning?. M Schuld, N Killoran, PRX Quantum. 3330101M. Schuld, and N. Killoran, \"Is quantum advantage the right goal for quantum machine learning?,\" PRX Quantum, vol. 3, no. 3, pp. 030101, 2022.\n\nMachine learning of noise-resilient quantum circuits. L Cincio, K Rudinger, PRX Quantum. 2110324L. Cincio, K. Rudinger et al., \"Machine learning of noise-resilient quantum circuits,\" PRX Quantum, vol. 2, no. 1, pp. 010324, 2021.\n\nAn adaptive optimizer for measurement-frugal variational algorithms. J M K\u00fcbler, A Arrasmith, 4263QuantumJ. M. K\u00fcbler, A. Arrasmith et al., \"An adaptive opti- mizer for measurement-frugal variational algorithms,\" Quantum, vol. 4, pp. 263, 2020.\n\nStochastic gradient descent for hybrid quantum-classical optimization. R Sweke, F Wilde, 4314QuantumR. Sweke, F. Wilde et al., \"Stochastic gradient descent for hybrid quantum-classical optimization,\" Quantum, vol. 4, pp. 314, 2020.\n\nQuantum natural gradient. J Stokes, J Izaac, 4269QuantumJ. Stokes, J. Izaac et al., \"Quantum natural gradient,\" Quantum, vol. 4, pp. 269, 2020.\n\nHardware-efficient variational quantum eigensolver for small molecules and quantum magnets. A Kandala, A Mezzacapo, Nature. 5497671A. Kandala, A. Mezzacapo et al., \"Hardware-efficient variational quantum eigensolver for small molecules and quantum magnets,\" Nature, vol. 549, no. 7671, pp. 242-246, 2017.\n\nCorrelation-informed permutation of qubits for reducing ansatz depth in the variational quantum eigensolver. N V Tkachenko, J Sud, PRX Quantum. 2220337N. V. Tkachenko, J. Sud et al., \"Correlation-informed permutation of qubits for reducing ansatz depth in the variational quantum eigensolver,\" PRX Quantum, vol. 2, no. 2, pp. 020337, 2021.\n\nA quantum approximate optimization algorithm. E Farhi, J Goldstone, arXiv:1411.4028arXiv preprintE. Farhi, J. Goldstone et al., \"A quantum ap- proximate optimization algorithm,\" arXiv preprint arXiv:1411.4028, 2014.\n\nFrom the quantum approximate optimization algorithm to a quantum alternating operator ansatz. S Hadfield, Z Wang, Algorithms. 122S. Hadfield, Z. Wang et al., \"From the quantum approx- imate optimization algorithm to a quantum alternating operator ansatz,\" Algorithms, vol. 12, no. 2, 2019.\n\nOn the universality of the quantum approximate optimization algorithm. M E S Morales, J D Biamonte, Quantum Information Processing. 19291M. E. S. Morales, J. D. Biamonte et al., \"On the uni- versality of the quantum approximate optimization al- gorithm,\" Quantum Information Processing, vol. 19, no. 9, pp. 291, 2020.\n\nQuantum computing. E , Nature. 4637280E. Knill, \"Quantum computing,\" Nature, vol. 463, no. 7280, pp. 441-443, 2010.\n\nQuantum computing in the NISQ era and beyond. J , Quantum. 279J. Preskill, \"Quantum computing in the NISQ era and beyond,\" Quantum, vol. 2, pp. 79, 2018.\n\nRealamplitudes documentation. Qiskit Development Team, Qiskit Development Team. \"Realamplitudes documen- tation,\" https://qiskit.org/documentation/stubs/qiski- t.circuit.library.RealAmplitudes.html.\n\nQuantum fingerprinting. H Buhrman, R Cleve, Physical Review Letters. 8716167902H. Buhrman, R. Cleve et al., \"Quantum fingerprinting,\" Physical Review Letters, vol. 87, no. 16, pp. 167902, 2001.\n\nA polynomial quantum algorithm for approximating the jones polynomial. D Aharonov, V Jones, Algorithmica. 553D. Aharonov, V. Jones et al., \"A polynomial quantum algorithm for approximating the jones polynomial,\" Al- gorithmica, vol. 55, no. 3, pp. 395-421, 2009.\n\nSummary of quantum operations. Qiskit Development Team, Qiskit Development Team. \"Summary of quantum op- erations,\" https://qiskit.org/documentation/tutorials/ circuits/3 summary of quantum operations.html.\n\nGradient-based learning applied to document recognition. Y Lecun, L Bottou, Proceedings of the IEEE. 8611Y. Lecun, L. Bottou et al., \"Gradient-based learning applied to document recognition,\" Proceedings of the IEEE, vol. 86, no. 11, pp. 2278-2324, 1998.\n", "annotations": {"author": "[{\"end\":97,\"start\":73},{\"end\":123,\"start\":98},{\"end\":157,\"start\":124},{\"end\":172,\"start\":158},{\"end\":203,\"start\":173}]", "publisher": null, "author_last_name": "[{\"end\":96,\"start\":93},{\"end\":122,\"start\":118},{\"end\":156,\"start\":152},{\"end\":171,\"start\":166},{\"end\":202,\"start\":200}]", "author_first_name": "[{\"end\":92,\"start\":85},{\"end\":117,\"start\":110},{\"end\":151,\"start\":144},{\"end\":165,\"start\":158},{\"end\":199,\"start\":192}]", "author_affiliation": null, "title": "[{\"end\":60,\"start\":1},{\"end\":263,\"start\":204}]", "venue": null, "abstract": "[{\"end\":2324,\"start\":441}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2432,\"start\":2429},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2768,\"start\":2765},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2934,\"start\":2931},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3024,\"start\":3021},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3027,\"start\":3024},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3030,\"start\":3027},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3033,\"start\":3030},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3066,\"start\":3063},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3069,\"start\":3066},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3072,\"start\":3069},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3085,\"start\":3081},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3111,\"start\":3107},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3220,\"start\":3216},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3326,\"start\":3322},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3903,\"start\":3899},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3907,\"start\":3903},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3911,\"start\":3907},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3915,\"start\":3911},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3919,\"start\":3915},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4003,\"start\":3999},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4006,\"start\":4003},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4226,\"start\":4222},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4229,\"start\":4226},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4305,\"start\":4301},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4308,\"start\":4305},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4637,\"start\":4633},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4871,\"start\":4867},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":5176,\"start\":5172},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5196,\"start\":5193},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5200,\"start\":5196},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5204,\"start\":5200},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5208,\"start\":5204},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":5256,\"start\":5252},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5304,\"start\":5300},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5354,\"start\":5350},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5358,\"start\":5354},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5362,\"start\":5358},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6263,\"start\":6259},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6267,\"start\":6263},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6271,\"start\":6267},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6649,\"start\":6645},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6653,\"start\":6649},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6657,\"start\":6653},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8602,\"start\":8598},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8654,\"start\":8650},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8694,\"start\":8690},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8743,\"start\":8739},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":8817,\"start\":8813},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8855,\"start\":8851},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8982,\"start\":8978},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9026,\"start\":9022},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9029,\"start\":9026},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9129,\"start\":9125},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9132,\"start\":9129},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9240,\"start\":9236},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9294,\"start\":9290},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9552,\"start\":9548},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10024,\"start\":10021},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10954,\"start\":10950},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":11022,\"start\":11018},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":11025,\"start\":11022},{\"end\":11592,\"start\":11589},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":11971,\"start\":11967},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":12005,\"start\":12001},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":12036,\"start\":12032},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":12325,\"start\":12321},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":12328,\"start\":12325},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":12346,\"start\":12342},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":12350,\"start\":12346},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":12354,\"start\":12350},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":12916,\"start\":12912},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":15702,\"start\":15698},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":17073,\"start\":17069},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":17095,\"start\":17091},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17664,\"start\":17660},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":18011,\"start\":18007},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":18377,\"start\":18373},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22837,\"start\":22833},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":25661,\"start\":25657},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":26443,\"start\":26439},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":26745,\"start\":26741},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":27316,\"start\":27312},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":28611,\"start\":28607},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":28642,\"start\":28638},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":29406,\"start\":29402},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":30335,\"start\":30332},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":30339,\"start\":30335},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":30343,\"start\":30339},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":30347,\"start\":30343},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":30351,\"start\":30347},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":31344,\"start\":31341},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":31348,\"start\":31344},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":31352,\"start\":31348},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":31356,\"start\":31352},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":31360,\"start\":31356},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":31724,\"start\":31720},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":32108,\"start\":32104},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":32111,\"start\":32108},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":32238,\"start\":32235},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":32241,\"start\":32238},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":32339,\"start\":32336},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":32518,\"start\":32515},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":32540,\"start\":32536},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":33130,\"start\":33127},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":33134,\"start\":33130},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":33138,\"start\":33134},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":33142,\"start\":33138},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":33146,\"start\":33142},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":38285,\"start\":38281},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":38288,\"start\":38285},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":38661,\"start\":38657},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":39336,\"start\":39332},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":39357,\"start\":39353},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":41357,\"start\":41353},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":43737,\"start\":43733},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":44070,\"start\":44066},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":44612,\"start\":44608},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":44951,\"start\":44947},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":46605,\"start\":46601}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":43465,\"start\":43438},{\"attributes\":{\"id\":\"fig_1\"},\"end\":43568,\"start\":43466},{\"attributes\":{\"id\":\"fig_2\"},\"end\":43643,\"start\":43569},{\"attributes\":{\"id\":\"fig_3\"},\"end\":43685,\"start\":43644},{\"attributes\":{\"id\":\"fig_4\"},\"end\":43738,\"start\":43686},{\"attributes\":{\"id\":\"fig_5\"},\"end\":43835,\"start\":43739},{\"attributes\":{\"id\":\"fig_6\"},\"end\":44019,\"start\":43836},{\"attributes\":{\"id\":\"fig_7\"},\"end\":44071,\"start\":44020},{\"attributes\":{\"id\":\"fig_8\"},\"end\":44322,\"start\":44072},{\"attributes\":{\"id\":\"fig_9\"},\"end\":44348,\"start\":44323},{\"attributes\":{\"id\":\"fig_10\"},\"end\":44523,\"start\":44349},{\"attributes\":{\"id\":\"fig_11\"},\"end\":45067,\"start\":44524},{\"attributes\":{\"id\":\"fig_12\"},\"end\":45230,\"start\":45068},{\"attributes\":{\"id\":\"fig_13\"},\"end\":45280,\"start\":45231},{\"attributes\":{\"id\":\"fig_14\"},\"end\":45361,\"start\":45281},{\"attributes\":{\"id\":\"fig_15\"},\"end\":45506,\"start\":45362},{\"attributes\":{\"id\":\"fig_16\"},\"end\":45583,\"start\":45507},{\"attributes\":{\"id\":\"fig_17\"},\"end\":45692,\"start\":45584},{\"attributes\":{\"id\":\"fig_18\"},\"end\":45736,\"start\":45693},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":46548,\"start\":45737},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":47059,\"start\":46549},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":47825,\"start\":47060}]", "paragraph": "[{\"end\":5835,\"start\":2340},{\"end\":6944,\"start\":5837},{\"end\":8001,\"start\":6946},{\"end\":8346,\"start\":8003},{\"end\":8437,\"start\":8364},{\"end\":9553,\"start\":8466},{\"end\":10041,\"start\":9582},{\"end\":10153,\"start\":10069},{\"end\":10325,\"start\":10189},{\"end\":10451,\"start\":10367},{\"end\":10680,\"start\":10473},{\"end\":11472,\"start\":10714},{\"end\":12360,\"start\":11516},{\"end\":12733,\"start\":12362},{\"end\":12924,\"start\":12757},{\"end\":12993,\"start\":12945},{\"end\":13134,\"start\":13016},{\"end\":13389,\"start\":13162},{\"end\":13946,\"start\":13426},{\"end\":14001,\"start\":13948},{\"end\":14285,\"start\":14102},{\"end\":14708,\"start\":14324},{\"end\":14755,\"start\":14752},{\"end\":14980,\"start\":14778},{\"end\":15043,\"start\":15040},{\"end\":15550,\"start\":15074},{\"end\":15813,\"start\":15603},{\"end\":16712,\"start\":15842},{\"end\":17277,\"start\":16743},{\"end\":17708,\"start\":17316},{\"end\":18070,\"start\":17871},{\"end\":18286,\"start\":18114},{\"end\":18472,\"start\":18288},{\"end\":18597,\"start\":18591},{\"end\":19850,\"start\":18875},{\"end\":20607,\"start\":19885},{\"end\":20700,\"start\":20609},{\"end\":20783,\"start\":20702},{\"end\":20882,\"start\":20785},{\"end\":21037,\"start\":20930},{\"end\":22373,\"start\":21039},{\"end\":22466,\"start\":22375},{\"end\":22549,\"start\":22468},{\"end\":22648,\"start\":22551},{\"end\":22888,\"start\":22650},{\"end\":23032,\"start\":22932},{\"end\":23368,\"start\":23056},{\"end\":23651,\"start\":23370},{\"end\":24003,\"start\":23653},{\"end\":25125,\"start\":24065},{\"end\":25159,\"start\":25154},{\"end\":25434,\"start\":25216},{\"end\":25593,\"start\":25459},{\"end\":25692,\"start\":25595},{\"end\":25754,\"start\":25694},{\"end\":25761,\"start\":25756},{\"end\":26276,\"start\":25763},{\"end\":26377,\"start\":26320},{\"end\":26557,\"start\":26379},{\"end\":26822,\"start\":26559},{\"end\":27381,\"start\":26884},{\"end\":27577,\"start\":27481},{\"end\":27641,\"start\":27579},{\"end\":27659,\"start\":27643},{\"end\":27915,\"start\":27661},{\"end\":28258,\"start\":28044},{\"end\":28336,\"start\":28260},{\"end\":28643,\"start\":28381},{\"end\":28760,\"start\":28703},{\"end\":29263,\"start\":28870},{\"end\":29628,\"start\":29265},{\"end\":29884,\"start\":29630},{\"end\":30261,\"start\":29921},{\"end\":31030,\"start\":30263},{\"end\":32940,\"start\":31052},{\"end\":33653,\"start\":32942},{\"end\":34126,\"start\":33655},{\"end\":34360,\"start\":34220},{\"end\":34410,\"start\":34362},{\"end\":34444,\"start\":34412},{\"end\":35603,\"start\":34488},{\"end\":35832,\"start\":35605},{\"end\":36109,\"start\":35909},{\"end\":36369,\"start\":36111},{\"end\":36549,\"start\":36371},{\"end\":37203,\"start\":36551},{\"end\":37240,\"start\":37205},{\"end\":37494,\"start\":37269},{\"end\":37780,\"start\":37535},{\"end\":37911,\"start\":37812},{\"end\":38129,\"start\":37913},{\"end\":38508,\"start\":38193},{\"end\":38877,\"start\":38548},{\"end\":39207,\"start\":38879},{\"end\":39688,\"start\":39233},{\"end\":40165,\"start\":39714},{\"end\":40260,\"start\":40167},{\"end\":40989,\"start\":40262},{\"end\":41298,\"start\":40991},{\"end\":41586,\"start\":41300},{\"end\":41886,\"start\":41588},{\"end\":43208,\"start\":41901},{\"end\":43437,\"start\":43210}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10068,\"start\":10042},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10188,\"start\":10154},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10346,\"start\":10326},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10366,\"start\":10346},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10472,\"start\":10452},{\"attributes\":{\"id\":\"formula_5\"},\"end\":11515,\"start\":11473},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12944,\"start\":12925},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13015,\"start\":12994},{\"attributes\":{\"id\":\"formula_8\"},\"end\":13161,\"start\":13135},{\"attributes\":{\"id\":\"formula_9\"},\"end\":14101,\"start\":14002},{\"attributes\":{\"id\":\"formula_10\"},\"end\":14323,\"start\":14286},{\"attributes\":{\"id\":\"formula_11\"},\"end\":14730,\"start\":14709},{\"attributes\":{\"id\":\"formula_12\"},\"end\":14751,\"start\":14730},{\"attributes\":{\"id\":\"formula_13\"},\"end\":14777,\"start\":14756},{\"attributes\":{\"id\":\"formula_14\"},\"end\":15010,\"start\":14981},{\"attributes\":{\"id\":\"formula_15\"},\"end\":15039,\"start\":15010},{\"attributes\":{\"id\":\"formula_16\"},\"end\":15073,\"start\":15044},{\"attributes\":{\"id\":\"formula_17\"},\"end\":15602,\"start\":15551},{\"attributes\":{\"id\":\"formula_18\"},\"end\":15841,\"start\":15814},{\"attributes\":{\"id\":\"formula_19\"},\"end\":17315,\"start\":17278},{\"attributes\":{\"id\":\"formula_20\"},\"end\":17870,\"start\":17709},{\"attributes\":{\"id\":\"formula_21\"},\"end\":18590,\"start\":18473},{\"attributes\":{\"id\":\"formula_22\"},\"end\":18731,\"start\":18598},{\"attributes\":{\"id\":\"formula_23\"},\"end\":18874,\"start\":18731},{\"attributes\":{\"id\":\"formula_24\"},\"end\":22913,\"start\":22889},{\"attributes\":{\"id\":\"formula_25\"},\"end\":24064,\"start\":24004},{\"attributes\":{\"id\":\"formula_26\"},\"end\":25153,\"start\":25126},{\"attributes\":{\"id\":\"formula_27\"},\"end\":25215,\"start\":25160},{\"attributes\":{\"id\":\"formula_29\"},\"end\":26319,\"start\":26277},{\"attributes\":{\"id\":\"formula_31\"},\"end\":26883,\"start\":26823},{\"attributes\":{\"id\":\"formula_32\"},\"end\":27406,\"start\":27382},{\"attributes\":{\"id\":\"formula_33\"},\"end\":27431,\"start\":27406},{\"attributes\":{\"id\":\"formula_34\"},\"end\":27480,\"start\":27431},{\"attributes\":{\"id\":\"formula_36\"},\"end\":28043,\"start\":27916},{\"attributes\":{\"id\":\"formula_37\"},\"end\":28380,\"start\":28337},{\"attributes\":{\"id\":\"formula_38\"},\"end\":28702,\"start\":28644},{\"attributes\":{\"id\":\"formula_39\"},\"end\":28869,\"start\":28761},{\"attributes\":{\"id\":\"formula_41\"},\"end\":34487,\"start\":34445},{\"attributes\":{\"id\":\"formula_43\"},\"end\":37268,\"start\":37241},{\"attributes\":{\"id\":\"formula_44\"},\"end\":37534,\"start\":37495},{\"attributes\":{\"id\":\"formula_45\"},\"end\":37811,\"start\":37781}]", "table_ref": null, "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2338,\"start\":2326},{\"attributes\":{\"n\":\"2\"},\"end\":8362,\"start\":8349},{\"attributes\":{\"n\":\"2.1\"},\"end\":8464,\"start\":8440},{\"attributes\":{\"n\":\"2.2\"},\"end\":9580,\"start\":9556},{\"attributes\":{\"n\":\"2.3\"},\"end\":10712,\"start\":10683},{\"attributes\":{\"n\":\"2.4\"},\"end\":12755,\"start\":12736},{\"attributes\":{\"n\":\"3\"},\"end\":13424,\"start\":13392},{\"attributes\":{\"n\":\"3.1\"},\"end\":16741,\"start\":16715},{\"attributes\":{\"n\":\"3.2\"},\"end\":18112,\"start\":18073},{\"attributes\":{\"n\":\"4\"},\"end\":19883,\"start\":19853},{\"attributes\":{\"n\":\"4.1\"},\"end\":20928,\"start\":20885},{\"attributes\":{\"n\":\"4.2\"},\"end\":22930,\"start\":22915},{\"attributes\":{\"n\":\"4.2.1\"},\"end\":23054,\"start\":23035},{\"attributes\":{\"n\":\"4.2.2\"},\"end\":25457,\"start\":25437},{\"attributes\":{\"n\":\"5\"},\"end\":29919,\"start\":29887},{\"attributes\":{\"n\":\"5.1\"},\"end\":31050,\"start\":31033},{\"attributes\":{\"n\":\"5.2\"},\"end\":34218,\"start\":34129},{\"attributes\":{\"n\":\"5.3\"},\"end\":35907,\"start\":35835},{\"attributes\":{\"n\":\"5.4\"},\"end\":38191,\"start\":38132},{\"attributes\":{\"n\":\"5.4.1\"},\"end\":38546,\"start\":38511},{\"attributes\":{\"n\":\"5.4.2\"},\"end\":39231,\"start\":39210},{\"attributes\":{\"n\":\"5.4.3\"},\"end\":39712,\"start\":39691},{\"attributes\":{\"n\":\"6\"},\"end\":41899,\"start\":41889},{\"end\":43447,\"start\":43439},{\"end\":43578,\"start\":43570},{\"end\":43748,\"start\":43740},{\"end\":43845,\"start\":43837},{\"end\":44081,\"start\":44073},{\"end\":44327,\"start\":44324},{\"end\":44358,\"start\":44350},{\"end\":44533,\"start\":44525},{\"end\":45290,\"start\":45282},{\"end\":45372,\"start\":45363},{\"end\":45594,\"start\":45585},{\"end\":45703,\"start\":45694}]", "table": "[{\"end\":46548,\"start\":45963},{\"end\":47059,\"start\":46606},{\"end\":47825,\"start\":47445}]", "figure_caption": "[{\"end\":43465,\"start\":43449},{\"end\":43568,\"start\":43468},{\"end\":43643,\"start\":43580},{\"end\":43685,\"start\":43646},{\"end\":43738,\"start\":43688},{\"end\":43835,\"start\":43750},{\"end\":44019,\"start\":43847},{\"end\":44071,\"start\":44022},{\"end\":44322,\"start\":44083},{\"end\":44348,\"start\":44329},{\"end\":44523,\"start\":44360},{\"end\":45067,\"start\":44535},{\"end\":45230,\"start\":45070},{\"end\":45280,\"start\":45233},{\"end\":45361,\"start\":45292},{\"end\":45506,\"start\":45375},{\"end\":45583,\"start\":45509},{\"end\":45692,\"start\":45597},{\"end\":45736,\"start\":45706},{\"end\":45963,\"start\":45739},{\"end\":46606,\"start\":46551},{\"end\":47445,\"start\":47062}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11181,\"start\":11175},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11423,\"start\":11417},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12382,\"start\":12376},{\"end\":15214,\"start\":15208},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":15225,\"start\":15219},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":15884,\"start\":15878},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":15892,\"start\":15886},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":15903,\"start\":15897},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21036,\"start\":21030},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21065,\"start\":21059},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21328,\"start\":21314},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":21336,\"start\":21330},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":21347,\"start\":21341},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21482,\"start\":21460},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":21493,\"start\":21487},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21560,\"start\":21554},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":21744,\"start\":21738},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22060,\"start\":22053},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22133,\"start\":22126},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":22745,\"start\":22739},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23031,\"start\":23025},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":23533,\"start\":23527},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":23672,\"start\":23666},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":24074,\"start\":24068},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":25725,\"start\":25719},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":26206,\"start\":26200},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":26476,\"start\":26470},{\"end\":26778,\"start\":26772},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":28335,\"start\":28329},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":28445,\"start\":28439},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":28759,\"start\":28753},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":33899,\"start\":33893},{\"attributes\":{\"ref_id\":\"fig_14\"},\"end\":34960,\"start\":34954},{\"attributes\":{\"ref_id\":\"fig_14\"},\"end\":35230,\"start\":35224},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":36045,\"start\":36039},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36078,\"start\":36071},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36130,\"start\":36123},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36427,\"start\":36420},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36646,\"start\":36639},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38962,\"start\":38955},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38985,\"start\":38978},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":39813,\"start\":39806},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":39975,\"start\":39968},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":40127,\"start\":40120},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":40826,\"start\":40819},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":41024,\"start\":41017}]", "bib_author_first_name": "[{\"end\":47885,\"start\":47884},{\"end\":47892,\"start\":47891},{\"end\":48157,\"start\":48156},{\"end\":48168,\"start\":48167},{\"end\":48434,\"start\":48433},{\"end\":48441,\"start\":48440},{\"end\":48449,\"start\":48448},{\"end\":48457,\"start\":48456},{\"end\":48464,\"start\":48463},{\"end\":48471,\"start\":48470},{\"end\":48855,\"start\":48854},{\"end\":48862,\"start\":48861},{\"end\":48871,\"start\":48870},{\"end\":48877,\"start\":48876},{\"end\":48884,\"start\":48883},{\"end\":48893,\"start\":48892},{\"end\":49293,\"start\":49292},{\"end\":49301,\"start\":49300},{\"end\":49308,\"start\":49307},{\"end\":49315,\"start\":49314},{\"end\":49322,\"start\":49321},{\"end\":49670,\"start\":49669},{\"end\":49677,\"start\":49676},{\"end\":49685,\"start\":49684},{\"end\":49693,\"start\":49692},{\"end\":49700,\"start\":49699},{\"end\":49702,\"start\":49701},{\"end\":49710,\"start\":49709},{\"end\":50015,\"start\":50014},{\"end\":50023,\"start\":50022},{\"end\":50032,\"start\":50031},{\"end\":50041,\"start\":50040},{\"end\":50325,\"start\":50324},{\"end\":50332,\"start\":50331},{\"end\":50625,\"start\":50624},{\"end\":50634,\"start\":50633},{\"end\":50950,\"start\":50949},{\"end\":50956,\"start\":50955},{\"end\":51306,\"start\":51305},{\"end\":51315,\"start\":51314},{\"end\":51602,\"start\":51601},{\"end\":51614,\"start\":51613},{\"end\":51616,\"start\":51615},{\"end\":51956,\"start\":51955},{\"end\":51963,\"start\":51962},{\"end\":52292,\"start\":52291},{\"end\":52308,\"start\":52307},{\"end\":52576,\"start\":52575},{\"end\":52585,\"start\":52584},{\"end\":52869,\"start\":52868},{\"end\":52878,\"start\":52877},{\"end\":53090,\"start\":53089},{\"end\":53092,\"start\":53091},{\"end\":53101,\"start\":53100},{\"end\":53326,\"start\":53325},{\"end\":53328,\"start\":53327},{\"end\":53338,\"start\":53337},{\"end\":53570,\"start\":53569},{\"end\":53572,\"start\":53571},{\"end\":53759,\"start\":53758},{\"end\":53761,\"start\":53760},{\"end\":53771,\"start\":53770},{\"end\":53968,\"start\":53967},{\"end\":53970,\"start\":53969},{\"end\":53979,\"start\":53978},{\"end\":54194,\"start\":54193},{\"end\":54204,\"start\":54203},{\"end\":54450,\"start\":54449},{\"end\":54452,\"start\":54451},{\"end\":54460,\"start\":54459},{\"end\":54689,\"start\":54688},{\"end\":54700,\"start\":54699},{\"end\":55069,\"start\":55068},{\"end\":55076,\"start\":55075},{\"end\":55332,\"start\":55331},{\"end\":55340,\"start\":55339},{\"end\":55633,\"start\":55632},{\"end\":55639,\"start\":55638},{\"end\":55847,\"start\":55846},{\"end\":55854,\"start\":55853},{\"end\":56112,\"start\":56111},{\"end\":56123,\"start\":56122},{\"end\":56398,\"start\":56397},{\"end\":56405,\"start\":56404},{\"end\":56745,\"start\":56744},{\"end\":56752,\"start\":56751},{\"end\":57012,\"start\":57011},{\"end\":57021,\"start\":57020},{\"end\":57265,\"start\":57264},{\"end\":57273,\"start\":57272},{\"end\":57540,\"start\":57539},{\"end\":57548,\"start\":57547},{\"end\":57732,\"start\":57731},{\"end\":58011,\"start\":58010},{\"end\":58018,\"start\":58017},{\"end\":58297,\"start\":58296},{\"end\":58304,\"start\":58303},{\"end\":58511,\"start\":58510},{\"end\":58523,\"start\":58522},{\"end\":58715,\"start\":58714},{\"end\":58727,\"start\":58726},{\"end\":58729,\"start\":58728},{\"end\":58952,\"start\":58951},{\"end\":58962,\"start\":58961},{\"end\":59191,\"start\":59190},{\"end\":59200,\"start\":59199},{\"end\":59381,\"start\":59380},{\"end\":59383,\"start\":59382},{\"end\":59391,\"start\":59390},{\"end\":59604,\"start\":59603},{\"end\":59617,\"start\":59616},{\"end\":59888,\"start\":59887},{\"end\":59896,\"start\":59895},{\"end\":60155,\"start\":60154},{\"end\":60165,\"start\":60164},{\"end\":60395,\"start\":60394},{\"end\":60405,\"start\":60404},{\"end\":60640,\"start\":60639},{\"end\":60642,\"start\":60641},{\"end\":60652,\"start\":60651},{\"end\":60888,\"start\":60887},{\"end\":60897,\"start\":60896},{\"end\":61076,\"start\":61075},{\"end\":61086,\"start\":61085},{\"end\":61287,\"start\":61286},{\"end\":61298,\"start\":61297},{\"end\":61610,\"start\":61609},{\"end\":61612,\"start\":61611},{\"end\":61625,\"start\":61624},{\"end\":61888,\"start\":61887},{\"end\":61897,\"start\":61896},{\"end\":62153,\"start\":62152},{\"end\":62165,\"start\":62164},{\"end\":62421,\"start\":62420},{\"end\":62425,\"start\":62422},{\"end\":62436,\"start\":62435},{\"end\":62438,\"start\":62437},{\"end\":62688,\"start\":62687},{\"end\":62832,\"start\":62831},{\"end\":62976,\"start\":62970},{\"end\":63165,\"start\":63164},{\"end\":63176,\"start\":63175},{\"end\":63407,\"start\":63406},{\"end\":63419,\"start\":63418},{\"end\":63636,\"start\":63630},{\"end\":63865,\"start\":63864},{\"end\":63874,\"start\":63873}]", "bib_author_last_name": "[{\"end\":47889,\"start\":47886},{\"end\":47897,\"start\":47893},{\"end\":48165,\"start\":48158},{\"end\":48176,\"start\":48169},{\"end\":48438,\"start\":48435},{\"end\":48446,\"start\":48442},{\"end\":48454,\"start\":48450},{\"end\":48461,\"start\":48458},{\"end\":48468,\"start\":48465},{\"end\":48475,\"start\":48472},{\"end\":48859,\"start\":48856},{\"end\":48868,\"start\":48863},{\"end\":48874,\"start\":48872},{\"end\":48881,\"start\":48878},{\"end\":48890,\"start\":48885},{\"end\":48897,\"start\":48894},{\"end\":49298,\"start\":49294},{\"end\":49305,\"start\":49302},{\"end\":49312,\"start\":49309},{\"end\":49319,\"start\":49316},{\"end\":49326,\"start\":49323},{\"end\":49674,\"start\":49671},{\"end\":49682,\"start\":49678},{\"end\":49690,\"start\":49686},{\"end\":49697,\"start\":49694},{\"end\":49707,\"start\":49703},{\"end\":49713,\"start\":49711},{\"end\":50020,\"start\":50016},{\"end\":50029,\"start\":50024},{\"end\":50038,\"start\":50033},{\"end\":50044,\"start\":50042},{\"end\":50329,\"start\":50326},{\"end\":50335,\"start\":50333},{\"end\":50631,\"start\":50626},{\"end\":50637,\"start\":50635},{\"end\":50953,\"start\":50951},{\"end\":50960,\"start\":50957},{\"end\":51312,\"start\":51307},{\"end\":51318,\"start\":51316},{\"end\":51611,\"start\":51603},{\"end\":51620,\"start\":51617},{\"end\":51960,\"start\":51957},{\"end\":51967,\"start\":51964},{\"end\":52305,\"start\":52293},{\"end\":52319,\"start\":52309},{\"end\":52582,\"start\":52577},{\"end\":52597,\"start\":52586},{\"end\":52875,\"start\":52870},{\"end\":52883,\"start\":52879},{\"end\":53098,\"start\":53093},{\"end\":53106,\"start\":53102},{\"end\":53335,\"start\":53329},{\"end\":53349,\"start\":53339},{\"end\":53577,\"start\":53573},{\"end\":53768,\"start\":53762},{\"end\":53781,\"start\":53772},{\"end\":53976,\"start\":53971},{\"end\":53985,\"start\":53980},{\"end\":54201,\"start\":54195},{\"end\":54211,\"start\":54205},{\"end\":54457,\"start\":54453},{\"end\":54471,\"start\":54461},{\"end\":54697,\"start\":54690},{\"end\":54707,\"start\":54701},{\"end\":55073,\"start\":55070},{\"end\":55080,\"start\":55077},{\"end\":55337,\"start\":55333},{\"end\":55344,\"start\":55341},{\"end\":55636,\"start\":55634},{\"end\":55644,\"start\":55640},{\"end\":55851,\"start\":55848},{\"end\":55859,\"start\":55855},{\"end\":56120,\"start\":56113},{\"end\":56131,\"start\":56124},{\"end\":56402,\"start\":56399},{\"end\":56408,\"start\":56406},{\"end\":56749,\"start\":56746},{\"end\":56757,\"start\":56753},{\"end\":57018,\"start\":57013},{\"end\":57024,\"start\":57022},{\"end\":57270,\"start\":57266},{\"end\":57278,\"start\":57274},{\"end\":57545,\"start\":57541},{\"end\":57553,\"start\":57549},{\"end\":57739,\"start\":57733},{\"end\":58015,\"start\":58012},{\"end\":58021,\"start\":58019},{\"end\":58301,\"start\":58298},{\"end\":58309,\"start\":58305},{\"end\":58520,\"start\":58512},{\"end\":58530,\"start\":58524},{\"end\":58724,\"start\":58716},{\"end\":58738,\"start\":58730},{\"end\":58959,\"start\":58953},{\"end\":58971,\"start\":58963},{\"end\":59197,\"start\":59192},{\"end\":59208,\"start\":59201},{\"end\":59388,\"start\":59384},{\"end\":59401,\"start\":59392},{\"end\":59614,\"start\":59605},{\"end\":59623,\"start\":59618},{\"end\":59893,\"start\":59889},{\"end\":59904,\"start\":59897},{\"end\":60162,\"start\":60156},{\"end\":60174,\"start\":60166},{\"end\":60402,\"start\":60396},{\"end\":60414,\"start\":60406},{\"end\":60649,\"start\":60643},{\"end\":60662,\"start\":60653},{\"end\":60894,\"start\":60889},{\"end\":60903,\"start\":60898},{\"end\":61083,\"start\":61077},{\"end\":61092,\"start\":61087},{\"end\":61295,\"start\":61288},{\"end\":61308,\"start\":61299},{\"end\":61622,\"start\":61613},{\"end\":61629,\"start\":61626},{\"end\":61894,\"start\":61889},{\"end\":61907,\"start\":61898},{\"end\":62162,\"start\":62154},{\"end\":62170,\"start\":62166},{\"end\":62433,\"start\":62426},{\"end\":62447,\"start\":62439},{\"end\":62993,\"start\":62977},{\"end\":63173,\"start\":63166},{\"end\":63182,\"start\":63177},{\"end\":63416,\"start\":63408},{\"end\":63425,\"start\":63420},{\"end\":63653,\"start\":63637},{\"end\":63871,\"start\":63866},{\"end\":63881,\"start\":63875}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":251951787},\"end\":48127,\"start\":47827},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":13756489},\"end\":48358,\"start\":48129},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":206769320},\"end\":48775,\"start\":48360},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":232307921},\"end\":49200,\"start\":48777},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":260432782},\"end\":49609,\"start\":49202},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":51615798},\"end\":49939,\"start\":49611},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":254093711},\"end\":50244,\"start\":49941},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":244666399},\"end\":50559,\"start\":50246},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":229678744},\"end\":50897,\"start\":50561},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":59413863},\"end\":51233,\"start\":50899},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":231963451},\"end\":51551,\"start\":51235},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":231718848},\"end\":51880,\"start\":51553},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":232352874},\"end\":52225,\"start\":51882},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":119207848},\"end\":52503,\"start\":52227},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":245424762},\"end\":52800,\"start\":52505},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":204836822},\"end\":53040,\"start\":52802},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":227254333},\"end\":53251,\"start\":53042},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":249276257},\"end\":53521,\"start\":53253},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":92357636},\"end\":53723,\"start\":53523},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":2514901},\"end\":53914,\"start\":53725},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":251132664},\"end\":54133,\"start\":53916},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":252323115},\"end\":54371,\"start\":54135},{\"attributes\":{\"id\":\"b22\"},\"end\":54629,\"start\":54373},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":234479192},\"end\":54990,\"start\":54631},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":36142268},\"end\":55268,\"start\":54992},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":246816683},\"end\":55566,\"start\":55270},{\"attributes\":{\"doi\":\"arxiv:2205.05625\",\"id\":\"b26\"},\"end\":55805,\"start\":55568},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":247446635},\"end\":56044,\"start\":55807},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":3908336},\"end\":56315,\"start\":56046},{\"attributes\":{\"id\":\"b29\"},\"end\":56682,\"start\":56317},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":237960244},\"end\":56965,\"start\":56684},{\"attributes\":{\"id\":\"b31\"},\"end\":57262,\"start\":56967},{\"attributes\":{\"doi\":\"arXiv:2109.01840\",\"id\":\"b32\"},\"end\":57498,\"start\":57264},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":53642483},\"end\":57694,\"start\":57500},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":220055855},\"end\":57924,\"start\":57696},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":247958338},\"end\":58230,\"start\":57926},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":52957715},\"end\":58482,\"start\":58232},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":25062002},\"end\":58654,\"start\":58484},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":51680972},\"end\":58897,\"start\":58656},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":73432134},\"end\":59150,\"start\":58899},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":5589954},\"end\":59351,\"start\":59152},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":119198869},\"end\":59542,\"start\":59353},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":189999815},\"end\":59820,\"start\":59544},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":220830859},\"end\":60085,\"start\":59822},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":247222732},\"end\":60338,\"start\":60087},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":220301669},\"end\":60568,\"start\":60340},{\"attributes\":{\"id\":\"b46\"},\"end\":60814,\"start\":60570},{\"attributes\":{\"id\":\"b47\"},\"end\":61047,\"start\":60816},{\"attributes\":{\"id\":\"b48\"},\"end\":61192,\"start\":61049},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":4390182},\"end\":61498,\"start\":61194},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":221585935},\"end\":61839,\"start\":61500},{\"attributes\":{\"doi\":\"arXiv:1411.4028\",\"id\":\"b51\"},\"end\":62056,\"start\":61841},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":85518851},\"end\":62347,\"start\":62058},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":202539236},\"end\":62666,\"start\":62349},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":2556329},\"end\":62783,\"start\":62668},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":44098998},\"end\":62938,\"start\":62785},{\"attributes\":{\"id\":\"b56\"},\"end\":63138,\"start\":62940},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":1096490},\"end\":63333,\"start\":63140},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":7058660},\"end\":63597,\"start\":63335},{\"attributes\":{\"id\":\"b59\"},\"end\":63805,\"start\":63599},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":14542261},\"end\":64061,\"start\":63807}]", "bib_title": "[{\"end\":47882,\"start\":47827},{\"end\":48154,\"start\":48129},{\"end\":48431,\"start\":48360},{\"end\":48852,\"start\":48777},{\"end\":49290,\"start\":49202},{\"end\":49667,\"start\":49611},{\"end\":50012,\"start\":49941},{\"end\":50322,\"start\":50246},{\"end\":50622,\"start\":50561},{\"end\":50947,\"start\":50899},{\"end\":51303,\"start\":51235},{\"end\":51599,\"start\":51553},{\"end\":51953,\"start\":51882},{\"end\":52289,\"start\":52227},{\"end\":52573,\"start\":52505},{\"end\":52866,\"start\":52802},{\"end\":53087,\"start\":53042},{\"end\":53323,\"start\":53253},{\"end\":53567,\"start\":53523},{\"end\":53756,\"start\":53725},{\"end\":53965,\"start\":53916},{\"end\":54191,\"start\":54135},{\"end\":54686,\"start\":54631},{\"end\":55066,\"start\":54992},{\"end\":55329,\"start\":55270},{\"end\":55844,\"start\":55807},{\"end\":56109,\"start\":56046},{\"end\":56395,\"start\":56317},{\"end\":56742,\"start\":56684},{\"end\":57009,\"start\":56967},{\"end\":57537,\"start\":57500},{\"end\":57729,\"start\":57696},{\"end\":58008,\"start\":57926},{\"end\":58294,\"start\":58232},{\"end\":58508,\"start\":58484},{\"end\":58712,\"start\":58656},{\"end\":58949,\"start\":58899},{\"end\":59188,\"start\":59152},{\"end\":59378,\"start\":59353},{\"end\":59601,\"start\":59544},{\"end\":59885,\"start\":59822},{\"end\":60152,\"start\":60087},{\"end\":60392,\"start\":60340},{\"end\":61284,\"start\":61194},{\"end\":61607,\"start\":61500},{\"end\":62150,\"start\":62058},{\"end\":62418,\"start\":62349},{\"end\":62685,\"start\":62668},{\"end\":62829,\"start\":62785},{\"end\":63162,\"start\":63140},{\"end\":63404,\"start\":63335},{\"end\":63862,\"start\":63807}]", "bib_author": "[{\"end\":47891,\"start\":47884},{\"end\":47899,\"start\":47891},{\"end\":48167,\"start\":48156},{\"end\":48178,\"start\":48167},{\"end\":48440,\"start\":48433},{\"end\":48448,\"start\":48440},{\"end\":48456,\"start\":48448},{\"end\":48463,\"start\":48456},{\"end\":48470,\"start\":48463},{\"end\":48477,\"start\":48470},{\"end\":48861,\"start\":48854},{\"end\":48870,\"start\":48861},{\"end\":48876,\"start\":48870},{\"end\":48883,\"start\":48876},{\"end\":48892,\"start\":48883},{\"end\":48899,\"start\":48892},{\"end\":49300,\"start\":49292},{\"end\":49307,\"start\":49300},{\"end\":49314,\"start\":49307},{\"end\":49321,\"start\":49314},{\"end\":49328,\"start\":49321},{\"end\":49676,\"start\":49669},{\"end\":49684,\"start\":49676},{\"end\":49692,\"start\":49684},{\"end\":49699,\"start\":49692},{\"end\":49709,\"start\":49699},{\"end\":49715,\"start\":49709},{\"end\":50022,\"start\":50014},{\"end\":50031,\"start\":50022},{\"end\":50040,\"start\":50031},{\"end\":50046,\"start\":50040},{\"end\":50331,\"start\":50324},{\"end\":50337,\"start\":50331},{\"end\":50633,\"start\":50624},{\"end\":50639,\"start\":50633},{\"end\":50955,\"start\":50949},{\"end\":50962,\"start\":50955},{\"end\":51314,\"start\":51305},{\"end\":51320,\"start\":51314},{\"end\":51613,\"start\":51601},{\"end\":51622,\"start\":51613},{\"end\":51962,\"start\":51955},{\"end\":51969,\"start\":51962},{\"end\":52307,\"start\":52291},{\"end\":52321,\"start\":52307},{\"end\":52584,\"start\":52575},{\"end\":52599,\"start\":52584},{\"end\":52877,\"start\":52868},{\"end\":52885,\"start\":52877},{\"end\":53100,\"start\":53089},{\"end\":53108,\"start\":53100},{\"end\":53337,\"start\":53325},{\"end\":53351,\"start\":53337},{\"end\":53579,\"start\":53569},{\"end\":53770,\"start\":53758},{\"end\":53783,\"start\":53770},{\"end\":53978,\"start\":53967},{\"end\":53987,\"start\":53978},{\"end\":54203,\"start\":54193},{\"end\":54213,\"start\":54203},{\"end\":54459,\"start\":54449},{\"end\":54473,\"start\":54459},{\"end\":54699,\"start\":54688},{\"end\":54709,\"start\":54699},{\"end\":55075,\"start\":55068},{\"end\":55082,\"start\":55075},{\"end\":55339,\"start\":55331},{\"end\":55346,\"start\":55339},{\"end\":55638,\"start\":55632},{\"end\":55646,\"start\":55638},{\"end\":55853,\"start\":55846},{\"end\":55861,\"start\":55853},{\"end\":56122,\"start\":56111},{\"end\":56133,\"start\":56122},{\"end\":56404,\"start\":56397},{\"end\":56410,\"start\":56404},{\"end\":56751,\"start\":56744},{\"end\":56759,\"start\":56751},{\"end\":57020,\"start\":57011},{\"end\":57026,\"start\":57020},{\"end\":57272,\"start\":57264},{\"end\":57280,\"start\":57272},{\"end\":57547,\"start\":57539},{\"end\":57555,\"start\":57547},{\"end\":57741,\"start\":57731},{\"end\":58017,\"start\":58010},{\"end\":58023,\"start\":58017},{\"end\":58303,\"start\":58296},{\"end\":58311,\"start\":58303},{\"end\":58522,\"start\":58510},{\"end\":58532,\"start\":58522},{\"end\":58726,\"start\":58714},{\"end\":58740,\"start\":58726},{\"end\":58961,\"start\":58951},{\"end\":58973,\"start\":58961},{\"end\":59199,\"start\":59190},{\"end\":59210,\"start\":59199},{\"end\":59390,\"start\":59380},{\"end\":59403,\"start\":59390},{\"end\":59616,\"start\":59603},{\"end\":59625,\"start\":59616},{\"end\":59895,\"start\":59887},{\"end\":59906,\"start\":59895},{\"end\":60164,\"start\":60154},{\"end\":60176,\"start\":60164},{\"end\":60404,\"start\":60394},{\"end\":60416,\"start\":60404},{\"end\":60651,\"start\":60639},{\"end\":60664,\"start\":60651},{\"end\":60896,\"start\":60887},{\"end\":60905,\"start\":60896},{\"end\":61085,\"start\":61075},{\"end\":61094,\"start\":61085},{\"end\":61297,\"start\":61286},{\"end\":61310,\"start\":61297},{\"end\":61624,\"start\":61609},{\"end\":61631,\"start\":61624},{\"end\":61896,\"start\":61887},{\"end\":61909,\"start\":61896},{\"end\":62164,\"start\":62152},{\"end\":62172,\"start\":62164},{\"end\":62435,\"start\":62420},{\"end\":62449,\"start\":62435},{\"end\":62691,\"start\":62687},{\"end\":62835,\"start\":62831},{\"end\":62995,\"start\":62970},{\"end\":63175,\"start\":63164},{\"end\":63184,\"start\":63175},{\"end\":63418,\"start\":63406},{\"end\":63427,\"start\":63418},{\"end\":63655,\"start\":63630},{\"end\":63873,\"start\":63864},{\"end\":63883,\"start\":63873}]", "bib_venue": "[{\"end\":47961,\"start\":47899},{\"end\":48227,\"start\":48178},{\"end\":48539,\"start\":48477},{\"end\":48961,\"start\":48899},{\"end\":49390,\"start\":49328},{\"end\":49747,\"start\":49715},{\"end\":50078,\"start\":50046},{\"end\":50388,\"start\":50337},{\"end\":50701,\"start\":50639},{\"end\":51023,\"start\":50962},{\"end\":51377,\"start\":51320},{\"end\":51696,\"start\":51622},{\"end\":52033,\"start\":51969},{\"end\":52338,\"start\":52321},{\"end\":52622,\"start\":52599},{\"end\":52891,\"start\":52885},{\"end\":53115,\"start\":53108},{\"end\":53357,\"start\":53351},{\"end\":53602,\"start\":53579},{\"end\":53789,\"start\":53783},{\"end\":53993,\"start\":53987},{\"end\":54241,\"start\":54213},{\"end\":54447,\"start\":54373},{\"end\":54792,\"start\":54709},{\"end\":55111,\"start\":55082},{\"end\":55400,\"start\":55346},{\"end\":55630,\"start\":55568},{\"end\":55913,\"start\":55861},{\"end\":56154,\"start\":56133},{\"end\":56486,\"start\":56410},{\"end\":56810,\"start\":56759},{\"end\":57102,\"start\":57026},{\"end\":57357,\"start\":57296},{\"end\":57569,\"start\":57555},{\"end\":57790,\"start\":57741},{\"end\":58051,\"start\":58023},{\"end\":58332,\"start\":58311},{\"end\":58538,\"start\":58532},{\"end\":58746,\"start\":58740},{\"end\":58996,\"start\":58973},{\"end\":59224,\"start\":59210},{\"end\":59420,\"start\":59403},{\"end\":59655,\"start\":59625},{\"end\":59927,\"start\":59906},{\"end\":60187,\"start\":60176},{\"end\":60427,\"start\":60416},{\"end\":60637,\"start\":60570},{\"end\":60885,\"start\":60816},{\"end\":61073,\"start\":61049},{\"end\":61316,\"start\":61310},{\"end\":61642,\"start\":61631},{\"end\":61885,\"start\":61841},{\"end\":62182,\"start\":62172},{\"end\":62479,\"start\":62449},{\"end\":62697,\"start\":62691},{\"end\":62842,\"start\":62835},{\"end\":62968,\"start\":62940},{\"end\":63207,\"start\":63184},{\"end\":63439,\"start\":63427},{\"end\":63628,\"start\":63599},{\"end\":63906,\"start\":63883},{\"end\":51071,\"start\":51025}]"}}}, "year": 2023, "month": 12, "day": 17}