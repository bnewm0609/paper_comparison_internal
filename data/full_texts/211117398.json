{"id": 211117398, "updated": "2023-10-06 18:38:51.548", "metadata": {"title": "Mining Discriminative Food Regions for Accurate Food Recognition", "authors": "[{\"first\":\"Jianing\",\"last\":\"Qiu\",\"middle\":[]},{\"first\":\"Frank\",\"last\":\"Lo\",\"middle\":[\"P.-W.\"]},{\"first\":\"Yingnan\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Siyao\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Benny\",\"last\":\"Lo\",\"middle\":[]}]", "venue": "ArXiv", "journal": "158", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Automatic food recognition is the very first step towards passive dietary monitoring. In this paper, we address the problem of food recognition by mining discriminative food regions. Taking inspiration from Adversarial Erasing, a strategy that progressively discovers discriminative object regions for weakly supervised semantic segmentation, we propose a novel network architecture in which a primary network maintains the base accuracy of classifying an input image, an auxiliary network adversarially mines discriminative food regions, and a region network classifies the resulting mined regions. The global (the original input image) and the local (the mined regions) representations are then integrated for the final prediction. The proposed architecture denoted as PAR-Net is end-to-end trainable, and highlights discriminative regions in an online fashion. In addition, we introduce a new fine-grained food dataset named as Sushi-50, which consists of 50 different sushi categories. Extensive experiments have been conducted to evaluate the proposed approach. On three food datasets chosen (Food-101, Vireo-172, and Sushi-50), our approach performs consistently and achieves state-of-the-art results (top-1 testing accuracy of $90.4\\%$, $90.2\\%$, $92.0\\%$, respectively) compared with other existing approaches. Dataset and code are available at https://github.com/Jianing-Qiu/PARNet", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2207.03692", "mag": "3021798166", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2207-03692", "doi": "10.48550/arxiv.2207.03692"}}, "content": {"source": {"pdf_hash": "b1bf58d7e0857a25f1ace0fbd6944a112ba2287e", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2207.03692v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "1122a29b1f386142935c3eb68274794435c67d2b", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/b1bf58d7e0857a25f1ace0fbd6944a112ba2287e.txt", "contents": "\nMining Discriminative Food Regions for Accurate Food Recognition\n\n\nJianing Qiu jianing.qiu17@imperial.ac.uk \nThe Hamlyn Centre Imperial College London London\nUK\n\nDepartment of Computing\nImperial College London London\nUK\n\nP.-W Frank \nLo \nThe Hamlyn Centre Imperial College London London\nUK\n\nYingnan Sun \nThe Hamlyn Centre Imperial College London London\nUK\n\nDepartment of Computing\nImperial College London London\nUK\n\nSiyao Wang s.wang18@imperial.ac.uk \nThe Hamlyn Centre Imperial College London London\nUK\n\nBenny Lo benny.lo@imperial.ac.uk \nThe Hamlyn Centre Imperial College London London\nUK\n\nMining Discriminative Food Regions for Accurate Food Recognition\nQIU ET AL.: MINING DISCRIMINATIVE FOOD REGIONS 1\nAutomatic food recognition is the very first step towards passive dietary monitoring. In this paper, we address the problem of food recognition by mining discriminative food regions. Taking inspiration from Adversarial Erasing, a strategy that progressively discovers discriminative object regions for weakly supervised semantic segmentation, we propose a novel network architecture in which a primary network maintains the base accuracy of classifying an input image, an auxiliary network adversarially mines discriminative food regions, and a region network classifies the resulting mined regions. The global (the original input image) and the local (the mined regions) representations are then integrated for the final prediction. The proposed architecture denoted as PAR-Net is end-to-end trainable, and highlights discriminative regions in an online fashion. In addition, we introduce a new fine-grained food dataset named as Sushi-50, which consists of 50 different sushi categories. Extensive experiments have been conducted to evaluate the proposed approach. On three food datasets chosen , our approach performs consistently and achieves state-of-the-art results (top-1 testing accuracy of 90.4%, 90.2%, 92.0%, respectively) compared with other existing approaches. Dataset and code are available at https://github.com/Jianing-Qiu/PARNet\n\nIntroduction\n\nDiet-induced diseases are becoming increasingly prevalent among populations. One underlying factor is people's poor management of their daily dietary intake. The other factor is that there is currently no accurate measurement of dietary intake. Dietary measurement in nutritional epidemiology is heavily based on self-reported data that are highly inaccurate and subjective [15], which hinders nutritionists from designing effective dietary guidance. To mitigate the problem of existing dietary measurement techniques that require extensive user input and produce unsatisfactory results, the concept of passive dietary monitoring is proposed [12], which relies on sensors such as cameras to pervasively record eating episodes and automatically perform food recognition, volume estimation, and deduce dietary intake. In realising this concept of passive monitoring, food recognition is the first and a crucial step as any misrecognition will lead to inaccurate measurements afterwards. With recent advances in computer vision, recognising pictured dishes have achieved promising results but still remains as a challenging field of research given that there are enormous varieties of dishes and even the same type of food can have very different appearances. In this work, we aim to achieve accurate food recognition by mining discriminative regions of a food image. This is motivated by the previous work done by Bossard et al. [3] that utilises random forests to mine discriminative components from food images. Unlike [3], we develop a convolutional neural network (CNN) model and utilise a weakly supervised method to discover discriminative food regions. This weakly supervised method used in both network training and inference is based on Adversarial Erasing (AE) [19], a strategy developed for weakly supervised semantic segmentation. One prominent feature of AE is that it enables the discriminative region of an object of interest to be discovered progressively, which in our case enables better recognition of food items. Our implementation of AE however differs from [19] in that we integrate it into a new network architecture for object recognition (food recognition in particular), and all sub-networks involved are trained jointly, which is more convenient, compared to its original usage in which networks need to be trained independently. Although region mining is performed, the proposed approach still predicts the food class of an input image efficiently in an end-to-end manner, which will be detailed in Sections 3.1 and 3.2.\n\nThe contributions of our work are twofold: (i) we propose a new network architecture that is able to mine discriminative food regions in a weakly supervised fashion and be trained endto-end. The mining strategy is adopted and optimised for food recognition. Comprehensive experiments are performed to validate the proposed approach; (ii) we introduce a new finegrained food dataset which consists of 50 sub-categories of one common food class, i.e., sushi, in contrast to most existing datasets that only contain coarse food classes.\n\n\nRelated Work\n\nFood Recognition. Existing vision-based approaches for food recognition either use handcrafted features in conjunction with SVMs, or use CNNs alone. Works in the former such as [2,5] resort to color and SIFT based features for food image classification. Pairwise local features are developed in [21] to capture spatial relationships between the ingredients. In [3], food recognition is decomposed into two steps: first scoring an image's superpixels using the component models trained by random forest mined food parts, and then predicting the food class using a multi-class SVM. The results of these approaches are not satisfactory as neither the learned representations nor the models are capable of distinguishing food items with high intra-class variations. Among the latter CNN-based approaches, Chen and Ngo [4] designed a series of CNN architectures and utilised multi-task learning to simultaneously recognise the food category and ingredients composition. Based on the observation that certain food dishes have a clear vertical layer structure, Martinel et al. [13] later proposed a slice convolutional layer to learn such information. Integrated with the wide residual network [23], the resulting architecture shows promising results on food recognition. Some previous works [1,2,14] also tried to narrow down the number of food categories by using the restaurant context. Food recognition recently has also been proposed as a topic of challenges in the fine-grained visual categorization competition [6,22]. Adversarial Erasing.\n\nAdversarial Erasing (AE) is initially introduced by Wei et al. [19] aiming to address the weakly supervised semantic segmentation problem. It is an iterative process that enforces a succeeding classification network to discover a new discriminative region from an image with the more discriminative regions removed by the previous networks. Each discriminative region is obtained by thresholding the associated class activation map (CAM) [25], which is a visualisation map that highlights the areas in a given image that a classification network relies on for identifying the target class. By merging these mined regions, the object of interest can then be well segmented. To accomplish this, a sequence of networks were needed, and each of them was trained independently. Zhang et al. [24] later employed AE for weakly supervised object localisation, and trained two complementary classifiers jointly with localisation maps being online inferred. AE is adopted in this work to enhance food recognition. To this end, a completely different model from the ones used by the above two approaches is developed. In addition, we calculate CAM in an online manner to enable the end-to-end training. There are several other works sharing the similar idea with AE. For example, Singh and Lee [17] proposed to hide patches randomly in an input image to boost weakly supervised object localisation. For the same purpose, Kim et al. [10] adopted a two-phase learning strategy of suppressing the second network's activation conditioned on the outcome of the first network.\n\n\nMethod\n\nWe use AE to progressively mine discriminative regions from an input food image. The representations of the mined regions and that of the original input image are concatenated as the final representation for food recognition. Our network architecture denoted as PAR-Net has three sub-networks (i.e., a primary network, an auxiliary network and a region network) and an extra fully connected layer. We use CAM in an online fashion to highlight the discriminative region at each mining step, which is described in Section 3.1. The network architecture as well as the training and inference procedures are detailed in Section 3.2.\n\n\nCalculating CAM\n\nTo calculate CAM [25], a classification network normally follows the structure of convgap-fc-softmax on top. The generic way of producing CAM thus is defined as in Eqn. 1.\nCAM(I, c) = N \u2211 k=1 w k,c \u00b7 F k(1)\nwhere c denotes the target class of an image I; F k is the k th feature map (N in total) of the last convolutional layer; w k,c is the weight in the fully connected layer that represents the contribution of the k th neuron of the global average pooling (GAP) layer makes for identifying the class c for I. We integrate CAM into both network training and inference, and calculate it differently during these two procedures. Specifically, in training, c is always adopted as the ground truth label associated with the image I. Whereas in inference, c is the predicted top-1 class. Figure 1: Overview of the network structure. The proposed PAR-Net consists of three subnetworks and an extra fully connected layer. The final prediction of an input food image I i is based on the concatenated representations of the input full image and the mined discriminative regions. Note that the last discriminative-region-erased image I i,T does not need to be calculated as its classification loss is never accumulated during training.\n\n\nPAR-Net\n\nThe network structure consists of four major parts: (i) a primary network (P-Net) which classifies the original full input image; (ii) an auxiliary network (A-Net) which classifies images with discriminative region(s) erased; (iii) a region network (R-Net) which classifies the cropped and upsampled discriminative regions; (iv) an extra fully connected layer which classifies the concatenated representations of the input full image and the mined regions. An overview of the network architecture is shown in Figure 1. The training and inference procedures are designed as follows: Training. We adopt the standard cross-entropy loss for all the classification involved during training. Concretely, given an image-label pair {I i , y i } as the input, and the number of times of region mining T , we first feed I i into the P-Net for classification and denote the resulting loss as L p . In the meantime, the corresponding class activation map CAM(I i , y i ) is calculated based on Eqn. 1, and upsampled to obtain a heatmap indicating the discriminative region. In the following, we use M i,t , R i,t and I i,t to denote the CAM-based heatmap, the mined region, and the image with the discriminative region(s) erased at mining step t (t \u2264 T ). All the R i,t and I i,t inherit the ground truth label y i from the input image I i .\n\nTherefore, after upsampling CAM(I i , y i ) to the same size as I i , the first heatmap M i,1 is obtained. We threshold M i,1 to keep the values that are above \u03b1 \u00d7 max(M i,1 ), and then find the connected components. For each connected component, we sum all values inside. The one with the largest sum is used to indicate the most discriminative region in I i . This operation for finding the discriminative region is slightly different from the original CAM implementation for object localisation [25] since we are more interested in the discriminativeness than the localisation accuracy. We denote the most discriminative region highlighted by M i,1 in I i as R i,1 , which could be of any shape. A tight bounding box covering R i,1 is then used to crop it out. We upsample the cropped patch using bilinear interpolation to the same size as I i , and feed it into the R-Net for recognition. Simultaneously, we replace the pixels inside R i,1 in I i with zeros (as the first erasing operation), resulting in the first discriminativeregion-erased image I i,1 which is then fed into the A-Net for classification. Note that the P-Net is only responsible for classifying the original full image and finding the first discriminative region. The remaining regions are mined by the A-Net. This is mainly to ensure that the accuracy of classifying a full input image can be preserved such that the input image's extracted representation is of high discriminativeness.\n\nIf T > 1, the A-Net then continues to discover the next discriminative region by calculating CAM(I i,t , y i ),t \u2208 {1, ..., T \u2212 1} while classifying I i,t (the A-Net classifies I i,t in an adversarial manner as t increases, the I i,t will have less discriminative regions left for the A-Net to rely on for identifying the correct class), and the R-Net keeps recognising the new discriminative region R i,t ,t \u2208 {2, ..., T } fed from the A-Net. It is worth noting that R i,t+1 is always cropped from the original input image I i based on M i,t+1 and the same ratio \u03b1 for calculating the threshold \u03b1 \u00d7 max(M i,t+1 ), and I i,t+1 is always obtained by erasing R i,t+1 from I i,t . For each classification of the A-Net and the R-Net at each mining step t, we denote their losses as L a,t and L r,t , respectively. We extract the representation of the input image I i and each region R i,t ,t \u2208 {1, ..., T } from the GAP layer of the P-Net and the R-Net, respectively. These representations are concatenated and input into the additional fully connected layer for classification. We use y i to calculate the loss, and denote it as L concat . Thus, depending on the number of times of region mining T , the total loss of the whole model is defined as in Eqn. 2.\nL = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 L p , if T = 0 L p + L r,1 + L concat , if T = 1 L p + \u2211 T t=1 L r,t + L concat + \u2211 T \u22121 t=1 L a,t , if T > 1(2)\nNotice that when T = 0, the overall loss L is equivalent to the loss L p (i.e., classify the input image I i only by the P-Net), and when T = 1, the A-Net's loss L a,1 is not counted. As a matter of fact, loss L a,t is accumulated only if a discriminative region is mined from I i,t . The PAR-Net as a whole is then trained end-to-end. Inference. In inference, since only an image I i is provided as the input, the calculation of CAM is different from that in training as mentioned in Section 3.1. The other parts however remain the same. To be more specific, when T \u2265 1, the P-Net still mines the first discriminative region but instead uses CAM(I i , c i ) where c i is the predicted top-1 class of the input image I i by the P-Net, and the A-Net mines the rest discriminative regions with CAM(I i,t , c i,t ),t \u2208 {1, ..., T \u2212 1} where c i,t is the estimated top-1 class of the image I i,t by the A-Net. All mined regions are fed into the R-Net to obtain their representations. The output of the additional fully connected layer is used as the final prediction\u0177 i for the input food image I i .\n\n\nExperiments\n\nIn this section, we first introduce the datasets used to validate the proposed approach, and then describe the implementation details. The results are then presented followed by the \n\n\nDatasets\n\nThe proposed method was evaluated on the following three food datasets: Food-101. Food-101 [3] contains 101 food classes and each class has 1,000 images. The whole dataset is split into 75,750 images for training and 25,250 images for testing. Vireo-172. Vireo-172 [4] is a large scale Chinese food dataset with 172 food classes and 110,241 food images in total. The dataset is divided into training, validation, and test sets. Each class assigns 60% of its images for training, 10% for validation, and the rest 30% for testing. Sushi-50. We built Sushi-50 1 as a fine-grained food dataset, which has 50 different sushi classes. These classes are selected based on a sushi guide 2 with those not having sufficient images excluded. All images were downloaded from google and duplicates were removed. This new dataset has 3,963 images. Each class has 50% of its images assigned for training and another 50% retained for testing. Figure 2 shows one sushi sample for each class and Figure 3 shows the number of images of each class.\n\n\nImplementation Details\n\nEach sub-network within the PAR-Net is based on the ResNet [9]. We attempt various combinations of the sub-networks by using ResNets with different depth. In training, a crop is randomly sampled from the original image and resized to 224 \u00d7 224 with scale and aspect ratio augmentation [18] as the input, or its horizontal flip. No any further data augmentation technique is used. In inference, we use the center crop (1-crop) of size 224 \u00d7 224 from the original image (resized to 256 \u00d7 256). Following the practice in [13], we also test our approach with the standard ten crops (10-crop). The PAR-Net is implemented with PyTorch. We initialise each sub-network with the weights of the corresponding ResNet model pretrained on ImageNet [7] and fine-tune all layers afterwards. We train the PAR-Net for up to 35 epochs using stochastic gradient descent (SGD) with a momentum of 0.9, and a minibatch size of 16. The initial learning rate is set to 10 \u22123 and divided by 10 every 7 epochs. A\n\n\nMethod\n\nFood-101 (Top-1) Vireo-172 (Top-1) Sushi-50 (Top-1) The PAR-Net is tested with different combinations of its subnets (e.g., P50 denotes the P-Net is based on ResNet-50). The best accuracy of both 1-crop and 10-crop is written in bold. All the PAR-Net versions reported mine 3 discriminative regions and use a ratio \u03b1 of 0.5. * was trained with a batch size of 8 for up to 60 epochs, and the learning rate was divided by 10 every 14 epochs.\n\n\nMethod\n\nTop-1 RFDC [3] 50.76 DCNN-FOOD [20] 70.41 DeepFood [11] 77.4 Inception V3 [8] 88.28 DLA (CVPR2018) [22] 90.0 WISeR (WACV2018) [13] 90.27 DSTL (CVPR2018) [6] 90.4\n\nPAR-Net (Ours) 90.4  Table 4: Comparison on Sushi-50 weight decay of 10 \u22124 is adopted. We set the ratio \u03b1 to 0.5 and the number of times of region mining T to 3. More detailed analysis about the choice of \u03b1 and T is given in Section 4.4.\n\n\nResults\n\nThe overall results on the test sets are shown in Table 1. The accuracy of the PAR-Net is the accuracy of classifying the concatenated representation. The PAR-Net with its P-Net and R-Net being two individual ResNet-50s and its A-Net being a ResNet-34 (represented as P50+A34+R50 in Table 1) improves the 1-crop testing accuracy by 1.4%, 1.8%, 2.3% on Food-101, Vireo-172, and Sushi-50, respectively, compared to the baseline results of using a single ResNet-50. Similar improvements can also be observed in the 10-crop testing. When the P-Net is instantiated with a ResNet-101, the accuracy on all the three datasets further increases. We achieve the best 10-crop accuracy on all the datasets when using ResNet-101 for each sub-network.\n\nWe then compare our approach with other state-of-the-arts. The result achieved by our approach on Food-101 (see Table 2) outperforms [13] in which more data augmentation techniques are used, and matches [6] whereas in [6] higher image resolution and a more sophisticated transfer learning approach are adopted. On Vireo-172, our approach achieves the current best accuracy (see Table 3), and on Sushi-50, compared to a baseline method, Figure 4: visualisation of the predicted results of the PAR-Net (P101+A101+R101) on the test sets. Two examples are shown for each food dataset. The CAM-based heatmap M i,t has been projected onto the image it is calculated from. Each discriminative region R i,t has been cropped using the bounding box (drawn as red in M i,t ) and upsampled to the same size as the input I i (refer to Section 3.2). The shaded area in M i,2 and M i,3 are the (accumulated) erased region(s). We use pink to indicate the incorrectly predicted class and green the correctly predicted as well as the ground truth class. Input Img. I i is classified by the P-Net and R i,t the R-Net. Concat. Pred. is the prediction based on the concatenated representation. the proposed approach yields better performance (see Table 4). We visualise some predicted results of the PAR-Net on the test sets (two examples for each dataset) in Figure 4. In the first four rows, the input food image is incorrectly recognised by the P-Net. However, the following mined discriminative regions are successfully classified by the R-Net. The final predication based on the concatenated representation therefore is correct. In the bottom two rows, the input as well as the mined regions are correctly recognised by the P-Net and the R-Net, respectively, which leads to the desired prediction after concatenating representations. Diversity of the mined discriminative regions within each image can also be observed in Figure 4, which is beneficial for the more accurate final prediction. The results obtained demonstrate the effectiveness of using mined discriminative regions to improve food recognition and show consistent performance of our method across multiple food datasets.  Table 5: Ablation analysis of the impact of the number of times of region mining T , the ratio \u03b1, and the different network structure on the accuracy (%) of food recognition.\n\nDataset PAR-Net (P50+A34+R50) PAR-Net (P101+A101+R101)  \nI R 1 R 2 R 3 I 1 I 2 concat I R 1 R 2 R 3 I 1 I 2 concat\n\nAblation Studies\n\nImpact of the number of times of region mining. To study how many regions should be mined so as to achieve the optimal result, we conducted experiments with the PAR-Net (P50+A34+R50) on Vireo-172 and Sushi-50. As shown in the left of Table 5, mining 3 discriminative regions in general is sufficient for achieving satisfactory performance. The accuracy drops when less regions are mined. Although the accuracy is slightly higher on Vireo-172 when the number of mined regions increases to 4, it inevitably requires more overhead in training and inference. Therefore, we adopt T = 3 for most of our experiments. Impact of the ratio used to calculate the threshold. We threshold a CAM-based heatmap by using a ratio \u03b1 multiplied with the maximum value of the heatmap, after which we use the connected component with the largest sum to indicate the most discriminative region. The \u03b1 therefore determines the size of the discriminative region, the smaller the \u03b1, the larger the size. We show the impact of the value of \u03b1 on the accuracy of food recognition in the middle of Table 5. The experiments are based on using the PAR-Net (P50+A34+R50) and a fixed T = 3. In general, \u03b1 = 0.5 works well on both Vireo-172 and Sushi-50. However, the results suggest that the choice of \u03b1 could be dataset-dependent as \u03b1 = 0.3 is only marginally worse than \u03b1 = 0.5 whereas \u03b1 = 0.7 causes a clear drop in the accuracy on Vireo-172. An opposite trend can be observed on Sushi-50. Impact of the network structure. So far, the accuracy reported of the PAR-Net is based on using three individual sub-networks. As both P-Net and R-Net learn food representations (global and local, respectively), an intuition is that they could share weights. Therefore, we used a single network (ResNet-50) to replace the P-Net and R-Net, and still kept the A-Net (ResNet-34). This modified structure was trained with the same setup as in Section 4.2. It can be observed from the right of Table 5 that this structure is of inferior performance compared to the original PAR-Net (P50+A34+R50). When we forced all three sub-networks to share weights, i.e., replacing them with a single ResNet-50, the accuracy further decreases. This can be interpreted as a limitation of the PAR-Net that it is necessary to have independent sub-networks to conduct different tasks in order to achieve good performance. Accuracy of each sub-network. As classification occurs in each sub-network (the P-Net classifies the input image, the R-Net classifies the mined regions, and the A-Net classifies discriminative-region-erased images), we show each sub-network's classification accuracy on the corresponding target, which is summarised in Table 6. As mining continues, the accuracy of classifying the mined region decreases as expected (R 1 > R 2 > R 3 ), because the later mined region is less discriminative than the earlier ones. This is also true for the images with discriminative region(s) erased, the accuracy degrades (I 1 > I 2 ) as the later image have less discriminative regions left compared to the previous one. It is worth noting that the accuracy based on the concatenated representation is always higher than that of the input image and the mined regions, which verifies that the integration of global and local representations contributes to better food recognition.\n\n\nConclusions\n\nWe introduced a novel model for food recognition, which progressively mines discriminative food regions and merges their representations with the full input image's to make an accurate prediction. The model has been validated on multiple food datasets including a new fine-grained one introduced by this paper, and yields state-of-the-art performance. By providing more accurate recognition results, the proposed model is expected to facilitate the development of passive dietary monitoring. Investigating vision-based approaches for food ingredient level analysis is planned in future work.\n\nFigure 2 :Figure 3 :\n23Samples of the Sushi-50 dataset (one sample is shown for each class) The number of images of each category of the Sushi-50 dataset comparison between our approach and other state-of-the-arts. The ablation studies are reported at the end.\n\n\nTop-1 testing accuracy (%) on the three food datasets. ResNet-50 and ResNet-101 are based on our test.1-crop 10-crop 1-crop 10-crop 1-crop 10-crop \n\nResNet-50 \n87.1 \n88.2 \n87.0 \n87.7 \n88.9 \n89.0 \nResNet-101 \n88.1 \n89.0 \n87.5 \n88.3 \n89.7 \n90.0 \nPAR-Net (P50+A34+R50) \n88.5 \n89.5 \n88.8 \n89.3 \n91.2 \n91.0 \nPAR-Net (P101+A34+R50) \n89.3 \n90.2 \n89.2 \n89.7 \n92.3 \n91.9 \nPAR-Net (P101+A101+R101)* \n89.3 \n90.4 \n89.6 \n90.2 \n91.8 \n92.0 \nTable 1: \n\nTable 2 :\n2Results compared with other state-ofthe-art methods on Food-101Method \nTop-1 \n\nVGG [16] \n80.41 \nArch-D (ACMMM2016) [4] 82.06 \n\nPAR-Net (Ours) \n90.2 \nTable 3: Comparison on Vireo-172 \n\nMethod \nTop-1 \n\nResNet-101 [9] \n90.0 \n\nPAR-Net (Ours) \n92.0 \n\n\nTable 6 :\n6Average top-1 testing accuracy (%) on the full input image (I), discriminative region R t and region-erased image I t of mining step t, and the concatenated representation. Two different PAR-Nets' performances are shown when mining with 3 discriminative regions.\nhttp://www.doc.ic.ac.uk/~jq916/Sushi-50.zip 2 https://www.japan-talk.com/jt/new/sushi-list\n\nMenumatch: Restaurant-specific food logging from images. Oscar Beijbom, Neel Joshi, Dan Morris, Scott Saponas, Siddharth Khullar, WACV. Oscar Beijbom, Neel Joshi, Dan Morris, Scott Saponas, and Siddharth Khullar. Menu- match: Restaurant-specific food logging from images. In WACV, 2015.\n\nLeveraging context to support automated food recognition in restaurants. Vinay Bettadapura, Edison Thomaz, Aman Parnami, D Gregory, Irfan Abowd, Essa, WACV. Vinay Bettadapura, Edison Thomaz, Aman Parnami, Gregory D Abowd, and Irfan Essa. Leveraging context to support automated food recognition in restaurants. In WACV, 2015.\n\nFood-101-mining discriminative components with random forests. Lukas Bossard, Matthieu Guillaumin, Luc Van Gool, ECCV. Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101-mining discrimi- native components with random forests. In ECCV, 2014.\n\nDeep-based ingredient recognition for cooking recipe retrieval. Jingjing Chen, Chong-Wah Ngo, ACM Multimedia. Jingjing Chen and Chong-Wah Ngo. Deep-based ingredient recognition for cooking recipe retrieval. In ACM Multimedia, 2016.\n\nPfid: Pittsburgh fast-food image dataset. Mei Chen, Kapil Dhingra, Wen Wu, Lei Yang, Rahul Sukthankar, Jie Yang, ICIP. Mei Chen, Kapil Dhingra, Wen Wu, Lei Yang, Rahul Sukthankar, and Jie Yang. Pfid: Pittsburgh fast-food image dataset. In ICIP, 2009.\n\nLarge scale fine-grained categorization and domain-specific transfer learning. Yin Cui, Yang Song, Chen Sun, Andrew Howard, Serge Belongie, CVPR. Yin Cui, Yang Song, Chen Sun, Andrew Howard, and Serge Belongie. Large scale fine-grained categorization and domain-specific transfer learning. In CVPR, 2018.\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, CVPR. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009.\n\nFood image recognition using very deep convolutional networks. Hamid Hassannejad, Guido Matrella, Paolo Ciampolini, Monica Ilaria De Munari, Stefano Mordonini, Cagnoni, MADiMa. ACMHamid Hassannejad, Guido Matrella, Paolo Ciampolini, Ilaria De Munari, Monica Mordonini, and Stefano Cagnoni. Food image recognition using very deep convolu- tional networks. In MADiMa. ACM, 2016.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.\n\nDonggeun Yoo, and In So Kweon. Two-phase learning for weakly supervised object localization. Dahun Kim, Donghyeon Cho, Dahun Kim, Donghyeon Cho, Donggeun Yoo, and In So Kweon. Two-phase learning for weakly supervised object localization. In ICCV, 2017.\n\nDeepfood: Deep learning-based food image recognition for computer-aided dietary assessment. Chang Liu, Yu Cao, Yan Luo, Guanling Chen, Vinod Vokkarane, Yunsheng Ma, ICOST. Chang Liu, Yu Cao, Yan Luo, Guanling Chen, Vinod Vokkarane, and Yunsheng Ma. Deepfood: Deep learning-based food image recognition for computer-aided dietary assessment. In ICOST, 2016.\n\nAn innovative passive dietary monitoring system. Benny Lo, Benny Lo. An innovative passive dietary monitoring system. http:// dietaryintake.org/index.php?id=1, 2017.\n\nWide-slice residual networks for food recognition. Niki Martinel, Gian Luca Foresti, Christian Micheloni, In WACV. Niki Martinel, Gian Luca Foresti, and Christian Micheloni. Wide-slice residual net- works for food recognition. In WACV, 2018.\n\nIm2calories: towards an automated mobile vision food diary. Austin Meyers, Nick Johnston, Vivek Rathod, Anoop Korattikara, Alex Gorban, Nathan Silberman, Sergio Guadarrama, George Papandreou, Jonathan Huang, Kevin P Murphy, ICCV. Austin Meyers, Nick Johnston, Vivek Rathod, Anoop Korattikara, Alex Gorban, Nathan Silberman, Sergio Guadarrama, George Papandreou, Jonathan Huang, and Kevin P Murphy. Im2calories: towards an automated mobile vision food diary. In ICCV, 2015.\n\nDietary assessment methods in epidemiologic studies. Jee-Seon, Kyungwon Shim, Hyeon Chang Oh, Kim, Epidemiology and health. 36Jee-Seon Shim, Kyungwon Oh, and Hyeon Chang Kim. Dietary assessment methods in epidemiologic studies. Epidemiology and health, 36, 2014.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, ICLR. K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015.\n\nHide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization. Krishna Kumar Singh, Yong Jae Lee, Krishna Kumar Singh and Yong Jae Lee. Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization. In ICCV, 2017.\n\nGoing deeper with convolutions. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, CVPR. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, 2015.\n\nObject region mining with adversarial erasing: A simple classification to semantic segmentation approach. Yunchao Wei, Jiashi Feng, Xiaodan Liang, Ming-Ming Cheng, Yao Zhao, Shuicheng Yan, CVPR. Yunchao Wei, Jiashi Feng, Xiaodan Liang, Ming-Ming Cheng, Yao Zhao, and Shuicheng Yan. Object region mining with adversarial erasing: A simple classifica- tion to semantic segmentation approach. In CVPR, 2017.\n\nFood image recognition using deep convolutional network with pre-training and fine-tuning. Keiji Yanai, Yoshiyuki Kawano, ICMEW. Keiji Yanai and Yoshiyuki Kawano. Food image recognition using deep convolutional network with pre-training and fine-tuning. In ICMEW, 2015.\n\nFood recognition using statistics of pairwise local features. Shulin Yang, Mei Chen, Dean Pomerleau, Rahul Sukthankar, CVPR. Shulin Yang, Mei Chen, Dean Pomerleau, and Rahul Sukthankar. Food recognition using statistics of pairwise local features. In CVPR, 2010.\n\nDeep layer aggregation. Fisher Yu, Dequan Wang, Evan Shelhamer, Trevor Darrell, CVPR. Fisher Yu, Dequan Wang, Evan Shelhamer, and Trevor Darrell. Deep layer aggregation. In CVPR, 2018.\n\nWide residual networks. Sergey Zagoruyko, Nikos Komodakis, BMVC. Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In BMVC, 2016.\n\nAdversarial complementary learning for weakly supervised object localization. Xiaolin Zhang, Yunchao Wei, Jiashi Feng, Yi Yang, Thomas S Huang, CVPR. Xiaolin Zhang, Yunchao Wei, Jiashi Feng, Yi Yang, and Thomas S Huang. Adversarial complementary learning for weakly supervised object localization. In CVPR, 2018.\n\nLearning deep features for discriminative localization. Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, Antonio Torralba, CVPR. Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features for discriminative localization. In CVPR, 2016.\n", "annotations": {"author": "[{\"end\":221,\"start\":68},{\"end\":233,\"start\":222},{\"end\":290,\"start\":234},{\"end\":415,\"start\":291},{\"end\":504,\"start\":416},{\"end\":591,\"start\":505}]", "publisher": null, "author_last_name": "[{\"end\":79,\"start\":76},{\"end\":232,\"start\":227},{\"end\":236,\"start\":234},{\"end\":302,\"start\":299},{\"end\":426,\"start\":422},{\"end\":513,\"start\":511}]", "author_first_name": "[{\"end\":75,\"start\":68},{\"end\":226,\"start\":222},{\"end\":298,\"start\":291},{\"end\":421,\"start\":416},{\"end\":510,\"start\":505}]", "author_affiliation": "[{\"end\":161,\"start\":110},{\"end\":220,\"start\":163},{\"end\":289,\"start\":238},{\"end\":355,\"start\":304},{\"end\":414,\"start\":357},{\"end\":503,\"start\":452},{\"end\":590,\"start\":539}]", "title": "[{\"end\":65,\"start\":1},{\"end\":656,\"start\":592}]", "venue": null, "abstract": "[{\"end\":2052,\"start\":706}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2446,\"start\":2442},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2714,\"start\":2710},{\"end\":3498,\"start\":3480},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3590,\"start\":3587},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3841,\"start\":3837},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4149,\"start\":4145},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5346,\"start\":5343},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5348,\"start\":5346},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5465,\"start\":5461},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5530,\"start\":5527},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5983,\"start\":5980},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6240,\"start\":6236},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6357,\"start\":6353},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":6454,\"start\":6451},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":6456,\"start\":6454},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6459,\"start\":6456},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6680,\"start\":6677},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6683,\"start\":6680},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6774,\"start\":6770},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7149,\"start\":7145},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7497,\"start\":7493},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7994,\"start\":7990},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8132,\"start\":8128},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8945,\"start\":8941},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11997,\"start\":11993},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15741,\"start\":15738},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":15915,\"start\":15912},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16764,\"start\":16761},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":16991,\"start\":16987},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":17224,\"start\":17220},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17440,\"start\":17437},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18163,\"start\":18160},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":18184,\"start\":18180},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":18204,\"start\":18200},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18226,\"start\":18223},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18252,\"start\":18248},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":18279,\"start\":18275},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18305,\"start\":18302},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":19437,\"start\":19433},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":19506,\"start\":19503},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":19521,\"start\":19518}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":25975,\"start\":25714},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":26413,\"start\":25976},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":26671,\"start\":26414},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":26946,\"start\":26672}]", "paragraph": "[{\"end\":4614,\"start\":2068},{\"end\":5149,\"start\":4616},{\"end\":6705,\"start\":5166},{\"end\":8266,\"start\":6707},{\"end\":8904,\"start\":8277},{\"end\":9095,\"start\":8924},{\"end\":10152,\"start\":9131},{\"end\":11493,\"start\":10164},{\"end\":12955,\"start\":11495},{\"end\":14212,\"start\":12957},{\"end\":15436,\"start\":14340},{\"end\":15634,\"start\":15452},{\"end\":16675,\"start\":15647},{\"end\":17688,\"start\":16702},{\"end\":18138,\"start\":17699},{\"end\":18310,\"start\":18149},{\"end\":18549,\"start\":18312},{\"end\":19298,\"start\":18561},{\"end\":21645,\"start\":19300},{\"end\":21703,\"start\":21647},{\"end\":25106,\"start\":21781},{\"end\":25713,\"start\":25122}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9130,\"start\":9096},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14339,\"start\":14213},{\"attributes\":{\"id\":\"formula_2\"},\"end\":21761,\"start\":21704}]", "table_ref": "[{\"end\":18340,\"start\":18333},{\"end\":18618,\"start\":18611},{\"end\":18851,\"start\":18844},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":19419,\"start\":19412},{\"end\":19685,\"start\":19678},{\"end\":20533,\"start\":20526},{\"end\":21478,\"start\":21471},{\"end\":22022,\"start\":22015},{\"end\":22857,\"start\":22850},{\"end\":23737,\"start\":23730},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":24468,\"start\":24461}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2066,\"start\":2054},{\"attributes\":{\"n\":\"2\"},\"end\":5164,\"start\":5152},{\"attributes\":{\"n\":\"3\"},\"end\":8275,\"start\":8269},{\"attributes\":{\"n\":\"3.1\"},\"end\":8922,\"start\":8907},{\"attributes\":{\"n\":\"3.2\"},\"end\":10162,\"start\":10155},{\"attributes\":{\"n\":\"4\"},\"end\":15450,\"start\":15439},{\"attributes\":{\"n\":\"4.1\"},\"end\":15645,\"start\":15637},{\"attributes\":{\"n\":\"4.2\"},\"end\":16700,\"start\":16678},{\"end\":17697,\"start\":17691},{\"end\":18147,\"start\":18141},{\"attributes\":{\"n\":\"4.3\"},\"end\":18559,\"start\":18552},{\"attributes\":{\"n\":\"4.4\"},\"end\":21779,\"start\":21763},{\"attributes\":{\"n\":\"5\"},\"end\":25120,\"start\":25109},{\"end\":25735,\"start\":25715},{\"end\":26424,\"start\":26415},{\"end\":26682,\"start\":26673}]", "table": "[{\"end\":26413,\"start\":26080},{\"end\":26671,\"start\":26489}]", "figure_caption": "[{\"end\":25975,\"start\":25738},{\"end\":26080,\"start\":25978},{\"end\":26489,\"start\":26426},{\"end\":26946,\"start\":26684}]", "figure_ref": "[{\"end\":9718,\"start\":9710},{\"end\":10681,\"start\":10673},{\"end\":16582,\"start\":16574},{\"end\":16633,\"start\":16625},{\"end\":19744,\"start\":19736},{\"end\":20647,\"start\":20639},{\"end\":21214,\"start\":21206}]", "bib_author_first_name": "[{\"end\":27101,\"start\":27096},{\"end\":27115,\"start\":27111},{\"end\":27126,\"start\":27123},{\"end\":27140,\"start\":27135},{\"end\":27159,\"start\":27150},{\"end\":27405,\"start\":27400},{\"end\":27425,\"start\":27419},{\"end\":27438,\"start\":27434},{\"end\":27449,\"start\":27448},{\"end\":27464,\"start\":27459},{\"end\":27722,\"start\":27717},{\"end\":27740,\"start\":27732},{\"end\":27756,\"start\":27753},{\"end\":27980,\"start\":27972},{\"end\":27996,\"start\":27987},{\"end\":28186,\"start\":28183},{\"end\":28198,\"start\":28193},{\"end\":28211,\"start\":28208},{\"end\":28219,\"start\":28216},{\"end\":28231,\"start\":28226},{\"end\":28247,\"start\":28244},{\"end\":28475,\"start\":28472},{\"end\":28485,\"start\":28481},{\"end\":28496,\"start\":28492},{\"end\":28508,\"start\":28502},{\"end\":28522,\"start\":28517},{\"end\":28755,\"start\":28752},{\"end\":28765,\"start\":28762},{\"end\":28779,\"start\":28772},{\"end\":28794,\"start\":28788},{\"end\":28802,\"start\":28799},{\"end\":28809,\"start\":28807},{\"end\":29033,\"start\":29028},{\"end\":29052,\"start\":29047},{\"end\":29068,\"start\":29063},{\"end\":29087,\"start\":29081},{\"end\":29113,\"start\":29106},{\"end\":29396,\"start\":29389},{\"end\":29408,\"start\":29401},{\"end\":29424,\"start\":29416},{\"end\":29434,\"start\":29430},{\"end\":29661,\"start\":29656},{\"end\":29676,\"start\":29667},{\"end\":29914,\"start\":29909},{\"end\":29922,\"start\":29920},{\"end\":29931,\"start\":29928},{\"end\":29945,\"start\":29937},{\"end\":29957,\"start\":29952},{\"end\":29977,\"start\":29969},{\"end\":30229,\"start\":30224},{\"end\":30397,\"start\":30393},{\"end\":30412,\"start\":30408},{\"end\":30417,\"start\":30413},{\"end\":30436,\"start\":30427},{\"end\":30651,\"start\":30645},{\"end\":30664,\"start\":30660},{\"end\":30680,\"start\":30675},{\"end\":30694,\"start\":30689},{\"end\":30712,\"start\":30708},{\"end\":30727,\"start\":30721},{\"end\":30745,\"start\":30739},{\"end\":30764,\"start\":30758},{\"end\":30785,\"start\":30777},{\"end\":30798,\"start\":30793},{\"end\":30800,\"start\":30799},{\"end\":31130,\"start\":31122},{\"end\":31148,\"start\":31137},{\"end\":31392,\"start\":31391},{\"end\":31404,\"start\":31403},{\"end\":31647,\"start\":31640},{\"end\":31653,\"start\":31648},{\"end\":31665,\"start\":31661},{\"end\":31669,\"start\":31666},{\"end\":31874,\"start\":31865},{\"end\":31887,\"start\":31884},{\"end\":31901,\"start\":31893},{\"end\":31913,\"start\":31907},{\"end\":31929,\"start\":31924},{\"end\":31944,\"start\":31936},{\"end\":31962,\"start\":31955},{\"end\":31977,\"start\":31970},{\"end\":31995,\"start\":31989},{\"end\":32322,\"start\":32315},{\"end\":32334,\"start\":32328},{\"end\":32348,\"start\":32341},{\"end\":32365,\"start\":32356},{\"end\":32376,\"start\":32373},{\"end\":32392,\"start\":32383},{\"end\":32711,\"start\":32706},{\"end\":32728,\"start\":32719},{\"end\":32954,\"start\":32948},{\"end\":32964,\"start\":32961},{\"end\":32975,\"start\":32971},{\"end\":32992,\"start\":32987},{\"end\":33180,\"start\":33174},{\"end\":33191,\"start\":33185},{\"end\":33202,\"start\":33198},{\"end\":33220,\"start\":33214},{\"end\":33366,\"start\":33360},{\"end\":33383,\"start\":33378},{\"end\":33564,\"start\":33557},{\"end\":33579,\"start\":33572},{\"end\":33591,\"start\":33585},{\"end\":33600,\"start\":33598},{\"end\":33615,\"start\":33607},{\"end\":33854,\"start\":33849},{\"end\":33867,\"start\":33861},{\"end\":33881,\"start\":33876},{\"end\":33897,\"start\":33893},{\"end\":33912,\"start\":33905}]", "bib_author_last_name": "[{\"end\":27109,\"start\":27102},{\"end\":27121,\"start\":27116},{\"end\":27133,\"start\":27127},{\"end\":27148,\"start\":27141},{\"end\":27167,\"start\":27160},{\"end\":27417,\"start\":27406},{\"end\":27432,\"start\":27426},{\"end\":27446,\"start\":27439},{\"end\":27457,\"start\":27450},{\"end\":27470,\"start\":27465},{\"end\":27476,\"start\":27472},{\"end\":27730,\"start\":27723},{\"end\":27751,\"start\":27741},{\"end\":27765,\"start\":27757},{\"end\":27985,\"start\":27981},{\"end\":28000,\"start\":27997},{\"end\":28191,\"start\":28187},{\"end\":28206,\"start\":28199},{\"end\":28214,\"start\":28212},{\"end\":28224,\"start\":28220},{\"end\":28242,\"start\":28232},{\"end\":28252,\"start\":28248},{\"end\":28479,\"start\":28476},{\"end\":28490,\"start\":28486},{\"end\":28500,\"start\":28497},{\"end\":28515,\"start\":28509},{\"end\":28531,\"start\":28523},{\"end\":28760,\"start\":28756},{\"end\":28770,\"start\":28766},{\"end\":28786,\"start\":28780},{\"end\":28797,\"start\":28795},{\"end\":28805,\"start\":28803},{\"end\":28817,\"start\":28810},{\"end\":29045,\"start\":29034},{\"end\":29061,\"start\":29053},{\"end\":29079,\"start\":29069},{\"end\":29104,\"start\":29088},{\"end\":29123,\"start\":29114},{\"end\":29132,\"start\":29125},{\"end\":29399,\"start\":29397},{\"end\":29414,\"start\":29409},{\"end\":29428,\"start\":29425},{\"end\":29438,\"start\":29435},{\"end\":29665,\"start\":29662},{\"end\":29680,\"start\":29677},{\"end\":29918,\"start\":29915},{\"end\":29926,\"start\":29923},{\"end\":29935,\"start\":29932},{\"end\":29950,\"start\":29946},{\"end\":29967,\"start\":29958},{\"end\":29980,\"start\":29978},{\"end\":30232,\"start\":30230},{\"end\":30406,\"start\":30398},{\"end\":30425,\"start\":30418},{\"end\":30446,\"start\":30437},{\"end\":30658,\"start\":30652},{\"end\":30673,\"start\":30665},{\"end\":30687,\"start\":30681},{\"end\":30706,\"start\":30695},{\"end\":30719,\"start\":30713},{\"end\":30737,\"start\":30728},{\"end\":30756,\"start\":30746},{\"end\":30775,\"start\":30765},{\"end\":30791,\"start\":30786},{\"end\":30807,\"start\":30801},{\"end\":31120,\"start\":31112},{\"end\":31135,\"start\":31131},{\"end\":31151,\"start\":31149},{\"end\":31156,\"start\":31153},{\"end\":31401,\"start\":31393},{\"end\":31414,\"start\":31405},{\"end\":31659,\"start\":31654},{\"end\":31673,\"start\":31670},{\"end\":31882,\"start\":31875},{\"end\":31891,\"start\":31888},{\"end\":31905,\"start\":31902},{\"end\":31922,\"start\":31914},{\"end\":31934,\"start\":31930},{\"end\":31953,\"start\":31945},{\"end\":31968,\"start\":31963},{\"end\":31987,\"start\":31978},{\"end\":32006,\"start\":31996},{\"end\":32326,\"start\":32323},{\"end\":32339,\"start\":32335},{\"end\":32354,\"start\":32349},{\"end\":32371,\"start\":32366},{\"end\":32381,\"start\":32377},{\"end\":32396,\"start\":32393},{\"end\":32717,\"start\":32712},{\"end\":32735,\"start\":32729},{\"end\":32959,\"start\":32955},{\"end\":32969,\"start\":32965},{\"end\":32985,\"start\":32976},{\"end\":33003,\"start\":32993},{\"end\":33183,\"start\":33181},{\"end\":33196,\"start\":33192},{\"end\":33212,\"start\":33203},{\"end\":33228,\"start\":33221},{\"end\":33376,\"start\":33367},{\"end\":33393,\"start\":33384},{\"end\":33570,\"start\":33565},{\"end\":33583,\"start\":33580},{\"end\":33596,\"start\":33592},{\"end\":33605,\"start\":33601},{\"end\":33621,\"start\":33616},{\"end\":33859,\"start\":33855},{\"end\":33874,\"start\":33868},{\"end\":33891,\"start\":33882},{\"end\":33903,\"start\":33898},{\"end\":33921,\"start\":33913}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":206858072},\"end\":27325,\"start\":27039},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":1582990},\"end\":27652,\"start\":27327},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":12726540},\"end\":27906,\"start\":27654},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":207240186},\"end\":28139,\"start\":27908},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1548631},\"end\":28391,\"start\":28141},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":43993788},\"end\":28697,\"start\":28393},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":57246310},\"end\":28963,\"start\":28699},{\"attributes\":{\"id\":\"b7\"},\"end\":29341,\"start\":28965},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":206594692},\"end\":29561,\"start\":29343},{\"attributes\":{\"id\":\"b9\"},\"end\":29815,\"start\":29563},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":14068125},\"end\":30173,\"start\":29817},{\"attributes\":{\"id\":\"b11\"},\"end\":30340,\"start\":30175},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":17006311},\"end\":30583,\"start\":30342},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":206770267},\"end\":31057,\"start\":30585},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":18486940},\"end\":31321,\"start\":31059},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":14124313},\"end\":31534,\"start\":31323},{\"attributes\":{\"id\":\"b16\"},\"end\":31831,\"start\":31536},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":206592484},\"end\":32207,\"start\":31833},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":6793190},\"end\":32613,\"start\":32209},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":206822389},\"end\":32884,\"start\":32615},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":304354},\"end\":33148,\"start\":32886},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":30834643},\"end\":33334,\"start\":33150},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":15276198},\"end\":33477,\"start\":33336},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":4940580},\"end\":33791,\"start\":33479},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":6789015},\"end\":34077,\"start\":33793}]", "bib_title": "[{\"end\":27094,\"start\":27039},{\"end\":27398,\"start\":27327},{\"end\":27715,\"start\":27654},{\"end\":27970,\"start\":27908},{\"end\":28181,\"start\":28141},{\"end\":28470,\"start\":28393},{\"end\":28750,\"start\":28699},{\"end\":29387,\"start\":29343},{\"end\":29907,\"start\":29817},{\"end\":30391,\"start\":30342},{\"end\":30643,\"start\":30585},{\"end\":31110,\"start\":31059},{\"end\":31389,\"start\":31323},{\"end\":31863,\"start\":31833},{\"end\":32313,\"start\":32209},{\"end\":32704,\"start\":32615},{\"end\":32946,\"start\":32886},{\"end\":33172,\"start\":33150},{\"end\":33358,\"start\":33336},{\"end\":33555,\"start\":33479},{\"end\":33847,\"start\":33793}]", "bib_author": "[{\"end\":27111,\"start\":27096},{\"end\":27123,\"start\":27111},{\"end\":27135,\"start\":27123},{\"end\":27150,\"start\":27135},{\"end\":27169,\"start\":27150},{\"end\":27419,\"start\":27400},{\"end\":27434,\"start\":27419},{\"end\":27448,\"start\":27434},{\"end\":27459,\"start\":27448},{\"end\":27472,\"start\":27459},{\"end\":27478,\"start\":27472},{\"end\":27732,\"start\":27717},{\"end\":27753,\"start\":27732},{\"end\":27767,\"start\":27753},{\"end\":27987,\"start\":27972},{\"end\":28002,\"start\":27987},{\"end\":28193,\"start\":28183},{\"end\":28208,\"start\":28193},{\"end\":28216,\"start\":28208},{\"end\":28226,\"start\":28216},{\"end\":28244,\"start\":28226},{\"end\":28254,\"start\":28244},{\"end\":28481,\"start\":28472},{\"end\":28492,\"start\":28481},{\"end\":28502,\"start\":28492},{\"end\":28517,\"start\":28502},{\"end\":28533,\"start\":28517},{\"end\":28762,\"start\":28752},{\"end\":28772,\"start\":28762},{\"end\":28788,\"start\":28772},{\"end\":28799,\"start\":28788},{\"end\":28807,\"start\":28799},{\"end\":28819,\"start\":28807},{\"end\":29047,\"start\":29028},{\"end\":29063,\"start\":29047},{\"end\":29081,\"start\":29063},{\"end\":29106,\"start\":29081},{\"end\":29125,\"start\":29106},{\"end\":29134,\"start\":29125},{\"end\":29401,\"start\":29389},{\"end\":29416,\"start\":29401},{\"end\":29430,\"start\":29416},{\"end\":29440,\"start\":29430},{\"end\":29667,\"start\":29656},{\"end\":29682,\"start\":29667},{\"end\":29920,\"start\":29909},{\"end\":29928,\"start\":29920},{\"end\":29937,\"start\":29928},{\"end\":29952,\"start\":29937},{\"end\":29969,\"start\":29952},{\"end\":29982,\"start\":29969},{\"end\":30234,\"start\":30224},{\"end\":30408,\"start\":30393},{\"end\":30427,\"start\":30408},{\"end\":30448,\"start\":30427},{\"end\":30660,\"start\":30645},{\"end\":30675,\"start\":30660},{\"end\":30689,\"start\":30675},{\"end\":30708,\"start\":30689},{\"end\":30721,\"start\":30708},{\"end\":30739,\"start\":30721},{\"end\":30758,\"start\":30739},{\"end\":30777,\"start\":30758},{\"end\":30793,\"start\":30777},{\"end\":30809,\"start\":30793},{\"end\":31122,\"start\":31112},{\"end\":31137,\"start\":31122},{\"end\":31153,\"start\":31137},{\"end\":31158,\"start\":31153},{\"end\":31403,\"start\":31391},{\"end\":31416,\"start\":31403},{\"end\":31661,\"start\":31640},{\"end\":31675,\"start\":31661},{\"end\":31884,\"start\":31865},{\"end\":31893,\"start\":31884},{\"end\":31907,\"start\":31893},{\"end\":31924,\"start\":31907},{\"end\":31936,\"start\":31924},{\"end\":31955,\"start\":31936},{\"end\":31970,\"start\":31955},{\"end\":31989,\"start\":31970},{\"end\":32008,\"start\":31989},{\"end\":32328,\"start\":32315},{\"end\":32341,\"start\":32328},{\"end\":32356,\"start\":32341},{\"end\":32373,\"start\":32356},{\"end\":32383,\"start\":32373},{\"end\":32398,\"start\":32383},{\"end\":32719,\"start\":32706},{\"end\":32737,\"start\":32719},{\"end\":32961,\"start\":32948},{\"end\":32971,\"start\":32961},{\"end\":32987,\"start\":32971},{\"end\":33005,\"start\":32987},{\"end\":33185,\"start\":33174},{\"end\":33198,\"start\":33185},{\"end\":33214,\"start\":33198},{\"end\":33230,\"start\":33214},{\"end\":33378,\"start\":33360},{\"end\":33395,\"start\":33378},{\"end\":33572,\"start\":33557},{\"end\":33585,\"start\":33572},{\"end\":33598,\"start\":33585},{\"end\":33607,\"start\":33598},{\"end\":33623,\"start\":33607},{\"end\":33861,\"start\":33849},{\"end\":33876,\"start\":33861},{\"end\":33893,\"start\":33876},{\"end\":33905,\"start\":33893},{\"end\":33923,\"start\":33905}]", "bib_venue": "[{\"end\":27173,\"start\":27169},{\"end\":27482,\"start\":27478},{\"end\":27771,\"start\":27767},{\"end\":28016,\"start\":28002},{\"end\":28258,\"start\":28254},{\"end\":28537,\"start\":28533},{\"end\":28823,\"start\":28819},{\"end\":29026,\"start\":28965},{\"end\":29444,\"start\":29440},{\"end\":29654,\"start\":29563},{\"end\":29987,\"start\":29982},{\"end\":30222,\"start\":30175},{\"end\":30455,\"start\":30448},{\"end\":30813,\"start\":30809},{\"end\":31181,\"start\":31158},{\"end\":31420,\"start\":31416},{\"end\":31638,\"start\":31536},{\"end\":32012,\"start\":32008},{\"end\":32402,\"start\":32398},{\"end\":32742,\"start\":32737},{\"end\":33009,\"start\":33005},{\"end\":33234,\"start\":33230},{\"end\":33399,\"start\":33395},{\"end\":33627,\"start\":33623},{\"end\":33927,\"start\":33923}]"}}}, "year": 2023, "month": 12, "day": 17}