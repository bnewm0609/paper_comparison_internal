{"id": 2272015, "updated": "2023-11-08 05:56:42.502", "metadata": {"title": "Learning to Extract Semantic Structure from Documents Using Multimodal Fully Convolutional Neural Network", "authors": "[{\"first\":\"Xiao\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Ersin\",\"last\":\"Yumer\",\"middle\":[]},{\"first\":\"Paul\",\"last\":\"Asente\",\"middle\":[]},{\"first\":\"Mike\",\"last\":\"Kraley\",\"middle\":[]},{\"first\":\"Daniel\",\"last\":\"Kifer\",\"middle\":[]},{\"first\":\"C.\",\"last\":\"Giles\",\"middle\":[\"Lee\"]}]", "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2017, "month": 6, "day": 7}, "abstract": "We present an end-to-end, multimodal, fully convolutional network for extracting semantic structures from document images. We consider document semantic structure extraction as a pixel-wise segmentation task, and propose a unified model that classifies pixels based not only on their visual appearance, as in the traditional page segmentation task, but also on the content of underlying text. Moreover, we propose an efficient synthetic document generation process that we use to generate pretraining data for our network. Once the network is trained on a large set of synthetic documents, we fine-tune the network on unlabeled real documents using a semi-supervised approach. We systematically study the optimum network architecture and show that both our multimodal approach and the synthetic data pretraining significantly boost the performance.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1706.02337", "mag": "2964346820", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/YangYAKKG17", "doi": "10.1109/cvpr.2017.462"}}, "content": {"source": {"pdf_hash": "d967254bf2e421204cfd92baa09d9633bde7e5ac", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1706.02337v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4c61824951f222cd767b242b5bd099e38b7bc5fb", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d967254bf2e421204cfd92baa09d9633bde7e5ac.txt", "contents": "\nLearning to Extract Semantic Structure from Documents Using Multimodal Fully Convolutional Neural Networks\n\n\nXiao Yang \nThe Pennsylvania State University\n\n\nErsin Yumer yumer@adobe.com \nAdobe Research\n\n\nPaul Asente asente@adobe.com \nAdobe Research\n\n\nMike Kraley mkraley@adobe.com \nAdobe Document Cloud\n\n\nDaniel Kifer dkifer@cse.psu.edu \nThe Pennsylvania State University\n\n\nC Lee Giles giles@ist.psu.edu \nThe Pennsylvania State University\n\n\nLearning to Extract Semantic Structure from Documents Using Multimodal Fully Convolutional Neural Networks\n\nWe present an end-to-end, multimodal, fully convolutional network for extracting semantic structures from document images. We consider document semantic structure extraction as a pixel-wise segmentation task, and propose a unified model that classifies pixels based not only on their visual appearance, as in the traditional page segmentation task, but also on the content of underlying text. Moreover, we propose an efficient synthetic document generation process that we use to generate pretraining data for our network. Once the network is trained on a large set of synthetic documents, we fine-tune the network on unlabeled real documents using a semi-supervised approach. We systematically study the optimum network architecture and show that both our multimodal approach and the synthetic data pretraining significantly boost the performance.\n\nIntroduction\n\nDocument semantic structure extraction (DSSE) is an actively-researched area dedicated to understanding images of documents. The goal is to split a document image into regions of interest and to recognize the role of each region. It is usually done in two steps: the first step, often referred to as page segmentation, is appearance-based and attempts to distinguish text regions from regions like figures, tables and line segments. The second step, often referred to as logical structure analysis, is semantics-based and categorizes each region into semantically-relevant classes like paragraph and caption.\n\nIn this work, we propose a unified multimodal fully convolutional network (MFCN) that simultaneously identifies both appearance-based and semantics-based classes. It is a generalized page segmentation model that additionally performs fine-grained recognition on text regions: text regions are assigned specific labels based on their semantic functionality in the document. Our approach simplifies DSSE and better supports document image understanding.\n\nWe consider DSSE as a pixel-wise segmentation problem: each pixel is labeled as background, figure, table, paragraph, section heading, list, caption, etc. We show that our MFCN model trained in an end-to-end, pixels-topixels manner on document images exceeds the state-ofthe-art significantly. It eliminates the need to design complex heuristic rules and extract hand-crafted features [31,23,22,47,5].\n\nIn many cases, regions like section headings or captions can be visually identified. In Fig. 1 (a), one can easily recognize the different roles of the same name. However, a robust DSSE system needs the semantic information of the text to disambiguate possible false identifications. For example, in Fig. 1 (b), the text in the large font might look like section heading, but it does not function that way; the lines beginning with dashes might be mislabeled as a list.\n\nTo this end, our multimodal fully convolutional network is designed to leverage the textual information in the document as well. To incorporate textual information in a CNNbased architecture, we build a text embedding map and feed it to our MFCN. More specifically, we embed each sentence and map the embedding to the corresponding pixels where the sentence is represented in the document. Fig. 2 summarizes the architecture of the proposed MFCN model. Our Figure 2: The architecture of the proposed multimodal fully convolutional neural network. It consists of four parts: an encoder that learns a hierarchy of feature representations, a decoder that outputs segmentation masks, an auxiliary decoder for unsupervised reconstruction, and a bridge that merges visual representations and textual representations. The auxiliary decoder only exists during training. model consists of four parts: an encoder that learns a hierarchy of feature representations, a decoder that outputs segmentation masks, an auxiliary decoder for reconstruction during training, and a bridge that merges visual representations and textual representations. We assume that the document text has been pre-extracted. For document images this can be done with modern OCR engines [48,1,3].\n\nOne of the bottlenecks in training fully convolutional networks is the need for pixel-wise ground truth data. Previous document understanding datasets [32,45,51,7] are limited by both their small size and the lack of fine-grained semantic labels such as section headings, lists, or figure and table captions. To address these issues, we propose an efficient synthetic document generation process and use it to generate large-scale pretraining data for our network. Furthermore, we propose two unsupervised tasks for better generalization to real documents: reconstruction and consistency tasks. The former enables better representation learning by reconstructing the input image, whereas the latter encourages pixels belonging to the same regions have similar representation.\n\nOur main contributions are summarized as follows:\n\n\u2022 We propose an end-to-end, unified network to address document semantic structure extraction. Unlike previous two-step processes, we simultaneously identify both appearance-based and semantics-based classes.\n\n\u2022 Our network supports both supervised training on image and text of documents, as well as unsupervised auxiliary training for better representation learning.\n\n\u2022 We propose a synthetic data generation process and use it to synthesize a large-scale dataset for training the supervised part of our deep MFCN model.\n\n\nBackground\n\nPage Segmentation. Most earlier works on page segmentation [31,23,22,47,5,46] fall into two categories: bottom-up and top-down approaches. Bottom-up approaches [31,47,5] first detect words based on local features (white/black pixels or connected components), then sequentially group words into text lines and paragraphs. However, such approaches suffer from the identification and grouping of connected components being time-consuming. Top-down approaches [23,22] iteratively split a page into columns, blocks, text lines and words. With both of these approaches it is difficult to correctly segment documents with complex layout, for example a document with nonrectangular figures [39].\n\nWith recent advances in deep convolutional neural networks, several neural-based models have been proposed. Chen et al. [13] applied a convolutional auto-encoder to learn features from cropped document image patches, then use these features to train a SVM [16] classifier. Vo et al. [53] proposed using FCN to detect lines in handwritten document images. However, these methods are strictly restricted to visual cues, and thus are not able to discover the semantic meaning of the underlying text.\n\nLogical Structure Analysis. Logical structure is defined as a hierarchy of logical components in documents, such as section headings, paragraphs and lists [39]. Early work in logical structure discovery [19,30,25,15] focused on using a set of heuristic rules based on the location, font and text of each sentence. Shilman et al. [46] modeled document layout as a grammar and used machine learning to minimize the cost of a invalid parsing. Luong et al. [36] proposed using a conditional random fields model to jointly label each sentence based on several hand-crafted features. However, the performance of these methods is limited by their reliance on hand-crafted features, which cannot capture the highly semantic context.\n\nSemantic Segmentation. Large-scale annotations [33] and the development of deep neural network approaches such as the fully convolutional network (FCN) [34] have led to rapid improvement of the accuracy of semantic segmentation [14,43,42,55]. However, the originally proposed FCN model has several limitations, such as ignoring small objects and mislabeling large objects due to the fixed receptive field size. To address this issue, Noh et al. [42] proposed using unpooling, a technique that reuses the pooled \"location\" at the up-sampling stage. Pinheiro et al. [44] attempted to use skip connections to refine segmentation boundaries. Our model addresses this issue by using a dilated block, inspired by dilated convolutions [55] and recent work [50,24] that groups several layers together . We further investigate the effectiveness of different approaches to optimize our network architecture.\n\nCollecting pixel-wise annotations for thousands or millions of images requires massive labor and cost. To this end, several methods [43,57,35] have been proposed to harness weak annotations (bounding-box level or image level annotations) in neural network training. Our consistency loss relies on similar intuition but does not require a \"class label\" for each bounding box.\n\nUnsupervised Learning. Several methods have been proposed to use unsupervised learning to improve supervised learning tasks. Mairal et al. [37] proposed a sparse coding method that learns sparse local features by sparsityconstrained reconstruction loss functions. Zhao et al. [59] proposed a Stacked What-Where Auto-Encoder that uses unpooling during reconstruction. By injecting noise into the input and the middle features, a denoising auto-encoder [52] can learn robust filters that recover uncorrupted input. The main focus in unsupervised learning has been image-level classification and generative approaches, whereas in this paper we explore the potential of such methods for pixel-wise semantic segmentation.\n\nWen et al. [54] recently proposed a center loss that encourages data samples with the same label to have a similar visual representation. Similarly, we introduce an intra-class consistency constraint. However, the \"center\" for each class in their loss is determined by data samples across the whole dataset, while in our case the \"center\" is locally determined by pixels within the same region in each image.\n\nLanguage and Vision. Several joint learning tasks such as image captioning [17,29], visual question answering [6,21,38], and one-shot learning [20,49,12] have demonstrated the significant impact of using textual and visual representations in a joint framework. Our work is unique in that we use textual embedding directly for a seg-mentation task for the first time, and we show that our approach improves the results of traditional segmentation approaches that only use visual cues.\n\n\nMethod\n\nOur method does supervised training for pixel-wise segmentation with a specialized multimodal fully convolutional network that uses a text embedding map jointly with the visual cues. Moreover, our MFCN architecture also supports two unsupervised learning tasks to improve the learned document representation: a reconstruction task based on an auxiliary decoder and a consistency task evaluated in the main decoder branch along with the per-pixel segmentation loss.\n\n\nMultimodal Fully Convolutional Network\n\nAs shown in Fig. 2, our MFCN model has four parts: an encoder, two decoders and a bridge. The encoder and decoder parts roughly follow the architecture guidelines set forth by Noh et al. [42]. However, several changes have been made to better address document segmentation.\n\nFirst, we observe that several semantic-based classes such as section heading and caption usually occupy relatively small areas. Moreover, correctly identifying certain regions often relies on small visual cues, like lists being identified by small bullets or numbers in front of each item. This suggests that low-level features need to be used. However, because max-pooling naturally loses information during downsampling, FCN often performs poorly for small objects. Long et al. [34] attempt to avoid this problem using skip connections. However, simply averaging independent predictions based on features at different scales does not provide a satisfying solution. Low-level representations, limited by the local receptive field, are not aware of objectlevel semantic information; on the other hand, high-level features are not necessarily aligned consistently with object boundaries because CNN models are invariant to translation. We propose an alternative skip connection implementation, illustrated by the blue arrows in Fig. 2, similar to that used in the independent work SharpMask [44]. However, they use bilinear upsampling after skip connection while we use unpooling to preserve more spatial information.\n\nWe also notice that broader context information is needed to identify certain objects. For an instance, it is often difficult to tell the difference between a list and several paragraphs by only looking at parts of them. In Fig. 3, to correctly segment the right part of the list, the receptive fields must be large enough to capture the bullets on the left. Inspired by the Inception architecture [50] and dilated convolution [55], we propose a dilated convolution block, which is illustrated in Fig. 4 (left). Each dilated convolution block consists of 5 dilated convolutions with a 3 \u00d7 3 kernel size and a dilation d = 1, 2, 4, 8, 16. \n\n\nText Embedding Map\n\nTraditional image semantic segmentation models learn the semantic meanings of objects from a visual perspective. Our task, however, also requires understanding the text in images from a linguistic perspective. Therefore, we build a text embedding map and feed it to our multimodal model to make use of both visual and textual representations.\n\nWe treat a sentence as the minimum unit that conveys certain semantic meanings, and represent it using a lowdimensional vector. Our sentence embedding is built by averaging embeddings for individual words. This is a simple yet effective method that has been shown to be useful in many applications, including sentiment analysis [27] and text classification [28]. Using such embeddings, we create a text embedding map as follows: for each pixel inside the area of a sentence, we use the corresponding sentence embedding as the input. Pixels that belong to the same sentence thus share the same embedding. Pixels that do not belong to any sentences will be filled with zero vectors. For a document image of size H \u00d7 W , this process results in an embedding map of size N \u00d7 H \u00d7 W if the learned sentence embeddings are N -dimensional vectors. The embedding map is later concatenated with a feature response along the number-of-channel dimensions (see Fig. 2).\n\nSpecifically, our word embedding is learned using the skip-gram model [40,41]. Fig. 4 (right) shows the basic diagram. Let V be the number of words in a vocabulary and w be a V -dimensional one-hot vector representing a word. The training objective is to find a N -dimensional (N V ) vector representation for each word that is useful for predicting the neighboring words. More formally, given a sequence of words [w 1 , w 2 , \u00b7 \u00b7 \u00b7 , w T ], we maximize the average log probability\n1 T T t=1 \u2212C\u2264j\u2264C,j =0 logP (w t+j |w t )(1)\nwhere T is the length of the sequence and C is the size of the context window. The probability of outputting a word w o given an input word w i is defined using softmax:\nP (w o |w i ) = exp(v wo v wi ) V w=1 exp(v w v wi )(2)\nwhere v w and v w are the \"input\" and \"output\" Ndimensional vector representations of w.\n\n\nUnsupervised Tasks\n\nAlthough our synthetic documents (Sec. 4) provide a large amount of labeled data for training, they are limited in the variations of their layouts. To this end, we define two unsupervised loss functions to make use of real documents and to encourage better representation learning.\n\nReconstruction Task. It has been shown that reconstruction can help learning better representations and therefore improves performance for supervised tasks [59,58]. We thus introduce a second decoder pathway ( Fig. 2 -axillary decoder), denoted as D rec , and define a reconstruction loss at intermediate features. This auxiliary decoder only exists during the training phase.\n\nLet a l , l = 1, 2, \u00b7 \u00b7 \u00b7 L be the activations of the l th layer of the encoder, and a 0 be the input image. For a feed-forward convolutional network, a l is a feature map of size C l \u00d7H l \u00d7 W l . Our auxiliary decoder D rec attempts to reconstruct a hierarchy of feature maps {\u00e3 l }. Reconstruction loss L (l) rec for a specific l is therefore defined as\nL (l) rec = 1 C l H l W l a l \u2212\u00e3 l 2 2 , l = 0, 1, 2, \u00b7 \u00b7 \u00b7 L(3)\nConsistency Task. Pixel-wise annotations are laborintensive to obtain, however it is relatively easy to get a set of bounding boxes for detected objects in a document. For documents in PDF format, one can find bounding boxes by analyzing the rendering commands in the PDF files (See our supplementary document for typical examples). Even if their labels remain unknown, these bounding boxes are still beneficial: they provide knowledge of which parts of a document belongs to the same objects and thus should not be segmented into different fragments.\n\nBy building on the intuition that regions belonging to same objects should have similar feature representations, we define the consistency task loss L cons as follows. Let p (i,j) (i = 1, 2, \u00b7 \u00b7 \u00b7 H, j = 1, 2, \u00b7 \u00b7 \u00b7 W ) be activations at location (i, j) in a feature map of size C \u00d7 H \u00d7 W , and b be the rectangular area in a bounding box. Let each rectangular area b is of size H b \u00d7 W b . Then, for each b \u2208 B, L cons will be given by\nL cons = 1 H b W b (i,j)\u2208b p (i,j) \u2212 p (b) 2 2 (4) p (b) = 1 H b W b (i,j)\u2208b p (i,j)(5)\nMinimizing consistency loss L cons encourages intra-region consistency.\n\nThe consistency loss L cons is differentiable and can be optimized using stochastic gradient descent. The gradient of L cons with respect to p (i,j) is\n\u2202L cons \u2202p (i,j) = 2 H 2 b W 2 b (p (i,j) \u2212 p (b) )(H b W b \u2212 1)+ 2 H 2 b W 2 b (u,v)\u2208b (u,v) =(i,j) (p (b) \u2212 p (u,v) )(6)\nsince H b W b 1, for efficiency it can be approximated by:\n\u2202L cons \u2202p (i,j) \u2248 2 H b W b p (i,j) \u2212 p (b) .(7)\nWe use the unsupervised consistency loss, L cons , as a loss layer, that is evaluated at the main decoder branch (blue branch in Fig. 2) along with supervised segmentation loss.\n\n\nSynthetic Document Data\n\nSince our MFCN aims to generate a segmentation mask of the whole document image, pixel-wise annotations are required for the supervised task. While there are several publicly available datasets for page segmentation [45,51,7], there are only a few hundred to a few thousand pages in each. Furthermore, the types of labels are limited, for example to text, figure and table, however our goal is to perform a much more granular segmentation.\n\nTo address these issues, we created a synthetic data engine, capable of generating large-scale, pixel-wise annotated documents.\n\nOur synthetic document engine uses two methods to generate documents. The first produces completely automated and random layout of partial data scraped from the web. More specifically, we generate LaTeX source files in which paragraphs, figures, tables, captions, section headings and lists are randomly arranged to make up single, double, or triple-column PDFs. Candidate figures include academicstyle figures and graphic drawings downloaded using web image search, and natural images from MS COCO [33], which associates each image with several captions. Candidate tables are downloaded using web image search. Various queries are used to increase the diversity of downloaded tables. Since our MFCN model relies on the semantic meaning of text to make prediction, the content of text regions (paragraph, section heading, list, caption) must be carefully selected:\n\n\u2022 For paragraphs, we randomly sample sentences from a 2016 English Wikipedia dump [4].\n\n\u2022 For section headings, we only sample sentences and phrases that are section or subsection headings in the \"Contents\" block in a Wikipedia page.\n\n\u2022 For lists, we ensure that all items in a list come from the same Wikipedia page.\n\n\u2022 For captions, we either use the associated caption (for images from MS COCO) or the title of the image in web image search, which can be found in the span with class name \"irc pt\".\n\nTo further increase the complexity of the generated document layouts, we collected and labeled 271 documents with varied, complicated layouts. We then randomly replaced each element with a standalone paragraph, figure, table, caption, section heading or list generated as stated above.\n\nIn total, our synthetic dataset contains 135,000 document images. Examples of our synthetic documents are shown in Fig. 5. Please refer to our supplementary document for more examples of synthetic documents and individual elements used in the generation process. Fig. 2 summarizes the architecture of our model. The auxiliary decoder only exists in the training phase. All convolutional layers have a 3 \u00d7 3 kernel size and a stride of 1. The pooling (in the encoders) and unpooling (in the decoders) have a kernel size of 2 \u00d7 2. We adopt batch normalization [26] immediately after each convolution and before all non-linear functions.\n\n\nImplementation Details\n\nWe perform per-channel mean subtraction and resize each input image so that its longer side is less than 384 pixels. No other pre-processing is applied. We use Adadelta [56] with a mini-batch size of 2. During semisupervised training, mini-batches of synthetic and real documents are used alternatively. For synthetic documents, both per-pixel classification loss and the unsupervised losses are active at back-propagation, while for real documents, only the unsupervised losses are active. Since the labels are unbalanced (e.g. the area of paragraphs is much larger than that of caption), class weights for the perpixel classification loss are set differently according to the total number of pixels in each class in the training set.\n\nFor text embedding, we represent each word as a 128dimensional vector and train a skip-gram model on the 2016 English Wikipedia dump [4]. Embeddings for outof-dictionary words are obtained following Bojanowski et al. [10]. We use Tesseract [48] as our OCR engine.\n\nPost-processing. We apply an optional post-processing step as a cleanup strategy for segment masks. For documents in PDF format, we obtain a set of candidate bounding boxes by analyzing the PDF format to find element boxes. We then refine the segmentation masks by first calculating the average class probability for pixels belonging to the same box, followed by assigning the most likely label to these pixels.\n\n\nExperiments\n\nWe used three datasets for evaluations: ICDAR2015 [7], SectLabel [36] and our new dataset named DSSE-200. ICDAR2015 [7] is a dataset used in the biennial IC-DAR page segmentation competitions [8] focusing more on appearance-based regions. The evaluation set of IC-DAR2015 consists of 70 sampled pages from contemporary magazines and technical articles. SectLabel [36] consists of 40 academic papers with 347 pages in the field of computer science. Each text line in these papers is manually assigned a semantics-based label such as text, section heading or list item. In addition to these two datasets, we introduce DSSE-200 1 , which provides both appearance-based and semantics-based labels. DSSE-200 contains 200 pages from magazines and academic papers. Regions in a page are assigned labels from the following dictionary: figure, table, section, caption, list and paragraph. Note that DSSE-200 has a more granular segmentation than previously released benchmark datasets.\n\nThe performance is measured in terms of pixel-wise intersection-over-union (IoU), which is standard in semantic segmentation tasks. We optimize the architecture of our MFCN model based on the DSSE-200 dataset since it contains both appearance-based and semantics-based labels. Sec. 6.4 compares our results to state-of-the-art methods on the ICDAR2015 and SectLabel datasets.\n\n\nAblation Experiment on Model Architecture\n\nWe first systematically evaluate the effectiveness of different network architectures. Results are shown in Table 1. Note that these results do not incorporate textual information or unsupervised learning tasks. The purpose of this experiment is to find the best \"base\" architecture to be used in the following experiments. All models are trained from scratch and evaluated on the DSSE-200 dataset.\n\nAs a simple baseline (Table 1 Model1), we train a plain encoder-decoder style model for document segmentation. It consists of a feed-forward convolutional network as an encoder, and a decoder implemented by a fully convolutional network. Upsampling is done by bilinear interpolation. This model achieves a mean IoU of 61.4%.\n\nNext, we add skip connections to the model, resulting in Model2. Note that this model is similar to the SharpMask model. We observe a mean IoU of 65.4%, 4% better than the base model. The improvements are even more significant for small objects like captions.\n\nWe further evaluate the effectiveness of replacing bilinear upsampling with unpooling, giving Model3. All upsampling layers in Model2 are replaced by unpooling while other parts are kept unchanged. Doing so results in a significant improvement for mean IoU (65.4% vs. 71.2%). This suggests that the pooled index should not be discarded during decoding. These indexes are helpful to disambiguate the location information when constructing the segmentation mask in the decoder.\n\nFinally, we investigate the use of dilated convolutions. Model3 is equivalent to using dilated convolution when d = 1. Model4 sets d = 8 while Model5 uses the dilated block illustrated in Fig. 4 (left). The number of output channels are adjusted such that the total number of parame- SectLabel. Since these documents are not in PDF format, the simple post-processing in Sec. 5 can not be applied. One may consider exploiting a CRF [14] to refine the segmentation, but that is beyond the main focus of this paper. Segmentation label colors are: figure ,  ters are similar. Comparing the results for these three models, we can see that the IoU of Model4 for each class is on par with or worse than Model3, while Model5 is better than both Model3 and Model4 for all classes.\n\n\nAdding Textual Information\n\nWe now investigate the importance of textual information in our multimodal model. We take the best architecture, Model5, as our vision-only model, and incorporate a text embedding map via a bridge module depicted in Fig. 2. This combined model is fine-tuned on our synthetic documents. As shown in Table 2, using text as well improves the performance for textual classes. The accuracy for section heading, caption, list and paragraph is boosted by 1.1%, 0.1%, 1.7% and 2.2%, respectively.\n\nWe rely on existing OCR engines [48] to extract text, but they are not always reliable for scanned documents of low quality. To quantitatively analyze the effects of using extracted text, we compare the performance of using extracted text versus real text. The comparison is conducted on a subset of our synthetic dataset (200 images), since ground-truth text is naturally available. As shown in Table 2, using real text leads to a remarkable improvement (6.4%) for mean IoU, suggesting the effectiveness of incorporating textual information. Using OCR extracted text is not as effective, but still results in 2.6% improvement. It is better than the 0.3% improvement on DSSE-200 dataset; we attribute this to our synthetic data not being as complicated as DSSE-200, so extracting text becomes easier.  Table 2: IoU scores (%) on the DSSE-200 (D) and synthetic dataset (S) using text embedding map. On synthetic dataset, we further investigate the effects of using extracted text versus real text when building the text embedding map.\n\nL cls L rec L cons L rec+con mean 73.3 73.9\n\n75.4 75.9 Table 3: IoU scores (%) when using different training objectives on DSSE-200 dataset. cls: pixel-wise classification task, rec: reconstruction task and cons: consistency task.\n\n\nMethods\n\nnon-text text Leptonica [9] 84.7 86.8 Bukhari et al. [11] 90  Table 5: F1 scores on the SectLabel dataset. Note that our model can also identify non-text classes such as figures and tables.\n\n\nUnsupervised Learning Tasks\n\nHere, we examine how the proposed two unsupervised learning tasks -reconstruction and consistency taskscan complement the pixel-wise classification during training. We take the best model in Sec. 6.2, and only change the training objectives. Our model is then fine-tuned in a semisupervised manner as described in Sec. 5. The results are shown in Table 3. Adding the reconstruction task slightly improves the mean IoU by 0.6%, while adding the consistency task leads to a boost of 1.9%. These results justify our hypothesis that harnessing region information is beneficial. Combining both tasks results in a mean IoU of 75.9%. Table 4 and 5 present comparisons with several methods that have previously reported performance on the IC-DAR2015 and SectLabel datasets. It is worth emphasiz-ing that our MFCN model simultaneously predicts both appearance-based and semantics-based classes while other methods can not.\n\n\nComparisons with Prior Art\n\nComparisons on ICDAR2015 dataset (Table 4). Previous pixel-wise page segmentation models usually solve a binary segmentation problem and do not make predictions for fine-grained classes. For fair comparison, we change the number of output channels of the last layer to 3 (background, figure and text) and fine-tune this last layer. Our binary MFCN model achieves 94.5%, 91.0% and 77.1% IoU scores for non-text (background and figure), text and figure regions, outperforming other models.\n\nComparisons on SectLabel dataset (Table 5). Luong et at. [36] first use Omnipage [3] to localize and recognize text lines, then predict the semantics-based label for each line. The F1 score for each class was reported. For fair comparison, we use the same set of text line bounding boxes, and use the averaged pixel-wise prediction as the label for each text line. Our model achieves better F1 scores for section heading (0.919 VS 0.916), caption (0.893 VS 0.781) and list (0.793 VS 0.712), while being capable of identifying figures and tables.\n\n\nConclusion\n\nWe proposed a multimodal fully convolutional network (MFCN) for document semantic structure extraction. The proposed model uses both visual and textual information. Moreover, we propose an efficient synthetic data generation method that yields per-pixel ground-truth. Our unsupervised auxiliary tasks help boost performance tapping into unlabeled real documents, facilitating better representation learning. We showed that both the multimodal approach and unsupervised tasks can help improve performance. Our results indicate that we have improved the state of the art on previously established benchmarks. In addition, we are publicly providing the large synthetic dataset (135,000 pages) as well as a new benchmark dataset: DSSE-200.\n\n\nA. Synthetic Document Data\n\nWe introduced two methods to generate documents. In the first method, we generate LaTeX source files in which elements like paragraphs, figures, tables, captions, section headings and lists are randomly arranged using the \"textblock\" environment from the \"textpos\" package. Compiling these LaTeX files gives single, double, or triplecolumn PDFs. The generation process is summarized in Algorithm 1. s e \u2190 a string of LaTeX code that generates e using the \"textblock\" environment 7: s \u2190 s + s e 8: end while Output: s Output: A PDF document after compiling s Elements in a document are carefully selected following the guidelines below. Figure 7 shows several examples of the figures and tables used in the synthetic data generation.\n\n\nAlgorithm 1 Synthetic Document Generation\n\n\u2022 Candidate figures include natural images from MS COCO [33], academic-style figures and graphic drawings downloaded using web image search.\n\n\u2022 Candidates tables include table images downloaded using web image search. Various queries are used to increase the diversity of downloaded tables.\n\n\u2022 For paragraphs, we randomly sample sentences from a 2016 English Wikipedia dump [4].\n\n\u2022 For section headings, we sample sentences and phrases that are section or subsection headings in the \"Contents\" block in a Wikipedia page.\n\n\u2022 For lists, we sample list items from Wikipedia pages, ensuring that all items in a list come from the same Wikipedia page.\n\n\u2022 For captions, we either use the associated caption (for images from MS COCO) or the title of the image in web image search, which can be found in the span with class name \"irc pt\".\n\nIn the second document generation method, we collected and labeled 271 documents with varied, complicated layouts. We then randomly replaced each element with a standalone paragraph, figure, table, caption, section heading or list generated as stated above. Figure 8 shows several examples from the 271 documents.\n\n\nB. Visualizing the Segmentation Results\n\nEach pixel p in the model's output layer is assigned the color of the most likely class label l. The RGB value of that color is then weighted by the probability P(l).\n\n\nC. Post-processing\n\nWe apply an optional post-processing step to clean up segment masks for documents in PDF format. First, we obtain candidate bounding boxes by using the auto-tagging capabilities of Adobe Acrobat [2] and parsing the results. Boxes are stored in a tree structure, and each node's box can be a TextRun (a sequence of characters), TextLine (potentially a text line), Paragraph (potentially a paragraph) or Container (potentially figures or tables). Note that we ignore the semantic meanings associated with these boxes and only use the boxes as candidate bounding boxes in postprocessing. Figure 9 (2) and 10 (2) illustrate candidate bounding boxes for each document.\n\n\nAlgorithm 2 Segmentation Post-processing\n\nInput: P \u2190 probability map, P (u, v) \u2208 R |C| is a vector containing the probability of each class c \u2208 C at location (u, v) Input: Boxes \u2190 candidate bounding boxes 1: S \u2190 segmentation to be generated 2: for each location (x, y) \u2208 S do 3:\n\nS(x, y) \u2190 background 4: end for 5: for each b \u2208 Boxes do parent box comes before child boxes 6:p \u2190 (u,v)\u2208b P (u, v) 7: l \u2190 argmaxp 8: for each location (u, v) \u2208 b do 9: if S(u, v) is background then end for 13: end for Output: S Using these bounding box candidates, we refine the segmentation masks by first calculating the average class probability for pixels belonging to the same box, followed by assigning the most likely label to these pixels. The process is summarized in Algorithm 2.     \n\nFigure 1 :\n1(a) Examples that are difficult to identify if only based on text. The same name can be a title, an author or a figure caption. (b) Examples that are difficult to identify if only based on visual appearance. Text in the large font might be mislabeled as a section heading. Text with dashes might be mislabeled as a list.\n\nFigure 3 :\n3A cropped document image and its segmentation mask generated by our model. Note that the top-right corner of the list is yellow instead of cyan, indicating that it has been mislabeled as a paragraph.\n\nFigure 4 :\n4Left: A dilated block that contains 5 dilated convolutional layers with different dilation d. Batch-Normalization and non-linearity are not shown for brevity. Right: The skip-gram model for word embeddings.\n\nFigure 5 :\n5Example synthetic documents, raw segmentations and results after optional post-processing (Sec. 5). Segmentation label colors are: figure , table , section heading , caption , list and paragraph .\n\nFigure 6 :\n6Example real documents and their corresponding segmentation. Top: DSSE-200. Middle: ICDAR2015. Bottom:\n\nFigure 7 :\n7Sample figures and tables used in synthetic documents generation. (1) Natural images from MS COCO dataset. (2) Academic-style figures from web image search. (3) Symbols and graphic drawings from web image search. (4) Tables from web image search.\n\nFigure 8 :\n8Examples of documents with complicated layout. We labeled regions in each document and then randomly replaced them with a standalone paragraph, figure, table, caption, section heading or list, as described in Sec. AD. Additional Visualization ResultsFigures 9 and 10 show additional visualization examples of synthetic documents, andFigure 11shows additional examples of real documents.\n\nFigure 9 :\n9Synthetic documents and the corresponding segmentations. (1) Input synthetic documents. (2) Candidate bounding boxes obtained by parsing the PDF rendering commands. (3) Raw segmentation outputs. (4) Segmentations after postprocessing. Segmentation label colors are: figure , table , section heading , caption , list and paragraph .\n\nFigure 10 :\n10Synthetic documents and the corresponding segmentations. (1) Input synthetic documents. (2) Candidate bounding boxes obtained by parsing the PDF rendering commands. (3) Raw segmentation outputs. (4) Segmentations after postprocessing. Segmentation label colors are: figure , table , section heading , caption , list and paragraph .\n\nFigure 11 :\n11Real documents and the corresponding segmentations. Segmentation label colors are: figure , table , section heading , caption , list and paragraph .\n\n\ntable , section heading , caption , list and paragraph .Model# dilation upsampling skip \nbkg figure table section caption list paragraph mean \n1 \n1 \nbilinear \nno \n80.3 75.4 \n62.7 \n50.0 \n33.8 \n57.3 \n70.4 \n61.4 \n2 \n1 \nbilinear \nyes \n82.1 76.7 \n74.4 \n51.8 \n42.4 \n58.7 \n74.4 \n65.4 \n3 \n1 \nunpooling \nyes \n84.1 81.2 \n77.6 \n54.6 \n60.3 \n65.9 \n74.8 \n71.2 \n4 \n8 \nunpooling \nyes \n83.9 74.9 \n69.7 \n57.2 \n60.2 \n64.6 \n76.1 \n69.5 \n5 \nblock \nunpooling \nyes \n84.6 83.3 \n79.4 \n58.3 \n61.0 \n66.7 \n77.1 \n73.0 \n\nTable 1: Ablation experiments on DSSE-200 dataset. The architecture of each model is characterized by the dilation in \nconvolution layers, the way of upsampling and the use of skip connection. IoU scores (%) are reported. \n\n\n\n\nIoU scores (%) for page segmentation on the ICDAR2015 dataset. For comparison purpose, only IoU scores for non-text, text and figure are shown. However our model can make fine-grained predictions as well..6 \n90.3 \nOurs (binary) \n94.5 \n91.0 \nMethods \nfigure \ntext \nFernandez et al. [18] \n70.1 \n85.8 \nOurs (binary) \n77.1 \n91.0 \nTable 4: Methods \nsection caption \nlist \npara. \nLuong et al. [36] 0.916 \n0.781 \n0.712 0.969 \nOurs \n0.919 \n0.893 \n0.793 0.969 \n\n\n\n1: s \u2190 a string containing preamble and necessary packages of a LaTeX source file 2: Select a LaTeX source file type T \u2208 {single-column, double-column, triple-column} 3: while space remains on the page do Select an element type E \u2208 {figure, table, caption, section heading, list, paragraph} Select an example e of type E4: \n\n5: \n\n6: \n\n\nhttp://personal.psu.edu/xuy111/projects/ cvpr2017_doc.html.\nAcknowledgmentThis work started during Xiao Yang's internship at Adobe Research. This work was supported by NSF grant CCF 1317560 and Adobe Systems Inc.\n. Abbyy, Abbyy. https://www.abbyy.com/. 2\n\n. Adobe Acrobat, Adobe Acrobat. http://www.adobe.com/ accessibity/products/acrobat.html. 11\n\n. Omnipage, Omnipage. https://goo.gl/nDQEpC. 2, 8\n\n. Wikipedia, 611Wikipedia. https://dumps.wikimedia.org/. 5, 6, 11\n\nPage segmentation and classification utilizing bottom-up approach. A Amin, R Shiu, International Journal of Image and Graphics. 102A. Amin and R. Shiu. Page segmentation and classification utilizing bottom-up approach. International Journal of Im- age and Graphics, 1(02):345-361, 2001. 1, 2\n\nVqa: Visual question answering. S Antol, A Agrawal, J Lu, M Mitchell, D Batra, C Lawrence Zitnick, D Parikh, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionS. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. Lawrence Zitnick, and D. Parikh. Vqa: Visual question answering. In Proceedings of the IEEE International Con- ference on Computer Vision, pages 2425-2433, 2015. 3\n\nA realistic dataset for performance evaluation of document layout analysis. A Antonacopoulos, D Bridson, C Papadopoulos, S Pletschacher, 10th International Conference on Document Analysis and Recognition. 56A. Antonacopoulos, D. Bridson, C. Papadopoulos, and S. Pletschacher. A realistic dataset for performance evalua- tion of document layout analysis. In 2009 10th International Conference on Document Analysis and Recognition, pages 296-300. IEEE, 2009. 2, 5, 6\n\nIcdar2015 competition on recognition of documents with complex layouts-rdcl2015. A Antonacopoulos, C Clausner, C Papadopoulos, S Pletschacher, Document Analysis and Recognition (ICDAR), 2015 13th International Conference on. IEEEA. Antonacopoulos, C. Clausner, C. Papadopoulos, and S. Pletschacher. Icdar2015 competition on recognition of documents with complex layouts-rdcl2015. In Document Analysis and Recognition (ICDAR), 2015 13th International Conference on, pages 1151-1155. IEEE, 2015. 6\n\nDocument image applications. D S Bloomberg, L Vincent, Morphologie Mathmatique. 8D. S. Bloomberg and L. Vincent. Document image applica- tions. Morphologie Mathmatique, 2007. 8\n\nP Bojanowski, E Grave, A Joulin, T Mikolov, arXiv:1607.04606Enriching word vectors with subword information. arXiv preprintP. Bojanowski, E. Grave, A. Joulin, and T. Mikolov. Enrich- ing word vectors with subword information. arXiv preprint arXiv:1607.04606, 2016. 6\n\nImproved document image segmentation algorithm using multiresolution morphology. S S Bukhari, F Shafait, T M Breuel, IS&T/SPIE Electronic Imaging, pages 78740D-78740D. International Society for Optics and Photonics. S. S. Bukhari, F. Shafait, and T. M. Breuel. Improved document image segmentation algorithm using multiresolu- tion morphology. In IS&T/SPIE Electronic Imaging, pages 78740D-78740D. International Society for Optics and Pho- tonics, 2011. 8\n\nSynthesized classifiers for zero-shot learning. S Changpinyo, W.-L Chao, B Gong, F Sha, arXiv:1603.00550arXiv preprintS. Changpinyo, W.-L. Chao, B. Gong, and F. Sha. Syn- thesized classifiers for zero-shot learning. arXiv preprint arXiv:1603.00550, 2016. 3\n\nPage segmentation of historical document images with convolutional autoencoders. K Chen, M Seuret, M Liwicki, J Hennebert, R Ingold, Document Analysis and Recognition (ICDAR). 13th International Conference onK. Chen, M. Seuret, M. Liwicki, J. Hennebert, and R. In- gold. Page segmentation of historical document images with convolutional autoencoders. In Document Analysis and Recognition (ICDAR), 2015 13th International Conference on, pages 1011-1015. IEEE, 2015. 2\n\nSemantic image segmentation with deep convolutional nets and fully connected crfs. L.-C Chen, G Papandreou, I Kokkinos, K Murphy, A L Yuille, arXiv:1412.706237arXiv preprintL.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Semantic image segmentation with deep con- volutional nets and fully connected crfs. arXiv preprint arXiv:1412.7062, 2014. 3, 7\n\nPage grammars and page parsing. a syntactic approach to document layout recognition. A Conway, Proceedings of the Second International Conference on. the Second International Conference onDocument Analysis and RecognitionA. Conway. Page grammars and page parsing. a syntactic ap- proach to document layout recognition. In Document Analy- sis and Recognition, 1993., Proceedings of the Second Inter- national Conference on, pages 761-764. IEEE, 1993. 2\n\nSupport-vector networks. C Cortes, V Vapnik, Machine learning. 203C. Cortes and V. Vapnik. Support-vector networks. Machine learning, 20(3):273-297, 1995. 2\n\nLong-term recurrent convolutional networks for visual recognition and description. J Donahue, L Hendricks, S Guadarrama, M Rohrbach, S Venugopalan, K Saenko, T Darrell, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJ. Donahue, L. Anne Hendricks, S. Guadarrama, M. Rohrbach, S. Venugopalan, K. Saenko, and T. Dar- rell. Long-term recurrent convolutional networks for visual recognition and description. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2625-2634, 2015. 3\n\nDocument segmentation using relative location features. F C Fern\u00e1ndez, O R Terrades, Pattern Recognition (ICPR), 2012 21st International Conference on. IEEEF. C. Fern\u00e1ndez and O. R. Terrades. Document segmenta- tion using relative location features. In Pattern Recognition (ICPR), 2012 21st International Conference on, pages 1562- 1565. IEEE, 2012. 8\n\nLogical structure descriptions of segmented document images. J L Fisher, Proceedings of International Con ference on Document Analysis and Recognition. International Con ference on Document Analysis and RecognitionJ. L. Fisher. Logical structure descriptions of segmented doc- ument images. Proceedings of International Con ference on Document Analysis and Recognition, pages 302-310, 1991. 2\n\nDevise: A deep visual-semantic embedding model. A Frome, G S Corrado, J Shlens, S Bengio, J Dean, T Mikolov, Advances in neural information processing systems. A. Frome, G. S. Corrado, J. Shlens, S. Bengio, J. Dean, T. Mikolov, et al. Devise: A deep visual-semantic embed- ding model. In Advances in neural information processing systems, pages 2121-2129, 2013. 3\n\nAre you talking to a machine? dataset and methods for multilingual image question. H Gao, J Mao, J Zhou, Z Huang, L Wang, W Xu, Advances in Neural Information Processing Systems. H. Gao, J. Mao, J. Zhou, Z. Huang, L. Wang, and W. Xu. Are you talking to a machine? dataset and methods for mul- tilingual image question. In Advances in Neural Information Processing Systems, pages 2296-2304, 2015. 3\n\nDocument page decomposition by the bounding-box project. J Ha, R M Haralick, I T Phillips, Proceedings of the Third International Conference on. the Third International Conference onIEEE2Document Analysis and RecognitionJ. Ha, R. M. Haralick, and I. T. Phillips. Document page decomposition by the bounding-box project. In Document Analysis and Recognition, 1995., Proceedings of the Third International Conference on, volume 2, pages 1119-1122. IEEE, 1995. 1, 2\n\nRecursive xy cut using bounding boxes of connected components. J Ha, R M Haralick, I T Phillips, Proceedings of the Third International Conference on. the Third International Conference onIEEE2Document Analysis and RecognitionJ. Ha, R. M. Haralick, and I. T. Phillips. Recursive xy cut using bounding boxes of connected components. In Doc- ument Analysis and Recognition, 1995., Proceedings of the Third International Conference on, volume 2, pages 952- 955. IEEE, 1995. 1, 2\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, arXiv:1512.03385arXiv preprintK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn- ing for image recognition. arXiv preprint arXiv:1512.03385, 2015. 3\n\nA top-down document analysis method for logical structure recognition. R Ingold, D , Proceedings of International Conference on Document Analysis and Recognition. International Conference on Document Analysis and RecognitionR. Ingold and D. Armangil. A top-down document analysis method for logical structure recognition. In Proceedings of International Conference on Document Analysis and Recog- nition, pages 41-49, 1991. 2\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, arXiv:1502.03167arXiv preprintS. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167, 2015. 5\n\nDeep unordered composition rivals syntactic methods for text classification. M Iyyer, V Manjunatha, J Boyd-Graber, H Daum\u00e9, Proceedings of the Association for Computational Linguistics. the Association for Computational LinguisticsM. Iyyer, V. Manjunatha, J. Boyd-Graber, and H. Daum\u00e9 III. Deep unordered composition rivals syntactic methods for text classification. In Proceedings of the Association for Computational Linguistics, 2015. 4\n\nBag of tricks for efficient text classification. A Joulin, E Grave, P Bojanowski, T Mikolov, arXiv:1607.01759arXiv preprintA. Joulin, E. Grave, P. Bojanowski, and T. Mikolov. Bag of tricks for efficient text classification. arXiv preprint arXiv:1607.01759, 2016. 4\n\nDeep visual-semantic alignments for generating image descriptions. A Karpathy, L Fei-Fei, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionA. Karpathy and L. Fei-Fei. Deep visual-semantic align- ments for generating image descriptions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3128-3137, 2015. 3\n\nSyntactic segmentation and labeling of digitized pages from technical journals. M Krishnamoorthy, G Nagy, S Seth, M Viswanathan, IEEE Transactions on Pattern Analysis and Machine Intelligence. 157M. Krishnamoorthy, G. Nagy, S. Seth, and M. Viswanathan. Syntactic segmentation and labeling of digitized pages from technical journals. IEEE Transactions on Pattern Analysis and Machine Intelligence, 15(7):737-747, 1993. 2\n\nA fast and efficient method for extracting text paragraphs and graphics from unconstrained documents. F Lebourgeois, Z Bublinski, H Emptoz, Pattern Recognition, 1992. Vol. II. Conference B: Pattern Recognition Methodology and Systems, Proceedings., 11th IAPR International Conference on. IEEE1F. Lebourgeois, Z. Bublinski, and H. Emptoz. A fast and efficient method for extracting text paragraphs and graph- ics from unconstrained documents. In Pattern Recognition, 1992. Vol. II. Conference B: Pattern Recognition Method- ology and Systems, Proceedings., 11th IAPR International Conference on, pages 272-276. IEEE, 1992. 1, 2\n\nUwisl document image analysis toolbox: An experimental environment. J Liang, R Rogers, R M Haralick, I T Phillips, Proceedings of the Fourth International Conference on. the Fourth International Conference onIEEEDocument Analysis and RecognitionJ. Liang, R. Rogers, R. M. Haralick, and I. T. Phillips. Uw- isl document image analysis toolbox: An experimental en- vironment. In Document Analysis and Recognition, 1997., Proceedings of the Fourth International Conference on, vol- ume 2, pages 984-988. IEEE, 1997. 2\n\nMicrosoft coco: Common objects in context. T.-Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Doll\u00e1r, C L Zitnick, European Conference on Computer Vision. Springer511T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra- manan, P. Doll\u00e1r, and C. L. Zitnick. Microsoft coco: Com- mon objects in context. In European Conference on Com- puter Vision, pages 740-755. Springer, 2014. 3, 5, 11\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJ. Long, E. Shelhamer, and T. Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 3431-3440, 2015. 3\n\nLearning from weak and noisy labels for semantic segmentation. Z Lu, Z Fu, T Xiang, P Han, L Wang, X Gao, Z. Lu, Z. Fu, T. Xiang, P. Han, L. Wang, and X. Gao. Learn- ing from weak and noisy labels for semantic segmentation. 2016. 3\n\nLogical structure recovery in scholarly articles with rich document features. Multimedia Storage and Retrieval Innovations for Digital Library Systems. M.-T Luong, T D Nguyen, M.-Y Kan, 270M.-T. Luong, T. D. Nguyen, and M.-Y. Kan. Logical struc- ture recovery in scholarly articles with rich document fea- tures. Multimedia Storage and Retrieval Innovations for Dig- ital Library Systems, 270, 2012. 2, 6, 8\n\nSupervised dictionary learning. J Mairal, J Ponce, G Sapiro, A Zisserman, F R Bach, Advances in neural information processing systems. J. Mairal, J. Ponce, G. Sapiro, A. Zisserman, and F. R. Bach. Supervised dictionary learning. In Advances in neural infor- mation processing systems, pages 1033-1040, 2009. 3\n\nAsk your neurons: A neural-based approach to answering questions about images. M Malinowski, M Rohrbach, M Fritz, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionM. Malinowski, M. Rohrbach, and M. Fritz. Ask your neu- rons: A neural-based approach to answering questions about images. In Proceedings of the IEEE International Confer- ence on Computer Vision, pages 1-9, 2015. 3\n\nDocument structure analysis algorithms: a literature survey. S Mao, A Rosenfeld, T Kanungo, Electronic Imaging 2003. S. Mao, A. Rosenfeld, and T. Kanungo. Document structure analysis algorithms: a literature survey. In Electronic Imag- ing 2003, pages 197-207. International Society for Optics and Photonics, 2003. 2\n\nEfficient estimation of word representations in vector space. T Mikolov, K Chen, G Corrado, J Dean, arXiv:1301.3781arXiv preprintT. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013. 4\n\nDistributed representations of words and phrases and their compositionality. T Mikolov, I Sutskever, K Chen, G S Corrado, J Dean, Advances in neural information processing systems. T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural informa- tion processing systems, pages 3111-3119, 2013. 4\n\nLearning deconvolution network for semantic segmentation. H Noh, S Hong, B Han, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionH. Noh, S. Hong, and B. Han. Learning deconvolution net- work for semantic segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pages 1520- 1528, 2015. 3\n\nWeakly-and semi-supervised learning of a dcnn for semantic image segmentation. G Papandreou, L.-C Chen, K Murphy, A L Yuille, arXiv:1502.02734arXiv preprintG. Papandreou, L.-C. Chen, K. Murphy, and A. L. Yuille. Weakly-and semi-supervised learning of a dcnn for seman- tic image segmentation. arXiv preprint arXiv:1502.02734, 2015. 3\n\nLearning to refine object segments. P O Pinheiro, T.-Y Lin, R Collobert, P Doll\u00e1r, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)P. O. Pinheiro, T.-Y. Lin, R. Collobert, and P. Doll\u00e1r. Learn- ing to refine object segments. In Proceedings of the Euro- pean Conference on Computer Vision (ECCV), 2016. 3\n\nMediateam document database ii. A CD-ROM collection of document images, University of Oulu Finland. J Sauvola, H Kauniskangas, 25J. Sauvola and H. Kauniskangas. Mediateam document database ii. A CD-ROM collection of document images, Uni- versity of Oulu Finland, 1999. 2, 5\n\nLearning nongenerative grammatical models for document analysis. M Shilman, P Liang, P Viola, Tenth IEEE International Conference on Computer Vision (ICCV'05. 1M. Shilman, P. Liang, and P. Viola. Learning nongenerative grammatical models for document analysis. In Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1, volume 2, pages 962-969. IEEE, 2005. 2\n\nA fast algorithm for bottom-up document layout analysis. A Simon, J.-C Pret, A P Johnson, IEEE Transactions on Pattern Analysis and Machine Intelligence. 193A. Simon, J.-C. Pret, and A. P. Johnson. A fast algorithm for bottom-up document layout analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 19(3):273-277, 1997. 1, 2\n\nAn overview of the tesseract ocr engine. R Smith, 7R. Smith. An overview of the tesseract ocr engine. 2007. 2, 6, 7\n\nZero-shot learning through cross-modal transfer. R Socher, M Ganjoo, C D Manning, A Ng, Advances in neural information processing systems. R. Socher, M. Ganjoo, C. D. Manning, and A. Ng. Zero-shot learning through cross-modal transfer. In Advances in neural information processing systems, pages 935-943, 2013. 3\n\nGoing deeper with convolutions. C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, D Anguelov, D Erhan, V Vanhoucke, A Rabinovich, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionC. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich. Going deeper with convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1-9, 2015. 3\n\nThe uva color document dataset. L Todoran, M Worring, A W Smeulders, International Journal of Document Analysis and Recognition (IJDAR). 745L. Todoran, M. Worring, and A. W. Smeulders. The uva color document dataset. International Journal of Document Analysis and Recognition (IJDAR), 7(4):228-240, 2005. 2, 5\n\nExtracting and composing robust features with denoising autoencoders. P Vincent, H Larochelle, Y Bengio, P.-A Manzagol, Proceedings of the 25th international conference on Machine learning. the 25th international conference on Machine learningACMP. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting and composing robust features with denoising au- toencoders. In Proceedings of the 25th international confer- ence on Machine learning, pages 1096-1103. ACM, 2008. 3\n\nDense prediction for text line segmentation in handwritten document images. Q N Vo, G Lee, Image Processing (ICIP), 2016 IEEE International Conference on. Q. N. Vo and G. Lee. Dense prediction for text line segmen- tation in handwritten document images. In Image Process- ing (ICIP), 2016 IEEE International Conference on, pages 3264-3268. IEEE, 2016. 2\n\nA discriminative feature learning approach for deep face recognition. Y Wen, K Zhang, Z Li, Y Qiao, European Conference on Computer Vision. SpringerY. Wen, K. Zhang, Z. Li, and Y. Qiao. A discrimina- tive feature learning approach for deep face recognition. In European Conference on Computer Vision, pages 499-515. Springer, 2016. 3\n\nF Yu, V Koltun, arXiv:1511.07122Multi-scale context aggregation by dilated convolutions. arXiv preprintF. Yu and V. Koltun. Multi-scale context aggregation by di- lated convolutions. arXiv preprint arXiv:1511.07122, 2015. 3\n\nM D Zeiler, arXiv:1212.5701Adadelta: an adaptive learning rate method. arXiv preprintM. D. Zeiler. Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701, 2012. 5\n\nWeakly supervised semantic segmentation for social images. W Zhang, S Zeng, D Wang, X Xue, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionW. Zhang, S. Zeng, D. Wang, and X. Xue. Weakly super- vised semantic segmentation for social images. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2718-2726, 2015. 3\n\nAugmenting supervised neural networks with unsupervised objectives for large-scale image classification. Y Zhang, E K Lee, E H Lee, U Edu, Y. Zhang, E. K. Lee, E. H. Lee, and U. EDU. Augmenting supervised neural networks with unsupervised objectives for large-scale image classification. 4\n\nStacked what-where auto-encoders. J Zhao, M Mathieu, R Goroshin, Y Lecun, arXiv:1506.0235134arXiv preprintJ. Zhao, M. Mathieu, R. Goroshin, and Y. Lecun. Stacked what-where auto-encoders. arXiv preprint arXiv:1506.02351, 2015. 3, 4\n", "annotations": {"author": "[{\"end\":156,\"start\":110},{\"end\":202,\"start\":157},{\"end\":249,\"start\":203},{\"end\":303,\"start\":250},{\"end\":372,\"start\":304},{\"end\":439,\"start\":373}]", "publisher": null, "author_last_name": "[{\"end\":119,\"start\":115},{\"end\":168,\"start\":163},{\"end\":214,\"start\":208},{\"end\":261,\"start\":255},{\"end\":316,\"start\":311},{\"end\":384,\"start\":379}]", "author_first_name": "[{\"end\":114,\"start\":110},{\"end\":162,\"start\":157},{\"end\":207,\"start\":203},{\"end\":254,\"start\":250},{\"end\":310,\"start\":304},{\"end\":374,\"start\":373},{\"end\":378,\"start\":375}]", "author_affiliation": "[{\"end\":155,\"start\":121},{\"end\":201,\"start\":186},{\"end\":248,\"start\":233},{\"end\":302,\"start\":281},{\"end\":371,\"start\":337},{\"end\":438,\"start\":404}]", "title": "[{\"end\":107,\"start\":1},{\"end\":546,\"start\":440}]", "venue": null, "abstract": "[{\"end\":1396,\"start\":548}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2864,\"start\":2860},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2867,\"start\":2864},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2870,\"start\":2867},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":2873,\"start\":2870},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2875,\"start\":2873},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":4603,\"start\":4599},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4605,\"start\":4603},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4607,\"start\":4605},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4765,\"start\":4761},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":4768,\"start\":4765},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":4771,\"start\":4768},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4773,\"start\":4771},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6038,\"start\":6034},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6041,\"start\":6038},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6044,\"start\":6041},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":6047,\"start\":6044},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6049,\"start\":6047},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":6052,\"start\":6049},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6139,\"start\":6135},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":6142,\"start\":6139},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6144,\"start\":6142},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6435,\"start\":6431},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6438,\"start\":6435},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6661,\"start\":6657},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6788,\"start\":6784},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6924,\"start\":6920},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":6951,\"start\":6947},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7321,\"start\":7317},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7369,\"start\":7365},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7372,\"start\":7369},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7375,\"start\":7372},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7378,\"start\":7375},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7495,\"start\":7491},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7619,\"start\":7615},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7939,\"start\":7935},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8044,\"start\":8040},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8120,\"start\":8116},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8123,\"start\":8120},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8126,\"start\":8123},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":8129,\"start\":8126},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8337,\"start\":8333},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8456,\"start\":8452},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":8620,\"start\":8616},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":8641,\"start\":8637},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8644,\"start\":8641},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8923,\"start\":8919},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":8926,\"start\":8923},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":8929,\"start\":8926},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9306,\"start\":9302},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":9443,\"start\":9439},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":9618,\"start\":9614},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":9896,\"start\":9892},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10370,\"start\":10366},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10373,\"start\":10370},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10404,\"start\":10401},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":10407,\"start\":10404},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":10410,\"start\":10407},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10438,\"start\":10434},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":10441,\"start\":10438},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10444,\"start\":10441},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":11483,\"start\":11479},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12052,\"start\":12048},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12662,\"start\":12658},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":13188,\"start\":13184},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":13217,\"start\":13213},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14123,\"start\":14119},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":14152,\"start\":14148},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":14823,\"start\":14819},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":14826,\"start\":14823},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":16055,\"start\":16051},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":16058,\"start\":16055},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":18654,\"start\":18650},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":18657,\"start\":18654},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":18659,\"start\":18657},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":19507,\"start\":19503},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":19955,\"start\":19952},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":21222,\"start\":21218},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":21494,\"start\":21490},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22194,\"start\":22191},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":22279,\"start\":22275},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":22302,\"start\":22298},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":22803,\"start\":22800},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":22819,\"start\":22815},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":22869,\"start\":22866},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":22945,\"start\":22942},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":23117,\"start\":23113},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":26048,\"start\":26044},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":26941,\"start\":26937},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":28209,\"start\":28206},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":28239,\"start\":28235},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":29897,\"start\":29893},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":29920,\"start\":29917},{\"end\":31643,\"start\":31641},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":32000,\"start\":31996},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":32317,\"start\":32314},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":33516,\"start\":33513},{\"end\":34227,\"start\":34225},{\"end\":34382,\"start\":34380},{\"end\":34397,\"start\":34395},{\"end\":34432,\"start\":34430}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":35093,\"start\":34760},{\"attributes\":{\"id\":\"fig_1\"},\"end\":35306,\"start\":35094},{\"attributes\":{\"id\":\"fig_2\"},\"end\":35526,\"start\":35307},{\"attributes\":{\"id\":\"fig_3\"},\"end\":35736,\"start\":35527},{\"attributes\":{\"id\":\"fig_4\"},\"end\":35852,\"start\":35737},{\"attributes\":{\"id\":\"fig_6\"},\"end\":36112,\"start\":35853},{\"attributes\":{\"id\":\"fig_7\"},\"end\":36512,\"start\":36113},{\"attributes\":{\"id\":\"fig_8\"},\"end\":36857,\"start\":36513},{\"attributes\":{\"id\":\"fig_9\"},\"end\":37204,\"start\":36858},{\"attributes\":{\"id\":\"fig_10\"},\"end\":37368,\"start\":37205},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":38085,\"start\":37369},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":38540,\"start\":38086},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":38878,\"start\":38541}]", "paragraph": "[{\"end\":2020,\"start\":1412},{\"end\":2473,\"start\":2022},{\"end\":2876,\"start\":2475},{\"end\":3347,\"start\":2878},{\"end\":4608,\"start\":3349},{\"end\":5385,\"start\":4610},{\"end\":5436,\"start\":5387},{\"end\":5646,\"start\":5438},{\"end\":5806,\"start\":5648},{\"end\":5960,\"start\":5808},{\"end\":6662,\"start\":5975},{\"end\":7160,\"start\":6664},{\"end\":7886,\"start\":7162},{\"end\":8785,\"start\":7888},{\"end\":9161,\"start\":8787},{\"end\":9879,\"start\":9163},{\"end\":10289,\"start\":9881},{\"end\":10774,\"start\":10291},{\"end\":11249,\"start\":10785},{\"end\":11565,\"start\":11292},{\"end\":12784,\"start\":11567},{\"end\":13424,\"start\":12786},{\"end\":13789,\"start\":13447},{\"end\":14747,\"start\":13791},{\"end\":15230,\"start\":14749},{\"end\":15444,\"start\":15275},{\"end\":15589,\"start\":15501},{\"end\":15893,\"start\":15612},{\"end\":16271,\"start\":15895},{\"end\":16628,\"start\":16273},{\"end\":17245,\"start\":16694},{\"end\":17683,\"start\":17247},{\"end\":17843,\"start\":17772},{\"end\":17996,\"start\":17845},{\"end\":18178,\"start\":18120},{\"end\":18406,\"start\":18229},{\"end\":18873,\"start\":18434},{\"end\":19002,\"start\":18875},{\"end\":19868,\"start\":19004},{\"end\":19956,\"start\":19870},{\"end\":20103,\"start\":19958},{\"end\":20187,\"start\":20105},{\"end\":20371,\"start\":20189},{\"end\":20658,\"start\":20373},{\"end\":21294,\"start\":20660},{\"end\":22056,\"start\":21321},{\"end\":22321,\"start\":22058},{\"end\":22734,\"start\":22323},{\"end\":23726,\"start\":22750},{\"end\":24103,\"start\":23728},{\"end\":24547,\"start\":24149},{\"end\":24873,\"start\":24549},{\"end\":25134,\"start\":24875},{\"end\":25611,\"start\":25136},{\"end\":26384,\"start\":25613},{\"end\":26903,\"start\":26415},{\"end\":27938,\"start\":26905},{\"end\":27983,\"start\":27940},{\"end\":28170,\"start\":27985},{\"end\":28371,\"start\":28182},{\"end\":29316,\"start\":28403},{\"end\":29834,\"start\":29347},{\"end\":30381,\"start\":29836},{\"end\":31131,\"start\":30396},{\"end\":31894,\"start\":31162},{\"end\":32080,\"start\":31940},{\"end\":32230,\"start\":32082},{\"end\":32318,\"start\":32232},{\"end\":32460,\"start\":32320},{\"end\":32586,\"start\":32462},{\"end\":32770,\"start\":32588},{\"end\":33085,\"start\":32772},{\"end\":33295,\"start\":33129},{\"end\":33981,\"start\":33318},{\"end\":34262,\"start\":34026},{\"end\":34759,\"start\":34264}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":15274,\"start\":15231},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15500,\"start\":15445},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16693,\"start\":16629},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17771,\"start\":17684},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18119,\"start\":17997},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18228,\"start\":18179}]", "table_ref": "[{\"end\":24264,\"start\":24257},{\"end\":24578,\"start\":24570},{\"end\":26720,\"start\":26713},{\"end\":27308,\"start\":27301},{\"end\":27714,\"start\":27707},{\"end\":28002,\"start\":27995},{\"end\":28251,\"start\":28244},{\"end\":28757,\"start\":28750},{\"end\":29037,\"start\":29030},{\"end\":29388,\"start\":29380},{\"end\":29878,\"start\":29869}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1410,\"start\":1398},{\"attributes\":{\"n\":\"2.\"},\"end\":5973,\"start\":5963},{\"attributes\":{\"n\":\"3.\"},\"end\":10783,\"start\":10777},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11290,\"start\":11252},{\"attributes\":{\"n\":\"3.2.\"},\"end\":13445,\"start\":13427},{\"attributes\":{\"n\":\"3.3.\"},\"end\":15610,\"start\":15592},{\"attributes\":{\"n\":\"4.\"},\"end\":18432,\"start\":18409},{\"attributes\":{\"n\":\"5.\"},\"end\":21319,\"start\":21297},{\"attributes\":{\"n\":\"6.\"},\"end\":22748,\"start\":22737},{\"attributes\":{\"n\":\"6.1.\"},\"end\":24147,\"start\":24106},{\"attributes\":{\"n\":\"6.2.\"},\"end\":26413,\"start\":26387},{\"end\":28180,\"start\":28173},{\"attributes\":{\"n\":\"6.3.\"},\"end\":28401,\"start\":28374},{\"attributes\":{\"n\":\"6.4.\"},\"end\":29345,\"start\":29319},{\"attributes\":{\"n\":\"7.\"},\"end\":30394,\"start\":30384},{\"end\":31160,\"start\":31134},{\"end\":31938,\"start\":31897},{\"end\":33127,\"start\":33088},{\"end\":33316,\"start\":33298},{\"end\":34024,\"start\":33984},{\"end\":34771,\"start\":34761},{\"end\":35105,\"start\":35095},{\"end\":35318,\"start\":35308},{\"end\":35538,\"start\":35528},{\"end\":35748,\"start\":35738},{\"end\":35864,\"start\":35854},{\"end\":36124,\"start\":36114},{\"end\":36524,\"start\":36514},{\"end\":36870,\"start\":36859},{\"end\":37217,\"start\":37206}]", "table": "[{\"end\":38085,\"start\":37427},{\"end\":38540,\"start\":38292},{\"end\":38878,\"start\":38863}]", "figure_caption": "[{\"end\":35093,\"start\":34773},{\"end\":35306,\"start\":35107},{\"end\":35526,\"start\":35320},{\"end\":35736,\"start\":35540},{\"end\":35852,\"start\":35750},{\"end\":36112,\"start\":35866},{\"end\":36512,\"start\":36126},{\"end\":36857,\"start\":36526},{\"end\":37204,\"start\":36873},{\"end\":37368,\"start\":37220},{\"end\":37427,\"start\":37371},{\"end\":38292,\"start\":38088},{\"end\":38863,\"start\":38543}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2976,\"start\":2966},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3188,\"start\":3178},{\"end\":3745,\"start\":3739},{\"end\":3814,\"start\":3806},{\"end\":11310,\"start\":11304},{\"end\":12601,\"start\":12595},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13016,\"start\":13010},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13289,\"start\":13283},{\"end\":14745,\"start\":14739},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14834,\"start\":14828},{\"end\":16111,\"start\":16105},{\"end\":18364,\"start\":18358},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20781,\"start\":20775},{\"end\":20929,\"start\":20923},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":25814,\"start\":25801},{\"end\":26165,\"start\":26157},{\"end\":26637,\"start\":26631},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":31806,\"start\":31798},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":33038,\"start\":33030},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":33911,\"start\":33903}]", "bib_author_first_name": "[{\"end\":39142,\"start\":39137},{\"end\":39414,\"start\":39413},{\"end\":39422,\"start\":39421},{\"end\":39672,\"start\":39671},{\"end\":39681,\"start\":39680},{\"end\":39692,\"start\":39691},{\"end\":39698,\"start\":39697},{\"end\":39710,\"start\":39709},{\"end\":39719,\"start\":39718},{\"end\":39728,\"start\":39720},{\"end\":39739,\"start\":39738},{\"end\":40166,\"start\":40165},{\"end\":40184,\"start\":40183},{\"end\":40195,\"start\":40194},{\"end\":40211,\"start\":40210},{\"end\":40637,\"start\":40636},{\"end\":40655,\"start\":40654},{\"end\":40667,\"start\":40666},{\"end\":40683,\"start\":40682},{\"end\":41082,\"start\":41081},{\"end\":41084,\"start\":41083},{\"end\":41097,\"start\":41096},{\"end\":41231,\"start\":41230},{\"end\":41245,\"start\":41244},{\"end\":41254,\"start\":41253},{\"end\":41264,\"start\":41263},{\"end\":41580,\"start\":41579},{\"end\":41582,\"start\":41581},{\"end\":41593,\"start\":41592},{\"end\":41604,\"start\":41603},{\"end\":41606,\"start\":41605},{\"end\":42004,\"start\":42003},{\"end\":42021,\"start\":42017},{\"end\":42029,\"start\":42028},{\"end\":42037,\"start\":42036},{\"end\":42295,\"start\":42294},{\"end\":42303,\"start\":42302},{\"end\":42313,\"start\":42312},{\"end\":42324,\"start\":42323},{\"end\":42337,\"start\":42336},{\"end\":42769,\"start\":42765},{\"end\":42777,\"start\":42776},{\"end\":42791,\"start\":42790},{\"end\":42803,\"start\":42802},{\"end\":42813,\"start\":42812},{\"end\":42815,\"start\":42814},{\"end\":43139,\"start\":43138},{\"end\":43532,\"start\":43531},{\"end\":43542,\"start\":43541},{\"end\":43748,\"start\":43747},{\"end\":43759,\"start\":43758},{\"end\":43772,\"start\":43771},{\"end\":43786,\"start\":43785},{\"end\":43798,\"start\":43797},{\"end\":43813,\"start\":43812},{\"end\":43823,\"start\":43822},{\"end\":44326,\"start\":44325},{\"end\":44328,\"start\":44327},{\"end\":44341,\"start\":44340},{\"end\":44343,\"start\":44342},{\"end\":44684,\"start\":44683},{\"end\":44686,\"start\":44685},{\"end\":45065,\"start\":45064},{\"end\":45074,\"start\":45073},{\"end\":45076,\"start\":45075},{\"end\":45087,\"start\":45086},{\"end\":45097,\"start\":45096},{\"end\":45107,\"start\":45106},{\"end\":45115,\"start\":45114},{\"end\":45465,\"start\":45464},{\"end\":45472,\"start\":45471},{\"end\":45479,\"start\":45478},{\"end\":45487,\"start\":45486},{\"end\":45496,\"start\":45495},{\"end\":45504,\"start\":45503},{\"end\":45838,\"start\":45837},{\"end\":45844,\"start\":45843},{\"end\":45846,\"start\":45845},{\"end\":45858,\"start\":45857},{\"end\":45860,\"start\":45859},{\"end\":46308,\"start\":46307},{\"end\":46314,\"start\":46313},{\"end\":46316,\"start\":46315},{\"end\":46328,\"start\":46327},{\"end\":46330,\"start\":46329},{\"end\":46768,\"start\":46767},{\"end\":46774,\"start\":46773},{\"end\":46783,\"start\":46782},{\"end\":46790,\"start\":46789},{\"end\":47025,\"start\":47024},{\"end\":47035,\"start\":47034},{\"end\":47475,\"start\":47474},{\"end\":47484,\"start\":47483},{\"end\":47763,\"start\":47762},{\"end\":47772,\"start\":47771},{\"end\":47786,\"start\":47785},{\"end\":47801,\"start\":47800},{\"end\":48176,\"start\":48175},{\"end\":48186,\"start\":48185},{\"end\":48195,\"start\":48194},{\"end\":48209,\"start\":48208},{\"end\":48460,\"start\":48459},{\"end\":48472,\"start\":48471},{\"end\":48909,\"start\":48908},{\"end\":48927,\"start\":48926},{\"end\":48935,\"start\":48934},{\"end\":48943,\"start\":48942},{\"end\":49352,\"start\":49351},{\"end\":49367,\"start\":49366},{\"end\":49380,\"start\":49379},{\"end\":49946,\"start\":49945},{\"end\":49955,\"start\":49954},{\"end\":49965,\"start\":49964},{\"end\":49967,\"start\":49966},{\"end\":49979,\"start\":49978},{\"end\":49981,\"start\":49980},{\"end\":50440,\"start\":50436},{\"end\":50447,\"start\":50446},{\"end\":50456,\"start\":50455},{\"end\":50468,\"start\":50467},{\"end\":50476,\"start\":50475},{\"end\":50486,\"start\":50485},{\"end\":50497,\"start\":50496},{\"end\":50507,\"start\":50506},{\"end\":50509,\"start\":50508},{\"end\":50856,\"start\":50855},{\"end\":50864,\"start\":50863},{\"end\":50877,\"start\":50876},{\"end\":51297,\"start\":51296},{\"end\":51303,\"start\":51302},{\"end\":51309,\"start\":51308},{\"end\":51318,\"start\":51317},{\"end\":51325,\"start\":51324},{\"end\":51333,\"start\":51332},{\"end\":51622,\"start\":51618},{\"end\":51631,\"start\":51630},{\"end\":51633,\"start\":51632},{\"end\":51646,\"start\":51642},{\"end\":51908,\"start\":51907},{\"end\":51918,\"start\":51917},{\"end\":51927,\"start\":51926},{\"end\":51937,\"start\":51936},{\"end\":51950,\"start\":51949},{\"end\":51952,\"start\":51951},{\"end\":52266,\"start\":52265},{\"end\":52280,\"start\":52279},{\"end\":52292,\"start\":52291},{\"end\":52700,\"start\":52699},{\"end\":52707,\"start\":52706},{\"end\":52720,\"start\":52719},{\"end\":53019,\"start\":53018},{\"end\":53030,\"start\":53029},{\"end\":53038,\"start\":53037},{\"end\":53049,\"start\":53048},{\"end\":53312,\"start\":53311},{\"end\":53323,\"start\":53322},{\"end\":53336,\"start\":53335},{\"end\":53344,\"start\":53343},{\"end\":53346,\"start\":53345},{\"end\":53357,\"start\":53356},{\"end\":53696,\"start\":53695},{\"end\":53703,\"start\":53702},{\"end\":53711,\"start\":53710},{\"end\":54106,\"start\":54105},{\"end\":54123,\"start\":54119},{\"end\":54131,\"start\":54130},{\"end\":54141,\"start\":54140},{\"end\":54143,\"start\":54142},{\"end\":54398,\"start\":54397},{\"end\":54400,\"start\":54399},{\"end\":54415,\"start\":54411},{\"end\":54422,\"start\":54421},{\"end\":54435,\"start\":54434},{\"end\":54834,\"start\":54833},{\"end\":54845,\"start\":54844},{\"end\":55074,\"start\":55073},{\"end\":55085,\"start\":55084},{\"end\":55094,\"start\":55093},{\"end\":55445,\"start\":55444},{\"end\":55457,\"start\":55453},{\"end\":55465,\"start\":55464},{\"end\":55467,\"start\":55466},{\"end\":55775,\"start\":55774},{\"end\":55900,\"start\":55899},{\"end\":55910,\"start\":55909},{\"end\":55920,\"start\":55919},{\"end\":55922,\"start\":55921},{\"end\":55933,\"start\":55932},{\"end\":56197,\"start\":56196},{\"end\":56208,\"start\":56207},{\"end\":56215,\"start\":56214},{\"end\":56222,\"start\":56221},{\"end\":56234,\"start\":56233},{\"end\":56242,\"start\":56241},{\"end\":56254,\"start\":56253},{\"end\":56263,\"start\":56262},{\"end\":56276,\"start\":56275},{\"end\":56703,\"start\":56702},{\"end\":56714,\"start\":56713},{\"end\":56725,\"start\":56724},{\"end\":56727,\"start\":56726},{\"end\":57052,\"start\":57051},{\"end\":57063,\"start\":57062},{\"end\":57077,\"start\":57076},{\"end\":57090,\"start\":57086},{\"end\":57540,\"start\":57539},{\"end\":57542,\"start\":57541},{\"end\":57548,\"start\":57547},{\"end\":57889,\"start\":57888},{\"end\":57896,\"start\":57895},{\"end\":57905,\"start\":57904},{\"end\":57911,\"start\":57910},{\"end\":58154,\"start\":58153},{\"end\":58160,\"start\":58159},{\"end\":58379,\"start\":58378},{\"end\":58381,\"start\":58380},{\"end\":58622,\"start\":58621},{\"end\":58631,\"start\":58630},{\"end\":58639,\"start\":58638},{\"end\":58647,\"start\":58646},{\"end\":59111,\"start\":59110},{\"end\":59120,\"start\":59119},{\"end\":59122,\"start\":59121},{\"end\":59129,\"start\":59128},{\"end\":59131,\"start\":59130},{\"end\":59138,\"start\":59137},{\"end\":59331,\"start\":59330},{\"end\":59339,\"start\":59338},{\"end\":59350,\"start\":59349},{\"end\":59362,\"start\":59361}]", "bib_author_last_name": "[{\"end\":39099,\"start\":39094},{\"end\":39150,\"start\":39143},{\"end\":39238,\"start\":39230},{\"end\":39290,\"start\":39281},{\"end\":39419,\"start\":39415},{\"end\":39427,\"start\":39423},{\"end\":39678,\"start\":39673},{\"end\":39689,\"start\":39682},{\"end\":39695,\"start\":39693},{\"end\":39707,\"start\":39699},{\"end\":39716,\"start\":39711},{\"end\":39736,\"start\":39729},{\"end\":39746,\"start\":39740},{\"end\":40181,\"start\":40167},{\"end\":40192,\"start\":40185},{\"end\":40208,\"start\":40196},{\"end\":40224,\"start\":40212},{\"end\":40652,\"start\":40638},{\"end\":40664,\"start\":40656},{\"end\":40680,\"start\":40668},{\"end\":40696,\"start\":40684},{\"end\":41094,\"start\":41085},{\"end\":41105,\"start\":41098},{\"end\":41242,\"start\":41232},{\"end\":41251,\"start\":41246},{\"end\":41261,\"start\":41255},{\"end\":41272,\"start\":41265},{\"end\":41590,\"start\":41583},{\"end\":41601,\"start\":41594},{\"end\":41613,\"start\":41607},{\"end\":42015,\"start\":42005},{\"end\":42026,\"start\":42022},{\"end\":42034,\"start\":42030},{\"end\":42041,\"start\":42038},{\"end\":42300,\"start\":42296},{\"end\":42310,\"start\":42304},{\"end\":42321,\"start\":42314},{\"end\":42334,\"start\":42325},{\"end\":42344,\"start\":42338},{\"end\":42774,\"start\":42770},{\"end\":42788,\"start\":42778},{\"end\":42800,\"start\":42792},{\"end\":42810,\"start\":42804},{\"end\":42822,\"start\":42816},{\"end\":43146,\"start\":43140},{\"end\":43539,\"start\":43533},{\"end\":43549,\"start\":43543},{\"end\":43756,\"start\":43749},{\"end\":43769,\"start\":43760},{\"end\":43783,\"start\":43773},{\"end\":43795,\"start\":43787},{\"end\":43810,\"start\":43799},{\"end\":43820,\"start\":43814},{\"end\":43831,\"start\":43824},{\"end\":44338,\"start\":44329},{\"end\":44352,\"start\":44344},{\"end\":44693,\"start\":44687},{\"end\":45071,\"start\":45066},{\"end\":45084,\"start\":45077},{\"end\":45094,\"start\":45088},{\"end\":45104,\"start\":45098},{\"end\":45112,\"start\":45108},{\"end\":45123,\"start\":45116},{\"end\":45469,\"start\":45466},{\"end\":45476,\"start\":45473},{\"end\":45484,\"start\":45480},{\"end\":45493,\"start\":45488},{\"end\":45501,\"start\":45497},{\"end\":45507,\"start\":45505},{\"end\":45841,\"start\":45839},{\"end\":45855,\"start\":45847},{\"end\":45869,\"start\":45861},{\"end\":46311,\"start\":46309},{\"end\":46325,\"start\":46317},{\"end\":46339,\"start\":46331},{\"end\":46771,\"start\":46769},{\"end\":46780,\"start\":46775},{\"end\":46787,\"start\":46784},{\"end\":46794,\"start\":46791},{\"end\":47032,\"start\":47026},{\"end\":47481,\"start\":47476},{\"end\":47492,\"start\":47485},{\"end\":47769,\"start\":47764},{\"end\":47783,\"start\":47773},{\"end\":47798,\"start\":47787},{\"end\":47807,\"start\":47802},{\"end\":48183,\"start\":48177},{\"end\":48192,\"start\":48187},{\"end\":48206,\"start\":48196},{\"end\":48217,\"start\":48210},{\"end\":48469,\"start\":48461},{\"end\":48480,\"start\":48473},{\"end\":48924,\"start\":48910},{\"end\":48932,\"start\":48928},{\"end\":48940,\"start\":48936},{\"end\":48955,\"start\":48944},{\"end\":49364,\"start\":49353},{\"end\":49377,\"start\":49368},{\"end\":49387,\"start\":49381},{\"end\":49952,\"start\":49947},{\"end\":49962,\"start\":49956},{\"end\":49976,\"start\":49968},{\"end\":49990,\"start\":49982},{\"end\":50444,\"start\":50441},{\"end\":50453,\"start\":50448},{\"end\":50465,\"start\":50457},{\"end\":50473,\"start\":50469},{\"end\":50483,\"start\":50477},{\"end\":50494,\"start\":50487},{\"end\":50504,\"start\":50498},{\"end\":50517,\"start\":50510},{\"end\":50861,\"start\":50857},{\"end\":50874,\"start\":50865},{\"end\":50885,\"start\":50878},{\"end\":51300,\"start\":51298},{\"end\":51306,\"start\":51304},{\"end\":51315,\"start\":51310},{\"end\":51322,\"start\":51319},{\"end\":51330,\"start\":51326},{\"end\":51337,\"start\":51334},{\"end\":51628,\"start\":51623},{\"end\":51640,\"start\":51634},{\"end\":51650,\"start\":51647},{\"end\":51915,\"start\":51909},{\"end\":51924,\"start\":51919},{\"end\":51934,\"start\":51928},{\"end\":51947,\"start\":51938},{\"end\":51957,\"start\":51953},{\"end\":52277,\"start\":52267},{\"end\":52289,\"start\":52281},{\"end\":52298,\"start\":52293},{\"end\":52704,\"start\":52701},{\"end\":52717,\"start\":52708},{\"end\":52728,\"start\":52721},{\"end\":53027,\"start\":53020},{\"end\":53035,\"start\":53031},{\"end\":53046,\"start\":53039},{\"end\":53054,\"start\":53050},{\"end\":53320,\"start\":53313},{\"end\":53333,\"start\":53324},{\"end\":53341,\"start\":53337},{\"end\":53354,\"start\":53347},{\"end\":53362,\"start\":53358},{\"end\":53700,\"start\":53697},{\"end\":53708,\"start\":53704},{\"end\":53715,\"start\":53712},{\"end\":54117,\"start\":54107},{\"end\":54128,\"start\":54124},{\"end\":54138,\"start\":54132},{\"end\":54150,\"start\":54144},{\"end\":54409,\"start\":54401},{\"end\":54419,\"start\":54416},{\"end\":54432,\"start\":54423},{\"end\":54442,\"start\":54436},{\"end\":54842,\"start\":54835},{\"end\":54858,\"start\":54846},{\"end\":55082,\"start\":55075},{\"end\":55091,\"start\":55086},{\"end\":55100,\"start\":55095},{\"end\":55451,\"start\":55446},{\"end\":55462,\"start\":55458},{\"end\":55475,\"start\":55468},{\"end\":55781,\"start\":55776},{\"end\":55907,\"start\":55901},{\"end\":55917,\"start\":55911},{\"end\":55930,\"start\":55923},{\"end\":55936,\"start\":55934},{\"end\":56205,\"start\":56198},{\"end\":56212,\"start\":56209},{\"end\":56219,\"start\":56216},{\"end\":56231,\"start\":56223},{\"end\":56239,\"start\":56235},{\"end\":56251,\"start\":56243},{\"end\":56260,\"start\":56255},{\"end\":56273,\"start\":56264},{\"end\":56287,\"start\":56277},{\"end\":56711,\"start\":56704},{\"end\":56722,\"start\":56715},{\"end\":56737,\"start\":56728},{\"end\":57060,\"start\":57053},{\"end\":57074,\"start\":57064},{\"end\":57084,\"start\":57078},{\"end\":57099,\"start\":57091},{\"end\":57545,\"start\":57543},{\"end\":57552,\"start\":57549},{\"end\":57893,\"start\":57890},{\"end\":57902,\"start\":57897},{\"end\":57908,\"start\":57906},{\"end\":57916,\"start\":57912},{\"end\":58157,\"start\":58155},{\"end\":58167,\"start\":58161},{\"end\":58388,\"start\":58382},{\"end\":58628,\"start\":58623},{\"end\":58636,\"start\":58632},{\"end\":58644,\"start\":58640},{\"end\":58651,\"start\":58648},{\"end\":59117,\"start\":59112},{\"end\":59126,\"start\":59123},{\"end\":59135,\"start\":59132},{\"end\":59142,\"start\":59139},{\"end\":59336,\"start\":59332},{\"end\":59347,\"start\":59340},{\"end\":59359,\"start\":59351},{\"end\":59368,\"start\":59363}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":39133,\"start\":39092},{\"attributes\":{\"id\":\"b1\"},\"end\":39226,\"start\":39135},{\"attributes\":{\"id\":\"b2\"},\"end\":39277,\"start\":39228},{\"attributes\":{\"id\":\"b3\"},\"end\":39344,\"start\":39279},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":13020550},\"end\":39637,\"start\":39346},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":3180429},\"end\":40087,\"start\":39639},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":11850358},\"end\":40553,\"start\":40089},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":206777599},\"end\":41050,\"start\":40555},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":18138884},\"end\":41228,\"start\":41052},{\"attributes\":{\"doi\":\"arXiv:1607.04606\",\"id\":\"b9\"},\"end\":41496,\"start\":41230},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":2551062},\"end\":41953,\"start\":41498},{\"attributes\":{\"doi\":\"arXiv:1603.00550\",\"id\":\"b11\"},\"end\":42211,\"start\":41955},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":9814021},\"end\":42680,\"start\":42213},{\"attributes\":{\"doi\":\"arXiv:1412.7062\",\"id\":\"b13\"},\"end\":43051,\"start\":42682},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":42019065},\"end\":43504,\"start\":43053},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":52874011},\"end\":43662,\"start\":43506},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":5736847},\"end\":44267,\"start\":43664},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":11671554},\"end\":44620,\"start\":44269},{\"attributes\":{\"id\":\"b18\"},\"end\":45014,\"start\":44622},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":261138},\"end\":45379,\"start\":45016},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":209217},\"end\":45778,\"start\":45381},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":3361432},\"end\":46242,\"start\":45780},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":17179084},\"end\":46719,\"start\":46244},{\"attributes\":{\"doi\":\"arXiv:1512.03385\",\"id\":\"b23\"},\"end\":46951,\"start\":46721},{\"attributes\":{\"id\":\"b24\"},\"end\":47378,\"start\":46953},{\"attributes\":{\"doi\":\"arXiv:1502.03167\",\"id\":\"b25\"},\"end\":47683,\"start\":47380},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":216848261},\"end\":48124,\"start\":47685},{\"attributes\":{\"doi\":\"arXiv:1607.01759\",\"id\":\"b27\"},\"end\":48390,\"start\":48126},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":8517067},\"end\":48826,\"start\":48392},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":16107554},\"end\":49247,\"start\":48828},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":62602509},\"end\":49875,\"start\":49249},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":7869564},\"end\":50391,\"start\":49877},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":14113767},\"end\":50797,\"start\":50393},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":1629541},\"end\":51231,\"start\":50799},{\"attributes\":{\"id\":\"b34\"},\"end\":51464,\"start\":51233},{\"attributes\":{\"id\":\"b35\"},\"end\":51873,\"start\":51466},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":682971},\"end\":52184,\"start\":51875},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":738850},\"end\":52636,\"start\":52186},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":6128200},\"end\":52954,\"start\":52638},{\"attributes\":{\"doi\":\"arXiv:1301.3781\",\"id\":\"b39\"},\"end\":53232,\"start\":52956},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":16447573},\"end\":53635,\"start\":53234},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":623137},\"end\":54024,\"start\":53637},{\"attributes\":{\"doi\":\"arXiv:1502.02734\",\"id\":\"b42\"},\"end\":54359,\"start\":54026},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":15278025},\"end\":54731,\"start\":54361},{\"attributes\":{\"id\":\"b44\"},\"end\":55006,\"start\":54733},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":10072745},\"end\":55385,\"start\":55008},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":29276706},\"end\":55731,\"start\":55387},{\"attributes\":{\"id\":\"b47\"},\"end\":55848,\"start\":55733},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":2808203},\"end\":56162,\"start\":55850},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":206592484},\"end\":56668,\"start\":56164},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":8774176},\"end\":56979,\"start\":56670},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":207168299},\"end\":57461,\"start\":56981},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":1610874},\"end\":57816,\"start\":57463},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":4711865},\"end\":58151,\"start\":57818},{\"attributes\":{\"doi\":\"arXiv:1511.07122\",\"id\":\"b54\"},\"end\":58376,\"start\":58153},{\"attributes\":{\"doi\":\"arXiv:1212.5701\",\"id\":\"b55\"},\"end\":58560,\"start\":58378},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":6558539},\"end\":59003,\"start\":58562},{\"attributes\":{\"id\":\"b57\"},\"end\":59294,\"start\":59005},{\"attributes\":{\"doi\":\"arXiv:1506.02351\",\"id\":\"b58\"},\"end\":59527,\"start\":59296}]", "bib_title": "[{\"end\":39411,\"start\":39346},{\"end\":39669,\"start\":39639},{\"end\":40163,\"start\":40089},{\"end\":40634,\"start\":40555},{\"end\":41079,\"start\":41052},{\"end\":41577,\"start\":41498},{\"end\":42292,\"start\":42213},{\"end\":43136,\"start\":43053},{\"end\":43529,\"start\":43506},{\"end\":43745,\"start\":43664},{\"end\":44323,\"start\":44269},{\"end\":44681,\"start\":44622},{\"end\":45062,\"start\":45016},{\"end\":45462,\"start\":45381},{\"end\":45835,\"start\":45780},{\"end\":46305,\"start\":46244},{\"end\":47022,\"start\":46953},{\"end\":47760,\"start\":47685},{\"end\":48457,\"start\":48392},{\"end\":48906,\"start\":48828},{\"end\":49349,\"start\":49249},{\"end\":49943,\"start\":49877},{\"end\":50434,\"start\":50393},{\"end\":50853,\"start\":50799},{\"end\":51905,\"start\":51875},{\"end\":52263,\"start\":52186},{\"end\":52697,\"start\":52638},{\"end\":53309,\"start\":53234},{\"end\":53693,\"start\":53637},{\"end\":54395,\"start\":54361},{\"end\":55071,\"start\":55008},{\"end\":55442,\"start\":55387},{\"end\":55897,\"start\":55850},{\"end\":56194,\"start\":56164},{\"end\":56700,\"start\":56670},{\"end\":57049,\"start\":56981},{\"end\":57537,\"start\":57463},{\"end\":57886,\"start\":57818},{\"end\":58619,\"start\":58562}]", "bib_author": "[{\"end\":39101,\"start\":39094},{\"end\":39152,\"start\":39137},{\"end\":39240,\"start\":39230},{\"end\":39292,\"start\":39281},{\"end\":39421,\"start\":39413},{\"end\":39429,\"start\":39421},{\"end\":39680,\"start\":39671},{\"end\":39691,\"start\":39680},{\"end\":39697,\"start\":39691},{\"end\":39709,\"start\":39697},{\"end\":39718,\"start\":39709},{\"end\":39738,\"start\":39718},{\"end\":39748,\"start\":39738},{\"end\":40183,\"start\":40165},{\"end\":40194,\"start\":40183},{\"end\":40210,\"start\":40194},{\"end\":40226,\"start\":40210},{\"end\":40654,\"start\":40636},{\"end\":40666,\"start\":40654},{\"end\":40682,\"start\":40666},{\"end\":40698,\"start\":40682},{\"end\":41096,\"start\":41081},{\"end\":41107,\"start\":41096},{\"end\":41244,\"start\":41230},{\"end\":41253,\"start\":41244},{\"end\":41263,\"start\":41253},{\"end\":41274,\"start\":41263},{\"end\":41592,\"start\":41579},{\"end\":41603,\"start\":41592},{\"end\":41615,\"start\":41603},{\"end\":42017,\"start\":42003},{\"end\":42028,\"start\":42017},{\"end\":42036,\"start\":42028},{\"end\":42043,\"start\":42036},{\"end\":42302,\"start\":42294},{\"end\":42312,\"start\":42302},{\"end\":42323,\"start\":42312},{\"end\":42336,\"start\":42323},{\"end\":42346,\"start\":42336},{\"end\":42776,\"start\":42765},{\"end\":42790,\"start\":42776},{\"end\":42802,\"start\":42790},{\"end\":42812,\"start\":42802},{\"end\":42824,\"start\":42812},{\"end\":43148,\"start\":43138},{\"end\":43541,\"start\":43531},{\"end\":43551,\"start\":43541},{\"end\":43758,\"start\":43747},{\"end\":43771,\"start\":43758},{\"end\":43785,\"start\":43771},{\"end\":43797,\"start\":43785},{\"end\":43812,\"start\":43797},{\"end\":43822,\"start\":43812},{\"end\":43833,\"start\":43822},{\"end\":44340,\"start\":44325},{\"end\":44354,\"start\":44340},{\"end\":44695,\"start\":44683},{\"end\":45073,\"start\":45064},{\"end\":45086,\"start\":45073},{\"end\":45096,\"start\":45086},{\"end\":45106,\"start\":45096},{\"end\":45114,\"start\":45106},{\"end\":45125,\"start\":45114},{\"end\":45471,\"start\":45464},{\"end\":45478,\"start\":45471},{\"end\":45486,\"start\":45478},{\"end\":45495,\"start\":45486},{\"end\":45503,\"start\":45495},{\"end\":45509,\"start\":45503},{\"end\":45843,\"start\":45837},{\"end\":45857,\"start\":45843},{\"end\":45871,\"start\":45857},{\"end\":46313,\"start\":46307},{\"end\":46327,\"start\":46313},{\"end\":46341,\"start\":46327},{\"end\":46773,\"start\":46767},{\"end\":46782,\"start\":46773},{\"end\":46789,\"start\":46782},{\"end\":46796,\"start\":46789},{\"end\":47034,\"start\":47024},{\"end\":47038,\"start\":47034},{\"end\":47483,\"start\":47474},{\"end\":47494,\"start\":47483},{\"end\":47771,\"start\":47762},{\"end\":47785,\"start\":47771},{\"end\":47800,\"start\":47785},{\"end\":47809,\"start\":47800},{\"end\":48185,\"start\":48175},{\"end\":48194,\"start\":48185},{\"end\":48208,\"start\":48194},{\"end\":48219,\"start\":48208},{\"end\":48471,\"start\":48459},{\"end\":48482,\"start\":48471},{\"end\":48926,\"start\":48908},{\"end\":48934,\"start\":48926},{\"end\":48942,\"start\":48934},{\"end\":48957,\"start\":48942},{\"end\":49366,\"start\":49351},{\"end\":49379,\"start\":49366},{\"end\":49389,\"start\":49379},{\"end\":49954,\"start\":49945},{\"end\":49964,\"start\":49954},{\"end\":49978,\"start\":49964},{\"end\":49992,\"start\":49978},{\"end\":50446,\"start\":50436},{\"end\":50455,\"start\":50446},{\"end\":50467,\"start\":50455},{\"end\":50475,\"start\":50467},{\"end\":50485,\"start\":50475},{\"end\":50496,\"start\":50485},{\"end\":50506,\"start\":50496},{\"end\":50519,\"start\":50506},{\"end\":50863,\"start\":50855},{\"end\":50876,\"start\":50863},{\"end\":50887,\"start\":50876},{\"end\":51302,\"start\":51296},{\"end\":51308,\"start\":51302},{\"end\":51317,\"start\":51308},{\"end\":51324,\"start\":51317},{\"end\":51332,\"start\":51324},{\"end\":51339,\"start\":51332},{\"end\":51630,\"start\":51618},{\"end\":51642,\"start\":51630},{\"end\":51652,\"start\":51642},{\"end\":51917,\"start\":51907},{\"end\":51926,\"start\":51917},{\"end\":51936,\"start\":51926},{\"end\":51949,\"start\":51936},{\"end\":51959,\"start\":51949},{\"end\":52279,\"start\":52265},{\"end\":52291,\"start\":52279},{\"end\":52300,\"start\":52291},{\"end\":52706,\"start\":52699},{\"end\":52719,\"start\":52706},{\"end\":52730,\"start\":52719},{\"end\":53029,\"start\":53018},{\"end\":53037,\"start\":53029},{\"end\":53048,\"start\":53037},{\"end\":53056,\"start\":53048},{\"end\":53322,\"start\":53311},{\"end\":53335,\"start\":53322},{\"end\":53343,\"start\":53335},{\"end\":53356,\"start\":53343},{\"end\":53364,\"start\":53356},{\"end\":53702,\"start\":53695},{\"end\":53710,\"start\":53702},{\"end\":53717,\"start\":53710},{\"end\":54119,\"start\":54105},{\"end\":54130,\"start\":54119},{\"end\":54140,\"start\":54130},{\"end\":54152,\"start\":54140},{\"end\":54411,\"start\":54397},{\"end\":54421,\"start\":54411},{\"end\":54434,\"start\":54421},{\"end\":54444,\"start\":54434},{\"end\":54844,\"start\":54833},{\"end\":54860,\"start\":54844},{\"end\":55084,\"start\":55073},{\"end\":55093,\"start\":55084},{\"end\":55102,\"start\":55093},{\"end\":55453,\"start\":55444},{\"end\":55464,\"start\":55453},{\"end\":55477,\"start\":55464},{\"end\":55783,\"start\":55774},{\"end\":55909,\"start\":55899},{\"end\":55919,\"start\":55909},{\"end\":55932,\"start\":55919},{\"end\":55938,\"start\":55932},{\"end\":56207,\"start\":56196},{\"end\":56214,\"start\":56207},{\"end\":56221,\"start\":56214},{\"end\":56233,\"start\":56221},{\"end\":56241,\"start\":56233},{\"end\":56253,\"start\":56241},{\"end\":56262,\"start\":56253},{\"end\":56275,\"start\":56262},{\"end\":56289,\"start\":56275},{\"end\":56713,\"start\":56702},{\"end\":56724,\"start\":56713},{\"end\":56739,\"start\":56724},{\"end\":57062,\"start\":57051},{\"end\":57076,\"start\":57062},{\"end\":57086,\"start\":57076},{\"end\":57101,\"start\":57086},{\"end\":57547,\"start\":57539},{\"end\":57554,\"start\":57547},{\"end\":57895,\"start\":57888},{\"end\":57904,\"start\":57895},{\"end\":57910,\"start\":57904},{\"end\":57918,\"start\":57910},{\"end\":58159,\"start\":58153},{\"end\":58169,\"start\":58159},{\"end\":58390,\"start\":58378},{\"end\":58630,\"start\":58621},{\"end\":58638,\"start\":58630},{\"end\":58646,\"start\":58638},{\"end\":58653,\"start\":58646},{\"end\":59119,\"start\":59110},{\"end\":59128,\"start\":59119},{\"end\":59137,\"start\":59128},{\"end\":59144,\"start\":59137},{\"end\":59338,\"start\":59330},{\"end\":59349,\"start\":59338},{\"end\":59361,\"start\":59349},{\"end\":59370,\"start\":59361}]", "bib_venue": "[{\"end\":39472,\"start\":39429},{\"end\":39815,\"start\":39748},{\"end\":40292,\"start\":40226},{\"end\":40778,\"start\":40698},{\"end\":41130,\"start\":41107},{\"end\":41337,\"start\":41290},{\"end\":41712,\"start\":41615},{\"end\":42001,\"start\":41955},{\"end\":42387,\"start\":42346},{\"end\":42763,\"start\":42682},{\"end\":43201,\"start\":43148},{\"end\":43567,\"start\":43551},{\"end\":43910,\"start\":43833},{\"end\":44419,\"start\":44354},{\"end\":44772,\"start\":44695},{\"end\":45174,\"start\":45125},{\"end\":45558,\"start\":45509},{\"end\":45923,\"start\":45871},{\"end\":46393,\"start\":46341},{\"end\":46765,\"start\":46721},{\"end\":47114,\"start\":47038},{\"end\":47472,\"start\":47380},{\"end\":47869,\"start\":47809},{\"end\":48173,\"start\":48126},{\"end\":48559,\"start\":48482},{\"end\":49019,\"start\":48957},{\"end\":49535,\"start\":49389},{\"end\":50045,\"start\":49992},{\"end\":50557,\"start\":50519},{\"end\":50964,\"start\":50887},{\"end\":51294,\"start\":51233},{\"end\":51616,\"start\":51466},{\"end\":52008,\"start\":51959},{\"end\":52367,\"start\":52300},{\"end\":52753,\"start\":52730},{\"end\":53016,\"start\":52956},{\"end\":53413,\"start\":53364},{\"end\":53784,\"start\":53717},{\"end\":54103,\"start\":54026},{\"end\":54508,\"start\":54444},{\"end\":54831,\"start\":54733},{\"end\":55165,\"start\":55102},{\"end\":55539,\"start\":55477},{\"end\":55772,\"start\":55733},{\"end\":55987,\"start\":55938},{\"end\":56366,\"start\":56289},{\"end\":56805,\"start\":56739},{\"end\":57169,\"start\":57101},{\"end\":57616,\"start\":57554},{\"end\":57956,\"start\":57918},{\"end\":58240,\"start\":58185},{\"end\":58447,\"start\":58405},{\"end\":58730,\"start\":58653},{\"end\":59108,\"start\":59005},{\"end\":59328,\"start\":59296},{\"end\":39869,\"start\":39817},{\"end\":43241,\"start\":43203},{\"end\":43974,\"start\":43912},{\"end\":44836,\"start\":44774},{\"end\":45962,\"start\":45925},{\"end\":46432,\"start\":46395},{\"end\":47177,\"start\":47116},{\"end\":47916,\"start\":47871},{\"end\":48623,\"start\":48561},{\"end\":50085,\"start\":50047},{\"end\":51028,\"start\":50966},{\"end\":52421,\"start\":52369},{\"end\":53838,\"start\":53786},{\"end\":54559,\"start\":54510},{\"end\":56430,\"start\":56368},{\"end\":57224,\"start\":57171},{\"end\":58794,\"start\":58732}]"}}}, "year": 2023, "month": 12, "day": 17}