{"id": 202539519, "updated": "2023-10-06 23:02:23.31", "metadata": {"title": "KG-BERT: BERT for Knowledge Graph Completion", "authors": "[{\"first\":\"Liang\",\"last\":\"Yao\",\"middle\":[]},{\"first\":\"Chengsheng\",\"last\":\"Mao\",\"middle\":[]},{\"first\":\"Yuan\",\"last\":\"Luo\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2019, "month": 9, "day": 7}, "abstract": "Knowledge graphs are important resources for many artificial intelligence tasks but often suffer from incompleteness. In this work, we propose to use pre-trained language models for knowledge graph completion. We treat triples in knowledge graphs as textual sequences and propose a novel framework named Knowledge Graph Bidirectional Encoder Representations from Transformer (KG-BERT) to model these triples. Our method takes entity and relation descriptions of a triple as input and computes scoring function of the triple with the KG-BERT language model. Experimental results on multiple benchmark knowledge graphs show that our method can achieve state-of-the-art performance in triple classification, link prediction and relation prediction tasks.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1909.03193", "mag": "2972167903", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-1909-03193", "doi": null}}, "content": {"source": {"pdf_hash": "31184789ef4c3084af930b1e0dede3215b4a9240", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1909.03193v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "4dee009b5b768b0fc7689aa6a224277b5a91dc12", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/31184789ef4c3084af930b1e0dede3215b4a9240.txt", "contents": "\nKG-BERT: BERT for Knowledge Graph Completion\n\n\nLiang Yao liang.yao@northwestern.edu \nNorthwestern University Chicago IL\n60611\n\nChengsheng Mao chengsheng.mao@northwestern.edu \nNorthwestern University Chicago IL\n60611\n\nYuan Luo yuan.luo@northwestern.edu \nNorthwestern University Chicago IL\n60611\n\nKG-BERT: BERT for Knowledge Graph Completion\n\nKnowledge graphs are important resources for many artificial intelligence tasks but often suffer from incompleteness. In this work, we propose to use pre-trained language models for knowledge graph completion. We treat triples in knowledge graphs as textual sequences and propose a novel framework named Knowledge Graph Bidirectional Encoder Representations from Transformer (KG-BERT) to model these triples. Our method takes entity and relation descriptions of a triple as input and computes scoring function of the triple with the KG-BERT language model. Experimental results on multiple benchmark knowledge graphs show that our method can achieve state-of-the-art performance in triple classification, link prediction and relation prediction tasks.\n\nIntroduction\n\nLarge-scale knowledge graphs (KG) such as FreeBase (Bollacker et al. 2008), YAGO (Suchanek, Kasneci, and Weikum 2007) and WordNet (Miller 1995) provide effective basis for many important AI tasks such as semantic search, recommendation (Zhang et al. 2016) and question answering (Cui et al. 2017). A KG is typically a multi-relational graph containing entities as nodes and relations as edges. Each edge is represented as a triplet (head entity, relation, tail entity) ((h, r, t) for short), indicating the relation between two entities, e.g., (Steve Jobs, founded, Apple Inc.). Despite their effectiveness, knowledge graphs are still far from being complete. This problem motivates the task of knowledge graph completion, which is targeted at assessing the plausibility of triples not present in a knowledge graph.\n\nMuch research work has been devoted to knowledge graph completion. A common approach is called knowledge graph embedding which represents entities and relations in triples as real-valued vectors and assess triples' plausibility with these vectors . However, most knowledge graph embedding models only use structure information in observed triple facts, which suffer from the sparseness of knowledge graphs. Some recent studies incorporate textual information to enrich knowledge repre-sentation (Socher et al. 2013;Xiao et al. 2017), but they learn unique text embedding for the same entity/relation in different triples, which ignore contextual information. For instance, different words in the description of Steve Jobs should have distinct importance weights connected to two relations \"founded\" and \"isCitizenOf\", the relation \"wroteMusicFor\" can have two different meanings \"writes lyrics\" and \"composes musical compositions\" given different entities. On the other hand, syntactic and semantic information in large-scale text data is not fully utilized, as they only employ entity descriptions, relation mentions or word co-occurrence with entities (Wang and Li 2016;Xu et al. 2017;An et al. 2018).\n\nRecently, pre-trained language models such as ELMo (Peters et al. 2018), GPT (Radford et al. 2018), BERT (Devlin et al. 2019) and XLNet (Yang et al. 2019) have shown great success in natural language processing (NLP), these models can learn contextualized word embeddings with large amount of free text data and achieve state-of-the-art performance in many language understanding tasks. Among them, BERT is the most prominent one by pre-training the bidirectional Transformer encoder through masked language modeling and next sentence prediction. It can capture rich linguistic knowledge in pre-trained model weights.\n\nIn this study, we propose a novel method for knowledge graph completion using pre-trained language models. Specifically, we first treat entities, relations and triples as textual sequences and turn knowledge graph completion into a sequence classification problem. We then fine-tune BERT model on these sequences for predicting the plausibility of a triple or a relation. The method can achieve strong performance in several KG completion tasks. Our source code is available at https://github.com/yao8839836/kg-bert. Our contributions are summarized as follows:\n\n\u2022 We propose a new language modeling method for knowledge graph completion. To the best of our knowledge, this is the first study to model triples' plausibility with a pretrained contextual language model.\n\n\u2022 Results on several benchmark datasets show that our method can achieve state-of-the-art results in triple classification, relation prediction and link prediction tasks.  . These methods can be classified into translational distance models and semantic matching models based on different scoring functions for a triple (h, r, t). Translational distance models use distance-based scoring functions. They assess the plausibility of a triple (h, r, t) by the distance between the two entity vectors h and t, typically after a translation performed by the relation vector r. The representative models are TransE (Bordes et al. 2013) and its extensions including TransH (Wang et al. 2014b). For TransE, the scoring function is defined as the negative translational distance f (h, r, t) = \u2212||h + r \u2212 t||. Semantic matching models employ similarity-based scoring functions. The representative models are RESCAL (Nickel, Tresp, and Kriegel 2011), DistMult (Yang et al. 2015) and their extensions. For Dist-Mult, the scoring function is defined as a bilinear function f (h, r, t) = h, r, t . Recently, convolutional neural networks also show promising results for knowledge graph completion (Dettmers et al. 2018;Nguyen et al. 2018a;Schlichtkrull et al. 2018). The above methods conduct knowledge graph completion using only structural information observed in triples, while different kinds of external information like entity types, logical rules and textual descriptions can be introduced to improve the performance ). For textual descriptions, (Socher et al. 2013) firstly represented entities by averaging the word embeddings contained in their names, where the word embeddings are learned from an external corpus. (Wang et al. 2014a) proposed to jointly embed entities and words into the same vector space by aligning Wikipedia anchors and entity names. ) use convolutional neural networks (CNN) to encode word sequences in entity descriptions.  proposed semantic space projection (SSP) which jointly learns topics and KG embeddings by characterizing the strong correlations between fact triples and textual descriptions. Despite their success, these models learn the same textual representations of entities and relations while words in entity/relation descriptions can have different meanings or importance weights in different triples.\n\nTo address the above problems, (Wang and Li 2016) presented a text-enhanced KG embedding model TEKE which can assign different embeddings to a relation in different triples. TEKE utilizes co-occurrences of entities and words in an entity-annotated text corpus. (Xu et al. 2017) used an LSTM encoder with attention mechanism to construct contextual text representations given different relations.  proposed an accurate text-enhanced KG embedding method by exploiting triple specific relation mentions and a mutual attention mechanism between relation mention and entity description. Although these methods can handle the semantic variety of entities and relations in distinct triples, they could not make full use of syntactic and semantic information in large scale free text data, as only entity descriptions, relation mentions and word co-occurrence with entities are utilized. Compared with these methods, our method can learn context-aware text embeddings with rich language information via pre-trained language models.\n\n\nLanguage Model Pre-training\n\nPre-trained language representation models can be divided into two categories: feature-based and fine tuning approaches. Traditional word embedding methods such as Word2Vec (Mikolov et al. 2013) and Glove (Pennington, Socher, and Manning 2014) aimed at adopting feature-based approaches to learn context-independent words vectors. ELMo (Peters et al. 2018) generalized traditional word embeddings to context-aware word embeddings, where word polysemy can be properly handled. Different from featurebased approaches, fine tuning approaches like GPT (Radford et al. 2018) and BERT (Devlin et al. 2019) used the pre-trained model architecture and parameters as a starting point for specific NLP tasks. The pre-trained models capture rich semantic patterns from free text. Recently, pre-trained language models have also been explored in the context of KG. (Wang, Kulkarni, and Wang 2018) learned contextual embeddings on entity-relation chains (sentences) generated from random walks in KG, then used the embeddings as initialization of KG embeddings models like TransE. (Zhang et al. 2019) incorporated informative entities in KG to enhance BERT language representation. (Bosselut et al. 2019) used GPT to generate tail phrase tokens given head phrases and relation types in a common sense knowledge base which does not cleanly fit into a schema comparing two entities with a known relation. The method focuses on generating new entities and relations. Unlike these studies, we use names or descriptions of entities and relations as input and fine-tune BERT to compute plausibility scores of triples.\n\n\nMethod Bidirectional Encoder Representations from Transformers (BERT)\n\nBERT (Devlin et al. 2019) is a state-of-the-art pre-trained contextual language representation model built on a multilayer bidirectional Transformer encoder (Vaswani et al. 2017). The Transformer encoder is based on self-attention mechanism. There are two steps in BERT framework: pre-training and fine-tuning. During pre-training, BERT is trained on large-scale unlabeled general domain corpus (3,300M words from BooksCorpus and English Wikipedia) over two self-supervised tasks: masked language modeling and next sentence prediction. In masked language modeling, BERT predicts randomly masked input tokens. In next sentence prediction, BERT predicts whether two input sentences are consecutive. For fine-tuning, BERT is initialized with the pre-trained parameter weights, and all of the parameters are fine-tuned using labeled data from downstream tasks such as sentence pair classification, question answering and sequence labeling.\n\n\nKnowledge Graph BERT (KG-BERT)\n\nTo take full advantage of contextual representation with rich language patterns, We fine tune pre-trained BERT for\nKG-BERT(a) [CLS] Tok h 1 ... Tok h a [SEP] Tok r 1 ... Tok r b [SEP] Tok t 1 ... Tok t c [SEP]\nHead Entity Relation Tail Entity\nE [CLS] E h 1 ... E h a E h [SEP] E r 1 ... E r b E r [SEP] E t 1 ... E t c E t [SEP] C Triple Label y \u2208 {0, 1} T h 1 ... T h a T h [SEP] T r 1 ... T r b T r [SEP] T t 1 ... T t c T t [SEP]\nFigure 1: Illustrations of fine-tuning KG-BERT for predicting the plausibility of a triple.\n\n\nKG-BERT(b)\n[CLS] Tok h 1 ... Tok h a [SEP] Tok t 1 ... Tok t c [SEP]\nHead Entity Tail Entity\nE [CLS] E h 1 ... E h a E r [SEP] E t 1 ... E t c E t [SEP] C Relation Label y \u2208 {1, . . . , R} T h 1 ... T h a T r [SEP] T t 1 ... T t c T t [SEP]\nFigure 2: Illustrations of fine-tuning KG-BERT for predicting the relation between two entities.\n\nknowledge graph completion. We represent entities and relations as their names or descriptions, then take the name/description word sequences as the input sentence of the BERT model for fine-tuning. As original BERT, a \"sentence\" can be an arbitrary span of contiguous text or word sequence, rather than an actual linguistic sentence. To model the plausibility of a triple, we packed the sentences of (h, r, t) as a single sequence. A sequence means the input token sequence to BERT, which may be two entity name/description sentences or three sentences of (h, r, t) packed together. The architecture of the KG-BERT for modeling triples is shown in Figure 1. We name this KG-BERT version KG-BERT(a). The first token of every input sequence is always a special classification token [CLS]. The head entity is represented as a sentence containing tokens Tok h 1 , ..., Tok h a , e.g., \"Steven Paul Jobs was an American business magnate, entrepreneur and investor.\" or \"Steve Jobs\", the relation is represented as a sentence containing tokens Tok r 1 , ..., Tok r b , e.g., \"founded\", the tail entity is represented as a sentence containing tokens Tok t 1 , ..., Tok t c , e.g., \"Apple Inc. is an American multinational technology company headquartered in Cupertino, California.\" or \"Apple Inc.\". The sentences of entities and relations are separated by a special token [SEP]. For a given token, its input representation is constructed by summing the corresponding token, segment and position embeddings. Different elements separated by [SEP] have different segment embeddings, the tokens in sentences of head and tail entity share the same segment embedding e A , while the tokens in relation sentence have a different segment embedding e B . Different tokens in the same position i \u2208 {1, 2, 3, . . . , 512} have a same position embedding. Each input token i has a input representation E i . The token representations are fed into the BERT model architecture which is a multi-layer bidirectional Transformer encoder based on the original implementation described in (Vaswani et al. 2017). The final hidden vector of the special [CLS] token and i-th input token are denoted as C \u2208 R H and T i \u2208 R H , where H is the hidden state size in pre-trained BERT. The final hidden state C corresponding to [CLS] is used as the aggregate sequence representation for computing triple scores. The only new parameters introduced during triple classification fine-tuning are classification layer weights W \u2208 R 2\u00d7H . The scoring function for a triple \u03c4 = (h, r, t) is s \u03c4 = f (h, r, t) = sigmoid(CW T ), s \u03c4 \u2208 R 2 is a 2-dimensional real vector with s \u03c4 0 , s \u03c4 1 \u2208 [0, 1] and s \u03c4 0 + s \u03c4 1 = 1. Given the positive triple set D + and a negative triple set D \u2212 constructed accordingly, we compute a cross-entropy loss with s \u03c4 and triple labels:\nL = \u2212 \u03c4 \u2208D + \u222aD \u2212 (y \u03c4 log(s \u03c4 0 ) + (1 \u2212 y \u03c4 ) log(s \u03c4 1 )) (1)\nwhere y \u03c4 \u2208 {0, 1} is the label (negative or positive) of that triple. The negative triple set D \u2212 is simply generated by replacing head entity h or tail entity t in a positive triple (h, r, t) \u2208 D + with a random entity h or t , i.e.,\nD \u2212 = {(h , r, t)|h \u2208 E \u2227 h = h \u2227 (h , r, t) / \u2208 D + } \u222a{(h, r, t )|t \u2208 E \u2227 t = t \u2227 (h, r, t ) / \u2208 D + }(2)\nwhere E is the set of entities. Note that a triple will not be treated as a negative example if it is already in positive set D + . The pre-trained parameter weights and new weights W can be updated via gradient descent. The architecture of the KG-BERT for predicting relations is shown in Figure 2. We name this KG-BERT version KG-BERT(b). We only use sentences of the two entities h and t to predict the relation r between them. In our preliminary experiment, we found predicting relations with two entities directly is better than using KG-BERT(a) with relation corruption, i.e., generating negative triples by replacing relation r with a random relation r . As KG-BERT(a), the final hidden state C corresponding to [CLS] is used as the representation of the two entities. The only new parameters introduced in relation prediction fine-tuning are classification layer weights W \u2208 R R\u00d7H , where R is the number of relations in a KG. The scoring function for a triple \u03c4 = (h, r, t)\nis s \u03c4 = f (h, r, t) = softmax(CW T ), s \u03c4 \u2208 R R is a R- dimensional real vector with s \u03c4 i \u2208 [0, 1] and R i s \u03c4 i = 1.\nWe compute the following cross-entropy loss with s \u03c4 and relation labels:\nL = \u2212 \u03c4 \u2208D + R i=1 y \u03c4 i log(s \u03c4 i )(3)\nwhere \u03c4 is an observed positive triple, y \u03c4 i is the relation indicator for the triple \u03c4 , y \u03c4 i = 1 when r = i and y \u03c4 i = 0 when r = i.\n\n\nExperiments\n\nIn this section we evaluate our KG-BERT on three experimental tasks. Specifically we want to determine: \u2022 Can our model judge whether an unseen triple fact (h, r, t) is true or not? \u2022 Can our model predict an entity given another entity and a specific relation? \u2022 Can our model predict relations given two entities?\n\nDatasets. We ran our experiments on six widely used benchmark KG datasets: WN11 (Socher et al. 2013), FB13 (Socher et al. 2013), FB15K (Bordes et al. 2013), WN18RR, FB15k-237 and UMLS (Dettmers et al. 2018   . For all datasets, we use relation names as relation sentences.\n\nBaselines. We compare our KG-BERT with multiple state-of-the-art KG embedding methods as follows: TransE and its extensions TransH Settings. We choose pre-trained BERT-Base model with 12 layers, 12 self-attention heads and H = 768 as the initialization of KG-BERT, then fine tune KG-BERT with Adam implemented in BERT. In our preliminary experiment, we found BERT-Base model can achieve better results than BERT-Large in general, and BERT-Base is simpler and less sensitive to hyper-parameter choices. Following original BERT, we set the following hyper-parameters in KG-BERT fine-tuning: batch size: 32, learning rate: 5e-5, dropout rate: 0.1. We also tried other values of these hyper-parameters in (Devlin et al. 2019) but didn't find much difference. We tuned number of epochs for different tasks: 3 for triple classification, 5 for link (entity) prediction and 20 for relation prediction. We found more epochs can lead to better results in relation prediction but not in other two tasks. For triple classification training, we sample 1 negative triple for a positive triple which can ensure class balance in binary classification. For link (entity) prediction training, we sample 5 negative triples for a positive triple, we tried 1, 3, 5 and 10 and found 5 is the best.  Triple Classification. Triple classification aims to judge whether a given triple (h, r, t) is correct or not. Table 2 presents triple classification accuracy of different methods on WN11 and FB13. We can see that KG-BERT(a) clearly outperforms all baselines by a large margin, which shows the effectiveness of our method. We ran our models 10 times and found the standard deviations are less than 0.2, and the improvements are significant (p < 0.01). To our knowledge, KG-BERT(a) achieves the best results so far. For more in-depth performance analysis, we note that TransE could not achieve high accuracy scores because it could not deal with 1-to-N, N-to-1, and N-to-N relations. TransH, TransR, TransD, TranSparse and TransG outperform TransE by introducing relation specific parameters. DistMult performs relatively well, and can also be improved by hierarchical relation structure information used in DistMult-HRS. Con-vKB shows decent results, which suggests that CNN models can capture global interactions among the entity and relation embeddings. DOLORES further improves ConvKB by incorporating contextual information in entity-relation random walk chains. NTN also achieves competitive performances especially on FB13, which means it's an expressive model, and representing entities with word embeddings is helpful. Other text-enhanced KG embeddings TEKE and AATE outperform their base models like TransE and TransH, which demonstrates the benefit of external text data. However, their improvements are still limited due to less utilization of rich language patterns. The improvement of KG-BERT(a) over baselines on WN11 is larger than FB13, because WordNet is a linguistic knowledge graph which is closer to linguistic patterns contained in pre-trained language models. Figure 3 reports triple classification accuracy with 5%, 10%, 15%, 20% and 30% of original WN11 and FB13 training triples. We note that KG-BERT(a) can achieve higher test accuracy with limited training triples. For instance, KG-BERT(a) achieves a test accuracy of 88.1% on FB13 with only 5% training triples and a test accuracy of 87.0% on WN11 with only 10% training triples which are higher than some baseline models (including text-enhanced models) with even the full training triples. These encouraging results suggest that KG-BERT(a) can fully utilize rich linguistic patterns in large external text data to overcome the sparseness of knowledge graphs.\n\nThe main reasons why KG-BERT(a) performs well are four fold: 1) The input sequence contains both entity and relation word sequences; 2) The triple classification task is very similar to next sentence prediction task in BERT pretraining which captures relationship between two sentences in large free text, thus the pre-trained BERT weights are well positioned for the inference of relationship among different elements in a triple; 3) The token hidden vectors are contextual embeddings. The same token can have different hidden vectors in different triples, thus contextual information is explicitly used. 4) The self-attention mechanism can discover the most important words connected to the triple fact.\n\nLink Prediction. The link (entity) prediction task predicts the head entity h given (?, r, t) or predicts the tail entity t given (h, r, ?) where ? means the missing element. The results are evaluated using a ranking produced by the scoring function f (h, r, t) (s \u03c4 0 in our method) on test triples. Each correct test triple (h, r, t) is corrupted by replacing either its head or tail entity with every entity e \u2208 E, then these candidates are ranked in descending order of their plausibility score. We report two common metrics, Mean Rank (MR) of correct entities and Hits@10 which means the proportion of correct entities in top 10. A lower MR is better while a higher Hits@10 is better. Following (Nguyen et al. 2018b), we only report results under the filtered setting (Bordes et al. 2013)    training, development, and test set before getting the ranking lists. Table 3 shows link prediction performance of various models. We test some classical baseline models with OpenKE toolkit ) 1 , other results are taken from the original papers. We can observe that: 1) KG-BERT(a) can achieve lower MR than baseline models, and it achieves the lowest mean ranks on WN18RR and FB15k-237 to our knowledge. 2) The Hits@10 scores of KG-BERT(a) is lower than some state-of-the-art methods. KG-BERT(a) can avoid very high ranks with semantic relatedness of entity and relation sentences, but the KG structure information is not explicitly modeled, thus it could not rank some neighbor entities of a given entity in top 10. CNN models ConvE and ConvKB perform better compared to the 1 https://github.com/thunlp/OpenKE graph convolutional network R-GCN. ComplEx could not perform well on WN18RR and FB15k-237, but can be improved using adversarial negative sampling in KBGAN and RotatE.\n\nRelation Prediction. This task predicts relations between two given entities, i.e., (h, ?, t). The procedure is similar to link prediction while we rank the candidates with the relation scores s \u03c4 . We evaluate the relation ranking using Mean Rank (MR) and Hits@1 with filtered setting. Table 4 reports relation prediction results on FB15K. We note that KG-BERT(b) also shows promising results and achieves the highest Hits@1 so far. The KG-BERT(b) is analogous to sentence pair classification in BERT finetuning and can also benefit from BERT pre-training. Textenhanced models DKRL and SSP can also outperform structure only methods TransE and TransH. TKRL and PTransE work well with hierarchical entity categories and extended path information. ProjE achieves very competitive results by treating KG completion as a ranking problem and optimizing ranking score vectors.\n\nAttention Visualization. We show attention patterns of KG-BERT in Figure 4 and Figure 5. We use the visualization tool released by (Vig 2019) 2 . Figure 4 depicts the attention patterns of KG-BERT(a). A positive training triple ( twenty dollar bill NN 1, hypernym, note NN 6) from WN18RR is taken as the example. The entity descriptions \"a United States bill worth 20 dollars\" and \"a piece of paper money\" as well as the relation name \"hypernym\" are used as the input sequence. We observe that some important words such as \"paper\" and \"money\" have higher attention scores connected to the label token [CLS], while some less related words like \"united\" and \"states\" obtain less attentions. On the other hand, we can see that different attention [SEP] is highlighted by the same six attention heads, \"a\" and \"piece\" are highlighted by the three same attention heads, while \"paper\" and \"money\" are highlighted by other four attention heads. As mentioned in (Vaswani et al. 2017), multi-head attention allows KG-BERT to jointly attend to information from different representation subspaces at different positions, different attention heads are concatenated to compute the final attention values. Figure 5 illustrates attention patterns of KG-BERT(b). The triple (20th century, /time/event/includes event, World War II) from FB15K is taken as input. We can see similar attention patterns as in KG-BERT(a), six attention heads attend to \"century\" in head entity, while other three attention heads focus on \"war\" and \"ii\" in tail entity. Multi-head attention can attend to different aspects of two entities in a triple.\n\nDiscussions. From experimental results, we note that KG-BERT can achieve strong performance in three KG completion tasks. However, a major limitation is that BERT model is expensive, which makes the link prediction evaluation very time consuming, link prediction evaluation needs to replace head or tail entity with almost all entities, and all corrupted triple sequences are fed into the 12 layer Transformer model. Possible solutions are introducing 1-N scoring models like ConvE or using lightweight language models. The example is taken from FB15K. Two entities 20th century and World War II are used as input, the relation label is /time/event/includes event.\n\n\nConclusion and Future Work\n\nIn this work, we propose a novel knowledge graph completion method termed Knowledge Graph BERT (KG-BERT). We represent entities and relations as their name/description textual sequences, and turn knowledge graph completion problem into a sequence classification problem. KG-BERT can make use of rich language information in large amount free text and highlight most important words connected to a triple. The proposed method demonstrates promising results by outperforming state-of-the-art results on multiple benchmark KG datasets.\n\nSome future directions include improving the results by jointly modeling textual information with KG structures, or utilizing pre-trained models with more text data like XLNet. And applying our KG-BERT as a knowledge-enhanced language model to language understanding tasks is an interesting future work we are going to explore.\n\n\n(Wang et al. 2014b), TransD(Ji et al. 2015), TransR(Lin et al. 2015b), TransG(Xiao, Huang, and Zhu 2016), TranSparse(Ji et al. 2016) and PTransE(Lin et al. 2015a), DistMult and its extension DistMult-HRS(Zhang et al. 2018) which only used structural information in KG. The neural tensor network NTN(Socher et al. 2013) and its simplified version ProjE(Shi and Weninger 2017). CNN models: Con-vKB(Nguyen et al. 2018a), ConvE(Dettmers et al. 2018) and R-GCN(Schlichtkrull et al. 2018). KG embeddings with textual information: TEKE(Wang and Li 2016), DKRL, SSP, AATE. KG embeddings with entity hierarchical types: TKRL. Contextualized KG embeddings: DOLORES(Wang, Kulkarni, and Wang 2018). Complex-valued KG embeddings ComplEx(Trouillon et al. 2016) andRotatE (Sun et al. 2019). Adversarial learning framework: KBGAN(Cai and Wang 2018).\n\nFigure 3 :\n3Test accuracy of triple classification by varying training data proportions.\n\nFigure 4 :\n4Illustrations of attention patterns of KG-BERT(a). A positive training triple ( twenty dollar bill NN 1, hypernym, note NN 6) from WN18RR is used as the example. Different colors mean different attention heads. Transparencies of colors reflect the attention scores. We show the attention weights between [CLS] and other tokens in layer 11 of the Transformer model. heads focus on different tokens.\n\nFigure 5 :\n5Illustrations of attention patterns of KG-BERT(b).\n\n\n). WN11 and WN18RR are two subsets of WordNet, FB15K and FB15k-237 are two subsets of Freebase. WordNet is aDataset \n# Ent \n# Rel # Train \n# Dev \n# Test \nWN11 \n38,696 \n11 \n112,581 \n2,609 \n10,544 \nFB13 \n75,043 \n13 \n316,232 \n5,908 \n23,733 \nWN18RR \n40,943 \n11 \n86,835 \n3,034 \n3,134 \nFB15K \n14,951 1,345 483,142 50,000 59,071 \nFB15k-237 14,541 \n237 \n272,115 17,535 20,466 \nUMLS \n135 \n46 \n5,216 \n652 \n661 \n\n\n\nTable 1 :\n1Summary statistics of datasets. For WN11, FB15K and UMLS, we use entity names as input sentences. For FB13, we use entity descriptions in Wikipedia as input sentences. For FB15k-237, we used entity descriptions made bylarge lexical KG of English where each entity as a synset \nwhich is consisting of several words and corresponds to a \ndistinct word sense. Freebase is a large knowledge graph of \ngeneral world facts. UMLS is a medical semantic network \ncontaining semantic types (entities) and semantic relations. \nThe test sets of WN11 and FB13 contain positive and neg-\native triplets which can be used for triple classification. The \ntest set of WN18RR, FB15K, FB15k-237 and UMLS only \ncontain correct triples, we perform link (entity) prediction \nand relation prediction on these datasets. Table 1 provides \nstatistics of all datasets we used. \nFor WN18RR, we use synsets definitions as entity sen-\ntences. \n\nTable 2 :\n2Triple classification accuracy (in percentage) for different embedding methods. The baseline results are obtained from corresponding papers.\n\n\nwhich removes all corrupted triples appeared inMethod \n\nWN18RR \nFB15k-237 \nUMLS \nMR Hits@10 MR Hits@10 MR Hits@10 \nTransE (our results) \n2365 \n50.5 \n223 \n47.4 \n1.84 \n98.9 \nTransH (our results) \n2524 \n50.3 \n255 \n48.6 \n1.80 \n99.5 \nTransR (our results) \n3166 \n50.7 \n237 \n51.1 \n1.81 \n99.4 \nTransD (our results) \n2768 \n50.7 \n246 \n48.4 \n1.71 \n99.3 \nDistMult (our results) \n3704 \n47.7 \n411 \n41.9 \n5.52 \n84.6 \nComplEx (our results) \n3921 \n48.3 \n508 \n43.4 \n2.59 \n96.7 \nConvE (Dettmers et al. 2018) \n5277 \n48 \n246 \n49.1 \n-\n-\nConvKB (Nguyen et al. 2018a) \n2554 \n52.5 \n257 \n51.7 \n-\n-\nR-GCN (Schlichtkrull et al. 2018) \n-\n-\n-\n41.7 \n-\n-\nKBGAN (Cai and Wang 2018) \n-\n48.1 \n-\n45.8 \n-\n-\nRotatE (Sun et al. 2019) \n3340 \n57.1 \n177 \n53.3 \n-\n-\nKG-BERT(a) \n97 \n52.4 \n153 \n42.0 \n1.47 \n99.0 \n\n\n\nTable 3 :\n3Link prediction results on WN18RR, FB15k-237 and UMLS datasets. The baseline models denoted (our results) are \nimplemented using OpenKE toolkit (Han et al. 2018), other baseline results are taken from the original papers. \n\nMethod \nMean Rank \nHits@1 \n\nTransE (Lin et al. 2015a) \n2.5 \n84.3 \n\nTransR (Xie, Liu, and Sun 2016) \n2.1 \n91.6 \n\nDKRL (CNN) (Xie et al. 2016) \n2.5 \n89.0 \n\nDKRL (CNN) + TransE (Xie et al. 2016) \n2.0 \n90.8 \n\nDKRL (CBOW) (Xie et al. 2016) \n2.5 \n82.7 \n\nTKRL (RHE) (Xie, Liu, and Sun 2016) \n1.7 \n92.8 \n\nTKRL (RHE) (Xie, Liu, and Sun 2016) \n1.8 \n92.5 \n\nPTransE (ADD, len-2 path) (Lin et al. 2015a) \n1.2 \n93.6 \n\nPTransE (RNN, len-2 path) (Lin et al. 2015a) \n1.4 \n93.2 \n\nPTransE (ADD, len-3 path) (Lin et al. 2015a) \n1.4 \n94.0 \n\nSSP (Xiao et al. 2017) \n1.2 \n-\n\nProjE (pointwise) (Shi and Weninger 2017) \n1.3 \n95.6 \n\nProjE (listwise) (Shi and Weninger 2017) \n1.2 \n95.7 \n\nProjE (wlistwise) (Shi and Weninger 2017) \n1.2 \n95.6 \n\nKG-BERT (b) \n1.2 \n96.0 \n\n\n\nTable 4 :\n4Relation prediction results on FB15K dataset. The baseline results are obtained from corresponding papers.\nhttps://github.com/jessevig/bertviz\n\nAccurate text-enhanced knowledge graph representation learning. B An, B Chen, X Han, L Sun, NAACL. An, B.; Chen, B.; Han, X.; and Sun, L. 2018. Accurate text-enhanced knowledge graph representation learning. In NAACL, 745-755.\n\nFreebase: a collaboratively created graph database for structuring human knowledge. K Bollacker, C Evans, P Paritosh, T Sturge, J Taylor, SIGMOD. Bollacker, K.; Evans, C.; Paritosh, P.; Sturge, T.; and Taylor, J. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In SIGMOD, 1247-1250.\n\nTranslating embeddings for modeling multi-relational data. A Bordes, N Usunier, A Garcia-Duran, J Weston, O Yakhnenko, NIPS. Bordes, A.; Usunier, N.; Garcia-Duran, A.; Weston, J.; and Yakhnenko, O. 2013. Translating embeddings for modeling multi-relational data. In NIPS, 2787-2795.\n\nCOMET: Commonsense transformers for automatic knowledge graph construction. A Bosselut, H Rashkin, M Sap, C Malaviya, A Celikyilmaz, Y Choi, ACL. Bosselut, A.; Rashkin, H.; Sap, M.; Malaviya, C.; Celiky- ilmaz, A.; and Choi, Y. 2019. COMET: Commonsense transformers for automatic knowledge graph construction. In ACL, 4762-4779.\n\nKBGAN: Adversarial learning for knowledge graph embeddings. L Cai, W Y Wang, NAACL. Cai, L., and Wang, W. Y. 2018. KBGAN: Adversarial learn- ing for knowledge graph embeddings. In NAACL, 1470- 1480.\n\nKBQA: learning question answering over qa corpora and knowledge bases. W Cui, Y Xiao, H Wang, Y Song, S Hwang, W Wang, Proceedings of the VLDB Endowment. 105Cui, W.; Xiao, Y.; Wang, H.; Song, Y.; Hwang, S.-w.; and Wang, W. 2017. KBQA: learning question answering over qa corpora and knowledge bases. Proceedings of the VLDB Endowment 10(5):565-576.\n\nConvolutional 2d knowledge graph embeddings. T Dettmers, P Minervini, P Stenetorp, S Riedel, AAAI. Dettmers, T.; Minervini, P.; Stenetorp, P.; and Riedel, S. 2018. Convolutional 2d knowledge graph embeddings. In AAAI, 1811-1818.\n\nBert: Pre-training of deep bidirectional transformers for language understanding. J Devlin, M.-W Chang, K Lee, K Toutanova, NAACL. Devlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. In NAACL, 4171-4186.\n\nOpenKE: An open toolkit for knowledge embedding. X Han, S Cao, X Lv, Y Lin, Z Liu, M Sun, J Li, EMNLP. Han, X.; Cao, S.; Lv, X.; Lin, Y.; Liu, Z.; Sun, M.; and Li, J. 2018. OpenKE: An open toolkit for knowledge embedding. In EMNLP, 139-144.\n\nKnowledge graph embedding via dynamic mapping matrix. G Ji, S He, L Xu, K Liu, J Zhao, ACL. Ji, G.; He, S.; Xu, L.; Liu, K.; and Zhao, J. 2015. Knowledge graph embedding via dynamic mapping matrix. In ACL, 687-696.\n\nKnowledge graph completion with adaptive sparse transfer matrix. G Ji, K Liu, S He, J Zhao, AAAI. Ji, G.; Liu, K.; He, S.; and Zhao, J. 2016. Knowledge graph completion with adaptive sparse transfer matrix. In AAAI.\n\nModeling relation paths for representation learning of knowledge bases. Y Lin, Z Liu, H Luan, M Sun, S Rao, S Liu, EMNLP. Lin, Y.; Liu, Z.; Luan, H.; Sun, M.; Rao, S.; and Liu, S. 2015a. Modeling relation paths for representation learning of knowledge bases. In EMNLP, 705-714.\n\nLearning entity and relation embeddings for knowledge graph completion. Y Lin, Z Liu, M Sun, Y Liu, X Zhu, AAAI. Lin, Y.; Liu, Z.; Sun, M.; Liu, Y.; and Zhu, X. 2015b. Learn- ing entity and relation embeddings for knowledge graph completion. In AAAI.\n\nDistributed representations of words and phrases and their compositionality. T Mikolov, I Sutskever, K Chen, G S Corrado, J Dean, NIPS. Mikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and Dean, J. 2013. Distributed representations of words and phrases and their compositionality. In NIPS, 3111-3119.\n\nWordnet: a lexical database for english. G A Miller, Communications of the ACM. 3811Miller, G. A. 1995. Wordnet: a lexical database for english. Communications of the ACM 38(11):39-41.\n\nA convolutional neural network-based model for knowledge base completion and its application to search personalization. D Q Nguyen, D Q Nguyen, T D Nguyen, D Phung, Semantic WebNguyen, D. Q.; Nguyen, D. Q.; Nguyen, T. D.; and Phung, D. 2018a. A convolutional neural network-based model for knowledge base completion and its application to search personalization. Semantic Web.\n\nA novel embedding model for knowledge base completion based on convolutional neural network. D Q Nguyen, T D Nguyen, D Q Nguyen, D Phung, NAACL. Nguyen, D. Q.; Nguyen, T. D.; Nguyen, D. Q.; and Phung, D. 2018b. A novel embedding model for knowledge base completion based on convolutional neural network. In NAACL, 327-333.\n\nA threeway model for collective learning on multi-relational data. M Nickel, V Tresp, H.-P Kriegel, ICML. Nickel, M.; Tresp, V.; and Kriegel, H.-P. 2011. A three- way model for collective learning on multi-relational data. In ICML, 809-816.\n\nGlove: Global vectors for word representation. J Pennington, R Socher, C Manning, EMNLP. Pennington, J.; Socher, R.; and Manning, C. 2014. Glove: Global vectors for word representation. In EMNLP.\n\nM E Peters, M Neumann, M Iyyer, M Gardner, C Clark, K Lee, L Zettlemoyer, Deep contextualized word representations. In NAACL. Peters, M. E.; Neumann, M.; Iyyer, M.; Gardner, M.; Clark, C.; Lee, K.; and Zettlemoyer, L. 2018. Deep contextualized word representations. In NAACL, 2227-2237.\n\n. A Radford, K Narasimhan, T Salimans, I Sutskever, Improving language understanding by generative pre-trainingRadford, A.; Narasimhan, K.; Salimans, T.; and Sutskever, I. 2018. Improving language understanding by generative pre-training.\n\nModeling relational data with graph convolutional networks. M Schlichtkrull, T N Kipf, P Bloem, Van Den, R Berg, I Titov, M Welling, ESWC. Schlichtkrull, M.; Kipf, T. N.; Bloem, P.; Van Den Berg, R.; Titov, I.; and Welling, M. 2018. Modeling relational data with graph convolutional networks. In ESWC, 593-607.\n\nProjE: Embedding projection for knowledge graph completion. B Shi, T Weninger, AAAI. Shi, B., and Weninger, T. 2017. ProjE: Embedding projec- tion for knowledge graph completion. In AAAI.\n\nReasoning with neural tensor networks for knowledge base completion. R Socher, D Chen, C D Manning, A Ng, NIPS. Socher, R.; Chen, D.; Manning, C. D.; and Ng, A. 2013. Reasoning with neural tensor networks for knowledge base completion. In NIPS, 926-934.\n\nRotate: Knowledge graph embedding by relational rotation in complex space. F M Suchanek, G Kasneci, G Weikum, Z.-H Deng, J.-Y Nie, J Tang, ICLR. WWWSuchanek, F. M.; Kasneci, G.; and Weikum, G. 2007. Yago: a core of semantic knowledge. In WWW, 697-706. ACM. Sun, Z.; Deng, Z.-H.; Nie, J.-Y.; and Tang, J. 2019. Ro- tate: Knowledge graph embedding by relational rotation in complex space. In ICLR.\n\nComplex embeddings for simple link prediction. T Trouillon, J Welbl, S Riedel, \u00c9 Gaussier, G Bouchard, ICML. Trouillon, T.; Welbl, J.; Riedel, S.; Gaussier,\u00c9.; and Bouchard, G. 2016. Complex embeddings for simple link prediction. In ICML, 2071-2080.\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, \u0141 Kaiser, I Polosukhin, NIPS. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, \u0141.; and Polosukhin, I. 2017. At- tention is all you need. In NIPS, 5998-6008.\n\nA multiscale visualization of attention in the transformer model. J Vig, arXiv:1906.05714arXiv preprintVig, J. 2019. A multiscale visualization of attention in the transformer model. arXiv preprint arXiv:1906.05714.\n\nText-enhanced representation learning for knowledge graph. Z Wang, J.-Z Li, IJCAI. Wang, Z., and Li, J.-Z. 2016. Text-enhanced representation learning for knowledge graph. In IJCAI, 1293-1299.\n\nKnowledge graph and text jointly embedding. Z Wang, J Zhang, J Feng, Z Chen, EMNLP. Wang, Z.; Zhang, J.; Feng, J.; and Chen, Z. 2014a. Knowl- edge graph and text jointly embedding. In EMNLP.\n\nKnowledge graph embedding by translating on hyperplanes. Z Wang, J Zhang, J Feng, Z Chen, AAAI. Wang, Z.; Zhang, J.; Feng, J.; and Chen, Z. 2014b. Knowl- edge graph embedding by translating on hyperplanes. In AAAI.\n\nKnowledge graph embedding: A survey of approaches and applications. Q Wang, Z Mao, B Wang, L Guo, IEEE TKDE. 2912Wang, Q.; Mao, Z.; Wang, B.; and Guo, L. 2017. Knowledge graph embedding: A survey of approaches and applications. IEEE TKDE 29(12):2724-2743.\n\nH Wang, V Kulkarni, W Y Wang, arXiv:1811.00147Deep contextualized knowledge graph embeddings. DoloresarXiv preprintWang, H.; Kulkarni, V.; and Wang, W. Y. 2018. Dolores: Deep contextualized knowledge graph embeddings. arXiv preprint arXiv:1811.00147.\n\nSSP: semantic space projection for knowledge graph embedding with text descriptions. H Xiao, M Huang, L Meng, X Zhu, AAAI. Xiao, H.; Huang, M.; Meng, L.; and Zhu, X. 2017. SSP: semantic space projection for knowledge graph embedding with text descriptions. In AAAI.\n\nTransG: A generative model for knowledge graph embedding. H Xiao, M Huang, X Zhu, ACL. 1Xiao, H.; Huang, M.; and Zhu, X. 2016. TransG: A gener- ative model for knowledge graph embedding. In ACL, vol- ume 1, 2316-2325.\n\nRepresentation learning of knowledge graphs with entity descriptions. R Xie, Z Liu, J Jia, H Luan, M Sun, AAAI. Xie, R.; Liu, Z.; Jia, J.; Luan, H.; and Sun, M. 2016. Repre- sentation learning of knowledge graphs with entity descrip- tions. In AAAI.\n\nRepresentation learning of knowledge graphs with hierarchical types. R Xie, Z Liu, M Sun, IJCAI. Xie, R.; Liu, Z.; and Sun, M. 2016. Representation learn- ing of knowledge graphs with hierarchical types. In IJCAI, 2965-2971.\n\nKnowledge graph representation with jointly structural and textual encoding. J Xu, X Qiu, K Chen, X Huang, IJCAI. Xu, J.; Qiu, X.; Chen, K.; and Huang, X. 2017. Knowl- edge graph representation with jointly structural and textual encoding. In IJCAI, 1318-1324.\n\nEmbedding entities and relations for learning and inference in knowledge bases. B Yang, W Yih, X He, J Gao, L Deng, ICLR. Yang, B.; Yih, W.-t.; He, X.; Gao, J.; and Deng, L. 2015. Embedding entities and relations for learning and inference in knowledge bases. In ICLR.\n\nZ Yang, Z Dai, Y Yang, J Carbonell, R Salakhutdinov, Q V Le, arXiv:1906.08237XLNet: Generalized autoregressive pretraining for language understanding. arXiv preprintYang, Z.; Dai, Z.; Yang, Y.; Carbonell, J.; Salakhutdinov, R.; and Le, Q. V. 2019. XLNet: Generalized autoregres- sive pretraining for language understanding. arXiv preprint arXiv:1906.08237.\n\nCollaborative knowledge base embedding for recommender systems. F Zhang, N J Yuan, D Lian, X Xie, W.-Y Ma, In KDD. ACMZhang, F.; Yuan, N. J.; Lian, D.; Xie, X.; and Ma, W.-Y. 2016. Collaborative knowledge base embedding for recom- mender systems. In KDD, 353-362. ACM.\n\nKnowledge graph embedding with hierarchical relation structure. Z Zhang, F Zhuang, M Qu, F Lin, Q He, EMNLP. Zhang, Z.; Zhuang, F.; Qu, M.; Lin, F.; and He, Q. 2018. Knowledge graph embedding with hierarchical relation structure. In EMNLP, 3198-3207.\n\nERNIE: Enhanced language representation with informative entities. Z Zhang, X Han, Z Liu, X Jiang, M Sun, Q Liu, ACL. Zhang, Z.; Han, X.; Liu, Z.; Jiang, X.; Sun, M.; and Liu, Q. 2019. ERNIE: Enhanced language representation with informative entities. In ACL, 1441-1451.\n", "annotations": {"author": "[{\"end\":127,\"start\":48},{\"end\":217,\"start\":128},{\"end\":295,\"start\":218}]", "publisher": null, "author_last_name": "[{\"end\":57,\"start\":54},{\"end\":142,\"start\":139},{\"end\":226,\"start\":223}]", "author_first_name": "[{\"end\":53,\"start\":48},{\"end\":138,\"start\":128},{\"end\":222,\"start\":218}]", "author_affiliation": "[{\"end\":126,\"start\":86},{\"end\":216,\"start\":176},{\"end\":294,\"start\":254}]", "title": "[{\"end\":45,\"start\":1},{\"end\":340,\"start\":296}]", "venue": null, "abstract": "[{\"end\":1093,\"start\":342}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1183,\"start\":1160},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":1226,\"start\":1190},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1252,\"start\":1239},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":1363,\"start\":1345},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1405,\"start\":1388},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2441,\"start\":2421},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2457,\"start\":2441},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3098,\"start\":3080},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3113,\"start\":3098},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3128,\"start\":3113},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3202,\"start\":3182},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3229,\"start\":3208},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3256,\"start\":3236},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3285,\"start\":3267},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5148,\"start\":5129},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5205,\"start\":5186},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5458,\"start\":5425},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":5487,\"start\":5469},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5725,\"start\":5703},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5745,\"start\":5725},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5771,\"start\":5745},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6079,\"start\":6059},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6250,\"start\":6231},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6906,\"start\":6888},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7134,\"start\":7118},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8106,\"start\":8085},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8155,\"start\":8117},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8267,\"start\":8248},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8480,\"start\":8460},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8510,\"start\":8491},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8796,\"start\":8765},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":8998,\"start\":8980},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9103,\"start\":9081},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9608,\"start\":9589},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9762,\"start\":9741},{\"end\":12206,\"start\":12201},{\"end\":12791,\"start\":12786},{\"end\":12958,\"start\":12953},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":13504,\"start\":13483},{\"end\":15379,\"start\":15374},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16441,\"start\":16422},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":16469,\"start\":16449},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":16496,\"start\":16477},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":16547,\"start\":16526},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":17336,\"start\":17317},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21761,\"start\":21741},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21834,\"start\":21814},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24666,\"start\":24645},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":26883,\"start\":26864},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26907,\"start\":26891},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":26933,\"start\":26915},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":26968,\"start\":26941},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":26996,\"start\":26980},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":27026,\"start\":27008},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":27085,\"start\":27067},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":27181,\"start\":27162},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":27238,\"start\":27215},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":27280,\"start\":27259},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27309,\"start\":27287},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":27346,\"start\":27319},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":27410,\"start\":27392},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":27549,\"start\":27518},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":27610,\"start\":27587},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":27696,\"start\":27677}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":27697,\"start\":26862},{\"attributes\":{\"id\":\"fig_1\"},\"end\":27787,\"start\":27698},{\"attributes\":{\"id\":\"fig_2\"},\"end\":28198,\"start\":27788},{\"attributes\":{\"id\":\"fig_3\"},\"end\":28262,\"start\":28199},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":28667,\"start\":28263},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":29592,\"start\":28668},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":29745,\"start\":29593},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":30517,\"start\":29746},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":31495,\"start\":30518},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":31614,\"start\":31496}]", "paragraph": "[{\"end\":1924,\"start\":1109},{\"end\":3129,\"start\":1926},{\"end\":3748,\"start\":3131},{\"end\":4311,\"start\":3750},{\"end\":4518,\"start\":4313},{\"end\":6855,\"start\":4520},{\"end\":7880,\"start\":6857},{\"end\":9510,\"start\":7912},{\"end\":10519,\"start\":9584},{\"end\":10668,\"start\":10554},{\"end\":10796,\"start\":10764},{\"end\":11078,\"start\":10987},{\"end\":11173,\"start\":11150},{\"end\":11418,\"start\":11322},{\"end\":14245,\"start\":11420},{\"end\":14546,\"start\":14311},{\"end\":15637,\"start\":14655},{\"end\":15831,\"start\":15758},{\"end\":16009,\"start\":15872},{\"end\":16340,\"start\":16025},{\"end\":16614,\"start\":16342},{\"end\":20332,\"start\":16616},{\"end\":21039,\"start\":20334},{\"end\":22816,\"start\":21041},{\"end\":23689,\"start\":22818},{\"end\":25303,\"start\":23691},{\"end\":25969,\"start\":25305},{\"end\":26532,\"start\":26000},{\"end\":26861,\"start\":26534}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10763,\"start\":10669},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10986,\"start\":10797},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11149,\"start\":11092},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11321,\"start\":11174},{\"attributes\":{\"id\":\"formula_4\"},\"end\":14310,\"start\":14246},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14654,\"start\":14547},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15757,\"start\":15638},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15871,\"start\":15832}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":18011,\"start\":18004},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":21915,\"start\":21908},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":23112,\"start\":23105}]", "section_header": "[{\"end\":1107,\"start\":1095},{\"end\":7910,\"start\":7883},{\"end\":9582,\"start\":9513},{\"end\":10552,\"start\":10522},{\"end\":11091,\"start\":11081},{\"end\":16023,\"start\":16012},{\"end\":25998,\"start\":25972},{\"end\":27709,\"start\":27699},{\"end\":27799,\"start\":27789},{\"end\":28210,\"start\":28200},{\"end\":28678,\"start\":28669},{\"end\":29603,\"start\":29594},{\"end\":30528,\"start\":30519},{\"end\":31506,\"start\":31497}]", "table": "[{\"end\":28667,\"start\":28373},{\"end\":29592,\"start\":28898},{\"end\":30517,\"start\":29795},{\"end\":31495,\"start\":30530}]", "figure_caption": "[{\"end\":27697,\"start\":26864},{\"end\":27787,\"start\":27711},{\"end\":28198,\"start\":27801},{\"end\":28262,\"start\":28212},{\"end\":28373,\"start\":28265},{\"end\":28898,\"start\":28680},{\"end\":29745,\"start\":29605},{\"end\":29795,\"start\":29748},{\"end\":31614,\"start\":31508}]", "figure_ref": "[{\"end\":12077,\"start\":12069},{\"end\":14953,\"start\":14945},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19683,\"start\":19675},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23765,\"start\":23757},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23778,\"start\":23770},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":23845,\"start\":23837},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24891,\"start\":24883}]", "bib_author_first_name": "[{\"end\":31717,\"start\":31716},{\"end\":31723,\"start\":31722},{\"end\":31731,\"start\":31730},{\"end\":31738,\"start\":31737},{\"end\":31965,\"start\":31964},{\"end\":31978,\"start\":31977},{\"end\":31987,\"start\":31986},{\"end\":31999,\"start\":31998},{\"end\":32009,\"start\":32008},{\"end\":32266,\"start\":32265},{\"end\":32276,\"start\":32275},{\"end\":32287,\"start\":32286},{\"end\":32303,\"start\":32302},{\"end\":32313,\"start\":32312},{\"end\":32567,\"start\":32566},{\"end\":32579,\"start\":32578},{\"end\":32590,\"start\":32589},{\"end\":32597,\"start\":32596},{\"end\":32609,\"start\":32608},{\"end\":32624,\"start\":32623},{\"end\":32881,\"start\":32880},{\"end\":32888,\"start\":32887},{\"end\":32890,\"start\":32889},{\"end\":33092,\"start\":33091},{\"end\":33099,\"start\":33098},{\"end\":33107,\"start\":33106},{\"end\":33115,\"start\":33114},{\"end\":33123,\"start\":33122},{\"end\":33132,\"start\":33131},{\"end\":33416,\"start\":33415},{\"end\":33428,\"start\":33427},{\"end\":33441,\"start\":33440},{\"end\":33454,\"start\":33453},{\"end\":33683,\"start\":33682},{\"end\":33696,\"start\":33692},{\"end\":33705,\"start\":33704},{\"end\":33712,\"start\":33711},{\"end\":33946,\"start\":33945},{\"end\":33953,\"start\":33952},{\"end\":33960,\"start\":33959},{\"end\":33966,\"start\":33965},{\"end\":33973,\"start\":33972},{\"end\":33980,\"start\":33979},{\"end\":33987,\"start\":33986},{\"end\":34193,\"start\":34192},{\"end\":34199,\"start\":34198},{\"end\":34205,\"start\":34204},{\"end\":34211,\"start\":34210},{\"end\":34218,\"start\":34217},{\"end\":34420,\"start\":34419},{\"end\":34426,\"start\":34425},{\"end\":34433,\"start\":34432},{\"end\":34439,\"start\":34438},{\"end\":34644,\"start\":34643},{\"end\":34651,\"start\":34650},{\"end\":34658,\"start\":34657},{\"end\":34666,\"start\":34665},{\"end\":34673,\"start\":34672},{\"end\":34680,\"start\":34679},{\"end\":34923,\"start\":34922},{\"end\":34930,\"start\":34929},{\"end\":34937,\"start\":34936},{\"end\":34944,\"start\":34943},{\"end\":34951,\"start\":34950},{\"end\":35180,\"start\":35179},{\"end\":35191,\"start\":35190},{\"end\":35204,\"start\":35203},{\"end\":35212,\"start\":35211},{\"end\":35214,\"start\":35213},{\"end\":35225,\"start\":35224},{\"end\":35451,\"start\":35450},{\"end\":35453,\"start\":35452},{\"end\":35716,\"start\":35715},{\"end\":35718,\"start\":35717},{\"end\":35728,\"start\":35727},{\"end\":35730,\"start\":35729},{\"end\":35740,\"start\":35739},{\"end\":35742,\"start\":35741},{\"end\":35752,\"start\":35751},{\"end\":36067,\"start\":36066},{\"end\":36069,\"start\":36068},{\"end\":36079,\"start\":36078},{\"end\":36081,\"start\":36080},{\"end\":36091,\"start\":36090},{\"end\":36093,\"start\":36092},{\"end\":36103,\"start\":36102},{\"end\":36365,\"start\":36364},{\"end\":36375,\"start\":36374},{\"end\":36387,\"start\":36383},{\"end\":36587,\"start\":36586},{\"end\":36601,\"start\":36600},{\"end\":36611,\"start\":36610},{\"end\":36737,\"start\":36736},{\"end\":36739,\"start\":36738},{\"end\":36749,\"start\":36748},{\"end\":36760,\"start\":36759},{\"end\":36769,\"start\":36768},{\"end\":36780,\"start\":36779},{\"end\":36789,\"start\":36788},{\"end\":36796,\"start\":36795},{\"end\":37027,\"start\":37026},{\"end\":37038,\"start\":37037},{\"end\":37052,\"start\":37051},{\"end\":37064,\"start\":37063},{\"end\":37325,\"start\":37324},{\"end\":37342,\"start\":37341},{\"end\":37344,\"start\":37343},{\"end\":37352,\"start\":37351},{\"end\":37370,\"start\":37369},{\"end\":37378,\"start\":37377},{\"end\":37387,\"start\":37386},{\"end\":37637,\"start\":37636},{\"end\":37644,\"start\":37643},{\"end\":37835,\"start\":37834},{\"end\":37845,\"start\":37844},{\"end\":37853,\"start\":37852},{\"end\":37855,\"start\":37854},{\"end\":37866,\"start\":37865},{\"end\":38096,\"start\":38095},{\"end\":38098,\"start\":38097},{\"end\":38110,\"start\":38109},{\"end\":38121,\"start\":38120},{\"end\":38134,\"start\":38130},{\"end\":38145,\"start\":38141},{\"end\":38152,\"start\":38151},{\"end\":38465,\"start\":38464},{\"end\":38478,\"start\":38477},{\"end\":38487,\"start\":38486},{\"end\":38497,\"start\":38496},{\"end\":38509,\"start\":38508},{\"end\":38696,\"start\":38695},{\"end\":38707,\"start\":38706},{\"end\":38718,\"start\":38717},{\"end\":38728,\"start\":38727},{\"end\":38741,\"start\":38740},{\"end\":38750,\"start\":38749},{\"end\":38752,\"start\":38751},{\"end\":38761,\"start\":38760},{\"end\":38771,\"start\":38770},{\"end\":39022,\"start\":39021},{\"end\":39232,\"start\":39231},{\"end\":39243,\"start\":39239},{\"end\":39411,\"start\":39410},{\"end\":39419,\"start\":39418},{\"end\":39428,\"start\":39427},{\"end\":39436,\"start\":39435},{\"end\":39616,\"start\":39615},{\"end\":39624,\"start\":39623},{\"end\":39633,\"start\":39632},{\"end\":39641,\"start\":39640},{\"end\":39843,\"start\":39842},{\"end\":39851,\"start\":39850},{\"end\":39858,\"start\":39857},{\"end\":39866,\"start\":39865},{\"end\":40032,\"start\":40031},{\"end\":40040,\"start\":40039},{\"end\":40052,\"start\":40051},{\"end\":40054,\"start\":40053},{\"end\":40369,\"start\":40368},{\"end\":40377,\"start\":40376},{\"end\":40386,\"start\":40385},{\"end\":40394,\"start\":40393},{\"end\":40609,\"start\":40608},{\"end\":40617,\"start\":40616},{\"end\":40626,\"start\":40625},{\"end\":40840,\"start\":40839},{\"end\":40847,\"start\":40846},{\"end\":40854,\"start\":40853},{\"end\":40861,\"start\":40860},{\"end\":40869,\"start\":40868},{\"end\":41090,\"start\":41089},{\"end\":41097,\"start\":41096},{\"end\":41104,\"start\":41103},{\"end\":41324,\"start\":41323},{\"end\":41330,\"start\":41329},{\"end\":41337,\"start\":41336},{\"end\":41345,\"start\":41344},{\"end\":41589,\"start\":41588},{\"end\":41597,\"start\":41596},{\"end\":41604,\"start\":41603},{\"end\":41610,\"start\":41609},{\"end\":41617,\"start\":41616},{\"end\":41779,\"start\":41778},{\"end\":41787,\"start\":41786},{\"end\":41794,\"start\":41793},{\"end\":41802,\"start\":41801},{\"end\":41815,\"start\":41814},{\"end\":41832,\"start\":41831},{\"end\":41834,\"start\":41833},{\"end\":42201,\"start\":42200},{\"end\":42210,\"start\":42209},{\"end\":42212,\"start\":42211},{\"end\":42220,\"start\":42219},{\"end\":42228,\"start\":42227},{\"end\":42238,\"start\":42234},{\"end\":42471,\"start\":42470},{\"end\":42480,\"start\":42479},{\"end\":42490,\"start\":42489},{\"end\":42496,\"start\":42495},{\"end\":42503,\"start\":42502},{\"end\":42726,\"start\":42725},{\"end\":42735,\"start\":42734},{\"end\":42742,\"start\":42741},{\"end\":42749,\"start\":42748},{\"end\":42758,\"start\":42757},{\"end\":42765,\"start\":42764}]", "bib_author_last_name": "[{\"end\":31720,\"start\":31718},{\"end\":31728,\"start\":31724},{\"end\":31735,\"start\":31732},{\"end\":31742,\"start\":31739},{\"end\":31975,\"start\":31966},{\"end\":31984,\"start\":31979},{\"end\":31996,\"start\":31988},{\"end\":32006,\"start\":32000},{\"end\":32016,\"start\":32010},{\"end\":32273,\"start\":32267},{\"end\":32284,\"start\":32277},{\"end\":32300,\"start\":32288},{\"end\":32310,\"start\":32304},{\"end\":32323,\"start\":32314},{\"end\":32576,\"start\":32568},{\"end\":32587,\"start\":32580},{\"end\":32594,\"start\":32591},{\"end\":32606,\"start\":32598},{\"end\":32621,\"start\":32610},{\"end\":32629,\"start\":32625},{\"end\":32885,\"start\":32882},{\"end\":32895,\"start\":32891},{\"end\":33096,\"start\":33093},{\"end\":33104,\"start\":33100},{\"end\":33112,\"start\":33108},{\"end\":33120,\"start\":33116},{\"end\":33129,\"start\":33124},{\"end\":33137,\"start\":33133},{\"end\":33425,\"start\":33417},{\"end\":33438,\"start\":33429},{\"end\":33451,\"start\":33442},{\"end\":33461,\"start\":33455},{\"end\":33690,\"start\":33684},{\"end\":33702,\"start\":33697},{\"end\":33709,\"start\":33706},{\"end\":33722,\"start\":33713},{\"end\":33950,\"start\":33947},{\"end\":33957,\"start\":33954},{\"end\":33963,\"start\":33961},{\"end\":33970,\"start\":33967},{\"end\":33977,\"start\":33974},{\"end\":33984,\"start\":33981},{\"end\":33990,\"start\":33988},{\"end\":34196,\"start\":34194},{\"end\":34202,\"start\":34200},{\"end\":34208,\"start\":34206},{\"end\":34215,\"start\":34212},{\"end\":34223,\"start\":34219},{\"end\":34423,\"start\":34421},{\"end\":34430,\"start\":34427},{\"end\":34436,\"start\":34434},{\"end\":34444,\"start\":34440},{\"end\":34648,\"start\":34645},{\"end\":34655,\"start\":34652},{\"end\":34663,\"start\":34659},{\"end\":34670,\"start\":34667},{\"end\":34677,\"start\":34674},{\"end\":34684,\"start\":34681},{\"end\":34927,\"start\":34924},{\"end\":34934,\"start\":34931},{\"end\":34941,\"start\":34938},{\"end\":34948,\"start\":34945},{\"end\":34955,\"start\":34952},{\"end\":35188,\"start\":35181},{\"end\":35201,\"start\":35192},{\"end\":35209,\"start\":35205},{\"end\":35222,\"start\":35215},{\"end\":35230,\"start\":35226},{\"end\":35460,\"start\":35454},{\"end\":35725,\"start\":35719},{\"end\":35737,\"start\":35731},{\"end\":35749,\"start\":35743},{\"end\":35758,\"start\":35753},{\"end\":36076,\"start\":36070},{\"end\":36088,\"start\":36082},{\"end\":36100,\"start\":36094},{\"end\":36109,\"start\":36104},{\"end\":36372,\"start\":36366},{\"end\":36381,\"start\":36376},{\"end\":36395,\"start\":36388},{\"end\":36598,\"start\":36588},{\"end\":36608,\"start\":36602},{\"end\":36619,\"start\":36612},{\"end\":36746,\"start\":36740},{\"end\":36757,\"start\":36750},{\"end\":36766,\"start\":36761},{\"end\":36777,\"start\":36770},{\"end\":36786,\"start\":36781},{\"end\":36793,\"start\":36790},{\"end\":36808,\"start\":36797},{\"end\":37035,\"start\":37028},{\"end\":37049,\"start\":37039},{\"end\":37061,\"start\":37053},{\"end\":37074,\"start\":37065},{\"end\":37339,\"start\":37326},{\"end\":37349,\"start\":37345},{\"end\":37358,\"start\":37353},{\"end\":37367,\"start\":37360},{\"end\":37375,\"start\":37371},{\"end\":37384,\"start\":37379},{\"end\":37395,\"start\":37388},{\"end\":37641,\"start\":37638},{\"end\":37653,\"start\":37645},{\"end\":37842,\"start\":37836},{\"end\":37850,\"start\":37846},{\"end\":37863,\"start\":37856},{\"end\":37869,\"start\":37867},{\"end\":38107,\"start\":38099},{\"end\":38118,\"start\":38111},{\"end\":38128,\"start\":38122},{\"end\":38139,\"start\":38135},{\"end\":38149,\"start\":38146},{\"end\":38157,\"start\":38153},{\"end\":38475,\"start\":38466},{\"end\":38484,\"start\":38479},{\"end\":38494,\"start\":38488},{\"end\":38506,\"start\":38498},{\"end\":38518,\"start\":38510},{\"end\":38704,\"start\":38697},{\"end\":38715,\"start\":38708},{\"end\":38725,\"start\":38719},{\"end\":38738,\"start\":38729},{\"end\":38747,\"start\":38742},{\"end\":38758,\"start\":38753},{\"end\":38768,\"start\":38762},{\"end\":38782,\"start\":38772},{\"end\":39026,\"start\":39023},{\"end\":39237,\"start\":39233},{\"end\":39246,\"start\":39244},{\"end\":39416,\"start\":39412},{\"end\":39425,\"start\":39420},{\"end\":39433,\"start\":39429},{\"end\":39441,\"start\":39437},{\"end\":39621,\"start\":39617},{\"end\":39630,\"start\":39625},{\"end\":39638,\"start\":39634},{\"end\":39646,\"start\":39642},{\"end\":39848,\"start\":39844},{\"end\":39855,\"start\":39852},{\"end\":39863,\"start\":39859},{\"end\":39870,\"start\":39867},{\"end\":40037,\"start\":40033},{\"end\":40049,\"start\":40041},{\"end\":40059,\"start\":40055},{\"end\":40374,\"start\":40370},{\"end\":40383,\"start\":40378},{\"end\":40391,\"start\":40387},{\"end\":40398,\"start\":40395},{\"end\":40614,\"start\":40610},{\"end\":40623,\"start\":40618},{\"end\":40630,\"start\":40627},{\"end\":40844,\"start\":40841},{\"end\":40851,\"start\":40848},{\"end\":40858,\"start\":40855},{\"end\":40866,\"start\":40862},{\"end\":40873,\"start\":40870},{\"end\":41094,\"start\":41091},{\"end\":41101,\"start\":41098},{\"end\":41108,\"start\":41105},{\"end\":41327,\"start\":41325},{\"end\":41334,\"start\":41331},{\"end\":41342,\"start\":41338},{\"end\":41351,\"start\":41346},{\"end\":41594,\"start\":41590},{\"end\":41601,\"start\":41598},{\"end\":41607,\"start\":41605},{\"end\":41614,\"start\":41611},{\"end\":41622,\"start\":41618},{\"end\":41784,\"start\":41780},{\"end\":41791,\"start\":41788},{\"end\":41799,\"start\":41795},{\"end\":41812,\"start\":41803},{\"end\":41829,\"start\":41816},{\"end\":41837,\"start\":41835},{\"end\":42207,\"start\":42202},{\"end\":42217,\"start\":42213},{\"end\":42225,\"start\":42221},{\"end\":42232,\"start\":42229},{\"end\":42241,\"start\":42239},{\"end\":42477,\"start\":42472},{\"end\":42487,\"start\":42481},{\"end\":42493,\"start\":42491},{\"end\":42500,\"start\":42497},{\"end\":42506,\"start\":42504},{\"end\":42732,\"start\":42727},{\"end\":42739,\"start\":42736},{\"end\":42746,\"start\":42743},{\"end\":42755,\"start\":42750},{\"end\":42762,\"start\":42759},{\"end\":42769,\"start\":42766}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":44113572},\"end\":31878,\"start\":31652},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":207167677},\"end\":32204,\"start\":31880},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":14941970},\"end\":32488,\"start\":32206},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":189762527},\"end\":32818,\"start\":32490},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":3401524},\"end\":33018,\"start\":32820},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":18928027},\"end\":33368,\"start\":33020},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":4328400},\"end\":33598,\"start\":33370},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":52967399},\"end\":33894,\"start\":33600},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":53223679},\"end\":34136,\"start\":33896},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":11202498},\"end\":34352,\"start\":34138},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":38485677},\"end\":34569,\"start\":34354},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1969092},\"end\":34848,\"start\":34571},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":2949428},\"end\":35100,\"start\":34850},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":16447573},\"end\":35407,\"start\":35102},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1671874},\"end\":35593,\"start\":35409},{\"attributes\":{\"id\":\"b15\"},\"end\":35971,\"start\":35595},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":3882054},\"end\":36295,\"start\":35973},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":1157792},\"end\":36537,\"start\":36297},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1957433},\"end\":36734,\"start\":36539},{\"attributes\":{\"id\":\"b19\"},\"end\":37022,\"start\":36736},{\"attributes\":{\"id\":\"b20\"},\"end\":37262,\"start\":37024},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":5458500},\"end\":37574,\"start\":37264},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":18367155},\"end\":37763,\"start\":37576},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":8429835},\"end\":38018,\"start\":37765},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":67855617},\"end\":38415,\"start\":38020},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":15150247},\"end\":38666,\"start\":38417},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":13756489},\"end\":38953,\"start\":38668},{\"attributes\":{\"doi\":\"arXiv:1906.05714\",\"id\":\"b27\"},\"end\":39170,\"start\":38955},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":16160185},\"end\":39364,\"start\":39172},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":5241137},\"end\":39556,\"start\":39366},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":15027084},\"end\":39772,\"start\":39558},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":19135805},\"end\":40029,\"start\":39774},{\"attributes\":{\"doi\":\"arXiv:1811.00147\",\"id\":\"b32\"},\"end\":40281,\"start\":40031},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":3150829},\"end\":40548,\"start\":40283},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":11091552},\"end\":40767,\"start\":40550},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":31606602},\"end\":41018,\"start\":40769},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":1743664},\"end\":41244,\"start\":41020},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":3919235},\"end\":41506,\"start\":41246},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":2768038},\"end\":41776,\"start\":41508},{\"attributes\":{\"doi\":\"arXiv:1906.08237\",\"id\":\"b39\"},\"end\":42134,\"start\":41778},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":7062707},\"end\":42404,\"start\":42136},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":53080968},\"end\":42656,\"start\":42406},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":158046772},\"end\":42928,\"start\":42658}]", "bib_title": "[{\"end\":31714,\"start\":31652},{\"end\":31962,\"start\":31880},{\"end\":32263,\"start\":32206},{\"end\":32564,\"start\":32490},{\"end\":32878,\"start\":32820},{\"end\":33089,\"start\":33020},{\"end\":33413,\"start\":33370},{\"end\":33680,\"start\":33600},{\"end\":33943,\"start\":33896},{\"end\":34190,\"start\":34138},{\"end\":34417,\"start\":34354},{\"end\":34641,\"start\":34571},{\"end\":34920,\"start\":34850},{\"end\":35177,\"start\":35102},{\"end\":35448,\"start\":35409},{\"end\":36064,\"start\":35973},{\"end\":36362,\"start\":36297},{\"end\":36584,\"start\":36539},{\"end\":37322,\"start\":37264},{\"end\":37634,\"start\":37576},{\"end\":37832,\"start\":37765},{\"end\":38093,\"start\":38020},{\"end\":38462,\"start\":38417},{\"end\":38693,\"start\":38668},{\"end\":39229,\"start\":39172},{\"end\":39408,\"start\":39366},{\"end\":39613,\"start\":39558},{\"end\":39840,\"start\":39774},{\"end\":40366,\"start\":40283},{\"end\":40606,\"start\":40550},{\"end\":40837,\"start\":40769},{\"end\":41087,\"start\":41020},{\"end\":41321,\"start\":41246},{\"end\":41586,\"start\":41508},{\"end\":42198,\"start\":42136},{\"end\":42468,\"start\":42406},{\"end\":42723,\"start\":42658}]", "bib_author": "[{\"end\":31722,\"start\":31716},{\"end\":31730,\"start\":31722},{\"end\":31737,\"start\":31730},{\"end\":31744,\"start\":31737},{\"end\":31977,\"start\":31964},{\"end\":31986,\"start\":31977},{\"end\":31998,\"start\":31986},{\"end\":32008,\"start\":31998},{\"end\":32018,\"start\":32008},{\"end\":32275,\"start\":32265},{\"end\":32286,\"start\":32275},{\"end\":32302,\"start\":32286},{\"end\":32312,\"start\":32302},{\"end\":32325,\"start\":32312},{\"end\":32578,\"start\":32566},{\"end\":32589,\"start\":32578},{\"end\":32596,\"start\":32589},{\"end\":32608,\"start\":32596},{\"end\":32623,\"start\":32608},{\"end\":32631,\"start\":32623},{\"end\":32887,\"start\":32880},{\"end\":32897,\"start\":32887},{\"end\":33098,\"start\":33091},{\"end\":33106,\"start\":33098},{\"end\":33114,\"start\":33106},{\"end\":33122,\"start\":33114},{\"end\":33131,\"start\":33122},{\"end\":33139,\"start\":33131},{\"end\":33427,\"start\":33415},{\"end\":33440,\"start\":33427},{\"end\":33453,\"start\":33440},{\"end\":33463,\"start\":33453},{\"end\":33692,\"start\":33682},{\"end\":33704,\"start\":33692},{\"end\":33711,\"start\":33704},{\"end\":33724,\"start\":33711},{\"end\":33952,\"start\":33945},{\"end\":33959,\"start\":33952},{\"end\":33965,\"start\":33959},{\"end\":33972,\"start\":33965},{\"end\":33979,\"start\":33972},{\"end\":33986,\"start\":33979},{\"end\":33992,\"start\":33986},{\"end\":34198,\"start\":34192},{\"end\":34204,\"start\":34198},{\"end\":34210,\"start\":34204},{\"end\":34217,\"start\":34210},{\"end\":34225,\"start\":34217},{\"end\":34425,\"start\":34419},{\"end\":34432,\"start\":34425},{\"end\":34438,\"start\":34432},{\"end\":34446,\"start\":34438},{\"end\":34650,\"start\":34643},{\"end\":34657,\"start\":34650},{\"end\":34665,\"start\":34657},{\"end\":34672,\"start\":34665},{\"end\":34679,\"start\":34672},{\"end\":34686,\"start\":34679},{\"end\":34929,\"start\":34922},{\"end\":34936,\"start\":34929},{\"end\":34943,\"start\":34936},{\"end\":34950,\"start\":34943},{\"end\":34957,\"start\":34950},{\"end\":35190,\"start\":35179},{\"end\":35203,\"start\":35190},{\"end\":35211,\"start\":35203},{\"end\":35224,\"start\":35211},{\"end\":35232,\"start\":35224},{\"end\":35462,\"start\":35450},{\"end\":35727,\"start\":35715},{\"end\":35739,\"start\":35727},{\"end\":35751,\"start\":35739},{\"end\":35760,\"start\":35751},{\"end\":36078,\"start\":36066},{\"end\":36090,\"start\":36078},{\"end\":36102,\"start\":36090},{\"end\":36111,\"start\":36102},{\"end\":36374,\"start\":36364},{\"end\":36383,\"start\":36374},{\"end\":36397,\"start\":36383},{\"end\":36600,\"start\":36586},{\"end\":36610,\"start\":36600},{\"end\":36621,\"start\":36610},{\"end\":36748,\"start\":36736},{\"end\":36759,\"start\":36748},{\"end\":36768,\"start\":36759},{\"end\":36779,\"start\":36768},{\"end\":36788,\"start\":36779},{\"end\":36795,\"start\":36788},{\"end\":36810,\"start\":36795},{\"end\":37037,\"start\":37026},{\"end\":37051,\"start\":37037},{\"end\":37063,\"start\":37051},{\"end\":37076,\"start\":37063},{\"end\":37341,\"start\":37324},{\"end\":37351,\"start\":37341},{\"end\":37360,\"start\":37351},{\"end\":37369,\"start\":37360},{\"end\":37377,\"start\":37369},{\"end\":37386,\"start\":37377},{\"end\":37397,\"start\":37386},{\"end\":37643,\"start\":37636},{\"end\":37655,\"start\":37643},{\"end\":37844,\"start\":37834},{\"end\":37852,\"start\":37844},{\"end\":37865,\"start\":37852},{\"end\":37871,\"start\":37865},{\"end\":38109,\"start\":38095},{\"end\":38120,\"start\":38109},{\"end\":38130,\"start\":38120},{\"end\":38141,\"start\":38130},{\"end\":38151,\"start\":38141},{\"end\":38159,\"start\":38151},{\"end\":38477,\"start\":38464},{\"end\":38486,\"start\":38477},{\"end\":38496,\"start\":38486},{\"end\":38508,\"start\":38496},{\"end\":38520,\"start\":38508},{\"end\":38706,\"start\":38695},{\"end\":38717,\"start\":38706},{\"end\":38727,\"start\":38717},{\"end\":38740,\"start\":38727},{\"end\":38749,\"start\":38740},{\"end\":38760,\"start\":38749},{\"end\":38770,\"start\":38760},{\"end\":38784,\"start\":38770},{\"end\":39028,\"start\":39021},{\"end\":39239,\"start\":39231},{\"end\":39248,\"start\":39239},{\"end\":39418,\"start\":39410},{\"end\":39427,\"start\":39418},{\"end\":39435,\"start\":39427},{\"end\":39443,\"start\":39435},{\"end\":39623,\"start\":39615},{\"end\":39632,\"start\":39623},{\"end\":39640,\"start\":39632},{\"end\":39648,\"start\":39640},{\"end\":39850,\"start\":39842},{\"end\":39857,\"start\":39850},{\"end\":39865,\"start\":39857},{\"end\":39872,\"start\":39865},{\"end\":40039,\"start\":40031},{\"end\":40051,\"start\":40039},{\"end\":40061,\"start\":40051},{\"end\":40376,\"start\":40368},{\"end\":40385,\"start\":40376},{\"end\":40393,\"start\":40385},{\"end\":40400,\"start\":40393},{\"end\":40616,\"start\":40608},{\"end\":40625,\"start\":40616},{\"end\":40632,\"start\":40625},{\"end\":40846,\"start\":40839},{\"end\":40853,\"start\":40846},{\"end\":40860,\"start\":40853},{\"end\":40868,\"start\":40860},{\"end\":40875,\"start\":40868},{\"end\":41096,\"start\":41089},{\"end\":41103,\"start\":41096},{\"end\":41110,\"start\":41103},{\"end\":41329,\"start\":41323},{\"end\":41336,\"start\":41329},{\"end\":41344,\"start\":41336},{\"end\":41353,\"start\":41344},{\"end\":41596,\"start\":41588},{\"end\":41603,\"start\":41596},{\"end\":41609,\"start\":41603},{\"end\":41616,\"start\":41609},{\"end\":41624,\"start\":41616},{\"end\":41786,\"start\":41778},{\"end\":41793,\"start\":41786},{\"end\":41801,\"start\":41793},{\"end\":41814,\"start\":41801},{\"end\":41831,\"start\":41814},{\"end\":41839,\"start\":41831},{\"end\":42209,\"start\":42200},{\"end\":42219,\"start\":42209},{\"end\":42227,\"start\":42219},{\"end\":42234,\"start\":42227},{\"end\":42243,\"start\":42234},{\"end\":42479,\"start\":42470},{\"end\":42489,\"start\":42479},{\"end\":42495,\"start\":42489},{\"end\":42502,\"start\":42495},{\"end\":42508,\"start\":42502},{\"end\":42734,\"start\":42725},{\"end\":42741,\"start\":42734},{\"end\":42748,\"start\":42741},{\"end\":42757,\"start\":42748},{\"end\":42764,\"start\":42757},{\"end\":42771,\"start\":42764}]", "bib_venue": "[{\"end\":31749,\"start\":31744},{\"end\":32024,\"start\":32018},{\"end\":32329,\"start\":32325},{\"end\":32634,\"start\":32631},{\"end\":32902,\"start\":32897},{\"end\":33172,\"start\":33139},{\"end\":33467,\"start\":33463},{\"end\":33729,\"start\":33724},{\"end\":33997,\"start\":33992},{\"end\":34228,\"start\":34225},{\"end\":34450,\"start\":34446},{\"end\":34691,\"start\":34686},{\"end\":34961,\"start\":34957},{\"end\":35236,\"start\":35232},{\"end\":35487,\"start\":35462},{\"end\":35713,\"start\":35595},{\"end\":36116,\"start\":36111},{\"end\":36401,\"start\":36397},{\"end\":36626,\"start\":36621},{\"end\":36860,\"start\":36810},{\"end\":37401,\"start\":37397},{\"end\":37659,\"start\":37655},{\"end\":37875,\"start\":37871},{\"end\":38163,\"start\":38159},{\"end\":38524,\"start\":38520},{\"end\":38788,\"start\":38784},{\"end\":39019,\"start\":38955},{\"end\":39253,\"start\":39248},{\"end\":39448,\"start\":39443},{\"end\":39652,\"start\":39648},{\"end\":39881,\"start\":39872},{\"end\":40123,\"start\":40077},{\"end\":40404,\"start\":40400},{\"end\":40635,\"start\":40632},{\"end\":40879,\"start\":40875},{\"end\":41115,\"start\":41110},{\"end\":41358,\"start\":41353},{\"end\":41628,\"start\":41624},{\"end\":41927,\"start\":41855},{\"end\":42249,\"start\":42243},{\"end\":42513,\"start\":42508},{\"end\":42774,\"start\":42771},{\"end\":40132,\"start\":40125}]"}}}, "year": 2023, "month": 12, "day": 17}