{"id": 220646832, "updated": "2023-10-06 12:50:54.958", "metadata": {"title": "Interpretable, Multidimensional, Multimodal Anomaly Detection with Negative Sampling for Detection of Device Failure", "authors": "[{\"first\":\"John\",\"last\":\"Sipple\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 7, "day": 12}, "abstract": "Complex devices are connected daily and eagerly generate vast streams of multidimensional state measurements. These devices often operate in distinct modes based on external conditions (day/night, occupied/vacant, etc.), and to prevent complete or partial system outage, we would like to recognize as early as possible when these devices begin to operate outside the normal modes. Unfortunately, it is often impractical or impossible to predict failures using rules or supervised machine learning, because failure modes are too complex, devices are too new to adequately characterize in a specific environment, or environmental change puts the device into an unpredictable condition. We propose an unsupervised anomaly detection method that creates a negative sample from the positive, observed sample, and trains a classifier to distinguish between positive and negative samples. Using the Contraction Principle, we explain why such a classifier ought to establish suitable decision boundaries between normal and anomalous regions, and show how Integrated Gradients can attribute the anomaly to specific variables within the anomalous state vector. We have demonstrated that negative sampling with random forest or neural network classifiers yield significantly higher AUC scores than Isolation Forest, One Class SVM, and Deep SVDD, against (a) a synthetic dataset with dimensionality ranging between 2 and 128, with 1, 2, and 3 modes, and with and without noise dimensions; (b) four standard benchmark datasets; and (c) a multidimensional, multimodal dataset from real climate control devices. Finally, we describe how negative sampling with neural network classifiers have been successfully deployed at large scale to predict failures in real time in over 15,000 climate-control and power meter devices in 145 Google office buildings.", "fields_of_study": "[\"Engineering\",\"Computer Science\"]", "external_ids": {"arxiv": "2007.10088", "mag": "3035141043", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/Sipple20", "doi": null}}, "content": {"source": {"pdf_hash": "888dbb50bd63a94e349081b27b78c671607452f4", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2007.10088v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "badc8817ebcbd08744e7b3b7dd5e94d98ee0fde3", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/888dbb50bd63a94e349081b27b78c671607452f4.txt", "contents": "\nInterpretable, Multidimensional, Multimodal Anomaly Detection with Negative Sampling for Detection of Device Failure\n2020\n\nJohn Sipple \nJohn Sipple \nCom&gt; \nInterpretable, Multidimensional, Multimodal Anomaly Detection with Negative Sampling for Detection of Device Failure\n\nProceedings of the 37 th International Conference on Machine Learning\nthe 37 th International Conference on Machine LearningVienna, Austria20201 Google, Mountain View, California, USA. Correspondence to:\nIn this paper we propose a scalable, unsupervised approach for detecting anomalies in the Internet of Things (IoT). Complex devices are connected daily and eagerly generate vast streams of multidimensional telemetry. These devices often operate in distinct modes based on external conditions (day/night, occupied/vacant, etc.), and to prevent complete or partial system outage, we would like to recognize as early as possible when these devices begin to operate outside the normal modes. We propose an unsupervised anomaly detection method that creates a negative sample from the positive, observed sample, and trains a classifier to distinguish between positive and negative samples. Using the Concentration Phenomenon, we explain why such a classifier ought to establish suitable decision boundaries between normal and anomalous regions, and show how Integrated Gradients can attribute the anomaly to specific dimensions within the anomalous state vector. We have demonstrated that negative sampling with random forest or neural network classifiers yield significantly higher AUC scores compared to state-ofthe-art approaches against benchmark anomaly detection datasets, and a multidimensional, multimodal dataset from real climate control devices. Finally, we describe how negative sampling with neural network classifiers have been successfully deployed at large scale to predict failures in real time in over 15,000 climate-control and power meter devices in 145 office buildings within the California Bay Area.\n\nIntroduction\n\nIn this paper we propose a scalable, unsupervised approach to detecting anomalies in data streams of Internet of Things (IoT) connected devices. Complex devices are connected daily and eagerly generate vast streams of multidimensional measurements characterizing their momentary state. Occasionally, some devices fail, resulting in a system outage, and postmortem analysis reveals that these devices generated unusual states before the outage. Had technical staff been promptly alerted, they could have preemptively fixed the device. It is often impractical or impossible to predict failures on fixed rules or supervised machine learning methods, because failures are too complex, devices are too new to adequately characterize both normal and failure modes in a specific environment, or the environment changes and puts the device into an unpredictable condition. Examples of such complex networked devices include power and climate control in commercial buildings, servers and computers in data centers, badge readers and alarms in physical security systems, and electromechanical components in powerplants. In this paper, we explore an approach that automatically observes a complex system, generates a normal, multidimensional baseline, and detects anomalous measurements from the incoming data stream.\n\nAnomaly detection refers to the problem of finding patterns in data that do not conform to expected behavior (Chandola et al., 2009). Multidimensional data points may be anomalous because, in one or more dimensions, the value lies outside of an expected range, or because an expected correlation between two or more dimensions is violated. For example, suppose a thermostat reports periodic updates of an observed temperature and a setpoint temperature. An anomaly may occur when the observed temperature exceeds the normal range (i.e., too hot or too cold), or when the observed temperature and setpoint temperature values no longer correlate, even if both values themselves fall within normal ranges (i.e., the climate control device fails to maintain a desired setpoint temperature).\n\nAnomaly detection algorithms generally proceed in two steps. The first step is to characterize a normal baseline model. The second step is to score each new data point as normal or anomalous. This score may be a simple bi-arXiv:2007.10088v1 [eess.SP] 12 Jul 2020 nary label or a real-valued outlier score that quantifies how anomalous the data point is, such as a class probability or confidence (Aggarwal, 2016) .\n\nThere are several factors that should be considered when developing multidimensional anomaly detection solutions:\n\nNoise Dimensions. IoT devices generate logs or telemetry data streams with high dimensionality, where the anomaly is observable in a subset of the dimensions, but masked in noise dimensions. Unfortunately, with increasing dimensionality, many conventional anomaly detections methods fail to work effectively.\n\nCorrelations. Baseline correlations are multidimensional and may be nonlinear with some dimensions exhibiting strong positive or negative correlations, while others may exhibit weak or no correlations. For example, under normal conditions, the desired setpoint temperature and observed temperature are highly correlated, but the relative humidity and carbon dioxide level may only be weakly correlated.\n\nMultimodal. Oftentimes real-world, normal processes operate in multiple modes. For example, energy efficient thermostats may operate in an eco-mode while the zone is vacant, and comfort-mode while the zone is occupied. These modes may be in distinct regions in feature space, producing none or few intermediate data points from transitions between modes.\n\nInterpretable. To aid diagnosis and defect remediation, it is often desirable, especially with high-dimensional data, to provide additional information to help understanding the anomaly along with the anomaly score. The anomaly detector should therefore attribute a proportional blame to those variables that contributed to the anomaly score.\n\nIn section 2, we present relevant work in anomaly detection, Concentration Phenomenon, and model interpretabilty. Section 3 discusses how negative sampling and binary classifiers can be used for anomaly detection, and applies Integrated Gradients for interpreting anomalies. Section 4 compares performance of our methods against state-of-the-art methods, and demonstrates anomaly interpretability. In section 5, we describe our large-scale implementation to monitor climate control devices and recommend future work.\n\nIn summary, this paper makes the following contributions:\n\n\u2022 a scalable approach to detecting multidimensional outliers, robust under multimodal conditions,\n\n\u2022 an alternative to one-class anomaly detection using negative sampling, and\n\n\u2022 a novel application of Integrated Gradients for anomaly interpretation.\n\n\nBackground and Related Work\n\nWe consider anomaly detection to be fundamentally a binary classification task. Each new data point is assigned an anomaly score that indicates how likely it is a member of the normal or anomalous class. However, because comprehensive labeled data are not usually available, the anomalous class is underrepresented, and failure modes cannot be fully characterized, using supervised methods is not straightforward. Previous works have used various methods to overcome these challenges, including: (1) negative sampling approaches that explicitly characterize the anomalous space, (2) one-class classifiers that learn a transformation of the normal data to a characteristic manifold, (3) autoencoder-based methods that detect anomalies with large reconstruction errors, and (4) density-based approaches that identify anomalies as points within sparsely populated regions. We also review how previous works have applied the Concentration Phenomenon for anomaly detection, and describe relevant works in interpretability.\n\nNegative Selection Algorithms (NSA) were initially proposed as a biologically-inspired method of detecting computer viruses (Forrest et al., 1994). Most NSAs apply search algorithms that attempt to emulate how antibodies distinguish pathogens from body cells. (Gonzalez et al., 2002) first proposed a method that applies NSA to generate a negative sample from an unlabeled positive sample containing both normal and anomalous data, and then trains a classifier on the negative and positive samples to learn a decision boundary between normal and anomalous subspaces. Multiple papers have been written on variations on search-based NSA techniques and evaluating NSA runtime complexity (Ji & Dasgupta, 2007;Yang et al., 2017;Hosseini & Seilani, 2019). Because NSA-based anomaly detection relies on many detectors to explore the boundaries of the positive (self) sample, it suffers from large time complexity and space complexity (Jinyin et al., 2011;Ayara et al., 2002). (Stibor et al., 2005) demonstrated that One-Class SVM outperformed a variable-sized NSA detector algorithm on the Iris-Fisher and biomedical data sets, and questioned whether negative selection is appropriate for anomaly detection. In this paper we apply negative selection, but propose a much simpler approach with uniform sampling and a binary classifier to build decision boundaries around regions with different sample densities.\n\nOne-class classifiers are trained to learn a transformation function f : X \u2192 c that generates a scalar value as a class score c \u2282 C, when the input data resembles the observed, and mostly normal, data stream (Aggarwal, 2016). However, when c deviates significantly from C, X is anomalous. Probably the most prominent one-class classifier is the One-Class SVM (OC-SVM) (Sch\u00f6lkopf et al., 2001). The OC-SVM learns a maximum margin hyperplane that best separates the normal class from the origin given by\nW \u00b7 \u03a6(X) \u2212 b = 0, where \u03a6(\u00b7)\nis an unknown transformation function, W are its coefficients, and b is a bias. The positive set represents the normal space, and the origin is the only labeled negative instance. The transformation function \u03a6(\u00b7) is learned indirectly by applying the kernel trick within a Lagrangian dual formulation. Many additional variations to one-class classifiers have been proposed. One Class Neural Networks (OC-NN) (Chalapathy et al., 2018) is a deep learning extension to OC-SVM that replaces the SVM with a deep neural network. Deep Support Vector Data Description (Deep SVDD) (Ruff et al., 2018) is an extension to the OC-SVM that trains a neural network while minimizing the volume of a lower-dimensional hypersphere that encloses the networks representation of the data. Anomalous data is likely to fall outside the sphere, and normal data is likely to fall inside the sphere. The soft-boundary Deep SVDD objective consists of a hypersphere volume term, a penalty term incurred by normal points outside the sphere, and a regularization term. The penalty term contains a contamination parameter that controls the trade-off between the volume of the sphere and any violations of the boundary, allowing a configurable fraction of points to be truly anomalous. While Deep SVDD was shown to work using image-based data sets (CIFAR-10, MNIST, and GTSRB), the Deep SVDD objective functions can be readily adapted to multidimensional device state vectors.\n\nAutoencoder and generative methods use deep learning encoder-decoder architectures, where outliers tend to have greater reconstruction errors than normal points. Deep Autoencoders with Density-Based Clustering (DAE-DBC) (Amarbayasgalan et al., 2018) train an auto encoder to minimize the reconstruction error, and then apply density-based clustering (DBSCAN) in the latent space to identify outlier points and clusters. In One Class Classification using Intra-Class Splitting (OC-ICS) (Schlachter et al., 2019), the training data is refined by splitting it into typical and atypical normal sets, using a predesignated threshold on reconstruction error from an autoencoder. Generative adversarial methods, such as AnoGAN (Schlegl et al., 2017) and GANomaly (Akcay et al., 2018), include discriminators that refine the autoencoder's ability to learn subtle, contextual information and increase the representative power and specificity.\n\nDensity-based approaches identify points that are in sparsely populated regions. One widely used approach originally proposed by (Liu et al., 2008), called Isolation Forest, is an ensemble of random trees that recursively partition the data space until all points are isolated. Since anomalies are few and different, they require fewer partitions on average than the normal points, which are located in dense regions. Consequently, anomalies have shorter path lengths -the number of edges from the point to the root of the tree -than normal points. To compute a points anomaly score, the path lengths from all trees are averaged and normalized, and conditioned to return a value of 1 for normal points and 0 for anomalous points. With multimodal datasets, (Hariri et al., 2018) showed that the Isolation Forest can miss anomalies because it forms false normal regions, proposed an enhancement that partitions the data using hyperplanes with random slopes, called Extended Isolation Forest, and demonstrated improved anomaly detection performance.\n\nThe Concentration Phenomenon describes how manifolds distort as dimensionality increases, and is useful for characterizing anomaly detection algorithms in high-dimensional spaces, but has not been applied extensively in anomaly detection, with a few exceptions. The Concentration Phenomenon was applied previously in a supporting lemma to guarantee convergence for estimating a baseline matrix that is used to quantify the surprise of network user-to-object accesses (Gutflaish et al., 2017). Distance concentration was applied to prove that the proposed outlier factor does not degrade with increasing dimensionality, thereby mitigating the curse of dimensionality.\n\nModel Interpretability is an active area of research that seeks an understandable explanation for a models decision (Doshi-Velez & Kim, 2017). Numerous papers have been written about attribution, a type of interpretability that explains what features were most influential in a classifiers decision (Baehrens et al., 2009;Simonyan et al., 2013;Shrikumar et al., 2017;Binder et al., 2016). Integrated Gradients (Sundararajan et al., 2017) is an attribution method originally proposed for highlighting important features used by deep neural network classifiers. General approaches to enhancing anomaly detection with interpretability have not been adequately addressed in the literature. However, a few domain-specific implementations have been proposed, such as using Markov Chain methods and cross entropy for identifying attributing anomalous network traffic to multiple correlated traffic flows (Nevat et al., 2018).\n\n\nDetecting and Interpreting Anomalies\n\nConsider a target process that generates a sequential stream of multidimensional data points x(1), x(2), ..., where the\ni th data point 1 x(i) , is a D-dimensional vector x = {x 1 , x 2 , ..., x D } in D .\nThe target process may be either a single discrete unit, or a homogeneous cohort 2 of equivalent units. Our objectives are to (a) estimate P (x \u2208 N ormal), and (b) attribute the anomaly score on to each dimension as an interpretation for each anomalous x.\n\nDefinition 1. An anomaly is any data point x with a near zero probability that it was generated by the Normal process:\nP (x \u2208 N ormal) \u2248 0.\nThe Normal process occupies one or more discrete manifolds or volumes of unknown shape, in D , which are populated in high density by the target process, except for the small subset of anomalous points that fall outside the Normal volume(s). The Anomalous volume is the complement of the Normal volume in D . A unimodal process occupies a single discrete volume, and a multimodal process occupies two or more spatially disconnected volumes.\n\n\nDetecting Anomalies with Negative Sampling\n\nThe Concentration Phenomenon states that there is exponentially more room in higher dimensions than in lower dimensions. Intuitively, as the number of dimensions D increases, the cube has 2 D corners, so most of the volume is concentrated near them, and manifolds tend to occupy less volume relative to the full space (van Handel, 2016;Vershynin, 2018). Based on the Concentration Phenomenon, we propose a simple method for developing a labeled data set to train a classifier for our anomaly detection task.\n\nOur approach to anomaly detection is to define two class samples, and train a classifier function F : D \u2192 [0, 1] to distinguish between the two classes. The positive class sample 3 U = {u(1), u(2), ..., u(M )} is the set of M Ddimensional data points 4 that were observed by the target process, including a small number of unlabeled, actual anomalies, which our anomaly detection algorithm is expected to detect. Because the positive sample is contaminated with a small number of anomalous data points, there is also a false positive labeling error, P (u \u2208 Anomalous) > 0. However, because anomalies are rare by definition, the probability that any point drawn from the positive sample is normal is nearly one, independent of dimensionality D.\n\nThe negative class sample V = {v (1), v(2), ..., v(N )} is chosen independently and uniformly from the cube bounded by the extrema of each dimension plus some small \u00b1\u03b4, where the volume bounded by the negative sample is strictly greater than the volume bounded by the positive sample, 3 Because a positive outcome from a statistical test ordinarily implies the anomalous case, a statistical perspective might argue for reversing the terminology to use positive to indicate an Anomalous case and negative to indicate a Normal case. However, in this work we use the term positive to refer to the space that is sampled by observation and is mostly Normal, and negative to refer to an unobserved complement space from which a labeled sample has to be generated. 4 Every u and v are D-dimensional points like x, and we use letters u and v to distinguish the positive (observed) sample U from the negative sample V .\n\nV ol(U ) < V ol(V ). The sample ratio, r s = N M , governs the ratio between the negative sample size and the positive sample size, where r s = 0 represents the one-class anomaly detection classifier. Since the negative class is intended to represent the space of anomalies, and there is a nonzero probability that a negative sample point will land within the normal region, P (v \u2208 N ormal) > 0, we also have a false negative labeling error.\n\nAssumption 1 (Sufficiency): The positive sample U is representative of a stationary, ergodic Normal process. As with all supervised approaches, the training set should be representative of the prediction set, and it is essential to sample enough to reflect all Normal modes of behavior. In monitoring climate control devices, it is important to sample from all hours of the day, and days of the week, even when fixing seasonal conditions. Ideally, we would like to train a binary classifier F with data that has as few labeling errors as possible. Intuitively, it makes sense to develop an algorithm that carefully selects the negative sample to avoid the Normal space. For example, (Gonzalez et al., 2002) proposes a type of regiongrowing approach that avoids choosing points close to the positive sample. However, such a sampling approach is difficult and/or computationally expensive in high dimensions because we are not able to characterize the positive volume. Instead, we observe that volumes tend to contract in high-dimensional spaces, and propose using uniform i.i.d. sampling for generating the negative sample. Proof. While it can be shown that specific shapes such as the sphere or Gaussian contract to zero volume in high dimensions, we choose the hypercube for the normal volume with lengths bounded by the extrema \u2206u d = max (U d ) \u2212 min (U d ) since its relative volume decreases most slowly with increasing dimension. Since we are sampling uniformly, the probability of a false positive is the relative volume P (v \u2208 N ormal) = V ol(V ) V ol(U ) . The length of dimension d in the negative volume is \u2206v d = \u2206u d + 2\u03b4 . Since \u2206u d < \u2206v d for all d \u2264 D, as the dimensionality D increases, the false negative error descends exponentially to\nzero P (v \u2208 N ormal) = lim D\u2192\u221e D d \u2206v d \u2206u d = 0 .\nThe rate at which P (v \u2208 N ormal) decreases depends on the geometry and volume of the Normal region. Proposition 1 provides upper-bounded asymptotic guarantees that can be strengthened given knowledge of the geometry of the Normal volume. However, when the characteristics of the Normal region are unknown, without loss of generality, we can use Proposition 1 to bound the false negative probability P (v \u2208 N ormal) \u2264 D d \u2206v d \u2206u d Next, we apply Proposition 1 to develop a simple procedure for developing a dataset that can be used to train an anomaly detection classifier.\n\nProposition 2. (Labeled Training Set for Anomaly Detection): Given a sufficiently sampled, high-dimensional dataset from a target process and uniform negative sampling, we can generate a labeled two-class dataset to train a classifier F for detecting anomalies.\n\nA good training set requires a low number of labeling errors for a classifier to learn decision boundaries. In this application, both false positive and false negative errors are small. By our definition of an anomaly, false negative occurrences are rare and P (u \u2208 Anomalous) \u2248 0, so U can be used in training data to represent the Normal class. By Proposition 1, uniform negative sampling can be used to generate accurate labeled data representing the anomalous regions, when D \u2192 \u221e.\n\nThe sampling ratio, r s , specifies the density of negative data points since the Negative Sample volume is fixed. In lower dimensions, it is possible to oversample in the Negative Sample, such that P (v \u2208 N ormal) \u2248 P (u \u2208 N ormal) and a classifier is unable to learn decision boundaries. The sample ratio should be chosen within r s,min : P (u \u2208 Anomalous) P (v \u2208 Anomalous) and r s,max : P (v \u2208 N ormal) P (u \u2208 N ormal).\n\nGiven that the classifiers are universal function approximators, such as deep ReLU networks (Hanin, 2019), there is no limitation to the number of distinct modes, shapes, or orientations of continuous, high-dimensional Normal volumes. Negative-sampling classifiers detect bad interactions, where the values of all dimensions are within Normal ranges, but in aggregate, the points are in the Anomalous region.\n\nIf the positive sample size, the dimensionality, and the classifier hyperparameters (number of estimators, tree depth, number layers, layer width, etc.) are fixed, and we assume that, in general, classifier training time grows linearly with the size of input, then the run-time complexity incurred during training of negative sampling depends on the sampling ratio only, O(r s ), and remains constant, O(1), at inference.\n\n\nInterpreting Anomalies with Integrated Gradients\n\nIn practice, knowing which dimensions caused an anomaly score helps the user identify the root cause and choose an appropriate fix. In climate control devices, dimensions like zone temperature, carbon dioxide level, humidity, etc., are codependent. Highlighting a single anomalous dimension usually identifies a broken sensor, whereas identifying multiple dimensions helps pinpoint a mechanical failure caused by a defective valve, stuck damper, etc. As a first step in anomaly interpretation, we would like to quantify a proportional blame for each dimension. Recent work on deep network model interpretability can be applied to variable attribution. For example, Integrated Gradients (Sundararajan et al., 2017), have been shown to indicate what pixels contributed most to an image classification, or what words contributed to a text classification. The Integrated Gradients method computes and integrates the gradient for each dimension from a neutral baseline point to the observed point. In the image classification task, a black image is commonly used as the neutral baseline, and in the text classification task, a zero-embedding token vector is suitable. However, in anomaly interpretation the neutral baseline is not immediately obvious. Fundamentally, given an anomalous point x, we would like to know how to transform x into a suitable normal point u * . For example, if the thermostat is generating anomalous data while in eco-mode, we would like to understand which dimensions must be altered to transform the anomalous point to a normal point in eco-mode, rather than in a more remote comfort mode.\n\nA key step in applying Integrated Gradients is to select a good baseline (Sundararajan et al., 2017), so we first discuss how to choose a baseline, and then describe how to use Integrated Gradients for anomaly interpretation.\n\n\nProposition 3. (Baseline Set for Anomaly Detection)\n\nPoints from the positive sample used to train the anomaly detection classifier with high Normal class confidence scores, U * \u2282 U : \u2200 u\u2208U * F (x) \u2265 1 \u2212 are a sufficient baseline set.\n\nProof. In the original formulation of Integrated Gradients, x has a high class-confidence score, which requires a baseline point that yields a near-zero class confidence score. In anomaly interpretation it is reversed; since x is an anomaly, F (x) \u2248 0, the baseline point must be from from the Normal set, F (u) \u2248 1. By Assumption 1, the positive sample is sufficient and stable. A trained classifier will assign highest Normal class scores to points from regions with the greatest difference in positive and negative sample densities. Tolerance depends on the classifier and data, but should be large enough to accumulate enough points to cover all highconfidence Normal regions. Since a uniform distribution guarantees a constant negative sample density, these highest scoring points in U * are in regions with the maximum density of Normal observations.\n\nOnce we have established the neutral baseline set, we then determine how to apply it to any anomalous point. To simplify interpretability, we will choose the single nearest point from U * to anomalous point x as an approximation for the closest point on the surface of the Normal volume. Since integrated gradients computes the gradient along the straightline path between the point and the baseline, we chose the chose the baseline point from U * with the minimum Euclidean distance dist, u * = argmin u\u2208U * {dist (x, u)}.\n\n(Using the Euclidean distance assumes a Euclidean space, which requires first normalizing all dimensions.) Along with the proportional blame provided by Integrated Gradients, we have found that the values of u * provide useful interpretation as expected values of the nearest normal.\n\nNow that we have a baseline point for each anomaly, we can apply the Integrated Gradients Equation (1) to assign a proportional blame, B d (x) along the d th dimension for anomaly x and nearest normal baseline u * , where \u2202F (x) \u2202x d is the gradient of the anomaly classifier function F , and \u03b1 is path variable that ranges from 0 at x to 1 at u * .\nB d (x) \u2261 (u * d \u2212 x d ) \u00d7 1 \u03b1=0 \u2202F (x + \u03b1 \u00d7 (u * \u2212 x)) \u2202x d d\u03b1(1)\nThe Completeness Axiom of Integrated Gradients ensures that each dimension is assigned a proportional blame and the sum of the blame across each dimension is bounded by tolerance and very close to 1:\n1 \u2212 d\u2208D B d (x) \u2264 .\nIf we assume the cost for computing the distance and gradient are both constant, then the run-time complexity for applying Integrated Gradients for variable attribution is linear with (a) the number of baseline points |U * |, and (b) the number of k steps to compute the gradient along the path variable \u03b1 from anomaly x to the nearest baseline point u * ,\nO (|U * |) + O (k).\nPutting it all together, this approach is an anomaly detector with a novel method of interpreting the anomaly. Propositions 1 and 2 describe a method of creating a labeled data set to train an anomaly detection classifier, and Proposition 3 proposes a way of applying Integrated Gradients and a deep network anomaly classifier to interpret an anomaly score. In the following section we will demonstrate the performance compared to other anomaly detection methods with various data sets.\n\n\nExperiments\n\nIn his section we demonstrate the performance of two negative sampling classifiers against benchmark data tests, a real-world data set from climate control devices, and multimodel, multidimensional synthetic data sets. All results are presented as ROC (i.e., True Positive vs. False Positive Rates) AUC. We also demonstrate how Integrated Gradients can be used to help interpret anomalies.\n\nAs described in the previous section, after normalizing the positive sample, we apply uniform negative sampling with a constant \u03b4 = 0.05 but vary the sample ratio, r s . The negative sample is assigned class label 0, and the entire positive sample is labeled 1 (i.e., no real anomalies are omitted).\n\n\nAnomaly Detection\n\nWe present two negative sampling classifiers: one using random forests and the other based on neural networks. The Negative Sampling Random Forest (NS-RF) is parameterized by the number of estimators, the splitting criterion (gini or entropy), maximum tree depth, minimum samples per split and leaf, and maximum number of features per estimator. The Negative Sampling Neural Network (NS-NN) varies the number of hidden layers, where each hidden layer has a dense ReLU sublayer, and a dropout sub-layer. Each layer is configured with the same input width and dropout probability. The output is a sigmoid layer. Training is performed via standard backpropagation using binary cross-entropy loss function. In addition to the number of layers, layer width, and dropout probability, we varied the batch size, the number of epochs, and steps per epoch.\n\nWe compared the negative sampling anomaly detectors with two prominent anomaly detectors, and two recent extensions of them. One-Class SVM (OC-SVM) 5 is parameterized by the kernel function (linear, polynomial, RBF, or sigmoid), enabling or disabling the shrinking heuristic, and the contamination factor. Like NS-RF, the Isolation Forest (ISO) 6 is an ensemble-based anomaly detection and is parameterized by the number of estimators, maximum number of samples per estimator, maximum number of features per estimator, and contamination. Deep SVDD is a new deep learning adaptation based on OC-SVM. In its original version, Deep-SVDD performance was demonstrated exclusively on image-based datasets with a LeNet-type CNN to process the 2D spatial features (Ruff et al., 2018). In this study, we replaced the CNN layers with a variable number of dense and dropout layers, but applied the soft boundary and one-class Deep SVDD objective functions. We varied the hidden layer width and dropout probability, the number of epochs, the steps per epoch, contamination, the period to recompute the hypersphere radius, learning rate, decay, and momentum. Extended Isolation Forest (EIF) 7 reduces false positive regions that may occur with Isolation Forest on multimodal datasets by slicing the data along hyperplanes with random slopes (Hariri et al., 2018). We vary the number of estimators, the maximum tree depth per estimator. We selected several familiar benchmark anomaly detection datasets from the Outlier Detection Dataset (Rayana, 2016), summarized in Table 1. Additionally, we introduced realworld data from smart buildings, which uniquely exhibit multimodal behavior. When climate control devices fail, they cannot meet the required comfort conditions, and/or consume more energy than fully functional units. The Smart Buildings anomaly dataset 8 consists of 60,425 multidimensional, multimodal observations derived from 15 Variable Air Volume (VAV) climate control devices collected over 14 days, 8 -21 October 2019, from office buildings in the California Bay Area. In 1,921 (3.2%) anomalous observations, the zone air temperatures fall below the zone air heating setpoint or above the zone air cooling setpoint, and are of interest to facilities technicians. In other words, the zone air temperature is normal when it remains above the zone air heating setpoint, and below the zone air cooling setpoint. During working periods, the devices operate in comfort mode with tight constraints between the heating and cooling setpoints. During non-working periods, the setpoints are wider to reduce energy consumption, and hence, there are comfort and eco operating modes. The seven numeric dimensions are: zone air cooling temperature setpoint, zone air heating temperature setpoint, zone air temperature sensor, supply air flowrate sensor, supply air damper percentage command, supply air flowrate setpoint, integer day of week (0-6), integer hour of day (0-23).\n\nBefore conducting record trial runs, we performed hyperparameter optimization on AUC for each algorithm, and selected the highest performing parameters. We conducted four formal trials with five-fold cross validation for each dataset and anomaly detector, which generated a total of twenty AUC results per detector algorithm and dataset against a held-out 20% validation slice. The mean and standard deviation of each dataset-detector combination are presented in Table 2 as percentages. For each of the six detectors, we performed a pairwise Wilcoxon rank-sum test of significance and highlighted top performing algorithms, using a significance threshold of 5%. The selected sampling ratios r s ranged from 30 (Shuttle) to 60 (Satellite). NS-NN architectures ranged from one hidden layer with a width of 64 (Smart Buildings) to 3 hidden layers with a width of 461 (Forest Cover). Performance was better with high dropout rates, ranging between 0.34 (Mammography) to 0.8 (Shuttle). Overall, wide and shallow performed better than deep and narrow architectures. NS-RF configurations ranged between 50 estimators with a maximum depth of 50 (Shuttle) to 150 estimators with a maximum depth of 50 (Smart Buildings).\n\nWe performed supplemental analysis using synthetic data sets to evaluate how negative sampling anomaly detection algorithms perform with 4, 8, 16, and 32 dimensions and 1, 2, and 3 Gaussian modes. We found that for any dimension, both NS-NN and NS-RF maintained similar performance with each mode, maintained AUCs above the OC-SVM, Deep-SVDD, ISO, and EIF. Even when replacing 25% of the dimensions with uniform noise, we observed less than 4% degradation compared to the same number of modes and dimensions with no noise dimensions.\n\n\nAnomaly Interpretation\n\nWe used a synthetic dataset to demonstrate how Integrated Gradients can be used to rank anomalous dimensions and suggest expected normal values. The positive sample consists of 2,500 points drawn from two 16-dimensional Gaussian modes \u00b5 1,2 = \u00b12.4 \u00d7 I 16 and \u03a3 1,2 = 0.5 \u00d7 I 16 , with an additional 125 points (5%) drawn from the uniform distribution to represent true anomalies. Using five-fold cross validation, we trained a NS-NN anomaly detector with two hidden layers, width 64, dropout of 0.1, for 100 epochs, and a sampling ratio r s = 2.0. All data were normalized before training. After the model is trained, we select a reference baseline U * by predicting on the training set, and selecting all training points based on = 0.01.\n\nUsing the reference baseline set, we use Integrated Gradients to compute the proportional blame on any new point x. We selected the one baseline point, u * , with the smallest Euclidean distance to x. Then, we accumulated the gradients with k = 2, 000 steps along the straight line path from x to u * . The Completeness Theorem requires that the gradients will sum to nearly 0 for Normal points and near 1 for Anomalous points. In Figure 1, we first illustrate the response for a Normal point, and contrast it with an Anomalous point with three anomalous dimensions in Figure 2.  \n\n\nDiscussion and Conclusion\n\nIt is remarkable that good anomaly detection results are possible with binary classifiers and uniform negative sampling. It is also interesting that suitable sampling ratios do not appear to grow exponentially with the number of dimensions, making the solution scalable, even to high dimensional spaces. Both Random Forest and Neural Nets generalize well, despite some labeling errors, suggesting that these classifiers are sensitive to the relative sample densities, and generate equivalent results even with different sampling ratios. Because both NS-RF and NS-NN yield fairly similar results, we believe the performance is more associated with the relative sampling densities, than with the type of classifier.\n\nWe have applied the Concentration Phenomenon to explain why negative sampling can be combined with classifiers to perform anomaly detection, and show how Integrated Gradients can attribute the anomaly to specific values, even with high-dimensional state vectors. We have demonstrated that negative sampling with random forest or neural network classifiers yield equivalent or higher AUC scores than Isolation Forest, One Class SVM, Deep SVDD, and Extended Isolation Forest against four of five standard benchmark datasets and one multidimensional, multimodal dataset from real climate control devices.\n\nNS-NN is an integral part of a pilot deployment of our Smart Buildings Fault Detection and Diagnostics (FDD) project. FDD actively monitors over 15,000 power and climate control devices, such as Variable Air Volume (VAV) devices, Fan Coil Units (FCU) and air handlers, boilers, chillers, shade controllers, and electric power meters, etc., installed in 145 office buildings. Because devices in other buildings are periodically added into the platform, it is important that FDD accepts new devices without requiring any manual configuration. Each device reports a multidimensional, numerical state vector in five-to ten-minute intervals with dimensionality ranging from 4 to 20, depending on the device type. All devices are periodically rediscovered and clustered into homogeneous cohorts, and each device cohort is then assigned to its own independent anomaly detection instance. Within a cohort, the dimensionality is fixed; however, the devices in a cohort routinely operate in an occupancy mode during business hours with very strict climate control settings, and an efficiency mode with wider temperature and ventilation tolerances during non-working hours. No comprehensive labeled dataset of failure conditions to train a supervised fault detector is available, and rules-based failure detectors generate an intolerably high false alarm rate. Each NS-NN instance is associated with a single cohort, and periodically retrains a model over sliding historical window to adapt to seasonal changes, and predicts an anomaly score to each new state vector. Persistent anomalous devices are ranked to update a live, enterprisewide fault detection list. We used Integrated Gradients to help the technicians understand the anomaly by assigning a proportional blame to individual dimensions. The baseline point used by Integrated Gradients for comparison is the nearest normal point observed in the historical training set. The facilities management team reviews daily each of the anomalies and determines which anomalies require a trouble ticket. Over 44% of all device-level anomalies result in calling technician support. The types of anomalies that generate trouble tickets include stuck airflow dampers and under-ventilated areas, failing sensors, undersized units, unbalanced or uncalibrated units, etc. Most non-actionable anomalies are due to exceptional climate zones, like labs or unoccupied zones.\n\nThere are numerous meaningful directions for future work that could extend negative sampling anomaly detection beyond just numeric features and fixed dimensionality. To increase its applicability and accuracy, negative sampling anomaly detection can be extended to handle categorical dimensions missing values. Many devices generate unstructured or semi-structured textual loglines, and using custom embedding models could be combined with negative sampling anomaly detection. Negative sampling anomaly detection could also be combined with time series analyses to learn and predict sequences of anomaly types, where an anomaly type is localizable to specific region in anomalous space. Some feature dimensions could be readily converted from the time domain into the frequency domain to identify unusual and potentially problematic duty cycles or oscillations. With so many more IoT devices being connected daily in so many different domains, the demand for zero-config, unsupervised anomaly detection will surely continue to provide fertile ground for future research and development.\n\n\nSoftware and Data\n\nThe Python language source code of this work is provided along with the Smart Buildings data set in Multidimensional Anomaly Detection with Interpretability (MADI) at https://github.com/google/madi.\n\nProposition 1 .\n1(Uniform Negative Sampling): For each di-mension d \u2264 D, let lim d = [min (U d ) \u2212 \u03b4, max (U d ) + \u03b4]be a range bounded by the extrema of the positive sample U extended by a conservative positive length \u03b4 that extends lim d beyond the normal space. We assume that the sample size of U is sufficiently large to bound the Normal region. Choose a negative sample V , by selecting N points uniformly i.i.d. bounded by lim d for each d \u2264 D. In high dimension, D \u2192 \u221e, false negative sampling error decays exponentially to zero, regardless of the shape of the Normal region.\n\nFigure 1 .\n1Anomaly Interpretation of a Normal point x. The left image shows F (x) = 1 in the center green circle, and the proportional blame B d against dimensions x005, x008, and x009 as exterior wedges. The right chart displays the stepwise integrated gradients from x at k = 0 to the nearest baseline u * at k = 2, 000. Since the point is normal, the gradients are very small, with B d \u2248 0.\n\nFigure 2 .\n2Anomaly Interpretation of an Anomalous point x with F (x) = 0, Three dimensions (x002, x015, and x007) assigned most of the blame, B d \u2248 1. The observed and expected normal values, x d (u * d ), are displayed next to each wedge.\n\nTable 1 .\n1Summary of Anomaly Detection Datasets.DATA SET \nSIZE \nDIM \nANOMALY \n\nFOREST COVER (FC) \n286,048 \n10 \n2,747 (0.9%) \nSHUTTLE (SH) \n49,097 \n9 \n3,511 (7%) \nMAMMOGRAPHY (MM) \n11,183 \n6 \n260 (2.3%) \nMULCROSS (MC) \n262,144 \n4 \n26,214 (10%) \nSATELLITE (SA) \n6,435 \n36 \n2,036 (32%) \nSMART BUILDINGS (SB) \n60,425 \n7 \n1,921 (3.2%) \n\n\n\nTable 2 .\n2Meanand Standard Deviations of AUC values as % for \nbenchmark datasets and the Smart Buildings dataset. Highlighted \nvalues are the top-scoring detectors based on a 5% significance \nthreshold. \n\nOCSVM \nDSVDD \nISO \nEIF \nNSRF \nNSNN \n\nFC \n\n53\u00b120 69\u00b17 \n85\u00b14 93\u00b11 80\u00b12 86\u00b14 \n\nSH \n\n93\u00b10 \n88\u00b19 \n96\u00b11 91\u00b11 93\u00b17 96\u00b15 \n\nMM \n\n71\u00b17 \n78\u00b16 \n77\u00b12 86\u00b12 85\u00b14 84\u00b12 \n\nMC \n\n90\u00b10 \n54\u00b117 88\u00b10 66\u00b14 94\u00b11 99\u00b11 \n\nSA \n\n51\u00b11 \n62\u00b13 \n67\u00b12 71\u00b13 65\u00b14 73\u00b13 \n\nSB \n\n76\u00b11 \n60\u00b17 \n71\u00b17 80\u00b14 95\u00b11 93\u00b11 \n\n\nFor clarity, we drop the sequential index i : x(i) = x, and reserve the subscript to index dimensionality.2 In practice, it is often beneficial to group many equivalent devices together into a cohort, because (a) the cohorts baseline is more robust, and (b) anomaly detection can be performed on multiple devices effectively in parallel within the same execution process.\nWe used scikit-learn version 0.22 of OC-SVM6  We used scikit-learn version 0.22 IsolationForest implementation.7  We used the EIF authors open source implementation available at: https://github.com/sahandha/eif.\nThis data set is not intended to characterize all possible failure modes from climate control devices; the anomaly labels represent only one type of failure mode.\nAcknowledgementsThe author would like to thank the Google Research community and Mukund Sundararajan for instructive and practical advice and for a detailed technical review; Marc Pawliger and the Google Carson Smart Buildings Team for offering the opportunity to apply this approach to its first real-world application; and Rich Dutton and the Google Corp Eng Enterprise AI Team for constructive recommendations and technical discussions. The author would also like to thank the anonymous reviewers for identifying gaps and suggesting improvements.\n. C C Aggarwal, Outlier Analysis, Springer Publishing Company, Incorporated33194757702nd editionAggarwal, C. C. Outlier Analysis. Springer Publish- ing Company, Incorporated, 2nd edition, 2016. ISBN 3319475770.\n\nSemi-supervised anomaly detection via adversarial training. S Akcay, A A Abarghouei, T P Breckon, Ganomaly, abs/1805.06725CoRRAkcay, S., Abarghouei, A. A., and Breckon, T. P. Ganomaly: Semi-supervised anomaly detection via adversarial train- ing. CoRR, abs/1805.06725, 2018. URL http:// arxiv.org/abs/1805.06725.\n\nUnsupervised novelty detection using deep autoencoders with density based clustering. T Amarbayasgalan, B Jargalsaikhan, K Ryu, 10.3390/app8091468Applied Sciences. 8Amarbayasgalan, T., Jargalsaikhan, B., and Ryu, K. Unsu- pervised novelty detection using deep autoencoders with density based clustering. Applied Sciences, 8, 08 2018. doi: 10.3390/app8091468.\n\nNegative selection: How to generate detectors. M Ayara, J Timmis, R Lemos, L De Castro, Duncan , R , Proceedings of the 1st International Conference on Artificial Immune Systems (ICARIS). the 1st International Conference on Artificial Immune Systems (ICARIS)Ayara, M., Timmis, J., Lemos, R., De Castro, L., and Dun- can, R. Negative selection: How to generate detectors. Proceedings of the 1st International Conference on Artifi- cial Immune Systems (ICARIS), 01 2002.\n\n. D Baehrens, T Schroeter, S Harmeling, M Kawanabe, K Hansen, K.-R Mueller, How to explain individual classification decisions. Baehrens, D., Schroeter, T., Harmeling, S., Kawanabe, M., Hansen, K., and Mueller, K.-R. How to explain individual classification decisions, 2009.\n\nLayer-wise relevance propagation for neural networks with local renormalization layers. A Binder, G Montavon, S Bach, K M\u00fcller, W Samek, abs/1604.00825CoRRBinder, A., Montavon, G., Bach, S., M\u00fcller, K., and Samek, W. Layer-wise relevance propagation for neu- ral networks with local renormalization layers. CoRR, abs/1604.00825, 2016. URL http://arxiv.org/ abs/1604.00825.\n\nAnomaly detection using one-class neural networks. R Chalapathy, A K Menon, S Chawla, CoRR, abs/1802.06360Chalapathy, R., Menon, A. K., and Chawla, S. Anomaly detection using one-class neural networks. CoRR, abs/1802.06360, 2018. URL http://arxiv.org/ abs/1802.06360.\n\nAnomaly detection: A survey. V Chandola, A Banerjee, V Kumar, 10.1145/1541880.1541882ACM Comput. Surv. 413Chandola, V., Banerjee, A., and Kumar, V. Anomaly detection: A survey. ACM Comput. Surv., 41(3), July 2009. ISSN 0360-0300. doi: 10.1145/1541880. 1541882. URL https://doi.org/10.1145/ 1541880.1541882.\n\nTowards a rigorous science of interpretable machine learning. F Doshi-Velez, B Kim, Doshi-Velez, F. and Kim, B. Towards a rigorous science of interpretable machine learning, 2017.\n\nSelfnonself discrimination in a computer. S Forrest, A S Perelson, L Allen, R Cherukuri, 10.1109/RISP.1994.296580Proceedings of 1994 IEEE Computer Society Symposium on Research in Security and Privacy. 1994 IEEE Computer Society Symposium on Research in Security and PrivacyForrest, S., Perelson, A. S., Allen, L., and Cherukuri, R. Self- nonself discrimination in a computer. In Proceedings of 1994 IEEE Computer Society Symposium on Research in Security and Privacy, pp. 202-212, May 1994. doi: 10.1109/RISP.1994.296580.\n\nCombining negative selection and classification techniques for anomaly detection. F Gonzalez, D Dasgupta, R Kozma, doi: 10.1109/ CEC.2002.1007012Proceedings of the 2002 Congress on Evolutionary Computation. CEC'02. the 2002 Congress on Evolutionary Computation. CEC'021Cat. No.02TH8600Gonzalez, F., Dasgupta, D., and Kozma, R. Combining neg- ative selection and classification techniques for anomaly detection. In Proceedings of the 2002 Congress on Evo- lutionary Computation. CEC'02 (Cat. No.02TH8600), volume 1, pp. 705-710 vol.1, May 2002. doi: 10.1109/ CEC.2002.1007012.\n\nTemporal anomaly detection: calibrating the surprise. CoRR, abs/1705.10085. E Gutflaish, A Kontorovich, S Sabato, O Biller, O Sofer, Gutflaish, E., Kontorovich, A., Sabato, S., Biller, O., and Sofer, O. Temporal anomaly detection: calibrating the surprise. CoRR, abs/1705.10085, 2017. URL http: //arxiv.org/abs/1705.10085.\n\nUniversal function approximation by deep neural nets with bounded width and relu activations. B Hanin, 10.3390/math7100992Mathematics. 710992Hanin, B. Universal function approximation by deep neural nets with bounded width and relu activations. Mathe- matics, 7(10):992, Oct 2019. ISSN 2227-7390. doi: 10.3390/math7100992. URL http://dx.doi.org/ 10.3390/math7100992.\n\nExtended isolation forest. CoRR, abs/1811.02141. S Hariri, M C Kind, R J Brunner, Hariri, S., Kind, M. C., and Brunner, R. J. Extended isolation forest. CoRR, abs/1811.02141, 2018. URL http://arxiv.org/abs/1811.02141.\n\nAnomaly process detection using negative selection algorithm and classification techniques. S Hosseini, H Seilani, 10.1007/s12530-019-09317-1Evolving Systems. 122019Hosseini, S. and Seilani, H. Anomaly process detec- tion using negative selection algorithm and classifica- tion techniques. Evolving Systems, 12 2019. doi: 10.1007/s12530-019-09317-1.\n\nRevisiting negative selection algorithms. Z Ji, D Dasgupta, 10.1162/evco.2007.15.2.22317535140Evolutionary Computation. 152Ji, Z. and Dasgupta, D. Revisiting negative selection algo- rithms. Evolutionary Computation, 15(2):223-251, 2007. doi: 10.1162/evco.2007.15.2.223. URL https://doi. org/10.1162/evco.2007.15.2.223. PMID: 17535140.\n\nA study of detector generation algorithms based on artificial immune in intrusion detection system. C Jinyin, D Yang, M Naofumi, 10.1109/ICCRD.2011.5763961ICCRD2011 -2011 3rd International Conference on Computer Research and Development. 1Jinyin, C., Yang, D., and Naofumi, M. A study of detec- tor generation algorithms based on artificial immune in intrusion detection system. ICCRD2011 -2011 3rd Inter- national Conference on Computer Research and Devel- opment, 1, 03 2011. doi: 10.1109/ICCRD.2011.5763961.\n\nIsolation forest. F T Liu, K M Ting, Z Zhou, 10.1109/ICDM.2008.17Eighth IEEE International Conference on Data Mining. Liu, F. T., Ting, K. M., and Zhou, Z. Isolation forest. In 2008 Eighth IEEE International Conference on Data Mining, pp. 413-422, Dec 2008. doi: 10.1109/ICDM.2008.17.\n\nAnomaly detection and attribution in networks with temporally correlated traffic. I Nevat, D M Divakaran, S G Nagarajan, P Zhang, L Su, L L Ko, V L L Thing, Nevat, I., Divakaran, D. M., Nagarajan, S. G., Zhang, P., Su, L., Ko, L. L., and Thing, V. L. L. Anomaly detection and attribution in networks with temporally correlated traffic.\n\n. 10.1109/TNET.2017.2765719IEEE/ACM Transactions on Networking. 261IEEE/ACM Transactions on Networking, 26(1):131-144, Feb 2018. ISSN 1558-2566. doi: 10.1109/TNET.2017. 2765719.\n\n. S Rayana, Library, Rayana, S. ODDS library, 2016. URL http://odds. cs.stonybrook.edu.\n\nDeep one-class classification. L Ruff, R Vandermeulen, N Goernitz, L Deecke, S A Siddiqui, A Binder, E M\u00fcller, M Kloft, Proceedings of the 35th International Conference on Machine Learning. Dy, J. and Krause, A.the 35th International Conference on Machine LearningStockholmsmssan, Stockholm Sweden80Ruff, L., Vandermeulen, R., Goernitz, N., Deecke, L., Siddiqui, S. A., Binder, A., M\u00fcller, E., and Kloft, M. Deep one-class classification. In Dy, J. and Krause, A. (eds.), Proceedings of the 35th Interna- tional Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 4393- 4402, Stockholmsmssan, Stockholm Sweden, 10-15 Jul 2018. PMLR. URL http://proceedings.mlr. press/v80/ruff18a.html.\n\nDeep one-class classification using intra-class splitting. P Schlachter, Y Liao, Yang , B , 10.1109/DSW.2019.8755576IEEE Data Science Workshop. DSWSchlachter, P., Liao, Y., and Yang, B. Deep one-class classi- fication using intra-class splitting. 2019 IEEE Data Sci- ence Workshop (DSW), Jun 2019. doi: 10.1109/dsw.2019. 8755576. URL http://dx.doi.org/10.1109/ DSW.2019.8755576.\n\nUnsupervised anomaly detection with generative adversarial networks to guide marker discovery. T Schlegl, P Seeb\u00f6ck, S M Waldstein, U Schmidt-Erfurth, G Langs, abs/1703.05921CoRRSchlegl, T., Seeb\u00f6ck, P., Waldstein, S. M., Schmidt-Erfurth, U., and Langs, G. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. CoRR, abs/1703.05921, 2017. URL http: //arxiv.org/abs/1703.05921.\n\nEstimating the support of a high-dimensional distribution. B Sch\u00f6lkopf, J C Platt, J C Shawe-Taylor, A J Smola, R C Williamson, 10.1162/0899766017502649650899-7667. doi: 10.1162/ 089976601750264965Neural Comput. 13714431471Sch\u00f6lkopf, B., Platt, J. C., Shawe-Taylor, J. C., Smola, A. J., and Williamson, R. C. Estimating the support of a high-dimensional distribution. Neural Comput., 13(7): 14431471, July 2001. ISSN 0899-7667. doi: 10.1162/ 089976601750264965. URL https://doi.org/10. 1162/089976601750264965.\n\nLearning important features through propagating activation differences. A Shrikumar, P Greenside, A Kundaje, Shrikumar, A., Greenside, P., and Kundaje, A. Learning important features through propagating activation differ- ences, 2017.\n\nDeep inside convolutional networks: Visualising image classification models and saliency maps. K Simonyan, A Vedaldi, A Zisserman, Simonyan, K., Vedaldi, A., and Zisserman, A. Deep inside convolutional networks: Visualising image classification models and saliency maps, 2013.\n\nIs negative selection appropriate for anomaly detection. T Stibor, P Mohr, J Timmis, C Eckert, 10.1145/1068009.1068061Stibor, T., Mohr, P., Timmis, J., and Eckert, C. Is negative selection appropriate for anomaly detection? pp. 321- 328, 01 2005. doi: 10.1145/1068009.1068061.\n\nAxiomatic attribution for deep networks. CoRR, abs/1703.01365. M Sundararajan, A Taly, Yan , Q , Sundararajan, M., Taly, A., and Yan, Q. Axiomatic attri- bution for deep networks. CoRR, abs/1703.01365, 2017. URL http://arxiv.org/abs/1703.01365.\n\nProbability in high dimension apc 550 lecture notes. R Van Handel, van Handel, R. Probability in high dimension apc 550 lecture notes, December 2016. URL https://web. math.princeton.edu/\u02dcrvan/APC550.pdf.\n\nR Vershynin, High-Dimensional, Probability, An Introduction with Applications in Data Science. Cambridge Series in Statistical and Probabilistic Mathematics. Cambridge University PressVershynin, R. High-Dimensional Probability: An In- troduction with Applications in Data Science. Cam- bridge Series in Statistical and Probabilistic Mathe- matics. Cambridge University Press, 2018. ISBN 9781108415194. URL https://books.google. com/books?id=J-VjswEACAAJ.\n\nA real negative selection algorithm with evolutionary preference for anomaly detection. T Yang, W Chen, Li , T , doi: 10.1515/ phys-2017-0013Open Physics. 15Yang, T., Chen, W., and Li, T. A real negative selection algorithm with evolutionary preference for anomaly de- tection. Open Physics, 15, 04 2017. doi: 10.1515/ phys-2017-0013.\n", "annotations": {"author": "[{\"end\":136,\"start\":124},{\"end\":149,\"start\":137},{\"end\":158,\"start\":150}]", "publisher": null, "author_last_name": "[{\"end\":135,\"start\":129},{\"end\":148,\"start\":142}]", "author_first_name": "[{\"end\":128,\"start\":124},{\"end\":141,\"start\":137},{\"end\":157,\"start\":150}]", "author_affiliation": null, "title": "[{\"end\":117,\"start\":1},{\"end\":275,\"start\":159}]", "venue": "[{\"end\":346,\"start\":277}]", "abstract": "[{\"end\":1998,\"start\":481}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3454,\"start\":3431},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4522,\"start\":4506},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8079,\"start\":8057},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8216,\"start\":8193},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8638,\"start\":8617},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8656,\"start\":8638},{\"end\":8681,\"start\":8656},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8881,\"start\":8860},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8900,\"start\":8881},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8923,\"start\":8902},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9561,\"start\":9545},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9729,\"start\":9705},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10300,\"start\":10276},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":10459,\"start\":10440},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11564,\"start\":11535},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11825,\"start\":11800},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12057,\"start\":12035},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12091,\"start\":12071},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12397,\"start\":12379},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13027,\"start\":13006},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":13789,\"start\":13765},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14288,\"start\":14265},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14310,\"start\":14288},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":14333,\"start\":14310},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":14353,\"start\":14333},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14403,\"start\":14376},{\"end\":14883,\"start\":14863},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":16351,\"start\":16333},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":16367,\"start\":16351},{\"end\":18028,\"start\":18027},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":19330,\"start\":19307},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":22286,\"start\":22273},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":24779,\"start\":24752},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":30662,\"start\":30643},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31236,\"start\":31215},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":31425,\"start\":31411}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":41591,\"start\":41007},{\"attributes\":{\"id\":\"fig_1\"},\"end\":41987,\"start\":41592},{\"attributes\":{\"id\":\"fig_2\"},\"end\":42229,\"start\":41988},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":42564,\"start\":42230},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":43043,\"start\":42565}]", "paragraph": "[{\"end\":3320,\"start\":2014},{\"end\":4108,\"start\":3322},{\"end\":4524,\"start\":4110},{\"end\":4639,\"start\":4526},{\"end\":4949,\"start\":4641},{\"end\":5353,\"start\":4951},{\"end\":5709,\"start\":5355},{\"end\":6053,\"start\":5711},{\"end\":6571,\"start\":6055},{\"end\":6630,\"start\":6573},{\"end\":6729,\"start\":6632},{\"end\":6807,\"start\":6731},{\"end\":6882,\"start\":6809},{\"end\":7931,\"start\":6914},{\"end\":9335,\"start\":7933},{\"end\":9838,\"start\":9337},{\"end\":11313,\"start\":9868},{\"end\":12248,\"start\":11315},{\"end\":13296,\"start\":12250},{\"end\":13964,\"start\":13298},{\"end\":14884,\"start\":13966},{\"end\":15044,\"start\":14925},{\"end\":15386,\"start\":15131},{\"end\":15506,\"start\":15388},{\"end\":15968,\"start\":15528},{\"end\":16522,\"start\":16015},{\"end\":17267,\"start\":16524},{\"end\":18179,\"start\":17269},{\"end\":18622,\"start\":18181},{\"end\":20379,\"start\":18624},{\"end\":21005,\"start\":20431},{\"end\":21268,\"start\":21007},{\"end\":21754,\"start\":21270},{\"end\":22179,\"start\":21756},{\"end\":22589,\"start\":22181},{\"end\":23012,\"start\":22591},{\"end\":24677,\"start\":23065},{\"end\":24904,\"start\":24679},{\"end\":25141,\"start\":24960},{\"end\":25999,\"start\":25143},{\"end\":26524,\"start\":26001},{\"end\":26809,\"start\":26526},{\"end\":27160,\"start\":26811},{\"end\":27427,\"start\":27228},{\"end\":27804,\"start\":27448},{\"end\":28311,\"start\":27825},{\"end\":28716,\"start\":28327},{\"end\":29017,\"start\":28718},{\"end\":29885,\"start\":29039},{\"end\":32851,\"start\":29887},{\"end\":34064,\"start\":32853},{\"end\":34599,\"start\":34066},{\"end\":35364,\"start\":34626},{\"end\":35946,\"start\":35366},{\"end\":36689,\"start\":35976},{\"end\":37292,\"start\":36691},{\"end\":39698,\"start\":37294},{\"end\":40786,\"start\":39700},{\"end\":41006,\"start\":40808}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9867,\"start\":9839},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15130,\"start\":15045},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15527,\"start\":15507},{\"attributes\":{\"id\":\"formula_3\"},\"end\":20430,\"start\":20380},{\"attributes\":{\"id\":\"formula_4\"},\"end\":27227,\"start\":27161},{\"attributes\":{\"id\":\"formula_5\"},\"end\":27447,\"start\":27428},{\"attributes\":{\"id\":\"formula_6\"},\"end\":27824,\"start\":27805}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":31448,\"start\":31441},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":33324,\"start\":33317}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2012,\"start\":2000},{\"attributes\":{\"n\":\"2.\"},\"end\":6912,\"start\":6885},{\"attributes\":{\"n\":\"3.\"},\"end\":14923,\"start\":14887},{\"attributes\":{\"n\":\"3.1.\"},\"end\":16013,\"start\":15971},{\"attributes\":{\"n\":\"3.2.\"},\"end\":23063,\"start\":23015},{\"end\":24958,\"start\":24907},{\"attributes\":{\"n\":\"4.\"},\"end\":28325,\"start\":28314},{\"attributes\":{\"n\":\"4.1.\"},\"end\":29037,\"start\":29020},{\"attributes\":{\"n\":\"4.2.\"},\"end\":34624,\"start\":34602},{\"attributes\":{\"n\":\"5.\"},\"end\":35974,\"start\":35949},{\"end\":40806,\"start\":40789},{\"end\":41023,\"start\":41008},{\"end\":41603,\"start\":41593},{\"end\":41999,\"start\":41989},{\"end\":42240,\"start\":42231},{\"end\":42575,\"start\":42566}]", "table": "[{\"end\":42564,\"start\":42280},{\"end\":43043,\"start\":42581}]", "figure_caption": "[{\"end\":41591,\"start\":41025},{\"end\":41987,\"start\":41605},{\"end\":42229,\"start\":42001},{\"end\":42280,\"start\":42242},{\"end\":42581,\"start\":42577}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35805,\"start\":35797},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":35943,\"start\":35935}]", "bib_author_first_name": "[{\"end\":44344,\"start\":44343},{\"end\":44346,\"start\":44345},{\"end\":44614,\"start\":44613},{\"end\":44623,\"start\":44622},{\"end\":44625,\"start\":44624},{\"end\":44639,\"start\":44638},{\"end\":44641,\"start\":44640},{\"end\":44954,\"start\":44953},{\"end\":44972,\"start\":44971},{\"end\":44989,\"start\":44988},{\"end\":45275,\"start\":45274},{\"end\":45284,\"start\":45283},{\"end\":45294,\"start\":45293},{\"end\":45303,\"start\":45302},{\"end\":45321,\"start\":45315},{\"end\":45325,\"start\":45324},{\"end\":45700,\"start\":45699},{\"end\":45712,\"start\":45711},{\"end\":45725,\"start\":45724},{\"end\":45738,\"start\":45737},{\"end\":45750,\"start\":45749},{\"end\":45763,\"start\":45759},{\"end\":46062,\"start\":46061},{\"end\":46072,\"start\":46071},{\"end\":46084,\"start\":46083},{\"end\":46092,\"start\":46091},{\"end\":46102,\"start\":46101},{\"end\":46399,\"start\":46398},{\"end\":46413,\"start\":46412},{\"end\":46415,\"start\":46414},{\"end\":46424,\"start\":46423},{\"end\":46646,\"start\":46645},{\"end\":46658,\"start\":46657},{\"end\":46670,\"start\":46669},{\"end\":46987,\"start\":46986},{\"end\":47002,\"start\":47001},{\"end\":47148,\"start\":47147},{\"end\":47159,\"start\":47158},{\"end\":47161,\"start\":47160},{\"end\":47173,\"start\":47172},{\"end\":47182,\"start\":47181},{\"end\":47712,\"start\":47711},{\"end\":47724,\"start\":47723},{\"end\":47736,\"start\":47735},{\"end\":48283,\"start\":48282},{\"end\":48296,\"start\":48295},{\"end\":48311,\"start\":48310},{\"end\":48321,\"start\":48320},{\"end\":48331,\"start\":48330},{\"end\":48625,\"start\":48624},{\"end\":48948,\"start\":48947},{\"end\":48958,\"start\":48957},{\"end\":48960,\"start\":48959},{\"end\":48968,\"start\":48967},{\"end\":48970,\"start\":48969},{\"end\":49210,\"start\":49209},{\"end\":49222,\"start\":49221},{\"end\":49511,\"start\":49510},{\"end\":49517,\"start\":49516},{\"end\":49906,\"start\":49905},{\"end\":49916,\"start\":49915},{\"end\":49924,\"start\":49923},{\"end\":50336,\"start\":50335},{\"end\":50338,\"start\":50337},{\"end\":50345,\"start\":50344},{\"end\":50347,\"start\":50346},{\"end\":50355,\"start\":50354},{\"end\":50686,\"start\":50685},{\"end\":50695,\"start\":50694},{\"end\":50697,\"start\":50696},{\"end\":50710,\"start\":50709},{\"end\":50712,\"start\":50711},{\"end\":50725,\"start\":50724},{\"end\":50734,\"start\":50733},{\"end\":50740,\"start\":50739},{\"end\":50742,\"start\":50741},{\"end\":50748,\"start\":50747},{\"end\":50752,\"start\":50749},{\"end\":51122,\"start\":51121},{\"end\":51240,\"start\":51239},{\"end\":51248,\"start\":51247},{\"end\":51264,\"start\":51263},{\"end\":51276,\"start\":51275},{\"end\":51286,\"start\":51285},{\"end\":51288,\"start\":51287},{\"end\":51300,\"start\":51299},{\"end\":51310,\"start\":51309},{\"end\":51320,\"start\":51319},{\"end\":51993,\"start\":51992},{\"end\":52007,\"start\":52006},{\"end\":52018,\"start\":52014},{\"end\":52022,\"start\":52021},{\"end\":52409,\"start\":52408},{\"end\":52420,\"start\":52419},{\"end\":52431,\"start\":52430},{\"end\":52433,\"start\":52432},{\"end\":52446,\"start\":52445},{\"end\":52465,\"start\":52464},{\"end\":52792,\"start\":52791},{\"end\":52805,\"start\":52804},{\"end\":52807,\"start\":52806},{\"end\":52816,\"start\":52815},{\"end\":52818,\"start\":52817},{\"end\":52834,\"start\":52833},{\"end\":52836,\"start\":52835},{\"end\":52845,\"start\":52844},{\"end\":52847,\"start\":52846},{\"end\":53317,\"start\":53316},{\"end\":53330,\"start\":53329},{\"end\":53343,\"start\":53342},{\"end\":53576,\"start\":53575},{\"end\":53588,\"start\":53587},{\"end\":53599,\"start\":53598},{\"end\":53816,\"start\":53815},{\"end\":53826,\"start\":53825},{\"end\":53834,\"start\":53833},{\"end\":53844,\"start\":53843},{\"end\":54100,\"start\":54099},{\"end\":54116,\"start\":54115},{\"end\":54126,\"start\":54123},{\"end\":54130,\"start\":54129},{\"end\":54336,\"start\":54335},{\"end\":54488,\"start\":54487},{\"end\":55032,\"start\":55031},{\"end\":55040,\"start\":55039},{\"end\":55049,\"start\":55047},{\"end\":55053,\"start\":55052}]", "bib_author_last_name": "[{\"end\":44355,\"start\":44347},{\"end\":44373,\"start\":44357},{\"end\":44620,\"start\":44615},{\"end\":44636,\"start\":44626},{\"end\":44649,\"start\":44642},{\"end\":44659,\"start\":44651},{\"end\":44969,\"start\":44955},{\"end\":44986,\"start\":44973},{\"end\":44993,\"start\":44990},{\"end\":45281,\"start\":45276},{\"end\":45291,\"start\":45285},{\"end\":45300,\"start\":45295},{\"end\":45313,\"start\":45304},{\"end\":45709,\"start\":45701},{\"end\":45722,\"start\":45713},{\"end\":45735,\"start\":45726},{\"end\":45747,\"start\":45739},{\"end\":45757,\"start\":45751},{\"end\":45771,\"start\":45764},{\"end\":46069,\"start\":46063},{\"end\":46081,\"start\":46073},{\"end\":46089,\"start\":46085},{\"end\":46099,\"start\":46093},{\"end\":46108,\"start\":46103},{\"end\":46410,\"start\":46400},{\"end\":46421,\"start\":46416},{\"end\":46431,\"start\":46425},{\"end\":46655,\"start\":46647},{\"end\":46667,\"start\":46659},{\"end\":46676,\"start\":46671},{\"end\":46999,\"start\":46988},{\"end\":47006,\"start\":47003},{\"end\":47156,\"start\":47149},{\"end\":47170,\"start\":47162},{\"end\":47179,\"start\":47174},{\"end\":47192,\"start\":47183},{\"end\":47721,\"start\":47713},{\"end\":47733,\"start\":47725},{\"end\":47742,\"start\":47737},{\"end\":48293,\"start\":48284},{\"end\":48308,\"start\":48297},{\"end\":48318,\"start\":48312},{\"end\":48328,\"start\":48322},{\"end\":48337,\"start\":48332},{\"end\":48631,\"start\":48626},{\"end\":48955,\"start\":48949},{\"end\":48965,\"start\":48961},{\"end\":48978,\"start\":48971},{\"end\":49219,\"start\":49211},{\"end\":49230,\"start\":49223},{\"end\":49514,\"start\":49512},{\"end\":49526,\"start\":49518},{\"end\":49913,\"start\":49907},{\"end\":49921,\"start\":49917},{\"end\":49932,\"start\":49925},{\"end\":50342,\"start\":50339},{\"end\":50352,\"start\":50348},{\"end\":50360,\"start\":50356},{\"end\":50692,\"start\":50687},{\"end\":50707,\"start\":50698},{\"end\":50722,\"start\":50713},{\"end\":50731,\"start\":50726},{\"end\":50737,\"start\":50735},{\"end\":50745,\"start\":50743},{\"end\":50758,\"start\":50753},{\"end\":51129,\"start\":51123},{\"end\":51138,\"start\":51131},{\"end\":51245,\"start\":51241},{\"end\":51261,\"start\":51249},{\"end\":51273,\"start\":51265},{\"end\":51283,\"start\":51277},{\"end\":51297,\"start\":51289},{\"end\":51307,\"start\":51301},{\"end\":51317,\"start\":51311},{\"end\":51326,\"start\":51321},{\"end\":52004,\"start\":51994},{\"end\":52012,\"start\":52008},{\"end\":52417,\"start\":52410},{\"end\":52428,\"start\":52421},{\"end\":52443,\"start\":52434},{\"end\":52462,\"start\":52447},{\"end\":52471,\"start\":52466},{\"end\":52802,\"start\":52793},{\"end\":52813,\"start\":52808},{\"end\":52831,\"start\":52819},{\"end\":52842,\"start\":52837},{\"end\":52858,\"start\":52848},{\"end\":53327,\"start\":53318},{\"end\":53340,\"start\":53331},{\"end\":53351,\"start\":53344},{\"end\":53585,\"start\":53577},{\"end\":53596,\"start\":53589},{\"end\":53609,\"start\":53600},{\"end\":53823,\"start\":53817},{\"end\":53831,\"start\":53827},{\"end\":53841,\"start\":53835},{\"end\":53851,\"start\":53845},{\"end\":54113,\"start\":54101},{\"end\":54121,\"start\":54117},{\"end\":54347,\"start\":54337},{\"end\":54498,\"start\":54489},{\"end\":54516,\"start\":54500},{\"end\":54529,\"start\":54518},{\"end\":55037,\"start\":55033},{\"end\":55045,\"start\":55041}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":44551,\"start\":44341},{\"attributes\":{\"doi\":\"abs/1805.06725\",\"id\":\"b1\"},\"end\":44865,\"start\":44553},{\"attributes\":{\"doi\":\"10.3390/app8091468\",\"id\":\"b2\",\"matched_paper_id\":115671315},\"end\":45225,\"start\":44867},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":14644089},\"end\":45695,\"start\":45227},{\"attributes\":{\"id\":\"b4\"},\"end\":45971,\"start\":45697},{\"attributes\":{\"doi\":\"abs/1604.00825\",\"id\":\"b5\"},\"end\":46345,\"start\":45973},{\"attributes\":{\"doi\":\"CoRR, abs/1802.06360\",\"id\":\"b6\"},\"end\":46614,\"start\":46347},{\"attributes\":{\"doi\":\"10.1145/1541880.1541882\",\"id\":\"b7\",\"matched_paper_id\":207172599},\"end\":46922,\"start\":46616},{\"attributes\":{\"id\":\"b8\"},\"end\":47103,\"start\":46924},{\"attributes\":{\"doi\":\"10.1109/RISP.1994.296580\",\"id\":\"b9\"},\"end\":47627,\"start\":47105},{\"attributes\":{\"doi\":\"doi: 10.1109/ CEC.2002.1007012\",\"id\":\"b10\",\"matched_paper_id\":60817314},\"end\":48204,\"start\":47629},{\"attributes\":{\"id\":\"b11\"},\"end\":48528,\"start\":48206},{\"attributes\":{\"doi\":\"10.3390/math7100992\",\"id\":\"b12\",\"matched_paper_id\":10737109},\"end\":48896,\"start\":48530},{\"attributes\":{\"id\":\"b13\"},\"end\":49115,\"start\":48898},{\"attributes\":{\"doi\":\"10.1007/s12530-019-09317-1\",\"id\":\"b14\",\"matched_paper_id\":213654269},\"end\":49466,\"start\":49117},{\"attributes\":{\"doi\":\"10.1162/evco.2007.15.2.223\",\"id\":\"b15\",\"matched_paper_id\":6519512},\"end\":49803,\"start\":49468},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":16148649},\"end\":50315,\"start\":49805},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":6505449},\"end\":50601,\"start\":50317},{\"attributes\":{\"id\":\"b18\"},\"end\":50938,\"start\":50603},{\"attributes\":{\"id\":\"b19\"},\"end\":51117,\"start\":50940},{\"attributes\":{\"id\":\"b20\"},\"end\":51206,\"start\":51119},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":49312162},\"end\":51931,\"start\":51208},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":75137296},\"end\":52311,\"start\":51933},{\"attributes\":{\"id\":\"b23\"},\"end\":52730,\"start\":52313},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2110475},\"end\":53242,\"start\":52732},{\"attributes\":{\"id\":\"b25\"},\"end\":53478,\"start\":53244},{\"attributes\":{\"id\":\"b26\"},\"end\":53756,\"start\":53480},{\"attributes\":{\"id\":\"b27\"},\"end\":54034,\"start\":53758},{\"attributes\":{\"id\":\"b28\"},\"end\":54280,\"start\":54036},{\"attributes\":{\"id\":\"b29\"},\"end\":54485,\"start\":54282},{\"attributes\":{\"id\":\"b30\"},\"end\":54941,\"start\":54487},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":126334162},\"end\":55277,\"start\":54943}]", "bib_title": "[{\"end\":44951,\"start\":44867},{\"end\":45272,\"start\":45227},{\"end\":46643,\"start\":46616},{\"end\":47145,\"start\":47105},{\"end\":47709,\"start\":47629},{\"end\":48622,\"start\":48530},{\"end\":49207,\"start\":49117},{\"end\":49508,\"start\":49468},{\"end\":49903,\"start\":49805},{\"end\":50333,\"start\":50317},{\"end\":51237,\"start\":51208},{\"end\":51990,\"start\":51933},{\"end\":52789,\"start\":52732},{\"end\":55029,\"start\":54943}]", "bib_author": "[{\"end\":44357,\"start\":44343},{\"end\":44375,\"start\":44357},{\"end\":44622,\"start\":44613},{\"end\":44638,\"start\":44622},{\"end\":44651,\"start\":44638},{\"end\":44661,\"start\":44651},{\"end\":44971,\"start\":44953},{\"end\":44988,\"start\":44971},{\"end\":44995,\"start\":44988},{\"end\":45283,\"start\":45274},{\"end\":45293,\"start\":45283},{\"end\":45302,\"start\":45293},{\"end\":45315,\"start\":45302},{\"end\":45324,\"start\":45315},{\"end\":45328,\"start\":45324},{\"end\":45711,\"start\":45699},{\"end\":45724,\"start\":45711},{\"end\":45737,\"start\":45724},{\"end\":45749,\"start\":45737},{\"end\":45759,\"start\":45749},{\"end\":45773,\"start\":45759},{\"end\":46071,\"start\":46061},{\"end\":46083,\"start\":46071},{\"end\":46091,\"start\":46083},{\"end\":46101,\"start\":46091},{\"end\":46110,\"start\":46101},{\"end\":46412,\"start\":46398},{\"end\":46423,\"start\":46412},{\"end\":46433,\"start\":46423},{\"end\":46657,\"start\":46645},{\"end\":46669,\"start\":46657},{\"end\":46678,\"start\":46669},{\"end\":47001,\"start\":46986},{\"end\":47008,\"start\":47001},{\"end\":47158,\"start\":47147},{\"end\":47172,\"start\":47158},{\"end\":47181,\"start\":47172},{\"end\":47194,\"start\":47181},{\"end\":47723,\"start\":47711},{\"end\":47735,\"start\":47723},{\"end\":47744,\"start\":47735},{\"end\":48295,\"start\":48282},{\"end\":48310,\"start\":48295},{\"end\":48320,\"start\":48310},{\"end\":48330,\"start\":48320},{\"end\":48339,\"start\":48330},{\"end\":48633,\"start\":48624},{\"end\":48957,\"start\":48947},{\"end\":48967,\"start\":48957},{\"end\":48980,\"start\":48967},{\"end\":49221,\"start\":49209},{\"end\":49232,\"start\":49221},{\"end\":49516,\"start\":49510},{\"end\":49528,\"start\":49516},{\"end\":49915,\"start\":49905},{\"end\":49923,\"start\":49915},{\"end\":49934,\"start\":49923},{\"end\":50344,\"start\":50335},{\"end\":50354,\"start\":50344},{\"end\":50362,\"start\":50354},{\"end\":50694,\"start\":50685},{\"end\":50709,\"start\":50694},{\"end\":50724,\"start\":50709},{\"end\":50733,\"start\":50724},{\"end\":50739,\"start\":50733},{\"end\":50747,\"start\":50739},{\"end\":50760,\"start\":50747},{\"end\":51131,\"start\":51121},{\"end\":51140,\"start\":51131},{\"end\":51247,\"start\":51239},{\"end\":51263,\"start\":51247},{\"end\":51275,\"start\":51263},{\"end\":51285,\"start\":51275},{\"end\":51299,\"start\":51285},{\"end\":51309,\"start\":51299},{\"end\":51319,\"start\":51309},{\"end\":51328,\"start\":51319},{\"end\":52006,\"start\":51992},{\"end\":52014,\"start\":52006},{\"end\":52021,\"start\":52014},{\"end\":52025,\"start\":52021},{\"end\":52419,\"start\":52408},{\"end\":52430,\"start\":52419},{\"end\":52445,\"start\":52430},{\"end\":52464,\"start\":52445},{\"end\":52473,\"start\":52464},{\"end\":52804,\"start\":52791},{\"end\":52815,\"start\":52804},{\"end\":52833,\"start\":52815},{\"end\":52844,\"start\":52833},{\"end\":52860,\"start\":52844},{\"end\":53329,\"start\":53316},{\"end\":53342,\"start\":53329},{\"end\":53353,\"start\":53342},{\"end\":53587,\"start\":53575},{\"end\":53598,\"start\":53587},{\"end\":53611,\"start\":53598},{\"end\":53825,\"start\":53815},{\"end\":53833,\"start\":53825},{\"end\":53843,\"start\":53833},{\"end\":53853,\"start\":53843},{\"end\":54115,\"start\":54099},{\"end\":54123,\"start\":54115},{\"end\":54129,\"start\":54123},{\"end\":54133,\"start\":54129},{\"end\":54349,\"start\":54335},{\"end\":54500,\"start\":54487},{\"end\":54518,\"start\":54500},{\"end\":54531,\"start\":54518},{\"end\":55039,\"start\":55031},{\"end\":55047,\"start\":55039},{\"end\":55052,\"start\":55047},{\"end\":55056,\"start\":55052}]", "bib_venue": "[{\"end\":44611,\"start\":44553},{\"end\":45029,\"start\":45013},{\"end\":45413,\"start\":45328},{\"end\":45823,\"start\":45773},{\"end\":46059,\"start\":45973},{\"end\":46396,\"start\":46347},{\"end\":46717,\"start\":46701},{\"end\":46984,\"start\":46924},{\"end\":47305,\"start\":47218},{\"end\":47842,\"start\":47774},{\"end\":48280,\"start\":48206},{\"end\":48663,\"start\":48652},{\"end\":48945,\"start\":48898},{\"end\":49274,\"start\":49258},{\"end\":49586,\"start\":49562},{\"end\":50041,\"start\":49960},{\"end\":50433,\"start\":50382},{\"end\":50683,\"start\":50603},{\"end\":51002,\"start\":50967},{\"end\":51396,\"start\":51328},{\"end\":52075,\"start\":52049},{\"end\":52406,\"start\":52313},{\"end\":52942,\"start\":52929},{\"end\":53314,\"start\":53244},{\"end\":53573,\"start\":53480},{\"end\":53813,\"start\":53758},{\"end\":54097,\"start\":54036},{\"end\":54333,\"start\":54282},{\"end\":54643,\"start\":54531},{\"end\":55096,\"start\":55084},{\"end\":45485,\"start\":45415},{\"end\":47379,\"start\":47307},{\"end\":47897,\"start\":47844},{\"end\":51505,\"start\":51419}]"}}}, "year": 2023, "month": 12, "day": 17}