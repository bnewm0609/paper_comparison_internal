{"id": 48352434, "updated": "2023-10-01 22:44:26.393", "metadata": {"title": "Knowledge Distillation by On-the-Fly Native Ensemble", "authors": "[{\"first\":\"Xu\",\"last\":\"Lan\",\"middle\":[]},{\"first\":\"Xiatian\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Shaogang\",\"last\":\"Gong\",\"middle\":[]}]", "venue": "NeurIPS", "journal": "7528-7538", "publication_date": {"year": 2018, "month": 6, "day": 12}, "abstract": "Knowledge distillation is effective to train small and generalisable network models for meeting the low-memory and fast running requirements. Existing offline distillation methods rely on a strong pre-trained teacher, which enables favourable knowledge discovery and transfer but requires a complex two-phase training procedure. Online counterparts address this limitation at the price of lacking a highcapacity teacher. In this work, we present an On-the-fly Native Ensemble (ONE) strategy for one-stage online distillation. Specifically, ONE trains only a single multi-branch network while simultaneously establishing a strong teacher on-the- fly to enhance the learning of target network. Extensive evaluations show that ONE improves the generalisation performance a variety of deep neural networks more significantly than alternative methods on four image classification dataset: CIFAR10, CIFAR100, SVHN, and ImageNet, whilst having the computational efficiency advantages.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1806.04606", "mag": "2964220233", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/LanZG18", "doi": null}}, "content": {"source": {"pdf_hash": "ea3670c27619c892bc768b36ea056dcf00616f29", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1806.04606v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "85914ee4c626f30eda95cc45b7b83e81e514291e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/ea3670c27619c892bc768b36ea056dcf00616f29.txt", "contents": "\nKnowledge Distillation by On-the-Fly Native Ensemble\n\n\nXu Lan \nQueen Mary University of London\n\n\nXiatian Zhu \nVision Semantics Ltd\n\n\nShaogang Gong \nQueen Mary University of London\n\n\nKnowledge Distillation by On-the-Fly Native Ensemble\n\nKnowledge distillation is effective to train small and generalisable network models for meeting the low-memory and fast running requirements. Existing offline distillation methods rely on a strong pre-trained teacher, which enables favourable knowledge discovery and transfer but requires a complex two-phase training procedure. Online counterparts address this limitation at the price of lacking a highcapacity teacher. In this work, we present an On-the-fly Native Ensemble (ONE) learning strategy for one-stage online distillation. Specifically, ONE trains only a single multi-branch network while simultaneously establishing a strong teacher onthe-fly to enhance the learning of target network. Extensive evaluations show that ONE improves the generalisation performance a variety of deep neural networks more significantly than alternative methods on four image classification dataset: CIFAR10, CIFAR100, SVHN, and ImageNet, whilst having the computational efficiency advantages.\n\nIntroduction\n\nDeep neural networks have gained impressive success in many computer vision tasks [1; 2; 3; 4; 5; 6; 7; 8]. However, the performance advantages are often gained at the cost of training and deploying resource-intensive networks with large depth and/or width [9; 4; 2]. This leads to the necessity of developing compact yet still discriminative models. Knowledge distillation [10] is one generic meta-solution among the others such as parameter binarisation [11; 12] and filter pruning [13]. The distillation process begins with training a high-capacity teacher model (or an ensemble of networks), followed by learning a smaller student model which is encouraged to match the teacher's predictions [10] and/or feature representations [14; 15]. Whilst promising the student model quality improvement from aligning with a pre-trained teacher model, this strategy requires a longer training time, significant extra computational cost and large memory (for heavy teacher) with the need for a more complex multi-stage training process, all of which are commercially unattractive [16].\n\nTo simplify the distillation training process as above, simultaneous distillation algorithms [17; 16] have been developed to perform online knowledge teaching in a one-phase learning procedure. Instead of pre-training a static teacher model, these methods train simultaneously a set of (typically two) student models which learn from each other in a peer-teaching manner. This approach merges the training processes of the teacher and student models, and uses the peer network to provide the teaching knowledge. Beyond the original understanding of distillation that requires the teacher model larger than the student, they allow to improve any-capacity model performance, leading to a more generically applicable technique. This peer-teaching strategy sometimes even outperforms the teacher based offline distillation, with the plausible reason that the large teacher model tends to overfit the training set and finally provides less information additional to the original training labels [16].\n\nHowever, existing online distillation has a number of drawbacks: (1) Each peer-student may only provide limited extra information and resulting in suboptimal distillation; (2) Training multiple students significantly increases the computational cost and resource burdens; (3) It requires asynchronous model updating with a notorious need of carefully ordering the operations of prediction and back-propagation across networks. We consider that all the weaknesses are due to the lacking of an appropriate teacher role in the online distillation processing.\n\nIn this work, we propose a novel online knowledge distillation method that is not only more efficient (lower training cost) and but also more effective (higher model generalisation improvement) as compared to previous alternative methods. In training, the proposed approach constructs a multibranch variant of a given target network by adding auxiliary branches, creates a native ensemble teacher model from all branches on-the-fly, and learns simultaneously each branch plus the teacher model subject to the same target label constraints. Each branch is trained with two objective loss terms: a conventional softmax cross-entropy loss which matches with the ground-truth label distributions, and a distillation loss which aligns to the teacher's prediction distributions. Comparing with creating a set of student networks, a multi-branch single model is more efficient to train whilst achieving superior generalisation performance and avoiding asynchronous model update. In test, we simply convert the trained multi-branch model back to the original (single-branch) network architecture by removing the auxiliary branches, therefore introducing no test-time cost increase. In doing so, we derive an On-the-Fly Native Ensemble (ONE) teacher based simultaneous distillation training approach that not only eliminates the computationally expensive need for pre-training the teacher model in an isolated stage as the offline counterparts, but also further improves the quality of online distillation.\n\nExtensive comparative experiments on four benchmarks (CIFAR10/100, SVHN, and ImageNet) show that the proposed ONE distillation method enables to train more generalisable target models in a one-phase process than the alternative strategies of offline learning a larger teacher network or simultaneously distilling peer students, the previous state-of-the-art techniques for training small target models.\n\n\nRelated Work\n\nKnowledge Distillation. There have been a number of attempts to transfer knowledge between varying-capacity network models [18; 10; 14; 15]. Hinton et al. [10] distilled knowledge from a large pre-trained teacher model to improve a small target net. The rationale behind is the use of extra supervision from teacher model in target model training, beyond a conventional supervised learning objective such as the cross-entropy loss subject to labelled training data. The extra supervision were typically obtained from a pre-trained powerful teacher model in the form of classification probabilities [10], feature representation [14; 15], or inter-layer flow (the inner product of feature maps) [19].\n\nRecently, knowledge distillation has been exploited to distil easy-to-train large networks into harderto-train small networks [15]. Previous distillation methods often take offline learning strategies, requiring at least two phases of training. The more recently proposed deep mutual learning [17] overcomes this limitation by conducting online distillation in one-phase training between two peer student models. Anil et al. [16] further extended this idea to accelerate large scale distributed neural network training.\n\nHowever, existing online distillation methods lacks a strong \"teacher\" model which limits the efficacy of knowledge discovery and transfer. Like offline counterpart, multiple nets are needed to be trained and therefore computationally expensive. We overcome both limitations by designing a new variant of online distillation training algorithm characterised by simultaneously learning a teacher on-the-fly and the target net and performing batch-wise knowledge transfer in a one-phase procedure.\n\nMulti-branch Architectures. Multi-branch based neural networks have been widely exploited in computer vision tasks [3; 20; 4]. For example, ResNet [4] can be thought of as a category of twobranch networks where one branch is the identity mapping. Recently, \"grouped convolution\" [21; 22] has been used as a replacement of standard convolution in constructing multi-branch net architectures. These building blocks are often utilised as templates to build deeper networks for achieving stronger modelling capacity. Whilst sharing the multi-branch principle, our ONE method is fundamentally different from these above existing methods since our objective is to improve the training quality of any given target network, rather than presenting a new multi-branch building block. In other words, our method is a meta network learning algorithm, independent of specific network architectures.\n\n\nKnowledge Distillation by On-the-Fly Native Ensemble\nClassifier \u2026 Branch 0 Gate Logits \"# # \"# $ \"# % \"# & '( $ '( % '( & $ * % * & * # % & $ # * Ensemble Predictions\n\nKnowledge Distillation\n\nEnsemble Logits\n\nHigh-Level Layers (Res4X block)\nLow-Level Layers (Conv1, Res2X, Res3X) Branch 1\nBranch m Predictions Figure 1: Overview of online distillation training of ResNet-110 by the proposed On-the-fly Native Ensemble (ONE). With ONE, we reconfigure the network by adding m auxiliary branches which share the low-level layers with the target net. Each branch with shared layers makes an individual model, and their ensemble is used to build the teacher model. During a mini-batch training process, we employ the teacher to collect knowledge from individual branch models on-the-fly, which in turn is distilled back to all branches to enhance model learning in a close-loop form. In test, auxiliary branches can be either discarded or kept based on the deployment efficiency requirement.\n\nWe formulate an online distillation training method based on a concept of On-the-fly Native Ensemble (ONE). For understanding convenience, we take ResNet-110 [4] on CIFAR100 dataset as an example. It is straightforward to apply ONE to other network architectures. For model training, we often have access to n labelled training samples D = {(x i , y i )} n i with each belonging to one of C classes y i \u2208 Y = {1, 2, \u00b7 \u00b7 \u00b7 , C}. The network \u03b8 outputs a probabilistic class posterior p(c|x, \u03b8) for a sample x over a class c as:\np(c|x, \u03b8) = f sm (z) = exp(z c ) C j=1 exp(z j ) , c \u2208 Y (1)\nwhere z is the logits or unnormalised log probability outputted by the network \u03b8. To train a multi-class classification model, we often adopt the Cross-Entropy (CE) measurement between the predicted and ground-truth label distributions as the objective function:\nL ce = \u2212 C c=1 \u03b4 c,y log p(c|x, \u03b8)(2)\nwhere \u03b4 c,y is Dirac delta which returns 1 if c is the ground-truth label, and 0 otherwise. With the CE loss, the network is trained to predict the correct class label in a principle of maximum likelihood. To further enhance the model generalisation, we concurrently distil extra knowledge from an on-the-fly native ensemble (ONE) teacher in training.\n\nOn-the-Fly Native Ensemble. Overview of the ONE architecture is depicted in Fig 1. The ONE consists of two components: (1) m auxiliary branches with the same configuration (Res4X block and an individual classifier), each of which serves as an independent classification model with shared low-level stages/layers. This is because low-level features are largely shared across different network instances which allows to reduce the training cost.\n\n(2) A gate component which learn to ensemble all (m+1) branches to build a stronger teacher model. It is constructed by one FC layer followed by batch normalisation, ReLU activation, and softmax, and uses the same input features as the branches.\n\nOur ONE method is established based on a multi-branch design specially for model training with several merits: (1) Enable the possibility of creating a strong teacher model without training a set of networks at a high computational cost; (2) Introduce a multi-branch simultaneous learning regularisation which benefits model generalisation (Fig 2(a)); (3) Avoid the tedious need for asynchronous update between multiple networks.\n\nUnder the reconfiguration of network, we add a separate CE loss L i ce to all branches which simultaneously learn to predict the same ground-truth class label of a training sample. While sharing the most layers, each branch can be considered as an independent multi-class classifier in that all of them independently learn high-level semantic representations. Consequently, taking the ensemble of all branches (classifiers) can make a stronger teacher model. One common way of ensembling models is to average individual predictions. This may ignore the diversity and importance variety of member models in the ensemble. We therefore learn to ensemble by the gating component as:\nz e = m i=0 g i \u00b7 z i (3)\nwhere g i is the importance score of the i-th branch's logits z i , and z e is the logits of the ONE teacher.\n\nIn particular, we denote the original branch as i = 0 for indexing convenience. We train the ONE teacher model with the CE loss L e ce (Eq (2)) same as the branch models. Knowledge Distillation. Given the teacher logits for each training sample, we distil knowledge back into all branches in a closed-loop form. For facilitating knowledge transfer, we compute soft probability distributions at a temperature of T for individual branches and the ONE teacher as:\np i (c|x, \u03b8 i ) = exp(z c i /T ) C j=1 exp(z j i /T ) , c \u2208 Y (4) p e (c|x, \u03b8 e ) = exp(z c e /T ) C j=1 exp(z j e /T ) , c \u2208 Y(5)\nwhere i denotes the branch index, i = 0, \u00b7 \u00b7 \u00b7 , m, \u03b8 i and \u03b8 e the parameters of branch and teacher models respectively. Higher values of T lead to more softened distributions.\n\nTo quantify the alignment between individual branches and the teacher in their predictions, we use the Kullback Leibler divergence written as:\nL kl = m i=0 C j=1p e (j|x, \u03b8 e ) logp e (j|x, \u03b8 e ) p i (j|x, \u03b8 i ) .(6)\nOverall Loss Function. We obtain the overall loss function for online distillation training by the proposed ONE as:\nL = m i=0 L i ce + L e ce + T 2 * L kl(7)\nwhere L i ce and L e ce are the conventional CE loss terms associated with the i-th branch and the ONE teacher, respectively. The gradient magnitudes produced by the soft targetsp are scaled by 1 T 2 , so we multiply the distillation loss term by a factor T 2 to ensure that the relative contributions of ground-truth and teacher probability distributions remain roughly unchanged. Following [10], we set T = 3 in our all experiments.\n\nModel Training and Deployment. The model optimisation and deployment details are summarised in Alg 1. Unlike the two-phase offline distillation training, the target network and the ONE teacher are trained simultaneously and collaboratively, with the knowledge distillation from the teacher to the target being conducted in each mini-batch and throughout the whole training procedure. Since there is only one multi-branch network rather than multiple networks, we only need to carry out the same stochastic gradient descent through (m + 1) branches, and training the whole network until Compute the teacher logits (Eq (3)); 8: Compute the soft targets of all branches and teacher (Eq (4)); 9: Distil knowledge from the teacher to all branches (Eq (6)); 10: Obtain the final loss function (Eq (7)); 11: Update the model parameters {\u03b8 i } m i=0 by SGD. 12 Performance metric. We adopted the common top-n (n=1, 5) classification error rate. For computational cost of model training and test, we used the criterion of floating point operations (FLOPs). For any network trained by ONE, we report the average performance of all branch outputs with standard deviation.\n\nExperiments setup. We implemented all networks and model training procedures in Pytorch. For all datasets, we adopted the same experimental settings as [27; 21] for making fair comparisons. We used the SGD with Nesterov momentum and set the momentum to 0.9, following a standard learning rate schedule that drops from 0.1 to 0.01 at 50% training and to 0.001 at 75%. For the training budget, CIFAR/SVHN/ImageNet used 300/40/90 epochs respectively. We adopt a 3-branch ONE (m = 2) design unless stated otherwise. We separate the last block of each backbone net from the parameter sharing (except on ImageNet we separate the last 2 blocks for giving more learning capacity to branches) without extra structural optimisation (see ResNet-110 for example in Fig 1).\n\nMost state-of-the-art nets are in block structure designs.\n\n\nEvaluation of On-the-Fly Native Ensemble\n\nResults on CIFAR and SVHN.  Results on ImageNet. Table 2 shows the comparative performances on the 1000-classes ImageNet. It is shown that the proposed ONE learning algorithm still yields more effective training and more generalisable models in comparison to vanilla SGD. This indicates that our method can be generically applied to large scale image classification settings.\n\n\nComparison with Distillation Methods\n\nWe compared our ONE method with two representative distillation methods: Knowledge Distillation (KD) [10] and Deep Mutual Learning (DML) [17]. For the offline competitor KD, we used a large network ResNet-110 as the teacher and a small network ResNet-32 as the student. For the online methods DML and ONE, we evaluated their performance using either ResNet-32 or ResNet-110 as the target model. We observed from Table 3 that: (1) ONE outperforms both KD (offline) and DML (online) distillation methods in error rates, validating the performance advantages of our method over alternative algorithms when applied to different CNN models. (2) ONE takes the least model training cost and the same test cost as others, and therefore leading to the most cost-effective solution.   Table 4: Comparison with ensembling methods on CIFAR100. \"*\": Reported results. TrCost/TeCost: Training/test cost, in unit of 10 8 FLOPs. Red/Blue: Best and second best results.   Table 5 shows the benefits of individual ONE components on CIFAR100 using ResNet-110. We have these observations: (1) Without online distillation (Eq (6)), the target network suffers a performance drop of 3.11% (24.73-21.62) in test error rate. This performance drop validates the efficacy and quality of ONE teacher in terms of performance superiority over individual branch models. This can be more clearly seen in Fig 2 that ONE teacher fits better to training data and generalises better to test data. Due to the closed-loop design, ONE teacher also mutually benefits from distillation, reducing its error rate from 21.84% to 21.03%. With distillation, the target model effectively approaches ONE teacher (Fig 2(a) vs. 2(b)) on both training and test error performance, indicating the success of teacher knowledge transfer. Interestingly, even without distillation, ONE still achieves better generalisation than the vanilla algorithm. This suggests that our multi-branch design brings some positive regularisation effect by concurrently and jointly learning the shared low-level layers subject to more diverse high-level representation knowledge. (2) Without sharing the low-level layers not only increases the training cost (83% increase), but also leads to weaker performance (0.83% error rate increase). The plausible reason is the lacking of multi-branch regularisation effect as indicated in Fig 2(a). (3) Using average ensemble of branches without gating (Eq (3)) causes a performance decrease of 0.64% (22.26-21.62). This suggests the benefit of adaptively exploiting the branch diversity in forming the ONE teacher.\n\n\nComparison with Ensembling Methods\n\n\nModel Component Analysis\n\nThe main experiments use 3 branches in ONE.   \n\n\nModel Generalisation Analysis\n\nWe aim to give insights on why ONE trained networks yield a better generalisation capability. A few previous studies [31 ; 32] demonstrate that the width of a local optimum is related to model generalisation. The general understanding is that, the surfaces of training and test error largely mirror each other and it is favourable to converge the models to broader optima. As such, the trained model remains approximately optimal even under small perturbations in test time. Next, we exploited this criterion to examine the quality of model solutions \u03b8 v , \u03b8 m , \u03b8 o discovered by the vanilla, DML and ONE training algorithms respectively. This analysis was conducted on CIFAR100 using ResNet-110. Specifically, to test the width of local optimum, we added small perturbations to the solutions as \u03b8 * (d, v) = \u03b8 * +d \u00b7 v, * \u2208 {v, m, o} where v is a uniform distributed direction vector with a unit length, and d \u2208 [0, 5] controls the change magnitude. At each magnitude scale, we further sampled randomly 5 different direction vectors to disturb the solutions. We then tested the robustness of all perturbed models in training and test error rates. The training error is quantified as the cross-entropy measurement between the predicted and ground-truth label distributions.\n\nWe observed in Fig 3 that: (1) The robustness of each solution against parameter perturbation appears to indicate the width of local optima as: \u03b8 v < \u03b8 m < \u03b8 o . That is, ONE seems to find the widest local minimum among three therefore more likely to generalise better than others.\n\n(2) Comparing with DML, vanilla and ONE found deeper local optima with lower training errors. This indicates that DML may probably get stuck in training, therefore scarifying the vanilla's exploring capability for more generalisable solutions to exchange the ability of identifying wider optima. In contrast, our method further improves the capability of identifying wider minima over DML whilst maintaining the original exploration quality. We analysed the variance of ONE's branches over the training epochs in comparison to the conventional ensemble method. We used ResNet-32 as the base net and test CIFAR100. We quantified the model variance by the average prediction differences on training samples between every two models/branches in Euclidean space. Fig. 4 shows that a 3-Net Ensemble involves larger inter-model variances than ONE with 3 branches throughout the training process. This means that the branches of ONE have higher correlations, due to the proposed learning constraint from the distillation loss that enforces them align to the same teacher prediction, which probably hurts the ensemble performance. However, in the mean generalisation capability (another fundamental aspect in ensemble learning), ONE's branches (the average error rate 26.61\u00b10.06%) are much superior to individual models of a conventional ensemble (31.07\u00b10.41%), leading to stronger ensembling performance.\n\n\nConclusion\n\nIn this work, we presented a novel On-the-fly Native Ensemble (ONE) strategy for improving deep network learning through online knowledge distillation in a one-stage training procedure. With ONE, we can more discriminatively learn both small and large networks with less computational cost, beyond the conventional offline alternatives that are typically formulated to learn better small models alone. Our method is also superior over existing online counterparts due to the unique capability of constructing a high-capacity online teacher to more effectively mine knowledge from the training data and supervise the target network concurrently. Extensive experiments show that a variety of deep networks can all benefit from the ONE approach on four image classification benchmarks. Significantly, smaller networks obtain more performance gains, making our method specially good for low-memory and fast execution scenarios.\n\nAlgorithm 1\n1Knowledge Distillation by On-the-Fly Native Ensemble 1: Input: Labelled training data D; Training epochs \u03c4 ; Auxiliary branch number m; 2: Output: Trained target CNN model \u03b8 0 , and auxiliary models {\u03b8 i\n\nFigure 2 :\n2Effect of online distillation. Network: ResNet-110.\n\nFigure 3 :\n3Robustness test of ResNet-110 solutions found by ONE, DML, and vanilla training algorithms on CIFAR100. Each curve corresponds to a specific perturbation direction v.\n\n( 3 )Figure 4 :\n34For each solution, the performance changes on training and test data are highly consistent, confirming the earlier observation[31; 32]. Model variance during training.\n\n\n: end 13: /* Testing */ 14: Single model deployment: Use \u03b8 0 ; 15: Ensemble deployment (ONE-E): Use {\u03b8 i } m i=0 .convergence, as the standard single-model incremental batch-wise training. There is no complexity of asynchronous updating among different networks which is required in deep mutual learning[17].Once the model is trained, we simply remove all the auxiliary branches and obtain the original network architecture for deployment. Hence, our ONE method does not increase test-time cost. However, if there is less constraint on computation budget and model performance is more important, we can deploy it as an ensemble with all trained branches, denoted as \"ONE-E\".4 Experiments \n\nDatasets. We used four multi-class categorisation benchmark datasets in our evaluations. (1) CIFAR-\n10 [23]: A natural images dataset that contains 50,000/10,000 training/test samples drawn from 10 \nobject classes (in total 60,000 images). Each class has 6,000 images sized at 32 \u00d7 32 pixels. (2) \nCIFAR-100 [23]: A similar dataset as CIFAR10 that also contains 50,000/10,000 training/test images \nbut covering 100 fine-grained classes. Each class has 600 images. (3) SVHN: The Street View House \nNumbers (SVHN) dataset consists of 73,257/26,032 standard training/text images and an extra set of \n531,131 training images. We used all the training data without data augmentation as [24; 25]. (4) \nImageNet: The 1,000-class dataset from ILSVRC 2012 [26] provides 1.2 million images for training, \nand 50,000 for validation. \n\n\n\nTable 1 Table 1 :\n11BC(L=190, k=40) + ONE 3.13\u00b10.07 16.35\u00b10.05 1.63\u00b10.05 25.6M Evaluation of our ONE method on CIFAR and SVHN. Metric: Error rate (%). particularly for small models achieving larger performance gains. This suggests the generic superiority of our method for online knowledge distillation from the on-the-fly teacher to the target model. (2) All individual branches have similar performances, indicating that they have made sufficient agreement and exchanged respective knowledge to each other well through the proposed ONE teacher model during training. ResNet-18 + ONE 29.02\u00b10.17 10.13\u00b10.12compares top-1 error rate performances of four varying-\ncapacity state-of-the-art network models trained by the conventional and our ONE learning algorithms. \nWe have these observations: (1) All different networks benefit from the ONE training algorithm, \n\n\nTable 2 :\n2Evaluation of our ONE method on ImageNet. Metric: Error rate (%).\n\nTable 3 :\n3Comparison with knowledge distillation methods on CIFAR100. \"*\": Reported results. \nTrCost/TeCost: Training/test cost, in unit of 10 8 FLOPs. Red/Blue: Best and second best results. \n\n\nTable 4\n4compares the performances of our multi-branch (3 branches) based model ONE-E and \nstandard ensembling methods. It is shown that ONE-E yields not only the best test error but also \nallows for most efficient deployment with lowest test cost. These advantages are achieved at second \nlowest training cost. Whilst Snapshot Ensemble takes the least training cost, its generalisation \nperformance is unsatisfied with a notorious drawback of highest deployment cost. \n\nIt is worth noting that ONE (without branch ensemble) already outperforms comprehensively 2-Net \nEnsemble in terms of error rate, training and test cost. Comparing 3-Net Ensemble, ONE is able to \napproach the generalisation performance whilst having even larger model training and test efficiency \nadvantages. \n\nConfiguration \nFull \nW/O Online Distillation W/O Sharing Layers W/O Gating \nONE \n21.62\u00b10.26 \n24.73\u00b10.20 \n22.45\u00b10.52 \n22.26\u00b10.23 \nONE-E \n21.03 \n21.84 \n20.57 \n21.79 \n\n\n\nTable 5 :\n5Model component analysis on CIFAR100. Network: ResNet-110.\n\nTable 6\n6shows that ONE scales well with more branches \n\n\nTable 6 :\n6Benefit of adding branches to ONE on CIFAR100. Network: ResNet-32.during training hence its performance advantage over the independently trained network (31.18% \nerror rate). \n\n0 \n50 \n100 150 200 250 300 \nEpoch \n\n0 \n\n10 \n\n20 \n\n30 \n\n40 \n\n50 \n\nError (%) \n\nVanilla (Train) \nVanilla (Test) \nONE (Train) \nONE (Test) \nONE-E (Train) \nONE-E (Test) \n\n\nAcknowledgementsThis work was partly supported by the China Scholarship Council, Vision Semantics Limited, the Royal Society Newton Advanced Fellowship Programme (NA150459), and Innovate UK Industrial Challenge Project on Developing and Commercialising Intelligent Video Analytics Solutions for Public Safety (98111-571149).\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Advances in Neural Information Processing Systems. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, 2012.\n\nVery deep convolutional networks for large-scale image recognition. Karen Simonyan, Andrew Zisserman, arXivKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv, 2015.\n\nGoing deeper with convolutions. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, IEEE Conference on Computer Vision and Pattern Recognition. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich, et al. Going deeper with convolutions. In IEEE Conference on Computer Vision and Pattern Recognition, 2015.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, IEEE Conference on Computer Vision and Pattern Recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition, pages 770-778, 2016.\n\nFast r-cnn. Ross Girshick, IEEE International Conference on Computer Vision. Ross Girshick. Fast r-cnn. In IEEE International Conference on Computer Vision, 2015.\n\nFully convolutional networks for semantic segmentation. Jonathan Long, Evan Shelhamer, Trevor Darrell, IEEE Conference on Computer Vision and Pattern Recognition. Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition, 2015.\n\nPerson re-identification by deep joint learning of multi-loss classification. Wei Li, Xiatian Zhu, Shaogang Gong, International Joint Conference of Artificial Intelligence. Wei Li, Xiatian Zhu, and Shaogang Gong. Person re-identification by deep joint learning of multi-loss classification. In International Joint Conference of Artificial Intelligence, 2017.\n\nPerson search by multi-scale matching. Xu Lan, Xiatian Zhu, Shaogang Gong, European Conference on Computer Vision. Xu Lan, Xiatian Zhu, and Shaogang Gong. Person search by multi-scale matching. In European Conference on Computer Vision, 2018.\n\nSergey Zagoruyko, Nikos Komodakis, Wide residual networks. arXiv. Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv, 2016.\n\nDistilling the knowledge in a neural network. Geoffrey Hinton, Oriol Vinyals, Jeff Dean, arXivGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv, 2015.\n\nXnor-net: Imagenet classification using binary convolutional neural networks. Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, Ali Farhadi, European Conference on Computer Vision. Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. Xnor-net: Imagenet classification using binary convolutional neural networks. In European Conference on Computer Vision, pages 525-542, 2016.\n\nDeep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. Song Han, Huizi Mao, William J Dally, International Conference on Learning Representations. Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016.\n\nPruning filters for efficient convnets. Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, Hans Peter Graf, International Conference on Learning Representations. Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning filters for efficient convnets. In International Conference on Learning Representations, 2017.\n\nDo deep nets really need to be deep?. Jimmy Ba, Rich Caruana, Advances in neural information processing systems. Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In Advances in neural information processing systems, pages 2654-2662, 2014.\n\nAdriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, Yoshua Bengio, Fitnets: Hints for thin deep nets. arXiv. Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. arXiv, 2014.\n\nLarge scale distributed neural network training through online distillation. Rohan Anil, Gabriel Pereyra, Alexandre Passos, Robert Ormandi, George E Dahl, Geoffrey E Hinton, International Conference on Learning Representations. Rohan Anil, Gabriel Pereyra, Alexandre Passos, Robert Ormandi, George E Dahl, and Geof- frey E Hinton. Large scale distributed neural network training through online distillation. In International Conference on Learning Representations, 2018.\n\nDeep mutual learning. Ying Zhang, Tao Xiang, Timothy M Hospedales, Huchuan Lu, arXiv:1706.00384arXiv preprintYing Zhang, Tao Xiang, Timothy M Hospedales, and Huchuan Lu. Deep mutual learning. arXiv preprint arXiv:1706.00384, 2017.\n\nModel compression. Cristian Bucilua, Rich Caruana, Alexandru Niculescu-Mizil, Proceedings of the 12th ACM SIGKDD. the 12th ACM SIGKDDACMCristian Bucilua, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD, pages 535-541. ACM, 2006.\n\nA gift from knowledge distillation: Fast optimization, network minimization and transfer learning. Junho Yim, Donggyu Joo, Jihoon Bae, Junmo Kim, IEEE Conference on Computer Vision and Pattern Recognition. Junho Yim, Donggyu Joo, Jihoon Bae, and Junmo Kim. A gift from knowledge distillation: Fast optimization, network minimization and transfer learning. In IEEE Conference on Computer Vision and Pattern Recognition, 2017.\n\nRethinking the inception architecture for computer vision. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Re- thinking the inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2818-2826, 2016.\n\nAggregated residual transformations for deep neural networks. Saining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, Kaiming He, Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on. IEEESaining Xie, Ross Girshick, Piotr Doll\u00e1r, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on, pages 5987-5995. IEEE, 2017.\n\nGao Huang, Shichen Liu, Laurens Van Der Maaten, Kilian Q Weinberger, arXiv:1711.09224Condensenet: An efficient densenet using learned group convolutions. arXiv preprintGao Huang, Shichen Liu, Laurens van der Maaten, and Kilian Q Weinberger. Condensenet: An efficient densenet using learned group convolutions. arXiv preprint arXiv:1711.09224, 2017.\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, Geoffrey Hinton, Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. 2009.\n\n. Gao Huang, Zhuang Liu, Q Kilian, Laurens Weinberger, Van Der Maaten, Densely connected convolutional networks. arXivGao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolutional networks. arXiv, 2016.\n\nDeeplysupervised nets. Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, Zhuowen Tu, Artificial Intelligence and Statistics. Chen-Yu Lee, Saining Xie, Patrick Gallagher, Zhengyou Zhang, and Zhuowen Tu. Deeply- supervised nets. In Artificial Intelligence and Statistics, pages 562-570, 2015.\n\nImagenet large scale visual recognition challenge. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, International Journal of Computer Vision. 1153Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision, 115(3):211-252, 2015.\n\nDeep networks with stochastic depth. Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, Kilian Q Weinberger, European Conference on Computer Vision. Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In European Conference on Computer Vision, 2016.\n\nDensely connected convolutional networks. Gao Huang, Zhuang Liu, Q Kilian, Laurens Weinberger, Van Der Maaten, IEEE Conference on Computer Vision and Pattern Recognition. 13Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolutional networks. In IEEE Conference on Computer Vision and Pattern Recognition, volume 1, page 3, 2017.\n\n. Shen Li Sun Gang Hu, arXiv:1709.015077Jie. Squeeze-and-excitation networks. arXiv preprintShen Li Sun Gang Hu, Jie. Squeeze-and-excitation networks. arXiv preprint arXiv:1709.01507, 7, 2017.\n\nGao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E Hopcroft, Kilian Q Weinberger, Snapshot ensembles: Train 1, get m for free. International Conference on Learning Representations. Gao Huang, Yixuan Li, Geoff Pleiss, Zhuang Liu, John E Hopcroft, and Kilian Q Weinberger. Snapshot ensembles: Train 1, get m for free. International Conference on Learning Representa- tions, 2017.\n\nOn large-batch training for deep learning: Generalization gap and sharp minima. Dheevatsa Nitish Shirish Keskar, Jorge Mudigere, Mikhail Nocedal, Ping Tak Peter Smelyanskiy, Tang, arXivNitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization gap and sharp minima. arXiv, 2016.\n\nEntropy-sgd: Biasing gradient descent into wide valleys. Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann Lecun, Carlo Baldassi, Christian Borgs, Jennifer Chayes, Levent Sagun, Riccardo Zecchina, arXiv:1611.01838arXiv preprintPratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi, Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina. Entropy-sgd: Biasing gradient descent into wide valleys. arXiv preprint arXiv:1611.01838, 2016.\n", "annotations": {"author": "[{\"end\":97,\"start\":56},{\"end\":133,\"start\":98},{\"end\":182,\"start\":134}]", "publisher": null, "author_last_name": "[{\"end\":62,\"start\":59},{\"end\":109,\"start\":106},{\"end\":147,\"start\":143}]", "author_first_name": "[{\"end\":58,\"start\":56},{\"end\":105,\"start\":98},{\"end\":142,\"start\":134}]", "author_affiliation": "[{\"end\":96,\"start\":64},{\"end\":132,\"start\":111},{\"end\":181,\"start\":149}]", "title": "[{\"end\":53,\"start\":1},{\"end\":235,\"start\":183}]", "venue": null, "abstract": "[{\"end\":1221,\"start\":237}]", "bib_ref": "[{\"end\":1343,\"start\":1319},{\"end\":1503,\"start\":1494},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1615,\"start\":1611},{\"end\":1701,\"start\":1693},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":1725,\"start\":1721},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1937,\"start\":1933},{\"end\":1977,\"start\":1969},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2313,\"start\":2309},{\"end\":2417,\"start\":2409},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3310,\"start\":3306},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3488,\"start\":3485},{\"end\":5927,\"start\":5911},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5947,\"start\":5943},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6390,\"start\":6386},{\"end\":6423,\"start\":6415},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6485,\"start\":6481},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6618,\"start\":6614},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6785,\"start\":6781},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6917,\"start\":6913},{\"end\":7631,\"start\":7621},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7656,\"start\":7653},{\"end\":7793,\"start\":7785},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9543,\"start\":9540},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14104,\"start\":14100},{\"end\":14769,\"start\":14767},{\"end\":14835,\"start\":14833},{\"end\":14899,\"start\":14896},{\"end\":14944,\"start\":14941},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":14996,\"start\":14994},{\"end\":15466,\"start\":15458},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":16692,\"start\":16688},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":16728,\"start\":16724},{\"end\":19067,\"start\":19055},{\"end\":19441,\"start\":19436},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":24169,\"start\":24165}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":23427,\"start\":23210},{\"attributes\":{\"id\":\"fig_1\"},\"end\":23492,\"start\":23428},{\"attributes\":{\"id\":\"fig_2\"},\"end\":23672,\"start\":23493},{\"attributes\":{\"id\":\"fig_3\"},\"end\":23859,\"start\":23673},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":25376,\"start\":23860},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":26240,\"start\":25377},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":26318,\"start\":26241},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":26514,\"start\":26319},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":27464,\"start\":26515},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":27535,\"start\":27465},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":27593,\"start\":27536},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":27948,\"start\":27594}]", "paragraph": "[{\"end\":2314,\"start\":1237},{\"end\":3311,\"start\":2316},{\"end\":3868,\"start\":3313},{\"end\":5367,\"start\":3870},{\"end\":5771,\"start\":5369},{\"end\":6486,\"start\":5788},{\"end\":7007,\"start\":6488},{\"end\":7504,\"start\":7009},{\"end\":8391,\"start\":7506},{\"end\":8601,\"start\":8586},{\"end\":8634,\"start\":8603},{\"end\":9380,\"start\":8683},{\"end\":9907,\"start\":9382},{\"end\":10231,\"start\":9969},{\"end\":10621,\"start\":10270},{\"end\":11066,\"start\":10623},{\"end\":11313,\"start\":11068},{\"end\":11744,\"start\":11315},{\"end\":12424,\"start\":11746},{\"end\":12560,\"start\":12451},{\"end\":13022,\"start\":12562},{\"end\":13331,\"start\":13154},{\"end\":13475,\"start\":13333},{\"end\":13665,\"start\":13550},{\"end\":14142,\"start\":13708},{\"end\":15304,\"start\":14144},{\"end\":16066,\"start\":15306},{\"end\":16126,\"start\":16068},{\"end\":16546,\"start\":16171},{\"end\":19169,\"start\":16587},{\"end\":19281,\"start\":19235},{\"end\":20589,\"start\":19315},{\"end\":20872,\"start\":20591},{\"end\":22271,\"start\":20874},{\"end\":23209,\"start\":22286}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8560,\"start\":8447},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8682,\"start\":8635},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9968,\"start\":9908},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10269,\"start\":10232},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12450,\"start\":12425},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13153,\"start\":13023},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13549,\"start\":13476},{\"attributes\":{\"id\":\"formula_7\"},\"end\":13707,\"start\":13666}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":16227,\"start\":16220},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":17006,\"start\":16999},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":17369,\"start\":17362},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":17549,\"start\":17542}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1235,\"start\":1223},{\"attributes\":{\"n\":\"2\"},\"end\":5786,\"start\":5774},{\"attributes\":{\"n\":\"3\"},\"end\":8446,\"start\":8394},{\"end\":8584,\"start\":8562},{\"attributes\":{\"n\":\"4.1\"},\"end\":16169,\"start\":16129},{\"attributes\":{\"n\":\"4.2\"},\"end\":16585,\"start\":16549},{\"attributes\":{\"n\":\"4.3\"},\"end\":19206,\"start\":19172},{\"attributes\":{\"n\":\"4.4\"},\"end\":19233,\"start\":19209},{\"attributes\":{\"n\":\"4.5\"},\"end\":19313,\"start\":19284},{\"attributes\":{\"n\":\"5\"},\"end\":22284,\"start\":22274},{\"end\":23222,\"start\":23211},{\"end\":23439,\"start\":23429},{\"end\":23504,\"start\":23494},{\"end\":23689,\"start\":23674},{\"end\":25395,\"start\":25378},{\"end\":26251,\"start\":26242},{\"end\":26329,\"start\":26320},{\"end\":26523,\"start\":26516},{\"end\":27475,\"start\":27466},{\"end\":27544,\"start\":27537},{\"end\":27604,\"start\":27595}]", "table": "[{\"end\":25376,\"start\":24536},{\"end\":26240,\"start\":25984},{\"end\":26514,\"start\":26331},{\"end\":27464,\"start\":26525},{\"end\":27593,\"start\":27546},{\"end\":27948,\"start\":27672}]", "figure_caption": "[{\"end\":23427,\"start\":23224},{\"end\":23492,\"start\":23441},{\"end\":23672,\"start\":23506},{\"end\":23859,\"start\":23692},{\"end\":24536,\"start\":23862},{\"end\":25984,\"start\":25398},{\"end\":26318,\"start\":26253},{\"end\":27535,\"start\":27477},{\"end\":27672,\"start\":27606}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8712,\"start\":8704},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10705,\"start\":10699},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11664,\"start\":11655},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16065,\"start\":16059},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17969,\"start\":17959},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18260,\"start\":18251},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18951,\"start\":18943},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20617,\"start\":20606},{\"end\":21639,\"start\":21633}]", "bib_author_first_name": "[{\"end\":28343,\"start\":28339},{\"end\":28360,\"start\":28356},{\"end\":28380,\"start\":28372},{\"end\":28382,\"start\":28381},{\"end\":28697,\"start\":28692},{\"end\":28714,\"start\":28708},{\"end\":28891,\"start\":28882},{\"end\":28904,\"start\":28901},{\"end\":28918,\"start\":28910},{\"end\":28930,\"start\":28924},{\"end\":28946,\"start\":28941},{\"end\":28961,\"start\":28953},{\"end\":28979,\"start\":28972},{\"end\":28994,\"start\":28987},{\"end\":29012,\"start\":29006},{\"end\":29390,\"start\":29383},{\"end\":29402,\"start\":29395},{\"end\":29418,\"start\":29410},{\"end\":29428,\"start\":29424},{\"end\":29696,\"start\":29692},{\"end\":29908,\"start\":29900},{\"end\":29919,\"start\":29915},{\"end\":29937,\"start\":29931},{\"end\":30265,\"start\":30262},{\"end\":30277,\"start\":30270},{\"end\":30291,\"start\":30283},{\"end\":30585,\"start\":30583},{\"end\":30598,\"start\":30591},{\"end\":30612,\"start\":30604},{\"end\":30794,\"start\":30788},{\"end\":30811,\"start\":30806},{\"end\":30984,\"start\":30976},{\"end\":30998,\"start\":30993},{\"end\":31012,\"start\":31008},{\"end\":31217,\"start\":31209},{\"end\":31236,\"start\":31229},{\"end\":31252,\"start\":31246},{\"end\":31264,\"start\":31261},{\"end\":31636,\"start\":31632},{\"end\":31647,\"start\":31642},{\"end\":31660,\"start\":31653},{\"end\":31662,\"start\":31661},{\"end\":31979,\"start\":31976},{\"end\":31988,\"start\":31984},{\"end\":32000,\"start\":31996},{\"end\":32018,\"start\":32013},{\"end\":32030,\"start\":32026},{\"end\":32036,\"start\":32031},{\"end\":32315,\"start\":32310},{\"end\":32324,\"start\":32320},{\"end\":32534,\"start\":32527},{\"end\":32550,\"start\":32543},{\"end\":32565,\"start\":32559},{\"end\":32574,\"start\":32566},{\"end\":32589,\"start\":32582},{\"end\":32605,\"start\":32600},{\"end\":32619,\"start\":32613},{\"end\":32906,\"start\":32901},{\"end\":32920,\"start\":32913},{\"end\":32939,\"start\":32930},{\"end\":32954,\"start\":32948},{\"end\":32970,\"start\":32964},{\"end\":32972,\"start\":32971},{\"end\":32987,\"start\":32979},{\"end\":32989,\"start\":32988},{\"end\":33322,\"start\":33318},{\"end\":33333,\"start\":33330},{\"end\":33348,\"start\":33341},{\"end\":33350,\"start\":33349},{\"end\":33370,\"start\":33363},{\"end\":33555,\"start\":33547},{\"end\":33569,\"start\":33565},{\"end\":33588,\"start\":33579},{\"end\":33916,\"start\":33911},{\"end\":33929,\"start\":33922},{\"end\":33941,\"start\":33935},{\"end\":33952,\"start\":33947},{\"end\":34306,\"start\":34297},{\"end\":34323,\"start\":34316},{\"end\":34341,\"start\":34335},{\"end\":34352,\"start\":34349},{\"end\":34369,\"start\":34361},{\"end\":34838,\"start\":34831},{\"end\":34848,\"start\":34844},{\"end\":34864,\"start\":34859},{\"end\":34880,\"start\":34873},{\"end\":34892,\"start\":34885},{\"end\":35215,\"start\":35212},{\"end\":35230,\"start\":35223},{\"end\":35243,\"start\":35236},{\"end\":35268,\"start\":35260},{\"end\":35621,\"start\":35617},{\"end\":35642,\"start\":35634},{\"end\":35755,\"start\":35752},{\"end\":35769,\"start\":35763},{\"end\":35776,\"start\":35775},{\"end\":35792,\"start\":35785},{\"end\":36026,\"start\":36019},{\"end\":36039,\"start\":36032},{\"end\":36052,\"start\":36045},{\"end\":36072,\"start\":36064},{\"end\":36087,\"start\":36080},{\"end\":36354,\"start\":36350},{\"end\":36371,\"start\":36368},{\"end\":36381,\"start\":36378},{\"end\":36394,\"start\":36386},{\"end\":36410,\"start\":36403},{\"end\":36425,\"start\":36421},{\"end\":36437,\"start\":36430},{\"end\":36451,\"start\":36445},{\"end\":36468,\"start\":36462},{\"end\":36484,\"start\":36477},{\"end\":36851,\"start\":36848},{\"end\":36861,\"start\":36859},{\"end\":36873,\"start\":36867},{\"end\":36885,\"start\":36879},{\"end\":36901,\"start\":36893},{\"end\":37156,\"start\":37153},{\"end\":37170,\"start\":37164},{\"end\":37177,\"start\":37176},{\"end\":37193,\"start\":37186},{\"end\":37504,\"start\":37488},{\"end\":37683,\"start\":37680},{\"end\":37697,\"start\":37691},{\"end\":37707,\"start\":37702},{\"end\":37722,\"start\":37716},{\"end\":37732,\"start\":37728},{\"end\":37734,\"start\":37733},{\"end\":37753,\"start\":37745},{\"end\":38152,\"start\":38143},{\"end\":38181,\"start\":38176},{\"end\":38199,\"start\":38192},{\"end\":38223,\"start\":38209},{\"end\":38509,\"start\":38503},{\"end\":38525,\"start\":38521},{\"end\":38546,\"start\":38539},{\"end\":38559,\"start\":38555},{\"end\":38572,\"start\":38567},{\"end\":38592,\"start\":38583},{\"end\":38608,\"start\":38600},{\"end\":38623,\"start\":38617},{\"end\":38639,\"start\":38631}]", "bib_author_last_name": "[{\"end\":28354,\"start\":28344},{\"end\":28370,\"start\":28361},{\"end\":28389,\"start\":28383},{\"end\":28706,\"start\":28698},{\"end\":28724,\"start\":28715},{\"end\":28899,\"start\":28892},{\"end\":28908,\"start\":28905},{\"end\":28922,\"start\":28919},{\"end\":28939,\"start\":28931},{\"end\":28951,\"start\":28947},{\"end\":28970,\"start\":28962},{\"end\":28985,\"start\":28980},{\"end\":29004,\"start\":28995},{\"end\":29023,\"start\":29013},{\"end\":29393,\"start\":29391},{\"end\":29408,\"start\":29403},{\"end\":29422,\"start\":29419},{\"end\":29432,\"start\":29429},{\"end\":29705,\"start\":29697},{\"end\":29913,\"start\":29909},{\"end\":29929,\"start\":29920},{\"end\":29945,\"start\":29938},{\"end\":30268,\"start\":30266},{\"end\":30281,\"start\":30278},{\"end\":30296,\"start\":30292},{\"end\":30589,\"start\":30586},{\"end\":30602,\"start\":30599},{\"end\":30617,\"start\":30613},{\"end\":30804,\"start\":30795},{\"end\":30821,\"start\":30812},{\"end\":30991,\"start\":30985},{\"end\":31006,\"start\":30999},{\"end\":31017,\"start\":31013},{\"end\":31227,\"start\":31218},{\"end\":31244,\"start\":31237},{\"end\":31259,\"start\":31253},{\"end\":31272,\"start\":31265},{\"end\":31640,\"start\":31637},{\"end\":31651,\"start\":31648},{\"end\":31668,\"start\":31663},{\"end\":31982,\"start\":31980},{\"end\":31994,\"start\":31989},{\"end\":32011,\"start\":32001},{\"end\":32024,\"start\":32019},{\"end\":32041,\"start\":32037},{\"end\":32318,\"start\":32316},{\"end\":32332,\"start\":32325},{\"end\":32541,\"start\":32535},{\"end\":32557,\"start\":32551},{\"end\":32580,\"start\":32575},{\"end\":32598,\"start\":32590},{\"end\":32611,\"start\":32606},{\"end\":32626,\"start\":32620},{\"end\":32911,\"start\":32907},{\"end\":32928,\"start\":32921},{\"end\":32946,\"start\":32940},{\"end\":32962,\"start\":32955},{\"end\":32977,\"start\":32973},{\"end\":32996,\"start\":32990},{\"end\":33328,\"start\":33323},{\"end\":33339,\"start\":33334},{\"end\":33361,\"start\":33351},{\"end\":33373,\"start\":33371},{\"end\":33563,\"start\":33556},{\"end\":33577,\"start\":33570},{\"end\":33604,\"start\":33589},{\"end\":33920,\"start\":33917},{\"end\":33933,\"start\":33930},{\"end\":33945,\"start\":33942},{\"end\":33956,\"start\":33953},{\"end\":34314,\"start\":34307},{\"end\":34333,\"start\":34324},{\"end\":34347,\"start\":34342},{\"end\":34359,\"start\":34353},{\"end\":34375,\"start\":34370},{\"end\":34842,\"start\":34839},{\"end\":34857,\"start\":34849},{\"end\":34871,\"start\":34865},{\"end\":34883,\"start\":34881},{\"end\":34895,\"start\":34893},{\"end\":35221,\"start\":35216},{\"end\":35234,\"start\":35231},{\"end\":35258,\"start\":35244},{\"end\":35279,\"start\":35269},{\"end\":35632,\"start\":35622},{\"end\":35649,\"start\":35643},{\"end\":35761,\"start\":35756},{\"end\":35773,\"start\":35770},{\"end\":35783,\"start\":35777},{\"end\":35803,\"start\":35793},{\"end\":35819,\"start\":35805},{\"end\":36030,\"start\":36027},{\"end\":36043,\"start\":36040},{\"end\":36062,\"start\":36053},{\"end\":36078,\"start\":36073},{\"end\":36090,\"start\":36088},{\"end\":36366,\"start\":36355},{\"end\":36376,\"start\":36372},{\"end\":36384,\"start\":36382},{\"end\":36401,\"start\":36395},{\"end\":36419,\"start\":36411},{\"end\":36428,\"start\":36426},{\"end\":36443,\"start\":36438},{\"end\":36460,\"start\":36452},{\"end\":36475,\"start\":36469},{\"end\":36494,\"start\":36485},{\"end\":36857,\"start\":36852},{\"end\":36865,\"start\":36862},{\"end\":36877,\"start\":36874},{\"end\":36891,\"start\":36886},{\"end\":36912,\"start\":36902},{\"end\":37162,\"start\":37157},{\"end\":37174,\"start\":37171},{\"end\":37184,\"start\":37178},{\"end\":37204,\"start\":37194},{\"end\":37220,\"start\":37206},{\"end\":37507,\"start\":37505},{\"end\":37689,\"start\":37684},{\"end\":37700,\"start\":37698},{\"end\":37714,\"start\":37708},{\"end\":37726,\"start\":37723},{\"end\":37743,\"start\":37735},{\"end\":37764,\"start\":37754},{\"end\":38174,\"start\":38153},{\"end\":38190,\"start\":38182},{\"end\":38207,\"start\":38200},{\"end\":38235,\"start\":38224},{\"end\":38241,\"start\":38237},{\"end\":38519,\"start\":38510},{\"end\":38537,\"start\":38526},{\"end\":38553,\"start\":38547},{\"end\":38565,\"start\":38560},{\"end\":38581,\"start\":38573},{\"end\":38598,\"start\":38593},{\"end\":38615,\"start\":38609},{\"end\":38629,\"start\":38624},{\"end\":38648,\"start\":38640}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":195908774},\"end\":28622,\"start\":28274},{\"attributes\":{\"id\":\"b1\"},\"end\":28848,\"start\":28624},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":206592484},\"end\":29335,\"start\":28850},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":206594692},\"end\":29678,\"start\":29337},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":206770307},\"end\":29842,\"start\":29680},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1629541},\"end\":30182,\"start\":29844},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":3489845},\"end\":30542,\"start\":30184},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":49907834},\"end\":30786,\"start\":30544},{\"attributes\":{\"id\":\"b8\"},\"end\":30928,\"start\":30788},{\"attributes\":{\"id\":\"b9\"},\"end\":31129,\"start\":30930},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":14925907},\"end\":31524,\"start\":31131},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":2134321},\"end\":31934,\"start\":31526},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":14089312},\"end\":32270,\"start\":31936},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":11536917},\"end\":32525,\"start\":32272},{\"attributes\":{\"id\":\"b14\"},\"end\":32822,\"start\":32527},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":2331610},\"end\":33294,\"start\":32824},{\"attributes\":{\"doi\":\"arXiv:1706.00384\",\"id\":\"b16\"},\"end\":33526,\"start\":33296},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":11253972},\"end\":33810,\"start\":33528},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":206596723},\"end\":34236,\"start\":33812},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":206593880},\"end\":34767,\"start\":34238},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":8485068},\"end\":35210,\"start\":34769},{\"attributes\":{\"doi\":\"arXiv:1711.09224\",\"id\":\"b21\"},\"end\":35560,\"start\":35212},{\"attributes\":{\"id\":\"b22\"},\"end\":35748,\"start\":35562},{\"attributes\":{\"id\":\"b23\"},\"end\":35994,\"start\":35750},{\"attributes\":{\"id\":\"b24\"},\"end\":36297,\"start\":35996},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":2930547},\"end\":36809,\"start\":36299},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":6773885},\"end\":37109,\"start\":36811},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":9433631},\"end\":37484,\"start\":37111},{\"attributes\":{\"doi\":\"arXiv:1709.01507\",\"id\":\"b28\"},\"end\":37678,\"start\":37486},{\"attributes\":{\"id\":\"b29\"},\"end\":38061,\"start\":37680},{\"attributes\":{\"id\":\"b30\"},\"end\":38444,\"start\":38063},{\"attributes\":{\"doi\":\"arXiv:1611.01838\",\"id\":\"b31\"},\"end\":38926,\"start\":38446}]", "bib_title": "[{\"end\":28337,\"start\":28274},{\"end\":28880,\"start\":28850},{\"end\":29381,\"start\":29337},{\"end\":29690,\"start\":29680},{\"end\":29898,\"start\":29844},{\"end\":30260,\"start\":30184},{\"end\":30581,\"start\":30544},{\"end\":31207,\"start\":31131},{\"end\":31630,\"start\":31526},{\"end\":31974,\"start\":31936},{\"end\":32308,\"start\":32272},{\"end\":32899,\"start\":32824},{\"end\":33545,\"start\":33528},{\"end\":33909,\"start\":33812},{\"end\":34295,\"start\":34238},{\"end\":34829,\"start\":34769},{\"end\":36017,\"start\":35996},{\"end\":36348,\"start\":36299},{\"end\":36846,\"start\":36811},{\"end\":37151,\"start\":37111}]", "bib_author": "[{\"end\":28356,\"start\":28339},{\"end\":28372,\"start\":28356},{\"end\":28391,\"start\":28372},{\"end\":28708,\"start\":28692},{\"end\":28726,\"start\":28708},{\"end\":28901,\"start\":28882},{\"end\":28910,\"start\":28901},{\"end\":28924,\"start\":28910},{\"end\":28941,\"start\":28924},{\"end\":28953,\"start\":28941},{\"end\":28972,\"start\":28953},{\"end\":28987,\"start\":28972},{\"end\":29006,\"start\":28987},{\"end\":29025,\"start\":29006},{\"end\":29395,\"start\":29383},{\"end\":29410,\"start\":29395},{\"end\":29424,\"start\":29410},{\"end\":29434,\"start\":29424},{\"end\":29707,\"start\":29692},{\"end\":29915,\"start\":29900},{\"end\":29931,\"start\":29915},{\"end\":29947,\"start\":29931},{\"end\":30270,\"start\":30262},{\"end\":30283,\"start\":30270},{\"end\":30298,\"start\":30283},{\"end\":30591,\"start\":30583},{\"end\":30604,\"start\":30591},{\"end\":30619,\"start\":30604},{\"end\":30806,\"start\":30788},{\"end\":30823,\"start\":30806},{\"end\":30993,\"start\":30976},{\"end\":31008,\"start\":30993},{\"end\":31019,\"start\":31008},{\"end\":31229,\"start\":31209},{\"end\":31246,\"start\":31229},{\"end\":31261,\"start\":31246},{\"end\":31274,\"start\":31261},{\"end\":31642,\"start\":31632},{\"end\":31653,\"start\":31642},{\"end\":31670,\"start\":31653},{\"end\":31984,\"start\":31976},{\"end\":31996,\"start\":31984},{\"end\":32013,\"start\":31996},{\"end\":32026,\"start\":32013},{\"end\":32043,\"start\":32026},{\"end\":32320,\"start\":32310},{\"end\":32334,\"start\":32320},{\"end\":32543,\"start\":32527},{\"end\":32559,\"start\":32543},{\"end\":32582,\"start\":32559},{\"end\":32600,\"start\":32582},{\"end\":32613,\"start\":32600},{\"end\":32628,\"start\":32613},{\"end\":32913,\"start\":32901},{\"end\":32930,\"start\":32913},{\"end\":32948,\"start\":32930},{\"end\":32964,\"start\":32948},{\"end\":32979,\"start\":32964},{\"end\":32998,\"start\":32979},{\"end\":33330,\"start\":33318},{\"end\":33341,\"start\":33330},{\"end\":33363,\"start\":33341},{\"end\":33375,\"start\":33363},{\"end\":33565,\"start\":33547},{\"end\":33579,\"start\":33565},{\"end\":33606,\"start\":33579},{\"end\":33922,\"start\":33911},{\"end\":33935,\"start\":33922},{\"end\":33947,\"start\":33935},{\"end\":33958,\"start\":33947},{\"end\":34316,\"start\":34297},{\"end\":34335,\"start\":34316},{\"end\":34349,\"start\":34335},{\"end\":34361,\"start\":34349},{\"end\":34377,\"start\":34361},{\"end\":34844,\"start\":34831},{\"end\":34859,\"start\":34844},{\"end\":34873,\"start\":34859},{\"end\":34885,\"start\":34873},{\"end\":34897,\"start\":34885},{\"end\":35223,\"start\":35212},{\"end\":35236,\"start\":35223},{\"end\":35260,\"start\":35236},{\"end\":35281,\"start\":35260},{\"end\":35634,\"start\":35617},{\"end\":35651,\"start\":35634},{\"end\":35763,\"start\":35752},{\"end\":35775,\"start\":35763},{\"end\":35785,\"start\":35775},{\"end\":35805,\"start\":35785},{\"end\":35821,\"start\":35805},{\"end\":36032,\"start\":36019},{\"end\":36045,\"start\":36032},{\"end\":36064,\"start\":36045},{\"end\":36080,\"start\":36064},{\"end\":36092,\"start\":36080},{\"end\":36368,\"start\":36350},{\"end\":36378,\"start\":36368},{\"end\":36386,\"start\":36378},{\"end\":36403,\"start\":36386},{\"end\":36421,\"start\":36403},{\"end\":36430,\"start\":36421},{\"end\":36445,\"start\":36430},{\"end\":36462,\"start\":36445},{\"end\":36477,\"start\":36462},{\"end\":36496,\"start\":36477},{\"end\":36859,\"start\":36848},{\"end\":36867,\"start\":36859},{\"end\":36879,\"start\":36867},{\"end\":36893,\"start\":36879},{\"end\":36914,\"start\":36893},{\"end\":37164,\"start\":37153},{\"end\":37176,\"start\":37164},{\"end\":37186,\"start\":37176},{\"end\":37206,\"start\":37186},{\"end\":37222,\"start\":37206},{\"end\":37509,\"start\":37488},{\"end\":37691,\"start\":37680},{\"end\":37702,\"start\":37691},{\"end\":37716,\"start\":37702},{\"end\":37728,\"start\":37716},{\"end\":37745,\"start\":37728},{\"end\":37766,\"start\":37745},{\"end\":38176,\"start\":38143},{\"end\":38192,\"start\":38176},{\"end\":38209,\"start\":38192},{\"end\":38237,\"start\":38209},{\"end\":38243,\"start\":38237},{\"end\":38521,\"start\":38503},{\"end\":38539,\"start\":38521},{\"end\":38555,\"start\":38539},{\"end\":38567,\"start\":38555},{\"end\":38583,\"start\":38567},{\"end\":38600,\"start\":38583},{\"end\":38617,\"start\":38600},{\"end\":38631,\"start\":38617},{\"end\":38650,\"start\":38631}]", "bib_venue": "[{\"end\":33661,\"start\":33642},{\"end\":34518,\"start\":34456},{\"end\":28440,\"start\":28391},{\"end\":28690,\"start\":28624},{\"end\":29083,\"start\":29025},{\"end\":29492,\"start\":29434},{\"end\":29755,\"start\":29707},{\"end\":30005,\"start\":29947},{\"end\":30355,\"start\":30298},{\"end\":30657,\"start\":30619},{\"end\":30852,\"start\":30823},{\"end\":30974,\"start\":30930},{\"end\":31312,\"start\":31274},{\"end\":31722,\"start\":31670},{\"end\":32095,\"start\":32043},{\"end\":32383,\"start\":32334},{\"end\":32668,\"start\":32628},{\"end\":33050,\"start\":32998},{\"end\":33316,\"start\":33296},{\"end\":33640,\"start\":33606},{\"end\":34016,\"start\":33958},{\"end\":34454,\"start\":34377},{\"end\":34968,\"start\":34897},{\"end\":35364,\"start\":35297},{\"end\":35615,\"start\":35562},{\"end\":36130,\"start\":36092},{\"end\":36536,\"start\":36496},{\"end\":36952,\"start\":36914},{\"end\":37280,\"start\":37222},{\"end\":37863,\"start\":37766},{\"end\":38141,\"start\":38063},{\"end\":38501,\"start\":38446}]"}}}, "year": 2023, "month": 12, "day": 17}