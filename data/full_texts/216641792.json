{"id": 216641792, "updated": "2023-10-06 16:11:24.365", "metadata": {"title": "Automated Retrieval of ATT&CK Tactics and Techniques for Cyber Threat Reports", "authors": "[{\"first\":\"Valentine\",\"last\":\"Legoy\",\"middle\":[]},{\"first\":\"Marco\",\"last\":\"Caselli\",\"middle\":[]},{\"first\":\"Christin\",\"last\":\"Seifert\",\"middle\":[]},{\"first\":\"Andreas\",\"last\":\"Peter\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 4, "day": 29}, "abstract": "Over the last years, threat intelligence sharing has steadily grown, leading cybersecurity professionals to access increasingly larger amounts of heterogeneous data. Among those, cyber attacks' Tactics, Techniques and Procedures (TTPs) have proven to be particularly valuable to characterize threat actors' behaviors and, thus, improve defensive countermeasures. Unfortunately, this information is often hidden within human-readable textual reports and must be extracted manually. In this paper, we evaluate several classification approaches to automatically retrieve TTPs from unstructured text. To implement these approaches, we take advantage of the MITRE ATT&CK framework, an open knowledge base of adversarial tactics and techniques, to train classifiers and label results. Finally, we present rcATT, a tool built on top of our findings and freely distributed to the security community to support cyber threat report automated analysis.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2004.14322", "mag": "3023308726", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2004-14322", "doi": null}}, "content": {"source": {"pdf_hash": "fa0e8871efa0bc46be86f2ab39d76a17a316ea77", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2004.14322v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "d816656f41be407429769781ba89bdf1325a2c0a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/fa0e8871efa0bc46be86f2ab39d76a17a316ea77.txt", "contents": "\nAutomated Retrieval of ATT&CK Tactics and Techniques for Cyber Threat Reports\n\n\nValentine Legoy \nUniversity of Twente\n7522 NBEnschedeThe Netherlands\n\nMarco Caselli marco.caselli@siemens.com \nSiemens Corporate Technology\n81739MunichGermany\n\nChristin Seifert c.seifert@utwente.nl \nUniversity of Twente\n7522 NBEnschedeThe Netherlands\n\nAndreas Peter a.peter@utwente.nl \nUniversity of Twente\n7522 NBEnschedeThe Netherlands\n\nAutomated Retrieval of ATT&CK Tactics and Techniques for Cyber Threat Reports\nAutomation \u00b7 Cyber Threat Intelligence \u00b7 ATT&CK Tactics and Techniques \u00b7 Multi-Label Classification\nOver the last years, threat intelligence sharing has steadily grown, leading cybersecurity professionals to access increasingly larger amounts of heterogeneous data. Among those, cyber attacks' Tactics, Techniques and Procedures (TTPs) have proven to be particularly valuable to characterize threat actors' behaviors and, thus, improve defensive countermeasures. Unfortunately, this information is often hidden within human-readable textual reports and must be extracted manually. In this paper, we evaluate several classification approaches to automatically retrieve TTPs from unstructured text. To implement these approaches, we take advantage of the MITRE ATT&CK framework, an open knowledge base of adversarial tactics and techniques, to train classifiers and label results. Finally, we present rcATT, a tool built on top of our findings and freely distributed to the security community to support cyber threat report automated analysis.\n\nIntroduction\n\nAccording to Gartner, Cyber Threat Intelligence (CTI) can be generally identified as any piece of information corresponding to \"evidence-based knowledge (...) about an existing or emerging menace\" [9]. Despite lacking a precise definition that comprehensively describes its properties, the security community generally agrees on differentiating among three main types of CTI: \"tactical\", \"operational\" and \"strategical\". Tactical CTI refers to information that is a direct manifestation of adversary actions within a compromised system; examples are IP addresses, file hashes and other artifacts that are traces of actual malicious activities. Operational CTI relates to more general information that has an impact on day-to-day security decision making; for instance statistics of ongoing cyberattack campaigns requiring security teams to deploy specific countermeasures. Finally, strategic CTI refers to knowledge about threat actors' capabilities and motivations, such as characteristics and behavior of advanced persistent threats.\n\nOver the last year, the sharing of these three types of CTI has steadily grown. Threat information sources have increased in number, supported by the development of standardized technologies and formats, like STIX [3]. Available information is either open source and freely accessible over the Internet or selectively provided within private feeds managed by specialized companies, like IBM X Force [13] and OTX AlienVault [1].\n\nIt is worth noting that the amount of shared information is not equally distributed among the three types of CTI. In fact, most sharing activities involve tactical CTI in the form of Indicators of Compromise (IOCs). Malicious IPs and URLs as well as signatures of malware executables cover the vast majority of CTI exchanges while operational and, especially, strategic CTI corresponds to almost negligible percentages despite their importance [7]. Furthermore, operational and strategic CTI does not usually come within structured formats but is embedded within human-readable cyber threat reports (CTRs). If structured data can be generally handled simply by using web scrapers and parsers, unstructured data almost always requires security experts to read and manually extract the most relevant information. Considering the number of CTRs published each day, 3 dealing with unstructured security information currently remains a cumbersome task.\n\nOur work aims to overcome this challenging task by simplifying the extraction of valuable security information contained in CTRs, namely cyber threats' Tactics, Techniques and Procedures (TTPs). To achieve this, we use the ATT&CK (Adversarial Tactics, Techniques, and Common Knowledge) framework [23], an open knowledge base of adversarial tactics and techniques, and we aim at labelling each CTR with the TTPs that most likely match its content. Automating this labeling allows security experts to use the information within any given CTR more efficiently and thus support the definition of prevention, detection and mitigation methods for the related threats. Our contribution specifically focuses on three main aspects:\n\n-We implement and evaluate standard multi-label text classification models and adapt them to retrieve ATT&CK tactics and techniques from CTRs; -We test several post-processing techniques based on the hierarchical structure of the ATT&CK framework to enhance classification results; -We present and distribute rcATT, 4 a tool based on our findings that: 1. predicts ATT&CK tactics and techniques related to a given CTR, 2. allows the user to correct predictions and give feedback on the classification engine to improve results over time, and 3. outputs the results using the Structured Threat Information eXpression (STIX) format.\n\nFinally, we provide a comparison with available open-source solutions showing rcATT's improvements over the state of the art.\n\n\nATT&CK framework\n\nMITRE ATT&CK (Adversarial Tactics, Techniques, and Common Knowledge) framework [23] is a \"globally-accessible knowledge base of adversary tactics and techniques based on real-world observations\". Its comprehensive collection of tactics and techniques has gained popularity over recent years and has been integrated into popular threat information sharing technologies [3]. ATT&CK provides a structured taxonomy to describe several different adversary behaviours. It formally divides into three \"technology domains\":\n\n-\"Enterprise\", which describes behaviours on standard IT systems (e.g., Linux, Windows). -\"Mobile\", which focuses on mobile devices (e.g., Android, iOS).\n\n-\"ICS\", which relates to industrial control and, more in general, to cyberphysical systems\n\nBeyond these domains, ATT&CK also documents behaviours for reconnaissance and weaponisation under the \"PRE-ATT&CK\" designation. Although our work applies likewise to every domain, we will focus this paper on the ATT&CK \"Enterprise\". The \"Enterprise\" ATT&CK framework is usually represented as a matrix of tactics and techniques where:\n\nthe tactics represent possible goals of an attacker (e.g., \"Initial Access\", \"Privilege Escalation\", etc.) the techniques identify \"how\" an attacker might fulfill a specific goal (e.g., \"Access Token Manipulation\", \"Accessibility Features\", etc.)\n\nTactics and techniques are the key focus of our work as they will correspond to the labels of our classification.\n\nEach technique is associated with one or more tactics. Furthermore, the \"Enterprise\" ATT&CK framework collects information about recorded adversary use of every available technique (e.g., involved threat actors, notorious malware, etc.) and possible mitigation approaches. All this information will be used to train our classifiers.\n\n\nData Characterization\n\nThe actual ATT&CK data employed in our work is stored on GitHub 5 and represented using the STIX 2.0 format [3]. Figure 1 shows the structure of the employed dataset and emphasizes the relationships linking all available information to the related tactics and techniques. Every piece of information includes several external references (e.g., usually threat reports or technical descriptions). All text linked by these references was considered part of the dataset and used to train the classifiers. Overall, the \"Enterprise\" ATT&CK dataset is enriched by 1490 different security reports either in HTML (1311) or in PDF (179) format. We handled and parsed documents in PDF format using state-of-the-art tools [4]. To extract content from HTML resources, we focused only on HTML paragraph tags to simplify the process and avoid noise. All available text was eventually checked to remove strings that can hinder the classification (e.g., we removed all stop words provided in the list of the Natural Language Toolkit (NLTK) [17])\n\nIn the end, all ATT&CK tactics linked to at least 80 reports and provide a suitable dataset for the training of the classifiers. Unfortunately, this was not the same for the techniques which presented instead an imbalance (e.g., some techniques corresponding to just one report). We omitted all techniques that had less than 5 reports in the dataset. This ensures that we can at least train on 4 reports and have 1 report for testing in a 5-fold cross-validation setting.\n\n\nMethodology overview\n\nConsidering that CTRs may refer to more than one tactic (or technique) at the same time, the machine learning problem is a multi-class multi-label approach. We tested both general approach to multi-label classification: i) dedicated multilabel classifiers and ii) multiple single-label classifiers, each of which is responsible to decide whether its class (e.g., one specific tactic) should be assigned or not.\n\nDuring the evaluation, we tested several classifiers simultaneously comparing different text representation methods such as term-frequency (TF) and term frequency-inverse document frequency (TF-IDF) weighting factors [19] as well as Word2Vec [22]. Finally, we defined several post-processing approaches to improve classification results. These approaches are based on properties and characteristics of the \"Enterprise\" ATT&CK framework. For example, we leverage the hierarchical structure of the framework by filtering the classification results based on a coherent matching of tactics and techniques belonging to those tactics. In all experiments, we performed 5-fold cross-validation.\n\n\nMetrics\n\nOur evaluation metrics of choice are: precision, recall, and F 0.5 score. The F 0.5 score represents a weighted average between the precision \u03c0 and recall \u03c1 where the precision is considered twice as important as the recall. The F 0.5 is computed as follows: \u03c0 = T P T P + F P , \u03c1 = T P T P + F N , F 0.5 = 1 + 0.5 2 \u00b7 \u03c0 \u00b7 \u03c1 0.5 2 \u03c0 + \u03c1 with TP being true positives, FP false positives and FN false negatives. We chose F 0.5 to emphasise precision over recall, i.e., the assigned labels need to be correct (precision), but there might be labels we do not assign (recall). Finally, we employ macro-averaging and micro-averaging [30].\n\n\nBaseline\n\nTo simplify the analysis of different classification models and quickly identify unsuccessful ones, we defined a \"naive\" baseline by simply attributing to each testing instance the most frequent label of the training set. Table 1 shows the results of this baseline. \n\n\nMulti-label classification\n\nAs introduced in the previous section, our evaluation consists on testing different text representation methods with several multi-label classification models.\n\n\nText representation\n\nOur primary approach is based on standard weighting functions such as termfrequency (TF) and term frequency-inverse document frequency (TF-IDF) [19]. These weighting functions were tested employing either simple bag-of-words or bi-grams and tri-grams. The use of TF and TF-IDF weighting functions relied on the application of a maximum and a minimum frequency of appearance in the overall corpus. According to the performed tests, a simple bags-of-words representation performed better than any other grouping technique. Furthermore, decreasing the number of features and thus selecting half of the words with highest TF/TF-IDF scores outperformed both the choice of a constant number of words for each report or not limiting the number of features at all.\n\nBesides TF and TF-IDF, our analysis includes tests based on Word2Vec [22]. In this case, Word2Vec was trained on our dataset instead of taking advantage of the existing pre-trained versions. This was due to some inconsistencies found in some early results. Tests with Word2Vec were performed both averaging and summing word vectors representing the text.\n\n\nClassification models\n\nOur analysis includes two main strategies to tackle the multi-label classification problem: adapting state-of-the-art classification algorithms or approaching the overall problem from a different perspective. In this last case, the approaches are: using binary relevance and training a binary classifier for each label independently [18], employing a classifier chain (which is similar to binary relevance but uses the relation between labels [26]), or adopting label power sets (basically transforming them into a multi-class problems between all labels combination [29]). A power-set model is, however, difficult to apply to our case, as we have too many labels and not enough data to cover all possible combinations. Therefore, we only focus on binary relevance and classifier chains and test different types of classifiers.\n\nAll implementations of multi-label classifiers are built on top of the Scikitlearn library (version 0.21.3) [27]. Our tests include: multi-label K-Nearest Neighbours, multi-label Decision Tree and Extra Tree techniques, and Extra Trees, Random Forest ensemble methods for multi-label classification. For what concerns binary relevance and classifier chains, we tested several linear models. These are: Logistic Regression, Perceptron, Ridge, Bernoulli Naive Bayes classifier, K-Nearest Neighbors and Linear Support Vector Machine (Linear SVC). We also included multiple tree-based models such as: Decision Tree, Extra Tree classifiers, Random Forest, Extra Trees, AdaBoost Decision Tree, Bagging Decision Tree and Gradient Boosting.\n\nIt is to be noted that in classification models were the correlations between labels is used (i.e. multi-label classifiers and classifier chains), we predicted the tactics and techniques together, with the best parameters possible for both label types. In the cases in which the label relationship did not matter (i.e. binary relevance), we split the classification by tactics and techniques, applying to each the best parameters possible for their category.\n\nTo avoid overfitting, in addition to the previously mentioned reduction of the number of features, we tested regularising models, fine-tuning the hyperparameters of the classifiers and aim at simple models.\n\nAs an attempt to solve the imbalance in the dataset, we used a random resampling approach on the models with binary relevance, associated with TF and TF-IDF. In the case of the tactics, the resampling method was a combination of over-and undersampling for all tactics to have in their training set with 400 positive reports and 400 negative reports. For the techniques, we randomly sampled 125 positive reports and 500 negative reports. Tables 2 and 3 show the performance of the tactics and techniques classification, respectively. Classifiers that are not shown in the table either did not outperform our naive baseline or obtained overall lower results than those present.  When we compare the text representation methods, we can observe that, overall, models using Word2Vec either with a sum or an averaging of the word vectors, underperformed compared to the ones using a TF or TF-IDF weighting system. We observed minor differences in the use of the average against the sum of the vector in the Word2Vec approach, but we cannot claim that one way is better than the other as the results depend on the employed classifier.\n\n\nResults and discussion\n\nConsidering the results of the different classifiers, we can see that adapted algorithms tend to underperform compared to the classifier chains and the binary relevance models, for both tactics and techniques predictions. Overall, the classification made with binary relevance instead of classifier chain performs slightly better, which means that the relationship between labels did not have as much impact as expected.\n\nSome classifiers stand out among the others for each type of labels. For the classification by tactics, the AdaBoost Decision Tree, the Gradient Tree Boosting, Perceptron and the Linear SVC in classifier chains or binary relevance models have the best performance, independently from using TF-IDF or TF. The Bagging Decision tree, the Ridge classifier and the Logistic Regression perform well when used in binary relevance. They also perform well in a classifier chain if Logistic Regression and the Bagging Decision Tree classify text use TF weighting and the Ridge classifier use TF-IDF. Similar models work as well for techniques prediction, in addition to the Decision Tree classifier with binary relevance or classifier chain models.\n\nThe resampling on the tactics prediction resulted in an overall increase of the recall, but also a decrease in precision and, thus, in F 0.5 score. Only a few models (e.g. Extra Trees and Random Forest) improve because of the resampling. The Logistic Regression model does also increase its F 0.5 macro-averaged score when paired with TF-IDF, which is the maximum obtained in the resampling evaluation. On the techniques prediction, similarly, the recall increases, while the precision and the F 0.5 score decrease. The only improvements in performance are for models which performed very poorly without resampling.\n\nEven though all metrics are essential, we want to give equal importance to each label. Thus looking at the macro-averaged F 0.5 scores, the binary relevance Linear SVC with a TF-IDF weighted text representation stands out compared to the other models, for both tactics and techniques predictions.\n\n\nPost-processing of classification results\n\nFrom the analysis presented in the previous section, we select the binary relevance Linear SVC model with TF-IDF weighted bag-of-words. To improve its results, we now try to take advantage of different properties of the \"Enterprise\" ATT&CK framework such as the relationships between tactics and techniques. It is worth noting that, an effect of this property might have already been tested in classification models involving classifier chains. However, this issue does not concern the selected SVC model that can, therefore, benefit from this additional information.\n\n\nUndertaken approaches\n\nRelationships between tactics and techniques Given the relationship between each technique and the tactic it belongs to, we use this property of the framework in the following ways:\n\nDirect mapping from techniques to tactics This approach defines the tactics of a report based on the classification over the techniques. Because each technique belongs to one or more tactics, we can consider simple post-processing rules adding tactics to labels. For example, if a technique T e 1 is a label of a given report and T e 1 belongs to tactic T a 1 , then T a 1 is also a label of the report.\n\nTactics as features Since the prediction of techniques underperformed the one on tactics, we can use the results from the classification by tactics to improve the one by techniques. In this case, we use the results of the tactics prediction as features for the classification by techniques. This is close to the classifier chain method, but it solely uses the tactics prediction to influence the techniques one.\n\nConfidence propagation Here, we want to use the method described in Ayoade et al. [2] (as well as Wu et al.'s work [32]). The idea behind this approach is to use the confidence score of each technique and the confidence score of the associated tactics to create boosting factors. Each boosting factors are multiplied to each associated tactic confidence score, and this ensemble is added to the pre-existing technique confidence score. Based on the associated tactics confidence score, the techniques' confidence score increases or decreases, thus impacting the final predictions. To apply this method, it is worth noting that, Scikit-learn's Linear SVC required us to use the decision function scores instead of the confidence scores of the classifier as directly normalising the confidence scores (not automatically performed in the library) would have worsened model's performance.\n\nInput: R /* report */ T a R i , ...T e R k /* all tactics and techniques predicted as being present in the R */ T ex /* one of the techniques */ T ay /* one of the tactics */ p(T ex \u2208 R) /* the probability of T ex being predicted for R */ p(T ay \u2208 R) /* the probability of T ay being predicted for R */ Output: T a R i , ...T e R k /* updated ensemble of tactics and techniques being present in the R */ Data: th \u2208 R /* classification threshold */ a, b, c, d \u2208 R /* defined thresholds */ begin if T ex \u2192 T ay then if p(T ex \u2208 R) > a > th and b < p(T ay \u2208 R) < th then T a R i , ...T e R k + = T ay /* adding T ay to the ensemble of tactics and techniques being present in the R */\n\nif th < p(T ex \u2208 R) < c and p(T ay \u2208 R) < d < th then T a R i , ...T e R k \u2212 = T ex /* removing T ex to the ensemble of tactics and techniques being present in the R */\n\n\nAlgorithm 1. Hanging node approach\n\nHanging node This approach is based on the observation that for 30% of the techniques predicted in a report, not all related tactics were predicted, meaning that either the techniques or the tactics were incorrectly identified. The analysis of the distribution of the confidences' frequency shows that false predictions tend to be closer to the \"successful\" classification threshold (especially when it comes to tactics). The Hanging node approach tries to leverage this aspect by considering all confidence scores when adding tactics or removing techniques from the labels. As presented in Algorithm 1, for a connected pair \"tactic-technique\", if the technique is predicted with a high confidence score, while the tactic is not predicted, but with a confidence score close to the classification threshold th, then we can add the tactic to the tactics and techniques labeling the report. On the contrary, if the techniques are present, with a confidence score near to the classification threshold th and the tactic is not predicted for the report, with a low confidence score, then we can remove the techniques from the predicted labels. The thresholds a, b, c, d were defined after testing different values and comparing the improvement in classification performance. According to our tests, a classification threshold of th = 0.5 and thresholds a = 0.55, b = 0.05, c = 0.95, d = 0.30, allow the highest macro-averaged F 0.5 score. As we use the Linear SVC from Scikit-learn, we defined the confidence score by scaling the decision function scores using min-max scaling with min = \u2212 1 and max = 1.\n\nFurther similar strategies based on relationships between techniques and tactics are difficult to implement, as several techniques correspond to the same tactics. Adding a technique if a related tactic is also present in the report would be misguided, as the high probability could be due to another technique. Similarly, removing a tactic, if a related technique is absent, would be disadvantageous for basically the same reason.\n\nRelationships between techniques Always based on the ATT&CK framework structure, we can compute joint probabilities between a couple of techniques based on their common appearances within the same malware, tool or group. Using these probabilities, we decided to test three different methods to improve techniques predictions.\n\nRare association rules This approach follows the work of Benites and Sapozhnikova [5], based on a selection of association rules between techniques. The first step is to calculate the Kulczynski measure [15] for each pair of techniques. These values are forming a curve from which we can determine the variance. If this variance is low, the threshold to decide on the pairing rules based on their Kulczynski measure is based on the median of differences between neighbour values. If this variance is high, it is based on the average of the values slightly lower than the mean of this curve.\n\nSteiner tree association rules This approach has been described by Soni et al. [28], and focuses on a formulation of the label coherence as a Steiner Tree Approximation problem [10]. Once the techniques prediction is performed, for each report, we create a directed tree [21] in which edges have weights corresponding to the conditional probabilities between the two nodes and the direction is given by the following criterion:\nT e i \u2192 T e j , p(T e i |T e j ) \u2264 p(T e j |T e i ), T e i \u2190 T e j ,\notherwise.\n\nThen, we use Edmond's algorithm [8] to obtain a reduced tree and a limited number of connections between techniques. Based on the predictions of the classification, we search the graph for techniques which descend from the predicted techniques with the K highest weights. In our tests, we found that K = 15 is the value maximizing the success of this pre-processing strategy.\n\nKnapsack This approach also comes from the paper of Soni et al. [28] but, this time, the authors consider the label assignment as a resource allocation problem. In this case, we solve the 0-1 Knapsack problem [20], in which a new label is selected if its conditional probability (based on the predicted labels) increases the overall log-likelihood. Table 4 shows the final results of all our post-processing approaches. To better investigate the effects of post-processing approaches we also applied those assuming a perfect match of techniques (when predicting the tactics) and vice-versa (Table 5). For all results, we use a new baseline given by the pure classification without any post-processing. When it comes to tactics prediction, the independent classification is the best. This might be related to the fact that techniques predictions had worse results than tactics ones and thus any post-processing depending on these last could decrease results on the tactics. However, considering a perfect techniques prediction, both post-processing approaches would help improve the tactics classification. The approach \"Direct mapping from techniques to tactics\" would have a perfect performance, as it is purely rule-based. However, it is unlikely that the techniques prediction would improve without the tactic prediction improving. Because of that, approach \"Hanging node\" seems to be the most promising approach to improve the independent classification. For what concerns techniques prediction, the independent classification started with a lower baseline. The use of the approaches \"Confidence propagation\" and \"Hanging node\" are likely possible improvements. In all cases, except for the approach \"Hanging node\", the F 0.5 score changed mainly due to lower precision and increased recall related to the addition of techniques to the predicted set. In this case, the F 0.5 score increasing in \"Hanging node\" is especially interesting, as it is due to the decrease of false-positive and, thus, the increase of the precision.\n\n\nResults and discussion\n\nApproaches relying on the relationship between techniques are close to the classification without post-processing. From these results, we conclude that these approaches might not well adapt to our problem or to the data we use. Approach \"Rare association rules\" could probably fit better in a hierarchical environment with known conditional probabilities. It is also possible that the ground truth we used is incomplete, as, even though it is based on data collected and analysed by experts, it represents only a sample of malware, tools and campaigns being observed in the past years.\n\nIf tactics prediction were perfect, and without any change in the prediction of techniques, approach \"Tactics as features\" would have the highest score for all metrics. Once again, we cannot choose an approach based on this test and the results from Table 5, since the techniques prediction would probably improve with the perfect tactics prediction. However, since all approaches are better or equal than the independent classification, we conclude that post-processing is needed for techniques prediction. It is worth noting that, approach \"Hanging node\" is barely higher than the independent classification compared to \"Confidence propagation\", while the opposite is true in the actual tests. Approach \"Hanging node\" also presents worse results when the tactics prediction is per-fect. For this reason, we conclude that approach \"Confidence propagation\" might likely overperform \"Hanging node\". Furthermore, \"Hanging node\" and \"Confidence propagation\" approaches carry on very different strategies and, from our testing, they seem targeting different types of techniques. \"Hanging node\" seems to affect very little, but efficiently, the results from the independent classification and only modifies results of the techniques with the highest number of reports. \"Confidence propagation\" targets any techniques and modify many predictions, which would potentially improve the classification for the hard-to-predict techniques. However, from our observation, this approach succeeds almost as often as it fails, thus worsening the prediction of some labels while improving the prediction of others.\n\nAt the end of the evaluation, we are not in a position to draw conclusive proves for precisely ranking the described post-processing methods. Based on the analysis of the results, not using any post-processing method for the tactics classification is probably a safe choice. For what concerns the techniques, we believe that one among \"Confidence propagation\" and \"Hanging node\" would likely improve the independent classification. In what follows, within our tool, we decided to implement both approaches but to use \"Hanging node\" as default due to its better results in our current dataset. However, in case of retraining on a new dataset, the tool will be compare results coming from the two approaches and dynamically choose the one that best perform.\n\n\nrcATT\n\nOne important contribution of our research was to implement our findings in a tool that would allow security experts to automate the analysis of CTRs. We call our tool rcATT, as for \"reports classification by adversarial tactics and techniques\". Based on the previous two sections, we decided to implement a classification that uses a TF-IDF weighted bag-of-words text representation with a binary relevance Linear SVC. This classification is followed with a post-processing performed either via the hanging node approach or the confidence propagation one (depending on the training data). Figure 2 illustrate tools' main functional building blocks. From a cyber threat report, in a text format, entered by the users, our trained model predicts different tactics and techniques. The user will be able to visualize the results and the confidence score associated with each prediction, either based on the Min-Max scaling of the decision function score from the classifier or the re-evaluation score, based on the post-processing method. When users do not agree with the results, they can change them. In all cases, new results can be saved with the old training data (the original labelled cyber threat reports) so to be used to train the classifier again. The retraining of the classifier must be activated manually by the user. This is done to avoid automated retraining that might slow down the tool or mistakenly involve unwanted data. As already mentioned, during the retraining, the tool also autonomously choose which post-processing method to use to maximize predictions results. This approach involves also the dynamic  definition of specific thresholds (e.g., the one related to the hanging node methods). Classification results can be saved in a JSON file using STIX format. This later allows any other tool to easily access all identified tactics and techniques. General information about the predictions is exported in a STIX \"Report\" object (e.g., title of the original report, dates, etc.). The \"description\" property of the report object represents the original content of the CTR. Most important, all prediction results (as well as any prediction added by the users) are available using the specific identification numbers available within the STIX version of the ATT&CK framework and linked to the Report object using the \"object refs\" property.\n\nFinally, the tool works with two graphic modes: command-line and graphical interface (Figure 3). The command-line version of the tool is meant to be efficient to use (e.g., cases in which the user want to predict TTPs from a large number of CTRs). The graphical interface is interactive and allows users to modify classification results and save the changes to the training set and thus give feedback to the tool. \n\n\nRelated work and comparison with similar approaches\n\nThe vast majority of existing scientific work on the automated analysis of (unstructured) CTRs concerns the extraction of IOCs [16,34]. Further related work looks at other unstructured sources, like hacker forums, to extract insights on adversarial tools [6], or on more general security vulnerabilities [24] and related concepts [14,25].\n\nOnly few research papers aim to retrieve TTPs in unstructured data, as we do. One of them was written by Zhu et al. [33] and presents FeatureSmith, a tool that mines the security literature to learn features for training machine learning classifiers to detect Android malware. FeatureSmith is specifically tailored to the task of malware detection and cannot be easily generalized to other other settings. In contrast, our focus lies on the automated extraction of general ATT&CK tactics and techniques.\n\nTTPDrill [11] and ActionMiner [12], developed by Husari et al., but also the tool by Ayoade et al. [2] are the closest tools to ours, as they extract threat actions from CTRs in order to link them to specific ATT&CK tactics and techniques. The biggest problem with those works is, however, that their achieved results cannot be reproduced by the scientific community at large as their used datasets or their tools are not openly available. In fact, [2] reports a big difference in accuracy when running TTPDrill [11] on their own (closed) datasets in comparison to the accuracy achieved on the (closed) dataset as reported in [11]. Due to the closed source nature of these works, these inconsistencies cannot be explained and motivate our open study on the extraction of TTPs from CTRs. In comparison to all related work, our approach is described for easy reproducibility by only using open data and by making our final tool publicly available as open source.\n\nOn the conceptual level, however, there are also some key differences between the above-stated approaches and ours. First of all, TTPDrill [11] uses a different classification approach without a clear separation among tactics and techniques (treated as similar labels). In this regard, Ayoade et al. [2] describe a more similar approach, clearly separating tactics and techniques, despite not implementing a fully-contained tool in the end. Both tools employ different classifier and/or pre-processing approaches with respect to the ones implemented within rcATT. Finally, none of the two works allow users to feedback results back to the tool making any improvement impossible. Again, due to the lack of open-source code or used datasets (as well as any detailed description on how to re-implement their solutions), any further comparison on classification results is not possible.\n\nA different tool, called Unfetter Insight, was instead available on Github [31]. Despite lacking a comprehensive description (e.g., publication) that allows a qualitative analysis with respect to rcATT, we could in this case carry out a quantitative comparisong between the classification results. Unfetter Insight has been developed to detect possible ATT&CK tactics and techniques within reports in TXT, PDF or HTML format. Based on our understanding of the code, the detection is implemented by the Babelfish software created by W. Kinsman in 2017. This software learns topics from a wiki-like corpus and retrieve the presence of those topics in a document. The tool creates text convolutions from unknown documents, identifying those having a good overlap with the ones presented in the corpus. Based on the term frequency of the vocabulary in the selected convolutions, tactics and techniques are predicted using a binary relevance Multinomial Naive Bayes classifier. It is finally worth noting that Unfetter Insight assumptions for the models are quite different with respect to the ones used in rcATT. In fact, we base our classification on a training set of labelled reports while they use a dictionary-like learning set. For this reason, we could not add our training set without compromising the concept behind Babelfish. Therefore, we decided to proceed with a comparison by using their training set within both solutions 6 . Table 6 presents the classification results of rcATT and Unfetter Insight on identical training set and dataset. The table shows that Unfetter Insight underperforms in the majority of the proposed metrics and overperforms only in the one we considered the least important in our first assumptions. \n\n\nConclusion\n\nWith its constant growth, CTI sharing has become an essential activity of the security lifecycle. Despite this, security teams around the world still face the challenge of handling huge amount of threat intelligence data. Above all, dealing with unstructured data remain a cumbersome and time-consuming task. The work presented in this paper aims at simplifying this task by enabling automated extraction of valuable CTI information, namely Tactics, Techniques and Procedures (TTPs), from textual cyber security reports. Our approach takes a cue from the ones of Ayoade et al. [2] and Husari et al. [11] and investigates text multi-label classification techniques that can fulfill the aforementioned goal. Additionally, we propose approaches for post-processing classification results and improving classifiers' overall performance in terms of precision and recall. By taking advantage of our training dataset, the MITRE ATT&CK framework, we show that relationships among the chosen classification labels, the framework's tactics and techniques, can help refining the classification and decreasing the number false-positives.\n\nBased on our findings, we developed rcATT, an interactive tool for the automated analysis of cyber threat reports. The tool aims at supporting security experts to dig into human-readable data and extract ATT&CK tactics and techniques that most likely describe the content of the text. rcATT gives users the possibility to dinamically adjust or fix the classification results and, consequently, feedback the internal classifier with correct information, improving its performance over time.\n\nThe tool has been tested and compared with state-of-the-art approaches discussed in literature or already available as open-source solutions. Results of these comparisons show rcATT competitiveness and validate the research directions foreseen within our work. Finally, rcATT has been recently integrated into tool-chains of real operational corporate Computer Emergency Response Teams (CERTs) to support Threat Intelligence sharing and analytics processes. This last experience further confirms the importance of researches of this kind and their likely impact in broader contexts such as automated incident handling and response.\n\nFig. 1 :\n1\"Enterprise\" ATT&CK data schema and corresponding STIX 2.0 objects\n\nFig. 2 :\n2Organisation of rcATT.\n\nFig. 3 :\n3rcATT graphical interface.\n\nTable 1 :\n1Naive baseline based on the attribution of the most frequent labelMajority \nMicro \nMacro \nPrecision Recall F 0.5 Precision Recall F 0.5 \nTactics \n48.72% 19.00% 37.10% 4.43% 9.09% 4.93% \nTechniques 9.57% 2.51% 6.11% 0.05% 0.48% 0.06% \n\n\n\nTable 2 :\n2Classification results for tactics prediction. Abbreviations: CC = Classifier Chain, BR = Binary relevance, DT = Decision Tree, T = Tree? KNN = K Nearest Neighbors Gradient T Boosting 71.42% 48.19% 65.04% 66.35% 40.16% 56.78% 54.87% 66.41% 51.94% 57.66% 72.47% 53.92% Gradient T Boosting 70.13% 46.85% 63.66% 65.08% 38.94% 55.03% 54.30% 63.40% 50.79% 56.92% 70.53% 52.48% BR Logistic Regression 71.04% 50.70% 65.61% 59.00% 40.53% 51.21% 59.25% 63.13% 56.69% 61.09% 69.80% 57.14% BR Perceptron 65.20% 55.35% 62.80% 60.54% 48.29% 56.24% 57.26% 62.68% 53.97% 59.28% 69.11% 54.98% BR Ridge Classifier 72.40% 48.90% 65.83% 66.57% 38.58% 53.32% 59.47% 62.28% 55.77% 61.13% 68.89% 56.59% BR Linear SVC 65.64% 64.69% 65.38% 60.26% 58.50% 59.47% 57.71% 66.17% 53.88% 59.97% 71.18% 55.75% CC Gradient T Boosting 63.33% 42.22% 57.58% 55.10% 33.38% 47.52%Without resampling \nWith resampling \nMicro \nMacro \nMicro \nMacro \nPrecision Recall \nF0.5 Precision Recall \nF0.5 \nPrecision Recall \nF0.5 Precision Recall \nF0.5 \nMajority Baseline \n48.72% 19.00% 37.10% 4.43% \n9.09% 4.93% \n-\n-\n-\n-\n-\n-\nTerm Frequency \nCC Adaboost DT \n64.73% 50.84% 61.30% 60.87% 45.20% 56.45% \n-\n-\n-\n-\n-\n-\nCC Bagging DT \n67.19% 40.57% 59.38% 64.01% 33.46% 51.89% \n-\n-\n-\n-\n-\n-\nCC Gradient T Boosting 71.84% 44.33% 63.61% 66.72% 37.42% 55.91% \n-\n-\n-\n-\n-\n-\nCC Logistic Regression \n63.81% 54.35% 61.61% 58.86% 47.78% 55.85% \n-\n-\n-\n-\n-\n-\nCC Perceptron \n56.34% 56.58% 56.35% 51.63% 50.79% 51.04% \n-\n-\n-\n-\n-\n-\nCC Linear SVC \n63.81% 51.02% 60.71% 58.89% 44.31% 54.64% \n-\n-\n-\n-\n-\n-\nBR AdaBoost DT \n62.30% 49.27% 59.08% 57.26% 42.37% 52.91% 49.77% 60.45% 47.08% 52.24% 65.22% 48.91% \nBR Bagging DT \n68.26% 42.36% 60.75% 63.71% 34.30% 51.54% 52.39% 58.59% 49.36% 54.32% 63.73% 50.48% \nBR BR Logistic Regression \n63.71% 54.42% 61.54% 58.74% 47.81% 55.76% 56.94% 61.56% 52.44% 58.66% 66.74% 53.90% \nBR Ridge Classifier \n61.85% 51.46% 59.39% 55.64% 45.31% 52.92% 49.93% 55.86% 45.48% 51.50% 58.97% 47.03% \nBR Linear SVC \n64.70% 51.55% 61.51% 59.20% 44.36% 54.89% 54.95% 57.69% 50.76% 56.45% 63.37% 51.83% \nAdapted KNN \n57.93% 38.66% 52.55% 52.55% 30.41% 43.06% \n-\n-\n-\n-\n-\n-\nTerm Frequency-Inverse Document Frequency \nCC Adaboost DT \n61.42% 49.86% 58.59% 57.71% 44.09% 53.79% \n-\n-\n-\n-\n-\n-\nCC Gradient T Boosting 71.15% 43.15% 62.52% 67.40% 36.29% 54.97% \n-\n-\n-\n-\n-\n-\nCC Perceptron \n62.06% 54.97% 60.28% 58.05% 49.26% 54.72% \n-\n-\n-\n-\n-\n-\nCC Ridge Classifier \n74.40% 41.27% 63.27% 67.63% 33.31% 51.74% \n-\n-\n-\n-\n-\n-\nCC Linear SVC \n71.63% 44.89% 63.41% 65.70% 36.76% 54.59% \n-\n-\n-\n-\n-\n-\nBR AdaBoost DT \n61.02% 51.02% 58.61% 56.61% 44.67% 53.19% 49.52% 58.46% 46.12% 51.84% 63.79% 47.86% \nBR Bagging DT \n66.88% 41.44% 59.39% 63.92% 34.95% 52.29% 53.15% 56.68% 50.06% 54.70% 61.94% 50.87% \nBR Adapted KNN \n62.45% 45.24% 57.89% 57.82% 36.75% 49.63% \n-\n-\n-\n-\n-\n-\nWord2Vec average \nCC Adaboost DT \n58.59% 44.21% 54.98% 52.69% 36.95% 47.34% \n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\nCC Logistic Regression \n62.80% 34.15% 53.78% 55.27% 26.87% 42.63% \n-\n-\n-\n-\n-\n-\nCC KNN \n53.40% 51.78% 52.93% 48.15% 43.84% 45.79% \n-\n-\n-\n-\n-\n-\nCC Linear SVC \n62.88% 40.82% 56.75% 59.98% 34.04% 49.66% \n-\n-\n-\n-\n-\n-\nBR AdaBoost DT \n57.97% 46.24% 55.11% 50.77% 38.54% 46.70% \n-\n-\n-\n-\n-\n-\nBR Gradient T Boosting 64.53% 44.77% 59.23% 55.65% 35.07% 46.78% \n-\n-\n-\n-\n-\n-\nBR Logistic Regression \n66.86% 41.94% 59.68% 61.02% 31.95% 46.17% \n-\n-\n-\n-\n-\n-\nBR KNN \n58.08% 51.48% 56.58% 51.80% 41.86% 47.45% \n-\n-\n-\n-\n-\n-\nBR Linear SVC \n64.85% 44.91% 59.49% 56.57% 35.35% 47.81% \n-\n-\n-\n-\n-\n-\nAdapted KNN \n57.33% 51.03% 55.92% 52.11% 41.78% 47.43% \n-\n-\n-\n-\n-\n-\nWord2Vec sum \nCC Adaboost DT \n60.26% 43.49% 55.82% 54.81% 36.87% 48.68% \n-\n-\n-\n-\n-\n-\nCC Gradient T Boosting 60.21% 40.00% 54.69% 50.19% 30.78% 43.26% \n-\n-\n-\n-\n-\n-\nCC Logistic Regression \n54.83% 50.41% 53.89% 49.18% 42.97% 47.26% \n-\n-\n-\n-\n-\n-\nCC KNN \n57.61% 48.22% 55.39% 54.82% 39.33% 47.90% \n-\n-\n-\n-\n-\n-\nBR AdaBoost DT \n59.75% 45.73% 56.15% 52.96% 38.41% 48.22% \n-\n-\n-\n-\n-\n-\nBR Gradient T Boosting 64.54% 45.72% 59.53% 55.37% 35.96% 47.76% \n-\n-\n-\n-\n-\n-\nBR Logistic Regression \n58.71% 48.01% 56.13% 52.07% 42.03% 49.31% \n-\n-\n-\n-\n-\n-\nBR KNN \n57.70% 48.23% 55.45% 54.14% 39.26% 47.62% \n-\n-\n-\n-\n-\n-\nAdapted KNN \n57.58% 48.08% 55.33% 53.64% 38.95% 47.25% \n-\n-\n-\n-\n-\n-\n\n\nTable 3 :\n3Classification results for techniques prediction. Abbreviations: CC = Classifier Chain, BR = Binary relevance, DT = Decision Tree, T = Tree, NB = Naive Bayes 53% 11.88% 26.99% 12.79% 16.81% 12.34% 15.43% 10.75% BR Gradient T Boosting 27.60% 12.67% 22.31% 20.25% 12.72% 16.26% 18.47% 20.32% 16.73% 14.80% 24.51% 13.96% BR Ridge Classifier 14.96% 25.96% 16.33% 10.98% 23.50% 11.74% 11.40% 31.83% 12.83% 9.05% 32.50% 10.04% BR Linear SVC 22.87% 19.22% 21.98% 15.36% 11.39% 13.41% 17.85% 24.59% 17.97% 12.85% 17.01% 11.95% BR Decision Tree 23.06% 18.53% 21.94% 18.00% 14.76% 16.18% 17.46% 24.90% 17.11% 14.70% 21.66% 13.61% 04% 14.77% 27.41% 17.23% 9.05% 13.36% 27.53% 18.12% 22.32% 14.55% 13.00% 11.85% BR Gradient T Boosting 25.85% 11.60% 20.72% 18.83% 12.09% 15.21% 19.45% 16.76% 17.10% 15.33% 21.69% 14.38%Without resampling \nWith resampling \nMicro \nMacro \nMicro \nMacro \nPrecision Recall \nF0.5 Precision Recall \nF0.5 \nPrecision Recall \nF0.5 Precision Recall \nF0.5 \nMajority Baseline \n9.57% \n2.51% 6.11% \n0.05% \n0.48% 0.06% \n-\n-\n-\n-\n-\n-\nTerm Frequency \nCC Adaboost DT \n36.64% 13.27% 27.05% 18.38% 8.98% 13.72% \n-\n-\n-\n-\n-\n-\nCC Bagging DT \n39.95% 5.83% 18.24% 18.50% 7.90% 12.32% \n-\n-\n-\n-\n-\n-\nCC Ridge Classifier \n14.82% 25.59% 16.16% 10.88% 23.40% 11.66% \n-\n-\n-\n-\n-\n-\nCC Decision Tree \n22.44% 17.94% 21.31% 18.64% 14.98% 16.46% \n-\n-\n-\n-\n-\n-\nBR AdaBoost DT \n35.60% 16.04% 28.56% 18.23% 9.74% 14.23% 26.44% 23.02% 22.79% 14.65% 16.16% 12.45% \nBR Bagging DT \n39.30% 7.63% 21.42% 16.92% 7.Adapted Decision Tree \n16.23% 14.06% 15.71% 9.99% \n8.27% 9.00% \n-\n-\n-\n-\n-\n-\nAdapted Extra Tree \n12.86% 10.10% 12.19% 6.33% \n4.95% 5.55% \n-\n-\n-\n-\n-\n-\nTerm Frequency-Inverse Document Frequency \nCC Adaboost DT \n37.06% 13.06% 26.98% 17.70% 8.44% 13.19% \n-\n-\n-\n-\n-\n-\nCC Decision Tree \n23.18% 18.47% 22.02% 18.83% 14.85% 16.77% \n-\n-\n-\n-\n-\n-\nBR AdaBoost DT \n35.Logistic Regression \n52.82% 3.66% 14.33% 7.86% \n2.76% 5.18% 42.05% 4.98% 12.41% 7.53% \n4.79% 5.64% \nBR Perceptron \n30.45% 18.26% 26.82% 21.89% 14.61% 18.32% 25.16% 22.56% 23.03% 19.50% 21.29% 17.32% \nBR Linear SVC \n37.18% 29.79% 35.02% 28.84% 22.67% 25.06% 31.16% 32.86% 29.37% 26.27% 28.05% 22.87% \nBR Decision Tree \n20.72% 18.31% 20.15% 16.88% 14.57% 15.55% 17.44% 22.91% 17.02% 15.45% 20.21% 14.21% \nAdapted Decision Tree \n14.11% 12.19% 13.65% 8.14% \n7.19% 7.44% \n-\n-\n-\n-\n-\n-\nAdapted Extra Tree \n12.54% 9.83% 11.85% 5.70% \n4.68% 5.15% \n-\n-\n-\n-\n-\n-\nWord2Vec average \nCC Adaboost DT \n29.88% 9.29% 20.67% 9.35% \n4.05% 6.70% \n-\n-\n-\n-\n-\n-\nCC Linear SVC \n36.21% 3.33% 11.70% 7.58% \n3.60% 5.35% \n-\n-\n-\n-\n-\n-\nBR AdaBoost DT \n26.88% 11.79% 21.36% 9.15% \n4.82% 7.31% \n-\n-\n-\n-\n-\n-\nBR Perceptron \n17.43% 15.88% 17.03% 8.33% \n7.80% 6.37% \n-\n-\n-\n-\n-\n-\nBR Bernoulli NB \n10.54% 47.77% 12.47% 5.21% 22.33% 6.06% \n-\n-\n-\n-\n-\n-\nBR Linear SVC \n49.01% 6.33% 20.70% 8.57% \n3.27% 5.64% \n-\n-\n-\n-\n-\n-\nAdapted Extra Tree \n12.53% 14.41% 12.85% 5.17% \n5.75% 5.05% \n-\n-\n-\n-\n-\n-\nWord2Vec sum \nCC Adaboost DT \n27.88% 7.34% 17.68% 8.77% \n3.47% 6.14% \n-\n-\n-\n-\n-\n-\nBR AdaBoost DT \n28.34% 11.69% 21.93% 9.63% \n4.50% 7.30% \n-\n-\n-\n-\n-\n-\nBR Gradient T Boosting 21.14% 8.41% 16.14% 7.38% \n2.96% 5.00% \n-\n-\n-\n-\n-\n-\nBR Logistic Regression \n20.49% 18.60% 19.98% 11.48% 10.53% 10.62% \n-\n-\n-\n-\n-\n-\nBR Perceptron \n16.39% 18.52% 16.77% 6.27% \n7.96% 5.65% \n-\n-\n-\n-\n-\n-\nBR Bernoulli NB \n9.98% 49.20% 11.86% 5.10% 23.51% 5.93% \n-\n-\n-\n-\n-\n-\nBR Linear SVC \n11.64% 22.82% 12.87% 8.30% 15.49% 8.42% \n-\n-\n-\n-\n-\n-\nAdapted Decision Tree 513.39% 14.23% 13.51% 5.63% \n5.73% 5.42% \n-\n-\n-\n-\n-\n-\n\n\n\nTable 4 :\n4Comparison of post-processing approaches for tactics and techniques.Approaches \nMicro \nMacro \nPrecision \nRecall \nF0.5 \nPrecision \nRecall \nF0.5 \nTactics \nInde. \n65.64% \u00b13.76% 64.69% \u00b13% 65.38% \u00b12.87% 60.26% \u00b13.2% 58.50% \u00b13.68% 59.47% \u00b12.29% \n1.a. \n63.32% \u00b16.42% 52.34% \u00b15.08% 60.45% \u00b13.8% 57.49% \u00b15.41% 47.94% \u00b15.87% 54.59% \u00b13% \n1.d. \n59.60% \u00b12.89% 68.04% \u00b13.06% 61.08% \u00b13.42% 54.41% \u00b12.97% 61.28% \u00b13.12% 55.42% \u00b13.2% \nTechniques \nInde. \n37.18% \u00b16.75% 29.79% \u00b15.91% 35.02% \u00b15.32% 28.84% \u00b16.9% \n22.67% \u00b15.94% 25.06% \u00b16.09% \n1.b. \n31.55% \u00b15% \n35.17% \u00b16% \n31.97% \u00b14.31% 24.70% \u00b16.29% 22.74% \u00b16.3% \n22.38% \u00b15.72% \n1.c. \n33.07% \u00b17.11% 38.17% \u00b15.41% 33.69% \u00b16.2% 28.14% \u00b16.72% 28.19% \u00b14.99% 26.06% \u00b15.69% \n1.d. \n32.19% \u00b16.05% 29.27% \u00b16.2% \n31.34% \u00b15.23% 32.35% \u00b16.68% 22.21% \u00b14.89% 27.52% \u00b16.03% \n2.a. \n33.70% \u00b16.79% 36.26% \u00b15.16% 33.89% \u00b15.76% 28.08% \u00b16.8% \n25.18% \u00b15.22% 25.20% \u00b15.78% \n2.b. \n37.06% \u00b16.77% 29.79% \u00b15.99% 34.93% \u00b15.34% 28.84% \u00b16.94% 22.67% \u00b15.91% 25.06% \u00b16.11% \n2.c. \n33.98% \u00b15.92% 33.88% \u00b16% \n33.68% \u00b1%5.02 28.38% \u00b16.88% 23.60% \u00b15.9% \n24.80% \u00b16.06% \n\n\n\nTable 5 :\n5Comparison of post-processing approaches for tactics and techniques (considering a perfect techniques and tactics predictions respectively).Approaches \nMicro \nMacro \nPrecision \nRecall \nF0.5 \nPrecision \nRecall \nF0.5 \nTactics \nInde. \n65.64% \u00b13.76% \n64.69% \u00b13% 65.38% \u00b12.87% 60.26% \u00b13.2% 58.50% \u00b13.68% 59.47% \u00b12.29% \n1.a. \n100% \u00b10% \n100% \u00b10% \n100% \u00b10% \n100% \u00b10% \n100% \u00b10% \n100% \u00b10% \n1.d. \n68.09% \u00b14.02% 72.37% \u00b12.97% 68.84% \u00b13.24% 63.32% \u00b13.1% 66.29% \u00b14% \n63.54% \u00b12.08% \nTechniques \nInde. \n37.18% \u00b16.75% 29.79% \u00b15.91% 35.02% \u00b15.32% 28.84% \u00b16.9% 22.67% \u00b15.94% 25.06% \u00b16.09% \n1.b. \n51.54% \u00b15.01% 55.90% \u00b14.23% 52.24% \u00b14.4% 41.05% \u00b13.94% 39.04% \u00b15.13% 38.35% \u00b13.16% \n1.c. \n36.51% \u00b18% \n48.75% \u00b14.13% 38.14% \u00b17.54% 34.21% \u00b17.43% 37.12% \u00b14.13% 32.36% \u00b16.24% \n1.d. \n38.40% \u00b17% \n28.39% \u00b16.14% 35.44% \u00b15.42% 29.14% \u00b17.1% 22.40% \u00b16.22% 25.23% \u00b16.37% \n\n\n\nTable 6 :\n6Comparison between rcATT and Unfetter Insight. Tactics rcATT 79.31% 12.20% 37.75% 81.73% 8.21% 23.23% Unfetter insight 19.09% 26.76% 20.25% 19.17% 23.14% 19.28%Tools \nMicro \nMacro \nPrecision Recall \nF 0.5 Precision Recall \nF 0.5 \nTechniques \nrcATT 72.22% 2.07% 9.30% 20.60% 4.33% 10.11% \nUnfetter \ninsight \n3.54% \n1.11% 2.46% \n1.31% \n0.64% 0.74% \n\n\nIBM X Force and OTX AlienVault alone have an average of 15 new reports per day. 4 https://github.com/vlegoy/rcATT\nhttps://github.com/mitre/cti/tree/master/enterprise-attack, accessed February 2020\nInterestingly, their wiki-like corpus is, in fact, the content of the ATT&CK website with the techniques definitions, and thus a subset of our dataset.\n\nAT&T: Alienvault open threat exchange. AT&T: Alienvault open threat exchange (2019), https://otx.alienvault.com\n\nAutomated threat report classification over multi-source data. G Ayoade, S Chandra, L Khan, K Hamlen, B Thuraisingham, 2018 IEEE 4th International Conference on Collaboration and Internet Computing (CIC). IEEEAyoade, G., Chandra, S., Khan, L., Hamlen, K., Thuraisingham, B.: Automated threat report classification over multi-source data. In: 2018 IEEE 4th International Conference on Collaboration and Internet Computing (CIC). pp. 236-245. IEEE (2018)\n\nStandardizing cyber threat intelligence information with the structured threat information expression (stix). S Barnum, Mitre Corporation 11Barnum, S.: Standardizing cyber threat intelligence information with the struc- tured threat information expression (stix). Mitre Corporation 11, 1-22 (2012)\n\nA benchmark and evaluation for text extraction from pdf. H Bast, C Korzen, Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries. the 17th ACM/IEEE Joint Conference on Digital LibrariesIEEE PressBast, H., Korzen, C.: A benchmark and evaluation for text extraction from pdf. In: Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries. pp. 99-108. IEEE Press (2017)\n\nImproving multi-label classification by means of cross-ontology association rules. F Benites, E Sapozhnikova, bioinformatics. 2920Benites, F., Sapozhnikova, E.: Improving multi-label classification by means of cross-ontology association rules. bioinformatics 2(9), 20 (2015)\n\nCollecting cyber threat intelligence from hacker forums via a two-stage, hybrid process using support vector machines and latent dirichlet allocation. I Deliu, C Leichter, K Franke, 2018 IEEE International Conference on Big Data (Big Data). IEEEDeliu, I., Leichter, C., Franke, K.: Collecting cyber threat intelligence from hacker forums via a two-stage, hybrid process using support vector machines and latent dirichlet allocation. In: 2018 IEEE International Conference on Big Data (Big Data). pp. 5008-5013. IEEE (2018)\n\nStrategic cyber threat intelligence sharing: a case study of ids logs. S E Dog, A Tweed, L Rouse, B Chu, D Qi, Y Hu, J Yang, E Al-Shaer, 2016 25th International Conference on Computer Communication and Networks (ICCCN). IEEEDog, S.E., Tweed, A., Rouse, L., Chu, B., Qi, D., Hu, Y., Yang, J., Al-Shaer, E.: Strategic cyber threat intelligence sharing: a case study of ids logs. In: 2016 25th International Conference on Computer Communication and Networks (ICCCN). pp. 1-6. IEEE (2016)\n\nOptimum branchings. J Edmonds, Journal of Research of the national Bureau of Standards B. 714Edmonds, J.: Optimum branchings. Journal of Research of the national Bureau of Standards B 71(4), 233-240 (1967)\n\nGartner: Threat intelligence definition. Gartner: Threat intelligence definition (2013), https://www.gartner.com/en/ documents/2487216\n\nSteiner minimal trees. E N Gilbert, H O Pollak, SIAM Journal on Applied Mathematics. 161Gilbert, E.N., Pollak, H.O.: Steiner minimal trees. SIAM Journal on Applied Math- ematics 16(1), 1-29 (1968)\n\nTtpdrill: Automatic and accurate extraction of threat actions from unstructured text of cti sources. G Husari, E Al-Shaer, M Ahmed, B Chu, X Niu, Proceedings of the 33rd Annual Computer Security Applications Conference. the 33rd Annual Computer Security Applications ConferenceACMHusari, G., Al-Shaer, E., Ahmed, M., Chu, B., Niu, X.: Ttpdrill: Automatic and accurate extraction of threat actions from unstructured text of cti sources. In: Proceedings of the 33rd Annual Computer Security Applications Conference. pp. 103-115. ACM (2017)\n\nUsing entropy and mutual information to extract threat actions from cyber threat intelligence. G Husari, X Niu, B Chu, E Al-Shaer, 2018 IEEE International Conference on Intelligence and Security Informatics (ISI). IEEEHusari, G., Niu, X., Chu, B., Al-Shaer, E.: Using entropy and mutual information to extract threat actions from cyber threat intelligence. In: 2018 IEEE International Conference on Intelligence and Security Informatics (ISI). pp. 1-6. IEEE (2018)\n\nIBM: Ibm x-force exchange. IBM: Ibm x-force exchange (2019), https://exchange.xforce.ibmcloud.com\n\nTowards a relation extraction framework for cyber-security concepts. C L Jones, R A Bridges, K M Huffer, J R Goodall, Proceedings of the 10th Annual Cyber and Information Security Research Conference. the 10th Annual Cyber and Information Security Research ConferenceACM11Jones, C.L., Bridges, R.A., Huffer, K.M., Goodall, J.R.: Towards a relation extrac- tion framework for cyber-security concepts. In: Proceedings of the 10th Annual Cyber and Information Security Research Conference. p. 11. ACM (2015)\n\nDie pflanzenassoziationen der pieninen. S Kulczynski, Proceedings of the Eighth International Joint Conference on Natural Language Processing. the Eighth International Joint Conference on Natural Language Processing2Bulletin International de l'Academie Polonaise des Sciences et des LettresShort PapersKulczynski, S.: Die pflanzenassoziationen der pieninen. In: Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers), Bulletin International de l'Academie Polonaise des Sciences et des Lettres, Classe des Sciences Mathematiques et Naturelles B. pp. 57-203 (1927)\n\nAcing the ioc game: Toward automatic discovery and analysis of open-source cyber threat intelligence. X Liao, K Yuan, X Wang, Z Li, L Xing, R Beyah, Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. the 2016 ACM SIGSAC Conference on Computer and Communications SecurityACMLiao, X., Yuan, K., Wang, X., Li, Z., Xing, L., Beyah, R.: Acing the ioc game: Toward automatic discovery and analysis of open-source cyber threat intelligence. In: Proceedings of the 2016 ACM SIGSAC Conference on Computer and Commu- nications Security. pp. 755-766. ACM (2016)\n\nNltk: The natural language toolkit. E Loper, S Bird, Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational Linguistics. the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Language Processing and Computational LinguisticsPhiladelphiaAssociation for Computational LinguisticsLoper, E., Bird, S.: Nltk: The natural language toolkit. In: In Proceedings of the ACL Workshop on Effective Tools and Methodologies for Teaching Natural Lan- guage Processing and Computational Linguistics. Philadelphia: Association for Computational Linguistics (2002), https://www.nltk.org/\n\nBinary relevance efficacy for multilabel classification. O Luaces, J D\u00edez, J Barranquero, J J Del Coz, A Bahamonde, Progress in Artificial Intelligence. 14Luaces, O., D\u00edez, J., Barranquero, J., del Coz, J.J., Bahamonde, A.: Binary rele- vance efficacy for multilabel classification. Progress in Artificial Intelligence 1(4), 303-313 (2012)\n\nScoring, term weighting and the vector space model. C D Manning, P Raghavan, H Sch\u00fctze, Introduction to information retrieval. 100Manning, C.D., Raghavan, P., Sch\u00fctze, H.: Scoring, term weighting and the vector space model. Introduction to information retrieval 100, 2-4 (2008)\n\nKnapsack problems: algorithms and computer implementations. S Martello, Wiley-Interscience series in discrete mathematics and optimiza tion. Martello, S.: Knapsack problems: algorithms and computer implementations. Wiley-Interscience series in discrete mathematics and optimiza tion (1990)\n\nA faster approximation algorithm for the steiner problem in graphs. K Mehlhorn, Information Processing Letters. 273Mehlhorn, K.: A faster approximation algorithm for the steiner problem in graphs. Information Processing Letters 27(3), 125-128 (1988)\n\nEfficient estimation of word representations in vector space. T Mikolov, K Chen, G Corrado, J Dean, arXiv:1301.378123arXiv preprintmitre: Mitre att&ckMikolov, T., Chen, K., Corrado, G., Dean, J.: Efficient estimation of word repre- sentations in vector space. arXiv preprint arXiv:1301.3781 (2013) 23. mitre: Mitre att&ck (2019), https://attack.mitre.org/\n\nExtracting information about security vulnerabilities from web text. V Mulwad, W Li, A Joshi, T Finin, K Viswanathan, Proceedings of the. theMulwad, V., Li, W., Joshi, A., Finin, T., Viswanathan, K.: Extracting informa- tion about security vulnerabilities from web text. In: Proceedings of the 2011\n\n. Ieee/Wic/Acm, International Conferences on Web Intelligence and Intelligent Agent Technology. 03IEEE Computer SocietyIEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology-Volume 03. pp. 257-260. IEEE Computer Society (2011)\n\nSemi-automated information extraction from unstructured threat advisories. R R Ramnani, K Shivaram, S Sengupta, Proceedings of the 10th Innovations in Software Engineering Conference. the 10th Innovations in Software Engineering ConferenceACMRamnani, R.R., Shivaram, K., Sengupta, S., et al.: Semi-automated information extraction from unstructured threat advisories. In: Proceedings of the 10th Inno- vations in Software Engineering Conference. pp. 181-187. ACM (2017)\n\nClassifier chains for multi-label classification. J Read, B Pfahringer, G Holmes, E Frank, Machine learning. 853333Read, J., Pfahringer, B., Holmes, G., Frank, E.: Classifier chains for multi-label classification. Machine learning 85(3), 333 (2011)\n\nPost-processing techniques for improving predictions of multilabel learning approaches. A Soni, A Pappu, J C Ni, T Chevalier, Proceedings of the Eighth International Joint Conference on Natural Language Processing. the Eighth International Joint Conference on Natural Language ProcessingShort Papers2Soni, A., Pappu, A., Ni, J.C.m., Chevalier, T.: Post-processing techniques for im- proving predictions of multilabel learning approaches. In: Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers). pp. 61-66 (2017)\n\nA comparison of multilabel feature selection methods using the problem transformation approach. N Spola\u00f4r, E A Cherman, M C Monard, H D Lee, Electronic Notes in Theoretical Computer Science. 292Spola\u00f4R, N., Cherman, E.A., Monard, M.C., Lee, H.D.: A comparison of multi- label feature selection methods using the problem transformation approach. Elec- tronic Notes in Theoretical Computer Science 292, 135-151 (2013)\n\nMining multi-label data. G Tsoumakas, I Katakis, I Vlahavas, Data mining and knowledge discovery handbook. SpringerTsoumakas, G., Katakis, I., Vlahavas, I.: Mining multi-label data. In: Data mining and knowledge discovery handbook, pp. 667-685. Springer (2009)\n\n. Unfetter: Unfetter insight. Unfetter: Unfetter insight (2018), https://github.com/unfetter-discover/ unfetter-insight\n\nOntology-based multi-classification learning for video concept detection. Y Wu, B L Tseng, J R Smith, 10.1109/ICME.2004.13943722004 IEEE International Conference on Multimedia and Expo (ICME) (IEEE Cat. 2Wu, Y., Tseng, B.L., Smith, J.R.: Ontology-based multi-classification learning for video concept detection. In: 2004 IEEE International Conference on Multimedia and Expo (ICME) (IEEE Cat. No.04TH8763). vol. 2, pp. 1003-1006 Vol.2 (June 2004). https://doi.org/10.1109/ICME.2004.1394372\n\nFeaturesmith: Automatically engineering features for malware detection by mining the security literature. Z Zhu, T Dumitra\u015f, Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. the 2016 ACM SIGSAC Conference on Computer and Communications SecurityACMZhu, Z., Dumitra\u015f, T.: Featuresmith: Automatically engineering features for mal- ware detection by mining the security literature. In: Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security. pp. 767-778. ACM (2016)\n\nChainsmith: Automatically learning the semantics of malicious campaigns by mining threat intelligence reports. Z Zhu, T Dumitras, 2018 IEEE European Symposium on Security and Privacy (EuroS&P). IEEEZhu, Z., Dumitras, T.: Chainsmith: Automatically learning the semantics of ma- licious campaigns by mining threat intelligence reports. In: 2018 IEEE European Symposium on Security and Privacy (EuroS&P). pp. 458-472. IEEE (2018)\n", "annotations": {"author": "[{\"end\":150,\"start\":81},{\"end\":240,\"start\":151},{\"end\":332,\"start\":241},{\"end\":419,\"start\":333}]", "publisher": null, "author_last_name": "[{\"end\":96,\"start\":91},{\"end\":164,\"start\":157},{\"end\":257,\"start\":250},{\"end\":346,\"start\":341}]", "author_first_name": "[{\"end\":90,\"start\":81},{\"end\":156,\"start\":151},{\"end\":249,\"start\":241},{\"end\":340,\"start\":333}]", "author_affiliation": "[{\"end\":149,\"start\":98},{\"end\":239,\"start\":192},{\"end\":331,\"start\":280},{\"end\":418,\"start\":367}]", "title": "[{\"end\":78,\"start\":1},{\"end\":497,\"start\":420}]", "venue": null, "abstract": "[{\"end\":1539,\"start\":598}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1755,\"start\":1752},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2809,\"start\":2806},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2995,\"start\":2991},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3018,\"start\":3015},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3468,\"start\":3465},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3884,\"start\":3883},{\"end\":4270,\"start\":4266},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5843,\"start\":5840},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7404,\"start\":7401},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8005,\"start\":8002},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8319,\"start\":8315},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9451,\"start\":9447},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9476,\"start\":9472},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10559,\"start\":10555},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11201,\"start\":11197},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11884,\"start\":11880},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12528,\"start\":12524},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12638,\"start\":12634},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12762,\"start\":12758},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":19376,\"start\":19373},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":19410,\"start\":19406},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":23510,\"start\":23507},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":23632,\"start\":23628},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24100,\"start\":24096},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":24198,\"start\":24194},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24292,\"start\":24288},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":24561,\"start\":24558},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":24971,\"start\":24967},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25116,\"start\":25112},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":32875,\"start\":32871},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":32878,\"start\":32875},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":33002,\"start\":32999},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":33052,\"start\":33048},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":33078,\"start\":33074},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":33081,\"start\":33078},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":33204,\"start\":33200},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":33602,\"start\":33598},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":33623,\"start\":33619},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":33691,\"start\":33688},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":34041,\"start\":34038},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":34105,\"start\":34101},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":34219,\"start\":34215},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":34694,\"start\":34690},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":34854,\"start\":34851},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":35514,\"start\":35510},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":37765,\"start\":37762},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":37788,\"start\":37784}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":39512,\"start\":39435},{\"attributes\":{\"id\":\"fig_1\"},\"end\":39546,\"start\":39513},{\"attributes\":{\"id\":\"fig_2\"},\"end\":39584,\"start\":39547},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":39832,\"start\":39585},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":44046,\"start\":39833},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":47534,\"start\":44047},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":48609,\"start\":47535},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":49461,\"start\":48610},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":49822,\"start\":49462}]", "paragraph": "[{\"end\":2590,\"start\":1555},{\"end\":3019,\"start\":2592},{\"end\":3968,\"start\":3021},{\"end\":4692,\"start\":3970},{\"end\":5324,\"start\":4694},{\"end\":5451,\"start\":5326},{\"end\":5987,\"start\":5472},{\"end\":6142,\"start\":5989},{\"end\":6234,\"start\":6144},{\"end\":6570,\"start\":6236},{\"end\":6818,\"start\":6572},{\"end\":6933,\"start\":6820},{\"end\":7267,\"start\":6935},{\"end\":8320,\"start\":7293},{\"end\":8793,\"start\":8322},{\"end\":9228,\"start\":8818},{\"end\":9916,\"start\":9230},{\"end\":10560,\"start\":9928},{\"end\":10839,\"start\":10573},{\"end\":11029,\"start\":10870},{\"end\":11809,\"start\":11053},{\"end\":12165,\"start\":11811},{\"end\":13018,\"start\":12191},{\"end\":13752,\"start\":13020},{\"end\":14212,\"start\":13754},{\"end\":14420,\"start\":14214},{\"end\":15549,\"start\":14422},{\"end\":15996,\"start\":15576},{\"end\":16736,\"start\":15998},{\"end\":17353,\"start\":16738},{\"end\":17651,\"start\":17355},{\"end\":18264,\"start\":17697},{\"end\":18471,\"start\":18290},{\"end\":18876,\"start\":18473},{\"end\":19289,\"start\":18878},{\"end\":20175,\"start\":19291},{\"end\":20857,\"start\":20177},{\"end\":21027,\"start\":20859},{\"end\":22664,\"start\":21066},{\"end\":23096,\"start\":22666},{\"end\":23423,\"start\":23098},{\"end\":24015,\"start\":23425},{\"end\":24444,\"start\":24017},{\"end\":24524,\"start\":24514},{\"end\":24901,\"start\":24526},{\"end\":26932,\"start\":24903},{\"end\":27544,\"start\":26959},{\"end\":29143,\"start\":27546},{\"end\":29900,\"start\":29145},{\"end\":32272,\"start\":29910},{\"end\":32688,\"start\":32274},{\"end\":33082,\"start\":32744},{\"end\":33587,\"start\":33084},{\"end\":34549,\"start\":33589},{\"end\":35433,\"start\":34551},{\"end\":37170,\"start\":35435},{\"end\":38310,\"start\":37185},{\"end\":38801,\"start\":38312},{\"end\":39434,\"start\":38803}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":24513,\"start\":24445}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":10802,\"start\":10795},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":14873,\"start\":14859},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":25259,\"start\":25252},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":25502,\"start\":25493},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":27803,\"start\":27796},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":36879,\"start\":36872}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1553,\"start\":1541},{\"attributes\":{\"n\":\"2\"},\"end\":5470,\"start\":5454},{\"attributes\":{\"n\":\"2.1\"},\"end\":7291,\"start\":7270},{\"attributes\":{\"n\":\"3\"},\"end\":8816,\"start\":8796},{\"attributes\":{\"n\":\"3.1\"},\"end\":9926,\"start\":9919},{\"attributes\":{\"n\":\"3.2\"},\"end\":10571,\"start\":10563},{\"attributes\":{\"n\":\"4\"},\"end\":10868,\"start\":10842},{\"attributes\":{\"n\":\"4.1\"},\"end\":11051,\"start\":11032},{\"attributes\":{\"n\":\"4.2\"},\"end\":12189,\"start\":12168},{\"attributes\":{\"n\":\"4.3\"},\"end\":15574,\"start\":15552},{\"attributes\":{\"n\":\"5\"},\"end\":17695,\"start\":17654},{\"attributes\":{\"n\":\"5.1\"},\"end\":18288,\"start\":18267},{\"end\":21064,\"start\":21030},{\"attributes\":{\"n\":\"5.2\"},\"end\":26957,\"start\":26935},{\"attributes\":{\"n\":\"6\"},\"end\":29908,\"start\":29903},{\"attributes\":{\"n\":\"7\"},\"end\":32742,\"start\":32691},{\"attributes\":{\"n\":\"8\"},\"end\":37183,\"start\":37173},{\"end\":39444,\"start\":39436},{\"end\":39522,\"start\":39514},{\"end\":39556,\"start\":39548},{\"end\":39595,\"start\":39586},{\"end\":39843,\"start\":39834},{\"end\":44057,\"start\":44048},{\"end\":47545,\"start\":47536},{\"end\":48620,\"start\":48611},{\"end\":49472,\"start\":49463}]", "table": "[{\"end\":39832,\"start\":39663},{\"end\":44046,\"start\":40688},{\"end\":47534,\"start\":44865},{\"end\":48609,\"start\":47615},{\"end\":49461,\"start\":48762},{\"end\":49822,\"start\":49634}]", "figure_caption": "[{\"end\":39512,\"start\":39446},{\"end\":39546,\"start\":39524},{\"end\":39584,\"start\":39558},{\"end\":39663,\"start\":39597},{\"end\":40688,\"start\":39845},{\"end\":44865,\"start\":44059},{\"end\":47615,\"start\":47547},{\"end\":48762,\"start\":48622},{\"end\":49634,\"start\":49474}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7414,\"start\":7406},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":30508,\"start\":30500},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":32368,\"start\":32359}]", "bib_author_first_name": "[{\"end\":50350,\"start\":50349},{\"end\":50360,\"start\":50359},{\"end\":50371,\"start\":50370},{\"end\":50379,\"start\":50378},{\"end\":50389,\"start\":50388},{\"end\":50851,\"start\":50850},{\"end\":51097,\"start\":51096},{\"end\":51105,\"start\":51104},{\"end\":51521,\"start\":51520},{\"end\":51532,\"start\":51531},{\"end\":51865,\"start\":51864},{\"end\":51874,\"start\":51873},{\"end\":51886,\"start\":51885},{\"end\":52309,\"start\":52308},{\"end\":52311,\"start\":52310},{\"end\":52318,\"start\":52317},{\"end\":52327,\"start\":52326},{\"end\":52336,\"start\":52335},{\"end\":52343,\"start\":52342},{\"end\":52349,\"start\":52348},{\"end\":52355,\"start\":52354},{\"end\":52363,\"start\":52362},{\"end\":52744,\"start\":52743},{\"end\":53090,\"start\":53089},{\"end\":53092,\"start\":53091},{\"end\":53103,\"start\":53102},{\"end\":53105,\"start\":53104},{\"end\":53366,\"start\":53365},{\"end\":53376,\"start\":53375},{\"end\":53388,\"start\":53387},{\"end\":53397,\"start\":53396},{\"end\":53404,\"start\":53403},{\"end\":53899,\"start\":53898},{\"end\":53909,\"start\":53908},{\"end\":53916,\"start\":53915},{\"end\":53923,\"start\":53922},{\"end\":54438,\"start\":54437},{\"end\":54440,\"start\":54439},{\"end\":54449,\"start\":54448},{\"end\":54451,\"start\":54450},{\"end\":54462,\"start\":54461},{\"end\":54464,\"start\":54463},{\"end\":54474,\"start\":54473},{\"end\":54476,\"start\":54475},{\"end\":54915,\"start\":54914},{\"end\":55599,\"start\":55598},{\"end\":55607,\"start\":55606},{\"end\":55615,\"start\":55614},{\"end\":55623,\"start\":55622},{\"end\":55629,\"start\":55628},{\"end\":55637,\"start\":55636},{\"end\":56121,\"start\":56120},{\"end\":56130,\"start\":56129},{\"end\":56807,\"start\":56806},{\"end\":56817,\"start\":56816},{\"end\":56825,\"start\":56824},{\"end\":56840,\"start\":56839},{\"end\":56842,\"start\":56841},{\"end\":56853,\"start\":56852},{\"end\":57143,\"start\":57142},{\"end\":57145,\"start\":57144},{\"end\":57156,\"start\":57155},{\"end\":57168,\"start\":57167},{\"end\":57430,\"start\":57429},{\"end\":57729,\"start\":57728},{\"end\":57974,\"start\":57973},{\"end\":57985,\"start\":57984},{\"end\":57993,\"start\":57992},{\"end\":58004,\"start\":58003},{\"end\":58338,\"start\":58337},{\"end\":58348,\"start\":58347},{\"end\":58354,\"start\":58353},{\"end\":58363,\"start\":58362},{\"end\":58372,\"start\":58371},{\"end\":58909,\"start\":58908},{\"end\":58911,\"start\":58910},{\"end\":58922,\"start\":58921},{\"end\":58934,\"start\":58933},{\"end\":59355,\"start\":59354},{\"end\":59363,\"start\":59362},{\"end\":59377,\"start\":59376},{\"end\":59387,\"start\":59386},{\"end\":59643,\"start\":59642},{\"end\":59651,\"start\":59650},{\"end\":59660,\"start\":59659},{\"end\":59662,\"start\":59661},{\"end\":59668,\"start\":59667},{\"end\":60225,\"start\":60224},{\"end\":60236,\"start\":60235},{\"end\":60238,\"start\":60237},{\"end\":60249,\"start\":60248},{\"end\":60251,\"start\":60250},{\"end\":60261,\"start\":60260},{\"end\":60263,\"start\":60262},{\"end\":60571,\"start\":60570},{\"end\":60584,\"start\":60583},{\"end\":60595,\"start\":60594},{\"end\":61003,\"start\":61002},{\"end\":61009,\"start\":61008},{\"end\":61011,\"start\":61010},{\"end\":61020,\"start\":61019},{\"end\":61022,\"start\":61021},{\"end\":61525,\"start\":61524},{\"end\":61532,\"start\":61531},{\"end\":62062,\"start\":62061},{\"end\":62069,\"start\":62068}]", "bib_author_last_name": "[{\"end\":50357,\"start\":50351},{\"end\":50368,\"start\":50361},{\"end\":50376,\"start\":50372},{\"end\":50386,\"start\":50380},{\"end\":50403,\"start\":50390},{\"end\":50858,\"start\":50852},{\"end\":51102,\"start\":51098},{\"end\":51112,\"start\":51106},{\"end\":51529,\"start\":51522},{\"end\":51545,\"start\":51533},{\"end\":51871,\"start\":51866},{\"end\":51883,\"start\":51875},{\"end\":51893,\"start\":51887},{\"end\":52315,\"start\":52312},{\"end\":52324,\"start\":52319},{\"end\":52333,\"start\":52328},{\"end\":52340,\"start\":52337},{\"end\":52346,\"start\":52344},{\"end\":52352,\"start\":52350},{\"end\":52360,\"start\":52356},{\"end\":52372,\"start\":52364},{\"end\":52752,\"start\":52745},{\"end\":53100,\"start\":53093},{\"end\":53112,\"start\":53106},{\"end\":53373,\"start\":53367},{\"end\":53385,\"start\":53377},{\"end\":53394,\"start\":53389},{\"end\":53401,\"start\":53398},{\"end\":53408,\"start\":53405},{\"end\":53906,\"start\":53900},{\"end\":53913,\"start\":53910},{\"end\":53920,\"start\":53917},{\"end\":53932,\"start\":53924},{\"end\":54446,\"start\":54441},{\"end\":54459,\"start\":54452},{\"end\":54471,\"start\":54465},{\"end\":54484,\"start\":54477},{\"end\":54926,\"start\":54916},{\"end\":55604,\"start\":55600},{\"end\":55612,\"start\":55608},{\"end\":55620,\"start\":55616},{\"end\":55626,\"start\":55624},{\"end\":55634,\"start\":55630},{\"end\":55643,\"start\":55638},{\"end\":56127,\"start\":56122},{\"end\":56135,\"start\":56131},{\"end\":56814,\"start\":56808},{\"end\":56822,\"start\":56818},{\"end\":56837,\"start\":56826},{\"end\":56850,\"start\":56843},{\"end\":56863,\"start\":56854},{\"end\":57153,\"start\":57146},{\"end\":57165,\"start\":57157},{\"end\":57176,\"start\":57169},{\"end\":57439,\"start\":57431},{\"end\":57738,\"start\":57730},{\"end\":57982,\"start\":57975},{\"end\":57990,\"start\":57986},{\"end\":58001,\"start\":57994},{\"end\":58009,\"start\":58005},{\"end\":58345,\"start\":58339},{\"end\":58351,\"start\":58349},{\"end\":58360,\"start\":58355},{\"end\":58369,\"start\":58364},{\"end\":58384,\"start\":58373},{\"end\":58582,\"start\":58570},{\"end\":58919,\"start\":58912},{\"end\":58931,\"start\":58923},{\"end\":58943,\"start\":58935},{\"end\":59360,\"start\":59356},{\"end\":59374,\"start\":59364},{\"end\":59384,\"start\":59378},{\"end\":59393,\"start\":59388},{\"end\":59648,\"start\":59644},{\"end\":59657,\"start\":59652},{\"end\":59665,\"start\":59663},{\"end\":59678,\"start\":59669},{\"end\":60233,\"start\":60226},{\"end\":60246,\"start\":60239},{\"end\":60258,\"start\":60252},{\"end\":60267,\"start\":60264},{\"end\":60581,\"start\":60572},{\"end\":60592,\"start\":60585},{\"end\":60604,\"start\":60596},{\"end\":61006,\"start\":61004},{\"end\":61017,\"start\":61012},{\"end\":61028,\"start\":61023},{\"end\":61529,\"start\":61526},{\"end\":61541,\"start\":61533},{\"end\":62066,\"start\":62063},{\"end\":62078,\"start\":62070}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":50284,\"start\":50173},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":53772734},\"end\":50738,\"start\":50286},{\"attributes\":{\"id\":\"b2\"},\"end\":51037,\"start\":50740},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":4910532},\"end\":51435,\"start\":51039},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":12294032},\"end\":51711,\"start\":51437},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":59230924},\"end\":52235,\"start\":51713},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":17528433},\"end\":52721,\"start\":52237},{\"attributes\":{\"id\":\"b7\"},\"end\":52928,\"start\":52723},{\"attributes\":{\"id\":\"b8\"},\"end\":53064,\"start\":52930},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":16858373},\"end\":53262,\"start\":53066},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":20396776},\"end\":53801,\"start\":53264},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":57190904},\"end\":54267,\"start\":53803},{\"attributes\":{\"id\":\"b12\"},\"end\":54366,\"start\":54269},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":16870348},\"end\":54872,\"start\":54368},{\"attributes\":{\"id\":\"b14\"},\"end\":55494,\"start\":54874},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":6118751},\"end\":56082,\"start\":55496},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":1438450},\"end\":56747,\"start\":56084},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":207475088},\"end\":57088,\"start\":56749},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":115935391},\"end\":57367,\"start\":57090},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":62152318},\"end\":57658,\"start\":57369},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":7772232},\"end\":57909,\"start\":57660},{\"attributes\":{\"doi\":\"arXiv:1301.3781\",\"id\":\"b21\"},\"end\":58266,\"start\":57911},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":8200836},\"end\":58566,\"start\":58268},{\"attributes\":{\"id\":\"b23\"},\"end\":58831,\"start\":58568},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":3445200},\"end\":59302,\"start\":58833},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":7679549},\"end\":59552,\"start\":59304},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":28816964},\"end\":60126,\"start\":59554},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":14891665},\"end\":60543,\"start\":60128},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":22998},\"end\":60805,\"start\":60545},{\"attributes\":{\"id\":\"b29\"},\"end\":60926,\"start\":60807},{\"attributes\":{\"doi\":\"10.1109/ICME.2004.1394372\",\"id\":\"b30\",\"matched_paper_id\":9479207},\"end\":61416,\"start\":60928},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":12958836},\"end\":61948,\"start\":61418},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":13757099},\"end\":62376,\"start\":61950}]", "bib_title": "[{\"end\":50347,\"start\":50286},{\"end\":51094,\"start\":51039},{\"end\":51518,\"start\":51437},{\"end\":51862,\"start\":51713},{\"end\":52306,\"start\":52237},{\"end\":52741,\"start\":52723},{\"end\":53087,\"start\":53066},{\"end\":53363,\"start\":53264},{\"end\":53896,\"start\":53803},{\"end\":54435,\"start\":54368},{\"end\":54912,\"start\":54874},{\"end\":55596,\"start\":55496},{\"end\":56118,\"start\":56084},{\"end\":56804,\"start\":56749},{\"end\":57140,\"start\":57090},{\"end\":57427,\"start\":57369},{\"end\":57726,\"start\":57660},{\"end\":58335,\"start\":58268},{\"end\":58906,\"start\":58833},{\"end\":59352,\"start\":59304},{\"end\":59640,\"start\":59554},{\"end\":60222,\"start\":60128},{\"end\":60568,\"start\":60545},{\"end\":61000,\"start\":60928},{\"end\":61522,\"start\":61418},{\"end\":62059,\"start\":61950}]", "bib_author": "[{\"end\":50359,\"start\":50349},{\"end\":50370,\"start\":50359},{\"end\":50378,\"start\":50370},{\"end\":50388,\"start\":50378},{\"end\":50405,\"start\":50388},{\"end\":50860,\"start\":50850},{\"end\":51104,\"start\":51096},{\"end\":51114,\"start\":51104},{\"end\":51531,\"start\":51520},{\"end\":51547,\"start\":51531},{\"end\":51873,\"start\":51864},{\"end\":51885,\"start\":51873},{\"end\":51895,\"start\":51885},{\"end\":52317,\"start\":52308},{\"end\":52326,\"start\":52317},{\"end\":52335,\"start\":52326},{\"end\":52342,\"start\":52335},{\"end\":52348,\"start\":52342},{\"end\":52354,\"start\":52348},{\"end\":52362,\"start\":52354},{\"end\":52374,\"start\":52362},{\"end\":52754,\"start\":52743},{\"end\":53102,\"start\":53089},{\"end\":53114,\"start\":53102},{\"end\":53375,\"start\":53365},{\"end\":53387,\"start\":53375},{\"end\":53396,\"start\":53387},{\"end\":53403,\"start\":53396},{\"end\":53410,\"start\":53403},{\"end\":53908,\"start\":53898},{\"end\":53915,\"start\":53908},{\"end\":53922,\"start\":53915},{\"end\":53934,\"start\":53922},{\"end\":54448,\"start\":54437},{\"end\":54461,\"start\":54448},{\"end\":54473,\"start\":54461},{\"end\":54486,\"start\":54473},{\"end\":54928,\"start\":54914},{\"end\":55606,\"start\":55598},{\"end\":55614,\"start\":55606},{\"end\":55622,\"start\":55614},{\"end\":55628,\"start\":55622},{\"end\":55636,\"start\":55628},{\"end\":55645,\"start\":55636},{\"end\":56129,\"start\":56120},{\"end\":56137,\"start\":56129},{\"end\":56816,\"start\":56806},{\"end\":56824,\"start\":56816},{\"end\":56839,\"start\":56824},{\"end\":56852,\"start\":56839},{\"end\":56865,\"start\":56852},{\"end\":57155,\"start\":57142},{\"end\":57167,\"start\":57155},{\"end\":57178,\"start\":57167},{\"end\":57441,\"start\":57429},{\"end\":57740,\"start\":57728},{\"end\":57984,\"start\":57973},{\"end\":57992,\"start\":57984},{\"end\":58003,\"start\":57992},{\"end\":58011,\"start\":58003},{\"end\":58347,\"start\":58337},{\"end\":58353,\"start\":58347},{\"end\":58362,\"start\":58353},{\"end\":58371,\"start\":58362},{\"end\":58386,\"start\":58371},{\"end\":58584,\"start\":58570},{\"end\":58921,\"start\":58908},{\"end\":58933,\"start\":58921},{\"end\":58945,\"start\":58933},{\"end\":59362,\"start\":59354},{\"end\":59376,\"start\":59362},{\"end\":59386,\"start\":59376},{\"end\":59395,\"start\":59386},{\"end\":59650,\"start\":59642},{\"end\":59659,\"start\":59650},{\"end\":59667,\"start\":59659},{\"end\":59680,\"start\":59667},{\"end\":60235,\"start\":60224},{\"end\":60248,\"start\":60235},{\"end\":60260,\"start\":60248},{\"end\":60269,\"start\":60260},{\"end\":60583,\"start\":60570},{\"end\":60594,\"start\":60583},{\"end\":60606,\"start\":60594},{\"end\":61008,\"start\":61002},{\"end\":61019,\"start\":61008},{\"end\":61030,\"start\":61019},{\"end\":61531,\"start\":61524},{\"end\":61543,\"start\":61531},{\"end\":62068,\"start\":62061},{\"end\":62080,\"start\":62068}]", "bib_venue": "[{\"end\":50210,\"start\":50173},{\"end\":50489,\"start\":50405},{\"end\":50848,\"start\":50740},{\"end\":51184,\"start\":51114},{\"end\":51561,\"start\":51547},{\"end\":51952,\"start\":51895},{\"end\":52455,\"start\":52374},{\"end\":52811,\"start\":52754},{\"end\":52969,\"start\":52930},{\"end\":53149,\"start\":53114},{\"end\":53482,\"start\":53410},{\"end\":54015,\"start\":53934},{\"end\":54294,\"start\":54269},{\"end\":54567,\"start\":54486},{\"end\":55015,\"start\":54928},{\"end\":55730,\"start\":55645},{\"end\":56276,\"start\":56137},{\"end\":56900,\"start\":56865},{\"end\":57215,\"start\":57178},{\"end\":57508,\"start\":57441},{\"end\":57770,\"start\":57740},{\"end\":57971,\"start\":57911},{\"end\":58404,\"start\":58386},{\"end\":58662,\"start\":58584},{\"end\":59015,\"start\":58945},{\"end\":59411,\"start\":59395},{\"end\":59767,\"start\":59680},{\"end\":60317,\"start\":60269},{\"end\":60650,\"start\":60606},{\"end\":60835,\"start\":60809},{\"end\":61129,\"start\":61055},{\"end\":61628,\"start\":61543},{\"end\":62142,\"start\":62080},{\"end\":51241,\"start\":51186},{\"end\":53541,\"start\":53484},{\"end\":54635,\"start\":54569},{\"end\":55089,\"start\":55017},{\"end\":55802,\"start\":55732},{\"end\":56414,\"start\":56278},{\"end\":58409,\"start\":58406},{\"end\":59072,\"start\":59017},{\"end\":59841,\"start\":59769},{\"end\":61700,\"start\":61630}]"}}}, "year": 2023, "month": 12, "day": 17}