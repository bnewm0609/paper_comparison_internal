{"id": 218674528, "updated": "2023-10-06 15:31:04.177", "metadata": {"title": "Conformer: Convolution-augmented Transformer for Speech Recognition", "authors": "[{\"first\":\"Anmol\",\"last\":\"Gulati\",\"middle\":[]},{\"first\":\"James\",\"last\":\"Qin\",\"middle\":[]},{\"first\":\"Chung-Cheng\",\"last\":\"Chiu\",\"middle\":[]},{\"first\":\"Niki\",\"last\":\"Parmar\",\"middle\":[]},{\"first\":\"Yu\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Jiahui\",\"last\":\"Yu\",\"middle\":[]},{\"first\":\"Wei\",\"last\":\"Han\",\"middle\":[]},{\"first\":\"Shibo\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Zhengdong\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Yonghui\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Ruoming\",\"last\":\"Pang\",\"middle\":[]}]", "venue": "Interspeech 2020", "journal": "Interspeech 2020", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "Recently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit lo-cal features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-ef\ufb01cient way. To this regard, we propose the convolution-augmented trans-former for speech recognition, named Conformer . Conformer signi\ufb01cantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother. We also observe competitive performance of 2.7%/6.3% with a small model of only 10M parameters.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2005.08100", "mag": "3097777922", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/interspeech/GulatiQCPZYHWZW20", "doi": "10.21437/interspeech.2020-3015"}}, "content": {"source": {"pdf_hash": "0170fc76e934ee643f869df18fb617d5357e8b4e", "pdf_src": "Arxiv", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2005.08100", "status": "GREEN"}}, "grobid": {"id": "68f038c02c2dc8dd36ed52bf8d5558348b0337af", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0170fc76e934ee643f869df18fb617d5357e8b4e.txt", "contents": "\nConformer: Convolution-augmented Transformer for Speech Recognition\n\n\nAnmol Gulati anmolgulati@google.com \nGoogle Inc\n\n\nJames Qin jamesqin@google.com \nGoogle Inc\n\n\nChung-Cheng Chiu chungchengc@google.com \nGoogle Inc\n\n\nNiki Parmar nikip@google.com \nGoogle Inc\n\n\nYu Zhang zhangzd@google.com \nGoogle Inc\n\n\nJiahui Yu jiahuiyu@google.com \nGoogle Inc\n\n\nWei Han weihan@google.com \nGoogle Inc\n\n\nShibo Wang shibow@google.com \nGoogle Inc\n\n\nZhengdong Zhang \nGoogle Inc\n\n\nYonghui Wu yonghui@google.com \nGoogle Inc\n\n\nRuoming Pang rpang@google.com \nGoogle Inc\n\n\nConformer: Convolution-augmented Transformer for Speech Recognition\nIndex Terms: speech recognitionattentionconvolutional neu- ral networkstransformerend-to-end\nRecently Transformer and Convolution neural network (CNN) based models have shown promising results in Automatic Speech Recognition (ASR), outperforming Recurrent neural networks (RNNs). Transformer models are good at capturing content-based global interactions, while CNNs exploit local features effectively. In this work, we achieve the best of both worlds by studying how to combine convolution neural networks and transformers to model both local and global dependencies of an audio sequence in a parameter-efficient way. To this regard, we propose the convolution-augmented transformer for speech recognition, named Conformer. Conformer significantly outperforms the previous Transformer and CNN based models achieving state-of-the-art accuracies. On the widely used LibriSpeech benchmark, our model achieves WER of 2.1%/4.3% without using a language model and 1.9%/3.9% with an external language model on test/testother. We also observe competitive performance of 2.7%/6.3% with a small model of only 10M parameters.\n\nIntroduction\n\nEnd-to-end automatic speech recognition (ASR) systems based on neural networks have seen large improvements in recent years. Recurrent neural networks (RNNs) have been the defacto choice for ASR [1,2,3,4] as they can model the temporal dependencies in the audio sequences effectively [5]. Recently, the Transformer architecture based on self-attention [6,7] has enjoyed widespread adoption for modeling sequences due to its ability to capture long distance interactions and the high training efficiency. Alternatively, convolutions have also been successful for ASR [8,9,10,11,12], which capture local context progressively via a local receptive field layer by layer.\n\nHowever, models with self-attention or convolutions each has its limitations. While Transformers are good at modeling long-range global context, they are less capable to extract finegrained local feature patterns. Convolution neural networks (CNNs), on the other hand, exploit local information and are used as the de-facto computational block in vision. They learn shared position-based kernels over a local window which maintain translation equivariance and are able to capture features like edges and shapes. One limitation of using local connectivity is that you need many more layers or parameters to capture global information. To combat this issue, contemporary work Con-textNet [10] adopts the squeeze-and-excitation module [13] in each residual block to capture longer context. However, it is still limited in capturing dynamic global context as it only applies a global averaging over the entire sequence.\n\nRecent works have shown that combining convolution and  Figure 1: Conformer encoder model architecture. Conformer comprises of two macaron-like feed-forward layers with halfstep residual connections sandwiching the multi-headed selfattention and convolution modules. This is followed by a post layernorm.\n\nself-attention improves over using them individually [14]. Together, they are able to learn both position-wise local features, and use content-based global interactions. Concurrently, papers like [15,16] have augmented self-attention with relative position based information that maintains equivariance. Wu et al. [17] proposed a multi-branch architecture with splitting the input into two branches: self-attention and convolution; and concatenating their outputs. Their work targeted mobile applications and showed improvements in machine translation tasks.\n\nIn this work, we study how to organically combine convolutions with self-attention in ASR models. We hypothesize that both global and local interactions are important for being parameter efficient. To achieve this, we propose a novel combination of self-attention and convolution will achieve the best of both worlds -self-attention learns the global interaction whilst the convolutions efficiently capture the relative-offset-based local correlations. Inspired by Wu et al. [17,18], we introduce a novel combination of self-attention and convolution, sandwiched between a pair feed forward modules, as illustrated in Fig 1. Our proposed model, named Conformer, achieves state-ofthe-art results on LibriSpeech, outperforming the previous best published Transformer Transducer [7] Figure 2: Convolution module. The convolution module contains a pointwise convolution with an expansion factor of 2 projecting the number of channels with a GLU activation layer, followed by a 1-D Depthwise convolution. The 1-D depthwise conv is followed by a Batchnorm and then a swish activation layer.\n\nment on the testother dataset with an external language model. We present three models based on model parameter limit constraints of 10M , 30M and 118M. Our 10M model shows an improvement when compared to similar sized contemporary work [10] with 2.7%/6.3% on test/testother datasets. Our medium 30M parameters-sized model already outperforms transformer transducer published in [7] which uses 139M model parameters.\n\nWith the big 118M parameter model, we are able to achieve 2.1%/4.3% without using language models and 1.9%/3.9% with an external language model.\n\nWe further carefully study the effects of the number of attention heads, convolution kernel sizes, activation functions, placement of feed-forward layers, and different strategies of adding convolution modules to a Transformer-based network, and shed light on how each contributes to the accuracy improvements.\n\n\nConformer Encoder\n\nOur audio encoder first processes the input with a convolution subsampling layer and then with a number of conformer blocks, as illustrated in Figure 1. The distinctive feature of our model is the use of Conformer blocks in the place of Transformer blocks as in [7,19].\n\nA conformer block is composed of four modules stacked together, i.e, a feed-forward module, a self-attention module, a convolution module, and a second feed-forward module in the end. Sections 2.1, 1, and 2.3 introduce the self-attention, convolution, and feed-forward modules, respectively. Finally, 2.4 describes how these sub blocks are combined.\n\n\nMulti-Headed Self-Attention Module\n\nWe employ multi-headed self-attention (MHSA) while integrating an important technique from Transformer-XL [20], the relative sinusoidal positional encoding scheme. The relative positional encoding allows the self-attention module to generalize better on different input length and the resulting encoder is more robust to the variance of the utterance length. We use prenorm residual units [21,22] with dropout which helps training and regularizing deeper models. Figure 3 below illustrates the multi-headed self-attention block.\n\n\nLayernorm\n\n\nMulti-Head Attention with\n\nRelative Positional Embedding Dropout + Figure 3: Multi-Headed self-attention module. We use multiheaded self-attention with relative positional embedding in a pre-norm residual unit.\n\n\nConvolution Module\n\nInspired by [17], the convolution module starts with a gating mechanism [23]-a pointwise convolution and a gated linear unit (GLU). This is followed by a single 1-D depthwise convolution layer. Batchnorm is deployed just after the convolution to aid training deep models. Figure 2 illustrates the convolution block.\n\n\nFeed Forward Module\n\nThe Transformer architecture as proposed in [6] deploys a feed forward module after the MHSA layer and is composed of two linear transformations and a nonlinear activation in between. A residual connection is added over the feed-forward layers, followed by layer normalization. This structure is also adopted by Transformer ASR models [7,24]. We follow pre-norm residual units [21,22] and apply layer normalization within the residual unit and on the input before the first linear layer. We also apply Swish activation [25] and dropout, which helps regularizing the network. Figure 4 illustrates the Feed Forward (FFN) module.\n\n\nConformer Block\n\nOur proposed Conformer block contains two Feed Forward modules sandwiching the Multi-Headed Self-Attention module and the Convolution module, as shown in Figure 1.\n\nThis sandwich structure is inspired by Macaron-Net [18], which proposes replacing the original feed-forward layer in the Transformer block into two half-step feed-forward layers, one before the attention layer and one after. As in Macron-Net, we employ half-step residual weights in our feed-forward (FFN) modules. The second feed-forward module is followed by a final layernorm layer. Mathematically, this means, for input xi to a Conformer block i, the output yi of the block is:\nxi = xi + 1 2 FFN(xi) x i =xi + MHSA(xi) x i = x i + Conv(x i ) yi = Layernorm(x i + 1 2 FFN(x i ))(1)\nwhere FFN refers to the Feed forward module, MHSA refers to the Multi-Head Self-Attention module, and Conv refers to the Convolution module as described in the preceding sections. Our ablation study discussed in Sec 3.4.3 compares the Macaron-style half-step FFNs with the vanilla FFN as used in previous works. We find that having two Macaron-net style feed-forward layers with half-step residual connections sandwiching the attention and convolution modules in between provides a significant improvement over having a single feedforward module in our Conformer architecture. The combination of convolution and self-attention has been studied before and one can imagine many ways to achieve that. Different options of augmenting convolutions with selfattention are studied in Sec 3.4.2. We found that convolution module stacked after the self-attention module works best for speech recognition.\n\n\nExperiments\n\n\nData\n\nWe evaluate the proposed model on the LibriSpeech [26] dataset, which consists of 970 hours of labeled speech and an additional 800M word token text-only corpus for building language model. We extracted 80-channel filterbanks features computed from a 25ms window with a stride of 10ms. We use SpecAugment [27,28] with mask parameter (F = 27), and ten time masks with maximum time-mask ratio (pS = 0.05), where the maximum-size of the time mask is set to pS times the length of the utterance.\n\n\nConformer Transducer\n\nWe identify three models, small, medium and large, with 10M, 30M, and 118M params, respectively, by sweeping different combinations of network depth, model dimensions, number of attention heads and choosing the best performing one within model parameter size constraints. We use a single-LSTM-layer decoder in all our models. Table 1 describes their architecture hyper-parameters. For regularization, we apply dropout [29] in each residual unit of the conformer, i.e, to the output of each module, before it is added to the module input. We use a rate of P drop = 0.1. Variational noise [5,30] is introduced to the model as a regularization. A 2 regularization with 1e \u2212 6 weight is also added to all the trainable weights in the network. We train the models with the Adam optimizer [31] with \u03b21 = 0.9, \u03b22 = 0.98 and = 10 \u22129 and a transformer learning rate schedule [6], with 10k warm-up steps and peak learning rate 0.05/ \u221a d where d is the model dimension in conformer encoder.\n\nWe use a 3-layer LSTM language model (LM) with width 4096 trained on the LibriSpeech langauge model corpus with the LibriSpeech960h transcripts added, tokenized with the 1k WPM built from LibriSpeech 960h. The LM has word-level perplexity 63.9 on the dev-set transcripts. The LM weight \u03bb for shallow fusion is tuned on the dev-set via grid search. All models are implemented with Lingvo toolkit [32]. Table 2 compares the (WER) result of our model on Lib-riSpeech test-clean/test-other with a few state-of-the-art models include: ContextNet [10], Transformer transducer [7], and QuartzNet [9]. All our evaluation results round up to 1 digit after decimal point. Without a language model, the performance of our medium model already achieve competitive results of 2.3/5.0 on test/testother outperforming the best known Transformer, LSTM based model, or a similar sized convolution model. With the language model added, our model achieves the lowest word  error rate among all the existing models. This clearly demonstrates the effectiveness of combining Transformer and convolution in a single neural network.\n\n\nResults on LibriSpeech\n\n\nAblation Studies\n\n\nConformer Block vs. Transformer Block\n\nA Conformer block differs from a Transformer block in a number of ways, in particular, the inclusion of a convolution block and having a pair of FFNs surrounding the block in the Macaron-style. Below we study these effects of these differences by mutating a Conformer block towards a Transformer block, while keeping the total number of parameters unchanged. Table 3 shows the impact of each change to the Conformer block. Among all differences, convolution sub-block is the most important feature, while having a Macaron-style FFN pair is also more effective than a single FFN of the same number of parameters. Using swish activations led to faster convergence in the Conformer models. \n\n\nCombinations of Convolution and Transformer Modules\n\nWe study the effects of various different ways of combining the multi-headed self-attention (MHSA) module with the convolution module. First, we try replacing the depthwise convolution in the convolution module with a lightweight convolution [35], see a significant drop in the performance especially on the devother dataset. Second, we study placing the convolution module before the MHSA module in our Conformer model and find that it degrades the results by 0.1 on dev-other. Another possible way of the architecture is to split the input into parallel branches of multi-headed self attention module and a convolution module with their output concatenated as suggested in [17]. We found that this worsens the performance when compared to our proposed architecture. These results in Table 4 suggest the advantage of placing the convolution module after the self-attention module in the Conformer block. \n\n\nMacaron Feed Forward Modules\n\nInstead of a single feed-forward module (FFN) post the attention blocks as in the Transformer models, the Conformer block has a pair of macaron-like Feed forward modules sandwiching the self-attention and convolution modules. Further, the Conformer feed forward modules are used with half-step residuals. Table 5 shows the impact of changing the Conformer block to use a single FFN or full-step residuals. \n\n\nNumber of Attention Heads\n\nIn self-attention, each attention head learns to focus on different parts of the input, making it possible to improve predictions beyond the simple weighted average. We perform experiments to study the effect of varying the number of attention heads from 4 to 32 in our large model, using the same number of heads in all layers. We find that increasing attention heads up to 16 improves the accuracy, especially over the devother datasets, as shown in Table 6. \n\n\nConvolution Kernel Sizes\n\nTo study the effect of kernel sizes in the depthwise convolution, we sweep the kernel size in {3, 7, 17, 32, 65} of the large model, using the same kernel size for all layers. We find that the performance improves with larger kernel sizes till kernel sizes 17 and 32 but worsens in the case of kernel size 65, as show in Table 7. On comparing the second decimal in dev WER, we find kernel size 32 to perform better than rest. \n\n\nConclusion\n\nIn this work, we introduced Conformer, an architecture that integrates components from CNNs and Transformers for endto-end speech recognition. We studied the importance of each component, and demonstrated that the inclusion of convolution modules is critical to the performance of the Conformer model. The model exhibits better accuracy with fewer parameters than previous work on the LibriSpeech dataset, and achieves a new state-of-the-art performance at 1.9%/3.9% for test/testother.\n\nFigure 4 :\n4Feed forward module. The first linear layer uses an expansion factor of 4 and the second linear layer projects it back to the model dimension. We use swish activation and a pre-norm residual units in feed forward module.\n\n\nby 15% relative improve-arXiv:2005.08100v1 [eess.AS] 16 May 2020Layernorm \n\nGlu \nActivation \n\nPointwise \nConv \nBatchNorm \nSwish \nActivation \n\n1D \nDepthwise \nConv \n\nPointwise \nConv \nDropout \n\n+ \n\n\n\nTable 1 :\n1Model hyper-parameters for Conformer S, M, and L models, found via sweeping different combinations and choosing the best performing models within the parameter limits.Model \nConformer \n(S) \n\nConformer \n(M) \n\nConformer \n(L) \nNum Params (M) \n10.3 \n30.7 \n118.8 \nEncoder Layers \n16 \n16 \n17 \nEncoder Dim \n144 \n256 \n512 \nAttention Heads \n4 \n4 \n8 \nConv Kernel Size \n32 \n32 \n32 \nDecoder Layers \n1 \n1 \n1 \nDecoder Dim \n320 \n640 \n640 \n\n\n\nTable 2 :\n2Comparisonof Conformer with recent published mod-\nels. Our model shows improvements consistently over various \nmodel parameter size constraints. At 10.3M parameters, our \nmodel is 0.7% better on testother when compared to contempo-\nrary work, ContextNet(S) [10]. At 30.7M model parameters our \nmodel already significantly outperforms the previous published \nstate of the art results of Transformer Transducer [7] with 139M \nparameters. \n\nMethod \n#Params (M) \nWER Without LM \nWER With LM \n\ntestclean testother testclean testother \n\nHybrid \nTransformer [33] \n-\n-\n-\n2.26 \n4.85 \nCTC \nQuartzNet [9] \n19 \n3.90 \n11.28 \n2.69 \n7.25 \nLAS \nTransformer [34] \n270 \n2.89 \n6.98 \n2.33 \n5.17 \nTransformer [19] \n-\n2.2 \n5.6 \n2.6 \n5.7 \nLSTM \n360 \n2.6 \n6.0 \n2.2 \n5.2 \nTransducer \nTransformer [7] \n139 \n2.4 \n5.6 \n2.0 \n4.6 \nContextNet(S) [10] \n10.8 \n2.9 \n7.0 \n2.3 \n5.5 \nContextNet(M) [10] \n31.4 \n2.4 \n5.4 \n2.0 \n4.5 \nContextNet(L) [10] \n112.7 \n2.1 \n4.6 \n1.9 \n4.1 \n\nConformer (Ours) \nConformer(S) \n10.3 \n2.7 \n6.3 \n2.1 \n5.0 \nConformer(M) \n30.7 \n2.3 \n5.0 \n2.0 \n4.3 \nConformer(L) \n118.8 \n2.1 \n4.3 \n1.9 \n3.9 \n\n\n\nTable 3 :\n3Disentangling Conformer.Starting from a Conformer \nblock, we remove its features and move towards a vanilla Trans-\nformer block: (1) replacing SWISH with ReLU; (2) remov-\ning the convolution sub-block; (3) replacing the Macaron-style \nFFN pairs with a single FFN; (4) replacing self-attention with \nrelative positional embedding [20] with a vanilla self-attention \nlayer [6]. All ablation study results are evaluated without the \nexternal LM. \n\nModel \nArchitecture \n\ndev \nclean \n\ndev \nother \n\ntest \nclean \n\ntest \nother \nConformer Model \n1.9 \n4.4 \n2.1 \n4.3 \n-SWISH + ReLU \n1.9 \n4.4 \n2.0 \n4.5 \n-Convolution Block \n2.1 \n4.8 \n2.1 \n4.9 \n-Macaron FFN \n2.1 \n5.1 \n2.1 \n5.0 \n-Relative Pos. Emb. \n2.3 \n5.8 \n2.4 \n5.6 \n\n\n\nTable 4 :\n4Ablation study of Conformer Attention Convolution Blocks. Varying the combination of the convolution block with the multi-headed self attention: (1) Conformer architecture; (2) Using Lightweight convolutions instead of depthwise convolution in the convolution block in Conformer; (3) Convolution before multi-headed self attention; (4) Convolution and MHSA in parallel with their output concatenated[17].Model Architecture \ndev \nclean \n\ndev \nother \nConformer \n1.9 \n4.4 \n-Depthwise conv + Lightweight convolution \n2.0 \n4.8 \nConvolution block before MHSA \n1.9 \n4.5 \nParallel MHSA and Convolution \n2.0 \n4.9 \n\n\n\nTable 5 :\n5Ablation study of Macaron-net Feed Forward modules. Ablating the differences between the Conformer feed forward module with that of a single FFN used in Transformer models: (1) Conformer; (2) Conformer with full-step residuals in Feed forward modules; (3) replacing the Macaron-style FFN pair with a single FFN.Model \nArchitecture \n\ndev \nclean \n\ndev \nother \n\ntest \nclean \n\ntest \nother \nConformer \n1.9 \n4.4 \n2.1 \n4.3 \nSingle FFN \n1.9 \n4.5 \n2.1 \n4.5 \nFull step residuals \n1.9 \n4.5 \n2.1 \n4.5 \n\n\n\nTable 6 :\n6Ablation study on the attention heads in multi-headed self attention.Attention \nHeads \n\nDim per \nHead \n\ndev \nclean \n\ndev \nother \n\ntest \nclean \n\ntest \nother \n4 \n128 \n1.9 \n4.6 \n2.0 \n4.5 \n8 \n64 \n1.9 \n4.4 \n2.1 \n4.3 \n16 \n32 \n2.0 \n4.3 \n2.2 \n4.4 \n32 \n16 \n1.9 \n4.4 \n2.1 \n4.5 \n\n\n\nTable 7 :\n7Ablation study on depthwise convolution kernel sizes.Kernel \nsize \n\ndev \nclean \n\ndev \nother \n\ntest \nclean \n\ntest \nother \n3 \n1.88 \n4.41 \n1.99 \n4.39 \n7 \n1.88 \n4.30 \n2.02 \n4.44 \n17 \n1.87 \n4.31 \n2.04 \n4.38 \n32 \n1.83 \n4.30 \n2.03 \n4.29 \n65 \n1.89 \n4.47 \n1.98 \n4.46 \n\n\n\nStateof-the-art speech recognition with sequence-to-sequence models. C.-C Chiu, T N Sainath, Y Wu, R Prabhavalkar, P Nguyen, Z Chen, A Kannan, R J Weiss, K Rao, E Gonina, 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEEICASSPC.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen, Z. Chen, A. Kannan, R. J. Weiss, K. Rao, E. Gonina et al., \"State- of-the-art speech recognition with sequence-to-sequence models,\" in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 4774-4778.\n\nExploring architectures, data and units for streaming end-to-end speech recognition with rnn-transducer. K Rao, H Sak, R Prabhavalkar, 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEEK. Rao, H. Sak, and R. Prabhavalkar, \"Exploring architectures, data and units for streaming end-to-end speech recognition with rnn-transducer,\" in 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2017, pp. 193-199.\n\nStreaming End-to-end Speech Recognition For Mobile Devices. Y He, T N Sainath, R Prabhavalkar, I Mcgraw, R Alvarez, D Zhao, D Rybach, A Kannan, Y Wu, R Pang, Q Liang, D Bhatia, Y Shangguan, B Li, G Pundak, K C Sim, T Bagby, S.-Y Chang, K Rao, A Gruenstein, Proc. ICASSP. ICASSPY. He, T. N. Sainath, R. Prabhavalkar, I. McGraw, R. Alvarez, D. Zhao, D. Rybach, A. Kannan, Y. Wu, R. Pang, Q. Liang, D. Bhatia, Y. Shangguan, B. Li, G. Pundak, K. C. Sim, T. Bagby, S.-Y. Chang, K. Rao, and A. Gruenstein, \"Streaming End-to-end Speech Recognition For Mobile Devices,\" in Proc. ICASSP, 2019.\n\nA streaming on-device end-to-end model surpassing server-side conventional model quality and latency. T N Sainath, Y He, B Li, A Narayanan, R Pang, A Bruguier, S Chang, W Li, R Alvarez, Z Chen, ICASSP. T. N. Sainath, Y. He, B. Li, A. Narayanan, R. Pang, A. Bruguier, S.-y. Chang, W. Li, R. Alvarez, Z. Chen, and et al., \"A streaming on-device end-to-end model surpassing server-side conventional model quality and latency,\" in ICASSP, 2020.\n\nA Graves, arXiv:1211.3711Sequence transduction with recurrent neural networks. arXiv preprintA. Graves, \"Sequence transduction with recurrent neural net- works,\" arXiv preprint arXiv:1211.3711, 2012.\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, L Kaiser, I Polosukhin, A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, \"Attention is all you need,\" 2017.\n\nTransformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss. Q Zhang, H Lu, H Sak, A Tripathi, E Mcdermott, S Koo, S Kumar, ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEEICASSPQ. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo, and S. Kumar, \"Transformer transducer: A streamable speech recog- nition model with transformer encoders and rnn-t loss,\" in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 7829-7833.\n\nJasper: An end-to-end convolutional neural acoustic model. J Li, V Lavrukhin, B Ginsburg, R Leary, O Kuchaiev, J M Cohen, H Nguyen, R T Gadde, arXiv:1904.03288arXiv preprintJ. Li, V. Lavrukhin, B. Ginsburg, R. Leary, O. Kuchaiev, J. M. Co- hen, H. Nguyen, and R. T. Gadde, \"Jasper: An end-to-end convo- lutional neural acoustic model,\" arXiv preprint arXiv:1904.03288, 2019.\n\nQuartznet: Deep automatic speech recognition with 1d time-channel separable convolutions. S Kriman, S Beliaev, B Ginsburg, J Huang, O Kuchaiev, V Lavrukhin, R Leary, J Li, Y Zhang, arXiv:1910.10261arXiv preprintS. Kriman, S. Beliaev, B. Ginsburg, J. Huang, O. Kuchaiev, V. Lavrukhin, R. Leary, J. Li, and Y. Zhang, \"Quartznet: Deep automatic speech recognition with 1d time-channel separable con- volutions,\" arXiv preprint arXiv:1910.10261, 2019.\n\nContextnet: Improving convolutional neural networks for automatic speech recognition with global context. W Han, Z Zhang, Y Zhang, J Yu, C.-C Chiu, J Qin, A Gulati, R Pang, Y Wu, arXiv:2005.03191arXiv preprintW. Han, Z. Zhang, Y. Zhang, J. Yu, C.-C. Chiu, J. Qin, A. Gulati, R. Pang, and Y. Wu, \"Contextnet: Improving convolutional neural networks for automatic speech recognition with global context,\" arXiv preprint arXiv:2005.03191, 2020.\n\nDeep convolutional neural networks for lvcsr. T N Sainath, A Mohamed, B Kingsbury, B Ramabhadran, 2013 IEEE international conference on acoustics, speech and signal processing. IEEET. N. Sainath, A.-r. Mohamed, B. Kingsbury, and B. Ramabhad- ran, \"Deep convolutional neural networks for lvcsr,\" in 2013 IEEE international conference on acoustics, speech and signal process- ing. IEEE, 2013, pp. 8614-8618.\n\nConvolutional neural networks for speech recognition. O Hamid, A Mohamed, H Jiang, L Deng, G Penn, D Yu, IEEE/ACM Transactions on audio, speech, and language processing. 2210O. Abdel-Hamid, A.-r. Mohamed, H. Jiang, L. Deng, G. Penn, and D. Yu, \"Convolutional neural networks for speech recogni- tion,\" IEEE/ACM Transactions on audio, speech, and language processing, vol. 22, no. 10, pp. 1533-1545, 2014.\n\nSqueeze-and-excitation networks. J Hu, L Shen, G Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionJ. Hu, L. Shen, and G. Sun, \"Squeeze-and-excitation networks,\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 7132-7141.\n\nAttention augmented convolutional networks. I Bello, B Zoph, A Vaswani, J Shlens, Q V Le, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionI. Bello, B. Zoph, A. Vaswani, J. Shlens, and Q. V. Le, \"Attention augmented convolutional networks,\" in Proceedings of the IEEE International Conference on Computer Vision, 2019, pp. 3286- 3295.\n\nConvolutional self-attention networks. B Yang, L Wang, D Wong, L S Chao, Z Tu, arXiv:1904.03107arXiv preprintB. Yang, L. Wang, D. Wong, L. S. Chao, and Z. Tu, \"Convolu- tional self-attention networks,\" arXiv preprint arXiv:1904.03107, 2019.\n\nQanet: Combining local convolution with global self-attention for reading comprehension. A W Yu, D Dohan, M.-T Luong, R Zhao, K Chen, M Norouzi, Q V Le, arXiv:1804.09541arXiv preprintA. W. Yu, D. Dohan, M.-T. Luong, R. Zhao, K. Chen, M. Norouzi, and Q. V. Le, \"Qanet: Combining local convolution with global self-attention for reading comprehension,\" arXiv preprint arXiv:1804.09541, 2018.\n\nLite transformer with long-short range attention. Z Wu, Z Liu, J Lin, Y Lin, S Han, arXiv:2004.11886arXiv preprintZ. Wu, Z. Liu, J. Lin, Y. Lin, and S. Han, \"Lite transformer with long-short range attention,\" arXiv preprint arXiv:2004.11886, 2020.\n\nUnderstanding and improving transformer from a multi-particle dynamic system point of view. Y Lu, Z Li, D He, Z Sun, B Dong, T Qin, L Wang, T.-Y Liu, arXiv:1906.02762arXiv preprintY. Lu, Z. Li, D. He, Z. Sun, B. Dong, T. Qin, L. Wang, and T.-Y. Liu, \"Understanding and improving transformer from a multi-particle dynamic system point of view,\" arXiv preprint arXiv:1906.02762, 2019.\n\nS Karita, N Chen, T Hayashi, T Hori, H Inaguma, Z Jiang, M Someki, N E Y Soplin, R Yamamoto, X Wang, arXiv:1909.06317A comparative study on transformer vs rnn in speech applications. arXiv preprintS. Karita, N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang, M. Someki, N. E. Y. Soplin, R. Yamamoto, X. Wang et al., \"A comparative study on transformer vs rnn in speech applications,\" arXiv preprint arXiv:1909.06317, 2019.\n\nTransformer-xl: Attentive language models beyond a fixed-length context. Z Dai, Z Yang, Y Yang, J Carbonell, Q V Le, R Salakhutdinov, Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhut- dinov, \"Transformer-xl: Attentive language models beyond a fixed-length context,\" 2019.\n\nLearning deep transformer models for machine translation. Q Wang, B Li, T Xiao, J Zhu, C Li, D F Wong, L S Chao, Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics. the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational LinguisticsQ. Wang, B. Li, T. Xiao, J. Zhu, C. Li, D. F. Wong, and L. S. Chao, \"Learning deep transformer models for machine translation,\" in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Lin- guistics, Jul. 2019, pp. 1810-1822.\n\nTransformers without tears: Improving the normalization of self-attention. T Q Nguyen, J Salazar, arXiv:1910.05895arXiv preprintT. Q. Nguyen and J. Salazar, \"Transformers without tears: Improving the normalization of self-attention,\" arXiv preprint arXiv:1910.05895, 2019.\n\nLanguage modeling with gated convolutional networks. Y N Dauphin, A Fan, M Auli, D Grangier, Proceedings of the 34th International Conference on Machine Learning. the 34th International Conference on Machine Learning70Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier, \"Language modeling with gated convolutional networks,\" in Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017, pp. 933-941.\n\nSpeech-transformer: a no-recurrence sequence-to-sequence model for speech recognition. L Dong, S Xu, B Xu, 2018 IEEE International Conference on Acoustics, Speech and Signal Processing. L. Dong, S. Xu, and B. Xu, \"Speech-transformer: a no-recurrence sequence-to-sequence model for speech recognition,\" in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5884-5888.\n\nSearching for activation functions. P Ramachandran, B Zoph, Q V Le, arXiv:1710.05941arXiv preprintP. Ramachandran, B. Zoph, and Q. V. Le, \"Searching for activa- tion functions,\" arXiv preprint arXiv:1710.05941, 2017.\n\nLibrispeech: an asr corpus based on public domain audio books. V Panayotov, G Chen, D Povey, S Khudanpur, 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEEV. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \"Lib- rispeech: an asr corpus based on public domain audio books,\" in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 5206-5210.\n\nSpecaugment: A simple data augmentation method for automatic speech recognition. D S Park, W Chan, Y Zhang, C.-C Chiu, B Zoph, E D Cubuk, Q V Le, arXiv:1904.08779arXiv preprintD. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le, \"Specaugment: A simple data augmen- tation method for automatic speech recognition,\" arXiv preprint arXiv:1904.08779, 2019.\n\nD S Park, Y Zhang, C.-C Chiu, Y Chen, B Li, W Chan, Q V Le, Y Wu, arXiv:1912.05533Specaugment on large scale datasets. arXiv preprintD. S. Park, Y. Zhang, C.-C. Chiu, Y. Chen, B. Li, W. Chan, Q. V. Le, and Y. Wu, \"Specaugment on large scale datasets,\" arXiv preprint arXiv:1912.05533, 2019.\n\nDropout: A simple way to prevent neural networks from overfitting. N Srivastava, G Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov, Journal of Machine Learning Research. 1556N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, \"Dropout: A simple way to prevent neural net- works from overfitting,\" Journal of Machine Learning Research, vol. 15, no. 56, pp. 1929-1958, 2014.\n\nAn analysis of noise in recurrent neural networks: convergence and generalization. K.-C Jim, C L Giles, B G Horne, IEEE Transactions on neural networks. 76K.-C. Jim, C. L. Giles, and B. G. Horne, \"An analysis of noise in recurrent neural networks: convergence and generalization,\" IEEE Transactions on neural networks, vol. 7, no. 6, pp. 1424- 1438, 1996.\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980arXiv preprintD. P. Kingma and J. Ba, \"Adam: A method for stochastic opti- mization,\" arXiv preprint arXiv:1412.6980, 2014.\n\nLingvo: a modular and scalable framework for sequence-to-sequence modeling. J Shen, P Nguyen, Y Wu, Z Chen, J. Shen, P. Nguyen, Y. Wu, Z. Chen, and et al., \"Lingvo: a modu- lar and scalable framework for sequence-to-sequence modeling,\" 2019.\n\nTransformerbased acoustic modeling for hybrid speech recognition. Y Wang, A Mohamed, D Le, C Liu, A Xiao, J Mahadeokar, H Huang, A Tjandra, X Zhang, F Zhang, arXiv:1910.09799arXiv preprintY. Wang, A. Mohamed, D. Le, C. Liu, A. Xiao, J. Mahadeokar, H. Huang, A. Tjandra, X. Zhang, F. Zhang et al., \"Transformer- based acoustic modeling for hybrid speech recognition,\" arXiv preprint arXiv:1910.09799, 2019.\n\nEnd-toend asr: from supervised to semi-supervised learning with modern architectures. G Synnaeve, Q Xu, J Kahn, T Likhomanenko, E Grave, V Pratap, A Sriram, V Liptchinsky, R Collobert, G. Synnaeve, Q. Xu, J. Kahn, T. Likhomanenko, E. Grave, V. Pratap, A. Sriram, V. Liptchinsky, and R. Collobert, \"End-to- end asr: from supervised to semi-supervised learning with modern architectures,\" 2019.\n\nPay less attention with lightweight and dynamic convolutions. F Wu, A Fan, A Baevski, Y N Dauphin, M Auli, arXiv:1901.10430arXiv preprintF. Wu, A. Fan, A. Baevski, Y. N. Dauphin, and M. Auli, \"Pay less attention with lightweight and dynamic convolutions,\" arXiv preprint arXiv:1901.10430, 2019.\n", "annotations": {"author": "[{\"end\":120,\"start\":71},{\"end\":164,\"start\":121},{\"end\":218,\"start\":165},{\"end\":261,\"start\":219},{\"end\":303,\"start\":262},{\"end\":347,\"start\":304},{\"end\":387,\"start\":348},{\"end\":430,\"start\":388},{\"end\":460,\"start\":431},{\"end\":504,\"start\":461},{\"end\":548,\"start\":505}]", "publisher": null, "author_last_name": "[{\"end\":83,\"start\":77},{\"end\":130,\"start\":127},{\"end\":181,\"start\":177},{\"end\":230,\"start\":224},{\"end\":270,\"start\":265},{\"end\":313,\"start\":311},{\"end\":355,\"start\":352},{\"end\":398,\"start\":394},{\"end\":446,\"start\":441},{\"end\":471,\"start\":469},{\"end\":517,\"start\":513}]", "author_first_name": "[{\"end\":76,\"start\":71},{\"end\":126,\"start\":121},{\"end\":176,\"start\":165},{\"end\":223,\"start\":219},{\"end\":264,\"start\":262},{\"end\":310,\"start\":304},{\"end\":351,\"start\":348},{\"end\":393,\"start\":388},{\"end\":440,\"start\":431},{\"end\":468,\"start\":461},{\"end\":512,\"start\":505}]", "author_affiliation": "[{\"end\":119,\"start\":108},{\"end\":163,\"start\":152},{\"end\":217,\"start\":206},{\"end\":260,\"start\":249},{\"end\":302,\"start\":291},{\"end\":346,\"start\":335},{\"end\":386,\"start\":375},{\"end\":429,\"start\":418},{\"end\":459,\"start\":448},{\"end\":503,\"start\":492},{\"end\":547,\"start\":536}]", "title": "[{\"end\":68,\"start\":1},{\"end\":616,\"start\":549}]", "venue": null, "abstract": "[{\"end\":1732,\"start\":710}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1946,\"start\":1943},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1948,\"start\":1946},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1950,\"start\":1948},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1952,\"start\":1950},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2035,\"start\":2032},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2103,\"start\":2100},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2105,\"start\":2103},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2317,\"start\":2314},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2319,\"start\":2317},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2322,\"start\":2319},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2325,\"start\":2322},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2328,\"start\":2325},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3107,\"start\":3103},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3153,\"start\":3149},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3697,\"start\":3693},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3840,\"start\":3836},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3843,\"start\":3840},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3958,\"start\":3954},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4679,\"start\":4675},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4682,\"start\":4679},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4979,\"start\":4976},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5527,\"start\":5523},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5668,\"start\":5665},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6447,\"start\":6444},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6450,\"start\":6447},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6951,\"start\":6947},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7234,\"start\":7230},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7237,\"start\":7234},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7633,\"start\":7629},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8003,\"start\":8000},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8294,\"start\":8291},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8297,\"start\":8294},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8337,\"start\":8333},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8340,\"start\":8337},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8479,\"start\":8475},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8822,\"start\":8818},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10324,\"start\":10320},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10579,\"start\":10575},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10582,\"start\":10579},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":11208,\"start\":11204},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":11376,\"start\":11373},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11379,\"start\":11376},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11573,\"start\":11569},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":11655,\"start\":11652},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":12166,\"start\":12162},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12312,\"start\":12308},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":12340,\"start\":12337},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12359,\"start\":12356},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":13950,\"start\":13946},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":14383,\"start\":14379},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19596,\"start\":19592}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":16729,\"start\":16496},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":16927,\"start\":16730},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":17365,\"start\":16928},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":18459,\"start\":17366},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":19180,\"start\":18460},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":19799,\"start\":19181},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":20303,\"start\":19800},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":20585,\"start\":20304},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":20858,\"start\":20586}]", "paragraph": "[{\"end\":2415,\"start\":1748},{\"end\":3332,\"start\":2417},{\"end\":3638,\"start\":3334},{\"end\":4198,\"start\":3640},{\"end\":5284,\"start\":4200},{\"end\":5702,\"start\":5286},{\"end\":5848,\"start\":5704},{\"end\":6160,\"start\":5850},{\"end\":6451,\"start\":6182},{\"end\":6802,\"start\":6453},{\"end\":7369,\"start\":6841},{\"end\":7594,\"start\":7411},{\"end\":7932,\"start\":7617},{\"end\":8582,\"start\":7956},{\"end\":8765,\"start\":8602},{\"end\":9248,\"start\":8767},{\"end\":10247,\"start\":9352},{\"end\":10761,\"start\":10270},{\"end\":11765,\"start\":10786},{\"end\":12875,\"start\":11767},{\"end\":13648,\"start\":12961},{\"end\":14609,\"start\":13704},{\"end\":15048,\"start\":14642},{\"end\":15539,\"start\":15078},{\"end\":15994,\"start\":15568},{\"end\":16495,\"start\":16009}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9351,\"start\":9249}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":11119,\"start\":11112},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":12175,\"start\":12168},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":13327,\"start\":13320},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":14496,\"start\":14489},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":14954,\"start\":14947},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":15537,\"start\":15530},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":15896,\"start\":15889}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1746,\"start\":1734},{\"attributes\":{\"n\":\"2.\"},\"end\":6180,\"start\":6163},{\"attributes\":{\"n\":\"2.1.\"},\"end\":6839,\"start\":6805},{\"end\":7381,\"start\":7372},{\"end\":7409,\"start\":7384},{\"attributes\":{\"n\":\"2.2.\"},\"end\":7615,\"start\":7597},{\"attributes\":{\"n\":\"2.3.\"},\"end\":7954,\"start\":7935},{\"attributes\":{\"n\":\"2.4.\"},\"end\":8600,\"start\":8585},{\"attributes\":{\"n\":\"3.\"},\"end\":10261,\"start\":10250},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10268,\"start\":10264},{\"attributes\":{\"n\":\"3.2.\"},\"end\":10784,\"start\":10764},{\"attributes\":{\"n\":\"3.3.\"},\"end\":12900,\"start\":12878},{\"attributes\":{\"n\":\"3.4.\"},\"end\":12919,\"start\":12903},{\"attributes\":{\"n\":\"3.4.1.\"},\"end\":12959,\"start\":12922},{\"attributes\":{\"n\":\"3.4.2.\"},\"end\":13702,\"start\":13651},{\"attributes\":{\"n\":\"3.4.3.\"},\"end\":14640,\"start\":14612},{\"attributes\":{\"n\":\"3.4.4.\"},\"end\":15076,\"start\":15051},{\"attributes\":{\"n\":\"3.4.5.\"},\"end\":15566,\"start\":15542},{\"attributes\":{\"n\":\"4.\"},\"end\":16007,\"start\":15997},{\"end\":16507,\"start\":16497},{\"end\":16938,\"start\":16929},{\"end\":17376,\"start\":17367},{\"end\":18470,\"start\":18461},{\"end\":19191,\"start\":19182},{\"end\":19810,\"start\":19801},{\"end\":20314,\"start\":20305},{\"end\":20596,\"start\":20587}]", "table": "[{\"end\":16927,\"start\":16796},{\"end\":17365,\"start\":17107},{\"end\":18459,\"start\":17388},{\"end\":19180,\"start\":18496},{\"end\":19799,\"start\":19597},{\"end\":20303,\"start\":20123},{\"end\":20585,\"start\":20385},{\"end\":20858,\"start\":20651}]", "figure_caption": "[{\"end\":16729,\"start\":16509},{\"end\":16796,\"start\":16732},{\"end\":17107,\"start\":16940},{\"end\":17388,\"start\":17378},{\"end\":18496,\"start\":18472},{\"end\":19597,\"start\":19193},{\"end\":20123,\"start\":19812},{\"end\":20385,\"start\":20316},{\"end\":20651,\"start\":20598}]", "figure_ref": "[{\"end\":3398,\"start\":3390},{\"end\":4824,\"start\":4818},{\"end\":4988,\"start\":4980},{\"end\":6333,\"start\":6325},{\"end\":7312,\"start\":7304},{\"end\":7459,\"start\":7451},{\"end\":7897,\"start\":7889},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8539,\"start\":8531},{\"end\":8764,\"start\":8756}]", "bib_author_first_name": "[{\"end\":20933,\"start\":20929},{\"end\":20941,\"start\":20940},{\"end\":20943,\"start\":20942},{\"end\":20954,\"start\":20953},{\"end\":20960,\"start\":20959},{\"end\":20976,\"start\":20975},{\"end\":20986,\"start\":20985},{\"end\":20994,\"start\":20993},{\"end\":21004,\"start\":21003},{\"end\":21006,\"start\":21005},{\"end\":21015,\"start\":21014},{\"end\":21022,\"start\":21021},{\"end\":21539,\"start\":21538},{\"end\":21546,\"start\":21545},{\"end\":21553,\"start\":21552},{\"end\":21954,\"start\":21953},{\"end\":21960,\"start\":21959},{\"end\":21962,\"start\":21961},{\"end\":21973,\"start\":21972},{\"end\":21989,\"start\":21988},{\"end\":21999,\"start\":21998},{\"end\":22010,\"start\":22009},{\"end\":22018,\"start\":22017},{\"end\":22028,\"start\":22027},{\"end\":22038,\"start\":22037},{\"end\":22044,\"start\":22043},{\"end\":22052,\"start\":22051},{\"end\":22061,\"start\":22060},{\"end\":22071,\"start\":22070},{\"end\":22084,\"start\":22083},{\"end\":22090,\"start\":22089},{\"end\":22100,\"start\":22099},{\"end\":22102,\"start\":22101},{\"end\":22109,\"start\":22108},{\"end\":22121,\"start\":22117},{\"end\":22130,\"start\":22129},{\"end\":22137,\"start\":22136},{\"end\":22582,\"start\":22581},{\"end\":22584,\"start\":22583},{\"end\":22595,\"start\":22594},{\"end\":22601,\"start\":22600},{\"end\":22607,\"start\":22606},{\"end\":22620,\"start\":22619},{\"end\":22628,\"start\":22627},{\"end\":22640,\"start\":22639},{\"end\":22649,\"start\":22648},{\"end\":22655,\"start\":22654},{\"end\":22666,\"start\":22665},{\"end\":22922,\"start\":22921},{\"end\":23150,\"start\":23149},{\"end\":23161,\"start\":23160},{\"end\":23172,\"start\":23171},{\"end\":23182,\"start\":23181},{\"end\":23195,\"start\":23194},{\"end\":23204,\"start\":23203},{\"end\":23206,\"start\":23205},{\"end\":23215,\"start\":23214},{\"end\":23225,\"start\":23224},{\"end\":23481,\"start\":23480},{\"end\":23490,\"start\":23489},{\"end\":23496,\"start\":23495},{\"end\":23503,\"start\":23502},{\"end\":23515,\"start\":23514},{\"end\":23528,\"start\":23527},{\"end\":23535,\"start\":23534},{\"end\":24017,\"start\":24016},{\"end\":24023,\"start\":24022},{\"end\":24036,\"start\":24035},{\"end\":24048,\"start\":24047},{\"end\":24057,\"start\":24056},{\"end\":24069,\"start\":24068},{\"end\":24071,\"start\":24070},{\"end\":24080,\"start\":24079},{\"end\":24090,\"start\":24089},{\"end\":24092,\"start\":24091},{\"end\":24424,\"start\":24423},{\"end\":24434,\"start\":24433},{\"end\":24445,\"start\":24444},{\"end\":24457,\"start\":24456},{\"end\":24466,\"start\":24465},{\"end\":24478,\"start\":24477},{\"end\":24491,\"start\":24490},{\"end\":24500,\"start\":24499},{\"end\":24506,\"start\":24505},{\"end\":24889,\"start\":24888},{\"end\":24896,\"start\":24895},{\"end\":24905,\"start\":24904},{\"end\":24914,\"start\":24913},{\"end\":24923,\"start\":24919},{\"end\":24931,\"start\":24930},{\"end\":24938,\"start\":24937},{\"end\":24948,\"start\":24947},{\"end\":24956,\"start\":24955},{\"end\":25272,\"start\":25271},{\"end\":25274,\"start\":25273},{\"end\":25285,\"start\":25284},{\"end\":25296,\"start\":25295},{\"end\":25309,\"start\":25308},{\"end\":25687,\"start\":25686},{\"end\":25696,\"start\":25695},{\"end\":25707,\"start\":25706},{\"end\":25716,\"start\":25715},{\"end\":25724,\"start\":25723},{\"end\":25732,\"start\":25731},{\"end\":26072,\"start\":26071},{\"end\":26078,\"start\":26077},{\"end\":26086,\"start\":26085},{\"end\":26445,\"start\":26444},{\"end\":26454,\"start\":26453},{\"end\":26462,\"start\":26461},{\"end\":26473,\"start\":26472},{\"end\":26483,\"start\":26482},{\"end\":26485,\"start\":26484},{\"end\":26848,\"start\":26847},{\"end\":26856,\"start\":26855},{\"end\":26864,\"start\":26863},{\"end\":26872,\"start\":26871},{\"end\":26874,\"start\":26873},{\"end\":26882,\"start\":26881},{\"end\":27140,\"start\":27139},{\"end\":27142,\"start\":27141},{\"end\":27148,\"start\":27147},{\"end\":27160,\"start\":27156},{\"end\":27169,\"start\":27168},{\"end\":27177,\"start\":27176},{\"end\":27185,\"start\":27184},{\"end\":27196,\"start\":27195},{\"end\":27198,\"start\":27197},{\"end\":27492,\"start\":27491},{\"end\":27498,\"start\":27497},{\"end\":27505,\"start\":27504},{\"end\":27512,\"start\":27511},{\"end\":27519,\"start\":27518},{\"end\":27783,\"start\":27782},{\"end\":27789,\"start\":27788},{\"end\":27795,\"start\":27794},{\"end\":27801,\"start\":27800},{\"end\":27808,\"start\":27807},{\"end\":27816,\"start\":27815},{\"end\":27823,\"start\":27822},{\"end\":27834,\"start\":27830},{\"end\":28075,\"start\":28074},{\"end\":28085,\"start\":28084},{\"end\":28093,\"start\":28092},{\"end\":28104,\"start\":28103},{\"end\":28112,\"start\":28111},{\"end\":28123,\"start\":28122},{\"end\":28132,\"start\":28131},{\"end\":28142,\"start\":28141},{\"end\":28146,\"start\":28143},{\"end\":28156,\"start\":28155},{\"end\":28168,\"start\":28167},{\"end\":28573,\"start\":28572},{\"end\":28580,\"start\":28579},{\"end\":28588,\"start\":28587},{\"end\":28596,\"start\":28595},{\"end\":28609,\"start\":28608},{\"end\":28611,\"start\":28610},{\"end\":28617,\"start\":28616},{\"end\":28848,\"start\":28847},{\"end\":28856,\"start\":28855},{\"end\":28862,\"start\":28861},{\"end\":28870,\"start\":28869},{\"end\":28877,\"start\":28876},{\"end\":28883,\"start\":28882},{\"end\":28885,\"start\":28884},{\"end\":28893,\"start\":28892},{\"end\":28895,\"start\":28894},{\"end\":29517,\"start\":29516},{\"end\":29519,\"start\":29518},{\"end\":29529,\"start\":29528},{\"end\":29769,\"start\":29768},{\"end\":29771,\"start\":29770},{\"end\":29782,\"start\":29781},{\"end\":29789,\"start\":29788},{\"end\":29797,\"start\":29796},{\"end\":30239,\"start\":30238},{\"end\":30247,\"start\":30246},{\"end\":30253,\"start\":30252},{\"end\":30609,\"start\":30608},{\"end\":30625,\"start\":30624},{\"end\":30633,\"start\":30632},{\"end\":30635,\"start\":30634},{\"end\":30854,\"start\":30853},{\"end\":30867,\"start\":30866},{\"end\":30875,\"start\":30874},{\"end\":30884,\"start\":30883},{\"end\":31307,\"start\":31306},{\"end\":31309,\"start\":31308},{\"end\":31317,\"start\":31316},{\"end\":31325,\"start\":31324},{\"end\":31337,\"start\":31333},{\"end\":31345,\"start\":31344},{\"end\":31353,\"start\":31352},{\"end\":31355,\"start\":31354},{\"end\":31364,\"start\":31363},{\"end\":31366,\"start\":31365},{\"end\":31606,\"start\":31605},{\"end\":31608,\"start\":31607},{\"end\":31616,\"start\":31615},{\"end\":31628,\"start\":31624},{\"end\":31636,\"start\":31635},{\"end\":31644,\"start\":31643},{\"end\":31650,\"start\":31649},{\"end\":31658,\"start\":31657},{\"end\":31660,\"start\":31659},{\"end\":31666,\"start\":31665},{\"end\":31965,\"start\":31964},{\"end\":31979,\"start\":31978},{\"end\":31989,\"start\":31988},{\"end\":32003,\"start\":32002},{\"end\":32016,\"start\":32015},{\"end\":32386,\"start\":32382},{\"end\":32393,\"start\":32392},{\"end\":32395,\"start\":32394},{\"end\":32404,\"start\":32403},{\"end\":32406,\"start\":32405},{\"end\":32701,\"start\":32700},{\"end\":32703,\"start\":32702},{\"end\":32713,\"start\":32712},{\"end\":32935,\"start\":32934},{\"end\":32943,\"start\":32942},{\"end\":32953,\"start\":32952},{\"end\":32959,\"start\":32958},{\"end\":33168,\"start\":33167},{\"end\":33176,\"start\":33175},{\"end\":33187,\"start\":33186},{\"end\":33193,\"start\":33192},{\"end\":33200,\"start\":33199},{\"end\":33208,\"start\":33207},{\"end\":33222,\"start\":33221},{\"end\":33231,\"start\":33230},{\"end\":33242,\"start\":33241},{\"end\":33251,\"start\":33250},{\"end\":33595,\"start\":33594},{\"end\":33607,\"start\":33606},{\"end\":33613,\"start\":33612},{\"end\":33621,\"start\":33620},{\"end\":33637,\"start\":33636},{\"end\":33646,\"start\":33645},{\"end\":33656,\"start\":33655},{\"end\":33666,\"start\":33665},{\"end\":33681,\"start\":33680},{\"end\":33965,\"start\":33964},{\"end\":33971,\"start\":33970},{\"end\":33978,\"start\":33977},{\"end\":33989,\"start\":33988},{\"end\":33991,\"start\":33990},{\"end\":34002,\"start\":34001}]", "bib_author_last_name": "[{\"end\":20938,\"start\":20934},{\"end\":20951,\"start\":20944},{\"end\":20957,\"start\":20955},{\"end\":20973,\"start\":20961},{\"end\":20983,\"start\":20977},{\"end\":20991,\"start\":20987},{\"end\":21001,\"start\":20995},{\"end\":21012,\"start\":21007},{\"end\":21019,\"start\":21016},{\"end\":21029,\"start\":21023},{\"end\":21543,\"start\":21540},{\"end\":21550,\"start\":21547},{\"end\":21566,\"start\":21554},{\"end\":21957,\"start\":21955},{\"end\":21970,\"start\":21963},{\"end\":21986,\"start\":21974},{\"end\":21996,\"start\":21990},{\"end\":22007,\"start\":22000},{\"end\":22015,\"start\":22011},{\"end\":22025,\"start\":22019},{\"end\":22035,\"start\":22029},{\"end\":22041,\"start\":22039},{\"end\":22049,\"start\":22045},{\"end\":22058,\"start\":22053},{\"end\":22068,\"start\":22062},{\"end\":22081,\"start\":22072},{\"end\":22087,\"start\":22085},{\"end\":22097,\"start\":22091},{\"end\":22106,\"start\":22103},{\"end\":22115,\"start\":22110},{\"end\":22127,\"start\":22122},{\"end\":22134,\"start\":22131},{\"end\":22148,\"start\":22138},{\"end\":22592,\"start\":22585},{\"end\":22598,\"start\":22596},{\"end\":22604,\"start\":22602},{\"end\":22617,\"start\":22608},{\"end\":22625,\"start\":22621},{\"end\":22637,\"start\":22629},{\"end\":22646,\"start\":22641},{\"end\":22652,\"start\":22650},{\"end\":22663,\"start\":22656},{\"end\":22671,\"start\":22667},{\"end\":22929,\"start\":22923},{\"end\":23158,\"start\":23151},{\"end\":23169,\"start\":23162},{\"end\":23179,\"start\":23173},{\"end\":23192,\"start\":23183},{\"end\":23201,\"start\":23196},{\"end\":23212,\"start\":23207},{\"end\":23222,\"start\":23216},{\"end\":23236,\"start\":23226},{\"end\":23487,\"start\":23482},{\"end\":23493,\"start\":23491},{\"end\":23500,\"start\":23497},{\"end\":23512,\"start\":23504},{\"end\":23525,\"start\":23516},{\"end\":23532,\"start\":23529},{\"end\":23541,\"start\":23536},{\"end\":24020,\"start\":24018},{\"end\":24033,\"start\":24024},{\"end\":24045,\"start\":24037},{\"end\":24054,\"start\":24049},{\"end\":24066,\"start\":24058},{\"end\":24077,\"start\":24072},{\"end\":24087,\"start\":24081},{\"end\":24098,\"start\":24093},{\"end\":24431,\"start\":24425},{\"end\":24442,\"start\":24435},{\"end\":24454,\"start\":24446},{\"end\":24463,\"start\":24458},{\"end\":24475,\"start\":24467},{\"end\":24488,\"start\":24479},{\"end\":24497,\"start\":24492},{\"end\":24503,\"start\":24501},{\"end\":24512,\"start\":24507},{\"end\":24893,\"start\":24890},{\"end\":24902,\"start\":24897},{\"end\":24911,\"start\":24906},{\"end\":24917,\"start\":24915},{\"end\":24928,\"start\":24924},{\"end\":24935,\"start\":24932},{\"end\":24945,\"start\":24939},{\"end\":24953,\"start\":24949},{\"end\":24959,\"start\":24957},{\"end\":25282,\"start\":25275},{\"end\":25293,\"start\":25286},{\"end\":25306,\"start\":25297},{\"end\":25321,\"start\":25310},{\"end\":25693,\"start\":25688},{\"end\":25704,\"start\":25697},{\"end\":25713,\"start\":25708},{\"end\":25721,\"start\":25717},{\"end\":25729,\"start\":25725},{\"end\":25735,\"start\":25733},{\"end\":26075,\"start\":26073},{\"end\":26083,\"start\":26079},{\"end\":26090,\"start\":26087},{\"end\":26451,\"start\":26446},{\"end\":26459,\"start\":26455},{\"end\":26470,\"start\":26463},{\"end\":26480,\"start\":26474},{\"end\":26488,\"start\":26486},{\"end\":26853,\"start\":26849},{\"end\":26861,\"start\":26857},{\"end\":26869,\"start\":26865},{\"end\":26879,\"start\":26875},{\"end\":26885,\"start\":26883},{\"end\":27145,\"start\":27143},{\"end\":27154,\"start\":27149},{\"end\":27166,\"start\":27161},{\"end\":27174,\"start\":27170},{\"end\":27182,\"start\":27178},{\"end\":27193,\"start\":27186},{\"end\":27201,\"start\":27199},{\"end\":27495,\"start\":27493},{\"end\":27502,\"start\":27499},{\"end\":27509,\"start\":27506},{\"end\":27516,\"start\":27513},{\"end\":27523,\"start\":27520},{\"end\":27786,\"start\":27784},{\"end\":27792,\"start\":27790},{\"end\":27798,\"start\":27796},{\"end\":27805,\"start\":27802},{\"end\":27813,\"start\":27809},{\"end\":27820,\"start\":27817},{\"end\":27828,\"start\":27824},{\"end\":27838,\"start\":27835},{\"end\":28082,\"start\":28076},{\"end\":28090,\"start\":28086},{\"end\":28101,\"start\":28094},{\"end\":28109,\"start\":28105},{\"end\":28120,\"start\":28113},{\"end\":28129,\"start\":28124},{\"end\":28139,\"start\":28133},{\"end\":28153,\"start\":28147},{\"end\":28165,\"start\":28157},{\"end\":28173,\"start\":28169},{\"end\":28577,\"start\":28574},{\"end\":28585,\"start\":28581},{\"end\":28593,\"start\":28589},{\"end\":28606,\"start\":28597},{\"end\":28614,\"start\":28612},{\"end\":28631,\"start\":28618},{\"end\":28853,\"start\":28849},{\"end\":28859,\"start\":28857},{\"end\":28867,\"start\":28863},{\"end\":28874,\"start\":28871},{\"end\":28880,\"start\":28878},{\"end\":28890,\"start\":28886},{\"end\":28900,\"start\":28896},{\"end\":29526,\"start\":29520},{\"end\":29537,\"start\":29530},{\"end\":29779,\"start\":29772},{\"end\":29786,\"start\":29783},{\"end\":29794,\"start\":29790},{\"end\":29806,\"start\":29798},{\"end\":30244,\"start\":30240},{\"end\":30250,\"start\":30248},{\"end\":30256,\"start\":30254},{\"end\":30622,\"start\":30610},{\"end\":30630,\"start\":30626},{\"end\":30638,\"start\":30636},{\"end\":30864,\"start\":30855},{\"end\":30872,\"start\":30868},{\"end\":30881,\"start\":30876},{\"end\":30894,\"start\":30885},{\"end\":31314,\"start\":31310},{\"end\":31322,\"start\":31318},{\"end\":31331,\"start\":31326},{\"end\":31342,\"start\":31338},{\"end\":31350,\"start\":31346},{\"end\":31361,\"start\":31356},{\"end\":31369,\"start\":31367},{\"end\":31613,\"start\":31609},{\"end\":31622,\"start\":31617},{\"end\":31633,\"start\":31629},{\"end\":31641,\"start\":31637},{\"end\":31647,\"start\":31645},{\"end\":31655,\"start\":31651},{\"end\":31663,\"start\":31661},{\"end\":31669,\"start\":31667},{\"end\":31976,\"start\":31966},{\"end\":31986,\"start\":31980},{\"end\":32000,\"start\":31990},{\"end\":32013,\"start\":32004},{\"end\":32030,\"start\":32017},{\"end\":32390,\"start\":32387},{\"end\":32401,\"start\":32396},{\"end\":32412,\"start\":32407},{\"end\":32710,\"start\":32704},{\"end\":32716,\"start\":32714},{\"end\":32940,\"start\":32936},{\"end\":32950,\"start\":32944},{\"end\":32956,\"start\":32954},{\"end\":32964,\"start\":32960},{\"end\":33173,\"start\":33169},{\"end\":33184,\"start\":33177},{\"end\":33190,\"start\":33188},{\"end\":33197,\"start\":33194},{\"end\":33205,\"start\":33201},{\"end\":33219,\"start\":33209},{\"end\":33228,\"start\":33223},{\"end\":33239,\"start\":33232},{\"end\":33248,\"start\":33243},{\"end\":33257,\"start\":33252},{\"end\":33604,\"start\":33596},{\"end\":33610,\"start\":33608},{\"end\":33618,\"start\":33614},{\"end\":33634,\"start\":33622},{\"end\":33643,\"start\":33638},{\"end\":33653,\"start\":33647},{\"end\":33663,\"start\":33657},{\"end\":33678,\"start\":33667},{\"end\":33691,\"start\":33682},{\"end\":33968,\"start\":33966},{\"end\":33975,\"start\":33972},{\"end\":33986,\"start\":33979},{\"end\":33999,\"start\":33992},{\"end\":34007,\"start\":34003}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":206742954},\"end\":21431,\"start\":20860},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":10462962},\"end\":21891,\"start\":21433},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":53669807},\"end\":22477,\"start\":21893},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":214714370},\"end\":22919,\"start\":22479},{\"attributes\":{\"doi\":\"arXiv:1211.3711\",\"id\":\"b4\"},\"end\":23120,\"start\":22921},{\"attributes\":{\"id\":\"b5\"},\"end\":23374,\"start\":23122},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":211066611},\"end\":23955,\"start\":23376},{\"attributes\":{\"doi\":\"arXiv:1904.03288\",\"id\":\"b7\"},\"end\":24331,\"start\":23957},{\"attributes\":{\"doi\":\"arXiv:1910.10261\",\"id\":\"b8\"},\"end\":24780,\"start\":24333},{\"attributes\":{\"doi\":\"arXiv:2005.03191\",\"id\":\"b9\"},\"end\":25223,\"start\":24782},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":13816461},\"end\":25630,\"start\":25225},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":206602362},\"end\":26036,\"start\":25632},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":140309863},\"end\":26398,\"start\":26038},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":127951164},\"end\":26806,\"start\":26400},{\"attributes\":{\"doi\":\"arXiv:1904.03107\",\"id\":\"b14\"},\"end\":27048,\"start\":26808},{\"attributes\":{\"doi\":\"arXiv:1804.09541\",\"id\":\"b15\"},\"end\":27439,\"start\":27050},{\"attributes\":{\"doi\":\"arXiv:2004.11886\",\"id\":\"b16\"},\"end\":27688,\"start\":27441},{\"attributes\":{\"doi\":\"arXiv:1906.02762\",\"id\":\"b17\"},\"end\":28072,\"start\":27690},{\"attributes\":{\"doi\":\"arXiv:1909.06317\",\"id\":\"b18\"},\"end\":28497,\"start\":28074},{\"attributes\":{\"id\":\"b19\"},\"end\":28787,\"start\":28499},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":174799399},\"end\":29439,\"start\":28789},{\"attributes\":{\"doi\":\"arXiv:1910.05895\",\"id\":\"b21\"},\"end\":29713,\"start\":29441},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":16119010},\"end\":30149,\"start\":29715},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":52287921},\"end\":30570,\"start\":30151},{\"attributes\":{\"doi\":\"arXiv:1710.05941\",\"id\":\"b24\"},\"end\":30788,\"start\":30572},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":2191379},\"end\":31223,\"start\":30790},{\"attributes\":{\"doi\":\"arXiv:1904.08779\",\"id\":\"b26\"},\"end\":31603,\"start\":31225},{\"attributes\":{\"doi\":\"arXiv:1912.05533\",\"id\":\"b27\"},\"end\":31895,\"start\":31605},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":6844431},\"end\":32297,\"start\":31897},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":15729978},\"end\":32654,\"start\":32299},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b30\"},\"end\":32856,\"start\":32656},{\"attributes\":{\"id\":\"b31\"},\"end\":33099,\"start\":32858},{\"attributes\":{\"doi\":\"arXiv:1910.09799\",\"id\":\"b32\"},\"end\":33506,\"start\":33101},{\"attributes\":{\"id\":\"b33\"},\"end\":33900,\"start\":33508},{\"attributes\":{\"doi\":\"arXiv:1901.10430\",\"id\":\"b34\"},\"end\":34196,\"start\":33902}]", "bib_title": "[{\"end\":20927,\"start\":20860},{\"end\":21536,\"start\":21433},{\"end\":21951,\"start\":21893},{\"end\":22579,\"start\":22479},{\"end\":23478,\"start\":23376},{\"end\":25269,\"start\":25225},{\"end\":25684,\"start\":25632},{\"end\":26069,\"start\":26038},{\"end\":26442,\"start\":26400},{\"end\":28845,\"start\":28789},{\"end\":29766,\"start\":29715},{\"end\":30236,\"start\":30151},{\"end\":30851,\"start\":30790},{\"end\":31962,\"start\":31897},{\"end\":32380,\"start\":32299}]", "bib_author": "[{\"end\":20940,\"start\":20929},{\"end\":20953,\"start\":20940},{\"end\":20959,\"start\":20953},{\"end\":20975,\"start\":20959},{\"end\":20985,\"start\":20975},{\"end\":20993,\"start\":20985},{\"end\":21003,\"start\":20993},{\"end\":21014,\"start\":21003},{\"end\":21021,\"start\":21014},{\"end\":21031,\"start\":21021},{\"end\":21545,\"start\":21538},{\"end\":21552,\"start\":21545},{\"end\":21568,\"start\":21552},{\"end\":21959,\"start\":21953},{\"end\":21972,\"start\":21959},{\"end\":21988,\"start\":21972},{\"end\":21998,\"start\":21988},{\"end\":22009,\"start\":21998},{\"end\":22017,\"start\":22009},{\"end\":22027,\"start\":22017},{\"end\":22037,\"start\":22027},{\"end\":22043,\"start\":22037},{\"end\":22051,\"start\":22043},{\"end\":22060,\"start\":22051},{\"end\":22070,\"start\":22060},{\"end\":22083,\"start\":22070},{\"end\":22089,\"start\":22083},{\"end\":22099,\"start\":22089},{\"end\":22108,\"start\":22099},{\"end\":22117,\"start\":22108},{\"end\":22129,\"start\":22117},{\"end\":22136,\"start\":22129},{\"end\":22150,\"start\":22136},{\"end\":22594,\"start\":22581},{\"end\":22600,\"start\":22594},{\"end\":22606,\"start\":22600},{\"end\":22619,\"start\":22606},{\"end\":22627,\"start\":22619},{\"end\":22639,\"start\":22627},{\"end\":22648,\"start\":22639},{\"end\":22654,\"start\":22648},{\"end\":22665,\"start\":22654},{\"end\":22673,\"start\":22665},{\"end\":22931,\"start\":22921},{\"end\":23160,\"start\":23149},{\"end\":23171,\"start\":23160},{\"end\":23181,\"start\":23171},{\"end\":23194,\"start\":23181},{\"end\":23203,\"start\":23194},{\"end\":23214,\"start\":23203},{\"end\":23224,\"start\":23214},{\"end\":23238,\"start\":23224},{\"end\":23489,\"start\":23480},{\"end\":23495,\"start\":23489},{\"end\":23502,\"start\":23495},{\"end\":23514,\"start\":23502},{\"end\":23527,\"start\":23514},{\"end\":23534,\"start\":23527},{\"end\":23543,\"start\":23534},{\"end\":24022,\"start\":24016},{\"end\":24035,\"start\":24022},{\"end\":24047,\"start\":24035},{\"end\":24056,\"start\":24047},{\"end\":24068,\"start\":24056},{\"end\":24079,\"start\":24068},{\"end\":24089,\"start\":24079},{\"end\":24100,\"start\":24089},{\"end\":24433,\"start\":24423},{\"end\":24444,\"start\":24433},{\"end\":24456,\"start\":24444},{\"end\":24465,\"start\":24456},{\"end\":24477,\"start\":24465},{\"end\":24490,\"start\":24477},{\"end\":24499,\"start\":24490},{\"end\":24505,\"start\":24499},{\"end\":24514,\"start\":24505},{\"end\":24895,\"start\":24888},{\"end\":24904,\"start\":24895},{\"end\":24913,\"start\":24904},{\"end\":24919,\"start\":24913},{\"end\":24930,\"start\":24919},{\"end\":24937,\"start\":24930},{\"end\":24947,\"start\":24937},{\"end\":24955,\"start\":24947},{\"end\":24961,\"start\":24955},{\"end\":25284,\"start\":25271},{\"end\":25295,\"start\":25284},{\"end\":25308,\"start\":25295},{\"end\":25323,\"start\":25308},{\"end\":25695,\"start\":25686},{\"end\":25706,\"start\":25695},{\"end\":25715,\"start\":25706},{\"end\":25723,\"start\":25715},{\"end\":25731,\"start\":25723},{\"end\":25737,\"start\":25731},{\"end\":26077,\"start\":26071},{\"end\":26085,\"start\":26077},{\"end\":26092,\"start\":26085},{\"end\":26453,\"start\":26444},{\"end\":26461,\"start\":26453},{\"end\":26472,\"start\":26461},{\"end\":26482,\"start\":26472},{\"end\":26490,\"start\":26482},{\"end\":26855,\"start\":26847},{\"end\":26863,\"start\":26855},{\"end\":26871,\"start\":26863},{\"end\":26881,\"start\":26871},{\"end\":26887,\"start\":26881},{\"end\":27147,\"start\":27139},{\"end\":27156,\"start\":27147},{\"end\":27168,\"start\":27156},{\"end\":27176,\"start\":27168},{\"end\":27184,\"start\":27176},{\"end\":27195,\"start\":27184},{\"end\":27203,\"start\":27195},{\"end\":27497,\"start\":27491},{\"end\":27504,\"start\":27497},{\"end\":27511,\"start\":27504},{\"end\":27518,\"start\":27511},{\"end\":27525,\"start\":27518},{\"end\":27788,\"start\":27782},{\"end\":27794,\"start\":27788},{\"end\":27800,\"start\":27794},{\"end\":27807,\"start\":27800},{\"end\":27815,\"start\":27807},{\"end\":27822,\"start\":27815},{\"end\":27830,\"start\":27822},{\"end\":27840,\"start\":27830},{\"end\":28084,\"start\":28074},{\"end\":28092,\"start\":28084},{\"end\":28103,\"start\":28092},{\"end\":28111,\"start\":28103},{\"end\":28122,\"start\":28111},{\"end\":28131,\"start\":28122},{\"end\":28141,\"start\":28131},{\"end\":28155,\"start\":28141},{\"end\":28167,\"start\":28155},{\"end\":28175,\"start\":28167},{\"end\":28579,\"start\":28572},{\"end\":28587,\"start\":28579},{\"end\":28595,\"start\":28587},{\"end\":28608,\"start\":28595},{\"end\":28616,\"start\":28608},{\"end\":28633,\"start\":28616},{\"end\":28855,\"start\":28847},{\"end\":28861,\"start\":28855},{\"end\":28869,\"start\":28861},{\"end\":28876,\"start\":28869},{\"end\":28882,\"start\":28876},{\"end\":28892,\"start\":28882},{\"end\":28902,\"start\":28892},{\"end\":29528,\"start\":29516},{\"end\":29539,\"start\":29528},{\"end\":29781,\"start\":29768},{\"end\":29788,\"start\":29781},{\"end\":29796,\"start\":29788},{\"end\":29808,\"start\":29796},{\"end\":30246,\"start\":30238},{\"end\":30252,\"start\":30246},{\"end\":30258,\"start\":30252},{\"end\":30624,\"start\":30608},{\"end\":30632,\"start\":30624},{\"end\":30640,\"start\":30632},{\"end\":30866,\"start\":30853},{\"end\":30874,\"start\":30866},{\"end\":30883,\"start\":30874},{\"end\":30896,\"start\":30883},{\"end\":31316,\"start\":31306},{\"end\":31324,\"start\":31316},{\"end\":31333,\"start\":31324},{\"end\":31344,\"start\":31333},{\"end\":31352,\"start\":31344},{\"end\":31363,\"start\":31352},{\"end\":31371,\"start\":31363},{\"end\":31615,\"start\":31605},{\"end\":31624,\"start\":31615},{\"end\":31635,\"start\":31624},{\"end\":31643,\"start\":31635},{\"end\":31649,\"start\":31643},{\"end\":31657,\"start\":31649},{\"end\":31665,\"start\":31657},{\"end\":31671,\"start\":31665},{\"end\":31978,\"start\":31964},{\"end\":31988,\"start\":31978},{\"end\":32002,\"start\":31988},{\"end\":32015,\"start\":32002},{\"end\":32032,\"start\":32015},{\"end\":32392,\"start\":32382},{\"end\":32403,\"start\":32392},{\"end\":32414,\"start\":32403},{\"end\":32712,\"start\":32700},{\"end\":32718,\"start\":32712},{\"end\":32942,\"start\":32934},{\"end\":32952,\"start\":32942},{\"end\":32958,\"start\":32952},{\"end\":32966,\"start\":32958},{\"end\":33175,\"start\":33167},{\"end\":33186,\"start\":33175},{\"end\":33192,\"start\":33186},{\"end\":33199,\"start\":33192},{\"end\":33207,\"start\":33199},{\"end\":33221,\"start\":33207},{\"end\":33230,\"start\":33221},{\"end\":33241,\"start\":33230},{\"end\":33250,\"start\":33241},{\"end\":33259,\"start\":33250},{\"end\":33606,\"start\":33594},{\"end\":33612,\"start\":33606},{\"end\":33620,\"start\":33612},{\"end\":33636,\"start\":33620},{\"end\":33645,\"start\":33636},{\"end\":33655,\"start\":33645},{\"end\":33665,\"start\":33655},{\"end\":33680,\"start\":33665},{\"end\":33693,\"start\":33680},{\"end\":33970,\"start\":33964},{\"end\":33977,\"start\":33970},{\"end\":33988,\"start\":33977},{\"end\":34001,\"start\":33988},{\"end\":34009,\"start\":34001}]", "bib_venue": "[{\"end\":22170,\"start\":22164},{\"end\":26233,\"start\":26171},{\"end\":26611,\"start\":26559},{\"end\":29149,\"start\":29034},{\"end\":29931,\"start\":29878},{\"end\":21108,\"start\":21031},{\"end\":21640,\"start\":21568},{\"end\":22162,\"start\":22150},{\"end\":22679,\"start\":22673},{\"end\":22998,\"start\":22946},{\"end\":23147,\"start\":23122},{\"end\":23632,\"start\":23543},{\"end\":24014,\"start\":23957},{\"end\":24421,\"start\":24333},{\"end\":24886,\"start\":24782},{\"end\":25400,\"start\":25323},{\"end\":25800,\"start\":25737},{\"end\":26169,\"start\":26092},{\"end\":26557,\"start\":26490},{\"end\":26845,\"start\":26808},{\"end\":27137,\"start\":27050},{\"end\":27489,\"start\":27441},{\"end\":27780,\"start\":27690},{\"end\":28255,\"start\":28191},{\"end\":28570,\"start\":28499},{\"end\":29032,\"start\":28902},{\"end\":29514,\"start\":29441},{\"end\":29876,\"start\":29808},{\"end\":30335,\"start\":30258},{\"end\":30606,\"start\":30572},{\"end\":30982,\"start\":30896},{\"end\":31304,\"start\":31225},{\"end\":31722,\"start\":31687},{\"end\":32068,\"start\":32032},{\"end\":32450,\"start\":32414},{\"end\":32698,\"start\":32656},{\"end\":32932,\"start\":32858},{\"end\":33165,\"start\":33101},{\"end\":33592,\"start\":33508},{\"end\":33962,\"start\":33902}]"}}}, "year": 2023, "month": 12, "day": 17}