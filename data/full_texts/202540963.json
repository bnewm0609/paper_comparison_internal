{"id": 202540963, "updated": "2023-10-06 23:10:31.055", "metadata": {"title": "On Learning Disentangled Representations for Gait Recognition", "authors": "[{\"first\":\"Ziyuan\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Luan\",\"last\":\"Tran\",\"middle\":[]},{\"first\":\"Feng\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Xiaoming\",\"last\":\"Liu\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2019, "month": 9, "day": 5}, "abstract": "Gait, the walking pattern of individuals, is one of the important biometrics modalities. Most of the existing gait recognition methods take silhouettes or articulated body models as gait features. These methods suffer from degraded recognition performance when handling confounding variables, such as clothing, carrying and viewing angle. To remedy this issue, we propose a novel AutoEncoder framework, GaitNet, to explicitly disentangle appearance, canonical and pose features from RGB imagery. The LSTM integrates pose features over time as a dynamic gait feature while canonical features are averaged as a static gait feature. Both of them are utilized as classification features. In addition, we collect a Frontal-View Gait (FVG) dataset to focus on gait recognition from frontal-view walking, which is a challenging problem since it contains minimal gait cues compared to other views. FVG also includes other important variations, e.g., walking speed, carrying, and clothing. With extensive experiments on CASIA-B, USF, and FVG datasets, our method demonstrates superior performance to the SOTA quantitatively, the ability of feature disentanglement qualitatively, and promising computational efficiency. We further compare our GaitNet with state-of-the-art face recognition to demonstrate the advantages of gait biometrics identification under certain scenarios, e.g., long distance/lower resolutions, cross viewing angles.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1909.03051", "mag": "3046961188", "acl": null, "pubmed": "32750777", "pubmedcentral": null, "dblp": "journals/pami/ZhangTLL22", "doi": "10.1109/tpami.2020.2998790"}}, "content": {"source": {"pdf_hash": "439aca3a3d4497f822b058ccabcbfc94a83c5ee9", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1909.03051v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1909.03051", "status": "GREEN"}}, "grobid": {"id": "41ce238b48212fd3e10918187d99e0357f859292", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/439aca3a3d4497f822b058ccabcbfc94a83c5ee9.txt", "contents": "\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 On Learning Disentangled Representations for Gait Recognition\n\n\nZiyuan Zhang \nLuan Tran \nMember, IEEEFeng Liu \nMember, IEEEXiaoming Liu \nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 On Learning Disentangled Representations for Gait Recognition\nIndex Terms-Gait recognitiondeep convolutional neural networksdisentangled representation learningauto-encoderLSTMcanonical representationface recognition\nGait, the walking pattern of individuals, is one of the important biometrics modalities. Most of the existing gait recognition methods take silhouettes or articulated body models as gait features. These methods suffer from degraded recognition performance when handling confounding variables, such as clothing, carrying and viewing angle. To remedy this issue, we propose a novel AutoEncoder framework, GaitNet, to explicitly disentangle appearance, canonical and pose features from RGB imagery. The LSTM integrates pose features over time as a dynamic gait feature while canonical features are averaged as a static gait feature. Both of them are utilized as classification features. In addition, we collect a Frontal-View Gait (FVG) dataset to focus on gait recognition from frontal-view walking, which is a challenging problem since it contains minimal gait cues compared to other views. FVG also includes other important variations, e.g., walking speed, carrying, and clothing. With extensive experiments on CASIA-B, USF, and FVG datasets, our method demonstrates superior performance to the SOTA quantitatively, the ability of feature disentanglement qualitatively, and promising computational efficiency. We further compare our GaitNet with state-of-the-art face recognition to demonstrate the advantages of gait biometrics identification under certain scenarios, e.g., long distance/lower resolutions, cross viewing angles.\n\nINTRODUCTION\n\nB IOMETRICS measures people's unique physical and behavioral characteristics to recognize the identity of an individual. Gait [1], the walking pattern of an individual, is one of biometrics modalities besides face, fingerprint, iris, etc. Gait recognition has the advantage that it can operate at a distance without users' cooperation. Also, it is difficult to camouflage. Due to these advantages, gait recognition is applicable to many applications such as person identification, criminal investigation, and healthcare.\n\nAs other recognition problems, gait data can usually be captured by five types of sensors [2], i.e., RGB camera, RGB-D camera [3], [4], accelerometer [5], floor sensor [6], and continuous-wave radar [7]. Among them, RGB camera is not only the most popular one due to the low sensor cost, but also the most challenging one since RGB pixels might not be effective in capturing the motion cues. This work studies gait recognition from RGB cameras.\n\nThe core of gait recognition lies in extracting gait features from the video frames of a walking person, where the prior work can be categorized into two types: appearance-based and model-based methods. The appearance-based methods, e.g., Gait Energy Image (GEI) [8], take the averaged silhouette image as the gait feature. While having a low computational cost and being able to handle low-resolution imagery, it can be sensitive to variations such as cloth change, carrying, viewing angles and walking speed [9]- [15]. The model-based methods use the articulated body skeleton from pose estimation as the gait feature. They show more robustness to aforementioned variations but at a price of a higher computational cost and dependency on pose estimation accuracy [16]- [18].\n\nIt is understandable that the challenge in designing a gait feature is the necessity of being invariant to the appearance variation due to clothing, viewing angle, carrying, etc. Therefore, our desire is to \u2022 Ziyuan Zhang, Luan Tran, Feng Liu, and  disentangle the gait feature from the non-gait-related appearance of the walking person. For both appearance-based or modelbased methods, such disentanglement is achieved by manually handcrafting the GEI-like [8], [10] or body skeleton-like [16]- [18] features, since neither has color or texture information. However, we argue that these manual disentanglements may be sensitive to changes in walking condition. In other words, they can lose certain or create redundant gait information. E.g., GEI-like features have distinct silhouettes for the same subject wearing different clothes. For skeleton-like features, when carrying accessories (e.g., bags, umbrella), certain body joints such as hands may have fixed positions, and hence are redundant information to gait.\n\nTo remedy the aforementioned issues in handcrafted features, as shown in Fig. 1 (a), this paper proposes a novel approach to learn gait representations from the RGB video directly. Specifically, we aim to automatically disentangle dynamic pose features (trajectory of gait) from pose-irrelevant features. To further distill identity information from pose-irrelevant features, we disentangle the poseirrelevant features into appearance (i.e., clothing) and canonical features. Here, the canonical feature refers to a standard and unique representation of human body, such as body ratio, width and limb lengths, etc. The pose features and canonical features are discriminative in identity and are used for gait recognition. Fig. 1 (b) visualizes the three disentangled features. This disentanglement is realized by designing an autoencoderbased Convolutional Neural Network (CNN), GaitNet, with novel loss functions. For each video frame, the encoder estimates three latent representations: pose, canonical and appearance features, by employing three loss functions: 1) cross reconstruction loss enforces that the canonical and appearance features of one frame, fused with the pose feature of another frame, can be decoded to the latter frame; 2) pose similarity loss forces a sequence of pose features extracted from a video sequence, of the same subject to be similar even under different conditions; 3) canonical  Fig. 3 show that, the appearance feature is video-specific capturing clothing information; the canonical feature is subject-specific capturing the overall body shape at a standard pose; the pose feature is frame-specific capturing body poses at individual frames.\n\nconsistency loss favors consistent canonical features among videos of the same subject under different conditions. Finally, the pose features of a sequence are fed into a multi-layer LSTM with our designed incremental identity loss to generate the sequence-based dynamic gait feature. The average of canonical features results in the sequence-based static gait feature. Given two gait videos, the cosine distances between their respective dynamic and static gait features are computed and their summation is the final video-tovideo gait similarity metric. In addition, most prior work [8], [10], [14], [16], [19]- [24] choose the walking video of the side view, which has the richest gait information, as the gallery sequence. However, in practices other viewing angles, such as the frontal view, can be very common when pedestrians walk toward or away from the surveillance camera. Also, the prior work [25]- [28] that focuses on frontal view are often based on RGB-D videos, which have additional depth information than RGB. Therefore, to encourage gait recognition from frontalview RGB videos that generally has the minimal amount of gait information, we collect a high-definition (HD, 1080p) Frontal-View Gait database, named FVG, with a wide range of variations. It has three frontal-view angles where the subject walks from left 45 \u2022 , 0 \u2022 , and right 45 \u2022 off the optical axes of the camera. For each of three angles, different variants are explicitly captured including walking speed, clothing, carrying, multiple people, etc.\n\nA preliminary version of this work was published in the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) 2019 [29]. We extend the work from three aspects. 1) Instead of disentangling features in two components: pose and poseirrelevant [29], we further decouple the pose-irrelevant features into discriminative canonical feature and appearance feature. By devising an effective canonical consistency loss, the canonical feature helps to improve gait recognition accuracy. 2) We conduct more insightful ablation studies to analyze the relationship between our disentanglement losses and features, gait recognition over time, and contributions of dynamic and static gait features. 3) We perform side-by-side comparison between gait recognition and the state-of-the-art (SOTA) face recognition on the same dataset.\n\nIn summary, this paper makes the following contributions:\n\nOur proposed GaitNet directly learns disentangled representations from RGB videos, which is in sharp contrast to the conventional appearance-based or model-based methods.\n\nWe introduce a Frontal-View Gait database, including various variations of viewing angles, walking speeds, carrying, clothing changes, background and time gaps. This is the first HD gait database, with nearly twice the number of subjects compared to existing RGB gait databases.\n\nOur proposed method outperforms the state of the arts on three benchmarks, CASIA-B, USF, and FVG datasets.\n\nWe demonstrate the strength of gait recognition over face recognition in the task of person recognition from surveillancequality videos.\n\n\nRELATED WORK\n\nGait Representation. Most prior works are based on two types of gait representations. In appearance-based methods, gait energy image (GEI) [8] or gait entropy image (GEnI) [10] are defined by extracting silhouette masks. Specifically, GEI uses an averaged silhouette image as the gait representation for a video. These methods are popular in the gait recognition community for their simplicity and effectiveness. However, they often suffer from sizeable intra-subject appearance changes due to covariates such as clothing, carrying, views, and walking speed. On the other hand, model-based methods [17], [18] fit articulated body models to images and extract kinematic features such as 2D body joints. While they are robust to some covariates such as clothing and speed, they require a relatively higher image resolution for reliable pose estimation and higher computational costs.\n\nIn contrast, our approach learns gait representation directly from raw RGB video frames which contain richer information, thus with higher potential of extracting more discriminative gait features. The most relevant work to ours is [33], which learns gait features from RGB images via Conditional Random Field. Compared to [33], our proposed approach learns two complimentary features: dynamic gait, and static gait features, and has the advantage of being able to leverage a large amount of training data and learning more discriminative representation from data with multiple covariates. In addition, some recent works [11], [15], [21], [22], [34] use CNN to learn more discriminative features from GEI. However, the source of the learning, GEI, already loses dynamic information since a random shuffle of video frames results in the identical GEI feature. In contrast, the proposed GaitNet learns features from RGB imagery instead, which allows the network to explore richer information for representation learning. This is demonstrated by our comparison with [11], [33] in Sec. 5.2.1 and Sec. 5.2.3.\n\nGait Databases. There are many classic gait databases such as SOTON Large dataset [35], USF [9], CASIA-B [30], OU-ISIR [32], and TUM GAID [36]. We compare our FVG database with the widely used ones in Tab Disentanglement Learning. Besides model-based approaches representing data with semantic latent vectors [37]- [40], datadriven disentangled representation learning approaches are gaining popularity in the computer vision community. DrNet [41] disentangles content and pose vectors with a two-encoders architecture, which removes content information in the pose vector by generative adversarial training. The work of [42] segments foreground masks of body parts by 2D pose joints via U-Net [43] and then transforms body parts to desired motion with adversarial training. Similarly, [44] utilizes U-net and Variational Auto Encoder (VAE) [45] to disentangle an image into appearance and shape. DR-GAN [46], [47] achieves SOTA performances on pose-invariant face recognition by explicitly disentangling pose variation with a multi-task GAN [48]. Different from [41], [42], [44], our method has only one encoder to disentangle the three latent features, through the design of novel loss functions without the need for adversarial training. Further, pose labels are used in DR-GAN training so as to disentangle identity feature from the pose. However, to disentangle pose and appearance features from RGB, there is no pose nor appearance label to be utilized for our method, since it is nontrivial to define the types of walking pattern or clothes as discrete classes. Gait vs. Face recognition. Both gait and face are popular biometrics modalities, especially in covert identification-at-adistance applications. Hence, it is valuable to understand the pros and cons of each modality if the SOTA gait recognition and face recognition algorithms are deployed. Along this direction, most of the prior works focus on the fusion of both modalities and evaluate on relatively small datasets [49]- [51]. In contrast, we conduct comprehensive evaluations using SOTA face and gait recognition algorithms, across various conditions of CASIA-B and FVG databases. Further, the performances are measured along the video duration to explore the impact of person-to-camera distances.\n\n\nPROPOSED APPROACH\n\n\nOverview\n\nLet us start with a simple example. Assuming there are three videos, where videos 1 and 2 capture subject A wearing t-shirt and long down coat respectively, and in video 3 subject B wears the same long down coat as in video 2. The objective is to design an algorithm, from which the gait features of video 1 and 2 are the same, while those of video 2 and 3 are different. Clearly, this is a challenging objective, as the long down coat can easily dominate the extracted feature, which would make video 2 and 3 to be more similar than 1 and 2 in the latent space of gait features. Indeed the core challenge, as well as the objective, of gait recognition is to extract gait features that are discriminative among subjects, but invariant to different confounding factors, such as viewing  , and invariant within the same subject (a). These include overall body shape, arm length, torso vs. leg ratio, etc. We define canonical feature to specifically describe these characteristics.\n\nangles, walking speeds and changing clothes. Table 2 summarizes the symbol and notation used in this paper. Our approach to achieve this objective is feature disentanglement. In our preliminary work [29], we disentangle features into two components: pose and \"appearance\" features. However, further research discovered that the \"appearance\" feature still contains certain discriminative information, which can be useful for identity classification. For instance, as in Fig. 2, imagining if we would ignore the body pose, e.g., position of arms and legs, and clothing information, e.g., color and texture of clothes, we may still tell apart different subjects by their inherent body characteristics, which can include categories of overall body shape (e.g., rectangle, triangle, inverted triangle, and hourglass [52]), arm length, torso vs. leg ratio [53], etc. In other words, even when different people wearing exactly the same clothing and standing still, these characteristics are still subject dependent. In the meantime, for the same subject under various conditions, these characteristics are relatively constant. In this work, we term the feature describing these characteristics as the canonical feature. Hence, given a walking video X c under condition c, our framework disentangle the encoded feature into three components: the pose feature f p , the appearance feature f a and the canonical feature f c . We also term the concatenation of f a and f c as the pose-irrelevant feature, which is conceptually equivalent to the \"appearance\" feature in [29]. The pose feature describes the positions of body parts, and their dynamic over time is essentially the core element of gait; the canonical feature defines the unique characteristics of individual body; and the appearance feature describes the subject's clothing.\n\nThe above feature disentanglement can be naturally implemented as an encoder-decoder network. Specifically, as depicted in Fig. 3, the input to our GaitNet is a video sequence, with background removed using any off-the-shelf pedestrian detection and segmentation method [54]- [56]. With carefully designed loss functions, an encoder is learned to disentangle the pose, canonical and appearance features for each video frame. Then, a multilayer LSTM explores the temporal dynamics of pose features and The properties of three disentangled features in terms of its constancy across frames and conditions, and discriminativeness. These properties are the basis for us to design loss functions for feature disentanglement. aggregates them to a sequence-based dynamic gait feature. In the meantime, the average of all the canonical features is defined as the static gait feature. Measuring distances of both dynamic and static features between the gallery and probe walking videos provides the final matching score. In this section, we first present the feature disentanglement, followed by temporal aggregation, model inference and finally implementation details.\n\n\nConstant Across Frames Constant Across Conditions Discriminative\n\n\nFeature Disentanglement\n\nFor the majority of gait datasets, there is limited intra-subject appearance variation. Hence, appearance could be a discriminative cue for identification during training as many subjects can be easily distinguished by their clothes. Unfortunately, any feature extractors relying on appearance will not generalize well on the test set or in practice, due to potentially diverse clothing or appearance between two videos of the same subject. This limitation on training sets also prevents us from learning ideal feature extractors if solely relying on identification objective. Hence we propose to learn to disentangle the canonical and pose feature from the visual appearance. Since a video is composed of frames, disentanglement should be conducted at the frame level first. Before presenting the details of how we conduct disentanglement, let us first understand the various properties of three types of features, as summarized in Tab. 3. These properties are crucial in guiding us to define effective loss functions for disentanglement. The appearance feature mainly describes the clothing information of the subject. Hence it is constant within a video sequence, but often different across different conditions. Of course it is not discriminative among individuals. The canonical feature is subjectspecific, and is therefore constant across both video frames, and conditions. The pose feature is obviously different across video frames, but is assumed to be constant across conditions. Since the pose feature is the manifestation of video-based gait information at a specific frame, the pose feature itself might not be discriminative. However, the dynamics of pose features over time will constitute the dynamic gait feature, which is discriminative among individuals.\n\nTo this end, we propose to use an encoder-decoder network architecture with carefully designed loss functions to disentangle the pose feature and canonical feature from appearance feature. The encoder, E, encodes a feature representation of each frame, x, and explicitly splits it into three components, namely appearance feature f a , canonical feature f c and pose feature f p :\nf a , f c , f p = E(x).(1)\nCollectively these three features are expected to fully describe the original input image. As they can be decoded back to the original input through a decoder D:\nx = D(f a , f c , f p ).(2)\nWe now define the various loss functions to jointly learn the encoder E and decoder D. Cross Reconstruction Loss. The reconstructed imagex should be close to the original input x. However, enforcing self-reconstruction loss as in typical auto-encoder cannot ensure the meaningful disentanglement as in our design. Hence, we propose the cross reconstruction loss, using the appearance feature f t1 a and canonical feature f t1 c of frame t 1 and the pose feature f t2 p of frame t 2 to reconstruct the latter frame:\n\u0141 xrecon = D(f t2 a , f t1 c , f t1 p ) \u2212 x t2 2 2 .(3)\nThe cross reconstruction loss, on one hand, can act as the selfreconstruction loss to make sure the three features are sufficiently representative to reconstruct a video frame. On the other hand, as we can pair a pose feature of a current frame with the canonical and appearance features of any frame in the same video to reconstruct the same target, it enforces both the canonical and appearance features to be similar across all frames within a video. Indeed, according to Tab. 3, between the pose-irrelevant feature, f a &f c , and the pose feature f p , the main distinct property is that the former is constant across frames while the latter is not. This is the basis for designing our cross reconstruction loss.\n\nPose Similarity Loss. The cross reconstruction loss is able to prevent the pose-irrelevant feature, f a &f c , to be contaminated by the pose information that changes across frames. If not, i.e., f a or f c contains some pose information, D(f t2 a , f t1 c , f t1 p ) and x t2 would have different poses. However, clothing/texture and body information may still be leaked into the pose feature f p . In the extreme case, f c and f a could be constant vectors while f p encodes all the information of a video frame.\n\nTo encourage f p including only the pose information, we leverage multiple videos of the same subject. Given two videos of the same subject with length n 1 , n 2 in two different conditions c 1 , c 2 , they contain difference in the person's appearance, i.e., cloth changes. Despite appearance changes, the gait information is assumed to be constant between two videos. Since it's almost impossible to enforce similarity on f p between video frames as it requires precise frame-level alignment, we minimize the similarity between two videos' averaged pose features:\n\u0141 pose-sim = 1 n 1 n1 t=1 f (t,c1) p \u2212 1 n 2 n2 t=1 f (t,c2) p 2 2 .(4)\nAccording to Tab. 3, the pose feature is constant across conditions, which is the basis of our pose similarity loss.\n\nCanonical Consistency Loss. The canonical feature describes the subject's body characteristics, which is unique over all video frames. To be specific, for two videos of the same subject k in two different conditions c 1 , c 2 , the canonical feature is constant across both frames and conditions, as illustrated in Tab. 3. Tab. 3 also states that the canonical feature is discriminative across subjects. Hence, to enforce the two constancy and the discriminativeness, we define the canonical consistency loss as follows:\n\u0141 cano-cons = 1 n 2 1 i =j f (ti,c1) c \u2212 f (tj ,c1) c 2 2 + 1 n 1 i f (ti,c1) c \u2212 f (ti,c2) c 2 2 + 1 n 1 i \u2212 log(C sg k (f (t1,c1) c ))),(5)\nwhere the three terms measure the consistency across frames in a single video, consistency across different videos of the same subject, and identity classification using a classifier C sg , respectively.\n\n\nGait Feature Learning and Aggregation\n\nEven when we can disentangle pose, canonical and appearance information for each video frame, the f p and f c have to be aggregated over time, since 1) gait recognition is conducted between two videos instead of two images; 2) not all the f c from every single frame is guaranteed to have same canonical information; 3) the current feature f p only represents the walking pose of the person at a specific instance, which can share similarity with another instance of a different individual. Here, we are looking for discriminative characteristics in a person's walking pattern. Therefore, modeling its aggregation for f c and temporal change for f p is critical.\n\n\nStatic Gait Feature via Canonical Feature Aggregation\n\nAfter learning f c for every single frame as defined in Eqn. 5, we explore the best representation of f c features across all frames of a video sequence. Since f c is assumed to be constant over time, we compute the averaged f c features as a way to aggregate the canonical features over time. Given that f c describes the body characteristics as if we freeze the gait, we call the aggregated f c as the static gait feature f sta-gait .\nf sta-gait = 1 n n t=1 f t c . (6)\n\nDynamic Gait Feature via Pose Feature Aggregation\n\nFor temporal modeling of poses, this is where temporal modeling architectures like the recurrent neural network or long short-term memory (LSTM) work best. Specifically, in this work, we utilize a multi-layer LSTM structure to explore temporal information of pose features, e.g., how the trajectory of subjects' body parts changes over time. As shown in Fig. 3, pose features extracted from one video sequence are fed into a 3-layer LSTM. The output of the LSTM is connected to a classifier C dg , in this case, a linear classifier is used, to classify the subject's identity. Let h t be the output of the LSTM at time step t, which is accumulative after feeding t pose features f p into it:\nh t = LSTM(f 1 p , f 2 p , ..., f t p ).(7)\nNow we define the loss function for LSTM. A trivial option for identification is to add the classification loss on top of the LSTM output of the final time step:\n\u0141 id-single = \u2212 log(C dg k (h n )),(8)\nwhich is the negative log likelihood that the classifier C dg correctly identifies the final output h n as its identity label k.\n\nIdentification with Averaged Feature. By the nature of LSTM, the output h t can be greatly affected by its last input f t p . Hence the LSTM output, h t , could be unstable across time steps. With a desire to obtain a gait feature that is robust to the final instance of a walking cycle, we choose to use the averaged LSTM output as our gait feature for identification:\nf t dyn-gait = 1 t t s=1 h s .(9)\nThe identification loss can be rewritten as: Incremental Identity Loss. LSTM is expected to learn that, the longer the video sequence, the more walking information it processes thus the more confident it identifies the subject. Instead of minimizing the loss at the final time step, we propose to use all the intermediate outputs of every time step weighted by w t :\n\u0141 id-avg = \u2212 log(C dg k (f n dyn-gait )) = \u2212 log C dg k 1 n n s=1 h s .(10)\u0141 id-inc-avg = 1 n t=1 w t n t=1 \u2212w t log C dg k 1 t t s=1 h s ,(11)\nwhere we set w t = t 2 and other options such as w t = 1 also yield similar performance. In the experiments, we will ablate the impact of three options in classification loss: \u0141 id-single , \u0141 id-avg , and \u0141 id-inc-avg . To this end, the overall loss function is:\n\u0141 = \u0141 id-inc-avg + \u03bb r \u0141 xrecon + \u03bb d \u0141 pose-sim + \u03bb s \u0141 cano-sim .(12)\nThe entire system, including encoder, decoder, and LSTM, are jointly trained. Updating E to optimize \u0141 id-inc-avg also helps to further generate pose feature that has identity information and from which LSTM is able to explore temporal dynamics.\n\n\nModel Inference\n\nSince GaitNet takes one video sequence as input and outputs f dyn-gait and f sta-gait as shown in Fig. 3, one single score is needed to measure the similarity between the gallery and probe videos for either gait authentication or identification. During testing, both f sta-gait and f dyn-gait are used as the identity features for score calculation. We use the cosine similarity scores, normalized to the range of [0, 1] via min-max. The static and dynamic scores are finally fused by a weighted sum rule:\nScore = (1 \u2212 \u03b1) * cos (f g sta-gait , f p sta-gait ) + \u03b1 * cos (f g dyn-gait , f p dyn-gait ),(13)\nwhere g and p represent gallery and probe, respectively.\n\n\nImplementation Details\n\nDetection and Segmentation. Our GaitNet receives video frames with the person of interest segmented. The foreground mask is obtained from the SOTA instance segmentation algorithm, Mask R-CNN [54]. Instead of using a zero-one mask by hard thresholding, we maintain the soft mask returned by the network, where each pixel indicates the probability of being a person. This is partially due to the difficulty in choosing an appropriate threshold suitable for multiple databases. Also, it remedies the loss in information due to the mask estimation error. We use a bounding box with a fixed ratio of width : height = 1 : 2 with the absolute height and center location given by the Mask R-CNN network. The input of Network Structure and Hyperparameter. Our encoder-decoder network is a typical CNN, illustrated in Tab. 4. Different from our preliminary work [29], we replace stride-2 convolution layers with stride-1 convolution layers and max pooling layers, since we find the latter is able to achieve the similar results with less hyper-parameter searching for different training scenarios. Each convolution layer is followed by Batch Normalization and Leaky ReLU activation. The decoder structure, similar to [57], is built from transposed 2D convolution, Batch Normalization and Leaky ReLU layers. The final layer is a Sigmoid activation which can output the value into [0, 1] range as the input. All the transposed convolutions are with stride of 2 to up sample images and all the Leaky ReLU are with slope of 0.2. The classification part is a stacked 3-layer LSTM [58], which has 256 hidden units in each cell. The length of f a , f c and f p is 128, 128 and 64 respectively, as shown in Tab. 2. The Adam optimizer [59] is initialized with the learning rate of 0.0001, and the momentum of 0.9. To prevent over-fitting, the weights decay of 0.001 is applied to all the experiments, and the learning rate decays by multiplying 0.9 in every 500 iterations. For each batch, we use video frames from 16 or 32 different clips depending on different experiment protocols. Since video lengths are varied, a random crop of 20-frame sequence is applied during training; all shorter videos are discarded. The \u03bb r , \u03bb s and \u03bb d in Eqn. 12 are all set to 1 in all experiments.\n\n\nFRONT-VIEW GAIT (FVG) DATABASE\n\nCollection. To facilitate the research of gait recognition from frontal-view angles, we collect the Front-View Gait (FVG) database in a course of two years (2017 and 2018). During the capturing, we place the camera (Logitech C920 Pro Webcam or GoPro Hero 5) on a tripod at the height of 1.50 meters. We require each of 226 subjects to walk toward the camera 12 times starting from around  Protocols. Different from prior gait databases, subjects in FVG are walking toward the camera, which creates a great challenge on exploiting gait information as the visual difference in consecutive frames is normally much smaller than side-view walking. We focus our evaluation on variations that are challenging, e.g., different clothes, carrying a bag while wearing a hat, or are not presented in prior databases, e.g., multi-person. To benchmark research on FVG, we define 5 evaluation protocols, among which there are two commonalities: 1) the first 136 and remaining 90 subjects are used for training and testing respectively; 2) the video 2, the normal frontal-view walking, is always used as the gallery. \n\n\nEXPERIMENTAL RESULTS\n\nWe evaluate the proposed approach on three gait databases, CASIA-B [30], USF [9] and FVG. As mentioned in Sec. 2, CASIA-B and USF are the most widely used gait databases, which helps us to make the comprehensive comparison with prior works. We compare our method with [11], [33], [60], [61] on these two databases, by following the respective experimental protocols of the baselines. These are either the most recent and SOTA work, or classic gait recognition methods. The OU-ISIR database [32] is not evaluated, and related results [24] are not compared since our work consumes RGB video input, but OU-ISIR only releases silhouettes. Finally, we also conduct experiments to compare our gait recognition with the state-of-the-art face recognition method ArcFace [62] on the CASIA-B and FVG datasets.  \n\n\nAblation Study\n\n\nFeature Visualization Through Synthesis\n\nWhile our decoder is only useful in training, but not model inference, it can enable us to visualize the disentangled features as a synthetic image, by feeding either the feature itself, or their random concatenation, to our learned decoder D. This synthesis helps to gain more understanding of the feature disentanglement.\n\nVisualization of Features in One Frame. Our decoder requires dynamic gait features f dyn-gait . We select 5 subjects each with two videos of NM vs. CL conditions. Each point represents a single frame, whose color is for subject ID, shape of 'dot' and 'cross' is NM and CL respectively, and size is frame index. We see that fc and f dyn-gait are far more discriminative than fa and fp.\n\nthe concatenation of three vectors for synthesis. Hence, to visualize each individual feature, we concatenate it with two vectors of zeros and then feed to decoder. In Fig. 5, we show the disentanglement visualization of 4 subjects (two frontal and two side views), each under the NM and CL conditions. First of all, the canonical feature discovers a standard body pose that is consistent across both subjects, which is more visible in the side view. Under such a standard body pose, the canonical feature then depicts the unique body shape, which is consistent within a subject but different between subjects. The appearance feature faithfully recovers the color and texture of clothing, at the standard body pose specified by the canonical feature. The pose feature captures the walking pose of the input frame. Finally, combining all three features can closely reconstruct the original input. This shows that our disentanglement not only preserves all information of the input, but also fulfills all the desired properties described in Tab. 3.\n\nVisualization of Features in Two Frames. As shown in Fig. 6, each result is generated by pairing the pose-irrelevant feature {f a , f c } in the first column, and the pose feature f p in the first row. The synthesized images show that indeed pose-irrelevant feature contributes all the appearance and body information, e.g., cloth, body width, as they are consistent across each row. Meanwhile, f p contributes all the pose information, e.g., positions of hand and feet, which share similarity across columns. Despite that concatenating vectors from different subjects may create samples outside the input distribution of D, the visual quality of synthetic images shows that D is versatile to these new samples. \n\n\nFeature Visualization Through t-SNE\n\nTo gain more insight into the frame-level features f a , f c , f p and sequence-level LSTM feature aggregation, we apply t-SNE [63] to these features to visualize their distribution in a 2D space. With the learnt models in Sec. 5.1.1, we randomly select two videos under NM and CL conditions for each of 5 subjects. Fig. 7 (a,b) visualizes the f a and f c features. Obviously, for the appearance feature f a , the margins between intra-class and inter-class distances are unpromising, which shows that f a has limited discrimination power. In contrast, the canonical feature f c has both the compact intra-class variations and separable interclass differences -useful for identity classification. In addition, we visualize the f p from E and its corresponding f dyn-gait at each time step in Fig. 7 (c-d). As defined in Eqn. 4, we enforce the averaged f p of the same subject to be consistent under different conditions. Since Eqn. 4 only minimizes the intra-class distance, it cannot guarantee the discrimination among subjects. However, after aggregation by the LSTM network, distances of points at longer time duration for inter-class are substantially enlarged.\n\n\nLoss Function's Impact on Performance\n\nDisentanglement with Pose Similarity Loss. With the cross reconstruction loss, the appearance feature f a and canonical feature f c can be enforced to represent static information that shares across the video. However, as discussed, f p could be contaminated by the appearance information or even encode the entire video frame. Here we show the benefit of the pose similarity loss \u0141 pose-sim to feature disentanglement. Fig. 8 shows the cross visualization of two different models learned with and without \u0141 pose-sim . Without \u0141 pose-sim the decoded image shares some appearance and body characteristic, e.g., cloth style, contour, with f p . Meanwhile, with \u0141 pose-sim , appearance better matches with f a and f c .\n\nLoss Function's Impact on Recognition Performance. As there are various options in designing our framework, we ablate their effect on the final recognition performance from three perspectives: the disentanglement loss, the classification loss, and the classification feature. Tab. 6 reports the Rank-1 recognition accuracy of different variants of our framework on CASIA-B under NM vs. CL and lateral view. The model is trained with all videos of the first 74 subjects and tested on the remaining 50 subjects.  Fig. 9: The t-SNE visualization of f dyn-gait from 5 subjects, each with 2 videos (NM vs. CL). The symbols are defined the same as Fig. 7. The top and bottom rows are two models learnt with L id-single and \u0141 id-inc-avg loss respectively. From left to tight, the points are f dyn-gait of the first 10 frames, 10-30 frames, and 30-60 frames. Learning with \u0141 id-inc-avg leads to more discriminative dynamic features for the entire duration.\n\nWe first explore the effects of different disentanglement losses applied to f dyn-gait and use f dyn-gait only for classification. Using \u0141 id-inc-avg as the classification loss, we train different variants of our framework: a baseline without any disentanglement losses, a model with \u0141 xrecon and our model with both \u0141 xrecon and \u0141 pose-sim . The baseline achieves the accuracy of 56.0%. Adding \u0141 xrecon slightly improves the accuracy to 60.2%. By combining with \u0141 pose-sim , our model significantly improves the accuracy to 85.6%. Between \u0141 xrecon and \u0141 pose-sim , the pose similarity loss plays a more critical role as \u0141 xrecon is mainly designed to constrain the appearance feature, which does not directly benefit identification.\n\nWe also compare the effects of different classification losses applied to f dyn-gait . Even though the classification loss only affects f dyn-gait , we report the performance with both f dyn-gait and f sta-gait for a direct comparison with our full model in the last row. With the disentanglement loss of \u0141 xrecon , \u0141 pose-sim and \u0141 cano-sim , we benchmark different options of the classification loss as presented in Sec. 3.2, as well as the autoencoder loss by Srivastava et al. [64]. The model using the conventional identity loss on the final LSTM Fig. 10: Recognition by fusing f dyn-gait and f sta-gait scores with different weights as defined in Eqn. 13. Rank-1 accuracy and TAR@1% FAR is calculated for CASIA-B and FVG, respectively. output \u0141 id-single achieves the rank-1 accuracy of 72.5%. Using the average output of LSTM as the identity feature, \u0141 id-avg improves the accuracy to 82.6%. The autoencoder loss [64] achieves a good performance of 76.5%. However, it is still far from our proposed incremental identity loss \u0141 id-inc-avg 's performance at 92.1%. Fig. 9 further visualizes the f dyn-gait over time, for two models learnt with L id-single and \u0141 id-inc-avg loss respectively. Clearly, even with less than 10 frames, the model with \u0141 id-inc-avg shows more discriminativeness, which also increases rapidly as time progresses.\n\nFinally, we compare different features in computing the final classification score. The performance is based on the model with full disentanglement losses and \u0141 id-inc-avg as the classification loss. When f a is utilized in cosine distance calculation, the rank-1 accuracy is merely 33.4%, while f sta-gait and f dyn-gait achieve 76.3% and 85.9% respectively. The results prove the learnt f c and f p are effective for classification while f a has limited discriminative power. Also, by combining both f sta-gait and f dyn-gait features, the recognition performance can be further improved to 92.1%. We believe that such performance gain is owing to the complementary discriminative information offered by f sta-gait w.r.t. f dyn-gait .\n\n\nDynamic vs. Static Gait Features\n\nSince f dyn-gait and f sta-gait are complementary in classification, it is interesting to understand their relative contributions, especially in the various scenarios of gait recognition. This amounts to exploring a global weight \u03b1 for the GaitNet on various training data, where \u03b1 ranges from 0 to 1. There are three protocols on CASIA-B and hence three GaitNet models are trained respectively. We calculate the weighted score of all three models on the training data of protocol 1, since it is the most comprehensive and representative protocol covering all the viewing angles and conditions. The same experiment is conducted on \"ALL\" protocol of the FVG dataset.\n\nAs shown in Fig. 10, GaitNet has the best average performance on CASIA-B when \u03b1 is around 0.2, while on FVG \u03b1 is around 0.75. According to Eqn. 13, f sta-gait has relatively more classification contributions on CASIA-B. One potential reason is that it is more challenging to match dynamic walking poses under large range of viewing angles. In comparison, FVG favors f dyn-gait . Since FVG is an all-frontal-walking dataset containing varying distances or resolutions, dynamic gait is relatively easier to learn with the fixed view, while f sta-gait might be sensitive to resolution changes.\n\nNevertheless, note that in the two extreme cases, where only f sta-gait or f dyn-gait is used, there is relatively small performance gap between them. This means that either feature is effective in classification. Considering this observation and the balance between databases, we choose to set \u03b1=0.5, which will be used in all subsequent experiments. is on side-side views.\n\n\nGait Recognition Over Time\n\nOne interesting question to study is that, how many video frames are needed to achieve reliable gait recognition. To answer this question, we compare the performance with different feature scores (f sta-gait , f dyn-gait and their fusion) for identification, with different video lengths. As shown in Fig. 11, both dynamic and static features achieve stable performance starting from about 10 frames, after which the gain in performance is relatively small. At 15 FPS, a clip of 10 frames is equivalent to merely 0.7 seconds of walking. Further, the static gait feature has notable good performance even with a single video frame. This impressive result shows the strength of our GaitNet in processing very short clips. Finally, for most of the frames in this duration, the fusion outperforms both the static and dynamic gait feature alone.\n\n\nEvaluation on Benchmark Datasets\n\n\nCASIA-B\n\nSince various experimental protocols have been defined on CASIA-B, for a fair comparison, we strictly follow the respective protocols in the baseline methods. Following [11], Protocol 1 uses the first 74 subjects for training and remaining 50 for testing, regarding variations of NM (normal), BG (carrying bag) and CL (wearing a coat) with crossing viewing angles of 0 \u2022 to 180 \u2022 . Three models are trained for comparison in Tab. 7. For the detailed protocol, please refer to [11]. Here we mainly compare our work to Wu et al. [11], along with other methods [34], [65]. We denote our preliminary work [29] as GaitNet-pre and this work as GaitNet. Under multiple viewing angles and across three variations, GaitNet achieves the best performance compared to all SOTA methods and GaitNet-pre since f c can distill more discriminative information under various viewing angles and conditions. Recently, Chen et al. [11] propose new protocols to unify the training and testing where only one single model is trained for each protocol. Protocol 2 focuses on walking direction variations, where all videos used are in the NM subset. The training set includes videos of first 24 subjects in all viewing angles. The rest 100 subjects are for testing. The gallery is made of four videos at 90 \u2022 view for each subject. The first two videos from remaining viewing angles are the probe. The Rank-1 recognition accuracies are reported in Tab. 8. GaitNet achieves the best average accuracy of 87.3% across 10 viewing angles, with significant improvement on extreme views compared to our preliminary work [29]. For example, at viewing angles of 0 \u2022 , and 180 \u2022 , the improvement margins are both 14%. This shows that more discriminative gait information, such as a canonical body shape information, under   outperforming on CL subset. The proposed GaitNet outperforms on both subsets. Note that due to the challenge of CL protocol, there is a significant performance gap between BG and CL for all methods except ours, which is yet another evidence that our gait feature has strong invariance to all major gait variations.\n\nAcross all evaluation protocols, GaitNet consistently outperforms the state of the art. This shows the superior of GaitNet on learning a robust representation under different variations. It is contributed to our ability to disentangle pose/gait information from appearance variations. Comparing with our preliminary work, the canonical feature f c contains discriminative power which can further improve the recognition performance.\n\n\nUSF\n\nThe original protocol of USF [9] and the methods [68]- [71] does not define a training set, which is not applicable to our method, as well as [11], that require data to train the models. Hence following the experiment setting in [11], which randomly partitions the dataset into the non-overlapping training and test sets, each with half of the subjects. We test on Probe A, defined in [11], where the probe is different from the gallery by the viewpoint. We achieve the identification accuracy of 99.7 \u00b1 0.2%, which is better than 99.5\u00b10.2% of our preliminary work GaitNet-pre [29], the reported 96.7 \u00b1 0.5% of LB network [11], and 94.7 \u00b1 2.2% of multi-task GAN [72].\n\n\nFVG\n\nGiven that FVG is a newly collected database and no reported performance from prior work, we make the efforts to implement 4 classic or SOTA methods on gait recognition [8], [11], [13], [14]. Furthermore, given the large amount of effort in human pose estimation [18], aggregating joint locations over time can be a good candidate for gait features. Therefore we define another baseline, named PE-LSTM, using pose estimation results as the input to the same LSTM and classification loss as ours. Using SOTA 2D pose estimation [73], we extract 14 joints' locations, feed to the 3-layer-LSTM, and train with our proposed LSTM incremental loss. For each of 5 baselines and our GaitNet, one model is trained with the 136-subject training set and tested on all 5 protocols. As shown in Tab. 10, our method shows state-of-the-art performance compared with baselines, including the recent CNNbased methods. Among 5 protocols, CL is the most challenging variation as in CASIA-B. Comparing with all different methods, GEI based methods suffer from frontal view due to the lack of walking information. Again, thanks to the discriminative canonical feature f c , GaitNet achieves better recognition accuracies than GaitNet-pre. Also, the superior performance of our GaitNet over PE-LSTM demonstrates that our feature f p and f c does explore more discriminate information than the joints' locations alone.\n\n\nComparison to Face Recognition\n\nFace recognition aims to identify subjects by extracting discriminative identity features, or representation, from face images. Due to the vigorous development in the past few years, face recognition system is one of the most studied and deployed systems in the vision community, even superior to human on some tasks [74].\n\nHowever, the challenge is particularly prominent in the video surveillance scenario, where low-resolution and/or non-frontal faces are acquired at a distance. While gait, as a behavioral biometric compared to face, might have more advantages in those scenarios since the dynamic information can be more resistant even at a lower resolution and different viewing angles. Especially for GaitNet, f sta-gait and f dyn-gait can have complementary contributions in changing distances, resolutions and viewing angles. Therefore, to explore the advantages and disadvantages of gait recognition and face recognition in surveillance scenario, we compare our GaitNet with the most recent SOTA face recognition method, ArcFace [62], on the CASIA-B and FVG databases.\n\nSpecifically, for face recognition, we first employ SOTA face detection algorithm RetinaFace [75] to detect face and ArcFace to extract features for each frame of gallery and probe videos. Then the features over all frames of a video are aggregated by average pooling, an effective scheme used in prior video-based face recognition work [76]. We measure the similarity of features by their cosine distance. To keep consistency with above gait recognition experiments, both face and gait report TAR at 1% FAR for FVG and Rank-1 score for CASIA-B. To evaluate effects of time, we use the entire sequence as gallery and partial (e.g., 10%) sequence as probe on 10 points on the time axis ranging from 10% to 100%.\n\n\nGait vs. Face Recognition on CASIA-B\n\nIn this experiment, we select the videos of the NM as gallery and both CL and BG are probes. We compare gait and face recognition in three scenarios: frontal-frontal, side-side and sidefrontal viewing angles. Fig. 12 shows the Rank-1 scores over the time duration. As the video begins, GaitNet is significantly superior to face in all scenarios since our f sta-gait can capture discriminative information such as body shape in low-resolution images, as mentioned in Sec. 5.1.5, while faces are of too low resolution to perform meaningful recognition. As time progresses, GaitNet is stable to the resolution change and view variations, with increasing accuracy. In comparison, face recognition always has lower accuracies throughout the entire duration, except the frontalfrontal view face recognition slightly outperforms gait in the last 20% of the duration, which is expected as this is toward the ideal scenario for face recognition to shine. Unfortunately, for side-side or side-frontal views, face recognition continues to struggle even at the end of the duration.\n\n\nGait vs. Face Recognition on FVG\n\nWe further compare GaitNet with ArcFace on FVG with NM-BGHT and NM-ALL* protocols. Note that the videos of NM-BGHT contain variations in carrying bags and wearing hat. The videos of ALL*, different from ALL in Tab. 10, include all the variations in FVG except carrying and wearing hat variations (refer to Tab. 5 for details). As shown in Fig. 12, on the BGHT protocol, gait outperforms face in the entire duration, since wearing hat dramatically affects face recognition but not gait recognition. For ALL* protocol, face outperforms gait in the last 20% duration because by then low resolution is not an issue and FVG has frontalview faces. Figure 13 shows some examples in CASIB-B and FVG, which are incorrectly recognized by face recognition. We also show some images (video frames) for which our GaitNet fails to recognize in Fig. 14. The low resolution and illumination conditions in these videos are the main reasons for failure. Note that while video-based alignment [77], [78] or super-resolution approaches [79] might help to enhance the image quality, their impact to recognition is beyond the scope of this work.\n\n\nRuntime Speed\n\nSystem efficiency is an essential metric for many vision systems including gait recognition. We calculate the efficiency while each of the 5 gait recognition methods processing one video of FVG dataset on the same desktop with GeForce GTX 1080 Ti GPU. All the coding are implemented in PyTorch Framework of Python programming language. Parallel computing of batch processing is enabled for GPU on all the inference models, where batch size is number of samples in the probe. Alphapose and Mask-R-CNN takes batch size of 1 as input in inference. As shown in Tab. 11, our method is faster than the pose estimation method because of 1) an accurate, yet slow, version of AlphaPose [73] is required for model-based gait recognition method; 2) only low-resolution input of 32 \u00d7 64 pixels is needed for GaitNet. Further, our method has similar efficiency as the recent CNN-based gait recognition methods.  \n\n\nCONCLUSION\n\nThis paper presents an autoencoder-based method termed GaitNet that can disentangle appearance and gait feature representation from raw RGB frames, and utilize a multi-layer LSTM structure to further leverage temporal information to generate a gait representation for each video sequence. We compare our method extensively with the state of the arts on CASIA-B, USF, and our collected FVG datasets. The superior results show the generalization and promise of the proposed feature disentanglement approach. We hope that in the future, this disentanglement approach is a viable option for other vision problems where motion dynamics needs to be extracted while being invariant to confounding factors, e.g., expression recognition with invariance to facial appearance, activity recognition with invariance to clothing.\n\nFig. 1 :\n1(a) While conventional gait databases capture side-view imagery, we collect a new gait database (FVG) with focus on more challenging frontal views. We propose a novel CNN-based model, termed GaitNet, to directly learn the disentangled appearance, canonical and pose features from walking videos, as opposed to handcrafted GEI or skeleton features. (b) Given 2 videos of Subject 1 and 1 video of Subject 2, feature visualizations by our decoder in\n\nFig. 2 :\n2If we may ignore the differences in color/texture of clothing and the body pose, there are inherent body characteristics that are different across subjects (b)\n\nFig. 3 :\n3The overall architecture of proposed GaitNet. The bottom right block indicates the inference process, while the remaining illustrates the training process with the four color-coded loss functions.\n\nFig. 4 :\n4Examples of FVG Dataset. (a) Samples of the near frontal middle, left and right walking viewing angles in Session 1 (se1) of the first subject (s1). se3-s1 is the same subject in Session 3. (b) Samples of slow and fast walking speed for another subject in Session 1. Frames in the second row are normal and in the third row are fast walking. Carrying bag and wearing hat sample is shown below. (c) Samples of changing clothes and with multiple people background from one subject in Session 2.GaitNet is obtained by pixel-wise multiplication between the mask and the [0, 1]-normalized RGB values, and then resizing to 32 \u00d7 64 pixels. This applies to all the experiments on CASIA-B, USF and FVG datasets in Sec. 5.\n\n\n16 meters away from the camera, which results in 12 videos per subject. The videos are captured at 1, 080 \u00d7 1, 920 resolution with 15 FPS and the average length of 10 seconds. The height of body in the video ranges from 101 to 909 pixels, and the height of faces ranges from 17 to 467 pixels. These 12 walks have the combination of three angles toward the camera (\u221245 \u2022 , 0 \u2022 , 45 \u2022 off the optical axes of the camera), and four variations. As detailed in Tab. 5, FVG is collected in three sessions with five variations: normal, walking speed (slow and fast), clothing changes, carrying/wearing change (bag or hat), and clutter background (multiple persons). The five variations are well balanced in three sessions.Fig. 4shows exemplar images from FVG.\n\n\nThe 5 protocols differ in their respective probe data, which cover the variations of Walking Speed (WS), Carrying Bag while Wearing a Hat (BGHT), Changing Clothes (CL), Multiple Persons (MP), and all variations (ALL). At the top part of Tab. 5, we list the detailed probe sets for all 5 protocols. For instance, for the WS protocol, the probes are video 4\u22129 in Session 1 and video 4\u22126 in Session 2. In all protocols, the performance metrics are the True Accept Rate (TAR) at 1% and 5% False Alarm Rate (FAR).\n\nFig. 5 :\n5Synthesis by decoding three features individually, fa, fc and fp, and their concatenation. Left and right parts are two learnt models on frontal and side views of CASIA-B. The top two rows are two frames of the same subject under different conditions (NM vs. CL) and the bottom two are another subject. The reconstructed framesx closely match the original input. fc shows consistent body shape for the same subject while different for different subjects. fa recovers the appearance of clothes, at the pose specified by fc. The body pose of fp matches with the input frame.\n\nFig. 6 :\n6Synthesis by decoding pairs of pose features fp and poseirrelevant features, {fa, fc}. Left and right parts are examples of frontal and side views of CASIA-B. In either part, each of 4 \u00d7 4 synthetic images is D(f l a , f l c , f t p ), where {f l a , f l c } is extracted from images in the first column and f t p is from the top row. The synthetic images resemble the appearance of the first column and the pose of the top row.\n\nFig. 7 :\n7The t-SNE visualization of (a) appearance features fa, (b) canonical features fc, (c) pose features fp, and (d)\n\nFig. 8 :\n8Synthesis on CASIA-B by decoding pose-irrelevant feature {fa, fc} and pose feature fp from videos under NM vs. CL conditions. Left and right parts are two examples. For each example, {fa, fc} is extracted from the first column (CL) and fp is from the top row (NM). Top row synthetic images are generated from model trained without \u0141 pose-sim loss, bottom row is with the loss. To show the difference, details in synthetic images are magnified.\n\nFig. 11 :\n11Recognition performance at different video lengths. We use different feature scores (f sta-gait , f dyn-gait , and their fusion) on NM-CL,BG conditions of CAISA-B. (a) is on frontal-frontal view and (b)\n\nFig. 12 :\n12Comparison of gait and face recognition on CASIA-B and FVG. Classification accuracy scores along with video duration percentage are calculated. (a) In CASIA-B, both gait and face recognition are performed in three scenarios: frontal-frontal (0 \u2022 vs. 0 \u2022 ), side-side (90 \u2022 vs. 90 \u2022 ) and frontal-side (0 \u2022 vs. 90 \u2022 ). (b) In FVG, both recognitions use NM vs. BGHT and NM vs. ALL* protocols. Detected face examples are shown on the top of each frontal and side view plots under various video duration percentage.\n\nFig. 13 :\n13Examples in CASIA-B and FVG where the SOTA face recognizer ArcFace fails. The first row is the image of probe set; the second row is the recognized wrong person in gallery; and the third row shows the genuine gallery. The first three columns are three scenarios of CASIA-B and the last two columns are two protocols of FVG.\n\nFig. 14 :\n14Failure cases of GaitNet on CASIB-B and FVG due to blurry and illumination conditions. The rows and columns are defined the same asFig. 13.\n\n\nXiaoming Liu are with the Department of Computer Science and Engineering, Michigan State University. E-mail: {zhang835, tranluan, liufeng6}@msu.edu, liuxm@cse.msu.edu\n\nTABLE 1 :\n1Comparison of existing gait databases and our collected FVG database.Dataset \n#Subjects #Videos Environment FPS \nResolution \nFormat \nVariations \n\nCASIA-B [30] \n124 \n13, 640 \nIndoor \n25 \n320\u00d7240 \nRGB \nView, Clothing, Carrying \nUSF [9] \n122 \n1, 870 \nOutdoor \n30 \n720\u00d7480 \nRGB \nView, Ground Surface, Shoes, Carrying, Time \nOU-ISIR-LP [31] \n4, 007 \n-\nIndoor \n-\n640\u00d7480 \nSilhouette View \nOU-ISIR-LP-Bag [32] \n62, 528 \n-\nIndoor \n-\n1, 280\u00d7980 \nSilhouette Carrying \nFVG (ours) \n226 \n2, 856 \nOutdoor \n15 \n1, 920\u00d71, 080 \nRGB \nView, Walking Speed, Carrying, Clothing, Multiple people, Time \n\n\n\nTABLE 2 :\n2Symbols and notations.Symbol \nDim. \nNotation \n\ns \nscalar \nIndex of subject \nc \nscalar \nCondition \nt \nscalar \nTime step in a video \nn \nscalar \nNumber of frames in a video \nX c \nmatrices \nGait video under condition c \nx c,t \nmatrix \nFrame t of video X \u0109 \nx \nmatrix \nReconstructed frame via D \nE \n-\nEncoder network \nD \n-\nDecoder network \nC sg \n-\nClassifier for fc \nC dg \n-\nClassifier for f dyn-gait \nfp \n64 \u00d7 1 \nPose feature \nfc \n128 \u00d7 1 \nCanonical feature \nfa \n128 \u00d7 1 \nAppearance feature \nf dyn-gait \n256 \u00d7 1 \nDynamic gait feature \nf sta-gait \n128 \u00d7 1 \nStatic gait feature \nh t \n128 \u00d7 1 \nThe output of LSTM at step t \n\u0141xrecon \n-\nReconstruction loss \n\u0141 pose-sim \n-\nPose similarity loss \n\u0141 cano-sim \n-\nCanonical similarity loss \n\u0141 id-inc-avg \n-\nIncremental identity loss \n\n\n\nTABLE 3 :\n3\n\nTABLE 4 :\n4The architecture of E and D networks.Note the layer with \n\n\nTABLE 5 :\n5The FVG database. The last 5 rows show the specific variations that are captured by each of 12 videos per subject.Collection Year \n2017 \n2018 \nSession \n1 \n2 \n3 \nNumber of Subjects \n147 \n79 \n12 \nViewing Angle ( \u2022 ) \n-45 \n0 \n45 \n-45 \n0 \n45 \n-45 \n0 \n45 \nNormal \n1 \n2 \n3 \n1 \n2 \n3 \n1 \n2 \n3 \nFast / Slow Walking \n4/7 \n5/8 \n6/9 \n4 \n5 \n6 \n4 \n5 \n6 \nCarrying Bag / Hat \n10 \n11 \n12 \n-\n-\n-\n-\n-\n-\nChange Clothes \n-\n-\n-\n7 \n8 \n9 \n7 \n8 \n9 \nMultiple Person \n-\n-\n-\n10 \n11 \n12 \n10 \n11 \n12 \n\n\n\nTABLE 6 :\n6Ablation study on various options of the disentanglement loss, classification loss, and classification features. A GaitNet model is trained on NM and CL conditions of lateral view with the first 74 subjects of CASIA-B and tested on remaining subjects.Disentanglement Loss \nClassification Loss Classification Feature Rank-1 \n\n-\n\u0141id-inc-avg \nfdyn-gait \n56.0 \n\u0141xrecon \n\u0141id-inc-avg \nfdyn-gait \n60.2 \n\u0141xrecon + \u0141pose-sim \n\u0141id-inc-avg \nfdyn-gait \n85.6 \n\u0141xrecon + \u0141pose-sim + \u0141cano-sim \n\u0141id-single \nfdyn-gait & fsta-gait \n72.5 \n\u0141xrecon + \u0141pose-sim + \u0141cano-sim \n\u0141id-ae [64] \nfdyn-gait & fsta-gait \n76.5 \n\u0141xrecon + \u0141pose-sim + \u0141cano-sim \n\u0141id-avg \nfdyn-gait & fsta-gait \n82.6 \n\u0141xrecon + \u0141pose-sim + \u0141cano-sim \n\u0141id-inc-avg \nfa \n33.4 \n\u0141xrecon + \u0141pose-sim + \u0141cano-sim \n\u0141id-inc-avg \nfsta-gait \n76.3 \n\u0141xrecon + \u0141pose-sim + \u0141cano-sim \n\u0141id-inc-avg \nfdyn-gait \n85.9 \n\u0141xrecon + \u0141pose-sim + \u0141cano-sim \n\u0141id-inc-avg \nfdyn-gait & fsta-gait \n92.1 \n\n(a) \n(b) \n(c) \n\n(d) \n(e) \n(f) \n\n\n\nTABLE 7 :\n7Comparison on CASIA-B with cross view and conditions. Three models are trained for NM-NM, NM-BG, NM-CL. Average accuracies are calculated excluding probe viewing angles.Gallery NM #1-4 \n0 \u2022 -180 \u2022 (exclude identical viewing angle) \n\nProbe NM #5-6 \n0 \u2022 \n18 \u2022 \n36 \u2022 \n54 \u2022 \n72 \u2022 \n90 \u2022 \n108 \u2022 \n126 \u2022 \n144 \u2022 \n162 \u2022 \n180 \u2022 \nMean \n\nViDP [65] \n-\n-\n-\n64.2 \n-\n60.4 \n-\n65.0 \n-\n-\n-\n-\nLB [11] \n82.6 \n90.3 \n96.1 \n94.3 \n90.1 \n87.4 \n89.9 \n94.0 \n94.7 \n91.3 \n78.5 \n89.9 \n3D MT network [11] \n87.1 \n93.2 \n97.0 \n94.6 \n90.2 \n88.3 \n91.1 \n93.8 \n96.5 \n96.0 \n85.7 \n92.1 \nJ-CNN [34] \n87.2 \n93.2 \n96.3 \n95.9 \n91.6 \n86.5 \n89.8 \n93.8 \n95.1 \n93.0 \n80.8 \n91.2 \nGaitNet-pre [29] \n91.2 \n92.0 \n90.5 \n95.6 \n86.9 \n92.6 \n93.5 \n96.0 \n90.9 \n88.8 \n89 \n91.6 \nGaitNet \n93.1 92.6 90.8 92.4 87.6 95.1 \n94.2 \n95.8 \n92.6 \n90.4 \n90.2 \n92.3 \nProbe BG #1-2 \n0 \u2022 \n18 \u2022 \n36 \u2022 \n54 \u2022 \n72 \u2022 \n90 \u2022 \n108 \u2022 \n126 \u2022 \n144 \u2022 \n162 \u2022 \n180 \u2022 \nMean \n\nLB-subGEI [11] \n64.2 \n80.6 \n82.7 \n76.9 \n64.8 \n63.1 \n68.0 \n76.9 \n82.2 \n75.4 \n61.3 \n72.4 \nJ-CNN [34] \n73.1 \n78.1 \n83.1 \n81.6 \n71.6 \n65.5 \n71.0 \n80.7 \n79.1 \n78.6 \n68.0 \n75.0 \nGaitNet-pre [29] \n83.0 \n87.8 \n88.3 \n93.3 \n82.6 \n74.8 \n89.5 \n91.0 \n86.1 \n81.2 \n85.6 \n85.7 \nGaitNet \n88.8 88.7 88.7 94.3 85.4 92.7 \n91.1 \n92.6 \n84.9 \n84.4 \n86.7 \n88.9 \nProbe CL #1-2 \n0 \u2022 \n18 \u2022 \n36 \u2022 \n54 \u2022 \n72 \u2022 \n90 \u2022 \n108 \u2022 \n126 \u2022 \n144 \u2022 \n162 \u2022 \n180 \u2022 \nMean \n\nLB-subGEI [11] \n37.7 \n57.2 \n66.6 \n61.1 \n55.2 \n54.6 \n55.2 \n59.1 \n58.9 \n48.8 \n39.4 \n53.98 \nJ-CNN [34] \n46.1 \n58.4 \n64.4 \n64.2 \n55.5 \n50.5 \n54.7 \n55.8 \n53.3 \n51.3 \n39.9 \n54.01 \nGaitNet-pre [29] \n42.1 \n58.2 \n65.1 \n70.7 \n68.0 \n70.6 \n65.3 \n69.4 \n51.5 \n50.1 \n36.6 \n58.9 \nGaitNet \n50.1 60.7 72.4 72.1 74.6 78.4 \n70.3 \n68.2 \n53.5 \n44.1 \n40.8 \n62.3 \n\n\n\nTABLE 8 :\n8Recognition accuracy cross views under NM on CASIA-B dataset. One single GaitNet model is trained for all the viewing angles.Methods \n0 \u2022 \n18 \u2022 \n36 \u2022 \n54 \u2022 \n72 \u2022 \n108 \u2022 \n126 \u2022 \n144 \u2022 \n162 \u2022 \n180 \u2022 \nAverage \n\nCPM [33] \n13 \n14 \n17 \n27 \n62 \n65 \n22 \n20 \n15 \n10 \n24.1 \nGEI-SVR [60] \n16 \n22 \n35 \n63 \n95 \n95 \n65 \n38 \n20 \n13 \n42.0 \nCMCC [66] \n18 \n24 \n41 \n66 \n96 \n95 \n68 \n41 \n21 \n13 \n43.9 \nViDP [65] \n8 \n12 \n45 \n80 \n100 \n100 \n81 \n50 \n15 \n8 \n45.4 \nSTIP+NN [61] \n-\n-\n-\n-\n84.0 \n86.4 \n-\n-\n-\n-\n-\nLB [11] \n18 \n36 \n67.5 \n93 \n99.5 \n99.5 \n92 \n66 \n36 \n18 \n56.9 \nL-CRF [33] \n38 \n75 \n68 \n93 \n98 \n99 \n93 \n67 \n76 \n39 \n67.8 \nGaitNet-pre [29] \n68 \n74 \n88 \n91 \n99 \n98 \n84 \n75 \n76 \n65 \n81.8 \nGaitNet \n82 \n83 \n86 \n91 \n93 \n98 \n92 \n90 \n79 \n79 \n87.3 \n\n\n\nTABLE 9 :\n9Comparison focuses on appearance variations. Training sets have videos under BG and CL. There are 34 subjects in total with 54 \u2022 to 144 \u2022 viewing angles. Different test sets are made with the different combination of viewing angles of the gallery and probe as well as the appearance condition (BG or CL). The results are presented in Tab. 9. Our preliminary work has comparable performance as the SOTA method L-CRF [33] on BG subset while significantlywith [33] and [11] under different walking \nconditions on CASIA-B by accuracies. One single GaitNet model is \ntrained with all gallery and probe views and the two conditions. \n\nProbe Gallery GaitNet \nGaitNet-pre \n[29] \n\nJUCNet \n[21] \n\nL-CRF \n[33] \n\nLB \n[11] \n\nRLTDA \n[67] \n\nSubset \nBG \n54 \n36 \n93.5 \n91.6 \n91.8 \n93.8 \n92.7 \n80.8 \n54 \n72 \n94.1 \n90.0 \n93.9 \n91.2 \n90.4 \n71.5 \n90 \n72 \n98.6 \n95.6 \n95.9 \n94.4 \n93.3 \n75.3 \n90 \n108 \n99.3 \n87.4 \n95.9 \n89.2 \n88.9 \n76.5 \n126 \n108 \n99.5 \n90.1 \n93.9 \n92.5 \n93.3 \n66.5 \n126 \n144 \n90.0 \n93.8 \n87.8 \n88.1 \n86.0 \n72.3 \nMean \n95.8 \n91.4 \n93.2 \n91.5 \n90.8 \n73.8 \n\nSubset \nCL \n54 \n36 \n97.5 \n87.0 \n-\n59.8 \n49.7 \n69.4 \n54 \n72 \n98.6 \n90.0 \n-\n72.5 \n62.0 \n57.8 \n90 \n72 \n99.3 \n94.2 \n-\n88.5 \n78.3 \n63.2 \n90 \n108 \n99.6 \n86.5 \n-\n85.7 \n75.6 \n72.1 \n126 \n108 \n98.3 \n89.8 \n-\n68.8 \n58.1 \n64.6 \n126 \n144 \n86.6 \n91.2 \n-\n62.5 \n51.4 \n64.2 \nMean \n96.7 \n89.8 \n-\n73.0 \n62.5 \n65.2 \n\ndifferent views are learned in f c , which contributes to the final \nrecognition accuracy. \nProtocol 3 \n\nTABLE 10 :\n10Definition of FVG protocols and performance comparison. Under each of the 5 protocols, the first/second columns indicate the indexes of videos used in gallery/probe.Protocol \nWS \nBGHT \nCL \nMP \nALL \n\nIndex of Gallery & Probe videos \n\nSession 1 \n2 \n4-9 \n2 \n10-12 \n-\n-\n-\n-\n2 \n1,3-12 \nSession 2 \n2 \n4-6 \n-\n-\n2 \n7-9 \n2 \n10-12 \n2 \n1,3-12 \nSession 3 \n-\n-\n-\n-\n-\n-\n-\n-\n-\n1 \u2212 12 \n\nTAR@FAR \n1% \n5% \n1% \n5% \n1% \n5% \n1% \n5% \n1% \n5% \n\nPE-LSTM \n79.3 \n87.3 \n59.1 \n78.6 \n55.4 \n67.5 \n61.6 \n72.2 \n65.4 \n74.1 \nGEI [8] \n9.4 \n19.5 \n6.1 \n12.5 \n5.7 \n13.2 \n6.3 \n16.7 \n5.8 \n16.1 \nGEINet [14] \n15.5 \n35.2 \n11.8 \n24.7 \n6.5 \n16.7 \n17.3 \n35.2 \n13.0 \n29.2 \nDCNN [13] \n11.0 \n23.6 \n5.7 \n12.7 \n7.0 \n15.9 \n8.1 \n20.9 \n7.9 \n19.0 \nLB [11] \n53.4 \n73.1 \n23.1 \n50.3 \n23.2 \n38.5 \n56.1 \n74.3 \n40.7 \n61.6 \nGaitNet-pre [29] \n91.8 \n96.6 \n74.2 \n85.1 \n56.8 \n72.0 \n92.3 \n97.0 \n81.2 \n87.8 \nGaitNet \n96.2 \n97.5 \n92.3 \n96.4 \n70.4 87.5 \n92.5 \n96.0 \n91.9 \n96.3 \n\n\n\nTABLE 11 :\n11Runtime (ms per frame) comparison on FVG dataset.Methods \nPre-processing Inference \nTotal \n\nPE-LSTM \n224.4 \n0.1 \n224.5 \nGEINet [14] \n89.5 \n1.5 \n91.0 \nDCNN [13] \n89.5 \n1.7 \n91.2 \nLB [11] \n89.5 \n1.3 \n90.8 \nGaitNet (ours) \n89.5 \n1.0 \n90.5 \n\n\nACKNOWLEDGMENTSThis work was partially sponsored by the Ford-MSU Alliance program, and the Army Research Office under Grant Number W911NF-18-1-0330. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.His main research interests focus on computer vision and pattern recognition, specifically for 3D modeling, 2D and 3D face recognition.\nHuman Identification Based on Gait. M S Nixon, T Tan, R Chellappa, Springer Science & Business MediaM. S. Nixon, T. Tan, and R. Chellappa, Human Identification Based on Gait. Springer Science & Business Media, 2010.\n\nA survey on gait recognition. C Wan, L Wang, V V Phoha, ACM Computing Surveys (CSUR). 51535C. Wan, L. Wang, and V. V. Phoha, \"A survey on gait recognition,\" ACM Computing Surveys (CSUR), vol. 51, no. 5, pp. 89:1-89:35, 2018.\n\nEV-Gait: Event-based robust gait recognition using dynamic vision sensors. Y Wang, B Du, Y Shen, K Wu, G Zhao, J Sun, H Wen, Computer Vision and Pattern Recognition (CVPR). Y. Wang, B. Du, Y. Shen, K. Wu, G. Zhao, J. Sun, and H. Wen, \"EV-Gait: Event-based robust gait recognition using dynamic vision sensors,\" in Computer Vision and Pattern Recognition (CVPR), 2019.\n\nRobust gait recognition by integrating inertial and RGBD sensors. Q Zou, L Ni, Q Wang, Q Li, S Wang, IEEE Transactions on Cybernetics. 484Q. Zou, L. Ni, Q. Wang, Q. Li, and S. Wang, \"Robust gait recognition by integrating inertial and RGBD sensors,\" IEEE Transactions on Cybernetics, vol. 48, no. 4, pp. 1136-1150, 2017.\n\nAccelerometerbased gait recognition by sparse representation of signature points with clusters. Y Zhang, G Pan, K Jia, M Lu, Y Wang, Z Wu, IEEE Transactions on Cybernetics. 459Y. Zhang, G. Pan, K. Jia, M. Lu, Y. Wang, and Z. Wu, \"Accelerometer- based gait recognition by sparse representation of signature points with clusters,\" IEEE Transactions on Cybernetics, vol. 45, no. 9, pp. 1864- 1875, 2014.\n\nA floor sensor system for gait recognition. L Middleton, A A Buss, A Bazin, M S Nixon, Workshop on Automatic Identification Advanced Technologies (AutoID). L. Middleton, A. A. Buss, A. Bazin, and M. S. Nixon, \"A floor sensor system for gait recognition,\" in Workshop on Automatic Identification Advanced Technologies (AutoID), 2005.\n\nGait recognition using WiFi signals. W Wang, A X Liu, M Shahzad, Pervasive and Ubiquitous Computing (UbiComp). W. Wang, A. X. Liu, and M. Shahzad, \"Gait recognition using WiFi signals,\" in Pervasive and Ubiquitous Computing (UbiComp), 2016.\n\nIndividual Recognition Using Gait Energy Image. J Han, B Bhanu, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). 282J. Han and B. Bhanu, \"Individual Recognition Using Gait Energy Image,\" IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), vol. 28, no. 2, pp. 316-322, 2005.\n\nThe Human ID Gait Challenge Problem: Data Sets, Performance, and Analysis. S Sarkar, P J Phillips, Z Liu, I R Vega, P Grother, K W Bowyer, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). 272S. Sarkar, P. J. Phillips, Z. Liu, I. R. Vega, P. Grother, and K. W. Bowyer, \"The Human ID Gait Challenge Problem: Data Sets, Performance, and Analysis,\" IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), vol. 27, no. 2, pp. 162-177, 2005.\n\nGait Recognition Using Gait Entropy Image. K Bashir, T Xiang, S Gong, International Conference on Imaging for Crime Detection and Prevention (ICDP). K. Bashir, T. Xiang, and S. Gong, \"Gait Recognition Using Gait Entropy Image,\" in International Conference on Imaging for Crime Detection and Prevention (ICDP), 2010.\n\nA Comprehensive Study on Cross-View Gait Based Human Identification with Deep CNN. Z Wu, Y Huang, L Wang, X Wang, T Tan, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). 392Z. Wu, Y. Huang, L. Wang, X. Wang, and T. Tan, \"A Comprehensive Study on Cross-View Gait Based Human Identification with Deep CNN,\" IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), vol. 39, no. 2, pp. 209-226, 2016.\n\nClothing-invariant gait identification using part-based clothing categorization and adaptive weight control. M A Hossain, Y Makihara, J Wang, Y Yagi, Pattern Recognition. 436M. A. Hossain, Y. Makihara, J. Wang, and Y. Yagi, \"Clothing-invariant gait identification using part-based clothing categorization and adaptive weight control,\" Pattern Recognition, vol. 43, no. 6, pp. 2281-2291, 2010.\n\nImproved Gait recognition based on specialized deep convolutional neural networks. M Alotaibi, A Mahmood, Computer Vision and Image Understanding (CVIU). 164M. Alotaibi and A. Mahmood, \"Improved Gait recognition based on specialized deep convolutional neural networks,\" Computer Vision and Image Understanding (CVIU), vol. 164, pp. 103-110, 2017.\n\nGEINet: View-Invariant Gait Recognition Using a Convolutional Neural Network. K Shiraga, Y Makihara, D Muramatsu, T Echigo, Y Yagi, International Conference on Biometrics (ICB). K. Shiraga, Y. Makihara, D. Muramatsu, T. Echigo, and Y. Yagi, \"GEINet: View-Invariant Gait Recognition Using a Convolutional Neural Network,\" in International Conference on Biometrics (ICB), 2016.\n\nGaitGanv2: Invariant gait feature extraction using generative adversarial networks. S Yu, R Liao, W An, H Chen, E B G Reyes, Y Huang, N Poh, Pattern Recognition. 87S. Yu, R. Liao, W. An, H. Chen, E. B. G. Reyes, Y. Huang, and N. Poh, \"GaitGanv2: Invariant gait feature extraction using generative adversarial networks,\" Pattern Recognition, vol. 87, pp. 179-189, 2019.\n\nMarionette mass-spring model for 3D gait biometrics. G Ariyanto, M S Nixon, International Conference on Biometrics (ICB). G. Ariyanto and M. S. Nixon, \"Marionette mass-spring model for 3D gait biometrics,\" in International Conference on Biometrics (ICB), 2012.\n\nSkeleton-based gait recognition via robust Frame-level matching. S Choi, J Kim, W Kim, C Kim, IEEE Transactions on Information Forensics and Security. 1410S. Choi, J. Kim, W. Kim, and C. Kim, \"Skeleton-based gait recognition via robust Frame-level matching,\" IEEE Transactions on Information Forensics and Security, vol. 14, no. 10, pp. 2577-2592, 2019.\n\nLearning Effective Gait Features Using LSTM. Y Feng, Y Li, J Luo, International Conference on Pattern Recognition (ICPR). Y. Feng, Y. Li, and J. Luo, \"Learning Effective Gait Features Using LSTM,\" in International Conference on Pattern Recognition (ICPR), 2016.\n\nGait Recognition Using Static, Activity-Specific Parameters. A F Bobick, A Y Johnson, Computer Vision and Pattern Recognition (CVPR). A. F. Bobick and A. Y. Johnson, \"Gait Recognition Using Static, Activity- Specific Parameters,\" in Computer Vision and Pattern Recognition (CVPR), 2001.\n\nAutomatic extraction and description of human gait models for recognition purposes. D Cunado, M S Nixon, J N Carter, Computer Vision and Image Understanding (CVIU). 901D. Cunado, M. S. Nixon, and J. N. Carter, \"Automatic extraction and description of human gait models for recognition purposes,\" Computer Vision and Image Understanding (CVIU), vol. 90, no. 1, pp. 1-41, 2003.\n\nLearning joint gait representation via quintuplet loss minimization. K Zhang, W Luo, L Ma, W Liu, H Li, Computer Vision and Pattern Recognition (CVPR). K. Zhang, W. Luo, L. Ma, W. Liu, and H. Li, \"Learning joint gait representation via quintuplet loss minimization,\" in Computer Vision and Pattern Recognition (CVPR), 2019.\n\nGait recognition by deformable registration. Y Makihara, D Adachi, C Xu, Y Yagi, Computer Vision and Pattern Recognition Workshops (CVPRW). Y. Makihara, D. Adachi, C. Xu, and Y. Yagi, \"Gait recognition by deformable registration,\" in Computer Vision and Pattern Recognition Workshops (CVPRW), 2018.\n\nGeneral tensor discriminant analysis and gabor features for gait recognition. D Tao, X Li, X Wu, S J Maybank, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). 2910D. Tao, X. Li, X. Wu, and S. J. Maybank, \"General tensor discriminant analysis and gabor features for gait recognition,\" IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), vol. 29, no. 10, pp. 1700-1715, 2007.\n\nJoint Intensity and Spatial Metric Learning for Robust Gait Recognition. Y Makihara, A Suzuki, D Muramatsu, X Li, Y Yagi, Computer Vision and Pattern Recognition (CVPR. Y. Makihara, A. Suzuki, D. Muramatsu, X. Li, and Y. Yagi, \"Joint Intensity and Spatial Metric Learning for Robust Gait Recognition,\" in Computer Vision and Pattern Recognition (CVPR), 2017.\n\nGait Energy Volumes and Frontal Gait Recognition using Depth Images. S Sivapalan, D Chen, S Denman, S Sridharan, C Fookes, International Joint Conference on Biometrics (IJCB). S. Sivapalan, D. Chen, S. Denman, S. Sridharan, and C. Fookes, \"Gait Energy Volumes and Frontal Gait Recognition using Depth Images,\" in International Joint Conference on Biometrics (IJCB), 2011.\n\nPose Depth Volume extraction from RGB-D streams for frontal gait recognition. P Chattopadhyay, A Roy, S Sural, J Mukhopadhyay, Journal of Visual Communication and Image Representation. 251P. Chattopadhyay, A. Roy, S. Sural, and J. Mukhopadhyay, \"Pose Depth Volume extraction from RGB-D streams for frontal gait recognition,\" Journal of Visual Communication and Image Representation, vol. 25, no. 1, pp. 53-63, 2014.\n\nFrontal Gait Recognition From Incomplete Sequences Using RGB-D Camera. P Chattopadhyay, S Sural, J Mukherjee, IEEE Transactions on Information Forensics and Security. 911P. Chattopadhyay, S. Sural, and J. Mukherjee, \"Frontal Gait Recognition From Incomplete Sequences Using RGB-D Camera,\" IEEE Transactions on Information Forensics and Security, vol. 9, no. 11, pp. 1843-1856, 2014.\n\nFrontal Gait Recognition Combining 2D and 3D Data. A M Nambiar, P L Correia, L D Soares, ACM Workshop on Multimedia and Security. A. M. Nambiar, P. L. Correia, and L. D. Soares, \"Frontal Gait Recognition Combining 2D and 3D Data,\" in ACM Workshop on Multimedia and Security, 2012.\n\nGait Recognition via Disentangled Representation Learning. Z Zhang, L Tran, X Yin, Y Atoum, X Liu, J Wan, N Wang, Computer Vision and Pattern Recognition (CVPR). Z. Zhang, L. Tran, X. Yin, Y. Atoum, X. Liu, J. Wan, and N. Wang, \"Gait Recognition via Disentangled Representation Learning,\" in Computer Vision and Pattern Recognition (CVPR), 2019.\n\nA Framework for Evaluating the Effect of View Angle, Clothing and Carrying Condition on Gait Recognition. S Yu, D Tan, T Tan, International Conference on Pattern Recognition (ICPR). S. Yu, D. Tan, and T. Tan, \"A Framework for Evaluating the Effect of View Angle, Clothing and Carrying Condition on Gait Recognition,\" in International Conference on Pattern Recognition (ICPR), 2006.\n\nThe OU-ISIR Gait Database Comprising the Large Population Dataset and Performance Evaluation of Gait Recognition. H Iwama, M Okumura, Y Makihara, Y Yagi, IEEE Transactions on Information Forensics and Security. 75H. Iwama, M. Okumura, Y. Makihara, and Y. Yagi, \"The OU-ISIR Gait Database Comprising the Large Population Dataset and Performance Evaluation of Gait Recognition,\" IEEE Transactions on Information Forensics and Security, vol. 7, no. 5, pp. 1511-1521, 2012.\n\nY Makihara, H Mannami, A Tsuji, M A Hossain, K Sugiura, A Mori, Y Yagi, The OU-ISIR Gait Database Comprising the Treadmill Dataset. 4Y. Makihara, H. Mannami, A. Tsuji, M. A. Hossain, K. Sugiura, A. Mori, and Y. Yagi, \"The OU-ISIR Gait Database Comprising the Treadmill Dataset,\" IPSJ Transactions on Computer Vision and Applications, vol. 4, pp. 53-62, 2012.\n\nMulti-Gait Recognition Based on Attribute Discovery. X Chen, J Weng, W Lu, J Xu, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). 40X. Chen, J. Weng, W. Lu, and J. Xu, \"Multi-Gait Recognition Based on Attribute Discovery,\" IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), vol. 40, no. 7, pp. 1697-1710, 2017.\n\nA comprehensive study on gait biometrics using a joint CNN-based method. Y Zhang, Y Huang, L Wang, S Yu, Pattern Recognition. 93Y. Zhang, Y. Huang, L. Wang, and S. Yu, \"A comprehensive study on gait biometrics using a joint CNN-based method,\" Pattern Recognition, vol. 93, pp. 228-236, 2019.\n\nOn a Large Sequence-Based Human Gait Database. J D Shutler, M G Grant, M S Nixon, J N Carter, Applications and Science in Soft Computing. J. D. Shutler, M. G. Grant, M. S. Nixon, and J. N. Carter, \"On a Large Sequence-Based Human Gait Database,\" in Applications and Science in Soft Computing, 2004.\n\nThe TUM Gait from Audio, Image and Depth (GAID) database: Multimodal recognition of subjects and traits. M Hofmann, J Geiger, S Bachmann, B Schuller, G Rigoll, Journal of Visual Communication and Image Representation. 251M. Hofmann, J. Geiger, S. Bachmann, B. Schuller, and G. Rigoll, \"The TUM Gait from Audio, Image and Depth (GAID) database: Multimodal recognition of subjects and traits,\" Journal of Visual Communication and Image Representation, vol. 25, no. 1, pp. 195-206, 2014.\n\nNonlinear 3D Face Morphable Model. L Tran, X Liu, Computer Vision and Pattern Recognition (CVPR). L. Tran and X. Liu, \"Nonlinear 3D Face Morphable Model,\" in Computer Vision and Pattern Recognition (CVPR), 2018.\n\nOn learning 3D face morphable model from in-the-wild images. file:/localhost/opt/grobid/grobid-home/tmp/10.1109/tpami.2019.2927975IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). --, \"On learning 3D face morphable model from in-the-wild images,\" IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2019, doi: 10.1109/tpami.2019.2927975.\n\nTowards High-fidelity Nonlinear 3D Face Morphable Model. L Tran, F Liu, X Liu, Computer Vision and Pattern Recognition (CVPR). L. Tran, F. Liu, and X. Liu, \"Towards High-fidelity Nonlinear 3D Face Morphable Model,\" in Computer Vision and Pattern Recognition (CVPR), 2019.\n\nDisentangling Features in 3D Face Shapes for Joint Face Reconstruction and Recognition. F Liu, D Zeng, Q Zhao, X Liu, Computer Vision and Pattern Recognition (CVPR). F. Liu, D. Zeng, Q. Zhao, and X. Liu, \"Disentangling Features in 3D Face Shapes for Joint Face Reconstruction and Recognition,\" in Computer Vision and Pattern Recognition (CVPR), 2018.\n\nUnsupervised Learning of Disentangled Representations from Video. E Denton, B Vighnesh, Neural Information Processing Systems (NeurIPS). E. Denton and B. Vighnesh, \"Unsupervised Learning of Disentangled Representations from Video,\" in Neural Information Processing Systems (NeurIPS), 2017.\n\nSynthesizing Images of Humans in Unseen Poses. G Balakrishnan, A Zhao, A V Dalca, F Durand, J Guttag, Computer Vision and Pattern Recognition (CVPR). G. Balakrishnan, A. Zhao, A. V. Dalca, F. Durand, and J. Guttag, \"Synthesizing Images of Humans in Unseen Poses,\" in Computer Vision and Pattern Recognition (CVPR), 2018.\n\nU-Net: Convolutional Networks for Biomedical Image Segmentation. O Ronneberger, P Fischer, T Brox, International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI). O. Ronneberger, P. Fischer, and T. Brox, \"U-Net: Convolutional Networks for Biomedical Image Segmentation,\" in International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2015.\n\nA Variational U-Net for Conditional Appearance and Shape Generation. P Esser, E Sutter, B Ommer, Computer Vision and Pattern Recognition (CVPR). P. Esser, E. Sutter, and B. Ommer, \"A Variational U-Net for Conditional Appearance and Shape Generation,\" in Computer Vision and Pattern Recognition (CVPR), 2018.\n\nAuto-encoding variational bayes. D P Kingma, M Welling, arXiv:1312.6114arXiv preprintD. P. Kingma and M. Welling, \"Auto-encoding variational bayes,\" arXiv preprint arXiv:1312.6114, 2013.\n\nDisentangled Representation Learning GAN for Pose-Invariant Face Recognition. L Tran, X Yin, X Liu, Computer Vision and Pattern Recognition (CVPR). L. Tran, X. Yin, and X. Liu, \"Disentangled Representation Learning GAN for Pose-Invariant Face Recognition,\" in Computer Vision and Pattern Recognition (CVPR), 2017.\n\nRepresentation Learning by Rotating Your Faces. file:/localhost/opt/grobid/grobid-home/tmp/10.1109/tpami.2018.2868350IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). --, \"Representation Learning by Rotating Your Faces,\" IEEE Transac- tions on Pattern Analysis and Machine Intelligence (TPAMI), 2018, doi: 10.1109/tpami.2018.2868350.\n\nGenerative Adversarial Nets. I Goodfellow, J Pouget-Abadie, M Mirza, B Xu, D Warde-Farley, S Ozair, A Courville, Y Bengio, Neural Information Processing Systems (NeurIPS). I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, \"Generative Adversarial Nets,\" in Neural Information Processing Systems (NeurIPS), 2014.\n\nIntegrated face and gait recognition from multiple views. G Shakhnarovich, L Lee, T Darrell, Computer Vision and Pattern Recognition (CVPR). G. Shakhnarovich, L. Lee, and T. Darrell, \"Integrated face and gait recognition from multiple views,\" in Computer Vision and Pattern Recognition (CVPR), 2001.\n\nFusion of gait and face for human identification. A Kale, A K Roychowdhury, R Chellappa, Acoustics, Speech, and Signal Processing (ICASSP). A. Kale, A. K. RoyChowdhury, and R. Chellappa, \"Fusion of gait and face for human identification,\" in Acoustics, Speech, and Signal Processing (ICASSP), 2004.\n\nIntegrating face and gait for human recognition at a distance in video. X Zhou, B Bhanu, IEEE Transactions on Systems, Man, and Cybernetics. 375Part B (Cybernetics)X. Zhou and B. Bhanu, \"Integrating face and gait for human recognition at a distance in video,\" IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 37, no. 5, pp. 1119-1137, 2007.\n\nBody shape assessment scale: Instrument development foranalyzing female figures. L J Connell, P V Ulrich, E L Brannon, M Alexander, A B Presley, Clothing and Textiles Research Journal (CTRJ). 242L. J. Connell, P. V. Ulrich, E. L. Brannon, M. Alexander, and A. B. Presley, \"Body shape assessment scale: Instrument development foranalyzing female figures,\" Clothing and Textiles Research Journal (CTRJ), vol. 24, no. 2, pp. 80-95, 2006.\n\nShape-from-silhouette of articulated objects and its use for human body kinematics estimation and motion capture. K Cheung, S Baker, T Kanade, Computer Vision and Pattern Recognition (CVPR). K. Cheung, S. Baker, and T. Kanade, \"Shape-from-silhouette of articulated objects and its use for human body kinematics estimation and motion capture,\" in Computer Vision and Pattern Recognition (CVPR), 2003.\n\nMask R-CNN. K He, G Gkioxari, P Doll\u00e1r, R Girshick, International Conference on Computer Vision (ICCV. K. He, G. Gkioxari, P. Doll\u00e1r, and R. Girshick, \"Mask R-CNN,\" in International Conference on Computer Vision (ICCV), 2017.\n\nIlluminating Pedestrians via Simultaneous Detection and Segmentation. G Brazil, X Yin, X Liu, International Conference on Computer Vision (ICCV. G. Brazil, X. Yin, and X. Liu, \"Illuminating Pedestrians via Simultaneous Detection and Segmentation,\" in International Conference on Computer Vision (ICCV), 2017.\n\nPedestrian Detection with Autoregressive Network Phases. G Brazil, X Liu, Computer Vision and Pattern Recognition (CVPR). G. Brazil and X. Liu, \"Pedestrian Detection with Autoregressive Network Phases,\" in Computer Vision and Pattern Recognition (CVPR), 2019.\n\nUnsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. A Radford, L Metz, S Chintala, International Conference on Learning Representations (ICLR). A. Radford, L. Metz, and S. Chintala, \"Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks,\" in International Conference on Learning Representations (ICLR), 2016.\n\nLearning to forget: continual prediction with LSTM. F A Gers, J Schmidhuber, F Cummins, Neural Computation. 1210F. A. Gers, J. Schmidhuber, and F. Cummins, \"Learning to forget: continual prediction with LSTM,\" Neural Computation, vol. 12, no. 10, pp. 2451-2471, 2000.\n\nAdam: A Method for Stochastic Optimization. D P Kingma, J Ba, arXiv:1412.6980arXiv preprintD. P. Kingma and J. Ba, \"Adam: A Method for Stochastic Optimization,\" arXiv preprint arXiv:1412.6980, 2014.\n\nSupport Vector Regression for Multi-View Gait Recognition based on Local Motion Feature Selection. W Kusakunniran, Q Wu, J Zhang, H Li, Computer Vision and Pattern Recognition (CVPR). W. Kusakunniran, Q. Wu, J. Zhang, and H. Li, \"Support Vector Regres- sion for Multi-View Gait Recognition based on Local Motion Feature Selection,\" in Computer Vision and Pattern Recognition (CVPR), 2010.\n\nRecognizing Gaits Across Views Through Correlated Motion Co-Clustering. W Kusakunniran, Q Wu, J Zhang, H Li, L Wang, IEEE Transactions on Image Processing. 232W. Kusakunniran, Q. Wu, J. Zhang, H. Li, and L. Wang, \"Recognizing Gaits Across Views Through Correlated Motion Co-Clustering,\" IEEE Transactions on Image Processing, vol. 23, no. 2, pp. 696-709, 2013.\n\nArcface: Additive angular margin loss for deep face recognition. J Deng, J Guo, N Xue, S Zafeiriou, Computer Vision and Pattern Recognition (CVPR). J. Deng, J. Guo, N. Xue, and S. Zafeiriou, \"Arcface: Additive angular margin loss for deep face recognition,\" in Computer Vision and Pattern Recognition (CVPR), 2019.\n\nVisualizing data using t-SNE. L V D Maaten, G Hinton, Journal of Machine Learning Research. 9L. v. d. Maaten and G. Hinton, \"Visualizing data using t-SNE,\" Journal of Machine Learning Research, vol. 9, no. Nov, pp. 2579-2605, 2008.\n\nUnsupervised Learning of Video Representations using LSTMs. N Srivastava, E Mansimov, R Salakhudinov, International Conference on Machine Learning (ICML). N. Srivastava, E. Mansimov, and R. Salakhudinov, \"Unsupervised Learn- ing of Video Representations using LSTMs,\" in International Conference on Machine Learning (ICML), 2015.\n\nView-Invariant Discriminative Projection for Multi-View Gait-Based Human Identification. M Hu, Y Wang, Z Zhang, J J Little, D Huang, IEEE Transactions on Information Forensics and Security. 812M. Hu, Y. Wang, Z. Zhang, J. J. Little, and D. Huang, \"View-Invariant Discriminative Projection for Multi-View Gait-Based Human Identifica- tion,\" IEEE Transactions on Information Forensics and Security, vol. 8, no. 12, pp. 2034-2045, 2013.\n\nRecognizing Gaits on Spatio-Temporal Feature Domain. W Kusakunniran, IEEE Transactions on Information Forensics and Security. 99W. Kusakunniran, \"Recognizing Gaits on Spatio-Temporal Feature Domain,\" IEEE Transactions on Information Forensics and Security, vol. 9, no. 9, pp. 1416-1423, 2014.\n\nEnhanced Gabor Feature Based Classification Using a Regularized Locally Tensor Discriminant Model for Multiview Gait Recognition. H Hu, IEEE Transactions on Circuits and Systems for Video Technology. 23H. Hu, \"Enhanced Gabor Feature Based Classification Using a Regularized Locally Tensor Discriminant Model for Multiview Gait Recognition,\" IEEE Transactions on Circuits and Systems for Video Technology, vol. 23, no. 7, pp. 1274-1286, 2013.\n\nHuman identification using temporal information preserving gait template. W Chen, J Zhang, L Wang, J Pu, X Yuan, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). 3411W. Chen, J. Zhang, L. Wang, J. Pu, and X. Yuan, \"Human identification using temporal information preserving gait template,\" IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), vol. 34, no. 11, pp. 2164-2176, 2011.\n\nMarginal fisher analysis and its variants for human gait recognition and content-based image retrieval. D Xu, S Yan, D Tao, S Lin, H.-J Zhang, IEEE Transactions on Image processing. 1611D. Xu, S. Yan, D. Tao, S. Lin, and H.-J. Zhang, \"Marginal fisher analysis and its variants for human gait recognition and content-based image retrieval,\" IEEE Transactions on Image processing, vol. 16, no. 11, pp. 2811-2821, 2007.\n\nOn reducing the effect of covariate factors in gait recognition: a classifier ensemble method. Y Guan, C.-T Li, F Roli, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). 377Y. Guan, C.-T. Li, and F. Roli, \"On reducing the effect of covariate factors in gait recognition: a classifier ensemble method,\" IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), vol. 37, no. 7, pp. 1521-1528, 2014.\n\nCovariate conscious approach for gait recognition based upon Zernike moment invariants. H Aggarwal, D K Vishwakarma, IEEE Transactions on Cognitive and Developmental Systems (TCDS). 102H. Aggarwal and D. K. Vishwakarma, \"Covariate conscious approach for gait recognition based upon Zernike moment invariants,\" IEEE Transactions on Cognitive and Developmental Systems (TCDS), vol. 10, no. 2, pp. 397-407, 2017.\n\nMulti-Task GANs for View-Specific Feature Learning in Gait Recognition. Y He, J Zhang, H Shan, L Wang, IEEE Transactions on Information Forensics and Security. 141Y. He, J. Zhang, H. Shan, and L. Wang, \"Multi-Task GANs for View- Specific Feature Learning in Gait Recognition,\" IEEE Transactions on Information Forensics and Security, vol. 14, no. 1, pp. 102-113, 2018.\n\nRMPE: Regional Multi-Person Pose Estimation. H.-S Fang, S Xie, Y.-W Tai, C Lu, International Conference on Computer Vision (ICCV. H.-S. Fang, S. Xie, Y.-W. Tai, and C. Lu, \"RMPE: Regional Multi-Person Pose Estimation,\" in International Conference on Computer Vision (ICCV), 2017.\n\nRepresentation Learning and Image Synthesis for Deep Face Recognition. X Yin, Michigan State UniversityX. Yin, Representation Learning and Image Synthesis for Deep Face Recognition. Michigan State University, 2018.\n\nRetinaFace: Single-stage dense face localisation in the wild. J Deng, J Guo, Z Yuxiang, J Yu, I Kotsia, S Zafeiriou, arXiv:1905.00641in arXiv preprintJ. Deng, J. Guo, Z. Yuxiang, J. Yu, I. Kotsia, and S. Zafeiriou, \"RetinaFace: Single-stage dense face localisation in the wild,\" in arXiv preprint arXiv:1905.00641, 2019.\n\nLow quality video face recognition: Multi-mode aggregation recurrent network (MARN). S Gong, Y Shi, A K Jain, International Conference on Computer Vision Workshops (ICCVW). S. Gong, Y. Shi, and A. K. Jain, \"Low quality video face recognition: Multi-mode aggregation recurrent network (MARN),\" in International Conference on Computer Vision Workshops (ICCVW), 2019.\n\nVideo-based face model fitting using adaptive active appearance model. X Liu, Image and Vision Computing. 287X. Liu, \"Video-based face model fitting using adaptive active appearance model,\" Image and Vision Computing, vol. 28, no. 7, pp. 1162-1172, 2010.\n\nTowards highly accurate and stable face alignment for high-resolution videos. Y Tai, Y Liang, X Liu, L Duan, J Li, C Wang, F Huang, Y Chen, AAAI Conference on Artificial Intelligence (AAAI). 33Y. Tai, Y. Liang, X. Liu, L. Duan, J. Li, C. Wang, F. Huang, and Y. Chen, \"Towards highly accurate and stable face alignment for high-resolution videos,\" in AAAI Conference on Artificial Intelligence (AAAI), vol. 33, 2019, pp. 8893-8900.\n\nFSRNet: End-to-end learning face super-resolution with facial priors. Y Chen, Y Tai, X Liu, C Shen, J Yang, Computer Vision and Pattern Recognition (CVPR). Y. Chen, Y. Tai, X. Liu, C. Shen, and J. Yang, \"FSRNet: End-to-end learning face super-resolution with facial priors,\" in Computer Vision and Pattern Recognition (CVPR), 2018.\n", "annotations": {"author": "[{\"end\":143,\"start\":130},{\"end\":154,\"start\":144},{\"end\":176,\"start\":155},{\"end\":202,\"start\":177}]", "publisher": null, "author_last_name": "[{\"end\":142,\"start\":137},{\"end\":153,\"start\":149},{\"end\":175,\"start\":172},{\"end\":201,\"start\":198}]", "author_first_name": "[{\"end\":136,\"start\":130},{\"end\":148,\"start\":144},{\"end\":171,\"start\":167},{\"end\":197,\"start\":189}]", "author_affiliation": null, "title": "[{\"end\":127,\"start\":1},{\"end\":329,\"start\":203}]", "venue": null, "abstract": "[{\"end\":1914,\"start\":485}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2059,\"start\":2056},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2545,\"start\":2542},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2581,\"start\":2578},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2586,\"start\":2583},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2605,\"start\":2602},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2623,\"start\":2620},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2654,\"start\":2651},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3164,\"start\":3161},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3411,\"start\":3408},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3417,\"start\":3413},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3667,\"start\":3663},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3673,\"start\":3669},{\"end\":3923,\"start\":3910},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4137,\"start\":4134},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4143,\"start\":4139},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4170,\"start\":4166},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4176,\"start\":4172},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6964,\"start\":6961},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6970,\"start\":6966},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6976,\"start\":6972},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6982,\"start\":6978},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6988,\"start\":6984},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":6994,\"start\":6990},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7284,\"start\":7280},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7290,\"start\":7286},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8047,\"start\":8043},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8172,\"start\":8168},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9659,\"start\":9656},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":9693,\"start\":9689},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10119,\"start\":10115},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":10125,\"start\":10121},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10636,\"start\":10632},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10727,\"start\":10723},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11025,\"start\":11021},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":11031,\"start\":11027},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11037,\"start\":11033},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11043,\"start\":11039},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":11049,\"start\":11045},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11467,\"start\":11463},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11473,\"start\":11469},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11591,\"start\":11587},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":11600,\"start\":11597},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11614,\"start\":11610},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11628,\"start\":11624},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11647,\"start\":11643},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11818,\"start\":11814},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11824,\"start\":11820},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":11952,\"start\":11948},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":12130,\"start\":12126},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":12203,\"start\":12199},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12295,\"start\":12291},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":12350,\"start\":12346},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":12413,\"start\":12409},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":12419,\"start\":12415},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":12551,\"start\":12547},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":12572,\"start\":12568},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":12578,\"start\":12574},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12584,\"start\":12580},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":13495,\"start\":13491},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":13501,\"start\":13497},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14990,\"start\":14986},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":15602,\"start\":15598},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":15641,\"start\":15637},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":16349,\"start\":16345},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":16889,\"start\":16885},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":16895,\"start\":16891},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":28421,\"start\":28417},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":29082,\"start\":29078},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":29437,\"start\":29433},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":29795,\"start\":29791},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":29946,\"start\":29942},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":31722,\"start\":31718},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":31731,\"start\":31728},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":31923,\"start\":31919},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":31929,\"start\":31925},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":31935,\"start\":31931},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":31941,\"start\":31937},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":32145,\"start\":32141},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":32188,\"start\":32184},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":32417,\"start\":32413},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":35155,\"start\":35151},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":39119,\"start\":39115},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":39558,\"start\":39554},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":43477,\"start\":43473},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":43784,\"start\":43780},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":43835,\"start\":43831},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":43866,\"start\":43862},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":43872,\"start\":43868},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":43909,\"start\":43905},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":44218,\"start\":44214},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":44896,\"start\":44892},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":45882,\"start\":45879},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":45903,\"start\":45899},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":45909,\"start\":45905},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":45996,\"start\":45992},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":46083,\"start\":46079},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":46239,\"start\":46235},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":46431,\"start\":46427},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":46476,\"start\":46472},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":46516,\"start\":46512},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":46697,\"start\":46694},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":46703,\"start\":46699},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":46709,\"start\":46705},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":46715,\"start\":46711},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":46792,\"start\":46788},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":47055,\"start\":47051},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":48275,\"start\":48271},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":48998,\"start\":48994},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":49132,\"start\":49128},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":49376,\"start\":49372},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":51870,\"start\":51866},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":51876,\"start\":51872},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":51912,\"start\":51908},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":52714,\"start\":52710}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":54220,\"start\":53763},{\"attributes\":{\"id\":\"fig_2\"},\"end\":54391,\"start\":54221},{\"attributes\":{\"id\":\"fig_3\"},\"end\":54599,\"start\":54392},{\"attributes\":{\"id\":\"fig_4\"},\"end\":55323,\"start\":54600},{\"attributes\":{\"id\":\"fig_5\"},\"end\":56078,\"start\":55324},{\"attributes\":{\"id\":\"fig_6\"},\"end\":56589,\"start\":56079},{\"attributes\":{\"id\":\"fig_7\"},\"end\":57173,\"start\":56590},{\"attributes\":{\"id\":\"fig_8\"},\"end\":57613,\"start\":57174},{\"attributes\":{\"id\":\"fig_9\"},\"end\":57736,\"start\":57614},{\"attributes\":{\"id\":\"fig_10\"},\"end\":58191,\"start\":57737},{\"attributes\":{\"id\":\"fig_11\"},\"end\":58407,\"start\":58192},{\"attributes\":{\"id\":\"fig_12\"},\"end\":58932,\"start\":58408},{\"attributes\":{\"id\":\"fig_13\"},\"end\":59269,\"start\":58933},{\"attributes\":{\"id\":\"fig_14\"},\"end\":59422,\"start\":59270},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":59591,\"start\":59423},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":60185,\"start\":59592},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":60968,\"start\":60186},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":60981,\"start\":60969},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":61052,\"start\":60982},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":61537,\"start\":61053},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":62507,\"start\":61538},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":64176,\"start\":62508},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":64910,\"start\":64177},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":66372,\"start\":64911},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":67296,\"start\":66373},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":67549,\"start\":67297}]", "paragraph": "[{\"end\":2450,\"start\":1930},{\"end\":2896,\"start\":2452},{\"end\":3674,\"start\":2898},{\"end\":4694,\"start\":3676},{\"end\":6374,\"start\":4696},{\"end\":7910,\"start\":6376},{\"end\":8743,\"start\":7912},{\"end\":8802,\"start\":8745},{\"end\":8974,\"start\":8804},{\"end\":9254,\"start\":8976},{\"end\":9362,\"start\":9256},{\"end\":9500,\"start\":9364},{\"end\":10398,\"start\":9517},{\"end\":11503,\"start\":10400},{\"end\":13774,\"start\":11505},{\"end\":14785,\"start\":13807},{\"end\":16613,\"start\":14787},{\"end\":17774,\"start\":16615},{\"end\":19642,\"start\":17869},{\"end\":20024,\"start\":19644},{\"end\":20213,\"start\":20052},{\"end\":20756,\"start\":20242},{\"end\":21530,\"start\":20813},{\"end\":22046,\"start\":21532},{\"end\":22613,\"start\":22048},{\"end\":22802,\"start\":22686},{\"end\":23324,\"start\":22804},{\"end\":23670,\"start\":23467},{\"end\":24374,\"start\":23712},{\"end\":24868,\"start\":24432},{\"end\":25647,\"start\":24956},{\"end\":25853,\"start\":25692},{\"end\":26021,\"start\":25893},{\"end\":26392,\"start\":26023},{\"end\":26793,\"start\":26427},{\"end\":27200,\"start\":26938},{\"end\":27518,\"start\":27273},{\"end\":28043,\"start\":27538},{\"end\":28199,\"start\":28143},{\"end\":30490,\"start\":28226},{\"end\":31626,\"start\":30525},{\"end\":32452,\"start\":31651},{\"end\":32836,\"start\":32513},{\"end\":33222,\"start\":32838},{\"end\":34270,\"start\":33224},{\"end\":34984,\"start\":34272},{\"end\":36189,\"start\":35024},{\"end\":36947,\"start\":36231},{\"end\":37897,\"start\":36949},{\"end\":38632,\"start\":37899},{\"end\":39978,\"start\":38634},{\"end\":40716,\"start\":39980},{\"end\":41418,\"start\":40753},{\"end\":42010,\"start\":41420},{\"end\":42386,\"start\":42012},{\"end\":43257,\"start\":42417},{\"end\":45408,\"start\":43304},{\"end\":45842,\"start\":45410},{\"end\":46517,\"start\":45850},{\"end\":47919,\"start\":46525},{\"end\":48276,\"start\":47954},{\"end\":49033,\"start\":48278},{\"end\":49745,\"start\":49035},{\"end\":50855,\"start\":49786},{\"end\":52015,\"start\":50892},{\"end\":52932,\"start\":52033},{\"end\":53762,\"start\":52947}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":20051,\"start\":20025},{\"attributes\":{\"id\":\"formula_1\"},\"end\":20241,\"start\":20214},{\"attributes\":{\"id\":\"formula_2\"},\"end\":20812,\"start\":20757},{\"attributes\":{\"id\":\"formula_3\"},\"end\":22685,\"start\":22614},{\"attributes\":{\"id\":\"formula_4\"},\"end\":23466,\"start\":23325},{\"attributes\":{\"id\":\"formula_5\"},\"end\":24903,\"start\":24869},{\"attributes\":{\"id\":\"formula_6\"},\"end\":25691,\"start\":25648},{\"attributes\":{\"id\":\"formula_7\"},\"end\":25892,\"start\":25854},{\"attributes\":{\"id\":\"formula_8\"},\"end\":26426,\"start\":26393},{\"attributes\":{\"id\":\"formula_9\"},\"end\":26869,\"start\":26794},{\"attributes\":{\"id\":\"formula_10\"},\"end\":26937,\"start\":26869},{\"attributes\":{\"id\":\"formula_11\"},\"end\":27272,\"start\":27201},{\"attributes\":{\"id\":\"formula_12\"},\"end\":28142,\"start\":28044}]", "table_ref": "[{\"end\":11709,\"start\":11706},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":14839,\"start\":14832}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1928,\"start\":1916},{\"attributes\":{\"n\":\"2\"},\"end\":9515,\"start\":9503},{\"attributes\":{\"n\":\"3\"},\"end\":13794,\"start\":13777},{\"attributes\":{\"n\":\"3.1\"},\"end\":13805,\"start\":13797},{\"end\":17841,\"start\":17777},{\"attributes\":{\"n\":\"3.2\"},\"end\":17867,\"start\":17844},{\"attributes\":{\"n\":\"3.3\"},\"end\":23710,\"start\":23673},{\"attributes\":{\"n\":\"3.3.1\"},\"end\":24430,\"start\":24377},{\"attributes\":{\"n\":\"3.3.2\"},\"end\":24954,\"start\":24905},{\"attributes\":{\"n\":\"3.4\"},\"end\":27536,\"start\":27521},{\"attributes\":{\"n\":\"3.5\"},\"end\":28224,\"start\":28202},{\"attributes\":{\"n\":\"4\"},\"end\":30523,\"start\":30493},{\"attributes\":{\"n\":\"5\"},\"end\":31649,\"start\":31629},{\"attributes\":{\"n\":\"5.1\"},\"end\":32469,\"start\":32455},{\"attributes\":{\"n\":\"5.1.1\"},\"end\":32511,\"start\":32472},{\"attributes\":{\"n\":\"5.1.2\"},\"end\":35022,\"start\":34987},{\"attributes\":{\"n\":\"5.1.3\"},\"end\":36229,\"start\":36192},{\"attributes\":{\"n\":\"5.1.4\"},\"end\":40751,\"start\":40719},{\"attributes\":{\"n\":\"5.1.5\"},\"end\":42415,\"start\":42389},{\"attributes\":{\"n\":\"5.2\"},\"end\":43292,\"start\":43260},{\"attributes\":{\"n\":\"5.2.1\"},\"end\":43302,\"start\":43295},{\"attributes\":{\"n\":\"5.2.2\"},\"end\":45848,\"start\":45845},{\"attributes\":{\"n\":\"5.2.3\"},\"end\":46523,\"start\":46520},{\"attributes\":{\"n\":\"5.3\"},\"end\":47952,\"start\":47922},{\"attributes\":{\"n\":\"5.3.1\"},\"end\":49784,\"start\":49748},{\"attributes\":{\"n\":\"5.3.2\"},\"end\":50890,\"start\":50858},{\"attributes\":{\"n\":\"5.4\"},\"end\":52031,\"start\":52018},{\"attributes\":{\"n\":\"6\"},\"end\":52945,\"start\":52935},{\"end\":53772,\"start\":53764},{\"end\":54230,\"start\":54222},{\"end\":54401,\"start\":54393},{\"end\":54609,\"start\":54601},{\"end\":56599,\"start\":56591},{\"end\":57183,\"start\":57175},{\"end\":57623,\"start\":57615},{\"end\":57746,\"start\":57738},{\"end\":58202,\"start\":58193},{\"end\":58418,\"start\":58409},{\"end\":58943,\"start\":58934},{\"end\":59280,\"start\":59271},{\"end\":59602,\"start\":59593},{\"end\":60196,\"start\":60187},{\"end\":60979,\"start\":60970},{\"end\":60992,\"start\":60983},{\"end\":61063,\"start\":61054},{\"end\":61548,\"start\":61539},{\"end\":62518,\"start\":62509},{\"end\":64187,\"start\":64178},{\"end\":64921,\"start\":64912},{\"end\":66384,\"start\":66374},{\"end\":67308,\"start\":67298}]", "table": "[{\"end\":60185,\"start\":59673},{\"end\":60968,\"start\":60220},{\"end\":61052,\"start\":61031},{\"end\":61537,\"start\":61179},{\"end\":62507,\"start\":61801},{\"end\":64176,\"start\":62689},{\"end\":64910,\"start\":64314},{\"end\":66372,\"start\":65375},{\"end\":67296,\"start\":66552},{\"end\":67549,\"start\":67360}]", "figure_caption": "[{\"end\":54220,\"start\":53774},{\"end\":54391,\"start\":54232},{\"end\":54599,\"start\":54403},{\"end\":55323,\"start\":54611},{\"end\":56078,\"start\":55326},{\"end\":56589,\"start\":56081},{\"end\":57173,\"start\":56601},{\"end\":57613,\"start\":57185},{\"end\":57736,\"start\":57625},{\"end\":58191,\"start\":57748},{\"end\":58407,\"start\":58205},{\"end\":58932,\"start\":58421},{\"end\":59269,\"start\":58946},{\"end\":59422,\"start\":59283},{\"end\":59591,\"start\":59425},{\"end\":59673,\"start\":59604},{\"end\":60220,\"start\":60198},{\"end\":61031,\"start\":60994},{\"end\":61179,\"start\":61065},{\"end\":61801,\"start\":61550},{\"end\":62689,\"start\":62520},{\"end\":64314,\"start\":64189},{\"end\":65375,\"start\":64923},{\"end\":66552,\"start\":66387},{\"end\":67360,\"start\":67311}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":4779,\"start\":4769},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":5428,\"start\":5418},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":6117,\"start\":6111},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15262,\"start\":15256},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16744,\"start\":16738},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25316,\"start\":25310},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27642,\"start\":27636},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":33398,\"start\":33392},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":34331,\"start\":34325},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":35351,\"start\":35340},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":35828,\"start\":35816},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":36657,\"start\":36651},{\"end\":37466,\"start\":37460},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":37597,\"start\":37591},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":39193,\"start\":39186},{\"end\":39710,\"start\":39704},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":41439,\"start\":41432},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":42725,\"start\":42718},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":50002,\"start\":49995},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":51238,\"start\":51231},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":51543,\"start\":51534},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":51729,\"start\":51722}]", "bib_author_first_name": "[{\"end\":68241,\"start\":68240},{\"end\":68243,\"start\":68242},{\"end\":68252,\"start\":68251},{\"end\":68259,\"start\":68258},{\"end\":68452,\"start\":68451},{\"end\":68459,\"start\":68458},{\"end\":68467,\"start\":68466},{\"end\":68469,\"start\":68468},{\"end\":68723,\"start\":68722},{\"end\":68731,\"start\":68730},{\"end\":68737,\"start\":68736},{\"end\":68745,\"start\":68744},{\"end\":68751,\"start\":68750},{\"end\":68759,\"start\":68758},{\"end\":68766,\"start\":68765},{\"end\":69083,\"start\":69082},{\"end\":69090,\"start\":69089},{\"end\":69096,\"start\":69095},{\"end\":69104,\"start\":69103},{\"end\":69110,\"start\":69109},{\"end\":69435,\"start\":69434},{\"end\":69444,\"start\":69443},{\"end\":69451,\"start\":69450},{\"end\":69458,\"start\":69457},{\"end\":69464,\"start\":69463},{\"end\":69472,\"start\":69471},{\"end\":69785,\"start\":69784},{\"end\":69798,\"start\":69797},{\"end\":69800,\"start\":69799},{\"end\":69808,\"start\":69807},{\"end\":69817,\"start\":69816},{\"end\":69819,\"start\":69818},{\"end\":70112,\"start\":70111},{\"end\":70120,\"start\":70119},{\"end\":70122,\"start\":70121},{\"end\":70129,\"start\":70128},{\"end\":70365,\"start\":70364},{\"end\":70372,\"start\":70371},{\"end\":70710,\"start\":70709},{\"end\":70720,\"start\":70719},{\"end\":70722,\"start\":70721},{\"end\":70734,\"start\":70733},{\"end\":70741,\"start\":70740},{\"end\":70743,\"start\":70742},{\"end\":70751,\"start\":70750},{\"end\":70762,\"start\":70761},{\"end\":70764,\"start\":70763},{\"end\":71154,\"start\":71153},{\"end\":71164,\"start\":71163},{\"end\":71173,\"start\":71172},{\"end\":71511,\"start\":71510},{\"end\":71517,\"start\":71516},{\"end\":71526,\"start\":71525},{\"end\":71534,\"start\":71533},{\"end\":71542,\"start\":71541},{\"end\":71973,\"start\":71972},{\"end\":71975,\"start\":71974},{\"end\":71986,\"start\":71985},{\"end\":71998,\"start\":71997},{\"end\":72006,\"start\":72005},{\"end\":72341,\"start\":72340},{\"end\":72353,\"start\":72352},{\"end\":72684,\"start\":72683},{\"end\":72695,\"start\":72694},{\"end\":72707,\"start\":72706},{\"end\":72720,\"start\":72719},{\"end\":72730,\"start\":72729},{\"end\":73067,\"start\":73066},{\"end\":73073,\"start\":73072},{\"end\":73081,\"start\":73080},{\"end\":73087,\"start\":73086},{\"end\":73095,\"start\":73094},{\"end\":73099,\"start\":73096},{\"end\":73108,\"start\":73107},{\"end\":73117,\"start\":73116},{\"end\":73406,\"start\":73405},{\"end\":73418,\"start\":73417},{\"end\":73420,\"start\":73419},{\"end\":73680,\"start\":73679},{\"end\":73688,\"start\":73687},{\"end\":73695,\"start\":73694},{\"end\":73702,\"start\":73701},{\"end\":74015,\"start\":74014},{\"end\":74023,\"start\":74022},{\"end\":74029,\"start\":74028},{\"end\":74294,\"start\":74293},{\"end\":74296,\"start\":74295},{\"end\":74306,\"start\":74305},{\"end\":74308,\"start\":74307},{\"end\":74605,\"start\":74604},{\"end\":74615,\"start\":74614},{\"end\":74617,\"start\":74616},{\"end\":74626,\"start\":74625},{\"end\":74628,\"start\":74627},{\"end\":74967,\"start\":74966},{\"end\":74976,\"start\":74975},{\"end\":74983,\"start\":74982},{\"end\":74989,\"start\":74988},{\"end\":74996,\"start\":74995},{\"end\":75268,\"start\":75267},{\"end\":75280,\"start\":75279},{\"end\":75290,\"start\":75289},{\"end\":75296,\"start\":75295},{\"end\":75601,\"start\":75600},{\"end\":75608,\"start\":75607},{\"end\":75614,\"start\":75613},{\"end\":75620,\"start\":75619},{\"end\":75622,\"start\":75621},{\"end\":76014,\"start\":76013},{\"end\":76026,\"start\":76025},{\"end\":76036,\"start\":76035},{\"end\":76049,\"start\":76048},{\"end\":76055,\"start\":76054},{\"end\":76370,\"start\":76369},{\"end\":76383,\"start\":76382},{\"end\":76391,\"start\":76390},{\"end\":76401,\"start\":76400},{\"end\":76414,\"start\":76413},{\"end\":76752,\"start\":76751},{\"end\":76769,\"start\":76768},{\"end\":76776,\"start\":76775},{\"end\":76785,\"start\":76784},{\"end\":77162,\"start\":77161},{\"end\":77179,\"start\":77178},{\"end\":77188,\"start\":77187},{\"end\":77526,\"start\":77525},{\"end\":77528,\"start\":77527},{\"end\":77539,\"start\":77538},{\"end\":77541,\"start\":77540},{\"end\":77552,\"start\":77551},{\"end\":77554,\"start\":77553},{\"end\":77816,\"start\":77815},{\"end\":77825,\"start\":77824},{\"end\":77833,\"start\":77832},{\"end\":77840,\"start\":77839},{\"end\":77849,\"start\":77848},{\"end\":77856,\"start\":77855},{\"end\":77863,\"start\":77862},{\"end\":78210,\"start\":78209},{\"end\":78216,\"start\":78215},{\"end\":78223,\"start\":78222},{\"end\":78601,\"start\":78600},{\"end\":78610,\"start\":78609},{\"end\":78621,\"start\":78620},{\"end\":78633,\"start\":78632},{\"end\":78958,\"start\":78957},{\"end\":78970,\"start\":78969},{\"end\":78981,\"start\":78980},{\"end\":78990,\"start\":78989},{\"end\":78992,\"start\":78991},{\"end\":79003,\"start\":79002},{\"end\":79014,\"start\":79013},{\"end\":79022,\"start\":79021},{\"end\":79371,\"start\":79370},{\"end\":79379,\"start\":79378},{\"end\":79387,\"start\":79386},{\"end\":79393,\"start\":79392},{\"end\":79747,\"start\":79746},{\"end\":79756,\"start\":79755},{\"end\":79765,\"start\":79764},{\"end\":79773,\"start\":79772},{\"end\":80014,\"start\":80013},{\"end\":80016,\"start\":80015},{\"end\":80027,\"start\":80026},{\"end\":80029,\"start\":80028},{\"end\":80038,\"start\":80037},{\"end\":80040,\"start\":80039},{\"end\":80049,\"start\":80048},{\"end\":80051,\"start\":80050},{\"end\":80372,\"start\":80371},{\"end\":80383,\"start\":80382},{\"end\":80393,\"start\":80392},{\"end\":80405,\"start\":80404},{\"end\":80417,\"start\":80416},{\"end\":80788,\"start\":80787},{\"end\":80796,\"start\":80795},{\"end\":81404,\"start\":81403},{\"end\":81412,\"start\":81411},{\"end\":81419,\"start\":81418},{\"end\":81708,\"start\":81707},{\"end\":81715,\"start\":81714},{\"end\":81723,\"start\":81722},{\"end\":81731,\"start\":81730},{\"end\":82038,\"start\":82037},{\"end\":82048,\"start\":82047},{\"end\":82310,\"start\":82309},{\"end\":82326,\"start\":82325},{\"end\":82334,\"start\":82333},{\"end\":82336,\"start\":82335},{\"end\":82345,\"start\":82344},{\"end\":82355,\"start\":82354},{\"end\":82650,\"start\":82649},{\"end\":82665,\"start\":82664},{\"end\":82676,\"start\":82675},{\"end\":83065,\"start\":83064},{\"end\":83074,\"start\":83073},{\"end\":83084,\"start\":83083},{\"end\":83338,\"start\":83337},{\"end\":83340,\"start\":83339},{\"end\":83350,\"start\":83349},{\"end\":83571,\"start\":83570},{\"end\":83579,\"start\":83578},{\"end\":83586,\"start\":83585},{\"end\":84194,\"start\":84193},{\"end\":84208,\"start\":84207},{\"end\":84225,\"start\":84224},{\"end\":84234,\"start\":84233},{\"end\":84240,\"start\":84239},{\"end\":84256,\"start\":84255},{\"end\":84265,\"start\":84264},{\"end\":84278,\"start\":84277},{\"end\":84591,\"start\":84590},{\"end\":84608,\"start\":84607},{\"end\":84615,\"start\":84614},{\"end\":84884,\"start\":84883},{\"end\":84892,\"start\":84891},{\"end\":84894,\"start\":84893},{\"end\":84910,\"start\":84909},{\"end\":85206,\"start\":85205},{\"end\":85214,\"start\":85213},{\"end\":85587,\"start\":85586},{\"end\":85589,\"start\":85588},{\"end\":85600,\"start\":85599},{\"end\":85602,\"start\":85601},{\"end\":85612,\"start\":85611},{\"end\":85614,\"start\":85613},{\"end\":85625,\"start\":85624},{\"end\":85638,\"start\":85637},{\"end\":85640,\"start\":85639},{\"end\":86056,\"start\":86055},{\"end\":86066,\"start\":86065},{\"end\":86075,\"start\":86074},{\"end\":86355,\"start\":86354},{\"end\":86361,\"start\":86360},{\"end\":86373,\"start\":86372},{\"end\":86383,\"start\":86382},{\"end\":86640,\"start\":86639},{\"end\":86650,\"start\":86649},{\"end\":86657,\"start\":86656},{\"end\":86937,\"start\":86936},{\"end\":86947,\"start\":86946},{\"end\":87235,\"start\":87234},{\"end\":87246,\"start\":87245},{\"end\":87254,\"start\":87253},{\"end\":87584,\"start\":87583},{\"end\":87586,\"start\":87585},{\"end\":87594,\"start\":87593},{\"end\":87609,\"start\":87608},{\"end\":87845,\"start\":87844},{\"end\":87847,\"start\":87846},{\"end\":87857,\"start\":87856},{\"end\":88100,\"start\":88099},{\"end\":88116,\"start\":88115},{\"end\":88122,\"start\":88121},{\"end\":88131,\"start\":88130},{\"end\":88463,\"start\":88462},{\"end\":88479,\"start\":88478},{\"end\":88485,\"start\":88484},{\"end\":88494,\"start\":88493},{\"end\":88500,\"start\":88499},{\"end\":88818,\"start\":88817},{\"end\":88826,\"start\":88825},{\"end\":88833,\"start\":88832},{\"end\":88840,\"start\":88839},{\"end\":89099,\"start\":89098},{\"end\":89103,\"start\":89100},{\"end\":89113,\"start\":89112},{\"end\":89362,\"start\":89361},{\"end\":89376,\"start\":89375},{\"end\":89388,\"start\":89387},{\"end\":89722,\"start\":89721},{\"end\":89728,\"start\":89727},{\"end\":89736,\"start\":89735},{\"end\":89745,\"start\":89744},{\"end\":89747,\"start\":89746},{\"end\":89757,\"start\":89756},{\"end\":90121,\"start\":90120},{\"end\":90492,\"start\":90491},{\"end\":90879,\"start\":90878},{\"end\":90887,\"start\":90886},{\"end\":90896,\"start\":90895},{\"end\":90904,\"start\":90903},{\"end\":90910,\"start\":90909},{\"end\":91333,\"start\":91332},{\"end\":91339,\"start\":91338},{\"end\":91346,\"start\":91345},{\"end\":91353,\"start\":91352},{\"end\":91363,\"start\":91359},{\"end\":91742,\"start\":91741},{\"end\":91753,\"start\":91749},{\"end\":91759,\"start\":91758},{\"end\":92169,\"start\":92168},{\"end\":92181,\"start\":92180},{\"end\":92183,\"start\":92182},{\"end\":92564,\"start\":92563},{\"end\":92570,\"start\":92569},{\"end\":92579,\"start\":92578},{\"end\":92587,\"start\":92586},{\"end\":92910,\"start\":92906},{\"end\":92918,\"start\":92917},{\"end\":92928,\"start\":92924},{\"end\":92935,\"start\":92934},{\"end\":93214,\"start\":93213},{\"end\":93421,\"start\":93420},{\"end\":93429,\"start\":93428},{\"end\":93436,\"start\":93435},{\"end\":93447,\"start\":93446},{\"end\":93453,\"start\":93452},{\"end\":93463,\"start\":93462},{\"end\":93766,\"start\":93765},{\"end\":93774,\"start\":93773},{\"end\":93781,\"start\":93780},{\"end\":93783,\"start\":93782},{\"end\":94118,\"start\":94117},{\"end\":94381,\"start\":94380},{\"end\":94388,\"start\":94387},{\"end\":94397,\"start\":94396},{\"end\":94404,\"start\":94403},{\"end\":94412,\"start\":94411},{\"end\":94418,\"start\":94417},{\"end\":94426,\"start\":94425},{\"end\":94435,\"start\":94434},{\"end\":94805,\"start\":94804},{\"end\":94813,\"start\":94812},{\"end\":94820,\"start\":94819},{\"end\":94827,\"start\":94826},{\"end\":94835,\"start\":94834}]", "bib_author_last_name": "[{\"end\":68249,\"start\":68244},{\"end\":68256,\"start\":68253},{\"end\":68269,\"start\":68260},{\"end\":68456,\"start\":68453},{\"end\":68464,\"start\":68460},{\"end\":68475,\"start\":68470},{\"end\":68728,\"start\":68724},{\"end\":68734,\"start\":68732},{\"end\":68742,\"start\":68738},{\"end\":68748,\"start\":68746},{\"end\":68756,\"start\":68752},{\"end\":68763,\"start\":68760},{\"end\":68770,\"start\":68767},{\"end\":69087,\"start\":69084},{\"end\":69093,\"start\":69091},{\"end\":69101,\"start\":69097},{\"end\":69107,\"start\":69105},{\"end\":69115,\"start\":69111},{\"end\":69441,\"start\":69436},{\"end\":69448,\"start\":69445},{\"end\":69455,\"start\":69452},{\"end\":69461,\"start\":69459},{\"end\":69469,\"start\":69465},{\"end\":69475,\"start\":69473},{\"end\":69795,\"start\":69786},{\"end\":69805,\"start\":69801},{\"end\":69814,\"start\":69809},{\"end\":69825,\"start\":69820},{\"end\":70117,\"start\":70113},{\"end\":70126,\"start\":70123},{\"end\":70137,\"start\":70130},{\"end\":70369,\"start\":70366},{\"end\":70378,\"start\":70373},{\"end\":70717,\"start\":70711},{\"end\":70731,\"start\":70723},{\"end\":70738,\"start\":70735},{\"end\":70748,\"start\":70744},{\"end\":70759,\"start\":70752},{\"end\":70771,\"start\":70765},{\"end\":71161,\"start\":71155},{\"end\":71170,\"start\":71165},{\"end\":71178,\"start\":71174},{\"end\":71514,\"start\":71512},{\"end\":71523,\"start\":71518},{\"end\":71531,\"start\":71527},{\"end\":71539,\"start\":71535},{\"end\":71546,\"start\":71543},{\"end\":71983,\"start\":71976},{\"end\":71995,\"start\":71987},{\"end\":72003,\"start\":71999},{\"end\":72011,\"start\":72007},{\"end\":72350,\"start\":72342},{\"end\":72361,\"start\":72354},{\"end\":72692,\"start\":72685},{\"end\":72704,\"start\":72696},{\"end\":72717,\"start\":72708},{\"end\":72727,\"start\":72721},{\"end\":72735,\"start\":72731},{\"end\":73070,\"start\":73068},{\"end\":73078,\"start\":73074},{\"end\":73084,\"start\":73082},{\"end\":73092,\"start\":73088},{\"end\":73105,\"start\":73100},{\"end\":73114,\"start\":73109},{\"end\":73121,\"start\":73118},{\"end\":73415,\"start\":73407},{\"end\":73426,\"start\":73421},{\"end\":73685,\"start\":73681},{\"end\":73692,\"start\":73689},{\"end\":73699,\"start\":73696},{\"end\":73706,\"start\":73703},{\"end\":74020,\"start\":74016},{\"end\":74026,\"start\":74024},{\"end\":74033,\"start\":74030},{\"end\":74303,\"start\":74297},{\"end\":74316,\"start\":74309},{\"end\":74612,\"start\":74606},{\"end\":74623,\"start\":74618},{\"end\":74635,\"start\":74629},{\"end\":74973,\"start\":74968},{\"end\":74980,\"start\":74977},{\"end\":74986,\"start\":74984},{\"end\":74993,\"start\":74990},{\"end\":74999,\"start\":74997},{\"end\":75277,\"start\":75269},{\"end\":75287,\"start\":75281},{\"end\":75293,\"start\":75291},{\"end\":75301,\"start\":75297},{\"end\":75605,\"start\":75602},{\"end\":75611,\"start\":75609},{\"end\":75617,\"start\":75615},{\"end\":75630,\"start\":75623},{\"end\":76023,\"start\":76015},{\"end\":76033,\"start\":76027},{\"end\":76046,\"start\":76037},{\"end\":76052,\"start\":76050},{\"end\":76060,\"start\":76056},{\"end\":76380,\"start\":76371},{\"end\":76388,\"start\":76384},{\"end\":76398,\"start\":76392},{\"end\":76411,\"start\":76402},{\"end\":76421,\"start\":76415},{\"end\":76766,\"start\":76753},{\"end\":76773,\"start\":76770},{\"end\":76782,\"start\":76777},{\"end\":76798,\"start\":76786},{\"end\":77176,\"start\":77163},{\"end\":77185,\"start\":77180},{\"end\":77198,\"start\":77189},{\"end\":77536,\"start\":77529},{\"end\":77549,\"start\":77542},{\"end\":77561,\"start\":77555},{\"end\":77822,\"start\":77817},{\"end\":77830,\"start\":77826},{\"end\":77837,\"start\":77834},{\"end\":77846,\"start\":77841},{\"end\":77853,\"start\":77850},{\"end\":77860,\"start\":77857},{\"end\":77868,\"start\":77864},{\"end\":78213,\"start\":78211},{\"end\":78220,\"start\":78217},{\"end\":78227,\"start\":78224},{\"end\":78607,\"start\":78602},{\"end\":78618,\"start\":78611},{\"end\":78630,\"start\":78622},{\"end\":78638,\"start\":78634},{\"end\":78967,\"start\":78959},{\"end\":78978,\"start\":78971},{\"end\":78987,\"start\":78982},{\"end\":79000,\"start\":78993},{\"end\":79011,\"start\":79004},{\"end\":79019,\"start\":79015},{\"end\":79027,\"start\":79023},{\"end\":79376,\"start\":79372},{\"end\":79384,\"start\":79380},{\"end\":79390,\"start\":79388},{\"end\":79396,\"start\":79394},{\"end\":79753,\"start\":79748},{\"end\":79762,\"start\":79757},{\"end\":79770,\"start\":79766},{\"end\":79776,\"start\":79774},{\"end\":80024,\"start\":80017},{\"end\":80035,\"start\":80030},{\"end\":80046,\"start\":80041},{\"end\":80058,\"start\":80052},{\"end\":80380,\"start\":80373},{\"end\":80390,\"start\":80384},{\"end\":80402,\"start\":80394},{\"end\":80414,\"start\":80406},{\"end\":80424,\"start\":80418},{\"end\":80793,\"start\":80789},{\"end\":80800,\"start\":80797},{\"end\":81409,\"start\":81405},{\"end\":81416,\"start\":81413},{\"end\":81423,\"start\":81420},{\"end\":81712,\"start\":81709},{\"end\":81720,\"start\":81716},{\"end\":81728,\"start\":81724},{\"end\":81735,\"start\":81732},{\"end\":82045,\"start\":82039},{\"end\":82057,\"start\":82049},{\"end\":82323,\"start\":82311},{\"end\":82331,\"start\":82327},{\"end\":82342,\"start\":82337},{\"end\":82352,\"start\":82346},{\"end\":82362,\"start\":82356},{\"end\":82662,\"start\":82651},{\"end\":82673,\"start\":82666},{\"end\":82681,\"start\":82677},{\"end\":83071,\"start\":83066},{\"end\":83081,\"start\":83075},{\"end\":83090,\"start\":83085},{\"end\":83347,\"start\":83341},{\"end\":83358,\"start\":83351},{\"end\":83576,\"start\":83572},{\"end\":83583,\"start\":83580},{\"end\":83590,\"start\":83587},{\"end\":84205,\"start\":84195},{\"end\":84222,\"start\":84209},{\"end\":84231,\"start\":84226},{\"end\":84237,\"start\":84235},{\"end\":84253,\"start\":84241},{\"end\":84262,\"start\":84257},{\"end\":84275,\"start\":84266},{\"end\":84285,\"start\":84279},{\"end\":84605,\"start\":84592},{\"end\":84612,\"start\":84609},{\"end\":84623,\"start\":84616},{\"end\":84889,\"start\":84885},{\"end\":84907,\"start\":84895},{\"end\":84920,\"start\":84911},{\"end\":85211,\"start\":85207},{\"end\":85220,\"start\":85215},{\"end\":85597,\"start\":85590},{\"end\":85609,\"start\":85603},{\"end\":85622,\"start\":85615},{\"end\":85635,\"start\":85626},{\"end\":85648,\"start\":85641},{\"end\":86063,\"start\":86057},{\"end\":86072,\"start\":86067},{\"end\":86082,\"start\":86076},{\"end\":86358,\"start\":86356},{\"end\":86370,\"start\":86362},{\"end\":86380,\"start\":86374},{\"end\":86392,\"start\":86384},{\"end\":86647,\"start\":86641},{\"end\":86654,\"start\":86651},{\"end\":86661,\"start\":86658},{\"end\":86944,\"start\":86938},{\"end\":86951,\"start\":86948},{\"end\":87243,\"start\":87236},{\"end\":87251,\"start\":87247},{\"end\":87263,\"start\":87255},{\"end\":87591,\"start\":87587},{\"end\":87606,\"start\":87595},{\"end\":87617,\"start\":87610},{\"end\":87854,\"start\":87848},{\"end\":87860,\"start\":87858},{\"end\":88113,\"start\":88101},{\"end\":88119,\"start\":88117},{\"end\":88128,\"start\":88123},{\"end\":88134,\"start\":88132},{\"end\":88476,\"start\":88464},{\"end\":88482,\"start\":88480},{\"end\":88491,\"start\":88486},{\"end\":88497,\"start\":88495},{\"end\":88505,\"start\":88501},{\"end\":88823,\"start\":88819},{\"end\":88830,\"start\":88827},{\"end\":88837,\"start\":88834},{\"end\":88850,\"start\":88841},{\"end\":89110,\"start\":89104},{\"end\":89120,\"start\":89114},{\"end\":89373,\"start\":89363},{\"end\":89385,\"start\":89377},{\"end\":89401,\"start\":89389},{\"end\":89725,\"start\":89723},{\"end\":89733,\"start\":89729},{\"end\":89742,\"start\":89737},{\"end\":89754,\"start\":89748},{\"end\":89763,\"start\":89758},{\"end\":90134,\"start\":90122},{\"end\":90495,\"start\":90493},{\"end\":90884,\"start\":90880},{\"end\":90893,\"start\":90888},{\"end\":90901,\"start\":90897},{\"end\":90907,\"start\":90905},{\"end\":90915,\"start\":90911},{\"end\":91336,\"start\":91334},{\"end\":91343,\"start\":91340},{\"end\":91350,\"start\":91347},{\"end\":91357,\"start\":91354},{\"end\":91369,\"start\":91364},{\"end\":91747,\"start\":91743},{\"end\":91756,\"start\":91754},{\"end\":91764,\"start\":91760},{\"end\":92178,\"start\":92170},{\"end\":92195,\"start\":92184},{\"end\":92567,\"start\":92565},{\"end\":92576,\"start\":92571},{\"end\":92584,\"start\":92580},{\"end\":92592,\"start\":92588},{\"end\":92915,\"start\":92911},{\"end\":92922,\"start\":92919},{\"end\":92932,\"start\":92929},{\"end\":92938,\"start\":92936},{\"end\":93218,\"start\":93215},{\"end\":93426,\"start\":93422},{\"end\":93433,\"start\":93430},{\"end\":93444,\"start\":93437},{\"end\":93450,\"start\":93448},{\"end\":93460,\"start\":93454},{\"end\":93473,\"start\":93464},{\"end\":93771,\"start\":93767},{\"end\":93778,\"start\":93775},{\"end\":93788,\"start\":93784},{\"end\":94122,\"start\":94119},{\"end\":94385,\"start\":94382},{\"end\":94394,\"start\":94389},{\"end\":94401,\"start\":94398},{\"end\":94409,\"start\":94405},{\"end\":94415,\"start\":94413},{\"end\":94423,\"start\":94419},{\"end\":94432,\"start\":94427},{\"end\":94440,\"start\":94436},{\"end\":94810,\"start\":94806},{\"end\":94817,\"start\":94814},{\"end\":94824,\"start\":94821},{\"end\":94832,\"start\":94828},{\"end\":94840,\"start\":94836}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":68419,\"start\":68204},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":38904390},\"end\":68645,\"start\":68421},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":192656293},\"end\":69014,\"start\":68647},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":788439},\"end\":69336,\"start\":69016},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":7117552},\"end\":69738,\"start\":69338},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":671217},\"end\":70072,\"start\":69740},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":14391542},\"end\":70314,\"start\":70074},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":765267},\"end\":70632,\"start\":70316},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":7693282},\"end\":71108,\"start\":70634},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":11176755},\"end\":71425,\"start\":71110},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":5854795},\"end\":71861,\"start\":71427},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":4649104},\"end\":72255,\"start\":71863},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":2719355},\"end\":72603,\"start\":72257},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":12632343},\"end\":72980,\"start\":72605},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":56487001},\"end\":73350,\"start\":72982},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":8698967},\"end\":73612,\"start\":73352},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":86745098},\"end\":73967,\"start\":73614},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":6703188},\"end\":74230,\"start\":73969},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":2200276},\"end\":74518,\"start\":74232},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1622910},\"end\":74895,\"start\":74520},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":195439889},\"end\":75220,\"start\":74897},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":53356071},\"end\":75520,\"start\":75222},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":2371570},\"end\":75938,\"start\":75522},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":26821493},\"end\":76298,\"start\":75940},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":135334},\"end\":76671,\"start\":76300},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1977817},\"end\":77088,\"start\":76673},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":17460702},\"end\":77472,\"start\":77090},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":5252975},\"end\":77754,\"start\":77474},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":115147653},\"end\":78101,\"start\":77756},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":1815453},\"end\":78484,\"start\":78103},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":4646924},\"end\":78955,\"start\":78486},{\"attributes\":{\"id\":\"b31\"},\"end\":79315,\"start\":78957},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":27168999},\"end\":79671,\"start\":79317},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":149498114},\"end\":79964,\"start\":79673},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":45064783},\"end\":80264,\"start\":79966},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":17445139},\"end\":80750,\"start\":80266},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":4564472},\"end\":80963,\"start\":80752},{\"attributes\":{\"doi\":\"file:/localhost/opt/grobid/grobid-home/tmp/10.1109/tpami.2019.2927975\",\"id\":\"b37\",\"matched_paper_id\":52125180},\"end\":81344,\"start\":80965},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":128351002},\"end\":81617,\"start\":81346},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":4488285},\"end\":81969,\"start\":81619},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":13349803},\"end\":82260,\"start\":81971},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":5038229},\"end\":82582,\"start\":82262},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":3719281},\"end\":82993,\"start\":82584},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":4874038},\"end\":83302,\"start\":82995},{\"attributes\":{\"doi\":\"arXiv:1312.6114\",\"id\":\"b44\"},\"end\":83490,\"start\":83304},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":21011865},\"end\":83805,\"start\":83492},{\"attributes\":{\"doi\":\"file:/localhost/opt/grobid/grobid-home/tmp/10.1109/tpami.2018.2868350\",\"id\":\"b46\",\"matched_paper_id\":23042392},\"end\":84162,\"start\":83807},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":1033682},\"end\":84530,\"start\":84164},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":14230118},\"end\":84831,\"start\":84532},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":2747210},\"end\":85131,\"start\":84833},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":11109668},\"end\":85503,\"start\":85133},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":110119063},\"end\":85939,\"start\":85505},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":12283655},\"end\":86340,\"start\":85941},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":54465873},\"end\":86567,\"start\":86342},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":20340159},\"end\":86877,\"start\":86569},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":54218995},\"end\":87138,\"start\":86879},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":11758569},\"end\":87529,\"start\":87140},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":11598600},\"end\":87798,\"start\":87531},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b58\"},\"end\":87998,\"start\":87800},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":7701900},\"end\":88388,\"start\":88000},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":4095685},\"end\":88750,\"start\":88390},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":8923541},\"end\":89066,\"start\":88752},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":5855042},\"end\":89299,\"start\":89068},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":11699847},\"end\":89630,\"start\":89301},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":15344786},\"end\":90065,\"start\":89632},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":14191767},\"end\":90359,\"start\":90067},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":20904262},\"end\":90802,\"start\":90361},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":8729458},\"end\":91226,\"start\":90804},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":4591333},\"end\":91644,\"start\":91228},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":364124},\"end\":92078,\"start\":91646},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":565821},\"end\":92489,\"start\":92080},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":50771534},\"end\":92859,\"start\":92491},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":6529517},\"end\":93140,\"start\":92861},{\"attributes\":{\"id\":\"b73\"},\"end\":93356,\"start\":93142},{\"attributes\":{\"doi\":\"arXiv:1905.00641\",\"id\":\"b74\"},\"end\":93678,\"start\":93358},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":202637290},\"end\":94044,\"start\":93680},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":15688186},\"end\":94300,\"start\":94046},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":53198232},\"end\":94732,\"start\":94302},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":4564405},\"end\":95065,\"start\":94734}]", "bib_title": "[{\"end\":68449,\"start\":68421},{\"end\":68720,\"start\":68647},{\"end\":69080,\"start\":69016},{\"end\":69432,\"start\":69338},{\"end\":69782,\"start\":69740},{\"end\":70109,\"start\":70074},{\"end\":70362,\"start\":70316},{\"end\":70707,\"start\":70634},{\"end\":71151,\"start\":71110},{\"end\":71508,\"start\":71427},{\"end\":71970,\"start\":71863},{\"end\":72338,\"start\":72257},{\"end\":72681,\"start\":72605},{\"end\":73064,\"start\":72982},{\"end\":73403,\"start\":73352},{\"end\":73677,\"start\":73614},{\"end\":74012,\"start\":73969},{\"end\":74291,\"start\":74232},{\"end\":74602,\"start\":74520},{\"end\":74964,\"start\":74897},{\"end\":75265,\"start\":75222},{\"end\":75598,\"start\":75522},{\"end\":76011,\"start\":75940},{\"end\":76367,\"start\":76300},{\"end\":76749,\"start\":76673},{\"end\":77159,\"start\":77090},{\"end\":77523,\"start\":77474},{\"end\":77813,\"start\":77756},{\"end\":78207,\"start\":78103},{\"end\":78598,\"start\":78486},{\"end\":79368,\"start\":79317},{\"end\":79744,\"start\":79673},{\"end\":80011,\"start\":79966},{\"end\":80369,\"start\":80266},{\"end\":80785,\"start\":80752},{\"end\":81024,\"start\":80965},{\"end\":81401,\"start\":81346},{\"end\":81705,\"start\":81619},{\"end\":82035,\"start\":81971},{\"end\":82307,\"start\":82262},{\"end\":82647,\"start\":82584},{\"end\":83062,\"start\":82995},{\"end\":83568,\"start\":83492},{\"end\":83853,\"start\":83807},{\"end\":84191,\"start\":84164},{\"end\":84588,\"start\":84532},{\"end\":84881,\"start\":84833},{\"end\":85203,\"start\":85133},{\"end\":85584,\"start\":85505},{\"end\":86053,\"start\":85941},{\"end\":86352,\"start\":86342},{\"end\":86637,\"start\":86569},{\"end\":86934,\"start\":86879},{\"end\":87232,\"start\":87140},{\"end\":87581,\"start\":87531},{\"end\":88097,\"start\":88000},{\"end\":88460,\"start\":88390},{\"end\":88815,\"start\":88752},{\"end\":89096,\"start\":89068},{\"end\":89359,\"start\":89301},{\"end\":89719,\"start\":89632},{\"end\":90118,\"start\":90067},{\"end\":90489,\"start\":90361},{\"end\":90876,\"start\":90804},{\"end\":91330,\"start\":91228},{\"end\":91739,\"start\":91646},{\"end\":92166,\"start\":92080},{\"end\":92561,\"start\":92491},{\"end\":92904,\"start\":92861},{\"end\":93763,\"start\":93680},{\"end\":94115,\"start\":94046},{\"end\":94378,\"start\":94302},{\"end\":94802,\"start\":94734}]", "bib_author": "[{\"end\":68251,\"start\":68240},{\"end\":68258,\"start\":68251},{\"end\":68271,\"start\":68258},{\"end\":68458,\"start\":68451},{\"end\":68466,\"start\":68458},{\"end\":68477,\"start\":68466},{\"end\":68730,\"start\":68722},{\"end\":68736,\"start\":68730},{\"end\":68744,\"start\":68736},{\"end\":68750,\"start\":68744},{\"end\":68758,\"start\":68750},{\"end\":68765,\"start\":68758},{\"end\":68772,\"start\":68765},{\"end\":69089,\"start\":69082},{\"end\":69095,\"start\":69089},{\"end\":69103,\"start\":69095},{\"end\":69109,\"start\":69103},{\"end\":69117,\"start\":69109},{\"end\":69443,\"start\":69434},{\"end\":69450,\"start\":69443},{\"end\":69457,\"start\":69450},{\"end\":69463,\"start\":69457},{\"end\":69471,\"start\":69463},{\"end\":69477,\"start\":69471},{\"end\":69797,\"start\":69784},{\"end\":69807,\"start\":69797},{\"end\":69816,\"start\":69807},{\"end\":69827,\"start\":69816},{\"end\":70119,\"start\":70111},{\"end\":70128,\"start\":70119},{\"end\":70139,\"start\":70128},{\"end\":70371,\"start\":70364},{\"end\":70380,\"start\":70371},{\"end\":70719,\"start\":70709},{\"end\":70733,\"start\":70719},{\"end\":70740,\"start\":70733},{\"end\":70750,\"start\":70740},{\"end\":70761,\"start\":70750},{\"end\":70773,\"start\":70761},{\"end\":71163,\"start\":71153},{\"end\":71172,\"start\":71163},{\"end\":71180,\"start\":71172},{\"end\":71516,\"start\":71510},{\"end\":71525,\"start\":71516},{\"end\":71533,\"start\":71525},{\"end\":71541,\"start\":71533},{\"end\":71548,\"start\":71541},{\"end\":71985,\"start\":71972},{\"end\":71997,\"start\":71985},{\"end\":72005,\"start\":71997},{\"end\":72013,\"start\":72005},{\"end\":72352,\"start\":72340},{\"end\":72363,\"start\":72352},{\"end\":72694,\"start\":72683},{\"end\":72706,\"start\":72694},{\"end\":72719,\"start\":72706},{\"end\":72729,\"start\":72719},{\"end\":72737,\"start\":72729},{\"end\":73072,\"start\":73066},{\"end\":73080,\"start\":73072},{\"end\":73086,\"start\":73080},{\"end\":73094,\"start\":73086},{\"end\":73107,\"start\":73094},{\"end\":73116,\"start\":73107},{\"end\":73123,\"start\":73116},{\"end\":73417,\"start\":73405},{\"end\":73428,\"start\":73417},{\"end\":73687,\"start\":73679},{\"end\":73694,\"start\":73687},{\"end\":73701,\"start\":73694},{\"end\":73708,\"start\":73701},{\"end\":74022,\"start\":74014},{\"end\":74028,\"start\":74022},{\"end\":74035,\"start\":74028},{\"end\":74305,\"start\":74293},{\"end\":74318,\"start\":74305},{\"end\":74614,\"start\":74604},{\"end\":74625,\"start\":74614},{\"end\":74637,\"start\":74625},{\"end\":74975,\"start\":74966},{\"end\":74982,\"start\":74975},{\"end\":74988,\"start\":74982},{\"end\":74995,\"start\":74988},{\"end\":75001,\"start\":74995},{\"end\":75279,\"start\":75267},{\"end\":75289,\"start\":75279},{\"end\":75295,\"start\":75289},{\"end\":75303,\"start\":75295},{\"end\":75607,\"start\":75600},{\"end\":75613,\"start\":75607},{\"end\":75619,\"start\":75613},{\"end\":75632,\"start\":75619},{\"end\":76025,\"start\":76013},{\"end\":76035,\"start\":76025},{\"end\":76048,\"start\":76035},{\"end\":76054,\"start\":76048},{\"end\":76062,\"start\":76054},{\"end\":76382,\"start\":76369},{\"end\":76390,\"start\":76382},{\"end\":76400,\"start\":76390},{\"end\":76413,\"start\":76400},{\"end\":76423,\"start\":76413},{\"end\":76768,\"start\":76751},{\"end\":76775,\"start\":76768},{\"end\":76784,\"start\":76775},{\"end\":76800,\"start\":76784},{\"end\":77178,\"start\":77161},{\"end\":77187,\"start\":77178},{\"end\":77200,\"start\":77187},{\"end\":77538,\"start\":77525},{\"end\":77551,\"start\":77538},{\"end\":77563,\"start\":77551},{\"end\":77824,\"start\":77815},{\"end\":77832,\"start\":77824},{\"end\":77839,\"start\":77832},{\"end\":77848,\"start\":77839},{\"end\":77855,\"start\":77848},{\"end\":77862,\"start\":77855},{\"end\":77870,\"start\":77862},{\"end\":78215,\"start\":78209},{\"end\":78222,\"start\":78215},{\"end\":78229,\"start\":78222},{\"end\":78609,\"start\":78600},{\"end\":78620,\"start\":78609},{\"end\":78632,\"start\":78620},{\"end\":78640,\"start\":78632},{\"end\":78969,\"start\":78957},{\"end\":78980,\"start\":78969},{\"end\":78989,\"start\":78980},{\"end\":79002,\"start\":78989},{\"end\":79013,\"start\":79002},{\"end\":79021,\"start\":79013},{\"end\":79029,\"start\":79021},{\"end\":79378,\"start\":79370},{\"end\":79386,\"start\":79378},{\"end\":79392,\"start\":79386},{\"end\":79398,\"start\":79392},{\"end\":79755,\"start\":79746},{\"end\":79764,\"start\":79755},{\"end\":79772,\"start\":79764},{\"end\":79778,\"start\":79772},{\"end\":80026,\"start\":80013},{\"end\":80037,\"start\":80026},{\"end\":80048,\"start\":80037},{\"end\":80060,\"start\":80048},{\"end\":80382,\"start\":80371},{\"end\":80392,\"start\":80382},{\"end\":80404,\"start\":80392},{\"end\":80416,\"start\":80404},{\"end\":80426,\"start\":80416},{\"end\":80795,\"start\":80787},{\"end\":80802,\"start\":80795},{\"end\":81411,\"start\":81403},{\"end\":81418,\"start\":81411},{\"end\":81425,\"start\":81418},{\"end\":81714,\"start\":81707},{\"end\":81722,\"start\":81714},{\"end\":81730,\"start\":81722},{\"end\":81737,\"start\":81730},{\"end\":82047,\"start\":82037},{\"end\":82059,\"start\":82047},{\"end\":82325,\"start\":82309},{\"end\":82333,\"start\":82325},{\"end\":82344,\"start\":82333},{\"end\":82354,\"start\":82344},{\"end\":82364,\"start\":82354},{\"end\":82664,\"start\":82649},{\"end\":82675,\"start\":82664},{\"end\":82683,\"start\":82675},{\"end\":83073,\"start\":83064},{\"end\":83083,\"start\":83073},{\"end\":83092,\"start\":83083},{\"end\":83349,\"start\":83337},{\"end\":83360,\"start\":83349},{\"end\":83578,\"start\":83570},{\"end\":83585,\"start\":83578},{\"end\":83592,\"start\":83585},{\"end\":84207,\"start\":84193},{\"end\":84224,\"start\":84207},{\"end\":84233,\"start\":84224},{\"end\":84239,\"start\":84233},{\"end\":84255,\"start\":84239},{\"end\":84264,\"start\":84255},{\"end\":84277,\"start\":84264},{\"end\":84287,\"start\":84277},{\"end\":84607,\"start\":84590},{\"end\":84614,\"start\":84607},{\"end\":84625,\"start\":84614},{\"end\":84891,\"start\":84883},{\"end\":84909,\"start\":84891},{\"end\":84922,\"start\":84909},{\"end\":85213,\"start\":85205},{\"end\":85222,\"start\":85213},{\"end\":85599,\"start\":85586},{\"end\":85611,\"start\":85599},{\"end\":85624,\"start\":85611},{\"end\":85637,\"start\":85624},{\"end\":85650,\"start\":85637},{\"end\":86065,\"start\":86055},{\"end\":86074,\"start\":86065},{\"end\":86084,\"start\":86074},{\"end\":86360,\"start\":86354},{\"end\":86372,\"start\":86360},{\"end\":86382,\"start\":86372},{\"end\":86394,\"start\":86382},{\"end\":86649,\"start\":86639},{\"end\":86656,\"start\":86649},{\"end\":86663,\"start\":86656},{\"end\":86946,\"start\":86936},{\"end\":86953,\"start\":86946},{\"end\":87245,\"start\":87234},{\"end\":87253,\"start\":87245},{\"end\":87265,\"start\":87253},{\"end\":87593,\"start\":87583},{\"end\":87608,\"start\":87593},{\"end\":87619,\"start\":87608},{\"end\":87856,\"start\":87844},{\"end\":87862,\"start\":87856},{\"end\":88115,\"start\":88099},{\"end\":88121,\"start\":88115},{\"end\":88130,\"start\":88121},{\"end\":88136,\"start\":88130},{\"end\":88478,\"start\":88462},{\"end\":88484,\"start\":88478},{\"end\":88493,\"start\":88484},{\"end\":88499,\"start\":88493},{\"end\":88507,\"start\":88499},{\"end\":88825,\"start\":88817},{\"end\":88832,\"start\":88825},{\"end\":88839,\"start\":88832},{\"end\":88852,\"start\":88839},{\"end\":89112,\"start\":89098},{\"end\":89122,\"start\":89112},{\"end\":89375,\"start\":89361},{\"end\":89387,\"start\":89375},{\"end\":89403,\"start\":89387},{\"end\":89727,\"start\":89721},{\"end\":89735,\"start\":89727},{\"end\":89744,\"start\":89735},{\"end\":89756,\"start\":89744},{\"end\":89765,\"start\":89756},{\"end\":90136,\"start\":90120},{\"end\":90497,\"start\":90491},{\"end\":90886,\"start\":90878},{\"end\":90895,\"start\":90886},{\"end\":90903,\"start\":90895},{\"end\":90909,\"start\":90903},{\"end\":90917,\"start\":90909},{\"end\":91338,\"start\":91332},{\"end\":91345,\"start\":91338},{\"end\":91352,\"start\":91345},{\"end\":91359,\"start\":91352},{\"end\":91371,\"start\":91359},{\"end\":91749,\"start\":91741},{\"end\":91758,\"start\":91749},{\"end\":91766,\"start\":91758},{\"end\":92180,\"start\":92168},{\"end\":92197,\"start\":92180},{\"end\":92569,\"start\":92563},{\"end\":92578,\"start\":92569},{\"end\":92586,\"start\":92578},{\"end\":92594,\"start\":92586},{\"end\":92917,\"start\":92906},{\"end\":92924,\"start\":92917},{\"end\":92934,\"start\":92924},{\"end\":92940,\"start\":92934},{\"end\":93220,\"start\":93213},{\"end\":93428,\"start\":93420},{\"end\":93435,\"start\":93428},{\"end\":93446,\"start\":93435},{\"end\":93452,\"start\":93446},{\"end\":93462,\"start\":93452},{\"end\":93475,\"start\":93462},{\"end\":93773,\"start\":93765},{\"end\":93780,\"start\":93773},{\"end\":93790,\"start\":93780},{\"end\":94124,\"start\":94117},{\"end\":94387,\"start\":94380},{\"end\":94396,\"start\":94387},{\"end\":94403,\"start\":94396},{\"end\":94411,\"start\":94403},{\"end\":94417,\"start\":94411},{\"end\":94425,\"start\":94417},{\"end\":94434,\"start\":94425},{\"end\":94442,\"start\":94434},{\"end\":94812,\"start\":94804},{\"end\":94819,\"start\":94812},{\"end\":94826,\"start\":94819},{\"end\":94834,\"start\":94826},{\"end\":94842,\"start\":94834}]", "bib_venue": "[{\"end\":68238,\"start\":68204},{\"end\":68505,\"start\":68477},{\"end\":68818,\"start\":68772},{\"end\":69149,\"start\":69117},{\"end\":69509,\"start\":69477},{\"end\":69894,\"start\":69827},{\"end\":70183,\"start\":70139},{\"end\":70450,\"start\":70380},{\"end\":70843,\"start\":70773},{\"end\":71257,\"start\":71180},{\"end\":71618,\"start\":71548},{\"end\":72032,\"start\":72013},{\"end\":72409,\"start\":72363},{\"end\":72781,\"start\":72737},{\"end\":73142,\"start\":73123},{\"end\":73472,\"start\":73428},{\"end\":73763,\"start\":73708},{\"end\":74089,\"start\":74035},{\"end\":74364,\"start\":74318},{\"end\":74683,\"start\":74637},{\"end\":75047,\"start\":75001},{\"end\":75360,\"start\":75303},{\"end\":75702,\"start\":75632},{\"end\":76107,\"start\":76062},{\"end\":76474,\"start\":76423},{\"end\":76856,\"start\":76800},{\"end\":77255,\"start\":77200},{\"end\":77602,\"start\":77563},{\"end\":77916,\"start\":77870},{\"end\":78283,\"start\":78229},{\"end\":78695,\"start\":78640},{\"end\":79087,\"start\":79029},{\"end\":79468,\"start\":79398},{\"end\":79797,\"start\":79778},{\"end\":80102,\"start\":80060},{\"end\":80482,\"start\":80426},{\"end\":80848,\"start\":80802},{\"end\":81165,\"start\":81095},{\"end\":81471,\"start\":81425},{\"end\":81783,\"start\":81737},{\"end\":82106,\"start\":82059},{\"end\":82410,\"start\":82364},{\"end\":82778,\"start\":82683},{\"end\":83138,\"start\":83092},{\"end\":83335,\"start\":83304},{\"end\":83638,\"start\":83592},{\"end\":83994,\"start\":83924},{\"end\":84334,\"start\":84287},{\"end\":84671,\"start\":84625},{\"end\":84971,\"start\":84922},{\"end\":85272,\"start\":85222},{\"end\":85695,\"start\":85650},{\"end\":86130,\"start\":86084},{\"end\":86443,\"start\":86394},{\"end\":86712,\"start\":86663},{\"end\":86999,\"start\":86953},{\"end\":87324,\"start\":87265},{\"end\":87637,\"start\":87619},{\"end\":87842,\"start\":87800},{\"end\":88182,\"start\":88136},{\"end\":88544,\"start\":88507},{\"end\":88898,\"start\":88852},{\"end\":89158,\"start\":89122},{\"end\":89454,\"start\":89403},{\"end\":89820,\"start\":89765},{\"end\":90191,\"start\":90136},{\"end\":90559,\"start\":90497},{\"end\":90987,\"start\":90917},{\"end\":91408,\"start\":91371},{\"end\":91836,\"start\":91766},{\"end\":92260,\"start\":92197},{\"end\":92649,\"start\":92594},{\"end\":92989,\"start\":92940},{\"end\":93211,\"start\":93142},{\"end\":93418,\"start\":93358},{\"end\":93851,\"start\":93790},{\"end\":94150,\"start\":94124},{\"end\":94491,\"start\":94442},{\"end\":94888,\"start\":94842}]"}}}, "year": 2023, "month": 12, "day": 17}