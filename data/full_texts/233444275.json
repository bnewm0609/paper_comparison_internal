{"id": 233444275, "updated": "2023-10-06 04:26:41.894", "metadata": {"title": "Experts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation", "authors": "[{\"first\":\"Markus\",\"last\":\"Freitag\",\"middle\":[]},{\"first\":\"George\",\"last\":\"Foster\",\"middle\":[]},{\"first\":\"David\",\"last\":\"Grangier\",\"middle\":[]},{\"first\":\"Viresh\",\"last\":\"Ratnakar\",\"middle\":[]},{\"first\":\"Qijun\",\"last\":\"Tan\",\"middle\":[]},{\"first\":\"Wolfgang\",\"last\":\"Macherey\",\"middle\":[]}]", "venue": "ArXiv", "journal": "Transactions of the Association for Computational Linguistics", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Human evaluation of modern high-quality machine translation systems is a difficult problem, and there is increasing evidence that inadequate evaluation procedures can lead to erroneous conclusions. While there has been considerable research on human evaluation, the field still lacks a commonly-accepted standard procedure. As a step toward this goal, we propose an evaluation methodology grounded in explicit error analysis, based on the Multidimensional Quality Metrics (MQM) framework. We carry out the largest MQM research study to date, scoring the outputs of top systems from the WMT 2020 shared task in two language pairs using annotations provided by professional translators with access to full document context. We analyze the resulting data extensively, finding among other results a substantially different ranking of evaluated systems from the one established by the WMT crowd workers, exhibiting a clear preference for human over machine output. Surprisingly, we also find that automatic metrics based on pre-trained embeddings can outperform human crowd workers. We make our corpus publicly available for further research.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2104.14478", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tacl/FreitagFGRTM21", "doi": "10.1162/tacl_a_00437"}}, "content": {"source": {"pdf_hash": "476d79d1f5650c5361104ed468e75bfc4732622d", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2104.14478v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "ee59b00c8b772ff60f4a31b1776f2e1324d1063f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/476d79d1f5650c5361104ed468e75bfc4732622d.txt", "contents": "\nExperts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation\n\n\nMarkus Freitag freitag@google.com \nGeorge Foster fosterg@google.com \nDavid Grangier grangier@google.com \nViresh Ratnakar vratnakar@google.com \nQijun Tan qijuntan@google.com \nWolfgang Macherey \nGoogle Research \nExperts, Errors, and Context: A Large-Scale Study of Human Evaluation for Machine Translation\n1\nHuman evaluation of modern high-quality machine translation systems is a difficult problem, and there is increasing evidence that inadequate evaluation procedures can lead to erroneous conclusions. While there has been considerable research on human evaluation, the field still lacks a commonlyaccepted standard procedure. As a step toward this goal, we propose an evaluation methodology grounded in explicit error analysis, based on the Multidimensional Quality Metrics (MQM) framework. We carry out the largest MQM research study to date, scoring the outputs of top systems from the WMT 2020 shared task in two language pairs using annotations provided by professional translators with access to full document context. We analyze the resulting data extensively, finding among other results a substantially different ranking of evaluated systems from the one established by the WMT crowd workers, exhibiting a clear preference for human over machine output. Surprisingly, we also find that automatic metrics based on pre-trained embeddings can outperform human crowd workers. We make our corpus publicly available for further research.\n\nIntroduction\n\nLike many natural language generation tasks, machine translation (MT) is difficult to evaluate because the set of correct answers for each input is large and usually unknown. This limits the accuracy of automatic metrics, and necessitates costly human evaluation to provide a reliable gold standard for measuring MT quality and progress. Yet even human evaluation is problematic. For instance, we often wish to decide which of two translations is better, and by how much, but what should this take into account? If one translation sounds somewhat more natural than another, but contains a slight inaccuracy, what is the best way to quantify this? To what extent will different raters agree on their assessments?\n\nThe complexities of evaluating translationsboth machine and human-have been extensively studied, and there are many recommended best practices. However, due to expedience, human evaluation of MT is frequently carried out on isolated sentences by inexperienced raters with the aim of assigning a single score or ranking. When MT quality is poor, this can provide a useful signal; but as quality improves, there is a risk that the signal will become lost in rater noise or bias. Recent papers have argued that poor human evaluation practices have led to misleading results, including erroneous claims that MT has achieved human parity (Toral, 2020;L\u00e4ubli et al., 2018).\n\nThis paper aims to contribute to the evolution of standard practices for human evaluation of highquality MT. Our key insight is that any scoring or ranking of translations is implicitly based on an identification of errors and other imperfections. Making such an identification explicit by enumerating errors provides a \"platinum standard\" from which various gold-standard scorings can be derived, depending on the importance placed on different categories of errors for different downstream tasks. This is not a new insight: it is the conceptual basis for the Multidimensional Quality Metrics (MQM) framework developed in the EU QTLaunchPad and QT21 projects (www.qt21.eu), which we endorse and adopt for our experiments.\n\nMQM is a generic framework that provides a hierarchy of translation errors which can be tailored to specific applications. We identified a hierarchy appropriate for broad-coverage MT, and annotated outputs from 10 top-performing \"systems\" (including human references) for both the English\u2192German (EnDe) and Chinese\u2192English (ZhEn) language directions in the WMT 2020 news translation task (Barrault et al., 2020), using arXiv:2104.14478v1 [cs.CL] 29 Apr 2021 professional translators with access to full document context. For comparison purposes, we also collected scalar ratings on a 7-point scale from both professionals and crowd workers.\n\nWe analyze the resulting data along many different dimensions: comparing the system rankings resulting from different rating methods, including the original WMT scores; characterizing the error patterns of modern neural MT systems, including profiles of difficulty across documents, and comparing them to human translations (HT); measuring MQM inter-annotator agreement; and re-evaluating the performance of automatic metrics submitted to the WMT 2020 metrics task. Our most striking finding is that MQM ratings sharply revise the original WMT ranking of translations, exhibiting a clear preference for HT over MT, and promoting some low-ranked MT systems to much higher positions. This in turn changes the conclusions about the relative performance of different automatic metrics; interestingly, we find that most metrics correlate better with MQM rankings than WMT human scores do. We hope these results will underscore and help publicize the need for more careful human evaluation, particularly in shared tasks intended to assess MT or metric performance. We release our corpus to encourage further research. 1 Our main contributions are:\n\n\u2022 A proposal for a standard MQM scoring scheme appropriate for broad-coverage MT.\n\n\u2022 Release of a large-scale MQM corpus with annotations for over 100k HT and highquality-MT segments in two language pairs (EnDe and ZhEn) from WMT 2020. This is by far the largest study of human evaluation results released to the public.\n\n\u2022 Re-evaluation of the performance of MT systems and automatic metrics on our corpus, showing clear distinctions between HT and MT based on MQM ratings, adding to the evidence against claims of human parity.\n\n\u2022 Demonstration that crowd-worker evaluation has low correlation with our MQM-based evaluation, calling into question conclusions drawn on the basis of previous crowdsourced evaluations.\n\n1 https://github.com/google/ wmt-mqm-human-evaluation \u2022 Demonstration that automatic metrics based on pre-trained embeddings can outperform human crowd workers.\n\n\u2022 Characterization of current error types in HT and MT, identifying specific MT weaknesses.\n\n\u2022 Recommendations for the number of ratings needed to establish a reliable human benchmark, and for the most efficient way of distributing them across documents.\n\n\nRelated Work\n\nOne of the earliest formal mentions of human evaluation for MT occurs in the ALPAC report (1966), which defines an evaluation methodology based on \"intelligibility\" (comprehensibility) and \"fidelity\" (adequacy). The ARPA MT Initiative (White et al., 1994) defines an overall quality score based on \"adequacy\", \"fluency\" and \"comprehension\". In 2006, the first WMT evaluation campaign (Koehn and Monz, 2006) used adequacy and fluency ratings on a 5 point scale acquired from participants as their main metric. Vilar et al. (2007) proposed a ranking-based evaluation approach which became the official metric at WMT from 2008 until 2016 (Callison-Burch et al., 2008). The ratings were still acquired from the participants of the evaluation campaign. Graham et al. (2013) compared human assessor consistency levels for judgments collected on a fivepoint interval-level scale to those collected on a 1-100 continuous scale, using machine translation fluency as a test case. They claim that the use of a continuous scale eliminates individual judge preferences, resulting in higher levels of interannotator consistency. Bojar et al. (2016) came to the conclusion that fluency evaluation is highly correlated to adequacy evaluation. As a consequence of the latter two papers, continuous direct assessment focusing on adequacy has been the official WMT metric since 2017 (Bojar et al., 2017). Due to budget constraints, WMT understandably conducts its human evaluation with researchers and/or crowd-workers. Avramidis et al. (2012) used professional translators to rate MT output on three different tasks: ranking, error classification and post-editing. Castilho et al. (2017) found that crowd workers lack knowledge of translation and, compared to professional translators, tend to be more accepting of (subtle) translation errors. Graham et al. (2017) showed that crowd-worker evaluation has to be filtered to avoid contamination of results through the inclusion of false assessments. The quality of ratings acquired by either researchers or crowd workers has further been questioned by (Toral et al., 2018;L\u00e4ubli et al., 2020), who demonstrated that professional translators can discriminate between human and machine translations where crowd-workers were not able to do so. Mathur et al. (2020) re-evaluated a subset of WMT submissions with professional translators and showed that the resulting rankings changed and were better aligned with automatic scores. Fischer and L\u00e4ubli (2020) found that the number of segments with wrong terminology, omissions, and typographical problems for MT output is similar to HT. Fomicheva et al. (2017);Bentivogli et al. (2018) raised the concern that reference-based human evaluation might penalise correct translations that diverge too much from the reference. The literature mostly agrees that source-based rather than reference-based evaluation should be conducted (L\u00e4ubli et al., 2020). The impact of translationese (Koppel and Ordan, 2011) on human evaluation of MT has recently received attention (Toral et al., 2018;Zhang and Toral, 2019;Freitag et al., 2019;Graham et al., 2020). These papers show that the nature of source sentences is important and that only natural source sentences should be used for human evaluation.\n\nAs alternatives to adequacy and fluency, Scarton and Specia (2016) presented reading comprehension for MT quality evaluation. Forcada et al. (2018) proposed gap-filling, where certain words are removed from reference translations and readers are asked to fill the gaps left using the machinetranslated text as a hint. Popovi\u0107 (2020) proposed a new method for manual evaluation based on marking actual issues in the translated text. Instead of assigning a score, annotators are asked to just label problematic parts of the translations.\n\nThe Multidimensional Quality Metrics (MQM) framework was developed in the EU QT-LaunchPad and QT21 projects (2012-2018) (www.qt21.eu) to address the shortcomings of previous quality evaluation methods (Lommel et al., 2014). MQM provides a generic methodology for assessing translation quality that can be adapted to a wide range of evaluation needs. Klubi\u010dka et al. (2018) designed an MQM-compliant error taxonomy tailored to the relevant linguistic phe-nomena of Slavic languages to run a case study for 3 MT systems for English\u2192Croatian. More recently, Rei et al. (2020) used MQM labels to finetune COMET for automatic evaluation.\n\n\nHuman Evaluation Methodologies\n\nWe compared three human evaluation techniques: the WMT 2020 baseline; ratings on a 7-point Likert-type scale which we refer to as a Scalar Quality Metric (SQM); and evaluations under the MQM framework. We describe these methodologies in the following three sections, deferring concrete experimental details about annotators and data to the subsequent section.\n\n\nWMT\n\nAs part of the WMT evaluation campaign (Barrault et al., 2020), WMT runs human evaluation of the primary submissions for each language pair. The organizers collect segment-level ratings with document context (SR+DC) on a 0-100 scale using either source-based evaluation with a mix of researchers/translators (for translations out of English) or reference-based evaluation with crowdworkers (for translations into English). In addition, WMT conducts rater quality controls to remove ratings from raters that are not trustworthy. In general, for each system, only a subset of documents receive ratings, with the rated subset differing across systems. The organizers provide two different segment-level scores, averaged across one or more raters: (a) the raw score; and (b) a z-score which is standardized for each annotator. Document-and system-level scores are averages over segment-level scores. For more details, we refer the reader to the WMT findings papers.\n\n\nSQM\n\nSimilar to the WMT setting, the Scalar Quality Metric (SQM) evaluation collects segment-level scalar ratings with document context. Different from the 0-100 assessment of translation quality used in WMT, SQM uses a 0-6 scale for translation quality assessment, with the quality levels described as follows:\n\n6: Perfect Meaning and Grammar: The meaning of the translation is completely consistent with the source and the surrounding context (if applicable). The grammar is also correct. 4: Most Meaning Preserved and Few Grammar Mistakes: The translation retains most of the meaning of the source. It may have some grammar mistakes or minor contextual inconsistencies.\n\n2: Some Meaning Preserved: The translation preserves some of the meaning of the source but misses significant parts. The narrative is hard to follow due to fundamental errors. Grammar may be poor. 0: Nonsense/No meaning preserved: Nearly all information is lost between the translation and source. Grammar is irrelevant.\n\nThis evaluation presents each source segment and translated segment from a document in a table row, asking the rater to pick a rating from 0 through 6 (including the intermediate levels 1, 3, and 5). The rater can scroll up or down to see all the other source/translation segments from the document. Our SQM experiments used the 0-6 rating scale described above, instead of the wider, continuous scale recommended by (Graham et al., 2013), as this scale has been an established part of our existing MT evaluation ecosystem. It is possible that system rankings may be slightly sensitive to this nuance, but less so with raters who are translators rather than crowd workers, we believe.\n\n\nMQM\n\nTo adapt the generic MQM framework for our context, we followed the official guidelines for scientific research (MQM-usage-guidelines.pdf). For space reasons we give only the salient features of our MQM customization here, referring the reader to appendix A for a summary of MQM, and to appendix B for full details of our framework.\n\nOur annotators were instructed to identify all errors within each segment in a document, paying particular attention to document context; see Table 12 for complete annotator guidelines. Each error was highlighted in the text, and labeled with an error category from Table 10 and a severity  from Table 11. To temper the effect of long segments, we imposed a maximum of five errors per segment, instructing raters to choose the five most severe errors for segments containing more errors.\n\nOur error hierarchy includes the standard toplevel categories Accuracy, Fluency, Terminology, Style, and Locale, each with a specific set of subcategories. After an initial pilot run, we introduced a special Non-translation error that can be used to tag an entire segment which is too badly garbled to permit reliable identification of individual errors.\n\nError severities are assigned independent of cat-egory, and consist of Major, Minor, and Neutral levels, corresponding respectively to actual translation or grammatical errors, smaller imperfections, and purely subjective opinions about the translation. Many MQM schemes include an additional Critical severity which is worse than Major, but we dropped this because its definition is often context-specific. We felt that for broad coverage MT, the distinction between Major and Critical was likely to be highly subjective, while Major errors (true errors) would be easier to distinguish from Minor ones (imperfections).\n\nSince we are ultimately interested in scoring segments, we require a weighting on error types. We fixed the weight on Minor errors at 1, and explored a range of Major weights from 1 to 10 (the Major weight recommended in the MQM standard). For each weight combination we examined the stability of system ranking using a resampling technique. We found that a Major weight of 5 gave the best balance between stability and ability to discriminate among systems.\n\nThese weights apply to all error categories with two exceptions. We assigned a weight of 0.1 to Minor Fluency/Punctuation errors to reflect their mostly non-linguistic nature. Decisions like the style of quotation mark to use or the spacing around punctuation affect the appearance of a text but do not change its meaning. Unlike other kinds of Minor errors, these are easy to correct algorithmically, so we assign a low weight to ensure that their main role is to distinguish between systems that are equivalent in other respects. Major Fluency/Punctuation errors, which render a text ungrammatical or change its meaning (eg, eliding the comma in \"Let's eat, grandma\"), have standard weighting. The second exception is the singleton Non-translation category, with a weight of 25, equivalent to five Major errors. Table 1 summarizes our weighting scheme, in which segment-level scores can range from 0 (perfect) to 25 (worst). The final segment-level score is an average over scores from all annotators.  \n\n\nExperimental Setup\n\nWe re-annotated the WMT 2020 English\u2192German and Chinese\u2192English test sets, comprising 1418 segments (130 documents) and 2000 segments (155 documents) respectively. For each set we chose 10 \"systems\" for annotation, including the three reference translations available for English\u2192German and the two references available for Chinese\u2192English. The MT outputs included all top-performing systems according to the WMT human evaluation, augmented with systems we selected to increase diversity. Tables 3 and 4 list all evaluated systems. Table 2 summarizes rating information for the WMT evaluation and for the additional evaluations we conducted: SQM with crowd workers (cSQM), SQM with professional translators (pSQM), and MQM. We used disjoint professional translator pools for pSQM and MQM in order to avoid bias. All members of our rater pools were native speakers of the target language. Note that the average number of ratings per segment is less than 1 for the WMT evaluations because not all ratings survived the quality control.  To ensure maximum diversity in ratings for pSQM and MQM, we assigned documents in round-robin fashion to all 20 different sets of 3 raters from these pools. We chose an assignment order that roughly balanced the number of documents and segments per rater. Each rater was assigned a subset of documents, and annotated outputs from all 10 systems for those documents. Both documents and systems were anonymized and presented in a different random order to each rater. The number of segments per rater ranged from 6,830-7,220 for English\u2192German and from 9,860-10,210 for Chinese\u2192English.\n\n\nResults\n\n\nOverall System Rankings\n\nFor each human evaluation setup, we calculate a system-level score by averaging the segmentlevel scores for each system. Results are summarized in Table 3 (English\u2192German) and Table 4 (Chinese\u2192English). The system-and segmentlevel correlations to our platinum MQM ratings are shown in Figure 1 and 2 (English\u2192German), and Figure 3 and 4 (Chinese\u2192English). Segmentlevel correlations are calculated only for segments that were evaluated by WMT. For both language pairs, we observe similar patterns when looking at the results of the different human evaluations and come to the following findings:    (7) 3.68 (7) 2.33(6) 1.18 (7) 1.16(5) 0.56(4) 1.78(7) Tencent_Translation 0.386 (6) 84.3(8) 5.06 (4) 3.77 (6) 2.35 (7) 1.15 (6) 1.22 (8) 0.63 (7) 1.73(6) Huoshan_Translate 0.326 (7) 84.6(6) 5.00 (8) (9) 3.60(9) 2.48 (9) 1.34 (9) 1.20 (7) 0.64 (9) 1.84 (9) (1) 3.98 (7) 5.34 (7) 4.61 (7) 0.75 (3) 1.27 (6) 4.07 (9) (9) 3.95 (9) 5.48 (9) 4.73(9) 0.77 (5) 1.43 (9) 4.05(8) Online-B 0'.06 (5) 77.77 (2) 4.83 (10) 3.89(10) 5.85(10) 5.08(10) 0.79 (7) 1.51 (10) 4.34(10)  are ranked first by both the pSQM and MQM evaluations for both language pairs. The gap between human translations and MT is even more visible when looking at the MQM ratings which sets the human translations first by a large margin, demonstrating that the quality difference between MT and human translation is still large. Another interesting observation is the ranking of Human-P for English\u2192German. Human-P is a reference translation generated using the paraphrasing method of (Freitag et al., 2020) which asked linguists to paraphrase existing reference translations as much as possible while also suggesting using synonyms and different sentence structures. Our results support the assumption that crowd-workers are biased to prefer literal, easy-to-rate translations and rank Human-P low. Professional translators on the other hand are able to see the correctness of the paraphrased translations and ranked them higher than any MT output. Similar to the standard human translations, the gap between Human-P and the MT systems is larger when looking at the MQM ratings. In MQM, raters have to justify their ratings by labelling the error spans which helps to avoid penalizing non-literal translations.\n\n(ii) WMT has low correlation with MQM:\n\nThe human evaluation in WMT was conducted by crowd-workers (Chinese\u2192English) or a mix of researchers/translators (English\u2192German) during the WMT evaluation campaign. Further, different to all other evaluations in this paper, WMT conducted a reference-based/monolingual human evaluation for Chinese\u2192English in which the machine translation output was compared to a human-generated reference. When comparing the system ranks based on WMT for both language pairs with the ones generated by MQM, we can see low correlation for English\u2192German (see Figure 1) and even negative correlation for Chinese\u2192English (see Figure 3). We also see very low segment-level correlation for both language pairs (see Figure 2 and Figure 4). Later, we will also show that the correlation of SOTA automatic metrics are higher than the human ratings generated by WMT. The results at least question the reliability of the human ratings acquired by WMT.\n\n(iii) pSQM has high system-level correlation with MQM: The results for both language pairs suggest that pSQM and MQM are of similar quality as their system rankings mostly agree. Nevertheless, when zooming into the segment-level correlations, we observe a much lower correlation of \u223c0.5 based on Kendall tau for both language pairs. The difference of the two approaches is also visible in the absolute differences of the individual systems. For instance the submissions of DiDi_NLP and Tencent_Translation for Chinese\u2192English are close for pSQM (only 0.04 absolute difference). MQM on the other hand shows a larger difference of 0.19 points. When the quality of two systems gets closer, a more finegrained evaluation schema like MQM is needed. This is also important when doing system development where the difference between two variations for two systems can be minor. Looking into the future when we get closer to human translation quality, MQM will be needed for reliable evaluation. On the other hand, pSQM seems to be sufficient for an evaluation campaign like WMT.\n\n(iv) MQM results are mainly driven by major and accuracy errors: In Table 3 and Table 4, we also show the MQM error scores only based on Major/Minor errors or only based on Fluency or Accuracy errors. Interestingly, the MQM score based on accuracy errors or based on Major errors gives us almost the same rank as the full MQM score. Later in the paper, we will see that the majority of major errors are accuracy errors. This suggests the quality of an MT system is still driven mostly by accuracy errors as most fluency errors are judged minor.\n\n\nError Category Distribution\n\nMQM provides fine-grained error categories grouped under 4 main categories (accuracy, fluency, terminology and style). The absolute error counts for all 3 ratings for all 10 systems are shown in Tables 5 and 6. The error category Accuracy/Mistranslation is responsible for the majority of major errors for both language pairs. This suggests that the main problem of MT is still mistranslation of words or phrases. The absolute number of errors is much higher for Chinese\u2192English which demonstrates that this translation pair is more challenging than English\u2192German. Table 5 decomposes system and human MQM scores per category for English\u2192German. Human translations get lower error counts in all categories, except for additions. It seems that human translators might add tokens for fluency which are not supported by the source. Both systems and humans are mostly penalized by accuracy/mistranslation errors, but systems record 4x more error points in these categories. Similarly, sentences with more than 5 major errors (nontranslation) are much more frequent for systems (\u223c 28x the human rate). The best systems are quite different across categories. Tohoku is average in fluency but outstanding in accuracy, eTranslation is excellent in fluency but worse in accuracy, and OPPO ranks between the two other systems for both aspects. Compared to humans, the best systems are mostly penalized for mistranslations and non-translation (badly garbled sentences). Table 6 shows that the Chinese\u2192English translation task is more difficult than English\u2192German translation, with higher MQM error scores for human translations. Again, humans are performing better than systems across all categories except for additions, omissions and spelling. Many spelling mistakes relate to name formatting and capitalization which is difficult for this language pair (see name formatting errors). Additions and omissions again highlight that humans might be ready to compromise accuracy for fluency in some cases. Mistranslation and name formatting are the categories where the systems are penalized the most compared to humans. When comparing systems, the differences between the best systems is less pronounced than for English\u2192German, both in term of aggregate score and per-category counts.    (A, B), machine translations (all systems) and some of the best systems (VolcTrans, WeChat, Tencent). The ratio of system over human scores is in italics. Errors (%) report the fraction of the total error counts in a category, Major (%) report the fraction of major error for each category.\n\n\nDocument-error Distribution\n\nWe calculate document-level scores by averaging the segment level scores of each document.\n\nWe show the average document scores of all MT systems and all human translations (HT) for English\u2192German in Figure 5. The translation quality of humans is very consistent over all docu-  ments and gets a MQM score of around 1 which is equivalent to one minor error. This demonstrates that the translation quality of humans is consistent independent of the underlying source sentence. The distribution of MQM errors for machine translations looks much different. For some documents, MT gets very close to human performance, while for other documents the gap is clearly visible. Interestingly, all MT systems have similar problems with the same subset of documents which demonstrated that the quality of MT output is more conditioned on the actual input sentence and not only on the underlying MT system. The MQM document-level scores for Chinese\u2192English are shown in Figure 6. The distribution of MQM errors for the MT output looks very similar to the ones for English\u2192German. There are documents that are more challenging for some MT systems than others. Although the document-level scores are mostly lower for human translations, the distribution looks similar to the ones from MT systems. We first suspected that the reference translations were post-edited from MT. This is not the case: these translations originate from professional translators without access to post-editing but with access to CAT tools (mem-source and translation memory). Another possible explanation is the nature of the source sentences. Most sentences come from Chinese government news pages which have a formal style that may be difficult to render in English.\n\n\nAnnotator Agreement and Reliability\n\nOur annotations were performed by professional raters with MQM training. All raters were given roughly the same amount of work, with the same number of segments from each system. This setup should result in similar aggregated rater scores. Table 7(a) reports the scores per rater aggregated over the main error categories for English\u2192German.\n\nAll raters provide scores within \u00b120% around the mean, with rater 3 being the most severe rater and rater 1 the most permissive. Looking at individual ratings, rater 2 rated fewer errors in accuracy categories but used the style/awkward category more for errors outside of fluency/accuracy. Conversely, rater 6 barely used this category. Differences in error rates among raters are not severe but could be reduced with corrections from an annotation model (Paun et al., 2018), especially when working with larger annotator pools.\n\nThe rater comparison on Chinese\u2192English in Table 7(b) reports a wider range of scores than for English\u2192German. All raters provide scores within \u00b130% around the mean. This difference might be due to the greater difficulty of the translation task itself introducing more ambiguity in the labeling. In the future, it would be interesting to compare if translation between languages of different families suffer larger annotator disagreement for MQM ratings.\n\n\nNumber of MQM Ratings Required\n\nHuman evaluation with professional translators is more expensive than using the crowd. To keep the cost as low as possible, we compute the minimum number of ratings required to get a reliable human evaluation. We simulate new MQM rating projects by bootstrapping from the existing MQM data. 2 We compute Kendall's \u03c4 correlation of the simulated system level scores with the system level scores obtained from the full MQM data set. Note that later should be considered as the ground truth when estimating the accuracy of simulated MQM projects. See Figure 7 for the change of distributions of Kendall's \u03c4 for English\u2192German as the number of ratings increases.  : Distributions of Kendall's \u03c4 of system level scores for English\u2192German. As the number of ratings increases, the distribution of Kendall's \u03c4 converges to the Dirac distribution at 1. All systems use 1 rater per sentence and 3 consecutive sentences per document. The width of 95% CI is small (< 0.02), and thus is not shown here. Figure 8 shows the effect of different distributing schema for a fixed budget of 900 segment-level ratings. The system level scores become more accurate when limiting the number of segment-level ratings to 3 consecutive sentences in each document and thus distributing the 900 segment-level scores over more documents.  Once the items to be rated is fixed for one system, aligning the ratings across different systems makes the comparison of two system more accurate. For MQM, this means that to compare different systems, it helps to rate the same documents, and the same sentences in the corresponding documents. When possible, using the same rater(s) to rate the corresponding sentences for different systems further improves the accuracy of the comparison between systems.\n\nFinally, we estimate the number of ratings needed for MQM on different language pairs. The estimations are for systems with 3 consecutive sentences rated per document, and 1 rating per sentence. We further align the documents and the sentences rated across systems, but we do not align raters for corresponding sentences. We estimate the minimum number of ratings required such that the expected Kendall's \u03c4 correlation with the full data set \u2265 0.9. \n\n\nImpact on Automatic Evaluation\n\nWe compared the performance of automatic metrics submitted to the WMT20 Metrics Task when gold scores came from the original WMT ratings to the performance when gold scores were derived from our MQM ratings. Figure 9 shows Kendall's tau correlation for selected metrics at the system level for English\u2192German and Chinese\u2192English; 3 full results are in Appendix C. As would be expected from the low correlation between MQM and WMT scores, the ranking of metrics changes completely under MQM. In general, metrics that are not solely based on surface characteristics do somewhat better, though this pattern is not consistent (for example, chrF has a correlation of 0.8 for EnDe). Metrics tend to correlate better with MQM than they do with WMT, and almost all achieve better MQM correlation than WMT does (horizontal dotted line).  Adding human translations 4 to the outputs scored by the metrics results in a large drop in perfor-mance, especially for MQM due to human outputs being rated unambiguously higher than MT by MQM. Segment-level correlations are generally much lower than system-level, though they are significant due to having greater support. MQM correlations are again higher than WMT at this granularity, and are higher for ZhEn than EnDe, reversing the pattern from system-level results and suggesting a potential for improved system-level metric performance through better aggregation of segment-level scores.\n\n\nConclusion\n\nAs part of this work, we proposed a standard MQM scoring scheme that is appropriate for highquality MT. We used MQM to acquire ratings by professional translators for the recent WMT 2020 evaluation campaign for Chinese\u2192English and English\u2192German and used them as a platinum standard for comparison to different simpler evaluation methodologies and crowd worker evaluations. We release all ratings acquired in this study to encourage further research on this dataset for both human evaluation and automatic evaluation.\n\nOur study shows that crowd-worker human evaluations (as conducted by WMT) have low correlation with MQM, and the resulting systemlevel rankings are quite different. This finding questions previous conclusions made on the basis of crowd-worker human evaluation, especially for high-quality MT. We further come to the surprising finding that many automatic metrics, and in particular embedding-based ones, already outperform crowd-worker human evaluation. Unlike ratings acquired by crowd-worker and ratings acquired by professional translators on simpler human evaluation methodologies, MQM labels acquired with professional translators show a large gap between the quality of human and machine generated translations. This demonstrates that MT is still far from human parity. Furthermore, we characterize the current error types in human and machine translations, highlighting which error types are responsible for the difference between the two. We hope that researchers will use this as motivation to establish more error-type specific research directions. Finally, we give recommendations of how many MQM labels are required to establish a reliable human evaluation and how these ratings should be distributed across documents. \n\n\nA MQM Summary\n\nThe Multidimensional Quality Metrics (MQM) framework was developed in the EU QT-LaunchPad andQT21 projects (2012-2018) (www.qt21.eu). It provides a generic methodology for assessing translation quality that can be adapted to a wide range of evaluation needs. The central idea is to establish a standard hierarchy of translation issues (potential errors) that can be pruned or extended with new issues as required.\n\nAnnotators identify issues in text at a suitable granularity, and the results are summarized using a procedure that is specific to the application. The MQM standard (www.qt21.eu/mqmdefinition) consists of a controlled vocabulary for describing issues, a scoring mechanism for aggregating annotation results, an XML formalism for describing specific metrics (instantiations of MQM), a set of guidelines for selecting issues, and mappings from legacy metrics to MQM. All components except the vocabulary and XML mechanism are considered suggestive, and may be modified as required. Figure 10 depicts the MQM Core issue hierarchy, intended to cover common issues arising in translated texts.\n\nGuidelines for adapting MQM to scientific research are provided in the standard, and augmented by (MQM-usage-guidelines.pdf). The main points can be summarized as follows:\n\n\u2022 Choose an issue hierarchy suitable to the research questions being addressed, introducing new issues as needed, 5 and pruning irrelevant issues to reduce ambiguity and cognitive load. Specify the granularity of the text units to which the issues will apply; this may range from sub-sentential spans to multi-document collections.\n\n\u2022 If possible, use expert human translators or translators to perform annotations; three annotators per text item is recommended. Provide training in the use of the annotation tool, and guidelines for interpreting the issue hierarchy. These may be augmented with examples or decision trees, and a calibration set containing known errors can be used to assure annotator competence.\n\n\u2022 Annotation should proceed in short segments 5 These must not overlap semantically with issues in the controlled vocabulary.\n\n(30 minutes), and the allocated time should take text difficulty into account. Annotation cost is estimated to be approximately 1 USD / segment (assuming three annotators), but can be highly variable. Annotation within document context is assumed implicitly.\n\n\u2022 Analysis can produce aggregate scores or finer-grained summaries. The specification recommends that each issue be graded with a severity: none, minor, major, or critical. Aggregate scores can weight each issue by type (the default is to weight all types equally) and by severity (recommended scores are 0, 1, 10, and 100, respectively).\n\n\nB MQM for Broad-Coverage MT Annotation\n\nOur broad-coverage MT issue hierarchy is shown in Table 10. It is intended to be applied at the segment level by annotators with access to document context. We based it loosely on the MQM core hierarchy, with modifications established in collaboration with expert translators from our rater pool who had MQM experience. After an initial pilot run, we added several sub-categories to Locale convention for the sake of consistency. 6 Apart from clarifying the definitions of some categories, our main change was to add a Non-translation category to cover situations where identifying individual errors would be meaningless. At most one Non-translation error can be assigned to a segment, and choosing Non-translation precludes the identification of other errors in that segment. Table 11 shows descriptions for three severity levels that raters must assign to errors independent of their category. Many MQM schemes include an additional \"Critical\" severity which is worse than Major, but we dropped this because its definition is often context-specific, capturing errors that are disproportionately harmful for a particular application. We felt that for broad coverage MT the distinction between Major and Critical was likely to be highly subjective, while Major errors (actual errors) would be easier to distinguish from Minor ones (imperfections). Neutral severity allows annotators to express subjective opinions about the translation without affecting its rating. Annotator instructions are shown in Table 12. We kept these minimal because our raters were professionals with previous experience in assessing translation quality, including with MQM. There are many subtle issues that arise in error annotation, such as the correct way to translate units (eg, should 1 inch be translated as 1 Zoll, 1cm, or 2.54cm?), but we resisted the temptation to establish an extensive list of context-specific guidelines, relying instead on the judgment of our annotators. In order to temper the effect of long segments, we imposed a maximum of five errors per segment. For segments with more errors, we asked raters to identify only the five most severe. Thus we do not distinguish between segments containing five or more than five Major errors, although we do distinguish between segments with many identifiable errors and those that are categorized as entirely Non-translation. To focus our raters on careful error identification, and to provide potentially useful information for further studies, we had them highlight error spans in the text, following the conventions laid out in Table 12.\n\n\nScoring\n\nSince we are ultimately interested in deriving scores for sentences, we require a weighting on error categories and severities. We set the weight on Minor errors to 1, and explored a range of Major error weights from 1 to 10 (the Major weight recommended in the MQM standard). For each weight combination we examined the stability of system ranking using a resampling technique. We found that a Major weight of 5 gave the best bal-ance of stability and ability to discriminate among systems.\n\nThese weights apply to all error categories except Fluency/Punctuation and Nontranslation. We assigned a weight of 0.1 for Fluency/Punctuation to reflect its mostly nonlinguistic character. Decisions like the kind of quotation mark to use or the spacing between words and punctuation affect the appearance of a text but do not change its meaning. Unlike other kinds of minor errors, these are easy to correct algorithmically, so we assign them a low weight to ensure that their main role is to distinguish between systems that are equivalent in other respects. Our decision is supported by evidence from professional translators, who tend to treat minor punctuation errors as insignificant for the purpose of scoring, even when they are required to annotate them within the MQM framework. Note that this category does not include punctuation errors that render a text ungrammatical or change its meaning (eg, eliding the comma in \"Let's eat, grandma\"), which have the same weight as other Major errors. Source errors are ignored in our current study but give us the ability to discard badly garbled source sentences, which might be prevalent in certain genres. The singleton Nontranslation category has a weight of 25, equivalent to five Major errors, the worst segment-level score possible in our annotation scheme.\n\nOur current weighting ignores the text span of errors, as this provides little information relevant to scoring once severity and category are taken into account. Other Any other issues.\n\nSource error An error in the source.\n\nNon-translation Impossible to reliably characterize the 5 most severe errors. \n\n\nSeverity Description\n\nMajor Errors that may confuse or mislead the reader due to significant change in meaning or because they appear in a visible or important part of the content.\n\n\nMinor\n\nErrors that don't lead to loss of meaning and wouldn't confuse or mislead the reader but would be noticed, would decrease stylistic quality, fluency or clarity, or would make the content less appealing.\n\n\nNeutral\n\nUse to log additional information, problems or changes to be made that don't count as errors, e.g. they reflect a reviewer's choice or preferred style.  Table 1 summarizes our weighting scheme. The score of a segment is the sum of all errors it contains, averaged over all annotators, and ranges from 0 (perfect) to 25 (maximally bad). Segment scores are averaged to provide documentand system-level scores. Figure 12 shows the system-level Kendall tau correlations for all metrics from the WMT 2020 metrics task, completing the partial picture given in Figure 9. Figure 11 contains the corresponding plots for Pearson correlation. Figure 13 shows Kendall correlation for English\u2192German for metrics using the paraphrased references available for that language pair; this substantially changes metric ranking and performance. Finally, Figure 17 shows performance when human outputs were included among the systems to be scored, resulting in lower correlations compared to MQM gold scores, and much lower correlations compared to WMT gold scores.\n\n\nC Analysis of Metric Performance\n\nFor segment-level correlations, we adopted the WMT \"Kendall-like\" measure to deal with missing and unreliable segment-level annotations in the WMT data. This discards pairwise rankings when annotations are missing or when raw scores differ by less than 25. This statistic aggregates pairwise rankings over system scores for each segment rather than working from a single global list of segment-level scores, independent of which system they pertain to. For MQM correlations, lacking a way to establish a comparable threshold, and because we expected small differences to You will be assessing translations at the segment level, where a segment may contain one or more sentences. Each segment is aligned with a corresponding source segment, and both segments are displayed within their respective documents. Annotate segments in natural order, as if you were reading the document. You may return to revise previous segments.\n\nPlease identify all errors within each translated segment, up to a maximum of five. If there are more than five errors, identify only the five most severe. If it is not possible to reliably identify distinct errors because the translation is too badly garbled or is unrelated to the source, then mark a single Non-translation error that spans the entire segment.\n\nTo identify an error, highlight the relevant span of text, and select a category/sub-category and severity level from the available options. (The span of text may be in the source segment if the error is a source error or an omission.) When identifying errors, please be as fine-grained as possible. For example, if a sentence contains two words that are each mistranslated, two separate mistranslation errors should be recorded. If a single stretch of text contains multiple errors, you only need to indicate the one that is most severe. If all have the same severity, choose the first matching category listed in the error typology (eg, Accuracy, then Fluency, then Terminology, etc).\n\nPlease pay particular attention to document context when annotating. If a translation might be questionable on its own but is fine in the context of the document, it should not be considered erroneous; conversely, if a translation might be acceptable in some context, but not within the current document, it should be marked as wrong.\n\nThere are two special error categories: Source error and Non-translation. Source errors should be annotated separately, highlighting the relevant span in the source segment. They do not count against the 5-error limit for target errors, which should be handled in the usual way, whether or not they resulted from a source error. There can be at most one Non-translation error per segment, and it should span the entire segment. No other errors should be identified if Non-Translation is selected.  Figures 15, 16, and 17 for standard references, paraphrased references, and with human outputs included, respectively. In general, segment-level correlations are much lower than system-level, but patterns of differences between WMT and MQM correlations remain similar.       \n\nFigure 1 :Figure 2 :\n12English\u2192German: System correlation with the platinum ratings acquired with MQM. English\u2192German: Segment correlation with the platinum ratings acquired with MQM.\n\n( i )\niHuman translations are underestimated by crowd workers: Already in 2016, Hassan et al. (2018) claimed human parity for news-translation for Chinese\u2192English. We confirm the findings of Toral et al. (2018); L\u00e4ubli et al. (2018) that when human evaluation is conducted correctly, professional translators can discriminate between human and machine translations. All human translations\n\nFigure 3 :Figure 4 :\n34Chinese\u2192English: System-level correlation with the platinum ratings acquired with MQM. Chinese\u2192English: Segment correlation with the platinum ratings acquired with MQM.\n\nFigure 5 :Figure 6 :\n56EnDe: Document-level MQM scores. ZhEn: Document-level MQM scores.\n\nFigure 7\n7Figure 7: Distributions of Kendall's \u03c4 of system level scores for English\u2192German. As the number of ratings increases, the distribution of Kendall's \u03c4 converges to the Dirac distribution at 1. All systems use 1 rater per sentence and 3 consecutive sentences per document. The width of 95% CI is small (< 0.02), and thus is not shown here.\n\nFigure 8 :\n8System-level Kendall's \u03c4 for different distribution schema of 900 segment-level ratings for English\u2192German.\n\nTable 8 : 9 Figure 9 :\n899MQM: Number of required ratings per system to achieve Kendall's \u03c4 of 0.System-level metric performance with MQM and WMT scoring for: (a) EnDe, top panel; and (b) ZhEn, bottom panel. The horizontal blue line indicates the correlation between MQM and WMT human scores.\n\nFigure 10 :\n10MQM Core issue hierarchy.\n\nFigure 11 :\n11System-level Pearson correlation with MQM and WMT scoring.\n\nFigure 12 :\n12System-level Kendall correlation with MQM and WMT scoring.\n\nFigure 13 :\n13System-level Kendall correlation with MQM and WMT scoring when metrics use paraphrased reference.\n\nFigure 14 :\n14System-level Kendall correlation with MQM and WMT scoring when human outputs are included among systems to be scored.\n\nFigure 15 :\n15Segment-level Kendall correlation with MQM and WMT scoring.\n\nFigure 16 :\n16Segment-level Kendall correlation with MQM and WMT scoring when metrics use paraphrased reference.\n\nFigure 17 :\n17Segment-level Kendall correlation with MQM and WMT scoring when human outputs are included among systems to be scored.\n\nTable 1 :\n1MQM error weighting.\n\nTable 2 :\n2Details of all human evaluations.\n\nTable 3 :\n3English\u2192German: Different human evaluations for 10 submissions of the WMT20 evaluation campaign.System \nWMT\u2191 WMT RAW\u2191 cSQM\u2191 pSQM\u2191 MQM \u2193 Major\u2193 \nMinor\u2193 Fluency\u2193 Accuracy\u2193 \n\nHuman-A \n-\n-\n5.09(2) \n4.34(1) \n3.43(1) \n2.71(1) \n0.74(1) \n0.91(1) \n2.52(1) \nHuman-B \n-0.029(9) \n74.8(9) \n5.03(7) \n4.29(2) \n3.62(2) \n2.81(2) 0.82(10) 0.95(2) \n2.66(2) \nVolcTrans \n0.102(1) \n77.47(5) \n5.04(5) \n4.03(3) \n5.03(3) \n4.26(3) \n0.79(6) \n1.31(7) \n3.71(3) \nWeChat_AI \n0.077(3) \n77.35(6) \n4.99(8) \n4.02(4) \n5.13(4) \n4.39(4) \n0.76(4) \n1.24(5) \n3.89(4) \nTencent_Translation 0.063(4) \n76.67(7) \n5.04(6) \n3.99(5) \n5.19(5) \n4.43(6) \n0.79(8) \n1.23(4) \n3.96(5) \nOPPO \n0.051(7) \n77.51(4) \n5.07(4) \n3.99(5) \n5.20(6) \n4.41(5) \n0.81(9) \n1.23(3) \n3.97(6) \nTHUNLP \n0.028(8) \n76.48(8) \n5.11\n\nTable 4 :\n4Chinese\u2192English: Different human evaluations for 10 submissions of the WMT20 evaluation campaign.\n\nTable 5 :\n5MQM MQM vs H. MQM vs H. MQM vs H. MQM vs H.Category breakdown of MQM scores for English\u2192German for human translations (A, B), machine \ntranslations (all systems) and some of the best systems (Tohohku, OPPO, eTranslation). The ratio of system over \nhuman scores is in italics. Errors (%) report the fraction of the total error counts in a category, Major (%) report \nthe fraction of major error for each category. \n\nError Categories \nErrors Major Human \nAll MT \nVolcTrans \nWeChat \nTencent \n(%) \n(%) \nAccuracy/Mistranslation \n42.2 \n71.5 \n1.687 3.218 \n1.9 \n2.974 \n1.8 3.108 \n1.8 3.157 \n1.9 \nAccuracy/Omission \n8.6 \n61.3 \n0.646 0.505 \n0.8 \n0.468 \n0.7 0.534 \n0.8 0.547 \n0.8 \nFluency/Grammar \n13.8 \n18.4 \n0.381 0.442 \n1.2 \n0.414 \n1.1 0.392 \n1.0 0.425 \n1.1 \nLocale/Name format \n6.4 \n74.5 \n0.250 0.505 \n2.0 \n0.506 \n2.0 0.491 \n2.0 0.433 \n1.7 \nTerminology/Inappropriate \n5.1 \n31.1 \n0.139 0.221 \n1.6 \n0.220 \n1.6 0.217 \n1.6 0.202 \n1.5 \nStyle/Awkward \n5.7 \n17.1 \n0.122 0.182 \n1.5 \n0.193 \n1.6 0.180 \n1.5 0.185 \n1.5 \nAccuracy/Addition \n0.9 \n40.2 \n0.110 0.025 \n0.2 \n0.017 \n0.1 0.013 \n0.1 0.018 \n0.2 \nFluency/Spelling \n3.6 \n5.1 \n0.107 0.071 \n0.7 \n0.071 \n0.7 0.059 \n0.6 0.073 \n0.7 \nFluency/Punctuation \n11.1 \n1.4 \n0.028 0.035 \n1.2 \n0.035 \n1.3 0.031 \n1.1 0.033 \n1.2 \nLocale/Currency format \n0.4 \n8.8 \n0.011 0.010 \n0.9 \n0.010 \n0.9 0.010 \n0.9 0.010 \n0.9 \nFluency/Inconsistency \n0.8 \n27.5 \n0.011 0.036 \n3.3 \n0.028 \n2.7 0.026 \n2.4 0.038 \n3.5 \nFluency/Register \n0.4 \n6.5 \n0.008 0.008 \n1.0 \n0.008 \n0.9 0.008 \n1.0 0.009 \n1.1 \nLocale/Address format \n0.3 \n65.7 \n0.008 0.025 \n3.3 \n0.036 \n4.7 0.033 \n4.3 0.015 \n2.0 \nNon-translation \n0.0 100.0 \n0.006 0.024 \n3.9 \n0.021 \n3.3 0.012 \n2.0 0.029 \n4.7 \nTerminology/Inconsistent \n0.3 \n16.1 \n0.004 0.008 \n2.3 \n0.007 \n1.8 0.004 \n1.2 0.010 \n2.8 \nOther \n0.1 \n4.1 \n0.003 0.003 \n0.9 \n0.005 \n1.7 0.002 \n0.6 0.001 \n0.4 \n\nAll accuracy \n51.7 \n69.3 \n2.444 3.748 \n1.5 \n3.463 \n1.4 3.655 \n1.5 3.721 \n1.5 \nAll fluency \n29.8 \n10.5 \n0.535 0.593 \n1.1 \n0.557 \n1.0 0.517 \n1.0 0.580 \n1.1 \nAll except acc. & fluency \n18.5 \n41.7 \n0.546 0.986 \n1.8 \n1.005 \n1.8 0.955 \n1.7 0.891 \n1.6 \n\nAll categories \n100.0 \n46.7 \n3.525 5.327 \n1.5 \n5.025 \n1.4 5.127 \n1.5 5.192 \n1.5 \n\n\n\nTable 6 :\n6Category breakdown of MQM scores for Chinese\u2192English for human translations\n\n\nMQM vs avg. MQM vs avg. MQM vs avg. MQM vs avg. MQM vs avg. MQM vs avg. MQM vs avg. MQM vs avg. MQM vs avg. MQM vs avg. MQM vs avg. MQM vs avg.(a) English\u2192German \n\nCategories \nRater 1 \nRater 2 \nRater 3 \nRater 4 \nRater 5 \nRater 6 \nAccuracy \n1.02 \n0.84 \n0.82 \n0.68 \n1.55 \n1.28 \n1.42 \n1.18 \n1.23 \n1.02 \n1.21 \n1.00 \nFluency \n0.26 \n0.96 \n0.34 \n1.27 \n0.32 \n1.18 \n0.28 \n1.04 \n0.19 \n0.70 \n0.23 \n0.86 \nOthers \n0.41 \n0.80 \n0.63 \n1.23 \n0.59 \n1.14 \n0.57 \n1.10 \n0.57 \n1.10 \n0.32 \n0.63 \n\nAll \n1.69 \n0.85 \n1.79 \n0.90 \n2.45 \n1.23 \n2.27 \n1.14 \n1.98 \n1.00 \n1.76 \n0.88 \n\n(b) Chinese\u2192English \n\nCategories \nRater 1 \nRater 2 \nRater 3 \nRater 4 \nRater 5 \nRater 6 \nAccuracy \n3.34 \n0.96 \n3.26 \n0.94 \n3.31 \n0.95 \n2.51 \n0.72 \n4.57 \n1.31 \n3.91 \n1.12 \nFluency \n0.39 \n0.68 \n0.50 \n0.87 \n1.13 \n1.95 \n0.33 \n0.57 \n0.59 \n1.02 \n0.53 \n0.92 \nOthers \n0.70 \n0.78 \n0.75 \n0.83 \n0.85 \n0.94 \n0.66 \n0.74 \n1.11 \n1.24 \n1.32 \n1.47 \n\nAll \n4.43 \n0.89 \n4.51 \n0.91 \n5.29 \n1.07 \n3.50 \n0.71 \n6.27 \n1.26 \n5.76 \n1.16 \n\n\n\nTable 7 :\n7MQM per rater and category. The ratio of a rater score over the average score is in italics.\n\nTable 9\n9shows average correlations with WMT and MQM gold scores for different subsets of metrics at different granularities. At the system level, correlations are higher for MQM than WMT, and for EnDe than ZhEn. Correlations to MQM areAverage \nEnDe \nZhEn \ncorrelations \nWMT MQM WMT MQM \n\nPearson, sys-level \n0.539 0.883 0.318 0.551 \n0.23 \n0.02 \n0.41 \n0.21 \nKendall, sys-level \n0.436 0.637 0.309 0.443 \n0.27 \n0.10 \n0.42 \n0.23 \nKendall, sys-level, 0.467 0.676 0.514 0.343 \nbaseline metrics \n0.20 \n0.06 \n0.10 \n0.34 \nKendall, sys-level, 0.387 0.123 0.426 0.159 \n+ human \n0.26 \n0.68 \n0.20 \n0.64 \n\nKendall, seg-level \n0.170 0.228 0.159 0.298 \n0.00 \n0.00 \n0.00 \n0.00 \nKendall, seg-level, 0.159 0.161 0.157 0.276 \n+ human \n0.00 \n0.00 \n0.00 \n0.00 \n\n\n\nTable 9 :\n9Average correlations for various subsets of \nmetrics at different granularities. Numbers in italics \nare average p-values from two-tailed tests, indicating \nthe probability that the observed correlation was due to \nchance. \n\nquite good, though on average they are statistically \nsignificant only for EnDe. Interestingly, the aver-\nage performance of baseline metrics (BLEU, sent-\nBLEU, TER, chrF, chrF++) is similar to the global \naverage for all metrics in all conditions except \nfor ZhEn WMT, where it is substantially better. \n\n\n\nReferencesALPAC. 1966. Language and Machines: Computers in Translation and Linguistics; a Report, volume 1416. National Academies.Eleftherios Avramidis, Aljoscha Burchardt, Chris-\ntian Federmann, Maja Popovi\u0107, Cindy Tscher-\nwinka, and David Vilar. 2012. \nInvolving \nLanguage Professionals in the Evaluation of \nMachine Translation. In Proceedings of the \nEighth International Conference on Language \nResources and Evaluation (LREC'12), pages \n1127-1130, Istanbul, Turkey. European Lan-\nguage Resources Association (ELRA). \n\nLo\u00efc Barrault, Magdalena Biesialska, Ond\u0159ej Bo-\njar, Marta R. Costa-juss\u00e0, Christian Feder-\nmann, Yvette Graham, Roman Grundkiewicz, \nBarry Haddow, Matthias Huck, Eric Joanis, \nTom Kocmi, Philipp Koehn, Chi-kiu Lo, Nikola \nLjube\u0161i\u0107, Christof Monz, Makoto Morishita, \nMasaaki Nagata, Toshiaki Nakazawa, Santanu \nPal, Matt Post, and Marcos Zampieri. 2020. \nFindings of the 2020 Conference on Machine \nTranslation (WMT20). \nIn Proceedings of \nthe Fifth Conference on Machine Translation, \npages 1-55, Online. Association for Computa-\ntional Linguistics. \n\nLuisa Bentivogli, Mauro Cettolo, Marcello Fed-\nerico, and Christian Federmann. 2018. Machine \nTranslation Human Evaluation: an investigation \nof evaluation based on Post-Editing and its rela-\ntion with Direct Assessment. In International \nWorkshop on Spoken Language Translation. \n\nOndrej Bojar, Rajen Chatterjee, Federmann Chris-\ntian, Graham Yvette, Haddow Barry, Huck \nMatthias, Koehn Philipp, Liu Qun, Logacheva \nVarvara, Monz Christof, et al. 2017. Findings \nof the 2017 Conference on Machine Transla-\ntion (WMT17). In Second Conference onMa-\nchine Translation, pages 169-214. The Associ-\nation for Computational Linguistics. \nVerspoor, and Marcos Zampieri. 2016. Find-\nings of the 2016 Conference on Machine Trans-\nlation. In Proceedings of the First Conference \non Machine Translation: Volume 2, Shared Task \nPapers, pages 131-198, Berlin, Germany. Asso-\nciation for Computational Linguistics. \n\nChris Callison-Burch, Philipp Koehn, Christof \nMonz, Josh Schroeder, and Cameron Shaw \nFordyce. 2008. Proceedings of the Third Work-\nshop on Statistical Machine Translation. In \nProceedings of the Third Workshop on Statis-\ntical Machine Translation. \n\nSheila Castilho, Joss Moorkens, Federico Gas-\npari, Rico Sennrich, Vilelmini Sosoni, Panay-\nota Georgakopoulou, Pintu Lohar, Andy Way, \nAntonio Valerio Miceli Barone, and Maria Gi-\nalama. 2017. A Comparative Quality Evalu-\nation of PBSMT and NMT using Professional \nTranslators. AAMT. \n\nLukas Fischer and Samuel L\u00e4ubli. 2020. What's \nthe Difference Between Professional Human \nand Machine Translation? A Blind Multi-\nlanguage Study on Domain-specific MT. In \nProceedings of the 22nd Annual Conference of \nthe European Association for Machine Trans-\nlation, pages 215-224, Lisboa, Portugal. Euro-\npean Association for Machine Translation. \n\nMarina Fomicheva et al. 2017. The Role of Human \nReference Translation in Machine Translation \nEvaluation. Ph.D. thesis, Universitat Pompeu \nFabra. \n\nMikel L Forcada, Carolina Scarton, Lucia Spe-\ncia, Barry Haddow, and Alexandra Birch. 2018. \nExploring Gap Filling as a Cheaper Alterna-\ntive to Reading Comprehension Questionnaires \nwhen Evaluating Machine Translation for Gist-\ning. In Proceedings of the Third Conference on \nMachine Translation: Research Papers, pages \n192-203. \n\nMarkus Freitag, Isaac Caswell, and Scott Roy. \n2019. APE at Scale and Its Implications on MT \nEvaluation Biases. In Proceedings of the Fourth \nConference on Machine Translation, pages 34-\n44, Florence, Italy. Association for Computa-\ntional Linguistics. \n\nMarkus Freitag, David Grangier, and Isaac \nCaswell. 2020. BLEU might be Guilty but Ref-\nerences Are Not Innocent. In Proceedings of \n\nthe 2020 Conference on Empirical Methods in \nNatural Language Processing (EMNLP), pages \n61-71. \n\nYvette Graham, Timothy Baldwin, Alistair Mof-\nfat, and Justin Zobel. 2013. Continuous Mea-\nsurement Scales in Human Evaluation of Ma-\nchine Translation. In Proceedings of the 7th \nLinguistic Annotation Workshop and Interoper-\nability with Discourse, pages 33-41. \n\nYvette Graham, Timothy Baldwin, Alistair Mof-\nfat, and Justin Zobel. 2017. Can Machine \nTranslation Systems be Evaluated by the Crowd \nAlone? \nNatural Language Engineering, \n23(1):3-30. \n\nYvette Graham, Barry Haddow, and Philipp \nKoehn. 2020. Translationese in Machine Trans-\nlation Evaluation. In Proceedings of the 2020 \nConference on Empirical Methods in Natural \nLanguage Processing (EMNLP), pages 72-81. \n\nHany Hassan, Anthony Aue, Chang Chen, Vishal \nChowdhary, Jonathan Clark, Christian Fed-\nermann, Xuedong Huang, Marcin Junczys-\nDowmunt, William Lewis, Mu Li, et al. 2018. \nAchieving Human Parity on Automatic Chinese \nto English News Translation. arXiv preprint \narXiv:1803.05567. \n\nFilip Klubi\u010dka, Antonio Toral, and V\u00edctor M \nS\u00e1nchez-Cartagena. 2018. Quantitative Fine-\nGrained Human Evaluation of Machine Trans-\nlation Systems: a Case Study on English to \nCroatian. Machine Translation, 32(3):195-215. \n\nPhilipp Koehn and Christof Monz. 2006. Manual \nand Automatic Evaluation of Machine Transla-\ntion between European Languages. In Proceed-\nings on the Workshop on Statistical Machine \nTranslation, pages 102-121. \n\nMoshe Koppel and Noam Ordan. 2011. Trans-\nlationese and Its Dialects. In Proceedings of \nthe 49th Annual Meeting of the Association for \nComputational Linguistics: Human Language \nTechnologies -Volume 1, pages 1318-1326. \n\nSamuel L\u00e4ubli, Sheila Castilho, Graham Neubig, \nRico Sennrich, Qinlan Shen, and Antonio Toral. \n2020. A Set of Recommendations for Assess-\ning Human-Machine Parity in Language Trans-\nlation. Journal of Artificial Intelligence Re-\nsearch, 67:653-672. \nSamuel L\u00e4ubli, Rico Sennrich, and Martin Volk. \n2018. Has Machine Translation Achieved Hu-\nman Parity? A Case for Document-level Eval-\nuation. In Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Lan-\nguage Processing, pages 4791-4796. \n\nArle Lommel, Hans Uszkoreit, and Aljoscha Bur-\nchardt. 2014. Multidimensional Quality Met-\nrics (MQM) : A Framework for Declaring and \nDescribing Translation Quality Metrics. Trad-\num\u00e0tica, pages 0455-463. \n\nNitika Mathur, Johnny Wei, Markus Freitag, \nQingsong Ma, and Ond\u0159ej Bojar. 2020. Results \nof the WMT20 Metrics Shared Task. In Pro-\nceedings of the Fifth Conference on Machine \nTranslation, pages 688-725, Online. Associa-\ntion for Computational Linguistics. \n\nSilviu Paun, Bob Carpenter, Jon Chamberlain, \nDirk Hovy, Udo Kruschwitz, and Massimo Poe-\nsio. 2018. Comparing Bayesian Models of An-\nnotation. Transactions of the Association for \nComputational Linguistics, 6:571-585. \n\nMaja Popovi\u0107. 2020. Informative Manual Evalu-\nation of Machine Translation Output. In Pro-\nceedings of the 28th International Conference \non Computational Linguistics, pages 5059-\n5069. \n\nRicardo Rei, Craig Stewart, Ana C Farinha, and \nAlon Lavie. 2020. COMET: A Neural Frame-\nwork for MT Evaluation. In Proceedings of \nthe 2020 Conference on Empirical Methods in \nNatural Language Processing (EMNLP), pages \n2685-2702, Online. Association for Computa-\ntional Linguistics. \n\nCarolina Scarton and Lucia Specia. 2016. A \nReading Comprehension Corpus for Machine \nTranslation Evaluation. In Proceedings of the \nTenth International Conference on Language \nResources and Evaluation (LREC'16), pages \n3652-3658. \n\nAntonio Toral. 2020. Reassessing claims of hu-\nman parity and super-human performance in \nmachine translation at wmt 2019. In Proceed-\nings of the 22nd Annual Conference of the Eu-\nropean Association for Machine Translation, \npages 185-194. \n\nAntonio Toral, Sheila Castilho, Ke Hu, and Andy \nWay. 2018. Attaining the Unattainable? Re-\nassessing Claims of Human Parity in Neural \nMachine Translation. In Proceedings of the \nThird Conference on Machine Translation: Re-\nsearch Papers, pages 113-123, Belgium, Brus-\nsels. Association for Computational Linguis-\ntics. \n\nDavid Vilar, Gregor Leusch, Hermann Ney, and \nRafael E Banchs. 2007. Human Evaluation of \nMachine Translation Through Binary System \nComparisons. In Proceedings of the Second \nWorkshop on Statistical Machine Translation, \npages 96-103. \n\nJohn S White, Theresa A O'Connell, and Fran-\ncis E O'Mara. 1994. The arpa mt evaluation \nmethodologies: evolution, lessons, and future \napproaches. In Proceedings of the First Confer-\nence of the Association for Machine Translation \nin the Americas. \n\nMike Zhang and Antonio Toral. 2019. The Effect \nof Translationese in Machine Translation Test \nSets. In Proceedings of the Fourth Conference \non Machine Translation (Volume 1: Research \nPapers), pages 73-81. \n\n\n\nAccuracyAddition Translation includes information not present in the source. OmissionTranslation is missing content from the source.MistranslationTranslation does not accurately represent the source. Untranslated text Source text has been left untranslated.TerminologyInappropriate for context Terminology is non-standard or does not fit context. Inconsistent use Terminology is used inconsistently.Error Category \nDescription \n\nFluency \nPunctuation \nIncorrect punctuation (for locale or style). \nSpelling \nIncorrect spelling or capitalization. \nGrammar \nProblems with grammar, other than orthography. \nRegister \nWrong grammatical register (eg, inappropriately informal pronouns). \nInconsistency \nInternal inconsistency (not related to terminology). \nCharacter encoding \nCharacters are garbled due to incorrect encoding. \n\nStyle \nAwkward \nTranslation has stylistic problems. \n\nLocale \nAddress format \nWrong format for addresses. \nconvention \nCurrency format \nWrong format for currency. \nDate format \nWrong format for dates. \nName format \nWrong format for names. \nTelephone format \nWrong format for telephone numbers. \nTime format \nWrong format for time expressions. \n\n\n\nTable 10 :\n10MQM hierarchy.\n\nTable 11 :\n11MQM severity levels.\n\nTable 12 :\n12MQM annotator guidelines be significant, we used a threshold of 0. The results are shown in\nTo make the bootstrapping more efficient, we computed the covariance matrix of the MQM ratings of all translation systems, and bootstrapped from a multi-variate Gaussian.\nThe official WMT system-level results use Pearson correlation, but since we are rating fewer systems (only 7 in the case of EnDe), Kendall is more meaningful; it also corresponds more directly to the main use case of system ranking.\nOne additional standard reference and one paraphrased reference for EnDe, and one standard reference for ZhEn.\nAn alternative and arguably preferable strategy would have been to collapse all sub-categories for locale.\n", "annotations": {"author": "[{\"end\":131,\"start\":97},{\"end\":165,\"start\":132},{\"end\":201,\"start\":166},{\"end\":239,\"start\":202},{\"end\":270,\"start\":240},{\"end\":289,\"start\":271},{\"end\":306,\"start\":290}]", "publisher": null, "author_last_name": "[{\"end\":111,\"start\":104},{\"end\":145,\"start\":139},{\"end\":180,\"start\":172},{\"end\":217,\"start\":209},{\"end\":249,\"start\":246},{\"end\":288,\"start\":280},{\"end\":305,\"start\":297}]", "author_first_name": "[{\"end\":103,\"start\":97},{\"end\":138,\"start\":132},{\"end\":171,\"start\":166},{\"end\":208,\"start\":202},{\"end\":245,\"start\":240},{\"end\":279,\"start\":271},{\"end\":296,\"start\":290}]", "author_affiliation": null, "title": "[{\"end\":94,\"start\":1},{\"end\":400,\"start\":307}]", "venue": null, "abstract": "[{\"end\":1539,\"start\":403}]", "bib_ref": "[{\"end\":2914,\"start\":2901},{\"end\":2934,\"start\":2914},{\"end\":4072,\"start\":4049},{\"end\":6694,\"start\":6688},{\"end\":6853,\"start\":6833},{\"end\":7004,\"start\":6982},{\"end\":7126,\"start\":7107},{\"end\":7262,\"start\":7233},{\"end\":7366,\"start\":7346},{\"end\":7732,\"start\":7713},{\"end\":7982,\"start\":7962},{\"end\":8122,\"start\":8099},{\"end\":8267,\"start\":8245},{\"end\":8444,\"start\":8424},{\"end\":8700,\"start\":8680},{\"end\":8720,\"start\":8700},{\"end\":8889,\"start\":8869},{\"end\":9080,\"start\":9055},{\"end\":9233,\"start\":9205},{\"end\":9257,\"start\":9233},{\"end\":9520,\"start\":9499},{\"end\":9654,\"start\":9634},{\"end\":9676,\"start\":9654},{\"end\":9697,\"start\":9676},{\"end\":9717,\"start\":9697},{\"end\":10010,\"start\":9989},{\"end\":10622,\"start\":10601},{\"end\":10772,\"start\":10750},{\"end\":10972,\"start\":10955},{\"end\":11496,\"start\":11473},{\"end\":13832,\"start\":13811},{\"end\":29105,\"start\":29086},{\"end\":29942,\"start\":29941},{\"end\":35203,\"start\":35187},{\"end\":35228,\"start\":35203}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":47341,\"start\":47157},{\"attributes\":{\"id\":\"fig_1\"},\"end\":47731,\"start\":47342},{\"attributes\":{\"id\":\"fig_2\"},\"end\":47924,\"start\":47732},{\"attributes\":{\"id\":\"fig_3\"},\"end\":48014,\"start\":47925},{\"attributes\":{\"id\":\"fig_5\"},\"end\":48363,\"start\":48015},{\"attributes\":{\"id\":\"fig_7\"},\"end\":48484,\"start\":48364},{\"attributes\":{\"id\":\"fig_8\"},\"end\":48778,\"start\":48485},{\"attributes\":{\"id\":\"fig_9\"},\"end\":48819,\"start\":48779},{\"attributes\":{\"id\":\"fig_10\"},\"end\":48893,\"start\":48820},{\"attributes\":{\"id\":\"fig_11\"},\"end\":48967,\"start\":48894},{\"attributes\":{\"id\":\"fig_12\"},\"end\":49080,\"start\":48968},{\"attributes\":{\"id\":\"fig_13\"},\"end\":49213,\"start\":49081},{\"attributes\":{\"id\":\"fig_14\"},\"end\":49288,\"start\":49214},{\"attributes\":{\"id\":\"fig_15\"},\"end\":49402,\"start\":49289},{\"attributes\":{\"id\":\"fig_16\"},\"end\":49536,\"start\":49403},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":49569,\"start\":49537},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":49615,\"start\":49570},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":50379,\"start\":49616},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":50489,\"start\":50380},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":52656,\"start\":50490},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":52744,\"start\":52657},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":53709,\"start\":52745},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":53814,\"start\":53710},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":54557,\"start\":53815},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":55100,\"start\":54558},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":63733,\"start\":55101},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":64904,\"start\":63734},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":64933,\"start\":64905},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":64968,\"start\":64934},{\"attributes\":{\"id\":\"tab_21\",\"type\":\"table\"},\"end\":65074,\"start\":64969}]", "paragraph": "[{\"end\":2266,\"start\":1555},{\"end\":2935,\"start\":2268},{\"end\":3659,\"start\":2937},{\"end\":4301,\"start\":3661},{\"end\":5444,\"start\":4303},{\"end\":5527,\"start\":5446},{\"end\":5766,\"start\":5529},{\"end\":5975,\"start\":5768},{\"end\":6163,\"start\":5977},{\"end\":6325,\"start\":6165},{\"end\":6418,\"start\":6327},{\"end\":6581,\"start\":6420},{\"end\":9861,\"start\":6598},{\"end\":10398,\"start\":9863},{\"end\":11032,\"start\":10400},{\"end\":11426,\"start\":11067},{\"end\":12395,\"start\":11434},{\"end\":12709,\"start\":12403},{\"end\":13070,\"start\":12711},{\"end\":13392,\"start\":13072},{\"end\":14078,\"start\":13394},{\"end\":14418,\"start\":14086},{\"end\":14907,\"start\":14420},{\"end\":15263,\"start\":14909},{\"end\":15884,\"start\":15265},{\"end\":16344,\"start\":15886},{\"end\":17351,\"start\":16346},{\"end\":18992,\"start\":17374},{\"end\":21299,\"start\":19030},{\"end\":21339,\"start\":21301},{\"end\":22267,\"start\":21341},{\"end\":23340,\"start\":22269},{\"end\":23886,\"start\":23342},{\"end\":26485,\"start\":23918},{\"end\":26607,\"start\":26517},{\"end\":28247,\"start\":26609},{\"end\":28628,\"start\":28287},{\"end\":29159,\"start\":28630},{\"end\":29615,\"start\":29161},{\"end\":31416,\"start\":29650},{\"end\":31868,\"start\":31418},{\"end\":33327,\"start\":31903},{\"end\":33859,\"start\":33342},{\"end\":35092,\"start\":33861},{\"end\":35523,\"start\":35110},{\"end\":36213,\"start\":35525},{\"end\":36386,\"start\":36215},{\"end\":36719,\"start\":36388},{\"end\":37101,\"start\":36721},{\"end\":37228,\"start\":37103},{\"end\":37488,\"start\":37230},{\"end\":37828,\"start\":37490},{\"end\":40456,\"start\":37871},{\"end\":40959,\"start\":40468},{\"end\":42277,\"start\":40961},{\"end\":42464,\"start\":42279},{\"end\":42502,\"start\":42466},{\"end\":42582,\"start\":42504},{\"end\":42765,\"start\":42607},{\"end\":42977,\"start\":42775},{\"end\":44033,\"start\":42989},{\"end\":44993,\"start\":44070},{\"end\":45357,\"start\":44995},{\"end\":46045,\"start\":45359},{\"end\":46381,\"start\":46047},{\"end\":47156,\"start\":46383}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":14724,\"start\":14686},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":17167,\"start\":17160},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":17913,\"start\":17906},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":19184,\"start\":19177},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":19213,\"start\":19206},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":23429,\"start\":23410},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":24491,\"start\":24484},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":25384,\"start\":25377},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":28534,\"start\":28527},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":29211,\"start\":29204},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":37929,\"start\":37921},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":38656,\"start\":38648},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":39381,\"start\":39373},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":40455,\"start\":40447},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":43149,\"start\":43142}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1553,\"start\":1541},{\"attributes\":{\"n\":\"2\"},\"end\":6596,\"start\":6584},{\"attributes\":{\"n\":\"3\"},\"end\":11065,\"start\":11035},{\"attributes\":{\"n\":\"3.1\"},\"end\":11432,\"start\":11429},{\"attributes\":{\"n\":\"3.2\"},\"end\":12401,\"start\":12398},{\"attributes\":{\"n\":\"3.3\"},\"end\":14084,\"start\":14081},{\"attributes\":{\"n\":\"3.4\"},\"end\":17372,\"start\":17354},{\"attributes\":{\"n\":\"4\"},\"end\":19002,\"start\":18995},{\"attributes\":{\"n\":\"4.1\"},\"end\":19028,\"start\":19005},{\"attributes\":{\"n\":\"4.2\"},\"end\":23916,\"start\":23889},{\"attributes\":{\"n\":\"4.3\"},\"end\":26515,\"start\":26488},{\"attributes\":{\"n\":\"4.4\"},\"end\":28285,\"start\":28250},{\"attributes\":{\"n\":\"4.5\"},\"end\":29648,\"start\":29618},{\"attributes\":{\"n\":\"4.6\"},\"end\":31901,\"start\":31871},{\"attributes\":{\"n\":\"5\"},\"end\":33340,\"start\":33330},{\"end\":35108,\"start\":35095},{\"end\":37869,\"start\":37831},{\"end\":40466,\"start\":40459},{\"end\":42605,\"start\":42585},{\"end\":42773,\"start\":42768},{\"end\":42987,\"start\":42980},{\"end\":44068,\"start\":44036},{\"end\":47178,\"start\":47158},{\"end\":47348,\"start\":47343},{\"end\":47753,\"start\":47733},{\"end\":47946,\"start\":47926},{\"end\":48024,\"start\":48016},{\"end\":48375,\"start\":48365},{\"end\":48508,\"start\":48486},{\"end\":48791,\"start\":48780},{\"end\":48832,\"start\":48821},{\"end\":48906,\"start\":48895},{\"end\":48980,\"start\":48969},{\"end\":49093,\"start\":49082},{\"end\":49226,\"start\":49215},{\"end\":49301,\"start\":49290},{\"end\":49415,\"start\":49404},{\"end\":49547,\"start\":49538},{\"end\":49580,\"start\":49571},{\"end\":49626,\"start\":49617},{\"end\":50390,\"start\":50381},{\"end\":50500,\"start\":50491},{\"end\":52667,\"start\":52658},{\"end\":53720,\"start\":53711},{\"end\":53823,\"start\":53816},{\"end\":54568,\"start\":54559},{\"end\":64916,\"start\":64906},{\"end\":64945,\"start\":64935},{\"end\":64980,\"start\":64970}]", "table": "[{\"end\":50379,\"start\":49724},{\"end\":52656,\"start\":50545},{\"end\":53709,\"start\":52890},{\"end\":54557,\"start\":54052},{\"end\":55100,\"start\":54570},{\"end\":63733,\"start\":55233},{\"end\":64904,\"start\":64135}]", "figure_caption": "[{\"end\":47341,\"start\":47181},{\"end\":47731,\"start\":47350},{\"end\":47924,\"start\":47756},{\"end\":48014,\"start\":47949},{\"end\":48363,\"start\":48026},{\"end\":48484,\"start\":48377},{\"end\":48778,\"start\":48512},{\"end\":48819,\"start\":48794},{\"end\":48893,\"start\":48835},{\"end\":48967,\"start\":48909},{\"end\":49080,\"start\":48983},{\"end\":49213,\"start\":49096},{\"end\":49288,\"start\":49229},{\"end\":49402,\"start\":49304},{\"end\":49536,\"start\":49418},{\"end\":49569,\"start\":49549},{\"end\":49615,\"start\":49582},{\"end\":49724,\"start\":49628},{\"end\":50489,\"start\":50392},{\"end\":50545,\"start\":50502},{\"end\":52744,\"start\":52669},{\"end\":52890,\"start\":52747},{\"end\":53814,\"start\":53722},{\"end\":54052,\"start\":53825},{\"end\":55233,\"start\":55103},{\"end\":64135,\"start\":63736},{\"end\":64933,\"start\":64919},{\"end\":64968,\"start\":64948},{\"end\":65074,\"start\":64983}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19323,\"start\":19315},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19360,\"start\":19352},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21892,\"start\":21884},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":21957,\"start\":21949},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22044,\"start\":22036},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22057,\"start\":22049},{\"end\":26201,\"start\":26195},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":26725,\"start\":26717},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":27483,\"start\":27475},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":30206,\"start\":30198},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":30648,\"start\":30640},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":32119,\"start\":32111},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":36114,\"start\":36105},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":43406,\"start\":43397},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":43551,\"start\":43543},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":43562,\"start\":43553},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":43630,\"start\":43621},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":43832,\"start\":43823},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":46895,\"start\":46881}]", "bib_author_first_name": null, "bib_author_last_name": null, "bib_entry": null, "bib_title": null, "bib_author": null, "bib_venue": null}}}, "year": 2023, "month": 12, "day": 17}