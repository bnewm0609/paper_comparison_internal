{"id": 117928740, "updated": "2023-11-07 19:31:33.968", "metadata": {"title": "Quantum algorithms for supervised and unsupervised machine learning", "authors": "[{\"first\":\"Seth\",\"last\":\"Lloyd\",\"middle\":[]},{\"first\":\"Masoud\",\"last\":\"Mohseni\",\"middle\":[]},{\"first\":\"Patrick\",\"last\":\"Rebentrost\",\"middle\":[]}]", "venue": null, "journal": "arXiv: Quantum Physics", "publication_date": {"year": 2013, "month": 7, "day": 1}, "abstract": "Machine-learning tasks frequently involve problems of manipulating and classifying large numbers of vectors in high-dimensional spaces. Classical algorithms for solving such problems typically take time polynomial in the number of vectors and the dimension of the space. Quantum computers are good at manipulating high-dimensional vectors in large tensor product spaces. This paper provides supervised and unsupervised quantum machine learning algorithms for cluster assignment and cluster finding. Quantum machine learning can take time logarithmic in both the number of vectors and their dimension, an exponential speed-up over classical algorithms.", "fields_of_study": "[\"Physics\"]", "external_ids": {"arxiv": "1307.0411", "mag": "199424061", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": null, "doi": null}}, "content": {"source": {"pdf_hash": "9b0aa51901f05278928bdfcb4e9826a429a81293", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1307.0411v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "a887aec3ab061868a7caefc7471b657c11ad2c45", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9b0aa51901f05278928bdfcb4e9826a429a81293.txt", "contents": "\nQuantum algorithms for supervised and unsupervised machine learning\n4 Nov 2013\n\nSeth Lloyd \nResearch Laboratory for Electronics\nMassachusetts Institute of Technology\n\n\nMasoud Mohseni \nGoogle Research\n\nPatrick Rebentrost \nResearch Laboratory for Electronics\nMassachusetts Institute of Technology\n\n\nQuantum algorithms for supervised and unsupervised machine learning\n4 Nov 20133. To whom correspondence should be addressed: slloyd@mit.edu\nMachine-learning tasks frequently involve problems of manipulating and classifying large numbers of vectors in high-dimensional spaces. Classical algorithms for solving such problems typically take time polynomial in the number of vectors and the dimension of the space. Quantum computers are good at manipulating high-dimensional vectors in large tensor product spaces. This paper provides supervised and unsupervised quantum machine learning algorithms for cluster assignment and cluster finding. Quantum machine learning can take time logarithmic in both the number of vectors and their dimension, an exponential speed-up over classical algorithms. In machine learning, information processors perform tasks of sorting, assembling, assimilating, and classifying information [1-2]. In supervised learning, the machine infers a function from a set of training examples. In unsupervised learning the machine tries to find hidden structure in unlabeled data. Recent studies and applications focus in particular on the problem of large-scale machine learning [2] -big data -where the training set and/or the number of features is large. Various results on quantum machine learning investigate the use of quantum information processors to perform machine learning tasks [3-9], including pattern matching [3], Probably Approximately Correct learning [4], feedback learning for quantum measurement [5], binary classifiers [6-7], and quantum support vector machines [8].\n\nThis paper shows that quantum machine learning can provide exponential speed-ups over classical computers for a variety of learning tasks. The intuition is straightforward.\n\nMachine learning is about manipulating and classifying large amounts of data. The data is typically post-processed and ordered in arrays (vectors) and arrays of arrays (tensor products): quantum computers are good at manipulating vectors and tensor products in high-dimensional spaces. In different machine learning settings, the speed-up plays out in different fashions. First, classical data expressed in the form of N -dimensional complex vectors can be mapped onto a quantum states over log 2 N qubits: when the data is stored in a quantum random access memory (qRAM), this mapping takes O(log 2 N ) steps [10][11][12][13][14][15][16]. Once it is in quantum form, the data can be post-processed by various quantum algorithms (quantum Fourier transforms [17], matrix inversion [18], etc.), which take time O(poly(log N )). Estimating distances and inner products between post-processed vectors in N -dimensional vector spaces then takes time O(log N ) on a quantum computer. By contrast, as noted by Aaronson [19], sampling and estimating distances and inner products between post-processed vectors on a classical computer is apparently exponentially hard.\n\nQuantum machine learning provides an exponential speed-ups over all known classical algorithms for problems involving evaluating distances and inner products between large vectors.\n\nIn this paper, we show that the problem of assigning N -dimensional vectors to one of several clusters of M states takes time O(log(M N )) on a quantum computer, compared with time O(poly(M N )) for the best known classical algorithm. That is, quantum machine learning can provide an exponential speed-up for problems involving large numbers of vectors as well (\"big quantum data\"). We present a quantum version of Lloyd's algorithm for performing k-means clustering: using a novel version of the quantum adiabatic algorithm one can classify M vectors into k clusters in time O(k log kM N ).\n\nFinally, we note that in addition to supplying exponential speed-ups in both number of vectors and their dimension, quantum machine learning allows enhanced privacy: only O(log(M N )) calls to the quantum data-base are required to perform cluster assignment, while O(M N ) are required to uncover the actual data. The data-base user can still obtain information about the desired patterns, while the data-base owner is assured that the user has only accessed an exponentially small fraction of the data base.\n\n\nData preparation and pre-processing\n\nClassically, data sets are typically presented as arrays of symbols and numbers. We assume that our data sets that consist of arrays of numbers (vectors), and arrays of arrays (collections of vectors), originally stored in random access memory (RAM) in the classical case, or in quantum random access memory (qRAM) in the quantum case [10][11][12][13][14][15][16]. The key feature of quantum machine learning is that quantum random access memory allows us to access the data in quantum parallel. Begin with state preparation. Consider the vector N = 2 n dimensional complex vector v with components {v i = |v i |e i\u03c6 i }. Assume that {|v i |, \u03c6 i } are stored as floating point numbers in quantum random access memory.\n\nConstructing the log 2 N qubit quantum state |v = | v| \u22121/2 v then takes O(log 2 N ) steps as long as the sub-norms n \u2113 = \u2113 i=1 |v i | 2 can be estimated efficiently [20][21][22]. Alternatively, we can assume that these sub-norms are also given in qRAM in which case any quantum state can be constructed in O(log N ) steps. As will now be shown, this allows us to evaluate generalized inner products u|QF T |v , and u|f (A)|v between the quantum vectors. By contrast, as noted by Aaronson [19], the best known algorithms for evaluating the classical versions of these generalized inner products u \u2020 F T v, u \u2020 f (A) v, via sampling and classical postprocessing takes time O(polyN ).\n\n\nSupervised cluster assignment:\n\nConsider the task of assigning a post-processed vector u \u2208 R N to one of two sets V, W , given M representative samples v j \u2208 V and M samples w k \u2208 W . A common method for such an assignment is to evaluate the distance | u\u2212(1/M ) j v j | between u and the mean of the vectors in V , and to assign u to V if this distance is smaller than the distance between u and the mean of W . We now exhibit a quantum algorithm for performing the assignment problem that takes time O(\u01eb \u22121 log(M N )). In the quantum assignment problem, assume that the vectors are presented as quantum states |u , {|v j }, {|w k }. If the vectors are are not normalized to one, assume that their normalizations | v j |, | w k | are given separately.\n\nTo evaluate the distance from u to the mean of V , adjoin an ancilla variable with\nM + 1 states. First, construct the state |\u03c8 = (1/ \u221a 2) |0 |u + (1/ \u221a M ) M j=1\n|j |v j for system and ancilla by querying the quantum RAM or by the subroutine described above.\n\nSecond, use a swap test [17] to perform a projective measurement on the ancilla alone to\nsee if it is in the state |\u03c6 = (1/ \u221a Z)(| u| |0 \u2212 (1/ \u221a M ) j | v j | |j ) for the ancilla alone, where Z = | u| 2 + (1/M ) j | v j | 2 .\nIt is straightforward to verify that the desired distance,\n| u \u2212 (1/M ) j v j | 2 ,\nis equal to Z times the probability of success for this measurement.\n\nThe state |\u03c6 can be generated by using quantum access to the norms together with quantum simulation to apply the unitary transformation\ne \u2212iHt , where H = ( | u||0 0| + j | v j ||j j| ) \u2297 \u03c3 x , to the state (1/ \u221a 2)( |0 \u2212 (1/ \u221a M ) j |j ) \u2297 |0 . The result is the state (1/ \u221a 2) cos(| u|t) |0 \u2212 (1/ \u221a M ) j cos(| v j |t) |j \u2297 |0 \u2212 (i/ \u221a 2) sin(| u|t) |0 \u2212 (1/ \u221a M ) j sin(| v j |t) |j \u2297 |1 .(1)\nChoosing t so that | u|t, | v j |t \u226a 1 and measuring the ancilla bit then yields the state |\u03c6\nwith probability (1/2)(| u| 2 + (1/M ) j | v j | 2 )t 2 = Z 2 t 2 .\nThis procedure creates the desired state and, when repeated, also allows the quantity Z to be estimated. A more efficient way to create the state and to estimate Z to accuracy \u01eb is to use Grover's algorithm/quantum counting [17]. Quantum counting takes time O(\u01eb \u22121 log M ), and also allows quantum coherence to be preserved during the state creation.\n\n\nUnsupervised quantum learning:\n\nThe exponential quantum speed-up above holds for supervised learning. A similar speed-up extends to unsupervised learning. Consider the k-means problem of assigning M vectors to k clusters in a way that minimizes the average distance to the centroid of the cluster. The standard method for solving k-means is Lloyd's algorithm [1][2] (no relation to the co-author of this paper): (0) choose the initial centroid randomly or by a method such as k-means ++ ; (1) assign each vector to the cluster with the closest mean; (2)  The distance-finding algorithm given above allows us to apply any Hamiltonian of the form,\nH s = j 1 ...j k f ({| v j \u2113 \u2212 v j \u2113 \u2032 | 2 })|j 1 j 1 | \u2297 . . . \u2297 |j k j k |.(2)\nTo find good seeds for k-means, use a final Hamiltonian for the adiabatic algorithm of the\nform (2) with f = \u2212 k \u2113,\u2113 \u2032 =1 | v j \u2113 \u2212 v j \u2113 \u2032 | 2 .\nThe ground state of this final Hamiltonian is the seed set that maximizes the average distance squared between seeds.\n\nWe can also use the adiabatic algorithm to find sets of r vectors that should lie in the same cluster. Here the final Hamiltonian is of the form\nH c = j 1 ...j r f ({| v j \u2113 \u2212 v j \u2113 \u2032 | 2 })|j 1 j 1 | \u2297 . . . \u2297 |j r j r |,(3)where f = r \u2113,\u2113 \u2032 =1 | v j \u2113 \u2212 v j \u2113 \u2032 | 2 + \u03ba\u03b4 j \u2113 ,j \u2113 \u2032 , \u03ba > 0.\nBecause of the overall positive sign, the distance term now rewards sets of vectors that are clustered closely, while the \u03ba\u03b4 j \u2113 ,j \u2113 \u2032 ensures that the vectors in the \u2113 and \u2113 \u2032 positions are different (we already know that a vector lies in the same cluster as itself). To find such sets of vectors that are expected to lie in the same cluster can take time O(r log M N ), depending on the probability of success of the quantum adiabatic algorithm (see next paragraph). Combining this 'attractive'\n\nHamiltonian with the 'repulsive' Hamiltonian of (2) allows one to find kr representative groups of r vectors from each of the k clusters.\n\nThe success of the quantum adiabatic algorithm in finding the ground state of the final Hamiltonian relies on traversing the minimum gap point of the quantum phase transition between the initial and final Hamiltonians sufficiently slowly. Finding the optimal seed set of size k classically is a combinatorially hard problem in k, and finding the optimal cluster of r vectors is combinatorially hard in r. Accordingly, the minimum gap and time to find the ground state may well scale exponentially in k, r. Indeed, optimal kmeans is an NP-complete problem which we do not expect to solve in polynomial time on a computer, classical or quantum. Approximate solutions of these hard problems are well within the grasp of the quantum adiabatic algorithm, however. k-means ++ does not require an optimal seed set, but merely a good seed set with well-separated vectors. In addition, in k-means we are interested in finding various sets of highly clustered vectors, not only the optimal set. Even running the algorithm for a time linear in O(k log M N ) is likely to suffice to construct pretty good seed sets and clusters. We can reasonably hope that an adiabatic quantum computer that traverses the minimum gap in finite time \u03c4 at finite temperature T should be able to find approximate solutions whose energy is within max{O(kT ), O(h/\u03c4 )} of the minimum energy. The question of how well the adiabatic algorithm performs on average is an open one.\n\n\nExtension to nonlinear metrics:\n\nThe quantum algorithm for determining distance can be generalized to nonlinear metrics to compare vectors using the results of [18]. Given q copies of |u , |v , the quantum phase algorithm can be used to evaluate ( u| v|) \u2297k L(|u |v ) \u2297k for arbitrary Hermitian L, allowing distance measures that are q'th order polynomials in the u j , v j . Measurement of the expectation value of L to accuracy \u01eb using quantum counting requires O(\u01eb \u22121 q log N ) steps. Once again, the quantum algorithm reduces the dimension dependence of the evaluation of distance to O(log N ).\n\n\nDiscussion:\n\nThe power of quantum computers to manipulate large numbers of high-dimensional vectors makes them natural systems for performing vector-based machine learning tasks.\n\nOperations that involve taking vector dot products, overlaps, norms, etc., in N -dimensional vector spaces that take time O(N ) in the classical machine learning algorithms, take time O(log N ) in the quantum version. These abilities, combined with the quantum linear systems algorithm [18], represent a powerful suite of tools for manipulating large amounts of data. Once the data has been processed in a quantum form, as in the adiabatic quantum algorithm for search engine ranking [23], then measurements can be made on the processed data to reveal aspects of the data that can take exponentially longer to reveal by classical algorithms Here, we presented a quantum algorithm for assigning a vector to clusters of M vectors that takes time O(log M N ), an exponential speed-up in both M (quantum big data) and N . We used this algorithm as a subroutine for the standard k-means algorithm to provide an exponential speed-up for unsupervised learning (quantum Lloyd's algorithm) via the adiabatic algorithm.\n\nCurrently, the rate of generation of electronic data generated per year is estimated to be on the order of 10 18 bits. This entire data set could be represented by a quantum state using 60 bits, and the clustering analysis could be performed using a few hundred operations. Even if the number of bits to be analyzed were to expand to the entire information content of the universe within the particle horizon, O(10 90 \u2248 2 300 ) bits, in principle the data representation and analysis would be well within the capacity of a relatively small quantum computer.\n\nThe generic nature of the quantum speed-ups for dealing with large numbers of highdimensional vectors suggests that a wide variety of machine learning algorithms may be susceptible to exponential speed-up on a quantum computer. Quantum machine learning also provides advantages in terms of privacy: the data base itself is of size O(M N ), but the owner of the data base supplies only O(log M N ) quantum bits to the user who is performing the quantum machine learning algorithm. In addition to supplying an exponential speedup over classical machine learning algorithms, quantum machine learning methods for analyzing large data sets ('big quantum data') supply significant advantages in terms of privacy for the owners of that data. That is, the adiabatic algorithm can be used to assign states to clusters in the next step of the quantum Lloyd's algorithm.\n\nRepeating d times to create d copies, one can now iterate this quantum adiabatic algorithm to create a quantum superposition of the cluster assignments at each step. Continue the reassignment until the cluster assignment state is unchanged (which can be verified, e.g., using a swap test). Since Lloyd's algorithm typically converges after a small number of steps, we rapidly arrive at the clustering state |\u03c7 = (1/ \u221a M ) c,j\u2208c |c |j \u2208 c . The resulting k-means clustered quantum state |\u03c7 contains the final optimized k-means clusters in quantum superposition and can be sampled to obtain information about the contents of the individual clusters.\n\nTo calculate the scaling of finding the clustering state, note first that each distance evaluation is essentially a weak measurement [24] that perturbs the clustered state |\u03c8 \u2113 \u2297d at the previous level by an amount < d \u221a d\u03b4 2 (measured by fidelity), where \u03b4 is the accuracy of the distance evaluation. Accordingly, as long as the desired accuracy is \u03b4 > 1/d 2/3 , d copies of the next cluster assignment state can be created from the d copies of the previous cluster assignment state.\n\nTo evaluate the time that the adiabatic algorithm takes note that adiabatic part of the algorithm acts only on the c \u2032 cluster labels, and that the overlap squared between the initial state of each step (S6) and the final state (S7) is O(1/k). Accordingly, the time per step that the algorithm requires is no greater than O(k log kM N ) (and could be as small as O(log kM N ) if the minimum gap during the adiabatic stage is O(1)). As Lloyd's algorithm typically converges after a relatively small number of steps, our estimate for the overall algorithm to construct the clustering state |\u03c7 is O(k log kM N ).\n\n\nOnce the exponentially compressed quantum versions of the vectors have been created, we can postprocess them using quantum Fourier transforms, matrix inversion, etc., to create vectors of the form QF T |v , f (A)|v , where A is a sparse Hermitian matrix and f is a computable function, e.g., f (A) = A \u22121 . The postprocessing takes time O(poly(log N ))\n\n\nrecalculate the centroids of the clusters; repeat steps (1-2) until a stationary assignment is attained. When classical estimation of the distance to the centroids in the N -dimensional space takes time O(N ), each step of the classical algorithm takes time O(M 2 N ), while the quantum Lloyd's algorithm takes time O(M log(M N )). The additional factor of M in both classical and quantum algorithms arises because every vector is tested individually for reassignment at each step.The quantum Lloyd's algorithm can be improved by noting that the k-means problem can be rephrased as a quadratic programming problem which is amenable to solution by the adiabatic algorithm. As will now be seen, such unsupervised quantum machine learn-ing takes time at most O(k log(M N )) and can even take only O(log(kM N )). In order to reduce the dependence on the number of vectors from O(M log M ) to O(log M ), the output of the computation can no longer be a list of the M vectors and their cluster assignments. Instead, the output is a quantum state |\u03c7 = (1/ \u221a M ) j |c j |j = (1/ \u221a M ) c,j\u2208c |c |j that contains the labels j of vectors correlated with their cluster assignments c j in superposition: we can then sample from that state to obtain a statistical picture of the clustering. The procedure for constructing the clustering state |\u03c7 via the quantum adiabatic algorithm is given in the supplementary material. The algorithm takes time no greater than O(\u01eb \u22121 k log kM N ) to construct this state to accuracy \u01eb, and could take time as little as O(\u01eb \u22121 log kM N ) if the clusters are relatively well separated, so that the gap of the adiabatic algorithm is O(1). Any algorithm that reveals the assignment of all M vectors necessarily takes time O(M ) merely to print out the output. Many questions about the k-means clustering can be answered using smaller outputs. As we now show, adiabatic algorithms provide a powerful method for answering clustering questions. First, look at the problem of finding initial seeds for the clusters. As the efficiency of the k-means ++ algorithm shows, the performance of Lloyd's algorithm, classical or quantum, depends strongly on a good choice of initial seeds. Initial seed vectors should be spread as far apart from each other as possible. Begin the adiabatic seed-finding algorithm in the state |\u03a8 = |\u03c8 1 \u2297 . . . |\u03c8 k , where |\u03c8 = (1/ \u221a M ) M j=1 |j is the uniform superposition of vector labels, and with initial Hamiltonian H 0 = 1 \u2212 |\u03a8 \u03a8|.\n\n\nAcknowledgments: This work was supported by DARPA, Google, NSF, ARO under a MURI program, Jeffrey Epstein, and FQXi. The authors thank Scott Aaronson for helpful discussions.\nSupplementary material:Here we present an adiabatic algorithm for constructing a quantum statec,j\u2208c |c |j (S1) that contains the output of the unsupervised k-means clustering algorithm in quantum form. This state contains a uniform superposition of all the vectors, each assigned to its appropriate cluster, and can be sampled to provide information about which states are in the same or in different clusters. For the quantum clustering algorithm, proceed as in the original Lloyd's algorithm, but express all means in quantum superposition. At the first step, select k vectors with labels i c as initial seeds for each of the clusters. These may be chosen at random, or in a way that maximizes the average distance between them, as in k-means ++ . Then re-cluster. We show by induction that the re-clustering can be performed efficiently by the quantum adiabatic algorithm.For the first step, begin with the state9The multiple copies of the seed state 1 \u221a k c |c |i c combined with the distance evaluation techniques given in the paper allow one to evaluate the distances | v j \u2212 v i c \u2032 | 2 in the c \u2032 j component of the superposition, and to apply the phase. This is equivalent to applying the HamiltonianNow perform the adiabatic algorithm with the initial Hamiltonian H 0 = 1 \u2212 |\u03c6 \u03c6|, where to perform the adiabatic algorithm accurately will be evaluated below. The result is the first-order clustering statewhere each j is associated with the c with the closest seed vector i c . By constructing multiple copies of this state, one can also construct the individual cluster states |\u03c6 c 1 = (1/ \u221a M c ) j\u2208c |j and estimate the number of states M c in the c'th cluster. Now continue. At the next re-clustering step, assume that d copies of the state |\u03c8 1 are made available from the previous step. The ability to construct the individual cluster states |\u03c6 c 1 together with the ability to perform the distance evaluation as in the paper allows to evaluate the average distance between v j and the mean of clusterThis ability in turn allows us to apply a phase e \u2212i| v j \u2212v c \u2032 | 2 \u03b4t to each component |c \u2032 |j of the superposition, which is equivalent to applying the HamiltonianNow, perform the adiabatic algorithm, starting with the state\nE Alpaydn, Introduction to Machine Learning (Adaptive Computation and Machine Learning). CambridgeMIT PressE. Alpaydn Introduction to Machine Learning (Adaptive Computation and Machine Learning), MIT Press, Cambridge (2004).\n\nInformation Theory, Inference and Learning Algorithms. D Mackay, Cambridge University PressCambridgeD. Mackay Information Theory, Inference and Learning Algorithms, Cambridge Uni- versity Press, Cambridge (2003).\n\n. M Sasaki, A Carlini, arXiv:quant-ph/0202173Phys. Rev. A. 6622303M. Sasaki, A. Carlini, Phys. Rev. A 66, 022303 (2002); arXiv: quant-ph/0202173.\n\n. R A Servedio, S J Gortler, Siam J Comput, arXiv:quant-ph/0007036331067R. A. Servedio and S. J. Gortler, SIAM J. Comput. 33, 1067, (2004); arXiv: quant- ph/0007036.\n\n. A Hentschel, B C Sanders, arXiv:0910.0762Phys. Rev. Lett. 10463603A. Hentschel, B.C. Sanders, Phys. Rev. Lett. 104 (2010), 063603; arXiv: 0910.0762.\n\nH Neven, V S Denchev, G Rose, W G Macready, arXiv:0912.0779Training a Large Scale Classifier with the Quantum Adiabatic Algorithm. H. Neven, V.S. Denchev, G. Rose, W.G. Macready, Training a Large Scale Classifier with the Quantum Adiabatic Algorithm, arXiv: quant-ph/0811.0416; arXiv: 0912.0779.\n\n. K L Pudenz, D A Lidar, arXiv:1109.0325Quant. Inf. Proc. 122027K.L. Pudenz, D.A. Lidar, Quant. Inf. Proc. 12, 2027 (2013); arXiv: 1109.0325.\n\n. D Anguita, S Ridella, F Rivieccion, R Zunino, Neural Networks. 16D. Anguita, S. Ridella, F. Rivieccion, R. Zunino, Neural Networks 16, 763-770 (2003).\n\nMachine learning in a quantum world. E A\u00efmeur, G Brassard, S Gambs, Advances in Artificial Intelligence. Luc Lamontagne and Mario MarchandBerlin/HeidelbergSpringer4013431E. A\u00efmeur, G. Brassard, S. Gambs, Machine learning in a quantum world, In Luc Lamontagne and Mario Marchand, editors, Advances in Artificial Intelligence, volume 4013 of Lecture Notes in Computer Science, page 431. Springer, Berlin/Heidelberg, 2006.\n\n. V Giovannetti, S Lloyd, L Maccone, arXiv:0708.1879Phys.Rev.Lett. 100160501V. Giovannetti, S. Lloyd, L. Maccone, Phys.Rev.Lett. 100, 160501 (2008); arXiv: 0708.1879.\n\n. V Giovannetti, S Lloyd, L Maccone, arXiv:0807.4994Phys.Rev.A. 7852310V. Giovannetti, S. Lloyd, L. Maccone, Phys.Rev.A 78, 052310 (2008); arXiv: 0807.4994.\n\n. F De Martini, V Giovannetti, S Lloyd, L Maccone, E Nagali, L Sansoni, F Sciarrino, arXiv:0902.0222Phys. Rev. A. 8010302F. De Martini, V. Giovannetti, S. Lloyd, L. Maccone, E. Nagali, L. Sansoni, F. Sciar- rino, Phys. Rev. A 80, 010302(R) (2009); arXiv: 0902.0222.\n\n. I Chiorescu, N Groll, S Bertaina, T Mori, S Miyashita, Phys. Rev. B. 8224413I. Chiorescu, N. Groll, S. Bertaina, T. Mori, S. Miyashita, Phys. Rev. B 82, 024413 (2010).\n\n. D I Schuster, A P Sears, E Ginossar, L Dicarlo, L Frunzio, J J L Morton, H , D.I. Schuster, A. P. Sears, E. Ginossar, L. DiCarlo, L. Frunzio, J. J. L. Morton, H.\n\n. G A D Wu, B B Briggs, D D Buckley, R J Awschalom, Schoelkopf, Phys. Rev. Lett. 105140501Wu, G. A. D. Briggs, B. B. Buckley, D. D. Awschalom, R. J. Schoelkopf, Phys. Rev. Lett. 105, 140501 (2010).\n\n. Y Kubo, F R Ong, P Bertet, D Vion, V Jacques, D Zheng, A Drau, J.-F Roch, A Auffeves, F Jelezko, J Wrachtrup, M F Barthe, P Bergonzo, D Esteve, Phys. Rev. Lett. 105140502Y. Kubo, F. R. Ong, P. Bertet, D. Vion, V. Jacques, D. Zheng, A. Drau, J.-F. Roch, A. Auffeves, F. Jelezko, J. Wrachtrup, M. F. Barthe, P. Bergonzo, D. Esteve, Phys. Rev. Lett. 105, 140502 (2010).\n\n. H Wu, R E George, J H Wesenberg, K Mlmer, D I Schuster, R J Schoelkopf, K M , H. Wu, R.E. George, J.H. Wesenberg, K. Mlmer, D.I. Schuster, R.J. Schoelkopf, K.M.\n\n. A Itoh, J J L Ardavan, G A D Morton, Briggs, Phys. Rev. Lett. 105140503Itoh, A. Ardavan, J.J.L. Morton, G.A.D. Briggs, Phys. Rev. Lett. 105, 140503 (2010).\n\nM S Nielsen, I L Chuang, Quantum computation and quantum information. CambridgeCambridge University PressM.S. Nielsen, I.L. Chuang, Quantum computation and quantum information, Cam- bridge University Press, Cambridge, 2000.\n\n. A W Harrow, A Hassidim, S Lloyd, arXiv:0811.3171Phys. Rev. Lett. 15150502A.W. Harrow, A. Hassidim, S. Lloyd, Phys. Rev. Lett. 15, 150502 (2009); arXiv: 0811.3171.\n\nBQP and the polynomial hierarchy. S Aaronson, arXiv:0910.4698S. Aaronson, 'BQP and the polynomial hierarchy,' arXiv:0910.4698.\n\nCreating superpositions that correspond to efficiently integrable probability distributions. L Grover, T Rudolpha, arXiv:quant-ph/0208112L. Grover, T. Rudolpha 'Creating superpositions that correspond to efficiently inte- grable probability distributions,' arXiv: quant-ph/0208112.\n\nP Kaye, M Mosca, arXiv:quant-ph/0407102Proceedings of the International Conference on Quantum Information. the International Conference on Quantum InformationRochester, New YorkP. Kaye, M. Mosca, in Proceedings of the International Conference on Quantum In- formation, Rochester, New York, 2001; arXiv: quant-ph/0407102.\n\n. A N Soklakov, R Schack, Phys. Rev. A. 7312307A. N. Soklakov, R. Schack, Phys. Rev. A 73, 012307 (2006).\n\n. S Garnerone, P Zanardi, D A Lidar, arXiv:1109.6546Phys. Rev. Lett. 108230506S. Garnerone, P. Zanardi, D.A. Lidar, Phys. Rev. Lett. 108, 230506 (2012): arXiv: 1109.6546.\n\n. S Lloyd, J.-J E Slotine, arXiv:quant-ph/9905064Phys. Rev. A. 62012307S. Lloyd, J.-J.E. Slotine, Phys. Rev. A 6201, 2307 (2000); arXiv: quant-ph/9905064.\n", "annotations": {"author": "[{\"end\":168,\"start\":81},{\"end\":201,\"start\":169},{\"end\":297,\"start\":202}]", "publisher": null, "author_last_name": "[{\"end\":91,\"start\":86},{\"end\":183,\"start\":176},{\"end\":220,\"start\":210}]", "author_first_name": "[{\"end\":85,\"start\":81},{\"end\":175,\"start\":169},{\"end\":209,\"start\":202}]", "author_affiliation": "[{\"end\":167,\"start\":93},{\"end\":200,\"start\":185},{\"end\":296,\"start\":222}]", "title": "[{\"end\":68,\"start\":1},{\"end\":365,\"start\":298}]", "venue": null, "abstract": "[{\"end\":1901,\"start\":438}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2691,\"start\":2687},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2695,\"start\":2691},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2699,\"start\":2695},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2703,\"start\":2699},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2707,\"start\":2703},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2711,\"start\":2707},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2715,\"start\":2711},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2838,\"start\":2834},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2861,\"start\":2857},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3093,\"start\":3089},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4900,\"start\":4896},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4904,\"start\":4900},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4908,\"start\":4904},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4912,\"start\":4908},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4916,\"start\":4912},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4920,\"start\":4916},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4924,\"start\":4920},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5451,\"start\":5447},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5455,\"start\":5451},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5459,\"start\":5455},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5774,\"start\":5770},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7007,\"start\":7003},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8145,\"start\":8141},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8632,\"start\":8629},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8635,\"start\":8632},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":11803,\"start\":11799},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12710,\"start\":12706},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12908,\"start\":12904},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":15637,\"start\":15633}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":16950,\"start\":16596},{\"attributes\":{\"id\":\"fig_1\"},\"end\":19431,\"start\":16951},{\"attributes\":{\"id\":\"fig_2\"},\"end\":19608,\"start\":19432}]", "paragraph": "[{\"end\":2075,\"start\":1903},{\"end\":3236,\"start\":2077},{\"end\":3418,\"start\":3238},{\"end\":4011,\"start\":3420},{\"end\":4521,\"start\":4013},{\"end\":5279,\"start\":4561},{\"end\":5963,\"start\":5281},{\"end\":6717,\"start\":5998},{\"end\":6801,\"start\":6719},{\"end\":6977,\"start\":6881},{\"end\":7067,\"start\":6979},{\"end\":7264,\"start\":7206},{\"end\":7358,\"start\":7290},{\"end\":7495,\"start\":7360},{\"end\":7848,\"start\":7755},{\"end\":8267,\"start\":7917},{\"end\":8915,\"start\":8302},{\"end\":9087,\"start\":8997},{\"end\":9260,\"start\":9143},{\"end\":9406,\"start\":9262},{\"end\":10052,\"start\":9555},{\"end\":10191,\"start\":10054},{\"end\":11636,\"start\":10193},{\"end\":12237,\"start\":11672},{\"end\":12418,\"start\":12253},{\"end\":13429,\"start\":12420},{\"end\":13988,\"start\":13431},{\"end\":14849,\"start\":13990},{\"end\":15498,\"start\":14851},{\"end\":15984,\"start\":15500},{\"end\":16595,\"start\":15986}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6880,\"start\":6802},{\"attributes\":{\"id\":\"formula_1\"},\"end\":7205,\"start\":7068},{\"attributes\":{\"id\":\"formula_2\"},\"end\":7289,\"start\":7265},{\"attributes\":{\"id\":\"formula_3\"},\"end\":7754,\"start\":7496},{\"attributes\":{\"id\":\"formula_4\"},\"end\":7916,\"start\":7849},{\"attributes\":{\"id\":\"formula_5\"},\"end\":8996,\"start\":8916},{\"attributes\":{\"id\":\"formula_6\"},\"end\":9142,\"start\":9088},{\"attributes\":{\"id\":\"formula_7\"},\"end\":9487,\"start\":9407},{\"attributes\":{\"id\":\"formula_8\"},\"end\":9554,\"start\":9487}]", "table_ref": null, "section_header": "[{\"end\":4559,\"start\":4524},{\"end\":5996,\"start\":5966},{\"end\":8300,\"start\":8270},{\"end\":11670,\"start\":11639},{\"end\":12251,\"start\":12240}]", "table": null, "figure_caption": "[{\"end\":16950,\"start\":16598},{\"end\":19431,\"start\":16953},{\"end\":19608,\"start\":19434}]", "figure_ref": null, "bib_author_first_name": "[{\"end\":21855,\"start\":21854},{\"end\":22136,\"start\":22135},{\"end\":22297,\"start\":22296},{\"end\":22307,\"start\":22306},{\"end\":22444,\"start\":22443},{\"end\":22446,\"start\":22445},{\"end\":22458,\"start\":22457},{\"end\":22460,\"start\":22459},{\"end\":22474,\"start\":22470},{\"end\":22476,\"start\":22475},{\"end\":22611,\"start\":22610},{\"end\":22624,\"start\":22623},{\"end\":22626,\"start\":22625},{\"end\":22761,\"start\":22760},{\"end\":22770,\"start\":22769},{\"end\":22772,\"start\":22771},{\"end\":22783,\"start\":22782},{\"end\":22791,\"start\":22790},{\"end\":22793,\"start\":22792},{\"end\":23060,\"start\":23059},{\"end\":23062,\"start\":23061},{\"end\":23072,\"start\":23071},{\"end\":23074,\"start\":23073},{\"end\":23203,\"start\":23202},{\"end\":23214,\"start\":23213},{\"end\":23225,\"start\":23224},{\"end\":23239,\"start\":23238},{\"end\":23392,\"start\":23391},{\"end\":23402,\"start\":23401},{\"end\":23414,\"start\":23413},{\"end\":23778,\"start\":23777},{\"end\":23793,\"start\":23792},{\"end\":23802,\"start\":23801},{\"end\":23946,\"start\":23945},{\"end\":23961,\"start\":23960},{\"end\":23970,\"start\":23969},{\"end\":24104,\"start\":24103},{\"end\":24118,\"start\":24117},{\"end\":24133,\"start\":24132},{\"end\":24142,\"start\":24141},{\"end\":24153,\"start\":24152},{\"end\":24163,\"start\":24162},{\"end\":24174,\"start\":24173},{\"end\":24371,\"start\":24370},{\"end\":24384,\"start\":24383},{\"end\":24393,\"start\":24392},{\"end\":24405,\"start\":24404},{\"end\":24413,\"start\":24412},{\"end\":24542,\"start\":24541},{\"end\":24544,\"start\":24543},{\"end\":24556,\"start\":24555},{\"end\":24558,\"start\":24557},{\"end\":24567,\"start\":24566},{\"end\":24579,\"start\":24578},{\"end\":24590,\"start\":24589},{\"end\":24601,\"start\":24600},{\"end\":24605,\"start\":24602},{\"end\":24615,\"start\":24614},{\"end\":24707,\"start\":24706},{\"end\":24711,\"start\":24708},{\"end\":24717,\"start\":24716},{\"end\":24719,\"start\":24718},{\"end\":24729,\"start\":24728},{\"end\":24731,\"start\":24730},{\"end\":24742,\"start\":24741},{\"end\":24744,\"start\":24743},{\"end\":24906,\"start\":24905},{\"end\":24914,\"start\":24913},{\"end\":24916,\"start\":24915},{\"end\":24923,\"start\":24922},{\"end\":24933,\"start\":24932},{\"end\":24941,\"start\":24940},{\"end\":24952,\"start\":24951},{\"end\":24961,\"start\":24960},{\"end\":24972,\"start\":24968},{\"end\":24980,\"start\":24979},{\"end\":24992,\"start\":24991},{\"end\":25003,\"start\":25002},{\"end\":25016,\"start\":25015},{\"end\":25018,\"start\":25017},{\"end\":25028,\"start\":25027},{\"end\":25040,\"start\":25039},{\"end\":25276,\"start\":25275},{\"end\":25282,\"start\":25281},{\"end\":25284,\"start\":25283},{\"end\":25294,\"start\":25293},{\"end\":25296,\"start\":25295},{\"end\":25309,\"start\":25308},{\"end\":25318,\"start\":25317},{\"end\":25320,\"start\":25319},{\"end\":25332,\"start\":25331},{\"end\":25334,\"start\":25333},{\"end\":25348,\"start\":25347},{\"end\":25350,\"start\":25349},{\"end\":25440,\"start\":25439},{\"end\":25448,\"start\":25447},{\"end\":25452,\"start\":25449},{\"end\":25463,\"start\":25462},{\"end\":25467,\"start\":25464},{\"end\":25597,\"start\":25596},{\"end\":25599,\"start\":25598},{\"end\":25610,\"start\":25609},{\"end\":25612,\"start\":25611},{\"end\":25824,\"start\":25823},{\"end\":25826,\"start\":25825},{\"end\":25836,\"start\":25835},{\"end\":25848,\"start\":25847},{\"end\":26022,\"start\":26021},{\"end\":26209,\"start\":26208},{\"end\":26219,\"start\":26218},{\"end\":26399,\"start\":26398},{\"end\":26407,\"start\":26406},{\"end\":26723,\"start\":26722},{\"end\":26725,\"start\":26724},{\"end\":26737,\"start\":26736},{\"end\":26830,\"start\":26829},{\"end\":26843,\"start\":26842},{\"end\":26854,\"start\":26853},{\"end\":26856,\"start\":26855},{\"end\":27002,\"start\":27001},{\"end\":27014,\"start\":27010},{\"end\":27016,\"start\":27015}]", "bib_author_last_name": "[{\"end\":21863,\"start\":21856},{\"end\":22143,\"start\":22137},{\"end\":22304,\"start\":22298},{\"end\":22315,\"start\":22308},{\"end\":22455,\"start\":22447},{\"end\":22468,\"start\":22461},{\"end\":22483,\"start\":22477},{\"end\":22621,\"start\":22612},{\"end\":22634,\"start\":22627},{\"end\":22767,\"start\":22762},{\"end\":22780,\"start\":22773},{\"end\":22788,\"start\":22784},{\"end\":22802,\"start\":22794},{\"end\":23069,\"start\":23063},{\"end\":23080,\"start\":23075},{\"end\":23211,\"start\":23204},{\"end\":23222,\"start\":23215},{\"end\":23236,\"start\":23226},{\"end\":23246,\"start\":23240},{\"end\":23399,\"start\":23393},{\"end\":23411,\"start\":23403},{\"end\":23420,\"start\":23415},{\"end\":23790,\"start\":23779},{\"end\":23799,\"start\":23794},{\"end\":23810,\"start\":23803},{\"end\":23958,\"start\":23947},{\"end\":23967,\"start\":23962},{\"end\":23978,\"start\":23971},{\"end\":24115,\"start\":24105},{\"end\":24130,\"start\":24119},{\"end\":24139,\"start\":24134},{\"end\":24150,\"start\":24143},{\"end\":24160,\"start\":24154},{\"end\":24171,\"start\":24164},{\"end\":24184,\"start\":24175},{\"end\":24381,\"start\":24372},{\"end\":24390,\"start\":24385},{\"end\":24402,\"start\":24394},{\"end\":24410,\"start\":24406},{\"end\":24423,\"start\":24414},{\"end\":24553,\"start\":24545},{\"end\":24564,\"start\":24559},{\"end\":24576,\"start\":24568},{\"end\":24587,\"start\":24580},{\"end\":24598,\"start\":24591},{\"end\":24612,\"start\":24606},{\"end\":24714,\"start\":24712},{\"end\":24726,\"start\":24720},{\"end\":24739,\"start\":24732},{\"end\":24754,\"start\":24745},{\"end\":24766,\"start\":24756},{\"end\":24911,\"start\":24907},{\"end\":24920,\"start\":24917},{\"end\":24930,\"start\":24924},{\"end\":24938,\"start\":24934},{\"end\":24949,\"start\":24942},{\"end\":24958,\"start\":24953},{\"end\":24966,\"start\":24962},{\"end\":24977,\"start\":24973},{\"end\":24989,\"start\":24981},{\"end\":25000,\"start\":24993},{\"end\":25013,\"start\":25004},{\"end\":25025,\"start\":25019},{\"end\":25037,\"start\":25029},{\"end\":25047,\"start\":25041},{\"end\":25279,\"start\":25277},{\"end\":25291,\"start\":25285},{\"end\":25306,\"start\":25297},{\"end\":25315,\"start\":25310},{\"end\":25329,\"start\":25321},{\"end\":25345,\"start\":25335},{\"end\":25445,\"start\":25441},{\"end\":25460,\"start\":25453},{\"end\":25474,\"start\":25468},{\"end\":25482,\"start\":25476},{\"end\":25607,\"start\":25600},{\"end\":25619,\"start\":25613},{\"end\":25833,\"start\":25827},{\"end\":25845,\"start\":25837},{\"end\":25854,\"start\":25849},{\"end\":26031,\"start\":26023},{\"end\":26216,\"start\":26210},{\"end\":26228,\"start\":26220},{\"end\":26404,\"start\":26400},{\"end\":26413,\"start\":26408},{\"end\":26734,\"start\":26726},{\"end\":26744,\"start\":26738},{\"end\":26840,\"start\":26831},{\"end\":26851,\"start\":26844},{\"end\":26862,\"start\":26857},{\"end\":27008,\"start\":27003},{\"end\":27024,\"start\":27017}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":22078,\"start\":21854},{\"attributes\":{\"id\":\"b1\"},\"end\":22292,\"start\":22080},{\"attributes\":{\"doi\":\"arXiv:quant-ph/0202173\",\"id\":\"b2\"},\"end\":22439,\"start\":22294},{\"attributes\":{\"doi\":\"arXiv:quant-ph/0007036\",\"id\":\"b3\"},\"end\":22606,\"start\":22441},{\"attributes\":{\"doi\":\"arXiv:0910.0762\",\"id\":\"b4\"},\"end\":22758,\"start\":22608},{\"attributes\":{\"doi\":\"arXiv:0912.0779\",\"id\":\"b5\"},\"end\":23055,\"start\":22760},{\"attributes\":{\"doi\":\"arXiv:1109.0325\",\"id\":\"b6\"},\"end\":23198,\"start\":23057},{\"attributes\":{\"id\":\"b7\"},\"end\":23352,\"start\":23200},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":206598347},\"end\":23773,\"start\":23354},{\"attributes\":{\"doi\":\"arXiv:0708.1879\",\"id\":\"b9\"},\"end\":23941,\"start\":23775},{\"attributes\":{\"doi\":\"arXiv:0807.4994\",\"id\":\"b10\"},\"end\":24099,\"start\":23943},{\"attributes\":{\"doi\":\"arXiv:0902.0222\",\"id\":\"b11\"},\"end\":24366,\"start\":24101},{\"attributes\":{\"id\":\"b12\"},\"end\":24537,\"start\":24368},{\"attributes\":{\"id\":\"b13\"},\"end\":24702,\"start\":24539},{\"attributes\":{\"id\":\"b14\"},\"end\":24901,\"start\":24704},{\"attributes\":{\"id\":\"b15\"},\"end\":25271,\"start\":24903},{\"attributes\":{\"id\":\"b16\"},\"end\":25435,\"start\":25273},{\"attributes\":{\"id\":\"b17\"},\"end\":25594,\"start\":25437},{\"attributes\":{\"id\":\"b18\"},\"end\":25819,\"start\":25596},{\"attributes\":{\"doi\":\"arXiv:0811.3171\",\"id\":\"b19\"},\"end\":25985,\"start\":25821},{\"attributes\":{\"doi\":\"arXiv:0910.4698\",\"id\":\"b20\"},\"end\":26113,\"start\":25987},{\"attributes\":{\"doi\":\"arXiv:quant-ph/0208112\",\"id\":\"b21\"},\"end\":26396,\"start\":26115},{\"attributes\":{\"doi\":\"arXiv:quant-ph/0407102\",\"id\":\"b22\"},\"end\":26718,\"start\":26398},{\"attributes\":{\"id\":\"b23\"},\"end\":26825,\"start\":26720},{\"attributes\":{\"doi\":\"arXiv:1109.6546\",\"id\":\"b24\"},\"end\":26997,\"start\":26827},{\"attributes\":{\"doi\":\"arXiv:quant-ph/9905064\",\"id\":\"b25\"},\"end\":27153,\"start\":26999}]", "bib_title": "[{\"end\":23389,\"start\":23354}]", "bib_author": "[{\"end\":21865,\"start\":21854},{\"end\":22145,\"start\":22135},{\"end\":22306,\"start\":22296},{\"end\":22317,\"start\":22306},{\"end\":22457,\"start\":22443},{\"end\":22470,\"start\":22457},{\"end\":22485,\"start\":22470},{\"end\":22623,\"start\":22610},{\"end\":22636,\"start\":22623},{\"end\":22769,\"start\":22760},{\"end\":22782,\"start\":22769},{\"end\":22790,\"start\":22782},{\"end\":22804,\"start\":22790},{\"end\":23071,\"start\":23059},{\"end\":23082,\"start\":23071},{\"end\":23213,\"start\":23202},{\"end\":23224,\"start\":23213},{\"end\":23238,\"start\":23224},{\"end\":23248,\"start\":23238},{\"end\":23401,\"start\":23391},{\"end\":23413,\"start\":23401},{\"end\":23422,\"start\":23413},{\"end\":23792,\"start\":23777},{\"end\":23801,\"start\":23792},{\"end\":23812,\"start\":23801},{\"end\":23960,\"start\":23945},{\"end\":23969,\"start\":23960},{\"end\":23980,\"start\":23969},{\"end\":24117,\"start\":24103},{\"end\":24132,\"start\":24117},{\"end\":24141,\"start\":24132},{\"end\":24152,\"start\":24141},{\"end\":24162,\"start\":24152},{\"end\":24173,\"start\":24162},{\"end\":24186,\"start\":24173},{\"end\":24383,\"start\":24370},{\"end\":24392,\"start\":24383},{\"end\":24404,\"start\":24392},{\"end\":24412,\"start\":24404},{\"end\":24425,\"start\":24412},{\"end\":24555,\"start\":24541},{\"end\":24566,\"start\":24555},{\"end\":24578,\"start\":24566},{\"end\":24589,\"start\":24578},{\"end\":24600,\"start\":24589},{\"end\":24614,\"start\":24600},{\"end\":24618,\"start\":24614},{\"end\":24716,\"start\":24706},{\"end\":24728,\"start\":24716},{\"end\":24741,\"start\":24728},{\"end\":24756,\"start\":24741},{\"end\":24768,\"start\":24756},{\"end\":24913,\"start\":24905},{\"end\":24922,\"start\":24913},{\"end\":24932,\"start\":24922},{\"end\":24940,\"start\":24932},{\"end\":24951,\"start\":24940},{\"end\":24960,\"start\":24951},{\"end\":24968,\"start\":24960},{\"end\":24979,\"start\":24968},{\"end\":24991,\"start\":24979},{\"end\":25002,\"start\":24991},{\"end\":25015,\"start\":25002},{\"end\":25027,\"start\":25015},{\"end\":25039,\"start\":25027},{\"end\":25049,\"start\":25039},{\"end\":25281,\"start\":25275},{\"end\":25293,\"start\":25281},{\"end\":25308,\"start\":25293},{\"end\":25317,\"start\":25308},{\"end\":25331,\"start\":25317},{\"end\":25347,\"start\":25331},{\"end\":25353,\"start\":25347},{\"end\":25447,\"start\":25439},{\"end\":25462,\"start\":25447},{\"end\":25476,\"start\":25462},{\"end\":25484,\"start\":25476},{\"end\":25609,\"start\":25596},{\"end\":25621,\"start\":25609},{\"end\":25835,\"start\":25823},{\"end\":25847,\"start\":25835},{\"end\":25856,\"start\":25847},{\"end\":26033,\"start\":26021},{\"end\":26218,\"start\":26208},{\"end\":26230,\"start\":26218},{\"end\":26406,\"start\":26398},{\"end\":26415,\"start\":26406},{\"end\":26736,\"start\":26722},{\"end\":26746,\"start\":26736},{\"end\":26842,\"start\":26829},{\"end\":26853,\"start\":26842},{\"end\":26864,\"start\":26853},{\"end\":27010,\"start\":27001},{\"end\":27026,\"start\":27010}]", "bib_venue": "[{\"end\":21941,\"start\":21865},{\"end\":22133,\"start\":22080},{\"end\":22351,\"start\":22339},{\"end\":22666,\"start\":22651},{\"end\":22889,\"start\":22819},{\"end\":23113,\"start\":23097},{\"end\":23263,\"start\":23248},{\"end\":23457,\"start\":23422},{\"end\":23840,\"start\":23827},{\"end\":24005,\"start\":23995},{\"end\":24213,\"start\":24201},{\"end\":24437,\"start\":24425},{\"end\":24783,\"start\":24768},{\"end\":25064,\"start\":25049},{\"end\":25499,\"start\":25484},{\"end\":25664,\"start\":25621},{\"end\":25886,\"start\":25871},{\"end\":26019,\"start\":25987},{\"end\":26206,\"start\":26115},{\"end\":26503,\"start\":26437},{\"end\":26758,\"start\":26746},{\"end\":26894,\"start\":26879},{\"end\":27060,\"start\":27048},{\"end\":21952,\"start\":21943},{\"end\":23509,\"start\":23492},{\"end\":25675,\"start\":25666},{\"end\":26575,\"start\":26505}]"}}}, "year": 2023, "month": 12, "day": 17}