{"id": 212675399, "updated": "2023-10-06 17:54:23.243", "metadata": {"title": "CC2Vec: Distributed Representations of Code Changes", "authors": "[{\"first\":\"Thong\",\"last\":\"Hoang\",\"middle\":[]},{\"first\":\"Hong\",\"last\":\"Kang\",\"middle\":[\"Jin\"]},{\"first\":\"Julia\",\"last\":\"Lawall\",\"middle\":[]},{\"first\":\"David\",\"last\":\"Lo\",\"middle\":[]}]", "venue": "2020 IEEE/ACM 42nd International Conference on Software Engineering (ICSE)", "journal": "Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering", "publication_date": {"year": 2020, "month": 3, "day": 12}, "abstract": "Existing work on software patches often use features specific to a single task. These works often rely on manually identified features, and human effort is required to identify these features for each task. In this work, we propose CC2Vec, a neural network model that learns a representation of code changes guided by their accompanying log messages, which represent the semantic intent of the code changes. CC2Vec models the hierarchical structure of a code change with the help of the attention mechanism and uses multiple comparison functions to identify the differences between the removed and added code. To evaluate if CC2Vec can produce a distributed representation of code changes that is general and useful for multiple tasks on software patches, we use the vectors produced by CC2Vec for three tasks: log message generation, bug fixing patch identification, and just-in-time defect prediction. In all tasks, the models using CC2Vec outperform the state-of-the-art techniques.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2003.05620", "mag": "3105867435", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icse/HoangK0L20", "doi": "10.1145/3377811.3380361"}}, "content": {"source": {"pdf_hash": "0dfe706526e5234338411489e1826b4060acc4e8", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2003.05620v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://hal.inria.fr/hal-03030530/file/main.pdf", "status": "GREEN"}}, "grobid": {"id": "4e811f7df0fcc85631731ebb4a78ad34e4e676e2", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0dfe706526e5234338411489e1826b4060acc4e8.txt", "contents": "\nCC2Vec: Distributed Representations of Code Changes\n\n\nThong Hoang \nSingapore Management University\nSingapore\n\nHong Jin Kang \nSingapore Management University\nSingapore\n\nDavid Lo davidlo@smu.edu.sg \nSingapore Management University\nSingapore\n\nJulia Lawall julia.lawall@inria.fr \nSorbonne University\n/Inria/LIP6France\n\nCC2Vec: Distributed Representations of Code Changes\n10.1145/3377811.3380361\nExisting work on software patches often use features specific to a single task. These works often rely on manually identified features, and human effort is required to identify these features for each task. In this work, we propose CC2Vec, a neural network model that learns a representation of code changes guided by their accompanying log messages, which represent the semantic intent of the code changes. CC2Vec models the hierarchical structure of a code change with the help of the attention mechanism and uses multiple comparison functions to identify the differences between the removed and added code.To evaluate if CC2Vec can produce a distributed representation of code changes that is general and useful for multiple tasks on software patches, we use the vectors produced by CC2Vec for three tasks: log message generation, bug fixing patch identification, and just-in-time defect prediction. In all tasks, the models using CC2Vec outperform the state-of-the-art techniques.\n\nINTRODUCTION\n\nPatches, used to edit source code, are often created by developers to describe new features, fix bugs, or maintain existing functionality (e.g., API updates, refactoring, etc.). Patches contain two main pieces of information, a log message and a code change. The log message, used to describe the semantics of the code changes, is written in natural language by the developers. The code change indicates the lines of code to remove or add across one or multiple files. Research has shown that the study of historical patches can be employed to solve software engineering problems, such as just-in-time defect prediction [21,28], identification of bug fixing patches [22,57], tangled change prediction [34], recommendation of a code reviewer for a patch [50], and many more.\n\nExploring patches to solve software engineering problems requires choosing a representation of the patch data. Most prior work involves manually crafting a set of features to represent a patch and using these features for further processing [28-30, 44, 57, 60]. These features have mostly been extracted from properties of patches, such as the modifications to source code (e.g., number of removed and added lines, the number of files modified), the history of changes (e.g., the number of prior or recent changes to the updated files), the record of patch authors and reviewers (e.g., the number of developers or reviewers who contributed to the patch), etc. These Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ICSE '20, May [23][24][25][26][27][28][29]2020 features can be used as an input to a machine learning classifier (e.g., Support Vector Machine, Logistic Regression, Random Forest, etc.) to address various software engineering tasks [28,34,50,57]. Extracting a suitable vector representation to represent the \"meaning\" of a patch is certainly crucial. Intuitively, the quality of a patch representation plays a major role in determining the eventual learning outcome.\n\nIn this paper, to boost the effectiveness of existing solutions that employ the properties of patches, we wish to learn vector representations of the code changes in patches that can be used for a number of tasks. We propose a new deep learning architecture named CC2Vec that can effectively embed a code change into a vector space where similar changes are close to each other. As log messages, written by developers, are used to describe the semantics of the code change, we use them to supervise the learning of code changes' representations from patches. Specifically, CC2Vec optimizes the vector representation of a code change in a patch to predict appropriate words, extracted from the first line of the log message. We consider only the first line, as it is the focus of many prior works [39,49], and is considered to carry the most semantic meaning with the least noise. 1 CC2Vec analyzes the code change, i.e., scattered fragments of removed and added code across multiple files. Code removed or added from a file follows a hierarchical structure (words form line, lines form hunks). Recent work has suggested that the attention mechanism can help in modelling structural dependencies [3,32], thus, we hypothesize that the attention mechanism may be effective for modelling the structure of a code change. We propose a specialized hierarchical attention network (HAN) to construct a vector representation of the removed code (and another for the added code) of each affected file in a given patch. Our HAN first builds vector representations of lines; these vectors are then used to construct vector representations of hunks; and we then aggregate these vectors to construct the embedding vector of the removed or added code. Next, we employ multiple comparison functions to capture the difference between two embedding vectors representing removed and added code. This produces features representing the relationship between the removed and added code. Each comparison function produces a vector and these vectors are then concatenated to form an embedding vector for the affected file. Finally, the embedding vectors of all the affected files are concatenated to build a vector representation of the code change in a patch. After training is completed, CC2Vec can be used to extract representations of code changes even from patches with empty or meaningless log messages (which are common in practice [26,38,39]). CC2Vec is also programminglanguage agnostic; one can use it to learn vector representations of code changes for any language.\n\nThe code change representation enables us to employ the power of (potentially a large number of) unlabeled patch data to improve the effectiveness of supervised learning tasks (also known as semisupervised learning [12]). We can use the code change representation to boost the effectiveness of many supervised learning tasks (e.g., identification of bug fixing patches, just-in-time defect prediction, etc.), especially on those tasks for which only a limited set of labeled data may be available.\n\nCC2Vec converts code changes into their distributed representations by learning from a large collection of patches. The distributed representation captures pertinent features of the code changes by considering the characteristics of the whole collection of patches. Such distributed representations can be used as additional features for other tasks. Past studies have demonstrated the value of distributed representations to improve text classification [43], action recognition [40], image classification [13], etc. Unfortunately, prior to our work, there is no existing solution that can produce a distributed representation of a code change.\n\nTo evaluate the effectiveness of CC2Vec, we employ the representation learned by CC2Vec in three software engineering tasks: 1) log message generation [39] 2) bug fixing patch identification [22] and 3) just-in-time defect prediction [21]. In the first task of log message generation, we generate the first line of a log message given a code change. CC2Vec can be used to improve over the best baseline by 24.73% in terms of BLEU score (an accuracy measure that is widely used to evaluate machine translation systems). For the task of identifying bug fixing patches, CC2Vec helps to improve the best performing baseline by 5.22%, 9.18%, 4.36%, and 6.51% in terms of accuracy, precision, F1, and Area Under the Curve (AUC). For just-in-time defect prediction, CC2Vec helps to improve the AUC metric by 7.03% and 7.72% on the QT and OPENSTACK datasets [42] as compared to the best baseline.\n\nThe main contributions of this work are as follows:\n\n\u2022 We propose a deep learning architecture, namely CC2Vec, that learns distributed representations of code changes guided by the semantic meaning contained in log messages. To the best of our knowledge, our work is the first work in this direction. \u2022 We empirically investigate the value of integrating the code change vectors generated by CC2Vec and feature vectors used by state-of-the-art approaches on three tasks (i.e., log message generation, bug fixing patch identification, and justin-time defect prediction) and demonstrate improvements.\n\nThe rest of this paper is organized as follows. Section 2 elaborates the design of CC2Vec. Section 3 describes the experiments that demonstrate the value of our learned code change representations to aid in the three different tasks. Section 4 presents an ablation study and some threats to validity. Section 5 describes related prior studies. We conclude and mention future work in Section 6.\n\n\nAPPROACH\n\nIn this section, we first present an overview of our framework. We then describe the details of each part of the framework. Finally, we present an algorithm for learning effective settings of our model's parameters.  Figure 1: The overall framework of CC2Vec. Feature extraction layers are used to construct the embedding vectors for each affected file from a given patch (i.e., e f 1 , e f 2 , etc). The embedding vectors are then concatenated to build a vector representation for the code change in the patch (code change vector). The code change vector is connected to the fully connected layer and is learned by minimizing an objective function of the word prediction layer. Figure 1 illustrates the overall framework of CC2Vec. CC2Vec takes the code change of a patch as input and generates its distributed representation. CC2Vec uses the first line of the log message of the patch to supervise learning the code change representation. Specifically, the framework of CC2Vec includes five parts:\n\n\nFramework Overview\n\n\u2022 Preprocessing: This part takes information from the code change of the given patch as an input and outputs a list of files. Each file includes a set of removed code lines and added code lines. \u2022 Input layer: This part encodes each changed file as a threedimensional matrix to be given as input to the hierarchical attention network (HAN) for extracting features. \u2022 Feature extraction layers: This part extracts the embedding vector (a.k.a. features) of each changed file. The resulting embedding vectors are then concatenated to form the vector representation of the code change in a given patch.\n\n\u2022 Feature fusion layers and word prediction layer: This part maps the vector representation of the code change to a word vector extracted from the first line of log message; the word vector indicates the probabilities that various words describe the patch.\n\nCC2Vec employs the first line of the log message of a patch to guide the learning of a suitable vector that represents the code change. Words, extracted from the first line of log message, can be viewed as semantic labels provided by developers. Specifically, we define a learning task to construct a prediction function f : P \u2192 Y, where y i \u2208 Y indicates the set of words extracted from the first line of the log message of the patch p i \u2208 P. The prediction function f is learned by minimizing the differences between the predicted and actual words chosen to describe the patch. After the prediction function f is learned, for each patch, we can obtain its code change vector from the intermediate output between the feature extraction and feature fusion layers (see Figure 1). We explain the details of each part in the following subsections.\n\n\nPreprocessing\n\nThe code change of the given patch includes changes made to one or more files. Each changed file contains a set of lines of removed code and added code. We process the code change of each patch by the following steps:\n\n\u2022 Split the code change based on the affected files. We first separate the information about the code change to each changed file into a separate code document (i.e., File 1 , File 2 , etc., see Figure 1). \u2022 Tokenize the removed code and added code lines. For the changes affecting each changed file, we employ the NLTK library [9] for natural language processing (NLP) to parse its removed code lines or added code lines into a sequence of words. We ignore blank lines in the changed file. \u2022 Construct a code vocabulary. Based on the code changes of the patches in the training data, we build a vocabulary V C . This vocabulary contains the set of code tokens that appear in the code changes of the collection of patches.\n\nAt the end of this step, all the changed files of the given patch are extracted from the code changes and they are fed to the input layer of our framework for further processing.\n\n\nInput Layer\n\nA code change may include changes to multiple files; the changes to each file may contain changes to different hunks; and each hunk contains a list of removed and/or added code lines. To preserve this structural information, in each changed file, we represent the removed (added) code as a three-dimensional matrix, i.e., B \u2208 R H\u00d7L\u00d7W , where H is the number of hunks, L is the number of removed (added) code lines for each hunk, and W is the number of words in each removed (added) code line in the affected file. We use B r and B a to denote the three-dimensional matrix of the removed and added code respectively.\n\nNote that each patch may contain a different number of affected files (F ), each file may contain a different number of hunks (H ), each hunk may contain a different number of lines (L), and each line may contain a different number of words (W ). For parallelization [21,31], each input instance is padded or truncated to the same F , H , L, and W .\n\n\nFeature Extraction Layers\n\nThe feature extraction layers are used to automatically build an embedding vector representing the code change made to a given file in the patch. The embedding vectors of code changes to multiple files are then concatenated into a single vector representing the code change made by the patch. As shown in Figure 2, for each affected file, the feature extraction layers take as input two matrices (denoted by \"-\" and \"+\" in Figure 2) representing the removed code and added code, respectively. These two matrices are passed to the hierarchical attention network to construct corresponding embedding vectors: e r representing the removed code and e a representing the added code (see Figure 2). These two embedding vectors are fed to the comparison layers to produce the vectors representing the difference between the removed code and the added code. These vectors are then concatenated to represent the code changes in each affected file. We present the hierarchical attention network and the comparison layers in the following sections. Figure 3. A HAN takes the removed (added) code of an affected file of a given patch as an input and outputs the embedding vector representing the removed (added) code. Our HAN consists of several parts: a word sequence encoder, a word-level attention layer, a line encoder, a line-level attention layer, a hunk sequence encoder, and a hunk attention layer.\n\n\nHierarchical Attention Network. The architecture of our hierarchical attention network (HAN) is shown in\n\nSuppose that the removed (added) code of the affected file contains a sequence of hunks H = [t 1 , t 2 , . . . , t H ], each hunk t i includes a sequence of lines [s i1 , s i2 , . . . , s i L ], and each line s i j contains a sequence of words [w i j1 , w i j2 , . . . , w i jW ]. w i jk with k \u2208 [1,W ] represents the word in the j\u2212th line in the i\u2212th hunk. Now, we describe how the embedding vector of the removed (added) code is built using the hierarchical structure. Word encoder. Given a line s i j with a sequence of words w i jk and a word embedding matrix W \u2208 R | V C |\u00d7d , where V C is the vocabulary containing all words extracted from the code changes and d is the dimension of the representation of word, we first build the matrix representation of each word in the sequence as follows:\nw i jk = W[w i jk ](1)\nwhere w i jk \u2208 R d indicates the vector representation of word w i jk in the word embedding matrix W. We employ a bidirectional GRU to summarize information from the context of a word in both directions [6]. To capture this contextual information, the bidirectional GRU includes a forward GRU that reads the line s i j from w i j1 to w i jW and a backward GRU that reads the line s i j from w i jW to w i j1 .\n\u2212 \u2212 \u2192 h i jk = \u2212 \u2212\u2212 \u2192 GRU (w i jk ), k \u2208 [1,W ] \u2190 \u2212 \u2212 h i jk = \u2190 \u2212\u2212 \u2212 GRU (w i jk ), k \u2208 [W , 1](2)\nWe obtain an annotation of a given word w i jk by concatenating the forward hidden state \u2212 \u2212 \u2192 h i jk and the backward hidden state\n\u2190 \u2212 \u2212 h i jk of this word, i.e., h i jk = [ \u2212 \u2212 \u2192 h i jk \u2295 \u2190 \u2212 \u2212 h i jk ] (\u2295 is the concatenation operator)\n. h i jk summarizes the word w i jk considering its neighboring words. Word attention. Based on the intuition that not all words contribute equally to extract the \"meaning\" of the line, we use the attention mechanism to highlight words important for predicting the content of the log message. The attention mechanism was previously used in source code summarization and was shown to be effective for encoding source code sequences [26,36]. We also use the attention mechanism to form an embedding vector of the line. We first feed an annotation of a given word w i jk (i.e., h i jk ) through a fully connnected layer (i.e., W w ) to get a hidden representation (i.e., u i jk ) of h i jk as follows:\nu i jk = ReLU(W w h i jk + b w )(3)\nwhere ReLU is the rectified linear unit activation function [45], as it generally provides better performance in various deep learning tasks [4,14]. Similar to Yang et al. [61], we define a word context vector (u w ) that can be seen as a high level representation of the answer to the fixed query \"what is the most informative word\" over the words. The word context vector u w is randomly initialized and learned during the training process. We then measure the importance of the word as the similarity of u i jk with the word context vector u w and get a normalized importance weight \u03b1 i jk through a softmax function [10]:\n\u03b1 i jk = exp(u T i jk u w ) k exp(u T i jk u w )(4)\nFor each line s i j , its vector is computed as a weighted sum of the embedding vectors of the words based on their importance as follows:\ns i j = k \u03b1 i jk h i jk(5)\nLine encoder. Given a line vector (i.e., s i j ), we also use a bidirectional GRU to encode the line as follows:\n\u2212 \u2192 h i j = \u2212 \u2212\u2212 \u2192 GRU (s i j ), j \u2208 [1, L] \u2190 \u2212 h i j = \u2190 \u2212\u2212 \u2212 GRU (s i j ), j \u2208 [L, 1](6)\nSimilar to the word encoder, we obtain an annotation of the line s i j by concatenating the forward hidden state \u2212 \u2192 h i j and backward hidden state \u2190 \u2212 h i j of this line. The annotation of the line s i j is denoted\nas h i j = [ \u2212 \u2192 h i j \u2295 \u2190 \u2212 h i j ]\n, which summarizes the line s i j considering its neighboring lines.\n\nLine attention. We use an attention mechanism to learn the important lines to be used to form a hunk vector as follows:\nu i j = ReLU(W s h i j + b s ) (7) \u03b1 i j = exp(u T i j u s ) j exp(u T i j u s ) (8) t i = j \u03b1 i j h i j (9)\nW s is the fully connected layer to which we need to feed an annotation of the given line (i.e., s i j ). We define u s as the line context vector that can be seen as a high level representation of the answer to the fixed query \"what is the informative line\" over the lines. u s is randomly initialized and learned during the training process. t i is the hunk vector of the i-th hunk in the removed (added) code. Hunk encoder. Given a hunk vector t i , we again use a bidirectional GRU to encode the hunk as follows:\n\u2212 \u2192 h i = \u2212 \u2212\u2212 \u2192 GRU (t i ), t \u2208 [1, H ] \u2190 \u2212 h i = \u2190 \u2212\u2212 \u2212 GRU (t i ), t \u2208 [H, 1](10)\nAn annotation of the hunk t i is then obtained by concatenating the forward hidden state \u2212 \u2192 h i and the backward hidden state\n\u2190 \u2212 h i , i.e., h i = [ \u2212 \u2192 h i , \u2190 \u2212 h i ].\nh i summarizes the hunk t i considering the other hunks around it. Hunk attention. We again use an attention mechanism to learn important hunks used to form an embedding vector of the removed (added) code as follows:\nu i = ReLU(W h h i + b h )(11)\u03b1 i = exp(u T i u t ) i exp(u T i u t ) (12) e = i \u03b1 i h i(13)\nW h is the fully connected layer used to feed an annotation of a given hunk (i.e., h i ). u t is the hunk context vector that can be seen as a high level representation of the answer to the fixed query \"what is the informative hunk\" over the hunks. Similar to u w and u s , u t is randomly initialized and learned during the training process. e, collected at the end of this part, is the embedding vector of the removed (added) code. For convenience, we denote e r and e a as the embedding vectors of the removed code and added code, respectively.\n\n\nComparison\n\nLayers. The goal of the comparison layers is to build the vectors that capture the differences between the removed code and added code of the affected file in a given patch. We use multiple comparison functions [59] to represent different angles of comparison. These comparison functions were previously used in a question answering task. The comparison layers take as input the embedding vectors of the removed code and added code (denoted by e r and e a , respectively) and output the vectors representing the difference between the removed code and the added code.  of the affected file in the given patch. Figure 4 shows the five comparison functions used in the comparison layers to capture the difference between the removed code and added code. We briefly explain these comparison functions in the following paragraphs.\n\n(a) Neural Tensor Network. Inspired by previous works in visual question answering [7], we employ a neural tensor network [52] as follows:\ne NT = ReLU(e T r T [1, ...,n] e a + b NT )(14)\nT i \u2208 R n\u00d7n is a tensor and b NT is the bias value. These parameters are learned during the training process. Note that both the removed code and added code have the same dimension (i.e., e r \u2208 R n , e a ).\n\n(b) Neural Network. We consider a simple feed forward neural network [54]. The output is computed as follows:\ne NN = ReLU(W[e a \u2295 e r ] + b NN )(15)\n\u2295 is the concatenation operator, the matrix W \u2208 R n\u00d72n , and the bias value b NN are parameters to be learned.  \n\nEUC(\u00b7) and COS(\u00b7) are the euclidean distance and cosine similarity, respectively. Note that e sim is a two-dimensional vector.\n\n(d) Element-wise subtraction. We simply perform a subtraction between the embedding vector of the removed code and the embedding vector of the added code.\ne sub = e r \u2212 e a(17)\n(e) Element-wise multiplication. We perform element-wise multiplication for the embedding vectors of the removed code and added code.\ne mul = e r \u2299 e a(18)\nwhere \u2299 is the element-wise multiplication operator. The vectors resulting from applying these five different comparison functions are then concatenated to represent the embedding vector of the affected file (denoted by e f i ) in the given patch as follows:\ne f i = e NT \u2295 e NN \u2295 e sim \u2295 e sub \u2295 e mul(19)\nwhere f i is the i-th file of the code change in the given patch. \n\n\nFeature Fusion and Word Prediction Layers\ne p = e f 1 \u2295 e f 2 \u2295 \u00b7 \u00b7 \u00b7 \u2295 e f F(20)\nWe pass the embedding vector (e p ) into a hidden layer (a fully connected layer) to produce a vector h:\nh = \u03b1(w h e p + b h )(21)\nwhere w h is the weight matrix used to connect the embedding vector e p with the hidden layer and b h is the bias value. Finally, the vector h is passed to a word prediction layer to produce the following:\no = \u2212hw o(22)\nwhere w o is the weight matrix between the hidden layer and the word prediction layer, and o \u2208 R |V M |\u00d71 (V M is a set of words extracted from the first line of log messages). We then apply the sigmoid function [10] to get the probability distribution over words as follows:\np(o i |p i ) = 1 1 + exp(o i )(23)\nwhere o i \u2208 o is the probability score of the i th word and p i is the patch that we want to assign words to.\n\n\nParameter Learning\n\nOur model involves the following parameters: the word embedding matrix of code changes, the hidden states in the different encoders (i.e., the word encoder, line encoder, and hunk encoder), the context vectors of words, lines, and hunks, the weight matrices and the bias values of the neural tensor network and the neural net in the comparison layers, and the weight matrices and the bias values of the hidden layer and the word prediction layer. After these parameters are learned, the vector representation of the code change of each patch can be determined. These parameters are learned by minimizing the following objective function:\nO = y i \u2208y (y i \u00d7 \u2212 log(p(o i |p i )) + (1 \u2212 y i ) \u00d7 \u2212 log(1 \u2212 p(o i |p i ))) + \u03bb 2 \u2225\u03b8 \u2225 2 2(24)\nwhere p(o i |p i ) is the predicted word probability defined in Equation 23, y i = {0, 1} indicates whether the i-th word is part of the log message of the patch p i , and \u03b8 are all parameters of our model. The regularization term, \u03bb 2 \u2225\u03b8 \u2225 2 2 , is used to prevent overfitting in the training process [11]. We employ the dropout technique [53] to improve the robustness of CC2Vec. Since Adam [33] has been shown to be computationally efficient and require low memory consumption, we use it to minimize the objective function (i.e., Equation 24). We also use backpropagation [19], a simple implementation of the chain rule of partial derivatives, to efficiently update the parameters during the training process.\n\n\nEXPERIMENTS\n\nThe goal of this work is to build a representation of code changes that can be applied to multiple tasks. To evaluate the effectiveness of this representation, we employ our framework, namely CC2Vec, on three different tasks, i.e., log message generation [39], bug fixing patch identification [22] and just-in-time defect prediction [21].\n\nIn the first task of log message generation, we use the vector representation of code changes, extracted by CC2Vec, to find a patch that is most similar to another. For the other two tasks, CC2Vec is used to extract additional features that are input to the models of bug fixing patch identification and just-in-time defect prediction. We compare the resulting performance with and without using our code change vector. We next elaborate on the three tasks, the baselines, and results.\n\n\nTask 1: Log Message Generation\n\n3.1.1 Problem Formulation. While we learn representations of code changes with the aid of log messages, we also study the task of generating log messages from code changes. Developers do not always write high-quality log messages. Dyer et al. [16] reported that around 14% of log messages in 23,000 Java projects on SourceForge 2 were empty. Log messages are important for program comprehension and understanding the evolution of software, therefore this motivates the need for the automatic generation of log messages. In this task, given the code change of a given patch, we aim to produce a brief log message summarizing it.\n\n\nState-of-the-art Approach. The state-of-the-art approach is\n\nNNGen [39], which takes as input a new code change with an unknown log message and a training dataset (patches), and outputs a log message for the new code change. NNGen first extracts code changes from the training set. Each code change in the training set and the new code change are represented as vectors in the form of a \"bag-of-words\" [41]. NNGen then calculates the cosine similarity between the vector of the new code change and the vector of each code change in the training set, and selects the top-k nearest neighbouring code changes in the training dataset. From these k nearest neighbours, the BLEU-4 score [48] is computed between each of the code changes in the top-k and the new code change with an unknown log message. A log message of the code change in the top-k with the highest BLEU-4 score is reused as the log message of the new code change.\n\nThe BLEU-4 score is a measure used to evaluate the quality of machine translation systems, measuring the closeness of a translation to a human translation. It is computed as follows:\nBLEU = BP \u00b7 exp N n=1 1 N log (p n ) BP = 1 if c > r e (1\u2212r /c) if c \u2264 r\nN is the maximum number of N-grams. Following the previous work [39], we select N = 4. p n is the ratio of length n subsequences that are present in both the output and reference translation. BP is a brevity penalty to penalize short output sentences. Finally, c is the length of the output translation and r is the length of the reference translation.\n\nA deep learning approach was previously proposed for this task by Jiang et al. [26], however, it underperformed the simpler baseline NNGen. In this study, we refer to their work as NMT. Their approach modelled this task as a neural machine translation task, translating the code change to a target log message. Like our work, they proposed an attention-based model, however, our work differs 2 https://sourceforge.net/  [39], after identifying the closest code changes, we reuse the log message as the output.\n\n\nExperimental Setting.\n\nThe purpose of evaluating CC2Vec on this task is to determine if the code change representations received from CC2Vec outperform the naive representation used by Liu et al. [39]. Jiang et al. [26] originally collected and filtered the commits to construct the original dataset. Another version of the dataset was used by Liu et al. [39], who modified the original dataset. Jiang et al. extracted a total of 2 million patches from the 1K most starred Java projects. They collected the first line of each log message. To normalize the dataset, patch ids and issue ids were removed from the code changes and log messages. Patches were filtered to remove merges, rollbacks, and patches that were too long. The log messages that do not conform to verb-direct-object pattern, e.g. \"delete a method\", are also removed. After filtering, the dataset contains 32K patches.\n\nStill, even with all this cleaning, Liu et al. [39] investigated the dataset and found that there were many patches with bot messages and trivial messages. Bot messages refer to messages produced automatically by other development tools, such as continuous integration bots. Trivial messages refer to messages containing only information that can be obtained by looking at the names of the changed files (e.g. \"modify dockerfile\"). Such messages are of low quality and Liu et al. used regular expressions to locate and remove these patches.\n\nWe used the original dataset of Jiang et al. [26] and the cleaned dataset of Liu et al. [39] for evaluation. While the original dataset consists of a training dataset of 30K patches and a testing dataset of 3K patches, the cleaned dataset consists of a training dataset of 22K patches and a testing dataset of 2.5K patches. To compare the different approaches, we use BLEU-4 to evaluate each approach since this was used in both previous works.\n\n\nResults.\n\nWe report the performance of LogGen, NNGen and NMT in Table 1. LogGen outperforms both NNGen and NMT. The\n\nClean dataset refers to the dataset which Liu et al. filtered out patches with bot and trivial log messages. On this dataset, LogGen outperforms NNGen and NMT by a BLEU-4 score of 4.06 and 6.29 respectively. LogGen improves over the performance of NNGen by 24.75%, a greater improvement than NNGen's improvement over NMT of 15.70% . On the original dataset collected by Jiang et al., LogGen outperforms NNGen and NMT by a BLEU-4 score of 4.65 and 11.28. These results indicate that LogGen can improve over the performance of NNGen and NMT by 12.06% and 2.07% in terms of the BLEU-4 score respectively.\n\nThus, we conclude that the log messages retrieved by LogGen are closer in quality to a human translation than those retrieved by NNGen and the log messages generated by NMT. This suggests that CC2Vec produces vector representations of patches that correlate to the meaning of the patch more strongly than a bag-of-words.\n\n\nTask 2: Bug Fixing Patch Identification\n\n\nProblem Formulation.\n\nSoftware requires continuous evolution to keep up with new requirements, but this also introduces new bugs. Backporting bugfixes to older versions of a project may be required when a legacy code base is supported. For example, Linux kernel developers regularly backport bugfixes from the latest version to older versions that are still under support. However, the maintainers of older versions may overlook relevant patches in the latest version. Thus, an automated method to identify bug fixing patches may be helpful. We treat the problem as a binary classification problem, in which each patch is labelled as a bug-fixing patch or not. Given the code change and log message, we produce one of the two labels as the output.\n\n\n3.2.2\n\nState-of-the-art Approach. The state-of-the-art approach is PatchNet [22], which represents the removed (added) code as a three dimensional matrix. The dimensions of the matrix are the number of hunks, the number of lines in each hunk, and the number of words in each line. PatchNet employs a 3D-CNN [25] that automatically extracts features from this matrix. Unfortunately, the 3D-CNN lacks a mechanism to identify important words, lines, and hunks. To address this limitation, we propose a specialized hierarchical attention neural network to quantify the importance of words, lines, and hunks in our model (CC2Vec).\n\nAnother approach was proposed by Tian et al. [57] that combines Learning from Positive and Unlabelled examples (LPU) [37] and Support Vector Machine (SVM) [27] to build a patch classification model. Unlike CC2Vec, this approach requires the use of manually selected features. These features include word features, which is a \"bag-of-words\" extracted from log messages, and 52 features, manually extracted from the code change (e.g., the number of loops added in a patch and if certain words appear in the log message).\n\n\nOur Approach.\n\nCC2Vec is first used to learn a distributed representation of code changes on the whole dataset. All patches from the training and test dataset are used since the log messages of the test dataset are not the target of the task. Next, we integrate these vector representations of the code changes with the two existing approaches. To use CC2Vec in PatchNet, we concatenate the vector representation of the code change extracted by CC2Vec with the two embedding vectors extracted from the log message and code change by PatchNet to form a new embedding vector. The new embedding vector is fed into PatchNet's classification module to predict whether a given patch is a bug fixing patch. For the approach proposed by Tian et al. [57] which uses an SVM as the classifier, we pass the vectors produced by CC2Vec from the code change into the SVM as features.\n\n\nExperimental\n\nSetting. The goal of this task is to investigate if CC2Vec helps existing approaches to effectively classify bug-fixing patches. We use the dataset of Linux kernel bug-fixing patches used in the PatchNet paper. This dataset consists of 42K bug-fixing patches and 40K non-bug-fixing patches collected from the Linux kernel versions v3.0 to v4.12, released in July 2011 and July 2017 respectively. Patches in this dataset are limited to 100 lines of changed code, in line with the Linux kernel stable patch guidelines. The nonbug-fixing patches are selected such that they have a similar size, in terms of the number of files and the number of modified lines, as the bug-fixing patches. Following the PatchNet paper, we use 5-fold cross-validation for the evaluation.\n\nTo compare the performance of the approaches, we employ the following metrics:\n\n\u2022 Accuracy: The ratio of correct predictions to the total number of predictions. \u2022 Precision: The ratio of correct predictions of bug-fixing patches to the total number of bug-fixing patch predictions \u2022 Recall: The ratio of correct predictions of bug-fixing patches to the total number of bug-fixing patches. \u2022 F1: Harmonic mean between precision and recall.\n\n\u2022 AUC: Area under the curve plotting the true positive rate against the false positive rate. AUC values range from 0 to 1, with a value of 1 indicating perfect discrimination.\n\nThese metrics were also used in previous studies on this task.\n\n\nResults.\n\nWe report the performance of the different approaches in Table 2. We observe that the best performing approach is Patch-Net augmented with CC2Vec. For both Tian et al.'s model (LPU-SVM) and PatchNet, the versions augmented with CC2Vec outperform the original versions. Specifically, CC2Vec helps to improve the best performing baseline (i.e, PatchNet) by 5.22%, 9.18%, 4.37%, and 6.51% in terms of accuracy, precision, F1, and AUC. CC2Vec also helps to improve the performance of LPU-SVM by 5.47%, 2.80%, 11.45%, 7.09%, and 4.24% in accuracy, precision, recall, F1, and AUC. This suggests that CC2Vec can learn patch representations that are general and useful beyond the task it was trained on. 3.3 Task 3: Just-in-Time Defect Prediction 3.3.1 Problem Formulation. The task of just-in-time (JIT) defect prediction refers to the identification of defective patches. JIT defect prediction tools provide early feedback to software developers to optimize their effort for inspection, and have been used at large software companies [44,51,55]. We model the task as a binary classification task, in which each patch is labelled as a patch containing a defect or not. Given a patch containing a code change and a log message with unknown label, we label the patch with one of the two labels.\n\n\nState-of-the-art Approach. The state-of-the-art approach is\n\nDeepJIT, proposed by Hoang et al. [21]. DeepJIT takes as input the log message and code change of a given patch and outputs a probability score to predict whether the patch is buggy. DeepJIT employs a Convolutional Neural Network (CNN) [31] to automatically extract features from the code change and log message of the given patch. However, DeepJIT ignores information about the structure of the removed code or added code, instead relying on CNN to automatically extract such information.\n\n\nOur\n\nApproach. Similar to the previous task (i.e., bug fixing patch identification), CC2Vec is first used to learn distributed representations of the code changes in the whole dataset. All patches from the training and test dataset are used since the log messages of the test dataset are not part of the predictions of the task. We then integrate CC2Vec with DeepJIT. To use CC2Vec with DeepJIT, for each patch, we concatenate the vector representation of the code change extracted by CC2Vec with two embedding vectors extracted from the log message and code change of the given patch extracted by DeepJIT to form a new embedding vector. The new embedding vector is fed into DeepJIT's feature combination layers, to predict whether the given patch is defective.\n\n\nExperimental\n\nSetting. The purpose of this task is to evaluate if CC2Vec can be used to augment existing approaches in effectively classifying defective patches. Our evaluation is performed on two datasets, the QT and OPENSTACK datasets, which contain patches collected from the QT and OPENSTACK software projects respectively by McIntosh and Kamei [42]. The QT dataset contains 25K patches over 2 years and 9 months while the OPENSTACK dataset contains 12K patches over 2 years and 3 months. 8% and 13% of the patches are defective in the QT dataset and the OPENSTACK datasets respectively. Like Hoang et al. [21], we use 5-fold cross validation for the evaluation.\n\nTo compute the effectiveness of the approaches, we use the Area Under the receiver operator characteristics Curve (AUC), similar to the previous studies.\n\n\nResults.\n\nThe evaluation results for this task are reported in Table 3. The use of CC2Vec with DeepJIT improves the AUCS score of DeepJIT, from 76.8 and 75.1 to 82.2 and 80.9 on the QT and OPENSTACK datasets respectively. Specifically, CC2Vec helps to improve the AUC metric by 7.03% and 7.72% for the QT and OPENSTACK datasets, respectively, as compared to DeepJIT. This indicates that CC2Vec is effective in learning a useful representation of patches that an existing state-of-the-art technique can utilize.\n\n\nDISCUSSION 4.1 Ablation Study\n\nOur approach involves five comparison functions for calculating the difference between the removed code and added code. To estimate the usefulness of comparison functions (see Section 2.4.2), we conduct an ablation study on the three tasks: log message generation, bug fixing patch identification, and just-in-time defect prediction. Specifically, we first remove the comparison functions entirely and then remove these functions one-by-one. For each task, we compare the CC2Vec model and its six reduced variants: All\u2212all (omit all comparison functions), All\u2212NT (omit the neural network tensor comparison function), All\u2212NN (omit the neural network comparison function), All\u2212sim (omit the similarity comparison function), All\u2212sub (omit the subtraction comparison function), and All\u2212mul (omit the multiplication comparison function). Table 4 summarizes the results of our ablation test on three different tasks. We see that CC2Vec model always performs better than the reduced variants for all three tasks. This suggests that each comparison function plays an important role and omitting these comparison functions may greatly affect the overall performance. All\u2212all (CC2Vec model without using any comparison functions) performs the worst. Among the five remaining variants (i.e., All\u2212NT, All\u2212NN, All\u2212sim, All\u2212sub, and All\u2212mul), All-NT performs the worst. This suggests that the neural network tensor comparison function is more important the other comparison functions (i.e., neural network, similarity, subtraction, and multiplication).\n\n\nThreats to Validity\n\nThreats to internal validity refer to errors in our experiments and experimenter bias. For each task, we reuse existing implementations of the baseline approaches whenever available. We have double checked our code and data, but errors may remain.\n\nThreats to external validity concern the generalizability of our work. In our experiments, we have studied only three tasks to evaluate the generality of CC2Vec. This may be a threat to external validity since CC2Vec may not generalize beyond the tasks that we have considered. However, each task involves different software projects and different programming languages. As such, we believe that there is minimal threat to external validity. To minimize threats to construct validity, we have used the same evaluation metrics that were used in previous studies.\n\n\nRELATED WORK\n\nThere are many studies on the representation of source code, including recent studies proposing distributed representations for identifiers [17], APIs [46,47], and software libraries [56]. A comprehensive survey of learning the representation of source code has been done by Allamanis et al. [1].\n\nSome studies transform the source code into a different form, such as control-flow graphs [15] and symbolic traces [20], or collect  [58], before learning distributed representations. DeFreez et al. [15] found function synonyms by learning embeddings through random walks of the interprocedural controlflow graph of a program. These embeddings are then used in a single downstream task of mining error-handling specifications.\n\nHenkel et al. [20] described a toolchain to produce abstracted intraprocedural symbolic traces for learning word embeddings. They experimented on a downstream task to find and repair bugs related to incorrect error codes. Wang et al. [58] used execution traces to learn embeddings. They integrate their embeddings into a program repair system in order to produce fixes to correct student errors in programming assignments. These studies differ from our work as we leverage natural language data as well as source code.\n\nThere have been other studies using deep learning of both source code and natural language data, for example, joint learning of embeddings for both text and source code to improve code search [18]. Other studies proposed approaches to learn distributed representations of source code on prediction tasks with natural language output. Iyer et al. [24] proposed a model using LSTM networks with attention for code summarization, and Yin et al. [62] trained a model to align source code to natural language text from Stack-Overflow posts. However, unlike our work, these studies do not use structural information of the source code.\n\nSeveral studies [2,3,23,35] account for structural information but differ from our work. Hu et al. [23] proposed an approach to use Sequence-to-Sequence Neural Machine Translation to generate method-level code comments. By prefixing the AST node type in each token and traversing the AST of methods such that the original AST can be unambiguously reconstructed, they convert the AST of each method into a sequence that preserves structural information. Alon et al. proposed code2vec [3], which represents code as paths in an AST, learning the vector representation of each AST path. They trained their model on the task of predicting a label, such as the method name, of the code snippet. In a later work, they proposed code2seq [2]. Instead of predicting a single label, they generate a sequence of natural language words. Similar to our work, structural information of the input source code is encoded in the model's architecture, however, in these studies, the input code snippet is required to be parseable to build an AST.\n\nAs our work focuses on the representation of software patches, we deliberately designed CC2Vec to not require parseable code in its input. This is done for two reasons. Firstly, a small but still significant proportion of patches may have compilation errors. A study by Beller et al. on Travis CI build failures revealed that about 4% of Java project build failures are due to compilation errors [8]. CC2Vec is designed to be usable even for these patches. Secondly, parsing will require the entire file with the changed code. Retrieving this information and parsing the entire file will be time consuming.\n\nAll the studies above proposed general representations of source code. The representations they learn, with the exception of DeFreez et al. [15], are of source code contained in a single function. In contrast, we learn representations of code changes, which can contain modifications to multiple different functions, across multiple files.\n\nSeveral of the models related to code changes' representation were discussed in Section 3. These models often do not model the hierarchical structure of a code change or require handcrafted features that may be specific to a single task [5, 26, 28-30, 39, 44, 57, 60].\n\nTwo techniques using deep neural networks, PatchNet [22] and DeepJIT [21], are most similar to our work. However, as discussed earlier, our work differs from theirs in various ways. A fundamental difference is in the generality of the techniques. CC2Vec is not specific to a single task. Rather, CC2Vec can be trained for multiple tasks, including both generative and classification tasks. In fact, CC2Vec is orthogonal to these approaches. The objective of CC2Vec is to produce high quality representations of code changes that can be integrated into PatchNet, DeepJIT, and similar models. We showed in Section 3 that the performance of these models improves when they are augmented with the code change representation learned by CC2Vec.\n\n\nCONCLUSION\n\nWe propose CC2Vec, which produces distributed representations of code changes through a hierarchical attention network. In CC2Vec, we model the structural information of a code change and use the attention mechanism to identify important aspects of the code change with respect to the log message accompanying it. This allows CC2Vec to learn high-quality vector representations that can be used in existing state-of-the-art models on tasks involving code changes.\n\nWe empirically evaluated CC2Vec on three tasks and demonstrated that approaches using or augmented with CC2Vec embeddings outperform existing state-of-the-art approaches that do not use the embeddings. Finally, we performed an ablation study to evaluate the usefulness of comparison functions. The results show that the comparison functions play an important role and omitting them in part or in full affects the overall performance.\n\nAs future work, to reduce the threat to external validity, we will integrate of CC2Vec into other tools and experiments on other tasks involving software patches. Package. The replication package is available at https://github. com/CC2Vec/CC2Vec.\n\nFigure 2 :\n2Architecture of the feature extraction layers for mapping the code change of the affected file in a given patch to an embedding vector. The input of the module is the removed code and added code of the affected file, denoted by \"-\" and \"+\", respectively.\n\nFigure 3 :\n3The overall framework of our hierarchical attention network (HAN). The HAN takes as input the removed (added) code of the affected file of a given patch and outputs the embedding vector (denoted by e) of the removed (added) code.\n\n\nThese vectors are then concatenated to represent an embedding vector\n\nFigure 4 :\n4A list of comparison functions in the comparison layers.\n\nFigure 5 :\n5The details of the red dashed box inFigure 1. It takes as input a list of embedding vectors of the affected files of a given patch (i.e., e f 1 , e f 2 , . . . , e f F ). e p is the vector representation of the code change and is fed to a hidden layer to produce the word vector (i.e., the probability distribution over words). V M is a set of words extracted from the first line of the log messages.between the removed code and added code as follows:e sim = EUC(e r , e a ) \u2295 COS(e r , e a ) EUC(e r , e a ) = ||e r \u2212 e a || 2 COS(e r , e a ) = e r e a ||e r ||||e a ||\n\nFigure 5\n5shows the details of the part of the architecture shown inside the red (dashed) box inFigure 1. The inputs of this part are the list of embedding vectors (i.e., e f 1 , e f 2 , . . . , e f F ) representing the features extracted from the list of affected files of a given patch. These embedding vectors are concatenated to construct a new embedding vector (e p ) representing the code change in a given patch as follows:\n\n\n, Seoul, Republic of Korea \u00a9 2020 Association for Computing Machinery. ACM ISBN 978-1-4503-7121-6/20/05. . . $15.00 https://doi.org/10.1145/3377811.3380361\n\n\nFile !Hierarchical attention network \n\nCode change vector \n\nFully connected layer for \nmapping code change vector \n\nA set of words extracted from \nlog message \n\nFeature \nfusion \nlayers \n\nWord \nprediction \nlayer \n\nFile \" \n\nInput layer \n\nFeature \nextraction \nlayers \n\nPreprocessing \n\ne $ \ne $ \n\nEncoding \n\nCode changes \n\n\n\nTable 1 :\n1Performance of each approach on the original and cleaned dataset reported in BLEU-4LogGen NNGen NMT \nOriginal \n43.20 \n38.55 \n31.92 \nClean \n20.48 \n16.42 \n14.19 \n\nfrom theirs as ours incorporates the structure of code changes. Liu \net al. [39] investigated the performance of Jiang et al.'s attention \nmodel; they found that once they remove trivial and automatically-\ngenerated messages, the performance of the model decreased sig-\nnificantly, suggesting that this model does not generalize. \n\n3.1.3 Our Approach. To use CC2Vec for this task, we propose \nLogGen. Similar to the nearest neighbours approach used by Liu \net al. [39], LogGen reuses and outputs a log message from the \ntraining set. However, instead of treating each code change as a bag \nof words, LogGen uses code change vectors produced by CC2Vec. \nCC2Vec is first trained over the training dataset. Given a new code \nchange from the test dataset with an unknown log message, we find \nthe code changes with a known log message that have the closest \nCC2Vec vector. Like Liu et al. \n\nTable 2 :\n2Evaluation of the approaches on the bug-fixing patch identification taskAcc. Prec. Recall \nF1 \nAUC \nLPU-SVM \n73.1 75.1 \n71.6 \n73.3 73.1 \nLPU-SVM + CC2Vec 77.1 77.2 \n79.8 78.5 76.2 \nPatchNet \n86.2 83.9 \n90.1 \n87.1 86.0 \nPatchNet + CC2Vec 90.7 91.6 \n90.1 90.9 91.6 \n\n\n\nTable 3 :\n3The AUC results of the various approachesQT OPENSTACK \nDeepJIT \n76.8 \n75.1 \nDeepJIT + CC2Vec 82.2 \n80.9 \n\n\n\nTable 4 :\n4Results of an ablation study Log generation (BLEU-4) Bug fix identification (F1)Just-in-time defect prediction (AUC)Clean \nDrops by (%) \nBFP \nDrops by (%) \nQT Drops by (%) OPENSTACK Drops by (%) \nAll\u2212all \n18.30 \n10.64 \n87.1 \n4.18 \n77.4 \n5.84 \n76.7 \n5.19 \nAll\u2212NT 19.36 \n5.47 \n88.7 \n2.42 \n79.8 \n2.92 \n79.2 \n2.10 \nAll\u2212NN 19.80 \n3.32 \n88.8 \n2.31 \n80.1 \n2.55 \n79.5 \n1.73 \nAll\u2212sim 20.41 \n0.34 \n90.2 \n0.77 \n81.9 \n0.36 \n80.5 \n0.49 \nAll\u2212sub 20.13 \n1.71 \n89.6 \n1.43 \n80.7 \n1.82 \n80.1 \n0.99 \nAll\u2212mul 20.25 \n1.12 \n89.7 \n1.32 \n81.1 \n1.34 \n80.5 \n0.49 \nAll \n20.48 \n0 \n90.9 \n0 \n82.2 \n0 \n80.9 \n0 \n\nruntime execution traces \nhttps://chris.beams.io/posts/git-commit/ arXiv:2003.05620v1 [cs.SE] 12 Mar 2020\nAcknowledgement. This research was supported by the Singapore National Research Foundation (Award number: NRF2016-NRF-ANR003) and the ANR ITrans project.\nA survey of machine learning for big code and naturalness. Miltiadis Allamanis, T Earl, Premkumar Barr, Charles Devanbu, Sutton, ACM Computing Surveys (CSUR). 5181Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. 2018. A survey of machine learning for big code and naturalness. ACM Computing Surveys (CSUR) 51, 4 (2018), 81.\n\ncode2seq: Generating Sequences from Structured Representations of Code. Uri Alon, Shaked Brody, Omer Levy, Eran Yahav, International Conference on Learning Representations. Uri Alon, Shaked Brody, Omer Levy, and Eran Yahav. 2019. code2seq: Generating Sequences from Structured Representations of Code. In International Conference on Learning Representations.\n\nLearning distributed representations of code. Uri Alon, Meital Zilberstein, Omer Levy, Eran Yahav, Proceedings of the ACM on Programming Languages. 240POPLUri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav. 2019. code2vec: Learn- ing distributed representations of code. Proceedings of the ACM on Programming Languages 3, POPL (2019), 40.\n\nUnivariate hyperbolic tangent neural network approximation. A George, Anastassiou, Mathematical and Computer Modelling. 53George A Anastassiou. 2011. Univariate hyperbolic tangent neural network approximation. Mathematical and Computer Modelling 53, 5-6 (2011), 1111-1132.\n\nLearning from bug-introducing changes to prevent fault prone code. Lerina Aversano, Luigi Cerulo, Concettina Del Grosso, Ninth international workshop on Principles of software evolution: in conjunction with the 6th ESEC/FSE joint meeting. ACMLerina Aversano, Luigi Cerulo, and Concettina Del Grosso. 2007. Learning from bug-introducing changes to prevent fault prone code. In Ninth international workshop on Principles of software evolution: in conjunction with the 6th ESEC/FSE joint meeting. ACM, 19-26.\n\nNeural machine translation by jointly learning to align and translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, arXiv:1409.0473arXiv preprintDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2014. Neural ma- chine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473 (2014).\n\nDeep attention neural tensor network for visual question answering. Yalong Bai, Jianlong Fu, Tiejun Zhao, Tao Mei, Proceedings of the European Conference on Computer Vision (ECCV. the European Conference on Computer Vision (ECCVYalong Bai, Jianlong Fu, Tiejun Zhao, and Tao Mei. 2018. Deep attention neural tensor network for visual question answering. In Proceedings of the European Conference on Computer Vision (ECCV). 20-35.\n\nOops, my tests broke the build: An explorative analysis of Travis CI with GitHub. Moritz Beller, Georgios Gousios, Andy Zaidman, 2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR). IEEEMoritz Beller, Georgios Gousios, and Andy Zaidman. 2017. Oops, my tests broke the build: An explorative analysis of Travis CI with GitHub. In 2017 IEEE/ACM 14th International Conference on Mining Software Repositories (MSR). IEEE, 356- 367.\n\nNLTK: the natural language toolkit. Steven Bird, Edward Loper, Proceedings of the ACL 2004 on Interactive poster and demonstration sessions. the ACL 2004 on Interactive poster and demonstration sessionsAssociation for Computational Linguistics31Steven Bird and Edward Loper. 2004. NLTK: the natural language toolkit. In Proceedings of the ACL 2004 on Interactive poster and demonstration sessions. Association for Computational Linguistics, 31.\n\nEfficient bounds for the softmax function and applications to approximate inference in hybrid models. Guillaume Bouchard, NIPS 2007 workshop for approximate Bayesian inference in continuous/hybrid systems. Guillaume Bouchard. 2007. Efficient bounds for the softmax function and appli- cations to approximate inference in hybrid models. In NIPS 2007 workshop for approximate Bayesian inference in continuous/hybrid systems.\n\nOverfitting in neural nets: Backpropagation, conjugate gradient, and early stopping. Rich Caruana, Steve Lawrence, C Lee Giles, Advances in neural information processing systems. Rich Caruana, Steve Lawrence, and C Lee Giles. 2001. Overfitting in neural nets: Backpropagation, conjugate gradient, and early stopping. In Advances in neural information processing systems. 402-408.\n\nSemi-supervised learning (chapelle, o. Olivier Chapelle, Bernhard Scholkopf, Alexander Zien, . et al.book reviewsOlivier Chapelle, Bernhard Scholkopf, and Alexander Zien. 2009. Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews].\n\n. IEEE Transactions on Neural Networks. 20IEEE Transactions on Neural Networks 20, 3 (2009), 542-542.\n\nConvolutional neural networks applied to human face classification. Brian Cheung, 2012 11th International Conference on Machine Learning and Applications. IEEE2Brian Cheung. 2012. Convolutional neural networks applied to human face classification. In 2012 11th International Conference on Machine Learning and Applications, Vol. 2. IEEE, 580-583.\n\nImproving deep neural networks for LVCSR using rectified linear units and dropout. E George, Tara N Dahl, Geoffrey E Sainath, Hinton, 2013 IEEE international conference on acoustics, speech and signal processing. IEEEGeorge E Dahl, Tara N Sainath, and Geoffrey E Hinton. 2013. Improving deep neural networks for LVCSR using rectified linear units and dropout. In 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 8609-8613.\n\nPath-based function embedding and its application to error-handling specification mining. Daniel Defreez, Aditya V Thakur, Cindy Rubio-Gonz\u00e1lez, Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software EngineeringACMDaniel DeFreez, Aditya V Thakur, and Cindy Rubio-Gonz\u00e1lez. 2018. Path-based function embedding and its application to error-handling specification mining. In Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. ACM, 423-433.\n\nBoa: A language and infrastructure for analyzing ultra-large-scale software repositories. Robert Dyer, Anh Hoan, Hridesh Nguyen, Tien N Rajan, Nguyen, Proceedings of the 2013 International Conference on Software Engineering. the 2013 International Conference on Software EngineeringIEEE PressRobert Dyer, Hoan Anh Nguyen, Hridesh Rajan, and Tien N Nguyen. 2013. Boa: A language and infrastructure for analyzing ultra-large-scale software repositories. In Proceedings of the 2013 International Conference on Software Engineering. IEEE Press, 422-431.\n\nSemantic source code models using identifier embeddings. Vasiliki Efstathiou, Diomidis Spinellis, Proceedings of the 16th International Conference on Mining Software Repositories. the 16th International Conference on Mining Software RepositoriesIEEE PressVasiliki Efstathiou and Diomidis Spinellis. 2019. Semantic source code models using identifier embeddings. In Proceedings of the 16th International Conference on Mining Software Repositories. IEEE Press, 29-33.\n\nDeep code search. Xiaodong Gu, Hongyu Zhang, Sunghun Kim, 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE). IEEEXiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code search. In 2018 IEEE/ACM 40th International Conference on Software Engineering (ICSE). IEEE, 933-944.\n\nTraining feedforward networks with the Marquardt algorithm. T Martin, Mohammad B Hagan, Menhaj, IEEE transactions on Neural Networks. 5Martin T Hagan and Mohammad B Menhaj. 1994. Training feedforward networks with the Marquardt algorithm. IEEE transactions on Neural Networks 5, 6 (1994), 989-993.\n\nCode vectors: Understanding programs through embedded abstracted symbolic traces. Jordan Henkel, K Shuvendu, Ben Lahiri, Thomas Liblit, Reps, Jordan Henkel, Shuvendu K Lahiri, Ben Liblit, and Thomas Reps. 2018. Code vectors: Understanding programs through embedded abstracted symbolic traces.\n\nProceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering. the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software EngineeringACMIn Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineer- ing Conference and Symposium on the Foundations of Software Engineering. ACM, 163-174.\n\nDeepJIT: an end-to-end deep learning framework for just-in-time defect prediction. Thong Hoang, Hoa Khanh Dam, Yasutaka Kamei, David Lo, Naoyasu Ubayashi, Proceedings of the 16th International Conference on Mining Software Repositories. the 16th International Conference on Mining Software RepositoriesIEEE PressThong Hoang, Hoa Khanh Dam, Yasutaka Kamei, David Lo, and Naoyasu Ubayashi. 2019. DeepJIT: an end-to-end deep learning framework for just-in-time defect prediction. In Proceedings of the 16th International Conference on Mining Software Repositories. IEEE Press, 34-45.\n\nPatchNet: Hierarchical Deep Learning-Based Stable Patch Identification for the Linux Kernel. Thong Hoang, Julia Lawall, Yuan Tian, J Richard, David Oentaryo, Lo, IEEE Transactions on Software Engineering. Thong Hoang, Julia Lawall, Yuan Tian, Richard J Oentaryo, and David Lo. 2019. PatchNet: Hierarchical Deep Learning-Based Stable Patch Identification for the Linux Kernel. IEEE Transactions on Software Engineering (2019).\n\nDeep code comment generation. Xing Hu, Ge Li, Xin Xia, David Lo, Zhi Jin, Proceedings of the 26th Conference on Program Comprehension. the 26th Conference on Program ComprehensionACMXing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2018. Deep code comment generation. In Proceedings of the 26th Conference on Program Comprehension. ACM, 200-210.\n\nSummarizing source code using a neural attention model. Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Luke Zettlemoyer, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. the 54th Annual Meeting of the Association for Computational LinguisticsLong Papers1Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016. Summarizing source code using a neural attention model. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2073-2083.\n\n3D convolutional neural networks for human action recognition. Shuiwang Ji, Wei Xu, Ming Yang, Kai Yu, 35Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 2012. 3D convolutional neural networks for human action recognition. IEEE transactions on pattern analysis and machine intelligence 35, 1 (2012), 221-231.\n\nAutomatically generating commit messages from diffs using neural machine translation. Siyuan Jiang, Ameer Armaly, Collin Mcmillan, Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering. the 32nd IEEE/ACM International Conference on Automated Software EngineeringIEEE PressSiyuan Jiang, Ameer Armaly, and Collin McMillan. 2017. Automatically generat- ing commit messages from diffs using neural machine translation. In Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering. IEEE Press, 135-146.\n\nSvmlight: Support vector machine. Thorsten Joachims, 194University of DortmundSVM-Light Support Vector MachineThorsten Joachims. 1999. Svmlight: Support vector machine. SVM-Light Support Vector Machine http://svmlight. joachims. org/, University of Dortmund 19, 4 (1999).\n\nStudying just-in-time defect prediction using cross-project models. Yasutaka Kamei, Takafumi Fukushima, Shane Mcintosh, Kazuhiro Yamashita, Naoyasu Ubayashi, Ahmed E Hassan, Empirical Software Engineering. 21Yasutaka Kamei, Takafumi Fukushima, Shane McIntosh, Kazuhiro Yamashita, Naoyasu Ubayashi, and Ahmed E Hassan. 2016. Studying just-in-time defect prediction using cross-project models. Empirical Software Engineering 21, 5 (2016), 2072-2106.\n\nA large-scale empirical study of just-in-time quality assurance. Yasutaka Kamei, Emad Shihab, Bram Adams, Ahmed E Hassan, Audris Mockus, Anand Sinha, Naoyasu Ubayashi, IEEE Transactions on Software Engineering. 39Yasutaka Kamei, Emad Shihab, Bram Adams, Ahmed E Hassan, Audris Mockus, Anand Sinha, and Naoyasu Ubayashi. 2012. A large-scale empirical study of just-in-time quality assurance. IEEE Transactions on Software Engineering 39, 6 (2012), 757-773.\n\nClassifying software changes: Clean or buggy?. Sunghun Kim, James WhiteheadJr, Yi Zhang, IEEE Transactions on Software Engineering. 34Sunghun Kim, E James Whitehead Jr, and Yi Zhang. 2008. Classifying software changes: Clean or buggy? IEEE Transactions on Software Engineering 34, 2 (2008), 181-196.\n\nConvolutional Neural Networks for Sentence Classification. Yoon Kim, 10.3115/v1/D14-1181Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Doha, QatarAssociation for Computational LinguisticsYoon Kim. 2014. Convolutional Neural Networks for Sentence Classification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, Doha, Qatar, 1746-1751. https://doi.org/10.3115/v1/D14-1181\n\nYoon Kim, Carl Denton, Luong Hoang, Alexander M Rush, arXiv:1702.00887Structured attention networks. arXiv preprintYoon Kim, Carl Denton, Luong Hoang, and Alexander M Rush. 2017. Structured attention networks. arXiv preprint arXiv:1702.00887 (2017).\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti- mization. arXiv preprint arXiv:1412.6980 (2014).\n\nHey! are you committing tangled changes. Hiroyuki Kirinuki, Yoshiki Higo, Keisuke Hotta, Shinji Kusumoto, Proceedings of the 22nd International Conference on Program Comprehension. the 22nd International Conference on Program ComprehensionACMHiroyuki Kirinuki, Yoshiki Higo, Keisuke Hotta, and Shinji Kusumoto. 2014. Hey! are you committing tangled changes?. In Proceedings of the 22nd International Conference on Program Comprehension. ACM, 262-265.\n\nPathMiner: a library for mining of path-based representations of code. Vladimir Kovalenko, Egor Bogomolov, Timofey Bryksin, Alberto Bacchelli, Proceedings of the 16th International Conference on Mining Software Repositories. the 16th International Conference on Mining Software RepositoriesIEEE PressVladimir Kovalenko, Egor Bogomolov, Timofey Bryksin, and Alberto Bacchelli. 2019. PathMiner: a library for mining of path-based representations of code. In Proceedings of the 16th International Conference on Mining Software Repositories. IEEE Press, 13-17.\n\nA neural model for generating natural language summaries of program subroutines. Alexander Leclair, Siyuan Jiang, Collin Mcmillan, Proceedings of the 41st International Conference on Software Engineering. the 41st International Conference on Software EngineeringIEEE PressAlexander LeClair, Siyuan Jiang, and Collin McMillan. 2019. A neural model for generating natural language summaries of program subroutines. In Proceedings of the 41st International Conference on Software Engineering. IEEE Press, 795-806.\n\nLearning with positive and unlabeled examples using weighted logistic regression. Bing Wee Sun Lee, Liu, In ICML. 3Wee Sun Lee and Bing Liu. 2003. Learning with positive and unlabeled examples using weighted logistic regression. In ICML, Vol. 3. 448-455.\n\nChangescribe: A tool for automatically generating commit messages. Mario Linares-V\u00e1squez, Luis Fernando Cort\u00e9s-Coy, Jairo Aponte, Denys Poshyvanyk, 2015 IEEE/ACM 37th IEEE International Conference on Software Engineering. IEEE2Mario Linares-V\u00e1squez, Luis Fernando Cort\u00e9s-Coy, Jairo Aponte, and Denys Poshyvanyk. 2015. Changescribe: A tool for automatically generating commit messages. In 2015 IEEE/ACM 37th IEEE International Conference on Software Engineering, Vol. 2. IEEE, 709-712.\n\nNeural-machine-translation-based commit message generation: how far are we. Zhongxin Liu, Xin Xia, Ahmed E Hassan, David Lo, Zhenchang Xing, Xinyu Wang, Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering. the 33rd ACM/IEEE International Conference on Automated Software EngineeringACMZhongxin Liu, Xin Xia, Ahmed E Hassan, David Lo, Zhenchang Xing, and Xinyu Wang. 2018. Neural-machine-translation-based commit message generation: how far are we?. In Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering. ACM, 373-384.\n\nAction recognition from a distributed representation of pose and appearance. Subhransu Maji, Lubomir Bourdev, Jitendra Malik, CVPR 2011. IEEE. Subhransu Maji, Lubomir Bourdev, and Jitendra Malik. 2011. Action recognition from a distributed representation of pose and appearance. In CVPR 2011. IEEE, 3177-3184.\n\nIntroduction to information retrieval. Christopher Manning, Prabhakar Raghavan, Hinrich Sch\u00fctze, Natural Language Engineering. 16Christopher Manning, Prabhakar Raghavan, and Hinrich Sch\u00fctze. 2010. Intro- duction to information retrieval. Natural Language Engineering 16, 1 (2010), 100-103.\n\nAre fix-inducing changes a moving target? a longitudinal case study of just-in-time defect prediction. Shane Mcintosh, Yasutaka Kamei, IEEE Transactions on Software Engineering. 44Shane McIntosh and Yasutaka Kamei. 2017. Are fix-inducing changes a mov- ing target? a longitudinal case study of just-in-time defect prediction. IEEE Transactions on Software Engineering 44, 5 (2017), 412-428.\n\nDistributed representations of words and phrases and their compositionality. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, Jeff Dean, Advances in neural information processing systems. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems. 3111-3119.\n\nPredicting risk of software changes. Audris Mockus, M David, Weiss, Bell Labs Technical Journal. 5Audris Mockus and David M Weiss. 2000. Predicting risk of software changes. Bell Labs Technical Journal 5, 2 (2000), 169-180.\n\nRectified linear units improve restricted boltzmann machines. Vinod Nair, Geoffrey E Hinton, Proceedings of the 27th international conference on machine learning. the 27th international conference on machine learningVinod Nair and Geoffrey E Hinton. 2010. Rectified linear units improve re- stricted boltzmann machines. In Proceedings of the 27th international conference on machine learning (ICML-10). 807-814.\n\nMapping API elements for code migration with vector representations. Anh Tuan Trong Duc Nguyen, Tien N Nguyen, Nguyen, 2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C). IEEETrong Duc Nguyen, Anh Tuan Nguyen, and Tien N Nguyen. 2016. Mapping API elements for code migration with vector representations. In 2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C). IEEE, 756-758.\n\nExploring API embedding for API usages and applications. Anh Tuan Trong Duc Nguyen, Hung Nguyen, Tien N Dang Phan, Nguyen, Trong Duc Nguyen, Anh Tuan Nguyen, Hung Dang Phan, and Tien N Nguyen. 2017. Exploring API embedding for API usages and applications. In 2017\n\nIEEE/ACM 39th International Conference on Software Engineering (ICSE). IEEEIEEE/ACM 39th International Conference on Software Engineering (ICSE). IEEE, 438-449.\n\nBLEU: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th annual meeting on association for computational linguistics. the 40th annual meeting on association for computational linguisticsAssociation for Computational LinguisticsKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational Linguistics, 311-318.\n\nTextRank based search term identification for software change tasks. Mohammad Masudur Rahman, Roy, IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER). IEEEMohammad Masudur Rahman and Chanchal K Roy. 2015. TextRank based search term identification for software change tasks. In 2015 IEEE 22nd International Conference on Software Analysis, Evolution, and Reengineering (SANER). IEEE, 540-544.\n\nCorrect: code reviewer recommendation in github based on cross-project and technology experience. Mohammad Masudur Rahman, Jason A Roy, Collins, 2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C). IEEEMohammad Masudur Rahman, Chanchal K Roy, and Jason A Collins. 2016. Cor- rect: code reviewer recommendation in github based on cross-project and tech- nology experience. In 2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C). IEEE, 222-231.\n\nAn industrial study on the risk of software changes. Emad Shihab, Ahmed E Hassan, Bram Adams, Zhen Ming Jiang, Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering. the ACM SIGSOFT 20th International Symposium on the Foundations of Software EngineeringACM62Emad Shihab, Ahmed E Hassan, Bram Adams, and Zhen Ming Jiang. 2012. An industrial study on the risk of software changes. In Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering. ACM, 62.\n\nRecursive deep models for semantic compositionality over a sentiment treebank. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, D Christopher, Andrew Manning, Christopher Ng, Potts, Proceedings of the 2013 conference on empirical methods in natural language processing. the 2013 conference on empirical methods in natural language processingRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural language processing. 1631-1642.\n\nDropout: a simple way to prevent neural networks from overfitting. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, Ruslan Salakhutdinov, The Journal of Machine Learning Research. 15Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overfitting. The Journal of Machine Learning Research 15, 1 (2014), 1929-1958.\n\nIntroduction to multilayer feed-forward neural networks. Chemometrics and intelligent laboratory systems. Daniel Svozil, Vladimir Kvasnicka, Jiri Pospichal, 39Daniel Svozil, Vladimir Kvasnicka, and Jiri Pospichal. 1997. Introduction to multi- layer feed-forward neural networks. Chemometrics and intelligent laboratory systems 39, 1 (1997), 43-62.\n\nThe impact of mislabelling on the performance and interpretation of defect prediction models. Chakkrit Tantithamthavorn, Shane Mcintosh, Ahmed E Hassan, Akinori Ihara, Kenichi Matsumoto, 2015 IEEE/ACM 37th IEEE International Conference on Software Engineering. IEEE1Chakkrit Tantithamthavorn, Shane McIntosh, Ahmed E Hassan, Akinori Ihara, and Kenichi Matsumoto. 2015. The impact of mislabelling on the performance and interpretation of defect prediction models. In 2015 IEEE/ACM 37th IEEE International Conference on Software Engineering, Vol. 1. IEEE, 812-823.\n\nImport2vec learning embeddings for software libraries. Bart Theeten, Frederik Vandeputte, Tom Van Cutsem, Proceedings of the 16th International Conference on Mining Software Repositories. the 16th International Conference on Mining Software RepositoriesIEEE PressBart Theeten, Frederik Vandeputte, and Tom Van Cutsem. 2019. Import2vec learning embeddings for software libraries. In Proceedings of the 16th International Conference on Mining Software Repositories. IEEE Press, 18-28.\n\nIdentifying Linux bug fixing patches. Yuan Tian, Julia Lawall, David Lo, Proceedings of the 34th International Conference on Software Engineering. the 34th International Conference on Software EngineeringIEEE PressYuan Tian, Julia Lawall, and David Lo. 2012. Identifying Linux bug fixing patches. In Proceedings of the 34th International Conference on Software Engineering. IEEE Press, 386-396.\n\nDynamic neural program embedding for program repair. Ke Wang, Rishabh Singh, Zhendong Su, arXiv:1711.07163arXiv preprintKe Wang, Rishabh Singh, and Zhendong Su. 2017. Dynamic neural program embedding for program repair. arXiv preprint arXiv:1711.07163 (2017).\n\nA Compare-Aggregate Model for Matching Text Sequences. Shuohang Wang, Jing Jiang, 5th International Conference on Learning Representations. Toulon, FranceConference Track ProceedingsShuohang Wang and Jing Jiang. 2017. A Compare-Aggregate Model for Matching Text Sequences. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. https: //openreview.net/forum?id=HJTzHtqee\n\nDeep learning for just-in-time defect prediction. Xinli Yang, David Lo, Xin Xia, Yun Zhang, Jianling Sun, 2015 IEEE International Conference on Software Quality, Reliability and Security. IEEEXinli Yang, David Lo, Xin Xia, Yun Zhang, and Jianling Sun. 2015. Deep learn- ing for just-in-time defect prediction. In 2015 IEEE International Conference on Software Quality, Reliability and Security. IEEE, 17-26.\n\nHierarchical attention networks for document classification. Zichao Yang, Diyi Yang, Chris Dyer, Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies. the 2016 conference of the North American chapter of the association for computational linguistics: human language technologiesXiaodong He, Alex Smola, and Eduard HovyZichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, and Eduard Hovy. 2016. Hierarchical attention networks for document classification. In Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: human language technologies. 1480-1489.\n\nLearning to mine aligned code and natural language pairs from stack overflow. Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, Graham Neubig, 2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR). IEEEPengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and Graham Neubig. 2018. Learning to mine aligned code and natural language pairs from stack overflow. In 2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR). IEEE, 476-486.\n", "annotations": {"author": "[{\"end\":110,\"start\":55},{\"end\":168,\"start\":111},{\"end\":240,\"start\":169},{\"end\":315,\"start\":241}]", "publisher": null, "author_last_name": "[{\"end\":66,\"start\":61},{\"end\":124,\"start\":120},{\"end\":177,\"start\":175},{\"end\":253,\"start\":247}]", "author_first_name": "[{\"end\":60,\"start\":55},{\"end\":115,\"start\":111},{\"end\":119,\"start\":116},{\"end\":174,\"start\":169},{\"end\":246,\"start\":241}]", "author_affiliation": "[{\"end\":109,\"start\":68},{\"end\":167,\"start\":126},{\"end\":239,\"start\":198},{\"end\":314,\"start\":277}]", "title": "[{\"end\":52,\"start\":1},{\"end\":367,\"start\":316}]", "venue": null, "abstract": "[{\"end\":1376,\"start\":392}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2016,\"start\":2012},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2019,\"start\":2016},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2062,\"start\":2058},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":2065,\"start\":2062},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2097,\"start\":2093},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":2149,\"start\":2145},{\"end\":2427,\"start\":2408},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3427,\"start\":3423},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3431,\"start\":3427},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3435,\"start\":3431},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3439,\"start\":3435},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3443,\"start\":3439},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3447,\"start\":3443},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3451,\"start\":3447},{\"end\":3455,\"start\":3451},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3645,\"start\":3641},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3648,\"start\":3645},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":3651,\"start\":3648},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":3654,\"start\":3651},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":4677,\"start\":4673},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":4680,\"start\":4677},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4758,\"start\":4757},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5075,\"start\":5072},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5078,\"start\":5075},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6295,\"start\":6291},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6298,\"start\":6295},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6301,\"start\":6298},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6650,\"start\":6646},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":7388,\"start\":7384},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7413,\"start\":7409},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7440,\"start\":7436},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7731,\"start\":7727},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7771,\"start\":7767},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7814,\"start\":7810},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8430,\"start\":8426},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":12764,\"start\":12761},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14239,\"start\":14235},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14242,\"start\":14239},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":16879,\"start\":16876},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":17858,\"start\":17854},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":17861,\"start\":17858},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":18222,\"start\":18218},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":18302,\"start\":18299},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18305,\"start\":18302},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":18334,\"start\":18330},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":18782,\"start\":18778},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":21620,\"start\":21616},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":22319,\"start\":22316},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":22359,\"start\":22355},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":22701,\"start\":22697},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":24377,\"start\":24373},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":25645,\"start\":25641},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":25683,\"start\":25679},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":25736,\"start\":25732},{\"end\":25883,\"start\":25872},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":25918,\"start\":25914},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":26326,\"start\":26322},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":26364,\"start\":26360},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":26404,\"start\":26400},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":27174,\"start\":27170},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":27628,\"start\":27624},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":27963,\"start\":27959},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":28242,\"start\":28238},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":28808,\"start\":28804},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":29177,\"start\":29173},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":29518,\"start\":29514},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":29806,\"start\":29802},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":29825,\"start\":29821},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":29965,\"start\":29961},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":30544,\"start\":30540},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":31084,\"start\":31080},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":31127,\"start\":31123},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":33397,\"start\":33393},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":33628,\"start\":33624},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":33993,\"start\":33989},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":34065,\"start\":34061},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":34103,\"start\":34099},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":35210,\"start\":35206},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":37841,\"start\":37837},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":37844,\"start\":37841},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":37847,\"start\":37844},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":38196,\"start\":38192},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":38398,\"start\":38394},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":39767,\"start\":39763},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":40028,\"start\":40024},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":43315,\"start\":43311},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":43326,\"start\":43322},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":43329,\"start\":43326},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":43358,\"start\":43354},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":43466,\"start\":43463},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":43563,\"start\":43559},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":43588,\"start\":43584},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":43606,\"start\":43602},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":43672,\"start\":43668},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":43915,\"start\":43911},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":44135,\"start\":44131},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":44613,\"start\":44609},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":44767,\"start\":44763},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":44863,\"start\":44859},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":45067,\"start\":45064},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":45069,\"start\":45067},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":45072,\"start\":45069},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":45075,\"start\":45072},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":45151,\"start\":45147},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":45534,\"start\":45531},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":45780,\"start\":45777},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":46476,\"start\":46473},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":46829,\"start\":46825},{\"end\":47293,\"start\":47263},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":47352,\"start\":47348},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":47369,\"start\":47365}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":49463,\"start\":49196},{\"attributes\":{\"id\":\"fig_1\"},\"end\":49706,\"start\":49464},{\"attributes\":{\"id\":\"fig_2\"},\"end\":49777,\"start\":49707},{\"attributes\":{\"id\":\"fig_3\"},\"end\":49847,\"start\":49778},{\"attributes\":{\"id\":\"fig_4\"},\"end\":50431,\"start\":49848},{\"attributes\":{\"id\":\"fig_5\"},\"end\":50863,\"start\":50432},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":51021,\"start\":50864},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":51343,\"start\":51022},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":52402,\"start\":51344},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":52680,\"start\":52403},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":52799,\"start\":52681},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":53418,\"start\":52800}]", "paragraph": "[{\"end\":2165,\"start\":1392},{\"end\":3875,\"start\":2167},{\"end\":6429,\"start\":3877},{\"end\":6928,\"start\":6431},{\"end\":7574,\"start\":6930},{\"end\":8464,\"start\":7576},{\"end\":8517,\"start\":8466},{\"end\":9064,\"start\":8519},{\"end\":9459,\"start\":9066},{\"end\":10471,\"start\":9472},{\"end\":11092,\"start\":10494},{\"end\":11350,\"start\":11094},{\"end\":12196,\"start\":11352},{\"end\":12431,\"start\":12214},{\"end\":13155,\"start\":12433},{\"end\":13335,\"start\":13157},{\"end\":13966,\"start\":13351},{\"end\":14317,\"start\":13968},{\"end\":15741,\"start\":14347},{\"end\":16649,\"start\":15850},{\"end\":17082,\"start\":16673},{\"end\":17314,\"start\":17183},{\"end\":18121,\"start\":17423},{\"end\":18783,\"start\":18158},{\"end\":18974,\"start\":18836},{\"end\":19114,\"start\":19002},{\"end\":19422,\"start\":19206},{\"end\":19528,\"start\":19460},{\"end\":19649,\"start\":19530},{\"end\":20275,\"start\":19759},{\"end\":20487,\"start\":20361},{\"end\":20749,\"start\":20533},{\"end\":21390,\"start\":20843},{\"end\":22231,\"start\":21405},{\"end\":22371,\"start\":22233},{\"end\":22626,\"start\":22420},{\"end\":22737,\"start\":22628},{\"end\":22889,\"start\":22777},{\"end\":23017,\"start\":22891},{\"end\":23173,\"start\":23019},{\"end\":23329,\"start\":23196},{\"end\":23610,\"start\":23352},{\"end\":23725,\"start\":23659},{\"end\":23914,\"start\":23810},{\"end\":24146,\"start\":23941},{\"end\":24436,\"start\":24161},{\"end\":24581,\"start\":24472},{\"end\":25241,\"start\":24604},{\"end\":26051,\"start\":25339},{\"end\":26405,\"start\":26067},{\"end\":26892,\"start\":26407},{\"end\":27554,\"start\":26927},{\"end\":28482,\"start\":27618},{\"end\":28666,\"start\":28484},{\"end\":29092,\"start\":28740},{\"end\":29603,\"start\":29094},{\"end\":30491,\"start\":29629},{\"end\":31033,\"start\":30493},{\"end\":31479,\"start\":31035},{\"end\":31597,\"start\":31492},{\"end\":32200,\"start\":31599},{\"end\":32522,\"start\":32202},{\"end\":33314,\"start\":32589},{\"end\":33942,\"start\":33324},{\"end\":34462,\"start\":33944},{\"end\":35333,\"start\":34480},{\"end\":36115,\"start\":35350},{\"end\":36195,\"start\":36117},{\"end\":36555,\"start\":36197},{\"end\":36732,\"start\":36557},{\"end\":36796,\"start\":36734},{\"end\":38094,\"start\":36809},{\"end\":38647,\"start\":38158},{\"end\":39411,\"start\":38655},{\"end\":40080,\"start\":39428},{\"end\":40235,\"start\":40082},{\"end\":40748,\"start\":40248},{\"end\":42320,\"start\":40782},{\"end\":42591,\"start\":42344},{\"end\":43154,\"start\":42593},{\"end\":43467,\"start\":43171},{\"end\":43895,\"start\":43469},{\"end\":44415,\"start\":43897},{\"end\":45046,\"start\":44417},{\"end\":46075,\"start\":45048},{\"end\":46683,\"start\":46077},{\"end\":47024,\"start\":46685},{\"end\":47294,\"start\":47026},{\"end\":48034,\"start\":47296},{\"end\":48512,\"start\":48049},{\"end\":48947,\"start\":48514},{\"end\":49195,\"start\":48949}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":16672,\"start\":16650},{\"attributes\":{\"id\":\"formula_1\"},\"end\":17182,\"start\":17083},{\"attributes\":{\"id\":\"formula_2\"},\"end\":17422,\"start\":17315},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18157,\"start\":18122},{\"attributes\":{\"id\":\"formula_4\"},\"end\":18835,\"start\":18784},{\"attributes\":{\"id\":\"formula_5\"},\"end\":19001,\"start\":18975},{\"attributes\":{\"id\":\"formula_6\"},\"end\":19205,\"start\":19115},{\"attributes\":{\"id\":\"formula_7\"},\"end\":19459,\"start\":19423},{\"attributes\":{\"id\":\"formula_8\"},\"end\":19758,\"start\":19650},{\"attributes\":{\"id\":\"formula_9\"},\"end\":20360,\"start\":20276},{\"attributes\":{\"id\":\"formula_10\"},\"end\":20532,\"start\":20488},{\"attributes\":{\"id\":\"formula_11\"},\"end\":20780,\"start\":20750},{\"attributes\":{\"id\":\"formula_12\"},\"end\":20842,\"start\":20780},{\"attributes\":{\"id\":\"formula_13\"},\"end\":22419,\"start\":22372},{\"attributes\":{\"id\":\"formula_14\"},\"end\":22776,\"start\":22738},{\"attributes\":{\"id\":\"formula_17\"},\"end\":23195,\"start\":23174},{\"attributes\":{\"id\":\"formula_18\"},\"end\":23351,\"start\":23330},{\"attributes\":{\"id\":\"formula_19\"},\"end\":23658,\"start\":23611},{\"attributes\":{\"id\":\"formula_20\"},\"end\":23809,\"start\":23770},{\"attributes\":{\"id\":\"formula_21\"},\"end\":23940,\"start\":23915},{\"attributes\":{\"id\":\"formula_22\"},\"end\":24160,\"start\":24147},{\"attributes\":{\"id\":\"formula_23\"},\"end\":24471,\"start\":24437},{\"attributes\":{\"id\":\"formula_24\"},\"end\":25338,\"start\":25242},{\"attributes\":{\"id\":\"formula_25\"},\"end\":28739,\"start\":28667}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":31553,\"start\":31546},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":36873,\"start\":36866},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":40308,\"start\":40301},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":41622,\"start\":41615}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1390,\"start\":1378},{\"attributes\":{\"n\":\"2\"},\"end\":9470,\"start\":9462},{\"attributes\":{\"n\":\"2.1\"},\"end\":10492,\"start\":10474},{\"attributes\":{\"n\":\"2.2\"},\"end\":12212,\"start\":12199},{\"attributes\":{\"n\":\"2.3\"},\"end\":13349,\"start\":13338},{\"attributes\":{\"n\":\"2.4\"},\"end\":14345,\"start\":14320},{\"attributes\":{\"n\":\"2.4.1\"},\"end\":15848,\"start\":15744},{\"attributes\":{\"n\":\"2.4.2\"},\"end\":21403,\"start\":21393},{\"attributes\":{\"n\":\"2.5\"},\"end\":23769,\"start\":23728},{\"attributes\":{\"n\":\"2.6\"},\"end\":24602,\"start\":24584},{\"attributes\":{\"n\":\"3\"},\"end\":26065,\"start\":26054},{\"attributes\":{\"n\":\"3.1\"},\"end\":26925,\"start\":26895},{\"attributes\":{\"n\":\"3.1.2\"},\"end\":27616,\"start\":27557},{\"attributes\":{\"n\":\"3.1.4\"},\"end\":29627,\"start\":29606},{\"attributes\":{\"n\":\"3.1.5\"},\"end\":31490,\"start\":31482},{\"attributes\":{\"n\":\"3.2\"},\"end\":32564,\"start\":32525},{\"attributes\":{\"n\":\"3.2.1\"},\"end\":32587,\"start\":32567},{\"end\":33322,\"start\":33317},{\"attributes\":{\"n\":\"3.2.3\"},\"end\":34478,\"start\":34465},{\"attributes\":{\"n\":\"3.2.4\"},\"end\":35348,\"start\":35336},{\"attributes\":{\"n\":\"3.2.5\"},\"end\":36807,\"start\":36799},{\"attributes\":{\"n\":\"3.3.2\"},\"end\":38156,\"start\":38097},{\"attributes\":{\"n\":\"3.3.3\"},\"end\":38653,\"start\":38650},{\"attributes\":{\"n\":\"3.3.4\"},\"end\":39426,\"start\":39414},{\"attributes\":{\"n\":\"3.3.5\"},\"end\":40246,\"start\":40238},{\"attributes\":{\"n\":\"4\"},\"end\":40780,\"start\":40751},{\"attributes\":{\"n\":\"4.2\"},\"end\":42342,\"start\":42323},{\"attributes\":{\"n\":\"5\"},\"end\":43169,\"start\":43157},{\"attributes\":{\"n\":\"6\"},\"end\":48047,\"start\":48037},{\"end\":49207,\"start\":49197},{\"end\":49475,\"start\":49465},{\"end\":49789,\"start\":49779},{\"end\":49859,\"start\":49849},{\"end\":50441,\"start\":50433},{\"end\":51354,\"start\":51345},{\"end\":52413,\"start\":52404},{\"end\":52691,\"start\":52682},{\"end\":52810,\"start\":52801}]", "table": "[{\"end\":51343,\"start\":51030},{\"end\":52402,\"start\":51439},{\"end\":52680,\"start\":52487},{\"end\":52799,\"start\":52734},{\"end\":53418,\"start\":52928}]", "figure_caption": "[{\"end\":49463,\"start\":49209},{\"end\":49706,\"start\":49477},{\"end\":49777,\"start\":49709},{\"end\":49847,\"start\":49791},{\"end\":50431,\"start\":49861},{\"end\":50863,\"start\":50443},{\"end\":51021,\"start\":50866},{\"end\":51030,\"start\":51024},{\"end\":51439,\"start\":51356},{\"end\":52487,\"start\":52415},{\"end\":52734,\"start\":52693},{\"end\":52928,\"start\":52812}]", "figure_ref": "[{\"end\":9697,\"start\":9689},{\"end\":10159,\"start\":10151},{\"end\":12128,\"start\":12120},{\"end\":12636,\"start\":12628},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14660,\"start\":14652},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14778,\"start\":14770},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15037,\"start\":15029},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":15393,\"start\":15385},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22023,\"start\":22015}]", "bib_author_first_name": "[{\"end\":53721,\"start\":53712},{\"end\":53734,\"start\":53733},{\"end\":53750,\"start\":53741},{\"end\":53764,\"start\":53757},{\"end\":54077,\"start\":54074},{\"end\":54090,\"start\":54084},{\"end\":54102,\"start\":54098},{\"end\":54113,\"start\":54109},{\"end\":54411,\"start\":54408},{\"end\":54424,\"start\":54418},{\"end\":54442,\"start\":54438},{\"end\":54453,\"start\":54449},{\"end\":54768,\"start\":54767},{\"end\":55054,\"start\":55048},{\"end\":55070,\"start\":55065},{\"end\":55089,\"start\":55079},{\"end\":55093,\"start\":55090},{\"end\":55566,\"start\":55559},{\"end\":55586,\"start\":55577},{\"end\":55598,\"start\":55592},{\"end\":55881,\"start\":55875},{\"end\":55895,\"start\":55887},{\"end\":55906,\"start\":55900},{\"end\":55916,\"start\":55913},{\"end\":56325,\"start\":56319},{\"end\":56342,\"start\":56334},{\"end\":56356,\"start\":56352},{\"end\":56737,\"start\":56731},{\"end\":56750,\"start\":56744},{\"end\":57252,\"start\":57243},{\"end\":57654,\"start\":57650},{\"end\":57669,\"start\":57664},{\"end\":57685,\"start\":57680},{\"end\":57992,\"start\":57985},{\"end\":58011,\"start\":58003},{\"end\":58032,\"start\":58023},{\"end\":58374,\"start\":58369},{\"end\":58733,\"start\":58732},{\"end\":58746,\"start\":58742},{\"end\":58748,\"start\":58747},{\"end\":58763,\"start\":58755},{\"end\":58765,\"start\":58764},{\"end\":59205,\"start\":59199},{\"end\":59221,\"start\":59215},{\"end\":59223,\"start\":59222},{\"end\":59237,\"start\":59232},{\"end\":59956,\"start\":59950},{\"end\":59966,\"start\":59963},{\"end\":59980,\"start\":59973},{\"end\":59995,\"start\":59989},{\"end\":60476,\"start\":60468},{\"end\":60497,\"start\":60489},{\"end\":60904,\"start\":60896},{\"end\":60915,\"start\":60909},{\"end\":60930,\"start\":60923},{\"end\":61240,\"start\":61239},{\"end\":61257,\"start\":61249},{\"end\":61259,\"start\":61258},{\"end\":61566,\"start\":61560},{\"end\":61576,\"start\":61575},{\"end\":61590,\"start\":61587},{\"end\":61605,\"start\":61599},{\"end\":62313,\"start\":62308},{\"end\":62324,\"start\":62321},{\"end\":62330,\"start\":62325},{\"end\":62344,\"start\":62336},{\"end\":62357,\"start\":62352},{\"end\":62369,\"start\":62362},{\"end\":62905,\"start\":62900},{\"end\":62918,\"start\":62913},{\"end\":62931,\"start\":62927},{\"end\":62939,\"start\":62938},{\"end\":62954,\"start\":62949},{\"end\":63268,\"start\":63264},{\"end\":63275,\"start\":63273},{\"end\":63283,\"start\":63280},{\"end\":63294,\"start\":63289},{\"end\":63302,\"start\":63299},{\"end\":63645,\"start\":63635},{\"end\":63659,\"start\":63652},{\"end\":63674,\"start\":63669},{\"end\":63687,\"start\":63683},{\"end\":64205,\"start\":64197},{\"end\":64213,\"start\":64210},{\"end\":64222,\"start\":64218},{\"end\":64232,\"start\":64229},{\"end\":64531,\"start\":64525},{\"end\":64544,\"start\":64539},{\"end\":64559,\"start\":64553},{\"end\":65052,\"start\":65044},{\"end\":65359,\"start\":65351},{\"end\":65375,\"start\":65367},{\"end\":65392,\"start\":65387},{\"end\":65411,\"start\":65403},{\"end\":65430,\"start\":65423},{\"end\":65446,\"start\":65441},{\"end\":65448,\"start\":65447},{\"end\":65805,\"start\":65797},{\"end\":65817,\"start\":65813},{\"end\":65830,\"start\":65826},{\"end\":65843,\"start\":65838},{\"end\":65845,\"start\":65844},{\"end\":65860,\"start\":65854},{\"end\":65874,\"start\":65869},{\"end\":65889,\"start\":65882},{\"end\":66243,\"start\":66236},{\"end\":66254,\"start\":66249},{\"end\":66270,\"start\":66268},{\"end\":66553,\"start\":66549},{\"end\":67087,\"start\":67083},{\"end\":67097,\"start\":67093},{\"end\":67111,\"start\":67106},{\"end\":67130,\"start\":67119},{\"end\":67379,\"start\":67378},{\"end\":67395,\"start\":67390},{\"end\":67610,\"start\":67602},{\"end\":67628,\"start\":67621},{\"end\":67642,\"start\":67635},{\"end\":67656,\"start\":67650},{\"end\":68092,\"start\":68084},{\"end\":68108,\"start\":68104},{\"end\":68127,\"start\":68120},{\"end\":68144,\"start\":68137},{\"end\":68661,\"start\":68652},{\"end\":68677,\"start\":68671},{\"end\":68691,\"start\":68685},{\"end\":69169,\"start\":69165},{\"end\":69411,\"start\":69406},{\"end\":69433,\"start\":69429},{\"end\":69442,\"start\":69434},{\"end\":69460,\"start\":69455},{\"end\":69474,\"start\":69469},{\"end\":69909,\"start\":69901},{\"end\":69918,\"start\":69915},{\"end\":69929,\"start\":69924},{\"end\":69931,\"start\":69930},{\"end\":69945,\"start\":69940},{\"end\":69959,\"start\":69950},{\"end\":69971,\"start\":69966},{\"end\":70511,\"start\":70502},{\"end\":70525,\"start\":70518},{\"end\":70543,\"start\":70535},{\"end\":70786,\"start\":70775},{\"end\":70805,\"start\":70796},{\"end\":70823,\"start\":70816},{\"end\":71135,\"start\":71130},{\"end\":71154,\"start\":71146},{\"end\":71501,\"start\":71496},{\"end\":71515,\"start\":71511},{\"end\":71530,\"start\":71527},{\"end\":71541,\"start\":71537},{\"end\":71543,\"start\":71542},{\"end\":71557,\"start\":71553},{\"end\":71879,\"start\":71873},{\"end\":71889,\"start\":71888},{\"end\":72128,\"start\":72123},{\"end\":72143,\"start\":72135},{\"end\":72145,\"start\":72144},{\"end\":72546,\"start\":72543},{\"end\":72551,\"start\":72547},{\"end\":72576,\"start\":72570},{\"end\":72981,\"start\":72978},{\"end\":72986,\"start\":72982},{\"end\":73009,\"start\":73005},{\"end\":73024,\"start\":73018},{\"end\":73419,\"start\":73412},{\"end\":73435,\"start\":73430},{\"end\":73448,\"start\":73444},{\"end\":73463,\"start\":73455},{\"end\":74499,\"start\":74494},{\"end\":74501,\"start\":74500},{\"end\":74942,\"start\":74938},{\"end\":74956,\"start\":74951},{\"end\":74958,\"start\":74957},{\"end\":74971,\"start\":74967},{\"end\":74988,\"start\":74979},{\"end\":75516,\"start\":75509},{\"end\":75529,\"start\":75525},{\"end\":75545,\"start\":75541},{\"end\":75555,\"start\":75550},{\"end\":75565,\"start\":75564},{\"end\":75585,\"start\":75579},{\"end\":75606,\"start\":75595},{\"end\":76150,\"start\":76144},{\"end\":76171,\"start\":76163},{\"end\":76184,\"start\":76180},{\"end\":76201,\"start\":76197},{\"end\":76219,\"start\":76213},{\"end\":76626,\"start\":76620},{\"end\":76643,\"start\":76635},{\"end\":76659,\"start\":76655},{\"end\":76965,\"start\":76957},{\"end\":76989,\"start\":76984},{\"end\":77005,\"start\":77000},{\"end\":77007,\"start\":77006},{\"end\":77023,\"start\":77016},{\"end\":77038,\"start\":77031},{\"end\":77486,\"start\":77482},{\"end\":77504,\"start\":77496},{\"end\":77520,\"start\":77517},{\"end\":77953,\"start\":77949},{\"end\":77965,\"start\":77960},{\"end\":77979,\"start\":77974},{\"end\":78362,\"start\":78360},{\"end\":78376,\"start\":78369},{\"end\":78392,\"start\":78384},{\"end\":78631,\"start\":78623},{\"end\":78642,\"start\":78638},{\"end\":79077,\"start\":79072},{\"end\":79089,\"start\":79084},{\"end\":79097,\"start\":79094},{\"end\":79106,\"start\":79103},{\"end\":79122,\"start\":79114},{\"end\":79498,\"start\":79492},{\"end\":79509,\"start\":79505},{\"end\":79521,\"start\":79516},{\"end\":80230,\"start\":80221},{\"end\":80241,\"start\":80236},{\"end\":80253,\"start\":80248},{\"end\":80266,\"start\":80260},{\"end\":80284,\"start\":80278}]", "bib_author_last_name": "[{\"end\":53731,\"start\":53722},{\"end\":53739,\"start\":53735},{\"end\":53755,\"start\":53751},{\"end\":53772,\"start\":53765},{\"end\":53780,\"start\":53774},{\"end\":54082,\"start\":54078},{\"end\":54096,\"start\":54091},{\"end\":54107,\"start\":54103},{\"end\":54119,\"start\":54114},{\"end\":54416,\"start\":54412},{\"end\":54436,\"start\":54425},{\"end\":54447,\"start\":54443},{\"end\":54459,\"start\":54454},{\"end\":54775,\"start\":54769},{\"end\":54788,\"start\":54777},{\"end\":55063,\"start\":55055},{\"end\":55077,\"start\":55071},{\"end\":55100,\"start\":55094},{\"end\":55575,\"start\":55567},{\"end\":55590,\"start\":55587},{\"end\":55605,\"start\":55599},{\"end\":55885,\"start\":55882},{\"end\":55898,\"start\":55896},{\"end\":55911,\"start\":55907},{\"end\":55920,\"start\":55917},{\"end\":56332,\"start\":56326},{\"end\":56350,\"start\":56343},{\"end\":56364,\"start\":56357},{\"end\":56742,\"start\":56738},{\"end\":56756,\"start\":56751},{\"end\":57261,\"start\":57253},{\"end\":57662,\"start\":57655},{\"end\":57678,\"start\":57670},{\"end\":57691,\"start\":57686},{\"end\":58001,\"start\":57993},{\"end\":58021,\"start\":58012},{\"end\":58037,\"start\":58033},{\"end\":58381,\"start\":58375},{\"end\":58740,\"start\":58734},{\"end\":58753,\"start\":58749},{\"end\":58773,\"start\":58766},{\"end\":58781,\"start\":58775},{\"end\":59213,\"start\":59206},{\"end\":59230,\"start\":59224},{\"end\":59252,\"start\":59238},{\"end\":59961,\"start\":59957},{\"end\":59971,\"start\":59967},{\"end\":59987,\"start\":59981},{\"end\":60001,\"start\":59996},{\"end\":60009,\"start\":60003},{\"end\":60487,\"start\":60477},{\"end\":60507,\"start\":60498},{\"end\":60907,\"start\":60905},{\"end\":60921,\"start\":60916},{\"end\":60934,\"start\":60931},{\"end\":61247,\"start\":61241},{\"end\":61265,\"start\":61260},{\"end\":61273,\"start\":61267},{\"end\":61573,\"start\":61567},{\"end\":61585,\"start\":61577},{\"end\":61597,\"start\":61591},{\"end\":61612,\"start\":61606},{\"end\":61618,\"start\":61614},{\"end\":62319,\"start\":62314},{\"end\":62334,\"start\":62331},{\"end\":62350,\"start\":62345},{\"end\":62360,\"start\":62358},{\"end\":62378,\"start\":62370},{\"end\":62911,\"start\":62906},{\"end\":62925,\"start\":62919},{\"end\":62936,\"start\":62932},{\"end\":62947,\"start\":62940},{\"end\":62963,\"start\":62955},{\"end\":62967,\"start\":62965},{\"end\":63271,\"start\":63269},{\"end\":63278,\"start\":63276},{\"end\":63287,\"start\":63284},{\"end\":63297,\"start\":63295},{\"end\":63306,\"start\":63303},{\"end\":63650,\"start\":63646},{\"end\":63667,\"start\":63660},{\"end\":63681,\"start\":63675},{\"end\":63699,\"start\":63688},{\"end\":64208,\"start\":64206},{\"end\":64216,\"start\":64214},{\"end\":64227,\"start\":64223},{\"end\":64235,\"start\":64233},{\"end\":64537,\"start\":64532},{\"end\":64551,\"start\":64545},{\"end\":64568,\"start\":64560},{\"end\":65061,\"start\":65053},{\"end\":65365,\"start\":65360},{\"end\":65385,\"start\":65376},{\"end\":65401,\"start\":65393},{\"end\":65421,\"start\":65412},{\"end\":65439,\"start\":65431},{\"end\":65455,\"start\":65449},{\"end\":65811,\"start\":65806},{\"end\":65824,\"start\":65818},{\"end\":65836,\"start\":65831},{\"end\":65852,\"start\":65846},{\"end\":65867,\"start\":65861},{\"end\":65880,\"start\":65875},{\"end\":65898,\"start\":65890},{\"end\":66247,\"start\":66244},{\"end\":66264,\"start\":66255},{\"end\":66276,\"start\":66271},{\"end\":66557,\"start\":66554},{\"end\":67091,\"start\":67088},{\"end\":67104,\"start\":67098},{\"end\":67117,\"start\":67112},{\"end\":67135,\"start\":67131},{\"end\":67388,\"start\":67380},{\"end\":67402,\"start\":67396},{\"end\":67406,\"start\":67404},{\"end\":67619,\"start\":67611},{\"end\":67633,\"start\":67629},{\"end\":67648,\"start\":67643},{\"end\":67665,\"start\":67657},{\"end\":68102,\"start\":68093},{\"end\":68118,\"start\":68109},{\"end\":68135,\"start\":68128},{\"end\":68154,\"start\":68145},{\"end\":68669,\"start\":68662},{\"end\":68683,\"start\":68678},{\"end\":68700,\"start\":68692},{\"end\":69181,\"start\":69170},{\"end\":69186,\"start\":69183},{\"end\":69427,\"start\":69412},{\"end\":69453,\"start\":69443},{\"end\":69467,\"start\":69461},{\"end\":69485,\"start\":69475},{\"end\":69913,\"start\":69910},{\"end\":69922,\"start\":69919},{\"end\":69938,\"start\":69932},{\"end\":69948,\"start\":69946},{\"end\":69964,\"start\":69960},{\"end\":69976,\"start\":69972},{\"end\":70516,\"start\":70512},{\"end\":70533,\"start\":70526},{\"end\":70549,\"start\":70544},{\"end\":70794,\"start\":70787},{\"end\":70814,\"start\":70806},{\"end\":70831,\"start\":70824},{\"end\":71144,\"start\":71136},{\"end\":71160,\"start\":71155},{\"end\":71509,\"start\":71502},{\"end\":71525,\"start\":71516},{\"end\":71535,\"start\":71531},{\"end\":71551,\"start\":71544},{\"end\":71562,\"start\":71558},{\"end\":71886,\"start\":71880},{\"end\":71895,\"start\":71890},{\"end\":71902,\"start\":71897},{\"end\":72133,\"start\":72129},{\"end\":72152,\"start\":72146},{\"end\":72568,\"start\":72552},{\"end\":72583,\"start\":72577},{\"end\":72591,\"start\":72585},{\"end\":73003,\"start\":72987},{\"end\":73016,\"start\":73010},{\"end\":73034,\"start\":73025},{\"end\":73042,\"start\":73036},{\"end\":73428,\"start\":73420},{\"end\":73442,\"start\":73436},{\"end\":73453,\"start\":73449},{\"end\":73467,\"start\":73464},{\"end\":74027,\"start\":74004},{\"end\":74032,\"start\":74029},{\"end\":74492,\"start\":74469},{\"end\":74505,\"start\":74502},{\"end\":74514,\"start\":74507},{\"end\":74949,\"start\":74943},{\"end\":74965,\"start\":74959},{\"end\":74977,\"start\":74972},{\"end\":74994,\"start\":74989},{\"end\":75523,\"start\":75517},{\"end\":75539,\"start\":75530},{\"end\":75548,\"start\":75546},{\"end\":75562,\"start\":75556},{\"end\":75577,\"start\":75566},{\"end\":75593,\"start\":75586},{\"end\":75609,\"start\":75607},{\"end\":75616,\"start\":75611},{\"end\":76161,\"start\":76151},{\"end\":76178,\"start\":76172},{\"end\":76195,\"start\":76185},{\"end\":76211,\"start\":76202},{\"end\":76233,\"start\":76220},{\"end\":76633,\"start\":76627},{\"end\":76653,\"start\":76644},{\"end\":76669,\"start\":76660},{\"end\":76982,\"start\":76966},{\"end\":76998,\"start\":76990},{\"end\":77014,\"start\":77008},{\"end\":77029,\"start\":77024},{\"end\":77048,\"start\":77039},{\"end\":77494,\"start\":77487},{\"end\":77515,\"start\":77505},{\"end\":77531,\"start\":77521},{\"end\":77958,\"start\":77954},{\"end\":77972,\"start\":77966},{\"end\":77982,\"start\":77980},{\"end\":78367,\"start\":78363},{\"end\":78382,\"start\":78377},{\"end\":78395,\"start\":78393},{\"end\":78636,\"start\":78632},{\"end\":78648,\"start\":78643},{\"end\":79082,\"start\":79078},{\"end\":79092,\"start\":79090},{\"end\":79101,\"start\":79098},{\"end\":79112,\"start\":79107},{\"end\":79126,\"start\":79123},{\"end\":79503,\"start\":79499},{\"end\":79514,\"start\":79510},{\"end\":79526,\"start\":79522},{\"end\":80234,\"start\":80231},{\"end\":80246,\"start\":80242},{\"end\":80258,\"start\":80254},{\"end\":80276,\"start\":80267},{\"end\":80291,\"start\":80285}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":207591052},\"end\":54000,\"start\":53653},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":51926976},\"end\":54360,\"start\":54002},{\"attributes\":{\"id\":\"b2\"},\"end\":54705,\"start\":54362},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":2212026},\"end\":54979,\"start\":54707},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":16296713},\"end\":55486,\"start\":54981},{\"attributes\":{\"doi\":\"arXiv:1409.0473\",\"id\":\"b5\"},\"end\":55805,\"start\":55488},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":52956432},\"end\":56235,\"start\":55807},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":217191279},\"end\":56693,\"start\":56237},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1438450},\"end\":57139,\"start\":56695},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":16929540},\"end\":57563,\"start\":57141},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":7365231},\"end\":57944,\"start\":57565},{\"attributes\":{\"id\":\"b11\"},\"end\":58196,\"start\":57946},{\"attributes\":{\"id\":\"b12\"},\"end\":58299,\"start\":58198},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":14191784},\"end\":58647,\"start\":58301},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":6299466},\"end\":59107,\"start\":58649},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":53083549},\"end\":59858,\"start\":59109},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":14187723},\"end\":60409,\"start\":59860},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":118640240},\"end\":60876,\"start\":60411},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":47021242},\"end\":61177,\"start\":60878},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":16872686},\"end\":61476,\"start\":61179},{\"attributes\":{\"id\":\"b20\"},\"end\":61770,\"start\":61478},{\"attributes\":{\"id\":\"b21\"},\"end\":62223,\"start\":61772},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":195298550},\"end\":62805,\"start\":62225},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":207852453},\"end\":63232,\"start\":62807},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":49584534},\"end\":63577,\"start\":63234},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":8820379},\"end\":64132,\"start\":63579},{\"attributes\":{\"id\":\"b26\"},\"end\":64437,\"start\":64134},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":9237290},\"end\":65008,\"start\":64439},{\"attributes\":{\"id\":\"b28\"},\"end\":65281,\"start\":65010},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":13978413},\"end\":65730,\"start\":65283},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":18077633},\"end\":66187,\"start\":65732},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":14525572},\"end\":66488,\"start\":66189},{\"attributes\":{\"doi\":\"10.3115/v1/D14-1181\",\"id\":\"b32\",\"matched_paper_id\":9672033},\"end\":67081,\"start\":66490},{\"attributes\":{\"doi\":\"arXiv:1702.00887\",\"id\":\"b33\"},\"end\":67332,\"start\":67083},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b34\"},\"end\":67559,\"start\":67334},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":16297289},\"end\":68011,\"start\":67561},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":195298554},\"end\":68569,\"start\":68013},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":59606259},\"end\":69081,\"start\":68571},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":14322823},\"end\":69337,\"start\":69083},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":1839883},\"end\":69823,\"start\":69339},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":52070089},\"end\":70423,\"start\":69825},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":2493017},\"end\":70734,\"start\":70425},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":32493971},\"end\":71025,\"start\":70736},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":21675120},\"end\":71417,\"start\":71027},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":16447573},\"end\":71834,\"start\":71419},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":10267782},\"end\":72059,\"start\":71836},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":15539264},\"end\":72472,\"start\":72061},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":18434647},\"end\":72919,\"start\":72474},{\"attributes\":{\"id\":\"b48\"},\"end\":73184,\"start\":72921},{\"attributes\":{\"id\":\"b49\"},\"end\":73346,\"start\":73186},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":11080756},\"end\":73933,\"start\":73348},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":11456012},\"end\":74369,\"start\":73935},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":15787524},\"end\":74883,\"start\":74371},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":6410607},\"end\":75428,\"start\":74885},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":990233},\"end\":76075,\"start\":75430},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":6844431},\"end\":76512,\"start\":76077},{\"attributes\":{\"id\":\"b56\"},\"end\":76861,\"start\":76514},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":9963},\"end\":77425,\"start\":76863},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":102353251},\"end\":77909,\"start\":77427},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":2604620},\"end\":78305,\"start\":77911},{\"attributes\":{\"doi\":\"arXiv:1711.07163\",\"id\":\"b60\"},\"end\":78566,\"start\":78307},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":822804},\"end\":79020,\"start\":78568},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":17864628},\"end\":79429,\"start\":79022},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":6857205},\"end\":80141,\"start\":79431},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":43922261},\"end\":80640,\"start\":80143}]", "bib_title": "[{\"end\":53710,\"start\":53653},{\"end\":54072,\"start\":54002},{\"end\":54406,\"start\":54362},{\"end\":54765,\"start\":54707},{\"end\":55046,\"start\":54981},{\"end\":55873,\"start\":55807},{\"end\":56317,\"start\":56237},{\"end\":56729,\"start\":56695},{\"end\":57241,\"start\":57141},{\"end\":57648,\"start\":57565},{\"end\":58367,\"start\":58301},{\"end\":58730,\"start\":58649},{\"end\":59197,\"start\":59109},{\"end\":59948,\"start\":59860},{\"end\":60466,\"start\":60411},{\"end\":60894,\"start\":60878},{\"end\":61237,\"start\":61179},{\"end\":62306,\"start\":62225},{\"end\":62898,\"start\":62807},{\"end\":63262,\"start\":63234},{\"end\":63633,\"start\":63579},{\"end\":64523,\"start\":64439},{\"end\":65349,\"start\":65283},{\"end\":65795,\"start\":65732},{\"end\":66234,\"start\":66189},{\"end\":66547,\"start\":66490},{\"end\":67600,\"start\":67561},{\"end\":68082,\"start\":68013},{\"end\":68650,\"start\":68571},{\"end\":69163,\"start\":69083},{\"end\":69404,\"start\":69339},{\"end\":69899,\"start\":69825},{\"end\":70500,\"start\":70425},{\"end\":70773,\"start\":70736},{\"end\":71128,\"start\":71027},{\"end\":71494,\"start\":71419},{\"end\":71871,\"start\":71836},{\"end\":72121,\"start\":72061},{\"end\":72541,\"start\":72474},{\"end\":73410,\"start\":73348},{\"end\":74002,\"start\":73935},{\"end\":74467,\"start\":74371},{\"end\":74936,\"start\":74885},{\"end\":75507,\"start\":75430},{\"end\":76142,\"start\":76077},{\"end\":76955,\"start\":76863},{\"end\":77480,\"start\":77427},{\"end\":77947,\"start\":77911},{\"end\":78621,\"start\":78568},{\"end\":79070,\"start\":79022},{\"end\":79490,\"start\":79431},{\"end\":80219,\"start\":80143}]", "bib_author": "[{\"end\":53733,\"start\":53712},{\"end\":53741,\"start\":53733},{\"end\":53757,\"start\":53741},{\"end\":53774,\"start\":53757},{\"end\":53782,\"start\":53774},{\"end\":54084,\"start\":54074},{\"end\":54098,\"start\":54084},{\"end\":54109,\"start\":54098},{\"end\":54121,\"start\":54109},{\"end\":54418,\"start\":54408},{\"end\":54438,\"start\":54418},{\"end\":54449,\"start\":54438},{\"end\":54461,\"start\":54449},{\"end\":54777,\"start\":54767},{\"end\":54790,\"start\":54777},{\"end\":55065,\"start\":55048},{\"end\":55079,\"start\":55065},{\"end\":55102,\"start\":55079},{\"end\":55577,\"start\":55559},{\"end\":55592,\"start\":55577},{\"end\":55607,\"start\":55592},{\"end\":55887,\"start\":55875},{\"end\":55900,\"start\":55887},{\"end\":55913,\"start\":55900},{\"end\":55922,\"start\":55913},{\"end\":56334,\"start\":56319},{\"end\":56352,\"start\":56334},{\"end\":56366,\"start\":56352},{\"end\":56744,\"start\":56731},{\"end\":56758,\"start\":56744},{\"end\":57263,\"start\":57243},{\"end\":57664,\"start\":57650},{\"end\":57680,\"start\":57664},{\"end\":57693,\"start\":57680},{\"end\":58003,\"start\":57985},{\"end\":58023,\"start\":58003},{\"end\":58039,\"start\":58023},{\"end\":58383,\"start\":58369},{\"end\":58742,\"start\":58732},{\"end\":58755,\"start\":58742},{\"end\":58775,\"start\":58755},{\"end\":58783,\"start\":58775},{\"end\":59215,\"start\":59199},{\"end\":59232,\"start\":59215},{\"end\":59254,\"start\":59232},{\"end\":59963,\"start\":59950},{\"end\":59973,\"start\":59963},{\"end\":59989,\"start\":59973},{\"end\":60003,\"start\":59989},{\"end\":60011,\"start\":60003},{\"end\":60489,\"start\":60468},{\"end\":60509,\"start\":60489},{\"end\":60909,\"start\":60896},{\"end\":60923,\"start\":60909},{\"end\":60936,\"start\":60923},{\"end\":61249,\"start\":61239},{\"end\":61267,\"start\":61249},{\"end\":61275,\"start\":61267},{\"end\":61575,\"start\":61560},{\"end\":61587,\"start\":61575},{\"end\":61599,\"start\":61587},{\"end\":61614,\"start\":61599},{\"end\":61620,\"start\":61614},{\"end\":62321,\"start\":62308},{\"end\":62336,\"start\":62321},{\"end\":62352,\"start\":62336},{\"end\":62362,\"start\":62352},{\"end\":62380,\"start\":62362},{\"end\":62913,\"start\":62900},{\"end\":62927,\"start\":62913},{\"end\":62938,\"start\":62927},{\"end\":62949,\"start\":62938},{\"end\":62965,\"start\":62949},{\"end\":62969,\"start\":62965},{\"end\":63273,\"start\":63264},{\"end\":63280,\"start\":63273},{\"end\":63289,\"start\":63280},{\"end\":63299,\"start\":63289},{\"end\":63308,\"start\":63299},{\"end\":63652,\"start\":63635},{\"end\":63669,\"start\":63652},{\"end\":63683,\"start\":63669},{\"end\":63701,\"start\":63683},{\"end\":64210,\"start\":64197},{\"end\":64218,\"start\":64210},{\"end\":64229,\"start\":64218},{\"end\":64237,\"start\":64229},{\"end\":64539,\"start\":64525},{\"end\":64553,\"start\":64539},{\"end\":64570,\"start\":64553},{\"end\":65063,\"start\":65044},{\"end\":65367,\"start\":65351},{\"end\":65387,\"start\":65367},{\"end\":65403,\"start\":65387},{\"end\":65423,\"start\":65403},{\"end\":65441,\"start\":65423},{\"end\":65457,\"start\":65441},{\"end\":65813,\"start\":65797},{\"end\":65826,\"start\":65813},{\"end\":65838,\"start\":65826},{\"end\":65854,\"start\":65838},{\"end\":65869,\"start\":65854},{\"end\":65882,\"start\":65869},{\"end\":65900,\"start\":65882},{\"end\":66249,\"start\":66236},{\"end\":66268,\"start\":66249},{\"end\":66278,\"start\":66268},{\"end\":66559,\"start\":66549},{\"end\":67093,\"start\":67083},{\"end\":67106,\"start\":67093},{\"end\":67119,\"start\":67106},{\"end\":67137,\"start\":67119},{\"end\":67390,\"start\":67378},{\"end\":67404,\"start\":67390},{\"end\":67408,\"start\":67404},{\"end\":67621,\"start\":67602},{\"end\":67635,\"start\":67621},{\"end\":67650,\"start\":67635},{\"end\":67667,\"start\":67650},{\"end\":68104,\"start\":68084},{\"end\":68120,\"start\":68104},{\"end\":68137,\"start\":68120},{\"end\":68156,\"start\":68137},{\"end\":68671,\"start\":68652},{\"end\":68685,\"start\":68671},{\"end\":68702,\"start\":68685},{\"end\":69183,\"start\":69165},{\"end\":69188,\"start\":69183},{\"end\":69429,\"start\":69406},{\"end\":69455,\"start\":69429},{\"end\":69469,\"start\":69455},{\"end\":69487,\"start\":69469},{\"end\":69915,\"start\":69901},{\"end\":69924,\"start\":69915},{\"end\":69940,\"start\":69924},{\"end\":69950,\"start\":69940},{\"end\":69966,\"start\":69950},{\"end\":69978,\"start\":69966},{\"end\":70518,\"start\":70502},{\"end\":70535,\"start\":70518},{\"end\":70551,\"start\":70535},{\"end\":70796,\"start\":70775},{\"end\":70816,\"start\":70796},{\"end\":70833,\"start\":70816},{\"end\":71146,\"start\":71130},{\"end\":71162,\"start\":71146},{\"end\":71511,\"start\":71496},{\"end\":71527,\"start\":71511},{\"end\":71537,\"start\":71527},{\"end\":71553,\"start\":71537},{\"end\":71564,\"start\":71553},{\"end\":71888,\"start\":71873},{\"end\":71897,\"start\":71888},{\"end\":71904,\"start\":71897},{\"end\":72135,\"start\":72123},{\"end\":72154,\"start\":72135},{\"end\":72570,\"start\":72543},{\"end\":72585,\"start\":72570},{\"end\":72593,\"start\":72585},{\"end\":73005,\"start\":72978},{\"end\":73018,\"start\":73005},{\"end\":73036,\"start\":73018},{\"end\":73044,\"start\":73036},{\"end\":73430,\"start\":73412},{\"end\":73444,\"start\":73430},{\"end\":73455,\"start\":73444},{\"end\":73469,\"start\":73455},{\"end\":74029,\"start\":74004},{\"end\":74034,\"start\":74029},{\"end\":74494,\"start\":74469},{\"end\":74507,\"start\":74494},{\"end\":74516,\"start\":74507},{\"end\":74951,\"start\":74938},{\"end\":74967,\"start\":74951},{\"end\":74979,\"start\":74967},{\"end\":74996,\"start\":74979},{\"end\":75525,\"start\":75509},{\"end\":75541,\"start\":75525},{\"end\":75550,\"start\":75541},{\"end\":75564,\"start\":75550},{\"end\":75579,\"start\":75564},{\"end\":75595,\"start\":75579},{\"end\":75611,\"start\":75595},{\"end\":75618,\"start\":75611},{\"end\":76163,\"start\":76144},{\"end\":76180,\"start\":76163},{\"end\":76197,\"start\":76180},{\"end\":76213,\"start\":76197},{\"end\":76235,\"start\":76213},{\"end\":76635,\"start\":76620},{\"end\":76655,\"start\":76635},{\"end\":76671,\"start\":76655},{\"end\":76984,\"start\":76957},{\"end\":77000,\"start\":76984},{\"end\":77016,\"start\":77000},{\"end\":77031,\"start\":77016},{\"end\":77050,\"start\":77031},{\"end\":77496,\"start\":77482},{\"end\":77517,\"start\":77496},{\"end\":77533,\"start\":77517},{\"end\":77960,\"start\":77949},{\"end\":77974,\"start\":77960},{\"end\":77984,\"start\":77974},{\"end\":78369,\"start\":78360},{\"end\":78384,\"start\":78369},{\"end\":78397,\"start\":78384},{\"end\":78638,\"start\":78623},{\"end\":78650,\"start\":78638},{\"end\":79084,\"start\":79072},{\"end\":79094,\"start\":79084},{\"end\":79103,\"start\":79094},{\"end\":79114,\"start\":79103},{\"end\":79128,\"start\":79114},{\"end\":79505,\"start\":79492},{\"end\":79516,\"start\":79505},{\"end\":79528,\"start\":79516},{\"end\":80236,\"start\":80221},{\"end\":80248,\"start\":80236},{\"end\":80260,\"start\":80248},{\"end\":80278,\"start\":80260},{\"end\":80293,\"start\":80278}]", "bib_venue": "[{\"end\":53810,\"start\":53782},{\"end\":54173,\"start\":54121},{\"end\":54508,\"start\":54461},{\"end\":54825,\"start\":54790},{\"end\":55218,\"start\":55102},{\"end\":55557,\"start\":55488},{\"end\":55985,\"start\":55922},{\"end\":56447,\"start\":56366},{\"end\":56834,\"start\":56758},{\"end\":57345,\"start\":57263},{\"end\":57742,\"start\":57693},{\"end\":57983,\"start\":57946},{\"end\":58236,\"start\":58200},{\"end\":58454,\"start\":58383},{\"end\":58860,\"start\":58783},{\"end\":59401,\"start\":59254},{\"end\":60083,\"start\":60011},{\"end\":60589,\"start\":60509},{\"end\":61010,\"start\":60936},{\"end\":61311,\"start\":61275},{\"end\":61558,\"start\":61478},{\"end\":61919,\"start\":61772},{\"end\":62460,\"start\":62380},{\"end\":63010,\"start\":62969},{\"end\":63367,\"start\":63308},{\"end\":63788,\"start\":63701},{\"end\":64195,\"start\":64134},{\"end\":64661,\"start\":64570},{\"end\":65042,\"start\":65010},{\"end\":65487,\"start\":65457},{\"end\":65941,\"start\":65900},{\"end\":66319,\"start\":66278},{\"end\":66672,\"start\":66578},{\"end\":67182,\"start\":67153},{\"end\":67376,\"start\":67334},{\"end\":67740,\"start\":67667},{\"end\":68236,\"start\":68156},{\"end\":68774,\"start\":68702},{\"end\":69195,\"start\":69188},{\"end\":69559,\"start\":69487},{\"end\":70069,\"start\":69978},{\"end\":70566,\"start\":70551},{\"end\":70861,\"start\":70833},{\"end\":71203,\"start\":71162},{\"end\":71613,\"start\":71564},{\"end\":71931,\"start\":71904},{\"end\":72222,\"start\":72154},{\"end\":72679,\"start\":72593},{\"end\":72976,\"start\":72921},{\"end\":73255,\"start\":73186},{\"end\":73552,\"start\":73469},{\"end\":74127,\"start\":74034},{\"end\":74602,\"start\":74516},{\"end\":75098,\"start\":74996},{\"end\":75704,\"start\":75618},{\"end\":76275,\"start\":76235},{\"end\":76618,\"start\":76514},{\"end\":77122,\"start\":77050},{\"end\":77613,\"start\":77533},{\"end\":78056,\"start\":77984},{\"end\":78358,\"start\":78307},{\"end\":78706,\"start\":78650},{\"end\":79208,\"start\":79128},{\"end\":79670,\"start\":79528},{\"end\":80374,\"start\":80293},{\"end\":56035,\"start\":55987},{\"end\":56897,\"start\":56836},{\"end\":59535,\"start\":59403},{\"end\":60142,\"start\":60085},{\"end\":60656,\"start\":60591},{\"end\":62053,\"start\":61921},{\"end\":62527,\"start\":62462},{\"end\":63413,\"start\":63369},{\"end\":63862,\"start\":63790},{\"end\":64739,\"start\":64663},{\"end\":66764,\"start\":66674},{\"end\":67800,\"start\":67742},{\"end\":68303,\"start\":68238},{\"end\":68833,\"start\":68776},{\"end\":70147,\"start\":70071},{\"end\":72277,\"start\":72224},{\"end\":73622,\"start\":73554},{\"end\":75187,\"start\":75100},{\"end\":75777,\"start\":75706},{\"end\":77680,\"start\":77615},{\"end\":78115,\"start\":78058},{\"end\":78722,\"start\":78708},{\"end\":79799,\"start\":79672}]"}}}, "year": 2023, "month": 12, "day": 17}