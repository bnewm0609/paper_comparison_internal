{"id": 257921883, "updated": "2023-10-05 02:31:11.478", "metadata": {"title": "MM-BSN: Self-Supervised Image Denoising for Real-World with Multi-Mask based on Blind-Spot Network", "authors": "[{\"first\":\"Dan\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Fangfang\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Yuwen\",\"last\":\"Jiang\",\"middle\":[]},{\"first\":\"Zhengming\",\"last\":\"Fu\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Recent advances in deep learning have been pushing image denoising techniques to a new level. In self-supervised image denoising, blind-spot network (BSN) is one of the most common methods. However, most of the existing BSN algorithms use a dot-based central mask, which is recognized as inefficient for images with large-scale spatially correlated noise. In this paper, we give the definition of large-noise and propose a multi-mask strategy using multiple convolutional kernels masked in different shapes to further break the noise spatial correlation. Furthermore, we propose a novel self-supervised image denoising method that combines the multi-mask strategy with BSN (MM-BSN). We show that different masks can cause significant performance differences, and the proposed MM-BSN can efficiently fuse the features extracted by multi-masked layers, while recovering the texture structures destroyed by multi-masking and information transmission. Our MM-BSN can be used to address the problem of large-noise denoising, which cannot be efficiently handled by other BSN methods. Extensive experiments on public real-world datasets demonstrate that the proposed MM-BSN achieves state-of-the-art performance among self-supervised and even unpaired image denoising methods for sRGB images denoising, without any labelling effort or prior knowledge. Code can be found in https://github.com/dannie125/MM-BSN.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2304.01598", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/ZhangZJF23", "doi": "10.1109/cvprw59228.2023.00441"}}, "content": {"source": {"pdf_hash": "5cd662006548f34a5e3427bdff83980e293a0233", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2304.01598v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "7dccba35f54ec67a7d1b71296628ee983af1c647", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/5cd662006548f34a5e3427bdff83980e293a0233.txt", "contents": "\nMM-BSN: Self-Supervised Image Denoising for Real-World with Multi-Mask based on Blind-Spot Network\n\n\nDan Zhang \nSenslab Technology\nShanghaiChina\n\nFangfang Zhou \nSenslab Technology\nShanghaiChina\n\nYuwen Jiang jiang.yuwen@senslab.com \nSenslab Technology\nShanghaiChina\n\nZhengming Fu \nNeuroSens Technology\nAustinU.S\n\nMM-BSN: Self-Supervised Image Denoising for Real-World with Multi-Mask based on Blind-Spot Network\n\nRecent advances in deep learning have been pushing image denoising techniques to a new level. In self-supervised image denoising, blind-spot network (BSN) is one of the most common methods. However, most of the existing BSN algorithms use a dot-based central mask, which is recognized as inefficient for images with large-scale spatially correlated noise. In this paper, we give the definition of large-noise and propose a multi-mask strategy using multiple convolutional kernels masked in different shapes to further break the noise spatial correlation. Furthermore, we propose a novel self-supervised image denoising method that combines the multi-mask strategy with BSN (MM-BSN). We show that different masks can cause significant performance differences, and the proposed MM-BSN can efficiently fuse the features extracted by multi-masked layers, while recovering the texture structures destroyed by multimasking and information transmission. Our MM-BSN can be used to address the problem of large-noise denoising, which cannot be efficiently handled by other BSN methods. Extensive experiments on public real-world datasets demonstrate that the proposed MM-BSN achieves state-ofthe-art performance among self-supervised and even unpaired image denoising methods for sRGB images denoising, without any labelling effort or prior knowledge. Code can be found in https://github.com/dannie125/MM-BSN.\n\nIntroduction\n\nImage denoising is a key step in image processing, and the denoising performance has a significant impact on the subsequent image processing tasks. Traditional image denoising methods [8,11,27] are time consuming and costly, but usually have poor robustness in real-world applications. * Corresponding author (a) DnCNN [43] (Supervised) (b) C2N [16]+DIDN [39] (Unpaired) (c) AP-BSN [22] (Self-supervised) (d) MM-BSN (Self-supervised) Figure 1. Visual comparison of our MM-BSN with other competing methods on the DND benchmark. (a) DnCNN is trained on real-world noisy-clean pairs from the SIDD Medium dataset [1]. (b) C2N uses clean SIDD [1] and noisy DND [33] samples to simulate the real-world noise distribution in an unsupervised manner. (c) AP-BSN is trained directly on the noisy images in the SIDD Medium dataset [1]. (d) MM-BSN is trained on images with real noise from SIDD. We mark the PSNR (dB) and SSIM with respect to the groundtruth for the quantitative comparison.\n\nWith the advancement of deep learning, learning-based image denoising algorithms have made great progress and can be divided into two classes, supervised methods and selfsupervised methods.\n\nThe supervised denoising methods [2,5,7,12,40,41,43] have relatively better performance than the self-supervised. However, supervised image denoising requires a large number of noisy-clean image pairs, which are difficult to collect in practical applications, and generating such image pairs requires massive human effort and cost. One of the most common ways is to add simulated real-world noises, such as Additive White Gaussian Noise (AWGN), to clean images to artificially synthesize noisy images so as to obtain synthetic noisy-clean pairs [12,17,25,32,43,44]. Nevertheless, there is always an unavoidable gap between the synthetic noise and the real noise, which severely affects the performance of these supervised models trained on synthetic noise in the real-world image denoising applications. In addition, in some cases, it is also difficult to obtain clean images.\n\nIn this situation, many self-supervised image denoising methods [3,15,18,19,23,45] that do not require noisy-clean image pairs have been proposed. Noise2Noise [23] used noisy-noisy image pairs to train the model, which achieved comparable performance to supervised algorithms. But it requires two perfectly aligned noisy images, which are difficult to obtain in practice. Noisier2Noise [29] and NAC [38] added the same type of noise as the existing noise to the original noisy image to form noisier-noisy pairs as the training set. This requires the model users to know the specific types of the noise in the image, which is unrealistic in practice because the causes of noise are diverse and the type of noise can change constantly in real-world. IDR [45] adopted an iterative approach, taking the noisy images as inputs to the existing denoising model trained by noisier-noisy pairs, and treating the output as the next round optimization target to further refine the denoising model. In this way, the denoising model is optimized by iterations, which can easily lead to the final denoised image being over-smoothened. Noise2Void [18] proposed a blind spot network (BSN) denoising method based on the assumption that pixel signals in the image are spatially correlated in the image, and noise signals are spatially independent with zero-mean. In recent years, several publications [13,18,20,37] have shown that BSN is effective in synthesizing noise for denoising. However, real-world noise is usually spatially continuous. In most existing BSN denoising methods [3,13,18,19,22,37], the masks used to generate blind spots have a single pixel blinded in the center, which makes it difficult to denoise when the noise correlated area is large. Zhang et al. [42] combined Transformer and CNN to achieve a trade-off between denoising images with global spatially correlated noise and preserving local detail. However, Transformer is computationally intensive, making it difficult to deploy in practical applications on mobile devices [36].\n\nMotivated by the fact that different shapes of convolution kernels can extract different features, we propose a variety of masks with different shapes to generate blind spots, such as '+'-shaped mask, ' '-shaped mask, '\u00d7'-shaped mask, and so on. The multi-masks with different blind spots are used to mask the surrounding pixels at different positions, so as to destroy the spatial correlations of the noise in multidirecion. And we systematically demonstrate the effectiveness of using different masks or different mask combinations for image denoising. In addition, we propose an enhanced BSN that combines with the multi-mask strategy, namely MM-BSN, to more efficiently integrate multi-mask paths, recover the destroyed textures, and control the model size. Extensive experiments demonstrate the effectiveness and superiority of the proposed method.\n\nThe main contributions of our work are as follows: 1.To the best of our knowledge, we are the first to explore the combination of different convolution kernels with multi-mask to extract features, and to perform denoising on images with large-scale spatially correlated noise in selfsupervised. Furthermore, our multi-mask strategy can be integrated with other methods.\n\n2. We propose a novel self-supervised MM-BSN that can integrate the features extracted by multi-masked convolution kernels, control the model size growth, and preserve the image detail when denoising.\n\n3. Our approach achieves the state-of-the-art performance among published self-supervised sRGB image denoising methods, which is significant for practical applications.\n\n\nRelated Work\n\nSupervised image denoising. Zhang et al. [43] first proposed a deep learning based image denoising method called DnCNN, which trained the model with generating noisyclean pairs by manually adding AWGN to clean images. Subsequently, many researchers proposed other image denoising methods [2,5,9,10,17,21,25,32] based on deep learning by adding AWGN to clean sRGB images. However, the denoising performance of these models in the real world was unsatisfactory due to the large gap between artificial and real-world noise. Scholars [4,28] proposed to convert sRGB images to rawRGB first, and then added Poisson noise corresponding to shot noise and Gaussian noise corresponding to read noise to rawRGB. After denoising in rawRGB space, the final denoised result image was converted back to sRGB space using ISP tools. For this denoising method, accurate noise estimation and modelling was essential for success. Although the noise obtained by statistical modelling reduced the gap between the synthetic noise and the real noise, the injected noise was not real and external factors could alter the accuracy of the noise modelling. To this end, it was recognized that the most effective way to denoise was to use the noisy-clean pairs [7,14,34,41] directly from the real-world when available. However, such a noisy-clean pair dataset requires a huge amount of human labour to collect and a huge amount of time to construct in the real world, and was even more impractical given the diverse application scenarios.\n\nSelf-supervised image denoising. Noise2Noise [23] used two perfectly aligned noisy images from the same scene as input and target, respectively. L2 loss was used to minimize the difference between the two noisy images in order to make the model capable of denoising. Then Noise2Void [18], Noise2Self [3], Probabilistic Noise2Void [19], Neighbor2Neighbor [15], IDR [45], CVF-SID [30], Blind2Unblind [35] and AP-BSN [22] were proposed to use only noisy images for training. As the most widely used self-supervised denoising method, BSN was firstly proposed in Noise2Void [19], which is a special CNN that masks pixel in the center of the receptive field, and uses the surrounding information to reconstruct the information of the masked pixels. Its denoising capability is restricted to the assumption that the noise is spatially independent. Noise2Void [19] took the masked image as the input and the fully noisy image as the target to train the model. The masked pixels are not used during training, which can easily lead to loss of detail and over-smoothing of the image. Neighbor2Neighbor [15] synthesized two sub-noisyimages by randomly selecting two adjacent pixels from the 4\u00d74 neighbourhood of the rawRGB image. Two sub-noisyimages were used as input and target for training, respectively, forming noisy-noisy pairs. However, training directly on sub-noisy-images would inevitably lose some image detail. To improve this, Blind2Unblind [35] used all the pixels for training by generating sub-masked-images with pixels masked at different positions, and then used a global mask strategy to collect all pixels from the masked positions in the sub-masked-images after denoising. Although Blind2Unblind makes full use of all pixel information, it is difficult to denoise large-noise using only dot-based masks.\n\nLaine19 [20] occluded half of the receptive fields in four different directions, achieving the effect that the center of the receptive field is not seen. D-BSN [37] and David et al. [13] used the center-masked convolution kernel and the dilated convolution layer (DCL) with a specific step size to construct the BSN. The publications proved that BSN is effective in synthesizing noise for denoising. However, the real-world noise is usually spatially continuous and BSNs would fail to handle it. To break the spatial correlation of real-world noise, AP-BSN [22] adopted Pixel-shuffle Downsampling (PD) with 5-pixel stride on images before training, and utilized center-masked convolution kernel and dilated convolution layer (DCL) to achieve the effect of blind spots during training. However, AP-BSN relies on the PD with limited stride to break the spatial correlation of the noise. If large-noise exists in the image, blindly increasing the PD stride will cause irreversible damage to image details [22]. Therefore, it is challenging for AP-BSN to strike a balance between the noise removal and texture informa- In this paper, we propose a joint feature-extraction method using multi-masked convolutional kernels to destroy largenoise correlations. We also propose a novel architecture that combines the multi-mask convolutional kernels with BSN (MM-BSN) to make full use of the extracted features and preserve texture structures of the original image as much as possible.\n\n\nMotivation\n\nWe explore the noise spatially correlations that have different shapes as shown in Figure 2. The sub-images in Figure 2a all have a size of height\u00d7width as 10\u00d710, which shows that the correlation area is large. We also computes the proportion of spatially correlated noise in different areas of the image in Figure 2b. We define the spatially correlated noise with a area bigger than 25 as large-noise. Figure 2b shows that the large-noise, which theoretically cannot be handled by PD stride not bigger than 5, occupies more than 1/3.\n\nRecently published BSN methods, either the mask in the input [3,15,18,19,35] or the mask in the network [13,22,37], which used a dot-based mask, is not enough to break the correlation of large-noise. In this way, the blind pixels recovered from the surrounding information would still contain noise. Motivated by the prior knowledge that filters with different shapes can be designed to target different types of noise, such as '+', ' ', etc., we propose a novel multi-mask strategy, which ultilizes different convolution kernels masked in different shapes to further destroy the spatial connection of noise.\n\n\nMain Method\n\nMulti-Mask Strategy. We propose to use the multimask strategy to further destroy the spatial connection of the noise, while preserving useful texture information of the image. Figure 3 shows the shapes of different masks when the convolution kernel size is 5\u00d75, such as '+', ' ', '-',\n(a) o (b) - (c) + (d) / (e) \u00d7 (f) (g) | (h) (i) \\ (j)'|', '/' , '\\', '\u00d7', etc.\nTheoretically, we can arbitrarily combine multiple masks of different shapes to achieve different denoising results. When using n number of types of masked convolutional kernels types, the same operations are performed for each path until the final concatenation, where all features extracted by several different masked convolutional kernels are fused together. In this way, we can obtain a number of basis multi-mask BSN models, whose architectures contain multiple branches corresponding to the number of masks. However, the model size obtained by this naive method of simple stacking is almost n times the size of the basic network. Consequently, the workload on the hardware device is multiplied by n. To control the model size, make full use of the information around the blind spot and avoid information redundancy, we generally use a combination of only two masks. The feature extracted by 'o'-shaped mask contains complete information. However, it may contain more unconducive information for denoising because it could not break the spatial connection of the noise sufficiently. The other types of masks mask more pixels of the surrounding pixels, which can break the spatial connection of the noise more, but lose more image information. So we can combine the feature extracted by 'o' to provide more detail and the other shape of masks to break the spatial correlation of the noise and reinforce each other to get a better denoising performance. Of course, two masks with complementary mask shapes also can break the spatial connection of the noise while extracting information from the surrounding pixels. Multi-mask combinations can be flexibly adjusted according to the real noise distribution.\n\nOur multi-mask strategy can be integrated with other methods by simply stacking the different mask paths. However, in this way, the increasing number of different mask types will explode the model size. In addition, the fea-tures extracted by different masks have no interaction between the processing paths at the intermediate stages before the final concatenation. Without such interaction, information transfer and co-optimization between these processing paths is not possible. Therefore, how to use multi-mask to destroy the spatial connection of noise while retaining more texture information is also a challenge. Last but not the least, as the mask area increases, the texture information of the image itself is increasingly destroyed. Finally, we propose a novel MM-BSN, to address these challenges.\n\nMM-BSN Architecture. MM-BSN is initially motivated by AP-BSN [22]. We also use masked convolutional kernels to extract the shallow features. But instead of using only the center mask, we add other shapes of masks to extract the masked features.\n\nThe architecture of MM-BSN is shown in Figure 4. The workflow consists of four steps. First, a linear transformation is performed on the noisy image with a 1\u00d71 convolutional layer, and the output feature containing the complete image information passes through several different masked convolutional layers in parallel. Second, each masked feature passes through three layers in parallel, two 1\u00d71 convolutional layers and a Concatenation-based Dilated Convolutional Layer (CDCL). CDCL contains a small number of DCLs (set to 2 in this article) and its output features are combined with one linearly transformed feature from a 1\u00d71 convolutional layer using a concatenation according to the mask size. The features extracted by the same size but different masked convolution kernels are fused together. Third, after passing through several DCLs (set to 7 in this article), all features are concatenated together, and the features extracted by different masked convolution kernels of different sizes are fused. Finally, the output is obtained by channel transformation and feature fusion with several 1\u00d71 convolutional layers.\n\nDue to the interaction between different feature pathways, the resulting MM-BSN parameter set of 5.3M is larger than AP-BSN of 3.7M, but much smaller than the model size of a simple stack of AP-BSN with multi-mask (namely SMM-BSN) of 7.3M. The ablation experiments of several models are detailed in Section 5.3.\n\nLoss Chosen. In this paper, we use L1 loss function to train our MM-BSN:\nE = I out \u2212 I N 1(1)I out = P D \u22121 (M (P D(I N )))(2)\nWhere M denotes the MM-BSN model, I out is the result of P D \u22121 , and I N is the noisy input. Similar to AP-BSN [22], we use PD to preliminarily break the spatial connection between the noises of adjacent pixels. After PD with stride S pd , we obtain a group of small sub-images that are inputs to MM-BSN. The denoised result I out , which has the same size as the original image, is decoded by operating P D \u22121 to the outputs of the model.\n\n\nExperiments\n\n\nImplementation Details\n\nDatasets. We take the public datasets of SIDD [1] and DND [33] for our experiments. We take the noisy sRGB images in SIDD Medium dataset that contains 320 pairs of noisy-clean images as the training set and SIDD validation as the valid set, respectively. SIDD validation and SIDD benchmark can be used as test sets. DND dataset that contains only 50 noisy image are generally used as the test dataset. Since only noisy images are needed to train our self-supervised models, we use DND as both the training set and the test set.\n\nTraining Details. All models are trained with the same hyperparameters. The batch size is 8 and the number of training epochs is 30. The optimization function adopted is Adam. The initial learning rate is 0.0001, and the learning rate of every 8 epochs is multiplied by 0.1. The images are resized to 128\u00d7128, and are randomly rotated within a range of 90\u00b0in the horizontal or vertical direction before training. All experiments are run on a server with python 3.8.0, py-torch1.12.0, and Nvidia Tesla T4 GPUs. For a relatively fair comparison, unless otherwise stated, we set the PD stride as 5 for training, 2 for testing, and the same post-processing as AP-BSN [22].  \n\n\nMask SIDD Validation SIDD\n\n\nAnalyzing Multi-Mask strategy in BSN\n\nTo compare the performance of the proposed method with different mask combinations, we trained several MM-BSN models with different masks on SIDD Medium dataset. All trained models are quantitatively evaluated on SIDD validation and benchmark. Existing Python toolkits are used to compute the PSNR/SSIM of SIDD validation. At the same time, we upload the denoised results of SIDD benchmark to the official website and obtain the reported PSNR/SSIM.\n\nSignificant effect on breaking the noise structure. To check the effectiveness of the mask in breaking the structure of large-noise, we take the image after PD with S pd =2 as input to train. Figure 5 shows the denoising performance of the models with different S pd s or masks but the same other settings. AP-BSN [22] shows that when S pd =2, the spatial connection of the noise in the image cannot be broken well. Using only the center mask, the model is weakly able to denoise, as shown in Figure 5b. But if we use the ' ' mask when S pd =2, the denoised result is even better than the 'o'-shaped masked model with S pd =5 [22] as shown in Figure 5c and 5d.  datasets are greatly improved when the center mask is replaced with the ' ' mask when S pd =2. This proves that the large-noise structure can be better broken by the ' ' mask, and it is not only an exception shown in Figure 5, but also a common case.\n\nQuantitative comparison of MM-BSN models. We summarize the following points from Table 2: (1) Different mask combinations achieve different denoising performance. The reason is that different masks target different noise correlations, and it is common sense that the final denoised result will be different. (2) The mask combinations combined with 'o' are overall better than the combinations without it, due to that the feature extracted by the 'o'-shaped mask preserves the texture information of the image itself more completely. (3) The combination of '/' and '\\' gives the best performance, followed by the combination of 'o' and '/', which indicates that the dataset has more '/' and '\\' shaped spatially related noise. For different datasets, the noise structures are different, and users can freely choose the combination of masks or design the mask shape suitable for the real dataset according to the needs.\n\nComparison of BSN models with increasing mask types. Figure 6 shows the qualitative denoising performance of models with different number of mask types in the same framework on the SIDD validation dataset. Comparing Figure 6c, 6d and 6e, it can be observed that by adding other types of masks based on the 'o'-shaped mask, the denoising performance is significantly improved. Especially in the second row, the denoised result of AP-BSN has unacceptable color shifts, while SMM-BSN restores the original color perfectly, indicating that adding masks of other shapes can effectively destroy the noise correlation during feature\n\n\nMasks\n\nTest extraction. In addition, it can be seen from the second row of Figure 6b and Figure 6c that the PSNR value after denoising decreases from 22.62dB to 21.69dB when only the center mask is used, but it increases to 39.84dB/39.66dB when the multi-mask is used. This indicates that when the spatially correlated noise region is large, the center mask alone cannot break the noise structure sufficiently. Since the features extracted by the center mask alone may still be noisy, the final denoising result will be biased by massive noise. Figure 6d and 6e show that increasing the number of mask types does not always improve the denoising performance. The possible reason for this is that features extracted by increasing types of masks lead to the information redundancy, which is unsensive and unuseful for denoising.   \ndatasets o -| + / \\ \u00d7\n\nAnalyzing our network architecture\n\nFor fairly comparing, all models are trained on SIDD Medium dataset and evaluated on SIDD validation. AP-BSN [22] uses only the center mask, SMM-BSN and MM-BSN use the combination of '/'-shaped mask and '\\'shaped mask for training, and other settings are the same as before. Table 3 compares AP-BSN and its corresponding extended versions SMM-BSN, which indicates that the denoising performance is significantly improved by applying the multi-mask strategy. The PSNR/SSIM of SIDD validation shows that MM-BSN (37.38/0.882) outperforms AP-BSN (35.91/0.870) by a large margin. Figure 7b shows that using AP-BSN, the alphabets in the yellow box of the image are blurred and a lot of detail is lost. Figure 7c shows that the alphabets in the yellow box are generally preserved. This observation suggests that by adding concatenationbased skip-connections from the shallow features in MM-BSN, the lost detail can be supplemented in time.\n\nWe classify images with different large-noise ratios for SIDD validation and calculate the average PSNR of AP-BSN and MM-BSN in each image set, as shown in Figure 8. Our MM-BSN with multi-mask strategy outperforms AP-BSN with only 'o' masks by a large magin, with PSNR improvements of up to 4, particularly in large-noise.\n\n\nMM-BSN in real-world sRGB image denosing\n\nThe proposed MM-BSN aims to denoise large-noise in sRGB images by combining multi-mask in the selfsupervised manner, while preserving the texture detail and controlling the model size. Table 4 quantitatively compares the denoising performance of several traditional algorithms, supervised denoising algorithms, unsupervised and self-supervised algorithms on SSID and DND benchmarks. The table shows that MM-BSN performs best in self-supervised methods and even outperforms some supervised algorithms. Furthermore, our MM-BSN does not require rawRGB images and noise estimation like R2R, nor real noisy-clean pairs like supervised models. Therefore, in practical applications, researchers can train MM-BSN directly on the noisy images from the target scene for denoising, avoiding degradation of the model performance when the scenario changes. Figure 9 qualitatively compares the visual denoising performance of state-of-the-art models on a random image in SIDD and DND benchmarks. Compared with its yellow box in the upper images, the lines area denoised by selfsupervised models in Figure 9d and Figure 9e are more smoothing, while there are unwanted but obvious burring effects near the lines denoised by other models shown in Figure 9a, Figure 9b and Figure 9c. MM-BSN performs better than most of the models, and can even compete with the supervised method of CBDNet [12] with slightly lower PSNR/SSIM. Comparing the yellow box in the lower images, our MM-BSN has a more clear boundary contour of outer boundary of the alphabets, and the noise on the alphabets themselves is more obviously reduced.\n\n\nConclusion\n\nIn this paper, we propose a multi-mask strategy worked on BSNs for self-supervised sRGB image denoising. Multimask can significantly break the large-noise structure, which previously cannot be efficiently handled by the onlycenter-masked models. In addition, we develop MM-BSN to effectively combine the features extracted by multimasked convolutional layers and control the model size to grow without explosion. In particular, the utilization of concatenation-based skip-connections can help to compensate for the loss of information caused by the masks. Extensive experiments prove that our method can effectively denoise the images with a large scale spatially correlated  Table 4. Quantitative comparison of different denoising models on SIDD and DND benchmarks. By default, we get the official evaluation results from SIDD and DND benchmark websites. indicates that we have retrained the model, uploaded the test results and received the results. R indicates that the result is reported by R2R [31]. * denotes the method with self-ensemble strategy [24]. \u2020 denotes the model trained with the same training and test data sets. The highest value is highlighted in bold for each type of denoising model.  noise and can preserve more textures, achieving a better denoising performance than other unsupervised and self-supervised methods in the literature. Our proposed MM-BSN is well suited for a real practical application scenario considering that it only needs noisy sRGB images to train.\n\n\ncorrelated noise shown by the black area in different shapes. (b) Proportion of different noise area on a full image tion preservation, especially when denoising large-noise.\n\nFigure 3 .\n3Multi-mask is shown on 5\u00d75 kernel. Gray dots represent 0, and blue dots represent 1. (a) is the central mask with a single blind spot and (f) is a ' '-shaped mask. (b) is a '-'-shaped mask and (g) is a '|'-shaped mask. (c) is a '+'-shaped mask and (h) is a ' '-shaped mask. (d) is a '/'-shaped mask and (i) is a '\\-shaped mask. (e) is a '\u00d7'-shaped mask and (j) is a ' '-shaped mask.\n\nFigure 4 .\n4MM-BSN Architecture. The channel of feature maps that are not marked is 128. N indicates that DCL repeats N times.\n\nFigure 5 .\n5) S pd =2, 'o' (c) S pd =5, 'o' (d) S pd =2, ' ' Visualization performance of several models with the same architecture but different mask type or different S pd . (a) Noisy image. (b)With a small stride factor S pd =2 and center mask, the method cannot remove noise from noisy image. (c) With S pd =5 and 'o'-shaped mask, the model can denoise better. (d) With S pd =2 and ' '-shaped mask, the model can get a better performance than that with 'o'-shaped mask.\n\nFigure 6 .\n6Qualitative comparison of several methods with different mask combinations on SIDD validation dataset. (a) Clean images. (b) Noisy images. (c) Denoised images by AP-BSN [22]. (d) Denoised images by SMM-BSN using a combination of 'o' mask and '+' mask. (e) Denoised images by SMM-BSN using a combination of 'o', ' ' and '+'-shaped mask.\n\nFigure 7 .\n7Visual comparison of AP-BSN and MM-BSN on the SIDD benchmark. They are trained on SIDD Medium dataset using the same center mask. (a) Noisy image. (b) Denoised result by AP-BSN [22]. (c) Denoised result by MM-BSN.\n\nFigure 8 .\n8Comparison of denoising results of AP-BSN and MM-BSN on the noisy sets of SIDD Validation.\n\nFigure 9 .\n9Qualitative comparison between different denoising methods on SIDD and DND benchmarks. (a) DnCNN is trained on the real paired SIDD Medium dataset. (b) C2N generates a realistic noisy image from the clean input, where the following denoising model, i.e., DIDN, is trained on the generated pairs. (c) CBDNet is trained in a supervised manner using noisy-clean pairs, where the noisy image is obtained by adding synthetic noise to the clean image. (d-e) The methods are trained directly on real sRGB images. Note that the DND benchmark (upper) provides some per-sample PSNR/SSIMs, while SIDD benchmark (lower) does not.\n\n\nBenchmarkS pd =2 \n'o' \n24.27/0.361 \n27.48/0.627 \n' ' \n35.29/0.854 \n36.84/0.932 \n\nTable 1. Quantitative comparison of the same network us-\ning different masks with S pd =2. PSNR/SSIM results are cal-\nculated between the denoised-clean pairs using the Python toolkit \nfor SIDD validation, and the official toolkit for SIDD benchmark. \n\n\n\nTable 1\n1quantitatively shows that, the PSNR/SSIM on the SIDD validation and benchmark\n\nA high-quality denoising dataset for smartphone cameras. Abdelrahman Abdelhamed, Stephen Lin, Michael S Brown, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionAbdelrahman Abdelhamed, Stephen Lin, and Michael S Brown. A high-quality denoising dataset for smartphone cameras. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition, pages 1692-1700, 2018.\n\nReal image denoising with feature attention. Saeed Anwar, Nick Barnes, Proceedings of the IEEE/CVF international conference on computer vision. the IEEE/CVF international conference on computer visionSaeed Anwar and Nick Barnes. Real image denoising with feature attention. In Proceedings of the IEEE/CVF inter- national conference on computer vision, pages 3155-3164, 2019.\n\nNoise2self: Blind denoising by self-supervision. Joshua Batson, Loic Royer, International Conference on Machine Learning. PMLRJoshua Batson and Loic Royer. Noise2self: Blind denoising by self-supervision. In International Conference on Machine Learning, pages 524-533. PMLR, 2019.\n\nUnprocessing images for learned raw denoising. Tim Brooks, Ben Mildenhall, Tianfan Xue, Jiawen Chen, Dillon Sharlet, Jonathan T Barron, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionTim Brooks, Ben Mildenhall, Tianfan Xue, Jiawen Chen, Dillon Sharlet, and Jonathan T Barron. Unprocessing images for learned raw denoising. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11036-11045, 2019.\n\nSpatialadaptive network for single image denoising. Meng Chang, Qi Li, Huajun Feng, Zhihai Xu, European Conference on Computer Vision. SpringerMeng Chang, Qi Li, Huajun Feng, and Zhihai Xu. Spatial- adaptive network for single image denoising. In European Conference on Computer Vision, pages 171-187. Springer, 2020.\n\nImage blind denoising with generative adversarial network based noise modeling. Jingwen Chen, Jiawei Chen, Hongyang Chao, Ming Yang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionJingwen Chen, Jiawei Chen, Hongyang Chao, and Ming Yang. Image blind denoising with generative adversarial net- work based noise modeling. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 3155-3164, 2018.\n\nSimple baselines for image restoration. Liangyu Chen, Xiaojie Chu, Xiangyu Zhang, Jian Sun, arXiv:2204.04676arXiv preprintLiangyu Chen, Xiaojie Chu, Xiangyu Zhang, and Jian Sun. Simple baselines for image restoration. arXiv preprint arXiv:2204.04676, 2022.\n\nImage denoising by sparse 3-d transformdomain collaborative filtering. Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, Karen Egiazarian, IEEE Transactions on image processing. 168Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and Karen Egiazarian. Image denoising by sparse 3-d transform- domain collaborative filtering. IEEE Transactions on image processing, 16(8):2080-2095, 2007.\n\nMultilevel edge features guided network for image denoising. Faming Fang, Juncheng Li, Yiting Yuan, Tieyong Zeng, Guixu Zhang, IEEE Transactions on Neural Networks and Learning Systems. 329Faming Fang, Juncheng Li, Yiting Yuan, Tieyong Zeng, and Guixu Zhang. Multilevel edge features guided network for image denoising. IEEE Transactions on Neural Networks and Learning Systems, 32(9):3956-3970, 2020.\n\nSelf-guided network for fast image denoising. Shuhang Gu, Yawei Li, Luc Van Gool, Radu Timofte, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionShuhang Gu, Yawei Li, Luc Van Gool, and Radu Timofte. Self-guided network for fast image denoising. In Proceed- ings of the IEEE/CVF International Conference on Com- puter Vision, pages 2511-2520, 2019.\n\nWeighted nuclear norm minimization with application to image denoising. Shuhang Gu, Lei Zhang, Wangmeng Zuo, Xiangchu Feng, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionShuhang Gu, Lei Zhang, Wangmeng Zuo, and Xiangchu Feng. Weighted nuclear norm minimization with applica- tion to image denoising. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 2862-2869, 2014.\n\nToward convolutional blind denoising of real photographs. Zifei Shi Guo, Kai Yan, Wangmeng Zhang, Lei Zuo, Zhang, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionShi Guo, Zifei Yan, Kai Zhang, Wangmeng Zuo, and Lei Zhang. Toward convolutional blind denoising of real pho- tographs. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1712-1722, 2019.\n\nEfficient blind-spot neural network architecture for image denoising. David Honz\u00e1tko, A Siavash, Engin Bigdeli, L Andrea T\u00fcretken, Dunbar, 2020 7th Swiss Conference on Data Science (SDS). IEEEDavid Honz\u00e1tko, Siavash A Bigdeli, Engin T\u00fcretken, and L Andrea Dunbar. Efficient blind-spot neural network archi- tecture for image denoising. In 2020 7th Swiss Conference on Data Science (SDS), pages 59-60. IEEE, 2020.\n\nPseudo 3d autocorrelation network for real image denoising. Xiaowan Hu, Ruijun Ma, Zhihong Liu, Yuanhao Cai, Xiaole Zhao, Yulun Zhang, Haoqian Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionXiaowan Hu, Ruijun Ma, Zhihong Liu, Yuanhao Cai, Xiaole Zhao, Yulun Zhang, and Haoqian Wang. Pseudo 3d auto- correlation network for real image denoising. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16175-16184, 2021.\n\nNeighbor2neighbor: Self-supervised denoising from single noisy images. Tao Huang, Songjiang Li, Xu Jia, Huchuan Lu, Jianzhuang Liu, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionTao Huang, Songjiang Li, Xu Jia, Huchuan Lu, and Jianzhuang Liu. Neighbor2neighbor: Self-supervised de- noising from single noisy images. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 14781-14790, 2021.\n\nC2n: Practical generative noise modeling for real-world denoising. Geonwoon Jang, Wooseok Lee, Sanghyun Son, Kyoung Mu Lee, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionGeonwoon Jang, Wooseok Lee, Sanghyun Son, and Ky- oung Mu Lee. C2n: Practical generative noise modeling for real-world denoising. In Proceedings of the IEEE/CVF Inter- national Conference on Computer Vision, pages 2350-2359, 2021.\n\nTransfer learning from synthetic to real-noise denoising with adaptive instance normalization. Yoonsik Kim, Jae Woong Soh, Yong Gu, Nam Ik Park, Cho, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionYoonsik Kim, Jae Woong Soh, Gu Yong Park, and Nam Ik Cho. Transfer learning from synthetic to real-noise denois- ing with adaptive instance normalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3482-3492, 2020.\n\nNoise2void-learning denoising from single noisy images. Alexander Krull, Tim-Oliver Buchholz, Florian Jug, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionAlexander Krull, Tim-Oliver Buchholz, and Florian Jug. Noise2void-learning denoising from single noisy images. In Proceedings of the IEEE/CVF conference on computer vi- sion and pattern recognition, pages 2129-2137, 2019.\n\nProbabilistic noise2void: Unsupervised content-aware denoising. Alexander Krull, Tom\u00e1\u0161 Vi\u010dar, Mangal Prakash, Manan Lalit, Florian Jug, Frontiers in Computer Science. 25Alexander Krull, Tom\u00e1\u0161 Vi\u010dar, Mangal Prakash, Manan Lalit, and Florian Jug. Probabilistic noise2void: Unsuper- vised content-aware denoising. Frontiers in Computer Sci- ence, 2:5, 2020.\n\nHigh-quality self-supervised deep image denoising. Samuli Laine, Tero Karras, Jaakko Lehtinen, Timo Aila, Advances in Neural Information Processing Systems. 32Samuli Laine, Tero Karras, Jaakko Lehtinen, and Timo Aila. High-quality self-supervised deep image denoising. Ad- vances in Neural Information Processing Systems, 32, 2019.\n\nImage denoising via deep residual convolutional neural networks. Signal, Image and Video Processing. Rushi Lan, Haizhang Zou, Cheng Pang, Yanru Zhong, Zhenbing Liu, Xiaonan Luo, 15Rushi Lan, Haizhang Zou, Cheng Pang, Yanru Zhong, Zhen- bing Liu, and Xiaonan Luo. Image denoising via deep resid- ual convolutional neural networks. Signal, Image and Video Processing, 15(1):1-8, 2021.\n\nApbsn: Self-supervised denoising for real-world images via asymmetric pd and blind-spot network. Wooseok Lee, Sanghyun Son, Kyoung Mu Lee, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionWooseok Lee, Sanghyun Son, and Kyoung Mu Lee. Ap- bsn: Self-supervised denoising for real-world images via asymmetric pd and blind-spot network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17725-17734, 2022.\n\nJaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, arXiv:1803.04189Tero Karras, Miika Aittala, and Timo Aila. Noise2noise: Learning image restoration without clean data. arXiv preprintJaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, and Timo Aila. Noise2noise: Learning image restoration without clean data. arXiv preprint arXiv:1803.04189, 2018.\n\nEnhanced deep residual networks for single image super-resolution. Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee, Proceedings of the IEEE conference on computer vision and pattern recognition workshops. the IEEE conference on computer vision and pattern recognition workshopsBee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep residual networks for single image super-resolution. In Proceedings of the IEEE confer- ence on computer vision and pattern recognition workshops, pages 136-144, 2017.\n\nMulti-level wavelet convolutional neural networks. Pengju Liu, Hongzhi Zhang, Wei Lian, Wangmeng Zuo, IEEE Access. 7Pengju Liu, Hongzhi Zhang, Wei Lian, and Wangmeng Zuo. Multi-level wavelet convolutional neural networks. IEEE Ac- cess, 7:74973-74985, 2019.\n\nMulti-level wavelet-cnn for image restoration. Pengju Liu, Hongzhi Zhang, Kai Zhang, Liang Lin, Wangmeng Zuo, Proceedings of the IEEE conference on computer vision and pattern recognition workshops. the IEEE conference on computer vision and pattern recognition workshopsPengju Liu, Hongzhi Zhang, Kai Zhang, Liang Lin, and Wangmeng Zuo. Multi-level wavelet-cnn for image restora- tion. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 773-782, 2018.\n\nAdaptive image denoising by targeted databases. Enming Luo, H Stanley, Chan, Truong Q Nguyen, IEEE transactions on image processing. 247Enming Luo, Stanley H Chan, and Truong Q Nguyen. Adap- tive image denoising by targeted databases. IEEE transac- tions on image processing, 24(7):2167-2181, 2015.\n\nBurst denoising with kernel prediction networks. Ben Mildenhall, Jonathan T Barron, Jiawen Chen, Dillon Sharlet, Ren Ng, Robert Carroll, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionBen Mildenhall, Jonathan T Barron, Jiawen Chen, Dillon Sharlet, Ren Ng, and Robert Carroll. Burst denoising with kernel prediction networks. In Proceedings of the IEEE con- ference on computer vision and pattern recognition, pages 2502-2510, 2018.\n\nNoisier2noise: Learning to denoise from unpaired noisy data. Nick Moran, Dan Schmidt, Yu Zhong, Patrick Coady, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionNick Moran, Dan Schmidt, Yu Zhong, and Patrick Coady. Noisier2noise: Learning to denoise from unpaired noisy data. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition, pages 12064-12072, 2020.\n\nCvf-sid: Cyclic multi-variate function for self-supervised image denoising by disentangling noise from image. Reyhaneh Neshatavar, Mohsen Yavartanoo, Sanghyun Son, Kyoung Mu Lee, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionReyhaneh Neshatavar, Mohsen Yavartanoo, Sanghyun Son, and Kyoung Mu Lee. Cvf-sid: Cyclic multi-variate function for self-supervised image denoising by disentangling noise from image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17583- 17591, 2022.\n\nRecorrupted-to-recorrupted: unsupervised deep learning for image denoising. Tongyao Pang, Huan Zheng, Yuhui Quan, Hui Ji, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. the IEEE/CVF conference on computer vision and pattern recognitionTongyao Pang, Huan Zheng, Yuhui Quan, and Hui Ji. Recorrupted-to-recorrupted: unsupervised deep learning for image denoising. In Proceedings of the IEEE/CVF con- ference on computer vision and pattern recognition, pages 2043-2052, 2021.\n\nDensely connected hierarchical network for image denoising. Bumjun Park, Songhyun Yu, Jechang Jeong, Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops. the IEEE/CVF conference on computer vision and pattern recognition workshopsBumjun Park, Songhyun Yu, and Jechang Jeong. Densely connected hierarchical network for image denoising. In Pro- ceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 0-0, 2019.\n\nBenchmarking denoising algorithms with real photographs. Tobias Plotz, Stefan Roth, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionTobias Plotz and Stefan Roth. Benchmarking denoising algorithms with real photographs. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 1586-1595, 2017.\n\nLearning medical image denoising with deep dynamic residual attention network. Rizwan Ali Sma Sharif, Mithun Naqvi, Biswas, Mathematics. 8122192SMA Sharif, Rizwan Ali Naqvi, and Mithun Biswas. Learn- ing medical image denoising with deep dynamic residual at- tention network. Mathematics, 8(12):2192, 2020.\n\nBlind2unblind: Self-supervised image denoising with visible blind spots. Zejin Wang, Jiazheng Liu, Guoqing Li, Hua Han, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionZejin Wang, Jiazheng Liu, Guoqing Li, and Hua Han. Blind2unblind: Self-supervised image denoising with visi- ble blind spots. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2027- 2036, 2022.\n\nSoftware hint-driven data management for hybrid memory in mobile systems. Fei Wen, Mian Qin, Paul Gratz, Narasimha Reddy, ACM Transactions on Embedded Computing Systems (TECS). 211Fei Wen, Mian Qin, Paul Gratz, and Narasimha Reddy. Soft- ware hint-driven data management for hybrid memory in mo- bile systems. ACM Transactions on Embedded Computing Systems (TECS), 21(1):1-18, 2022.\n\nUnpaired learning of deep image denoising. Xiaohe Wu, Ming Liu, Yue Cao, Dongwei Ren, Wangmeng Zuo, European conference on computer vision. SpringerXiaohe Wu, Ming Liu, Yue Cao, Dongwei Ren, and Wang- meng Zuo. Unpaired learning of deep image denoising. In European conference on computer vision, pages 352-368. Springer, 2020.\n\nNoisy-as-clean: Learning selfsupervised denoising from corrupted image. Jun Xu, Yuan Huang, Ming-Ming Cheng, Li Liu, Fan Zhu, Zhou Xu, Ling Shao, IEEE Transactions on Image Processing. 29Jun Xu, Yuan Huang, Ming-Ming Cheng, Li Liu, Fan Zhu, Zhou Xu, and Ling Shao. Noisy-as-clean: Learning self- supervised denoising from corrupted image. IEEE Transac- tions on Image Processing, 29:9316-9329, 2020.\n\nDeep iterative down-up cnn for image denoising. Songhyun Yu, Bumjun Park, Jechang Jeong, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. the IEEE/CVF Conference on Computer Vision and Pattern Recognition WorkshopsSonghyun Yu, Bumjun Park, and Jechang Jeong. Deep iter- ative down-up cnn for image denoising. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 0-0, 2019.\n\nVariational denoising network: Toward blind noise modeling and removal. Zongsheng Yue, Hongwei Yong, Qian Zhao, Deyu Meng, Lei Zhang, Advances in neural information processing systems. 32Zongsheng Yue, Hongwei Yong, Qian Zhao, Deyu Meng, and Lei Zhang. Variational denoising network: Toward blind noise modeling and removal. Advances in neural information processing systems, 32, 2019.\n\nDual adversarial network: Toward real-world noise removal and noise generation. Zongsheng Yue, Qian Zhao, Lei Zhang, Deyu Meng, European Conference on Computer Vision. SpringerZongsheng Yue, Qian Zhao, Lei Zhang, and Deyu Meng. Dual adversarial network: Toward real-world noise removal and noise generation. In European Conference on Computer Vision, pages 41-58. Springer, 2020.\n\nSelf-supervised image denoising for real-world images with context-aware transformer. Dan Zhang, Fangfang Zhou, IEEE AccessDan Zhang and Fangfang Zhou. Self-supervised image denoising for real-world images with context-aware trans- former. IEEE Access, 2023.\n\nBeyond a gaussian denoiser: Residual learning of deep cnn for image denoising. Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, Lei Zhang, IEEE transactions on image processing. 267Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. IEEE transactions on image processing, 26(7):3142-3155, 2017.\n\nFfdnet: Toward a fast and flexible solution for cnn-based image denoising. Kai Zhang, Wangmeng Zuo, Lei Zhang, IEEE Transactions on Image Processing. 279Kai Zhang, Wangmeng Zuo, and Lei Zhang. Ffdnet: Toward a fast and flexible solution for cnn-based image denoising. IEEE Transactions on Image Processing, 27(9):4608-4622, 2018.\n\nIdr: Self-supervised image denoising via iterative data refinement. Yi Zhang, Dasong Li, Ka Lung Law, Xiaogang Wang, Hongwei Qin, Hongsheng Li, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionYi Zhang, Dasong Li, Ka Lung Law, Xiaogang Wang, Hong- wei Qin, and Hongsheng Li. Idr: Self-supervised image denoising via iterative data refinement. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2098-2107, 2022.\n", "annotations": {"author": "[{\"end\":146,\"start\":102},{\"end\":195,\"start\":147},{\"end\":266,\"start\":196},{\"end\":312,\"start\":267}]", "publisher": null, "author_last_name": "[{\"end\":111,\"start\":106},{\"end\":160,\"start\":156},{\"end\":207,\"start\":202},{\"end\":279,\"start\":277}]", "author_first_name": "[{\"end\":105,\"start\":102},{\"end\":155,\"start\":147},{\"end\":201,\"start\":196},{\"end\":276,\"start\":267}]", "author_affiliation": "[{\"end\":145,\"start\":113},{\"end\":194,\"start\":162},{\"end\":265,\"start\":233},{\"end\":311,\"start\":281}]", "title": "[{\"end\":99,\"start\":1},{\"end\":411,\"start\":313}]", "venue": null, "abstract": "[{\"end\":1813,\"start\":413}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2016,\"start\":2013},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2019,\"start\":2016},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2022,\"start\":2019},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2152,\"start\":2148},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2178,\"start\":2174},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2188,\"start\":2184},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2215,\"start\":2211},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2441,\"start\":2438},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2470,\"start\":2467},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2489,\"start\":2485},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2652,\"start\":2649},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3037,\"start\":3034},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3039,\"start\":3037},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3041,\"start\":3039},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3044,\"start\":3041},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":3047,\"start\":3044},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3050,\"start\":3047},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3053,\"start\":3050},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3550,\"start\":3546},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3553,\"start\":3550},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3556,\"start\":3553},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3559,\"start\":3556},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3562,\"start\":3559},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3565,\"start\":3562},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3946,\"start\":3943},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3949,\"start\":3946},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3952,\"start\":3949},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3955,\"start\":3952},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":3958,\"start\":3955},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3961,\"start\":3958},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4042,\"start\":4038},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4269,\"start\":4265},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":4282,\"start\":4278},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":4635,\"start\":4631},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5015,\"start\":5011},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5266,\"start\":5262},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5269,\"start\":5266},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5272,\"start\":5269},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5275,\"start\":5272},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5447,\"start\":5444},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5450,\"start\":5447},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5453,\"start\":5450},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5456,\"start\":5453},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5459,\"start\":5456},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5462,\"start\":5459},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5640,\"start\":5636},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":5915,\"start\":5911},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7576,\"start\":7572},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7822,\"start\":7819},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7824,\"start\":7822},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7826,\"start\":7824},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7829,\"start\":7826},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7832,\"start\":7829},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7835,\"start\":7832},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":7838,\"start\":7835},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7841,\"start\":7838},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8064,\"start\":8061},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8067,\"start\":8064},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8766,\"start\":8763},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8769,\"start\":8766},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8772,\"start\":8769},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8775,\"start\":8772},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9091,\"start\":9087},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9329,\"start\":9325},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9345,\"start\":9342},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9376,\"start\":9372},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9400,\"start\":9396},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":9410,\"start\":9406},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9424,\"start\":9420},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9444,\"start\":9440},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9460,\"start\":9456},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9615,\"start\":9611},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9898,\"start\":9894},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":10137,\"start\":10133},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10488,\"start\":10484},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10868,\"start\":10864},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11020,\"start\":11016},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11042,\"start\":11038},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11417,\"start\":11413},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11862,\"start\":11858},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12946,\"start\":12943},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12949,\"start\":12946},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12952,\"start\":12949},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":12955,\"start\":12952},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12958,\"start\":12955},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12990,\"start\":12986},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12993,\"start\":12990},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12996,\"start\":12993},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":16455,\"start\":16451},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18317,\"start\":18313},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":18731,\"start\":18728},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":18744,\"start\":18740},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":19878,\"start\":19874},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":20718,\"start\":20714},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":21030,\"start\":21026},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":21625,\"start\":21622},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":23863,\"start\":23859},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":26427,\"start\":26423},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":27672,\"start\":27668},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":27727,\"start\":27723}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28338,\"start\":28162},{\"attributes\":{\"id\":\"fig_1\"},\"end\":28734,\"start\":28339},{\"attributes\":{\"id\":\"fig_2\"},\"end\":28862,\"start\":28735},{\"attributes\":{\"id\":\"fig_3\"},\"end\":29337,\"start\":28863},{\"attributes\":{\"id\":\"fig_4\"},\"end\":29686,\"start\":29338},{\"attributes\":{\"id\":\"fig_5\"},\"end\":29913,\"start\":29687},{\"attributes\":{\"id\":\"fig_6\"},\"end\":30017,\"start\":29914},{\"attributes\":{\"id\":\"fig_8\"},\"end\":30648,\"start\":30018},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":30985,\"start\":30649},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":31073,\"start\":30986}]", "paragraph": "[{\"end\":2808,\"start\":1829},{\"end\":2999,\"start\":2810},{\"end\":3877,\"start\":3001},{\"end\":5916,\"start\":3879},{\"end\":6771,\"start\":5918},{\"end\":7142,\"start\":6773},{\"end\":7344,\"start\":7144},{\"end\":7514,\"start\":7346},{\"end\":9040,\"start\":7531},{\"end\":10854,\"start\":9042},{\"end\":12331,\"start\":10856},{\"end\":12880,\"start\":12346},{\"end\":13490,\"start\":12882},{\"end\":13790,\"start\":13506},{\"end\":15579,\"start\":13870},{\"end\":16388,\"start\":15581},{\"end\":16634,\"start\":16390},{\"end\":17759,\"start\":16636},{\"end\":18072,\"start\":17761},{\"end\":18146,\"start\":18074},{\"end\":18641,\"start\":18201},{\"end\":19209,\"start\":18682},{\"end\":19881,\"start\":19211},{\"end\":20398,\"start\":19950},{\"end\":21312,\"start\":20400},{\"end\":22231,\"start\":21314},{\"end\":22858,\"start\":22233},{\"end\":23690,\"start\":22868},{\"end\":24682,\"start\":23750},{\"end\":25006,\"start\":24684},{\"end\":26654,\"start\":25051},{\"end\":28161,\"start\":26669}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13844,\"start\":13791},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13869,\"start\":13844},{\"attributes\":{\"id\":\"formula_2\"},\"end\":18167,\"start\":18147},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18200,\"start\":18167},{\"attributes\":{\"id\":\"formula_4\"},\"end\":23712,\"start\":23691}]", "table_ref": "[{\"end\":21402,\"start\":21395},{\"end\":24032,\"start\":24025},{\"end\":25243,\"start\":25236},{\"end\":27352,\"start\":27345}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1827,\"start\":1815},{\"attributes\":{\"n\":\"2.\"},\"end\":7529,\"start\":7517},{\"attributes\":{\"n\":\"3.\"},\"end\":12344,\"start\":12334},{\"attributes\":{\"n\":\"4.\"},\"end\":13504,\"start\":13493},{\"attributes\":{\"n\":\"5.\"},\"end\":18655,\"start\":18644},{\"attributes\":{\"n\":\"5.1.\"},\"end\":18680,\"start\":18658},{\"end\":19909,\"start\":19884},{\"attributes\":{\"n\":\"5.2.\"},\"end\":19948,\"start\":19912},{\"end\":22866,\"start\":22861},{\"attributes\":{\"n\":\"5.3.\"},\"end\":23748,\"start\":23714},{\"attributes\":{\"n\":\"5.4.\"},\"end\":25049,\"start\":25009},{\"attributes\":{\"n\":\"6.\"},\"end\":26667,\"start\":26657},{\"end\":28350,\"start\":28340},{\"end\":28746,\"start\":28736},{\"end\":28874,\"start\":28864},{\"end\":29349,\"start\":29339},{\"end\":29698,\"start\":29688},{\"end\":29925,\"start\":29915},{\"end\":30029,\"start\":30019},{\"end\":30994,\"start\":30987}]", "table": "[{\"end\":30985,\"start\":30660}]", "figure_caption": "[{\"end\":28338,\"start\":28164},{\"end\":28734,\"start\":28352},{\"end\":28862,\"start\":28748},{\"end\":29337,\"start\":28876},{\"end\":29686,\"start\":29351},{\"end\":29913,\"start\":29700},{\"end\":30017,\"start\":29927},{\"end\":30648,\"start\":30031},{\"end\":30660,\"start\":30651},{\"end\":31073,\"start\":30996}]", "figure_ref": "[{\"end\":2271,\"start\":2263},{\"end\":12437,\"start\":12429},{\"end\":12466,\"start\":12457},{\"end\":12663,\"start\":12654},{\"end\":12758,\"start\":12749},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13690,\"start\":13682},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16683,\"start\":16675},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20600,\"start\":20592},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20902,\"start\":20893},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21052,\"start\":21043},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":21287,\"start\":21279},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22294,\"start\":22286},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22458,\"start\":22449},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22945,\"start\":22936},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":22959,\"start\":22950},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":23415,\"start\":23406},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":24334,\"start\":24325},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":24455,\"start\":24446},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":24848,\"start\":24840},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":25903,\"start\":25895},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":26144,\"start\":26135},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":26158,\"start\":26149},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":26290,\"start\":26281},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":26301,\"start\":26292},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":26315,\"start\":26306}]", "bib_author_first_name": "[{\"end\":31143,\"start\":31132},{\"end\":31163,\"start\":31156},{\"end\":31178,\"start\":31169},{\"end\":31600,\"start\":31595},{\"end\":31612,\"start\":31608},{\"end\":31981,\"start\":31975},{\"end\":31994,\"start\":31990},{\"end\":32258,\"start\":32255},{\"end\":32270,\"start\":32267},{\"end\":32290,\"start\":32283},{\"end\":32302,\"start\":32296},{\"end\":32315,\"start\":32309},{\"end\":32333,\"start\":32325},{\"end\":32335,\"start\":32334},{\"end\":32801,\"start\":32797},{\"end\":32811,\"start\":32809},{\"end\":32822,\"start\":32816},{\"end\":32835,\"start\":32829},{\"end\":33151,\"start\":33144},{\"end\":33164,\"start\":33158},{\"end\":33179,\"start\":33171},{\"end\":33190,\"start\":33186},{\"end\":33632,\"start\":33625},{\"end\":33646,\"start\":33639},{\"end\":33659,\"start\":33652},{\"end\":33671,\"start\":33667},{\"end\":33922,\"start\":33914},{\"end\":33940,\"start\":33930},{\"end\":33954,\"start\":33946},{\"end\":33971,\"start\":33966},{\"end\":34303,\"start\":34297},{\"end\":34318,\"start\":34310},{\"end\":34329,\"start\":34323},{\"end\":34343,\"start\":34336},{\"end\":34355,\"start\":34350},{\"end\":34692,\"start\":34685},{\"end\":34702,\"start\":34697},{\"end\":34710,\"start\":34707},{\"end\":34725,\"start\":34721},{\"end\":35147,\"start\":35140},{\"end\":35155,\"start\":35152},{\"end\":35171,\"start\":35163},{\"end\":35185,\"start\":35177},{\"end\":35634,\"start\":35629},{\"end\":35647,\"start\":35644},{\"end\":35661,\"start\":35653},{\"end\":35672,\"start\":35669},{\"end\":36139,\"start\":36134},{\"end\":36151,\"start\":36150},{\"end\":36166,\"start\":36161},{\"end\":36184,\"start\":36176},{\"end\":36545,\"start\":36538},{\"end\":36556,\"start\":36550},{\"end\":36568,\"start\":36561},{\"end\":36581,\"start\":36574},{\"end\":36593,\"start\":36587},{\"end\":36605,\"start\":36600},{\"end\":36620,\"start\":36613},{\"end\":37119,\"start\":37116},{\"end\":37136,\"start\":37127},{\"end\":37143,\"start\":37141},{\"end\":37156,\"start\":37149},{\"end\":37171,\"start\":37161},{\"end\":37651,\"start\":37643},{\"end\":37665,\"start\":37658},{\"end\":37679,\"start\":37671},{\"end\":37694,\"start\":37685},{\"end\":38163,\"start\":38156},{\"end\":38172,\"start\":38169},{\"end\":38178,\"start\":38173},{\"end\":38188,\"start\":38184},{\"end\":38196,\"start\":38193},{\"end\":38199,\"start\":38197},{\"end\":38690,\"start\":38681},{\"end\":38708,\"start\":38698},{\"end\":38726,\"start\":38719},{\"end\":39177,\"start\":39168},{\"end\":39190,\"start\":39185},{\"end\":39204,\"start\":39198},{\"end\":39219,\"start\":39214},{\"end\":39234,\"start\":39227},{\"end\":39517,\"start\":39511},{\"end\":39529,\"start\":39525},{\"end\":39544,\"start\":39538},{\"end\":39559,\"start\":39555},{\"end\":39899,\"start\":39894},{\"end\":39913,\"start\":39905},{\"end\":39924,\"start\":39919},{\"end\":39936,\"start\":39931},{\"end\":39952,\"start\":39944},{\"end\":39965,\"start\":39958},{\"end\":40281,\"start\":40274},{\"end\":40295,\"start\":40287},{\"end\":40310,\"start\":40301},{\"end\":40728,\"start\":40722},{\"end\":40744,\"start\":40739},{\"end\":40758,\"start\":40755},{\"end\":40777,\"start\":40771},{\"end\":41194,\"start\":41191},{\"end\":41208,\"start\":41200},{\"end\":41220,\"start\":41214},{\"end\":41234,\"start\":41226},{\"end\":41249,\"start\":41240},{\"end\":41724,\"start\":41718},{\"end\":41737,\"start\":41730},{\"end\":41748,\"start\":41745},{\"end\":41763,\"start\":41755},{\"end\":41979,\"start\":41973},{\"end\":41992,\"start\":41985},{\"end\":42003,\"start\":42000},{\"end\":42016,\"start\":42011},{\"end\":42030,\"start\":42022},{\"end\":42481,\"start\":42475},{\"end\":42488,\"start\":42487},{\"end\":42779,\"start\":42776},{\"end\":42800,\"start\":42792},{\"end\":42802,\"start\":42801},{\"end\":42817,\"start\":42811},{\"end\":42830,\"start\":42824},{\"end\":42843,\"start\":42840},{\"end\":42854,\"start\":42848},{\"end\":43319,\"start\":43315},{\"end\":43330,\"start\":43327},{\"end\":43342,\"start\":43340},{\"end\":43357,\"start\":43350},{\"end\":43861,\"start\":43853},{\"end\":43880,\"start\":43874},{\"end\":43901,\"start\":43893},{\"end\":43916,\"start\":43907},{\"end\":44450,\"start\":44443},{\"end\":44461,\"start\":44457},{\"end\":44474,\"start\":44469},{\"end\":44484,\"start\":44481},{\"end\":44942,\"start\":44936},{\"end\":44957,\"start\":44949},{\"end\":44969,\"start\":44962},{\"end\":45430,\"start\":45424},{\"end\":45444,\"start\":45438},{\"end\":45872,\"start\":45866},{\"end\":45876,\"start\":45873},{\"end\":45895,\"start\":45889},{\"end\":46173,\"start\":46168},{\"end\":46188,\"start\":46180},{\"end\":46201,\"start\":46194},{\"end\":46209,\"start\":46206},{\"end\":46678,\"start\":46675},{\"end\":46688,\"start\":46684},{\"end\":46698,\"start\":46694},{\"end\":46715,\"start\":46706},{\"end\":47034,\"start\":47028},{\"end\":47043,\"start\":47039},{\"end\":47052,\"start\":47049},{\"end\":47065,\"start\":47058},{\"end\":47079,\"start\":47071},{\"end\":47389,\"start\":47386},{\"end\":47398,\"start\":47394},{\"end\":47415,\"start\":47406},{\"end\":47425,\"start\":47423},{\"end\":47434,\"start\":47431},{\"end\":47444,\"start\":47440},{\"end\":47453,\"start\":47449},{\"end\":47771,\"start\":47763},{\"end\":47782,\"start\":47776},{\"end\":47796,\"start\":47789},{\"end\":48263,\"start\":48254},{\"end\":48276,\"start\":48269},{\"end\":48287,\"start\":48283},{\"end\":48298,\"start\":48294},{\"end\":48308,\"start\":48305},{\"end\":48658,\"start\":48649},{\"end\":48668,\"start\":48664},{\"end\":48678,\"start\":48675},{\"end\":48690,\"start\":48686},{\"end\":49039,\"start\":49036},{\"end\":49055,\"start\":49047},{\"end\":49292,\"start\":49289},{\"end\":49308,\"start\":49300},{\"end\":49320,\"start\":49314},{\"end\":49331,\"start\":49327},{\"end\":49341,\"start\":49338},{\"end\":49675,\"start\":49672},{\"end\":49691,\"start\":49683},{\"end\":49700,\"start\":49697},{\"end\":49998,\"start\":49996},{\"end\":50012,\"start\":50006},{\"end\":50019,\"start\":50017},{\"end\":50024,\"start\":50020},{\"end\":50038,\"start\":50030},{\"end\":50052,\"start\":50045},{\"end\":50067,\"start\":50058}]", "bib_author_last_name": "[{\"end\":31154,\"start\":31144},{\"end\":31167,\"start\":31164},{\"end\":31184,\"start\":31179},{\"end\":31606,\"start\":31601},{\"end\":31619,\"start\":31613},{\"end\":31988,\"start\":31982},{\"end\":32000,\"start\":31995},{\"end\":32265,\"start\":32259},{\"end\":32281,\"start\":32271},{\"end\":32294,\"start\":32291},{\"end\":32307,\"start\":32303},{\"end\":32323,\"start\":32316},{\"end\":32342,\"start\":32336},{\"end\":32807,\"start\":32802},{\"end\":32814,\"start\":32812},{\"end\":32827,\"start\":32823},{\"end\":32838,\"start\":32836},{\"end\":33156,\"start\":33152},{\"end\":33169,\"start\":33165},{\"end\":33184,\"start\":33180},{\"end\":33195,\"start\":33191},{\"end\":33637,\"start\":33633},{\"end\":33650,\"start\":33647},{\"end\":33665,\"start\":33660},{\"end\":33675,\"start\":33672},{\"end\":33928,\"start\":33923},{\"end\":33944,\"start\":33941},{\"end\":33964,\"start\":33955},{\"end\":33982,\"start\":33972},{\"end\":34308,\"start\":34304},{\"end\":34321,\"start\":34319},{\"end\":34334,\"start\":34330},{\"end\":34348,\"start\":34344},{\"end\":34361,\"start\":34356},{\"end\":34695,\"start\":34693},{\"end\":34705,\"start\":34703},{\"end\":34719,\"start\":34711},{\"end\":34733,\"start\":34726},{\"end\":35150,\"start\":35148},{\"end\":35161,\"start\":35156},{\"end\":35175,\"start\":35172},{\"end\":35190,\"start\":35186},{\"end\":35642,\"start\":35635},{\"end\":35651,\"start\":35648},{\"end\":35667,\"start\":35662},{\"end\":35676,\"start\":35673},{\"end\":35683,\"start\":35678},{\"end\":36148,\"start\":36140},{\"end\":36159,\"start\":36152},{\"end\":36174,\"start\":36167},{\"end\":36193,\"start\":36185},{\"end\":36201,\"start\":36195},{\"end\":36548,\"start\":36546},{\"end\":36559,\"start\":36557},{\"end\":36572,\"start\":36569},{\"end\":36585,\"start\":36582},{\"end\":36598,\"start\":36594},{\"end\":36611,\"start\":36606},{\"end\":36625,\"start\":36621},{\"end\":37125,\"start\":37120},{\"end\":37139,\"start\":37137},{\"end\":37147,\"start\":37144},{\"end\":37159,\"start\":37157},{\"end\":37175,\"start\":37172},{\"end\":37656,\"start\":37652},{\"end\":37669,\"start\":37666},{\"end\":37683,\"start\":37680},{\"end\":37698,\"start\":37695},{\"end\":38167,\"start\":38164},{\"end\":38182,\"start\":38179},{\"end\":38191,\"start\":38189},{\"end\":38204,\"start\":38200},{\"end\":38209,\"start\":38206},{\"end\":38696,\"start\":38691},{\"end\":38717,\"start\":38709},{\"end\":38730,\"start\":38727},{\"end\":39183,\"start\":39178},{\"end\":39196,\"start\":39191},{\"end\":39212,\"start\":39205},{\"end\":39225,\"start\":39220},{\"end\":39238,\"start\":39235},{\"end\":39523,\"start\":39518},{\"end\":39536,\"start\":39530},{\"end\":39553,\"start\":39545},{\"end\":39564,\"start\":39560},{\"end\":39903,\"start\":39900},{\"end\":39917,\"start\":39914},{\"end\":39929,\"start\":39925},{\"end\":39942,\"start\":39937},{\"end\":39956,\"start\":39953},{\"end\":39969,\"start\":39966},{\"end\":40285,\"start\":40282},{\"end\":40299,\"start\":40296},{\"end\":40314,\"start\":40311},{\"end\":40737,\"start\":40729},{\"end\":40753,\"start\":40745},{\"end\":40769,\"start\":40759},{\"end\":40783,\"start\":40778},{\"end\":41198,\"start\":41195},{\"end\":41212,\"start\":41209},{\"end\":41224,\"start\":41221},{\"end\":41238,\"start\":41235},{\"end\":41253,\"start\":41250},{\"end\":41728,\"start\":41725},{\"end\":41743,\"start\":41738},{\"end\":41753,\"start\":41749},{\"end\":41767,\"start\":41764},{\"end\":41983,\"start\":41980},{\"end\":41998,\"start\":41993},{\"end\":42009,\"start\":42004},{\"end\":42020,\"start\":42017},{\"end\":42034,\"start\":42031},{\"end\":42485,\"start\":42482},{\"end\":42496,\"start\":42489},{\"end\":42502,\"start\":42498},{\"end\":42519,\"start\":42504},{\"end\":42790,\"start\":42780},{\"end\":42809,\"start\":42803},{\"end\":42822,\"start\":42818},{\"end\":42838,\"start\":42831},{\"end\":42846,\"start\":42844},{\"end\":42862,\"start\":42855},{\"end\":43325,\"start\":43320},{\"end\":43338,\"start\":43331},{\"end\":43348,\"start\":43343},{\"end\":43363,\"start\":43358},{\"end\":43872,\"start\":43862},{\"end\":43891,\"start\":43881},{\"end\":43905,\"start\":43902},{\"end\":43920,\"start\":43917},{\"end\":44455,\"start\":44451},{\"end\":44467,\"start\":44462},{\"end\":44479,\"start\":44475},{\"end\":44487,\"start\":44485},{\"end\":44947,\"start\":44943},{\"end\":44960,\"start\":44958},{\"end\":44975,\"start\":44970},{\"end\":45436,\"start\":45431},{\"end\":45449,\"start\":45445},{\"end\":45887,\"start\":45877},{\"end\":45901,\"start\":45896},{\"end\":45909,\"start\":45903},{\"end\":46178,\"start\":46174},{\"end\":46192,\"start\":46189},{\"end\":46204,\"start\":46202},{\"end\":46213,\"start\":46210},{\"end\":46682,\"start\":46679},{\"end\":46692,\"start\":46689},{\"end\":46704,\"start\":46699},{\"end\":46721,\"start\":46716},{\"end\":47037,\"start\":47035},{\"end\":47047,\"start\":47044},{\"end\":47056,\"start\":47053},{\"end\":47069,\"start\":47066},{\"end\":47083,\"start\":47080},{\"end\":47392,\"start\":47390},{\"end\":47404,\"start\":47399},{\"end\":47421,\"start\":47416},{\"end\":47429,\"start\":47426},{\"end\":47438,\"start\":47435},{\"end\":47447,\"start\":47445},{\"end\":47458,\"start\":47454},{\"end\":47774,\"start\":47772},{\"end\":47787,\"start\":47783},{\"end\":47802,\"start\":47797},{\"end\":48267,\"start\":48264},{\"end\":48281,\"start\":48277},{\"end\":48292,\"start\":48288},{\"end\":48303,\"start\":48299},{\"end\":48314,\"start\":48309},{\"end\":48662,\"start\":48659},{\"end\":48673,\"start\":48669},{\"end\":48684,\"start\":48679},{\"end\":48695,\"start\":48691},{\"end\":49045,\"start\":49040},{\"end\":49060,\"start\":49056},{\"end\":49298,\"start\":49293},{\"end\":49312,\"start\":49309},{\"end\":49325,\"start\":49321},{\"end\":49336,\"start\":49332},{\"end\":49347,\"start\":49342},{\"end\":49681,\"start\":49676},{\"end\":49695,\"start\":49692},{\"end\":49706,\"start\":49701},{\"end\":50004,\"start\":49999},{\"end\":50015,\"start\":50013},{\"end\":50028,\"start\":50025},{\"end\":50043,\"start\":50039},{\"end\":50056,\"start\":50053},{\"end\":50070,\"start\":50068}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":52059988},\"end\":31548,\"start\":31075},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":118713138},\"end\":31924,\"start\":31550},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":59523708},\"end\":32206,\"start\":31926},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":53770387},\"end\":32743,\"start\":32208},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":210932492},\"end\":33062,\"start\":32745},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":51989956},\"end\":33583,\"start\":33064},{\"attributes\":{\"doi\":\"arXiv:2204.04676\",\"id\":\"b6\"},\"end\":33841,\"start\":33585},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1475121},\"end\":34234,\"start\":33843},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":221345469},\"end\":34637,\"start\":34236},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":204955838},\"end\":35066,\"start\":34639},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1663191},\"end\":35569,\"start\":35068},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":49672261},\"end\":36062,\"start\":35571},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":220734100},\"end\":36476,\"start\":36064},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":235679062},\"end\":37043,\"start\":36478},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":231419143},\"end\":37574,\"start\":37045},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":244426949},\"end\":38059,\"start\":37576},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":211506288},\"end\":38623,\"start\":38061},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":53751136},\"end\":39102,\"start\":38625},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":173990717},\"end\":39458,\"start\":39104},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":173990648},\"end\":39791,\"start\":39460},{\"attributes\":{\"id\":\"b20\"},\"end\":40175,\"start\":39793},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":247596985},\"end\":40720,\"start\":40177},{\"attributes\":{\"doi\":\"arXiv:1803.04189\",\"id\":\"b22\"},\"end\":41122,\"start\":40722},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":6540453},\"end\":41665,\"start\":41124},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":195225324},\"end\":41924,\"start\":41667},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":29151865},\"end\":42425,\"start\":41926},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":16386052},\"end\":42725,\"start\":42427},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":4381469},\"end\":43252,\"start\":42727},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":204904999},\"end\":43741,\"start\":43254},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":247627958},\"end\":44365,\"start\":43743},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":235719899},\"end\":44874,\"start\":44367},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":198922180},\"end\":45365,\"start\":44876},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":9715523},\"end\":45785,\"start\":45367},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":230533248},\"end\":46093,\"start\":45787},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":247447122},\"end\":46599,\"start\":46095},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":245992749},\"end\":46983,\"start\":46601},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":221376798},\"end\":47312,\"start\":46985},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":222154570},\"end\":47713,\"start\":47314},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":198166487},\"end\":48180,\"start\":47715},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":201667906},\"end\":48567,\"start\":48182},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":220495900},\"end\":48948,\"start\":48569},{\"attributes\":{\"id\":\"b41\"},\"end\":49208,\"start\":48950},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":996788},\"end\":49595,\"start\":49210},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":10514149},\"end\":49926,\"start\":49597},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":244714180},\"end\":50479,\"start\":49928}]", "bib_title": "[{\"end\":31130,\"start\":31075},{\"end\":31593,\"start\":31550},{\"end\":31973,\"start\":31926},{\"end\":32253,\"start\":32208},{\"end\":32795,\"start\":32745},{\"end\":33142,\"start\":33064},{\"end\":33912,\"start\":33843},{\"end\":34295,\"start\":34236},{\"end\":34683,\"start\":34639},{\"end\":35138,\"start\":35068},{\"end\":35627,\"start\":35571},{\"end\":36132,\"start\":36064},{\"end\":36536,\"start\":36478},{\"end\":37114,\"start\":37045},{\"end\":37641,\"start\":37576},{\"end\":38154,\"start\":38061},{\"end\":38679,\"start\":38625},{\"end\":39166,\"start\":39104},{\"end\":39509,\"start\":39460},{\"end\":40272,\"start\":40177},{\"end\":41189,\"start\":41124},{\"end\":41716,\"start\":41667},{\"end\":41971,\"start\":41926},{\"end\":42473,\"start\":42427},{\"end\":42774,\"start\":42727},{\"end\":43313,\"start\":43254},{\"end\":43851,\"start\":43743},{\"end\":44441,\"start\":44367},{\"end\":44934,\"start\":44876},{\"end\":45422,\"start\":45367},{\"end\":45864,\"start\":45787},{\"end\":46166,\"start\":46095},{\"end\":46673,\"start\":46601},{\"end\":47026,\"start\":46985},{\"end\":47384,\"start\":47314},{\"end\":47761,\"start\":47715},{\"end\":48252,\"start\":48182},{\"end\":48647,\"start\":48569},{\"end\":49287,\"start\":49210},{\"end\":49670,\"start\":49597},{\"end\":49994,\"start\":49928}]", "bib_author": "[{\"end\":31156,\"start\":31132},{\"end\":31169,\"start\":31156},{\"end\":31186,\"start\":31169},{\"end\":31608,\"start\":31595},{\"end\":31621,\"start\":31608},{\"end\":31990,\"start\":31975},{\"end\":32002,\"start\":31990},{\"end\":32267,\"start\":32255},{\"end\":32283,\"start\":32267},{\"end\":32296,\"start\":32283},{\"end\":32309,\"start\":32296},{\"end\":32325,\"start\":32309},{\"end\":32344,\"start\":32325},{\"end\":32809,\"start\":32797},{\"end\":32816,\"start\":32809},{\"end\":32829,\"start\":32816},{\"end\":32840,\"start\":32829},{\"end\":33158,\"start\":33144},{\"end\":33171,\"start\":33158},{\"end\":33186,\"start\":33171},{\"end\":33197,\"start\":33186},{\"end\":33639,\"start\":33625},{\"end\":33652,\"start\":33639},{\"end\":33667,\"start\":33652},{\"end\":33677,\"start\":33667},{\"end\":33930,\"start\":33914},{\"end\":33946,\"start\":33930},{\"end\":33966,\"start\":33946},{\"end\":33984,\"start\":33966},{\"end\":34310,\"start\":34297},{\"end\":34323,\"start\":34310},{\"end\":34336,\"start\":34323},{\"end\":34350,\"start\":34336},{\"end\":34363,\"start\":34350},{\"end\":34697,\"start\":34685},{\"end\":34707,\"start\":34697},{\"end\":34721,\"start\":34707},{\"end\":34735,\"start\":34721},{\"end\":35152,\"start\":35140},{\"end\":35163,\"start\":35152},{\"end\":35177,\"start\":35163},{\"end\":35192,\"start\":35177},{\"end\":35644,\"start\":35629},{\"end\":35653,\"start\":35644},{\"end\":35669,\"start\":35653},{\"end\":35678,\"start\":35669},{\"end\":35685,\"start\":35678},{\"end\":36150,\"start\":36134},{\"end\":36161,\"start\":36150},{\"end\":36176,\"start\":36161},{\"end\":36195,\"start\":36176},{\"end\":36203,\"start\":36195},{\"end\":36550,\"start\":36538},{\"end\":36561,\"start\":36550},{\"end\":36574,\"start\":36561},{\"end\":36587,\"start\":36574},{\"end\":36600,\"start\":36587},{\"end\":36613,\"start\":36600},{\"end\":36627,\"start\":36613},{\"end\":37127,\"start\":37116},{\"end\":37141,\"start\":37127},{\"end\":37149,\"start\":37141},{\"end\":37161,\"start\":37149},{\"end\":37177,\"start\":37161},{\"end\":37658,\"start\":37643},{\"end\":37671,\"start\":37658},{\"end\":37685,\"start\":37671},{\"end\":37700,\"start\":37685},{\"end\":38169,\"start\":38156},{\"end\":38184,\"start\":38169},{\"end\":38193,\"start\":38184},{\"end\":38206,\"start\":38193},{\"end\":38211,\"start\":38206},{\"end\":38698,\"start\":38681},{\"end\":38719,\"start\":38698},{\"end\":38732,\"start\":38719},{\"end\":39185,\"start\":39168},{\"end\":39198,\"start\":39185},{\"end\":39214,\"start\":39198},{\"end\":39227,\"start\":39214},{\"end\":39240,\"start\":39227},{\"end\":39525,\"start\":39511},{\"end\":39538,\"start\":39525},{\"end\":39555,\"start\":39538},{\"end\":39566,\"start\":39555},{\"end\":39905,\"start\":39894},{\"end\":39919,\"start\":39905},{\"end\":39931,\"start\":39919},{\"end\":39944,\"start\":39931},{\"end\":39958,\"start\":39944},{\"end\":39971,\"start\":39958},{\"end\":40287,\"start\":40274},{\"end\":40301,\"start\":40287},{\"end\":40316,\"start\":40301},{\"end\":40739,\"start\":40722},{\"end\":40755,\"start\":40739},{\"end\":40771,\"start\":40755},{\"end\":40785,\"start\":40771},{\"end\":41200,\"start\":41191},{\"end\":41214,\"start\":41200},{\"end\":41226,\"start\":41214},{\"end\":41240,\"start\":41226},{\"end\":41255,\"start\":41240},{\"end\":41730,\"start\":41718},{\"end\":41745,\"start\":41730},{\"end\":41755,\"start\":41745},{\"end\":41769,\"start\":41755},{\"end\":41985,\"start\":41973},{\"end\":42000,\"start\":41985},{\"end\":42011,\"start\":42000},{\"end\":42022,\"start\":42011},{\"end\":42036,\"start\":42022},{\"end\":42487,\"start\":42475},{\"end\":42498,\"start\":42487},{\"end\":42504,\"start\":42498},{\"end\":42521,\"start\":42504},{\"end\":42792,\"start\":42776},{\"end\":42811,\"start\":42792},{\"end\":42824,\"start\":42811},{\"end\":42840,\"start\":42824},{\"end\":42848,\"start\":42840},{\"end\":42864,\"start\":42848},{\"end\":43327,\"start\":43315},{\"end\":43340,\"start\":43327},{\"end\":43350,\"start\":43340},{\"end\":43365,\"start\":43350},{\"end\":43874,\"start\":43853},{\"end\":43893,\"start\":43874},{\"end\":43907,\"start\":43893},{\"end\":43922,\"start\":43907},{\"end\":44457,\"start\":44443},{\"end\":44469,\"start\":44457},{\"end\":44481,\"start\":44469},{\"end\":44489,\"start\":44481},{\"end\":44949,\"start\":44936},{\"end\":44962,\"start\":44949},{\"end\":44977,\"start\":44962},{\"end\":45438,\"start\":45424},{\"end\":45451,\"start\":45438},{\"end\":45889,\"start\":45866},{\"end\":45903,\"start\":45889},{\"end\":45911,\"start\":45903},{\"end\":46180,\"start\":46168},{\"end\":46194,\"start\":46180},{\"end\":46206,\"start\":46194},{\"end\":46215,\"start\":46206},{\"end\":46684,\"start\":46675},{\"end\":46694,\"start\":46684},{\"end\":46706,\"start\":46694},{\"end\":46723,\"start\":46706},{\"end\":47039,\"start\":47028},{\"end\":47049,\"start\":47039},{\"end\":47058,\"start\":47049},{\"end\":47071,\"start\":47058},{\"end\":47085,\"start\":47071},{\"end\":47394,\"start\":47386},{\"end\":47406,\"start\":47394},{\"end\":47423,\"start\":47406},{\"end\":47431,\"start\":47423},{\"end\":47440,\"start\":47431},{\"end\":47449,\"start\":47440},{\"end\":47460,\"start\":47449},{\"end\":47776,\"start\":47763},{\"end\":47789,\"start\":47776},{\"end\":47804,\"start\":47789},{\"end\":48269,\"start\":48254},{\"end\":48283,\"start\":48269},{\"end\":48294,\"start\":48283},{\"end\":48305,\"start\":48294},{\"end\":48316,\"start\":48305},{\"end\":48664,\"start\":48649},{\"end\":48675,\"start\":48664},{\"end\":48686,\"start\":48675},{\"end\":48697,\"start\":48686},{\"end\":49047,\"start\":49036},{\"end\":49062,\"start\":49047},{\"end\":49300,\"start\":49289},{\"end\":49314,\"start\":49300},{\"end\":49327,\"start\":49314},{\"end\":49338,\"start\":49327},{\"end\":49349,\"start\":49338},{\"end\":49683,\"start\":49672},{\"end\":49697,\"start\":49683},{\"end\":49708,\"start\":49697},{\"end\":50006,\"start\":49996},{\"end\":50017,\"start\":50006},{\"end\":50030,\"start\":50017},{\"end\":50045,\"start\":50030},{\"end\":50058,\"start\":50045},{\"end\":50072,\"start\":50058}]", "bib_venue": "[{\"end\":31327,\"start\":31265},{\"end\":31750,\"start\":31694},{\"end\":32493,\"start\":32427},{\"end\":33338,\"start\":33276},{\"end\":34864,\"start\":34808},{\"end\":35333,\"start\":35271},{\"end\":35834,\"start\":35768},{\"end\":36776,\"start\":36710},{\"end\":37326,\"start\":37260},{\"end\":37829,\"start\":37773},{\"end\":38360,\"start\":38294},{\"end\":38881,\"start\":38815},{\"end\":40465,\"start\":40399},{\"end\":41416,\"start\":41344},{\"end\":42197,\"start\":42125},{\"end\":43005,\"start\":42943},{\"end\":43514,\"start\":43448},{\"end\":44071,\"start\":44005},{\"end\":44638,\"start\":44572},{\"end\":45146,\"start\":45070},{\"end\":45592,\"start\":45530},{\"end\":46364,\"start\":46298},{\"end\":47973,\"start\":47897},{\"end\":50221,\"start\":50155},{\"end\":31263,\"start\":31186},{\"end\":31692,\"start\":31621},{\"end\":32046,\"start\":32002},{\"end\":32425,\"start\":32344},{\"end\":32878,\"start\":32840},{\"end\":33274,\"start\":33197},{\"end\":33623,\"start\":33585},{\"end\":34021,\"start\":33984},{\"end\":34420,\"start\":34363},{\"end\":34806,\"start\":34735},{\"end\":35269,\"start\":35192},{\"end\":35766,\"start\":35685},{\"end\":36250,\"start\":36203},{\"end\":36708,\"start\":36627},{\"end\":37258,\"start\":37177},{\"end\":37771,\"start\":37700},{\"end\":38292,\"start\":38211},{\"end\":38813,\"start\":38732},{\"end\":39269,\"start\":39240},{\"end\":39615,\"start\":39566},{\"end\":39892,\"start\":39793},{\"end\":40397,\"start\":40316},{\"end\":40902,\"start\":40801},{\"end\":41342,\"start\":41255},{\"end\":41780,\"start\":41769},{\"end\":42123,\"start\":42036},{\"end\":42558,\"start\":42521},{\"end\":42941,\"start\":42864},{\"end\":43446,\"start\":43365},{\"end\":44003,\"start\":43922},{\"end\":44570,\"start\":44489},{\"end\":45068,\"start\":44977},{\"end\":45528,\"start\":45451},{\"end\":45922,\"start\":45911},{\"end\":46296,\"start\":46215},{\"end\":46776,\"start\":46723},{\"end\":47123,\"start\":47085},{\"end\":47497,\"start\":47460},{\"end\":47895,\"start\":47804},{\"end\":48365,\"start\":48316},{\"end\":48735,\"start\":48697},{\"end\":49034,\"start\":48950},{\"end\":49386,\"start\":49349},{\"end\":49745,\"start\":49708},{\"end\":50153,\"start\":50072}]"}}}, "year": 2023, "month": 12, "day": 17}