{"id": 173990648, "updated": "2023-10-02 11:09:22.177", "metadata": {"title": "High-Quality Self-Supervised Deep Image Denoising", "authors": "[{\"first\":\"Samuli\",\"last\":\"Laine\",\"middle\":[]},{\"first\":\"Tero\",\"last\":\"Karras\",\"middle\":[]},{\"first\":\"Jaakko\",\"last\":\"Lehtinen\",\"middle\":[]},{\"first\":\"Timo\",\"last\":\"Aila\",\"middle\":[]}]", "venue": "ArXiv", "journal": "6968-6978", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "We describe a novel method for training high-quality image denoising models based on unorganized collections of corrupted images. The training does not need access to clean reference images, or explicit pairs of corrupted images, and can thus be applied in situations where such data is unacceptably expensive or impossible to acquire. We build on a recent technique that removes the need for reference data by employing networks with a\"blind spot\"in the receptive field, and significantly improve two key aspects: image quality and training efficiency. Our result quality is on par with state-of-the-art neural network denoisers in the case of i.i.d. additive Gaussian noise, and not far behind with Poisson and impulse noise. We also successfully handle cases where parameters of the noise model are variable and/or unknown in both training and evaluation data.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1901.10277", "mag": "2971082124", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/LaineKLA19", "doi": null}}, "content": {"source": {"pdf_hash": "43eff313f8c9ced25f553669152ed04d5b243cab", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1901.10277v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "ef71e228c9545b383df71d19c5afbd04bab8c887", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/43eff313f8c9ced25f553669152ed04d5b243cab.txt", "contents": "\nHigh-Quality Self-Supervised Deep Image Denoising\n\n\nSamuli Laine Nvidia \nNVIDIA\nNVIDIA\nAalto University\nNVIDIA\n\n\nTero Karras \nNVIDIA\nNVIDIA\nAalto University\nNVIDIA\n\n\nJaakko Lehtinen \nNVIDIA\nNVIDIA\nAalto University\nNVIDIA\n\n\nTimo Aila \nNVIDIA\nNVIDIA\nAalto University\nNVIDIA\n\n\nHigh-Quality Self-Supervised Deep Image Denoising\n\nWe describe a novel method for training high-quality image denoising models based on unorganized collections of corrupted images. The training does not need access to clean reference images, or explicit pairs of corrupted images, and can thus be applied in situations where such data is unacceptably expensive or impossible to acquire. We build on a recent technique that removes the need for reference data by employing networks with a \"blind spot\" in the receptive field, and significantly improve two key aspects: image quality and training efficiency. Our result quality is on par with state-of-the-art neural network denoisers in the case of i.i.d. additive Gaussian noise, and not far behind with Poisson and impulse noise. We also successfully handle cases where parameters of the noise model are variable and/or unknown in both training and evaluation data. *\n\nIntroduction\n\nDenoising, the removal of noise from images, is a major application of deep learning. Several architectures have been proposed for general-purpose image restoration tasks, e.g., U-Nets [29], hierarchical residual networks [26], and residual dense networks [38]. Traditionally, the models are trained in a supervised fashion with corrupted images as inputs and clean images as targets, so that the network learns to remove the corruption.\n\nLehtinen et al. [22] introduced NOISE2NOISE training, where pairs of corrupted images are used as training data. They observe that when certain statistical conditions are met, a network faced with the impossible task of mapping corrupted images to corrupted images learns, loosely speaking, to output the \"average\" image. For a large class of image corruptions, the clean image is a simple per-pixel statistic -such as mean, median, or mode -over the stochastic corruption process, and hence the restoration model can be supervised using corrupted data by choosing the appropriate loss function to recover the statistic of interest.\n\nWhile removing the need for clean training images, NOISE2NOISE training still requires at least two independent realizations of the corruption for each training image. While this eases data collection significantly compared to noisy-clean pairs, large collections of (single) poor images are still much more widespread. This motivates investigation of self-supervised training: how much can we learn from just looking at corrupted data? While foregoing supervision would lead to the expectation of some regression in performance, can we make up for it by making stronger assumptions about the corruption process? In this paper, we show that for several noise models that are i.i.d. between pixels (Gaussian, Poisson, impulse), only minor concessions in denoising performance are necessary. We furthermore show that the parameters of the noise models do not need to be known in advance.\n\nWe draw inspiration from the recent NOISE2VOID training technique of Krull et al. [19]. The algorithm needs no image pairs, and uses just individual noisy images as training data, assuming that the corruption is zero-mean and independent between pixels. The method is based on blindspot networks where the receptive field of the network does not include the center pixel. This C C C C C C C 1 1 1 C C C C C C C 1 1 1 R R -1 Figure 1: Top: In our blind-spot network architecture, we effectively construct four denoiser network branches, each having its receptive field restricted to a different direction. A single-pixel offset at the end of each branch separates the receptive field from the center pixel. The results are then combined by 1\u00d71 convolutions. Bottom: In practice, we run four rotated versions of each input image through a single receptive field -restricted branch, yielding a simpler architecture that performs the same function. This also implicitly shares the convolution kernels between the branches and thus avoids the four-fold increase in the number of trainable weights. allows using the same noisy image as both training input and training target -because the network cannot see the correct answer, using the same image as target is equivalent to using a different noisy realization. This approach is self-supervised in the sense that the surrounding context is used to predict the value of the output pixel without a separate reference image [10].\n\nThe networks used by Krull et al. [19] do not have a blind spot by design, but are trained to ignore the center pixel using a masking scheme where only a few output pixels can contribute to the loss function, reducing training efficiency considerably. We remedy this with a novel architecture that allows efficient training without masking. Furthermore, the existence of the blind spot leads to poor denoising quality. We derive a scheme for combining the network output with data in the blind spot, bringing the denoising quality on par with, or at least much closer to, conventionally trained networks.\n\n\nConvolutional blind-spot network architectures\n\nOur convolutional blind-spot networks are designed by combining multiple branches that each have their receptive field restricted to a half-plane ( Figure 1) that does not contain the center pixel. We combine the four branches with a series of 1\u00d71 convolutions to obtain a receptive field that can extend arbitrarily far in every direction but does not contain the center pixel. The principle of limiting the receptive field has been previously used in PixelCNN [36,35,30] image synthesis networks, where only pixels synthesized before the current pixel are allowed in the receptive field. 2 The benefit of our architecture compared to the masking-based training of Krull et al. [19] is that all output pixels can contribute to the loss function as in conventional training.\n\nIn order to transform a restoration network into one with a restricted receptive field, we modify each individual layer so that its receptive field is fully contained within one half-plane, including the center row/column. The receptive field of the resulting network includes the center pixel, so we offset the feature maps by one pixel before combining them. Layers that do not extend the receptive field, e.g., concatenation, summation, 1\u00d71 convolution, etc., can be used without modifications.\n\nConvolution layers To restrict the receptive field of a zero-padding convolution layer to extend only, say, upwards, the easiest solution is to offset the feature maps downwards when performing the convolution operation. For an h \u00d7 w kernel size, a downwards offset of k = h/2 pixels is equivalent to using a kernel that is shifted upwards so that all weights below the center row are zero. Specifically, we first append k rows of zeros to the top of input tensor, then perform the convolution, and finally crop out the k bottom rows of the output.\n\nDownsampling and upsampling layers Many image restoration networks involve downsampling and upsampling layers, and by default, these extend the receptive field in all directions. Consider, e.g., a 2 \u00d7 2 average downsampling step followed immediately by a nearest-neighbor 2 \u00d7 2 upsampling step. The contents of every 2 \u00d7 2 pixel block in the output now correspond to the average of this block in the input, i.e., information has been transferred in every direction within the block. We fix this problem by again applying an offset to the data. It is sufficient to restrict the receptive field for the pair of downsampling and upsampling layers, which means that only one of the layers needs to be modified, and we have chosen to attach the offsets to the downsampling layers. For a 2 \u00d7 2 average downsampling layer, we can restrict the receptive field to extend upwards only by padding the input tensor with one row of zeros at top and cropping out the bottom row before performing the actual downsampling operation.\n\n3 Self-supervised Bayesian denoising with blind-spot networks\n\nConsider the prediction of the clean value x for a noisy pixel y. As the pixels in an image are not independent, all denoising algorithms assume the clean value depends not only on the noisy measurement y, but also on the context of neighboring (noisy) pixels that we denote by \u2126 y . For our convolutional networks, the context corresponds to the receptive field sans the central pixel. From this point of view, denoising can be thought of as statistical inference on the probability distribution p(x|y, \u2126 y ) over the clean pixel value x conditioned with both the context \u2126 y and the measurement y. Concretely, a standard supervised regression model trained with corrupted-clean pairs and L 2 loss will return an estimate of E x [p(x|y, \u2126 y )], i.e., the mean over all possible clean pixel values given the noisy pixel and its context.\n\nAssuming the noise is independent between pixels and independent of the context, the blind-spot network introduced by Krull et al. [19] predicts the clean value based purely on the context, using the noisy measurement y as a training target, drawing on the NOISE2NOISE approach [22]. Concretely, their regressor learns to estimate E x [p(x|\u2126 y )], i.e., the mean of all potential clean values consistent with the context. Batson and Royer [2] present an elegant general formulation for self-supervised models like this. However, methods that ignore the corrupted measurement y at test-time clearly leave useful information unused, potentially leading to reduced performance.\n\nWe bring in extra information in the form of an explicit model of the corruption, provided as a likelihood p(y|x) of the observation given the clean value, which we assume to be independent of the context and i.i.d. between pixels. This allows us to connect the observed marginal distribution of the noisy training data to the unobserved distribution of clean data:\np(y|\u2126 y ) Training data = p(y|x) Noise model p(x|\u2126 y ) Unobserved dx(1)\nThis functional relationship suggests that even though we only observe corrupted training data, the known noise model should help us learn to predict a parametric model for the distribution p(x|\u2126 y ). Specifically, we model p(x|\u2126 y ) as a multivariate Gaussian N (\u00b5 x , \u03a3 x ) over color components. For many noise models, the marginal likelihood p(y|\u2126 y ) can then be computed in closed form, allowing us to train a neural network to map the context \u2126 y to the mean \u00b5 x and covariance \u03a3 x by maximizing the likelihood of the data under Equation (1).\n\nThe approximate distribution p(x|\u2126 y ) allows us to now apply Bayesian reasoning to include information from y at test-time. Specifically, the (unnormalized) posterior probability of the clean value x given observations of both the noisy pixel y and its context is given by Bayes' rule as follows:\np(x|y, \u2126 y ) Posterior \u221d p(y|x) Noise model p(x|\u2126 y ) Prior(2)\nFrom this point of view, the distribution p(x|\u2126 y ) takes the role of the prior, encoding our beliefs on the possible xs before observing y. (Note that even though we represent the prior as a Gaussian, the posterior is generally not Gaussian due to the multiplication with the noise likelihood.) With the posterior at hand, standard Bayesian inference tools become available: for instance, a maximum a posteriori (MAP) estimate would pick the x that maximizes the posterior; we use the posterior mean E x [p(x|y, \u2126 y )] for all denoising results as it minimizes MSE and consequently maximizes PSNR. To summarize, our approach consists of (1) standard training phase and (2) two-step testing phase:\n\n(1) Train a neural network to map the context \u2126 y to the mean \u00b5 x and variance \u03a3 x of a Gaussian approximation to the prior p(x|\u2126 y ).\n\n(2) At test time, first feed context \u2126 y to neural network to yield \u00b5 x and \u03a3 x ; then compute posterior mean E x [p(x|y, \u2126 y )] by closed-form analytic integration.\n\nLooping back to the beginning of this section, we note that the estimate found by standard supervised training with the L 2 loss is precisely the same posterior mean E x [p(x|y, \u2126 y )] we seek. Unfortunately, this does not imply that our self-supervised technique would be guaranteed to find the same optimum: we approximate the prior distribution with a Gaussian, whereas standard supervised training corresponds to a Gaussian approximation of the posterior. However, benign noise models, such as additive Gaussian noise or Poisson noise, interact with the prior in a way that the result is almost as good, as demonstrated below.\n\nIn concurrent work, Krull at al. [20] describe a similar algorithm for monochromatic data. Instead of an analytical solution, they use a sampling-based method to describe the prior and posterior, and represent an arbitrary noise model as a discretized two-dimensional histogram.\n\n\nPractical experiments\n\nIn this section, we detail the implementation of our denoising scheme in Gaussian, Poisson, and impulse noise. In all our experiments, we use a modified version of the five-level U-Net [29] architecture used by Lehtinen et al. [22], to which we append three 1\u00d71 convolution layers. We construct our convolutional blind-spot networks based on this same architecture. Details regarding network architecture, training, and evaluation are provided in Appendix A. Our training data comes from the 50k images in the ILSVRC2012 (Imagenet) validation set, and our test datasets are the commonly used KODAK (24 images), BSD300 validation set (100 images), and SET14 (14 images).\n\n\nAdditive Gaussian noise\n\nLet us now realize the scheme outlined in Section 3 in the context of additive Gaussian noise. We will cover the general case of color images only, but the method simplifies trivially to monochromatic images by replacing all matrices and vectors with scalar values.\n\nThe blind-spot network outputs the parameters of a multivariate Gaussian N (\u00b5 x , \u03a3 x ) = p(x|\u2126 y ) representing the distribution of the clean signal. We parameterize the covariance matrix as\n\u03a3 x = A x T A x\nwhere A x is an upper triangular matrix. This ensures that \u03a3 x is a valid covariance matrix, i.e., symmetric and positive semidefinite. Thus we have a total of nine output components per pixel for RGB images: the three-component mean \u00b5 x and the six nonzero elements of A x .\n\nModeling the corruption process is particularly simple with additive zero-mean Gaussian noise. In this case, Eq. 1 performs a convolution of two mutually independent Gaussians, and the covariance of the result is simply the sum of the constituents [4]. Therefore,\n\u00b5 y = \u00b5 x and \u03a3 y = \u03a3 x + \u03c3 2 I,(3)\nwhere \u03c3 is the standard deviation of the Gaussian noise. We can either assume \u03c3 to be known for each training and validation image, or we can learn to estimate it during training. For a constant, unknown \u03c3, we add \u03c3 as one of the trainable parameters. For variable and unknown \u03c3, we learn an auxiliary neural network for predicting it during training. The architecture of this auxiliary network is the same as in the baseline networks except that only one scalar per pixel is produced, and the \u03c3 for the entire image is obtained by taking the mean over the output. It is quite likely that a simpler network would have sufficed for the task, but we did not attempt to optimize its architecture. Note that the \u03c3 estimation network is not trained with a known noise level as a target, but it learns to predict it as a part of the training process.\n\nTo fit N (\u00b5 y , \u03a3 y ) to the observed noisy training data, we minimize the corresponding negative loglikelihood loss during training [28,21,17]:\nloss(y, \u00b5 y , \u03a3 y ) = \u2212 log f (y; \u00b5 y , \u03a3 y ) = 1 2 [(y \u2212 \u00b5 y ) T \u03a3 \u22121 y (y \u2212 \u00b5 y )] + 1 2 log |\u03a3 y | + C,(4)\nwhere C subsumes additive constant terms that can be discarded, and f (y; \u00b5 y , \u03a3 y ) denotes the probability density of a multivariate Gaussian distribution N (\u00b5 y , \u03a3 y ) at pixel value y. In cases where \u03c3 is unknown and needs to be estimated, we add a small regularization term of \u22120.1\u03c3 to the loss. This encourages explaining the observed noise as corruption instead of uncertainty about the clean signal. As long as the regularization is gentle enough, the estimated \u03c3 does not overshoot -if it did, \u03a3 y = \u03a3 x + \u03c3 2 I would become too large to fit the observed data in easy-to-denoise regions.\n\nAt test time, we compute the mean of the posterior distribution. With additive Gaussian noise the product involves two Gaussians, and because both distributions are functions of x, we have\np(y|x) p(x|\u2126 y ) = f (x; y, \u03c3 2 I) f (x; \u00b5 x , \u03a3 x ),(5)\nwhere we have exploited the symmetry of Gaussian distribution in the first term to swap x and y. A product of two Gaussian functions is an unnormalized Gaussian function, whose mean [4] coincides with the desired posterior mean:\nE x [p(x|y, \u2126 y )] = (\u03a3 \u22121 x + \u03c3 \u22122 I) \u22121 (\u03a3 \u22121 x \u00b5 x + \u03c3 \u22122 y).(6)\nNote that we do not need to evaluate the normalizing constant (marginal likelihood), as scalar multiplication does not change the mean of a Gaussian.\n\nInformally, the formula can be seen to \"mix in\" some of the observed noisy pixel color y into the estimated mean \u00b5 x . When the network is certain about the clean signal (\u03a3 x is small), the estimated mean \u00b5 x dominates the result. Conversely, the larger the uncertainty of the clean signal is compared to \u03c3, the more of the noisy observed signal is included in the result.\n\nComparisons and ablations Table 1 shows the output image quality for the various methods and ablations tested. Example result images are shown in Figure 2. All methods are evaluated using the same corrupted input data, and thus the only sources of randomness are the network initialization and training data shuffling during training. Denoiser networks seem to be fairly robust to these effects, e.g. [22] reports \u00b10.02 dB variation in the averaged results. We expect the same bounds to hold for our results as well.\n\nLet us first consider the case where the amount of noise is fixed (top half of the table). The N2C baseline is trained with clean reference images as training targets, and unsurprisingly produces the best results that can be reached with a given network architecture. N2N [22] matches the results.\n\nOur method with a convolutional blind-spot network and posterior mean estimation is virtually as good as the baseline methods. This holds even when the amount of noise is unknown and needs to be estimated as part of the learning process. However, when we ablate our method by forcing the covariance matrix \u03a3 x to be diagonal, the quality of the results suffers considerably. This setup corresponds to treating each color component of the prior as a univariate, independent distribution, and the bad result quality highlights the need to treat the signal as a true multivariate distribution.  Figure 2: Example result images for methods corresponding to Table 1: Gaussian noise \u03c3 = 25 (\u03c3 not known). PSNRs refer to the individual images. Figure 5 shows additional result images. We can ablate the setup even further by having our blind-spot network architecture predict only the mean \u00b5 using standard L 2 loss, and using this predicted mean directly as the denoiser output. This corresponds to the setup of Krull et al. [19] in the sense that the center pixel is ignored. As expected, the image quality suffers greatly due to the inability to extract information from the center pixel.\n\nSince we do not perform posterior mean estimation in this setup, noise level \u03c3 does not appear in the calculations and knowing it would be of no use.\n\nFinally, we denoise the same test images using the official implementation of CBM3D [8], a stateof-the-art non-learned image denoising algorithm. 3 It uses no training data and relies on the contents of each individual test image for recovering the clean signal. With both known and automatically estimated (using the method of Chen et al. [7]) noise parameters, CBM3D outperforms our ablated setups but remains far from the quality of our full method and the baseline methods.\n\nThe lower half of Table 1 presents the same metrics in the case of variable Gaussian noise, i.e., when the noise parameters are chosen randomly within the specified range for each training and test image. The relative ordering of the methods remains the same as with a fixed amount of noise, although our method concedes 0.1dB relative to the baseline. Knowing the noise level in advance does not change the results. Table 2 illustrates the relationship between output quality and training set size. Without dataset augmentation, our method performs roughly on par with the baseline and surpasses it for very small datasets (<1000 images). For the smaller training sets, rotation augmentation becomes beneficial for the baseline method, whereas for our method it only improves the training of 1\u00d71 combination layers. With rotation augmentation enabled, our method therefore loses to the baseline method for very small datasets, although not by much. No other training runs in this paper use augmentation, as it provides no benefit when using the full training set.\n\nComparison to masking-based training Our \"\u00b5 only\" ablations illustrate the benefits of Bayesian training and posterior mean estimation compared to ignoring the center pixel as in the original NOISE2VOID method. Here, we shall separately estimate the advantages of having an architectural blind spot instead of masking-based training [19]. We trained several networks with our baseline architecture using masking. As recommended by Krull et al., we chose 64 pixels to be masked in each input crop using stratified sampling. Two masking strategies were evaluated: copying from another pixel in a 5\u00d75 neighborhood (denoted COPY) as advocated in [19], and overwriting the pixel with a random color in [0, 1] 3 (denoted RANDOM), as done by Batson and Royer [2].\n\nOur tests confirmed that the COPY strategy gave better results when the center pixel was ignored, but the RANDOM strategy gave consistently better results in the Bayesian setting. COPY probably Figure 3:\n\nRelative training costs for Gaussian noise (\u03c3 = 25, known) denoisers using the posterior mean estimation. For comparison, training a convolutional blind-spot network for 0.5M minibatches achieves 32.39 dB in KODAK. For the masking-based methods, the horizontal axis takes into account the approximately 4\u00d7 cheaper training compared to our convolutional blind-spot networks. For example, at x-axis position marked \"1\" they have been trained for 2M minibatches compared to 0.5M minibatches for our method. leads to the network learning to leak some of the center pixel value into the output, which may help by sharpening the output a bit even when done in such an ad hoc fashion. However, our Bayesian approach assumes that no such information leaking occurs, and therefore does not tolerate it.\n\nFocusing on the highest-quality setup with posterior mean estimation and RANDOM masking strategy, we estimate that training to a quality matching 0.5M minibatches with our convolutional blindspot architecture would require at least 20-100\u00d7 as much computation due to the loss function sparsity. This is based on a 10\u00d7 longer masking-based training run still not reaching comparable output quality, see Figure 3.\n\n\nPoisson noise\n\nIn our second experiment we consider Poisson noise which is an interesting practical case as it can be used to model the photon noise in imaging sensors. We denote the maximum event count as \u03bb and implement the noise as y i = Poisson(\u03bbx i )/\u03bb where i is the color channel and x i \u2208 [0, 1] is the clean color component. For denoising, we follow the common approach of approximating Poisson noise as signal-dependent Gaussian noise [13]. In this setup, the resulting standard deviation is \u03c3 i = x i /\u03bb and the corruption model is thus\n\u00b5 y = \u00b5 x and \u03a3 y = \u03a3 x + \u03bb \u22121 diag(\u00b5 x ).(7)\nNote that there is a second approximation in this approach -the marginalization over x (Eq. 1) is treated as a convolution with a fixed Gaussian even though p(y|x) should be different for each x.\n\nIn the formula above, we implicitly take this term to be p(y|\u00b5 x ) which is a good approximation in the common case of \u03a3 x being small. Aside from a different corruption model, both training and denoising are equivalent to the Gaussian case (Section 4.1). For cases where the noise parameters are unknown, we treat \u03bb \u22121 as the unknown parameter that is either learned directly or estimated via the auxiliary network, depending on whether the amount of noise is fixed or variable, respectively.\n\nComparisons Table 3, top half, shows the image quality results with Poisson noise, and Figure 4, top, shows example result images. Note that even though we internally model the noise as signaldependent Gaussian noise, we apply true Poisson noise to training and test data. In the case of fixed amount of noise, our method is within 0.1-0.2 dB from the N2C baseline. Curiously, the case where the \u03bb is unknown performs slightly better than the case where it is supplied. This is probably a consequence of the approximations discussed above, and the network may be able to fit the observed noisy distribution better when it is free to choose a different ratio between variance and mean.\n\nIn the case of variable noise, our method remains roughly as good when the noise parameters are known, but starts to have trouble when they need to be estimated from data. However, it appears that the problems are mainly concentrated to SET14 where there is a 1.2 dB drop whereas the other test sets suffer by only \u223c0.1 dB. The lone culprit for this drop is the POWERPOINT clip art image, where our method fails to estimate the noise level correctly, suffering a hefty 13dB penalty. Nonetheless, comparing to the \"\u00b5 only\" ablation with L 2 loss, i.e., ignoring the center pixel, shows that our method with posterior mean estimation still produces much higher output quality. Anscombe transform [25] is a classical non-learned baseline for denoising Poisson noise, and for reference we include the results for this method as reported in [22]. \n\n\nImpulse noise\n\nOur last example involves impulse noise where each pixel is, with probability \u03b1, replaced by an uniformly sampled random color in [0, 1] 3 . This corruption process is more complex than in the previous cases, as both mean and covariance are modified, and there is a Dirac peak at the clean color value. To derive the training loss, we again approximate p(y|\u2126 y ) with a Gaussian, and match its first and second raw moments to the data during training. Because the marginal likelihood is a mixture distribution, its raw moments are obtained by linearly interpolating, with parameter \u03b1, between the raw moments of p(x|\u2126 y ) and the raw moments of the uniform random distribution. The resulting mean and covariance are \n+ (1 \u2212 \u03b1)(\u03a3 x + \u00b5 x \u00b5 x T ) \u2212 \u00b5 y \u00b5 y T . (8)\nThis defines the approximate p(y|\u2126 y ) needed for training the denoiser network. As with previous noise types, in setups where parameter \u03b1 is unknown, we add it as a learned parameter or estimate it via a simultaneously trained auxiliary network. The unnormalized posterior is\np(y|x) p(x|\u2126 y ) = \u03b1 + (1 \u2212 \u03b1)\u03b4(y \u2212 x) f (x; \u00b5 x , \u03a3 x ) = \u03b1f (x; \u00b5 x , \u03a3 x ) + (1 \u2212 \u03b1)\u03b4(y \u2212 x)f (x; \u00b5 x , \u03a3 x )(9)\nfrom which we obtain the posterior mean:\nE x [p(x|y, \u2126 y )] = \u03b1\u00b5 x + (1 \u2212 \u03b1)f (y; \u00b5 x , \u03a3 x )y \u03b1 + (1 \u2212 \u03b1)f (y; \u00b5 x , \u03a3 x ) .(10)\nLooking at the formula, we can see that the result is a linear interpolation between the mean \u00b5 x predicted by the network and the potentially corrupted observed pixel value y. Informally, we can reason that the less likely the observed value y is to be drawn from the predicted distribution N (\u00b5 x , \u03a3 x ), the more likely it is to be corrupted, and therefore its weight is low compared to the predicted mean \u00b5 x . On the other hand, when the observed pixel value is consistent with the network prediction, it is weighted more heavily in the output color.\n\nComparisons Table 3, bottom half, shows the image quality results, and example result images are shown in Figure 4, bottom. The N2N baseline has more trouble with impulse noise than with  Gaussian or Poisson noise -note that it cannot be trained with standard L 2 loss because the noise is not zero-mean. Lehtinen et al. [22] recommend annealing from L 2 loss to L 0 loss in these cases. We experimented with several loss function schedules for N2N, and obtained the best results by annealing the loss exponent from 2 to 0.5 during the first 75% of training and holding it there for the remaining training time. Our method loses to the N2C baseline by \u223c0.4 dB in the case of fixed noise, and by \u223c0.3 dB with the more difficult variable noise. Notably, our method does not suffer from not knowing the noise parameter \u03b1 in either case. The ablated \"\u00b5 only\" setups were trained with the same loss schedules as the corresponding N2N baselines and lose to the other methods by multiple dB, highlighting the usefulness of the information in the center pixel in this type of noise.\n\n\nDiscussion and future work\n\nApplying Bayesian statistics to denoising has a long history. Non-local means [5], BM3D [9], and WNNM [11] identify a group of similar pixel neighborhoods and estimate the center pixel's color from those. Deep image prior [34] seeks a representation for the input image that is easiest to model with a convolutional network, often encountering a reasonable noise-free representation along the way. As with self-supervised training, these methods need only the noisy images, but while the explicit block-based methods determine a small number of neighborhoods from the input image alone, a deep denoising model may implicitly identify and regress an arbitrarily large number of neighborhoods from a collection of noisy training data.\n\nStein's unbiased risk estimator has been used for training deep denoisers for Gaussian noise [32,27], but compared to our work these methods leave a larger quality gap compared to supervised training. Jena [15] corrupts noisy training data further, and trains a network to reduce the amount of noise to the original level. This network can then iteratively restore images with the original amount of noise. Unfortunately, no comparisons against supervised training are given. Finally, FC-AIDE [6] features an interesting combination of supervised and unsupervised training, where a traditionally trained denoiser network is fine-tuned in an unsupervised fashion for each test image individually.\n\nWe have shown, for the first time, that deep denoising models trained in a self-supervised fashion can reach similar quality as comparable models trained using clean reference data, as long as the drawbacks imposed by self-supervision are appropriately remedied. Our method assumes pixelwise independent noise with a known analytic likelihood model, although we have demonstrated that individual parameters of the corruption model can also be successfully deducted from the noisy data. Real corrupted images rarely follow theoretical models exactly [12,23,31], and an important avenue for future work will be to learn as much of the noise model from the data as possible. By basing the learning exclusively on the dataset of interest, we should also be able to alleviate the concern that the training data (e.g., natural images) deviates from the intended use (e.g., medical images). Experiments with such real life data will be valuable next steps.\n\nA Network architecture, training and evaluation details Table 4 shows the network architecture used in our blind-spot and baseline networks. This is a slightly modified version of the five-level U-Net [29] architecture that was used by Lehtinen et al. [22]. We add three 1\u00d71 convolution layers at the end in all networks, so that the network depth is the same in both blind-spot and baseline networks. All convolution layers use leaky ReLU [24] with \u03b1 = 0.1, except the very last 1\u00d71 convolution that has linear activation function.\n\nWhen forming a blind-spot network, we add three additional layers, denoted ROTATE, SHIFT, and UNROTATE in the table. Layer ROTATE forms four rotated versions (by 0 \u2022 , 90 \u2022 , 180 \u2022 , 270 \u2022 ) of the input tensor and stacks them on the minibatch axis. Layer SHIFT pads and shifts every feature map downwards by one pixel, thereby raising the receptive field of every pixel upwards by one pixel. This is needed so that when the receptive fields are later combined, the combination excludes the pixel itself. Finally, layer UNROTATE splits the minibatch axis into four pieces, undoes the rotation done in layer ROTATE, and stacks the results on the channel axis, restoring the minibatch size to the original but quadrupling the feature map count. In addition, in blind-spot networks we modify the convolution layers and downsampling layers to extend their receptive field upwards only, as explained in Section 2.\n\nTraining and evaluation All networks were initialized following He et al. [14] and trained using Adam with default parameters [18], initial learning rate \u03bb = 0.0003, and minibatch size of 4. The minibatches were composed of random 256\u00d7256 crops from the training set. All networks except those used in impulse noise experiments were trained for 0.5M minibatches, i.e., until 2M training image crops were shown to the network. For the impulse noise experiments we trained the blindspot networks 2\u00d7 as long and the baseline networks 8\u00d7 as long in order to reach convergence. In all training runs, learning rate was ramped down during the last 30% of training using a cosine schedule.\n\nInternally, we use dynamic range of [0, 1] for the image data. The training data was selected to contain only images whose size was between 256\u00d7256 and 512\u00d7512 pixels, in order to exclude images that were too small for obtaining a training crop, or unnecessarily large compared to the test images. We thus used 44328 training images out of the 50k images in ILSVRC2012 validation set.\n\nTo run the test images through our rotation-based architecture, each of them was padded to a square shape using mirror padding, denoised, and cropped back to original size. To obtain reliable average PSNRs, we replicated each test set multiple times so that each clean image was corrupted by multiple different instances of noise and, in cases with variable noise parameters, different amounts of noise. Specifically, we replicated test sets KODAK, BSD300, and SET14, by 10, 3, and 20 times, yielding average dataset PSNRs that correspond to averages over 240, 300, and 280 individual denoised images, respectively. All methods were evaluated with the same corrupted input data.\n\nThe training runs were executed on NVIDIA DGX-1 servers using four Tesla V100 GPUs in parallel. A typical training run took \u223c4 hours if using the baseline architecture, and \u223c14 hours with the blind-spot architecture due to the fourfold increase in minibatch size inside the network. While training we (unnecessarily) computed the mean posterior estimate for every training crop to monitor convergence, performed frequent test set evaluations, etc., which leaves room for optimizing the training speed.\n\nMasking-based training In our training runs with masking-based training (end of Section 4.1), we examine convergence by maintaining a smoothed network whose weights follow the trained network using an exponential moving average. This is a commonly used technique in semi-supervised learning (e.g., [33,1]) and in evaluating Generative Adversarial Networks (e.g., [3,16]), and removes the need for a learning rate rampdown -and thus deciding the training length in advanceto measure the results near a local minimum.\n\nAll curves in Figure 3 were generated by evaluating the test set using this exponentially smoothed network. We verified in separate tests that the results obtained this way were in line with the usual fixed-length training runs with learning rate rampdown. In these examples the noise model parameters were fixed but unknown for all algorithms. All PSNRs refer to individual images. We recommend zooming in to the images on a computer screen to better view the differences.\n\n\nB Additional result images\n\nIn this larger set of images we can discern some characteristic failure modes of our ablated setups. When the signal covariance \u03a3 x is forced to be diagonal (\"Our ablated, diag. \u03a3\"), we can see color artifacts on, e.g., rows 6 and 9 of Figure 5. The diagonal covariance matrix corresponds to having a univariate, independent distribution for each color channel, and therefore the network cannot express being, e.g., certain of hue but uncertain of luminance. This may let the color of noise leak through to the result, as seen in some of the images. With full \u03a3 x no such color leaking occurs. The ablation which discards information in center pixel entirely (\"Our ablated, \u00b5 only\") produces strong pixel-scale diamond/checkerboard artifacts, some of which can also be seen in the results of Krull et al. [19]. In images produced by our full, non-ablated method (\"Our\"), some slight checkerboarding may be seen in high-frequency areas, especially with impulse noise (see, e.g., Figure 7, bottom row). However, in most cases our results are visually indistinguishable from the baseline results.  \n\nFigure 4 :\n4Example result images for Poisson (top) and Impulse noise (bottom). PSNRs refer to the individual images.Figures 6 and 7show additional result images.\n\nFigures 5 ,\n56 and 7 show additional denoising results for Gaussian, Poisson, and impulse noise, respectively.\n\nFigure 7 :\n7Additional result images for impulse noise, \u03b1 = 0.5.\n\nTable 1 :\n1Image quality results for Gaussian noise. Values of \u03c3 are shown in 8-bit units.Noise type Method \n\u03c3 known? KODAK BSD300 SET14 Average \n\nGaussian \n\u03c3 = 25 \n\nBaseline, N2C \nno \n32.46 \n31.08 \n31.26 \n31.60 \nBaseline, N2N \nno \n32.45 \n31.07 \n31.23 \n31.58 \nOur \nyes \n32.45 \n31.03 \n31.25 \n31.57 \nOur \nno \n32.44 \n31.02 \n31.22 \n31.56 \nOur ablated, diag. \u03a3 \nyes \n31.60 \n29.91 \n30.58 \n30.70 \nOur ablated, diag. \u03a3 \nno \n31.55 \n29.87 \n30.53 \n30.65 \nOur ablated, \u00b5 only \nno \n30.64 \n28.65 \n29.57 \n29.62 \nCBM3D \nyes \n31.82 \n30.40 \n30.68 \n30.96 \nCBM3D \nno \n31.81 \n30.40 \n30.66 \n30.96 \n\nGaussian \n\u03c3 \u2208 [5, 50] \n\nBaseline, N2C \nno \n32.57 \n31.29 \n31.27 \n31.71 \nBaseline, N2N \nno \n32.57 \n31.29 \n31.26 \n31.70 \nOur \nyes \n32.47 \n31.19 \n31.21 \n31.62 \nOur \nno \n32.46 \n31.18 \n31.13 \n31.59 \nOur ablated, diag. \u03a3 \nyes \n31.59 \n30.06 \n30.54 \n30.73 \nOur ablated, diag. \u03a3 \nno \n31.58 \n30.05 \n30.45 \n30.69 \nOur ablated, \u00b5 only \nno \n30.54 \n28.56 \n29.41 \n29.50 \nCBM3D \nyes \n31.99 \n30.67 \n30.78 \n31.15 \nCBM3D \nno \n31.99 \n30.67 \n30.72 \n31.13 \n\n\n\nTable 2 :\n2Average output quality for Gaussian noise (\u03c3 = 25, known) with smaller training sets.Training images \n\n\nTable 3 :\n3Image quality results for Poisson and impulse noise.Noise type Method \n\u03bb/\u03b1 known? KODAK BSD300 SET14 Average \n\nPoisson \n\u03bb = 30 \n\nBaseline, N2C \nno \n31.81 \n30.40 \n30.45 \n30.89 \nBaseline, N2N \nno \n31.80 \n30.39 \n30.44 \n30.88 \nOur \nyes \n31.65 \n30.25 \n30.29 \n30.73 \nOur \nno \n31.70 \n30.28 \n30.35 \n30.78 \nOur ablated, \u00b5 only \nno \n30.22 \n28.27 \n29.03 \n29.17 \nAnscombe [25] (from [22]) \nyes \n29.15 \n27.56 \n28.36 \n28.62 \n\nPoisson \n\u03bb \u2208 [5, 50] \n\nBaseline, N2C \nno \n31.33 \n29.91 \n29.96 \n30.40 \nBaseline, N2N \nno \n31.32 \n29.90 \n29.96 \n30.39 \nOur \nyes \n31.16 \n29.75 \n29.82 \n30.24 \nOur \nno \n31.02 \n29.69 \n28.65 \n29.79 \nOur ablated, \u00b5 only \nno \n29.88 \n27.95 \n28.67 \n28.84 \n\nImpulse \n\u03b1 = 0.5 \n\nBaseline, N2C \nno \n33.32 \n31.20 \n31.42 \n31.98 \nBaseline, N2N \nno \n32.88 \n30.85 \n30.94 \n31.56 \nOur \nyes \n32.98 \n30.78 \n31.06 \n31.61 \nOur \nno \n32.93 \n30.71 \n31.09 \n31.57 \nOur ablated, \u00b5 only \nno \n30.82 \n28.52 \n29.05 \n29.46 \n\nImpulse \n\u03b1 \u2208 [0, 1] \n\nBaseline, N2C \nno \n31.69 \n30.27 \n29.77 \n30.58 \nBaseline, N2N \nno \n31.53 \n30.11 \n29.51 \n30.38 \nOur \nyes \n31.36 \n30.00 \n29.47 \n30.28 \nOur \nno \n31.40 \n29.98 \n29.51 \n30.29 \nOur ablated, \u00b5 only \nno \n27.16 \n25.55 \n25.56 \n26.09 \n\n\n\nTable 4 :\n4Network architecture used in our experiments. Layers marked with * are present only in the blind-spot variants. Layer NIN A has 384 output feature maps in the blind-spot networks and 96 in the baseline networks.NAME \nN out \nFUNCTION \n\nINPUT \n\n3 \n* ROTATE \n3 \nRotate and stack \n\nENC CONV0 \n\n48 \nConvolution 3 \u00d7 3 \n\nENC CONV1 \n\n48 \nConvolution 3 \u00d7 3 \n\nPOOL1 \n\n48 \nMaxpool 2 \u00d7 2 \n\nENC CONV2 \n\n48 \nConvolution 3 \u00d7 3 \n\nPOOL2 \n\n48 \nMaxpool 2 \u00d7 2 \n\nENC CONV3 \n\n48 \nConvolution 3 \u00d7 3 \n\nPOOL3 \n\n48 \nMaxpool 2 \u00d7 2 \n\nENC CONV4 \n\n48 \nConvolution 3 \u00d7 3 \n\nPOOL4 \n\n48 \nMaxpool 2 \u00d7 2 \n\nENC CONV5 \n\n48 \nConvolution 3 \u00d7 3 \n\nPOOL5 \n\n48 \nMaxpool 2 \u00d7 2 \n\nENC CONV6 \n\n48 \nConvolution 3 \u00d7 3 \n\nUPSAMPLE5 \n\n48 \nUpsample 2 \u00d7 2 \n\nCONCAT5 \n\n96 \nConcatenate output of POOL4 \n\nDEC CONV5A \n\n96 \nConvolution 3 \u00d7 3 \n\nDEC CONV5B \n\n96 \nConvolution 3 \u00d7 3 \n\nUPSAMPLE4 \n\n96 \nUpsample 2 \u00d7 2 \n\nCONCAT4 \n\n144 \nConcatenate output of POOL3 \n\nDEC CONV4A \n\n96 \nConvolution 3 \u00d7 3 \n\nDEC CONV4B \n\n96 \nConvolution 3 \u00d7 3 \n\nUPSAMPLE3 \n\n96 \nUpsample 2 \u00d7 2 \n\nCONCAT3 \n\n144 \nConcatenate output of POOL2 \n\nDEC CONV3A \n\n96 \nConvolution 3 \u00d7 3 \n\nDEC CONV3B \n\n96 \nConvolution 3 \u00d7 3 \n\nUPSAMPLE2 \n\n96 \nUpsample 2 \u00d7 2 \n\nCONCAT2 \n\n144 \nConcatenate output of POOL1 \n\nDEC CONV2A \n\n96 \nConvolution 3 \u00d7 3 \n\nDEC CONV2B \n\n96 \nConvolution 3 \u00d7 3 \n\nUPSAMPLE1 \n\n96 \nUpsample 2 \u00d7 2 \n\nCONCAT1 \n\n99 \nConcatenate INPUT \n\nDEC CONV1A \n\n96 \nConvolution 3 \u00d7 3 \n\nDEC CONV1B \n\n96 \nConvolution 3 \u00d7 3 \n* SHIFT \n96 \nShift down by one pixel \n* UNROTATE \n384 \nUnstack, rotate, combine \n\nNIN A \n\n384/96 \nConvolution 1 \u00d7 1 \n\nNIN B \n\n96 \nConvolution 1 \u00d7 1 \n\nNIN C \n\n9 \nConvolution 1 \u00d7 1, linear act. \n\n\n\n\nFigure 5: Additional result images for Gaussian noise, \u03c3 = 25.Figure 6: Additional result images for Poisson noise, \u03bb = 30.KODAK-6 \n\n20.41 dB \n31.17 dB \n31.17 dB \n30.06 dB \n29.04 dB \n30.59 dB \n\nKODAK-14 \n20.42 dB \n30.88 dB \n30.80 dB \n29.89 dB \n28.98 dB \n30.01 dB \n\nKODAK-4 \n20.35 dB \n33.14 dB \n33.09 dB \n32.47 dB \n31.86 dB \n32.66 dB \n\nBSD300-18 \n20.30 dB \n31.98 dB \n31.93 dB \n30.77 dB \n29.43 dB \n31.30 dB \n\nBSD300-22 \n20.58 dB \n28.63 dB \n28.57 dB \n26.95 dB \n25.07 dB \n28.00 dB \n\nBSD300-28 \n20.24 dB \n26.78 dB \n26.78 dB \n24.16 dB \n21.52 dB \n26.37 dB \n\nBSD300-80 \n20.20 dB \n32.75 dB \n32.64 dB \n31.81 dB \n31.05 dB \n32.03 dB \n\nSET14-2 \n20.36 dB \n31.60 dB \n31.59 dB \n31.30 dB \n30.69 dB \n32.06 dB \n\nSET14-5 \n20.51 dB \n29.44 dB \n29.40 dB \n28.34 dB \n26.92 dB \n28.09 dB \n\nTest image \nNoisy input N2C baseline \nOur \nOur ablated, Our ablated, \nCBM3D \ndiag. \u03a3 \n\u00b5 only \n\nKODAK-23 \n\n19.13 dB \n34.83 dB \n34.63 dB \n33.98 dB \n\nKODAK-8 \n18.63 dB \n29.12 dB \n29.11 dB \n27.25 dB \n\nBSD300-3 \n21.87 dB \n28.57 dB \n28.43 dB \n23.91 dB \n\nBSD300-7 \n17.81 dB \n29.39 dB \n29.26 dB \n27.01 dB \n\nBSD300-11 \n19.91 dB \n31.72 dB \n31.63 dB \n30.01 dB \n\nBSD300-60 \n18.10 dB \n29.62 dB \n29.61 dB \n27.43 dB \n\nBSD300-19 \n18.74 dB \n29.49 dB \n29.37 dB \n27.46 dB \n\nBSD300-21 \n19.33 dB \n31.97 dB \n31.81 dB \n30.24 dB \n\nBSD300-25 \n19.23 dB \n26.75 dB \n26.65 dB \n23.31 dB \n\nTest image \nNoisy input \nN2C baseline \nOur \nOur ablated, \n\u00b5 only \n\nKODAK-15 \n\n10.11 dB \n35.13 dB \n34.76 dB \n32.84 dB \n\nBSD300-4 \n11.13 dB \n34.41 dB \n33.72 dB \n31.86 dB \n\nBSD300-26 \n12.08 dB \n29.39 dB \n28.88 dB \n26.45 dB \n\nBSD300-51 \n10.96 dB \n31.36 dB \n30.74 dB \n28.49 dB \n\nBSD300-56 \n11.07 dB \n33.03 dB \n32.64 dB \n30.71 dB \n\nBSD300-95 \n10.31 dB \n32.41 dB \n31.92 dB \n29.61 dB \n\nSET14-1 \n11.19 dB \n23.96 dB \n23.98 dB \n21.81 dB \n\nKODAK-20 \n9.30 dB \n34.90 dB \n34.55 dB \n32.13 dB \n\nKODAK-19 \n12.09 dB \n33.62 dB \n33.35 dB \n31.08 dB \n\nTest image \nNoisy input \nN2C baseline \nOur \nOur ablated, \n\u00b5 only \n\n\nRegrettably the term \"blind spot\" has a slightly different meaning in PixelCNN literature: van den Oord et al.[35] use it to denote valid input pixels that the network in question fails to see due to poor design, whereas we follow the naming convention of Krull et al.[19] so that a blind spot is always intentional.\nEven though (grayscale) WNNM[11] has been shown to be superior to (grayscale) BM3D[9], our experiments with the official implementation of MCWNNM[37], a multi-channel version of WNNM, indicated that CBM3D performs better on our test data where all color channels have the same amount of noise.\nAcknowledgements We thank Arno Solin and Samuel Kaski for helpful comments, and Janne Hellsten and Tero Kuosmanen for the compute infrastructure.\nThere are many consistent explanations of unlabeled data: Why you should average. B Athiwaratkun, M Finzi, P Izmailov, A G Wilson, Proc. International Conference on Learning Representations (ICLR). International Conference on Learning Representations (ICLR)B. Athiwaratkun, M. Finzi, P. Izmailov, and A. G. Wilson. There are many consistent expla- nations of unlabeled data: Why you should average. In Proc. International Conference on Learning Representations (ICLR), 2019. 13\n\nNoise2Self: Blind denoising by self-supervision. J Batson, L Royer, Proc. International Conference on Machine Learning (ICML). International Conference on Machine Learning (ICML)36J. Batson and L. Royer. Noise2Self: Blind denoising by self-supervision. In Proc. International Conference on Machine Learning (ICML), pages 524-533, 2019. 3, 6\n\nLarge scale GAN training for high fidelity natural image synthesis. A Brock, J Donahue, K Simonyan, Proc. International Conference on Learning Representations (ICLR). International Conference on Learning Representations (ICLR)A. Brock, J. Donahue, and K. Simonyan. Large scale GAN training for high fidelity natural image synthesis. In Proc. International Conference on Learning Representations (ICLR), 2019. 13\n\nProducts and convolutions of Gaussian distributions. P A Bromiley, 2003- 00345Technical ReportP. A. Bromiley. Products and convolutions of Gaussian distributions. Technical Report 2003- 003, www.tina-vision.net, 2003. 4, 5\n\nA non-local algorithm for image denoising. A Buades, B Coll, J.-M Morel, Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)A. Buades, B. Coll, and J.-M. Morel. A non-local algorithm for image denoising. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 60-65, 2005. 9\n\nFully convolutional pixel adaptive image denoiser. S Cha, T Moon, abs/1807.07569CoRRS. Cha and T. Moon. Fully convolutional pixel adaptive image denoiser. CoRR, abs/1807.07569, 2018. 9\n\nAn efficient statistical method for image noise level estimation. G Chen, F Zhu, P Ann Heng, Proc. IEEE International Conference on Computer Vision (ICCV). IEEE International Conference on Computer Vision (ICCV)G. Chen, F. Zhu, and P. Ann Heng. An efficient statistical method for image noise level estimation. In Proc. IEEE International Conference on Computer Vision (ICCV), pages 477- 485, 2015. 6\n\nColor image denoising via sparse 3D collaborative filtering with grouping constraint in luminance-chrominance space. K Dabov, A Foi, V Katkovnik, K Egiazarian, Proc. IEEE International Conference on Image Processing. IEEE International Conference on Image essingK. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian. Color image denoising via sparse 3D collaborative filtering with grouping constraint in luminance-chrominance space. In Proc. IEEE International Conference on Image Processing, pages 313-316, 2007. 6\n\nImage denoising by sparse 3-D transformdomain collaborative filtering. K Dabov, A Foi, V Katkovnik, K Egiazarian, IEEE Transactions on Image Processing. 1689K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian. Image denoising by sparse 3-D transform- domain collaborative filtering. IEEE Transactions on Image Processing, 16(8):2080-2095, 2007. 6, 9\n\nUnsupervised visual representation learning by context prediction. C Doersch, A Gupta, A A Efros, Proc. International Conference on Computer Vision (ICCV). International Conference on Computer Vision (ICCV)C. Doersch, A. Gupta, and A. A. Efros. Unsupervised visual representation learning by context prediction. In Proc. International Conference on Computer Vision (ICCV), pages 1422-1430, 2015. 2\n\nWeighted nuclear norm minimization with application to image denoising. S Gu, L Zhang, W Zuo, X Feng, Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)69S. Gu, L. Zhang, W. Zuo, and X. Feng. Weighted nuclear norm minimization with application to image denoising. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2862-2869, 2014. 6, 9\n\nToward convolutional blind denoising of real photographs. S Guo, Z Yan, K Zhang, W Zuo, L Zhang, Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)S. Guo, Z. Yan, K. Zhang, W. Zuo, and L. Zhang. Toward convolutional blind denoising of real photographs. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1712-1722, 2019. 9\n\nComputer Vision: A Reference Guide. S W Hasinoff, K. Ikeuchi, editorSpringer USPhoton, Poisson noiseS. W. Hasinoff. Photon, Poisson noise. In K. Ikeuchi, editor, Computer Vision: A Reference Guide, pages 608-610. Springer US, 2014. 7\n\nDelving deep into rectifiers: Surpassing human-level performance on imagenet classification. K He, X Zhang, S Ren, J Sun, abs/1502.01852CoRR12K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. CoRR, abs/1502.01852, 2015. 12\n\nAn approach to image denoising using manifold approximation without clean images. CoRR, abs/1904.12323. R Jena, R. Jena. An approach to image denoising using manifold approximation without clean images. CoRR, abs/1904.12323, 2019. 9\n\nProgressive growing of GANs for improved quality, stability, and variation. T Karras, T Aila, S Laine, J Lehtinen, Proc. International Conference on Learning Representations (ICLR). International Conference on Learning Representations (ICLR)T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. Proc. International Conference on Learning Representations (ICLR), 2018. 13\n\nWhat uncertainties do we need in Bayesian deep learning for computer vision?. A Kendall, Y Gal, Advances in Neural Information Processing Systems 30 (Proc. NIPS). A. Kendall and Y. Gal. What uncertainties do we need in Bayesian deep learning for computer vision? In Advances in Neural Information Processing Systems 30 (Proc. NIPS), pages 5574- 5584. 2017. 4\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, Proc. International Conference on Learning Representations (ICLR). International Conference on Learning Representations (ICLR)12D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In Proc. International Conference on Learning Representations (ICLR), 2015. 12\n\nNoise2Void -Learning denoising from single noisy images. A Krull, T.-O Buchholz, F Jug, Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)613A. Krull, T.-O. Buchholz, and F. Jug. Noise2Void -Learning denoising from single noisy images. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2129-2137, 2019. 1, 2, 3, 6, 13\n\nProbabilistic Noise2Void: Unsupervised content-aware denoising. A Krull, T Vicar, F Jug, abs/1906.00651CoRRA. Krull, T. Vicar, and F. Jug. Probabilistic Noise2Void: Unsupervised content-aware denois- ing. CoRR, abs/1906.00651, 2019. 4\n\nHeteroscedastic Gaussian process regression. Q V Le, A J Smola, S Canu, Proc. International Conference on Machine Learning (ICML). International Conference on Machine Learning (ICML)Q. V. Le, A. J. Smola, and S. Canu. Heteroscedastic Gaussian process regression. In Proc. International Conference on Machine Learning (ICML), pages 489-496, 2005. 4\n\nNoise2Noise: Learning image restoration without clean data. J Lehtinen, J Munkberg, J Hasselgren, S Laine, T Karras, M Aittala, T Aila, Proc. International Conference on Machine Learning (ICML). International Conference on Machine Learning (ICML)911J. Lehtinen, J. Munkberg, J. Hasselgren, S. Laine, T. Karras, M. Aittala, and T. Aila. Noise2Noise: Learning image restoration without clean data. In Proc. International Con- ference on Machine Learning (ICML), 2018. 1, 3, 4, 5, 7, 8, 9, 11\n\nDeep learning with inaccurate training data for image restoration. B Liu, X Shu, X Wu, abs/1811.07268CoRRB. Liu, X. Shu, and X. Wu. Deep learning with inaccurate training data for image restoration. CoRR, abs/1811.07268, 2018. 9\n\nRectifier nonlinearities improve neural network acoustic models. A L Maas, A Y Hannun, A Ng, Proc. International Conference on Machine Learning (ICML). International Conference on Machine Learning (ICML)A. L. Maas, A. Y. Hannun, and A. Ng. Rectifier nonlinearities improve neural network acoustic models. In Proc. International Conference on Machine Learning (ICML), 2013. 11\n\nOptimal inversion of the Anscombe transformation in low-count Poisson image denoising. M M\u00e4kitalo, A Foi, IEEE Transactions on Image Processing. 201M. M\u00e4kitalo and A. Foi. Optimal inversion of the Anscombe transformation in low-count Poisson image denoising. IEEE Transactions on Image Processing, 20(1):99-109, 2011. 7, 8\n\nImage restoration using very deep convolutional encoderdecoder networks with symmetric skip connections. X Mao, C Shen, Y Yang, Advances in Neural Information Processing Systems 29 (Proc. NIPS). X. Mao, C. Shen, and Y. Yang. Image restoration using very deep convolutional encoder- decoder networks with symmetric skip connections. In Advances in Neural Information Pro- cessing Systems 29 (Proc. NIPS), pages 2802-2810. 2016. 1\n\nUnsupervised learning with Stein's unbiased risk estimator. C A Metzler, A Mousavi, R Heckel, R G Baraniuk, abs/1805.10531CoRRC. A. Metzler, A. Mousavi, R. Heckel, and R. G. Baraniuk. Unsupervised learning with Stein's unbiased risk estimator. CoRR, abs/1805.10531, 2018. 9\n\nEstimating the mean and variance of the target probability distribution. D A Nix, A S Weigend, Proc. IEEE International Conference on Neural Networks (ICNN). IEEE International Conference on Neural Networks (ICNN)D. A. Nix and A. S. Weigend. Estimating the mean and variance of the target probability distribution. Proc. IEEE International Conference on Neural Networks (ICNN), pages 55-60, 1994. 4\n\nU-Net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, Medical Image Computing and Computer-Assisted Intervention (MICCAI). 935111O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolutional networks for biomedical im- age segmentation. Medical Image Computing and Computer-Assisted Intervention (MICCAI), 9351:234-241, 2015. 1, 4, 11\n\nPixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modifications. T Salimans, A Karpathy, X Chen, D P Kingma, Proc. International Conference on Learning Representations (ICLR. International Conference on Learning Representations (ICLRT. Salimans, A. Karpathy, X. Chen, and D. P. Kingma. PixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modifications. In Proc. International Conference on Learning Representations (ICLR), 2017. 2\n\nZero-shot\" super-resolution using deep internal learning. A Shocher, N Cohen, M Irani, Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)A. Shocher, N. Cohen, and M. Irani. \"Zero-shot\" super-resolution using deep internal learning. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3118- 3126, 2018. 9\n\nTraining deep learning based denoisers without ground truth data. S Soltanayev, S Y Chun, Advances in Neural Information Processing Systems 31 (Proc. NeurIPS). S. Soltanayev and S. Y. Chun. Training deep learning based denoisers without ground truth data. In Advances in Neural Information Processing Systems 31 (Proc. NeurIPS), pages 3257- 3267. 2018. 9\n\nMean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. A Tarvainen, H Valpola, Proc. Advances in Neural Information Processing Systems 30 (NIPS). Advances in Neural Information essing Systems 30 (NIPS)13A. Tarvainen and H. Valpola. Mean teachers are better role models: Weight-averaged con- sistency targets improve semi-supervised deep learning results. In Proc. Advances in Neural Information Processing Systems 30 (NIPS), pages 1195-1204. 2017. 13\n\nDeep image prior. D Ulyanov, A Vedaldi, V S Lempitsky, Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Conference on Computer Vision and Pattern Recognition (CVPR)D. Ulyanov, A. Vedaldi, and V. S. Lempitsky. Deep image prior. In Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 9446-9454, 2018. 9\n\nConditional image generation with PixelCNN decoders. A Van Den Oord, N Kalchbrenner, L Espeholt, K Kavukcuoglu, O Vinyals, A Graves, Advances in Neural Information Processing Systems 29 (Proc. NIPS). A. van den Oord, N. Kalchbrenner, L. Espeholt, K. Kavukcuoglu, O. Vinyals, and A. Graves. Conditional image generation with PixelCNN decoders. In Advances in Neural Information Processing Systems 29 (Proc. NIPS), pages 4790-4798. 2016. 2\n\nPixel recurrent neural networks. A Van Den Oord, N Kalchbrenner, K Kavukcuoglu, Proc. International Conference on Machine Learning (ICML). International Conference on Machine Learning (ICML)A. van den Oord, N. Kalchbrenner, and K. Kavukcuoglu. Pixel recurrent neural networks. In Proc. International Conference on Machine Learning (ICML), pages 1747-1756, 2016. 2\n\nMulti-channel weighted nuclear norm minimization for real color image denoising. J Xu, L Zhang, D Zhang, X Feng, Proc. IEEE International Conference on Computer Vision (ICCV). IEEE International Conference on Computer Vision (ICCV)J. Xu, L. Zhang, D. Zhang, and X. Feng. Multi-channel weighted nuclear norm minimization for real color image denoising. In Proc. IEEE International Conference on Computer Vision (ICCV), pages 1105-1113, 2017. 6\n\nResidual dense network for image restoration. Y Zhang, Y Tian, Y Kong, B Zhong, Y Fu, abs/1812.10477CoRRY. Zhang, Y. Tian, Y. Kong, B. Zhong, and Y. Fu. Residual dense network for image restoration. CoRR, abs/1812.10477, 2018. 1\n", "annotations": {"author": "[{\"end\":113,\"start\":53},{\"end\":166,\"start\":114},{\"end\":223,\"start\":167},{\"end\":274,\"start\":224}]", "publisher": null, "author_last_name": "[{\"end\":72,\"start\":66},{\"end\":125,\"start\":119},{\"end\":182,\"start\":174},{\"end\":233,\"start\":229}]", "author_first_name": "[{\"end\":59,\"start\":53},{\"end\":65,\"start\":60},{\"end\":118,\"start\":114},{\"end\":173,\"start\":167},{\"end\":228,\"start\":224}]", "author_affiliation": "[{\"end\":112,\"start\":74},{\"end\":165,\"start\":127},{\"end\":222,\"start\":184},{\"end\":273,\"start\":235}]", "title": "[{\"end\":50,\"start\":1},{\"end\":324,\"start\":275}]", "venue": null, "abstract": "[{\"end\":1193,\"start\":326}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b28\"},\"end\":1398,\"start\":1394},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":1435,\"start\":1431},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":1469,\"start\":1465},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":1668,\"start\":1664},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3255,\"start\":3251},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4639,\"start\":4635},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":4680,\"start\":4676},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":5763,\"start\":5759},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5766,\"start\":5763},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5769,\"start\":5766},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5888,\"start\":5887},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5980,\"start\":5976},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9176,\"start\":9172},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9323,\"start\":9319},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9483,\"start\":9480},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12738,\"start\":12734},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":13194,\"start\":13190},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13236,\"start\":13232},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14705,\"start\":14702},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15737,\"start\":15733},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":15740,\"start\":15737},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15743,\"start\":15740},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":16886,\"start\":16883},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":17928,\"start\":17924},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18317,\"start\":18313},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":19363,\"start\":19359},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":19764,\"start\":19761},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19824,\"start\":19823},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":20020,\"start\":20017},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":21559,\"start\":21555},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":21868,\"start\":21864},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":21977,\"start\":21974},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":23843,\"start\":23839},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":26064,\"start\":26060},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":26206,\"start\":26202},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":28395,\"start\":28391},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":29256,\"start\":29253},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":29266,\"start\":29263},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":29281,\"start\":29277},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":29401,\"start\":29397},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":30006,\"start\":30002},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":30009,\"start\":30006},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":30119,\"start\":30115},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":30405,\"start\":30402},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":31159,\"start\":31155},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":31162,\"start\":31159},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":31165,\"start\":31162},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":31762,\"start\":31758},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":31813,\"start\":31809},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":32001,\"start\":31997},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":33079,\"start\":33075},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":33131,\"start\":33127},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":35555,\"start\":35551},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":35557,\"start\":35555},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":35619,\"start\":35616},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":35622,\"start\":35619},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":37083,\"start\":37079},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":43658,\"start\":43654},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":43816,\"start\":43812},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":43893,\"start\":43889},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":43946,\"start\":43943},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":44010,\"start\":44006}]", "figure": "[{\"attributes\":{\"id\":\"fig_2\"},\"end\":37533,\"start\":37370},{\"attributes\":{\"id\":\"fig_3\"},\"end\":37645,\"start\":37534},{\"attributes\":{\"id\":\"fig_4\"},\"end\":37711,\"start\":37646},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":38725,\"start\":37712},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":38840,\"start\":38726},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":39998,\"start\":38841},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":41622,\"start\":39999},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":43543,\"start\":41623}]", "paragraph": "[{\"end\":1646,\"start\":1209},{\"end\":2280,\"start\":1648},{\"end\":3167,\"start\":2282},{\"end\":4640,\"start\":3169},{\"end\":5246,\"start\":4642},{\"end\":6071,\"start\":5297},{\"end\":6570,\"start\":6073},{\"end\":7120,\"start\":6572},{\"end\":8138,\"start\":7122},{\"end\":8201,\"start\":8140},{\"end\":9039,\"start\":8203},{\"end\":9715,\"start\":9041},{\"end\":10082,\"start\":9717},{\"end\":10704,\"start\":10155},{\"end\":11003,\"start\":10706},{\"end\":11764,\"start\":11067},{\"end\":11900,\"start\":11766},{\"end\":12067,\"start\":11902},{\"end\":12699,\"start\":12069},{\"end\":12979,\"start\":12701},{\"end\":13674,\"start\":13005},{\"end\":13967,\"start\":13702},{\"end\":14160,\"start\":13969},{\"end\":14452,\"start\":14177},{\"end\":14717,\"start\":14454},{\"end\":15598,\"start\":14754},{\"end\":15744,\"start\":15600},{\"end\":16453,\"start\":15855},{\"end\":16643,\"start\":16455},{\"end\":16929,\"start\":16701},{\"end\":17147,\"start\":16998},{\"end\":17521,\"start\":17149},{\"end\":18039,\"start\":17523},{\"end\":18338,\"start\":18041},{\"end\":19524,\"start\":18340},{\"end\":19675,\"start\":19526},{\"end\":20154,\"start\":19677},{\"end\":21220,\"start\":20156},{\"end\":21978,\"start\":21222},{\"end\":22183,\"start\":21980},{\"end\":22978,\"start\":22185},{\"end\":23391,\"start\":22980},{\"end\":23941,\"start\":23409},{\"end\":24183,\"start\":23988},{\"end\":24678,\"start\":24185},{\"end\":25364,\"start\":24680},{\"end\":26208,\"start\":25366},{\"end\":26942,\"start\":26226},{\"end\":27265,\"start\":26989},{\"end\":27422,\"start\":27382},{\"end\":28068,\"start\":27512},{\"end\":29144,\"start\":28070},{\"end\":29907,\"start\":29175},{\"end\":30604,\"start\":29909},{\"end\":31555,\"start\":30606},{\"end\":32089,\"start\":31557},{\"end\":32999,\"start\":32091},{\"end\":33682,\"start\":33001},{\"end\":34068,\"start\":33684},{\"end\":34748,\"start\":34070},{\"end\":35251,\"start\":34750},{\"end\":35768,\"start\":35253},{\"end\":36243,\"start\":35770},{\"end\":37369,\"start\":36274}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10154,\"start\":10083},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11066,\"start\":11004},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14176,\"start\":14161},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14753,\"start\":14718},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15854,\"start\":15745},{\"attributes\":{\"id\":\"formula_5\"},\"end\":16700,\"start\":16644},{\"attributes\":{\"id\":\"formula_6\"},\"end\":16997,\"start\":16930},{\"attributes\":{\"id\":\"formula_7\"},\"end\":23987,\"start\":23942},{\"attributes\":{\"id\":\"formula_8\"},\"end\":26988,\"start\":26943},{\"attributes\":{\"id\":\"formula_9\"},\"end\":27381,\"start\":27266},{\"attributes\":{\"id\":\"formula_10\"},\"end\":27511,\"start\":27423}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":17556,\"start\":17549},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":19000,\"start\":18993},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":20181,\"start\":20174},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":20580,\"start\":20573},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24699,\"start\":24692},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":28089,\"start\":28082},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":31620,\"start\":31613}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1207,\"start\":1195},{\"attributes\":{\"n\":\"2\"},\"end\":5295,\"start\":5249},{\"attributes\":{\"n\":\"4\"},\"end\":13003,\"start\":12982},{\"attributes\":{\"n\":\"4.1\"},\"end\":13700,\"start\":13677},{\"attributes\":{\"n\":\"4.2\"},\"end\":23407,\"start\":23394},{\"attributes\":{\"n\":\"4.3\"},\"end\":26224,\"start\":26211},{\"attributes\":{\"n\":\"5\"},\"end\":29173,\"start\":29147},{\"end\":36272,\"start\":36246},{\"end\":37381,\"start\":37371},{\"end\":37546,\"start\":37535},{\"end\":37657,\"start\":37647},{\"end\":37722,\"start\":37713},{\"end\":38736,\"start\":38727},{\"end\":38851,\"start\":38842},{\"end\":40009,\"start\":40000}]", "table": "[{\"end\":38725,\"start\":37803},{\"end\":38840,\"start\":38823},{\"end\":39998,\"start\":38905},{\"end\":41622,\"start\":40222},{\"end\":43543,\"start\":41748}]", "figure_caption": "[{\"end\":37533,\"start\":37383},{\"end\":37645,\"start\":37548},{\"end\":37711,\"start\":37659},{\"end\":37803,\"start\":37724},{\"end\":38823,\"start\":38738},{\"end\":38905,\"start\":38853},{\"end\":40222,\"start\":40011},{\"end\":41748,\"start\":41625}]", "figure_ref": "[{\"end\":3601,\"start\":3593},{\"end\":5453,\"start\":5445},{\"end\":17677,\"start\":17669},{\"end\":18940,\"start\":18932},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19085,\"start\":19077},{\"end\":22182,\"start\":22174},{\"end\":23390,\"start\":23382},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24775,\"start\":24767},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":28184,\"start\":28176},{\"end\":35792,\"start\":35784},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":36518,\"start\":36510},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":37260,\"start\":37252}]", "bib_author_first_name": "[{\"end\":44384,\"start\":44383},{\"end\":44400,\"start\":44399},{\"end\":44409,\"start\":44408},{\"end\":44421,\"start\":44420},{\"end\":44423,\"start\":44422},{\"end\":44830,\"start\":44829},{\"end\":44840,\"start\":44839},{\"end\":45191,\"start\":45190},{\"end\":45200,\"start\":45199},{\"end\":45211,\"start\":45210},{\"end\":45589,\"start\":45588},{\"end\":45591,\"start\":45590},{\"end\":45803,\"start\":45802},{\"end\":45813,\"start\":45812},{\"end\":45824,\"start\":45820},{\"end\":46200,\"start\":46199},{\"end\":46207,\"start\":46206},{\"end\":46401,\"start\":46400},{\"end\":46409,\"start\":46408},{\"end\":46416,\"start\":46415},{\"end\":46420,\"start\":46417},{\"end\":46854,\"start\":46853},{\"end\":46863,\"start\":46862},{\"end\":46870,\"start\":46869},{\"end\":46883,\"start\":46882},{\"end\":47322,\"start\":47321},{\"end\":47331,\"start\":47330},{\"end\":47338,\"start\":47337},{\"end\":47351,\"start\":47350},{\"end\":47667,\"start\":47666},{\"end\":47678,\"start\":47677},{\"end\":47687,\"start\":47686},{\"end\":47689,\"start\":47688},{\"end\":48071,\"start\":48070},{\"end\":48077,\"start\":48076},{\"end\":48086,\"start\":48085},{\"end\":48093,\"start\":48092},{\"end\":48514,\"start\":48513},{\"end\":48521,\"start\":48520},{\"end\":48528,\"start\":48527},{\"end\":48537,\"start\":48536},{\"end\":48544,\"start\":48543},{\"end\":48935,\"start\":48934},{\"end\":48937,\"start\":48936},{\"end\":49227,\"start\":49226},{\"end\":49233,\"start\":49232},{\"end\":49242,\"start\":49241},{\"end\":49249,\"start\":49248},{\"end\":49542,\"start\":49541},{\"end\":49748,\"start\":49747},{\"end\":49758,\"start\":49757},{\"end\":49766,\"start\":49765},{\"end\":49775,\"start\":49774},{\"end\":50191,\"start\":50190},{\"end\":50202,\"start\":50201},{\"end\":50517,\"start\":50516},{\"end\":50519,\"start\":50518},{\"end\":50529,\"start\":50528},{\"end\":50868,\"start\":50867},{\"end\":50880,\"start\":50876},{\"end\":50892,\"start\":50891},{\"end\":51314,\"start\":51313},{\"end\":51323,\"start\":51322},{\"end\":51332,\"start\":51331},{\"end\":51531,\"start\":51530},{\"end\":51533,\"start\":51532},{\"end\":51539,\"start\":51538},{\"end\":51541,\"start\":51540},{\"end\":51550,\"start\":51549},{\"end\":51895,\"start\":51894},{\"end\":51907,\"start\":51906},{\"end\":51919,\"start\":51918},{\"end\":51933,\"start\":51932},{\"end\":51942,\"start\":51941},{\"end\":51952,\"start\":51951},{\"end\":51963,\"start\":51962},{\"end\":52393,\"start\":52392},{\"end\":52400,\"start\":52399},{\"end\":52407,\"start\":52406},{\"end\":52621,\"start\":52620},{\"end\":52623,\"start\":52622},{\"end\":52631,\"start\":52630},{\"end\":52633,\"start\":52632},{\"end\":52643,\"start\":52642},{\"end\":53020,\"start\":53019},{\"end\":53032,\"start\":53031},{\"end\":53362,\"start\":53361},{\"end\":53369,\"start\":53368},{\"end\":53377,\"start\":53376},{\"end\":53747,\"start\":53746},{\"end\":53749,\"start\":53748},{\"end\":53760,\"start\":53759},{\"end\":53771,\"start\":53770},{\"end\":53781,\"start\":53780},{\"end\":53783,\"start\":53782},{\"end\":54035,\"start\":54034},{\"end\":54037,\"start\":54036},{\"end\":54044,\"start\":54043},{\"end\":54046,\"start\":54045},{\"end\":54427,\"start\":54426},{\"end\":54442,\"start\":54441},{\"end\":54453,\"start\":54452},{\"end\":54848,\"start\":54847},{\"end\":54860,\"start\":54859},{\"end\":54872,\"start\":54871},{\"end\":54880,\"start\":54879},{\"end\":54882,\"start\":54881},{\"end\":55311,\"start\":55310},{\"end\":55322,\"start\":55321},{\"end\":55331,\"start\":55330},{\"end\":55742,\"start\":55741},{\"end\":55756,\"start\":55755},{\"end\":55758,\"start\":55757},{\"end\":56153,\"start\":56152},{\"end\":56166,\"start\":56165},{\"end\":56568,\"start\":56567},{\"end\":56579,\"start\":56578},{\"end\":56590,\"start\":56589},{\"end\":56592,\"start\":56591},{\"end\":56961,\"start\":56960},{\"end\":56977,\"start\":56976},{\"end\":56993,\"start\":56992},{\"end\":57005,\"start\":57004},{\"end\":57020,\"start\":57019},{\"end\":57031,\"start\":57030},{\"end\":57380,\"start\":57379},{\"end\":57396,\"start\":57395},{\"end\":57412,\"start\":57411},{\"end\":57793,\"start\":57792},{\"end\":57799,\"start\":57798},{\"end\":57808,\"start\":57807},{\"end\":57817,\"start\":57816},{\"end\":58202,\"start\":58201},{\"end\":58211,\"start\":58210},{\"end\":58219,\"start\":58218},{\"end\":58227,\"start\":58226},{\"end\":58236,\"start\":58235}]", "bib_author_last_name": "[{\"end\":44397,\"start\":44385},{\"end\":44406,\"start\":44401},{\"end\":44418,\"start\":44410},{\"end\":44430,\"start\":44424},{\"end\":44837,\"start\":44831},{\"end\":44846,\"start\":44841},{\"end\":45197,\"start\":45192},{\"end\":45208,\"start\":45201},{\"end\":45220,\"start\":45212},{\"end\":45600,\"start\":45592},{\"end\":45810,\"start\":45804},{\"end\":45818,\"start\":45814},{\"end\":45830,\"start\":45825},{\"end\":46204,\"start\":46201},{\"end\":46212,\"start\":46208},{\"end\":46406,\"start\":46402},{\"end\":46413,\"start\":46410},{\"end\":46425,\"start\":46421},{\"end\":46860,\"start\":46855},{\"end\":46867,\"start\":46864},{\"end\":46880,\"start\":46871},{\"end\":46894,\"start\":46884},{\"end\":47328,\"start\":47323},{\"end\":47335,\"start\":47332},{\"end\":47348,\"start\":47339},{\"end\":47362,\"start\":47352},{\"end\":47675,\"start\":47668},{\"end\":47684,\"start\":47679},{\"end\":47695,\"start\":47690},{\"end\":48074,\"start\":48072},{\"end\":48083,\"start\":48078},{\"end\":48090,\"start\":48087},{\"end\":48098,\"start\":48094},{\"end\":48518,\"start\":48515},{\"end\":48525,\"start\":48522},{\"end\":48534,\"start\":48529},{\"end\":48541,\"start\":48538},{\"end\":48550,\"start\":48545},{\"end\":48946,\"start\":48938},{\"end\":49230,\"start\":49228},{\"end\":49239,\"start\":49234},{\"end\":49246,\"start\":49243},{\"end\":49253,\"start\":49250},{\"end\":49547,\"start\":49543},{\"end\":49755,\"start\":49749},{\"end\":49763,\"start\":49759},{\"end\":49772,\"start\":49767},{\"end\":49784,\"start\":49776},{\"end\":50199,\"start\":50192},{\"end\":50206,\"start\":50203},{\"end\":50526,\"start\":50520},{\"end\":50532,\"start\":50530},{\"end\":50874,\"start\":50869},{\"end\":50889,\"start\":50881},{\"end\":50896,\"start\":50893},{\"end\":51320,\"start\":51315},{\"end\":51329,\"start\":51324},{\"end\":51336,\"start\":51333},{\"end\":51536,\"start\":51534},{\"end\":51547,\"start\":51542},{\"end\":51555,\"start\":51551},{\"end\":51904,\"start\":51896},{\"end\":51916,\"start\":51908},{\"end\":51930,\"start\":51920},{\"end\":51939,\"start\":51934},{\"end\":51949,\"start\":51943},{\"end\":51960,\"start\":51953},{\"end\":51968,\"start\":51964},{\"end\":52397,\"start\":52394},{\"end\":52404,\"start\":52401},{\"end\":52410,\"start\":52408},{\"end\":52628,\"start\":52624},{\"end\":52640,\"start\":52634},{\"end\":52646,\"start\":52644},{\"end\":53029,\"start\":53021},{\"end\":53036,\"start\":53033},{\"end\":53366,\"start\":53363},{\"end\":53374,\"start\":53370},{\"end\":53382,\"start\":53378},{\"end\":53757,\"start\":53750},{\"end\":53768,\"start\":53761},{\"end\":53778,\"start\":53772},{\"end\":53792,\"start\":53784},{\"end\":54041,\"start\":54038},{\"end\":54054,\"start\":54047},{\"end\":54439,\"start\":54428},{\"end\":54450,\"start\":54443},{\"end\":54458,\"start\":54454},{\"end\":54857,\"start\":54849},{\"end\":54869,\"start\":54861},{\"end\":54877,\"start\":54873},{\"end\":54889,\"start\":54883},{\"end\":55319,\"start\":55312},{\"end\":55328,\"start\":55323},{\"end\":55337,\"start\":55332},{\"end\":55753,\"start\":55743},{\"end\":55763,\"start\":55759},{\"end\":56163,\"start\":56154},{\"end\":56174,\"start\":56167},{\"end\":56576,\"start\":56569},{\"end\":56587,\"start\":56580},{\"end\":56602,\"start\":56593},{\"end\":56974,\"start\":56962},{\"end\":56990,\"start\":56978},{\"end\":57002,\"start\":56994},{\"end\":57017,\"start\":57006},{\"end\":57028,\"start\":57021},{\"end\":57038,\"start\":57032},{\"end\":57393,\"start\":57381},{\"end\":57409,\"start\":57397},{\"end\":57424,\"start\":57413},{\"end\":57796,\"start\":57794},{\"end\":57805,\"start\":57800},{\"end\":57814,\"start\":57809},{\"end\":57822,\"start\":57818},{\"end\":58208,\"start\":58203},{\"end\":58216,\"start\":58212},{\"end\":58224,\"start\":58220},{\"end\":58233,\"start\":58228},{\"end\":58239,\"start\":58237}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":108297336},\"end\":44778,\"start\":44301},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":59523708},\"end\":45120,\"start\":44780},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":52889459},\"end\":45533,\"start\":45122},{\"attributes\":{\"doi\":\"2003- 003\",\"id\":\"b3\"},\"end\":45757,\"start\":45535},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":11206708},\"end\":46146,\"start\":45759},{\"attributes\":{\"doi\":\"abs/1807.07569\",\"id\":\"b5\"},\"end\":46332,\"start\":46148},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":206770723},\"end\":46734,\"start\":46334},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1887989},\"end\":47248,\"start\":46736},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1475121},\"end\":47597,\"start\":47250},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":9062671},\"end\":47996,\"start\":47599},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":1663191},\"end\":48453,\"start\":47998},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":49672261},\"end\":48896,\"start\":48455},{\"attributes\":{\"id\":\"b12\"},\"end\":49131,\"start\":48898},{\"attributes\":{\"doi\":\"abs/1502.01852\",\"id\":\"b13\"},\"end\":49435,\"start\":49133},{\"attributes\":{\"id\":\"b14\"},\"end\":49669,\"start\":49437},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":3568073},\"end\":50110,\"start\":49671},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":71134},\"end\":50470,\"start\":50112},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":6628106},\"end\":50808,\"start\":50472},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":53751136},\"end\":51247,\"start\":50810},{\"attributes\":{\"doi\":\"abs/1906.00651\",\"id\":\"b19\"},\"end\":51483,\"start\":51249},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":2891780},\"end\":51832,\"start\":51485},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":3846544},\"end\":52323,\"start\":51834},{\"attributes\":{\"doi\":\"abs/1811.07268\",\"id\":\"b22\"},\"end\":52553,\"start\":52325},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":16489696},\"end\":52930,\"start\":52555},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":10229455},\"end\":53254,\"start\":52932},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":10987457},\"end\":53684,\"start\":53256},{\"attributes\":{\"doi\":\"abs/1805.10531\",\"id\":\"b26\"},\"end\":53959,\"start\":53686},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":117583961},\"end\":54359,\"start\":53961},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":3719281},\"end\":54740,\"start\":54361},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":12663716},\"end\":55250,\"start\":54742},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":215825382},\"end\":55673,\"start\":55252},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":3707854},\"end\":56029,\"start\":55675},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":2759724},\"end\":56547,\"start\":56031},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":4531078},\"end\":56905,\"start\":56549},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":14989939},\"end\":57344,\"start\":56907},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":8142135},\"end\":57709,\"start\":57346},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":26598341},\"end\":58153,\"start\":57711},{\"attributes\":{\"doi\":\"abs/1812.10477\",\"id\":\"b37\"},\"end\":58383,\"start\":58155}]", "bib_title": "[{\"end\":44381,\"start\":44301},{\"end\":44827,\"start\":44780},{\"end\":45188,\"start\":45122},{\"end\":45800,\"start\":45759},{\"end\":46398,\"start\":46334},{\"end\":46851,\"start\":46736},{\"end\":47319,\"start\":47250},{\"end\":47664,\"start\":47599},{\"end\":48068,\"start\":47998},{\"end\":48511,\"start\":48455},{\"end\":49745,\"start\":49671},{\"end\":50188,\"start\":50112},{\"end\":50514,\"start\":50472},{\"end\":50865,\"start\":50810},{\"end\":51528,\"start\":51485},{\"end\":51892,\"start\":51834},{\"end\":52618,\"start\":52555},{\"end\":53017,\"start\":52932},{\"end\":53359,\"start\":53256},{\"end\":54032,\"start\":53961},{\"end\":54424,\"start\":54361},{\"end\":54845,\"start\":54742},{\"end\":55308,\"start\":55252},{\"end\":55739,\"start\":55675},{\"end\":56150,\"start\":56031},{\"end\":56565,\"start\":56549},{\"end\":56958,\"start\":56907},{\"end\":57377,\"start\":57346},{\"end\":57790,\"start\":57711}]", "bib_author": "[{\"end\":44399,\"start\":44383},{\"end\":44408,\"start\":44399},{\"end\":44420,\"start\":44408},{\"end\":44432,\"start\":44420},{\"end\":44839,\"start\":44829},{\"end\":44848,\"start\":44839},{\"end\":45199,\"start\":45190},{\"end\":45210,\"start\":45199},{\"end\":45222,\"start\":45210},{\"end\":45602,\"start\":45588},{\"end\":45812,\"start\":45802},{\"end\":45820,\"start\":45812},{\"end\":45832,\"start\":45820},{\"end\":46206,\"start\":46199},{\"end\":46214,\"start\":46206},{\"end\":46408,\"start\":46400},{\"end\":46415,\"start\":46408},{\"end\":46427,\"start\":46415},{\"end\":46862,\"start\":46853},{\"end\":46869,\"start\":46862},{\"end\":46882,\"start\":46869},{\"end\":46896,\"start\":46882},{\"end\":47330,\"start\":47321},{\"end\":47337,\"start\":47330},{\"end\":47350,\"start\":47337},{\"end\":47364,\"start\":47350},{\"end\":47677,\"start\":47666},{\"end\":47686,\"start\":47677},{\"end\":47697,\"start\":47686},{\"end\":48076,\"start\":48070},{\"end\":48085,\"start\":48076},{\"end\":48092,\"start\":48085},{\"end\":48100,\"start\":48092},{\"end\":48520,\"start\":48513},{\"end\":48527,\"start\":48520},{\"end\":48536,\"start\":48527},{\"end\":48543,\"start\":48536},{\"end\":48552,\"start\":48543},{\"end\":48948,\"start\":48934},{\"end\":49232,\"start\":49226},{\"end\":49241,\"start\":49232},{\"end\":49248,\"start\":49241},{\"end\":49255,\"start\":49248},{\"end\":49549,\"start\":49541},{\"end\":49757,\"start\":49747},{\"end\":49765,\"start\":49757},{\"end\":49774,\"start\":49765},{\"end\":49786,\"start\":49774},{\"end\":50201,\"start\":50190},{\"end\":50208,\"start\":50201},{\"end\":50528,\"start\":50516},{\"end\":50534,\"start\":50528},{\"end\":50876,\"start\":50867},{\"end\":50891,\"start\":50876},{\"end\":50898,\"start\":50891},{\"end\":51322,\"start\":51313},{\"end\":51331,\"start\":51322},{\"end\":51338,\"start\":51331},{\"end\":51538,\"start\":51530},{\"end\":51549,\"start\":51538},{\"end\":51557,\"start\":51549},{\"end\":51906,\"start\":51894},{\"end\":51918,\"start\":51906},{\"end\":51932,\"start\":51918},{\"end\":51941,\"start\":51932},{\"end\":51951,\"start\":51941},{\"end\":51962,\"start\":51951},{\"end\":51970,\"start\":51962},{\"end\":52399,\"start\":52392},{\"end\":52406,\"start\":52399},{\"end\":52412,\"start\":52406},{\"end\":52630,\"start\":52620},{\"end\":52642,\"start\":52630},{\"end\":52648,\"start\":52642},{\"end\":53031,\"start\":53019},{\"end\":53038,\"start\":53031},{\"end\":53368,\"start\":53361},{\"end\":53376,\"start\":53368},{\"end\":53384,\"start\":53376},{\"end\":53759,\"start\":53746},{\"end\":53770,\"start\":53759},{\"end\":53780,\"start\":53770},{\"end\":53794,\"start\":53780},{\"end\":54043,\"start\":54034},{\"end\":54056,\"start\":54043},{\"end\":54441,\"start\":54426},{\"end\":54452,\"start\":54441},{\"end\":54460,\"start\":54452},{\"end\":54859,\"start\":54847},{\"end\":54871,\"start\":54859},{\"end\":54879,\"start\":54871},{\"end\":54891,\"start\":54879},{\"end\":55321,\"start\":55310},{\"end\":55330,\"start\":55321},{\"end\":55339,\"start\":55330},{\"end\":55755,\"start\":55741},{\"end\":55765,\"start\":55755},{\"end\":56165,\"start\":56152},{\"end\":56176,\"start\":56165},{\"end\":56578,\"start\":56567},{\"end\":56589,\"start\":56578},{\"end\":56604,\"start\":56589},{\"end\":56976,\"start\":56960},{\"end\":56992,\"start\":56976},{\"end\":57004,\"start\":56992},{\"end\":57019,\"start\":57004},{\"end\":57030,\"start\":57019},{\"end\":57040,\"start\":57030},{\"end\":57395,\"start\":57379},{\"end\":57411,\"start\":57395},{\"end\":57426,\"start\":57411},{\"end\":57798,\"start\":57792},{\"end\":57807,\"start\":57798},{\"end\":57816,\"start\":57807},{\"end\":57824,\"start\":57816},{\"end\":58210,\"start\":58201},{\"end\":58218,\"start\":58210},{\"end\":58226,\"start\":58218},{\"end\":58235,\"start\":58226},{\"end\":58241,\"start\":58235}]", "bib_venue": "[{\"end\":44497,\"start\":44432},{\"end\":44905,\"start\":44848},{\"end\":45287,\"start\":45222},{\"end\":45586,\"start\":45535},{\"end\":45903,\"start\":45832},{\"end\":46197,\"start\":46148},{\"end\":46488,\"start\":46427},{\"end\":46951,\"start\":46896},{\"end\":47401,\"start\":47364},{\"end\":47753,\"start\":47697},{\"end\":48171,\"start\":48100},{\"end\":48623,\"start\":48552},{\"end\":48932,\"start\":48898},{\"end\":49224,\"start\":49133},{\"end\":49539,\"start\":49437},{\"end\":49851,\"start\":49786},{\"end\":50273,\"start\":50208},{\"end\":50599,\"start\":50534},{\"end\":50969,\"start\":50898},{\"end\":51311,\"start\":51249},{\"end\":51614,\"start\":51557},{\"end\":52027,\"start\":51970},{\"end\":52390,\"start\":52325},{\"end\":52705,\"start\":52648},{\"end\":53075,\"start\":53038},{\"end\":53449,\"start\":53384},{\"end\":53744,\"start\":53686},{\"end\":54117,\"start\":54056},{\"end\":54527,\"start\":54460},{\"end\":54955,\"start\":54891},{\"end\":55410,\"start\":55339},{\"end\":55833,\"start\":55765},{\"end\":56241,\"start\":56176},{\"end\":56675,\"start\":56604},{\"end\":57105,\"start\":57040},{\"end\":57483,\"start\":57426},{\"end\":57885,\"start\":57824},{\"end\":58199,\"start\":58155},{\"end\":44558,\"start\":44499},{\"end\":44958,\"start\":44907},{\"end\":45348,\"start\":45289},{\"end\":45970,\"start\":45905},{\"end\":46545,\"start\":46490},{\"end\":46998,\"start\":46953},{\"end\":47805,\"start\":47755},{\"end\":48238,\"start\":48173},{\"end\":48690,\"start\":48625},{\"end\":49912,\"start\":49853},{\"end\":50660,\"start\":50601},{\"end\":51036,\"start\":50971},{\"end\":51667,\"start\":51616},{\"end\":52080,\"start\":52029},{\"end\":52758,\"start\":52707},{\"end\":54174,\"start\":54119},{\"end\":55015,\"start\":54957},{\"end\":55477,\"start\":55412},{\"end\":56298,\"start\":56243},{\"end\":56742,\"start\":56677},{\"end\":57536,\"start\":57485},{\"end\":57942,\"start\":57887}]"}}}, "year": 2023, "month": 12, "day": 17}