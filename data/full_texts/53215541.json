{"id": 53215541, "updated": "2023-09-30 12:04:22.597", "metadata": {"title": "Semidefinite relaxations for certifying robustness to adversarial examples", "authors": "[{\"first\":\"Aditi\",\"last\":\"Raghunathan\",\"middle\":[]},{\"first\":\"Jacob\",\"last\":\"Steinhardt\",\"middle\":[]},{\"first\":\"Percy\",\"last\":\"Liang\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2018, "month": 11, "day": 2}, "abstract": "Despite their impressive performance on diverse tasks, neural networks fail catastrophically in the presence of adversarial inputs---imperceptibly but adversarially perturbed versions of natural inputs. We have witnessed an arms race between defenders who attempt to train robust networks and attackers who try to construct adversarial examples. One promise of ending the arms race is developing certified defenses, ones which are provably robust against all attackers in some family. These certified defenses are based on convex relaxations which construct an upper bound on the worst case loss over all attackers in the family. Previous relaxations are loose on networks that are not trained against the respective relaxation. In this paper, we propose a new semidefinite relaxation for certifying robustness that applies to arbitrary ReLU networks. We show that our proposed relaxation is tighter than previous relaxations and produces meaningful robustness guarantees on three different\"foreign networks\"whose training objectives are agnostic to our proposed relaxation.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1811.01057", "mag": "2950550894", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/RaghunathanSL18", "doi": null}}, "content": {"source": {"pdf_hash": "17c71e299b64f3838fc6a1173df799322d7f56bb", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1811.01057v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "9b7c7953b7f1db356de396379af9a6aa480576ae", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/17c71e299b64f3838fc6a1173df799322d7f56bb.txt", "contents": "\nSemidefinite relaxations for certifying robustness to adversarial examples\n\n\nAditi Raghunathan aditir@cs.stanford.edu \nStanford University\n\n\nJacob Steinhardt jsteinhardt@cs.stanford.edu \nStanford University\n\n\nPercy Liang pliang@cs.stanford.edu \nStanford University\n\n\nSemidefinite relaxations for certifying robustness to adversarial examples\n\nDespite their impressive performance on diverse tasks, neural networks fail catastrophically in the presence of adversarial inputs-imperceptibly but adversarially perturbed versions of natural inputs. We have witnessed an arms race between defenders who attempt to train robust networks and attackers who try to construct adversarial examples. One promise of ending the arms race is developing certified defenses, ones which are provably robust against all attackers in some family. These certified defenses are based on convex relaxations which construct an upper bound on the worst case loss over all attackers in the family. Previous relaxations are loose on networks that are not trained against the respective relaxation. In this paper, we propose a new semidefinite relaxation for certifying robustness that applies to arbitrary ReLU networks. We show that our proposed relaxation is tighter than previous relaxations and produces meaningful robustness guarantees on three different foreign networks whose training objectives are agnostic to our proposed relaxation.\n\nIntroduction\n\nMany state-of-the-art classifiers have been shown to fail catastrophically in the presence of small imperceptible but adversarial perturbations. Since the discovery of such adversarial examples [25], numerous defenses have been proposed in attempt to build classifiers that are robust to adversarial examples. However, defenses are routinely broken by new attackers who adapt to the proposed defense, leading to an arms race. For example, distillation was proposed [22] but shown to be ineffective [5]. A proposed defense based on transformations of test inputs [20] was broken in only five days [2]. Recently, seven defenses published at ICLR 2018 fell to the attacks of Athalye et al. [3].\n\nA recent body of work aims to break this arms race by training classifiers that are certifiably robust to all attacks within a fixed attack model [13,23,29,8]. These approaches construct a convex relaxation for computing an upper bound on the worst-case loss over all valid attacks-this upper bound serves as a certificate of robustness. In this work, we propose a new convex relaxation based on semidefinite programming (SDP) that is significantly tighter than previous relaxations based on linear programming (LP) [29,8,9] and handles arbitrary number of layers (unlike the formulation in [23], which was restricted to two). We summarize the properties of our relaxation as follows:\n\n1. Our new SDP relaxation reasons jointly about intermediate activations and captures interactions that the LP relaxation cannot. Theoretically, we prove that there is a square root dimension gap between the LP relaxation and our proposed SDP relaxation for neural networks with random weights.\n\n2. Empirically, the tightness of our proposed relaxation allows us to obtain tight certificates for foreign networks-networks that were not specifically trained towards the certification procedure. For instance, adversarial training against the Projected Gradient Descent (PGD) attack [21] has led to networks that are \"empirically\" robust against known attacks, but which have only been certified against small perturbations (e.g. = 0.05 in the \u221e -norm for the MNIST dataset [9]). We use our SDP 32nd Conference on Neural Information Processing Systems (NIPS 2018), Montr\u00e9al, Canada.\n\nto provide the first non-trivial certificate of robustness for a moderate-size adversarially-trained model on MNIST at = 0.1.\n\n3. Furthermore, training a network to minimize the optimum of particular relaxation produces networks for which the respective relaxation provides good robustness certificates [23]. Notably and surprisingly, on such networks, our relaxation provides tighter certificates than even the relaxation that was optimized for during training.\n\nRelated work. Certification methods which evaluate the performance of a given network against all possible attacks roughly fall into two categories. The first category leverages convex optimization and our work adds to this family. Convex relaxations are useful for various reasons. Wong and Kolter [29], Raghunathan et al. [23] exploited the theory of duality to train certifiably robust networks on MNIST. In recent work, Dvijotham et al. [8], Wong et al. [30] extended this approach to train bigger networks with improved certified error and on larger datasets. Solving a convex relaxation for certification typically involves standard techniques from convex optimization. This enables scalable certification by providing valid upper bounds at every step in the optimization [9].\n\nThe second category draws techniques from formal verification such as SMT [16,17,7,14], which aim to provide tight certificates for any network using discrete optimization. These techniques, while providing tight certificates on arbitrary networks, are often very slow and worst-case exponential in network size. In prior work, certification would take up to several hours or longer for a single example even for a small network with around 100 hidden units [7,16]. However, in concurrent work, Tjeng and Tedrake [26] impressively scaled up exact verification through careful preprocessing and efficient pruning that dramatically reduces the search space. In particular, they concurrently obtain non-trivial certificates of robustness on a moderately-sized network trained using the adversarial training objective of [21] on MNIST at perturbation level = 0.1.\n\n\nSetup\n\nOur main contribution is a semidefinite relaxation of an optimization objective that arises in certification of neural networks against adversarial examples. In this section, we set up relevant notation and present the optimization objective that will be the focus of the rest of the paper.\n\nNotation. For a vector z \u2208 R n , we use z i to denote the i th coordinate of z. For a matrix Z \u2208 R m\u00d7n , Z i \u2208 R n denotes the i th row. For any function f : R \u2192 R and a vector z \u2208 R n , f (z) is a vector in R n with (f (z)) i = f (z i ), e.g., z 2 \u2208 R n represents the function that squares each component. For z,y \u2208 R n , z y denotes that z i \u2265 y i for i = 1,2,...,n. We use z 1 z 2 to represent the elementwise product of the vectors z 1 and z 2 . We use B (x) def = {x | x\u2212x \u221e \u2264 } to denote the \u221e ball aroundx. When it is necessary to distinguish vectors from scalars (in Section 4.1), we use x to represent a vector in R n that is semantically associated with the scalar x. Finally, we denote the vector of all zeros by 0 and the vector of all ones by 1.\n\nMulti-layer ReLU networks for classification. We focus on multi-layer neural networks with ReLU activations. A network f with L hidden layers is defined as follows: let x 0 \u2208 R d denote the input and x 1 , ... , x L denote the activation vectors at the intermediate layers. Suppose the network has m i units in layer i. x i is related to\nx i\u22121 as x i = ReLU(W i\u22121 x i\u22121 ) = max(W i\u22121 x i\u22121 ,0), where W i\u22121 \u2208 R mi\u00d7mi\u22121\nare the weights of the network. For simplicity of exposition, we omit the bias terms associated with the activations (but consider them in the experiments). We are interested in neural networks for classification where we classify an input into one of k classes. The output of the network is f (x 0 ) \u2208 R k such that f (x 0 ) j = c j x L represents the score of class j. The class label y assigned to the input x 0 is the class with the highest score: y = argmax j=1,...,k f (x 0 ) j .\n\nAttack model and certificate of robustness. We study classification in the presence of an attacker A that takes a clean test inputx \u2208 R d and returns an adversarially perturbed input A(x). In this work, we focus on attackers that are bounded in the \u221e norm: A(x) \u2208 B (x) for some fixed > 0. The attacker is successful on a clean input label pair (x,\u0233) if f (A(x)) =\u0233, or equivalently if f (A(x)) y > f (x 0 )\u0233 for some y =\u0233.\n\nWe are interested in bounding the error against the worst-case attack (we assume the attacker has full knowledge of the neural network). Let y (x,\u0233) denote the worst-case margin of an incorrect class y that can be achieved in the attack model:\ny (x,\u0233) def = max A(x)\u2208B (x) (f (A(x)) y \u2212f (A(x))\u0233).(1)\nA network is certifiably robust on (x,\u0233) if y (x,\u0233) < 0 for all y =\u0233. Computing y (x,\u0233) for a neural network involves solving a non-convex optimization problem, which is intractable in general. In this work, we study convex relaxations to efficiently compute an upper bound L y (x,\u0233) \u2265 y (x,\u0233). When L y (x,\u0233) < 0, we have a certificate of robustness of the network on input (x,\u0233).\n\nOptimization objective. For a fixed class y, the worst-case margin y (x,\u0233) of a neural network f with weights W can be expressed as the following optimization problem. The decision variable is the input A(x), which we denote here by x 0 for notational convenience. The quantity we are interested in maximizing is f (x 0 ) y \u2212 f (x 0 )\u0233 = (c y \u2212 c\u0233) x L , where x L is the final layer activation. We set up the optimization problem by jointly optimizing over all the activations x 0 ,x 1 ,x 2 ,...x L , imposing consistency constraints dictated by the neural network, and restricting the input x 0 to be within the attack model. Formally,\ny (x,\u0233) = max x 0 ,...,x L (c y \u2212c\u0233) x L(2)\nsubject to x i = ReLU(W i\u22121 x i\u22121 ) for i = 1,2,...,L (Neural network constraints)\n\nx 0 j \u2212x j \u221e \u2264 for j = 1,2,...,d (Attack model constraints) Computing y is computationally hard in general. In the following sections, we present how to relax this objective to a convex semidefinite program and discuss some properties of this relaxation.\n\n\nSemidefinite relaxations\n\nIn this section, we present our approach to obtaining a computationally tractable upper bound to the solution of the optimization problem described in (2).\n\nKey insight. The source of the non-convexity in (2) is the ReLU constraints. Consider a ReLU constraint of the form z = max(x,0). The key observation is that this constraint can be expressed equivalently as the following three linear and quadratic constraints between z and x: (i) z(z\u2212x) = 0, (ii) z \u2265 x, and (iii) z \u2265 0. Constraint (i) ensures that z is equal to either x or 0 and constraints (ii) and (iii) together then ensure that z is at least as large as both. This reformulation allows us to replace the non-linear ReLU constraints of the optimization problem in 2 with linear and quadratic constraints, turning it into a quadratically constrained quadratic program (QCQP). We first show how this QCQP can be relaxed to a semidefinite program (SDP) for networks with one hidden layer. The relaxation for multiple layers is a straightforward extension and is presented in Section 5.\n\n\nRelaxation for one hidden layer\n\nConsider a neural network with one hidden layer containing m nodes. Let the input be denoted by x \u2208 R d . The hidden layer activations are denoted by z \u2208 R m and related to the input x as z = ReLU(W x) for weights W \u2208 R m\u00d7d . Suppose that we have lower and upper bounds l,u \u2208 R d on the inputs such that l j \u2264 x j \u2264 u j . For example, in the \u221e attack model we have l =x\u2212 1 and u =x+ 1 wherex is the clean input. For the multi-layer case, we discuss how to obtain these bounds for the intermediate activations in Section 5.2. We are interested in optimizing a linear function of the hidden layer: f (x) = c z, where c \u2208 R m . For instance, while computing the worst case margin of an incorrect label y over true label\u0233, c = c y \u2212c\u0233.\n\nWe use the key insight that the ReLU constraints can be written as linear and quadratic constraints, allowing us to embed these constraints into a QCQP. We can also express the input constraint l j \u2264 x j \u2264 u j as a quadratic constraint, which will be useful later. In particular,\nl j \u2264 x j \u2264 u j if and only if (x j \u2212l j )(x j \u2212u j ) \u2264 0, thereby yielding the quadratic constraint x 2 j \u2264 (l j +u j )x j \u2212l j u j .\nThis gives us the final QCQP below:\ny (x,\u0233) = f QCQP = max x,z c z (3) s.t. z \u2265 0, z \u2265 W x, z 2 = z (W x) (ReLU constraints)\nx 2 \u2264 (l+u) x\u2212l u (Input constraints) We now relax the non-convex QCQP (3) to a convex SDP. The basic idea is to introduce a new set of variables representing all linear and quadratic monomials in x and z; the constraints in (3) can then be written as linear functions of these new variables. \nP [1] P [x ] P [z ] P [x] P [xx ] P [xz ] P [z] P [zx ] P [zz ] \uf8f9 \uf8fb .\nThe SDP relaxation of (3) can be written in terms of the matrix P as follows.\nf SDP = max P c P [z](4)s.t P [z] \u2265 0, P [z] \u2265 W P [x], diag(P [zz ]) = diag(W P [xz ]) (ReLU constraints) diag(P [xx ]) \u2264 (l+u) P [x]\u2212l u (Input constraints) P [1] = 1, P 0 (Matrix constraints).\nWhen the matrix P admits a rank-one factorization vv , the entries of the matrix P exactly correspond to linear and quadratic monomials in x and z. In this case, the ReLU and input constraints of the SDP are identical to the constraints of the QCQP. However, this rank-one constraint on P would make the feasible set non-convex. We instead consider the relaxed constraint on P that allows factorizations of the form P = V V , where V can be full rank. Equivalently, we consider the set of matrices P such that P 0. This set is convex and is a superset of the original non-convex set. Therefore, the above SDP is a relaxation of the QCQP in 3 with f SDP \u2265 f QCQP , providing an upper bound on y (x,\u0233) that could serve as a certificate of robustness. We note that this SDP relaxation is different from the one proposed in [23], which applies only to neural networks with one hidden layer. In contrast, the construction presented here naturally generalizes to multiple layers, as we show in Section 5. Moreover, we will see in Section 6 that our new relaxation often yields substantially tighter bounds than the approach of [23].\n\n\nAnalysis of the relaxation\n\nBefore extending the SDP relaxation defined in (4) to multiple layers, we will provide some geometric intuition for the SDP relaxation.\n\n\nGeometric interpretation\n\nFirst consider the simple case where m = d = 1 and W = c = 1, so that the problem is to maximize z subject to z = ReLU(x) and l \u2264 x \u2264 u. In this case, the SDP relaxation of (4) is as follows: \nf SDP = max P P [z](5)s.t P [z] \u2265 0, P [z] \u2265 P [x], P [z 2 ] = P [xz] (ReLU constraints) P [x 2 ] \u2264 (l+u)P [x]\u2212lu (Input constraints) P [1] = 1, P 0 (Matrix constraints).1 = ReLU(x 1 +x 2 ) and z 2 = ReLU(x 1 \u2212x 2 ).\nOn fixing the inputs x 1 and x 2 (both equal to 0.5 ), we plot the feasible activations of the LP and SDP relaxation. The LP feasible set is a simple product over the independent sets, while the SDP enforces joint constraints to obtain a more complex convex set. (c) We plot the set (z 1 ,z 2 ) across all feasible inputs (x 1 ,x 2 ) for the same setup as (b) and the objective of maximizing z 1 +z 2 . We see that f SDP < f LP .\n\nThe SDP operates on a PSD matrix P and imposes linear constraints on the entries of the matrix. Since feasible P can be written as V V , the entries of P can be thought of as dot products between vectors, and constraints as operating on these dot products. For the simple example above, ] correspond to x\u00b7 z, x 2 and z 2 respectively. We now reason about the input and ReLU constraints and visualize the geometry (see Figure 1a).\nV def = \u2190 e \u2192 \u2190 x \u2192 \u2190 z \u2192 for some vectors e, x, z \u2208 R 3 .Input constraints. The input constraint P [x 2 ] \u2264 (l + u)P [x] \u2212 lu equivalently imposes x 2 \u2264 (l + u)( x \u00b7 e) \u2212 lu.\nGeometrically, this constrains vector x on a sphere with center at 1 2 (l + u) e and radius 1 2 (l\u2212u). Notice that this implicitly bounds the norm of x. This is illustrated in Figure 1a where the green circle represents the space of feasible vectors x, projected onto the plane containing e and x.\n\nReLU constraints. The constraint on the quadratic terms (P [z 2 ] = P [zx]) is the core of the SDP. It says that the vector z is perpendicular to z \u2212 x. We can visualize z on the plane containing x and e in Figure 1a; the component of z perpendicular to this plane is not relevant to the SDP, because it's neither constrained nor appears in the objective. The feasible z trace out a circle with 1 2 x as the center (because the angle inscribed in a semicircle is a right angle). The linear constraints restrict z to the arc that has a larger projection on e than x, and is positive.\n\nRemarks. This geometric picture allows us to make the following important observation about the objective value max z\u00b7 e of the SDP relaxation. The largest value that z\u00b7 e can take depends on the angle \u03b8 that x makes with e. In particular, as \u03b8 decreases, the relaxation becomes tighter and as the vector deviates from e, the relaxation gets looser. Figure 1b provides an illustration. For large \u03b8, the radius of the circle that z traces increases, allowing z\u00b7 e to take large values.\n\nThat leads to the natural question: For a fixed input value x \u00b7 e (corresponding to x), what controls \u03b8? Since x\u00b7 e = x cos\u03b8, as the norm of x increases, \u03b8 increases. Hence a constraint that forces x to be close to x\u00b7 e will cause the output z\u00b7 e to take smaller values. Porting this intuition into the matrix interpretation, this suggests that constraints forcing P [x 2 ] = x 2 to be small lead to tighter relaxations.\n\n\nComparison with linear programming relaxation\n\nIn contrast to the SDP, another approach is to relax the objective and constraints in (2) to a linear program (LP) [18,10,9]. As we will see below, a crucial difference from the LP is that our SDP can \"reason jointly\" about different activations of the network in a stronger way than the LP can. We briefly review the LP approach and then elaborate on this difference.\n\nReview of the LP relaxation. We present the LP relaxation for a neural network with one hidden layer, where the hidden layer activations z \u2208 R m are related to the input x \u2208 R d as z = ReLU(W x). As before, we have bounds l,u \u2208 R d such that l \u2264 x \u2264 u.\n\nIn the LP relaxation, we replace the ReLU constraints at hidden node j with a convex outer envelope as illustrated in Figure 2a. The envelope is lower bounded by the linear constraints z \u2265 W x and z \u2265 0. In order to construct the upper bounding linear constraints, we compute the extreme points s = min l\u2264x\u2264u W x and t = max l\u2264x\u2264u W x and construct lines that connect (s, ReLU(s)) and (t, ReLU(t)). The final LP for the neural network is then written by constructing the convex envelopes for each ReLU unit and optimizing over this set as follows:\nf LP = max c z (6) s.t z \u2265 0, z \u2265 W x, (Lower bound lines) z \u2264 ReLU(t)\u2212ReLU(s) t\u2212s \u00b7(W x\u2212s)+ReLU(s), (Upper bound lines) l \u2264 x \u2264 u (Input constraints).\nThe extreme points s and t are the optima of a linear transformation (by W ) over a box in R d and can be computed using interval arithmetic. In the \u221e attack model where l =x\u2212 1 and u =x+ 1, we have s j = Wx\u2212 W j 1 and t j = Wx+ W j 1 for j = 1,2,...m.\n\nFrom Figure 2a, we see that for a single ReLU unit taken in isolation, the LP is tighter than the SDP. However, when we have multiple units, the SDP is tighter than the LP. We illustrate this with a simple example in 2 dimensions with 2 hidden nodes (See Figure 2b). The LP constrains z 1 and z 2 independently. To see this, let us set the input x to a fixed value and look at the feasible values of z 1 and z 2 . In the LP, the convex outer envelope that bounds z 1 only depends on the input x and the bounds l and u and is independent of the value of z 2 . Similarly, the outer envelope of z 2 does not depend on the value of z 1 , and the feasible set for (z 1 ,z 2 ) is simply the product of the individual feasible sets.\n\nIn contrast, the SDP has constraints that couple z 1 and z 2 . As a result, the feasible set of (z 1 ,z 2 ) is a strict subset of the product of the individual feasible sets. Figure 2b plots the LP and SDP feasible sets [z 1 ,z 2 ] for x = [ 2 , 2 ]. Recall from the geometric observations (Section 4.1) that the arc of z 1 depends on the configuration of x 1 + x 2 , while that of z 2 depends on x 1 \u2212 x 2 . Since the vectors x 1 + x 2 and x 1 \u2212 x 2 are dependent, the feasible sets of z 1 and z 2 are also dependent on each other. An alternative way to see this is from the matrix constraint that P 0 in 4. This matrix constraint does not factor into terms that decouple the entries P [z 1 ] and P [z 2 ], hence z 1 and z 2 cannot vary independently.\n\nWhen we reason about the relaxation over all feasible points x, the joint reasoning of the SDP allows it to achieve a better objective value. Figure 2c plots the feasible sets [z 1 ,z 2 ] over all valid x where the optimal value of the SDP, f SDP , is less than that of the LP, f LP .\n\nWe can extend the preceding example to exhibit a dimension-dependent gap between the LP and the SDP for random weight matrices. In particular, for a random network with m hidden nodes and input dimension d, with high probability,\nf LP = \u0398(md) while f SDP = \u0398(m \u221a d+d \u221a m).\nMore formally: Proposition 1. Suppose that the weight matrix W \u2208 R m\u00d7d is generated randomly by sampling each element W ij uniformly and independently from {\u22121,+1}. Also let the output vector c be the all-1s vector, 1. Takex = 0 and = 1. Then, for some universal constant \u03b3,\nf LP \u2265 1 2 md almost surely, while f SDP \u2264 \u03b3 \u00b7(m \u221a d+d \u221a m) with probability 1\u2212exp(\u2212(d+m)).\nWe defer the proof of this to Section A.\n\n\nMulti-layer networks\n\nThe SDP relaxation to evaluate robustness for multi-layer networks is a straightforward generalization of the relaxation presented for one hidden layer in Section 3.1. . SDP-cert is consistently better than other certificates. All numbers are reported for \u221e attacks at = 0.1.\n\n\nGrad-NN [23] LP-NN [29] PGD-NN\nPGD-attack 15% 18% 9% SDP-cert (this work) 20% 20% 18% LP-cert 97% 22% 100% Grad-cert 35% 93% n/a\n\nGeneral SDP\n\nThe interactions between x i\u22121 and x i in (2) (via the ReLU constraint) are analogous to the interaction between the input and hidden layer for the one layer case. Suppose we have bounds l i\u22121 ,u i\u22121 \u2208 R mi\u22121 on the inputs to the ReLU units at layer i such that l i\u22121 \u2264 x i\u22121 \u2264 u i\u22121 . We discuss how to obtain these bounds and their significance in Section 5.2. Writing the constraints for each layer iteratively gives us the following SDP:\nf SDP y (x,\u0233) = max P (c y \u2212c\u0233) P [x L ](7)\ns.t. for i = 1,...,L\nP [x i ] \u2265 0, P [x i ] \u2265 W i\u22121 P [x i\u22121 ], diag(P [x i (x i ) ]) = diag(W P [x i\u22121 (x i ) ]), (ReLU constraints for layer i) diag(P [x i\u22121 (x i\u22121 ) ]) \u2264 (l i\u22121 +u i\u22121 ) P [x i\u22121 ]\u2212l i\u22121 u i\u22121 , (Input constraints for layer i) P [1] = 1, P 0 (Matrix constraints).\n\nBounds on intermediate activations\n\nFrom the geometric interpretation of Section 4.1, we made the important observation that adding constraints that keep P [x 2 ] small aid in obtaining tighter relaxations. For the multi-layer case, since the activations at layer i \u2212 1 act as input to the next layer i, adding constraints that restrict P [(x i j ) 2 ] will lead to a tighter relaxation for the overall objective. The SDP automatically obtains some bound on P [(x i j ) 2 ] from the bounds on the input, hence the SDP solution is well-defined and finite even without these bounds. However, we can tighten the bound on P [(x i j ) 2 ] by relating it to the linear monomial P [(x i j )] via bounds on the value of the activation x i j . One simple way to obtain bounds on activations x i j is to treat each hidden unit separately, using simple interval arithmetic to obtain l 0 =x\u2212 1 (Attack model),\nu 0 =x+ 1 (Attack model),(8)l i = [W i\u22121 ] + l i\u22121 +[W i\u22121 ] \u2212 u i\u22121 , u i = [W i\u22121 ] + u i\u22121 +[W i\u22121 ] \u2212 l i\u22121 , where ([M ] + ) ij = max(M ij ,0) and ([M ] \u2212 ) ij = min(M ij ,0).\nIn our experiments on real networks (Section 6), we observe that these simple bounds are sufficient to obtain good certificates. However tighter bounds could potentially lead to tighter certificates.\n\n\nExperiments\n\nIn this section, we evaluate the performance of our certificate (7) on neural networks trained using different robust training procedures, and compare against other certificates in the literature.\n\nNetworks. We consider feedforward networks that are trained on the MNIST dataset of handwritten digits using three different robust training procedures.\n\n1. Grad-NN. We use the two-layer network with 500 hidden nodes from [23], obtained by using an SDP-based bound on the gradient of the network (different from the SDP presented here) as a regularizer. We obtained the weights of this network from the authors of [23].\n\n\nLP-NN.\n\nWe use a two-layer network with 500 hidden nodes (matching that of Grad-NN) trained via the LP-based robust training procedure of [29]. The authors of [29] provided the weights. 3. PGD-NN. We consider a fully-connected network with four layers containing 200,100 and 50 hidden nodes (i.e., the architecture is 784-200-100-50-10). We train this network using adversarial training [12] against the strong PGD attack [21]. We train to minimize a weighted combination of the regular cross entropy loss and adversarial loss. We tuned the hyperparameters based on the performance of the PGD attack on a holdout set. The stepsize of the PGD attack was set to 0.1, number of iterations to 40, perturbation size = 0.3 and weight on adversarial loss to 1 3 . The training procedures for SDP-NN and LP-NN yield certificates of robustness (described in their corresponding papers), but the training procedure of PGD-NN does not. Note that all the networks are \"foreign networks\" to our SDP, as their training procedures do not incorporate the SDP relaxation.\n\nCertification procedures. Recall from Section 2 that an upper bound on the maximum incorrect margin can be used to obtain certificates. We consider certificates from three different upper bounds.\n\n1. SDP-cert. This is the certificate we propose in this work. This uses the SDP upper bound that we defined in Section 5. The exact optimization problem is presented in (7) and the bounds on intermediate activations are obtained using the interval arithmetic procedure presented in (8).\n\n2. LP-cert. This uses the upper bound based on the LP relaxation discussed in Section 4.2 which forms the basis for several existing works on scalable certification [9,10,28,29]. The LP uses layer-wise bounds for intermediate nodes, similar to l i ,u i in our SDP formulation (7). For Grad-NN and LP-NN with a single hidden layer, the layerwise bounds can be computed exactly using interval arithmetic. For the four-layer PGD-NN, in order to have a fair comparison with SDP-cert, we use the same procedure (interval arithmetic) (8).\n\n3. Grad-cert. We use the upper bound proposed in [23]. This upper bound is based on the maximum norm of the gradient of the network predictions and only holds for two-layer networks. Table 1 presents the performance of the three different certification procedures on the three networks. For each certification method and network, we evaluate the associated upper bounds on the same 1000 random test points and report the fraction of points that were not certified. Computing the exact worst-case adversarial error is not computationally tractable. Therefore, to provide a comparison, we also compute a lower bound on the adversarial error-the error obtained by the PGD attack.\n\nPerformance of proposed SDP-cert. SDP-cert provides non-vacuous certificates for all networks considered. In particular, we can certify that the four layer PGD-NN has an error of at most 18% at = 0.1. To compare, a lower bound on the robust error (PGD attack error) is 9%. On the two-layer networks, SDP-cert improves the previously-known bounds. For example, it certifies that Grad-NN has an error of at most 20% compared to the previously known 35%. Similarly, SDP-cert improves the bound for LP-NN from 22% to 20%.\n\nThe gap between the lower bound (PGD) and upper bound (SDP) is because of points that cannot be misclassified by PGD but are also not certified by the SDP. In order to further investigate these points, we look at the margins obtained by the PGD attack to estimate the robustness of different points. Formally, let x PGD be the adversarial example generated by the PGD attack on clean inputx with true label\u0233. We compute min\ny =\u0233 [f (x PGD )\u0233 \u2212f (x PGD ) y ]\n, the margin of the closest incorrect class. A small value indicates that the x PGD was close to being misclassified. Figure 3 shows the histograms of the above PGD margin. The examples which are not certified by the SDP have much smaller margins than those examples that are certified: the average PGD margin is 1.2 on points that are not certified and 4.5 on points that are certified. From Figure 3, we see that a large number of the SDP uncertified points have very small margin, suggesting that these points might be misclassified by stronger attacks.\n\nRemark. As discussed in Section 5, we could consider a version of the SDP that does not include the constraints relating linear and quadratic terms at the intermediate layers of the network. Empirically, such an SDP produces vacuous certificates (> 90% error). Therefore, these constraints at intermediate layers play a significant role in improving the empirical performance of the SDP relaxation.\n\nComparison with other certification approaches. From Table 1, we observe that SDP-cert consistently performs better than both LP-cert and Grad-cert for all three networks.\n\nGrad-cert and LP-cert provide vacuous (> 90% error) certificates on networks that are not trained to minimize these certificates. This is because these certificates are tight only under some special cases that can be enforced by training. For example, LP-cert is tight when the ReLU units do not switch linear regions [29]. While a typical input causes only 20% of the hidden units of LP-NN to switch regions, 75% of the hidden units of Grad-NN switch on a typical input. Grad-cert bounds the gradient uniformly across the entire input space. This makes the bound loose on arbitrary networks that could have a small gradient only on the data distribution of interest.\n\nComparison to concurrent work [26]. A variety of robust MNIST networks are certified by Tjeng and Tedrake [26]. On Grad-NN, their certified error is 30% which is looser than our SDP certified error (20%). They also consider the CNN counterparts of LP-NN and PGD-NN, trained using the procedures of [29] and [21]. The certified errors are 4.4% and 7.2% respectively. This reduction in the errors is due to the CNN architecture. Further discussion on applying our SDP to CNNs appears in Section 7.\n\nOptimization setup. We use the YALMIP toolbox [19] with MOSEK as a backend to solve the different convex programs that arise in these certification procedures. On a 4-core CPU, the average SDP computation took around 25 minutes and the LP around 5 minutes per example.\n\n\nDiscussion\n\nIn this work, we focused on fully connected feedforward networks for computational efficiency. In principle, our proposed SDP can be directly used to certify convolutional neural networks (CNNs); unrolling the convolution would result in a (large) feedforward network. Naively, current off-the-shelf solvers cannot handle the SDP formulation of such large networks. Robust training on CNNs leads to better error rates: for example, adversarial training against the PGD adversary on a four-layer feedforward network has error 9% against the PGD attack, while a four-layer CNN trained using a similar procedure has error less than 3% [21]. An immediate open question is whether the network in [21], which has so far withstood many different attacks, is truly robust on MNIST. We are hopeful that we can scale up our SDP to answer this question, perhaps borrowing ideas from work on highly scalable SDPs [1] and explicitly exploiting the sparsity and structure induced by the CNN architecture.\n\nCurrent work on certification of neural networks against adversarial examples has focused on perturbations bounded in some norm ball. In our work, we focused on the common \u221e attack because the problem of securing multi-layer ReLU networks remains unsolved even in this well-studied attack model. Different attack models lead to different constraints only at the input layer; our SDP framework can be applied to any attack model where these input constraints can be written as linear and quadratic constraints. In particular, it can also be used to certify robustness against attacks bounded in 2 norm. [13] provide alternative bounds for 2 norm attacks based on the local gradient.\n\nGuarantees for the bounded norm attack model in general are sufficient but not necessary for robustness against adversaries in the real world. Many successful attacks involve inconspicious but clearly visible perturbations [11,24,6,4], or large but semantics-preserving perturbations in the case of natural language [15]. These perturbations do not currently have well-defined mathematical models and present yet another layer of challenge. However, we believe that the mathematical ideas we develop for the bounded norm will be useful building blocks in the broader adversarial game.\n\n\nReproducibility.\n\nAll code, data and experiments for this paper are available on the Codalab platform at https://worksheets.codalab.org/worksheets/ 0x6933b8cdbbfd424584062cdf40865f30/.\n\n\nA Proof of Proposition 1\n\nWe first lower bound the LP value f LP , and then upper bound the SDP value f SDP .\n\nPart 1: Lower-bounding f LP . It suffices to exhibit a feasible solution for the constraints. Note that for a given hidden unit i, we have s i = \u2212 W i 1 and t i = W i 1 . In particular, at x = 0 a feasible value for z i is 1 2 W i 2 . For this feasible value of (x,z), we get that c z = m i=1\n1 2 W i 1 = 1 2 i,j |W ij |.\nIn other words, f LP is at least half the element-wise 1 -norm of W . Since W is a random sign matrix we have |W ij | = 1 for all i,j, hence f LP \u2265 1 2 md with probability 1. Part 2: Upper-bounding f SDP . We start by exhibiting a general upper bound on f SDP implied by the constraints: Lemma 1. For any weight matrices W and c, we have f SDP \u2264 \u221a d W 2 c 2 , where W 2 is the operator norm of W .\n\nThe proof of Lemma 1 is given later in this section. To apply the lemma, note that in our case c 2 = \u221a m, while W 2 \u2264 \u03b3 \u00b7( \u221a m+ \u221a d+ log(1/\u03b4)) with probability 1\u2212\u03b4, for some universal constant \u03b3 (see Theorem 5.39 of [27] \n\u2264 W 2 \u221a d trP [xx ].(16)\nHere (i) is H\u00f6lder's inequality, and (iii) uses the fact that P [x 2 j ] \u2264 1 for all j (due to the constraints imposed by l and u).\n\nSolving for trP [zz ], we obtain the bound trP [zz ] \u2264 W 2 2 d. Plugging back into the preceding inequality, we obtain c P [z] \u2264 c 2 W 2 \u221a d, as was to be shown.\n\nFigure 1 :.\n1(a) Plot showing the feasible regions for the vectors x (green) and z (red). The input constraints restrict x to lie within the green circle. The ReLU constraint z \u22a5 z\u2212 x forces z to lie on the dashed red circle and the constraint z\u00b7 e \u2265 x\u00b7 e restricts it to the solid arc. (b) For a fixed value of input x\u00b7 e, when the angle made by x with e increases, the arc spanned by z has a larger projection on e and leading to a looser relaxation. Secondly, for a fixed value of x\u00b7 e, as \u03b8 increases, the norm x increases and vice versa. We define a matrix P def = vv and use symbolic indexing P [\u00b7] to index the elements of P , i.e P = \uf8ee \uf8f0\n\nFigure 2 :\n2(a) Visualization of the LP and SDP for a single ReLU unit with input x and output z. The LP is bounded by the line joining the extreme points. (b) Let z\n\n\nSimple example to compare the LP and SDP. Consider a two dimensional example with input x = [x 1 ,x 2 ] and lower and upper bounds l = [\u2212 ,\u2212 ] and u = [ , ], respectively. The hidden layer activations z 1 and z 2 are related to the input as z 1 = ReLU(x 1 + x 2 ) and z 2 = ReLU(x 1 \u2212 x 2 ). The objective is to maximize z 1 +z 2 .\n\nFigure 3 :\n3Histogram of PGD margins for (a) points that are certified by the SDP and (b) points that are not certified by the SDP.\n\n\nThe constraint P [1] = 1, for example, imposes e\u00b7 e = 1 i.e., e is a unit vector. The linear monomials P [x],P [z] correspond to projections on this unit vector, x\u00b7 e and z \u00b7 e. Finally, the quadratic monomials P [xz], P [x 2 ] and P [z 2\n\nTable 1 :\n1Fraction of non-certified examples on MNIST.Different certification techniques (rows) on \n\n\n\n). Therefore, Lemma 1 yields the bound f SDP \u2264 \u03b3 \u00b7 with probability 1\u2212exp(\u2212(m+d)), as claimed.A.1 Proof of Lemma 1First note that sinceP [1] P [z ] P [z] P [zz ] 0,we have P [z]P [z] P [zz ] by Schur complements, and in particular P [z] 2 2 \u2264 trP [zz ] (by taking the trace of both sides). Using this, and letting \u00b7 * denote the nuclear norm (sum of singular values), we have c P [z] \u2264 c 2 P [z] 2(9)\u221a \nmd\u00b7( \n\u221a \nm+ \n\u221a \nd+ \n\u221a \nm+d) \u2264 \n2\u03b3 \u00b7(m \n\u221a \nd+d \n\u221a \nm) \u2264 c 2 trP [zz ]. \n(10) \n\nBut we also have \n\ntrP [zz ] = \n\nm \n\ni=1 \n\nP [z 2 \ni ] \n(11) \n\n= \n\nm \n\ni=1 \n\nW i P [xz i ] \n(12) \n\n= tr(W P [xz ]) \n(13) \n\n(i) \n\n\u2264 W 2 P [xz ]  *  \n(14) \n\n(ii) \n\n\u2264 W 2 tr(P [xx ])tr(P [zz ]) \n(15) \n\n(iii) \n\n\nAcknowledgements. This work was partially supported by a Future of Life Institute Research Award and Open Philanthrophy Project Award. JS was supported by a Fannie & John Hertz Foundation Fellowship and an NSF Graduate Research Fellowship. We thank Eric Wong for providing relevant experimental results. We are also grateful to Moses Charikar, Zico Kolter and Eric Wong for several helpful discussions and anonymous reviewers for useful feedback.\nDSOS and SDSOS optimization: more tractable alternatives to sum of squares and semidefinite optimization. A A Ahmadi, A Majumdar, arXiv:1706.02586arXiv preprintA. A. Ahmadi and A. Majumdar. DSOS and SDSOS optimization: more tractable alternatives to sum of squares and semidefinite optimization. arXiv preprint arXiv:1706.02586, 2017.\n\nA Athalye, I Sutskever, arXiv:1707.07397Synthesizing robust adversarial examples. arXiv preprintA. Athalye and I. Sutskever. Synthesizing robust adversarial examples. arXiv preprint arXiv:1707.07397, 2017.\n\nObfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. A Athalye, N Carlini, D Wagner, arXiv:1802.00420arXiv preprintA. Athalye, N. Carlini, and D. Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. arXiv preprint arXiv:1802.00420, 2018.\n\nT B Brown, D Man\u00e9, A Roy, M Abadi, J Gilmer, arXiv:1712.09665Adversarial patch. arXiv preprintT. B. Brown, D. Man\u00e9, A. Roy, M. Abadi, and J. Gilmer. Adversarial patch. arXiv preprint arXiv:1712.09665, 2017.\n\nTowards evaluating the robustness of neural networks. N Carlini, D Wagner, IEEE Symposium on Security and Privacy. N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In IEEE Symposium on Security and Privacy, pages 39-57, 2017.\n\nHidden voice commands. N Carlini, P Mishra, T Vaidya, Y Zhang, M Sherr, C Shields, D Wagner, W Zhou, USENIX Security. N. Carlini, P. Mishra, T. Vaidya, Y. Zhang, M. Sherr, C. Shields, D. Wagner, and W. Zhou. Hidden voice commands. In USENIX Security, 2016.\n\nN Carlini, G Katz, C Barrett, D L Dill, Ground-truth adversarial examples. arXiv. N. Carlini, G. Katz, C. Barrett, and D. L. Dill. Ground-truth adversarial examples. arXiv, 2017.\n\nTraining verified learners with learned verifiers. K Dvijotham, S Gowal, R Stanforth, R Arandjelovic, B O&apos;donoghue, J Uesato, P Kohli, arXiv:1805.10265arXiv preprintK. Dvijotham, S. Gowal, R. Stanforth, R. Arandjelovic, B. O'Donoghue, J. Uesato, and P. Kohli. Training verified learners with learned verifiers. arXiv preprint arXiv:1805.10265, 2018.\n\nA dual approach to scalable verification of deep networks. K Dvijotham, R Stanforth, S Gowal, T Mann, P Kohli, arXiv:1803.06567arXiv preprintK. Dvijotham, R. Stanforth, S. Gowal, T. Mann, and P. Kohli. A dual approach to scalable verification of deep networks. arXiv preprint arXiv:1803.06567, 2018.\n\nFormal verification of piece-wise linear feed-forward neural networks. R Ehlers, International Symposium on Automated Technology for Verification and Analysis (ATVA). R. Ehlers. Formal verification of piece-wise linear feed-forward neural networks. In International Symposium on Automated Technology for Verification and Analysis (ATVA), pages 269-286, 2017.\n\nRobust physical-world attacks on machine learning models. I Evtimov, K Eykholt, E Fernandes, T Kohno, B Li, A Prakash, A Rahmati, D Song, arXivI. Evtimov, K. Eykholt, E. Fernandes, T. Kohno, B. Li, A. Prakash, A. Rahmati, and D. Song. Robust physical-world attacks on machine learning models. arXiv, 2017.\n\nExplaining and harnessing adversarial examples. I J Goodfellow, J Shlens, C Szegedy, International Conference on Learning Representations (ICLR). I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. In International Conference on Learning Representations (ICLR), 2015.\n\nFormal guarantees on the robustness of a classifier against adversarial manipulation. M Hein, M Andriushchenko, Advances in Neural Information Processing Systems (NIPS). M. Hein and M. Andriushchenko. Formal guarantees on the robustness of a classifier against adversarial manipulation. In Advances in Neural Information Processing Systems (NIPS), pages 2263-2273, 2017.\n\nS Huang, N Papernot, I Goodfellow, Y Duan, P Abbeel, Adversarial attacks on neural network policies. arXiv. S. Huang, N. Papernot, I. Goodfellow, Y. Duan, and P. Abbeel. Adversarial attacks on neural network policies. arXiv, 2017.\n\nAdversarial examples for evaluating reading comprehension systems. R Jia, P Liang, Empirical Methods in Natural Language Processing. R. Jia and P. Liang. Adversarial examples for evaluating reading comprehension systems. In Empirical Methods in Natural Language Processing (EMNLP), 2017.\n\nG Katz, C Barrett, D Dill, K Julian, M Kochenderfer, arXiv:1702.01135Reluplex: An efficient SMT solver for verifying deep neural networks. arXiv preprintG. Katz, C. Barrett, D. Dill, K. Julian, and M. Kochenderfer. Reluplex: An efficient SMT solver for verifying deep neural networks. arXiv preprint arXiv:1702.01135, 2017.\n\nG Katz, C Barrett, D L Dill, K Julian, M J Kochenderfer, Towards proving the adversarial robustness of deep neural networks. arXiv. G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer. Towards proving the adversarial robustness of deep neural networks. arXiv, 2017.\n\nProvable defenses against adversarial examples via the convex outer adversarial polytope. J Z Kolter, E Wong, arXiv:1711.00851arXiv preprintpublished atJ. Z. Kolter and E. Wong. Provable defenses against adversarial examples via the convex outer adversarial polytope (published at ICML 2018). arXiv preprint arXiv:1711.00851, 2017.\n\nYALMIP: A toolbox for modeling and optimization in MATLAB. J L\u00f6fberg, CACSD. J. L\u00f6fberg. YALMIP: A toolbox for modeling and optimization in MATLAB. In CACSD, 2004.\n\nNo need to worry about adversarial examples in object detection in autonomous vehicles. J Lu, H Sibai, E Fabry, D Forsyth, arXiv:1707.03501arXiv preprintJ. Lu, H. Sibai, E. Fabry, and D. Forsyth. No need to worry about adversarial examples in object detection in autonomous vehicles. arXiv preprint arXiv:1707.03501, 2017.\n\nTowards deep learning models resistant to adversarial attacks. A Madry, A Makelov, L Schmidt, D Tsipras, A Vladu, International Conference on Learning Representations (ICLR). A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models resistant to adversarial attacks. In International Conference on Learning Representations (ICLR), 2018.\n\nDistillation as a defense to adversarial perturbations against deep neural networks. N Papernot, P Mcdaniel, X Wu, S Jha, A Swami, IEEE Symposium on Security and Privacy. N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. Distillation as a defense to adversarial perturbations against deep neural networks. In IEEE Symposium on Security and Privacy, pages 582-597, 2016.\n\nCertified defenses against adversarial examples. A Raghunathan, J Steinhardt, P Liang, International Conference on Learning Representations (ICLR). A. Raghunathan, J. Steinhardt, and P. Liang. Certified defenses against adversarial examples. In International Conference on Learning Representations (ICLR), 2018.\n\nAccessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. M Sharif, S Bhagavatula, L Bauer, M K Reiter, ACM SIGSAC Conference on Computer and Communications Security. M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter. Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. In ACM SIGSAC Conference on Computer and Communications Security, pages 1528-1540, 2016.\n\nIntriguing properties of neural networks. C Szegedy, W Zaremba, I Sutskever, J Bruna, D Erhan, I Goodfellow, R Fergus, International Conference on Learning Representations (ICLR). C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. In International Conference on Learning Representations (ICLR), 2014.\n\nV Tjeng, R Tedrake, arXiv:1711.07356Verifying neural networks with mixed integer programming. arXiv preprintV. Tjeng and R. Tedrake. Verifying neural networks with mixed integer programming. arXiv preprint arXiv:1711.07356, 2017.\n\nIntroduction to the non-asymptotic analysis of random matrices. R Vershynin, arXivR. Vershynin. Introduction to the non-asymptotic analysis of random matrices. arXiv, 2010.\n\nT Weng, H Zhang, H Chen, Z Song, C Hsieh, D Boning, I S Dhillon, L Daniel, arXiv:1804.09699Towards fast computation of certified robustness for relu networks. arXiv preprintT. Weng, H. Zhang, H. Chen, Z. Song, C. Hsieh, D. Boning, I. S. Dhillon, and L. Daniel. Towards fast computation of certified robustness for relu networks. arXiv preprint arXiv:1804.09699, 2018.\n\nProvable defenses against adversarial examples via the convex outer adversarial polytope. E Wong, J Z Kolter, International Conference on Machine Learning (ICML). E. Wong and J. Z. Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In International Conference on Machine Learning (ICML), 2018.\n\n. E Wong, F Schmidt, J H Metzen, J Z Kolter, arXiv:1805.12514Scaling provable adversarial defenses. arXiv preprintE. Wong, F. Schmidt, J. H. Metzen, and J. Z. Kolter. Scaling provable adversarial defenses. arXiv preprint arXiv:1805.12514, 2018.\n", "annotations": {"author": "[{\"end\":141,\"start\":78},{\"end\":209,\"start\":142},{\"end\":267,\"start\":210},{\"end\":141,\"start\":78},{\"end\":209,\"start\":142},{\"end\":267,\"start\":210}]", "publisher": null, "author_last_name": "[{\"end\":95,\"start\":84},{\"end\":158,\"start\":148},{\"end\":221,\"start\":216},{\"end\":95,\"start\":84},{\"end\":158,\"start\":148},{\"end\":221,\"start\":216}]", "author_first_name": "[{\"end\":83,\"start\":78},{\"end\":147,\"start\":142},{\"end\":215,\"start\":210},{\"end\":83,\"start\":78},{\"end\":147,\"start\":142},{\"end\":215,\"start\":210}]", "author_affiliation": "[{\"end\":140,\"start\":120},{\"end\":208,\"start\":188},{\"end\":266,\"start\":246},{\"end\":140,\"start\":120},{\"end\":208,\"start\":188},{\"end\":266,\"start\":246}]", "title": "[{\"end\":75,\"start\":1},{\"end\":342,\"start\":268},{\"end\":75,\"start\":1},{\"end\":342,\"start\":268}]", "venue": null, "abstract": "[{\"end\":1416,\"start\":344},{\"end\":1416,\"start\":344}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b24\"},\"end\":1630,\"start\":1626},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":1901,\"start\":1897},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1933,\"start\":1930},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":1998,\"start\":1994},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2031,\"start\":2028},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2122,\"start\":2119},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2275,\"start\":2271},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2278,\"start\":2275},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2281,\"start\":2278},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2283,\"start\":2281},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2645,\"start\":2641},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2647,\"start\":2645},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2649,\"start\":2647},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2720,\"start\":2716},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3396,\"start\":3392},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3586,\"start\":3583},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4000,\"start\":3996},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4460,\"start\":4456},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4485,\"start\":4481},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4601,\"start\":4598},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4619,\"start\":4615},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4938,\"start\":4935},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5019,\"start\":5015},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5022,\"start\":5019},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5024,\"start\":5022},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5027,\"start\":5024},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5402,\"start\":5399},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5405,\"start\":5402},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5458,\"start\":5454},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5762,\"start\":5758},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10081,\"start\":10078},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13743,\"start\":13739},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14044,\"start\":14040},{\"end\":16384,\"start\":16381},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":17645,\"start\":17641},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17648,\"start\":17645},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":17650,\"start\":17648},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":24486,\"start\":24482},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":24678,\"start\":24674},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":24824,\"start\":24820},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":24845,\"start\":24841},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25073,\"start\":25069},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25108,\"start\":25104},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25434,\"start\":25433},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26391,\"start\":26388},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26394,\"start\":26391},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26397,\"start\":26394},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":26400,\"start\":26397},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26502,\"start\":26499},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":26810,\"start\":26806},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":29865,\"start\":29861},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":30246,\"start\":30242},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":30322,\"start\":30318},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":30514,\"start\":30510},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":30523,\"start\":30519},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":30759,\"start\":30755},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":31628,\"start\":31624},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":31687,\"start\":31683},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":31896,\"start\":31893},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":32590,\"start\":32586},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":32894,\"start\":32890},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":32897,\"start\":32894},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":32899,\"start\":32897},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":32901,\"start\":32899},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":32987,\"start\":32983},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":34493,\"start\":34489},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":1630,\"start\":1626},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":1901,\"start\":1897},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1933,\"start\":1930},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":1998,\"start\":1994},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2031,\"start\":2028},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2122,\"start\":2119},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2275,\"start\":2271},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2278,\"start\":2275},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2281,\"start\":2278},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2283,\"start\":2281},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2645,\"start\":2641},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2647,\"start\":2645},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2649,\"start\":2647},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2720,\"start\":2716},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3396,\"start\":3392},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3586,\"start\":3583},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4000,\"start\":3996},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4460,\"start\":4456},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4485,\"start\":4481},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4601,\"start\":4598},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4619,\"start\":4615},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4938,\"start\":4935},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5019,\"start\":5015},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5022,\"start\":5019},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5024,\"start\":5022},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5027,\"start\":5024},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5402,\"start\":5399},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5405,\"start\":5402},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5458,\"start\":5454},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5762,\"start\":5758},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10081,\"start\":10078},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13743,\"start\":13739},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14044,\"start\":14040},{\"end\":16384,\"start\":16381},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":17645,\"start\":17641},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17648,\"start\":17645},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":17650,\"start\":17648},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":24486,\"start\":24482},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":24678,\"start\":24674},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":24824,\"start\":24820},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":24845,\"start\":24841},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":25073,\"start\":25069},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25108,\"start\":25104},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25434,\"start\":25433},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26391,\"start\":26388},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":26394,\"start\":26391},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":26397,\"start\":26394},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":26400,\"start\":26397},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":26502,\"start\":26499},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":26810,\"start\":26806},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":29865,\"start\":29861},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":30246,\"start\":30242},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":30322,\"start\":30318},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":30514,\"start\":30510},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":30523,\"start\":30519},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":30759,\"start\":30755},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":31628,\"start\":31624},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":31687,\"start\":31683},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":31896,\"start\":31893},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":32590,\"start\":32586},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":32894,\"start\":32890},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":32897,\"start\":32894},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":32899,\"start\":32897},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":32901,\"start\":32899},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":32987,\"start\":32983},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":34493,\"start\":34489}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":35461,\"start\":34815},{\"attributes\":{\"id\":\"fig_1\"},\"end\":35628,\"start\":35462},{\"attributes\":{\"id\":\"fig_2\"},\"end\":35962,\"start\":35629},{\"attributes\":{\"id\":\"fig_3\"},\"end\":36095,\"start\":35963},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":36336,\"start\":36096},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":36439,\"start\":36337},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":37130,\"start\":36440},{\"attributes\":{\"id\":\"fig_0\"},\"end\":35461,\"start\":34815},{\"attributes\":{\"id\":\"fig_1\"},\"end\":35628,\"start\":35462},{\"attributes\":{\"id\":\"fig_2\"},\"end\":35962,\"start\":35629},{\"attributes\":{\"id\":\"fig_3\"},\"end\":36095,\"start\":35963},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":36336,\"start\":36096},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":36439,\"start\":36337},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":37130,\"start\":36440}]", "paragraph": "[{\"end\":2123,\"start\":1432},{\"end\":2809,\"start\":2125},{\"end\":3105,\"start\":2811},{\"end\":3691,\"start\":3107},{\"end\":3818,\"start\":3693},{\"end\":4155,\"start\":3820},{\"end\":4939,\"start\":4157},{\"end\":5800,\"start\":4941},{\"end\":6100,\"start\":5810},{\"end\":6861,\"start\":6102},{\"end\":7200,\"start\":6863},{\"end\":7767,\"start\":7282},{\"end\":8192,\"start\":7769},{\"end\":8437,\"start\":8194},{\"end\":8876,\"start\":8495},{\"end\":9515,\"start\":8878},{\"end\":9642,\"start\":9560},{\"end\":9898,\"start\":9644},{\"end\":10082,\"start\":9927},{\"end\":10972,\"start\":10084},{\"end\":11739,\"start\":11008},{\"end\":12020,\"start\":11741},{\"end\":12191,\"start\":12156},{\"end\":12574,\"start\":12281},{\"end\":12722,\"start\":12645},{\"end\":14045,\"start\":12919},{\"end\":14211,\"start\":14076},{\"end\":14432,\"start\":14240},{\"end\":15079,\"start\":14650},{\"end\":15510,\"start\":15081},{\"end\":15984,\"start\":15687},{\"end\":16568,\"start\":15986},{\"end\":17054,\"start\":16570},{\"end\":17476,\"start\":17056},{\"end\":17894,\"start\":17526},{\"end\":18148,\"start\":17896},{\"end\":18697,\"start\":18150},{\"end\":19102,\"start\":18850},{\"end\":19829,\"start\":19104},{\"end\":20583,\"start\":19831},{\"end\":20869,\"start\":20585},{\"end\":21100,\"start\":20871},{\"end\":21418,\"start\":21144},{\"end\":21551,\"start\":21511},{\"end\":21851,\"start\":21576},{\"end\":22438,\"start\":21997},{\"end\":22503,\"start\":22483},{\"end\":23665,\"start\":22804},{\"end\":24046,\"start\":23847},{\"end\":24258,\"start\":24062},{\"end\":24412,\"start\":24260},{\"end\":24679,\"start\":24414},{\"end\":25736,\"start\":24690},{\"end\":25933,\"start\":25738},{\"end\":26221,\"start\":25935},{\"end\":26755,\"start\":26223},{\"end\":27433,\"start\":26757},{\"end\":27952,\"start\":27435},{\"end\":28377,\"start\":27954},{\"end\":28968,\"start\":28412},{\"end\":29368,\"start\":28970},{\"end\":29541,\"start\":29370},{\"end\":30210,\"start\":29543},{\"end\":30707,\"start\":30212},{\"end\":30977,\"start\":30709},{\"end\":31982,\"start\":30992},{\"end\":32665,\"start\":31984},{\"end\":33251,\"start\":32667},{\"end\":33438,\"start\":33272},{\"end\":33550,\"start\":33467},{\"end\":33844,\"start\":33552},{\"end\":34271,\"start\":33874},{\"end\":34494,\"start\":34273},{\"end\":34651,\"start\":34520},{\"end\":34814,\"start\":34653},{\"end\":2123,\"start\":1432},{\"end\":2809,\"start\":2125},{\"end\":3105,\"start\":2811},{\"end\":3691,\"start\":3107},{\"end\":3818,\"start\":3693},{\"end\":4155,\"start\":3820},{\"end\":4939,\"start\":4157},{\"end\":5800,\"start\":4941},{\"end\":6100,\"start\":5810},{\"end\":6861,\"start\":6102},{\"end\":7200,\"start\":6863},{\"end\":7767,\"start\":7282},{\"end\":8192,\"start\":7769},{\"end\":8437,\"start\":8194},{\"end\":8876,\"start\":8495},{\"end\":9515,\"start\":8878},{\"end\":9642,\"start\":9560},{\"end\":9898,\"start\":9644},{\"end\":10082,\"start\":9927},{\"end\":10972,\"start\":10084},{\"end\":11739,\"start\":11008},{\"end\":12020,\"start\":11741},{\"end\":12191,\"start\":12156},{\"end\":12574,\"start\":12281},{\"end\":12722,\"start\":12645},{\"end\":14045,\"start\":12919},{\"end\":14211,\"start\":14076},{\"end\":14432,\"start\":14240},{\"end\":15079,\"start\":14650},{\"end\":15510,\"start\":15081},{\"end\":15984,\"start\":15687},{\"end\":16568,\"start\":15986},{\"end\":17054,\"start\":16570},{\"end\":17476,\"start\":17056},{\"end\":17894,\"start\":17526},{\"end\":18148,\"start\":17896},{\"end\":18697,\"start\":18150},{\"end\":19102,\"start\":18850},{\"end\":19829,\"start\":19104},{\"end\":20583,\"start\":19831},{\"end\":20869,\"start\":20585},{\"end\":21100,\"start\":20871},{\"end\":21418,\"start\":21144},{\"end\":21551,\"start\":21511},{\"end\":21851,\"start\":21576},{\"end\":22438,\"start\":21997},{\"end\":22503,\"start\":22483},{\"end\":23665,\"start\":22804},{\"end\":24046,\"start\":23847},{\"end\":24258,\"start\":24062},{\"end\":24412,\"start\":24260},{\"end\":24679,\"start\":24414},{\"end\":25736,\"start\":24690},{\"end\":25933,\"start\":25738},{\"end\":26221,\"start\":25935},{\"end\":26755,\"start\":26223},{\"end\":27433,\"start\":26757},{\"end\":27952,\"start\":27435},{\"end\":28377,\"start\":27954},{\"end\":28968,\"start\":28412},{\"end\":29368,\"start\":28970},{\"end\":29541,\"start\":29370},{\"end\":30210,\"start\":29543},{\"end\":30707,\"start\":30212},{\"end\":30977,\"start\":30709},{\"end\":31982,\"start\":30992},{\"end\":32665,\"start\":31984},{\"end\":33251,\"start\":32667},{\"end\":33438,\"start\":33272},{\"end\":33550,\"start\":33467},{\"end\":33844,\"start\":33552},{\"end\":34271,\"start\":33874},{\"end\":34494,\"start\":34273},{\"end\":34651,\"start\":34520},{\"end\":34814,\"start\":34653}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7281,\"start\":7201},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8494,\"start\":8438},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9559,\"start\":9516},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12155,\"start\":12021},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12280,\"start\":12192},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12644,\"start\":12575},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12747,\"start\":12723},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12918,\"start\":12747},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14455,\"start\":14433},{\"attributes\":{\"id\":\"formula_9\"},\"end\":14603,\"start\":14455},{\"attributes\":{\"id\":\"formula_10\"},\"end\":14649,\"start\":14603},{\"attributes\":{\"id\":\"formula_11\"},\"end\":15569,\"start\":15511},{\"attributes\":{\"id\":\"formula_12\"},\"end\":15686,\"start\":15569},{\"attributes\":{\"id\":\"formula_13\"},\"end\":18849,\"start\":18698},{\"attributes\":{\"id\":\"formula_14\"},\"end\":21143,\"start\":21101},{\"attributes\":{\"id\":\"formula_15\"},\"end\":21510,\"start\":21419},{\"attributes\":{\"id\":\"formula_16\"},\"end\":21982,\"start\":21885},{\"attributes\":{\"id\":\"formula_17\"},\"end\":22482,\"start\":22439},{\"attributes\":{\"id\":\"formula_18\"},\"end\":22766,\"start\":22504},{\"attributes\":{\"id\":\"formula_19\"},\"end\":23694,\"start\":23666},{\"attributes\":{\"id\":\"formula_20\"},\"end\":23846,\"start\":23694},{\"attributes\":{\"id\":\"formula_21\"},\"end\":28411,\"start\":28378},{\"attributes\":{\"id\":\"formula_22\"},\"end\":33873,\"start\":33845},{\"attributes\":{\"id\":\"formula_23\"},\"end\":34519,\"start\":34495},{\"attributes\":{\"id\":\"formula_0\"},\"end\":7281,\"start\":7201},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8494,\"start\":8438},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9559,\"start\":9516},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12155,\"start\":12021},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12280,\"start\":12192},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12644,\"start\":12575},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12747,\"start\":12723},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12918,\"start\":12747},{\"attributes\":{\"id\":\"formula_8\"},\"end\":14455,\"start\":14433},{\"attributes\":{\"id\":\"formula_9\"},\"end\":14603,\"start\":14455},{\"attributes\":{\"id\":\"formula_10\"},\"end\":14649,\"start\":14603},{\"attributes\":{\"id\":\"formula_11\"},\"end\":15569,\"start\":15511},{\"attributes\":{\"id\":\"formula_12\"},\"end\":15686,\"start\":15569},{\"attributes\":{\"id\":\"formula_13\"},\"end\":18849,\"start\":18698},{\"attributes\":{\"id\":\"formula_14\"},\"end\":21143,\"start\":21101},{\"attributes\":{\"id\":\"formula_15\"},\"end\":21510,\"start\":21419},{\"attributes\":{\"id\":\"formula_16\"},\"end\":21982,\"start\":21885},{\"attributes\":{\"id\":\"formula_17\"},\"end\":22482,\"start\":22439},{\"attributes\":{\"id\":\"formula_18\"},\"end\":22766,\"start\":22504},{\"attributes\":{\"id\":\"formula_19\"},\"end\":23694,\"start\":23666},{\"attributes\":{\"id\":\"formula_20\"},\"end\":23846,\"start\":23694},{\"attributes\":{\"id\":\"formula_21\"},\"end\":28411,\"start\":28378},{\"attributes\":{\"id\":\"formula_22\"},\"end\":33873,\"start\":33845},{\"attributes\":{\"id\":\"formula_23\"},\"end\":34519,\"start\":34495}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26947,\"start\":26940},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":29430,\"start\":29423},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":26947,\"start\":26940},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":29430,\"start\":29423}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1430,\"start\":1418},{\"attributes\":{\"n\":\"2\"},\"end\":5808,\"start\":5803},{\"attributes\":{\"n\":\"3\"},\"end\":9925,\"start\":9901},{\"attributes\":{\"n\":\"3.1\"},\"end\":11006,\"start\":10975},{\"attributes\":{\"n\":\"4\"},\"end\":14074,\"start\":14048},{\"attributes\":{\"n\":\"4.1\"},\"end\":14238,\"start\":14214},{\"attributes\":{\"n\":\"4.2\"},\"end\":17524,\"start\":17479},{\"attributes\":{\"n\":\"5\"},\"end\":21574,\"start\":21554},{\"end\":21884,\"start\":21854},{\"attributes\":{\"n\":\"5.1\"},\"end\":21995,\"start\":21984},{\"attributes\":{\"n\":\"5.2\"},\"end\":22802,\"start\":22768},{\"attributes\":{\"n\":\"6\"},\"end\":24060,\"start\":24049},{\"attributes\":{\"n\":\"2.\"},\"end\":24688,\"start\":24682},{\"attributes\":{\"n\":\"7\"},\"end\":30990,\"start\":30980},{\"end\":33270,\"start\":33254},{\"end\":33465,\"start\":33441},{\"end\":34827,\"start\":34816},{\"end\":35473,\"start\":35463},{\"end\":35974,\"start\":35964},{\"end\":36347,\"start\":36338},{\"attributes\":{\"n\":\"1\"},\"end\":1430,\"start\":1418},{\"attributes\":{\"n\":\"2\"},\"end\":5808,\"start\":5803},{\"attributes\":{\"n\":\"3\"},\"end\":9925,\"start\":9901},{\"attributes\":{\"n\":\"3.1\"},\"end\":11006,\"start\":10975},{\"attributes\":{\"n\":\"4\"},\"end\":14074,\"start\":14048},{\"attributes\":{\"n\":\"4.1\"},\"end\":14238,\"start\":14214},{\"attributes\":{\"n\":\"4.2\"},\"end\":17524,\"start\":17479},{\"attributes\":{\"n\":\"5\"},\"end\":21574,\"start\":21554},{\"end\":21884,\"start\":21854},{\"attributes\":{\"n\":\"5.1\"},\"end\":21995,\"start\":21984},{\"attributes\":{\"n\":\"5.2\"},\"end\":22802,\"start\":22768},{\"attributes\":{\"n\":\"6\"},\"end\":24060,\"start\":24049},{\"attributes\":{\"n\":\"2.\"},\"end\":24688,\"start\":24682},{\"attributes\":{\"n\":\"7\"},\"end\":30990,\"start\":30980},{\"end\":33270,\"start\":33254},{\"end\":33465,\"start\":33441},{\"end\":34827,\"start\":34816},{\"end\":35473,\"start\":35463},{\"end\":35974,\"start\":35964},{\"end\":36347,\"start\":36338}]", "table": "[{\"end\":36439,\"start\":36393},{\"end\":37130,\"start\":36842},{\"end\":36439,\"start\":36393},{\"end\":37130,\"start\":36842}]", "figure_caption": "[{\"end\":35461,\"start\":34829},{\"end\":35628,\"start\":35475},{\"end\":35962,\"start\":35631},{\"end\":36095,\"start\":35976},{\"end\":36336,\"start\":36098},{\"end\":36393,\"start\":36349},{\"end\":36842,\"start\":36442},{\"end\":35461,\"start\":34829},{\"end\":35628,\"start\":35475},{\"end\":35962,\"start\":35631},{\"end\":36095,\"start\":35976},{\"end\":36336,\"start\":36098},{\"end\":36393,\"start\":36349},{\"end\":36842,\"start\":36442}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15509,\"start\":15499},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15872,\"start\":15863},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16202,\"start\":16193},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16929,\"start\":16920},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18277,\"start\":18268},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19118,\"start\":19109},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19369,\"start\":19359},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20015,\"start\":20006},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20736,\"start\":20727},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28538,\"start\":28530},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28813,\"start\":28805},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15509,\"start\":15499},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":15872,\"start\":15863},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16202,\"start\":16193},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":16929,\"start\":16920},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":18277,\"start\":18268},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19118,\"start\":19109},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":19369,\"start\":19359},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20015,\"start\":20006},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":20736,\"start\":20727},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28538,\"start\":28530},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":28813,\"start\":28805}]", "bib_author_first_name": "[{\"end\":37685,\"start\":37684},{\"end\":37687,\"start\":37686},{\"end\":37697,\"start\":37696},{\"end\":37915,\"start\":37914},{\"end\":37926,\"start\":37925},{\"end\":38223,\"start\":38222},{\"end\":38234,\"start\":38233},{\"end\":38245,\"start\":38244},{\"end\":38465,\"start\":38464},{\"end\":38467,\"start\":38466},{\"end\":38476,\"start\":38475},{\"end\":38484,\"start\":38483},{\"end\":38491,\"start\":38490},{\"end\":38500,\"start\":38499},{\"end\":38727,\"start\":38726},{\"end\":38738,\"start\":38737},{\"end\":38954,\"start\":38953},{\"end\":38965,\"start\":38964},{\"end\":38975,\"start\":38974},{\"end\":38985,\"start\":38984},{\"end\":38994,\"start\":38993},{\"end\":39003,\"start\":39002},{\"end\":39014,\"start\":39013},{\"end\":39024,\"start\":39023},{\"end\":39189,\"start\":39188},{\"end\":39200,\"start\":39199},{\"end\":39208,\"start\":39207},{\"end\":39219,\"start\":39218},{\"end\":39221,\"start\":39220},{\"end\":39420,\"start\":39419},{\"end\":39433,\"start\":39432},{\"end\":39442,\"start\":39441},{\"end\":39455,\"start\":39454},{\"end\":39471,\"start\":39470},{\"end\":39490,\"start\":39489},{\"end\":39500,\"start\":39499},{\"end\":39784,\"start\":39783},{\"end\":39797,\"start\":39796},{\"end\":39810,\"start\":39809},{\"end\":39819,\"start\":39818},{\"end\":39827,\"start\":39826},{\"end\":40097,\"start\":40096},{\"end\":40444,\"start\":40443},{\"end\":40455,\"start\":40454},{\"end\":40466,\"start\":40465},{\"end\":40479,\"start\":40478},{\"end\":40488,\"start\":40487},{\"end\":40494,\"start\":40493},{\"end\":40505,\"start\":40504},{\"end\":40516,\"start\":40515},{\"end\":40741,\"start\":40740},{\"end\":40743,\"start\":40742},{\"end\":40757,\"start\":40756},{\"end\":40767,\"start\":40766},{\"end\":41089,\"start\":41088},{\"end\":41097,\"start\":41096},{\"end\":41375,\"start\":41374},{\"end\":41384,\"start\":41383},{\"end\":41396,\"start\":41395},{\"end\":41410,\"start\":41409},{\"end\":41418,\"start\":41417},{\"end\":41674,\"start\":41673},{\"end\":41681,\"start\":41680},{\"end\":41896,\"start\":41895},{\"end\":41904,\"start\":41903},{\"end\":41915,\"start\":41914},{\"end\":41923,\"start\":41922},{\"end\":41933,\"start\":41932},{\"end\":42221,\"start\":42220},{\"end\":42229,\"start\":42228},{\"end\":42240,\"start\":42239},{\"end\":42242,\"start\":42241},{\"end\":42250,\"start\":42249},{\"end\":42260,\"start\":42259},{\"end\":42262,\"start\":42261},{\"end\":42593,\"start\":42592},{\"end\":42595,\"start\":42594},{\"end\":42605,\"start\":42604},{\"end\":42895,\"start\":42894},{\"end\":43089,\"start\":43088},{\"end\":43095,\"start\":43094},{\"end\":43104,\"start\":43103},{\"end\":43113,\"start\":43112},{\"end\":43388,\"start\":43387},{\"end\":43397,\"start\":43396},{\"end\":43408,\"start\":43407},{\"end\":43419,\"start\":43418},{\"end\":43430,\"start\":43429},{\"end\":43779,\"start\":43778},{\"end\":43791,\"start\":43790},{\"end\":43803,\"start\":43802},{\"end\":43809,\"start\":43808},{\"end\":43816,\"start\":43815},{\"end\":44119,\"start\":44118},{\"end\":44134,\"start\":44133},{\"end\":44148,\"start\":44147},{\"end\":44471,\"start\":44470},{\"end\":44481,\"start\":44480},{\"end\":44496,\"start\":44495},{\"end\":44505,\"start\":44504},{\"end\":44507,\"start\":44506},{\"end\":44855,\"start\":44854},{\"end\":44866,\"start\":44865},{\"end\":44877,\"start\":44876},{\"end\":44890,\"start\":44889},{\"end\":44899,\"start\":44898},{\"end\":44908,\"start\":44907},{\"end\":44922,\"start\":44921},{\"end\":45194,\"start\":45193},{\"end\":45203,\"start\":45202},{\"end\":45489,\"start\":45488},{\"end\":45599,\"start\":45598},{\"end\":45607,\"start\":45606},{\"end\":45616,\"start\":45615},{\"end\":45624,\"start\":45623},{\"end\":45632,\"start\":45631},{\"end\":45641,\"start\":45640},{\"end\":45651,\"start\":45650},{\"end\":45653,\"start\":45652},{\"end\":45664,\"start\":45663},{\"end\":46058,\"start\":46057},{\"end\":46066,\"start\":46065},{\"end\":46068,\"start\":46067},{\"end\":46312,\"start\":46311},{\"end\":46320,\"start\":46319},{\"end\":46331,\"start\":46330},{\"end\":46333,\"start\":46332},{\"end\":46343,\"start\":46342},{\"end\":46345,\"start\":46344},{\"end\":37685,\"start\":37684},{\"end\":37687,\"start\":37686},{\"end\":37697,\"start\":37696},{\"end\":37915,\"start\":37914},{\"end\":37926,\"start\":37925},{\"end\":38223,\"start\":38222},{\"end\":38234,\"start\":38233},{\"end\":38245,\"start\":38244},{\"end\":38465,\"start\":38464},{\"end\":38467,\"start\":38466},{\"end\":38476,\"start\":38475},{\"end\":38484,\"start\":38483},{\"end\":38491,\"start\":38490},{\"end\":38500,\"start\":38499},{\"end\":38727,\"start\":38726},{\"end\":38738,\"start\":38737},{\"end\":38954,\"start\":38953},{\"end\":38965,\"start\":38964},{\"end\":38975,\"start\":38974},{\"end\":38985,\"start\":38984},{\"end\":38994,\"start\":38993},{\"end\":39003,\"start\":39002},{\"end\":39014,\"start\":39013},{\"end\":39024,\"start\":39023},{\"end\":39189,\"start\":39188},{\"end\":39200,\"start\":39199},{\"end\":39208,\"start\":39207},{\"end\":39219,\"start\":39218},{\"end\":39221,\"start\":39220},{\"end\":39420,\"start\":39419},{\"end\":39433,\"start\":39432},{\"end\":39442,\"start\":39441},{\"end\":39455,\"start\":39454},{\"end\":39471,\"start\":39470},{\"end\":39490,\"start\":39489},{\"end\":39500,\"start\":39499},{\"end\":39784,\"start\":39783},{\"end\":39797,\"start\":39796},{\"end\":39810,\"start\":39809},{\"end\":39819,\"start\":39818},{\"end\":39827,\"start\":39826},{\"end\":40097,\"start\":40096},{\"end\":40444,\"start\":40443},{\"end\":40455,\"start\":40454},{\"end\":40466,\"start\":40465},{\"end\":40479,\"start\":40478},{\"end\":40488,\"start\":40487},{\"end\":40494,\"start\":40493},{\"end\":40505,\"start\":40504},{\"end\":40516,\"start\":40515},{\"end\":40741,\"start\":40740},{\"end\":40743,\"start\":40742},{\"end\":40757,\"start\":40756},{\"end\":40767,\"start\":40766},{\"end\":41089,\"start\":41088},{\"end\":41097,\"start\":41096},{\"end\":41375,\"start\":41374},{\"end\":41384,\"start\":41383},{\"end\":41396,\"start\":41395},{\"end\":41410,\"start\":41409},{\"end\":41418,\"start\":41417},{\"end\":41674,\"start\":41673},{\"end\":41681,\"start\":41680},{\"end\":41896,\"start\":41895},{\"end\":41904,\"start\":41903},{\"end\":41915,\"start\":41914},{\"end\":41923,\"start\":41922},{\"end\":41933,\"start\":41932},{\"end\":42221,\"start\":42220},{\"end\":42229,\"start\":42228},{\"end\":42240,\"start\":42239},{\"end\":42242,\"start\":42241},{\"end\":42250,\"start\":42249},{\"end\":42260,\"start\":42259},{\"end\":42262,\"start\":42261},{\"end\":42593,\"start\":42592},{\"end\":42595,\"start\":42594},{\"end\":42605,\"start\":42604},{\"end\":42895,\"start\":42894},{\"end\":43089,\"start\":43088},{\"end\":43095,\"start\":43094},{\"end\":43104,\"start\":43103},{\"end\":43113,\"start\":43112},{\"end\":43388,\"start\":43387},{\"end\":43397,\"start\":43396},{\"end\":43408,\"start\":43407},{\"end\":43419,\"start\":43418},{\"end\":43430,\"start\":43429},{\"end\":43779,\"start\":43778},{\"end\":43791,\"start\":43790},{\"end\":43803,\"start\":43802},{\"end\":43809,\"start\":43808},{\"end\":43816,\"start\":43815},{\"end\":44119,\"start\":44118},{\"end\":44134,\"start\":44133},{\"end\":44148,\"start\":44147},{\"end\":44471,\"start\":44470},{\"end\":44481,\"start\":44480},{\"end\":44496,\"start\":44495},{\"end\":44505,\"start\":44504},{\"end\":44507,\"start\":44506},{\"end\":44855,\"start\":44854},{\"end\":44866,\"start\":44865},{\"end\":44877,\"start\":44876},{\"end\":44890,\"start\":44889},{\"end\":44899,\"start\":44898},{\"end\":44908,\"start\":44907},{\"end\":44922,\"start\":44921},{\"end\":45194,\"start\":45193},{\"end\":45203,\"start\":45202},{\"end\":45489,\"start\":45488},{\"end\":45599,\"start\":45598},{\"end\":45607,\"start\":45606},{\"end\":45616,\"start\":45615},{\"end\":45624,\"start\":45623},{\"end\":45632,\"start\":45631},{\"end\":45641,\"start\":45640},{\"end\":45651,\"start\":45650},{\"end\":45653,\"start\":45652},{\"end\":45664,\"start\":45663},{\"end\":46058,\"start\":46057},{\"end\":46066,\"start\":46065},{\"end\":46068,\"start\":46067},{\"end\":46312,\"start\":46311},{\"end\":46320,\"start\":46319},{\"end\":46331,\"start\":46330},{\"end\":46333,\"start\":46332},{\"end\":46343,\"start\":46342},{\"end\":46345,\"start\":46344}]", "bib_author_last_name": "[{\"end\":37694,\"start\":37688},{\"end\":37706,\"start\":37698},{\"end\":37923,\"start\":37916},{\"end\":37936,\"start\":37927},{\"end\":38231,\"start\":38224},{\"end\":38242,\"start\":38235},{\"end\":38252,\"start\":38246},{\"end\":38473,\"start\":38468},{\"end\":38481,\"start\":38477},{\"end\":38488,\"start\":38485},{\"end\":38497,\"start\":38492},{\"end\":38507,\"start\":38501},{\"end\":38735,\"start\":38728},{\"end\":38745,\"start\":38739},{\"end\":38962,\"start\":38955},{\"end\":38972,\"start\":38966},{\"end\":38982,\"start\":38976},{\"end\":38991,\"start\":38986},{\"end\":39000,\"start\":38995},{\"end\":39011,\"start\":39004},{\"end\":39021,\"start\":39015},{\"end\":39029,\"start\":39025},{\"end\":39197,\"start\":39190},{\"end\":39205,\"start\":39201},{\"end\":39216,\"start\":39209},{\"end\":39226,\"start\":39222},{\"end\":39430,\"start\":39421},{\"end\":39439,\"start\":39434},{\"end\":39452,\"start\":39443},{\"end\":39468,\"start\":39456},{\"end\":39487,\"start\":39472},{\"end\":39497,\"start\":39491},{\"end\":39506,\"start\":39501},{\"end\":39794,\"start\":39785},{\"end\":39807,\"start\":39798},{\"end\":39816,\"start\":39811},{\"end\":39824,\"start\":39820},{\"end\":39833,\"start\":39828},{\"end\":40104,\"start\":40098},{\"end\":40452,\"start\":40445},{\"end\":40463,\"start\":40456},{\"end\":40476,\"start\":40467},{\"end\":40485,\"start\":40480},{\"end\":40491,\"start\":40489},{\"end\":40502,\"start\":40495},{\"end\":40513,\"start\":40506},{\"end\":40521,\"start\":40517},{\"end\":40754,\"start\":40744},{\"end\":40764,\"start\":40758},{\"end\":40775,\"start\":40768},{\"end\":41094,\"start\":41090},{\"end\":41112,\"start\":41098},{\"end\":41381,\"start\":41376},{\"end\":41393,\"start\":41385},{\"end\":41407,\"start\":41397},{\"end\":41415,\"start\":41411},{\"end\":41425,\"start\":41419},{\"end\":41678,\"start\":41675},{\"end\":41687,\"start\":41682},{\"end\":41901,\"start\":41897},{\"end\":41912,\"start\":41905},{\"end\":41920,\"start\":41916},{\"end\":41930,\"start\":41924},{\"end\":41946,\"start\":41934},{\"end\":42226,\"start\":42222},{\"end\":42237,\"start\":42230},{\"end\":42247,\"start\":42243},{\"end\":42257,\"start\":42251},{\"end\":42275,\"start\":42263},{\"end\":42602,\"start\":42596},{\"end\":42610,\"start\":42606},{\"end\":42903,\"start\":42896},{\"end\":43092,\"start\":43090},{\"end\":43101,\"start\":43096},{\"end\":43110,\"start\":43105},{\"end\":43121,\"start\":43114},{\"end\":43394,\"start\":43389},{\"end\":43405,\"start\":43398},{\"end\":43416,\"start\":43409},{\"end\":43427,\"start\":43420},{\"end\":43436,\"start\":43431},{\"end\":43788,\"start\":43780},{\"end\":43800,\"start\":43792},{\"end\":43806,\"start\":43804},{\"end\":43813,\"start\":43810},{\"end\":43822,\"start\":43817},{\"end\":44131,\"start\":44120},{\"end\":44145,\"start\":44135},{\"end\":44154,\"start\":44149},{\"end\":44478,\"start\":44472},{\"end\":44493,\"start\":44482},{\"end\":44502,\"start\":44497},{\"end\":44514,\"start\":44508},{\"end\":44863,\"start\":44856},{\"end\":44874,\"start\":44867},{\"end\":44887,\"start\":44878},{\"end\":44896,\"start\":44891},{\"end\":44905,\"start\":44900},{\"end\":44919,\"start\":44909},{\"end\":44929,\"start\":44923},{\"end\":45200,\"start\":45195},{\"end\":45211,\"start\":45204},{\"end\":45499,\"start\":45490},{\"end\":45604,\"start\":45600},{\"end\":45613,\"start\":45608},{\"end\":45621,\"start\":45617},{\"end\":45629,\"start\":45625},{\"end\":45638,\"start\":45633},{\"end\":45648,\"start\":45642},{\"end\":45661,\"start\":45654},{\"end\":45671,\"start\":45665},{\"end\":46063,\"start\":46059},{\"end\":46075,\"start\":46069},{\"end\":46317,\"start\":46313},{\"end\":46328,\"start\":46321},{\"end\":46340,\"start\":46334},{\"end\":46352,\"start\":46346},{\"end\":37694,\"start\":37688},{\"end\":37706,\"start\":37698},{\"end\":37923,\"start\":37916},{\"end\":37936,\"start\":37927},{\"end\":38231,\"start\":38224},{\"end\":38242,\"start\":38235},{\"end\":38252,\"start\":38246},{\"end\":38473,\"start\":38468},{\"end\":38481,\"start\":38477},{\"end\":38488,\"start\":38485},{\"end\":38497,\"start\":38492},{\"end\":38507,\"start\":38501},{\"end\":38735,\"start\":38728},{\"end\":38745,\"start\":38739},{\"end\":38962,\"start\":38955},{\"end\":38972,\"start\":38966},{\"end\":38982,\"start\":38976},{\"end\":38991,\"start\":38986},{\"end\":39000,\"start\":38995},{\"end\":39011,\"start\":39004},{\"end\":39021,\"start\":39015},{\"end\":39029,\"start\":39025},{\"end\":39197,\"start\":39190},{\"end\":39205,\"start\":39201},{\"end\":39216,\"start\":39209},{\"end\":39226,\"start\":39222},{\"end\":39430,\"start\":39421},{\"end\":39439,\"start\":39434},{\"end\":39452,\"start\":39443},{\"end\":39468,\"start\":39456},{\"end\":39487,\"start\":39472},{\"end\":39497,\"start\":39491},{\"end\":39506,\"start\":39501},{\"end\":39794,\"start\":39785},{\"end\":39807,\"start\":39798},{\"end\":39816,\"start\":39811},{\"end\":39824,\"start\":39820},{\"end\":39833,\"start\":39828},{\"end\":40104,\"start\":40098},{\"end\":40452,\"start\":40445},{\"end\":40463,\"start\":40456},{\"end\":40476,\"start\":40467},{\"end\":40485,\"start\":40480},{\"end\":40491,\"start\":40489},{\"end\":40502,\"start\":40495},{\"end\":40513,\"start\":40506},{\"end\":40521,\"start\":40517},{\"end\":40754,\"start\":40744},{\"end\":40764,\"start\":40758},{\"end\":40775,\"start\":40768},{\"end\":41094,\"start\":41090},{\"end\":41112,\"start\":41098},{\"end\":41381,\"start\":41376},{\"end\":41393,\"start\":41385},{\"end\":41407,\"start\":41397},{\"end\":41415,\"start\":41411},{\"end\":41425,\"start\":41419},{\"end\":41678,\"start\":41675},{\"end\":41687,\"start\":41682},{\"end\":41901,\"start\":41897},{\"end\":41912,\"start\":41905},{\"end\":41920,\"start\":41916},{\"end\":41930,\"start\":41924},{\"end\":41946,\"start\":41934},{\"end\":42226,\"start\":42222},{\"end\":42237,\"start\":42230},{\"end\":42247,\"start\":42243},{\"end\":42257,\"start\":42251},{\"end\":42275,\"start\":42263},{\"end\":42602,\"start\":42596},{\"end\":42610,\"start\":42606},{\"end\":42903,\"start\":42896},{\"end\":43092,\"start\":43090},{\"end\":43101,\"start\":43096},{\"end\":43110,\"start\":43105},{\"end\":43121,\"start\":43114},{\"end\":43394,\"start\":43389},{\"end\":43405,\"start\":43398},{\"end\":43416,\"start\":43409},{\"end\":43427,\"start\":43420},{\"end\":43436,\"start\":43431},{\"end\":43788,\"start\":43780},{\"end\":43800,\"start\":43792},{\"end\":43806,\"start\":43804},{\"end\":43813,\"start\":43810},{\"end\":43822,\"start\":43817},{\"end\":44131,\"start\":44120},{\"end\":44145,\"start\":44135},{\"end\":44154,\"start\":44149},{\"end\":44478,\"start\":44472},{\"end\":44493,\"start\":44482},{\"end\":44502,\"start\":44497},{\"end\":44514,\"start\":44508},{\"end\":44863,\"start\":44856},{\"end\":44874,\"start\":44867},{\"end\":44887,\"start\":44878},{\"end\":44896,\"start\":44891},{\"end\":44905,\"start\":44900},{\"end\":44919,\"start\":44909},{\"end\":44929,\"start\":44923},{\"end\":45200,\"start\":45195},{\"end\":45211,\"start\":45204},{\"end\":45499,\"start\":45490},{\"end\":45604,\"start\":45600},{\"end\":45613,\"start\":45608},{\"end\":45621,\"start\":45617},{\"end\":45629,\"start\":45625},{\"end\":45638,\"start\":45633},{\"end\":45648,\"start\":45642},{\"end\":45661,\"start\":45654},{\"end\":45671,\"start\":45665},{\"end\":46063,\"start\":46059},{\"end\":46075,\"start\":46069},{\"end\":46317,\"start\":46313},{\"end\":46328,\"start\":46321},{\"end\":46340,\"start\":46334},{\"end\":46352,\"start\":46346}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1706.02586\",\"id\":\"b0\"},\"end\":37912,\"start\":37578},{\"attributes\":{\"doi\":\"arXiv:1707.07397\",\"id\":\"b1\"},\"end\":38119,\"start\":37914},{\"attributes\":{\"doi\":\"arXiv:1802.00420\",\"id\":\"b2\"},\"end\":38462,\"start\":38121},{\"attributes\":{\"doi\":\"arXiv:1712.09665\",\"id\":\"b3\"},\"end\":38670,\"start\":38464},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":2893830},\"end\":38928,\"start\":38672},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":6217368},\"end\":39186,\"start\":38930},{\"attributes\":{\"id\":\"b6\"},\"end\":39366,\"start\":39188},{\"attributes\":{\"doi\":\"arXiv:1805.10265\",\"id\":\"b7\"},\"end\":39722,\"start\":39368},{\"attributes\":{\"doi\":\"arXiv:1803.06567\",\"id\":\"b8\"},\"end\":40023,\"start\":39724},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1931807},\"end\":40383,\"start\":40025},{\"attributes\":{\"id\":\"b10\"},\"end\":40690,\"start\":40385},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6706414},\"end\":41000,\"start\":40692},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":10490694},\"end\":41372,\"start\":41002},{\"attributes\":{\"id\":\"b13\"},\"end\":41604,\"start\":41374},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":7228830},\"end\":41893,\"start\":41606},{\"attributes\":{\"doi\":\"arXiv:1702.01135\",\"id\":\"b15\"},\"end\":42218,\"start\":41895},{\"attributes\":{\"id\":\"b16\"},\"end\":42500,\"start\":42220},{\"attributes\":{\"doi\":\"arXiv:1711.00851\",\"id\":\"b17\"},\"end\":42833,\"start\":42502},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":15458635},\"end\":42998,\"start\":42835},{\"attributes\":{\"doi\":\"arXiv:1707.03501\",\"id\":\"b19\"},\"end\":43322,\"start\":43000},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":3488815},\"end\":43691,\"start\":43324},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":2672720},\"end\":44067,\"start\":43693},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":11217889},\"end\":44380,\"start\":44069},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":207241700},\"end\":44810,\"start\":44382},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":604334},\"end\":45191,\"start\":44812},{\"attributes\":{\"doi\":\"arXiv:1711.07356\",\"id\":\"b25\"},\"end\":45422,\"start\":45193},{\"attributes\":{\"id\":\"b26\"},\"end\":45596,\"start\":45424},{\"attributes\":{\"doi\":\"arXiv:1804.09699\",\"id\":\"b27\"},\"end\":45965,\"start\":45598},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":3659467},\"end\":46307,\"start\":45967},{\"attributes\":{\"doi\":\"arXiv:1805.12514\",\"id\":\"b29\"},\"end\":46553,\"start\":46309},{\"attributes\":{\"doi\":\"arXiv:1706.02586\",\"id\":\"b0\"},\"end\":37912,\"start\":37578},{\"attributes\":{\"doi\":\"arXiv:1707.07397\",\"id\":\"b1\"},\"end\":38119,\"start\":37914},{\"attributes\":{\"doi\":\"arXiv:1802.00420\",\"id\":\"b2\"},\"end\":38462,\"start\":38121},{\"attributes\":{\"doi\":\"arXiv:1712.09665\",\"id\":\"b3\"},\"end\":38670,\"start\":38464},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":2893830},\"end\":38928,\"start\":38672},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":6217368},\"end\":39186,\"start\":38930},{\"attributes\":{\"id\":\"b6\"},\"end\":39366,\"start\":39188},{\"attributes\":{\"doi\":\"arXiv:1805.10265\",\"id\":\"b7\"},\"end\":39722,\"start\":39368},{\"attributes\":{\"doi\":\"arXiv:1803.06567\",\"id\":\"b8\"},\"end\":40023,\"start\":39724},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":1931807},\"end\":40383,\"start\":40025},{\"attributes\":{\"id\":\"b10\"},\"end\":40690,\"start\":40385},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6706414},\"end\":41000,\"start\":40692},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":10490694},\"end\":41372,\"start\":41002},{\"attributes\":{\"id\":\"b13\"},\"end\":41604,\"start\":41374},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":7228830},\"end\":41893,\"start\":41606},{\"attributes\":{\"doi\":\"arXiv:1702.01135\",\"id\":\"b15\"},\"end\":42218,\"start\":41895},{\"attributes\":{\"id\":\"b16\"},\"end\":42500,\"start\":42220},{\"attributes\":{\"doi\":\"arXiv:1711.00851\",\"id\":\"b17\"},\"end\":42833,\"start\":42502},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":15458635},\"end\":42998,\"start\":42835},{\"attributes\":{\"doi\":\"arXiv:1707.03501\",\"id\":\"b19\"},\"end\":43322,\"start\":43000},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":3488815},\"end\":43691,\"start\":43324},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":2672720},\"end\":44067,\"start\":43693},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":11217889},\"end\":44380,\"start\":44069},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":207241700},\"end\":44810,\"start\":44382},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":604334},\"end\":45191,\"start\":44812},{\"attributes\":{\"doi\":\"arXiv:1711.07356\",\"id\":\"b25\"},\"end\":45422,\"start\":45193},{\"attributes\":{\"id\":\"b26\"},\"end\":45596,\"start\":45424},{\"attributes\":{\"doi\":\"arXiv:1804.09699\",\"id\":\"b27\"},\"end\":45965,\"start\":45598},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":3659467},\"end\":46307,\"start\":45967},{\"attributes\":{\"doi\":\"arXiv:1805.12514\",\"id\":\"b29\"},\"end\":46553,\"start\":46309}]", "bib_title": "[{\"end\":38724,\"start\":38672},{\"end\":38951,\"start\":38930},{\"end\":40094,\"start\":40025},{\"end\":40738,\"start\":40692},{\"end\":41086,\"start\":41002},{\"end\":41671,\"start\":41606},{\"end\":42892,\"start\":42835},{\"end\":43385,\"start\":43324},{\"end\":43776,\"start\":43693},{\"end\":44116,\"start\":44069},{\"end\":44468,\"start\":44382},{\"end\":44852,\"start\":44812},{\"end\":46055,\"start\":45967},{\"end\":38724,\"start\":38672},{\"end\":38951,\"start\":38930},{\"end\":40094,\"start\":40025},{\"end\":40738,\"start\":40692},{\"end\":41086,\"start\":41002},{\"end\":41671,\"start\":41606},{\"end\":42892,\"start\":42835},{\"end\":43385,\"start\":43324},{\"end\":43776,\"start\":43693},{\"end\":44116,\"start\":44069},{\"end\":44468,\"start\":44382},{\"end\":44852,\"start\":44812},{\"end\":46055,\"start\":45967}]", "bib_author": "[{\"end\":37696,\"start\":37684},{\"end\":37708,\"start\":37696},{\"end\":37925,\"start\":37914},{\"end\":37938,\"start\":37925},{\"end\":38233,\"start\":38222},{\"end\":38244,\"start\":38233},{\"end\":38254,\"start\":38244},{\"end\":38475,\"start\":38464},{\"end\":38483,\"start\":38475},{\"end\":38490,\"start\":38483},{\"end\":38499,\"start\":38490},{\"end\":38509,\"start\":38499},{\"end\":38737,\"start\":38726},{\"end\":38747,\"start\":38737},{\"end\":38964,\"start\":38953},{\"end\":38974,\"start\":38964},{\"end\":38984,\"start\":38974},{\"end\":38993,\"start\":38984},{\"end\":39002,\"start\":38993},{\"end\":39013,\"start\":39002},{\"end\":39023,\"start\":39013},{\"end\":39031,\"start\":39023},{\"end\":39199,\"start\":39188},{\"end\":39207,\"start\":39199},{\"end\":39218,\"start\":39207},{\"end\":39228,\"start\":39218},{\"end\":39432,\"start\":39419},{\"end\":39441,\"start\":39432},{\"end\":39454,\"start\":39441},{\"end\":39470,\"start\":39454},{\"end\":39489,\"start\":39470},{\"end\":39499,\"start\":39489},{\"end\":39508,\"start\":39499},{\"end\":39796,\"start\":39783},{\"end\":39809,\"start\":39796},{\"end\":39818,\"start\":39809},{\"end\":39826,\"start\":39818},{\"end\":39835,\"start\":39826},{\"end\":40106,\"start\":40096},{\"end\":40454,\"start\":40443},{\"end\":40465,\"start\":40454},{\"end\":40478,\"start\":40465},{\"end\":40487,\"start\":40478},{\"end\":40493,\"start\":40487},{\"end\":40504,\"start\":40493},{\"end\":40515,\"start\":40504},{\"end\":40523,\"start\":40515},{\"end\":40756,\"start\":40740},{\"end\":40766,\"start\":40756},{\"end\":40777,\"start\":40766},{\"end\":41096,\"start\":41088},{\"end\":41114,\"start\":41096},{\"end\":41383,\"start\":41374},{\"end\":41395,\"start\":41383},{\"end\":41409,\"start\":41395},{\"end\":41417,\"start\":41409},{\"end\":41427,\"start\":41417},{\"end\":41680,\"start\":41673},{\"end\":41689,\"start\":41680},{\"end\":41903,\"start\":41895},{\"end\":41914,\"start\":41903},{\"end\":41922,\"start\":41914},{\"end\":41932,\"start\":41922},{\"end\":41948,\"start\":41932},{\"end\":42228,\"start\":42220},{\"end\":42239,\"start\":42228},{\"end\":42249,\"start\":42239},{\"end\":42259,\"start\":42249},{\"end\":42277,\"start\":42259},{\"end\":42604,\"start\":42592},{\"end\":42612,\"start\":42604},{\"end\":42905,\"start\":42894},{\"end\":43094,\"start\":43088},{\"end\":43103,\"start\":43094},{\"end\":43112,\"start\":43103},{\"end\":43123,\"start\":43112},{\"end\":43396,\"start\":43387},{\"end\":43407,\"start\":43396},{\"end\":43418,\"start\":43407},{\"end\":43429,\"start\":43418},{\"end\":43438,\"start\":43429},{\"end\":43790,\"start\":43778},{\"end\":43802,\"start\":43790},{\"end\":43808,\"start\":43802},{\"end\":43815,\"start\":43808},{\"end\":43824,\"start\":43815},{\"end\":44133,\"start\":44118},{\"end\":44147,\"start\":44133},{\"end\":44156,\"start\":44147},{\"end\":44480,\"start\":44470},{\"end\":44495,\"start\":44480},{\"end\":44504,\"start\":44495},{\"end\":44516,\"start\":44504},{\"end\":44865,\"start\":44854},{\"end\":44876,\"start\":44865},{\"end\":44889,\"start\":44876},{\"end\":44898,\"start\":44889},{\"end\":44907,\"start\":44898},{\"end\":44921,\"start\":44907},{\"end\":44931,\"start\":44921},{\"end\":45202,\"start\":45193},{\"end\":45213,\"start\":45202},{\"end\":45501,\"start\":45488},{\"end\":45606,\"start\":45598},{\"end\":45615,\"start\":45606},{\"end\":45623,\"start\":45615},{\"end\":45631,\"start\":45623},{\"end\":45640,\"start\":45631},{\"end\":45650,\"start\":45640},{\"end\":45663,\"start\":45650},{\"end\":45673,\"start\":45663},{\"end\":46065,\"start\":46057},{\"end\":46077,\"start\":46065},{\"end\":46319,\"start\":46311},{\"end\":46330,\"start\":46319},{\"end\":46342,\"start\":46330},{\"end\":46354,\"start\":46342},{\"end\":37696,\"start\":37684},{\"end\":37708,\"start\":37696},{\"end\":37925,\"start\":37914},{\"end\":37938,\"start\":37925},{\"end\":38233,\"start\":38222},{\"end\":38244,\"start\":38233},{\"end\":38254,\"start\":38244},{\"end\":38475,\"start\":38464},{\"end\":38483,\"start\":38475},{\"end\":38490,\"start\":38483},{\"end\":38499,\"start\":38490},{\"end\":38509,\"start\":38499},{\"end\":38737,\"start\":38726},{\"end\":38747,\"start\":38737},{\"end\":38964,\"start\":38953},{\"end\":38974,\"start\":38964},{\"end\":38984,\"start\":38974},{\"end\":38993,\"start\":38984},{\"end\":39002,\"start\":38993},{\"end\":39013,\"start\":39002},{\"end\":39023,\"start\":39013},{\"end\":39031,\"start\":39023},{\"end\":39199,\"start\":39188},{\"end\":39207,\"start\":39199},{\"end\":39218,\"start\":39207},{\"end\":39228,\"start\":39218},{\"end\":39432,\"start\":39419},{\"end\":39441,\"start\":39432},{\"end\":39454,\"start\":39441},{\"end\":39470,\"start\":39454},{\"end\":39489,\"start\":39470},{\"end\":39499,\"start\":39489},{\"end\":39508,\"start\":39499},{\"end\":39796,\"start\":39783},{\"end\":39809,\"start\":39796},{\"end\":39818,\"start\":39809},{\"end\":39826,\"start\":39818},{\"end\":39835,\"start\":39826},{\"end\":40106,\"start\":40096},{\"end\":40454,\"start\":40443},{\"end\":40465,\"start\":40454},{\"end\":40478,\"start\":40465},{\"end\":40487,\"start\":40478},{\"end\":40493,\"start\":40487},{\"end\":40504,\"start\":40493},{\"end\":40515,\"start\":40504},{\"end\":40523,\"start\":40515},{\"end\":40756,\"start\":40740},{\"end\":40766,\"start\":40756},{\"end\":40777,\"start\":40766},{\"end\":41096,\"start\":41088},{\"end\":41114,\"start\":41096},{\"end\":41383,\"start\":41374},{\"end\":41395,\"start\":41383},{\"end\":41409,\"start\":41395},{\"end\":41417,\"start\":41409},{\"end\":41427,\"start\":41417},{\"end\":41680,\"start\":41673},{\"end\":41689,\"start\":41680},{\"end\":41903,\"start\":41895},{\"end\":41914,\"start\":41903},{\"end\":41922,\"start\":41914},{\"end\":41932,\"start\":41922},{\"end\":41948,\"start\":41932},{\"end\":42228,\"start\":42220},{\"end\":42239,\"start\":42228},{\"end\":42249,\"start\":42239},{\"end\":42259,\"start\":42249},{\"end\":42277,\"start\":42259},{\"end\":42604,\"start\":42592},{\"end\":42612,\"start\":42604},{\"end\":42905,\"start\":42894},{\"end\":43094,\"start\":43088},{\"end\":43103,\"start\":43094},{\"end\":43112,\"start\":43103},{\"end\":43123,\"start\":43112},{\"end\":43396,\"start\":43387},{\"end\":43407,\"start\":43396},{\"end\":43418,\"start\":43407},{\"end\":43429,\"start\":43418},{\"end\":43438,\"start\":43429},{\"end\":43790,\"start\":43778},{\"end\":43802,\"start\":43790},{\"end\":43808,\"start\":43802},{\"end\":43815,\"start\":43808},{\"end\":43824,\"start\":43815},{\"end\":44133,\"start\":44118},{\"end\":44147,\"start\":44133},{\"end\":44156,\"start\":44147},{\"end\":44480,\"start\":44470},{\"end\":44495,\"start\":44480},{\"end\":44504,\"start\":44495},{\"end\":44516,\"start\":44504},{\"end\":44865,\"start\":44854},{\"end\":44876,\"start\":44865},{\"end\":44889,\"start\":44876},{\"end\":44898,\"start\":44889},{\"end\":44907,\"start\":44898},{\"end\":44921,\"start\":44907},{\"end\":44931,\"start\":44921},{\"end\":45202,\"start\":45193},{\"end\":45213,\"start\":45202},{\"end\":45501,\"start\":45488},{\"end\":45606,\"start\":45598},{\"end\":45615,\"start\":45606},{\"end\":45623,\"start\":45615},{\"end\":45631,\"start\":45623},{\"end\":45640,\"start\":45631},{\"end\":45650,\"start\":45640},{\"end\":45663,\"start\":45650},{\"end\":45673,\"start\":45663},{\"end\":46065,\"start\":46057},{\"end\":46077,\"start\":46065},{\"end\":46319,\"start\":46311},{\"end\":46330,\"start\":46319},{\"end\":46342,\"start\":46330},{\"end\":46354,\"start\":46342}]", "bib_venue": "[{\"end\":37682,\"start\":37578},{\"end\":37994,\"start\":37954},{\"end\":38220,\"start\":38121},{\"end\":38542,\"start\":38525},{\"end\":38785,\"start\":38747},{\"end\":39046,\"start\":39031},{\"end\":39268,\"start\":39228},{\"end\":39417,\"start\":39368},{\"end\":39781,\"start\":39724},{\"end\":40190,\"start\":40106},{\"end\":40441,\"start\":40385},{\"end\":40836,\"start\":40777},{\"end\":41170,\"start\":41114},{\"end\":41480,\"start\":41427},{\"end\":41737,\"start\":41689},{\"end\":42032,\"start\":41964},{\"end\":42350,\"start\":42277},{\"end\":42590,\"start\":42502},{\"end\":42910,\"start\":42905},{\"end\":43086,\"start\":43000},{\"end\":43497,\"start\":43438},{\"end\":43862,\"start\":43824},{\"end\":44215,\"start\":44156},{\"end\":44577,\"start\":44516},{\"end\":44990,\"start\":44931},{\"end\":45285,\"start\":45229},{\"end\":45486,\"start\":45424},{\"end\":45755,\"start\":45689},{\"end\":46128,\"start\":46077},{\"end\":37682,\"start\":37578},{\"end\":37994,\"start\":37954},{\"end\":38220,\"start\":38121},{\"end\":38542,\"start\":38525},{\"end\":38785,\"start\":38747},{\"end\":39046,\"start\":39031},{\"end\":39268,\"start\":39228},{\"end\":39417,\"start\":39368},{\"end\":39781,\"start\":39724},{\"end\":40190,\"start\":40106},{\"end\":40441,\"start\":40385},{\"end\":40836,\"start\":40777},{\"end\":41170,\"start\":41114},{\"end\":41480,\"start\":41427},{\"end\":41737,\"start\":41689},{\"end\":42032,\"start\":41964},{\"end\":42350,\"start\":42277},{\"end\":42590,\"start\":42502},{\"end\":42910,\"start\":42905},{\"end\":43086,\"start\":43000},{\"end\":43497,\"start\":43438},{\"end\":43862,\"start\":43824},{\"end\":44215,\"start\":44156},{\"end\":44577,\"start\":44516},{\"end\":44990,\"start\":44931},{\"end\":45285,\"start\":45229},{\"end\":45486,\"start\":45424},{\"end\":45755,\"start\":45689},{\"end\":46128,\"start\":46077}]"}}}, "year": 2023, "month": 12, "day": 17}