{"id": 221172825, "updated": "2023-10-06 11:58:05.241", "metadata": {"title": "How2Sign: A Large-scale Multimodal Dataset for Continuous American Sign Language", "authors": "[{\"first\":\"Amanda\",\"last\":\"Duarte\",\"middle\":[]},{\"first\":\"Shruti\",\"last\":\"Palaskar\",\"middle\":[]},{\"first\":\"Deepti\",\"last\":\"Ghadiyaram\",\"middle\":[]},{\"first\":\"Kenneth\",\"last\":\"DeHaan\",\"middle\":[]},{\"first\":\"Florian\",\"last\":\"Metze\",\"middle\":[]},{\"first\":\"Jordi\",\"last\":\"Torres\",\"middle\":[]},{\"first\":\"Xavier\",\"last\":\"Giro-i-Nieto\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 8, "day": 18}, "abstract": "Sign Language is the primary means of communication for the majority of the Deaf community. One of the factors that has hindered the progress in the areas of automatic sign language recognition, generation, and translation is the absence of large annotated datasets, especially continuous sign language datasets, i.e. datasets that are annotated and segmented at the sentence or utterance level. Towards this end, in this work we introduce How2Sign, a work-in-progress dataset collection. How2Sign consists of a parallel corpus of 80 hours of sign language videos (collected with multi-view RGB and depth sensor data) with corresponding speech transcriptions and gloss annotations. In addition, a three-hour subset was further recorded in a geodesic dome setup using hundreds of cameras and sensors, which enables detailed 3D reconstruction and pose estimation and paves the way for vision systems to understand the 3D geometry of sign language.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2008.08143", "mag": "3059502370", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/DuartePVGDMTG21", "doi": "10.1109/cvpr46437.2021.00276"}}, "content": {"source": {"pdf_hash": "932168d1c27390506049adf679b42d3aac53f61c", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2008.08143v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2008.08143", "status": "GREEN"}}, "grobid": {"id": "bba336cf71e8c62ec5052860df9f54524ff3d037", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/932168d1c27390506049adf679b42d3aac53f61c.txt", "contents": "\nHow2Sign: A Large-scale Multimodal Dataset for Continuous American Sign Language Gloss Annotation HI, ME FS-AMELIA WILL EXPLAIN HOW REMOVE GUM FROM YOUR HAIR Multi-view RGB videos RGB-D videos Body-face-hands keypoints Panoptic data (only for a subset) How2Sign dataset 2D keypoints estimation using OpenPose\n\n\nAmanda Duarte \nUniversitat Polit\u00e8cnica de Catalunya\n\n\nBarcelona Supercomputing Center\n\n\nShruti Palaskar \nCarnegie Mellon University\n\n\nDeepti Ghadiyaram \nKenneth Dehaan \nGallaudet University\n5 FacebookAI\n\nFlorian Metze \nCarnegie Mellon University\n\n\nJordi Torres \nUniversitat Polit\u00e8cnica de Catalunya\n\n\nBarcelona Supercomputing Center\n\n\nXavier Giro-I-Nieto \nUniversitat Polit\u00e8cnica de Catalunya\n\n\nBarcelona Supercomputing Center\n\n\nHow2Sign: A Large-scale Multimodal Dataset for Continuous American Sign Language Gloss Annotation HI, ME FS-AMELIA WILL EXPLAIN HOW REMOVE GUM FROM YOUR HAIR Multi-view RGB videos RGB-D videos Body-face-hands keypoints Panoptic data (only for a subset) How2Sign dataset 2D keypoints estimation using OpenPose\n3D keypoints estimation Multi-view VGA and HD videos Fig. 1. Samples of data included in the How2Sign dataset.\nHow2 datasetSpeech SignalEnglish TranscriptionHi, I'm Amelia and I'm going to talk to you about how to remove gum from hair.\n\nIntroduction\n\nSign Languages are the primary means of communication for an estimated 466 million deaf or hard-of-hearing people worldwide [1]. One of the most important factors that has hindered the progress in the areas of automatic sign language recognition, generation, and translation is the absence of large annotated datasets [2], especially continuous sign language datasets, i.e. datasets that are annotated and segmented at the sentence or utterance level.\n\nAn important factor for the lack of datasets is that collection and annotation of continuous sign language data is a laborious and expensive task. It needs to be done by linguistic experts together with a native speaker, e.g a Deaf person. Although there are datasets available from linguistic sources [14,12,7,6] and sign language interpretations from broadcast [5,17,10], they are usually weakly annotated or have a small vocabulary size, and often lack all the modalities required for cross-modal sign language translation research (e.g spoken language and the corresponding aligned sign language translation). Table 1 provides an overview of publicly available datasets for continuous sign language in comparison with the How2Sign, our work-in-progress dataset collection. How2Sign is a large-scale collection of multi-view and multimodal signing videos in American Sign Language (ASL) for over 2500 instructional videos from the existing How2 dataset [13]. Figure 1 shows sample of the data contained in the dataset. Working in close collaboration with native ASL speakers and professional interpreters, we collected approximately 80 hours of multiview (multiple RGB and a depth sensor) signing videos and corresponding gloss annotations 1 [11]. In addition, a three-hour subset was further recorded in a geodesic dome setup using hundreds of cameras and sensors, which enables detailed 3D reconstruction and pose estimation and paves the way for vision systems to understand the 3D geometry of sign language. The How2Sign dataset consists of a parallel corpus of instructional videos and their corresponding American Sign Language translation (ASL) videos and annotations. 80 hours of multiview ASL videos were collected, as well as gloss [11] annotations. The multiple data sources allow for high-quality automatic annotations of 2D body, face and hand keypoints that will be made available together with the videos. For the subset that was recorded in the Panoptic studio [9], accurate 3D keypoints will also be made available.\n\n\nThe How2Sign dataset\n\nThe instructional videos that were translated to ASL come from the existing How2 dataset [13], a publicly available large-scale multi-modal dataset that covers a variety of topics with utterance-level time alignments between the speech and the ground-truth English transcription. We selected a 60-hour subset of the How2 300h set for the How2Sign training set, and used the complete 300h-subset validation and test sets as the How2Sign validation and test sets, respectively.\n\nDetailed statistics are presented in Table 2. The collected corpus contains video recordings by 11 different signers, covering more than 35k sentences, with a English vocabulary of more than 16k different English words.\n\nAs shown in Table 2, the validation and test sets explicitly contain videos from two signers that are not present in the training set. We envision this to be used for measuring the generalization across different signers. Moreover, a subset (approx. 70 min) of the test set has multiple ASL translation videos for the same instructional video, recorded both with a signer that appears in the training set and with a signer that does not. Recording Setup. To collect American Sign Language translation videos we collaborated with 11 signers. The following sample group self-identified as: 45% hearing (n=5), 36% Deaf (n=4), and 18% hard-of-hearing (n=2).\n\nThe video were recorded in a supervised setting, in two different studios: the green screen studio and the Panoptic studio, both presented below. We recorded the complete 80 hours of the dataset in the green screen studio setting. We then further collected duplicate recordings in the muti-view studio for a smaller subset of videos from the validation and test splits (approx. 3h in total).\n\nThe green screen studio was equipped with a depth and a high definition (HD) camera placed in frontal view of a green screen, and another HD camera placed at a lateral view. All three cameras recorded videos at 1280x720 resolution, at 30 frames per second. The Panoptic Studio [9] is a system equipped with 480 VGA cameras, 31 HD cameras and 10 RGB-D sensors all synchronized. All cameras were mounted over the surface of a geodesic dome 2 , providing redundancy for weak perceptual processes (such as pose detection and tracking) and robustness to occlusion. In addition to the multiview VGA and HD videos, the recording system also estimated 3D skeletons poses of the interpreters, that will also be made publicly available. Recording pipeline. Before recording the ASL translations for each video, the signer would watch the video and read the transcript as subtitles. After that, they were asked to performed the translation into ASL while watching the video with subtitles at a slightly slower-than-normal (0.75) speed. For each hour of video recorded, preparation, recording and video review required a 3 hour process on average. The dataset was recorded in 65 days within a period of 6 months. Privacy, Bias and Ethical Considerations: Privacy. All research steps followed procedures approved by an Institutional Review Board including a Human Subjects Research training done by the researchers and a consent form provided by the participants agreeing on being recorded and making their data available for research purposes. Data distribution and bias. In order to create the data as balanced as possible as well as a signer independent dataset, we distribute the signers in the recordings across the different splits. Geographic. All the participants were born and raised in the USA and learned ASL as their primary or second language at school time.\n\nSigner variety. Our dataset was recorded with the collaboration of 11 signers with different body proportions. Six of them were self-identified male and five self-identified female. Data bias. Our data does not contain large diversity in race/ethnicity, skin tone, background scenery, lighting conditions and camera quality.\n\nTable 1 .\n1Publicly available, continuous sign language datasets. SL refers to the sign language used, trans. refers to translation of the signing videos in its correspondent spoken language, gloss to gloss annotations[11] and speech to a parallel speech track.Name \nSL \nVocab. Duration(h) \nContent \ntrans. gloss speech \nVideo-Based CSL [7] \nChinese \n178 \n100 \nSIGNUM [15] \nGerman 450 \n55 \nRWTH-Phoenix-2014T [4] German \n3k \n11 \nDGS-Korpus [8] \nGerman \n-\n50 \nBoston104 [16] \nASL \n104 \n8.7 (min) \nHow2Sign (ours) \nASL \n16k \n79 \n\n\n\nTable 2 .\n2How2Sign dataset statistics. Keypoints for green screen studio were estimated by Open-Pose[3]. The number of unique signers is 11.Green screen studio Panoptic studio \ntrain \nval \ntest \nval \ntest \nVideos \n2,213 \n132 \n184 \n48 \n76 \nDuration (h) \n69.62 \n3.91 \n5.59 \n1.14 1.82 \nSentences \n31,128 1,741 2,322 \nVocabulary size \n15,686 3,218 3,670 \nSigners not in train set \n0 \n1 \n2 \n2 \n\n\nwork in progress.\nhttp://www.cs.cmu.edu/~hanbyulj/panoptic-studio/\nAcknowledgementsThis work received funding from Facebook through gifts to Carnegie Mellon University and Universitat Polit\u00e8cnica de Catalunya. Amanda Duarte has received support from la Caixa Foundation (ID 100010434) under the fellowship code LCF/BQ/IN18/11660029. Shruti Palaskar was supported by the Facebook Fellowship program. This work has also received funding by the project TEC2016-75976-R of the Spanish Ministerio de Economa y Competitividad and the European Regional Development Fund. The authors would like to thank Chinmay H\u00e9jmadi, Xabier Garcia and Brandon Taylor for their help during the data collection and processing.\nDeafness and hearing loss. W H O , . 2019, W.H.O.: Deafness and hearing loss, https://www.who.int/news-room/ fact-sheets/detail/deafness-and-hearing-loss 1\n\nSign language recognition, generation, and translation: An interdisciplinary perspective. D Bragg, O Koller, M Bellard, L Berke, P Boudreault, A Braffort, N Caselli, M Huenerfauth, H Kacorri, T Verhoef, ACM SIGAC-CESS Conference on Computers and Accessibility. 1Bragg, D., Koller, O., Bellard, M., Berke, L., Boudreault, P., Braffort, A., Caselli, N., Huenerfauth, M., Kacorri, H., Verhoef, T., et al.: Sign language recognition, generation, and translation: An interdisciplinary perspective. In: ACM SIGAC- CESS Conference on Computers and Accessibility. pp. 16-31 (2019) 1\n\nOpenpose: realtime multi-person 2d pose estimation using part affinity fields. Z Cao, G Hidalgo, T Simon, S E Wei, Y Sheikh, arXiv:1812.08008arXiv preprintCao, Z., Hidalgo, G., Simon, T., Wei, S.E., Sheikh, Y.: Openpose: real- time multi-person 2d pose estimation using part affinity fields. arXiv preprint arXiv:1812.08008 (2018) 2\n\nNeural sign language translation. Cihan Camgoz, N Hadfield, S Koller, O Ney, H Bowden, R , CVPR. pp. Cihan Camgoz, N., Hadfield, S., Koller, O., Ney, H., Bowden, R.: Neural sign language translation. In: CVPR. pp. 7784-7793 (2018) 2\n\nLearning signs from subtitles: A weakly supervised approach to sign language recognition. H Cooper, R Bowden, CVPR. IEEE1Cooper, H., Bowden, R.: Learning signs from subtitles: A weakly supervised ap- proach to sign language recognition. In: CVPR. pp. 2568-2574. IEEE (2009) 1\n\nEfficient approximations to modelbased joint tracking and recognition of continuous sign language. P Dreuw, J Forster, T Deselaers, H Ney, International Conference on Automatic Face & Gesture Recognition. IEEE1Dreuw, P., Forster, J., Deselaers, T., Ney, H.: Efficient approximations to model- based joint tracking and recognition of continuous sign language. In: International Conference on Automatic Face & Gesture Recognition. pp. 1-6. IEEE (2008) 1\n\nVideo-based sign language recognition without temporal segmentation. J Huang, W Zhou, Q Zhang, H Li, W Li, AAAI. 1Huang, J., Zhou, W., Zhang, Q., Li, H., Li, W.: Video-based sign language recog- nition without temporal segmentation. In: AAAI (2018) 1, 2\n\nPublishing dgs corpus data: Different formats for different needs. E Jahn, R Konrad, G Langer, S Wagner, T Hanke, LREC. Jahn, E., Konrad, R., Langer, G., Wagner, S., Hanke, T.: Publishing dgs corpus data: Different formats for different needs. In: LREC. pp. 83-90 (1981) 2\n\nPanoptic studio: A massively multiview system for social motion capture. H Joo, H Liu, L Tan, L Gui, B Nabbe, I Matthews, T Kanade, S Nobuhara, Y Sheikh, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer Vision23Joo, H., Liu, H., Tan, L., Gui, L., Nabbe, B., Matthews, I., Kanade, T., Nobuhara, S., Sheikh, Y.: Panoptic studio: A massively multiview system for social motion capture. In: Proceedings of the IEEE International Conference on Computer Vi- sion. pp. 3334-3342 (2015) 2, 3\n\nContinuous sign language recognition: Towards large vocabulary statistical recognition systems handling multiple signers. Computer Vision and Image Understanding 141. O Koller, J Forster, H Ney, 1Koller, O., Forster, J., Ney, H.: Continuous sign language recognition: Towards large vocabulary statistical recognition systems handling multiple signers. Com- puter Vision and Image Understanding 141, 108-125 (2015) 1\n\nS K Liddell, Grammar, gesture, and meaning in American Sign Language. Cambridge University Press2Liddell, S.K., et al.: Grammar, gesture, and meaning in American Sign Language. Cambridge University Press (2003) 2\n\nDicta-sign-building a multilingual sign language corpus. S Matthes, T Hanke, A Regen, J Storz, S Worseck, E Efthimiou, A L Dimou, A Braffort, J Glauert, E Safar, LREC. Istanbul1Matthes, S., Hanke, T., Regen, A., Storz, J., Worseck, S., Efthimiou, E., Dimou, A.L., Braffort, A., Glauert, J., Safar, E.: Dicta-sign-building a multilingual sign language corpus. In: LREC. Istanbul (2012) 1\n\nR Sanabria, O Caglayan, S Palaskar, D Elliott, L Barrault, L Specia, F Metze, arXiv:1811.00347How2: a large-scale dataset for multimodal language understanding. arXiv preprintSanabria, R., Caglayan, O., Palaskar, S., Elliott, D., Barrault, L., Specia, L., Metze, F.: How2: a large-scale dataset for multimodal language understanding. arXiv preprint arXiv:1811.00347 (2018) 2\n\nBuilding the british sign language corpus. Language Documentation & Conservation pp. A Schembri, J Fenlon, R Rentelis, S Reynolds, K Cormier, 1Schembri, A., Fenlon, J., Rentelis, R., Reynolds, S., Cormier, K.: Building the british sign language corpus. Language Documentation & Conservation pp. 136- 154 (2013) 1\n\nSignum database: Video corpus for signer-independent continuous sign language recognition. Von Agris, U Kraiss, K F , Workshop on Representation and Processing of Sign Languages. 2Von Agris, U., Kraiss, K.F.: Signum database: Video corpus for signer-independent continuous sign language recognition. In: Workshop on Representation and Pro- cessing of Sign Languages. pp. 243-246 (2010) 2\n\nContinuous sign language recognition-approaches from speech recognition and available data resources. M Zahedi, P Dreuw, D Rybach, T Deselaers, H Ney, Workshop on Representation and Processing of Sign Languages. Zahedi, M., Dreuw, P., Rybach, D., Deselaers, T., Ney, H.: Continuous sign lan- guage recognition-approaches from speech recognition and available data resources. In: Workshop on Representation and Processing of Sign Languages (2006) 2\n\nNn-based czech sign language synthesis. J Zelinka, J Kanis, P Salajka, International Conf. on Speech and Computer. Springer1Zelinka, J., Kanis, J., Salajka, P.: Nn-based czech sign language synthesis. In: International Conf. on Speech and Computer. pp. 559-568. Springer (2019) 1\n", "annotations": {"author": "[{\"end\":399,\"start\":312},{\"end\":445,\"start\":400},{\"end\":464,\"start\":446},{\"end\":515,\"start\":465},{\"end\":559,\"start\":516},{\"end\":646,\"start\":560},{\"end\":740,\"start\":647}]", "publisher": null, "author_last_name": "[{\"end\":325,\"start\":319},{\"end\":415,\"start\":407},{\"end\":463,\"start\":453},{\"end\":479,\"start\":473},{\"end\":529,\"start\":524},{\"end\":572,\"start\":566},{\"end\":666,\"start\":654}]", "author_first_name": "[{\"end\":318,\"start\":312},{\"end\":406,\"start\":400},{\"end\":452,\"start\":446},{\"end\":472,\"start\":465},{\"end\":523,\"start\":516},{\"end\":565,\"start\":560},{\"end\":653,\"start\":647}]", "author_affiliation": "[{\"end\":364,\"start\":327},{\"end\":398,\"start\":366},{\"end\":444,\"start\":417},{\"end\":514,\"start\":481},{\"end\":558,\"start\":531},{\"end\":611,\"start\":574},{\"end\":645,\"start\":613},{\"end\":705,\"start\":668},{\"end\":739,\"start\":707}]", "title": "[{\"end\":309,\"start\":1},{\"end\":1049,\"start\":741}]", "venue": null, "abstract": "[{\"end\":1285,\"start\":1161}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1428,\"start\":1425},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1622,\"start\":1619},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2060,\"start\":2056},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2063,\"start\":2060},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2065,\"start\":2063},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2067,\"start\":2065},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2120,\"start\":2117},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2123,\"start\":2120},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2126,\"start\":2123},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2714,\"start\":2710},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3003,\"start\":2999},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3503,\"start\":3499},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3737,\"start\":3734},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3907,\"start\":3903},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7968,\"start\":7964},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":8380,\"start\":8377}]", "figure": "[{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":8274,\"start\":7745},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":8667,\"start\":8275}]", "paragraph": "[{\"end\":1752,\"start\":1301},{\"end\":3789,\"start\":1754},{\"end\":4289,\"start\":3814},{\"end\":4510,\"start\":4291},{\"end\":5165,\"start\":4512},{\"end\":5558,\"start\":5167},{\"end\":7418,\"start\":5560},{\"end\":7744,\"start\":7420}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":2375,\"start\":2368},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":4335,\"start\":4328},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":4531,\"start\":4524}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1299,\"start\":1287},{\"attributes\":{\"n\":\"2\"},\"end\":3812,\"start\":3792},{\"end\":7755,\"start\":7746},{\"end\":8285,\"start\":8276}]", "table": "[{\"end\":8274,\"start\":8007},{\"end\":8667,\"start\":8417}]", "figure_caption": "[{\"end\":8007,\"start\":7757},{\"end\":8417,\"start\":8287}]", "figure_ref": "[{\"end\":2724,\"start\":2716}]", "bib_author_first_name": "[{\"end\":9400,\"start\":9399},{\"end\":9404,\"start\":9401},{\"end\":9620,\"start\":9619},{\"end\":9629,\"start\":9628},{\"end\":9639,\"start\":9638},{\"end\":9650,\"start\":9649},{\"end\":9659,\"start\":9658},{\"end\":9673,\"start\":9672},{\"end\":9685,\"start\":9684},{\"end\":9696,\"start\":9695},{\"end\":9711,\"start\":9710},{\"end\":9722,\"start\":9721},{\"end\":10185,\"start\":10184},{\"end\":10192,\"start\":10191},{\"end\":10203,\"start\":10202},{\"end\":10212,\"start\":10211},{\"end\":10214,\"start\":10213},{\"end\":10221,\"start\":10220},{\"end\":10478,\"start\":10473},{\"end\":10488,\"start\":10487},{\"end\":10500,\"start\":10499},{\"end\":10510,\"start\":10509},{\"end\":10517,\"start\":10516},{\"end\":10527,\"start\":10526},{\"end\":10764,\"start\":10763},{\"end\":10774,\"start\":10773},{\"end\":11050,\"start\":11049},{\"end\":11059,\"start\":11058},{\"end\":11070,\"start\":11069},{\"end\":11083,\"start\":11082},{\"end\":11473,\"start\":11472},{\"end\":11482,\"start\":11481},{\"end\":11490,\"start\":11489},{\"end\":11499,\"start\":11498},{\"end\":11505,\"start\":11504},{\"end\":11726,\"start\":11725},{\"end\":11734,\"start\":11733},{\"end\":11744,\"start\":11743},{\"end\":11754,\"start\":11753},{\"end\":11764,\"start\":11763},{\"end\":12006,\"start\":12005},{\"end\":12013,\"start\":12012},{\"end\":12020,\"start\":12019},{\"end\":12027,\"start\":12026},{\"end\":12034,\"start\":12033},{\"end\":12043,\"start\":12042},{\"end\":12055,\"start\":12054},{\"end\":12065,\"start\":12064},{\"end\":12077,\"start\":12076},{\"end\":12651,\"start\":12650},{\"end\":12661,\"start\":12660},{\"end\":12672,\"start\":12671},{\"end\":12901,\"start\":12900},{\"end\":12903,\"start\":12902},{\"end\":13172,\"start\":13171},{\"end\":13183,\"start\":13182},{\"end\":13192,\"start\":13191},{\"end\":13201,\"start\":13200},{\"end\":13210,\"start\":13209},{\"end\":13221,\"start\":13220},{\"end\":13234,\"start\":13233},{\"end\":13236,\"start\":13235},{\"end\":13245,\"start\":13244},{\"end\":13257,\"start\":13256},{\"end\":13268,\"start\":13267},{\"end\":13503,\"start\":13502},{\"end\":13515,\"start\":13514},{\"end\":13527,\"start\":13526},{\"end\":13539,\"start\":13538},{\"end\":13550,\"start\":13549},{\"end\":13562,\"start\":13561},{\"end\":13572,\"start\":13571},{\"end\":13964,\"start\":13963},{\"end\":13976,\"start\":13975},{\"end\":13986,\"start\":13985},{\"end\":13998,\"start\":13997},{\"end\":14010,\"start\":14009},{\"end\":14286,\"start\":14283},{\"end\":14295,\"start\":14294},{\"end\":14305,\"start\":14304},{\"end\":14307,\"start\":14306},{\"end\":14684,\"start\":14683},{\"end\":14694,\"start\":14693},{\"end\":14703,\"start\":14702},{\"end\":14713,\"start\":14712},{\"end\":14726,\"start\":14725},{\"end\":15071,\"start\":15070},{\"end\":15082,\"start\":15081},{\"end\":15091,\"start\":15090}]", "bib_author_last_name": "[{\"end\":9626,\"start\":9621},{\"end\":9636,\"start\":9630},{\"end\":9647,\"start\":9640},{\"end\":9656,\"start\":9651},{\"end\":9670,\"start\":9660},{\"end\":9682,\"start\":9674},{\"end\":9693,\"start\":9686},{\"end\":9708,\"start\":9697},{\"end\":9719,\"start\":9712},{\"end\":9730,\"start\":9723},{\"end\":10189,\"start\":10186},{\"end\":10200,\"start\":10193},{\"end\":10209,\"start\":10204},{\"end\":10218,\"start\":10215},{\"end\":10228,\"start\":10222},{\"end\":10485,\"start\":10479},{\"end\":10497,\"start\":10489},{\"end\":10507,\"start\":10501},{\"end\":10514,\"start\":10511},{\"end\":10524,\"start\":10518},{\"end\":10771,\"start\":10765},{\"end\":10781,\"start\":10775},{\"end\":11056,\"start\":11051},{\"end\":11067,\"start\":11060},{\"end\":11080,\"start\":11071},{\"end\":11087,\"start\":11084},{\"end\":11479,\"start\":11474},{\"end\":11487,\"start\":11483},{\"end\":11496,\"start\":11491},{\"end\":11502,\"start\":11500},{\"end\":11508,\"start\":11506},{\"end\":11731,\"start\":11727},{\"end\":11741,\"start\":11735},{\"end\":11751,\"start\":11745},{\"end\":11761,\"start\":11755},{\"end\":11770,\"start\":11765},{\"end\":12010,\"start\":12007},{\"end\":12017,\"start\":12014},{\"end\":12024,\"start\":12021},{\"end\":12031,\"start\":12028},{\"end\":12040,\"start\":12035},{\"end\":12052,\"start\":12044},{\"end\":12062,\"start\":12056},{\"end\":12074,\"start\":12066},{\"end\":12084,\"start\":12078},{\"end\":12658,\"start\":12652},{\"end\":12669,\"start\":12662},{\"end\":12676,\"start\":12673},{\"end\":12911,\"start\":12904},{\"end\":13180,\"start\":13173},{\"end\":13189,\"start\":13184},{\"end\":13198,\"start\":13193},{\"end\":13207,\"start\":13202},{\"end\":13218,\"start\":13211},{\"end\":13231,\"start\":13222},{\"end\":13242,\"start\":13237},{\"end\":13254,\"start\":13246},{\"end\":13265,\"start\":13258},{\"end\":13274,\"start\":13269},{\"end\":13512,\"start\":13504},{\"end\":13524,\"start\":13516},{\"end\":13536,\"start\":13528},{\"end\":13547,\"start\":13540},{\"end\":13559,\"start\":13551},{\"end\":13569,\"start\":13563},{\"end\":13578,\"start\":13573},{\"end\":13973,\"start\":13965},{\"end\":13983,\"start\":13977},{\"end\":13995,\"start\":13987},{\"end\":14007,\"start\":13999},{\"end\":14018,\"start\":14011},{\"end\":14292,\"start\":14287},{\"end\":14302,\"start\":14296},{\"end\":14691,\"start\":14685},{\"end\":14700,\"start\":14695},{\"end\":14710,\"start\":14704},{\"end\":14723,\"start\":14714},{\"end\":14730,\"start\":14727},{\"end\":15079,\"start\":15072},{\"end\":15088,\"start\":15083},{\"end\":15099,\"start\":15092}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":9527,\"start\":9372},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":201645446},\"end\":10103,\"start\":9529},{\"attributes\":{\"doi\":\"arXiv:1812.08008\",\"id\":\"b2\"},\"end\":10437,\"start\":10105},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":4724109},\"end\":10671,\"start\":10439},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":2741978},\"end\":10948,\"start\":10673},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":668905},\"end\":11401,\"start\":10950},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":9005234},\"end\":11656,\"start\":11403},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":245129734},\"end\":11930,\"start\":11658},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":15005115},\"end\":12481,\"start\":11932},{\"attributes\":{\"id\":\"b9\"},\"end\":12898,\"start\":12483},{\"attributes\":{\"id\":\"b10\"},\"end\":13112,\"start\":12900},{\"attributes\":{\"id\":\"b11\"},\"end\":13500,\"start\":13114},{\"attributes\":{\"doi\":\"arXiv:1811.00347\",\"id\":\"b12\"},\"end\":13876,\"start\":13502},{\"attributes\":{\"id\":\"b13\"},\"end\":14190,\"start\":13878},{\"attributes\":{\"id\":\"b14\"},\"end\":14579,\"start\":14192},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":15953986},\"end\":15028,\"start\":14581},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":199537324},\"end\":15309,\"start\":15030}]", "bib_title": "[{\"end\":9617,\"start\":9529},{\"end\":10471,\"start\":10439},{\"end\":10761,\"start\":10673},{\"end\":11047,\"start\":10950},{\"end\":11470,\"start\":11403},{\"end\":11723,\"start\":11658},{\"end\":12003,\"start\":11932},{\"end\":14281,\"start\":14192},{\"end\":14681,\"start\":14581},{\"end\":15068,\"start\":15030}]", "bib_author": "[{\"end\":9407,\"start\":9399},{\"end\":9628,\"start\":9619},{\"end\":9638,\"start\":9628},{\"end\":9649,\"start\":9638},{\"end\":9658,\"start\":9649},{\"end\":9672,\"start\":9658},{\"end\":9684,\"start\":9672},{\"end\":9695,\"start\":9684},{\"end\":9710,\"start\":9695},{\"end\":9721,\"start\":9710},{\"end\":9732,\"start\":9721},{\"end\":10191,\"start\":10184},{\"end\":10202,\"start\":10191},{\"end\":10211,\"start\":10202},{\"end\":10220,\"start\":10211},{\"end\":10230,\"start\":10220},{\"end\":10487,\"start\":10473},{\"end\":10499,\"start\":10487},{\"end\":10509,\"start\":10499},{\"end\":10516,\"start\":10509},{\"end\":10526,\"start\":10516},{\"end\":10530,\"start\":10526},{\"end\":10773,\"start\":10763},{\"end\":10783,\"start\":10773},{\"end\":11058,\"start\":11049},{\"end\":11069,\"start\":11058},{\"end\":11082,\"start\":11069},{\"end\":11089,\"start\":11082},{\"end\":11481,\"start\":11472},{\"end\":11489,\"start\":11481},{\"end\":11498,\"start\":11489},{\"end\":11504,\"start\":11498},{\"end\":11510,\"start\":11504},{\"end\":11733,\"start\":11725},{\"end\":11743,\"start\":11733},{\"end\":11753,\"start\":11743},{\"end\":11763,\"start\":11753},{\"end\":11772,\"start\":11763},{\"end\":12012,\"start\":12005},{\"end\":12019,\"start\":12012},{\"end\":12026,\"start\":12019},{\"end\":12033,\"start\":12026},{\"end\":12042,\"start\":12033},{\"end\":12054,\"start\":12042},{\"end\":12064,\"start\":12054},{\"end\":12076,\"start\":12064},{\"end\":12086,\"start\":12076},{\"end\":12660,\"start\":12650},{\"end\":12671,\"start\":12660},{\"end\":12678,\"start\":12671},{\"end\":12913,\"start\":12900},{\"end\":13182,\"start\":13171},{\"end\":13191,\"start\":13182},{\"end\":13200,\"start\":13191},{\"end\":13209,\"start\":13200},{\"end\":13220,\"start\":13209},{\"end\":13233,\"start\":13220},{\"end\":13244,\"start\":13233},{\"end\":13256,\"start\":13244},{\"end\":13267,\"start\":13256},{\"end\":13276,\"start\":13267},{\"end\":13514,\"start\":13502},{\"end\":13526,\"start\":13514},{\"end\":13538,\"start\":13526},{\"end\":13549,\"start\":13538},{\"end\":13561,\"start\":13549},{\"end\":13571,\"start\":13561},{\"end\":13580,\"start\":13571},{\"end\":13975,\"start\":13963},{\"end\":13985,\"start\":13975},{\"end\":13997,\"start\":13985},{\"end\":14009,\"start\":13997},{\"end\":14020,\"start\":14009},{\"end\":14294,\"start\":14283},{\"end\":14304,\"start\":14294},{\"end\":14310,\"start\":14304},{\"end\":14693,\"start\":14683},{\"end\":14702,\"start\":14693},{\"end\":14712,\"start\":14702},{\"end\":14725,\"start\":14712},{\"end\":14732,\"start\":14725},{\"end\":15081,\"start\":15070},{\"end\":15090,\"start\":15081},{\"end\":15101,\"start\":15090}]", "bib_venue": "[{\"end\":12207,\"start\":12155},{\"end\":9397,\"start\":9372},{\"end\":9788,\"start\":9732},{\"end\":10182,\"start\":10105},{\"end\":10538,\"start\":10530},{\"end\":10787,\"start\":10783},{\"end\":11153,\"start\":11089},{\"end\":11514,\"start\":11510},{\"end\":11776,\"start\":11772},{\"end\":12153,\"start\":12086},{\"end\":12648,\"start\":12483},{\"end\":12968,\"start\":12913},{\"end\":13169,\"start\":13114},{\"end\":13661,\"start\":13596},{\"end\":13961,\"start\":13878},{\"end\":14369,\"start\":14310},{\"end\":14791,\"start\":14732},{\"end\":15143,\"start\":15101}]"}}}, "year": 2023, "month": 12, "day": 17}