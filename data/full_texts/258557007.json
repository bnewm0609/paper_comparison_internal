{"id": 258557007, "updated": "2023-10-05 01:12:49.571", "metadata": {"title": "Replicating Complex Dialogue Policy of Humans via Offline Imitation Learning with Supervised Regularization", "authors": "[{\"first\":\"Zhoujian\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Chenyang\",\"last\":\"Zhao\",\"middle\":[]},{\"first\":\"Zhengxing\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Nai\",\"last\":\"Ding\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Policy learning (PL) is a module of a task-oriented dialogue system that trains an agent to make actions in each dialogue turn. Imitating human action is a fundamental problem of PL. However, both supervised learning (SL) and reinforcement learning (RL) frameworks cannot imitate humans well. Training RL models require online interactions with user simulators, while simulating complex human policy is hard. Performances of SL-based models are restricted because of the covariate shift problem. Specifically, a dialogue is a sequential decision-making process where slight differences in current utterances and actions will cause significant differences in subsequent utterances. Therefore, the generalize ability of SL models is restricted because statistical characteristics of training and testing dialogue data gradually become different. This study proposed an offline imitation learning model that learns policy from real dialogue datasets and does not require user simulators. It also utilizes state transition information, which alleviates the influence of the covariate shift problem. We introduced a regularization trick to make our model can be effectively optimized. We investigated the performance of our model on four independent public dialogue datasets. The experimental result showed that our model performed better in the action prediction task.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2305.03987", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2305-03987", "doi": "10.48550/arxiv.2305.03987"}}, "content": {"source": {"pdf_hash": "a2e2652a81ac5ae0f9ec391593ac496248350a17", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2305.03987v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "174ad96d8f31f7181220cfea7c7c18db2539213a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a2e2652a81ac5ae0f9ec391593ac496248350a17.txt", "contents": "\nReplicating Complex Dialogue Policy of Humans via Offline Imitation Learning with Supervised Regularization\n\n\nZhoujian Sun \nZhejiang Lab\n\n\nChenyang Zhao c.zhao@zhejianglab.com \nZhejiang Lab\n\n\nZhengxing Huang zhengxinghuang@zju.edu.cn \nZhejiang University\n\n\nNai Ding nai_ding@zju.edu.cn \nZhejiang Lab\n\n\nZhejiang University\n\n\nReplicating Complex Dialogue Policy of Humans via Offline Imitation Learning with Supervised Regularization\n\nPolicy learning (PL) is a module of a taskoriented dialogue system that trains an agent to make actions in each dialogue turn. Imitating human action is a fundamental problem of PL. However, both supervised learning (SL) and reinforcement learning (RL) frameworks cannot imitate humans well. Training RL models require online interactions with user simulators, while simulating complex human policy is hard. Performances of SL-based models are restricted because of the covariate shift problem. Specifically, a dialogue is a sequential decision-making process where slight differences in current utterances and actions will cause significant differences in subsequent utterances. Therefore, the generalize ability of SL models is restricted because statistical characteristics of training and testing dialogue data gradually become different. This study proposed an offline imitation learning model that learns policy from real dialogue datasets and does not require user simulators. It also utilizes state transition information, which alleviates the influence of the covariate shift problem. We introduced a regularization trick to make our model can be effectively optimized. We investigated the performance of our model on four independent public dialogue datasets. The experimental result showed that our model performed better in the action prediction task.\n\nIntroduction\n\nPolicy learning (PL) is a module of a task-oriented dialogue system that trains an agent to make actions in each dialogue turn, where each action represents the topic of the following system utterance (Ni et al., 2022). Recent studies usually adopted the reinforcement learning (RL) framework to train a dialogue agent. These studies generally regarded a dialogue as a multi-round question-answer process, and the agent is responsible for asking questions to users (e.g., EAR, UniCorn (Lei et al., 2020;Deng et al., 2021)) or response prompts from users Seller: Good to know. I would have recommended White Christmas. The old Tim Allen Santa Clause movie is super fun and so is Home Alone.\n\nBuyer : Is the Tim Allen Santa Clause movie funny? I would love to watch a funny movie.\n\nSeller: Yes it is! It's a heart warming family comedy. Buyer : I accept this recommendation and would love to watch it with my family! [acknowledgment] [opinion enquiry]\n\n[preference confirmation]\n\n[acknowledgment]\n\n[recommendation]\n\n[personal experience]\n\n[acknowledgment] [credibility] Figure 1: A conversation snippet between a buyer without an explicit goal and a seller with sociable actions.\n\n(e.g., instructGPT, chatGPT 1 (Ouyang et al., 2022;OpenAI, 2022)) appropriately to obtain maximum rewards. These studies require simulators to train agents (Sutton and Barto, 2018). However, real dialogues are not limited to multiround question-answer processes. For example, when a seller needs to recommend a movie to a buyer, the buyer may not know what they want to buy. The seller needs to lead the conversation proactively and recommend items even if they are unfamiliar with the buyer's preferences. The seller must also adopt social skills such as acknowledging and sharing personal experiences to persuade the buyer to accept the recommended item (see Fig. 1, (Hayati et al., 2020)). Training RL-based agents to learn such complex policies requires interacting with a simulated buyer who can behave like real humans, which is beyond current technology's ability. Therefore, we argue that the RL-based agent cannot behave like a human in many complex dialogue scenarios .\n\nThis study aims to explore how to train an agent that can imitate the complex policy of humans. A straightforward approach would be to use the supervised learning (SL) method , which theoretically can learn arbitrary complex dialogue policy with annotated data. However, the practical performance of SL-based models is restricted because of the covariate shift problem (Brantley et al., 2019;Merdivan et al., 2017). Specifically, a dialogue is a sequential decisionmaking process where slight differences in current utterances and actions will cause significant differences in subsequent utterances. Under this circumstance, training and testing dialogue data gradually violate the independent and identically distributed (i.i.d.) assumption with the development of dialogues, leading the chance of SL-based models making mistakes to grow quadratically with dialogue turns increasing (Ross and Bagnell, 2010).\n\nThe main contribution of this study is that we proposed an offline imitation learning (OIL) based model named Supervise regularized Distributional correction estimation (SD) that alleviates the effect of covariate shift problem and learns dialogue policy better. Specifically, The SD model formulates a dialogue as a Markov Decision Process (MDP) and utilizes state-action-next-state tuples to train the agent. Ross et al. (2011) proved that utilizing the sequential state transition information can decrease the mistake growth speed from quadratic to linear, bringing performance improvement potential. The SD model learns policy from real annotated dialogue datasets, making it can learn complex human policy in an offline manner. Furthermore, we introduced a supervised regularization trick to optimize the SD model effectively.\n\nWe investigated the performance of our model on four independent public dialogue datasets with complex human policy. The result showed that our model performed better in predicting next-turn actions than SL and OIL baselines. Meanwhile, we empirically demonstrated that our SD model tackles the covariate shift problem better by comparing action prediction performances in the first and second half of dialogues.\n\n2 Related Work 2.1 Policy Learning SL has been utilized in PL for several decades and exhibited great ability (Ni et al., 2022;Henderson et al., 2008). Recent SL-based studies usually proposed end-to-end models tackling PL and natural language generation tasks simultaneously. For example, Lubis et al. (2020) utilized a variational auto-encoding method to optimize dialogue policy and generate responses; Sun et al. (2022) proposed a contrastive learning method for task-oriented dialogue. However, these studies did not investigate whether their models can replicate human action accurately. RL-based studies typically assume a user has an explicit goal when a dialogue starts. They investigated how to ask the most appropriate problem to know the user's goal as quickly as possible. These studies generally require an agenda-based user simulator (Li et al., 2016). For example, Deng et al. (2021) proposed a graph-based model for PL; Ren et al. (2021) improved model performance by introducing an external knowledge graph. Recently, Ouyang et al. (2022); OpenAI (2022) adopted the prompt method and RL framework to construct an end-to-end general dialogue system. These models obtained astonishing performance in answering user questions (response to user prompts).\n\n\nImitation Learning\n\nImitation learning (apprenticeship learning) trains agents via expert demonstration datasets, which consist of two categories of methods named behavior cloning and inverse RL, respectively (Abbeel and Ng, 2004). Behavior cloning can be regarded as a synonym for SL, while inverse RL trains an agent by optimizing the reward function and policy function alternatively (Abbeel and Ng, 2004). As the proposed SD model is a variant of an inverse RL model, we only briefly introduce related inverse RL-based advances here.  proved that the inverse RL training process is equivalent to minimizing the occupancy measure of expert policy and target policy under a regularized function in their GAIL model. However, the original inverse RL model and Ho's GAIL model are infeasible in purely offline settings, as estimating the occupancy measure of a target policy also requires online environment interaction. To solve this problem, Lee et al. (2019) proposed the DSFN that utilized a neural network to estimate the target policy's occupancy measure. Then, Nachum et al. (2019) and Kostrikov et al. (2020) tackle the problem by indirectly estimating the distribution ratio, respectively. Chan and van der Schaar (2021) proposed a variational inference-based model to learn policy, and Jarrett et al. (2020) utilized the energybased distribution and a surrogate function to learn policies, respectively. All these studies only focus on control problems with a low-dimensional state space. We did not find studies that used OIL in dialogue systems.\n\n\nMethodlogy\n\n\nPreliminary\n\nWe represent a dialogue as a model-free MDP without rewards (MDP\\R) defined by a tuple (S, A, p 0 , p(s |s, a), \u03c0(a|s), \u03b3), where S is the state space and s t = {U m 0 , U u 0 , ..., U m t , U u t } is a sample of S at dialogue turn t. U m t and U u t are utterance token lists of the agent and user at turn t, respectively. In other words, we define the dialogue context at turn t as the state. In the following content, we also use s, a and s , a to represent the current state, current action, next state, and next action, respectively. a t \u2208 [0, 1] k is a k-dimensional binary vector that represents an action in action set A. Each action consists of k types of sub-action, and we allow the agent to take multiple sub-actions simultaneously. The utterance order of sub-actions is ignored for simplicity. p 0 and p(s |s, a) represent the initial and state-action transition distribution, respectively. \u03c0(a|s) represents the policy function. \u03b3 is the discounted factor.\n\nWe train the agent in a minimal setting for ease of deployment. Specifically, the agent cannot interact with simulators, does not require dialogue state annotation, and has no access to external knowledge. Under the MDP\\R assumption, we represent the expert demonstration dataset as a set of the initial state and state-action-next-state transition\ntuples D = {s i 0 , s i , a i , s i } D\ni generated by an unknown expert policy \u03c0 e . s i 0 is the initial state of the state sequence that contains s i .\n\n\nImitation Learning Objective\n\nThis study learns a policy \u03c0 : S \u2192 R |k| to recover \u03c0 e according to D. To utilize the sequential state transition information, we utilized the occupancy measure \u03c1 \u03c0 : S \u00d7 A \u2192 R defined as in Eq. 1 and Eq. 2 (Nachum et al., 2019):\n\u03b2 \u03c0,t (s) P (s = s t |s 0 \u223c p 0 , a k \u223c \u03c0(s k ), s k+1 \u223c p(s k+1 |s k , a k )) 0 < k < t,(1)\u03c1 \u03c0 (s, a) = (1 \u2212 \u03b3) \u221e t=0 \u03b3 t [ P (s t = s, a t = a|\u03b2 \u03c0,t , \u03c0)].\n(2)\n\nThe occupancy measure implicitly describes statetransition information in \u03b2 \u03c0,t . The occupancy measure can be interpreted as the distribution of stateaction pairs that an agent encounters when navigating the environment with policy \u03c0. Syed et al. (2008) proved that \u03c0 is the only policy whose occupancy measure is \u03c1 \u03c0 , and vice versa. Thus, we can use sequential state transition information and recover the \u03c0 e via \u03c0 by minimizing the difference between \u03c1 \u03c0 and \u03c1 \u03c0e . Inspired by the work on distribution correction estimation framework, we used the Donsker Varadhan representation of Kullback Leibler (KL) divergence and derived the objective function (Nachum et al., 2019;Kostrikov et al., 2020;Donsker and Varadhan, 1975):\nmin KL(\u03c1 \u03c0 ||\u03c1 \u03c0e ) = min max x : S\u00d7A\u2192R [ logE (s,a)\u223c\u03c1\u03c0 e e x(s,a) \u2212 E (s,a)\u223c\u03c1\u03c0 x(s, a)].\n(3) x(s, a) is an unknown function. To make Eq. 3 can be optimized in practice, we used the variable changing trick proposed in Nachum et al. (2019) that defined another state-action value function \u03d5 : S \u00d7 A \u2192 R that satisfies:\n\u03d5(s, a) x(s, a) + B \u03c0 \u03d5(s, a),(4)\nB \u03c0 \u03d5(s, a) = \u03b3E s \u223cp(s |s,a),a \u223c\u03c0(s ) \u03d5(s , a ). (5) According to Eq. 4, the last term of Eq. 3 follows Nachum et al. (2019):\nE (s,a)\u223c\u03c1\u03c0 x(s, a)/(1 \u2212 \u03b3) = \u221e t=0 \u03b3 t E s\u223c\u03b2\u03c0,t,a\u223c\u03c0 [\u03d5(s, a) \u2212 \u03b3E s \u223cp,a \u223c\u03c0 \u03d5(s , a )] = E s 0 \u223cp 0 ,a 0 \u223c\u03c0 \u03d5(s 0 , a 0 ).\n(6) Then, Eq. 3 can be reorganized as:\nmin KL(\u03c1 \u03c0 ||\u03c1 \u03c0e ) = min \u03c0 max \u03d5 [ (logE (s,a)\u223c\u03c1\u03c0 e (e \u03d5(s,a)\u2212\u03b3B \u03c0 \u03d5(s,a) ))\u2212 ((1 \u2212 \u03b3)E s 0 \u223cp 0 ,a\u223c\u03c0(s 0 ) \u03d5(s 0 , a 0 ))].(7)\nThe quantity of the expected Bellman operator B \u03c0 \u03d5(s, a) can be approximated by using the Fenchel duality or replaced by a sample \u03d5(s ,\u00e2 ) (Nachum et al., 2019;Kostrikov et al., 2020). For simplicity, we use the \u03d5(s ,\u00e2 ) to replace B \u03c0 \u03d5(s, a) in this study. Then, we can write the empirical objective function\u0134 OIL as:\nJ OIL (\u03c0, \u03d5) = \u03b3 \u2212 1 |M| |M| i=1 \u03d5(s i 0 ,\u00e2 i 0 )+ log |M| i=1 e \u03d5(s i ,a i )\u2212\u03b3\u03d5(s i ,\u00e2 i ) .(8)\nM represents a mini-batch data sampled from D, where\u00e2 i 0 and\u00e2 i indicated the sample of a i 0 and a i generated by \u03c0, respectively. We utilized a neural network parameterized by \u03b8 and a neural network parameterized by \u03bd to model \u03c0 and \u03d5, respectively. Both \u03c0 \u03b8 and \u03d5 \u03bd are represented as an encoder with a multi-layer perception (MLP).\n\u03c0 \u03b8 (s) = MLP(Encoder \u03c0 (s)) (9) \u03d5 \u03bd (s, a) = MLP(Encoder \u03d5 (s) \u2295 a)(10)\nNeural network language models (e.g., recurrent neural networks or transformers) can be used as the encoder. The Gumbel-softmax reparameterization trick can be used to generate one-hot encoded differentiable samples of a i 0 and a i (Jang et al., 2017). However, we canceled sampling and used the stochastic distribution of a i 0 and a i directly to decrease variance. Experimental results demonstrated the effectiveness of cancel sampling. The design allows it to adopt both a current state and a subsequent state to optimize parameters during the training phase, while it only needs a current state to generate actions during the testing phase.\n\n\nSupervised Regularization\n\nAlthough the model introduced in the last subsection can learn policy theoretically, it may fail in practice because it directly accepts natural language as input. Given such high-dimensional and sparse inputs, parameters in the model are extremely hard to optimize (Chan and van der Schaar, 2021). To alleviate this problem, we utilize the loss of SL to optimize parameters more effectively (Eq. 11). The OIL model with SL loss is called SD. The effectiveness of introducing SL was demonstrated in previous studies (Ouyang et al., 2022). Finally, we obtain the objective function shown in 12.\n\n\nAlgorithm 1 SD Training Process\n\nInput: Expert Dataset D, learning rate l, weight l\u03c0, l\u03d5 Output: \u03bd, \u03b8 1: Random initialize : \u03bd, \u03b8 2: while \u03bd and \u03b8 not converge do 3:\nSample batch {s i 0 , s i , a i , s i } M i=1 \u223c D 4: Compute\u00e2 i 0 = \u03c0 \u03b8 (a|s i 0 ) \u2200i \u2208 M 5: Compute\u00e2 i = \u03c0 \u03b8 (a|s i ) \u2200i \u2208 M 6:\nCompute\u0134(\u03c0 \u03b8 , \u03d5\u03bd ) according to Eq. 12 7:\n\nCompute gradient w.r.t. \u03b8 and \u03bd. 8:\n\nUpdate \u03bd \u2190 \u03bd + l\u03d5 \u00d7 l \u00d7 \u2207\u03bd\u0134(\u03c0 \u03b8 , \u03d5\u03bd ) 9:\n\nUpdate \u03b8 \u2190 \u03b8 \u2212 l\u03c0 \u00d7 l \u00d7 \u2207 \u03b8\u0134 (\u03c0 \u03b8 , \u03d5\u03bd ) 10: end while 11: return \u03bd, \u03b8\nJ SL (\u03c0 \u03b8 ) = |M| i=1 |A| j=1 y ij log(\u03c0 \u03b8 (s i ) j )+ (1 \u2212 y ij )log(1 \u2212 \u03c0 \u03b8 (s i ) j ). (11) min\u0134(\u03c0 \u03b8 , \u03d5 \u03bd ) = min \u03c0 \u03b8 (\u03bb\u0134 SL + max \u03d5\u03bd\u0134 OIL ). (12)\nThe \u03bb is the tradeoff factor. The min-max problem in Eq.12 can be optimized by updating \u03b8 and \u03bd alternatively . The pseudo-code of the optimization is described in Algorithm 1.\n\n\nExperiment\n\n\nDatasets\n\nWe utilized four public datasets to investigate the performance of our model and baselines. (1) Inspired (Hayati et al., 2020). It is an English movie recommendation dialogue dataset with sociable communication strategies. (2) MultiWoZ 2.3 (Han et al., 2021). It is a corrected version of MultiWoZ 2.0, the largest English multi-domain dialogue public dataset in the world (Budzianowski et al., 2018).\n\n(3) TGRec . It is a Chinese dialogue dataset with topic threads to enforce natural semantic transitions. (4) DuRec . It is a Chinese dialogue dataset that contains multitype tasks.\n\nAll four datasets were split into train, validation, and test sets when they were released. We selected these datasets because they fully annotated agents' actions at every turn and contained complex social dialogue policy. The semantic meaning of the Inspired and the MultiWoZ 2.3 dataset is significantly more complex than the DuRec and the TGRec dataset because the agents in the DuRec  and the TGRec datasets adopt only one sub-action in a single utterance, while agents in the other two datasets may adopt multi sub-actions. Table 1 described the statistics of the four datasets.\n\n\nBaselines\n\nWe adopted an SL-based model and three recently proposed OIL models as baselines.\n\n(1) SL (PLM + MLP). It utilizes a pretrained language model (PLM) to learn state representations and follows an MLP to make predictions. Although simple, the PLM + MLP framework was adopted in many recent models that obtain state-of-the-art (SOTA) performance in the PL task, e.g., Galaxy, UniConv (Le et al., 2020;.\n\n(2) EDM (Jarrett et al., 2020). It utilizes an energy-based distribution matching model to imitate the demonstrator.\n\n(3) AVRIL (Chan and van der Schaar, 2021). It learns an approximate posterior distribution in a completely offline manner through a variational approach. (4) VDICE (Kostrikov et al., 2020). It transforms the objective function of distribution ratio estimation and yields a completely off-policy imitation learning objective. We did not compare RL-based models because they require a reward function while designing a reward function is a complex task and is beyond the scope of this study.\n\n\nImplementation Details\n\nWe used the Deberta-base (for English dialogues) and MacBert-base (for Chinese dialogues) as the encoder because they obtained impressive performance in many natural language processing tasks (He et al., 2021;Cui et al., 2020). The maximum input sequence length was set to 512 tokens after tokenization. If a state contains more than 512 tokens, only the latest 512 tokens are reserved. We selected four metrics to evaluate model performance in predicting the next actions. They are accuracy, the area under the operating receiver curve (AUC), average precision score (APS), and mean log probability. The accuracy estimates the ratio that a model can predict the turn-specific same action to a human. The optimal Youden's J statistic determined the cut-off point of accuracy. To estimate the stability of models, we independently ran all experiments five times to calculate and report the standard error.\n\nFor optimization, we used the Adam optimizer (Kingma and Ba, 2015). The learning rate was set to 1e-5, and the maximum iteration number was set to 30,000. We conducted training processes with a warmup proportion of 10% and let the learning rate decay linearly after the warmup phase. We selected these settings because the SL model obtained the best performance under these settings. The SD model contains about 200 million parameters, and a single optimization process requires about 5-10 hours in an Nvidia Tesla V100. The data and source code of this study was released at GitHub 2 . Table 2 depicts the action prediction performance. Our SD model performed significantly better than baselines in datasets with more complicated semantic meanings. Specifically, the SD model obtained 0.71 and 0.27 with respect to AUC and APS in the Inspired dataset, while the SL only obtained 0.63 and 0.19, and the three OIL-based baselines only obtained at most 0.67 and 0.23 in the two metrics. The SD model obtained 0.94 and 0.60 concerning AUC and APS in the MultiWoZ 2.3 dataset, while the SL only obtained 0.91 and 0.55, and the three OIL-based baselines only obtained at most 0.91 and 0.55 in the two metrics. The SD model also obtained significantly better performance in accuracy and log probability (except the accuracy of the Inspired dataset). The SD model obtained modestly better performance in the other two datasets, in which semantic meaning is relatively simple. These results showed that introducing state transition information may improve action prediction performance.\n\n\nResults\n\n\nAction Prediction Performance\n\nMeanwhile, we found that the EDM and the AVIRL only obtained similar performance compared to the SL model, while the VDICE completely failed to converge. Of note, the EDM and the AVRIL used SL information explicitly to train the agent, while the VDICE only utilized SL information implicitly (Jarrett et al., 2020;Chan and van der Schaar, 2021;. The SD obtained better performance than other OIL baselines,   so we only compared SD and SL models in the following content.  \n\n\nHalf Dialogue Action Prediction\n\nTo investigate whether the SD model alleviates the covariate shift problem, we reported the action prediction accuracy of the SD model and the SL model in the first half and the second half of dialogues (Table 3). Of note, the statistical difference increases with the development of dialogues. If the SD model indeed alleviates the covariate shift prob-lem, its performance will degenerate slower than the SL model (i.e., \u2206 is smaller). We found that the SD model obtained better performance partly because its performance degenerates slower with the dialogue turns increasing. Performances of both models degenerate in Multi-WoZ 2.3, TGRec, and DuRec datasets, while the accuracy difference of our SD model is smaller than the SL model. Specifically, the \u2206 of the SL model is 0.04, 0.06, and 0.03 in the three datasets, while the \u2206 of the SD model is 0.01, 0.02, and 0.00. Performances of both models improve in the inspired dataset. The accuracy difference of our SD model (\u2206 is -0.02) is larger than the SL model (\u2206 is -0.01). This finding accords with the assumption that utilizing state transition information can decrease the mistake growth speed (Ross and Bagnell, 2010).\n\n\nRegularization Analysis\n\nWhether OIL models can outperform SL models by utilizing state transition information is a controversial topic . Some recent studies claimed that SL models obtained worse performance because they met overfitting problems   . during training . We investigated models' performances under different l2 regularization settings (Fig. 2). For the space limit, we only describe the performance of -log probability (lower the better) in this and the following experiments.\n\nExperimental results indicate that the SL model met the overfitting problem to some extent. Adding l2 regularization to MLP layers with a small weight (e.g., 0.0001) improves model performance. The l2regularized SL model obtained the best (average) -log probability as 4. 14,5.24,0.44,and 0.67 in inspired,MultiWoZ 2.3,TGRec,and DuRec datasets,respectively. The original SL model only obtained 4.30,5.33,0.49,and 0.78 in these metrics. However, we surprisingly found that the performance of the SD model also improves by adding l2 regularization appropriately. The l2-regularized SD model obtained significantly better performance than the l2-regularized SL model in Inspired, MultiWoZ 2.3, and TGRec datasets. It also obtained slightly better in the DuRec dataset.\n\nExperimental results of the regularization analysis indicate that our SD model will likely outperform the SL model by utilizing the state transition information rather than avoiding overfitting.\n\n\nAblation Study\n\nWe conducted an ablation study to evaluate the effectiveness of the model design (Table 4). The basic version of the SD did not use the supervised regularization term, used the action sampling method, used short context (128 tokens), only optimized parameters of MLPs, and used a shared encoder in \u03c0 \u03b8 and \u03d5 \u03bd . Then, we evaluated model performance by adding the SL loss, canceling the action sampling process and directly using action distribution, using full context (512 tokens), optimizing all parameters in the model, using separate encoders in \u03c0 \u03b8 and \u03d5 \u03bd , and using a larger PLM step by step.\n\nResults show that the SD model cannot be optimized when we do not explicitly utilize supervised regularization. We observed apparent performance improvement when we replaced the action samples with action distributions. Experimental results also indicate that introducing longer context (512 tokens), optimizing parameters of both encoders and MLPs, and utilizing separate encoders to learn text representations benefit performances of the SD model, indicating our design is effective. Meanwhile, we did not observe performance gain by utilizing larger PLMs such as the Deberta-large.\n\n\nLow Resource Analysis\n\nA PL model needs to obtain acceptable performance with a low resource, as collecting human action annotation is expensive. Therefore, we investigated SD and SL when they were trained by a different proportion of data (Fig. 3).\n\nExperimental results indicate that our SD model almost always outperforms the SL model when the experimental setting is the same. Specifically, the SD model obtained significantly better perfor- mance than the SL model in 18 comparisons and obtained worse performance in only two comparisons. Experimental results also indicate that the SD model can utilize data more effectively. Specifically, the SD model obtained similar performance compared to the SL model by only using about 40% data in the Inspired, 20% data in the MultiWoZ 2.3, 40% data in the TGRec, and 60% in the DuRec dataset, respectively.\n\n\nHyperparameter Analysis\n\nTraining inverse RL agents is difficult because their performance heavily relies on the choice of hyperparameters. As SD is a variant of inverse RL model, investigating the hyperparameter sensitivity is important to evaluate its usability. We investigated the performance of the SD model under different learning rate ratios l \u03c0 /l \u03d5 and SL coefficient \u03bb. The l \u03c0 /l \u03d5 \u2208 {100, 10, 1, 0.1} (larger learning rate was set to 1e-5) and \u03bb \u2208 {10, 1, 0.1, 0.01}. We used the grid search method to choose 16 hyperparameter combinations to train the SD model for each dataset (Fig. 4). The performance of the SD model is stable when the value of l \u03c0 /l \u03d5 is between 1 to 100, and the value of \u03bb is between 0.01 to 1 in all four datasets. Model performance degenerates significantly when l \u03c0 /l \u03d5 is less than one or \u03bb is larger than 1. These experimental results indicate that our SD model is relatively in-sensitive to the choice of hyperparameters and has great usability. It converges in a large space.\n\n\nConvergence Analysis\n\nWe also investigated the convergence speed of our SD model because inverse RL agents typically require a long time to converge. Fig. 5 described the convergence speed of SL and SD models, respectively. Experimental results indicated that our SD model generally converged with 4000 to 15000 iterations. The -log probability of SD converged slightly faster than the SL model in the inspired, DuRec, and TGRec datasets and basically the same in the MultiWoZ 2.3 dataset.\n\n\nConclusion\n\nWe introduced an SD model consisting of an OIL framework with a supervised regularization component. The SD model can learn complex dialogue policy from human demonstrations, utilize state transition information, and does not require online environment interactions. Results demonstrated that the SD model significantly outperforms SLbased models when dialogue policy is complex and alleviate the covariate shift problem. Our SD model also obtained a similar convergence speed and is relatively insensitive to the choice of hyperparameters. Therefore, we argue that our SD model has great potential to be applied to PL tasks.\n\nWe did not presume a user has an explicit goal, while previous PL studies typically presume a user has an explicit goal (Lubis et al., 2020;Sun et al., 2022). Therefore, we adopted different metrics to evaluate model performance compared to previous studies. Although we argue that our assumption is appropriate as users do not have a goal in many scenarios, we cannot compare performance between our SD model and many previous PL models.\n\nWe empirically demonstrated that the OIL model could obtain better performance. This finding is different from the previous study , which claimed the SL model could learn nearoptimal policy in tabular deterministic decisionmaking tasks. It is important to conduct theoretical analysis to figure out why the OIL model can learn better policy when inputs are raw text and policy is stochastic. However, it is beyond the scope of this study.\n\nAlthough experimental results demonstrated that our SD model could replicate human policy better than SL and OIL baselines, its performance is still unsatisfying when dialogue policy is extremely complex.\n\n\nBuyer: I'm looking for recommendations on a great holiday movie, do you have any recommendations? Seller: For sure! First, do you have certain preferences or aversions? Like are you into musicals? Buyer : I do not really like musicals.\n\nFigure 2 :\n2L2-regularized SL Analysis. The SL-M, SL-F, SD-M, and SD-F indicate that we applied l2 regularization to MLP parameters or all parameters for the SL model or the SD model, respectively. 0, 0.0001, 0.001, and 0.01 indicate the weight of the l2 regularization term.\n\nFigure 3 :\n3Low Resource Analysis. Numbers in the x-axis represent the used fraction of training data.\n\nFigure 5 :\n5Convergence Analysis. Numbers in the x-axis represent training steps.\n\n\nIns. M.W.Du. \nTG. \n# Dialogues \n1K \n8K \n10K \n10K \n# Turns \n36K 114K 156K 129K \n# Sub-actions \n15 \n45 \n24 \n4 \nMulti sub-actions True \nTrue \nFalse False \n\n\n\nTable 1 :\n1Data Statistics. Ins., M.W., Du., and TG.mean \n\n\nTable 2 :\n2Next Action Prediction Performance \n\n\n\nTable 3 :\n3Half Dialogue Action Prediction Accuracy. \u2206 \nindicates the accuracy differences between the first half \nand the second half. \n\n\n\nTable 4 :\n4Ablation Study. Numbers in the table are the -log probability in predicting the next actions.\nSource codes are in supplementary in the review phase.\n\nApprenticeship learning via inverse reinforcement learning. Pieter Abbeel, Andrew Y Ng, 10.1145/1015330.1015430Proceedings of the Twenty-First International Conference on Machine Learning, ICML '04. the Twenty-First International Conference on Machine Learning, ICML '04New York, NY, USAAssociation for Computing MachineryPieter Abbeel and Andrew Y. Ng. 2004. Apprentice- ship learning via inverse reinforcement learning. In Proceedings of the Twenty-First International Con- ference on Machine Learning, ICML '04, page 1, New York, NY, USA. Association for Computing Machinery.\n\nDisagreement-regularized imitation learning. Kiante Brantley, Wen Sun, Mikael Henaff, International Conference on Learning Representations. Kiante Brantley, Wen Sun, and Mikael Henaff. 2019. Disagreement-regularized imitation learning. In International Conference on Learning Representa- tions.\n\nMultiwoz -a largescale multi-domain wizard-of-oz dataset for taskoriented dialogue modelling. Pawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I\u00f1igo Casanueva, Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing. the 2018 Conference on Empirical Methods in Natural Language ProcessingEMNLPUltes Stefan, Ramadan Osman, and Milica Ga\u0161i\u0107Pawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I\u00f1igo Casanueva, Ultes Stefan, Ramadan Os- man, and Milica Ga\u0161i\u0107. 2018. Multiwoz -a large- scale multi-domain wizard-of-oz dataset for task- oriented dialogue modelling. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP).\n\nScalable bayesian inverse reinforcement learning. Alex James , Chan , Mihaela Van Der Schaar, International Conference on Learning Representations. Alex James Chan and Mihaela van der Schaar. 2021. Scalable bayesian inverse reinforcement learning. In International Conference on Learning Represen- tations.\n\nRevisiting pretrained models for Chinese natural language processing. Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, Guoping Hu, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings. the 2020 Conference on Empirical Methods in Natural Language Processing: FindingsOnline. Association for Computational LinguisticsYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shi- jin Wang, and Guoping Hu. 2020. Revisiting pre- trained models for Chinese natural language process- ing. In Proceedings of the 2020 Conference on Em- pirical Methods in Natural Language Processing: Findings, pages 657-668, Online. Association for Computational Linguistics.\n\nUnified conversational recommendation policy learning via graph-based reinforcement learning. Yang Deng, Yaliang Li, Fei Sun, Bolin Ding, Wai Lam, 10.1145/3404835.3462913Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '21. the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '21New York, NY, USAAssociation for Computing MachineryYang Deng, Yaliang Li, Fei Sun, Bolin Ding, and Wai Lam. 2021. Unified conversational recommendation policy learning via graph-based reinforcement learn- ing. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '21, page 1431-1441, New York, NY, USA. Association for Computing Machinery.\n\nAsymptotic evaluation of certain markov process expectations for large time, i. M D Donsker, S R S Varadhan, 10.1002/cpa.3160280102Communications on Pure and Applied Mathematics. 281M. D. Donsker and S. R. S. Varadhan. 1975. Asymp- totic evaluation of certain markov process expecta- tions for large time, i. Communications on Pure and Applied Mathematics, 28(1):1-47.\n\nMultiwoz 2.3: A multidomain task-oriented dialogue dataset enhanced with annotation corrections and co-reference annotation. Ting Han, Ximing Liu, Ryuichi Takanabu, Yixin Lian, Chongxuan Huang, Dazhen Wan, Wei Peng, Minlie Huang, Natural Language Processing and Chinese Computing. ChamSpringer International PublishingTing Han, Ximing Liu, Ryuichi Takanabu, Yixin Lian, Chongxuan Huang, Dazhen Wan, Wei Peng, and Minlie Huang. 2021. Multiwoz 2.3: A multi- domain task-oriented dialogue dataset enhanced with annotation corrections and co-reference annota- tion. In Natural Language Processing and Chinese Computing, pages 206-218, Cham. Springer Interna- tional Publishing.\n\nIN-SPIRED: Toward sociable recommendation dialog systems. Dongyeop Shirley Anugrah Hayati, Qingxiaoyang Kang, Weiyan Zhu, Zhou Shi, Yu, 10.18653/v1/2020.emnlp-main.654Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Online. Association for Computational LinguisticsShirley Anugrah Hayati, Dongyeop Kang, Qingxi- aoyang Zhu, Weiyan Shi, and Zhou Yu. 2020. IN- SPIRED: Toward sociable recommendation dialog systems. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process- ing (EMNLP), pages 8142-8152, Online. Associa- tion for Computational Linguistics.\n\nDeberta: decoding-enhanced bert with distangled attention. Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chen, International Conference on Learning Representations. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. Deberta: decoding-enhanced bert with distangled attention. In International Con- ference on Learning Representations.\n\nGalaxy: A generative pre-trained model for task-oriented dialog with semi-supervised learning and explicit policy injection. Wanwei He, Yinpei Dai, Yinhe Zheng, Yuchuan Wu, Zheng Cao, Dermot Liu, Peng Jiang, Min Yang, Fei Huang, Luo Si, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial IntelligenceWanwei He, Yinpei Dai, Yinhe Zheng, Yuchuan Wu, Zheng Cao, Dermot Liu, Peng Jiang, Min Yang, Fei Huang, Luo Si, et al. 2022. Galaxy: A genera- tive pre-trained model for task-oriented dialog with semi-supervised learning and explicit policy injec- tion. Proceedings of the AAAI Conference on Arti- ficial Intelligence.\n\nHybrid reinforcement/supervised learning of dialogue policies from fixed data sets. James Henderson, Oliver Lemon, Kallirroi Georgila, 10.1162/coli.2008.07-028-R2-05-82Computational Linguistics. 344James Henderson, Oliver Lemon, and Kallirroi Georgila. 2008. Hybrid reinforcement/supervised learning of dialogue policies from fixed data sets. Computational Linguistics, 34(4):487-511.\n\nGenerative adversarial imitation learning. Jonathan Ho, Stefano Ermon, Advances in Neural Information Processing Systems. Curran Associates, Inc29Jonathan Ho and Stefano Ermon. 2016. Generative ad- versarial imitation learning. In Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc.\n\nModel-free imitation learning with policy optimization. Jonathan Ho, Jayesh K Gupta, Stefano Ermon, Proceedings of the 33rd International Conference on International Conference on Machine Learning. the 33rd International Conference on International Conference on Machine LearningJMLR.org48Jonathan Ho, Jayesh K. Gupta, and Stefano Ermon. 2016. Model-free imitation learning with policy optimization. In Proceedings of the 33rd Interna- tional Conference on International Conference on Machine Learning -Volume 48, ICML'16, page 2760-2769. JMLR.org.\n\nCategorical reparameterization with gumbel-softmax. Eric Jang, Shixiang Gu, Ben Poole, International Conference on Learning Representations. Eric Jang, Shixiang Gu, and Ben Poole. 2017. Cate- gorical reparameterization with gumbel-softmax. In International Conference on Learning Representa- tions.\n\nStrictly batch imitation learning by energybased distribution matching. Daniel Jarrett, Ioana Bica, Mihaela Van Der Schaar, Advances in Neural Information Processing Systems. Curran Associates, Inc33Daniel Jarrett, Ioana Bica, and Mihaela van der Schaar. 2020. Strictly batch imitation learning by energy- based distribution matching. In Advances in Neural Information Processing Systems, volume 33, pages 7354-7365. Curran Associates, Inc.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, 3rd International Conference on Learning Representations. San Diego, CA, USAConference Track ProceedingsDiederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd Inter- national Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.\n\nImitation learning via off-policy distribution matching. Ilya Kostrikov, Ofir Nachum, Jonathan Tompson, International Conference on Learning Representations. Ilya Kostrikov, Ofir Nachum, and Jonathan Tompson. 2020. Imitation learning via off-policy distribution matching. In International Conference on Learning Representations.\n\nUniConv: A unified conversational neural architecture for multi-domain task-oriented dialogues. Hung Le, Doyen Sahoo, Chenghao Liu, Nancy Chen, Steven C H Hoi, 10.18653/v1/2020.emnlp-main.146Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)Association for Computational LinguisticsOnlineHung Le, Doyen Sahoo, Chenghao Liu, Nancy Chen, and Steven C.H. Hoi. 2020. UniConv: A unified conversational neural architecture for multi-domain task-oriented dialogues. In Proceedings of the 2020 Conference on Empirical Methods in Natural Lan- guage Processing (EMNLP), pages 1860-1877, On- line. Association for Computational Linguistics.\n\nTruly batch apprenticeship learning with deep successor features. Donghun Lee, Srivatsan Srinivasan, Finale Doshi-Velez, 10.24963/ijcai.2019/819Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19. the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19International Joint Conferences on Artificial Intelligence OrganizationDonghun Lee, Srivatsan Srinivasan, and Finale Doshi- Velez. 2019. Truly batch apprenticeship learning with deep successor features. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, pages 5909-5915. International Joint Conferences on Artificial Intelli- gence Organization.\n\nEstimation-action-reflection: Towards deep interaction between conversational and recommender systems. Wenqiang Lei, Xiangnan He, Yisong Miao, Qingyun Wu, Richang Hong, Min-Yen Kan, Tat-Seng Chua, 10.1145/3336191.3371769Proceedings of the 13th International Conference on Web Search and Data Mining, WSDM '20. the 13th International Conference on Web Search and Data Mining, WSDM '20New York, NY, USAAssociation for Computing MachineryWenqiang Lei, Xiangnan He, Yisong Miao, Qingyun Wu, Richang Hong, Min-Yen Kan, and Tat-Seng Chua. 2020. Estimation-action-reflection: Towards deep interaction between conversational and recom- mender systems. In Proceedings of the 13th Interna- tional Conference on Web Search and Data Mining, WSDM '20, page 304-312, New York, NY, USA. Association for Computing Machinery.\n\nA user simulator for task-completion dialogues. Xiujun Li, C Zachary, Bhuwan Lipton, Lihong Dhingra, Jianfeng Li, Yun-Nung Gao, Chen, arXiv:1612.05688arXiv preprintXiujun Li, Zachary C Lipton, Bhuwan Dhingra, Lihong Li, Jianfeng Gao, and Yun-Nung Chen. 2016. A user simulator for task-completion dialogues. arXiv preprint arXiv:1612.05688.\n\nRethinking valuedice: Does it really improve performance?. Ziniu Li, Tian Xu, Zhiquan Luo, Blog Track at ICLR 2022. Ziniu Li, Tian Xu, and Zhiquan Luo. 2022. Rethinking valuedice: Does it really improve performance? In Blog Track at ICLR 2022.\n\nTowards conversational recommendation over multi-type dialogs. Zeming Liu, Haifeng Wang, Zheng-Yu Niu, Hua Wu, Wanxiang Che, Ting Liu, 10.18653/v1/2020.acl-main.98Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational LinguisticsZeming Liu, Haifeng Wang, Zheng-Yu Niu, Hua Wu, Wanxiang Che, and Ting Liu. 2020. Towards con- versational recommendation over multi-type dialogs. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1036-1049, Online. Association for Computational Linguistics.\n\nLAVA: Latent action spaces via variational auto-encoding for dialogue policy optimization. Nurul Lubis, Christian Geishauser, Michael Heck, Hsien-Chin Lin, Marco Moresi, 10.18653/v1/2020.coling-main.41Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsBarcelona, Spain (OnlineCarel van Niekerk, and Milica Gasic. International Committee on Computational LinguisticsNurul Lubis, Christian Geishauser, Michael Heck, Hsien-chin Lin, Marco Moresi, Carel van Niekerk, and Milica Gasic. 2020. LAVA: Latent action spaces via variational auto-encoding for dialogue policy optimization. In Proceedings of the 28th Inter- national Conference on Computational Linguistics, pages 465-479, Barcelona, Spain (Online). Interna- tional Committee on Computational Linguistics.\n\nReconstruct & crush network. Erinc Merdivan, Mohammad Reza Loghmani, Matthieu Geist, Advances in Neural Information Processing Systems. 30Erinc Merdivan, Mohammad Reza Loghmani, and Matthieu Geist. 2017. Reconstruct & crush network. Advances in Neural Information Processing Systems, 30.\n\nDualdice: Behavior-agnostic estimation of discounted stationary distribution corrections. Ofir Nachum, Yinlam Chow, Bo Dai, Lihong Li, Advances in Neural Information Processing Systems. Curran Associates, Inc32Ofir Nachum, Yinlam Chow, Bo Dai, and Lihong Li. 2019. Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections. In Ad- vances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.\n\nFuzhao Xue, and Erik Cambria. 2022. Recent advances in deep learning based dialogue systems: a systematic survey. Jinjie Ni, Tom Young, Vlad Pandelea, 10.1007/s10462-022-10248-8Artificial Intelligence Review. Jinjie Ni, Tom Young, Vlad Pandelea, Fuzhao Xue, and Erik Cambria. 2022. Recent advances in deep learn- ing based dialogue systems: a systematic survey. Ar- tificial Intelligence Review.\n\nChatgpt: Optimizing language models for dialogue. Openai, OpenAI. 2022. Chatgpt: Optimizing language models for dialogue.\n\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, L Carroll, Pamela Wainwright, Chong Mishkin, Sandhini Zhang, Katarina Agarwal, Alex Slama, Ray, arXiv:2203.02155Training language models to follow instructions with human feedback. arXiv preprintLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car- roll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow in- structions with human feedback. arXiv preprint arXiv:2203.02155.\n\nLearning to ask appropriate questions in conversational recommendation. Hongzhi Xuhui Ren, Tong Yin, Hao Chen, Zi Wang, Kai Huang, Zheng, 10.1145/3404835.3462839Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '21. the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '21New York, NY, USAAssociation for Computing MachineryXuhui Ren, Hongzhi Yin, Tong Chen, Hao Wang, Zi Huang, and Kai Zheng. 2021. Learning to ask ap- propriate questions in conversational recommenda- tion. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR '21, page 808-817, New York, NY, USA. Association for Computing Machinery.\n\nEfficient reductions for imitation learning. Stephane Ross, Drew Bagnell, PMLRProceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. the Thirteenth International Conference on Artificial Intelligence and StatisticsSardinia, Italy9Chia Laguna ResortStephane Ross and Drew Bagnell. 2010. Efficient re- ductions for imitation learning. In Proceedings of the Thirteenth International Conference on Artifi- cial Intelligence and Statistics, volume 9 of Proceed- ings of Machine Learning Research, pages 661-668, Chia Laguna Resort, Sardinia, Italy. PMLR.\n\nA reduction of imitation learning and structured prediction to no-regret online learning. Stephane Ross, Geoffrey Gordon, Drew Bagnell, PMLRProceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics. the Fourteenth International Conference on Artificial Intelligence and StatisticsFort Lauderdale, FL, USA15Stephane Ross, Geoffrey Gordon, and Drew Bagnell. 2011. A reduction of imitation learning and struc- tured prediction to no-regret online learning. In Pro- ceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics, volume 15 of Proceedings of Machine Learning Research, pages 627-635, Fort Lauderdale, FL, USA. PMLR.\n\nMars: Semantic-aware contrastive learning for end-to-end task-oriented dialog. Haipeng Sun, Junwei Bao, Youzheng Wu, Xiaodong He, arXiv:2210.08917arXiv preprintHaipeng Sun, Junwei Bao, Youzheng Wu, and Xi- aodong He. 2022. Mars: Semantic-aware con- trastive learning for end-to-end task-oriented dialog. arXiv preprint arXiv:2210.08917.\n\nReinforcement learning: An introduction. S Richard, Andrew G Sutton, Barto, MIT pressRichard S Sutton and Andrew G Barto. 2018. Rein- forcement learning: An introduction. MIT press.\n\nApprenticeship learning using linear programming. Umar Syed, Michael Bowling, Robert E Schapire, 10.1145/1390156.1390286Proceedings of the 25th International Conference on Machine Learning, ICML '08. the 25th International Conference on Machine Learning, ICML '08New York, NY, USAAssociation for Computing MachineryUmar Syed, Michael Bowling, and Robert E. Schapire. 2008. Apprenticeship learning using linear program- ming. In Proceedings of the 25th International Conference on Machine Learning, ICML '08, page 1032-1039, New York, NY, USA. Association for Computing Machinery.\n\nModelling hierarchical structure between dialogue policy and natural language generator with option framework for task-oriented dialogue system. Jianhong Wang, Yuan Zhang, Tae-Kyun Kim, Yunjie Gu, 9th International Conference on Learning Representations, ICLR 2021, Virtual Event. OpenReview.netJianhong Wang, Yuan Zhang, Tae-Kyun Kim, and Yun- jie Gu. 2021. Modelling hierarchical structure be- tween dialogue policy and natural language genera- tor with option framework for task-oriented dialogue system. In 9th International Conference on Learn- ing Representations, ICLR 2021, Virtual Event, Aus- tria, May 3-7, 2021. OpenReview.net.\n\nMulti-domain dialogue acts and response co-generation. Kai Wang, Junfeng Tian, Rui Wang, Xiaojun Quan, Jianxing Yu, 10.18653/v1/2020.acl-main.638Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. the 58th Annual Meeting of the Association for Computational LinguisticsOnline. Association for Computational LinguisticsKai Wang, Junfeng Tian, Rui Wang, Xiaojun Quan, and Jianxing Yu. 2020. Multi-domain dialogue acts and response co-generation. In Proceedings of the 58th Annual Meeting of the Association for Compu- tational Linguistics, pages 7125-7134, Online. As- sociation for Computational Linguistics.\n\nError bounds of imitating policies and environments for reinforcement learning. Tian Xu, Ziniu Li, Yang Yu, 10.1109/TPAMI.2021.3096966IEEE Transactions on Pattern Analysis and Machine Intelligence. 4410Tian Xu, Ziniu Li, and Yang Yu. 2022. Error bounds of imitating policies and environments for reinforce- ment learning. IEEE Transactions on Pattern Analy- sis and Machine Intelligence, 44(10):6968-6980.\n\nTowards topicguided conversational recommender system. Kun Zhou, Yuanhang Zhou, Wayne Xin Zhao, Xiaoke Wang, Ji-Rong Wen, 10.18653/v1/2020.coling-main.365Proceedings of the 28th International Conference on Computational Linguistics. the 28th International Conference on Computational LinguisticsBarcelona, SpainInternational Committee on Computational LinguisticsKun Zhou, Yuanhang Zhou, Wayne Xin Zhao, Xiaoke Wang, and Ji-Rong Wen. 2020. Towards topic- guided conversational recommender system. In Proceedings of the 28th International Conference on Computational Linguistics, pages 4128-4139, Barcelona, Spain (Online). International Committee on Computational Linguistics.\n", "annotations": {"author": "[{\"end\":139,\"start\":111},{\"end\":192,\"start\":140},{\"end\":257,\"start\":193},{\"end\":324,\"start\":258}]", "publisher": null, "author_last_name": "[{\"end\":123,\"start\":120},{\"end\":153,\"start\":149},{\"end\":208,\"start\":203},{\"end\":266,\"start\":262}]", "author_first_name": "[{\"end\":119,\"start\":111},{\"end\":148,\"start\":140},{\"end\":202,\"start\":193},{\"end\":261,\"start\":258}]", "author_affiliation": "[{\"end\":138,\"start\":125},{\"end\":191,\"start\":178},{\"end\":256,\"start\":236},{\"end\":301,\"start\":288},{\"end\":323,\"start\":303}]", "title": "[{\"end\":108,\"start\":1},{\"end\":432,\"start\":325}]", "venue": null, "abstract": "[{\"end\":1797,\"start\":434}]", "bib_ref": "[{\"end\":2031,\"start\":2014},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2316,\"start\":2298},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2334,\"start\":2316},{\"end\":2744,\"start\":2728},{\"end\":2880,\"start\":2867},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3043,\"start\":3022},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3056,\"start\":3043},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":3172,\"start\":3160},{\"end\":3682,\"start\":3653},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4365,\"start\":4342},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4387,\"start\":4365},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4881,\"start\":4857},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5313,\"start\":5295},{\"end\":6258,\"start\":6241},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6281,\"start\":6258},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6440,\"start\":6421},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6554,\"start\":6537},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6997,\"start\":6980},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7030,\"start\":7012},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7085,\"start\":7068},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7187,\"start\":7167},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7632,\"start\":7611},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7810,\"start\":7789},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8363,\"start\":8346},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8490,\"start\":8470},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8518,\"start\":8495},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8719,\"start\":8698},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":10726,\"start\":10705},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11145,\"start\":11127},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11569,\"start\":11548},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11592,\"start\":11569},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11619,\"start\":11592},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11859,\"start\":11839},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12552,\"start\":12531},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":12575,\"start\":12552},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13471,\"start\":13452},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14192,\"start\":14179},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":14432,\"start\":14411},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":15460,\"start\":15439},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15592,\"start\":15574},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":15734,\"start\":15707},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":16915,\"start\":16898},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16948,\"start\":16926},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":17224,\"start\":17200},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":17761,\"start\":17744},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":17778,\"start\":17761},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20394,\"start\":20372},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":20424,\"start\":20394},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":21767,\"start\":21743},{\"end\":22537,\"start\":22534},{\"end\":22542,\"start\":22537},{\"end\":22547,\"start\":22542},{\"end\":22568,\"start\":22547},{\"end\":22581,\"start\":22568},{\"end\":22587,\"start\":22581},{\"end\":22606,\"start\":22587},{\"end\":22661,\"start\":22606},{\"end\":22666,\"start\":22661},{\"end\":22671,\"start\":22666},{\"end\":22679,\"start\":22671},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":27584,\"start\":27564},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":27601,\"start\":27584}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":28766,\"start\":28529},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29043,\"start\":28767},{\"attributes\":{\"id\":\"fig_2\"},\"end\":29147,\"start\":29044},{\"attributes\":{\"id\":\"fig_3\"},\"end\":29230,\"start\":29148},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":29386,\"start\":29231},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":29446,\"start\":29387},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":29496,\"start\":29447},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":29636,\"start\":29497},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":29742,\"start\":29637}]", "paragraph": "[{\"end\":2502,\"start\":1813},{\"end\":2591,\"start\":2504},{\"end\":2762,\"start\":2593},{\"end\":2789,\"start\":2764},{\"end\":2807,\"start\":2791},{\"end\":2825,\"start\":2809},{\"end\":2848,\"start\":2827},{\"end\":2990,\"start\":2850},{\"end\":3971,\"start\":2992},{\"end\":4882,\"start\":3973},{\"end\":5715,\"start\":4884},{\"end\":6129,\"start\":5717},{\"end\":7399,\"start\":6131},{\"end\":8959,\"start\":7422},{\"end\":9959,\"start\":8988},{\"end\":10309,\"start\":9961},{\"end\":10464,\"start\":10350},{\"end\":10727,\"start\":10497},{\"end\":10889,\"start\":10886},{\"end\":11620,\"start\":10891},{\"end\":11938,\"start\":11711},{\"end\":12099,\"start\":11973},{\"end\":12261,\"start\":12223},{\"end\":12711,\"start\":12391},{\"end\":13145,\"start\":12809},{\"end\":13865,\"start\":13219},{\"end\":14488,\"start\":13895},{\"end\":14656,\"start\":14524},{\"end\":14828,\"start\":14786},{\"end\":14865,\"start\":14830},{\"end\":14908,\"start\":14867},{\"end\":14980,\"start\":14910},{\"end\":15308,\"start\":15132},{\"end\":15735,\"start\":15334},{\"end\":15917,\"start\":15737},{\"end\":16503,\"start\":15919},{\"end\":16598,\"start\":16517},{\"end\":16916,\"start\":16600},{\"end\":17034,\"start\":16918},{\"end\":17525,\"start\":17036},{\"end\":18456,\"start\":17552},{\"end\":20036,\"start\":18458},{\"end\":20553,\"start\":20080},{\"end\":21768,\"start\":20589},{\"end\":22260,\"start\":21796},{\"end\":23027,\"start\":22262},{\"end\":23223,\"start\":23029},{\"end\":23842,\"start\":23242},{\"end\":24428,\"start\":23844},{\"end\":24680,\"start\":24454},{\"end\":25286,\"start\":24682},{\"end\":26310,\"start\":25314},{\"end\":26802,\"start\":26335},{\"end\":27442,\"start\":26817},{\"end\":27882,\"start\":27444},{\"end\":28322,\"start\":27884},{\"end\":28528,\"start\":28324}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10349,\"start\":10310},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10820,\"start\":10728},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10885,\"start\":10820},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11710,\"start\":11621},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11972,\"start\":11939},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12222,\"start\":12100},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12390,\"start\":12262},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12808,\"start\":12712},{\"attributes\":{\"id\":\"formula_8\"},\"end\":13218,\"start\":13146},{\"attributes\":{\"id\":\"formula_9\"},\"end\":14785,\"start\":14657},{\"attributes\":{\"id\":\"formula_10\"},\"end\":15131,\"start\":14981}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":16456,\"start\":16449},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":19052,\"start\":19045},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":20801,\"start\":20792},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":23331,\"start\":23323}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1811,\"start\":1799},{\"attributes\":{\"n\":\"2.2\"},\"end\":7420,\"start\":7402},{\"attributes\":{\"n\":\"3\"},\"end\":8972,\"start\":8962},{\"attributes\":{\"n\":\"3.1\"},\"end\":8986,\"start\":8975},{\"attributes\":{\"n\":\"3.2\"},\"end\":10495,\"start\":10467},{\"attributes\":{\"n\":\"3.3\"},\"end\":13893,\"start\":13868},{\"end\":14522,\"start\":14491},{\"attributes\":{\"n\":\"4\"},\"end\":15321,\"start\":15311},{\"attributes\":{\"n\":\"4.1\"},\"end\":15332,\"start\":15324},{\"attributes\":{\"n\":\"4.2\"},\"end\":16515,\"start\":16506},{\"attributes\":{\"n\":\"4.3\"},\"end\":17550,\"start\":17528},{\"attributes\":{\"n\":\"4.4\"},\"end\":20046,\"start\":20039},{\"attributes\":{\"n\":\"4.4.1\"},\"end\":20078,\"start\":20049},{\"attributes\":{\"n\":\"4.4.2\"},\"end\":20587,\"start\":20556},{\"attributes\":{\"n\":\"4.4.3\"},\"end\":21794,\"start\":21771},{\"attributes\":{\"n\":\"4.4.4\"},\"end\":23240,\"start\":23226},{\"attributes\":{\"n\":\"4.4.5\"},\"end\":24452,\"start\":24431},{\"attributes\":{\"n\":\"4.4.6\"},\"end\":25312,\"start\":25289},{\"attributes\":{\"n\":\"4.4.7\"},\"end\":26333,\"start\":26313},{\"attributes\":{\"n\":\"5\"},\"end\":26815,\"start\":26805},{\"end\":28778,\"start\":28768},{\"end\":29055,\"start\":29045},{\"end\":29159,\"start\":29149},{\"end\":29397,\"start\":29388},{\"end\":29457,\"start\":29448},{\"end\":29507,\"start\":29498},{\"end\":29647,\"start\":29638}]", "table": "[{\"end\":29386,\"start\":29242},{\"end\":29446,\"start\":29440},{\"end\":29496,\"start\":29459},{\"end\":29636,\"start\":29509}]", "figure_caption": "[{\"end\":28766,\"start\":28531},{\"end\":29043,\"start\":28780},{\"end\":29147,\"start\":29057},{\"end\":29230,\"start\":29161},{\"end\":29242,\"start\":29233},{\"end\":29440,\"start\":29399},{\"end\":29742,\"start\":29649}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":22127,\"start\":22119},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24679,\"start\":24671},{\"end\":25889,\"start\":25881},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26469,\"start\":26463}]", "bib_author_first_name": "[{\"end\":29865,\"start\":29859},{\"end\":29880,\"start\":29874},{\"end\":29882,\"start\":29881},{\"end\":30430,\"start\":30424},{\"end\":30444,\"start\":30441},{\"end\":30456,\"start\":30450},{\"end\":30774,\"start\":30769},{\"end\":30800,\"start\":30789},{\"end\":30815,\"start\":30806},{\"end\":30828,\"start\":30823},{\"end\":31427,\"start\":31423},{\"end\":31433,\"start\":31428},{\"end\":31440,\"start\":31436},{\"end\":31450,\"start\":31443},{\"end\":31757,\"start\":31751},{\"end\":31771,\"start\":31763},{\"end\":31781,\"start\":31777},{\"end\":31791,\"start\":31787},{\"end\":31803,\"start\":31797},{\"end\":31817,\"start\":31810},{\"end\":32475,\"start\":32471},{\"end\":32489,\"start\":32482},{\"end\":32497,\"start\":32494},{\"end\":32508,\"start\":32503},{\"end\":32518,\"start\":32515},{\"end\":33270,\"start\":33269},{\"end\":33272,\"start\":33271},{\"end\":33283,\"start\":33282},{\"end\":33287,\"start\":33284},{\"end\":33688,\"start\":33684},{\"end\":33700,\"start\":33694},{\"end\":33713,\"start\":33706},{\"end\":33729,\"start\":33724},{\"end\":33745,\"start\":33736},{\"end\":33759,\"start\":33753},{\"end\":33768,\"start\":33765},{\"end\":33781,\"start\":33775},{\"end\":34300,\"start\":34292},{\"end\":34337,\"start\":34325},{\"end\":34350,\"start\":34344},{\"end\":34360,\"start\":34356},{\"end\":35014,\"start\":35005},{\"end\":35027,\"start\":35019},{\"end\":35041,\"start\":35033},{\"end\":35053,\"start\":35047},{\"end\":35429,\"start\":35423},{\"end\":35440,\"start\":35434},{\"end\":35451,\"start\":35446},{\"end\":35466,\"start\":35459},{\"end\":35476,\"start\":35471},{\"end\":35488,\"start\":35482},{\"end\":35498,\"start\":35494},{\"end\":35509,\"start\":35506},{\"end\":35519,\"start\":35516},{\"end\":35530,\"start\":35527},{\"end\":36053,\"start\":36048},{\"end\":36071,\"start\":36065},{\"end\":36088,\"start\":36079},{\"end\":36401,\"start\":36393},{\"end\":36413,\"start\":36406},{\"end\":36732,\"start\":36724},{\"end\":36743,\"start\":36737},{\"end\":36745,\"start\":36744},{\"end\":36760,\"start\":36753},{\"end\":37274,\"start\":37270},{\"end\":37289,\"start\":37281},{\"end\":37297,\"start\":37294},{\"end\":37596,\"start\":37590},{\"end\":37611,\"start\":37606},{\"end\":37625,\"start\":37618},{\"end\":38005,\"start\":38004},{\"end\":38021,\"start\":38016},{\"end\":38422,\"start\":38418},{\"end\":38438,\"start\":38434},{\"end\":38455,\"start\":38447},{\"end\":38791,\"start\":38787},{\"end\":38801,\"start\":38796},{\"end\":38817,\"start\":38809},{\"end\":38828,\"start\":38823},{\"end\":38841,\"start\":38835},{\"end\":38845,\"start\":38842},{\"end\":39520,\"start\":39513},{\"end\":39535,\"start\":39526},{\"end\":39554,\"start\":39548},{\"end\":40290,\"start\":40282},{\"end\":40304,\"start\":40296},{\"end\":40315,\"start\":40309},{\"end\":40329,\"start\":40322},{\"end\":40341,\"start\":40334},{\"end\":40355,\"start\":40348},{\"end\":40369,\"start\":40361},{\"end\":41043,\"start\":41037},{\"end\":41049,\"start\":41048},{\"end\":41065,\"start\":41059},{\"end\":41080,\"start\":41074},{\"end\":41098,\"start\":41090},{\"end\":41111,\"start\":41103},{\"end\":41394,\"start\":41389},{\"end\":41403,\"start\":41399},{\"end\":41415,\"start\":41408},{\"end\":41644,\"start\":41638},{\"end\":41657,\"start\":41650},{\"end\":41672,\"start\":41664},{\"end\":41681,\"start\":41678},{\"end\":41694,\"start\":41686},{\"end\":41704,\"start\":41700},{\"end\":42352,\"start\":42347},{\"end\":42369,\"start\":42360},{\"end\":42389,\"start\":42382},{\"end\":42406,\"start\":42396},{\"end\":42417,\"start\":42412},{\"end\":43141,\"start\":43136},{\"end\":43160,\"start\":43152},{\"end\":43165,\"start\":43161},{\"end\":43184,\"start\":43176},{\"end\":43490,\"start\":43486},{\"end\":43505,\"start\":43499},{\"end\":43514,\"start\":43512},{\"end\":43526,\"start\":43520},{\"end\":43963,\"start\":43957},{\"end\":43971,\"start\":43968},{\"end\":43983,\"start\":43979},{\"end\":44367,\"start\":44363},{\"end\":44380,\"start\":44376},{\"end\":44387,\"start\":44385},{\"end\":44400,\"start\":44395},{\"end\":44411,\"start\":44410},{\"end\":44427,\"start\":44421},{\"end\":44445,\"start\":44440},{\"end\":44463,\"start\":44455},{\"end\":44479,\"start\":44471},{\"end\":44493,\"start\":44489},{\"end\":44946,\"start\":44939},{\"end\":44962,\"start\":44958},{\"end\":44971,\"start\":44968},{\"end\":44980,\"start\":44978},{\"end\":44990,\"start\":44987},{\"end\":45714,\"start\":45706},{\"end\":45725,\"start\":45721},{\"end\":46353,\"start\":46345},{\"end\":46368,\"start\":46360},{\"end\":46381,\"start\":46377},{\"end\":47040,\"start\":47033},{\"end\":47052,\"start\":47046},{\"end\":47066,\"start\":47058},{\"end\":47079,\"start\":47071},{\"end\":47334,\"start\":47333},{\"end\":47350,\"start\":47344},{\"end\":47352,\"start\":47351},{\"end\":47529,\"start\":47525},{\"end\":47543,\"start\":47536},{\"end\":47559,\"start\":47553},{\"end\":47561,\"start\":47560},{\"end\":48209,\"start\":48201},{\"end\":48220,\"start\":48216},{\"end\":48236,\"start\":48228},{\"end\":48248,\"start\":48242},{\"end\":48754,\"start\":48751},{\"end\":48768,\"start\":48761},{\"end\":48778,\"start\":48775},{\"end\":48792,\"start\":48785},{\"end\":48807,\"start\":48799},{\"end\":49426,\"start\":49422},{\"end\":49436,\"start\":49431},{\"end\":49445,\"start\":49441},{\"end\":49807,\"start\":49804},{\"end\":49822,\"start\":49814},{\"end\":49834,\"start\":49829},{\"end\":49838,\"start\":49835},{\"end\":49851,\"start\":49845},{\"end\":49865,\"start\":49858}]", "bib_author_last_name": "[{\"end\":29872,\"start\":29866},{\"end\":29885,\"start\":29883},{\"end\":30439,\"start\":30431},{\"end\":30448,\"start\":30445},{\"end\":30463,\"start\":30457},{\"end\":30787,\"start\":30775},{\"end\":30804,\"start\":30801},{\"end\":30821,\"start\":30816},{\"end\":30838,\"start\":30829},{\"end\":31465,\"start\":31451},{\"end\":31761,\"start\":31758},{\"end\":31775,\"start\":31772},{\"end\":31785,\"start\":31782},{\"end\":31795,\"start\":31792},{\"end\":31808,\"start\":31804},{\"end\":31820,\"start\":31818},{\"end\":32480,\"start\":32476},{\"end\":32492,\"start\":32490},{\"end\":32501,\"start\":32498},{\"end\":32513,\"start\":32509},{\"end\":32522,\"start\":32519},{\"end\":33280,\"start\":33273},{\"end\":33296,\"start\":33288},{\"end\":33692,\"start\":33689},{\"end\":33704,\"start\":33701},{\"end\":33722,\"start\":33714},{\"end\":33734,\"start\":33730},{\"end\":33751,\"start\":33746},{\"end\":33763,\"start\":33760},{\"end\":33773,\"start\":33769},{\"end\":33787,\"start\":33782},{\"end\":34323,\"start\":34301},{\"end\":34342,\"start\":34338},{\"end\":34354,\"start\":34351},{\"end\":34364,\"start\":34361},{\"end\":34368,\"start\":34366},{\"end\":35017,\"start\":35015},{\"end\":35031,\"start\":35028},{\"end\":35045,\"start\":35042},{\"end\":35058,\"start\":35054},{\"end\":35432,\"start\":35430},{\"end\":35444,\"start\":35441},{\"end\":35457,\"start\":35452},{\"end\":35469,\"start\":35467},{\"end\":35480,\"start\":35477},{\"end\":35492,\"start\":35489},{\"end\":35504,\"start\":35499},{\"end\":35514,\"start\":35510},{\"end\":35525,\"start\":35520},{\"end\":35533,\"start\":35531},{\"end\":36063,\"start\":36054},{\"end\":36077,\"start\":36072},{\"end\":36097,\"start\":36089},{\"end\":36404,\"start\":36402},{\"end\":36419,\"start\":36414},{\"end\":36735,\"start\":36733},{\"end\":36751,\"start\":36746},{\"end\":36766,\"start\":36761},{\"end\":37279,\"start\":37275},{\"end\":37292,\"start\":37290},{\"end\":37303,\"start\":37298},{\"end\":37604,\"start\":37597},{\"end\":37616,\"start\":37612},{\"end\":37640,\"start\":37626},{\"end\":38014,\"start\":38006},{\"end\":38028,\"start\":38022},{\"end\":38032,\"start\":38030},{\"end\":38432,\"start\":38423},{\"end\":38445,\"start\":38439},{\"end\":38463,\"start\":38456},{\"end\":38794,\"start\":38792},{\"end\":38807,\"start\":38802},{\"end\":38821,\"start\":38818},{\"end\":38833,\"start\":38829},{\"end\":38849,\"start\":38846},{\"end\":39524,\"start\":39521},{\"end\":39546,\"start\":39536},{\"end\":39566,\"start\":39555},{\"end\":40294,\"start\":40291},{\"end\":40307,\"start\":40305},{\"end\":40320,\"start\":40316},{\"end\":40332,\"start\":40330},{\"end\":40346,\"start\":40342},{\"end\":40359,\"start\":40356},{\"end\":40374,\"start\":40370},{\"end\":41046,\"start\":41044},{\"end\":41057,\"start\":41050},{\"end\":41072,\"start\":41066},{\"end\":41088,\"start\":41081},{\"end\":41101,\"start\":41099},{\"end\":41115,\"start\":41112},{\"end\":41121,\"start\":41117},{\"end\":41397,\"start\":41395},{\"end\":41406,\"start\":41404},{\"end\":41419,\"start\":41416},{\"end\":41648,\"start\":41645},{\"end\":41662,\"start\":41658},{\"end\":41676,\"start\":41673},{\"end\":41684,\"start\":41682},{\"end\":41698,\"start\":41695},{\"end\":41708,\"start\":41705},{\"end\":42358,\"start\":42353},{\"end\":42380,\"start\":42370},{\"end\":42394,\"start\":42390},{\"end\":42410,\"start\":42407},{\"end\":42424,\"start\":42418},{\"end\":43150,\"start\":43142},{\"end\":43174,\"start\":43166},{\"end\":43190,\"start\":43185},{\"end\":43497,\"start\":43491},{\"end\":43510,\"start\":43506},{\"end\":43518,\"start\":43515},{\"end\":43529,\"start\":43527},{\"end\":43966,\"start\":43964},{\"end\":43977,\"start\":43972},{\"end\":43992,\"start\":43984},{\"end\":44296,\"start\":44290},{\"end\":44374,\"start\":44368},{\"end\":44383,\"start\":44381},{\"end\":44393,\"start\":44388},{\"end\":44408,\"start\":44401},{\"end\":44419,\"start\":44412},{\"end\":44438,\"start\":44428},{\"end\":44453,\"start\":44446},{\"end\":44469,\"start\":44464},{\"end\":44487,\"start\":44480},{\"end\":44499,\"start\":44494},{\"end\":44504,\"start\":44501},{\"end\":44956,\"start\":44947},{\"end\":44966,\"start\":44963},{\"end\":44976,\"start\":44972},{\"end\":44985,\"start\":44981},{\"end\":44996,\"start\":44991},{\"end\":45003,\"start\":44998},{\"end\":45719,\"start\":45715},{\"end\":45733,\"start\":45726},{\"end\":46358,\"start\":46354},{\"end\":46375,\"start\":46369},{\"end\":46389,\"start\":46382},{\"end\":47044,\"start\":47041},{\"end\":47056,\"start\":47053},{\"end\":47069,\"start\":47067},{\"end\":47082,\"start\":47080},{\"end\":47342,\"start\":47335},{\"end\":47359,\"start\":47353},{\"end\":47366,\"start\":47361},{\"end\":47534,\"start\":47530},{\"end\":47551,\"start\":47544},{\"end\":47570,\"start\":47562},{\"end\":48214,\"start\":48210},{\"end\":48226,\"start\":48221},{\"end\":48240,\"start\":48237},{\"end\":48251,\"start\":48249},{\"end\":48759,\"start\":48755},{\"end\":48773,\"start\":48769},{\"end\":48783,\"start\":48779},{\"end\":48797,\"start\":48793},{\"end\":48810,\"start\":48808},{\"end\":49429,\"start\":49427},{\"end\":49439,\"start\":49437},{\"end\":49448,\"start\":49446},{\"end\":49812,\"start\":49808},{\"end\":49827,\"start\":49823},{\"end\":49843,\"start\":49839},{\"end\":49856,\"start\":49852},{\"end\":49869,\"start\":49866}]", "bib_entry": "[{\"attributes\":{\"doi\":\"10.1145/1015330.1015430\",\"id\":\"b0\",\"matched_paper_id\":207155342},\"end\":30377,\"start\":29799},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":211131570},\"end\":30673,\"start\":30379},{\"attributes\":{\"id\":\"b2\"},\"end\":31371,\"start\":30675},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":231918471},\"end\":31679,\"start\":31373},{\"attributes\":{\"id\":\"b4\"},\"end\":32375,\"start\":31681},{\"attributes\":{\"doi\":\"10.1145/3404835.3462913\",\"id\":\"b5\",\"matched_paper_id\":234789923},\"end\":33187,\"start\":32377},{\"attributes\":{\"doi\":\"10.1002/cpa.3160280102\",\"id\":\"b6\",\"matched_paper_id\":122708573},\"end\":33557,\"start\":33189},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":235422153},\"end\":34232,\"start\":33559},{\"attributes\":{\"doi\":\"10.18653/v1/2020.emnlp-main.654\",\"id\":\"b8\",\"matched_paper_id\":222066712},\"end\":34944,\"start\":34234},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":219531210},\"end\":35296,\"start\":34946},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":244714676},\"end\":35962,\"start\":35298},{\"attributes\":{\"doi\":\"10.1162/coli.2008.07-028-R2-05-82\",\"id\":\"b11\",\"matched_paper_id\":9457948},\"end\":36348,\"start\":35964},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":16153365},\"end\":36666,\"start\":36350},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":6448021},\"end\":37216,\"start\":36668},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2428314},\"end\":37516,\"start\":37218},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":220055813},\"end\":37958,\"start\":37518},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":6628106},\"end\":38359,\"start\":37960},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":209202457},\"end\":38689,\"start\":38361},{\"attributes\":{\"doi\":\"10.18653/v1/2020.emnlp-main.146\",\"id\":\"b18\",\"matched_paper_id\":216642138},\"end\":39445,\"start\":38691},{\"attributes\":{\"doi\":\"10.24963/ijcai.2019/819\",\"id\":\"b19\",\"matched_paper_id\":85499320},\"end\":40177,\"start\":39447},{\"attributes\":{\"doi\":\"10.1145/3336191.3371769\",\"id\":\"b20\",\"matched_paper_id\":209366827},\"end\":40987,\"start\":40179},{\"attributes\":{\"doi\":\"arXiv:1612.05688\",\"id\":\"b21\"},\"end\":41328,\"start\":40989},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":246634968},\"end\":41573,\"start\":41330},{\"attributes\":{\"doi\":\"10.18653/v1/2020.acl-main.98\",\"id\":\"b23\",\"matched_paper_id\":218571035},\"end\":42254,\"start\":41575},{\"attributes\":{\"doi\":\"10.18653/v1/2020.coling-main.41\",\"id\":\"b24\",\"matched_paper_id\":227012658},\"end\":43105,\"start\":42256},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":292852},\"end\":43394,\"start\":43107},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":186206780},\"end\":43841,\"start\":43396},{\"attributes\":{\"doi\":\"10.1007/s10462-022-10248-8\",\"id\":\"b27\",\"matched_paper_id\":234342691},\"end\":44238,\"start\":43843},{\"attributes\":{\"id\":\"b28\"},\"end\":44361,\"start\":44240},{\"attributes\":{\"doi\":\"arXiv:2203.02155\",\"id\":\"b29\"},\"end\":44865,\"start\":44363},{\"attributes\":{\"doi\":\"10.1145/3404835.3462839\",\"id\":\"b30\",\"matched_paper_id\":234357478},\"end\":45659,\"start\":44867},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b31\",\"matched_paper_id\":8498625},\"end\":46253,\"start\":45661},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b32\",\"matched_paper_id\":103456},\"end\":46952,\"start\":46255},{\"attributes\":{\"doi\":\"arXiv:2210.08917\",\"id\":\"b33\"},\"end\":47290,\"start\":46954},{\"attributes\":{\"id\":\"b34\"},\"end\":47473,\"start\":47292},{\"attributes\":{\"doi\":\"10.1145/1390156.1390286\",\"id\":\"b35\",\"matched_paper_id\":14028349},\"end\":48054,\"start\":47475},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":219636414},\"end\":48694,\"start\":48056},{\"attributes\":{\"doi\":\"10.18653/v1/2020.acl-main.638\",\"id\":\"b37\",\"matched_paper_id\":216553145},\"end\":49340,\"start\":48696},{\"attributes\":{\"doi\":\"10.1109/TPAMI.2021.3096966\",\"id\":\"b38\",\"matched_paper_id\":235907258},\"end\":49747,\"start\":49342},{\"attributes\":{\"doi\":\"10.18653/v1/2020.coling-main.365\",\"id\":\"b39\",\"matched_paper_id\":222208984},\"end\":50425,\"start\":49749}]", "bib_title": "[{\"end\":29857,\"start\":29799},{\"end\":30422,\"start\":30379},{\"end\":30767,\"start\":30675},{\"end\":31421,\"start\":31373},{\"end\":31749,\"start\":31681},{\"end\":32469,\"start\":32377},{\"end\":33267,\"start\":33189},{\"end\":33682,\"start\":33559},{\"end\":34290,\"start\":34234},{\"end\":35003,\"start\":34946},{\"end\":35421,\"start\":35298},{\"end\":36046,\"start\":35964},{\"end\":36391,\"start\":36350},{\"end\":36722,\"start\":36668},{\"end\":37268,\"start\":37218},{\"end\":37588,\"start\":37518},{\"end\":38002,\"start\":37960},{\"end\":38416,\"start\":38361},{\"end\":38785,\"start\":38691},{\"end\":39511,\"start\":39447},{\"end\":40280,\"start\":40179},{\"end\":41387,\"start\":41330},{\"end\":41636,\"start\":41575},{\"end\":42345,\"start\":42256},{\"end\":43134,\"start\":43107},{\"end\":43484,\"start\":43396},{\"end\":43955,\"start\":43843},{\"end\":44937,\"start\":44867},{\"end\":45704,\"start\":45661},{\"end\":46343,\"start\":46255},{\"end\":47523,\"start\":47475},{\"end\":48199,\"start\":48056},{\"end\":48749,\"start\":48696},{\"end\":49420,\"start\":49342},{\"end\":49802,\"start\":49749}]", "bib_author": "[{\"end\":29874,\"start\":29859},{\"end\":29887,\"start\":29874},{\"end\":30441,\"start\":30424},{\"end\":30450,\"start\":30441},{\"end\":30465,\"start\":30450},{\"end\":30789,\"start\":30769},{\"end\":30806,\"start\":30789},{\"end\":30823,\"start\":30806},{\"end\":30840,\"start\":30823},{\"end\":31436,\"start\":31423},{\"end\":31443,\"start\":31436},{\"end\":31467,\"start\":31443},{\"end\":31763,\"start\":31751},{\"end\":31777,\"start\":31763},{\"end\":31787,\"start\":31777},{\"end\":31797,\"start\":31787},{\"end\":31810,\"start\":31797},{\"end\":31822,\"start\":31810},{\"end\":32482,\"start\":32471},{\"end\":32494,\"start\":32482},{\"end\":32503,\"start\":32494},{\"end\":32515,\"start\":32503},{\"end\":32524,\"start\":32515},{\"end\":33282,\"start\":33269},{\"end\":33298,\"start\":33282},{\"end\":33694,\"start\":33684},{\"end\":33706,\"start\":33694},{\"end\":33724,\"start\":33706},{\"end\":33736,\"start\":33724},{\"end\":33753,\"start\":33736},{\"end\":33765,\"start\":33753},{\"end\":33775,\"start\":33765},{\"end\":33789,\"start\":33775},{\"end\":34325,\"start\":34292},{\"end\":34344,\"start\":34325},{\"end\":34356,\"start\":34344},{\"end\":34366,\"start\":34356},{\"end\":34370,\"start\":34366},{\"end\":35019,\"start\":35005},{\"end\":35033,\"start\":35019},{\"end\":35047,\"start\":35033},{\"end\":35060,\"start\":35047},{\"end\":35434,\"start\":35423},{\"end\":35446,\"start\":35434},{\"end\":35459,\"start\":35446},{\"end\":35471,\"start\":35459},{\"end\":35482,\"start\":35471},{\"end\":35494,\"start\":35482},{\"end\":35506,\"start\":35494},{\"end\":35516,\"start\":35506},{\"end\":35527,\"start\":35516},{\"end\":35535,\"start\":35527},{\"end\":36065,\"start\":36048},{\"end\":36079,\"start\":36065},{\"end\":36099,\"start\":36079},{\"end\":36406,\"start\":36393},{\"end\":36421,\"start\":36406},{\"end\":36737,\"start\":36724},{\"end\":36753,\"start\":36737},{\"end\":36768,\"start\":36753},{\"end\":37281,\"start\":37270},{\"end\":37294,\"start\":37281},{\"end\":37305,\"start\":37294},{\"end\":37606,\"start\":37590},{\"end\":37618,\"start\":37606},{\"end\":37642,\"start\":37618},{\"end\":38016,\"start\":38004},{\"end\":38030,\"start\":38016},{\"end\":38034,\"start\":38030},{\"end\":38434,\"start\":38418},{\"end\":38447,\"start\":38434},{\"end\":38465,\"start\":38447},{\"end\":38796,\"start\":38787},{\"end\":38809,\"start\":38796},{\"end\":38823,\"start\":38809},{\"end\":38835,\"start\":38823},{\"end\":38851,\"start\":38835},{\"end\":39526,\"start\":39513},{\"end\":39548,\"start\":39526},{\"end\":39568,\"start\":39548},{\"end\":40296,\"start\":40282},{\"end\":40309,\"start\":40296},{\"end\":40322,\"start\":40309},{\"end\":40334,\"start\":40322},{\"end\":40348,\"start\":40334},{\"end\":40361,\"start\":40348},{\"end\":40376,\"start\":40361},{\"end\":41048,\"start\":41037},{\"end\":41059,\"start\":41048},{\"end\":41074,\"start\":41059},{\"end\":41090,\"start\":41074},{\"end\":41103,\"start\":41090},{\"end\":41117,\"start\":41103},{\"end\":41123,\"start\":41117},{\"end\":41399,\"start\":41389},{\"end\":41408,\"start\":41399},{\"end\":41421,\"start\":41408},{\"end\":41650,\"start\":41638},{\"end\":41664,\"start\":41650},{\"end\":41678,\"start\":41664},{\"end\":41686,\"start\":41678},{\"end\":41700,\"start\":41686},{\"end\":41710,\"start\":41700},{\"end\":42360,\"start\":42347},{\"end\":42382,\"start\":42360},{\"end\":42396,\"start\":42382},{\"end\":42412,\"start\":42396},{\"end\":42426,\"start\":42412},{\"end\":43152,\"start\":43136},{\"end\":43176,\"start\":43152},{\"end\":43192,\"start\":43176},{\"end\":43499,\"start\":43486},{\"end\":43512,\"start\":43499},{\"end\":43520,\"start\":43512},{\"end\":43531,\"start\":43520},{\"end\":43968,\"start\":43957},{\"end\":43979,\"start\":43968},{\"end\":43994,\"start\":43979},{\"end\":44298,\"start\":44290},{\"end\":44376,\"start\":44363},{\"end\":44385,\"start\":44376},{\"end\":44395,\"start\":44385},{\"end\":44410,\"start\":44395},{\"end\":44421,\"start\":44410},{\"end\":44440,\"start\":44421},{\"end\":44455,\"start\":44440},{\"end\":44471,\"start\":44455},{\"end\":44489,\"start\":44471},{\"end\":44501,\"start\":44489},{\"end\":44506,\"start\":44501},{\"end\":44958,\"start\":44939},{\"end\":44968,\"start\":44958},{\"end\":44978,\"start\":44968},{\"end\":44987,\"start\":44978},{\"end\":44998,\"start\":44987},{\"end\":45005,\"start\":44998},{\"end\":45721,\"start\":45706},{\"end\":45735,\"start\":45721},{\"end\":46360,\"start\":46345},{\"end\":46377,\"start\":46360},{\"end\":46391,\"start\":46377},{\"end\":47046,\"start\":47033},{\"end\":47058,\"start\":47046},{\"end\":47071,\"start\":47058},{\"end\":47084,\"start\":47071},{\"end\":47344,\"start\":47333},{\"end\":47361,\"start\":47344},{\"end\":47368,\"start\":47361},{\"end\":47536,\"start\":47525},{\"end\":47553,\"start\":47536},{\"end\":47572,\"start\":47553},{\"end\":48216,\"start\":48201},{\"end\":48228,\"start\":48216},{\"end\":48242,\"start\":48228},{\"end\":48253,\"start\":48242},{\"end\":48761,\"start\":48751},{\"end\":48775,\"start\":48761},{\"end\":48785,\"start\":48775},{\"end\":48799,\"start\":48785},{\"end\":48812,\"start\":48799},{\"end\":49431,\"start\":49422},{\"end\":49441,\"start\":49431},{\"end\":49450,\"start\":49441},{\"end\":49814,\"start\":49804},{\"end\":49829,\"start\":49814},{\"end\":49845,\"start\":49829},{\"end\":49858,\"start\":49845},{\"end\":49871,\"start\":49858}]", "bib_venue": "[{\"end\":30086,\"start\":29998},{\"end\":30999,\"start\":30928},{\"end\":32001,\"start\":31920},{\"end\":32795,\"start\":32671},{\"end\":33844,\"start\":33840},{\"end\":34576,\"start\":34497},{\"end\":35644,\"start\":35598},{\"end\":36947,\"start\":36866},{\"end\":38110,\"start\":38092},{\"end\":39057,\"start\":38978},{\"end\":39778,\"start\":39693},{\"end\":40579,\"start\":40489},{\"end\":41899,\"start\":41827},{\"end\":42622,\"start\":42536},{\"end\":45276,\"start\":45152},{\"end\":45933,\"start\":45837},{\"end\":46598,\"start\":46493},{\"end\":47755,\"start\":47675},{\"end\":49002,\"start\":48930},{\"end\":50060,\"start\":49982},{\"end\":29996,\"start\":29910},{\"end\":30517,\"start\":30465},{\"end\":30926,\"start\":30840},{\"end\":31519,\"start\":31467},{\"end\":31918,\"start\":31822},{\"end\":32669,\"start\":32547},{\"end\":33366,\"start\":33320},{\"end\":33838,\"start\":33789},{\"end\":34495,\"start\":34401},{\"end\":35112,\"start\":35060},{\"end\":35596,\"start\":35535},{\"end\":36157,\"start\":36132},{\"end\":36470,\"start\":36421},{\"end\":36864,\"start\":36768},{\"end\":37357,\"start\":37305},{\"end\":37691,\"start\":37642},{\"end\":38090,\"start\":38034},{\"end\":38517,\"start\":38465},{\"end\":38976,\"start\":38882},{\"end\":39691,\"start\":39591},{\"end\":40487,\"start\":40399},{\"end\":41035,\"start\":40989},{\"end\":41444,\"start\":41421},{\"end\":41825,\"start\":41738},{\"end\":42534,\"start\":42457},{\"end\":43241,\"start\":43192},{\"end\":43580,\"start\":43531},{\"end\":44050,\"start\":44020},{\"end\":44288,\"start\":44240},{\"end\":44589,\"start\":44522},{\"end\":45150,\"start\":45028},{\"end\":45835,\"start\":45739},{\"end\":46491,\"start\":46395},{\"end\":47031,\"start\":46954},{\"end\":47331,\"start\":47292},{\"end\":47673,\"start\":47595},{\"end\":48335,\"start\":48253},{\"end\":48928,\"start\":48841},{\"end\":49538,\"start\":49476},{\"end\":49980,\"start\":49903}]"}}}, "year": 2023, "month": 12, "day": 17}