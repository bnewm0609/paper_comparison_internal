{"id": 230796573, "updated": "2022-09-12 11:16:35.674", "metadata": {"title": "Deep learning model for classifying endometrial lesions", "authors": "[{\"first\":\"YunZheng\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"ZiHao\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Jin\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"CuiCui\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"YuShan\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Hao\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"LuHe\",\"last\":\"Shan\",\"middle\":[]},{\"first\":\"JiaNing\",\"last\":\"Huo\",\"middle\":[]},{\"first\":\"JiaHui\",\"last\":\"Gu\",\"middle\":[]},{\"first\":\"Xiaoxin\",\"last\":\"Ma\",\"middle\":[]}]", "venue": "Journal of Translational Medicine", "journal": "Journal of Translational Medicine", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Background Hysteroscopy is a commonly used technique for diagnosing endometrial lesions. It is essential to develop an objective model to aid clinicians in lesion diagnosis, as each type of lesion has a distinct treatment, and judgments of hysteroscopists are relatively subjective. This study constructs a convolutional neural network model that can automatically classify endometrial lesions using hysteroscopic images as input. Methods All histopathologically confirmed endometrial lesion images were obtained from the Shengjing Hospital of China Medical University, including endometrial hyperplasia without atypia, atypical hyperplasia, endometrial cancer, endometrial polyps, and submucous myomas. The study included 1851 images from 454 patients. After the images were preprocessed (histogram equalization, addition of noise, rotations, and flips), a training set of 6478 images was input into a tuned VGGNet-16 model; 250 images were used as the test set to evaluate the model\u2019s performance. Thereafter, we compared the model\u2019s results with the diagnosis of gynecologists. Results The overall accuracy of the VGGNet-16 model in classifying endometrial lesions is 80.8%. Its sensitivity to endometrial hyperplasia without atypia, atypical hyperplasia, endometrial cancer, endometrial polyp, and submucous myoma is 84.0%, 68.0%, 78.0%, 94.0%, and 80.0%, respectively; for these diagnoses, the model\u2019s specificity is 92.5%, 95.5%, 96.5%, 95.0%, and 96.5%, respectively. When classifying lesions as benign or as premalignant/malignant, the VGGNet-16 model\u2019s accuracy, sensitivity, and specificity are 90.8%, 83.0%, and 96.0%, respectively. The diagnostic performance of the VGGNet-16 model is slightly better than that of the three gynecologists in both classification tasks. With the aid of the model, the overall accuracy of the diagnosis of endometrial lesions by gynecologists can be improved. Conclusions The VGGNet-16 model performs well in classifying endometrial lesions from hysteroscopic images and can provide objective diagnostic evidence for hysteroscopists.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": "33407588", "pubmedcentral": "7788977", "dblp": null, "doi": "10.1186/s12967-020-02660-x"}}, "content": {"source": {"pdf_hash": "0d12cd5346cad2d54a6f5c524606fb45b56dbb5c", "pdf_src": "Anansi", "pdf_uri": "[\"https://translational-medicine.biomedcentral.com/track/pdf/10.1186/s12967-020-02660-x.pdf\"]", "oa_url_match": true, "oa_info": {"license": "CCBY", "open_access_url": "https://translational-medicine.biomedcentral.com/track/pdf/10.1186/s12967-020-02660-x", "status": "GOLD"}}, "grobid": {"id": "601aa89460f4f7b396a32da3ab8f0b12aed46288", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0d12cd5346cad2d54a6f5c524606fb45b56dbb5c.txt", "contents": "\nDeep learning model for classifying endometrial lesions\n\n\nYunzheng Zhang \nZihao Wang \nJin Zhang \nCuicui Wang \nYushan Wang \nHao Chen \nLuhe Shan \nJianing Huo \nJiahui Gu \nXiaoxin Ma \nDeep learning model for classifying endometrial lesions\n10.1186/s12967-020-02660-xZhang et al. J Transl Med (2021) 19:10 RESEARCH This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article' s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article' s zero/1.0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.Computer-aided diagnosisConvolutional neural networkEndometrial lesionHysteroscopyVGGNet\nBackground: Hysteroscopy is a commonly used technique for diagnosing endometrial lesions. It is essential to develop an objective model to aid clinicians in lesion diagnosis, as each type of lesion has a distinct treatment, and judgments of hysteroscopists are relatively subjective. This study constructs a convolutional neural network model that can automatically classify endometrial lesions using hysteroscopic images as input.Methods: All histopathologically confirmed endometrial lesion images were obtained from the Shengjing Hospital of China Medical University, including endometrial hyperplasia without atypia, atypical hyperplasia, endometrial cancer, endometrial polyps, and submucous myomas. The study included 1851 images from 454 patients. After the images were preprocessed (histogram equalization, addition of noise, rotations, and flips), a training set of 6478 images was input into a tuned VGGNet-16 model; 250 images were used as the test set to evaluate the model's performance. Thereafter, we compared the model's results with the diagnosis of gynecologists.Results:The overall accuracy of the VGGNet-16 model in classifying endometrial lesions is 80.8%. Its sensitivity to endometrial hyperplasia without atypia, atypical hyperplasia, endometrial cancer, endometrial polyp, and submucous myoma is 84.0%, 68.0%, 78.0%, 94.0%, and 80.0%, respectively; for these diagnoses, the model's specificity is 92.5%, 95.5%, 96.5%, 95.0%, and 96.5%, respectively. When classifying lesions as benign or as premalignant/malignant, the VGGNet-16 model's accuracy, sensitivity, and specificity are 90.8%, 83.0%, and 96.0%, respectively. The diagnostic performance of the VGGNet-16 model is slightly better than that of the three gynecologists in both classification tasks. With the aid of the model, the overall accuracy of the diagnosis of endometrial lesions by gynecologists can be improved.Conclusions:The VGGNet-16 model performs well in classifying endometrial lesions from hysteroscopic images and can provide objective diagnostic evidence for hysteroscopists.\n\nBackground\n\nAt the clinic, patients are often diagnosed with suspected endometrial lesion due to symptoms such as abnormal uterine bleeding or infertility [1,2]. Transvaginal ultrasound and diagnostic hysteroscopy are common gynecological examinations to diagnose endometrial lesions conclusively [3][4][5]. Transvaginal ultrasound is usually the first choice, but it has low diagnostic specificity and does not enable physicians to obtain pathological tissue specimen; in some cases, further hysteroscopy is required. [3][4][5]. Diagnostic hysteroscopy is a minimally invasive examination through which the hysteroscopist can directly observe the endometrial lesions and normal endometrium in the patient's uterine cavity, so that the gynecologist can make a more accurate primary diagnosis [6]. These endometrial lesions include endometrial polyps, submucous myomas, intrauterine adhesions, endometrial hyperplasia, malignancies, intrauterine foreign bodies, placental remnants, and endometritis [6]. An accurate primary diagnosis helps gynecologists to explain the condition to patients and decide on a primary treatment. However, the diagnostic performance of hysteroscopy for these lesions depends on the experience of the hysteroscopist, resulting in a degree of subjectivity in the gynecologist's diagnosis [7]. A stable and objective computer-aided diagnosis (CAD) system could shorten the learning curve of inexperienced gynecologists and effectively reduce the subjectivity (interobserver error) of gynecologist diagnosis.\n\nDeep learning is a discipline that has recently played a prominent role in fields such as computer vision, speech recognition, and natural language processing [8]. Many practices in the medical field have also benefited from the use of deep learning, including identifying potential depression patients in social networks and locating the cecum in surgical videos [9,10]. Convolutional neural networks (CNNs) are a class of algorithms that excel in image classification tasks in deep learning, especially for classifying or detecting objects that can be directly observed [11]. It has been reported that CNNs can diagnose skin cancer at a level no less than that of experts [12]. The ability of CNNs to classify laryngoscopic images in most cases exceeds that of physicians [13]. There have been many other reports of endoscopic CAD systems based on deep learning, and excellent results have been achieved in cystoscopy, gastroscopy, enteroscopy, and colposcopy [14][15][16][17]. Deep learning has previously been applied in the field of hysteroscopy: T\u00f6r\u00f6k reported the use of fully convolutional neural networks (FCNNs) to segment uterine myomas and normal uterine myometrium [18], and Burai used FCNNs to identify the uterine wall [19].However, no CNN-based CAD system for hysteroscopy has yet been reported.\n\nThis study considers the five most common endometrial lesions: endometrial hyperplasia without atypia (EH), including simple and complex hyperplasia; atypical hyperplasia (AH); endometrial cancer (EC); endometrial polyps (EPs); and submucous myomas (SMs) [20]. This study aimed to construct a CNN-based CAD system that can classify endometrial lesion images obtained from hysteroscopy and to evaluate the diagnostic performance of this model. The results show that the CAD system slightly outperforms gynecologists in classifying endometrial lesion images. It provides evidence of the feasibility of using artificial intelligence to assist in clinical diagnosis of endometrial lesions.\n\n\nMethods\n\n\nDataset\n\nThis study retrospectively collected images of patients who underwent hysteroscopic examination at the Shengjing Hospital of China Medical University from 2017 to 2019, which confirmed the presence of endometrial lesions. All images were taken using an Olympus OTV-S7 (Olympus, Tokyo, Japan) endoscopic camera system with a resolution of 720 \u00d7 576 pixels and were stored in JPEG format. Images meeting the following criteria were excluded: (a) poor quality or unclear images; (b) images with no lesions in the field of view; (c) images with a large amount of bleeding in the field of view; (d) images from patients with an intrauterine device or who were receiving hormone therapy; (e) images from patients with multiple uterine diseases; and (f ) images from patients without histopathological results. The resulting dataset included 1851 images from 454 patients, including 509 EH, 222 AH, 280 EC, 615 EP, and 225 SM images. We randomly extracted 250 images (50 images for each category) from the dataset as the testing and validation set, and the remaining images were used as the original training set for data augmentation and model training. Table 1 shows the detailed dataset partition used in this study. Subsequently, the test set was randomly divided into two parts (125 images per part) to explore the role of the model in assisting gynecologists to diagnose endometrial lesions. This study was approved by the Ethics Committee of Shengjing Hospital (No. 2017PS292K).\n\n\nData preprocessing\n\nAll images were manually cropped by gynecologists to remove excessive non-lesion regions and retain the region of interest, thus preventing irrelevant features from disturbing the performance of the deep learning model. To improve the generalizability and robustness of the deep learning model, we performed data augmentation on the training set, including color histogram equalization, random addition of salt-and-pepper noise, 90\u00b0 and 270\u00b0 rotations, and vertical and horizontal flips ( Fig. 1). The final training set was augmented from 1601 to 6478 images. The test set was not processed. Finally, all images were resized to 224 \u00d7 224 pixels and rescaled for training, validating, and testing.\n\n\nConvolutional neural network and transfer learning\n\nWe selected VGGNet [21] as the main structure of our deep learning model and tuned it to implement transfer learning [22]. VGGNet was developed by the Oxford Visual Geometry Group and won second place in the image classification task of the 2014 ImageNet Large Scale Visual Recognition Challenge (ILSVRC) [23]. It has a top-5 accuracy of 92.3% in classifying 1000 object categories. Compared to AlexNet [24], the winner of ILSVRC 2012, VGGNet uses a smaller convolution kernel and deepens the network to achieve better results [23]. VGGNet-16 and VGGNet-19 are commonly used versions of VGG-Net. There is no significant difference in the effect of the two in application, but VGGNet-16 has fewer layers and parameters than VGGNet-19 [21]. This provides VGG-Net-16 with shorter processing time and lower storage space usage than VGGNet-19, so we selected VGG-Net-16 as our model network.\n\nWe employed the VGGNet-16 CNN, pretrained on ImageNet, and adjusted its 4096 neurons in the fully connected layer to 512 neurons and its 1000-category output layer to 5 categories. We added a batch normalization layer after each convolutional layer to improve the training speed of the model [25]. Important training parameters were set as follows: (a) the input shape was 224 pixel \u00d7 224 pixel \u00d7 3 channel; (b) the batch size was 64; (c) the number of training epochs was 200; and (d) the optimizer used was stochastic gradient descent (SGD) with a learning rate of 0.00001 and a momentum of 0.9. The structure of our CNN is shown in Fig. 2, and a summary of the model is shown in Additional file 1: Table S1. The CNN for this research was built using the open source Keras neural network library [26]. Our fine-tuned VGG-Net-16 CNN was used for transfer learning and endometrial lesion classification task.\n\n\nPerformance evaluation metrics\n\nTo evaluate the diagnostic performance of the CNN model, one chief physician with more than 20 years of experience and two attending physicians with more than 10 years of experience in hysteroscopic examination and surgery diagnosed lesions using all images in the test set without knowing the histopathological results. These diagnoses were compared with the diagnostic results of the CNN model.\n\nTo explore the auxiliary role of the model in the diagnosis of endometrial lesions by gynecologists, three other licensed gynecologists performed direct diagnosis and model-aided diagnosis on two randomly divided test sets without knowing the histopathological results.\n\nWe present the results in two ways: five-category and two-category classification. In the first task, each lesion was classified as EH, AH, EC, EP, or SM. In the second task, lesions were categorized as premalignant/malignant (AH and EC) or benign (EH, EP, and SM).\n\nThe diagnostic performance of the model and that of gynecologists is initially demonstrated using confusion matrix, which records the samples in the test set according to their true and predicted categories in the form of a matrix, but it is not a direct evaluation metric. The actual evaluation metrics used in this study were derived from the confusion matrix, which shows the numbers of true positive (TP), false positive (FP), false negative (FN), and true negative (TN) classifications. The secondary evaluation metrics calculated from the primary evaluation metrics are as follows:\n\nAll calculation and visualization operations were implemented in Python Version 3.7.0.\n\n\nResults\n\nDuring training, the model's accuracy changed with increase in epochs, as shown in Fig. 3. After 90 epochs, the validation accuracy plateaued. F1 -Score = 2 \u00d7 P \u00d7 TPR/(P + TPR);\nAccuracy = 5 \u00d7 \ufffdTPi/\ufffd(TPi + FPi + FNi + TNi);\nArea under the curve (AUC): the area under the receiver operating characteristic (ROC) curve.\n\n\nFive-category classification task\n\nFor the five-category classification task, the VGGNet- 16   To directly observe the clustering of the five types of lesions, we applied the t-distributed stochastic neighbor embedding (t-SNE) [27] dimension reduction algorithm. The 512-dimensional output of all images in the test set of the last fully connected layer was reduced to two dimensions and is displayed in Fig. 6. We can see from this figure that most of the images are mapped on their own fixed areas, but there is an area of overlap between EH, AH, and EC. To deepen our understanding of the CNN's calculation process, we output the sum feature maps of an SM image in the test set at each convolutional layer, batch normalization layer, and MaxPool layer of the VGGNet-16 model and superimposed them on the original image after upsampling these sum feature maps. The superimposed heatmaps are shown in Fig. 7. Some examples of the model's classification are shown in Fig. 8.\n\n\nTwo-category classification task\n\nWhen classifying premalignant/malignant and benign lesions, the accuracy, sensitivity, specificity, precision, f1-score, and AUC of the VGGNet-16 model were 90.8%, 83.0%, 96.0%, 93.3%, 87.8%, and 0.944. The accuracy of the three gynecologists was 86.8%, 82.4%, and 84.8%, and their AUCs were 0.863, 0.813, and 0.842, respectively. In this task, both the model and the gynecologists improved their performance significantly compared with the fivecategory classification task. Detailed two-category diagnostic performance evaluation metrics are shown in Table 3. The two-category ROC curve of the model and gynecologists is shown in Fig. 9.\n\n\nComparison between model-aided diagnosis and direct diagnosis by gynecologists\n\nAfter we split the test set equally at random, the test sets Part I and Part II were used for direct diagnosis and model-aided diagnosis by gynecologists. The accuracies of direct diagnosis of test set Part I by the three  Figure S1. The confusion matrices of the direct diagnoses and model-aided diagnoses by gynecologists are shown in Additional file 3: Figure S2.\n\n\nDiscussion\n\nThis study explored the classification ability of the VGG-Net- 16  80.8% and 90.8% in our five-category and two-category classification tasks.\n\nThe benefit of CNN model is that the output provides the probability that a given hysteroscopy image belongs to each category. Even if the model makes a misclassification, the output contains a specific probability of the correct label. In contrast, it is difficult for hysteroscopists to give specific probabilities for their diagnoses. In most cases, gynecologists can only give two judgments: yes or no. The ability to harness probabilities is an important reason why the CNN model has a significantly higher AUC for each lesion type than the gynecologists. In this study, it has been confirmed that the model output probabilities can provide a b c d Dimension-reduced scatter plot of the last fully connected layer of the VGGNet-16 model. We output the 512-dimensional data of all images in the test set at the last fully connected layer of the optimal model and applied the t-SNE algorithm to reduce the data to two dimensions and show them in a scatter plot, along with some example images. AH atypical hyperplasia, EC endometrial cancer, EH endometrial hyperplasia without atypia, EP endometrial polyps, SM submucous myoma\nBlock1_Conv_1 Block1_Conv_2 B lock1_MaxPool Block2_Conv_1 Batch_Norm_1 Batch_Norm_2 Batch_Norm_3 Input image Block3_Conv_1 B lock3_Conv_2 Block3_Conv_3 Block2_MaxPool Batch_Norm_4 Batch_Norm_5 Batch_Norm_6 Block2_Conv_2 Block3_MaxPool Block4_Conv_1 Block4_Conv_2 B lock4_Conv_3 Batch_Norm_8 Batch_Norm_9 Batch_Norm_10 Batch_Norm_7 Block5_Conv_1 Block5_Conv_2 Block5_Conv_3 B lock5_MaxPool Batch_Norm_12 Batch_Norm_1\nBatch_Norm_13 Block4_MaxPool  a convincing diagnostic reference for gynecologists and effectively reduce the subjectivity of gynecologists' diagnoses. Although the CNN model is difficult to interpret [28], visualizing its calculations and outputs helps us to understand its working process.\n\nIn the absence of dynamic vision, diagnosis based only on static local hysteroscopy images led to lower sensitivity and specificity of the gynecologists' diagnoses in this study as compared to results reported in a meta-analysis [29]. Given the appearance similarities of EH, AH, and EC endometrial lesions, it is relatively difficult for both the model and the gynecologists to distinguish between them. In actual clinical practice, hysteroscopists achieve better diagnostic performance through retrospective case data and dynamic vision. Gynecologists will give full consideration to the specific conditions of patients before performing hysteroscopy. For these difficult to distinguish endometrial lesions, gynecologists will actively advise patients to take pathological tissue specimens and submit them for examination during hysteroscopy to confirm the diagnosis and avoid over-or undertreatment. At this stage, the VGGNet-16 model in our study can only be used as an auxiliary diagnostic tool for gynecologists. Gynecologists can refer to the probability provided by the model and combine it with other clinical data to obtain a more accurate preliminary clinical diagnosis before the histopathological results are clear. In future research, we aim to implement a multimodal deep learning model that similarly combines case data and hysteroscopic images [30].\n\nMachine learning and deep learning, an important branch of artificial intelligence, have also made outstanding contributions in the medical field, such as in clinical prediction models and radiomics [31,32]. Regardless of the research direction, these artificial intelligence technologies have considerable clinical application value. We believe that each technology plays a different role in diagnosis, treatment, and the prediction of clinical outcomes. The integration of an artificial intelligence system into each medical subdiscipline, conforming to the clinical diagnosis and treatment process, is the ultimate goal.\n\nThe results of this study have demonstrated the feasibility of applying deep learning techniques to the diagnosis of endometrial lesions. Although there is a gap between the diagnostic performance of the model and the histopathological results in this study, under the experimental conditions of this study, the CNN model's ability to classify hysteroscopic images slightly exceeded that of the gynecologists and can provide gynecologists with objective references.\n\nThere are some limitations to our research. First, this study included only the five most common endometrial lesions, and lesions with low incidence were not included. Moreover, all images were collected from the same endoscopic camera system of the same hospital, thus the images may lack diversity. Finally, no prospective validation was performed in this study. We speculate that by expanding the dataset samples, the retrained model should achieve better diagnostic performance and generalization capability. Our group will collect more data at multiple centers to retrain the model and implement prospective validation. The model that obtains better diagnostic performance will be considered for application to clinical practice.\n\n\nConclusions\n\nIn this study, we developed the first CNN-based CAD system for diagnostic hysteroscopy image classification. The VGGNet-16 model used in our study shows comparable diagnostic performance to expert gynecologists in classifying five types of endometrial lesion images. The model can provide objective diagnostic evidence for hysteroscopists and has potential clinical application value.  \n\n\nSensitivity(TPR) = TP/(TP + FN); Specificity(TNR) = TN/(TN + FP); Precision(P) = TP/(TP + FP);\n\nFig. 2 Fig. 3\n23Structure of the fine-tuned VGGNet-16 model. Our network structure is a tuned VGGNet-16 model. The data stream flows from left to right, and the cross-entropy loss is calculated from the prediction results of each category and their corresponding probabilities. The model iterates repeatedly to reduce the loss value, thereby improving its accuracy Training and validation accuracy by training epochs of VGGNet-16 convolutional neural network. During training, the overall accuracy of the model on the training and validation sets increases as the model iterates. The model's performance plateaus on the training and validation sets at epochs 190 and 90the three gynecologists in accurately diagnosing endometrial lesions.\n\nFig. 5 Fig. 6\n56Confusion matrices of the VGGNet-16 model and gynecologists. Confusion matrices: a, b, c, and d are the confusion matrices of the VGGNet-16 model and gynecologists 1, 2, and 3 in classifying the test set, respectively. The x axes are the predicted labels, which are the diagnoses made by the model or gynecologists. The y axes are the true labels, which is the histopathological result. The number in each small square represents the corresponding number of images with the same predicted true label and its percentage of the total number of images under the true label. AH atypical hyperplasia, EC endometrial cancer, EH endometrial hyperplasia without atypia, EP endometrial polyp, SM submucous myoma\n\nFig. 7 Fig. 8\n78Feature heatmaps of a submucous myoma image output by the VGGNet-16 model. The sum feature maps output by each convolutional layer, batch normalization layer, and MaxPool layer of the VGGNet-16 model for a submucous myoma image in the test set were up-sampled and superimposed on the original image and displayed as feature heatmaps Example classification results output by the VGGNet-16 model. The x axis is the predicted label of the model's output and the y axis is the histopathology result of these images. AH atypical hyperplasia, EC endometrial cancer, EH endometrial hyperplasia without atypia, EP endometrial polyp, SM submucous myoma\n\nFig. 9\n9Binary ROC curves of the VGGNet-16 model and gynecologists. Binary receiver operating characteristic (ROC) curves for classifying lesions as premalignant/malignant or benign. The model curve is shown as a gold line and the curves for gynecologists 1, 2, and 3 are marked with purple, blue, and scarlet diamonds\n\nTable 1\n1Partition details of the endometrial lesion dataset for classificationAH atypical hyperplasia, EC endometrial cancer, EH endometrial hyperplasia without atypia, EP endometrial polyp, SM submucous myomaCategory \nDataset \nTraining set \nTest set \n\nNo. of patients \nNo. of images \nNo. of original \nimages \n\nNo. of augmented \nimages \n\nNo. of test images \n\nEH \n124 \n509 \n459 \n915 \n50 \n\nAH \n53 \n222 \n172 \n1032 \n50 \n\nEC \n66 \n280 \n230 \n1055 \n50 \n\nEP \n148 \n615 \n565 \n825 \n50 \n\nSM \n63 \n225 \n175 \n1050 \n50 \n\nTotal \n454 \n1851 \n1601 \n4877 \n250 \n\n\n\n\nFig. 1Example of image preprocessing. We first cropped and resized all images in the dataset. Then, we augmented the resized training set to increase the amount of data, allowing us to improve the model's generalization ability and robustnessHistogram \nequalization \n\nData augmentation \n\nRandom noise 90\u00b0 Rotate \n\n270\u00b0 Rotate \nVertical flip Horizontal flip \n\nOriginal image \nCropped image \n\nResized image \n\nPage 4 of 13 \nZhang et al. J Transl Med \n(2021) 19:10 \n\n\n\n\nmodel achieves an accuracy of 80.8%. The model's sensitivity and specificity for diagnosing EH lesions are 84.0% and 92.5% (AUC = 0.926), 68.0% and 95.5% (AUC = 0.916) for AH lesions, 78.0% and 96.5% for EC (AUC = 0.952), 94.0% and 95.0% for EP (AUC = 0.981), and 80.0% and 96.5% for SM (AUC = 0.959). The accuracies of the three gynecologists were 72.8%, 69.2%, and 64.4%. Detailed five-category diagnostic performance evaluation metrics are shown in Table 2. The five-category ROC curves of the model and gynecologists are shown inFig. 4. The confusion matrices of the VGGNet-16 model and three gynecologists are shown inFig. 5. The VGGNet-16 modelBlock1 \nBlock2 \nBlock5 \nBlock4 \nBlock3 \n\nAH \n\nEC \n\nEH \n\nEP \n\nSM \n\nInput image \nFine-tuned VGGNet-16 convolutional neural network \nOutput probability \n\nDropout \n3x3 Convolution \nBatch normalization \nMaxPool \nFlatten \nFully connected \n\n\n\nTable 2\n2Diagnostic performance of the VGGNet-16 model and gynecologists in the five-category classification task AH atypical hyperplasia, AUC area under the receiver operating characteristic (ROC) curve, EC endometrial cancer, EH endometrial hyperplasia without atypia, EP endometrial polyp, SM submucous myomaCategory \nSensitivity (%) \nSpecificity (%) \nPrecision (%) \nF1-score (%) \nAUC \nAccuracy (%) \n\nVGGNet-16 \n\nEH \n84.0 \n92.5 \n73.7 \n78.5 \n0.926 \n80.8 \n\nAH \n68.0 \n95.5 \n79.1 \n73.1 \n0.916 \n\nEC \n78.0 \n96.5 \n84.8 \n81.3 \n0.952 \n\nEP \n94.0 \n95.0 \n82.5 \n87.9 \n0.981 \n\nSM \n80.0 \n96.5 \n85.1 \n82.5 \n0.959 \n\nGynecologist 1 \n\nEH \n70.0 \n90.0 \n63.6 \n66.7 \n0.800 \n72.8 \n\nAH \n58.0 \n92.5 \n65.9 \n61.7 \n0.753 \n\nEC \n74.0 \n90.0 \n64.9 \n69.2 \n0.820 \n\nEP \n86.0 \n95.0 \n81.1 \n83.5 \n0.905 \n\nSM \n76.0 \n98.5 \n92.7 \n83.5 \n0.873 \n\nGynecologist 2 \n\nEH \n64.0 \n94.5 \n74.4 \n68.8 \n0.792 \n69.2 \n\nAH \n54.0 \n90.0 \n57.4 \n55.7 \n0.720 \n\nEC \n68.0 \n92.5 \n69.4 \n68.7 \n0.803 \n\nEP \n90.0 \n87.0 \n63.4 \n74.4 \n0.885 \n\nSM \n70.0 \n97.5 \n87.5 \n77.8 \n0.838 \n\nGynecologist 3 \n\nEH \n52.0 \n95.0 \n72.2 \n60.5 \n0.735 \n64.4 \n\nAH \n54.0 \n87.0 \n50.9 \n52.4 \n0.705 \n\nEC \n66.0 \n93.0 \n70.2 \n68.0 \n0.795 \n\nEP \n80.0 \n87.0 \n60.6 \n69.0 \n0.835 \n\nSM \n70.0 \n93.5 \n72.9 \n71.4 \n0.818 \n\n\n\nTable 3\n3Diagnostic performance of the VGGNet-16 model and gynecologists in the two-category classification taskAUC area under the receiver operating characteristic (ROC) curve \n\nClassifier \nSensitivity (%) \nSpecificity (%) \nPrecision (%) \nF1-score (%) \nAUC \nAccuracy (%) \n\nVGGNet-16 \n83.0 \n96.0 \n93.3 \n87.8 \n0.944 \n90.8 \n\nGynecologist 1 \n84.0 \n88.7 \n83.2 \n83.6 \n0.863 \n86.8 \n\nGynecologist 2 \n76.0 \n86.7 \n79.2 \n77.6 \n0.813 \n82.4 \n\nGynecologist 3 \n81.0 \n87.3 \n81.0 \n81.0 \n0.842 \n84.8 \n\n\n\nTable 4\n4Comparison of direct/model-aided diagnostic performance of the gynecologists in the five-category classification taskAH atypical hyperplasia, AUC area under the receiver operating characteristic (ROC) curve, EC endometrial cancer, EH endometrial hyperplasia without atypia, EP endometrial polyp, SM submucous myomaCategory \nSensitivity (%) \nSpecificity (%) \nPrecision (%) \nF1-score (%) \nAUC \nAccuracy (%) \n\nGynecologist 4 \n\nEH \n59.1 \n94.2 \n68.4 \n63.4 \n0.766 \n64.0 \n\nAH \n58.6 \n93.8 \n73.9 \n65.4 \n0.762 \n\nEC \n56.5 \n89.2 \n54.2 \n55.3 \n0.729 \n\nEP \n78.3 \n85.3 \n54.5 \n64.3 \n0.818 \n\nSM \n67.9 \n92.8 \n73.1 \n70.4 \n0.803 \n\nModel-aided gynecologist 4 \n\nEH \n82.1 \n93.8 \n79.3 \n80.7 \n0.880 \n78.4 \n\nAH \n47.6 \n98.1 \n83.3 \n60.6 \n0.728 \n\nEC \n77.8 \n94.9 \n80.8 \n79.2 \n0.863 \n\nEP \n92.6 \n90.8 \n73.5 \n82.0 \n0.917 \n\nSM \n86.4 \n95.1 \n79.2 \n82.6 \n0.908 \n\nGynecologist 5 \n\nEH \n68.0 \n82.5 \n45.5 \n54.5 \n0.754 \n62.4 \n\nAH \n55.2 \n93.8 \n72.7 \n62.7 \n0.745 \n\nEC \n52.2 \n90.2 \n54.5 \n53.3 \n0.712 \n\nEP \n73.9 \n88.2 \n58.6 \n65.4 \n0.811 \n\nSM \n64.3 \n99.0 \n94.7% \n76.6 \n0.816 \n\nModel-aided gynecologist 5 \n\nEH \n82.1 \n84.5 \n60.5 \n69.7 \n0.833 \n72.8 \n\nAH \n42.9 \n92.3 \n52.9 \n47.4 \n0.676 \n\nEC \n74.1 \n96.9 \n87.0 \n80.0 \n0.855 \n\nEP \n81.5 \n94.9 \n81.5 \n81.5 \n0.882 \n\nSM \n77.3 \n97.1 \n85.0 \n81.0 \n0.872 \n\nGynecologist 6 \n\nEH \n59.1 \n96.1 \n76.5 \n66.7 \n0.776 \n69.2 \n\nAH \n58.6 \n85.4 \n54.8 \n56.7 \n0.720 \n\nEC \n52.2 \n92.2 \n60.0 \n55.8 \n0.722 \n\nEP \n91.3 \n93.1 \n75.0 \n82.4 \n0.922 \n\nSM \n78.6 \n92.8 \n75.9 \n77.2 \n0.857 \n\nModel-aided gynecologist 6 \n\nEH \n75.0 \n96.9 \n87.5 \n80.8 \n0.860 \n77.6 \n\nAH \n57.1 \n94.2 \n66.7 \n61.5 \n0.757 \n\nEC \n77.8 \n93.9 \n77.8 \n77.8 \n0.858 \n\nEP \n88.9 \n89.8 \n70.6 \n78.7 \n0.893 \n\nSM \n86.4 \n97.1 \n86.4 \n86.4 \n0.917 \n\nAcknowledgementsWe would like to thank Editage for English language editing.Competing interestsThe author declares that they have no competing interests.Received: 9 July 2020 Accepted: 3 December 2020Supplementary InformationThe online version contains supplementary material available at https ://doi. org/10.1186/s1296 7-020-02660 -x.Additional file 1:Table S1. Summary of fine-tuned VGGNet-16 model. Conv3: 3 \u00d7 3 convolutional layer; ReLU: rectified linear unit.Additional file 2:Figure S1. Five-category ROC curves of the gynecologists' direct diagnoses and model-aided diagnoses. Five-category receiver operating characteristic (ROC) curves: a, c, and e are the direct diagnostic ROC curves of gynecologists 4, 5, and 6, respectively. b, d, and f are the model-aided diagnostic ROC curves of gynecologists 4, 5, and 6, respectively. AH: atypical hyperplasia; EC: endometrial cancer; EH: endometrial hyperplasia without atypia; EP: endometrial polyp; SM: submucous myoma.Additional file 3:Figure S2. Confusion matrices of the gynecologists' direct diagnoses and model-aided diagnoses. Confusion matrices: a, c, and e are the direct diagnostic confusion matrices of gynecologists 4, 5, and 6, respectively. b, d, and f are the model-aided diagnostic confusion matrices of gynecologists 4, 5, and 6, respectively. The x axes are the predicted labels, which are the diagnoses made by the gynecologists or model-aided gynecologists. The y axes are the true labels, which is the histopathological result. The number in each small square represents the corresponding number of images with the same predicted true label and its percentage of the total number of images under the true label. AH: atypical hyperplasia; EC: endometrial cancer; EH: endometrial hyperplasia without atypia; EP: endometrial polyp; SM: submucous myoma.AbbreviationsAvailability of data and materialsAll data presented in this study are included in the article/additional material.Ethics approval and consent to participateThis study was approved by the Ethics Committee of Shengjing Hospital (No. 2017PS292K).Consent for publicationAll the authors in this paper consent to publication of the work.\nAbnormal uterine bleeding: current classification and clinical management. J L Bacon, Obstet Gynecol Clin North Am. 442Bacon JL. Abnormal uterine bleeding: current classification and clinical management. Obstet Gynecol Clin North Am. 2017;44(2):179-93.\n\nFertility and infertility: definition and epidemiology. M Vander Borght, C Wyns, Clin Biochem. 62Vander Borght M, Wyns C. Fertility and infertility: definition and epidemi- ology. Clin Biochem. 2018;62:2-10.\n\nComparison of endometrial assessment by transvaginal ultrasonography and hysteroscopy. D A Yela, P H Pini, C L Benetti-Pinto, Int J Gynaecol Obstet. 1431Yela DA, Pini PH, Benetti-Pinto CL. Comparison of endometrial assess- ment by transvaginal ultrasonography and hysteroscopy. Int J Gynaecol Obstet. 2018;143(1):32-6.\n\nComparison of transvaginal ultrasonography and hysteroscopy in the diagnosis of uterine pathologies. A Babacan, I Gun, C Kizilaslan, O Ozden, M Muhcu, E Mungen, V Atay, Int J Clin Exp Med. 73Babacan A, Gun I, Kizilaslan C, Ozden O, Muhcu M, Mungen E, Atay V. Comparison of transvaginal ultrasonography and hysteroscopy in the diagnosis of uterine pathologies. Int J Clin Exp Med. 2014;7(3):764-9.\n\nCost-effectiveness of diagnostic strategies for the management of abnormal uterine bleeding (heavy menstrual bleeding and post-menopausal bleeding): a decision analysis. N A Cooper, P M Barton, M Breijer, O Caffrey, B C Opmeer, A Timmermans, B W Mol, K S Khan, T J Clark, Health technology assessment. 1824Cooper NA, Barton PM, Breijer M, Caffrey O, Opmeer BC, Timmermans A, Mol BW, Khan KS, Clark TJ. Cost-effectiveness of diagnostic strategies for the management of abnormal uterine bleeding (heavy menstrual bleeding and post-menopausal bleeding): a decision analysis. Health technology assessment (Winchester, England). 2014, 18(24):1-201, v-vi.\n\n. ACOG Technology Assessment. 13113Obstet GynecolACOG Technology Assessment No. 13: hysteroscopy. Obstet Gynecol. 2018;131(5):e151-6.\n\nHysteroscopy in the Netherlands and Flanders: a survey amongst practicing gynaecologists. S Van Wessel, T Hamerlynck, B Schoot, S Weyers, Eur J Obstet Gynecol Reprod Biol. 223van Wessel S, Hamerlynck T, Schoot B, Weyers S. Hysteroscopy in the Netherlands and Flanders: a survey amongst practicing gynaecologists. Eur J Obstet Gynecol Reprod Biol. 2018;223:85-92.\n\nDeep learning. Y Lecun, Y Bengio, G Hinton, Nature. 5217553LeCun Y, Bengio Y, Hinton G. Deep learning. Nature. 2015;521(7553):436-44.\n\nModeling depression symptoms from social network data through multiple instance learning. A Wongkoblap, M A Vadillo, V Curcin, AMIA Jt Summits Transl Sci Proc. 2019Wongkoblap A, Vadillo MA, Curcin V. Modeling depression symptoms from social network data through multiple instance learning. AMIA Jt Summits Transl Sci Proc. 2019;2019:44-53.\n\nIdentification of cecum time-location in a colonoscopy video by deep learning analysis of colonoscope movement. M Cho, J H Kim, K S Hong, J S Kim, H J Kong, S Kim, PeerJ. 77256Cho M, Kim JH, Hong KS, Kim JS, Kong HJ, Kim S. Identification of cecum time-location in a colonoscopy video by deep learning analysis of colo- noscope movement. PeerJ. 2019;7:e7256.\n\nI Hadji, R P Wildes, arXiv:1803.08834What do we understand about convolutional networks? In: arXiv e-prints. Hadji I, Wildes RP. What do we understand about convolutional networks? In: arXiv e-prints. 2018. arXiv:1803.08834\n\nDermatologist-level classification of skin cancer with deep neural networks. A Esteva, B Kuprel, R A Novoa, J Ko, S M Swetter, H M Blau, S Thrun, Nature. 5427639Esteva A, Kuprel B, Novoa RA, Ko J, Swetter SM, Blau HM, Thrun S. Der- matologist-level classification of skin cancer with deep neural networks. Nature. 2017;542(7639):115-8.\n\nAutomatic recognition of laryngoscopic images using a deep-learning technique. J Ren, X Jing, J Wang, X Ren, Y Xu, Q Yang, L Ma, Y Sun, W Xu, N Yang, Laryngoscope. 130Ren J, Jing X, Wang J, Ren X, Xu Y, Yang Q, Ma L, Sun Y, Xu W, Yang N, et al. Automatic recognition of laryngoscopic images using a deep-learning technique. Laryngoscope. 2020;130:E686-93.\n\nSupport system of cystoscopic diagnosis for bladder cancer based on artificial intelligence. A Ikeda, H Nosato, Y Kochi, T Kojima, K Kawai, H Sakanashi, M Murakawa, H Nishiyama, J Endourol. 343Ikeda A, Nosato H, Kochi Y, Kojima T, Kawai K, Sakanashi H, Murakawa M, Nishiyama H. Support system of cystoscopic diagnosis for bladder cancer based on artificial intelligence. J Endourol. 2020;34(3):352-8.\n\nAutomated classification of gastric neoplasms in endoscopic images using a convolutional neural network. B J Cho, C S Bang, S W Park, Y J Yang, S I Seo, H Lim, W G Shin, J T Hong, Y T Yoo, S H Hong, Endoscopy. 5112Cho BJ, Bang CS, Park SW, Yang YJ, Seo SI, Lim H, Shin WG, Hong JT, Yoo YT, Hong SH, et al. Automated classification of gastric neoplasms in endoscopic images using a convolutional neural network. Endoscopy. 2019;51(12):1121-9.\n\nComputer-aided diagnosis based on convolutional neural network system for colorectal polyp classification: preliminary experience. Y Komeda, H Handa, T Watanabe, T Nomura, M Kitahashi, T Sakurai, A Okamoto, T Minami, M Kono, T Arizumi, Oncology. 931SupplKomeda Y, Handa H, Watanabe T, Nomura T, Kitahashi M, Sakurai T, Okamoto A, Minami T, Kono M, Arizumi T, et al. Computer-aided diagnosis based on convolutional neural network system for colorectal polyp clas- sification: preliminary experience. Oncology. 2017;93(Suppl 1):30-4.\n\nApplication of deep learning to the classification of uterine cervical squamous epithelial lesion from colposcopy images. Y Miyagi, K Takehara, T Miyake, Mol Clin Oncol. 11Miyagi Y, Takehara K, Miyake T. Application of deep learning to the clas- sification of uterine cervical squamous epithelial lesion from colposcopy images. Mol Clin Oncol. 2019;11:583-9.\n\nDigital image analysis with fully connected convolutional neural network to facilitate hysteroscopic fibroid resection. P T\u00f6r\u00f6k, B Harangi, Gynecol Obstet Invest. 836T\u00f6r\u00f6k P, Harangi B. Digital image analysis with fully connected convo- lutional neural network to facilitate hysteroscopic fibroid resection. Gynecol Obstet Invest. 2018;83(6):615-9.\n\nSegmentation of the uterine wall by an ensemble of fully convolutional neural networks. P Burai, A Hajdu, F E Manuel, B Harangi, Conf Proc IEEE Eng Med Biol Soc. Burai P, Hajdu A, Manuel FE, Harangi B. Segmentation of the uterine wall by an ensemble of fully convolutional neural networks. Conf Proc IEEE Eng Med Biol Soc. 2018;2018:49-52.\n\nIntroduction of WHO classification of tumours of female reproductive organs. Z Lu, J Chen, Zhonghua Bing Li Xue Za Zhi. 4310fourth editionLu Z, Chen J. Introduction of WHO classification of tumours of female reproductive organs, fourth edition. Zhonghua Bing Li Xue Za Zhi. 2014;43(10):649-50.\n\nVery deep convolutional networks for largescale image recognition. arXiv e-prints. K Simonyan, A Zisserman, arXiv:1409.1556Simonyan K, Zisserman A. Very deep convolutional networks for large- scale image recognition. arXiv e-prints; 2014. arXiv:1409.1556.\n\n. C Tan, F Sun, T Kong, W Zhang, C Yang, C Liu, A Survey on Deep Transfer Learning. Cham: Springer. Tan C, Sun F, Kong T, Zhang W, Yang C, Liu C. A Survey on Deep Transfer Learning. Cham: Springer; 2018. p. 270-9.\n\n. O Russakovsky, J Deng, H Su, J Krause, S Satheesh, S Ma, Z Huang, A Karpathy, A Khosla, M Bernstein, arXiv:1409.0575ImageNet large scale visual recognition challenge. arXiv e-printsRussakovsky O, Deng J, Su H, Krause J, Satheesh S, Ma S, Huang Z, Karpa- thy A, Khosla A, Bernstein M et al. ImageNet large scale visual recognition challenge. arXiv e-prints; 2014. arXiv:1409.0575.\n\nImageNet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, Commun ACM. 606Krizhevsky A, Sutskever I, Hinton GE. ImageNet classification with deep convolutional neural networks. Commun ACM. 2017;60(6):84-90.\n\nBatch normalization: accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, arXiv:1502.03167Ioffe S, Szegedy C. Batch normalization: accelerating deep network train- ing by reducing internal covariate shift; 2015. arXiv:1502.03167.\n\nF Chollet, GitHub repository. GitHub. Chollet F et al. Keras. In: GitHub repository. GitHub; 2015.\n\nVisualizing data using T-SNE. L Van Der Maaten, G Hinton, J Mach Learn Res. 911Van Der Maaten L, Hinton G. Visualizing data using T-SNE. J Mach Learn Res. 2008;9(11):2579-625.\n\nNetwork dissection: quantifying interpretability of deep visual representations. D Bau, B Zhou, A Khosla, A Oliva, A Torralba, arXiv:1704.05796arXiv e-printsBau D, Zhou B, Khosla A, Oliva A, Torralba A. Network dissection: quantify- ing interpretability of deep visual representations. arXiv e-prints; 2017. arXiv:1704.05796.\n\nHysteroscopy in women with abnormal uterine bleeding: a meta-analysis on four major endometrial pathologies. F Gkrozou, G Dimakopoulos, T Vrekoussis, L Lavasidis, A Koutlas, I Navrozoglou, T Stefos, M Paschopoulos, Arch Gynecol Obstet. 2916Gkrozou F, Dimakopoulos G, Vrekoussis T, Lavasidis L, Koutlas A, Navrozo- glou I, Stefos T, Paschopoulos M. Hysteroscopy in women with abnormal uterine bleeding: a meta-analysis on four major endometrial pathologies. Arch Gynecol Obstet. 2015;291(6):1347-54.\n\nMultimodal machine learning: a survey and taxonomy. T Baltru\u0161aitis, C Ahuja, L-P Morency, arXiv:1705.09406arXiv e-printsBaltru\u0161aitis T, Ahuja C, Morency L-P. Multimodal machine learning: a survey and taxonomy. arXiv e-prints; 2017. arXiv:1705.09406.\n\nHow to establish clinical prediction models. Y H Lee, H Bang, D J Kim, Endocrinol Metab (Seoul). 311Lee YH, Bang H, Kim DJ. How to establish clinical prediction models. Endocrinol Metab (Seoul). 2016;31(1):38-44.\n\nRadiomics: the bridge between medical imaging and personalized medicine. P Lambin, Rth Leijenaar, T M Deist, J Peerlings, Eec De Jong, J Van Timmeren, S Sanduleanu, R Larue, Ajg Even, A Jochems, Nat Rev Clin Oncol. 1412Lambin P, Leijenaar RTH, Deist TM, Peerlings J, de Jong EEC, van Timme- ren J, Sanduleanu S, Larue R, Even AJG, Jochems A, et al. Radiomics: the bridge between medical imaging and personalized medicine. Nat Rev Clin Oncol. 2017;14(12):749-62.\n\nPublisher's Note. Publisher's Note\n\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Springer Nature remains neutral with regard to jurisdictional claims in pub- lished maps and institutional affiliations.\n", "annotations": {"author": "[{\"end\":74,\"start\":59},{\"end\":86,\"start\":75},{\"end\":97,\"start\":87},{\"end\":110,\"start\":98},{\"end\":123,\"start\":111},{\"end\":133,\"start\":124},{\"end\":144,\"start\":134},{\"end\":157,\"start\":145},{\"end\":168,\"start\":158},{\"end\":180,\"start\":169}]", "publisher": null, "author_last_name": "[{\"end\":73,\"start\":68},{\"end\":85,\"start\":81},{\"end\":96,\"start\":91},{\"end\":109,\"start\":105},{\"end\":122,\"start\":118},{\"end\":132,\"start\":128},{\"end\":143,\"start\":139},{\"end\":156,\"start\":153},{\"end\":167,\"start\":165},{\"end\":179,\"start\":177}]", "author_first_name": "[{\"end\":67,\"start\":59},{\"end\":80,\"start\":75},{\"end\":90,\"start\":87},{\"end\":104,\"start\":98},{\"end\":117,\"start\":111},{\"end\":127,\"start\":124},{\"end\":138,\"start\":134},{\"end\":152,\"start\":145},{\"end\":164,\"start\":158},{\"end\":176,\"start\":169}]", "author_affiliation": null, "title": "[{\"end\":56,\"start\":1},{\"end\":236,\"start\":181}]", "venue": null, "abstract": "[{\"end\":3159,\"start\":1085}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3319,\"start\":3316},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3321,\"start\":3319},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3461,\"start\":3458},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3464,\"start\":3461},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3467,\"start\":3464},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3683,\"start\":3680},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3686,\"start\":3683},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3689,\"start\":3686},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3956,\"start\":3953},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4162,\"start\":4159},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4478,\"start\":4475},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4857,\"start\":4854},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5062,\"start\":5059},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5065,\"start\":5062},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5271,\"start\":5267},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5373,\"start\":5369},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5473,\"start\":5469},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5661,\"start\":5657},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5665,\"start\":5661},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5669,\"start\":5665},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":5673,\"start\":5669},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":5877,\"start\":5873},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5933,\"start\":5929},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6267,\"start\":6263},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8991,\"start\":8987},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9089,\"start\":9085},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9277,\"start\":9273},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9375,\"start\":9371},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9499,\"start\":9495},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9705,\"start\":9701},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10152,\"start\":10148},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":10658,\"start\":10654},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":12834,\"start\":12832},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12973,\"start\":12969},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":14920,\"start\":14918},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16749,\"start\":16745},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":17070,\"start\":17066},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":18202,\"start\":18198},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":18408,\"start\":18404},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":18411,\"start\":18408}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":20530,\"start\":20434},{\"attributes\":{\"id\":\"fig_1\"},\"end\":21270,\"start\":20531},{\"attributes\":{\"id\":\"fig_2\"},\"end\":21990,\"start\":21271},{\"attributes\":{\"id\":\"fig_3\"},\"end\":22651,\"start\":21991},{\"attributes\":{\"id\":\"fig_4\"},\"end\":22971,\"start\":22652},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":23514,\"start\":22972},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":23980,\"start\":23515},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":24867,\"start\":23981},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":26080,\"start\":24868},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":26567,\"start\":26081},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":28239,\"start\":26568}]", "paragraph": "[{\"end\":4693,\"start\":3173},{\"end\":6006,\"start\":4695},{\"end\":6693,\"start\":6008},{\"end\":8193,\"start\":6715},{\"end\":8913,\"start\":8216},{\"end\":9854,\"start\":8968},{\"end\":10764,\"start\":9856},{\"end\":11195,\"start\":10799},{\"end\":11466,\"start\":11197},{\"end\":11733,\"start\":11468},{\"end\":12322,\"start\":11735},{\"end\":12410,\"start\":12324},{\"end\":12599,\"start\":12422},{\"end\":12739,\"start\":12646},{\"end\":13716,\"start\":12777},{\"end\":14391,\"start\":13753},{\"end\":14840,\"start\":14474},{\"end\":14997,\"start\":14855},{\"end\":16128,\"start\":14999},{\"end\":16835,\"start\":16545},{\"end\":18203,\"start\":16837},{\"end\":18828,\"start\":18205},{\"end\":19295,\"start\":18830},{\"end\":20031,\"start\":19297},{\"end\":20433,\"start\":20047}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12645,\"start\":12600},{\"attributes\":{\"id\":\"formula_1\"},\"end\":16544,\"start\":16129}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":7870,\"start\":7863},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":10565,\"start\":10557},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":14312,\"start\":14305}]", "section_header": "[{\"end\":3171,\"start\":3161},{\"end\":6703,\"start\":6696},{\"end\":6713,\"start\":6706},{\"end\":8214,\"start\":8196},{\"end\":8966,\"start\":8916},{\"end\":10797,\"start\":10767},{\"end\":12420,\"start\":12413},{\"end\":12775,\"start\":12742},{\"end\":13751,\"start\":13719},{\"end\":14472,\"start\":14394},{\"end\":14853,\"start\":14843},{\"end\":20045,\"start\":20034},{\"end\":20545,\"start\":20532},{\"end\":21285,\"start\":21272},{\"end\":22005,\"start\":21992},{\"end\":22659,\"start\":22653},{\"end\":22980,\"start\":22973},{\"end\":24876,\"start\":24869},{\"end\":26089,\"start\":26082},{\"end\":26576,\"start\":26569}]", "table": "[{\"end\":23514,\"start\":23183},{\"end\":23980,\"start\":23759},{\"end\":24867,\"start\":24633},{\"end\":26080,\"start\":25180},{\"end\":26567,\"start\":26194},{\"end\":28239,\"start\":26892}]", "figure_caption": "[{\"end\":20530,\"start\":20436},{\"end\":21270,\"start\":20548},{\"end\":21990,\"start\":21288},{\"end\":22651,\"start\":22008},{\"end\":22971,\"start\":22661},{\"end\":23183,\"start\":22982},{\"end\":23759,\"start\":23517},{\"end\":24633,\"start\":23983},{\"end\":25180,\"start\":24878},{\"end\":26194,\"start\":26091},{\"end\":26892,\"start\":26578}]", "figure_ref": "[{\"end\":8711,\"start\":8705},{\"end\":10497,\"start\":10491},{\"end\":12511,\"start\":12505},{\"end\":13152,\"start\":13146},{\"end\":13650,\"start\":13644},{\"end\":13715,\"start\":13709},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":14390,\"start\":14384},{\"end\":14706,\"start\":14697},{\"end\":14839,\"start\":14830}]", "bib_author_first_name": "[{\"end\":30487,\"start\":30486},{\"end\":30489,\"start\":30488},{\"end\":30722,\"start\":30721},{\"end\":30739,\"start\":30738},{\"end\":30962,\"start\":30961},{\"end\":30964,\"start\":30963},{\"end\":30972,\"start\":30971},{\"end\":30974,\"start\":30973},{\"end\":30982,\"start\":30981},{\"end\":30984,\"start\":30983},{\"end\":31296,\"start\":31295},{\"end\":31307,\"start\":31306},{\"end\":31314,\"start\":31313},{\"end\":31328,\"start\":31327},{\"end\":31337,\"start\":31336},{\"end\":31346,\"start\":31345},{\"end\":31356,\"start\":31355},{\"end\":31763,\"start\":31762},{\"end\":31765,\"start\":31764},{\"end\":31775,\"start\":31774},{\"end\":31777,\"start\":31776},{\"end\":31787,\"start\":31786},{\"end\":31798,\"start\":31797},{\"end\":31809,\"start\":31808},{\"end\":31811,\"start\":31810},{\"end\":31821,\"start\":31820},{\"end\":31835,\"start\":31834},{\"end\":31837,\"start\":31836},{\"end\":31844,\"start\":31843},{\"end\":31846,\"start\":31845},{\"end\":31854,\"start\":31853},{\"end\":31856,\"start\":31855},{\"end\":32469,\"start\":32468},{\"end\":32483,\"start\":32482},{\"end\":32497,\"start\":32496},{\"end\":32507,\"start\":32506},{\"end\":32758,\"start\":32757},{\"end\":32767,\"start\":32766},{\"end\":32777,\"start\":32776},{\"end\":32968,\"start\":32967},{\"end\":32982,\"start\":32981},{\"end\":32984,\"start\":32983},{\"end\":32995,\"start\":32994},{\"end\":33331,\"start\":33330},{\"end\":33338,\"start\":33337},{\"end\":33340,\"start\":33339},{\"end\":33347,\"start\":33346},{\"end\":33349,\"start\":33348},{\"end\":33357,\"start\":33356},{\"end\":33359,\"start\":33358},{\"end\":33366,\"start\":33365},{\"end\":33368,\"start\":33367},{\"end\":33376,\"start\":33375},{\"end\":33579,\"start\":33578},{\"end\":33588,\"start\":33587},{\"end\":33590,\"start\":33589},{\"end\":33881,\"start\":33880},{\"end\":33891,\"start\":33890},{\"end\":33901,\"start\":33900},{\"end\":33903,\"start\":33902},{\"end\":33912,\"start\":33911},{\"end\":33918,\"start\":33917},{\"end\":33920,\"start\":33919},{\"end\":33931,\"start\":33930},{\"end\":33933,\"start\":33932},{\"end\":33941,\"start\":33940},{\"end\":34220,\"start\":34219},{\"end\":34227,\"start\":34226},{\"end\":34235,\"start\":34234},{\"end\":34243,\"start\":34242},{\"end\":34250,\"start\":34249},{\"end\":34256,\"start\":34255},{\"end\":34264,\"start\":34263},{\"end\":34270,\"start\":34269},{\"end\":34277,\"start\":34276},{\"end\":34283,\"start\":34282},{\"end\":34591,\"start\":34590},{\"end\":34600,\"start\":34599},{\"end\":34610,\"start\":34609},{\"end\":34619,\"start\":34618},{\"end\":34629,\"start\":34628},{\"end\":34638,\"start\":34637},{\"end\":34651,\"start\":34650},{\"end\":34663,\"start\":34662},{\"end\":35005,\"start\":35004},{\"end\":35007,\"start\":35006},{\"end\":35014,\"start\":35013},{\"end\":35016,\"start\":35015},{\"end\":35024,\"start\":35023},{\"end\":35026,\"start\":35025},{\"end\":35034,\"start\":35033},{\"end\":35036,\"start\":35035},{\"end\":35044,\"start\":35043},{\"end\":35046,\"start\":35045},{\"end\":35053,\"start\":35052},{\"end\":35060,\"start\":35059},{\"end\":35062,\"start\":35061},{\"end\":35070,\"start\":35069},{\"end\":35072,\"start\":35071},{\"end\":35080,\"start\":35079},{\"end\":35082,\"start\":35081},{\"end\":35089,\"start\":35088},{\"end\":35091,\"start\":35090},{\"end\":35474,\"start\":35473},{\"end\":35484,\"start\":35483},{\"end\":35493,\"start\":35492},{\"end\":35505,\"start\":35504},{\"end\":35515,\"start\":35514},{\"end\":35528,\"start\":35527},{\"end\":35539,\"start\":35538},{\"end\":35550,\"start\":35549},{\"end\":35560,\"start\":35559},{\"end\":35568,\"start\":35567},{\"end\":35998,\"start\":35997},{\"end\":36008,\"start\":36007},{\"end\":36020,\"start\":36019},{\"end\":36356,\"start\":36355},{\"end\":36365,\"start\":36364},{\"end\":36674,\"start\":36673},{\"end\":36683,\"start\":36682},{\"end\":36692,\"start\":36691},{\"end\":36694,\"start\":36693},{\"end\":36704,\"start\":36703},{\"end\":37004,\"start\":37003},{\"end\":37010,\"start\":37009},{\"end\":37305,\"start\":37304},{\"end\":37317,\"start\":37316},{\"end\":37481,\"start\":37480},{\"end\":37488,\"start\":37487},{\"end\":37495,\"start\":37494},{\"end\":37503,\"start\":37502},{\"end\":37512,\"start\":37511},{\"end\":37520,\"start\":37519},{\"end\":37696,\"start\":37695},{\"end\":37711,\"start\":37710},{\"end\":37719,\"start\":37718},{\"end\":37725,\"start\":37724},{\"end\":37735,\"start\":37734},{\"end\":37747,\"start\":37746},{\"end\":37753,\"start\":37752},{\"end\":37762,\"start\":37761},{\"end\":37774,\"start\":37773},{\"end\":37784,\"start\":37783},{\"end\":38142,\"start\":38141},{\"end\":38156,\"start\":38155},{\"end\":38169,\"start\":38168},{\"end\":38171,\"start\":38170},{\"end\":38424,\"start\":38423},{\"end\":38433,\"start\":38432},{\"end\":38601,\"start\":38600},{\"end\":38731,\"start\":38730},{\"end\":38749,\"start\":38748},{\"end\":38959,\"start\":38958},{\"end\":38966,\"start\":38965},{\"end\":38974,\"start\":38973},{\"end\":38984,\"start\":38983},{\"end\":38993,\"start\":38992},{\"end\":39314,\"start\":39313},{\"end\":39325,\"start\":39324},{\"end\":39341,\"start\":39340},{\"end\":39355,\"start\":39354},{\"end\":39368,\"start\":39367},{\"end\":39379,\"start\":39378},{\"end\":39394,\"start\":39393},{\"end\":39404,\"start\":39403},{\"end\":39757,\"start\":39756},{\"end\":39773,\"start\":39772},{\"end\":39784,\"start\":39781},{\"end\":40001,\"start\":40000},{\"end\":40003,\"start\":40002},{\"end\":40010,\"start\":40009},{\"end\":40018,\"start\":40017},{\"end\":40020,\"start\":40019},{\"end\":40243,\"start\":40242},{\"end\":40255,\"start\":40252},{\"end\":40268,\"start\":40267},{\"end\":40270,\"start\":40269},{\"end\":40279,\"start\":40278},{\"end\":40294,\"start\":40291},{\"end\":40305,\"start\":40304},{\"end\":40321,\"start\":40320},{\"end\":40335,\"start\":40334},{\"end\":40346,\"start\":40343},{\"end\":40354,\"start\":40353}]", "bib_author_last_name": "[{\"end\":30495,\"start\":30490},{\"end\":30736,\"start\":30723},{\"end\":30744,\"start\":30740},{\"end\":30969,\"start\":30965},{\"end\":30979,\"start\":30975},{\"end\":30998,\"start\":30985},{\"end\":31304,\"start\":31297},{\"end\":31311,\"start\":31308},{\"end\":31325,\"start\":31315},{\"end\":31334,\"start\":31329},{\"end\":31343,\"start\":31338},{\"end\":31353,\"start\":31347},{\"end\":31361,\"start\":31357},{\"end\":31772,\"start\":31766},{\"end\":31784,\"start\":31778},{\"end\":31795,\"start\":31788},{\"end\":31806,\"start\":31799},{\"end\":31818,\"start\":31812},{\"end\":31832,\"start\":31822},{\"end\":31841,\"start\":31838},{\"end\":31851,\"start\":31847},{\"end\":31862,\"start\":31857},{\"end\":32480,\"start\":32470},{\"end\":32494,\"start\":32484},{\"end\":32504,\"start\":32498},{\"end\":32514,\"start\":32508},{\"end\":32764,\"start\":32759},{\"end\":32774,\"start\":32768},{\"end\":32784,\"start\":32778},{\"end\":32979,\"start\":32969},{\"end\":32992,\"start\":32985},{\"end\":33002,\"start\":32996},{\"end\":33335,\"start\":33332},{\"end\":33344,\"start\":33341},{\"end\":33354,\"start\":33350},{\"end\":33363,\"start\":33360},{\"end\":33373,\"start\":33369},{\"end\":33380,\"start\":33377},{\"end\":33585,\"start\":33580},{\"end\":33597,\"start\":33591},{\"end\":33888,\"start\":33882},{\"end\":33898,\"start\":33892},{\"end\":33909,\"start\":33904},{\"end\":33915,\"start\":33913},{\"end\":33928,\"start\":33921},{\"end\":33938,\"start\":33934},{\"end\":33947,\"start\":33942},{\"end\":34224,\"start\":34221},{\"end\":34232,\"start\":34228},{\"end\":34240,\"start\":34236},{\"end\":34247,\"start\":34244},{\"end\":34253,\"start\":34251},{\"end\":34261,\"start\":34257},{\"end\":34267,\"start\":34265},{\"end\":34274,\"start\":34271},{\"end\":34280,\"start\":34278},{\"end\":34288,\"start\":34284},{\"end\":34597,\"start\":34592},{\"end\":34607,\"start\":34601},{\"end\":34616,\"start\":34611},{\"end\":34626,\"start\":34620},{\"end\":34635,\"start\":34630},{\"end\":34648,\"start\":34639},{\"end\":34660,\"start\":34652},{\"end\":34673,\"start\":34664},{\"end\":35011,\"start\":35008},{\"end\":35021,\"start\":35017},{\"end\":35031,\"start\":35027},{\"end\":35041,\"start\":35037},{\"end\":35050,\"start\":35047},{\"end\":35057,\"start\":35054},{\"end\":35067,\"start\":35063},{\"end\":35077,\"start\":35073},{\"end\":35086,\"start\":35083},{\"end\":35096,\"start\":35092},{\"end\":35481,\"start\":35475},{\"end\":35490,\"start\":35485},{\"end\":35502,\"start\":35494},{\"end\":35512,\"start\":35506},{\"end\":35525,\"start\":35516},{\"end\":35536,\"start\":35529},{\"end\":35547,\"start\":35540},{\"end\":35557,\"start\":35551},{\"end\":35565,\"start\":35561},{\"end\":35576,\"start\":35569},{\"end\":36005,\"start\":35999},{\"end\":36017,\"start\":36009},{\"end\":36027,\"start\":36021},{\"end\":36362,\"start\":36357},{\"end\":36373,\"start\":36366},{\"end\":36680,\"start\":36675},{\"end\":36689,\"start\":36684},{\"end\":36701,\"start\":36695},{\"end\":36712,\"start\":36705},{\"end\":37007,\"start\":37005},{\"end\":37015,\"start\":37011},{\"end\":37314,\"start\":37306},{\"end\":37327,\"start\":37318},{\"end\":37485,\"start\":37482},{\"end\":37492,\"start\":37489},{\"end\":37500,\"start\":37496},{\"end\":37509,\"start\":37504},{\"end\":37517,\"start\":37513},{\"end\":37524,\"start\":37521},{\"end\":37708,\"start\":37697},{\"end\":37716,\"start\":37712},{\"end\":37722,\"start\":37720},{\"end\":37732,\"start\":37726},{\"end\":37744,\"start\":37736},{\"end\":37750,\"start\":37748},{\"end\":37759,\"start\":37754},{\"end\":37771,\"start\":37763},{\"end\":37781,\"start\":37775},{\"end\":37794,\"start\":37785},{\"end\":38153,\"start\":38143},{\"end\":38166,\"start\":38157},{\"end\":38178,\"start\":38172},{\"end\":38430,\"start\":38425},{\"end\":38441,\"start\":38434},{\"end\":38609,\"start\":38602},{\"end\":38746,\"start\":38732},{\"end\":38756,\"start\":38750},{\"end\":38963,\"start\":38960},{\"end\":38971,\"start\":38967},{\"end\":38981,\"start\":38975},{\"end\":38990,\"start\":38985},{\"end\":39002,\"start\":38994},{\"end\":39322,\"start\":39315},{\"end\":39338,\"start\":39326},{\"end\":39352,\"start\":39342},{\"end\":39365,\"start\":39356},{\"end\":39376,\"start\":39369},{\"end\":39391,\"start\":39380},{\"end\":39401,\"start\":39395},{\"end\":39417,\"start\":39405},{\"end\":39770,\"start\":39758},{\"end\":39779,\"start\":39774},{\"end\":39792,\"start\":39785},{\"end\":40007,\"start\":40004},{\"end\":40015,\"start\":40011},{\"end\":40024,\"start\":40021},{\"end\":40250,\"start\":40244},{\"end\":40265,\"start\":40256},{\"end\":40276,\"start\":40271},{\"end\":40289,\"start\":40280},{\"end\":40302,\"start\":40295},{\"end\":40318,\"start\":40306},{\"end\":40332,\"start\":40322},{\"end\":40341,\"start\":40336},{\"end\":40351,\"start\":40347},{\"end\":40362,\"start\":40355}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":8062643},\"end\":30663,\"start\":30411},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":4040002},\"end\":30872,\"start\":30665},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":49212875},\"end\":31192,\"start\":30874},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":9737963},\"end\":31590,\"start\":31194},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":19508525},\"end\":32241,\"start\":31592},{\"attributes\":{\"id\":\"b5\"},\"end\":32376,\"start\":32243},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":4264874},\"end\":32740,\"start\":32378},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":1779661},\"end\":32875,\"start\":32742},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":195756451},\"end\":33216,\"start\":32877},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":199465657},\"end\":33576,\"start\":33218},{\"attributes\":{\"doi\":\"arXiv:1803.08834\",\"id\":\"b10\"},\"end\":33801,\"start\":33578},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":3767412},\"end\":34138,\"start\":33803},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":211158783},\"end\":34495,\"start\":34140},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":208744246},\"end\":34897,\"start\":34497},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":201631142},\"end\":35340,\"start\":34899},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":44091232},\"end\":35873,\"start\":35342},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":207903067},\"end\":36233,\"start\":35875},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":49709885},\"end\":36583,\"start\":36235},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":53099845},\"end\":36924,\"start\":36585},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":40017272},\"end\":37219,\"start\":36926},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b20\"},\"end\":37476,\"start\":37221},{\"attributes\":{\"id\":\"b21\"},\"end\":37691,\"start\":37478},{\"attributes\":{\"doi\":\"arXiv:1409.0575\",\"id\":\"b22\"},\"end\":38074,\"start\":37693},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":195908774},\"end\":38327,\"start\":38076},{\"attributes\":{\"doi\":\"arXiv:1502.03167\",\"id\":\"b24\"},\"end\":38598,\"start\":38329},{\"attributes\":{\"id\":\"b25\"},\"end\":38698,\"start\":38600},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":5855042},\"end\":38875,\"start\":38700},{\"attributes\":{\"doi\":\"arXiv:1704.05796\",\"id\":\"b27\"},\"end\":39202,\"start\":38877},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":27541832},\"end\":39702,\"start\":39204},{\"attributes\":{\"doi\":\"arXiv:1705.09406\",\"id\":\"b29\"},\"end\":39953,\"start\":39704},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":721815},\"end\":40167,\"start\":39955},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":21218696},\"end\":40630,\"start\":40169},{\"attributes\":{\"id\":\"b32\"},\"end\":40666,\"start\":40632},{\"attributes\":{\"id\":\"b33\"},\"end\":40907,\"start\":40668}]", "bib_title": "[{\"end\":30484,\"start\":30411},{\"end\":30719,\"start\":30665},{\"end\":30959,\"start\":30874},{\"end\":31293,\"start\":31194},{\"end\":31760,\"start\":31592},{\"end\":32466,\"start\":32378},{\"end\":32755,\"start\":32742},{\"end\":32965,\"start\":32877},{\"end\":33328,\"start\":33218},{\"end\":33878,\"start\":33803},{\"end\":34217,\"start\":34140},{\"end\":34588,\"start\":34497},{\"end\":35002,\"start\":34899},{\"end\":35471,\"start\":35342},{\"end\":35995,\"start\":35875},{\"end\":36353,\"start\":36235},{\"end\":36671,\"start\":36585},{\"end\":37001,\"start\":36926},{\"end\":38139,\"start\":38076},{\"end\":38728,\"start\":38700},{\"end\":39311,\"start\":39204},{\"end\":39998,\"start\":39955},{\"end\":40240,\"start\":40169}]", "bib_author": "[{\"end\":30497,\"start\":30486},{\"end\":30738,\"start\":30721},{\"end\":30746,\"start\":30738},{\"end\":30971,\"start\":30961},{\"end\":30981,\"start\":30971},{\"end\":31000,\"start\":30981},{\"end\":31306,\"start\":31295},{\"end\":31313,\"start\":31306},{\"end\":31327,\"start\":31313},{\"end\":31336,\"start\":31327},{\"end\":31345,\"start\":31336},{\"end\":31355,\"start\":31345},{\"end\":31363,\"start\":31355},{\"end\":31774,\"start\":31762},{\"end\":31786,\"start\":31774},{\"end\":31797,\"start\":31786},{\"end\":31808,\"start\":31797},{\"end\":31820,\"start\":31808},{\"end\":31834,\"start\":31820},{\"end\":31843,\"start\":31834},{\"end\":31853,\"start\":31843},{\"end\":31864,\"start\":31853},{\"end\":32482,\"start\":32468},{\"end\":32496,\"start\":32482},{\"end\":32506,\"start\":32496},{\"end\":32516,\"start\":32506},{\"end\":32766,\"start\":32757},{\"end\":32776,\"start\":32766},{\"end\":32786,\"start\":32776},{\"end\":32981,\"start\":32967},{\"end\":32994,\"start\":32981},{\"end\":33004,\"start\":32994},{\"end\":33337,\"start\":33330},{\"end\":33346,\"start\":33337},{\"end\":33356,\"start\":33346},{\"end\":33365,\"start\":33356},{\"end\":33375,\"start\":33365},{\"end\":33382,\"start\":33375},{\"end\":33587,\"start\":33578},{\"end\":33599,\"start\":33587},{\"end\":33890,\"start\":33880},{\"end\":33900,\"start\":33890},{\"end\":33911,\"start\":33900},{\"end\":33917,\"start\":33911},{\"end\":33930,\"start\":33917},{\"end\":33940,\"start\":33930},{\"end\":33949,\"start\":33940},{\"end\":34226,\"start\":34219},{\"end\":34234,\"start\":34226},{\"end\":34242,\"start\":34234},{\"end\":34249,\"start\":34242},{\"end\":34255,\"start\":34249},{\"end\":34263,\"start\":34255},{\"end\":34269,\"start\":34263},{\"end\":34276,\"start\":34269},{\"end\":34282,\"start\":34276},{\"end\":34290,\"start\":34282},{\"end\":34599,\"start\":34590},{\"end\":34609,\"start\":34599},{\"end\":34618,\"start\":34609},{\"end\":34628,\"start\":34618},{\"end\":34637,\"start\":34628},{\"end\":34650,\"start\":34637},{\"end\":34662,\"start\":34650},{\"end\":34675,\"start\":34662},{\"end\":35013,\"start\":35004},{\"end\":35023,\"start\":35013},{\"end\":35033,\"start\":35023},{\"end\":35043,\"start\":35033},{\"end\":35052,\"start\":35043},{\"end\":35059,\"start\":35052},{\"end\":35069,\"start\":35059},{\"end\":35079,\"start\":35069},{\"end\":35088,\"start\":35079},{\"end\":35098,\"start\":35088},{\"end\":35483,\"start\":35473},{\"end\":35492,\"start\":35483},{\"end\":35504,\"start\":35492},{\"end\":35514,\"start\":35504},{\"end\":35527,\"start\":35514},{\"end\":35538,\"start\":35527},{\"end\":35549,\"start\":35538},{\"end\":35559,\"start\":35549},{\"end\":35567,\"start\":35559},{\"end\":35578,\"start\":35567},{\"end\":36007,\"start\":35997},{\"end\":36019,\"start\":36007},{\"end\":36029,\"start\":36019},{\"end\":36364,\"start\":36355},{\"end\":36375,\"start\":36364},{\"end\":36682,\"start\":36673},{\"end\":36691,\"start\":36682},{\"end\":36703,\"start\":36691},{\"end\":36714,\"start\":36703},{\"end\":37009,\"start\":37003},{\"end\":37017,\"start\":37009},{\"end\":37316,\"start\":37304},{\"end\":37329,\"start\":37316},{\"end\":37487,\"start\":37480},{\"end\":37494,\"start\":37487},{\"end\":37502,\"start\":37494},{\"end\":37511,\"start\":37502},{\"end\":37519,\"start\":37511},{\"end\":37526,\"start\":37519},{\"end\":37710,\"start\":37695},{\"end\":37718,\"start\":37710},{\"end\":37724,\"start\":37718},{\"end\":37734,\"start\":37724},{\"end\":37746,\"start\":37734},{\"end\":37752,\"start\":37746},{\"end\":37761,\"start\":37752},{\"end\":37773,\"start\":37761},{\"end\":37783,\"start\":37773},{\"end\":37796,\"start\":37783},{\"end\":38155,\"start\":38141},{\"end\":38168,\"start\":38155},{\"end\":38180,\"start\":38168},{\"end\":38432,\"start\":38423},{\"end\":38443,\"start\":38432},{\"end\":38611,\"start\":38600},{\"end\":38748,\"start\":38730},{\"end\":38758,\"start\":38748},{\"end\":38965,\"start\":38958},{\"end\":38973,\"start\":38965},{\"end\":38983,\"start\":38973},{\"end\":38992,\"start\":38983},{\"end\":39004,\"start\":38992},{\"end\":39324,\"start\":39313},{\"end\":39340,\"start\":39324},{\"end\":39354,\"start\":39340},{\"end\":39367,\"start\":39354},{\"end\":39378,\"start\":39367},{\"end\":39393,\"start\":39378},{\"end\":39403,\"start\":39393},{\"end\":39419,\"start\":39403},{\"end\":39772,\"start\":39756},{\"end\":39781,\"start\":39772},{\"end\":39794,\"start\":39781},{\"end\":40009,\"start\":40000},{\"end\":40017,\"start\":40009},{\"end\":40026,\"start\":40017},{\"end\":40252,\"start\":40242},{\"end\":40267,\"start\":40252},{\"end\":40278,\"start\":40267},{\"end\":40291,\"start\":40278},{\"end\":40304,\"start\":40291},{\"end\":40320,\"start\":40304},{\"end\":40334,\"start\":40320},{\"end\":40343,\"start\":40334},{\"end\":40353,\"start\":40343},{\"end\":40364,\"start\":40353}]", "bib_venue": "[{\"end\":30525,\"start\":30497},{\"end\":30758,\"start\":30746},{\"end\":31021,\"start\":31000},{\"end\":31381,\"start\":31363},{\"end\":31892,\"start\":31864},{\"end\":32271,\"start\":32245},{\"end\":32548,\"start\":32516},{\"end\":32792,\"start\":32786},{\"end\":33035,\"start\":33004},{\"end\":33387,\"start\":33382},{\"end\":33685,\"start\":33615},{\"end\":33955,\"start\":33949},{\"end\":34302,\"start\":34290},{\"end\":34685,\"start\":34675},{\"end\":35107,\"start\":35098},{\"end\":35586,\"start\":35578},{\"end\":36043,\"start\":36029},{\"end\":36396,\"start\":36375},{\"end\":36745,\"start\":36714},{\"end\":37044,\"start\":37017},{\"end\":37302,\"start\":37221},{\"end\":37576,\"start\":37526},{\"end\":38190,\"start\":38180},{\"end\":38421,\"start\":38329},{\"end\":38636,\"start\":38611},{\"end\":38774,\"start\":38758},{\"end\":38956,\"start\":38877},{\"end\":39438,\"start\":39419},{\"end\":39754,\"start\":39704},{\"end\":40050,\"start\":40026},{\"end\":40382,\"start\":40364},{\"end\":40648,\"start\":40632},{\"end\":40785,\"start\":40668}]"}}}, "year": 2023, "month": 12, "day": 17}