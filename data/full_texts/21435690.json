{"id": 21435690, "updated": "2023-09-29 02:57:03.078", "metadata": {"title": "Matterport3D: Learning from RGB-D Data in Indoor Environments", "authors": "[{\"first\":\"Angel\",\"last\":\"Chang\",\"middle\":[]},{\"first\":\"Angela\",\"last\":\"Dai\",\"middle\":[]},{\"first\":\"Thomas\",\"last\":\"Funkhouser\",\"middle\":[]},{\"first\":\"Maciej\",\"last\":\"Halber\",\"middle\":[]},{\"first\":\"Matthias\",\"last\":\"Niessner\",\"middle\":[]},{\"first\":\"Manolis\",\"last\":\"Savva\",\"middle\":[]},{\"first\":\"Shuran\",\"last\":\"Song\",\"middle\":[]},{\"first\":\"Andy\",\"last\":\"Zeng\",\"middle\":[]},{\"first\":\"Yinda\",\"last\":\"Zhang\",\"middle\":[]}]", "venue": "2017 International Conference on 3D Vision (3DV)", "journal": "2017 International Conference on 3D Vision (3DV)", "publication_date": {"year": 2017, "month": 9, "day": 18}, "abstract": "Access to large, diverse RGB-D datasets is critical for training RGB-D scene understanding algorithms. However, existing datasets still cover only a limited number of views or a restricted scale of spaces. In this paper, we introduce Matterport3D, a large-scale RGB-D dataset containing 10,800 panoramic views from 194,400 RGB-D images of 90 building-scale scenes. Annotations are provided with surface reconstructions, camera poses, and 2D and 3D semantic segmentations. The precise global alignment and comprehensive, diverse panoramic set of views over entire buildings enable a variety of supervised and self-supervised computer vision tasks, including keypoint matching, view overlap prediction, normal prediction from color, semantic segmentation, and region classification.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1709.06158", "mag": "2964339842", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/3dim/ChangDFHNSSZZ17", "doi": "10.1109/3dv.2017.00081"}}, "content": {"source": {"pdf_hash": "8f403457ac94729138edbbac6ca5f4107219e483", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1709.06158v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1709.06158", "status": "GREEN"}}, "grobid": {"id": "5b7f1bbe0dc30af1ef4592d525f269d1a64a5725", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/8f403457ac94729138edbbac6ca5f4107219e483.txt", "contents": "\nMatterport3D: Learning from RGB-D Data in Indoor Environments\n\n\nAngel Chang \nPrinceton University\n\n\nAngela Dai \nStanford University\n\n\nThomas Funkhouser \nPrinceton University\n\n\nMaciej Halber \nPrinceton University\n\n\nMatthias Nie\u00dfner \nTechnical University of Munich\n\n\nManolis Savva \nPrinceton University\n\n\nShuran Song \nPrinceton University\n\n\nAndy Zeng \nPrinceton University\n\n\nYinda Zhang \nPrinceton University\n\n\nMatterport3D: Learning from RGB-D Data in Indoor Environments\n\nAccess to large, diverse RGB-D datasets is critical for training RGB-D scene understanding algorithms. However, existing datasets still cover only a limited number of views or a restricted scale of spaces. In this paper, we introduce Matterport3D, a large-scale RGB-D dataset containing 10,800 panoramic views from 194,400 RGB-D images of 90 building-scale scenes. Annotations are provided with surface reconstructions, camera poses, and 2D and 3D semantic segmentations. The precise global alignment and comprehensive, diverse panoramic set of views over entire buildings enable a variety of supervised and self-supervised computer vision tasks, including keypoint matching, view overlap prediction, normal prediction from color, semantic segmentation, and region classification.\n\nIntroduction\n\nScene understanding for RGB-D images of indoor home environments is a fundamental task for many applications of computer vision, including personal robotics, augmented reality, scene modeling, and perception assistance.\n\nAlthough there has been impressive research progress on this topic, a significant limitation is the availability suitable RGB-D datasets from which models can be trained. As with other computer vision tasks, the performance of data-driven models exceeds that of hand-tuned models and depends directly on the quantity and quality of training datasets. Unfortunately, current RGB-D datasets have small numbers of images [33], limited scene coverage [17], limited viewpoints [35], and/or motion blurred imagery. Most are restricted to single rooms [20,7], synthetic imagery [17,38], and/or a relatively small number of office environments [1].\n\nNo previous dataset provides high-quality RGB-D images for a diverse set of views in interior home environments.\n\nThis paper introduces the Matterport3D dataset and investigates new research opportunities it provides for learning about indoor home environments. The dataset comprises a set of 194,400 RGB-D images captured in 10,800 * authors are in alphabetical order Figure 1: The Matterport3D dataset provides visual data covering 90 buildings, including HDR color images, depth images, panoramic skyboxes, textured meshes, region layouts and categories, and object semantic segmentations. panorama with a Matterport camera 1 in home environments. Unlike previous datasets, it includes both depth and color 360 \u2022 panoramas for each viewpoint, samples human-height viewpoints uniformly throughout the entire environment, provides camera poses that are globally consistent and aligned with a textured surface reconstruction, includes instance-level semantic segmentations into region and object categories, and provides data collected from living spaces in private homes.\n\nThough the curation of the dataset is interesting in its own, the most compelling part of the project is the computer vision tasks enabled by it. In this paper, we investigate 5 tasks, each leveraging different properties of the dataset. The precise global alignment over building scale allows training for state-of-the-art keypoint descriptors that can robustly match keypoints from drastically varying camera views. The panoramic and comprehensive viewpoint sampling provides a large number of loop closure instances, allowing learning of loop closure detection through predicting view overlap. The surface normals estimated from highquality depths in diverse scenes allows training models for normal estimation from color images that outperform previous ones. The globally consistent registration of images to a surface mesh facilitates semantic annotation, enabling efficient 3D interfaces for object and region category annotation from which labels projected into images can train deep networks for semantic segmentation. For each of these tasks, we provide baseline results using variants of existing state-of-the-art algorithms demonstrating the benefits of the Matterport3D data; we hope that Matterport3D will inspire future work on many scene understanding tasks 2 .\n\n\nBackground and Related Work\n\nCollecting and analyzing RGB-D imagery to train algorithms for scene understanding is an active area of research with great interest in computer vision, graphics, and robotics [10]. Existing work on curation of RGB-D datasets has focused mostly on scans of individual objects [5], standalone rooms [20,28], views of a room [32,35], spaces from academic buildings and small apartments [7], and small collections of rooms or public spaces [44,1,22]. Some of these datasets provide 3D surface reconstructions and object-level semantic annotations [20,2,28,7]. However, none have the scale, coverage, alignment accuracy, or HDR imagery of the dataset presented in this paper.\n\nThese previous RGB-D datasets have been used to train models for several standard scene understanding tasks, including semantic segmentation [27,33,13,40,7], 3D object detection [31,24,14,36,37], normal estimation [41,23,9,3], camera relocalization [30,39], and others [47,11,12]. We add to this body of work by investigating tasks enabled by our dataset, including learning an image patch descriptor, predicting image overlaps, estimating normals, semantic voxel labeling, and classifying images by region category.\n\nThe work most closely related to ours is by Armeni et al. [1]. They also utilize a 3D dataset collected with Matterport cameras for scene understanding. However, there are several important differences. First, their data is collected in only 3 distinct office buildings, whereas we have data from 90 distinct buildings with a variety of scene types including homes (mostly), offices, and churches. Second, their dataset contains only RGB images and a coarse surface mesh from which they generate a point cloud -we additionally provide the raw depth and HDR images collected by Matterport. Third, their semantic annotations cover only 13 object categories, half of which are structural building elementswe collect an open set of category labels which we reduce to 40 categories with good coverage of both building elements and objects. Finally, their algorithms focus only on tasks related to semantic parsing of buildings into spaces and elements, while we consider a wide range of tasks enabled by both supervised and self-supervised learning.\n\n\nThe Matterport3D Dataset\n\nThis paper introduces a new RGB-D dataset of buildingscale scenes, and describes a set of scene understanding tasks that can be trained and tested from it. We describe the data in this section, along with a discussion of how it differs from prior work.\n\n\nData Acquisition Process\n\nThe Matterport data acquisition process uses a tripodmounted camera rig with three color and three depth cameras pointing slightly up, horizontal, and slightly down. For each panorama, it rotates around the direction of gravity to 6 distinct orientations, stopping at each to acquire an HDR photo from each of the 3 RGB cameras. The 3 depth cameras acquire data continuously as the rig rotates, which is integrated to synthesize a 1280x1024 depth image aligned with each color image. The result for each panorama is 18 RGB-D images with nearly coincident centers of projection at approximately the height of a human observer.\n\nFor each environment in the dataset, an operator captures a set of panoramas uniformly spaced at approximately 2.5m throughout the entire walkable floor plan of the environment ( Figure 2). The user tags windows and mirrors with an iPad app and uploads the data to Matterport. Matterport then processes the raw data by: 1) stitching the images within each panorama into a \"skybox\" suitable for panoramic viewing, 2) estimating the 6 DoF pose for each image with global The result of this process for each scene is a set of RGB-D images at 1280x1024 (with color in HDR) with a 6 DoF camera pose estimate for each, plus a skybox for each group of 18 images in the same panorama, and a textured mesh for the entire scene. In all, the dataset includes 90 buildings containing a total of 194,400 RGB-D images, 10,800 panorama, and 24,727,520 textured triangles; we provide textured mesh reconstructions obtained with [21] and [25].\n\n\nSemantic Annotation\n\nWe collect instance-level semantic annotations in 3D by first creating a floor plan annotation for each house, extracting room-like regions from the floor plan, and then using a crowdsourced painting interface to annotate object instances within each region.\n\nThe first step of our semantic annotation process is to break down each building into region components by specifying the 3D spatial extent and semantic category label for each room-like region. Annotators use a simple interactive tool in which the annotator selects a category and draws a 2D polygon on the floor for each region (see Figure 3). The tool then snaps the polygon to fit planar surfaces (walls and floor) and extrudes it to fit the ceiling.\n\nThe second step is to label 3D surfaces on objects in each region. To do that, we extract a mesh for each region using screened Poisson surface reconstruction [6]. Then, we use the ScanNet crowd-sourcing interface by Dai et al. [7] to \"paint\" triangles to segment and name all object instances within each region. We first collect an initial set of labels on Amazon Mechanical Turk (AMT), which we complete, fix, and verify by ten expert annotators. We ensure high-quality label standards, as well as high annotation coverage.\n\nThe 3D segmentations contain a total of 50,811 object instance annotations. Since AMT workers are allowed to provide freeform text labels, there were 1,659 unique text labels, which we then post-processed to establish a canonical set of 40 object categories mapped to WordNet synsets. Figure 5 shows the distribution of objects by semantic category and Figure 4 shows some examples illustrated as colored meshes. \n\n\nProperties of the Dataset\n\nIn comparison to previous datasets, Matterport3D has unique properties that open up new research opportunities:\n\nRGB-D Panoramas. Previous panorama datasets have provided either no depths at all [42] or approximate depths synthesized from meshes [1]. Matterport3D contains aligned 1280x1024 color and depth images for 18 viewpoints covering approximately 3.75sr (the entire sphere except the north and south poles), along with \"skybox\" images reconstructed for outward looking views aligned with sides of a cube centered at the panorama center. These RGB-D panorama provide new opportunities for recognizing scene categories, estimating region layout, learning contextual relationships, and more (see Section 4.4).\n\nPrecise Global Alignment. Previous RGB-D datasets have provided limited data about global alignment of camera poses. Some datasets targeted at SLAM applications [8] provide tracked camera poses covering parts of rooms [30] or estimated camera poses for individual rooms [7], and Armeni et al. [2] provides globally-registered camera poses for 6 floors of 3 buildings. Ours provides global registered imagery covering all floors of 90 reconstructed buildings. Although we do not have ground-truth camera poses for the dataset and so cannot measure errors objectively, we subjectively estimate that the average registration error between corresponding surface points is 1cm or less (see Figure  6). There are some surface misalignments as large as 10cm or more, but they are rare and usually for pairs of images whose viewpoints are separated by several meters.\n\nComprehensive Viewpoint Sampling. Previous datasets have contained either a small set of images captured for views around \"photograph viewpoints\" [35] or a sequence of video images aimed at up-close scanning of surfaces [7]. Category   wall  objects  door  chair  window  ceiling  picture  floor  misc  lighting  cushion  table  cabinet  curtain  plant  shelving  sink  mirror  chest  towel  stairs  railing  column  counter  stool  bed  sofa  shower  appliances  toilet  tv  seating  clothes  fireplace  bathtub  beam  furniture   gym equip. blinds board 0K 5K 10K Instances  Ours contains panoramic images captured from a comprehensive, sparse sampling of viewpoint space. Panoramic images are spaced nearly uniformly with separations of 2.25m \u00b1 0.57m, and thus most plausible human viewpoints are within 1.13m of a panorama center. This comprehensive sampling of viewpoint space provides new opportunities for learning about scenes as seen from arbitrary viewpoints that may be encountered by robots or wearable sensors as they navigate through them (see Section 4.2).\n\nStationary Cameras. Most RGB-D image datasets have been captured mostly with hand-held video cameras and thus suffer from motion blur and other artifacts typical of real-time scanning; e.g., pose errors, color-to-depth misalignments, and often contain largely incomplete scenes with limited coverage. Our dataset contains high dynamic range (HDR) images acquired in static scenes from stationary cameras mounted on a tripod, and thus has no motion blur. This property provides new opportunities to study fine-scale features of imagery in scenes, for example to train very precise keypoint or boundary detectors. Multiple, Diverse Views of Each Surface. Previous RGB-D datasets have provided a limited range of views for each surface patch. Most have expressly attempted to cover each surface patch once, either to improve the efficiency of scene reconstruction or to reduce bias in scene understanding datasets. Ours provides multiple views of surface patches from a wide variety of angles and distances (see Figure 7). Each surface patch is observed by 11 cameras on average (see Figure 8). The overall range of depths for all pixels has mean 2.125m and standard deviation 1.4356m, and the range of angles has mean 42.584 \u2022 and standard deviation 15.546 \u2022 . This multiplicity and diversity of views enables opportunities for learning to predict view-dependent surface properties, such as material reflectance [4,26], and for learning to factor out view-dependence when learning view-independent representations, such as patch descriptors [45,46] and normals [9,23,3,41,48] (see Section 4.3).\n\nEntire Buildings. Previous RGB-D datasets have provided data for single rooms or small sets of adjacent rooms [44,7], or single floors of a building [2]. Ours provides data for 90 entire buildings. On average, each scanned building has 2.61 floors, covers 2437.761m 2 of surface area, and has 517.34m 2 of floorspace. Providing scans of homes in their entirety enables opportunities for learning about long-range context, which is critical for holistic scene understanding and autonomous navigation.\n\nPersonal Living Spaces. Previous RGB-D datasets are often limited to academic buildings [1]. Ours contains imagery acquired from private homes (with permissions to dis- tribute them for academic research). Data of this type is difficult to capture and distribute due to privacy concerns, and thus it is very valuable for learning about the types of the personal living spaces targeted by most virtual reality, elderly assistance, home robotics, and other consumer-level scene understanding applications.\n\nScale. We believe that Matterport3D is the largest RGB-D dataset available. The BuldingParser dataset [2] provides data for 270 rooms spanning 6,020m 2 of floor space. ScanNet [7], provides images covering 78,595m 2 of surface area spanning 34,453m 2 of floor space in 707 distinct rooms. Our dataset covers 219,399m 2 of surface area in 2056 rooms with 46,561m 2 of floor space. This scale provides new opportunities for training data-hungry algorithms.\n\n\nLearning from the Data\n\nThe following subsections describe several tasks leveraging these unique properties of the Matterport3D dataset to provide new ways to learn representations of scenes. For all experiments, we have split the dataset into 61 scenes for training, 11 for validation, and 18 for testing (see the supplemental materials for details).\n\n\nKeypoint Matching\n\nMatching keypoints to establish correspondences between image data is an important task for many applications including mapping, pose estimation, recognition, and tracking. With the recent success of neural networks, several works have begun to explore the use of deep learning techniques for training state-of-the-art keypoint descriptors that can facilitate robust matching between keypoints and their local image features [45,34,16]. To enable training these deep descriptors, prior works leverage the vast amounts of correspondences found in existing RGB-D reconstruction datasets [29,46].\n\nWith the precise global alignment of RGB-D data and comprehensive view sampling, our Matterport3D dataset Figure 9: Example training correspondences (left) and image patches (right) extracted from Matterport3D. Triplets of matching patches (first and second columns) and nonmatching patches (third column) are used to train our deep local keypoint descriptor.\n\nprovides the unique opportunity to retrieve high quality, wide-baselined correspondences between image frames (see Figure 9). We demonstrate that by pretraining deep local descriptors over these correspondences, we can learn useful features to enable training even stronger descriptors. More specifically, we train a convolutional neural network (ResNet-50 [18]) to map an input image patch to a 512 dimensional descriptor. Similar to state of the art by [19], we train the ConvNet in a triplet Siamese fashion, where each training example contains two matching image patches and one non-matching image patch. Matches are extracted from SIFT keypoint locations which project to within 0.02m of each other in world space and have world normals within 100 \u2022 . To supervise the triplet model, we train with an L2 hinge embedding loss.\n\nFor evaluation, we train on correspondences from 61 Matterport3D scenes and 17 SUN3D scenes, and test on ground truth correspondences from 8 held out SUN3D scenes. The SUN3D ground truth correspondences and registrations are obtained from [15], using the training and testing scenes split from [46]. As in [16], we measure keypoint matching performance with the false-positive rate (error) at 95% recall, the lower the better. We train three models -one trained on Matterport3D data only, one trained on SUN3D data only, and another pretrained on Matterport3D and fine-tuned on SUN3D. Overall, we show that pretraining on Matterport3D yields a descriptor that achieves better keypoint matching performance on a SUN3D benchmark.\n\n\nView Overlap Prediction\n\nIdentifying previously visited scenes is a fundamental step for many reconstruction pipelines -i.e., to detect loop closures. While previous RGB-D video datasets may only have few instances of loop closures, the Matterport3D dataset has a large number of view overlaps between image frames due to the panoramic nature and comprehensive viewpoint sampling of the capturing process. This large SURF 46.8% SIFT 37.8% ResNet-50 w/ Matterport3D 10.6% ResNet-50 w/ SUN3D 10.5% ResNet-50 w/ Matterport3D + SUN3D 9.2% Table 1: Keypoint matching results. Error (%) at 95% recall on ground truth correspondences from the SUN3D testing scenes. We see an improvement in performance from pretraining on Matterport3D. In this work, we formalize loop closure detection as an image retrieval task. Given a query image, the goal is to find other images with \"as much overlap in surface visibility as possible.\" We quantify that notion as a real-numbered value modeled after intersection over union (IOU): overlap(A, B) = min(\u00c2,B)/(|A| + |B| \u2212 min(\u00c2,B)) where A and B are images, |A| is the number of pixels with valid depths in A,\u00c2 is the number of pixels of image A whose projection into world space lie within 5cm of any pixel of B.\n\nWe train a convolutional neural network (ResNet-50 [18]) to map each frame to features, where a closer L2 distance between two features indicates a higher overlap. Similar to keypoint matching, we train this model in a triplet Siamese fashion, using the distance ratio loss from [19]. However, unlike the keypoint matching task, where there is a clear definition of \"match\" and \"non-match,\" the overlap function can be any value ranging from 0 to 1. Therefore we add a regression loss on top of the triplet loss that directly regresses the overlap measurement between the \"matching\" image pairs (overlap ratio greater than 0.1). Table 2 shows an evaluation of this network trained on the Matterport3D training set and then tested on both the  Table 2: View overlap prediction results. Results on SUN3D and Matterport3D dataset measured by normalized discounted cumulative gain. From the comparison we can clearly see the performance improvement from training data with Matterport3D and from adding the extra overlap regression loss. We also note that overlap prediction is much harder in the Matterport3D dataset due to the wide baselines between camera poses.\n\nMatterport3D test set and the SUN3D dataset [44]. For each test, we generate a retrieval list sorted by predicted distance and evaluate it by computing the normalized discounted cumulative gain between predicted list and the best list from ground truth. To mimic real reconstruction scenarios, we only consider candidate image pairs that have travel distance greater than 0.5m apart. The experimental results show that training on the Matterport3D dataset helps find loop closures when testing on SUN3D, and that the extra supervision of overlap ratio regression helps to improve performance on both test sets. We can also notice that overlap prediction is much harder in our Matterport3D dataset due to the wide baseline between camera poses, which is very different from data captured with hand held devices like SUN3D (Figure 10).\n\n\nSurface Normal Estimation\n\nEstimating surface normals is a core task in scene reconstruction and scene understanding. Given a color image, the task is to estimate the surface normal direction for each pixel. Networks have been trained to perform that task using RGB-D datasets in the past [9,23,3,41,48]. However, the depths acquired from commodity RGB-D cameras are generally very noisy, and thus provide poor training data. In contrast, the Matterport camera acquires depth continuously as it rotates for each panorama and synthesizes all the data into depth images aligned with their color counterparts which produces normals with less noise.\n\nIn this section, we consider whether the normals in the Matterport3D dataset can be used to train better models for normal prediction on other datasets. For our study, we use the model proposed in Zhang et al. [48], which achieves the state of the art performance on the NYUv2 dataset. The model is a fully convolutional neural network consisting of an encoder, which shares the same architecture as VGG-16 from the beginning till the first fully connected layer, and a purely symmetric decoder. The network also contains shortcut link to copy the high resolution feature from the encoder Figure 11: Examples of surface normal estimation. We show results of images from NYUv2 testing set. The results from the model fine-tuned on Matterport3D (SUNCG-MP) shows the best quality visually, as it starts to capture small details while still produces smooth planar area. The model further fine-tuned on NYUv2 (SUNCG-MP-NYU) achieves the best quantitatively performance but tends to produce comparatively noisy results.   to the decoder to bring in details, and forces the up pooling to use the same sampling mask from the corresponding max pooling layer. Zhang et al. [48] demonstrate that by pretraining on a huge repository of high-quality synthetic data rendered from SUNCG [38] and then fine-tuning on NYUv2, the network can achieve significantly better performance than directly training on NYUv2. They also point out that the noisy ground truth on NYUv2 provides inaccurate supervision during the training, yielding results which tend to be blurry. With an absence of real-world high-quality depths, their model focuses solely on the improvement from pretraining on synthetic scenes and fine-tuning on real scenes.\n\nWe use Matterport3D data as a large-scale real dataset with high-quality surface normal maps for pretraining, and train the model with a variety of training strategies. For the Matterport3D data, we use only the horizontal and downward looking views as they are closer to canonical views a human observer would choose to look at a scene. Table 3 shows the performance of surface normal estimation. As can be seen, the model pretrained using both the synthetic data and Matterport3D data (the last row) outperforms the one using only the synthetic data (the 2nd row) and achieves best performance. We show the cross dataset accuracy in Table 4. We train models by first pretraining on synthetic data and then finetuning on each dataset; i.e., NYUv2 and Matterport3D, respectively. We evaluate two models on the test set of each dataset. The model trained on each dataset provides the best performance when testing on the same dataset. However, the NYUv2 model performs poorly when testing on Matterport3D, while the Matterport3D model still performs reasonably well on NYUv2. This demonstrates that model trained on Matterport3D data generalizes much better, with its higher quality of depth data and diversity of viewpoints. Figure 11 shows results on NYUv2 dataset. Compared to the model only trained on the synthetic data (SUNCG) or NYUv2 (SUNCG-NYU), the model fine-tuned on Matter-port3D shows the best visual quality, as it captures more detail on small objects, such as the paper tower and fire alarm on the wall, while still producing smooth planar regions. This improvement on surface normal estimation demonstrates the importance of having high quality depth. The model further fine-tuned on NYUv2 (SUNCG-MP-NYU) achieves the best quantitatively performance, but tends to produce comparatively noisy results since the model is \"contaminated\" by the noisy ground truth from NYUv2.\n\n\nRegion-Type Classification\n\nScene categorization is often considered as the first step for high-level scene understanding and reasoning. With the proposed dataset, which contains a large variety of indoor environments, we focus our problem on indoor region (room) classification -given an image, classify the image based on the semantic category of the region that contains its viewpoint (e.g., the camera is in a bedroom, or the camera is in a hallway).\n\nUnlike the semantic voxel labeling problem, region-level classification requires understanding global context that often goes beyond single view observations. While most of the scene categorization datasets [43,49] focus on single view scene classification, this dataset provides a unique opportunity to study the relationship between image field of view and scene classification performance.\n\nAs ground truth for this task, we use the 3D region annotations provided by people as described in Section 3.2. We choose the 12 most common categories in the dataset for this experiment. We assign the category label for each panorama or single image according to the label provided for the region containing it. We then train a convolutional neural network (ResNet-50 [18]) to classify each input image to predict the region type. Table 5 shows the classification accuracy (number of true positives over the total number of instances per region type). By comparing the accuracy between [single] and [pano], we can see an improvement in performance from increased image field of view for most region types. The lower performance in lounge and family room is due to confusion with other adjacent regions (e.g. they are often confused with adjacent hallways and kitchens, which are more visible with wider fields of view).\n\n\nSemantic Voxel Labeling\n\nSemantic voxel labeling -i.e., predicting a semantic object label for each voxel -is a fundamental task for semantic scene understanding; it is the analog of image segmentation in 3D space. We follow the description of the semantic voxel labeling task as introduced in ScanNet [7].\n\nFor training data generation, we first voxelize the train- ing scenes into a dense voxel grid of 2cm 3 voxels, where each voxel is associated with its occupancy and class label, using the object class annotations. We then randomly extract subvolumes from the scene of size 1.5m \u00d7 1.5m \u00d7 3m (31 \u00d7 31 \u00d7 62 voxels). Subvolumes are rejected if < 2% of the voxels are occupied or < 70% of these occupied voxels have valid annotations. Each subvolume is up-aligned, and augmented with 8 rotations.\n\nWe use 20 object class labels, and a network following the architecture of ScanNet [7], and training with 52,355 subvolume samples (418,840 augmented samples). Table 6 shows classification accuracy for our semantic voxel labeling on Matterport3D test scenes, with several visual results show in Figure 12.\n\n\nConclusion\n\nWe introduce Matterport3D, a large RGB-D dataset of 90 building-scale scenes. We provide instance-level semantic segmentations on the full 3D reconstruction of each building. In combination with the unique data characteristics of diverse, panoramic RGB-D views, precise global alignment over a building scale, and comprehensive semantic context over a variety of indoor living spaces, Matterport3D enables myriad computer vision tasks. We demonstrate that Matter-port3D data can be used to achieve state of the art performance on several scene understanding tasks and release the dataset for research use.\n\n\nAcknowledgements\n\nThe Matterport3D dataset is captured and gifted by Matterport for use by the academic community. We would like to thank Matt Bell, Craig Reynolds, and Kyle Simek for their help in accessing and processing the data, as well as the Matterport photographers who agreed to have their data be a part of this dataset. Development of tools for processing the data were supported by Google Tango \n\n\nA. Learning from the Data\n\nThe dataset is split into training, validation, and test set as shown in Fig. 14 -18. Each image shows the textured mesh for one scene from a bird's eye view. These images are helpful for getting a sense of the diversity and scale of the scenes in the dataset.\n\n\nA.1. Keypoint Matching\n\nThe first task considered in the paper is using Matter-port3D to learn a descriptor for keypoint matching. Unlike previous RGB-D datasets which contain video capture from hand-held sensors, Matterport3D comprises data captured from stationary cameras which comprehensively sample the viewpoint space. As shown in Fig 13, this allows keypoints to be seen from a wide variety of differing views. Thus the Matterport3D data allows training for such scenarios which often provide significant challenges in keypoint matching and tracking (e.g., detecting loop closures). Fig. 19 shows a t-SNE embedding of local keypoint patches based on the descriptors from our triplet Siamese network, demonstrating the ability to cluster similar keypoints even with significant changes in view. Figure 13: Visualization of the set of images visible to a keypoint. Each camera view sees the same key point (marked as red dots) from different view points; the smaller images are the patches that we use to train our keypoint descriptor.\n\n\nA.2. View Overlap Prediction\n\nThe second task investigated in the paper is predicting overlaps between pairs of views in the same scene -i.e., the fraction pixels observing the same surface in both images.\n\nThe Matterport3D dataset provides a unique opportunity to explore this task since it provides a large number and wide variety of overlapping views. Fig. 20 shows eight zoomed views of one scene. In each image, the surface reconstruction is shown in shaded gray and cameras are shown as short line segments indicating view positions and directions. The set of cameras within the same panorama look like a star because 18 separate images with 6 rotations and 3 tilt angles are captured for each tripod location. Please note the density and diversity of views in the scenes.\n\nIn each image of Fig. 20, a \"selected\" camera is highlighted in yellow, and all other cameras are colored according to how much they overlap with it -thin dull cyan is 0% overlap, thick bright red is \u226520% overlap, and the line width and red channel scale linearly in between. Please note that there are around a dozen significant overlaps in most of these examples, including overlaps between cameras with significantly different view directions and between viewpoints in different regions (please zoom the document as needed to see the figure at full resolution). Predicting overlaps in these cases is a challenging task.\n\n\nA.3. Surface Normal Estimation\n\nThe third task is to train a model to predict normals from RGB images. Fig. 21 shows comparisons of models trained with different combinations of datasets (please refer to paper for the details of each dataset). The 1st and 2nd columns show input color images and the ground truth normal map. The 3rd column shows the result of the model trained on physically based rendering from Zhang et al. [48]. The 4th and 5th columns show results of models further finetuned on NYUv2 and Matterport3D. The last column shows results of models pretrained with both synthetic data and Matter-port3D and then finetuned on NYUv2.\n\nWe can see that the \"SUNCG-MP\" model often produces clean result (e.g., in large areas of planar surface) with more details (e.g., painting on the wall, book on the table). Further finetuning on the noisy ground truth from NYUv2 actually hurts these desirable properties.\n\nTo further compare the quality of depth and surface normal provided in Matterport3D and NYUv2 and also their impacts on the model, we take the model pretrained on the synthetic data, and finetune on Matterport3D and NYUv2 respectively and evaluate both models on the testing set of two datasets. Fig. 22 and Fig. 23 show that qualitative results on images from Matterport3D and NYUv2 respectively. We can see that:\n\n\u2022 The quality of depth and surface normal from Mat-terport3D (Fig. 22 2nd, 3rd column) is better than NYUv2 (Fig. 23 2nd, 3rd column). The depth from Matterport3D presents cleaner flat surface with more details, whereas the noise in depth images from NYUv2 almost overwhelm local details.\n\n\u2022 On images from Matterport3D, the model finetuned on Matterport3D (Fig. 22 4th column) produces good results containing both clean flat region and local details. However, the model trained on NYUv2 (Fig. 22 5th column) does not work well.\n\n\u2022 On images from NYUv2, the model finetuned on NYUv2 (Fig. 23 4th column) produces good results. The model trained on Matterport3D (Fig. 23 5th column) still managed to produce reasonably good results, sometimes even better.\n\n\u2022 Our model can predict correctly surface normal for those areas with missing depth (gray area in the surface normal map), which implies the potential of improving raw depth images from sensor using our model.\n\n\nA.4. Region-Type Classification\n\nThe fourth task is to predict the category of the region (room) containing a given panorama. Fig. 25 and 26 show several examples of the region-type category annotations provided with Matterport3D. For each building, a person manually outlined the floorplan boundary of each region (shown in column b) and provided a semantic label for its category (bathroom, bedroom, closet, dining room, entryway, familyroom, garage, hallway, library, laundryroom, kitchen, livingroom, meetingroom, lounge, office, porch, recroom, stairs, toilet, utilityroom, gym, outdoor, otherroom, bar, classroom, diningbooth, spa, or junk). Then, the imprecisely drawn region boundaries are snapped to the mesh surfaces and extruded to provide a full semantic segmentation of the original surface mesh by region category (c) and instance (d). These category labels provide the ground truth for the region-type categorization task described in Section 4.4 of the main paper.\n\n\nA.5. Semantic Voxel Labeling\n\nThe final task is to predict per-voxel labels for a given scene voxelization. Fig. 24 shows an example house with instance-level semantic voxel annotations indicated by the colors. First, we partition the textured mesh of every house into region meshes as described in the previous section. Then, we obtain annotations for the object instances in each region using the crowdsourced semantic \"paint and name\" interface of Dai et al. [7]; see Fig. 24, top right. We then apply spell checking and flattening of synonyms to obtain raw object category labels (Fig. 24, bottom left). Using occurrence frequency sorting and collapsing categories that are refinements of other categories (e.g., \"dining chair\" and \"chair\"), we further reduce these labels to a canonical set of 40 categories (Fig. 24, bottom right). The canonical category labels constitute the ground truth for the semantic voxel labeling task described in Section 4.5 of the main paper.      We evaluate two models trained on NYUv2 and Matterport3D respectively (both by finetuning the pretrained model on synthetic data) using images from Matterport3D. The quality of depth (2nd column) and surface normal (3rd column) is much better than that of the NYUv2 shown in Fig. 23. The model trained on Matterport3D (4th column) does good job in predicting the surface normal, whereas model trained on NYUv2 (5th column) performs significantly worse. Figure 23: Surface Normal Estimation: Evaluation on NYUv2 images. We evaluate two models trained on NYUv2 and Matterport3D respectively (both by finetuning the pretrained model on synthetic data) using images from NYUv2. The quality of depth (2nd column) and surface normal (3rd column) is much worse than that of the Matterport3D shown in Fig. 22. The model trained on NYUv2 (4th column) does good job in predicting the surface normal, while model trained on Matterport3D (5th column) still produces reasonably good results, sometimes even cleaner.\n\n\nTextured mesh\n\nObject instance labels Raw object category labels Canonical 40 category object labels Figure 24: Semantic Voxel Label Segmentations. The dataset includes manually \"painted\" object instance and category labels. From top left: textured 3D mesh, object instances, object category labels, and finally canonicalized 40 category labels. Note that raw labels for different types of chairs such as \"dining chair\", and \"office chair\" are mapped to a single canonical \"chair\" category shown in light green. \n\nFigure 2 :\n2Panoramas are captured from viewpoints (green spheres) on average 2.25m apart.\n\nFigure 3 :\n3Annotator-specified floor plans. Floor plans are used to define regions for object-level semantic annotation. Left: floor plan with textured mesh. Right: floor plan alone (colored by region category). bundle adjustment, and 3) reconstructing a single textured mesh containing all visible surfaces of the environment.\n\nFigure 4 :\n4Instance-level semantic annotations. Example rooms annotated with semantic categories for all object instances. Left: 3D room mesh. Middle: object instance labels. Right: object category labels.\n\nFigure 5 :\n5Semantic annotation statistics. Total number of semantic annotations for the top object categories.\n\nFigure 6 :\n6Visualizations of point clouds (left-to-right: color, diffuse shading, and normals). These images show pixels from all RGB-D images back-projected into world space according to the provided camera poses. Please note the accuracy of the global alignment (no ghosting) and the relatively low noise in surface normals, even without advanced depth-fusion techniques.\n\nFigure 7 :\n7Visualization of the set of images visible to a selected surface point (shown as red visibility lines). (Please note that the mesh is highly decimated in this image for convenience of visualization)\n\nFigure 8 :\n8Histogram showing how many images observe each surface vertex. The mode is 7 and the average is 11.\n\nFigure 10 :\n10Example overlap views from SUN3D and Mat-terport3D ranked by their overlap ratio. In contrast to RGB-D video datasets captured with hand-held devices like SUN3D, Matterport3D provides a larger variety of camera view points and wide baseline correspondences, which enables training a stronger model for view overlap prediction under such challenging cases. number of loop closures provides an opportunity to train a deep model to recognize loop closures, which can be incorporated in future SLAM reconstruction pipelines.\n\nFigure 12 :\n12Semantic voxel labeling results on our Matter-port3D test scenes.\n\nFigure 14 : 20 Figure 15 : 40 Figure 16 : 61 Figure 17 :\n14201540166117Training Set. Examples 1 -Training Set. Examples 21 -Training Set. Examples 41-Validation Set.\n\nFigure 18 :\n18Test Set.\n\nFigure 19 :\n19t-SNE embedding of descriptors from our triplet Siamese network trained for keypoint matching on the Matter-port3D test set.\n\nFigure 20 :\n20View Overlap Prediction.: Eight views within the same scene showing overlaps between selected cameras (yellow) and all other cameras. The colors ranging from thin dull cyan (no overlap) to thick bright red (\u226520% overlap) indicate the fraction of overlap with the selected view.\n\nFigure 21 :\n21Surface Normal Estimation: Comparison of multiple training schema. We compare the model pretrained with different datasets on the NYUv2 testing set. The 1st and 2nd columns show input color images and the ground truth normal map. The 3rd column shows the result of the model trained on physically based rendering. The 4th and 5th columns show results of models further finetuned on NYUv2 and Matterport3D. The last column shows results of models pretrained with both synthetic data and Matterport3D and then finetuned on NYUv2.\n\nFigure 22 :\n22Surface Normal Estimation: Evaluation on Matterport3D images.\n\nFigure 25 :Figure 26 :\n2526Region-type classification. The dataset includes manually-specified boundary and category annotations for all regions (rooms) of all buildings. This figure shows for several examples (from left to right): the textured mesh, the floorplan colored by region category, the mesh surface colored by region category, and the mesh surface colored by region instance. Region-type classification. More examples like Fig. 25.\n\n\nTrain Set 1 Train Set 2 Train Set 3 Mean( \u2022 )\u2193 Median( \u2022 )\u2193 11.25(%)\u2191 22.5(%)\u2191 30(%)\u2191 \nSUNCG \n-\n-\n28.18 \n21.75 \n26.45 \n51.34 \n62.92 \nSUNCG \nNYUv2 \n-\n22.07 \n14.79 \n39.61 \n65.63 \n75.25 \nMP \n-\n-\n31.23 \n25.95 \n18.17 \n43.61 \n56.69 \nMP \nNYUv2 \n-\n24.34 \n16.94 \n35.09 \n60.72 \n71.13 \nSUNCG \nMP \n-\n26.34 \n21.08 \n23.04 \n53.36 \n67.45 \nSUNCG \nMP \nNYUv2 \n20.89 \n13.79 \n42.29 \n67.82 \n77.16 \n\n\n\nTable 3 :\n3Surface normal estimation results. Impact of training with Matterport3D (MP) on performance in the NYUv2 dataset. The columns show the mean and median angular error on a per pixel level, as well as the percentage of pixels with error less than 11.25 \u2022 , 22.5 \u2022 , and 30 \u2022 .Train \nTest \nMean( \u2022 )\u2193 Median( \u2022 )\u2193 11.25(%)\u2191 22.5(%)\u2191 30(%)\u2191 \nMP \nNYUv2 \n26.34 \n21.08 \n23.04 \n53.35 \n67.45 \nNYUv2 NYUv2 \n22.07 \n14.79 \n39.61 \n65.63 \n75.25 \nMP \nMP \n19.11 \n10.44 \n52.33 \n72.22 \n79.46 \nNYUv2 \nMP \n33.91 \n25.07 \n23.98 \n46.26 \n56.45 \n\n\n\nTable 4 :\n4Surfacenormal estimation cross dataset valida-\ntion. We investigate the influence of training and testing the \nmodel using permutation of datasets. Notice how the Mat-\nterport3D dataset is able to perform well on NYUv2, while \nthe converse is not true. \n\n\n\nTable 5 :\n5Region-type classification results.Each entry lists the prediction accuracy (percentage correct). By comparing the \naccuracy between [single] and [pano] we can see an improvement from increased image field of view for most regiontypes. \nHowever, the lower performance on lounge and family room may be caused by confusion from seeing multiple rooms in one \npanorama. \n\nClass \n% of Test Scenes Accuracy \nWall \n28.9% \n78.8% \nFloor \n22.6% \n92.6% \nChair \n2.7% \n91.1% \nDoor \n5.0% \n60.6% \nTable \n1.7% \n20.7% \nPicture \n1.1% \n28.4% \nCabinet \n2.9% \n14.4% \nWindow \n2.2% \n14.7% \nSofa \n0.1% \n0.004% \nBed \n0.9% \n1.0% \nPlant \n2.0% \n7.5% \nSink \n0.2% \n23.8% \nStairs \n1.5% \n54.0% \nCeiling \n8.1% \n85.4% \nToilet \n0.1% \n6.8% \nMirror \n0.4% \n20.2% \nBathtub \n0.2% \n5.1% \nCounter \n0.4% \n27.5% \nRailing \n0.7% \n18.3% \nShelving \n1.2% \n16.6% \nTotal \n-\n70.3% \n\n\n\nTable 6 :\n6Semantic voxel label prediction accuracy on our Matterport3D test scenes.\nAll data and code is publicly available: https://github.com/niessner/Matterport\nAppendix\nJoint 2D-3D-semantic data for indoor scene understanding. I Armeni, S Sax, A R Zamir, S Savarese, arXiv:1702.01105arXiv preprintI. Armeni, S. Sax, A. R. Zamir, and S. Savarese. Joint 2D- 3D-semantic data for indoor scene understanding. arXiv preprint arXiv:1702.01105, 2017.\n\n3D semantic parsing of largescale indoor spaces. I Armeni, O Sener, A R Zamir, H Jiang, I Brilakis, M Fischer, S Savarese, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionI. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis, M. Fischer, and S. Savarese. 3D semantic parsing of large- scale indoor spaces. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1534- 1543, 2016.\n\nMarr revisited: 2D-3D alignment via surface normal prediction. A Bansal, B Russell, A Gupta, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionA. Bansal, B. Russell, and A. Gupta. Marr revisited: 2D- 3D alignment via surface normal prediction. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5965-5974, 2016.\n\nIntrinsic images in the wild. S Bell, K Bala, N Snavely, ACM Trans. on Graphics (SIGGRAPH). 334S. Bell, K. Bala, and N. Snavely. Intrinsic images in the wild. ACM Trans. on Graphics (SIGGRAPH), 33(4), 2014.\n\nS Choi, Q.-Y Zhou, S Miller, V Koltun, arXiv:1602.02481A large dataset of object scans. arXiv preprintS. Choi, Q.-Y. Zhou, S. Miller, and V. Koltun. A large dataset of object scans. arXiv preprint arXiv:1602.02481, 2016.\n\nInteractive and anisotropic geometry processing using the screened poisson equation. M Chuang, M Kazhdan, ACM Transactions on Graphics (TOG). 30457M. Chuang and M. Kazhdan. Interactive and anisotropic geometry processing using the screened poisson equation. ACM Transactions on Graphics (TOG), 30(4):57, 2011.\n\nScannet: Richly-annotated 3d reconstructions of indoor scenes. A Dai, A X Chang, M Savva, M Halber, T Funkhouser, M Nie\u00dfner, A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nie\u00dfner. Scannet: Richly-annotated 3d recon- structions of indoor scenes. http://arxiv.org/abs/1702.04405, 2017.\n\nBundlefusion: Real-time globally consistent 3d reconstruction using on-the-fly surface re-integration. A Dai, M Nie\u00dfner, M Zoll\u00f6fer, S Izadi, C Theobalt, ACM Transactions on Graphics. 2017A. Dai, M. Nie\u00dfner, M. Zoll\u00f6fer, S. Izadi, and C. Theobalt. Bundlefusion: Real-time globally consistent 3d reconstruc- tion using on-the-fly surface re-integration. ACM Transac- tions on Graphics 2017 (TOG), 2017.\n\nPredicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. D Eigen, R Fergus, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionD. Eigen and R. Fergus. Predicting depth, surface normals and semantic labels with a common multi-scale convolu- tional architecture. In Proceedings of the IEEE International Conference on Computer Vision, pages 2650-2658, 2015.\n\nRGBD datasets: Past, present and future. M Firman, CVPR Workshop on Large Scale 3D Data: Acquisition, Modelling and Analysis. M. Firman. RGBD datasets: Past, present and future. In CVPR Workshop on Large Scale 3D Data: Acquisition, Mod- elling and Analysis, 2016.\n\nData-driven 3D primitives for single image understanding. D F Fouhey, A Gupta, M Hebert, ICCV. D. F. Fouhey, A. Gupta, and M. Hebert. Data-driven 3D primitives for single image understanding. In ICCV, 2013.\n\nUnfolding an indoor origami world. D F Fouhey, A Gupta, M Hebert, European Conference on Computer Vision. SpringerD. F. Fouhey, A. Gupta, and M. Hebert. Unfolding an indoor origami world. In European Conference on Computer Vision, pages 687-702. Springer, 2014.\n\nPerceptual organization and recognition of indoor scenes from RGB-D images. S Gupta, P Arbelaez, J Malik, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionS. Gupta, P. Arbelaez, and J. Malik. Perceptual organiza- tion and recognition of indoor scenes from RGB-D images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 564-571, 2013.\n\nLearning rich features from RGB-D images for object detection and segmentation: Supplementary material. S Gupta, R Girshick, P Arbel\u00e1ez, J Malik, S. Gupta, R. Girshick, P. Arbel\u00e1ez, and J. Malik. Learning rich features from RGB-D images for object detection and segmentation: Supplementary material, 2014.\n\nStructured global registration of rgb-d scans in indoor environments. M Halber, T Funkhouser, M. Halber and T. Funkhouser. Structured global registration of rgb-d scans in indoor environments. 2017.\n\nMatchnet: Unifying feature and metric learning for patchbased matching. X Han, T Leung, Y Jia, R Sukthankar, A C Berg, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionX. Han, T. Leung, Y. Jia, R. Sukthankar, and A. C. Berg. Matchnet: Unifying feature and metric learning for patch- based matching. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3279- 3286, 2015.\n\nSceneNet: Understanding real world indoor scenes with synthetic data. A Handa, V Patraucean, V Badrinarayanan, S Stent, R Cipolla, arXiv:1511.07041arXiv preprintA. Handa, V. Patraucean, V. Badrinarayanan, S. Stent, and R. Cipolla. SceneNet: Understanding real world indoor scenes with synthetic data. arXiv preprint arXiv:1511.07041, 2015.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionK. He, X. Zhang, S. Ren, and J. Sun. Deep residual learn- ing for image recognition. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, pages 770-778, 2016.\n\nDeep unsupervised learning through spatial contrasting. E Hoffer, I Hubara, N Ailon, arXiv:1610.00243arXiv preprintE. Hoffer, I. Hubara, and N. Ailon. Deep unsuper- vised learning through spatial contrasting. arXiv preprint arXiv:1610.00243, 2016.\n\nSceneNN: A scene meshes dataset with annotations. B.-S Hua, Q.-H Pham, D T Nguyen, M.-K Tran, L.-F Yu, S.-K Yeung, International Conference on 3D Vision (3DV). 1B.-S. Hua, Q.-H. Pham, D. T. Nguyen, M.-K. Tran, L.-F. Yu, and S.-K. Yeung. SceneNN: A scene meshes dataset with annotations. In International Conference on 3D Vision (3DV), volume 1, 2016.\n\nPoisson surface reconstruction. M Kazhdan, M Bolitho, H Hoppe, Proceedings of the fourth Eurographics symposium on Geometry processing. the fourth Eurographics symposium on Geometry processing7M. Kazhdan, M. Bolitho, and H. Hoppe. Poisson surface reconstruction. In Proceedings of the fourth Eurographics symposium on Geometry processing, volume 7, 2006.\n\nTanks and temples: Benchmarking large-scale scene reconstruction. A Knapitsch, J Park, Q.-Y Zhou, V Koltun, ACM Transactions on Graphics. 364A. Knapitsch, J. Park, Q.-Y. Zhou, and V. Koltun. Tanks and temples: Benchmarking large-scale scene reconstruc- tion. ACM Transactions on Graphics, 36(4), 2017.\n\nDepth and surface normal estimation from monocular images using regression on deep features and hierarchical CRFs. B Li, C Shen, Y Dai, A Van Den Hengel, M He, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionB. Li, C. Shen, Y. Dai, A. van den Hengel, and M. He. Depth and surface normal estimation from monocular images using regression on deep features and hierarchical CRFs. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1119-1127, 2015.\n\nHolistic scene understanding for 3D object detection with rgbd cameras. D Lin, S Fidler, R Urtasun, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionD. Lin, S. Fidler, and R. Urtasun. Holistic scene understand- ing for 3D object detection with rgbd cameras. In Proceed- ings of the IEEE International Conference on Computer Vi- sion, pages 1417-1424, 2013.\n\nReal-time 3D reconstruction at scale using voxel hashing. M Nie\u00dfner, M Zollh\u00f6fer, S Izadi, M Stamminger, ACM Transactions on Graphics. M. Nie\u00dfner, M. Zollh\u00f6fer, S. Izadi, and M. Stamminger. Real-time 3D reconstruction at scale using voxel hashing. ACM Transactions on Graphics (TOG), 2013.\n\nDeep reflectance maps. K Rematas, T Ritschel, M Fritz, E Gavves, T Tuytelaars, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionK. Rematas, T. Ritschel, M. Fritz, E. Gavves, and T. Tuyte- laars. Deep reflectance maps. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4508-4516, 2016.\n\nRGB-(D) scene labeling: Features and algorithms. X Ren, L Bo, D Fox, Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEEX. Ren, L. Bo, and D. Fox. RGB-(D) scene labeling: Fea- tures and algorithms. In Computer Vision and Pattern Recog- nition (CVPR), 2012 IEEE Conference on, pages 2759-2766. IEEE, 2012.\n\nPiGraphs: Learning Interaction Snapshots from Observations. M Savva, A X Chang, P Hanrahan, M Fisher, M Nie\u00dfner, ACM Transactions on Graphics (TOG). 354M. Savva, A. X. Chang, P. Hanrahan, M. Fisher, and M. Nie\u00dfner. PiGraphs: Learning Interaction Snapshots from Observations. ACM Transactions on Graphics (TOG), 35(4), 2016.\n\nSelf-supervised visual descriptor learning for dense correspondence. T Schmidt, R Newcombe, D Fox, IEEE Robotics and Automation Letters. 22T. Schmidt, R. Newcombe, and D. Fox. Self-supervised visual descriptor learning for dense correspondence. IEEE Robotics and Automation Letters, 2(2):420-427, 2017.\n\nScene coordinate regression forests for camera relocalization in RGB-D images. J Shotton, B Glocker, C Zach, S Izadi, A Criminisi, A Fitzgibbon, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJ. Shotton, B. Glocker, C. Zach, S. Izadi, A. Criminisi, and A. Fitzgibbon. Scene coordinate regression forests for cam- era relocalization in RGB-D images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 2930-2937, 2013.\n\nBuilding part-based object detectors via 3D geometry. A Shrivastava, A Gupta, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionA. Shrivastava and A. Gupta. Building part-based object de- tectors via 3D geometry. In Proceedings of the IEEE Inter- national Conference on Computer Vision, pages 1745-1752, 2013.\n\nIndoor scene segmentation using a structured light sensor. N Silberman, R Fergus, Computer Vision Workshops (ICCV Workshops. 2011 IEEE International Conference onN. Silberman and R. Fergus. Indoor scene segmentation us- ing a structured light sensor. In Computer Vision Workshops (ICCV Workshops), 2011 IEEE International Conference on, 2011.\n\nIndoor segmentation and support inference from RGBD images. N Silberman, D Hoiem, P Kohli, R Fergus, European Conference on Computer Vision. N. Silberman, D. Hoiem, P. Kohli, and R. Fergus. Indoor segmentation and support inference from RGBD images. In European Conference on Computer Vision, 2012.\n\nDiscriminative learning of deep convolutional feature point descriptors. E Simo-Serra, E Trulls, L Ferraz, I Kokkinos, P Fua, F Moreno-Noguer, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionE. Simo-Serra, E. Trulls, L. Ferraz, I. Kokkinos, P. Fua, and F. Moreno-Noguer. Discriminative learning of deep convolu- tional feature point descriptors. In Proceedings of the IEEE International Conference on Computer Vision, pages 118- 126, 2015.\n\nSUN RGB-D: A RGB-D scene understanding benchmark suite. S Song, S P Lichtenberg, J Xiao, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionS. Song, S. P. Lichtenberg, and J. Xiao. SUN RGB-D: A RGB-D scene understanding benchmark suite. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 567-576, 2015.\n\nSliding shapes for 3D object detection in depth images. S Song, J Xiao, European conference on computer vision. SpringerS. Song and J. Xiao. Sliding shapes for 3D object detection in depth images. In European conference on computer vision, pages 634-651. Springer, 2014.\n\nDeep sliding shapes for amodal 3D object detection in RGB-D images. S Song, J Xiao, S. Song and J. Xiao. Deep sliding shapes for amodal 3D object detection in RGB-D images. 2016.\n\nSemantic scene completion from a single depth image. S Song, F Yu, A Zeng, A X Chang, M Savva, T Funkhouser, arXiv:1611.08974arXiv preprintS. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. Funkhouser. Semantic scene completion from a single depth image. arXiv preprint arXiv:1611.08974, 2016.\n\nLearning to navigate the energy landscape. J Valentin, A Dai, M Nie\u00dfner, P Kohli, P Torr, S Izadi, C Keskin, arXiv:1603.05772arXiv preprintJ. Valentin, A. Dai, M. Nie\u00dfner, P. Kohli, P. Torr, S. Izadi, and C. Keskin. Learning to navigate the energy landscape. arXiv preprint arXiv:1603.05772, 2016.\n\nSe-manticPaint: Interactive 3D labeling and learning at your fingertips. J Valentin, V Vineet, M.-M Cheng, D Kim, J Shotton, P Kohli, M Nie\u00dfner, A Criminisi, S Izadi, P Torr, ACM Transactions on Graphics (TOG). 345154J. Valentin, V. Vineet, M.-M. Cheng, D. Kim, J. Shotton, P. Kohli, M. Nie\u00dfner, A. Criminisi, S. Izadi, and P. Torr. Se- manticPaint: Interactive 3D labeling and learning at your fin- gertips. ACM Transactions on Graphics (TOG), 34(5):154, 2015.\n\nDesigning deep networks for surface normal estimation. X Wang, D Fouhey, A Gupta, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionX. Wang, D. Fouhey, and A. Gupta. Designing deep net- works for surface normal estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 539-547, 2015.\n\nRecognizing scene viewpoint using panoramic place representation. J Xiao, K A Ehinger, A Oliva, A Torralba, Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. IEEEJ. Xiao, K. A. Ehinger, A. Oliva, and A. Torralba. Recogniz- ing scene viewpoint using panoramic place representation. In Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, pages 2695-2702. IEEE, 2012.\n\nSun database: Large-scale scene recognition from abbey to zoo. J Xiao, J Hays, K A Ehinger, A Oliva, A Torralba, Computer vision and pattern recognition (CVPR), 2010 IEEE conference on. IEEEJ. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In Computer vision and pattern recognition (CVPR), 2010 IEEE conference on, pages 3485-3492. IEEE, 2010.\n\nSUN3D: A database of big spaces reconstructed using SFM and object labels. J Xiao, A Owens, A Torralba, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionJ. Xiao, A. Owens, and A. Torralba. SUN3D: A database of big spaces reconstructed using SFM and object labels. In Proceedings of the IEEE International Conference on Com- puter Vision, pages 1625-1632, 2013.\n\nLIFT: Learned invariant feature transform. K M Yi, E Trulls, V Lepetit, P Fua, European Conference on Computer Vision. SpringerK. M. Yi, E. Trulls, V. Lepetit, and P. Fua. LIFT: Learned in- variant feature transform. In European Conference on Com- puter Vision, pages 467-483. Springer, 2016.\n\n3DMatch: Learning local geometric descriptors from RGB-D reconstructions. A Zeng, S Song, M Niessner, M Fisher, J Xiao, T Funkhouser, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionA. Zeng, S. Song, M. Niessner, M. Fisher, J. Xiao, and T. Funkhouser. 3DMatch: Learning local geometric descrip- tors from RGB-D reconstructions. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.\n\nEstimating the 3D layout of indoor scenes and its clutter from depth sensors. J Zhang, C Kan, A G Schwing, R Urtasun, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionJ. Zhang, C. Kan, A. G. Schwing, and R. Urtasun. Estimating the 3D layout of indoor scenes and its clutter from depth sen- sors. In Proceedings of the IEEE International Conference on Computer Vision, pages 1273-1280, 2013.\n\nPhysically-based rendering for indoor scene understanding using convolutional neural networks. Y Zhang, S Song, E Yumer, M Savva, J.-Y Lee, H Jin, T Funkhouser, arXiv:1612.07429arXiv preprintY. Zhang, S. Song, E. Yumer, M. Savva, J.-Y. Lee, H. Jin, and T. Funkhouser. Physically-based rendering for indoor scene understanding using convolutional neural networks. arXiv preprint arXiv:1612.07429, 2016.\n\nLearning deep features for scene recognition using places database. B Zhou, A Lapedriza, J Xiao, A Torralba, A Oliva, Advances in neural information processing systems. B. Zhou, A. Lapedriza, J. Xiao, A. Torralba, and A. Oliva. Learning deep features for scene recognition using places database. In Advances in neural information processing sys- tems, pages 487-495, 2014.\n", "annotations": {"author": "[{\"end\":100,\"start\":65},{\"end\":134,\"start\":101},{\"end\":176,\"start\":135},{\"end\":214,\"start\":177},{\"end\":265,\"start\":215},{\"end\":303,\"start\":266},{\"end\":339,\"start\":304},{\"end\":373,\"start\":340},{\"end\":409,\"start\":374}]", "publisher": null, "author_last_name": "[{\"end\":76,\"start\":71},{\"end\":111,\"start\":108},{\"end\":152,\"start\":142},{\"end\":190,\"start\":184},{\"end\":231,\"start\":224},{\"end\":279,\"start\":274},{\"end\":315,\"start\":311},{\"end\":349,\"start\":345},{\"end\":385,\"start\":380}]", "author_first_name": "[{\"end\":70,\"start\":65},{\"end\":107,\"start\":101},{\"end\":141,\"start\":135},{\"end\":183,\"start\":177},{\"end\":223,\"start\":215},{\"end\":273,\"start\":266},{\"end\":310,\"start\":304},{\"end\":344,\"start\":340},{\"end\":379,\"start\":374}]", "author_affiliation": "[{\"end\":99,\"start\":78},{\"end\":133,\"start\":113},{\"end\":175,\"start\":154},{\"end\":213,\"start\":192},{\"end\":264,\"start\":233},{\"end\":302,\"start\":281},{\"end\":338,\"start\":317},{\"end\":372,\"start\":351},{\"end\":408,\"start\":387}]", "title": "[{\"end\":62,\"start\":1},{\"end\":471,\"start\":410}]", "venue": null, "abstract": "[{\"end\":1253,\"start\":473}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b32\"},\"end\":1912,\"start\":1908},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":1941,\"start\":1937},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":1966,\"start\":1962},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2039,\"start\":2035},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2041,\"start\":2039},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2065,\"start\":2061},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2068,\"start\":2065},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2129,\"start\":2126},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4694,\"start\":4690},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4793,\"start\":4790},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":4816,\"start\":4812},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4819,\"start\":4816},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4841,\"start\":4837},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4844,\"start\":4841},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4901,\"start\":4898},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4955,\"start\":4951},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4957,\"start\":4955},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4960,\"start\":4957},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5062,\"start\":5058},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":5064,\"start\":5062},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5067,\"start\":5064},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5069,\"start\":5067},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5332,\"start\":5328},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5335,\"start\":5332},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5338,\"start\":5335},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5341,\"start\":5338},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5343,\"start\":5341},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":5369,\"start\":5365},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5372,\"start\":5369},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5375,\"start\":5372},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":5378,\"start\":5375},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5381,\"start\":5378},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5405,\"start\":5401},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5408,\"start\":5405},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5410,\"start\":5408},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5412,\"start\":5410},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5440,\"start\":5436},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":5443,\"start\":5440},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":5460,\"start\":5456},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5463,\"start\":5460},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5466,\"start\":5463},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5766,\"start\":5763},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8602,\"start\":8598},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8611,\"start\":8607},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9514,\"start\":9511},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":9583,\"start\":9580},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10522,\"start\":10518},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10572,\"start\":10569},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":11203,\"start\":11200},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":11261,\"start\":11257},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11312,\"start\":11309},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":11335,\"start\":11332},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":12050,\"start\":12046},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":12123,\"start\":12120},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":14386,\"start\":14383},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":14389,\"start\":14386},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":14516,\"start\":14512},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":14519,\"start\":14516},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14535,\"start\":14532},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14538,\"start\":14535},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14540,\"start\":14538},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":14543,\"start\":14540},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":14546,\"start\":14543},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":14681,\"start\":14677},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":14683,\"start\":14681},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":14719,\"start\":14716},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":15159,\"start\":15156},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":15678,\"start\":15675},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":15752,\"start\":15749},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":16832,\"start\":16828},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":16835,\"start\":16832},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16838,\"start\":16835},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":16992,\"start\":16988},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":16995,\"start\":16992},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":17720,\"start\":17716},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":17818,\"start\":17814},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18435,\"start\":18431},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":18490,\"start\":18486},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18502,\"start\":18498},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":20221,\"start\":20217},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":20449,\"start\":20445},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":21376,\"start\":21372},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":22456,\"start\":22453},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22459,\"start\":22456},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22461,\"start\":22459},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":22464,\"start\":22461},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":22467,\"start\":22464},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":23025,\"start\":23021},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":23978,\"start\":23974},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":24087,\"start\":24083},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":27086,\"start\":27082},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":27089,\"start\":27086},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":27642,\"start\":27638},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":28497,\"start\":28494},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":29079,\"start\":29076},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":33498,\"start\":33494},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":36822,\"start\":36819}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":38948,\"start\":38857},{\"attributes\":{\"id\":\"fig_1\"},\"end\":39278,\"start\":38949},{\"attributes\":{\"id\":\"fig_2\"},\"end\":39486,\"start\":39279},{\"attributes\":{\"id\":\"fig_3\"},\"end\":39599,\"start\":39487},{\"attributes\":{\"id\":\"fig_4\"},\"end\":39975,\"start\":39600},{\"attributes\":{\"id\":\"fig_5\"},\"end\":40187,\"start\":39976},{\"attributes\":{\"id\":\"fig_6\"},\"end\":40300,\"start\":40188},{\"attributes\":{\"id\":\"fig_7\"},\"end\":40836,\"start\":40301},{\"attributes\":{\"id\":\"fig_8\"},\"end\":40917,\"start\":40837},{\"attributes\":{\"id\":\"fig_9\"},\"end\":41084,\"start\":40918},{\"attributes\":{\"id\":\"fig_10\"},\"end\":41109,\"start\":41085},{\"attributes\":{\"id\":\"fig_11\"},\"end\":41249,\"start\":41110},{\"attributes\":{\"id\":\"fig_12\"},\"end\":41542,\"start\":41250},{\"attributes\":{\"id\":\"fig_13\"},\"end\":42085,\"start\":41543},{\"attributes\":{\"id\":\"fig_14\"},\"end\":42162,\"start\":42086},{\"attributes\":{\"id\":\"fig_15\"},\"end\":42606,\"start\":42163},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":42986,\"start\":42607},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":43520,\"start\":42987},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":43788,\"start\":43521},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":44632,\"start\":43789},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":44718,\"start\":44633}]", "paragraph": "[{\"end\":1488,\"start\":1269},{\"end\":2130,\"start\":1490},{\"end\":2244,\"start\":2132},{\"end\":3204,\"start\":2246},{\"end\":4482,\"start\":3206},{\"end\":5185,\"start\":4514},{\"end\":5703,\"start\":5187},{\"end\":6749,\"start\":5705},{\"end\":7030,\"start\":6778},{\"end\":7684,\"start\":7059},{\"end\":8612,\"start\":7686},{\"end\":8894,\"start\":8636},{\"end\":9350,\"start\":8896},{\"end\":9878,\"start\":9352},{\"end\":10293,\"start\":9880},{\"end\":10434,\"start\":10323},{\"end\":11037,\"start\":10436},{\"end\":11898,\"start\":11039},{\"end\":12971,\"start\":11900},{\"end\":14565,\"start\":12973},{\"end\":15066,\"start\":14567},{\"end\":15571,\"start\":15068},{\"end\":16027,\"start\":15573},{\"end\":16381,\"start\":16054},{\"end\":16996,\"start\":16403},{\"end\":17357,\"start\":16998},{\"end\":18190,\"start\":17359},{\"end\":18919,\"start\":18192},{\"end\":20164,\"start\":18947},{\"end\":21326,\"start\":20166},{\"end\":22161,\"start\":21328},{\"end\":22809,\"start\":22191},{\"end\":24526,\"start\":22811},{\"end\":26416,\"start\":24528},{\"end\":26873,\"start\":26447},{\"end\":27267,\"start\":26875},{\"end\":28189,\"start\":27269},{\"end\":28498,\"start\":28217},{\"end\":28991,\"start\":28500},{\"end\":29298,\"start\":28993},{\"end\":29918,\"start\":29313},{\"end\":30327,\"start\":29939},{\"end\":30617,\"start\":30357},{\"end\":31660,\"start\":30644},{\"end\":31868,\"start\":31693},{\"end\":32441,\"start\":31870},{\"end\":33065,\"start\":32443},{\"end\":33714,\"start\":33100},{\"end\":33987,\"start\":33716},{\"end\":34403,\"start\":33989},{\"end\":34693,\"start\":34405},{\"end\":34934,\"start\":34695},{\"end\":35160,\"start\":34936},{\"end\":35371,\"start\":35162},{\"end\":36354,\"start\":35407},{\"end\":38341,\"start\":36387},{\"end\":38856,\"start\":38359}]", "formula": null, "table_ref": "[{\"end\":12442,\"start\":12125},{\"end\":19464,\"start\":19457},{\"end\":20802,\"start\":20795},{\"end\":20916,\"start\":20909},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":24873,\"start\":24866},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":25170,\"start\":25163},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":27708,\"start\":27701},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":29160,\"start\":29153},{\"end\":30326,\"start\":30321}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1267,\"start\":1255},{\"attributes\":{\"n\":\"2.\"},\"end\":4512,\"start\":4485},{\"attributes\":{\"n\":\"3.\"},\"end\":6776,\"start\":6752},{\"attributes\":{\"n\":\"3.1.\"},\"end\":7057,\"start\":7033},{\"attributes\":{\"n\":\"3.2.\"},\"end\":8634,\"start\":8615},{\"attributes\":{\"n\":\"3.3.\"},\"end\":10321,\"start\":10296},{\"attributes\":{\"n\":\"4.\"},\"end\":16052,\"start\":16030},{\"attributes\":{\"n\":\"4.1.\"},\"end\":16401,\"start\":16384},{\"attributes\":{\"n\":\"4.2.\"},\"end\":18945,\"start\":18922},{\"attributes\":{\"n\":\"4.3.\"},\"end\":22189,\"start\":22164},{\"attributes\":{\"n\":\"4.4.\"},\"end\":26445,\"start\":26419},{\"attributes\":{\"n\":\"4.5.\"},\"end\":28215,\"start\":28192},{\"attributes\":{\"n\":\"5.\"},\"end\":29311,\"start\":29301},{\"attributes\":{\"n\":\"6.\"},\"end\":29937,\"start\":29921},{\"end\":30355,\"start\":30330},{\"end\":30642,\"start\":30620},{\"end\":31691,\"start\":31663},{\"end\":33098,\"start\":33068},{\"end\":35405,\"start\":35374},{\"end\":36385,\"start\":36357},{\"end\":38357,\"start\":38344},{\"end\":38868,\"start\":38858},{\"end\":38960,\"start\":38950},{\"end\":39290,\"start\":39280},{\"end\":39498,\"start\":39488},{\"end\":39611,\"start\":39601},{\"end\":39987,\"start\":39977},{\"end\":40199,\"start\":40189},{\"end\":40313,\"start\":40302},{\"end\":40849,\"start\":40838},{\"end\":40975,\"start\":40919},{\"end\":41097,\"start\":41086},{\"end\":41122,\"start\":41111},{\"end\":41262,\"start\":41251},{\"end\":41555,\"start\":41544},{\"end\":42098,\"start\":42087},{\"end\":42186,\"start\":42164},{\"end\":42997,\"start\":42988},{\"end\":43531,\"start\":43522},{\"end\":43799,\"start\":43790},{\"end\":44643,\"start\":44634}]", "table": "[{\"end\":42986,\"start\":42682},{\"end\":43520,\"start\":43272},{\"end\":43788,\"start\":43540},{\"end\":44632,\"start\":43836}]", "figure_caption": "[{\"end\":38948,\"start\":38870},{\"end\":39278,\"start\":38962},{\"end\":39486,\"start\":39292},{\"end\":39599,\"start\":39500},{\"end\":39975,\"start\":39613},{\"end\":40187,\"start\":39989},{\"end\":40300,\"start\":40201},{\"end\":40836,\"start\":40316},{\"end\":40917,\"start\":40852},{\"end\":41084,\"start\":40990},{\"end\":41109,\"start\":41100},{\"end\":41249,\"start\":41125},{\"end\":41542,\"start\":41265},{\"end\":42085,\"start\":41558},{\"end\":42162,\"start\":42101},{\"end\":42606,\"start\":42191},{\"end\":42682,\"start\":42609},{\"end\":43272,\"start\":42999},{\"end\":43540,\"start\":43533},{\"end\":43836,\"start\":43801},{\"end\":44718,\"start\":44645}]", "figure_ref": "[{\"end\":2509,\"start\":2501},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7873,\"start\":7865},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9239,\"start\":9231},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":10173,\"start\":10165},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10241,\"start\":10233},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":11733,\"start\":11724},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":13990,\"start\":13982},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":14062,\"start\":14054},{\"end\":17112,\"start\":17104},{\"end\":17482,\"start\":17474},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":22159,\"start\":22149},{\"end\":23409,\"start\":23400},{\"end\":25762,\"start\":25753},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":29297,\"start\":29288},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":30441,\"start\":30430},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":30964,\"start\":30957},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":31217,\"start\":31210},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":31430,\"start\":31421},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32025,\"start\":32018},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32467,\"start\":32460},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33178,\"start\":33171},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34292,\"start\":34285},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34304,\"start\":34297},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34474,\"start\":34466},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34538,\"start\":34513},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34782,\"start\":34762},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":34902,\"start\":34894},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35009,\"start\":34989},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35079,\"start\":35067},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":35514,\"start\":35500},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36472,\"start\":36465},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36835,\"start\":36828},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":36963,\"start\":36941},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37193,\"start\":37170},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37621,\"start\":37614},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":37801,\"start\":37792},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38139,\"start\":38132},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":38454,\"start\":38445}]", "bib_author_first_name": "[{\"end\":44867,\"start\":44866},{\"end\":44877,\"start\":44876},{\"end\":44884,\"start\":44883},{\"end\":44886,\"start\":44885},{\"end\":44895,\"start\":44894},{\"end\":45134,\"start\":45133},{\"end\":45144,\"start\":45143},{\"end\":45153,\"start\":45152},{\"end\":45155,\"start\":45154},{\"end\":45164,\"start\":45163},{\"end\":45173,\"start\":45172},{\"end\":45185,\"start\":45184},{\"end\":45196,\"start\":45195},{\"end\":45656,\"start\":45655},{\"end\":45666,\"start\":45665},{\"end\":45677,\"start\":45676},{\"end\":46066,\"start\":46065},{\"end\":46074,\"start\":46073},{\"end\":46082,\"start\":46081},{\"end\":46244,\"start\":46243},{\"end\":46255,\"start\":46251},{\"end\":46263,\"start\":46262},{\"end\":46273,\"start\":46272},{\"end\":46551,\"start\":46550},{\"end\":46561,\"start\":46560},{\"end\":46840,\"start\":46839},{\"end\":46847,\"start\":46846},{\"end\":46849,\"start\":46848},{\"end\":46858,\"start\":46857},{\"end\":46867,\"start\":46866},{\"end\":46877,\"start\":46876},{\"end\":46891,\"start\":46890},{\"end\":47183,\"start\":47182},{\"end\":47190,\"start\":47189},{\"end\":47201,\"start\":47200},{\"end\":47213,\"start\":47212},{\"end\":47222,\"start\":47221},{\"end\":47591,\"start\":47590},{\"end\":47600,\"start\":47599},{\"end\":48002,\"start\":48001},{\"end\":48284,\"start\":48283},{\"end\":48286,\"start\":48285},{\"end\":48296,\"start\":48295},{\"end\":48305,\"start\":48304},{\"end\":48469,\"start\":48468},{\"end\":48471,\"start\":48470},{\"end\":48481,\"start\":48480},{\"end\":48490,\"start\":48489},{\"end\":48773,\"start\":48772},{\"end\":48782,\"start\":48781},{\"end\":48794,\"start\":48793},{\"end\":49267,\"start\":49266},{\"end\":49276,\"start\":49275},{\"end\":49288,\"start\":49287},{\"end\":49300,\"start\":49299},{\"end\":49540,\"start\":49539},{\"end\":49550,\"start\":49549},{\"end\":49742,\"start\":49741},{\"end\":49749,\"start\":49748},{\"end\":49758,\"start\":49757},{\"end\":49765,\"start\":49764},{\"end\":49779,\"start\":49778},{\"end\":49781,\"start\":49780},{\"end\":50238,\"start\":50237},{\"end\":50247,\"start\":50246},{\"end\":50261,\"start\":50260},{\"end\":50279,\"start\":50278},{\"end\":50288,\"start\":50287},{\"end\":50555,\"start\":50554},{\"end\":50561,\"start\":50560},{\"end\":50570,\"start\":50569},{\"end\":50577,\"start\":50576},{\"end\":50972,\"start\":50971},{\"end\":50982,\"start\":50981},{\"end\":50992,\"start\":50991},{\"end\":51218,\"start\":51214},{\"end\":51228,\"start\":51224},{\"end\":51236,\"start\":51235},{\"end\":51238,\"start\":51237},{\"end\":51251,\"start\":51247},{\"end\":51262,\"start\":51258},{\"end\":51271,\"start\":51267},{\"end\":51549,\"start\":51548},{\"end\":51560,\"start\":51559},{\"end\":51571,\"start\":51570},{\"end\":51939,\"start\":51938},{\"end\":51952,\"start\":51951},{\"end\":51963,\"start\":51959},{\"end\":51971,\"start\":51970},{\"end\":52291,\"start\":52290},{\"end\":52297,\"start\":52296},{\"end\":52305,\"start\":52304},{\"end\":52312,\"start\":52311},{\"end\":52330,\"start\":52329},{\"end\":52826,\"start\":52825},{\"end\":52833,\"start\":52832},{\"end\":52843,\"start\":52842},{\"end\":53242,\"start\":53241},{\"end\":53253,\"start\":53252},{\"end\":53266,\"start\":53265},{\"end\":53275,\"start\":53274},{\"end\":53498,\"start\":53497},{\"end\":53509,\"start\":53508},{\"end\":53521,\"start\":53520},{\"end\":53530,\"start\":53529},{\"end\":53540,\"start\":53539},{\"end\":53940,\"start\":53939},{\"end\":53947,\"start\":53946},{\"end\":53953,\"start\":53952},{\"end\":54283,\"start\":54282},{\"end\":54292,\"start\":54291},{\"end\":54294,\"start\":54293},{\"end\":54303,\"start\":54302},{\"end\":54315,\"start\":54314},{\"end\":54325,\"start\":54324},{\"end\":54617,\"start\":54616},{\"end\":54628,\"start\":54627},{\"end\":54640,\"start\":54639},{\"end\":54931,\"start\":54930},{\"end\":54942,\"start\":54941},{\"end\":54953,\"start\":54952},{\"end\":54961,\"start\":54960},{\"end\":54970,\"start\":54969},{\"end\":54983,\"start\":54982},{\"end\":55457,\"start\":55456},{\"end\":55472,\"start\":55471},{\"end\":55844,\"start\":55843},{\"end\":55857,\"start\":55856},{\"end\":56189,\"start\":56188},{\"end\":56202,\"start\":56201},{\"end\":56211,\"start\":56210},{\"end\":56220,\"start\":56219},{\"end\":56502,\"start\":56501},{\"end\":56516,\"start\":56515},{\"end\":56526,\"start\":56525},{\"end\":56536,\"start\":56535},{\"end\":56548,\"start\":56547},{\"end\":56555,\"start\":56554},{\"end\":56999,\"start\":56998},{\"end\":57007,\"start\":57006},{\"end\":57009,\"start\":57008},{\"end\":57024,\"start\":57023},{\"end\":57432,\"start\":57431},{\"end\":57440,\"start\":57439},{\"end\":57716,\"start\":57715},{\"end\":57724,\"start\":57723},{\"end\":57881,\"start\":57880},{\"end\":57889,\"start\":57888},{\"end\":57895,\"start\":57894},{\"end\":57903,\"start\":57902},{\"end\":57905,\"start\":57904},{\"end\":57914,\"start\":57913},{\"end\":57923,\"start\":57922},{\"end\":58170,\"start\":58169},{\"end\":58182,\"start\":58181},{\"end\":58189,\"start\":58188},{\"end\":58200,\"start\":58199},{\"end\":58209,\"start\":58208},{\"end\":58217,\"start\":58216},{\"end\":58226,\"start\":58225},{\"end\":58499,\"start\":58498},{\"end\":58511,\"start\":58510},{\"end\":58524,\"start\":58520},{\"end\":58533,\"start\":58532},{\"end\":58540,\"start\":58539},{\"end\":58551,\"start\":58550},{\"end\":58560,\"start\":58559},{\"end\":58571,\"start\":58570},{\"end\":58584,\"start\":58583},{\"end\":58593,\"start\":58592},{\"end\":58944,\"start\":58943},{\"end\":58952,\"start\":58951},{\"end\":58962,\"start\":58961},{\"end\":59375,\"start\":59374},{\"end\":59383,\"start\":59382},{\"end\":59385,\"start\":59384},{\"end\":59396,\"start\":59395},{\"end\":59405,\"start\":59404},{\"end\":59782,\"start\":59781},{\"end\":59790,\"start\":59789},{\"end\":59798,\"start\":59797},{\"end\":59800,\"start\":59799},{\"end\":59811,\"start\":59810},{\"end\":59820,\"start\":59819},{\"end\":60213,\"start\":60212},{\"end\":60221,\"start\":60220},{\"end\":60230,\"start\":60229},{\"end\":60615,\"start\":60614},{\"end\":60617,\"start\":60616},{\"end\":60623,\"start\":60622},{\"end\":60633,\"start\":60632},{\"end\":60644,\"start\":60643},{\"end\":60940,\"start\":60939},{\"end\":60948,\"start\":60947},{\"end\":60956,\"start\":60955},{\"end\":60968,\"start\":60967},{\"end\":60978,\"start\":60977},{\"end\":60986,\"start\":60985},{\"end\":61451,\"start\":61450},{\"end\":61460,\"start\":61459},{\"end\":61467,\"start\":61466},{\"end\":61469,\"start\":61468},{\"end\":61480,\"start\":61479},{\"end\":61932,\"start\":61931},{\"end\":61941,\"start\":61940},{\"end\":61949,\"start\":61948},{\"end\":61958,\"start\":61957},{\"end\":61970,\"start\":61966},{\"end\":61977,\"start\":61976},{\"end\":61984,\"start\":61983},{\"end\":62308,\"start\":62307},{\"end\":62316,\"start\":62315},{\"end\":62329,\"start\":62328},{\"end\":62337,\"start\":62336},{\"end\":62349,\"start\":62348}]", "bib_author_last_name": "[{\"end\":44874,\"start\":44868},{\"end\":44881,\"start\":44878},{\"end\":44892,\"start\":44887},{\"end\":44904,\"start\":44896},{\"end\":45141,\"start\":45135},{\"end\":45150,\"start\":45145},{\"end\":45161,\"start\":45156},{\"end\":45170,\"start\":45165},{\"end\":45182,\"start\":45174},{\"end\":45193,\"start\":45186},{\"end\":45205,\"start\":45197},{\"end\":45663,\"start\":45657},{\"end\":45674,\"start\":45667},{\"end\":45683,\"start\":45678},{\"end\":46071,\"start\":46067},{\"end\":46079,\"start\":46075},{\"end\":46090,\"start\":46083},{\"end\":46249,\"start\":46245},{\"end\":46260,\"start\":46256},{\"end\":46270,\"start\":46264},{\"end\":46280,\"start\":46274},{\"end\":46558,\"start\":46552},{\"end\":46569,\"start\":46562},{\"end\":46844,\"start\":46841},{\"end\":46855,\"start\":46850},{\"end\":46864,\"start\":46859},{\"end\":46874,\"start\":46868},{\"end\":46888,\"start\":46878},{\"end\":46899,\"start\":46892},{\"end\":47187,\"start\":47184},{\"end\":47198,\"start\":47191},{\"end\":47210,\"start\":47202},{\"end\":47219,\"start\":47214},{\"end\":47231,\"start\":47223},{\"end\":47597,\"start\":47592},{\"end\":47607,\"start\":47601},{\"end\":48009,\"start\":48003},{\"end\":48293,\"start\":48287},{\"end\":48302,\"start\":48297},{\"end\":48312,\"start\":48306},{\"end\":48478,\"start\":48472},{\"end\":48487,\"start\":48482},{\"end\":48497,\"start\":48491},{\"end\":48779,\"start\":48774},{\"end\":48791,\"start\":48783},{\"end\":48800,\"start\":48795},{\"end\":49273,\"start\":49268},{\"end\":49285,\"start\":49277},{\"end\":49297,\"start\":49289},{\"end\":49306,\"start\":49301},{\"end\":49547,\"start\":49541},{\"end\":49561,\"start\":49551},{\"end\":49746,\"start\":49743},{\"end\":49755,\"start\":49750},{\"end\":49762,\"start\":49759},{\"end\":49776,\"start\":49766},{\"end\":49786,\"start\":49782},{\"end\":50244,\"start\":50239},{\"end\":50258,\"start\":50248},{\"end\":50276,\"start\":50262},{\"end\":50285,\"start\":50280},{\"end\":50296,\"start\":50289},{\"end\":50558,\"start\":50556},{\"end\":50567,\"start\":50562},{\"end\":50574,\"start\":50571},{\"end\":50581,\"start\":50578},{\"end\":50979,\"start\":50973},{\"end\":50989,\"start\":50983},{\"end\":50998,\"start\":50993},{\"end\":51222,\"start\":51219},{\"end\":51233,\"start\":51229},{\"end\":51245,\"start\":51239},{\"end\":51256,\"start\":51252},{\"end\":51265,\"start\":51263},{\"end\":51277,\"start\":51272},{\"end\":51557,\"start\":51550},{\"end\":51568,\"start\":51561},{\"end\":51577,\"start\":51572},{\"end\":51949,\"start\":51940},{\"end\":51957,\"start\":51953},{\"end\":51968,\"start\":51964},{\"end\":51978,\"start\":51972},{\"end\":52294,\"start\":52292},{\"end\":52302,\"start\":52298},{\"end\":52309,\"start\":52306},{\"end\":52327,\"start\":52313},{\"end\":52333,\"start\":52331},{\"end\":52830,\"start\":52827},{\"end\":52840,\"start\":52834},{\"end\":52851,\"start\":52844},{\"end\":53250,\"start\":53243},{\"end\":53263,\"start\":53254},{\"end\":53272,\"start\":53267},{\"end\":53286,\"start\":53276},{\"end\":53506,\"start\":53499},{\"end\":53518,\"start\":53510},{\"end\":53527,\"start\":53522},{\"end\":53537,\"start\":53531},{\"end\":53551,\"start\":53541},{\"end\":53944,\"start\":53941},{\"end\":53950,\"start\":53948},{\"end\":53957,\"start\":53954},{\"end\":54289,\"start\":54284},{\"end\":54300,\"start\":54295},{\"end\":54312,\"start\":54304},{\"end\":54322,\"start\":54316},{\"end\":54333,\"start\":54326},{\"end\":54625,\"start\":54618},{\"end\":54637,\"start\":54629},{\"end\":54644,\"start\":54641},{\"end\":54939,\"start\":54932},{\"end\":54950,\"start\":54943},{\"end\":54958,\"start\":54954},{\"end\":54967,\"start\":54962},{\"end\":54980,\"start\":54971},{\"end\":54994,\"start\":54984},{\"end\":55469,\"start\":55458},{\"end\":55478,\"start\":55473},{\"end\":55854,\"start\":55845},{\"end\":55864,\"start\":55858},{\"end\":56199,\"start\":56190},{\"end\":56208,\"start\":56203},{\"end\":56217,\"start\":56212},{\"end\":56227,\"start\":56221},{\"end\":56513,\"start\":56503},{\"end\":56523,\"start\":56517},{\"end\":56533,\"start\":56527},{\"end\":56545,\"start\":56537},{\"end\":56552,\"start\":56549},{\"end\":56569,\"start\":56556},{\"end\":57004,\"start\":57000},{\"end\":57021,\"start\":57010},{\"end\":57029,\"start\":57025},{\"end\":57437,\"start\":57433},{\"end\":57445,\"start\":57441},{\"end\":57721,\"start\":57717},{\"end\":57729,\"start\":57725},{\"end\":57886,\"start\":57882},{\"end\":57892,\"start\":57890},{\"end\":57900,\"start\":57896},{\"end\":57911,\"start\":57906},{\"end\":57920,\"start\":57915},{\"end\":57934,\"start\":57924},{\"end\":58179,\"start\":58171},{\"end\":58186,\"start\":58183},{\"end\":58197,\"start\":58190},{\"end\":58206,\"start\":58201},{\"end\":58214,\"start\":58210},{\"end\":58223,\"start\":58218},{\"end\":58233,\"start\":58227},{\"end\":58508,\"start\":58500},{\"end\":58518,\"start\":58512},{\"end\":58530,\"start\":58525},{\"end\":58537,\"start\":58534},{\"end\":58548,\"start\":58541},{\"end\":58557,\"start\":58552},{\"end\":58568,\"start\":58561},{\"end\":58581,\"start\":58572},{\"end\":58590,\"start\":58585},{\"end\":58598,\"start\":58594},{\"end\":58949,\"start\":58945},{\"end\":58959,\"start\":58953},{\"end\":58968,\"start\":58963},{\"end\":59380,\"start\":59376},{\"end\":59393,\"start\":59386},{\"end\":59402,\"start\":59397},{\"end\":59414,\"start\":59406},{\"end\":59787,\"start\":59783},{\"end\":59795,\"start\":59791},{\"end\":59808,\"start\":59801},{\"end\":59817,\"start\":59812},{\"end\":59829,\"start\":59821},{\"end\":60218,\"start\":60214},{\"end\":60227,\"start\":60222},{\"end\":60239,\"start\":60231},{\"end\":60620,\"start\":60618},{\"end\":60630,\"start\":60624},{\"end\":60641,\"start\":60634},{\"end\":60648,\"start\":60645},{\"end\":60945,\"start\":60941},{\"end\":60953,\"start\":60949},{\"end\":60965,\"start\":60957},{\"end\":60975,\"start\":60969},{\"end\":60983,\"start\":60979},{\"end\":60997,\"start\":60987},{\"end\":61457,\"start\":61452},{\"end\":61464,\"start\":61461},{\"end\":61477,\"start\":61470},{\"end\":61488,\"start\":61481},{\"end\":61938,\"start\":61933},{\"end\":61946,\"start\":61942},{\"end\":61955,\"start\":61950},{\"end\":61964,\"start\":61959},{\"end\":61974,\"start\":61971},{\"end\":61981,\"start\":61978},{\"end\":61995,\"start\":61985},{\"end\":62313,\"start\":62309},{\"end\":62326,\"start\":62317},{\"end\":62334,\"start\":62330},{\"end\":62346,\"start\":62338},{\"end\":62355,\"start\":62350}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1702.01105\",\"id\":\"b0\"},\"end\":45082,\"start\":44808},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":9649070},\"end\":45590,\"start\":45084},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3345396},\"end\":46033,\"start\":45592},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":495068},\"end\":46241,\"start\":46035},{\"attributes\":{\"doi\":\"arXiv:1602.02481\",\"id\":\"b4\"},\"end\":46463,\"start\":46243},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":2363667},\"end\":46774,\"start\":46465},{\"attributes\":{\"id\":\"b6\"},\"end\":47077,\"start\":46776},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":32286806},\"end\":47480,\"start\":47079},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":102496818},\"end\":47958,\"start\":47482},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":18238681},\"end\":48223,\"start\":47960},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":598952},\"end\":48431,\"start\":48225},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":9407131},\"end\":48694,\"start\":48433},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":12061055},\"end\":49160,\"start\":48696},{\"attributes\":{\"id\":\"b13\"},\"end\":49467,\"start\":49162},{\"attributes\":{\"id\":\"b14\"},\"end\":49667,\"start\":49469},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":15179402},\"end\":50165,\"start\":49669},{\"attributes\":{\"doi\":\"arXiv:1511.07041\",\"id\":\"b16\"},\"end\":50506,\"start\":50167},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":206594692},\"end\":50913,\"start\":50508},{\"attributes\":{\"doi\":\"arXiv:1610.00243\",\"id\":\"b18\"},\"end\":51162,\"start\":50915},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":11245438},\"end\":51514,\"start\":51164},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":14224},\"end\":51870,\"start\":51516},{\"attributes\":{\"id\":\"b21\"},\"end\":52173,\"start\":51872},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":206592782},\"end\":52751,\"start\":52175},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":7188439},\"end\":53181,\"start\":52753},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":207207064},\"end\":53472,\"start\":53183},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":3184707},\"end\":53888,\"start\":53474},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":15174950},\"end\":54220,\"start\":53890},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":7509088},\"end\":54545,\"start\":54222},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":2842187},\"end\":54849,\"start\":54547},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":8632684},\"end\":55400,\"start\":54851},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":6183017},\"end\":55782,\"start\":55402},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":13993169},\"end\":56126,\"start\":55784},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":545361},\"end\":56426,\"start\":56128},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":12646079},\"end\":56940,\"start\":56428},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":6242669},\"end\":57373,\"start\":56942},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":206685792},\"end\":57645,\"start\":57375},{\"attributes\":{\"id\":\"b36\"},\"end\":57825,\"start\":57647},{\"attributes\":{\"doi\":\"arXiv:1611.08974\",\"id\":\"b37\"},\"end\":58124,\"start\":57827},{\"attributes\":{\"doi\":\"arXiv:1603.05772\",\"id\":\"b38\"},\"end\":58423,\"start\":58126},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":9568132},\"end\":58886,\"start\":58425},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":9363077},\"end\":59306,\"start\":58888},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":14378881},\"end\":59716,\"start\":59308},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":1309931},\"end\":60135,\"start\":59718},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":6033252},\"end\":60569,\"start\":60137},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":602850},\"end\":60863,\"start\":60571},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":11446141},\"end\":61370,\"start\":60865},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":8190353},\"end\":61834,\"start\":61372},{\"attributes\":{\"doi\":\"arXiv:1612.07429\",\"id\":\"b47\"},\"end\":62237,\"start\":61836},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":1849990},\"end\":62611,\"start\":62239}]", "bib_title": "[{\"end\":45131,\"start\":45084},{\"end\":45653,\"start\":45592},{\"end\":46063,\"start\":46035},{\"end\":46548,\"start\":46465},{\"end\":47180,\"start\":47079},{\"end\":47588,\"start\":47482},{\"end\":47999,\"start\":47960},{\"end\":48281,\"start\":48225},{\"end\":48466,\"start\":48433},{\"end\":48770,\"start\":48696},{\"end\":49739,\"start\":49669},{\"end\":50552,\"start\":50508},{\"end\":51212,\"start\":51164},{\"end\":51546,\"start\":51516},{\"end\":51936,\"start\":51872},{\"end\":52288,\"start\":52175},{\"end\":52823,\"start\":52753},{\"end\":53239,\"start\":53183},{\"end\":53495,\"start\":53474},{\"end\":53937,\"start\":53890},{\"end\":54280,\"start\":54222},{\"end\":54614,\"start\":54547},{\"end\":54928,\"start\":54851},{\"end\":55454,\"start\":55402},{\"end\":55841,\"start\":55784},{\"end\":56186,\"start\":56128},{\"end\":56499,\"start\":56428},{\"end\":56996,\"start\":56942},{\"end\":57429,\"start\":57375},{\"end\":58496,\"start\":58425},{\"end\":58941,\"start\":58888},{\"end\":59372,\"start\":59308},{\"end\":59779,\"start\":59718},{\"end\":60210,\"start\":60137},{\"end\":60612,\"start\":60571},{\"end\":60937,\"start\":60865},{\"end\":61448,\"start\":61372},{\"end\":62305,\"start\":62239}]", "bib_author": "[{\"end\":44876,\"start\":44866},{\"end\":44883,\"start\":44876},{\"end\":44894,\"start\":44883},{\"end\":44906,\"start\":44894},{\"end\":45143,\"start\":45133},{\"end\":45152,\"start\":45143},{\"end\":45163,\"start\":45152},{\"end\":45172,\"start\":45163},{\"end\":45184,\"start\":45172},{\"end\":45195,\"start\":45184},{\"end\":45207,\"start\":45195},{\"end\":45665,\"start\":45655},{\"end\":45676,\"start\":45665},{\"end\":45685,\"start\":45676},{\"end\":46073,\"start\":46065},{\"end\":46081,\"start\":46073},{\"end\":46092,\"start\":46081},{\"end\":46251,\"start\":46243},{\"end\":46262,\"start\":46251},{\"end\":46272,\"start\":46262},{\"end\":46282,\"start\":46272},{\"end\":46560,\"start\":46550},{\"end\":46571,\"start\":46560},{\"end\":46846,\"start\":46839},{\"end\":46857,\"start\":46846},{\"end\":46866,\"start\":46857},{\"end\":46876,\"start\":46866},{\"end\":46890,\"start\":46876},{\"end\":46901,\"start\":46890},{\"end\":47189,\"start\":47182},{\"end\":47200,\"start\":47189},{\"end\":47212,\"start\":47200},{\"end\":47221,\"start\":47212},{\"end\":47233,\"start\":47221},{\"end\":47599,\"start\":47590},{\"end\":47609,\"start\":47599},{\"end\":48011,\"start\":48001},{\"end\":48295,\"start\":48283},{\"end\":48304,\"start\":48295},{\"end\":48314,\"start\":48304},{\"end\":48480,\"start\":48468},{\"end\":48489,\"start\":48480},{\"end\":48499,\"start\":48489},{\"end\":48781,\"start\":48772},{\"end\":48793,\"start\":48781},{\"end\":48802,\"start\":48793},{\"end\":49275,\"start\":49266},{\"end\":49287,\"start\":49275},{\"end\":49299,\"start\":49287},{\"end\":49308,\"start\":49299},{\"end\":49549,\"start\":49539},{\"end\":49563,\"start\":49549},{\"end\":49748,\"start\":49741},{\"end\":49757,\"start\":49748},{\"end\":49764,\"start\":49757},{\"end\":49778,\"start\":49764},{\"end\":49788,\"start\":49778},{\"end\":50246,\"start\":50237},{\"end\":50260,\"start\":50246},{\"end\":50278,\"start\":50260},{\"end\":50287,\"start\":50278},{\"end\":50298,\"start\":50287},{\"end\":50560,\"start\":50554},{\"end\":50569,\"start\":50560},{\"end\":50576,\"start\":50569},{\"end\":50583,\"start\":50576},{\"end\":50981,\"start\":50971},{\"end\":50991,\"start\":50981},{\"end\":51000,\"start\":50991},{\"end\":51224,\"start\":51214},{\"end\":51235,\"start\":51224},{\"end\":51247,\"start\":51235},{\"end\":51258,\"start\":51247},{\"end\":51267,\"start\":51258},{\"end\":51279,\"start\":51267},{\"end\":51559,\"start\":51548},{\"end\":51570,\"start\":51559},{\"end\":51579,\"start\":51570},{\"end\":51951,\"start\":51938},{\"end\":51959,\"start\":51951},{\"end\":51970,\"start\":51959},{\"end\":51980,\"start\":51970},{\"end\":52296,\"start\":52290},{\"end\":52304,\"start\":52296},{\"end\":52311,\"start\":52304},{\"end\":52329,\"start\":52311},{\"end\":52335,\"start\":52329},{\"end\":52832,\"start\":52825},{\"end\":52842,\"start\":52832},{\"end\":52853,\"start\":52842},{\"end\":53252,\"start\":53241},{\"end\":53265,\"start\":53252},{\"end\":53274,\"start\":53265},{\"end\":53288,\"start\":53274},{\"end\":53508,\"start\":53497},{\"end\":53520,\"start\":53508},{\"end\":53529,\"start\":53520},{\"end\":53539,\"start\":53529},{\"end\":53553,\"start\":53539},{\"end\":53946,\"start\":53939},{\"end\":53952,\"start\":53946},{\"end\":53959,\"start\":53952},{\"end\":54291,\"start\":54282},{\"end\":54302,\"start\":54291},{\"end\":54314,\"start\":54302},{\"end\":54324,\"start\":54314},{\"end\":54335,\"start\":54324},{\"end\":54627,\"start\":54616},{\"end\":54639,\"start\":54627},{\"end\":54646,\"start\":54639},{\"end\":54941,\"start\":54930},{\"end\":54952,\"start\":54941},{\"end\":54960,\"start\":54952},{\"end\":54969,\"start\":54960},{\"end\":54982,\"start\":54969},{\"end\":54996,\"start\":54982},{\"end\":55471,\"start\":55456},{\"end\":55480,\"start\":55471},{\"end\":55856,\"start\":55843},{\"end\":55866,\"start\":55856},{\"end\":56201,\"start\":56188},{\"end\":56210,\"start\":56201},{\"end\":56219,\"start\":56210},{\"end\":56229,\"start\":56219},{\"end\":56515,\"start\":56501},{\"end\":56525,\"start\":56515},{\"end\":56535,\"start\":56525},{\"end\":56547,\"start\":56535},{\"end\":56554,\"start\":56547},{\"end\":56571,\"start\":56554},{\"end\":57006,\"start\":56998},{\"end\":57023,\"start\":57006},{\"end\":57031,\"start\":57023},{\"end\":57439,\"start\":57431},{\"end\":57447,\"start\":57439},{\"end\":57723,\"start\":57715},{\"end\":57731,\"start\":57723},{\"end\":57888,\"start\":57880},{\"end\":57894,\"start\":57888},{\"end\":57902,\"start\":57894},{\"end\":57913,\"start\":57902},{\"end\":57922,\"start\":57913},{\"end\":57936,\"start\":57922},{\"end\":58181,\"start\":58169},{\"end\":58188,\"start\":58181},{\"end\":58199,\"start\":58188},{\"end\":58208,\"start\":58199},{\"end\":58216,\"start\":58208},{\"end\":58225,\"start\":58216},{\"end\":58235,\"start\":58225},{\"end\":58510,\"start\":58498},{\"end\":58520,\"start\":58510},{\"end\":58532,\"start\":58520},{\"end\":58539,\"start\":58532},{\"end\":58550,\"start\":58539},{\"end\":58559,\"start\":58550},{\"end\":58570,\"start\":58559},{\"end\":58583,\"start\":58570},{\"end\":58592,\"start\":58583},{\"end\":58600,\"start\":58592},{\"end\":58951,\"start\":58943},{\"end\":58961,\"start\":58951},{\"end\":58970,\"start\":58961},{\"end\":59382,\"start\":59374},{\"end\":59395,\"start\":59382},{\"end\":59404,\"start\":59395},{\"end\":59416,\"start\":59404},{\"end\":59789,\"start\":59781},{\"end\":59797,\"start\":59789},{\"end\":59810,\"start\":59797},{\"end\":59819,\"start\":59810},{\"end\":59831,\"start\":59819},{\"end\":60220,\"start\":60212},{\"end\":60229,\"start\":60220},{\"end\":60241,\"start\":60229},{\"end\":60622,\"start\":60614},{\"end\":60632,\"start\":60622},{\"end\":60643,\"start\":60632},{\"end\":60650,\"start\":60643},{\"end\":60947,\"start\":60939},{\"end\":60955,\"start\":60947},{\"end\":60967,\"start\":60955},{\"end\":60977,\"start\":60967},{\"end\":60985,\"start\":60977},{\"end\":60999,\"start\":60985},{\"end\":61459,\"start\":61450},{\"end\":61466,\"start\":61459},{\"end\":61479,\"start\":61466},{\"end\":61490,\"start\":61479},{\"end\":61940,\"start\":61931},{\"end\":61948,\"start\":61940},{\"end\":61957,\"start\":61948},{\"end\":61966,\"start\":61957},{\"end\":61976,\"start\":61966},{\"end\":61983,\"start\":61976},{\"end\":61997,\"start\":61983},{\"end\":62315,\"start\":62307},{\"end\":62328,\"start\":62315},{\"end\":62336,\"start\":62328},{\"end\":62348,\"start\":62336},{\"end\":62357,\"start\":62348}]", "bib_venue": "[{\"end\":44864,\"start\":44808},{\"end\":45284,\"start\":45207},{\"end\":45762,\"start\":45685},{\"end\":46125,\"start\":46092},{\"end\":46329,\"start\":46298},{\"end\":46605,\"start\":46571},{\"end\":46837,\"start\":46776},{\"end\":47261,\"start\":47233},{\"end\":47676,\"start\":47609},{\"end\":48084,\"start\":48011},{\"end\":48318,\"start\":48314},{\"end\":48537,\"start\":48499},{\"end\":48879,\"start\":48802},{\"end\":49264,\"start\":49162},{\"end\":49537,\"start\":49469},{\"end\":49865,\"start\":49788},{\"end\":50235,\"start\":50167},{\"end\":50660,\"start\":50583},{\"end\":50969,\"start\":50915},{\"end\":51322,\"start\":51279},{\"end\":51650,\"start\":51579},{\"end\":52008,\"start\":51980},{\"end\":52412,\"start\":52335},{\"end\":52920,\"start\":52853},{\"end\":53316,\"start\":53288},{\"end\":53630,\"start\":53553},{\"end\":54030,\"start\":53959},{\"end\":54369,\"start\":54335},{\"end\":54682,\"start\":54646},{\"end\":55073,\"start\":54996},{\"end\":55547,\"start\":55480},{\"end\":55907,\"start\":55866},{\"end\":56267,\"start\":56229},{\"end\":56638,\"start\":56571},{\"end\":57108,\"start\":57031},{\"end\":57485,\"start\":57447},{\"end\":57713,\"start\":57647},{\"end\":57878,\"start\":57827},{\"end\":58167,\"start\":58126},{\"end\":58634,\"start\":58600},{\"end\":59047,\"start\":58970},{\"end\":59487,\"start\":59416},{\"end\":59902,\"start\":59831},{\"end\":60308,\"start\":60241},{\"end\":60688,\"start\":60650},{\"end\":61076,\"start\":60999},{\"end\":61557,\"start\":61490},{\"end\":61929,\"start\":61836},{\"end\":62406,\"start\":62357},{\"end\":45348,\"start\":45286},{\"end\":45826,\"start\":45764},{\"end\":47730,\"start\":47678},{\"end\":48943,\"start\":48881},{\"end\":49929,\"start\":49867},{\"end\":50724,\"start\":50662},{\"end\":51708,\"start\":51652},{\"end\":52476,\"start\":52414},{\"end\":52974,\"start\":52922},{\"end\":53694,\"start\":53632},{\"end\":55137,\"start\":55075},{\"end\":55601,\"start\":55549},{\"end\":56692,\"start\":56640},{\"end\":57172,\"start\":57110},{\"end\":59111,\"start\":59049},{\"end\":60362,\"start\":60310},{\"end\":61140,\"start\":61078},{\"end\":61611,\"start\":61559}]"}}}, "year": 2023, "month": 12, "day": 17}