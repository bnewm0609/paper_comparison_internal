{"id": 234327665, "updated": "2022-09-29 22:31:27.388", "metadata": {"title": "Tropical Cyclone Intensity Estimation From Geostationary Satellite Imagery Using Deep Convolutional Neural Networks", "authors": "[{\"first\":\"Chong\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Gang\",\"last\":\"Zheng\",\"middle\":[]},{\"first\":\"Xiaofeng\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Qing\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Bin\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Jun\",\"last\":\"Zhang\",\"middle\":[]}]", "venue": "IEEE Transactions on Geoscience and Remote Sensing", "journal": "IEEE Transactions on Geoscience and Remote Sensing", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "In this study, a set of deep convolutional neural networks (CNNs) was designed for estimating the intensity of tropical cyclones (TCs) over the Northwest Pacific Ocean from the brightness temperature data observed by the Advanced Himawari Imager onboard the Himawari-8 geostationary satellite. We used 97 TC cases from 2015 to 2018 to train the CNN models. Several models with different inputs and parameters are designed. A comparative study showed that the selection of different infrared (IR) channels has a significant impact on the performance of the TC intensity estimate from the CNN models. Compared with the ground truth Best Track data of the maximum sustained wind speed, with a combination of four channels of data as input, the best multicategory CNN classification model has generated a fairly good accuracy (84.8%) and low root mean square error (RMSE, 5.24 m/s) and mean bias (\u22122.15 m/s) in TC intensity estimation. Adding attention layers after the input layer in the CNN helps to improve the model accuracy. The model is quite stable even with the influence of image noise. To reduce the side-effect of the very unbalanced distribution of TC category samples, we introduced a focal_loss function into the CNN model. After we transformed the multiclassification problem into a binary classification problem, the accuracy increased to 88.9%, and the RMSE and the mean bias are significantly reduced to 4.62 and \u22120.76 m/s, respectively. The results show that our CNN models are robust in estimating TC intensity from geostationary satellite images.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3143274035", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tgrs/WangZLXLZ22", "doi": "10.1109/tgrs.2021.3066299"}}, "content": {"source": {"pdf_hash": "e3dfe61854854e0eb4a5a6a0e3f1b4de1f36e617", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "74d085b98025f7f0cc80e1bed3c71b8ea4ea8f3b", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/e3dfe61854854e0eb4a5a6a0e3f1b4de1f36e617.txt", "contents": "\nTropical Cyclone Intensity Estimation From Geostationary Satellite Imagery Using Deep Convolutional Neural Networks\n2022\n\nGang Zheng , Senior Member, IEEEChong Wang \nFellow, IEEEXiaofeng Li xiaofeng.li@ieee.org. \nMember, IEEEQing Xu \nMember, IEEEBin Liu \nSenior Member, IEEEJun Zhang \n\nMinistry of Natural Resources\nMinistry of Natural Resources\nHohai University\n210098, 266071Nanjing, QingdaoChina., China\n\n\nHohai University\n210098NanjingChina\n\n\nUniversity of China\n266100QingdaoChina\n\n\nShanghai Ocean University\n201306ShanghaiChina\n\nTropical Cyclone Intensity Estimation From Geostationary Satellite Imagery Using Deep Convolutional Neural Networks\n\nIEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING\n604101416202210.1109/TGRS.2021.3066299received July 15, 2020; revised October 3, 2020 and December 10, 2020; accepted March 11, 2021. Date of publication March 26, 2021; date of current version December 13, 2021.the Zhejiang Provincial Natural Science Foundation of China under Grant LR21D060002. (Corresponding authors: Qing Xu; Xiaofeng Li.) Chong Wang is with the Key Laboratory of Marine Hazards Forecasting, Qing Xu was with the Key Laboratory of Marine Hazards Forecasting, She is now with the College of Information Science and Engineering, Ocean Bin Liu is with the School of Marine Sciences,\nIn this study, a set of deep convolutional neural networks (CNNs) was designed for estimating the intensity of tropical cyclones (TCs) over the Northwest Pacific Ocean from the brightness temperature data observed by the Advanced Himawari Imager onboard the Himawari-8 geostationary satellite. We used 97 TC cases from 2015 to 2018 to train the CNN models. Several models with different inputs and parameters are designed. A comparative study showed that the selection of different infrared (IR) channels has a significant impact on the performance of the TC intensity estimate from the CNN models. Compared with the ground truth Best Track data of the maximum sustained wind speed, with a combination of four channels of data as input, the best multicategory CNN classification model has generated a fairly good accuracy (84.8%) and low root mean square error (RMSE, 5.24 m/s) and mean bias (\u22122.15 m/s) in TC intensity estimation. Adding attention layers after the input layer in the CNN helps to improve the model accuracy. The model is quite stable even with theManuscript influence of image noise. To reduce the side-effect of the very unbalanced distribution of TC category samples, we introduced a focal_loss function into the CNN model. After we transformed the multiclassification problem into a binary classification problem, the accuracy increased to 88.9%, and the RMSE and the mean bias are significantly reduced to 4.62 and \u22120.76 m/s, respectively. The results show that our CNN models are robust in estimating TC intensity from geostationary satellite images.Index Terms-Convolutional neural network (CNN), deep learning, remote sensing, tropical cyclone (TC).1558-0644\n\nI. INTRODUCTION\n\nT ROPICAL cyclones (TCs) can cause enormous damage to human lives and properties owing to the destructive winds, severe flooding, and coastal inundation from storm surges. The accurate estimation of a TC intensity is essential for forecasters and emergency responders. Different types of satellite remote sensing technologies have provided many practical methods for TC monitoring [1]- [8]. Operational forecast centers apply a variety of data sets to estimate TC intensity. Among the most frequently applied data sets are intensity estimates derived from the traditional, subjective Dvorak technique [9] and automated Dvorak algorithms [10], both of which rely on infrared (IR) satellite imagery. Other algorithms used to derive TC intensity from IR satellite data include the deviation-angle variance technique (DAVT) [5], [13], [14] and the convolutional neural network (CNN) [15]- [19], which are still in scientific research. Additional widely used methods to estimate TC intensity from satellite data include microwave sounder-based methods [20]- [22] and consensus methods [23], among others.\n\nThe classic Dvorak method [9] relates a TC rotation, eye shape, and deep thunderstorms to its enhancement or weakening. The technique assumes that cyclones with similar intensities tend to have similar patterns. It requires an expert to visually analyze a TC cloud structure in visible and IR images. Once a pattern is detected, features such as cloud organization properties, inner core characteristics, curvature radius, and rain rate are further analyzed to estimate the intensity [11], [12]. Although widely used, the Dvorak technique is subject to human errors and has some inherent limitations. The latest advanced Dvorak technique [10] applies automated algorithms to remove human subjectivity.\n\nThe DAVT technique is based on a directional gradient statistical analysis of IR satellite imagery to estimate the TC intensity over the North Atlantic [13] and North Pacific [14]. A priori cyclone center information is necessary for this approach. Although the DAVT technique can automatically locate the TC center, the location error may lead to slight uncertainty in intensity estimation.\n\nTC intensity estimation can also be performed using the CNN method [24]. CNN is a feedforward deep neural network inspired by the biological natural visual cognition mechanism [25]. This algorithm avoids the complicated preprocessing of images and has achieved great success in image classification, target recognition, and other research fields [25]- [27]. Recently, several CNN models have been developed to estimate TC intensity from IR satellite imagery by using the direct classification method [18], [19] or the indirect regression method [15]- [19]. The classification method first classifies a cyclone according to the intensity category (see Table I) and then calculates the maximum wind speed (MWS) of TCs. Compared with the Dvorak or DAVT technique, the CNN model obtained a higher accuracy [15]- [19].\n\nThe CNN model has produced a relatively high accuracy in estimating TC intensity for some ocean regions [15], [16], [18], [19]. Nevertheless, there are still some key issues to be solved. First, for satellite sensors with several IR channels, each channel provides a different distribution of brightness temperature. These data reveal the spectral characteristics of various atmospheric and oceanic variables, and some of them are closely linked with the TC structure, such as the cloud top, the lower/middle layer cloud, the water vapor, and sea surface temperature (SST). It has been pointed out that the cyclone intensity is proportional to the difference between the absolute temperature of the boundary layer and that of the storm top [28], and related to the upper tropospheric brightness temperature anomalies [29] and the water vapor in the upper and middle layers [16]. Recently, Sieron et al. [30] found that the intensity of a cyclone is necessarily directly related to the temperature deficit of cloud top versus sea surface, and the surplus in saturation entropy in the eyewall versus its surroundings. Therefore, the data from multiple channels can capture the atmospheric and sea surface variables that are closely related to TC intensity better than data from a single or two channels can. The accuracy of the CNN model in estimating TC intensity would be improved if multichannels of satellite data are utilized as input. However, only one or two channels of data have been used in previous studies. It is necessary to investigate the contribution of multichannel data and find an optimal channel combination for tropical intensity estimation.\n\nIt is also a great challenge to develop a robust CNN with a small number of samples and an unbalanced distribution of samples of different TC categories. To avoid the sideeffect of an unbalanced data set, Pradhan et al. [18] tried to balance the distribution at the cost of reducing training data size. However, this approach cannot solve the data imbalance problem fundamentally.\n\nIn this study, we developed a group of CNN models and selected the best one to estimate the TC intensity over the Northwest Pacific using high temporal and spatial coverage of Himawari-8 images. Our goal is to develop an objective and efficient TC intensity estimation method. We organize this article as follows. Section II introduces the data set and data pre-processing. The CNN models and experimental schemes are presented in Section III. The performance analysis and discussion are given in Section IV. The conclusion is given in Section V.\n\n\nII. DATA SET\n\n\nA. Best Track Data Set of TCs\n\nThe Best Track data set for TCs provided by the Shanghai Typhoon Institute, China Meteorological Administration (STI/CMA), Beijing, China [32] was used to label Himawari-8 (H-8) images. The track and MWS of a TC are recorded every 3 or 6 h in this data set. The 6-h Best Track data were all interpolated to 3 h to increase the data samples for TC intensity estimation.\n\n\nB. Geostationary Satellite Imagery of TCs\n\nH-8 is a geostationary meteorological satellite launched by the Japan Meteorological Agency (JMA), Tokyo, Japan, in October 2014. The Advanced Himawari Imager (AHI) onboard H-8 provides 10-min full-disk coverage in the Northwest Pacific from 16 channels: three visible, three near-IR, and ten thermal IR channels. The time interval of full-disk observations is 10 min. We downloaded the brightness temperature data from five IR channels (Channels 7, 8, 13, 14, and 15, see Table IV for details) from http://www.eorc.jaxa.jp/ptree. These five channels are for the monitoring of low-level clouds, natural disasters (Channel 7), water vapor in the upper and middle layers (Channel 8), and cloud top and SSTs (Channels 13-15) [31]. We obtained 6.690 five-channel composite images with a 3-h time interval and a 5-km resolution, which capture 97 TCs over the Northwest Pacific between 2015 and 2018. For each composite image, we extracted a 251 \u00d7 251 pixel subimage covering an area of 1255 km \u00d7 1255 km centered on the TC center determined from the Best Track data set of TCs, as shown in Fig. 1. One can see that each channel provides unique information about a TC. These 3-h composite images were labeled carefully according to eight categories using the Saffir-Simpson TC wind scale (H 1-H 5) along with intensity categorization for the tropical storm (TS) and tropical depression (TD) as TC intensity categories ( Table I). The images were randomly divided into training, validation, and test data at a ratio of 3:1:1, i.e., 4.014 images for training, 1.338 for validation, and 1.338 for testing.\n\n\nC. Data Preprocessing\n\nData normalization before the neural network training is required to speed up the calculations and obtain good results [33]. In this study, satellite-observed brightness temperature data were linearly transformed to the interval [0, 1] by\ny = x \u2212 x min x max \u2212 x min(1)\nwhere x max and x min are the maximum and minimum values of the brightness temperature x from all the five channels   Studies show that the rotation of images in a CNN model can reduce the orientation sensitivity and does not affect the classification accuracy [25], [34]. Therefore, the normalized training and validation images were artificially rotated 90 \u2022 , 180 \u2022 , and 270 \u2022 clockwise, as shown in Fig. 2. In this way, the number of samples increased by three times, and we obtained 16.056 training and 5.352 validation images (Table II).\n\n\nIII. DEEP CONVOLUTIONAL NEURAL NETWORKS FOR TC INTENSITY ESTIMATION\n\n\nA. CNN Model Configuration\n\nA simple CNN model contains various combinations of convolutional, pooling, and fully connected (FC) layers. The convolutional layer extracts image features. The pooling layer then filters the maximum value of these features to reduce the number of features. The FC layer learns all the features and eventually output the results. To prevent over-fitting, a dropout layer [35], which means setting the partial weight to zero, is usually added before the FC layer. A deep CNN is generally designed as a feedforward network and can be trained with the backpropagation algorithm. As the errors backpropagate in a   TABLE II  NUMBER OF TRAINING, VALIDATION, AND TEST DATA FOR THE  CNN-BASED TC INTENSITY ESTIMATION MODEL network, CNN is optimized by updating the weights and biases to minimize the loss function [25]. An example architecture of our CNN models is shown in Fig. 3. It consists of one input layer, four convolutional layers, four pooling layers, two FC layers, and one output layer. In this study, various network configuration performances in estimating the location and intensity of TCs were investigated. Table III lists the parameter sets in one example of the CNN (M1-M12 in Table IV) models. The parameters include the input shape, filter shape, output size, the number of parameters, along with the number of kernels used in each layer, and corresponding stride and padding. The output images for each layer are shown in Fig. 6. With the configuration of one  Table IV).   Table III, four-channel input data of 251 \u00d7 251 size are convolved in the first convolutional layer (Conv1, 16 kernels of 10 \u00d7 10 size, with stride = 3, and padding = 0). In this process, 16 feature maps of 84 \u00d7 84 size can be extracted, and 12832 parameters are learned. The first pooling layer (Pool1) then produces maps of 41 \u00d7 41 size by filtering these feature maps. In the dropout layer [35], half of the 128 feature maps generated with Conv3 move to the next layer. We obtain eight output values from the CNN model (hereafter called the multicategory CNN classification model). The nth value represents the confidence with which a TC is classified as the nth intensity category.\n\nIn addition, the channel attention layer and the spatial attention layer are also added to our model between the input layer and the first convolutional layer (Fig. 4). By generating the weights of different channels, the channel attention layer can tell us which channels are more important in the CNN model. Similarly, the spatial attention layer will learn the most relevant region for intensity estimation [36]. As a result, satellite images of TCs at different categories will be treated differently in the CNN model, as shown in Fig. 5 in Section IV-B.\n\n\nB. Setup of Comparative Experiments\n\nThe input satellite imagery quality and the parameter setting of a network affect the performance of a CNN model. To achieve an optimal TC monitoring scheme in this study, we designed numerous CNN model experiments with various configurations to locate a TC and estimate its intensity from satellite images with different channel combinations.\n\nTo address the question of which channel works better, we set up 12 experiments (M1-M12 in Table IV) to evaluate the CNN model's performance using H-8 images from five IR channels. With the basic CNN configuration shown in Table III, these CNN models' accuracies were first analyzed. Here, we adopted the nonlinear Relu as the activation function in the inner product calculation of convolutional and FC layers. The optimization function is Adam [37]. With an optimal channel combination of satellite imagery as input, CNN models' performances with different configurations (Table V) were then investigated.\n\n\nIV. RESULTS AND DISCUSSION\n\nSimilar to the criteria in [18], we evaluate our CNN models' performance concerning several aspects. 1) Accuracy: Accuracy measure is the number of exact-hits, which corresponds to the correct classification with the highest confidence. 2) Root Mean Square Error of TC Intensity: The estimated intensity or MWS (W ) of a TC is defined as the weighted average of the representative wind speeds associated with the two highest probability categories which can be evaluated by [18] \nW = U 1 \u00d7 P 1 + U 2 \u00d7 P 2(2)\nwhere P 1 and P 2 are the model outputs of the confidence of the TC categories with the highest and second-highest confidence, respectively; U 1 and U 2 are the representative wind speeds of the corresponding category.   Table VI, the confusion matrix shows the overall classification performance of a model. In a confusion matrix, the number along the diagonal line represents the number of correctly classified images for any category. The classification report (Table VII) \n\n\nA. Model Performance\n\nIn this section, the CNN model's accuracy with different input data or network configurations is analyzed by comparing the model result with the Best Track data set. Table IV shows the TC classification results for the CNN models using the 1338 testing images from different channels or channel combinations. The network configuration listed in Table III was used. For single-channel input to CNN models (M1-M5), we found that the highest accuracy was obtained with either Channel 14 or 15 as input. Both channels are in the thermal-IR bands, often used for cloud detection and SST calculation. The result confirms the conclusion of Sieron et al. [30] that the TC intensity is correlated with the temperature deficit of cloud top versus sea surface. As shown in Table IV, model M5 produces the best result with the smallest root mean square error (RMSE) in intensity estimation of the five single-channel input models. For testing using combinations of multiple channels, we first combined Channel 15 with the other channels, respectively, for M6-M9. Comparing M9 with M5, we can see that Channel 14 makes little contribution to the improvement of the model because its wavelength is close to Channel 15. These two thermal IR channels provide duplicate information for TC intensity estimation. CNN models M6-M8 show improvement when adding other IR channels, with M8 having a slightly better advantage.\n\nWe then investigated the influence of using three-channel or four-channel data on the accuracy of our CNN models. Among different combinations, inputting four-channel data into M12 generates the best result in Table IV. The accuracy reaches up to 82.9%, suggesting that this CNN model based on inputs from four-channel IR images delivers results that are more consistent with observations. Consistent with the previous theoretical study [28]- [30], our results indicate that the additional information of the water vapor or cloud characteristics provided by multichannels does play a significant role in the estimation of TC intensity. With multichannel images as input, CNN can learn more complex features of this complicated weather process, which may have been overlooked by the traditional methods.\n\nOnce we have found the best channel combination as input into the CNN model, M12, a variety of network configurations were further tested. The results are listed in Table V. CNN models M12-M15 consist of four convolutional layers, four pooling layers, and two FC layers. For M12-M15, each model holds a different number of kernels in the convolutional layers. In general, the model's accuracy is slightly increased with the increase in the number of kernels (M12 and M13) but then significantly reduced (M12) beyond a specific range of the kernel number. Although more feature maps can be extracted with more kernels, these maps may not always positively impact on the improvement of the model. This degradation may also be related to the relatively fewer training data in our model. The number of parameters to be learned by the model is more significant than that of the training data, making it challenging to generate good results. Compared with M14, the accuracy of M15 is even lower despite using different strides. If we add one channel attention layer and one spatial attention layer behind the input layer of M14 in M16, the CNN model's performance is further improved. M16 has generated a fairly good accuracy (86.0%) and low root mean square error (RMSE, 5.17 m/s). This improvement indicates that the attention layers can help the model to focus on the key factors.\n\n\nB. Discussion 1) Features Learned by CNN:\n\nIn the process of training the CNN models, the neural network learns many texture features of TCs. The correlation between these features and TC intensity determines the accuracy of the model. Using a deep visualization toolbox [38], we can analyze the image features learned by each network layer. We take the configuration of the CNN model M16 as an example to explain the features.\n\nWith four-channel H-8 images as input in M16, which involves channel and spatial attention layers, four weights were generated from the channel attention layer, with values of 0.3, \u22120.27, \u22121, and 0.75 for Channels 7, 8, 13, and 15. This implies that Channels 13 and 15 play a significant role in the CNN model M16, while Channels 7 and 8 play a supporting role, which is in good agreement with the results in Table V.\n\nFor TCs at different categories or stages, the CNN model focuses on different regions by utilizing the spatial attentional layer. Fig. 5 shows the heat map, and the value \"1\" represents the region of most interest in model M16. One can see the model focuses on the area with low brightness temperature for most category TCs except H 4. For stronger TCs, H 2, H 3, and H 5, these regions display clearly the eye and shape of the TC.\n\nThere are 32 feature maps generated from the first convolutional layer in M16, 16 of which are shown in Fig. 6(a). One can see the overall structure of a TC with the prominent eye located at the center of the image. The first pooling layer reduces the number of the extracted features through the maximum filter and makes them more blurred. The second convolutional layer extracts more image features, some of which have become abstract [ Fig. 6(b)]. By further studying these features in deeper layers, we got images with more abstract characteristics [ Fig. 6(c) and (d)]. All these feature maps demonstrate the central dense overcast and structure of a TC. The multichannel data discovered these features, and some of them might show the TC-related characteristics revealed in previous studies.\n\nTraditional techniques are mostly based on human understanding of TCs and manual analysis of specific features (e.g., spiraling, central cold clouds, and asymmetries). TC intensity is a function of these features. However, traditional techniques ignore features that have not yet been discovered but may be more relevant to TCs. Fundamentally, these methods are still subjective. CNN can extract more features from TC images, which may be undiscovered, and establish a relationship between these features and TC intensity. The CNN method is entirely objective, although there are no tools to analyze CNN-derived results intuitively. It may be possible to discover some unknown features that affect TCs by combining the information or features extracted from the CNN with the increasing understanding of physical mechanisms of TCs.\n\n2) Unbalanced Distribution of TC Categories: In general, the above multicategory CNN classification model M16 performs reasonably well in estimating TC intensity. However, as Tables VI and VII show the classification results are not very satisfactory for categories with small training samples. For example, only 58 out of 85 H 2-level TCs were correctly classified, and the precision is only 68.2%. The degradation is due to the unbalanced distribution of TC category samples while we trained the CNN-I model. Taking H2 as an example, the number of training data in this category only accounts for 5.5% of the total training number (Table II). The loss in a CNN model would only increase slightly, even if most H 2-level cyclones are misclassified. Since CNN updates each layer's weight value according to the loss, the network cannot learn the features of a category well with very few samples.\n\nTo reduce the side-effect of unbalanced training data, we can ideally eliminate part of the data to keep the number of TC data in each category balanced [18], [39]. However, this reduction would dramatically reduce the number of training data. Another possible solution is to use a focal_loss (FL) function in a model. This function helps a model learn the features better by increasing the weight of the category with fewer data in the loss. The function has shown its superiority in the field of target recognition [40]. As a result, we changed the loss function of our best CNN model M16 to FL to see if it can help to improve the classification accuracy.\n\nThe FL is [40] FL\n( p t ) = \u2212a t (1 \u2212 p t ) r log( p t )(3)\nwhere p t \u2208 [0, 1] is the model output of confidence for a category; a t \u2208 [0, 1] is the weight coefficient (the smaller the number of data in this category, the higher the value of a t ); and r is the tunable focusing parameter. For each TC category, the values of a t and r , determined by their proportions in the total data set, are listed in Table VIII. As shown in Table IX, by using the FL function in the multiclassification model M17, the TC intensity estimation accuracy is increased to 86.6%, and the RMSE is reduced by 2.1%. It is well known that a TC at different stages has different related features. One multiclassification model may not capture all these differences well. Hsu pointed out that, in many cases, the multiclassification problem can be transformed into a target recognition or binary-classification problem [41]. If we use eight binary models, with each model learning the TC relevant feature at the corresponding stage, better results may be obtained. Therefore, in model M18, eight binary CNN classification models were also set up to identify TC categories from NC to H 5. The configuration of each model is the same as that of M16. The FL with values of a t and r listed in Table VIII was also used. Classification label \"1\" or \"0\" represents a TC category. For example, suppose the MWS of a TC on a satellite image is 27.0 m/s, which corresponds to the TS category. In that case, the image label is \"1\" in the TS binary classification model and \"0\" in the training of other binary classification models. In this way, we only need to adjust each binary classification model as accurately as possible.\n\nWhen a TC image is input to eight CNN models, each model will generate an output with a value ranging from 0 to 1. The confidence of each TC category, for example, the TS category P TS is\nP TS = [(1 \u2212 P NC )+(1 \u2212 P TD )+ P TS +(1 \u2212 P H 1 )+(1 \u2212 P H 2 ) + (1 \u2212 P H 3 ) + (1 \u2212 P H 4 ) + (1 \u2212 P H 5 )]/8 (4)\nwhere P NC -P H 5 are the outputs of eight CNN binary models. The corresponding highest confidence number finally determines the intensity category of the TC and the MWS is calculated as\nW = U 1 \u00d7 P 1 + U 2 \u00d7 1 \u2212 P 2(5)\nwhere P 1 and P 2 are defined as the confidences of the TC categories with the highest and second-highest confidences, respectively; U 1 and U 2 are the representative wind speeds of the corresponding TC categories, respectively.\n\nFrom Table IX, one can see the binary classification model M18 generates the best results. The accuracy is much higher than the multiclassification model M16. Compared with M17, which also adopts the FL function, the RMSE of model M18 is greatly decreased to 4.62 m/s, and the correlation coefficient reaches up to 0.95 (Fig. 7). This improvement suggests that both the introduction of the FL function and the binary CNN classification model contribute to the improvement of the TC intensity estimation results in the case of unbalanced training data.\n\nTables X and XI show the confusion matrix and classification report of the binary classification model M18, which adopts the FL function. Compared with the results from the multiclassification model (Tables VI and VII H 1, H 2, H 3, H 4, and H 5 are all decreased. The biases for all categories in M18 are closer to 0, while M17 tends to produce negative biases.\n\nWe also compared our binary CNN classification model's performance in estimating TC intensity with the techniques listed in Table XII. Our model exhibits a smaller RMSE than the DAVT technique and most CNN regression or classification models. A comparative study on TC intensity estimation over different regions carried out by Chen et al. [15] showed that the result over the West Pacific was much worse than that over the global ocean. The worse result is attributable to higher   variability among TCs over the Northwest Pacific [15], [42], which makes it more challenging to estimate their intensities accurately. The accuracy of our model over the Northwest Pacific is comparable to the accuracy of the multiclassification CNN model proposed by Pradhan et al. [18] over the global ocean.\n\nThe performance of CNN models with the same structure may have some differences due to different training data. Therefore, we used the 6.690 four-channel H-8 images of TCs collected to retrain Chen's and Tian's model [15], [19]. The results (Table XII) show that our binary CNN classification model performs better than the other two CNN models if the same satellite data were used, which further demonstrates our model's robustness.\n\n3) Noise Resistance Ability of the CNN Model: As shown in Fig. 9, satellite images sometimes include noise, i.e., some distinct black spots on the Channel 7 brightness temperature images. To determine the extent to which different levels of  noise affect our model's performance, we carried out some antinoise experiments by attacking the optimal network with several adversarial attack methods [43], [44]. The adversarial attack means adding artificial noise to the input image and tricking a neural network into obtaining incorrect classification results. For example, for a TS, if we attack our model using an adversarial attack method (or noise adding method), the model may classify it as an H3 TC. In this way, we aim to test the stability or antinoise ability of our model. Among all the models in Table V, the M16 model generates the best classification results. In this study, we applied six gradient-based adversarial attack methods to test the M16 model's stability [43]- [45]. These attack methods include Random Projected Gradient Descent (Random PGD), Fast Gradient Sign Method (FGSM), Linfinity Basic Iterative Attack (LBIA), Basic Iterative Method (BIM), Saliency Map Attack (SMA), and Linfinity Basic Fast Gradient Sign Attack (LBFGSA) to test the stability of this model. When the loss of a neural network increases to a specific value by artificially adding some noises with an adversarial attack method, the image is misclassified. Here, we consider a case where TS is misclassified as H3. Fig. 10 shows examples of the original images and the noise added to the images using different methods. One can see that the FGSM method [ Fig. 10(b)] fails to misclassify our model, suggesting that the model is reasonably stable at this noise level. The other five methods succeeded in misclassifying the CNN-I model to some extent, but apparent anomalies can be observed in the images after adding noise by four methods (i.e., the LBIA, BIM, SMA, and LBFGSA methods). One can see that the accuracy of our model only decreases when we add very high-level noise. On the other hand, images with substantial noise can be easily identified and removed from the input data, implying that it is difficult for the four methods to influence our model performance in practice. Among the six methods, Random PGD is the only one that may misclassify the TC category effectively. To know how vulnerable our model is to the noise added by the Random PGD method, we analyzed its capability in classifying a TS with different attacking powers. Here, we use the noise-to-signal (NS) ratio, i.e., the absolute value of the ratio between the noise added to the signal and the original signal at each pixel of the image, to represent the adversarial attack power. Fig. 11 shows the variation of the confidence with which TS is misclassified as the H 3 category with the NS ratio. As the attack power increases, the confidence of misclassification increases as well. However, even under the lowest attack power, we can still observe that the image with added noise is rougher and fuzzier than the original image in Fig. 10. This result means that our model can defend against this adversarial attack method when the NS ratio is below 2%.   11. Variation of the confidence with which TS is misclassified as H 3 with NS ratio. The noise was added to the satellite images using the Random PGD method.\n\nThe traditional methods do not consider noise, which may affect results, so we cannot measure noise's influence on conventional methods. The above experiments prove that our CNN model can be unaffected by noise to a particular range.\n\n\nV. CONCLUSION\n\nTCs are one of the most severe disasters. Estimating their intensity is an essential step for forecasters and emergency responders to make disaster warnings. This article developed utterly objective CNN-based models to estimate TC intensity from H-8 geostationary satellite IR imagery, which can provide a reliable basis for TC prediction and early warning systems.\n\nResults show that the selection of satellite data channels has a significant impact on the TC intensity estimation models performance. Network parameters play an essential role in CNN models. Using four-channel (Channels 7, 8, 13, and 15) IR imagery, we found that the CNN model M16, which includes one input layer, one channel attention layer, one spatial attention layer, four convolutional layers, four pooling layers, two FC layers, and one output layer, has the best results among the multiclassification models. The model has generated a fairly good accuracy (86.0%), low root mean square error (RMSE, 5.17 m/s), and low bias (\u22122.15 m/s).\n\nSeveral other factors affect the accuracy of the CNN-based TC intensity model, such as the TC structure, and the imbalance of TC category samples. Our study shows that the TC eye, size, shape, and brightness temperature anomaly are the main factors possibly influencing the model performance.\n\nFor TC categories with much smaller training data set, the multiclassification model does not perform very well due to the TC categories' unbalanced distributions. In this study, for the first time, we introduced the FL function in the network to increase the weight of the TC category with fewer data in loss, and we adopted eight binary classification networks to reduce the side-effect of the unbalanced training data. The new binary classification model can better learn the features of these categories with fewer samples. The number of exact-hits for NC, TD, TS, H 1, H 2, H 3, and H 4 is increased. In particular, the precision of H 3 classification is significantly improved by 9.1%. Compared with the multiclassification model, the binary classification model M18 generates a higher accuracy (88.9%), and much lower RMSE (4.62 m/s) and bias (\u22120.76 m/s) in TC intensity estimation. The results are comparable to or even better than some of the state-of-the-art TC intensity estimation techniques.\n\nIn this work, we mainly used the CMA Best Track data set to label satellite images and train the TC intensity estimation model. As is known, any biases in the Best Track data set may introduce biases into the model. The difference in the Best Track data sets provided by different operational agencies such as the Japan Meteorological Agency (JMA) and Joint Typhoon Warning Center (JTWC) may also affect the model performance. Assuming that these biases are mainly caused by the differences in TC center positions provided by different agencies, we then used the JMA and JTWC determined TC center positions to re-extract satellite images in the testing data set for TCs between 2015 and 2018. The comparison between estimated TC intensity and the corresponding Best Track data shows that our CNN model also has very high accuracy with the RMSE of 4.72 and 4.70 m/s, respectively, quite close to that estimated based on CMA Best Track data (4.62 m/s). Note that the mean difference between the CMA TC center and JMA/JTWC TC center is 11.2 km/10.3 km. The result indicates that such a limited TC positioning error does not affect the performance of our CNN model greatly. For further development of the model, it is necessary to collect more satellite data such as HURSAT data to explore the TC characteristics and improve the CNN-based TC intensity estimation model [50].\n\n\nACKNOWLEDGMENT\n\nThe authors would like to thank the anonymous reviewers for their valuable comments and suggestions. Himawari-8 geostationary satellite data are obtained from Japan Meteorological Agency (http://www.jma.go.jp/). The Best Track data of tropical cyclones (TCs) are from the Shanghai Typhoon Institute, China Meteorological Administration (http://www.cma.gov.cn/).\n\nFig. 1 .\n1Brightness temperature (K) images of TC Halola from different channels. (a) Channel 7, (b) Channel 8, (c) Channel 13, (d) Channel 14, and (e) Channel 15, with a spatial coverage of 1255 km \u00d7 1255 km. The images were acquired at 0:00 UTC on July 13, 2015.\n\nFig. 2 .\n2Transformation of Fig. 1(e). (a) Original image. (b) 90 \u2022 rotation. (c) 180 \u2022 rotation. (d) 270 \u2022 rotation. (7, 8, 13, 14, and 15); y is the normalized pixel value limited to [0, 1].\n\nFig. 3 .\n3Architecture of one of our CNN models (M1-M12 in\n\nFig. 4 .\n4Diagram of the attention layers in the CNN model. (a) Overview of the attention layers. (b) Diagram of the channel attention layer. (c) Diagram of the spatial attention layer. \"+\" and \"\u00d7\" are plus and multiplication signs, respectively.\n\nFig. 5 .\n5Original H-8 image [(Left) unit: Brightness temperature, K] and (Right) heat map of the spatial attention layer in the CNN model M16. (a)-(h) TC with category of NC-H 5, respectively.\n\n\nIf the output of the model is [NC = 0.0, TD = 0.0, TS = 0.8, H 1 = 0.2, H 2 = 0.0, H 3 = 0.0, H 4 = 0.0, H 5 = 0.0], then P 1 = 0.8, P 2 = 0.2, and W = U TS \u00d7 0.8 + U H 1 \u00d7 0.2.\n\n\ndescribes the precision (P), recall (R, the confidence of detection), and f1-score (F1) of a CNN model. P and R are the ratios of the number of actual positive class values to the total number of positive classifications and the\n\nFig. 6 .\n6Feature maps generated from different layers in the CNN model M16. (a) First convolutional layer. (b) Second convolutional layer. (c) Third convolutional layer. (d) Last convolutional layer. The input data are four-channel (Channels 7, 8, 13, and 15) H-8 images (Fig. 1) of TC Halola in July 2015.\n\n\n), the numbers of exact-hits for NC, TD, TS, H 1, H 2, H 3, and H 4 are all increased. Fewer H 1 and H 4 TCs are misclassified as their adjacent categories. The H 1-H 4 classification precisions significantly improved by 4.9%, 7.4%, 9.1%, and 3.8%. As shown in Fig. 8, compared with the results of model M17, the RMSEs of M18 for TD, TS,\n\nFig. 7 .\n7(a) Comparison of the TC MSW from the Best Track data set with that estimated from model (a) M17 and (b) M18. The test data set consists of 1338 images. The least-squares regression line (black line), correlation coefficient R, and RMSE are also shown.\n\nFig. 8 .\n8RMSE and bias of the TC intensity estimated by M17 and M18 models.\n\nFig. 9 .\n9Brightness temperature images from Channel 7 with noise (black spots). (a) TC Halola (18:00 UTC on July 21, 2015), (b) TC Soudelor (15:00 UTC on August 4, 2015), (c) TC Tokage (12:00 UTC on November 24, 2016), and (d) TC Soulik (18:00 UTC on August 16, 2018).\n\nFig. 10 .\n10Adversarial attack experiment results using various attacking methods. (a) Random PGD, (b) FGSM, (c) LBIA, (d) BIM, (e) SMA, and (f) LBFGSA. For each figure, the left column is the original image from Channel 15, the middle column is the image after adding noise, and the right column shows the noise added to the original image.\n\nFig.\nFig. 11. Variation of the confidence with which TS is misclassified as H 3 with NS ratio. The noise was added to the satellite images using the Random PGD method.\n\nTABLE I\nISAFFIR-SIMPSON HURRICANE WIND SCALE AND RELATED CLASSIFICATIONS[18] \n\nTABLE III CONFIGURATION\nIIIOF A CNN MODEL (M 12 IN TABLE IV) USED IN THIS STUDY. \"s\" IS THE STRIDE, AND \" p\" IS THE NUMBER OF ZERO-PADDING.FOR \nEXAMPLE, CONV1(32 AT 3 \u00d7 3, s = 2, AND p = 0) DENOTES 32 KERNELS IN THE FIRST CONVOLUTIONAL LAYER WITH A SIZE OF 3 \u00d7 3, AND \nTHE STRIDE AND ZERO-PADDING ARE 2 AND 0, RESPECTIVELY \n\n\n\nTABLE IV TC\nIVINTENSITY CLASSIFICATION RESULTS USING H-8 IMAGES FROM DIFFERENT CHANNELS CNN model in\n\n\nTable I shows the wind speed range for different TC categories from TD to H 4. The representative wind speed is the median value of the speed range. For categories NC and H 5, which are not given the clear speed range in Table I, the representative wind speeds are calculated by averaging all the MWSs within each category. For example, we have collected 21 H 5 category images in this study, and the representative wind speed of these 21 H 5 storms is 76.0 m/s. 3) Confusion Matrix and Classification Report: As shown in\n\nTABLE V TABLE VII\nVVIITC INTENSITY CLASSIFICATION RESULTS FROM DIFFERENT NETWORK CONFIGURATIONS USING FOUR-CHANNEL (CHANNELS 7, 8, 13, AND 15) COMPOSITE H-8 DATA. TAKING THE CONFIGURATION OF M 14 AS AN EXAMPLE, THERE ARE FOUR CONVOLUTIONAL LAYERS (C1-C4) AND FOUR POOLING LAYERS (P1-P4), C32 AT 10 \u00d7 10 DENOTES 32 KERNELS IN THE FIRST CONVOLUTIONAL LAYER WITH A SIZE OF 10 \u00d7 10, AND THE STRIDE AND ZERO-PADDING ARE 3 AND 0, RESPECTIVELY TABLE VI CONFUSION MATRIX USING THE MULTICATEGORY CNN CLASSIFICATION MODEL M 16 CLASSIFICATION REPORT OF THE MULTICATEGORY CNN CLASSIFICATION MODEL M 16number of positive class values in the test data. F1 = \n2P \u00d7 R/(P + R) is the harmonic mean of recall and \nprecision. The closer the three values are, the more stable \nthe model is. \n\n\n\nTABLE VIII VALUESTABLE IX\nVIIIIXOF a t AND r USING THE FL FUNCTION FOR EACH TC CATEGORY COMPARISON OF CNN MODELS' PERFORMANCE USING MULTICATEGORY CLASSIFICATION AND BINARY CLASSIFICATION SCHEMES TABLE X CONFUSION MATRIX USING THE BINARY CNN CLASSIFICATION MODEL M 18\n\nTABLE XI CLASSIFICATION\nXIREPORT OF THE BINARY CNN CLASSIFICATION MODEL M 18\n\nTABLE XII COMPARISON\nXIIOF THE PERFORMANCES OF OUR MODEL AND OTHER TECHNIQUES IN ESTIMATING THE MWS OF TCS\nChong Wang received the B.S. degree in marine science from Hohai University, Nanjing, China, in 2018, where he is pursuing the master's degree in physical oceanography.His research interests include tropical cyclone monitoring and forecasting using remote sensing and artificial intelligence techniques.\nThe Dvorak tropical cyclone intensity estimation technique: A satellite-based method that has endured for over 30 years. C Velden, Bull. Amer. Meteorol. Soc. 879C. Velden et al., \"The Dvorak tropical cyclone intensity estimation technique: A satellite-based method that has endured for over 30 years,\" Bull. Amer. Meteorol. Soc., vol. 87, no. 9, pp. 1195-1210, Sep. 2006.\n\nTyphoon eye extraction with an automatic SAR image segmentation method. S Jin, S Wang, X Li, Int. J. Remote Sens. 35S. Jin, S. Wang, and X. Li, \"Typhoon eye extraction with an automatic SAR image segmentation method,\" Int. J. Remote Sens., vol. 35, nos. 11-12, pp. 3978-3993, Jun. 2014.\n\nA salient region detection and pattern matching-based algorithm for center detection of a partially covered tropical cyclone in a SAR image. S Jin, S Wang, X Li, L Jiao, J A Zhang, D Shen, IEEE Trans. Geosci. Remote Sens. 551S. Jin, S. Wang, X. Li, L. Jiao, J. A. Zhang, and D. Shen, \"A salient region detection and pattern matching-based algorithm for center detec- tion of a partially covered tropical cyclone in a SAR image,\" IEEE Trans. Geosci. Remote Sens., vol. 55, no. 1, pp. 280-291, Jan. 2017.\n\nIdentification of tropical cyclone centers in SAR imagery based on template matching and particle swarm optimization algorithms. S Jin, X Li, X Yang, J A Zhang, D Shen, IEEE Trans. Geosci. Remote Sens. 571S. Jin, X. Li, X. Yang, J. A. Zhang, and D. Shen, \"Identification of tropical cyclone centers in SAR imagery based on template matching and particle swarm optimization algorithms,\" IEEE Trans. Geosci. Remote Sens., vol. 57, no. 1, pp. 598-608, Jan. 2019.\n\nEstimating tropical cyclone intensity from infrared image data. M F Pi\u00f1eros, E A Ritchie, J S Tyo, Weather Forecasting. 265M. F. Pi\u00f1eros, E. A. Ritchie, and J. S. Tyo, \"Estimating tropical cyclone intensity from infrared image data,\" Weather Forecasting, vol. 26, no. 5, pp. 232-233, 2015.\n\nAn automatic method for tropical cyclone center determination from SAR. Q Xu, G Zhang, X Li, Y Cheng, Proc. IEEE Int. Geosci. Remote Sens. Symp. (IGARSS). IEEE Int. Geosci. Remote Sens. Symp. (IGARSS)Q. Xu, G. Zhang, X. Li, and Y. Cheng, \"An automatic method for tropical cyclone center determination from SAR,\" in Proc. IEEE Int. Geosci. Remote Sens. Symp. (IGARSS), Jul. 2016, pp. 2250-2252.\n\nComparison of typhoon centers from SAR and IR images and those from best track data sets. G Zheng, J Yang, A K Liu, X Li, W G Pichel, S He, IEEE Trans. Geosci. Remote Sens. 542G. Zheng, J. Yang, A. K. Liu, X. Li, W. G. Pichel, and S. He, \"Comparison of typhoon centers from SAR and IR images and those from best track data sets,\" IEEE Trans. Geosci. Remote Sens., vol. 54, no. 2, pp. 1000-1012, Feb. 2016.\n\nAutomatically locate tropical cyclone centers using top cloud motion data derived from geostationary satellite images. G Zheng, J Liu, J Yang, X Li, IEEE Trans. Geosci. Remote Sens. 5712G. Zheng, J. Liu, J. Yang, and X. Li, \"Automatically locate tropical cyclone centers using top cloud motion data derived from geostationary satellite images,\" IEEE Trans. Geosci. Remote Sens., vol. 57, no. 12, pp. 10175-10190, Dec. 2019.\n\nTropical cyclone intensity analysis using satellite data. V F Dvorak, NESDIS11NOAA. 47Tech. Rep.V. F. Dvorak, \"Tropical cyclone intensity analysis using satellite data,\" NOAA, Washington, DC, USA, Tech. Rep. NESDIS11, 1984, p. 47.\n\nReprocessing the most intense historical tropical cyclones in the satellite era using the advanced Dvorak technique. C Velden, T Olander, D Herndon, J P Kossin, Monthly Weather Rev. 1453C. Velden, T. Olander, D. Herndon, and J. P. Kossin, \"Reprocessing the most intense historical tropical cyclones in the satellite era using the advanced Dvorak technique,\" Monthly Weather Rev., vol. 145, no. 3, pp. 971-983, Mar. 2017.\n\nCyclone intensity estimation using similarity of satellite IR images based on histogram matching approach. N Jaiswal, C M Kishtawal, P K , Atmos. Res. 118N. Jaiswal, C. M. Kishtawal, and P. K. Pal, \"Cyclone intensity estimation using similarity of satellite IR images based on histogram matching approach,\" Atmos. Res., vol. 118, pp. 215-221, Nov. 2012.\n\nA multiple linear regression model for tropical cyclone intensity estimation from satellite infrared images. Y Zhao, C Zhao, R Sun, Z Wang, Atmosphere. 7340Y. Zhao, C. Zhao, R. Sun, and Z. Wang, \"A multiple linear regression model for tropical cyclone intensity estimation from satellite infrared images,\" Atmosphere, vol. 7, no. 3, p. 40, Mar. 2016.\n\nObjective measures of tropical cyclone structure and intensity change from remotely sensed infrared image data. M F Pi\u00f1eros, E A Ritchie, J S Tyo, IEEE Trans. Geosci. Remote Sens. 4611M. F. Pi\u00f1eros, E. A. Ritchie, and J. S. Tyo, \"Objective measures of tropical cyclone structure and intensity change from remotely sensed infrared image data,\" IEEE Trans. Geosci. Remote Sens., vol. 46, no. 11, pp. 3574-3580, Nov. 2008.\n\nSatellite-derived tropical cyclone intensity in the north Pacific Ocean using the deviation-angle variance technique. E A Ritchie, K M Wood, O G Rodr\u00edguez-Herrera, M F Pi\u00f1eros, J S Tyo, Weather Forecasting. 293E. A. Ritchie, K. M. Wood, O. G. Rodr\u00edguez-Herrera, M. F. Pi\u00f1eros, and J. S. Tyo, \"Satellite-derived tropical cyclone intensity in the north Pacific Ocean using the deviation-angle variance technique,\" Weather Forecasting, vol. 29, no. 3, pp. 505-516, Jun. 2014.\n\nRotation-blended CNNs on a new open dataset for tropical cyclone image-to-intensity regression. B Chen, B.-F Chen, H.-T Lin, Proc. Knowl. Discovery Data Mining. Knowl. Discovery Data MiningB. Chen, B.-F. Chen, and H.-T. Lin, \"Rotation-blended CNNs on a new open dataset for tropical cyclone image-to-intensity regression,\" in Proc. Knowl. Discovery Data Mining, Jul. 2018, pp. 90-99. [Online].\n\n. 10.1145/3219819.3219926Available: https://dl.acm.org/doi/10.1145/3219819.3219926\n\nEstimating tropical cyclone intensity by satellite imagery utilizing convolutional neural networks. B.-F Chen, B Chen, H.-T Lin, R L Elsberry, Weather Forecasting. 342B.-F. Chen, B. Chen, H.-T. Lin, and R. L. Elsberry, \"Estimating tropical cyclone intensity by satellite imagery utilizing convolutional neural networks,\" Weather Forecasting, vol. 34, no. 2, pp. 447-465, Apr. 2019.\n\nA convolutional neural network approach for estimating tropical cyclone intensity using satellite-based infrared images. J S Combinido, J R Mendoza, J Aborot, Proc. 24th Int. Conf. Pattern Recognit. (ICPR). 24th Int. Conf. Pattern Recognit. (ICPR)J. S. Combinido, J. R. Mendoza, and J. Aborot, \"A convolutional neural network approach for estimating tropical cyclone intensity using satellite-based infrared images,\" in Proc. 24th Int. Conf. Pattern Recog- nit. (ICPR), Aug. 2018, pp. 1474-1480.\n\nTropical cyclone intensity estimation using a deep convolutional neural network. R Pradhan, R S Aygun, M Maskey, R Ramachandran, D J Cecil, IEEE Trans. Image Process. 272R. Pradhan, R. S. Aygun, M. Maskey, R. Ramachandran, and D. J. Cecil, \"Tropical cyclone intensity estimation using a deep convolutional neural network,\" IEEE Trans. Image Process., vol. 27, no. 2, pp. 692-702, Feb. 2018.\n\nA CNN-based hybrid model for tropical cyclone intensity estimation in meteorological industry. W Tian, W Huang, L Yi, L Wu, C Wang, IEEE Access. 8W. Tian, W. Huang, L. Yi, L. Wu, and C. Wang, \"A CNN-based hybrid model for tropical cyclone intensity estimation in meteorological industry,\" IEEE Access, vol. 8, pp. 59158-59168, 2020.\n\nEvaluation of advanced microwave sounding unit tropical-cyclone intensity and size estimation algorithms. J L Demuth, M Demaria, J A Knaff, T H V Haar, J. Appl. Meteorol. 432J. L. Demuth, M. DeMaria, J. A. Knaff, and T. H. V. Haar, \"Evaluation of advanced microwave sounding unit tropical-cyclone intensity and size estimation algorithms,\" J. Appl. Meteorol., vol. 43, no. 2, pp. 282-296, Feb. 2004.\n\nSatellite-based tropical cyclone intensity estimation using the NOAA-KLM series advanced microwave sounding unit (AMSU). K F Brueske, C S Velden, Monthly Weather Rev. 1314K. F. Brueske and C. S. Velden, \"Satellite-based tropical cyclone intensity estimation using the NOAA-KLM series advanced microwave sounding unit (AMSU),\" Monthly Weather Rev., vol. 131, no. 4, pp. 687-697, Apr. 2003.\n\nUpgrades to the UW-CIMSS AMSUbased TC intensity algorithm. D Herndon, C Velden, Proc. 26th Conf. Hurricanes Tropical Meteorol. 26th Conf. Hurricanes Tropical MeteorolMiami, FL, USAAmerican Meteorological SocietyD. Herndon and C. Velden, \"Upgrades to the UW-CIMSS AMSU- based TC intensity algorithm,\" in Proc. 26th Conf. Hurricanes Tropical Meteorol. Miami, FL, USA: American Meteorological Society, 2004, pp. 118-119.\n\nA consensus approach for estimating tropical cyclone intensity from meteorological satellites: SATCON. C S Velden, D Herndon, Weather Forecasting. 354C. S. Velden and D. Herndon, \"A consensus approach for estimating tropical cyclone intensity from meteorological satellites: SATCON,\" Weather Forecasting, vol. 35, no. 4, pp. 1645-1662, Aug. 2020.\n\nDeep-learning-based information mining from ocean remote-sensing imagery. X Li, 10.1093/nsr/nwaa047Nat. Sci. Rev. 710X. Li et al., \"Deep-learning-based information mining from ocean remote-sensing imagery,\" Nat. Sci. Rev., vol. 7, no. 10, pp. 1584-1605, Oct. 2020, doi: 10.1093/nsr/nwaa047.\n\nY Bengio, I J Goodfellow, A Courville, Deep Learning. Cambridge, MA, USAMIT PressY. Bengio, I. J. Goodfellow, and A. Courville, Deep Learning. Cambridge, MA, USA: MIT Press, 2016.\n\nConvolutional networks for images, speech, and time series. Y Lecun, Y Bengio, The Handbook of Brain Theory and Neural Networks. Cambridge, MA, USAMIT PressY. Lecun and Y. Bengio, \"Convolutional networks for images, speech, and time series,\" in The Handbook of Brain Theory and Neural Net- works. Cambridge, MA, USA: MIT Press, 1998.\n\nDeep learning face representation from predicting 10,000 classes. Y Sun, X Wang, X Tang, Proc. Comput. Vis. Pattern Recognit. Comput. Vis. Pattern RecognitY. Sun, X. Wang, and X. Tang, \"Deep learning face representation from predicting 10,000 classes,\" in Proc. Comput. Vis. Pattern Recognit., 2014, pp. 1891-1898.\n\nUse of cloud radars and radiometers for tropical cyclone intensity estimation. V Wong, K Emanuel, 10.1029/2007GL029960Geophys. Res. Lett. 3412V. Wong and K. Emanuel, \"Use of cloud radars and radiometers for tropical cyclone intensity estimation,\" Geophys. Res. Lett., vol. 34, no. 12, 2007, Art. no. L12811, doi: 10.1029/2007GL029960.\n\nEstimating the intensity of tropical cyclone in western north Pacific basin with AMSU-A brightness temperature. Z Liu, W.-B Li, Z.-G Han, Z.-G Yao, F.-Y Zhang, Y.-J Zhu, Chin. J. Geophys. 511Z. Liu, W.-B. Li, Z.-G. Han, Z.-G. Yao, F.-Y. Zhang, and Y.-J. Zhu, \"Estimating the intensity of tropical cyclone in western north Pacific basin with AMSU-A brightness temperature,\" Chin. J. Geophys., vol. 51, no. 1, pp. 42-48, Jan. 2008.\n\nFeasibility of tropical cyclone intensity estimation using satellite-borne radiometer measurements: An observing system simulation experiment. S B Sieron, F Zhang, K A Emanuel, Geophys. Res. Lett. 4019S. B. Sieron, F. Zhang, and K. A. Emanuel, \"Feasibility of tropical cyclone intensity estimation using satellite-borne radiometer measure- ments: An observing system simulation experiment,\" Geophys. Res. Lett., vol. 40, no. 19, pp. 5332-5336, Oct. 2013.\n\nNew Geostationary Meteorological Satellites-Himawari-8/9. Japan Meteorological Agency. Japan Meteorological Agency. (2014). New Geostationary Meteorolog- ical Satellites-Himawari-8/9. [Online]. Available: https://www.jma.go. jp/jma/jma-eng/satellite/news/himawari89/himawari89_leaflet.pdf\n\nAn overview of the China meteorological administration tropical cyclone database. M Ying, J. Atmos. Ocean. Technol. 312M. Ying et al., \"An overview of the China meteorological administration tropical cyclone database,\" J. Atmos. Ocean. Technol., vol. 31, no. 2, pp. 287-301, Feb. 2014.\n\nImportance of input data normalization for the application of neural networks to complex industrial problems. J Sola, J Sevilla, IEEE Trans. Nucl. Sci. 443J. Sola and J. Sevilla, \"Importance of input data normalization for the application of neural networks to complex industrial problems,\" IEEE Trans. Nucl. Sci., vol. 44, no. 3, pp. 1464-1468, Jun. 1997.\n\nCyclone track forecasting based on satellite images using artificial neural networks. R Kovord\u00e1nyi, C Roy, ISPRS J. Photogramm. Remote Sens. 646R. Kovord\u00e1nyi and C. Roy, \"Cyclone track forecasting based on satellite images using artificial neural networks,\" ISPRS J. Photogramm. Remote Sens., vol. 64, no. 6, pp. 513-521, Nov. 2009.\n\nDropout: A simple way to prevent neural networks from overfitting. N Srivastava, G Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov, J. Mach. Learn. Res. 151N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, \"Dropout: A simple way to prevent neural networks from overfitting,\" J. Mach. Learn. Res., vol. 15, no. 1, pp. 1929-1958, 2014.\n\nCBAM: Convolutional block attention module. S Woo, J Park, J Lee, I S Kweon, Proc. Eur. Conf. Comput. Vis. Eur. Conf. Comput. VisS. Woo, J. Park, J. Lee, and I. S. Kweon, \"CBAM: Convolutional block attention module,\" in Proc. Eur. Conf. Comput. Vis., 2018, pp. 3-19.\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, arXiv:1412.6980D. P. Kingma and J. Ba, \"Adam: A method for stochastic opti- mization,\" 2014, arXiv:1412.6980. [Online]. Available: http://arxiv.org/ abs/1412.6980\n\n. C Francois, Deep Learning With Python. Shelter Island. Manning Publicationssec. 5.4.C. Francois, Deep Learning With Python. Shelter Island, NY, USA: Manning Publications, 2017, sec. 5.4.\n\nLIUBoost: Locality informed underboosting for imbalanced data classification. S Ahmed, arXiv:1711.05365S. Ahmed et al., \"LIUBoost: Locality informed underboosting for imbal- anced data classification,\" 2017, arXiv:1711.05365. [Online]. Available: https://arxiv.org/pdf/1711.05365.pdf\n\nFocal loss for dense object detection. T.-Y Lin, P Goyal, R Girshick, K He, P Doll\u00e1r, IEEE Trans. Pattern Anal. Mach. Intell. 422T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll\u00e1r, \"Focal loss for dense object detection,\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 42, no. 2, pp. 318-327, Feb. 2020.\n\nA comparison of methods for multi-class support vector machines. C W Hsu, C J Lin, IEEE Trans. Neural Netw. 132C. W. Hsu and C. J. Lin, \"A comparison of methods for multi-class support vector machines,\" IEEE Trans. Neural Netw., vol. 13, no. 2, pp. 415-425, Aug. 2002.\n\nAn empirical study of tropical cyclone activity in the Atlantic and Pacific Oceans. S Bao, L J Pietrafesa, N E Huang, Adv. Adapt. Data Anal. 33S. Bao, L. J. Pietrafesa, and N. E. Huang, \"An empirical study of tropical cyclone activity in the Atlantic and Pacific Oceans: 1851-2005,\" Adv. Adapt. Data Anal., vol. 3, no. 3, pp. 291-307, 2011.\n\nFoolbox: A Python toolbox to benchmark the robustness of machine learning models. J Rauber, W Brendel, M Bethge, arXiv:1707.04131J. Rauber, W. Brendel, and M. Bethge, \"Foolbox: A Python tool- box to benchmark the robustness of machine learning models,\" 2017, arXiv:1707.04131. [Online]. Available: http://arxiv.org/abs/1707.04131\n\nExplaining and harnessing adversarial examples. I J Goodfellow, J Shlens, C Szegedy, Proc. Int. Conf. Learn. Represent. Int. Conf. Learn. RepresentI. J. Goodfellow, J. Shlens, and C. Szegedy, \"Explaining and harnessing adversarial examples,\" in Proc. Int. Conf. Learn. Represent., 2015, pp. 1-11.\n\nIntriguing properties of neural networks. C Szegedy, Proc. Int. Conf. Learn. Represent. Int. Conf. Learn. RepresentC. Szegedy et al., \"Intriguing properties of neural networks,\" in Proc. Int. Conf. Learn. Represent., 2014, pp. 1-10.\n\nA Technique for Analysis and Forecasting of Tropical Cyclones Intensities From Satellite Picture. V F Dvorak, V. F. Dvorak. (1973). A Technique for Analysis and Forecasting of Tropical Cyclones Intensities From Satellite Picture. [Online]. Available: https://commons.wikimedia.org/wiki/File:DvorakCDP1973.png\n\nA globally consistent reanalysis of hurricane variability and trends. J P Kossin, K R Knapp, D J Vimont, Geophys. Res. Lett. 3404J. P. Kossin, K. R. Knapp, and D. J. Vimont, \"A globally consistent reanalysis of hurricane variability and trends,\" Geophys. Res. Lett., vol. 340, no. 4, pp. 344-356, 2007.\n\nTropical cyclone intensity estimation in the North Atlantic basin using an improved deviation angle variance technique. E A Ritchie, G Valliere-Kelley, M F Pi\u00f1eros, J S Tyo, Weather Forecasting. 275E. A. Ritchie, G. Valliere-Kelley, M. F. Pi\u00f1eros, and J. S. Tyo, \"Tropical cyclone intensity estimation in the North Atlantic basin using an improved deviation angle variance technique,\" Weather Forecasting, vol. 27, no. 5, pp. 1264-1277, Oct. 2012.\n\nObjective tropical cyclone intensity estimation using analogs of spatial features in satellite data. G Fetanat, A Homaifar, K R Knapp, Weather Forecasting. 286G. Fetanat, A. Homaifar, and K. R. Knapp, \"Objective tropical cyclone intensity estimation using analogs of spatial features in satellite data,\" Weather Forecasting, vol. 28, no. 6, pp. 1446-1459, Dec. 2013.\n\nNew global tropical cyclone data set from ISCCP B1 geostationary satellite observations. K R Knapp, J P Kossin, J. Appl. Remote Sens. 11Art. no. 013505K. R. Knapp and J. P. Kossin, \"New global tropical cyclone data set from ISCCP B1 geostationary satellite observations,\" J. Appl. Remote Sens., vol. 1, no. 1, Feb. 2007, Art. no. 013505.\n", "annotations": {"author": "[{\"end\":166,\"start\":123},{\"end\":213,\"start\":167},{\"end\":234,\"start\":214},{\"end\":255,\"start\":235},{\"end\":285,\"start\":256},{\"end\":408,\"start\":286},{\"end\":446,\"start\":409},{\"end\":487,\"start\":447},{\"end\":535,\"start\":488}]", "publisher": null, "author_last_name": "[{\"end\":165,\"start\":161},{\"end\":190,\"start\":188},{\"end\":233,\"start\":231},{\"end\":254,\"start\":251},{\"end\":284,\"start\":279}]", "author_first_name": "[{\"end\":160,\"start\":155},{\"end\":187,\"start\":179},{\"end\":230,\"start\":226},{\"end\":250,\"start\":247},{\"end\":278,\"start\":275}]", "author_affiliation": "[{\"end\":407,\"start\":287},{\"end\":445,\"start\":410},{\"end\":486,\"start\":448},{\"end\":534,\"start\":489}]", "title": "[{\"end\":116,\"start\":1},{\"end\":651,\"start\":536}]", "venue": "[{\"end\":703,\"start\":653}]", "abstract": "[{\"end\":2988,\"start\":1305}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3391,\"start\":3388},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3396,\"start\":3393},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3611,\"start\":3608},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3648,\"start\":3644},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3830,\"start\":3827},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3836,\"start\":3832},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3842,\"start\":3838},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3890,\"start\":3886},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3896,\"start\":3892},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4058,\"start\":4054},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":4064,\"start\":4060},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4091,\"start\":4087},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4137,\"start\":4134},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4596,\"start\":4592},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4602,\"start\":4598},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4750,\"start\":4746},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4967,\"start\":4963},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4990,\"start\":4986},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":5275,\"start\":5271},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5384,\"start\":5380},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":5554,\"start\":5550},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5560,\"start\":5556},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5708,\"start\":5704},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5714,\"start\":5710},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5753,\"start\":5749},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5759,\"start\":5755},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6010,\"start\":6006},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6016,\"start\":6012},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6127,\"start\":6123},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6133,\"start\":6129},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6139,\"start\":6135},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6145,\"start\":6141},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6763,\"start\":6759},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6840,\"start\":6836},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":6896,\"start\":6892},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6926,\"start\":6922},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7905,\"start\":7901},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":8800,\"start\":8796},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9798,\"start\":9794},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":10817,\"start\":10813},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":11229,\"start\":11225},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":11235,\"start\":11231},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11985,\"start\":11981},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12421,\"start\":12417},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":13496,\"start\":13492},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":14200,\"start\":14196},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":15179,\"start\":15175},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":15398,\"start\":15394},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":15845,\"start\":15841},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":17028,\"start\":17024},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":18222,\"start\":18218},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":18228,\"start\":18224},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":20240,\"start\":20236},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":23932,\"start\":23928},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":23938,\"start\":23934},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":24296,\"start\":24292},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":24449,\"start\":24445},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":25336,\"start\":25332},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":28148,\"start\":28144},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":28340,\"start\":28336},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":28346,\"start\":28342},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":28573,\"start\":28569},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":28819,\"start\":28815},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":28825,\"start\":28821},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":29432,\"start\":29428},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":29438,\"start\":29434},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":30014,\"start\":30010},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":30020,\"start\":30016},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":36357,\"start\":36353},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":39964,\"start\":39960}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":37004,\"start\":36739},{\"attributes\":{\"id\":\"fig_1\"},\"end\":37198,\"start\":37005},{\"attributes\":{\"id\":\"fig_2\"},\"end\":37258,\"start\":37199},{\"attributes\":{\"id\":\"fig_3\"},\"end\":37506,\"start\":37259},{\"attributes\":{\"id\":\"fig_4\"},\"end\":37701,\"start\":37507},{\"attributes\":{\"id\":\"fig_5\"},\"end\":37881,\"start\":37702},{\"attributes\":{\"id\":\"fig_6\"},\"end\":38112,\"start\":37882},{\"attributes\":{\"id\":\"fig_7\"},\"end\":38421,\"start\":38113},{\"attributes\":{\"id\":\"fig_8\"},\"end\":38761,\"start\":38422},{\"attributes\":{\"id\":\"fig_9\"},\"end\":39025,\"start\":38762},{\"attributes\":{\"id\":\"fig_10\"},\"end\":39103,\"start\":39026},{\"attributes\":{\"id\":\"fig_11\"},\"end\":39374,\"start\":39104},{\"attributes\":{\"id\":\"fig_12\"},\"end\":39717,\"start\":39375},{\"attributes\":{\"id\":\"fig_13\"},\"end\":39886,\"start\":39718},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":39965,\"start\":39887},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":40292,\"start\":39966},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":40394,\"start\":40293},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":40918,\"start\":40395},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":41693,\"start\":40919},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":41961,\"start\":41694},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":42039,\"start\":41962},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":42147,\"start\":42040}]", "paragraph": "[{\"end\":4106,\"start\":3007},{\"end\":4809,\"start\":4108},{\"end\":5202,\"start\":4811},{\"end\":6017,\"start\":5204},{\"end\":7679,\"start\":6019},{\"end\":8061,\"start\":7681},{\"end\":8609,\"start\":8063},{\"end\":9026,\"start\":8658},{\"end\":10668,\"start\":9072},{\"end\":10932,\"start\":10694},{\"end\":11508,\"start\":10964},{\"end\":13784,\"start\":11609},{\"end\":14344,\"start\":13786},{\"end\":14727,\"start\":14384},{\"end\":15336,\"start\":14729},{\"end\":15846,\"start\":15367},{\"end\":16352,\"start\":15876},{\"end\":17779,\"start\":16377},{\"end\":18583,\"start\":17781},{\"end\":19962,\"start\":18585},{\"end\":20392,\"start\":20008},{\"end\":20811,\"start\":20394},{\"end\":21244,\"start\":20813},{\"end\":22043,\"start\":21246},{\"end\":22875,\"start\":22045},{\"end\":23773,\"start\":22877},{\"end\":24433,\"start\":23775},{\"end\":24452,\"start\":24435},{\"end\":26129,\"start\":24495},{\"end\":26318,\"start\":26131},{\"end\":26622,\"start\":26436},{\"end\":26885,\"start\":26656},{\"end\":27438,\"start\":26887},{\"end\":27802,\"start\":27440},{\"end\":28596,\"start\":27804},{\"end\":29031,\"start\":28598},{\"end\":32422,\"start\":29033},{\"end\":32657,\"start\":32424},{\"end\":33040,\"start\":32675},{\"end\":33686,\"start\":33042},{\"end\":33980,\"start\":33688},{\"end\":34986,\"start\":33982},{\"end\":36358,\"start\":34988},{\"end\":36738,\"start\":36377}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10963,\"start\":10933},{\"attributes\":{\"id\":\"formula_1\"},\"end\":15875,\"start\":15847},{\"attributes\":{\"id\":\"formula_2\"},\"end\":24494,\"start\":24453},{\"attributes\":{\"id\":\"formula_3\"},\"end\":26435,\"start\":26319},{\"attributes\":{\"id\":\"formula_4\"},\"end\":26655,\"start\":26623}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":5862,\"start\":5855},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":9553,\"start\":9545},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":10493,\"start\":10486},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":11507,\"start\":11497},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":12325,\"start\":12217},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":12736,\"start\":12727},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":12807,\"start\":12799},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":13095,\"start\":13086},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":13108,\"start\":13099},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":14829,\"start\":14820},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":14961,\"start\":14952},{\"end\":15312,\"start\":15303},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":16105,\"start\":16097},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":16351,\"start\":16340},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":16551,\"start\":16543},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":16731,\"start\":16722},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":17147,\"start\":17139},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":17999,\"start\":17991},{\"end\":18757,\"start\":18750},{\"end\":20810,\"start\":20803},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":23520,\"start\":23510},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":24852,\"start\":24842},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":24874,\"start\":24866},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":25713,\"start\":25703},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":26900,\"start\":26892},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":27657,\"start\":27639},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":27937,\"start\":27928},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":28850,\"start\":28839},{\"end\":29845,\"start\":29838}]", "section_header": "[{\"end\":3005,\"start\":2990},{\"end\":8624,\"start\":8612},{\"end\":8656,\"start\":8627},{\"end\":9070,\"start\":9029},{\"end\":10692,\"start\":10671},{\"end\":11578,\"start\":11511},{\"end\":11607,\"start\":11581},{\"end\":14382,\"start\":14347},{\"end\":15365,\"start\":15339},{\"end\":16375,\"start\":16355},{\"end\":20006,\"start\":19965},{\"end\":32673,\"start\":32660},{\"end\":36375,\"start\":36361},{\"end\":36748,\"start\":36740},{\"end\":37014,\"start\":37006},{\"end\":37208,\"start\":37200},{\"end\":37268,\"start\":37260},{\"end\":37516,\"start\":37508},{\"end\":38122,\"start\":38114},{\"end\":38771,\"start\":38763},{\"end\":39035,\"start\":39027},{\"end\":39113,\"start\":39105},{\"end\":39385,\"start\":39376},{\"end\":39723,\"start\":39719},{\"end\":39895,\"start\":39888},{\"end\":39990,\"start\":39967},{\"end\":40305,\"start\":40294},{\"end\":40937,\"start\":40920},{\"end\":41720,\"start\":41695},{\"end\":41986,\"start\":41963},{\"end\":42061,\"start\":42041}]", "table": "[{\"end\":40292,\"start\":40106},{\"end\":41693,\"start\":41509}]", "figure_caption": "[{\"end\":37004,\"start\":36750},{\"end\":37198,\"start\":37016},{\"end\":37258,\"start\":37210},{\"end\":37506,\"start\":37270},{\"end\":37701,\"start\":37518},{\"end\":37881,\"start\":37704},{\"end\":38112,\"start\":37884},{\"end\":38421,\"start\":38124},{\"end\":38761,\"start\":38424},{\"end\":39025,\"start\":38773},{\"end\":39103,\"start\":39037},{\"end\":39374,\"start\":39115},{\"end\":39717,\"start\":39388},{\"end\":39886,\"start\":39724},{\"end\":39965,\"start\":39897},{\"end\":40106,\"start\":39994},{\"end\":40394,\"start\":40308},{\"end\":40918,\"start\":40397},{\"end\":41509,\"start\":40942},{\"end\":41961,\"start\":41727},{\"end\":42039,\"start\":41989},{\"end\":42147,\"start\":42065}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10163,\"start\":10157},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11374,\"start\":11368},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12483,\"start\":12477},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":13053,\"start\":13047},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13953,\"start\":13945},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":14327,\"start\":14321},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":20949,\"start\":20943},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":21356,\"start\":21350},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":21694,\"start\":21685},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":21807,\"start\":21801},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":27215,\"start\":27207},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":27676,\"start\":27658},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":29097,\"start\":29091},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30550,\"start\":30543},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":30693,\"start\":30683},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":31797,\"start\":31790},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32147,\"start\":32140},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":32267,\"start\":32265},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":33279,\"start\":33253}]", "bib_author_first_name": "[{\"end\":42574,\"start\":42573},{\"end\":42898,\"start\":42897},{\"end\":42905,\"start\":42904},{\"end\":42913,\"start\":42912},{\"end\":43255,\"start\":43254},{\"end\":43262,\"start\":43261},{\"end\":43270,\"start\":43269},{\"end\":43276,\"start\":43275},{\"end\":43284,\"start\":43283},{\"end\":43286,\"start\":43285},{\"end\":43295,\"start\":43294},{\"end\":43747,\"start\":43746},{\"end\":43754,\"start\":43753},{\"end\":43760,\"start\":43759},{\"end\":43768,\"start\":43767},{\"end\":43770,\"start\":43769},{\"end\":43779,\"start\":43778},{\"end\":44143,\"start\":44142},{\"end\":44145,\"start\":44144},{\"end\":44156,\"start\":44155},{\"end\":44158,\"start\":44157},{\"end\":44169,\"start\":44168},{\"end\":44171,\"start\":44170},{\"end\":44442,\"start\":44441},{\"end\":44448,\"start\":44447},{\"end\":44457,\"start\":44456},{\"end\":44463,\"start\":44462},{\"end\":44855,\"start\":44854},{\"end\":44864,\"start\":44863},{\"end\":44872,\"start\":44871},{\"end\":44874,\"start\":44873},{\"end\":44881,\"start\":44880},{\"end\":44887,\"start\":44886},{\"end\":44889,\"start\":44888},{\"end\":44899,\"start\":44898},{\"end\":45291,\"start\":45290},{\"end\":45300,\"start\":45299},{\"end\":45307,\"start\":45306},{\"end\":45315,\"start\":45314},{\"end\":45655,\"start\":45654},{\"end\":45657,\"start\":45656},{\"end\":45946,\"start\":45945},{\"end\":45956,\"start\":45955},{\"end\":45967,\"start\":45966},{\"end\":45978,\"start\":45977},{\"end\":45980,\"start\":45979},{\"end\":46358,\"start\":46357},{\"end\":46369,\"start\":46368},{\"end\":46371,\"start\":46370},{\"end\":46384,\"start\":46383},{\"end\":46386,\"start\":46385},{\"end\":46715,\"start\":46714},{\"end\":46723,\"start\":46722},{\"end\":46731,\"start\":46730},{\"end\":46738,\"start\":46737},{\"end\":47070,\"start\":47069},{\"end\":47072,\"start\":47071},{\"end\":47083,\"start\":47082},{\"end\":47085,\"start\":47084},{\"end\":47096,\"start\":47095},{\"end\":47098,\"start\":47097},{\"end\":47497,\"start\":47496},{\"end\":47499,\"start\":47498},{\"end\":47510,\"start\":47509},{\"end\":47512,\"start\":47511},{\"end\":47520,\"start\":47519},{\"end\":47522,\"start\":47521},{\"end\":47543,\"start\":47542},{\"end\":47545,\"start\":47544},{\"end\":47556,\"start\":47555},{\"end\":47558,\"start\":47557},{\"end\":47949,\"start\":47948},{\"end\":47960,\"start\":47956},{\"end\":47971,\"start\":47967},{\"end\":48435,\"start\":48431},{\"end\":48443,\"start\":48442},{\"end\":48454,\"start\":48450},{\"end\":48461,\"start\":48460},{\"end\":48463,\"start\":48462},{\"end\":48836,\"start\":48835},{\"end\":48838,\"start\":48837},{\"end\":48851,\"start\":48850},{\"end\":48853,\"start\":48852},{\"end\":48864,\"start\":48863},{\"end\":49293,\"start\":49292},{\"end\":49304,\"start\":49303},{\"end\":49306,\"start\":49305},{\"end\":49315,\"start\":49314},{\"end\":49325,\"start\":49324},{\"end\":49341,\"start\":49340},{\"end\":49343,\"start\":49342},{\"end\":49699,\"start\":49698},{\"end\":49707,\"start\":49706},{\"end\":49716,\"start\":49715},{\"end\":49722,\"start\":49721},{\"end\":49728,\"start\":49727},{\"end\":50044,\"start\":50043},{\"end\":50046,\"start\":50045},{\"end\":50056,\"start\":50055},{\"end\":50067,\"start\":50066},{\"end\":50069,\"start\":50068},{\"end\":50078,\"start\":50077},{\"end\":50082,\"start\":50079},{\"end\":50460,\"start\":50459},{\"end\":50462,\"start\":50461},{\"end\":50473,\"start\":50472},{\"end\":50475,\"start\":50474},{\"end\":50788,\"start\":50787},{\"end\":50799,\"start\":50798},{\"end\":51251,\"start\":51250},{\"end\":51253,\"start\":51252},{\"end\":51263,\"start\":51262},{\"end\":51570,\"start\":51569},{\"end\":51788,\"start\":51787},{\"end\":51798,\"start\":51797},{\"end\":51800,\"start\":51799},{\"end\":51814,\"start\":51813},{\"end\":52029,\"start\":52028},{\"end\":52038,\"start\":52037},{\"end\":52370,\"start\":52369},{\"end\":52377,\"start\":52376},{\"end\":52385,\"start\":52384},{\"end\":52699,\"start\":52698},{\"end\":52707,\"start\":52706},{\"end\":53068,\"start\":53067},{\"end\":53078,\"start\":53074},{\"end\":53087,\"start\":53083},{\"end\":53097,\"start\":53093},{\"end\":53107,\"start\":53103},{\"end\":53119,\"start\":53115},{\"end\":53530,\"start\":53529},{\"end\":53532,\"start\":53531},{\"end\":53542,\"start\":53541},{\"end\":53551,\"start\":53550},{\"end\":53553,\"start\":53552},{\"end\":54215,\"start\":54214},{\"end\":54530,\"start\":54529},{\"end\":54538,\"start\":54537},{\"end\":54864,\"start\":54863},{\"end\":54878,\"start\":54877},{\"end\":55179,\"start\":55178},{\"end\":55193,\"start\":55192},{\"end\":55203,\"start\":55202},{\"end\":55217,\"start\":55216},{\"end\":55230,\"start\":55229},{\"end\":55521,\"start\":55520},{\"end\":55528,\"start\":55527},{\"end\":55536,\"start\":55535},{\"end\":55543,\"start\":55542},{\"end\":55545,\"start\":55544},{\"end\":55789,\"start\":55788},{\"end\":55791,\"start\":55790},{\"end\":55801,\"start\":55800},{\"end\":55973,\"start\":55972},{\"end\":56239,\"start\":56238},{\"end\":56488,\"start\":56484},{\"end\":56495,\"start\":56494},{\"end\":56504,\"start\":56503},{\"end\":56516,\"start\":56515},{\"end\":56522,\"start\":56521},{\"end\":56819,\"start\":56818},{\"end\":56821,\"start\":56820},{\"end\":56828,\"start\":56827},{\"end\":56830,\"start\":56829},{\"end\":57108,\"start\":57107},{\"end\":57115,\"start\":57114},{\"end\":57117,\"start\":57116},{\"end\":57131,\"start\":57130},{\"end\":57133,\"start\":57132},{\"end\":57448,\"start\":57447},{\"end\":57458,\"start\":57457},{\"end\":57469,\"start\":57468},{\"end\":57745,\"start\":57744},{\"end\":57747,\"start\":57746},{\"end\":57761,\"start\":57760},{\"end\":57771,\"start\":57770},{\"end\":58037,\"start\":58036},{\"end\":58327,\"start\":58326},{\"end\":58329,\"start\":58328},{\"end\":58609,\"start\":58608},{\"end\":58611,\"start\":58610},{\"end\":58621,\"start\":58620},{\"end\":58623,\"start\":58622},{\"end\":58632,\"start\":58631},{\"end\":58634,\"start\":58633},{\"end\":58963,\"start\":58962},{\"end\":58965,\"start\":58964},{\"end\":58976,\"start\":58975},{\"end\":58995,\"start\":58994},{\"end\":58997,\"start\":58996},{\"end\":59008,\"start\":59007},{\"end\":59010,\"start\":59009},{\"end\":59393,\"start\":59392},{\"end\":59404,\"start\":59403},{\"end\":59416,\"start\":59415},{\"end\":59418,\"start\":59417},{\"end\":59749,\"start\":59748},{\"end\":59751,\"start\":59750},{\"end\":59760,\"start\":59759},{\"end\":59762,\"start\":59761}]", "bib_author_last_name": "[{\"end\":42581,\"start\":42575},{\"end\":42902,\"start\":42899},{\"end\":42910,\"start\":42906},{\"end\":42916,\"start\":42914},{\"end\":43259,\"start\":43256},{\"end\":43267,\"start\":43263},{\"end\":43273,\"start\":43271},{\"end\":43281,\"start\":43277},{\"end\":43292,\"start\":43287},{\"end\":43300,\"start\":43296},{\"end\":43751,\"start\":43748},{\"end\":43757,\"start\":43755},{\"end\":43765,\"start\":43761},{\"end\":43776,\"start\":43771},{\"end\":43784,\"start\":43780},{\"end\":44153,\"start\":44146},{\"end\":44166,\"start\":44159},{\"end\":44175,\"start\":44172},{\"end\":44445,\"start\":44443},{\"end\":44454,\"start\":44449},{\"end\":44460,\"start\":44458},{\"end\":44469,\"start\":44464},{\"end\":44861,\"start\":44856},{\"end\":44869,\"start\":44865},{\"end\":44878,\"start\":44875},{\"end\":44884,\"start\":44882},{\"end\":44896,\"start\":44890},{\"end\":44902,\"start\":44900},{\"end\":45297,\"start\":45292},{\"end\":45304,\"start\":45301},{\"end\":45312,\"start\":45308},{\"end\":45318,\"start\":45316},{\"end\":45664,\"start\":45658},{\"end\":45953,\"start\":45947},{\"end\":45964,\"start\":45957},{\"end\":45975,\"start\":45968},{\"end\":45987,\"start\":45981},{\"end\":46366,\"start\":46359},{\"end\":46381,\"start\":46372},{\"end\":46720,\"start\":46716},{\"end\":46728,\"start\":46724},{\"end\":46735,\"start\":46732},{\"end\":46743,\"start\":46739},{\"end\":47080,\"start\":47073},{\"end\":47093,\"start\":47086},{\"end\":47102,\"start\":47099},{\"end\":47507,\"start\":47500},{\"end\":47517,\"start\":47513},{\"end\":47540,\"start\":47523},{\"end\":47553,\"start\":47546},{\"end\":47562,\"start\":47559},{\"end\":47954,\"start\":47950},{\"end\":47965,\"start\":47961},{\"end\":47975,\"start\":47972},{\"end\":48440,\"start\":48436},{\"end\":48448,\"start\":48444},{\"end\":48458,\"start\":48455},{\"end\":48472,\"start\":48464},{\"end\":48848,\"start\":48839},{\"end\":48861,\"start\":48854},{\"end\":48871,\"start\":48865},{\"end\":49301,\"start\":49294},{\"end\":49312,\"start\":49307},{\"end\":49322,\"start\":49316},{\"end\":49338,\"start\":49326},{\"end\":49349,\"start\":49344},{\"end\":49704,\"start\":49700},{\"end\":49713,\"start\":49708},{\"end\":49719,\"start\":49717},{\"end\":49725,\"start\":49723},{\"end\":49733,\"start\":49729},{\"end\":50053,\"start\":50047},{\"end\":50064,\"start\":50057},{\"end\":50075,\"start\":50070},{\"end\":50087,\"start\":50083},{\"end\":50470,\"start\":50463},{\"end\":50482,\"start\":50476},{\"end\":50796,\"start\":50789},{\"end\":50806,\"start\":50800},{\"end\":51260,\"start\":51254},{\"end\":51271,\"start\":51264},{\"end\":51573,\"start\":51571},{\"end\":51795,\"start\":51789},{\"end\":51811,\"start\":51801},{\"end\":51824,\"start\":51815},{\"end\":52035,\"start\":52030},{\"end\":52045,\"start\":52039},{\"end\":52374,\"start\":52371},{\"end\":52382,\"start\":52378},{\"end\":52390,\"start\":52386},{\"end\":52704,\"start\":52700},{\"end\":52715,\"start\":52708},{\"end\":53072,\"start\":53069},{\"end\":53081,\"start\":53079},{\"end\":53091,\"start\":53088},{\"end\":53101,\"start\":53098},{\"end\":53113,\"start\":53108},{\"end\":53123,\"start\":53120},{\"end\":53539,\"start\":53533},{\"end\":53548,\"start\":53543},{\"end\":53561,\"start\":53554},{\"end\":54220,\"start\":54216},{\"end\":54535,\"start\":54531},{\"end\":54546,\"start\":54539},{\"end\":54875,\"start\":54865},{\"end\":54882,\"start\":54879},{\"end\":55190,\"start\":55180},{\"end\":55200,\"start\":55194},{\"end\":55214,\"start\":55204},{\"end\":55227,\"start\":55218},{\"end\":55244,\"start\":55231},{\"end\":55525,\"start\":55522},{\"end\":55533,\"start\":55529},{\"end\":55540,\"start\":55537},{\"end\":55551,\"start\":55546},{\"end\":55798,\"start\":55792},{\"end\":55804,\"start\":55802},{\"end\":55982,\"start\":55974},{\"end\":56245,\"start\":56240},{\"end\":56492,\"start\":56489},{\"end\":56501,\"start\":56496},{\"end\":56513,\"start\":56505},{\"end\":56519,\"start\":56517},{\"end\":56529,\"start\":56523},{\"end\":56825,\"start\":56822},{\"end\":56834,\"start\":56831},{\"end\":57112,\"start\":57109},{\"end\":57128,\"start\":57118},{\"end\":57139,\"start\":57134},{\"end\":57455,\"start\":57449},{\"end\":57466,\"start\":57459},{\"end\":57476,\"start\":57470},{\"end\":57758,\"start\":57748},{\"end\":57768,\"start\":57762},{\"end\":57779,\"start\":57772},{\"end\":58045,\"start\":58038},{\"end\":58336,\"start\":58330},{\"end\":58618,\"start\":58612},{\"end\":58629,\"start\":58624},{\"end\":58641,\"start\":58635},{\"end\":58973,\"start\":58966},{\"end\":58992,\"start\":58977},{\"end\":59005,\"start\":58998},{\"end\":59014,\"start\":59011},{\"end\":59401,\"start\":59394},{\"end\":59413,\"start\":59405},{\"end\":59424,\"start\":59419},{\"end\":59757,\"start\":59752},{\"end\":59769,\"start\":59763}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":15193271},\"end\":42823,\"start\":42452},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":129354398},\"end\":43111,\"start\":42825},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":206694588},\"end\":43615,\"start\":43113},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":57191028},\"end\":44076,\"start\":43617},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":14623536},\"end\":44367,\"start\":44078},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":19632191},\"end\":44762,\"start\":44369},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":23353851},\"end\":45169,\"start\":44764},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":203027299},\"end\":45594,\"start\":45171},{\"attributes\":{\"doi\":\"NESDIS11\",\"id\":\"b8\",\"matched_paper_id\":204255065},\"end\":45826,\"start\":45596},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":125250839},\"end\":46248,\"start\":45828},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":120230411},\"end\":46603,\"start\":46250},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":17356158},\"end\":46955,\"start\":46605},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":9213593},\"end\":47376,\"start\":46957},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":17696166},\"end\":47850,\"start\":47378},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":44131213},\"end\":48245,\"start\":47852},{\"attributes\":{\"doi\":\"10.1145/3219819.3219926\",\"id\":\"b15\"},\"end\":48329,\"start\":48247},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":126747591},\"end\":48712,\"start\":48331},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":54443158},\"end\":49209,\"start\":48714},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":7113724},\"end\":49601,\"start\":49211},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":215723287},\"end\":49935,\"start\":49603},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":221259124},\"end\":50336,\"start\":49937},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":118535606},\"end\":50726,\"start\":50338},{\"attributes\":{\"id\":\"b22\"},\"end\":51145,\"start\":50728},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":219930533},\"end\":51493,\"start\":51147},{\"attributes\":{\"doi\":\"10.1093/nsr/nwaa047\",\"id\":\"b24\",\"matched_paper_id\":216255712},\"end\":51785,\"start\":51495},{\"attributes\":{\"id\":\"b25\"},\"end\":51966,\"start\":51787},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":6916627},\"end\":52301,\"start\":51968},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":206592295},\"end\":52617,\"start\":52303},{\"attributes\":{\"doi\":\"10.1029/2007GL029960\",\"id\":\"b28\",\"matched_paper_id\":140712987},\"end\":52953,\"start\":52619},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":130105037},\"end\":53384,\"start\":52955},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":14123121},\"end\":53840,\"start\":53386},{\"attributes\":{\"id\":\"b31\"},\"end\":54130,\"start\":53842},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":123263526},\"end\":54417,\"start\":54132},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":110702742},\"end\":54775,\"start\":54419},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":8541111},\"end\":55109,\"start\":54777},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":6844431},\"end\":55474,\"start\":55111},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":49867180},\"end\":55742,\"start\":55476},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b37\"},\"end\":55968,\"start\":55744},{\"attributes\":{\"id\":\"b38\"},\"end\":56158,\"start\":55970},{\"attributes\":{\"doi\":\"arXiv:1711.05365\",\"id\":\"b39\"},\"end\":56443,\"start\":56160},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":47252984},\"end\":56751,\"start\":56445},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":14561315},\"end\":57021,\"start\":56753},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":38548719},\"end\":57363,\"start\":57023},{\"attributes\":{\"doi\":\"arXiv:1707.04131\",\"id\":\"b43\"},\"end\":57694,\"start\":57365},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":6706414},\"end\":57992,\"start\":57696},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":604334},\"end\":58226,\"start\":57994},{\"attributes\":{\"id\":\"b46\"},\"end\":58536,\"start\":58228},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":7196073},\"end\":58840,\"start\":58538},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":14110182},\"end\":59289,\"start\":58842},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":121977007},\"end\":59657,\"start\":59291},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":122321885},\"end\":59996,\"start\":59659}]", "bib_title": "[{\"end\":42571,\"start\":42452},{\"end\":42895,\"start\":42825},{\"end\":43252,\"start\":43113},{\"end\":43744,\"start\":43617},{\"end\":44140,\"start\":44078},{\"end\":44439,\"start\":44369},{\"end\":44852,\"start\":44764},{\"end\":45288,\"start\":45171},{\"end\":45652,\"start\":45596},{\"end\":45943,\"start\":45828},{\"end\":46355,\"start\":46250},{\"end\":46712,\"start\":46605},{\"end\":47067,\"start\":46957},{\"end\":47494,\"start\":47378},{\"end\":47946,\"start\":47852},{\"end\":48429,\"start\":48331},{\"end\":48833,\"start\":48714},{\"end\":49290,\"start\":49211},{\"end\":49696,\"start\":49603},{\"end\":50041,\"start\":49937},{\"end\":50457,\"start\":50338},{\"end\":50785,\"start\":50728},{\"end\":51248,\"start\":51147},{\"end\":51567,\"start\":51495},{\"end\":52026,\"start\":51968},{\"end\":52367,\"start\":52303},{\"end\":52696,\"start\":52619},{\"end\":53065,\"start\":52955},{\"end\":53527,\"start\":53386},{\"end\":53898,\"start\":53842},{\"end\":54212,\"start\":54132},{\"end\":54527,\"start\":54419},{\"end\":54861,\"start\":54777},{\"end\":55176,\"start\":55111},{\"end\":55518,\"start\":55476},{\"end\":56482,\"start\":56445},{\"end\":56816,\"start\":56753},{\"end\":57105,\"start\":57023},{\"end\":57742,\"start\":57696},{\"end\":58034,\"start\":57994},{\"end\":58606,\"start\":58538},{\"end\":58960,\"start\":58842},{\"end\":59390,\"start\":59291},{\"end\":59746,\"start\":59659}]", "bib_author": "[{\"end\":42583,\"start\":42573},{\"end\":42904,\"start\":42897},{\"end\":42912,\"start\":42904},{\"end\":42918,\"start\":42912},{\"end\":43261,\"start\":43254},{\"end\":43269,\"start\":43261},{\"end\":43275,\"start\":43269},{\"end\":43283,\"start\":43275},{\"end\":43294,\"start\":43283},{\"end\":43302,\"start\":43294},{\"end\":43753,\"start\":43746},{\"end\":43759,\"start\":43753},{\"end\":43767,\"start\":43759},{\"end\":43778,\"start\":43767},{\"end\":43786,\"start\":43778},{\"end\":44155,\"start\":44142},{\"end\":44168,\"start\":44155},{\"end\":44177,\"start\":44168},{\"end\":44447,\"start\":44441},{\"end\":44456,\"start\":44447},{\"end\":44462,\"start\":44456},{\"end\":44471,\"start\":44462},{\"end\":44863,\"start\":44854},{\"end\":44871,\"start\":44863},{\"end\":44880,\"start\":44871},{\"end\":44886,\"start\":44880},{\"end\":44898,\"start\":44886},{\"end\":44904,\"start\":44898},{\"end\":45299,\"start\":45290},{\"end\":45306,\"start\":45299},{\"end\":45314,\"start\":45306},{\"end\":45320,\"start\":45314},{\"end\":45666,\"start\":45654},{\"end\":45955,\"start\":45945},{\"end\":45966,\"start\":45955},{\"end\":45977,\"start\":45966},{\"end\":45989,\"start\":45977},{\"end\":46368,\"start\":46357},{\"end\":46383,\"start\":46368},{\"end\":46389,\"start\":46383},{\"end\":46722,\"start\":46714},{\"end\":46730,\"start\":46722},{\"end\":46737,\"start\":46730},{\"end\":46745,\"start\":46737},{\"end\":47082,\"start\":47069},{\"end\":47095,\"start\":47082},{\"end\":47104,\"start\":47095},{\"end\":47509,\"start\":47496},{\"end\":47519,\"start\":47509},{\"end\":47542,\"start\":47519},{\"end\":47555,\"start\":47542},{\"end\":47564,\"start\":47555},{\"end\":47956,\"start\":47948},{\"end\":47967,\"start\":47956},{\"end\":47977,\"start\":47967},{\"end\":48442,\"start\":48431},{\"end\":48450,\"start\":48442},{\"end\":48460,\"start\":48450},{\"end\":48474,\"start\":48460},{\"end\":48850,\"start\":48835},{\"end\":48863,\"start\":48850},{\"end\":48873,\"start\":48863},{\"end\":49303,\"start\":49292},{\"end\":49314,\"start\":49303},{\"end\":49324,\"start\":49314},{\"end\":49340,\"start\":49324},{\"end\":49351,\"start\":49340},{\"end\":49706,\"start\":49698},{\"end\":49715,\"start\":49706},{\"end\":49721,\"start\":49715},{\"end\":49727,\"start\":49721},{\"end\":49735,\"start\":49727},{\"end\":50055,\"start\":50043},{\"end\":50066,\"start\":50055},{\"end\":50077,\"start\":50066},{\"end\":50089,\"start\":50077},{\"end\":50472,\"start\":50459},{\"end\":50484,\"start\":50472},{\"end\":50798,\"start\":50787},{\"end\":50808,\"start\":50798},{\"end\":51262,\"start\":51250},{\"end\":51273,\"start\":51262},{\"end\":51575,\"start\":51569},{\"end\":51797,\"start\":51787},{\"end\":51813,\"start\":51797},{\"end\":51826,\"start\":51813},{\"end\":52037,\"start\":52028},{\"end\":52047,\"start\":52037},{\"end\":52376,\"start\":52369},{\"end\":52384,\"start\":52376},{\"end\":52392,\"start\":52384},{\"end\":52706,\"start\":52698},{\"end\":52717,\"start\":52706},{\"end\":53074,\"start\":53067},{\"end\":53083,\"start\":53074},{\"end\":53093,\"start\":53083},{\"end\":53103,\"start\":53093},{\"end\":53115,\"start\":53103},{\"end\":53125,\"start\":53115},{\"end\":53541,\"start\":53529},{\"end\":53550,\"start\":53541},{\"end\":53563,\"start\":53550},{\"end\":54222,\"start\":54214},{\"end\":54537,\"start\":54529},{\"end\":54548,\"start\":54537},{\"end\":54877,\"start\":54863},{\"end\":54884,\"start\":54877},{\"end\":55192,\"start\":55178},{\"end\":55202,\"start\":55192},{\"end\":55216,\"start\":55202},{\"end\":55229,\"start\":55216},{\"end\":55246,\"start\":55229},{\"end\":55527,\"start\":55520},{\"end\":55535,\"start\":55527},{\"end\":55542,\"start\":55535},{\"end\":55553,\"start\":55542},{\"end\":55800,\"start\":55788},{\"end\":55806,\"start\":55800},{\"end\":55984,\"start\":55972},{\"end\":56247,\"start\":56238},{\"end\":56494,\"start\":56484},{\"end\":56503,\"start\":56494},{\"end\":56515,\"start\":56503},{\"end\":56521,\"start\":56515},{\"end\":56531,\"start\":56521},{\"end\":56827,\"start\":56818},{\"end\":56836,\"start\":56827},{\"end\":57114,\"start\":57107},{\"end\":57130,\"start\":57114},{\"end\":57141,\"start\":57130},{\"end\":57457,\"start\":57447},{\"end\":57468,\"start\":57457},{\"end\":57478,\"start\":57468},{\"end\":57760,\"start\":57744},{\"end\":57770,\"start\":57760},{\"end\":57781,\"start\":57770},{\"end\":58047,\"start\":58036},{\"end\":58338,\"start\":58326},{\"end\":58620,\"start\":58608},{\"end\":58631,\"start\":58620},{\"end\":58643,\"start\":58631},{\"end\":58975,\"start\":58962},{\"end\":58994,\"start\":58975},{\"end\":59007,\"start\":58994},{\"end\":59016,\"start\":59007},{\"end\":59403,\"start\":59392},{\"end\":59415,\"start\":59403},{\"end\":59426,\"start\":59415},{\"end\":59759,\"start\":59748},{\"end\":59771,\"start\":59759}]", "bib_venue": "[{\"end\":44569,\"start\":44524},{\"end\":48041,\"start\":48013},{\"end\":48961,\"start\":48921},{\"end\":50908,\"start\":50855},{\"end\":51859,\"start\":51841},{\"end\":52115,\"start\":52097},{\"end\":52458,\"start\":52429},{\"end\":55605,\"start\":55583},{\"end\":57843,\"start\":57816},{\"end\":58109,\"start\":58082},{\"end\":42608,\"start\":42583},{\"end\":42937,\"start\":42918},{\"end\":43333,\"start\":43302},{\"end\":43817,\"start\":43786},{\"end\":44196,\"start\":44177},{\"end\":44522,\"start\":44471},{\"end\":44935,\"start\":44904},{\"end\":45351,\"start\":45320},{\"end\":45678,\"start\":45674},{\"end\":46008,\"start\":45989},{\"end\":46399,\"start\":46389},{\"end\":46755,\"start\":46745},{\"end\":47135,\"start\":47104},{\"end\":47583,\"start\":47564},{\"end\":48011,\"start\":47977},{\"end\":48493,\"start\":48474},{\"end\":48919,\"start\":48873},{\"end\":49376,\"start\":49351},{\"end\":49746,\"start\":49735},{\"end\":50106,\"start\":50089},{\"end\":50503,\"start\":50484},{\"end\":50853,\"start\":50808},{\"end\":51292,\"start\":51273},{\"end\":51607,\"start\":51594},{\"end\":51839,\"start\":51826},{\"end\":52095,\"start\":52047},{\"end\":52427,\"start\":52392},{\"end\":52755,\"start\":52737},{\"end\":53141,\"start\":53125},{\"end\":53581,\"start\":53563},{\"end\":53927,\"start\":53900},{\"end\":54246,\"start\":54222},{\"end\":54569,\"start\":54548},{\"end\":54916,\"start\":54884},{\"end\":55265,\"start\":55246},{\"end\":55581,\"start\":55553},{\"end\":55786,\"start\":55744},{\"end\":56025,\"start\":55984},{\"end\":56236,\"start\":56160},{\"end\":56569,\"start\":56531},{\"end\":56859,\"start\":56836},{\"end\":57162,\"start\":57141},{\"end\":57445,\"start\":57365},{\"end\":57814,\"start\":57781},{\"end\":58080,\"start\":58047},{\"end\":58324,\"start\":58228},{\"end\":58661,\"start\":58643},{\"end\":59035,\"start\":59016},{\"end\":59445,\"start\":59426},{\"end\":59791,\"start\":59771}]"}}}, "year": 2023, "month": 12, "day": 17}