{"id": 234470115, "updated": "2023-10-06 03:33:22.059", "metadata": {"title": "A Large-Scale Benchmark for Food Image Segmentation", "authors": "[{\"first\":\"Xiongwei\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Xin\",\"last\":\"Fu\",\"middle\":[]},{\"first\":\"Ying\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Ee-Peng\",\"last\":\"Lim\",\"middle\":[]},{\"first\":\"Steven\",\"last\":\"Hoi\",\"middle\":[\"C.H.\"]},{\"first\":\"Qianru\",\"last\":\"Sun\",\"middle\":[]}]", "venue": null, "journal": "Proceedings of the 29th ACM International Conference on Multimedia", "publication_date": {"year": 2021, "month": 5, "day": 12}, "abstract": "Food image segmentation is a critical and indispensible task for developing health-related applications such as estimating food calories and nutrients. Existing food image segmentation models are underperforming due to two reasons: (1) there is a lack of high quality food image datasets with fine-grained ingredient labels and pixel-wise location masks -- the existing datasets either carry coarse ingredient labels or are small in size; and (2) the complex appearance of food makes it difficult to localize and recognize ingredients in food images, e.g., the ingredients may overlap one another in the same image, and the identical ingredient may appear distinctly in different food images. In this work, we build a new food image dataset FoodSeg103 (and its extension FoodSeg154) containing 9,490 images. We annotate these images with 154 ingredient classes and each image has an average of 6 ingredient labels and pixel-wise masks. In addition, we propose a multi-modality pre-training approach called ReLeM that explicitly equips a segmentation model with rich and semantic food knowledge. In experiments, we use three popular semantic segmentation methods (i.e., Dilated Convolution based, Feature Pyramid based, and Vision Transformer based) as baselines, and evaluate them as well as ReLeM on our new datasets. We believe that the FoodSeg103 (and its extension FoodSeg154) and the pre-trained models using ReLeM can serve as a benchmark to facilitate future works on fine-grained food image understanding. We make all these datasets and methods public at \\url{https://xiongweiwu.github.io/foodseg103.html}.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2105.05409", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/mm/WuFLLHS21", "doi": "10.1145/3474085.3475201"}}, "content": {"source": {"pdf_hash": "99e6e9541166c1293d30e0ccf259f1d94f28e953", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2105.05409v1.pdf\"]", "oa_url_match": false, "oa_info": {"license": "CCBYNCND", "open_access_url": "https://ink.library.smu.edu.sg/sis_research/6269", "status": "GREEN"}}, "grobid": {"id": "5e325b23110179c47d0090b62715f981cb7186bb", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/99e6e9541166c1293d30e0ccf259f1d94f28e953.txt", "contents": "\nA Large-Scale Benchmark for Food Image Segmentation\n\n\nXiongwei Wu xwwu@smu.edu.sg \nSalesforce Research Asia\nSingapore Management University\nBeijing Jiaotong University\nSingapore Management University\nSingapore Management University\nSingapore Management University\nSingapore Management University\n\n\nXin Fu xinfu@bjtu.edu.cn \nSalesforce Research Asia\nSingapore Management University\nBeijing Jiaotong University\nSingapore Management University\nSingapore Management University\nSingapore Management University\nSingapore Management University\n\n\nYing Liu \nSalesforce Research Asia\nSingapore Management University\nBeijing Jiaotong University\nSingapore Management University\nSingapore Management University\nSingapore Management University\nSingapore Management University\n\n\nEe-Peng Lim eplim@smu.edu.sg \nSalesforce Research Asia\nSingapore Management University\nBeijing Jiaotong University\nSingapore Management University\nSingapore Management University\nSingapore Management University\nSingapore Management University\n\n\nSteven C H Hoi \nSalesforce Research Asia\nSingapore Management University\nBeijing Jiaotong University\nSingapore Management University\nSingapore Management University\nSingapore Management University\nSingapore Management University\n\n\nQianru Sun qianrusun@smu.edu.sg \nSalesforce Research Asia\nSingapore Management University\nBeijing Jiaotong University\nSingapore Management University\nSingapore Management University\nSingapore Management University\nSingapore Management University\n\n\nA Large-Scale Benchmark for Food Image Segmentation\n\nFood image segmentation is a critical and indispensible task for developing health-related applications such as estimating food calories and nutrients. Existing food image segmentation models are underperforming due to two reasons: (1) there is a lack of high quality food image datasets with fine-grained ingredient labels and pixel-wise location masks-the existing datasets either carry coarse ingredient labels or are small in size; and (2) the complex appearance of food makes it difficult to localize and recognize ingredients in food images, e.g., the ingredients may overlap one another in the same image, and the identical ingredient may appear distinctly in different food images.In this work, we build a new food image dataset FoodSeg103 (and its extension FoodSeg154) containing 9,490 images. We annotate these images with 154 ingredient classes and each image has an average of 6 ingredient labels and pixel-wise masks. In addition, we propose a multi-modality pre-training approach called ReLeM that explicitly equips a segmentation model with rich and semantic food knowledge. In experiments, we use three popular semantic segmentation methods (i.e., Dilated Convolution based [17], Feature Pyramid based[22], and Vision Transformer based [54]) as baselines, and evaluate them as well as ReLeM on our new datasets. We believe that the FoodSeg103 (and its extension FoodSeg154) and the pre-trained models using ReLeM can serve as a benchmark to facilitate future works on fine-grained food image understanding. We make all these datasets and methods public at\n\nINTRODUCTION\n\nFood computing has attracted increasing public attention in recent years, as it provides the core technologies for food and healthrelated research and applications. [2,9,31,43]. One of the important goals of food computing is to automatically recognize different types of food and profile their nutrition and calorie values. In computer vision, the related works include dish classification [11,50,52], recipe generation [14,39,46], and food image retrieval [6,42]. Most of them focus on representing and analysing the food image as a whole, and do not explicitly localize or classify its individual ingredients-the visible components in the cooked food. We call the former food image classification and the latter food image segmentation. Between the two, food image segmentation is more complex as it aims to recognize each ingredient category as well as its pixel-wise locations in the food image. As shown in Figure 1, given an \"hamburger\" example image, a good segmentation model  needs to recognize and mask out \"beef\", \"tomato\", \"lettuce\", \"onion\" and \"bread roll\" ingredients. Compared to semantic segmentation on general object images [3,17,22], food image segmentation is more challenging due to the large diversity in food appearances and the often imbalanced distribution of categories of ingredients. First, an ingredient cooked differently can vary a lot visually, e.g., \"pineapples\" cooked with meat in Figure 1 (a) versus the \"pineapples\" in a fruit platter in Figure 1 (b). Different ingredients may look very similar, e.g., \"pineapples\" cooked with meat cannot be easily distinguished from \"potatoes\" cooked with meat, as shown in Figures 1 (a) and (c) respectively. Second, food datasets usually suffer from imbalanced distributionboth food classes and ingredient classes often exist in long-tailed distributions. This is inevitable due to two reasons: 1) large number of food images are dominated by very few popular food classes while vast majority of food classes are unpopular; and 2) there is a selection bias in the construction of food image collection [44]. We will elaborate the detailed distribution analysis in Section 3.\n\nExisting food image datasets, such as ETH Food101 [1], Recipe1M [41], and Geo-Dish [52], mainly facilitate the research of dish classification or recipe generation. They do not have fine-grained ingredient masks or labels. UECFoodPix [13] and UECFoodPixComplete [35] are the only two public datasets for food image segmentation. However, their segmentation masks are annotated at dish level only. That is, each mask covers the region of an entire dish instead of that of food ingredients. We elaborate more dataset comparison in Section 3.3.\n\nDataset contribution: To facilitate fine-grained food image segmentation, we build a large-scale dataset called FoodSeg103, for which we have defined 103 ingredient classes and annotated 7,118 western food images using these labels together with the corresponding segmentation masks. Besides, we annotated an additional set of 2,372 images of Asian food which covers more diverse set of ingredients making these images more challenging than those in the main set (FoodSeg103). For this set, we defined 112 ingredient classes-55% overlap with the ingredient classes of the main set. In total, we annotated 154 classes of ingredients with around 60k masks (in the two datasets). We name the combined dataset as FoodSeg154. During the annotation, we carried out careful data selection, iterative refinement of labels and masks (to be further elaborated in Section 3.2), so as to guarantee high quality labels and masks in the dataset. Our annotation is thus expensive and time-consuming. In experiments, we use FoodSeg103 for in-domain training and testing, and use the additional set in FoodSeg154 for out-domain testing.\n\nModel contribution: The source images of FoodSeg103 are from another existing food dataset Recipe1M [41]-millions of images and cooking recipes, used for recipe generation. Each recipe contains not only \"how to cook\" but also \"what ingredient to use\". In our work, we leverage these recipe information as auxiliary information to train semantic segmentation models. We call this multi-modality knowledge transfer and name our training method ReLeM. Specifically, ReLeM integrates food recipe data, in the format of language embedding, with the visual representation of the food image. In this way, it forces the visual representation of an ingredient appearing in different dishes to have their appearances \"connected\" in the feature space through a common language embedding (extracted from the ingredient's label and its cooking instructions).\n\nExperiment contribution: We validate our proposed ReLeM model by plugging it into the state-of-the-art semantic segmentation models such as CCNet [17], Sem-FPN [22] and SeTR [54]. In experiments, we compare ReLeM-variants with these baseline models using both convolutional networks and transformer backbones. Our experiments show that ReLeM is generic to be applied into multiple segmentation frameworks, and it helps to achieve significant accuracy improvement when incorporated into the SOTA CNNbased model CCNet. This validates that our knowledge transfer approach works more efficient on stronger models-a characteristic preferred by the multimedia community.\n\nOur contributions are thus three-fold. i) We build a large-scale food image segmentation dataset called FoodSeg103 (and its extension FoodSeg154). It can facilitate a promising and challenging benchmark for the task of semantic segmentation in food images. ii) We propose a knowledge transfer approach ReLeM that utilizes the multi-modality information of recipe datasets. It can be incorporated into different semantic segmentation methods to boost the model performance. iii) We conduct extensive experiments that reveal the challenges of segmenting food on our FoodSeg103 dataset, and validate the efficiency of our ReLeM based on multiple baseline methods.\n\n\nRELATED WORKS\n\nFood Image Datasets. In recent years, the scale of food-related datasets has grown rapidly. For example, Bossard et al [1] built one large-scale food dataset ETH Food101, which contains 101 classes with 1,000 images per class. Matsuda et al. [30] constructed a Japanese food dataset UEC Food100 with 15K images in 100 dish categories. In comparison, ISIA Food500 [34] contains nearly 400k food images in 500 categories, which is the largest food image recognition. In addition, there are also recipe-related datasets. Salvador et al. [41] built the Recipe1M, with nearly 900k images and 1 million recipes, which is widely used in multi-modal learning between images and recipes. Based on Recipe1M, an even larger dataset Recipe1M+ [28] was constructed with more than 13 millions of food images. However, these datasets are mainly built to support food recognition and recipe generation research rather than food image segmentation, so they do not segment food images into multiple masks and labels of ingredient . UECFoodPix [13] and UECFoodPixComplete [35] are the only two datasets for food image segmentation, which contains 10,000 images with more than 100 categories. Nevertheless, their annotation are limited to dish-wise masks so they cannot be used for ingredient segmentation.\n\nIn this paper, we built FoodSeg103 dataset with 7,118 images and more than 40k masks covering 103 food ingredients. In addition, we have collected another image set for Asian food with 2,372 images (for cross-domain evaluation of the models). Combining the main set and the Asian set, we get the FoodSeg154 with nearly 10k images and 60k ingredient masks. To our best knowledge, FoodSeg154 is the first and the largest ingredient-level dataset for fine-grained food image segmentation. Dataset is a key step in developing deep learning based methods. We hope our dataset can inspire more efforts for the task of food image segmentation. Semantic Segmentation in Images. Deep learning based semantic segmentation is a super hot topic in recent years. Fully convolutional neural network (FCN) [27] is the first semantic segmentation framework based on deep convolutional neural networks. It predicts pixel-wise masks by replacing the fully connected layers with convolution layers and achieves a clear margin of improvement on the model performance. Chen et al. [3] proposed DeepLab which applies dilated convolutional layers in vanilla FCN. The trained model is more effective as the dilation mechanism enlarges the receptive fields while maintaining a high resolution in feature maps. Chen et al [4] proposed the DeepLab v2, which adds an ASPP module to integrate features of different dilation rates. To further include contextual cues, PSPNet [53] proposed a PPM module that aggregates the contextual information using different-size pooling layers. Wang et al. [48] proposed the non-local networks to encode the relationship between each pair of pixels in the feature map. Based on the non-local networks, CCNet [17] adopted a criss-cross attention layer to significantly economize the computation costs of calculating attentions. Most recently, vision transformer (attentionbased) [12,45] was adapted to tackle semantic segmentation problems in [54]. recently and achieves state-of-the-art results [54]. In this paper, we conduct extensive experiments on our dataset using  three representative semantic segmentation methods: CCNet [17], FPN [22] and SeTR [54]. We also plug the proposed ReLeM into these methods to show its general efficiency.\n\n\nFOOD IMAGE SEGMENTATION DATASET\n\nFoodSeg103 is a subset of FoodSeg154, and the latter includes an additional subset of Asian food images and annotations. Some example images and their annotations can be found in Figure 2.\n\nIn FoodSeg103, we have defined 103 ingredient categories and assigned these category labels as well as the segmentation masks to 7,118 images. The images are from an existing recipe dataset called Recipe1M [41]. For the additional subset in FoodSeg154, we specially collect 2,372 images of Asian food which is of larger diversity than the Western food in FoodSeg103. We use this subset to evaluate the domain adaptation performance of our food image segmentation models. We release FoodSeg103 to facilitate public research, but currently we cannot make the Asian food set public due to the confidentiality of the images.\n\n\nCollecting Food Images\n\nWe use FoodSeg103 as an example to elaborate the dataset construction process. We elaborate the image source, category compilation and image selection as follows. Source: We used Recipe1M [28,41] as our source dataset. This dataset contains 900k images with cooking instructions and ingredient labels, which are used for food image retrieval and recipe generation tasks. Categories: First, we counted the frequency of all ingredient categories in Recipe1M. While there are around 1.5k ingredient categories [40], most of them are not easy to be masked out from images. Hence, we kept only the top 124 ingredient categories (with further refinement, this number became 103) and assigned ingredients with the \"others\" category when they do not fall under the above 124 categories. Finally, we grouped these categories into 14 superclass categories, e.g., \"Main\" (i.e., main staple) is a superclass category covering more fine-grained categories such as \"noodle\" and \"rice\". Images:\n\nIn each fine-grained ingredient category, we sampled Recipe1M images based on the following two criteria: 1) the image should contain at least two ingredients (with the same or different categories) but not more than 16 ingredients; and 2) the ingredients should be visible in the images and easy-to-annotate. Finally, we obtained 7,118 images to annotate masks.\n\n\nAnnotating Ingredient Labels and Masks\n\nGiven the above images, the next step is to annotate segmentation masks, i.e., the polygons covering the pixel-wise locations of different ingredients. This effort includes the mask annotation and mask refinement steps. Annotation: We engaged a data annotation company to perform mask annotation, a laborious and painstaking job. For each image, a human annotator first identifies the categories of ingredients in the image, tags each ingredient with the appropriate category label and draws the pixel-wise mask. We asked the annotators to ignore tiny image regions (even if it may contain some ingredients) with area covering less than 5% of the whole image. Refinement: After receiving all masks from the annotation company, we further conducted an overall refinement. We followed three refinement criteria: 1) correcting mislabeled data; 2) deleting unpopular category labels that are assigned to less than 5 images, and 3) merging visually similar ingredient categories, such as orange and citrus. After refinement, we reduced the initial set of 125 ingredient categories to 103. Figure 5 shows some examples refined by us. The annotation and refinement works took around one year. We show some data examples in Figure 2. In Figure 2 (a), we give some easy cases where the boundaries of ingredients are clear and the image compositions are not complex. In Figure 2 (b) and (c), we show some difficult cases with overlapped ingredient regions and complex compositions in the images. Figure 3 shows the distributions of fine-grained ingredient categories and superclass categories. Figures 3(a) and 3(c) show partial statistics for small subsets of categories due to page limit. The complete statistics will be published when releasing the dataset.\n\n\nComparing with Food Image Datasets\n\nFood Image Datasets. We summarize the comparison results in Table 1. We only include datasets that are mainly used for food recognition tasks. They contain images and dish-level labels, and therefore    [35]. Ingredient-level annotation contains more details.\n\nthey do not have any ingredient-level annotations. Recipe1M and Recipe1M+ include ingredient labels for each images but not the segmentation masks. Notably, there are two datasets for food image segmentation: UECFoodPix [13] and UECFoodPixComplete [35]. Below, we compare these two with our datasets FoodSeg103 and FoodSeg154 in detail. Food Image Segmentation Datasets. UECFoodPix and UECFood-PixComplete (UECFoodPixComp.) are two public datasets for food image segmentation, with 10k images and 102 dish categories. Detailed comparison numbers are given in Table 2. We highlight three advantages of our FoodSeg103 and FoodSeg154: 1) the number of pixel-wise masks of FoodSeg (40k and 60k) is significantly larger than UEC dataset (only 10k); 2) the annotation mask in UECFood-Pix and UECFoodPixComp covers entire dish but not ingredients (dish components), while our FoodSeg154 and FoodSeg103 have ingredient-wise masks, which better capture the characteristic of the food. Illustrative comparisons are given in Figure 4. In Table 2, we not only present the statistic numbers but also evaluate FoodSeg103, UECFoodPix and UECFoodPixComplete using deeplabv3+ as a baseline model. The last row of the table shows that FoodSeg103 serves as a more challenging benchmark for semantic segmentation. Moreover, fine-grained ingredient annotations in our datasets are more useful for analyzing food nutrition and estimating calories in health-related applications.\n\n\nFOOD IMAGE SEGMENTATION FRAMEWORK\n\nAs shown in Figure 6, our food image segmentation framework contains two modules. One is the recipe learning module (ReLeM) to incorporate recipes in the form of language embedding into the visual representation of a food image. We call this approach multi-modality knowledge transfer. In this approach, we explicitly force the visual representations of the same ingredient appearing in different dishes to be \"connected\" in the feature space through\n\n\nDataset\n\nYear Type #Dish #Ingr. Images PFID [5] 2009   the common language embedding (extracted from the ingredient label and its cooking instructions), so as to handle the high variance of the ingredient appearing in different dishes. The other module of our framework is the encoder-decoder based image segmentation. Its encoder is initialized using the one trained by ReLeM, and its decoder is randomly initialized and trained with the segmentation masks. We next introduce the two modules in detail. Food image segmentation can be viewed as a special type of semantic segmentation [25,54]. It is more difficult than normal image segmentation due to: 1) the ingredient cooked with different methods can vary a lot by appearances, and 2) ingredient distribution is inevitably long-tailed making the data very sparse for ingredients in the long tail. Given a food image, the Segmenter identifies the ingredient categories and also mask out the corresponding pixels for each category (class). The common metrics for measuring Segmenter's performance include mIoU (mean IoU over each class), mACC (mean accuracy over all classes) and aAcc (over all pixels), See Figure 7 for more details of IoU and accuracy (Acc) calculation.  Figure 6: Our food image segmentation framework consists of two modules: Recipe Learning Module (ReLeM) and Image Segmentation Module (Segmenter). For ReLeM, we encode the recipe information into the visual representation of the food image. We deploy the cosine similarity to compute the distance between two distinct-modality models, together with a semantic loss [41]. After training, we use the trained encoder to initialize the encoder of the Segmenter. The decoder of the Segmenter is trained with the segmentation masks from a random initialization. \n\n\nRecipe Learning Module (ReLeM)\n\nOverview. We propose ReLeM to reduce the large intra-variance of ingredients caused by different cooking methods mentioned in the recipes. Specifically, our training method integrates the recipe information into the visual representation of the corresponding image. Assume an ingredient in two different images are cooked in different methods. The visual representations of the ingredients from vision encoder are denoted as 1 and 2 , where 1 and 2 have significant difference in the visual space. ReLeM aims to reduce this difference according to its word embedding of the cooking instructions of the two recipes 1 and 2 respectively in the language space.\n| ( 1 | 1 ) \u2212 ( 2 | 2 )| < | ( 1 ) \u2212 ( 2 )|(1)\nwhere is the vision decoder in the Segmenter (elaborated in Section 4.2). Our ReLeM is optimized by using two loss terms: cosine similarity loss between features, and semantic loss (distance) between the text representation and the visual representation of the same image:\ncosine (( , ), ) = 1 \u2212 ( , ) = 1 (0, ( , ) \u2212 ) = \u22121 (2) semantic (( , ), , ) = CE( , ) + CE( , )(3)\nwhere denotes whether and are from the same recipe. and denote the semantic class of and respectively, and is the margin parameter, which is set to 0.1. As Recipe1M does not contain specific semantic labels (i.e., dish names), we define 2,000 semantic labels for it by selecting the most frequent dish names appeared in its recipe titles. Preprocessing. Each recipe contains ingredients and cooking instructions. Some preprocessing steps are required to encode ingredients and instructions from raw text into the fixed length vectors before they are fed into the text encoder. Specifically, we first extract useful ingredient and instruction texts from the raw recipe data by removing redundant words. For each ingredient, we learn a word2vec [32] representation using a bi-directional LSTM. As the sequence of instructions can be long, it is difficult for LSTM to encode them, due to the gradient vanishing issue. Following a previous work [41], we encode the instructions with a skip-instructions [23] to generate the feature vectors with a fixed length. Text Encoder. The text encoder is a general module to extract text knowledge from ingredient labels and cooking instructions. We use two types of text encoders: LSTM-based encoder and transformerbased encoder. For LSTM-based, we use a bi-directional LSTM to encode ingredient features and a LSTM to encode instruction features. For transformer-based model, we use two light-weight transformers, each of which contains 2 transformer layers with 4-head self-attention modules. Vision Encoder. The vision encoder used in ReLeM aims to extract the visual knowledge from the input image, and the weights will initialize the vision encoder in the segmenter. In this paper, two vision encoders are used: ResNet-50 [15] based on convolutional neural network and ViT-16/B [12] based on vision transformers.\n\n\nImage Segmentation Module (Segmenter)\n\nOur framework follows the standard paradigm of semantic segmentation, where the input image is first encoded in a vision encoder, and then goes through a vision decoder for mask prediction. The existing segmentation models can be roughly divided into three groups, based on the different designs of encoder and decoder: Dilation based, Feature Pyramid Networks (FPN) based and Transformer based. Dilation based. Dilation convolution layers aim to enlarge the receptive fields without sacrificing the resolution, as shown in Figure 8 (a). In its decoder, only the last-layer feature maps are used for prediction [4,17], as shown in Figure 9 (a). FPN based. FPN integrates feature maps in different layers by the lateral connection. The shallow-layer image representation is enhanced by integrating the feature maps generated in deep layers, as shown in Figure 8 (b). In its decoder, a set of feature pyramids are merged together followed with a mask predictor, as shown in Figure 9 (b). Transformer based. Transformer is based on attention, which suits semantic segmentation tasks well--the contextual information is important in segmenting objects. Moreover, the receptive fields can be enlarged via attention mechanism [45,54]. The transformerbased model reshapes the image into a sequence of regions and then encodes them by a sequence of attention modules, as shown in Figure 8 (c). Its decoder predicts segmentation masks on the last-layer feature maps, as shown in Figure 9(c).\n\nIn this paper, we conduct experiments using three representative frameworks of these three types, respectively, i.e., CCNet (Dilation) [17], FPN [22] and SeTR (Transformer) [54]. Note that the encoder of Segmenter is pre-trained by our ReLeM. With LSTM and transformer-based text encoding, we arrive at 6 different ReLeM models, i.e., ReLeM-{ CCNet, FPN, SeTR}\u00d7 ({ LSTM, Transformer}). We use the standard pixel-wise cross-entropy loss to optimize segmentation models.\n\n\nEXPERIMENTS\n\nWe conduct extensive experiments on our dataset FoodSeg103 and implement our proposed ReLeM by incorporating three baseline methods of semantic segmentation. Below, we first elaborate the experimental settings and the results of an ablation study. Then, we show the performance gaps of the top model in the typical semantic segmentation task and our food image segmentation task. We also evaluate the model adaptability using the Asian food data splits in our FoodSeg154. Lastly, we provide some qualitative results of our best segmentation models.\n\n\nImplementation Details\n\nDataset Settings In our experiments, we use FoodSeg103 for indomain training and testing, and use the additional Asian food set for out-domain testing. We randomly divide FoodSeg103 dataset into two splits: training set and testing set, according to the 7:3 ratio. Our training set contains 4,983 images with 29,530 ingredient masks, while testing set contains 2,135 images with 12,567 ingredient masks. For ReLeM training, we use the training set of Recipe1M+ to learn the recipe representations (with test images in FoodSeg103 hidden from training). Segmenter Settings We conduct experiments based on two types of vision encoders: ResNet-50 [15] based on convolutional neural networks, and ViT-16/B [12] based on vision transformer. ResNet-50 is initialized from the pre-training model on ImageNet-1k [10], which is widely used in multiple vision tasks [4,24,37]. ViT-16/B [12] is a transformer-based model, which is initialized from the pretraining model on ImageNet-21k. ViT-16/B contains 12 transformer encoders with 12-head self-attention modules. We use the bilinear interpolation method to reinitialize the pre-trained positional embedding. In this paper, we use three types of segmentors: CC-Net [17], FPN [22] and SeTR [54]. CCNet and FPN are based on ResNet-50, while SeTR is based on ViT-16/B. Notably, SeTR extracts feature maps from 12 th transformer encoders, followed by two sets of convolution layers for prediction. Other components of the segmentors follow the default settings with random initialization. ReLeM Settings We use two types of vision encoders in ReLeM: ResNet-50 and ViT-16/B, which follow the same setting as Segmenter. In text preprocessing step, we use the skip-instruction models from the pre-trained weights in [29]. Learning Parameters of Segmenter Each image will be resized into a fixed size of 2049 \u00d7 1024 pixels with a ratio range from 0.5  to 2.0. A 768 \u00d7 768 patch is cropped from the resized images, and random horizontal flipping and color jitter are applied. We trained the models with 80k iterations based on 8 images per batch, and optimized the models by SGD solvers, with a momentum as 0.9 and weight decay as 0.0005. For CCNet and FPN, we set the initial learning rate to 1e-3, while for SeTR we set initial learning rate to 1e-3. According to the general settings [17,47], the learning rate is decayed by a power of 0.9 according to the polynomial decay schedule. For simplicity, we do not apply hard negative mining during training, and our framework is based on the widely used platform mmsegmentation [7]. All experiments were conducted on 4 Tesla-V100 GPU cards. Learning Parameters of ReLeM Each input image are resized into a size of 256 \u00d7 256 pixels and a 224 \u00d7 224 patch is cropped from the resized images as the input of the vision encoder. The model is trained for 720 epochs and each batch contains 160 images. We use Adam solver [21] to optimize the models, with a learning rate of 1e-4, Here we follow a two-stage optimization strategy. We first freeze the weights of the vision encoder and optimize the text encoder. After the text encoder converges, we start to train the vision encoder and freeze the parameters of the text encoder.\n\n\nResults and Observations\n\nThe experiment results of CCNet, FPN and SeTR on FoodSeg103 are shown in Table 3.\n\nThe Segmenters of all CCNet, FPN and SeTR achieve significant improvements when incorporating with either LSTM-based or transformer-based ReLeM (1.3%, 1.3% and 2.6% improvement). This confirms that ReLeM is effective in enhancing both convolution based and transformer based semantic segmentation models. Besides, we can see that the performance of using LSTM-based ReLeM is consistently superior than using transformer-based ReLeM across all the model configurations.  Table 3: Semantic segmentation results of our ReLeM plugged into three baseline methods (on the FoodSeg103 dataset). We implement two variants of ReLeM using LSTM and Transformer, respectively, to encode recipes.\n\n\nComparing FoodSeg103 with Cityscapes\n\nWe compare the food image segmentation task with conventional semantic segmentation to compare the degree of difficulty of the two types of segmentation tasks. We include three types of state-ofthe-art segmentation algorithms, CCNet, SeTR and FPN. They are evaluated on FoodSeg103 and Cityscapes [8] datasets. Cityscapes contains around 5,000 images captured on the streets of German cities, and 20 types of objects as segmentation targets. As we can see from Table 4, all baseline methods achieve satisfactory results on Cityscapes, but suffer significant performance drops on our FoodSeg103. This indirectly shows the greater level of difficulty in the food image segmentation problem.\n\n\nQualitative Examples\n\nIn Figure 10, we show some qualitative results of using CCNet and ReLeM-CCNet on the testing set of FoodSed103. The first two rows clearly show that ReLeM-CCNet produces more accurate and    \n\n\nCross-Domain Evaluation\n\nWe conduct an out-domain model evaluation using the Asian food data set in FoodSeg154. With the model trained on FoodSeg103, we adapt it to the subset of FoodSeg154, the Asian food data set. Specifically, the Asia food set is evenly divided into the training and testing splits. We fine-tune the trained model on the training set and then run the model on the testing data. In Table 5, we show the performances of three models trained with the following settings: 1) without ReLeM, 2) with ReLeM and 3) with ReLeM and fine-tuned on the training split of the Asian food set. For the first two settings, we only evaluate the 62 classes in Asian food set overlapped with FoodSeg103, and for the last setting, we evaluate 112 classes (all). From the results in Table 5, we observe that using ReLeM consistently outperforms baselines in both cases-with and without model fine-tuning on the training split of Asian food data.\n\n\nCONCLUSIONS\n\nWe construct a large-scale image dataset FoodSeg103 (and its extension FoodSeg154) for food image segmentation research. We use around 10k images and annotate 60k segmentation masks in total, covering highly diverse appearances among 154 ingredients. In addition, we propose a multi-modality based pre-training method ReLeM, and validate its effectiveness by incorporating three baseline semantic segmentation methods and conducting extensive experiments on the FoodSeg103, i.e., using the typical setting, as well as on the FoodSeg154, i.e., using the challenging cross-domain setting.  Table 6, and the more detailed statistic can be found in Table 9. In our experiments, we use FoodSeg103 for in-domain training and testing, and use the additional Asian set for out-domain evaluation. Structure of FoodSeg103 FoodSeg103 contains 103 ingredient categories which belong to 15 super categories. In Figure 12, we show the dataset structure of FoodSeg103, where the inner circle plots the names of super classes, and the outer circle plots the corresponding ingredient categories.\n\n\nACKNOWLEDGEMENT\n\n\nVisualization\n\nVisualization of FoodSeg103. In Figure 11, we show more visualization examples of the source image and its corresponding mask annotation in FoodSeg103.\n\n\nAnalysis on Transformer-based Models\n\nVision Transformers have been intensively studied recently, and a bunch of new algorithms have been proposed. The new proposed vision transformers have achieved significantly better performance than conventional CNN-based models in multiple vision tasks. In this section, we explore the performance of applying vision transformers into food image segmentation task. We adopt the vision transformers: ViT [12], Swin [26] and PVT [47] as segmentation encoders. We follow the default design of decoders, where FPN is used in PVT models and UperNet [51] is used in Swin models. For ViT models, we use the two default settings in SeTR: Naive and MLA, as decoders. All the models are trained with the default learning settings with 80k iterations. The results are shown in Table 8. ReLeM-variants show consistent improvement on both PVT and ViT-Naive models (0.7% and 2.6% improvement). However, in ViT-MLA model, the baseline shows better performance. In MLA decoder, feature maps from different level transformer encoders are integrated for final prediction. In ReLeM, however, only the last feature map is extracted for recipe learning. We argue ReLeM can also learn strong multi-level representation by extracting feature maps of different levels for recipe learning, and we leave it as the future work. In addition, larger backbones cannot guarantee improvement and may even hurt the performances (44.5% vs 45.1% in ViT, and 41.2% vs 41.6% in Swin). Besides, Swin achieves much better performance than ViT in other vision tasks [26], but in food image segmentation, the performance of Swin is much worse than ViT models, even with more parameters. These results show that food image segmentation task is more challenging and naively boosting the power of backbone cannot guarantee performance gain. Finally, decoders play important roles in transformer-based segmenters, but few efforts have been made to design a food-aware decoders, which is also an important research problem in the future.    All models are trained based on the default learning settings with 4 images per batch for 80k iterations. \"S\", \"B\" and \"L\" denote \"Small\", \"Base\" and \"Large\" models respectively.  \n\nFigure 1 :\n1The first row shows a source image and its segmentation masks on our FoodSeg103. The second row shows example images to reveal the difficulties of food image segmentation, e.g., the pineapples in (a) and (b) look different, while the pineapple in (a) and the potato in (c) look quite similar.\n\nFigure 2 :\n2Foodseg103 examples: source images (left) and annotations (right).\n\nFigure 3 :\n3Category statistics for our FoodSeg103 dataset in (a) and (b), and the Asian food image set (i.e., the additional set in FoodSeg154) in (c) and (d).\n\nFigure 4 :\n4Comparison of different annotation styles for masking food images: (a) source images, and (b) ingredientlevel annotation (ours), and (c) dish-level annotation\n\nFigure 5 :\n5Examples of dataset refinement. (a) sources images (b) before refinement (wrong or confusing labels exist), and (c) after refinement.\n\nFigure 7 :\n7Calculating IoU and Acc, taking the \"cake\" mask as an example. IoU = ( TP TP+FP+FN ) and Acc = ( TP TP+FN ).\n\nFigure 8 :Figure 9 :\n89Different Different types of decoder for food image segmentation\n\nFigure 10 :\n10Visualization results on FoodSeg103. ReLeM-CCNet can make more accurate predictions.\n\nFigure 11 :\n11More annotation examples of FoodSeg103. The source images are in the left hand, while the annotation masks are in the right hand.\n\nFigure 12 :\n12The dataset structure of FoodSeg103. The inner circle plots the super classes and the outer circle plots the corresponding sub-classes.\n\nTable 1 :\n1A global view of existing food image datasets. (CLS: \nno recipe and masks, Recipe: with recipe, SEG: with segmen-\ntation masks ) \n\nStatistics \nFoodSeg103 FoodSeg154 UECFood UECFoodComp. \n# Dish \n730 \n730 \n102 \n102 \n# Ingr. \n103 \n154 \n0 \n0 \n# images \n7,118 \n9,490 \n10,000 \n10,000 \n# masks \n42,097 \n59,773 \n14,011 \n16,060 \nmean image width \n771 pixels \n776 pixels \n442 pixels \n442 pixels \nmean image height \n647 pixels \n656 pixels \n349 pixels \n349 pixels \nmIoU@deeplabv3+ \n34.2 \nN.A. \n41.6 \n55.5 \n\n\n\nTable 2 :\n2Data summary and comparison with existing food image segmentation datasets.\n\nTable 4 :\n4Semantic segmentation results on Cityscape [8] and \nour FoodSeg103, showing that our FoodSeg103 is much more \nchallenging than the object image dataset for the task of se-\nmantic segmentation. \n\nMethods \nmIoU mAcc aAcc \nCCNet \n28.6 \n47.8 \n78.9 \nReLeM-CCNet \n29.2 \n47.5 \n79.3 \nCCNet-Finetune \n41.3 \n53.8 \n87.7 \nReLeM-CCNet-Finetune 47.1 \n59.5 \n85.5 \nFPN \n21.9 \n41.7 \n75.5 \nReLeM-FPN \n22.9 \n42.3 \n77.0 \nFPN-Finetune \n27.1 \n38.0 \n82.6 \nReLeM-FPN-Finetune \n30.8 \n40.7 \n78.9 \n\n\n\nTable 5 :\n5Cross-domain adaptation results. We use LSTM based ReLeM.detailed predictions than the vanilla CCNet, demonstrating the \neffectiveness of ReLeM. In the last row, we show a failure case. It is \nactually a hard example with no clear boundaries among different \ningredients. \n\n\n\n\nThis research is supported by the National Research Foundation, Singapore under its International Research Centres in Singapore Funding Initiative. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore. It is also partially supported by A*STAR under its AME YIRG Grant (Project No. A20E6c0101).[54] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. 2020. Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers. arXiv preprint arXiv:2012.15840 (2020). Image Collection. For FoodSeg103, we first shuffle all the images and randomly select 70% images (4983 images) as training set and the left 30% images as testing set. For Asian Set, we randomly sample 50% images (1186 images) for each dish class, and the left 50% are used for testing. The basic information of training and testing set is listed inAPPENDIX: MORE DETAILS OF FOODSEG103 \nAND FOODSEG154 \n7.1 Statistics \n\n\n\nTable 6 :\n6Statistic of training and testing set for FoodSeg103, Asian Set and FoodSeg154.S-classes Number S-classes \nNumber S-classes Number \nDessert \n3913 \nMeat \n4956 \nSoy \n148 \nBeverage \n844 \nCondiment \n1543 \nVegetable \n15719 \nNut \n912 \nSeafood \n920 \nFungus \n592 \nEgg \n424 \nSoup \n121 \nSalad \n23 \nFruit \n6007 \nMain \n5634 \nOthers \n341 \n\n\n\nTable 7 :\n7The ingredient number of all super classes in Food-Seg103.Encoder \nDecoder mIoU mAcc Model Size \nPVT-S \nFPN \n31.3 \n43.0 \n202M \nReLeM-PVT-S \nFPN \n32.0 \n44.1 \n202M \nViT-16/B \nNaive \n41.3 \n52.7 \n723M \nReLeM-ViT-16/B \nNaive \n43.9 \n57.0 \n723M \nViT-16/B \nMLA \n45.1 \n57.4 \n711M \nReLeM-ViT-16/B \nMLA \n43.3 \n55.9 \n711M \nViT-16/L \nMLA \n44.5 \n56.6 \n2.4G \nSwin-S \nUper \n41.6 \n53.6 \n931M \nSwin-B \nUper \n41.2 \n53.9 \n1.4G \n\n\n\nTable 8 :\n8Semantic segmentation results of different vision transformers.\nFoodSeg103Asian Set FoodSeg154Class Id Class NameTrain Test Total  Train Test  Total  1  candy  58  43  101  0  0  101  2  egg tart  8  6  14  0  0  14  3  french fries  190  87  277  95  83  455  4  chocolate  158  59  217  0  0  217  5  biscuit  393  122  515  4  1  520  6popcorn 37110  0  748  11  wine  117  50  167  15  19  201  12  milkshake  107  32  139  0  0  139  13  coffee  136  62  198  8  12  218  14  juice  157  64  221  71  72  364  15  milk  48  36  84  5  4  93  16  tea  29  6  35  15  6  56  17  almond  268  74  342  0  0  342  18  red beans  46  27  73  0  0  73  19  cashew  44  43  87  0  0  87  20 dried cranberries  79  55  134  0  0  134  21  soy  41  18  59  0  0  59  22  walnut  100  81  181  0  0  181  23  peanut  16  20  36  93  95  224  24  egg  321  103  424  162  161  747  25  apple  195  80  275  29  49  353  26  date  14  3  17  51  43  111  27  apricot  39  18  57  0  0  57  28  avocado  104  35  139  90  0  327  51  lamb  85  34  119  0  0  119  52  sauce  1124 419 1543  19  15  1577  53  crab  19  11  30  38  37  105  54  fish  348  138  486  103  126  715  55  shellfish  77  27  104  37  40  181  56  shrimp  211  89  300  51  54  405  57  soup  92  29  121  0  0  121  58  bread  1698 738 2436  49  40  2525  59  corn  411  170  581  29  35  645  60  hamburg  7  1  8  0  0  8  61  pizza  83  22  105  0  0  105  62  hanamaki baozi  22  14  36  0  0  36  63 wonton dumplings  10  10  20  165  149  334  64  pasta  171  59  230  18  3  251  65  noodles  337  140  477  811  836  2124  66  rice  655  277  932  294  306  1532  67  pie  563  246  809  20  17  846  68  tofu  111  37  148  73  57  278  69  eggplant  34  9  43  38  12  93  70  potato  1041 400 1441  110  111  1662  71  garlic  143  29  172  40  36  248  72  cauliflower  237  100  337  43  32  412  73  tomato  1404 687 2091  124  100  2315  74  kelp  4  5  9  0  0  9A Large-Scale Benchmark for Food Image SegmentationFoodSeg103Asian Set FoodSeg154Class Id Class NameTrain Test Total  Train Test  Total  75  seaweed  16  10  26  29  29  84  76  spring onion  285  113  398  556  561  1515  77  rape  59  23  82  360  429  871  78  ginger  25  12  37  24  34  95  79  okra  35  9103  49  152  6  16  174  91  cabbage  139  39  178  25  13  216  92  bean sprouts  35  20  55  34  34  123  93  onion  732  304 1036  85  103  1224  94  pepper  552  242  794  189  191  1174  95  green beans  237  125  362  40  37  439  96  French beans  360  168  528  39  34  601  97 king oyster mushroom  12  3  15  0  0  15  98  shiitake  185  106  291  167  205  663  99  enoki mushroom  9  5  14  25  31  70  100  oyster mushroom  11  4  15  0  0  15  101 white button mushroom  195  62  257  35  26  318  102  salad  12  11  23  0  0  23  103  other ingredients  230  111  341  667  738  1746  104  water  0  0  0  2  4  6  105  goji berry  0  0  0  33  50  83  106  ribs  0  0  0  148  135  283  107  tripe  0  0  0  31  36  67  108  meat slices  0  0  0  135  170  305  109  minced meat  0  0  0  95  69  164  110  pork belly  0  0  0  87  76  163  111  pork intestine  0  0  0  16  16  32  112  pork skin  0  0  0  33  15  48  113  blood  0  0  0  4  4  8  114  pork liver  0  0  0  26  16  42  115shredded pork 0 0 0 25 34 590  0  0  30  25  55  130  garlic sauce  0  0  0  8  8  16  131  cuttlefish  0  0  0  4  3  7  132  squid  0  0  0  32  31  63  133  fish cakes  0  0  0  78  100  178  134  fish Ball  0  0  0  220  205  425  135  fish tofu  0  0  0  27  26  53  136  fried fish  0  0  0  76  66  142  137  small dried fish  0  0  0  73  71  144  138  yut yiao  0  0  0  46  56  102  139  porridge  0  0  0  36  55  91  140  fried banana leaves  0  0  0  23  32  55  141  rice cake  0  0  0  16  14  30  142  yuba  0  0  0  27  29  56  143  fried tofu  0  0  0  11  24  35  144  beancurd puff  0  0  0  26  33  59  145  preserved vegetable  0  0  0  7  17  24  146  salted vegetables  0  0  0  32  25  57  147  pea seedlings  0  0  0  13  15  28  148  kai lan  0  0  0  6  11  17  149  lotus root  0  0  0  26  26  52  150  amaranth  0  0  0  23  16  39  151  millet spicy  0  0  0  64  65  129  152  bitter gourd  0  0  0  16  17  33  153  daylily  0  0  0  1  5  6  154  agaric  0  0  0  33  42  75  -Summary  29530 12567 42097  8795 8881  59773   Table 9: Statistic of ingredients per class for FoodSeg103, Asian set and FoodSeg154.\nFood-101-mining discriminative components with random forests. Lukas Bossard, Matthieu Guillaumin, Luc Van Gool, ECCV. Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. 2014. Food-101-mining discriminative components with random forests. In ECCV. 446-461.\n\nTraining in cognitive strategies reduces eating and improves food choice. G Rebecca, Wendy Boswell, Shosuke Sun, Hedy Suzuki, Kober, PNASRebecca G Boswell, Wendy Sun, Shosuke Suzuki, and Hedy Kober. 2018. Training in cognitive strategies reduces eating and improves food choice. PNAS (2018), E11238-E11247.\n\nSemantic image segmentation with deep convolutional nets and fully connected crfs. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L Yuille, ICLR. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. 2015. Semantic image segmentation with deep convolutional nets and fully connected crfs. In ICLR.\n\nDeeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, Alan L Yuille, TPAMI. Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. 2017. Deeplab: Semantic image segmentation with deep con- volutional nets, atrous convolution, and fully connected crfs. TPAMI (2017), 834-848.\n\nPFID: Pittsburgh fast-food image dataset. Mei Chen, Kapil Dhingra, Wen Wu, Lei Yang, Rahul Sukthankar, Jie Yang, Mei Chen, Kapil Dhingra, Wen Wu, Lei Yang, Rahul Sukthankar, and Jie Yang. 2009. PFID: Pittsburgh fast-food image dataset. In ICIP. 289-292.\n\nLearning CNN-based features for retrieval of food images. Gianluigi Ciocca, Paolo Napoletano, Raimondo Schettini, ICIAP. Gianluigi Ciocca, Paolo Napoletano, and Raimondo Schettini. 2017. Learning CNN-based features for retrieval of food images. In ICIAP. 426-434.\n\nMMSegmentation Contributors. 2020. MMSegmentation: OpenMMLab Semantic Segmentation Toolbox and Benchmark. MMSegmentation Contributors. 2020. MMSegmentation: OpenMMLab Seman- tic Segmentation Toolbox and Benchmark. https://github.com/open-mmlab/ mmsegmentation.\n\nUwe Franke, Stefan Roth, and Bernt Schiele. 2016. The Cityscapes Dataset for Semantic Urban Scene Understanding. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, CVPR. Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus En- zweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. 2016. The Cityscapes Dataset for Semantic Urban Scene Understanding. In CVPR.\n\nGlobal diets link environmental sustainability and human health. Tilman David, Clark Michael, Nature. Tilman David and Clark Michael. 2014. Global diets link environmental sustain- ability and human health. Nature (2014), 518-22.\n\nImagenet: A large-scale hierarchical image database. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, Li Fei-Fei, CVPR. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In CVPR.\n\nMixed-dish recognition with contextual relation networks. Lixi Deng, Jingjing Chen, Qianru Sun, Xiangnan He, Sheng Tang, Zhaoyan Ming, Yongdong Zhang, Tat Seng Chua, Proceedings of ACM international conference on Multimedia. ACM international conference on MultimediaLixi Deng, Jingjing Chen, Qianru Sun, Xiangnan He, Sheng Tang, Zhaoyan Ming, Yongdong Zhang, and Tat Seng Chua. 2019. Mixed-dish recognition with contextual relation networks. In Proceedings of ACM international conference on Multimedia. 112-120.\n\nJakob Uszkoreit, and Neil Houlsby. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, ICLR. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xi- aohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In ICLR.\n\nA New Large-scale Food Image Segmentation Dataset and Its Application to Food Calorie Estimation Based on Grains of Rice. Takumi Ege, Keiji Yanai, Takumi Ege and Keiji Yanai. 2019. A New Large-scale Food Image Segmentation Dataset and Its Application to Food Calorie Estimation Based on Grains of Rice. In MADiMa. 82-87.\n\nRecipeGPT: Generative pre-training based cooking recipe generation and evaluation system. Helena H Lee, Ke Shu, Palakorn Achananuparp, Yue Philips Kokoh Prasetyo, Ee-Peng Liu, Lav R Lim, Varshney, WWW. Helena H. Lee, Ke Shu, Palakorn Achananuparp, Philips Kokoh Prasetyo, Yue Liu, Ee-Peng Lim, and Lav R Varshney. 2020. RecipeGPT: Generative pre-training based cooking recipe generation and evaluation system. In WWW. 181-184.\n\nDeep Residual Learning for Image Recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual Learning for Image Recognition. In CVPR. 770-778.\n\nImage recognition of 85 food categories by feature fusion. Hajime Hoashi, Taichi Joutou, Keiji Yanai, ISM. Hajime Hoashi, Taichi Joutou, and Keiji Yanai. 2010. Image recognition of 85 food categories by feature fusion. In ISM. 296-301.\n\nCCNet: Criss-Cross Attention for Semantic Segmentation. Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, Wenyu Liu, Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. 2019. CCNet: Criss-Cross Attention for Semantic Segmentation. In ICCV. 603-612.\n\nA food image recognition system with multiple kernel learning. Taichi Joutou, Keiji Yanai, ICIP. Taichi Joutou and Keiji Yanai. 2009. A food image recognition system with multiple kernel learning. In ICIP. 285-288.\n\nFoodX-251: A Dataset for Fine-grained Food Classification. Parneet Kaur, Karan Sikka, Weijun Wang, Serge J Belongie, Ajay Divakaran, CVPRW. Parneet Kaur, Karan Sikka, Weijun Wang, Serge J. Belongie, and Ajay Divakaran. 2019. FoodX-251: A Dataset for Fine-grained Food Classification. In CVPRW.\n\nAutomatic expansion of a food image dataset leveraging existing categories with domain adaptation. Yoshiyuki Kawano, Keiji Yanai, ECCV. Yoshiyuki Kawano and Keiji Yanai. 2014. Automatic expansion of a food image dataset leveraging existing categories with domain adaptation. In ECCV. 3-17.\n\nAdam: A Method for Stochastic Optimization. P Diederik, Jimmy Kingma, Ba, ICLR. Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Opti- mization. In ICLR.\n\nPanoptic feature pyramid networks. Alexander Kirillov, Ross Girshick, CVPR. Kaiming He, and Piotr Doll\u00e1rAlexander Kirillov, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. 2019. Panoptic feature pyramid networks. In CVPR. 6399-6408.\n\nRyan Kiros, Yukun Zhu, Ruslan Salakhutdinov, S Richard, Antonio Zemel, Torralba, arXiv:1506.06726Raquel Urtasun, and Sanja Fidler. 2015. Skip-thought vectors. arXiv preprintRyan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard S Zemel, Antonio Torralba, Raquel Urtasun, and Sanja Fidler. 2015. Skip-thought vectors. arXiv preprint arXiv:1506.06726 (2015).\n\nImageNet Classification with Deep Convolutional Neural Networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, In NeurIPSAlex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. ImageNet Classifi- cation with Deep Convolutional Neural Networks. In NeurIPS.\n\nKaiming He, Bharath Hariharan, and Serge Belongie. 2017. Feature Pyramid Networks for Object Detection. Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, CVPR. Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. 2017. Feature Pyramid Networks for Object Detection. In CVPR.\n\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo, arXiv:2103.14030Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. arXiv preprintZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. 2021. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. arXiv preprint arXiv:2103.14030 (2021).\n\nFully convolutional networks for semantic segmentation. Jonathan Long, Evan Shelhamer, Trevor Darrell, CVPR. Jonathan Long, Evan Shelhamer, and Trevor Darrell. 2015. Fully convolutional networks for semantic segmentation. In CVPR. 3431-3440.\n\nRecipe1M+: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images. Javier Marin, Aritro Biswas, Ferda Ofli, Nicholas Hynes, Amaia Salvador, Yusuf Aytar, Ingmar Weber, Antonio Torralba, TPAMI. Javier Marin, Aritro Biswas, Ferda Ofli, Nicholas Hynes, Amaia Salvador, Yusuf Aytar, Ingmar Weber, and Antonio Torralba. 2019. Recipe1M+: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images. TPAMI (2019), 187-203.\n\nJavier Mar\u00edn, Aritro Biswas, Ferda Ofli, Nicholas Hynes, Amaia Salvador, Yusuf Aytar, Ingmar Weber, Antonio Torralba, 2021. Recipe1M+: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images. Javier Mar\u00edn, Aritro Biswas, Ferda Ofli, Nicholas Hynes, Amaia Salvador, Yusuf Aytar, Ingmar Weber, and Antonio Torralba. 2021. Recipe1M+: A Dataset for Learning Cross-Modal Embeddings for Cooking Recipes and Food Images. TPAMI (2021), 187-203.\n\nMultiple-food recognition considering cooccurrence employing manifold ranking. Yuji Matsuda, Keiji Yanai, Yuji Matsuda and Keiji Yanai. 2012. Multiple-food recognition considering co- occurrence employing manifold ranking. In ICPR. 2017-2020.\n\nIm2Calories: towards an automated mobile vision food diary. Austin Meyers, Nick Johnston, Vivek Rathod, Anoop Korattikara, Alex Gorban, Nathan Silberman, Sergio Guadarrama, George Papandreou, Jonathan Huang, Kevin P Murphy, Austin Meyers, Nick Johnston, Vivek Rathod, Anoop Korattikara, Alex Gorban, Nathan Silberman, Sergio Guadarrama, George Papandreou, Jonathan Huang, and Kevin P Murphy. 2015. Im2Calories: towards an automated mobile vision food diary. In ICCV. 1233-1241.\n\nEfficient estimation of word representations in vector space. Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean, arXiv:1301.3781arXiv preprintTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781 (2013).\n\nIngredient-Guided Cascaded Multi-Attention Network for Food Recognition. Weiqing Min, Linhu Liu, Zhengdong Luo, Shuqiang Jiang, Proceedings of ACM international conference on Multimedia. ACM international conference on MultimediaWeiqing Min, Linhu Liu, Zhengdong Luo, and Shuqiang Jiang. 2019. Ingredient- Guided Cascaded Multi-Attention Network for Food Recognition. In Proceedings of ACM international conference on Multimedia. 1331-1339.\n\nISIA Food-500: A Dataset for Large-Scale Food Recognition via Stacked Global-Local Attention Network. Weiqing Min, Linhu Liu, Zhiling Wang, Zhengdong Luo, Xiaoming Wei, Xiaolin Wei, Proceedings of ACM international conference on Multimedia. ACM international conference on MultimediaWeiqing Min, Linhu Liu, Zhiling Wang, Zhengdong Luo, Xiaoming Wei, and Xiaolin Wei. 2020. ISIA Food-500: A Dataset for Large-Scale Food Recognition via Stacked Global-Local Attention Network. In Proceedings of ACM international conference on Multimedia. 393-401.\n\nUEC-FoodPIX Complete: A Large-scale Food Image Segmentation Dataset. Kaimu Okamoto, Keiji Yanai, MADiMaKaimu Okamoto and Keiji Yanai. 2021. UEC-FoodPIX Complete: A Large-scale Food Image Segmentation Dataset. In MADiMa.\n\nMining Discriminative Food Regions for Accurate Food Recognition. Jianing Qiu, P.-W Frank, Yingnan Lo, Siyao Sun, Benny Wang, Lo, BMVC. Jianing Qiu, Frank P.-W. Lo, Yingnan Sun, Siyao Wang, and Benny Lo. 2019. Mining Discriminative Food Regions for Accurate Food Recognition. In BMVC.\n\nFaster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. Kaiming Shaoqing Ren, Ross He, Jian Girshick, Sun, NeurIPS. Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. In NeurIPS.\n\nFoodAI: Food Image Recognition via Deep Learning for Smart Food Logging. Doyen Sahoo, Wang Hao, Shu Ke, Xiongwei Wu, Hung Le, Palakorn Achananuparp, Ee-Peng Lim, Steven C H Hoi, Doyen Sahoo, Wang Hao, Shu Ke, Xiongwei Wu, Hung Le, Palakorn Achananu- parp, Ee-Peng Lim, and Steven C. H. Hoi. 2019. FoodAI: Food Image Recognition via Deep Learning for Smart Food Logging. In KDD. 2260-2268.\n\nInverse cooking: Recipe generation from food images. Amaia Salvador, Michal Drozdzal, Xavier Giro-I Nieto, Adriana Romero, CVPR. Amaia Salvador, Michal Drozdzal, Xavier Giro-i Nieto, and Adriana Romero. 2019. Inverse cooking: Recipe generation from food images. In CVPR. 10453-10462.\n\nInverse Cooking: Recipe Generation From Food Images. Amaia Salvador, Michal Drozdzal, Xavier Giro-I Nieto, Adriana Romero, CVPR. Amaia Salvador, Michal Drozdzal, Xavier Giro-i Nieto, and Adriana Romero. 2019. Inverse Cooking: Recipe Generation From Food Images. In CVPR. 10453-10462.\n\nLearning cross-modal embeddings for cooking recipes and food images. Amaia Salvador, Nicholas Hynes, Yusuf Aytar, Javier Marin, Ferda Ofli, Ingmar Weber, Antonio Torralba, Amaia Salvador, Nicholas Hynes, Yusuf Aytar, Javier Marin, Ferda Ofli, Ingmar Weber, and Antonio Torralba. 2017. Learning cross-modal embeddings for cook- ing recipes and food images. In CVPR. 3020-3028.\n\nLearning food image similarity for food image retrieval. Wataru Shimoda, Keiji Yanai, BigMM. Wataru Shimoda and Keiji Yanai. 2017. Learning food image similarity for food image retrieval. In BigMM. 165-168.\n\nNutrition5k: Towards Automatic Nutritional Understanding of Generic Food. Quin Thames, Arjun Karpur, Wade Norris, Fangting Xia, Liviu Panait, Tobias Weyand, Jack Sim, CVPR. Quin Thames, Arjun Karpur, Wade Norris, Fangting Xia, Liviu Panait, Tobias Weyand, and Jack Sim. 2021. Nutrition5k: Towards Automatic Nutritional Under- standing of Generic Food. In CVPR.\n\nUnbiased look at dataset bias. Antonio Torralba, Alexei A Efros, CVPR. Antonio Torralba and Alexei A Efros. 2011. Unbiased look at dataset bias. In CVPR. 1521-1528.\n\nAttention is All you Need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Illia Kaiser, Polosukhin, NeurIPS, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. GarnettAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141 ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you Need. In NeurIPS, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (Eds.).\n\nStructure-Aware Generation Network for Recipe Generation from Images. Hao Wang, Guosheng Lin, C H Steven, Chunyan Hoi, Miao, ECCV. Hao Wang, Guosheng Lin, Steven CH Hoi, and Chunyan Miao. 2020. Structure- Aware Generation Network for Recipe Generation from Images. In ECCV. 359- 374.\n\nPyramid vision transformer: A versatile backbone for dense prediction without convolutions. Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, Ling Shao, arXiv:2102.12122arXiv preprintWenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. 2021. Pyramid vision transformer: A versatile back- bone for dense prediction without convolutions. arXiv preprint arXiv:2102.12122 (2021).\n\nAbhinav Gupta, and Kaiming He. Xiaolong Wang, Ross Girshick, CVPR. Non-local neural networksXiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. 2018. Non-local neural networks. In CVPR. 7794-7803.\n\nRecipe recognition with large multimodal food dataset. Xin Wang, Devinder Kumar, Nicolas Thome, Matthieu Cord, Frederic Precioso, ICME. Xin Wang, Devinder Kumar, Nicolas Thome, Matthieu Cord, and Frederic Pre- cioso. 2015. Recipe recognition with large multimodal food dataset. In ICME. 1-6.\n\nMixed dish recognition through multi-label learning. Yunan Wang, Jing-Jing Chen, Chong-Wah Ngo, Tat-Seng Chua, Wanli Zuo, Zhaoyan Ming, Proceedings of the 11th Workshop on Multimedia for Cooking and Eating Activities. the 11th Workshop on Multimedia for Cooking and Eating ActivitiesYunan Wang, Jing-jing Chen, Chong-Wah Ngo, Tat-Seng Chua, Wanli Zuo, and Zhaoyan Ming. 2019. Mixed dish recognition through multi-label learning. In Proceedings of the 11th Workshop on Multimedia for Cooking and Eating Activities. 1-8.\n\nUnified perceptual parsing for scene understanding. Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, Jian Sun, ECCV. Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. 2018. Unified perceptual parsing for scene understanding. In ECCV. 418-434.\n\nGeolocalized modeling for dish recognition. Ruihan Xu, Luis Herranz, Shuqiang Jiang, Shuang Wang, Xinhang Song, Ramesh Jain, IEEE Transactions on Multimedia. Ruihan Xu, Luis Herranz, Shuqiang Jiang, Shuang Wang, Xinhang Song, and Ramesh Jain. 2015. Geolocalized modeling for dish recognition. IEEE Transactions on Multimedia (2015), 1187-1199.\n\nPyramid scene parsing network. Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, Jiaya Jia, CVPR. Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. 2017. Pyramid scene parsing network. In CVPR. 2881-2890.\n", "annotations": {"author": "[{\"end\":298,\"start\":55},{\"end\":539,\"start\":299},{\"end\":764,\"start\":540},{\"end\":1009,\"start\":765},{\"end\":1240,\"start\":1010},{\"end\":1488,\"start\":1241}]", "publisher": null, "author_last_name": "[{\"end\":66,\"start\":64},{\"end\":305,\"start\":303},{\"end\":548,\"start\":545},{\"end\":776,\"start\":773},{\"end\":1024,\"start\":1021},{\"end\":1251,\"start\":1248}]", "author_first_name": "[{\"end\":63,\"start\":55},{\"end\":302,\"start\":299},{\"end\":544,\"start\":540},{\"end\":772,\"start\":765},{\"end\":1016,\"start\":1010},{\"end\":1020,\"start\":1017},{\"end\":1247,\"start\":1241}]", "author_affiliation": "[{\"end\":297,\"start\":84},{\"end\":538,\"start\":325},{\"end\":763,\"start\":550},{\"end\":1008,\"start\":795},{\"end\":1239,\"start\":1026},{\"end\":1487,\"start\":1274}]", "title": "[{\"end\":52,\"start\":1},{\"end\":1540,\"start\":1489}]", "venue": null, "abstract": "[{\"end\":3114,\"start\":1542}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3298,\"start\":3295},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3300,\"start\":3298},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3303,\"start\":3300},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":3306,\"start\":3303},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3525,\"start\":3521},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":3528,\"start\":3525},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":3531,\"start\":3528},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3555,\"start\":3551},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":3558,\"start\":3555},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":3561,\"start\":3558},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3591,\"start\":3588},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3594,\"start\":3591},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4277,\"start\":4274},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4280,\"start\":4277},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4283,\"start\":4280},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":5213,\"start\":5209},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":5336,\"start\":5333},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5351,\"start\":5347},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":5370,\"start\":5366},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5521,\"start\":5517},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5549,\"start\":5545},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7051,\"start\":7047},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7944,\"start\":7940},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":7958,\"start\":7954},{\"end\":7972,\"start\":7968},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":9260,\"start\":9257},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9384,\"start\":9380},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9505,\"start\":9501},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9676,\"start\":9672},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9873,\"start\":9869},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10167,\"start\":10163},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":10195,\"start\":10191},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11221,\"start\":11217},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11489,\"start\":11486},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11725,\"start\":11722},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":11875,\"start\":11871},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":11994,\"start\":11990},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12145,\"start\":12141},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":12315,\"start\":12311},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":12318,\"start\":12315},{\"end\":12379,\"start\":12375},{\"end\":12432,\"start\":12428},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12566,\"start\":12562},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12576,\"start\":12572},{\"end\":12590,\"start\":12586},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":13110,\"start\":13106},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13739,\"start\":13735},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":13742,\"start\":13739},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":14058,\"start\":14054},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":16929,\"start\":16925},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":17207,\"start\":17203},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":17235,\"start\":17231},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":18977,\"start\":18974},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":19519,\"start\":19515},{\"end\":19522,\"start\":19519},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":20526,\"start\":20522},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22573,\"start\":22569},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":22771,\"start\":22767},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":22829,\"start\":22825},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":23594,\"start\":23590},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":23650,\"start\":23646},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24336,\"start\":24333},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":24339,\"start\":24336},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":24946,\"start\":24942},{\"end\":24949,\"start\":24946},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":25345,\"start\":25341},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":25355,\"start\":25351},{\"end\":25383,\"start\":25379},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":26912,\"start\":26908},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":26970,\"start\":26966},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":27072,\"start\":27068},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":27123,\"start\":27120},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":27126,\"start\":27123},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":27129,\"start\":27126},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":27144,\"start\":27140},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":27474,\"start\":27470},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":27484,\"start\":27480},{\"end\":27498,\"start\":27494},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":28018,\"start\":28014},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":28587,\"start\":28583},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":28590,\"start\":28587},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":28826,\"start\":28823},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":29164,\"start\":29160},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":30601,\"start\":30598},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":33882,\"start\":33878},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":33893,\"start\":33889},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":33906,\"start\":33902},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":34023,\"start\":34019},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":35005,\"start\":35001}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":35956,\"start\":35651},{\"attributes\":{\"id\":\"fig_3\"},\"end\":36036,\"start\":35957},{\"attributes\":{\"id\":\"fig_5\"},\"end\":36198,\"start\":36037},{\"attributes\":{\"id\":\"fig_6\"},\"end\":36370,\"start\":36199},{\"attributes\":{\"id\":\"fig_7\"},\"end\":36517,\"start\":36371},{\"attributes\":{\"id\":\"fig_8\"},\"end\":36639,\"start\":36518},{\"attributes\":{\"id\":\"fig_10\"},\"end\":36728,\"start\":36640},{\"attributes\":{\"id\":\"fig_11\"},\"end\":36828,\"start\":36729},{\"attributes\":{\"id\":\"fig_12\"},\"end\":36973,\"start\":36829},{\"attributes\":{\"id\":\"fig_13\"},\"end\":37124,\"start\":36974},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":37633,\"start\":37125},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":37721,\"start\":37634},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":38206,\"start\":37722},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":38493,\"start\":38207},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":39622,\"start\":38494},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":39962,\"start\":39623},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":40384,\"start\":39963},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":40460,\"start\":40385}]", "paragraph": "[{\"end\":5281,\"start\":3130},{\"end\":5824,\"start\":5283},{\"end\":6945,\"start\":5826},{\"end\":7792,\"start\":6947},{\"end\":8458,\"start\":7794},{\"end\":9120,\"start\":8460},{\"end\":10424,\"start\":9138},{\"end\":12674,\"start\":10426},{\"end\":12898,\"start\":12710},{\"end\":13520,\"start\":12900},{\"end\":14526,\"start\":13547},{\"end\":14890,\"start\":14528},{\"end\":16683,\"start\":14933},{\"end\":16981,\"start\":16722},{\"end\":18439,\"start\":16983},{\"end\":18927,\"start\":18477},{\"end\":20713,\"start\":18939},{\"end\":21405,\"start\":20748},{\"end\":21725,\"start\":21453},{\"end\":23680,\"start\":21826},{\"end\":25204,\"start\":23722},{\"end\":25674,\"start\":25206},{\"end\":26238,\"start\":25690},{\"end\":29467,\"start\":26265},{\"end\":29577,\"start\":29496},{\"end\":30261,\"start\":29579},{\"end\":30989,\"start\":30302},{\"end\":31205,\"start\":31014},{\"end\":32152,\"start\":31233},{\"end\":33246,\"start\":32168},{\"end\":33433,\"start\":33282},{\"end\":35650,\"start\":33474}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":21452,\"start\":21406},{\"attributes\":{\"id\":\"formula_1\"},\"end\":21825,\"start\":21726}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":17549,\"start\":17542},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":18017,\"start\":18010},{\"end\":29576,\"start\":29569},{\"end\":30056,\"start\":30049},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":30769,\"start\":30762},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":31617,\"start\":31610},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":31997,\"start\":31990},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":32763,\"start\":32756},{\"end\":32820,\"start\":32813},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":34248,\"start\":34241}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":3128,\"start\":3116},{\"attributes\":{\"n\":\"2\"},\"end\":9136,\"start\":9123},{\"attributes\":{\"n\":\"3\"},\"end\":12708,\"start\":12677},{\"attributes\":{\"n\":\"3.1\"},\"end\":13545,\"start\":13523},{\"attributes\":{\"n\":\"3.2\"},\"end\":14931,\"start\":14893},{\"attributes\":{\"n\":\"3.3\"},\"end\":16720,\"start\":16686},{\"attributes\":{\"n\":\"4\"},\"end\":18475,\"start\":18442},{\"end\":18937,\"start\":18930},{\"attributes\":{\"n\":\"4.1\"},\"end\":20746,\"start\":20716},{\"attributes\":{\"n\":\"4.2\"},\"end\":23720,\"start\":23683},{\"attributes\":{\"n\":\"5\"},\"end\":25688,\"start\":25677},{\"attributes\":{\"n\":\"5.1\"},\"end\":26263,\"start\":26241},{\"attributes\":{\"n\":\"5.2\"},\"end\":29494,\"start\":29470},{\"attributes\":{\"n\":\"5.3\"},\"end\":30300,\"start\":30264},{\"attributes\":{\"n\":\"5.4\"},\"end\":31012,\"start\":30992},{\"attributes\":{\"n\":\"5.5\"},\"end\":31231,\"start\":31208},{\"attributes\":{\"n\":\"6\"},\"end\":32166,\"start\":32155},{\"attributes\":{\"n\":\"7\"},\"end\":33264,\"start\":33249},{\"attributes\":{\"n\":\"7.2\"},\"end\":33280,\"start\":33267},{\"attributes\":{\"n\":\"7.3\"},\"end\":33472,\"start\":33436},{\"end\":35662,\"start\":35652},{\"end\":35968,\"start\":35958},{\"end\":36048,\"start\":36038},{\"end\":36210,\"start\":36200},{\"end\":36382,\"start\":36372},{\"end\":36529,\"start\":36519},{\"end\":36661,\"start\":36641},{\"end\":36741,\"start\":36730},{\"end\":36841,\"start\":36830},{\"end\":36986,\"start\":36975},{\"end\":37135,\"start\":37126},{\"end\":37644,\"start\":37635},{\"end\":37732,\"start\":37723},{\"end\":38217,\"start\":38208},{\"end\":39633,\"start\":39624},{\"end\":39973,\"start\":39964},{\"end\":40395,\"start\":40386}]", "table": "[{\"end\":37633,\"start\":37137},{\"end\":38206,\"start\":37734},{\"end\":38493,\"start\":38276},{\"end\":39622,\"start\":39551},{\"end\":39962,\"start\":39714},{\"end\":40384,\"start\":40033}]", "figure_caption": "[{\"end\":35956,\"start\":35664},{\"end\":36036,\"start\":35970},{\"end\":36198,\"start\":36050},{\"end\":36370,\"start\":36212},{\"end\":36517,\"start\":36384},{\"end\":36639,\"start\":36531},{\"end\":36728,\"start\":36664},{\"end\":36828,\"start\":36744},{\"end\":36973,\"start\":36844},{\"end\":37124,\"start\":36989},{\"end\":37721,\"start\":37646},{\"end\":38276,\"start\":38219},{\"end\":39551,\"start\":38496},{\"end\":39714,\"start\":39635},{\"end\":40033,\"start\":39975},{\"end\":40460,\"start\":40397}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":4051,\"start\":4043},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":4556,\"start\":4548},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":4615,\"start\":4607},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":4792,\"start\":4779},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":12897,\"start\":12889},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":16025,\"start\":16017},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16157,\"start\":16149},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16170,\"start\":16162},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":16301,\"start\":16293},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":16427,\"start\":16419},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":16529,\"start\":16517},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":18005,\"start\":17997},{\"end\":18497,\"start\":18489},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":20099,\"start\":20091},{\"end\":20165,\"start\":20157},{\"end\":24254,\"start\":24246},{\"end\":24361,\"start\":24353},{\"end\":24586,\"start\":24574},{\"end\":24702,\"start\":24694},{\"end\":25102,\"start\":25094},{\"end\":25200,\"start\":25192},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":31026,\"start\":31017},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":33075,\"start\":33066},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":33323,\"start\":33314}]", "bib_author_first_name": "[{\"end\":44878,\"start\":44873},{\"end\":44896,\"start\":44888},{\"end\":44912,\"start\":44909},{\"end\":45146,\"start\":45145},{\"end\":45161,\"start\":45156},{\"end\":45178,\"start\":45171},{\"end\":45188,\"start\":45184},{\"end\":45473,\"start\":45462},{\"end\":45486,\"start\":45480},{\"end\":45506,\"start\":45499},{\"end\":45522,\"start\":45517},{\"end\":45535,\"start\":45531},{\"end\":45537,\"start\":45536},{\"end\":45863,\"start\":45852},{\"end\":45876,\"start\":45870},{\"end\":45896,\"start\":45889},{\"end\":45912,\"start\":45907},{\"end\":45925,\"start\":45921},{\"end\":45927,\"start\":45926},{\"end\":46221,\"start\":46218},{\"end\":46233,\"start\":46228},{\"end\":46246,\"start\":46243},{\"end\":46254,\"start\":46251},{\"end\":46266,\"start\":46261},{\"end\":46282,\"start\":46279},{\"end\":46498,\"start\":46489},{\"end\":46512,\"start\":46507},{\"end\":46533,\"start\":46525},{\"end\":47077,\"start\":47071},{\"end\":47093,\"start\":47086},{\"end\":47110,\"start\":47101},{\"end\":47122,\"start\":47118},{\"end\":47138,\"start\":47132},{\"end\":47157,\"start\":47150},{\"end\":47467,\"start\":47461},{\"end\":47480,\"start\":47475},{\"end\":47683,\"start\":47680},{\"end\":47693,\"start\":47690},{\"end\":47707,\"start\":47700},{\"end\":47722,\"start\":47716},{\"end\":47730,\"start\":47727},{\"end\":47737,\"start\":47735},{\"end\":47955,\"start\":47951},{\"end\":47970,\"start\":47962},{\"end\":47983,\"start\":47977},{\"end\":47997,\"start\":47989},{\"end\":48007,\"start\":48002},{\"end\":48021,\"start\":48014},{\"end\":48036,\"start\":48028},{\"end\":48052,\"start\":48044},{\"end\":48531,\"start\":48525},{\"end\":48550,\"start\":48545},{\"end\":48567,\"start\":48558},{\"end\":48584,\"start\":48580},{\"end\":48605,\"start\":48598},{\"end\":48618,\"start\":48612},{\"end\":48639,\"start\":48632},{\"end\":48658,\"start\":48650},{\"end\":48674,\"start\":48669},{\"end\":48691,\"start\":48684},{\"end\":49136,\"start\":49130},{\"end\":49147,\"start\":49142},{\"end\":49426,\"start\":49420},{\"end\":49428,\"start\":49427},{\"end\":49436,\"start\":49434},{\"end\":49450,\"start\":49442},{\"end\":49468,\"start\":49465},{\"end\":49500,\"start\":49493},{\"end\":49511,\"start\":49506},{\"end\":49811,\"start\":49804},{\"end\":49823,\"start\":49816},{\"end\":49839,\"start\":49831},{\"end\":49849,\"start\":49845},{\"end\":50052,\"start\":50046},{\"end\":50067,\"start\":50061},{\"end\":50081,\"start\":50076},{\"end\":50286,\"start\":50280},{\"end\":50302,\"start\":50294},{\"end\":50315,\"start\":50309},{\"end\":50328,\"start\":50323},{\"end\":50343,\"start\":50336},{\"end\":50354,\"start\":50349},{\"end\":50594,\"start\":50588},{\"end\":50608,\"start\":50603},{\"end\":50807,\"start\":50800},{\"end\":50819,\"start\":50814},{\"end\":50833,\"start\":50827},{\"end\":50845,\"start\":50840},{\"end\":50847,\"start\":50846},{\"end\":50862,\"start\":50858},{\"end\":51144,\"start\":51135},{\"end\":51158,\"start\":51153},{\"end\":51372,\"start\":51371},{\"end\":51388,\"start\":51383},{\"end\":51546,\"start\":51537},{\"end\":51561,\"start\":51557},{\"end\":51737,\"start\":51733},{\"end\":51750,\"start\":51745},{\"end\":51762,\"start\":51756},{\"end\":51779,\"start\":51778},{\"end\":51796,\"start\":51789},{\"end\":52158,\"start\":52154},{\"end\":52175,\"start\":52171},{\"end\":52195,\"start\":52187},{\"end\":52197,\"start\":52196},{\"end\":52470,\"start\":52462},{\"end\":52481,\"start\":52476},{\"end\":52494,\"start\":52490},{\"end\":52670,\"start\":52668},{\"end\":52682,\"start\":52676},{\"end\":52691,\"start\":52688},{\"end\":52700,\"start\":52697},{\"end\":52711,\"start\":52705},{\"end\":52722,\"start\":52717},{\"end\":52737,\"start\":52730},{\"end\":52750,\"start\":52743},{\"end\":53135,\"start\":53127},{\"end\":53146,\"start\":53142},{\"end\":53164,\"start\":53158},{\"end\":53414,\"start\":53408},{\"end\":53428,\"start\":53422},{\"end\":53442,\"start\":53437},{\"end\":53457,\"start\":53449},{\"end\":53470,\"start\":53465},{\"end\":53486,\"start\":53481},{\"end\":53500,\"start\":53494},{\"end\":53515,\"start\":53508},{\"end\":53785,\"start\":53779},{\"end\":53799,\"start\":53793},{\"end\":53813,\"start\":53808},{\"end\":53828,\"start\":53820},{\"end\":53841,\"start\":53836},{\"end\":53857,\"start\":53852},{\"end\":53871,\"start\":53865},{\"end\":53886,\"start\":53879},{\"end\":54326,\"start\":54322},{\"end\":54341,\"start\":54336},{\"end\":54553,\"start\":54547},{\"end\":54566,\"start\":54562},{\"end\":54582,\"start\":54577},{\"end\":54596,\"start\":54591},{\"end\":54614,\"start\":54610},{\"end\":54629,\"start\":54623},{\"end\":54647,\"start\":54641},{\"end\":54666,\"start\":54660},{\"end\":54687,\"start\":54679},{\"end\":54700,\"start\":54695},{\"end\":54702,\"start\":54701},{\"end\":55033,\"start\":55028},{\"end\":55046,\"start\":55043},{\"end\":55057,\"start\":55053},{\"end\":55074,\"start\":55067},{\"end\":55355,\"start\":55348},{\"end\":55366,\"start\":55361},{\"end\":55381,\"start\":55372},{\"end\":55395,\"start\":55387},{\"end\":55826,\"start\":55819},{\"end\":55837,\"start\":55832},{\"end\":55850,\"start\":55843},{\"end\":55866,\"start\":55857},{\"end\":55880,\"start\":55872},{\"end\":55893,\"start\":55886},{\"end\":56338,\"start\":56333},{\"end\":56353,\"start\":56348},{\"end\":56558,\"start\":56551},{\"end\":56568,\"start\":56564},{\"end\":56583,\"start\":56576},{\"end\":56593,\"start\":56588},{\"end\":56604,\"start\":56599},{\"end\":56858,\"start\":56851},{\"end\":56877,\"start\":56873},{\"end\":56886,\"start\":56882},{\"end\":57143,\"start\":57138},{\"end\":57155,\"start\":57151},{\"end\":57164,\"start\":57161},{\"end\":57177,\"start\":57169},{\"end\":57186,\"start\":57182},{\"end\":57199,\"start\":57191},{\"end\":57221,\"start\":57214},{\"end\":57233,\"start\":57227},{\"end\":57237,\"start\":57234},{\"end\":57513,\"start\":57508},{\"end\":57530,\"start\":57524},{\"end\":57547,\"start\":57541},{\"end\":57569,\"start\":57562},{\"end\":57798,\"start\":57793},{\"end\":57815,\"start\":57809},{\"end\":57832,\"start\":57826},{\"end\":57854,\"start\":57847},{\"end\":58099,\"start\":58094},{\"end\":58118,\"start\":58110},{\"end\":58131,\"start\":58126},{\"end\":58145,\"start\":58139},{\"end\":58158,\"start\":58153},{\"end\":58171,\"start\":58165},{\"end\":58186,\"start\":58179},{\"end\":58465,\"start\":58459},{\"end\":58480,\"start\":58475},{\"end\":58688,\"start\":58684},{\"end\":58702,\"start\":58697},{\"end\":58715,\"start\":58711},{\"end\":58732,\"start\":58724},{\"end\":58743,\"start\":58738},{\"end\":58758,\"start\":58752},{\"end\":58771,\"start\":58767},{\"end\":59010,\"start\":59003},{\"end\":59027,\"start\":59021},{\"end\":59029,\"start\":59028},{\"end\":59171,\"start\":59165},{\"end\":59185,\"start\":59181},{\"end\":59199,\"start\":59195},{\"end\":59213,\"start\":59208},{\"end\":59230,\"start\":59225},{\"end\":59243,\"start\":59238},{\"end\":59245,\"start\":59244},{\"end\":59258,\"start\":59253},{\"end\":59722,\"start\":59719},{\"end\":59737,\"start\":59729},{\"end\":59744,\"start\":59743},{\"end\":59746,\"start\":59745},{\"end\":59762,\"start\":59755},{\"end\":60032,\"start\":60026},{\"end\":60043,\"start\":60039},{\"end\":60054,\"start\":60049},{\"end\":60068,\"start\":60059},{\"end\":60080,\"start\":60074},{\"end\":60091,\"start\":60087},{\"end\":60103,\"start\":60099},{\"end\":60112,\"start\":60108},{\"end\":60122,\"start\":60118},{\"end\":60446,\"start\":60438},{\"end\":60457,\"start\":60453},{\"end\":60672,\"start\":60669},{\"end\":60687,\"start\":60679},{\"end\":60702,\"start\":60695},{\"end\":60718,\"start\":60710},{\"end\":60733,\"start\":60725},{\"end\":60965,\"start\":60960},{\"end\":60981,\"start\":60972},{\"end\":60997,\"start\":60988},{\"end\":61011,\"start\":61003},{\"end\":61023,\"start\":61018},{\"end\":61036,\"start\":61029},{\"end\":61483,\"start\":61479},{\"end\":61499,\"start\":61490},{\"end\":61510,\"start\":61505},{\"end\":61523,\"start\":61517},{\"end\":61535,\"start\":61531},{\"end\":61740,\"start\":61734},{\"end\":61749,\"start\":61745},{\"end\":61767,\"start\":61759},{\"end\":61781,\"start\":61775},{\"end\":61795,\"start\":61788},{\"end\":61808,\"start\":61802},{\"end\":62076,\"start\":62066},{\"end\":62091,\"start\":62083},{\"end\":62105,\"start\":62097},{\"end\":62118,\"start\":62110},{\"end\":62130,\"start\":62125}]", "bib_author_last_name": "[{\"end\":44886,\"start\":44879},{\"end\":44907,\"start\":44897},{\"end\":44921,\"start\":44913},{\"end\":45154,\"start\":45147},{\"end\":45169,\"start\":45162},{\"end\":45182,\"start\":45179},{\"end\":45195,\"start\":45189},{\"end\":45202,\"start\":45197},{\"end\":45478,\"start\":45474},{\"end\":45497,\"start\":45487},{\"end\":45515,\"start\":45507},{\"end\":45529,\"start\":45523},{\"end\":45544,\"start\":45538},{\"end\":45868,\"start\":45864},{\"end\":45887,\"start\":45877},{\"end\":45905,\"start\":45897},{\"end\":45919,\"start\":45913},{\"end\":45934,\"start\":45928},{\"end\":46226,\"start\":46222},{\"end\":46241,\"start\":46234},{\"end\":46249,\"start\":46247},{\"end\":46259,\"start\":46255},{\"end\":46277,\"start\":46267},{\"end\":46287,\"start\":46283},{\"end\":46505,\"start\":46499},{\"end\":46523,\"start\":46513},{\"end\":46543,\"start\":46534},{\"end\":47084,\"start\":47078},{\"end\":47099,\"start\":47094},{\"end\":47116,\"start\":47111},{\"end\":47130,\"start\":47123},{\"end\":47148,\"start\":47139},{\"end\":47166,\"start\":47158},{\"end\":47473,\"start\":47468},{\"end\":47488,\"start\":47481},{\"end\":47688,\"start\":47684},{\"end\":47698,\"start\":47694},{\"end\":47714,\"start\":47708},{\"end\":47725,\"start\":47723},{\"end\":47733,\"start\":47731},{\"end\":47745,\"start\":47738},{\"end\":47960,\"start\":47956},{\"end\":47975,\"start\":47971},{\"end\":47987,\"start\":47984},{\"end\":48000,\"start\":47998},{\"end\":48012,\"start\":48008},{\"end\":48026,\"start\":48022},{\"end\":48042,\"start\":48037},{\"end\":48057,\"start\":48053},{\"end\":48543,\"start\":48532},{\"end\":48556,\"start\":48551},{\"end\":48578,\"start\":48568},{\"end\":48596,\"start\":48585},{\"end\":48610,\"start\":48606},{\"end\":48630,\"start\":48619},{\"end\":48648,\"start\":48640},{\"end\":48667,\"start\":48659},{\"end\":48682,\"start\":48675},{\"end\":48697,\"start\":48692},{\"end\":49140,\"start\":49137},{\"end\":49153,\"start\":49148},{\"end\":49432,\"start\":49429},{\"end\":49440,\"start\":49437},{\"end\":49463,\"start\":49451},{\"end\":49491,\"start\":49469},{\"end\":49504,\"start\":49501},{\"end\":49515,\"start\":49512},{\"end\":49525,\"start\":49517},{\"end\":49814,\"start\":49812},{\"end\":49829,\"start\":49824},{\"end\":49843,\"start\":49840},{\"end\":49853,\"start\":49850},{\"end\":50059,\"start\":50053},{\"end\":50074,\"start\":50068},{\"end\":50087,\"start\":50082},{\"end\":50292,\"start\":50287},{\"end\":50307,\"start\":50303},{\"end\":50321,\"start\":50316},{\"end\":50334,\"start\":50329},{\"end\":50347,\"start\":50344},{\"end\":50358,\"start\":50355},{\"end\":50601,\"start\":50595},{\"end\":50614,\"start\":50609},{\"end\":50812,\"start\":50808},{\"end\":50825,\"start\":50820},{\"end\":50838,\"start\":50834},{\"end\":50856,\"start\":50848},{\"end\":50872,\"start\":50863},{\"end\":51151,\"start\":51145},{\"end\":51164,\"start\":51159},{\"end\":51381,\"start\":51373},{\"end\":51395,\"start\":51389},{\"end\":51399,\"start\":51397},{\"end\":51555,\"start\":51547},{\"end\":51570,\"start\":51562},{\"end\":51743,\"start\":51738},{\"end\":51754,\"start\":51751},{\"end\":51776,\"start\":51763},{\"end\":51787,\"start\":51780},{\"end\":51802,\"start\":51797},{\"end\":51812,\"start\":51804},{\"end\":52169,\"start\":52159},{\"end\":52185,\"start\":52176},{\"end\":52204,\"start\":52198},{\"end\":52474,\"start\":52471},{\"end\":52488,\"start\":52482},{\"end\":52503,\"start\":52495},{\"end\":52674,\"start\":52671},{\"end\":52686,\"start\":52683},{\"end\":52695,\"start\":52692},{\"end\":52703,\"start\":52701},{\"end\":52715,\"start\":52712},{\"end\":52728,\"start\":52723},{\"end\":52741,\"start\":52738},{\"end\":52754,\"start\":52751},{\"end\":53140,\"start\":53136},{\"end\":53156,\"start\":53147},{\"end\":53172,\"start\":53165},{\"end\":53420,\"start\":53415},{\"end\":53435,\"start\":53429},{\"end\":53447,\"start\":53443},{\"end\":53463,\"start\":53458},{\"end\":53479,\"start\":53471},{\"end\":53492,\"start\":53487},{\"end\":53506,\"start\":53501},{\"end\":53524,\"start\":53516},{\"end\":53791,\"start\":53786},{\"end\":53806,\"start\":53800},{\"end\":53818,\"start\":53814},{\"end\":53834,\"start\":53829},{\"end\":53850,\"start\":53842},{\"end\":53863,\"start\":53858},{\"end\":53877,\"start\":53872},{\"end\":53895,\"start\":53887},{\"end\":54334,\"start\":54327},{\"end\":54347,\"start\":54342},{\"end\":54560,\"start\":54554},{\"end\":54575,\"start\":54567},{\"end\":54589,\"start\":54583},{\"end\":54608,\"start\":54597},{\"end\":54621,\"start\":54615},{\"end\":54639,\"start\":54630},{\"end\":54658,\"start\":54648},{\"end\":54677,\"start\":54667},{\"end\":54693,\"start\":54688},{\"end\":54709,\"start\":54703},{\"end\":55041,\"start\":55034},{\"end\":55051,\"start\":55047},{\"end\":55065,\"start\":55058},{\"end\":55079,\"start\":55075},{\"end\":55359,\"start\":55356},{\"end\":55370,\"start\":55367},{\"end\":55385,\"start\":55382},{\"end\":55401,\"start\":55396},{\"end\":55830,\"start\":55827},{\"end\":55841,\"start\":55838},{\"end\":55855,\"start\":55851},{\"end\":55870,\"start\":55867},{\"end\":55884,\"start\":55881},{\"end\":55897,\"start\":55894},{\"end\":56346,\"start\":56339},{\"end\":56359,\"start\":56354},{\"end\":56562,\"start\":56559},{\"end\":56574,\"start\":56569},{\"end\":56586,\"start\":56584},{\"end\":56597,\"start\":56594},{\"end\":56609,\"start\":56605},{\"end\":56613,\"start\":56611},{\"end\":56871,\"start\":56859},{\"end\":56880,\"start\":56878},{\"end\":56895,\"start\":56887},{\"end\":56900,\"start\":56897},{\"end\":57149,\"start\":57144},{\"end\":57159,\"start\":57156},{\"end\":57167,\"start\":57165},{\"end\":57180,\"start\":57178},{\"end\":57189,\"start\":57187},{\"end\":57212,\"start\":57200},{\"end\":57225,\"start\":57222},{\"end\":57241,\"start\":57238},{\"end\":57522,\"start\":57514},{\"end\":57539,\"start\":57531},{\"end\":57560,\"start\":57548},{\"end\":57576,\"start\":57570},{\"end\":57807,\"start\":57799},{\"end\":57824,\"start\":57816},{\"end\":57845,\"start\":57833},{\"end\":57861,\"start\":57855},{\"end\":58108,\"start\":58100},{\"end\":58124,\"start\":58119},{\"end\":58137,\"start\":58132},{\"end\":58151,\"start\":58146},{\"end\":58163,\"start\":58159},{\"end\":58177,\"start\":58172},{\"end\":58195,\"start\":58187},{\"end\":58473,\"start\":58466},{\"end\":58486,\"start\":58481},{\"end\":58695,\"start\":58689},{\"end\":58709,\"start\":58703},{\"end\":58722,\"start\":58716},{\"end\":58736,\"start\":58733},{\"end\":58750,\"start\":58744},{\"end\":58765,\"start\":58759},{\"end\":58775,\"start\":58772},{\"end\":59019,\"start\":59011},{\"end\":59035,\"start\":59030},{\"end\":59179,\"start\":59172},{\"end\":59193,\"start\":59186},{\"end\":59206,\"start\":59200},{\"end\":59223,\"start\":59214},{\"end\":59236,\"start\":59231},{\"end\":59251,\"start\":59246},{\"end\":59265,\"start\":59259},{\"end\":59277,\"start\":59267},{\"end\":59727,\"start\":59723},{\"end\":59741,\"start\":59738},{\"end\":59753,\"start\":59747},{\"end\":59766,\"start\":59763},{\"end\":59772,\"start\":59768},{\"end\":60037,\"start\":60033},{\"end\":60047,\"start\":60044},{\"end\":60057,\"start\":60055},{\"end\":60072,\"start\":60069},{\"end\":60085,\"start\":60081},{\"end\":60097,\"start\":60092},{\"end\":60106,\"start\":60104},{\"end\":60116,\"start\":60113},{\"end\":60127,\"start\":60123},{\"end\":60451,\"start\":60447},{\"end\":60466,\"start\":60458},{\"end\":60677,\"start\":60673},{\"end\":60693,\"start\":60688},{\"end\":60708,\"start\":60703},{\"end\":60723,\"start\":60719},{\"end\":60742,\"start\":60734},{\"end\":60970,\"start\":60966},{\"end\":60986,\"start\":60982},{\"end\":61001,\"start\":60998},{\"end\":61016,\"start\":61012},{\"end\":61027,\"start\":61024},{\"end\":61041,\"start\":61037},{\"end\":61488,\"start\":61484},{\"end\":61503,\"start\":61500},{\"end\":61515,\"start\":61511},{\"end\":61529,\"start\":61524},{\"end\":61539,\"start\":61536},{\"end\":61743,\"start\":61741},{\"end\":61757,\"start\":61750},{\"end\":61773,\"start\":61768},{\"end\":61786,\"start\":61782},{\"end\":61800,\"start\":61796},{\"end\":61813,\"start\":61809},{\"end\":62081,\"start\":62077},{\"end\":62095,\"start\":62092},{\"end\":62108,\"start\":62106},{\"end\":62123,\"start\":62119},{\"end\":62134,\"start\":62131}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":12726540},\"end\":45069,\"start\":44810},{\"attributes\":{\"id\":\"b1\"},\"end\":45377,\"start\":45071},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":1996665},\"end\":45737,\"start\":45379},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3429309},\"end\":46174,\"start\":45739},{\"attributes\":{\"id\":\"b4\"},\"end\":46429,\"start\":46176},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":29468815},\"end\":46694,\"start\":46431},{\"attributes\":{\"id\":\"b6\"},\"end\":46956,\"start\":46696},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":502946},\"end\":47394,\"start\":46958},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":4453972},\"end\":47625,\"start\":47396},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":57246310},\"end\":47891,\"start\":47627},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":201696286},\"end\":48406,\"start\":47893},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":225039882},\"end\":49006,\"start\":48408},{\"attributes\":{\"id\":\"b12\"},\"end\":49328,\"start\":49008},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":212415056},\"end\":49756,\"start\":49330},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":206594692},\"end\":49985,\"start\":49758},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":13207256},\"end\":50222,\"start\":49987},{\"attributes\":{\"id\":\"b16\"},\"end\":50523,\"start\":50224},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":15978957},\"end\":50739,\"start\":50525},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":196623408},\"end\":51034,\"start\":50741},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":14915460},\"end\":51325,\"start\":51036},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":6628106},\"end\":51500,\"start\":51327},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":57721164},\"end\":51731,\"start\":51502},{\"attributes\":{\"doi\":\"arXiv:1506.06726\",\"id\":\"b22\"},\"end\":52087,\"start\":51733},{\"attributes\":{\"id\":\"b23\"},\"end\":52356,\"start\":52089},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":10716717},\"end\":52666,\"start\":52358},{\"attributes\":{\"doi\":\"arXiv:2103.14030\",\"id\":\"b25\"},\"end\":53069,\"start\":52668},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":1629541},\"end\":53312,\"start\":53071},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":195892710},\"end\":53777,\"start\":53314},{\"attributes\":{\"id\":\"b28\"},\"end\":54241,\"start\":53779},{\"attributes\":{\"id\":\"b29\"},\"end\":54485,\"start\":54243},{\"attributes\":{\"id\":\"b30\"},\"end\":54964,\"start\":54487},{\"attributes\":{\"doi\":\"arXiv:1301.3781\",\"id\":\"b31\"},\"end\":55273,\"start\":54966},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":204837612},\"end\":55715,\"start\":55275},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":221112548},\"end\":56262,\"start\":55717},{\"attributes\":{\"id\":\"b34\"},\"end\":56483,\"start\":56264},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":211117398},\"end\":56769,\"start\":56485},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":10328909},\"end\":57063,\"start\":56771},{\"attributes\":{\"id\":\"b37\"},\"end\":57453,\"start\":57065},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":56279477},\"end\":57738,\"start\":57455},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":56279477},\"end\":58023,\"start\":57740},{\"attributes\":{\"id\":\"b40\"},\"end\":58400,\"start\":58025},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":29872421},\"end\":58608,\"start\":58402},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":232135291},\"end\":58970,\"start\":58610},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":2777306},\"end\":59136,\"start\":58972},{\"attributes\":{\"id\":\"b44\"},\"end\":59647,\"start\":59138},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":221445828},\"end\":59932,\"start\":59649},{\"attributes\":{\"doi\":\"arXiv:2102.12122\",\"id\":\"b46\"},\"end\":60405,\"start\":59934},{\"attributes\":{\"id\":\"b47\"},\"end\":60612,\"start\":60407},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":206822288},\"end\":60905,\"start\":60614},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":184482895},\"end\":61425,\"start\":60907},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":50781105},\"end\":61688,\"start\":61427},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":10704475},\"end\":62033,\"start\":61690},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":5299559},\"end\":62272,\"start\":62035}]", "bib_title": "[{\"end\":44871,\"start\":44810},{\"end\":45460,\"start\":45379},{\"end\":45850,\"start\":45739},{\"end\":46487,\"start\":46431},{\"end\":47069,\"start\":46958},{\"end\":47459,\"start\":47396},{\"end\":47678,\"start\":47627},{\"end\":47949,\"start\":47893},{\"end\":48523,\"start\":48408},{\"end\":49418,\"start\":49330},{\"end\":49802,\"start\":49758},{\"end\":50044,\"start\":49987},{\"end\":50586,\"start\":50525},{\"end\":50798,\"start\":50741},{\"end\":51133,\"start\":51036},{\"end\":51369,\"start\":51327},{\"end\":51535,\"start\":51502},{\"end\":52460,\"start\":52358},{\"end\":53125,\"start\":53071},{\"end\":53406,\"start\":53314},{\"end\":55346,\"start\":55275},{\"end\":55817,\"start\":55717},{\"end\":56549,\"start\":56485},{\"end\":56849,\"start\":56771},{\"end\":57506,\"start\":57455},{\"end\":57791,\"start\":57740},{\"end\":58457,\"start\":58402},{\"end\":58682,\"start\":58610},{\"end\":59001,\"start\":58972},{\"end\":59717,\"start\":59649},{\"end\":60436,\"start\":60407},{\"end\":60667,\"start\":60614},{\"end\":60958,\"start\":60907},{\"end\":61477,\"start\":61427},{\"end\":61732,\"start\":61690},{\"end\":62064,\"start\":62035}]", "bib_author": "[{\"end\":44888,\"start\":44873},{\"end\":44909,\"start\":44888},{\"end\":44923,\"start\":44909},{\"end\":45156,\"start\":45145},{\"end\":45171,\"start\":45156},{\"end\":45184,\"start\":45171},{\"end\":45197,\"start\":45184},{\"end\":45204,\"start\":45197},{\"end\":45480,\"start\":45462},{\"end\":45499,\"start\":45480},{\"end\":45517,\"start\":45499},{\"end\":45531,\"start\":45517},{\"end\":45546,\"start\":45531},{\"end\":45870,\"start\":45852},{\"end\":45889,\"start\":45870},{\"end\":45907,\"start\":45889},{\"end\":45921,\"start\":45907},{\"end\":45936,\"start\":45921},{\"end\":46228,\"start\":46218},{\"end\":46243,\"start\":46228},{\"end\":46251,\"start\":46243},{\"end\":46261,\"start\":46251},{\"end\":46279,\"start\":46261},{\"end\":46289,\"start\":46279},{\"end\":46507,\"start\":46489},{\"end\":46525,\"start\":46507},{\"end\":46545,\"start\":46525},{\"end\":47086,\"start\":47071},{\"end\":47101,\"start\":47086},{\"end\":47118,\"start\":47101},{\"end\":47132,\"start\":47118},{\"end\":47150,\"start\":47132},{\"end\":47168,\"start\":47150},{\"end\":47475,\"start\":47461},{\"end\":47490,\"start\":47475},{\"end\":47690,\"start\":47680},{\"end\":47700,\"start\":47690},{\"end\":47716,\"start\":47700},{\"end\":47727,\"start\":47716},{\"end\":47735,\"start\":47727},{\"end\":47747,\"start\":47735},{\"end\":47962,\"start\":47951},{\"end\":47977,\"start\":47962},{\"end\":47989,\"start\":47977},{\"end\":48002,\"start\":47989},{\"end\":48014,\"start\":48002},{\"end\":48028,\"start\":48014},{\"end\":48044,\"start\":48028},{\"end\":48059,\"start\":48044},{\"end\":48545,\"start\":48525},{\"end\":48558,\"start\":48545},{\"end\":48580,\"start\":48558},{\"end\":48598,\"start\":48580},{\"end\":48612,\"start\":48598},{\"end\":48632,\"start\":48612},{\"end\":48650,\"start\":48632},{\"end\":48669,\"start\":48650},{\"end\":48684,\"start\":48669},{\"end\":48699,\"start\":48684},{\"end\":49142,\"start\":49130},{\"end\":49155,\"start\":49142},{\"end\":49434,\"start\":49420},{\"end\":49442,\"start\":49434},{\"end\":49465,\"start\":49442},{\"end\":49493,\"start\":49465},{\"end\":49506,\"start\":49493},{\"end\":49517,\"start\":49506},{\"end\":49527,\"start\":49517},{\"end\":49816,\"start\":49804},{\"end\":49831,\"start\":49816},{\"end\":49845,\"start\":49831},{\"end\":49855,\"start\":49845},{\"end\":50061,\"start\":50046},{\"end\":50076,\"start\":50061},{\"end\":50089,\"start\":50076},{\"end\":50294,\"start\":50280},{\"end\":50309,\"start\":50294},{\"end\":50323,\"start\":50309},{\"end\":50336,\"start\":50323},{\"end\":50349,\"start\":50336},{\"end\":50360,\"start\":50349},{\"end\":50603,\"start\":50588},{\"end\":50616,\"start\":50603},{\"end\":50814,\"start\":50800},{\"end\":50827,\"start\":50814},{\"end\":50840,\"start\":50827},{\"end\":50858,\"start\":50840},{\"end\":50874,\"start\":50858},{\"end\":51153,\"start\":51135},{\"end\":51166,\"start\":51153},{\"end\":51383,\"start\":51371},{\"end\":51397,\"start\":51383},{\"end\":51401,\"start\":51397},{\"end\":51557,\"start\":51537},{\"end\":51572,\"start\":51557},{\"end\":51745,\"start\":51733},{\"end\":51756,\"start\":51745},{\"end\":51778,\"start\":51756},{\"end\":51789,\"start\":51778},{\"end\":51804,\"start\":51789},{\"end\":51814,\"start\":51804},{\"end\":52171,\"start\":52154},{\"end\":52187,\"start\":52171},{\"end\":52206,\"start\":52187},{\"end\":52476,\"start\":52462},{\"end\":52490,\"start\":52476},{\"end\":52505,\"start\":52490},{\"end\":52676,\"start\":52668},{\"end\":52688,\"start\":52676},{\"end\":52697,\"start\":52688},{\"end\":52705,\"start\":52697},{\"end\":52717,\"start\":52705},{\"end\":52730,\"start\":52717},{\"end\":52743,\"start\":52730},{\"end\":52756,\"start\":52743},{\"end\":53142,\"start\":53127},{\"end\":53158,\"start\":53142},{\"end\":53174,\"start\":53158},{\"end\":53422,\"start\":53408},{\"end\":53437,\"start\":53422},{\"end\":53449,\"start\":53437},{\"end\":53465,\"start\":53449},{\"end\":53481,\"start\":53465},{\"end\":53494,\"start\":53481},{\"end\":53508,\"start\":53494},{\"end\":53526,\"start\":53508},{\"end\":53793,\"start\":53779},{\"end\":53808,\"start\":53793},{\"end\":53820,\"start\":53808},{\"end\":53836,\"start\":53820},{\"end\":53852,\"start\":53836},{\"end\":53865,\"start\":53852},{\"end\":53879,\"start\":53865},{\"end\":53897,\"start\":53879},{\"end\":54336,\"start\":54322},{\"end\":54349,\"start\":54336},{\"end\":54562,\"start\":54547},{\"end\":54577,\"start\":54562},{\"end\":54591,\"start\":54577},{\"end\":54610,\"start\":54591},{\"end\":54623,\"start\":54610},{\"end\":54641,\"start\":54623},{\"end\":54660,\"start\":54641},{\"end\":54679,\"start\":54660},{\"end\":54695,\"start\":54679},{\"end\":54711,\"start\":54695},{\"end\":55043,\"start\":55028},{\"end\":55053,\"start\":55043},{\"end\":55067,\"start\":55053},{\"end\":55081,\"start\":55067},{\"end\":55361,\"start\":55348},{\"end\":55372,\"start\":55361},{\"end\":55387,\"start\":55372},{\"end\":55403,\"start\":55387},{\"end\":55832,\"start\":55819},{\"end\":55843,\"start\":55832},{\"end\":55857,\"start\":55843},{\"end\":55872,\"start\":55857},{\"end\":55886,\"start\":55872},{\"end\":55899,\"start\":55886},{\"end\":56348,\"start\":56333},{\"end\":56361,\"start\":56348},{\"end\":56564,\"start\":56551},{\"end\":56576,\"start\":56564},{\"end\":56588,\"start\":56576},{\"end\":56599,\"start\":56588},{\"end\":56611,\"start\":56599},{\"end\":56615,\"start\":56611},{\"end\":56873,\"start\":56851},{\"end\":56882,\"start\":56873},{\"end\":56897,\"start\":56882},{\"end\":56902,\"start\":56897},{\"end\":57151,\"start\":57138},{\"end\":57161,\"start\":57151},{\"end\":57169,\"start\":57161},{\"end\":57182,\"start\":57169},{\"end\":57191,\"start\":57182},{\"end\":57214,\"start\":57191},{\"end\":57227,\"start\":57214},{\"end\":57243,\"start\":57227},{\"end\":57524,\"start\":57508},{\"end\":57541,\"start\":57524},{\"end\":57562,\"start\":57541},{\"end\":57578,\"start\":57562},{\"end\":57809,\"start\":57793},{\"end\":57826,\"start\":57809},{\"end\":57847,\"start\":57826},{\"end\":57863,\"start\":57847},{\"end\":58110,\"start\":58094},{\"end\":58126,\"start\":58110},{\"end\":58139,\"start\":58126},{\"end\":58153,\"start\":58139},{\"end\":58165,\"start\":58153},{\"end\":58179,\"start\":58165},{\"end\":58197,\"start\":58179},{\"end\":58475,\"start\":58459},{\"end\":58488,\"start\":58475},{\"end\":58697,\"start\":58684},{\"end\":58711,\"start\":58697},{\"end\":58724,\"start\":58711},{\"end\":58738,\"start\":58724},{\"end\":58752,\"start\":58738},{\"end\":58767,\"start\":58752},{\"end\":58777,\"start\":58767},{\"end\":59021,\"start\":59003},{\"end\":59037,\"start\":59021},{\"end\":59181,\"start\":59165},{\"end\":59195,\"start\":59181},{\"end\":59208,\"start\":59195},{\"end\":59225,\"start\":59208},{\"end\":59238,\"start\":59225},{\"end\":59253,\"start\":59238},{\"end\":59267,\"start\":59253},{\"end\":59279,\"start\":59267},{\"end\":59729,\"start\":59719},{\"end\":59743,\"start\":59729},{\"end\":59755,\"start\":59743},{\"end\":59768,\"start\":59755},{\"end\":59774,\"start\":59768},{\"end\":60039,\"start\":60026},{\"end\":60049,\"start\":60039},{\"end\":60059,\"start\":60049},{\"end\":60074,\"start\":60059},{\"end\":60087,\"start\":60074},{\"end\":60099,\"start\":60087},{\"end\":60108,\"start\":60099},{\"end\":60118,\"start\":60108},{\"end\":60129,\"start\":60118},{\"end\":60453,\"start\":60438},{\"end\":60468,\"start\":60453},{\"end\":60679,\"start\":60669},{\"end\":60695,\"start\":60679},{\"end\":60710,\"start\":60695},{\"end\":60725,\"start\":60710},{\"end\":60744,\"start\":60725},{\"end\":60972,\"start\":60960},{\"end\":60988,\"start\":60972},{\"end\":61003,\"start\":60988},{\"end\":61018,\"start\":61003},{\"end\":61029,\"start\":61018},{\"end\":61043,\"start\":61029},{\"end\":61490,\"start\":61479},{\"end\":61505,\"start\":61490},{\"end\":61517,\"start\":61505},{\"end\":61531,\"start\":61517},{\"end\":61541,\"start\":61531},{\"end\":61745,\"start\":61734},{\"end\":61759,\"start\":61745},{\"end\":61775,\"start\":61759},{\"end\":61788,\"start\":61775},{\"end\":61802,\"start\":61788},{\"end\":61815,\"start\":61802},{\"end\":62083,\"start\":62066},{\"end\":62097,\"start\":62083},{\"end\":62110,\"start\":62097},{\"end\":62125,\"start\":62110},{\"end\":62136,\"start\":62125}]", "bib_venue": "[{\"end\":44927,\"start\":44923},{\"end\":45143,\"start\":45071},{\"end\":45550,\"start\":45546},{\"end\":45941,\"start\":45936},{\"end\":46216,\"start\":46176},{\"end\":46550,\"start\":46545},{\"end\":46800,\"start\":46696},{\"end\":47172,\"start\":47168},{\"end\":47496,\"start\":47490},{\"end\":47751,\"start\":47747},{\"end\":48116,\"start\":48059},{\"end\":48703,\"start\":48699},{\"end\":49128,\"start\":49008},{\"end\":49530,\"start\":49527},{\"end\":49859,\"start\":49855},{\"end\":50092,\"start\":50089},{\"end\":50278,\"start\":50224},{\"end\":50620,\"start\":50616},{\"end\":50879,\"start\":50874},{\"end\":51170,\"start\":51166},{\"end\":51405,\"start\":51401},{\"end\":51576,\"start\":51572},{\"end\":51890,\"start\":51830},{\"end\":52152,\"start\":52089},{\"end\":52509,\"start\":52505},{\"end\":52843,\"start\":52772},{\"end\":53178,\"start\":53174},{\"end\":53531,\"start\":53526},{\"end\":53995,\"start\":53897},{\"end\":54320,\"start\":54243},{\"end\":54545,\"start\":54487},{\"end\":55026,\"start\":54966},{\"end\":55460,\"start\":55403},{\"end\":55956,\"start\":55899},{\"end\":56331,\"start\":56264},{\"end\":56619,\"start\":56615},{\"end\":56909,\"start\":56902},{\"end\":57136,\"start\":57065},{\"end\":57582,\"start\":57578},{\"end\":57867,\"start\":57863},{\"end\":58092,\"start\":58025},{\"end\":58493,\"start\":58488},{\"end\":58781,\"start\":58777},{\"end\":59041,\"start\":59037},{\"end\":59163,\"start\":59138},{\"end\":59778,\"start\":59774},{\"end\":60024,\"start\":59934},{\"end\":60472,\"start\":60468},{\"end\":60748,\"start\":60744},{\"end\":61123,\"start\":61043},{\"end\":61545,\"start\":61541},{\"end\":61846,\"start\":61815},{\"end\":62140,\"start\":62136},{\"end\":48160,\"start\":48118},{\"end\":55504,\"start\":55462},{\"end\":56000,\"start\":55958},{\"end\":61190,\"start\":61125}]"}}}, "year": 2023, "month": 12, "day": 17}