{"id": 211677656, "updated": "2023-11-11 03:49:08.697", "metadata": {"title": "Towards Using Count-level Weak Supervision for Crowd Counting", "authors": "[{\"first\":\"Yinjie\",\"last\":\"Lei\",\"middle\":[]},{\"first\":\"Yan\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Pingping\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Lingqiao\",\"last\":\"Liu\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 2, "day": 29}, "abstract": "Most existing crowd counting methods require object location-level annotation, i.e., placing a dot at the center of an object. While being simpler than the bounding-box or pixel-level annotation, obtaining this annotation is still labor-intensive and time-consuming especially for images with highly crowded scenes. On the other hand, weaker annotations that only know the total count of objects can be almost effortless in many practical scenarios. Thus, it is desirable to develop a learning method that can effectively train models from count-level annotations. To this end, this paper studies the problem of weakly-supervised crowd counting which learns a model from only a small amount of location-level annotations (fully-supervised) but a large amount of count-level annotations (weakly-supervised). To perform effective training in this scenario, we observe that the direct solution of regressing the integral of density map to the object count is not sufficient and it is beneficial to introduce stronger regularizations on the predicted density map of weakly-annotated images. We devise a simple-yet-effective training strategy, namely Multiple Auxiliary Tasks Training (MATT), to construct regularizes for restricting the freedom of the generated density maps. Through extensive experiments on existing datasets and a newly proposed dataset, we validate the effectiveness of the proposed weakly-supervised method and demonstrate its superior performance over existing solutions.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3081099313", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/pr/LeiLZL21", "doi": "10.1016/j.patcog.2020.107616"}}, "content": {"source": {"pdf_hash": "6b76f517e70b6d407e395d8ee750653b31b1d473", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2003.00164v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2003.00164", "status": "GREEN"}}, "grobid": {"id": "447f9079eb1298d5bd1c84fc1be3903d704fad1a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/6b76f517e70b6d407e395d8ee750653b31b1d473.txt", "contents": "\nTowards Using Count-level Weak Supervision for Crowd Counting\n29 Feb 2020\n\nYinjie Lei es:yinjie@scu.edu.cnyinjielei \nCollege of Electronics and Information Engineering\nSichuan University\nChengduChina\n\nYan Liu yanliu27@stu.scu.edu.cn \nCollege of Electronics and Information Engineering\nSichuan University\nChengduChina\n\nPingping Zhang \nSchool of Information and Communication Engineering\nDalian University of Technology\nDalianChina\n\nLingqiao Liu lingqiao.liu@adelaide.edu.au \nSchool of Computer Science\nThe University of Adelaide\nAdelaideAustralia\n\nTowards Using Count-level Weak Supervision for Crowd Counting\n29 Feb 2020Preprint submitted to Elsevier March 3, 2020* Corresponding authorcrowd countingcount-level annotationweak supervisionauxiliary tasks learningasymmetry training\nMost existing crowd counting methods require object location-level annotation, i.e., placing a dot at the center of an object. While being simpler than the boundingbox or pixel-level annotation, obtaining this annotation is still labor-intensive and time-consuming especially for images with highly crowded scenes. On the other hand, weaker annotations that only know the total count of objects can be almost effortless in many practical scenarios. Thus, it is desirable to develop a learning method that can effectively train models from count-level annotations. To this end, this paper studies the problem of weakly-supervised crowd counting which learns a model from only a small amount of location-level annotations (fully-supervised) but a large amount of count-level annotations (weakly-supervised). To perform effective training in this scenario, we observe that the direct solution of regressing the integral of density map to the object count is not sufficient and it is beneficial to introduce stronger regularizations on the predicted density map of weaklyannotated images. We devise a simple-yet-effective training strategy, namely Multiple Auxiliary Tasks Training (MATT), to construct regularizes for restricting the freedom of the generated density maps. Through extensive experiments on existing datasets and a newly proposed dataset, we validate the effectiveness of the proposed weakly-supervised method and demonstrate its superior performance over existing solutions.\n\nIntroduction\n\nCrowd counting aims to estimate the number of object-of-interest within an input image [1,2] or video sequence [3,4]. The modern solution to the crowd counting problem is based on the idea of \"learning to count \" [5], that is, building a learnable model to directly or indirectly estimate the count number [6,7]. To train such a crowd counting model, annotation on the object location is often needed. In the most commonly used annotation protocol, a dot is put on the object center to create a \"dot map\" to indicate the object location. Then after convolving with a Gaussian kernel, a \"dot-map\" can covert to a \"density map\" for model training. Comparing with the bounding box annotation in object detection [8], or the pixel-level annotation in object segmentation [9], the dot-map based locationlevel annotation seems to be easier to obtain. However, for a crowd scene, the number of objects can be large and in such a scenario collecting the location-level object annotations can become labor-intensive and time-consuming.\n\nOn the other hand, the count-level annotation, i.e., the total count of objects, can usually be effortlessly obtained in many practical scenarios. For example, we can take images of the same set of objects with different spatial arrangements or from different viewpoints and the total object count remains the same. Once we know the total count of one image, i.e., \"seed image\" 1 , the total count for the remaining ones are known 2 . For the sake of reducing annotation cost, it is desirable to develop methods that can fully leverage the count-level annotation to train a crowd counting model.\n\nDespite being cheap to collect, count-level annotation induces weaker supervision signals than the location-level annotation since the latter naturally derives the count-level annotation but not vice versa. In the literature, training a model 1 The total count of seed image can be obtained by using manually counting. Note that it is common practice to put a dot mark on already counted objects for the convenience of counting. So the location-level annotation for the seed images will be naturally obtained as the byproduct of the counting process. 2 Note that in this example, we only assume that the camera is movable at the training stage. The learned crowd counter can be applied to the scenario that the camera is fixed. Therefore, the multi-view information or motion information for crowd counting is not always available at the test stage. by using a large number of images with only count-level annotation has not been systematically studied, and it is still unclear to what extent using the count-level annotation can benefit model training.\n\nTo fill this gap, this paper studies how to train a crowd counting model in the weakly-supervised training setting which assumes the availability of a small amount of location-level annotations and a large amount of count-level annotation, as demonstrated in Figure 1. At first glance, training under such weakly-supervised training setting can be easily achieved with a loss function to encourage the integration of the estimated density map being close to the ground-truth total count. However, through our study, we discover that this naive solution is not sufficiently effective since it does not properly constrain the predicted density maps with only count-level annotation, a.k.a, weakly-annotated images. To overcome this drawback, we introduce more loss terms to regularize the predicted density maps. These losses are constructed by adding extra branches and auxiliary training tasks at the training stage. Specifically, we introduce multiple auxiliary prediction branches to produce different but equivalent density maps. Two types of losses are imposed at each branch: (1) the integral of the predicted density map from each branch should be close to the object count. (2) density map estimations from different branches should be consistent. Those auxiliary branches essentially encourage the feature extractor to encode more accurate location information to support diverse density map realizations. Consequently, a better feature extractor and the corresponding crowd counting model can be obtained.\n\nFurthermore, a new dataset is released to verify that if the count information obtained effortlessly can benefit the model training and whether the proposed method is more robust than the naive solution. Such dataset uses the progressive adding/removing strategy to collect a large amount of images with easily obtained total count annotation. We evaluate our method on this newly introduced dataset and the traditional crowd counting datasets. The results demonstrate the advantage of the count-level weakly-supervised training and the superior performance over the baseline methods. The main contributions of this paper can be summarized as follows:\n\n\u2022 We identify the count-level weakly-supervised method as an effective way to alleviate the burden of annotations.\n\n\u2022 We develop a novel Multiple Auxiliary Tasks Training strategy to achieve better prediction performance.\n\n\u2022 We introduce a new dataset that is designed for evaluating the weakly-   supervised crowd counting solutions.\n\n\nRelated Works\n\n\nCrowd Counting by Detection\n\nEarly methods rely on extracting low-level or hand-craft features [10] to obtain global count in a specific scene, e.g., Histogram Oriented Gradients (HOG) [11] and Haar wavelets [12]. Then object detection algorithms apply to crowd counting and approaches based on detecting individual heads or body parts [13] are widely used. Detection-based methods [14] are qualified to sparsely populated regions. Nevertheless, when it comes to some extreme cases, such as low-resolution, heavy occlusion, counting-by-detection are impotent to give an appropriate prediction.\n\n\nCrowd Counting by Regression\n\nTo handle images with a dense crowd, the feature-regression-based methods [15] are proposed. These approaches learn a mapping function from the feature domain to corresponding counts [5] or density [1]. These methods are relying on the feature extraction mechanism, such as the Gaussian Process [16,17] and Random Forests regression [18,19]. In the early years, regressing count value is widely adopted. To make use of spatial information, later works convert this task to a density map regression problem, and integrating a density map can be used to obtain the final crowd counts.\n\n\nCrowd Counting by CNN\n\nBenefited from the deep convolutional neural network [20], counting-by-CNN methods have received much attention. CNN-based methods [21,22,23] typically center around density map prediction and crowd count regression leveraging a Fully Convolutional Network (FCN) [24,25,26]. Most recent crowd counting methods follow the framework developed in [5], which solves crowd counting problem by regressing to the object density map. The regression framework is more robust than other options such as counting by detection [27,28], counting by segmentation [29,30], or directly estimating the count value [16,31].\n\nSince then, many works have been proposed to modify this framework. For example, the work in [32] proposes a switching network architecture to better handle the multi-scale issue in crowd counting. The context information is exploited in [33] to produce more accurate density maps. The state-of-the-art network architecture [34] in image segmentation is also shown to be useful for crowd counting in [35]. This work is then upgraded by incorporating a two-stream network architecture [36]. Recently, networks with multiple prediction targets, such as predicting density maps with different resolutions [37] or predicting density maps with various smoothness levels [38] are shown to be effective. As shown in [39], creating an ensemble of predictors can also boost the prediction accuracy. Besides, the attention mechanism shows the effectiveness of reducing the estimation error caused by background noise [40,41]. Incorporating the strength of CNN-based and detection-based solutions to settle the nonuniform distribution of the crowd is also effective [42,3]. The work in [43] proposes a deep structured scale integration network to handle scale variation. Recently, the spatial information is identified to be effective to solve the density variation problem [44]. Leveraging foreground and background mask information [45] is able to improve the robustness and effectiveness of crowd counting . Besides, a novel Bayesian based loss function [46] is proposed to enhance the supervision reliability for crowd counting. In combination with temporal information, Long Short-Term Memory (LSTM) is widely used in video crowd counting [4,47].\n\n\nCrowd Counting by Low Supervision Methods\n\nHowever, the above works mainly focus on the crowd counting with the fullysupervised setting, i.e., all images are labeled with location-level annotations.\n\nRecently, some low supervision-based methods (i.e., semi, weakly or even unsupervised) are proposed with the considerations of reducing the annotation burden. Few works sought other solutions to train the crowd counting models under semisupervised settings [48,49]. They collect abundant unlabeled crowd images as extra images of a whole location-level annotated crowd counting dataset. Besides, they construct a rank loss function on these unlabeled images to achieve a more accurate prediction. Furthermore, the work in [17] proposes a weakly-supervised solution based on the Gaussian process for crowd density estimation. In their work, all samples are annotated with count-level supervision on the training set. A novel crowd counting solution leveraging sparse features is proposed in [50] to train a crowd counting model under an almost unsupervised manner. In their work, most parameters are trained without any labeled data, and only small part parameters are updated with location-level annotated data. More recently, the use of synthetic images is explored to reduce the burden of labor-intensive annotation [51], which shows the feasibility of transforming synthetic images to authentic ones. Then, the synthetic images are labeled with location-level annotations automatically. Moreover, a GAN-based adaptation method to learn from synthetic images and the corresponding free density maps is proposed in the work [52]. Also, the work [53] constructs a domain transfer based framework transferring synthetic images to realistic images to train a crowd counter without any manual label. Those pseudo labels are produced from a Gaussian-prior Reconstruction. However, the count-level weakly-supervised crowd counting, i.e., learning a counting model by using count-level supervision, is still not well-explored. This paper identifies its potential value in reducing the annotation workload and proposes a new method to improve the existing methods. At the methodology level, our work is also related to a recently proposed semi-supervised learning method [54] which is primarily developed for Natural Language Processing (NLP) applications. Inspired by the nature of crowd counting, our construction of auxiliary tasks is significantly diverse from that in [54].\n\n\nMethodology\n\nIn this section, we first introduce the the traditional location-level fully-supervised crowd counting setting in Subsection 3.1. Then, Subsection 3.2 describes the acquisition of the count-level annotations and the weakly-supervised crowd counting setting addressed in this paper. In Subsection 3.3 we discuss a naive solution to weakly-supervised crowd counting. Later, Subsection 3.4 elaborates the details of the proposed Multiple Auxiliary Tasks Training method. Last, Subsection 3.5 describes the details of the introduced asymmetry training strategy.\n\n\nLocation-level Fully-supervised Crowd Counting\n\nAs described above, most recent crowd counting methods [33,55] require labeling a dot on each object-of-interest to provide the location-level annotation. With such full supervision, the crowd counting can be formulated as a density map regression problem. The density map is usually obtained by convolving the location-level annotation with a Gaussian kernel, which is expressed as:\nD(x) = xp G( x \u2212 x p 2 \u03c3 2 ),(1)\nwhere x \u2208 R 2 denotes the coordinate of a pixel and x p denotes the p-th annotated point.\nG( x\u2212xp 2 \u03c3 2\n) indicates a Gaussian kernel with x p as the mean vector and \u03c3 2 as the empirically chosen variance term. Note that since G( x\u2212xp 2 \u03c3 2 )dx = 1, the integral of D(x) equals the total object count. Thus as long as the density map is accurately estimated by the learned model, the object count can be readily obtained by taking the integral of the estimated density map [5].\n\nThe Mean Square Error (MSE) loss is widely used in training a density map predictor, which is formulated as:\nL M SE = m\u2208A (F (x|I m , \u03bb) \u2212 D m (x)) 2 dx,(2)\nwhere F (x|I m , \u03bb) denotes the density value estimation at point x for the m-th image given by the model \u03bb, which is a deep neural network in our case. For the sake of simplicity, we use F (x) to denote the estimated density map in the following part.\n\n\nCount-level Weak Supervision and Problem Formulation\n\nAs described in Section 1, collecting location-level annotations can be laborintensive especially for highly crowd scenes. On the contrary, the total object count can be obtained easily in many practical scenarios. This subsection reviews several methods to obtain such count-level annotation: (1) Taking images of the same set of objects from different viewpoints or shuffling the object arrangement before taking the image again. This method can generate many images with identical object count. Once the total count of one image, i.e., a \"seed image\", is known, \n\n\nVariables\n\nDescription I m \u2208 R 2 the image with location-level annotation I n \u2208 R 2 the image with count-level annotation D m \u2208 R 2 the ground truth density map for I m c n \u2208 R the ground truth total count for I n F 0 (x) \u2208 R 2 the predicted density map from primary branch F k (x) \u2208 R 2 the predicted density map from the k-th auxiliary branch f b the feature extractor backbone g 0 the primary branch g 1 , ..., g k the auxiliary branch L the loss function of model the counts for the remaining images are known. (2) Similar to (1), but each time removing or adding a small amount (which is easy to count) of objects before re-taking the image. This approach can effortlessly generate a sequence of images with diverse object counts. (3) the total number of objects can be found out via other measurements, e.g., derived from the total weight of objects or estimated through other sensors at the offline stage. Although count-level annotation is cheap to obtain, its supervision signal is weaker in comparison with the traditional location-level supervision. To achieve appropriate performance, this paper considers a semi-supervised alike setting.\n\nOur setting: We first assume there is a small set of images A F with a locationlevel annotation, where image I m in A F is annotated with the ground-truth density map D m . Then we also assume there is a large set of images A W with a count-level annotation, where image I n in A W is annotated with the ground-truth object count c n . In the scenario of taking multiple images of the same set of objects, a small number of location-level annotations can be obtained as a byproduct of counting the objects in the \"seed image\", which makes this assumption more realistic in practical applications. For ease of reading, we summarize a set of important notations used in this paper as Table 1.\n\n\nNaive Solution for Weakly-supervised Crowd Counting\n\nFor model training in the weakly-supervised setting, it is straightforward to extend the traditional density map estimation framework by imposing the integral the estimated density map being close to the ground-truth object count for count-level annotated images. The overall training objective function for count-level weakly-supervised crowd counting can be written as:\nL Base = L M SE + \u03b1L count = m\u2208A F (F (x) \u2212 D m (x)) 2 dx + \u03b1 n\u2208A W | F (x)dx \u2212 c n |(3)\nAt first glance, this simple solution seems to be sufficient for training the crowd counting network. However, we find that it leads to unsatisfying results in practice. Its major weakness is that the count loss, i.e., the second term of Eq.3, holds a very weak constraint on the generated density map. The network can easily achieve low count-loss by producing less desirable density map which does not encode the accurate object locations and results in poor generalization performance.\n\n\nMultiple Auxiliary Tasks Training (MATT)\n\nTo overcome the limitation of the naive solution described in Subsection 3.3, we introduce more regularization terms to restrict the freedom of the estimated density maps on the weakly-supervised images. Our idea is to construct multiple auxiliary branches in addition to the primary branch which produces the density map used during the test stage. Both the primary branch and the auxiliary branches generate density maps. Those density maps are supposed to be diverse but equivalent, that is, all the density maps could be derived from the same dot-map (i.e., location-level annotation) but with different smoothness levels.\n\nTo this end, we introduce auxiliary tasks in the form of auxiliary losses to each branch. Specifically, our losses reflect two requirements: (1) The integral of the predicted density map should be close to the ground-truth object count.\n\n(2) The predicted density maps from an auxiliary branch should be consistent with the predicted density map from the primary branch. Rather than directly minimizing the discrepancy of the predicted density maps between primary and auxiliary branches, we introduce an additional step to convolve the prediction from each auxiliary branch with a different predefined Gaussian kernel, and require convolution result to be close to the density map generated from the primary branch. The introduction of this extra convolution operation avoids setting the identical prediction target for each auxiliary branch. It can prevent the auxiliary branch from collapsing into the identical regressor as the primary branch. The scheme of our strategy is illustrated in Figure 2. Formally, the whole loss function of our method can be written as:  Figure 2: Overview of the proposed method. For location-level annotated images, we require the predicted density map close to ground-truth. The proposed method also generates multiple auxiliary branches. All branches share a same feature extractor. Once training is done, the auxiliary branches will be discarded and only the feature extractor and the primary branch will be used during testing phase.\nL aux = \u03b2 1 k ((F k * h k )(x) \u2212 F 0 (x)) 2 dx+ \u03b2 2 k | F k (x)dx \u2212 c n | k \u2208 {1, \u00b7 \u00b7 \u00b7 , K},(4)\nwhere K is the total number of auxiliary branches. h k is a predefined convolution kernel for the k-th auxiliary branch. In our implementation, we constructed those convolution kernels by using a Gaussian function with relatively low variance (\u03c3 = 1) and different kernel sizes, e.g., 3 \u00d7 3, 5 \u00d7 5, 3 \u00d7 5 and 5 \u00d7 3 for h 1 to h 4 , respectively. Note that the low variance Gaussian function will have significant values outside the kernel boundaries, so each h k is essentially a truncated Gaussian kernel. We normalize the values inside a kernel to make their integral equal to 1. This normalization ensures that the ground-truth total count of each branch is equal to the total count of the main branch. F k is the predicted density map from the k-th auxiliary branch g k . F 0 (x) is the density map estimated from the primary branch g 0 . All the branches share the same feature extractor f b . Our method can be intuitively understood as follows: a crowd counting network can be decomposed into a feature extractor and a density map regressor. The feature extractor produces features that implicitly encode the object location information and the density map regressor converts those features into a realization of density map (recall that the density map is derived from the location annotation and it is not unique). Our method essentially requires the extracted features to support multiple realizations of density maps and their relationships. These requirements can enforce the feature extractor to encode location information more accurately since by doing so the features can be easily converted to the different realization of density maps.\n\n\nAsymmetry Training\n\nDirectly training the model with the loss in Eq. 4 can result in less satisfying results. The reason is that the primary branch and the auxiliary branches can co-adapt with each other to produce low consistency loss (i.e., the first term in Eq. 4). To solve this issue, we devise an asymmetry training strategy that ensures the training signal of the primary branch only comes from the credible sources, i.e. the ground-truth density map from fully-supervised data or the ground-truth count from weakly-supervised data. In other words, when optimizing Eq. 4, the parameters of the primary branch is fixed due to predicted density maps in other branches are not very reliable. F 0 (x) is treated as a constant in Eq. 4, which means the gradient will not back-propagate through the primary branch g 0 . This treatment essentially imposes such an asymmetry learning strategy: the auxiliary branches will learn from the primary branch but not vice versa. This is because we want the auxiliary tasks only to assist training the feature extractor but not affecting the primary task training. We postulate that the density map regressor is sensitive to the input training signal, but the feature extractor is insensitive to it. Feature extractor training is more robust to the imprecise training signal since its output can be further adapted by the regressor. On this basis, the asymmetry training avoids the gradient of the auxiliary branches from flowing into the primary branch. The overall auxiliary training objective function for count-level weaklysupervised crowd counting in our method can be formulated as:\nL M AT T = L M SE + \u03b1L count + L aux = m\u2208A F (F 0 (x) \u2212 D m (x)) 2 dx + \u03b1 n\u2208A W | F 0 (x)dx \u2212 c n | + \u03b2 1 k ((F k * h k )(x) \u2212 F 0 (x)) 2 dx + \u03b2 2 k | F k (x)dx \u2212 c n | k \u2208 {1, \u00b7 \u00b7 \u00b7 , K}.(5)\nIn practice, we use a stochastic gradient descent algorithm to train the model and the pseudo-code of our methods is shown in Algorithm 1. Note that the training procedure for a count-level annotated sample essentially consists of two steps: the update of the primary branch and the update of the auxiliary branches.\n\n\nMulti-Shot Crowd Counting (MSCC) dataset\n\nThis work advocates the advantage of using count-level annotation and proposes a new way for weakly-supervised crowd counting. To verify the effectiveness of the proposed method, we certainly can evaluate our method on the existing crowd counting datasets and modify their evaluation protocol to suit the weakly-supervised setting. However, the total counts of the object in those datasets are not collected in the way as discussed in Subsection 3.2. One can integral ground-truth density map to obtain total object count in those datasets. One may wonder whether the samples collected in a way in Subsection 3.2 can benefit for network training and whether the proposed method can be effective in using those samples. To address above issues, we collect a new dataset by following one of the sample collection strategies discussed in Section 3.2. Specifically, we collect crowd object images for 15 different categories (details see Figure 3), including Marble, Biscuit, two types of Candies (i.e., Candy1 and Candy2), three types of Capacitors (i.e., Cap1, Cap2 and Cap3), Capsule, M&M beans (i.e., MM), Go, Cherry Tomato (i.e., Tomato), Longan, Pin-Header and two types of Resistances (i.e., Res1 and Res2). For each category, we choose a \"seed image\" and perform the location-level annotation on it. This will give the total object count for the \"seed image\". Remove Figure 4: The procedure of generating the MSCC dataset. We obtain count-level annotations from one \"seed image\", once we know the total count of \"seed image\". When we shuffle the arrangement of the objects, i.e., add or remove a small amount of objects before retaking, we can infer the amount of objects of other images. We collect more images by taking images from different viewpoints, shuffling the object layout and changing the background before re-taking the image. To collect various quantity levels in categories, we sequentially remove or add a small amount of object. With those operations shown in Figure 4, we can quickly gather a large number of images and infer their object count from the count of the \"seed image\". In our dataset, we collect 200 images with ten quantity levels per category. Since the images are collected by taking multiple shots at the same pile of objects, we name our dataset as Multi-Shot Crowd Counting (MSCC) dataset.\n\nFor each object category, we use 170 images as the training set, consisting of 169 images with the count-level annotation (i.e., weakly-supervised images) and one image with the location-level annotation (i.e., fully-supervised image). The other 30 images with the count-level annotation are used as the validation set. To build the test set, we manually annotated 100 additional images with a total count value. The total count value in the test set is designed to be more diverse than that in the training set. This enables us to test if our trained model can generalize to images with a different object count.\n\n\nExperimental Results\n\n\nExperimental Setting\n\nIn this section, we demonstrate the effectiveness of the proposed weaklysupervised crowd counting method by conducting experiments on three traditional benchmark datasets: ShanghaiTech [2], UCF CC 50 [56] and WorldExpo'10 [1] datasets, along with the proposed MSCC dataset. Different from the evaluation protocol of other location-level fully-supervised crowd counting methods, we have modified their evaluation protocols to suit for our count-level weaklysupervised setting. For each dataset, we divide the original training images into a weakly-supervised part (i.e., count-level annotation ) and fully-supervised part (i.e., location-level annotation). Following the existing works [1,35], we use the Mean Absolute Error (MAE) and Mean Square Error (MSE) as the evaluation metrics for the above three traditional crowd counting datasets.\n\nMeanwhile, two measures are used as the evaluation metrics for the proposed MSCC dataset, i.e., the MAE and the Relative Error Ratio (RER) which is derived from MAE by dividing the total object count. The RER is an indicator of relative error and it is meaningful because we usually allow larger estimation error for a more crowded scene and vice versa. For example, incorrectly predicting 10 objects for an image with 1,000 objects is more favorable than incorrectly predicting 10 objects for an image with only 20 objects. These three metrics are defined as follows:\nM AE = 1 N N i=1 |P re i \u2212 GT i | ,(6)M SE = 1 N N i=1 |P re i \u2212 GT i | 2 ,(7)RER = 1 N N i=1 |P re i \u2212 GT i | GT i ,(8)\nwhere N is the number of test images, P re i is the predicted object count of image I i and GT i is the corresponding ground-truth total count. The lower result of these metrics means the better performance. Three methods compared in our experiment are:\n\n\u2022 Baseline1 only utilizes the part of images with location-level annotation to train the feature extractor f b and primary branch g 0 directly. In our proposed settings, only a small set of images are provided with location-level annotation, it can be seen as a \"reduced\" training set sampled from the original training set.\n\n\u2022 Baseline2 the straightforward solution to incorporate the count-level weak supervision in Subsection 3.3. In our network, this method is equivalent to only training a single branch: the feature extractor f b and the primary branch g 0 . Different from the \"Baseline1\" method, it jointly considers a small mount of location-level annotations and a large mount of count-level annotations. Straightly we require training image I m with location-level annotation generate density map close to ground-truth density map D m . For training image I n with count-level annotation, we require the integral of predicted density map close to ground-truth total count c n .\n\n\u2022 MATT is the proposed method that leverages multiple auxiliary tasks for model training. Notice that \"Baseline1\" and \"Baseline2\" only leverage single branch: the feature extractor f b and primary branch g 0 . For the sake of train a more robust f b , we propose the MATT method. When the model meets image I j with count-level weak supervision, firstly we introduce four auxiliary branches to predict density map, then according to Algorithm 1 we update the corresponding parameters. The auxiliary branch can promote the training of f b .\n\nThe differences of the above three methods are demonstrated in Table 2. Note that, during testing we discard g 1 ,...,g k and only use f b and g 0 for the final density maps.  \n\n\nImplementation Details\n\nOur feature extractor is realized based on the CSRNet [35], which is one of the state-of-the-art solutions for crowd counting. Certainly, the proposed method can be extended to other feature extractor architectures. Specifically, the layers to the third-last layer of CSRNet are used as the backbone which produces a feature map with 256 channels. One primary branch and four auxiliary branches are used in our implementation. Each branch contains three convolutional layers, the kernel size of each convolutional layer is 3 \u00d7 3 and the channel number reduced from 256 to 128, 64 and finally 1. For four auxiliary branches, different convolutional kernels h k are used to construct the loss in Eq. 4. We choose them as predefined Gaussian kernels with different covariance matrices (see details in Subsection 3.4). The PyTorch [57] is applied to implement the proposed method. Adam [58] is used as the optimizer. All the hyper-parameters are chosen by using the validation set. We will release the source code and the MSCC dataset upon the acceptance of this work.\n\n\nDatasets and Results\n\nIn this subsection, comprehensive experiments are conducted to evaluate the performance of the proposed method. Please note that, our method only uses a few training images with location-level annotation and a large amount of training images with count-level annotation, which is only comparable to methods in the  \"weakly\" part. However we also list the fully-supervised crowd counting state-ofthe-art results in the \"fully\" part just for reference in Table 3-5.\n\n\nEvaluation on the ShanghaiTech dataset\n\nThe ShanghaiTech dataset is a large-scale crowd counting dataset containing 1,198 images with 330,165 annotated heads [2], which is divided into two parts. Part A contains 482 images with 241,667 annotated heads, and 300 images are used for training while the rest for testing. In our experiment, we only use the count-level annotation for 270 images from the training set and the object locationlevel annotation from the remaining 30 images. Part B has 716 images with 88,498 annotated heads taken from street scenes in the Shanghai city, with 400 images for training. Similar to Part A, we only use count-level weak annotation for 380 images from the training set and use location-level annotation for the remaining 20 images. This modification suits for the evaluation in the weakly-supervised setting.\n\nThe experimental results are shown in Table 3. In the traditional fully-supervised setting, works based on CSRNet [35] leverage attention mechanism [40,59] and context information [60] can regress more accurate density maps and obtain lower MAE in the test stage. While our method is comparable to those methods. In addition, in the weak supervision setting, if we train our network only with the location-level annotations (i.e. \"Baseline1\"). The performance is not satisfying, and it only achieves a MAE of 106.5 on Part A and a MAE of 16.4 on Part B. From the results reported in CSRNet [35], if the location-level annotations are used for all the samples, the performance of the same network will achieve a MAE of 68.2 and 10.6 on Part A and Part B respectively. Thus, reducing the number of location-level annotations incurs a significant performance drop due to a lack of location-level annotated images. By using the naive method of incorporating count-level supervision, the performance is significantly improved. In Part A, the MAE is reduced to 89.3. The proposed method can also make a significant improvement over the \"Baseline1\", and the improvement is much larger than the naive method. Comparing with \"Baseline2\", we achieve an improvement by 9.2 heads in the terms of MAE on Part A. On Part B, our method also reduces the MAE by around 1.8 heads. The MAE improvement is smaller because Part B contains fewer crowd scenes, and the MAE differences between various methods tend to be smaller. It is easy to calculate that each image in Part A contains 501.4 persons averagely as four times as Part B nearly. In other words, Part A is more challenging than Part B. This is also a trend observed in the works in the location-level supervised setting mentioned in Table 3. In Figure 5, we also visualize the estimated density maps of different methods. It is interesting to find that the density map generated by the proposed method is more similar to the ground-truth density map. In comparison, the density map generated by the naive method tends to exhibit large homogeneous regions. \n\n\nEvaluation on the UCF CC 50 Dataset\n\nThe UCF CC 50 dataset [56] contains only 50 black and white images which are considered to be challenging due to the high object density in the images. Its count value varies from 94 to 4,543. In our weakly-supervised setting, 5 images are with location-level annotation and the rest 45 images are with only count-level annotation. 20 patches with half image size are cropped from each image, and 5fold cross-validation is used to evaluate the proposed method [56]. 4 location-level annotated images with ground-truth location-level supervision and 36 count-level annotated images with ground-truth count-level supervision are randomly selected to form the training set.\n\nThe performance of the compared methods is shown in Table 4. As seen, with a small amount of location-level supervised samples, the crowd counting performance is poor. With the count-level supervision, even the naive method can reduce the MAE from 461.4 to 405. Our proposed method again shows a significant improvement over the naive method. It improves the MAE almost by 50 heads comparing with the naive method. This clearly demonstrates the advantage of the proposed method.  [1]. 3,380 frames are used for training while the remaining 600 frames for testing. In our setting, 5% frames (169), in the training set are used as the location-level annotated images while the remaining 95% frames (3,211) are used as count-level annotated images.\n\nThe results are shown in Table 5. From the results, We can see that our proposed method is superior to the baselines. The proposed method achieve the highest average MAE, improving more than 2.4 heads over the \"Baseline1\". For the performance in each section, our method attains 4 lowest MAE among all 5 scenarios. In comparison, the \"Baseline2\" does not improve too much over the \"Baseline1\". This indicates an appropriate learning method can play an important role when using the count-level annotations. Table 5: The performance comparison in terms of MAE on the WorldExpo'10 dataset. It can be seen that the performance of the proposed method MATT is close to the performance of methods that all training images using location-level supervision. The best results are in underline and bold font for \"Fully\" and \"Weakly\" methods respectively.\n\n\nMethod\n\nSce. \n\n\nEvaluation on the MSCC Dataset\n\nFinally, we compare our method on the proposed MSCC dataset. We strictly follow the experimental protocol in Section 4. The results are shown in Table 6. As seen, for all categories, the proposed method achieves the best MAE and RER, especially for categories with irregularly shaped objects such as pin headers. For this category, the proposed method reduces MAE from 27.9 to 5.8 and delivers 87.8% lower RER. On average, the proposed method achieves a MAE of 6.8 and a RER of 15.7. It is 63.8% lower MAE and 76.4% lower RER than a MAE of 18.8 and a RER of 66.5 from the \"Baseline1\" method. Note that the object count annotation on the MSCC dataset is collected with less effort. The better performance of the \"Baseline2\" and \"MATT\" comparing with the \"Baseline1\" clearly demonstrates that using the effortlessly collected count-level weakly supervised images is beneficial for training a crowd counting network. Also, the consistently superior performance of the proposed approach over the naive method demonstrates that our method is useful in practice.\n\n\nAblation Study 5.4.1. Other Alternative Auxiliary Loss Functions\n\nThe auxiliary loss in this paper involves two terms for count-level annotated images: a count loss term and a density map consistency loss. To examine their impacts, we conduct an ablation study on the ShanghaiTech Part A dataset by constructing two alternative variations of the proposed method. The first variation only uses a count loss. The second variation does not use the total count loss but only uses the first term in Eq. 4. The asymmetry training strategy is also used in those variations.\n\nThe performance comparison is shown in Table 7. As seen, the proposed method achieves better performance than the first variation which can be seen as a multi-branch variation of \"Baseline2\". In comparison, the second variation achieves better performance and is even comparable to the performance of MATT. This suggests that MSE loss is the most effective part of the total loss function in the proposed method. Combining both loss terms can achieve the best performance.  \n\n\nVarying the Number of Auxiliary Branches\n\nIn the proposed method, we use four auxiliary branches for the simplicity of implementation. It would be interesting to see if using more or fewer branches will result in better or poor performance. The experimental results on Shanghai Tech Part A dataset are presented in Figure 6. It can be observed that if only two auxiliary branches are used, the performance will not be better than the naive method -both methods achieve MAE around 90.8 heads. However, with the increase in the number of branches, the estimation error is reduced. The experiment results show that the best performance is achieved when using 4 to 6 auxiliary branches. From those results, it is clear to see the importance of using more branches to maintain the diversity of the auxiliary supervision signals.\n\n\nThe Importance of the Asymmetry Training\n\nOur work uses an asymmetry training strategy to learn the auxiliary tasks, and the auxiliary tasks only learn from the primary task while the primary task will not learn from the auxiliary tasks. In our strategy, the primary task is used for all images and learn from the discrepancy between predicted density map F 0 (x) and ground-truth density map D m (x) or ground-truth total count c n .\n\nThe auxiliary task works only when taking count-level annotated images I n . Learning from the discrepancy between predicted density map F k (x) and (1) corresponding density map F 0 (x) generated at the primary branch, (2) groundtruth total count c n . We postulate that this is important because otherwise, the auxiliary supervisions signals will flow into the primary branch. Because the auxiliary branch will fit a less accurate density map -the predicted density map from the primary task, the supervisions signals from the auxiliary task is inevitably noisy. Thus, it might be harmful to allow its gradient to update the primary branch. In this study, we conduct an experiment to verify our postulation on the ShanghaiTech Part A dataset. As shown in Table 8, after removing the asymmetry training strategy, we only obtained the MAE of 89.0 and MSE of 140.6 on the ShanghaiTech Part A dataset. This is much worse than its original version and this validates our asymmetry training postulation. \n\n\nMATT for Fully Annotated Data\n\nThe proposed method is in a spirit similar to the multi-task learning and one may wonder the good performance of our method essentially comes from multi-task learning. In other words, our method may lead to better baseline performance even with the fully-annotated part of training data. To verify this, we apply the proposed MATT method on the fully annotated part of the training set on the ShanghaiTech Part A dataset, and report the results in Table 9. Interestingly, using MATT for a location-level annotated image only performs slight better than \"Baseline1\", which only includes the primary branch (i.e., single task learning). This indicates that the superior performance of the proposed method is largely originated from its ability to better exploiting the information provided in the count-level annotated image not only the multi-task learning. Table 9: Results of using proposed method for training based on the fully annotated part of data on the ShanghaiTech Part A dataset. The best results are in bold font.\n\n\nMethod\n\nMAE MSE MATT on Fully 104.6 162.7\n\nBaseline1 106.5 167.4\n\n\nConclusions\n\nIn this paper, we point out that the count-level annotations can be easily obtained. We also identify the limitation of the straightforward method to weakly supervised counting and propose a novel Multiple Auxiliary Task Training (MATT) scheme to learn a better crowd counting model. To verify the benefits of countlevel weakly supervised learning, we also introduce a new dataset. By performing experiments on the newly introduced dataset and the traditional publicly available crowd counting datasets, we demonstrate that the proposed method is superior to the straightforward weakly supervised crowd counting method and can leverage the count-level weak supervision to build a better crowd counting model.\n\n( b )\nbProposed weak supervision setting.\n\nFigure 1 :\n1(a) Traditional full supervision setting. All images are labeled with location-level annotations. (b) Proposed weak supervision setting. Only a small number of images are annotated with location-level annotations, while most images are annotated with the total count.\n\nAlgorithm 1 :\n1Multiple auxiliary task training. Input: Current mini-batch B = {I m , D m } {I n , c n }, feature extractor backbone f b , primary branch g 0 , and auxiliary branches {g k }. Output: Updated f b , g 0 and {g k }. 1 for s in B do 2 if s is labeled with the location-level annotations (density map) D m then 3Use L M SE = (F 0 (x) \u2212 D m (x)) 2 dx and back-propagate gradients to f b and g labeled with count-level annotations (total count) c n , calculate the density map estimation F 0 from the primary branch6 Use L count = \u03b1| F 0 (x)dx \u2212 c n | and update f b and g 0 .7 Frozen the parameters in F 0 and use L aux in Eq. 4 to update f b and {g k }.\n\nFigure 3 :\n3Typical images of the 15 categories from the MSCC dataset.\n\n\nf b and g 0 Auxiliary loss f b and g 1 ,...,g k\n\nFigure 5 :\n5A comparison of the density map generated by different methods of the proposed method on the ShanghaiTech Part A dataset.\n\nFigure 6 :\n6Impact of different number of auxiliary branches. Evaluated in terms of MAE on the ShanghaiTech Part A dataset.\n\nTable 1 :\n1The notation of variables in this paper.\n\nTable 2 :\n2The comparison between three methods.Train method Annotated type Loss function Parameters in model \n\n\n\nTable 3 :\n3The comparison on the ShanghaiTech dataset. The best results are in underline and bold font for \"Fully\" and \"Weakly\" methods respectively.Method \nPart A \nPart B \nMAE MSE MAE MSE \n\nFully \n\nMCNN [2] \n110.2 173.2 26.4 41.3 \nSwitch-CNN [32] 90.4 135.0 21.6 33.4 \nACSCP [61] \n75.7 102.7 17.2 27.4 \nCP-CNN [33] \n73.6 106.4 20.1 30.1 \nCSRNet [35] \n68.2 115.0 10.6 16.0 \nSANet [62] \n67.0 104.5 8.4 \n13.6 \nADCrowdNet [40] 63.2 98.9 \n8.2 \n15.7 \nCAN [60] \n62.3 \n100 \n7.8 \n12.2 \nSFANet [36] \n59.8 99.3 \n6.9 \n10.9 \nW-NET [59] \n59.5 \n97.3 \n6.9 \n10.3 \n\nWeakly \n\nBaseline1 \n106.5 167.4 16.4 25.7 \nBaseline2 \n89.3 135.8 13.5 19.9 \nMATT \n80.1 129.4 11.7 17.5 \n\n\n\nTable 4 :\n4The performance comparison on the UCFF CC 50 dataset. The best results are in underline and bold font for \"Fully\" and \"Weakly\" methods respectively.Method \nMAE MSE \n\nFully \n\nMCNN [2] \n377.6 509.1 \nSwitch-CNN [32] 318.1 439.2 \nCP-CNN [33] \n295.8 320.9 \nACSCP [61] \n291.0 404.6 \nCSRNet [35] \n266.1 397.5 \nADCrowdNet [40] 257.1 363.5 \nSANet [62] \n258.4 334.9 \nSFANet [36] \n219.6 316.2 \nCAN [60] \n212.2 243.7 \nW-Net[59] \n201.9 309.2 \n\nWeakly \n\nBaseline1 \n461.4 779.9 \nBaseline2 \n405.0 586.2 \nMATT \n355.0 550.2 \n\n5.3.3. Evaluation on the WorldExpo'10 Dataset \nThis dataset contains 3,980 uniformly sampled frames from video sequences \ncaptured by 108 surveillance cameras from Shanghai 2010 WorldExpo \n\nTable 6 :\n6The performance comparison for different methods on the MSCC dataset. The best results are in bold font.Marble \nBiscuit \nCandy1 \nCandy2 \nCap1 \nBaseline1 \nMAE\\RER \n\n8.7\\26.3 11.4\\41.4 16.2\\86.7 26.7\\52.3 34.5\\180.8 \nBaseline2 \n6.2\\22.2 6.0\\12.3 \n12.0\\28.7 \n8.7\\17.0 \n9.3\\28.1 \nMATT \n4.8\\14.3 \n3.9\\9.4 \n8.4\\19.7 \n5.4\\11.7 \n7.7\\23.3 \nCap2 \nCap3 \nCapsule \nMM \nGo \nBaseline1 \nMAE\\RER \n\n21.6\\86.5 23.9\\86.7 19.9\\119.1 8.1\\30.9 \n9.6\\23.3 \nBaseline2 \n7.7\\16.9 10.6\\22.7 11.1\\20.4 14.9\\33.2 8.8\\18.7 \nMATT \n6.4\\14.3 7.2\\17.4 \n4.3\\9.8 \n5.4\\12.6 \n8.3\\18.4 \nTomato \nLongan Pin-Header \nRes1 \nRes2 \nAvg. \nBaseline1 \nMAE\\RER \n\n13.8\\39.7 5.4\\16.0 27.9\\103.8 20.0\\45.4 34.5\\59.8 18.8\\66.5 \nBaseline2 \n11.3\\35.2 10.8\\27.2 15.1\\26.7 23.0\\35.9 15.0\\31.6 11.4\\25.1 \nMATT \n7.3\\21.9 4.0\\10.6 \n5.8\\12.7 \n15.8\\24.2 7.2\\14.6 \n6.8\\15.7 \n\n\n\nTable 7 :\n7Results of alternative constructions of the auxiliary loss. Evaluated on the ShanghaiTech Part A dataset. The best results are in bold font.Method \nMAE MSE \nonly count loss 85.7 138.0 \nonly MSE loss 83.3 132.6 \nMATT \n80.1 129.4 \n\n2 \n3 \n4 \n5 \n6 \nNumber of Auxiliary Branches \n\n70 \n\n75 \n\n80 \n\n85 \n\n90 \n\n95 \n\nMAE \n\n77.89 \n\n81.42 \n81.69 \n\n84.14 \n\n90.81 \n\n\n\nTable 8 :\n8Impact of the asymmetry training strategy. Evaluated on the ShanghaiTech Part A dataset. The best results are in bold font.Method \nMAE MSE \nsymmetry 89.0 140.6 \nasymmetry 80.1 129.4 \n\n\n\nCross-scene crowd counting via deep convolutional neural networks. C Zhang, H Li, X Wang, X Yang, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). C. Zhang, H. Li, X. Wang, X. Yang, Cross-scene crowd counting via deep convolutional neural networks, in: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 833-841 (2015).\n\nSingle-image crowd counting via multi-column convolutional neural network. Y Zhang, D Zhou, S Chen, S Gao, Y Ma, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Y. Zhang, D. Zhou, S. Chen, S. Gao, Y. Ma, Single-image crowd counting via multi-column convolutional neural network, in: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 589-597 (2016).\n\nDynamic region division for adaptive learning pedestrian counting. G He, Z Ma, B Huang, B Sheng, Y Yuan, IEEE International Conference on Multimedia and Expo (ICME). G. He, Z. Ma, B. Huang, B. Sheng, Y. Yuan, Dynamic region division for adaptive learning pedestrian counting, in: IEEE International Conference on Multimedia and Expo (ICME), 2019, pp. 1120-1125 (2019).\n\nEnhanced 3d convolutional networks for crowd counting. Z Zou, H Shao, X Q Qu, W Wei, P Zhou, British Machine Vision Conference (BMVC). 2019Z. Zou, H. Shao, X. Q. Qu, W. Wei, P. Zhou, Enhanced 3d convolutional net- works for crowd counting, in: British Machine Vision Conference (BMVC), 2019 (2019).\n\nLearning to count objects in images. V Lempitsky, A Zisserman, Advances in Neural Information Processing Systems (NIPS). V. Lempitsky, A. Zisserman, Learning to count objects in images, in: Ad- vances in Neural Information Processing Systems (NIPS), 2010, pp. 1324- 1332 (2010).\n\nR Guerrero-G\u00f3mez-Olmedo, B Torre-Jim\u00e9nez, R L\u00f3pez-Sastre, S Maldonado-Basc\u00f3n, D Onoro-Rubio, Iberian Conference on Pattern Recognition and Image Analysis (IbPRIA). Extremely overlapping vehicle countingR. Guerrero-G\u00f3mez-Olmedo, B. Torre-Jim\u00e9nez, R. L\u00f3pez-Sastre, S. Maldonado-Basc\u00f3n, D. Onoro-Rubio, Extremely overlapping vehicle counting, in: Iberian Conference on Pattern Recognition and Image Analysis (IbPRIA), 2015, pp. 423-431 (2015).\n\nMicroscopy cell counting and detection with fully convolutional regression networks. W Xie, J A Noble, A Zisserman, Computer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization. 63W. Xie, J. A. Noble, A. Zisserman, Microscopy cell counting and detection with fully convolutional regression networks, Computer Methods in Biome- chanics and Biomedical Engineering: Imaging & Visualization 6 (3) (2018) 283-292 (2018).\n\nLearning intelligent dialogs for bounding box annotation. K Konyushkova, J Uijlings, C H Lampert, V Ferrari, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). K. Konyushkova, J. Uijlings, C. H. Lampert, V. Ferrari, Learning intelligent dialogs for bounding box annotation, in: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 9175-9184 (2018).\n\nFrom image-level to pixel-level labeling with convolutional networks. P O Pinheiro, R Collobert, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). P. O. Pinheiro, R. Collobert, From image-level to pixel-level labeling with convolutional networks, in: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 1713-1721 (2015).\n\nPrivacy preserving crowd monitoring: Counting people without people models or tracking. A B Chan, Z.-S J Liang, N Vasconcelos, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). A. B. Chan, Z.-S. J. Liang, N. Vasconcelos, Privacy preserving crowd monitor- ing: Counting people without people models or tracking, in: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2008, pp. 1-7 (2008).\n\nHistograms of oriented gradients for human detection. N Dalal, B Triggs, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). N. Dalal, B. Triggs, Histograms of oriented gradients for human detection, in: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2005, pp. 886-893 (2005).\n\nRobust real-time face detection. P Viola, M J Jones, International Journal of Computer Vision (IJCV). 572P. Viola, M. J. Jones, Robust real-time face detection, International Journal of Computer Vision (IJCV) 57 (2) (2004) 137-154 (2004).\n\nCounting people in the crowd using a generic head detector. V B Subburaman, A Descamps, C Carincotte, IEEE International Conference on Advanced Video and Signal-Based Surveillance (AVSS). V. B. Subburaman, A. Descamps, C. Carincotte, Counting people in the crowd using a generic head detector, in: IEEE International Conference on Advanced Video and Signal-Based Surveillance (AVSS), 2012, pp. 470-475 (2012).\n\nSmall instance detection by integer programming on object density maps. Z Ma, L Yu, A B Chan, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Z. Ma, L. Yu, A. B. Chan, Small instance detection by integer programming on object density maps, in: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 3689-3697 (2015).\n\nFeature mining for localised crowd counting. K Chen, C C Loy, S Gong, T Xiang, British Machine Vision Conference (BMVC). 2012K. Chen, C. C. Loy, S. Gong, T. Xiang, Feature mining for localised crowd counting., in: British Machine Vision Conference (BMVC), 2012 (2012).\n\nBayesian poisson regression for crowd counting. A B Chan, N Vasconcelos, IEEE International Conference on Computer Vision (ICCV). 2010A. B. Chan, N. Vasconcelos, Bayesian poisson regression for crowd counting, in: IEEE International Conference on Computer Vision (ICCV), 2010 (2010).\n\nGaussian process density counting from weak supervision. M Borstel, M Kandemir, P Schmidt, M K Rao, K Rajamani, F A Hamprecht, European Conference on Computer Vision (ECCV). M. von Borstel, M. Kandemir, P. Schmidt, M. K. Rao, K. Rajamani, F. A. Hamprecht, Gaussian process density counting from weak supervision, in: European Conference on Computer Vision (ECCV), 2016, pp. 365-380 (2016).\n\nLearning to count with regression forest and structured labels. L Fiaschi, U K\u00f6the, R Nair, F A Hamprecht, International Conference on Pattern Recognition (ICPR). L. Fiaschi, U. K\u00f6the, R. Nair, F. A. Hamprecht, Learning to count with regression forest and structured labels, in: International Conference on Pattern Recognition (ICPR), 2012, pp. 2685-2688 (2012).\n\nCount forest: Co-voting uncertain number of targets using random forest for crowd density estimation. V.-Q Pham, T Kozakaya, O Yamaguchi, R Okada, IEEE International Conference on Computer Vision (ICCV). V.-Q. Pham, T. Kozakaya, O. Yamaguchi, R. Okada, Count forest: Co-voting uncertain number of targets using random forest for crowd density estimation, in: IEEE International Conference on Computer Vision (ICCV), 2015, pp. 3253-3261 (2015).\n\nA Krizhevsky, I Sutskever, G E Hinton, Imagenet classification with deep convolutional neural networks. Advances in Neural Information Processing Systems (NIPS)A. Krizhevsky, I. Sutskever, G. E. Hinton, Imagenet classification with deep convolutional neural networks, in: Advances in Neural Information Process- ing Systems (NIPS), 2012, pp. 1097-1105 (2012).\n\nLearning to count with cnn boosting. E Walach, L Wolf, European Conference on Computer Vision (ECCV). E. Walach, L. Wolf, Learning to count with cnn boosting, in: European Conference on Computer Vision (ECCV), 2016, pp. 660-676 (2016).\n\nTowards perspective-free object counting with deep learning. D Onoro-Rubio, R J L\u00f3pez-Sastre, European Conference on Computer Vision (ECCV). D. Onoro-Rubio, R. J. L\u00f3pez-Sastre, Towards perspective-free object counting with deep learning, in: European Conference on Computer Vision (ECCV), 2016, pp. 615-629 (2016).\n\nM Marsden, K Mcguinness, S Little, N E O&apos;connor, International Conference on Computer Vision Theory and Applications. M. Marsden, K. McGuinness, S. Little, N. E. O'Connor, Fully convolutional crowd counting on highly congested scenes, in: International Conference on Computer Vision Theory and Applications (VISAPP), 2017, pp. 27-33 (2017).\n\nFully convolutional networks for semantic segmentation. J Long, E Shelhamer, T Darrell, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). J. Long, E. Shelhamer, T. Darrell, Fully convolutional networks for seman- tic segmentation, in: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 3431-3440 (2015).\n\nSegnet: A deep convolutional encoder-decoder architecture for image segmentation. V Badrinarayanan, A Kendall, R Cipolla, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). 3912V. Badrinarayanan, A. Kendall, R. Cipolla, Segnet: A deep convolutional encoder-decoder architecture for image segmentation, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 39 (12) (2017) 2481- 2495 (2017).\n\nLarge kernel matters-improve semantic segmentation by global convolutional network. C Peng, X Zhang, G Yu, G Luo, J Sun, IEEE Conference on Computer Vision and Pattern Recognition (CVPR. C. Peng, X. Zhang, G. Yu, G. Luo, J. Sun, Large kernel matters-improve semantic segmentation by global convolutional network, in: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 4353-4361 (2017).\n\nCrowd counting using multiple local features. D Ryan, S Denman, C Fookes, S Sridharan, Digital Image Computing: Techniques and Applications (DICTA). D. Ryan, S. Denman, C. Fookes, S. Sridharan, Crowd counting using multiple local features, in: Digital Image Computing: Techniques and Applications (DICTA), 2009, pp. 81-88 (2009).\n\nPedestrian detection: An evaluation of the state of the art. P Dollar, C Wojek, B Schiele, P Perona, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). 344P. Dollar, C. Wojek, B. Schiele, P. Perona, Pedestrian detection: An evaluation of the state of the art, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 34 (4) (2012) 743-761 (2012).\n\nCrowd counting and segmentation in visual surveillance. L Wang, N H C Yung, International Conference on Image Processing. L. Wang, N. H. C. Yung, Crowd counting and segmentation in visual surveil- lance, in: International Conference on Image Processing (ICIP), 2009, pp. 2573-2576 (2009).\n\nK Kang, X Wang, arXiv:1411.4464Fully convolutional neural networks for crowd segmentation. arXiv preprintK. Kang, X. Wang, Fully convolutional neural networks for crowd segmenta- tion, arXiv preprint arXiv:1411.4464 (2014).\n\nBayesian model adaptation for crowd counts. B Liu, N Vasconcelos, IEEE International Conference on Computer Vision (ICCV). B. Liu, N. Vasconcelos, Bayesian model adaptation for crowd counts, in: IEEE International Conference on Computer Vision (ICCV), 2015, pp. 4175- 4183 (2015).\n\nSwitching convolutional neural network for crowd counting. D B Sam, S Surya, R V Babu, IEEE Conference on Computer Vision and Pattern Recognition (CVPR. D. B. Sam, S. Surya, R. V. Babu, Switching convolutional neural network for crowd counting, in: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017, pp. 5744-5752 (2017).\n\nGenerating high-quality crowd density maps using contextual pyramid cnns. V A Sindagi, V M Patel, IEEE International Conference on Computer Vision (ICCV. V. A. Sindagi, V. M. Patel, Generating high-quality crowd density maps using contextual pyramid cnns, in: IEEE International Conference on Computer Vision (ICCV), 2017, pp. 1861-1870 (2017).\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, A Zisserman, International Conference on Learning Representations (ICLR). 2015K. Simonyan, A. Zisserman, Very deep convolutional networks for large-scale image recognition, in: International Conference on Learning Representations (ICLR), 2015 (2015).\n\nCsrnet: Dilated convolutional neural networks for understanding the highly congested scenes. Y Li, X Zhang, D Chen, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Y. Li, X. Zhang, D. Chen, Csrnet: Dilated convolutional neural networks for understanding the highly congested scenes, in: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 1091-1100 (2018).\n\nDual path multi-scale fusion networks with attention for crowd counting. L Zhu, Z Zhao, C Lu, Y Lin, P Yao, T Yao, arXiv:1902.01115arXiv preprintL. Zhu, Z. Zhao, C. Lu, Y. Lin, P. Yao, T. Yao, Dual path multi-scale fusion networks with attention for crowd counting, arXiv preprint arXiv:1902.01115 (2019).\n\nV Ranjan, H Le, M Hoai, European Conference on Computer Vision (ECCV). V. Ranjan, H. Le, M. Hoai, Iterative crowd counting, in: European Conference on Computer Vision (ECCV), 2018, pp. 270-285 (2018).\n\nComposition loss for counting, density map estimation and localization in dense crowds. H Idrees, M Tayyab, K Athrey, D Zhang, S Al-Maadeed, N Rajpoot, M Shah, European Conference on Computer Vision (ECCV). H. Idrees, M. Tayyab, K. Athrey, D. Zhang, S. Al-Maadeed, N. Rajpoot, M. Shah, Composition loss for counting, density map estimation and localiza- tion in dense crowds, in: European Conference on Computer Vision (ECCV), 2018, pp. 532-546 (2018).\n\nSelective ensemble network for accurate crowd density estimation. J Jeong, H Jeong, J Lim, J Choi, S Yun, J Choi, International Conference on Pattern Recognition (ICPR). J. Jeong, H. Jeong, J. Lim, J. Choi, S. Yun, J. Choi, Selective ensemble network for accurate crowd density estimation, in: International Conference on Pattern Recognition (ICPR), 2018, pp. 320-325 (2018).\n\nAdcrowdnet: An attentioninjective deformable convolutional network for crowd understanding. N Liu, Y Long, C Zou, Q Niu, L Pan, H Wu, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2019N. Liu, Y. Long, C. Zou, Q. Niu, L. Pan, H. Wu, Adcrowdnet: An attention- injective deformable convolutional network for crowd understanding, in: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019 (2019).\n\nCrowd counting and density estimation by trellis encoder-decoder networks. X Jiang, Z Xiao, B Zhang, X Zhen, X Cao, D Doermann, L Shao, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). X. Jiang, Z. Xiao, B. Zhang, X. Zhen, X. Cao, D. Doermann, L. Shao, Crowd counting and density estimation by trellis encoder-decoder networks, in: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 6133-6142 (2019).\n\nDecidenet: Counting varying density crowds through attention guided detection and density estimation. J Liu, C Gao, D Meng, A G Hauptmann, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). J. Liu, C. Gao, D. Meng, A. G. Hauptmann, Decidenet: Counting varying density crowds through attention guided detection and density estimation, in: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 9175-9184 (2018).\n\nL Liu, Z Qiu, G Li, S Liu, W Ouyang, L Lin, Crowd counting with deep structured scale integration network. 2019IEEE International Conference on Computer Vision (ICCV)L. Liu, Z. Qiu, G. Li, S. Liu, W. Ouyang, L. Lin, Crowd counting with deep structured scale integration network, in: IEEE International Conference on Computer Vision (ICCV), 2019 (2019).\n\nLearn to scale: Generating multipolar normalized density maps for crowd counting. C Xu, K Qiu, J Fu, S Bai, Y Xu, X Bai, IEEE International Conference on Computer Vision (ICCV). 2019C. Xu, K. Qiu, J. Fu, S. Bai, Y. Xu, X. Bai, Learn to scale: Generating multi- polar normalized density maps for crowd counting, in: IEEE International Conference on Computer Vision (ICCV), 2019 (2019).\n\nMask-aware networks for crowd counting. S Jiang, X Lu, Y Lei, L Liu, IEEE Transactions on Circuits and Systems for Video Technology (CSVT). S. Jiang, X. Lu, Y. Lei, L. Liu, Mask-aware networks for crowd counting, IEEE Transactions on Circuits and Systems for Video Technology (CSVT) (2019).\n\nBayesian loss for crowd count estimation with point supervision. Z Ma, X Wei, X Hong, Y Gong, IEEE International Conference on Computer Vision (ICCV). 2019Z. Ma, X. Wei, X. Hong, Y. Gong, Bayesian loss for crowd count estimation with point supervision, in: IEEE International Conference on Computer Vision (ICCV), 2019 (2019).\n\nLocality-constrained spatial transformer network for video crowd counting. Y Fang, B Zhan, W Cai, S Gao, B Hu, IEEE International Conference on Multimedia and Expo (ICME). Y. Fang, B. Zhan, W. Cai, S. Gao, B. Hu, Locality-constrained spatial trans- former network for video crowd counting, in: IEEE International Conference on Multimedia and Expo (ICME), 2019, pp. 814-819 (2019).\n\nExploiting unlabeled data in cnns by self-supervised learning to rank. X Liu, J Van De Weijer, A D Bagdanov, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). 8X. Liu, J. Van De Weijer, A. D. Bagdanov, Exploiting unlabeled data in cnns by self-supervised learning to rank, IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) 41 (8) (2019) 1862-1878 (2019).\n\nLeveraging unlabeled data for crowd counting by learning to rank. X Liu, J Van De Weijer, A D Bagdanov, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). X. Liu, J. van de Weijer, A. D. Bagdanov, Leveraging unlabeled data for crowd counting by learning to rank, in: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 7661-7669 (2018).\n\nAlmost unsupervised learning for dense crowd counting. D B Sam, N N Sajjan, H Maurya, R V Babu, AAAI Conference on Artificial Intelligence (AAAI). 2019D. B. Sam, N. N. Sajjan, H. Maurya, R. V. Babu, Almost unsupervised learn- ing for dense crowd counting, in: AAAI Conference on Artificial Intelligence (AAAI), 2019 (2019).\n\nLearning from synthetic data for crowd counting in the wild. Q Wang, J Gao, W Lin, Y Yuan, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2019Q. Wang, J. Gao, W. Lin, Y. Yuan, Learning from synthetic data for crowd counting in the wild, in: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019 (2019).\n\nFeature-aware adaptation and structured density alignment for crowd counting in video surveillance. J Gao, Q Wang, Y Yuan, arXiv:1912.03672arXiv preprintJ. Gao, Q. Wang, Y. Yuan, Feature-aware adaptation and structured den- sity alignment for crowd counting in video surveillance, arXiv preprint arXiv:1912.03672 (2019).\n\nDomain-adaptive crowd counting via inter-domain features segregation and gaussian-prior reconstruction. J Gao, T Han, Q Wang, Y Yuan, arXiv:1912.03677arXiv preprintJ. Gao, T. Han, Q. Wang, Y. Yuan, Domain-adaptive crowd counting via inter-domain features segregation and gaussian-prior reconstruction, arXiv preprint arXiv:1912.03677 (2019).\n\nSemi-supervised sequence modeling with cross-view training. K Clark, M.-T Luong, C D Manning, Q V Le, Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language ProcessingK. Clark, M.-T. Luong, C. D. Manning, Q. V. Le, Semi-supervised sequence modeling with cross-view training, in: Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018, pp. 1914-1925 (2018).\n\nY Tian, Y Lei, J Zhang, J Z Wang, arXiv:1811.02805Padnet: Pan-density crowd counting. arXiv preprintY. Tian, Y. Lei, J. Zhang, J. Z. Wang, Padnet: Pan-density crowd counting, arXiv preprint arXiv:1811.02805 (2018).\n\nMulti-source multi-scale counting in extremely dense crowd images. H Idrees, I Saleemi, C Seibert, M Shah, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). H. Idrees, I. Saleemi, C. Seibert, M. Shah, Multi-source multi-scale counting in extremely dense crowd images, in: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013, pp. 2547-2554 (2013).\n\nA Paszke, S Gross, S Chintala, G Chanan, E Yang, Z Devito, Z Lin, A Desmaison, L Antiga, A Lerer, Automatic differentiation in pytorch. A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, A. Lerer, Automatic differentiation in pytorch (2017).\n\nAdam: A method for stochastic optimization. D P Kingma, J Ba, International Conference on Learning Representations (ICLR). D. P. Kingma, J. Ba, Adam: A method for stochastic optimization, in: Inter- national Conference on Learning Representations (ICLR), 2014, pp. 1-13 (2014).\n\nV K Valloli, K Mehta, arXiv:1903.11249W-net: Reinforced u-net for density map estimation. arXiv preprintV. K. Valloli, K. Mehta, W-net: Reinforced u-net for density map estimation, arXiv preprint arXiv:1903.11249 (2019).\n\nContext-aware crowd counting. W Liu, M Salzmann, P Fua, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). W. Liu, M. Salzmann, P. Fua, Context-aware crowd counting, in: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019, pp. 5099-5108 (2019).\n\nCrowd counting via adversarial cross-scale consistency pursuit. Z Shen, Y Xu, B Ni, M Wang, J Hu, X Yang, IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Z. Shen, Y. Xu, B. Ni, M. Wang, J. Hu, X. Yang, Crowd counting via adversarial cross-scale consistency pursuit, in: IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 5245-5254 (2018).\n\nScale aggregation network for accurate and efficient crowd counting. X Cao, Z Wang, Y Zhao, F Su, European Conference on Computer Vision (ECCV). X. Cao, Z. Wang, Y. Zhao, F. Su, Scale aggregation network for accurate and efficient crowd counting, in: European Conference on Computer Vision (ECCV), 2018, pp. 734-750 (2018).\n", "annotations": {"author": "[{\"end\":201,\"start\":76},{\"end\":318,\"start\":202},{\"end\":431,\"start\":319},{\"end\":547,\"start\":432}]", "publisher": null, "author_last_name": "[{\"end\":86,\"start\":83},{\"end\":209,\"start\":206},{\"end\":333,\"start\":328},{\"end\":444,\"start\":441}]", "author_first_name": "[{\"end\":82,\"start\":76},{\"end\":205,\"start\":202},{\"end\":327,\"start\":319},{\"end\":440,\"start\":432}]", "author_affiliation": "[{\"end\":200,\"start\":118},{\"end\":317,\"start\":235},{\"end\":430,\"start\":335},{\"end\":546,\"start\":475}]", "title": "[{\"end\":62,\"start\":1},{\"end\":609,\"start\":548}]", "venue": null, "abstract": "[{\"end\":2269,\"start\":782}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2375,\"start\":2372},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2377,\"start\":2375},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2399,\"start\":2396},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2401,\"start\":2399},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2501,\"start\":2498},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2594,\"start\":2591},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2596,\"start\":2594},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2997,\"start\":2994},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3055,\"start\":3052},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4154,\"start\":4153},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4462,\"start\":4461},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7586,\"start\":7582},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7676,\"start\":7672},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7699,\"start\":7695},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7827,\"start\":7823},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7873,\"start\":7869},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8191,\"start\":8187},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8299,\"start\":8296},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":8314,\"start\":8311},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8412,\"start\":8408},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8415,\"start\":8412},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8450,\"start\":8446},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8453,\"start\":8450},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8778,\"start\":8774},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8856,\"start\":8852},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8859,\"start\":8856},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8862,\"start\":8859},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":8988,\"start\":8984},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8991,\"start\":8988},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8994,\"start\":8991},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9068,\"start\":9065},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9240,\"start\":9236},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9243,\"start\":9240},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9274,\"start\":9270},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9277,\"start\":9274},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9322,\"start\":9318},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9325,\"start\":9322},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9425,\"start\":9421},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9570,\"start\":9566},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":9656,\"start\":9652},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9732,\"start\":9728},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":9816,\"start\":9812},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9934,\"start\":9930},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9997,\"start\":9993},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":10041,\"start\":10037},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":10239,\"start\":10235},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":10242,\"start\":10239},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10387,\"start\":10383},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10389,\"start\":10387},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10407,\"start\":10403},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":10595,\"start\":10591},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":10655,\"start\":10651},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":10778,\"start\":10774},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10964,\"start\":10961},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":10967,\"start\":10964},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":11432,\"start\":11428},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":11435,\"start\":11432},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11697,\"start\":11693},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":11965,\"start\":11961},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":12293,\"start\":12289},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":12600,\"start\":12596},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":12621,\"start\":12617},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":13239,\"start\":13235},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":13441,\"start\":13437},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14125,\"start\":14121},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":14128,\"start\":14125},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14959,\"start\":14956},{\"end\":26295,\"start\":26289},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":28105,\"start\":28102},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":28121,\"start\":28117},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":28142,\"start\":28139},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":28605,\"start\":28602},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":28608,\"start\":28605},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":31496,\"start\":31492},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":32269,\"start\":32265},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":32324,\"start\":32320},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":33154,\"start\":33151},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":33958,\"start\":33954},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":33992,\"start\":33988},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":33995,\"start\":33992},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":34024,\"start\":34020},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":34434,\"start\":34430},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":36003,\"start\":35999},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":36441,\"start\":36437},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":37132,\"start\":37129},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":37348,\"start\":37345},{\"end\":37351,\"start\":37348},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":45357,\"start\":45356},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":45418,\"start\":45417}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":44549,\"start\":44507},{\"attributes\":{\"id\":\"fig_1\"},\"end\":44830,\"start\":44550},{\"attributes\":{\"id\":\"fig_2\"},\"end\":45496,\"start\":44831},{\"attributes\":{\"id\":\"fig_3\"},\"end\":45568,\"start\":45497},{\"attributes\":{\"id\":\"fig_4\"},\"end\":45618,\"start\":45569},{\"attributes\":{\"id\":\"fig_6\"},\"end\":45753,\"start\":45619},{\"attributes\":{\"id\":\"fig_7\"},\"end\":45878,\"start\":45754},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":45931,\"start\":45879},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":46045,\"start\":45932},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":46701,\"start\":46046},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":47410,\"start\":46702},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":48234,\"start\":47411},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":48598,\"start\":48235},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":48795,\"start\":48599}]", "paragraph": "[{\"end\":3311,\"start\":2285},{\"end\":3908,\"start\":3313},{\"end\":4963,\"start\":3910},{\"end\":6479,\"start\":4965},{\"end\":7132,\"start\":6481},{\"end\":7248,\"start\":7134},{\"end\":7355,\"start\":7250},{\"end\":7468,\"start\":7357},{\"end\":8080,\"start\":7516},{\"end\":8695,\"start\":8113},{\"end\":9326,\"start\":8721},{\"end\":10968,\"start\":9328},{\"end\":11169,\"start\":11014},{\"end\":13442,\"start\":11171},{\"end\":14015,\"start\":13458},{\"end\":14449,\"start\":14066},{\"end\":14572,\"start\":14483},{\"end\":14960,\"start\":14587},{\"end\":15070,\"start\":14962},{\"end\":15371,\"start\":15119},{\"end\":15993,\"start\":15428},{\"end\":17146,\"start\":16007},{\"end\":17838,\"start\":17148},{\"end\":18265,\"start\":17894},{\"end\":18843,\"start\":18355},{\"end\":19514,\"start\":18888},{\"end\":19752,\"start\":19516},{\"end\":20988,\"start\":19754},{\"end\":22739,\"start\":21086},{\"end\":24371,\"start\":22762},{\"end\":24880,\"start\":24564},{\"end\":27254,\"start\":24925},{\"end\":27869,\"start\":27256},{\"end\":28757,\"start\":27917},{\"end\":29327,\"start\":28759},{\"end\":29702,\"start\":29449},{\"end\":30028,\"start\":29704},{\"end\":30692,\"start\":30030},{\"end\":31233,\"start\":30694},{\"end\":31411,\"start\":31235},{\"end\":32502,\"start\":31438},{\"end\":32990,\"start\":32527},{\"end\":33838,\"start\":33033},{\"end\":35937,\"start\":33840},{\"end\":36647,\"start\":35977},{\"end\":37394,\"start\":36649},{\"end\":38240,\"start\":37396},{\"end\":38256,\"start\":38251},{\"end\":39347,\"start\":38291},{\"end\":39916,\"start\":39416},{\"end\":40392,\"start\":39918},{\"end\":41218,\"start\":40437},{\"end\":41655,\"start\":41263},{\"end\":42657,\"start\":41657},{\"end\":43715,\"start\":42691},{\"end\":43759,\"start\":43726},{\"end\":43782,\"start\":43761},{\"end\":44506,\"start\":43798}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":14482,\"start\":14450},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14586,\"start\":14573},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15118,\"start\":15071},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18354,\"start\":18266},{\"attributes\":{\"id\":\"formula_4\"},\"end\":21085,\"start\":20989},{\"attributes\":{\"id\":\"formula_5\"},\"end\":24563,\"start\":24372},{\"attributes\":{\"id\":\"formula_6\"},\"end\":29366,\"start\":29328},{\"attributes\":{\"id\":\"formula_7\"},\"end\":29406,\"start\":29366},{\"attributes\":{\"id\":\"formula_8\"},\"end\":29448,\"start\":29406}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":17837,\"start\":17830},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":31305,\"start\":31298},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":32987,\"start\":32980},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":33885,\"start\":33878},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":35621,\"start\":35614},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":36708,\"start\":36701},{\"end\":37428,\"start\":37421},{\"end\":37910,\"start\":37903},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":38443,\"start\":38436},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":39964,\"start\":39957},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":42421,\"start\":42414},{\"end\":43146,\"start\":43139},{\"end\":43555,\"start\":43548}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2283,\"start\":2271},{\"attributes\":{\"n\":\"2.\"},\"end\":7484,\"start\":7471},{\"attributes\":{\"n\":\"2.1.\"},\"end\":7514,\"start\":7487},{\"attributes\":{\"n\":\"2.2.\"},\"end\":8111,\"start\":8083},{\"attributes\":{\"n\":\"2.3.\"},\"end\":8719,\"start\":8698},{\"attributes\":{\"n\":\"2.4.\"},\"end\":11012,\"start\":10971},{\"attributes\":{\"n\":\"3.\"},\"end\":13456,\"start\":13445},{\"attributes\":{\"n\":\"3.1.\"},\"end\":14064,\"start\":14018},{\"attributes\":{\"n\":\"3.2.\"},\"end\":15426,\"start\":15374},{\"end\":16005,\"start\":15996},{\"attributes\":{\"n\":\"3.3.\"},\"end\":17892,\"start\":17841},{\"attributes\":{\"n\":\"3.4.\"},\"end\":18886,\"start\":18846},{\"attributes\":{\"n\":\"3.5.\"},\"end\":22760,\"start\":22742},{\"attributes\":{\"n\":\"4.\"},\"end\":24923,\"start\":24883},{\"attributes\":{\"n\":\"5.\"},\"end\":27892,\"start\":27872},{\"attributes\":{\"n\":\"5.1.\"},\"end\":27915,\"start\":27895},{\"attributes\":{\"n\":\"5.2.\"},\"end\":31436,\"start\":31414},{\"attributes\":{\"n\":\"5.3.\"},\"end\":32525,\"start\":32505},{\"attributes\":{\"n\":\"5.3.1.\"},\"end\":33031,\"start\":32993},{\"attributes\":{\"n\":\"5.3.2.\"},\"end\":35975,\"start\":35940},{\"end\":38249,\"start\":38243},{\"attributes\":{\"n\":\"5.3.4.\"},\"end\":38289,\"start\":38259},{\"attributes\":{\"n\":\"5.4.\"},\"end\":39414,\"start\":39350},{\"attributes\":{\"n\":\"5.4.2.\"},\"end\":40435,\"start\":40395},{\"attributes\":{\"n\":\"5.4.3.\"},\"end\":41261,\"start\":41221},{\"attributes\":{\"n\":\"5.4.4.\"},\"end\":42689,\"start\":42660},{\"end\":43724,\"start\":43718},{\"attributes\":{\"n\":\"6.\"},\"end\":43796,\"start\":43785},{\"end\":44513,\"start\":44508},{\"end\":44561,\"start\":44551},{\"end\":44845,\"start\":44832},{\"end\":45508,\"start\":45498},{\"end\":45630,\"start\":45620},{\"end\":45765,\"start\":45755},{\"end\":45889,\"start\":45880},{\"end\":45942,\"start\":45933},{\"end\":46056,\"start\":46047},{\"end\":46712,\"start\":46703},{\"end\":47421,\"start\":47412},{\"end\":48245,\"start\":48236},{\"end\":48609,\"start\":48600}]", "table": "[{\"end\":46045,\"start\":45981},{\"end\":46701,\"start\":46196},{\"end\":47410,\"start\":46862},{\"end\":48234,\"start\":47527},{\"end\":48598,\"start\":48387},{\"end\":48795,\"start\":48734}]", "figure_caption": "[{\"end\":44549,\"start\":44515},{\"end\":44830,\"start\":44563},{\"end\":45496,\"start\":44847},{\"end\":45568,\"start\":45510},{\"end\":45618,\"start\":45571},{\"end\":45753,\"start\":45632},{\"end\":45878,\"start\":45767},{\"end\":45931,\"start\":45891},{\"end\":45981,\"start\":45944},{\"end\":46196,\"start\":46058},{\"end\":46862,\"start\":46714},{\"end\":47527,\"start\":47423},{\"end\":48387,\"start\":48247},{\"end\":48734,\"start\":48611}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":5232,\"start\":5224},{\"end\":20517,\"start\":20509},{\"end\":20595,\"start\":20587},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":25867,\"start\":25859},{\"end\":26304,\"start\":26296},{\"end\":26914,\"start\":26906},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":35634,\"start\":35626},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":40718,\"start\":40710}]", "bib_author_first_name": "[{\"end\":48865,\"start\":48864},{\"end\":48874,\"start\":48873},{\"end\":48880,\"start\":48879},{\"end\":48888,\"start\":48887},{\"end\":49238,\"start\":49237},{\"end\":49247,\"start\":49246},{\"end\":49255,\"start\":49254},{\"end\":49263,\"start\":49262},{\"end\":49270,\"start\":49269},{\"end\":49626,\"start\":49625},{\"end\":49632,\"start\":49631},{\"end\":49638,\"start\":49637},{\"end\":49647,\"start\":49646},{\"end\":49656,\"start\":49655},{\"end\":49984,\"start\":49983},{\"end\":49991,\"start\":49990},{\"end\":49999,\"start\":49998},{\"end\":50001,\"start\":50000},{\"end\":50007,\"start\":50006},{\"end\":50014,\"start\":50013},{\"end\":50266,\"start\":50265},{\"end\":50279,\"start\":50278},{\"end\":50509,\"start\":50508},{\"end\":50534,\"start\":50533},{\"end\":50551,\"start\":50550},{\"end\":50567,\"start\":50566},{\"end\":50587,\"start\":50586},{\"end\":51036,\"start\":51035},{\"end\":51043,\"start\":51042},{\"end\":51045,\"start\":51044},{\"end\":51054,\"start\":51053},{\"end\":51450,\"start\":51449},{\"end\":51465,\"start\":51464},{\"end\":51477,\"start\":51476},{\"end\":51479,\"start\":51478},{\"end\":51490,\"start\":51489},{\"end\":51852,\"start\":51851},{\"end\":51854,\"start\":51853},{\"end\":51866,\"start\":51865},{\"end\":52234,\"start\":52233},{\"end\":52236,\"start\":52235},{\"end\":52247,\"start\":52243},{\"end\":52249,\"start\":52248},{\"end\":52258,\"start\":52257},{\"end\":52622,\"start\":52621},{\"end\":52631,\"start\":52630},{\"end\":52914,\"start\":52913},{\"end\":52923,\"start\":52922},{\"end\":52925,\"start\":52924},{\"end\":53181,\"start\":53180},{\"end\":53183,\"start\":53182},{\"end\":53197,\"start\":53196},{\"end\":53209,\"start\":53208},{\"end\":53604,\"start\":53603},{\"end\":53610,\"start\":53609},{\"end\":53616,\"start\":53615},{\"end\":53618,\"start\":53617},{\"end\":53936,\"start\":53935},{\"end\":53944,\"start\":53943},{\"end\":53946,\"start\":53945},{\"end\":53953,\"start\":53952},{\"end\":53961,\"start\":53960},{\"end\":54209,\"start\":54208},{\"end\":54211,\"start\":54210},{\"end\":54219,\"start\":54218},{\"end\":54503,\"start\":54502},{\"end\":54514,\"start\":54513},{\"end\":54526,\"start\":54525},{\"end\":54537,\"start\":54536},{\"end\":54539,\"start\":54538},{\"end\":54546,\"start\":54545},{\"end\":54558,\"start\":54557},{\"end\":54560,\"start\":54559},{\"end\":54901,\"start\":54900},{\"end\":54912,\"start\":54911},{\"end\":54921,\"start\":54920},{\"end\":54929,\"start\":54928},{\"end\":54931,\"start\":54930},{\"end\":55306,\"start\":55302},{\"end\":55314,\"start\":55313},{\"end\":55326,\"start\":55325},{\"end\":55339,\"start\":55338},{\"end\":55646,\"start\":55645},{\"end\":55660,\"start\":55659},{\"end\":55673,\"start\":55672},{\"end\":55675,\"start\":55674},{\"end\":56044,\"start\":56043},{\"end\":56054,\"start\":56053},{\"end\":56305,\"start\":56304},{\"end\":56320,\"start\":56319},{\"end\":56322,\"start\":56321},{\"end\":56560,\"start\":56559},{\"end\":56571,\"start\":56570},{\"end\":56585,\"start\":56584},{\"end\":56595,\"start\":56594},{\"end\":56597,\"start\":56596},{\"end\":56963,\"start\":56962},{\"end\":56971,\"start\":56970},{\"end\":56984,\"start\":56983},{\"end\":57337,\"start\":57336},{\"end\":57355,\"start\":57354},{\"end\":57366,\"start\":57365},{\"end\":57768,\"start\":57767},{\"end\":57776,\"start\":57775},{\"end\":57785,\"start\":57784},{\"end\":57791,\"start\":57790},{\"end\":57798,\"start\":57797},{\"end\":58143,\"start\":58142},{\"end\":58151,\"start\":58150},{\"end\":58161,\"start\":58160},{\"end\":58171,\"start\":58170},{\"end\":58489,\"start\":58488},{\"end\":58499,\"start\":58498},{\"end\":58508,\"start\":58507},{\"end\":58519,\"start\":58518},{\"end\":58867,\"start\":58866},{\"end\":58875,\"start\":58874},{\"end\":58879,\"start\":58876},{\"end\":59101,\"start\":59100},{\"end\":59109,\"start\":59108},{\"end\":59370,\"start\":59369},{\"end\":59377,\"start\":59376},{\"end\":59667,\"start\":59666},{\"end\":59669,\"start\":59668},{\"end\":59676,\"start\":59675},{\"end\":59685,\"start\":59684},{\"end\":59687,\"start\":59686},{\"end\":60027,\"start\":60026},{\"end\":60029,\"start\":60028},{\"end\":60040,\"start\":60039},{\"end\":60042,\"start\":60041},{\"end\":60367,\"start\":60366},{\"end\":60379,\"start\":60378},{\"end\":60724,\"start\":60723},{\"end\":60730,\"start\":60729},{\"end\":60739,\"start\":60738},{\"end\":61106,\"start\":61105},{\"end\":61113,\"start\":61112},{\"end\":61121,\"start\":61120},{\"end\":61127,\"start\":61126},{\"end\":61134,\"start\":61133},{\"end\":61141,\"start\":61140},{\"end\":61340,\"start\":61339},{\"end\":61350,\"start\":61349},{\"end\":61356,\"start\":61355},{\"end\":61630,\"start\":61629},{\"end\":61640,\"start\":61639},{\"end\":61650,\"start\":61649},{\"end\":61660,\"start\":61659},{\"end\":61669,\"start\":61668},{\"end\":61683,\"start\":61682},{\"end\":61694,\"start\":61693},{\"end\":62062,\"start\":62061},{\"end\":62071,\"start\":62070},{\"end\":62080,\"start\":62079},{\"end\":62087,\"start\":62086},{\"end\":62095,\"start\":62094},{\"end\":62102,\"start\":62101},{\"end\":62465,\"start\":62464},{\"end\":62472,\"start\":62471},{\"end\":62480,\"start\":62479},{\"end\":62487,\"start\":62486},{\"end\":62494,\"start\":62493},{\"end\":62501,\"start\":62500},{\"end\":62880,\"start\":62879},{\"end\":62889,\"start\":62888},{\"end\":62897,\"start\":62896},{\"end\":62906,\"start\":62905},{\"end\":62914,\"start\":62913},{\"end\":62921,\"start\":62920},{\"end\":62933,\"start\":62932},{\"end\":63353,\"start\":63352},{\"end\":63360,\"start\":63359},{\"end\":63367,\"start\":63366},{\"end\":63375,\"start\":63374},{\"end\":63377,\"start\":63376},{\"end\":63701,\"start\":63700},{\"end\":63708,\"start\":63707},{\"end\":63715,\"start\":63714},{\"end\":63721,\"start\":63720},{\"end\":63728,\"start\":63727},{\"end\":63738,\"start\":63737},{\"end\":64137,\"start\":64136},{\"end\":64143,\"start\":64142},{\"end\":64150,\"start\":64149},{\"end\":64156,\"start\":64155},{\"end\":64163,\"start\":64162},{\"end\":64169,\"start\":64168},{\"end\":64481,\"start\":64480},{\"end\":64490,\"start\":64489},{\"end\":64496,\"start\":64495},{\"end\":64503,\"start\":64502},{\"end\":64798,\"start\":64797},{\"end\":64804,\"start\":64803},{\"end\":64811,\"start\":64810},{\"end\":64819,\"start\":64818},{\"end\":65136,\"start\":65135},{\"end\":65144,\"start\":65143},{\"end\":65152,\"start\":65151},{\"end\":65159,\"start\":65158},{\"end\":65166,\"start\":65165},{\"end\":65514,\"start\":65513},{\"end\":65521,\"start\":65520},{\"end\":65538,\"start\":65537},{\"end\":65540,\"start\":65539},{\"end\":65908,\"start\":65907},{\"end\":65915,\"start\":65914},{\"end\":65932,\"start\":65931},{\"end\":65934,\"start\":65933},{\"end\":66276,\"start\":66275},{\"end\":66278,\"start\":66277},{\"end\":66285,\"start\":66284},{\"end\":66287,\"start\":66286},{\"end\":66297,\"start\":66296},{\"end\":66307,\"start\":66306},{\"end\":66309,\"start\":66308},{\"end\":66607,\"start\":66606},{\"end\":66615,\"start\":66614},{\"end\":66622,\"start\":66621},{\"end\":66629,\"start\":66628},{\"end\":66988,\"start\":66987},{\"end\":66995,\"start\":66994},{\"end\":67003,\"start\":67002},{\"end\":67314,\"start\":67313},{\"end\":67321,\"start\":67320},{\"end\":67328,\"start\":67327},{\"end\":67336,\"start\":67335},{\"end\":67613,\"start\":67612},{\"end\":67625,\"start\":67621},{\"end\":67634,\"start\":67633},{\"end\":67636,\"start\":67635},{\"end\":67647,\"start\":67646},{\"end\":67649,\"start\":67648},{\"end\":68036,\"start\":68035},{\"end\":68044,\"start\":68043},{\"end\":68051,\"start\":68050},{\"end\":68060,\"start\":68059},{\"end\":68062,\"start\":68061},{\"end\":68319,\"start\":68318},{\"end\":68329,\"start\":68328},{\"end\":68340,\"start\":68339},{\"end\":68351,\"start\":68350},{\"end\":68637,\"start\":68636},{\"end\":68647,\"start\":68646},{\"end\":68656,\"start\":68655},{\"end\":68668,\"start\":68667},{\"end\":68678,\"start\":68677},{\"end\":68686,\"start\":68685},{\"end\":68696,\"start\":68695},{\"end\":68703,\"start\":68702},{\"end\":68716,\"start\":68715},{\"end\":68726,\"start\":68725},{\"end\":68971,\"start\":68970},{\"end\":68973,\"start\":68972},{\"end\":68983,\"start\":68982},{\"end\":69206,\"start\":69205},{\"end\":69208,\"start\":69207},{\"end\":69219,\"start\":69218},{\"end\":69458,\"start\":69457},{\"end\":69465,\"start\":69464},{\"end\":69477,\"start\":69476},{\"end\":69774,\"start\":69773},{\"end\":69782,\"start\":69781},{\"end\":69788,\"start\":69787},{\"end\":69794,\"start\":69793},{\"end\":69802,\"start\":69801},{\"end\":69808,\"start\":69807},{\"end\":70164,\"start\":70163},{\"end\":70171,\"start\":70170},{\"end\":70179,\"start\":70178},{\"end\":70187,\"start\":70186}]", "bib_author_last_name": "[{\"end\":48871,\"start\":48866},{\"end\":48877,\"start\":48875},{\"end\":48885,\"start\":48881},{\"end\":48893,\"start\":48889},{\"end\":49244,\"start\":49239},{\"end\":49252,\"start\":49248},{\"end\":49260,\"start\":49256},{\"end\":49267,\"start\":49264},{\"end\":49273,\"start\":49271},{\"end\":49629,\"start\":49627},{\"end\":49635,\"start\":49633},{\"end\":49644,\"start\":49639},{\"end\":49653,\"start\":49648},{\"end\":49661,\"start\":49657},{\"end\":49988,\"start\":49985},{\"end\":49996,\"start\":49992},{\"end\":50004,\"start\":50002},{\"end\":50011,\"start\":50008},{\"end\":50019,\"start\":50015},{\"end\":50276,\"start\":50267},{\"end\":50289,\"start\":50280},{\"end\":50531,\"start\":50510},{\"end\":50548,\"start\":50535},{\"end\":50564,\"start\":50552},{\"end\":50584,\"start\":50568},{\"end\":50599,\"start\":50588},{\"end\":51040,\"start\":51037},{\"end\":51051,\"start\":51046},{\"end\":51064,\"start\":51055},{\"end\":51462,\"start\":51451},{\"end\":51474,\"start\":51466},{\"end\":51487,\"start\":51480},{\"end\":51498,\"start\":51491},{\"end\":51863,\"start\":51855},{\"end\":51876,\"start\":51867},{\"end\":52241,\"start\":52237},{\"end\":52255,\"start\":52250},{\"end\":52270,\"start\":52259},{\"end\":52628,\"start\":52623},{\"end\":52638,\"start\":52632},{\"end\":52920,\"start\":52915},{\"end\":52931,\"start\":52926},{\"end\":53194,\"start\":53184},{\"end\":53206,\"start\":53198},{\"end\":53220,\"start\":53210},{\"end\":53607,\"start\":53605},{\"end\":53613,\"start\":53611},{\"end\":53623,\"start\":53619},{\"end\":53941,\"start\":53937},{\"end\":53950,\"start\":53947},{\"end\":53958,\"start\":53954},{\"end\":53967,\"start\":53962},{\"end\":54216,\"start\":54212},{\"end\":54231,\"start\":54220},{\"end\":54511,\"start\":54504},{\"end\":54523,\"start\":54515},{\"end\":54534,\"start\":54527},{\"end\":54543,\"start\":54540},{\"end\":54555,\"start\":54547},{\"end\":54570,\"start\":54561},{\"end\":54909,\"start\":54902},{\"end\":54918,\"start\":54913},{\"end\":54926,\"start\":54922},{\"end\":54941,\"start\":54932},{\"end\":55311,\"start\":55307},{\"end\":55323,\"start\":55315},{\"end\":55336,\"start\":55327},{\"end\":55345,\"start\":55340},{\"end\":55657,\"start\":55647},{\"end\":55670,\"start\":55661},{\"end\":55682,\"start\":55676},{\"end\":56051,\"start\":56045},{\"end\":56059,\"start\":56055},{\"end\":56317,\"start\":56306},{\"end\":56335,\"start\":56323},{\"end\":56568,\"start\":56561},{\"end\":56582,\"start\":56572},{\"end\":56592,\"start\":56586},{\"end\":56611,\"start\":56598},{\"end\":56968,\"start\":56964},{\"end\":56981,\"start\":56972},{\"end\":56992,\"start\":56985},{\"end\":57352,\"start\":57338},{\"end\":57363,\"start\":57356},{\"end\":57374,\"start\":57367},{\"end\":57773,\"start\":57769},{\"end\":57782,\"start\":57777},{\"end\":57788,\"start\":57786},{\"end\":57795,\"start\":57792},{\"end\":57802,\"start\":57799},{\"end\":58148,\"start\":58144},{\"end\":58158,\"start\":58152},{\"end\":58168,\"start\":58162},{\"end\":58181,\"start\":58172},{\"end\":58496,\"start\":58490},{\"end\":58505,\"start\":58500},{\"end\":58516,\"start\":58509},{\"end\":58526,\"start\":58520},{\"end\":58872,\"start\":58868},{\"end\":58884,\"start\":58880},{\"end\":59106,\"start\":59102},{\"end\":59114,\"start\":59110},{\"end\":59374,\"start\":59371},{\"end\":59389,\"start\":59378},{\"end\":59673,\"start\":59670},{\"end\":59682,\"start\":59677},{\"end\":59692,\"start\":59688},{\"end\":60037,\"start\":60030},{\"end\":60048,\"start\":60043},{\"end\":60376,\"start\":60368},{\"end\":60389,\"start\":60380},{\"end\":60727,\"start\":60725},{\"end\":60736,\"start\":60731},{\"end\":60744,\"start\":60740},{\"end\":61110,\"start\":61107},{\"end\":61118,\"start\":61114},{\"end\":61124,\"start\":61122},{\"end\":61131,\"start\":61128},{\"end\":61138,\"start\":61135},{\"end\":61145,\"start\":61142},{\"end\":61347,\"start\":61341},{\"end\":61353,\"start\":61351},{\"end\":61361,\"start\":61357},{\"end\":61637,\"start\":61631},{\"end\":61647,\"start\":61641},{\"end\":61657,\"start\":61651},{\"end\":61666,\"start\":61661},{\"end\":61680,\"start\":61670},{\"end\":61691,\"start\":61684},{\"end\":61699,\"start\":61695},{\"end\":62068,\"start\":62063},{\"end\":62077,\"start\":62072},{\"end\":62084,\"start\":62081},{\"end\":62092,\"start\":62088},{\"end\":62099,\"start\":62096},{\"end\":62107,\"start\":62103},{\"end\":62469,\"start\":62466},{\"end\":62477,\"start\":62473},{\"end\":62484,\"start\":62481},{\"end\":62491,\"start\":62488},{\"end\":62498,\"start\":62495},{\"end\":62504,\"start\":62502},{\"end\":62886,\"start\":62881},{\"end\":62894,\"start\":62890},{\"end\":62903,\"start\":62898},{\"end\":62911,\"start\":62907},{\"end\":62918,\"start\":62915},{\"end\":62930,\"start\":62922},{\"end\":62938,\"start\":62934},{\"end\":63357,\"start\":63354},{\"end\":63364,\"start\":63361},{\"end\":63372,\"start\":63368},{\"end\":63387,\"start\":63378},{\"end\":63705,\"start\":63702},{\"end\":63712,\"start\":63709},{\"end\":63718,\"start\":63716},{\"end\":63725,\"start\":63722},{\"end\":63735,\"start\":63729},{\"end\":63742,\"start\":63739},{\"end\":64140,\"start\":64138},{\"end\":64147,\"start\":64144},{\"end\":64153,\"start\":64151},{\"end\":64160,\"start\":64157},{\"end\":64166,\"start\":64164},{\"end\":64173,\"start\":64170},{\"end\":64487,\"start\":64482},{\"end\":64493,\"start\":64491},{\"end\":64500,\"start\":64497},{\"end\":64507,\"start\":64504},{\"end\":64801,\"start\":64799},{\"end\":64808,\"start\":64805},{\"end\":64816,\"start\":64812},{\"end\":64824,\"start\":64820},{\"end\":65141,\"start\":65137},{\"end\":65149,\"start\":65145},{\"end\":65156,\"start\":65153},{\"end\":65163,\"start\":65160},{\"end\":65169,\"start\":65167},{\"end\":65518,\"start\":65515},{\"end\":65535,\"start\":65522},{\"end\":65549,\"start\":65541},{\"end\":65912,\"start\":65909},{\"end\":65929,\"start\":65916},{\"end\":65943,\"start\":65935},{\"end\":66282,\"start\":66279},{\"end\":66294,\"start\":66288},{\"end\":66304,\"start\":66298},{\"end\":66314,\"start\":66310},{\"end\":66612,\"start\":66608},{\"end\":66619,\"start\":66616},{\"end\":66626,\"start\":66623},{\"end\":66634,\"start\":66630},{\"end\":66992,\"start\":66989},{\"end\":67000,\"start\":66996},{\"end\":67008,\"start\":67004},{\"end\":67318,\"start\":67315},{\"end\":67325,\"start\":67322},{\"end\":67333,\"start\":67329},{\"end\":67341,\"start\":67337},{\"end\":67619,\"start\":67614},{\"end\":67631,\"start\":67626},{\"end\":67644,\"start\":67637},{\"end\":67652,\"start\":67650},{\"end\":68041,\"start\":68037},{\"end\":68048,\"start\":68045},{\"end\":68057,\"start\":68052},{\"end\":68067,\"start\":68063},{\"end\":68326,\"start\":68320},{\"end\":68337,\"start\":68330},{\"end\":68348,\"start\":68341},{\"end\":68356,\"start\":68352},{\"end\":68644,\"start\":68638},{\"end\":68653,\"start\":68648},{\"end\":68665,\"start\":68657},{\"end\":68675,\"start\":68669},{\"end\":68683,\"start\":68679},{\"end\":68693,\"start\":68687},{\"end\":68700,\"start\":68697},{\"end\":68713,\"start\":68704},{\"end\":68723,\"start\":68717},{\"end\":68732,\"start\":68727},{\"end\":68980,\"start\":68974},{\"end\":68986,\"start\":68984},{\"end\":69216,\"start\":69209},{\"end\":69225,\"start\":69220},{\"end\":69462,\"start\":69459},{\"end\":69474,\"start\":69466},{\"end\":69481,\"start\":69478},{\"end\":69779,\"start\":69775},{\"end\":69785,\"start\":69783},{\"end\":69791,\"start\":69789},{\"end\":69799,\"start\":69795},{\"end\":69805,\"start\":69803},{\"end\":69813,\"start\":69809},{\"end\":70168,\"start\":70165},{\"end\":70176,\"start\":70172},{\"end\":70184,\"start\":70180},{\"end\":70190,\"start\":70188}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":2131202},\"end\":49160,\"start\":48797},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":4545310},\"end\":49556,\"start\":49162},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":199488299},\"end\":49926,\"start\":49558},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":199543540},\"end\":50226,\"start\":49928},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":18018217},\"end\":50506,\"start\":50228},{\"attributes\":{\"id\":\"b5\"},\"end\":50948,\"start\":50508},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":19076929},\"end\":51389,\"start\":50950},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":4410840},\"end\":51779,\"start\":51391},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":7656505},\"end\":52143,\"start\":51781},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":9059102},\"end\":52565,\"start\":52145},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":206590483},\"end\":52878,\"start\":52567},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":2796017},\"end\":53118,\"start\":52880},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":15508488},\"end\":53529,\"start\":53120},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":303636},\"end\":53888,\"start\":53531},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1910869},\"end\":54158,\"start\":53890},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":749620},\"end\":54443,\"start\":54160},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":37733756},\"end\":54834,\"start\":54445},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":16358367},\"end\":55198,\"start\":54836},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1718465},\"end\":55643,\"start\":55200},{\"attributes\":{\"id\":\"b19\"},\"end\":56004,\"start\":55645},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":26569197},\"end\":56241,\"start\":56006},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":40499053},\"end\":56557,\"start\":56243},{\"attributes\":{\"id\":\"b22\"},\"end\":56904,\"start\":56559},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":1629541},\"end\":57252,\"start\":56906},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":60814714},\"end\":57681,\"start\":57254},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":8238530},\"end\":58094,\"start\":57683},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":14897024},\"end\":58425,\"start\":58096},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":206764948},\"end\":58808,\"start\":58427},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":9665152},\"end\":59098,\"start\":58810},{\"attributes\":{\"doi\":\"arXiv:1411.4464\",\"id\":\"b29\"},\"end\":59323,\"start\":59100},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":1442165},\"end\":59605,\"start\":59325},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":1089358},\"end\":59950,\"start\":59607},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":2099022},\"end\":60296,\"start\":59952},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":14124313},\"end\":60628,\"start\":60298},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":3645757},\"end\":61030,\"start\":60630},{\"attributes\":{\"doi\":\"arXiv:1902.01115\",\"id\":\"b35\"},\"end\":61337,\"start\":61032},{\"attributes\":{\"id\":\"b36\"},\"end\":61539,\"start\":61339},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":51901514},\"end\":61993,\"start\":61541},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":54442882},\"end\":62370,\"start\":61995},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":53957027},\"end\":62802,\"start\":62372},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":67856061},\"end\":63248,\"start\":62804},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":3740753},\"end\":63698,\"start\":63250},{\"attributes\":{\"id\":\"b42\"},\"end\":64052,\"start\":63700},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":198968173},\"end\":64438,\"start\":64054},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":57373907},\"end\":64730,\"start\":64440},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":199543622},\"end\":65058,\"start\":64732},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":197544919},\"end\":65440,\"start\":65060},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":62841646},\"end\":65839,\"start\":65442},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":3787969},\"end\":66218,\"start\":65841},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":70298800},\"end\":66543,\"start\":66220},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":72941021},\"end\":66885,\"start\":66545},{\"attributes\":{\"doi\":\"arXiv:1912.03672\",\"id\":\"b51\"},\"end\":67207,\"start\":66887},{\"attributes\":{\"doi\":\"arXiv:1912.03677\",\"id\":\"b52\"},\"end\":67550,\"start\":67209},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":52811641},\"end\":68033,\"start\":67552},{\"attributes\":{\"doi\":\"arXiv:1811.02805\",\"id\":\"b54\"},\"end\":68249,\"start\":68035},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":9749221},\"end\":68634,\"start\":68251},{\"attributes\":{\"id\":\"b56\"},\"end\":68924,\"start\":68636},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":6628106},\"end\":69203,\"start\":68926},{\"attributes\":{\"doi\":\"arXiv:1903.11249\",\"id\":\"b58\"},\"end\":69425,\"start\":69205},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":53783843},\"end\":69707,\"start\":69427},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":52860247},\"end\":70092,\"start\":69709},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":52955811},\"end\":70417,\"start\":70094}]", "bib_title": "[{\"end\":48862,\"start\":48797},{\"end\":49235,\"start\":49162},{\"end\":49623,\"start\":49558},{\"end\":49981,\"start\":49928},{\"end\":50263,\"start\":50228},{\"end\":51033,\"start\":50950},{\"end\":51447,\"start\":51391},{\"end\":51849,\"start\":51781},{\"end\":52231,\"start\":52145},{\"end\":52619,\"start\":52567},{\"end\":52911,\"start\":52880},{\"end\":53178,\"start\":53120},{\"end\":53601,\"start\":53531},{\"end\":53933,\"start\":53890},{\"end\":54206,\"start\":54160},{\"end\":54500,\"start\":54445},{\"end\":54898,\"start\":54836},{\"end\":55300,\"start\":55200},{\"end\":56041,\"start\":56006},{\"end\":56302,\"start\":56243},{\"end\":56960,\"start\":56906},{\"end\":57334,\"start\":57254},{\"end\":57765,\"start\":57683},{\"end\":58140,\"start\":58096},{\"end\":58486,\"start\":58427},{\"end\":58864,\"start\":58810},{\"end\":59367,\"start\":59325},{\"end\":59664,\"start\":59607},{\"end\":60024,\"start\":59952},{\"end\":60364,\"start\":60298},{\"end\":60721,\"start\":60630},{\"end\":61627,\"start\":61541},{\"end\":62059,\"start\":61995},{\"end\":62462,\"start\":62372},{\"end\":62877,\"start\":62804},{\"end\":63350,\"start\":63250},{\"end\":64134,\"start\":64054},{\"end\":64478,\"start\":64440},{\"end\":64795,\"start\":64732},{\"end\":65133,\"start\":65060},{\"end\":65511,\"start\":65442},{\"end\":65905,\"start\":65841},{\"end\":66273,\"start\":66220},{\"end\":66604,\"start\":66545},{\"end\":67610,\"start\":67552},{\"end\":68316,\"start\":68251},{\"end\":68968,\"start\":68926},{\"end\":69455,\"start\":69427},{\"end\":69771,\"start\":69709},{\"end\":70161,\"start\":70094}]", "bib_author": "[{\"end\":48873,\"start\":48864},{\"end\":48879,\"start\":48873},{\"end\":48887,\"start\":48879},{\"end\":48895,\"start\":48887},{\"end\":49246,\"start\":49237},{\"end\":49254,\"start\":49246},{\"end\":49262,\"start\":49254},{\"end\":49269,\"start\":49262},{\"end\":49275,\"start\":49269},{\"end\":49631,\"start\":49625},{\"end\":49637,\"start\":49631},{\"end\":49646,\"start\":49637},{\"end\":49655,\"start\":49646},{\"end\":49663,\"start\":49655},{\"end\":49990,\"start\":49983},{\"end\":49998,\"start\":49990},{\"end\":50006,\"start\":49998},{\"end\":50013,\"start\":50006},{\"end\":50021,\"start\":50013},{\"end\":50278,\"start\":50265},{\"end\":50291,\"start\":50278},{\"end\":50533,\"start\":50508},{\"end\":50550,\"start\":50533},{\"end\":50566,\"start\":50550},{\"end\":50586,\"start\":50566},{\"end\":50601,\"start\":50586},{\"end\":51042,\"start\":51035},{\"end\":51053,\"start\":51042},{\"end\":51066,\"start\":51053},{\"end\":51464,\"start\":51449},{\"end\":51476,\"start\":51464},{\"end\":51489,\"start\":51476},{\"end\":51500,\"start\":51489},{\"end\":51865,\"start\":51851},{\"end\":51878,\"start\":51865},{\"end\":52243,\"start\":52233},{\"end\":52257,\"start\":52243},{\"end\":52272,\"start\":52257},{\"end\":52630,\"start\":52621},{\"end\":52640,\"start\":52630},{\"end\":52922,\"start\":52913},{\"end\":52933,\"start\":52922},{\"end\":53196,\"start\":53180},{\"end\":53208,\"start\":53196},{\"end\":53222,\"start\":53208},{\"end\":53609,\"start\":53603},{\"end\":53615,\"start\":53609},{\"end\":53625,\"start\":53615},{\"end\":53943,\"start\":53935},{\"end\":53952,\"start\":53943},{\"end\":53960,\"start\":53952},{\"end\":53969,\"start\":53960},{\"end\":54218,\"start\":54208},{\"end\":54233,\"start\":54218},{\"end\":54513,\"start\":54502},{\"end\":54525,\"start\":54513},{\"end\":54536,\"start\":54525},{\"end\":54545,\"start\":54536},{\"end\":54557,\"start\":54545},{\"end\":54572,\"start\":54557},{\"end\":54911,\"start\":54900},{\"end\":54920,\"start\":54911},{\"end\":54928,\"start\":54920},{\"end\":54943,\"start\":54928},{\"end\":55313,\"start\":55302},{\"end\":55325,\"start\":55313},{\"end\":55338,\"start\":55325},{\"end\":55347,\"start\":55338},{\"end\":55659,\"start\":55645},{\"end\":55672,\"start\":55659},{\"end\":55684,\"start\":55672},{\"end\":56053,\"start\":56043},{\"end\":56061,\"start\":56053},{\"end\":56319,\"start\":56304},{\"end\":56337,\"start\":56319},{\"end\":56570,\"start\":56559},{\"end\":56584,\"start\":56570},{\"end\":56594,\"start\":56584},{\"end\":56613,\"start\":56594},{\"end\":56970,\"start\":56962},{\"end\":56983,\"start\":56970},{\"end\":56994,\"start\":56983},{\"end\":57354,\"start\":57336},{\"end\":57365,\"start\":57354},{\"end\":57376,\"start\":57365},{\"end\":57775,\"start\":57767},{\"end\":57784,\"start\":57775},{\"end\":57790,\"start\":57784},{\"end\":57797,\"start\":57790},{\"end\":57804,\"start\":57797},{\"end\":58150,\"start\":58142},{\"end\":58160,\"start\":58150},{\"end\":58170,\"start\":58160},{\"end\":58183,\"start\":58170},{\"end\":58498,\"start\":58488},{\"end\":58507,\"start\":58498},{\"end\":58518,\"start\":58507},{\"end\":58528,\"start\":58518},{\"end\":58874,\"start\":58866},{\"end\":58886,\"start\":58874},{\"end\":59108,\"start\":59100},{\"end\":59116,\"start\":59108},{\"end\":59376,\"start\":59369},{\"end\":59391,\"start\":59376},{\"end\":59675,\"start\":59666},{\"end\":59684,\"start\":59675},{\"end\":59694,\"start\":59684},{\"end\":60039,\"start\":60026},{\"end\":60050,\"start\":60039},{\"end\":60378,\"start\":60366},{\"end\":60391,\"start\":60378},{\"end\":60729,\"start\":60723},{\"end\":60738,\"start\":60729},{\"end\":60746,\"start\":60738},{\"end\":61112,\"start\":61105},{\"end\":61120,\"start\":61112},{\"end\":61126,\"start\":61120},{\"end\":61133,\"start\":61126},{\"end\":61140,\"start\":61133},{\"end\":61147,\"start\":61140},{\"end\":61349,\"start\":61339},{\"end\":61355,\"start\":61349},{\"end\":61363,\"start\":61355},{\"end\":61639,\"start\":61629},{\"end\":61649,\"start\":61639},{\"end\":61659,\"start\":61649},{\"end\":61668,\"start\":61659},{\"end\":61682,\"start\":61668},{\"end\":61693,\"start\":61682},{\"end\":61701,\"start\":61693},{\"end\":62070,\"start\":62061},{\"end\":62079,\"start\":62070},{\"end\":62086,\"start\":62079},{\"end\":62094,\"start\":62086},{\"end\":62101,\"start\":62094},{\"end\":62109,\"start\":62101},{\"end\":62471,\"start\":62464},{\"end\":62479,\"start\":62471},{\"end\":62486,\"start\":62479},{\"end\":62493,\"start\":62486},{\"end\":62500,\"start\":62493},{\"end\":62506,\"start\":62500},{\"end\":62888,\"start\":62879},{\"end\":62896,\"start\":62888},{\"end\":62905,\"start\":62896},{\"end\":62913,\"start\":62905},{\"end\":62920,\"start\":62913},{\"end\":62932,\"start\":62920},{\"end\":62940,\"start\":62932},{\"end\":63359,\"start\":63352},{\"end\":63366,\"start\":63359},{\"end\":63374,\"start\":63366},{\"end\":63389,\"start\":63374},{\"end\":63707,\"start\":63700},{\"end\":63714,\"start\":63707},{\"end\":63720,\"start\":63714},{\"end\":63727,\"start\":63720},{\"end\":63737,\"start\":63727},{\"end\":63744,\"start\":63737},{\"end\":64142,\"start\":64136},{\"end\":64149,\"start\":64142},{\"end\":64155,\"start\":64149},{\"end\":64162,\"start\":64155},{\"end\":64168,\"start\":64162},{\"end\":64175,\"start\":64168},{\"end\":64489,\"start\":64480},{\"end\":64495,\"start\":64489},{\"end\":64502,\"start\":64495},{\"end\":64509,\"start\":64502},{\"end\":64803,\"start\":64797},{\"end\":64810,\"start\":64803},{\"end\":64818,\"start\":64810},{\"end\":64826,\"start\":64818},{\"end\":65143,\"start\":65135},{\"end\":65151,\"start\":65143},{\"end\":65158,\"start\":65151},{\"end\":65165,\"start\":65158},{\"end\":65171,\"start\":65165},{\"end\":65520,\"start\":65513},{\"end\":65537,\"start\":65520},{\"end\":65551,\"start\":65537},{\"end\":65914,\"start\":65907},{\"end\":65931,\"start\":65914},{\"end\":65945,\"start\":65931},{\"end\":66284,\"start\":66275},{\"end\":66296,\"start\":66284},{\"end\":66306,\"start\":66296},{\"end\":66316,\"start\":66306},{\"end\":66614,\"start\":66606},{\"end\":66621,\"start\":66614},{\"end\":66628,\"start\":66621},{\"end\":66636,\"start\":66628},{\"end\":66994,\"start\":66987},{\"end\":67002,\"start\":66994},{\"end\":67010,\"start\":67002},{\"end\":67320,\"start\":67313},{\"end\":67327,\"start\":67320},{\"end\":67335,\"start\":67327},{\"end\":67343,\"start\":67335},{\"end\":67621,\"start\":67612},{\"end\":67633,\"start\":67621},{\"end\":67646,\"start\":67633},{\"end\":67654,\"start\":67646},{\"end\":68043,\"start\":68035},{\"end\":68050,\"start\":68043},{\"end\":68059,\"start\":68050},{\"end\":68069,\"start\":68059},{\"end\":68328,\"start\":68318},{\"end\":68339,\"start\":68328},{\"end\":68350,\"start\":68339},{\"end\":68358,\"start\":68350},{\"end\":68646,\"start\":68636},{\"end\":68655,\"start\":68646},{\"end\":68667,\"start\":68655},{\"end\":68677,\"start\":68667},{\"end\":68685,\"start\":68677},{\"end\":68695,\"start\":68685},{\"end\":68702,\"start\":68695},{\"end\":68715,\"start\":68702},{\"end\":68725,\"start\":68715},{\"end\":68734,\"start\":68725},{\"end\":68982,\"start\":68970},{\"end\":68988,\"start\":68982},{\"end\":69218,\"start\":69205},{\"end\":69227,\"start\":69218},{\"end\":69464,\"start\":69457},{\"end\":69476,\"start\":69464},{\"end\":69483,\"start\":69476},{\"end\":69781,\"start\":69773},{\"end\":69787,\"start\":69781},{\"end\":69793,\"start\":69787},{\"end\":69801,\"start\":69793},{\"end\":69807,\"start\":69801},{\"end\":69815,\"start\":69807},{\"end\":70170,\"start\":70163},{\"end\":70178,\"start\":70170},{\"end\":70186,\"start\":70178},{\"end\":70192,\"start\":70186}]", "bib_venue": "[{\"end\":67803,\"start\":67737},{\"end\":48960,\"start\":48895},{\"end\":49340,\"start\":49275},{\"end\":49722,\"start\":49663},{\"end\":50061,\"start\":50021},{\"end\":50347,\"start\":50291},{\"end\":50670,\"start\":50601},{\"end\":51150,\"start\":51066},{\"end\":51565,\"start\":51500},{\"end\":51943,\"start\":51878},{\"end\":52337,\"start\":52272},{\"end\":52705,\"start\":52640},{\"end\":52980,\"start\":52933},{\"end\":53306,\"start\":53222},{\"end\":53690,\"start\":53625},{\"end\":54009,\"start\":53969},{\"end\":54288,\"start\":54233},{\"end\":54617,\"start\":54572},{\"end\":54997,\"start\":54943},{\"end\":55402,\"start\":55347},{\"end\":55747,\"start\":55684},{\"end\":56106,\"start\":56061},{\"end\":56382,\"start\":56337},{\"end\":56680,\"start\":56613},{\"end\":57059,\"start\":56994},{\"end\":57446,\"start\":57376},{\"end\":57868,\"start\":57804},{\"end\":58243,\"start\":58183},{\"end\":58598,\"start\":58528},{\"end\":58930,\"start\":58886},{\"end\":59189,\"start\":59131},{\"end\":59446,\"start\":59391},{\"end\":59758,\"start\":59694},{\"end\":60104,\"start\":60050},{\"end\":60450,\"start\":60391},{\"end\":60811,\"start\":60746},{\"end\":61103,\"start\":61032},{\"end\":61408,\"start\":61363},{\"end\":61746,\"start\":61701},{\"end\":62163,\"start\":62109},{\"end\":62571,\"start\":62506},{\"end\":63005,\"start\":62940},{\"end\":63454,\"start\":63389},{\"end\":63805,\"start\":63744},{\"end\":64230,\"start\":64175},{\"end\":64578,\"start\":64509},{\"end\":64881,\"start\":64826},{\"end\":65230,\"start\":65171},{\"end\":65621,\"start\":65551},{\"end\":66010,\"start\":65945},{\"end\":66365,\"start\":66316},{\"end\":66701,\"start\":66636},{\"end\":66985,\"start\":66887},{\"end\":67311,\"start\":67209},{\"end\":67735,\"start\":67654},{\"end\":68119,\"start\":68085},{\"end\":68423,\"start\":68358},{\"end\":68770,\"start\":68734},{\"end\":69047,\"start\":68988},{\"end\":69293,\"start\":69243},{\"end\":69548,\"start\":69483},{\"end\":69880,\"start\":69815},{\"end\":70237,\"start\":70192}]"}}}, "year": 2023, "month": 12, "day": 17}