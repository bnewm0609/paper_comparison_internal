{"id": 257019800, "updated": "2023-10-05 04:04:09.725", "metadata": {"title": "Consistent Diffusion Models: Mitigating Sampling Drift by Learning to be Consistent", "authors": "[{\"first\":\"Giannis\",\"last\":\"Daras\",\"middle\":[]},{\"first\":\"Yuval\",\"last\":\"Dagan\",\"middle\":[]},{\"first\":\"Alexandros\",\"last\":\"Dimakis\",\"middle\":[\"G.\"]},{\"first\":\"Constantinos\",\"last\":\"Daskalakis\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Imperfect score-matching leads to a shift between the training and the sampling distribution of diffusion models. Due to the recursive nature of the generation process, errors in previous steps yield sampling iterates that drift away from the training distribution. Yet, the standard training objective via Denoising Score Matching (DSM) is only designed to optimize over non-drifted data. To train on drifted data, we propose to enforce a \\emph{consistency} property which states that predictions of the model on its own generated data are consistent across time. Theoretically, we show that if the score is learned perfectly on some non-drifted points (via DSM) and if the consistency property is enforced everywhere, then the score is learned accurately everywhere. Empirically we show that our novel training objective yields state-of-the-art results for conditional and unconditional generation in CIFAR-10 and baseline improvements in AFHQ and FFHQ. We open-source our code and models: https://github.com/giannisdaras/cdm", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2302.09057", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2302-09057", "doi": "10.48550/arxiv.2302.09057"}}, "content": {"source": {"pdf_hash": "c8abd95ec7dfc6ed81ea27f577d3570c3896bcb1", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2302.09057v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "ac06b0d717e0c7478ec73d8d0083c9e2c5d558af", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/c8abd95ec7dfc6ed81ea27f577d3570c3896bcb1.txt", "contents": "\nConsistent Diffusion Models: Mitigating Sampling Drift by Learning to be Consistent\nFebruary 20, 2023\n\nGiannis Daras \nYuval Dagan \nAlexandros G Dimakis \nConstantinos Daskalakis \n\nDepartment of Computer Science\nElectrical Engineering and Computer Science\nDepartment of ECE\nUniversity of Texas at Austin\nUniversity of California\nBerkeley\n\n\nElectrical Engineering and Computer Science\nUniversity of Texas at Austin\nMassachusetts Institute of Technology\n\n\nConsistent Diffusion Models: Mitigating Sampling Drift by Learning to be Consistent\nFebruary 20, 2023\nImperfect score-matching leads to a shift between the training and the sampling distribution of diffusion models. Due to the recursive nature of the generation process, errors in previous steps yield sampling iterates that drift away from the training distribution. Yet, the standard training objective via Denoising Score Matching (DSM) is only designed to optimize over non-drifted data. To train on drifted data, we propose to enforce a consistency property which states that predictions of the model on its own generated data are consistent across time. Theoretically, we show that if the score is learned perfectly on some non-drifted points (via DSM) and if the consistency property is enforced everywhere, then the score is learned accurately everywhere. Empirically we show that our novel training objective yields state-ofthe-art results for conditional and unconditional generation in CIFAR-10 and baseline improvements in AFHQ and FFHQ. We open-source our code and models: https://github.com/giannisdaras/cdm.\n\nIntroduction\n\nThe diffusion-based (Sohl-Dickstein et al., 2015;Song and Ermon, 2019;Ho et al., 2020) approach to generative models has been successful across various modalities, including images (Ramesh et al., 2022;Saharia et al., 2022;Dhariwal and Nichol, 2021;Nichol and Dhariwal, 2021;Kim et al., 2022;Song et al., 2021b;Ruiz et al., 2022;Gal et al., 2022;Daras and Dimakis, 2022;Daras et al., 2022a), videos (Ho et al., 2022a,b;Hong et al., 2022), audio (Kong et al., 2021), 3D structures (Poole et al., 2022), proteins (Anand and Achim, 2022;Trippe et al., 2022;Schneuing et al., 2022;Corso et al., 2022), and medical applications (Jalal et al., 2021;Arvinte et al., 2022).\n\nDiffusion models generate data by first drawing a sample from a noisy distribution and slowly denoising this sample to ultimately obtain a sample from the target distribution. This is achieved by sampling, in reverse from time t = 1 down to t = 0, a stochastic process {x t } t\u2208[0,1] wherein x 0 is distributed according to the target distribution p 0 and, for all t,\n\nx t \u223c p t where p t := p 0 \u2295 N (0, \u03c3 2 t I d ).\n\n(1)\n\nThat is, p t is the distribution resulting from corrupting a sample from p 0 with noise sampled from N (0, \u03c3 2 t I d ), where \u03c3 t is an increasing function such that \u03c3 0 = 0 and \u03c3 1 is sufficiently large so that p 1 is nearly indistinguishable from pure noise. We note that diffusion models have been generalized to other types of corruptions by the recent works of Daras et al. (2022b); Bansal et al. (2022); Hoogeboom and Salimans (2022); Deasy et al. (2021); Nachmani et al. (2021).\n\nIn order to sample from a diffusion model, i.e. sample the afore-described process in reverse time, it suffices to know the score function s(x, t) = \u2207 x log p(x, t), where p(x, t) is the density of x t \u223c p t . Indeed, given a sample x t \u223c p t , one can use the score function at x t , i.e. s(x t , t), to generate a sample from p t\u2212dt by taking an infinitesimal step of a stochastic or an ordinary differential equation (Song et al., 2021b,a), or by using Langevin dynamics (Grenander and Miller, 1994;Song and Ermon, 2020). 1 Hence, in order to train a diffusion model to sample from a target distribution of interest p * 0 it suffices to learn the score function s * (x, t) using samples from the corrupted distributions p * t resulting from p * 0 and a particular noise schedule \u03c3 t . Notice that those samples can be easily drawn given samples from p * 0 .\n\nThe Sampling Drift Challenge: Unfortunately the true score function s * (x, t) is not perfectly learned during training. Thus, at generation time, the samples x t drawn using the learned score function, s(x, t), in the ways discussed above, drift astray in distribution from the true corrupted distributions p * t . This drift becomes larger for smaller t due to compounding of errors and is accentuated by the fact that the further away a sample x t is from the likely support of the true p * t the larger is also the error s(x t , t) \u2212 s * (x t , t) between the learned and the true score function at x t , which feeds into an even larger drift between the distribution of x t from p * t for t < t; see e.g. (Sehwag et al., 2022;Ho et al., 2020;Nichol and Dhariwal, 2021;Chen et al., 2022a). These challenges motivate the question:\n\nQuestion 1. How can one train diffusion models to improve the error s(x, t) \u2212 s * (x, t) between the learned and true score function on inputs (x, t) where x is unlikely under the target noisy distribution p * t ? A direct approach to this challenge is to train our model to minimize the afore-described error on pairs (x, t) where x is sampled from distributions other than p * t . However, there is no straightforward way to do so, because we do not have direct access to the values of the true score function s * (x, t).\n\nThis motivates us to propose a novel training method to mitigate sampling drift by enforcing that the learned score function satisfies an invariant, that we call \"consistency property.\" This property relates multiple inputs to s(\u00b7, \u00b7) and can be optimized without using any samples from the target distribution p * 0 . As we will show theoretically, enforcing this consistency in conjunction with minimizing a very weakened form of the standard score matching objective (for a single t and an open set of x's) suffices to learn the correct score everywhere. We also provide experiments illustrating that regularizing the standard score matching objective using our consistency property leads to state-of-the-art models.\n\nOur Approach: The true score function s * (x, t) is closely related to another function, called optimal denoiser, which predicts a clean sample x 0 \u223c p * 0 from a noisy observation x t = x 0 + \u03c3 t \u03b7 where the noise is \u03b7 \u223c N (0, I d ). The optimal denoiser (under the 2 loss) is the conditional expectation:\nh * (x, t) := E[x 0 | x t = x],\nand the true score function can be obtained from the optimal denoiser as follows: s * (x, t) = (h * (x, t) \u2212 x)/\u03c3 2 t . Indeed, the standard training technique, via score-matching, explicitly trains for the score through the denoiser h * (Vincent, 2011;Efron, 2011;Meng et al., 2021;Kim and Ye, 2021;Luo, 2022).\n\nWe are now ready to state our consistency property. We will say that a (denoising\n) function h(x, t) is consistent iff \u2200t, \u2200x : E[x 0 |x t = x] = h(x, t),\nwhere the expectation is with respect to a sample from the learned reverse process, defined in terms of the implied score function s(x, t) = (h(x, t) \u2212 x)/\u03c3 2 t , when this is initialized at x t = x and run backwards in time to sample x 0 . See Eq. (3) for the precise stochastic differential equation and its justification. In particular, h is called consistent if the prediction h(x, t) of the conditional expectation of the clean image x 0 given x t = x equals the expected value of an image that is generated by the learned reversed process, starting from x t = x.\n\nWhile there are several other properties that the score function of a diffusion process must satisfy, e.g. the Fokker-Planck equation (Lai et al., 2022), our first theoretical result is that the consistency of h(x, t) suffices (in conjunction with the conservativeness of its score function s(x, t) = (h(x, t) \u2212 x)/\u03c3 2 t ) to guarantee that s must be the score function of a diffusion process (and must thus satisfy any other property that a diffusion process must satisfy). If additionally s(x, t) equals the score function s * (x, t) of a target diffusion process at a single time t = t 0 and an open subset of x \u2208 R d , then it equals s * everywhere. Intuitively, this suggests that learning the score in-sample for a single t = t 0 , and satisfying the consistency and conservativeness properties off-sample, also yields a correct estimate off-sample. This can be summarized as follows below:\n\nTheorem 1.1 (informal). If some denoiser h(x, t) is consistent and its corresponding score function s(x, t) = (h(x, t) \u2212 x)/\u03c3 2 t is a conservative field, then s(x, t) is the score function of a diffusion process, i.e. the generation process using score function s is the inverse of a diffusion process. If additionally s(x, t) = s * (x, t) for a single t = t 0 and all x in an open subset of R d , where s * is the score function of a target diffusion process, then s(x, t) = s * (x, t) everywhere, i.e. to learn the score function everywhere it suffices to learn it for a single t 0 and an open subset of x's.\n\nWe propose a loss function to train for the consistency property and we show experimentally that regularizing the standard score matching objective using our consistency property leads to better models.\n\n\nSummary of Contributions:\n\n1. We identify an invariant property, consistency of the denoiser h, that any perfectly trained model should satisfy.\n\n2. We prove that if the denoiser h(x, t) is consistent and its implied score function s(\nx, t) = (h(x, t) \u2212 x)/\u03c3 2 t\nis a conservative field, then s(x, t) is the score function of some diffusion process, even if there are learning errors with respect to the score of the target process, which generates the training data.\n\n3. We prove that if these two properties are satisfied, then optimizing perfectly the score for a single t = t 0 and an open subset S \u2286 R d , guarantees that the score is learned perfectly everywhere.\n\n4. We propose a novel training objective that enforces the consistency property. Our new objective optimizes the network to have consistent predictions on data points from the learned distribution.\n\n5. We show experimentally that, paired with the original Denoising Score Matching (DSM) loss, our objective achieves a new state-of-the-art on conditional and unconditional generation in CIFAR10 and baseline improvements in AFHQ and FFHQ.\n\n6. We open-source our code and models: https://github.com/giannisdaras/cdm.\n\n\nBackground\n\nDiffusion processes, score functions and denoising. Diffusion models are trained by solving a supervised regression problem (Song and Ermon, 2019;Ho et al., 2020). The function that one aims to learn, called the score function, defined below, is equivalent (up to a linear transformation) to a denoising function (Efron, 2011;Vincent, 2011), whose goal is to denoise an image that was injected with noise. In particular, for some target distribution p 0 , one's goal is to learn the following function h :\nR d \u00d7 [0, 1] \u2192 R d : h(x, t) = E[x 0 | x t = x]; x 0 \u223c p 0 , x t \u223c N (x 0 , \u03c3 2 t I d ).(2)\nIn other words, the goal is to predict the expected \"clean\" image x 0 given a corrupted version of it, assuming that the image was sampled from p 0 and its corruption was done by adding to it noise from N (0, \u03c3 2 t I d ), where \u03c3 2 t is a non-negative and increasing function of t. Given such a function h, we can generate samples from p 0 by solving a Stochastic Differential Equation (SDE) that depends on h (Song et al., 2021b). Specifically, one starts by sampling x 1 from some fixed distribution and then runs the following SDE backwards in time:\ndx t = \u2212g(t) 2 h(x t , t) \u2212 x t \u03c3 2 t dt + g(t)dB t ,(3)\nwhere B t is a reverse-time Brownian motion and g(t) 2 = d\u03c3 2 t dt . To explain how Eq. (3) was derived, consider the forward SDE that starts with a clean image x 0 and slowly injects noise:\ndx t = g(t)dB t , x 0 \u223c p 0 .(4)\nWe notice here that the x t under Eq. (4) is N (x 0 , \u03c3 2 t I d ), where x 0 \u223c p 0 , so it has the same distribution that it has in Eq. (2). Remarkably, such SDEs are reversible in time (Anderson, 1982). Hence, the diffusion process of Eq. (4) can be viewed as a reversed-time diffusion:\ndx t = \u2212g(t) 2 \u2207 x log p(x t , t)dt + g(t)dB t ,(5)\nwhere p(x t , t) is the density of x t at time t. We note that s(x, t) := \u2207 x log p(x, t) is called the score function of x t at time t. Using Tweedie's lemma (Efron, 2011), one obtains the following relationship between the denoising function h and the score function:\n\u2207 x log p(x, t) = h(x, t) \u2212 x \u03c3 2 t .(6)\nSubstituting Eq. (6) in Eq. (5), one obtains Eq. (3).\n\nTraining via denoising score matching. The standard way to train for h is via denoising score matching. This is performed by obtaining samples of x 0 \u223c p 0 and x t \u223c N (x 0 , \u03c3 2 t I d ) and training to minimize\nE x0\u223cp0,xt\u223cN (x0,\u03c3 2 t I d ) L 1 t,xt,x0 (\u03b8) = E x0\u223cp0,xt\u223cN (x0,\u03c3 2 t I d ) h \u03b8 (x t , t) \u2212 x 0 2 ,\nwhere the optimization is over some family of functions, {h \u03b8 } \u03b8\u2208\u0398 . It was shown by Vincent (2011) that optimizing Eq. (2) is equivalent to optimizing h in mean-squared-error on a random point x t that is a noisy image,\nx t \u223c N (x 0 , \u03c3 2 t I d ) where x 0 \u223c p 0 : E xt h \u03b8 (x t , t) \u2212 h * (x t , t) 2 ,\nwhere h * is the true denoising function from Eq. (2).\n\n\nTheory\n\nWe define below the consistency property that a function h should satisfy. This states that the output of h(x, t) (which is meant to approximate the conditional expectation of x 0 conditioned on x t = x) is indeed consistent with the average point x 0 generated using h and conditioning on x t = x. Recall from the previous section that generation according to h conditionning on x t = x is done by running the following SDE backwards in time conditioning on x t = x:\ndx t = \u2212g(t) 2 h(x t , t) \u2212 x t \u03c3 2 t dt + g(t) 2 dB t ,(7)\nThe consistency property is therefore defined as follows:\nProperty 1 (Consistency.). A function h : R d \u00d7 [0, 1] \u2192 R d is said to be consistent iff for all t \u2208 (0, 1] and all x \u2208 R d , h(x, t) = E h [x 0 | x t = x],(8)\nwhere E h [x 0 | x t = x] corresponds to the conditional expectation of x 0 in the process that starts with x t = x and samples x 0 by running the SDE of Eq. (7) backwards in time (where note that the SDE uses h).\n\nThe following Lemma states that Property 1 holds if and only if the model prediction, h(x, t), is consistent with the average output of h on samples that are generated using h and conditioning on x t = x, i.e. that h(x t , t) is a reverse-Martingale under the same process of Eq. (7).\n\nLemma 3.1. Property 1 holds if and only if the following two properties hold:\n\n\u2022 The function h is a reverse-Martingale, namely: for all t > t and for any x:\nh(x, t) = E h [h(x t , t ) | x t = x],\nwhere the expectation is over x t that is sampled according to Eq. (7) with the same function h, given the initial condition x t = x.\n\u2022 For all x \u2208 R d , h(x, 0) = x.\nThe proof of this Lemma is included in the Appendix. Further, we introduce one more property that will be required for our theoretical results: the learned vector-field should be conservative.\nProperty 2 (Conservative vector field / Score Property.). Let h : R d \u00d7 [0, 1] \u2192 R d .\nWe say that h induces a conservative vector field (or that is satisfies the score property) if for any t \u2208 (0, 1] there exists some probability density p(\u00b7, t) such that\nh(x, t) \u2212 x \u03c3 2 t = \u2207 log p(x, t).\nWe note that the optimal denoiser, i.e. h defined as in Eq.\n\n(2) satisfies both of the properties we introduced. In the paper, we will focus on enforcing the consistency property and we are going to assume conservativeness for our theoretical results. This assumption can be relieved to hold only at a single t \u2208 (0, 1] using results of Lai et al. (2022).\n\nNext, we show the theoretical consequences of enforcing Properties 1 and 2. First, we show that this enforces h to indeed correspond to a denoising function, namely, h satisfies Eq. (2) for some distribution p 0 over x 0 . Yet, this does not imply that p 0 is the correct underlying distribution that we are trying to learn. Indeed, these properties can apply to any distribution p 0 . Yet, we can show that if we learn h correctly for some inputs and if these properties apply everywhere then h is learned correctly everywhere. 2. Assume that h satisfies Properties 1 and 2. Further, let h * be another function that corresponds to Eq.\n\n(2) with some initial distribution p * 0 . Assume that h = h * on some open set U \u2286 R d and some fixed\nt 0 \u2208 (0, 1], namely, h(x, t 0 ) = h * (x, t 0 ) for all x \u2208 U . Then, h * (x, t) = h(x, t)\nfor all x and all t.\n\nProof overview. We start with the first part of the theorem. We assume that h satisfies Properties 1 and 2 and we will show that h is defined by Eq. (2) for some distribution p 0 (while the other direction in the equivalence follows trivially from the definitions of these properties). Motivated by Eq. (6), define the function s : R d \u00d7 (0, 1] according to\ns(x, t) = h(x, t) \u2212 x \u03c3 2 t .(9)\nWe will first show that s satisfies the partial differential equation\n\u2202s \u2202t = g(t) 2 J s s + 1 2 s ,(10)\nwhere J s \u2208 R d\u00d7d is the Jacobian of s, (J s ) ij = \u2202si xj and each coordinate i of s \u2208 R d is the Laplacian of coordinate i of s, ( s) i = n j=1 \u2202 2 si \u2202x 2 j . In order to obtain Eq. (10), first, we use a generalization of Ito's lemma, which states that for an SDE\ndx t = \u00b5(x t , t)dt + g(t)dB t x(11)\nand for f :\nR d \u00d7 [0, 1] \u2192 R d , f (x t , t) satisfies the SDE df (x t , t) = \u2202f \u2202t + J f \u00b5 \u2212 g(t) 2 2 f dt + g(t)J f dB t .\nIf f is a reverse-Martingale then the term that multiplies dt has to equal zero, namely,\n\u2202f \u2202t + J f \u00b5 \u2212 g(t) 2 2 f = 0. By Lemma 3.1, h(x t , t)\nis a reverse Martingale, therefore we can substitute f = h and substitute \u00b5 = \u2212g(t) 2 s according to Eq. (7), to deduce that\n\u2202h \u2202t \u2212 g(t) 2 J h s \u2212 g(t) 2 2 h = 0.\nSubstituting h(x, t) = \u03c3 2 t s(x, t) + x according to Eq. (6) yields Eq. (10) as required. Next, we show that any s that is the score-function (i.e. gradient of log probability) of some diffusion process that follows the SDE Eq. (4), also satisfies Eq. (10). To obtain this, one can use the Fokker-Planck equation, whose special case states that the density function p(x, t) of any stochastic process that satisfies the SDE Eq. (4) satisfies the PDE \u2202p \u2202t = g(t) 2 2 p where corresponds to the Laplacian operator. Using this one can obtain a PDE for \u2207 x log p which happens to be exactly Eq. (10) if the process is defined by Eq. (4).\n\nNext, we use Property 2 to deduce that there exists some densities p(\u00b7, t) for t \u2208 [0, 1] such that\ns(x, t) = h(x, t) \u2212 x \u03c3 2 t = \u2207 x log p(x, t).\nDenote by p (x, t) the score function of the diffusion process that is defined by the SDE of Eq. (4) with the initial condition that p(x, 0) = p (x, 0) for all x. Denote by s (x, t) = \u2207 x log p (x, t) the score function of p . As we proved above, both s and s satisfy the PDE Eq. (10) and the same initial condition at t = 0. By the uniqueness of the PDE, it holds that s(x, t) = s (x, t) for all t \u2265 t 0 . Denote by h * the function that satisfies Eq.\n\n(2) with the initial condition x 0 \u223c p 0 . By Eq. (6),\ns (x, t) = h * (x, t) \u2212 x \u03c3 2 t .\nBy Eq. (9) and since s = s , it follows that h = h * and this is what we wanted to prove. We proceed with proving part 2 of the theorem. We use the notion of an analytic function on R d : that is a function f : R d \u2192 R such that at any x 0 \u2208 R d , the Taylor series of f centered at x 0 converges for all x \u2208 R d to f (x). We use the property that an analytic function is uniquely determined by its value on any open subset: If f and g are analytic functions that identify in some open subset U \u2282 R d then f = g everywhere. We prove this statement in the remainder of this paragraph, as follows: Represent f and g as Taylor series around some x 0 \u2208 U . The Taylor series of f and g identify: indeed, these series are functions of the derivatives of f and g which are functions of only the values in U . Since f and g equal their Taylor series, they are equal.\n\nNext, we will show that for any diffusion process that is defined by Eq. (4), the probability density of p(x, t 0 ) at any time t 0 > 0 is analytic as a function of x. Recall that the distribution of x 0 is defined in Eq. (4) as p 0 and it holds that the distribution of x t0 is obtained from p 0 by adding a Gaussian noise N (0, \u03c3 2 t I) and its density at any x equals\np(x, t 0 ) = a\u2208R d 1 \u221a 2\u03c0\u03c3 t0 exp \u2212 (x \u2212 a) 2 2\u03c3 2 t dp 0 (a).\nSince the function exp(\u2212(x \u2212 a) 2 /(2\u03c3 2 t )) is analytic, one could deduce that p(x, t 0 ) is also analytic. Further, p(x, t 0 ) > 0 for all x which implies that there is no singularity for log p(x, t 0 ) which can be used to deduce that log p(x, t 0 ) is also analytic and further that \u2207 x log p(x, t 0 ) is analytic as well.\n\nWe use the first part of the theorem to deduce that s is the score function of some diffusion process hence it is analytic. By assumption, s identifies with some target score function s * in some open subset U \u2286 R d at some t 0 , which, by the fact that s(x, t 0 ) and s * (x, t 0 ) are analytic, implies that s(x, t 0 ) = s * (x, t 0 ) for all x. Finally, since s and s * both satisfy the PDE Eq. (10) and they satsify the same initial condition at t 0 , it holds that by uniqueness of the PDE s(x, t) = s * (x, t) for all x and t.\n\n\nMethod\n\nTheorem 3.2 motivates enforcing the consistency property on the learned model. We notice that the consistency equation Eq. (8) may be expensive to train for, because it requires one to generate whole trajectories. Rather, we use the equivalent Martingale assumption of Lemma 3.1, which can be observed locally with only partial trajectories: 2 We suggest the following loss function, for some fixed t, t and x:\nL 2 t,t ,x (\u03b8) = (E \u03b8 [h \u03b8 (x t , t ) | x t = x] \u2212 h \u03b8 (x, t)) 2 /2, where the expectation E \u03b8 [\u00b7 | x t = x]\nis taken according to process Eq. (7) parameterized by h \u03b8 with the initial condition x t = x. Differentiating this expectation, one gets the following (see Section B.1 for full derivation):\n\u2207L 2 t,t ,x (\u03b8) = E \u03b8 [h \u03b8 (x t , t ) \u2212 h \u03b8 (x t , t) | x t = x] E \u03b8 h \u03b8 (x t , t )\u2207 \u03b8 log (p \u03b8 (x t | x t = x)) + \u2207 \u03b8 h \u03b8 (x t , t ) \u2212 \u2207 \u03b8 h \u03b8 (x t , t) x t = x ,\nwhere p \u03b8 corresponds to the same probability measure where the expectation E \u03b8 is taken from and \u2207 \u03b8 h \u03b8 corresponds to the Jacobian matrix of h \u03b8 where the derivatives are taken with respect to \u03b8. Notice, however, that computing the expectation accurately might require a large number of samples. Instead, it is possible to obtain a stochastic gradient of this target by taking two samples, x t and x t , independently, from the conditional distribution of x t conditioned on x t = x and replace each of the two expectations in the formula above with one of these two samples. We further notice the gradient of the consistency loss can be written as\n\u2207 \u03b8 L 2 t,t ,x (\u03b8) = 1 2 \u2207 \u03b8 E \u03b8 [h \u03b8 (x t , t )] \u2212 h \u03b8 (x, t) 2 + E \u03b8 [h \u03b8 (x t , t ) \u2212 h \u03b8 (x, t)] E \u03b8 []\u2207 \u03b8 log (p(x t )) h \u03b8 (x t , t )]\nIn order to save on computation time, we trained by taking gradient steps with respect to only the first summand in this decomposition and notice that if the consistency property is preserved then this term becomes zero, which implies that no update is made, as desired. It remains to determine how to select t, t and x t . Notice that t has to vary throughout the whole range of [0, 1] whereas t can either vary over [0, t], however, it sufficient to take t \u2208 [t \u2212 , t]. However, the further away t and t are, we need to run more steps of the reverse SDE to avoid large discretization errors. Instead, we enforce the property only on small time windows using that consistency over small intervals implies global consistency. We notice that x t can be chosen arbitrarily and two possible choises are to sample it from the target noisy distribution p t or from the model.\n\nRemark 4.1. It is important to sample x t conditioned on x t according to the specific SDE Eq. (7). While a variety of alternative SDEs exist which preserve the same marginal distribution at any t, they might not preseve the conditionals.\n\n\nExperiments\n\nFor all our experiments, we rely on the official open-sourced code and the training and evaluation hyperparameters from the paper \"Elucidating the Design Space of Diffusion-Based Generative Models\" (Karras et al., 2022) that, to the best of our knowledge, holds the current state-of-the-art on conditional generation on CIFAR-10 and unconditional generation on CIFAR-10, AFHQ (64x64 resolution), FFHQ (64x64 resolution). We refer to the models trained with our regularization as \"CDM (Ours)\" and to models trained with vanilla Denoising Score Matching (DSM) as \"EDM\" models. \"CDM\" models are trained with the weighted objective: Finally, for completeness, we also report scores from the models of Song et al. (2021b), following the practice of the EDM paper. We refer to the latter baselines as \"NCSNv3\" baselines. We train diffusion models, with and without our regularization, for conditional generation on CIFAR-10 and unconditional generation on CIFAR-10 and AFHQ (64x64 resolution). For the re-trained models on CIFAR-10, we use exactly the same training hyperparameters as in Karras et al. (2022) and we verify that our re-trained models match (within 1%) the FID numbers mentioned in the paper. For AFHQ, we had to drop the batch size from the suggested value of 512 to 256 to fit in memory, which increased the FID from 1.96 (reported value) to 2.29. All models were trained for 200k iterations, as in Karras et al. (2022). Finally, we retrain a baseline model on FFHQ for 150k iterations and we finetune it for 5k steps using our proposed objective.\nL ours \u03bb (\u03b8) = E t E x0\u223cp0,xt\u223cN (x0,\u03c3 2 t I d ) L 1 t,xt,x0 (\u03b8) + \u03bbE xt\u223cpt E t \u223cU [t\u2212 ,t] L 2 t,t ,xt (\u03b8) ,\nImplementation Choices and Computational Requirements. As mentioned, when enforcing the Consistency Property, we are free to choose t anywhere in the interval [0, t]. When t, t are far away, sampling x t from the distribution p \u03b8 t (x t |x t ) requires many sampling steps (to reduce discretization errors). Since this needs to be done for every Gradient Descent update, the training time increases significantly. Instead, we notice that local consistency implies global consistency. Hence, we first fix the number of sampling steps to run in every training iteration and then we sample t uniformly in the interval [t \u2212 , t] for some specified . For all our experiments, we fix the number of sampling steps to 6 which roughly increases the training time needed by 1.5x. We train all our models on a DGX server with 8 A100 GPUs with 80GBs of memory each.\n\n\nConsistency Property Testing\n\nWe are now ready to present our results. The first thing that we check is whether regularizing for the Consistency Property actually leads to models that are more consistent. Specifically, we want to check that the model trained with L ours \u03bb achieves lower consistency error, i.e. lower L 2 t,t ,xt . To check this, we do the following two tests: i) we fix t = 1 and we show how L 2 t,t ,xt changes as t changes in [0, 1], ii) we fix t = 0 and we show how the loss is changing as you change t in [0, 1]. Intuitively, the first test shows how the violation of the consistency property splits across the sampling process and the second test shows how much you finally (t = 0) violate the property if the violation started at time t. The results are shown in Figures 1a,  1b, respectively, for the models trained on AFHQ. We include additional results for CIFAR-10, FFHQ in Figures 4, 5, 6, 7   Performance. We evaluate performance of the models trained from scratch. Following the methodology of Karras et al. (2022), we generate 150k images from each model and we report the minimum FID computed on three sets of 50k images each. We keep checkpoints during training and we report FID for 30k, 70k, 100k, 150k, 180k and 200k iterations in Table 1. We also report the best FID found for each model, after evaluating checkpoints every 5k iterations (i.e. we evaluate 40 models spanning 200k steps of training). As shown in the Table, the proposed consistency regularization yields improvements throughout the training. In the case of CIFAR-10 (conditional and unconditional) where the re-trained baseline was trained with exactly the same hyperparameters as the models in the EDM Karras et al. (2022) paper, our CDM models achieve a new state-of-the-art.\n\nWe further show that our consistency regularization can be applied on top of a pre-trained model. Specifically, we train a baseline EDM-VP model on FFHQ 64x64 for 150k using vanilla Denoising Score Matching. We then do 5k steps of finetuning, with and without our consistency regularization and we measure the FID score of both models. The baseline model achieves FID 2.68 while the model finetuned with consistency regularization achieves 2.61. This experiment shows the potential of applying our consistency regularization to pre-trained models, potentially even at large scale, e.g. we could apply this idea with text-to-image models such as Stable Diffusion Rombach et al. (2022). We leave this direction for future work.\n\nUncurated samples from our best models on AFHQ, CIFAR-10 and FFHQ are given in Figures 2a, 2b, 8. One benefit of the deterministic samplers is the unique identifiability property (Song et al., 2021b). Intuitively, this means that by using the same noise and the same deterministic sampler, we can directly compare visually models that might have been trained in completely different ways. We select a couple of images from Figure  2a (AFHQ generations) and we compare the generated images from our model with the ones from the EDM baseline for the same noises. The results are shown in Figure 3. As shown, the consistency regularization fixes several geometric inconsistencies for the picked images. We underline that the shown images are examples for   which consistency regularization helped and that potentially there are images for which the baseline models give more realistic results. Ablation Study for Theoretical Predictions. One interesting implication of Theorem 3.2 is that it suggests that we only need to learn the score perfectly on some fixed t 0 and then the consistency property implies that the score is learned everywhere (for all t and in the whole space). This motivates the following Figure 3: Visual comparison of EDM model (top) and CDM model (Ours, bottom) using deterministic sampling initiated with the same noise. As seen, the consistency regularization fixes several geometric inconsistencies and artifacts in the generated images.\n\nModel FID EDM (baseline) 5.81 CDM, all times t 5.45 CDM, for some t 6.59 CDM, for some t, early stopped sampling 14.52 experiment: instead of using as our loss the weighted sum of DSM and our consistency regularization for all t, we will not use DSM for t \u2264 t threshold , for some t threshold that we test our theory for.\n\nWe pick t threshold such that for 20% of the diffusion (on the side of clean images), we do not train with DSM. For the rest 80% we train with both DSM and our consistency regularization. Since this is only an ablation study, we train for only 10k steps on (conditional) CIFAR-10. We report FID numbers for three models: i) training with only DSM, ii) training with DSM and consistency regularization everywhere, iii) training with DSM for 80% of times t and consistency regularization everywhere. In our reported models, we also include FID of an early stopped sampling of the latter model, i.e. we do not run the sampling for t < t threshold and we just output h \u03b8 (x t threshold , t threshold ). The numbers are summarized in Table 2. As shown, the theory is predictive since early stopping the generation at time t gives significantly worse results than continuing the sampling through the times that were never explicitly trained for approximating the score (i.e. we did not use DSM for those times). That said, the best results are obtained by combining DSM and our consistency regularization everywhere, which is what we did for all the other experiments in the paper.\n\n\nRelated Work\n\nThe fact that imperfect learning of the score function introduces a shift between the training and the sampling distribution has been well known. Chen et al. (2022a) analyze how the l 2 error in the approximation of the score function propagates to Total Variation distance error bounds between the true and the learned distribution. Several methods for mitigating this issue have been proposed, but the majority of the attempts focus on changing the sampling process Song et al. (2021b); Karras et al. (2022);Jolicoeur-Martineau et al. (2021); Sehwag et al. (2022). A related work is the Analog-Bits paper Chen et al. (2022b) that conditions the model during training with past model predictions. Karras et al. (2022) discusses potential violations of invariances, such as the non-conservativity of the induced vector field, due to imperfect score matching. However, they do not formally test or enforce this property. Lai et al. (2022) study the problem of regularizing diffusion models to satisfy the Fokker-Planck equation. While we show in Theorem 3.2 that perfect conservative training enforces the Fokker-Planck equation, we notice that their training method is different: they suggest to enforce the equation locally by using the finite differences method to approximate the derivatives. Further, they do not train on drifted data. Instead, we notice that our consistency loss is well suited to handle drifted data since it operates across trajectories generated by the model. Finally, they show benchmark improvements on MNIST whereas we achieve state-of-the-art performance and benchmark improvements in more challenging datasets such as CIFAR-10 and AFHQ.\n\n\nConclusions and Future Work\n\nWe proposed a novel objective that enforces the trained network to have self-consistent predictions over time. We optimize this objective with points from the sampling distribution, effectively reducing the sampling drift observed in prior empirical works. Theoretically, we show that the consistency property implies that we are sampling from the reverse of some diffusion process. Together with the assumption that the network has learned perfectly the score for some time t 0 and some open set U , we can prove that the consistency property implies that we learn the score perfectly everywhere. Empirically, we use our objective to obtain state-of-the-art for CIFAR-10 and baseline improvements on AFHQ and FFHQ.\n\nThere are limitations of our method and several directions for future work. The proposed regularization increases the training time by approximately 1.5x. It would be interesting to explore how to enforce consistency in more effective ways in future work. Further, our method does not test nor enforce that the induced vector-field is conservative, which is a key theoretical assumption. Our method guarantees only indirectly improve the performance in the samples from the learned distribution by enforcing some invariant. Finally, our theoretical result assumes perfect learning of the score in some subset of R d . An important next step would be to understand how errors propagate if the score-function is only approximately learned. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. (2021b). Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations.\n\nTrippe, B. L., Yim, J., Tischer, D., Broderick, T., Baker, D., Barzilay, R., and Jaakkola, T. (2022). Diffusion probabilistic modeling of protein backbones in 3d for the motif-scaffolding problem. arXiv preprint arXiv:2206.04119. , P. (2011). A connection between score matching and denoising autoencoders. Neural computation, 23 (7):1661-1674.\n\n\nVincent\n\n\nA Proof of Theorem 3.2\n\nIn Section A.1 we present some preliminaries to the proof, in Section A.2 we include the proof, with proofs of some lemmas ommitted and in the remaining sections we prove these lemmas.\n\n\nA.1 Preliminaries\n\nPreliminaries on diffusion processes In the next definition we define for a function F : R d \u2192 R d its Jacobian J F , its divergence \u2207 \u00b7 F and its Laplacian F that is computed separately on each coordinate of F :\nDefinition A.1. Given a function F = (f 1 , . . . , f n ) : R d \u2192 R d , denote by J F : R d \u2192 R d\u00d7d its Jacobian: (J F ) ij = \u2202f i (x) \u2202x j .\nThe divergence of F is defined as\n\u2207 \u00b7 F (x) := n i=1 \u2202f i (x) \u2202x i .\nDenote by F : R d \u2192 R d the function whose ith entry is the Laplacian of f i :\n( F (x)) i = n j=1 \u2202 2 f i (x) \u2202x 2 j .\nIf F is a function of both x \u2208 R d and t \u2208 R, then J F , f and \u2207 \u00b7 F correspond to F as a function of x, whereas t is kept fixed. In particular,\n(J F (x, t)) ij = \u2202f i (x, t) \u2202x j , ( F (x, t)) i = n j=1 \u2202 2 f i (x, t) \u2202x 2 j , \u2207 \u00b7 F = n i=1 \u2202f i (x, t) \u2202x i .\nWe use the celebrated Ito's lemma and some of its immediate generalizations:\n\nLemma A.2 (Ito's Lemma). Let x t be a stochastic process x t \u2208 R d , that is defined by the following SDE:\ndx t = \u00b5(x t , t)dt + g(t)dB t ,\nwhere B t is a standard Brownian motion. Let f :\nR d \u00d7 R \u2192 R. Then, df (x t , t) = df dt + \u2207 x f \u00b5(x t , t) + g(t) 2 2 f dt + g(t)\u2207 x f dB t . Further, if F : R d \u00d7 R \u2192 R d is a multi-valued function, then dF (x t , t) = dF dt + J F \u00b5 + g(t) 2 2 F dt + g(t)J F dB t .\nLastly, if x t is instead defined with a reverse noise,\ndx t = \u00b5(x t , t)dt + g(t)dB t ,\nthen the multi-valued Ito's lemma is modified as follows:\ndF (x t , t) = dF dt + J F \u00b5 \u2212 g(t) 2 2 F dt + g(t)J F dB t .(12)\nLastly, we present the Fokker-Planck equation which states that the probability distribution that corresponds to diffusion processes satisfy a certain partial differential equation: Fokker-Planck equation). Let x t be defined by\nLemma A.3 (dx t = \u00b5(x t , t)dt + g(t)dB t ,\nwhere x t , \u00b5(x, t) \u2208 R d and B t is a Brownian motion in R d . Denote by p(x, t) the density at point x on time t. Then,\n\u2202 \u2202t p(x, t) = \u2212\u2207 \u00b7 (\u00b5(x, t)p(x, t)) + g(t) 2 2 p(x, t) = \u2212p\u2207 \u00b7 \u00b5 \u2212 \u00b5\u2207 \u00b7 p + g(t) 2 2 p.\nPreliminaries on analytic functions\nDefinition A.4. A function f : R d \u2192 R is analytic on R d if for any x 0 , x \u2208 R d , the\nTaylor series of f around x 0 , evaluated at x, converges to f (x). We say that F = (f 1 , . . . , f n ) : R d \u2192 R d is an analytic function if f i is analytic for all i \u2208 {1, . . . , n}.\n\nThe following holds:\nLemma A.5. If F, G : R d \u2192 R d are two analytic functions and if F = G for all x \u2208 U where U \u2286 R d , U = 0, is an open set, then F = G on all R d .\nThis is a well known result and a proof sketch was given in Section 3.\n\nThe heat equation. The following is a Folklore lemma on the uniqueness of the solutions to the heat equation:\n\nLemma A.6. Let p and p be two continuous functions on R d \u00d7 [t 0 , 1] that satisfy the heat equation\n\u2202p \u2202t = g(t) 2 2 p.(13)\nFurther, assume that p(\u00b7, t 0 ) = p (\u00b7, t 0 ). Then, p = p for all t \u2208 [t 0 , 1].\n\n\nA.2 Main proof\n\nIn what appears below we denote\ns(x, t) := h(x, t) \u2212 x \u03c3 2 t .(14)\nWe start by claiming that if h satisfies Property 1, then s satisfies the PDE Eq. (10): (proof in Section A.3)\n\nLemma A.7. Let h satisfy Property 1 and define s according to Eq. (14). Then, s satisfies Eq. (10).\n\nNext, we claim that the score function of any diffusion process satisfies the PDE Eq. (10): (proof in Section A.4) Lemma A.8. Let s be the score function of some diffusion process that is defined by Eq. (4). Then, s satisfies the PDE Eq. (10).\n\nTo complete the first part of the proof, denote by p(\u00b7, t) the probability distribution such that s(x, t) = \u2207 log p(x, t), whose existence follows from Property 2. We would like to argue that {p(\u00b7, t)} t\u2208(0,1] corresponds the probability density of the diffusion dx t = g(t)dB t .\n\nIt suffices to show that for any t 0 > 0, {p(\u00b7, t)} t\u2208(t0,1] corresponds to the same diffusion. To show the latter, let t 0 \u2208 (0, 1) and consider the diffusion process according to Eq. (15) with the initial condition that\n\nx t0 \u223c p(\u00b7, t 0 ). Denote its score function by s and notice that it satisfies the PDE Eq. (10) and the initial condition s (x, t 0 ) = \u2207 x log p(x, t 0 ) = s(x, t 0 ), where the first equality follows from the definition of a score function and the second from the construction of p(x, t 0 ). Further, recall that s(x, t) satisfies the same PDE Eq. (10) by Lemma A.3. Next we will show that s = s for all t \u2208 [t 0 , 1], and this will follow from the following lemma: (proof in Section A.5) Lemma A.9. Let s and s be two solutions for the PDE (10) on the domain R d \u00d7 [t 0 , 1] that satisfy the same initial condition at t 0 : s(x, t 0 ) = s (x, t 0 ) for all x. Further, assume that for all t \u2208 [t 0 , 1] there exist probability densities p(\u00b7, t) and p (\u00b7, t) such that s(x, t) = \u2207 x log p(x, t) and s (x, t) = \u2207 x log p (x, t) for all x.\nThen, s = s on all of R d \u00d7 [t 0 , 1].\nThen, by uniqueness of the PDE one obtains that s = s for all t \u2208 [t 0 , 1]. Hence, s is the score of a diffusion for all t \u2265 t 0 and this holds for any t 0 > 0, hence this holds for any t > 0. This concludes the proof of the first part of the theorem.\n\nFor the second part, let s * denote some score function of a diffusion process that satisfies Eq. (4). Assume that for some t 0 > 0 and some open subset U \u2286 R d , s = s * , namely s(x, t 0 ) = s * (x, t 0 ) for all t 0 > 0 and all x \u2208 U . First, we would like to argue that if s(x, t) is the score function of some diffusion process that satisfies Eq. (4), then for any t 0 > 0 it holds that s(x, t 0 ) is an analytic function (proof in Section A.6) Lemma A.10. Let x t obey the SDE Eq. (4) with the initial condition x 0 \u223c \u00b5 0 . Let t > 0 and let s(x, t) denote the score function of x t , namely, s(x, t) = \u2207 x log p(x, t) where p(x, t) is the density of x t . Assume that \u00b5 0 is a bounded-support distribution. Then, s(x, t) is an analytic function.\n\nSince both s and s * are scores of diffusion processes, then s(x, t 0 ) and s * (x, t 0 ) are analytic functions. Using the fact that s = s * on U \u00d7 {t 0 } and using Lemma A.5 we derive that s(x, t 0 ) = s * (x, t 0 ) for all x. Let p and p * denote the densities that correspond to the score functions s and s * and by definition of a score function, we obtain that for all x,\n\u2207 log p(x, t 0 ) = s(x, t 0 ) = s * (x, t 0 ) = \u2207 log p * (x, t 0 ),\nwhich implies, by integration, that log p(x, t 0 ) = log p * (x, t 0 ) + c for some constant c \u2208 R. However, c = 0. Indeed, 1 = p(x, t 0 )dx = e log p(x,t0) dx = e log p * (x,t0)+c dx = p * (x, t 0 )e c dx = e c , which implies that c = 0 as required. As a consequence, the following lemma implies that p(x, 0) = p * (x, 0) for all x (proof in Section A.7):\n\nLemma A.11. Let x t and y t be stochastic processes that follow Eq. (4) with initial conditions x 0 \u223c \u00b5 0 and y 0 \u223c \u00b5 0 and assume that \u00b5 0 and \u00b5 0 are bounded-support. Assume that for some t 0 > 0, x t0 and y t0 have the same distribution. Then, \u00b5 0 = \u00b5 0 .\n\nWithout loss of generality, one can replace 0 with anyt \u2208 (0, t 0 ), to obtain that p(x,t) = p * (x,t) for an\u1ef9 t \u2208 [0, t 0 ]. Now, p(x, t 0 ) is analytic, from Lemma A.5, hence it is continuous. Consequently, Lemma A.6 implies that p = p * in R d \u00d7 [t 0 , 1]. This concludes that p = p * in all the domain, which implies that s = \u2207 log p = \u2207 log p * = s * , as required.\n\n\nA.3 Proof of Lemma A.7\n\nWe use Ito's lemma, and in particular Eq. (12), to get a PDE for the function h(x t , t) where x t satisfies the stochastic process dx t = \u2212g(t) 2 s(x t , t)dt + g(t)dB t .\n\nIto's formula yields that\ndh(x t , t) = \u2202h \u2202t \u2212 g(t) 2 J F s \u2212 g(t) 2 2 h dt + \u03c3J h dB t .\nSince (h, s) satisfies Property 1 and using Lemma 3.1, h is a reverse martingale which implies that the term that multiplies dt has to equal zero. In particular, we have that\n\u2202h \u2202t \u2212 g(t) 2 J h s \u2212 g(t) 2 2 h = 0.(16)\nBy Eq. (14),\ns = h \u2212 x \u03c3 2 t .\nTherefore, h = x + \u03c3 2 t s. Substituting this in Eq. (16) and using the relation d\u03c3 2 t /dt = g(t) 2 that follows from Eq. (14), one obtains that\n0 = \u2202 \u2202t (x + \u03c3 2 t s) \u2212 g(t) 2 J x+\u03c3 2 t s s \u2212 g(t) 2 2 (x + \u03c3 2 t s) = g(t) 2 s + \u03c3 2 t \u2202s \u2202t \u2212 g(t) 2 (I + \u03c3 2 t J s )s \u2212 g(t) 2 \u03c3 2 t 2 s = \u03c3 2 t \u2202s \u2202t \u2212 g(t) 2 \u03c3 2 t J s s \u2212 g(t) 2 \u03c3 2 t 2 s.\nDividing by \u03c3 2 t , we get that\n\u2202s \u2202t \u2212 g(t) 2 J s s \u2212 g(t) 2 2 s = 0,\nwhich is what we wanted to prove.\n\n\nA.4 Proof of Lemma A.8\n\nWe present as a consequence of the Fokker-Plank equation (Lemma A.3) a PDE for the log density log p:\n\nLemma A.12. Let x t be defined by\ndx t = \u00b5(x t , t)dt + g(t)dB t .\nThen,\n\u2202 log p \u2202t = \u2212\u2207 \u00b7 \u00b5 \u2212 \u00b5\u2207 \u00b7 log p + g(t) 2 \u2207 log p 2 2 + g(t) 2 log p 2\nProof. We would like to replace the partial derivatives of p that appears in Lemma A.3 with the partial derivatives of log p. Using the formula \u2202 log p \u2202t = 1 p \u2202p \u2202t , one obtains that \u2202p \u2202t = p \u2202 log p \u2202t .\n\nSimilarly,\n\u2202p \u2202x i = p \u2202 log p \u2202x i(17)\nwhich also implies that \u2207p = p\u2207 log p, \u2207 \u00b7 p = p\u2207 \u00b7 log p.\n\nDifferentiating Eq. (17) again with respect to x i and applying Eq. (17) once more, one obtains that\n\u2202 2 p \u2202x 2 i = \u2202 \u2202x i p \u2202 log p \u2202x i = \u2202p \u2202x i \u2202 log p \u2202x i + p \u2202 2 log p \u2202x 2 i = p \u2202 log p \u2202x i 2 + \u2202 2 log p \u2202x 2 i .\nSumming over i, one obtains that\np = p n i=1 \u2202 log p \u2202x i 2 + \u2202 2 log p \u2202x 2 i = p \u2207 log p 2 + p log p.(18)\nSubstituting the partials derivatives of p inside the Fokker-Planck equation in Lemma A.3, one obtains that\np \u2202 log p \u2202t = \u2212p\u2207 \u00b7 \u00b5 \u2212 \u00b5(p\u2207 \u00b7 log p) + g(t) 2 2 p \u2207 log p 2 + p log p .\nDividing by p, one gets that\n\u2202 log p \u2202t = \u2212\u2207 \u00b7 \u00b5 \u2212 \u00b5\u2207 \u00b7 log p + g(t) 2 \u2207 log p 2 2 + g(t) 2 log p 2 .\nas required.\n\nWe are ready to prove Lemma A.8: Substituting \u00b5 = 0 in Lemma A.12, on obtains that\n\u2202 log p \u2202t = g(t) 2 \u2207 log p 2 2 + g(t) 2 log p 2 .\nTaking the gradient with respect to x, one obtains that\n\u2207 \u2202 log p \u2202t = g(t) 2 \u2207 \u2207 log p 2 2 + g(t) 2 \u2207 log p 2 .(19)\nSince \u2202/\u2202x i commutes with \u2202/\u2202t, it holds that\n\u2207 \u2202 log p \u2202t = \u2202 \u2202t \u2207 log p = \u2202s \u2202t ,(20)\nrecalling that by definition s = \u2207 log p. Further,\n\u2202 \u2202x i \u2207 log p 2 = n j=1 \u2202 \u2202x i \u2202 log p \u2202x j 2 = 2 n j=1 \u2202 2 log p \u2202x i \u2202x j \u2202 log p \u2202x j = 2(H log p \u2207 log p) i ,\nwhere for any function f : R d \u2192 R, H f is the Hessian function of f that is defined by\n(H f ) ij = \u2202 2 f \u2202x i \u2202x j\nThis implies that \u2207 \u2207 log p 2 = 2H log p \u2207 log p.\n\n\nFurther, notice that\nH f = J \u2207f ,\nwhich implies that \u2207 \u2207 log p 2 = 2J \u2207 log p \u2207 log p = 2J s s.\n\nLastly, we get that by the commutative property of partial derivatives, \n\u2207 log p = \u2207 log p = s.(22)\u2202s \u2202t = g(t) 2 J s s + g(t) 2 s 2 ,\nas required.\n\nA.5 Proof of Lemma A.9\n\nWe will prove that p and p satisfy the same PDE (which is the heat equation). Recall that s and s satisfy\n\u2202s \u2202t = g(t) 2 J s s + 1 2 s = g(t) 2 2 \u2207 s 2 + s By substituting s = \u2207 log p, \u2202\u2207 log p \u2202t = g(t) 2 2 \u2207 \u2207 log p 2 + \u2207 log p .\nBy exchanging the order of derivatives, we obtain that\n\u2207 \u2202 log p \u2202t = \u2207 g(t) 2 2 \u2207 log p 2 + log p .\nBy integrating, this implies that\n\u2202 log p \u2202t = g(t) 2 2 \u2207 log p 2 + log p + c(t),\nwhere c(t) depends only on t. Eq. (18) shows that\nlog p = p p \u2212 \u2207 log p 2 .\nBy substituting this in the equation above, we obtain that\n\u2202 log p \u2202t = g(t) 2 2 p p + c(t).\nBy multiplying both sides with p, we get that\n\u2202p \u2202t = p \u2202 log p \u2202t = g(t) 2 2 p + c(t).(23)\nSince p is a probability distribution,\nR d p(x, t)dx = 1, therefore, \u2202p(x, t) \u2202t dx = \u2202 \u2202t R d p(x, t)dx = \u22021 \u2202t = 0.\nIntegrating over Eq. (23) we obtain that\n0 = g(t) 2 2 p + c(t)dx = 0 + c(t)dx,\nwhere the last equation holds since the integral of a Laplacian of probability density integrates to 0. It follows that c(t) = 0 which implies that \u2202p \u2202t = g(t) 2 2 p,\n\nand the same PDE holds where p replaces p, and this follows without loss of generality. Further, since log p and log p are differentiable, it holds that p(\u00b7, t) and p (\u00b7, t) are continuous for all fixed t. This implies that p and p are continuous as functions of x and t since they both satisfy the heat equation Eq. (13). Consequently, Lemma A.6 implies that p = p on R d \u00d7 [t 0 , 1]. Finally, s = \u2207 log p = \u2207 log p = s , as required.\n\nA.6 Proof of Lemma A.10\n\nFirst, recall that since x t satisfies Eq. (4) with the initial condition x 0 \u223c \u00b5 0 , then x t \u223c \u00b5 0 + N (0, \u03c3 2 t I), namely, x t is the addition of a random variable drawn from \u00b5 0 and an independent Gaussian N (0, \u03c3 2 t I). Therefore, the density of x t , which we denote by p(x, t), equals\np(x, a) = E a\u223c\u00b50 1 \u221a 2\u03c0\u03c3 t exp \u2212 x \u2212 a 2 2\u03c3 2 t .\nUsing the equation\n\u2207 x log f (x) = \u2207 x f (x) f (x) ,\nwe get that\ns(x, a) = \u2207 x log p(x, a) = \u2207 x p(x, a) p(x, a) = E a\u223c\u00b50 1 \u221a 2\u03c0\u03c3t x\u2212a \u03c3 2 t exp \u2212 x\u2212a 2 2\u03c3 2 t E a\u223c\u00b50 1 \u221a 2\u03c0\u03c3t exp \u2212 x\u2212a 2 2\u03c3 2 t(25)\nBy using the fact that the Taylor formula for e x equals e x = \u221e i=0 e i i! ,\n\nwe obtain that the right hand side of Eq. (25) equals\nE a\u223c\u00b50 1 \u221a 2\u03c0\u03c3t x\u2212a \u03c3 2 t \u221e i=0 (\u22121) i i! x\u2212a 2 2\u03c3 2 t i E a\u223c\u00b50 1 \u221a 2\u03c0\u03c3t \u221e i=0 (\u22121) i i! x\u2212a 2 2\u03c3 2 t i = E a\u223c\u00b50 x\u2212a \u03c3 2 t \u221e i=0 (\u22121) i i! x\u2212a 2 2\u03c3 2 t i E a\u223c\u00b50 \u221e i=0 (\u22121) i i! x\u2212a 2 2\u03c3 2 t i(26)\nWe will use the following property of analytic functions: if f and g are analytic functions over R d and g(x) = 0 for all x then f /g is analytic over R d . Since the denominator at the right hand side of Eq. (26) is nonzero, it suffices to prove that the numerator and the denominator are analytic. We will prove for the denominator and the proof for the numerator is nearly identical. By assumption of this lemma, the support of \u00b5 0 is bounded, hence there is some M > 0 such that x \u2264 M for any x in the support. Then,\n(\u22121) i i! x \u2212 a 2 2\u03c3 2 t i \u2264 1 i! x 2 + a 2 \u03c3 2 t i = M 2i \u03c3 2i t i! .\nThis bound is independent on a, and summing these abvolute values of coefficients for i \u2208 N, one obtains a convergent series. Hence we can replace the summation and the expectation in the denominator at the right hand side of Eq. (26) to get that it equals\n\u221e i=0 (\u22121) i i! E a\u223c\u00b50 x \u2212 a 2 2\u03c3 2 t i .(27)\nTo prove the second consequence, by Property 1\nh(x, 0) = E[x 0 | x 0 = x] = x 0 .\nThis concludes the first direction in the equivalence.\n\nTo prove the second direction, assume that h(x, t) = E[h(x t , t ) | x t = x] and that h(x, 0) = x and notice that by substituting t = 0 we derive the following:\nh(x, t) = E[h(x 0 , 0) | x t = x] = E[x 0 | x t = x],\nas required.   \n\nTheorem 3 . 2 .\n32Let h : R d \u00d7 [0, 1] \u2192 R d be a continuous function. Then: 1. The function h satisfies both Properties 1 and 2 if and only if h is defined by Eq. (2) for some distribution p 0 .\n\n\nwhile the \"EDM\" models are trained only with the first term of the outer expectation. We also denote in the name whether the models have been trained with the Variance Preserving (VP)Song et al. (2021b);Ho et al. (2020) or the Variance ExplodingSong et al. (2021b); Ermon (2020, 2019), e.g. we write EDM-VP.\n\nFigure 1 :\n1Consistency Property Testing on AFHQ.\n\n\nimages by our model trained on AFHQ. FID: 2.21, NFEs: 79. (b) Uncurated images by our conditional CIFAR-10 model. FID: 1.77, NFEs: 35.\n\nFigure 2 :\n2Comparison of uncurated images generated by two different models.\n\nFigure 4 :Figure 5 :Figure 6 :\n456Consistency Property Testing on CIFAR10. The plot illustrates how the Consistency Loss, L 2 t,t ,xt , behaves for t = 0, as t changes. Consistency Property Testing on CIFAR10. The plot illustrates how the Consistency Loss, L 2 t,t ,xt , behaves for t = 0, as t changes. Consistency Property Testing on FFHQ. The plot illustrates how the Consistency Loss, L 2 t,t ,xt , behaves for t = 0, as t changes.\n\nFigure 7 :Figure 8 :\n78Consistency Property Testing on FFHQ. The plot illustrates how the Consistency Loss, L 2 t,t ,xt , behaves for t = 0, as t changes. Uncurated generated images by our fine-tuned model on FFHQ. FID: 2.61, NFEs: 79.\n\n\nof the Appendix. As shown, indeed regularizing for the Consistency Loss drops the L 2 Consistency Property Testing on AFHQ. The plot illustrates how the Consistency Loss, L 2 t,t ,x t , behaves for t = 0, as t changes. Consistency Property Testing on AFHQ. The plot illustrates how the Consistency Loss, L 2 t,t ,x t , behaves for t = 0, as t changes.t,t ,xt \n\nas expected. \n\n0 \n10 \n20 \n30 \n40 \n50 \n60 \n70 \n80 \n\nt \n\n0.000 \n\n0.001 \n\n0.002 \n\n0.003 \n\n0.004 \n\n0.005 \n\n|| \nx0 p (x0|xt) [x \n\n0 ] h (x \n\nt , t)|| 2 \n\nBaseline \nOurs \n\nConsistency Property Testing (AFHQ) \nBackward Sampler (1000) steps \n\n(a) 0 \n10 \n20 \n30 \n40 \n50 \n60 \n70 \n80 \n\nt \n\n0.000 \n\n0.001 \n\n0.002 \n\n0.003 \n\n0.004 \n\n0.005 \n\n|| \nxt p (xt|x1) [h (x \n\nt , t)] h (x \n\n1 , 1.0)|| 2 \n\nBaseline \nOurs \n\nConsistency Property Testing (AFHQ) \nBackward Sampler (1000) steps \n\n(b) \n\nTable 2 :\n2Ablation study on removing the DSM loss for some t.Table reports FID results after 10k steps of training in CIFAR-10.\n\n\nSubstituting Eq. (20), Eq. (21) and Eq. (22) in Eq. (19), one obtains that\nSome of these methods, such as Langevin dynamics, require also to know the score function in the neighborhood of xt.\nAccording to Lemma 3.1, in order to completely train for Property 1, one has to also enforce h(x, 0) = x, however, this is taken care from the denoising score matching objective Eq. (2).\nAcknowledgmentsThis research has been supported by NSF Grants CCF 1763702, AF 1901292, CNS 2148141, Tripods CCF  1934932, IFML CCF 2019844This is the Taylor series around 0 of the above-described denominator it converges to the value of the denominator at any x. While this Taylor series is taken around 0, we note the Taylor series around any other point x 0 \u2208 R n converges as well. This can be shown by shifting the coordinate system by a constant vector such that x 0 shifts to 0 and applying the same proof. One deduces that the Taylor series for the denominator around any point x 0 converges on all R d , which implies that the denominator in the right hand side of Eq. (26) is analytic. The numerator is analytic as well by the same argument. Therefore the ratio, which equals s(x, t), is analytic as well as required.A.7 Proof of Lemma A.11Let t > 0, denote by \u00b5 t and \u00b5 t the distributions of x t and x t , respectively, and by p(x, t) and p (x, t) the densities of these variables. Then, \u00b5 t = \u00b5 0 + N (0, \u03c3 2 t I), namely, \u00b5 t is obtained by adding an independent sample from \u00b5 0 with an independent N (0, \u03c3 2 t I) variables, and similarly for \u00b5 t . Hence, the density p(x, t) is the convolution of the densities p(x, 0) with the density of a Gaussian N (0, \u03c3 2 t I). Denote byp(y, t) the Fourier transform of the density p(x, t) with respect to x (while keeping t fixed) and similarly definep as the Fourier transform of p . Denote by g and by\u011d the density of N (0, \u03c3 2 t I) and its Fourier transform, respectively. Denote the convolution of two functions by the operator * . Then,Since the Fourier transform turns convolutions into multiplications, one obtains thatSince the Fourier transform of a Gaussian is nonzero, we can divide by\u011d(y) to get that p(y, 0) =p (y, 0). This implies that the Fourier transform of p(x, 0) equals that of p (x, 0) hence p(x, 0) = p (x, 0) for all x, as required.B Other proofs B.1 Differentiating the loss function Denote our parameter space as \u0398 \u2286 R m . In order to differentiate L 1 t,t ,x (\u03b8) with respect to \u03b8 \u2208 \u0398, we make the following calculations below, and we notice that E \u03b8 is used to denote an expectation with respect to the distribution of x [t ,t] according to Eq. (7) with s = s \u03b8 and the initial condition x t = x. In other words, the expectation is over x[t ,t]that is taken with respect to the sampler that is parameterized by \u03b8 with the initial condition x t = x. We denote by p \u03b8 (x [t ,t] | x t = x) the corresponding density of x[t ,t]. For any functionLet us compute the gradient of the log density. We use the discrete process, and let us assume that t = t 0 > t 1 > \u00b7 \u00b7 \u00b7 > t k = t are the sampling times. Then,We assume that p \u03b8 (x ti | t i\u22121 ) = N (\u00b5 \u03b8,i , g i I d ).Then,where C corresponds to the normalizing factor that is independent of \u03b8. Differentiating, we get thatIn what appears below, the expectation E[\u00b7 | x t = x] is taken with respect to the distribution obtained by Eq.(7), namely, the backward SDE that corresponds to the function s, with the initial condition x t = x.Similarly, E[\u00b7 | x t ] is taken with the initial condition at x t . To prove the first direction in the equivalence, assume that Property 1 holds and our goal is to prove the two consequences as described in the lemma. To prove the first consequence, by the law of total expectation and by the fact that x t \u2212 x t \u2212 x 0 is a Markov chain, namely, x 0 and x t are independent conditioned on x t , we obtain that\nProtein structure and sequence generation with equivariant denoising diffusion probabilistic models. N Anand, T Achim, arXiv:2205.15019arXiv preprintAnand, N. and Achim, T. (2022). Protein structure and sequence generation with equivariant denoising diffusion probabilistic models. arXiv preprint arXiv:2205.15019.\n\nReverse-time diffusion equation models. B D Anderson, Stochastic Processes and their Applications. 12Anderson, B. D. (1982). Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3):313-326.\n\nSingle-shot adaptation using score-based models for mri reconstruction. M Arvinte, A Jalal, G Daras, E Price, A Dimakis, J I Tamir, International Society for Magnetic Resonance in Medicine, Annual Meeting. Arvinte, M., Jalal, A., Daras, G., Price, E., Dimakis, A., and Tamir, J. I. (2022). Single-shot adaptation using score-based models for mri reconstruction. In International Society for Magnetic Resonance in Medicine, Annual Meeting.\n\nA Bansal, E Borgnia, H.-M Chu, J S Li, H Kazemi, F Huang, M Goldblum, J Geiping, T Goldstein, arXiv:2208.09392Cold Diffusion: Inverting arbitrary image transforms without noise. arXiv preprintBansal, A., Borgnia, E., Chu, H.-M., Li, J. S., Kazemi, H., Huang, F., Goldblum, M., Geiping, J., and Goldstein, T. (2022). Cold Diffusion: Inverting arbitrary image transforms without noise. arXiv preprint arXiv:2208.09392.\n\nSampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. S Chen, S Chewi, J Li, Y Li, A Salim, A R Zhang, arXiv:2209.11215arXiv preprintChen, S., Chewi, S., Li, J., Li, Y., Salim, A., and Zhang, A. R. (2022a). Sampling is as easy as learning the score: theory for diffusion models with minimal data assumptions. arXiv preprint arXiv:2209.11215.\n\nAnalog bits: Generating discrete data using diffusion models with self-conditioning. T Chen, R Zhang, G Hinton, arXiv:2208.04202arXiv preprintChen, T., Zhang, R., and Hinton, G. (2022b). Analog bits: Generating discrete data using diffusion models with self-conditioning. arXiv preprint arXiv:2208.04202.\n\nG Corso, H St\u00e4rk, B Jing, R Barzilay, T Jaakkola, arXiv:2210.01776Diffdock: Diffusion steps, twists, and turns for molecular docking. arXiv preprintCorso, G., St\u00e4rk, H., Jing, B., Barzilay, R., and Jaakkola, T. (2022). Diffdock: Diffusion steps, twists, and turns for molecular docking. arXiv preprint arXiv:2210.01776.\n\nScore-guided intermediate layer optimization: Fast langevin mixing for inverse problem. G Daras, Y Dagan, A G Dimakis, C Daskalakis, arXiv:2206.09104arXiv preprintDaras, G., Dagan, Y., Dimakis, A. G., and Daskalakis, C. (2022a). Score-guided intermediate layer optimiza- tion: Fast langevin mixing for inverse problem. arXiv preprint arXiv:2206.09104.\n\nSoft diffusion: Score matching for general corruptions. G Daras, M Delbracio, H Talebi, A G Dimakis, P Milanfar, arXiv:2209.05442arXiv preprintDaras, G., Delbracio, M., Talebi, H., Dimakis, A. G., and Milanfar, P. (2022b). Soft diffusion: Score matching for general corruptions. arXiv preprint arXiv:2209.05442.\n\nMultiresolution textual inversion. G Daras, A G Dimakis, arXiv:2211.17115arXiv preprintDaras, G. and Dimakis, A. G. (2022). Multiresolution textual inversion. arXiv preprint arXiv:2211.17115.\n\nJ Deasy, N Simidjievski, P Li\u00f2, arXiv:2112.09788Heavy-tailed denoising score matching. arXiv preprintDeasy, J., Simidjievski, N., and Li\u00f2, P. (2021). Heavy-tailed denoising score matching. arXiv preprint arXiv:2112.09788.\n\nDiffusion models beat gans on image synthesis. P Dhariwal, A Nichol, Advances in Neural Information Processing Systems. 34Dhariwal, P. and Nichol, A. (2021). Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34:8780-8794.\n\nTweedie's formula and selection bias. B Efron, Journal of the American Statistical Association. 106496Efron, B. (2011). Tweedie's formula and selection bias. Journal of the American Statistical Association, 106(496):1602-1614.\n\nAn image is worth one word: Personalizing text-to-image generation using textual inversion. R Gal, Y Alaluf, Y Atzmon, O Patashnik, A H Bermano, G Chechik, D Cohen-Or, Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A. H., Chechik, G., and Cohen-Or, D. (2022). An image is worth one word: Personalizing text-to-image generation using textual inversion.\n\nRepresentations of knowledge in complex systems. U Grenander, M I Miller, Journal of the Royal Statistical Society: Series B (Methodological). 564Grenander, U. and Miller, M. I. (1994). Representations of knowledge in complex systems. Journal of the Royal Statistical Society: Series B (Methodological), 56(4):549-581.\n\nImagen video: High definition video generation with diffusion models. J Ho, W Chan, C Saharia, J Whang, R Gao, A Gritsenko, D P Kingma, B Poole, M Norouzi, D J Fleet, arXiv:2210.02303arXiv preprintHo, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J., et al. (2022a). Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303.\n\nDenoising diffusion probabilistic models. J Ho, A Jain, Abbeel , P , Advances in Neural Information Processing Systems. 33Ho, J., Jain, A., and Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840-6851.\n\nJ Ho, T Salimans, A Gritsenko, W Chan, M Norouzi, D J Fleet, arXiv:2204.03458Video diffusion models. Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet, D. J. (2022b). Video diffusion models. arXiv:2204.03458.\n\nCogvideo: Large-scale pretraining for text-to-video generation via transformers. W Hong, M Ding, W Zheng, X Liu, J Tang, arXiv:2205.15868arXiv preprintHong, W., Ding, M., Zheng, W., Liu, X., and Tang, J. (2022). Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868.\n\nBlurring diffusion models. E Hoogeboom, T Salimans, arXiv:2209.05557arXiv preprintHoogeboom, E. and Salimans, T. (2022). Blurring diffusion models. arXiv preprint arXiv:2209.05557.\n\nRobust compressed sensing mri with deep generative priors. A Jalal, M Arvinte, G Daras, E Price, A G Dimakis, J Tamir, Advances in Neural Information Processing Systems. 34Jalal, A., Arvinte, M., Daras, G., Price, E., Dimakis, A. G., and Tamir, J. (2021). Robust compressed sensing mri with deep generative priors. Advances in Neural Information Processing Systems, 34:14938-14954.\n\nGotta go fast when generating data with score-based models. A Jolicoeur-Martineau, K Li, R Pich\u00e9-Taillefer, T Kachman, I Mitliagkas, arXiv:2105.14080arXiv preprintJolicoeur-Martineau, A., Li, K., Pich\u00e9-Taillefer, R., Kachman, T., and Mitliagkas, I. (2021). Gotta go fast when generating data with score-based models. arXiv preprint arXiv:2105.14080.\n\nElucidating the design space of diffusion-based generative models. T Karras, M Aittala, T Aila, Laine , S , arXiv:2206.00364arXiv preprintKarras, T., Aittala, M., Aila, T., and Laine, S. (2022). Elucidating the design space of diffusion-based generative models. arXiv preprint arXiv:2206.00364.\n\nSoft truncation: A universal training technique of score-based diffusion model for high precision score estimation. D Kim, S Shin, K Song, W Kang, I.-C Moon, PMLRInternational Conference on Machine Learning. Kim, D., Shin, S., Song, K., Kang, W., and Moon, I.-C. (2022). Soft truncation: A universal training technique of score-based diffusion model for high precision score estimation. In International Conference on Machine Learning, pages 11201-11228. PMLR.\n\nNoise2score: tweedie's approach to self-supervised image denoising without clean images. K Kim, J C Ye, Advances in Neural Information Processing Systems. 34Kim, K. and Ye, J. C. (2021). Noise2score: tweedie's approach to self-supervised image denoising without clean images. Advances in Neural Information Processing Systems, 34:864-874.\n\nDiffwave: A versatile diffusion model for audio synthesis. Z Kong, W Ping, J Huang, K Zhao, B Catanzaro, International Conference on Learning Representations. Kong, Z., Ping, W., Huang, J., Zhao, K., and Catanzaro, B. (2021). Diffwave: A versatile diffusion model for audio synthesis. In International Conference on Learning Representations.\n\nRegularizing score-based models with score fokker-planck equations. C.-H Lai, Y Takida, N Murata, T Uesaka, Y Mitsufuji, S Ermon, NeurIPS 2022 Workshop on Score-Based Methods. Lai, C.-H., Takida, Y., Murata, N., Uesaka, T., Mitsufuji, Y., and Ermon, S. (2022). Regularizing score-based models with score fokker-planck equations. In NeurIPS 2022 Workshop on Score-Based Methods.\n\nUnderstanding diffusion models: A unified perspective. C Luo, arXiv:2208.11970arXiv preprintLuo, C. (2022). Understanding diffusion models: A unified perspective. arXiv preprint arXiv:2208.11970.\n\nEstimating high order gradients of the data distribution by denoising. C Meng, Y Song, W Li, S Ermon, Advances in Neural Information Processing Systems. 34Meng, C., Song, Y., Li, W., and Ermon, S. (2021). Estimating high order gradients of the data distribution by denoising. Advances in Neural Information Processing Systems, 34:25359-25369.\n\nDenoising diffusion gamma models. E Nachmani, R S Roman, L Wolf, arXiv:2110.05948arXiv preprintNachmani, E., Roman, R. S., and Wolf, L. (2021). Denoising diffusion gamma models. arXiv preprint arXiv:2110.05948.\n\nImproved denoising diffusion probabilistic models. A Q Nichol, P Dhariwal, PMLRInternational Conference on Machine Learning. Nichol, A. Q. and Dhariwal, P. (2021). Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, pages 8162-8171. PMLR.\n\nDreamfusion: Text-to. B Poole, A Jain, J T Barron, B Mildenhall, 3d using 2d diffusion. arXivPoole, B., Jain, A., Barron, J. T., and Mildenhall, B. (2022). Dreamfusion: Text-to-3d using 2d diffusion. arXiv.\n\nA Ramesh, P Dhariwal, A Nichol, C Chu, Chen , M , arXiv:2204.06125Hierarchical text-conditional image generation with clip latents. arXiv preprintRamesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. (2022). Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125.\n\nHigh-resolution image synthesis with latent diffusion models. R Rombach, A Blattmann, D Lorenz, P Esser, B Ommer, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionRombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. (2022). High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684-10695.\n\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. N Ruiz, Y Li, V Jampani, Y Pritch, M Rubinstein, Aberman , K , Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., and Aberman, K. (2022). Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation.\n\nC Saharia, W Chan, S Saxena, L Li, J Whang, E Denton, S K S Ghasemipour, B K Ayan, S S Mahdavi, R G Lopes, arXiv:2205.11487Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprintSaharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S., Lopes, R. G., et al. (2022). Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487.\n\nA Schneuing, Y Du, C Harris, A Jamasb, I Igashov, W Du, T Blundell, P Li\u00f3, C Gomes, M Welling, arXiv:2210.13695Structure-based drug design with equivariant diffusion models. arXiv preprintSchneuing, A., Du, Y., Harris, C., Jamasb, A., Igashov, I., Du, W., Blundell, T., Li\u00f3, P., Gomes, C., Welling, M., et al. (2022). Structure-based drug design with equivariant diffusion models. arXiv preprint arXiv:2210.13695.\n\nGenerating high fidelity data from low-density regions using diffusion models. V Sehwag, C Hazirbas, A Gordo, F Ozgenel, C Canton, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionSehwag, V., Hazirbas, C., Gordo, A., Ozgenel, F., and Canton, C. (2022). Generating high fidelity data from low-density regions using diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11492-11501.\n\nDeep unsupervised learning using nonequilibrium thermodynamics. J Sohl-Dickstein, E Weiss, N Maheswaranathan, S Ganguli, PMLRInternational Conference on Machine Learning. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and Ganguli, S. (2015). Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256-2265. PMLR.\n\nDenoising diffusion implicit models. J Song, C Meng, S Ermon, International Conference on Learning Representations. Song, J., Meng, C., and Ermon, S. (2021a). Denoising diffusion implicit models. In International Conference on Learning Representations.\n\nGenerative modeling by estimating gradients of the data distribution. Y Song, S Ermon, Advances in Neural Information Processing Systems. 32Song, Y. and Ermon, S. (2019). Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32.\n\nImproved techniques for training score-based generative models. Y Song, S Ermon, Advances in neural information processing systems. 33Song, Y. and Ermon, S. (2020). Improved techniques for training score-based generative models. Advances in neural information processing systems, 33:12438-12448.\n\nFor notational consistency, if f is a single-valued function, namely, if n = 1, then \u2207 \u03b8 f is a column vector. We begin with the followingFor notational consistency, if f is a single-valued function, namely, if n = 1, then \u2207 \u03b8 f is a column vector. We begin with the following:\n", "annotations": {"author": "[{\"end\":118,\"start\":104},{\"end\":131,\"start\":119},{\"end\":153,\"start\":132},{\"end\":178,\"start\":154},{\"end\":337,\"start\":179},{\"end\":452,\"start\":338}]", "publisher": null, "author_last_name": "[{\"end\":117,\"start\":112},{\"end\":130,\"start\":125},{\"end\":152,\"start\":145},{\"end\":177,\"start\":167}]", "author_first_name": "[{\"end\":111,\"start\":104},{\"end\":124,\"start\":119},{\"end\":142,\"start\":132},{\"end\":144,\"start\":143},{\"end\":166,\"start\":154}]", "author_affiliation": "[{\"end\":336,\"start\":180},{\"end\":451,\"start\":339}]", "title": "[{\"end\":84,\"start\":1},{\"end\":536,\"start\":453}]", "venue": null, "abstract": "[{\"end\":1575,\"start\":555}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b38\"},\"end\":1640,\"start\":1611},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":1661,\"start\":1640},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":1677,\"start\":1661},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":1793,\"start\":1772},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":1814,\"start\":1793},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1840,\"start\":1814},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":1866,\"start\":1840},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":1883,\"start\":1866},{\"end\":1902,\"start\":1883},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":1920,\"start\":1902},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":1937,\"start\":1920},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":1961,\"start\":1937},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1981,\"start\":1961},{\"end\":2010,\"start\":1990},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2028,\"start\":2010},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2055,\"start\":2036},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2091,\"start\":2071},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2125,\"start\":2102},{\"end\":2145,\"start\":2125},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2168,\"start\":2145},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2187,\"start\":2168},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2234,\"start\":2214},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2255,\"start\":2234},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3067,\"start\":3047},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3089,\"start\":3069},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3120,\"start\":3091},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3141,\"start\":3122},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3165,\"start\":3143},{\"end\":3610,\"start\":3588},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3670,\"start\":3642},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3691,\"start\":3670},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":4761,\"start\":4740},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4777,\"start\":4761},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4803,\"start\":4777},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":4822,\"start\":4803},{\"end\":6703,\"start\":6688},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6715,\"start\":6703},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6733,\"start\":6715},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6750,\"start\":6733},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6760,\"start\":6750},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7640,\"start\":7622},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":10550,\"start\":10528},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":10566,\"start\":10550},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10730,\"start\":10717},{\"end\":10744,\"start\":10730},{\"end\":11432,\"start\":11412},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":12038,\"start\":12022},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":12348,\"start\":12335},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":15676,\"start\":15659},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":24274,\"start\":24254},{\"end\":24772,\"start\":24753},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":25158,\"start\":25138},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":25486,\"start\":25466},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":27624,\"start\":27604},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":28306,\"start\":28286},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":29045,\"start\":29024},{\"end\":29288,\"start\":29268},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":32232,\"start\":32213},{\"end\":32554,\"start\":32535},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":32577,\"start\":32556},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":32610,\"start\":32577},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":32632,\"start\":32612},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":32693,\"start\":32674},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":32785,\"start\":32765},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":33004,\"start\":32987},{\"end\":35308,\"start\":35220},{\"end\":35684,\"start\":35673},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":50464,\"start\":50448}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":50242,\"start\":50046},{\"attributes\":{\"id\":\"fig_1\"},\"end\":50552,\"start\":50243},{\"attributes\":{\"id\":\"fig_2\"},\"end\":50603,\"start\":50553},{\"attributes\":{\"id\":\"fig_3\"},\"end\":50740,\"start\":50604},{\"attributes\":{\"id\":\"fig_4\"},\"end\":50819,\"start\":50741},{\"attributes\":{\"id\":\"fig_6\"},\"end\":51256,\"start\":50820},{\"attributes\":{\"id\":\"fig_7\"},\"end\":51493,\"start\":51257},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":52329,\"start\":51494},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":52459,\"start\":52330},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":52536,\"start\":52460}]", "paragraph": "[{\"end\":2256,\"start\":1591},{\"end\":2625,\"start\":2258},{\"end\":2674,\"start\":2627},{\"end\":2679,\"start\":2676},{\"end\":3166,\"start\":2681},{\"end\":4028,\"start\":3168},{\"end\":4863,\"start\":4030},{\"end\":5388,\"start\":4865},{\"end\":6109,\"start\":5390},{\"end\":6417,\"start\":6111},{\"end\":6761,\"start\":6450},{\"end\":6844,\"start\":6763},{\"end\":7486,\"start\":6918},{\"end\":8384,\"start\":7488},{\"end\":8997,\"start\":8386},{\"end\":9201,\"start\":8999},{\"end\":9348,\"start\":9231},{\"end\":9438,\"start\":9350},{\"end\":9671,\"start\":9467},{\"end\":9873,\"start\":9673},{\"end\":10072,\"start\":9875},{\"end\":10312,\"start\":10074},{\"end\":10389,\"start\":10314},{\"end\":10909,\"start\":10404},{\"end\":11554,\"start\":11002},{\"end\":11802,\"start\":11612},{\"end\":12123,\"start\":11836},{\"end\":12445,\"start\":12176},{\"end\":12540,\"start\":12487},{\"end\":12753,\"start\":12542},{\"end\":13075,\"start\":12854},{\"end\":13214,\"start\":13160},{\"end\":13692,\"start\":13225},{\"end\":13810,\"start\":13753},{\"end\":14185,\"start\":13972},{\"end\":14471,\"start\":14187},{\"end\":14550,\"start\":14473},{\"end\":14630,\"start\":14552},{\"end\":14803,\"start\":14670},{\"end\":15029,\"start\":14837},{\"end\":15286,\"start\":15117},{\"end\":15381,\"start\":15322},{\"end\":15677,\"start\":15383},{\"end\":16315,\"start\":15679},{\"end\":16419,\"start\":16317},{\"end\":16532,\"start\":16512},{\"end\":16891,\"start\":16534},{\"end\":16994,\"start\":16925},{\"end\":17296,\"start\":17030},{\"end\":17345,\"start\":17334},{\"end\":17547,\"start\":17459},{\"end\":17729,\"start\":17605},{\"end\":18403,\"start\":17769},{\"end\":18504,\"start\":18405},{\"end\":19004,\"start\":18552},{\"end\":19060,\"start\":19006},{\"end\":19954,\"start\":19095},{\"end\":20326,\"start\":19956},{\"end\":20717,\"start\":20390},{\"end\":21251,\"start\":20719},{\"end\":21672,\"start\":21262},{\"end\":21972,\"start\":21782},{\"end\":22788,\"start\":22137},{\"end\":23800,\"start\":22930},{\"end\":24040,\"start\":23802},{\"end\":25614,\"start\":24056},{\"end\":26576,\"start\":25723},{\"end\":28360,\"start\":26609},{\"end\":29087,\"start\":28362},{\"end\":30550,\"start\":29089},{\"end\":30873,\"start\":30552},{\"end\":32050,\"start\":30875},{\"end\":33733,\"start\":32067},{\"end\":34480,\"start\":33765},{\"end\":35441,\"start\":34482},{\"end\":35787,\"start\":35443},{\"end\":36008,\"start\":35824},{\"end\":36242,\"start\":36030},{\"end\":36418,\"start\":36385},{\"end\":36532,\"start\":36454},{\"end\":36717,\"start\":36573},{\"end\":36910,\"start\":36834},{\"end\":37018,\"start\":36912},{\"end\":37100,\"start\":37052},{\"end\":37375,\"start\":37320},{\"end\":37466,\"start\":37409},{\"end\":37761,\"start\":37533},{\"end\":37927,\"start\":37806},{\"end\":38052,\"start\":38017},{\"end\":38329,\"start\":38142},{\"end\":38351,\"start\":38331},{\"end\":38570,\"start\":38500},{\"end\":38681,\"start\":38572},{\"end\":38783,\"start\":38683},{\"end\":38889,\"start\":38808},{\"end\":38939,\"start\":38908},{\"end\":39085,\"start\":38975},{\"end\":39186,\"start\":39087},{\"end\":39431,\"start\":39188},{\"end\":39713,\"start\":39433},{\"end\":39936,\"start\":39715},{\"end\":40777,\"start\":39938},{\"end\":41069,\"start\":40817},{\"end\":41823,\"start\":41071},{\"end\":42202,\"start\":41825},{\"end\":42629,\"start\":42272},{\"end\":42889,\"start\":42631},{\"end\":43261,\"start\":42891},{\"end\":43460,\"start\":43288},{\"end\":43487,\"start\":43462},{\"end\":43727,\"start\":43553},{\"end\":43783,\"start\":43771},{\"end\":43947,\"start\":43802},{\"end\":44176,\"start\":44145},{\"end\":44249,\"start\":44216},{\"end\":44377,\"start\":44276},{\"end\":44412,\"start\":44379},{\"end\":44451,\"start\":44446},{\"end\":44731,\"start\":44523},{\"end\":44743,\"start\":44733},{\"end\":44831,\"start\":44773},{\"end\":44933,\"start\":44833},{\"end\":45087,\"start\":45055},{\"end\":45270,\"start\":45163},{\"end\":45373,\"start\":45345},{\"end\":45459,\"start\":45447},{\"end\":45543,\"start\":45461},{\"end\":45650,\"start\":45595},{\"end\":45758,\"start\":45712},{\"end\":45851,\"start\":45801},{\"end\":46054,\"start\":45967},{\"end\":46132,\"start\":46083},{\"end\":46230,\"start\":46169},{\"end\":46304,\"start\":46232},{\"end\":46379,\"start\":46367},{\"end\":46403,\"start\":46381},{\"end\":46510,\"start\":46405},{\"end\":46691,\"start\":46637},{\"end\":46771,\"start\":46738},{\"end\":46869,\"start\":46820},{\"end\":46954,\"start\":46896},{\"end\":47034,\"start\":46989},{\"end\":47119,\"start\":47081},{\"end\":47239,\"start\":47199},{\"end\":47445,\"start\":47278},{\"end\":47882,\"start\":47447},{\"end\":47907,\"start\":47884},{\"end\":48202,\"start\":47909},{\"end\":48271,\"start\":48253},{\"end\":48317,\"start\":48306},{\"end\":48529,\"start\":48452},{\"end\":48584,\"start\":48531},{\"end\":49301,\"start\":48781},{\"end\":49629,\"start\":49373},{\"end\":49722,\"start\":49676},{\"end\":49812,\"start\":49758},{\"end\":49975,\"start\":49814},{\"end\":50045,\"start\":50030}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6449,\"start\":6418},{\"attributes\":{\"id\":\"formula_1\"},\"end\":6917,\"start\":6845},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9466,\"start\":9439},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11001,\"start\":10910},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11611,\"start\":11555},{\"attributes\":{\"id\":\"formula_5\"},\"end\":11835,\"start\":11803},{\"attributes\":{\"id\":\"formula_6\"},\"end\":12175,\"start\":12124},{\"attributes\":{\"id\":\"formula_7\"},\"end\":12486,\"start\":12446},{\"attributes\":{\"id\":\"formula_8\"},\"end\":12853,\"start\":12754},{\"attributes\":{\"id\":\"formula_9\"},\"end\":13159,\"start\":13076},{\"attributes\":{\"id\":\"formula_10\"},\"end\":13752,\"start\":13693},{\"attributes\":{\"id\":\"formula_11\"},\"end\":13971,\"start\":13811},{\"attributes\":{\"id\":\"formula_12\"},\"end\":14669,\"start\":14631},{\"attributes\":{\"id\":\"formula_13\"},\"end\":14836,\"start\":14804},{\"attributes\":{\"id\":\"formula_14\"},\"end\":15116,\"start\":15030},{\"attributes\":{\"id\":\"formula_15\"},\"end\":15321,\"start\":15287},{\"attributes\":{\"id\":\"formula_16\"},\"end\":16511,\"start\":16420},{\"attributes\":{\"id\":\"formula_17\"},\"end\":16924,\"start\":16892},{\"attributes\":{\"id\":\"formula_18\"},\"end\":17029,\"start\":16995},{\"attributes\":{\"id\":\"formula_19\"},\"end\":17333,\"start\":17297},{\"attributes\":{\"id\":\"formula_20\"},\"end\":17458,\"start\":17346},{\"attributes\":{\"id\":\"formula_21\"},\"end\":17604,\"start\":17548},{\"attributes\":{\"id\":\"formula_22\"},\"end\":17768,\"start\":17730},{\"attributes\":{\"id\":\"formula_23\"},\"end\":18551,\"start\":18505},{\"attributes\":{\"id\":\"formula_24\"},\"end\":19094,\"start\":19061},{\"attributes\":{\"id\":\"formula_25\"},\"end\":20389,\"start\":20327},{\"attributes\":{\"id\":\"formula_26\"},\"end\":21781,\"start\":21673},{\"attributes\":{\"id\":\"formula_27\"},\"end\":22136,\"start\":21973},{\"attributes\":{\"id\":\"formula_28\"},\"end\":22929,\"start\":22789},{\"attributes\":{\"id\":\"formula_29\"},\"end\":25722,\"start\":25615},{\"attributes\":{\"id\":\"formula_30\"},\"end\":36384,\"start\":36243},{\"attributes\":{\"id\":\"formula_31\"},\"end\":36453,\"start\":36419},{\"attributes\":{\"id\":\"formula_32\"},\"end\":36572,\"start\":36533},{\"attributes\":{\"id\":\"formula_33\"},\"end\":36833,\"start\":36718},{\"attributes\":{\"id\":\"formula_34\"},\"end\":37051,\"start\":37019},{\"attributes\":{\"id\":\"formula_35\"},\"end\":37319,\"start\":37101},{\"attributes\":{\"id\":\"formula_36\"},\"end\":37408,\"start\":37376},{\"attributes\":{\"id\":\"formula_37\"},\"end\":37532,\"start\":37467},{\"attributes\":{\"id\":\"formula_38\"},\"end\":37773,\"start\":37762},{\"attributes\":{\"id\":\"formula_39\"},\"end\":37805,\"start\":37773},{\"attributes\":{\"id\":\"formula_40\"},\"end\":38016,\"start\":37928},{\"attributes\":{\"id\":\"formula_41\"},\"end\":38141,\"start\":38053},{\"attributes\":{\"id\":\"formula_42\"},\"end\":38499,\"start\":38352},{\"attributes\":{\"id\":\"formula_43\"},\"end\":38807,\"start\":38784},{\"attributes\":{\"id\":\"formula_44\"},\"end\":38974,\"start\":38940},{\"attributes\":{\"id\":\"formula_46\"},\"end\":40816,\"start\":40778},{\"attributes\":{\"id\":\"formula_47\"},\"end\":42271,\"start\":42203},{\"attributes\":{\"id\":\"formula_48\"},\"end\":43552,\"start\":43488},{\"attributes\":{\"id\":\"formula_49\"},\"end\":43770,\"start\":43728},{\"attributes\":{\"id\":\"formula_50\"},\"end\":43801,\"start\":43784},{\"attributes\":{\"id\":\"formula_51\"},\"end\":44144,\"start\":43948},{\"attributes\":{\"id\":\"formula_52\"},\"end\":44215,\"start\":44177},{\"attributes\":{\"id\":\"formula_53\"},\"end\":44445,\"start\":44413},{\"attributes\":{\"id\":\"formula_54\"},\"end\":44522,\"start\":44452},{\"attributes\":{\"id\":\"formula_55\"},\"end\":44772,\"start\":44744},{\"attributes\":{\"id\":\"formula_56\"},\"end\":45054,\"start\":44934},{\"attributes\":{\"id\":\"formula_57\"},\"end\":45162,\"start\":45088},{\"attributes\":{\"id\":\"formula_58\"},\"end\":45344,\"start\":45271},{\"attributes\":{\"id\":\"formula_59\"},\"end\":45446,\"start\":45374},{\"attributes\":{\"id\":\"formula_60\"},\"end\":45594,\"start\":45544},{\"attributes\":{\"id\":\"formula_61\"},\"end\":45711,\"start\":45651},{\"attributes\":{\"id\":\"formula_62\"},\"end\":45800,\"start\":45759},{\"attributes\":{\"id\":\"formula_63\"},\"end\":45966,\"start\":45852},{\"attributes\":{\"id\":\"formula_64\"},\"end\":46082,\"start\":46055},{\"attributes\":{\"id\":\"formula_65\"},\"end\":46168,\"start\":46156},{\"attributes\":{\"id\":\"formula_67\"},\"end\":46331,\"start\":46305},{\"attributes\":{\"id\":\"formula_68\"},\"end\":46366,\"start\":46331},{\"attributes\":{\"id\":\"formula_69\"},\"end\":46636,\"start\":46511},{\"attributes\":{\"id\":\"formula_70\"},\"end\":46737,\"start\":46692},{\"attributes\":{\"id\":\"formula_71\"},\"end\":46819,\"start\":46772},{\"attributes\":{\"id\":\"formula_72\"},\"end\":46895,\"start\":46870},{\"attributes\":{\"id\":\"formula_73\"},\"end\":46988,\"start\":46955},{\"attributes\":{\"id\":\"formula_74\"},\"end\":47080,\"start\":47035},{\"attributes\":{\"id\":\"formula_75\"},\"end\":47198,\"start\":47120},{\"attributes\":{\"id\":\"formula_76\"},\"end\":47277,\"start\":47240},{\"attributes\":{\"id\":\"formula_78\"},\"end\":48252,\"start\":48203},{\"attributes\":{\"id\":\"formula_79\"},\"end\":48305,\"start\":48272},{\"attributes\":{\"id\":\"formula_80\"},\"end\":48451,\"start\":48318},{\"attributes\":{\"id\":\"formula_81\"},\"end\":48780,\"start\":48585},{\"attributes\":{\"id\":\"formula_82\"},\"end\":49372,\"start\":49302},{\"attributes\":{\"id\":\"formula_83\"},\"end\":49675,\"start\":49630},{\"attributes\":{\"id\":\"formula_84\"},\"end\":49757,\"start\":49723},{\"attributes\":{\"id\":\"formula_85\"},\"end\":50029,\"start\":49976}]", "table_ref": "[{\"end\":27854,\"start\":27847},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":31611,\"start\":31604}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1589,\"start\":1577},{\"end\":9229,\"start\":9204},{\"attributes\":{\"n\":\"2\"},\"end\":10402,\"start\":10392},{\"attributes\":{\"n\":\"3\"},\"end\":13223,\"start\":13217},{\"attributes\":{\"n\":\"4\"},\"end\":21260,\"start\":21254},{\"attributes\":{\"n\":\"5\"},\"end\":24054,\"start\":24043},{\"attributes\":{\"n\":\"5.1\"},\"end\":26607,\"start\":26579},{\"attributes\":{\"n\":\"6\"},\"end\":32065,\"start\":32053},{\"attributes\":{\"n\":\"7\"},\"end\":33763,\"start\":33736},{\"end\":35797,\"start\":35790},{\"end\":35822,\"start\":35800},{\"end\":36028,\"start\":36011},{\"end\":38906,\"start\":38892},{\"end\":43286,\"start\":43264},{\"end\":44274,\"start\":44252},{\"end\":46155,\"start\":46135},{\"end\":50062,\"start\":50047},{\"end\":50564,\"start\":50554},{\"end\":50752,\"start\":50742},{\"end\":50851,\"start\":50821},{\"end\":51278,\"start\":51258},{\"end\":52340,\"start\":52331}]", "table": "[{\"end\":52329,\"start\":51847}]", "figure_caption": "[{\"end\":50242,\"start\":50065},{\"end\":50552,\"start\":50245},{\"end\":50603,\"start\":50566},{\"end\":50740,\"start\":50606},{\"end\":50819,\"start\":50754},{\"end\":51256,\"start\":50855},{\"end\":51493,\"start\":51281},{\"end\":51847,\"start\":51496},{\"end\":52459,\"start\":52342},{\"end\":52536,\"start\":52462}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27382,\"start\":27366},{\"end\":27499,\"start\":27481},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29185,\"start\":29168},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29522,\"start\":29512},{\"end\":29683,\"start\":29675},{\"end\":30304,\"start\":30296},{\"end\":37738,\"start\":37715},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":39157,\"start\":39153}]", "bib_author_first_name": "[{\"end\":56411,\"start\":56410},{\"end\":56420,\"start\":56419},{\"end\":56666,\"start\":56665},{\"end\":56668,\"start\":56667},{\"end\":56924,\"start\":56923},{\"end\":56935,\"start\":56934},{\"end\":56944,\"start\":56943},{\"end\":56953,\"start\":56952},{\"end\":56962,\"start\":56961},{\"end\":56973,\"start\":56972},{\"end\":56975,\"start\":56974},{\"end\":57292,\"start\":57291},{\"end\":57302,\"start\":57301},{\"end\":57316,\"start\":57312},{\"end\":57323,\"start\":57322},{\"end\":57325,\"start\":57324},{\"end\":57331,\"start\":57330},{\"end\":57341,\"start\":57340},{\"end\":57350,\"start\":57349},{\"end\":57362,\"start\":57361},{\"end\":57373,\"start\":57372},{\"end\":57812,\"start\":57811},{\"end\":57820,\"start\":57819},{\"end\":57829,\"start\":57828},{\"end\":57835,\"start\":57834},{\"end\":57841,\"start\":57840},{\"end\":57850,\"start\":57849},{\"end\":57852,\"start\":57851},{\"end\":58186,\"start\":58185},{\"end\":58194,\"start\":58193},{\"end\":58203,\"start\":58202},{\"end\":58407,\"start\":58406},{\"end\":58416,\"start\":58415},{\"end\":58425,\"start\":58424},{\"end\":58433,\"start\":58432},{\"end\":58445,\"start\":58444},{\"end\":58816,\"start\":58815},{\"end\":58825,\"start\":58824},{\"end\":58834,\"start\":58833},{\"end\":58836,\"start\":58835},{\"end\":58847,\"start\":58846},{\"end\":59137,\"start\":59136},{\"end\":59146,\"start\":59145},{\"end\":59159,\"start\":59158},{\"end\":59169,\"start\":59168},{\"end\":59171,\"start\":59170},{\"end\":59182,\"start\":59181},{\"end\":59429,\"start\":59428},{\"end\":59438,\"start\":59437},{\"end\":59440,\"start\":59439},{\"end\":59587,\"start\":59586},{\"end\":59596,\"start\":59595},{\"end\":59612,\"start\":59611},{\"end\":59857,\"start\":59856},{\"end\":59869,\"start\":59868},{\"end\":60119,\"start\":60118},{\"end\":60401,\"start\":60400},{\"end\":60408,\"start\":60407},{\"end\":60418,\"start\":60417},{\"end\":60428,\"start\":60427},{\"end\":60441,\"start\":60440},{\"end\":60443,\"start\":60442},{\"end\":60454,\"start\":60453},{\"end\":60465,\"start\":60464},{\"end\":60721,\"start\":60720},{\"end\":60734,\"start\":60733},{\"end\":60736,\"start\":60735},{\"end\":61062,\"start\":61061},{\"end\":61068,\"start\":61067},{\"end\":61076,\"start\":61075},{\"end\":61087,\"start\":61086},{\"end\":61096,\"start\":61095},{\"end\":61103,\"start\":61102},{\"end\":61116,\"start\":61115},{\"end\":61118,\"start\":61117},{\"end\":61128,\"start\":61127},{\"end\":61137,\"start\":61136},{\"end\":61148,\"start\":61147},{\"end\":61150,\"start\":61149},{\"end\":61470,\"start\":61469},{\"end\":61476,\"start\":61475},{\"end\":61489,\"start\":61483},{\"end\":61493,\"start\":61492},{\"end\":61699,\"start\":61698},{\"end\":61705,\"start\":61704},{\"end\":61717,\"start\":61716},{\"end\":61730,\"start\":61729},{\"end\":61738,\"start\":61737},{\"end\":61749,\"start\":61748},{\"end\":61751,\"start\":61750},{\"end\":62010,\"start\":62009},{\"end\":62018,\"start\":62017},{\"end\":62026,\"start\":62025},{\"end\":62035,\"start\":62034},{\"end\":62042,\"start\":62041},{\"end\":62283,\"start\":62282},{\"end\":62296,\"start\":62295},{\"end\":62497,\"start\":62496},{\"end\":62506,\"start\":62505},{\"end\":62517,\"start\":62516},{\"end\":62526,\"start\":62525},{\"end\":62535,\"start\":62534},{\"end\":62537,\"start\":62536},{\"end\":62548,\"start\":62547},{\"end\":62881,\"start\":62880},{\"end\":62904,\"start\":62903},{\"end\":62910,\"start\":62909},{\"end\":62929,\"start\":62928},{\"end\":62940,\"start\":62939},{\"end\":63239,\"start\":63238},{\"end\":63249,\"start\":63248},{\"end\":63260,\"start\":63259},{\"end\":63272,\"start\":63267},{\"end\":63276,\"start\":63275},{\"end\":63584,\"start\":63583},{\"end\":63591,\"start\":63590},{\"end\":63599,\"start\":63598},{\"end\":63607,\"start\":63606},{\"end\":63618,\"start\":63614},{\"end\":64019,\"start\":64018},{\"end\":64026,\"start\":64025},{\"end\":64028,\"start\":64027},{\"end\":64329,\"start\":64328},{\"end\":64337,\"start\":64336},{\"end\":64345,\"start\":64344},{\"end\":64354,\"start\":64353},{\"end\":64362,\"start\":64361},{\"end\":64684,\"start\":64680},{\"end\":64691,\"start\":64690},{\"end\":64701,\"start\":64700},{\"end\":64711,\"start\":64710},{\"end\":64721,\"start\":64720},{\"end\":64734,\"start\":64733},{\"end\":65047,\"start\":65046},{\"end\":65260,\"start\":65259},{\"end\":65268,\"start\":65267},{\"end\":65276,\"start\":65275},{\"end\":65282,\"start\":65281},{\"end\":65567,\"start\":65566},{\"end\":65579,\"start\":65578},{\"end\":65581,\"start\":65580},{\"end\":65590,\"start\":65589},{\"end\":65796,\"start\":65795},{\"end\":65798,\"start\":65797},{\"end\":65808,\"start\":65807},{\"end\":66055,\"start\":66054},{\"end\":66064,\"start\":66063},{\"end\":66072,\"start\":66071},{\"end\":66074,\"start\":66073},{\"end\":66084,\"start\":66083},{\"end\":66241,\"start\":66240},{\"end\":66251,\"start\":66250},{\"end\":66263,\"start\":66262},{\"end\":66273,\"start\":66272},{\"end\":66283,\"start\":66279},{\"end\":66287,\"start\":66286},{\"end\":66617,\"start\":66616},{\"end\":66628,\"start\":66627},{\"end\":66641,\"start\":66640},{\"end\":66651,\"start\":66650},{\"end\":66660,\"start\":66659},{\"end\":67145,\"start\":67144},{\"end\":67153,\"start\":67152},{\"end\":67159,\"start\":67158},{\"end\":67170,\"start\":67169},{\"end\":67180,\"start\":67179},{\"end\":67200,\"start\":67193},{\"end\":67204,\"start\":67203},{\"end\":67378,\"start\":67377},{\"end\":67389,\"start\":67388},{\"end\":67397,\"start\":67396},{\"end\":67407,\"start\":67406},{\"end\":67413,\"start\":67412},{\"end\":67422,\"start\":67421},{\"end\":67432,\"start\":67431},{\"end\":67436,\"start\":67433},{\"end\":67451,\"start\":67450},{\"end\":67453,\"start\":67452},{\"end\":67461,\"start\":67460},{\"end\":67463,\"start\":67462},{\"end\":67474,\"start\":67473},{\"end\":67476,\"start\":67475},{\"end\":67856,\"start\":67855},{\"end\":67869,\"start\":67868},{\"end\":67875,\"start\":67874},{\"end\":67885,\"start\":67884},{\"end\":67895,\"start\":67894},{\"end\":67906,\"start\":67905},{\"end\":67912,\"start\":67911},{\"end\":67924,\"start\":67923},{\"end\":67931,\"start\":67930},{\"end\":67940,\"start\":67939},{\"end\":68350,\"start\":68349},{\"end\":68360,\"start\":68359},{\"end\":68372,\"start\":68371},{\"end\":68381,\"start\":68380},{\"end\":68392,\"start\":68391},{\"end\":68873,\"start\":68872},{\"end\":68891,\"start\":68890},{\"end\":68900,\"start\":68899},{\"end\":68919,\"start\":68918},{\"end\":69230,\"start\":69229},{\"end\":69238,\"start\":69237},{\"end\":69246,\"start\":69245},{\"end\":69517,\"start\":69516},{\"end\":69525,\"start\":69524},{\"end\":69808,\"start\":69807},{\"end\":69816,\"start\":69815}]", "bib_author_last_name": "[{\"end\":56417,\"start\":56412},{\"end\":56426,\"start\":56421},{\"end\":56677,\"start\":56669},{\"end\":56932,\"start\":56925},{\"end\":56941,\"start\":56936},{\"end\":56950,\"start\":56945},{\"end\":56959,\"start\":56954},{\"end\":56970,\"start\":56963},{\"end\":56981,\"start\":56976},{\"end\":57299,\"start\":57293},{\"end\":57310,\"start\":57303},{\"end\":57320,\"start\":57317},{\"end\":57328,\"start\":57326},{\"end\":57338,\"start\":57332},{\"end\":57347,\"start\":57342},{\"end\":57359,\"start\":57351},{\"end\":57370,\"start\":57363},{\"end\":57383,\"start\":57374},{\"end\":57817,\"start\":57813},{\"end\":57826,\"start\":57821},{\"end\":57832,\"start\":57830},{\"end\":57838,\"start\":57836},{\"end\":57847,\"start\":57842},{\"end\":57858,\"start\":57853},{\"end\":58191,\"start\":58187},{\"end\":58200,\"start\":58195},{\"end\":58210,\"start\":58204},{\"end\":58413,\"start\":58408},{\"end\":58422,\"start\":58417},{\"end\":58430,\"start\":58426},{\"end\":58442,\"start\":58434},{\"end\":58454,\"start\":58446},{\"end\":58822,\"start\":58817},{\"end\":58831,\"start\":58826},{\"end\":58844,\"start\":58837},{\"end\":58858,\"start\":58848},{\"end\":59143,\"start\":59138},{\"end\":59156,\"start\":59147},{\"end\":59166,\"start\":59160},{\"end\":59179,\"start\":59172},{\"end\":59191,\"start\":59183},{\"end\":59435,\"start\":59430},{\"end\":59448,\"start\":59441},{\"end\":59593,\"start\":59588},{\"end\":59609,\"start\":59597},{\"end\":59616,\"start\":59613},{\"end\":59866,\"start\":59858},{\"end\":59876,\"start\":59870},{\"end\":60125,\"start\":60120},{\"end\":60405,\"start\":60402},{\"end\":60415,\"start\":60409},{\"end\":60425,\"start\":60419},{\"end\":60438,\"start\":60429},{\"end\":60451,\"start\":60444},{\"end\":60462,\"start\":60455},{\"end\":60474,\"start\":60466},{\"end\":60731,\"start\":60722},{\"end\":60743,\"start\":60737},{\"end\":61065,\"start\":61063},{\"end\":61073,\"start\":61069},{\"end\":61084,\"start\":61077},{\"end\":61093,\"start\":61088},{\"end\":61100,\"start\":61097},{\"end\":61113,\"start\":61104},{\"end\":61125,\"start\":61119},{\"end\":61134,\"start\":61129},{\"end\":61145,\"start\":61138},{\"end\":61156,\"start\":61151},{\"end\":61473,\"start\":61471},{\"end\":61481,\"start\":61477},{\"end\":61702,\"start\":61700},{\"end\":61714,\"start\":61706},{\"end\":61727,\"start\":61718},{\"end\":61735,\"start\":61731},{\"end\":61746,\"start\":61739},{\"end\":61757,\"start\":61752},{\"end\":62015,\"start\":62011},{\"end\":62023,\"start\":62019},{\"end\":62032,\"start\":62027},{\"end\":62039,\"start\":62036},{\"end\":62047,\"start\":62043},{\"end\":62293,\"start\":62284},{\"end\":62305,\"start\":62297},{\"end\":62503,\"start\":62498},{\"end\":62514,\"start\":62507},{\"end\":62523,\"start\":62518},{\"end\":62532,\"start\":62527},{\"end\":62545,\"start\":62538},{\"end\":62554,\"start\":62549},{\"end\":62901,\"start\":62882},{\"end\":62907,\"start\":62905},{\"end\":62926,\"start\":62911},{\"end\":62937,\"start\":62930},{\"end\":62951,\"start\":62941},{\"end\":63246,\"start\":63240},{\"end\":63257,\"start\":63250},{\"end\":63265,\"start\":63261},{\"end\":63588,\"start\":63585},{\"end\":63596,\"start\":63592},{\"end\":63604,\"start\":63600},{\"end\":63612,\"start\":63608},{\"end\":63623,\"start\":63619},{\"end\":64023,\"start\":64020},{\"end\":64031,\"start\":64029},{\"end\":64334,\"start\":64330},{\"end\":64342,\"start\":64338},{\"end\":64351,\"start\":64346},{\"end\":64359,\"start\":64355},{\"end\":64372,\"start\":64363},{\"end\":64688,\"start\":64685},{\"end\":64698,\"start\":64692},{\"end\":64708,\"start\":64702},{\"end\":64718,\"start\":64712},{\"end\":64731,\"start\":64722},{\"end\":64740,\"start\":64735},{\"end\":65051,\"start\":65048},{\"end\":65265,\"start\":65261},{\"end\":65273,\"start\":65269},{\"end\":65279,\"start\":65277},{\"end\":65288,\"start\":65283},{\"end\":65576,\"start\":65568},{\"end\":65587,\"start\":65582},{\"end\":65595,\"start\":65591},{\"end\":65805,\"start\":65799},{\"end\":65817,\"start\":65809},{\"end\":66061,\"start\":66056},{\"end\":66069,\"start\":66065},{\"end\":66081,\"start\":66075},{\"end\":66095,\"start\":66085},{\"end\":66248,\"start\":66242},{\"end\":66260,\"start\":66252},{\"end\":66270,\"start\":66264},{\"end\":66277,\"start\":66274},{\"end\":66625,\"start\":66618},{\"end\":66638,\"start\":66629},{\"end\":66648,\"start\":66642},{\"end\":66657,\"start\":66652},{\"end\":66666,\"start\":66661},{\"end\":67150,\"start\":67146},{\"end\":67156,\"start\":67154},{\"end\":67167,\"start\":67160},{\"end\":67177,\"start\":67171},{\"end\":67191,\"start\":67181},{\"end\":67386,\"start\":67379},{\"end\":67394,\"start\":67390},{\"end\":67404,\"start\":67398},{\"end\":67410,\"start\":67408},{\"end\":67419,\"start\":67414},{\"end\":67429,\"start\":67423},{\"end\":67448,\"start\":67437},{\"end\":67458,\"start\":67454},{\"end\":67471,\"start\":67464},{\"end\":67482,\"start\":67477},{\"end\":67866,\"start\":67857},{\"end\":67872,\"start\":67870},{\"end\":67882,\"start\":67876},{\"end\":67892,\"start\":67886},{\"end\":67903,\"start\":67896},{\"end\":67909,\"start\":67907},{\"end\":67921,\"start\":67913},{\"end\":67928,\"start\":67925},{\"end\":67937,\"start\":67932},{\"end\":67948,\"start\":67941},{\"end\":68357,\"start\":68351},{\"end\":68369,\"start\":68361},{\"end\":68378,\"start\":68373},{\"end\":68389,\"start\":68382},{\"end\":68399,\"start\":68393},{\"end\":68888,\"start\":68874},{\"end\":68897,\"start\":68892},{\"end\":68916,\"start\":68901},{\"end\":68927,\"start\":68920},{\"end\":69235,\"start\":69231},{\"end\":69243,\"start\":69239},{\"end\":69252,\"start\":69247},{\"end\":69522,\"start\":69518},{\"end\":69531,\"start\":69526},{\"end\":69813,\"start\":69809},{\"end\":69822,\"start\":69817}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2205.15019\",\"id\":\"b0\"},\"end\":56623,\"start\":56309},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3897405},\"end\":56849,\"start\":56625},{\"attributes\":{\"id\":\"b2\"},\"end\":57289,\"start\":56851},{\"attributes\":{\"doi\":\"arXiv:2208.09392\",\"id\":\"b3\"},\"end\":57707,\"start\":57291},{\"attributes\":{\"doi\":\"arXiv:2209.11215\",\"id\":\"b4\"},\"end\":58098,\"start\":57709},{\"attributes\":{\"doi\":\"arXiv:2208.04202\",\"id\":\"b5\"},\"end\":58404,\"start\":58100},{\"attributes\":{\"doi\":\"arXiv:2210.01776\",\"id\":\"b6\"},\"end\":58725,\"start\":58406},{\"attributes\":{\"doi\":\"arXiv:2206.09104\",\"id\":\"b7\"},\"end\":59078,\"start\":58727},{\"attributes\":{\"doi\":\"arXiv:2209.05442\",\"id\":\"b8\"},\"end\":59391,\"start\":59080},{\"attributes\":{\"doi\":\"arXiv:2211.17115\",\"id\":\"b9\"},\"end\":59584,\"start\":59393},{\"attributes\":{\"doi\":\"arXiv:2112.09788\",\"id\":\"b10\"},\"end\":59807,\"start\":59586},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":234357997},\"end\":60078,\"start\":59809},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":23284154},\"end\":60306,\"start\":60080},{\"attributes\":{\"id\":\"b13\"},\"end\":60669,\"start\":60308},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":117719085},\"end\":60989,\"start\":60671},{\"attributes\":{\"doi\":\"arXiv:2210.02303\",\"id\":\"b15\"},\"end\":61425,\"start\":60991},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":219955663},\"end\":61696,\"start\":61427},{\"attributes\":{\"doi\":\"arXiv:2204.03458\",\"id\":\"b17\"},\"end\":61926,\"start\":61698},{\"attributes\":{\"doi\":\"arXiv:2205.15868\",\"id\":\"b18\"},\"end\":62253,\"start\":61928},{\"attributes\":{\"doi\":\"arXiv:2209.05557\",\"id\":\"b19\"},\"end\":62435,\"start\":62255},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":236881431},\"end\":62818,\"start\":62437},{\"attributes\":{\"doi\":\"arXiv:2105.14080\",\"id\":\"b21\"},\"end\":63169,\"start\":62820},{\"attributes\":{\"doi\":\"arXiv:2206.00364\",\"id\":\"b22\"},\"end\":63465,\"start\":63171},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b23\",\"matched_paper_id\":248218418},\"end\":63927,\"start\":63467},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":235422658},\"end\":64267,\"start\":63929},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":221818900},\"end\":64610,\"start\":64269},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":252780150},\"end\":64989,\"start\":64612},{\"attributes\":{\"doi\":\"arXiv:2208.11970\",\"id\":\"b27\"},\"end\":65186,\"start\":64991},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":243847971},\"end\":65530,\"start\":65188},{\"attributes\":{\"doi\":\"arXiv:2110.05948\",\"id\":\"b29\"},\"end\":65742,\"start\":65532},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b30\",\"matched_paper_id\":231979499},\"end\":66030,\"start\":65744},{\"attributes\":{\"id\":\"b31\"},\"end\":66238,\"start\":66032},{\"attributes\":{\"doi\":\"arXiv:2204.06125\",\"id\":\"b32\"},\"end\":66552,\"start\":66240},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":245335280},\"end\":67056,\"start\":66554},{\"attributes\":{\"id\":\"b34\"},\"end\":67375,\"start\":67058},{\"attributes\":{\"doi\":\"arXiv:2205.11487\",\"id\":\"b35\"},\"end\":67853,\"start\":67377},{\"attributes\":{\"doi\":\"arXiv:2210.13695\",\"id\":\"b36\"},\"end\":68268,\"start\":67855},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":247839278},\"end\":68806,\"start\":68270},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b38\",\"matched_paper_id\":14888175},\"end\":69190,\"start\":68808},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":222140788},\"end\":69444,\"start\":69192},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":196470871},\"end\":69741,\"start\":69446},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":219708245},\"end\":70038,\"start\":69743},{\"attributes\":{\"id\":\"b42\"},\"end\":70317,\"start\":70040}]", "bib_title": "[{\"end\":56663,\"start\":56625},{\"end\":56921,\"start\":56851},{\"end\":59854,\"start\":59809},{\"end\":60116,\"start\":60080},{\"end\":60718,\"start\":60671},{\"end\":61467,\"start\":61427},{\"end\":62494,\"start\":62437},{\"end\":63581,\"start\":63467},{\"end\":64016,\"start\":63929},{\"end\":64326,\"start\":64269},{\"end\":64678,\"start\":64612},{\"end\":65257,\"start\":65188},{\"end\":65793,\"start\":65744},{\"end\":66614,\"start\":66554},{\"end\":68347,\"start\":68270},{\"end\":68870,\"start\":68808},{\"end\":69227,\"start\":69192},{\"end\":69514,\"start\":69446},{\"end\":69805,\"start\":69743}]", "bib_author": "[{\"end\":56419,\"start\":56410},{\"end\":56428,\"start\":56419},{\"end\":56679,\"start\":56665},{\"end\":56934,\"start\":56923},{\"end\":56943,\"start\":56934},{\"end\":56952,\"start\":56943},{\"end\":56961,\"start\":56952},{\"end\":56972,\"start\":56961},{\"end\":56983,\"start\":56972},{\"end\":57301,\"start\":57291},{\"end\":57312,\"start\":57301},{\"end\":57322,\"start\":57312},{\"end\":57330,\"start\":57322},{\"end\":57340,\"start\":57330},{\"end\":57349,\"start\":57340},{\"end\":57361,\"start\":57349},{\"end\":57372,\"start\":57361},{\"end\":57385,\"start\":57372},{\"end\":57819,\"start\":57811},{\"end\":57828,\"start\":57819},{\"end\":57834,\"start\":57828},{\"end\":57840,\"start\":57834},{\"end\":57849,\"start\":57840},{\"end\":57860,\"start\":57849},{\"end\":58193,\"start\":58185},{\"end\":58202,\"start\":58193},{\"end\":58212,\"start\":58202},{\"end\":58415,\"start\":58406},{\"end\":58424,\"start\":58415},{\"end\":58432,\"start\":58424},{\"end\":58444,\"start\":58432},{\"end\":58456,\"start\":58444},{\"end\":58824,\"start\":58815},{\"end\":58833,\"start\":58824},{\"end\":58846,\"start\":58833},{\"end\":58860,\"start\":58846},{\"end\":59145,\"start\":59136},{\"end\":59158,\"start\":59145},{\"end\":59168,\"start\":59158},{\"end\":59181,\"start\":59168},{\"end\":59193,\"start\":59181},{\"end\":59437,\"start\":59428},{\"end\":59450,\"start\":59437},{\"end\":59595,\"start\":59586},{\"end\":59611,\"start\":59595},{\"end\":59618,\"start\":59611},{\"end\":59868,\"start\":59856},{\"end\":59878,\"start\":59868},{\"end\":60127,\"start\":60118},{\"end\":60407,\"start\":60400},{\"end\":60417,\"start\":60407},{\"end\":60427,\"start\":60417},{\"end\":60440,\"start\":60427},{\"end\":60453,\"start\":60440},{\"end\":60464,\"start\":60453},{\"end\":60476,\"start\":60464},{\"end\":60733,\"start\":60720},{\"end\":60745,\"start\":60733},{\"end\":61067,\"start\":61061},{\"end\":61075,\"start\":61067},{\"end\":61086,\"start\":61075},{\"end\":61095,\"start\":61086},{\"end\":61102,\"start\":61095},{\"end\":61115,\"start\":61102},{\"end\":61127,\"start\":61115},{\"end\":61136,\"start\":61127},{\"end\":61147,\"start\":61136},{\"end\":61158,\"start\":61147},{\"end\":61475,\"start\":61469},{\"end\":61483,\"start\":61475},{\"end\":61492,\"start\":61483},{\"end\":61496,\"start\":61492},{\"end\":61704,\"start\":61698},{\"end\":61716,\"start\":61704},{\"end\":61729,\"start\":61716},{\"end\":61737,\"start\":61729},{\"end\":61748,\"start\":61737},{\"end\":61759,\"start\":61748},{\"end\":62017,\"start\":62009},{\"end\":62025,\"start\":62017},{\"end\":62034,\"start\":62025},{\"end\":62041,\"start\":62034},{\"end\":62049,\"start\":62041},{\"end\":62295,\"start\":62282},{\"end\":62307,\"start\":62295},{\"end\":62505,\"start\":62496},{\"end\":62516,\"start\":62505},{\"end\":62525,\"start\":62516},{\"end\":62534,\"start\":62525},{\"end\":62547,\"start\":62534},{\"end\":62556,\"start\":62547},{\"end\":62903,\"start\":62880},{\"end\":62909,\"start\":62903},{\"end\":62928,\"start\":62909},{\"end\":62939,\"start\":62928},{\"end\":62953,\"start\":62939},{\"end\":63248,\"start\":63238},{\"end\":63259,\"start\":63248},{\"end\":63267,\"start\":63259},{\"end\":63275,\"start\":63267},{\"end\":63279,\"start\":63275},{\"end\":63590,\"start\":63583},{\"end\":63598,\"start\":63590},{\"end\":63606,\"start\":63598},{\"end\":63614,\"start\":63606},{\"end\":63625,\"start\":63614},{\"end\":64025,\"start\":64018},{\"end\":64033,\"start\":64025},{\"end\":64336,\"start\":64328},{\"end\":64344,\"start\":64336},{\"end\":64353,\"start\":64344},{\"end\":64361,\"start\":64353},{\"end\":64374,\"start\":64361},{\"end\":64690,\"start\":64680},{\"end\":64700,\"start\":64690},{\"end\":64710,\"start\":64700},{\"end\":64720,\"start\":64710},{\"end\":64733,\"start\":64720},{\"end\":64742,\"start\":64733},{\"end\":65053,\"start\":65046},{\"end\":65267,\"start\":65259},{\"end\":65275,\"start\":65267},{\"end\":65281,\"start\":65275},{\"end\":65290,\"start\":65281},{\"end\":65578,\"start\":65566},{\"end\":65589,\"start\":65578},{\"end\":65597,\"start\":65589},{\"end\":65807,\"start\":65795},{\"end\":65819,\"start\":65807},{\"end\":66063,\"start\":66054},{\"end\":66071,\"start\":66063},{\"end\":66083,\"start\":66071},{\"end\":66097,\"start\":66083},{\"end\":66250,\"start\":66240},{\"end\":66262,\"start\":66250},{\"end\":66272,\"start\":66262},{\"end\":66279,\"start\":66272},{\"end\":66286,\"start\":66279},{\"end\":66290,\"start\":66286},{\"end\":66627,\"start\":66616},{\"end\":66640,\"start\":66627},{\"end\":66650,\"start\":66640},{\"end\":66659,\"start\":66650},{\"end\":66668,\"start\":66659},{\"end\":67152,\"start\":67144},{\"end\":67158,\"start\":67152},{\"end\":67169,\"start\":67158},{\"end\":67179,\"start\":67169},{\"end\":67193,\"start\":67179},{\"end\":67203,\"start\":67193},{\"end\":67207,\"start\":67203},{\"end\":67388,\"start\":67377},{\"end\":67396,\"start\":67388},{\"end\":67406,\"start\":67396},{\"end\":67412,\"start\":67406},{\"end\":67421,\"start\":67412},{\"end\":67431,\"start\":67421},{\"end\":67450,\"start\":67431},{\"end\":67460,\"start\":67450},{\"end\":67473,\"start\":67460},{\"end\":67484,\"start\":67473},{\"end\":67868,\"start\":67855},{\"end\":67874,\"start\":67868},{\"end\":67884,\"start\":67874},{\"end\":67894,\"start\":67884},{\"end\":67905,\"start\":67894},{\"end\":67911,\"start\":67905},{\"end\":67923,\"start\":67911},{\"end\":67930,\"start\":67923},{\"end\":67939,\"start\":67930},{\"end\":67950,\"start\":67939},{\"end\":68359,\"start\":68349},{\"end\":68371,\"start\":68359},{\"end\":68380,\"start\":68371},{\"end\":68391,\"start\":68380},{\"end\":68401,\"start\":68391},{\"end\":68890,\"start\":68872},{\"end\":68899,\"start\":68890},{\"end\":68918,\"start\":68899},{\"end\":68929,\"start\":68918},{\"end\":69237,\"start\":69229},{\"end\":69245,\"start\":69237},{\"end\":69254,\"start\":69245},{\"end\":69524,\"start\":69516},{\"end\":69533,\"start\":69524},{\"end\":69815,\"start\":69807},{\"end\":69824,\"start\":69815}]", "bib_venue": "[{\"end\":56408,\"start\":56309},{\"end\":56722,\"start\":56679},{\"end\":57055,\"start\":56983},{\"end\":57467,\"start\":57401},{\"end\":57809,\"start\":57709},{\"end\":58183,\"start\":58100},{\"end\":58538,\"start\":58472},{\"end\":58813,\"start\":58727},{\"end\":59134,\"start\":59080},{\"end\":59426,\"start\":59393},{\"end\":59671,\"start\":59634},{\"end\":59927,\"start\":59878},{\"end\":60174,\"start\":60127},{\"end\":60398,\"start\":60308},{\"end\":60812,\"start\":60745},{\"end\":61059,\"start\":60991},{\"end\":61545,\"start\":61496},{\"end\":61797,\"start\":61775},{\"end\":62007,\"start\":61928},{\"end\":62280,\"start\":62255},{\"end\":62605,\"start\":62556},{\"end\":62878,\"start\":62820},{\"end\":63236,\"start\":63171},{\"end\":63673,\"start\":63629},{\"end\":64082,\"start\":64033},{\"end\":64426,\"start\":64374},{\"end\":64786,\"start\":64742},{\"end\":65044,\"start\":64991},{\"end\":65339,\"start\":65290},{\"end\":65564,\"start\":65532},{\"end\":65867,\"start\":65823},{\"end\":66052,\"start\":66032},{\"end\":66370,\"start\":66306},{\"end\":66749,\"start\":66668},{\"end\":67142,\"start\":67058},{\"end\":67578,\"start\":67500},{\"end\":68027,\"start\":67966},{\"end\":68482,\"start\":68401},{\"end\":68977,\"start\":68933},{\"end\":69306,\"start\":69254},{\"end\":69582,\"start\":69533},{\"end\":69873,\"start\":69824},{\"end\":70149,\"start\":70040},{\"end\":66817,\"start\":66751},{\"end\":68550,\"start\":68484}]"}}}, "year": 2023, "month": 12, "day": 17}