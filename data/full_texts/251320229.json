{"id": 251320229, "updated": "2023-10-05 11:48:47.894", "metadata": {"title": "Globally Consistent Video Depth and Pose Estimation with Efficient Test-Time Training", "authors": "[{\"first\":\"Yao-Chih\",\"last\":\"Lee\",\"middle\":[]},{\"first\":\"Kuan-Wei\",\"last\":\"Tseng\",\"middle\":[]},{\"first\":\"Guan-Sheng\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Chu-Song\",\"last\":\"Chen\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Dense depth and pose estimation is a vital prerequisite for various video applications. Traditional solutions suffer from the robustness of sparse feature tracking and insufficient camera baselines in videos. Therefore, recent methods utilize learning-based optical flow and depth prior to estimate dense depth. However, previous works require heavy computation time or yield sub-optimal depth results. We present GCVD, a globally consistent method for learning-based video structure from motion (SfM) in this paper. GCVD integrates a compact pose graph into the CNN-based optimization to achieve globally consistent estimation from an effective keyframe selection mechanism. It can improve the robustness of learning-based methods with flow-guided keyframes and well-established depth prior. Experimental results show that GCVD outperforms the state-of-the-art methods on both depth and pose estimation. Besides, the runtime experiments reveal that it provides strong efficiency in both short- and long-term videos with global consistency provided.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2208.02709", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2208-02709", "doi": "10.48550/arxiv.2208.02709"}}, "content": {"source": {"pdf_hash": "3e8eb241d5c8e4ee9f599919db4b730e71691e63", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2208.02709v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "8ff12d1d5720d11c0d8795fe7b7d32d81580a34a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/3e8eb241d5c8e4ee9f599919db4b730e71691e63.txt", "contents": "\nGlobally Consistent Video Depth and Pose Estimation with Efficient Test-Time Training\n\n\nYao-Chih Lee \nNational Taiwan University\n\n\nKuan-Wei Tseng \nNational Taiwan University\n\n\nGuan-Sheng Chen \nNational Taiwan University\n\n\nChu-Song Chen \nNational Taiwan University\n\n\nGlobally Consistent Video Depth and Pose Estimation with Efficient Test-Time Training\n\nDense depth and pose estimation is a vital prerequisite for various video applications. Traditional solutions suffer from the robustness of sparse feature tracking and insufficient camera baselines in videos. Therefore, recent methods utilize learning-based optical flow and depth prior to estimate dense depth. However, previous works require heavy computation time or yield sub-optimal depth results. We present GCVD, a globally consistent method for learning-based video structure from motion (SfM) in this paper. GCVD integrates a compact pose graph into the CNN-based optimization to achieve globally consistent estimation from an effective keyframe selection mechanism. It can improve the robustness of learning-based methods with flow-guided keyframes and well-established depth prior. Experimental results show that GCVD outperforms the state-of-the-art methods on both depth and pose estimation. Besides, the runtime experiments reveal that it provides strong efficiency in both short-and long-term videos with global consistency provided.\n\nIntroduction\n\nAcquiring dense depth and camera pose from videos is essential for various applications including augmented reality (Holynski and Kopf 2018;Du et al. 2020), video frame interpolation (Bao et al. 2019), view synthesis (Choi et al. 2019;Yoon et al. 2020;Liu et al. 2021) and stabilization (Liu et al. 2009;Lee et al. 2021). In this paper, we propose a learning-based approach to achieve the concurrent inference of scene depth and camera pose from offline videos. Our study belongs to the research track of SfM with the videos acquired. Unlike visual SLAM (Engel, Sch\u00f6ps, and Cremers 2014;Mur-Artal, Montiel, and Tardos 2015;Teed and Deng 2021) assuming streaming videos that should be estimated on-line, more information of batched frames stored in the video can be used for a globally consistency estimation.\n\nInferring both depths and poses for every frame is essentially a challenging chicken-and-egg problem. Traditional solutions rely on the established SfM tools (e.g., COLMAP (Schonberger and Frahm 2016)) to estimate the camera trajectory and then perform multi-view stereo. However, the tools often yield incomplete depth and suffer from the robustness issue due to the fragile, sparse feature tracking.\n\nCopyright \u00a9 2022, All rights reserved. The proposed method outperforms the state-of-the-art methods CVD2 (Kopf, Rong, and Huang 2021) and Deep3D (Lee et al. 2021) on both depth and pose estimation on 7-Scenes dataset (Shotton et al. 2013). Furthermore, our method performs the fastest in short videos; and slightly slower than Deep3D in long videos while Deep3D does not maintain global consistency.\n\nEspecially in the videos containing dynamic objects, motion blur, or large texture-less regions, the estimation process usually fail early even in the pre-processing step.\n\nIn learning-based studies, many works (Eigen, Puhrsch, and Fergus 2014;Ranftl et al. 2020;Miangoleh et al. 2021) focus on estimating the depth using a single image (i.e., single depth estimation). Though every single depth is visually plausible in 2D, it is up-to-scale or even a projective ambiguity (Hartley and Zisserman 2003). Thus the individual depths obtained from sequential frames suffer easily from temporal and geometric inconsistency. To learn from videos, unsupervised (or self-supervised) methods Yin and Shi 2018;Godard et al. 2019;Bian et al. 2019) are proposed. They may maintain the scale consistency (Bian et al. 2019) but their capability of generalizing the pre-trained model to the testing scene is usually restricted by the training data.\n\nTo overcome these issues, test-time training video-based SfM methods are introduced (Luo et al. 2020;Kopf, Rong, and Huang 2021;Lee et al. 2021), which optimize the depth and pose of a test video directly. The solutions provide a promising way to maintain the depth and ego-motion co-herence in an input video. Luo et al. propose CVD (Luo et al. 2020) to refine the single-depth network for enhancing depth consistency of a monocular video. However, it relies on the COLMAP tool in advance to obtain the camera poses and thus is still limited by the robustness issue of traditional SfM. Kopf et al. extend CVD to CVD2 (Kopf, Rong, and Huang 2021) without requiring SfM tools. CVD2 performs simultaneous depth and pose optimization with the off-theshelf single depth network, MiDaS (Ranftl et al. 2020), as initial. An issue is that the optimization is often affected by the bias of single depths and therefore yields poor pose results. Lee et al. present Deep3D (Lee et al. 2021) to learn depth and pose of the test video to solve video stabilization problem. However, the optimization still results in suboptimal estimations for short-camera-baseline videos due to the lack of single depth prior. Moreover, CVD2 and Deep3D ensure temporal consistency in a local range, whereas neither of them consider global consistency which is crucial especially in long videos.\n\nGlobal-consistency of video depth: We argue that current test-time training methods lack of the ability to achieve global consistency between the depth and pose, which is a demanding issue for accurate and long-term SfM estimation and thus they will yield drifting outcomes for the input videos. We propose GCVD, a globally consistent test-time training method for depth and pose estimation based on offline videos. GCVD has the advantage that it can achieve robust inference without relying on traditional SfM tools. Our method integrates a keyframe-based pose graph into learning to attain the global consistency. Fig. 2 illustrates its pipeline. In our method, the keyframes are extracted from the input video to compose a pose graph with sequential and associated non-sequential edges. It estimates the depth and pose of keyframes and performs pose graph optimization to fulfill the global consistency. Then, the depth and pose of the remaining frames can be estimated efficiently leveraging the keyframes. The experimental results validate the strength of GCVD ( Fig. 1) that our approach outperforms the state-ofthe-art approaches, CVD2 and Deep3D, on both depth and pose estimations (7-Scenes dataset (Shotton et al. 2013)). Besides, it is significantly faster than CVD2 and achieves a good trade-off between efficiency and global consistency in contrast to Deep3D. The main characteristics of our GCVD include:\n\nGlobal consistency: To tackle the challenging on joint depth and pose estimation based on video collections, to our knowledge, we introduce the first test-time-training method that enforces the global consistency with robustness.\n\nEfficient global-pose-graph and optimizer: Our method requires merely the pose-only bundle adjustment on the keyframes, and can then leverage network learning for estimating the depths and poses for all frames efficiently.\n\nPerformance improvements and competitive speed: Our method outperforms SOTA on both depth and pose with over 19% improvement on 7-Scenes dataset (Shotton et al. 2013), and also shows strong computational efficiency.\n\n\nRelated Work\n\nTraditional Approach: Traditional SfM (Wu et al. 2011;Moulon et al. 2016;Schonberger and Frahm 2016) jointly estimate the 3D structure and camera poses of multi-view images via bundle adjustment (Agarwal, Mierle, and Others 2022;K\u00fcmmerle et al. 2011) on the local features. Subsequently, multi-view stereo (MVS) (Furukawa and Hern\u00e1ndez 2015) following the estimated pose obtain dense depth but often yield holes and noises in general.\n\nIn another track, traditional visual odometry (VO) and SLAM (Klein and Murray 2007;Engel, Sch\u00f6ps, and Cremers 2014;Mur-Artal, Montiel, and Tardos 2015) usually maintain keyframes and a pose graph to perform efficient and consistent localization. Besides, bundle adjustment (BA) (Triggs et al. 1999) and pose graph optimization (K\u00fcmmerle et al. 2011) can be introduced to prevent drifts and enhance global consistency.\n\nThe performance of traditional methods generally rely on successful feature tracking. The sparse local features (Lowe 2004;Bay, Tuytelaars, and Van Gool 2006;Rublee et al. 2011) extracted are often fragile in various challenging conditions such as homogeneous areas, motion blur, and illumination changes. Hence, they are demanding to obtain dense depth maps and non-robust enough to handle the texture-less and blur situations for the videos in the wild. Learning Depth Only (Given Pose): Supervised learning has been widely used for the ill-posed single depth estimation problem (Eigen, Puhrsch, and Fergus 2014;Liu et al. 2015;Eigen and Fergus 2015). Apart from acquiring real depth as groundtruth, some methods (Garg et al. 2016;Godard, Mac Aodha, and Brostow 2017;Gonzalez and Kim 2021) learn the single depth with binocular pairs. Other studies leverage synthetic data (Mayer et al. 2016) or pseudo groundrtuth (Chen et al. 2016;Li and Snavely 2018;Chen et al. 2020;Li et al. 2019b). MiDaS (Ranftl et al. 2020) obtains relative depth of stereo pairs from large-scale and diverse 3D movies. Recently, Miangoleh et al. (Miangoleh et al. 2021) integrate multi-scale depth of MiDaS to handle high-resolution images. Yin et al. (Yin et al. 2021) utilize point cloud networks to solve the perspective ambiguity of a single depth. Despite the single-depth methods show visual plausibility on individual depth maps, the issue of geometrical inconsistency among multi-views is not addressed.\n\nWith known camera poses, learning-based multi-view stereo can estimate dense depth of multiple images. In (Huang et al. 2018;Yao et al. 2018;Im et al. 2019), plane sweep algorithm is employed to estimate dense depth in a supervised manner. The methods in (Long et al. 2021;Wimbauer et al. 2021) estimate video depth with known camera pose or obtain the pose via COLMAP. Depth and/or Pose Supervision and Joint Estimation: Approaches of this type utilize groundtruth depths and/or known-pose views for training; the obtained jointdepth/pose estimator is then applied to the test-scene sequences of unknown poses. For depth estimation, UNetlike models are used to perform per-pixel depth regression (Ummenhofer et al. 2017;Zhou, Ummenhofer, and Brox 2018;Bloesch et al. 2018;Czarnowski et al. 2020). Deep cost volume with plane sweep structure are also 2: Pipeline overview. GCVD takes a monocular RGB video as input to estimate per-frame depth and pose. The preprocessing and pose graph generation stage (Sec. ) first estimates adjacent optical flows and find static masks to filter likelydynamic regions. It then uses single depths as prior for the optimization stage and constructs a pose graph by selecting the keyframes as vertices. The sequential (blue) and non-sequential (red) edges are thus formed via keyframe association. The optimization stage (Sec. ) takes the estimations in the pre-processing stage as priors to optimize the depth and pose of input frames using the same Joint Optimization of Depth and Pose (Sec. ). The keyframe 3D estimation is followed by pose-only bundle adjustment (BA) for global consistency. The depth of keyframes and depth/pose of the remaining non-keyframes can then be optimized more efficiently through learning.\n\nadopted (Wei et al. 2020;Teed and Deng 2019;Wang et al. 2021). To estimate the pose, some methods directly use networks for regression (Ummenhofer et al. 2017;Zhou, Ummenhofer, and Brox 2018;Wei et al. 2020;Teed and Deng 2021); some others leverage the fundamental multi-view and epipolar geometry Bloesch et al. 2018;Teed and Deng 2019;Czarnowski et al. 2020;Wang et al. 2021). However, acquiring the groundtruth pose or depth in real world is non-trivial (e.g., requiring LiDAR (Saxena, Sun, and Ng 2008;Geiger et al. 2013) or RGB-D cameras (Sturm et al. 2012;Nathan Silberman and Fergus 2012;Shotton et al. 2013)). The demanding ability of generalization to the testing data of unknown scenes also restricts the usage in practice. Self-Supervision for Depth and Pose: Methods in this category learn a joint depth/pose inference network in a selfsupervised (or unsupervised) manner without relying on pregiven groundtruths Gordon et al. 2019;Godard et al. 2019;Chen, Schmid, and Sminchisescu 2019;Li et al. 2019a;Bian et al. 2019;Ranjan et al. 2019;Zou et al. 2020;Jiao, Tran, and Shi 2021;Watson et al. 2021;Zhou et al. 2021;Jung, Park, and Yoo 2021). They cooperatively regulate the depths and poses on monocular videos via warping-based view synthesis from the training data. Further approaches use optical flow (Yin and Shi 2018;Ranjan et al. 2019;Jiao, Tran, and Shi 2021) and segmentation (Casser et al. 2019;Ranjan et al. 2019) to improve the performance and handle dynamic objects. Yet, in test time, most networks still perform single-depth estimation and thus yield incoherent results. The methods also suffer from the generalization issue due to the domain gap between training and testing data. Test-Time Training Approach: To avoid the difficulty of applying the inference model learned from given environment to unseen environment, test-time-training methods have been proposed more recently, making the solutions better for practical use. Despite the speed of test-time-training is slower than pure inference, the approaches meet the requirement of offline videos that need no real-time processing. CVD (Luo et al. 2020) is the first attempt toward test-time training to obtain geometrically consistent depth and pose estimation of a video. However, CVD relies on the SfM tool COLMAP which suffers from the fragile and sparse reconstruction and the computation is slow. Deep3D (Lee et al. 2021) uses a network-based optimization framework to learn video depth and pose for video stabilization, but the depth performance is restricted due to the lack of single depth prior. CVD2 (Kopf, Rong, and Huang 2021) uses a deformative mesh on single depth with a post-processing filter to show promising depth results; however, the pose performance still suffers from the bias of single depth as no finetuning mechanisms have been accommodated for the depth net. Besides, CVD2 requires tremendous optimization time. Its quantitative performance is only evaluated on the videos of 50 frames (Wulff et al. 2012).\n\nCurrent works only ensure local consistency with nearby frames, whereas the global consistency is not tackled. We introduce GCVD that is the first test-time-training method with global consistency, which can obtain more accurate and robust results. Due to the global consistency, GCVD is scalable to long videos. In our experiments, 1000-frames videos are used to validate the performance.\n\n\nMethodology\n\nGiven an N -frame video I 1..N , our goal is to estimate dense depth maps D 1..N and camera poses P 1..N . Our CNN-based optimization framework utilizes the learning-based SfM to jointly estimate depth and pose of each frame. Nevertheless, applying SfM to a video may suffer from various challenges such as small camera baseline motion and lacking of covisible scenes among frames. Proper baselines arrangement: Current test-time-training solutions (e.g., Deep3D (Lee et al. 2021) and CVD2 (Kopf, Rong, and Huang 2021)) simply take near-to-distant neighbor frames with certain frame intervals empirically selected (e.g., 1, 2, 4, 8) to ensure a coherence baseline. The strategy does not take the real disparities between the frames into account, and does not guarantee the proper baselines among the frames for SfM. In our work, we leverage the recent progress of deep optical flow estimation, and introduce dense flow-guided keyframes to perform initial optimization with adequate baseline motions. The pipeline (Fig. 2) is introduced in Pre-processing and Pose Graph Generation and Keyframe-based 3D Optimization and the networkbased optimization module is elaborated in Joint Optimization of Depth and Pose.\n\n\nPre-processing and Pose Graph Generation\n\nVideos often contain motion blurs and large view-direction changes, yielding failures of traditional sparse features on matching. We use learning-based optical flows among video frames to overcome this difficulty and obtain dense flow maps. However, in contrast to existing approaches CVD2 and Deep3D which need the computation of flows from a target frame to many other frames, our GCVD initially estimates the depth and pose of keyframes with reliable camera baselines and then enforces global optimizations later; thus, we can only take the flow of adjacent views (i.e.,F t t\u00b11 ) to save the computation burden.\n\nBesides, to prevent dynamic objects from disturbing pose estimation, semantic segmentation (e.g., Mask-RCNN in Detectron2 (Wu et al. 2019)) is used to obtain a binary mask M t that filters out the likely-dynamic pixels in each frame I t . We also take the single depths of MiDaS (Ranftl et al. 2020) as the depth prior to regularize the optimization.\n\nThe ideas of keyframe and pose graph have been widely used in traditional SLAM (Klein and Murray 2007;Engel, Sch\u00f6ps, and Cremers 2014;Mur-Artal, Montiel, and Tardos 2015) and SfM (Schonberger and Frahm 2016;Barath et al. 2021) to reduce the computational complexity and ensure global consistency for large-scale reconstruction. Our method constructs a pose graph. Nevertheless, unlike traditional SLAM or SfM, we use learning-based approach to provide better robustness in the optimization. The pose graph has k keyframes (\u03ba 1..k ) as its vertices, and the edges of the graph include the sequential edges and the nonsequential co-visible edges, as depicted below. Dense-flow-guided keyframe decision. To sample representative keyframes from videos, traditional solutions rely on sparse feature tracking while long feature tracks are challenging to obtain. Instead, the dense optical flow acquired in the pre-processing can provide a reliable reference for keyframe decision. Thus, the frame I t is chosen as a keyframe if the accumulated flow magnitudeF \u03ba * t from the last selected keyframe \u03ba * exceeds a movement threshold \u2206.\nF \u03ba * t = t\u22121 i=\u03ba * \uf8eb \uf8ed 1 |M i | x\u2208Mi F i i+1 2 \uf8f6 \uf8f8 ,(1)\nwhere we set \u2206 = 0.1 and only use the static regionsM for evaluation. Then, the flow of adjacent keyframes,F \u03bai \u03bai\u00b11 , are established for the keyframe optimization. As we use the accumulated adjacent flows to pick just-needed frames, compared to uniform selection, a more compact and exemplary keyframe set can be built. After selecting k keyframes, the sequential edges are formed by connecting the keyframes of nearby indices within a subset of \u03ba i\u00b1\u03b1 in the pose graph to ensure local consistency. For those keyframe pairs with Figure 3: Joint optimization of depth and pose. The 3D optimization module takes a set of frame pairs as input to estimate the depths and poses. The depth component adopts MiDaS (Ranftl et al. 2020) with an additional layer normalization and a learnable mesh deformation (Kopf, Rong, and Huang 2021). The pose component consists of ResNetbased encoder-decoder (Godard et al. 2019) with a positional encoding (Vaswani et al. 2017) to encourage better convergence. Both Steps 1 and 3 in the Keyframe-based 3D Optimization stage of Fig. 2 use this module for learning. the index differences exceeding \u03b1, we will consider their co-visibility of shared scenes to form the further edges of non-sequential co-visible views to enforce better the global consistency. CNN-based keyframe association. The keyframe pairs with high image similarities are picked and geometrically verified to serve as non-sequential co-visible edges. We leverage the deep features of the k keyframes and compute their cosine similarity one-by-one to form a similarity matrix A k\u00d7k . For each keyframe, the feature is extracted by an ImageNet-pretrained ResNet encoder (Godard et al. 2019). The output feature is then passed through a global average pooling layer and L2 normalization. Thus, A k\u00d7k can be simply computed by the inner-product of the k normalized feature vectors. Then, the associated pairs are sampled from A by a similarity threshold \u03b4 (= 0.9) and max-pooling for reducing redundant pairs.\n\nBesides, the geometric verifications are necessary to filter the noisy associated keyframe pairs. Traditional verifications utilize the inlier ratio of estimated fundamental matrix or homography via SIFT (Lowe 2004) and RANSAC (Fischler and Bolles 1981), while it is not reliable enough. We additionally examine the forward-backward consistency of the dense optical flow of each associated pair. Moreover, to guarantee adequate co-visible areas for optimization, the associated pairs are removed if the average flow magnitude exceeds the movement threshold \u2206 of keyframe decision.\n\n\nKeyframe-based 3D Optimization\n\nThe proposed pipeline performs keyframe optimization with suitable camera movement to achieve robust estimation as initials. Then, pose graph optimization is utilized to retain global consistency of keyframes' pose estimations. Finally, the depth and pose of the remaining frames are estimated efficiently according to the pose graph obtained. In the following, we give an overview of the procedures. Details of our joint depth-and-pose deep network model is depicted in Joint Optimization of Depth and Pose. Keyframe 3D estimation via deep model. For each keyframe \u03ba i , the depth and pose are learned with multiple nearby keyframes of \u03ba i\u00b1\u03c4 (i.e., the sequential edges in pose graph) with a descending weight 1 \u03c4 to acquire consistent results. Besides, a mini-batch of keyframes are optimized simultaneously with a GPU. Thus, the mini-batches are overlapped with the interval \u03c4 to ensure coherent solutions (Lee et al. 2021). In our work, we set \u03c4 \u2208 {1, 2, 4, 8}. The relative poses obtained for sequential edges are then recorded for the next pose-only bundle adjustment. Likewise, we optimize the non-sequential co-visible keyframe pairs (without decreasing weights) and obtain the their relative poses accordingly. Hence, each edge of the pose graph is set up with an initial relative-pose transformation. Global pose graph optimization with efficiency. The pose graph optimization (K\u00fcmmerle et al. 2011) is used to refine the pose estimations of keyframes from the above initialization. Note that we perform pose-only bundle adjustment for P \u03ba 1..k rather than depth and pose bundle adjustment for efficiency. Instead, the depths of keyframes are fine-tuned in the next step leveraging the learning models. The extensive bundle-adjustment overhead can thus be shared with deep networks that are cooperated to yield a more efficient optimization. Non-keyframe 3D optimization and keyframe depth refinement. Besides fine-tuning the keyframe depth, the remaining non-keyframes are optimized with the fixed keyframe poses (i.e., P \u03ba 1..k ) over fewer iterations via the deep network. Likewise, (D t , P t ) of each frame is optimized with multiple nearby views t \u00b1 \u03c4 . We then obtain the depth D 1..N and pose P 1..N of the entire video.\n\n\nJoint Optimization of Depth and Pose\n\nIn this section, we introduce the optimization module for 3D estimation. Our approach leverages the deep networks to save the scene information in the model weights to facilitate sequential fragments optimization. The module takes a set of frame pairs as input and estimate the depths and poses simultaneously. For simplicity, we depict the module with a pair of images (I a , I b ) as input. More pairs simply use the sum of respective loses. The networks learn both depth and pose to obtain the the output D a , D b , P a , P b with the objectives designed below. Depth and pose components. As shown in Fig. 3, the depth and pose components estimate the individual depth D t and 6-DoF global pose P t , respectively, associated with an input RGB frame I t . The depth component exploits a MiDaSpretrained network (Ranftl et al. 2020) with an additional layer normalization to stabilize the output scale of depth estimation. Then, a learnable mesh deformation (Kopf, Rong, and Huang 2021) is adopted to achieve better alignments among sequential depths. Unlike CVD2 (Kopf, Rong, and Huang 2021) that directly takes fixed single depths as initials, we use a trainable depth network that can refine the bias of the initial depth to encourage better estimations. While due to the time and space efficiency, only the last two convolutional layers of MiDaS network are used to learn a larger span of frames at a time.\n\nThe   (Ranftl et al. 2020). The result without L grad may introduce blurring after updating the depth component.\n\nto the depth component, the pose encoder is frozen with ImageNet-pretrained weights and only the decoder can be optimized. The information saved in the learned weights of the decoder can help boost the convergence for the next optimization. Moreover, the feature map is added with a positional encoding (Vaswani et al. 2017) which encodes the chronological order of the entire sequence to enhance the learning for the sequence.\n\nObjectives. The proposed objectives include inter-frame constraints for geometrical consistency and intra-frame regularization. The inter-frame objectives ensure consistent estimations between two views. The test-time learner exploits the point transformation between (I a , I b ) via 3D projection as formulated below:\nx b \u223c KP b P \u22121 a D a (x a )K \u22121x a ,(2)\nwherex a andx b denote the homogeneous form of a pixel in I a and I b , respectively. K stands for the camera intrinsics. Accordingly, the rigid flow F b a = x b \u2212 x a is used to realize the inter-frame constraints in the following three aspects.\n\nThe photometric loss computes the appearance bias between I a and the synthesized I b a (warped using F b a ) by L 1 and structure dissimilarity (Wang et al. 2004) losses:\nL photo a,b = 1 |V b a | x\u2208V b a I b a \u2212 I a 1 + DSSIM (I b a , I a ),(3)\nwhere V b a denotes the valid points projected from I b onto the image plane of I a and excluding likely-dynamic pixels byM a .\n\nThe optical flow loss measures the displacement error in the image space. Hence, the flowF generated in the preprocessing is used as the supervision for the rigid flow F . We examine the forward-backward consistency between the pre-processing flowsF b a andF a b to form a binary mask\u1e7c b a , and letV\nb a =\u1e7c b a \u2229 V b a . L f low a,b = 1 |V b a | x\u2208V b a F b a \u2212F b a 1 .(4)\nThe depth consistency loss L const (Bian et al. 2019) assesses the inconsistency between individually estimated depth D a and D b .\nL const a,b = 1 |V a | xa\u2208Va D b a \u2212 D a 1 D b a + D a ,(5)\nwhere D b a is the transformed depth of I a using D b , P b P \u22121 a .  Table 1: Quantitative evaluations of depth and pose on 7-Scenes dataset (Shotton et al. 2013). The standard depth evaluation measures per-frame errors and accuracy metrics. The pose evaluation computes the per-sequence pose errors. \u2020We also refer to the approaches of multi-view stereo and 3D reconstruction with known camera pose and compare with the results reported in NeuralRecon (Sun et al. 2021).\n\nApart from inter-frame losses, the intra-frame objectives are utilized to regularize each depth D t , including the dynamic areas. We conduct the depth gradient loss L grad t to preserve the depth prior from pre-computed MiDaS dept\u0125 D t . The optimization can refine the bias of initial single depth. Thus, the single depthD t obtained in the preprocessing is exploited to provide the supervision of depth edge (Fig. 4). Let \u2207D s t (x) denote the 2D gradient vector of pixel x in the downsampled depth map D s t , s \u2208 {0, 1, 2}. We measure the orientation difference of depth gradients to avoid scale difference betweenD t and D t .\nL grad t = s x 1 \u2212 \u2207D s t (x) \u00b7 \u2207D s t (x) \u2207D s t (x) 2 \u2207D s t (x) 2 2 .(6)\nThe regularization of deformation proposed by Kopf et al. (Kopf, Rong, and Huang 2021) maintains the spatial smoothness of learnable mesh for a flexible deformation. Likewise, we use the regularization loss L def orm t to encourage smoothness in dynamic area 1 \u2212M t . Finally, the total loss L of a pair of (I a , I b ) is conducted as:\nL =\u03bb photo L photo + \u03bb f low L f low + \u03bb const L const + \u03bb grad L grad + \u03bb def orm L def orm ,\nwhere the inter-frame objectives compute the bidirectional losses and the intra-frame objectives sum up the losses of individual frames. The weights \u03bb photo , \u03bb f low , \u03bb const , \u03bb grad , \u03bb def orm are set as 1, 10, 0.5, 0.1, 0.5, respectively. Note that L f low is used only when I a and I b are adjacent.\n\n\nImplementation Detail\n\nThe approach is realized in PyTorch with Adam and g2o library (K\u00fcmmerle et al. 2011). The resolution of depth and deformation mesh are 384 and 17, respectively, following CVD2 (Kopf, Rong, and Huang 2021) for the longer side of frame. RTX3090 GPU is used on the mini-batch size 40. We run the optimizations of sequential keyframes, nonsequential keyframes, and non-keyframes with 300, 100 and 100 iterations, respectively. We further perform flow-guided depth filter like CVD2 (Kopf, Rong, and Huang 2021) as post-processing. The optical flow (Teed and Deng 2020) is used. More details are given in the appendix.\n\n\nExperiments\n\nWe compare the proposed method with the SOTA test-timetraining methods, CVD2 (Kopf, Rong, and Huang 2021) and Deep3D (Lee et al. 2021). CVD2 jointly optimizes pose and learnable deformation from initial MiDaS (Ranftl et al. 2020) depths. Deep3D takes DepthNet and PoseNet (Godard et al. 2019) and learns from ImageNet pretrained weight to acquire depth and pose. For fair comparisons, we assume an ideal camera intrinsic and re-implement Deep3D with the same resolution of depth, optical flow estimation (Teed and Deng 2020) and static masks as CVD2 and ours. In addition, we compare our approach with the SOTA supervised SLAM systems Deng 2019, 2021), where the SLAM mode of DeepV2D (Teed and Deng 2019) performs pose optimization in a tracking window and DROID-SLAM (Teed and Deng 2021) utilizes dense bundle adjustment to achieve global consistency.\n\nDatasets. In contrast to CVD2 conducting evaluations on synthetic video clips (with each only 50-frames long) of Sintel dataset (Wulff et al. 2012), We conduct the experiments on long sequences (500 to 3000 frames) of realworld datasets.\n\n\u2022 7-Scenes RGB-D dataset (Shotton et al. 2013) has 46 sequences with either 500 or 1000 frames. The indoor scenes are grabbed with a Kinect camera at size 640\u00d7480.\n\n\u2022 TUM RGB-D dataset (Sturm et al. 2012) is gathered by a handheld Kinect camera with more demanding cases such as texture-less area and abrupt motions. Seven representative sequences (613\u223c2965 frames) in TUM RGB-D are used for evaluation.\n\n\u2022 EuRoC dataset (Burri et al. 2016) has 11 sequences (1710\u223c3682 frames) filmed by a micro aerial vehicle. We demonstrate the comparisons in our appendix.\n\nEvaluation metrics. We follow the standard depth evaluation (Eigen, Puhrsch, and Fergus 2014) to align the scales between the estimated and groundtruth depths by median scaling. The pose evaluation uses the metric of visual odometry (Sturm et al. 2012;Zhang and Scaramuzza 2018), including absolute trajectory error (ATE) and relative pose error (RPE) with 7-DoF alignment.  (Shotton et al. 2013). The 3D point cloud is back-projected from a view with the estimated depth. Deep3D (Lee et al. 2021) produces in weak depth results and even collapse (constant value). CVD2 (Kopf, Rong, and Huang 2021) results in poor pose performance (observed via the bad alignment of the green and blue trajectories) despite the plausible depth. Our method shows the most favorable performance on both depth and pose. Zoom in for more detailed visualization is suggested. Evaluation on 7-Scenes (Shotton et al. 2013) Table 1 shows the quantitative comparisons of our approach to the SOTA methods. Although DeepV2D (Teed and Deng 2019) and DROID-SLAM (Teed and Deng 2021) utilize pose graph for the pose refinement and bundle adjustment in testing, they are restricted by the generalization capability of supervised learning in different datasets. Test-time training approach Deep3D (Lee et al. 2021) shows the second best pose performance; however, it results in worse depth estimation. CVD2 (Kopf, Rong, and Huang 2021) maintains more depth priors from MiDaS while the bias in the depth prior leads in poor pose estimation. Besides, we compare our GCVD with other depth-estimation approaches with known camera pose. The quantitative depth scores of DPSNet (Im et al. 2019), CNMNet (Long et al. 2020), and NeuralRe-con (Sun et al. 2021) are provided by (Sun et al. 2021). Similar to DeepV2D and DROID-SLAM, the supervised methods with known pose may still suffer from the domain discrepancy between training and test data. In contrast, the proposed GCVD achieves globally consistent optimization on test data and demonstrates the best performance on both standard depth and pose metrics.\n\nWe conduct visual comparisons by displaying the backprojected point clouds and the pose estimation with the groundtruth trajectory by 7-DoF alignment (Zhang and Scaramuzza 2018). As shown in Fig. 5, although Deep3D demonstrates some promising camera estimations, it is prone to collapse on the depth estimation. CVD2 provides plausible depths with the aids of MiDaS while yields weak pose results due to the lack of global consistency. Again, our method reveals better results in 3D visualization. Figure 6: Visual comparisons of geometric consistency. Visualizing the geometric consistency via the point clouds between two distant views (200-frames interval). As shown in the groundtruth, the back-projected point clouds (brown and cyan) from two views are supposed to align in the co-visible area. Our GCVD and Deep3D (Lee et al. 2021) maintain better geometric consistency and show promising alignments of point clouds, while CVD2 (Kopf, Rong, and Huang 2021) Table 2: Quantitative evaluation on TUM RGB-D (Sturm et al. 2012). We present the depth and pose errors of each sequence and compare with Deep3D (Lee et al. 2021), and CVD2 (Kopf, Rong, and Huang 2021). We additionally provide the pose results of COLMAP (Schonberger and Frahm 2016) as reference.\n\nEvaluation on TUM-RGBD (Sturm et al. 2012) In this experiment, we compare our GCVD with the testtime training approaches (CVD2 and Deep3D) and also COLMAP which is a traditional representative solution having maintaining global consistency. Table 2 provides the per-sequence quantitative errors. We only present the pose results of COLMAP since its dense depths via multi-view stereo still contain holes and noises. Although COLMAP shows superior pose results, it completely fails (collapses) in a sequence (marked as red in Table 2) due to the fragile sparse reconstruction. As for test-time training, our method attains the lowest depth and pose errors in overall. We handle depth prior properly with the learnable networks to facilitate pose learning. Thus, our approach shows better pose estimation on most sequences compared with CVD2, which accumulates more pose errors for long sequences. Our GCVD can refine the long pose estimation for maintaining global consistency. On the other hand, we find the weak pose performance of some sequences affected by the unreliable precomputed optical flow, which will be discussed in the limitations of our approach in the appendix. Besides, we compare the geometric consistency by visualizing the alignment of point clouds from different views. The two distant frames viewing the common scenes are back-projected the 3D point clouds. Hence, the geometric inconsistency can be seen by the misalignment between the two point clouds in the co-visible area. As shown in Fig. 6, Deep3D and ours show similar point cloud alignments to the groundtruth's because both approaches have the depth finetuning mechanisms. Nevertheless, the accumulated depth bias leads CVD2 to yield poor pose estimation and large misalignment between the views.\n\n\nAblation studies\n\nWe conduct ablation studies on 7-Scenes dataset (Shotton et al. 2013) in Table 3. For the depth component, the inserted layer normalization stabilizes the depth scale of Mi-DaS (Ranftl et al. 2020) network and hence facilitates the depth and pose performance by 35% and 40%, respectively. The flexible deformation (Kopf, Rong, and Huang 2021) regulates the spatial misalignments of each depth to improve depth estimation by 4%. Besides, the depth gradient loss retains the initial depth priors during refining the depth bias. The well-handled depth prior can encourage a better joint optimization, thus reducing the pose errors by 15% (0.327 to 0.277). Moreover, the method without the keyframe strategy raises 11% pose error (0.277 to 0.308) due to the lack of proper camera baselines for initial optimization. Finally, the pose graph optimization for global consistency further cuts down the pose error by 10% (0.277 to 0.249).  Table 3: Ablation studies on 7-Scenes (Shotton et al. 2013). We validate the effectiveness of the keyframe strategy (KF), the added layer normalization and mesh deformation in the depth component, depth gradient loss, and pose graph optimization (PGO) in our pipeline.  Table 4: Runtime comparison. We compute the per-frame runtime (sec) in different lengths of videos. Our method shows the fastest speed in short videos and achieves a good trade-off between efficiency and global consistency compared with Deep3D.\n\n\nRuntime comparisons\n\nWe compare the runtime of our GCVD with the test-timetraining methods. We select five long videos and extract the first n frames of the videos to compose different lengths of sequences for runtime evaluation. The execution times of the videos are measured on an i7-11700K with a RTX3090 GPU. We present the averaged per-frame time in Table 4. Note that we do not compare with COLMAP which requires extremely expensive time (e.g., \u223c56 secs for each frame on a 2000-frame video). CVD2 shows about 6 to 9 times slower than our method due to the preparation of multiple pairs of optical flow and the traditional optimizer with CPU. Deep3D provides strong efficiency in long videos; however, it tends to yield drifts and collapse in depth estimation. In contrast, our method performs fastest in short videos by learning few keyframes first then optimizing the non-keyframes with fewer iterations. For long videos, our GCVD is slightly slower than Deep3D due to keyframe association on more keyframes for global consistency.\n\n\nConclusion\n\nWe present GCVD, a learning-based method for video depth and pose estimation with global consistency and efficiency. To our knowledge, this is the first study tackling global consistency for test-time training. Based on the global poses of keyframes from the pose-only bundle adjustment, the deep networks jointly learn keyframe depth refinement and the depth and pose of the remaining frames efficiently. In addition, our proposed method can better handle single depth prior properly and fine-tune the depth network to alleviate depth bias and achieve robust and consistent 3D estima-tion. Experimental results show that GCVD outperforms the state-of-the-art approaches on both depth and pose evaluation. Moreover, GCVD achieves high efficiency by keeping the scene knowledge in network weights to boost the optimization of next fragment of frames. We will release our codes to public. In contrast to COLMAP that uses traditional techniques, our GCVD is a fundamental deep-learning tool for the offline-video SfM.\n\n\nImplementation Detail\n\nKeyframe-based pose graph optimization. We perform pose graph optimization with g2o (K\u00fcmmerle et al. 2011) for globally consistent pose estimation. The edges of the pose graph include sequential and non-sequential edges. Each sequential edge e i,i\u00b1\u03c4 , \u03c4 \u2208 {1, 2, 4, 8} of the pose graph connects the keyframe pair (\u03ba i , \u03ba i\u00b1\u03c4 ) with the relative pose P \u22121 \u03bai P \u03bai\u00b1\u03c4 and the weight matrix diag( 1 \u03c4 ) to ensure temporal coherence in a local range. The sequential edges contain relatively nearby views determined by the opticalflows. However, in addition to the sequential edges, there could be farther views which share co-visible scenes. Hence, we also establish the non-sequential edges via keyframe association, which connects the keyframe pairs with the index differences exceeding \u03b1 = max(\u03c4 ) but sharing a covisible scene to enhance global consistency. Similarly, the non-sequential co-visible edges are constructed with the optimized relative poses and identity weight matrices. The pose graph optimization is conducted with at most 100 iterations to obtain global pose estimation P \u03ba 1..k . Afterward, the depth and pose of the remaining frames and the depth of keyframes are estimated simultaneously with the frozen keyframe poses to maintain global consistency. Figure 7 demonstrates the effectiveness of the pose graph optimization for achieving the global consistency. Detail of joint depth and pose optimization. The testtime optimization framework is implemented in PyTorh with Adam (with \u03b2 1 = 0.9, \u03b2 2 = 0.999). The learning rates for sequential keyframes, non-sequential keyframes, and nonkeyframes are 2\u00d710 \u22124 , 5\u00d710 \u22125 , and 1\u00d710 \u22124 , respectively. To speed up the optimization, we compute the loss with a quarter scale of depth estimation in the keyframe optimization. Thus, the depth of keyframes will be further refined in the non-keyframe optimization with original scale (i.e., 384 for the longer side of the frame). Post-processing. We follow the flow-guided depth filter proposed in CVD2 (Kopf, Rong, and Huang 2021) to further enhance the temporal consistency of edge details in depth maps. The final filtered depthD t acquires the depth details from neighboring depths D t\u2212\u2126..t+\u2126 (\u2126 = 4) with the chaining optical flowF .D\nt = t+\u2126 i=t\u2212\u2126 \u03c9 i\u2192t D i\u2192t ,(7)\nwhere D i\u2192t is the projected depth of I t by transforming D i with P t , P i and the chaining flowF i\u2192t to align the pixel coordinate. The maximum span \u2126 is set as 4 by default, and the weight term \u03c9 i\u2192t considers both the depth reprojection error and the forward-backward inconsistent error of the chaining flow as follows:\n\u03c9 i\u2192t = exp \u2212\u03b3 1 max(D t , D i\u2192t ) min(D t , D i\u2192t ) \u2212 \u03b3 2F dif f i\u2192t ,(8)\nwhereF dif f i\u2192t denotes the forward-backward inconsistency between the chaining flowF i\u2192t andF t\u2192i . The \u03b3 1 = 2 and \u03b3 2 = 0.1 balance the strength of the temporal depth filter. In the end, the final consistent depthD 1..N and pose P 1..N of the entire video I 1..N is accomplished.\n\n\nComparison with SOTA learning-based SLAMs\n\nAlthough the SLAM approach reconstructs depth and pose from online (streaming) videos, which is different from our problem setting of video SfM for offline videos, we compare with the state-of-the-art supervised SLAMs Deng 2019, 2021) which tackle global consistency. DeepV2D (Teed and Deng 2019) performs global pose optimization with a tracking window of eight frames. DROID-SLAM (Teed and Deng 2021) utilizes dense and full bundle adjustment to achieve global consistency. Table 5 presents the quantitative comparison on 7-Scenes (Shotton et al. 2013) and TUM RGBD (Sturm et al. 2012) datasets. Though DROID-SLAM achieves a superior score on the pose metric of Absolute Trajectory Error (ATE) on TUM RGBD, it shows the deficiency on the other pose metric, Relative  (Shotton et al. 2013) and TUM RGBD dataset (Sturm et al. 2012 We also compare GCVD with the traditional state-of-theart SLAM approach ORB-SLAM2 (Mur-Artal, Montiel, and Tardos 2015), which performs loop closure to retain globally consistent poses and 3D map. Table 6 shows the pose comparisons with traditional COLMAP (Schonberger and Frahm 2016) and ORB-SLAM2 (Mur-Artal, Montiel, and Tardos 2015) on TUM RGBD dataset (Sturm et al. 2012). In general, ORB-SLAM2 shows the most accurate pose results with sparse hand-crafted features. Nonetheless, the sparse 3D reconstruction cannot provide complete dense depth for various video processing applications. Moreover, COLMAP and ORB-SLAM2 suffer from the robustness issues of the fragile hand-crafted features. They failed on reconstruction/tracking on one and two sequences as shown in Table 6, respectively. On the other hand, the test-time training-based approach can overcome the robustness issue and provide dense depths with dense flow. Note that our GCVD shows promising pose results close to COLMAP's on average (excluding the failed sequences). Our GCVD thus provides a fundamental video SfM tool on dense depth reconstruction for video processing applications.\n\n\nEvaluation on EuRoC (Burri et al. 2016)\n\nThe challenging EuRoC dataset (Burri et al. 2016) consists of 11 gray-scale sequences from a stereo camera mounted on a micro aerial vehicle in relatively large indoor environments. The groundtruth camera poses are captured by a laser tracker and motion capture system. We present the absolute trajectory error (ATE) of each sequence in Table 7 and the qualitative comparison of depth estimation in Figure 8. Our method shows the lowest pose error in average. Besides, our GCVD and CVD2 maintain the depth prior of MiDaS while Deep3D produces sub-optimal depth or collapse.  (Burri et al. 2016). CVD2 (Kopf, Rong, and Huang 2021) and our method show plausible depths while Deep3D (Lee et al. 2021) tends to collapse with training from ImageNet pretrained weights.\n\n\nRuntime Analysis\n\nWe analyze the runtime of each stage in our algorithm in detail. The pipeline is divided into three main steps, pre-processing, main procedure, and post-processing for fair comparison with Deep3D (Lee et al. 2021) and CVD2 (Kopf, Rong, and Huang 2021). The averaged perframe runtimes of videos of varying length are presented in Table 8. Note that Deep3D does not perform post-processing for video depth and pose estimation.\n\nIn the pre-processing step, our method takes fewer time since we only requires adjacent optical flow. In contrast, Deep3D and CVD2 requires multiple pairs of optical flow for a target frame (e.g., F t\u00b1\u03b3\u2192t , \u03b3 \u2208 {1, 2, 4, 8}) an thus consumes more time.\n\nIn the main procedure for joint depth and pose optimization, although Deep3D takes fewer runtime by reducing the iterations for the optimization of non-first fragments, it may lead to sub-optimal results and yield global inconsistency. On the other hand, CVD2 consumes extremely long time due to the traditional optimization with CPU. The per-frame runtime of our main procedure slightly increase with the longer length of videos mainly due to the geometric verification in keyframe association and increasing non-sequential edges for pose graph optimization. Nevertheless, we emphasize the global consistency, especially in long videos. In the last post-processing step, we re-implement the postprocessing proposed by CVD2 with GPU to shorten the computational time.\n\nIn sum, the proposed method shows strong efficiency by using keyframe-based pose graph and optimization. Our Table 6: Comparison of Absolute Trajectory Error (ATE) with traditional COLMAP (Schonberger and Frahm 2016) and ORB-SLAM2 (Mur-Artal, Montiel, and Tardos 2015) on TUM RGBD dataset (Sturm et al. 2012  globally-consistent method is slightly slower than Deep3D for the videos which is greater than 500 frames, yet is able to perform the fastest for the video less than 500 frames.\n\n\nLimitations\n\nOur method can achieve globally consistent depth and pose estimation with efficient test-time training. Nevertheless, we discuss the following cases that may introduce poor performance.\n\nRestriction by optical flow estimation. Although the dense optical flow estimation can improve the robustness of traditional sparse features, the performance of depth and pose substantially relies on the accurate optical flow. Yet, the state-of-the-art optical flow estimation could still suffer from the generalization issues and thus produce un-satisfied flow estimation in some cases. Furthermore, the forwardbackward consistency check is helpful but still cannot fully guarantee the accuracy of optical flow. Hence, how to further improve the dense optical flow estimation remains a promising future direction. Learnable camera intrinsic parameters. In this work, we assume an ideal camera intrinsic with fixed focal length to simplify the learning of scale consistency in depth and camera pose. It is still challenging on handing the videos with varying focal lengths for global consistency. Thus, we put the reconstruction of varying focal length in our future directions.  \n\nFigure 1 :\n1Comparison of estimation errors and runtime.\n\n\npose component adopts the PoseNet (Godard et al. 2019) based on ResNet encoder (He et al. 2016). Similar\n\nFigure 4 :\n4Depth gradient loss L grad retains the depth prior of initial single depth\n\nFigure 5 :\n5Visual comparisons with state-of-the-art on 7-Scenes\n\nFigure 7 :\n7Visualization of global consistency. Our method reduces 18% pose error by using pose graph optimization (PGO) on the sequence fr3 nstr tex near loop in TUM RGB-D dataset(Sturm et al. 2012).\n\nFigure 8 :\n8Depth visualization on EuRoC dataset\n\n\nresults in severe misalignment between the distant views.Sequence \nPose Error (ATE in meters) \u2193 \nDepth Error (Abs Rel) \u2193 \nCOLMAP Deep3D \nCVD2 \nGCVD \nDeep3D \nCVD2 \nGCVD \nfr1 desk \n0.019 \n0.580 \n0.273 \n0.229 \n0.1940 \n0.1090 \n0.0940 \nfr1 desk2 \n0.027 \n0.611 \n0.314 \n0.156 \n0.2282 \n0.1139 \n0.1305 \nfr2 desk \n0.818 \n0.260 \n0.613 \n0.422 \n0.0973 \n0.1544 \n0.1130 \nfr3 cabinet \nfailed \n1.225 \n0.444 \n0.850 \n0.4613 \n0.1236 \n0.1832 \nfr3 nstr tex near loop \n0.015 \n0.694 \n0.491 \n0.399 \n0.2437 \n0.0615 \n0.0352 \nfr3 sitting static \n0.033 \n0.006 \n0.029 \n0.006 \n0.2368 \n0.1260 \n0.1243 \nfr3 str tex far \n0.008 \n0.089 \n0.169 \n0.131 \n0.0511 \n0.0742 \n0.0810 \nMean \n0.153 \n0.495 \n0.333 \n0.313 \n0.2160 \n0.1089 \n0.1087 \n\n\n\nTable 5 :\n5Comparison with learning-based SLAMs on 7-Scenes\n\n\n).Pose Error (RPE), which is used for measuring the drift. Both DeepV2D and DROID-SLAM are supervised methods trained on other datasets with groudtruth depth or pose. They suffer from generalization ability due to domain discrepancy and thus result in weak results on 7-Scenes dataset. In contrast, our test-time training approach directly learns on the input test video to address the generalization issue and achieves the best scores on 7-Scenes.Dataset \nMethod \n\nDepth Metrics \nPose Metrics \n\nAbsRel\u2193 \nSqRel\u2193 \nRMSE\u2193 \n\u03b4 < \nATE \nRPE Trans \nRPE Rot \n\n1.25 \u2191 \n(m)\u2193 \n(m) \u2193 \n(deg) \u2193 \n\n7-Scenes \n\nDeepV2D (Teed and Deng 2019) \n0.162 \n0.092 \n0.380 \n0.767 \n0.471 \n1.018 \n60.979 \nDROID-SLAM (Teed and Deng 2021) \n0.209 \n0.132 \n0.462 \n0.665 \n0.463 \n0.928 \n40.143 \nOurs \n0.124 \n0.054 \n0.307 \n0.858 \n0.249 \n0.257 \n8.155 \n\nTUM \nRGBD \n\nDeepV2D (Teed and Deng 2019) \n0.166 \n0.153 \n0.648 \n0.745 \n0.460 \n1.360 \n60.479 \nDROID-SLAM (Teed and Deng 2021) \n0.214 \n0.211 \n0.778 \n0.639 \n0.013 \n1.327 \n50.794 \nOurs \n0.109 \n0.077 \n0.461 \n0.858 \n0.313 \n0.277 \n15.919 \n\nComparison with ORB-SLAM2 (Mur-Artal, \nMontiel, and Tardos 2015) \n\n\n\n\n). We only show the pose results since both COLMAP and ORB-SLAM2 perform 3D reconstruction with sparse point clouds instead of dense depths.COLMAP \nORB-\nDeep3D \nCVD2 \nOur \nSequence \nSLAM2 \nGCVD \nfr1 desk \n0.019 \n0.013 \n0.580 \n0.273 \n0.229 \nfr1 desk2 \n0.027 \nfailed \n0.611 \n0.314 \n0.156 \nfr2 desk \n0.818 \n0.009 \n0.260 \n0.613 \n0.422 \nfr3 cabinet \nfailed \nfailed \n1.225 \n0.444 \n0.850 \nfr3 nstr tex near loop \n0.015 \n0.010 \n0.694 \n0.491 \n0.399 \nfr3 sitting static \n0.033 \n0.023 \n0.006 \n0.029 \n0.006 \nfr3 str tex far \n0.008 \n0.009 \n0.089 \n0.169 \n0.131 \nMean \n(exclude fr1 desk2,fr3 cabinet) \n0.179 \n0.013 \n0.326 \n0.315 \n0.237 \n\n\n\nTable 7 :\n7Pose evaluation on EuRoC dataset(Burri et al. 2016). The per-sequence absolute trajectory errors (ATE) are reported in meters.Sequence \nMH \nMH \nMH \nMH \nMH \nV1 \nV1 \nV1 \nV2 \nV2 \nV2 \nMean \n01 \n02 \n03 \n04 \n05 \n01 \n02 \n03 \n01 \n02 \n03 \nDeep3D \n3.24 \n3.82 \n2.82 \n4.26 \n5.06 \n1.53 \n1.57 \n1.32 \n1.97 \n1.87 \n1.73 \n2.65 \nCVD2 \n1.48 \n1.32 2.63 \n4.20 \n4.31 0.94 \n1.63 \n1.19 1.43 \n1.56 1.74 \n2.04 \nOur GCVD \n1.33 1.72 \n1.99 \n3.78 4.59 \n1.11 \n1.07 1.33 \n0.96 1.88 \n1.46 \n1.93 \n\n\n\n\nKF pre-process. decision optim. pose graph optim. KF pre-process. decision optim. pose graph optim. KF pre-process. decision optim. pose graph optim. KF pre-process. decision optim. pose graph optim. KF pre-process. decision optim. pose graph optim.per-frame runtime on a 50-frame video \nPre-\nMain \nPost-\nSum \nprocess. \nprocedure \nprocess. \nDeep3D \n1.85 \n2.27 \n-\n4.13 \nCVD2 \n1.73 \n7.08 \n5.09 \n13.90 \n\nOurs \n\n0.97 \n1.40 \n\n0.04 \n2.40 \nper-frame \nKF \nKF \nKF association+ \nnon-optim. \n0.78 \n0.19 \n0.33 \n0.01 \n1.05 \n\nper-frame runtime on a 200-frame video \nPre-\nMain \nPost-\nSum \nprocess. \nprocedure \nprocess. \nDeep3D \n1.88 \n1.14 \n-\n4.13 \nCVD2 \n1.61 \n8.15 \n5.07 \n14.83 \n\nOurs \n\n0.97 \n1.53 \n\n0.03 \n2.53 \nper-frame \nKF \nKF \nKF association+ \nnon-optim. \n0.67 \n0.30 \n0.29 \n0.11 \n1.12 \n\nper-frame runtime on a 500-frame video \nPre-\nMain \nPost-\nSum \nprocess. \nprocedure \nprocess. \nDeep3D \n1.90 \n0.85 \n-\n2.75 \nCVD2 \n1.60 \n10.69 \n5.05 \n17.34 \n\nOurs \n\n1.00 \n1.71 \n\n0.03 \n2.74 \nper-frame \nKF \nKF \nKF association+ \nnon-optim. \n0.64 \n0.36 \n0.35 \n0.11 \n1.25 \n\nper-frame runtime on a 1000-frame video \nPre-\nMain \nPost-\nSum \nprocess. \nprocedure \nprocess. \nDeep3D \n1.91 \n0.75 \n-\n2.66 \nCVD2 \n1.59 \n15.07 \n5.07 \n21.73 \n\nOurs \n\n1.03 \n1.77 \n\n0.03 \n2.83 \nper-frame \nKF \nKF \nKF association+ \nnon-optim. \n0.64 \n0.39 \n0.37 \n0.13 \n1.28 \n\nper-frame runtime on a 2000-frame video \nPre-\nMain \nPost-\nSum \nprocess. \nprocedure \nprocess. \nDeep3D \n1.90 \n0.70 \n-\n2.61 \nCVD2 \n1.60 \n19.44 \n5.09 \n26.13 \n\nOurs \n\n1.04 \n1.96 \n\n0.03 \n3.04 \nper-frame \nKF \nKF \nKF association+ \nnon-optim. \n0.63 \n0.41 \n0.39 \n0.25 \n1.31 \n\n\n\nTable 8 :\n8Runtime comparisons broken down by three main steps. The per-frame runtime are reported in seconds. (KF = keyframe)\nAppendixWe present GCVD, a test-time training method for videobased 3D estimation based on offline videos. There are still few test-time training studies for video-based SfM. Existing approaches are, however, robust to only local consistency for video depth estimation. Our approach is the first global-consistency solution to this direction. It needs only light-weight pose-only bundle adjustment as initial, and then takes advantage of neural-networks learning for global optimization of poses and depths simultaneously. Our GCVD can serve as a generally useful tool for offline video-based SfM (like the renowned tool COLMAP using traditional approach), where it can provide dense 3D estimations instead of fragile or sparse 3D outputs for challenging conditions such as homogeneous areas and motion blurs. Compared to the representative test-time-training approaches (such as CVD2(Kopf, Rong, and Huang 2021)), our approach can handle long videos at reasonable runtime. Unlike the approach of(Kopf, Rong, and Huang 2021)that only validates the performance using 50-frames video clips, we validate the performance for 500\u223c3682 frames, which considerably boosts the validation to practically useful situations.In this appendix, we present additional details to complement our main paper, including implementation details, comparisons with the state-of-the-art SLAM approaches, evaluation on EuRoC dataset(Burri et al. 2016), runtime analysis, and limitations of our GCVD.\n. S Agarwal, K Mierle, Others, Ceres Solver. Agarwal, S.; Mierle, K.; and Others. 2022. Ceres Solver. http://ceres-solver.org.\n\nDepth-aware video frame interpolation. W Bao, W.-S Lai, C Ma, X Zhang, Z Gao, M.-H Yang, CVPR. Bao, W.; Lai, W.-S.; Ma, C.; Zhang, X.; Gao, Z.; and Yang, M.-H. 2019. Depth-aware video frame interpolation. In CVPR.\n\nEfficient Initial Pose-graph Generation for Global SfM. D Barath, D Mishkin, I Eichhardt, I Shipachev, J Matas, CVPR. Barath, D.; Mishkin, D.; Eichhardt, I.; Shipachev, I.; and Matas, J. 2021. Efficient Initial Pose-graph Generation for Global SfM. In CVPR.\n\nSurf: Speeded up robust features. H Bay, T Tuytelaars, L Van Gool, ECCV. Bay, H.; Tuytelaars, T.; and Van Gool, L. 2006. Surf: Speeded up robust features. In ECCV.\n\nUnsupervised scale-consistent depth and ego-motion learning from monocular video. J Bian, Z Li, N Wang, H Zhan, C Shen, M.-M Cheng, I Reid, J Czarnowski, R Clark, S Leutenegger, A J Davison, CVPR. CodeSLAM-learning a compact, optimisable representation for dense visual SLAMBian, J.; Li, Z.; Wang, N.; Zhan, H.; Shen, C.; Cheng, M.- M.; and Reid, I. 2019. Unsupervised scale-consistent depth and ego-motion learning from monocular video. NeurIPS. Bloesch, M.; Czarnowski, J.; Clark, R.; Leutenegger, S.; and Davison, A. J. 2018. CodeSLAM-learning a compact, op- timisable representation for dense visual SLAM. In CVPR.\n\nThe Eu-RoC micro aerial vehicle datasets. M Burri, J Nikolic, P Gohl, T Schneider, J Rehder, S Omari, M W Achtelik, R Siegwart, The International Journal of Robotics Research. IJRRBurri, M.; Nikolic, J.; Gohl, P.; Schneider, T.; Rehder, J.; Omari, S.; Achtelik, M. W.; and Siegwart, R. 2016. The Eu- RoC micro aerial vehicle datasets. The International Journal of Robotics Research (IJRR).\n\nDepth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos. V Casser, S Pirk, R Mahjourian, A Angelova, AAAI. Casser, V.; Pirk, S.; Mahjourian, R.; and Angelova, A. 2019. Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos. In AAAI.\n\nSingle-image depth perception in the wild. W Chen, Z Fu, D Yang, J Deng, NeurIPSChen, W.; Fu, Z.; Yang, D.; and Deng, J. 2016. Single-image depth perception in the wild. NeurIPS.\n\nOasis: A large-scale dataset for single image 3d in the wild. W Chen, S Qian, D Fan, N Kojima, M Hamilton, J Deng, CVPR. Chen, W.; Qian, S.; Fan, D.; Kojima, N.; Hamilton, M.; and Deng, J. 2020. Oasis: A large-scale dataset for single image 3d in the wild. In CVPR.\n\nSelfsupervised learning with geometric constraints in monocular video: Connecting flow, depth, and camera. Y Chen, C Schmid, C Sminchisescu, ICCV. Chen, Y.; Schmid, C.; and Sminchisescu, C. 2019. Self- supervised learning with geometric constraints in monocular video: Connecting flow, depth, and camera. In ICCV.\n\nExtreme view synthesis. I Choi, O Gallo, A Troccoli, M H Kim, J Kautz, CVPR. Choi, I.; Gallo, O.; Troccoli, A.; Kim, M. H.; and Kautz, J. 2019. Extreme view synthesis. In CVPR.\n\nDeepfactors: Real-time probabilistic dense monocular slam. J Czarnowski, T Laidlow, R Clark, A J Davison, IEEE Robotics and Automation Letters. Czarnowski, J.; Laidlow, T.; Clark, R.; and Davison, A. J. 2020. Deepfactors: Real-time probabilistic dense monocular slam. IEEE Robotics and Automation Letters.\n\nDepth-Lab: Real-Time 3D Interaction With Depth Maps for Mobile Augmented Reality. R Du, E Turner, M Dzitsiuk, L Prasso, I Duarte, J Dourgarian, J Afonso, J Pascoal, J Gladstone, N Cruces, S Izadi, A Kowdle, K Tsotsos, D Kim, Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology, UIST. the 33rd Annual ACM Symposium on User Interface Software and Technology, UISTACMDu, R.; Turner, E.; Dzitsiuk, M.; Prasso, L.; Duarte, I.; Dour- garian, J.; Afonso, J.; Pascoal, J.; Gladstone, J.; Cruces, N.; Izadi, S.; Kowdle, A.; Tsotsos, K.; and Kim, D. 2020. Depth- Lab: Real-Time 3D Interaction With Depth Maps for Mo- bile Augmented Reality. In Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technol- ogy, UIST. ACM.\n\nPredicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. D Eigen, Fergus , R , ICCV. Eigen, D.; and Fergus, R. 2015. Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. In ICCV.\n\nDepth Map Prediction from a Single Image using a Multi-Scale Deep Network. D Eigen, C Puhrsch, Fergus , R , NeurIPS. Curran Associates, IncEigen, D.; Puhrsch, C.; and Fergus, R. 2014. Depth Map Prediction from a Single Image using a Multi-Scale Deep Network. In NeurIPS. Curran Associates, Inc.\n\nCnn-slam: Real-time dense monocular slam with learned depth prediction. J Engel, T Sch\u00f6ps, D Cremers, CVPR. ECCV. et al., K. T. 2017LSD-SLAM: Large-scale direct monocular SLAMEngel, J.; Sch\u00f6ps, T.; and Cremers, D. 2014. LSD-SLAM: Large-scale direct monocular SLAM. In ECCV. et al., K. T. 2017. Cnn-slam: Real-time dense monocular slam with learned depth prediction. In CVPR.\n\nRandom sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. M A Fischler, R C Bolles, Communications of the ACMFischler, M. A.; and Bolles, R. C. 1981. Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography. Communica- tions of the ACM.\n\nMulti-View Stereo: A. Y Furukawa, C Hern\u00e1ndez, Tutorial. Found. Trends. Comput. Graph. Vis. Furukawa, Y.; and Hern\u00e1ndez, C. 2015. Multi-View Stereo: A Tutorial. Found. Trends. Comput. Graph. Vis.\n\nUnsupervised cnn for single view depth estimation: Geometry to the rescue. R Garg, V K Bg, G Carneiro, I Reid, ECCV. Garg, R.; Bg, V. K.; Carneiro, G.; and Reid, I. 2016. Unsu- pervised cnn for single view depth estimation: Geometry to the rescue. In ECCV.\n\nVision meets Robotics: The KITTI Dataset. A Geiger, P Lenz, C Stiller, R Urtasun, International Journal of Robotics Research. IJRRGeiger, A.; Lenz, P.; Stiller, C.; and Urtasun, R. 2013. Vision meets Robotics: The KITTI Dataset. International Journal of Robotics Research (IJRR).\n\nUnsupervised monocular depth estimation with left-right consistency. C Godard, O Mac Aodha, G J Brostow, CVPR. Godard, C.; Mac Aodha, O.; and Brostow, G. J. 2017. Un- supervised monocular depth estimation with left-right con- sistency. In CVPR.\n\nDigging into self-supervised monocular depth estimation. C Godard, O Mac Aodha, M Firman, G J Brostow, ICCV. Godard, C.; Mac Aodha, O.; Firman, M.; and Brostow, G. J. 2019. Digging into self-supervised monocular depth estima- tion. In ICCV.\n\nPLADE-Net: Towards Pixel-Level Accuracy for Self-Supervised Single-View Depth Estimation With Neural Positional Encoding and Distilled Matting Loss. J L Gonzalez, M Kim, CVPR. Gonzalez, J. L.; and Kim, M. 2021. PLADE-Net: To- wards Pixel-Level Accuracy for Self-Supervised Single- View Depth Estimation With Neural Positional Encoding and Distilled Matting Loss. In CVPR.\n\nDepth from videos in the wild: Unsupervised monocular depth learning from unknown cameras. A Gordon, H Li, R Jonschkowski, A Angelova, ICCV. Gordon, A.; Li, H.; Jonschkowski, R.; and Angelova, A. 2019. Depth from videos in the wild: Unsupervised monoc- ular depth learning from unknown cameras. In ICCV.\n\nMultiple view geometry in computer vision. R Hartley, A Zisserman, Cambridge university pressHartley, R.; and Zisserman, A. 2003. Multiple view geometry in computer vision. Cambridge university press.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR. He, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual learning for image recognition. In CVPR.\n\nFast Depth Densification for Occlusion-aware Augmented Reality. A Holynski, J Kopf, 37Holynski, A.; and Kopf, J. 2018. Fast Depth Densification for Occlusion-aware Augmented Reality. 37(6).\n\nDeepmvs: Learning multi-view stereopsis. P.-H Huang, K Matzen, J Kopf, N Ahuja, J.-B Huang, CVPR. Huang, P.-H.; Matzen, K.; Kopf, J.; Ahuja, N.; and Huang, J.-B. 2018. Deepmvs: Learning multi-view stereopsis. In CVPR.\n\nS Im, H.-G Jeon, S Lin, I S Kweon, arXiv:1905.00538Dpsnet: End-to-end deep plane sweep stereo. arXiv preprintIm, S.; Jeon, H.-G.; Lin, S.; and Kweon, I. S. 2019. Dp- snet: End-to-end deep plane sweep stereo. arXiv preprint arXiv:1905.00538.\n\nEffiScene: Efficient Per-Pixel Rigidity Inference for Unsupervised Joint Learning of Optical Flow, Depth, Camera Pose and Motion Segmentation. Y Jiao, T D Tran, G Shi, CVPR. Jiao, Y.; Tran, T. D.; and Shi, G. 2021. EffiScene: Efficient Per-Pixel Rigidity Inference for Unsupervised Joint Learn- ing of Optical Flow, Depth, Camera Pose and Motion Seg- mentation. In CVPR.\n\nFine-Grained Semantics-Aware Representation Enhancement for Self-Supervised Monocular Depth Estimation. H Jung, E Park, S Yoo, ICCV. Jung, H.; Park, E.; and Yoo, S. 2021. Fine-Grained Semantics-Aware Representation Enhancement for Self- Supervised Monocular Depth Estimation. In ICCV.\n\nParallel tracking and mapping for small AR workspaces. G Klein, D Murray, 6th IEEE and ACM International Symposium on Mixed and Augmented Reality. Klein, G.; and Murray, D. 2007. Parallel tracking and map- ping for small AR workspaces. In 2007 6th IEEE and ACM International Symposium on Mixed and Augmented Reality.\n\nRobust consistent video depth estimation. J Kopf, X Rong, J.-B Huang, CVPR. Kopf, J.; Rong, X.; and Huang, J.-B. 2021. Robust consistent video depth estimation. In CVPR.\n\ng2o: A general framework for graph optimization. R K\u00fcmmerle, G Grisetti, H Strasdat, K Konolige, W Burgard, ICRA. K\u00fcmmerle, R.; Grisetti, G.; Strasdat, H.; Konolige, K.; and Burgard, W. 2011. g2o: A general framework for graph op- timization. In ICRA.\n\n3D Video Stabilization With Depth Estimation by CNN-Based Optimization. Y.-C Lee, K.-W Tseng, Y.-T Chen, C.-C Chen, C.-S Chen, Y.-P Hung, CVPR. Lee, Y.-C.; Tseng, K.-W.; Chen, Y.-T.; Chen, C.-C.; Chen, C.-S.; and Hung, Y.-P. 2021. 3D Video Stabilization With Depth Estimation by CNN-Based Optimization. In CVPR.\n\nSequential adversarial learning for self-supervised deep visual odometry. S Li, F Xue, X Wang, Z Yan, H Zha, ICCV. Li, S.; Xue, F.; Wang, X.; Yan, Z.; and Zha, H. 2019a. Se- quential adversarial learning for self-supervised deep visual odometry. In ICCV.\n\nLearning the depths of moving people by watching frozen people. Z Li, T Dekel, F Cole, R Tucker, N Snavely, C Liu, W T Freeman, CVPR. Li, Z.; Dekel, T.; Cole, F.; Tucker, R.; Snavely, N.; Liu, C.; and Freeman, W. T. 2019b. Learning the depths of moving people by watching frozen people. In CVPR.\n\nMegadepth: Learning singleview depth prediction from internet photos. Z Li, N Snavely, CVPR. Li, Z.; and Snavely, N. 2018. Megadepth: Learning single- view depth prediction from internet photos. In CVPR.\n\nInfinite nature: Perpetual view generation of natural scenes from a single image. A Liu, R Tucker, V Jampani, A Makadia, N Snavely, A Kanazawa, ICCV. Liu, A.; Tucker, R.; Jampani, V.; Makadia, A.; Snavely, N.; and Kanazawa, A. 2021. Infinite nature: Perpetual view gen- eration of natural scenes from a single image. In ICCV.\n\nContent-preserving warps for 3D video stabilization. F Liu, M Gleicher, H Jin, A Agarwala, ACM TOGLiu, F.; Gleicher, M.; Jin, H.; and Agarwala, A. 2009. Content-preserving warps for 3D video stabilization. ACM TOG.\n\nLearning depth from single monocular images using deep convolutional neural fields. F Liu, C Shen, G Lin, I Reid, IEEE TPAMI. Liu, F.; Shen, C.; Lin, G.; and Reid, I. 2015. Learning depth from single monocular images using deep convolu- tional neural fields. IEEE TPAMI.\n\n. X Long, L Liu, W Li, C Theobalt, W Wang, Long, X.; Liu, L.; Li, W.; Theobalt, C.; and Wang, W.\n\nMulti-view Depth Estimation using Epipolar Spatio-Temporal Networks. CVPR. Multi-view Depth Estimation using Epipolar Spatio- Temporal Networks. In CVPR.\n\nOcclusion-aware depth estimation with adaptive normal constraints. X Long, L Liu, C Theobalt, W Wang, ECCV. Long, X.; Liu, L.; Theobalt, C.; and Wang, W. 2020. Occlusion-aware depth estimation with adaptive normal constraints. In ECCV.\n\nDistinctive image features from scaleinvariant keypoints. D G Lowe, IJCVLowe, D. G. 2004. Distinctive image features from scale- invariant keypoints. IJCV.\n\nConsistent video depth estimation. X Luo, J.-B Huang, R Szeliski, K Matzen, J Kopf, ACM TOGLuo, X.; Huang, J.-B.; Szeliski, R.; Matzen, K.; and Kopf, J. 2020. Consistent video depth estimation. ACM TOG.\n\nA large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. N Mayer, E Ilg, P Hausser, P Fischer, D Cremers, A Dosovitskiy, T Brox, CVPR. Mayer, N.; Ilg, E.; Hausser, P.; Fischer, P.; Cremers, D.; Dosovitskiy, A.; and Brox, T. 2016. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In CVPR.\n\nBoosting Monocular Depth Estimation Models to High-Resolution via Content-Adaptive Multi-Resolution Merging. S M H Miangoleh, S Dille, L Mai, S Paris, Y Aksoy, CVPR. Miangoleh, S. M. H.; Dille, S.; Mai, L.; Paris, S.; and Ak- soy, Y. 2021. Boosting Monocular Depth Estimation Models to High-Resolution via Content-Adaptive Multi-Resolution Merging. In CVPR.\n\nOpenmvg: Open multiple view geometry. P Moulon, P Monasse, R Perrot, R Marlet, International Workshop on Reproducible Research in Pattern Recognition. Moulon, P.; Monasse, P.; Perrot, R.; and Marlet, R. 2016. Openmvg: Open multiple view geometry. In International Workshop on Reproducible Research in Pattern Recognition.\n\nORB-SLAM: a versatile and accurate monocular SLAM system. R Mur-Artal, J M M Montiel, J D Tardos, IEEE Transactions on Robotics. Mur-Artal, R.; Montiel, J. M. M.; and Tardos, J. D. 2015. ORB-SLAM: a versatile and accurate monocular SLAM system. IEEE Transactions on Robotics.\n\nIndoor Segmentation and Support Inference from RGBD Images. P K Nathan Silberman, Derek Hoiem, Fergus , R , ECCV. Nathan Silberman, P. K., Derek Hoiem; and Fergus, R. 2012. Indoor Segmentation and Support Inference from RGBD Images. In ECCV.\n\nTowards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer. R Ranftl, K Lasinger, D Hafner, K Schindler, V Koltun, IEEE TPAMI. Ranftl, R.; Lasinger, K.; Hafner, D.; Schindler, K.; and Koltun, V. 2020. Towards Robust Monocular Depth Estima- tion: Mixing Datasets for Zero-shot Cross-dataset Transfer. IEEE TPAMI.\n\nCompetitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation. A Ranjan, V Jampani, L Balles, K Kim, D Sun, J Wulff, M J Black, CVPR. Ranjan, A.; Jampani, V.; Balles, L.; Kim, K.; Sun, D.; Wulff, J.; and Black, M. J. 2019. Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation. In CVPR.\n\nORB: An efficient alternative to SIFT or SURF. E Rublee, V Rabaud, K Konolige, G Bradski, ICCV. Rublee, E.; Rabaud, V.; Konolige, K.; and Bradski, G. 2011. ORB: An efficient alternative to SIFT or SURF. In ICCV.\n\nMake3d: Learning 3d scene structure from a single still image. A Saxena, M Sun, A Y Ng, IEEE TPAMI. Saxena, A.; Sun, M.; and Ng, A. Y. 2008. Make3d: Learning 3d scene structure from a single still image. IEEE TPAMI.\n\nStructure-frommotion revisited. J L Schonberger, J.-M Frahm, CVPR. Schonberger, J. L.; and Frahm, J.-M. 2016. Structure-from- motion revisited. In CVPR.\n\nScene coordinate regression forests for camera relocalization in RGB-D images. J Shotton, B Glocker, C Zach, S Izadi, A Criminisi, A Fitzgibbon, CVPR. Shotton, J.; Glocker, B.; Zach, C.; Izadi, S.; Criminisi, A.; and Fitzgibbon, A. 2013. Scene coordinate regression forests for camera relocalization in RGB-D images. In CVPR.\n\nA Benchmark for the Evaluation of RGB-D SLAM Systems. J Sturm, N Engelhard, F Endres, W Burgard, D Cremers, IEEE/RSJ Int. Conf. Intell. Robot. Syst. (IROS). Sturm, J.; Engelhard, N.; Endres, F.; Burgard, W.; and Cre- mers, D. 2012. A Benchmark for the Evaluation of RGB-D SLAM Systems. In IEEE/RSJ Int. Conf. Intell. Robot. Syst. (IROS).\n\nNeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video. J Sun, Y Xie, L Chen, X Zhou, H Bao, CVPR. Sun, J.; Xie, Y.; Chen, L.; Zhou, X.; and Bao, H. 2021. NeuralRecon: Real-Time Coherent 3D Reconstruction from Monocular Video. In CVPR.\n\nDeepV2D: Video to Depth with Differentiable Structure from Motion. Z Teed, J Deng, International Conference on Learning Representations. Teed, Z.; and Deng, J. 2019. DeepV2D: Video to Depth with Differentiable Structure from Motion. In International Conference on Learning Representations.\n\nRaft: Recurrent all-pairs field transforms for optical flow. Z Teed, J Deng, ECCV. Teed, Z.; and Deng, J. 2020. Raft: Recurrent all-pairs field transforms for optical flow. In ECCV.\n\nDroid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. Z Teed, J Deng, NeurIPSTeed, Z.; and Deng, J. 2021. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. NeurIPS.\n\nDemon: Depth and motion network for learning monocular stereo. B Triggs, P F Mclauchlan, R I Hartley, A W Fitzgibbon, H Zhou, J Uhrig, N Mayer, E Ilg, A Dosovitskiy, T Brox, International workshop on vision algorithms. Ummenhofer, BSpringerCVPRTriggs, B.; McLauchlan, P. F.; Hartley, R. I.; and Fitzgibbon, A. W. 1999. Bundle adjustment-a modern synthesis. In International workshop on vision algorithms. Springer. Ummenhofer, B.; Zhou, H.; Uhrig, J.; Mayer, N.; Ilg, E.; Dosovitskiy, A.; and Brox, T. 2017. Demon: Depth and mo- tion network for learning monocular stereo. In CVPR.\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, \u0141 Kaiser, I Polosukhin, NeurIPS. Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, \u0141.; and Polosukhin, I. 2017. At- tention is all you need. In NeurIPS.\n\nDeep Two-View Structure-from-Motion Revisited. J Wang, Y Zhong, Y Dai, S Birchfield, K Zhang, N Smolyanskiy, H Li, CVPR. Wang, J.; Zhong, Y.; Dai, Y.; Birchfield, S.; Zhang, K.; Smolyanskiy, N.; and Li, H. 2021. Deep Two-View Structure-from-Motion Revisited. In CVPR.\n\nImage quality assessment: from error visibility to structural similarity. Z Wang, A C Bovik, H R Sheikh, E P Simoncelli, IEEE TIP. Wang, Z.; Bovik, A. C.; Sheikh, H. R.; and Simoncelli, E. P. 2004. Image quality assessment: from error visibility to structural similarity. IEEE TIP.\n\nThe Temporal Opportunist: Self-Supervised Multi-Frame Monocular Depth. J Watson, O Mac Aodha, V Prisacariu, G Brostow, M Firman, CVPR. Watson, J.; Mac Aodha, O.; Prisacariu, V.; Brostow, G.; and Firman, M. 2021. The Temporal Opportunist: Self- Supervised Multi-Frame Monocular Depth. In CVPR.\n\nDeepsfm: Structure from motion via deep bundle adjustment. X Wei, Y Zhang, Z Li, Y Fu, X Xue, ECCV. Wei, X.; Zhang, Y.; Li, Z.; Fu, Y.; and Xue, X. 2020. Deepsfm: Structure from motion via deep bundle adjust- ment. In ECCV.\n\nMonoRec: Semi-supervised dense reconstruction in dynamic environments from a single moving camera. F Wimbauer, N Yang, L Von Stumberg, N Zeller, D Cremers, CVPR. Wimbauer, F.; Yang, N.; von Stumberg, L.; Zeller, N.; and Cremers, D. 2021. MonoRec: Semi-supervised dense re- construction in dynamic environments from a single moving camera. In CVPR.\n\nVisualSFM: A visual structure from motion system. C Wu, Wu, C.; et al. 2011. VisualSFM: A visual structure from motion system.\n\n. Y Wu, A Kirillov, F Massa, W.-Y Lo, R Girshick, Wu, Y.; Kirillov, A.; Massa, F.; Lo, W.-Y.; and Girshick, R. 2019. Detectron2. https://github.com/facebookresearch/ detectron2.\n\nLessons and insights from creating a synthetic optical flow benchmark. J Wulff, D J Butler, G B Stanley, M J Black, ECCV Workshop on Unsolved Problems in Optical Flow and Stereo Estimation. Wulff, J.; Butler, D. J.; Stanley, G. B.; and Black, M. J. 2012. Lessons and insights from creating a synthetic optical flow benchmark. In ECCV Workshop on Unsolved Problems in Optical Flow and Stereo Estimation.\n\nMvsnet: Depth inference for unstructured multi-view stereo. Y Yao, Z Luo, S Li, T Fang, L Quan, ECCV. Yao, Y.; Luo, Z.; Li, S.; Fang, T.; and Quan, L. 2018. Mvs- net: Depth inference for unstructured multi-view stereo. In ECCV.\n\nLearning to recover 3d scene shape from a single image. W Yin, J Zhang, O Wang, S Niklaus, L Mai, S Chen, C Shen, CVPR. Yin, W.; Zhang, J.; Wang, O.; Niklaus, S.; Mai, L.; Chen, S.; and Shen, C. 2021. Learning to recover 3d scene shape from a single image. In CVPR.\n\nGeonet: Unsupervised learning of dense depth, optical flow and camera pose. Z Yin, J Shi, CVPR. Yin, Z.; and Shi, J. 2018. Geonet: Unsupervised learning of dense depth, optical flow and camera pose. In CVPR.\n\nNovel view synthesis of dynamic scenes with globally coherent depths from a monocular camera. J S Yoon, K Kim, O Gallo, H S Park, J Kautz, CVPR. Yoon, J. S.; Kim, K.; Gallo, O.; Park, H. S.; and Kautz, J. 2020. Novel view synthesis of dynamic scenes with globally coherent depths from a monocular camera. In CVPR.\n\nA Tutorial on Quantitative Trajectory Evaluation for Visual(-Inertial) Odometry. Z Zhang, D Scaramuzza, IEEE/RSJ Int. Conf. Intell. Robot. Syst. (IROS). Zhang, Z.; and Scaramuzza, D. 2018. A Tutorial on Quanti- tative Trajectory Evaluation for Visual(-Inertial) Odometry. In IEEE/RSJ Int. Conf. Intell. Robot. Syst. (IROS).\n\nDeeptam: Deep tracking and mapping. H Zhou, B Ummenhofer, T Brox, ECCV. Zhou, H.; Ummenhofer, B.; and Brox, T. 2018. Deeptam: Deep tracking and mapping. In ECCV.\n\nUnsupervised learning of depth and ego-motion from video. T Zhou, M Brown, N Snavely, D G Lowe, CVPR. Zhou, T.; Brown, M.; Snavely, N.; and Lowe, D. G. 2017. Unsupervised learning of depth and ego-motion from video. In CVPR.\n\nR-MSFM: Recurrent Multi-Scale Feature Modulation for Monocular Depth Estimating. Z Zhou, X Fan, P Shi, Y Xin, ICCV. Zhou, Z.; Fan, X.; Shi, P.; and Xin, Y. 2021. R-MSFM: Recurrent Multi-Scale Feature Modulation for Monocular Depth Estimating. In ICCV.\n\nLearning monocular visual odometry via selfsupervised long-term modeling. Y Zou, P Ji, Q.-H Tran, J.-B Huang, M Chandraker, ECCV. Zou, Y.; Ji, P.; Tran, Q.-H.; Huang, J.-B.; and Chandraker, M. 2020. Learning monocular visual odometry via self- supervised long-term modeling. In ECCV.\n", "annotations": {"author": "[{\"end\":131,\"start\":89},{\"end\":176,\"start\":132},{\"end\":222,\"start\":177},{\"end\":266,\"start\":223}]", "publisher": null, "author_last_name": "[{\"end\":101,\"start\":98},{\"end\":146,\"start\":141},{\"end\":192,\"start\":188},{\"end\":236,\"start\":232}]", "author_first_name": "[{\"end\":97,\"start\":89},{\"end\":140,\"start\":132},{\"end\":187,\"start\":177},{\"end\":231,\"start\":223}]", "author_affiliation": "[{\"end\":130,\"start\":103},{\"end\":175,\"start\":148},{\"end\":221,\"start\":194},{\"end\":265,\"start\":238}]", "title": "[{\"end\":86,\"start\":1},{\"end\":352,\"start\":267}]", "venue": null, "abstract": "[{\"end\":1402,\"start\":354}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b26\"},\"end\":1558,\"start\":1534},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":1573,\"start\":1558},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1618,\"start\":1601},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1653,\"start\":1635},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":1670,\"start\":1653},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":1686,\"start\":1670},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":1722,\"start\":1705},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":1738,\"start\":1722},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2005,\"start\":1972},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":2041,\"start\":2005},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":2060,\"start\":2041},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":2428,\"start\":2400},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2764,\"start\":2736},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2793,\"start\":2776},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":2869,\"start\":2848},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3276,\"start\":3243},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":3295,\"start\":3276},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3317,\"start\":3295},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3534,\"start\":3506},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":3733,\"start\":3716},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3752,\"start\":3733},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3768,\"start\":3752},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3841,\"start\":3824},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":4069,\"start\":4052},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4096,\"start\":4069},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":4112,\"start\":4096},{\"end\":4319,\"start\":4279},{\"end\":4614,\"start\":4555},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":4769,\"start\":4749},{\"end\":4947,\"start\":4904},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":6564,\"start\":6543},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":7376,\"start\":7355},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":7496,\"start\":7480},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":7515,\"start\":7496},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":7542,\"start\":7515},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7671,\"start\":7637},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7691,\"start\":7671},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7783,\"start\":7754},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7961,\"start\":7938},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7993,\"start\":7961},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":8029,\"start\":7993},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":8175,\"start\":8156},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8227,\"start\":8205},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8420,\"start\":8409},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8455,\"start\":8420},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":8473,\"start\":8455},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8911,\"start\":8878},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8927,\"start\":8911},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":8949,\"start\":8927},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9030,\"start\":9012},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9066,\"start\":9030},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9088,\"start\":9066},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":9191,\"start\":9172},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9232,\"start\":9214},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9252,\"start\":9232},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":9269,\"start\":9252},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9285,\"start\":9269},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":9313,\"start\":9293},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":9443,\"start\":9420},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":9543,\"start\":9515},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":9912,\"start\":9893},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":9928,\"start\":9912},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":9943,\"start\":9928},{\"end\":10060,\"start\":10042},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":10081,\"start\":10060},{\"end\":10508,\"start\":10484},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":10540,\"start\":10508},{\"end\":10560,\"start\":10540},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10583,\"start\":10560},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":11568,\"start\":11551},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":11587,\"start\":11568},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":11604,\"start\":11587},{\"end\":11702,\"start\":11678},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":11734,\"start\":11702},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":11750,\"start\":11734},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":11769,\"start\":11750},{\"end\":11861,\"start\":11841},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":11880,\"start\":11861},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11903,\"start\":11880},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":11920,\"start\":11903},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":12049,\"start\":12023},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":12068,\"start\":12049},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":12105,\"start\":12086},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":12138,\"start\":12105},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":12158,\"start\":12138},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12487,\"start\":12468},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":12506,\"start\":12487},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12542,\"start\":12506},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":12558,\"start\":12542},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12575,\"start\":12558},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":12594,\"start\":12575},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":12610,\"start\":12594},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12635,\"start\":12610},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":12654,\"start\":12635},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":12671,\"start\":12654},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":12696,\"start\":12671},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":12878,\"start\":12860},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":12897,\"start\":12878},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12922,\"start\":12897},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":12960,\"start\":12940},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":12979,\"start\":12960},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":13679,\"start\":13663},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":13953,\"start\":13937},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":14166,\"start\":14138},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":14560,\"start\":14541},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":15448,\"start\":15431},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":15486,\"start\":15458},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":16977,\"start\":16961},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":17137,\"start\":17118},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":17293,\"start\":17270},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":17325,\"start\":17293},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":17361,\"start\":17325},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":17398,\"start\":17370},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":17417,\"start\":17398},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":19105,\"start\":19085},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":19206,\"start\":19178},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":19286,\"start\":19267},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":19335,\"start\":19315},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":20065,\"start\":20045},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":20598,\"start\":20588},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":21925,\"start\":21908},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":22407,\"start\":22386},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":24114,\"start\":24094},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":24267,\"start\":24240},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":24374,\"start\":24346},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":24719,\"start\":24700},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":25131,\"start\":25111},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":26008,\"start\":25991},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":26648,\"start\":26631},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":26951,\"start\":26930},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":27259,\"start\":27242},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":28057,\"start\":28029},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":28819,\"start\":28797},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":28939,\"start\":28911},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":29240,\"start\":29212},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":29468,\"start\":29440},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":29497,\"start\":29480},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":29592,\"start\":29572},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":29654,\"start\":29635},{\"end\":30014,\"start\":29998},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":30364,\"start\":30345},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":30501,\"start\":30481},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":30659,\"start\":30641},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":30895,\"start\":30877},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":31109,\"start\":31076},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":31268,\"start\":31249},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":31294,\"start\":31268},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":31412,\"start\":31391},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":31513,\"start\":31496},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":31614,\"start\":31586},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":31914,\"start\":31894},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":32069,\"start\":32049},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":32298,\"start\":32281},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":32419,\"start\":32391},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":32672,\"start\":32656},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":32699,\"start\":32681},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":32735,\"start\":32718},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":32769,\"start\":32752},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":33265,\"start\":33238},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":33925,\"start\":33908},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":34050,\"start\":34022},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":34116,\"start\":34097},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":34213,\"start\":34196},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":34252,\"start\":34224},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":34333,\"start\":34305},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":34391,\"start\":34372},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":36216,\"start\":36195},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":36344,\"start\":36324},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":36489,\"start\":36461},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":37137,\"start\":37116},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":39795,\"start\":39773},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":41731,\"start\":41703},{\"end\":42934,\"start\":42918},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":42996,\"start\":42976},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":43102,\"start\":43082},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":43253,\"start\":43233},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":43287,\"start\":43268},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":43490,\"start\":43469},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":43530,\"start\":43512},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":43815,\"start\":43787},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":43907,\"start\":43888},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":44779,\"start\":44760},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":45324,\"start\":45305},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":45359,\"start\":45331},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":45427,\"start\":45410},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":45727,\"start\":45710},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":45765,\"start\":45737},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":47179,\"start\":47151},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":47270,\"start\":47252},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":49153,\"start\":49134},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":51770,\"start\":51751}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":48690,\"start\":48633},{\"attributes\":{\"id\":\"fig_1\"},\"end\":48797,\"start\":48691},{\"attributes\":{\"id\":\"fig_2\"},\"end\":48885,\"start\":48798},{\"attributes\":{\"id\":\"fig_3\"},\"end\":48951,\"start\":48886},{\"attributes\":{\"id\":\"fig_4\"},\"end\":49154,\"start\":48952},{\"attributes\":{\"id\":\"fig_5\"},\"end\":49204,\"start\":49155},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":49905,\"start\":49205},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":49966,\"start\":49906},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":51080,\"start\":49967},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":51706,\"start\":51081},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":52182,\"start\":51707},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":53758,\"start\":52183},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":53886,\"start\":53759}]", "paragraph": "[{\"end\":2226,\"start\":1418},{\"end\":2629,\"start\":2228},{\"end\":3030,\"start\":2631},{\"end\":3203,\"start\":3032},{\"end\":3966,\"start\":3205},{\"end\":5333,\"start\":3968},{\"end\":6753,\"start\":5335},{\"end\":6984,\"start\":6755},{\"end\":7208,\"start\":6986},{\"end\":7425,\"start\":7210},{\"end\":7876,\"start\":7442},{\"end\":8295,\"start\":7878},{\"end\":9785,\"start\":8297},{\"end\":11541,\"start\":9787},{\"end\":14561,\"start\":11543},{\"end\":14952,\"start\":14563},{\"end\":16178,\"start\":14968},{\"end\":16837,\"start\":16223},{\"end\":17189,\"start\":16839},{\"end\":18318,\"start\":17191},{\"end\":20382,\"start\":18376},{\"end\":20964,\"start\":20384},{\"end\":23238,\"start\":20999},{\"end\":24692,\"start\":23279},{\"end\":24806,\"start\":24694},{\"end\":25235,\"start\":24808},{\"end\":25556,\"start\":25237},{\"end\":25844,\"start\":25598},{\"end\":26017,\"start\":25846},{\"end\":26219,\"start\":26092},{\"end\":26521,\"start\":26221},{\"end\":26727,\"start\":26596},{\"end\":27260,\"start\":26788},{\"end\":27894,\"start\":27262},{\"end\":28307,\"start\":27971},{\"end\":28709,\"start\":28403},{\"end\":29347,\"start\":28735},{\"end\":30215,\"start\":29363},{\"end\":30454,\"start\":30217},{\"end\":30619,\"start\":30456},{\"end\":30859,\"start\":30621},{\"end\":31014,\"start\":30861},{\"end\":33086,\"start\":31016},{\"end\":34347,\"start\":33088},{\"end\":36126,\"start\":34349},{\"end\":37592,\"start\":36147},{\"end\":38634,\"start\":37616},{\"end\":39663,\"start\":38649},{\"end\":41939,\"start\":39689},{\"end\":42295,\"start\":41971},{\"end\":42654,\"start\":42371},{\"end\":44686,\"start\":42700},{\"end\":45493,\"start\":44730},{\"end\":45938,\"start\":45514},{\"end\":46192,\"start\":45940},{\"end\":46961,\"start\":46194},{\"end\":47449,\"start\":46963},{\"end\":47650,\"start\":47465},{\"end\":48632,\"start\":47652}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":18375,\"start\":18319},{\"attributes\":{\"id\":\"formula_1\"},\"end\":25597,\"start\":25557},{\"attributes\":{\"id\":\"formula_2\"},\"end\":26091,\"start\":26018},{\"attributes\":{\"id\":\"formula_3\"},\"end\":26595,\"start\":26522},{\"attributes\":{\"id\":\"formula_4\"},\"end\":26787,\"start\":26728},{\"attributes\":{\"id\":\"formula_5\"},\"end\":27970,\"start\":27895},{\"attributes\":{\"id\":\"formula_6\"},\"end\":28402,\"start\":28308},{\"attributes\":{\"id\":\"formula_7\"},\"end\":41970,\"start\":41940},{\"attributes\":{\"id\":\"formula_8\"},\"end\":42370,\"start\":42296}]", "table_ref": "[{\"end\":26865,\"start\":26858},{\"end\":31923,\"start\":31916},{\"end\":34058,\"start\":34051},{\"end\":34597,\"start\":34590},{\"end\":34881,\"start\":34874},{\"end\":36227,\"start\":36220},{\"end\":37085,\"start\":37078},{\"end\":37355,\"start\":37348},{\"end\":37957,\"start\":37950},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":43183,\"start\":43176},{\"end\":43735,\"start\":43728},{\"end\":44310,\"start\":44303},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":45074,\"start\":45067},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":45850,\"start\":45843},{\"end\":47079,\"start\":47072}]", "section_header": "[{\"end\":1416,\"start\":1404},{\"end\":7440,\"start\":7428},{\"end\":14966,\"start\":14955},{\"end\":16221,\"start\":16181},{\"end\":20997,\"start\":20967},{\"end\":23277,\"start\":23241},{\"end\":28733,\"start\":28712},{\"end\":29361,\"start\":29350},{\"end\":36145,\"start\":36129},{\"end\":37614,\"start\":37595},{\"end\":38647,\"start\":38637},{\"end\":39687,\"start\":39666},{\"end\":42698,\"start\":42657},{\"end\":44728,\"start\":44689},{\"end\":45512,\"start\":45496},{\"end\":47463,\"start\":47452},{\"end\":48644,\"start\":48634},{\"end\":48809,\"start\":48799},{\"end\":48897,\"start\":48887},{\"end\":48963,\"start\":48953},{\"end\":49166,\"start\":49156},{\"end\":49916,\"start\":49907},{\"end\":51717,\"start\":51708},{\"end\":53769,\"start\":53760}]", "table": "[{\"end\":49905,\"start\":49264},{\"end\":51080,\"start\":50417},{\"end\":51706,\"start\":51223},{\"end\":52182,\"start\":51845},{\"end\":53758,\"start\":52434}]", "figure_caption": "[{\"end\":48690,\"start\":48646},{\"end\":48797,\"start\":48693},{\"end\":48885,\"start\":48811},{\"end\":48951,\"start\":48899},{\"end\":49154,\"start\":48965},{\"end\":49204,\"start\":49168},{\"end\":49264,\"start\":49207},{\"end\":49966,\"start\":49918},{\"end\":50417,\"start\":49969},{\"end\":51223,\"start\":51083},{\"end\":51845,\"start\":51719},{\"end\":52434,\"start\":52185},{\"end\":53886,\"start\":53771}]", "figure_ref": "[{\"end\":5957,\"start\":5951},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6409,\"start\":6403},{\"end\":15989,\"start\":15981},{\"end\":18915,\"start\":18907},{\"end\":19442,\"start\":19436},{\"end\":23890,\"start\":23884},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":27681,\"start\":27673},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":33285,\"start\":33279},{\"end\":33594,\"start\":33586},{\"end\":35866,\"start\":35860},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":40969,\"start\":40961},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":45137,\"start\":45129}]", "bib_author_first_name": "[{\"end\":55364,\"start\":55363},{\"end\":55375,\"start\":55374},{\"end\":55529,\"start\":55528},{\"end\":55539,\"start\":55535},{\"end\":55546,\"start\":55545},{\"end\":55552,\"start\":55551},{\"end\":55561,\"start\":55560},{\"end\":55571,\"start\":55567},{\"end\":55761,\"start\":55760},{\"end\":55771,\"start\":55770},{\"end\":55782,\"start\":55781},{\"end\":55795,\"start\":55794},{\"end\":55808,\"start\":55807},{\"end\":55998,\"start\":55997},{\"end\":56005,\"start\":56004},{\"end\":56019,\"start\":56018},{\"end\":56211,\"start\":56210},{\"end\":56219,\"start\":56218},{\"end\":56225,\"start\":56224},{\"end\":56233,\"start\":56232},{\"end\":56241,\"start\":56240},{\"end\":56252,\"start\":56248},{\"end\":56261,\"start\":56260},{\"end\":56269,\"start\":56268},{\"end\":56283,\"start\":56282},{\"end\":56292,\"start\":56291},{\"end\":56307,\"start\":56306},{\"end\":56309,\"start\":56308},{\"end\":56791,\"start\":56790},{\"end\":56800,\"start\":56799},{\"end\":56811,\"start\":56810},{\"end\":56819,\"start\":56818},{\"end\":56832,\"start\":56831},{\"end\":56842,\"start\":56841},{\"end\":56851,\"start\":56850},{\"end\":56853,\"start\":56852},{\"end\":56865,\"start\":56864},{\"end\":57248,\"start\":57247},{\"end\":57258,\"start\":57257},{\"end\":57266,\"start\":57265},{\"end\":57280,\"start\":57279},{\"end\":57520,\"start\":57519},{\"end\":57528,\"start\":57527},{\"end\":57534,\"start\":57533},{\"end\":57542,\"start\":57541},{\"end\":57719,\"start\":57718},{\"end\":57727,\"start\":57726},{\"end\":57735,\"start\":57734},{\"end\":57742,\"start\":57741},{\"end\":57752,\"start\":57751},{\"end\":57764,\"start\":57763},{\"end\":58031,\"start\":58030},{\"end\":58039,\"start\":58038},{\"end\":58049,\"start\":58048},{\"end\":58263,\"start\":58262},{\"end\":58271,\"start\":58270},{\"end\":58280,\"start\":58279},{\"end\":58292,\"start\":58291},{\"end\":58294,\"start\":58293},{\"end\":58301,\"start\":58300},{\"end\":58476,\"start\":58475},{\"end\":58490,\"start\":58489},{\"end\":58501,\"start\":58500},{\"end\":58510,\"start\":58509},{\"end\":58512,\"start\":58511},{\"end\":58806,\"start\":58805},{\"end\":58812,\"start\":58811},{\"end\":58822,\"start\":58821},{\"end\":58834,\"start\":58833},{\"end\":58844,\"start\":58843},{\"end\":58854,\"start\":58853},{\"end\":58868,\"start\":58867},{\"end\":58878,\"start\":58877},{\"end\":58889,\"start\":58888},{\"end\":58902,\"start\":58901},{\"end\":58912,\"start\":58911},{\"end\":58921,\"start\":58920},{\"end\":58931,\"start\":58930},{\"end\":58942,\"start\":58941},{\"end\":59603,\"start\":59602},{\"end\":59617,\"start\":59611},{\"end\":59621,\"start\":59620},{\"end\":59856,\"start\":59855},{\"end\":59865,\"start\":59864},{\"end\":59881,\"start\":59875},{\"end\":59885,\"start\":59884},{\"end\":60149,\"start\":60148},{\"end\":60158,\"start\":60157},{\"end\":60168,\"start\":60167},{\"end\":60570,\"start\":60569},{\"end\":60572,\"start\":60571},{\"end\":60584,\"start\":60583},{\"end\":60586,\"start\":60585},{\"end\":60831,\"start\":60830},{\"end\":60843,\"start\":60842},{\"end\":61081,\"start\":61080},{\"end\":61089,\"start\":61088},{\"end\":61091,\"start\":61090},{\"end\":61097,\"start\":61096},{\"end\":61109,\"start\":61108},{\"end\":61306,\"start\":61305},{\"end\":61316,\"start\":61315},{\"end\":61324,\"start\":61323},{\"end\":61335,\"start\":61334},{\"end\":61614,\"start\":61613},{\"end\":61624,\"start\":61623},{\"end\":61637,\"start\":61636},{\"end\":61639,\"start\":61638},{\"end\":61848,\"start\":61847},{\"end\":61858,\"start\":61857},{\"end\":61871,\"start\":61870},{\"end\":61881,\"start\":61880},{\"end\":61883,\"start\":61882},{\"end\":62182,\"start\":62181},{\"end\":62184,\"start\":62183},{\"end\":62196,\"start\":62195},{\"end\":62497,\"start\":62496},{\"end\":62507,\"start\":62506},{\"end\":62513,\"start\":62512},{\"end\":62529,\"start\":62528},{\"end\":62754,\"start\":62753},{\"end\":62765,\"start\":62764},{\"end\":62959,\"start\":62958},{\"end\":62965,\"start\":62964},{\"end\":62974,\"start\":62973},{\"end\":62981,\"start\":62980},{\"end\":63160,\"start\":63159},{\"end\":63172,\"start\":63171},{\"end\":63331,\"start\":63327},{\"end\":63340,\"start\":63339},{\"end\":63350,\"start\":63349},{\"end\":63358,\"start\":63357},{\"end\":63370,\"start\":63366},{\"end\":63506,\"start\":63505},{\"end\":63515,\"start\":63511},{\"end\":63523,\"start\":63522},{\"end\":63530,\"start\":63529},{\"end\":63532,\"start\":63531},{\"end\":63891,\"start\":63890},{\"end\":63899,\"start\":63898},{\"end\":63901,\"start\":63900},{\"end\":63909,\"start\":63908},{\"end\":64224,\"start\":64223},{\"end\":64232,\"start\":64231},{\"end\":64240,\"start\":64239},{\"end\":64461,\"start\":64460},{\"end\":64470,\"start\":64469},{\"end\":64766,\"start\":64765},{\"end\":64774,\"start\":64773},{\"end\":64785,\"start\":64781},{\"end\":64944,\"start\":64943},{\"end\":64956,\"start\":64955},{\"end\":64968,\"start\":64967},{\"end\":64980,\"start\":64979},{\"end\":64992,\"start\":64991},{\"end\":65223,\"start\":65219},{\"end\":65233,\"start\":65229},{\"end\":65245,\"start\":65241},{\"end\":65256,\"start\":65252},{\"end\":65267,\"start\":65263},{\"end\":65278,\"start\":65274},{\"end\":65535,\"start\":65534},{\"end\":65541,\"start\":65540},{\"end\":65548,\"start\":65547},{\"end\":65556,\"start\":65555},{\"end\":65563,\"start\":65562},{\"end\":65781,\"start\":65780},{\"end\":65787,\"start\":65786},{\"end\":65796,\"start\":65795},{\"end\":65804,\"start\":65803},{\"end\":65814,\"start\":65813},{\"end\":65825,\"start\":65824},{\"end\":65832,\"start\":65831},{\"end\":65834,\"start\":65833},{\"end\":66084,\"start\":66083},{\"end\":66090,\"start\":66089},{\"end\":66301,\"start\":66300},{\"end\":66308,\"start\":66307},{\"end\":66318,\"start\":66317},{\"end\":66329,\"start\":66328},{\"end\":66340,\"start\":66339},{\"end\":66351,\"start\":66350},{\"end\":66599,\"start\":66598},{\"end\":66606,\"start\":66605},{\"end\":66618,\"start\":66617},{\"end\":66625,\"start\":66624},{\"end\":66846,\"start\":66845},{\"end\":66853,\"start\":66852},{\"end\":66861,\"start\":66860},{\"end\":66868,\"start\":66867},{\"end\":67036,\"start\":67035},{\"end\":67044,\"start\":67043},{\"end\":67051,\"start\":67050},{\"end\":67057,\"start\":67056},{\"end\":67069,\"start\":67068},{\"end\":67354,\"start\":67353},{\"end\":67362,\"start\":67361},{\"end\":67369,\"start\":67368},{\"end\":67381,\"start\":67380},{\"end\":67582,\"start\":67581},{\"end\":67584,\"start\":67583},{\"end\":67716,\"start\":67715},{\"end\":67726,\"start\":67722},{\"end\":67735,\"start\":67734},{\"end\":67747,\"start\":67746},{\"end\":67757,\"start\":67756},{\"end\":67989,\"start\":67988},{\"end\":67998,\"start\":67997},{\"end\":68005,\"start\":68004},{\"end\":68016,\"start\":68015},{\"end\":68027,\"start\":68026},{\"end\":68038,\"start\":68037},{\"end\":68053,\"start\":68052},{\"end\":68385,\"start\":68384},{\"end\":68389,\"start\":68386},{\"end\":68402,\"start\":68401},{\"end\":68411,\"start\":68410},{\"end\":68418,\"start\":68417},{\"end\":68427,\"start\":68426},{\"end\":68673,\"start\":68672},{\"end\":68683,\"start\":68682},{\"end\":68694,\"start\":68693},{\"end\":68704,\"start\":68703},{\"end\":69016,\"start\":69015},{\"end\":69029,\"start\":69028},{\"end\":69033,\"start\":69030},{\"end\":69044,\"start\":69043},{\"end\":69046,\"start\":69045},{\"end\":69295,\"start\":69294},{\"end\":69297,\"start\":69296},{\"end\":69321,\"start\":69316},{\"end\":69335,\"start\":69329},{\"end\":69339,\"start\":69338},{\"end\":69575,\"start\":69574},{\"end\":69585,\"start\":69584},{\"end\":69597,\"start\":69596},{\"end\":69607,\"start\":69606},{\"end\":69620,\"start\":69619},{\"end\":69946,\"start\":69945},{\"end\":69956,\"start\":69955},{\"end\":69967,\"start\":69966},{\"end\":69977,\"start\":69976},{\"end\":69984,\"start\":69983},{\"end\":69991,\"start\":69990},{\"end\":70000,\"start\":69999},{\"end\":70002,\"start\":70001},{\"end\":70281,\"start\":70280},{\"end\":70291,\"start\":70290},{\"end\":70301,\"start\":70300},{\"end\":70313,\"start\":70312},{\"end\":70510,\"start\":70509},{\"end\":70520,\"start\":70519},{\"end\":70527,\"start\":70526},{\"end\":70529,\"start\":70528},{\"end\":70696,\"start\":70695},{\"end\":70698,\"start\":70697},{\"end\":70716,\"start\":70712},{\"end\":70897,\"start\":70896},{\"end\":70908,\"start\":70907},{\"end\":70919,\"start\":70918},{\"end\":70927,\"start\":70926},{\"end\":70936,\"start\":70935},{\"end\":70949,\"start\":70948},{\"end\":71199,\"start\":71198},{\"end\":71208,\"start\":71207},{\"end\":71221,\"start\":71220},{\"end\":71231,\"start\":71230},{\"end\":71242,\"start\":71241},{\"end\":71556,\"start\":71555},{\"end\":71563,\"start\":71562},{\"end\":71570,\"start\":71569},{\"end\":71578,\"start\":71577},{\"end\":71586,\"start\":71585},{\"end\":71804,\"start\":71803},{\"end\":71812,\"start\":71811},{\"end\":72089,\"start\":72088},{\"end\":72097,\"start\":72096},{\"end\":72282,\"start\":72281},{\"end\":72290,\"start\":72289},{\"end\":72478,\"start\":72477},{\"end\":72488,\"start\":72487},{\"end\":72490,\"start\":72489},{\"end\":72504,\"start\":72503},{\"end\":72506,\"start\":72505},{\"end\":72517,\"start\":72516},{\"end\":72519,\"start\":72518},{\"end\":72533,\"start\":72532},{\"end\":72541,\"start\":72540},{\"end\":72550,\"start\":72549},{\"end\":72559,\"start\":72558},{\"end\":72566,\"start\":72565},{\"end\":72581,\"start\":72580},{\"end\":73025,\"start\":73024},{\"end\":73036,\"start\":73035},{\"end\":73047,\"start\":73046},{\"end\":73057,\"start\":73056},{\"end\":73070,\"start\":73069},{\"end\":73079,\"start\":73078},{\"end\":73081,\"start\":73080},{\"end\":73090,\"start\":73089},{\"end\":73100,\"start\":73099},{\"end\":73327,\"start\":73326},{\"end\":73335,\"start\":73334},{\"end\":73344,\"start\":73343},{\"end\":73351,\"start\":73350},{\"end\":73365,\"start\":73364},{\"end\":73374,\"start\":73373},{\"end\":73389,\"start\":73388},{\"end\":73623,\"start\":73622},{\"end\":73631,\"start\":73630},{\"end\":73633,\"start\":73632},{\"end\":73642,\"start\":73641},{\"end\":73644,\"start\":73643},{\"end\":73654,\"start\":73653},{\"end\":73656,\"start\":73655},{\"end\":73903,\"start\":73902},{\"end\":73913,\"start\":73912},{\"end\":73926,\"start\":73925},{\"end\":73940,\"start\":73939},{\"end\":73951,\"start\":73950},{\"end\":74185,\"start\":74184},{\"end\":74192,\"start\":74191},{\"end\":74201,\"start\":74200},{\"end\":74207,\"start\":74206},{\"end\":74213,\"start\":74212},{\"end\":74450,\"start\":74449},{\"end\":74462,\"start\":74461},{\"end\":74470,\"start\":74469},{\"end\":74486,\"start\":74485},{\"end\":74496,\"start\":74495},{\"end\":74750,\"start\":74749},{\"end\":74830,\"start\":74829},{\"end\":74836,\"start\":74835},{\"end\":74848,\"start\":74847},{\"end\":74860,\"start\":74856},{\"end\":74866,\"start\":74865},{\"end\":75078,\"start\":75077},{\"end\":75087,\"start\":75086},{\"end\":75089,\"start\":75088},{\"end\":75099,\"start\":75098},{\"end\":75101,\"start\":75100},{\"end\":75112,\"start\":75111},{\"end\":75114,\"start\":75113},{\"end\":75471,\"start\":75470},{\"end\":75478,\"start\":75477},{\"end\":75485,\"start\":75484},{\"end\":75491,\"start\":75490},{\"end\":75499,\"start\":75498},{\"end\":75696,\"start\":75695},{\"end\":75703,\"start\":75702},{\"end\":75712,\"start\":75711},{\"end\":75720,\"start\":75719},{\"end\":75731,\"start\":75730},{\"end\":75738,\"start\":75737},{\"end\":75746,\"start\":75745},{\"end\":75983,\"start\":75982},{\"end\":75990,\"start\":75989},{\"end\":76210,\"start\":76209},{\"end\":76212,\"start\":76211},{\"end\":76220,\"start\":76219},{\"end\":76227,\"start\":76226},{\"end\":76236,\"start\":76235},{\"end\":76238,\"start\":76237},{\"end\":76246,\"start\":76245},{\"end\":76512,\"start\":76511},{\"end\":76521,\"start\":76520},{\"end\":76792,\"start\":76791},{\"end\":76800,\"start\":76799},{\"end\":76814,\"start\":76813},{\"end\":76977,\"start\":76976},{\"end\":76985,\"start\":76984},{\"end\":76994,\"start\":76993},{\"end\":77005,\"start\":77004},{\"end\":77007,\"start\":77006},{\"end\":77226,\"start\":77225},{\"end\":77234,\"start\":77233},{\"end\":77241,\"start\":77240},{\"end\":77248,\"start\":77247},{\"end\":77472,\"start\":77471},{\"end\":77479,\"start\":77478},{\"end\":77488,\"start\":77484},{\"end\":77499,\"start\":77495},{\"end\":77508,\"start\":77507}]", "bib_author_last_name": "[{\"end\":55372,\"start\":55365},{\"end\":55382,\"start\":55376},{\"end\":55390,\"start\":55384},{\"end\":55533,\"start\":55530},{\"end\":55543,\"start\":55540},{\"end\":55549,\"start\":55547},{\"end\":55558,\"start\":55553},{\"end\":55565,\"start\":55562},{\"end\":55576,\"start\":55572},{\"end\":55768,\"start\":55762},{\"end\":55779,\"start\":55772},{\"end\":55792,\"start\":55783},{\"end\":55805,\"start\":55796},{\"end\":55814,\"start\":55809},{\"end\":56002,\"start\":55999},{\"end\":56016,\"start\":56006},{\"end\":56028,\"start\":56020},{\"end\":56216,\"start\":56212},{\"end\":56222,\"start\":56220},{\"end\":56230,\"start\":56226},{\"end\":56238,\"start\":56234},{\"end\":56246,\"start\":56242},{\"end\":56258,\"start\":56253},{\"end\":56266,\"start\":56262},{\"end\":56280,\"start\":56270},{\"end\":56289,\"start\":56284},{\"end\":56304,\"start\":56293},{\"end\":56317,\"start\":56310},{\"end\":56797,\"start\":56792},{\"end\":56808,\"start\":56801},{\"end\":56816,\"start\":56812},{\"end\":56829,\"start\":56820},{\"end\":56839,\"start\":56833},{\"end\":56848,\"start\":56843},{\"end\":56862,\"start\":56854},{\"end\":56874,\"start\":56866},{\"end\":57255,\"start\":57249},{\"end\":57263,\"start\":57259},{\"end\":57277,\"start\":57267},{\"end\":57289,\"start\":57281},{\"end\":57525,\"start\":57521},{\"end\":57531,\"start\":57529},{\"end\":57539,\"start\":57535},{\"end\":57547,\"start\":57543},{\"end\":57724,\"start\":57720},{\"end\":57732,\"start\":57728},{\"end\":57739,\"start\":57736},{\"end\":57749,\"start\":57743},{\"end\":57761,\"start\":57753},{\"end\":57769,\"start\":57765},{\"end\":58036,\"start\":58032},{\"end\":58046,\"start\":58040},{\"end\":58062,\"start\":58050},{\"end\":58268,\"start\":58264},{\"end\":58277,\"start\":58272},{\"end\":58289,\"start\":58281},{\"end\":58298,\"start\":58295},{\"end\":58307,\"start\":58302},{\"end\":58487,\"start\":58477},{\"end\":58498,\"start\":58491},{\"end\":58507,\"start\":58502},{\"end\":58520,\"start\":58513},{\"end\":58809,\"start\":58807},{\"end\":58819,\"start\":58813},{\"end\":58831,\"start\":58823},{\"end\":58841,\"start\":58835},{\"end\":58851,\"start\":58845},{\"end\":58865,\"start\":58855},{\"end\":58875,\"start\":58869},{\"end\":58886,\"start\":58879},{\"end\":58899,\"start\":58890},{\"end\":58909,\"start\":58903},{\"end\":58918,\"start\":58913},{\"end\":58928,\"start\":58922},{\"end\":58939,\"start\":58932},{\"end\":58946,\"start\":58943},{\"end\":59609,\"start\":59604},{\"end\":59862,\"start\":59857},{\"end\":59873,\"start\":59866},{\"end\":60155,\"start\":60150},{\"end\":60165,\"start\":60159},{\"end\":60176,\"start\":60169},{\"end\":60581,\"start\":60573},{\"end\":60593,\"start\":60587},{\"end\":60840,\"start\":60832},{\"end\":60853,\"start\":60844},{\"end\":61086,\"start\":61082},{\"end\":61094,\"start\":61092},{\"end\":61106,\"start\":61098},{\"end\":61114,\"start\":61110},{\"end\":61313,\"start\":61307},{\"end\":61321,\"start\":61317},{\"end\":61332,\"start\":61325},{\"end\":61343,\"start\":61336},{\"end\":61621,\"start\":61615},{\"end\":61634,\"start\":61625},{\"end\":61647,\"start\":61640},{\"end\":61855,\"start\":61849},{\"end\":61868,\"start\":61859},{\"end\":61878,\"start\":61872},{\"end\":61891,\"start\":61884},{\"end\":62193,\"start\":62185},{\"end\":62200,\"start\":62197},{\"end\":62504,\"start\":62498},{\"end\":62510,\"start\":62508},{\"end\":62526,\"start\":62514},{\"end\":62538,\"start\":62530},{\"end\":62762,\"start\":62755},{\"end\":62775,\"start\":62766},{\"end\":62962,\"start\":62960},{\"end\":62971,\"start\":62966},{\"end\":62978,\"start\":62975},{\"end\":62985,\"start\":62982},{\"end\":63169,\"start\":63161},{\"end\":63177,\"start\":63173},{\"end\":63337,\"start\":63332},{\"end\":63347,\"start\":63341},{\"end\":63355,\"start\":63351},{\"end\":63364,\"start\":63359},{\"end\":63376,\"start\":63371},{\"end\":63509,\"start\":63507},{\"end\":63520,\"start\":63516},{\"end\":63527,\"start\":63524},{\"end\":63538,\"start\":63533},{\"end\":63896,\"start\":63892},{\"end\":63906,\"start\":63902},{\"end\":63913,\"start\":63910},{\"end\":64229,\"start\":64225},{\"end\":64237,\"start\":64233},{\"end\":64244,\"start\":64241},{\"end\":64467,\"start\":64462},{\"end\":64477,\"start\":64471},{\"end\":64771,\"start\":64767},{\"end\":64779,\"start\":64775},{\"end\":64791,\"start\":64786},{\"end\":64953,\"start\":64945},{\"end\":64965,\"start\":64957},{\"end\":64977,\"start\":64969},{\"end\":64989,\"start\":64981},{\"end\":65000,\"start\":64993},{\"end\":65227,\"start\":65224},{\"end\":65239,\"start\":65234},{\"end\":65250,\"start\":65246},{\"end\":65261,\"start\":65257},{\"end\":65272,\"start\":65268},{\"end\":65283,\"start\":65279},{\"end\":65538,\"start\":65536},{\"end\":65545,\"start\":65542},{\"end\":65553,\"start\":65549},{\"end\":65560,\"start\":65557},{\"end\":65567,\"start\":65564},{\"end\":65784,\"start\":65782},{\"end\":65793,\"start\":65788},{\"end\":65801,\"start\":65797},{\"end\":65811,\"start\":65805},{\"end\":65822,\"start\":65815},{\"end\":65829,\"start\":65826},{\"end\":65842,\"start\":65835},{\"end\":66087,\"start\":66085},{\"end\":66098,\"start\":66091},{\"end\":66305,\"start\":66302},{\"end\":66315,\"start\":66309},{\"end\":66326,\"start\":66319},{\"end\":66337,\"start\":66330},{\"end\":66348,\"start\":66341},{\"end\":66360,\"start\":66352},{\"end\":66603,\"start\":66600},{\"end\":66615,\"start\":66607},{\"end\":66622,\"start\":66619},{\"end\":66634,\"start\":66626},{\"end\":66850,\"start\":66847},{\"end\":66858,\"start\":66854},{\"end\":66865,\"start\":66862},{\"end\":66873,\"start\":66869},{\"end\":67041,\"start\":67037},{\"end\":67048,\"start\":67045},{\"end\":67054,\"start\":67052},{\"end\":67066,\"start\":67058},{\"end\":67074,\"start\":67070},{\"end\":67359,\"start\":67355},{\"end\":67366,\"start\":67363},{\"end\":67378,\"start\":67370},{\"end\":67386,\"start\":67382},{\"end\":67589,\"start\":67585},{\"end\":67720,\"start\":67717},{\"end\":67732,\"start\":67727},{\"end\":67744,\"start\":67736},{\"end\":67754,\"start\":67748},{\"end\":67762,\"start\":67758},{\"end\":67995,\"start\":67990},{\"end\":68002,\"start\":67999},{\"end\":68013,\"start\":68006},{\"end\":68024,\"start\":68017},{\"end\":68035,\"start\":68028},{\"end\":68050,\"start\":68039},{\"end\":68058,\"start\":68054},{\"end\":68399,\"start\":68390},{\"end\":68408,\"start\":68403},{\"end\":68415,\"start\":68412},{\"end\":68424,\"start\":68419},{\"end\":68433,\"start\":68428},{\"end\":68680,\"start\":68674},{\"end\":68691,\"start\":68684},{\"end\":68701,\"start\":68695},{\"end\":68711,\"start\":68705},{\"end\":69026,\"start\":69017},{\"end\":69041,\"start\":69034},{\"end\":69053,\"start\":69047},{\"end\":69314,\"start\":69298},{\"end\":69327,\"start\":69322},{\"end\":69582,\"start\":69576},{\"end\":69594,\"start\":69586},{\"end\":69604,\"start\":69598},{\"end\":69617,\"start\":69608},{\"end\":69627,\"start\":69621},{\"end\":69953,\"start\":69947},{\"end\":69964,\"start\":69957},{\"end\":69974,\"start\":69968},{\"end\":69981,\"start\":69978},{\"end\":69988,\"start\":69985},{\"end\":69997,\"start\":69992},{\"end\":70008,\"start\":70003},{\"end\":70288,\"start\":70282},{\"end\":70298,\"start\":70292},{\"end\":70310,\"start\":70302},{\"end\":70321,\"start\":70314},{\"end\":70517,\"start\":70511},{\"end\":70524,\"start\":70521},{\"end\":70532,\"start\":70530},{\"end\":70710,\"start\":70699},{\"end\":70722,\"start\":70717},{\"end\":70905,\"start\":70898},{\"end\":70916,\"start\":70909},{\"end\":70924,\"start\":70920},{\"end\":70933,\"start\":70928},{\"end\":70946,\"start\":70937},{\"end\":70960,\"start\":70950},{\"end\":71205,\"start\":71200},{\"end\":71218,\"start\":71209},{\"end\":71228,\"start\":71222},{\"end\":71239,\"start\":71232},{\"end\":71250,\"start\":71243},{\"end\":71560,\"start\":71557},{\"end\":71567,\"start\":71564},{\"end\":71575,\"start\":71571},{\"end\":71583,\"start\":71579},{\"end\":71590,\"start\":71587},{\"end\":71809,\"start\":71805},{\"end\":71817,\"start\":71813},{\"end\":72094,\"start\":72090},{\"end\":72102,\"start\":72098},{\"end\":72287,\"start\":72283},{\"end\":72295,\"start\":72291},{\"end\":72485,\"start\":72479},{\"end\":72501,\"start\":72491},{\"end\":72514,\"start\":72507},{\"end\":72530,\"start\":72520},{\"end\":72538,\"start\":72534},{\"end\":72547,\"start\":72542},{\"end\":72556,\"start\":72551},{\"end\":72563,\"start\":72560},{\"end\":72578,\"start\":72567},{\"end\":72586,\"start\":72582},{\"end\":73033,\"start\":73026},{\"end\":73044,\"start\":73037},{\"end\":73054,\"start\":73048},{\"end\":73067,\"start\":73058},{\"end\":73076,\"start\":73071},{\"end\":73087,\"start\":73082},{\"end\":73097,\"start\":73091},{\"end\":73111,\"start\":73101},{\"end\":73332,\"start\":73328},{\"end\":73341,\"start\":73336},{\"end\":73348,\"start\":73345},{\"end\":73362,\"start\":73352},{\"end\":73371,\"start\":73366},{\"end\":73386,\"start\":73375},{\"end\":73392,\"start\":73390},{\"end\":73628,\"start\":73624},{\"end\":73639,\"start\":73634},{\"end\":73651,\"start\":73645},{\"end\":73667,\"start\":73657},{\"end\":73910,\"start\":73904},{\"end\":73923,\"start\":73914},{\"end\":73937,\"start\":73927},{\"end\":73948,\"start\":73941},{\"end\":73958,\"start\":73952},{\"end\":74189,\"start\":74186},{\"end\":74198,\"start\":74193},{\"end\":74204,\"start\":74202},{\"end\":74210,\"start\":74208},{\"end\":74217,\"start\":74214},{\"end\":74459,\"start\":74451},{\"end\":74467,\"start\":74463},{\"end\":74483,\"start\":74471},{\"end\":74493,\"start\":74487},{\"end\":74504,\"start\":74497},{\"end\":74753,\"start\":74751},{\"end\":74833,\"start\":74831},{\"end\":74845,\"start\":74837},{\"end\":74854,\"start\":74849},{\"end\":74863,\"start\":74861},{\"end\":74875,\"start\":74867},{\"end\":75084,\"start\":75079},{\"end\":75096,\"start\":75090},{\"end\":75109,\"start\":75102},{\"end\":75120,\"start\":75115},{\"end\":75475,\"start\":75472},{\"end\":75482,\"start\":75479},{\"end\":75488,\"start\":75486},{\"end\":75496,\"start\":75492},{\"end\":75504,\"start\":75500},{\"end\":75700,\"start\":75697},{\"end\":75709,\"start\":75704},{\"end\":75717,\"start\":75713},{\"end\":75728,\"start\":75721},{\"end\":75735,\"start\":75732},{\"end\":75743,\"start\":75739},{\"end\":75751,\"start\":75747},{\"end\":75987,\"start\":75984},{\"end\":75994,\"start\":75991},{\"end\":76217,\"start\":76213},{\"end\":76224,\"start\":76221},{\"end\":76233,\"start\":76228},{\"end\":76243,\"start\":76239},{\"end\":76252,\"start\":76247},{\"end\":76518,\"start\":76513},{\"end\":76532,\"start\":76522},{\"end\":76797,\"start\":76793},{\"end\":76811,\"start\":76801},{\"end\":76819,\"start\":76815},{\"end\":76982,\"start\":76978},{\"end\":76991,\"start\":76986},{\"end\":77002,\"start\":76995},{\"end\":77012,\"start\":77008},{\"end\":77231,\"start\":77227},{\"end\":77238,\"start\":77235},{\"end\":77245,\"start\":77242},{\"end\":77252,\"start\":77249},{\"end\":77476,\"start\":77473},{\"end\":77482,\"start\":77480},{\"end\":77493,\"start\":77489},{\"end\":77505,\"start\":77500},{\"end\":77519,\"start\":77509}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":55487,\"start\":55361},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":90239045},\"end\":55702,\"start\":55489},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":227151370},\"end\":55961,\"start\":55704},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":161878},\"end\":56126,\"start\":55963},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":201660344},\"end\":56746,\"start\":56128},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":9999787},\"end\":57137,\"start\":56748},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":53437459},\"end\":57474,\"start\":57139},{\"attributes\":{\"id\":\"b7\"},\"end\":57654,\"start\":57476},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":219617574},\"end\":57921,\"start\":57656},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":196470796},\"end\":58236,\"start\":57923},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":54474649},\"end\":58414,\"start\":58238},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":210701396},\"end\":58721,\"start\":58416},{\"attributes\":{\"id\":\"b12\"},\"end\":59492,\"start\":58723},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":102496818},\"end\":59778,\"start\":59494},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":2255738},\"end\":60074,\"start\":59780},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":206596482},\"end\":60450,\"start\":60076},{\"attributes\":{\"id\":\"b16\"},\"end\":60806,\"start\":60452},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":215935671},\"end\":61003,\"start\":60808},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":299085},\"end\":61261,\"start\":61005},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":9455111},\"end\":61542,\"start\":61263},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":206596513},\"end\":61788,\"start\":61544},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":46936649},\"end\":62030,\"start\":61790},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":232222923},\"end\":62403,\"start\":62032},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":131774103},\"end\":62708,\"start\":62405},{\"attributes\":{\"id\":\"b24\"},\"end\":62910,\"start\":62710},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":206594692},\"end\":63093,\"start\":62912},{\"attributes\":{\"id\":\"b26\"},\"end\":63284,\"start\":63095},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":4559916},\"end\":63503,\"start\":63286},{\"attributes\":{\"doi\":\"arXiv:1905.00538\",\"id\":\"b28\"},\"end\":63745,\"start\":63505},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":226975921},\"end\":64117,\"start\":63747},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":237213275},\"end\":64403,\"start\":64119},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":206986664},\"end\":64721,\"start\":64405},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":228083838},\"end\":64892,\"start\":64723},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":206849534},\"end\":65145,\"start\":64894},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":235703108},\"end\":65458,\"start\":65147},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":201645224},\"end\":65714,\"start\":65460},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":131775632},\"end\":66011,\"start\":65716},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":4572038},\"end\":66216,\"start\":66013},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":229297871},\"end\":66543,\"start\":66218},{\"attributes\":{\"id\":\"b39\"},\"end\":66759,\"start\":66545},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":15774646},\"end\":67031,\"start\":66761},{\"attributes\":{\"id\":\"b41\"},\"end\":67129,\"start\":67033},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":227209249},\"end\":67284,\"start\":67131},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":214774746},\"end\":67521,\"start\":67286},{\"attributes\":{\"id\":\"b44\"},\"end\":67678,\"start\":67523},{\"attributes\":{\"id\":\"b45\"},\"end\":67882,\"start\":67680},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":206594275},\"end\":68273,\"start\":67884},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":235248354},\"end\":68632,\"start\":68275},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":217148044},\"end\":68955,\"start\":68634},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":206775100},\"end\":69232,\"start\":68957},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":545361},\"end\":69475,\"start\":69234},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":195776274},\"end\":69825,\"start\":69477},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":52936213},\"end\":70231,\"start\":69827},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":206769866},\"end\":70444,\"start\":70233},{\"attributes\":{\"id\":\"b54\"},\"end\":70661,\"start\":70446},{\"attributes\":{\"id\":\"b55\"},\"end\":70815,\"start\":70663},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":8632684},\"end\":71142,\"start\":70817},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":206942855},\"end\":71481,\"start\":71144},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":232478414},\"end\":71734,\"start\":71483},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":54482591},\"end\":72025,\"start\":71736},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":214667893},\"end\":72208,\"start\":72027},{\"attributes\":{\"id\":\"b61\"},\"end\":72412,\"start\":72210},{\"attributes\":{\"id\":\"b62\",\"matched_paper_id\":6159584},\"end\":72995,\"start\":72414},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":13756489},\"end\":73277,\"start\":72997},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":232478376},\"end\":73546,\"start\":73279},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":207761262},\"end\":73829,\"start\":73548},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":233444160},\"end\":74123,\"start\":73831},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":209316336},\"end\":74348,\"start\":74125},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":227151533},\"end\":74697,\"start\":74350},{\"attributes\":{\"id\":\"b69\"},\"end\":74825,\"start\":74699},{\"attributes\":{\"id\":\"b70\"},\"end\":75004,\"start\":74827},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":8355130},\"end\":75408,\"start\":75006},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":4712004},\"end\":75637,\"start\":75410},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":229298063},\"end\":75904,\"start\":75639},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":3714620},\"end\":76113,\"start\":75906},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":214795169},\"end\":76428,\"start\":76115},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":53453235},\"end\":76753,\"start\":76430},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":51929808},\"end\":76916,\"start\":76755},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":11977588},\"end\":77142,\"start\":76918},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":244894891},\"end\":77395,\"start\":77144},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":220665428},\"end\":77680,\"start\":77397}]", "bib_title": "[{\"end\":55526,\"start\":55489},{\"end\":55758,\"start\":55704},{\"end\":55995,\"start\":55963},{\"end\":56208,\"start\":56128},{\"end\":56788,\"start\":56748},{\"end\":57245,\"start\":57139},{\"end\":57716,\"start\":57656},{\"end\":58028,\"start\":57923},{\"end\":58260,\"start\":58238},{\"end\":58473,\"start\":58416},{\"end\":58803,\"start\":58723},{\"end\":59600,\"start\":59494},{\"end\":59853,\"start\":59780},{\"end\":60146,\"start\":60076},{\"end\":60828,\"start\":60808},{\"end\":61078,\"start\":61005},{\"end\":61303,\"start\":61263},{\"end\":61611,\"start\":61544},{\"end\":61845,\"start\":61790},{\"end\":62179,\"start\":62032},{\"end\":62494,\"start\":62405},{\"end\":62956,\"start\":62912},{\"end\":63325,\"start\":63286},{\"end\":63888,\"start\":63747},{\"end\":64221,\"start\":64119},{\"end\":64458,\"start\":64405},{\"end\":64763,\"start\":64723},{\"end\":64941,\"start\":64894},{\"end\":65217,\"start\":65147},{\"end\":65532,\"start\":65460},{\"end\":65778,\"start\":65716},{\"end\":66081,\"start\":66013},{\"end\":66298,\"start\":66218},{\"end\":66843,\"start\":66761},{\"end\":67198,\"start\":67131},{\"end\":67351,\"start\":67286},{\"end\":67986,\"start\":67884},{\"end\":68382,\"start\":68275},{\"end\":68670,\"start\":68634},{\"end\":69013,\"start\":68957},{\"end\":69292,\"start\":69234},{\"end\":69572,\"start\":69477},{\"end\":69943,\"start\":69827},{\"end\":70278,\"start\":70233},{\"end\":70507,\"start\":70446},{\"end\":70693,\"start\":70663},{\"end\":70894,\"start\":70817},{\"end\":71196,\"start\":71144},{\"end\":71553,\"start\":71483},{\"end\":71801,\"start\":71736},{\"end\":72086,\"start\":72027},{\"end\":72475,\"start\":72414},{\"end\":73022,\"start\":72997},{\"end\":73324,\"start\":73279},{\"end\":73620,\"start\":73548},{\"end\":73900,\"start\":73831},{\"end\":74182,\"start\":74125},{\"end\":74447,\"start\":74350},{\"end\":75075,\"start\":75006},{\"end\":75468,\"start\":75410},{\"end\":75693,\"start\":75639},{\"end\":75980,\"start\":75906},{\"end\":76207,\"start\":76115},{\"end\":76509,\"start\":76430},{\"end\":76789,\"start\":76755},{\"end\":76974,\"start\":76918},{\"end\":77223,\"start\":77144},{\"end\":77469,\"start\":77397}]", "bib_author": "[{\"end\":55374,\"start\":55363},{\"end\":55384,\"start\":55374},{\"end\":55392,\"start\":55384},{\"end\":55535,\"start\":55528},{\"end\":55545,\"start\":55535},{\"end\":55551,\"start\":55545},{\"end\":55560,\"start\":55551},{\"end\":55567,\"start\":55560},{\"end\":55578,\"start\":55567},{\"end\":55770,\"start\":55760},{\"end\":55781,\"start\":55770},{\"end\":55794,\"start\":55781},{\"end\":55807,\"start\":55794},{\"end\":55816,\"start\":55807},{\"end\":56004,\"start\":55997},{\"end\":56018,\"start\":56004},{\"end\":56030,\"start\":56018},{\"end\":56218,\"start\":56210},{\"end\":56224,\"start\":56218},{\"end\":56232,\"start\":56224},{\"end\":56240,\"start\":56232},{\"end\":56248,\"start\":56240},{\"end\":56260,\"start\":56248},{\"end\":56268,\"start\":56260},{\"end\":56282,\"start\":56268},{\"end\":56291,\"start\":56282},{\"end\":56306,\"start\":56291},{\"end\":56319,\"start\":56306},{\"end\":56799,\"start\":56790},{\"end\":56810,\"start\":56799},{\"end\":56818,\"start\":56810},{\"end\":56831,\"start\":56818},{\"end\":56841,\"start\":56831},{\"end\":56850,\"start\":56841},{\"end\":56864,\"start\":56850},{\"end\":56876,\"start\":56864},{\"end\":57257,\"start\":57247},{\"end\":57265,\"start\":57257},{\"end\":57279,\"start\":57265},{\"end\":57291,\"start\":57279},{\"end\":57527,\"start\":57519},{\"end\":57533,\"start\":57527},{\"end\":57541,\"start\":57533},{\"end\":57549,\"start\":57541},{\"end\":57726,\"start\":57718},{\"end\":57734,\"start\":57726},{\"end\":57741,\"start\":57734},{\"end\":57751,\"start\":57741},{\"end\":57763,\"start\":57751},{\"end\":57771,\"start\":57763},{\"end\":58038,\"start\":58030},{\"end\":58048,\"start\":58038},{\"end\":58064,\"start\":58048},{\"end\":58270,\"start\":58262},{\"end\":58279,\"start\":58270},{\"end\":58291,\"start\":58279},{\"end\":58300,\"start\":58291},{\"end\":58309,\"start\":58300},{\"end\":58489,\"start\":58475},{\"end\":58500,\"start\":58489},{\"end\":58509,\"start\":58500},{\"end\":58522,\"start\":58509},{\"end\":58811,\"start\":58805},{\"end\":58821,\"start\":58811},{\"end\":58833,\"start\":58821},{\"end\":58843,\"start\":58833},{\"end\":58853,\"start\":58843},{\"end\":58867,\"start\":58853},{\"end\":58877,\"start\":58867},{\"end\":58888,\"start\":58877},{\"end\":58901,\"start\":58888},{\"end\":58911,\"start\":58901},{\"end\":58920,\"start\":58911},{\"end\":58930,\"start\":58920},{\"end\":58941,\"start\":58930},{\"end\":58948,\"start\":58941},{\"end\":59611,\"start\":59602},{\"end\":59620,\"start\":59611},{\"end\":59624,\"start\":59620},{\"end\":59864,\"start\":59855},{\"end\":59875,\"start\":59864},{\"end\":59884,\"start\":59875},{\"end\":59888,\"start\":59884},{\"end\":60157,\"start\":60148},{\"end\":60167,\"start\":60157},{\"end\":60178,\"start\":60167},{\"end\":60583,\"start\":60569},{\"end\":60595,\"start\":60583},{\"end\":60842,\"start\":60830},{\"end\":60855,\"start\":60842},{\"end\":61088,\"start\":61080},{\"end\":61096,\"start\":61088},{\"end\":61108,\"start\":61096},{\"end\":61116,\"start\":61108},{\"end\":61315,\"start\":61305},{\"end\":61323,\"start\":61315},{\"end\":61334,\"start\":61323},{\"end\":61345,\"start\":61334},{\"end\":61623,\"start\":61613},{\"end\":61636,\"start\":61623},{\"end\":61649,\"start\":61636},{\"end\":61857,\"start\":61847},{\"end\":61870,\"start\":61857},{\"end\":61880,\"start\":61870},{\"end\":61893,\"start\":61880},{\"end\":62195,\"start\":62181},{\"end\":62202,\"start\":62195},{\"end\":62506,\"start\":62496},{\"end\":62512,\"start\":62506},{\"end\":62528,\"start\":62512},{\"end\":62540,\"start\":62528},{\"end\":62764,\"start\":62753},{\"end\":62777,\"start\":62764},{\"end\":62964,\"start\":62958},{\"end\":62973,\"start\":62964},{\"end\":62980,\"start\":62973},{\"end\":62987,\"start\":62980},{\"end\":63171,\"start\":63159},{\"end\":63179,\"start\":63171},{\"end\":63339,\"start\":63327},{\"end\":63349,\"start\":63339},{\"end\":63357,\"start\":63349},{\"end\":63366,\"start\":63357},{\"end\":63378,\"start\":63366},{\"end\":63511,\"start\":63505},{\"end\":63522,\"start\":63511},{\"end\":63529,\"start\":63522},{\"end\":63540,\"start\":63529},{\"end\":63898,\"start\":63890},{\"end\":63908,\"start\":63898},{\"end\":63915,\"start\":63908},{\"end\":64231,\"start\":64223},{\"end\":64239,\"start\":64231},{\"end\":64246,\"start\":64239},{\"end\":64469,\"start\":64460},{\"end\":64479,\"start\":64469},{\"end\":64773,\"start\":64765},{\"end\":64781,\"start\":64773},{\"end\":64793,\"start\":64781},{\"end\":64955,\"start\":64943},{\"end\":64967,\"start\":64955},{\"end\":64979,\"start\":64967},{\"end\":64991,\"start\":64979},{\"end\":65002,\"start\":64991},{\"end\":65229,\"start\":65219},{\"end\":65241,\"start\":65229},{\"end\":65252,\"start\":65241},{\"end\":65263,\"start\":65252},{\"end\":65274,\"start\":65263},{\"end\":65285,\"start\":65274},{\"end\":65540,\"start\":65534},{\"end\":65547,\"start\":65540},{\"end\":65555,\"start\":65547},{\"end\":65562,\"start\":65555},{\"end\":65569,\"start\":65562},{\"end\":65786,\"start\":65780},{\"end\":65795,\"start\":65786},{\"end\":65803,\"start\":65795},{\"end\":65813,\"start\":65803},{\"end\":65824,\"start\":65813},{\"end\":65831,\"start\":65824},{\"end\":65844,\"start\":65831},{\"end\":66089,\"start\":66083},{\"end\":66100,\"start\":66089},{\"end\":66307,\"start\":66300},{\"end\":66317,\"start\":66307},{\"end\":66328,\"start\":66317},{\"end\":66339,\"start\":66328},{\"end\":66350,\"start\":66339},{\"end\":66362,\"start\":66350},{\"end\":66605,\"start\":66598},{\"end\":66617,\"start\":66605},{\"end\":66624,\"start\":66617},{\"end\":66636,\"start\":66624},{\"end\":66852,\"start\":66845},{\"end\":66860,\"start\":66852},{\"end\":66867,\"start\":66860},{\"end\":66875,\"start\":66867},{\"end\":67043,\"start\":67035},{\"end\":67050,\"start\":67043},{\"end\":67056,\"start\":67050},{\"end\":67068,\"start\":67056},{\"end\":67076,\"start\":67068},{\"end\":67361,\"start\":67353},{\"end\":67368,\"start\":67361},{\"end\":67380,\"start\":67368},{\"end\":67388,\"start\":67380},{\"end\":67591,\"start\":67581},{\"end\":67722,\"start\":67715},{\"end\":67734,\"start\":67722},{\"end\":67746,\"start\":67734},{\"end\":67756,\"start\":67746},{\"end\":67764,\"start\":67756},{\"end\":67997,\"start\":67988},{\"end\":68004,\"start\":67997},{\"end\":68015,\"start\":68004},{\"end\":68026,\"start\":68015},{\"end\":68037,\"start\":68026},{\"end\":68052,\"start\":68037},{\"end\":68060,\"start\":68052},{\"end\":68401,\"start\":68384},{\"end\":68410,\"start\":68401},{\"end\":68417,\"start\":68410},{\"end\":68426,\"start\":68417},{\"end\":68435,\"start\":68426},{\"end\":68682,\"start\":68672},{\"end\":68693,\"start\":68682},{\"end\":68703,\"start\":68693},{\"end\":68713,\"start\":68703},{\"end\":69028,\"start\":69015},{\"end\":69043,\"start\":69028},{\"end\":69055,\"start\":69043},{\"end\":69316,\"start\":69294},{\"end\":69329,\"start\":69316},{\"end\":69338,\"start\":69329},{\"end\":69342,\"start\":69338},{\"end\":69584,\"start\":69574},{\"end\":69596,\"start\":69584},{\"end\":69606,\"start\":69596},{\"end\":69619,\"start\":69606},{\"end\":69629,\"start\":69619},{\"end\":69955,\"start\":69945},{\"end\":69966,\"start\":69955},{\"end\":69976,\"start\":69966},{\"end\":69983,\"start\":69976},{\"end\":69990,\"start\":69983},{\"end\":69999,\"start\":69990},{\"end\":70010,\"start\":69999},{\"end\":70290,\"start\":70280},{\"end\":70300,\"start\":70290},{\"end\":70312,\"start\":70300},{\"end\":70323,\"start\":70312},{\"end\":70519,\"start\":70509},{\"end\":70526,\"start\":70519},{\"end\":70534,\"start\":70526},{\"end\":70712,\"start\":70695},{\"end\":70724,\"start\":70712},{\"end\":70907,\"start\":70896},{\"end\":70918,\"start\":70907},{\"end\":70926,\"start\":70918},{\"end\":70935,\"start\":70926},{\"end\":70948,\"start\":70935},{\"end\":70962,\"start\":70948},{\"end\":71207,\"start\":71198},{\"end\":71220,\"start\":71207},{\"end\":71230,\"start\":71220},{\"end\":71241,\"start\":71230},{\"end\":71252,\"start\":71241},{\"end\":71562,\"start\":71555},{\"end\":71569,\"start\":71562},{\"end\":71577,\"start\":71569},{\"end\":71585,\"start\":71577},{\"end\":71592,\"start\":71585},{\"end\":71811,\"start\":71803},{\"end\":71819,\"start\":71811},{\"end\":72096,\"start\":72088},{\"end\":72104,\"start\":72096},{\"end\":72289,\"start\":72281},{\"end\":72297,\"start\":72289},{\"end\":72487,\"start\":72477},{\"end\":72503,\"start\":72487},{\"end\":72516,\"start\":72503},{\"end\":72532,\"start\":72516},{\"end\":72540,\"start\":72532},{\"end\":72549,\"start\":72540},{\"end\":72558,\"start\":72549},{\"end\":72565,\"start\":72558},{\"end\":72580,\"start\":72565},{\"end\":72588,\"start\":72580},{\"end\":73035,\"start\":73024},{\"end\":73046,\"start\":73035},{\"end\":73056,\"start\":73046},{\"end\":73069,\"start\":73056},{\"end\":73078,\"start\":73069},{\"end\":73089,\"start\":73078},{\"end\":73099,\"start\":73089},{\"end\":73113,\"start\":73099},{\"end\":73334,\"start\":73326},{\"end\":73343,\"start\":73334},{\"end\":73350,\"start\":73343},{\"end\":73364,\"start\":73350},{\"end\":73373,\"start\":73364},{\"end\":73388,\"start\":73373},{\"end\":73394,\"start\":73388},{\"end\":73630,\"start\":73622},{\"end\":73641,\"start\":73630},{\"end\":73653,\"start\":73641},{\"end\":73669,\"start\":73653},{\"end\":73912,\"start\":73902},{\"end\":73925,\"start\":73912},{\"end\":73939,\"start\":73925},{\"end\":73950,\"start\":73939},{\"end\":73960,\"start\":73950},{\"end\":74191,\"start\":74184},{\"end\":74200,\"start\":74191},{\"end\":74206,\"start\":74200},{\"end\":74212,\"start\":74206},{\"end\":74219,\"start\":74212},{\"end\":74461,\"start\":74449},{\"end\":74469,\"start\":74461},{\"end\":74485,\"start\":74469},{\"end\":74495,\"start\":74485},{\"end\":74506,\"start\":74495},{\"end\":74755,\"start\":74749},{\"end\":74835,\"start\":74829},{\"end\":74847,\"start\":74835},{\"end\":74856,\"start\":74847},{\"end\":74865,\"start\":74856},{\"end\":74877,\"start\":74865},{\"end\":75086,\"start\":75077},{\"end\":75098,\"start\":75086},{\"end\":75111,\"start\":75098},{\"end\":75122,\"start\":75111},{\"end\":75477,\"start\":75470},{\"end\":75484,\"start\":75477},{\"end\":75490,\"start\":75484},{\"end\":75498,\"start\":75490},{\"end\":75506,\"start\":75498},{\"end\":75702,\"start\":75695},{\"end\":75711,\"start\":75702},{\"end\":75719,\"start\":75711},{\"end\":75730,\"start\":75719},{\"end\":75737,\"start\":75730},{\"end\":75745,\"start\":75737},{\"end\":75753,\"start\":75745},{\"end\":75989,\"start\":75982},{\"end\":75996,\"start\":75989},{\"end\":76219,\"start\":76209},{\"end\":76226,\"start\":76219},{\"end\":76235,\"start\":76226},{\"end\":76245,\"start\":76235},{\"end\":76254,\"start\":76245},{\"end\":76520,\"start\":76511},{\"end\":76534,\"start\":76520},{\"end\":76799,\"start\":76791},{\"end\":76813,\"start\":76799},{\"end\":76821,\"start\":76813},{\"end\":76984,\"start\":76976},{\"end\":76993,\"start\":76984},{\"end\":77004,\"start\":76993},{\"end\":77014,\"start\":77004},{\"end\":77233,\"start\":77225},{\"end\":77240,\"start\":77233},{\"end\":77247,\"start\":77240},{\"end\":77254,\"start\":77247},{\"end\":77478,\"start\":77471},{\"end\":77484,\"start\":77478},{\"end\":77495,\"start\":77484},{\"end\":77507,\"start\":77495},{\"end\":77521,\"start\":77507}]", "bib_venue": "[{\"end\":59119,\"start\":59042},{\"end\":72646,\"start\":72633},{\"end\":55404,\"start\":55392},{\"end\":55582,\"start\":55578},{\"end\":55820,\"start\":55816},{\"end\":56034,\"start\":56030},{\"end\":56323,\"start\":56319},{\"end\":56922,\"start\":56876},{\"end\":57295,\"start\":57291},{\"end\":57517,\"start\":57476},{\"end\":57775,\"start\":57771},{\"end\":58068,\"start\":58064},{\"end\":58313,\"start\":58309},{\"end\":58558,\"start\":58522},{\"end\":59040,\"start\":58948},{\"end\":59628,\"start\":59624},{\"end\":59895,\"start\":59888},{\"end\":60182,\"start\":60178},{\"end\":60567,\"start\":60452},{\"end\":60898,\"start\":60855},{\"end\":61120,\"start\":61116},{\"end\":61387,\"start\":61345},{\"end\":61653,\"start\":61649},{\"end\":61897,\"start\":61893},{\"end\":62206,\"start\":62202},{\"end\":62544,\"start\":62540},{\"end\":62751,\"start\":62710},{\"end\":62991,\"start\":62987},{\"end\":63157,\"start\":63095},{\"end\":63382,\"start\":63378},{\"end\":63598,\"start\":63556},{\"end\":63919,\"start\":63915},{\"end\":64250,\"start\":64246},{\"end\":64550,\"start\":64479},{\"end\":64797,\"start\":64793},{\"end\":65006,\"start\":65002},{\"end\":65289,\"start\":65285},{\"end\":65573,\"start\":65569},{\"end\":65848,\"start\":65844},{\"end\":66104,\"start\":66100},{\"end\":66366,\"start\":66362},{\"end\":66596,\"start\":66545},{\"end\":66885,\"start\":66875},{\"end\":67204,\"start\":67200},{\"end\":67392,\"start\":67388},{\"end\":67579,\"start\":67523},{\"end\":67713,\"start\":67680},{\"end\":68064,\"start\":68060},{\"end\":68439,\"start\":68435},{\"end\":68783,\"start\":68713},{\"end\":69084,\"start\":69055},{\"end\":69346,\"start\":69342},{\"end\":69639,\"start\":69629},{\"end\":70014,\"start\":70010},{\"end\":70327,\"start\":70323},{\"end\":70544,\"start\":70534},{\"end\":70728,\"start\":70724},{\"end\":70966,\"start\":70962},{\"end\":71299,\"start\":71252},{\"end\":71596,\"start\":71592},{\"end\":71871,\"start\":71819},{\"end\":72108,\"start\":72104},{\"end\":72279,\"start\":72210},{\"end\":72631,\"start\":72588},{\"end\":73120,\"start\":73113},{\"end\":73398,\"start\":73394},{\"end\":73677,\"start\":73669},{\"end\":73964,\"start\":73960},{\"end\":74223,\"start\":74219},{\"end\":74510,\"start\":74506},{\"end\":74747,\"start\":74699},{\"end\":75194,\"start\":75122},{\"end\":75510,\"start\":75506},{\"end\":75757,\"start\":75753},{\"end\":76000,\"start\":75996},{\"end\":76258,\"start\":76254},{\"end\":76581,\"start\":76534},{\"end\":76825,\"start\":76821},{\"end\":77018,\"start\":77014},{\"end\":77258,\"start\":77254},{\"end\":77525,\"start\":77521}]"}}}, "year": 2023, "month": 12, "day": 17}