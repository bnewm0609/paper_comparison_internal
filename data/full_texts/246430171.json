{"id": 246430171, "updated": "2023-10-05 17:37:51.795", "metadata": {"title": "FEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting", "authors": "[{\"first\":\"Tian\",\"last\":\"Zhou\",\"middle\":[]},{\"first\":\"Ziqing\",\"last\":\"Ma\",\"middle\":[]},{\"first\":\"Qingsong\",\"last\":\"Wen\",\"middle\":[]},{\"first\":\"Xue\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Liang\",\"last\":\"Sun\",\"middle\":[]},{\"first\":\"Rong\",\"last\":\"Jin\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Although Transformer-based methods have significantly improved state-of-the-art results for long-term series forecasting, they are not only computationally expensive but more importantly, are unable to capture the global view of time series (e.g. overall trend). To address these problems, we propose to combine Transformer with the seasonal-trend decomposition method, in which the decomposition method captures the global profile of time series while Transformers capture more detailed structures. To further enhance the performance of Transformer for long-term prediction, we exploit the fact that most time series tend to have a sparse representation in well-known basis such as Fourier transform, and develop a frequency enhanced Transformer. Besides being more effective, the proposed method, termed as Frequency Enhanced Decomposed Transformer ({\\bf FEDformer}), is more efficient than standard Transformer with a linear complexity to the sequence length. Our empirical studies with six benchmark datasets show that compared with state-of-the-art methods, FEDformer can reduce prediction error by $14.8\\%$ and $22.6\\%$ for multivariate and univariate time series, respectively. Code is publicly available at https://github.com/MAZiqing/FEDformer.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2201.12740", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icml/ZhouMWW0022", "doi": null}}, "content": {"source": {"pdf_hash": "563bac1c5cdd5096e9dbf8d4f3d5b3c4f7284e06", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2201.12740v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "10abba1d7774c11d1a5e68507bab50e6ad90452c", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/563bac1c5cdd5096e9dbf8d4f3d5b3c4f7284e06.txt", "contents": "\nFEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting\n16 Jun 2022\n\nTian Zhou \nZiqing Ma \nQingsong Wen \nXue Wang \nLiang Sun \nRong Jin \nFEDformer: Frequency Enhanced Decomposed Transformer for Long-term Series Forecasting\n16 Jun 2022\nAlthough Transformer-based methods have significantly improved state-of-the-art results for long-term series forecasting, they are not only computationally expensive but more importantly, are unable to capture the global view of time series (e.g. overall trend). To address these problems, we propose to combine Transformer with the seasonal-trend decomposition method, in which the decomposition method captures the global profile of time series while Transformers capture more detailed structures. To further enhance the performance of Transformer for longterm prediction, we exploit the fact that most time series tend to have a sparse representation in well-known basis such as Fourier transform, and develop a frequency enhanced Transformer. Besides being more effective, the proposed method, termed as Frequency Enhanced Decomposed Transformer (FEDformer), is more efficient than standard Transformer with a linear complexity to the sequence length. Our empirical studies with six benchmark datasets show that compared with state-of-the-art methods, FEDformer can reduce prediction error by 14.8% and 22.6% for multivariate and univariate time series, respectively. Code is publicly available at https://github.com/MAZiqing/FEDformer.\n\nIntroduction\n\nLong-term time series forecasting is a long-standing challenge in various applications (e.g., energy, weather, traffic, economics). Despite the impressive results achieved by RNN-type methods (Rangapuram et al., 2018;Flunkert et al., 2017), they often suffer from the problem of gradi-* Equal contribution 1 Machine Intelligence Technology, Alibaba Group.. Correspondence to: Tian Zhou <tian.zt@alibabainc.com>, Rong Jin <jinrong.jr@alibaba-inc.com>.\n\nProceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s). ent vanishing or exploding (Pascanu et al., 2013), significantly limiting their performance. Following the recent success in NLP and CV community (Vaswani et al., 2017;Devlin et al., 2019;Dosovitskiy et al., 2021;Rao et al., 2021), Transformer (Vaswani et al., 2017) has been introduced to capture long-term dependencies in time series forecasting and shows promising results Wu et al., 2021). Since high computational complexity and memory requirement make it difficult for Transformer to be applied to long sequence modeling, numerous studies are devoted to reduce the computational cost of Transformer (Li et al., Kitaev et al., 2020;Wang et al., 2020;Xiong et al., 2021;Ma et al., 2021). A through overview of this line of works can be found in Appendix A.\n\nDespite the progress made by Transformer-based methods for time series forecasting, they tend to fail in capturing the overall characteristics/distribution of time series in some cases. In Figure 1, we compare the time series of ground truth with that predicted by the vanilla Transformer method (Vaswani et al., 2017) in a real-world ETTm1 dataset . It is clear that the predicted time series shared a different distribution from that of ground truth. The discrepancy between ground truth and prediction could be explained by the point-wise attention and prediction in Transformer. Since prediction for each timestep is made individually and independently, it is likely that the model fails to maintain the global property and statistics of time series as a whole. To address this problem, we exploit two ideas in this work. The first idea is to incorporate a seasonal-trend decomposition approach (Cleveland et al., 1990;Wen et al., 2019), which is widely used in time series analysis, into the Transformerbased method. Although this idea has been exploited before (Oreshkin et al., 2019;Wu et al., 2021), we present a special design of network that is effective in bringing the distribution of prediction close to that of ground truth, according to Kologrov-Smirnov distribution test. Our second idea is to combine Fourier analysis with the Transformerbased method. Instead of applying Transformer to the time domain, we apply it to the frequency domain which helps Transformer better capture global properties of time series. Combining both ideas, we propose a Frequency Enhanced Decomposition Transformer, or, FEDformer for short, for long-term time series forecasting.\n\nOne critical question with FEDformer is which subset of frequency components should be used by Fourier analysis to represent time series. A common wisdom is to keep lowfrequency components and throw away the high-frequency ones. This may not be appropriate for time series forecasting as some of trend changes in time series are related to important events, and this piece of information could be lost if we simply remove all high-frequency components. We address this problem by effectively exploiting the fact that time series tend to have (unknown) sparse representations on a basis like Fourier basis. According to our theoretical analysis, a randomly selected subset of frequency components, including both low and high ones, will give a better representation for time series, which is further verified by extensive empirical studies. Besides being more effective for long term forecasting, combining Transformer with frequency analysis allows us to reduce the computational cost of Transformer from quadratic to linear complexity. We note that this is different from previous efforts on speeding up Transformer, which often leads to a performance drop.\n\nIn short, we summarize the key contributions of this work as follows:\n\n1. We propose a frequency enhanced decomposed Transformer architecture with mixture of experts for seasonal-trend decomposition in order to better capture global properties of time series.\n\n2. We propose Fourier enhanced blocks and Wavelet enhanced blocks in the Transformer structure that allows us to capture important structures in time series through frequency domain mapping. They serve as substitutions for both self-attention and crossattention blocks.\n\n3. By randomly selecting a fixed number of Fourier components, the proposed model achieves linear computational complexity and memory cost. The effectiveness of this selection method is verified both theoretically and empirically.\n\n4. We conduct extensive experiments over 6 benchmark datasets across multiple domains (energy, traffic, economics, weather and disease). Our empirical studies show that the proposed model improves the performance of state-of-the-art methods by 14.8% and 22.6% for multivariate and univariate forecasting, respectively. Figure 1. Different distribution between ground truth and forecasting output from vanilla Transformer in a real-world ETTm1 dataset. Left: frequency mode and trend shift. Right: trend shift.\n\n\nCompact Representation of Time Series in Frequency Domain\n\nIt is well-known that time series data can be modeled from the time domain and frequency domain. One key contribution of our work which separates from other long-term forecasting algorithms is the frequency-domain operation with a neural network. As Fourier analysis is a common tool to dive into the frequency domain, while how to appropriately represent the information in time series using Fourier analysis is critical. Simply keeping all the frequency components may result in inferior representations since many high-frequency changes in time series are due to noisy inputs. On the other hand, only keeping the low-frequency components may also be inappropriate for series forecasting as some trend changes in time series represent important events. Instead, keeping a compact representation of time series using a small number of selected Fourier components will lead to efficient computation of transformer, which is crucial for modelling long sequences. We propose to represent time series by randomly selecting a constant number of Fourier components, including both highfrequency and low-frequency. Below, an analysis that justifies the random selection is presented theoretically. Empirical verification can be found in the experimental session.\n\nConsider we have m time series, denoted as X 1 (t), . . . , X m (t). By applying Fourier transform to each time series, we turn each X i (t) into a vector a i = (a i,1 , . . . , a i,d ) \u22a4 \u2208 R d . By putting all the Fourier transform vectors into a matrix, we have A = (a 1 , a 2 , . . . , a m ) \u22a4 \u2208 R m\u00d7d , with each row corresponding to a different time series and each column corresponding to a different Fourier component. Although using all the Fourier components allows us to best preserve the history information in the time series, it may potentially lead to overfitting of the history data and consequentially a poor prediction of future signals. Hence, we need to select a subset of Fourier components, that on the one hand should be small enough to avoid the overfitting problem and on the other hand, should be able to preserve most of the history information. Here, we propose to select s components from the d Fourier components (s < d) S l,2 en (or X l en ) uniformly at random. More specifically, we denote by i 1 < i 2 < . . . < i s the randomly selected components. We construct matrix S \u2208 {0, 1} s\u00d7d , with S i,k = 1 if i = i k and S i,k = 0 otherwise. Then, our representation of multivariate time series becomes A \u2032 = AS \u22a4 \u2208 R m\u00d7s . Below, we will show that, although the Fourier basis are randomly selected, under a mild condition, A \u2032 is able to preserve most of the information from A.\nX 0 en \u2208 R I\u00d7D X 0 de \u2208 R (I/2+O)\u00d7D T 0 de \u2208 R (I/2+O)\u00d7D X l\u22121 en T l\u22121 de X l\u22121 de T l de S l,3 de (or X l de ) Figure 2.\nIn order to measure how well A \u2032 is able to preserve information from A, we project each column vector of A into the subspace spanned by the column vectors in A \u2032 . We denote by P A \u2032 (A) the resulting matrix after the projection, where P A \u2032 (\u00b7) represents the projection operator. If A \u2032 preserves a large portion of information from A, we would expect a small error between A and P A \u2032 (A), i.e. |A \u2212 P A \u2032 (A)|. Let A k represent the approximation of A by its first k largest single value decomposition. The theorem below shows that |A\u2212P A \u2032 (A)| is close to |A\u2212A k | if the number of randomly sampled Fourier components s is on the order of k 2 .\n\nTheorem 1. Assume that \u00b5(A), the coherence measure of matrix A, is \u2126(k/n). Then, with a high probability, we have\n|A \u2212 P A \u2032 (A)| \u2264 (1 + \u01eb)|A \u2212 A k | if s = O(k 2 /\u01eb 2 ).\nThe detailed analysis can be found in Appendix C.\n\nFor real-world multivariate times series, the corresponding matrix A from Fourier transform often exhibit low rank property, since those univaraite variables in multivariate times series depend not only on its past values but also has dependency on each other, as well as share similar frequency components. Therefore, as indicated by the Theorem 1, randomly selecting a subset of Fourier components allows us to appropriately represent the information in Fourier matrix A.\n\nSimilarly, wavelet orthogonal polynomials, such as Legendre Polynomials, obey restricted isometry property (RIP) and can be used for capture information in time series as well. Compared to Fourier basis, wavelet based representation is more effective in capturing local structures in time series and thus can be more effective for some forecasting tasks. We defer the discussion of wavelet based representation in Appendix B. In the next section, we will present the design of frequency enhanced decomposed Transformer architecture that incorporate the Fourier transform into transformer.\n\n\nModel Structure\n\nIn this section, we will introduce (1) the overall structure of FEDformer, as shown in Figure 2, (2) two subversion structures for signal process: one uses Fourier basis and the other uses Wavelet basis, (3) the mixture of experts mechanism for seasonal-trend decomposition, and (4) the complexity analysis of the proposed model.\n\n\nFEDformer Framework\n\nPreliminary Long-term time series forecasting is a sequence to sequence problem. We denote the input length as I and output length as O. We denote D as the hidden states of the series. The input of the encoder is a I \u00d7 D matrix and the decoder has (I/2 + O) \u00d7 D input.\n\nFEDformer Structure Inspired by the seasonal-trend decomposition and distribution analysis as discussed in Section 1, we renovate Transformer as a deep decomposition architecture as shown in Figure 2, including Frequency Enhanced Block (FEB), Frequency Enhanced Attention (FEA) connecting encoder and decoder, and the Mixture Of Experts Decomposition block (MOEDecomp). The detailed description of FEB, FEA, and MOEDecomp blocks will be given in the following Section 3.2, 3.3, and 3.4 respectively.\n\nThe encoder adopts a multilayer structure as: X l en = Encoder(X l\u22121 en ), where l \u2208 {1, \u00b7 \u00b7 \u00b7 , N } denotes the output of l-th encoder layer and X 0 en \u2208 R I\u00d7D is the embedded historical series. The Encoder(\u00b7) is formalized as \n\nwhere S l,i en , i \u2208 {1, 2} represents the seasonal component after the i-th decomposition block in the l-th layer respectively. For FEB module, it has two different versions (FEB-f & FEB-w) which are implemented through Discrete Fourier transform (DFT) and Discrete Wavelet transform (DWT) mechanism respectively and can seamlessly replace the self-attention block.\n\nThe decoder also adopts a multilayer structure as:\nX l de , T l de = Decoder(X l\u22121 de , T l\u22121 de ), where l \u2208 {1\n, \u00b7 \u00b7 \u00b7 , M } denotes the output of l-th decoder layer. The Decoder(\u00b7) is formalized as where S l,i de , T l,i de , i \u2208 {1, 2, 3} represent the seasonal and trend component after the i-th decomposition block in the lth layer respectively. W l,i , i \u2208 {1, 2, 3} represents the projector for the i-th extracted trend T l,i de . Similar to FEB, FEA has two different versions (FEA-f & FEA-w) which are implemented through DFT and DWT projection respectively with attention design, and can replace the cross-attention block. The detailed description of FEA(\u00b7) will be given in the following Section 3.3.\n\nThe final prediction is the sum of the two refined decomposed components as W S \u00b7 X M de + T M de , where W S is to project the deep transformed seasonal component X M de to the target dimension.\n\n\nFourier Enhanced Structure\n\nDiscrete Fourier Transform (DFT) The proposed Fourier Enhanced Structures use discrete Fourier transform (DFT). Let F denotes the Fourier transform and F \u22121 de- Figure 3. Frequency Enhanced Block with Fourier transform (FEB-f) structure. Here a random subset of the Fourier basis is used and the scale of the subset is bounded by a scalar. When we choose the mode index before DFT and reverse DFT operations, the computation complexity can be further reduced to O(N ).\nF F \u22121 q \u2208 R L\u00d7D Sampling \u00d7 Q \u2208 C N \u00d7D C M \u00d7D Q \u2208 R \u2208\u1ef8 \u2208 C M \u00d7D padding Y \u2208 C N \u00d7D y \u2208 R Lde\u00d7D C M \u00d7D\u00d7D X l\u22121 en/de MLP\u00d7 MLP v F + Sampling MLP MLPQ K V F + Sampling F + Sampling \u03c3(\u00b7) \u00d7 Padding + F \u22121 y \u2208 R Lde\u00d7D q \u2208 R Lde\u00d7D kQK \u22a4 X l en S l,1 de\n\nFrequency Enhanced Block with Fourier Transform\n\n(FEB-f) The FEB-f is used in both encoder and decoder as shown in Figure 2. The input (x \u2208 R N \u00d7D ) of the FEB-f block is first linearly projected with w \u2208 R D\u00d7D , so q = x\u00b7w. Then q is converted from the time domain to the frequency domain. The Fourier transform of q is denoted as Q \u2208 C N \u00d7D . In frequency domain, only the randomly selected M modes are kept so we use a select operator as\nQ = Select(Q) = Select(F(q)),(3)\nwhereQ \u2208 C M\u00d7D and M << N . Then, the FEB-f is defined as\nFEB-f(q) = F \u22121 (Padding(Q \u2299 R)),(4)\nwhere R \u2208 C D\u00d7D\u00d7M is a parameterized kernel initialized randomly. Let Y = Q \u2299 C, with Y \u2208 C M\u00d7D . The production operator \u2299 is defined as: Y m,do = D di=0 Q m,di \u00b7 R di,do,m , where d i = 1, 2...D is the input channel and d o = 1, 2...D is the output channel. The result of Q \u2299 R is then zero-padded to C N \u00d7D before performing inverse Fourier transform back to the time domain. The structure is shown in Figure 3.\n\nFrequency Enhanced Attention with Fourier Transform (FEA-f) We use the expression of the canonical transformer. The input: queries, keys, values are denoted as q \u2208 R L\u00d7D , k \u2208 R L\u00d7D , v \u2208 R L\u00d7D . In cross-attention, the queries come from the decoder and can be obtained by q = x en \u00b7 w q , where w q \u2208 R D\u00d7D . The keys and values are from the encoder and can be obtained by k = x de \u00b7 w k and v = x de \u00b7 w v , where w k , w v \u2208 R D\u00d7D . Formally, the canonical attention can be written as\nAtten(q, k, v) = Softmax( qk \u22a4 dq )v.(5)\nIn FEA-f, we convert the queries, keys, and values with Fourier Transform and perform a similar attention mechanism in the frequency domain, by randomly selecting M modes. We denote the selected version after Fourier Transform asQ \u2208 C M\u00d7D ,K \u2208 C M\u00d7D ,\u1e7c \u2208 C M\u00d7D . The FEA-f is defined asQ\n= Select(F(q)) K = Select(F(k)) V = Select(F(v)) (6) FEA-f(q, k, v) = F \u22121 (Padding(\u03c3(Q \u00b7K \u22a4 ) \u00b7\u1e7c )),(7)\nwhere \u03c3 is the activation function. We use softmax or tanh for activation, since their converging performance differs in different data sets. Let Y = \u03c3(Q \u00b7K \u22a4 ) \u00b7\u1e7c , and Y \u2208 C M\u00d7D needs to be zero-padded to C L\u00d7D before performing inverse Fourier transform. The FEA-f structure is shown in Figure 4.\n\n\nWavelet Enhanced Structure\n\nDiscrete Wavelet Transform (DWT) While the Fourier transform creates a representation of the signal in the frequency domain, the Wavelet transform creates a representation in both the frequency and time domain, allowing efficient access of localized information of the signal. The multiwavelet transform synergizes the advantages of orthogonal polynomials as well as wavelets. For a given f (x), the multiwavelet coefficients at the scale n can be defined\nas s n l = f, \u03c6 n il \u00b5n k\u22121 i=0 , d n l = f, \u03c8 n il \u00b5n k\u22121 i=0\n, respectively, w.r.t. measure \u00b5 n with s n l , d n l \u2208 R k\u00d72 n . \u03c6 n il are wavelet orthonormal basis of piecewise polynomials. The decomposition/reconstruction across scales is defined as\ns n l = H (0) s n+1 2l + H (1) s n+1 2l+1 , s n+1 2l = \u03a3 (0) H (0)T s n l + G (0)T d n l , d n l = G (0) s n+1 2l + H (1) s n+1 2l+1 , s n+1 2l+1 = \u03a3 (1) H (1)T s n l + G (1)T d n l ,(8)\nwhere H (0) , H (1) , G (0) , G (1) are linear coefficients for multiwavelet decomposition filters. They are fixed matrices used for wavelet decomposition. The multiwavelet representation of a signal can be obtained by the tensor product of multiscale and multiwavelet basis. Note that the basis at various scales are coupled by the tensor product, so we need to untangle it. Inspired by (Gupta et al., 2021), we adapt a non-standard wavelet representation to reduce the model complexity. For a map function F (x) = x \u2032 , the map under multiwavelet domain can be written as U n dl = And n l + Bns n l , U n sl = Cnd n l , U L sl =F s L l , (9) where (U n sl , U n dl , s n l , d n l ) are the multiscale, multiwavelet coefficients, L is the coarsest scale under recursive decomposition, and A n , B n , C n are three independent FEB-f blocks modules used for processing different signal during decomposition and reconstruction. HereF is a single-layer of perceptrons which processes the remaining coarsest signal after L decomposed steps. More designed detail is described in Appendix D.\nDecomposed Matrix X(L) Lf :X(L+1) FEB-f FEB-f Hf Hf Ud(L) Us(L) Ud(L) Us(L) X'(L+1) + X'(L) Reconstruction matrix FEB-f Lf + Decomposed Matrix q(L) k(L) v(L) Lf :q(L+1) k(L+1) v(L+1) FEA-f FEA-f\nFrequency Enhanced Block with Wavelet Transform (FEB-w) The overall FEB-w architecture is shown in Figure 5. It differs from FEB-f in the recursive mechanism: the input is decomposed into 3 parts recursively and operates individually. For the wavelet decomposition part, we implement the fixed Legendre wavelets basis decomposition matrix. Three FEB-f modules are used to process the resulting high-frequency part, low-frequency part, and remaining part from wavelet decomposition respectively. For each cycle L, it produces a processed high-frequency tensor U d(L), a processed low-frequency frequency tensor U s(L), and the raw low-frequency tensor X(L+1). This is a ladder-down approach, and the decomposition stage performs the decimation of the signal by a factor of 1/2, running for a maximum of L cycles, where L < log 2 (M ) for a given input sequence of size M . In practice, L is set as a fixed argument parameter. The three sets of FEB-f blocks are shared during different decomposition cycles L. For the wavelet reconstruction part, we recursively build up our output tensor as well. For each cycle L, we combine X(L+1), U s(L), and U d(L) produced from the decomposition part and produce X(L) for the next reconstruction cycle. For each cycle, the length dimension of the signal tensor is increased by 2 times.\n\nFrequency Enhanced Attention with Wavelet Transform (FEA-w) FEA-w contains the decomposition stage and reconstruction stage like FEB-w. Here we keep the reconstruction stage unchanged. The only difference lies in the decomposition stage. The same decomposed matrix is used to decompose q, k, v signal separately, and q, k, v share the same sets of module to process them as well. As shown above, a frequency enhanced block with wavelet decomposition block (FEB-w) contains three FEB-f blocks for the signal process. We can view the FEB-f as a substitution of self-attention mechanism. We use a straightforward way to build the frequency enhanced cross attention with wavelet decomposition, substituting each FEB-f with a FEA-f module. Besides, another FEA-f module is added to process the coarsest remaining q(L), k(L), v(L) signal.\n\n\nMixture of Experts for Seasonal-Trend Decomposition\n\nBecause of the commonly observed complex periodic pattern coupled with the trend component on real-world data, extracting the trend can be hard with fixed window average pooling. To overcome such a problem, we design a Mixture Of Experts Decomposition block (MOEDecomp). It contains a set of average filters with different sizes to extract multiple trend components from the input signal and a set of data-dependent weights for combining them as the final trend. Formally, we have\nX trend = Softmax(L(x)) * (F (x)),(10)\nwhere F (\u00b7) is a set of average pooling filters and Softmax(L(x)) is the weights for mixing these extracted trends.\n\n\nComplexity Analysis\n\nFor FEDformer-f, the computational complexity for time and memory is O(L) with a fixed number of randomly selected modes in FEB & FEA blocks. We set modes number M = 64 as default value. Though the complexity Table 1. Complexity analysis of different forecasting models.\n\n\nMethods\n\nTraining  Table 1. It can be seen that the proposed FEDformer achieves the best overall complexity among Transformer-based forecasting models.\nTesting Time Memory Steps FEDformer O(L) O(L) 1 Autoformer O(L log L) O(L log L) 1 Informer O(L log L) O(L log L) 1 Transformer O L 2 O L 2 L LogTrans O(L log L) O L 2 1 Reformer O(L log L) O(L log L) L LSTM O(L) O(L) L of full DFT transformation by FFT is (O(L log(L)),\n\nExperiments\n\nTo evaluate the proposed FEDformer, we conduct extensive experiments on six popular real-world datasets, including energy, economics, traffic, weather, and disease. Since classic models like ARIMA and basic RNN/CNN models perform relatively inferior as shown in ( as baseline models. Note that since Autoformer holds the best performance in all the six benchmarks, it is used as the main baseline model for comparison. More details about baseline models, datasets, and implementation are described in Appendix A.2, F.1, and F.2, respectively.\n\n\nMain Results\n\nFor better comparison, we follow the experiment settings of Autoformer in (Wu et al., 2021) where the input length is fixed to 96, and the prediction lengths for both training and evaluation are fixed to be 96, 192, 336, and 720, respectively.\n\nMultivariate Results For the multivariate forecasting, FEDformer achieves the best performance on all six benchmark datasets at all horizons as shown in Table 2. Compared with Autoformer, the proposed FEDformer yields an overall 14.8% relative MSE reduction. It is worth noting   that for some of the datasets, such as Exchange and ILI, the improvement is even more significant (over 20%). Note that the Exchange dataset does not exhibit clear periodicity in its time series, but FEDformer can still achieve superior performance. Overall, the improvement made by FEDformer is consistent with varying horizons, implying its strength in long term forecasting. More detailed results on ETT full benchmark are provided in Appendix F.3.\n\n\nUnivariate Results\n\nThe results for univariate time series forecasting are summarized in Table 3. Compared with Autoformer, FEDformer yields an overall 22.6% relative MSE reduction, and on some datasets, such as traffic and weather, the improvement can be more than 30%. It again verifies that FEDformer is more effective in long-term forecasting. Note that due to the difference between Fourier and wavelet basis, FEDformer-f and FEDformer-w perform well on different datasets, making them complementary choice for long term forecasting. More detailed results on ETT full benchmark are provided in Appendix F.3.\n\n\nAblation Studies\n\nIn this section, the ablation experiments are conducted, aiming at comparing the performance of frequency enhanced block and its alternatives. The current SOTA results of Aut-oformer which uses the autocorrelation mechanism serve as the baseline. Three ablation variants of FEDformer are tested: 1) FEDformer V1: we use FEB to substitute selfattention only; 2) FEDformer V2: we use FEA to substitute cross attention only; 3) FEDFormer V3: we use FEA to substitute both self and cross attention. The ablated versions of FEDformer-f as well as the SOTA models are compared in Table 4, and we use a bold number if the ablated version brings improvements compared with Autoformer. We omit the similar results in FEDformer-w due to space limit. It can be seen in Table 4 \n\n\nMode Selection Policy\n\nThe selection of discrete Fourier basis is the key to effectively representing the signal and maintaining the model's linear complexity. As we discussed in Section 2, random Fourier mode selection is a better policy in forecasting tasks. more importantly, random policy requires no prior knowledge of the input and generalizes easily in new tasks. Here we empirically compare the random selection policy with fixed selection policy, and summarize the experimental results in Figure 6. It can be observed that the adopted random policy achieves better performance than the common fixed policy which only keeps the low frequency modes. Meanwhile, the random policy exhibits some mode saturation effect, indicating an appropriate random number of modes instead of all modes would bring better performance, which is also consistent with the theoretical analysis in Section 2.\n\n\nDistribution Analysis of Forecasting Output\n\nIn this section, we evaluate the distribution similarity between the input sequence and forecasting output of different transformer models quantitatively. In Table 5, we applied the Kolmogrov-Smirnov test to check if the forecasting results of different models made on ETTm1 and ETTm2 are consistent with the input sequences. In particular, we test if the input sequence of fixed 96-time steps come from the same distribution as the predicted sequence, with the null hypothesis that both sequences come from the same distribution. On both datasets, by setting the common P-value as 0.01, various existing Transformer baseline models have much less values than 0.01 except Autoformer, which indicates their forecasting output have a higher probability to be sampled from the different distributions compared to the input sequence. In contrast, Autoformer and FEDformer have much larger P-value compared to others, which mainly contributes to their seasonal-trend decomposition mechanism. Though we get close results from ETTm2 by both models, the proposed FEDformer has much larger P-value in ETTm1. And it's the only model whose null hypothesis can not be rejected with P-value larger than 0.01 in all cases of the two datasets, implying that the output sequence generated by FEDformer shares a more similar distribution as the input sequence than others and thus justifies the our design motivation of FEDformer as discussed in Section 1. More detailed analysis are provided in Appendix E.\n\n\nDifferences Compared to Autoformer baseline\n\nSince we use the decomposed encoder-decoder overall architecture as Autoformer, we think it is critical to emphasize the differences. In Autoformer, the authors consider a nice idea to use the top-k sub-sequence correlation (autocorrelation) module instead of point-wise attention, and the Fourier method is applied to improve the efficiency for subsequence level similarity computation. In general, Autoformer can be considered as decomposing the sequence into multiple time domain sub-sequences for feature exaction. In contrast, We use frequency transform to decompose the sequence into multiple frequency domain modes to extract the feature. In particular, we do not use a selective approach in sub-sequence selection. Instead, all frequency features are computed from the whole sequence, and this global property makes our model engage better performance for long sequence.\n\n\nConclusions\n\nThis paper proposes a frequency enhanced transformer model for long-term series forecasting which achieves state-of-the-art performance and enjoys linear computational complexity and memory cost. We propose an attention mechanism with low-rank approximation in frequency and a mixture of experts decomposition to control the distribution shifting. The proposed frequency enhanced structure decouples the input sequence length and the attention matrix dimension, leading to the linear complexity. Moreover, we theoretically and empirically prove the effectiveness of the adopted random mode selection policy in frequency. Lastly, extensive experiments show that the proposed model achieves the best forecasting performance on six benchmark datasets in comparison with four state-ofthe-art algorithms. \n\n\nA. Related Work\n\nIn this section, an overview of the literature for time series forecasting will be given. The relevant works include traditional times series models (A.1), deep learning models (A.1), Transformer-based models (A.2), and the Fourier Transform in neural networks (A.3).\n\n\nA.1. Traditional Time Series Models\n\nData-driven time series forecasting helps researchers understand the evolution of the systems without architecting the exact physics law behind them. After decades of renovation, time series models have been well developed and served as the backbone of various projects in numerous application fields. The first generation of data-driven methods can date back to 1970. ARIMA (Box & Jenkins, 1968;Box & Pierce, 1970) follows the Markov process and builds an auto-regressive model for recursively sequential forecasting. However, an autoregressive process is not enough to deal with nonlinear and non-stationary sequences. With the bloom of deep neural networks in the new century, recurrent neural networks (RNN) was designed especially for tasks involving sequential data. Among the family of RNNs, LSTM (Hochreiter & Schmidhuber, 1997) and GRU (Chung et al., 2014) employ gated structure to control the information flow to deal with the gradient vanishing or exploration problem. DeepAR (Flunkert et al., 2017) uses a sequential architecture for probabilistic forecasting by incorporating binomial likelihood. Attention based RNN (Qin et al., 2017) uses temporal attention to capture long-range dependencies. However, the recurrent model is not parallelizable and unable to handle long dependencies. The temporal convolutional network (Sen et al., 2019) is another family efficient in sequential tasks. However, limited to the reception field of the kernel, the features extracted still stay local and long-term dependencies are hard to grasp.\n\n\nA.2. Transformers for Time Series Forecasting\n\nWith the innovation of transformers in natural language processing (Vaswani et al., 2017;Devlin et al., 2019) and computer vision tasks (Dosovitskiy et al., 2021;Rao et al., 2021), transformer-based models are also discussed, renovated, and applied in time series forecasting Wu et al., 2021). In sequence to sequence time series forecasting tasks an encoder-decoder architecture is popularly employed. The self-attention and cross-attention mechanisms are used as the core layers in transformers. However, when employing a point-wise connected matrix, the transformers suffer from quadratic computation complexity.\n\nTo get efficient computation without sacrificing too much on performance, the earliest modifications specify the attention matrix with predefined patterns. Examples include: (Qiu et al., 2020) uses block-wise attention which reduces the complexity to the square of block size. Longformer (Beltagy et al., 2020)  Another emerging strategy is to employ a low-rank approximation of the attention matrix. Linformer (Wang et al., 2020) uses trainable linear projection to compress the sequence length and achieves O(n) complexity and theoretically proves the boundary of approximation error based on JL lemma. Luna (Ma et al., 2021) develops a nested linear structure with O(n) complexity. Nystr\u00f6former (Xiong et al., 2021) leverages the idea of Nystr\u00f6m approximation in the attention mechanism and achieves an O(n) complexity. Performer (Choromanski et al., 2021) adopts an orthogonal random features approach to efficiently model kernelizable attention mechanisms.\n\n\nA.3. Fourier Transform in Transformers\n\nThanks to the algorithm of fast Fourier transform (FFT), the computation complexity of Fourier transform is compressed from N 2 to N log N . The Fourier transform has the property that convolution in the time domain is equivalent to multiplication in the frequency domain. Thus the FFT can be used in the acceleration of convolutional networks (Mathieu et al., 2014). FFT can also be used in efficient computing of auto-correlation function, which can be used as a building neural networks block (Wu et al., 2021) and also useful in numerous anomaly detection tasks (Homayouni et al., 2020). (Li et al., 2020;Gupta et al., 2021) first introduced Fourier Neural Operator in solving partial differential equations (PDEs). FNO is used as an inner block of networks to perform efficient representation learning in the low-frequency domain. FNO is also proved efficient in computer vision tasks (Rao et al., 2021). It also serves as a working horse to build the Wavelet Neural Operator (WNO), which is recently introduced in solving PEDs (Gupta et al., 2021). While FNO keeps the spectrum modes in low frequency, random Fourier method use randomly selected modes. (Rahimi & Recht, 2008) proposes to map the input data to a randomized low-dimensional feature space to accelerate the training of kernel machines. (Rawat et al., 2019) proposes the Random Fourier softmax (RF-softmax) method that utilizes the powerful Random Fourier Features to enable more efficient and accurate sampling from an approximate softmax distribution.\n\nTo the best of our knowledge, our proposed method is the first work to achieve fast attention mechanism through low rank approximated transformation in frequency domain for time series forecasting.\n\n\nB. Low-rank Approximation of Attention\n\nIn this section, we discuss the low-rank approximation of the attention mechanism. First, we present the Restricted Isometry Property (RIP) matrices whose approximate error bound could be theoretically given in B.1. Then in B.2, we follow prior work and present how to leverage RIP matrices and attention mechanisms.\n\nIf the signal of interest is sparse or compressible on a fixed basis, then it is possible to recover the signal from fewer measurements. (Wang et al., 2020;Xiong et al., 2021) suggest that the attention matrix is low-rank, so the attention matrix can be well approximated if being projected into a subspace where the attention matrix is sparse. For the efficient computation of the attention matrix, how to properly select the basis of the projection yet remains to be an open question. The basis which follows the RIP is a potential candidate.\n\n\nB.1. RIP Matrices\n\nThe definition of the RIP matrices is:\n\nDefinition B.1. RIP matrices. Let m < n be positive integers, \u03a6 be a m \u00d7 n matrix with real entries, \u03b4 > 0, and K < m be an integer. We say that \u03a6 is\n(K, \u03b4) \u2212 RIP , if for every K-sparse vector x \u2208 R n we have (1 \u2212 \u03b4) x \u2264 \u03a6x \u2264 (1 + \u03b4) x .\nRIP matrices are the matrices that satisfy the restricted isometry property, discovered by D. Donoho, E. Cand\u00e8s and T. Tao in the field of compressed sensing. RIP matrices might be good choices for low-rank approximation because of their good properties. A random matrix has a negligible probability of not satisfying the RIP and many kinds of matrices have proven to be RIP, for example, Gaussian basis, Bernoulli basis, and Fourier basis.\n\nTheorem 2. Let m < n be positive integers, \u03b4 > 0, and K = O( m log 4 n ). Let \u03a6 be the random matrix defined by one of the following methods: \nThen \u03a6 is (K, \u03c3) \u2212 RIP with probability p \u2248 1 \u2212 e \u2212n .\nTheorem 2 states that Gaussian basis, Bernoulli basis and Fourier basis follow RIP. In the following section, the Fourier basis is used as an example and show how to use RIP basis in low-rank approximation in the attention mechanism.\n\n\nB.2. Low-rank Approximation with Fourier\n\nBasis/Legendre Polynomials Linformer (Wang et al., 2020) demonstrates that the attention mechanism can be approximated by a low-rank matrix. Linformer uses a trainable kernel initialized with Gaussian distribution for the low-rank approximation, While our proposed FEDformer uses Fourier basis/Legendre Polynomials, Gaussian basis, Fourier basis, and Legendre Polynomials all obey RIP, so similar conclusions could be drawn.\n\nStarting from Johnson-Lindenstrauss lemma (Johnson, 1984) and using the version from (Arriaga & Vempala, 2006), Linformer proves that a low-rank approximation of the attention matrix could be made.\n\nLet \u03a6 \u2208 R N \u00d7M be the random selected Fourier basis/Legendre Polynomials. \u03a6 is RIP matrix. Referring to Theorem 2, with a probability p \u2248 1\u2212e \u2212n , for any x \u2208 R N , we have\n(1 \u2212 \u03b4) x \u2264 \u03a6x \u2264 (1 + \u03b4) x .(11)\nReferring to (Arriaga & Vempala, 2006), with a probability p \u2248 1 \u2212 4e \u2212n , for any x 1 , x 2 \u2208 R N , we have\n(1 \u2212 \u03b4) x 1 x \u22a4 2 \u2264 x 1 \u03a6 \u22a4 \u03a6x \u22a4 2 \u2264 (1 + \u03b4) x 1 x \u22a4 2 .(12)\nWith the above inequation function, we now discuss the case in attention mechanism. Let the attention matrix\nB = sof tmax( QK \u22a4 \u221a d ) = exp(A) \u00b7 D \u22121 A , where (D A ) ii = N n=1 exp(A ni )\n. Following Linformer, we can conclude a theorem as (please refer to (Wang et al., 2020) for the detailed proof) Theorem 3. For any row vector p \u2208 R N of matrix B and any column vector v \u2208 R N of matrix V, with a probability\np = 1 \u2212 o(1), we have b\u03a6 \u22a4 \u03a6v \u22a4 \u2212 bv \u22a4 \u2264 \u03b4 bv \u22a4 .(13)\nTheorem 3 points out the fact that, using Fourier basis/Legendre Polynomials \u03a6 between the multiplication of attention matrix (P ) and values (V ), the computation complexity can be reduced from\nO(N 2 d) to O(N M d),\nwhere d is the hidden dimension of the matrix. In the meantime, the error of the low-rank approximation is bounded. However, Theorem 3 only discussed the case which is without the activation function.\n\nFurthermore, with the Cauchy inequality and the fact that the exponential function is Lipchitz continuous in a compact region (please refer to (Wang et al., 2020) for the proof), we can draw the following theorem:\n\n\nTheorem 4. For any row vector\nA i \u2208 R N in matrix A (A = QK \u22a4 \u221a d ), with a probability of p = 1 \u2212 o(1), we have exp(A i \u03a6 \u22a4 )\u03a6v \u22a4 \u2212exp(A i )v \u22a4 \u2264 \u03b4 exp(A i )v \u22a4 .(14)\nTheorem 4 states that with the activation function (softmax), the above discussed bound still holds.\n\nIn summary, we can leverage RIP matrices for low-rank approximation of attention. Moreover, there exists theoretical error bound when using a randomly selected Fourier basis for low-rank approximation in the attention mechanism.\n\n\nC. Fourier Component Selection\n\nLet X 1 (t), . . . , X m (t) be m time series. By applying Fourier transform to each time series, we turn each X i (t) into a vector a i = (a i,1 , . . . , a i,d ) \u22a4 \u2208 R d . By putting all the Fourier transform vectors into a matrix, we have A = (a 1 , a 2 , . . . , a m ) \u22a4 \u2208 R m\u00d7d , with each row corresponding to a different time series and each column corresponding to a different Fourier component. Here, we propose to select s components from the d Fourier components (s < d) uniformly at random. More specifically, we denote by i 1 < i 2 < . . . < i s the randomly selected components. We construct matrix S \u2208 {0, 1} s\u00d7d , with S i,k = 1 if i = i k and S i,k = 0 otherwise. Then, our representation of multivariate time series becomes A \u2032 = AS \u22a4 \u2208 R m\u00d7s . The following theorem shows that, although the Fourier basis is randomly selected, under a mild condition, A \u2032 can preserve most of the information from A.\n\nTheorem 5. Assume that \u00b5(A), the coherence measure of matrix A, is \u2126(k/n). Then, with a high probability, we have\n|A \u2212 P A \u2032 (A)| \u2264 (1 + \u01eb)|A \u2212 A k | if s = O(k 2 /\u01eb 2 ).\nProof. Following the analysis in Theorem 3 from (Drineas et al., 2007), we have\n|A \u2212 P A \u2032 (A)| \u2264 |A \u2212 A \u2032 (A \u2032 ) \u2020 A k | = |A \u2212 (AS \u22a4 )(AS \u22a4 ) \u2020 A k | = |A \u2212 (AS \u22a4 )(A k S \u22a4 ) \u2020 A k |.\nUsing Theorem 5 from (Drineas et al., 2007), we have, with a probability at least 0.7,\n|A \u2212 (AS \u22a4 )(A k S \u22a4 ) \u2020 A k | \u2264 (1 + \u01eb)|A \u2212 A k | if s = O(k 2 /\u01eb 2 \u00d7 \u00b5(A)n/k). The theorem follows because \u00b5(A) = O(k/n).\n\nD. Wavelets\n\nIn this section, we present some technical background about Wavelet transform which is used in our proposed framework.\n\n\nD.1. Continuous Wavelet Transform\n\nFirst, let's see how a function f (t) is decomposed into a set of basis functions \u03c8 s,\u03c4 (t), called the wavelets. It is known as the continuous wavelet transform or CW T . More formally it is written as\n\u03b3(s, \u03c4 ) = f (t)\u03a8 * s,\u03c4 (t)dt,\nwhere * denotes complex conjugation. This equation shows the variables \u03b3(s, \u03c4 ), s and \u03c4 are the new dimensions, scale, and translation after the wavelet transform, respectively.\n\nThe wavelets are generated from a single basic wavelet \u03a8(t), the so-called mother wavelet, by scaling and translation as\n\u03c8 s,\u03c4 (t) = 1 \u221a s \u03c8 t \u2212 \u03c4 s ,\nwhere s is the scale factor, \u03c4 is the translation factor, and \u221a s is used for energy normalization across the different scales.\n\n\nD.2. Discrete Wavelet Transform\n\nContinues wavelet transform maps a one-dimensional signal to a two-dimensional time-scale joint representation which is highly redundant. To overcome this problem, people introduce discrete wavelet transformation (DWT) with mother wavelet as\n\u03c8 j,k (t) = 1 s j 0 \u03c8 t \u2212 k\u03c4 0 s j 0 s j 0\nDWT is not continuously scalable and translatable but can be scaled and translated in discrete steps. Here j and k are integers and s 0 > 1 is a fixed dilation step. The translation factor \u03c4 0 depends on the dilation step. The effect of discretizing the wavelet is that the time-scale space is now sampled at discrete intervals. We usually choose s 0 = 2 so that the sampling of the frequency axis corresponds to dyadic sampling. For the translation factor, we usually choose \u03c4 0 = 1 so that we also have a dyadic sampling of the time axis.\n\nWhen discrete wavelets are used to transform a continuous signal, the result will be a series of wavelet coefficients and it is referred to as the wavelet decomposition.\n\n\nD.3. Orthogonal Polynomials\n\nThe next thing we need to focus on is orthogonal polynomials (OPs), which will serve as the mother wavelet function we introduce before. A lot of properties have to be maintained to be a mother wavelet, like admissibility condition, regularity conditions, and vanishing moments. In short, we are interested in the OPs that are non-zero over a finite domain and are zero almost everywhere else. Legendre is a popular set of OPs used it in our work here. Some other popular OPs can also be used here like Chebyshev without much modification.\n\n\nD.4. Legendre Polynomails\n\nThe Legendre polynomials are defined with respect to (w.r.t.) a uniform weight function w L (x) = 1 for \u22121\nx 1 or w L (x) = 1 [\u22121,1] (x) such that 1 \u22121 P i (x)P j (x)dx = 2 2i+1 i = j, 0 i = j.\nHere the function is defined over [\u22121, 1], but it can be extended to any interval [a, b] by performing different shift and scale operations.\n\n\nD.5. Multiwavelets\n\nThe multiwavelets which we use in this work combine advantages of the wavelet and OPs we introduce be-fore. Other than projecting a given function onto a single wavelet function, multiwavelet projects it onto a subspace of degree-restricted polynomials. In this work, we restricted our exploration to one family of OPs: Legendre Polynomials.\n\nFirst, the basis is defined as: A set of orthonormal basis w.r.t. measure \u00b5, are \u03c6 0 , . . . , \u03c6 k\u22121 such that \u03c6 i , \u03c6 j \u00b5 = \u03b4 ij . With a specific measure (weighting function w(x)), the orthonormality condition can be written as\n\u03c6 i (x)\u03c6 j (x)w(x)dx = \u03b4 ij .\nFollow the derivation in (Gupta et al., 2021), through using the tools of Gaussian Quadrature and Gram-Schmidt Orthogonalizaition, the filter coefficients of multiwavelets using Legendre polynomials can be written as\nH (0) ij = \u221a 2 1/2 0 \u03c6 i (x)\u03c6 j (2x)w L (2x \u2212 1)dx = 1 \u221a 2 1 0 \u03c6 i (x/2)\u03c6 j (x)dx = 1 \u221a 2 k i=1 \u03c9 i \u03c6 i x i 2 \u03c6 j (x i ) .\nFor example, if k = 3, following the formula, the filter coefficients are derived as follows\nH 0 = [ 1 \u221a 2 0 0 \u2212 \u221a 3 2 \u221a 2 1 2 \u221a 2 0 0 \u2212 \u221a 15 4 \u221a 2 1 4 \u221a 2 ], H 1 = [ 1 \u221a 2 0 0 \u221a 3 2 \u221a 2 1 2 \u221a 2 0 0 \u221a 15 4 \u221a 2 1 4 \u221a 2 ], G 0 = [ 1 2 \u221a 2 \u221a 3 2 \u221a 2 0 0 1 4 \u221a 2 \u221a 15 4 \u221a 2 0 0 1 \u221a 2 ], G 1 = [ \u2212 1 2 \u221a 2 \u221a 3 2 \u221a 2 0 0 \u2212 1 4 \u221a 2 \u221a 15 4 \u221a 2 0 0 \u2212 1 \u221a 2 ]\nE. Output Distribution Analysis E.1. Bad Case Analysis\n\nUsing vanilla Transformer as baseline model, we demonstrate two bad long-term series forecasting cases in ETTm1 dataset as shown in the following Figure 7. The forecasting shifts in Figure 7 is particularly related to the point-wise generation mechanism adapted by the vanilla Transformer model. To the contrary of classic models like Autoregressive integrated moving average (ARIMA) which has a predefined data bias structure for output distribution, Transformer-based models forecast each point independently and solely based on the overall MSE loss learning. This would result in different distribution between ground truth and forecasting output in some cases, leading to performance degradation.\n\n\nE.2. Kolmogorov-Smirnov Test\n\nWe adopt Kolmogorov-Smirnov (KS) test to check whether the two data samples come from the same distribution. KS test is a nonparametric test of the equality of continuous or discontinuous, two-dimensional probability distributions. In essence, the test answers the question \"what is the probability that these two sets of samples were drawn from the same (but unknown) probability distribution\". It quantifies a distance between the empirical distribution function of two samples. The Kolmogorov-Smirnov statistic is\nD n,m = sup x |F 1,n (x) \u2212 F 2,m (x)|\nwhere F 1,n and F 2,m are the empirical distribution functions of the first and the second sample respectively, and sup is the supremum function. For large samples, the null hypothesis is rejected at level \u03b1 if\nD n,m > \u2212 1 2 ln \u03b1 2 \u00b7 n + m n \u00b7 m ,\nwhere n and m are the sizes of the first and second samples respectively.\n\n\nE.3. Distribution Experiments and Analysis\n\nThough the KS test omits the temporal information from the input and output sequence, it can be used as a tool to measure the global property of the foretasting output sequence compared to the input sequence. The null hypothesis is that the two samples come from the same distribution. We can tell that if the P-value of the KS test is large and then the null hypothesis is less likely to be rejected for true output distribution.\n\nWe applied KS test on the output sequence of 96-720 prediction tasks for various models on the ETTm1 and ETTm2 datasets, and the results are summarized in Table 6. In the test, we compare the fixed 96-time step input sequence distribution with the output sequence distribution of different lengths. Using a 0.01 P-value as statistics, various existing Transformer baseline models have much less P-value than 0.01 except Autoformer, which indicates they have a higher probability to be sampled from the different distributions.  Autoformer and FEDformer have much larger P value compared to other models, which mainly contributes to their seasonal trend decomposition mechanism. Though we get close results from ETTm1 by both models, the proposed FEDformer has much larger P-values in ETTm1. And it is the only model whose null hypothesis can not be rejected with P-value larger than 0.01 in all cases of the two datasets, implying that the output sequence generated by FEDformer shares a more similar distribution as the input sequence than others and thus justifies the our design motivation of FEDformer as discussed in Section 1.\n\nNote that in the ETTm1 dataset, the True output sequence has a smaller P-value compared to our FEDformer's predicted output, it shows that the model's close output distribution is achieved through model's control other than merely more accurate prediction. This analysis shed some light on why the seasonal-trend decomposition architecture can give us better performance in long-term forecasting. The design is used to constrain the trend (mean) of the output distribution. Inspired by such observation, we design frequency enhanced block to constrain the seasonality (frequency mode) of the output distribution. \n\n\nF.2. Implementation Details\n\nOur model is trained using ADAM (Kingma & Ba, 2017) optimizer with a learning rate of 1e \u22124 . The batch size is set to 32. An early stopping counter is employed to stop the training process after three epochs if no loss degradation on the valid set is observed. The mean square error (MSE) and mean absolute error (MAE) are used as metrics. All experiments are repeated 5 times and the mean of the metrics is used in the final results. All the deep learning networks are implemented in PyTorch (Paszke et al., 2019) and trained on NVIDIA V100 32GB GPUs.\n\n\nF.3. ETT Full Benchmark\n\nWe present the full-benchmark on the four ETT datasets  in Table 8 (multivariate forecasting) and Table 9 (univariate forecasting). The ETTh1 and ETTh2 are recorded hourly while ETTm1 and ETTm2 are recorded every 15 minutes. The time series in ETTh1 and ETTm1 follow the same pattern, and the only difference is the sampling rate, similarly for ETTh2 and ETTm2. On average, our FEDformer yields a 11.5% relative MSE reduction for multivariate forecasting, and a 9.4% reduction for univariate forecasting over the SOTA results from Autoformer.\n\n\nF.4. Cross Attention Visualization\n\nThe \u03c3(Q \u00b7K \u22a4 ) can be viewed as the cross attention weight for our proposed frequency enhanced cross attention block. Several different activation functions can be used for attention matrix activation. Tanh and softmax are tested in this work with various performances on different datasets. We use tanh as the default one. Different attention patterns are visualized in Figure 8. Here two samples of cross attention maps are shown for FEDformer-f training on the ETTm2 dataset using tanh and softmax respectively. It can be seen that attention with Softmax as activation function seems to be more sparse than using tanh. Overall we can see attention in the frequency domain is much sparser compared to the normal attention graph in the time domain, which indicates our proposed attention can represent the signal more compactly. Also this compact representation supports our random mode selection mechanism to achieve linear complexity.\n\n\nF.5. Improvements of Mixture of Experts Decomposition\n\nWe design a mixture of experts decomposition mechanism which adopts a set of average pooling layers to extract the trend and a set of data-dependent weights to combine them. The default average pooling layers contain filters with kernel size 7, 12, 14, 24 and 48 respectively. For compari- son, we use single expert decomposition mechanism which employs a single average pooling layer with a fixed kernel size of 24 as the baseline. In Table 10, a comparison study of multivariate forecasting is shown using FEDformer-f model on two typical datasets. It is observed that the designed mixture of experts decomposition brings better performance than the single decomposition scheme.\n\nF.6. Multiple random runs Table 11 lists both mean and standard deviation (STD) for FEDformer-f and Autoformer with 5 runs. We observe a small variance in the performance of FEDformer-f, despite the randomness in frequency selection.\n\n\nF.7. Sensitivity to the number of modes: ETTx1 vs ETTx2\n\nThe choice of modes number depends on data complexity. The time series that exhibits the higher complex patterns requires the larger the number of modes. To verify this claim, we summarize the complexity of ETT datasets, measured by permutation entropy and SVD entropy, in Table 12. It is observed that ETTx1 has a significantly higher complexity (corresponding to a higher entropy value) than ETTx2, thus requiring a larger number of modes. \n\n\nF.8. When Fourier/Wavelet model performs better\n\nOur high level principle of model deployment is that Fourier-based model is usually better for less complex time series, while wavelet is normally more suitable for complex ones. Specifically, we found that wavelet-based model is more effective on multivariate time series, while Fourierbased one normally achieves better results on univariate time series. As indicated in Table 13, complexity measures on multivariate time series are higher than those on univariate ones. \n\nFigure 4 .\n4Frequency Enhanced Attention with Fourier transform (FEA-f) structure, \u03c3(\u00b7) is the activation function. notes the inverse Fourier transform. Given a sequence of real numbers x n in time domain, where n = 1, 2...N . DFT is defined as X l = N \u22121 n=0 x n e \u2212i\u03c9ln , where i is the imaginary unit and X l , l = 1, 2...L is a sequence of complex numbers in the frequency domain. Similarly, the inverse DFT is defined as x n = L\u22121 l=0 X l e i\u03c9ln . The complexity of DFT is O(N 2 ). With fast Fourier transform (FFT), the computation complexity can be reduced to O(N log N ).\n\nFigure 5 .\n5Top Left: Wavelet frequency enhanced block decomposition stage. Top Right: Wavelet block reconstruction stage shared by FEB-w and FEA-w. Bottom: Wavelet frequency enhanced cross attention decomposition stage.\n\n\nour model only needs O(L) cost and memory complexity with the pre-selected set of Fourier basis for quick implementation. For FEDformer-w, when we set the recursive decompose step to a fixed number L and use a fixed number of randomly selected modes the same as FEDformer-f, the time complexity and memory usage are O(L) as well. In practice, we choose L = 3 and modes number M = 64 as default value. The comparisons of the time complexity and memory usage in training and the inference steps in testing are summarized in\n\n\n and(Wu et al., 2021), we mainly include four state-of-theart transformer-based models for comparison, i.e.,Autoformer (Wu et al., 2021), Informer (Zhou et al., 2021,Log- Trans (Li et al., 2019)  andReformer (Kitaev et al., 2020)   \n\nFigure 6 .\n6Comparison of two base-modes selection method (Fix&Rand). Rand policy means randomly selecting a subset of modes, Fix policy means selecting the lowest frequency modes. Two policies are compared on a variety of base-modes number M \u2208 {2, 4, 8...256} on ETT full-benchmark (h1, m1, h2, m2).\n\n(\nGaussian basis) Let the entries of \u03a6 be i.i.d. with a normal distribution N (0, 1 m ). (Bernoulli basis) Let the entries of \u03a6 be i.i.d. with a Bernoulli distribution taking the values \u00b1 1 \u221a m m, each with 50% probability. (Random selected Discrete Fourier basis) Let A \u2282 {0, ..., n \u2212 1} be a random subset of size m. Let \u03a6 be the matrix obtained from the Discrete Fourier transform matrix (i.e. the matrix F with entries F [l, j] = exp \u22122\u03c0ilj/n / \u221a n) for l, j \u2208 {0, .., n \u2212 1} by selecting the rows indexed by A.\n\nFigure 7 .\n7Different distribution between ground truth and forecasting output from vanilla Transformer in a real-world ETTm1 dataset. Left: frequency mode and trend shift. Right: trend shift.\n\nFigure 8 .\n8Multihead attention map with 8 heads using tanh (top) and softmax (bottom) as activation map for the FEDformer-f training on ETTm2 dataset.\n\n\nFEDformer Structure. The FEDformer consists of N encoders and M decoders.The Frequency Enhanced Block (FEB, green \nblocks) and Frequency Enhanced Attention (FEA, red blocks) are used to perform representation learning in frequency domain. Either \nFEB or FEA has two subversions (FEB-f & FEB-w or FEA-f & FEA-w), where '-f' means using Fourier basis and '-w' means using \nWavelet basis. The Mixture Of Expert Decomposition Blocks (MOEDecomp, yellow blocks) are used to extract seasonal-trend patterns \nfrom the input data. \n\n\n\nTable 2 .\n2Multivariate long-term series forecasting results on six datasets with input length I = 96 and prediction length O \u2208 {96, 192, 336, 720} (For ILI dataset, we use input length I = 36 and prediction length O \u2208 {24, 36, 48, 60}). A lower MSE indicates better performance, and the best results are highlighted in bold.Methods \nMetric \nETTm2 \nElectricity \nExchange \nTraffic \nWeather \nILI \n96 \n192 336 720 \n96 \n192 336 720 \n96 \n192 336 720 \n96 \n192 \n336 \n720 \n96 \n192 336 720 \n24 \n36 \n48 \n60 \n\nFEDformer-f \nMSE 0.203 0.269 0.325 0.421 0.193 0.201 0.214 0.246 0.148 0.271 0.460 1.195 0.587 0.604 0.621 0.626 0.217 0.276 0.339 0.403 3.228 2.679 2.622 2.857 \nMAE 0.287 0.328 0.366 0.415 0.308 0.315 0.329 0.355 0.278 0.380 0.500 0.841 0.366 0.373 0.383 0.382 0.296 0.336 0.380 0.428 1.260 1.080 1.078 1.157 \n\nFEDformer-w \nMSE 0.204 0.316 0.359 0.433 0.183 0.195 0.212 0.231 0.139 0.256 0.426 1.090 0.562 0.562 0.570 0.596 0.227 0.295 0.381 0.424 2.203 2.272 2.209 2.545 \nMAE 0.288 0.363 0.387 0.432 0.297 0.308 0.313 0.343 0.276 0.369 0.464 0.800 0.349 0.346 0.323 0.368 0.304 0.363 0.416 0.434 0.963 0.976 0.981 1.061 \n\nAutoformer \nMSE 0.255 0.281 0.339 0.422 0.201 0.222 0.231 0.254 0.197 0.300 0.509 1.447 0.613 0.616 0.622 0.660 0.266 0.307 0.359 0.419 3.483 3.103 2.669 2.770 \nMAE 0.339 0.340 0.372 0.419 0.317 0.334 0.338 0.361 0.323 0.369 0.524 0.941 0.388 0.382 0.337 0.408 0.336 0.367 0.395 0.428 1.287 1.148 1.085 1.125 \n\nInformer \nMSE 0.365 0.533 1.363 3.379 0.274 0.296 0.300 0.373 0.847 1.204 1.672 2.478 0.719 0.696 0.777 0.864 0.300 0.598 0.578 1.059 5.764 4.755 4.763 5.264 \nMAE 0.453 0.563 0.887 1.338 0.368 0.386 0.394 0.439 0.752 0.895 1.036 1.310 0.391 0.379 0.420 0.472 0.384 0.544 0.523 0.741 1.677 1.467 1.469 1.564 \n\nLogTrans \nMSE 0.768 0.989 1.334 3.048 0.258 0.266 0.280 0.283 0.968 1.040 1.659 1.941 0.684 0.685 0.7337 0.717 0.458 0.658 0.797 0.869 4.480 4.799 4.800 5.278 \nMAE 0.642 0.757 0.872 1.328 0.357 0.368 0.380 0.376 0.812 0.851 1.081 1.127 0.384 0.390 0.408 0.396 0.490 0.589 0.652 0.675 1.444 1.467 1.468 1.560 \n\nReformer \nMSE 0.658 1.078 1.549 2.631 0.312 0.348 0.350 0.340 1.065 1.188 1.357 1.510 0.732 0.733 0.742 0.755 0.689 0.752 0.639 1.130 4.400 4.783 4.832 4.882 \nMAE 0.619 0.827 0.972 1.242 0.402 0.433 0.433 0.420 0.829 0.906 0.976 1.016 0.423 0.420 0.420 423 0.596 0.638 0.596 0.792 1.382 1.448 1.465 1.483 \n\n\n\nTable 3 .\n3Univariate long-term series forecasting results on six datasets with input length I = 96 and prediction length O \u2208 {96, 192, 336, 720} (For ILI dataset, we use input length I = 36 and prediction length O \u2208 {24, 36, 48, 60}). A lower MSE indicates better performance, and the best results are highlighted in bold.Methods \nMetric \nETTm2 \nElectricity \nExchange \nTraffic \nWeather \nILI \n96 \n192 336 720 \n96 \n192 336 720 \n96 \n192 \n336 720 \n96 \n192 336 720 \n96 \n192 \n336 \n720 \n24 \n36 \n48 \n60 \n\n\n\n\nTable 4. Ablation studies: multivariate long-term series forecasting results on ETTm1 and ETTm2 with input length I = 96 and prediction length O \u2208 {96, 192, 336, 720}. Three variants of FEDformer-f are compared with baselines. The best results are highlighted in bold.that FEDformer V1 \nbrings improvement in 10/16 cases, while FEDformer V2 \nimproves in 12/16 cases. The best performance is achieved \nin our FEDformer with FEB and FEA blocks which im-\nproves performance in all 16/16 cases. This verifies the \neffectiveness of the designed FEB, FEA for substituting \nself and cross attention. Furthermore, experiments on ETT \nand Weather datasets show that the adopted MOEDecomp \n(mixture of experts decomposition) scheme can bring an \naverage of 2.96% improvement compared with the single \ndecomposition scheme. More details are provided in Ap-\npendix F.5. \nMethods \nTransformer \nInformer \nAutoformer \nFEDformer V1 FEDformer V2 FEDformer V3 \nFEDformer-f \n\nSelf-att \nFullAtt \nProbAtt \nAutoCorr \nFEB-f(Eq. 4) \nAutoCorr \nFEA-f(Eq. 7) \nFEB-f(Eq. 4) \nCross-att \nFullAtt \nProbAtt \nAutoCorr \nAutoCorr \nFEA-f(Eq. 7) \nFEA-f(Eq. 7) \nFEA-f(Eq. 7) \n\nMetric \nMSE MAE MSE MAE MSE MAE MSE \nMAE \nMSE \nMAE \nMSE \nMAE \nMSE MAE \n\n\n\nTable 5 .\n5P-values of Kolmogrov-Smirnov test of different trans-\nformer models for long-term forecasting output on ETTm1 and \nETTm2 dataset. Larger value indicates the hypothesis (the input \nsequence and forecasting output come from the same distribution) \nis less likely to be rejected. The best results are highlighted. \n\nMethods Transformer Informer Autoformer FEDformer \nTrue \n\nETTm1 \n96 \n0.0090 \n0.0055 \n0.020 \n0.048 \n0.023 \n192 \n0.0052 \n0.0029 \n0.015 \n0.028 \n0.013 \n336 \n0.0022 \n0.0019 \n0.012 \n0.015 \n0.010 \n720 \n0.0023 \n0.0016 \n0.008 \n0.014 \n0.004 \n\nETTm2 \n96 \n0.0012 \n0.0008 \n0.079 \n0.071 \n0.087 \n192 \n0.0011 \n0.0006 \n0.047 \n0.045 \n0.060 \n336 \n0.0005 \n0.00009 \n0.027 \n0.028 \n0.042 \n720 \n0.0008 \n0.0002 \n0.023 \n0.021 \n0.023 \n\nCount \n0 \n0 \n3 \n5 \nNA \n\n\n\n\nLai, G., Chang, W.-C., Yang, Y., and Liu, H. Modeling long-and short-term temporal patterns with deep neural networks. In The 41st International ACM SIGIR Con-Devlin, J., Chang, M., Lee, K., and Toutanova, K. BERT: \npre-training of deep bidirectional transformers for lan-\nguage understanding. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Association \nfor Computational Linguistics: Human Language Tech-\nnologies (NAACL-HLT), Minneapolis, MN, USA, June 2-\n7, 2019, pp. 4171-4186, 2019. \n\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, \nD., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, \nM., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, \nN. An image is worth 16x16 words: Transformers for \nimage recognition at scale. In 9th International Confer-\nence on Learning Representations, ICLR 2021, Virtual \nEvent, Austria, May 3-7, 2021. OpenReview.net, 2021. \n\nDrineas, P., Mahoney, M. W., and Muthukrishnan, S. \nRelative-error CUR matrix decompositions. CoRR, \nabs/0708.3696, 2007. \n\nFlunkert, V., Salinas, D., and Gasthaus, J. Deepar: Prob-\nabilistic forecasting with autoregressive recurrent net-\nworks. CoRR, abs/1704.04110, 2017. \nGupta, G., Xiao, X., and Bogdan, P. Multiwavelet-based \noperator learning for differential equations, 2021. \n\nHochreiter, S. and Schmidhuber, J. Long Short-Term Mem-\nory. Neural Computation, 9(8):1735-1780, November \n1997. ISSN 0899-7667, 1530-888X. \n\nHomayouni, H., Ghosh, S., Ray, I., Gondalia, S., Dug-\ngan, J., and Kahn, M. G. An autocorrelation-based lstm-\nautoencoder for anomaly detection on time-series data. \nIn 2020 IEEE International Conference on Big Data (Big \nData), pp. 5068-5077, 2020. \n\nJohnson, W. B. Extensions of lipschitz mappings into \nhilbert space. Contemporary mathematics, 26:189-206, \n1984. \n\nKingma, D. P. and Ba, J. Adam: A Method for Stochas-\ntic Optimization. arXiv:1412.6980 [cs], January 2017. \narXiv: 1412.6980. \n\nKitaev, N., Kaiser, L., and Levskaya, A. Reformer: The \nefficient transformer. In 8th International Conference \non Learning Representations, ICLR 2020, Addis Ababa, \nEthiopia, April 26-30, 2020, 2020. \n\nference on Research & Development in Information Re-\ntrieval, pp. 95-104, 2018. \n\nLi, S., Jin, X., Xuan, Y., Zhou, X., Chen, W., Wang, Y.-\nX., and Yan, X. Enhancing the locality and breaking the \nmemory bottleneck of transformer on time series fore-\ncasting. In Advances in Neural Information Processing \nSystems, volume 32, 2019. \n\nLi, Z., Kovachki, N. B., Azizzadenesheli, K., Liu, B., Bhat-\ntacharya, K., Stuart, A. M., and Anandkumar, A. Fourier \nneural operator for parametric partial differential equa-\ntions. CoRR, abs/2010.08895, 2020. \nTay, Y., Bahri, D., Yang, L., Metzler, D., and Juan, D. \nSparse sinkhorn attention. In Proceedings of the 37th \nInternational Conference on Machine Learning, ICML \n2020, 13-18 July 2020, Virtual Event, volume 119 of Pro-\nceedings of Machine Learning Research, pp. 9438-9447. \nPMLR, 2020. \n\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, \nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Atten-\ntion is all you need. CoRR, abs/1706.03762, 2017. \n\nWang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. \nLinformer: Self-attention with linear complexity. CoRR, \nabs/2006.04768, 2020. \n\nWen, Q., Gao, J., Song, X., Sun, L., Xu, H., and Zhu, \nS. RobustSTL: A robust seasonal-trend decomposition \nalgorithm for long time series. In Proceedings of the \nAAAI Conference on Artificial Intelligence, volume 33, \npp. 5409-5416, 2019. \n\nWu, H., Xu, J., Wang, J., and Long, M. Autoformer: De-\ncomposition transformers with auto-correlation for long-\nterm series forecasting. In Proceedings of the Advances \nin Neural Information Processing Systems (NeurIPS), pp. \n101-112, 2021. \n\nXiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, \nY., and Singh, V. Nystr\u00f6mformer: A nystr\u00f6m-based al-\ngorithm for approximating self-attention. In Thirty-Fifth \nAAAI Conference on Artificial Intelligence, pp. 14138-\n14148, 2021. \n\nZaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Al-\nberti, C., Onta\u00f1\u00f3n, S., Pham, P., Ravula, A., Wang, Q., \nYang, L., and Ahmed, A. Big bird: Transformers for \nlonger sequences. In Annual Conference on Neural In-\nformation Processing Systems (NeurIPS), December 6-\n12, 2020, virtual, 2020. \n\nZhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., \nand Zhang, W. Informer: Beyond efficient transformer \nfor long sequence time-series forecasting. In The Thirty-\nFifth AAAI Conference on Artificial Intelligence, AAAI \n2021, Virtual Conference, volume 35, pp. 11106-11115. \nAAAI Press, 2021. \n\nZhu, Z. and Soricut, R. H-transformer-1d: Fast one-\ndimensional hierarchical attention for sequences. In Pro-\nceedings of the 59th Annual Meeting of the Associa-\ntion for Computational Linguistics (ACL) 2021, Virtual \nEvent, August 1-6, 2021, pp. 3801-3815, 2021. \n\n\n\n\nplace of canonical attention to get the sub-series level attention, which achieves N log N complexity with the help of Fast Fourier transform and top-k selection in an auto-correlation matrix.employs a stride window \nwith fixed intervals. LogTrans (Li et al., 2019) uses log-\nsparse attention and achieves N log 2 N complexity. H-\ntransformer (Zhu & Soricut, 2021) uses a hierarchical pat-\ntern for sparse approximation of attention matrix with O(n) \ncomplexity. Some work uses a combination of patterns \n(BIGBIRD (Zaheer et al., 2020)) mentioned above. An-\nother strategy is to use dynamic patterns: Reformer (Kitaev \net al., 2020) introduces a local-sensitive hashing which re-\nduces the complexity to N log N . (Zhu & Soricut, 2021) in-\ntroduces a hierarchical pattern. Sinkhorn (Tay et al., 2020) \nemploys a block sorting method to achieve quasi-global at-\ntention with only local windows. \n\nSimilarly, some work employs a top-k truncating to accel-\nerate computing: Informer (Zhou et al., 2021) uses a KL-\ndivergence based method to select top-k in attention matrix. \nThis sparser matrix costs only N log N in complexity. Aut-\noformer (Wu et al., 2021) introduces an auto-correlation \nblock in \n\nTable 6 .\n6Kolmogrov-Smirnov test P value for long sequence time-series forecasting output on ETT dataset (full experiment)Methods Transformer LogTrans Informer Reformer Autoformer FEDformer TrueETTm1 \n96 \n0.0090 \n0.0073 \n0.0055 \n0.0055 \n0.020 \n0.048 \n0.023 \n192 \n0.0052 \n0.0043 \n0.0029 \n0.0013 \n0.015 \n0.028 \n0.013 \n336 \n0.0022 \n0.0026 \n0.0019 \n0.0006 \n0.012 \n0.015 \n0.010 \n720 \n0.0023 \n0.0064 \n0.0016 \n0.0011 \n0.008 \n0.014 \n0.004 \n\nETTm2 \n96 \n0.0012 \n0.0025 \n0.0008 \n0.0028 \n0.078 \n0.071 \n0.087 \n192 \n0.0011 \n0.0011 \n0.0006 \n0.0015 \n0.047 \n0.045 \n0.060 \n336 \n0.0005 \n0.0011 \n0.00009 \n0.0007 \n0.027 \n0.028 \n0.042 \n720 \n0.0008 \n0.0005 \n0.0002 \n0.0005 \n0.023 \n0.021 \n0.023 \n\nCount \n0 \n0 \n0 \n0 \n3 \n5 \nNA \n\n\n\nTable 7 .\n7Summarized feature details of six datasets.DATASET \n\nLEN \nDIM \nFREQ \n\nETTM2 \n69680 \n8 \n15 MIN \nELECTRICITY 26304 322 \n1H \nEXCHANGE \n7588 \n9 \n1 DAY \nTRAFFIC \n17544 863 \n1H \nWEATHER \n52696 \n22 \n10 MIN \nILI \n966 \n8 \n7 DAYS \n\n\n\n\nF. Supplemental ExperimentsF.1. Dataset DetailsIn this paragraph, the details of the experiment datasets are summarized as follows: 1) ETT (Zhou et al., 2021) dataset contains two sub-dataset: ETT1 and ETT2, collected from two electricity transformers at two stations. Each of them has two versions in different resolutions (15min & 1h). ETT dataset contains multiple series of loads and one series of oil temperatures. 2) Electricity 1 dataset contains the electricity consumption of clients with each column corresponding to one client. 3) Exchange (Lai et al., 2018) contains the current exchange of 8 countries. 4) Traffic 2 dataset contains the occupation rate of freeway system across the State of California. 5) Weather 3 dataset contains 21 meteorological indicators for a range of 1 year in Germany. 6) Illness 4 dataset contains the influenza-like illness patients in the United States.Table 7summarizes feature details (Sequence Length: Len, Dimension: Dim, Frequency: Freq) of the six datasets. All datasets are split into the training set, validation set and test set by the ratio of 7:1:2.\n\nTable 8 .\n8Multivariate long sequence time-series forecasting results on ETT full benchmark. The best results are highlighted in bold. Metric MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE MSE MAE ET T h1Methods FEDformer-f FEDformer-w \nAutoformer \nInformer \nLogTrans \nReformer \n\n\n\nTable 10 .\n10Performance improvement of the designed mixture of experts decomposition scheme.Methods \nFEDformer-f \nFEDformer-f \n\nDataset \nETTh1 \nWeather \n\nMechanism \nMOE Single MOE Single \n\n96 \n0.217 0.238 \n0.376 \n0.375 \n192 \n0.276 0.291 \n0.420 \n0.412 \n336 \n0.339 0.352 \n0.450 0.455 \n720 \n0.403 0.413 \n0.496 \n0.502 \n\nImprovement \n5.35% \n0.57% \n\n\n\nTable 11 .\n11A subset of the benchmark showing both Mean and STD. 203 \u00b1 0.0042 0.194 \u00b1 0.0008 0.148 \u00b1 0.002 0.217 \u00b1 0.008 192 0.269 \u00b1 0.0023 0.201\u00b1 0.0015 0.270\u00b1 0.008 0.604 \u00b1 0.004 336 0.325 \u00b1 0.0015 0.215\u00b1 0.0018 0.460\u00b1 0.016 0.621 \u00b1 0.006 720 0.421 \u00b1 0.0038 0.246\u00b1 0.0020 1.195\u00b1 0.026 0.626 \u00b1 0.003MSE \nETTm2 \nElectricity \nExchange \nTraffic \n\nFED-f \n96 \n0.Autoformer \n96 \n0.255 \u00b1 0.020 \n0.201\u00b1 0.003 \n0.197\u00b1 0.019 \n0.613\u00b1 0.028 \n192 \n0.281 \u00b1 0.027 \n0.222\u00b1 0.003 \n0.300\u00b1 0.020 \n0.616\u00b1 0.042 \n336 \n0.339 \u00b1 0.018 \n0.231\u00b1 0.006 \n0.509\u00b1 0.041 \n0.622\u00b1 0.016 \n720 \n0.422 \u00b1 0.015 \n0.254\u00b1 0.007 \n1.447\u00b1 0.084 \n0.419\u00b1 0.017 \n\n\n\nTable 12 .\n12Complexity experiments for datasetsMethods \nETTh1 ETTh2 ETTm1 ETTm2 \n\nPermutation Entropy \n0.954 \n0.866 \n0.959 \n0.788 \n\nSVD Entropy \n0.807 \n0.495 \n0.589 \n0.361 \n\n\n\nTable 13 .\n13Perm Entropy Complexity comparison for multi vs uni Permutation Entropy Electricity Traffic Exchange IllnessMultivariate \n0.910 \n0.792 \n0.961 \n0.960 \n\nUnivariate \n0.902 \n0.790 \n0.949 \n0.867 \n\nT l,2 de T l,1 de T l,3 de S l,2 de S l,1 de S l,1 en\nS l,1 de , T l,1 de = MOEDecomp FEB X l\u22121 de + X l\u22121 de , S l,2 de , T l,2 de = MOEDecomp FEA S l,1 de , X N en + S l,1 de , S l,3 de , T l,3 de = MOEDecomp FeedForward S l,2 de + S l,2 de , X l de = S l,3 de , T l de = T l\u22121 de + W l,1 \u00b7T l,1 de + W l,2 \u00b7T l,2 de + W l,3 \u00b7T l,3 de ,(2)\nhttps://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams 20112014 2 http://pems.dot.ca.gov 3 https://www.bgc-jena.mpg.de/wetter/ 4 https://gis.cdc.gov/grasp/fluview/fluportaldashboard.html\n\nAn algorithmic theory of learning: Robust concepts and random projection. R I Arriaga, S S Vempala, Mach. Learn. 632Arriaga, R. I. and Vempala, S. S. An algorithmic theory of learning: Robust concepts and random projection. Mach. Learn., 63(2):161-182, 2006.\n\nThe long-document transformer. CoRR, abs. I Beltagy, M E Peters, A Cohan, Longformer, Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. CoRR, abs/2004.05150, 2020.\n\nSome recent advances in forecasting and control. G E P Box, G M Jenkins, Journal of the Royal Statistical Society. Series C (Applied Statistics). 172Box, G. E. P. and Jenkins, G. M. Some recent advances in forecasting and control. Journal of the Royal Statisti- cal Society. Series C (Applied Statistics), 17(2):91-109, 1968.\n\nDistribution of residual autocorrelations in autoregressive-integrated moving average time series models. G E P Box, D A Pierce, 65Box, G. E. P. and Pierce, D. A. Distribution of residual autocorrelations in autoregressive-integrated moving av- erage time series models. volume 65, pp. 1509-1526.\n\n. &amp; Taylor, Francis, Taylor & Francis, 1970.\n\nRethinking attention with performers. K M Choromanski, V Likhosherstov, D Dohan, X Song, A Gane, T Sarl\u00f3s, P Hawkins, J Q Davis, A Mohiuddin, L Kaiser, D B Belanger, L J Colwell, A Weller, 9th International Conference on Learning Representations (ICLR), Virtual Event. AustriaChoromanski, K. M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarl\u00f3s, T., Hawkins, P., Davis, J. Q., Mohi- uddin, A., Kaiser, L., Belanger, D. B., Colwell, L. J., and Weller, A. Rethinking attention with performers. In 9th International Conference on Learning Representations (ICLR), Virtual Event, Austria, May 3-7, 2021, 2021.\n\nEmpirical evaluation of gated recurrent neural networks on sequence modeling. J Chung, \u00c7 G\u00fcl\u00e7ehre, K Cho, Y Bengio, abs/1412.3555CoRRChung, J., G\u00fcl\u00e7ehre, \u00c7 ., Cho, K., and Bengio, Y. Empiri- cal evaluation of gated recurrent neural networks on se- quence modeling. CoRR, abs/1412.3555, 2014.\n\nStl: A seasonal-trend decomposition. R B Cleveland, W S Cleveland, J E Mcrae, I Terpenning, Journal of official statistics. 61Cleveland, R. B., Cleveland, W. S., McRae, J. E., and Ter- penning, I. Stl: A seasonal-trend decomposition. Jour- nal of official statistics, 6(1):3-73, 1990.\n\n. X Ma, X Kong, S Wang, C Zhou, J May, H Ma, L Zettlemoyer, Luna, abs/2106.01540Ma, X., Kong, X., Wang, S., Zhou, C., May, J., Ma, H., and Zettlemoyer, L. Luna: Linear unified nested attention. CoRR, abs/2106.01540, 2021.\n\nFast training of convolutional networks through ffts. M Mathieu, M Henaff, Y Lecun, 2nd International Conference on Learning Representations (ICLR). Banff, AB, CanadaConference Track ProceedingsMathieu, M., Henaff, M., and LeCun, Y. Fast training of convolutional networks through ffts. In 2nd Interna- tional Conference on Learning Representations (ICLR), Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014.\n\nNeural basis expansion analysis for interpretable time series forecasting. B N Oreshkin, D Carpov, N Chapados, Y Bengio, N-Beats, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)Oreshkin, B. N., Carpov, D., Chapados, N., and Bengio, Y. N-BEATS: Neural basis expansion analysis for inter- pretable time series forecasting. In Proceedings of the International Conference on Learning Representations (ICLR), 2019.\n\nOn the difficulty of training recurrent neural networks. R Pascanu, T Mikolov, Y Bengio, Proceedings of the 30th International Conference on Machine Learning, ICML 2013. the 30th International Conference on Machine Learning, ICML 2013Atlanta, GA, USA28Pascanu, R., Mikolov, T., and Bengio, Y. On the difficulty of training recurrent neural networks. In Proceedings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, vol- ume 28, pp. 1310-1318, 2013.\n\nAn imperative style, high-performance deep learning library. A Paszke, S Gross, F Massa, A Lerer, J Bradbury, G Chanan, T Killeen, Z Lin, N Gimelshein, L Antiga, A Desmaison, A Kopf, E Yang, Z Devito, M Raison, A Tejani, S Chilamkurthy, B Steiner, L Fang, J Bai, S Chintala, Pytorch, Advances in Neural Information Processing Systems. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Rai- son, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems, pp. 8024-8035.\n\nA dual-stage attention-based recurrent neural network for time series prediction. Y Qin, D Song, H Chen, W Cheng, G Jiang, G W Cottrell, Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI). the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI)Melbourne, AustraliaQin, Y., Song, D., Chen, H., Cheng, W., Jiang, G., and Cottrell, G. W. A dual-stage attention-based recurrent neural network for time series prediction. In Proceed- ings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI), Melbourne, Australia, August 19-25, 2017, pp. 2627-2633. ijcai.org, 2017.\n\nBlockwise self-attention for long document understanding. J Qiu, H Ma, O Levy, W Yih, S Wang, J Tang, 16-20 Novem- ber 2020Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event. Association for Computational LinguisticsEMNLP 2020 of Findings of ACLQiu, J., Ma, H., Levy, O., Yih, W., Wang, S., and Tang, J. Blockwise self-attention for long document understand- ing. In Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 Novem- ber 2020, volume EMNLP 2020 of Findings of ACL, pp. 2555-2565. Association for Computational Linguistics, 2020.\n\nRandom features for large-scale kernel machines. A Rahimi, B Recht, Advances in Neural Information Processing Systems. Platt, J., Koller, D., Singer, Y., and Roweis, S.Curran Associates, Inc20Rahimi, A. and Recht, B. Random features for large-scale kernel machines. In Platt, J., Koller, D., Singer, Y., and Roweis, S. (eds.), Advances in Neural Information Processing Systems, volume 20. Curran Associates, Inc., 2008.\n\nDeep state space models for time series forecasting. S S Rangapuram, M W Seeger, J Gasthaus, L Stella, Y Wang, Januschowski , T , Advances in Neural Information Processing Systems. Curran Associates, Inc31Rangapuram, S. S., Seeger, M. W., Gasthaus, J., Stella, L., Wang, Y., and Januschowski, T. Deep state space mod- els for time series forecasting. In Advances in Neural Information Processing Systems, volume 31. Curran As- sociates, Inc., 2018.\n\nGlobal filter networks for image classification. Y Rao, W Zhao, Z Zhu, J Lu, J Zhou, abs/2107.00645CoRRRao, Y., Zhao, W., Zhu, Z., Lu, J., and Zhou, J. Global filter networks for image classification. CoRR, abs/2107.00645, 2021.\n\nSampled softmax with random fourier features. A S Rawat, J Chen, F X Yu, A T Suresh, S Kumar, Advances in Neural Information Processing Systems (NeurIPS). Vancouver, BC, CanadaRawat, A. S., Chen, J., Yu, F. X., Suresh, A. T., and Kumar, S. Sampled softmax with random fourier fea- tures. In Advances in Neural Information Processing Sys- tems (NeurIPS), December 8-14, 2019, Vancouver, BC, Canada, pp. 13834-13844, 2019.\n\nThink globally, act locally: A deep neural network approach to highdimensional time series forecasting. R Sen, H Yu, I S Dhillon, Advances in Neural Information Processing Systems (NeurIPS), December 8-14. Vancouver, BC, CanadaSen, R., Yu, H., and Dhillon, I. S. Think globally, act locally: A deep neural network approach to high- dimensional time series forecasting. In Advances in Neu- ral Information Processing Systems (NeurIPS), Decem- ber 8-14, 2019, Vancouver, BC, Canada, pp. 4838-4847, 2019.\n", "annotations": {"author": "[{\"end\":110,\"start\":100},{\"end\":121,\"start\":111},{\"end\":135,\"start\":122},{\"end\":145,\"start\":136},{\"end\":156,\"start\":146},{\"end\":166,\"start\":157}]", "publisher": null, "author_last_name": "[{\"end\":109,\"start\":105},{\"end\":120,\"start\":118},{\"end\":134,\"start\":131},{\"end\":144,\"start\":140},{\"end\":155,\"start\":152},{\"end\":165,\"start\":162}]", "author_first_name": "[{\"end\":104,\"start\":100},{\"end\":117,\"start\":111},{\"end\":130,\"start\":122},{\"end\":139,\"start\":136},{\"end\":151,\"start\":146},{\"end\":161,\"start\":157}]", "author_affiliation": null, "title": "[{\"end\":86,\"start\":1},{\"end\":252,\"start\":167}]", "venue": null, "abstract": "[{\"end\":1505,\"start\":265}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b16\"},\"end\":1738,\"start\":1713},{\"end\":1760,\"start\":1738},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2168,\"start\":2146},{\"end\":2287,\"start\":2265},{\"end\":2307,\"start\":2287},{\"end\":2332,\"start\":2307},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2349,\"start\":2332},{\"end\":2385,\"start\":2351},{\"end\":2511,\"start\":2495},{\"end\":2756,\"start\":2736},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2774,\"start\":2756},{\"end\":2793,\"start\":2774},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2809,\"start\":2793},{\"end\":3199,\"start\":3177},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3804,\"start\":3780},{\"end\":3821,\"start\":3804},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3971,\"start\":3948},{\"end\":3987,\"start\":3971},{\"end\":23827,\"start\":23810},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":31014,\"start\":30993},{\"end\":31033,\"start\":31014},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":31483,\"start\":31463},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":31767,\"start\":31749},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":31972,\"start\":31954},{\"end\":32301,\"start\":32279},{\"end\":32321,\"start\":32301},{\"end\":32374,\"start\":32348},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":32391,\"start\":32374},{\"end\":32504,\"start\":32488},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":33021,\"start\":33003},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":33139,\"start\":33117},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":33455,\"start\":33439},{\"end\":33547,\"start\":33527},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":33688,\"start\":33662},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":34199,\"start\":34177},{\"end\":34346,\"start\":34329},{\"end\":34423,\"start\":34399},{\"end\":34442,\"start\":34425},{\"end\":34461,\"start\":34442},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":34741,\"start\":34723},{\"end\":34886,\"start\":34866},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":35014,\"start\":34992},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":35159,\"start\":35139},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":36071,\"start\":36052},{\"end\":36090,\"start\":36071},{\"end\":38161,\"start\":38146},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":38214,\"start\":38189},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":38547,\"start\":38522},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":38956,\"start\":38937},{\"end\":41476,\"start\":41454},{\"end\":41635,\"start\":41613},{\"end\":45303,\"start\":45283},{\"end\":50386,\"start\":50365}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":54551,\"start\":53971},{\"attributes\":{\"id\":\"fig_2\"},\"end\":54773,\"start\":54552},{\"attributes\":{\"id\":\"fig_3\"},\"end\":55297,\"start\":54774},{\"attributes\":{\"id\":\"fig_4\"},\"end\":55532,\"start\":55298},{\"attributes\":{\"id\":\"fig_6\"},\"end\":55834,\"start\":55533},{\"attributes\":{\"id\":\"fig_7\"},\"end\":56351,\"start\":55835},{\"attributes\":{\"id\":\"fig_8\"},\"end\":56545,\"start\":56352},{\"attributes\":{\"id\":\"fig_9\"},\"end\":56698,\"start\":56546},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":57225,\"start\":56699},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":59587,\"start\":57226},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":60087,\"start\":59588},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":61301,\"start\":60088},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":62061,\"start\":61302},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":66939,\"start\":62062},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":68141,\"start\":66940},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":68847,\"start\":68142},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":69082,\"start\":68848},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":70188,\"start\":69083},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":70463,\"start\":70189},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":70810,\"start\":70464},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":71431,\"start\":70811},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":71608,\"start\":71432},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":71814,\"start\":71609}]", "paragraph": "[{\"end\":1971,\"start\":1521},{\"end\":2879,\"start\":1973},{\"end\":4555,\"start\":2881},{\"end\":5715,\"start\":4557},{\"end\":5786,\"start\":5717},{\"end\":5976,\"start\":5788},{\"end\":6247,\"start\":5978},{\"end\":6479,\"start\":6249},{\"end\":6990,\"start\":6481},{\"end\":8308,\"start\":7052},{\"end\":9718,\"start\":8310},{\"end\":10493,\"start\":9842},{\"end\":10608,\"start\":10495},{\"end\":10715,\"start\":10666},{\"end\":11190,\"start\":10717},{\"end\":11780,\"start\":11192},{\"end\":12129,\"start\":11800},{\"end\":12421,\"start\":12153},{\"end\":12922,\"start\":12423},{\"end\":13152,\"start\":12924},{\"end\":13520,\"start\":13154},{\"end\":13572,\"start\":13522},{\"end\":14234,\"start\":13635},{\"end\":14431,\"start\":14236},{\"end\":14930,\"start\":14462},{\"end\":15619,\"start\":15228},{\"end\":15710,\"start\":15653},{\"end\":16162,\"start\":15748},{\"end\":16651,\"start\":16164},{\"end\":16980,\"start\":16693},{\"end\":17385,\"start\":17086},{\"end\":17871,\"start\":17416},{\"end\":18124,\"start\":17935},{\"end\":19399,\"start\":18312},{\"end\":20918,\"start\":19595},{\"end\":21752,\"start\":20920},{\"end\":22288,\"start\":21808},{\"end\":22443,\"start\":22328},{\"end\":22737,\"start\":22467},{\"end\":22891,\"start\":22749},{\"end\":23719,\"start\":23177},{\"end\":23979,\"start\":23736},{\"end\":24712,\"start\":23981},{\"end\":25327,\"start\":24735},{\"end\":26114,\"start\":25348},{\"end\":27011,\"start\":26140},{\"end\":28549,\"start\":27059},{\"end\":29475,\"start\":28597},{\"end\":30291,\"start\":29491},{\"end\":30578,\"start\":30311},{\"end\":32162,\"start\":30618},{\"end\":32827,\"start\":32212},{\"end\":33790,\"start\":32829},{\"end\":35355,\"start\":33833},{\"end\":35554,\"start\":35357},{\"end\":35913,\"start\":35597},{\"end\":36459,\"start\":35915},{\"end\":36519,\"start\":36481},{\"end\":36670,\"start\":36521},{\"end\":37200,\"start\":36760},{\"end\":37344,\"start\":37202},{\"end\":37633,\"start\":37400},{\"end\":38102,\"start\":37678},{\"end\":38301,\"start\":38104},{\"end\":38475,\"start\":38303},{\"end\":38617,\"start\":38509},{\"end\":38787,\"start\":38679},{\"end\":39092,\"start\":38868},{\"end\":39341,\"start\":39147},{\"end\":39564,\"start\":39364},{\"end\":39779,\"start\":39566},{\"end\":40050,\"start\":39950},{\"end\":40280,\"start\":40052},{\"end\":41233,\"start\":40315},{\"end\":41348,\"start\":41235},{\"end\":41485,\"start\":41406},{\"end\":41678,\"start\":41592},{\"end\":41935,\"start\":41817},{\"end\":42175,\"start\":41973},{\"end\":42385,\"start\":42207},{\"end\":42507,\"start\":42387},{\"end\":42665,\"start\":42538},{\"end\":42942,\"start\":42701},{\"end\":43526,\"start\":42986},{\"end\":43697,\"start\":43528},{\"end\":44268,\"start\":43729},{\"end\":44404,\"start\":44298},{\"end\":44632,\"start\":44492},{\"end\":44996,\"start\":44655},{\"end\":45227,\"start\":44998},{\"end\":45474,\"start\":45258},{\"end\":45690,\"start\":45598},{\"end\":46002,\"start\":45948},{\"end\":46704,\"start\":46004},{\"end\":47253,\"start\":46737},{\"end\":47502,\"start\":47292},{\"end\":47613,\"start\":47540},{\"end\":48090,\"start\":47660},{\"end\":49224,\"start\":48092},{\"end\":49839,\"start\":49226},{\"end\":50424,\"start\":49871},{\"end\":50994,\"start\":50452},{\"end\":51970,\"start\":51033},{\"end\":52708,\"start\":52028},{\"end\":52943,\"start\":52710},{\"end\":53445,\"start\":53003},{\"end\":53970,\"start\":53497}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9841,\"start\":9719},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10665,\"start\":10609},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13634,\"start\":13573},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15050,\"start\":14931},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15177,\"start\":15050},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15652,\"start\":15620},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15747,\"start\":15711},{\"attributes\":{\"id\":\"formula_8\"},\"end\":16692,\"start\":16652},{\"attributes\":{\"id\":\"formula_9\"},\"end\":17085,\"start\":16981},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17934,\"start\":17872},{\"attributes\":{\"id\":\"formula_11\"},\"end\":18311,\"start\":18125},{\"attributes\":{\"id\":\"formula_12\"},\"end\":19594,\"start\":19400},{\"attributes\":{\"id\":\"formula_13\"},\"end\":22327,\"start\":22289},{\"attributes\":{\"id\":\"formula_14\"},\"end\":23162,\"start\":22892},{\"attributes\":{\"id\":\"formula_15\"},\"end\":36759,\"start\":36671},{\"attributes\":{\"id\":\"formula_16\"},\"end\":37399,\"start\":37345},{\"attributes\":{\"id\":\"formula_17\"},\"end\":38508,\"start\":38476},{\"attributes\":{\"id\":\"formula_18\"},\"end\":38678,\"start\":38618},{\"attributes\":{\"id\":\"formula_19\"},\"end\":38867,\"start\":38788},{\"attributes\":{\"id\":\"formula_20\"},\"end\":39146,\"start\":39093},{\"attributes\":{\"id\":\"formula_21\"},\"end\":39363,\"start\":39342},{\"attributes\":{\"id\":\"formula_22\"},\"end\":39949,\"start\":39812},{\"attributes\":{\"id\":\"formula_23\"},\"end\":41405,\"start\":41349},{\"attributes\":{\"id\":\"formula_24\"},\"end\":41591,\"start\":41486},{\"attributes\":{\"id\":\"formula_25\"},\"end\":41802,\"start\":41679},{\"attributes\":{\"id\":\"formula_26\"},\"end\":42206,\"start\":42176},{\"attributes\":{\"id\":\"formula_27\"},\"end\":42537,\"start\":42508},{\"attributes\":{\"id\":\"formula_28\"},\"end\":42985,\"start\":42943},{\"attributes\":{\"id\":\"formula_29\"},\"end\":44491,\"start\":44405},{\"attributes\":{\"id\":\"formula_30\"},\"end\":45257,\"start\":45228},{\"attributes\":{\"id\":\"formula_31\"},\"end\":45597,\"start\":45475},{\"attributes\":{\"id\":\"formula_32\"},\"end\":45947,\"start\":45691},{\"attributes\":{\"id\":\"formula_33\"},\"end\":47291,\"start\":47254},{\"attributes\":{\"id\":\"formula_34\"},\"end\":47539,\"start\":47503}]", "table_ref": "[{\"end\":22683,\"start\":22676},{\"end\":22766,\"start\":22759},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":24141,\"start\":24134},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":24811,\"start\":24804},{\"end\":25929,\"start\":25922},{\"end\":26113,\"start\":26106},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":27224,\"start\":27217},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":48254,\"start\":48247},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":50518,\"start\":50511},{\"end\":50557,\"start\":50550},{\"attributes\":{\"ref_id\":\"tab_12\"},\"end\":52472,\"start\":52464},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":52744,\"start\":52736},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":53284,\"start\":53276},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":53878,\"start\":53870}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1519,\"start\":1507},{\"attributes\":{\"n\":\"2.\"},\"end\":7050,\"start\":6993},{\"attributes\":{\"n\":\"3.\"},\"end\":11798,\"start\":11783},{\"attributes\":{\"n\":\"3.1.\"},\"end\":12151,\"start\":12132},{\"attributes\":{\"n\":\"3.2.\"},\"end\":14460,\"start\":14434},{\"end\":15226,\"start\":15179},{\"attributes\":{\"n\":\"3.3.\"},\"end\":17414,\"start\":17388},{\"attributes\":{\"n\":\"3.4.\"},\"end\":21806,\"start\":21755},{\"attributes\":{\"n\":\"3.5.\"},\"end\":22465,\"start\":22446},{\"end\":22747,\"start\":22740},{\"attributes\":{\"n\":\"4.\"},\"end\":23175,\"start\":23164},{\"attributes\":{\"n\":\"4.1.\"},\"end\":23734,\"start\":23722},{\"end\":24733,\"start\":24715},{\"attributes\":{\"n\":\"4.2.\"},\"end\":25346,\"start\":25330},{\"attributes\":{\"n\":\"4.3.\"},\"end\":26138,\"start\":26117},{\"attributes\":{\"n\":\"4.4.\"},\"end\":27057,\"start\":27014},{\"attributes\":{\"n\":\"4.5.\"},\"end\":28595,\"start\":28552},{\"attributes\":{\"n\":\"5.\"},\"end\":29489,\"start\":29478},{\"end\":30309,\"start\":30294},{\"end\":30616,\"start\":30581},{\"end\":32210,\"start\":32165},{\"end\":33831,\"start\":33793},{\"end\":35595,\"start\":35557},{\"end\":36479,\"start\":36462},{\"end\":37676,\"start\":37636},{\"end\":39811,\"start\":39782},{\"end\":40313,\"start\":40283},{\"end\":41815,\"start\":41804},{\"end\":41971,\"start\":41938},{\"end\":42699,\"start\":42668},{\"end\":43727,\"start\":43700},{\"end\":44296,\"start\":44271},{\"end\":44653,\"start\":44635},{\"end\":46735,\"start\":46707},{\"end\":47658,\"start\":47616},{\"end\":49869,\"start\":49842},{\"end\":50450,\"start\":50427},{\"end\":51031,\"start\":50997},{\"end\":52026,\"start\":51973},{\"end\":53001,\"start\":52946},{\"end\":53495,\"start\":53448},{\"end\":53982,\"start\":53972},{\"end\":54563,\"start\":54553},{\"end\":55544,\"start\":55534},{\"end\":55837,\"start\":55836},{\"end\":56363,\"start\":56353},{\"end\":56557,\"start\":56547},{\"end\":57236,\"start\":57227},{\"end\":59598,\"start\":59589},{\"end\":61312,\"start\":61303},{\"end\":68152,\"start\":68143},{\"end\":68858,\"start\":68849},{\"end\":70199,\"start\":70190},{\"end\":70475,\"start\":70465},{\"end\":70822,\"start\":70812},{\"end\":71443,\"start\":71433},{\"end\":71620,\"start\":71610}]", "table": "[{\"end\":57225,\"start\":56774},{\"end\":59587,\"start\":57552},{\"end\":60087,\"start\":59912},{\"end\":61301,\"start\":60358},{\"end\":62061,\"start\":61314},{\"end\":66939,\"start\":62223},{\"end\":68141,\"start\":67134},{\"end\":68847,\"start\":68338},{\"end\":69082,\"start\":68903},{\"end\":70463,\"start\":70387},{\"end\":70810,\"start\":70558},{\"end\":71431,\"start\":71113},{\"end\":71608,\"start\":71481},{\"end\":71814,\"start\":71731}]", "figure_caption": "[{\"end\":54551,\"start\":53984},{\"end\":54773,\"start\":54565},{\"end\":55297,\"start\":54776},{\"end\":55532,\"start\":55300},{\"end\":55834,\"start\":55546},{\"end\":56351,\"start\":55838},{\"end\":56545,\"start\":56365},{\"end\":56698,\"start\":56559},{\"end\":56774,\"start\":56701},{\"end\":57552,\"start\":57238},{\"end\":59912,\"start\":59600},{\"end\":60358,\"start\":60090},{\"end\":62223,\"start\":62064},{\"end\":67134,\"start\":66942},{\"end\":68338,\"start\":68154},{\"end\":68903,\"start\":68860},{\"end\":70188,\"start\":69085},{\"end\":70387,\"start\":70201},{\"end\":70558,\"start\":70478},{\"end\":71113,\"start\":70825},{\"end\":71481,\"start\":71446},{\"end\":71731,\"start\":71623}]", "figure_ref": "[{\"end\":3078,\"start\":3070},{\"end\":6808,\"start\":6800},{\"end\":11895,\"start\":11887},{\"end\":12622,\"start\":12614},{\"end\":14631,\"start\":14623},{\"end\":15302,\"start\":15294},{\"end\":16161,\"start\":16153},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":17384,\"start\":17376},{\"end\":23440,\"start\":23439},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":26623,\"start\":26615},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":46158,\"start\":46150},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":46194,\"start\":46186},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":51412,\"start\":51404}]", "bib_author_first_name": "[{\"end\":72429,\"start\":72428},{\"end\":72431,\"start\":72430},{\"end\":72442,\"start\":72441},{\"end\":72444,\"start\":72443},{\"end\":72657,\"start\":72656},{\"end\":72668,\"start\":72667},{\"end\":72670,\"start\":72669},{\"end\":72680,\"start\":72679},{\"end\":72864,\"start\":72863},{\"end\":72868,\"start\":72865},{\"end\":72875,\"start\":72874},{\"end\":72877,\"start\":72876},{\"end\":73248,\"start\":73247},{\"end\":73252,\"start\":73249},{\"end\":73259,\"start\":73258},{\"end\":73261,\"start\":73260},{\"end\":73446,\"start\":73441},{\"end\":73528,\"start\":73527},{\"end\":73530,\"start\":73529},{\"end\":73545,\"start\":73544},{\"end\":73562,\"start\":73561},{\"end\":73571,\"start\":73570},{\"end\":73579,\"start\":73578},{\"end\":73587,\"start\":73586},{\"end\":73597,\"start\":73596},{\"end\":73608,\"start\":73607},{\"end\":73610,\"start\":73609},{\"end\":73619,\"start\":73618},{\"end\":73632,\"start\":73631},{\"end\":73642,\"start\":73641},{\"end\":73644,\"start\":73643},{\"end\":73656,\"start\":73655},{\"end\":73658,\"start\":73657},{\"end\":73669,\"start\":73668},{\"end\":74182,\"start\":74181},{\"end\":74191,\"start\":74190},{\"end\":74203,\"start\":74202},{\"end\":74210,\"start\":74209},{\"end\":74434,\"start\":74433},{\"end\":74436,\"start\":74435},{\"end\":74449,\"start\":74448},{\"end\":74451,\"start\":74450},{\"end\":74464,\"start\":74463},{\"end\":74466,\"start\":74465},{\"end\":74475,\"start\":74474},{\"end\":74685,\"start\":74684},{\"end\":74691,\"start\":74690},{\"end\":74699,\"start\":74698},{\"end\":74707,\"start\":74706},{\"end\":74715,\"start\":74714},{\"end\":74722,\"start\":74721},{\"end\":74728,\"start\":74727},{\"end\":74960,\"start\":74959},{\"end\":74971,\"start\":74970},{\"end\":74981,\"start\":74980},{\"end\":75413,\"start\":75412},{\"end\":75415,\"start\":75414},{\"end\":75427,\"start\":75426},{\"end\":75437,\"start\":75436},{\"end\":75449,\"start\":75448},{\"end\":75902,\"start\":75901},{\"end\":75913,\"start\":75912},{\"end\":75924,\"start\":75923},{\"end\":76410,\"start\":76409},{\"end\":76420,\"start\":76419},{\"end\":76429,\"start\":76428},{\"end\":76438,\"start\":76437},{\"end\":76447,\"start\":76446},{\"end\":76459,\"start\":76458},{\"end\":76469,\"start\":76468},{\"end\":76480,\"start\":76479},{\"end\":76487,\"start\":76486},{\"end\":76501,\"start\":76500},{\"end\":76511,\"start\":76510},{\"end\":76524,\"start\":76523},{\"end\":76532,\"start\":76531},{\"end\":76540,\"start\":76539},{\"end\":76550,\"start\":76549},{\"end\":76560,\"start\":76559},{\"end\":76570,\"start\":76569},{\"end\":76586,\"start\":76585},{\"end\":76597,\"start\":76596},{\"end\":76605,\"start\":76604},{\"end\":76612,\"start\":76611},{\"end\":77167,\"start\":77166},{\"end\":77174,\"start\":77173},{\"end\":77182,\"start\":77181},{\"end\":77190,\"start\":77189},{\"end\":77199,\"start\":77198},{\"end\":77208,\"start\":77207},{\"end\":77210,\"start\":77209},{\"end\":77813,\"start\":77812},{\"end\":77820,\"start\":77819},{\"end\":77826,\"start\":77825},{\"end\":77834,\"start\":77833},{\"end\":77841,\"start\":77840},{\"end\":77849,\"start\":77848},{\"end\":78415,\"start\":78414},{\"end\":78425,\"start\":78424},{\"end\":78840,\"start\":78839},{\"end\":78842,\"start\":78841},{\"end\":78856,\"start\":78855},{\"end\":78858,\"start\":78857},{\"end\":78868,\"start\":78867},{\"end\":78880,\"start\":78879},{\"end\":78890,\"start\":78889},{\"end\":78909,\"start\":78897},{\"end\":78913,\"start\":78912},{\"end\":79286,\"start\":79285},{\"end\":79293,\"start\":79292},{\"end\":79301,\"start\":79300},{\"end\":79308,\"start\":79307},{\"end\":79314,\"start\":79313},{\"end\":79513,\"start\":79512},{\"end\":79515,\"start\":79514},{\"end\":79524,\"start\":79523},{\"end\":79532,\"start\":79531},{\"end\":79534,\"start\":79533},{\"end\":79540,\"start\":79539},{\"end\":79542,\"start\":79541},{\"end\":79552,\"start\":79551},{\"end\":79993,\"start\":79992},{\"end\":80000,\"start\":79999},{\"end\":80006,\"start\":80005},{\"end\":80008,\"start\":80007}]", "bib_author_last_name": "[{\"end\":72439,\"start\":72432},{\"end\":72452,\"start\":72445},{\"end\":72665,\"start\":72658},{\"end\":72677,\"start\":72671},{\"end\":72686,\"start\":72681},{\"end\":72698,\"start\":72688},{\"end\":72872,\"start\":72869},{\"end\":72885,\"start\":72878},{\"end\":73256,\"start\":73253},{\"end\":73268,\"start\":73262},{\"end\":73453,\"start\":73447},{\"end\":73462,\"start\":73455},{\"end\":73542,\"start\":73531},{\"end\":73559,\"start\":73546},{\"end\":73568,\"start\":73563},{\"end\":73576,\"start\":73572},{\"end\":73584,\"start\":73580},{\"end\":73594,\"start\":73588},{\"end\":73605,\"start\":73598},{\"end\":73616,\"start\":73611},{\"end\":73629,\"start\":73620},{\"end\":73639,\"start\":73633},{\"end\":73653,\"start\":73645},{\"end\":73666,\"start\":73659},{\"end\":73676,\"start\":73670},{\"end\":74188,\"start\":74183},{\"end\":74200,\"start\":74192},{\"end\":74207,\"start\":74204},{\"end\":74217,\"start\":74211},{\"end\":74446,\"start\":74437},{\"end\":74461,\"start\":74452},{\"end\":74472,\"start\":74467},{\"end\":74486,\"start\":74476},{\"end\":74688,\"start\":74686},{\"end\":74696,\"start\":74692},{\"end\":74704,\"start\":74700},{\"end\":74712,\"start\":74708},{\"end\":74719,\"start\":74716},{\"end\":74725,\"start\":74723},{\"end\":74740,\"start\":74729},{\"end\":74746,\"start\":74742},{\"end\":74968,\"start\":74961},{\"end\":74978,\"start\":74972},{\"end\":74987,\"start\":74982},{\"end\":75424,\"start\":75416},{\"end\":75434,\"start\":75428},{\"end\":75446,\"start\":75438},{\"end\":75456,\"start\":75450},{\"end\":75465,\"start\":75458},{\"end\":75910,\"start\":75903},{\"end\":75921,\"start\":75914},{\"end\":75931,\"start\":75925},{\"end\":76417,\"start\":76411},{\"end\":76426,\"start\":76421},{\"end\":76435,\"start\":76430},{\"end\":76444,\"start\":76439},{\"end\":76456,\"start\":76448},{\"end\":76466,\"start\":76460},{\"end\":76477,\"start\":76470},{\"end\":76484,\"start\":76481},{\"end\":76498,\"start\":76488},{\"end\":76508,\"start\":76502},{\"end\":76521,\"start\":76512},{\"end\":76529,\"start\":76525},{\"end\":76537,\"start\":76533},{\"end\":76547,\"start\":76541},{\"end\":76557,\"start\":76551},{\"end\":76567,\"start\":76561},{\"end\":76583,\"start\":76571},{\"end\":76594,\"start\":76587},{\"end\":76602,\"start\":76598},{\"end\":76609,\"start\":76606},{\"end\":76621,\"start\":76613},{\"end\":76630,\"start\":76623},{\"end\":77171,\"start\":77168},{\"end\":77179,\"start\":77175},{\"end\":77187,\"start\":77183},{\"end\":77196,\"start\":77191},{\"end\":77205,\"start\":77200},{\"end\":77219,\"start\":77211},{\"end\":77817,\"start\":77814},{\"end\":77823,\"start\":77821},{\"end\":77831,\"start\":77827},{\"end\":77838,\"start\":77835},{\"end\":77846,\"start\":77842},{\"end\":77854,\"start\":77850},{\"end\":78422,\"start\":78416},{\"end\":78431,\"start\":78426},{\"end\":78853,\"start\":78843},{\"end\":78865,\"start\":78859},{\"end\":78877,\"start\":78869},{\"end\":78887,\"start\":78881},{\"end\":78895,\"start\":78891},{\"end\":79290,\"start\":79287},{\"end\":79298,\"start\":79294},{\"end\":79305,\"start\":79302},{\"end\":79311,\"start\":79309},{\"end\":79319,\"start\":79315},{\"end\":79521,\"start\":79516},{\"end\":79529,\"start\":79525},{\"end\":79537,\"start\":79535},{\"end\":79549,\"start\":79543},{\"end\":79558,\"start\":79553},{\"end\":79997,\"start\":79994},{\"end\":80003,\"start\":80001},{\"end\":80016,\"start\":80009}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":17741789},\"end\":72612,\"start\":72354},{\"attributes\":{\"id\":\"b1\"},\"end\":72812,\"start\":72614},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":227323668},\"end\":73139,\"start\":72814},{\"attributes\":{\"id\":\"b3\"},\"end\":73437,\"start\":73141},{\"attributes\":{\"id\":\"b4\"},\"end\":73487,\"start\":73439},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":222067132},\"end\":74101,\"start\":73489},{\"attributes\":{\"doi\":\"abs/1412.3555\",\"id\":\"b6\"},\"end\":74394,\"start\":74103},{\"attributes\":{\"id\":\"b7\"},\"end\":74680,\"start\":74396},{\"attributes\":{\"doi\":\"abs/2106.01540\",\"id\":\"b8\"},\"end\":74903,\"start\":74682},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":18233038},\"end\":75335,\"start\":74905},{\"attributes\":{\"id\":\"b10\"},\"end\":75842,\"start\":75337},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":14650762},\"end\":76346,\"start\":75844},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":202786778},\"end\":77082,\"start\":76348},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":4564012},\"end\":77752,\"start\":77084},{\"attributes\":{\"doi\":\"16-20 Novem- ber 2020\",\"id\":\"b14\",\"matched_paper_id\":207847640},\"end\":78363,\"start\":77754},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":877929},\"end\":78784,\"start\":78365},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":53991169},\"end\":79234,\"start\":78786},{\"attributes\":{\"doi\":\"abs/2107.00645\",\"id\":\"b17\"},\"end\":79464,\"start\":79236},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":198897717},\"end\":79886,\"start\":79466},{\"attributes\":{\"id\":\"b19\"},\"end\":80389,\"start\":79888}]", "bib_title": "[{\"end\":72426,\"start\":72354},{\"end\":72861,\"start\":72814},{\"end\":73525,\"start\":73489},{\"end\":74431,\"start\":74396},{\"end\":74957,\"start\":74905},{\"end\":75410,\"start\":75337},{\"end\":75899,\"start\":75844},{\"end\":76407,\"start\":76348},{\"end\":77164,\"start\":77084},{\"end\":77810,\"start\":77754},{\"end\":78412,\"start\":78365},{\"end\":78837,\"start\":78786},{\"end\":79510,\"start\":79466},{\"end\":79990,\"start\":79888}]", "bib_author": "[{\"end\":72441,\"start\":72428},{\"end\":72454,\"start\":72441},{\"end\":72667,\"start\":72656},{\"end\":72679,\"start\":72667},{\"end\":72688,\"start\":72679},{\"end\":72700,\"start\":72688},{\"end\":72874,\"start\":72863},{\"end\":72887,\"start\":72874},{\"end\":73258,\"start\":73247},{\"end\":73270,\"start\":73258},{\"end\":73455,\"start\":73441},{\"end\":73464,\"start\":73455},{\"end\":73544,\"start\":73527},{\"end\":73561,\"start\":73544},{\"end\":73570,\"start\":73561},{\"end\":73578,\"start\":73570},{\"end\":73586,\"start\":73578},{\"end\":73596,\"start\":73586},{\"end\":73607,\"start\":73596},{\"end\":73618,\"start\":73607},{\"end\":73631,\"start\":73618},{\"end\":73641,\"start\":73631},{\"end\":73655,\"start\":73641},{\"end\":73668,\"start\":73655},{\"end\":73678,\"start\":73668},{\"end\":74190,\"start\":74181},{\"end\":74202,\"start\":74190},{\"end\":74209,\"start\":74202},{\"end\":74219,\"start\":74209},{\"end\":74448,\"start\":74433},{\"end\":74463,\"start\":74448},{\"end\":74474,\"start\":74463},{\"end\":74488,\"start\":74474},{\"end\":74690,\"start\":74684},{\"end\":74698,\"start\":74690},{\"end\":74706,\"start\":74698},{\"end\":74714,\"start\":74706},{\"end\":74721,\"start\":74714},{\"end\":74727,\"start\":74721},{\"end\":74742,\"start\":74727},{\"end\":74748,\"start\":74742},{\"end\":74970,\"start\":74959},{\"end\":74980,\"start\":74970},{\"end\":74989,\"start\":74980},{\"end\":75426,\"start\":75412},{\"end\":75436,\"start\":75426},{\"end\":75448,\"start\":75436},{\"end\":75458,\"start\":75448},{\"end\":75467,\"start\":75458},{\"end\":75912,\"start\":75901},{\"end\":75923,\"start\":75912},{\"end\":75933,\"start\":75923},{\"end\":76419,\"start\":76409},{\"end\":76428,\"start\":76419},{\"end\":76437,\"start\":76428},{\"end\":76446,\"start\":76437},{\"end\":76458,\"start\":76446},{\"end\":76468,\"start\":76458},{\"end\":76479,\"start\":76468},{\"end\":76486,\"start\":76479},{\"end\":76500,\"start\":76486},{\"end\":76510,\"start\":76500},{\"end\":76523,\"start\":76510},{\"end\":76531,\"start\":76523},{\"end\":76539,\"start\":76531},{\"end\":76549,\"start\":76539},{\"end\":76559,\"start\":76549},{\"end\":76569,\"start\":76559},{\"end\":76585,\"start\":76569},{\"end\":76596,\"start\":76585},{\"end\":76604,\"start\":76596},{\"end\":76611,\"start\":76604},{\"end\":76623,\"start\":76611},{\"end\":76632,\"start\":76623},{\"end\":77173,\"start\":77166},{\"end\":77181,\"start\":77173},{\"end\":77189,\"start\":77181},{\"end\":77198,\"start\":77189},{\"end\":77207,\"start\":77198},{\"end\":77221,\"start\":77207},{\"end\":77819,\"start\":77812},{\"end\":77825,\"start\":77819},{\"end\":77833,\"start\":77825},{\"end\":77840,\"start\":77833},{\"end\":77848,\"start\":77840},{\"end\":77856,\"start\":77848},{\"end\":78424,\"start\":78414},{\"end\":78433,\"start\":78424},{\"end\":78855,\"start\":78839},{\"end\":78867,\"start\":78855},{\"end\":78879,\"start\":78867},{\"end\":78889,\"start\":78879},{\"end\":78897,\"start\":78889},{\"end\":78912,\"start\":78897},{\"end\":78916,\"start\":78912},{\"end\":79292,\"start\":79285},{\"end\":79300,\"start\":79292},{\"end\":79307,\"start\":79300},{\"end\":79313,\"start\":79307},{\"end\":79321,\"start\":79313},{\"end\":79523,\"start\":79512},{\"end\":79531,\"start\":79523},{\"end\":79539,\"start\":79531},{\"end\":79551,\"start\":79539},{\"end\":79560,\"start\":79551},{\"end\":79999,\"start\":79992},{\"end\":80005,\"start\":79999},{\"end\":80018,\"start\":80005}]", "bib_venue": "[{\"end\":73765,\"start\":73758},{\"end\":75071,\"start\":75054},{\"end\":75610,\"start\":75547},{\"end\":76094,\"start\":76014},{\"end\":77422,\"start\":77320},{\"end\":79642,\"start\":79621},{\"end\":80115,\"start\":80094},{\"end\":72465,\"start\":72454},{\"end\":72654,\"start\":72614},{\"end\":72958,\"start\":72887},{\"end\":73245,\"start\":73141},{\"end\":73756,\"start\":73678},{\"end\":74179,\"start\":74103},{\"end\":74518,\"start\":74488},{\"end\":75052,\"start\":74989},{\"end\":75545,\"start\":75467},{\"end\":76012,\"start\":75933},{\"end\":76681,\"start\":76632},{\"end\":77318,\"start\":77221},{\"end\":77960,\"start\":77877},{\"end\":78482,\"start\":78433},{\"end\":78965,\"start\":78916},{\"end\":79283,\"start\":79236},{\"end\":79619,\"start\":79560},{\"end\":80092,\"start\":80018}]"}}}, "year": 2023, "month": 12, "day": 17}