{"id": 243582411, "updated": "2023-10-18 15:39:36.886", "metadata": {"title": "A methodology for procedural piano music composition with mood templates using genetic algorithms", "authors": "[{\"first\":\"L.\",\"last\":\"Rocha de Azevedo Santos\",\"middle\":[]},{\"first\":\"C.\",\"last\":\"Silla Jr.\",\"middle\":[\"N.\"]},{\"first\":\"M.\",\"last\":\"Costa-Abreu\",\"middle\":[\"D.\"]}]", "venue": "11th International Conference of Pattern Recognition Systems (ICPRS 2021)", "journal": "11th International Conference of Pattern Recognition Systems (ICPRS 2021)", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "Creating music in an automatic way has been studied since the beginning of artificial intelligence. One of the biggest obstacles of music generation is the vagueness and subjectivity of the mood or emotion transmitted by a music piece. In this work, we experiment with the generation of piano music using template pieces, represented in MIDI format, as a mood directive. We generated a population of random pieces for templates of two opposing moods - happy and sad - and evolved them with a genetic algorithm until their intended mood was close enough to their respective templates. The fitness function that we implemented uses MIDI statistical features to calculate the distance between the given piece and the template. The generated music pieces were evaluated by human listeners thorough a questionnaire. This evaluation has shown that the generated music pieces were able to express the same mood as the template. However, they still sounded computer-generated, probably due to the lack of rhythm regularity and synchronicity.", "fields_of_study": null, "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": null, "doi": "10.1049/icp.2021.1435"}}, "content": {"source": {"pdf_hash": "f152c56f58c1769a3061ba7a85210261a7ce0831", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://shura.shu.ac.uk/28011/1/paper_music_generator_IA.pdf", "status": "GREEN"}}, "grobid": {"id": "5298dc6a6419fcad51f0ae379768e2cb8fbafc20", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/f152c56f58c1769a3061ba7a85210261a7ce0831.txt", "contents": "\nA methodology for procedural piano music composition with mood templates using genetic algorithms\n\n\nLu\u00edsa Rocha \nAzevedo Santos \nFederal University of Rio Grande do Norte\nNatalBR\n\nCarlos Nascimento \nPUC-PR\nCuritibaPRBrazil\n\nSilla Jr\nM\u00e1rjory Da Costa-Abreu \nDepartment of Computing\nSheffield Hallam University\nSheffieldU.K\n\nA methodology for procedural piano music composition with mood templates using genetic algorithms\nmusic compositionmusic emotionmusic moodgenetic algorithmprocedural content generation\nCreating music in an automatic way has been studied since the beginning of artificial intelligence. One of the biggest obstacles of music generation is the vagueness and subjectivity of the mood or emotion transmitted by a music piece. In this work, we experiment with the generation of piano music using template pieces, represented in MIDI format, as a mood directive. We generated a population of random pieces for templates of two opposing moodshappy and sad -and evolved them with a genetic algorithm until their intended mood was close enough to their respective templates. The fitness function that we implemented uses MIDI statistical features to calculate the distance between the given piece and the template. The generated music pieces were evaluated by human listeners thorough a questionnaire. This evaluation has shown that the generated music pieces were able to express the same mood as the template. However, they still sounded computer-generated, probably due to the lack of rhythm regularity and synchronicity.\n\nIntroduction\n\nThe area of music generation has been increasingly explored throughout the years [9,15]. The approaches vary from Markov models or hidden Markov models [22], to neural networks [1], evolutionary algorithms [20], and several other methods [12].\n\nTraditionally, however, music generation researches focus only on generating good music, without considering that whoever uses the system may want to use the music for a specific situation, so they might need it to transmit a particular feeling. Examples of this are the incidental (background) music used in games [23] and movies [2].\n\nIt was observed that proprieties in music such as scale modes, presence of dissonance, melody motion and rhythm consistency can significantly influence the mood of a musical piece [8], and that this mood can affect the listener's emotions in many ways [11].\n\nKeeping that in mind, we decided to study the generation of music with a mood directive. We will experiment with musical properties to see how they can influence the mood of the final piece.\n\nThe goal of this work is to design a system that automatically generates a musical piece, given a template piece as mood directive, without requiring in-depth knowledge about music theory and composition. This template piece may be any composition that suggests the intended mood and respects the system's limitations. This composition will be represented in MIDI [7] -Musical Instrument Digital Interface: a widely used file format which can be used both to play music and to represent it as a musical score [10].\n\nThe generated music will have two parts: the melody, a sequence of individual notes played one at a time that gives identity to the piece; and the harmony, the background chords that accompany the melody.\n\nA template piece with these two parts will be provided to the system. After analyzing the piece and extracting the most important features, the system will generate another -unique -piece, also with these two parts, with mood that is as similar as possible to the mood of the original template.\n\nThe method we will experiment with is the genetic algorithm [5] to evolve an initial population of randomly generated pieces. The fitness function that we will use for the algorithm is a similarity function that compares the mood of the generated piece with the mood of the given piece, using an euclidean distance calculated from musical features. This way, a unique piece should be generated since the starting population is also unique, but the mood should still be similar to the template if it is well represented in the fitness function.\n\nIn the next sections, we will briefly present the musical terminology used in this work and some of the previous works that are related to ours (Section 2). Next, in Section 3, we will describe our modelling for this problem, including genetic operations and representation of a piece. In Section 4, we will present the results of the tests that we executed. Lastly, in Section 5, we will summarise the results and propose future works.\n\nIn our work, we will use the terminology defined by Crotch [3], where he defines the basis of modern musical theory including notes, pitch, interval, octave, scale and others.\n\nMcauley [17] described the time signature of a piece as the specifier of two numbers: the first, the numerator, is the size of a beat and the second, the denominator, is the number of beats per measure. The reference time unit used in this work for chords duration. The tempo of a piece is the speed in which the notes are playedindicated by the number of beats per minute (BPM).\n\nHarmony is the sound that accompany the foreground melody in a musical piece. Crotch [3] defines harmony as a sequence of chords, where each chord is a set of notes played at the same time or within a limited interval of time. A sequence of chords is also called a chord progression [21].\n\nThe notes in a chord may be played in an arpeggio, in which each note is played one after another instead of at the same time [21].\n\nWe will need a method to compute the distance between two music pieces. In this section we review the works that model and evaluate some possible theoretical features to describe a music piece.\n\nThere are several ways to extract features from songs, depending of the type of data we have available. In this work, we are using MIDI as a representation and for this reason we are interested in works that deal with symbolic music representation. Meanwhile, [6] also proposed some metrics to classify music. The accuracy varied from 70.66% (12 classes) to 87.75% (2 classes).\n\nThe fitness function and generic operations varied a lot in works that used genetic algorithm for music generation. The works [27,28] used the judgment of a human listener to evaluate the individuals. A very common fitness function is a combination of rule-based objectives [4,16,20,25,26]. Another common fitness function is a classifier trained with human-made examples [13,14,19].\n\nThe works [16,26], like ours, also use a template piece as a guide to the generated pieces, but differ from ours in the representation of a piece and in the genetic operators. Both works represented piece as a string of numbers, while in our it is represented with an abstract model. For the genetic operations, the work [26] used standard crossover and mutation operations, while the work [16] used mutation that altered the notes' pitches and no crossover operator.\n\nIn our work, the mutation operations include other variations based on musical theory, and the crossover operator that we use also works in a different way: instead of the standard cut-and-swap crossover, we combine musical parts from three different parents.\n\n\nModelling\n\nTo reduce the scope of our problem, we limited the music pieces (both input and output) to: using only diatonic scales; a single track for the melody (monophonic); a single track for the harmonic chords; no percussion (drums, cymbals, etc.) tracks; piano as the only possible instrument; only one chord per measure; pieces with four measures; a single arpeggio pattern for all four measures. Also, we will not analyze patterns, repetitions or overall high-level musical structure, and instead we will focus on generating a single motif, long enough to communicate a feeling.\n\nInstead of using typical strings of numbers as an individual, here we use an abstract hierarchical model to represent a music piece, which will be an individual in the population. This way, the application of the genetic operators are more intuitive, since they modify abstract musical properties instead of individual numbers.\n\nIn this model, a piece is as a tuple that contains: the length of the piece in measures (integer value); the time signature (two integer values, numerator and denominator); the BPM (float value); the piece's scale; the foreground melody; and the background harmony. Except for the melody and harmony parts, all the other information is obtained directly from the MIDI file's header. The last three fields are described in the next subsections.\n\nA scale is a pair consisting of its root pitch, varying from 0 to 11, and its mode, varying from 0 to 6. It is important to note that a scale is represented by displacements from the cannon scale, C major. The mode value is the number of times the Major mode's pattern 2-2-1-2-2-2-1 is displaced by one. That being said, each mode is represented as: 0 -Ionian (Major); 1 -Dorian; 2 -Phrygian; 3 -Lydian; 4 -Mixolydian; 5 -Aeolian (minor); and 6 -Locrian.\n\nA pitch, which in the MIDI file is represented by an integer from 0 to 127, interpreted as the absolute sound frequency. Here, it is converted to a pitch relative to its scale, represented by: the pitch's degree, a number from 0 to 6, which is the pitch's position relative to the scale; the pitch's accidental (0 is natural, -1 is flat ( ) and 1 is sharp ( )); and the pitch's octave, relative to the scale's root.\n\nThe melody is a set of notes, each one defined by: the pitch being played; the time in beats in which this note is played (also known as attack); and the duration in beats of the note.\n\nThe harmony is a pair containing a sequence of chords and the arpeggio pattern, a set of notes just like the melody, used for the chords. Each chord is represented by a pitch, which is the chord's tonic relative to piece's scale (in this work, we use the lowest pitch in the chord as its tonic). The tonic is then used to generate a scale, which is the base scale for the arpeggio's notes.\n\nIn our proposed genetic algorithm, each initial individual will have the same length of the target piece, the same number of notes in the melody and the same notes in the arpeggio pattern. The generation of a random piece follows the steps below:\n\n1. The scale, BPM and tempo will be the same as the target's. The arpeggio structure -notes played relative to the chord's root -were copied from the template.\n\n2. The start of each note is chosen by a random number from the start of the piece to the end of it. The number of notes in the piece will be the same as the number of notes in the template.\n\n3. The pitch and the duration of each note are uniformly generated from the range in the original piece -from the shortest to the longest duration, and from the lowest to the highest pitch.\n\nAfter that, the musical features of each individual are calculated and stored in a vector. This vector is used to calculate the individual's proximity to the template, which is an Euclidean distance between the piece's feature vector and the template's feature vector.\n\nFor the features, we selected some of the features proposed by [18] that are applicable to our problem restrictions. The complete list of features that we implemented is detailed in [24]. They are separated by classes: 10 rhythm features, 14 pitch features, 13 note features and 15 interval features. The rhythm features are calculated once, the pitch and notes features twice each, and the interval features three times, totalizing 109 features.\n\nThe final fitness of a piece p given the template t is\ndescribed as: f = 109 i=1 (\u03b1 i * p i \u2212 \u03b1 i * t i ) 2 , where each \u03b1 i\nis the weight given to the feature i; p i is the feature i extracted from piece p; and t i is the feature i extracted from the template. We did not compute the square root of the sum because the exact value of the distance was not used; the summation is calculated only to compare and sort individuals by distance. Next, the population is sort by fitness, and a fraction of the population with the fittest individuals (the elite) is separated. The remaining individuals are replaced by new ones in the next generation, created from a crossover of elite individuals.\n\nFor the mutation, we defined three sets of mutation operations. Each mutation operation is applied independently, according to its rate.\n\nThe first set of mutation operations are applied to the whole piece at once, since it changes the scale and rhythm signatures. These operations are Change mode which changes the scale mode, keeping the root key and the degrees of each note in the melody, using a random number from 0 to 6; and Change BPM: changes a new BPM value, from 50% to 150% of the current BPM.\n\nThis set of operations is applied to the notes in the melody are Change start time which moves the note start time, using a random float between the end of the previous and the next notes in the melody; and Change duration, which changes the note end time, using a random float from the end of the previous note to the start of the next note in the melody.\n\nThis set is applied to a set of pitches -both to the melody and the chords' tonics. These operations are: Change pitch degree that changes scale degree of the pitch, decreasing or increasing it by one scale step; Change pitch accidental that changes the accidental of the pitch, increasing or decreasing the pitch by a half tone (one semitone); and Change octave which increases or decreases all pitches by one octave.\n\nThere is a single crossover operation, which will occur with three selected individuals: the first one will provide the signature (scale and tempo), the second one will provide the melody notes and the third one will provide the chords. These three individuals are selected uniformly from the set of elite individuals. To separate the elite set, the population is sorted by fitness, and the n fittest individuals are selected as the elite.\n\n\nExperiments and Results\n\nDuring this section, we will indicate links to our music showcase site, where reader can listen to the mentioned pieces, visualize their music sheets and download the MIDI files. We implemented our system using a Java library called jMusic. The code is published on GitHub. For all tests, we used four templates that were hand-made by us, two \"happy\" pieces and two \"sad\".\n\nAll the experiments were executed with a population size of 60, and an elite size of 15. Due to time limitations, we were not able to test variations of these values.\n\n\nHuman Evaluation\n\nWe published a questionnaire via Google Forms showing 4 templates, two \"happy\" ones and two \"sad\" ones, and also 2 procedurally generated pieces based on each template, one using the random seed 0 1 and other using seed 1 2 , totalizing 12 pieces. We selected the fittest pieces from the 4000 th generation. For each piece -either a template or a generated piece -, we asked the participants to: (1) Rank the mood transmitted by the music piece, from 1 (sad) to 5 (happy); and (2) Evaluate how they thought the piece was composed, being 1 (certainly by a computer) to 5 (certainly by a human). In total 40 different people answered: 20 with no music theory background, with 8 a basic background, 9 intermediary and 3 advanced.\n\nIn general, the generated pieces transmit a mood similar to the template it is based on. The pieces generated from Happy2 and Sad2 were pretty close to the intended mood, while with Happy1 they sounded slightly sadder, and with Sad1 they sounded slightly happier.\n\nIn the case of the Happy1 pieces, we believe that the chords in the harmony contributed for the sadder mood. Both random seeds generated the same chord progression (vi-I-vi-IV), but this progression was not the same as the one in the template (I-vi-IV-V). The first progression may sound sadder because it contains two minor chords (vi), while the second one only contains one. Besides this, there may be something in the melody intervals that was not captured by the features we calculate. The irregular rhythm could also have contributed to the \"weirdness\" in the piece, which disturbs the happy mood.\n\nIn the case of the Sad1 pieces, both pieces had the same chord progression as the template, so it was not responsible for the mood difference. The generated pieces have a little more note pitch variety, which usually makes music sound happier, so this could have been the reason. This, and the fact that the notes in the template are longer than the notes in the generated pieces, and longer notes usually sound sadder. Just like the Happy1 pieces, it could also have been some interval information that was not taken into account.\n\nThe listeners seemed to prefer the template pieces. Among the templates, Happy2 was the only one that sounded more like a computer-generated piece then like a human-made one, and one of the pieces generated from it sounded more natural than the original. It has likely happened because the beginning of the piece sounded slightly dissonant.\n\nIt was reported by one of the people who answered the questionnaire that sometimes the procedurally made pieces could be identified by its lack of regularity and melody-harmony synchrony in the rhythm, so this problem may be investigated in future works to make the pieces sound more natural.\n\nHowever, according to the results obtained from the questionnaire, the piece with the smaller distance did not necessarily perform the best to the human ear. This suggests that the weights in the fitness function could still be improved.\n\n\nFeature Weights\n\nAfter the results of the questionnaire, we decided to test our fitness function separately. The initial weights were chosen subjectively between a few combinations tested. Some features could be normalized, but some could not, so we had to manually try different configurations. What piece actually sounds better is really a subjective matter, so we did not try to find exact values for the weights.\n\nFor the initial tests, these weights were good enough to evolve the population. Comparing the fittest individuals from generations 1, 100, 1,000 and 4,000. We used the template Sad1 to demonstrate it since it was the one that showed the largest differences from one generation to another. From generation 1,000 to 4,000 it did not show a great improvement, but it is clear that from 1 to 100 the fittest individual was a lot different, sounding sadder.\n\nFrom 100 to 1,000 it improved a little the intervals and note durations.\n\nAfter the results from the human evaluation, we compared the performance of two versions of the fitness function: the one with the weights we proposed and the one with all weights equal to 1. For this, we only generated the initial population, without evolving it, and sorted it by fitness using these weights.\n\nWe first compared these two weight configurations using the template Happy1. For each one, we uploaded a pair of pieces, the best and worst. Being the first generation, it is expected that neither of the pieces are satisfactory results, but we can still compare the best with the worst of each configuration.\n\nBoth configurations selected reasonable pieces for the fittest. The weighted fittest sounds a little less random than the non weighted fittest, probably because of the biggest weights given for the attack variation. The most noticeable difference, however, is between the two least fit pieces: the weighted one sounded way more dissonant because of the high weights given for accidentals and dissonant intervals, which means that the dissonant pieces are way more penalized than the more harmonious ones.\n\nHowever, our next test shows that this fitness function might be too \"afraid\" of selecting individuals with a high incidence of accidentals and dissonance. Testing with pieces generated with template Sad1 the non-weighted version was able to select better the pieces that sounded that most similar to the template -that is, it performed better for the mood.\n\nThis demonstrated one of the most difficult parts for us in choosing the weights: the balance between the importance of sounding natural and the importance of transmitting the correct mood. The higher weight given to attack variation and the accidental incidence were chosen to avoid dissonance and randomity in the rhythm, both elements that sound unpleasant and unnatural to the human ear. However, increasing these weights sometimes makes the algorithm select pieces that do not have the right mood. A possible solution to this is to separate the generation of the melody, which will be focused on the pleasantness of is sound, from the modification until it sounds like the template. Another possible improvement is to remove the weight given to dissonance and prevent its occurrence by other means, like conditional mutation.\n\nIn summary, these results suggest that the fitness function that we initially proposed may work for some types of template better than others. Different weight configurations for the feature classes could be tested and evaluated by other questionnaires in the future.\n\n\nMutation Rate\n\nNow we will discuss the effects of the mutation rates in the final results and the convergence of the individuals. We noticed a great importance of the signature mutations for the final mood. Even with static melody notes and harmony chords, just changing the scale and the BPM of the piece significantly change the mood, sometimes even making the melody unrecognizable.\n\nInstead of random individuals, we populated the system using clones of one of our hand-made templates, Happy1. With only these two mutation operations, both with the weights 0.25, we evolved it to the 100 th generation, using as a template the Sad1 music piece to make it sound sad, and compared it to the original Happy1 piece. The modified version sounds way slower, resulting from the BPM mutation, and more dissonant, resulting from the scale mode mutation.\n\nAs expected, note features did not appear in the list, since they were kept the same in the modified version. The biggest differences were in the intervals. The Sad1 piece has a lot more chromatic motion than Happy1 piece, as well as other dissonant intervals, which helps the piece sound more unsettling. Note density were the only rhythm feature in the list, but had a really important role in the mood, since it was what made the piece slower.\n\nThe inverse was also tested and we saw the biggest differences between the original Sad1 piece and the fittest piece from the 100 th generation using Happy1 as template. In this case, the final piece did not sound as happy as the template, but it was probably because of the lack of harmony mutation, since the falling progression in the sad piece did not change and has a significant influence in the mood. However, it is noticeable that the modified version sounds happier than the original. It sounds faster, because of the higher note density, and brighter, because of the lower dissonance ratio and higher presence of stepwise motion and other harmonic intervals.\n\nIn both cases, we can see some details that could still be improved, like the excessively higher ratio of tritones -an interval that sounds dissonant -, which could have happened because of the lack of mutation in chords' tonics and melody pitches.\n\nNext, we defined initial melody and harmony note mutation rates, and also the melody rhythm mutation rates (for change attack and duration, both, with 0.025). Next, we evolved the population that were created without a base piece for the melody, that is, randomly, through the method described in Section ??. We executed the same tests as the previous subsection.\n\nThe modified Happy1 version did sound a little sadder than the original, but, compared to the one generated with the signature mutation, this one is happier. This is probably because of the higher note density, which did not appear in the table, since it did not change. The falling motion in the chords and the more dissonant intervals were the most responsible for the sadder mood in the modified version in this case.\n\nThe modified Sad1 version, however, still sounded too sad. Since it could not change the note density, it would always sound too slow.\n\nThe features that changed the most are basically the same in both cases, but the qualities of the results were very different. We believe that a greater number of occurrences of the tonic note would also make piece sound happier, since the tonic note does occur more in the Happy1 template.\n\nWe noticed, however, that the distances from these two pieces to their templates where smaller than the distances from the pieces generated with the signature mutation, despite the latter ones sounding better. This is another indicative that the weights given for the features are not accurate.\n\nWith our initial mutation rates, we plotted the distance of the best individuals from generations 1, 100, 1000, and 4000. The initial distance is very high and variable in the first generation since it depends entirely on the random seed. However, all pieces improved extremely fast from the first generation to the 100 th , to less than 10,000, no matter how unfit was the first generation. From generation 100 to 1000, all of them improved significantly, but some more than others. And from 1000 to 4000, some of them still improved, but some had already converged, like Happy2 (seed 1) and Sad2 (seed 0).\n\nExcept for Sad2 (seed 1), which reached a distance way smaller than the others, all pieces have reached a similar distance in generation 4000, which means that the mutation rates are probably generic enough to work for both happy and sad moods with little dependence on the random seed.\n\nLastly, we tried to vary a little the mutation rates from the initial parameters described in the previous subsections, to see if the populations would evolve faster. We used the template Happy1 and evolved until generation 4000, using random seed 0. The distances are showed in Table 1. From all the combinations, the best results seemed to be the ones with a lower signature and higher note duration mutation rates. We believe that this happened because a change in the signature can drastically change the mood, for better but also for worse, so many potentially good pieces may be ruined by a wrong signature. Also, a higher duration mutation rate could have helped to find fitter individuals because the initial population has all notes with full duration and, since the Happy1 has some silent time in the melody, the initial rate of 0.025 could have made it too difficult to generate a piece with better note duration.\n\nThe combination that performed the worst was the change in the melody note accidental. This is expected, since it was given a high weight to the accidental feature. The only time this mutation is useful is when the template has an accidental: after the piece gets the same number of accidental notes as the template, this mutation ideally should never happen again. If this rate is too high, though, there is a high chance that some note in the melody receives an accidental, which would increase its distance a lot.\n\nThese results suggest that the mutation rates should be defined according to the feature weights. For this reason, a conditional mutation -such as increasing the accidental mutation rate when the difference in accidental ratio is high, and decreasing it when the distance is lowcould help the population to converge faster.\n\n\nConclusions\n\nOur main objective with this project was to provide a way to generate music automatically with some kind of directive for the mood expressed by the piece. Therefore, we proposed a system where the user can provide a template piece to direct the mood of the piece she or he needs, and then unique pieces would be generated by the system according to this mood.\n\nWe modelled a musical piece as an individual from a population evolved with a genetic algorithm. The results of our experiments demonstrated to be relatively effective in its original goal, which is to generate pieces according to the given mood template. We noticed that more weight on the dissonance features helped to select the most pleasing pieces, but not the most fitted for the mood described by the template.\n\nMoreover, the fittest piece was not always the most natural-sounding either. The responses showed that, in general, the generated pieces sound more artificial than the hand-made ones. We made a few tests varying the mutation rates to see if they could generate fitter individuals in the same number of generations.\n\n\nTable 1. Distances of the fittest individuals to the template, for each mutation rate variation.Mutation \nNew Value \nFittest Distance \n-\n-\n282.76 \nSignature (scale mode and BPM) \n0.0005 \n88.56 \nSignature (scale mode and BPM) \n0.025 \n228.43 \nMelody note attack \n0.25 \n158.29 \nMelody note duration \n0.25 \n73.80 \nMelody note degree \n0.025 \n144.66 \nMelody note degree \n0.25 \n1890.33 \nMelody note accidental \n0.25 \n13772.98 \nMelody note octave \n0.025 \n222.84 \nMelody note octave \n0.25 \n297.79 \nChord tonic degree \n0.025 \n111.67 \nChord tonic degree \n0.4 \n569.49 \nChord tonic accidental \n0.25 \n183.77 \nChord tonic octave \n0.025 \n222.84 \nChord tonic octave \n0.4 \n297.79 \n\n\nhttps://luisaras.github.io/piano-repo/#seed0 2 https://luisaras.github.io/piano-repo/#seed1\n\nGeneration of composed musical structures through recurrent neural networks based on chaotic inspiration. A E Coca, R A F Romero, L Zhao, The 2011 International Joint Conference on. IEEENeural Networks (IJCNN)A.E. Coca, R.A.F. Romero, and L. Zhao. Generation of com- posed musical structures through recurrent neural networks based on chaotic inspiration. In Neural Networks (IJCNN), The 2011 International Joint Conference on, pages 3220-3226. IEEE, 2011.\n\nMusic as a source of emotion in film. A J Cohen, A.J. Cohen. Music as a source of emotion in film, 2011.\n\nElements of musical composition. W Crotch, 1812W. Crotch. Elements of musical composition, 1812.\n\nEvolving four-part harmony using genetic algorithms. P Donnelly, J Sheppard, European Conference on the Applications of Evolutionary Computation. SpringerP. Donnelly and J. Sheppard. Evolving four-part harmony using genetic algorithms. In European Conference on the Applications of Evolutionary Computation, pages 273-282. Springer, 2011.\n\nArtificial intelligence through simulated evolution. L J Fogel, A J Owens, M J Walsh, L.J. Fogel, A.J. Owens, and M.J. Walsh. Artificial intelligence through simulated evolution, 1966.\n\nDetecting emotions in classical music from midi files. J Grekow, Z W Ras, International Symposium on Methodologies for Intelligent Systems. SpringerJ. Grekow and Z.W. Ras. Detecting emotions in classical music from midi files. In International Symposium on Methodologies for Intelligent Systems, pages 261-270. Springer, 2009.\n\nMidi power! the comprehensive guide. R Guerin, R. Guerin. Midi power! the comprehensive guide, 2010.\n\nExperimental studies of the elements of expression in music. K Hevner, The American Journal of Psychology. 482K. Hevner. Experimental studies of the elements of expression in music. The American Journal of Psychology, 48(2):246-268, 1936.\n\nExperimental music: composition with an electronic computer. L A Hiller, L M Isaacson, Hiller, L.A. and Isaacson, L.M. Experimental music: composition with an electronic computer, 1959.\n\nMidifind: fast and effective similarity searching in large midi databases. T Huang, G Xia, Y Ma, R Dannenberg, C Faloutsos, Proceedings of the 10th International Symposium on Computer Music Multidisciplinary Research. the 10th International Symposium on Computer Music Multidisciplinary ResearchT. Huang, G. Xia, Y. Ma, R. Dannenberg, and C. Faloutsos. Mid- ifind: fast and effective similarity searching in large midi databases. In Proceedings of the 10th International Symposium on Computer Music Multidisciplinary Research, pages 209-224, 2013.\n\nMusic: A link between cognition and emotion. Current directions in psychological science. C L Krumhansl, 11C.L. Krumhansl. Music: A link between cognition and emotion. Current directions in psychological science, 11(2):45-50, 2002.\n\nMidinet: A convolutional generative adversarial network for symbolic-domain music generation. Y Lichia, C Szuyu, Y Yihsuan, Y. LiChia, C. SzuYu, and Y. YiHsuan. Midinet: A convolutional generative adversarial network for symbolic-domain music genera- tion, 2017.\n\nEvolving musical sequences with n-gram based trainable fitness functions. M Y Lo, S M Lucas, IEEE Congress on evolutionary computation. M.Y. Lo and S.M. Lucas. Evolving musical sequences with n-gram based trainable fitness functions. In IEEE Congress on evolution- ary computation, pages 601-608, 2006.\n\nA corpus-based hybrid approach to music analysis and composition. B Manaris, P Roos, P Machado, D Krehbiel, L Pellicoro, J Romero, Proceedings of the National Conference on Artificial Intelligence. the National Conference on Artificial IntelligenceMenlo Park, CA; Cambridge, MA; LondonMIT Press22839B. Manaris, P. Roos, P. Machado, D. Krehbiel, L. Pellicoro, and J. Romero. A corpus-based hybrid approach to music analysis and composition. In Proceedings of the National Conference on Arti- ficial Intelligence, volume 22:1, page 839. Menlo Park, CA; Cam- bridge, MA; London; AAAI Press; MIT Press; 1999, 2007.\n\nDeepj: Style-specific music generation. H H Mao, T Shin, G Cottrell, IEEE 12th International Conference on Semantic Computing (ICSC). H.H. Mao, T. Shin, and G. Cottrell. Deepj: Style-specific music generation. In 2018 IEEE 12th International Conference on Se- mantic Computing (ICSC), pages 377-382, Jan 2018.\n\nA genetic algorithm for composing music. D Matic, Yugoslav Journal of Operations Research. 201D. Matic. A genetic algorithm for composing music. Yugoslav Journal of Operations Research, 20(1):157-177, 2010.\n\nTempo and rhythm. Music Perception. J Mcauley, J Mcauley. Tempo and rhythm. Music Perception, pages 165-199, 08 2010.\n\nAutomatic genre classification of MIDI recordings. C Mckay, McGill University CanadaPhD thesisC. McKay. Automatic genre classification of MIDI recordings. PhD thesis, McGill University Canada, 2004.\n\nUsing recurrent neural networks to judge fitness in musical genetic algorithms. P Mitrano, A Lockman, J Honicker, S Barton, P. Mitrano, A. Lockman, J. Honicker, and S. Barton. Using recur- rent neural networks to judge fitness in musical genetic algorithms, 06 2017.\n\nA genetic algorithm for generating improvised music. E Ozcan, T , International Conference on Artificial Evolution (Evolution Artificielle). SpringerE. Ozcan and T. Ercal. A genetic algorithm for generating impro- vised music. In International Conference on Artificial Evolution (Evolution Artificielle), pages 266-277. Springer, 2007.\n\nScales, chords, arpeggios and cadences. W A Palmer, W.A. Palmer. Scales, chords, arpeggios and cadences, 1994.\n\nStatistical learning of harmonic movement. D Ponsford, G Wiggins, C Mellish, Journal of New Music Research. 282D. Ponsford, G. Wiggins, and C. Mellish. Statistical learning of harmonic movement. Journal of New Music Research, 28(2):150- 177, 1999.\n\nAdaptive music generation for computer games. A , The Open UniversityPhD thesisA. Prechtl. Adaptive music generation for computer games. PhD thesis, The Open University, 2016.\n\nAn analysis of procedural piano music composition with mood templates using genetic algorithms. L R A Santos, L.R.A. Santos. An analysis of procedural piano music composition with mood templates using genetic algorithms, 2018.\n\nAffective evolutionary music composition with metacompose. Genetic Programming and Evolvable Machines. M Scirea, J Togelius, P Eklund, S Risi, 18M. Scirea, J. Togelius, P. Eklund, and S. Risi. Affective evolutionary music composition with metacompose. Genetic Programming and Evolvable Machines, 18(4):433-465, 2017.\n\nA novel automatic composition system using evolutionary algorithm and phrase imitation. C K Ting, C L Wu, C H Liu, IEEE Systems Journal. 113C.K. Ting, C.L. Wu, and C.H. Liu. A novel automatic composition system using evolutionary algorithm and phrase imitation. IEEE Systems Journal, 11(3):1284-1295, 2017.\n\nMusic composition with interactive evolutionary computation. N Tokui, H Iba, Proceedings of the 3rd international conference on generative art. the 3rd international conference on generative art17N. Tokui and H. Iba. Music composition with interactive evolution- ary computation. In Proceedings of the 3rd international confer- ence on generative art, volume 17:2, pages 215-226, 2000.\n\nEmotional music generation using interactive genetic algorithm. H Zhu, S Wang, Z Wang, Computer Science and Software Engineering. IEEE1H. Zhu, S. Wang, and Z. Wang. Emotional music generation using interactive genetic algorithm. In Computer Science and Software Engineering, 2008 International Conference on, volume 1, pages 345-348. IEEE, 2008.\n", "annotations": {"author": "[{\"end\":113,\"start\":101},{\"end\":180,\"start\":114},{\"end\":224,\"start\":181},{\"end\":233,\"start\":225},{\"end\":323,\"start\":234}]", "publisher": null, "author_last_name": "[{\"end\":112,\"start\":107},{\"end\":128,\"start\":122},{\"end\":198,\"start\":188},{\"end\":256,\"start\":242}]", "author_first_name": "[{\"end\":106,\"start\":101},{\"end\":121,\"start\":114},{\"end\":187,\"start\":181},{\"end\":230,\"start\":225},{\"end\":241,\"start\":234}]", "author_affiliation": "[{\"end\":179,\"start\":130},{\"end\":223,\"start\":200},{\"end\":322,\"start\":258}]", "title": "[{\"end\":98,\"start\":1},{\"end\":421,\"start\":324}]", "venue": null, "abstract": "[{\"end\":1538,\"start\":509}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1638,\"start\":1635},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1641,\"start\":1638},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":1710,\"start\":1706},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1734,\"start\":1731},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":1764,\"start\":1760},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1796,\"start\":1792},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2118,\"start\":2114},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2133,\"start\":2130},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2319,\"start\":2316},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2392,\"start\":2388},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2954,\"start\":2951},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3100,\"start\":3096},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3668,\"start\":3665},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4650,\"start\":4647},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4777,\"start\":4773},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5234,\"start\":5231},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5433,\"start\":5429},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5566,\"start\":5562},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6027,\"start\":6024},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6273,\"start\":6269},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6276,\"start\":6273},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":6420,\"start\":6417},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6423,\"start\":6420},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6426,\"start\":6423},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6429,\"start\":6426},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6432,\"start\":6429},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6519,\"start\":6515},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6522,\"start\":6519},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6525,\"start\":6522},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6542,\"start\":6538},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6545,\"start\":6542},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6853,\"start\":6849},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6922,\"start\":6918},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11199,\"start\":11195},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11318,\"start\":11314}]", "figure": "[{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":29297,\"start\":28631}]", "paragraph": "[{\"end\":1797,\"start\":1554},{\"end\":2134,\"start\":1799},{\"end\":2393,\"start\":2136},{\"end\":2585,\"start\":2395},{\"end\":3101,\"start\":2587},{\"end\":3307,\"start\":3103},{\"end\":3603,\"start\":3309},{\"end\":4148,\"start\":3605},{\"end\":4586,\"start\":4150},{\"end\":4763,\"start\":4588},{\"end\":5144,\"start\":4765},{\"end\":5434,\"start\":5146},{\"end\":5567,\"start\":5436},{\"end\":5762,\"start\":5569},{\"end\":6141,\"start\":5764},{\"end\":6526,\"start\":6143},{\"end\":6995,\"start\":6528},{\"end\":7256,\"start\":6997},{\"end\":7844,\"start\":7270},{\"end\":8173,\"start\":7846},{\"end\":8618,\"start\":8175},{\"end\":9074,\"start\":8620},{\"end\":9491,\"start\":9076},{\"end\":9677,\"start\":9493},{\"end\":10068,\"start\":9679},{\"end\":10316,\"start\":10070},{\"end\":10477,\"start\":10318},{\"end\":10669,\"start\":10479},{\"end\":10860,\"start\":10671},{\"end\":11130,\"start\":10862},{\"end\":11578,\"start\":11132},{\"end\":11634,\"start\":11580},{\"end\":12270,\"start\":11705},{\"end\":12408,\"start\":12272},{\"end\":12777,\"start\":12410},{\"end\":13135,\"start\":12779},{\"end\":13555,\"start\":13137},{\"end\":13996,\"start\":13557},{\"end\":14396,\"start\":14024},{\"end\":14564,\"start\":14398},{\"end\":15311,\"start\":14585},{\"end\":15576,\"start\":15313},{\"end\":16181,\"start\":15578},{\"end\":16714,\"start\":16183},{\"end\":17056,\"start\":16716},{\"end\":17350,\"start\":17058},{\"end\":17589,\"start\":17352},{\"end\":18008,\"start\":17609},{\"end\":18462,\"start\":18010},{\"end\":18536,\"start\":18464},{\"end\":18848,\"start\":18538},{\"end\":19158,\"start\":18850},{\"end\":19664,\"start\":19160},{\"end\":20023,\"start\":19666},{\"end\":20855,\"start\":20025},{\"end\":21124,\"start\":20857},{\"end\":21512,\"start\":21142},{\"end\":21975,\"start\":21514},{\"end\":22423,\"start\":21977},{\"end\":23093,\"start\":22425},{\"end\":23343,\"start\":23095},{\"end\":23708,\"start\":23345},{\"end\":24130,\"start\":23710},{\"end\":24266,\"start\":24132},{\"end\":24558,\"start\":24268},{\"end\":24854,\"start\":24560},{\"end\":25463,\"start\":24856},{\"end\":25751,\"start\":25465},{\"end\":26677,\"start\":25753},{\"end\":27195,\"start\":26679},{\"end\":27520,\"start\":27197},{\"end\":27895,\"start\":27536},{\"end\":28314,\"start\":27897},{\"end\":28630,\"start\":28316}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11704,\"start\":11635}]", "table_ref": "[{\"end\":26039,\"start\":26032}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1552,\"start\":1540},{\"attributes\":{\"n\":\"3\"},\"end\":7268,\"start\":7259},{\"attributes\":{\"n\":\"4\"},\"end\":14022,\"start\":13999},{\"attributes\":{\"n\":\"4.1\"},\"end\":14583,\"start\":14567},{\"attributes\":{\"n\":\"4.2\"},\"end\":17607,\"start\":17592},{\"attributes\":{\"n\":\"4.3\"},\"end\":21140,\"start\":21127},{\"attributes\":{\"n\":\"5\"},\"end\":27534,\"start\":27523}]", "table": "[{\"end\":29297,\"start\":28729}]", "figure_caption": "[{\"end\":28729,\"start\":28633}]", "figure_ref": null, "bib_author_first_name": "[{\"end\":29498,\"start\":29497},{\"end\":29500,\"start\":29499},{\"end\":29508,\"start\":29507},{\"end\":29512,\"start\":29509},{\"end\":29522,\"start\":29521},{\"end\":29888,\"start\":29887},{\"end\":29890,\"start\":29889},{\"end\":29989,\"start\":29988},{\"end\":30107,\"start\":30106},{\"end\":30119,\"start\":30118},{\"end\":30447,\"start\":30446},{\"end\":30449,\"start\":30448},{\"end\":30458,\"start\":30457},{\"end\":30460,\"start\":30459},{\"end\":30469,\"start\":30468},{\"end\":30471,\"start\":30470},{\"end\":30635,\"start\":30634},{\"end\":30645,\"start\":30644},{\"end\":30647,\"start\":30646},{\"end\":30945,\"start\":30944},{\"end\":31071,\"start\":31070},{\"end\":31311,\"start\":31310},{\"end\":31313,\"start\":31312},{\"end\":31323,\"start\":31322},{\"end\":31325,\"start\":31324},{\"end\":31512,\"start\":31511},{\"end\":31521,\"start\":31520},{\"end\":31528,\"start\":31527},{\"end\":31534,\"start\":31533},{\"end\":31548,\"start\":31547},{\"end\":32076,\"start\":32075},{\"end\":32078,\"start\":32077},{\"end\":32313,\"start\":32312},{\"end\":32323,\"start\":32322},{\"end\":32332,\"start\":32331},{\"end\":32557,\"start\":32556},{\"end\":32559,\"start\":32558},{\"end\":32565,\"start\":32564},{\"end\":32567,\"start\":32566},{\"end\":32853,\"start\":32852},{\"end\":32864,\"start\":32863},{\"end\":32872,\"start\":32871},{\"end\":32883,\"start\":32882},{\"end\":32895,\"start\":32894},{\"end\":32908,\"start\":32907},{\"end\":33439,\"start\":33438},{\"end\":33441,\"start\":33440},{\"end\":33448,\"start\":33447},{\"end\":33456,\"start\":33455},{\"end\":33751,\"start\":33750},{\"end\":33954,\"start\":33953},{\"end\":34088,\"start\":34087},{\"end\":34317,\"start\":34316},{\"end\":34328,\"start\":34327},{\"end\":34339,\"start\":34338},{\"end\":34351,\"start\":34350},{\"end\":34558,\"start\":34557},{\"end\":34567,\"start\":34566},{\"end\":34882,\"start\":34881},{\"end\":34884,\"start\":34883},{\"end\":34997,\"start\":34996},{\"end\":35009,\"start\":35008},{\"end\":35020,\"start\":35019},{\"end\":35249,\"start\":35248},{\"end\":35476,\"start\":35475},{\"end\":35480,\"start\":35477},{\"end\":35711,\"start\":35710},{\"end\":35721,\"start\":35720},{\"end\":35733,\"start\":35732},{\"end\":35743,\"start\":35742},{\"end\":36014,\"start\":36013},{\"end\":36016,\"start\":36015},{\"end\":36024,\"start\":36023},{\"end\":36026,\"start\":36025},{\"end\":36032,\"start\":36031},{\"end\":36034,\"start\":36033},{\"end\":36295,\"start\":36294},{\"end\":36304,\"start\":36303},{\"end\":36685,\"start\":36684},{\"end\":36692,\"start\":36691},{\"end\":36700,\"start\":36699}]", "bib_author_last_name": "[{\"end\":29505,\"start\":29501},{\"end\":29519,\"start\":29513},{\"end\":29527,\"start\":29523},{\"end\":29896,\"start\":29891},{\"end\":29996,\"start\":29990},{\"end\":30116,\"start\":30108},{\"end\":30128,\"start\":30120},{\"end\":30455,\"start\":30450},{\"end\":30466,\"start\":30461},{\"end\":30477,\"start\":30472},{\"end\":30642,\"start\":30636},{\"end\":30651,\"start\":30648},{\"end\":30952,\"start\":30946},{\"end\":31078,\"start\":31072},{\"end\":31320,\"start\":31314},{\"end\":31334,\"start\":31326},{\"end\":31518,\"start\":31513},{\"end\":31525,\"start\":31522},{\"end\":31531,\"start\":31529},{\"end\":31545,\"start\":31535},{\"end\":31558,\"start\":31549},{\"end\":32088,\"start\":32079},{\"end\":32320,\"start\":32314},{\"end\":32329,\"start\":32324},{\"end\":32340,\"start\":32333},{\"end\":32562,\"start\":32560},{\"end\":32573,\"start\":32568},{\"end\":32861,\"start\":32854},{\"end\":32869,\"start\":32865},{\"end\":32880,\"start\":32873},{\"end\":32892,\"start\":32884},{\"end\":32905,\"start\":32896},{\"end\":32915,\"start\":32909},{\"end\":33445,\"start\":33442},{\"end\":33453,\"start\":33449},{\"end\":33465,\"start\":33457},{\"end\":33757,\"start\":33752},{\"end\":33962,\"start\":33955},{\"end\":34094,\"start\":34089},{\"end\":34325,\"start\":34318},{\"end\":34336,\"start\":34329},{\"end\":34348,\"start\":34340},{\"end\":34358,\"start\":34352},{\"end\":34564,\"start\":34559},{\"end\":34891,\"start\":34885},{\"end\":35006,\"start\":34998},{\"end\":35017,\"start\":35010},{\"end\":35028,\"start\":35021},{\"end\":35487,\"start\":35481},{\"end\":35718,\"start\":35712},{\"end\":35730,\"start\":35722},{\"end\":35740,\"start\":35734},{\"end\":35748,\"start\":35744},{\"end\":36021,\"start\":36017},{\"end\":36029,\"start\":36027},{\"end\":36038,\"start\":36035},{\"end\":36301,\"start\":36296},{\"end\":36308,\"start\":36305},{\"end\":36689,\"start\":36686},{\"end\":36697,\"start\":36693},{\"end\":36705,\"start\":36701}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":3528727},\"end\":29847,\"start\":29391},{\"attributes\":{\"id\":\"b1\"},\"end\":29953,\"start\":29849},{\"attributes\":{\"id\":\"b2\"},\"end\":30051,\"start\":29955},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":7395677},\"end\":30391,\"start\":30053},{\"attributes\":{\"id\":\"b4\"},\"end\":30577,\"start\":30393},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":19555571},\"end\":30905,\"start\":30579},{\"attributes\":{\"id\":\"b6\"},\"end\":31007,\"start\":30907},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":21717372},\"end\":31247,\"start\":31009},{\"attributes\":{\"id\":\"b8\"},\"end\":31434,\"start\":31249},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":16049781},\"end\":31983,\"start\":31436},{\"attributes\":{\"id\":\"b10\"},\"end\":32216,\"start\":31985},{\"attributes\":{\"id\":\"b11\"},\"end\":32480,\"start\":32218},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":5069278},\"end\":32784,\"start\":32482},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":10025831},\"end\":33396,\"start\":32786},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":4590477},\"end\":33707,\"start\":33398},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":5947777},\"end\":33915,\"start\":33709},{\"attributes\":{\"id\":\"b16\"},\"end\":34034,\"start\":33917},{\"attributes\":{\"id\":\"b17\"},\"end\":34234,\"start\":34036},{\"attributes\":{\"id\":\"b18\"},\"end\":34502,\"start\":34236},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":16887164},\"end\":34839,\"start\":34504},{\"attributes\":{\"id\":\"b20\"},\"end\":34951,\"start\":34841},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":17439435},\"end\":35200,\"start\":34953},{\"attributes\":{\"id\":\"b22\"},\"end\":35377,\"start\":35202},{\"attributes\":{\"id\":\"b23\"},\"end\":35605,\"start\":35379},{\"attributes\":{\"id\":\"b24\"},\"end\":35923,\"start\":35607},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":20872350},\"end\":36231,\"start\":35925},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":6899248},\"end\":36618,\"start\":36233},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":13905130},\"end\":36965,\"start\":36620}]", "bib_title": "[{\"end\":29495,\"start\":29391},{\"end\":30104,\"start\":30053},{\"end\":30632,\"start\":30579},{\"end\":31068,\"start\":31009},{\"end\":31509,\"start\":31436},{\"end\":32554,\"start\":32482},{\"end\":32850,\"start\":32786},{\"end\":33436,\"start\":33398},{\"end\":33748,\"start\":33709},{\"end\":34555,\"start\":34504},{\"end\":34994,\"start\":34953},{\"end\":36011,\"start\":35925},{\"end\":36292,\"start\":36233},{\"end\":36682,\"start\":36620}]", "bib_author": "[{\"end\":29507,\"start\":29497},{\"end\":29521,\"start\":29507},{\"end\":29529,\"start\":29521},{\"end\":29898,\"start\":29887},{\"end\":29998,\"start\":29988},{\"end\":30118,\"start\":30106},{\"end\":30130,\"start\":30118},{\"end\":30457,\"start\":30446},{\"end\":30468,\"start\":30457},{\"end\":30479,\"start\":30468},{\"end\":30644,\"start\":30634},{\"end\":30653,\"start\":30644},{\"end\":30954,\"start\":30944},{\"end\":31080,\"start\":31070},{\"end\":31322,\"start\":31310},{\"end\":31336,\"start\":31322},{\"end\":31520,\"start\":31511},{\"end\":31527,\"start\":31520},{\"end\":31533,\"start\":31527},{\"end\":31547,\"start\":31533},{\"end\":31560,\"start\":31547},{\"end\":32090,\"start\":32075},{\"end\":32322,\"start\":32312},{\"end\":32331,\"start\":32322},{\"end\":32342,\"start\":32331},{\"end\":32564,\"start\":32556},{\"end\":32575,\"start\":32564},{\"end\":32863,\"start\":32852},{\"end\":32871,\"start\":32863},{\"end\":32882,\"start\":32871},{\"end\":32894,\"start\":32882},{\"end\":32907,\"start\":32894},{\"end\":32917,\"start\":32907},{\"end\":33447,\"start\":33438},{\"end\":33455,\"start\":33447},{\"end\":33467,\"start\":33455},{\"end\":33759,\"start\":33750},{\"end\":33964,\"start\":33953},{\"end\":34096,\"start\":34087},{\"end\":34327,\"start\":34316},{\"end\":34338,\"start\":34327},{\"end\":34350,\"start\":34338},{\"end\":34360,\"start\":34350},{\"end\":34566,\"start\":34557},{\"end\":34570,\"start\":34566},{\"end\":34893,\"start\":34881},{\"end\":35008,\"start\":34996},{\"end\":35019,\"start\":35008},{\"end\":35030,\"start\":35019},{\"end\":35252,\"start\":35248},{\"end\":35489,\"start\":35475},{\"end\":35720,\"start\":35710},{\"end\":35732,\"start\":35720},{\"end\":35742,\"start\":35732},{\"end\":35750,\"start\":35742},{\"end\":36023,\"start\":36013},{\"end\":36031,\"start\":36023},{\"end\":36040,\"start\":36031},{\"end\":36303,\"start\":36294},{\"end\":36310,\"start\":36303},{\"end\":36691,\"start\":36684},{\"end\":36699,\"start\":36691},{\"end\":36707,\"start\":36699}]", "bib_venue": "[{\"end\":31731,\"start\":31654},{\"end\":33071,\"start\":32984},{\"end\":36427,\"start\":36377},{\"end\":29571,\"start\":29529},{\"end\":29885,\"start\":29849},{\"end\":29986,\"start\":29955},{\"end\":30197,\"start\":30130},{\"end\":30444,\"start\":30393},{\"end\":30717,\"start\":30653},{\"end\":30942,\"start\":30907},{\"end\":31114,\"start\":31080},{\"end\":31308,\"start\":31249},{\"end\":31652,\"start\":31560},{\"end\":32073,\"start\":31985},{\"end\":32310,\"start\":32218},{\"end\":32616,\"start\":32575},{\"end\":32982,\"start\":32917},{\"end\":33530,\"start\":33467},{\"end\":33798,\"start\":33759},{\"end\":33951,\"start\":33917},{\"end\":34085,\"start\":34036},{\"end\":34314,\"start\":34236},{\"end\":34643,\"start\":34570},{\"end\":34879,\"start\":34841},{\"end\":35059,\"start\":35030},{\"end\":35246,\"start\":35202},{\"end\":35473,\"start\":35379},{\"end\":35708,\"start\":35607},{\"end\":36060,\"start\":36040},{\"end\":36375,\"start\":36310},{\"end\":36748,\"start\":36707}]"}}}, "year": 2023, "month": 12, "day": 17}