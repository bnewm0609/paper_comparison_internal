{"id": 221408358, "updated": "2022-01-22 13:00:33.569", "metadata": {"title": "Weakly-Supervised Crowd Counting Learns from Sorting rather than Locations", "authors": "[{\"middle\":[],\"last\":\"Yang\",\"first\":\"Yifan\"},{\"middle\":[],\"last\":\"Wu\",\"first\":\"Zhe\"},{\"middle\":[],\"last\":\"Su\",\"first\":\"Li\"},{\"middle\":[],\"last\":\"Huang\",\"first\":\"Qingming\"},{\"middle\":[],\"last\":\"Sebe\",\"first\":\"Nicu\"}]", "venue": "Computer Vision \u2013 ECCV 2020", "journal": "Computer Vision \u2013 ECCV 2020", "publication_date": {"year": 2020, "month": null, "day": null}, "abstract": "In crowd counting datasets, the location labels are costly, yet, they are not taken into the evaluation metrics. Besides, existing multi-task approaches employ high-level tasks to improve counting accuracy. This research tendency increases the demand for more annotations. In this paper, we propose a weakly-supervised counting network, which directly regresses the crowd numbers without the location supervision. Moreover, we train the network to count by exploiting the relationship among the images. We propose a soft-label sorting network along with the counting network, which sorts the given images by their crowd numbers. The sorting network drives the shared backbone CNN model to obtain density-sensitive ability explicitly. Therefore, the proposed method improves the counting accuracy by utilizing the information hidden in crowd numbers, rather than learning from extra labels, such as locations and perspectives. We evaluate our proposed method on three crowd counting datasets, and the performance of our method plays favorably against the fully supervised state-of-the-art approaches.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "3097407159", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/eccv/YangLW0HS20", "doi": "10.1007/978-3-030-58598-3_1"}}, "content": {"source": {"pdf_hash": "12deed8798a59fa4d2d38f06f6878af4dd274d9c", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "c2815500d2d95d9ec0395ec9aebe8c5582a1a576", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/12deed8798a59fa4d2d38f06f6878af4dd274d9c.txt", "contents": "\nWeakly-Supervised Crowd Counting Learns from Sorting rather than Locations\n\n\nYifan Yang \nSchool of Computer Science and Technology\nUCAS\nBeijingChina\n\nGuorong Li \nSchool of Computer Science and Technology\nUCAS\nBeijingChina\n\nKey Lab of Big Data Mining and Knowledge Management\nUCAS\nBeijingChina\n\nZhe Wu \nSchool of Computer Science and Technology\nUCAS\nBeijingChina\n\nSu Li \nKey Lab of Big Data Mining and Knowledge Management\nUCAS\nBeijingChina\n\nQingming Huang \nSchool of Computer Science and Technology\nUCAS\nBeijingChina\n\nKey Lab of Big Data Mining and Knowledge Management\nUCAS\nBeijingChina\n\nKey Lab of Intelligent Information Processing\nICT, CAS\nBeijingChina\n\nNicu Sebe \nSchool of Computer Science and Technology\nUCAS\nBeijingChina\n\nUniversity of Trento\nTrentoItaly\n\nWeakly-Supervised Crowd Counting Learns from Sorting rather than Locations\nWeakly-supervisedSortingMulti-framesCrowd Counting\nIn crowd counting datasets, the location labels are costly, yet, they are not taken into the evaluation metrics. Besides, existing multi-task approaches employ high-level tasks to improve counting accuracy. This research tendency increases the demand for more annotations. In this paper, we propose a weakly-supervised counting network, which directly regresses the crowd numbers without the location supervision. Moreover, we train the network to count by exploiting the relationship among the images. We propose a soft-label sorting network along with the counting network, which sorts the given images by their crowd numbers. The sorting network drives the shared backbone CNN model to obtain density-sensitive ability explicitly. Therefore, the proposed method improves the counting accuracy by utilizing the information hidden in crowd numbers, rather than learning from extra labels, such as locations and perspectives. We evaluate our proposed method on three crowd counting datasets, and the performance of our method plays favorably against the fully supervised state-of-the-art approaches.\n\nIntroduction\n\nCounting objects is a hot topic in computer vision because of its wide applications in many areas. Significant effort has been devoted to this task [34,15,18,38,30,2,21,11]. These approaches either employ a detection framework [18,22] or a regression framework [34,15,18]. However, in congested scenes, there are many occlusions, and it is difficult for the detection approaches to recognize the person. Therefore, the density estimation based methods [34,15,18], in particular, have received increasing research focus.\n\nHowever, there are still several drawbacks. Firstly, the annotations of the crowd counting are generally expensive. The existing counting datasets [46,45,3,9] provide the location of each instance to train the counting networks, while in the evaluation stage, these location labels are not taken into account, and the performance metrics only evaluate the estimation accuracy of the crowd number.\n\nIn fact, without the demand for locations, the crowd numbers can be obtained in other economical ways. For instance, with an already collected dataset, the crowd numbers can be obtained by gathering the environmental information, e.g., detection of disturbances in spaces, or estimation of the number of moving crowds. Chan et al. [3] segment the scene by crowd motions and estimate the crowd number by calculating the area of the segmented regions. To collect a novel counting dataset, we can employ sensor technology to obtain the crowd number in constrained scenes, such as mobile crowd sensing technology [10]. Moreover, Sheng et al. [35] propose a GPS-less energy-efficient sensing scheduling to acquire the crowd number more economically. On the other hand, several approaches [17,4,14,23] prove that, in the estimated results, there is no tight bond between the crowd number and the location. Finally, in the existing multitask approaches, high level tasks are employed to improve the counting accuracy, for instance, tracking [23], detection [18,22], segmentation [37,47], localization [25,17], depth prediction [47] and scene analysis [20,36,45,44,19]. This research tendency increases the demand for more annotations.\n\nIn this work, we propose a weakly-supervised framework to directly regress the crowd number without the supervision of location labels. To our best knowledge, we are the first to train a counting network without location supervision. Moreover, we train the network to count by exploiting the relationship among the images. We propose an end-to-end trainable soft-label sorting network along with the counting network, which sorts the given images by their crowd numbers. The sorting network drives the shared backbone CNN model to obtain densitysensitive ability explicitly. Therefore, the proposed method improves the counting accuracy by utilizing the relationship among crowd numbers, rather than learning from extra labels, such as locations and perspectives. More concretely, the proposed sorting network processes several images and employs a soft-sort layer to generate dense order matrixes. The previous sorting works [16,8,6] employ hard-labels to train the sorting network, for instance, one-hot vectors [16,8] or indexes which are real integers [6]. However, we find that hard-labels are incapable of capturing the complexity of sorting task. As the candidates may have limited variations or even have the same values, the hard-labels introduce ambiguous supervision to the training stage. Therefore, we propose an informative soft-label, which introduces the Rayleigh distribution to characterize the sorting complexity. The proposed soft-labels have high entropy. They provide much more information per training case than hard-labels and much less variance in the gradient between training cases.\n\nThe main contributions of our method are summarized as follows:\n\n-We propose a weakly-supervised counting network, which directly regresses the crowd number without the supervision of location labels. -We propose a soft-label sorting network to facilitate the counting task, which sorts the images by their crowd numbers. The proposed framework improves the counting task without extra labels, especially costly semantic labels. -The proposed weakly-supervised approach plays favorably against fully supervised state-of-the-art approaches on three datasets.\n\n\nRelated Work\n\n\nDensity Estimation Based Methods\n\nThe counting datasets provide a location label for each person. The fully supervised density estimation based methods have to generate density maps with various strategies. Several approaches [46,15] coarsely estimate the instance scales by the interval distances and employ Gaussian kernels with various scales to represent the objects. Wan et al. [41] propose a network to adaptively generate density maps, and train the generative network with the regression network. The obtained density maps are better recognized by the counting network. However, as proved by several approaches [17,4,14,23], in the estimated density maps, there is no tight bond between the crowd number and the location. In the scenes with large perspective distortions, regardless of the low regression errors, dense-crowd regions are usually underestimated, while sparse-crowd regions are overestimated. Du et al. [23] prove a similar phenomenon in the scenes with limited scale variations. The existing approaches employ various strategies to improve counting accuracy. Several approaches employ multiple receptive fields to evaluate the instances with various scales. To obtain the multiple receptive fields, Zhang et al. [46], Deb et al. [7] and Sam et al. [29] employ multi-column networks; several approaches [2,13,19] utilize inception blocks. Besides changing the convolution kernels, a deep network can also obtain various receptive fields from its different layers. Several counting approaches [34,2,20,13] utilize similar architectures with U-net [28]. However, Li et al. [15] prove that the multiple receptive fields deliver similar results. Moreover, several methods employ extra supervision to improve evaluation accuracy. For instance, Liu et al. [20], Shi et al. [36], Yan et al. [44] and Zhang et al. [45] employ perspective maps to smooth the final density maps. However, the perspective maps are delivered from extra annotations. Besides, the existing multi-task approaches utilize high-level tasks to improve the estimation accuracy, for instance, tracking [23], detection [18,22], segmentation [37,47], localization [25,17], and depth prediction [47]. These multi-task approaches boost the counting task with extra semantic labels.\n\nIn this work, we propose a weakly-supervised counting approach, which is trained without location labels. Moreover, we propose a soft-label sorting network to improve the counting accuracy, which sorts the images with various crowd numbers. The proposed framework improves the counting task without extra labels, especially costly semantic labels.\n\n\nMethods Dealing with the Lack of Labelled Data\n\nSeveral approaches are proposed to relieve the expensive labeling work in crowd counting. One of the most relevant works for our method is L2R [21], which facilitates the counting task by ranking the image patches. However, L2R is fully supervised by using the location labels. Moreover, the ranking network only operates on the image patch and one of its sub-patches. Our proposed network is trained without location labels. Besides, the sorting network processes the whole image, and the number of the candidate images are not fixed.\n\nWang et al. [42] generate synthetic crowd scenes and simultaneously annotate them. The proposed network is pre-trained on the synthetic dataset and then fine-tuned with real data. Although, this approach improves the counting performance with less expensive labels. Labeling the real data is still expensive and challenging. Loy et al. [24] employ active learning to label more representative frames of the videos. This strategy efficiently releases the laborious work. However, it only works on the video counting task, where the video data are assumed to lie along a low-dimensional manifold. Sam et al. [31] pre-train the feature extractor with several restricted Boltzmann machines progressively in an unsupervised way, but the training of the top regression layers is fully-supervised.\n\nOut of these mentioned approaches, only our method and Loy et al. [24] employ fewer labels to train the networks. Still, only our approach trains the network without the supervision of locations.\n\n\nLearning from Sorting\n\nSorting is used pervasively in machine learning. However, it is also a poor match for the differentiable pipelines of deep learning. Currently, several approaches combine the sorting layers with deep networks. Several works [26,33,43] encode the permutations into indexes and train the network to regress the index. While others algorithms [16,8,6] propose differentiable operators to directly regress the order.\n\nSeveral self-supervised approaches employ a sorting task to pre-train the feature extractor. Noroozi et al. [26] first propose a self-supervised network to learn a feature domain by solving Jigsaw puzzles. Inspired by this work, Sermanet et al. [33] propose a similar framework to sort the shuffled video frames, and the learned features are used as video representation. Xu et al. [43] also learn video representations by sorting shuffled frames. However, they employ video clips rather than single frames to train the network. These approaches have several restrictions. Firstly, they employ indexes to represent all the possible permutations and train the network to regress the index. This strategy reduces the information embedded in the supervision labels. For example, in [26], there are 9 elements to sort and the number of possible permutations is 9!=362,880. The massive numbers inhibit the methods to sort more elements; for instance, Xu et al. [43] restrict the number of clips between 2 to 5. Moreover, with the casual encoding strategy, a slight variation between two permutations may cause a dramatic difference in their indexes. Therefore, sorting networks cannot learn a representation efficiently. Otherwise, the feature extractors are pre-trained. Our proposed method employs a dense order matrix to capture the possible permutations and end-to-end trains the sorting network with the counting network.\n\nOn the other hand, several approaches propose differentiable soft-sort methods to tackle these issues. Sinkhorn distance [5] has been initially proposed to tackle optimal transportation, while Linderman et al. [16] employ it to address sorting issues. Grover et al. [8] propose an attractive task, which sorts n numbers between 0 and 9999 given as four concatenated MNIST images. They also propose a differentiable neural sort method to tackle this task. Based on the Sinkhorn method, Cuturi et al. [6] further propose a differentiable soft-sort algorithm, which directly generates the sort and rank indexes of a vector. However, these approaches employ hard-labels to train the network, and this leads to the loss of valuable information.\n\nIn this paper, instead of pre-training a feature extractor, we train the sorting network with counting network in an end-to-end manner. In the final layer, we also employ a differentiable soft-sort operator to generate dense order matrixes. Moreover, we propose an informative soft-label to train the sorting network.\n\n\nMethod\n\nIn this paper, we propose a weakly-supervised counting approach, which does not rely on location supervision for training. Besides, to improve the counting task, we exploit the relationship among images. We propose a novel soft-label sorting network along with the regression network, which sorts the images by their various crowd numbers. Both the regression network and the sorting network share a same backbone, and both networks are end-to-end trained. As both networks estimate the crowd numbers of the given images, they promote each other without extra labels, such as location and perspective.\n\nMore concretely, the regression network employs several adaptive pooling layers to formulate a pyramidal feature vector. Besides, the sorting network employs a network to formulate the comparison and uses a differentiable soft-sort layer to generate dense order predictions. Moreover, to train the network effectively, we propose soft-labels, which have high entropy and provide much more information. The soft-labels employ the Rayleigh distribution to characterize the complexity of sorting tasks. We will elaborate on the details of the regression network, the sorting network, and the training method in the following subsections.\n\n\nRegression Network\n\nAs shown in Fig. 1, the regression network directly regresses the crowd number from the whole frame. Moreover, the front-end of the network, which delivers the pyramidal feature vector, is shared with the sorting network.\n\nIn the front-end network, we first employ the first 13 layers of VGG-16 [39] as the backbone of our network, similar to previous methods [1,32,40,15]. The front-end is marked as G b (\u03b3), where \u03b3 stands for the parameters, and the output size is 1/8 of the original input size. Then the front-end network regresses a single channel density map based on the extracted features, which is formulated as:\nf d = F d (G b (x, \u03b3), \u03b6),(1)\nwhere f d \u2208 R 1W H represents the estimated density map, W, H are the width and height of the feature map respectively. Moreover, F d (\u00b7) denotes the convolution operation, and \u03b6 stands for the parameters of the convolution layers.  The network needs the front-end to be sensitive to both the densities of the local and global crowds. Therefore, we propose to use adaptive pooling layers, denoted as P, to extract a pyramidal feature vector from f d . The adaptive pooling layers consist of global sub-cluster layers and local sub-cluster layers. Each global sub-cluster layer is denoted as P i,S(i) G , which employs an adaptive average pooling with a high sampling rate S(i) to integrate the global information. Here, i \u2208 {1, \u00b7 \u00b7 \u00b7 , N }, N is the number of the global sub-cluster layers. Besides, each local sub-cluster layer is denoted as P j,S(j+N ) L , which employs an adaptive max pooling with a lower sampling rate S(j + N ) to extract the most discriminative features. Here, j \u2208 {1, \u00b7 \u00b7 \u00b7 , T \u2212 N }, T is the total number of the pooling layers. The sampling rates of these pooling layers belong to the sampling rate set, which is denoted as S. We concatenate the outputs of adaptive pooling layers to formulate the pyramidal feature vector:\nf pf v = P(f d , S).(2)\nThe back-end of the regression network employs several fully-connected layers to predict the crowd number:\nc = F r (f pf v , \u03c8),(3)\nwhere c is the predicted crowd number, and F r (\u00b7) stands for the regression layers with parameters \u03c8.\n\n\nSorting Network\n\nTo process multi-frames, the sorting network employs a multi-branch network to extract the pyramidal feature vectors. Each branch is shared with the regression network, while all the branches share the same parameters. The multi-branch network is formulated as G s (\u03c9), and the pyramidal feature vectors are denoted as\n{f 1 pf v , f 2 pf v , \u00b7 \u00b7 \u00b7 , f K pf v }.\nHere, \u03c9 represents the shared parameters, and K is the number of the given images. The sorting network then utilizes a comparing network to regress the order feature. The comparing network first organizes K(K \u2212 1)/2 non-repeating tuples, each of which has two elements. Then the network calculates the difference between each pair: f ij m = f i pf v \u2212 f j pf v , and concatenates the difference features: f dif f = f 12 m ||f 13 m || \u00b7 \u00b7 \u00b7 ||f 1K m ||f 23 m ||f 24 m || \u00b7 \u00b7 \u00b7 ||f 2K m \u00b7 \u00b7 \u00b7 ||f K\u22121,K m , where || denotes the concatenation operation. Finally, the sorting network regresses the order feature of the given images. The order feature is formulated as\nf o , where f o \u2208 R K .\nWe employ the Sinkhorn layer [5] to transfer f o into an order matrix P, where P ij is the probability that the i-th element is ordered in j, and P \u2208 R KK . More importantly, we propose a soft-label to characterize the complexity of the sorting task, which is more informative than the hard-label. We elaborate on this in the following subsections.\n\nSinkhorn Operator The Sinkhorn method is proposed to solve the optimal transportation issue. After several iterations, it generates a matrix to capture the transportation probabilities between two distributions. As the method is differentiable, recently, it has been combined with the deep networks to solve the sorting problems.\n\nIn the proposed sorting network, we employ a Sinkhorn layer to generate the transportation matrix between the order feature f o and the order vector y o . In the Sinkhorn layer, we first initialize the transportation matrix P by:\nP ij = exp \u2212 |f i o \u2212 y j o | ,(4)\nwhere, is a control factor. In the iterations, the P is updated as:\nv = 1 P uK , u = 1 PvK ,(5)\nwhere u = 1 K . The iterations stop when \u2206(vP u, 1 K /K) < \u03b7. The max iteration number l is depend on : typically, the smaller , the larger l is needed to ensure that vP u is close to 1 K /K.\n\n\nSoft-Label\n\nTo train the sorting network, we transfer the permutation \u03c3 to the ground truth transportation matrix P GT . Previous works generate a hard label, where P GT (i, \u03c3(i)) = 1. However, the sorting task is complex, and the hardlabels are unable to cover all the situations. For instance, there may be several candidates with similar or even identical values. Therefore, we propose softlabels with high entropy to capture transportation probabilities. They provide not only much more information per training case than hard-labels but also much less variance in the gradient between training cases. The soft-labels introduce the Rayleigh distribution to capture the relations between one element and its neighbors in the permutation. We denote the differences between one element and its neighbours in permutation as \u2206 i+1 , \u2206 i\u22121 , where \u2206 i+1 = |c \u03c3(i) \u2212 c \u03c3(i)+1 |. We set a threshold, which is denoted as \u2206 thr , as the sensitivity of the network. If the difference between the two elements is less than the threshold, the network considers them as being similar instances. The elements of the transportation matrix are calculated as:\nP GT (i, \u03c3(i) + j) = (h(j) + \u00b5) \u03c3 2 e \u2212(h(j)+\u00b5) 2 2\u03c3 2 , j \u2208 {\u22121, 0, 1}.(6)\nTo ensure the correct calculation in the edges, before calculating each element, we pad the matrix, and then crop it after calculations. The rate of both operations is 1. The \u00b5, \u03c3, h(x) are determined by the differences between neighbours in permutation:\n\uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f3 \u00b5 = 1, \u03c3 = 0.5, h(x) = x; \u2206 i+1 > \u2206 thr , \u2206 i\u22121 > \u2206 thr , \u00b5 = 1, \u03c3 = 1.0, h(x) = x; \u2206 i+1 \u2206 thr , \u2206 i\u22121 > \u2206 thr , \u00b5 = 1, \u03c3 = 1.0, h(x) = \u2212x; \u2206 i+1 > \u2206 thr , \u2206 i\u22121 \u2206 thr , \u00b5 = 2, \u03c3 = 2.0, h(x) = x; \u2206 i+1 \u2206 thr , \u2206 i\u22121 \u2206 thr .(7)\nFinally, we obtain the soft-label transportation matrix P GT by normalizing P GT :\nP GT (i, \u03c3(j)) = P GT (i, \u03c3(j)) K j=1 P GT (i, \u03c3(j))(8)\n\nTraining Method\n\nWe use a straightforward way to train both the regression network and the sorting network as an end-to-end structure. The first 10 convolutional layers are fine-tuned from a pre-trained VGG-16. For the other layers, the initial values come from a Gaussian initialization with 0.01 standard deviation. Stochastic gradient descent (SGD) is applied with a fixed learning rate. While training on the image dataset and the video dataset, we employ various sampling strategies. This is because the video surveillance scene pays more attention to the variation of pedestrian flow in a constrained scene. With the image dataset, we randomly select images from the dataset and train the network with their crowd numbers. With the video dataset, we first randomly select the video fragments of the same scene. We then randomly choose images within these clips.\n\nWe utilize MSE loss to train the regression network:\nL r = 1 n n i=1 (c GT i \u2212 c i ) 2 ,(9)\nwhere c GT i is the ground truth crowd number of i-th image. We employ the cross-entropy loss to supervise the sorting network:\nL s = \u2212 1 K 2 K i=1 K j=1 (P GT ij log(P ij ) + (1 \u2212 P GT ij ) log(1 \u2212 P ij )).(10)\nThe total loss is formulated as: L total = L r + \u03be L s , where \u03be is a scalar to balance the two losses.\n\n\nExperiments\n\nWe evaluate our approach on three datasets: WorldExpo10 [12], UCSD [3], and ShanghaiTech [46]. In this section, we first provide the implementation details and evaluation metrics. We then evaluate and compare our method with the previous fully-supervised state-of-the-art approaches [46,2,15,27,19,13] on all these datasets. In the last subsection, we present ablation study results on the WorldExpo10 dataset.\n\n\nImplementation Details\n\nTo avoid over-fitting, we employ dropout layers in the fully-connected layers of both the regression network and sorting network. The ratio of dropout is 0.5. In the regression network, we set the sampling rate set as S = {{1, 2} Avg , {8, 16, 32} M ax }. As the order feature f o \u2208 R K , the sorting network employs different regression networks while sorting the various number of candidates. However, we organize each sorting network with the same structure. In the evaluate and compare subsection, we report the regression results of the proposed method, and the candidate number K is 3. In the ablation study subsection, we report the results of sorting the various number of candidates. In the Sinkhorn layer, we set as 1e-1, and \u03b7 as 1e-3. When generating the soft-label, we set \u2206 thr as 5.0. In the training stage, the learning rate is set to 1e-7, the \u03be is set to 1e2, and the batch size is set to 10.\n\n\nEvaluation Metrics\n\nSimilar to Sam et al. [30], we use the MAE and the MSE for evaluation:\nMAE = 1 N N i=1 c i \u2212 c GT i ,(11)MSE = 1 N N i=1 c i \u2212 c GT i 2 ,(12)\nwhere N is the number of images in one test set, and c GT i is the ground-truth crowd number.\n\n\nEvaluation and Comparison\n\nWorldExpo10 [46] is a video counting dataset. The training set has 3,380 videos in 106 scenes, in which 3,380 frames are labeled with point labels. Besides, the testing set has 5 videos in 5 scenes, and 600 frames are labeled. In each training clip, we randomly select 3 videos with the same scene to ensure that the crowd numbers of chosen images have enough diversity. We list the result comparisons of MAE in Table 1, where our method achieves 9.6 average MAE. Without location supervision, our method plays favorably against the fully-supervised approaches. We visualize the density maps delivered from the internal layer and show a successful example in Fig. 3 (a), where the regression results have low errors. Moreover, the predicted orders are correct, and each prediction has high confidence. When processing the image clip in Fig. 3 (b), although the regression network delivers accurate estimations, the sorting network  Fig. 3. In the upper row of each example, each image is labeled with its crowd number. Moreover, it is labeled with the order in the tuple and the corresponding probability. In the lower row of each example, we report the MAE of each estimation, and the predicted order with corresponding probability. encounters a failure. This is because the images have similar crowd numbers. The experiments prove that the proposed counting network can accurately estimate the crowd numbers without location supervision. Moreover, the sorting network is capable of sorting the image clips.\n\nUCSD [3] contains 2,000 frames which are captured by surveillance cameras, and the frames have the same perspective. The comparison between existing approaches and our method is summarized in Table 1. Proved by the results on UCSD and WorldExpo10 datasets, our method overall performs comparably with the fully-supervised approaches in the video surveillance scene.\n\nShanghaiTech [45] is an image counting dataset. There are 1,198 images with different perspectives and resolutions. This dataset has two parts named Part A and Part B. We report the comparison between our method and state-of-arts in \n\n\nAblation Study\n\nIn this section, we conduct several experiments to study the effect of different aspects of our method on WorldExpo10 and show the results in Table 2 and  Table 3. In the first part, we conduct experiments to verify the dependency between the sorting and regression tasks. In the second part, we experiment to verify the efficiency of the proposed soft-label. While in the last part, we test several modifications to the proposed method.\n\nOnly Sorting and Only Regression To verify the dependency between the sorting and regression networks, we train the two networks separately and report the results in Table 2. When we train the sorting network alone, the sorting accuracy decreases by 35.3%. While we train the regression network alone, the regression accuracy decreases by 109.4%. The experiments demonstrate that the two tasks can promote each other. This is because the two tasks both estimate the crowd numbers and are closely related.\n\n\nSoft-Label and Hard-Label\n\nWe employ the hard-label to train the proposed framework and report the results in Table 2. Each line of the hard-label is a one-hot vector. The sorting accuracy obtains 13.4% improvement, while the performance of counting task drops by 28.1%. This is because soft-labels have high entropy, and it is hard for the sorting network to predict accurate transportation probabilities. However, the soft-labels also contain much more information. Thus, they facilitate the counting task to improve performance. Different Backbones When evaluating on the video datasets, we employ images and clips to train the network, respectively. Each video clip contains 5 frames, which are sampled every 10 frames. To process the video clips, we employ the R3D network [43] as the backbone, which uses 3D convolutional layers and residual connections. The R3D network obtains improvements on several tasks, for instance, action recognition. However, as shown in Table 3, the performance of the modified method drops by 5.2%. The experiments show that the time dimension has a limited influence on the counting results. In the counting dataset, there is no regular pattern for the crowd movement. Therefore, the 3D convolutional layers extract less discriminative features.\n\n\nDifferent Frames Numbers\n\nWe employ various numbers of frames to train the sorting network and report the results in Table 3. As mentioned above, the regression networks of the candidate sorting networks have various structures. However, regression networks have the same structure. The sorting task is more difficult while sorting more candidates. For instance, when sorting 3 images, a randomly guess has a probability of 1/6 to be right, while sorting 5 images, the right probability drops to 1/120. Therefore, the sorting accuracies of 4candidates network and 5-candidates decrease by 61.5% and 79.1%, respectively. The regression accuracies of the two networks drop by 30.2% and 36.4%. This phenomenon affirms that more accurate sorting facilitates the counting task more. However, the regression accuracies of both candidates are still higher than that of the one without the assistant of sorting network.\n\n\nDifferent Sampling Methods\n\nIn the counting network, we employ various pooling cluster operations to extract the features. The candidate method employs the same sampling rates, yet all the layers use adaptive average pooling layers. The performance of the modified network drops by 18.8%. This result suggests that max-pooling layers are more efficient while extracting the local features. In Fig. 4, we show an example, which is an internal density map delivered from the proposed method. As the density map is noisy, the max-pooling layers extract most discriminative features. Meanwhile, the internal feature map is not supervised by the artificial density maps. Therefore, the responses of this density map are not ideal Gaussian signals. The obtained density map maintains the original semantic information. This phenomenon affirms that without the demand for regressing the instance locations with Gaussian kernels, the counting network concentrates on regressing the crowd number. Therefore, the weakly-supervised crowd counting is a promising research tendency.\n\n\nConclusions\n\nIn this paper, we propose a weakly-supervised counting method, which is trained without location supervision. Moreover, we exploit the relationship among the images to improve the counting accuracy. We propose a novel soft-label sorting network along with the counting network, which sorts the given images by their crowd numbers. We train end-to-end both the sorting network and the regression network. During training, the sorting network drives the shared backbone CNN model to obtain density-sensitive ability explicitly. Therefore, the proposed method improves the counting accuracy by using the information among crowd numbers, rather than learning from extra labels. In the proposed sorting network, we propose a more informative soft-label to capture the complexity of the sorting task. We conduct experiments on three datasets and compare the proposed weakly-supervised approach with the fully-supervised methods. Extensive experimental results demonstrate the state-of-the-art performance of our method.\n\nIn future work, we will propose a corresponding weakly-supervised benchmark to facilitate this task.\n\nFig. 1 .\n1Framework of the regression network, which contains a shared front-end and a back-end to regress the crowd number.\n\nFig. 2 .\n2Framework of the sorting network, which contains several branches to extract the pyramidal feature vectors and a comparing network to regress the order features. The Sinkhorn layer transfers the order feature to an order matrix. We employ the proposed soft-labels to train the sorting network.\n\nTable 1 .\n1The evaluation results on WorldExpo10, UCSD, and ShanghaiTech datasets. SHA represents ShanghaiTech Part A, while SHB represents ShanghaiTech Part B. The results reported on WorldExpo10 are only evaluated with the MAE metric.Label \nWorldExpo10 \nUCSD \nSHA \nSHB \nMethod \nLocation Crowd Number Sce.1 Sce.2 Sce.3 Sce.4 Sce.5 Avg. MAE MSE MAE MSE MAE MSE \nMCMM [46] \n3.4 20.6 12.9 13.0 8.1 11.6 1.07 1.35 110.2 173.2 26.4 41.3 \nSANet [2] \n2.6 13.2 9.0 13.3 3.0 8.2 1.02 1.29 67.0 104.5 8.4 13.6 \nCSRNet [15] \n2.9 11.5 8.6 16.6 3.4 8.6 1.16 1.47 68.2 115.0 10.6 16.0 \nIG-CNN [27] \n2.6 16.1 10.15 20.2 7.6 11.3 \n-\n-\n72.5 118.2 13.6 21.1 \nTEDnet [13] \n2.3 10.1 11.3 13.8 2.6 8.0 \n-\n-\n64.2 109.1 8.2 12.8 \nADCrowdNet [19] \n1.6 15.8 11.0 10.9 3.2 8.5 0.98 1.25 63.2 98.9 8.2 15.7 \nOurs \n-\n3.5 13.2 12.4 13.5 5.4 9.6 1.8 2.8 104.6 145.2 12.3 21.2 \n\n\n\nTable 2 .\n2We conduct experiments to verify the efficiency of proposed framework and soft-label on WorldExpo10.Sorting \nNetwork \n\nRegression \nNetwork \nSoft-Label MAE \nSorting \nAccuracy(%) \n-\n-\n50.6 \n-\n20.1 \n-\n-\n13.2 \n89.1 \n9.6 \n78.2 \n\nTable 1. Compared with the supervised approaches, our method achieves com-\nparable performance on Part B. While in Part A, our method has a particular \ngap with other methods. As there is a significant gap between the crowd number \ndistributions of testing set and training set of Part A. More concretely, in the \ntesting set, the mean and standard variance are 354.7 and 433.9, while in the \ntraining set, the mean and standard variance are 505.3 and 542.4. On the con-\ntrary, the crowd numbers in both sub-sets of Part B have a similar distribution. \nIn the testing set, the mean and standard variance are 95.3 and 124.1, while in \nthe training set, the mean and standard variance are 94.0 and 123.2. The non-\nlinear regression network can not solve the unbalance distributions of dataset \nonly with the crowd number labels, and needs more powerful supervision, for \ninstance, the location and perspective labels. \n\n\n\nTable 3 .\n3We evaluate several modifications to the proposed method, and report the results on WorldExpo10.Fig. 4.In the left image, we show an example frame and label its crowd number. While in the right image, we show the corresponding density map, which is an internal feature map of the network. Moreover, we label the density map with the estimation error.Backbone \nPooling \nCluster \n\nFrame \nNumber \nMAE \nSorting \nAccuracy(%) \nR3D \nMAx & Avg \n3 \n10.1 \n72.2 \nVGG \nAvg \n3 \n11.4 \n58.5 \nVGG \nMAx & Avg \n4 \n12.5 \n30.1 \nVGG \nMAx & Avg \n5 \n13.1 \n16.3 \nVGG \nMAx & Avg \n3 \n9.6 \n78.2 \n\nCrowd Number: 145 \nMAE: 0.32 \n\n\n\nCrowdnet: A deep convolutional network for dense crowd counting. L Boominathan, S S S Kruthiventi, R V Babu, ACM MultimediaBoominathan, L., Kruthiventi, S.S.S., Babu, R.V.: Crowdnet: A deep convolutional network for dense crowd counting. ACM Multimedia pp. 640-644 (2016)\n\nScale aggregation network for accurate and efficient crowd counting. X Cao, Z Wang, Y Zhao, F Su, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR. Cao, X., Wang, Z., Zhao, Y., Su, F.: Scale aggregation network for accurate and efficient crowd counting. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) pp. 757-773 (2018)\n\nPrivacy preserving crowd monitoring: Counting people without people models or tracking. A B Chan, Z S J Liang, N Vasconcelos, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR. Chan, A.B., Liang, Z.S.J., Vasconcelos, N.: Privacy preserving crowd monitoring: Counting people without people models or tracking. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) pp. 1-7 (2008)\n\nLearning spatial awareness to improve crowd counting. Z Cheng, J Li, Q Dai, X Wu, A G Hauptmann, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR. Cheng, Z., Li, J., Dai, Q., Wu, X., Hauptmann, A.G.: Learning spatial awareness to improve crowd counting. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) pp. 6152-6161 (2019)\n\nSinkhorn distances: Lightspeed computation of optimal transportation distances. M Cuturi, Neural Information Processing Systems. Cuturi, M.: Sinkhorn distances: Lightspeed computation of optimal transportation distances. Neural Information Processing Systems pp. 2292-2300 (2013)\n\nM Cuturi, O Teboul, J Vert, Differentiable ranks and sorting using optimal transport. Conference on Neural Information Processing Systems. Cuturi, M., Teboul, O., Vert, J.: Differentiable ranks and sorting using optimal transport. Conference on Neural Information Processing Systems (2019)\n\nAn aggregated multicolumn dilated convolution network for perspective-free counting. D Deb, J Ventura, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR. Deb, D., Ventura, J.: An aggregated multicolumn dilated convolution network for perspective-free counting. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) pp. 195-204 (2018)\n\nA Grover, E H Wang, A Zweig, S Ermon, Stochastic optimization of sorting networks via continuous relaxations. International Conference on Learning Representations. Grover, A., Wang, E.H., Zweig, A., Ermon, S.: Stochastic optimization of sorting networks via continuous relaxations. International Conference on Learning Repre- sentations (2019)\n\nR Guerrerogomezolmedo, B Torrejimenez, R J Lopezsastre, S Maldonadobascon, D Onororubio, Extremely overlapping vehicle counting. Iberian Conference on Pattern Recognition and Image Analysis. Guerrerogomezolmedo, R., Torrejimenez, B., Lopezsastre, R.J., Maldonadobascon, S., Onororubio, D.: Extremely overlapping vehicle counting. Iberian Conference on Pattern Recognition and Image Analysis. pp. 423-431 (2015)\n\nMobile crowd sensing and computing: The review of an emerging human-powered sensing paradigm. B Guo, Z Wang, Z Yu, Y Wang, N Y Yen, R Huang, X Zhou, ACM Computing Surveys. 48131Guo, B., Wang, Z., Yu, Z., Wang, Y., Yen, N.Y., Huang, R., Zhou, X.: Mobile crowd sensing and computing: The review of an emerging human-powered sensing paradigm. ACM Computing Surveys 48(1), 7:1-7:31 (2015)\n\nBody structure aware deep crowd counting. S Huang, X Li, Z Zhang, F Wu, S Gao, R Ji, J Han, IEEE Transactions on Image Processing. 273Huang, S., Li, X., Zhang, Z., Wu, F., Gao, S., Ji, R., Han, J.: Body structure aware deep crowd counting. IEEE Transactions on Image Processing 27(3), 1049-1059 (2018)\n\nMulti-source multi-scale counting in extremely dense crowd images. H Idrees, I Saleemi, C Seibert, M Shah, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR. Idrees, H., Saleemi, I., Seibert, C., Shah, M.: Multi-source multi-scale counting in extremely dense crowd images. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) pp. 2547-2554 (2013)\n\nCrowd counting and density estimation by trellis encoder-decoder network. X Jiang, Z Xiao, B Zhang, X Zhen, X Cao, D Doermann, L Shao, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Jiang, X., Xiao, Z., Zhang, B., Zhen, X., Cao, X., Doermann, D., Shao, L.: Crowd counting and density estimation by trellis encoder-decoder network. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019)\n\nLearning to count objects in images. V S Lempitsky, A Zisserman, NIPSLempitsky, V.S., Zisserman, A.: Learning to count objects in images. In: NIPS (2010)\n\nCsrnet: Dilated convolutional neural networks for understanding the highly congested scenes. Y Li, X Zhang, D Chen, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR. Li, Y., Zhang, X., Chen, D.: Csrnet: Dilated convolutional neural networks for understanding the highly congested scenes. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) pp. 1091-1100 (2018)\n\nS W Linderman, G Mena, H Cooper, L Paninski, J P Cunningham, Reparameterizing the birkhoff polytope for variational permutation inference. International Conference on Artificial Intelligence and Statistics. Linderman, S.W., Mena, G., Cooper, H., Paninski, L., Cunningham, J.P.: Repa- rameterizing the birkhoff polytope for variational permutation inference. Interna- tional Conference on Artificial Intelligence and Statistics. (2017)\n\nRecurrent attentive zooming for joint crowd counting and precise localization. C Liu, X Wen, Y Mu, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Liu, C., Wen, X., Mu, Y.: Recurrent attentive zooming for joint crowd counting and precise localization. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019)\n\nDecidenet: Counting varying density crowds through attention guided detection and density estimation. J Liu, C Gao, D Meng, A G Hauptmann, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR. Liu, J., Gao, C., Meng, D., Hauptmann, A.G.: Decidenet: Counting varying den- sity crowds through attention guided detection and density estimation. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) pp. 5197-5206 (2018)\n\nAdcrowdnet: An attentioninjective deformable convolutional network for crowd understanding. N Liu, Y Long, C Zou, Q Niu, L Pan, H Wu, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Liu, N., Long, Y., Zou, C., Niu, Q., Pan, L., Wu, H.: Adcrowdnet: An attention- injective deformable convolutional network for crowd understanding. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2018)\n\nContext-aware crowd counting. W Liu, M Salzmann, P Fua, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Liu, W., Salzmann, M., Fua, P.: Context-aware crowd counting. The IEEE Con- ference on Computer Vision and Pattern Recognition (CVPR) (2018)\n\nLeveraging unlabeled data for crowd counting by learning to rank. X Liu, J V De Weijer, A D Bagdanov, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR. Liu, X., De Weijer, J.V., Bagdanov, A.D.: Leveraging unlabeled data for crowd counting by learning to rank. The IEEE Conference on Computer Vision and Pat- tern Recognition (CVPR) pp. 7661-7669 (2018)\n\nPoint in, box out: Beyond counting persons in crowds. Y Liu, M Shi, Q Zhao, X Wang, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Liu, Y., Shi, M., Zhao, Q., Wang, X.: Point in, box out: Beyond counting persons in crowds. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019)\n\nDrone-based joint density map estimation, localization and tracking with spacetime multi-scale attention network. W Longyin, D Dawei, Z Pengfei, H Qinghua, W Qilong, B Liefeng, L Siwei, arxivLongyin, W., Dawei, D., Pengfei, Z., Qinghua, H., Qilong, W., Liefeng, B., Siwei, L.: Drone-based joint density map estimation, localization and tracking with space- time multi-scale attention network. arxiv (2020)\n\nFrom semi-supervised to transfer counting of crowds. C C Loy, S Gong, T Xiang, International Conference on Computer Vision. Loy, C.C., Gong, S., Xiang, T.: From semi-supervised to transfer counting of crowds. International Conference on Computer Vision pp. 2256-2263 (2013)\n\nBayesian loss for crowd count estimation with point supervision. Z Ma, X Wei, X Hong, Y Gong, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Ma, Z., Wei, X., Hong, X., Gong, Y.: Bayesian loss for crowd count estimation with point supervision. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019)\n\nUnsupervised learning of visual representations by solving jigsaw puzzles. M Noroozi, P Favaro, Noroozi, M., Favaro, P.: Unsupervised learning of visual representations by solving jigsaw puzzles. European Conference on Computer Vision pp. 69-84 (2016)\n\nV Ranjan, H Le, M Hoai, Iterative crowd counting. European Conference on Computer Vision. Ranjan, V., Le, H., Hoai, M.: Iterative crowd counting. European Conference on Computer Vision pp. 278-293 (2018)\n\nU-net: Convolutional networks for biomedical image segmentation. Medical Image Computing and Computer Assisted Intervention pp. O Ronneberger, P Fischer, T Brox, Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomed- ical image segmentation. Medical Image Computing and Computer Assisted Inter- vention pp. 234-241 (2015)\n\nD B Sam, R V Babu, Top-down feedback for crowd counting convolutional neural network. National Conference on Artificial Intelligence. Sam, D.B., Babu, R.V.: Top-down feedback for crowd counting convolutional neu- ral network. National Conference on Artificial Intelligence pp. 7323-7330 (2018)\n\nDivide and grow: Capturing huge diversity in crowd images with incrementally growing cnn. D B Sam, N Sajjan, R V Babu, M Srinivasan, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR. Sam, D.B., Sajjan, N., Babu, R.V., Srinivasan, M.: Divide and grow: Capturing huge diversity in crowd images with incrementally growing cnn. The IEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR) pp. 3618-3626 (2018)\n\nAlmost unsupervised learning for dense crowd counting. Association for Advancement of Artificial Intelligence. D B Sam, N N Sajjan, H Maurya, V B Radhakrishnan, 33Sam, D.B., Sajjan, N.N., Maurya, H., Radhakrishnan, V.B.: Almost unsupervised learning for dense crowd counting. Association for Advancement of Artificial In- telligence 33, 8868-8875 (2019)\n\nSwitching convolutional neural network for crowd counting. D B Sam, S Surya, R V Babu, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR. Sam, D.B., Surya, S., Babu, R.V.: Switching convolutional neural network for crowd counting. The IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR) pp. 4031-4039 (2017)\n\nTime-contrastive networks: Self-supervised learning from video. P Sermanet, C Lynch, Y Chebotar, J Hsu, E Jang, S Schaal, S Levine, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR. Sermanet, P., Lynch, C., Chebotar, Y., Hsu, J., Jang, E., Schaal, S., Levine, S.: Time-contrastive networks: Self-supervised learning from video. The IEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR) (2017)\n\nCrowd counting via adversarial cross-scale consistency pursuit. Z Shen, Y Xu, B Ni, M Wang, J Hu, X Yang, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Shen, Z., Xu, Y., Ni, B., Wang, M., Hu, J., Yang, X.: Crowd counting via adversarial cross-scale consistency pursuit. In: The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (June 2018)\n\nLeveraging gps-less sensing scheduling for green mobile crowd sensing. X Sheng, J Tang, X Xiao, G Xue, IEEE Internet of Things Journal. 14Sheng, X., Tang, J., Xiao, X., Xue, G.: Leveraging gps-less sensing scheduling for green mobile crowd sensing. IEEE Internet of Things Journal 1(4), 328-336 (2014)\n\nRevisiting perspective information for efficient crowd counting. M Shi, Z Yang, C Xu, Q Chen, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Shi, M., Yang, Z., Xu, C., Chen, Q.: Revisiting perspective information for ef- ficient crowd counting. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2018)\n\nZ Shi, P Mettes, C G Snoek, Counting with focus for free. International Conference on Computer Vision. Shi, Z., Mettes, P., Snoek, C.G.M.: Counting with focus for free. International Conference on Computer Vision pp. 4200-4209 (2019)\n\nCrowd counting with deep negative correlation learning. Z Shi, L Zhang, Y Liu, X Cao, Y Ye, M Cheng, G Zheng, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR. Shi, Z., Zhang, L., Liu, Y., Cao, X., Ye, Y., Cheng, M., Zheng, G.: Crowd counting with deep negative correlation learning. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) pp. 5382-5390 (2018)\n\nK Simonyan, A Zisserman, Very deep convolutional networks for large-scale image recognition. International Conference on Learning Representations. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale image recognition. International Conference on Learning Representations (2015)\n\nGenerating high-quality crowd density maps using contextual pyramid cnns. V A Sindagi, V M Patel, International Conference on Computer Vision. Sindagi, V.A., Patel, V.M.: Generating high-quality crowd density maps using contextual pyramid cnns. International Conference on Computer Vision pp. 1879- 1888 (2017)\n\nAdaptive density map generation for crowd counting. J Wan, A B Chan, Wan, J., Chan, A.B.: Adaptive density map generation for crowd counting. Inter- national Conference on Computer Vision pp. 1130-1139 (2019)\n\nLearning from synthetic data for crowd counting in the wild. Q Wang, J Gao, W Lin, Y Yuan, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Wang, Q., Gao, J., Lin, W., Yuan, Y.: Learning from synthetic data for crowd counting in the wild. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (2019)\n\nSelf-supervised spatiotemporal learning via video clip order prediction. D Xu, J Xiao, Z Zhao, J Shao, D Xie, Y Zhuang, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR. Xu, D., Xiao, J., Zhao, Z., Shao, J., Xie, D., Zhuang, Y.: Self-supervised spatiotem- poral learning via video clip order prediction. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) pp. 10334-10343 (2019)\n\nPerspectiveguided convolution networks for crowd counting. Z Yan, Y Yuan, W Zuo, X Tan, Y Wang, S Wen, E Ding, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Yan, Z., Yuan, Y., Zuo, W., Tan, X., Wang, Y., Wen, S., Ding, E.: Perspective- guided convolution networks for crowd counting. The IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR) (2019)\n\nCross-scene crowd counting via deep convolutional neural networks. C Zhang, H Li, X Wang, X Yang, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR. Zhang, C., Li, H., Wang, X., Yang, X.: Cross-scene crowd counting via deep convo- lutional neural networks. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) pp. 833-841 (2015)\n\nSingle-image crowd counting via multi-column convolutional neural network. Y Zhang, D Zhou, S Chen, S Gao, Y Ma, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR. Zhang, Y., Zhou, D., Chen, S., Gao, S., Ma, Y.: Single-image crowd counting via multi-column convolutional neural network. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) pp. 589-597 (2016)\n\nLeveraging heterogeneous auxiliary tasks to assist crowd counting. M Zhao, J Zhang, C Zhang, W Zhang, The IEEE Conference on Computer Vision and Pattern Recognition (CVPR. Zhao, M., Zhang, J., Zhang, C., Zhang, W.: Leveraging heterogeneous auxiliary tasks to assist crowd counting. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) pp. 12736-12745 (2019)\n", "annotations": {"author": "[{\"start\":\"78\",\"end\":\"150\"},{\"start\":\"151\",\"end\":\"294\"},{\"start\":\"295\",\"end\":\"363\"},{\"start\":\"364\",\"end\":\"441\"},{\"start\":\"442\",\"end\":\"658\"},{\"start\":\"659\",\"end\":\"764\"}]", "publisher": null, "author_last_name": "[{\"start\":\"84\",\"end\":\"88\"},{\"start\":\"159\",\"end\":\"161\"},{\"start\":\"299\",\"end\":\"301\"},{\"start\":\"367\",\"end\":\"369\"},{\"start\":\"451\",\"end\":\"456\"},{\"start\":\"664\",\"end\":\"668\"}]", "author_first_name": "[{\"start\":\"78\",\"end\":\"83\"},{\"start\":\"151\",\"end\":\"158\"},{\"start\":\"295\",\"end\":\"298\"},{\"start\":\"364\",\"end\":\"366\"},{\"start\":\"442\",\"end\":\"450\"},{\"start\":\"659\",\"end\":\"663\"}]", "author_affiliation": "[{\"start\":\"90\",\"end\":\"149\"},{\"start\":\"163\",\"end\":\"222\"},{\"start\":\"224\",\"end\":\"293\"},{\"start\":\"303\",\"end\":\"362\"},{\"start\":\"371\",\"end\":\"440\"},{\"start\":\"458\",\"end\":\"517\"},{\"start\":\"519\",\"end\":\"588\"},{\"start\":\"590\",\"end\":\"657\"},{\"start\":\"670\",\"end\":\"729\"},{\"start\":\"731\",\"end\":\"763\"}]", "title": "[{\"start\":\"1\",\"end\":\"75\"},{\"start\":\"765\",\"end\":\"839\"}]", "venue": null, "abstract": "[{\"start\":\"891\",\"end\":\"1990\"}]", "bib_ref": "[{\"start\":\"2154\",\"end\":\"2158\",\"attributes\":{\"ref_id\":\"b33\"}},{\"start\":\"2158\",\"end\":\"2161\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"2161\",\"end\":\"2164\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"2164\",\"end\":\"2167\",\"attributes\":{\"ref_id\":\"b37\"}},{\"start\":\"2167\",\"end\":\"2170\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"2170\",\"end\":\"2172\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"2172\",\"end\":\"2175\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"2175\",\"end\":\"2178\",\"attributes\":{\"ref_id\":\"b10\"}},{\"start\":\"2233\",\"end\":\"2237\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"2237\",\"end\":\"2240\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"2267\",\"end\":\"2271\",\"attributes\":{\"ref_id\":\"b33\"}},{\"start\":\"2271\",\"end\":\"2274\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"2274\",\"end\":\"2277\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"2458\",\"end\":\"2462\",\"attributes\":{\"ref_id\":\"b33\"}},{\"start\":\"2462\",\"end\":\"2465\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"2465\",\"end\":\"2468\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"2674\",\"end\":\"2678\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"2678\",\"end\":\"2681\",\"attributes\":{\"ref_id\":\"b44\"}},{\"start\":\"2681\",\"end\":\"2683\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"2683\",\"end\":\"2685\",\"attributes\":{\"ref_id\":\"b8\"}},{\"start\":\"3256\",\"end\":\"3259\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"3534\",\"end\":\"3538\",\"attributes\":{\"ref_id\":\"b9\"}},{\"start\":\"3563\",\"end\":\"3567\",\"attributes\":{\"ref_id\":\"b34\"}},{\"start\":\"3708\",\"end\":\"3712\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"3712\",\"end\":\"3714\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"3714\",\"end\":\"3717\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"3717\",\"end\":\"3720\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"3959\",\"end\":\"3963\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"3975\",\"end\":\"3979\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"3979\",\"end\":\"3982\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"3997\",\"end\":\"4001\",\"attributes\":{\"ref_id\":\"b36\"}},{\"start\":\"4001\",\"end\":\"4004\",\"attributes\":{\"ref_id\":\"b46\"}},{\"start\":\"4019\",\"end\":\"4023\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"4023\",\"end\":\"4026\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"4045\",\"end\":\"4049\",\"attributes\":{\"ref_id\":\"b46\"}},{\"start\":\"4069\",\"end\":\"4073\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"4073\",\"end\":\"4076\",\"attributes\":{\"ref_id\":\"b35\"}},{\"start\":\"4076\",\"end\":\"4079\",\"attributes\":{\"ref_id\":\"b44\"}},{\"start\":\"4079\",\"end\":\"4082\",\"attributes\":{\"ref_id\":\"b43\"}},{\"start\":\"4082\",\"end\":\"4085\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"5080\",\"end\":\"5084\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"5084\",\"end\":\"5086\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"5086\",\"end\":\"5088\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"5168\",\"end\":\"5172\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"5172\",\"end\":\"5174\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"5210\",\"end\":\"5213\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"6566\",\"end\":\"6570\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"6570\",\"end\":\"6573\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"6723\",\"end\":\"6727\",\"attributes\":{\"ref_id\":\"b40\"}},{\"start\":\"6959\",\"end\":\"6963\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"6963\",\"end\":\"6965\",\"attributes\":{\"ref_id\":\"b3\"}},{\"start\":\"6965\",\"end\":\"6968\",\"attributes\":{\"ref_id\":\"b13\"}},{\"start\":\"6968\",\"end\":\"6971\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"7265\",\"end\":\"7269\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"7575\",\"end\":\"7579\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"7592\",\"end\":\"7595\",\"attributes\":{\"ref_id\":\"b6\"}},{\"start\":\"7611\",\"end\":\"7615\",\"attributes\":{\"ref_id\":\"b28\"}},{\"start\":\"7665\",\"end\":\"7668\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"7668\",\"end\":\"7671\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"7671\",\"end\":\"7674\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"7854\",\"end\":\"7858\",\"attributes\":{\"ref_id\":\"b33\"}},{\"start\":\"7858\",\"end\":\"7860\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"7860\",\"end\":\"7863\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"7863\",\"end\":\"7866\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"7908\",\"end\":\"7912\",\"attributes\":{\"ref_id\":\"b27\"}},{\"start\":\"7933\",\"end\":\"7937\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"8112\",\"end\":\"8116\",\"attributes\":{\"ref_id\":\"b19\"}},{\"start\":\"8129\",\"end\":\"8133\",\"attributes\":{\"ref_id\":\"b35\"}},{\"start\":\"8146\",\"end\":\"8150\",\"attributes\":{\"ref_id\":\"b43\"}},{\"start\":\"8168\",\"end\":\"8172\",\"attributes\":{\"ref_id\":\"b44\"}},{\"start\":\"8427\",\"end\":\"8431\",\"attributes\":{\"ref_id\":\"b22\"}},{\"start\":\"8443\",\"end\":\"8447\",\"attributes\":{\"ref_id\":\"b17\"}},{\"start\":\"8447\",\"end\":\"8450\",\"attributes\":{\"ref_id\":\"b21\"}},{\"start\":\"8465\",\"end\":\"8469\",\"attributes\":{\"ref_id\":\"b36\"}},{\"start\":\"8469\",\"end\":\"8472\",\"attributes\":{\"ref_id\":\"b46\"}},{\"start\":\"8487\",\"end\":\"8491\",\"attributes\":{\"ref_id\":\"b24\"}},{\"start\":\"8491\",\"end\":\"8494\",\"attributes\":{\"ref_id\":\"b16\"}},{\"start\":\"8517\",\"end\":\"8521\",\"attributes\":{\"ref_id\":\"b46\"}},{\"start\":\"9145\",\"end\":\"9149\",\"attributes\":{\"ref_id\":\"b20\"}},{\"start\":\"9551\",\"end\":\"9555\",\"attributes\":{\"ref_id\":\"b41\"}},{\"start\":\"9875\",\"end\":\"9879\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"10145\",\"end\":\"10149\",\"attributes\":{\"ref_id\":\"b30\"}},{\"start\":\"10397\",\"end\":\"10401\",\"attributes\":{\"ref_id\":\"b23\"}},{\"start\":\"10776\",\"end\":\"10780\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"10780\",\"end\":\"10783\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"10783\",\"end\":\"10786\",\"attributes\":{\"ref_id\":\"b42\"}},{\"start\":\"10892\",\"end\":\"10896\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"10896\",\"end\":\"10898\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"10898\",\"end\":\"10900\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"11074\",\"end\":\"11078\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"11211\",\"end\":\"11215\",\"attributes\":{\"ref_id\":\"b32\"}},{\"start\":\"11348\",\"end\":\"11352\",\"attributes\":{\"ref_id\":\"b42\"}},{\"start\":\"11745\",\"end\":\"11749\",\"attributes\":{\"ref_id\":\"b25\"}},{\"start\":\"11922\",\"end\":\"11926\",\"attributes\":{\"ref_id\":\"b42\"}},{\"start\":\"12510\",\"end\":\"12513\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"12599\",\"end\":\"12603\",\"attributes\":{\"ref_id\":\"b15\"}},{\"start\":\"12655\",\"end\":\"12658\",\"attributes\":{\"ref_id\":\"b7\"}},{\"start\":\"12888\",\"end\":\"12891\",\"attributes\":{\"ref_id\":\"b5\"}},{\"start\":\"15013\",\"end\":\"15017\",\"attributes\":{\"ref_id\":\"b38\"}},{\"start\":\"15078\",\"end\":\"15081\",\"attributes\":{\"ref_id\":\"b0\"}},{\"start\":\"15081\",\"end\":\"15084\",\"attributes\":{\"ref_id\":\"b31\"}},{\"start\":\"15084\",\"end\":\"15087\",\"attributes\":{\"ref_id\":\"b39\"}},{\"start\":\"15087\",\"end\":\"15090\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"17979\",\"end\":\"17982\",\"attributes\":{\"ref_id\":\"b4\"}},{\"start\":\"22397\",\"end\":\"22401\",\"attributes\":{\"ref_id\":\"b11\"}},{\"start\":\"22408\",\"end\":\"22411\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"22430\",\"end\":\"22434\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"22624\",\"end\":\"22628\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"22628\",\"end\":\"22630\",\"attributes\":{\"ref_id\":\"b1\"}},{\"start\":\"22630\",\"end\":\"22633\",\"attributes\":{\"ref_id\":\"b14\"}},{\"start\":\"22633\",\"end\":\"22636\",\"attributes\":{\"ref_id\":\"b26\"}},{\"start\":\"22636\",\"end\":\"22639\",\"attributes\":{\"ref_id\":\"b18\"}},{\"start\":\"22639\",\"end\":\"22642\",\"attributes\":{\"ref_id\":\"b12\"}},{\"start\":\"23733\",\"end\":\"23737\",\"attributes\":{\"ref_id\":\"b29\"}},{\"start\":\"23988\",\"end\":\"23992\",\"attributes\":{\"ref_id\":\"b45\"}},{\"start\":\"25491\",\"end\":\"25494\",\"attributes\":{\"ref_id\":\"b2\"}},{\"start\":\"25866\",\"end\":\"25870\",\"attributes\":{\"ref_id\":\"b44\"}},{\"start\":\"27829\",\"end\":\"27833\",\"attributes\":{\"ref_id\":\"b42\"}}]", "figure": "[{\"start\":\"31450\",\"end\":\"31575\",\"attributes\":{\"id\":\"fig_0\"}},{\"start\":\"31576\",\"end\":\"31880\",\"attributes\":{\"id\":\"fig_1\"}},{\"start\":\"31881\",\"end\":\"32731\",\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"}},{\"start\":\"32732\",\"end\":\"33886\",\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"}},{\"start\":\"33887\",\"end\":\"34500\",\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"}}]", "paragraph": "[{\"start\":\"2006\",\"end\":\"2525\"},{\"start\":\"2527\",\"end\":\"2923\"},{\"start\":\"2925\",\"end\":\"4152\"},{\"start\":\"4154\",\"end\":\"5763\"},{\"start\":\"5765\",\"end\":\"5828\"},{\"start\":\"5830\",\"end\":\"6322\"},{\"start\":\"6374\",\"end\":\"8602\"},{\"start\":\"8604\",\"end\":\"8951\"},{\"start\":\"9002\",\"end\":\"9537\"},{\"start\":\"9539\",\"end\":\"10329\"},{\"start\":\"10331\",\"end\":\"10526\"},{\"start\":\"10552\",\"end\":\"10964\"},{\"start\":\"10966\",\"end\":\"12387\"},{\"start\":\"12389\",\"end\":\"13128\"},{\"start\":\"13130\",\"end\":\"13447\"},{\"start\":\"13458\",\"end\":\"14059\"},{\"start\":\"14061\",\"end\":\"14695\"},{\"start\":\"14718\",\"end\":\"14939\"},{\"start\":\"14941\",\"end\":\"15340\"},{\"start\":\"15371\",\"end\":\"16621\"},{\"start\":\"16646\",\"end\":\"16752\"},{\"start\":\"16778\",\"end\":\"16880\"},{\"start\":\"16900\",\"end\":\"17218\"},{\"start\":\"17262\",\"end\":\"17925\"},{\"start\":\"17950\",\"end\":\"18298\"},{\"start\":\"18300\",\"end\":\"18629\"},{\"start\":\"18631\",\"end\":\"18860\"},{\"start\":\"18896\",\"end\":\"18963\"},{\"start\":\"18992\",\"end\":\"19183\"},{\"start\":\"19198\",\"end\":\"20331\"},{\"start\":\"20408\",\"end\":\"20662\"},{\"start\":\"20909\",\"end\":\"20991\"},{\"start\":\"21066\",\"end\":\"21916\"},{\"start\":\"21918\",\"end\":\"21970\"},{\"start\":\"22010\",\"end\":\"22137\"},{\"start\":\"22222\",\"end\":\"22325\"},{\"start\":\"22341\",\"end\":\"22751\"},{\"start\":\"22778\",\"end\":\"23688\"},{\"start\":\"23711\",\"end\":\"23781\"},{\"start\":\"23853\",\"end\":\"23946\"},{\"start\":\"23976\",\"end\":\"25484\"},{\"start\":\"25486\",\"end\":\"25851\"},{\"start\":\"25853\",\"end\":\"26086\"},{\"start\":\"26105\",\"end\":\"26542\"},{\"start\":\"26544\",\"end\":\"27048\"},{\"start\":\"27078\",\"end\":\"28332\"},{\"start\":\"28361\",\"end\":\"29246\"},{\"start\":\"29277\",\"end\":\"30318\"},{\"start\":\"30334\",\"end\":\"31347\"},{\"start\":\"31349\",\"end\":\"31449\"}]", "formula": "[{\"start\":\"15341\",\"end\":\"15370\",\"attributes\":{\"id\":\"formula_0\"}},{\"start\":\"16622\",\"end\":\"16645\",\"attributes\":{\"id\":\"formula_1\"}},{\"start\":\"16753\",\"end\":\"16777\",\"attributes\":{\"id\":\"formula_2\"}},{\"start\":\"17219\",\"end\":\"17261\",\"attributes\":{\"id\":\"formula_3\"}},{\"start\":\"17926\",\"end\":\"17949\",\"attributes\":{\"id\":\"formula_4\"}},{\"start\":\"18861\",\"end\":\"18895\",\"attributes\":{\"id\":\"formula_5\"}},{\"start\":\"18964\",\"end\":\"18991\",\"attributes\":{\"id\":\"formula_6\"}},{\"start\":\"20332\",\"end\":\"20407\",\"attributes\":{\"id\":\"formula_7\"}},{\"start\":\"20663\",\"end\":\"20908\",\"attributes\":{\"id\":\"formula_8\"}},{\"start\":\"20992\",\"end\":\"21047\",\"attributes\":{\"id\":\"formula_9\"}},{\"start\":\"21971\",\"end\":\"22009\",\"attributes\":{\"id\":\"formula_10\"}},{\"start\":\"22138\",\"end\":\"22221\",\"attributes\":{\"id\":\"formula_11\"}},{\"start\":\"23782\",\"end\":\"23816\",\"attributes\":{\"id\":\"formula_12\"}},{\"start\":\"23816\",\"end\":\"23852\",\"attributes\":{\"id\":\"formula_13\"}}]", "table_ref": "[{\"start\":\"24388\",\"end\":\"24395\",\"attributes\":{\"ref_id\":\"tab_1\"}},{\"start\":\"25678\",\"end\":\"25685\",\"attributes\":{\"ref_id\":\"tab_1\"}},{\"start\":\"26247\",\"end\":\"26267\",\"attributes\":{\"ref_id\":\"tab_3\"}},{\"start\":\"26710\",\"end\":\"26717\",\"attributes\":{\"ref_id\":\"tab_3\"}},{\"start\":\"27161\",\"end\":\"27168\",\"attributes\":{\"ref_id\":\"tab_3\"}},{\"start\":\"28022\",\"end\":\"28029\",\"attributes\":{\"ref_id\":\"tab_4\"}},{\"start\":\"28452\",\"end\":\"28459\",\"attributes\":{\"ref_id\":\"tab_4\"}}]", "section_header": "[{\"start\":\"1992\",\"end\":\"2004\",\"attributes\":{\"n\":\"1\"}},{\"start\":\"6325\",\"end\":\"6337\",\"attributes\":{\"n\":\"2\"}},{\"start\":\"6340\",\"end\":\"6372\",\"attributes\":{\"n\":\"2.1\"}},{\"start\":\"8954\",\"end\":\"9000\",\"attributes\":{\"n\":\"2.2\"}},{\"start\":\"10529\",\"end\":\"10550\",\"attributes\":{\"n\":\"2.3\"}},{\"start\":\"13450\",\"end\":\"13456\",\"attributes\":{\"n\":\"3\"}},{\"start\":\"14698\",\"end\":\"14716\",\"attributes\":{\"n\":\"3.1\"}},{\"start\":\"16883\",\"end\":\"16898\",\"attributes\":{\"n\":\"3.2\"}},{\"start\":\"19186\",\"end\":\"19196\"},{\"start\":\"21049\",\"end\":\"21064\",\"attributes\":{\"n\":\"3.3\"}},{\"start\":\"22328\",\"end\":\"22339\",\"attributes\":{\"n\":\"4\"}},{\"start\":\"22754\",\"end\":\"22776\",\"attributes\":{\"n\":\"4.1\"}},{\"start\":\"23691\",\"end\":\"23709\",\"attributes\":{\"n\":\"4.2\"}},{\"start\":\"23949\",\"end\":\"23974\",\"attributes\":{\"n\":\"4.3\"}},{\"start\":\"26089\",\"end\":\"26103\",\"attributes\":{\"n\":\"4.4\"}},{\"start\":\"27051\",\"end\":\"27076\"},{\"start\":\"28335\",\"end\":\"28359\"},{\"start\":\"29249\",\"end\":\"29275\"},{\"start\":\"30321\",\"end\":\"30332\",\"attributes\":{\"n\":\"5\"}},{\"start\":\"31451\",\"end\":\"31459\"},{\"start\":\"31577\",\"end\":\"31585\"},{\"start\":\"31882\",\"end\":\"31891\"},{\"start\":\"32733\",\"end\":\"32742\"},{\"start\":\"33888\",\"end\":\"33897\"}]", "table": "[{\"start\":\"32118\",\"end\":\"32731\"},{\"start\":\"32844\",\"end\":\"33886\"},{\"start\":\"34249\",\"end\":\"34500\"}]", "figure_caption": "[{\"start\":\"31461\",\"end\":\"31575\"},{\"start\":\"31587\",\"end\":\"31880\"},{\"start\":\"31893\",\"end\":\"32118\"},{\"start\":\"32744\",\"end\":\"32844\"},{\"start\":\"33899\",\"end\":\"34249\"}]", "figure_ref": "[{\"start\":\"14730\",\"end\":\"14736\",\"attributes\":{\"ref_id\":\"fig_0\"}},{\"start\":\"24635\",\"end\":\"24645\"},{\"start\":\"24812\",\"end\":\"24822\"},{\"start\":\"24908\",\"end\":\"24914\"},{\"start\":\"29642\",\"end\":\"29648\"}]", "bib_author_first_name": "[{\"start\":\"34567\",\"end\":\"34568\"},{\"start\":\"34582\",\"end\":\"34583\"},{\"start\":\"34584\",\"end\":\"34587\"},{\"start\":\"34601\",\"end\":\"34602\"},{\"start\":\"34603\",\"end\":\"34604\"},{\"start\":\"34844\",\"end\":\"34845\"},{\"start\":\"34851\",\"end\":\"34852\"},{\"start\":\"34859\",\"end\":\"34860\"},{\"start\":\"34867\",\"end\":\"34868\"},{\"start\":\"35227\",\"end\":\"35228\"},{\"start\":\"35229\",\"end\":\"35230\"},{\"start\":\"35237\",\"end\":\"35238\"},{\"start\":\"35239\",\"end\":\"35242\"},{\"start\":\"35250\",\"end\":\"35251\"},{\"start\":\"35607\",\"end\":\"35608\"},{\"start\":\"35616\",\"end\":\"35617\"},{\"start\":\"35622\",\"end\":\"35623\"},{\"start\":\"35629\",\"end\":\"35630\"},{\"start\":\"35635\",\"end\":\"35636\"},{\"start\":\"35637\",\"end\":\"35638\"},{\"start\":\"35999\",\"end\":\"36000\"},{\"start\":\"36200\",\"end\":\"36201\"},{\"start\":\"36210\",\"end\":\"36211\"},{\"start\":\"36220\",\"end\":\"36221\"},{\"start\":\"36576\",\"end\":\"36577\"},{\"start\":\"36583\",\"end\":\"36584\"},{\"start\":\"36861\",\"end\":\"36862\"},{\"start\":\"36871\",\"end\":\"36872\"},{\"start\":\"36873\",\"end\":\"36874\"},{\"start\":\"36881\",\"end\":\"36882\"},{\"start\":\"36890\",\"end\":\"36891\"},{\"start\":\"37206\",\"end\":\"37207\"},{\"start\":\"37229\",\"end\":\"37230\"},{\"start\":\"37245\",\"end\":\"37246\"},{\"start\":\"37247\",\"end\":\"37248\"},{\"start\":\"37262\",\"end\":\"37263\"},{\"start\":\"37281\",\"end\":\"37282\"},{\"start\":\"37712\",\"end\":\"37713\"},{\"start\":\"37719\",\"end\":\"37720\"},{\"start\":\"37727\",\"end\":\"37728\"},{\"start\":\"37733\",\"end\":\"37734\"},{\"start\":\"37741\",\"end\":\"37742\"},{\"start\":\"37743\",\"end\":\"37744\"},{\"start\":\"37750\",\"end\":\"37751\"},{\"start\":\"37759\",\"end\":\"37760\"},{\"start\":\"38046\",\"end\":\"38047\"},{\"start\":\"38055\",\"end\":\"38056\"},{\"start\":\"38061\",\"end\":\"38062\"},{\"start\":\"38070\",\"end\":\"38071\"},{\"start\":\"38076\",\"end\":\"38077\"},{\"start\":\"38083\",\"end\":\"38084\"},{\"start\":\"38089\",\"end\":\"38090\"},{\"start\":\"38374\",\"end\":\"38375\"},{\"start\":\"38384\",\"end\":\"38385\"},{\"start\":\"38395\",\"end\":\"38396\"},{\"start\":\"38406\",\"end\":\"38407\"},{\"start\":\"38765\",\"end\":\"38766\"},{\"start\":\"38774\",\"end\":\"38775\"},{\"start\":\"38782\",\"end\":\"38783\"},{\"start\":\"38791\",\"end\":\"38792\"},{\"start\":\"38799\",\"end\":\"38800\"},{\"start\":\"38806\",\"end\":\"38807\"},{\"start\":\"38818\",\"end\":\"38819\"},{\"start\":\"39161\",\"end\":\"39162\"},{\"start\":\"39163\",\"end\":\"39164\"},{\"start\":\"39176\",\"end\":\"39177\"},{\"start\":\"39372\",\"end\":\"39373\"},{\"start\":\"39378\",\"end\":\"39379\"},{\"start\":\"39387\",\"end\":\"39388\"},{\"start\":\"39679\",\"end\":\"39680\"},{\"start\":\"39681\",\"end\":\"39682\"},{\"start\":\"39694\",\"end\":\"39695\"},{\"start\":\"39702\",\"end\":\"39703\"},{\"start\":\"39712\",\"end\":\"39713\"},{\"start\":\"39724\",\"end\":\"39725\"},{\"start\":\"39726\",\"end\":\"39727\"},{\"start\":\"40194\",\"end\":\"40195\"},{\"start\":\"40201\",\"end\":\"40202\"},{\"start\":\"40208\",\"end\":\"40209\"},{\"start\":\"40570\",\"end\":\"40571\"},{\"start\":\"40577\",\"end\":\"40578\"},{\"start\":\"40584\",\"end\":\"40585\"},{\"start\":\"40592\",\"end\":\"40593\"},{\"start\":\"40594\",\"end\":\"40595\"},{\"start\":\"41010\",\"end\":\"41011\"},{\"start\":\"41017\",\"end\":\"41018\"},{\"start\":\"41025\",\"end\":\"41026\"},{\"start\":\"41032\",\"end\":\"41033\"},{\"start\":\"41039\",\"end\":\"41040\"},{\"start\":\"41046\",\"end\":\"41047\"},{\"start\":\"41379\",\"end\":\"41380\"},{\"start\":\"41386\",\"end\":\"41387\"},{\"start\":\"41398\",\"end\":\"41399\"},{\"start\":\"41684\",\"end\":\"41685\"},{\"start\":\"41691\",\"end\":\"41692\"},{\"start\":\"41693\",\"end\":\"41694\"},{\"start\":\"41706\",\"end\":\"41707\"},{\"start\":\"41708\",\"end\":\"41709\"},{\"start\":\"42046\",\"end\":\"42047\"},{\"start\":\"42053\",\"end\":\"42054\"},{\"start\":\"42060\",\"end\":\"42061\"},{\"start\":\"42068\",\"end\":\"42069\"},{\"start\":\"42431\",\"end\":\"42432\"},{\"start\":\"42442\",\"end\":\"42443\"},{\"start\":\"42451\",\"end\":\"42452\"},{\"start\":\"42462\",\"end\":\"42463\"},{\"start\":\"42473\",\"end\":\"42474\"},{\"start\":\"42483\",\"end\":\"42484\"},{\"start\":\"42494\",\"end\":\"42495\"},{\"start\":\"42777\",\"end\":\"42778\"},{\"start\":\"42779\",\"end\":\"42780\"},{\"start\":\"42786\",\"end\":\"42787\"},{\"start\":\"42794\",\"end\":\"42795\"},{\"start\":\"43064\",\"end\":\"43065\"},{\"start\":\"43070\",\"end\":\"43071\"},{\"start\":\"43077\",\"end\":\"43078\"},{\"start\":\"43085\",\"end\":\"43086\"},{\"start\":\"43419\",\"end\":\"43420\"},{\"start\":\"43430\",\"end\":\"43431\"},{\"start\":\"43597\",\"end\":\"43598\"},{\"start\":\"43607\",\"end\":\"43608\"},{\"start\":\"43613\",\"end\":\"43614\"},{\"start\":\"43930\",\"end\":\"43931\"},{\"start\":\"43945\",\"end\":\"43946\"},{\"start\":\"43956\",\"end\":\"43957\"},{\"start\":\"44152\",\"end\":\"44153\"},{\"start\":\"44154\",\"end\":\"44155\"},{\"start\":\"44161\",\"end\":\"44162\"},{\"start\":\"44163\",\"end\":\"44164\"},{\"start\":\"44537\",\"end\":\"44538\"},{\"start\":\"44539\",\"end\":\"44540\"},{\"start\":\"44546\",\"end\":\"44547\"},{\"start\":\"44556\",\"end\":\"44557\"},{\"start\":\"44558\",\"end\":\"44559\"},{\"start\":\"44566\",\"end\":\"44567\"},{\"start\":\"44996\",\"end\":\"44997\"},{\"start\":\"44998\",\"end\":\"44999\"},{\"start\":\"45005\",\"end\":\"45006\"},{\"start\":\"45007\",\"end\":\"45008\"},{\"start\":\"45017\",\"end\":\"45018\"},{\"start\":\"45027\",\"end\":\"45028\"},{\"start\":\"45029\",\"end\":\"45030\"},{\"start\":\"45299\",\"end\":\"45300\"},{\"start\":\"45301\",\"end\":\"45302\"},{\"start\":\"45308\",\"end\":\"45309\"},{\"start\":\"45317\",\"end\":\"45318\"},{\"start\":\"45319\",\"end\":\"45320\"},{\"start\":\"45648\",\"end\":\"45649\"},{\"start\":\"45660\",\"end\":\"45661\"},{\"start\":\"45669\",\"end\":\"45670\"},{\"start\":\"45681\",\"end\":\"45682\"},{\"start\":\"45688\",\"end\":\"45689\"},{\"start\":\"45696\",\"end\":\"45697\"},{\"start\":\"45706\",\"end\":\"45707\"},{\"start\":\"46076\",\"end\":\"46077\"},{\"start\":\"46084\",\"end\":\"46085\"},{\"start\":\"46090\",\"end\":\"46091\"},{\"start\":\"46096\",\"end\":\"46097\"},{\"start\":\"46104\",\"end\":\"46105\"},{\"start\":\"46110\",\"end\":\"46111\"},{\"start\":\"46465\",\"end\":\"46466\"},{\"start\":\"46474\",\"end\":\"46475\"},{\"start\":\"46482\",\"end\":\"46483\"},{\"start\":\"46490\",\"end\":\"46491\"},{\"start\":\"46762\",\"end\":\"46763\"},{\"start\":\"46769\",\"end\":\"46770\"},{\"start\":\"46777\",\"end\":\"46778\"},{\"start\":\"46783\",\"end\":\"46784\"},{\"start\":\"47044\",\"end\":\"47045\"},{\"start\":\"47051\",\"end\":\"47052\"},{\"start\":\"47061\",\"end\":\"47062\"},{\"start\":\"47063\",\"end\":\"47064\"},{\"start\":\"47335\",\"end\":\"47336\"},{\"start\":\"47342\",\"end\":\"47343\"},{\"start\":\"47351\",\"end\":\"47352\"},{\"start\":\"47358\",\"end\":\"47359\"},{\"start\":\"47365\",\"end\":\"47366\"},{\"start\":\"47371\",\"end\":\"47372\"},{\"start\":\"47380\",\"end\":\"47381\"},{\"start\":\"47675\",\"end\":\"47676\"},{\"start\":\"47687\",\"end\":\"47688\"},{\"start\":\"48054\",\"end\":\"48055\"},{\"start\":\"48056\",\"end\":\"48057\"},{\"start\":\"48067\",\"end\":\"48068\"},{\"start\":\"48069\",\"end\":\"48070\"},{\"start\":\"48344\",\"end\":\"48345\"},{\"start\":\"48351\",\"end\":\"48352\"},{\"start\":\"48353\",\"end\":\"48354\"},{\"start\":\"48563\",\"end\":\"48564\"},{\"start\":\"48571\",\"end\":\"48572\"},{\"start\":\"48578\",\"end\":\"48579\"},{\"start\":\"48585\",\"end\":\"48586\"},{\"start\":\"48914\",\"end\":\"48915\"},{\"start\":\"48920\",\"end\":\"48921\"},{\"start\":\"48928\",\"end\":\"48929\"},{\"start\":\"48936\",\"end\":\"48937\"},{\"start\":\"48944\",\"end\":\"48945\"},{\"start\":\"48951\",\"end\":\"48952\"},{\"start\":\"49318\",\"end\":\"49319\"},{\"start\":\"49325\",\"end\":\"49326\"},{\"start\":\"49333\",\"end\":\"49334\"},{\"start\":\"49340\",\"end\":\"49341\"},{\"start\":\"49347\",\"end\":\"49348\"},{\"start\":\"49355\",\"end\":\"49356\"},{\"start\":\"49362\",\"end\":\"49363\"},{\"start\":\"49715\",\"end\":\"49716\"},{\"start\":\"49724\",\"end\":\"49725\"},{\"start\":\"49730\",\"end\":\"49731\"},{\"start\":\"49738\",\"end\":\"49739\"},{\"start\":\"50089\",\"end\":\"50090\"},{\"start\":\"50098\",\"end\":\"50099\"},{\"start\":\"50106\",\"end\":\"50107\"},{\"start\":\"50114\",\"end\":\"50115\"},{\"start\":\"50121\",\"end\":\"50122\"},{\"start\":\"50477\",\"end\":\"50478\"},{\"start\":\"50485\",\"end\":\"50486\"},{\"start\":\"50494\",\"end\":\"50495\"},{\"start\":\"50503\",\"end\":\"50504\"}]", "bib_author_last_name": "[{\"start\":\"34569\",\"end\":\"34580\"},{\"start\":\"34588\",\"end\":\"34599\"},{\"start\":\"34605\",\"end\":\"34609\"},{\"start\":\"34846\",\"end\":\"34849\"},{\"start\":\"34853\",\"end\":\"34857\"},{\"start\":\"34861\",\"end\":\"34865\"},{\"start\":\"34869\",\"end\":\"34871\"},{\"start\":\"35231\",\"end\":\"35235\"},{\"start\":\"35243\",\"end\":\"35248\"},{\"start\":\"35252\",\"end\":\"35263\"},{\"start\":\"35609\",\"end\":\"35614\"},{\"start\":\"35618\",\"end\":\"35620\"},{\"start\":\"35624\",\"end\":\"35627\"},{\"start\":\"35631\",\"end\":\"35633\"},{\"start\":\"35639\",\"end\":\"35648\"},{\"start\":\"36001\",\"end\":\"36007\"},{\"start\":\"36202\",\"end\":\"36208\"},{\"start\":\"36212\",\"end\":\"36218\"},{\"start\":\"36222\",\"end\":\"36226\"},{\"start\":\"36578\",\"end\":\"36581\"},{\"start\":\"36585\",\"end\":\"36592\"},{\"start\":\"36863\",\"end\":\"36869\"},{\"start\":\"36875\",\"end\":\"36879\"},{\"start\":\"36883\",\"end\":\"36888\"},{\"start\":\"36892\",\"end\":\"36897\"},{\"start\":\"37208\",\"end\":\"37227\"},{\"start\":\"37231\",\"end\":\"37243\"},{\"start\":\"37249\",\"end\":\"37260\"},{\"start\":\"37264\",\"end\":\"37279\"},{\"start\":\"37283\",\"end\":\"37293\"},{\"start\":\"37714\",\"end\":\"37717\"},{\"start\":\"37721\",\"end\":\"37725\"},{\"start\":\"37729\",\"end\":\"37731\"},{\"start\":\"37735\",\"end\":\"37739\"},{\"start\":\"37745\",\"end\":\"37748\"},{\"start\":\"37752\",\"end\":\"37757\"},{\"start\":\"37761\",\"end\":\"37765\"},{\"start\":\"38048\",\"end\":\"38053\"},{\"start\":\"38057\",\"end\":\"38059\"},{\"start\":\"38063\",\"end\":\"38068\"},{\"start\":\"38072\",\"end\":\"38074\"},{\"start\":\"38078\",\"end\":\"38081\"},{\"start\":\"38085\",\"end\":\"38087\"},{\"start\":\"38091\",\"end\":\"38094\"},{\"start\":\"38376\",\"end\":\"38382\"},{\"start\":\"38386\",\"end\":\"38393\"},{\"start\":\"38397\",\"end\":\"38404\"},{\"start\":\"38408\",\"end\":\"38412\"},{\"start\":\"38767\",\"end\":\"38772\"},{\"start\":\"38776\",\"end\":\"38780\"},{\"start\":\"38784\",\"end\":\"38789\"},{\"start\":\"38793\",\"end\":\"38797\"},{\"start\":\"38801\",\"end\":\"38804\"},{\"start\":\"38808\",\"end\":\"38816\"},{\"start\":\"38820\",\"end\":\"38824\"},{\"start\":\"39165\",\"end\":\"39174\"},{\"start\":\"39178\",\"end\":\"39187\"},{\"start\":\"39374\",\"end\":\"39376\"},{\"start\":\"39380\",\"end\":\"39385\"},{\"start\":\"39389\",\"end\":\"39393\"},{\"start\":\"39683\",\"end\":\"39692\"},{\"start\":\"39696\",\"end\":\"39700\"},{\"start\":\"39704\",\"end\":\"39710\"},{\"start\":\"39714\",\"end\":\"39722\"},{\"start\":\"39728\",\"end\":\"39738\"},{\"start\":\"40196\",\"end\":\"40199\"},{\"start\":\"40203\",\"end\":\"40206\"},{\"start\":\"40210\",\"end\":\"40212\"},{\"start\":\"40572\",\"end\":\"40575\"},{\"start\":\"40579\",\"end\":\"40582\"},{\"start\":\"40586\",\"end\":\"40590\"},{\"start\":\"40596\",\"end\":\"40605\"},{\"start\":\"41012\",\"end\":\"41015\"},{\"start\":\"41019\",\"end\":\"41023\"},{\"start\":\"41027\",\"end\":\"41030\"},{\"start\":\"41034\",\"end\":\"41037\"},{\"start\":\"41041\",\"end\":\"41044\"},{\"start\":\"41048\",\"end\":\"41050\"},{\"start\":\"41381\",\"end\":\"41384\"},{\"start\":\"41388\",\"end\":\"41396\"},{\"start\":\"41400\",\"end\":\"41403\"},{\"start\":\"41686\",\"end\":\"41689\"},{\"start\":\"41695\",\"end\":\"41704\"},{\"start\":\"41710\",\"end\":\"41718\"},{\"start\":\"42048\",\"end\":\"42051\"},{\"start\":\"42055\",\"end\":\"42058\"},{\"start\":\"42062\",\"end\":\"42066\"},{\"start\":\"42070\",\"end\":\"42074\"},{\"start\":\"42433\",\"end\":\"42440\"},{\"start\":\"42444\",\"end\":\"42449\"},{\"start\":\"42453\",\"end\":\"42460\"},{\"start\":\"42464\",\"end\":\"42471\"},{\"start\":\"42475\",\"end\":\"42481\"},{\"start\":\"42485\",\"end\":\"42492\"},{\"start\":\"42496\",\"end\":\"42501\"},{\"start\":\"42781\",\"end\":\"42784\"},{\"start\":\"42788\",\"end\":\"42792\"},{\"start\":\"42796\",\"end\":\"42801\"},{\"start\":\"43066\",\"end\":\"43068\"},{\"start\":\"43072\",\"end\":\"43075\"},{\"start\":\"43079\",\"end\":\"43083\"},{\"start\":\"43087\",\"end\":\"43091\"},{\"start\":\"43421\",\"end\":\"43428\"},{\"start\":\"43432\",\"end\":\"43438\"},{\"start\":\"43599\",\"end\":\"43605\"},{\"start\":\"43609\",\"end\":\"43611\"},{\"start\":\"43615\",\"end\":\"43619\"},{\"start\":\"43932\",\"end\":\"43943\"},{\"start\":\"43947\",\"end\":\"43954\"},{\"start\":\"43958\",\"end\":\"43962\"},{\"start\":\"44156\",\"end\":\"44159\"},{\"start\":\"44165\",\"end\":\"44169\"},{\"start\":\"44541\",\"end\":\"44544\"},{\"start\":\"44548\",\"end\":\"44554\"},{\"start\":\"44560\",\"end\":\"44564\"},{\"start\":\"44568\",\"end\":\"44578\"},{\"start\":\"45000\",\"end\":\"45003\"},{\"start\":\"45009\",\"end\":\"45015\"},{\"start\":\"45019\",\"end\":\"45025\"},{\"start\":\"45031\",\"end\":\"45044\"},{\"start\":\"45303\",\"end\":\"45306\"},{\"start\":\"45310\",\"end\":\"45315\"},{\"start\":\"45321\",\"end\":\"45325\"},{\"start\":\"45650\",\"end\":\"45658\"},{\"start\":\"45662\",\"end\":\"45667\"},{\"start\":\"45671\",\"end\":\"45679\"},{\"start\":\"45683\",\"end\":\"45686\"},{\"start\":\"45690\",\"end\":\"45694\"},{\"start\":\"45698\",\"end\":\"45704\"},{\"start\":\"45708\",\"end\":\"45714\"},{\"start\":\"46078\",\"end\":\"46082\"},{\"start\":\"46086\",\"end\":\"46088\"},{\"start\":\"46092\",\"end\":\"46094\"},{\"start\":\"46098\",\"end\":\"46102\"},{\"start\":\"46106\",\"end\":\"46108\"},{\"start\":\"46112\",\"end\":\"46116\"},{\"start\":\"46467\",\"end\":\"46472\"},{\"start\":\"46476\",\"end\":\"46480\"},{\"start\":\"46484\",\"end\":\"46488\"},{\"start\":\"46492\",\"end\":\"46495\"},{\"start\":\"46764\",\"end\":\"46767\"},{\"start\":\"46771\",\"end\":\"46775\"},{\"start\":\"46779\",\"end\":\"46781\"},{\"start\":\"46785\",\"end\":\"46789\"},{\"start\":\"47046\",\"end\":\"47049\"},{\"start\":\"47053\",\"end\":\"47059\"},{\"start\":\"47065\",\"end\":\"47070\"},{\"start\":\"47337\",\"end\":\"47340\"},{\"start\":\"47344\",\"end\":\"47349\"},{\"start\":\"47353\",\"end\":\"47356\"},{\"start\":\"47360\",\"end\":\"47363\"},{\"start\":\"47367\",\"end\":\"47369\"},{\"start\":\"47373\",\"end\":\"47378\"},{\"start\":\"47382\",\"end\":\"47387\"},{\"start\":\"47677\",\"end\":\"47685\"},{\"start\":\"47689\",\"end\":\"47698\"},{\"start\":\"48058\",\"end\":\"48065\"},{\"start\":\"48071\",\"end\":\"48076\"},{\"start\":\"48346\",\"end\":\"48349\"},{\"start\":\"48355\",\"end\":\"48359\"},{\"start\":\"48565\",\"end\":\"48569\"},{\"start\":\"48573\",\"end\":\"48576\"},{\"start\":\"48580\",\"end\":\"48583\"},{\"start\":\"48587\",\"end\":\"48591\"},{\"start\":\"48916\",\"end\":\"48918\"},{\"start\":\"48922\",\"end\":\"48926\"},{\"start\":\"48930\",\"end\":\"48934\"},{\"start\":\"48938\",\"end\":\"48942\"},{\"start\":\"48946\",\"end\":\"48949\"},{\"start\":\"48953\",\"end\":\"48959\"},{\"start\":\"49320\",\"end\":\"49323\"},{\"start\":\"49327\",\"end\":\"49331\"},{\"start\":\"49335\",\"end\":\"49338\"},{\"start\":\"49342\",\"end\":\"49345\"},{\"start\":\"49349\",\"end\":\"49353\"},{\"start\":\"49357\",\"end\":\"49360\"},{\"start\":\"49364\",\"end\":\"49368\"},{\"start\":\"49717\",\"end\":\"49722\"},{\"start\":\"49726\",\"end\":\"49728\"},{\"start\":\"49732\",\"end\":\"49736\"},{\"start\":\"49740\",\"end\":\"49744\"},{\"start\":\"50091\",\"end\":\"50096\"},{\"start\":\"50100\",\"end\":\"50104\"},{\"start\":\"50108\",\"end\":\"50112\"},{\"start\":\"50116\",\"end\":\"50119\"},{\"start\":\"50123\",\"end\":\"50125\"},{\"start\":\"50479\",\"end\":\"50483\"},{\"start\":\"50487\",\"end\":\"50492\"},{\"start\":\"50496\",\"end\":\"50501\"},{\"start\":\"50505\",\"end\":\"50510\"}]", "bib_entry": "[{\"start\":\"34502\",\"end\":\"34773\",\"attributes\":{\"id\":\"b0\"}},{\"start\":\"34775\",\"end\":\"35137\",\"attributes\":{\"matched_paper_id\":\"52955811\",\"id\":\"b1\"}},{\"start\":\"35139\",\"end\":\"35551\",\"attributes\":{\"matched_paper_id\":\"9059102\",\"id\":\"b2\"}},{\"start\":\"35553\",\"end\":\"35917\",\"attributes\":{\"matched_paper_id\":\"202577235\",\"id\":\"b3\"}},{\"start\":\"35919\",\"end\":\"36198\",\"attributes\":{\"matched_paper_id\":\"88520431\",\"id\":\"b4\"}},{\"start\":\"36200\",\"end\":\"36489\",\"attributes\":{\"id\":\"b5\"}},{\"start\":\"36491\",\"end\":\"36859\",\"attributes\":{\"matched_paper_id\":\"5056720\",\"id\":\"b6\"}},{\"start\":\"36861\",\"end\":\"37204\",\"attributes\":{\"id\":\"b7\"}},{\"start\":\"37206\",\"end\":\"37616\",\"attributes\":{\"id\":\"b8\"}},{\"start\":\"37618\",\"end\":\"38002\",\"attributes\":{\"matched_paper_id\":\"202606496\",\"id\":\"b9\"}},{\"start\":\"38004\",\"end\":\"38305\",\"attributes\":{\"matched_paper_id\":\"9552718\",\"id\":\"b10\"}},{\"start\":\"38307\",\"end\":\"38689\",\"attributes\":{\"matched_paper_id\":\"9749221\",\"id\":\"b11\"}},{\"start\":\"38691\",\"end\":\"39122\",\"attributes\":{\"matched_paper_id\":\"67856061\",\"id\":\"b12\"}},{\"start\":\"39124\",\"end\":\"39277\",\"attributes\":{\"id\":\"b13\"}},{\"start\":\"39279\",\"end\":\"39677\",\"attributes\":{\"matched_paper_id\":\"3645757\",\"id\":\"b14\"}},{\"start\":\"39679\",\"end\":\"40113\",\"attributes\":{\"id\":\"b15\"}},{\"start\":\"40115\",\"end\":\"40466\",\"attributes\":{\"matched_paper_id\":\"195473446\",\"id\":\"b16\"}},{\"start\":\"40468\",\"end\":\"40916\",\"attributes\":{\"matched_paper_id\":\"3740753\",\"id\":\"b17\"}},{\"start\":\"40918\",\"end\":\"41347\",\"attributes\":{\"matched_paper_id\":\"53957027\",\"id\":\"b18\"}},{\"start\":\"41349\",\"end\":\"41616\",\"attributes\":{\"matched_paper_id\":\"53783843\",\"id\":\"b19\"}},{\"start\":\"41618\",\"end\":\"41990\",\"attributes\":{\"matched_paper_id\":\"3787969\",\"id\":\"b20\"}},{\"start\":\"41992\",\"end\":\"42315\",\"attributes\":{\"matched_paper_id\":\"91184269\",\"id\":\"b21\"}},{\"start\":\"42317\",\"end\":\"42722\",\"attributes\":{\"id\":\"b22\"}},{\"start\":\"42724\",\"end\":\"42997\",\"attributes\":{\"matched_paper_id\":\"18676710\",\"id\":\"b23\"}},{\"start\":\"42999\",\"end\":\"43342\",\"attributes\":{\"matched_paper_id\":\"199543622\",\"id\":\"b24\"}},{\"start\":\"43344\",\"end\":\"43595\",\"attributes\":{\"id\":\"b25\"}},{\"start\":\"43597\",\"end\":\"43800\",\"attributes\":{\"id\":\"b26\"}},{\"start\":\"43802\",\"end\":\"44150\",\"attributes\":{\"id\":\"b27\"}},{\"start\":\"44152\",\"end\":\"44445\",\"attributes\":{\"id\":\"b28\"}},{\"start\":\"44447\",\"end\":\"44883\",\"attributes\":{\"matched_paper_id\":\"50785961\",\"id\":\"b29\"}},{\"start\":\"44885\",\"end\":\"45238\",\"attributes\":{\"id\":\"b30\"}},{\"start\":\"45240\",\"end\":\"45582\",\"attributes\":{\"matched_paper_id\":\"1089358\",\"id\":\"b31\"}},{\"start\":\"45584\",\"end\":\"46010\",\"attributes\":{\"matched_paper_id\":\"3997350\",\"id\":\"b32\"}},{\"start\":\"46012\",\"end\":\"46392\",\"attributes\":{\"matched_paper_id\":\"52860247\",\"id\":\"b33\"}},{\"start\":\"46394\",\"end\":\"46695\",\"attributes\":{\"matched_paper_id\":\"17249259\",\"id\":\"b34\"}},{\"start\":\"46697\",\"end\":\"47042\",\"attributes\":{\"matched_paper_id\":\"54461738\",\"id\":\"b35\"}},{\"start\":\"47044\",\"end\":\"47277\",\"attributes\":{\"id\":\"b36\"}},{\"start\":\"47279\",\"end\":\"47673\",\"attributes\":{\"matched_paper_id\":\"52243494\",\"id\":\"b37\"}},{\"start\":\"47675\",\"end\":\"47978\",\"attributes\":{\"id\":\"b38\"}},{\"start\":\"47980\",\"end\":\"48290\",\"attributes\":{\"matched_paper_id\":\"2099022\",\"id\":\"b39\"}},{\"start\":\"48292\",\"end\":\"48500\",\"attributes\":{\"id\":\"b40\"}},{\"start\":\"48502\",\"end\":\"48839\",\"attributes\":{\"matched_paper_id\":\"72941021\",\"id\":\"b41\"}},{\"start\":\"48841\",\"end\":\"49257\",\"attributes\":{\"matched_paper_id\":\"195504152\",\"id\":\"b42\"}},{\"start\":\"49259\",\"end\":\"49646\",\"attributes\":{\"matched_paper_id\":\"202577232\",\"id\":\"b43\"}},{\"start\":\"49648\",\"end\":\"50012\",\"attributes\":{\"matched_paper_id\":\"2131202\",\"id\":\"b44\"}},{\"start\":\"50014\",\"end\":\"50408\",\"attributes\":{\"matched_paper_id\":\"4545310\",\"id\":\"b45\"}},{\"start\":\"50410\",\"end\":\"50784\",\"attributes\":{\"matched_paper_id\":\"198161572\",\"id\":\"b46\"}}]", "bib_title": "[{\"start\":\"34775\",\"end\":\"34842\"},{\"start\":\"35139\",\"end\":\"35225\"},{\"start\":\"35553\",\"end\":\"35605\"},{\"start\":\"35919\",\"end\":\"35997\"},{\"start\":\"36491\",\"end\":\"36574\"},{\"start\":\"37618\",\"end\":\"37710\"},{\"start\":\"38004\",\"end\":\"38044\"},{\"start\":\"38307\",\"end\":\"38372\"},{\"start\":\"38691\",\"end\":\"38763\"},{\"start\":\"39279\",\"end\":\"39370\"},{\"start\":\"40115\",\"end\":\"40192\"},{\"start\":\"40468\",\"end\":\"40568\"},{\"start\":\"40918\",\"end\":\"41008\"},{\"start\":\"41349\",\"end\":\"41377\"},{\"start\":\"41618\",\"end\":\"41682\"},{\"start\":\"41992\",\"end\":\"42044\"},{\"start\":\"42724\",\"end\":\"42775\"},{\"start\":\"42999\",\"end\":\"43062\"},{\"start\":\"44447\",\"end\":\"44535\"},{\"start\":\"45240\",\"end\":\"45297\"},{\"start\":\"45584\",\"end\":\"45646\"},{\"start\":\"46012\",\"end\":\"46074\"},{\"start\":\"46394\",\"end\":\"46463\"},{\"start\":\"46697\",\"end\":\"46760\"},{\"start\":\"47279\",\"end\":\"47333\"},{\"start\":\"47980\",\"end\":\"48052\"},{\"start\":\"48502\",\"end\":\"48561\"},{\"start\":\"48841\",\"end\":\"48912\"},{\"start\":\"49259\",\"end\":\"49316\"},{\"start\":\"49648\",\"end\":\"49713\"},{\"start\":\"50014\",\"end\":\"50087\"},{\"start\":\"50410\",\"end\":\"50475\"}]", "bib_author": "[{\"start\":\"34567\",\"end\":\"34582\"},{\"start\":\"34582\",\"end\":\"34601\"},{\"start\":\"34601\",\"end\":\"34611\"},{\"start\":\"34844\",\"end\":\"34851\"},{\"start\":\"34851\",\"end\":\"34859\"},{\"start\":\"34859\",\"end\":\"34867\"},{\"start\":\"34867\",\"end\":\"34873\"},{\"start\":\"35227\",\"end\":\"35237\"},{\"start\":\"35237\",\"end\":\"35250\"},{\"start\":\"35250\",\"end\":\"35265\"},{\"start\":\"35607\",\"end\":\"35616\"},{\"start\":\"35616\",\"end\":\"35622\"},{\"start\":\"35622\",\"end\":\"35629\"},{\"start\":\"35629\",\"end\":\"35635\"},{\"start\":\"35635\",\"end\":\"35650\"},{\"start\":\"35999\",\"end\":\"36009\"},{\"start\":\"36200\",\"end\":\"36210\"},{\"start\":\"36210\",\"end\":\"36220\"},{\"start\":\"36220\",\"end\":\"36228\"},{\"start\":\"36576\",\"end\":\"36583\"},{\"start\":\"36583\",\"end\":\"36594\"},{\"start\":\"36861\",\"end\":\"36871\"},{\"start\":\"36871\",\"end\":\"36881\"},{\"start\":\"36881\",\"end\":\"36890\"},{\"start\":\"36890\",\"end\":\"36899\"},{\"start\":\"37206\",\"end\":\"37229\"},{\"start\":\"37229\",\"end\":\"37245\"},{\"start\":\"37245\",\"end\":\"37262\"},{\"start\":\"37262\",\"end\":\"37281\"},{\"start\":\"37281\",\"end\":\"37295\"},{\"start\":\"37712\",\"end\":\"37719\"},{\"start\":\"37719\",\"end\":\"37727\"},{\"start\":\"37727\",\"end\":\"37733\"},{\"start\":\"37733\",\"end\":\"37741\"},{\"start\":\"37741\",\"end\":\"37750\"},{\"start\":\"37750\",\"end\":\"37759\"},{\"start\":\"37759\",\"end\":\"37767\"},{\"start\":\"38046\",\"end\":\"38055\"},{\"start\":\"38055\",\"end\":\"38061\"},{\"start\":\"38061\",\"end\":\"38070\"},{\"start\":\"38070\",\"end\":\"38076\"},{\"start\":\"38076\",\"end\":\"38083\"},{\"start\":\"38083\",\"end\":\"38089\"},{\"start\":\"38089\",\"end\":\"38096\"},{\"start\":\"38374\",\"end\":\"38384\"},{\"start\":\"38384\",\"end\":\"38395\"},{\"start\":\"38395\",\"end\":\"38406\"},{\"start\":\"38406\",\"end\":\"38414\"},{\"start\":\"38765\",\"end\":\"38774\"},{\"start\":\"38774\",\"end\":\"38782\"},{\"start\":\"38782\",\"end\":\"38791\"},{\"start\":\"38791\",\"end\":\"38799\"},{\"start\":\"38799\",\"end\":\"38806\"},{\"start\":\"38806\",\"end\":\"38818\"},{\"start\":\"38818\",\"end\":\"38826\"},{\"start\":\"39161\",\"end\":\"39176\"},{\"start\":\"39176\",\"end\":\"39189\"},{\"start\":\"39372\",\"end\":\"39378\"},{\"start\":\"39378\",\"end\":\"39387\"},{\"start\":\"39387\",\"end\":\"39395\"},{\"start\":\"39679\",\"end\":\"39694\"},{\"start\":\"39694\",\"end\":\"39702\"},{\"start\":\"39702\",\"end\":\"39712\"},{\"start\":\"39712\",\"end\":\"39724\"},{\"start\":\"39724\",\"end\":\"39740\"},{\"start\":\"40194\",\"end\":\"40201\"},{\"start\":\"40201\",\"end\":\"40208\"},{\"start\":\"40208\",\"end\":\"40214\"},{\"start\":\"40570\",\"end\":\"40577\"},{\"start\":\"40577\",\"end\":\"40584\"},{\"start\":\"40584\",\"end\":\"40592\"},{\"start\":\"40592\",\"end\":\"40607\"},{\"start\":\"41010\",\"end\":\"41017\"},{\"start\":\"41017\",\"end\":\"41025\"},{\"start\":\"41025\",\"end\":\"41032\"},{\"start\":\"41032\",\"end\":\"41039\"},{\"start\":\"41039\",\"end\":\"41046\"},{\"start\":\"41046\",\"end\":\"41052\"},{\"start\":\"41379\",\"end\":\"41386\"},{\"start\":\"41386\",\"end\":\"41398\"},{\"start\":\"41398\",\"end\":\"41405\"},{\"start\":\"41684\",\"end\":\"41691\"},{\"start\":\"41691\",\"end\":\"41706\"},{\"start\":\"41706\",\"end\":\"41720\"},{\"start\":\"42046\",\"end\":\"42053\"},{\"start\":\"42053\",\"end\":\"42060\"},{\"start\":\"42060\",\"end\":\"42068\"},{\"start\":\"42068\",\"end\":\"42076\"},{\"start\":\"42431\",\"end\":\"42442\"},{\"start\":\"42442\",\"end\":\"42451\"},{\"start\":\"42451\",\"end\":\"42462\"},{\"start\":\"42462\",\"end\":\"42473\"},{\"start\":\"42473\",\"end\":\"42483\"},{\"start\":\"42483\",\"end\":\"42494\"},{\"start\":\"42494\",\"end\":\"42503\"},{\"start\":\"42777\",\"end\":\"42786\"},{\"start\":\"42786\",\"end\":\"42794\"},{\"start\":\"42794\",\"end\":\"42803\"},{\"start\":\"43064\",\"end\":\"43070\"},{\"start\":\"43070\",\"end\":\"43077\"},{\"start\":\"43077\",\"end\":\"43085\"},{\"start\":\"43085\",\"end\":\"43093\"},{\"start\":\"43419\",\"end\":\"43430\"},{\"start\":\"43430\",\"end\":\"43440\"},{\"start\":\"43597\",\"end\":\"43607\"},{\"start\":\"43607\",\"end\":\"43613\"},{\"start\":\"43613\",\"end\":\"43621\"},{\"start\":\"43930\",\"end\":\"43945\"},{\"start\":\"43945\",\"end\":\"43956\"},{\"start\":\"43956\",\"end\":\"43964\"},{\"start\":\"44152\",\"end\":\"44161\"},{\"start\":\"44161\",\"end\":\"44171\"},{\"start\":\"44537\",\"end\":\"44546\"},{\"start\":\"44546\",\"end\":\"44556\"},{\"start\":\"44556\",\"end\":\"44566\"},{\"start\":\"44566\",\"end\":\"44580\"},{\"start\":\"44996\",\"end\":\"45005\"},{\"start\":\"45005\",\"end\":\"45017\"},{\"start\":\"45017\",\"end\":\"45027\"},{\"start\":\"45027\",\"end\":\"45046\"},{\"start\":\"45299\",\"end\":\"45308\"},{\"start\":\"45308\",\"end\":\"45317\"},{\"start\":\"45317\",\"end\":\"45327\"},{\"start\":\"45648\",\"end\":\"45660\"},{\"start\":\"45660\",\"end\":\"45669\"},{\"start\":\"45669\",\"end\":\"45681\"},{\"start\":\"45681\",\"end\":\"45688\"},{\"start\":\"45688\",\"end\":\"45696\"},{\"start\":\"45696\",\"end\":\"45706\"},{\"start\":\"45706\",\"end\":\"45716\"},{\"start\":\"46076\",\"end\":\"46084\"},{\"start\":\"46084\",\"end\":\"46090\"},{\"start\":\"46090\",\"end\":\"46096\"},{\"start\":\"46096\",\"end\":\"46104\"},{\"start\":\"46104\",\"end\":\"46110\"},{\"start\":\"46110\",\"end\":\"46118\"},{\"start\":\"46465\",\"end\":\"46474\"},{\"start\":\"46474\",\"end\":\"46482\"},{\"start\":\"46482\",\"end\":\"46490\"},{\"start\":\"46490\",\"end\":\"46497\"},{\"start\":\"46762\",\"end\":\"46769\"},{\"start\":\"46769\",\"end\":\"46777\"},{\"start\":\"46777\",\"end\":\"46783\"},{\"start\":\"46783\",\"end\":\"46791\"},{\"start\":\"47044\",\"end\":\"47051\"},{\"start\":\"47051\",\"end\":\"47061\"},{\"start\":\"47061\",\"end\":\"47072\"},{\"start\":\"47335\",\"end\":\"47342\"},{\"start\":\"47342\",\"end\":\"47351\"},{\"start\":\"47351\",\"end\":\"47358\"},{\"start\":\"47358\",\"end\":\"47365\"},{\"start\":\"47365\",\"end\":\"47371\"},{\"start\":\"47371\",\"end\":\"47380\"},{\"start\":\"47380\",\"end\":\"47389\"},{\"start\":\"47675\",\"end\":\"47687\"},{\"start\":\"47687\",\"end\":\"47700\"},{\"start\":\"48054\",\"end\":\"48067\"},{\"start\":\"48067\",\"end\":\"48078\"},{\"start\":\"48344\",\"end\":\"48351\"},{\"start\":\"48351\",\"end\":\"48361\"},{\"start\":\"48563\",\"end\":\"48571\"},{\"start\":\"48571\",\"end\":\"48578\"},{\"start\":\"48578\",\"end\":\"48585\"},{\"start\":\"48585\",\"end\":\"48593\"},{\"start\":\"48914\",\"end\":\"48920\"},{\"start\":\"48920\",\"end\":\"48928\"},{\"start\":\"48928\",\"end\":\"48936\"},{\"start\":\"48936\",\"end\":\"48944\"},{\"start\":\"48944\",\"end\":\"48951\"},{\"start\":\"48951\",\"end\":\"48961\"},{\"start\":\"49318\",\"end\":\"49325\"},{\"start\":\"49325\",\"end\":\"49333\"},{\"start\":\"49333\",\"end\":\"49340\"},{\"start\":\"49340\",\"end\":\"49347\"},{\"start\":\"49347\",\"end\":\"49355\"},{\"start\":\"49355\",\"end\":\"49362\"},{\"start\":\"49362\",\"end\":\"49370\"},{\"start\":\"49715\",\"end\":\"49724\"},{\"start\":\"49724\",\"end\":\"49730\"},{\"start\":\"49730\",\"end\":\"49738\"},{\"start\":\"49738\",\"end\":\"49746\"},{\"start\":\"50089\",\"end\":\"50098\"},{\"start\":\"50098\",\"end\":\"50106\"},{\"start\":\"50106\",\"end\":\"50114\"},{\"start\":\"50114\",\"end\":\"50121\"},{\"start\":\"50121\",\"end\":\"50127\"},{\"start\":\"50477\",\"end\":\"50485\"},{\"start\":\"50485\",\"end\":\"50494\"},{\"start\":\"50494\",\"end\":\"50503\"},{\"start\":\"50503\",\"end\":\"50512\"}]", "bib_venue": "[{\"start\":\"34502\",\"end\":\"34565\"},{\"start\":\"34873\",\"end\":\"34941\"},{\"start\":\"35265\",\"end\":\"35333\"},{\"start\":\"35650\",\"end\":\"35718\"},{\"start\":\"36009\",\"end\":\"36046\"},{\"start\":\"36228\",\"end\":\"36337\"},{\"start\":\"36594\",\"end\":\"36662\"},{\"start\":\"36899\",\"end\":\"37023\"},{\"start\":\"37295\",\"end\":\"37395\"},{\"start\":\"37767\",\"end\":\"37788\"},{\"start\":\"38096\",\"end\":\"38133\"},{\"start\":\"38414\",\"end\":\"38482\"},{\"start\":\"38826\",\"end\":\"38895\"},{\"start\":\"39124\",\"end\":\"39159\"},{\"start\":\"39395\",\"end\":\"39463\"},{\"start\":\"39740\",\"end\":\"39884\"},{\"start\":\"40214\",\"end\":\"40283\"},{\"start\":\"40607\",\"end\":\"40675\"},{\"start\":\"41052\",\"end\":\"41121\"},{\"start\":\"41405\",\"end\":\"41474\"},{\"start\":\"41720\",\"end\":\"41788\"},{\"start\":\"42076\",\"end\":\"42145\"},{\"start\":\"42317\",\"end\":\"42429\"},{\"start\":\"42803\",\"end\":\"42846\"},{\"start\":\"43093\",\"end\":\"43162\"},{\"start\":\"43344\",\"end\":\"43417\"},{\"start\":\"43621\",\"end\":\"43685\"},{\"start\":\"43802\",\"end\":\"43928\"},{\"start\":\"44171\",\"end\":\"44284\"},{\"start\":\"44580\",\"end\":\"44648\"},{\"start\":\"44885\",\"end\":\"44994\"},{\"start\":\"45327\",\"end\":\"45395\"},{\"start\":\"45716\",\"end\":\"45784\"},{\"start\":\"46118\",\"end\":\"46187\"},{\"start\":\"46497\",\"end\":\"46528\"},{\"start\":\"46791\",\"end\":\"46860\"},{\"start\":\"47072\",\"end\":\"47145\"},{\"start\":\"47389\",\"end\":\"47457\"},{\"start\":\"47700\",\"end\":\"47820\"},{\"start\":\"48078\",\"end\":\"48121\"},{\"start\":\"48292\",\"end\":\"48342\"},{\"start\":\"48593\",\"end\":\"48662\"},{\"start\":\"48961\",\"end\":\"49029\"},{\"start\":\"49370\",\"end\":\"49439\"},{\"start\":\"49746\",\"end\":\"49814\"},{\"start\":\"50127\",\"end\":\"50195\"},{\"start\":\"50512\",\"end\":\"50580\"}]"}}}, "year": 2023, "month": 12, "day": 17}