{"id": 220686452, "updated": "2023-10-06 13:23:47.488", "metadata": {"title": "Instance-aware Self-supervised Learning for Nuclei Segmentation", "authors": "[{\"first\":\"Xinpeng\",\"last\":\"Xie\",\"middle\":[]},{\"first\":\"Jiawei\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Yuexiang\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Linlin\",\"last\":\"Shen\",\"middle\":[]},{\"first\":\"Kai\",\"last\":\"Ma\",\"middle\":[]},{\"first\":\"Yefeng\",\"last\":\"Zheng\",\"middle\":[]}]", "venue": "Medical Image Computing and Computer Assisted Intervention \u2013 MICCAI 2020", "journal": "Medical Image Computing and Computer Assisted Intervention \u2013 MICCAI 2020", "publication_date": {"year": 2020, "month": 7, "day": 22}, "abstract": "Due to the wide existence and large morphological variances of nuclei, accurate nuclei instance segmentation is still one of the most challenging tasks in computational pathology. The annotating of nuclei instances, requiring experienced pathologists to manually draw the contours, is extremely laborious and expensive, which often results in the deficiency of annotated data. The deep learning based segmentation approaches, which highly rely on the quantity of training data, are difficult to fully demonstrate their capacity in this area. In this paper, we propose a novel self-supervised learning framework to deeply exploit the capacity of widely-used convolutional neural networks (CNNs) on the nuclei instance segmentation task. The proposed approach involves two sub-tasks (i.e., scale-wise triplet learning and count ranking), which enable neural networks to implicitly leverage the prior-knowledge of nuclei size and quantity, and accordingly mine the instance-aware feature representations from the raw data. Experimental results on the publicly available MoNuSeg dataset show that the proposed self-supervised learning approach can remarkably boost the segmentation accuracy of nuclei instance---a new state-of-the-art average Aggregated Jaccard Index (AJI) of 70.63%, is achieved by our self-supervised ResUNet-101. To our best knowledge, this is the first work focusing on the self-supervised learning for instance segmentation.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2007.11186", "mag": "3045051936", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/miccai/XieCLSMZ20a", "doi": "10.1007/978-3-030-59722-1_33"}}, "content": {"source": {"pdf_hash": "22b6e4d554b3220f88eccc850efea56ecca177ee", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2007.11186v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2007.11186", "status": "GREEN"}}, "grobid": {"id": "80a4e85dfcac0789051846b47a8e6dc97392937f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/22b6e4d554b3220f88eccc850efea56ecca177ee.txt", "contents": "\nInstance-aware Self-supervised Learning for Nuclei Segmentation\n22 Jul 2020\n\nXinpeng Xie \nComputer Vision Institute\nShenzhen University\nShenzhenChina\n\nJiawei Chen \nTencent Jarvis Lab\nShenzhenChina\n\nYuexiang Li \nTencent Jarvis Lab\nShenzhenChina\n\nLinlin Shen llshen@szu.edu.cn \nComputer Vision Institute\nShenzhen University\nShenzhenChina\n\nKai Ma \nTencent Jarvis Lab\nShenzhenChina\n\nYefeng Zheng \nTencent Jarvis Lab\nShenzhenChina\n\nInstance-aware Self-supervised Learning for Nuclei Segmentation\n22 Jul 2020Self-supervised Learning \u00b7 Nuclei Instance Segmentation \u00b7 Histopathological Images\nDue to the wide existence and large morphological variances of nuclei, accurate nuclei instance segmentation is still one of the most challenging tasks in computational pathology. The annotating of nuclei instances, requiring experienced pathologists to manually draw the contours, is extremely laborious and expensive, which often results in the deficiency of annotated data. The deep learning based segmentation approaches, which highly rely on the quantity of training data, are difficult to fully demonstrate their capacity in this area. In this paper, we propose a novel self-supervised learning framework to deeply exploit the capacity of widely-used convolutional neural networks (CNNs) on the nuclei instance segmentation task. The proposed approach involves two sub-tasks (i.e., scale-wise triplet learning and count ranking), which enable neural networks to implicitly leverage the prior-knowledge of nuclei size and quantity, and accordingly mine the instance-aware feature representations from the raw data. Experimental results on the publicly available MoNuSeg dataset show that the proposed self-supervised learning approach can remarkably boost the segmentation accuracy of nuclei instance-a new state-of-the-art average Aggregated Jaccard Index (AJI) of 70.63%, is achieved by our self-supervised ResUNet-101. To our best knowledge, this is the first work focusing on the self-supervised learning for instance segmentation.\n\nIntroduction\n\nNuclei instance segmentation provides not only location and density information but also rich morphology features (e.g., magnitude and the cytoplasmic ratio) for the tumor diagnosis and related treatment procedures [1]. To this end, many researches have been proposed to establish automated systems for accurate nuclei segmentation. For examples, Xie et al. [14] utilized Mask R-CNN to directly localize and segment the nuclei instances in histopathological images. Oda et al. [9] proposed a deep learning method called Boundary-Enhanced Segmentation Network (BESNet) to segment cell instances from pathological images. The proposed BESNet had similar architecture to U-Net but utilized two decoders to enhance cell boundaries and segment entire cells, respectively. Inspired by [9], Zhou et al. [15] proposed a multi-level information aggregation module to fuse the features extracted by the two decoders of BESNet. The proposed framework, namely Contour-aware Informative Aggregation Network (CIA-Net), achieved an excellent accuracy of nuclei instance segmentation and won the first prize on the Multi-Organ-Nuclei-Segmentation (MoNuSeg) challenge. Although deep learning based approaches achieve outstanding segmentation accuracy for nuclei instances, they share a common challenge for further improvements-the deficiency of annotated data. Due to the wide existence and large morphological variances of nuclei, the annotating of nuclei instances requires experienced physicians to repetitively investigate the histopathological images and carefully draw the contours, which is extremely laborious and expensive. Therefore, the performance of deep learning based approaches suffers from the limited quantity of annotated histopathological images.\n\nSelf-supervised learning, as a solution to loose the requirement of manual annotations for neural networks, attracts increasing attentions from the community. The pipeline usually consists of two steps: 1) pre-train the network model on a proxy task with a large unlabeled dataset; 2) fine-tune the pre-trained network for the specific target task with a small set of annotated data. Recent studies have validated the effectiveness of self-supervised learning on multiple tasks of medical image processing such as brain area segmentation [13], brain tumor segmentation [17] and organ segmentation [16]. However, few studies focused on the topic of instance segmentation, which is a totally different area from the semantic segmentation [11]. The neural network performing instance segmentation needs to identify not only the object category but also the object instance for each pixel belonging to. Therefore, the self-supervised learning approach is required to implicitly learn to be self-aware of object instance from the raw data for more accurate nuclei segmentation.\n\nIn this paper, we propose an instance-aware self-supervised learning approach to loose the requirement of manual annotations in deep convolutional neural networks. The proposed self-supervised proxy task involves two sub-tasks (i.e., scale-wise triplet learning and count ranking), which enforce the neural network to autonomously learn the prior-knowledge of nuclei size and quantity by deeply exploiting the rich information contained in the raw data. The publicly available MoNuSeg dataset is adopted to evaluate the improvements yielded by the proposed proxy task. The experimental results demonstrate that our selfsupervised learning approach can significantly boost the accuracy of nuclei instance segmentation-a new state-of-the-art average Aggravated Jaccard Index (AJI) of 70.63% is achieved by the self-supervised ResUNet-101. \n\n\nMethod\n\nIn this section, the proposed instance-aware self-supervised proxy tasks are introduced in details.\n\n\nImage Manipulation\n\nMultiple example learning (e.g., pair-wise and triplet learning), which aims to learn an embedding space that captures dissimilarity among data points, is adopted to encourage neural networks to implicitly learn the characteristics of nuclei instances (i.e., nuclei size and quantity). An example of generating triplet samples for a given histopathological image is shown in Fig. 1.\n\nNuclei Size. Specifically, as presented in Fig. 1, for a histopathological image of 1000 \u00d7 1000 pixels from the MoNuSeg dataset, we first crop a patch with 768 \u00d7 768 pixels (i.e., the red square) as the anchor. Next, we generate a positive sample containing nuclei with similar sizes to the anchor patch by cropping an adjacent patch (i.e., the blue square) with the same size (768 \u00d7 768 pixels) from the histopathological image. To better embed the information of nuclei size into self-supervised learning, a negative sample containing nucleus with larger sizes is generated-a sub-patch (i.e., the green square) random cropped from the positive sample and resized to 768 \u00d7 768 pixels. To increase the diversity of negative samples, the scale of green square is randomly selected from a pool {512 \u00d7 512, 256 \u00d7 256, 128 \u00d7 128, 64 \u00d7 64} for each triplet. The anchor, positive and negative samples form a standard triplet data, which is used for the proxy task in self-supervised learning. Nuclei Quantity. The positive and negative samples not only contain nuclei with different sizes (S), but also different quantities of nuclei (C)-the number of nuclei in negative samples is always lower than that of positive samples. Therefore, we propose to adopt a pair-wise count ranking metric to reflect the difference of nuclei quantity during self-supervised learning.\nAnchor Positive Negative ! \" # \" $ % &' ( ! , ) , * + * ) \" - % ./ ( ! , * + f (\u00b7)\n\nSelf-supervised Approach with Triplet Learning and Ranking\n\nWith the triplet samples (i.e., anchor A, positive P , and negative N ), we formulate two self-supervised proxy tasks to pre-train the neural networks for nuclei instance segmentation. The pipeline of the proposed proxy task is illustrated in Fig. 2, which consists of three shared-weight encoders supervised by two lossesscale-wise triplet loss and count ranking loss. As aforementioned, the scale-wise triplet learning and count ranking aim to extract features related to knowledge of nuclei size and quantity, respectively. The shared-weight encoders embed the triplet samples into a latent feature space (Z), which can be formulated as:\nE A : A \u2192 z a , E P : P \u2192 z p , E N : N \u2192 z n ,\nwhere z a , z p , and z n are 128-d features.\n\nProxy Task 1: Scale-wise Triplet Learning. The triplet learning [12] encourages samples from the same class to be closer and pushes apart samples from different classes in the embedding space. The proposed approach labels the samples cropped in the same scale with the same class, while treating samples in different scales as different classes. Therefore, the scale-wise triplet loss (L ST ) for the embedded triplet features (z a , z p , z n ) can be formulated as:\nL ST (z a , z p , z n ) = max (0, d (z a , z p ) \u2212 d (z a , z n ) + m 1 )(1)\nwhere m 1 is a margin (which is empirically set to 1.0); d(.) is the squared L 2 distance between two features. Regularized by the triplet loss, the network narrows down the perceptional distance between anchor and positive samples in the feature space and enlarge the semantic dissimilarity (i.e., nuclei size) between the anchor and negative samples.\n\nProxy Task 2: Count Ranking. Based on aforementioned observation-the positive sample always contains more nuclei than the negative one, we propose a pair-wise count ranking loss (L CR ) to enforce the network to identify the sample containing a larger crowd of nuclei. A mapping function f is first applied to the embedded features (z p , z n ) to arrive at a scalar value whose relative rank is known. And in our experiment, f is implemented by a fully convolution layer. Then, the loss function for the embedded features (z p , z n ) can be defined as:\nL CR = max(0, f (z n ) \u2212 f (z p ) + m 2 )(2)\nwhere m 2 is a margin (which is empirically set to 1.0). The well-trained network is implicitly regularized to be aware of the nuclei quantity. To further illustrate this, the features extracted from last deconvolution layer are visualized as the nuclei density maps in Fig. 3. It can be observed that the density maps of negative samples cropped from positive samples are ordered and sparse, which demonstrate the relative rank created by the mapping function f and the effectiveness of our count ranking loss. Objective. With the previously defined scale-wise triplet loss L ST and count ranking loss L CR , the full objective L for our self-supervised approach is summarized as:\nL = L ST + L CR .(3)\nFine-tuning on Target Task. The recently proposed one-stage framework [2] is adopted to perform nuclei instance segmentation. The framework has a U-shape architecture [10], which classifies each pixel to three categories (i.e., nuclei body, nuclei boundary and background). We pre-train the encoder of onestage framework with the proposed proxy tasks to extract instance-aware feature representations and then transfer the pre-trained weights to the target task with a randomly initialized decoder. The widely-used ResNet-101 [4] is adopted as the backbone of the encoder. Henceforth, the one-stage framework adopted in this study is referred as ResUNet-101.\n\n\nExperiments\n\nThe proposed instance-aware self-supervised learning approach is evaluated on the publicly available MoNuSeg dataset to demonstrate its effectiveness on improving segmentation accuracy of nuclei instance.\n\n\nDatasets\n\nMoNuSeg 2018 Dataset [7]. The dataset consists of diverse H&E stained tissue images captured from seven different organs (e.g., breast, liver, kidney, prostate, bladder, colon and stomach), which were collected from 18 institutes. The dataset has a public training set and a public test set. The training set contains 30 histopathological images with hand-annotated nuclei, while the test set consists of 14 images. The resolution of the histopathological images is 1000\u00d7 1000 pixels. In our experiments, we separate the public training set to training and validation sets according to the ratio of 80:20. To evaluate the segmentation accuracy of nuclei instance, we adopt the Aggregated Jaccard Index (AJI) [7] as the metric. The AJI [7] is proved to be a more suitable metric to evaluate the segmentation performance at the object level, which involves matching per ground truth nucleus to one segmented necleus by maximizing the Jaccard index. \n\n\nPerformance Evaluation\n\nWe first evaluate the improvement yielded by the proposed self-supervised learning proxy tasks to the performance of instance segmentation. The nuclei instance segmentation results of ResUNet-101 trained with different strategies are presented in Fig. 4. It can be observed that pre-trained ResUNet-101 produces more plausible segmentation of nuclei instances, especially for the overlapping nuclei marked by yellow arrows, compared to the one trained from scratch. For further evaluation, the AJIs of the two approaches and our self-supervised learning framework finetuned with different amounts of labeled data are evaluated and presented in Table 1. Due to the gap between natural and medical images, the ImageNet pre-trained weights yield marginal improvement (e.g., +0.54% with 100% annotations) to train-from-scratch. It can be observed that our selfsupervised learning proxy tasks significantly and consistently improve the AJI under all conditions, especially with the extremely small quantity (e.g., 10%) of annotations, i.e., +11.43% higher than train-from-scratch. For comprehensive quantitative analysis, ResUNet-101 trained with different strategies, including state-of-the-art self-supervised learning approaches [8,3,6], is evaluated and the results are presented in Table 2. The accuracy of the top-5 teams on MoNuSeg 2018 Segmentation Challenge leaderboard 3 are also involved for comparison. As shown in Table 2, the proposed self-supervised learning pretrained model significantly boosts the accuracy of nuclei instance segmentation (+5.34%), compared with train-from-scratch. We believe that the improvement comes from the prior knowledge of nuclei size and quantity that is learned implicitly in the self-supervised proxy tasks, since our approach outperforms all the listed self-supervised methods, which do not take the instance-related knowledge into consideration. Our self-supervised pre-trained ResUNet-101 is also observed to surpass the winner on the leaderboard (i.e., CIA-Net [15]), which leads to a new state-of-the-art, i.e., 70.63%, on the MoNuSeg test set. It is worthwhile to mention that our framework has a much lower computational complexity and fewer network parameters, compared to the CIA-Net, which utilizes DenseNet [5] as the encoder and has two decoders for nuclei body and boundary, respectively.\n\nAblation Study. To assess the accuracy improvement yielded by each component of our self-supervised learning proxy tasks, we conduct an ablation study. The experimental results are presented in Table 3. Compared to train-fromscratch, fine-tuning from the L ST -only and L CR -only pre-trained weights improves the segmentation accuracy by +4.35% and +4.80%, respectively. Since jointly pre-training on the two sub-tasks (i.e., L ST and L CR ) increases the diversity of feature representation learned by neural networks, it provides the highest improvement, i.e., +5.34%.\n\nValidation on Another Dataset. The evaluation results on the Computational Precision Medicine (CPM) dataset can be found in Table 4. \n\n\nConclusion\n\nIn this paper, we proposed an instance-aware self-supervised learning framework for nuclei segmentation. The proposed proxy consists of two sub-tasks (i.e., scale-wise triplet learning and count ranking), which enable the neural network to implicitly acquire the knowledge of nuclei size and quantity. The proposed self-supervised learning proxy tasks were evaluated on the publicly available MoNuSeg dataset and a new state-of-the-art AJI (i.e., 70.63%) was achieved.\n\nFig. 1 .\n1Image manipulation of a histopathological image. The S(.) and C(.) represent the average nuclei size and the number of nuclei of generated samples (i.e., anchor A, positive P , and negative N ), respectively. The two equations reflect heuristic relationship among the samples.\n\nFig. 2 .\n2The pipeline of the proposed self-supervised proxy tasks. The knowledge of nuclei size and quantity is implicitly captured by the scale-wise triplet learning and pair-wise count ranking, respectively.\n\nFig. 3 .\n3Density maps of samples containing different quantities of nuclei. The even columns on top represents the negative samples cropped from the positive samples (odd columns). The neural network realizes the variation of nucleus quantity between the positive and negative samples, and activates dense areas in the positive one.\n\nFig. 4 .\n4The nuclei instance segmentation results produced by ResUNet-101 train-fromscratch and self-supervised learning (SSL) pre-trained, respectively.\n\nTable 1 .\n1AJI (%) for models finetuned with different amounts of labeled data on the MoNuSeg 2018 test set.ResUNet-101 \nAJI(%) \n100% 70% 50% 30% 10% \nTrain-from-scratch \n65.29 60.33 51.45 44.32 43.58 \nImageNet Pre-trained 65.83 62.60 53.54 48.57 48.31 \nSSL (Ours) \n70.63 68.87 62.34 60.31 55.01 \n\n\n\nTable 2 .\n2AJI (%) for ResUNet-101 trained with different strategies and the top-5 approaches on the MoNuSeg 2018 test set.Training strategy (ResUNet-101) MoNuSeg 2018 Leaderboard \nTrain-from-scratch \n65.29 \nNavid Alemi \n67.79 \nImageNet Pre-trained \n65.83 \nYunzhi \n67.88 \nJigsaw Puzzles [8] \n66.68 \nPku.hzq \n68.52 \nRotNet [3] \n67.61 \nBUPT.J.LI \n68.68 \nColorMe [6] \n67.94 \nCIA-Net [15] \n69.07 \nSSL (Ours) \n70.63 \n\nTable 3. Performance produced by different self-supervised proxy tasks on the \nMoNuSeg 2018 test set. \n\nSetup \nResUnet \n+LST +LCR \n+LST +LCR \nAJI (%) \n65.29 \n69.64 \n70.09 \n70.63 \n\n\n\nTable 4 .\n4Dice score (%) and AJI (%) on the Computational Precision Medicine (CPM) dataset*. A 5-fold cross validation is conducted. Apart from the AJI, we also evaluate the Dice score, which proposed by the CPM competition. An average Dice of 86.36% is achieved by our self-supervised ResUNet-101, which is comparable to the winner of CPM 2018 competition (i.e., 87.00%).* https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=37224869Fold 1 \nFold 2 \nFold 3 \nFold 4 \nFold 5 \nAverage \n\nDice score \nTrain-from-scratch 85.29 \n83.99 \n84.12 \n82.89 \n86.05 \n84.47 \nSSL (Ours) \n86.54 \n85.18 \n85.84 \n86.08 \n88.14 \n86.36 \nAJI \nTrain-from-scratch 74.43 \n72.79 \n72.80 \n71.03 \n75.60 \n73.33 \nSSL (Ours) \n76.34 \n74.56 \n75.37 \n75.84 \n78.83 \n76.19 \n\n\nhttps://monuseg.grand-challenge.org/Results/\n\nInvariant delineation of nuclear architecture in glioblastoma multiforme for clinical and molecular association. H Chang, J Han, A Borowsky, L Loss, J W Gray, P T Spellman, B Parvin, IEEE Transactions on Medical Imaging. 324Chang, H., Han, J., Borowsky, A., Loss, L., Gray, J.W., Spellman, P.T., Parvin, B.: Invariant delineation of nuclear architecture in glioblastoma multiforme for clinical and molecular association. IEEE Transactions on Medical Imaging 32(4), 670-682 (2013)\n\nA deep learning algorithm for one-step contour aware nuclei segmentation of histopathology images. Y Cui, G Zhang, Z Liu, Z Xiong, J Hu, Medical & Biological Engineering & Computing. 57Cui, Y., Zhang, G., Liu, Z., Xiong, Z., Hu, J.: A deep learning algorithm for one-step contour aware nuclei segmentation of histopathology images. Medical & Biological Engineering & Computing 57, 2027-2043 (2019)\n\nUnsupervised representation learning by predicting image rotations. S Gidaris, P Singh, N Komodakis, International Conference on Learning Representations. Gidaris, S., Singh, P., Komodakis, N.: Unsupervised representation learning by pre- dicting image rotations. In: International Conference on Learning Representations (2018)\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, IEEE Conference on Computer Vision and Pattern Recognition. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: IEEE Conference on Computer Vision and Pattern Recognition. pp. 770-778 (2016)\n\nDensely connected convolutional networks. G Huang, Z Liu, L Van Der Maaten, K Q Weinberger, IEEE Conference on Computer Vision and Pattern Recognition. Huang, G., Liu, Z., van der Maaten, L., Weinberger, K.Q.: Densely connected convolutional networks. In: IEEE Conference on Computer Vision and Pattern Recognition. pp. 4700-4708 (2017)\n\nA multi-task self-supervised learning framework for scopy images. Y Li, J Chen, Y Zheng, IEEE International Symposium on Biomedical Imaging. Li, Y., Chen, J., Zheng, Y.: A multi-task self-supervised learning framework for scopy images. In: IEEE International Symposium on Biomedical Imaging (2020)\n\nSegmentation of nuclei in histopathology images by deep regression of the distance map. P Naylor, M Lae, F Reyal, T Walter, IEEE Transactions on Medical Imaging. 382Naylor, P., Lae, M., Reyal, F., Walter, T.: Segmentation of nuclei in histopathology images by deep regression of the distance map. IEEE Transactions on Medical Imaging 38(2), 448-459 (2018)\n\nUnsupervised learning of visual representations by solving jigsaw puzzles. M Noroozi, P Favaro, European Conference on Computer Vision. Noroozi, M., Favaro, P.: Unsupervised learning of visual representations by solving jigsaw puzzles. In: European Conference on Computer Vision. pp. 69-84 (2016)\n\nBESNet: Boundary-enhanced segmentation of cells in histopathological images. H Oda, H R Roth, K Chiba, J Sokolic, T Kitasaka, M Oda, A Hinoki, H Uchida, J A Schnabel, K Mori, International Conference on Medical Image Computing and Computer Assisted Intervention. Oda, H., Roth, H.R., Chiba, K., Sokolic, J., Kitasaka, T., Oda, M., Hinoki, A., Uchida, H., Schnabel, J.A., Mori, K.: BESNet: Boundary-enhanced segmentation of cells in histopathological images. In: International Conference on Medical Image Computing and Computer Assisted Intervention. pp. 228-236 (2018)\n\nU-net: Convolutional networks for biomedical image segmentation. O Ronneberger, P Fischer, T Brox, International Conference on Medical Image Computing and Computer Assisted Intervention. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedi- cal image segmentation. In: International Conference on Medical Image Computing and Computer Assisted Intervention. pp. 234-241 (2015)\n\nSemantic versus instance segmentation in microscopic algae detection. Engineering Applications of. J Ruiz-Santaquiteria, G Bueno, O Deniz, N Vallez, G Cristobal, Artificial Intelligence. 87103271Ruiz-Santaquiteria, J., Bueno, G., Deniz, O., Vallez, N., Cristobal, G.: Semantic versus instance segmentation in microscopic algae detection. Engineering Applica- tions of Artificial Intelligence 87, 103271 (2020)\n\nA unified embedding for face recognition and clustering. F Schroff, D Kalenichenko, J Philbin, IEEE Conference on Computer Vision and Pattern Recognition. Schroff, F., Kalenichenko, D., Philbin, J.: A unified embedding for face recognition and clustering. In: IEEE Conference on Computer Vision and Pattern Recognition. pp. 815-823 (2015)\n\nImproving cytoarchitectonic segmentation of human brain areas with self-supervised siamese networks. H Spitzer, K Kiwitz, K Amunts, S Harmeling, T Dickscheid, International Conference on Medical Image Computing and Computer Assisted Intervention. Spitzer, H., Kiwitz, K., Amunts, K., Harmeling, S., Dickscheid, T.: Improving cy- toarchitectonic segmentation of human brain areas with self-supervised siamese networks. In: International Conference on Medical Image Computing and Com- puter Assisted Intervention. pp. 663-671 (2018)\n\nRobust segmentation of nucleus in histopathology images via Mask R-CNN. In: Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries. X Xie, Y Li, M Zhang, L Shen, Xie, X., Li, Y., Zhang, M., Shen, L.: Robust segmentation of nucleus in histopathol- ogy images via Mask R-CNN. In: Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries. pp. 428-436 (2018)\n\nCIA-Net: Robust nuclei instance segmentation with contour-aware information aggregation. Y Zhou, O F Onder, Q Dou, E Tsougenis, H Chen, P A Heng, International Conference on Information Processing in Medical Imaging. Zhou, Y., Onder, O.F., Dou, Q., Tsougenis, E., Chen, H., Heng, P.A.: CIA-Net: Robust nuclei instance segmentation with contour-aware information aggregation. In: International Conference on Information Processing in Medical Imaging. pp. 682-693 (2019)\n\nModels genesis: Generic autodidactic models for 3D medical image analysis. Z Zhou, V Sodha, M M R Siddiquee, R Feng, N Tajbakhsh, M B Gotway, J Liang, International Conference on Medical Image Computing and Computer Assisted Intervention. Zhou, Z., Sodha, V., Siddiquee, M.M.R., Feng, R., Tajbakhsh, N., Gotway, M.B., Liang, J.: Models genesis: Generic autodidactic models for 3D medical image anal- ysis. In: International Conference on Medical Image Computing and Computer Assisted Intervention. pp. 384-393 (2019)\n\nSelf-supervised feature learning for 3D medical images by playing a Rubik's cube. X Zhuang, Y Li, Y Hu, K Ma, Y Yang, Y Zheng, International Conference on Medical Image Computing and Computer Assisted Intervention. Zhuang, X., Li, Y., Hu, Y., Ma, K., Yang, Y., Zheng, Y.: Self-supervised feature learning for 3D medical images by playing a Rubik's cube. In: International Con- ference on Medical Image Computing and Computer Assisted Intervention. pp. 420-428 (2019)\n", "annotations": {"author": "[{\"end\":151,\"start\":78},{\"end\":198,\"start\":152},{\"end\":245,\"start\":199},{\"end\":337,\"start\":246},{\"end\":379,\"start\":338},{\"end\":427,\"start\":380}]", "publisher": null, "author_last_name": "[{\"end\":89,\"start\":86},{\"end\":163,\"start\":159},{\"end\":210,\"start\":208},{\"end\":257,\"start\":253},{\"end\":344,\"start\":342},{\"end\":392,\"start\":387}]", "author_first_name": "[{\"end\":85,\"start\":78},{\"end\":158,\"start\":152},{\"end\":207,\"start\":199},{\"end\":252,\"start\":246},{\"end\":341,\"start\":338},{\"end\":386,\"start\":380}]", "author_affiliation": "[{\"end\":150,\"start\":91},{\"end\":197,\"start\":165},{\"end\":244,\"start\":212},{\"end\":336,\"start\":277},{\"end\":378,\"start\":346},{\"end\":426,\"start\":394}]", "title": "[{\"end\":64,\"start\":1},{\"end\":491,\"start\":428}]", "venue": null, "abstract": "[{\"end\":2026,\"start\":586}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2260,\"start\":2257},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2404,\"start\":2400},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2522,\"start\":2519},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2824,\"start\":2821},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2842,\"start\":2838},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4336,\"start\":4332},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4367,\"start\":4363},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4395,\"start\":4391},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4534,\"start\":4530},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8532,\"start\":8528},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10739,\"start\":10736},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10837,\"start\":10833},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11195,\"start\":11192},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11581,\"start\":11578},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":12268,\"start\":12265},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":12295,\"start\":12292},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":13761,\"start\":13758},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":13763,\"start\":13761},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":13765,\"start\":13763},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14542,\"start\":14538},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":14794,\"start\":14791}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":16353,\"start\":16066},{\"attributes\":{\"id\":\"fig_1\"},\"end\":16565,\"start\":16354},{\"attributes\":{\"id\":\"fig_2\"},\"end\":16900,\"start\":16566},{\"attributes\":{\"id\":\"fig_3\"},\"end\":17056,\"start\":16901},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":17356,\"start\":17057},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":17951,\"start\":17357},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":18701,\"start\":17952}]", "paragraph": "[{\"end\":3792,\"start\":2042},{\"end\":4866,\"start\":3794},{\"end\":5705,\"start\":4868},{\"end\":5815,\"start\":5716},{\"end\":6220,\"start\":5838},{\"end\":7583,\"start\":6222},{\"end\":8368,\"start\":7728},{\"end\":8462,\"start\":8417},{\"end\":8931,\"start\":8464},{\"end\":9361,\"start\":9009},{\"end\":9917,\"start\":9363},{\"end\":10644,\"start\":9963},{\"end\":11324,\"start\":10666},{\"end\":11544,\"start\":11340},{\"end\":12504,\"start\":11557},{\"end\":14874,\"start\":12531},{\"end\":15447,\"start\":14876},{\"end\":15582,\"start\":15449},{\"end\":16065,\"start\":15597}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7666,\"start\":7584},{\"attributes\":{\"id\":\"formula_1\"},\"end\":8416,\"start\":8369},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9008,\"start\":8932},{\"attributes\":{\"id\":\"formula_3\"},\"end\":9962,\"start\":9918},{\"attributes\":{\"id\":\"formula_4\"},\"end\":10665,\"start\":10645}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":13182,\"start\":13175},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":13820,\"start\":13813},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":13960,\"start\":13953},{\"end\":15077,\"start\":15070},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":15580,\"start\":15573}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2040,\"start\":2028},{\"attributes\":{\"n\":\"2\"},\"end\":5714,\"start\":5708},{\"attributes\":{\"n\":\"2.1\"},\"end\":5836,\"start\":5818},{\"attributes\":{\"n\":\"2.2\"},\"end\":7726,\"start\":7668},{\"attributes\":{\"n\":\"3\"},\"end\":11338,\"start\":11327},{\"attributes\":{\"n\":\"3.1\"},\"end\":11555,\"start\":11547},{\"attributes\":{\"n\":\"3.2\"},\"end\":12529,\"start\":12507},{\"attributes\":{\"n\":\"4\"},\"end\":15595,\"start\":15585},{\"end\":16075,\"start\":16067},{\"end\":16363,\"start\":16355},{\"end\":16575,\"start\":16567},{\"end\":16910,\"start\":16902},{\"end\":17067,\"start\":17058},{\"end\":17367,\"start\":17358},{\"end\":17962,\"start\":17953}]", "table": "[{\"end\":17356,\"start\":17166},{\"end\":17951,\"start\":17481},{\"end\":18701,\"start\":18403}]", "figure_caption": "[{\"end\":16353,\"start\":16077},{\"end\":16565,\"start\":16365},{\"end\":16900,\"start\":16577},{\"end\":17056,\"start\":16912},{\"end\":17166,\"start\":17069},{\"end\":17481,\"start\":17369},{\"end\":18403,\"start\":17964}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6219,\"start\":6213},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6271,\"start\":6265},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":7977,\"start\":7971},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":10239,\"start\":10233},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":12784,\"start\":12778}]", "bib_author_first_name": "[{\"end\":18862,\"start\":18861},{\"end\":18871,\"start\":18870},{\"end\":18878,\"start\":18877},{\"end\":18890,\"start\":18889},{\"end\":18898,\"start\":18897},{\"end\":18900,\"start\":18899},{\"end\":18908,\"start\":18907},{\"end\":18910,\"start\":18909},{\"end\":18922,\"start\":18921},{\"end\":19329,\"start\":19328},{\"end\":19336,\"start\":19335},{\"end\":19345,\"start\":19344},{\"end\":19352,\"start\":19351},{\"end\":19361,\"start\":19360},{\"end\":19697,\"start\":19696},{\"end\":19708,\"start\":19707},{\"end\":19717,\"start\":19716},{\"end\":20004,\"start\":20003},{\"end\":20010,\"start\":20009},{\"end\":20019,\"start\":20018},{\"end\":20026,\"start\":20025},{\"end\":20302,\"start\":20301},{\"end\":20311,\"start\":20310},{\"end\":20318,\"start\":20317},{\"end\":20336,\"start\":20335},{\"end\":20338,\"start\":20337},{\"end\":20664,\"start\":20663},{\"end\":20670,\"start\":20669},{\"end\":20678,\"start\":20677},{\"end\":20985,\"start\":20984},{\"end\":20995,\"start\":20994},{\"end\":21002,\"start\":21001},{\"end\":21011,\"start\":21010},{\"end\":21329,\"start\":21328},{\"end\":21340,\"start\":21339},{\"end\":21629,\"start\":21628},{\"end\":21636,\"start\":21635},{\"end\":21638,\"start\":21637},{\"end\":21646,\"start\":21645},{\"end\":21655,\"start\":21654},{\"end\":21666,\"start\":21665},{\"end\":21678,\"start\":21677},{\"end\":21685,\"start\":21684},{\"end\":21695,\"start\":21694},{\"end\":21705,\"start\":21704},{\"end\":21707,\"start\":21706},{\"end\":21719,\"start\":21718},{\"end\":22187,\"start\":22186},{\"end\":22202,\"start\":22201},{\"end\":22213,\"start\":22212},{\"end\":22627,\"start\":22626},{\"end\":22649,\"start\":22648},{\"end\":22658,\"start\":22657},{\"end\":22667,\"start\":22666},{\"end\":22677,\"start\":22676},{\"end\":22996,\"start\":22995},{\"end\":23007,\"start\":23006},{\"end\":23023,\"start\":23022},{\"end\":23380,\"start\":23379},{\"end\":23391,\"start\":23390},{\"end\":23401,\"start\":23400},{\"end\":23411,\"start\":23410},{\"end\":23424,\"start\":23423},{\"end\":23965,\"start\":23964},{\"end\":23972,\"start\":23971},{\"end\":23978,\"start\":23977},{\"end\":23987,\"start\":23986},{\"end\":24298,\"start\":24297},{\"end\":24306,\"start\":24305},{\"end\":24308,\"start\":24307},{\"end\":24317,\"start\":24316},{\"end\":24324,\"start\":24323},{\"end\":24337,\"start\":24336},{\"end\":24345,\"start\":24344},{\"end\":24347,\"start\":24346},{\"end\":24754,\"start\":24753},{\"end\":24762,\"start\":24761},{\"end\":24771,\"start\":24770},{\"end\":24775,\"start\":24772},{\"end\":24788,\"start\":24787},{\"end\":24796,\"start\":24795},{\"end\":24809,\"start\":24808},{\"end\":24811,\"start\":24810},{\"end\":24821,\"start\":24820},{\"end\":25279,\"start\":25278},{\"end\":25289,\"start\":25288},{\"end\":25295,\"start\":25294},{\"end\":25301,\"start\":25300},{\"end\":25307,\"start\":25306},{\"end\":25315,\"start\":25314}]", "bib_author_last_name": "[{\"end\":18868,\"start\":18863},{\"end\":18875,\"start\":18872},{\"end\":18887,\"start\":18879},{\"end\":18895,\"start\":18891},{\"end\":18905,\"start\":18901},{\"end\":18919,\"start\":18911},{\"end\":18929,\"start\":18923},{\"end\":19333,\"start\":19330},{\"end\":19342,\"start\":19337},{\"end\":19349,\"start\":19346},{\"end\":19358,\"start\":19353},{\"end\":19364,\"start\":19362},{\"end\":19705,\"start\":19698},{\"end\":19714,\"start\":19709},{\"end\":19727,\"start\":19718},{\"end\":20007,\"start\":20005},{\"end\":20016,\"start\":20011},{\"end\":20023,\"start\":20020},{\"end\":20030,\"start\":20027},{\"end\":20308,\"start\":20303},{\"end\":20315,\"start\":20312},{\"end\":20333,\"start\":20319},{\"end\":20349,\"start\":20339},{\"end\":20667,\"start\":20665},{\"end\":20675,\"start\":20671},{\"end\":20684,\"start\":20679},{\"end\":20992,\"start\":20986},{\"end\":20999,\"start\":20996},{\"end\":21008,\"start\":21003},{\"end\":21018,\"start\":21012},{\"end\":21337,\"start\":21330},{\"end\":21347,\"start\":21341},{\"end\":21633,\"start\":21630},{\"end\":21643,\"start\":21639},{\"end\":21652,\"start\":21647},{\"end\":21663,\"start\":21656},{\"end\":21675,\"start\":21667},{\"end\":21682,\"start\":21679},{\"end\":21692,\"start\":21686},{\"end\":21702,\"start\":21696},{\"end\":21716,\"start\":21708},{\"end\":21724,\"start\":21720},{\"end\":22199,\"start\":22188},{\"end\":22210,\"start\":22203},{\"end\":22218,\"start\":22214},{\"end\":22646,\"start\":22628},{\"end\":22655,\"start\":22650},{\"end\":22664,\"start\":22659},{\"end\":22674,\"start\":22668},{\"end\":22687,\"start\":22678},{\"end\":23004,\"start\":22997},{\"end\":23020,\"start\":23008},{\"end\":23031,\"start\":23024},{\"end\":23388,\"start\":23381},{\"end\":23398,\"start\":23392},{\"end\":23408,\"start\":23402},{\"end\":23421,\"start\":23412},{\"end\":23435,\"start\":23425},{\"end\":23969,\"start\":23966},{\"end\":23975,\"start\":23973},{\"end\":23984,\"start\":23979},{\"end\":23992,\"start\":23988},{\"end\":24303,\"start\":24299},{\"end\":24314,\"start\":24309},{\"end\":24321,\"start\":24318},{\"end\":24334,\"start\":24325},{\"end\":24342,\"start\":24338},{\"end\":24352,\"start\":24348},{\"end\":24759,\"start\":24755},{\"end\":24768,\"start\":24763},{\"end\":24785,\"start\":24776},{\"end\":24793,\"start\":24789},{\"end\":24806,\"start\":24797},{\"end\":24818,\"start\":24812},{\"end\":24827,\"start\":24822},{\"end\":25286,\"start\":25280},{\"end\":25292,\"start\":25290},{\"end\":25298,\"start\":25296},{\"end\":25304,\"start\":25302},{\"end\":25312,\"start\":25308},{\"end\":25321,\"start\":25316}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":7748330},\"end\":19227,\"start\":18748},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3740145},\"end\":19626,\"start\":19229},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":4009713},\"end\":19955,\"start\":19628},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":206594692},\"end\":20257,\"start\":19957},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":9433631},\"end\":20595,\"start\":20259},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":218895138},\"end\":20894,\"start\":20597},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":59601271},\"end\":21251,\"start\":20896},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":187547},\"end\":21549,\"start\":21253},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":52270994},\"end\":22119,\"start\":21551},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":3719281},\"end\":22525,\"start\":22121},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":208102826},\"end\":22936,\"start\":22527},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":206592766},\"end\":23276,\"start\":22938},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":49183911},\"end\":23808,\"start\":23278},{\"attributes\":{\"id\":\"b13\"},\"end\":24206,\"start\":23810},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":76666202},\"end\":24676,\"start\":24208},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":201070166},\"end\":25194,\"start\":24678},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":203837701},\"end\":25662,\"start\":25196}]", "bib_title": "[{\"end\":18859,\"start\":18748},{\"end\":19326,\"start\":19229},{\"end\":19694,\"start\":19628},{\"end\":20001,\"start\":19957},{\"end\":20299,\"start\":20259},{\"end\":20661,\"start\":20597},{\"end\":20982,\"start\":20896},{\"end\":21326,\"start\":21253},{\"end\":21626,\"start\":21551},{\"end\":22184,\"start\":22121},{\"end\":22624,\"start\":22527},{\"end\":22993,\"start\":22938},{\"end\":23377,\"start\":23278},{\"end\":24295,\"start\":24208},{\"end\":24751,\"start\":24678},{\"end\":25276,\"start\":25196}]", "bib_author": "[{\"end\":18870,\"start\":18861},{\"end\":18877,\"start\":18870},{\"end\":18889,\"start\":18877},{\"end\":18897,\"start\":18889},{\"end\":18907,\"start\":18897},{\"end\":18921,\"start\":18907},{\"end\":18931,\"start\":18921},{\"end\":19335,\"start\":19328},{\"end\":19344,\"start\":19335},{\"end\":19351,\"start\":19344},{\"end\":19360,\"start\":19351},{\"end\":19366,\"start\":19360},{\"end\":19707,\"start\":19696},{\"end\":19716,\"start\":19707},{\"end\":19729,\"start\":19716},{\"end\":20009,\"start\":20003},{\"end\":20018,\"start\":20009},{\"end\":20025,\"start\":20018},{\"end\":20032,\"start\":20025},{\"end\":20310,\"start\":20301},{\"end\":20317,\"start\":20310},{\"end\":20335,\"start\":20317},{\"end\":20351,\"start\":20335},{\"end\":20669,\"start\":20663},{\"end\":20677,\"start\":20669},{\"end\":20686,\"start\":20677},{\"end\":20994,\"start\":20984},{\"end\":21001,\"start\":20994},{\"end\":21010,\"start\":21001},{\"end\":21020,\"start\":21010},{\"end\":21339,\"start\":21328},{\"end\":21349,\"start\":21339},{\"end\":21635,\"start\":21628},{\"end\":21645,\"start\":21635},{\"end\":21654,\"start\":21645},{\"end\":21665,\"start\":21654},{\"end\":21677,\"start\":21665},{\"end\":21684,\"start\":21677},{\"end\":21694,\"start\":21684},{\"end\":21704,\"start\":21694},{\"end\":21718,\"start\":21704},{\"end\":21726,\"start\":21718},{\"end\":22201,\"start\":22186},{\"end\":22212,\"start\":22201},{\"end\":22220,\"start\":22212},{\"end\":22648,\"start\":22626},{\"end\":22657,\"start\":22648},{\"end\":22666,\"start\":22657},{\"end\":22676,\"start\":22666},{\"end\":22689,\"start\":22676},{\"end\":23006,\"start\":22995},{\"end\":23022,\"start\":23006},{\"end\":23033,\"start\":23022},{\"end\":23390,\"start\":23379},{\"end\":23400,\"start\":23390},{\"end\":23410,\"start\":23400},{\"end\":23423,\"start\":23410},{\"end\":23437,\"start\":23423},{\"end\":23971,\"start\":23964},{\"end\":23977,\"start\":23971},{\"end\":23986,\"start\":23977},{\"end\":23994,\"start\":23986},{\"end\":24305,\"start\":24297},{\"end\":24316,\"start\":24305},{\"end\":24323,\"start\":24316},{\"end\":24336,\"start\":24323},{\"end\":24344,\"start\":24336},{\"end\":24354,\"start\":24344},{\"end\":24761,\"start\":24753},{\"end\":24770,\"start\":24761},{\"end\":24787,\"start\":24770},{\"end\":24795,\"start\":24787},{\"end\":24808,\"start\":24795},{\"end\":24820,\"start\":24808},{\"end\":24829,\"start\":24820},{\"end\":25288,\"start\":25278},{\"end\":25294,\"start\":25288},{\"end\":25300,\"start\":25294},{\"end\":25306,\"start\":25300},{\"end\":25314,\"start\":25306},{\"end\":25323,\"start\":25314}]", "bib_venue": "[{\"end\":18967,\"start\":18931},{\"end\":19410,\"start\":19366},{\"end\":19781,\"start\":19729},{\"end\":20090,\"start\":20032},{\"end\":20409,\"start\":20351},{\"end\":20736,\"start\":20686},{\"end\":21056,\"start\":21020},{\"end\":21387,\"start\":21349},{\"end\":21812,\"start\":21726},{\"end\":22306,\"start\":22220},{\"end\":22712,\"start\":22689},{\"end\":23091,\"start\":23033},{\"end\":23523,\"start\":23437},{\"end\":23962,\"start\":23810},{\"end\":24423,\"start\":24354},{\"end\":24915,\"start\":24829},{\"end\":25409,\"start\":25323}]"}}}, "year": 2023, "month": 12, "day": 17}