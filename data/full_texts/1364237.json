{"id": 1364237, "updated": "2023-09-27 04:29:42.181", "metadata": {"title": "BUbiNG: Massive Crawling for the Masses", "authors": "[{\"first\":\"Paolo\",\"last\":\"Boldi\",\"middle\":[]},{\"first\":\"Andrea\",\"last\":\"Marino\",\"middle\":[]},{\"first\":\"Massimo\",\"last\":\"Santini\",\"middle\":[]},{\"first\":\"Sebastiano\",\"last\":\"Vigna\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2016, "month": 1, "day": 26}, "abstract": "Although web crawlers have been around for twenty years by now, there is virtually no freely available, opensource crawling software that guarantees high throughput, overcomes the limits of single-machine systems and at the same time scales linearly with the amount of resources available. This paper aims at filling this gap, through the description of BUbiNG, our next-generation web crawler built upon the authors' experience with UbiCrawler [Boldi et al. 2004] and on the last ten years of research on the topic. BUbiNG is an opensource Java fully distributed crawler; a single BUbiNG agent, using sizeable hardware, can crawl several thousands pages per second respecting strict politeness constraints, both host- and IP-based. Unlike existing open-source distributed crawlers that rely on batch techniques (like MapReduce), BUbiNG job distribution is based on modern high-speed protocols so to achieve very high throughput.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1601.06919", "mag": "2952276602", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tweb/BoldiMSV18", "doi": "10.1145/2567948.2577304"}}, "content": {"source": {"pdf_hash": "0ee5abec0c7002c759d70e4d75921b65a6d8666a", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1601.06919v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "cf321d06308cc8420d9e007997d8212d51672529", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/0ee5abec0c7002c759d70e4d75921b65a6d8666a.txt", "contents": "\nBUbiNG: Massive Crawling for the Masses\n\n\nPaolo Boldi \nDipartimento di Informatica\nUniversit\u00e0 degli Studi di Milano\nItaly\n\nDipartimento di Informatica\nANDREA MARINO\nUniversit\u00e0 degli Studi di Milano\nItaly\n\nDipartimento di Informatica\nMASSIMO SANTINI\nUniversit\u00e0 degli Studi di Milano\nItaly\n\nDipartimento di Informatica\nSEBASTIANO VIGNA\nUniversit\u00e0 degli Studi di Milano\nItaly\n\nBUbiNG: Massive Crawling for the Masses\nH33 [Information search and retrieval]: Clustering General Terms: AlgorithmsExperimentationPerformance Additional Key Words and Phrases: Web crawlingDistributed systems\nAlthough web crawlers have been around for twenty years by now, there is virtually no freely available, opensource crawling software that guarantees high throughput, overcomes the limits of single-machine systems and at the same time scales linearly with the amount of resources available. This paper aims at filling this gap, through the description of BUbiNG, our next-generation web crawler built upon the authors' experience with UbiCrawler[Boldi et al. 2004] and on the last ten years of research on the topic. BUbiNG is an opensource Java fully distributed crawler; a single BUbiNG agent, using sizeable hardware, can crawl several thousands pages per second respecting strict politeness constraints, both host-and IP-based. Unlike existing open-source distributed crawlers that rely on batch techniques (like MapReduce), BUbiNG job distribution is based on modern high-speed protocols so to achieve very high throughput.\n\nINTRODUCTION\n\nA web crawler (sometimes also known as a (ro)bot or spider) is a tool that downloads systematically a large number of web pages starting from a seed. Web crawlers are, of course, used by search engines, but also by companies selling \"Search-Engine Optimization\" services, by archiving projects such as the Internet Archive, by surveillance systems (e.g., that scan the web looking for cases of plagiarism), and by entities performing statistical studies of the structure and the content of the web, just to name a few.\n\nThe basic inner working of a crawler is surprisingly simple from a theoretical viewpoint: it is a form of traversal (for example, a breadth-first visit). Starting from a given seed of URLs, the set of associated pages is downloaded, their content is parsed, and the resulting links are used iteratively to collect new pages.\n\nAlbeit in principle a crawler just performs a visit of the web, there are a number of factors that make the visit of a crawler inherently different from a textbook algorithm. The first and most important difference is that the size of the graph to be explored is unknown and huge; in fact, infinite. The second difference is that visiting a node (i.e., downloading a page) is a complex process that has intrinsic limits due to network speed, latency, and politeness-the requirement of not overloading servers during the download. Not to mention the countless problems (errors in DNS resolutions, protocol or network errors, presence of traps) that the crawler may find on its way.\n\nIn this paper we describe the design and implementation of BUbiNG, our new web crawler built upon our experience with UbiCrawler [Boldi et al. 2004] and on the last ten years of research on the topic. 1 BUbiNG aims at filling an important gap in the range of available crawlers. In particular:\n\n-It is a pure-Java, open-source crawler released under the Gnu GPLv3.\n\n1 A preliminary poster appeared in ].\n\n-It is fully distributed: multiple agents perform the crawl concurrently and handle the necessary coordination without the need of any central control; given enough bandwidth, the crawling speed grows linearly with the number of agents. -Its design acknowledges that CPUs and OS kernels have become extremely efficient in handling a large number of threads (in particular, threads that are mainly I/Obound) and that large amounts of RAM are by now easily available at a moderate cost. More in detail, we assume that the memory used by an agent must be constant in the number of discovered URLs, but that it can scale linearly in the number of discovered hosts. This assumption simplifies the overall design and makes several data structures more efficient. -It is very fast: on a 64-core, 64 GB workstation it can download hundreds of million of pages at more than 10 000 pages per second respecting politeness both by host and by IP, analyzing, compressing and storing more than 160 MB/s of data. -It is extremely configurable: beyond choosing the sizes of the various data structures and the communication parameters involved, implementations can be specified by reflection in a configuration file and the whole dataflow followed by a discovered URL can be controlled by arbitrary user-defined filters, that can further be combined with standard Boolean-algebra operators. -It fully respects the robot exclusion protocol, a de facto standard that well-behaved crawlers are expected to obey. -It guarantees that hostwise the visit is an exact breadth-first visit (albeit the global policy can be customized), thus collecting pages in a more predictible and principled manner. -It guarantees that politeness constraints are satisfied both at the host and the IP level, that is, that two data requests to the same host (name) or IP are separated by at least a specified amount of time. The two intervals can be set independently, and, in principle, customized per host or IP.\n\nWhen designing a crawler, one should always ponder over the specific usage the crawler is intended for. This decision influences many of the design details that need to be taken. Our main goal is to provide a crawler that can be used out-of-the-box as an archival crawler, but that can be easily modified to accomplish other tasks. Being an archival crawler, it does not perform any refresh of the visited pages, and moreover it tries to perform a visit that is as close to breadth-first as possible (more about this below). Both behaviors can in fact be modified easily in case of need, but this discussion (on the possible ways to customize BUbiNG) is out of the scope of this paper.\n\nWe plan to use BUbiNG to provide new data sets for the research community. Datasets crawled by UbiCrawler have been used in hundreds of scientific publications, but BUbiNG makes it possible to gather data orders of magnitude larger.\n\n\nMOTIVATION\n\nThere are four main reasons why we decided to design BUbiNG as we described above.\n\nPrincipled sampling. Analyzing the properties of the web graph has proven to be an elusive goal. A recent large-scale study [Meusel et al. 2015] has shown, once again, that many alleged properties of the web are actually due to crawling and parsing artifacts instead. By creating an open-source crawler that enforces a breadth-first visit strategy, altered by politeness constraints only, we aim at creating web snapshots providing more reproducible results. While breadth-first visits have their own artifacts (e.g., they can induce an apparent indegree power law even on regular graphs [Achlioptas et al. 2009]), they are a principled approach that has been widely studied and adopted. A more detailed analysis, like spam detection, topic selection, and so on, can be per-formed offline. A focused crawling activity can actually be detrimental to the study of the web, which should be sampled \"as it is\". Coherent time frame. Developing a crawler with speed as a main goal might seem restrictive. Nonetheless, for the purpose of studying the web, speed is essential, as gathering large snapshots over a long period of time might introduce biases that would be very difficult to detect and undo. Pushing hardware to the limit. BUbiNG is designed to exploit hardware to its limits, by carefully removing bottlenecks and contention usually present in highly parallel distributed crawlers. As a consequence, it makes performing large-scale crawling possible even with limited hardware resources. Consistent crawling and analysis. BUbiNG comes along with a series of tools that make it possible to analyze the harvested data in a distributed fashion, also exploiting multicore parallelism. In particular, the construction of the web graph associated with a crawl uses the same parser as the crawler. In the past, a major problem in the analysis of web crawls turned out to be the inconsistency between the parsing as performed at crawl time and the parsing as performed at graph-construction time, which introduced artifacts such as spurious components (see the comments in [Meusel et al. 2015]). By providing a complete framework that uses the same code both online and offline we hope to increase the reliability and reproducibility of the analysis of web snapshots.\n\n\nRELATED WORKS\n\nWeb crawlers have been developed since the very birth of the web. The first-generation crawlers date back to the early 90s: World Wide Web Worm [McBryan 1994], RBSE spider [Eichmann 1994], MOMspider [Fielding 1994], WebCrawler [Pinkerton 1994]. One of the main contributions of these works has been that of pointing out some of the main algorithmic and design issues of crawlers. In the meanwhile, several commercial search engines, having their own crawler (e.g., AltaVista), were born. In the second half of the 90s, the fast growth of the web called for the need of large-scale crawlers, like the Module crawler [Burner 1997] of the Internet Archive (a non-profit corporation aiming to keep large archival-quality historical records of the world-wide web) and the first generation of the Google crawler [Brin and Page 1998]. This generation of spiders was able to download efficiently tens of millions of pages. At the beginning of 2000, the scalability, the extensibility, and the distribution of the crawlers become a key design point: this was the case of the Java crawler Mercator [Najork and Heydon 2002] (the distributed version of [Heydon and Najork 1999]), Polybot [Shkapenyuk and Suel 2002], IBM WebFountain [Edwards et al. 2001], and UbiCrawler [Boldi et al. 2004]. These crawlers were able to produce snapshots of the web of hundreds of millions of pages.\n\nRecently, a new generation of crawlers was designed, aiming to download billions of pages, like [Lee et al. 2009]. Nonetheless, none of them is freely available and open source: BUbiNG is the first open-source crawler designed to be fast, scalable and runnable on commodity hardware.\n\nFor more details about previous works or about the main issues in the design of crawlers, we refer the reader to [Olston and Najork 2010;Mirtaheri et al. 2013].\n\n\nOpen-source crawlers\n\nAlthough web crawlers have been around for twenty years by now (since the spring of 1993, according to [Olston and Najork 2010]), the area of freely available ones, let alone open-source, is still quite narrow. With the few exceptions that will be discussed below, most stable projects we are aware of (GNU wget or mngoGoSearch, to cite a few) do not (and are not designed to) scale to download more than few thousands or tens of thousands pages. They can be useful to build an intranet search engine, but not for web-scale experiments. Heritrix [Her 2003;Mohr et al. 2004] is one of the few examples of an open-source search engine designed to download large datasets: it was developed starting from 2003 by Internet Archive [Int 1996] and it has been since actively developed. Heritrix (available under the Apache license), although it is of course multi-threaded, is a single-machine crawler, which is one of the main hindrances to its scalability. The default crawl order is breadth-first, as suggested by the archival goals behind its design. On the other hand, it provides a powerful checkpointing mechanism and a flexible way of filtering and processing URLs after and before fetching. It is worth noting that the Internet Archive proposed, implemented (in Heritrix) and fostered a standard format for archiving web content, called WARC, that is now an ISO standard [Iso 2009] and that BUbiNG is also adopting for storing the downloaded pages.\n\nNutch [Khare et al. 2004] is one of the best known existing open-source web crawlers; in fact, the goal of Nutch itself is much broader in scope, because it aims at offering a full-fledged search engine under all respects: besides crawling, Nutch implements features such as (hyper)text-indexing, link analysis, query resolution, result ranking and summarization. It is natively distributed (using Apache Hadoop as task-distribution backbone) and quite configurable; it also adopts breadth-first as basic visit mechanism, but can be optionally configured to go depth-first or even largest-score first, where scores are computed using some scoring strategy which is itself configurable. Scalability and speed are the main design goals of Nutch; for example, Nutch was used to collect TREC ClueWeb09 dataset 2 , the largest web dataset publicly available as of today consisting of 1 040 809 705 pages, that were downloaded at the speed of 755.31 pages/s [ Clu 2009]; to do this they used a Hadoop cluster of 100 machines [Callan 2012], so their real throughput was of about 7.55 pages/s per machine. This poor performance is not unexpected: using Hadoop to distribute the crawling jobs is easy, but not efficient, because it constrains the crawler to work in a batch 3 fashion. It should not be surprising that using a modern job-distribution framework like BUbiNG does increases the throughput by orders of magnitude.\n\n\nARCHITECTURE OVERVIEW\n\nBUbiNG stands on a few architectural choices which in some cases contrast the common folklore wisdom. We took our decisions after carefully comparing and benchmarking several options and gathering the hands-on experience of similar projects.\n\n-The fetching logic of BUbiNG is built around thousands of identical fetching threads performing only synchronous (blocking) I/O. Experience with recent Linux kernels and increase in the number of cores per machine shows that this approach consistently outperforms asynchronous I/O. This strategy simplifies significantly the code complexity, and makes it trivial to implement features like HTTP/1.1 \"keepalive\" multiple-resource downloads. -Lock-free [Michael and Scott 1996] data structures are used to \"sandwich\" fetching threads, so that they never have to access lock-based data structures. This approach is particularly useful to avoid direct access to synchronized data structures with log- arithmic modification time, such as priority queues, as contention between fetching threads can become very significant. -URL storage (both in memory and on disk) is entirely performed using byte arrays.\n\nWhile this approach might seen anachronistic, the Java String class can easily occupy three times the memory used by a URL in byte-array form (both due to additional fields and to 16-bit characters) and doubles the number of objects. BUbiNG aims at exploiting the large memory sizes available today, but garbage collection has a linear cost in the number of objects: this factor must be taken into account. -Following UbiCrawler's design [Boldi et al. 2004], BUbiNG agents are identical and autonomous. The assignment of URLs to agents is entirely customizable, but by default we use consistent hashing as a fault-tolerant, self-configuring assignment function.\n\nIn this section, we overview the structure of a BUbiNG agent: the following sections detail the behavior of each component. The inner structure and data flow of an agent is depicted in Figure 1.\n\nThe bulk of the work of an agent is carried out by low-priority fetching threads, which download pages, and parsing threads, which parse and extract information from downloaded pages. Fetching threads are usually thousands, and spend most of their time waiting for network data, whereas one usually allocates as many parsing threads as the number of available cores, because their activity is mostly CPU bound.\n\nFetching threads are connected to parsing threads using a lock-free result list in which fetching threads enqueue buffers of fetched data, and wait for a parsing thread to analyze them. Parsing threads poll the result list using an exponential backoff scheme, perform actions such as parsing and link extraction, and signal back to the fetching thread that that the buffer can be filled again.\n\nAs parsing threads discover new URLs, they enqueue them to a sieve that keeps track of which URLs have been already discovered. A sieve is a data structure similar to a queue with memory: each enqueued element will be dequeued at some later time, with the guarantee that an element that is enqueued multiple times will be dequeued just once. URLs are added to the sieve as they are discovered by parsing.\n\nIn fact, every time a URL is discovered it is checked first against a high-performance approximate LRU cache containing 128-bit fingerprints: more than 90% of the URLs discovered are discarded at this stage. The cache avoids that frequently found URLs put the sieve under stress, and it has also another important goal: it avoids that frequently found URLs assigned to another agent are retransmitted several times.\n\nURLs that come out of the sieve are ready to be visited, and they are taken care of (stored, organized and managed) by the frontier, which is actually itself decomposed into several modules.\n\nThe most important data structure of the frontier is the workbench, an in-memory data structure that keeps track of the visit state of each host currently being visited and that can check in constant time whether some host can be accessed for download without violating the politeness constraints. Note that to attain the goal of several thousands downloaded pages per second without violating politeness constraints it is necessary to keep track of the visit state of hundreds of thousands of hosts.\n\nWhen a host is ready for download, its visit state is extracted from the workbench and moved to a lock-free todo queue by a suitable thread. Fetching threads poll the todo queue with an exponential backoff, fetch resources from the retrieved visit state 4 and then put it back onto the workbench. Note that we expect that once a large crawl has started, the todo queue will never be empty, so fetching threads will never have to wait. Most of the design challenges of the frontier components are actually geared towards avoiding that fetching threads ever wait on an empty todo queue.\n\nThe main active component of the frontier is the distributor: it is a high-priority thread that processes URLs coming out of the sieve (and that must therefore be crawled). Assuming for a moment that memory is unbounded, the only task of the distributor is that of iteratively dequeueing a URL from the sieve, checking whether it belongs to a host for which a visit state already exists, and then either creating a new visit state or enqueuing the URL to an existing one. If a new visit state is necessary, it is passed to a set of DNS threads that perform DNS resolution and then move the visit state onto the workbench.\n\nSince, however, breadth-first visit queues grow exponentially, and the workbench can use only a fixed amount of in-core memory, it is necessary to virtualize a part of the workbench, that is, writing on disk part of the URLs coming out of the sieve. To decide whether to keep a visit state entirely in the workbench or to virtualize it, and also to decide when and how URLs should be moved from the virtualizer to the workbench, the distributor uses a policy that is described later.\n\nFinally, every agent stores resources in its store (that may possibly reside on a distributed or remote file system). The native BUbiNG store is a compressed file in the Web ARChive (WARC) format (the standard proposed and made popular by Heritrix). This standard specifies how to combine several digital resources with other information into an aggregate archive file. In BUbiNG compression happens in a heavily parallelized way, with parsing threads compressing independently pages and using concurrent primitives to pass compressed data to a flushing thread.\n\n\nThe sieve\n\nA sieve is a queue with memory: it provides enqueue and dequeue primitives, similarly to a standard queue; each element enqueued to a sieve will be eventually dequeued later. However, a sieve guarantees also that if an element is enqueued multiple times, it will be anyway dequeued just once. Sieves (albeit not called with this name) have always been recognized as a fundamental basic data structure for a crawler: their main implementation issue lies in the unbounded, exponential growth of the number of discovered URLs. While it is easy to write enqueued elements to a disk file, checking that an element is not returned multiple times requires ad-hoc data structures-standard dictionaries would use too much in-core memory.\n\nThe actual sieve implementation used by BUbiNG can be customized, but the default one, called MercatorSieve, is similar to the one suggested in [Heydon and Najork 1999] (hence its name). Each element known to the sieve is stored as a 64-bit hash in a disk file. Every time a new element is enqueued, its hash is stored in an in-memory array, and the element is saved in an auxiliary file. When the array is full, it is sorted and compared with the set of elements known to the sieve. The auxiliary file is then scanned, and previously unseen elements are stored for later dequeueing. All these operations require only sequential access to all files involved, and the sizing of the array is based on the amount of in-core memory available. Note that the output order is guaranteed to be the same of the input order (i.e., elements are appended in the order of their first appearance).\n\nA generalization of the idea of a sieve, with the additional possibility of associating values with the elements, is the DRUM (Disk Repository with Update Management) structure used by IRLBot and described in [Lee et al. 2009]. A DRUM provides additional operations to retrieve or update the values associated with the elements. From an implementation viewpoint, DRUM is a Mercator sieve with multiple arrays, called buckets, in which a careful orchestration of in-memory and on-disk data makes it possible to sort in one shot sets that are an order of magnitude larger than what the Mercator sieve would allow using the same quantity of in-core memory. However, to do so DRUM must sacrifice breadth-first order: due to the inherent randomization of the way keys are placed in the buckets, there is no guarantee that URLs will be crawled in breadth-first order, not even per host. Finally, the tight analysis in [Lee et al. 2009] about the properties of DRUM is unavoidably bound to the single-agent approach of IRLBot: for example, the authors conclude that a URL cache is not useful to reduce the number of insertions in the DRUM, but the same cache reduces significantly network transmissions. Based on our experience, once the cache is in place the Mercator sieve becomes much more competitive.\n\nThere are several other implementations of the sieve logic currently used. A quite common choice is to adopt an explicit queue and a Bloom filter [Bloom 1970] to remember enqueued elements. Albeit popular, this choice has no theoretical guarantees: while it is possible to decide a priori the maximum number of pages that will ever be crawled, it is very difficult to bound in advance the number of discovered URLs, and this number is essential in sizing the Bloom filter. If the discovered URLs are significantly more than expected, several pages are likely to be lost because of false positives. A better choice is to use a dictionary of fixed-size fingerprints obtained from URLs using a suitable hash function. The disadvantage is that the structure would no longer use constant memory.\n\n\nThe workbench\n\nThe workbench is an in-memory data structure that contains the next URLs to be visited. It is one of the main novel ideas in BUbiNG's design, and it is one of the main reasons why we can attain a very high throughput. It is a significant improvement over IRLBot's two-queues approach [Lee et al. 2009], as it can detect in constant time whether a URL is ready for download without violating politeness limits.\n\nFirst of all, URLs associated with a specific host 5 are kept in a structure called visit state, containing a FIFO queue of the next URLs to be crawled for that host along with a next-fetch field that specifies the first instant in time when a URL from the queue can be downloaded, according to the per-host politeness configuration. Note that inside a visit state we only store a byte-array representation of the path and query of a URL: this approach significantly reduces object creation, and provides a simple form of compression by prefix omission.\n\nVisit states are further grouped into workbench entries based on their IP address; every time the first URL for a given host is found, a new visit state is created and then the IP address is determined (by one of the DNS threads): the new visit state is either put in a new workbench entry (if no known host was as associated to that IP address yet), or in an existing one.\n\nA workbench entry contains a queue of visit states (associated with the same IP) prioritized by their next-fetch field, and an IP-specific next-fetch, containing the first instant in time when the IP address can be accessed again, according to the per-IP politeness configuration. The workbench is the queue of all workbench entries, prioritized on the next-fetch field of each entry maximized with the next-fetch field on the top element of its queue of visit states. In other words, the workbench is a priority queue of priority queues of FIFO queues.\n\nWe remark that due to our choice of priorities there is a host that can be visited without violating host or IP politeness constraints if and only if the first URL of the top visit state of the top workbench entry can be visited. Moreover, if there is no such host, the delay after which a host will be ready is given by the priority of the top workbench entry minus the current time.\n\nTherefore, the workbench acts as a delay queue: its dequeue operation waits, if necessary, until a host is ready to be visited. At that point, the top entry E is removed from the workbench and the top visit state is removed from E. Both removals happen in logarithmic time (in the number of visit states). The visit state and the associated workbench entry act as a token that is virtually passed between BUbiNG's components to guarantee that no component is working on the same workbench entry at the same time (in particular, this forces both kinds of politeness). In practice, as we mentioned in the overview, dequeueing is performed by a high-priority thread, the todo thread, that constantly dequeues visit states from the workbench and enqueues them into a lockfree todo queue, which is then accessed by fetching threads. This approach, besides avoiding contention by thousands of threads on a relatively slow structure, makes the number of visit states that are ready for downloads easily measurable: it is just the size of the todo queue. The downside is that, in principle, using very skewed per-host or per-IP politeness delays might cause the order of the todo queue not to reflect the actual priority of the visit states contained therein.\n\n\nFetching threads\n\nA fetching thread is a very simple thread that iteratively extracts visit states from the todo queue. If the todo queue is empty, a standard exponential backoff procedure is used to avoid polling the list too frequently, but the design of BUbiNG aims at keeping the todo queue nonempty and avoiding backoff altogether.\n\nOnce a fetching thread acquires a visit state, it tries to fetch the first URL of the visit state FIFO queue. If suitably configured, a fetching thread can also iterate the fetching process on more URLs for a fixed amount of time, so to exploit the \"keepalive\" feature of HTTP 1.1.\n\nEach fetching thread has an associated fetch data instance in which the downloaded data are buffered. Fetch data include a transparent buffering method that keeps a fixed amount of data in memory and dumps on disk the remaining part. By sizing the fixed amount suitably, most requests can be completed without accessing the disk, but at the same time rare large requests can be handled without allocating additional memory.\n\nAfter a resource has been fetched, the fetch data is put in the results queue so that one of the parsing threads can parse it. Once this process is over, the parsing thread sends a signal back so that the fetching thread is able to start working on a new URL. Once a fetching thread has to work on a new visit state, it puts the current visit state in a done queue, from which it will be dequeued by a suitable thread that will then put it back on the workbench together with its associated entry.\n\nMost of the time, a fetching thread is blocked on I/O, which makes it possible to run thousands of them in parallel. Indeed, the number of fetching threads determines the amount of parallelization BUbiNG can achieve while fetching data from the network, so it should be chosen as large as possible, compatibly with the amount of bandwidth available and with the memory used by fetch data.\n\n\nParsing threads\n\nA parsing thread iteratively extracts from the results queue the fetch data that have been previously enqueued by a fetching thread. Then, the content of the HTTP response is analyzed and possibly parsed. If the response contains a HTML page, the parser will produce a set of URLs that will be first checked against the URL cache, and then, if not already seen, either sent to another agent, or enqueued to the same agent's sieve.\n\nDuring the parsing phase, a parsing thread computes a digest of the response content. The signature is stored in a Bloom filter [Bloom 1970] and it is used to avoid saving several times the same page (or near-duplicate pages). Finally, the content of the response is saved to the store.\n\nSince two pages are considered (near-)duplicates whether they have the same signature, the digest computation is responsible for content-based duplicate detection. In the case of HTML pages, in order to collapse near-duplicates, some heuristic is used. In particular, an hash fingerprint is computed on a summarized content, which is obtained by stripping HTML attributes, and discarding digits and dates from the response content. This simple heuristic allows for instance to collapse pages that differs just for visitor counters or calendars. In a post-crawl phase, there are several more sophisticated approaches that can be applied, like shingling [Broder et al. 1997], simhash [Charikar 2002], fuzzy fingerprinting [Fetterly et al. 2003;Chakrabarti 2003], and others, e.g., [Manku et al. 2007].\n\nFor the sake of description, we will refer to duplicates as the pages which are (near-)duplicates of some other page previously crawled according to the above definition, while we will call archetypes the set of pages which are not duplicates.\n\n\nDNS threads\n\nDNS threads are used to solve host names of new hosts: a DNS thread continuously dequeues from the list of newly discovered visit states and resolves its host name, adding it to a workbench entry (or creating a new one, if the IP address itself is new), and putting it on the workbench. In our experience, it is essential to run a local recursive DNS server to avoid the bottleneck caused by an external server.\n\n\nThe workbench virtualizer\n\nThe workbench virtualizer maintains on disk a mapping from hosts to FIFO virtual queues of URLs. Conceptually, all URLs that have been extracted from the sieve but have not yet been fetched are enqueued in the workbench visit state they belong to, in the exact order in which they came out of the sieve. Since, however, we aim at crawling with an amount of memory that is constant in the number of discovered URLs, part of the queues must be written on disk. Each virtual queue contains a fraction of URLs from each visit state, in such a way that the overall URL order respects, per host, the original breadth-first order.\n\nVirtual queues are consumed as the visit proceeds, following the natural per-host breadth-first order. As fetching threads download URLs, the workbench is partially freed and can be filled with URLs coming from the virtual queues. This action is performed by the same thread emptying the done queue (the queue containing the visit states after fetching): as it puts visit states back on the workbench, it selects visit states with URLs on disk but no more URLs on the workbench and puts them on a refill queue that will be later read by the distributor.\n\nInitially, we experimented with virtualizers inspired by the BEAST module of IRLbot [Lee et al. 2009], although many crucial details of their implementation were missing (e.g., the treatment of HTTP and connection errors); moreover, due to the static once-for-all distribution of URLs among a number of physical on-disk queues, it was impossible to guarantee adherence to a breadth-first visit in the face of unpredictable network-related faults.\n\nOur second implementation was based on the Berkeley DB, a key/value store that is also used by Heritrix. While extremely popular, Berkeley DB is a general-purpose storage system, and in particular in Java it has a very heavy load in terms of object creation and corresponding garbage collection. While providing in principle services like URL-level prioritization (which was not one of our design goals), Berkeley DB was soon detected to be a serious bottleneck in the overall design.\n\nWe thus decided to develop an ad-hoc virtualizer oriented towards breadth-first visits. We borrowed from Berkeley DB the idea of writing data in log files that are periodically collected, but we decided to rely on memory mapping to lessen the I/O burden.\n\nIn our virtualizer, on-disk URL queues are stored in log files that are memory mapped and transparently thought of as a contiguous memory region. Each URL stored on disk is prefixed with a pointer to the position of the next URL for the same host. Whenever we append a new URL, we modify the pointer of the last stored URL for the same host accordingly. A small amount of metadata associated with each host (e.g., the head and tail of its queue) is stored in main memory.\n\nAs URLs are dequeued to fill the workbench, part of the log files become free. When the ratio between the used and allocated space goes below a threshold (e.g., 50%), a garbage-collection process is started. Due to the fact that URLs are always appended, there is no need to keep track of free space: we just scan the queues in order of first appearance in the log files and gather them at the start of the memory-mapped space. By keeping track (in a priority queue) of the position of the next URL to be collected in each queue, we can move items directly to their final position, updating the queue after each move. We stop when enough space has been freed, and delete the log files that are now entirely unused.\n\nNote that most of the activity of our virtualizer is caused by appends and garbage collections (reads are a lower-impact activity that is necessarily bound by the network throughput). Both activities are highly localized (at the end of the currently used region in the case of appends, and at the current collection point in the case of garbage collections), which makes a good use of the caching facilities of the operating system.\n\n\nThe distributor\n\nThe distributor is a high-priority thread that orchestrates the movement of URLs out of the sieve, and loads URLs from virtual queues into the workbench as necessary.\n\nAs the crawl proceeds, URLs get accumulated in visit states at different speeds, both because hosts have different responsiveness and because websites have different sizes and branching factors. Moreover, the workbench has a (configurable) limit size that cannot be exceeded, since one of the central design goals of BUbiNG is that the amount of main memory occupied cannot grow unboundedly in the number of the discovered URLs, but only in the number of hosts discovered. Thus, filling the workbench blindly with URLs coming out of the sieve would soon result in having in the workbench only URLs belonging to a limited number of hosts.\n\nThe front of a crawl, at any given time, is the number of visit states that are ready for download respecting the politeness constraints. The front size determines the overall throughput of the crawler-because of politeness, the number of distinct hosts currently being visited is the crucial datum that establishes how fast or slow the crawl is going to be.\n\nOne of the two forces driving the distributor is, indeed, that the front should always be large enough so that no fetching thread has ever to wait. To attain this goal, the distributor enlarges dynamically the required front size: each time a fetching thread has to wait, albeit the current front size is larger than the current required front size, the latter is increased. After a warm-up phase, the required front size stabilizes to a value that depends on the kind of hosts visited and on the amount of resources available. At that point, it is impossible to have a faster crawl given the resources available, as all fetching threads are continuously downloading data. Increasing the number of fetching threads, of course, may cause an increase of the required front size.\n\nThe second force driving the distributor is the (somewhat informal) requirement that we try to be as close to a breadth-first visit as possible. Note that this force works in an opposite direction with respect to enlarging the front-URLs that are already in existing visit states should be in principle visited before any URL in the sieve, but enlarging the front requires dequeueing more URLs from the sieve to find new hosts.\n\nThe distributor is also responsible for filling the workbench with URLs coming either out of the sieve, or out of virtual queues (circle numbered (1) in Figure 1). Once again, staying close to a breadth-first visit requires loading URLs in virtual queues, but keeping the front large might call for reading URLs from the sieve to discover new hosts.\n\nThe distributor privileges refilling the queues of the workbench using URLs from the virtualizer, because this makes the visit closer to an exact breadth-first. However, if no refill has to be performed and the front is not large enough, the distributor will read from the sieve, hoping to find new hosts to make the front larger.\n\nWhen the distributor reads a URL from the sieve, the URL can either be put in the workbench or written in a virtual queue, depending on whether there are already URLs on disk for the same host, and on the number of URLs per IP address that should be in the workbench to keep it full, but not overflowing, when the front is of the required size.\n\n\nConfigurability\n\nTo make BUbiNG capable of a versatile set of tasks and behaviors, every crawling phase (fetching, parsing, following the URLs of a page, scheduling new URLs, storing  pages) is controlled by a filter, a Boolean predicate that determines whether a given resource should be accepted or not. Filters can be configured both at startup and at runtime allowing for a very fine-grained control.\n\nThe type of objects a filter considers is called the base type of the filter. In most cases, the base type is going to be a URL or a fetched page. More precisely, a prefetch filter is one that has a BUbiNG URL as its base type (typically: to decide whether a URL should be scheduled for later visit, or should be fetched); a postfetch filter is one that has a fetched response as base type and decides whether to do something with that response (typically: whether to parse it, to store it, etc.).\n\n\nHeuristics\n\nSome classes of BUbiNG contain the distillation of heuristics we developed in almost twenty years of work with web crawling.\n\nOne important such class is BURL (a short name for \"BUbiNG URL\"), which is the class responsible for parsing and normalizing URLs found in web pages. The topic of parsing and normalization is much more involved than one might expect-very recently, the failure in building a sensible web graph from the ClueWeb09 collection stemmed in part from the lack of suitable normalization of the URLs involved. BURL takes care of fine details such as escaping and de-escaping (when unnecessary) of nonspecial characters, case normalization of percent-escape.\n\n\nDistributed crawling\n\nBUbiNG crawling activity can be distributed by running several agents over multiple machines. Similarly to UbiCrawler [Boldi et al. 2004], all agents are identical instances of BUbiNG, without any explicit leadership: all data structures described above are part of each agent.\n\nURL assignment to agents is entirely configurable. By default, BUbiNG uses just the host to assign a URL to an agent, which avoids that two different agents can crawl the same host at the same time. Moreover, since most hyperlinks are local, each agent will be himself responsible for the large majority of URLs found in a typical HTML page [Olston and Najork 2010]. Assignment of hosts to agents is by default performed using consistent hashing [Boldi et al. 2004]. Table I. Comparison between BUbiNG and the main existing open-source crawlers. Resources are HTML pages for ClueWeb09 and IRLBot, but include other data types (e.g., images) for ClueWeb12. For reference, we also report the throughput of IRLbot [Lee et al. 2009], although the latter is not open source. Note that ClueWeb09 was gathered using a heavily customized version of Nutch.\n\n\nResources\n\nResources Communication of URLs between agents is handled by the message-passing methods of the JGroups Java library; in particular, to make communication lightweight URLs are by default distributed using UDP. More sophisticated communications between the agents rely on the TCP-based JMX Java standard remote-control mechanism, which exposes most of the internal configuration parameters and statistics. Almost all crawler structures are indeed modifiable at runtime.\n\n\nEXPERIMENTS\n\nTesting a crawler is a delicate, intricate, arduous task: on one hand, every real-world experiment is obviously influenced by the hardware at one's disposal (in particular, by the available bandwidth). Moreover, real-world tests are difficult to repeat many times with different parameters: you will either end up disturbing the same sites over and over again, or choosing to visit every time a different portion of the web, with the risk of introducing artifacts in the evaluation. Given these considerations, we ran two kinds of experiments: one batch was performed in vitro with a HTTP proxy 6 simulating network connections towards the web and generating fake HTML pages (with a configurable behavior that includes delays, protocol exceptions etc.), and another batch of experiments was performed in vivo.\n\n\nIn vitro experiments: BUbiNG\n\nTo verify the robustness of BUbiNG when varying some basic parameters, such as the number of fetching threads or the IP delay, we decided to run some in vitro simulations on a group of four machines sporting 64 cores and 64 GB of core memory. In all experiments, the number of parsing and DNS threads was fixed and set respectively to 64 and 10. The size of the workbench was set to 512MB, while the size of the sieve was set to 256MB. We always set the host politeness delay equal to the IP politeness delay. Every in vitro experiment was run for 90 minutes. Fetching threads. The first thing we wanted to test was that increasing the number of fetching threads yields a better usage of the network, and hence a larger number of requests per second, until the bandwidth is saturated. The results of this experiment are shown in Figure 3 and have been obtained by having the proxy simulate a network that saturates quickly, using no politeness delay. The behavior visible in the plot tells us that the increase in the number of fetching threads yields a linear increase in speed until the available (simulated) bandwidth is reached; after that, the number of requests stabilizes to a plateau. Also this part of the plot tells us something: after saturating the bandwidth, we do not see any decrease in the throughput, witnessing the fact that our infrastructure does not cause any hindrance to the crawl. Politeness. The experiment described so far uses a small number of fetching threads, because the purpose was to show what happens before saturation. Now we show what happens under a heavy load. Our second in vitro experiment keeps the number of fetching threads fixed but increases the amount of politeness, as determined by the IP delay. We plot BUbiNG's throughput as the IP delay (hence the host delay) increases in Figure 4 (top): to maintain the same throughput, the front size (i.e., the number of hosts being visited in parallel) must increase, as expected. Moreover, this is independent on the number of threads (of course, until the network bandwidth is saturated).\n\nIn the same figure we show that the average throughput is independent from the politeness (and almost independent from the number of fetching threads), and the same is true of the CPU load. Even if this fact might seem surprising, this is the natural consequence of two observations: first, even with a small number of fetching threads, BUbiNG always tries to fill the bandwidth and to maximize its usage of computational resources; second, even varying the IP and host delay, BUbiNG modifies the number of hosts under visit to tune the interleaving between their processing.\n\n\nRaw speed.\n\nWe wanted to test the raw speed of a cluster of BUbiNG agents. We thus ran four agents using 1 000 fetching threads, until we gathered one billion pages, averaging 40 600 pages per second on the whole cluster. We also ran the same test on a single machine, obtaining essentially the same per-machine speed, showing that BUbiNG scales linearly with the number of agents in the cluster.\n\nTesting for bottlenecks: no I/O. Finally, we wanted to test whether our lock-free architecture was actually able to sustain a very high parallelism. To do so, we ran a no-I/O test on a 40-core workstation. The purpose of the test was to stress the computation and contention bottlenecks in absence of any interference from I/O: thus, input from the network was generated internally using the same logic of our proxy, and while data was fully processed (e.g., compressed) no actual storage was performed. After 100 million pages, the average speed was 16 000 pages/s (peak 22 500) up to 6 000 threads. We detected the first small decrease in speed (15 300 pages/s, peak 20 500) at 8 000 threads, which we believe is physiological due to increased context switch and Java garbage collection.  Fig. 4. The average size of the front, the average number of requests per second, and the average CPU load with respect to the IP delay (the host delay is set to eight times the IP delay). Note that the front adapts linearly to the growth of the IP delay, and, due to the essentially unlimited bandwidth of the proxy, the number of fetching threads is almost irrelevant.\n\n\nIn vitro experiments: Heritrix\n\nTo provide a comparison of BUbiNG with another crawler in a completely equivalent setting, we ran a raw-speed test using Heritrix 3.2.0 on the same hardware as in the BUbiNG raw-speed experiment, always using a proxy with the same setup. We configured Heritrix to use the same amount of memory, 20% of which was reserved for the Berkeley DB cache. We used 1 000 threads, locked the politeness interval to 10 seconds regardless of the download time (by default, Heritrix uses an adaptive scheme), and enabled content-based duplicate detection. 7 The results obtained will be presented and discussed in Section 5.4.\n\n\nIn vivo experiments\n\nWe performed a number of experiments in vivo at different sites. The main problem we had to face is that a single BUbiNG agent on sizable hardware can saturate a 1 Gb/s geographic link, so, in fact, we were not initially able to perform any test in which the network was not capping the crawler. Finally, iStella, an Italian commercial search engine provided us with a 48-core, 512 GB RAM with a 2 Gb/s link. The results confirm the knowledge we have gathered with our in vitro experiment: in the iStella experiment we were able to keep a steady download speed of 1.2 Gb/s using a single BUbiNG agent crawling the .it domain. The overall CPU load was about 85%.\n\n\nComparison\n\nWhen comparing crawlers, many measures are possible, and depending on the task at hand, different measures might be suitable. For instance, crawling all types of data (CSS, images, etc.) usually yields a significantly higher throughput than crawling just HTML, since HTML pages are often rendered dynamically, sometimes causing a significant delay, whereas most other types are served statically. The crawling policy has also a huge influence on the throughput: prioritizing by indegree (as IRLBot does [Lee et al. 2009]) or alternative importance measure shifts most of the crawl on sites hosted on powerful servers with large-bandwidth connection. Ideally, crawlers should be compared on a crawl with given number of pages in breadth-first fashion from a fixed seed, but some crawlers are not available to the public, which makes this goal unattainable.\n\nIn Table I we gather some evidence of the excellent performance of BUbiNG. Part of the data is from the literature, and part has been generated during our experiments.\n\nFirst of all, we report performance data for Nutch and Heritrix from the recent crawls made for the ClueWeb project (ClueWeb09 and ClueWeb12). The figures are those available in [Callan 2012] along with those found in [Clu 2009] and http: //boston.lti.cs.cmu.edu/crawler/crawlerstats.html: notice that the data we have about those collections are sometimes slightly contradictory (we report the best figures). The comparison with the ClueWeb09 crawl is somewhat unfair (the hardware used for that dataset was \"retired search-engine hardware\"), whereas the comparison with ClueWeb12 is more unbiased, as the hardware used was more recent. We report the throughput declared by IRLBot [Lee et al. 2009], too, albeit the latter is not open source and the downloaded data is not publicly available.\n\nThen, we report experimental in vitro data about Heritrix and BUbiNG obtained, as explained in the previous section, using the same hardware, a similar setup, and a HTTP proxy generating web pages. 8 This figures are the ones that can be compared more appropriately. Finally, we report the data of the iStella experiment.\n\nThe results of the comparison show quite clearly that the speed of BUbiNG is several times that of IRLBot and one to two orders of magnitude larger than that of Heritrix or Nutch. All in all, our experiments show that BUbiNG's adaptive design provides a very high throughput, in particular when a strong politeness is desired: indeed, from our comparison, the highest throughput. The fact that the throughput can be scaled linearly just by adding agents makes it by far the fastest crawling system publicly available.\n\n\nTHREE DATASETS\n\nAs a stimulating glimpse into the capabilities of BUbiNG to collect interesting datasets, we describe the main features of three snapshots collected with different criteria. All snapshots contain about one billion unique pages (the actual crawls are significantly larger, due to duplicates).\n\nuk-2014: a snapshot of the .uk domain, taken with a limit of 10 000 pages per host starting from the BBC website. eu-2015: a \"deep\" snapshot of the national domains of the European Union, taken with a limit of 10 000 000 pages per host starting from europa.eu. gsh-2015: a general \"shallow\" worldwide snapshot, taken with a limit of 100 pages per host, always starting from europa.eu.\n\nThe uk-2014 snapshot follows the tradition of our laboratory of taking snapshots of the .uk domain for linguistic uniformity, and to obtain a regional snapshot. The second and third snapshot aims at exploring the difference in the degree distribution and in website centrality in two very different kinds of data-gathering activities. In the first case, the limit on the pages per host is so large that, in fact, it was never reached; it is a quite faithful \"snowball sampling\" due to the breadth-first nature of BUbiNG's visits.\n\nIn the second case, we aim at maximizing the number of collected hosts by downloading very few pages per host. One of the questions we are trying to answer using the latter two snapshots is: how much is the indegree distribution dependent on the cardinality of sites (root pages have an indegree usually at least as large as the site size), and how much is it dependent on inter-site connections?\n\nThe main data, and some useful statistics about the three datasets, are shown in Table II. Among these, we have the average number of links per page (average outdegree) and the average number of links per page whose destination is on a different host (average external outdegree). Moreover, concerning the graph induced by the pages of our crawls, we also report the average distance, the harmonic diameter (e.g., the harmonic mean of all the distances), and the percentage of reachable pairs of pages in this graph (e.g., pairs of nodes (x, y) for which there exists a directed path from x to y).\n\n\nDegreee distribution\n\nThe indegree and outdegree distributions are shown in Figures 5, 6, 7 and 8. We provide both a degree-frequency plot decorated with Fibonacci binning [Vigna 2013], and a degree-rank plot 9 to highlight with more precision the tail behaviour.\n\nFrom Table II, we can see that pages at low depth tend to have less outlinks, but more external links than inner pages. The content is similarly smaller (content lives deeper in the structure of websites). Not surprisingly, moreover, pages of the shallow snapshot are closer to one another.\n\nThe most striking feature of the indegree distribution is an answer to our question: the tail of the indegree distribution is, by and large, shaped by the number of intrahost inlinks of root pages. This is very visible in the uk-2014 snapshot, where limiting the host size at 10 000 causes a sharp step in the degree-rank plot; and the same happens at 100 for gsh-2015. But what is maybe even more interesting is that the visible curvature of eu-2015 is almost absent from gsh-2015. Thus, if the latter (being mainly shaped by inter-host links) has some chance of being a power-law, as proposed by the class of \"richer get richer\" models, the former has none. Its curvature clearly shows that the indegree distribution is not a power-law (a phenomenon already noted in the analysis of the Common Crawl 2012 dataset [Meusel et al. 2015]): fitting it with the method by Clauset, Shalizi and Newman ] gives a p-value < 10 \u22125 (and the same happens for the top-level domain graph). Table III, IV and V report centrality data about our three snapshots. Since the pagelevel graph gives rise to extremely noisy results, we computed the host graph and the top-level domain graph. In the first graph, a node is a host, and there is an arc from host x to host y if some page of x points to some page of y. The second graph is built similarly, but now a node is a set of hosts sharing the same top-level domain (TLD). The TLD of a URL is determined from its host using the Public Suffix List published by the Mozilla Foundation, 10 and it is defined as one dot level above that the public suffix of the host: for example, a.com for b.a.com (as .com is on the public suffix list) and c.co.uk for a.b.c.co.uk (as .co.uk is on the public suffix list). 11 For each graph, we display the top ten nodes by indegree, PageRank (with constant preference vector and \u03b1 = 0.85) and by harmonic centrality , the harmonic mean of all distance towards a node. PageRank was computed with the highest possible precision in IEEE format using the LAW library, whereas harmonic centrality was approximated using HyperBall [Boldi and Vigna 2013].\n\n\nCentrality\n\nBesides the obvious shift of importance (UK government sites for uk-2014, government/news sites in eu-2015 and large US companies in gsh-2015), we con confirm the results of [Meusel et al. 2015]: on these kinds of graphs, harmonic centrality is much more precise and less prone to spam than indegree or PageRank. In the host graphs, almost all results of indegree and most results of PageRank are spam or service sites, whereas harmonic centrality identifies sites of interest (in particular in uk-2014 and eu-2015). At the TLD level, noise decreases significantly, but the difference in behavior is still striking, with PageRank and indegree still displaying several service sites, hosting providers and domain sellers as top results.\n\n\nCONCLUSIONS\n\nIn this paper we have presented BUbiNG, a new distributed open-source Java crawler. BUbiNG is orders of magnitudes faster than existing open-source crawlers, scales linearly with the number of agents, and will provide the scientific community with a reliable tool to gather large data sets.\n\nThe main novel ideas in the design of BUbiNG are:  -a pervasive usage of modern lock-free data structures to avoid contention among I/Obound fetching threads; -a new data structure, the workbench, that is able to provide in constant time the next URL to be fetched respecting politeness both at the host and IP level; -a simple but effective virtualizer-a memory-mapped, on-disk store of FIFO queues of URLs that do not fit into memory.\n\nBUbiNG pushes software components to their limits by using massive parallelism (typically, several thousand fetching threads); the result is a beneficial fallout on all related projects, as witnessed by several enhancements and bug reports to important software libraries like the Jericho HTML parser and the Apache Software Foundation HTTP client, in particular in the area of object creation and lock contention. In some cases, like a recent regression bug in the ASF client (JIRA issue 1461), it was exactly BUbiNG's high parallelism that made it possible to diagnose the regression.\n\nFuture work on BUbiNG includes integration with spam-detection software, and proper handling of spider traps (especially, but not only, those consisting in infinite non-cyclic HTTP-redirects); we also plan to implement policies for IP/host politeness throttling based on download times and site branching speed, and to integrate BUb-iNG with different stores like HBase, HyperTable and similar distributed storage systems. As briefly mentioned, it is easy to let BUbiNG follow a different priority order  than breadth first, provided that the priority is per host and per agent; the latter restriction can be removed at a moderate inter-agent communication cost. Prioritization at the level of URLs requires deeper changes in the inner structure of visit states and may be implemented using, for example, the Berkeley DB as a virtualizer: this idea will be a subject of future investigations. Another interesting direction is the integration with recently developed libraries which provides fibers, a user-space, lightweight alternative to threads that might further increase the amount of parallelism available using our synchronous I/O design.\n\nFig. 2 .\n2How the distributor interacts with the sieve, the workbench and the workbench virtualizer.\n\nFig. 3 .\n3The average number of pages per second with respect to the number of threads using a simulated slow connection. Note the linear increase in speed until the plateau, due to the limited (300) number of threads of the proxy.\n\nFig. 5 .Fig. 6 .\n56Indegree plots for uk-2014, eu-2015 and gsh-2015 (degree/frequency plots with Fibonacci binning). Indegree plots for uk-2014, eu-2015 and gsh-2015 (cumulative degree/rank plots).\n\nFig. 7 .Fig. 8 .\n78Outdegree plots for uk-2014, eu-2015 and gsh-2015 (degree/frequency plots with Fibonacci binning). Outdegree plots for uk-2014, eu-2015 and gsh-2015 (cumulative degree/rank plots).\n\n\nFig. 1. Overview of the architecture of a BUbiNG agent. Ovals represent data structures, whereas rectangles represent threads (or sets of threads).) \n\nSieve \nDistributor \n\nURL \n\nhost \u21a6 visit state \n\nDNSThread \n\nURL in \nnew host \n\nworkbench entry \n\nIP \u21a6 workbench entry \n\nWorkbench \n\nURL in \nknown host \n\nvisit state \n(acquire) \n\nTodoThread \n\nTodo queue \n\nFetchingThread \n\nResults queue \n\nParsingThread \n\nparsed! \n\nvisit state \n(put back) \n\nURL \ncache \n\nStore \n\nWorkbench \nVirtualizer \n\n(disk queues) \n\n(in memory) \n\npage, \nheaders \netc. \n\nURLs \nfound \n\nURL \n\n(2) \n\nother \nagents \n\n(3) \n\nURLs \n\nFrontier \n\nDoneThread \n\nDone queue \n\nRefill queue \n\nvisit state \n(to refill) \n\n\n\nTable II .\nIIBasic datauk-2014 \ngsh-2015 \neu-2015 \nOverall \n1 477 881 641 \n1 265 847 463 \n1 301 211 841 \nArchetypes \n787 830 045 \n1 001 310 571 \n1 070 557 254 \nAvg. content length \n56 039 \n32 526 \n57 027 \nAvg. outdegree \n105.86 \n96.34 \n142.60 \nAvg. external outdegree \n25.53 \n33.68 \n25.34 \nAvg. distance \n20.61 \n12.32 \n12.45 \nHarmonic diameter \n24.63 \n14.91 \n14.18 \nReachable pairs \n67.27% \n80.29% \n85.14% \n\n\n\nTable III .\nIIIMost relevant hosts and TPDs of uk-2014 by different centrality measures. Indegree PageRank Harmonic centrality Host Graph postcodeof.co.uk 726240 postcodeof.co.uk 0.02988 postcodeof.co.uk 1318833.61 namefun.co.uk 661692 namefun.co.ukTable IV. Most relevant hosts and TPDs of eu-2015 by different centrality measures. Host Graph www.toplist.cz 174433 www.myblog.de Table V. Most relevant hosts and TPDs of gsh-2015 by different centrality measures. Indegree PageRank Harmonic centrality Host Graph gmpg.org 2423978 wordpress.org 0.00885 www.google.com 18398649.60 www.google.com 1787380 www.google.comIndegree \n\nPageRank \n\nHarmonic centrality \n\n\nThe new ClueWeb12 dataset, that is going to be released soon, was collected using Heritrix, instead: five instances of Heritrix, running on five Dell PowerEdge R410, were run for three months, collecting 1.2 billions of pages. The average speed was of about 38.6 pages per second per machine. 3 In theory, Hadoop may perform the prioritization, de-duplication and distribution tasks while the crawler itself is running, but this choice would make the design very complex and we do not know of any implementation that chose to follow this approach.\nPossibly multiple resources on a single TCP connection using the \"keepalive\" feature of HTTP 1.1.\nEvery URL is made [Berners-Lee et al. 2005] by a scheme (also popularly called \"protocol\"), an authority (a host, optionally a port number, and perhaps some user information) and a path to the resource, possibly followed by a query (that is separated from the path by a \"?\"). BUbiNG's data structures are built around the pair scheme+authority, but in this paper we will use the more common word \"host\" to refer to it.\nThe proxy software is distributed along with the rest of BUbiNG.\nWe thank Gordon Mohr, one of the authors of Heritrix, for suggesting us how to configure it for a large workstation.\nNote that, with the purpose of stress testing the crawler internals, our HTTP proxy generates fairly short pages. This feature explains the wildly different ratio between MB/s and resources/s when looking at in vitro and in vivo experiments. 9 Degree-rank plots are the numerosity-based discrete analogous of the complementary cumulative distribution function of degrees. They give a much clearer picture than frequency dot plots when the data points are scattered and highly variable.\nhttp://publicsuffix.org/list/ 11 Top-level domains have been called pay-level domain in[Meusel et al. 2015] \nACKNOWLEDGMENTSWe thank our university for providing bandwidth for our experiments (and being patient with bugged releases). We thank Giuseppe Attardi, Antonio Cisternino and Maurizio Davini for providing the hardware, and the GARR Consortium for providing the bandwidth for experiments performed at the Universit\u00e0 di Pisa. Finally, we thank Domenico Dato and Renato Soru for providing the hardware and bandwidth for the iStella experiments.\n. Internet Archive Website, Internet Archive website. http://archive.org/web/web.php. (1996).\n\n. Web Heritrix, Site, Heritrix Web Site. https://webarchive.jira.com/wiki/display/Heritrix/. (2003).\n\nThe ClueWeb09 Dataset. The ClueWeb09 Dataset. http://lemurproject.org/clueweb09/. (2009).\n\nISO 28500:2009, Information and documentation -WARC file format. ISO 28500:2009, Information and documentation -WARC file format. http://www.iso.org/iso/catalogue detail.htm?csnumber=44717. (2009).\n\nOn the bias of traceroute sampling: Or, power-law degree distributions in regular graphs. Dimitris Achlioptas, Aaron Clauset, David Kempe, Cristopher Moore, Journal ACM. 5628Dimitris Achlioptas, Aaron Clauset, David Kempe, and Cristopher Moore. 2009. On the bias of traceroute sampling: Or, power-law degree distributions in regular graphs. Journal ACM 56, 4 (2009), 21:1-21:28.\n\nUniform Resource Identifier (URI): Generic Syntax. Tim Berners-Lee, Roy Thomas Fielding, Larry Masinter, Tim Berners-Lee, Roy Thomas Fielding, and Larry Masinter. 2005. Uniform Resource Identifier (URI): Generic Syntax. http://www.ietf.org/rfc/rfc3986.txt. (2005).\n\nSpace-Time Trade-offs in Hash Coding with Allowable Errors. H Burton, Bloom, Comm. ACM. 13Burton H. Bloom. 1970. Space-Time Trade-offs in Hash Coding with Allowable Errors. Comm. ACM 13, 7 (1970), 422-426.\n\nUbiCrawler: A Scalable Fully Distributed Web Crawler. Paolo Boldi, Bruno Codenotti, Massimo Santini, Sebastiano Vigna, Software: Practice & Experience. 34Paolo Boldi, Bruno Codenotti, Massimo Santini, and Sebastiano Vigna. 2004. UbiCrawler: A Scalable Fully Distributed Web Crawler. Software: Practice & Experience 34, 8 (2004), 711-726.\n\nBUbiNG: massive crawling for the masses. Paolo Boldi, Andrea Marino, Massimo Santini, Sebastiano Vigna, WWW'14 Companion. Paolo Boldi, Andrea Marino, Massimo Santini, and Sebastiano Vigna. 2014. BUbiNG: massive crawling for the masses. In WWW'14 Companion. 227-228.\n\nIn-Core Computation of Geometric Centralities with HyperBall: A Hundred Billion Nodes and Beyond. Paolo Boldi, Sebastiano Vigna, Proc. of 2013 IEEE 13th International Conference on Data Mining Workshops. of 2013 IEEE 13th International Conference on Data Mining WorkshopsIEEEPaolo Boldi and Sebastiano Vigna. 2013. In-Core Computation of Geometric Centralities with HyperBall: A Hundred Billion Nodes and Beyond. In Proc. of 2013 IEEE 13th International Conference on Data Mining Workshops (ICDMW 2013). IEEE.\n\nAxioms for Centrality. Paolo Boldi, Sebastiano Vigna, Internet Math. 10Paolo Boldi and Sebastiano Vigna. 2014. Axioms for Centrality. Internet Math. 10, 3-4 (2014), 222-262.\n\nThe anatomy of a large-scale hypertextual Web search engine. Computer Networks and ISDN Systems. Sergey Brin, Lawrence Page, 30Sergey Brin and Lawrence Page. 1998. The anatomy of a large-scale hypertextual Web search engine. Com- puter Networks and ISDN Systems 30, 1 (1998), 107-117.\n\nSyntactic clustering of the Web. Andrei Z Broder, Steven C Glassman, Mark S Manasse, Geoffrey Zweig, Selected papers from the sixth international conference on World Wide Web. Essex, UKElsevier Science Publishers LtdAndrei Z. Broder, Steven C. Glassman, Mark S. Manasse, and Geoffrey Zweig. 1997. Syntactic clustering of the Web. In Selected papers from the sixth international conference on World Wide Web. Elsevier Science Publishers Ltd., Essex, UK, 1157-1166. http://dl.acm.org/citation.cfm?id=283554.283370\n\nCrawling Towards Eternity: Building an Archive of the World Wide Web. M Burner, Web Techniques. 25M. Burner. 1997. Crawling Towards Eternity: Building an Archive of the World Wide Web. Web Techniques 2, 5 (1997).\n\nThe Lemur Project and its ClueWeb12 Dataset. Invited talk at the SIGIR 2012 Workshop on Open-Source Information Retrieval. Jamie Callan, Jamie Callan. 2012. The Lemur Project and its ClueWeb12 Dataset. Invited talk at the SIGIR 2012 Workshop on Open-Source Information Retrieval. (2012). http://opensearchlab.otago.ac.nz/ SIGIR12-OSIR-callan.pdf\n\nMining the web -discovering knowledge from hypertext data. Morgan Kaufmann. I-XVIII, 1-345 pages. Soumen Chakrabarti, Soumen Chakrabarti. 2003. Mining the web -discovering knowledge from hypertext data. Morgan Kauf- mann. I-XVIII, 1-345 pages.\n\nSimilarity Estimation Techniques from Rounding Algorithms. Moses Charikar, STOC. Moses Charikar. 2002. Similarity Estimation Techniques from Rounding Algorithms. In STOC. 380-388.\n\nPower-Law Distributions in Empirical Data. Aaron Clauset, Cosma Rohilla, M E J Shalizi, Newman, SIAM Rev. 51Aaron Clauset, Cosma Rohilla Shalizi, and M. E. J. Newman. 2009. Power-Law Distributions in Empirical Data. SIAM Rev. 51, 4 (2009), 661-703.\n\nAn adaptive model for optimizing performance of an incremental web crawler. Jenny Edwards, Kevin Mccurley, John Tomlin, 10.1145/371920.371960Proceedings of the 10th international conference on World Wide Web (WWW '01). the 10th international conference on World Wide Web (WWW '01)New York, NY, USAACMJenny Edwards, Kevin McCurley, and John Tomlin. 2001. An adaptive model for optimizing performance of an incremental web crawler. In Proceedings of the 10th international conference on World Wide Web (WWW '01). ACM, New York, NY, USA, 106-113. DOI:http://dx.doi.org/10.1145/371920.371960\n\nThe RBSE spider: balancing effective search against web load. D Eichmann, Proceedings of the first World Wide Web Conference. the first World Wide Web ConferenceGeneva, SwitzerlandD. Eichmann. 1994. The RBSE spider: balancing effective search against web load. In Proceedings of the first World Wide Web Conference. Geneva, Switzerland.\n\nA large-scale study of the evolution of Web pages. Dennis Fetterly, Mark Manasse, Marc Najork, Janet L Wiener, Proceedings of the Twelfth Conference on World Wide Web. the Twelfth Conference on World Wide WebBudapest, HungaryACM PressDennis Fetterly, Mark Manasse, Marc Najork, and Janet L. Wiener. 2003. A large-scale study of the evolu- tion of Web pages. In Proceedings of the Twelfth Conference on World Wide Web. ACM Press, Budapest, Hungary.\n\nMaintaining Distributed Hypertext Infostructures: Welcome to MOMspider. R Fielding, Proceedings of the 1st International Conference on the World Wide Web. the 1st International Conference on the World Wide WebR. Fielding. 1994. Maintaining Distributed Hypertext Infostructures: Welcome to MOMspider. In Proceed- ings of the 1st International Conference on the World Wide Web.\n\nMercator: A scalable, extensible Web crawler. Allan Heydon, Marc Najork, World Wide Web. 24Allan Heydon and Marc Najork. 1999. Mercator: A scalable, extensible Web crawler. World Wide Web 2, 4 (April 1999), 219-229.\n\nNutch: A flexible and scalable open-source web search engine. R Khare, D Cutting, K Sitaker, A Rifkin, Oregon State UniversityR. Khare, D. Cutting, K. Sitaker, and A. Rifkin. 2004. Nutch: A flexible and scalable open-source web search engine. Oregon State University (2004).\n\nIRLbot: Scaling to 6 billion pages and beyond. Hsin-Tsang Lee, Derek Leonard, Xiaoming Wang, Dmitri Loguinov, 10.1145/1541822.1541823ACM Trans. Web. 38Hsin-Tsang Lee, Derek Leonard, Xiaoming Wang, and Dmitri Loguinov. 2009. IRLbot: Scaling to 6 billion pages and beyond. ACM Trans. Web 3, 3, Article 8 (July 2009), 34 pages. DOI:http://dx.doi.org/10.1145/1541822.1541823\n\nDetecting near-duplicates for web crawling. Gurmeet Singh Manku, Arvind Jain, Anish Das Sarma, WWW '07: Proceedings of the 16th international conference on World Wide Web. New York, NY, USAACMGurmeet Singh Manku, Arvind Jain, and Anish Das Sarma. 2007. Detecting near-duplicates for web crawl- ing. In WWW '07: Proceedings of the 16th international conference on World Wide Web. ACM, New York, NY, USA, 141-150.\n\nGENVL and WWWW: Tools for Taming the Web. A Oliver, Mcbryan, Proceedings of the first World Wide Web Conference. the first World Wide Web ConferenceOliver A. McBryan. 1994. GENVL and WWWW: Tools for Taming the Web. In Proceedings of the first World Wide Web Conference. 79-90.\n\nRobert Meusel, Sebastiano Vigna, Oliver Lehmberg, Christian Bizer, The Graph Structure in the Web-Analyzed on Different Aggregation Levels. 1Robert Meusel, Sebastiano Vigna, Oliver Lehmberg, and Christian Bizer. 2015. The Graph Structure in the Web-Analyzed on Different Aggregation Levels. The Journal of Web Science 1, 1 (2015), 33-47.\n\nSimple, fast, and practical non-blocking and blocking concurrent queue algorithms. M Maged, Michael L Michael, Scott, Proceedings of the fifteenth annual ACM symposium on Principles of distributed computing (PODC '96). the fifteenth annual ACM symposium on Principles of distributed computing (PODC '96)ACMMaged M. Michael and Michael L. Scott. 1996. Simple, fast, and practical non-blocking and blocking con- current queue algorithms. In Proceedings of the fifteenth annual ACM symposium on Principles of dis- tributed computing (PODC '96). ACM, 267-275.\n\nA Brief History of Web Crawlers. M Seyed, Mustafa Emre Mirtaheri, Salman Dincturk, Hooshmand, V Gregor, Guy-Vincent Bochmann, Iosif-Viorel Jourdan, Onut, CASCON. Seyed M Mirtaheri, Mustafa Emre Dincturk, Salman Hooshmand, Gregor V Bochmann, Guy-Vincent Jour- dan, and Iosif-Viorel Onut. 2013. A Brief History of Web Crawlers. In CASCON.\n\nIntroduction to Heritrix, an archival quality web crawler. Gordon Mohr, Michele Kimpton, Micheal Stack, Igor Ranitovic, Proceedings of the 4th International Web Archiving Workshop (IWAW'04). the 4th International Web Archiving Workshop (IWAW'04)Gordon Mohr, Michele Kimpton, Micheal Stack, and Igor Ranitovic. 2004. Introduction to Heritrix, an archival quality web crawler. In Proceedings of the 4th International Web Archiving Workshop (IWAW'04).\n\nHigh-performance web crawling. Marc Najork, Allan Heydon, Handbook of massive data sets, James Abello, Panos M. Pardalos. Mauricio G. C. ResendeKluwer Academic PublishersMarc Najork and Allan Heydon. 2002. High-performance web crawling. In Handbook of massive data sets, James Abello, Panos M. Pardalos, and Mauricio G. C. Resende (Eds.). Kluwer Academic Publishers, 25-45.\n\n. Christopher Olston, Marc Najork, Web Crawling. Foundations and Trends in Information Retrieval. 4Christopher Olston and Marc Najork. 2010. Web Crawling. Foundations and Trends in Information Retrieval 4, 3 (2010), 175-246.\n\nFinding What People Want: Experiences with the WebCrawler. Brian Pinkerton, Proceedings of the 2nd International World Wide Web (Online & CDROM review: the international journal of). the 2nd International World Wide Web (Online & CDROM review: the international journal of)Medford, NJ, USA18Learned InformationBrian Pinkerton. 1994. Finding What People Want: Experiences with the WebCrawler. In Proceedings of the 2nd International World Wide Web (Online & CDROM review: the international journal of), Anonymous (Ed.), Vol. 18(6). Learned Information, Medford, NJ, USA.\n\nDesign and Implementation of a High-Performance Distributed Web Crawler. Vladislav Shkapenyuk, Torsten Suel, Proc. of the Int. Conf. on Data Engineering. of the Int. Conf. on Data EngineeringVladislav Shkapenyuk and Torsten Suel. 2002. Design and Implementation of a High-Performance Dis- tributed Web Crawler. In In Proc. of the Int. Conf. on Data Engineering. 357-368.\n\nFibonacci Binning. Sebastiano Vigna, CoRR abs/1312.3749Sebastiano Vigna. 2013. Fibonacci Binning. CoRR abs/1312.3749 (2013).\n", "annotations": {"author": "[{\"end\":374,\"start\":43}]", "publisher": null, "author_last_name": "[{\"end\":54,\"start\":49}]", "author_first_name": "[{\"end\":48,\"start\":43}]", "author_affiliation": "[{\"end\":122,\"start\":56},{\"end\":204,\"start\":124},{\"end\":288,\"start\":206},{\"end\":373,\"start\":290}]", "title": "[{\"end\":40,\"start\":1},{\"end\":414,\"start\":375}]", "venue": null, "abstract": "[{\"end\":1511,\"start\":584}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3202,\"start\":3184},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6598,\"start\":6578},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7066,\"start\":7042},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8545,\"start\":8525},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8894,\"start\":8881},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8923,\"start\":8909},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8950,\"start\":8936},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8979,\"start\":8964},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9364,\"start\":9352},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":9562,\"start\":9543},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":9849,\"start\":9825},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":9902,\"start\":9878},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9939,\"start\":9913},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9978,\"start\":9957},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":10014,\"start\":9995},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10221,\"start\":10204},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10530,\"start\":10506},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":10551,\"start\":10530},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10705,\"start\":10681},{\"end\":11134,\"start\":11115},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11150,\"start\":11134},{\"end\":11314,\"start\":11304},{\"end\":11960,\"start\":11951},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":12054,\"start\":12036},{\"end\":12993,\"start\":12984},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13061,\"start\":13049},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":14191,\"start\":14180},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":15075,\"start\":15056},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20968,\"start\":20944},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":21911,\"start\":21894},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":22614,\"start\":22597},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":23142,\"start\":23131},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":24094,\"start\":24077},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":29854,\"start\":29842},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":30673,\"start\":30654},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":30698,\"start\":30684},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":30744,\"start\":30722},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":30761,\"start\":30744},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":30799,\"start\":30781},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":32783,\"start\":32767},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":40671,\"start\":40653},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":41179,\"start\":41155},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":41278,\"start\":41260},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":41542,\"start\":41525},{\"end\":47798,\"start\":47797},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":49088,\"start\":49070},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":49783,\"start\":49771},{\"end\":49821,\"start\":49811},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":50292,\"start\":50275},{\"end\":50890,\"start\":50872},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":53639,\"start\":53627},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":54847,\"start\":54827},{\"end\":55751,\"start\":55749},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":56124,\"start\":56102},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":56334,\"start\":56314},{\"end\":56655,\"start\":56617},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":62526,\"start\":62511},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":63678,\"start\":63658}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":59456,\"start\":59355},{\"attributes\":{\"id\":\"fig_1\"},\"end\":59689,\"start\":59457},{\"attributes\":{\"id\":\"fig_3\"},\"end\":59888,\"start\":59690},{\"attributes\":{\"id\":\"fig_4\"},\"end\":60089,\"start\":59889},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":60765,\"start\":60090},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":61175,\"start\":60766},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":61837,\"start\":61176}]", "paragraph": "[{\"end\":2045,\"start\":1527},{\"end\":2371,\"start\":2047},{\"end\":3053,\"start\":2373},{\"end\":3348,\"start\":3055},{\"end\":3419,\"start\":3350},{\"end\":3458,\"start\":3421},{\"end\":5434,\"start\":3460},{\"end\":6121,\"start\":5436},{\"end\":6355,\"start\":6123},{\"end\":6452,\"start\":6370},{\"end\":8719,\"start\":6454},{\"end\":10106,\"start\":8737},{\"end\":10391,\"start\":10108},{\"end\":10553,\"start\":10393},{\"end\":12028,\"start\":10578},{\"end\":13446,\"start\":12030},{\"end\":13713,\"start\":13472},{\"end\":14616,\"start\":13715},{\"end\":15279,\"start\":14618},{\"end\":15475,\"start\":15281},{\"end\":15887,\"start\":15477},{\"end\":16282,\"start\":15889},{\"end\":16688,\"start\":16284},{\"end\":17105,\"start\":16690},{\"end\":17297,\"start\":17107},{\"end\":17799,\"start\":17299},{\"end\":18385,\"start\":17801},{\"end\":19008,\"start\":18387},{\"end\":19493,\"start\":19010},{\"end\":20056,\"start\":19495},{\"end\":20798,\"start\":20070},{\"end\":21683,\"start\":20800},{\"end\":22983,\"start\":21685},{\"end\":23775,\"start\":22985},{\"end\":24202,\"start\":23793},{\"end\":24757,\"start\":24204},{\"end\":25132,\"start\":24759},{\"end\":25687,\"start\":25134},{\"end\":26073,\"start\":25689},{\"end\":27326,\"start\":26075},{\"end\":27665,\"start\":27347},{\"end\":27948,\"start\":27667},{\"end\":28373,\"start\":27950},{\"end\":28872,\"start\":28375},{\"end\":29262,\"start\":28874},{\"end\":29712,\"start\":29282},{\"end\":30000,\"start\":29714},{\"end\":30801,\"start\":30002},{\"end\":31046,\"start\":30803},{\"end\":31473,\"start\":31062},{\"end\":32126,\"start\":31503},{\"end\":32681,\"start\":32128},{\"end\":33129,\"start\":32683},{\"end\":33615,\"start\":33131},{\"end\":33871,\"start\":33617},{\"end\":34344,\"start\":33873},{\"end\":35060,\"start\":34346},{\"end\":35494,\"start\":35062},{\"end\":35680,\"start\":35514},{\"end\":36319,\"start\":35682},{\"end\":36679,\"start\":36321},{\"end\":37457,\"start\":36681},{\"end\":37886,\"start\":37459},{\"end\":38237,\"start\":37888},{\"end\":38569,\"start\":38239},{\"end\":38915,\"start\":38571},{\"end\":39322,\"start\":38935},{\"end\":39821,\"start\":39324},{\"end\":39960,\"start\":39836},{\"end\":40510,\"start\":39962},{\"end\":40812,\"start\":40535},{\"end\":41661,\"start\":40814},{\"end\":42143,\"start\":41675},{\"end\":42968,\"start\":42159},{\"end\":45080,\"start\":43001},{\"end\":45657,\"start\":45082},{\"end\":46056,\"start\":45672},{\"end\":47219,\"start\":46058},{\"end\":47867,\"start\":47254},{\"end\":48552,\"start\":47891},{\"end\":49422,\"start\":48567},{\"end\":49591,\"start\":49424},{\"end\":50386,\"start\":49593},{\"end\":50709,\"start\":50388},{\"end\":51228,\"start\":50711},{\"end\":51538,\"start\":51247},{\"end\":51924,\"start\":51540},{\"end\":52455,\"start\":51926},{\"end\":52853,\"start\":52457},{\"end\":53452,\"start\":52855},{\"end\":53718,\"start\":53477},{\"end\":54010,\"start\":53720},{\"end\":56125,\"start\":54012},{\"end\":56875,\"start\":56140},{\"end\":57181,\"start\":56891},{\"end\":57619,\"start\":57183},{\"end\":58207,\"start\":57621},{\"end\":59354,\"start\":58209}]", "formula": null, "table_ref": "[{\"end\":41288,\"start\":41281},{\"end\":49434,\"start\":49427},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":52944,\"start\":52936},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":53733,\"start\":53725},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":54998,\"start\":54989}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1525,\"start\":1513},{\"attributes\":{\"n\":\"2.\"},\"end\":6368,\"start\":6358},{\"attributes\":{\"n\":\"3.\"},\"end\":8735,\"start\":8722},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10576,\"start\":10556},{\"attributes\":{\"n\":\"4.\"},\"end\":13470,\"start\":13449},{\"attributes\":{\"n\":\"4.1.\"},\"end\":20068,\"start\":20059},{\"attributes\":{\"n\":\"4.2.\"},\"end\":23791,\"start\":23778},{\"attributes\":{\"n\":\"4.3.\"},\"end\":27345,\"start\":27329},{\"attributes\":{\"n\":\"4.4.\"},\"end\":29280,\"start\":29265},{\"attributes\":{\"n\":\"4.5.\"},\"end\":31060,\"start\":31049},{\"attributes\":{\"n\":\"4.6.\"},\"end\":31501,\"start\":31476},{\"attributes\":{\"n\":\"4.7.\"},\"end\":35512,\"start\":35497},{\"attributes\":{\"n\":\"4.8.\"},\"end\":38933,\"start\":38918},{\"attributes\":{\"n\":\"4.9.\"},\"end\":39834,\"start\":39824},{\"attributes\":{\"n\":\"4.10.\"},\"end\":40533,\"start\":40513},{\"end\":41673,\"start\":41664},{\"attributes\":{\"n\":\"5.\"},\"end\":42157,\"start\":42146},{\"attributes\":{\"n\":\"5.1.\"},\"end\":42999,\"start\":42971},{\"end\":45670,\"start\":45660},{\"attributes\":{\"n\":\"5.2.\"},\"end\":47252,\"start\":47222},{\"attributes\":{\"n\":\"5.3.\"},\"end\":47889,\"start\":47870},{\"attributes\":{\"n\":\"5.4.\"},\"end\":48565,\"start\":48555},{\"attributes\":{\"n\":\"6.\"},\"end\":51245,\"start\":51231},{\"attributes\":{\"n\":\"6.1.\"},\"end\":53475,\"start\":53455},{\"attributes\":{\"n\":\"6.2.\"},\"end\":56138,\"start\":56128},{\"attributes\":{\"n\":\"7.\"},\"end\":56889,\"start\":56878},{\"end\":59364,\"start\":59356},{\"end\":59466,\"start\":59458},{\"end\":59707,\"start\":59691},{\"end\":59906,\"start\":59890},{\"end\":60777,\"start\":60767},{\"end\":61188,\"start\":61177}]", "table": "[{\"end\":60765,\"start\":60239},{\"end\":61175,\"start\":60790},{\"end\":61837,\"start\":61793}]", "figure_caption": "[{\"end\":59456,\"start\":59366},{\"end\":59689,\"start\":59468},{\"end\":59888,\"start\":59710},{\"end\":60089,\"start\":59909},{\"end\":60239,\"start\":60092},{\"end\":60790,\"start\":60780},{\"end\":61793,\"start\":61192}]", "figure_ref": "[{\"end\":15474,\"start\":15466},{\"end\":38049,\"start\":38041},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":43838,\"start\":43830},{\"end\":44833,\"start\":44825},{\"end\":46855,\"start\":46849},{\"end\":53543,\"start\":53531}]", "bib_author_first_name": "[{\"end\":64222,\"start\":64219},{\"end\":64707,\"start\":64699},{\"end\":64725,\"start\":64720},{\"end\":64740,\"start\":64735},{\"end\":64758,\"start\":64748},{\"end\":65043,\"start\":65040},{\"end\":65060,\"start\":65057},{\"end\":65067,\"start\":65061},{\"end\":65083,\"start\":65078},{\"end\":65316,\"start\":65315},{\"end\":65521,\"start\":65516},{\"end\":65534,\"start\":65529},{\"end\":65553,\"start\":65546},{\"end\":65573,\"start\":65563},{\"end\":65847,\"start\":65842},{\"end\":65861,\"start\":65855},{\"end\":65877,\"start\":65870},{\"end\":65897,\"start\":65887},{\"end\":66171,\"start\":66166},{\"end\":66189,\"start\":66179},{\"end\":66607,\"start\":66602},{\"end\":66625,\"start\":66615},{\"end\":66857,\"start\":66851},{\"end\":66872,\"start\":66864},{\"end\":67079,\"start\":67073},{\"end\":67081,\"start\":67080},{\"end\":67096,\"start\":67090},{\"end\":67098,\"start\":67097},{\"end\":67113,\"start\":67109},{\"end\":67115,\"start\":67114},{\"end\":67133,\"start\":67125},{\"end\":67624,\"start\":67623},{\"end\":67895,\"start\":67890},{\"end\":68218,\"start\":68212},{\"end\":68423,\"start\":68418},{\"end\":68588,\"start\":68583},{\"end\":68614,\"start\":68613},{\"end\":68618,\"start\":68615},{\"end\":68871,\"start\":68866},{\"end\":68886,\"start\":68881},{\"end\":68901,\"start\":68897},{\"end\":69442,\"start\":69441},{\"end\":69774,\"start\":69768},{\"end\":69789,\"start\":69785},{\"end\":69803,\"start\":69799},{\"end\":69817,\"start\":69812},{\"end\":69819,\"start\":69818},{\"end\":70239,\"start\":70238},{\"end\":70594,\"start\":70589},{\"end\":70607,\"start\":70603},{\"end\":70823,\"start\":70822},{\"end\":70832,\"start\":70831},{\"end\":70843,\"start\":70842},{\"end\":70854,\"start\":70853},{\"end\":71093,\"start\":71083},{\"end\":71104,\"start\":71099},{\"end\":71122,\"start\":71114},{\"end\":71135,\"start\":71129},{\"end\":71459,\"start\":71452},{\"end\":71479,\"start\":71473},{\"end\":71495,\"start\":71486},{\"end\":71864,\"start\":71863},{\"end\":72105,\"start\":72099},{\"end\":72124,\"start\":72114},{\"end\":72138,\"start\":72132},{\"end\":72158,\"start\":72149},{\"end\":72522,\"start\":72521},{\"end\":72537,\"start\":72530},{\"end\":72539,\"start\":72538},{\"end\":73029,\"start\":73028},{\"end\":73044,\"start\":73037},{\"end\":73049,\"start\":73045},{\"end\":73067,\"start\":73061},{\"end\":73090,\"start\":73089},{\"end\":73110,\"start\":73099},{\"end\":73133,\"start\":73121},{\"end\":73398,\"start\":73392},{\"end\":73412,\"start\":73405},{\"end\":73429,\"start\":73422},{\"end\":73441,\"start\":73437},{\"end\":73818,\"start\":73814},{\"end\":73832,\"start\":73827},{\"end\":74171,\"start\":74160},{\"end\":74184,\"start\":74180},{\"end\":74448,\"start\":74443},{\"end\":75037,\"start\":75028},{\"end\":75057,\"start\":75050},{\"end\":75356,\"start\":75346}]", "bib_author_last_name": "[{\"end\":64148,\"start\":64124},{\"end\":64231,\"start\":64223},{\"end\":64237,\"start\":64233},{\"end\":64718,\"start\":64708},{\"end\":64733,\"start\":64726},{\"end\":64746,\"start\":64741},{\"end\":64764,\"start\":64759},{\"end\":65055,\"start\":65044},{\"end\":65076,\"start\":65068},{\"end\":65092,\"start\":65084},{\"end\":65323,\"start\":65317},{\"end\":65330,\"start\":65325},{\"end\":65527,\"start\":65522},{\"end\":65544,\"start\":65535},{\"end\":65561,\"start\":65554},{\"end\":65579,\"start\":65574},{\"end\":65853,\"start\":65848},{\"end\":65868,\"start\":65862},{\"end\":65885,\"start\":65878},{\"end\":65903,\"start\":65898},{\"end\":66177,\"start\":66172},{\"end\":66195,\"start\":66190},{\"end\":66613,\"start\":66608},{\"end\":66631,\"start\":66626},{\"end\":66862,\"start\":66858},{\"end\":66877,\"start\":66873},{\"end\":67088,\"start\":67082},{\"end\":67107,\"start\":67099},{\"end\":67123,\"start\":67116},{\"end\":67139,\"start\":67134},{\"end\":67631,\"start\":67625},{\"end\":67902,\"start\":67896},{\"end\":68230,\"start\":68219},{\"end\":68432,\"start\":68424},{\"end\":68596,\"start\":68589},{\"end\":68611,\"start\":68598},{\"end\":68626,\"start\":68619},{\"end\":68634,\"start\":68628},{\"end\":68879,\"start\":68872},{\"end\":68895,\"start\":68887},{\"end\":68908,\"start\":68902},{\"end\":69451,\"start\":69443},{\"end\":69783,\"start\":69775},{\"end\":69797,\"start\":69790},{\"end\":69810,\"start\":69804},{\"end\":69826,\"start\":69820},{\"end\":70248,\"start\":70240},{\"end\":70601,\"start\":70595},{\"end\":70614,\"start\":70608},{\"end\":70829,\"start\":70824},{\"end\":70840,\"start\":70833},{\"end\":70851,\"start\":70844},{\"end\":70861,\"start\":70855},{\"end\":71097,\"start\":71094},{\"end\":71112,\"start\":71105},{\"end\":71127,\"start\":71123},{\"end\":71144,\"start\":71136},{\"end\":71471,\"start\":71460},{\"end\":71484,\"start\":71480},{\"end\":71501,\"start\":71496},{\"end\":71871,\"start\":71865},{\"end\":71880,\"start\":71873},{\"end\":72112,\"start\":72106},{\"end\":72130,\"start\":72125},{\"end\":72147,\"start\":72139},{\"end\":72164,\"start\":72159},{\"end\":72528,\"start\":72523},{\"end\":72547,\"start\":72540},{\"end\":72554,\"start\":72549},{\"end\":73035,\"start\":73030},{\"end\":73059,\"start\":73050},{\"end\":73076,\"start\":73068},{\"end\":73087,\"start\":73078},{\"end\":73097,\"start\":73091},{\"end\":73119,\"start\":73111},{\"end\":73141,\"start\":73134},{\"end\":73147,\"start\":73143},{\"end\":73403,\"start\":73399},{\"end\":73420,\"start\":73413},{\"end\":73435,\"start\":73430},{\"end\":73451,\"start\":73442},{\"end\":73825,\"start\":73819},{\"end\":73839,\"start\":73833},{\"end\":74178,\"start\":74172},{\"end\":74191,\"start\":74185},{\"end\":74458,\"start\":74449},{\"end\":75048,\"start\":75038},{\"end\":75062,\"start\":75058},{\"end\":75362,\"start\":75357}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":64215,\"start\":64122},{\"attributes\":{\"id\":\"b1\"},\"end\":64317,\"start\":64217},{\"attributes\":{\"id\":\"b2\"},\"end\":64408,\"start\":64319},{\"attributes\":{\"id\":\"b3\"},\"end\":64607,\"start\":64410},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":785270},\"end\":64987,\"start\":64609},{\"attributes\":{\"id\":\"b5\"},\"end\":65253,\"start\":64989},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":7931252},\"end\":65460,\"start\":65255},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":325714},\"end\":65799,\"start\":65462},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":1364237},\"end\":66066,\"start\":65801},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":9744150},\"end\":66577,\"start\":66068},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":10140116},\"end\":66752,\"start\":66579},{\"attributes\":{\"id\":\"b11\"},\"end\":67038,\"start\":66754},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":9022773},\"end\":67551,\"start\":67040},{\"attributes\":{\"id\":\"b13\"},\"end\":67765,\"start\":67553},{\"attributes\":{\"id\":\"b14\"},\"end\":68112,\"start\":67767},{\"attributes\":{\"id\":\"b15\"},\"end\":68357,\"start\":68114},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":4229473},\"end\":68538,\"start\":68359},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":9155618},\"end\":68788,\"start\":68540},{\"attributes\":{\"doi\":\"10.1145/371920.371960\",\"id\":\"b18\",\"matched_paper_id\":10316730},\"end\":69377,\"start\":68790},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":6244421},\"end\":69715,\"start\":69379},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":14635587},\"end\":70164,\"start\":69717},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":1735160},\"end\":70541,\"start\":70166},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":207736356},\"end\":70758,\"start\":70543},{\"attributes\":{\"id\":\"b23\"},\"end\":71034,\"start\":70760},{\"attributes\":{\"doi\":\"10.1145/1541822.1541823\",\"id\":\"b24\",\"matched_paper_id\":322407},\"end\":71406,\"start\":71036},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":1414324},\"end\":71819,\"start\":71408},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":15161281},\"end\":72097,\"start\":71821},{\"attributes\":{\"id\":\"b27\"},\"end\":72436,\"start\":72099},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":5387463},\"end\":72993,\"start\":72438},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":15786644},\"end\":73331,\"start\":72995},{\"attributes\":{\"id\":\"b30\"},\"end\":73781,\"start\":73333},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":7738017},\"end\":74156,\"start\":73783},{\"attributes\":{\"id\":\"b32\"},\"end\":74382,\"start\":74158},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":58482468},\"end\":74953,\"start\":74384},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":10651529},\"end\":75325,\"start\":74955},{\"attributes\":{\"doi\":\"CoRR abs/1312.3749\",\"id\":\"b35\"},\"end\":75451,\"start\":75327}]", "bib_title": "[{\"end\":64697,\"start\":64609},{\"end\":65313,\"start\":65255},{\"end\":65514,\"start\":65462},{\"end\":65840,\"start\":65801},{\"end\":66164,\"start\":66068},{\"end\":66600,\"start\":66579},{\"end\":67071,\"start\":67040},{\"end\":67621,\"start\":67553},{\"end\":68416,\"start\":68359},{\"end\":68581,\"start\":68540},{\"end\":68864,\"start\":68790},{\"end\":69439,\"start\":69379},{\"end\":69766,\"start\":69717},{\"end\":70236,\"start\":70166},{\"end\":70587,\"start\":70543},{\"end\":71081,\"start\":71036},{\"end\":71450,\"start\":71408},{\"end\":71861,\"start\":71821},{\"end\":72519,\"start\":72438},{\"end\":73026,\"start\":72995},{\"end\":73390,\"start\":73333},{\"end\":73812,\"start\":73783},{\"end\":74441,\"start\":74384},{\"end\":75026,\"start\":74955}]", "bib_author": "[{\"end\":64150,\"start\":64124},{\"end\":64233,\"start\":64219},{\"end\":64239,\"start\":64233},{\"end\":64720,\"start\":64699},{\"end\":64735,\"start\":64720},{\"end\":64748,\"start\":64735},{\"end\":64766,\"start\":64748},{\"end\":65057,\"start\":65040},{\"end\":65078,\"start\":65057},{\"end\":65094,\"start\":65078},{\"end\":65325,\"start\":65315},{\"end\":65332,\"start\":65325},{\"end\":65529,\"start\":65516},{\"end\":65546,\"start\":65529},{\"end\":65563,\"start\":65546},{\"end\":65581,\"start\":65563},{\"end\":65855,\"start\":65842},{\"end\":65870,\"start\":65855},{\"end\":65887,\"start\":65870},{\"end\":65905,\"start\":65887},{\"end\":66179,\"start\":66166},{\"end\":66197,\"start\":66179},{\"end\":66615,\"start\":66602},{\"end\":66633,\"start\":66615},{\"end\":66864,\"start\":66851},{\"end\":66879,\"start\":66864},{\"end\":67090,\"start\":67073},{\"end\":67109,\"start\":67090},{\"end\":67125,\"start\":67109},{\"end\":67141,\"start\":67125},{\"end\":67633,\"start\":67623},{\"end\":67904,\"start\":67890},{\"end\":68232,\"start\":68212},{\"end\":68434,\"start\":68418},{\"end\":68598,\"start\":68583},{\"end\":68613,\"start\":68598},{\"end\":68628,\"start\":68613},{\"end\":68636,\"start\":68628},{\"end\":68881,\"start\":68866},{\"end\":68897,\"start\":68881},{\"end\":68910,\"start\":68897},{\"end\":69453,\"start\":69441},{\"end\":69785,\"start\":69768},{\"end\":69799,\"start\":69785},{\"end\":69812,\"start\":69799},{\"end\":69828,\"start\":69812},{\"end\":70250,\"start\":70238},{\"end\":70603,\"start\":70589},{\"end\":70616,\"start\":70603},{\"end\":70831,\"start\":70822},{\"end\":70842,\"start\":70831},{\"end\":70853,\"start\":70842},{\"end\":70863,\"start\":70853},{\"end\":71099,\"start\":71083},{\"end\":71114,\"start\":71099},{\"end\":71129,\"start\":71114},{\"end\":71146,\"start\":71129},{\"end\":71473,\"start\":71452},{\"end\":71486,\"start\":71473},{\"end\":71503,\"start\":71486},{\"end\":71873,\"start\":71863},{\"end\":71882,\"start\":71873},{\"end\":72114,\"start\":72099},{\"end\":72132,\"start\":72114},{\"end\":72149,\"start\":72132},{\"end\":72166,\"start\":72149},{\"end\":72530,\"start\":72521},{\"end\":72549,\"start\":72530},{\"end\":72556,\"start\":72549},{\"end\":73037,\"start\":73028},{\"end\":73061,\"start\":73037},{\"end\":73078,\"start\":73061},{\"end\":73089,\"start\":73078},{\"end\":73099,\"start\":73089},{\"end\":73121,\"start\":73099},{\"end\":73143,\"start\":73121},{\"end\":73149,\"start\":73143},{\"end\":73405,\"start\":73392},{\"end\":73422,\"start\":73405},{\"end\":73437,\"start\":73422},{\"end\":73453,\"start\":73437},{\"end\":73827,\"start\":73814},{\"end\":73841,\"start\":73827},{\"end\":74180,\"start\":74160},{\"end\":74193,\"start\":74180},{\"end\":74460,\"start\":74443},{\"end\":75050,\"start\":75028},{\"end\":75064,\"start\":75050},{\"end\":75364,\"start\":75346}]", "bib_venue": "[{\"end\":66339,\"start\":66272},{\"end\":67225,\"start\":67216},{\"end\":69087,\"start\":69009},{\"end\":69559,\"start\":69505},{\"end\":69942,\"start\":69885},{\"end\":70375,\"start\":70321},{\"end\":71597,\"start\":71580},{\"end\":71969,\"start\":71934},{\"end\":72741,\"start\":72657},{\"end\":73578,\"start\":73524},{\"end\":74673,\"start\":74567},{\"end\":75146,\"start\":75109},{\"end\":64340,\"start\":64319},{\"end\":64473,\"start\":64410},{\"end\":64777,\"start\":64766},{\"end\":65038,\"start\":64989},{\"end\":65341,\"start\":65332},{\"end\":65612,\"start\":65581},{\"end\":65921,\"start\":65905},{\"end\":66270,\"start\":66197},{\"end\":66646,\"start\":66633},{\"end\":66849,\"start\":66754},{\"end\":67214,\"start\":67141},{\"end\":67647,\"start\":67633},{\"end\":67888,\"start\":67767},{\"end\":68210,\"start\":68114},{\"end\":68438,\"start\":68434},{\"end\":68644,\"start\":68636},{\"end\":69007,\"start\":68931},{\"end\":69503,\"start\":69453},{\"end\":69883,\"start\":69828},{\"end\":70319,\"start\":70250},{\"end\":70630,\"start\":70616},{\"end\":70820,\"start\":70760},{\"end\":71183,\"start\":71169},{\"end\":71578,\"start\":71503},{\"end\":71932,\"start\":71882},{\"end\":72237,\"start\":72166},{\"end\":72655,\"start\":72556},{\"end\":73155,\"start\":73149},{\"end\":73522,\"start\":73453},{\"end\":73903,\"start\":73841},{\"end\":74254,\"start\":74193},{\"end\":74565,\"start\":74460},{\"end\":75107,\"start\":75064},{\"end\":75344,\"start\":75327}]"}}}, "year": 2023, "month": 12, "day": 17}