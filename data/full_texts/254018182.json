{"id": 254018182, "updated": "2023-11-17 15:36:59.615", "metadata": {"title": "Cross Aggregation Transformer for Image Restoration", "authors": "[{\"first\":\"Zheng\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Yulun\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Jinjin\",\"last\":\"Gu\",\"middle\":[]},{\"first\":\"Yongbing\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Linghe\",\"last\":\"Kong\",\"middle\":[]},{\"first\":\"Xin\",\"last\":\"Yuan\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Recently, Transformer architecture has been introduced into image restoration to replace convolution neural network (CNN) with surprising results. Considering the high computational complexity of Transformer with global attention, some methods use the local square window to limit the scope of self-attention. However, these methods lack direct interaction among different windows, which limits the establishment of long-range dependencies. To address the above issue, we propose a new image restoration model, Cross Aggregation Transformer (CAT). The core of our CAT is the Rectangle-Window Self-Attention (Rwin-SA), which utilizes horizontal and vertical rectangle window attention in different heads parallelly to expand the attention area and aggregate the features cross different windows. We also introduce the Axial-Shift operation for different window interactions. Furthermore, we propose the Locality Complementary Module to complement the self-attention mechanism, which incorporates the inductive bias of CNN (e.g., translation invariance and locality) into Transformer, enabling global-local coupling. Extensive experiments demonstrate that our CAT outperforms recent state-of-the-art methods on several image restoration applications. The code and models are available at https://github.com/zhengchen1999/CAT.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/ChenZGzKY22", "doi": "10.48550/arxiv.2211.13654"}}, "content": {"source": {"pdf_hash": "1c49e58a935c80c8ec8307e937fc61c8f1f80433", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2211.13654v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "10c54c174d4dcbc9d24a945191d2d161ba03aa49", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/1c49e58a935c80c8ec8307e937fc61c8f1f80433.txt", "contents": "\nCross Aggregation Transformer for Image Restoration\n\n\nZheng Chen \nYulun Zhang \nJinjin Gu \nThe University of Sydney\n\n\nYongbing Zhang \nHarbin Institute of Technology (Shenzhen)\n\n\nLinghe Kong \nXin Yuan \nWestlake University\n\n\nShanghai Jiao \nTong University \nEth Z\u00fcrich \nShanghai Ai Laboratory \nCross Aggregation Transformer for Image Restoration\n\nRecently, Transformer architecture has been introduced into image restoration to replace convolution neural network (CNN) with surprising results. Considering the high computational complexity of Transformer with global attention, some methods use the local square window to limit the scope of self-attention. However, these methods lack direct interaction among different windows, which limits the establishment of long-range dependencies. To address the above issue, we propose a new image restoration model, Cross Aggregation Transformer (CAT). The core of our CAT is the Rectangle-Window Self-Attention (Rwin-SA), which utilizes horizontal and vertical rectangle window attention in different heads parallelly to expand the attention area and aggregate the features cross different windows. We also introduce the Axial-Shift operation for different window interactions. Furthermore, we propose the Locality Complementary Module to complement the self-attention mechanism, which incorporates the inductive bias of CNN (e.g., translation invariance and locality) into Transformer, enabling global-local coupling. Extensive experiments demonstrate that our CAT outperforms recent state-of-theart methods on several image restoration applications. The code and models are available at https://github.com/zhengchen1999/CAT.\n\nIntroduction\n\nImage restoration is a long-term low-level vision problem, aimed at recovering high-quality (HQ) images from low-quality (LQ) counterparts. Depending on the type of degradation, there are many sub-problems, such as super-resolution (SR), image denoising, and JPEG compression artifact reduction. In recent decades, many researchers have proposed various theories and methods to solve this problem, e.g., inverse filtering and wiener filtering. Meanwhile, with the increase of computing resources, neural networks have become the mainstream of image restoration, especially convolutional neural network (CNN) [10,11,50], which achieve impressive performance over traditional methods.\n\nRecently, some researchers have been using Transformer proposed in natural language processing (NLP) to replace CNN and achieving excellent performance in multiple vision tasks, such as image classification [13], semantic segmentation [12], and object detection [41]. The core component of Transformer is the multi-head self-attention (MHSA), which can effectively model long-range dependencies, while CNN is more inclined to extract local features. Although Transformer can effectively handle dependencies between tokens, its complexity is quadratic with the image resolution (H\u00d7W ), i.e., O((HW ) 2 ), which limits its application to high-resolution image tasks (including image restoration). To apply Transformer in image restoration, some methods [5,21,44] propose to divide the image into smaller independent patches (e.g., 8\u00d78) and perform attention separately or apply self-attention (SA) in the feature dimension instead of the spatial dimension, thus achieving linear complexity. However, the square window lacks inter-window interaction, leading to the slow increase of the receptive field. Moreover, the channel-wise attention mechanism may lose some spatial information. All these problems restrict the performance of the Transformer in image restoration.\n\nTo solve the above problems of Transformer application in image restoration tasks, we propose a new image restoration Transformer model named Cross Aggregation Transformer (CAT). Our CAT utilizes local window self-attention to restrict the computational complexity and aggregates features cross different windows to expand the receptive field. The key part of our CAT is a new self-attention mechanism, named Rectangle-Window Self-Attention (Rwin-SA). Specifically, the Rwin-SA performs the attention operation in a separate rectangle window (sh =sw) rather than the square (sh=sw) one, where sh and sw are window height and width. And we divide multiple heads into two groups and perform horizontal (sh<sw) and vertical (sh>sw) rectangle window self-attention in parallel. This method effectively expands the attention area and aggregates different features in horizontal and vertical directions while keeping the computational complexity unchanged. Moreover, inspired by CSwin [12], which performs cross-shaped window self-attention, we fix the length of one side of the rectangle window to be H or W (image resolution). Then the window becomes a stripe along the axis, called axial rectangle window. It can generate more window interactions and obtain a larger attention area than the regular rectangle window.\n\nWe also propose a new shift operation named Axial-Shift Operation, which is applied between consecutive Rwin-SA, to further increase the interaction of the different windows. Like our Rwin-SA, axial-shift is also divided into horizontal and vertical directions, which acts on the corresponding window respectively. Compared with the shift operation in Swin Transformer [23], our axial-shift explicitly realizes the interaction between horizontal-horizontal or vertical-vertical windows, and implicitly enables the interaction between horizontal and vertical windows.\n\nFurthermore, we propose Locality Complementary Module (LCM), a convolution operation that it performed on the value (V ) in the self-attention mechanism in parallel with the attention part. Compared with Transformer that mainly builds global long-range dependencies, CNN has the characteristics of translation invariance and locality, and can effectively capture 2D local structures of the image (e.g., corners and edges). The LCM can complements Rwin-SA with locality information, enabling the coupling of global (self-attention) and local (convolution) information.\n\nBased on the above three designs, our CAT realizes the feature aggregation cross different windows, thus possessing long-range dependencies modeling capability. We apply our CAT to several classic image restoration tasks: image SR, JPEG compression artifacts reduction, and real image denoising. Extensive experiments demonstrate the superiority of our method. Our contributions are as follows:\n\n\u2022 We propose a new image restoration Transformer model named Cross Aggregation Transformer (CAT). The CAT utilizes the window self-attention and aggregates the features cross different windows, thus achieving a large receptive field with linear complexity. \u2022 We propose a novel self-attention mechanism, named Rwin-SA, using rectangle window self-attention with axial-shift operation. Moreover, we propose the locality complementary module to realize the coupling of global and local information. \u2022 We apply our CAT to classic image restoration tasks: image SR, JPEG compression artifact reduction, and real image denoising. Extensive experiments show that our proposed model achieves state-of-the-art performance both quantitatively and visually.\n\n\nRelated Work\n\nImage Restoration. CNNs have been achieving impressive performance over the traditional approaches and becoming the mainstream method in the field of image restoration since Dong et al. proposed SRCNN [11] and ARCNN [10] for image SR and JPEG compression artifact reduction, respectively. Currently, many efforts have been devoted to building very deep CNNs to improve performance through multiple methods, e.g., residual learning strategies [15], dense connection [16], dropout [19], and UNet architecture [34]. Meanwhile, spatial and channel attention mechanisms [51,31,52,20,6] are utilized to make the network more focused on specific information in the feature space. However, most CNN-based methods are still hard to capture global dependencies.\n\nVision Transformer. Vision Transformer (ViT) achieves surprising results in high-level vision tasks [13,41]. Meanwhile, many efficient attention mechanisms have been proposed to improve the performance of ViT. Swin Transformer [23] uses local window attention and realizes the interaction between windows through shift operations. CSwin [12] proposes cross-shaped window attention, and Twins [7] adopt a combination of global and local attention. Inspired by the success of Transformer in high-level tasks, some works have been trying to apply Transformer to low-level vision tasks, such as IPT [5]. However, employing original ViT [13] for restoration tasks is hard since the complexity of full self-attention is quadratic to image size. Some methods have been proposed to solve this problem. SwinIR [21], based on Swin Transformer, utilizes an 8\u00d78 window to divide the image and executes self-attention in each window separately. It also uses the shift operation.\n\nUformer [39] also applies 8\u00d78 local window and introduces UNet architecture to capture much more global dependencies. Restormer [44] calculates cross-covariance across channel dimensions rather than spatial ones, making the complexity of SA linear with image resolution. However, the 8\u00d78 square window limits the receptive field of the Transformer even with the shift operation. And the channel-wise attention mechanisms may lose spatial information. Furthermore, above methods do not effectively combine convolution with attention mechanisms, thus limiting model capabilities.\n\n\nMethod\n\nTo build an efficient Transformer model suitable for image restoration, we propose a rectanglewindow self-attention (Rwin-SA) to replace vanilla multi-head self-attention mechanism in ViT [13] and form the cross aggregation Transformer block (CATB). Then we use RCAN [51] as the backbone and replace the residual channel attention block (RCAB) proposed in RCAN with our CATB to build the model cross aggregation Transformer (CAT), which is illustrated in Fig. 1. We first describe the overall structure of CAT and then introduce the three core designs of the CATB, including rectangle-window self-attention, axial-shift operation, and locality complementary module.\n\n\nCAT Architecture\n\nAs shown in Fig. 1, following RCAN [51], our proposed CAT consists of three modules: shallow feature extraction, deep feature extraction, and reconstruction part. For a low-quality input image I LQ \u2208R H\u00d7W \u00d7Cin , we leverage one convolution layer as shallow feature extraction to get the lowlevel feature F 0 \u2208R H\u00d7W \u00d7C , where H, W, C in , C are the image height, width, input channel, and the feature number, respectively. Then, the shallow feature is processed by the deep feature extraction module, which is composed of N 1 residual groups (RG) [51] and one convolution layer, and extracted the deep feature (denoted as F DF \u2208R H\u00d7W \u00d7C ). The convolution layer here is employed to aggregate previously extracted features from RG. Moreover, a residual connection is applied in the deep feature extraction part to make the training stable. We use CATB to replace RCAB [51] in RG, as we mentioned above. So, RG consists of N 2 cross aggregation Transformer blocks and a convolution layer that can introduce the unique properties (translation invariance and locality) of CNN into the output of Transformer blocks. In addition, a residual strategy is also applied in each RG.\n\nFinally, we can get the high-quality output image I HQ \u2208R H\u00d7W \u00d7Cout through the reconstruction part, where C out is the output dimension. And following SwinIR [21], the composition of reconstruction module is different for different image restoration tasks. More specifically, for image SR, a sub-pixel convolution layer [36] is used to upsample the deep feature F DF to the same size of the highresolution output and there is a convolution layer before and after the upsampling module to aggregate the features. For JPEG compression artifact reduction, the reconstruction is just a convolution layer to adjust the channel dimension of the F DF from C to C out . The low-quality input I LQ is then added to the convolution output to produce a high-quality output I HQ . This residual learning can accelerate network model convergence. Moreover, for real image denoising, following Restormer [44], we apply our proposed CATB to the UNet architecture [34] to construct CAT. \n\n\nCross Aggregation Transformer Block\n\nThe cross aggregation Transformer block (CATB) is the key part of our CAT, shown in Fig. 1, which is more suitable for image restoration tasks than the previous Transformer structures [21,44]. It utilizes a new attention mechanism with three novel designs: rectangle-window self-attention, axial-shift operation, and locality complementary module. We will describe these parts in detail below.\n\nRectangle-Window Self-Attention. To alleviate the drawbacks of previous methods [5,21,44], we propose a new window attention mechanism as shown in Fig. 2(a), called rectangle-window selfattention (Rwin-SA). Rwin-SA uses the rectangle window (sh =sw) rather than the square (sh=sw) one, where sh and sw represent the height and width of the rectangle. Moreover, we divide the rectangle window into the horizontal one (sh<sw, denoted as H-Rwin) and the vertical one (sh>sw, denoted as V-Rwin), and employ them to different attention heads in parallel.\n\nSpecifically, given the input X\u2208R H\u00d7W \u00d7C , we execute the attention operation M (the heads number) times in parallel. For each attention head, we split X into non-overlapping sh\u00d7sw rectangle windows and denote the i-th rectangle window feature as X i \u2208R (sh\u00d7sw)\u00d7C , where i=1, . . . , H\u00d7W sh\u00d7sw . Then the self-attention of X i for the m-th head can be computed as\n(Q i m , K i m , V i m ) = (X i W m Q , X i W m K , X i W m V ), Y m i = Attention(Q i m , K i m , V i m ) = SoftMax( Q i m (K i m ) T \u221a d + B)V i m ,(1)\nwhere Y m i \u2208R (sh\u00d7sw)\u00d7D is the attention feature of X i in the m-th head, and Q i m , K i m , V i m \u2208R C\u00d7d represent the projection matrices of query, key, and value for m-th head. D=d= C M is the channel dimension in each head and B is the dynamic relative position encoding [38]. Performing the attention operation on all X i (i=1, . . . , H\u00d7W sh\u00d7sw ) and reshaping and merging them in the order of division, we can get the attention feature Y m \u2208R H\u00d7W \u00d7D of X. In general, the above attention operation is the same for H-Rwin and V-Rwin, where the sh and sw take different values.\n\nAssuming that the number of attention heads M is even, we divide the heads equally into two parts and execute H-Rwin for the first part and V-Rwin for the second part in parallel. Finally, the outputs of two parts are concatenated along the channel dimension. The process is formulated as\nRwin-SA(X) = Conact(Y 1 , Y 2 , . . . , Y M )W p ,(2)\nwhere Y 1 , . . . , Y M 2 are the output of the head using H-Rwin, and Y M 2 +1 , . . . , Y M are from the V-Rwin. The W p \u2208R C\u00d7C represents the projection matrix for feature fusion. As shown in Fig. 3, through H-Rwin and V-Rwin, we can aggregate features cross different windows and expand the attention area without increasing computational complexity. Moreover, the rectangle window can capture different features in horizontal and vertical directions for each pixel, which is hard for square window to handle with comparable computation resource. This property is crucial for some datasets (e.g., Urban100 [17]) that contain many directional and repetitive texture features. Furthermore, as shown in Fig. 3, inspired by CSwin [12], we fix the length of one side of the rectangle to be the image resolution H or W (the other side length denoted as sl). Then the window becomes a stripe along the axis, called axial rectangle window (denoted as axial-Rwin). Since the axial window changes as the image changes, the axial-Rwin is more flexible than the regular rectangle window, which side length is [sh, sw] or [sw, sh] (denoted as regular-Rwin). The computational complexity of regular-Rwin and axial-Rwin are\nO(regular-Rwin) = HW C \u00d7 (4C + 2sh \u00d7 sw), O(axial-Rwin) = HW C \u00d7 (4C + sl \u00d7 H + sl \u00d7 W ).(3)\nWhen H=W (square input), the complexity of regular-Rwin is O(H 2 ) and axial-Rwin is O(H 3 ). Although axial-Rwin is more complex than regular-Rwin, it is still smaller than full-attention (O(H 4 )), and applicable for high-resolution images. And axial-Rwin has a larger attention area than that in regular-Rwin, which can capture more information, especially on the axial direction.\n\nAxial-Shift Operation. To further expand the receptive field of Rwin-SA so that each pixel can aggregate more information, we propose a new shift operation called axial-shift, for Rwin-SA. As shown in Fig. 2(b), axial-shift includes two shift operations, the horizontal one is called H-Shift and the vertical one is called V-Shift, corresponding to H-Rwin and V-Rwin, respectively. The axial-shift operation moves the window partition down and left by sh 2 and sw 2 pixels, where sh and sw are the window height and width of H-Rwin and V-Rwin. In the implementation, we cyclically shift the feature map to down-left direction. Then we execute H-Rwin and V-Rwin on corresponding shifted feature maps and use a masking mechanism to avoid false interactions between non-adjacent pixels.\n\nThe axial-shift operation can be regarded as a more general shifted window operation in Swin Transformer [23]. Locality Complementary Module. Transformer can effectively capture global information and model long-range dependencies between pixels. However, the inductive bias (translation invariance and locality) of CNN are still indispensable in image restoration tasks. It can aggregate local features and extract the basic structures of an image (e.g., corners and edges). In order to complement the Transformer with the locality and achieve global and local coupling, we propose to add an independent convolution operation when calculating self-attention, called locality complementary module. As shown in Fig. 4, different from the previous practice in Transformer [42,8], we directly operate convolution on value V . The process is formulated as\nRwin-SA(X) = (Conact(Y 1 , Y 2 , . . . , Y M ) + Conv(V ))W P ,(4)\nwhere Y 1 , . . . , Y M , and W P are the same as Eq. (2). The V \u2208R H\u00d7W \u00d7C is the value projected directly from X without window partition (V i m is the partition of V in m-th attention head defined in Eq. (1)), and Conv(\u00b7) represents the 3\u00d73 depth-wise convolution.\n\nThis operation has two advantages over executing convolution sequentially or using convolution on X: (1) Using convolution as a parallel module allows Transformer block to adaptively choose to adopt attention or convolution operations, which is more flexible than sequential execution; (2) From Eq. (1), we can find that self-attention can be regarded as a content-dependent and dynamic weight acting on V . The convolution operation is equivalent to a learnable and static weight. Therefore, the convolution operation on V is conducted in the same feature domain as the attention operation.   Cross Aggregation Transformer Block. CATB is composed of Rwin-SA and MLP, which has two linear projection layers with a GELU non-linearity between them. We formulate the block as X = Rwin-SA(LN(X in )) + X in ,\nX out = MLP(LN(X )) + X ,(5)\nwhere X in and X out are the input and output feature maps of CATB; LN(\u00b7) is the LayerNorm operation. And axial-shift operation is used on the interval between two consecutive cross aggregation Transformer blocks to enhance the interaction among different windows.\n\n\nExperiments\n\n\nExperimental Settings\n\nData and Evaluation. For image SR, we choose DIV2K [37] and Flickr2K [22] as the training data. And benchmark datasets for evaluation include Set5 [3], Set14 [48], B100 [27], Urban100 [17], and Manga109 [28] with three upscaling factors: \u00d72, \u00d73, and \u00d74. The low-resolution images are generated by Bicubic downsampling. For JPEG compression artifact reduction, training set consists of DIV2K [37], Flickr2K [22], BSD500 [2], and WED [26]. And we have two testing datasets: Classic5 [14] and LIVE1 [35], with JPEG compression qualities of 10, 20, 30, and 40. For real image denoising, we train CAT on SIDD [1] dataset. And we have two testing datasets: SIDD validation set [1] and DND [33]. We use the metrics PSNR and SSIM [40] [44]. The total training iterations are 300K. We adopt AdamW optimizer [25] with \u03b2 1 =0.9 and \u03b2 2 =0.99 to minimize the L 1 loss. The initial learning rate is 3\u00d710 \u22124 and gradually reduced to 1\u00d710 \u22126 with the cosine annealing [24]. Furthermore, we randomly utilize rotation and flipping to augment the training data for image SR, JPEG compression artifact reduction, and real image denoising. We use PyTorch [32] to implement our models with 4 Tesla V100 GPUs. IGNN [54] HAN [31] CSNLN [30] SwinIR [21] CAT (ours) Figure 5: Visual comparison about image SR (\u00d74) in some challenging cases.\n\n\nAblation Study\n\nFor the ablation study, we use the dataset DIV2K [37] and Flickr2K [22] to train CAT on the image SR (\u00d72) task, where the input image patch size is 64\u00d764 and the iterations are 150K. The testing dataset is Urban100 [17]. FLOPs are calculated on input image size 3\u00d7128\u00d7128.\n\nRectangle-Window Self-Attention. Table 1a demonstrates that rectangle-window self-attention is more effective than the square window attention (e.g., SwinIR [21]). With no increase in computational complexity (e.g., FLOPs), the rectangle window can improve the performance from 32.50 dB to 32.66 dB compared to the square one. Moreover, when we adopt the axial-shift operation in Rwin-SA, PSNR can reach 32.91 dB. And the square window attention of the swin shift operation only obtains 32.75 dB. It can be found that our Rwin-SA can aggregate more information and is more suitable for image restoration than attention mechanisms in Swin Transformer [23].\n\nLocality Complementary Module. Table 1b shows the impact of locality complementary module on CAT-R (regular-Rwin) and CAT-A (axial-Rwin). With the LCM, CAT-R and CAT-A can achieve 0.07 dB and 0.10 dB gains over the cases without LCM, respectively. These results show that the inductive bias of CNN can effectively improve the performance of Transformer in image restoration. And comparing the complexity, it can be found that LCM only increases FLOPs by 0.26%~0.32%, which will not affect the model efficiency too much. Furthermore, LCM is embedded into self-attention as a new module without changing the implementation of the attention part.\n\nWindow Size Impact. From above analyses, we know that our Rwin-SA is more suitable for image restoration tasks than the swin attention mechanism. We further investigate the impact of different window sizes. The results are provided in Table 1c. Comparing regular-Rwin (CAT-R) and axial-Rwin (CAT-A), we can find that the performance of axial-Rwin is worse than regular-Rwin (32.97 dB vs. 32.98 dB), when the side of axial-Rwin is small (sl=2), despite its higher complexity (323.5G vs. 282.7G). It may be because the side of axial-Rwin (sl) is too narrow to capture enough information and may even introduce noise. Therefore, the result of axial-Rwin rises rapidly as sl increase. These results show that both side length and window size of Rwin are crucial for performance.\n\n\nImage Super-Resolution\n\nWe compare our two models CAT-R and CAT-A with state-of-the-art methods: EDSR [22], RCAN [51], SAN [9], IGNN [54], HAN [31], CSNLN [30], NLSA [29], IPT [5], and SwinIR [21]. Following the previous work [22,21,51], we use self-ensemble strategy in testing and mark the model with a symbol \"+\". We use CAT-A for visual comparisons, abbreviated as CAT.   Table 3: Quantitative comparison (PSNR/SSIM) with state-of-the-art methods for JPEG compression artifact reduction. Best and second best results are colored with red and blue.\n\nQuantitative Results. Table 2 shows PSNR/SSIM comparisons for \u00d72, \u00d73, and \u00d74 image SR. As we can see, our CAT-R (regular-Rwin) and CAT-A (axial-Rwin) significantly outperform other methods on all datasets with all scale factors. Among these five datasets, CAT-R and CAT-A have the most noticeable improvement in Urban100. Compared with the recent Transformer model (e.g., SwinIR), for CAT-R, the PSNR gain reaches 0.27 dB for scale factor 2, and for CAT-A, the PSNR increases by 0.45 dB. It shows that our CAT can capture more global information than previous CNN-based and Transformer-based models, which is very effective for images in Urban100 with a large number of repetitive texture structures. Moreover, compared with CAT-R, CAT-A yields 0.18~0.27 dB improvements on Urban100. All these results indicate the effectiveness of our method.\n\nVisual Results. We show visual comparisons (\u00d74) in Fig. 5. We can observe that most compared methods can hardly recover accurate textures and suffer from blurring artifacts in some challenging cases. In contrast, our CAT can reconstruct more high-frequency structural details and alleviate the blurring artifacts. For example, in img_092, our CAT can recover correct and sharp textures, while  most compared methods fail to recover image details. These results demonstrate that our CAT has more robust representational ability to recover structural contents and textural details. It also shows that, compared with other models, our CAT is more effective in modeling long-range dependencies, which are crucial for reconstructing high-quality images.\n\n\nJPEG Compression Artifact Reduction\n\nWe compare our CAT with state-of-the-art image compression artifact removal methods: RNAN [52], RDN [53], DRUNet [49], and SwinIR [21]. Here, we only focus on the restoration of Y channel (in YCbCr space). We also use self-ensemble strategy and mark the model with a symbol \"+\". Quantitative and visual comparisons are shown in Table 3 and Fig. 6, respectively.\n\nQuantitative Results. Table 3 shows quantitative comparisons with four JPEG quality settings: 10, 20, 30, and 40. Our CAT+ performs better than other compared methods on LIVE1 and Classic5 with all JPEG qualities. Even without self-ensemble, our RCAN also outperforms other compared methods, except for PSNR on Classic5 (q=10). Especially when q=40, CAT achieves 0.05 and 0.06 dB improvements over the Transformer-based model SwinIR. Even in a very low compression quality (e.g., 10), CAT achieves 29.89 dB in LIVE1, higher than previous methods.\n\nVisual Results. We show visual comparisons at very low image quality (q=10) in Fig. 6. For previous methods, RNAN [52], RDN [53], and SwinIR [21], blocking artifacts are hardly to be totally removed and some structures are over-smoothed in some challenging cases. In contrast, our CAT can recover more details and remove blocking artifacts, resulting in sharper edges and more explicit textures. These visual comparisons further demonstrate the effectiveness of our proposed CAT.\n\n\nReal Image Denoising\n\nWe further conduct real image denoising experiments. We compare our CAT with state-of-theart methods: DANet+ [43], CycleISP [45], MIRNet [46], MPRNet [47], Uformer [39], and Restormer [44]. We use self-ensemble [22] strategy and mark the model with a symbol \"+\".\n\nQuantitative Results. Table 4 shows quantitative comparisons for real image denoising. We re-test the SIDD results with all official pre-trained models on previous state-of-the-art methods. Our CAT performs better than other compared methods on SIDD and DND, except Restormer [44]. Meanwhile, our method has comparable performance to Restormer with fewer parameters. Our CAT achieves 0.02 dB improvements over Restormer on DND. All these show the superiority of our method.   Table 5 shows the comparison of performance, computational complexity (e.g., FLOPs), and parameter numbers on image SR. FLOPs are measured when output size is set to 3\u00d7512\u00d7512 and PSNR values are tested on Urban100 (\u00d74). Although our CAT achieves excellent performance, it has less computational complexity and parameters than EDSR [22]. And the computational complexity of our CAT is much lower than CSNLN [30]. Compared to other CNN-based and Transformer-based methods, our CAT still has a comparable complexity and model size. To demonstrate the effectiveness of our method, we provide another variant of CAT (CAT-R-2) with similar computational complexity and parameters to SwinIR. Our CAT-R-2 still outperforms SwinIR and other state-of-the-art methods. More details about CAT-R-2 are provided in the supplementary material.\n\n\nModel Size Analyses\n\n\nConclusion\n\nIn this paper, we propose a new Transformer model named cross aggregation Transformer (CAT) for image restoration with the rectangle-window self-attention (Rwin-SA) mechanism as the key part. Specifically, Rwin-SA performs H-Rwin and V-Rwin attention parallelly in different heads. This mechanism can aggregate the features cross different windows and increase the receptive field without additional computational cost. Meanwhile, we propose an axial-shift operation according to the property of rectangle-window to further fuse different windows, which complements Rwin-SA. Furthermore, the locality complementary module (LCM) introduces locality into self-attention, adding the inductive bias of CNN to Transformer. Through the usage of LCM, Rwin-SA can realize the coupling of global and local information, which is more conducive to image restoration. Extensive experiments on image SR, JPEG compression artifact reduction, and real image denoising demonstrate that our proposed CAT outperforms current state-of-the-art methods.\n\nFigure 1 :\n1(a) The architecture of our CAT. (b) Illustration of CATB.\n\nFigure 2 :\n2(a) Illustration of rectangle-window self-attention mechanism. The sh and sw are the window height and width for H-Rwin and V-Rwin. For a pixel (red dot), the attention area is the pink region of the middle feature map. (b) Illustration of axial-shift operation.\n\nFigure 3 :\n3Illustration of attention expansion, directional features aggregation, and axial-Rwin. H, W , and sl mean height, width, and the other side length.\n\nFigure 4 :\n4When sh and sw in H-Rwin and V-Rwin are all the same, it degenerates into the swin shift operation. Compared with the swin shift operation, our axial-shift enables a broader range of window interaction: (1) H(V)-Rwin with H(V)-Rwin. Intuitively, the new window partition enables the interactive fusion of different rectangle window of the same type (horizontal or vertical one), just like the swin shift operation. (2) H-Rwin with V-Rwin. The Rwin-SA performs H-Rwin and V-Rwin simultaneously and fuses them through the projection in Eq. (2). For this reason, axial-shift can implicitly implement interaction between horizontal and vertical rectangle windows. Illustration of locality complementary module.\n\n1 :\n1Ablation study. (a) Sq.: square window (8\u00d78) self-attention; Re.: regular rectangle window (4\u00d716) self-attention; shift: swin shift operation; axial: axial-shift operation. (b) C-R: CAT-R (regular-Rwin) model, [sw, sh] is [4, 16] (or 16, 4]); C-A: CAT-A (axial-Rwin) model, sl is [2, 2, 2, 4, 4, 4]; LCM: locality complementary module. (c) The sl of C-\n\nTable\n\n\n\nto evaluate the results. Details. We set the residual group (RG) number as N 1 =6 and the cross aggregation Transformer block (CATB) number as N 2 =6 for each RG. The channel dimension, attention head number, and MLP expansion ratio for each CATB are set as 180, 6, and 4, respectively. For image SR, we build two versions of CAT, called CAT-R and CAT-A. CAT-R uses the regular rectangle window (regular-Rwin), and CAT-A uses the axial rectangle window (axial-Rwin). The regular-Rwin and the axial-Rwin are defined in Sec. 3.2. For CAT-R, we set [sh, sw] as[4,16] (or[16,4]) for each CATB in each RG, and for CAT-A, we set sl as[2,2,2,4,4,4] for different residual groups. For JPEG compression artifact reduction, we also employ CAT-A with the same sl setting as in image SR. We apply axial-Rwin here and set sl as 4 for all CATB.Implementation For \nreal image denoising, following Restormer [44], we employ a 4-level encoder-decoder. The number \nof Transformer blocks is [4, 6, 6, 8], from level-1 to level-4. We set attention heads as [2, 2, 4, 8] and \nMLP expansion ratio as 4. Training Settings. For image SR, we train the model with batch size 32, where each input image \nis randomly cropped to 64\u00d764 size, and the total training iterations are 500K. We adopt Adam \noptimizer [18] with \u03b2 1 =0.9 and \u03b2 2 =0.99 to minimize the L 1 loss following the work [51, 52, 21]. The \ninitial learning rate is set as 2\u00d710 \u22124 and reduced by half at the milestone [250K,400K,450K,475K]. \nFor JPEG compression artifact reduction, the batch size and total iterations are 8 and 1600K, where \nthe input image size is 128\u00d7128. Same as image SR, we adopt Adam optimizer to minimize \nCharbonnier Loss [4] with =10 \u22123 , and the learning rate is initialized to 2\u00d710 \u22124 , which is halved at \n[800K,1200K,1400K,1500K]. For real image denoising, we use the progressive learning proposed by \nRestormer \n\nTable 2 :\n2Quantitative comparison (PSNR/SSIM) with state-of-the-art methods for image SR. Best \nand second best results are colored with red and blue. \n\nDataset \nq \nRNAN [52] \nRDN [53] \nDRUNet [49] \nSwinIR [21] \nCAT (ours) \nCAT+ (ours) \nPSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM PSNR SSIM \n\nLIVE1 \n\n10 29.63 0.8239 29.67 0.8247 29.79 0.8278 29.86 0.8287 29.89 0.8295 29.92 0.8299 \n20 32.03 0.8877 32.07 0.8882 32.17 0.8899 32.25 0.8909 32.30 0.8913 32.32 0.8915 \n30 33.45 0.9149 33.51 0.9153 33.59 0.9166 33.69 0.9174 33.73 0.9177 33.75 0.9179 \n40 34.47 0.9299 34.51 0.9302 34.58 0.9312 34.67 0.9317 34.72 0.9320 34.74 0.9322 \n\nClassic5 \n\n10 29.96 0.8178 30.00 0.8188 30.16 0.8234 30.27 0.8249 30.26 0.8250 30.30 0.8257 \n20 32.11 0.8693 32.15 0.8699 32.39 0.8734 32.52 0.8748 32.57 0.8754 32.60 0.8756 \n30 33.38 0.8924 33.43 0.8930 33.59 0.8949 33.73 0.8961 33.77 0.8964 33.80 0.8966 \n40 34.27 0.9061 34.27 0.9061 34.41 0.9075 34.52 0.9082 34.58 0.9087 34.60 0.9088 \n\n\n\n\nFigure 6: Visual comparison about JPEG compression artifacts reduction (q=10).LIVE1: buildings \n\nHQ \nJPEG (q=10) \nRNAN [52] \nRDN [53] \nSwinIR [21] \nCAT (ours) \n\nLIVE1: carnivaldolls \nHQ \nJPEG (q=10) \nRNAN [52] \nRDN [53] \nSwinIR [21] \nCAT (ours) \n\nDataset \nDANet+ [43] CycleISP [45] MIRNet [46] MPRNet [47] Uformer [39] Restormer [44] CAT (ours) CAT+ (ours) \nParameters (M) \n9.15 \n2.83 \n31.79 \n15.74 \n50.88 \n26.11 \n25.77 \n25.77 \n\nSIDD* \nPSNR \n39.47 \n39.52 \n39.72 \n39.71 \n39.89 \n40.02 \n40.01 \n40.05 \nSSIM \n0.9570 \n0.9571 \n0.9586 \n0.9586 \n0.9594 \n0.9603 \n0.9600 \n0.9602 \n\nDND \nPSNR \n39.58 \n39.56 \n39.88 \n39.82 \n39.98 \n40.03 \n40.05 \n40.08 \nSSIM \n0.9545 \n0.9564 \n0.9563 \n0.9540 \n0.9554 \n0.9564 \n0.9561 \n0.9563 \n\n\n\nTable 4 :\n4Quantitative comparison (PSNR/SSIM) for real image denoising. Best and second best results are colored with red and blue. * We re-test the SIDD with all official pre-trained models.\n\n\nMethod EDSR [22] RCAN [51] HAN [31] CSNLN [30] SwinIR [21] CAT-R (ours) CAT-A (ours) CAT-R-2 (ours)PSNR (dB) \n26.64 \n26.82 \n26.85 \n27.22 \n27.45 \n27.62 \n27.89 \n27.59 \nFLOPs (G) \n823.3 \n261.0 \n269.1 \n84,155.2 \n215.3 \n292.7 \n360.7 \n216.3 \nParameters (M) \n43.09 \n15.59 \n16.07 \n6.57 \n11.90 \n16.60 \n16.60 \n11.93 \n\n\n\nTable 5 :\n5Model complexity comparisons (\u00d74). Output size is 3\u00d7512\u00d7512 to calculate FLOPs.\nAll the code and trained models are available at https://github.com/zhengchen1999/CAT.\nAcknowledgments. This work is partly supported by NSFC (62141220, 61972253, U1908212, 72061127001), the Program for Professor of Special Appointment (Eastern Scholar) at Shanghai Institutions of Higher Learning. Xin Yuan acknowledges the support of NSFC (62271414), Westlake Foundation (2021B1501-2) and the Research Center for Industries of the Future (RCIF) at Westlake University.\nA high-quality denoising dataset for smartphone cameras. Abdelrahman Abdelhamed, Stephen Lin, Michael S Brown, CVPR. Abdelrahman Abdelhamed, Stephen Lin, and Michael S Brown. A high-quality denoising dataset for smartphone cameras. In CVPR, 2018. 6\n\nContour detection and hierarchical image segmentation. TPAMI. Pablo Arbelaez, Michael Maire, Charless Fowlkes, Jitendra Malik, Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Jitendra Malik. Contour detection and hierarchical image segmentation. TPAMI, 2010. 6\n\nChristine Guillemot, and Marie Line Alberi-Morel. Low-complexity single-image super-resolution based on nonnegative neighbor embedding. Marco Bevilacqua, Aline Roumy, BMVC. Marco Bevilacqua, Aline Roumy, Christine Guillemot, and Marie Line Alberi-Morel. Low-complexity single-image super-resolution based on nonnegative neighbor embedding. In BMVC, 2012. 6\n\nTwo deterministic halfquadratic regularization algorithms for computed imaging. Pierre Charbonnier, Laure Blanc-Feraud, Gilles Aubert, Michel Barlaud, ICIP. Pierre Charbonnier, Laure Blanc-Feraud, Gilles Aubert, and Michel Barlaud. Two deterministic half- quadratic regularization algorithms for computed imaging. In ICIP, 1994. 6\n\nPre-trained image processing transformer. Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, Wen Gao, CVPR. 7Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and Wen Gao. Pre-trained image processing transformer. In CVPR, 2021. 1, 3, 4, 7, 8\n\nAttention in attention network for image super-resolution. Haoyu Chen, Jinjin Gu, Zhi Zhang, arXiv:2104.09497arXiv preprintHaoyu Chen, Jinjin Gu, and Zhi Zhang. Attention in attention network for image super-resolution. arXiv preprint arXiv:2104.09497, 2021. 2\n\nTwins: Revisiting the design of spatial attention in vision transformers. Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, Chunhua Shen, NeurIPS. Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. In NeurIPS, 2021. 3\n\nConditional positional encodings for vision transformers. Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, Chunhua Shen, arXiv:2102.10882arXiv preprintXiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Conditional positional encodings for vision transformers. arXiv preprint arXiv:2102.10882, 2021. 5\n\nSecond-order attention network for single image super-resolution. Tao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, Lei Zhang, CVPR. 7Tao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, and Lei Zhang. Second-order attention network for single image super-resolution. In CVPR, 2019. 7, 8\n\nCompression artifacts reduction by a deep convolutional network. Chao Dong, Yubin Deng, Chen Change Loy, Xiaoou Tang, ICCV. 1Chao Dong, Yubin Deng, Chen Change Loy, and Xiaoou Tang. Compression artifacts reduction by a deep convolutional network. In ICCV, 2015. 1, 2\n\nLearning a deep convolutional network for image super-resolution. Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang, ECCV. 1Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Learning a deep convolutional network for image super-resolution. In ECCV, 2014. 1, 2\n\nCswin transformer: A general vision transformer backbone with cross-shaped windows. Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, Baining Guo, CVPR, 2022. 1. 25Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming Zhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo. Cswin transformer: A general vision transformer backbone with cross-shaped windows. In CVPR, 2022. 1, 2, 5\n\nAn image is worth 16x16 words: Transformers for image recognition at scale. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, ICLR, 2021. 1. 23Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. 1, 2, 3\n\nPointwise shape-adaptive dct for high-quality denoising and deblocking of grayscale and color images. TIP. Alessandro Foi, Vladimir Katkovnik, Karen Egiazarian, Alessandro Foi, Vladimir Katkovnik, and Karen Egiazarian. Pointwise shape-adaptive dct for high-quality denoising and deblocking of grayscale and color images. TIP, 2007. 6\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 2\n\nDensely connected convolutional networks. Gao Huang, Zhuang Liu, Q Kilian, Laurens Weinberger, Van Der Maaten, CVPR. Gao Huang, Zhuang Liu, Kilian Q Weinberger, and Laurens van der Maaten. Densely connected convolu- tional networks. In CVPR, 2017. 2\n\nSingle image super-resolution from transformed self-exemplars. Jia-Bin Huang, Abhishek Singh, Narendra Ahuja, CVPR. 47Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Single image super-resolution from transformed self-exemplars. In CVPR, 2015. 4, 6, 7\n\nAdam: A method for stochastic optimization. Diederik Kingma, Jimmy Ba, ICLR. Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 6\n\nReflash dropout in image super-resolution. Xiangtao Kong, Xina Liu, Jinjin Gu, Yu Qiao, Chao Dong, CVPR. 2022Xiangtao Kong, Xina Liu, Jinjin Gu, Yu Qiao, and Chao Dong. Reflash dropout in image super-resolution. In CVPR, 2022. 2\n\nBlueprint separable residual network for efficient image super-resolution. Zheyuan Li, Yingqi Liu, Xiangyu Chen, Haoming Cai, Jinjin Gu, Yu Qiao, Chao Dong, CVPR. 2022Zheyuan Li, Yingqi Liu, Xiangyu Chen, Haoming Cai, Jinjin Gu, Yu Qiao, and Chao Dong. Blueprint separable residual network for efficient image super-resolution. In CVPR, 2022. 2\n\nSwinir: Image restoration using swin transformer. Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, Radu Timofte, ICCVW. 10Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir: Image restoration using swin transformer. In ICCVW, 2021. 1, 3, 4, 6, 7, 8, 9, 10\n\nEnhanced deep residual networks for single image super-resolution. Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee, CVPRW. 10Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep residual networks for single image super-resolution. In CVPRW, 2017. 6, 7, 8, 9, 10\n\nSwin transformer: Hierarchical vision transformer using shifted windows. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo, ICCV. 7Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021. 2, 5, 7\n\nSgdr: Stochastic gradient descent with warm restarts. Ilya Loshchilov, Frank Hutter, ICLR. Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. In ICLR, 2017. 6\n\nDecoupled weight decay regularization. Ilya Loshchilov, Frank Hutter, ICLR. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. 6\n\nWaterloo exploration database: New challenges for image quality assessment models. Kede Ma, Zhengfang Duanmu, Qingbo Wu, Zhou Wang, Hongwei Yong, Hongliang Li, Lei Zhang, TIP. 6Kede Ma, Zhengfang Duanmu, Qingbo Wu, Zhou Wang, Hongwei Yong, Hongliang Li, and Lei Zhang. Waterloo exploration database: New challenges for image quality assessment models. TIP, 2016. 6\n\nA database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. David Martin, Charless Fowlkes, Doron Tal, Jitendra Malik, ICCV. David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In ICCV, 2001. 6\n\nSketch-based manga retrieval using manga109 dataset. Multimedia Tools and Applications. Yusuke Matsui, Kota Ito, Yuji Aramaki, Azuma Fujimoto, Toru Ogawa, Toshihiko Yamasaki, Kiyoharu Aizawa, Yusuke Matsui, Kota Ito, Yuji Aramaki, Azuma Fujimoto, Toru Ogawa, Toshihiko Yamasaki, and Kiyoharu Aizawa. Sketch-based manga retrieval using manga109 dataset. Multimedia Tools and Applications, 2017. 6\n\nImage super-resolution with non-local sparse attention. Yiqun Mei, Yuchen Fan, Yuqian Zhou, CVPR, 2021. 7Yiqun Mei, Yuchen Fan, and Yuqian Zhou. Image super-resolution with non-local sparse attention. In CVPR, 2021. 7, 8\n\nImage super-resolution with cross-scale non-local attention and exhaustive self-exemplars mining. Yiqun Mei, Yuchen Fan, Yuqian Zhou, Lichao Huang, S Thomas, Humphrey Huang, Shi, CVPR, 2020. 7. 810Yiqun Mei, Yuchen Fan, Yuqian Zhou, Lichao Huang, Thomas S Huang, and Humphrey Shi. Image super-resolution with cross-scale non-local attention and exhaustive self-exemplars mining. In CVPR, 2020. 7, 8, 10\n\nSingle image super-resolution via a holistic attention network. Ben Niu, Weilei Wen, Wenqi Ren, Xiangde Zhang, Lianping Yang, Shuzhen Wang, Kaihao Zhang, Xiaochun Cao, Haifeng Shen, ECCV. 810Ben Niu, Weilei Wen, Wenqi Ren, Xiangde Zhang, Lianping Yang, Shuzhen Wang, Kaihao Zhang, Xiaochun Cao, and Haifeng Shen. Single image super-resolution via a holistic attention network. In ECCV, 2020. 2, 7, 8, 10\n\nAutomatic differentiation in pytorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. 2017. 6\n\nBenchmarking denoising algorithms with real photographs. Tobias Plotz, Stefan Roth, CVPR. Tobias Plotz and Stefan Roth. Benchmarking denoising algorithms with real photographs. In CVPR, 2017. 6\n\nU-net: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, MICCAI. 23Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, 2015. 2, 3\n\nA statistical evaluation of recent full reference image quality assessment algorithms. R Hamid, Sheikh, F Muhammad, Alan C Sabir, Bovik, TIP. 6Hamid R Sheikh, Muhammad F Sabir, and Alan C Bovik. A statistical evaluation of recent full reference image quality assessment algorithms. TIP, 2006. 6\n\nReal-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, P Andrew, Rob Aitken, Daniel Bishop, Zehan Rueckert, Wang, CVPR. Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In CVPR, 2016. 3\n\nNtire 2017 challenge on single image super-resolution: Methods and results. Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming-Hsuan Yang, Lei Zhang, Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee, 67Radu Timofte, Eirikur Agustsson, Luc Van Gool, Ming-Hsuan Yang, Lei Zhang, Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, Kyoung Mu Lee, et al. Ntire 2017 challenge on single image super-resolution: Methods and results. In CVPRW, 2017. 6, 7\n\nCrossformer: A versatile vision transformer hinging on cross-scale attention. Wenxiao Wang, Lu Yao, Long Chen, Binbin Lin, Deng Cai, Xiaofei He, Wei Liu, ICLR, 2022. 4Wenxiao Wang, Lu Yao, Long Chen, Binbin Lin, Deng Cai, Xiaofei He, and Wei Liu. Crossformer: A versatile vision transformer hinging on cross-scale attention. In ICLR, 2022. 4\n\nUformer: A general u-shaped transformer for image restoration. Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, Houqiang Li, CVPR. 39Zhendong Wang, Xiaodong Cun, Jianmin Bao, Wengang Zhou, Jianzhuang Liu, and Houqiang Li. Uformer: A general u-shaped transformer for image restoration. In CVPR, 2022. 3, 9\n\nImage quality assessment: from error visibility to structural similarity. Zhou Wang, Alan C Bovik, R Hamid, Eero P Sheikh, Simoncelli, TIP. 6Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. TIP, 2004. 6\n\nFocal self-attention for local-global interactions in vision transformers. Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, Jianfeng Gao, NeurIPS. 1Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal self-attention for local-global interactions in vision transformers. In NeurIPS, 2021. 1, 2\n\nIncorporating convolution designs into visual transformers. Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, Wei Wu, CVPR. Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu. Incorporating convolution designs into visual transformers. In CVPR, 2021. 5\n\nDual adversarial network: Toward real-world noise removal and noise generation. Zongsheng Yue, Qian Zhao, Lei Zhang, Deyu Meng, ECCV. 2020Zongsheng Yue, Qian Zhao, Lei Zhang, and Deyu Meng. Dual adversarial network: Toward real-world noise removal and noise generation. In ECCV, 2020. 9\n\nRestormer: Efficient transformer for high-resolution image restoration. Aditya Syed Waqas Zamir, Salman Arora, Munawar Khan, Hayat, Ming-Hsuan Fahad Shahbaz Khan, Yang, CVPR. 69Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In CVPR, 2022. 1, 3, 4, 6, 9\n\nCycleisp: Real image restoration via improved data synthesis. Aditya Syed Waqas Zamir, Salman Arora, Munawar Khan, Hayat, Ming-Hsuan Fahad Shahbaz Khan, Ling Yang, Shao, CVPR. 2020Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling Shao. Cycleisp: Real image restoration via improved data synthesis. In CVPR, 2020. 9\n\nLearning enriched features for real image restoration and enhancement. Aditya Syed Waqas Zamir, Salman Arora, Munawar Khan, Hayat, Ming-Hsuan Fahad Shahbaz Khan, Ling Yang, Shao, ECCV. 2020Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling Shao. Learning enriched features for real image restoration and enhancement. In ECCV, 2020. 9\n\nMulti-stage progressive image restoration. Aditya Syed Waqas Zamir, Salman Arora, Munawar Khan, Hayat, Ming-Hsuan Fahad Shahbaz Khan, Ling Yang, Shao, CVPR. Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling Shao. Multi-stage progressive image restoration. In CVPR, 2021. 9\n\nOn single image scale-up using sparse-representations. Roman Zeyde, Michael Elad, Matan Protter, Proc. 7th Int. Conf. Curves Surf. 7th Int. Conf. Curves Surf6Roman Zeyde, Michael Elad, and Matan Protter. On single image scale-up using sparse-representations. In Proc. 7th Int. Conf. Curves Surf., 2010. 6\n\nPlug-and-play image restoration with deep denoiser prior. TPAMI. Kai Zhang, Yawei Li, Wangmeng Zuo, Lei Zhang, Luc Van Gool, Radu Timofte, 89Kai Zhang, Yawei Li, Wangmeng Zuo, Lei Zhang, Luc Van Gool, and Radu Timofte. Plug-and-play image restoration with deep denoiser prior. TPAMI, 2021. 8, 9\n\nBeyond a gaussian denoiser: Residual learning of deep cnn for image denoising. Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, Lei Zhang, TIP. 1Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising. TIP, 2017. 1\n\nImage super-resolution using very deep residual channel attention networks. Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, Yun Fu, ECCV. 10Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu. Image super-resolution using very deep residual channel attention networks. In ECCV, 2018. 2, 3, 6, 7, 8, 10\n\nResidual non-local attention networks for image restoration. Yulun Zhang, Kunpeng Li, Kai Li, Bineng Zhong, Yun Fu, ICLR. 89Yulun Zhang, Kunpeng Li, Kai Li, Bineng Zhong, and Yun Fu. Residual non-local attention networks for image restoration. In ICLR, 2019. 2, 6, 8, 9\n\nResidual dense network for image restoration. Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, Yun Fu, TPAMI. 89Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. Residual dense network for image restoration. TPAMI, 2020. 8, 9\n\nCross-scale internal graph neural network for image super-resolution. Shangchen Zhou, Jiawei Zhang, Wangmeng Zuo, Chen Change Loy, NeurIPS, 2020. 7Shangchen Zhou, Jiawei Zhang, Wangmeng Zuo, and Chen Change Loy. Cross-scale internal graph neural network for image super-resolution. In NeurIPS, 2020. 7, 8\n", "annotations": {"author": "[{\"end\":66,\"start\":55},{\"end\":79,\"start\":67},{\"end\":117,\"start\":80},{\"end\":177,\"start\":118},{\"end\":190,\"start\":178},{\"end\":222,\"start\":191},{\"end\":237,\"start\":223},{\"end\":254,\"start\":238},{\"end\":266,\"start\":255},{\"end\":290,\"start\":267},{\"end\":66,\"start\":55},{\"end\":79,\"start\":67},{\"end\":117,\"start\":80},{\"end\":177,\"start\":118},{\"end\":190,\"start\":178},{\"end\":222,\"start\":191},{\"end\":237,\"start\":223},{\"end\":254,\"start\":238},{\"end\":266,\"start\":255},{\"end\":290,\"start\":267}]", "publisher": null, "author_last_name": "[{\"end\":65,\"start\":61},{\"end\":78,\"start\":73},{\"end\":89,\"start\":87},{\"end\":132,\"start\":127},{\"end\":189,\"start\":185},{\"end\":199,\"start\":195},{\"end\":236,\"start\":232},{\"end\":253,\"start\":243},{\"end\":265,\"start\":259},{\"end\":289,\"start\":279},{\"end\":65,\"start\":61},{\"end\":78,\"start\":73},{\"end\":89,\"start\":87},{\"end\":132,\"start\":127},{\"end\":189,\"start\":185},{\"end\":199,\"start\":195},{\"end\":236,\"start\":232},{\"end\":253,\"start\":243},{\"end\":265,\"start\":259},{\"end\":289,\"start\":279}]", "author_first_name": "[{\"end\":60,\"start\":55},{\"end\":72,\"start\":67},{\"end\":86,\"start\":80},{\"end\":126,\"start\":118},{\"end\":184,\"start\":178},{\"end\":194,\"start\":191},{\"end\":231,\"start\":223},{\"end\":242,\"start\":238},{\"end\":258,\"start\":255},{\"end\":275,\"start\":267},{\"end\":278,\"start\":276},{\"end\":60,\"start\":55},{\"end\":72,\"start\":67},{\"end\":86,\"start\":80},{\"end\":126,\"start\":118},{\"end\":184,\"start\":178},{\"end\":194,\"start\":191},{\"end\":231,\"start\":223},{\"end\":242,\"start\":238},{\"end\":258,\"start\":255},{\"end\":275,\"start\":267},{\"end\":278,\"start\":276}]", "author_affiliation": "[{\"end\":116,\"start\":91},{\"end\":176,\"start\":134},{\"end\":221,\"start\":201},{\"end\":116,\"start\":91},{\"end\":176,\"start\":134},{\"end\":221,\"start\":201}]", "title": "[{\"end\":52,\"start\":1},{\"end\":342,\"start\":291},{\"end\":52,\"start\":1},{\"end\":342,\"start\":291}]", "venue": null, "abstract": "[{\"end\":1666,\"start\":344},{\"end\":1666,\"start\":344}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2294,\"start\":2290},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2297,\"start\":2294},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":2300,\"start\":2297},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2577,\"start\":2573},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2605,\"start\":2601},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2632,\"start\":2628},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3120,\"start\":3117},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3123,\"start\":3120},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3126,\"start\":3123},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4618,\"start\":4614},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5323,\"start\":5319},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7452,\"start\":7448},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7467,\"start\":7463},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7693,\"start\":7689},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7716,\"start\":7712},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7730,\"start\":7726},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7758,\"start\":7754},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":7816,\"start\":7812},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7819,\"start\":7816},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":7822,\"start\":7819},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7825,\"start\":7822},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7827,\"start\":7825},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8104,\"start\":8100},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8107,\"start\":8104},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8231,\"start\":8227},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8341,\"start\":8337},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8395,\"start\":8392},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8598,\"start\":8595},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8636,\"start\":8632},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8805,\"start\":8801},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8979,\"start\":8975},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9099,\"start\":9095},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9747,\"start\":9743},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":9826,\"start\":9822},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":10280,\"start\":10276},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":10792,\"start\":10788},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":11112,\"start\":11108},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11577,\"start\":11573},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11739,\"start\":11735},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12309,\"start\":12305},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12367,\"start\":12363},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12614,\"start\":12610},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12617,\"start\":12614},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12904,\"start\":12901},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12907,\"start\":12904},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12910,\"start\":12907},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":14172,\"start\":14168},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15434,\"start\":15430},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15554,\"start\":15550},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17405,\"start\":17401},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":18070,\"start\":18066},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18072,\"start\":18070},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":19676,\"start\":19672},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":19694,\"start\":19690},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19771,\"start\":19768},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":19783,\"start\":19779},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":19794,\"start\":19790},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19809,\"start\":19805},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19828,\"start\":19824},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":20016,\"start\":20012},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":20031,\"start\":20027},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20043,\"start\":20040},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20057,\"start\":20053},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20106,\"start\":20102},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":20121,\"start\":20117},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20228,\"start\":20225},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20295,\"start\":20292},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20308,\"start\":20304},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":20347,\"start\":20343},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":20352,\"start\":20348},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":20423,\"start\":20419},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":20578,\"start\":20574},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":20760,\"start\":20756},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":20818,\"start\":20814},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":20827,\"start\":20823},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":20838,\"start\":20834},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":20850,\"start\":20846},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":21008,\"start\":21004},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":21026,\"start\":21022},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21174,\"start\":21170},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21390,\"start\":21386},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":21883,\"start\":21879},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":23414,\"start\":23410},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":23425,\"start\":23421},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":23434,\"start\":23431},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":23445,\"start\":23441},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":23455,\"start\":23451},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":23467,\"start\":23463},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":23478,\"start\":23474},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":23487,\"start\":23484},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23504,\"start\":23500},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":23538,\"start\":23534},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23541,\"start\":23538},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":23544,\"start\":23541},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":25588,\"start\":25584},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":25598,\"start\":25594},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":25611,\"start\":25607},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25628,\"start\":25624},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":26523,\"start\":26519},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":26533,\"start\":26529},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":26550,\"start\":26546},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":27022,\"start\":27018},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":27037,\"start\":27033},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":27050,\"start\":27046},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":27063,\"start\":27059},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":27077,\"start\":27073},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":27097,\"start\":27093},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":27124,\"start\":27120},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":27453,\"start\":27449},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":27985,\"start\":27981},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":28060,\"start\":28056},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31706,\"start\":31703},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":31709,\"start\":31706},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":31717,\"start\":31713},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31719,\"start\":31717},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":31777,\"start\":31774},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":31779,\"start\":31777},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":31781,\"start\":31779},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31783,\"start\":31781},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31785,\"start\":31783},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31787,\"start\":31785},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2294,\"start\":2290},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2297,\"start\":2294},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":2300,\"start\":2297},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2577,\"start\":2573},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2605,\"start\":2601},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2632,\"start\":2628},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3120,\"start\":3117},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3123,\"start\":3120},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":3126,\"start\":3123},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4618,\"start\":4614},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5323,\"start\":5319},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7452,\"start\":7448},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":7467,\"start\":7463},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7693,\"start\":7689},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":7716,\"start\":7712},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7730,\"start\":7726},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7758,\"start\":7754},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":7816,\"start\":7812},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7819,\"start\":7816},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":7822,\"start\":7819},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7825,\"start\":7822},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":7827,\"start\":7825},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8104,\"start\":8100},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8107,\"start\":8104},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8231,\"start\":8227},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":8341,\"start\":8337},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":8395,\"start\":8392},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8598,\"start\":8595},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":8636,\"start\":8632},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8805,\"start\":8801},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8979,\"start\":8975},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":9099,\"start\":9095},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9747,\"start\":9743},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":9826,\"start\":9822},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":10280,\"start\":10276},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":10792,\"start\":10788},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":11112,\"start\":11108},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11577,\"start\":11573},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":11739,\"start\":11735},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12309,\"start\":12305},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":12367,\"start\":12363},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12614,\"start\":12610},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12617,\"start\":12614},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":12904,\"start\":12901},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12907,\"start\":12904},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":12910,\"start\":12907},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":14172,\"start\":14168},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":15434,\"start\":15430},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":15554,\"start\":15550},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":17405,\"start\":17401},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":18070,\"start\":18066},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":18072,\"start\":18070},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":19676,\"start\":19672},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":19694,\"start\":19690},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":19771,\"start\":19768},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":19783,\"start\":19779},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":19794,\"start\":19790},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19809,\"start\":19805},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19828,\"start\":19824},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":20016,\"start\":20012},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":20031,\"start\":20027},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20043,\"start\":20040},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":20057,\"start\":20053},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":20106,\"start\":20102},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":20121,\"start\":20117},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20228,\"start\":20225},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20295,\"start\":20292},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":20308,\"start\":20304},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":20347,\"start\":20343},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":20352,\"start\":20348},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":20423,\"start\":20419},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":20578,\"start\":20574},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":20760,\"start\":20756},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":20818,\"start\":20814},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":20827,\"start\":20823},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":20838,\"start\":20834},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":20850,\"start\":20846},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":21008,\"start\":21004},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":21026,\"start\":21022},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21174,\"start\":21170},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21390,\"start\":21386},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":21883,\"start\":21879},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":23414,\"start\":23410},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":23425,\"start\":23421},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":23434,\"start\":23431},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":23445,\"start\":23441},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":23455,\"start\":23451},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":23467,\"start\":23463},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":23478,\"start\":23474},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":23487,\"start\":23484},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23504,\"start\":23500},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":23538,\"start\":23534},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":23541,\"start\":23538},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":23544,\"start\":23541},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":25588,\"start\":25584},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":25598,\"start\":25594},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":25611,\"start\":25607},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25628,\"start\":25624},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":26523,\"start\":26519},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":26533,\"start\":26529},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":26550,\"start\":26546},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":27022,\"start\":27018},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":27037,\"start\":27033},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":27050,\"start\":27046},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":27063,\"start\":27059},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":27077,\"start\":27073},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":27097,\"start\":27093},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":27124,\"start\":27120},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":27453,\"start\":27449},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":27985,\"start\":27981},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":28060,\"start\":28056},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31706,\"start\":31703},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":31709,\"start\":31706},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":31717,\"start\":31713},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31719,\"start\":31717},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":31777,\"start\":31774},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":31779,\"start\":31777},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":31781,\"start\":31779},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31783,\"start\":31781},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31785,\"start\":31783},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":31787,\"start\":31785}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29619,\"start\":29548},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29895,\"start\":29620},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30056,\"start\":29896},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30776,\"start\":30057},{\"attributes\":{\"id\":\"fig_4\"},\"end\":31135,\"start\":30777},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":31143,\"start\":31136},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":33025,\"start\":31144},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":34004,\"start\":33026},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":34714,\"start\":34005},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":34908,\"start\":34715},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":35219,\"start\":34909},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":35311,\"start\":35220},{\"attributes\":{\"id\":\"fig_0\"},\"end\":29619,\"start\":29548},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29895,\"start\":29620},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30056,\"start\":29896},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30776,\"start\":30057},{\"attributes\":{\"id\":\"fig_4\"},\"end\":31135,\"start\":30777},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":31143,\"start\":31136},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":33025,\"start\":31144},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":34004,\"start\":33026},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":34714,\"start\":34005},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":34908,\"start\":34715},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":35219,\"start\":34909},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":35311,\"start\":35220}]", "paragraph": "[{\"end\":2364,\"start\":1682},{\"end\":3633,\"start\":2366},{\"end\":4948,\"start\":3635},{\"end\":5516,\"start\":4950},{\"end\":6085,\"start\":5518},{\"end\":6481,\"start\":6087},{\"end\":7230,\"start\":6483},{\"end\":7998,\"start\":7247},{\"end\":8965,\"start\":8000},{\"end\":9544,\"start\":8967},{\"end\":10220,\"start\":9555},{\"end\":11412,\"start\":10241},{\"end\":12386,\"start\":11414},{\"end\":12819,\"start\":12426},{\"end\":13370,\"start\":12821},{\"end\":13736,\"start\":13372},{\"end\":14475,\"start\":13891},{\"end\":14765,\"start\":14477},{\"end\":16032,\"start\":14820},{\"end\":16509,\"start\":16126},{\"end\":17294,\"start\":16511},{\"end\":18147,\"start\":17296},{\"end\":18481,\"start\":18215},{\"end\":19287,\"start\":18483},{\"end\":19581,\"start\":19317},{\"end\":20936,\"start\":19621},{\"end\":21227,\"start\":20955},{\"end\":21884,\"start\":21229},{\"end\":22529,\"start\":21886},{\"end\":23305,\"start\":22531},{\"end\":23859,\"start\":23332},{\"end\":24704,\"start\":23861},{\"end\":25454,\"start\":24706},{\"end\":25855,\"start\":25494},{\"end\":26403,\"start\":25857},{\"end\":26884,\"start\":26405},{\"end\":27171,\"start\":26909},{\"end\":28478,\"start\":27173},{\"end\":29547,\"start\":28515},{\"end\":2364,\"start\":1682},{\"end\":3633,\"start\":2366},{\"end\":4948,\"start\":3635},{\"end\":5516,\"start\":4950},{\"end\":6085,\"start\":5518},{\"end\":6481,\"start\":6087},{\"end\":7230,\"start\":6483},{\"end\":7998,\"start\":7247},{\"end\":8965,\"start\":8000},{\"end\":9544,\"start\":8967},{\"end\":10220,\"start\":9555},{\"end\":11412,\"start\":10241},{\"end\":12386,\"start\":11414},{\"end\":12819,\"start\":12426},{\"end\":13370,\"start\":12821},{\"end\":13736,\"start\":13372},{\"end\":14475,\"start\":13891},{\"end\":14765,\"start\":14477},{\"end\":16032,\"start\":14820},{\"end\":16509,\"start\":16126},{\"end\":17294,\"start\":16511},{\"end\":18147,\"start\":17296},{\"end\":18481,\"start\":18215},{\"end\":19287,\"start\":18483},{\"end\":19581,\"start\":19317},{\"end\":20936,\"start\":19621},{\"end\":21227,\"start\":20955},{\"end\":21884,\"start\":21229},{\"end\":22529,\"start\":21886},{\"end\":23305,\"start\":22531},{\"end\":23859,\"start\":23332},{\"end\":24704,\"start\":23861},{\"end\":25454,\"start\":24706},{\"end\":25855,\"start\":25494},{\"end\":26403,\"start\":25857},{\"end\":26884,\"start\":26405},{\"end\":27171,\"start\":26909},{\"end\":28478,\"start\":27173},{\"end\":29547,\"start\":28515}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13890,\"start\":13737},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14819,\"start\":14766},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16125,\"start\":16033},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18214,\"start\":18148},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19316,\"start\":19288},{\"attributes\":{\"id\":\"formula_0\"},\"end\":13890,\"start\":13737},{\"attributes\":{\"id\":\"formula_1\"},\"end\":14819,\"start\":14766},{\"attributes\":{\"id\":\"formula_2\"},\"end\":16125,\"start\":16033},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18214,\"start\":18148},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19316,\"start\":19288}]", "table_ref": "[{\"end\":21270,\"start\":21262},{\"end\":21925,\"start\":21917},{\"end\":22774,\"start\":22766},{\"end\":23691,\"start\":23684},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":23890,\"start\":23883},{\"end\":25829,\"start\":25822},{\"end\":25886,\"start\":25879},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":27202,\"start\":27195},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":27656,\"start\":27649},{\"end\":21270,\"start\":21262},{\"end\":21925,\"start\":21917},{\"end\":22774,\"start\":22766},{\"end\":23691,\"start\":23684},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":23890,\"start\":23883},{\"end\":25829,\"start\":25822},{\"end\":25886,\"start\":25879},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":27202,\"start\":27195},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":27656,\"start\":27649}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1680,\"start\":1668},{\"attributes\":{\"n\":\"2\"},\"end\":7245,\"start\":7233},{\"attributes\":{\"n\":\"3\"},\"end\":9553,\"start\":9547},{\"attributes\":{\"n\":\"3.1\"},\"end\":10239,\"start\":10223},{\"attributes\":{\"n\":\"3.2\"},\"end\":12424,\"start\":12389},{\"attributes\":{\"n\":\"4\"},\"end\":19595,\"start\":19584},{\"attributes\":{\"n\":\"4.1\"},\"end\":19619,\"start\":19598},{\"attributes\":{\"n\":\"4.2\"},\"end\":20953,\"start\":20939},{\"attributes\":{\"n\":\"4.3\"},\"end\":23330,\"start\":23308},{\"attributes\":{\"n\":\"4.4\"},\"end\":25492,\"start\":25457},{\"attributes\":{\"n\":\"4.5\"},\"end\":26907,\"start\":26887},{\"attributes\":{\"n\":\"4.6\"},\"end\":28500,\"start\":28481},{\"attributes\":{\"n\":\"5\"},\"end\":28513,\"start\":28503},{\"end\":29559,\"start\":29549},{\"end\":29631,\"start\":29621},{\"end\":29907,\"start\":29897},{\"end\":30068,\"start\":30058},{\"end\":30781,\"start\":30778},{\"end\":31142,\"start\":31137},{\"end\":33036,\"start\":33027},{\"end\":34725,\"start\":34716},{\"end\":35230,\"start\":35221},{\"attributes\":{\"n\":\"1\"},\"end\":1680,\"start\":1668},{\"attributes\":{\"n\":\"2\"},\"end\":7245,\"start\":7233},{\"attributes\":{\"n\":\"3\"},\"end\":9553,\"start\":9547},{\"attributes\":{\"n\":\"3.1\"},\"end\":10239,\"start\":10223},{\"attributes\":{\"n\":\"3.2\"},\"end\":12424,\"start\":12389},{\"attributes\":{\"n\":\"4\"},\"end\":19595,\"start\":19584},{\"attributes\":{\"n\":\"4.1\"},\"end\":19619,\"start\":19598},{\"attributes\":{\"n\":\"4.2\"},\"end\":20953,\"start\":20939},{\"attributes\":{\"n\":\"4.3\"},\"end\":23330,\"start\":23308},{\"attributes\":{\"n\":\"4.4\"},\"end\":25492,\"start\":25457},{\"attributes\":{\"n\":\"4.5\"},\"end\":26907,\"start\":26887},{\"attributes\":{\"n\":\"4.6\"},\"end\":28500,\"start\":28481},{\"attributes\":{\"n\":\"5\"},\"end\":28513,\"start\":28503},{\"end\":29559,\"start\":29549},{\"end\":29631,\"start\":29621},{\"end\":29907,\"start\":29897},{\"end\":30068,\"start\":30058},{\"end\":30781,\"start\":30778},{\"end\":31142,\"start\":31137},{\"end\":33036,\"start\":33027},{\"end\":34725,\"start\":34716},{\"end\":35230,\"start\":35221}]", "table": "[{\"end\":33025,\"start\":31976},{\"end\":34004,\"start\":33038},{\"end\":34714,\"start\":34085},{\"end\":35219,\"start\":35010},{\"end\":33025,\"start\":31976},{\"end\":34004,\"start\":33038},{\"end\":34714,\"start\":34085},{\"end\":35219,\"start\":35010}]", "figure_caption": "[{\"end\":29619,\"start\":29561},{\"end\":29895,\"start\":29633},{\"end\":30056,\"start\":29909},{\"end\":30776,\"start\":30070},{\"end\":31135,\"start\":30783},{\"end\":31976,\"start\":31146},{\"end\":34085,\"start\":34007},{\"end\":34908,\"start\":34727},{\"end\":35010,\"start\":34911},{\"end\":35311,\"start\":35232},{\"end\":29619,\"start\":29561},{\"end\":29895,\"start\":29633},{\"end\":30056,\"start\":29909},{\"end\":30776,\"start\":30070},{\"end\":31135,\"start\":30783},{\"end\":31976,\"start\":31146},{\"end\":34085,\"start\":34007},{\"end\":34908,\"start\":34727},{\"end\":35010,\"start\":34911},{\"end\":35311,\"start\":35232}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10016,\"start\":10010},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10259,\"start\":10253},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12516,\"start\":12510},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12974,\"start\":12968},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15021,\"start\":15015},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15530,\"start\":15524},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16718,\"start\":16712},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18012,\"start\":18006},{\"end\":20870,\"start\":20862},{\"end\":24763,\"start\":24757},{\"end\":25840,\"start\":25834},{\"end\":26490,\"start\":26484},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10016,\"start\":10010},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":10259,\"start\":10253},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":12516,\"start\":12510},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12974,\"start\":12968},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15021,\"start\":15015},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15530,\"start\":15524},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":16718,\"start\":16712},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":18012,\"start\":18006},{\"end\":20870,\"start\":20862},{\"end\":24763,\"start\":24757},{\"end\":25840,\"start\":25834},{\"end\":26490,\"start\":26484}]", "bib_author_first_name": "[{\"end\":35851,\"start\":35840},{\"end\":35871,\"start\":35864},{\"end\":35886,\"start\":35877},{\"end\":36100,\"start\":36095},{\"end\":36118,\"start\":36111},{\"end\":36134,\"start\":36126},{\"end\":36152,\"start\":36144},{\"end\":36441,\"start\":36436},{\"end\":36459,\"start\":36454},{\"end\":36744,\"start\":36738},{\"end\":36763,\"start\":36758},{\"end\":36784,\"start\":36778},{\"end\":36799,\"start\":36793},{\"end\":37039,\"start\":37032},{\"end\":37051,\"start\":37046},{\"end\":37064,\"start\":37058},{\"end\":37075,\"start\":37070},{\"end\":37086,\"start\":37080},{\"end\":37100,\"start\":37093},{\"end\":37111,\"start\":37106},{\"end\":37124,\"start\":37116},{\"end\":37133,\"start\":37129},{\"end\":37141,\"start\":37138},{\"end\":37409,\"start\":37404},{\"end\":37422,\"start\":37416},{\"end\":37430,\"start\":37427},{\"end\":37691,\"start\":37681},{\"end\":37700,\"start\":37697},{\"end\":37713,\"start\":37707},{\"end\":37722,\"start\":37720},{\"end\":37737,\"start\":37730},{\"end\":37750,\"start\":37743},{\"end\":37762,\"start\":37756},{\"end\":37775,\"start\":37768},{\"end\":38059,\"start\":38049},{\"end\":38068,\"start\":38065},{\"end\":38077,\"start\":38075},{\"end\":38092,\"start\":38085},{\"end\":38106,\"start\":38099},{\"end\":38118,\"start\":38112},{\"end\":38131,\"start\":38124},{\"end\":38430,\"start\":38427},{\"end\":38443,\"start\":38436},{\"end\":38457,\"start\":38449},{\"end\":38472,\"start\":38465},{\"end\":38481,\"start\":38478},{\"end\":38718,\"start\":38714},{\"end\":38730,\"start\":38725},{\"end\":38741,\"start\":38737},{\"end\":38748,\"start\":38742},{\"end\":38760,\"start\":38754},{\"end\":38987,\"start\":38983},{\"end\":38998,\"start\":38994},{\"end\":39005,\"start\":38999},{\"end\":39018,\"start\":39011},{\"end\":39029,\"start\":39023},{\"end\":39277,\"start\":39271},{\"end\":39291,\"start\":39284},{\"end\":39305,\"start\":39297},{\"end\":39319,\"start\":39312},{\"end\":39334,\"start\":39327},{\"end\":39341,\"start\":39339},{\"end\":39352,\"start\":39348},{\"end\":39366,\"start\":39359},{\"end\":39684,\"start\":39678},{\"end\":39703,\"start\":39698},{\"end\":39720,\"start\":39711},{\"end\":39737,\"start\":39733},{\"end\":39758,\"start\":39751},{\"end\":39771,\"start\":39765},{\"end\":39792,\"start\":39785},{\"end\":39811,\"start\":39803},{\"end\":39827,\"start\":39822},{\"end\":39844,\"start\":39837},{\"end\":40267,\"start\":40257},{\"end\":40281,\"start\":40273},{\"end\":40298,\"start\":40293},{\"end\":40538,\"start\":40531},{\"end\":40550,\"start\":40543},{\"end\":40566,\"start\":40558},{\"end\":40576,\"start\":40572},{\"end\":40752,\"start\":40749},{\"end\":40766,\"start\":40760},{\"end\":40773,\"start\":40772},{\"end\":40789,\"start\":40782},{\"end\":41028,\"start\":41021},{\"end\":41044,\"start\":41036},{\"end\":41060,\"start\":41052},{\"end\":41266,\"start\":41258},{\"end\":41280,\"start\":41275},{\"end\":41434,\"start\":41426},{\"end\":41445,\"start\":41441},{\"end\":41457,\"start\":41451},{\"end\":41464,\"start\":41462},{\"end\":41475,\"start\":41471},{\"end\":41695,\"start\":41688},{\"end\":41706,\"start\":41700},{\"end\":41719,\"start\":41712},{\"end\":41733,\"start\":41726},{\"end\":41745,\"start\":41739},{\"end\":41752,\"start\":41750},{\"end\":41763,\"start\":41759},{\"end\":42016,\"start\":42009},{\"end\":42032,\"start\":42024},{\"end\":42044,\"start\":42038},{\"end\":42053,\"start\":42050},{\"end\":42064,\"start\":42061},{\"end\":42079,\"start\":42075},{\"end\":42343,\"start\":42340},{\"end\":42357,\"start\":42349},{\"end\":42369,\"start\":42363},{\"end\":42383,\"start\":42375},{\"end\":42398,\"start\":42389},{\"end\":42655,\"start\":42653},{\"end\":42667,\"start\":42661},{\"end\":42676,\"start\":42673},{\"end\":42685,\"start\":42682},{\"end\":42696,\"start\":42690},{\"end\":42707,\"start\":42702},{\"end\":42722,\"start\":42715},{\"end\":42735,\"start\":42728},{\"end\":42995,\"start\":42991},{\"end\":43013,\"start\":43008},{\"end\":43177,\"start\":43173},{\"end\":43195,\"start\":43190},{\"end\":43388,\"start\":43384},{\"end\":43402,\"start\":43393},{\"end\":43417,\"start\":43411},{\"end\":43426,\"start\":43422},{\"end\":43440,\"start\":43433},{\"end\":43456,\"start\":43447},{\"end\":43464,\"start\":43461},{\"end\":43812,\"start\":43807},{\"end\":43829,\"start\":43821},{\"end\":43844,\"start\":43839},{\"end\":43858,\"start\":43850},{\"end\":44187,\"start\":44181},{\"end\":44200,\"start\":44196},{\"end\":44210,\"start\":44206},{\"end\":44225,\"start\":44220},{\"end\":44240,\"start\":44236},{\"end\":44257,\"start\":44248},{\"end\":44276,\"start\":44268},{\"end\":44551,\"start\":44546},{\"end\":44563,\"start\":44557},{\"end\":44575,\"start\":44569},{\"end\":44815,\"start\":44810},{\"end\":44827,\"start\":44821},{\"end\":44839,\"start\":44833},{\"end\":44852,\"start\":44846},{\"end\":44861,\"start\":44860},{\"end\":44878,\"start\":44870},{\"end\":45183,\"start\":45180},{\"end\":45195,\"start\":45189},{\"end\":45206,\"start\":45201},{\"end\":45219,\"start\":45212},{\"end\":45235,\"start\":45227},{\"end\":45249,\"start\":45242},{\"end\":45262,\"start\":45256},{\"end\":45278,\"start\":45270},{\"end\":45291,\"start\":45284},{\"end\":45563,\"start\":45559},{\"end\":45575,\"start\":45572},{\"end\":45590,\"start\":45583},{\"end\":45608,\"start\":45601},{\"end\":45623,\"start\":45617},{\"end\":45637,\"start\":45630},{\"end\":45652,\"start\":45646},{\"end\":45663,\"start\":45658},{\"end\":45679,\"start\":45675},{\"end\":45692,\"start\":45688},{\"end\":45955,\"start\":45949},{\"end\":45969,\"start\":45963},{\"end\":46156,\"start\":46152},{\"end\":46177,\"start\":46170},{\"end\":46193,\"start\":46187},{\"end\":46438,\"start\":46437},{\"end\":46455,\"start\":46454},{\"end\":46470,\"start\":46466},{\"end\":46472,\"start\":46471},{\"end\":46761,\"start\":46755},{\"end\":46771,\"start\":46767},{\"end\":46789,\"start\":46783},{\"end\":46806,\"start\":46798},{\"end\":46814,\"start\":46813},{\"end\":46826,\"start\":46823},{\"end\":46841,\"start\":46835},{\"end\":46855,\"start\":46850},{\"end\":47205,\"start\":47201},{\"end\":47222,\"start\":47215},{\"end\":47237,\"start\":47234},{\"end\":47258,\"start\":47248},{\"end\":47268,\"start\":47265},{\"end\":47279,\"start\":47276},{\"end\":47293,\"start\":47285},{\"end\":47305,\"start\":47299},{\"end\":47319,\"start\":47311},{\"end\":47671,\"start\":47664},{\"end\":47680,\"start\":47678},{\"end\":47690,\"start\":47686},{\"end\":47703,\"start\":47697},{\"end\":47713,\"start\":47709},{\"end\":47726,\"start\":47719},{\"end\":47734,\"start\":47731},{\"end\":48000,\"start\":47992},{\"end\":48015,\"start\":48007},{\"end\":48028,\"start\":48021},{\"end\":48041,\"start\":48034},{\"end\":48058,\"start\":48048},{\"end\":48072,\"start\":48064},{\"end\":48336,\"start\":48332},{\"end\":48347,\"start\":48343},{\"end\":48349,\"start\":48348},{\"end\":48358,\"start\":48357},{\"end\":48372,\"start\":48366},{\"end\":48633,\"start\":48626},{\"end\":48648,\"start\":48640},{\"end\":48662,\"start\":48653},{\"end\":48676,\"start\":48670},{\"end\":48685,\"start\":48682},{\"end\":48694,\"start\":48692},{\"end\":48709,\"start\":48701},{\"end\":48980,\"start\":48977},{\"end\":48995,\"start\":48987},{\"end\":49006,\"start\":49001},{\"end\":49017,\"start\":49012},{\"end\":49031,\"start\":49024},{\"end\":49039,\"start\":49036},{\"end\":49288,\"start\":49279},{\"end\":49298,\"start\":49294},{\"end\":49308,\"start\":49305},{\"end\":49320,\"start\":49316},{\"end\":49565,\"start\":49559},{\"end\":49590,\"start\":49584},{\"end\":49605,\"start\":49598},{\"end\":49629,\"start\":49619},{\"end\":49935,\"start\":49929},{\"end\":49960,\"start\":49954},{\"end\":49975,\"start\":49968},{\"end\":49999,\"start\":49989},{\"end\":50024,\"start\":50020},{\"end\":50316,\"start\":50310},{\"end\":50341,\"start\":50335},{\"end\":50356,\"start\":50349},{\"end\":50380,\"start\":50370},{\"end\":50405,\"start\":50401},{\"end\":50678,\"start\":50672},{\"end\":50703,\"start\":50697},{\"end\":50718,\"start\":50711},{\"end\":50742,\"start\":50732},{\"end\":50767,\"start\":50763},{\"end\":51019,\"start\":51014},{\"end\":51034,\"start\":51027},{\"end\":51046,\"start\":51041},{\"end\":51333,\"start\":51330},{\"end\":51346,\"start\":51341},{\"end\":51359,\"start\":51351},{\"end\":51368,\"start\":51365},{\"end\":51379,\"start\":51376},{\"end\":51394,\"start\":51390},{\"end\":51643,\"start\":51640},{\"end\":51659,\"start\":51651},{\"end\":51671,\"start\":51665},{\"end\":51682,\"start\":51678},{\"end\":51692,\"start\":51689},{\"end\":51944,\"start\":51939},{\"end\":51959,\"start\":51952},{\"end\":51967,\"start\":51964},{\"end\":51978,\"start\":51972},{\"end\":51991,\"start\":51985},{\"end\":52002,\"start\":51999},{\"end\":52263,\"start\":52258},{\"end\":52278,\"start\":52271},{\"end\":52286,\"start\":52283},{\"end\":52297,\"start\":52291},{\"end\":52308,\"start\":52305},{\"end\":52519,\"start\":52514},{\"end\":52533,\"start\":52527},{\"end\":52542,\"start\":52540},{\"end\":52555,\"start\":52549},{\"end\":52566,\"start\":52563},{\"end\":52785,\"start\":52776},{\"end\":52798,\"start\":52792},{\"end\":52814,\"start\":52806},{\"end\":52831,\"start\":52820},{\"end\":35851,\"start\":35840},{\"end\":35871,\"start\":35864},{\"end\":35886,\"start\":35877},{\"end\":36100,\"start\":36095},{\"end\":36118,\"start\":36111},{\"end\":36134,\"start\":36126},{\"end\":36152,\"start\":36144},{\"end\":36441,\"start\":36436},{\"end\":36459,\"start\":36454},{\"end\":36744,\"start\":36738},{\"end\":36763,\"start\":36758},{\"end\":36784,\"start\":36778},{\"end\":36799,\"start\":36793},{\"end\":37039,\"start\":37032},{\"end\":37051,\"start\":37046},{\"end\":37064,\"start\":37058},{\"end\":37075,\"start\":37070},{\"end\":37086,\"start\":37080},{\"end\":37100,\"start\":37093},{\"end\":37111,\"start\":37106},{\"end\":37124,\"start\":37116},{\"end\":37133,\"start\":37129},{\"end\":37141,\"start\":37138},{\"end\":37409,\"start\":37404},{\"end\":37422,\"start\":37416},{\"end\":37430,\"start\":37427},{\"end\":37691,\"start\":37681},{\"end\":37700,\"start\":37697},{\"end\":37713,\"start\":37707},{\"end\":37722,\"start\":37720},{\"end\":37737,\"start\":37730},{\"end\":37750,\"start\":37743},{\"end\":37762,\"start\":37756},{\"end\":37775,\"start\":37768},{\"end\":38059,\"start\":38049},{\"end\":38068,\"start\":38065},{\"end\":38077,\"start\":38075},{\"end\":38092,\"start\":38085},{\"end\":38106,\"start\":38099},{\"end\":38118,\"start\":38112},{\"end\":38131,\"start\":38124},{\"end\":38430,\"start\":38427},{\"end\":38443,\"start\":38436},{\"end\":38457,\"start\":38449},{\"end\":38472,\"start\":38465},{\"end\":38481,\"start\":38478},{\"end\":38718,\"start\":38714},{\"end\":38730,\"start\":38725},{\"end\":38741,\"start\":38737},{\"end\":38748,\"start\":38742},{\"end\":38760,\"start\":38754},{\"end\":38987,\"start\":38983},{\"end\":38998,\"start\":38994},{\"end\":39005,\"start\":38999},{\"end\":39018,\"start\":39011},{\"end\":39029,\"start\":39023},{\"end\":39277,\"start\":39271},{\"end\":39291,\"start\":39284},{\"end\":39305,\"start\":39297},{\"end\":39319,\"start\":39312},{\"end\":39334,\"start\":39327},{\"end\":39341,\"start\":39339},{\"end\":39352,\"start\":39348},{\"end\":39366,\"start\":39359},{\"end\":39684,\"start\":39678},{\"end\":39703,\"start\":39698},{\"end\":39720,\"start\":39711},{\"end\":39737,\"start\":39733},{\"end\":39758,\"start\":39751},{\"end\":39771,\"start\":39765},{\"end\":39792,\"start\":39785},{\"end\":39811,\"start\":39803},{\"end\":39827,\"start\":39822},{\"end\":39844,\"start\":39837},{\"end\":40267,\"start\":40257},{\"end\":40281,\"start\":40273},{\"end\":40298,\"start\":40293},{\"end\":40538,\"start\":40531},{\"end\":40550,\"start\":40543},{\"end\":40566,\"start\":40558},{\"end\":40576,\"start\":40572},{\"end\":40752,\"start\":40749},{\"end\":40766,\"start\":40760},{\"end\":40773,\"start\":40772},{\"end\":40789,\"start\":40782},{\"end\":41028,\"start\":41021},{\"end\":41044,\"start\":41036},{\"end\":41060,\"start\":41052},{\"end\":41266,\"start\":41258},{\"end\":41280,\"start\":41275},{\"end\":41434,\"start\":41426},{\"end\":41445,\"start\":41441},{\"end\":41457,\"start\":41451},{\"end\":41464,\"start\":41462},{\"end\":41475,\"start\":41471},{\"end\":41695,\"start\":41688},{\"end\":41706,\"start\":41700},{\"end\":41719,\"start\":41712},{\"end\":41733,\"start\":41726},{\"end\":41745,\"start\":41739},{\"end\":41752,\"start\":41750},{\"end\":41763,\"start\":41759},{\"end\":42016,\"start\":42009},{\"end\":42032,\"start\":42024},{\"end\":42044,\"start\":42038},{\"end\":42053,\"start\":42050},{\"end\":42064,\"start\":42061},{\"end\":42079,\"start\":42075},{\"end\":42343,\"start\":42340},{\"end\":42357,\"start\":42349},{\"end\":42369,\"start\":42363},{\"end\":42383,\"start\":42375},{\"end\":42398,\"start\":42389},{\"end\":42655,\"start\":42653},{\"end\":42667,\"start\":42661},{\"end\":42676,\"start\":42673},{\"end\":42685,\"start\":42682},{\"end\":42696,\"start\":42690},{\"end\":42707,\"start\":42702},{\"end\":42722,\"start\":42715},{\"end\":42735,\"start\":42728},{\"end\":42995,\"start\":42991},{\"end\":43013,\"start\":43008},{\"end\":43177,\"start\":43173},{\"end\":43195,\"start\":43190},{\"end\":43388,\"start\":43384},{\"end\":43402,\"start\":43393},{\"end\":43417,\"start\":43411},{\"end\":43426,\"start\":43422},{\"end\":43440,\"start\":43433},{\"end\":43456,\"start\":43447},{\"end\":43464,\"start\":43461},{\"end\":43812,\"start\":43807},{\"end\":43829,\"start\":43821},{\"end\":43844,\"start\":43839},{\"end\":43858,\"start\":43850},{\"end\":44187,\"start\":44181},{\"end\":44200,\"start\":44196},{\"end\":44210,\"start\":44206},{\"end\":44225,\"start\":44220},{\"end\":44240,\"start\":44236},{\"end\":44257,\"start\":44248},{\"end\":44276,\"start\":44268},{\"end\":44551,\"start\":44546},{\"end\":44563,\"start\":44557},{\"end\":44575,\"start\":44569},{\"end\":44815,\"start\":44810},{\"end\":44827,\"start\":44821},{\"end\":44839,\"start\":44833},{\"end\":44852,\"start\":44846},{\"end\":44861,\"start\":44860},{\"end\":44878,\"start\":44870},{\"end\":45183,\"start\":45180},{\"end\":45195,\"start\":45189},{\"end\":45206,\"start\":45201},{\"end\":45219,\"start\":45212},{\"end\":45235,\"start\":45227},{\"end\":45249,\"start\":45242},{\"end\":45262,\"start\":45256},{\"end\":45278,\"start\":45270},{\"end\":45291,\"start\":45284},{\"end\":45563,\"start\":45559},{\"end\":45575,\"start\":45572},{\"end\":45590,\"start\":45583},{\"end\":45608,\"start\":45601},{\"end\":45623,\"start\":45617},{\"end\":45637,\"start\":45630},{\"end\":45652,\"start\":45646},{\"end\":45663,\"start\":45658},{\"end\":45679,\"start\":45675},{\"end\":45692,\"start\":45688},{\"end\":45955,\"start\":45949},{\"end\":45969,\"start\":45963},{\"end\":46156,\"start\":46152},{\"end\":46177,\"start\":46170},{\"end\":46193,\"start\":46187},{\"end\":46438,\"start\":46437},{\"end\":46455,\"start\":46454},{\"end\":46470,\"start\":46466},{\"end\":46472,\"start\":46471},{\"end\":46761,\"start\":46755},{\"end\":46771,\"start\":46767},{\"end\":46789,\"start\":46783},{\"end\":46806,\"start\":46798},{\"end\":46814,\"start\":46813},{\"end\":46826,\"start\":46823},{\"end\":46841,\"start\":46835},{\"end\":46855,\"start\":46850},{\"end\":47205,\"start\":47201},{\"end\":47222,\"start\":47215},{\"end\":47237,\"start\":47234},{\"end\":47258,\"start\":47248},{\"end\":47268,\"start\":47265},{\"end\":47279,\"start\":47276},{\"end\":47293,\"start\":47285},{\"end\":47305,\"start\":47299},{\"end\":47319,\"start\":47311},{\"end\":47671,\"start\":47664},{\"end\":47680,\"start\":47678},{\"end\":47690,\"start\":47686},{\"end\":47703,\"start\":47697},{\"end\":47713,\"start\":47709},{\"end\":47726,\"start\":47719},{\"end\":47734,\"start\":47731},{\"end\":48000,\"start\":47992},{\"end\":48015,\"start\":48007},{\"end\":48028,\"start\":48021},{\"end\":48041,\"start\":48034},{\"end\":48058,\"start\":48048},{\"end\":48072,\"start\":48064},{\"end\":48336,\"start\":48332},{\"end\":48347,\"start\":48343},{\"end\":48349,\"start\":48348},{\"end\":48358,\"start\":48357},{\"end\":48372,\"start\":48366},{\"end\":48633,\"start\":48626},{\"end\":48648,\"start\":48640},{\"end\":48662,\"start\":48653},{\"end\":48676,\"start\":48670},{\"end\":48685,\"start\":48682},{\"end\":48694,\"start\":48692},{\"end\":48709,\"start\":48701},{\"end\":48980,\"start\":48977},{\"end\":48995,\"start\":48987},{\"end\":49006,\"start\":49001},{\"end\":49017,\"start\":49012},{\"end\":49031,\"start\":49024},{\"end\":49039,\"start\":49036},{\"end\":49288,\"start\":49279},{\"end\":49298,\"start\":49294},{\"end\":49308,\"start\":49305},{\"end\":49320,\"start\":49316},{\"end\":49565,\"start\":49559},{\"end\":49590,\"start\":49584},{\"end\":49605,\"start\":49598},{\"end\":49629,\"start\":49619},{\"end\":49935,\"start\":49929},{\"end\":49960,\"start\":49954},{\"end\":49975,\"start\":49968},{\"end\":49999,\"start\":49989},{\"end\":50024,\"start\":50020},{\"end\":50316,\"start\":50310},{\"end\":50341,\"start\":50335},{\"end\":50356,\"start\":50349},{\"end\":50380,\"start\":50370},{\"end\":50405,\"start\":50401},{\"end\":50678,\"start\":50672},{\"end\":50703,\"start\":50697},{\"end\":50718,\"start\":50711},{\"end\":50742,\"start\":50732},{\"end\":50767,\"start\":50763},{\"end\":51019,\"start\":51014},{\"end\":51034,\"start\":51027},{\"end\":51046,\"start\":51041},{\"end\":51333,\"start\":51330},{\"end\":51346,\"start\":51341},{\"end\":51359,\"start\":51351},{\"end\":51368,\"start\":51365},{\"end\":51379,\"start\":51376},{\"end\":51394,\"start\":51390},{\"end\":51643,\"start\":51640},{\"end\":51659,\"start\":51651},{\"end\":51671,\"start\":51665},{\"end\":51682,\"start\":51678},{\"end\":51692,\"start\":51689},{\"end\":51944,\"start\":51939},{\"end\":51959,\"start\":51952},{\"end\":51967,\"start\":51964},{\"end\":51978,\"start\":51972},{\"end\":51991,\"start\":51985},{\"end\":52002,\"start\":51999},{\"end\":52263,\"start\":52258},{\"end\":52278,\"start\":52271},{\"end\":52286,\"start\":52283},{\"end\":52297,\"start\":52291},{\"end\":52308,\"start\":52305},{\"end\":52519,\"start\":52514},{\"end\":52533,\"start\":52527},{\"end\":52542,\"start\":52540},{\"end\":52555,\"start\":52549},{\"end\":52566,\"start\":52563},{\"end\":52785,\"start\":52776},{\"end\":52798,\"start\":52792},{\"end\":52814,\"start\":52806},{\"end\":52831,\"start\":52820}]", "bib_author_last_name": "[{\"end\":35862,\"start\":35852},{\"end\":35875,\"start\":35872},{\"end\":35892,\"start\":35887},{\"end\":36109,\"start\":36101},{\"end\":36124,\"start\":36119},{\"end\":36142,\"start\":36135},{\"end\":36158,\"start\":36153},{\"end\":36452,\"start\":36442},{\"end\":36465,\"start\":36460},{\"end\":36756,\"start\":36745},{\"end\":36776,\"start\":36764},{\"end\":36791,\"start\":36785},{\"end\":36807,\"start\":36800},{\"end\":37044,\"start\":37040},{\"end\":37056,\"start\":37052},{\"end\":37068,\"start\":37065},{\"end\":37078,\"start\":37076},{\"end\":37091,\"start\":37087},{\"end\":37104,\"start\":37101},{\"end\":37114,\"start\":37112},{\"end\":37127,\"start\":37125},{\"end\":37136,\"start\":37134},{\"end\":37145,\"start\":37142},{\"end\":37414,\"start\":37410},{\"end\":37425,\"start\":37423},{\"end\":37436,\"start\":37431},{\"end\":37695,\"start\":37692},{\"end\":37705,\"start\":37701},{\"end\":37718,\"start\":37714},{\"end\":37728,\"start\":37723},{\"end\":37741,\"start\":37738},{\"end\":37754,\"start\":37751},{\"end\":37766,\"start\":37763},{\"end\":37780,\"start\":37776},{\"end\":38063,\"start\":38060},{\"end\":38073,\"start\":38069},{\"end\":38083,\"start\":38078},{\"end\":38097,\"start\":38093},{\"end\":38110,\"start\":38107},{\"end\":38122,\"start\":38119},{\"end\":38136,\"start\":38132},{\"end\":38434,\"start\":38431},{\"end\":38447,\"start\":38444},{\"end\":38463,\"start\":38458},{\"end\":38476,\"start\":38473},{\"end\":38487,\"start\":38482},{\"end\":38723,\"start\":38719},{\"end\":38735,\"start\":38731},{\"end\":38752,\"start\":38749},{\"end\":38765,\"start\":38761},{\"end\":38992,\"start\":38988},{\"end\":39009,\"start\":39006},{\"end\":39021,\"start\":39019},{\"end\":39034,\"start\":39030},{\"end\":39282,\"start\":39278},{\"end\":39295,\"start\":39292},{\"end\":39310,\"start\":39306},{\"end\":39325,\"start\":39320},{\"end\":39337,\"start\":39335},{\"end\":39346,\"start\":39342},{\"end\":39357,\"start\":39353},{\"end\":39370,\"start\":39367},{\"end\":39696,\"start\":39685},{\"end\":39709,\"start\":39704},{\"end\":39731,\"start\":39721},{\"end\":39749,\"start\":39738},{\"end\":39763,\"start\":39759},{\"end\":39783,\"start\":39772},{\"end\":39801,\"start\":39793},{\"end\":39820,\"start\":39812},{\"end\":39835,\"start\":39828},{\"end\":39850,\"start\":39845},{\"end\":40271,\"start\":40268},{\"end\":40291,\"start\":40282},{\"end\":40309,\"start\":40299},{\"end\":40541,\"start\":40539},{\"end\":40556,\"start\":40551},{\"end\":40570,\"start\":40567},{\"end\":40580,\"start\":40577},{\"end\":40758,\"start\":40753},{\"end\":40770,\"start\":40767},{\"end\":40780,\"start\":40774},{\"end\":40800,\"start\":40790},{\"end\":40816,\"start\":40802},{\"end\":41034,\"start\":41029},{\"end\":41050,\"start\":41045},{\"end\":41066,\"start\":41061},{\"end\":41273,\"start\":41267},{\"end\":41283,\"start\":41281},{\"end\":41439,\"start\":41435},{\"end\":41449,\"start\":41446},{\"end\":41460,\"start\":41458},{\"end\":41469,\"start\":41465},{\"end\":41480,\"start\":41476},{\"end\":41698,\"start\":41696},{\"end\":41710,\"start\":41707},{\"end\":41724,\"start\":41720},{\"end\":41737,\"start\":41734},{\"end\":41748,\"start\":41746},{\"end\":41757,\"start\":41753},{\"end\":41768,\"start\":41764},{\"end\":42022,\"start\":42017},{\"end\":42036,\"start\":42033},{\"end\":42048,\"start\":42045},{\"end\":42059,\"start\":42054},{\"end\":42073,\"start\":42065},{\"end\":42087,\"start\":42080},{\"end\":42347,\"start\":42344},{\"end\":42361,\"start\":42358},{\"end\":42373,\"start\":42370},{\"end\":42387,\"start\":42384},{\"end\":42402,\"start\":42399},{\"end\":42659,\"start\":42656},{\"end\":42671,\"start\":42668},{\"end\":42680,\"start\":42677},{\"end\":42688,\"start\":42686},{\"end\":42700,\"start\":42697},{\"end\":42713,\"start\":42708},{\"end\":42726,\"start\":42723},{\"end\":42739,\"start\":42736},{\"end\":43006,\"start\":42996},{\"end\":43020,\"start\":43014},{\"end\":43188,\"start\":43178},{\"end\":43202,\"start\":43196},{\"end\":43391,\"start\":43389},{\"end\":43409,\"start\":43403},{\"end\":43420,\"start\":43418},{\"end\":43431,\"start\":43427},{\"end\":43445,\"start\":43441},{\"end\":43459,\"start\":43457},{\"end\":43470,\"start\":43465},{\"end\":43819,\"start\":43813},{\"end\":43837,\"start\":43830},{\"end\":43848,\"start\":43845},{\"end\":43864,\"start\":43859},{\"end\":44194,\"start\":44188},{\"end\":44204,\"start\":44201},{\"end\":44218,\"start\":44211},{\"end\":44234,\"start\":44226},{\"end\":44246,\"start\":44241},{\"end\":44266,\"start\":44258},{\"end\":44283,\"start\":44277},{\"end\":44555,\"start\":44552},{\"end\":44567,\"start\":44564},{\"end\":44580,\"start\":44576},{\"end\":44819,\"start\":44816},{\"end\":44831,\"start\":44828},{\"end\":44844,\"start\":44840},{\"end\":44858,\"start\":44853},{\"end\":44868,\"start\":44862},{\"end\":44884,\"start\":44879},{\"end\":44889,\"start\":44886},{\"end\":45187,\"start\":45184},{\"end\":45199,\"start\":45196},{\"end\":45210,\"start\":45207},{\"end\":45225,\"start\":45220},{\"end\":45240,\"start\":45236},{\"end\":45254,\"start\":45250},{\"end\":45268,\"start\":45263},{\"end\":45282,\"start\":45279},{\"end\":45296,\"start\":45292},{\"end\":45570,\"start\":45564},{\"end\":45581,\"start\":45576},{\"end\":45599,\"start\":45591},{\"end\":45615,\"start\":45609},{\"end\":45628,\"start\":45624},{\"end\":45644,\"start\":45638},{\"end\":45656,\"start\":45653},{\"end\":45673,\"start\":45664},{\"end\":45686,\"start\":45680},{\"end\":45698,\"start\":45693},{\"end\":45961,\"start\":45956},{\"end\":45974,\"start\":45970},{\"end\":46168,\"start\":46157},{\"end\":46185,\"start\":46178},{\"end\":46198,\"start\":46194},{\"end\":46444,\"start\":46439},{\"end\":46452,\"start\":46446},{\"end\":46464,\"start\":46456},{\"end\":46478,\"start\":46473},{\"end\":46485,\"start\":46480},{\"end\":46765,\"start\":46762},{\"end\":46781,\"start\":46772},{\"end\":46796,\"start\":46790},{\"end\":46811,\"start\":46807},{\"end\":46821,\"start\":46815},{\"end\":46833,\"start\":46827},{\"end\":46848,\"start\":46842},{\"end\":46864,\"start\":46856},{\"end\":46870,\"start\":46866},{\"end\":47213,\"start\":47206},{\"end\":47232,\"start\":47223},{\"end\":47246,\"start\":47238},{\"end\":47263,\"start\":47259},{\"end\":47274,\"start\":47269},{\"end\":47283,\"start\":47280},{\"end\":47297,\"start\":47294},{\"end\":47309,\"start\":47306},{\"end\":47323,\"start\":47320},{\"end\":47338,\"start\":47325},{\"end\":47676,\"start\":47672},{\"end\":47684,\"start\":47681},{\"end\":47695,\"start\":47691},{\"end\":47707,\"start\":47704},{\"end\":47717,\"start\":47714},{\"end\":47729,\"start\":47727},{\"end\":47738,\"start\":47735},{\"end\":48005,\"start\":48001},{\"end\":48019,\"start\":48016},{\"end\":48032,\"start\":48029},{\"end\":48046,\"start\":48042},{\"end\":48062,\"start\":48059},{\"end\":48075,\"start\":48073},{\"end\":48341,\"start\":48337},{\"end\":48355,\"start\":48350},{\"end\":48364,\"start\":48359},{\"end\":48379,\"start\":48373},{\"end\":48391,\"start\":48381},{\"end\":48638,\"start\":48634},{\"end\":48651,\"start\":48649},{\"end\":48668,\"start\":48663},{\"end\":48680,\"start\":48677},{\"end\":48690,\"start\":48686},{\"end\":48699,\"start\":48695},{\"end\":48713,\"start\":48710},{\"end\":48985,\"start\":48981},{\"end\":48999,\"start\":48996},{\"end\":49010,\"start\":49007},{\"end\":49022,\"start\":49018},{\"end\":49034,\"start\":49032},{\"end\":49042,\"start\":49040},{\"end\":49292,\"start\":49289},{\"end\":49303,\"start\":49299},{\"end\":49314,\"start\":49309},{\"end\":49325,\"start\":49321},{\"end\":49582,\"start\":49566},{\"end\":49596,\"start\":49591},{\"end\":49610,\"start\":49606},{\"end\":49617,\"start\":49612},{\"end\":49648,\"start\":49630},{\"end\":49654,\"start\":49650},{\"end\":49952,\"start\":49936},{\"end\":49966,\"start\":49961},{\"end\":49980,\"start\":49976},{\"end\":49987,\"start\":49982},{\"end\":50018,\"start\":50000},{\"end\":50029,\"start\":50025},{\"end\":50035,\"start\":50031},{\"end\":50333,\"start\":50317},{\"end\":50347,\"start\":50342},{\"end\":50361,\"start\":50357},{\"end\":50368,\"start\":50363},{\"end\":50399,\"start\":50381},{\"end\":50410,\"start\":50406},{\"end\":50416,\"start\":50412},{\"end\":50695,\"start\":50679},{\"end\":50709,\"start\":50704},{\"end\":50723,\"start\":50719},{\"end\":50730,\"start\":50725},{\"end\":50761,\"start\":50743},{\"end\":50772,\"start\":50768},{\"end\":50778,\"start\":50774},{\"end\":51025,\"start\":51020},{\"end\":51039,\"start\":51035},{\"end\":51054,\"start\":51047},{\"end\":51339,\"start\":51334},{\"end\":51349,\"start\":51347},{\"end\":51363,\"start\":51360},{\"end\":51374,\"start\":51369},{\"end\":51388,\"start\":51380},{\"end\":51402,\"start\":51395},{\"end\":51649,\"start\":51644},{\"end\":51663,\"start\":51660},{\"end\":51676,\"start\":51672},{\"end\":51687,\"start\":51683},{\"end\":51698,\"start\":51693},{\"end\":51950,\"start\":51945},{\"end\":51962,\"start\":51960},{\"end\":51970,\"start\":51968},{\"end\":51983,\"start\":51979},{\"end\":51997,\"start\":51992},{\"end\":52005,\"start\":52003},{\"end\":52269,\"start\":52264},{\"end\":52281,\"start\":52279},{\"end\":52289,\"start\":52287},{\"end\":52303,\"start\":52298},{\"end\":52311,\"start\":52309},{\"end\":52525,\"start\":52520},{\"end\":52538,\"start\":52534},{\"end\":52547,\"start\":52543},{\"end\":52561,\"start\":52556},{\"end\":52569,\"start\":52567},{\"end\":52790,\"start\":52786},{\"end\":52804,\"start\":52799},{\"end\":52818,\"start\":52815},{\"end\":52835,\"start\":52832},{\"end\":35862,\"start\":35852},{\"end\":35875,\"start\":35872},{\"end\":35892,\"start\":35887},{\"end\":36109,\"start\":36101},{\"end\":36124,\"start\":36119},{\"end\":36142,\"start\":36135},{\"end\":36158,\"start\":36153},{\"end\":36452,\"start\":36442},{\"end\":36465,\"start\":36460},{\"end\":36756,\"start\":36745},{\"end\":36776,\"start\":36764},{\"end\":36791,\"start\":36785},{\"end\":36807,\"start\":36800},{\"end\":37044,\"start\":37040},{\"end\":37056,\"start\":37052},{\"end\":37068,\"start\":37065},{\"end\":37078,\"start\":37076},{\"end\":37091,\"start\":37087},{\"end\":37104,\"start\":37101},{\"end\":37114,\"start\":37112},{\"end\":37127,\"start\":37125},{\"end\":37136,\"start\":37134},{\"end\":37145,\"start\":37142},{\"end\":37414,\"start\":37410},{\"end\":37425,\"start\":37423},{\"end\":37436,\"start\":37431},{\"end\":37695,\"start\":37692},{\"end\":37705,\"start\":37701},{\"end\":37718,\"start\":37714},{\"end\":37728,\"start\":37723},{\"end\":37741,\"start\":37738},{\"end\":37754,\"start\":37751},{\"end\":37766,\"start\":37763},{\"end\":37780,\"start\":37776},{\"end\":38063,\"start\":38060},{\"end\":38073,\"start\":38069},{\"end\":38083,\"start\":38078},{\"end\":38097,\"start\":38093},{\"end\":38110,\"start\":38107},{\"end\":38122,\"start\":38119},{\"end\":38136,\"start\":38132},{\"end\":38434,\"start\":38431},{\"end\":38447,\"start\":38444},{\"end\":38463,\"start\":38458},{\"end\":38476,\"start\":38473},{\"end\":38487,\"start\":38482},{\"end\":38723,\"start\":38719},{\"end\":38735,\"start\":38731},{\"end\":38752,\"start\":38749},{\"end\":38765,\"start\":38761},{\"end\":38992,\"start\":38988},{\"end\":39009,\"start\":39006},{\"end\":39021,\"start\":39019},{\"end\":39034,\"start\":39030},{\"end\":39282,\"start\":39278},{\"end\":39295,\"start\":39292},{\"end\":39310,\"start\":39306},{\"end\":39325,\"start\":39320},{\"end\":39337,\"start\":39335},{\"end\":39346,\"start\":39342},{\"end\":39357,\"start\":39353},{\"end\":39370,\"start\":39367},{\"end\":39696,\"start\":39685},{\"end\":39709,\"start\":39704},{\"end\":39731,\"start\":39721},{\"end\":39749,\"start\":39738},{\"end\":39763,\"start\":39759},{\"end\":39783,\"start\":39772},{\"end\":39801,\"start\":39793},{\"end\":39820,\"start\":39812},{\"end\":39835,\"start\":39828},{\"end\":39850,\"start\":39845},{\"end\":40271,\"start\":40268},{\"end\":40291,\"start\":40282},{\"end\":40309,\"start\":40299},{\"end\":40541,\"start\":40539},{\"end\":40556,\"start\":40551},{\"end\":40570,\"start\":40567},{\"end\":40580,\"start\":40577},{\"end\":40758,\"start\":40753},{\"end\":40770,\"start\":40767},{\"end\":40780,\"start\":40774},{\"end\":40800,\"start\":40790},{\"end\":40816,\"start\":40802},{\"end\":41034,\"start\":41029},{\"end\":41050,\"start\":41045},{\"end\":41066,\"start\":41061},{\"end\":41273,\"start\":41267},{\"end\":41283,\"start\":41281},{\"end\":41439,\"start\":41435},{\"end\":41449,\"start\":41446},{\"end\":41460,\"start\":41458},{\"end\":41469,\"start\":41465},{\"end\":41480,\"start\":41476},{\"end\":41698,\"start\":41696},{\"end\":41710,\"start\":41707},{\"end\":41724,\"start\":41720},{\"end\":41737,\"start\":41734},{\"end\":41748,\"start\":41746},{\"end\":41757,\"start\":41753},{\"end\":41768,\"start\":41764},{\"end\":42022,\"start\":42017},{\"end\":42036,\"start\":42033},{\"end\":42048,\"start\":42045},{\"end\":42059,\"start\":42054},{\"end\":42073,\"start\":42065},{\"end\":42087,\"start\":42080},{\"end\":42347,\"start\":42344},{\"end\":42361,\"start\":42358},{\"end\":42373,\"start\":42370},{\"end\":42387,\"start\":42384},{\"end\":42402,\"start\":42399},{\"end\":42659,\"start\":42656},{\"end\":42671,\"start\":42668},{\"end\":42680,\"start\":42677},{\"end\":42688,\"start\":42686},{\"end\":42700,\"start\":42697},{\"end\":42713,\"start\":42708},{\"end\":42726,\"start\":42723},{\"end\":42739,\"start\":42736},{\"end\":43006,\"start\":42996},{\"end\":43020,\"start\":43014},{\"end\":43188,\"start\":43178},{\"end\":43202,\"start\":43196},{\"end\":43391,\"start\":43389},{\"end\":43409,\"start\":43403},{\"end\":43420,\"start\":43418},{\"end\":43431,\"start\":43427},{\"end\":43445,\"start\":43441},{\"end\":43459,\"start\":43457},{\"end\":43470,\"start\":43465},{\"end\":43819,\"start\":43813},{\"end\":43837,\"start\":43830},{\"end\":43848,\"start\":43845},{\"end\":43864,\"start\":43859},{\"end\":44194,\"start\":44188},{\"end\":44204,\"start\":44201},{\"end\":44218,\"start\":44211},{\"end\":44234,\"start\":44226},{\"end\":44246,\"start\":44241},{\"end\":44266,\"start\":44258},{\"end\":44283,\"start\":44277},{\"end\":44555,\"start\":44552},{\"end\":44567,\"start\":44564},{\"end\":44580,\"start\":44576},{\"end\":44819,\"start\":44816},{\"end\":44831,\"start\":44828},{\"end\":44844,\"start\":44840},{\"end\":44858,\"start\":44853},{\"end\":44868,\"start\":44862},{\"end\":44884,\"start\":44879},{\"end\":44889,\"start\":44886},{\"end\":45187,\"start\":45184},{\"end\":45199,\"start\":45196},{\"end\":45210,\"start\":45207},{\"end\":45225,\"start\":45220},{\"end\":45240,\"start\":45236},{\"end\":45254,\"start\":45250},{\"end\":45268,\"start\":45263},{\"end\":45282,\"start\":45279},{\"end\":45296,\"start\":45292},{\"end\":45570,\"start\":45564},{\"end\":45581,\"start\":45576},{\"end\":45599,\"start\":45591},{\"end\":45615,\"start\":45609},{\"end\":45628,\"start\":45624},{\"end\":45644,\"start\":45638},{\"end\":45656,\"start\":45653},{\"end\":45673,\"start\":45664},{\"end\":45686,\"start\":45680},{\"end\":45698,\"start\":45693},{\"end\":45961,\"start\":45956},{\"end\":45974,\"start\":45970},{\"end\":46168,\"start\":46157},{\"end\":46185,\"start\":46178},{\"end\":46198,\"start\":46194},{\"end\":46444,\"start\":46439},{\"end\":46452,\"start\":46446},{\"end\":46464,\"start\":46456},{\"end\":46478,\"start\":46473},{\"end\":46485,\"start\":46480},{\"end\":46765,\"start\":46762},{\"end\":46781,\"start\":46772},{\"end\":46796,\"start\":46790},{\"end\":46811,\"start\":46807},{\"end\":46821,\"start\":46815},{\"end\":46833,\"start\":46827},{\"end\":46848,\"start\":46842},{\"end\":46864,\"start\":46856},{\"end\":46870,\"start\":46866},{\"end\":47213,\"start\":47206},{\"end\":47232,\"start\":47223},{\"end\":47246,\"start\":47238},{\"end\":47263,\"start\":47259},{\"end\":47274,\"start\":47269},{\"end\":47283,\"start\":47280},{\"end\":47297,\"start\":47294},{\"end\":47309,\"start\":47306},{\"end\":47323,\"start\":47320},{\"end\":47338,\"start\":47325},{\"end\":47676,\"start\":47672},{\"end\":47684,\"start\":47681},{\"end\":47695,\"start\":47691},{\"end\":47707,\"start\":47704},{\"end\":47717,\"start\":47714},{\"end\":47729,\"start\":47727},{\"end\":47738,\"start\":47735},{\"end\":48005,\"start\":48001},{\"end\":48019,\"start\":48016},{\"end\":48032,\"start\":48029},{\"end\":48046,\"start\":48042},{\"end\":48062,\"start\":48059},{\"end\":48075,\"start\":48073},{\"end\":48341,\"start\":48337},{\"end\":48355,\"start\":48350},{\"end\":48364,\"start\":48359},{\"end\":48379,\"start\":48373},{\"end\":48391,\"start\":48381},{\"end\":48638,\"start\":48634},{\"end\":48651,\"start\":48649},{\"end\":48668,\"start\":48663},{\"end\":48680,\"start\":48677},{\"end\":48690,\"start\":48686},{\"end\":48699,\"start\":48695},{\"end\":48713,\"start\":48710},{\"end\":48985,\"start\":48981},{\"end\":48999,\"start\":48996},{\"end\":49010,\"start\":49007},{\"end\":49022,\"start\":49018},{\"end\":49034,\"start\":49032},{\"end\":49042,\"start\":49040},{\"end\":49292,\"start\":49289},{\"end\":49303,\"start\":49299},{\"end\":49314,\"start\":49309},{\"end\":49325,\"start\":49321},{\"end\":49582,\"start\":49566},{\"end\":49596,\"start\":49591},{\"end\":49610,\"start\":49606},{\"end\":49617,\"start\":49612},{\"end\":49648,\"start\":49630},{\"end\":49654,\"start\":49650},{\"end\":49952,\"start\":49936},{\"end\":49966,\"start\":49961},{\"end\":49980,\"start\":49976},{\"end\":49987,\"start\":49982},{\"end\":50018,\"start\":50000},{\"end\":50029,\"start\":50025},{\"end\":50035,\"start\":50031},{\"end\":50333,\"start\":50317},{\"end\":50347,\"start\":50342},{\"end\":50361,\"start\":50357},{\"end\":50368,\"start\":50363},{\"end\":50399,\"start\":50381},{\"end\":50410,\"start\":50406},{\"end\":50416,\"start\":50412},{\"end\":50695,\"start\":50679},{\"end\":50709,\"start\":50704},{\"end\":50723,\"start\":50719},{\"end\":50730,\"start\":50725},{\"end\":50761,\"start\":50743},{\"end\":50772,\"start\":50768},{\"end\":50778,\"start\":50774},{\"end\":51025,\"start\":51020},{\"end\":51039,\"start\":51035},{\"end\":51054,\"start\":51047},{\"end\":51339,\"start\":51334},{\"end\":51349,\"start\":51347},{\"end\":51363,\"start\":51360},{\"end\":51374,\"start\":51369},{\"end\":51388,\"start\":51380},{\"end\":51402,\"start\":51395},{\"end\":51649,\"start\":51644},{\"end\":51663,\"start\":51660},{\"end\":51676,\"start\":51672},{\"end\":51687,\"start\":51683},{\"end\":51698,\"start\":51693},{\"end\":51950,\"start\":51945},{\"end\":51962,\"start\":51960},{\"end\":51970,\"start\":51968},{\"end\":51983,\"start\":51979},{\"end\":51997,\"start\":51992},{\"end\":52005,\"start\":52003},{\"end\":52269,\"start\":52264},{\"end\":52281,\"start\":52279},{\"end\":52289,\"start\":52287},{\"end\":52303,\"start\":52298},{\"end\":52311,\"start\":52309},{\"end\":52525,\"start\":52520},{\"end\":52538,\"start\":52534},{\"end\":52547,\"start\":52543},{\"end\":52561,\"start\":52556},{\"end\":52569,\"start\":52567},{\"end\":52790,\"start\":52786},{\"end\":52804,\"start\":52799},{\"end\":52818,\"start\":52815},{\"end\":52835,\"start\":52832}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":52059988},\"end\":36031,\"start\":35783},{\"attributes\":{\"id\":\"b1\"},\"end\":36298,\"start\":36033},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":5250573},\"end\":36656,\"start\":36300},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":38030033},\"end\":36988,\"start\":36658},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":227239228},\"end\":37343,\"start\":36990},{\"attributes\":{\"doi\":\"arXiv:2104.09497\",\"id\":\"b5\"},\"end\":37605,\"start\":37345},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":234364557},\"end\":37989,\"start\":37607},{\"attributes\":{\"doi\":\"arXiv:2102.10882\",\"id\":\"b7\"},\"end\":38359,\"start\":37991},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":174788791},\"end\":38647,\"start\":38361},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":9569924},\"end\":38915,\"start\":38649},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":18874645},\"end\":39185,\"start\":38917},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":235694312},\"end\":39600,\"start\":39187},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":225039882},\"end\":40148,\"start\":39602},{\"attributes\":{\"id\":\"b13\"},\"end\":40483,\"start\":40150},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":206594692},\"end\":40705,\"start\":40485},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":9433631},\"end\":40956,\"start\":40707},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":8282555},\"end\":41212,\"start\":40958},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":6628106},\"end\":41381,\"start\":41214},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":245385444},\"end\":41611,\"start\":41383},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":248721662},\"end\":41957,\"start\":41613},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":237266491},\"end\":42271,\"start\":41959},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6540453},\"end\":42578,\"start\":42273},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":232352874},\"end\":42935,\"start\":42580},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":14337532},\"end\":43132,\"start\":42937},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":53592270},\"end\":43299,\"start\":43134},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":4840263},\"end\":43665,\"start\":43301},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":64193},\"end\":44091,\"start\":43667},{\"attributes\":{\"id\":\"b27\"},\"end\":44488,\"start\":44093},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":235641598},\"end\":44710,\"start\":44490},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":219179900},\"end\":45114,\"start\":44712},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":221187023},\"end\":45519,\"start\":45116},{\"attributes\":{\"id\":\"b31\"},\"end\":45890,\"start\":45521},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":9715523},\"end\":46085,\"start\":45892},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":3719281},\"end\":46348,\"start\":46087},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":9247572},\"end\":46644,\"start\":46350},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":7037846},\"end\":47123,\"start\":46646},{\"attributes\":{\"id\":\"b36\"},\"end\":47584,\"start\":47125},{\"attributes\":{\"doi\":\"ICLR, 2022. 4\",\"id\":\"b37\"},\"end\":47927,\"start\":47586},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":235358213},\"end\":48256,\"start\":47929},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":207761262},\"end\":48549,\"start\":48258},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":235694438},\"end\":48915,\"start\":48551},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":232307700},\"end\":49197,\"start\":48917},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":220495900},\"end\":49485,\"start\":49199},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":244346144},\"end\":49865,\"start\":49487},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":212737191},\"end\":50237,\"start\":49867},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":212725053},\"end\":50627,\"start\":50239},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":231802205},\"end\":50957,\"start\":50629},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":2356330},\"end\":51263,\"start\":50959},{\"attributes\":{\"id\":\"b48\"},\"end\":51559,\"start\":51265},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":996788},\"end\":51861,\"start\":51561},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":49657846},\"end\":52195,\"start\":51863},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":85501306},\"end\":52466,\"start\":52197},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":57189262},\"end\":52704,\"start\":52468},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":220265400},\"end\":53010,\"start\":52706},{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":52059988},\"end\":36031,\"start\":35783},{\"attributes\":{\"id\":\"b1\"},\"end\":36298,\"start\":36033},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":5250573},\"end\":36656,\"start\":36300},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":38030033},\"end\":36988,\"start\":36658},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":227239228},\"end\":37343,\"start\":36990},{\"attributes\":{\"doi\":\"arXiv:2104.09497\",\"id\":\"b5\"},\"end\":37605,\"start\":37345},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":234364557},\"end\":37989,\"start\":37607},{\"attributes\":{\"doi\":\"arXiv:2102.10882\",\"id\":\"b7\"},\"end\":38359,\"start\":37991},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":174788791},\"end\":38647,\"start\":38361},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":9569924},\"end\":38915,\"start\":38649},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":18874645},\"end\":39185,\"start\":38917},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":235694312},\"end\":39600,\"start\":39187},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":225039882},\"end\":40148,\"start\":39602},{\"attributes\":{\"id\":\"b13\"},\"end\":40483,\"start\":40150},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":206594692},\"end\":40705,\"start\":40485},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":9433631},\"end\":40956,\"start\":40707},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":8282555},\"end\":41212,\"start\":40958},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":6628106},\"end\":41381,\"start\":41214},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":245385444},\"end\":41611,\"start\":41383},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":248721662},\"end\":41957,\"start\":41613},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":237266491},\"end\":42271,\"start\":41959},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6540453},\"end\":42578,\"start\":42273},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":232352874},\"end\":42935,\"start\":42580},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":14337532},\"end\":43132,\"start\":42937},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":53592270},\"end\":43299,\"start\":43134},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":4840263},\"end\":43665,\"start\":43301},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":64193},\"end\":44091,\"start\":43667},{\"attributes\":{\"id\":\"b27\"},\"end\":44488,\"start\":44093},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":235641598},\"end\":44710,\"start\":44490},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":219179900},\"end\":45114,\"start\":44712},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":221187023},\"end\":45519,\"start\":45116},{\"attributes\":{\"id\":\"b31\"},\"end\":45890,\"start\":45521},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":9715523},\"end\":46085,\"start\":45892},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":3719281},\"end\":46348,\"start\":46087},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":9247572},\"end\":46644,\"start\":46350},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":7037846},\"end\":47123,\"start\":46646},{\"attributes\":{\"id\":\"b36\"},\"end\":47584,\"start\":47125},{\"attributes\":{\"doi\":\"ICLR, 2022. 4\",\"id\":\"b37\"},\"end\":47927,\"start\":47586},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":235358213},\"end\":48256,\"start\":47929},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":207761262},\"end\":48549,\"start\":48258},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":235694438},\"end\":48915,\"start\":48551},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":232307700},\"end\":49197,\"start\":48917},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":220495900},\"end\":49485,\"start\":49199},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":244346144},\"end\":49865,\"start\":49487},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":212737191},\"end\":50237,\"start\":49867},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":212725053},\"end\":50627,\"start\":50239},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":231802205},\"end\":50957,\"start\":50629},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":2356330},\"end\":51263,\"start\":50959},{\"attributes\":{\"id\":\"b48\"},\"end\":51559,\"start\":51265},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":996788},\"end\":51861,\"start\":51561},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":49657846},\"end\":52195,\"start\":51863},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":85501306},\"end\":52466,\"start\":52197},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":57189262},\"end\":52704,\"start\":52468},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":220265400},\"end\":53010,\"start\":52706}]", "bib_title": "[{\"end\":35838,\"start\":35783},{\"end\":36434,\"start\":36300},{\"end\":36736,\"start\":36658},{\"end\":37030,\"start\":36990},{\"end\":37679,\"start\":37607},{\"end\":38425,\"start\":38361},{\"end\":38712,\"start\":38649},{\"end\":38981,\"start\":38917},{\"end\":39269,\"start\":39187},{\"end\":39676,\"start\":39602},{\"end\":40529,\"start\":40485},{\"end\":40747,\"start\":40707},{\"end\":41019,\"start\":40958},{\"end\":41256,\"start\":41214},{\"end\":41424,\"start\":41383},{\"end\":41686,\"start\":41613},{\"end\":42007,\"start\":41959},{\"end\":42338,\"start\":42273},{\"end\":42651,\"start\":42580},{\"end\":42989,\"start\":42937},{\"end\":43171,\"start\":43134},{\"end\":43382,\"start\":43301},{\"end\":43805,\"start\":43667},{\"end\":44544,\"start\":44490},{\"end\":44808,\"start\":44712},{\"end\":45178,\"start\":45116},{\"end\":45947,\"start\":45892},{\"end\":46150,\"start\":46087},{\"end\":46435,\"start\":46350},{\"end\":46753,\"start\":46646},{\"end\":47990,\"start\":47929},{\"end\":48330,\"start\":48258},{\"end\":48624,\"start\":48551},{\"end\":48975,\"start\":48917},{\"end\":49277,\"start\":49199},{\"end\":49557,\"start\":49487},{\"end\":49927,\"start\":49867},{\"end\":50308,\"start\":50239},{\"end\":50670,\"start\":50629},{\"end\":51012,\"start\":50959},{\"end\":51638,\"start\":51561},{\"end\":51937,\"start\":51863},{\"end\":52256,\"start\":52197},{\"end\":52512,\"start\":52468},{\"end\":52774,\"start\":52706},{\"end\":35838,\"start\":35783},{\"end\":36434,\"start\":36300},{\"end\":36736,\"start\":36658},{\"end\":37030,\"start\":36990},{\"end\":37679,\"start\":37607},{\"end\":38425,\"start\":38361},{\"end\":38712,\"start\":38649},{\"end\":38981,\"start\":38917},{\"end\":39269,\"start\":39187},{\"end\":39676,\"start\":39602},{\"end\":40529,\"start\":40485},{\"end\":40747,\"start\":40707},{\"end\":41019,\"start\":40958},{\"end\":41256,\"start\":41214},{\"end\":41424,\"start\":41383},{\"end\":41686,\"start\":41613},{\"end\":42007,\"start\":41959},{\"end\":42338,\"start\":42273},{\"end\":42651,\"start\":42580},{\"end\":42989,\"start\":42937},{\"end\":43171,\"start\":43134},{\"end\":43382,\"start\":43301},{\"end\":43805,\"start\":43667},{\"end\":44544,\"start\":44490},{\"end\":44808,\"start\":44712},{\"end\":45178,\"start\":45116},{\"end\":45947,\"start\":45892},{\"end\":46150,\"start\":46087},{\"end\":46435,\"start\":46350},{\"end\":46753,\"start\":46646},{\"end\":47990,\"start\":47929},{\"end\":48330,\"start\":48258},{\"end\":48624,\"start\":48551},{\"end\":48975,\"start\":48917},{\"end\":49277,\"start\":49199},{\"end\":49557,\"start\":49487},{\"end\":49927,\"start\":49867},{\"end\":50308,\"start\":50239},{\"end\":50670,\"start\":50629},{\"end\":51012,\"start\":50959},{\"end\":51638,\"start\":51561},{\"end\":51937,\"start\":51863},{\"end\":52256,\"start\":52197},{\"end\":52512,\"start\":52468},{\"end\":52774,\"start\":52706}]", "bib_author": "[{\"end\":35864,\"start\":35840},{\"end\":35877,\"start\":35864},{\"end\":35894,\"start\":35877},{\"end\":36111,\"start\":36095},{\"end\":36126,\"start\":36111},{\"end\":36144,\"start\":36126},{\"end\":36160,\"start\":36144},{\"end\":36454,\"start\":36436},{\"end\":36467,\"start\":36454},{\"end\":36758,\"start\":36738},{\"end\":36778,\"start\":36758},{\"end\":36793,\"start\":36778},{\"end\":36809,\"start\":36793},{\"end\":37046,\"start\":37032},{\"end\":37058,\"start\":37046},{\"end\":37070,\"start\":37058},{\"end\":37080,\"start\":37070},{\"end\":37093,\"start\":37080},{\"end\":37106,\"start\":37093},{\"end\":37116,\"start\":37106},{\"end\":37129,\"start\":37116},{\"end\":37138,\"start\":37129},{\"end\":37147,\"start\":37138},{\"end\":37416,\"start\":37404},{\"end\":37427,\"start\":37416},{\"end\":37438,\"start\":37427},{\"end\":37697,\"start\":37681},{\"end\":37707,\"start\":37697},{\"end\":37720,\"start\":37707},{\"end\":37730,\"start\":37720},{\"end\":37743,\"start\":37730},{\"end\":37756,\"start\":37743},{\"end\":37768,\"start\":37756},{\"end\":37782,\"start\":37768},{\"end\":38065,\"start\":38049},{\"end\":38075,\"start\":38065},{\"end\":38085,\"start\":38075},{\"end\":38099,\"start\":38085},{\"end\":38112,\"start\":38099},{\"end\":38124,\"start\":38112},{\"end\":38138,\"start\":38124},{\"end\":38436,\"start\":38427},{\"end\":38449,\"start\":38436},{\"end\":38465,\"start\":38449},{\"end\":38478,\"start\":38465},{\"end\":38489,\"start\":38478},{\"end\":38725,\"start\":38714},{\"end\":38737,\"start\":38725},{\"end\":38754,\"start\":38737},{\"end\":38767,\"start\":38754},{\"end\":38994,\"start\":38983},{\"end\":39011,\"start\":38994},{\"end\":39023,\"start\":39011},{\"end\":39036,\"start\":39023},{\"end\":39284,\"start\":39271},{\"end\":39297,\"start\":39284},{\"end\":39312,\"start\":39297},{\"end\":39327,\"start\":39312},{\"end\":39339,\"start\":39327},{\"end\":39348,\"start\":39339},{\"end\":39359,\"start\":39348},{\"end\":39372,\"start\":39359},{\"end\":39698,\"start\":39678},{\"end\":39711,\"start\":39698},{\"end\":39733,\"start\":39711},{\"end\":39751,\"start\":39733},{\"end\":39765,\"start\":39751},{\"end\":39785,\"start\":39765},{\"end\":39803,\"start\":39785},{\"end\":39822,\"start\":39803},{\"end\":39837,\"start\":39822},{\"end\":39852,\"start\":39837},{\"end\":40273,\"start\":40257},{\"end\":40293,\"start\":40273},{\"end\":40311,\"start\":40293},{\"end\":40543,\"start\":40531},{\"end\":40558,\"start\":40543},{\"end\":40572,\"start\":40558},{\"end\":40582,\"start\":40572},{\"end\":40760,\"start\":40749},{\"end\":40772,\"start\":40760},{\"end\":40782,\"start\":40772},{\"end\":40802,\"start\":40782},{\"end\":40818,\"start\":40802},{\"end\":41036,\"start\":41021},{\"end\":41052,\"start\":41036},{\"end\":41068,\"start\":41052},{\"end\":41275,\"start\":41258},{\"end\":41285,\"start\":41275},{\"end\":41441,\"start\":41426},{\"end\":41451,\"start\":41441},{\"end\":41462,\"start\":41451},{\"end\":41471,\"start\":41462},{\"end\":41482,\"start\":41471},{\"end\":41700,\"start\":41688},{\"end\":41712,\"start\":41700},{\"end\":41726,\"start\":41712},{\"end\":41739,\"start\":41726},{\"end\":41750,\"start\":41739},{\"end\":41759,\"start\":41750},{\"end\":41770,\"start\":41759},{\"end\":42024,\"start\":42009},{\"end\":42038,\"start\":42024},{\"end\":42050,\"start\":42038},{\"end\":42061,\"start\":42050},{\"end\":42075,\"start\":42061},{\"end\":42089,\"start\":42075},{\"end\":42349,\"start\":42340},{\"end\":42363,\"start\":42349},{\"end\":42375,\"start\":42363},{\"end\":42389,\"start\":42375},{\"end\":42404,\"start\":42389},{\"end\":42661,\"start\":42653},{\"end\":42673,\"start\":42661},{\"end\":42682,\"start\":42673},{\"end\":42690,\"start\":42682},{\"end\":42702,\"start\":42690},{\"end\":42715,\"start\":42702},{\"end\":42728,\"start\":42715},{\"end\":42741,\"start\":42728},{\"end\":43008,\"start\":42991},{\"end\":43022,\"start\":43008},{\"end\":43190,\"start\":43173},{\"end\":43204,\"start\":43190},{\"end\":43393,\"start\":43384},{\"end\":43411,\"start\":43393},{\"end\":43422,\"start\":43411},{\"end\":43433,\"start\":43422},{\"end\":43447,\"start\":43433},{\"end\":43461,\"start\":43447},{\"end\":43472,\"start\":43461},{\"end\":43821,\"start\":43807},{\"end\":43839,\"start\":43821},{\"end\":43850,\"start\":43839},{\"end\":43866,\"start\":43850},{\"end\":44196,\"start\":44181},{\"end\":44206,\"start\":44196},{\"end\":44220,\"start\":44206},{\"end\":44236,\"start\":44220},{\"end\":44248,\"start\":44236},{\"end\":44268,\"start\":44248},{\"end\":44285,\"start\":44268},{\"end\":44557,\"start\":44546},{\"end\":44569,\"start\":44557},{\"end\":44582,\"start\":44569},{\"end\":44821,\"start\":44810},{\"end\":44833,\"start\":44821},{\"end\":44846,\"start\":44833},{\"end\":44860,\"start\":44846},{\"end\":44870,\"start\":44860},{\"end\":44886,\"start\":44870},{\"end\":44891,\"start\":44886},{\"end\":45189,\"start\":45180},{\"end\":45201,\"start\":45189},{\"end\":45212,\"start\":45201},{\"end\":45227,\"start\":45212},{\"end\":45242,\"start\":45227},{\"end\":45256,\"start\":45242},{\"end\":45270,\"start\":45256},{\"end\":45284,\"start\":45270},{\"end\":45298,\"start\":45284},{\"end\":45572,\"start\":45559},{\"end\":45583,\"start\":45572},{\"end\":45601,\"start\":45583},{\"end\":45617,\"start\":45601},{\"end\":45630,\"start\":45617},{\"end\":45646,\"start\":45630},{\"end\":45658,\"start\":45646},{\"end\":45675,\"start\":45658},{\"end\":45688,\"start\":45675},{\"end\":45700,\"start\":45688},{\"end\":45963,\"start\":45949},{\"end\":45976,\"start\":45963},{\"end\":46170,\"start\":46152},{\"end\":46187,\"start\":46170},{\"end\":46200,\"start\":46187},{\"end\":46446,\"start\":46437},{\"end\":46454,\"start\":46446},{\"end\":46466,\"start\":46454},{\"end\":46480,\"start\":46466},{\"end\":46487,\"start\":46480},{\"end\":46767,\"start\":46755},{\"end\":46783,\"start\":46767},{\"end\":46798,\"start\":46783},{\"end\":46813,\"start\":46798},{\"end\":46823,\"start\":46813},{\"end\":46835,\"start\":46823},{\"end\":46850,\"start\":46835},{\"end\":46866,\"start\":46850},{\"end\":46872,\"start\":46866},{\"end\":47215,\"start\":47201},{\"end\":47234,\"start\":47215},{\"end\":47248,\"start\":47234},{\"end\":47265,\"start\":47248},{\"end\":47276,\"start\":47265},{\"end\":47285,\"start\":47276},{\"end\":47299,\"start\":47285},{\"end\":47311,\"start\":47299},{\"end\":47325,\"start\":47311},{\"end\":47340,\"start\":47325},{\"end\":47678,\"start\":47664},{\"end\":47686,\"start\":47678},{\"end\":47697,\"start\":47686},{\"end\":47709,\"start\":47697},{\"end\":47719,\"start\":47709},{\"end\":47731,\"start\":47719},{\"end\":47740,\"start\":47731},{\"end\":48007,\"start\":47992},{\"end\":48021,\"start\":48007},{\"end\":48034,\"start\":48021},{\"end\":48048,\"start\":48034},{\"end\":48064,\"start\":48048},{\"end\":48077,\"start\":48064},{\"end\":48343,\"start\":48332},{\"end\":48357,\"start\":48343},{\"end\":48366,\"start\":48357},{\"end\":48381,\"start\":48366},{\"end\":48393,\"start\":48381},{\"end\":48640,\"start\":48626},{\"end\":48653,\"start\":48640},{\"end\":48670,\"start\":48653},{\"end\":48682,\"start\":48670},{\"end\":48692,\"start\":48682},{\"end\":48701,\"start\":48692},{\"end\":48715,\"start\":48701},{\"end\":48987,\"start\":48977},{\"end\":49001,\"start\":48987},{\"end\":49012,\"start\":49001},{\"end\":49024,\"start\":49012},{\"end\":49036,\"start\":49024},{\"end\":49044,\"start\":49036},{\"end\":49294,\"start\":49279},{\"end\":49305,\"start\":49294},{\"end\":49316,\"start\":49305},{\"end\":49327,\"start\":49316},{\"end\":49584,\"start\":49559},{\"end\":49598,\"start\":49584},{\"end\":49612,\"start\":49598},{\"end\":49619,\"start\":49612},{\"end\":49650,\"start\":49619},{\"end\":49656,\"start\":49650},{\"end\":49954,\"start\":49929},{\"end\":49968,\"start\":49954},{\"end\":49982,\"start\":49968},{\"end\":49989,\"start\":49982},{\"end\":50020,\"start\":49989},{\"end\":50031,\"start\":50020},{\"end\":50037,\"start\":50031},{\"end\":50335,\"start\":50310},{\"end\":50349,\"start\":50335},{\"end\":50363,\"start\":50349},{\"end\":50370,\"start\":50363},{\"end\":50401,\"start\":50370},{\"end\":50412,\"start\":50401},{\"end\":50418,\"start\":50412},{\"end\":50697,\"start\":50672},{\"end\":50711,\"start\":50697},{\"end\":50725,\"start\":50711},{\"end\":50732,\"start\":50725},{\"end\":50763,\"start\":50732},{\"end\":50774,\"start\":50763},{\"end\":50780,\"start\":50774},{\"end\":51027,\"start\":51014},{\"end\":51041,\"start\":51027},{\"end\":51056,\"start\":51041},{\"end\":51341,\"start\":51330},{\"end\":51351,\"start\":51341},{\"end\":51365,\"start\":51351},{\"end\":51376,\"start\":51365},{\"end\":51390,\"start\":51376},{\"end\":51404,\"start\":51390},{\"end\":51651,\"start\":51640},{\"end\":51665,\"start\":51651},{\"end\":51678,\"start\":51665},{\"end\":51689,\"start\":51678},{\"end\":51700,\"start\":51689},{\"end\":51952,\"start\":51939},{\"end\":51964,\"start\":51952},{\"end\":51972,\"start\":51964},{\"end\":51985,\"start\":51972},{\"end\":51999,\"start\":51985},{\"end\":52007,\"start\":51999},{\"end\":52271,\"start\":52258},{\"end\":52283,\"start\":52271},{\"end\":52291,\"start\":52283},{\"end\":52305,\"start\":52291},{\"end\":52313,\"start\":52305},{\"end\":52527,\"start\":52514},{\"end\":52540,\"start\":52527},{\"end\":52549,\"start\":52540},{\"end\":52563,\"start\":52549},{\"end\":52571,\"start\":52563},{\"end\":52792,\"start\":52776},{\"end\":52806,\"start\":52792},{\"end\":52820,\"start\":52806},{\"end\":52837,\"start\":52820},{\"end\":35864,\"start\":35840},{\"end\":35877,\"start\":35864},{\"end\":35894,\"start\":35877},{\"end\":36111,\"start\":36095},{\"end\":36126,\"start\":36111},{\"end\":36144,\"start\":36126},{\"end\":36160,\"start\":36144},{\"end\":36454,\"start\":36436},{\"end\":36467,\"start\":36454},{\"end\":36758,\"start\":36738},{\"end\":36778,\"start\":36758},{\"end\":36793,\"start\":36778},{\"end\":36809,\"start\":36793},{\"end\":37046,\"start\":37032},{\"end\":37058,\"start\":37046},{\"end\":37070,\"start\":37058},{\"end\":37080,\"start\":37070},{\"end\":37093,\"start\":37080},{\"end\":37106,\"start\":37093},{\"end\":37116,\"start\":37106},{\"end\":37129,\"start\":37116},{\"end\":37138,\"start\":37129},{\"end\":37147,\"start\":37138},{\"end\":37416,\"start\":37404},{\"end\":37427,\"start\":37416},{\"end\":37438,\"start\":37427},{\"end\":37697,\"start\":37681},{\"end\":37707,\"start\":37697},{\"end\":37720,\"start\":37707},{\"end\":37730,\"start\":37720},{\"end\":37743,\"start\":37730},{\"end\":37756,\"start\":37743},{\"end\":37768,\"start\":37756},{\"end\":37782,\"start\":37768},{\"end\":38065,\"start\":38049},{\"end\":38075,\"start\":38065},{\"end\":38085,\"start\":38075},{\"end\":38099,\"start\":38085},{\"end\":38112,\"start\":38099},{\"end\":38124,\"start\":38112},{\"end\":38138,\"start\":38124},{\"end\":38436,\"start\":38427},{\"end\":38449,\"start\":38436},{\"end\":38465,\"start\":38449},{\"end\":38478,\"start\":38465},{\"end\":38489,\"start\":38478},{\"end\":38725,\"start\":38714},{\"end\":38737,\"start\":38725},{\"end\":38754,\"start\":38737},{\"end\":38767,\"start\":38754},{\"end\":38994,\"start\":38983},{\"end\":39011,\"start\":38994},{\"end\":39023,\"start\":39011},{\"end\":39036,\"start\":39023},{\"end\":39284,\"start\":39271},{\"end\":39297,\"start\":39284},{\"end\":39312,\"start\":39297},{\"end\":39327,\"start\":39312},{\"end\":39339,\"start\":39327},{\"end\":39348,\"start\":39339},{\"end\":39359,\"start\":39348},{\"end\":39372,\"start\":39359},{\"end\":39698,\"start\":39678},{\"end\":39711,\"start\":39698},{\"end\":39733,\"start\":39711},{\"end\":39751,\"start\":39733},{\"end\":39765,\"start\":39751},{\"end\":39785,\"start\":39765},{\"end\":39803,\"start\":39785},{\"end\":39822,\"start\":39803},{\"end\":39837,\"start\":39822},{\"end\":39852,\"start\":39837},{\"end\":40273,\"start\":40257},{\"end\":40293,\"start\":40273},{\"end\":40311,\"start\":40293},{\"end\":40543,\"start\":40531},{\"end\":40558,\"start\":40543},{\"end\":40572,\"start\":40558},{\"end\":40582,\"start\":40572},{\"end\":40760,\"start\":40749},{\"end\":40772,\"start\":40760},{\"end\":40782,\"start\":40772},{\"end\":40802,\"start\":40782},{\"end\":40818,\"start\":40802},{\"end\":41036,\"start\":41021},{\"end\":41052,\"start\":41036},{\"end\":41068,\"start\":41052},{\"end\":41275,\"start\":41258},{\"end\":41285,\"start\":41275},{\"end\":41441,\"start\":41426},{\"end\":41451,\"start\":41441},{\"end\":41462,\"start\":41451},{\"end\":41471,\"start\":41462},{\"end\":41482,\"start\":41471},{\"end\":41700,\"start\":41688},{\"end\":41712,\"start\":41700},{\"end\":41726,\"start\":41712},{\"end\":41739,\"start\":41726},{\"end\":41750,\"start\":41739},{\"end\":41759,\"start\":41750},{\"end\":41770,\"start\":41759},{\"end\":42024,\"start\":42009},{\"end\":42038,\"start\":42024},{\"end\":42050,\"start\":42038},{\"end\":42061,\"start\":42050},{\"end\":42075,\"start\":42061},{\"end\":42089,\"start\":42075},{\"end\":42349,\"start\":42340},{\"end\":42363,\"start\":42349},{\"end\":42375,\"start\":42363},{\"end\":42389,\"start\":42375},{\"end\":42404,\"start\":42389},{\"end\":42661,\"start\":42653},{\"end\":42673,\"start\":42661},{\"end\":42682,\"start\":42673},{\"end\":42690,\"start\":42682},{\"end\":42702,\"start\":42690},{\"end\":42715,\"start\":42702},{\"end\":42728,\"start\":42715},{\"end\":42741,\"start\":42728},{\"end\":43008,\"start\":42991},{\"end\":43022,\"start\":43008},{\"end\":43190,\"start\":43173},{\"end\":43204,\"start\":43190},{\"end\":43393,\"start\":43384},{\"end\":43411,\"start\":43393},{\"end\":43422,\"start\":43411},{\"end\":43433,\"start\":43422},{\"end\":43447,\"start\":43433},{\"end\":43461,\"start\":43447},{\"end\":43472,\"start\":43461},{\"end\":43821,\"start\":43807},{\"end\":43839,\"start\":43821},{\"end\":43850,\"start\":43839},{\"end\":43866,\"start\":43850},{\"end\":44196,\"start\":44181},{\"end\":44206,\"start\":44196},{\"end\":44220,\"start\":44206},{\"end\":44236,\"start\":44220},{\"end\":44248,\"start\":44236},{\"end\":44268,\"start\":44248},{\"end\":44285,\"start\":44268},{\"end\":44557,\"start\":44546},{\"end\":44569,\"start\":44557},{\"end\":44582,\"start\":44569},{\"end\":44821,\"start\":44810},{\"end\":44833,\"start\":44821},{\"end\":44846,\"start\":44833},{\"end\":44860,\"start\":44846},{\"end\":44870,\"start\":44860},{\"end\":44886,\"start\":44870},{\"end\":44891,\"start\":44886},{\"end\":45189,\"start\":45180},{\"end\":45201,\"start\":45189},{\"end\":45212,\"start\":45201},{\"end\":45227,\"start\":45212},{\"end\":45242,\"start\":45227},{\"end\":45256,\"start\":45242},{\"end\":45270,\"start\":45256},{\"end\":45284,\"start\":45270},{\"end\":45298,\"start\":45284},{\"end\":45572,\"start\":45559},{\"end\":45583,\"start\":45572},{\"end\":45601,\"start\":45583},{\"end\":45617,\"start\":45601},{\"end\":45630,\"start\":45617},{\"end\":45646,\"start\":45630},{\"end\":45658,\"start\":45646},{\"end\":45675,\"start\":45658},{\"end\":45688,\"start\":45675},{\"end\":45700,\"start\":45688},{\"end\":45963,\"start\":45949},{\"end\":45976,\"start\":45963},{\"end\":46170,\"start\":46152},{\"end\":46187,\"start\":46170},{\"end\":46200,\"start\":46187},{\"end\":46446,\"start\":46437},{\"end\":46454,\"start\":46446},{\"end\":46466,\"start\":46454},{\"end\":46480,\"start\":46466},{\"end\":46487,\"start\":46480},{\"end\":46767,\"start\":46755},{\"end\":46783,\"start\":46767},{\"end\":46798,\"start\":46783},{\"end\":46813,\"start\":46798},{\"end\":46823,\"start\":46813},{\"end\":46835,\"start\":46823},{\"end\":46850,\"start\":46835},{\"end\":46866,\"start\":46850},{\"end\":46872,\"start\":46866},{\"end\":47215,\"start\":47201},{\"end\":47234,\"start\":47215},{\"end\":47248,\"start\":47234},{\"end\":47265,\"start\":47248},{\"end\":47276,\"start\":47265},{\"end\":47285,\"start\":47276},{\"end\":47299,\"start\":47285},{\"end\":47311,\"start\":47299},{\"end\":47325,\"start\":47311},{\"end\":47340,\"start\":47325},{\"end\":47678,\"start\":47664},{\"end\":47686,\"start\":47678},{\"end\":47697,\"start\":47686},{\"end\":47709,\"start\":47697},{\"end\":47719,\"start\":47709},{\"end\":47731,\"start\":47719},{\"end\":47740,\"start\":47731},{\"end\":48007,\"start\":47992},{\"end\":48021,\"start\":48007},{\"end\":48034,\"start\":48021},{\"end\":48048,\"start\":48034},{\"end\":48064,\"start\":48048},{\"end\":48077,\"start\":48064},{\"end\":48343,\"start\":48332},{\"end\":48357,\"start\":48343},{\"end\":48366,\"start\":48357},{\"end\":48381,\"start\":48366},{\"end\":48393,\"start\":48381},{\"end\":48640,\"start\":48626},{\"end\":48653,\"start\":48640},{\"end\":48670,\"start\":48653},{\"end\":48682,\"start\":48670},{\"end\":48692,\"start\":48682},{\"end\":48701,\"start\":48692},{\"end\":48715,\"start\":48701},{\"end\":48987,\"start\":48977},{\"end\":49001,\"start\":48987},{\"end\":49012,\"start\":49001},{\"end\":49024,\"start\":49012},{\"end\":49036,\"start\":49024},{\"end\":49044,\"start\":49036},{\"end\":49294,\"start\":49279},{\"end\":49305,\"start\":49294},{\"end\":49316,\"start\":49305},{\"end\":49327,\"start\":49316},{\"end\":49584,\"start\":49559},{\"end\":49598,\"start\":49584},{\"end\":49612,\"start\":49598},{\"end\":49619,\"start\":49612},{\"end\":49650,\"start\":49619},{\"end\":49656,\"start\":49650},{\"end\":49954,\"start\":49929},{\"end\":49968,\"start\":49954},{\"end\":49982,\"start\":49968},{\"end\":49989,\"start\":49982},{\"end\":50020,\"start\":49989},{\"end\":50031,\"start\":50020},{\"end\":50037,\"start\":50031},{\"end\":50335,\"start\":50310},{\"end\":50349,\"start\":50335},{\"end\":50363,\"start\":50349},{\"end\":50370,\"start\":50363},{\"end\":50401,\"start\":50370},{\"end\":50412,\"start\":50401},{\"end\":50418,\"start\":50412},{\"end\":50697,\"start\":50672},{\"end\":50711,\"start\":50697},{\"end\":50725,\"start\":50711},{\"end\":50732,\"start\":50725},{\"end\":50763,\"start\":50732},{\"end\":50774,\"start\":50763},{\"end\":50780,\"start\":50774},{\"end\":51027,\"start\":51014},{\"end\":51041,\"start\":51027},{\"end\":51056,\"start\":51041},{\"end\":51341,\"start\":51330},{\"end\":51351,\"start\":51341},{\"end\":51365,\"start\":51351},{\"end\":51376,\"start\":51365},{\"end\":51390,\"start\":51376},{\"end\":51404,\"start\":51390},{\"end\":51651,\"start\":51640},{\"end\":51665,\"start\":51651},{\"end\":51678,\"start\":51665},{\"end\":51689,\"start\":51678},{\"end\":51700,\"start\":51689},{\"end\":51952,\"start\":51939},{\"end\":51964,\"start\":51952},{\"end\":51972,\"start\":51964},{\"end\":51985,\"start\":51972},{\"end\":51999,\"start\":51985},{\"end\":52007,\"start\":51999},{\"end\":52271,\"start\":52258},{\"end\":52283,\"start\":52271},{\"end\":52291,\"start\":52283},{\"end\":52305,\"start\":52291},{\"end\":52313,\"start\":52305},{\"end\":52527,\"start\":52514},{\"end\":52540,\"start\":52527},{\"end\":52549,\"start\":52540},{\"end\":52563,\"start\":52549},{\"end\":52571,\"start\":52563},{\"end\":52792,\"start\":52776},{\"end\":52806,\"start\":52792},{\"end\":52820,\"start\":52806},{\"end\":52837,\"start\":52820}]", "bib_venue": "[{\"end\":35898,\"start\":35894},{\"end\":36093,\"start\":36033},{\"end\":36471,\"start\":36467},{\"end\":36813,\"start\":36809},{\"end\":37151,\"start\":37147},{\"end\":37402,\"start\":37345},{\"end\":37789,\"start\":37782},{\"end\":38047,\"start\":37991},{\"end\":38493,\"start\":38489},{\"end\":38771,\"start\":38767},{\"end\":39040,\"start\":39036},{\"end\":39385,\"start\":39372},{\"end\":39865,\"start\":39852},{\"end\":40255,\"start\":40150},{\"end\":40586,\"start\":40582},{\"end\":40822,\"start\":40818},{\"end\":41072,\"start\":41068},{\"end\":41289,\"start\":41285},{\"end\":41486,\"start\":41482},{\"end\":41774,\"start\":41770},{\"end\":42094,\"start\":42089},{\"end\":42409,\"start\":42404},{\"end\":42745,\"start\":42741},{\"end\":43026,\"start\":43022},{\"end\":43208,\"start\":43204},{\"end\":43475,\"start\":43472},{\"end\":43870,\"start\":43866},{\"end\":44179,\"start\":44093},{\"end\":44592,\"start\":44582},{\"end\":44904,\"start\":44891},{\"end\":45302,\"start\":45298},{\"end\":45557,\"start\":45521},{\"end\":45980,\"start\":45976},{\"end\":46206,\"start\":46200},{\"end\":46490,\"start\":46487},{\"end\":46876,\"start\":46872},{\"end\":47199,\"start\":47125},{\"end\":47662,\"start\":47586},{\"end\":48081,\"start\":48077},{\"end\":48396,\"start\":48393},{\"end\":48722,\"start\":48715},{\"end\":49048,\"start\":49044},{\"end\":49331,\"start\":49327},{\"end\":49660,\"start\":49656},{\"end\":50041,\"start\":50037},{\"end\":50422,\"start\":50418},{\"end\":50784,\"start\":50780},{\"end\":51088,\"start\":51056},{\"end\":51328,\"start\":51265},{\"end\":51703,\"start\":51700},{\"end\":52011,\"start\":52007},{\"end\":52317,\"start\":52313},{\"end\":52576,\"start\":52571},{\"end\":52850,\"start\":52837},{\"end\":35898,\"start\":35894},{\"end\":36093,\"start\":36033},{\"end\":36471,\"start\":36467},{\"end\":36813,\"start\":36809},{\"end\":37151,\"start\":37147},{\"end\":37402,\"start\":37345},{\"end\":37789,\"start\":37782},{\"end\":38047,\"start\":37991},{\"end\":38493,\"start\":38489},{\"end\":38771,\"start\":38767},{\"end\":39040,\"start\":39036},{\"end\":39385,\"start\":39372},{\"end\":39865,\"start\":39852},{\"end\":40255,\"start\":40150},{\"end\":40586,\"start\":40582},{\"end\":40822,\"start\":40818},{\"end\":41072,\"start\":41068},{\"end\":41289,\"start\":41285},{\"end\":41486,\"start\":41482},{\"end\":41774,\"start\":41770},{\"end\":42094,\"start\":42089},{\"end\":42409,\"start\":42404},{\"end\":42745,\"start\":42741},{\"end\":43026,\"start\":43022},{\"end\":43208,\"start\":43204},{\"end\":43475,\"start\":43472},{\"end\":43870,\"start\":43866},{\"end\":44179,\"start\":44093},{\"end\":44592,\"start\":44582},{\"end\":44904,\"start\":44891},{\"end\":45302,\"start\":45298},{\"end\":45557,\"start\":45521},{\"end\":45980,\"start\":45976},{\"end\":46206,\"start\":46200},{\"end\":46490,\"start\":46487},{\"end\":46876,\"start\":46872},{\"end\":47199,\"start\":47125},{\"end\":47662,\"start\":47586},{\"end\":48081,\"start\":48077},{\"end\":48396,\"start\":48393},{\"end\":48722,\"start\":48715},{\"end\":49048,\"start\":49044},{\"end\":49331,\"start\":49327},{\"end\":49660,\"start\":49656},{\"end\":50041,\"start\":50037},{\"end\":50422,\"start\":50418},{\"end\":50784,\"start\":50780},{\"end\":51088,\"start\":51056},{\"end\":51328,\"start\":51265},{\"end\":51703,\"start\":51700},{\"end\":52011,\"start\":52007},{\"end\":52317,\"start\":52313},{\"end\":52576,\"start\":52571},{\"end\":52850,\"start\":52837},{\"end\":51116,\"start\":51090},{\"end\":51116,\"start\":51090}]"}}}, "year": 2023, "month": 12, "day": 17}