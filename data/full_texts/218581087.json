{"id": 218581087, "updated": "2023-10-06 16:14:19.094", "metadata": {"title": "A Simple Semi-Supervised Learning Framework for Object Detection", "authors": "[{\"first\":\"Kihyuk\",\"last\":\"Sohn\",\"middle\":[]},{\"first\":\"Zizhao\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Chun-Liang\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Han\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Chen-Yu\",\"last\":\"Lee\",\"middle\":[]},{\"first\":\"Tomas\",\"last\":\"Pfister\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2020, "month": 5, "day": 10}, "abstract": "Semi-supervised learning (SSL) has promising potential for improving the predictive performance of machine learning models using unlabeled data. There has been remarkable progress, but the scope of demonstration in SSL has been limited to image classification tasks. In this paper, we propose STAC, a simple yet effective SSL framework for visual object detection along with a data augmentation strategy. STAC deploys highly confident pseudo labels of localized objects from an unlabeled image and updates the model by enforcing consistency via strong augmentations. We propose new experimental protocols to evaluate performance of semi-supervised object detection using MS-COCO and demonstrate the efficacy of STAC on both MS-COCO and VOC07. On VOC07, STAC improves the AP$^{0.5}$ from 76.30 to 79.08; on MS-COCO, STAC demonstrates 2x higher data efficiency by achieving 24.38 mAP using only 5% labeled data than supervised baseline that marks 23.86% using 10% labeled data. The code is available at \\url{https://github.com/google-research/ssl_detection/}.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2005.04757", "mag": "3021542222", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2005-04757", "doi": null}}, "content": {"source": {"pdf_hash": "611439578bde943bbe67a65a5489de6da58a7d69", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2005.04757v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "0db99d0b99c8d378829a935bf09d4dc07225584f", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/611439578bde943bbe67a65a5489de6da58a7d69.txt", "contents": "\nA Simple Semi-Supervised Learning Framework for Object Detection\n\n\nKihyuk Sohn ksohn@google.com \nGoogle Cloud AI\nGoogle Brain\n\nZizhao Zhang zizhaoz@google.com \nGoogle Cloud AI\nGoogle Brain\n\nChun-Liang Li chunliang@google.com \nGoogle Cloud AI\nGoogle Brain\n\nHan Zhang zhanghan@google.com \nGoogle Cloud AI\nGoogle Brain\n\nChen-Yu Lee chenyulee@google.com \nGoogle Cloud AI\nGoogle Brain\n\nTomas Pfister tpfister@google.com \nGoogle Cloud AI\nGoogle Brain\n\nA Simple Semi-Supervised Learning Framework for Object Detection\nSemi-supervised learningobject detectionself-trainingaug- mentation consistency\nSemi-supervised learning (SSL) has promising potential for improving the predictive performance of machine learning models using unlabeled data. There has been remarkable progress, but the scope of demonstration in SSL has been limited to image classification tasks. In this paper, we propose STAC, a simple yet effective SSL framework for visual object detection along with a data augmentation strategy. STAC deploys highly confident pseudo labels of localized objects from an unlabeled image and updates the model by enforcing consistency via strong augmentations. We propose new experimental protocols to evaluate performance of semi-supervised object detection using MS-COCO and demonstrate the efficacy of STAC on both MS-COCO and VOC07. On VOC07, STAC improves the AP 0.5 from 76.30 to 79.08; on MS-COCO, STAC demonstrates 2\u00d7 higher data efficiency by achieving 24.38 mAP using only 5% labeled data than supervised baseline that marks 23.86% using 10% labeled data. The code is available at https://github.com/google-research/ssl_detection/.\n\nIntroduction\n\nSemi-supervised learning (SSL) has received growing attention in recent years as it provides means of using unlabeled data to improve model performance when large-scale annotated data is not available. A popular class of SSL methods is based on Consistency-based Self-Training [27,38,25,44,54,35,4,58,3,59,49].\n\nThe key idea is to first generate the artificial labels for the unlabeled data and train the model to predict these artificial labels when feeding the unlabeled data with semanticity-preserving stochastic augmentations. The artificial label can either be a one-hot prediction (hard) or the model's predictive distribution (soft). The other pillar for the success of SSL is from advancements in data augmentations. Data augmentations improve the robustness of deep neural networks [48,24] and has been shown to be particularly effective for consistencybased self-training [58,3,59,49]. The augmentation strategy spans from a manual  The proposed semi-supervised learning framework for object detection, STAC, consistently improves upon supervised baselines and those with data augmentation using different amount of labeled training data on MS-COCO [30].\n\ncombination of basic image transformations, such as rotation, translation, flipping, or color jittering, to neural image synthesis [68,21,62] and policies learned by reinforcement learning [6,69]. Lately, complex data augmentation strategies, such as RandAugment [7] or CTAugment [3], have turned out to be powerful for SSL on image classification [58,3,49,59].\n\nWhile having made remarkable progress, SSL methods have been mostly applied to image classification, whose labeling cost is relatively cheaper compared to other important problems in computer vision, such as object detection. Due to its expensive labeling cost, object detection demands a higher level of label efficiency, necessitating the development of strong SSL methods. On the other hand, the majority of existing works on object detection has focused on training a stronger [47, 28,29] and faster [14,41,8] detector given sufficient amount of annotated data, such as MS-COCO [30]. Few existing works on SSL for object detection [53, 34,43] rely on additional context, such as categorical similarities of objects or temporal consistency from video.\n\nIn this work, we leverage lessons learned from deep SSL on image classification to tackle SSL for object detection. To this end, we propose a SSL framework for object detection that combines self-training (via pseudo label) [46,33] and consistency regularization based on the strong data augmentations [6,7,69]. Inspired by the framework in Noisy-Student [59], our system contains two stages of training. In the first stage, we train an object detector (e.g., Faster RCNN [41]) using all labeled data until convergence. The trained detector is then used to predict bounding boxes and class labels of localized objects for unlabeled images as shown in Figure 2. Then, we apply confidence-based box filtering to each predicted box (after non-maximum suppression) with high threshold value to obtain pseudo labels with high precision, inspired by the design of FixMatch [49]. In the second stage, the strong data augmentations are applied to each unlabeled image and the model is trained with labeled data and unlabeled data with its corresponding pseudo labels generated in the first stage. Encouraged by RandAugment [7] and its successful adaptation to SSL [58,49] and object The proposed SSL framework for object detection. We generate pseudo labels (i.e., bounding boxes and their class labels) for unlabeled data using testtime inference, including non-maximum suppression (NMS) [15], of the teacher model trained with labeled data. We compute unsupervised loss with respect to pseudo labels whose confidence scores are above a threshold \u03c4 . The strong augmentations are applied for augmentation consistency during the model training.\n\nTarget boxes are augmented when global geometric transformations are used.\n\ndetection [69], we design our augmentation strategy for object detection, which consists of global color transformation, global or box-level [69] geometric transformations, and Cutout [10]. We test the efficacy of STAC on public datasets: MS-COCO [30] and PAS-CAL VOC [13]. We design new experimental protocols using MS-COCO dataset to evaluate the semi-supervised performance of object detection. We use 1, 2, 5 and 10% of labeled data as labeled sets and the remainder as unlabeled sets to evaluate the effectiveness of SSL methods in the low-label regime. In addition, following [37,52], we evaluate using all labeled data as the labeled set and additional unlabeled data provided by MS-COCO as the unlabeled set. Following [23], we use trainval of VOC07 as the labeled set and that of VOC12 with or without unlabeled data of MS-COCO as unlabeled sets. While being simple, STAC brings significant gain in mAPs: 18.47 to 24.38 on 5% protocol, 23.86 to 28.64 on 10% protocol as in Figure 1, and 42.60 to 46.01 on PASCAL VOC.\n\nThe contribution of this paper is as follows:\n\n1. We develop STAC, a SSL framework for object detection that seamlessly extends the class of state-of-the-art SSL methods for classification based on self-training and augmentation-driven consistency regularization.\n\n2. STAC is simple and introduces only two new hyperparameters: the confidence threshold \u03c4 and the unsupervised loss weight \u03bb u , which do not require an extensive additional effort for tuning.\n\n3. We propose new experimental protocols for SSL object detection using MS-COCO and demonstrate the efficacy of STAC on MS-COCO and PASCAL VOC in Faster RCNN framework.\n\n\nRelated Work\n\nObject detection is a fundamental computer vision task and has been extensively studied in the literature [15,14,41,17,28,5,39,40,31,29]. Popular object detection frameworks include Region-based CNN (RCNN) [15,14,41,17,28], YOLO [39,40], SSD [31], etc [26,51,11]. The progress made by existing works is mainly on training a stronger or faster object detector given sufficient amount of annotated data. There is growing interest in improving detectors using unlabeled training data through a semi-supervised object detection framework [53,34]. Before deep learning, the idea has been explored by [42]. Recently, [23] proposes a consistency-based semi-supervised object detection method, which enforces the consistent prediction of an unlabeled image and its flipped counterpart. Their method requires a more sophisticated Jensen-Shannon Divergence for consistency regularization computation. Similar ideas to consistency regularization have also been studied in the active learning settings for object detection [55].\n\n[52] introduces a self-supervised proposal learning module to learn context-aware and noise-robust proposal features from unlabeled data.\n\n[37] proposes data distillation that generates labels by ensembling predictions of multiple transformations of unlabeled data. We argue that stronger semi-supervised detectors require further investigation of unsupervised objectives and data augmentations. Semi-supervised learning (SSL) for image classification has been dramatically improved recently. Consistency regularization becomes one of the popular approaches among recent methods [2,45,25,63,58] and inspires [23] on object detection. The idea is to enforce the model to generate consistent predictions across label-preserving data augmentations. Some exemplars include Mean-Teacher [54], UDA [58], and MixMatch [4]. Another popular class of SSL is pseudo labeling [27,2], which can be viewed as a hard version of consistency regularization: the model is performing self-training to generate pseudo labels of unlabeled data and thereby train randomly-augmented unlabeled data to match the respective pseudo labels (i.e. being consistent in predictions of the same unlabeled example). How to use pseudo labels is critical to the success of SSL. For instance, Noisy-Student [59] demonstrates an iterative teacher-student framework that repeats the process of labeling assignments using a teacher model and then training a larger student model. This method achieves state-of-the-art performance on ImageNet classification by leveraging extra unlabeled images in the wild. FixMatch [49] demonstrates a simple algorithm which outperforms previous approaches and establishes state-of-the-art performance, especially on diverse small labeled data regimes. The key idea behind FixMatch is matching the prediction of the strongly-augmented unlabeled data to the pseudo label of the weakly-augmented counterpart when the model confidence on the weaklyaugmented one is high. In light of the success of these methods, this paper exploits the effective usage of pseudo labeling and pseudo boxes as well as data augmentations to improve object detectors. Data augmentations are critical to improve model generalization and robustness [6,7,19,69,64,67,10,12,20], especially gradually become a major impetus on semi-supervised learning [4,3,58,49]. Finding appropriate color transformations and geometric transformations of input spaces has been shown to be critical to improve generalization [6,19]. However, most augmentations are mainly studied in image classification. The complexity of data augmentations for object detection is much higher than image classification [69], since global geometric transformations of data affect bounding box annotations. Some works have presented augmentation techniques for supervised object detection, such as MixUp [64,65], CutMix [61], or augmentation strategy learning [69]. The recent consistencybased SSL object detection method [23] utilizes global horizontal flipping (weak augmentation) to construct the consistency loss. To the best of our knowledge, the impact of intensive data augmentations on semi-supervised object detection has not been well studied.\n\n\nMethodology\n\n\nBackground: Unsupervised Loss in SSL\n\nFormulating an unsupervised loss that leverages unlabeled data is the key in SSL. Many advancements in SSL for classification rely on some forms of consistency regularization [27,38,25,44,54,35,4,58,3,59,49]. Inspired by a comparison in [49], we provide a unified view of consistency regularization for image classification. For K-way classification, the consistency regularization is written as follows:\nu = x\u2208X w(x) (q(x), p(x; \u03b8))(1)\nwhere x \u2208 X is an image, p, q : X \u2192 [0, 1] K map x into a (K\u22121)-simplex, and w : X \u2192 {0, 1} maps x into a binary value. (\u00b7, \u00b7) measures a distance between two vectors. Typical choices include L 2 distance and cross entropy. Here, p represents the prediction of the model parameterized by \u03b8, q is the prediction target, and w is the weight that determines the contribution of x to the loss. As an example, pseudo labeling [27] has the following configurations:\n\nq(x) = one hot(arg max (p(x; \u03b8))), w(x) = 1 if max(p(x; \u03b8)) \u2265 \u03c4\n\nWe refer readers to the supplementary material for configurations of a comprehensive list of SSL methods. State-of-the-art SSL algorithms, such as Unsupervised Data Augmentation (UDA) [58] or FixMatch [49], apply strong data augmentation A, such as RandAugment [58] or CTAugment [3], to the model prediction p(A(x); \u03b8) for improved robustness. Noisy-Student [59] applies diverse forms of stochastic noise to the model prediction, including input augmentations via RandAugment, and network augmentations via dropout [50] and stochastic depth [22]. While sharing similarities on the model prediction, they differ in q that generates the prediction target as detailed in Appendix C. Besides the use of soft or hard targets, Different from (2) and many aforementioned algorithms, Noisy-Student employs a \"teacher\" network other than p(\u00b7, \u03b8) to generates pseudo labels q(x). Note that the teacher network is independent of the model at training and this gives a scalability and flexibility on the choice of network architectures or optimization such as learning schedules.\n\n\nSTAC: SSL for Object Detection\n\nWe develop a novel SSL framework for object detection, called STAC, based on the Self-Training (via pseudo label) and the Augmentation driven Consistency regularization. First, we adopt a stage-wise training of Noisy-Student [59] for its scalability and flexibility. This involves at least two stages of training, where in the first stage, we train a teacher model using all available labeled data, and in the second stage, we train STAC using both labeled and unlabeled data. Second, we use a threshold with a high value for the confidence-based thresholding inspired by FixMatch [49] to control the quality of pseudo labels in object detection, which is comprised of bounding boxes and their class labels. The steps for training STAC are summarized as follows:\n\n1. Train a teacher model on available labeled images. 2. Generate pseudo labels of unlabeled images (i.e., bounding boxes and their class labels) using the trained teacher model. 3. Apply strong data augmentations to unlabeled images, and augment pseudo labels (i.e. bounding boxes) correspondingly when global geometric transformations are applied. 4. Compute unsupervised loss and supervised loss to train a detector.\n\nTraining a Teacher Model. We develop our formulation based on the Faster RCNN [41] as it has been one of the most representative detection framework. Faster RCNN has a classifier (CLS) and a region proposal network (RPN) heads on top of the shared backbone network. Each head has two modules, namely region classifiers (e.g., a (K+1)-way classifier for the CLS head or a binary classifier for the RPN head) and bounding box regressors. We present the supervised and unsupervised losses of the Faster RCNN for the RPN head for simplicity. The supervised loss is written as follows:\ns (x, p * , t * ) = b s,b x, p * b , t * b = b 1 N cls i L cls p i , p * i,b + \u03bb N reg i p * i,b L reg (t i , t * b )(3)\nwhere b is an index of the ground-truth bounding box and i is an index of an anchor. p i is the predictive probability of an anchor being positive, t i is the 4dimensional coordinates of an anchor. p * i,b is the binary label of an anchor with respect to the box b, t * b is the ground-truth box coordinates of the box b. To train an RPN, p * i,b needs to be determined for all anchors, box pairs. Note that we define a loss per box for presentation clarity, which is slightly different from that in [41].\n\nGenerating Pseudo Labels. We perform a test-time inference of the object detector from the teacher model to generate pseudo labels. That being said, the pseudo label generation involves not only the forward pass of the backbone, RPN and CLS networks, but also the post-processing such as non-maximum suppression (NMS). This is different from conventional approaches for classification where the confidence score is computed from the raw predictive probability. We use the score of each returned bounding box after NMS, which aggregates the prediction probabilities of anchor boxes. Using box predictions after NMS has an advantage over using raw predictions (before NMS) since it removes repetitive detection. However, this does not filter out boxes at wrong locations as visualized in Figure 2 and Figure 5a. Unsupervised Loss. When given an unlabeled image x and set of predicted bounding boxes and their confidence scores, we determine q * i,b , a binary label of an anchor i with respect to the pseudo box b, for all anchor, box pairs. Let s * b be the box coordinates of pseudo box b. The unsupervised loss of STAC is written as follows:\nu (x, q * , s * ) = b w b (x) u,b (x, q * b , s * ) = b w b (x) 1 N cls i L cls p i , q * i,b + \u03bb N reg i q * i,b L reg (t i , s * b ) (4)\nwhere w b (x) = 1 if the confidence score of the predicted box b is higher than the threshold value \u03c4 and 0 otherwise. Decomposing the loss formulation of the Faster RCNN into a sum of losses for individual boxes makes the conversion from classification (Equation (1)) to detection (Equation (4)) much more transparent. Also note that the unsupervised loss is masked per box instead of per image, which is well aligned with our intuition. Overall, the RPN is trained by jointly minimizing two losses as follows:\n= s (x s , p * , t * ) + \u03bb u u (A(x u , s * ), q * )(5)\nwhere A is a strong data augmentation applied to an unlabeled image. Since some transformation operations are not invariant to the box coordinates (e.g., global geometric transformation [69]), the augmentation operator A is applied on the pseudo box coordinates s * as well.\n\nThe loss formulation of STAC introduces two hyperparameters \u03c4 and \u03bb u . In the experiments, we find that \u03c4 = 0.9 and \u03bb u \u2208 [1, 2] work well. Note that the consistency-based SSL object detection method in [23] requires sophisticated three-staged weighting schedule of \u03bb u that includes temporal ramp-up and rampdown. On the contrary, our system demonstrates effective performance with a simple constant weighting schedule because our framework enforces the consistency using a strong data augmentation strategy. Data Augmentation Strategy The key factor for the success of consistencybased SSL methods, such as UDA [58] or FixMatch [49], is a strong data augmentation. While the augmentation strategy for supervised and semi-supervised image classification has been extensively studied [6,7,3,58,49], not much effort has been made yet for object detection. We extend the RandAugment for object detection used in [6] using the augmentation search space recently proposed by [69] (e.g., box-level transformation) along with the Cutout [10]. For completeness, we describe the list of transformation operations below. Each operation has a magnitude that decides the augmentation degree of strength. 1 1. Global color transformation (C): Color transformation operations in [7] and the suggested ranges of magnitude for each op are used. 2. Global geometric transformation (G): Geometric transformation operations in [7], namely, x-y translation, rotation, and x-y shear, are used. For each image, we apply transformation operations in sequence as follows. First, we apply one of the operations sampled from C. Second, we apply one of the operations sampled from either G or B. Finally, we apply Cutout at multiple random locations 4 of a whole image to prevent a trivial solution when applied exclusively inside the bounding box. We visualize transformed images with aforementioned augmentation strategies in Figure 3.\n\n\nExperiments\n\nWe test the efficacy of our proposed method on MS-COCO [30], which is one of the most popular public benchmarks for object detection. MS-COCO contains more than 118k labeled images and 850k labeled object instances from 80 object categories for training. In addition, there are 123k unlabeled images that can be used for semi-supervised learning. We experiment two SSL settings. First, we randomly sample 1, 2, 5 and 10% of labeled training data as a labeled set and use the rest of labeled training data as an unlabeled set. For these experiments, we create 5 data folds. 1% protocol contains approximately 1.2k labeled images randomly selected from the labeled set of MS-COCO. 2% protocol contains additional \u223c1.2k images and 5, 10% protocol datasets are constructed in a similar way. Second, following [52], we use an entire labeled training data as a labeled set and additional unlabeled data as an unlabeled set. Note that the first protocol tests the efficacy of STAC when only few labeled examples are available, while the second protocol evaluates the potential to improve the state-of-the-art object detector with unlabeled data in addition to already a large-scale labeled data. We report the mAP over 80 classes. We also test on PASCAL VOC [13] following [23]. The trainval set of VOC07, containing 5,011 images from 20 object categories, is used as a labeled training data, and 11,540 images from the trainval set of VOC12 are used for an unlabeled training data. The detection performance is evaluated on the test set of VOC07 and mAP at IoU of 0.5 (AP 0.5 ) is reported in addition to the MS-COCO metric.\n\n\nImplementation Details\n\nOur implementation is based on the Faster RCNN and FPN library of Tensorpack [56]. We use ResNet-50 [18] backbone for our object detector models. Unless otherwise stated, the network weights are initialized by the ImageNet-pretrained model 5 at all stages of training.\n\nSince the training of the object detector is quite involved, we stay with the default learning settings for all our experiments other than the learning schedule. Most of our experiments are conducted using the quick learning schedule 6 with an exception for 100% MS-COCO protocol. 7 We find that the model's performance is benefited significantly by longer training when more labeled training data and more complex data augmentation strategies are used. STAC introduces two new hyperparameters \u03c4 for the confidence threshold and \u03bb u for the unsupervised loss. We use \u03c4 = 0.9 and \u03bb u = 2 for all experiments except for the 100% protocol of MS-COCO, where we lower threshold \u03c4 = 0.5 to increase the recall of pseudo labels. We refer readers to Appendix A for complete learning settings.  Table 2: Comparison in mAPs for different methods on VOC07. We report both mAPs at IoU=0.5:0.95, a standard metric for MS-COCO, as well as at IoU=0.5 (AP 0.5 ), since AP 0.5 is a saturated metric as pointed out by [5]. For STAC, we follow [23] to have different level of unlabeled sources, including VOC12 and the subset of MS-COCO data with the same classes as PASCAL VOC.\n\n\nResults\n\nSince deep semi-supervised learning of visual object detectors has not been widely studied yet, we mainly compare STAC with the supervised models (i.e., models trained with labeled data only) for various experimental protocols using different data augmentation strategies. Table 1 summarizes the results. For 1, 2, 5 and 10% protocols, we train models with a quick learning schedule and report mAPs averaged over 5 data folds and their standard deviation. For 100% protocol, we employ standard with 3\u00d7 longer learning schedule and report a single mAP value for each model. Firstly, we confirm the findings of [7] with varying amount of labeled training data that the RandAugment improves the supervised learning performance of a detector by a significant margin, 2.71 mAP at 5% protocol, 2.32 mAP at 10% protocol, and 1.85 mAP for 100% protocol, upon the supervised baselines with default data augmentation of resizing and horizontal flipping.\n\nSTAC further improves the performance upon stronger supervised models. We find it to be particularly effective for protocols with small labeled training data, showing 5.91 mAP improvement at 5% protocol and 4.78 mAP at 10% protocol. Interestingly, STAC is proven to be at least 2\u00d7 more data efficient than the baseline models for both 5% (24.36 for STAC v.s. 23.86 for supervised model with 10% labeled training data) and 10% protocols (28.56 for STAC v.s.  . We hypothesize that the pseudo label training benefits by a larger amount of unlabeled data relative to the size of labeled data and study its effectiveness with respect to the scale of unlabeled data in Section 5.\n\nWe have a similar finding for experiments on PASCAL VOC. In Table 2, the mAP of the supervised models increases from 42.6 to 43.4, and AP 0.5 increases from 76.30 to 78.21. A large-scale unlabeled data from VOC12 and MS-COCO further improves the performance, achieving 46.01 mAP and 79.08 AP 0.5 .\n\n\nAblation Study\n\nWe perform ablation study on the key components of STAC. The study analyzes the impact on the detector performance of 1) different data augmentations and learning schedule strategies, 2) different sizes of unlabeled sets, 3) the hyperparameters \u03bb u , coefficient for unsupervised loss, and \u03c4 , confidence threshold, and 4) quality of pseudo labels and their impact on the proposed STAC.\n\n\nData Augmentation and Learning Schedule\n\nIn this section, we evaluate the performance of supervised detector models with different data augmentation strategies and learning rate schedules while varying the amount of training data. We consider different combinations of augmentation modules, including the default augmentations of horizontal image flip, color only (C), color followed by geometric or box-level transforms (C+{G,B}), and the one followed by Cutout (C+{G,B}+Cutout). For {G,B}, we sample randomly and uniformly between geometric and box-level transform modules for each image. We consider different learning schedules, including quick, standard, and standard [n]\u00d7 (standard setting with [n] times longer training). While the number of weight updates are the same, the quick schedule uses lower resolution image as an input and smaller batch size for training.\n\nThe summary results are provided in Table 3. With small amount of labeled training data, we observe an increasing positive impact on detector performance with more complex (thus stronger) augmentation strategies. The trend holds true with the standard schedule, but we find that the quick schedule is beneficial on the low-labeled data regime due to its fast training and less issue of overfitting. On the other hand, we observe that the network significantly underfits with our augmentation strategies when all labeled data is used for training. For example, with 100% labeled data, we achieve even lower mAP of 36.12 with C+{G,B}+Cutout strategy than that of 37.42 with default augmentations. We find that the issue can be alleviated by longer training. Moreover, while the performance with default augmentations saturates and starts to decrease as it is trained longer, the models with strong data augmentation start to outperform, demonstrating their effectiveness on training with large-scale labeled data.\n\nSTAC contains two key components: self-training and strong data augmentation. We also verify the importance of data augmentation in , which is in line with recent findings in SSL for image classification [49]. We evaluate the performance of STAC with the default augmentations (horizontal flip). On a single fold of 10% protocol, we observe a good improvement in mAP upon baseline model (from 24.05 to 26.27), but the gain is not as significant as STAC (29.00). On 100% protocol, we observe slight decrease in performance when trained with self-training only (from 37.63 to 37.57), while STAC achieves 39.21 in mAP.\n\n\nSize of Unlabeled Data\n\nWhile the importance of large-scale labeled data for supervised learning has been broadly studied and emphasized [9,57,30], the importance on the scale of unlabeled data for semi-supervised learning has been often overlooked [36]. In this study, we highlight the importance of large-scale unlabeled data in the context of semi-supervised object detector learning. We experiment with 5% and 10% labeled data of MS-COCO while varying the amount of unlabeled data from 1, 2, 4, and 8 times to that of the labeled data.\n\nThe summary results are given in Table 4. While there still exists the improvement in mAPs when STAC is trained with small amount of unlabeled data, the gain is less significant compared to that of supervised model with strong data augmentation. We observe clearly from Table 4 that STAC benefits from the larger amount of unlabeled training data. We make a similar observation from experiments on PASCAL VOC in Table 2, where the AP 0.5 of STAC trained using trainval of VOC12 as unlabeled data achieves 77.45, which is lower than that of supervised model with strong augmentations (78.21). On the other hand, STAC trained with large amount of unlabeled data by combining VOC12 and MS-COCO achieves 79.08 AP 0.5 . This analysis may explain the slightly lower mAP of STAC for 100% protocol of MS-COCO than that of the supervised model with strong data augmentation since the size of available unlabeled data is roughly the same as that of the labeled data.\n\n\nHyperparameters \u03bb u and \u03c4\n\nWe study the impact of \u03bb u , a regularization coefficient for unsupervised loss, and \u03c4 , the confidence threshold. Specifically, we test the STAC with different  Table 4: mAPs of STAC trained with varying amount of unlabeled data.\n\n[n]\u00d7 refers that the amount of unlabeled data is [n] times larger than that of labeled data. We test on a single fold of 5% and 10% protocols.  Figure 4. Firstly, the best performance of STAC is obtained when \u03bb u = 2 and \u03c4 = 0.9. We observe that the performance of STAC deteriorates when \u03bb u is too large (> 2) or too small (< 0.5), but it improves upon strong baseline consistently for \u03bb u \u2208 [1,2]. When there is no confidence-based box filtering, the gain of STAC, if any, is marginal over the strong baseline. This is because lots of predicted boxes are indeed inaccurate, as shown in Figure 5a. Using larger value of \u03c4 allows to have pseudo box labels with higher precision (i.e., remaining boxes whose confidence is higher than \u03c4 are accurate), as in Figure 5e. However, if \u03c4 becomes too large, one would get a lower recall (e.g., bounding box at sofa in Figure 5c is filtered out in Figure 5d). Figure 4 shows that the high precision (i.e., larger value of \u03c4 ) is preferred to high recall (i.e., smaller value of \u03c4 ) on 10% protocol.\n\n\nQuality of Pseudo Labels\n\nOne intriguing question is whether the semi-supervised performance of the model improves with pseudo labels of higher quality. To validate the hypothesis, we train two additional STAC models for 10% protocol, where models are provided pseudo labels predicted by two different supervised models trained with 5% and 100% labeled data, whose mAPs are 18.67 and 37.63, respectively. Note that the STAC on 10% protocol achieves 29.00 mAP. STAC trained with less accurate pseudo labels achieves only 24.25 mAP, while the one with more accurate pseudo labels achieves 30.30 mAP, confirming the importance of pseudo label quality. Inspired by this observation, we increase the augmentation strength to train the teacher model in order to get better pseudo labels, expecting a further im-\n(a) \u03c4 = 0 (b) \u03c4 = 0.3 (c) \u03c4 = 0.5 (d) \u03c4 = 0.7 (e) \u03c4 = 0.9\nFig. 5: Visualization of predicted bounding boxes whose confidences are larger than \u03c4 for unlabeled data. Larger value of \u03c4 results in higher precision (e.g., remaining boxes after thresholding detect objects accurately), but lower recall (e.g., detected box at sofa is removed when \u03c4 \u2265 0.7).  Table 5: mAPs of supervised models and STAC tested on a single fold of 5% and 10% protocols. We first train supervised models with different augmentation strategies (first row of each protocol), and pseudo labels generated form each supervised model are used to train STAC models (second row of each protocol) accordingly.\n\nprovement for STAC. To this end, we train STAC using different sets of pseudo labels that are provided by the supervised models trained with different data augmentation schemes. As in Table 5, the performance of supervised models vary from mAP of 18.67 to 21.16 with 5% labeled data and from 24.05 to 26.34 with 10% labeled data. We observe an improvement in mAP by using more accurate pseudo labels on 5% protocol, but the gain is not as substantial. We also do not observe a clear correlation between the accuracy of pseudo label and the performance of STAC on 10% protocol. While STAC brings a significant gain in mAP using pseudo labels, our results suggest that the incremental improvement on the quality of pseudo labels may not bring in a significant extra benefit.\n\n\nDiscussion and Conclusion\n\nWhile SSL for classification has made significant strides, not much effort has been put to date for detection that demands a higher level of label-efficient training. We propose a simple (introducing only two hyperparameters that are easy to tune) and effective (2\u00d7 label efficiency in low-label regime) SSL framework for object detection by leveraging lessons from SSL methods for classification. The simplicity of our method will provide a flexibility for further development towards solving SSL for object detection. The proposed framework is amenable to many variations, including using soft labels for classification loss, other detector frameworks than Faster RCNN, and other data augmentation strategies. While STAC demonstrates an impressive performance gain already without taking confirmation bias [66,1] issue into account, it could be problematic when using a detection framework with a stronger form of hard negative mining [47,29] because noisy pseudo labels can be overlyused. Further investigation in learning with noisy labels, confidence calibration, and uncertainty estimation in the context of object detection are few important topics to further enhance the performance of SSL object detection. \n\n\nA Learning Schedules\n\nIn this section, we provide complete descriptions on different learning schedules used in our experiments. Note that the schedule VOC is only used for experiments related to PASCAL VOC. Besides specified below, we adopt the learning settings as follows \n\n\nB Data Augmentation in STAC\n\nThis section provides more comprehensive results of Section 5.1 to validate the importance of data augmentation in STAC. In Table 6, we provide two rows of results with STAC (bottom) and the STAC without strong data augmentation, i.e., \"Self-Training\". We observe significant gain in mAP on all cases, which validates the importance of the data augmentation in STAC.\n\n\nMethods\n\n5% COCO 10% COCO 100%  Table 6: Comparison in mAPs for different SSL methods on MS-COCO. We report the mean and standard deviation over 5 data folds for 5% and 10% protocols. \"Self-Training\" refers to STAC but without strong data augmentation on unlabeled data. We train STAC with the strong augmentation for unlabeled data.\n\n\nC Extended Background: Unsupervised Loss in SSL\n\nIn this section, we extend Section 3.1 and provide unsupervised loss formulations for comprehensive list of SSL algorithms whose loss can be represented in Equation (1). For presentation clarity, let us reiterate definitions as follows:\nu = x\u2208X w(x) (q(x), p(x))(6)\nHere, we use p(x) instead of p(x; \u03b8) as in Equation (1) for generality. Instead, let us denote p(x; \u03b8) as a prediction of the model with parameters \u03b8 at training. Note that the unsupervised loss formulation of STAC is following the form of Noisy Student (Section C.9), which can be viewed as a combination of Self-Training (Section C.1) and strong data augmentation. While we have shown such a simple formulation of STAC brings in a significant performance gain at object detection, more complicated formulations (e.g., Mean Teacher (Section C.5) or MixMatch/ReMixMatch (Section C.10)) are amenable to be used in place of several design choices made for STAC. Further investigation of STAC variants is in the scope of the future work.\n\nC.1 Bootstrapping (a.k.a. Self-Training) [60,32] (q, p) = H(q, p)\n\nw(x) = 1 if max(p(x;\u03b8)) \u2265 \u03c4 else 0 (8) q(x) = p(x;\u03b8)\n\np(x) = p(x; \u03b8)\n\nwhere\u03b8 is the parameter of the existing model, which usually refers to a model trained on labeled data only until convergence.\n\nC.2 Entropy Minimization [16] (q, p) = H(q, p)\n\nw(x) = 1 (12) q(x) = p(x; \u03b8) (13) p(x) = p(x; \u03b8)\n\nNote that gradient flows both to q and p. To our best knowledge, Entropy Minimization is the only method that backpropagates the gradient through q.\n\nC.3 Pseudo Labeling [27] (q, p) = H(q, p)\n\nw(x) = 1 if max(p(x; \u03b8)) \u2265 \u03c4 else 0 (16) q(x) = one hot (arg max (p(x; \u03b8))) (17) p(x) = p(x; \u03b8)\n\nC.4 Temporal Ensembling [25] (q, p) = q \u2212 p 2 2 (19) w(x) = 1 (20)\nq (t) (x) = \u03b1q (t\u22121) (x) + (1 \u2212 \u03b1)p(x; \u03b8)(21)\np(x) = p(x; \u03b8)\n\nWe omit the ramp up and ramp down for w(\u00b7) in our formulation since it is dependent on the optimization framework. See [25] for more details.\n\n\nC.5 Mean Teacher [54]\n\n(q, p) = q \u2212 p 2 2 (23)\n\nw(x) = 1 (24) q(x) = p(x; \u03b8 EMA ), \u03b8 EMA = \u03b1\u03b8 EMA + (1 \u2212 \u03b1)\u03b8 (t) (25) p(x) = p(x; \u03b8 (t) )\n\nWe omit the ramp up and ramp down for w(\u00b7) in our formulation since it is dependent on the optimization framework. See [54] for more details.\n\n\nC.6 Virtual Adversarial Training [35]\n\n(q, p) = H(q, p)\n\nw(x) = 1 (28) q(x) = p(x; \u03b8) (29) p(x) = p(AP(x); \u03b8), AP(\u00b7): adversarial perturbation (30) C.7 Unsupervised Data Augmentation (UDA) [58] UDA uses a weak (\u03b1(\u00b7)), such as translation and horizontal flip, to generate a pseudo label, and strong augmentation (A(\u00b7)), such as RandAugment [7] followed by Cutout [10], for model training.\n\n(q, p) = H(q, p)\n\nw(x) = 1 if max(p(\u03b1(x); \u03b8)) \u2265 \u03c4 else 0 (32) q(x) \u221d p(\u03b1(x); \u03b8) 1 T\n\np(x) = p(A(x); \u03b8) (34)\n\n\nC.8 FixMatch [49]\n\nFixMatch also uses a weak (\u03b1(\u00b7)), such as translation and horizontal flip, to generate a pseudo label, and strong augmentation (A(\u00b7)), such as RandAugment [7] or CTAugment [3] followed by Cutout [10], for model training.\n\n(q, p) = H(q, p)\n\nw(x) = 1 if max(p(\u03b1(x); \u03b8)) \u2265 \u03c4 else 0 (36)\n\nq(x) = one hot(arg max (p(\u03b1(x); \u03b8))) (37)\n\np(x) = p(A(x); \u03b8) (38)\n\nC.9 Noisy Student [59] (q, p) = H(q, p)\n\nw(x) = 1 if max(p(x;\u03b8)) \u2265 \u03c4 else 0 (40) q(x) = one hot(arg max(p(x;\u03b8)))) (41)\np(x) = p(A(x); \u03b8)(42)\nwhere\u03b8 is the parameter of the model that is trained on labeled data only until convergence. In addition, Noisy Student perform data balancing across classes, which is not reflected in this formulation.\n\nFig. 1 :\n1Fig. 1: The proposed semi-supervised learning framework for object detection, STAC, consistently improves upon supervised baselines and those with data augmentation using different amount of labeled training data on MS-COCO [30].\n\n\nFig. 2: The proposed SSL framework for object detection. We generate pseudo labels (i.e., bounding boxes and their class labels) for unlabeled data using testtime inference, including non-maximum suppression (NMS) [15], of the teacher model trained with labeled data. We compute unsupervised loss with respect to pseudo labels whose confidence scores are above a threshold \u03c4 . The strong augmentations are applied for augmentation consistency during the model training. Target boxes are augmented when global geometric transformations are used.\n\n2 3 .\n3Box-level transformation[69] (B): Three transformation operations from global geometric transformations are used, but with smaller magnitude ranges.3 \n\nFig. 4 :\n4mAPs of STAC with different values of \u03bb u \u2208 {0.1, 0.5, 1, 2, 4} and \u03c4 \u2208 {0, 0.3, 0.5, 0.7, 0.9}. We test on a single fold of 10% protocol. Different colors represent mAPs of models with different \u03c4 values. \"Sup\" represents the mAP of supervised model with default augmentations and \"Sup*\" represents that with C+{G,B}+Cutout. values of \u03bb u \u2208 {0.1, 0.5, 1, 2, 4} and \u03c4 \u2208 {0, 0.3, 0.5, 0.7, 0.9} on a single fold of 10% protocol. The summary results are provided in\n\n\nOliver, A., Odena, A., Raffel, C.A., Cubuk, E.D., Goodfellow, I.: Realistic evaluation of deep semi-supervised learning algorithms. In: NeurIPS (2018) 12 37. Radosavovic, I., Doll\u00e1r, P., Girshick, R., Gkioxari, G., He, K.: Data distillation: Towards omni-supervised learning. In: CVPR (2018) 3, 4 38. Rasmus, A., Berglund, M., Honkala, M., Valpola, H., Raiko, T.: Semi-supervised learning with ladder networks. In: NeurIPS (2015) 1, 5 39. Redmon, J., Divvala, S., Girshick, R., Farhadi, A.: You only look once: Unified, real-time object detection. In: CVPR (2016) 4 40. Redmon, J., Farhadi, A.: Yolo9000: better, faster, stronger. In: CVPR (2017) 4 41. Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time object detection with region proposal networks. In: NeurIPS (2015) 2, 4, 6 42. Rosenberg, C., Hebert, M., Schneiderman, H.: Semi-supervised self-training of object detection models. In: Proc IEEE Workshops on Application of Computer Vision (2005) 4 43. RoyChowdhury, A., Chakrabarty, P., Singh, A., Jin, S., Jiang, H., Cao, L., Learned-Miller, E.: Automatic adaptation of object detectors to new domains using selftraining. In: CVPR (2019) 2 44. Sajjadi, M., Javanmardi, M., Tasdizen, T.: Mutual exclusivity loss for semisupervised deep learning. In: ICIP (2016) 1, 5 45. Sajjadi, M., Javanmardi, M., Tasdizen, T.: Regularization with stochastic transformations and perturbations for deep semi-supervised learning. In: NeurIPS (2016) 4 46. Scudder, H.: Probability of error of some adaptive pattern-recognition machines. IEEE Transactions on Information Theory 11(3) (1965) 2 47. Shrivastava, A., Gupta, A., Girshick, R.: Training region-based object detectors with online hard example mining. In: CVPR (2016) 2, 15 48. Simard, P.Y., Steinkraus, D., Platt, J.C., et al.: Best practices for convolutional neural networks applied to visual document analysis. In: ICDAR (2003) 1 49. Sohn, K., Berthelot, D., Li, C.L., Zhang, Z., Carlini, N., Cubuk, E.D., Kurakin, A., Zhang, H., Raffel, C.: Fixmatch: Simplifying semi-supervised learning with consistency and confidence. arXiv preprint arXiv:2001.07685 (2020) 1, 2, 4, 5, 6, 7, 12, 22 50. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.: Dropout: a simple way to prevent neural networks from overfitting. JMLR 15(1), 1929-1958 (2014) 5 51. Tan, M., Pang, R., Le, Q.V.: Efficientdet: Scalable and efficient object detection. arXiv preprint arXiv:1911.09070 (2019) 4 52. Tang, P., Ramaiah, C., Xu, R., Xiong, C.: Proposal learning for semi-supervised object detection. arXiv preprint arXiv:2001.05086 (2020) 3, 4, 9 53. Tang, Y., Wang, J., Gao, B., Dellandr\u00e9a, E., Gaizauskas, R., Chen, L.: Large scale semi-supervised object detection using visual and semantic knowledge transfer. In: CVPR (2016) 2, 4 54. Tarvainen, A., Valpola, H.: Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In: NeurIPS (2017) 1, 4, 5, 21 55. Wang, K., Yan, X., Zhang, D., Zhang, L., Lin, L.: Towards human-machine cooperation: Self-supervised sample mining for object detection. In: CVPR (2018) 4\n\n\nFig. 3: Visualization of different types of augmentation strategies. From left to right: original image, color transformation, global geometric transformation, boxlevel geometric transformation, box-level geometric transformation, and Cutout.Original \nColor \nGlobal \ngeometric \n\nBox-level \ngeometric \nCutout \n\n\n\nTable 3 :\n3mAPs of supervised models trained with different augmentation and learning schedules. We test on a single fold of 5% and 10% protocols. See Section 5.1 for more details. Bold text indicates the best number in each row.28.63 for the supervised model with 20% labeled training data). For the 100% \nprotocol, STAC achieves 39.21 mAP. This improves upon the baseline (37.63 \nmAP), but falls short of the supervised model with a strong data augmentation \n(39.48 mAP)\n\n\n: https://github.com/tensorpack/tensorpack/blob/ master/examples/FasterRCNN/config.py. Data processing: Short edge size is sampled between 500 and 800 if the long edge is less than 1024 after resizing. \u2022 Batch per image for training Faster RCNN head: 64 A.2 Standard, [n]\u00d7 \u2022 LR Decay: 0.01 (\u2264120k), 0.001 (\u2264160k), 0.0001 (\u2264180k) \u2022 LR Decay (2\u00d7): 0.01 (\u2264240k), 0.001 (\u2264320k), 0.0001 (\u2264360k) \u2022 LR Decay (3\u00d7): 0.01 (\u2264420k), 0.001 (\u2264500k), 0.0001 (\u2264540k) \u2022 Data processing: Short edge size is fixed to 800 if the long edge is less than 1333 after resizing. \u2022 Batch per image for training Faster RCNN head: 512 A.3 VOC \u2022 LR Decay: 0.001 (\u2264120k), 0.0005 (\u2264160k) \u2022 Data processing: Short edge size is fixed to 600 if the long edge is less than 1000 after resizing. Image is resized to have its longer edge to be 1000 if long edge is longer than 1000. \u2022 Batch per image for training Faster RCNN head: 256 \u2022 RPN Anchor Sizes: 8, 16, 32A.1 Quick \n\n\u2022 LR Decay: 0.01 (\u2264120k), 0.001 (\u2264160k), 0.0001 (\u2264180k) \n\u2022 \nThe range of degrees is empirically chosen without tuning.2 The translation range in percentage is [\u221210%, 10%] of image widths or heights. The rotation and shear ranges are [\u221230%, 30%] in degrees.3 The translation range in percentage is [\u22125%, 5%] of image widths or heights. The rotation and shear range is [\u221210%, 10%] in degree.4 The number of Cutout regions is sampled from[1,5], and the region size is sampled from [0%, 20%] of the short edge of the applied image.\nhttp://models.tensorpack.com/FasterRCNN/ImageNet-R50-AlignPadding.npz 6 Please refer to Section 5.1 for the definition of different learning schedules. 7 https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN#results 8 We note that the number from [23] using ResNet101 with R-FCN while all the results from our implementation use ResNet50 with FPN.\nAcknowledgmentWe thank Qizhe Xie, Ekin D. Cubuk, Sercan Arik, Minh-Thang Luong, David Berthelot, Tsung-Yi Lin, Quoc V. Le, Samuel Schulter for their comments.C.10 MixMatch[4]Note that MixMatch uses MixUp[64]for unsupervised loss. It uses weak augmentation \u03b1(\u00b7), such as translation and horizontal flip.where x and z are unlabeled data and \u03b2 is drawn from Beta distribution. While we present MixUp only between unlabeled data for presentation clarity, one may apply MixUp between labeled (with ground-truth label forq) and unlabeled data as well[4].C.11 ReMixMatch [3]Note that ReMixMatch uses MixUp[64]for unsupervised loss. It also uses weak augmentation \u03b1(\u00b7), such as translation and horizontal flip, and strong augmentation A(\u00b7), such as CTAugment[3].where x and z are unlabeled data and \u03b2 is drawn from Beta distribution. While we present MixUp only between unlabeled data for presentation clarity, one may apply MixUp between labeled (with ground-truth label forq) and unlabeled data as well[3].\nPseudolabeling and confirmation bias in deep semi-supervised learning. E Arazo, D Ortego, P Albert, N E O&apos;connor, K Mcguinness, arXiv:1908.0298315arXiv preprintArazo, E., Ortego, D., Albert, P., O'Connor, N.E., McGuinness, K.: Pseudo- labeling and confirmation bias in deep semi-supervised learning. arXiv preprint arXiv:1908.02983 (2019) 15\n\nLearning with pseudo-ensembles. P Bachman, O Alsharif, D Precup, NeurIPS. 4Bachman, P., Alsharif, O., Precup, D.: Learning with pseudo-ensembles. In: NeurIPS (2014) 4\n\nRemixmatch: Semi-supervised learning with distribution matching and augmentation anchoring. D Berthelot, N Carlini, E D Cubuk, A Kurakin, K Sohn, H Zhang, C Raffel, ICLR. 23Berthelot, D., Carlini, N., Cubuk, E.D., Kurakin, A., Sohn, K., Zhang, H., Raffel, C.: Remixmatch: Semi-supervised learning with distribution matching and aug- mentation anchoring. In: ICLR (2020) 1, 2, 4, 5, 7, 22, 23\n\nMixmatch: A holistic approach to semi-supervised learning. D Berthelot, N Carlini, I Goodfellow, N Papernot, A Oliver, C A Raffel, NeurIPS (2019) 1, 4. 523Berthelot, D., Carlini, N., Goodfellow, I., Papernot, N., Oliver, A., Raffel, C.A.: Mixmatch: A holistic approach to semi-supervised learning. In: NeurIPS (2019) 1, 4, 5, 23\n\nCascade r-cnn: Delving into high quality object detection. Z Cai, N Vasconcelos, CVPR410Cai, Z., Vasconcelos, N.: Cascade r-cnn: Delving into high quality object detection. In: CVPR (2018) 4, 10\n\nAutoaugment: Learning augmentation strategies from data. E D Cubuk, B Zoph, D Mane, V Vasudevan, Q V Le, CVPR (2019) 2, 4. 57Cubuk, E.D., Zoph, B., Mane, D., Vasudevan, V., Le, Q.V.: Autoaugment: Learning augmentation strategies from data. In: CVPR (2019) 2, 4, 5, 7\n\nRandaugment: Practical data augmentation with no separate search. E D Cubuk, B Zoph, J Shlens, Q V Le, arXiv:1909.137191022arXiv preprintCubuk, E.D., Zoph, B., Shlens, J., Le, Q.V.: Randaugment: Practical data aug- mentation with no separate search. arXiv preprint arXiv:1909.13719 (2019) 2, 4, 7, 8, 10, 22\n\nR-fcn: Object detection via region-based fully convolutional networks. J Dai, Y Li, K He, J Sun, NIPS. 2Dai, J., Li, Y., He, K., Sun, J.: R-fcn: Object detection via region-based fully convolutional networks. In: NIPS (2016) 2\n\nImagenet: A large-scale hierarchical image database. J Deng, W Dong, R Socher, L J Li, K Li, L Fei-Fei, CVPR12Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet: A large-scale hierarchical image database. In: CVPR (2009) 12\n\nT Devries, G W Taylor, arXiv:1708.04552Improved regularization of convolutional neural networks with cutout. 322arXiv preprintDeVries, T., Taylor, G.W.: Improved regularization of convolutional neural net- works with cutout. arXiv preprint arXiv:1708.04552 (2017) 3, 4, 8, 22\n\nX Du, T Y Lin, P Jin, G Ghiasi, M Tan, Y Cui, Q V Le, X Song, arXiv:1912.05027Spinenet: Learning scale-permuted backbone for recognition and localization. 4arXiv preprintDu, X., Lin, T.Y., Jin, P., Ghiasi, G., Tan, M., Cui, Y., Le, Q.V., Song, X.: Spinenet: Learning scale-permuted backbone for recognition and localization. arXiv preprint arXiv:1912.05027 (2019) 4\n\nCut, paste and learn: Surprisingly easy synthesis for instance detection. D Dwibedi, I Misra, M Hebert, 4Dwibedi, D., Misra, I., Hebert, M.: Cut, paste and learn: Surprisingly easy synthesis for instance detection. In: ICCV (2017) 4\n\nThe pascal visual object classes (voc) challenge. M Everingham, L Van Gool, C K Williams, J Winn, A Zisserman, IJCV. 8829Everingham, M., Van Gool, L., Williams, C.K., Winn, J., Zisserman, A.: The pascal visual object classes (voc) challenge. IJCV 88(2), 303-338 (2010) 3, 9\n\nFast r-cnn. R Girshick, ICCV. 24Girshick, R.: Fast r-cnn. In: ICCV (2015) 2, 4\n\nRich feature hierarchies for accurate object detection and semantic segmentation. R Girshick, J Donahue, T Darrell, J Malik, CVPR34Girshick, R., Donahue, J., Darrell, T., Malik, J.: Rich feature hierarchies for accu- rate object detection and semantic segmentation. In: CVPR (2014) 3, 4\n\nSemi-supervised learning by entropy minimization. Y Grandvalet, Y Bengio, Advances in neural information processing systems. 21Grandvalet, Y., Bengio, Y.: Semi-supervised learning by entropy minimization. In: Advances in neural information processing systems. pp. 529-536 (2005) 21\n\nMask r-cnn. K He, G Gkioxari, P Doll\u00e1r, R Girshick, 4He, K., Gkioxari, G., Doll\u00e1r, P., Girshick, R.: Mask r-cnn. In: ICCV (2017) 4\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR9He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR (2016) 9\n\nAugmix: A simple data processing method to improve robustness and uncertainty. D Hendrycks, N Mu, E D Cubuk, B Zoph, J Gilmer, B Lakshminarayanan, arXiv:1912.0278145arXiv preprintHendrycks, D., Mu, N., Cubuk, E.D., Zoph, B., Gilmer, J., Lakshminarayanan, B.: Augmix: A simple data processing method to improve robustness and uncertainty. arXiv preprint arXiv:1912.02781 (2019) 4, 5\n\nPopulation based augmentation: Efficient learning of augmentation policy schedules. D Ho, E Liang, I Stoica, P Abbeel, X Chen, arXiv:1905.053934arXiv preprintHo, D., Liang, E., Stoica, I., Abbeel, P., Chen, X.: Population based aug- mentation: Efficient learning of augmentation policy schedules. arXiv preprint arXiv:1905.05393 (2019) 4\n\nJ Hoffman, E Tzeng, T Park, J Y Zhu, P Isola, K Saenko, A A Efros, T Darrell, arXiv:1711.03213Cycada: Cycle-consistent adversarial domain adaptation. arXiv preprintHoffman, J., Tzeng, E., Park, T., Zhu, J.Y., Isola, P., Saenko, K., Efros, A.A., Dar- rell, T.: Cycada: Cycle-consistent adversarial domain adaptation. arXiv preprint arXiv:1711.03213 (2017) 2\n\nDeep networks with stochastic depth. G Huang, Y Sun, Z Liu, D Sedra, K Q Weinberger, ECCV5Huang, G., Sun, Y., Liu, Z., Sedra, D., Weinberger, K.Q.: Deep networks with stochastic depth. In: ECCV (2016) 5\n\nConsistency-based semi-supervised learning for object detection. J Jeong, S Lee, J Kim, N Kwak, NeurIPS (2019) 3, 4, 5. 710Jeong, J., Lee, S., Kim, J., Kwak, N.: Consistency-based semi-supervised learning for object detection. In: NeurIPS (2019) 3, 4, 5, 7, 9, 10\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, NeurIPS. 1Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep con- volutional neural networks. In: NeurIPS (2012) 1\n\nTemporal ensembling for semi-supervised learning. S Laine, T Aila, ICLR (2017) 1, 4. 521Laine, S., Aila, T.: Temporal ensembling for semi-supervised learning. In: ICLR (2017) 1, 4, 5, 21\n\nCornernet: Detecting objects as paired keypoints. H Law, J Deng, In: ECCV. 4Law, H., Deng, J.: Cornernet: Detecting objects as paired keypoints. In: ECCV (2018) 4\n\nPseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. D H Lee, ICML Workshops (2013) 1, 4. 521Lee, D.H.: Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. In: ICML Workshops (2013) 1, 4, 5, 21\n\nFeature pyramid networks for object detection. T Y Lin, P Doll\u00e1r, R Girshick, K He, B Hariharan, S Belongie, 24Lin, T.Y., Doll\u00e1r, P., Girshick, R., He, K., Hariharan, B., Belongie, S.: Feature pyramid networks for object detection. In: CVPR (2017) 2, 4\n\nFocal loss for dense object detection. T Y Lin, P Goyal, R Girshick, K He, P Doll\u00e1r, 215Lin, T.Y., Goyal, P., Girshick, R., He, K., Doll\u00e1r, P.: Focal loss for dense object detection. In: ICCV (2017) 2, 4, 15\n\nMicrosoft coco: Common objects in context. T Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Doll\u00e1r, C L Zitnick, ECCV212Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll\u00e1r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV (2014) 2, 3, 9, 12\n\nSsd: Single shot multibox detector. W Liu, D Anguelov, D Erhan, C Szegedy, S Reed, C Y Fu, A C Berg, ECCV4Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y., Berg, A.C.: Ssd: Single shot multibox detector. In: ECCV (2016) 4\n\nEffective self-training for parsing. D Mcclosky, E Charniak, M Johnson, Proceedings of the main conference on human language technology conference of the North American Chapter of the Association of Computational Linguistics. the main conference on human language technology conference of the North American Chapter of the Association of Computational LinguisticsAssociation for Computational Linguistics20McClosky, D., Charniak, E., Johnson, M.: Effective self-training for parsing. In: Proceedings of the main conference on human language technology conference of the North American Chapter of the Association of Computational Linguistics. pp. 152-159. Association for Computational Linguistics (2006) 20\n\nIterative reclassification procedure for constructing an asymptotically optimal rule of allocation in discriminant analysis. G J Mclachlan, Journal of the American Statistical Association. 70350McLachlan, G.J.: Iterative reclassification procedure for constructing an asymptot- ically optimal rule of allocation in discriminant analysis. Journal of the American Statistical Association 70(350), 365-369 (1975) 2\n\nWatch and learn: Semi-supervised learning for object detectors from video. I Misra, A Shrivastava, M Hebert, CVPR. 24Misra, I., Shrivastava, A., Hebert, M.: Watch and learn: Semi-supervised learning for object detectors from video. In: CVPR (2015) 2, 4\n\n. Y Wu, 9Wu, Y., et al.: Tensorpack. https://github.com/tensorpack/ (2016) 9\n\nSun database: Large-scale scene recognition from abbey to zoo. J Xiao, J Hays, K A Ehinger, A Oliva, A Torralba, CVPR12Xiao, J., Hays, J., Ehinger, K.A., Oliva, A., Torralba, A.: Sun database: Large-scale scene recognition from abbey to zoo. In: CVPR (2010) 12\n\nUnsupervised data augmentation for consistency training. Q Xie, Z Dai, E Hovy, M T Luong, Q V Le, arXiv:1904.12848722arXiv preprintXie, Q., Dai, Z., Hovy, E., Luong, M.T., Le, Q.V.: Unsupervised data augmentation for consistency training. arXiv preprint arXiv:1904.12848 (2019) 1, 2, 4, 5, 7, 22\n\nQ Xie, E Hovy, M T Luong, Q V Le, arXiv:1911.04252Self-training with noisy student improves imagenet classification. 622arXiv preprintXie, Q., Hovy, E., Luong, M.T., Le, Q.V.: Self-training with noisy student improves imagenet classification. arXiv preprint arXiv:1911.04252 (2019) 1, 2, 4, 5, 6, 22\n\nUnsupervised word sense disambiguation rivaling supervised methods. D Yarowsky, 33rd annual meeting of the association for computational linguistics. 20Yarowsky, D.: Unsupervised word sense disambiguation rivaling supervised meth- ods. In: 33rd annual meeting of the association for computational linguistics. pp. 189-196 (1995) 20\n\nCutmix: Regularization strategy to train strong classifiers with localizable features. S Yun, D Han, S J Oh, S Chun, J Choe, Y Yoo, ICCV5Yun, S., Han, D., Oh, S.J., Chun, S., Choe, J., Yoo, Y.: Cutmix: Regularization strategy to train strong classifiers with localizable features. In: ICCV (2019) 5\n\nDeceptionnet: Network-driven domain randomization. S Zakharov, W Kehl, S Ilic, ICCV. 2Zakharov, S., Kehl, W., Ilic, S.: Deceptionnet: Network-driven domain randomiza- tion. In: ICCV (2019) 2\n\nS 4 l: Self-supervised semi-supervised learning. X Zhai, A Oliver, A Kolesnikov, L Beyer, ICCV4Zhai, X., Oliver, A., Kolesnikov, A., Beyer, L.: S 4 l: Self-supervised semi-supervised learning. In: ICCV (2019) 4\n\nH Zhang, M Cisse, Y N Dauphin, D Lopez-Paz, arXiv:1710.09412mixup: Beyond empirical risk minimization. 423arXiv preprintZhang, H., Cisse, M., Dauphin, Y.N., Lopez-Paz, D.: mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412 (2017) 4, 5, 23\n\nZ Zhang, T He, H Zhang, Z Zhang, J Xie, M Li, arXiv:1902.04103Bag of freebies for training object detection neural networks. 5arXiv preprintZhang, Z., He, T., Zhang, H., Zhang, Z., Xie, J., Li, M.: Bag of freebies for training object detection neural networks. arXiv preprint arXiv:1902.04103 (2019) 5\n\nEnhanced semi-supervised learning for multimodal emotion recognition. Z Zhang, F Ringeval, B Dong, E Coutinho, E Marchi, B Sch\u00fcller, ICASSP. 15Zhang, Z., Ringeval, F., Dong, B., Coutinho, E., Marchi, E., Sch\u00fcller, B.: Enhanced semi-supervised learning for multimodal emotion recognition. In: ICASSP (2016) 15\n\nZ Zhong, L Zheng, G Kang, S Li, Y Yang, arXiv:1708.04896Random erasing data augmentation. 4arXiv preprintZhong, Z., Zheng, L., Kang, G., Li, S., Yang, Y.: Random erasing data augmenta- tion. arXiv preprint arXiv:1708.04896 (2017) 4\n\nUnpaired image-to-image translation using cycle-consistent adversarial networks. J Y Zhu, T Park, P Isola, A A Efros, Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation using cycle-consistent adversarial networks. In: ICCV (2017) 2\n\nLearning data augmentation strategies for object detection. B Zoph, E D Cubuk, G Ghiasi, T Y Lin, J Shlens, Q V Le, arXiv:1906.111727arXiv preprintZoph, B., Cubuk, E.D., Ghiasi, G., Lin, T.Y., Shlens, J., Le, Q.V.: Learning data augmentation strategies for object detection. arXiv preprint arXiv:1906.11172 (2019) 2, 3, 4, 5, 7, 8\n", "annotations": {"author": "[{\"end\":127,\"start\":68},{\"end\":190,\"start\":128},{\"end\":256,\"start\":191},{\"end\":317,\"start\":257},{\"end\":381,\"start\":318},{\"end\":446,\"start\":382}]", "publisher": null, "author_last_name": "[{\"end\":79,\"start\":75},{\"end\":140,\"start\":135},{\"end\":204,\"start\":202},{\"end\":266,\"start\":261},{\"end\":329,\"start\":326},{\"end\":395,\"start\":388}]", "author_first_name": "[{\"end\":74,\"start\":68},{\"end\":134,\"start\":128},{\"end\":201,\"start\":191},{\"end\":260,\"start\":257},{\"end\":325,\"start\":318},{\"end\":387,\"start\":382}]", "author_affiliation": "[{\"end\":126,\"start\":98},{\"end\":189,\"start\":161},{\"end\":255,\"start\":227},{\"end\":316,\"start\":288},{\"end\":380,\"start\":352},{\"end\":445,\"start\":417}]", "title": "[{\"end\":65,\"start\":1},{\"end\":511,\"start\":447}]", "venue": null, "abstract": "[{\"end\":1639,\"start\":592}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b26\"},\"end\":1936,\"start\":1932},{\"end\":1939,\"start\":1936},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":1942,\"start\":1939},{\"end\":1945,\"start\":1942},{\"end\":1948,\"start\":1945},{\"end\":1951,\"start\":1948},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1953,\"start\":1951},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":1956,\"start\":1953},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1958,\"start\":1956},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":1961,\"start\":1958},{\"end\":1964,\"start\":1961},{\"end\":2451,\"start\":2447},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2454,\"start\":2451},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":2542,\"start\":2538},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2544,\"start\":2542},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":2547,\"start\":2544},{\"end\":2550,\"start\":2547},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2819,\"start\":2815},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":2957,\"start\":2953},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2960,\"start\":2957},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":2963,\"start\":2960},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3014,\"start\":3011},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3017,\"start\":3014},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3088,\"start\":3085},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3105,\"start\":3102},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":3174,\"start\":3170},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3176,\"start\":3174},{\"end\":3179,\"start\":3176},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3182,\"start\":3179},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3674,\"start\":3671},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3677,\"start\":3674},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":3693,\"start\":3689},{\"end\":3696,\"start\":3693},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3698,\"start\":3696},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3771,\"start\":3767},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3828,\"start\":3825},{\"end\":3830,\"start\":3828},{\"end\":4169,\"start\":4165},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":4172,\"start\":4169},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4246,\"start\":4243},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4248,\"start\":4246},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":4251,\"start\":4248},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":4300,\"start\":4296},{\"end\":4812,\"start\":4808},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5059,\"start\":5056},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5101,\"start\":5097},{\"end\":5104,\"start\":5101},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5326,\"start\":5322},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":5669,\"start\":5665},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":5800,\"start\":5796},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":5843,\"start\":5839},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5906,\"start\":5902},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5927,\"start\":5923},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6386,\"start\":6382},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7436,\"start\":7432},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7439,\"start\":7436},{\"end\":7442,\"start\":7439},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7445,\"start\":7442},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7448,\"start\":7445},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7450,\"start\":7448},{\"end\":7453,\"start\":7450},{\"end\":7456,\"start\":7453},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7459,\"start\":7456},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7462,\"start\":7459},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7536,\"start\":7532},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7539,\"start\":7536},{\"end\":7542,\"start\":7539},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7545,\"start\":7542},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":7548,\"start\":7545},{\"end\":7559,\"start\":7555},{\"end\":7562,\"start\":7559},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7572,\"start\":7568},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7582,\"start\":7578},{\"end\":7585,\"start\":7582},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7588,\"start\":7585},{\"end\":7864,\"start\":7860},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7867,\"start\":7864},{\"end\":7925,\"start\":7921},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":7941,\"start\":7937},{\"end\":8341,\"start\":8337},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8926,\"start\":8923},{\"end\":8929,\"start\":8926},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8932,\"start\":8929},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8935,\"start\":8932},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8938,\"start\":8935},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8956,\"start\":8952},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9140,\"start\":9136},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9158,\"start\":9155},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9212,\"start\":9208},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9214,\"start\":9212},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":9619,\"start\":9615},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10566,\"start\":10563},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":10568,\"start\":10566},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10571,\"start\":10568},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":10574,\"start\":10571},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":10577,\"start\":10574},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":10580,\"start\":10577},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10583,\"start\":10580},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10586,\"start\":10583},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10589,\"start\":10586},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10666,\"start\":10663},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":10668,\"start\":10666},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":10671,\"start\":10668},{\"end\":10674,\"start\":10671},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10823,\"start\":10820},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":10826,\"start\":10823},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":11003,\"start\":10999},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":11186,\"start\":11182},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":11189,\"start\":11186},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":11202,\"start\":11198},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":11242,\"start\":11238},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11304,\"start\":11300},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11765,\"start\":11761},{\"end\":11768,\"start\":11765},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":11771,\"start\":11768},{\"end\":11774,\"start\":11771},{\"end\":11777,\"start\":11774},{\"end\":11780,\"start\":11777},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11782,\"start\":11780},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":11785,\"start\":11782},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11787,\"start\":11785},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":11790,\"start\":11787},{\"end\":11793,\"start\":11790},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12448,\"start\":12444},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12737,\"start\":12733},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":12814,\"start\":12810},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":12831,\"start\":12828},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":12911,\"start\":12907},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":13094,\"start\":13090},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":13880,\"start\":13876},{\"end\":16042,\"start\":16038},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":18085,\"start\":18081},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":18379,\"start\":18375},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":18789,\"start\":18785},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18959,\"start\":18956},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":18961,\"start\":18959},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":18963,\"start\":18961},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":18966,\"start\":18963},{\"end\":18969,\"start\":18966},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":19085,\"start\":19082},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":19147,\"start\":19143},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":19207,\"start\":19203},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":19366,\"start\":19365},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19441,\"start\":19438},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":19584,\"start\":19581},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":20158,\"start\":20154},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":21354,\"start\":21350},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":21369,\"start\":21365},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":21825,\"start\":21821},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":21848,\"start\":21844},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":22296,\"start\":22295},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":23017,\"start\":23014},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23043,\"start\":23039},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":23797,\"start\":23794},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":24491,\"start\":24489},{\"end\":27607,\"start\":27603},{\"end\":27807,\"start\":27801},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":28157,\"start\":28154},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":28160,\"start\":28157},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":28163,\"start\":28160},{\"end\":28270,\"start\":28266},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":30172,\"start\":30169},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":30174,\"start\":30172},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":33914,\"start\":33910},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":33916,\"start\":33914},{\"end\":34043,\"start\":34039},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":34046,\"start\":34043},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":36429,\"start\":36425},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":36432,\"start\":36429},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":36489,\"start\":36486},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":36678,\"start\":36674},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":36710,\"start\":36706},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":36730,\"start\":36726},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":36921,\"start\":36917},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":36980,\"start\":36976},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":37020,\"start\":37016},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":37065,\"start\":37061},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":37089,\"start\":37085},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":37289,\"start\":37285},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":37371,\"start\":37367},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":37427,\"start\":37423},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":37663,\"start\":37659},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":37683,\"start\":37679},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":37740,\"start\":37736},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":37786,\"start\":37782},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":37935,\"start\":37932},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":37959,\"start\":37955},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":38043,\"start\":38039},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":38269,\"start\":38266},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":38286,\"start\":38283},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":38310,\"start\":38306},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":38485,\"start\":38481},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":39631,\"start\":39627},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":39752,\"start\":39751},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":45222,\"start\":45221},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":45360,\"start\":45359},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":45493,\"start\":45492},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":45541,\"start\":45538},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":45543,\"start\":45541}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":39047,\"start\":38807},{\"attributes\":{\"id\":\"fig_2\"},\"end\":39594,\"start\":39048},{\"attributes\":{\"id\":\"fig_3\"},\"end\":39753,\"start\":39595},{\"attributes\":{\"id\":\"fig_4\"},\"end\":40228,\"start\":39754},{\"attributes\":{\"id\":\"fig_5\"},\"end\":43375,\"start\":40229},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":43688,\"start\":43376},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":44162,\"start\":43689},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":45162,\"start\":44163}]", "paragraph": "[{\"end\":1965,\"start\":1655},{\"end\":2820,\"start\":1967},{\"end\":3183,\"start\":2822},{\"end\":3939,\"start\":3185},{\"end\":5577,\"start\":3941},{\"end\":5653,\"start\":5579},{\"end\":6680,\"start\":5655},{\"end\":6727,\"start\":6682},{\"end\":6945,\"start\":6729},{\"end\":7139,\"start\":6947},{\"end\":7309,\"start\":7141},{\"end\":8342,\"start\":7326},{\"end\":8481,\"start\":8344},{\"end\":11531,\"start\":8483},{\"end\":11990,\"start\":11586},{\"end\":12482,\"start\":12023},{\"end\":12547,\"start\":12484},{\"end\":13616,\"start\":12549},{\"end\":14413,\"start\":13651},{\"end\":14834,\"start\":14415},{\"end\":15416,\"start\":14836},{\"end\":16043,\"start\":15538},{\"end\":17187,\"start\":16045},{\"end\":17838,\"start\":17327},{\"end\":18169,\"start\":17895},{\"end\":20083,\"start\":18171},{\"end\":21717,\"start\":20099},{\"end\":22012,\"start\":21744},{\"end\":23173,\"start\":22014},{\"end\":24128,\"start\":23185},{\"end\":24804,\"start\":24130},{\"end\":25103,\"start\":24806},{\"end\":25508,\"start\":25122},{\"end\":26384,\"start\":25552},{\"end\":27397,\"start\":26386},{\"end\":28014,\"start\":27399},{\"end\":28556,\"start\":28041},{\"end\":29514,\"start\":28558},{\"end\":29774,\"start\":29544},{\"end\":30815,\"start\":29776},{\"end\":31623,\"start\":30844},{\"end\":32298,\"start\":31682},{\"end\":33072,\"start\":32300},{\"end\":34318,\"start\":33102},{\"end\":34596,\"start\":34343},{\"end\":34994,\"start\":34628},{\"end\":35330,\"start\":35006},{\"end\":35618,\"start\":35382},{\"end\":36382,\"start\":35648},{\"end\":36449,\"start\":36384},{\"end\":36503,\"start\":36451},{\"end\":36519,\"start\":36505},{\"end\":36647,\"start\":36521},{\"end\":36695,\"start\":36649},{\"end\":36745,\"start\":36697},{\"end\":36895,\"start\":36747},{\"end\":36938,\"start\":36897},{\"end\":37035,\"start\":36940},{\"end\":37103,\"start\":37037},{\"end\":37164,\"start\":37150},{\"end\":37307,\"start\":37166},{\"end\":37356,\"start\":37333},{\"end\":37447,\"start\":37358},{\"end\":37590,\"start\":37449},{\"end\":37648,\"start\":37632},{\"end\":37980,\"start\":37650},{\"end\":37998,\"start\":37982},{\"end\":38065,\"start\":38000},{\"end\":38089,\"start\":38067},{\"end\":38331,\"start\":38111},{\"end\":38349,\"start\":38333},{\"end\":38394,\"start\":38351},{\"end\":38437,\"start\":38396},{\"end\":38461,\"start\":38439},{\"end\":38502,\"start\":38463},{\"end\":38581,\"start\":38504},{\"end\":38806,\"start\":38604}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":12022,\"start\":11991},{\"attributes\":{\"id\":\"formula_2\"},\"end\":15537,\"start\":15417},{\"attributes\":{\"id\":\"formula_3\"},\"end\":17326,\"start\":17188},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17894,\"start\":17839},{\"attributes\":{\"id\":\"formula_5\"},\"end\":31681,\"start\":31624},{\"attributes\":{\"id\":\"formula_6\"},\"end\":35647,\"start\":35619},{\"attributes\":{\"id\":\"formula_14\"},\"end\":37149,\"start\":37104},{\"attributes\":{\"id\":\"formula_22\"},\"end\":38603,\"start\":38582}]", "table_ref": "[{\"end\":22807,\"start\":22800},{\"end\":23465,\"start\":23458},{\"end\":24873,\"start\":24866},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":26429,\"start\":26422},{\"end\":28598,\"start\":28591},{\"end\":28835,\"start\":28828},{\"end\":28977,\"start\":28970},{\"end\":29713,\"start\":29706},{\"end\":31983,\"start\":31976},{\"end\":32491,\"start\":32484},{\"end\":34759,\"start\":34752},{\"end\":35036,\"start\":35029}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1653,\"start\":1641},{\"attributes\":{\"n\":\"2\"},\"end\":7324,\"start\":7312},{\"attributes\":{\"n\":\"3\"},\"end\":11545,\"start\":11534},{\"attributes\":{\"n\":\"3.1\"},\"end\":11584,\"start\":11548},{\"attributes\":{\"n\":\"3.2\"},\"end\":13649,\"start\":13619},{\"attributes\":{\"n\":\"4\"},\"end\":20097,\"start\":20086},{\"attributes\":{\"n\":\"4.1\"},\"end\":21742,\"start\":21720},{\"attributes\":{\"n\":\"4.2\"},\"end\":23183,\"start\":23176},{\"attributes\":{\"n\":\"5\"},\"end\":25120,\"start\":25106},{\"attributes\":{\"n\":\"5.1\"},\"end\":25550,\"start\":25511},{\"attributes\":{\"n\":\"5.2\"},\"end\":28039,\"start\":28017},{\"attributes\":{\"n\":\"5.3\"},\"end\":29542,\"start\":29517},{\"attributes\":{\"n\":\"5.4\"},\"end\":30842,\"start\":30818},{\"attributes\":{\"n\":\"6\"},\"end\":33100,\"start\":33075},{\"end\":34341,\"start\":34321},{\"end\":34626,\"start\":34599},{\"end\":35004,\"start\":34997},{\"end\":35380,\"start\":35333},{\"end\":37331,\"start\":37310},{\"end\":37630,\"start\":37593},{\"end\":38109,\"start\":38092},{\"end\":38816,\"start\":38808},{\"end\":39601,\"start\":39596},{\"end\":39763,\"start\":39755},{\"end\":43699,\"start\":43690}]", "table": "[{\"end\":43688,\"start\":43620},{\"end\":44162,\"start\":43919},{\"end\":45162,\"start\":45091}]", "figure_caption": "[{\"end\":39047,\"start\":38818},{\"end\":39594,\"start\":39050},{\"end\":39753,\"start\":39603},{\"end\":40228,\"start\":39765},{\"end\":43375,\"start\":40231},{\"end\":43620,\"start\":43378},{\"end\":43919,\"start\":43701},{\"end\":45091,\"start\":44165}]", "figure_ref": "[{\"end\":4600,\"start\":4592},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6645,\"start\":6637},{\"end\":16839,\"start\":16831},{\"end\":16853,\"start\":16844},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20082,\"start\":20074},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29928,\"start\":29920},{\"end\":30373,\"start\":30364},{\"end\":30541,\"start\":30532},{\"end\":30645,\"start\":30636},{\"end\":30674,\"start\":30665},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":30685,\"start\":30677}]", "bib_author_first_name": "[{\"end\":47071,\"start\":47070},{\"end\":47080,\"start\":47079},{\"end\":47090,\"start\":47089},{\"end\":47100,\"start\":47099},{\"end\":47102,\"start\":47101},{\"end\":47119,\"start\":47118},{\"end\":47380,\"start\":47379},{\"end\":47391,\"start\":47390},{\"end\":47403,\"start\":47402},{\"end\":47608,\"start\":47607},{\"end\":47621,\"start\":47620},{\"end\":47632,\"start\":47631},{\"end\":47634,\"start\":47633},{\"end\":47643,\"start\":47642},{\"end\":47654,\"start\":47653},{\"end\":47662,\"start\":47661},{\"end\":47671,\"start\":47670},{\"end\":47968,\"start\":47967},{\"end\":47981,\"start\":47980},{\"end\":47992,\"start\":47991},{\"end\":48006,\"start\":48005},{\"end\":48018,\"start\":48017},{\"end\":48028,\"start\":48027},{\"end\":48030,\"start\":48029},{\"end\":48298,\"start\":48297},{\"end\":48305,\"start\":48304},{\"end\":48492,\"start\":48491},{\"end\":48494,\"start\":48493},{\"end\":48503,\"start\":48502},{\"end\":48511,\"start\":48510},{\"end\":48519,\"start\":48518},{\"end\":48532,\"start\":48531},{\"end\":48534,\"start\":48533},{\"end\":48769,\"start\":48768},{\"end\":48771,\"start\":48770},{\"end\":48780,\"start\":48779},{\"end\":48788,\"start\":48787},{\"end\":48798,\"start\":48797},{\"end\":48800,\"start\":48799},{\"end\":49083,\"start\":49082},{\"end\":49090,\"start\":49089},{\"end\":49096,\"start\":49095},{\"end\":49102,\"start\":49101},{\"end\":49293,\"start\":49292},{\"end\":49301,\"start\":49300},{\"end\":49309,\"start\":49308},{\"end\":49319,\"start\":49318},{\"end\":49321,\"start\":49320},{\"end\":49327,\"start\":49326},{\"end\":49333,\"start\":49332},{\"end\":49486,\"start\":49485},{\"end\":49497,\"start\":49496},{\"end\":49499,\"start\":49498},{\"end\":49763,\"start\":49762},{\"end\":49769,\"start\":49768},{\"end\":49771,\"start\":49770},{\"end\":49778,\"start\":49777},{\"end\":49785,\"start\":49784},{\"end\":49795,\"start\":49794},{\"end\":49802,\"start\":49801},{\"end\":49809,\"start\":49808},{\"end\":49811,\"start\":49810},{\"end\":49817,\"start\":49816},{\"end\":50204,\"start\":50203},{\"end\":50215,\"start\":50214},{\"end\":50224,\"start\":50223},{\"end\":50414,\"start\":50413},{\"end\":50428,\"start\":50427},{\"end\":50440,\"start\":50439},{\"end\":50442,\"start\":50441},{\"end\":50454,\"start\":50453},{\"end\":50462,\"start\":50461},{\"end\":50651,\"start\":50650},{\"end\":50801,\"start\":50800},{\"end\":50813,\"start\":50812},{\"end\":50824,\"start\":50823},{\"end\":50835,\"start\":50834},{\"end\":51057,\"start\":51056},{\"end\":51071,\"start\":51070},{\"end\":51302,\"start\":51301},{\"end\":51308,\"start\":51307},{\"end\":51320,\"start\":51319},{\"end\":51330,\"start\":51329},{\"end\":51468,\"start\":51467},{\"end\":51474,\"start\":51473},{\"end\":51483,\"start\":51482},{\"end\":51490,\"start\":51489},{\"end\":51683,\"start\":51682},{\"end\":51696,\"start\":51695},{\"end\":51702,\"start\":51701},{\"end\":51704,\"start\":51703},{\"end\":51713,\"start\":51712},{\"end\":51721,\"start\":51720},{\"end\":51731,\"start\":51730},{\"end\":52071,\"start\":52070},{\"end\":52077,\"start\":52076},{\"end\":52086,\"start\":52085},{\"end\":52096,\"start\":52095},{\"end\":52106,\"start\":52105},{\"end\":52326,\"start\":52325},{\"end\":52337,\"start\":52336},{\"end\":52346,\"start\":52345},{\"end\":52354,\"start\":52353},{\"end\":52356,\"start\":52355},{\"end\":52363,\"start\":52362},{\"end\":52372,\"start\":52371},{\"end\":52382,\"start\":52381},{\"end\":52384,\"start\":52383},{\"end\":52393,\"start\":52392},{\"end\":52721,\"start\":52720},{\"end\":52730,\"start\":52729},{\"end\":52737,\"start\":52736},{\"end\":52744,\"start\":52743},{\"end\":52753,\"start\":52752},{\"end\":52755,\"start\":52754},{\"end\":52953,\"start\":52952},{\"end\":52962,\"start\":52961},{\"end\":52969,\"start\":52968},{\"end\":52976,\"start\":52975},{\"end\":53218,\"start\":53217},{\"end\":53232,\"start\":53231},{\"end\":53245,\"start\":53244},{\"end\":53247,\"start\":53246},{\"end\":53451,\"start\":53450},{\"end\":53460,\"start\":53459},{\"end\":53639,\"start\":53638},{\"end\":53646,\"start\":53645},{\"end\":53850,\"start\":53849},{\"end\":53852,\"start\":53851},{\"end\":54084,\"start\":54083},{\"end\":54086,\"start\":54085},{\"end\":54093,\"start\":54092},{\"end\":54103,\"start\":54102},{\"end\":54115,\"start\":54114},{\"end\":54121,\"start\":54120},{\"end\":54134,\"start\":54133},{\"end\":54330,\"start\":54329},{\"end\":54332,\"start\":54331},{\"end\":54339,\"start\":54338},{\"end\":54348,\"start\":54347},{\"end\":54360,\"start\":54359},{\"end\":54366,\"start\":54365},{\"end\":54543,\"start\":54542},{\"end\":54545,\"start\":54544},{\"end\":54552,\"start\":54551},{\"end\":54561,\"start\":54560},{\"end\":54573,\"start\":54572},{\"end\":54581,\"start\":54580},{\"end\":54591,\"start\":54590},{\"end\":54602,\"start\":54601},{\"end\":54612,\"start\":54611},{\"end\":54614,\"start\":54613},{\"end\":54838,\"start\":54837},{\"end\":54845,\"start\":54844},{\"end\":54857,\"start\":54856},{\"end\":54866,\"start\":54865},{\"end\":54877,\"start\":54876},{\"end\":54885,\"start\":54884},{\"end\":54887,\"start\":54886},{\"end\":54893,\"start\":54892},{\"end\":54895,\"start\":54894},{\"end\":55079,\"start\":55078},{\"end\":55091,\"start\":55090},{\"end\":55103,\"start\":55102},{\"end\":55875,\"start\":55874},{\"end\":55877,\"start\":55876},{\"end\":56238,\"start\":56237},{\"end\":56247,\"start\":56246},{\"end\":56262,\"start\":56261},{\"end\":56419,\"start\":56418},{\"end\":56558,\"start\":56557},{\"end\":56566,\"start\":56565},{\"end\":56574,\"start\":56573},{\"end\":56576,\"start\":56575},{\"end\":56587,\"start\":56586},{\"end\":56596,\"start\":56595},{\"end\":56814,\"start\":56813},{\"end\":56821,\"start\":56820},{\"end\":56828,\"start\":56827},{\"end\":56836,\"start\":56835},{\"end\":56838,\"start\":56837},{\"end\":56847,\"start\":56846},{\"end\":56849,\"start\":56848},{\"end\":57054,\"start\":57053},{\"end\":57061,\"start\":57060},{\"end\":57069,\"start\":57068},{\"end\":57071,\"start\":57070},{\"end\":57080,\"start\":57079},{\"end\":57082,\"start\":57081},{\"end\":57423,\"start\":57422},{\"end\":57775,\"start\":57774},{\"end\":57782,\"start\":57781},{\"end\":57789,\"start\":57788},{\"end\":57791,\"start\":57790},{\"end\":57797,\"start\":57796},{\"end\":57805,\"start\":57804},{\"end\":57813,\"start\":57812},{\"end\":58039,\"start\":58038},{\"end\":58051,\"start\":58050},{\"end\":58059,\"start\":58058},{\"end\":58229,\"start\":58228},{\"end\":58237,\"start\":58236},{\"end\":58247,\"start\":58246},{\"end\":58261,\"start\":58260},{\"end\":58392,\"start\":58391},{\"end\":58401,\"start\":58400},{\"end\":58410,\"start\":58409},{\"end\":58412,\"start\":58411},{\"end\":58423,\"start\":58422},{\"end\":58656,\"start\":58655},{\"end\":58665,\"start\":58664},{\"end\":58671,\"start\":58670},{\"end\":58680,\"start\":58679},{\"end\":58689,\"start\":58688},{\"end\":58696,\"start\":58695},{\"end\":59029,\"start\":59028},{\"end\":59038,\"start\":59037},{\"end\":59050,\"start\":59049},{\"end\":59058,\"start\":59057},{\"end\":59070,\"start\":59069},{\"end\":59080,\"start\":59079},{\"end\":59269,\"start\":59268},{\"end\":59278,\"start\":59277},{\"end\":59287,\"start\":59286},{\"end\":59295,\"start\":59294},{\"end\":59301,\"start\":59300},{\"end\":59583,\"start\":59582},{\"end\":59585,\"start\":59584},{\"end\":59592,\"start\":59591},{\"end\":59600,\"start\":59599},{\"end\":59609,\"start\":59608},{\"end\":59611,\"start\":59610},{\"end\":59825,\"start\":59824},{\"end\":59833,\"start\":59832},{\"end\":59835,\"start\":59834},{\"end\":59844,\"start\":59843},{\"end\":59854,\"start\":59853},{\"end\":59856,\"start\":59855},{\"end\":59863,\"start\":59862},{\"end\":59873,\"start\":59872},{\"end\":59875,\"start\":59874}]", "bib_author_last_name": "[{\"end\":47077,\"start\":47072},{\"end\":47087,\"start\":47081},{\"end\":47097,\"start\":47091},{\"end\":47116,\"start\":47103},{\"end\":47130,\"start\":47120},{\"end\":47388,\"start\":47381},{\"end\":47400,\"start\":47392},{\"end\":47410,\"start\":47404},{\"end\":47618,\"start\":47609},{\"end\":47629,\"start\":47622},{\"end\":47640,\"start\":47635},{\"end\":47651,\"start\":47644},{\"end\":47659,\"start\":47655},{\"end\":47668,\"start\":47663},{\"end\":47678,\"start\":47672},{\"end\":47978,\"start\":47969},{\"end\":47989,\"start\":47982},{\"end\":48003,\"start\":47993},{\"end\":48015,\"start\":48007},{\"end\":48025,\"start\":48019},{\"end\":48037,\"start\":48031},{\"end\":48302,\"start\":48299},{\"end\":48317,\"start\":48306},{\"end\":48500,\"start\":48495},{\"end\":48508,\"start\":48504},{\"end\":48516,\"start\":48512},{\"end\":48529,\"start\":48520},{\"end\":48537,\"start\":48535},{\"end\":48777,\"start\":48772},{\"end\":48785,\"start\":48781},{\"end\":48795,\"start\":48789},{\"end\":48803,\"start\":48801},{\"end\":49087,\"start\":49084},{\"end\":49093,\"start\":49091},{\"end\":49099,\"start\":49097},{\"end\":49106,\"start\":49103},{\"end\":49298,\"start\":49294},{\"end\":49306,\"start\":49302},{\"end\":49316,\"start\":49310},{\"end\":49324,\"start\":49322},{\"end\":49330,\"start\":49328},{\"end\":49341,\"start\":49334},{\"end\":49494,\"start\":49487},{\"end\":49506,\"start\":49500},{\"end\":49766,\"start\":49764},{\"end\":49775,\"start\":49772},{\"end\":49782,\"start\":49779},{\"end\":49792,\"start\":49786},{\"end\":49799,\"start\":49796},{\"end\":49806,\"start\":49803},{\"end\":49814,\"start\":49812},{\"end\":49822,\"start\":49818},{\"end\":50212,\"start\":50205},{\"end\":50221,\"start\":50216},{\"end\":50231,\"start\":50225},{\"end\":50425,\"start\":50415},{\"end\":50437,\"start\":50429},{\"end\":50451,\"start\":50443},{\"end\":50459,\"start\":50455},{\"end\":50472,\"start\":50463},{\"end\":50660,\"start\":50652},{\"end\":50810,\"start\":50802},{\"end\":50821,\"start\":50814},{\"end\":50832,\"start\":50825},{\"end\":50841,\"start\":50836},{\"end\":51068,\"start\":51058},{\"end\":51078,\"start\":51072},{\"end\":51305,\"start\":51303},{\"end\":51317,\"start\":51309},{\"end\":51327,\"start\":51321},{\"end\":51339,\"start\":51331},{\"end\":51471,\"start\":51469},{\"end\":51480,\"start\":51475},{\"end\":51487,\"start\":51484},{\"end\":51494,\"start\":51491},{\"end\":51693,\"start\":51684},{\"end\":51699,\"start\":51697},{\"end\":51710,\"start\":51705},{\"end\":51718,\"start\":51714},{\"end\":51728,\"start\":51722},{\"end\":51748,\"start\":51732},{\"end\":52074,\"start\":52072},{\"end\":52083,\"start\":52078},{\"end\":52093,\"start\":52087},{\"end\":52103,\"start\":52097},{\"end\":52111,\"start\":52107},{\"end\":52334,\"start\":52327},{\"end\":52343,\"start\":52338},{\"end\":52351,\"start\":52347},{\"end\":52360,\"start\":52357},{\"end\":52369,\"start\":52364},{\"end\":52379,\"start\":52373},{\"end\":52390,\"start\":52385},{\"end\":52401,\"start\":52394},{\"end\":52727,\"start\":52722},{\"end\":52734,\"start\":52731},{\"end\":52741,\"start\":52738},{\"end\":52750,\"start\":52745},{\"end\":52766,\"start\":52756},{\"end\":52959,\"start\":52954},{\"end\":52966,\"start\":52963},{\"end\":52973,\"start\":52970},{\"end\":52981,\"start\":52977},{\"end\":53229,\"start\":53219},{\"end\":53242,\"start\":53233},{\"end\":53254,\"start\":53248},{\"end\":53457,\"start\":53452},{\"end\":53465,\"start\":53461},{\"end\":53643,\"start\":53640},{\"end\":53651,\"start\":53647},{\"end\":53856,\"start\":53853},{\"end\":54090,\"start\":54087},{\"end\":54100,\"start\":54094},{\"end\":54112,\"start\":54104},{\"end\":54118,\"start\":54116},{\"end\":54131,\"start\":54122},{\"end\":54143,\"start\":54135},{\"end\":54336,\"start\":54333},{\"end\":54345,\"start\":54340},{\"end\":54357,\"start\":54349},{\"end\":54363,\"start\":54361},{\"end\":54373,\"start\":54367},{\"end\":54549,\"start\":54546},{\"end\":54558,\"start\":54553},{\"end\":54570,\"start\":54562},{\"end\":54578,\"start\":54574},{\"end\":54588,\"start\":54582},{\"end\":54599,\"start\":54592},{\"end\":54609,\"start\":54603},{\"end\":54622,\"start\":54615},{\"end\":54842,\"start\":54839},{\"end\":54854,\"start\":54846},{\"end\":54863,\"start\":54858},{\"end\":54874,\"start\":54867},{\"end\":54882,\"start\":54878},{\"end\":54890,\"start\":54888},{\"end\":54900,\"start\":54896},{\"end\":55088,\"start\":55080},{\"end\":55100,\"start\":55092},{\"end\":55111,\"start\":55104},{\"end\":55887,\"start\":55878},{\"end\":56244,\"start\":56239},{\"end\":56259,\"start\":56248},{\"end\":56269,\"start\":56263},{\"end\":56422,\"start\":56420},{\"end\":56563,\"start\":56559},{\"end\":56571,\"start\":56567},{\"end\":56584,\"start\":56577},{\"end\":56593,\"start\":56588},{\"end\":56605,\"start\":56597},{\"end\":56818,\"start\":56815},{\"end\":56825,\"start\":56822},{\"end\":56833,\"start\":56829},{\"end\":56844,\"start\":56839},{\"end\":56852,\"start\":56850},{\"end\":57058,\"start\":57055},{\"end\":57066,\"start\":57062},{\"end\":57077,\"start\":57072},{\"end\":57085,\"start\":57083},{\"end\":57432,\"start\":57424},{\"end\":57779,\"start\":57776},{\"end\":57786,\"start\":57783},{\"end\":57794,\"start\":57792},{\"end\":57802,\"start\":57798},{\"end\":57810,\"start\":57806},{\"end\":57817,\"start\":57814},{\"end\":58048,\"start\":58040},{\"end\":58056,\"start\":58052},{\"end\":58064,\"start\":58060},{\"end\":58234,\"start\":58230},{\"end\":58244,\"start\":58238},{\"end\":58258,\"start\":58248},{\"end\":58267,\"start\":58262},{\"end\":58398,\"start\":58393},{\"end\":58407,\"start\":58402},{\"end\":58420,\"start\":58413},{\"end\":58433,\"start\":58424},{\"end\":58662,\"start\":58657},{\"end\":58668,\"start\":58666},{\"end\":58677,\"start\":58672},{\"end\":58686,\"start\":58681},{\"end\":58693,\"start\":58690},{\"end\":58699,\"start\":58697},{\"end\":59035,\"start\":59030},{\"end\":59047,\"start\":59039},{\"end\":59055,\"start\":59051},{\"end\":59067,\"start\":59059},{\"end\":59077,\"start\":59071},{\"end\":59089,\"start\":59081},{\"end\":59275,\"start\":59270},{\"end\":59284,\"start\":59279},{\"end\":59292,\"start\":59288},{\"end\":59298,\"start\":59296},{\"end\":59306,\"start\":59302},{\"end\":59589,\"start\":59586},{\"end\":59597,\"start\":59593},{\"end\":59606,\"start\":59601},{\"end\":59617,\"start\":59612},{\"end\":59830,\"start\":59826},{\"end\":59841,\"start\":59836},{\"end\":59851,\"start\":59845},{\"end\":59860,\"start\":59857},{\"end\":59870,\"start\":59864},{\"end\":59878,\"start\":59876}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1908.02983\",\"id\":\"b0\"},\"end\":47345,\"start\":46999},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":8307266},\"end\":47513,\"start\":47347},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":213757781},\"end\":47906,\"start\":47515},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":146808485},\"end\":48236,\"start\":47908},{\"attributes\":{\"id\":\"b4\"},\"end\":48432,\"start\":48238},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":196208260},\"end\":48700,\"start\":48434},{\"attributes\":{\"doi\":\"arXiv:1909.13719\",\"id\":\"b6\"},\"end\":49009,\"start\":48702},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":7428689},\"end\":49237,\"start\":49011},{\"attributes\":{\"id\":\"b8\"},\"end\":49483,\"start\":49239},{\"attributes\":{\"doi\":\"arXiv:1708.04552\",\"id\":\"b9\"},\"end\":49760,\"start\":49485},{\"attributes\":{\"doi\":\"arXiv:1912.05027\",\"id\":\"b10\"},\"end\":50127,\"start\":49762},{\"attributes\":{\"id\":\"b11\"},\"end\":50361,\"start\":50129},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":4246903},\"end\":50636,\"start\":50363},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":206770307},\"end\":50716,\"start\":50638},{\"attributes\":{\"id\":\"b14\"},\"end\":51004,\"start\":50718},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":7890982},\"end\":51287,\"start\":51006},{\"attributes\":{\"id\":\"b16\"},\"end\":51419,\"start\":51289},{\"attributes\":{\"id\":\"b17\"},\"end\":51601,\"start\":51421},{\"attributes\":{\"doi\":\"arXiv:1912.02781\",\"id\":\"b18\"},\"end\":51984,\"start\":51603},{\"attributes\":{\"doi\":\"arXiv:1905.05393\",\"id\":\"b19\"},\"end\":52323,\"start\":51986},{\"attributes\":{\"doi\":\"arXiv:1711.03213\",\"id\":\"b20\"},\"end\":52681,\"start\":52325},{\"attributes\":{\"id\":\"b21\"},\"end\":52885,\"start\":52683},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":202782547},\"end\":53150,\"start\":52887},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":195908774},\"end\":53398,\"start\":53152},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":13123084},\"end\":53586,\"start\":53400},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":51923817},\"end\":53750,\"start\":53588},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":18507866},\"end\":54034,\"start\":53752},{\"attributes\":{\"id\":\"b27\"},\"end\":54288,\"start\":54036},{\"attributes\":{\"id\":\"b28\"},\"end\":54497,\"start\":54290},{\"attributes\":{\"id\":\"b29\"},\"end\":54799,\"start\":54499},{\"attributes\":{\"id\":\"b30\"},\"end\":55039,\"start\":54801},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":628455},\"end\":55747,\"start\":55041},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":120764023},\"end\":56160,\"start\":55749},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":3238213},\"end\":56414,\"start\":56162},{\"attributes\":{\"id\":\"b34\"},\"end\":56492,\"start\":56416},{\"attributes\":{\"id\":\"b35\"},\"end\":56754,\"start\":56494},{\"attributes\":{\"doi\":\"arXiv:1904.12848\",\"id\":\"b36\"},\"end\":57051,\"start\":56756},{\"attributes\":{\"doi\":\"arXiv:1911.04252\",\"id\":\"b37\"},\"end\":57352,\"start\":57053},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":1487550},\"end\":57685,\"start\":57354},{\"attributes\":{\"id\":\"b39\"},\"end\":57985,\"start\":57687},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":102350583},\"end\":58177,\"start\":57987},{\"attributes\":{\"id\":\"b41\"},\"end\":58389,\"start\":58179},{\"attributes\":{\"doi\":\"arXiv:1710.09412\",\"id\":\"b42\"},\"end\":58653,\"start\":58391},{\"attributes\":{\"doi\":\"arXiv:1902.04103\",\"id\":\"b43\"},\"end\":58956,\"start\":58655},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":3559264},\"end\":59266,\"start\":58958},{\"attributes\":{\"doi\":\"arXiv:1708.04896\",\"id\":\"b45\"},\"end\":59499,\"start\":59268},{\"attributes\":{\"id\":\"b46\"},\"end\":59762,\"start\":59501},{\"attributes\":{\"doi\":\"arXiv:1906.11172\",\"id\":\"b47\"},\"end\":60094,\"start\":59764}]", "bib_title": "[{\"end\":47377,\"start\":47347},{\"end\":47605,\"start\":47515},{\"end\":47965,\"start\":47908},{\"end\":48489,\"start\":48434},{\"end\":49080,\"start\":49011},{\"end\":50411,\"start\":50363},{\"end\":50648,\"start\":50638},{\"end\":51054,\"start\":51006},{\"end\":52950,\"start\":52887},{\"end\":53215,\"start\":53152},{\"end\":53448,\"start\":53400},{\"end\":53636,\"start\":53588},{\"end\":53847,\"start\":53752},{\"end\":55076,\"start\":55041},{\"end\":55872,\"start\":55749},{\"end\":56235,\"start\":56162},{\"end\":57420,\"start\":57354},{\"end\":58036,\"start\":57987},{\"end\":59026,\"start\":58958}]", "bib_author": "[{\"end\":47079,\"start\":47070},{\"end\":47089,\"start\":47079},{\"end\":47099,\"start\":47089},{\"end\":47118,\"start\":47099},{\"end\":47132,\"start\":47118},{\"end\":47390,\"start\":47379},{\"end\":47402,\"start\":47390},{\"end\":47412,\"start\":47402},{\"end\":47620,\"start\":47607},{\"end\":47631,\"start\":47620},{\"end\":47642,\"start\":47631},{\"end\":47653,\"start\":47642},{\"end\":47661,\"start\":47653},{\"end\":47670,\"start\":47661},{\"end\":47680,\"start\":47670},{\"end\":47980,\"start\":47967},{\"end\":47991,\"start\":47980},{\"end\":48005,\"start\":47991},{\"end\":48017,\"start\":48005},{\"end\":48027,\"start\":48017},{\"end\":48039,\"start\":48027},{\"end\":48304,\"start\":48297},{\"end\":48319,\"start\":48304},{\"end\":48502,\"start\":48491},{\"end\":48510,\"start\":48502},{\"end\":48518,\"start\":48510},{\"end\":48531,\"start\":48518},{\"end\":48539,\"start\":48531},{\"end\":48779,\"start\":48768},{\"end\":48787,\"start\":48779},{\"end\":48797,\"start\":48787},{\"end\":48805,\"start\":48797},{\"end\":49089,\"start\":49082},{\"end\":49095,\"start\":49089},{\"end\":49101,\"start\":49095},{\"end\":49108,\"start\":49101},{\"end\":49300,\"start\":49292},{\"end\":49308,\"start\":49300},{\"end\":49318,\"start\":49308},{\"end\":49326,\"start\":49318},{\"end\":49332,\"start\":49326},{\"end\":49343,\"start\":49332},{\"end\":49496,\"start\":49485},{\"end\":49508,\"start\":49496},{\"end\":49768,\"start\":49762},{\"end\":49777,\"start\":49768},{\"end\":49784,\"start\":49777},{\"end\":49794,\"start\":49784},{\"end\":49801,\"start\":49794},{\"end\":49808,\"start\":49801},{\"end\":49816,\"start\":49808},{\"end\":49824,\"start\":49816},{\"end\":50214,\"start\":50203},{\"end\":50223,\"start\":50214},{\"end\":50233,\"start\":50223},{\"end\":50427,\"start\":50413},{\"end\":50439,\"start\":50427},{\"end\":50453,\"start\":50439},{\"end\":50461,\"start\":50453},{\"end\":50474,\"start\":50461},{\"end\":50662,\"start\":50650},{\"end\":50812,\"start\":50800},{\"end\":50823,\"start\":50812},{\"end\":50834,\"start\":50823},{\"end\":50843,\"start\":50834},{\"end\":51070,\"start\":51056},{\"end\":51080,\"start\":51070},{\"end\":51307,\"start\":51301},{\"end\":51319,\"start\":51307},{\"end\":51329,\"start\":51319},{\"end\":51341,\"start\":51329},{\"end\":51473,\"start\":51467},{\"end\":51482,\"start\":51473},{\"end\":51489,\"start\":51482},{\"end\":51496,\"start\":51489},{\"end\":51695,\"start\":51682},{\"end\":51701,\"start\":51695},{\"end\":51712,\"start\":51701},{\"end\":51720,\"start\":51712},{\"end\":51730,\"start\":51720},{\"end\":51750,\"start\":51730},{\"end\":52076,\"start\":52070},{\"end\":52085,\"start\":52076},{\"end\":52095,\"start\":52085},{\"end\":52105,\"start\":52095},{\"end\":52113,\"start\":52105},{\"end\":52336,\"start\":52325},{\"end\":52345,\"start\":52336},{\"end\":52353,\"start\":52345},{\"end\":52362,\"start\":52353},{\"end\":52371,\"start\":52362},{\"end\":52381,\"start\":52371},{\"end\":52392,\"start\":52381},{\"end\":52403,\"start\":52392},{\"end\":52729,\"start\":52720},{\"end\":52736,\"start\":52729},{\"end\":52743,\"start\":52736},{\"end\":52752,\"start\":52743},{\"end\":52768,\"start\":52752},{\"end\":52961,\"start\":52952},{\"end\":52968,\"start\":52961},{\"end\":52975,\"start\":52968},{\"end\":52983,\"start\":52975},{\"end\":53231,\"start\":53217},{\"end\":53244,\"start\":53231},{\"end\":53256,\"start\":53244},{\"end\":53459,\"start\":53450},{\"end\":53467,\"start\":53459},{\"end\":53645,\"start\":53638},{\"end\":53653,\"start\":53645},{\"end\":53858,\"start\":53849},{\"end\":54092,\"start\":54083},{\"end\":54102,\"start\":54092},{\"end\":54114,\"start\":54102},{\"end\":54120,\"start\":54114},{\"end\":54133,\"start\":54120},{\"end\":54145,\"start\":54133},{\"end\":54338,\"start\":54329},{\"end\":54347,\"start\":54338},{\"end\":54359,\"start\":54347},{\"end\":54365,\"start\":54359},{\"end\":54375,\"start\":54365},{\"end\":54551,\"start\":54542},{\"end\":54560,\"start\":54551},{\"end\":54572,\"start\":54560},{\"end\":54580,\"start\":54572},{\"end\":54590,\"start\":54580},{\"end\":54601,\"start\":54590},{\"end\":54611,\"start\":54601},{\"end\":54624,\"start\":54611},{\"end\":54844,\"start\":54837},{\"end\":54856,\"start\":54844},{\"end\":54865,\"start\":54856},{\"end\":54876,\"start\":54865},{\"end\":54884,\"start\":54876},{\"end\":54892,\"start\":54884},{\"end\":54902,\"start\":54892},{\"end\":55090,\"start\":55078},{\"end\":55102,\"start\":55090},{\"end\":55113,\"start\":55102},{\"end\":55889,\"start\":55874},{\"end\":56246,\"start\":56237},{\"end\":56261,\"start\":56246},{\"end\":56271,\"start\":56261},{\"end\":56424,\"start\":56418},{\"end\":56565,\"start\":56557},{\"end\":56573,\"start\":56565},{\"end\":56586,\"start\":56573},{\"end\":56595,\"start\":56586},{\"end\":56607,\"start\":56595},{\"end\":56820,\"start\":56813},{\"end\":56827,\"start\":56820},{\"end\":56835,\"start\":56827},{\"end\":56846,\"start\":56835},{\"end\":56854,\"start\":56846},{\"end\":57060,\"start\":57053},{\"end\":57068,\"start\":57060},{\"end\":57079,\"start\":57068},{\"end\":57087,\"start\":57079},{\"end\":57434,\"start\":57422},{\"end\":57781,\"start\":57774},{\"end\":57788,\"start\":57781},{\"end\":57796,\"start\":57788},{\"end\":57804,\"start\":57796},{\"end\":57812,\"start\":57804},{\"end\":57819,\"start\":57812},{\"end\":58050,\"start\":58038},{\"end\":58058,\"start\":58050},{\"end\":58066,\"start\":58058},{\"end\":58236,\"start\":58228},{\"end\":58246,\"start\":58236},{\"end\":58260,\"start\":58246},{\"end\":58269,\"start\":58260},{\"end\":58400,\"start\":58391},{\"end\":58409,\"start\":58400},{\"end\":58422,\"start\":58409},{\"end\":58435,\"start\":58422},{\"end\":58664,\"start\":58655},{\"end\":58670,\"start\":58664},{\"end\":58679,\"start\":58670},{\"end\":58688,\"start\":58679},{\"end\":58695,\"start\":58688},{\"end\":58701,\"start\":58695},{\"end\":59037,\"start\":59028},{\"end\":59049,\"start\":59037},{\"end\":59057,\"start\":59049},{\"end\":59069,\"start\":59057},{\"end\":59079,\"start\":59069},{\"end\":59091,\"start\":59079},{\"end\":59277,\"start\":59268},{\"end\":59286,\"start\":59277},{\"end\":59294,\"start\":59286},{\"end\":59300,\"start\":59294},{\"end\":59308,\"start\":59300},{\"end\":59591,\"start\":59582},{\"end\":59599,\"start\":59591},{\"end\":59608,\"start\":59599},{\"end\":59619,\"start\":59608},{\"end\":59832,\"start\":59824},{\"end\":59843,\"start\":59832},{\"end\":59853,\"start\":59843},{\"end\":59862,\"start\":59853},{\"end\":59872,\"start\":59862},{\"end\":59880,\"start\":59872}]", "bib_venue": "[{\"end\":55404,\"start\":55267},{\"end\":47068,\"start\":46999},{\"end\":47419,\"start\":47412},{\"end\":47684,\"start\":47680},{\"end\":48058,\"start\":48039},{\"end\":48295,\"start\":48238},{\"end\":48555,\"start\":48539},{\"end\":48766,\"start\":48702},{\"end\":49112,\"start\":49108},{\"end\":49290,\"start\":49239},{\"end\":49592,\"start\":49524},{\"end\":49915,\"start\":49840},{\"end\":50201,\"start\":50129},{\"end\":50478,\"start\":50474},{\"end\":50666,\"start\":50662},{\"end\":50798,\"start\":50718},{\"end\":51129,\"start\":51080},{\"end\":51299,\"start\":51289},{\"end\":51465,\"start\":51421},{\"end\":51680,\"start\":51603},{\"end\":52068,\"start\":51986},{\"end\":52473,\"start\":52419},{\"end\":52718,\"start\":52683},{\"end\":53005,\"start\":52983},{\"end\":53263,\"start\":53256},{\"end\":53483,\"start\":53467},{\"end\":53661,\"start\":53653},{\"end\":53884,\"start\":53858},{\"end\":54081,\"start\":54036},{\"end\":54327,\"start\":54290},{\"end\":54540,\"start\":54499},{\"end\":54835,\"start\":54801},{\"end\":55265,\"start\":55113},{\"end\":55936,\"start\":55889},{\"end\":56275,\"start\":56271},{\"end\":56555,\"start\":56494},{\"end\":56811,\"start\":56756},{\"end\":57168,\"start\":57103},{\"end\":57502,\"start\":57434},{\"end\":57772,\"start\":57687},{\"end\":58070,\"start\":58066},{\"end\":58226,\"start\":58179},{\"end\":58492,\"start\":58451},{\"end\":58778,\"start\":58717},{\"end\":59097,\"start\":59091},{\"end\":59356,\"start\":59324},{\"end\":59580,\"start\":59501},{\"end\":59822,\"start\":59764}]"}}}, "year": 2023, "month": 12, "day": 17}