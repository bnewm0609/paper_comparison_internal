{"id": 261530763, "updated": "2023-12-13 03:49:14.186", "metadata": {"title": "QuantEase: Optimization-based Quantization for Language Models", "authors": "[{\"first\":\"Kayhan\",\"last\":\"Behdin\",\"middle\":[]},{\"first\":\"Ayan\",\"last\":\"Acharya\",\"middle\":[]},{\"first\":\"Aman\",\"last\":\"Gupta\",\"middle\":[]},{\"first\":\"Qingquan\",\"last\":\"Song\",\"middle\":[]},{\"first\":\"Siyu\",\"last\":\"Zhu\",\"middle\":[]},{\"first\":\"Sathiya\",\"last\":\"Keerthi\",\"middle\":[]},{\"first\":\"Rahul\",\"last\":\"Mazumder\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "With the rising popularity of Large Language Models (LLMs), there has been an increasing interest in compression techniques that enable their efficient deployment. This study focuses on the Post-Training Quantization (PTQ) of LLMs. Drawing from recent advances, our work introduces QuantEase, a layer-wise quantization framework where individual layers undergo separate quantization. The problem is framed as a discrete-structured non-convex optimization, prompting the development of algorithms rooted in Coordinate Descent (CD) techniques. These CD-based methods provide high-quality solutions to the complex non-convex layer-wise quantization problems. Notably, our CD-based approach features straightforward updates, relying solely on matrix and vector operations, circumventing the need for matrix inversion or decomposition. We also explore an outlier-aware variant of our approach, allowing for retaining significant weights (outliers) with complete precision. Our proposal attains state-of-the-art performance in terms of perplexity and zero-shot accuracy in empirical evaluations across various LLMs and datasets, with relative improvements up to 15% over methods such as GPTQ. Leveraging careful linear algebra optimizations, QuantEase can quantize models like Falcon-180B on a single NVIDIA A100 GPU in $\\sim$3 hours. Particularly noteworthy is our outlier-aware algorithm's capability to achieve near or sub-3-bit quantization of LLMs with an acceptable drop in accuracy, obviating the need for non-uniform quantization or grouping techniques, improving upon methods such as SpQR by up to two times in terms of perplexity.", "fields_of_study": "[\"Mathematics\",\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2309-01885", "doi": "10.48550/arxiv.2309.01885"}}, "content": {"source": {"pdf_hash": "4280793a1b52794ca9ecb891e302185557edb8d4", "pdf_src": "ArXiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2309.01885v2.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://arxiv.org/pdf/2309.01885", "status": "CLOSED"}}, "grobid": {"id": "5e34e8ddaad84e58fd8dc58455da69d2c5878c9d", "type": "plain-text", "url": "s3://ai2-s2-science-parse-plus-prod/parse-results/s2orc_worker/4280793a1b52794ca9ecb891e302185557edb8d4.txt", "contents": "\nQuantEase: Optimization-based Quantization for Language Models\n1 Dec 2023\n\nKayhan Behdin behdin1675@gmail.com \nLinkedIn\nSunnyvaleCA\n\nMassachusetts Institute of Technology\nCambridgeMA\n\nAyan Acharya \nLinkedIn\nSunnyvaleCA\n\nAman Gupta \nLinkedIn\nSunnyvaleCA\n\nQingquan Song \nLinkedIn\nSunnyvaleCA\n\nSiyu Zhu \nLinkedIn\nSunnyvaleCA\n\nSathiya Keerthi \nLinkedIn\nSunnyvaleCA\n\nRahul Mazumder \nLinkedIn\nSunnyvaleCA\n\nMassachusetts Institute of Technology\nCambridgeMA\n\nQuantEase: Optimization-based Quantization for Language Models\n1 Dec 202355B5CE448A4137D85CA7CAF1DD2DC1F6arXiv:2309.01885v2[stat.ML]\nWith the rising popularity of Large Language Models (LLMs), there has been an increasing interest in compression techniques that enable their efficient deployment.This study focuses on the Post-Training Quantization (PTQ) of LLMs.Drawing from recent advances, our work introduces QuantEase, a layer-wise quantization framework where individual layers undergo separate quantization.The problem is framed as a discrete-structured non-convex optimization, prompting the development of algorithms rooted in Coordinate Descent (CD) techniques.These CD-based methods provide high-quality solutions to the complex non-convex layer-wise quantization problems.Notably, our CD-based approach features straightforward updates, relying solely on matrix and vector operations, circumventing the need for matrix inversion or decomposition.We also explore an outlier-aware variant of our approach, allowing for retaining significant weights (outliers) with complete precision.Our proposal attains state-of-the-art performance in terms of perplexity and zero-shot accuracy in empirical evaluations across various LLMs and datasets, with relative improvements up to 15% over methods such as GPTQ.Leveraging careful linear algebra optimizations, QuantEase can quantize models like Falcon-180B on a single NVIDIA A100 GPU in \u223c3 hours.Particularly noteworthy is our outlier-aware algorithm's capability to achieve near or sub-3-bit quantization of LLMs with an acceptable drop in accuracy, obviating the need for non-uniform quantization or grouping techniques, improving upon methods such as SpQR by up to two times in terms of perplexity.\n\nIntroduction\n\nRecent years have witnessed an explosive emergence of Large Language Models (LLMs) (Brown et al., 2020;Devlin et al., 2018;Lauren\u00e7on et al., 2022;Radford et al., 2019;Touvron et al., 2023a,b;Zhang et al., 2022) and their ability to solve complex language modelling tasks for settings like zero-shot or instruction finetuning (OpenAI, 2023;Wei et al., 2022a).Consequently, there has been increased interest to utilize LLMs for real-world use cases.\n\nThe success of LLMs can be attributed to an increase in training data size and the number of model parameters (Hoffmann et al., 2022;Kaplan et al., 2020).As a result, modern LLMs have ballooned to hundreds of billions of parameters in size (Brown et al., 2020;Zhang et al., 2022).While the ability of these models to solve tasks is remarkable, efficiently serving them remains a formidable challenge.For instance, the GPT3 model (Brown et al., 2020) contains approximately 175 billion parameters and has a memory footprint of more than 300GB.Consequently, deploying such large models on a single contemporary GPU (such as NVIDIA A6000, A100, H100) for inference has become infeasible.Another notable challenge is increased inference latency, which can prove detrimental for practical, real-world applications (Frantar et al., 2023).\n\nModel compression has emerged as a viable approach to tackle the critical challenges of storage footprint and inference speed for LLMs (Hoefler et al., 2021).Within the modern landscape of deep learning research, numerous techniques exist that leverage weight sparsification or quantization to compress large models (Agustsson et al., 2017;Bulat and Tzimiropoulos, 2019;Rastegari et al., 2016).Since modern LLMs take significant compute resources, time, and potentially millions of dollars to train, compression-aware retraining is generally not practicable.This makes post-training quantization (PTQ) an attractive proposition.While numerous practical PTQ algorithms have already been developed (Frantar and Alistarh, 2022;Hubara et al., 2021a;Nagel et al., 2020), it is only in the most recent past that algorithms capable of effectively and efficiently quantizing and/or sparsifying extremely large LLMs have become available.Among such methods, prominent techniques include GPTQ (Frantar et al., 2023), SpQR (Dettmers et al., 2023) and AWQ (Lin et al., 2023), among others.These methods aim to compress a 16-bit model into 3 or 4 bits while striving to maintain predictive accuracy.Despite promising progress in this realm, a discernible drop in the performance of quantized models persists compared to their unquantized counterparts.In this paper, we focus on developing a new algorithm for PTQ of network weights of LLMs. Figure 1: Zero-Shot accuracy on the LAMBADA (Paperno et al., 2016) benchmark for 3-bit quantization.See Section 5.3 for more details on experimental setup.QuantEase consistently outperforms methods like GPTQ and AWQ, sometimes by 15% in terms of relative improvement.\n\n\nOur Approach\n\nIn this paper, we propose QuantEase -a new algorithm to obtain high-quality feasible (i.e.quantized) solutions to layerwise post-training quantization of LLMs.QuantEase leverages cyclic Coordinate Descent (CD) (Tseng, 2001).CD-type methods have traditionally been used to address statistical problems (Behdin et al., 2023;Chang and Lin, 2011;Friedman et al., 2010;Hazimeh and Mazumder, 2020;Mazumder and Hastie, 2012;Shevade and Keerthi, 2003).We show that QuantEase is a more principled optimization method when compared to methods like GPTQ, guaranteeing a non-increasing sequence of the objective value under feasibility of the initial solution.QuantEase cycles through coordinates in each layer, updating each weight such that it minimizes the objective while keeping other weights fixed.This allows for simple closed-form updates for each weight.Empirically, this results in up to 30% improvement in relative quantization error over GPTQ for 4-bit and 3-bit quantization (see Figure 2).QuantEase's simplicity allows it to have several advantages over other methods, while making it extremely scalable and easy to use.Unlike other methods, QuantEase does not require expensive matrix inversion or Cholesky factorization.This helps avoid numerical issues while also lowering memory requirements.QuantEase is extremely easy to implement, making it a drop-in replacement for most other methods.Remarkably, QuantEase can efficiently quantize a 66B parameter model on a single NVIDIA V100 GPU, whereas methods like GPTQ and AWQ run out of memory.QuantEase is also extremely time efficient and is able to quantize extremely large models like Falcon-180b (Penedo et al., 2023) within a few hours, where each iteration has speed that is comparable to a well-optimized implementation of GPTQ.\n\nQuantEase's effectiveness in achieving lower quantization error also translates to improvements on language modeling benchmarks.Experiments on several LLM model families (Lauren\u00e7on et al., 2022;Penedo et al., 2023;Zhang et al., 2022) and language tasks show that QuantEase outperforms state-of-the-art uniform quantization methods such as GPTQ and AWQ in terms of perplexity and zero-shot accuracy (Paperno et al., 2016), both in the 4-bit and 3-bit regimes (see Tables 1, 2, 3 and Figures 1, 4).For the 3-bit regime, QuantEase is especially effective for zero-shot accuracy, achieving strong relative improvements (up to 15%) over GPTQ (see Figure 1).\n\nWe also propose a variant of QuantEase to handle weight outliers.This is achieved by dividing the set of layer weights into a group of quantized weights and very few unquantized ones.We propose a block coordinate descent method based on iterative hard thresholding (Blumensath and Davies, 2009) for the outlier-aware version of our method.This version of QuantEase is able to improve upon outlier-based models such as SpQR.Particularly, we show this outlier-aware method can achieve acceptable accuracy for sub-3 bit regimes, improving upon current available methods such as SpQR by up to 2.5 times in terms of perplexity.We hope that the simplicity and effectiveness of QuantEase inspires further research in this area.\n\nOur contributions can be summarised as follows:\n\n1. Principled optimization -We propose QuantEase, an optimization framework for post-training quantization of LLMs based on minimizing a layerwise (least squares) reconstruction error.We propose a CD framework updating network weights one-at-a-time, avoiding memory-expensive matrix inversions/factorizations. Particularly, we make the CD updates efficient by exploiting the problem structure resulting in closed form updates for weights.\n\n2. Outlier awareness -We also propose an outlier-aware version of our framework where a few (outlier) weights are kept unquantized.We discuss an algorithm for outlier-aware QuantEase based on block coordinate descent and iterative hard thresholding.\n\n3. Improved accuracy -Experiments on LLMs with billions of parameters show that QuantEase outperforms recent PTQ methods such as GPTQ and AWQ in text generation and zero-shot tasks for 3 and 4-bit quantization, often by a large margin.Additional experiments show that the outlier-aware QuantEase outperforms methods such as SpQR by up to 2.5 times in terms of perplexity, in the near-3 or sub-3 bit quantization regimes.\n\n\nRelated Work\n\nRecently, there has been a mounting interest in the layerwise quantization of LLMs.One prominent method for post-training quantization of LLMs is GPTQ (Frantar et al., 2023).GPTQ extends the Optimal Brain Surgeon (OBS) framework (Frantar and Alistarh, 2022;Hassibi and Stork, 1992;LeCun et al., 1989), incorporating strategies for layerwise compression (Dong et al., 2017).Empirical evaluations of GPTQ demonstrate encouraging results, revealing a marginal drop in accuracy in text generation and zero-shot benchmarks.\n\nAnother avenue for achieving layer-wise quantization of LLMs is the recent work by Lin et al. (2023), referred to as AWQ.This approach centres on preserving the weights that influence activations most.GPTQ and AWQ represent two prominent foundational techniques, and we will compare our methodology against these in our numerical experiments.\n\nIt is widely acknowledged that transformer models, inclusive of LLMs, confront challenges tied to outliers when undergoing quantization to lower bit-widths (Bondarenko et al., 2021;Kim et al., 2023;Wei et al., 2022b).This predicament arises from the notable impact of extremely large or small weights on the quantization range, thereby leading to supplementary errors.As a result, specific research endeavours delve into the notion of non-uniform quantization.SqueezeLLM (Kim et al., 2023) seeks to identify and preserve outlier weights (for example, very large or small weights) that might affect the output the most, allowing for improved accuracy.Similarly, SpQR (Dettmers et al., 2023) combines GPTQ with outlier detection to achieve a lower loss of accuracy.We use SpQR as a benchmark in our experiments with outlier detection.\n\nA long line of work discuss and study quantization methods for large neural networks from an implementation perspective, including hardware-level optimizations for low-bit calculations and inference time activation quantization, which are beyond the scope of this paper.For a more exhaustive exposition, please check Gholami et al. (2022); Hubara et al. (2021b); Wang et al. (2020); Xiao et al. (2023); Yao et al. (2022Yao et al. ( , 2023) ) and the references therein.\n\n\nBackground\n\n2.1 Problem Formulation: Layerwise Quantization A standard approach to LLM compression is layerwise compression (Dong et al., 2017), where layers are compressed/quantized one at a time.This allows the task of compressing a very large network to be broken down into compressing several smaller layers, which is more practical than simultaneously quantizing multiple layers.In this paper, we pursue a layerwise framework for quantization.\n\nFocusing on layerwise quantization, let us consider a linear layer with some (nonlinear) activation function.Within this context, let X \u2208 R p\u00d7n represent the input matrix feeding into this particular layer, where p denotes the number of input features and n denotes the number of training data points fed through the network.Additionally, let W \u2208 R q\u00d7p symbolize the weights matrix corresponding to this layer, characterized by q output channels.\n\nFor a given output channel i \u2208 [q], we designate the predetermined, finite set of per-channel quantization levels for this channel as Q i \u2286 R. In this work, following Frantar et al. (2023), we focus on the case where the quantization levels within Q i are uniformly spaced (We note however, that our approach can extend to quantization schemes that do not follow this assumption.).We can then formulate the layerwise quantization task as the following optimization problem:\nmin \u0174 f ( \u0174 ) := \u2225W X \u2212 \u0174 X\u2225 2 F s.t. \u0174i,j \u2208 Q i , (i, j) \u2208 [q] \u00d7 [p].(1)\nThe objective of Problem (1) captures the distance between the original pre-activation output obtained from unquantized weights (W X), and the pre-activation output stemming from quantized weights ( \u0174 X), subject to \u0174 adhering to quantization constraints.\n\nNotation: For i \u2208 [q] := {1, . . ., q}, we define the quantization operator q i with respect to quantization levels Q i as follows:\nq i (x) \u2208 argmin y\u2208Qi (x \u2212 y) 2 . (2)\nFor a matrix such as A, \u2225A\u2225 F denotes its Frobenius norm.Moreover, for i < j, A :,i and A :,i:j denote the i-th column, and columns i to j of A, respectively.The proofs of the main results are relegated to Appendix B.\n\n\nTwo Key Algorithms\n\n\nGPTQ\n\nAs mentioned earlier, GPTQ (Frantar et al., 2023) extends the OBS-based framework of Frantar and Alistarh (2022); Hassibi and Stork (1992).GPTQ performs quantization of W one column at a time.Specifically, GPTQ starts with the initialization, \u0174 \u2190 W .Then, it cycles through columns j = 1, \u2022 \u2022 \u2022 , p and for each j, it quantizes column j of \u0174 .For the j-th column it quantizes all its entries via the updates: \u0174 + i,j = q i ( \u0174i,j ), i \u2208 [q].After updating the j-th column, GPTQ proceeds to update the other weights in the layer to ensure that the error in (1) does not increase too much.To make our exposition resemble that of the OBS framework, we note that GPTQ updates \u0174 :,j+1:p by approximately solving the least-squares problem: min\n\u0174 :,j:p \u2225W X \u2212 \u0174 X\u2225 2 F s.t. \u0174 :,j = \u0174 + :,j ,(3)\nwhere the constraint above implies that we are effectively optimizing over \u0174 :j+1:p (but we choose this representation following Frantar and Alistarh (2022); Hassibi and Stork (1992)).We note that in (3), the quantization constraints are dropped as otherwise, (3) would be as hard as (1) in general.Moreover, entries up to the j-th column i.e, \u0174 :,1:j are not updated to ensure they remained quantized.Since the OBS framework is set up to optimize a homogeneous quadratic\n\nwhere\nA = (W :,1:j\u22121 \u2212 \u0174 :,1:j\u22121 )X 1:j\u22121,: .(5)\nUpon inspection one can see that the objective in ( 4) is not homogeneous quadratic, and hence does not fit into the OBS framework.Therefore, A is dropped and this problem is replaced by the formulation:\nmin \u0174 :,j:p \u2225(W :,j \u2212 \u0174 :,j )X j,: + (W :,j+1:p \u2212 \u0174 :,j+1:p )X j+1:p,: \u2225 2 F s.t. \u0174 :,j = \u0174 + :,j(a)\n= min\n\u0174 :,j:p Tr((W :,j:p \u2212 \u0174 :,j:p ) T \u03a3 F (W :,j:p \u2212 \u0174 :,j:p )) s.t. \u0174 :,j = \u0174 + :,j(6)\nwhere \u03a3 F = X j:p,: X T j:p,: , F refers to the {j, \u2022 \u2022 \u2022 , p} indices and (a) is by \u2225M \u2225 2 F = Tr(M T M ) for any matrix M .Therefore, after updating the j-th column to \u0174 + :,j , we can update \u0174 :,j+1:p by the OBS updates (we refer to Frantar and Alistarh (2022); Frantar et al. (2023) for derivation details):\n\u03b4 \u2190 \u2212 \u0174 :,j \u2212 \u0174 + :,j [\u03a3 \u22121 F ] j,j \u0174 :,j+1:p \u2190 \u0174 :,j+1:p + \u03b4[\u03a3 \u22121 F ] j,j+1:p(7)\nWe note that the OBS updates in (7) require the calculation of \u03a3 \u22121 F .Therefore, using the updates in (7) can be expensive in practice.To improve efficiency, GPTQ uses a lazy-batch update scheme where at each step, only a subset (of size at most 128) of the remaining unquantized weights is updated.\n\n\nAWQ\n\nSimilar to GPTQ, AWQ (Lin et al., 2023) uses a layerwise quantization framework.However, different from GPTQ, the main idea behind AWQ is to find a rescaling of weights that does not result in high quantization error, rather than directly minimizing the least squares criteria for layerwise reconstruction.To this end, AWQ considers the following optimization problem: 8), s is the per-channel scaling.Problem ( 8) is non-differentiable and non-convex and cannot be efficiently solved.Therefore, Lin et al. (2023) discuss grid search heuristics for s to find a value that does not result in high quantization error.Particularly, they set s = s X \u03b1 * s W \u2212\u03b2 for some \u03b1, \u03b2 \u2208 [0, 1], where * is coordinate-wise multiplication, and s X , s W \u2208 R p are per-channel averages of magnitude of X and W , respectively.The values of \u03b1, \u03b2 are then chosen by grid search over the interval [0, 1].After choosing the value of s, the quantized weights are given as s \u22121 \u2299 q(s \u2299 W ).\nmin s\u2208R p \u2225W X \u2212 q(s \u2299 W )(X \u2299 s \u22121 )\u2225 2 F (8) where [q(W )] i,j = q i (W i,j ) quantizes a vector/matrix coordinate-wise, [s \u22121 ] i = s \u22121 i is the coordinate-wise inversion and \u2299 is the channel-wise multiplication, [s \u2299 W ] i,j = s j W i,j and [X \u2299 s \u22121 ] i,j = X i,j /s i . In Problem (\n\nOur Proposed Method\n\n\nOverview of QuantEase\n\nOur algorithm is based on the cyclic CD method (Tseng, 2001).At every update of our CD algorithm, we minimize the objective in (1) with respect to the coordinate \u0174i,j while making sure \u0174i,j \u2208 Q i .At each step, we keep all other weights fixed at their current value.For (i, j) \u2208 [q] \u00d7 [p] we update \u0174i,j , as follows:\n\u0174 + i,j \u2208 argmin \u0174i,j \u2208Qi f ( \u01741,1 , \u2022 \u2022 \u2022 , \u0174i,j , \u2022 \u2022 \u2022 , \u0174q,p )(9)\nwhere \u0174 + is the solution after the update of coordinate (i, j).In words, \u0174 + i,j is obtained by solving a 1D optimization problem: we minimize the 1D function \u0174i,j \u2192 f ( \u0174 ) under the quantization constraint.This 1D optimization problem, despite being non-convex, can be solved to optimality in closed-form (See Section 3.2 for details).A full pass over all coordinates (i, j) \u2208 [q] \u00d7 [p] completes one iteration of the CD algorithm.QuantEase usually makes several iterations to obtain a good solution to (1).\n\nWe initialize QuantEase with the unquantized original weights of the layer.This means that \u0174 is generally infeasible for Problem (1) (i.e., it does not fully lie on the quantization grid) till the end of the first iteration.However, from the second iteration onward, feasibility is maintained by QuantEase, while continuously decreasing f .This is useful because QuantEase can be terminated any time after the first iteration with a feasible solution.\n\nIt is worth mentioning that, unlike QuantEase, GPTQ does only one pass through the columns of \u0174 .Once \u0174 is feasible (fully quantized) at the end, the OBS idea is not usable any more.Hence further iterations are not possible and so GPTQ stops there.Interestingly, QuantEase can be initialized with the \u0174 obtained by GPTQ (or any other algorithm) and run for several iterations to optimize Problem (1) even further.\n\n\nComputational Considerations for QuantEase\n\nClosed-form updates: The efficiency of the CD method depends on how fast the update (9) can be calculated.Lemma 1 derives a closed form solution for Problem (9).\nLemma 1. Let \u03a3 = XX T . Then, \u0174 + i,j = q i ( \u03b2) in (9) where 2 \u03b2 = \u2212 \uf8ee \uf8f0 k\u0338 =j \u03a3 j,k \u0174i,k \u2212 (W \u03a3) i,j \uf8f9 \uf8fb /\u03a3 j,j .(10)\nProof of Lemma 1 can be found in Appendix B. We note that \u03b2 in (10) minimizes the one\n-dimensional function \u0174i,j \u2192 f ( \u01741,1 , \u2022 \u2022 \u2022 , \u0174i,j , \u2022 \u2022 \u2022 , \u0174q,p )\nwhere \u0174i,j is unconstrained (i.e., without any quantization constraint).Therefore, interestingly, Lemma 1 shows that to find the best quantized value for \u0174i,j in (9), it suffices to quantize the value that minimizes the one-dimensional function, \u0174i,j\n\u2192 f ( \u01741,1 , \u2022 \u2022 \u2022 , \u0174i,j , \u2022 \u2022 \u2022 , \u0174q,p )\nunder no quantization constraint.Since we find the minimizer per coordinate and then quantize the minimizer, this is different from quantizing the \"current\" weight.Memory Footprint: The matrices, \u03a3 and W \u03a3 do not change over iterations and can be stored with p 2 + O(pq) memory footprint.This is specially interesting as in practice, n \u226b p, q.QuantEase, unlike GPTQ, also does not require matrix inversion or Cholesky factorization which can be memory-inefficient (either adding up to O(p 2 ) storage).Parallelization over i \u2208 [q]: As seen in Lemma 1, for a given j 0 \u2208 [p], the updates of \u0174i,j0 are independent (for each i) and can be done simultaneously.Therefore, rather than updating a coordinate \u0174i,j at a time, we update a column of \u0174 , that is: \u0174:,j at each update.This allows us to better make use of the problem structure (see the rank-1 update below).\n\nRank-1 updates: Note that in (10), we need access to terms of the form k\u0338 =j \u03a3 j,k \u0174i,k .Such terms can be refactored as:\nk\u0338 =j \u03a3 j,k \u0174i,k = p k=1 \u03a3 j,k \u0174i,k \u2212 \u03a3 j,j \u0174i,j = \u0174 i,: \u03a3 :,j \u2212 \u03a3 j,j \u0174i,j .\nHowever, as noted above, we update all the rows corresponding to a given column of \u0174 at once.Therefore, to update a column of \u0174 , we need access to the vector\n\uf8eb \uf8ed k\u0338 =j \u03a3 j,k \u01741,k , \u2022 \u2022 \u2022 , k\u0338 =j \u03a3 j,k \u0174q,k \uf8f6 \uf8f8 T = ( \u0174 \u03a3) :,j \u2212 \u03a3 j,j \u0174 :,j .(11)\nDrawing from ( 11), maintaining a record of \u0174 \u03a3 exclusively for the updates outlined in (10) emerges as satisfactory, given that W and \u03a3 remain unaltered in the iterative process demonstrated in (10).Below, we show how the updates of \u0174 \u03a3 can be done with a notable degree of efficiency.First, consider the following observation.\n\nObservation: Suppose W 1 , W 2 differ only on a single column such as j.Then,\nW 2 \u03a3 = [W 1 \u03a3 \u2212 (W 1 ) :,j \u03a3 j,: ] (A) +(W 2 ) :,j \u03a3 j,:(B)\n.\n\nThus, given W 1 \u03a3, obtaining W 2 \u03a3 requires a rank-1 update, rather than a full matrix multiplication.We apply these updates to keep track of \u0174 \u03a3 when updating a column of \u0174 , as shown in Algorithm 1.\n\nInitialization: We initialize QuantEase with original unquantized weights.However, we include the following heuristic in QuantEase.In every other third iteration, we do not quantize weights (i.e.use \u03b2 from Lemma 1 directly).Though it introduces infeasibility, the following iteration brings back feasibility.We have observed that this heuristic helps with optimization performance, i.e., decreases f better.A summary of QuantEase can be found in Algorithm 1.In each iteration of QuantEase for a fixed j, the time complexity is dominated by rank-1 updates and is O(pq).Therefore, each iteration of QuantEase has time complexity of O(p 2 q).Combined with the initial cost of computing \u03a3 = XX T , W \u03a3, \u0174 \u03a3 and doing K iterations, the overall time complexity of QuantEase is O(pqn + Kp 2 q).Algorithm 1: QuantEase\nInitialize \u0174 for iter = 1, \u2022 \u2022 \u2022 , iter-max do for j = 1, \u2022 \u2022 \u2022 , p do u \u2190 ( \u0174 \u03a3) :,j \u2212 \u03a3 j,j \u0174 :,j \u2212 (W \u03a3) :,j /\u03a3 j,j // \u03b2 from Lemma 1 for column j \u0174 \u03a3 \u2190 \u0174 \u03a3 \u2212 \u0174 :,j \u03a3 j,: // Part (A) of rank-1 update from (12) \u0174i,j \u2190 q i (\u2212u i ), i \u2208 [q] // Perform updates from (10) \u0174 \u03a3 \u2190 \u0174 \u03a3 + \u0174 :,j \u03a3 j,: // Part (B) of rank-1 update from (12) end end return \u0174\nAccelerated QuantEase with partial update: In our initial experiments, we found that implementation for computing the outer product in frameworks like PyTorch is expensive and memory inefficient.We improve the efficiency of the algorithm based on three key observations: (1) Two rank-1 updates can be combined into one by bookkeeping the \u0174 :,j before and after updating with the quantized value u in each inner loop step with Equation ( 10).\n\n(2) \u0174 \u03a3 does not need to be fully updated in each inner loop since the update of each \u0174 :,j in each iteration only requires the update of ( \u0174 \u03a3) :,j = \u0174 :,1:j \u03a3 1:j,j .Instead of amortizing this update with j rank-1 outer products, we do this update one-time for each column \u0174 :,j to avoid redundant computation and memory allocation.\n\n(3) The division of \u03a3 j,j can be absorbed into the \u03a3 matrix with a column-wise normalization leading to a asymmetric square matrix with all-ones diagonal values.The term \u2212\u03a3 j,j \u0174 :,j in the update of u can be removed by setting diagonal to zero after the normalization, which is a one-time update at the beginning of the entire algorithm.The updated algorithm is described in Algorithm 2. The following rewriting of Equation ( 10) in terms of the notations of Algorithm 2 summarizes the computations: \u03b2i = P i,j \u2212 Pi,j + \u2206 \u0174 i,1:j \u03a3 norm 1:j,j .\n\nCompared to Algorithm 1, experiments reveal that the accelerated algorithm reduces the quantization time by over 34 times for the Falcon-180b model, reducing iteration time from 99 hours to 2.9 hours on a single NVIDIA A100 GPU without sacrificing accuracy.Each iteration of the optimized algorithm has speed comparabale to the GPTQ algorithm, which is optimized with a state-of-the-art well-optimized Cholesky kernel with lower memory usage.We leverage the optimized version of QuantEase in all the experiments in the paper.\n\nAlgorithm 2: Accelerated QuantEase with partial update\nInitialize \u0174 Initialize \u03a3 norm :,j = \u03a3 :,j /\u03a3 j,j Initialize P = W \u03a3 norm Set \u03a3 norm j,j = 0 // help absorb \u2212 \u0174 matrix into \u0174 \u03a3 norm for reducing computation for iter = 1, \u2022 \u2022 \u2022 , iter-max do \u2206 \u0174 = \u0174 P = \u0174 \u03a3 norm for j = 1, \u2022 \u2022 \u2022 , p do \u03b2 \u2190 P :,j \u2212 P :,j + \u2206 \u0174 :,1:j \u03a3 norm 1:j,j \u0174i,j \u2190 q i ( \u03b2i ), i \u2208 [q]\n\u2206 \u0174 :,j \u2190 \u2206 \u0174 :,j \u2212 \u0174 :,j end end return \u0174\n\n\nConvergence of QuantEase\n\nLet us now discuss the convergence of QuantEase.First, let us define Coordinate-Wise (CW) minima.\n\nDefinition 1 (CW-minimum, Beck and Eldar (2013); Hazimeh and Mazumder (2020)).We call W * a CW-minimum for Problem (1) iff for (i, j)\n\u2208 [q] \u00d7 [p], we have W * i,j \u2208 Q i and W * i,j \u2208 argmin \u0174i,j \u2208Qi f ( \u01741,1 , \u2022 \u2022 \u2022 , \u0174i,j , \u2022 \u2022 \u2022 , \u0174q,p ).\nIn words, a CW-minimum is a feasible solution that cannot be improved by updating only one coordinate of the solution, while keeping the rest fixed.Suppose we modify the basic CD update, (9) as follows: If \u0174 + i,j does not strictly decrease f , then set \u0174 + i,j = \u0174i,j .This avoids oscillations of the algorithm with a fixed f value.The following lemma shows that the sequence of weights from the modified CD converges to a CW-minimum.\n\nLemma 2. The sequence of \u0174 generated from modified QuantEase converges to a CW-minimum.\n\n\nOptimization Performance: GPTQ vs QuantEase\n\nWe now show that QuantEase indeed leads to lower (calibration) optimization error compared to GPTQ (see Section 5 for the experimental setup details).To this end, for a given layer and a feasible solution \u0174 , let us define the relative calibration error as Error( \u0174 ) = \u2225W X \u2212 \u0174 X\u2225 2 F /\u2225W X\u2225 2 F where X is the calibration set used for quantization.\n\nIn Figure 2, we report the relative error of QuantEase, Error( \u0174 QuantEase ), as well as the relative improvement of QuantEase over GPTQ in terms of error, (Error( \u0174 GPTQ ) \u2212 Error( \u0174 QuantEase ))/Error( \u0174 GPTQ ) for the BLOOM-1b1 model and 3/4 bit quantization.In the figure, we sort layers based on their QuantEase error, from the smallest to the largest.As can be seen, the QuantEase error over different layers can differ between almost zero to 5% for the 4-bit and zero to 15% for the 3-bit quantization.This shows different layers can have different levels of compressibility.Moreover, we see that QuantEase in almost all cases improves upon GPTQ, achieving a lower optimization error (up to 30%).This shows the benefit of QuantEase over GPTQ. Figure 2: Comparison of the optimization performance of QuantEase and GPTQ over all layers.The horizontal dashed line shows the median improvement of QuantEase over GPTQ for each case.As can be seen, QuantEase results in lower optimization error compared to GPTQ for most layers (up to 30% and on median 12%).Moreover, we see that the error in 3 bit quantization is larger than 4 bit quantization.\n\n\nbits\n\n4 Outlier-Aware Quantization\n\n\nFormulation\n\nIt has been observed that the activation might be more sensitive to some weights in a layer over others (Dettmers et al., 2023;Yao et al., 2022).For such sensitive weights, it might not be possible to find a suitable value on the quantization grid, leading to large quantization error.Moreover, some weights of a pre-trained network can be significantly larger or smaller than the rest-the quantization of LLMs can be affected by such weights (Bondarenko et al., 2021;Kim et al., 2023;Wei et al., 2022b).The existence of large/small weights increases the range of values that need to be quantized, which in turn increases quantization error.To this end, to better handle sensitive and large/small weights, which we collectively call outlier weights, we first introduce a modified version of the layerwise quantization problem (1).Our optimization formulation simultaneously identifies a collection of weights which are kept in full precision (aka the outliers weights) and quantizes the remaining weights.Before presenting our outlier-aware quantization formulation, we introduce some notation.Let S \u2286 [q] \u00d7 [p] denote a set of outlier indices-the corresponding weights are left at full precision.For any (i, j) / \u2208 S, the (i, j)-th weight is quantized and is chosen from the quantization grid Q i for the i-th channel.This is equivalent to substituting the set of weights for the layer W with \u0174 + \u0124 where \u0174 is quantized (like in Problem (1)) and \u0124 is sparse, with only a few nonzeros.In particular, for any (i, j) / \u2208 S, we have \u0124i,j = 0 implying the (i, j)-th weight can only have a quantized component.As S has a small cardinality, \u0124 is mostly zero.On the other hand, when (i, j) \u2208 S, we have \u0124i,j \u0338 = 0 and the corresponding weight is retained at full precision.In light of the above discussion, we present an outlier-aware version of Problem (1) given by: min\n\u0174 , \u0124 g( \u0174 , \u0124) := \u2225W X \u2212 ( \u0174 + \u0124)X\u2225 2 F s.t. \u0174i,j \u2208 Q i (i, j) \u2208 [q] \u00d7 [p], \u2225 \u0124\u2225 0 \u2264 s (14)\nwhere \u2225 \u2022 \u2225 0 denotes the number of nonzero elements of a vector/matrix, and s \u226a p, q is the total budget on the number of outliers.The constraint \u2225 \u0124\u2225 0 \u2264 s ensures the total number of outliers remains within the specified limit.\n\n\nSpQR\n\nBefore proceeding with our algorithm, we quickly review SpQR (Dettmers et al., 2023) which incorporates sensitivity-based quantization into GPTQ.Particularly, they seek to select few outliers that result in higher quantization error and keep them in full-precision.To this end, for each coordinate (i, j) \u2208 [q]\u00d7[p] SpQR calculates the sensitivity to this coordinate as the optimization error resulting from quantizing this coordinate.Formally, they define sensitivity as\n\u03c9 ij = min \u0174 \u2225W X \u2212 \u0174 X\u2225 2 F s.t. \u0174i,j = q i (W i,j ). (15)\nWe note that Problem (15) is in OBS form and therefore OBS is then used to calculate the sensitivity of coordinate (i, j).Then, any coordinate that has high sensitivity, for example, \u03c9 i,j > \u03c4 where \u03c4 > 0 is a predetermined threshold, is considered to be an outlier.After selecting outliers, similar to GPTQ, SpQR cycles through columns j = 1, \u2022 \u2022 \u2022 , p and updates each column based on OBS updates (see Section 2.2.1), keeping outlier weights in full-precision.\n\n\nOptimizing Problem (14)\n\nTo obtain good solutions to Problem (14), we use a block coordinate descent method where we alternate between updating \u0174 (with \u0124 fixed) and then \u0124 (with \u0174 fixed).For a fixed \u0124, Problem ( 14) has the same form as f ( \u0174 ) in (1) where W X is substituted with (W \u2212 \u0124)X.Therefore, we can use QuantEase as discussed in Section 3 to update \u0174 .Next, we discuss how to update \u0124.For a fixed \u0174 , Problem ( 14) is a least squares problem with a cardinality constraint.We use proximal gradient method (aka iterative hard thresholding method) (Blumensath and Davies, 2009) where we make a series of updates of the form:\n\u0124+ \u2208 argmin K g(K) s.t. \u2225K\u2225 0 \u2264 s = P s ( \u0124 \u2212 \u03b7\u2207 H g( \u0174 , \u0124)) (16)\nwhere P s (A) sets all coordinates of A to zero except the s-largest in absolute value,\ng(K) = L 2 K \u2212 \u0124 \u2212 1 L \u2207 \u0124 g( \u0174 , \u0124) 2 F ,(17)\nL = 1/\u03b7 = 2\u03bb max (XX T ) and \u03bb max (A) is the largest eigenvalue of the matrix A. Lemma 3 below establishes updates in ( 16) form a descent method if the initial \u0124 is sparse, \u2225 \u0124\u2225 0 \u2264 s.\n\nLemma 3.For any \u0174 and any \u0124 such that \u2225 \u0124\u2225 0 \u2264 s, we have\ng( \u0174 , \u0124+ ) \u2264 g( \u0174 , \u0124).\nWe note that when calculating Q i 's for Problem ( 14), we remove the top s largest coordinates of W (in absolute value) from the quantization pool, as the effect of those weights can be captured by \u0124 and we do not need to quantize them.This allows to reduce the range that each Q i needs to quantize, leading to lower error.Therefore, simultaneously, we preserve sensitive weights and reduce the quantization range by using outlier-aware QuantEase.\n\nThe details of the update discussed here are presented in Algorithm 3. In terms of initialization, similar to basic QuantEase, we set \u0124, \u0174 such that \u0124 + \u0174 = W . Particularly, we use the s-largest coordinates of W (in absolute value) to initialize \u0124: \u0124 = P s (W ), \u0174 = W \u2212 \u0124.Note that this leads to an infeasible initialization of \u0174 similar to basic QuantEase.However, as discussed in Section 3.1, after one iteration of QuantEase the solution becomes feasible and the descent property of Algorithm 3 holds.\n\nWe also note that as seen from Algorithm 3, in addition to storing \u0124, we need to store \u0124\u03a3, showing the memory footprint remains p 2 + O(pq), like basic QuantEase.In terms of computational complexity, in addition to basic QuantEase, the outlier-aware version requires calculating the largest eigenvalue of XX T , which can be done by iterative power method in O(p 2 ) only using matrix/vector multiplication.Additionally, calculating \u0124\u03a3 requires O(p 2 q) in each iteration, and finding the largest s coordinates of \u0124 can be done with average complexity of O(pq log pq).Therefore, the overall complexity is O(pqn + Kp 2 q + Kpq log pq) for K iterations.\n\nFinally, we note that unlike SpQR which fixes the location of outliers after selecting them, our method is able to add new outlier coordinates or remove them as the optimization progresses.This is because the location of nonzeros of \u0124 (i.e.outliers) gets updated, in addition to their values.Structured Outliers: Problem ( 14) does not have any constraints on the structure of the outliers.Serving a model with unstructured outliers may lead to increased serving latency because of added complexity to the GPU kernel implementation.\n\nA simple change to make the algorithm outlier aware is to constrain the selection of the outliers to be entire columns when performing the update in Equation ( 16).Instead of choosing the s-largest absolute values of A with P s , we can choose columns with the \u230a s q \u230b-largest l2 norm values, treating them as the most influential columns to keep as the outliers with full (or higher) precision.In practice, we can select different versions of the outlier-aware algorithm depending on the serving dequantization kernel implementation to balance the trade-off between serving efficiency and model accuracy.This exploration is beyond the scope of this paper.Empirical comparisons between various outlier-aware algorithms can be found in Section 5.4\n\n\nExperiments\n\nIn this section, we conduct several numerical experiments to demonstrate the effectiveness of QuantEase.A PyTorch-based implementation of QuantEase will be released on GitHub soon.Experimental Setup.For uniform quantization, we compare QuantEase to RTN (Dettmers et al., 2022;Yao et al., 2022), GPTQ (Frantar et al., 2023) and AWQ (Lin et al., 2023).For methods related to outlier detection, we compare with SpQR (Dettmers et al., 2023).We do not consider SqueezeLLM (Kim et al., 2023) as it mainly focuses on non-uniform quantization (whereas we focus on uniform quantization).Moreover, to the best of our knowledge, the quantization code for SqueezeLLM has not been made publicly available.\n\nIn our experiments, we do not use grouping for any method as our focus on understanding the optimization performance of various methods.Moreover, as discussed by Kim et al. (2023); Yao et al. (2023), grouping can lead to additional inference-time overhead, reducing the desirability of such tricks in practice.\n\nIn terms of models and data, we consider several models from the OPT (Zhang et al., 2022), BLOOM (Lauren\u00e7on et al., 2022), and Falcon (Penedo et al., 2023) families of LLMs.Note that we do not run AWQ on the BLOOM and Falcon models due to known architectural issues 3 .For the calibration (training) data, we randomly choose 128 sequences of length 2048 from the C4 dataset (Raffel et al., 2020).For validation data, following Frantar et al. (2023), we use datasets WikiText2 (Merity et al., 2016) and PTB (Marcus et al., 1994) in our experiments.In particular, after quantizing the model using the data from C4, we evaluate the model performance on PTB and WikiText2.Our experiments were conducted on a single NVIDIA A100 GPU with 80GB of memory.Notably, for the OPT-66b model, QuantEase works on a single NVIDIA V100 GPU whereas GPTQ (due to Cholesky factorization) and AWQ (due to batched calculations) ran out of memory.This further highlights the memory efficacy of QuantEase.Additional Experiments: Appendix A contains additional numerical results related to runtime and text generation.\n\n\nEffect of Number of Iterations\n\nWe first study the effect of the number of iterations of QuantEase on model performance.To this end, we consider OPT-350m in 3/4 bits and run quantization for different numbers of iterations, ranging from 10 to 30.The perplexity on WikiText2 is shown in Figure 3 for this case.The results show that increasing the number of iterations generally lowers perplexity as QuantEase reduces the error, although the improvement in perplexity for 4-bit quantization is small.Based on these results, 25 iterations seem to strike a good balance between accuracy and runtime, which we use in the rest of our experiments.\n\n\nEffect of number of iterations\n\n\nLanguage Generation Benchmarks\n\nNext, we study the effect of quantization on language generation tasks.Perplexity results for the WikiText2 data and OPT/BLOOM/Falcon families are shown in Tables 1, 2, and 3 respectively.Perplexity results for the PTB data can be found in Tables A.1, A.2, and A.3 in the Appendix.QuantEase achieves lower perplexity in all cases for 3-bit quantization.In the 4-bit regime, QuantEase almost always either improves upon baselines or achieves similar performance.Since perplexity is a stringent measure of model quality, these results demonstrate that QuantEase results in better quantization compared to methods like GPTQ and AWQ.\n\n\nLAMBADA Zero-Shot Benchmark\n\nFollowing (Frantar et al., 2023), we compare the performance of our method with baselines over a zero-shot task, namely LAMBADA (Paperno et al., 2016).The results for this task are shown in Figure 4 for the OPT and BLOOM families.For 3-bit quantization, QuantEase outperforms GPTQ and AWQ, often by a large margin.In the 4-bit regime, the performance of all methods is similar, although QuantEase is shown to be overall the most performant.Table 3: Falcon family perplexity for WikiText2 quantized on C4.QuantEase achieves lower perplexity in the majority of settings.GPTQ has numerical issue with most of the seeds we explored when computing Cholesky factorization when quantizing the Falcon-40b and 180b models with the default setup.We run a single iteration for QuantEase on Falcon-180b to prevent overfitting.\n\n\nOutliers-Aware Performance\n\nNext, we study the performance of the outlier-aware version of QuantEase.To this end, we consider 3-bit quantization and two sparsity levels of 0.5% and 1% (for example, s = 0.005pq or s = 0.01pq).Roughly speaking, a 0.5% outlier budget would lead to an additional 0.15 bits overhead (i.e.3.15 bits on average),  while the 1% version would lead to an additional overhead of 0.3 bits (i.e.3.3 bits on average).We compare our method with SpQR, with the threshold tuned to have at least 1% outliers on average.The rest of the experimental setup is shared from previous experiments.The perplexity results (on WikiText2) for this comparison are reported in Table 4 for the OPT family and in Table A.4 for the BLOOM family.Table A.6 contains results for the Falcon family.As is evident from the results, the QuantEase 0.5% outlier version is able to significantly outperform SpQR in all cases, and the 1% method does even better.This shows that outlier-aware QuantEase makes near-3 bit quantization possible without the need for any grouping.\n\n\nExtreme Quantization\n\nNext, we study extreme quantization of models to the 2-bit regime.Particularly, we consider the base number of bits of 2 and 2% outliers, resulting in roughly 2.6 bits on average.The results for this experiment for OPT are shown in Table 5 and for BLOOM in Table A.5. QuantEase significantly outperforms SpQR and is able to maintain acceptable accuracy in the sub-3-bit quantization regime.We observed empirically that in the absence of grouping and outlier detection, if we were to do 2-bit quantization, then the resulting solutions lead to a significant loss of accuracy.This finding also appears to be consistent with our exploration of other methods' publicly available code, such as GPTQ and AWQ.\n\n\nDiscussion\n\nWe study PTQ of LLMs and proposed a new layerwise PTQ framework, QuantEase.QuantEase is based on cyclic CD with simple updates for each coordinates.We also introduce an outlier aware version of QuantEase.\n\nOur numerical experiments show that our method outperforms the state-of-the-art methods such as GPTQ and AWQ for 3/4 bit uniform quantization.Our outlier aware QuantEase outperforms SpQR when quantizing to near-or sub-3 bit regimes.\n\nIn this work, we did not consider grouping.However, we note that grouping can be easily incorporated in QuantEase, as we only use standard quantizers.Investigating the performance of QuantEase with grouping is left for a future study.Moreover, we note that QuantEase can be paired with AWQ.As Lin et al. (2023)   restaurant.But it kept coming up with a recipe for a chocolate cake.\"I was like, 'What'?\"Kim said.\"I was like, 'This is not what I asked for'\".\n\n\u2022 GPTQ: Every time data scientist Kim ran her new algorithm, it kept outputting the same unexpected result: a recipe for a new kind of chocolate cake.Kim, who asked to be identified by her first name only, was working on a project to predict the likelihood of a person being a criminal.She was using a machine learning algorithm to analyze data from the FBI's National Crime Information Center, which contains information on more than 10 million criminal records.\n\n\u2022 QuantEase: Every time data scientist Kim ran her new algorithm, it kept outputting the same on the bed,\" he said.\"But only if they are on their backs,\" said his advisor.\"But only if they are on their backs,\" said the king.\n\nIn this case, both GPTQ and QuantEase diverge from the FP16 model, although the response from both seems to be coherent.However, we note that QuantEase is able to mention the \"advisor\" and \"king\" keeping the context relevant, while GPTQ refers to \"second cat\" and so on.\n\n\nA.2.3 Example 3\n\n\u2022 Prompt: Data scientist Grace was working late one night when she discovered a glitch in the matrix of her data.It spelled out\n\n\u2022 FP16: Data scientist Grace was working late one night when she discovered a glitch in the matrix of her data.It spelled out the word \"HELP\" in the form of a series of numbers.She was able to use the glitch to access a hidden database that contained the names and addresses of all the people who had ever used the service.\n\n\u2022 GPTQ: Data scientist Grace was working late one night when she discovered a glitch in the matrix of her data.It spelled out the word \"HELP\" in the form of a series of numbers.She was so excited that she called her friend, a programmer, and told him about it.\"I'm not sure what you're talking about,\" he said.\"I've never seen that before.\"\"Well,\n\n\u2022 QuantEase: Data scientist Grace was working late one night when she discovered a glitch in the matrix of her data.It spelled out the word \"HELP\" in the form of a series of numbers.She was able to use the glitch to access a hidden database that contained the names of all the people who had ever been in the database.She was able to access the names of all the people who had ever been in the database.\n\nIn this case, QuantEase seems to be closer to the FP16 response, although QuantEase repeats itself at the end of the sentence, showing QuantEase might lose the attention in this case.\n\n\nA.2.4 Example 4\n\n\u2022 Prompt: Last Tuesday, just as the sun was setting, my neighbor's dog, Rover, started to speak to me in perfect French.He said\n\n\u2022 FP16: Last Tuesday, just as the sun was setting, my neighbor's dog, Rover, started to speak to me in perfect French.He said, \"I'm sorry, I don' speak English.\"I was shocked.I've lived in this neighborhood for over a year and Rover has never spoken to me before.I've never seen him before.I've never heard him before.\n\n\u2022 GPTQ: Last Tuesday, just as the sun was setting, my neighbor's dog, Rover, started to speak to me in perfect French.He said, \"I'm Rover.I'm a dog.I'm a dog.\"I was stunned.I was speechless.I was in awe.I've been a dog lover all my life.I've always had dogs.I've always loved dogs.I've always wanted to be a\n\n\u2022 QuantEase: Last Tuesday, just as the sun was setting, my neighbor's dog, Rover, started to speak to me in perfect French.He said, \"I'm sorry, I'm so sorry.\"I was in the middle of a conversation with my friend, and I stopped to listen to Rover.He was standing in the middle of the street, looking up at the sky.\"I'm sorry, I'm so sorry,\" he said again.\n\nIn this case, none of the models appear to follow FP16 response.350m 1.3b 2.7b 6.7b 13b 66b\n\nQuantEase 25.8m 52.6m 1.4h 2.4h 3.8h 13.8h\n\n\nB.2 Proof of Lemma 2\n\nThe proof is a result of the observations that (a) the modified algorithm generates a sequence of \u0174 iterates with decreasing f values (after obtaining the first feasible solution, possibly after the first iteration) and (b) there are only a finite number of choices for \u0174 on the quantization grid.\n\n\nB.3 Proof of Lemma 3\n\nFirst, note that mapping \u0124 \u2192 \u2207 H g( \u0174 , \u0124) is L-Lipschitz,\n\u2207 H g( \u0174 , \u01241 ) \u2212 \u2207 H g( \u0174 , \u01242 ) F \u2264 L\u2225 \u01241 \u2212 \u01242 \u2225 F .\nTherefore, by Lemma 2.1 of Beck and Teboulle (2009) for \u0174 , \u01241 , \u01242 we have where the second inequality is by the definition of \u0124+ in ( 16) as \u0124 is a feasible solution for the optimization problem in (16).\ng( \u0174 , \u01241 ) \u2212 g( \u0174 , \u01242 ) \u2264 L 2 \u01241 \u2212 \u01242 \u2212 1 L \u2207 \u0124 g( \u0174 , \u01242 ) 2 F \u2212 1 2L \u2207 \u0124 g( \u0174 , \u01242 )\n\nC QuantEase algorithm with outliers\n\nAlgorithm 3: Outlier-Aware QuantEase Initialize \u0124, \u0174 \u03b7 \u2190 1/2\u03bb max (XX T ) // step size for iterative thresholding for iter = 1, \u2022 \u2022 \u2022 , iter-max do for j = 1, \u2022 \u2022 \u2022 , p do u \u2190 ( \u0174 \u03a3) :,j \u2212 \u03a3 j,j \u0174 :,j \u2212 ((W \u2212 \u0124)\u03a3) :,j /\u03a3 j,j // \u03b2 from Lemma 1 for column j.\n\nW is substituted with W \u2212 \u0124. \u0174 \u03a3 \u2190 \u0174 \u03a3 \u2212 \u0174 :,j \u03a3 j,: // Part (A) of rank-1 update from (12) \u0174 i,j \u2190 q i (\u2212u i ), i \u2208 [q] // Perform updates from (10) \u0174 \u03a3 \u2190 \u0174 \u03a3 + \u0174 :,j \u03a3 j,: // Part (B) of rank-1 update from (12) end \u2207 H g( \u0174 , \u0124) \u2190 2 \u0124\u03a3 + 2 \u0174 \u03a3 \u2212 2W \u03a3 // Calculate the gradient of g from (14).\u0124 \u2190 P s ( \u0124 \u2212 \u03b7\u2207 H g( \u0174 , \u0124)) // Perform update (16) end return \u0174 , \u0124\n\nFigure 3 :\n3\nFigure 3: Effect of varying the number of iterations of QuantEase on perplexity in Section 5.1.\n\n\n(\n\nwhere (a) is by \u2225A\u2225 2 F = Tr(A T A) and (b) is by Tr(AB) = Tr(BA).Next, let us only consider terms in (B.1) that depend on W :,j0 for a given j 0 .Letting \u03a3 = XX T , such terms can be written as from (A), j=k=j0 find the optimal value of \u0174 + i,j in (9) for (i, j 0 ) we need to solve problems of the formmin u\u2208Qi \u03a3 j0,j0 u 2 + 2 k\u0338 =j0 \u03a3 j0,k \u0174i,k u \u2212 2(W \u03a3) i,j0 u. (B.3) Claim: If a > 0, then min u\u2208Qi au 2 + bu = q i (\u2212b/2a).Proof of Claim: Writeau 2 + bu = a(u + (b/2a)) 2 \u2212 b 2 /(4a) the quantization function for the quantization grid Q i + b/(2a) = {a + b/(2a) : a \u2208 Q i }.This completes the proof of the claim and the lemma.\n\n\n\n\nthe definition of g in (17), we haveg( \u0174 , K) \u2212 g( \u0174 , \u0124) \u2264 g(K) \u2212 g(\n\n\nTable 1 :\n1\nOPT family perplexity for WikiText2 quantized on C4.QuantEase achieves lower perplexity in the majority of settings..0318.90 0.02 16.41 0.02 14.10 0.01 11.74 0.01 QuantEase 23.97 0.03 18.90 0.01 16.11 0.03 14.18 0.01 11.69 0.01\n350m1.3b2.7b6.7b13b66bfull22.0014.6212.4710.8610.139.34RTN64.561.33e41.56e46.00e33.36e36.12e33 bitsAWQ GPTQ32.38 0.11 33.60 0.3453.63 0.45 21.51 0.13201 6 17.02 0.1719.00 0.12 15.16 0.01 11.90 0.06 14.13 0.43 13.90 0.02 17.94 0.18QuantEase31.52 0.36 21.30 0.23 16.75 0.24 12.95 0.04 12.41 0.02 13.08 0.38RTN25.9448.1916.9212.1011.32110.524 bitsAWQ GPTQ24.05 0.03 24.29 0.1115.67 0.04 15.44 0.03 12.80 0.04 11.46 0.04 13.16 0.01 11.30 0.0110.36 0.01 10.34 0.019.58 0.01 9.58 0.05QuantEase23.91 0.05 15.28 0.04 13.05 0.01 11.21 0.01 10.32 0.019.47 0.02560m1b11b73b7b1full22.4117.6815.3913.4811.37RTN56.9950.0763.5039.2917.373 bitsGPTQ32.36 0.0725.18 0.0621.43 0.0717.50 0.0413.73 0.03QuantEase31.52 0.10 23.91 0.02 20.03 0.05 17.21 0.04 13.43 0.04RTN25.8919.9816.9714.7512.104 bitsGPTQ24.02 0\n\nTable 2 :\n2\nBLOOM family perplexity for WikiText2 quantized on C4.QuantEase achieves lower perplexity in the majority of settings.Only one seed succeeds among all trials while the rest have numerical issues, thus no standard deviation is reported here.\n7b40b180bfull6.595.233.30RTN5.67e23.89e71.55e43 bitsGPTQ9.62 0.13N/AN/AQuantEase8.83 0.07 6.20 0.07 5.19 0.10RTN9.965.6736.634 bitsGPTQ6.90 0.015.35 *N/AQuantEase6.92 0.01 5.36 0.02 3.72 0.01\n*\n\n\n\n\n.43 18.17 0.16 14.50 0.07 11.95 0.02 10.96 0.01 QuantEase 0.5% 27.52 0.05 16.68 0.14 13.72 0.04 11.49 0.02 10.70 0.01 QuantEase 1% 26.48 0.12 16.25 0.05 13.70 0.10 11.48 0.03 10.37 0.01 QuantEase structured 0.5% 31.090.86 18.86 0.36 15.80 0.37 12.15 0.06 12.18 0.04 QuantEase structured 1% 30.21 0.16 18.51 0.27 15.65 0.04 12.26 0.01 12.07 0.13\nZero-shot accuracy on OPT0.2 0.4 0.6FP16 QuantEase-4bit GPTQ-4bit AWQ-4bit QuantEase-3bit GPTQ-3bit AWQ-3bitZero-shot accuracy on BLOOM0.2 0.3 0.4 0.5 0.60.351.32.76.7130.561.11.737.1Model Size (billions of params)Model Size (billions of params)Figure 4: Zero-Shot accuracy on the LAMBADA (Paperno et al., 2016) benchmark for 3-bit and 4-bitquantization. See Section 5.3 for more details on experimental setup.350m1.3b2.7b6.7b13bfull22.0014.6212.4710.8610.133 bitsQuantEase31.52 0.12 21.30 0.23 16.75 0.24 12.95 0.04 12.41 0.02Outlier (3 bits) 31.67 04 bits SpQR 1% QuantEase 23.91 0.05 15.28 0.04 13.05 0.01 11.21 0.01 10.32 0.01\n\nTable 4 :\n4\nOPT family perplexity for WikiText2 quantized on C4.Outlier aware quantization is done with 3 bits.\n\n\nTable 5 :\n5\nQuantEase 2% 158 4 36.40.8 24.2 0.1 19.0 0.2 19.3 0.2 OPT family perplexity for WikiText2 quantized on C4.Outlier aware quantization is done with 2 bits.\n350m1.3b2.7b6.7b13bfull22.0014.6212.4710.8610.13OutlierSpQR 2%323 7155 370.5 2.734.0 1.222.3 0.2(2 bits)\n\n\n\nnote, incorporating AWQ into GPTQ can lead to improved numerical results, and as we have shown, QuantEase usually outperforms GPTQ.Therefore, we would expect AWQ+QuantEase would lead to even further improvements.Only one seed succeeds among all trials while the rest have numerical issues, thus no standard deviation is reported here.\n7b40b180bfull9.907.836.65RTN5.85e22.63e61.42e43 bitsGPTQ13.64 0.13N/AN/AQuantEase13.28 0.04 8.94 0.03 7.76 0.04RTN12.848.6160.444 bitsGPTQ10.40 0.038.01 *N/AQuantEase10.39 0.02 8.01 0.01 6.90 0.02\n*\n\n\n\n\nTable A.3: Falcon family perplexity for PTB quantized on C4.GPTQ has numerical issue when computing Cholesky factorization when quantizing the falcon-40b and 180b with default setup described in the original paper on most seeds.We run single iter for QuantEase on Falcon-180b to prevent overfitting issue..0522.51 0.11 19.16 0.05 15.95 0.06 12.88 0.01 QuantEase 0.5% 26.82 0.07 20.26 0.03 17.41 0.02 14.93 0.01 12.19 0.01 QuantEase 1% 25.80 0.09 19.60 0.02 17.06 0.02 14.65 0.02 12.03 0.01 QuantEase structured 0.5% 29.42 0.39 22.56 0.06 18.97 0.06 16.23 0.06 12.95 0.06 QuantEase structured 1% 29.00 0.24 22.49 0.12 18.89 0.05 15.99 0.06 12.97 0.04 4 bits QuantEase 23.97 0.03 18.90 0.01 16.11 0.03 14.18 0.01 11.69 0.01 QuantEase 2% 66.1 1.3 39.3 0.3 31.4 0.1 22.1 0.1 15.8 0.03 Table A.5: BLOOM family perplexity for WikiText2 quantized on C4.Outlier aware quantization is done with 2 bits.\n560m1b11b73b7b1full22.4117.6815.3913.4811.373 bitsQuantEase31.52 0.10 23.91 0.02 20.03 0.05 17.21 0.04 13.43 0.04Outlier (3 bits) 29.02 0Table A.4: BLOOM family perplexity for WikiText2 quantized on C4. Outlier aware quantization is done SpQR 1%with 3 bits.560m1b11b73b7b1full22.4117.6815.3913.4811.37OutlierSpQR 2%228 10126 5127 459.7 2.232.5 0.4(2 bits)\n\nTable A\nA.8: QuantEase runtime for OPT family560m1b11b73b7b1QuantEase 19.5m 29.6m 40.6m 1.1h 1.9hTable A.9: QuantEase runtime for BLOOM family7b40b180bQuantEase 2.3h 13.0h 2.9h (1 iter)\n\nTable A .\nA\n10: QuantEase runtime for Falcon family.We run 30 iterations on Falcon 7b/40b and 1 iter on Falcon-180b to prevent overfitting.\n\nA homogeneous quadratic function with decision variable u \u2208 R p is given by u T Qu where Q \u2208 R p\u00d7p and there is no linear term.\nWe assume that \u03a3 j,j > 0. Note that \u03a3 j,j = \u2225X j,: \u2225 2 . So, \u03a3 j,j = 0 would mean that X j,: = 0; hence, \u0174 :,j may be quantized arbitrarily and completely omitted from the problem. Such checks can be done before QuantEase is begun.\nSee https://github.com/mit-han-lab/llm-awq/issues/2 for more details.\nAcknowledgementsKayhan Behdin contributed to this work while he was an intern at LinkedIn during summer 2023.This work is not a part of his MIT research.Rahul Mazumder contributed to this work while he was a consultant for LinkedIn (in compliance with MIT's outside professional activities policies).This work is not a part of his MIT research.A.1 Additional Text Generation BenchmarksFirst, we present the results for uniform quantization evaluated on PTB dataset (the setup from Section 5.2).These results can be found in Tables A.1, A.2, and A.3.Additional outlier aware quantization results are presented in Tables A.4 and A.5 for BLOOM family and Tables A.6 for Falcon family from Section 5.4.A.2 Generative ExamplesIn this section, we present a few generative examples.We have used OPT-13b model quantized to 4 bits in all examples here.A.2.1 Example 1\u2022 Prompt: Every time data scientist Kim ran her new algorithm, it kept outputting the same unexpected result: a recipe for unexpected result: a recipe for a new drug.The algorithm was designed to predict the likelihood that a drug would be approved by the US Food and Drug Administration (FDA).But it kept coming up with a recipe for a new drug that was never approved.\"It was a bit of a shock,\" says Kim, a data scientist at the University of California, San Francisco.In this case, although GPTQ's response is initially similar to the FP16 response (both outputting chocolate cake), as the response continues, GPTQ and FP16 diverge.On the other hand, QuantEase produces a response that is overall coherent and logical.This is while GPTQ's response relates food to crime.A.2.2 Example 2\u2022 Prompt: In a parallel universe where cats are the rulers of Earth, our feline king was pondering his first decree\u2022 FP16: In a parallel universe where cats are the rulers of Earth, our feline king was pondering his first decree.\"I decree that all cats shall be given the right to vote,\" he said.\"And I decree that all cats shall be given the right to vote,\" said the cat next to him.\u2022 GPTQ: In a parallel universe where cats are the rulers of Earth, our feline king was pondering his first decree.\"Idecree that all cats shall be allowed to roam free,\" he said.\"And I decree that all cats shall be allowed to eat as much as they want,\" said a second cat.\"And I decree that all cats shall be allowed to sleep as much as they want,\" said a third cat.\u2022 QuantEase: In a parallel universe where cats are the rulers of Earth, our feline king was pondering his first decree.\"I decree that all cats shall be allowed to sleepA.3 RuntimeIn this section, we report the runtime of our QuantEase method.The numbers reported are for 3-bit quantization experiments from Tables1, 2, and 3 for OPT, BLOOM, and Falcon families, respectively.The runtime for different models are reported in Tables A.8, A.9, and A.10.We see that the runtime ranges from 10s of minutes for sub-billion models, up to around a day for 13b model.This shows that overall, QuantEase is computationally feasible, specially for models with 10b or fewer parameters.B Proof of Main Results\nSoft-tohard vector quantization for end-to-end learning compressible representations. E Agustsson, F Mentzer, M Tschannen, L Cavigelli, R Timofte, L Benini, L Van Gool, Proc. of Neurips. of Neurips2017\n\nSparsity constrained nonlinear optimization: Optimality conditions and algorithms. Amir Beck, Yonina C Eldar, SIAM Journal on Optimization. 2332013\n\nA fast iterative shrinkage-thresholding algorithm for linear inverse problems. Amir Beck, Marc Teboulle, SIAM journal on imaging sciences. 212009\n\nSparse gaussian graphical models with discrete optimization: Computational and statistical perspectives. Kayhan Behdin, Wenyu Chen, Rahul Mazumder, arXiv:2307.093662023arXiv preprint\n\nIterative hard thresholding for compressed sensing. Thomas Blumensath, Mike E Davies, Applied and computational harmonic analysis. 2732009\n\nUnderstanding and overcoming the challenges of efficient transformer quantization. Yelysei Bondarenko, Markus Nagel, Tijmen Blankevoort, arXiv:2109.129482021arXiv preprint\n\nLanguage models are few-shot learners. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Advances in neural information processing systems. 202033\n\nXnor-net++: Improved binary neural networks. A Bulat, G Tzimiropoulos, 2019\n\nLibsvm: a library for support vector machines. Chih-Chung Chang, Chih-Jen Lin, ACM transactions on intelligent systems and technology (TIST). 20112\n\nTim Dettmers, Mike Lewis, Younes Belkada, Luke Zettlemoyer, arXiv:2208.07339-bit matrix multiplication for transformers at scale. 20228arXiv preprintint8 (\n\nSpqr: A sparse-quantized representation for near-lossless llm weight compression. Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Alexander Saleh Ashkboos, Torsten Borzunov, Dan Hoefler, Alistarh, arXiv:2306.030782023arXiv preprint\n\nBert: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.048052018arXiv preprint\n\nLearning to prune deep neural networks via layer-wise optimal brain surgeon. Xin Dong, Shangyu Chen, Sinno Pan, 201730Advances in neural information processing systems\n\nOptimal brain compression: A framework for accurate post-training quantization and pruning. Elias Frantar, Dan Alistarh, Advances in Neural Information Processing Systems. 202235\n\nOPTQ: Accurate quantization for generative pre-trained transformers. Elias Frantar, Torsten Saleh Ashkboos, Dan Hoefler, Alistarh, The Eleventh International Conference on Learning Representations. 2023\n\nRegularization paths for generalized linear models via coordinate descent. Jerome Friedman, Trevor Hastie, Rob Tibshirani, Journal of statistical software. 33112010\n\nA survey of quantization methods for efficient neural network inference. Amir Gholami, Sehoon Kim, Zhen Dong, Zhewei Yao, Michael W Mahoney, Kurt Keutzer, Low-Power Computer Vision. Chapman and Hall/CRC2022\n\nSecond order derivatives for network pruning: Optimal brain surgeon. Babak Hassibi, David Stork, Advances in neural information processing systems. 51992\n\nFast best subset selection: Coordinate descent and local combinatorial optimization algorithms. Hussein Hazimeh, Rahul Mazumder, Operations Research. 6852020\n\nSparsity in deep learning: Pruning and growth for efficient inference and training in neural networks. Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, Alexandra Peste, 2021\n\nTraining compute-optimal large language models. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego De Las, Lisa Anne Casas, Johannes Hendricks, Aidan Welbl, Clark, arXiv:2203.155562022arXiv preprint\n\nAccurate post training quantization with small calibration sets. Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, Daniel Soudry, International Conference on Machine Learning. PMLR2021a\n\nAccurate post training quantization with small calibration sets. Itay Hubara, Yury Nahshan, Yair Hanani, Ron Banner, Daniel Soudry, International Conference on Machine Learning. PMLR2021b\n\nScaling laws for neural language models. Jared Kaplan, Sam Mccandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, Dario Amodei, 2020\n\nSehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, Kurt Keutzer, arXiv:2306.07629Squeezellm: Dense-and-sparse quantization. 2023arXiv preprint\n\nThe bigscience roots corpus: A 1.6 tb composite multilingual dataset. Lucile Hugo Lauren\u00e7on, Thomas Saulnier, Christopher Wang, Le Akiki ; Teven, Leandro Scao, Chenghao Von Werra, Eduardo Gonz\u00e1lez Mou, Huu Ponferrada, Nguyen, Advances in Neural Information Processing Systems. 202235Albert Villanova del Moral\n\nOptimal brain damage. Yann Lecun, John Denker, Sara Solla, Advances in neural information processing systems. 1989\n\nAwq: Activation-aware weight quantization for llm compression and acceleration. Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, Song Han, arXiv:2306.009782023arXiv preprint\n\nThe penn treebank: Annotating predicate argument structure. Mitch Marcus, Grace Kim, Mary , Ann Marcinkiewicz, Robert Macintyre, Ann Bies, Mark Ferguson, Karen Katz, Britta Schasberger, Human Language Technology: Proceedings of a Workshop. Plainsboro, New JerseyMarch 8-11, 1994. 1994\n\nThe graphical lasso: New insights and alternatives. Rahul Mazumder, Trevor Hastie, Electronic journal of statistics. 621252012\n\nStephen Merity, Caiming Xiong, James Bradbury, Richard Socher, arXiv:1609.07843Pointer sentinel mixture models. 2016arXiv preprint\n\nUp or down? adaptive rounding for post-training quantization. Markus Nagel, Rana Ali Amjad, Mart Van Baalen, Christos Louizos, Tijmen Blankevoort, International Conference on Machine Learning. PMLR2020\n\n. OpenAI. Gpt-4 technical report. 2023\n\nDenis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazaridou, Ngoc Quan, Raffaella Pham, Sandro Bernardi, Marco Pezzelle, Gemma Baroni, Raquel Boleda, Fern\u00e1ndez, arXiv:1606.06031The lambada dataset: Word prediction requiring a broad discourse context. 2016arXiv preprint\n\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, Julien Launay, arXiv:2306.01116The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. 2023arXiv preprint\n\nLanguage models are unsupervised multitask learners. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, OpenAI blog. 1892019\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, The Journal of Machine Learning Research. 2112020\n\nXnor-net: Imagenet classification using binary convolutional neural networks. M Rastegari, V Ordonez, J Redmon, A Farhadi, 2016\n\nA simple and efficient algorithm for gene selection using sparse logistic regression. Krishnaj Shirish, S Shevade, Keerthi Sathiya, Bioinformatics. 19172003\n\nThibaut Hugo Touvron, Gautier Lavril, Xavier Izacard, Marie-Anne Martinet, Timoth\u00e9e Lachaux, Baptiste Lacroix, Naman Rozi\u00e8re, Eric Goyal, Hambro, arXiv:2302.13971Faisal Azhar, et al. Llama: Open and efficient foundation language models. 2023aarXiv preprint\n\nLlama 2: Open foundation and fine-tuned chat models. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, arXiv:2307.092882023barXiv preprint\n\nConvergence of a block coordinate descent method for nondifferentiable minimization. Paul Tseng, Journal of optimization theory and applications. 10932001\n\nTowards accurate post-training network quantization via bit-split and stitching. Peisong Wang, Qiang Chen, Xiangyu He, Jian Cheng, International Conference on Machine Learning. PMLR2020\n\nFinetuned language models are zero-shot learners. Jason Wei, Maarten Bosma, Y Vincent, Kelvin Zhao, Adams Wei Guu, Brian Yu, Nan Lester, Andrew M Du, Dai, V Quoc, Le, 2022a\n\nOutlier suppression: Pushing the limit of low-bit transformer language models. Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, Xianglong Liu, Advances in Neural Information Processing Systems. 2022b35\n\nSmoothquant: Accurate and efficient post-training quantization for large language models. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, Song Han, International Conference on Machine Learning. PMLR2023\n\nZeroquant: Efficient and affordable post-training quantization for large-scale transformers. Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, Yuxiong He, Advances in Neural Information Processing Systems. 202235\n\nZeroquant-v2: Exploring post-training quantization in llms from comprehensive study to low rank compensation. Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, Yuxiong He, 2023\n\nOpt: Open pre-trained transformer language models. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, arXiv:2205.010682022arXiv preprint\n", "annotations": {"author": "[{\"end\":184,\"start\":76},{\"end\":220,\"start\":185},{\"end\":254,\"start\":221},{\"end\":291,\"start\":255},{\"end\":323,\"start\":292},{\"end\":362,\"start\":324},{\"end\":451,\"start\":363}]", "publisher": null, "author_last_name": "[{\"end\":89,\"start\":83},{\"end\":197,\"start\":190},{\"end\":231,\"start\":226},{\"end\":268,\"start\":264},{\"end\":300,\"start\":297},{\"end\":339,\"start\":332},{\"end\":377,\"start\":369}]", "author_first_name": "[{\"end\":82,\"start\":76},{\"end\":189,\"start\":185},{\"end\":225,\"start\":221},{\"end\":263,\"start\":255},{\"end\":296,\"start\":292},{\"end\":331,\"start\":324},{\"end\":368,\"start\":363}]", "author_affiliation": "[{\"end\":132,\"start\":112},{\"end\":183,\"start\":134},{\"end\":219,\"start\":199},{\"end\":253,\"start\":233},{\"end\":290,\"start\":270},{\"end\":322,\"start\":302},{\"end\":361,\"start\":341},{\"end\":399,\"start\":379},{\"end\":450,\"start\":401}]", "title": "[{\"end\":63,\"start\":1},{\"end\":514,\"start\":452}]", "venue": null, "abstract": "[{\"end\":2205,\"start\":585}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2324,\"start\":2304},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2344,\"start\":2324},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2367,\"start\":2344},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":2388,\"start\":2367},{\"end\":2412,\"start\":2388},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":2431,\"start\":2412},{\"end\":2560,\"start\":2546},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":2578,\"start\":2560},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2803,\"start\":2780},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2823,\"start\":2803},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2930,\"start\":2910},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":2949,\"start\":2930},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3119,\"start\":3099},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3501,\"start\":3479},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3661,\"start\":3639},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3844,\"start\":3820},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3874,\"start\":3844},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3897,\"start\":3874},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4228,\"start\":4200},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":4249,\"start\":4228},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4268,\"start\":4249},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4509,\"start\":4487},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4539,\"start\":4516},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4566,\"start\":4548},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4998,\"start\":4976},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5439,\"start\":5426},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":5538,\"start\":5517},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":5558,\"start\":5538},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5580,\"start\":5558},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5607,\"start\":5580},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":5633,\"start\":5607},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":5659,\"start\":5633},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6889,\"start\":6868},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7199,\"start\":7175},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":7219,\"start\":7199},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":7238,\"start\":7219},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7425,\"start\":7403},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7953,\"start\":7924},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9731,\"start\":9709},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9815,\"start\":9787},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9839,\"start\":9815},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":9858,\"start\":9839},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":9930,\"start\":9911},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":10178,\"start\":10161},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10603,\"start\":10578},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10620,\"start\":10603},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":10638,\"start\":10620},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10911,\"start\":10893},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11111,\"start\":11088},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11594,\"start\":11573},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11617,\"start\":11596},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":11637,\"start\":11619},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":11657,\"start\":11639},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":11675,\"start\":11659},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":11697,\"start\":11675},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11871,\"start\":11852},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":12814,\"start\":12793},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13897,\"start\":13875},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13960,\"start\":13933},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":13986,\"start\":13962},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":14792,\"start\":14765},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14818,\"start\":14794},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15816,\"start\":15789},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":15839,\"start\":15818},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16294,\"start\":16276},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":16768,\"start\":16751},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":17618,\"start\":17605},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25678,\"start\":25657},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":25707,\"start\":25680},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":28123,\"start\":28100},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":28140,\"start\":28123},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":28464,\"start\":28439},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":28481,\"start\":28464},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":28499,\"start\":28481},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":30277,\"start\":30254},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":31773,\"start\":31744},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":35478,\"start\":35455},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":35495,\"start\":35478},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":35524,\"start\":35502},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":35551,\"start\":35533},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":35638,\"start\":35615},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":35687,\"start\":35669},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":36075,\"start\":36058},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":36094,\"start\":36077},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":36297,\"start\":36277},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":36329,\"start\":36305},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":36363,\"start\":36342},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":36603,\"start\":36582},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":36656,\"start\":36635},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":36705,\"start\":36684},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":36735,\"start\":36714},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":38705,\"start\":38683},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":38823,\"start\":38801},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":42046,\"start\":42029},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":46345,\"start\":46321}]", "figure": "[{\"attributes\":{\"id\":\"fig_1\"},\"end\":47359,\"start\":47249},{\"attributes\":{\"id\":\"fig_2\"},\"end\":47997,\"start\":47360},{\"attributes\":{\"id\":\"fig_3\"},\"end\":48071,\"start\":47998},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":49103,\"start\":48072},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":49552,\"start\":49104},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":50531,\"start\":49553},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":50645,\"start\":50532},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":50917,\"start\":50646},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":51455,\"start\":50918},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":52708,\"start\":51456},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":52895,\"start\":52709},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":53037,\"start\":52896}]", "paragraph": "[{\"end\":2668,\"start\":2221},{\"end\":3502,\"start\":2670},{\"end\":5199,\"start\":3504},{\"end\":7003,\"start\":5216},{\"end\":7657,\"start\":7005},{\"end\":8379,\"start\":7659},{\"end\":8428,\"start\":8381},{\"end\":8868,\"start\":8430},{\"end\":9119,\"start\":8870},{\"end\":9541,\"start\":9121},{\"end\":10076,\"start\":9558},{\"end\":10420,\"start\":10078},{\"end\":11254,\"start\":10422},{\"end\":11725,\"start\":11256},{\"end\":12176,\"start\":11740},{\"end\":12624,\"start\":12178},{\"end\":13099,\"start\":12626},{\"end\":13429,\"start\":13174},{\"end\":13562,\"start\":13431},{\"end\":13818,\"start\":13601},{\"end\":14585,\"start\":13848},{\"end\":15107,\"start\":14636},{\"end\":15114,\"start\":15109},{\"end\":15361,\"start\":15158},{\"end\":15468,\"start\":15463},{\"end\":15864,\"start\":15553},{\"end\":16247,\"start\":15947},{\"end\":17221,\"start\":16255},{\"end\":17875,\"start\":17558},{\"end\":18456,\"start\":17946},{\"end\":18909,\"start\":18458},{\"end\":19324,\"start\":18911},{\"end\":19532,\"start\":19371},{\"end\":19738,\"start\":19653},{\"end\":20059,\"start\":19809},{\"end\":20964,\"start\":20103},{\"end\":21087,\"start\":20966},{\"end\":21324,\"start\":21166},{\"end\":21740,\"start\":21412},{\"end\":21819,\"start\":21742},{\"end\":21882,\"start\":21881},{\"end\":22084,\"start\":21884},{\"end\":22895,\"start\":22086},{\"end\":23687,\"start\":23246},{\"end\":24023,\"start\":23689},{\"end\":24570,\"start\":24025},{\"end\":25097,\"start\":24572},{\"end\":25153,\"start\":25099},{\"end\":25503,\"start\":25461},{\"end\":25629,\"start\":25532},{\"end\":25764,\"start\":25631},{\"end\":26307,\"start\":25872},{\"end\":26396,\"start\":26309},{\"end\":26794,\"start\":26444},{\"end\":27943,\"start\":26796},{\"end\":27980,\"start\":27952},{\"end\":29860,\"start\":27996},{\"end\":30184,\"start\":29954},{\"end\":30663,\"start\":30193},{\"end\":31186,\"start\":30724},{\"end\":31820,\"start\":31214},{\"end\":31975,\"start\":31888},{\"end\":32209,\"start\":32023},{\"end\":32268,\"start\":32211},{\"end\":32743,\"start\":32294},{\"end\":33251,\"start\":32745},{\"end\":33904,\"start\":33253},{\"end\":34438,\"start\":33906},{\"end\":35186,\"start\":34440},{\"end\":35894,\"start\":35202},{\"end\":36206,\"start\":35896},{\"end\":37301,\"start\":36208},{\"end\":37944,\"start\":37336},{\"end\":38641,\"start\":38012},{\"end\":39487,\"start\":38673},{\"end\":40554,\"start\":39518},{\"end\":41281,\"start\":40579},{\"end\":41500,\"start\":41296},{\"end\":41734,\"start\":41502},{\"end\":42192,\"start\":41736},{\"end\":42657,\"start\":42194},{\"end\":42883,\"start\":42659},{\"end\":43155,\"start\":42885},{\"end\":43302,\"start\":43175},{\"end\":43627,\"start\":43304},{\"end\":43975,\"start\":43629},{\"end\":44380,\"start\":43977},{\"end\":44565,\"start\":44382},{\"end\":44712,\"start\":44585},{\"end\":45032,\"start\":44714},{\"end\":45341,\"start\":45034},{\"end\":45696,\"start\":45343},{\"end\":45789,\"start\":45698},{\"end\":45833,\"start\":45791},{\"end\":46155,\"start\":45858},{\"end\":46238,\"start\":46180},{\"end\":46499,\"start\":46294},{\"end\":46883,\"start\":46627},{\"end\":47248,\"start\":46885},{\"end\":47358,\"start\":47263},{\"end\":47996,\"start\":47364},{\"end\":48070,\"start\":48001},{\"end\":48312,\"start\":48085},{\"end\":49357,\"start\":49117},{\"end\":49551,\"start\":49550},{\"end\":49900,\"start\":49556},{\"end\":50644,\"start\":50545},{\"end\":50812,\"start\":50659},{\"end\":51255,\"start\":50921},{\"end\":51454,\"start\":51453},{\"end\":52352,\"start\":51459},{\"end\":53036,\"start\":52909}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":13173,\"start\":13100},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13599,\"start\":13563},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13600,\"start\":13599},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14635,\"start\":14586},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15157,\"start\":15115},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15462,\"start\":15362},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15552,\"start\":15469},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15946,\"start\":15865},{\"attributes\":{\"id\":\"formula_9\"},\"end\":17511,\"start\":17222},{\"attributes\":{\"id\":\"formula_10\"},\"end\":17945,\"start\":17876},{\"attributes\":{\"id\":\"formula_11\"},\"end\":19652,\"start\":19533},{\"attributes\":{\"id\":\"formula_12\"},\"end\":19808,\"start\":19739},{\"attributes\":{\"id\":\"formula_13\"},\"end\":20102,\"start\":20060},{\"attributes\":{\"id\":\"formula_14\"},\"end\":21165,\"start\":21088},{\"attributes\":{\"id\":\"formula_15\"},\"end\":21411,\"start\":21325},{\"attributes\":{\"id\":\"formula_16\"},\"end\":21880,\"start\":21820},{\"attributes\":{\"id\":\"formula_18\"},\"end\":23245,\"start\":22896},{\"attributes\":{\"id\":\"formula_20\"},\"end\":25460,\"start\":25154},{\"attributes\":{\"id\":\"formula_21\"},\"end\":25871,\"start\":25765},{\"attributes\":{\"id\":\"formula_22\"},\"end\":29953,\"start\":29861},{\"attributes\":{\"id\":\"formula_23\"},\"end\":30722,\"start\":30664},{\"attributes\":{\"id\":\"formula_24\"},\"end\":30723,\"start\":30722},{\"attributes\":{\"id\":\"formula_25\"},\"end\":31886,\"start\":31821},{\"attributes\":{\"id\":\"formula_26\"},\"end\":31887,\"start\":31886},{\"attributes\":{\"id\":\"formula_27\"},\"end\":32022,\"start\":31976},{\"attributes\":{\"id\":\"formula_28\"},\"end\":32293,\"start\":32269},{\"attributes\":{\"id\":\"formula_29\"},\"end\":46293,\"start\":46239},{\"attributes\":{\"id\":\"formula_30\"},\"end\":46588,\"start\":46500}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":7499,\"start\":7475},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":38179,\"start\":38175},{\"end\":39120,\"start\":39119},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":40177,\"start\":40176},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":40211,\"start\":40210},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":40242,\"start\":40241},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":40818,\"start\":40817},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":40843,\"start\":40842}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2219,\"start\":2207},{\"attributes\":{\"n\":\"1.1\"},\"end\":5214,\"start\":5202},{\"attributes\":{\"n\":\"1.2\"},\"end\":9556,\"start\":9544},{\"attributes\":{\"n\":\"2\"},\"end\":11738,\"start\":11728},{\"attributes\":{\"n\":\"2.2\"},\"end\":13839,\"start\":13821},{\"attributes\":{\"n\":\"2.2.1\"},\"end\":13846,\"start\":13842},{\"attributes\":{\"n\":\"2.2.2\"},\"end\":16253,\"start\":16250},{\"attributes\":{\"n\":\"3\"},\"end\":17532,\"start\":17513},{\"attributes\":{\"n\":\"3.1\"},\"end\":17556,\"start\":17535},{\"attributes\":{\"n\":\"3.2\"},\"end\":19369,\"start\":19327},{\"attributes\":{\"n\":\"3.3\"},\"end\":25530,\"start\":25506},{\"attributes\":{\"n\":\"3.4\"},\"end\":26442,\"start\":26399},{\"attributes\":{\"n\":\"3\"},\"end\":27950,\"start\":27946},{\"attributes\":{\"n\":\"4.1\"},\"end\":27994,\"start\":27983},{\"attributes\":{\"n\":\"4.2\"},\"end\":30191,\"start\":30187},{\"attributes\":{\"n\":\"4.3\"},\"end\":31212,\"start\":31189},{\"attributes\":{\"n\":\"5\"},\"end\":35200,\"start\":35189},{\"attributes\":{\"n\":\"5.1\"},\"end\":37334,\"start\":37304},{\"end\":37977,\"start\":37947},{\"attributes\":{\"n\":\"5.2\"},\"end\":38010,\"start\":37980},{\"attributes\":{\"n\":\"5.3\"},\"end\":38671,\"start\":38644},{\"attributes\":{\"n\":\"5.4\"},\"end\":39516,\"start\":39490},{\"attributes\":{\"n\":\"5.4.1\"},\"end\":40577,\"start\":40557},{\"attributes\":{\"n\":\"6\"},\"end\":41294,\"start\":41284},{\"end\":43173,\"start\":43158},{\"end\":44583,\"start\":44568},{\"end\":45856,\"start\":45836},{\"end\":46178,\"start\":46158},{\"end\":46625,\"start\":46590},{\"end\":47260,\"start\":47250},{\"end\":47362,\"start\":47361},{\"end\":48082,\"start\":48073},{\"end\":49114,\"start\":49105},{\"end\":50542,\"start\":50533},{\"end\":50656,\"start\":50647},{\"end\":52717,\"start\":52710},{\"end\":52906,\"start\":52897}]", "table": "[{\"end\":49103,\"start\":48313},{\"end\":49549,\"start\":49358},{\"end\":50531,\"start\":49901},{\"end\":50917,\"start\":50813},{\"end\":51452,\"start\":51256},{\"end\":52708,\"start\":52353},{\"end\":52895,\"start\":52719}]", "figure_caption": "[{\"end\":47359,\"start\":47262},{\"end\":47997,\"start\":47363},{\"end\":48071,\"start\":48000},{\"end\":48313,\"start\":48084},{\"end\":49358,\"start\":49116},{\"end\":49901,\"start\":49555},{\"end\":50645,\"start\":50544},{\"end\":50813,\"start\":50658},{\"end\":51256,\"start\":50920},{\"end\":52353,\"start\":51458},{\"end\":53037,\"start\":52908}]", "figure_ref": "[{\"end\":4940,\"start\":4939},{\"end\":6205,\"start\":6204},{\"end\":7655,\"start\":7654},{\"end\":26807,\"start\":26806},{\"end\":27554,\"start\":27553},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":37598,\"start\":37597},{\"end\":38871,\"start\":38870}]", "bib_author_first_name": "[{\"end\":56644,\"start\":56643},{\"end\":56657,\"start\":56656},{\"end\":56668,\"start\":56667},{\"end\":56681,\"start\":56680},{\"end\":56694,\"start\":56693},{\"end\":56705,\"start\":56704},{\"end\":56715,\"start\":56714},{\"end\":56847,\"start\":56843},{\"end\":56860,\"start\":56854},{\"end\":56862,\"start\":56861},{\"end\":56992,\"start\":56988},{\"end\":57003,\"start\":56999},{\"end\":57167,\"start\":57161},{\"end\":57181,\"start\":57176},{\"end\":57193,\"start\":57188},{\"end\":57298,\"start\":57292},{\"end\":57315,\"start\":57311},{\"end\":57317,\"start\":57316},{\"end\":57470,\"start\":57463},{\"end\":57489,\"start\":57483},{\"end\":57503,\"start\":57497},{\"end\":57595,\"start\":57592},{\"end\":57611,\"start\":57603},{\"end\":57622,\"start\":57618},{\"end\":57637,\"start\":57630},{\"end\":57652,\"start\":57647},{\"end\":57654,\"start\":57653},{\"end\":57671,\"start\":57663},{\"end\":57688,\"start\":57682},{\"end\":57708,\"start\":57702},{\"end\":57722,\"start\":57716},{\"end\":57737,\"start\":57731},{\"end\":57851,\"start\":57850},{\"end\":57860,\"start\":57859},{\"end\":57939,\"start\":57929},{\"end\":57955,\"start\":57947},{\"end\":58034,\"start\":58031},{\"end\":58049,\"start\":58045},{\"end\":58063,\"start\":58057},{\"end\":58077,\"start\":58073},{\"end\":58273,\"start\":58270},{\"end\":58290,\"start\":58284},{\"end\":58309,\"start\":58305},{\"end\":58327,\"start\":58322},{\"end\":58345,\"start\":58340},{\"end\":58364,\"start\":58355},{\"end\":58388,\"start\":58381},{\"end\":58402,\"start\":58399},{\"end\":58545,\"start\":58540},{\"end\":58562,\"start\":58554},{\"end\":58576,\"start\":58570},{\"end\":58590,\"start\":58582},{\"end\":58718,\"start\":58715},{\"end\":58732,\"start\":58725},{\"end\":58744,\"start\":58739},{\"end\":58904,\"start\":58899},{\"end\":58917,\"start\":58914},{\"end\":59061,\"start\":59056},{\"end\":59078,\"start\":59071},{\"end\":59098,\"start\":59095},{\"end\":59272,\"start\":59266},{\"end\":59289,\"start\":59283},{\"end\":59301,\"start\":59298},{\"end\":59434,\"start\":59430},{\"end\":59450,\"start\":59444},{\"end\":59460,\"start\":59456},{\"end\":59473,\"start\":59467},{\"end\":59486,\"start\":59479},{\"end\":59488,\"start\":59487},{\"end\":59502,\"start\":59498},{\"end\":59639,\"start\":59634},{\"end\":59654,\"start\":59649},{\"end\":59823,\"start\":59816},{\"end\":59838,\"start\":59833},{\"end\":59989,\"start\":59982},{\"end\":60002,\"start\":59999},{\"end\":60016,\"start\":60013},{\"end\":60032,\"start\":60026},{\"end\":60050,\"start\":60041},{\"end\":60118,\"start\":60112},{\"end\":60138,\"start\":60129},{\"end\":60155,\"start\":60149},{\"end\":60169,\"start\":60164},{\"end\":60189,\"start\":60183},{\"end\":60200,\"start\":60195},{\"end\":60218,\"start\":60213},{\"end\":60231,\"start\":60227},{\"end\":60236,\"start\":60232},{\"end\":60252,\"start\":60244},{\"end\":60269,\"start\":60264},{\"end\":60389,\"start\":60385},{\"end\":60402,\"start\":60398},{\"end\":60416,\"start\":60412},{\"end\":60428,\"start\":60425},{\"end\":60443,\"start\":60437},{\"end\":60578,\"start\":60574},{\"end\":60591,\"start\":60587},{\"end\":60605,\"start\":60601},{\"end\":60617,\"start\":60614},{\"end\":60632,\"start\":60626},{\"end\":60744,\"start\":60739},{\"end\":60756,\"start\":60753},{\"end\":60772,\"start\":60769},{\"end\":60786,\"start\":60783},{\"end\":60788,\"start\":60787},{\"end\":60804,\"start\":60796},{\"end\":60817,\"start\":60812},{\"end\":60830,\"start\":60825},{\"end\":60841,\"start\":60837},{\"end\":60858,\"start\":60851},{\"end\":60868,\"start\":60863},{\"end\":60889,\"start\":60883},{\"end\":60902,\"start\":60895},{\"end\":60915,\"start\":60911},{\"end\":60929,\"start\":60925},{\"end\":60941,\"start\":60936},{\"end\":60951,\"start\":60946},{\"end\":60965,\"start\":60958},{\"end\":60967,\"start\":60966},{\"end\":60981,\"start\":60977},{\"end\":61146,\"start\":61140},{\"end\":61169,\"start\":61163},{\"end\":61191,\"start\":61180},{\"end\":61200,\"start\":61198},{\"end\":61223,\"start\":61216},{\"end\":61238,\"start\":61230},{\"end\":61257,\"start\":61250},{\"end\":61266,\"start\":61258},{\"end\":61275,\"start\":61272},{\"end\":61407,\"start\":61403},{\"end\":61419,\"start\":61415},{\"end\":61432,\"start\":61428},{\"end\":61579,\"start\":61577},{\"end\":61592,\"start\":61585},{\"end\":61606,\"start\":61599},{\"end\":61618,\"start\":61613},{\"end\":61631,\"start\":61625},{\"end\":61642,\"start\":61638},{\"end\":61749,\"start\":61744},{\"end\":61763,\"start\":61758},{\"end\":61773,\"start\":61769},{\"end\":61779,\"start\":61776},{\"end\":61801,\"start\":61795},{\"end\":61816,\"start\":61813},{\"end\":61827,\"start\":61823},{\"end\":61843,\"start\":61838},{\"end\":61856,\"start\":61850},{\"end\":62027,\"start\":62022},{\"end\":62044,\"start\":62038},{\"end\":62105,\"start\":62098},{\"end\":62121,\"start\":62114},{\"end\":62134,\"start\":62129},{\"end\":62152,\"start\":62145},{\"end\":62298,\"start\":62292},{\"end\":62310,\"start\":62306},{\"end\":62326,\"start\":62322},{\"end\":62347,\"start\":62339},{\"end\":62363,\"start\":62357},{\"end\":62478,\"start\":62473},{\"end\":62494,\"start\":62488},{\"end\":62515,\"start\":62507},{\"end\":62531,\"start\":62527},{\"end\":62547,\"start\":62538},{\"end\":62560,\"start\":62554},{\"end\":62576,\"start\":62571},{\"end\":62592,\"start\":62587},{\"end\":62607,\"start\":62601},{\"end\":62746,\"start\":62737},{\"end\":62762,\"start\":62755},{\"end\":62779,\"start\":62773},{\"end\":62797,\"start\":62789},{\"end\":62818,\"start\":62808},{\"end\":62834,\"start\":62829},{\"end\":62854,\"start\":62846},{\"end\":62871,\"start\":62864},{\"end\":62890,\"start\":62884},{\"end\":63095,\"start\":63091},{\"end\":63112,\"start\":63105},{\"end\":63122,\"start\":63117},{\"end\":63135,\"start\":63130},{\"end\":63147,\"start\":63142},{\"end\":63160,\"start\":63156},{\"end\":63282,\"start\":63277},{\"end\":63295,\"start\":63291},{\"end\":63309,\"start\":63305},{\"end\":63328,\"start\":63319},{\"end\":63340,\"start\":63334},{\"end\":63356,\"start\":63349},{\"end\":63370,\"start\":63365},{\"end\":63380,\"start\":63377},{\"end\":63390,\"start\":63385},{\"end\":63392,\"start\":63391},{\"end\":63528,\"start\":63527},{\"end\":63541,\"start\":63540},{\"end\":63552,\"start\":63551},{\"end\":63562,\"start\":63561},{\"end\":63672,\"start\":63664},{\"end\":63683,\"start\":63682},{\"end\":63700,\"start\":63693},{\"end\":63743,\"start\":63736},{\"end\":63765,\"start\":63758},{\"end\":63780,\"start\":63774},{\"end\":63800,\"start\":63790},{\"end\":63819,\"start\":63811},{\"end\":63837,\"start\":63829},{\"end\":63852,\"start\":63847},{\"end\":63866,\"start\":63862},{\"end\":64051,\"start\":64047},{\"end\":64066,\"start\":64061},{\"end\":64080,\"start\":64075},{\"end\":64093,\"start\":64088},{\"end\":64107,\"start\":64102},{\"end\":64126,\"start\":64119},{\"end\":64142,\"start\":64135},{\"end\":64160,\"start\":64154},{\"end\":64176,\"start\":64168},{\"end\":64193,\"start\":64187},{\"end\":64329,\"start\":64325},{\"end\":64484,\"start\":64477},{\"end\":64496,\"start\":64491},{\"end\":64510,\"start\":64503},{\"end\":64519,\"start\":64515},{\"end\":64638,\"start\":64633},{\"end\":64651,\"start\":64644},{\"end\":64660,\"start\":64659},{\"end\":64676,\"start\":64670},{\"end\":64688,\"start\":64683},{\"end\":64692,\"start\":64689},{\"end\":64703,\"start\":64698},{\"end\":64711,\"start\":64708},{\"end\":64726,\"start\":64720},{\"end\":64728,\"start\":64727},{\"end\":64739,\"start\":64738},{\"end\":64843,\"start\":64836},{\"end\":64856,\"start\":64849},{\"end\":64872,\"start\":64864},{\"end\":64886,\"start\":64880},{\"end\":64902,\"start\":64893},{\"end\":64912,\"start\":64910},{\"end\":64927,\"start\":64920},{\"end\":64941,\"start\":64932},{\"end\":65106,\"start\":65097},{\"end\":65115,\"start\":65113},{\"end\":65128,\"start\":65121},{\"end\":65140,\"start\":65137},{\"end\":65151,\"start\":65145},{\"end\":65165,\"start\":65161},{\"end\":65326,\"start\":65320},{\"end\":65336,\"start\":65332},{\"end\":65362,\"start\":65356},{\"end\":65377,\"start\":65370},{\"end\":65390,\"start\":65382},{\"end\":65402,\"start\":65395},{\"end\":65582,\"start\":65576},{\"end\":65595,\"start\":65588},{\"end\":65605,\"start\":65600},{\"end\":65617,\"start\":65610},{\"end\":65631,\"start\":65624},{\"end\":65698,\"start\":65693},{\"end\":65713,\"start\":65706},{\"end\":65727,\"start\":65722},{\"end\":65740,\"start\":65735},{\"end\":65754,\"start\":65750},{\"end\":65768,\"start\":65761},{\"end\":65786,\"start\":65775},{\"end\":65798,\"start\":65794},{\"end\":65809,\"start\":65805},{\"end\":65816,\"start\":65814}]", "bib_author_last_name": "[{\"end\":56654,\"start\":56645},{\"end\":56665,\"start\":56658},{\"end\":56678,\"start\":56669},{\"end\":56691,\"start\":56682},{\"end\":56702,\"start\":56695},{\"end\":56712,\"start\":56706},{\"end\":56724,\"start\":56716},{\"end\":56852,\"start\":56848},{\"end\":56868,\"start\":56863},{\"end\":56997,\"start\":56993},{\"end\":57012,\"start\":57004},{\"end\":57174,\"start\":57168},{\"end\":57186,\"start\":57182},{\"end\":57202,\"start\":57194},{\"end\":57309,\"start\":57299},{\"end\":57324,\"start\":57318},{\"end\":57481,\"start\":57471},{\"end\":57495,\"start\":57490},{\"end\":57515,\"start\":57504},{\"end\":57601,\"start\":57596},{\"end\":57616,\"start\":57612},{\"end\":57628,\"start\":57623},{\"end\":57645,\"start\":57638},{\"end\":57661,\"start\":57655},{\"end\":57680,\"start\":57672},{\"end\":57700,\"start\":57689},{\"end\":57714,\"start\":57709},{\"end\":57729,\"start\":57723},{\"end\":57744,\"start\":57738},{\"end\":57857,\"start\":57852},{\"end\":57874,\"start\":57861},{\"end\":57945,\"start\":57940},{\"end\":57959,\"start\":57956},{\"end\":58043,\"start\":58035},{\"end\":58055,\"start\":58050},{\"end\":58071,\"start\":58064},{\"end\":58089,\"start\":58078},{\"end\":58282,\"start\":58274},{\"end\":58303,\"start\":58291},{\"end\":58320,\"start\":58310},{\"end\":58338,\"start\":58328},{\"end\":58353,\"start\":58346},{\"end\":58379,\"start\":58365},{\"end\":58397,\"start\":58389},{\"end\":58410,\"start\":58403},{\"end\":58420,\"start\":58412},{\"end\":58552,\"start\":58546},{\"end\":58568,\"start\":58563},{\"end\":58580,\"start\":58577},{\"end\":58600,\"start\":58591},{\"end\":58723,\"start\":58719},{\"end\":58737,\"start\":58733},{\"end\":58748,\"start\":58745},{\"end\":58912,\"start\":58905},{\"end\":58926,\"start\":58918},{\"end\":59069,\"start\":59062},{\"end\":59093,\"start\":59079},{\"end\":59106,\"start\":59099},{\"end\":59116,\"start\":59108},{\"end\":59281,\"start\":59273},{\"end\":59296,\"start\":59290},{\"end\":59312,\"start\":59302},{\"end\":59442,\"start\":59435},{\"end\":59454,\"start\":59451},{\"end\":59465,\"start\":59461},{\"end\":59477,\"start\":59474},{\"end\":59496,\"start\":59489},{\"end\":59510,\"start\":59503},{\"end\":59647,\"start\":59640},{\"end\":59660,\"start\":59655},{\"end\":59831,\"start\":59824},{\"end\":59847,\"start\":59839},{\"end\":59997,\"start\":59990},{\"end\":60011,\"start\":60003},{\"end\":60024,\"start\":60017},{\"end\":60039,\"start\":60033},{\"end\":60056,\"start\":60051},{\"end\":60127,\"start\":60119},{\"end\":60147,\"start\":60139},{\"end\":60162,\"start\":60156},{\"end\":60181,\"start\":60170},{\"end\":60193,\"start\":60190},{\"end\":60211,\"start\":60201},{\"end\":60225,\"start\":60219},{\"end\":60242,\"start\":60237},{\"end\":60262,\"start\":60253},{\"end\":60275,\"start\":60270},{\"end\":60282,\"start\":60277},{\"end\":60396,\"start\":60390},{\"end\":60410,\"start\":60403},{\"end\":60423,\"start\":60417},{\"end\":60435,\"start\":60429},{\"end\":60450,\"start\":60444},{\"end\":60585,\"start\":60579},{\"end\":60599,\"start\":60592},{\"end\":60612,\"start\":60606},{\"end\":60624,\"start\":60618},{\"end\":60639,\"start\":60633},{\"end\":60751,\"start\":60745},{\"end\":60767,\"start\":60757},{\"end\":60781,\"start\":60773},{\"end\":60794,\"start\":60789},{\"end\":60810,\"start\":60805},{\"end\":60823,\"start\":60818},{\"end\":60835,\"start\":60831},{\"end\":60849,\"start\":60842},{\"end\":60861,\"start\":60859},{\"end\":60875,\"start\":60869},{\"end\":60893,\"start\":60890},{\"end\":60909,\"start\":60903},{\"end\":60923,\"start\":60916},{\"end\":60934,\"start\":60930},{\"end\":60944,\"start\":60942},{\"end\":60956,\"start\":60952},{\"end\":60975,\"start\":60968},{\"end\":60989,\"start\":60982},{\"end\":61161,\"start\":61147},{\"end\":61178,\"start\":61170},{\"end\":61196,\"start\":61192},{\"end\":61214,\"start\":61201},{\"end\":61228,\"start\":61224},{\"end\":61248,\"start\":61239},{\"end\":61270,\"start\":61267},{\"end\":61286,\"start\":61276},{\"end\":61294,\"start\":61288},{\"end\":61413,\"start\":61408},{\"end\":61426,\"start\":61420},{\"end\":61438,\"start\":61433},{\"end\":61583,\"start\":61580},{\"end\":61597,\"start\":61593},{\"end\":61611,\"start\":61607},{\"end\":61623,\"start\":61619},{\"end\":61636,\"start\":61632},{\"end\":61646,\"start\":61643},{\"end\":61756,\"start\":61750},{\"end\":61767,\"start\":61764},{\"end\":61793,\"start\":61780},{\"end\":61811,\"start\":61802},{\"end\":61821,\"start\":61817},{\"end\":61836,\"start\":61828},{\"end\":61848,\"start\":61844},{\"end\":61868,\"start\":61857},{\"end\":62036,\"start\":62028},{\"end\":62051,\"start\":62045},{\"end\":62112,\"start\":62106},{\"end\":62127,\"start\":62122},{\"end\":62143,\"start\":62135},{\"end\":62159,\"start\":62153},{\"end\":62304,\"start\":62299},{\"end\":62320,\"start\":62311},{\"end\":62337,\"start\":62327},{\"end\":62355,\"start\":62348},{\"end\":62375,\"start\":62364},{\"end\":62486,\"start\":62479},{\"end\":62505,\"start\":62495},{\"end\":62525,\"start\":62516},{\"end\":62536,\"start\":62532},{\"end\":62552,\"start\":62548},{\"end\":62569,\"start\":62561},{\"end\":62585,\"start\":62577},{\"end\":62599,\"start\":62593},{\"end\":62614,\"start\":62608},{\"end\":62625,\"start\":62616},{\"end\":62753,\"start\":62747},{\"end\":62771,\"start\":62763},{\"end\":62787,\"start\":62780},{\"end\":62806,\"start\":62798},{\"end\":62827,\"start\":62819},{\"end\":62844,\"start\":62835},{\"end\":62862,\"start\":62855},{\"end\":62882,\"start\":62872},{\"end\":62897,\"start\":62891},{\"end\":63103,\"start\":63096},{\"end\":63115,\"start\":63113},{\"end\":63128,\"start\":63123},{\"end\":63140,\"start\":63136},{\"end\":63154,\"start\":63148},{\"end\":63170,\"start\":63161},{\"end\":63289,\"start\":63283},{\"end\":63303,\"start\":63296},{\"end\":63317,\"start\":63310},{\"end\":63332,\"start\":63329},{\"end\":63347,\"start\":63341},{\"end\":63363,\"start\":63357},{\"end\":63375,\"start\":63371},{\"end\":63383,\"start\":63381},{\"end\":63396,\"start\":63393},{\"end\":63538,\"start\":63529},{\"end\":63549,\"start\":63542},{\"end\":63559,\"start\":63553},{\"end\":63570,\"start\":63563},{\"end\":63680,\"start\":63673},{\"end\":63691,\"start\":63684},{\"end\":63708,\"start\":63701},{\"end\":63756,\"start\":63744},{\"end\":63772,\"start\":63766},{\"end\":63788,\"start\":63781},{\"end\":63809,\"start\":63801},{\"end\":63827,\"start\":63820},{\"end\":63845,\"start\":63838},{\"end\":63860,\"start\":63853},{\"end\":63872,\"start\":63867},{\"end\":63880,\"start\":63874},{\"end\":64059,\"start\":64052},{\"end\":64073,\"start\":64067},{\"end\":64086,\"start\":64081},{\"end\":64100,\"start\":64094},{\"end\":64117,\"start\":64108},{\"end\":64133,\"start\":64127},{\"end\":64152,\"start\":64143},{\"end\":64166,\"start\":64161},{\"end\":64185,\"start\":64177},{\"end\":64201,\"start\":64194},{\"end\":64335,\"start\":64330},{\"end\":64489,\"start\":64485},{\"end\":64501,\"start\":64497},{\"end\":64513,\"start\":64511},{\"end\":64525,\"start\":64520},{\"end\":64642,\"start\":64639},{\"end\":64657,\"start\":64652},{\"end\":64668,\"start\":64661},{\"end\":64681,\"start\":64677},{\"end\":64696,\"start\":64693},{\"end\":64706,\"start\":64704},{\"end\":64718,\"start\":64712},{\"end\":64731,\"start\":64729},{\"end\":64736,\"start\":64733},{\"end\":64744,\"start\":64740},{\"end\":64748,\"start\":64746},{\"end\":64847,\"start\":64844},{\"end\":64862,\"start\":64857},{\"end\":64878,\"start\":64873},{\"end\":64891,\"start\":64887},{\"end\":64908,\"start\":64903},{\"end\":64918,\"start\":64913},{\"end\":64930,\"start\":64928},{\"end\":64945,\"start\":64942},{\"end\":65111,\"start\":65107},{\"end\":65119,\"start\":65116},{\"end\":65135,\"start\":65129},{\"end\":65143,\"start\":65141},{\"end\":65159,\"start\":65152},{\"end\":65169,\"start\":65166},{\"end\":65330,\"start\":65327},{\"end\":65354,\"start\":65337},{\"end\":65368,\"start\":65363},{\"end\":65380,\"start\":65378},{\"end\":65393,\"start\":65391},{\"end\":65405,\"start\":65403},{\"end\":65586,\"start\":65583},{\"end\":65598,\"start\":65596},{\"end\":65608,\"start\":65606},{\"end\":65622,\"start\":65618},{\"end\":65634,\"start\":65632},{\"end\":65704,\"start\":65699},{\"end\":65720,\"start\":65714},{\"end\":65733,\"start\":65728},{\"end\":65748,\"start\":65741},{\"end\":65759,\"start\":65755},{\"end\":65773,\"start\":65769},{\"end\":65792,\"start\":65787},{\"end\":65803,\"start\":65799},{\"end\":65812,\"start\":65810},{\"end\":65829,\"start\":65817}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":850237},\"end\":56758,\"start\":56557},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":9527832},\"end\":56907,\"start\":56760},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":3072879},\"end\":57054,\"start\":56909},{\"attributes\":{\"doi\":\"arXiv:2307.09366\",\"id\":\"b3\"},\"end\":57238,\"start\":57056},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":9315983},\"end\":57378,\"start\":57240},{\"attributes\":{\"doi\":\"arXiv:2109.12948\",\"id\":\"b5\"},\"end\":57551,\"start\":57380},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":218971783},\"end\":57803,\"start\":57553},{\"attributes\":{\"id\":\"b7\"},\"end\":57880,\"start\":57805},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":961425},\"end\":58029,\"start\":57882},{\"attributes\":{\"doi\":\"arXiv:2208.07339\",\"id\":\"b9\"},\"end\":58186,\"start\":58031},{\"attributes\":{\"doi\":\"arXiv:2306.03078\",\"id\":\"b10\"},\"end\":58456,\"start\":58188},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b11\"},\"end\":58636,\"start\":58458},{\"attributes\":{\"id\":\"b12\"},\"end\":58805,\"start\":58638},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":251765570},\"end\":58985,\"start\":58807},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":259298689},\"end\":59189,\"start\":58987},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":42639},\"end\":59355,\"start\":59191},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":232352683},\"end\":59563,\"start\":59357},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":7057040},\"end\":59718,\"start\":59565},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3799276},\"end\":59877,\"start\":59720},{\"attributes\":{\"id\":\"b19\"},\"end\":60062,\"start\":59879},{\"attributes\":{\"doi\":\"arXiv:2203.15556\",\"id\":\"b20\"},\"end\":60318,\"start\":60064},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":235825979},\"end\":60507,\"start\":60320},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":235825979},\"end\":60696,\"start\":60509},{\"attributes\":{\"id\":\"b23\"},\"end\":60881,\"start\":60698},{\"attributes\":{\"doi\":\"arXiv:2306.07629\",\"id\":\"b24\"},\"end\":61068,\"start\":60883},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":257378329},\"end\":61379,\"start\":61070},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":7785881},\"end\":61495,\"start\":61381},{\"attributes\":{\"doi\":\"arXiv:2306.00978\",\"id\":\"b27\"},\"end\":61682,\"start\":61497},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":5151364},\"end\":61968,\"start\":61684},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":6795998},\"end\":62096,\"start\":61970},{\"attributes\":{\"doi\":\"arXiv:1609.07843\",\"id\":\"b30\"},\"end\":62228,\"start\":62098},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":216056295},\"end\":62431,\"start\":62230},{\"attributes\":{\"id\":\"b32\"},\"end\":62471,\"start\":62433},{\"attributes\":{\"doi\":\"arXiv:1606.06031\",\"id\":\"b33\"},\"end\":62735,\"start\":62473},{\"attributes\":{\"doi\":\"arXiv:2306.01116\",\"id\":\"b34\"},\"end\":63036,\"start\":62737},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":160025533},\"end\":63192,\"start\":63038},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":204838007},\"end\":63447,\"start\":63194},{\"attributes\":{\"id\":\"b37\"},\"end\":63576,\"start\":63449},{\"attributes\":{\"id\":\"b38\"},\"end\":63734,\"start\":63578},{\"attributes\":{\"doi\":\"arXiv:2302.13971\",\"id\":\"b39\"},\"end\":63992,\"start\":63736},{\"attributes\":{\"doi\":\"arXiv:2307.09288\",\"id\":\"b40\"},\"end\":64238,\"start\":63994},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":18740145},\"end\":64394,\"start\":64240},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":221506623},\"end\":64581,\"start\":64396},{\"attributes\":{\"id\":\"b43\"},\"end\":64755,\"start\":64583},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":252545187},\"end\":65005,\"start\":64757},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":253708271},\"end\":65225,\"start\":65007},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":249395624},\"end\":65464,\"start\":65227},{\"attributes\":{\"id\":\"b47\"},\"end\":65640,\"start\":65466},{\"attributes\":{\"doi\":\"arXiv:2205.01068\",\"id\":\"b48\"},\"end\":65865,\"start\":65642}]", "bib_title": "[{\"end\":56641,\"start\":56557},{\"end\":56841,\"start\":56760},{\"end\":56986,\"start\":56909},{\"end\":57290,\"start\":57240},{\"end\":57590,\"start\":57553},{\"end\":57927,\"start\":57882},{\"end\":58897,\"start\":58807},{\"end\":59054,\"start\":58987},{\"end\":59264,\"start\":59191},{\"end\":59428,\"start\":59357},{\"end\":59632,\"start\":59565},{\"end\":59814,\"start\":59720},{\"end\":60383,\"start\":60320},{\"end\":60572,\"start\":60509},{\"end\":61138,\"start\":61070},{\"end\":61401,\"start\":61381},{\"end\":61742,\"start\":61684},{\"end\":62020,\"start\":61970},{\"end\":62290,\"start\":62230},{\"end\":63089,\"start\":63038},{\"end\":63275,\"start\":63194},{\"end\":63662,\"start\":63578},{\"end\":64323,\"start\":64240},{\"end\":64475,\"start\":64396},{\"end\":64834,\"start\":64757},{\"end\":65095,\"start\":65007},{\"end\":65318,\"start\":65227}]", "bib_author": "[{\"end\":56656,\"start\":56643},{\"end\":56667,\"start\":56656},{\"end\":56680,\"start\":56667},{\"end\":56693,\"start\":56680},{\"end\":56704,\"start\":56693},{\"end\":56714,\"start\":56704},{\"end\":56726,\"start\":56714},{\"end\":56854,\"start\":56843},{\"end\":56870,\"start\":56854},{\"end\":56999,\"start\":56988},{\"end\":57014,\"start\":56999},{\"end\":57176,\"start\":57161},{\"end\":57188,\"start\":57176},{\"end\":57204,\"start\":57188},{\"end\":57311,\"start\":57292},{\"end\":57326,\"start\":57311},{\"end\":57483,\"start\":57463},{\"end\":57497,\"start\":57483},{\"end\":57517,\"start\":57497},{\"end\":57603,\"start\":57592},{\"end\":57618,\"start\":57603},{\"end\":57630,\"start\":57618},{\"end\":57647,\"start\":57630},{\"end\":57663,\"start\":57647},{\"end\":57682,\"start\":57663},{\"end\":57702,\"start\":57682},{\"end\":57716,\"start\":57702},{\"end\":57731,\"start\":57716},{\"end\":57746,\"start\":57731},{\"end\":57859,\"start\":57850},{\"end\":57876,\"start\":57859},{\"end\":57947,\"start\":57929},{\"end\":57961,\"start\":57947},{\"end\":58045,\"start\":58031},{\"end\":58057,\"start\":58045},{\"end\":58073,\"start\":58057},{\"end\":58091,\"start\":58073},{\"end\":58284,\"start\":58270},{\"end\":58305,\"start\":58284},{\"end\":58322,\"start\":58305},{\"end\":58340,\"start\":58322},{\"end\":58355,\"start\":58340},{\"end\":58381,\"start\":58355},{\"end\":58399,\"start\":58381},{\"end\":58412,\"start\":58399},{\"end\":58422,\"start\":58412},{\"end\":58554,\"start\":58540},{\"end\":58570,\"start\":58554},{\"end\":58582,\"start\":58570},{\"end\":58602,\"start\":58582},{\"end\":58725,\"start\":58715},{\"end\":58739,\"start\":58725},{\"end\":58750,\"start\":58739},{\"end\":58914,\"start\":58899},{\"end\":58928,\"start\":58914},{\"end\":59071,\"start\":59056},{\"end\":59095,\"start\":59071},{\"end\":59108,\"start\":59095},{\"end\":59118,\"start\":59108},{\"end\":59283,\"start\":59266},{\"end\":59298,\"start\":59283},{\"end\":59314,\"start\":59298},{\"end\":59444,\"start\":59430},{\"end\":59456,\"start\":59444},{\"end\":59467,\"start\":59456},{\"end\":59479,\"start\":59467},{\"end\":59498,\"start\":59479},{\"end\":59512,\"start\":59498},{\"end\":59649,\"start\":59634},{\"end\":59662,\"start\":59649},{\"end\":59833,\"start\":59816},{\"end\":59849,\"start\":59833},{\"end\":59999,\"start\":59982},{\"end\":60013,\"start\":59999},{\"end\":60026,\"start\":60013},{\"end\":60041,\"start\":60026},{\"end\":60058,\"start\":60041},{\"end\":60129,\"start\":60112},{\"end\":60149,\"start\":60129},{\"end\":60164,\"start\":60149},{\"end\":60183,\"start\":60164},{\"end\":60195,\"start\":60183},{\"end\":60213,\"start\":60195},{\"end\":60227,\"start\":60213},{\"end\":60244,\"start\":60227},{\"end\":60264,\"start\":60244},{\"end\":60277,\"start\":60264},{\"end\":60284,\"start\":60277},{\"end\":60398,\"start\":60385},{\"end\":60412,\"start\":60398},{\"end\":60425,\"start\":60412},{\"end\":60437,\"start\":60425},{\"end\":60452,\"start\":60437},{\"end\":60587,\"start\":60574},{\"end\":60601,\"start\":60587},{\"end\":60614,\"start\":60601},{\"end\":60626,\"start\":60614},{\"end\":60641,\"start\":60626},{\"end\":60753,\"start\":60739},{\"end\":60769,\"start\":60753},{\"end\":60783,\"start\":60769},{\"end\":60796,\"start\":60783},{\"end\":60812,\"start\":60796},{\"end\":60825,\"start\":60812},{\"end\":60837,\"start\":60825},{\"end\":60851,\"start\":60837},{\"end\":60863,\"start\":60851},{\"end\":60877,\"start\":60863},{\"end\":60895,\"start\":60883},{\"end\":60911,\"start\":60895},{\"end\":60925,\"start\":60911},{\"end\":60936,\"start\":60925},{\"end\":60946,\"start\":60936},{\"end\":60958,\"start\":60946},{\"end\":60977,\"start\":60958},{\"end\":60991,\"start\":60977},{\"end\":61163,\"start\":61140},{\"end\":61180,\"start\":61163},{\"end\":61198,\"start\":61180},{\"end\":61216,\"start\":61198},{\"end\":61230,\"start\":61216},{\"end\":61250,\"start\":61230},{\"end\":61272,\"start\":61250},{\"end\":61288,\"start\":61272},{\"end\":61296,\"start\":61288},{\"end\":61415,\"start\":61403},{\"end\":61428,\"start\":61415},{\"end\":61440,\"start\":61428},{\"end\":61585,\"start\":61577},{\"end\":61599,\"start\":61585},{\"end\":61613,\"start\":61599},{\"end\":61625,\"start\":61613},{\"end\":61638,\"start\":61625},{\"end\":61648,\"start\":61638},{\"end\":61758,\"start\":61744},{\"end\":61769,\"start\":61758},{\"end\":61776,\"start\":61769},{\"end\":61795,\"start\":61776},{\"end\":61813,\"start\":61795},{\"end\":61823,\"start\":61813},{\"end\":61838,\"start\":61823},{\"end\":61850,\"start\":61838},{\"end\":61870,\"start\":61850},{\"end\":62038,\"start\":62022},{\"end\":62053,\"start\":62038},{\"end\":62114,\"start\":62098},{\"end\":62129,\"start\":62114},{\"end\":62145,\"start\":62129},{\"end\":62161,\"start\":62145},{\"end\":62306,\"start\":62292},{\"end\":62322,\"start\":62306},{\"end\":62339,\"start\":62322},{\"end\":62357,\"start\":62339},{\"end\":62377,\"start\":62357},{\"end\":62488,\"start\":62473},{\"end\":62507,\"start\":62488},{\"end\":62527,\"start\":62507},{\"end\":62538,\"start\":62527},{\"end\":62554,\"start\":62538},{\"end\":62571,\"start\":62554},{\"end\":62587,\"start\":62571},{\"end\":62601,\"start\":62587},{\"end\":62616,\"start\":62601},{\"end\":62627,\"start\":62616},{\"end\":62755,\"start\":62737},{\"end\":62773,\"start\":62755},{\"end\":62789,\"start\":62773},{\"end\":62808,\"start\":62789},{\"end\":62829,\"start\":62808},{\"end\":62846,\"start\":62829},{\"end\":62864,\"start\":62846},{\"end\":62884,\"start\":62864},{\"end\":62899,\"start\":62884},{\"end\":63105,\"start\":63091},{\"end\":63117,\"start\":63105},{\"end\":63130,\"start\":63117},{\"end\":63142,\"start\":63130},{\"end\":63156,\"start\":63142},{\"end\":63172,\"start\":63156},{\"end\":63291,\"start\":63277},{\"end\":63305,\"start\":63291},{\"end\":63319,\"start\":63305},{\"end\":63334,\"start\":63319},{\"end\":63349,\"start\":63334},{\"end\":63365,\"start\":63349},{\"end\":63377,\"start\":63365},{\"end\":63385,\"start\":63377},{\"end\":63398,\"start\":63385},{\"end\":63540,\"start\":63527},{\"end\":63551,\"start\":63540},{\"end\":63561,\"start\":63551},{\"end\":63572,\"start\":63561},{\"end\":63682,\"start\":63664},{\"end\":63693,\"start\":63682},{\"end\":63710,\"start\":63693},{\"end\":63758,\"start\":63736},{\"end\":63774,\"start\":63758},{\"end\":63790,\"start\":63774},{\"end\":63811,\"start\":63790},{\"end\":63829,\"start\":63811},{\"end\":63847,\"start\":63829},{\"end\":63862,\"start\":63847},{\"end\":63874,\"start\":63862},{\"end\":63882,\"start\":63874},{\"end\":64061,\"start\":64047},{\"end\":64075,\"start\":64061},{\"end\":64088,\"start\":64075},{\"end\":64102,\"start\":64088},{\"end\":64119,\"start\":64102},{\"end\":64135,\"start\":64119},{\"end\":64154,\"start\":64135},{\"end\":64168,\"start\":64154},{\"end\":64187,\"start\":64168},{\"end\":64203,\"start\":64187},{\"end\":64337,\"start\":64325},{\"end\":64491,\"start\":64477},{\"end\":64503,\"start\":64491},{\"end\":64515,\"start\":64503},{\"end\":64527,\"start\":64515},{\"end\":64644,\"start\":64633},{\"end\":64659,\"start\":64644},{\"end\":64670,\"start\":64659},{\"end\":64683,\"start\":64670},{\"end\":64698,\"start\":64683},{\"end\":64708,\"start\":64698},{\"end\":64720,\"start\":64708},{\"end\":64733,\"start\":64720},{\"end\":64738,\"start\":64733},{\"end\":64746,\"start\":64738},{\"end\":64750,\"start\":64746},{\"end\":64849,\"start\":64836},{\"end\":64864,\"start\":64849},{\"end\":64880,\"start\":64864},{\"end\":64893,\"start\":64880},{\"end\":64910,\"start\":64893},{\"end\":64920,\"start\":64910},{\"end\":64932,\"start\":64920},{\"end\":64947,\"start\":64932},{\"end\":65113,\"start\":65097},{\"end\":65121,\"start\":65113},{\"end\":65137,\"start\":65121},{\"end\":65145,\"start\":65137},{\"end\":65161,\"start\":65145},{\"end\":65171,\"start\":65161},{\"end\":65332,\"start\":65320},{\"end\":65356,\"start\":65332},{\"end\":65370,\"start\":65356},{\"end\":65382,\"start\":65370},{\"end\":65395,\"start\":65382},{\"end\":65407,\"start\":65395},{\"end\":65588,\"start\":65576},{\"end\":65600,\"start\":65588},{\"end\":65610,\"start\":65600},{\"end\":65624,\"start\":65610},{\"end\":65636,\"start\":65624},{\"end\":65706,\"start\":65693},{\"end\":65722,\"start\":65706},{\"end\":65735,\"start\":65722},{\"end\":65750,\"start\":65735},{\"end\":65761,\"start\":65750},{\"end\":65775,\"start\":65761},{\"end\":65794,\"start\":65775},{\"end\":65805,\"start\":65794},{\"end\":65814,\"start\":65805},{\"end\":65831,\"start\":65814}]", "bib_venue": "[{\"end\":56742,\"start\":56726},{\"end\":56898,\"start\":56870},{\"end\":57046,\"start\":57014},{\"end\":57159,\"start\":57056},{\"end\":57369,\"start\":57326},{\"end\":57461,\"start\":57380},{\"end\":57795,\"start\":57746},{\"end\":57848,\"start\":57805},{\"end\":58022,\"start\":57961},{\"end\":58159,\"start\":58107},{\"end\":58268,\"start\":58188},{\"end\":58538,\"start\":58458},{\"end\":58713,\"start\":58638},{\"end\":58977,\"start\":58928},{\"end\":59183,\"start\":59118},{\"end\":59345,\"start\":59314},{\"end\":59537,\"start\":59512},{\"end\":59711,\"start\":59662},{\"end\":59868,\"start\":59849},{\"end\":59980,\"start\":59879},{\"end\":60110,\"start\":60064},{\"end\":60496,\"start\":60452},{\"end\":60685,\"start\":60641},{\"end\":60737,\"start\":60698},{\"end\":61048,\"start\":61007},{\"end\":61345,\"start\":61296},{\"end\":61489,\"start\":61440},{\"end\":61575,\"start\":61497},{\"end\":61922,\"start\":61870},{\"end\":62085,\"start\":62053},{\"end\":62208,\"start\":62177},{\"end\":62421,\"start\":62377},{\"end\":62465,\"start\":62435},{\"end\":62715,\"start\":62643},{\"end\":63016,\"start\":62915},{\"end\":63183,\"start\":63172},{\"end\":63438,\"start\":63398},{\"end\":63525,\"start\":63449},{\"end\":63724,\"start\":63710},{\"end\":63971,\"start\":63898},{\"end\":64045,\"start\":63994},{\"end\":64384,\"start\":64337},{\"end\":64571,\"start\":64527},{\"end\":64631,\"start\":64583},{\"end\":64996,\"start\":64947},{\"end\":65215,\"start\":65171},{\"end\":65456,\"start\":65407},{\"end\":65574,\"start\":65466},{\"end\":65691,\"start\":65642},{\"end\":56754,\"start\":56744},{\"end\":61946,\"start\":61924}]"}}}, "year": 2023, "month": 12, "day": 17}