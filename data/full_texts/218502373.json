{"id": 218502373, "updated": "2023-10-06 16:29:24.315", "metadata": {"title": "Unsupervised Instance Segmentation in Microscopy Images via Panoptic Domain Adaptation and Task Re-weighting", "authors": "[{\"first\":\"Dongnan\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Donghao\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Yang\",\"last\":\"Song\",\"middle\":[]},{\"first\":\"Fan\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Lauren\",\"last\":\"O'Donnell\",\"middle\":[]},{\"first\":\"Heng\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Mei\",\"last\":\"Chen\",\"middle\":[]},{\"first\":\"Weidong\",\"last\":\"Cai\",\"middle\":[]}]", "venue": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2020, "month": 5, "day": 5}, "abstract": "Unsupervised domain adaptation (UDA) for nuclei instance segmentation is important for digital pathology, as it alleviates the burden of labor-intensive annotation and domain shift across datasets. In this work, we propose a Cycle Consistency Panoptic Domain Adaptive Mask R-CNN (CyC-PDAM) architecture for unsupervised nuclei segmentation in histopathology images, by learning from fluorescence microscopy images. More specifically, we first propose a nuclei inpainting mechanism to remove the auxiliary generated objects in the synthesized images. Secondly, a semantic branch with a domain discriminator is designed to achieve panoptic-level domain adaptation. Thirdly, in order to avoid the influence of the source-biased features, we propose a task re-weighting mechanism to dynamically add trade-off weights for the task-specific loss functions. Experimental results on three datasets indicate that our proposed method outperforms state-of-the-art UDA methods significantly, and demonstrates a similar performance as fully supervised methods.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2005.02066", "mag": "3034457289", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/LiuZS0OHC020", "doi": "10.1109/cvpr42600.2020.00430"}}, "content": {"source": {"pdf_hash": "974ffaf2775a8c32e405c551436a111714a5dc3c", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2005.02066v1.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2005.02066", "status": "GREEN"}}, "grobid": {"id": "64ee29c9eb9395d0ac96755ae550363c721fb5d1", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/974ffaf2775a8c32e405c551436a111714a5dc3c.txt", "contents": "\nUnsupervised Instance Segmentation in Microscopy Images via Panoptic Domain Adaptation and Task Re-weighting\n\n\nDongnan Liu \nSchool of Computer Science\nUniversity of Sydney\nAustralia\n\nDonghao Zhang fzhang@bwh.harvard.edu \nSchool of Computer Science\nUniversity of Sydney\nAustralia\n\nYang Song yang.song1@unsw.edu.au \nSchool of Computer Science and Engineering\nUniversity of New South Wales\nAustralia\n\nFan Zhang \nBrigham and Women's Hospital\nHarvard Medical School\nUSA\n\nLauren O&apos;donnell odonnell@bwh.harvard.edu \nBrigham and Women's Hospital\nHarvard Medical School\nUSA\n\nHeng Huang henghuanghh@gmail.com \nDepartment of Electrical and Computer Engineering\nUniversity of Pittsburgh\nUSA\n\nMei Chen \nMicrosoft Corporation\nUSA\n\nWeidong Cai \nSchool of Computer Science\nUniversity of Sydney\nAustralia\n\nUnsupervised Instance Segmentation in Microscopy Images via Panoptic Domain Adaptation and Task Re-weighting\n\nUnsupervised domain adaptation (UDA) for nuclei instance segmentation is important for digital pathology, as it alleviates the burden of labor-intensive annotation and domain shift across datasets. In this work, we propose a Cycle Consistency Panoptic Domain Adaptive Mask R-CNN (CyC-PDAM) architecture for unsupervised nuclei segmentation in histopathology images, by learning from fluorescence microscopy images. More specifically, we first propose a nuclei inpainting mechanism to remove the auxiliary generated objects in the synthesized images. Secondly, a semantic branch with a domain discriminator is designed to achieve panoptic-level domain adaptation. Thirdly, in order to avoid the influence of the source-biased features, we propose a task re-weighting mechanism to dynamically add trade-off weights for the task-specific loss functions. Experimental results on three datasets indicate that our proposed method outperforms state-of-the-art UDA methods significantly, and demonstrates a similar performance as fully supervised methods.\n\nIntroduction\n\nNuclei instance segmentation in histopathology images is an important step in the digital pathology workflow.\n\nPathologists are able to diagnose and prognose cancers according to mitosis counts, the morphological structure of each nucleus, and spatial distribution of a group of nuclei [7,25,5,1,34]. Currently, supervised learning- based methods for nuclei instance segmentation are prevalent as they are efficient while preserving high accuracy [24,35,3,9,33,50,29,28]. However, their performance heavily relies on large-scale training data, which requires expertise for annotation. This process is time-consuming and labor-intensive due to the complicated cellular structures, as shown in Fig. 1(b), and large image sizes. For example, annotating a histopathology dataset with 50 images and 12M pixels costs a pathologist 120 to 230 hours [16]. Moreover, in real clinical studies, even one whole slide image in 40\u00d7 objective magnification contains 1B pixels [10]. Therefore, investigating methods without depending on histopathology annotations is necessary. It can help pathologists to reduce the workload, and tackle the issue of lacking histopathology annotations.\n\nThe recently proposed unsupervised domain adaptation (UDA) methods tackle this issue by conducting supervised learning on the source domain and obtain a good performance model for the target domain without annotations [36,8,45]. Currently, UDA reduces distances between the distribution of feature maps of the source and target domains. In addition, some other methods focus on the pixelto-pixel translation from the source domain images to the target ones, for aligning cross-domain image appearances [20,52]. For these methods, there still remain some differences in the distributions between the synthesized and real images, due to the imperfect translations [15,2,21].\n\nTo incorporate the benefits of the image translation and the UDA methods, several works have been proposed to learn the domain-invariant features between the target and the synthesized target-like images [15,21,2]. Such methods achieve state-of-the-art performance on UDA classification, object detection, and semantic segmentation tasks. However, currently there is a lack of UDA methods specifically designed for instance segmentation, and directly extending the existing UDA methods on object detection [4,21,14] to the UDA nuclei instance segmentation task still suffers from challenges. First, existing UDA object detection methods focus on alleviating the domain bias at the image level (image contrast, brightness, etc.) and the instance level (object scale, style, etc.) [21,4,14]. They ignore the domain shift at the semantic level, such as the relationship between the foreground and background, and the spatial distribution of the objects. Second, these UDA object detection methods are multi-task learning paradigms, which optimize different loss functions simultaneously. If the feature extractors fail to generate domain-invariant features in some training iterations, then back-propagating the weights according to the task loss functions in these iterations causes the model bias towards the source domain.\n\nTo solve the aforementioned problems in UDA nuclei instance segmentation tasks in histopathology images, we propose a Cycle-Consistent Panoptic Domain Adaptive Mask R-CNN (CyC-PDAM) model. As none of the previous UDA methods are specially designed for instance segmentation, we extend the CyCADA [15] to an instance segmentation version based on Mask R-CNN [11], as our baseline. In our CyC-PDAM, we firstly propose a simple nuclei inpainting mechanism to remove the auxiliary nuclei in the synthesized histopathology images. Second, inspired by the panoptic segmentation architectures [23,22], we propose a semantic-level adaptation module for domain-invariant features based on the relationship between the foreground and the background. By reconciling the domain-invariant features at the semantic and instance levels, our proposed CyC-PDAM achieves panoptic-level domain adaptation. Furthermore, a task re-weighting mechanism is proposed to reset the importance for each task loss. During training, the specific task losses are down-weighted if the features for task predictions are not domain-invariant and source-biased, and up-weighted if the features are hard to differentiate.\n\nTo prove the effectiveness of our proposed CyC-PDAM architecture, experiments have been conducted on three public datasets for unsupervised nuclei instance segmentation of histopathology images on two different datasets by unsupervised domain adaptation from a fluorescence microscopy image dataset. Unlike histopathology images, no structures are similar to the nuclei in the background of fluorescence microscopy images, due to the differences between image acquisition techniques, as shown in Fig. 1(a). It is much easier to obtain manual annotation for the fluorescence microscopy images compared with histopathology images, therefore it is chosen as our source domain.\n\nOur contribution is summarized as follows: (1) We propose a CyC-PDAM model for UDA nuclei instance segmentation in histopathology images. To our best knowledge, this is the first UDA instance segmentation method.\n\n(2) A simple nuclei inpainting mechanism is proposed to remove false-positive objects in the synthesized images. (3) Our CyC-PDAM produces domain-invariant features at the panoptic level, by integrating the instance-level adaptation with a newly proposed semantic-level adaptation module. (4) A task re-weighting mechanism is proposed to alleviate the domain bias towards the source domain. (5) Compared with state-of-the-art UDA methods, our proposed CyC-PDAM paradigm outperforms them by a large margin. Moreover, it achieves competitive performance compared with state-of-the-art fully supervised methods for nuclei segmentation.\n\n\nRelated Work\n\n\nDomain Adaptation for Natural Images\n\nDomain adaptation aims at transferring the knowledge learned from one labeled domain to another without annotation [36]. Recently, UDA methods have reduced the crossdomain discrepancies based on the content in the feature level and the appearance in the pixel level. For the featurelevel adaptation, adversarial learning for domain-invariant features [8,45], Maximum Mean Discrepancy minimization (MMD) [32], local pattern alignment [48], and crossdomain covariance alignment [42] are widely employed for classification tasks. In addition, domain adaptation is further employed for other tasks such as semantic segmentation [46,26] and object detection [4,21,19,47]. In semantic segmentation tasks, the segmentation results are forced to be domain-invariant, together with intermediate feature maps [26,46,44]. Additionally, ADVENT [46] further minimized the Shannon entropy for the semantic segmentation predictions in source and target domains to alleviating the cross-domain discrepancy. For object detection, a domain adaptive Faster R-CNN [40], consisting of the image-and instance-level adaptions, was usually proposed for domain-invariant features of the whole image and each object [4,21,14]. On the other hand, imageto-image translation addresses the domain adaptation problems in the pixel level by generating target-like images and training task-specific fully supervised models on them [30,17,20,52,33,37]. However, domain bias still exists because of imperfect translation. Moreover, several methods have been proposed to align the feature-level adaptation with the pixel-level one, by learning domain-invariant features between the target images and the synthesized images [15,21,2].\n\n\nDomain Adaptation for Medical Images\n\nUnsupervised domain adaptation for medical image analysis has rarely been explored [39,51,2,18,16]. [39] and [18] solve the UDA histopathology images classification problems with GAN based architectures. In addition, DAM [6] is proposed to generate domain-invariant intermediate features and model predictions, for UDA semantic segmentation in CT images. With the help of cycleconsistency reconstruction, TD-GAN [51] and SIFA [2] are proposed for semantic segmentation on different medical images, with both pixel-and feature-level adaptations. However, none of them is designed for UDA nuclei instance segmentation. Even though Hou et al. [16] proposed to train a GAN based refiner and a nuclei segmentation model with the synthesized histopathology images for unsupervised nuclei instance segmentation, their paradigm only contains pixel-level adaptation and is still not capable for minimizing the domain gap in the feature level. In this work, we therefore propose a CyC-PDAM paradigm for UDA nuclei instance segmentation, which alleviates the domain bias issue in the pixel and feature levels.\n\n\nMethods\n\nOur proposed architecture is based on CyCADA and we fuse CyCADA with the instance segmentation framework Mask R-CNN. Furthermore, we improve it with nuclei inpainting mechanism, panoptic-level domain adaptation, and task re-weighting mechanism. Fig. 2 illustrates the overall architecture of our approach.\n\n\nCyCADA with Mask R-CNN\n\n\nName Hyperparamaters\n\nOutput size Input Table 1. The parameters for each block in the image-level discriminator for PDAM. k, s, and p denote the kernel size, stride, and padding of the convolution operation, respectively.\n256 \u00d7 8 \u00d7 8 Conv1 k = (3, 3), s = 1, p = 1 256 \u00d7 8 \u00d7 8 Conv2 k = (3, 3), s = 1, p = 1 512 \u00d7 8 \u00d7 8 Conv3 k = (3, 3), s = 1, p = 1 512 \u00d7 8 \u00d7 8 Conv4 k = (1, 1), s = 1, p = 0 2 \u00d7 8 \u00d7 8\nAs there is no UDA architectures targeting instance-level segmentation, we firstly design a domain adaptive Mask R-CNN. The backbone of the Mask R-CNN in this work is constructed with ResNet101 [12] and Feature Pyramid Network (FPN) [27]. Inspired by the previous UDA methods for object detection [4,21], we add one discriminator after FPN for the image-level adaptation, and the other after the instance branch for instance-level adaptation, as shown in Fig. 3. For the image-level adaptation, the multiresolution feature maps of the FPN output are firstly downsampled to the size 8 \u00d7 8 with average pooling, and then summed together for the image-level discriminator. The image-level discriminator consists of 4 convolutional layers (details in Table 1) and a gradient reversal layer (GRL) for adversarial learning. In the instance-level adaptation, the 14 \u00d7 14 \u00d7 256 feature map in the mask branch is downscaled to the size 2 \u00d7 2 \u00d7 256 with average pooling and then resized to 1024 \u00d7 1, to sum with the 1024 \u00d7 1 feature from the bounding box branch. The instance-level discriminator consists of 3 fully connected layers and a GRL, whose input is the summation of features mentioned above.\n\n\nNuclei Inpainting Mechanism\n\nEven though CycleGAN is effective for synthesizing histopathology-like images, due to the large domain gap and nuclei number incompatibility between the source and target domains, the label space for the generated images sometimes changes after transferring from the source domain. For example, there are redundant and undesired nuclei in the synthesized images shown in Fig. 4. If these images are directly used to train the task-specific CNN with the origi-  nal labels, the model is forced to regard redundant nuclei as background, even though they appear as real nuclei.\n\nTherefore, we propose an auxiliary nuclei inpainting mechanism to remove the nuclei which only appear in the synthesized images without corresponding annotations. Denoting a raw synthesized histopathology image by Cycle-GAN as S raw and its corresponding mask as M , we first obtain the mask predictions M aux of all the auxiliary generated nuclei, formulated as:\nM aux = (otsu(S raw ) \u222a M ) \u2212 M(1)\nwhere ostu(S raw ) represents a binary segmentation method for S raw based on Otsu threshold. In M aux , only auxiliary nuclei without annotation is labeled. Then, we get the newly synthesized image S inp after removing these nuclei, which can be represented as:\nS inp = inp(S raw , M aux )(2)\nwhere inp is a fast marching based method for inpainting objects [43], by replacing the pixel values for the auxiliary nuclei labeled in M aux with them for the unlabeled background. Fig. 4 illustrates the visual effectiveness of our proposed nuclei inpainting mechanism. However, some background materials are labeled as false positive predictions in M aux . Directly inpainting them makes the texture and appearance of synthesized images unrealistic, and enlarges the domain gap between the synthesized and real images. However, the image-level adaptation is able to address this issue by alleviating the domain bias on global visual information, such as curve, texture, and illumination. Our nuclei inpainting mechanism is time-efficient, which takes 0.09 second to process one single 256 \u00d7 256 synthesized histopathology patch, on average.\n\n\nPanoptic Level Domain Adaptation\n\nWe define the semantic-level features of an image as the relationship between its foreground and background. In addition to the image-and feature-level domain bias, the domain shift at the semantic level also exists. Due to the differences in the nuclei objects and background between the synthesized and real histopathology images, domain adaptive Mask R-CNN mentioned in Sec. 3.1 suffers from domain bias in the semantic-level features, as the Mask R-CNN only focuses on the local features for each object and lacks a semantic view of the whole image. Inspired by the previous panoptic segmentation architecture, which unified the semantic and instance segmentation to process the global and local features of the images, we propose a semantic-level adaptation to induce the model to learn domain-invariant features based on the relationship between the foreground and background. By incorporating the semantic-and instance-level adaptation, our panoptic domain adaptive method reduces the cross-domain discrepancies in a global and local view.\n\nAs shown in Fig. 3, a semantic branch for semantic segmentation prediction is added to the output of the FPN. Our semantic branch has the same implementation as [22]. As the fluorescence microscopy images and histopathology images can both be acquired from tissue samples and they can show complementary and correlated information, the semantic segmentation label spaces of the synthesized and real histopathology images have a strong similarity. In addition, aligning the cross-domain entropy distributions helps to minimize the entropy prediction in the target domain, which makes the model suitable for the target images [46]. Therefore, we use the Shannon entropy [41] of the softmax semantic predictions to induce the domain-invariant features to learn at the semantic level. Denoting the softmax semantic prediction as P and P \u2208 (0, 1), its Shannon entropy is defined as: \u2212plog(p). Fig. 3 and Table 2 indicate the detailed structure of the discriminator for semantic level adaptation. We employ residual connected CNN blocks to avoid gradient vanishing [12,13]. To make the adversarial learning more stable, instead of bilinear interpolation, we use stride convolutional layers for upsampling. Finally, the domain label is predicted as a 16 \u00d7 16 patch. Due to the small mini-batch size, the patch-based domain label prediction increases the number of training samples, to avoid overfitting.  \n\n\nTask Re-weighting Mechanism\n\nIn the previous UDA methods, the task-specific loss functions (segmentation, classification, and detection) are based on the source domain predictions. Even though several adversarial domain discriminators are employed to ensure the predicted feature maps are domain-invariant, the cross-domain discrepancies of these feature maps are still large in some training iterations, where the features are far from the decision boundaries of the domain discriminators. If the task-specific losses are updated to optimize the models with these easily-distinguished features, the models will bias towards the source images when testing it with the target data. To this end, we propose a task re-weighting mechanism to add a trade-off weight for each task-specific loss function according to the prediction of the domain discriminator. Denote the probability of the feature map before the final task prediction belonging to the source and target domains as p s and p t , respectively, and the task-specific loss function as L, then the re-weighted task-specific loss L rw is:\nL rw = min( p t p s , \u03b2)L = min( 1 \u2212 p s p s , \u03b2)L(3)\nwhere \u03b2 is a threshold value to avoid the 1\u2212ps ps becoming large and making the model collapse, when p s \u2192 0. According to Eq. 3, if the feature map deciding the task prediction belongs to the source domain (p s \u2192 1), the loss function is then down-weighted, to alleviate the source-bias feature learning of the model. As illustrated in Fig. 3, the loss function for the region proposal network (RPN), semantic branch, and the instance branch are re-weighted by the prediction at the image-, semantic-, and instance-level domain discriminators, respectively.\n\n\nNetwork Overview and Training Details\n\nIn our proposed CyC-PDAM, the CycleGAN has the same implementation as its original work [52]. When training the CycleGAN, the initial learning rate was set to 0.0001 for the first 1/2 of the total training iterations, and linearly decayed to 0 for the other 1/2.\n\nThe PDAM is trained with a batch size of 1 and each batch contains 2 images, one from the source and the other from the target domain. Due to the small batch size, we replace traditional batch normalization layers with group normalization [49] layers, with the default group number 32 as in [49].\n\nThe overall loss function of PDAM is defined as:\nL pdam = \u03b1 img L rpn + \u03b1 ins L det + \u03b1 sem L (sem\u2212seg) + \u03b1 da (L (img\u2212da) + L (sem\u2212da) + L (ins\u2212da) )(4)\nwhere L rpn is the loss function for the RPN, L det is the loss of class, bounding box, and instance mask prediction of Mask R-CNN, L (sem\u2212seg) is the cross entropy loss for semantic segmentation, L (img\u2212da) , L (sem\u2212da) and L (ins\u2212da) are cross entropy losses for domain classification at image, semantic and instance levels. \u03b1 img , \u03b1 ins , and \u03b1 isem are calculated according to Eq. 3 for task re-weighting. In our experiment, we set \u03b2 as 2. \u03b1 da is updated as:\n\u03b1 2 = 2 1 + exp(\u221210t) \u2212 1 (5)\nwhere t is the training progress and t \u2208 [0, 1]. Thus \u03b1 da is gradually changed from 0 to 1, to avoid the noise from the unstable domain discriminators in the early training stage. During training, the PDAM is optimized by SGD, with a weight decay of 0.001 and a momentum of 0.9. The initial learning rate is 0.002, with linear warming up in the first 500 iterations. The learning rate is then decreased to 0.0002 when it reaches 3/4 of the total training iteration. During inference, only the original Mask R-CNN architecture is used with the adapted weight and all of the hyperparameters for testing are fine-tuned on the validation set. All of our experiments were implemented with Pytorch [38], on two NVIDIA GeForce 1080Ti GPUs.\n\n\nExperiments\n\n\nDatasets Description and Evaluation Metrics\n\nOur proposed architecture was validated on three public datasets, referred to as Kumar [24], TNBC [35], and BBBC039V1 [31], respectively. Among them, Kumar and TNBC are histopathology datasets, while BBBC039V1 is a fluorescence microscopy dataset. Kumar was acquired from The Cancer Genome Atlas (TCGA) at 40\u00d7 magnification, containing 30 annotated 1000\u00d71000 patches from 30 whole slide images of different patients. All these images are from 18 different hospitals and 7 different organs (breast, liver, kidney, prostate, bladder, colon, and stomach). In contrast to the disease variability in Kumar, the TNBC dataset especially focuses on Triple-Negative Breast Cancer (TNBC) [35]. In TNBC, there are 50 annotated 512 \u00d7 512 patches from 11 different patients from the Curie Institute at 40\u00d7 magnification. BBBC039V1 is about U2OS cells under a high-throughput chemical screen [31]. It contains 200 520 \u00d7 696 images about bioactive compounds, with the DNA channel staining of a single field of view.\n\nFor evaluation, we employ three commonly used pixeland object-level metrics. Aggregated Jaccard Index (AJI) is an extended Jaccard Index for object-level evaluation [24], and object-level F1 score is the average harmonic mean between the precision and recall for each object. For pixellevel evaluation, we employ pixel-level F1 score for binarization predictions.\n\n\nExperiment Setting\n\nWe conducted our experiments on two nuclei segmentation tasks: adapting from BBBC039V1 to Kumar, and from BBBC039V1 to TNBC. As the source domain in two experiments, 100 training images and 50 validation images from BBBC039V1 are used, following the official data split 1 .\n\n1 https://data.broadinstitute.org/bbbc/BBBC039/ The annotations for Kumar and TNBC are not used during training the UDA architecture, only for evaluation.\n\nThe preprocessing for source fluorescence microscopy images has 3 steps. First, all images are normalized into range [0, 255]. Second, 10K patches in size 256 \u00d7 256 are randomly cropped from the 100 training images, with data augmentation including rotation, scaling, and flipping to avoid overfitting. Third, the patches with fewer than 3 objects are removed. For better synthesizing target-like histopathology images, we finally inverse the pixel value of foreground nuclei and background for all source fluorescence microscopy patches. For validation, 50 images in the BBBC039V1 validation set are transferred to synthesized histopathology images by CycleGAN and nuclei inpainting mechanism.\n\nFor the Kumar dataset as the target domain, we have the same data split as previous work in [24,35], with 16 images for training, and 14 for testing. When training the model, totally 10K patches in size 256 \u00d7 256 are randomly cropped from the 16 training histopathology images, with basic data augmentation including flipping and rotation, to avoid overfitting. As for TNBC, we use 8 cases with 40 images for training, and the remaining 3 cases with 10 images for testing. To train the model with TNBC, 10K 256\u00d7256 patches are randomly extracted from the training images with basic data augmentation including flipping and rotation.\n\n\nComparison Experiments\n\n\nComparison with Unsupervised Methods\n\nIn this section, our proposed CyC-PDAM is compared with several state-of-the-art UDA methods, including CyCADA [15], Chen et al. [4], SIFA [2], and DDMRL [21]. As the original CyCADA focuses on classification and semantic segmentation, we extend it with Mask R-CNN for UDA instance segmentation, as described in Sec. 3.1. Chen et al. [4] are originally for UDA object detection based on Faster R-CNN, by adapting the features at the image and instance levels. For UDA instance segmentation, we replace the original VGG16 based Faster R-CNN with the same Mask R-CNN in our architecture, and the original image-and instance-level adaptation in [4] with ours in Sec. 3.1. SIFA [2] is a UDA semantic segmentation architecture for CT and MR images, with a pixel-and feature-level adaptation. In our experiment, we add the watershed algorithm to separate the touching objects in the semantic segmentation prediction of SIFA, for a fair comparison. DDMRL [21] learns multi-domain-invariant features from various generated domains for UDA object detection and it is extended for instance segmentation, in a similar way as CyCADA [15] and Chen et al. [4]. In addition, we also compared with Hou et al. [16], which is particularly designed for unsupervised nuclei segmentation in histopathology images. They trained a multi-task (segmentation, detection, and refinement) CNN\nBBBC039 \u2192 Kumar BBBC039 \u2192 T N BC Methods AJI Pixel-F1 Object-F1 AJI Pixel-F1\nObject-F1 CyCADA [15] 0.4447 \u00b1 0.1069 0.7220 \u00b1 0.0802 0.6567 \u00b1 0.0837 0.4721 \u00b1 0.0906 0.7048 \u00b1 0.0946 0.6866 \u00b1 0.0637 Chen et al. [4] 0.3756 \u00b1 0.0977 0.6337 \u00b1 0.0897 0.5737 \u00b1 0.0983 0.4407 \u00b1 0.0623 0.6405 \u00b1 0.0660 0.6289 \u00b1 0.0609 SIFA [2] 0.3924 \u00b1 0.1062 0.6880 \u00b1 0.0882 0.6008 \u00b1 0.1006 0.4662 \u00b1 0.0902 0.6994 \u00b1 0.0942 0.6698 \u00b1 0.0771 DDMRL [21] 0.4860 \u00b1 0.0846 0.7109 \u00b1 0.0744 0.6833 \u00b1 0.0724 0.4642 \u00b1 0.0503 0.7000 \u00b1 0.0431 0.6872 \u00b1 0.0347 Hou et al. [16] 0.4980 \u00b1 0.1236 0.7500 \u00b1 0.0849 0.6890 \u00b1 0.0990 0.4775 \u00b1 0.1219 0.7029 \u00b1 0.1262 0.6779 \u00b1 0.0821 Proposed 0.5610 \u00b1 0.0718 0.7882 \u00b1 0.0533 0.7483 \u00b1 0.0525 0.5672 \u00b1 0.0646 0.7593 \u00b1 0.0566 0.7478 \u00b1 0.0417 Table 3. In comparison with other unsupervised methods on both two histopathology datasets. architecture with their synthesized histopathology images from randomly generated binary nuclei masks. Table 3 shows that our proposed method outperforms all the comparison methods by a large margin, on different histopathology datasets. In addition, the one-tailed paired t-test is employed to prove that all of our improvements are statistically significant, with all the p-values under 0.05. Chen et al. [4] learns the domain-invariant features at the image and instance levels. However, due to the large differences between the fluorescence microscopy and real histopathology images, feature-level adaptation only is not enough to reduce the domain gap. With pixel-level adaptation on appearance, all the other methods achieve better performance. Compared with the baseline method CyCADA [15], our CyC-PDAM has a large improvement of 6 \u2212 12%, due to the effectiveness of our proposed nuclei inpainting mechanism, panoptic-level adaptation, and task re-weighting mechanism. SIFA [2] focuses on domaininvariant features in the image and semantic levels, with a UDA semantic segmentation structure. As there exists a large number of nuclei objects in the histopathology im-  Table 4. Ablation study on BBBC039V1 to Kumar experiment. NI, TR, and SEM represent the nuclei inpainting mechanism, task re-weighting mechanism, and semantic branch, respectively. ages, the effectiveness of SIFA is still limited without any instance-level learning or adaptation. Although DDMRL [21] only adapts the features at the image level, its performance is still at the same level as CyCADA, by adapting knowledge across various domains. Among all the comparison methods, Hou et al. [16] achieves the second-best performance. Due to the effectiveness of panoptic-level feature adaptation and task re-weighting mechanism, our method still outperforms it under all three metrics, in both two experiments. Fig. 5 are visualization examples of all the comparison methods.\n\n\nAblation Study\n\nIn order to test the effectiveness of each component in our proposed CyC-PDAM, ablation experiments are conducted on the Kumar dataset. Based on our CyC-PDAM, we remove the nuclei inpainting mechanism, task re-weighting mechanism, and semantic branch for panoptic-level adaptation and train the ablated models with the same setting and dataset as Sec. 4.3.1. Table 4 and Fig. 6 show the detailed results of the ablation experiment. As shown in Fig. 6, the method without nuclei inpainting mechanism (w/o NI) tends to ignore some nuclei, which increases the false-negative predictions. Moreover, we notice that there are also false split and merged predictions for w/o NI model. It is because the increasing false negative predictions are harmful to the spatial distribution of all the objects, which further affects the effectiveness of the semantic-level adaptation. Among the predictions of the method without task re-weighting mechanism (w/o TR), there exist some objects with irregular sizes. The task re-weighting mechanism prevents the model from being influenced by the domainspecific features in the source domain, and removing it, therefore, incurs source-biased predictions. Compared with our method, the model without semantic-branch (w/o SEM) is not able to learn domain-invariant features at the semantic AJI Pixel-F1 Methods seen unseen all seen unseen all CNN3 [24] 0.5154 \u00b1 0.0835 0.4989 \u00b1 0.0806 0.5083 \u00b1 0.0695 0.7301 \u00b1 0.0590 0.8051 \u00b1 0.1006 0.7623 \u00b1 0.0946 DIST [35] 0.5594 \u00b1 0.0598 0.5604 \u00b1 0.0663 0.5598 \u00b1 0.0781 0.7756 \u00b1 0.0489 0.8005 \u00b1 0.0538 0.7863 \u00b1 0.0550 Proposed 0.5432 \u00b1 0.0477 0.5848 \u00b1 0.0951 0.5610 \u00b1 0.0982 0.7743 \u00b1 0.0358 0.8068 \u00b1 0.0698 0.7882 \u00b1 0.0533 Upper bound [22] 0.5703 \u00b1 0.0480 0.5778 \u00b1 0.0671 0.5735 \u00b1 0.0855 0.7796 \u00b1 0.0419 0.8007 \u00b1 0.0511 0.7886 \u00b1 0.0531 Table 5. Comparison experiments between our UDA method and fully supervised methods, for BBBC039V1 to Kumar experiment. For CNN3 and DIST, the results of object-level F1 are unknown. Figure 6. Visualization results for the ablation experiment. NI: nuclei inpainting mechanism; TR: task re-weighting mechanism; SEM: semantic branch. level, including the spatial distribution of the nuclei objects and the detailed information in the background. Therefore, there not only remain falsely split and merged predictions, but also false-positive and imperfect segmentation results. As shown in Table 4, the segmentation accuracy under three metrics decreases by 4 \u2212 6% after removing each module. In addition, the one-tailed paired t-test is employed to calculate the p-value between our proposed method and the other ablated methods. After adding each of the three modules, the improvements are statistically significant (P < 0.05), which further demonstrates the effectiveness of our proposed method.\n\n\nComparison with Fully Supervised Methods\n\nAs our data split in Kumar dataset is the same as several state-of-the-art methods for fully supervised nuclei segmentation, we compare their original reported results with ours. Table 5 illustrates the comparison results between our proposed UDA architecture and other fully supervised methods. CNN3 [24] is a contour-based nuclei segmentation architecture, which considers nuclei boundaries as the third class, in addition to the foreground and background classes. DIST [35] is a regression model based on the distance map. For Panoptic FPN [22], we directly train it using the same set of 16 real histopathology patches as CNN3 and DIST and it is employed as the upper bound of our unsupervised method. The testing images for Kumar are divided into two subsets: one contains 8 images from 4 organs known to training set, referred to as seen, and the other contains 6 images from 3 organs unknown to the training set, referred to as unseen.\n\nAs shown in Table 5, the performance of our proposed UDA architecture is superior to the fully supervised CNN3 and DIST. It is because our proposed method is able to process each ROI on the local level, while CNN3 and DIST only process the image at a global semantic level. By adapting the semantic-level features of the foreground and the background, the performance of our method is at the same level as the fully supervised Panoptic FPN for the pixellevel F1-score. Even though our AJI is slight lower than the fully supervised Panoptic FPN, we notice that our method works better when tested on the unseen testing set. This is because our proposed CyC-PDAM focuses on learning the domain-invariant features and avoids being influenced by the domain bias of testing images from unseen organs. These results show that, although there remains large differences between the fluorescence microscopy images and histopathology images, our proposed UDA architecture still successfully narrows the domain gap between them, and achieves even better performance compared with fully supervised methods requiring histopathology nuclei annotations.\n\n\nConclusion\n\nIn this work, we propose a CyC-PDAM architecture for UDA nuclei segmentation in histopathology images. We firstly design a baseline architecture for UDA instance segmentation, including appearance-, image-, and instancelevel adaptation. Next, a nuclei inpainting mechanism is designed to remove the auxiliary objects in the synthesized images, to further avoid false-negative predictions. In the feature-level adaptation, a semantic branch is proposed to adapt the features with respect to the foreground and background, and incorporating semantic-and instance-level adaptation enables the model to learn domain-invariant features at the panoptic level. In addition, a task re-weighting mechanism is proposed to reduce the bias. Extensive experiments on three public datasets indicate our proposed method outperforms the state-of-the-art UDA methods by a large margin and reaches the same level as the fully supervised methods. From a larger perspective, the UDA instance segmentation problems are not limited to histopathology image analysis. With the promising performance close to fully supervised methods in this work, we suggest that our proposed method can also contribute to other general image analysis applications.\n\nFigure 1 .\n1Example images of our proposed framework. (a) fluorescence microscopy images; (b) real histopathology images; (c) our synthesized histopathology images; (d) nuclei segmentation generated by our proposed UDA method; (e) ground truth.\n\nFigure 2 .\n2Overall architecture for our proposed CyC-PDAM architecture. The annotations of the real histopathology patches are not used during training.\n\nFigure 3 .\n3Detailed illustration of Panoptic Domain Adaptive Mask R-CNN (PDAM). Ci and F C represent a convolution layer, and a fully connected layer, respectively. Ri1 and Ri2 mean the first and second convolutional layers in the ith residual block, respectively. ReLU and normalization layers after each convolutional block are omitted for brevity.\n\nFigure 4 .\n4Visual results for the effectiveness of nuclei inpainting mechanism. (a) original fluorescence microscopy patches; (b) corresponding nuclei annotations; (c) initial synthesized images from CycleGAN; (d) final synthesized images after nuclei inpainting mechanism.\n\n\n2 \u00d7 256 \u00d7 256 C1 k = (7, 7), s = 2, p = 3 64 \u00d7 128 \u00d7 128 R11 and R12 k = (3, 3), s = 1, p = 1 64 \u00d7 128 \u00d7 128 C2 k = (5, 5), s = 2, p = 2 128 \u00d7 64 \u00d7 64 R21 and R22 k = (3, 3), s = 1, p = 1 128 \u00d7 64 \u00d7 64 C3 k = (5, 5), s = 2, p = 2 256 \u00d7 32 \u00d7 32 R31 and R32 k = (3, 3), s = 1, p = 1 256 \u00d7 32 \u00d7 32 C4 k = (5, 5), s = 2, p = 2 512 \u00d7 16 \u00d7 16 R41 and R42 k = (3, 3), s = 1, p = 1 512 \u00d7 16 \u00d7 16 C5 k = (1, 1), s = 1, p = 0 2 \u00d7 16 \u00d7 16 Output 2 \u00d7 16 \u00d7 16\n\nFigure 5 .\n5Visualization result for the comparison experiments experiment. The first 3 rows are from Kumar dataset, and the last 3 rows are from TNBC.\n\n\n5042 \u00b1 0.1034 0.7336 \u00b1 0.0839 0.6958 \u00b1 0.0832 w/o TR 0.4969 \u00b1 0.0972 0.7654 \u00b1 0.0678 0.6923 \u00b1 0.0778 w/o SEM 0.5046 \u00b1 0.1065 0.7470 \u00b1 0.0754 0.6965 \u00b1 0.0805 proposed 0.5610 \u00b1 0.0718 0.7882 \u00b1 0.0533 0.7483 \u00b1 0.0525\n\nTable 2 .\n2The parameters for each block in the semantic-level discriminator for PDAM. k, s, and p follow the same convention as inTable 1.\n\nMulti-field-of-view strategy for image-based outcome prediction of multi-parametric estrogen receptorpositive breast cancer histopathology: Comparison to oncotype dx. Ajay Basavanhally, Michael Feldman, Natalie Shih, Carolyn Mies, John Tomaszewski, Shridar Ganesan, Anant Madabhushi, Journal of pathology informatics. 2Ajay Basavanhally, Michael Feldman, Natalie Shih, Car- olyn Mies, John Tomaszewski, Shridar Ganesan, and Anant Madabhushi. Multi-field-of-view strategy for image-based outcome prediction of multi-parametric estrogen receptor- positive breast cancer histopathology: Comparison to onco- type dx. Journal of pathology informatics, 2, 2011.\n\nSynergistic image and feature adaptation: Towards cross-modality domain adaptation for medical image segmentation. Cheng Chen, Qi Dou, Hao Chen, Jing Qin, Pheng-Ann Heng, Association for the Advancement of Artificial Intelligence (AAAI). Cheng Chen, Qi Dou, Hao Chen, Jing Qin, and Pheng-Ann Heng. Synergistic image and feature adaptation: Towards cross-modality domain adaptation for medical image seg- mentation. In Association for the Advancement of Artificial Intelligence (AAAI), pages 865-872, 2019.\n\nDcan: Deep contour-aware networks for object instance segmentation from histology images. Xiaojuan Hao Chen, Lequan Qi, Qi Yu, Jing Dou, Pheng-Ann Qin, Heng, Medical Image Analysis. 36Hao Chen, Xiaojuan Qi, Lequan Yu, Qi Dou, Jing Qin, and Pheng-Ann Heng. Dcan: Deep contour-aware networks for object instance segmentation from histology images. Medi- cal Image Analysis, 36:135-146, 2017.\n\nDomain adaptive faster R-CNN for object detection in the wild. Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, Luc Van Gool, Computer Visionand Pattern Recognition (CVPR). Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Domain adaptive faster R-CNN for object detection in the wild. In Computer Visionand Pattern Recog- nition (CVPR), pages 3339-3348, 2018.\n\nPathologic correlates of survival in 378 lymph node-negative infiltrating ductal breast carcinomas. mitotic count is the best single predictor. Frederic Clayton, Cancer. 686Frederic Clayton. Pathologic correlates of survival in 378 lymph node-negative infiltrating ductal breast carcino- mas. mitotic count is the best single predictor. Cancer, 68(6):1309-1317, 1991.\n\nUnsupervised cross-modality domain adaptation of convnets for biomedical image segmentations with adversarial loss. Qi Dou, Cheng Ouyang, Cheng Chen, Hao Chen, Pheng-Ann Heng, International Joint Conferences on Artificial Intelligence (IJCAI). Qi Dou, Cheng Ouyang, Cheng Chen, Hao Chen, and Pheng- Ann Heng. Unsupervised cross-modality domain adaptation of convnets for biomedical image segmentations with adver- sarial loss. In International Joint Conferences on Artificial Intelligence (IJCAI), pages 691-697, 2018.\n\nPathological prognostic factors in breast cancer. I. the value of histological grade in breast cancer: experience from a large study with longterm follow-up. W Christopher, Ian O Elston, Ellis, Histopathology. 195Christopher W Elston and Ian O Ellis. Pathological prognos- tic factors in breast cancer. I. the value of histological grade in breast cancer: experience from a large study with long- term follow-up. Histopathology, 19(5):403-410, 1991.\n\nUnsupervised domain adaptation by backpropagation. Yaroslav Ganin, Victor Lempitsky, International Conference on Machine Learning (ICML). Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In International Conference on Machine Learning (ICML), 2015.\n\nHover-net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images. Simon Graham, Quoc Dang Vu, Shan E , Ahmed Raza, Ayesha Azam, Yee Wah Tsang, Jin Tae Kwak, Nasir Rajpoot, Medical Image Analysis. 58101563Simon Graham, Quoc Dang Vu, Shan E Ahmed Raza, Ayesha Azam, Yee Wah Tsang, Jin Tae Kwak, and Nasir Rajpoot. Hover-net: Simultaneous segmentation and classi- fication of nuclei in multi-tissue histology images. Medical Image Analysis, 58:101563, 2019.\n\nCancer digital slide archive: an informatics resource to support integrated in silico analysis of TCGA pathology data. Jake David A Gutman, Dhananjaya Cobb, Yuna Somanna, Fusheng Park, Tahsin Wang, Joel H Kurc, Saltz, J Daniel, Brat, A D Lee, Jun Cooper, Kong, Journal of the American Medical Informatics Association. 206David A Gutman, Jake Cobb, Dhananjaya Somanna, Yuna Park, Fusheng Wang, Tahsin Kurc, Joel H Saltz, Daniel J Brat, Lee AD Cooper, and Jun Kong. Cancer digital slide archive: an informatics resource to support integrated in silico analysis of TCGA pathology data. Journal of the American Medical Informatics Association, 20(6):1091- 1098, 2013.\n\nKaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, Ross Girshick Mask R-Cnn, International Conference on Computer Vision (ICCV). Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Gir- shick. Mask R-CNN. In International Conference on Com- puter Vision (ICCV), pages 2980-2988, 2017.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Computer Visionand Pattern Recognition (CVPR). Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Com- puter Visionand Pattern Recognition (CVPR), pages 770- 778, 2016.\n\nIdentity mappings in deep residual networks. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Eu-ropean Conference on Computer Vision (ECCV). SpringerKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In Eu-ropean Conference on Computer Vision (ECCV), pages 630-645. Springer, 2016.\n\nMulti-adversarial faster-rcnn for unrestricted object detection. Zhenwei He, Lei Zhang, International Conference on Computer Vision (ICCV). Zhenwei He and Lei Zhang. Multi-adversarial faster-rcnn for unrestricted object detection. In International Conference on Computer Vision (ICCV), pages 6668-6677, 2019.\n\nJudy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei A Efros, Trevor Darrell, Cycada: Cycle-consistent adversarial domain adaptation. International Conference on Machine Learning (ICML). Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei A Efros, and Trevor Dar- rell. Cycada: Cycle-consistent adversarial domain adap- tation. International Conference on Machine Learning (ICML), 2018.\n\nRobust histopathology image analysis: To label or to synthesize?. Le Hou, Ayush Agarwal, Dimitris Samaras, M Tahsin, Kurc, R Rajarsi, Joel H Gupta, Saltz, Computer Vision and Pattern Recognition (CVPR). Le Hou, Ayush Agarwal, Dimitris Samaras, Tahsin M Kurc, Rajarsi R Gupta, and Joel H Saltz. Robust histopathology image analysis: To label or to synthesize? In Computer Vision and Pattern Recognition (CVPR), pages 8533-8542, 2019.\n\nMultimodal unsupervised image-to-image translation. Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz, European Conference on Computer Vision (ECCV). Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-to-image translation. In Eu- ropean Conference on Computer Vision (ECCV), pages 172- 189, 2018.\n\nEpithelium-stroma classification via convolutional neural networks and unsupervised domain adaptation in histopathological images. Yue Huang, Han Zheng, Chi Liu, Xinghao Ding, Gustavo K Rohde, IEEE Journal of Biomedical and Health Informatics. 216Yue Huang, Han Zheng, Chi Liu, Xinghao Ding, and Gus- tavo K Rohde. Epithelium-stroma classification via convolu- tional neural networks and unsupervised domain adaptation in histopathological images. IEEE Journal of Biomedical and Health Informatics, 21(6):1625-1632, 2017.\n\nCross-domain weakly-supervised object detection through progressive domain adaptation. Naoto Inoue, Ryosuke Furuta, Toshihiko Yamasaki, Kiyoharu Aizawa, Computer Vision and Pattern Recognition (CVPR). Naoto Inoue, Ryosuke Furuta, Toshihiko Yamasaki, and Kiy- oharu Aizawa. Cross-domain weakly-supervised object de- tection through progressive domain adaptation. In Computer Vision and Pattern Recognition (CVPR), pages 5001-5009, 2018.\n\nImage-to-image translation with conditional adversarial networks. Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A Efros, Computer Vision and Pattern Recognition (CVPR). IEEEPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adver- sarial networks. In Computer Vision and Pattern Recognition (CVPR), pages 5967-5976. IEEE, 2017.\n\nDiversify and match: A domain adaptive representation learning paradigm for object detection. Taekyung Kim, Minki Jeong, Seunghyeon Kim, Seokeon Choi, Changick Kim, Computer Vision and Pattern Recognition (CVPR). Taekyung Kim, Minki Jeong, Seunghyeon Kim, Seokeon Choi, and Changick Kim. Diversify and match: A domain adaptive representation learning paradigm for object detec- tion. In Computer Vision and Pattern Recognition (CVPR), pages 12456-12465, 2019.\n\nPanoptic feature pyramid networks. Alexander Kirillov, Ross Girshick, Computer Vision and Pattern Recognition (CVPR). Kaiming He, and Piotr Doll\u00e1rAlexander Kirillov, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Panoptic feature pyramid networks. In Computer Vision and Pattern Recognition (CVPR), pages 6399-6408, 2019.\n\nPanoptic segmentation. Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, Piotr Doll\u00e1r, Computer Vision and Pattern Recognition (CVPR). Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Doll\u00e1r. Panoptic segmentation. In Com- puter Vision and Pattern Recognition (CVPR), pages 9404- 9413, 2019.\n\nA dataset and a technique for generalized nuclear segmentation for computational pathology. Neeraj Kumar, Ruchika Verma, Sanuj Sharma, Surabhi Bhargava, Abhishek Vahadane, Amit Sethi, IEEE Transactions on Medical Imaging. 367Neeraj Kumar, Ruchika Verma, Sanuj Sharma, Surabhi Bhargava, Abhishek Vahadane, and Amit Sethi. A dataset and a technique for generalized nuclear segmentation for computational pathology. IEEE Transactions on Medical Imaging, 36(7):1550-1560, 2017.\n\nPrognostic value of histologic grade nuclear components of Scarff-Bloom-Richardson (SBR). an improved score modification based on a multivariate analysis of 1262 invasive ductal breast carcinomas. V Le Doussal, S Tubiana-Hulin, K Friedman, Hacene, M Spyratos, Brunet, Cancer. 649V Le Doussal, M Tubiana-Hulin, S Friedman, K Hacene, F Spyratos, and M Brunet. Prognostic value of histologic grade nuclear components of Scarff-Bloom-Richardson (SBR). an improved score modification based on a multivariate anal- ysis of 1262 invasive ductal breast carcinomas. Cancer, 64(9):1914-1921, 1989.\n\nBidirectional learning for domain adaptation of semantic segmentation. Yunsheng Li, Lu Yuan, Nuno Vasconcelos, Computer Vision and Pattern Recognition (CVPR). Yunsheng Li, Lu Yuan, and Nuno Vasconcelos. Bidirec- tional learning for domain adaptation of semantic segmen- tation. In Computer Vision and Pattern Recognition (CVPR), pages 6936-6945, 2019.\n\nKaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Computer Vision and Pattern Recognition (CVPR). Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Computer Vision and Pat- tern Recognition (CVPR), pages 2117-2125, 2017.\n\nCell r-cnn v3: A novel panoptic paradigm for instance segmentation in biomedical images. Dongnan Liu, Donghao Zhang, Yang Song, Heng Huang, Weidong Cai, arXiv:2002.06345arXiv preprintDongnan Liu, Donghao Zhang, Yang Song, Heng Huang, and Weidong Cai. Cell r-cnn v3: A novel panoptic paradigm for instance segmentation in biomedical images. arXiv preprint arXiv:2002.06345, 2020.\n\nNuclei segmentation via a deep panoptic model with semantic feature fusion. Dongnan Liu, Donghao Zhang, Yang Song, Chaoyi Zhang, Fan Zhang, Lauren Odonnell, Weidong Cai, International Joint Conferences on Artificial Intelligence (IJCAI). AAAI PressDongnan Liu, Donghao Zhang, Yang Song, Chaoyi Zhang, Fan Zhang, Lauren ODonnell, and Weidong Cai. Nuclei seg- mentation via a deep panoptic model with semantic feature fusion. In International Joint Conferences on Artificial Intel- ligence (IJCAI), pages 861-868. AAAI Press, 2019.\n\nUnsupervised image-to-image translation networks. Ming-Yu Liu, Thomas Breuel, Jan Kautz, Advances in Neural Information Processing Systems (NeurIPS). Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks. In Advances in Neural Information Processing Systems (NeurIPS), pages 700-708, 2017.\n\nAnnotated high-throughput microscopy image sets for validation. Vebjorn Ljosa, Katherine L Sokolnicki, Anne E Carpenter, Nature Methods. 97Vebjorn Ljosa, Katherine L Sokolnicki, and Anne E Carpen- ter. Annotated high-throughput microscopy image sets for validation. Nature Methods, 9(7):637-637, 2012.\n\nMingsheng Long, Yue Cao, Jianmin Wang, Michael I Jordan , Learning transferable features with deep adaptation networks. International Conference on Machine Learning (ICML). Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I Jordan. Learning transferable features with deep adaptation networks. International Conference on Machine Learning (ICML), 2015.\n\nDeep adversarial training for multi-organ nuclei segmentation in histopathology images. Faisal Mahmood, Daniel Borders, Richard Chen, N Gregory, Kevan J Mckay, Alexander Salimian, Nicholas J Baras, Durr, IEEE Transactions on Medical Imaging. Faisal Mahmood, Daniel Borders, Richard Chen, Gregory N McKay, Kevan J Salimian, Alexander Baras, and Nicholas J Durr. Deep adversarial training for multi-organ nuclei seg- mentation in histopathology images. IEEE Transactions on Medical Imaging, 2018.\n\nComputational pathology: Exploring the spatial dimension of tumor ecology. Sidra Nawaz, Yinyin Yuan, Cancer letters. 3801Sidra Nawaz and Yinyin Yuan. Computational pathology: Exploring the spatial dimension of tumor ecology. Cancer letters, 380(1):296-303, 2016.\n\nSegmentation of nuclei in histopathology images by deep regression of the distance map. Peter Naylor, Marick La\u00e9, Fabien Reyal, Thomas Walter, IEEE Transactions on Medical Imaging. Peter Naylor, Marick La\u00e9, Fabien Reyal, and Thomas Walter. Segmentation of nuclei in histopathology images by deep re- gression of the distance map. IEEE Transactions on Medical Imaging, 2018.\n\nA survey on transfer learning. Qiang Sinno Jialin Pan, Yang, IEEE Transactions on Knowledge and Data Engineering. 2210Sinno Jialin Pan and Qiang Yang. A survey on transfer learn- ing. IEEE Transactions on Knowledge and Data Engineer- ing, 22(10):1345-1359, 2009.\n\nSemantic image synthesis with spatially-adaptive normalization. Taesung Park, Ming-Yu Liu, Ting-Chun Wang, Jun-Yan Zhu, Computer Vision and Pattern Recognition (CVPR). Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive nor- malization. In Computer Vision and Pattern Recognition (CVPR), pages 2337-2346, 2019.\n\nAutomatic differentiation in pytorch. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary Devito, Zeming Lin, Alban Desmaison, Luca Antiga, Adam Lerer, Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al- ban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in pytorch. NeurIPS 2017 Autodiff Workshop, 2017.\n\nAdversarial domain adaptation for classification of prostate histopathology whole-slide images. Jian Ren, Ilker Hacihaliloglu, Eric A Singer, J David, Xin Foran, Qi, International Conference On Medical Image Computing Computer Assisted Intervention (MICCAI). SpringerJian Ren, Ilker Hacihaliloglu, Eric A Singer, David J Foran, and Xin Qi. Adversarial domain adaptation for classification of prostate histopathology whole-slide images. In Interna- tional Conference On Medical Image Computing Computer Assisted Intervention (MICCAI), pages 201-209. Springer, 2018.\n\nFaster R-CNN: Towards real-time object detection with region proposal networks. Kaiming Shaoqing Ren, Ross He, Jian Girshick, Sun, Advances in Neural Information Processing Systems (NeurIPS). Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detection with re- gion proposal networks. In Advances in Neural Information Processing Systems (NeurIPS), pages 91-99, 2015.\n\nA mathematical theory of communication. Claude Elwood Shannon, Bell System Technical Journal. 273Claude Elwood Shannon. A mathematical theory of com- munication. Bell System Technical Journal, 27(3):379-423, 1948.\n\nReturn of frustratingly easy domain adaptation. Baochen Sun, Jiashi Feng, Kate Saenko, Association for the Advancement of Artificial Intelligence (AAAI). Baochen Sun, Jiashi Feng, and Kate Saenko. Return of frus- tratingly easy domain adaptation. In Association for the Ad- vancement of Artificial Intelligence (AAAI), 2016.\n\nAn image inpainting technique based on the fast marching method. Alexandru Telea, Journal of Graphics Tools. 91Alexandru Telea. An image inpainting technique based on the fast marching method. Journal of Graphics Tools, 9(1):23-34, 2004.\n\nLearning to adapt structured output space for semantic segmentation. Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, Manmohan Chandraker, Computer Vision and Pattern Recognition (CVPR). Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Ki- hyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic seg- mentation. In Computer Vision and Pattern Recognition (CVPR), pages 7472-7481, 2018.\n\nAdversarial discriminative domain adaptation. Eric Tzeng, Judy Hoffman, Kate Saenko, Trevor Darrell, Computer Vision and Pattern Recognition (CVPR). Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Computer Vision and Pattern Recognition (CVPR), pages 7167-7176, 2017.\n\nADVENT: Adversarial entropy minimization for domain adaptation in semantic segmentation. Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, Patrick P\u00e9rez, Computer Vision and Pattern Recognition (CVPR). Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick P\u00e9rez. ADVENT: Adversarial entropy minimization for domain adaptation in semantic segmenta- tion. In Computer Vision and Pattern Recognition (CVPR), pages 2517-2526, 2019.\n\nFewshot adaptive faster r-cnn. Tao Wang, Xiaopeng Zhang, Li Yuan, Jiashi Feng, Computer Vision and Pattern Recognition (CVPR). Tao Wang, Xiaopeng Zhang, Li Yuan, and Jiashi Feng. Few- shot adaptive faster r-cnn. In Computer Vision and Pattern Recognition (CVPR), pages 7173-7182, 2019.\n\nExploiting local feature patterns for unsupervised domain adaptation. Jun Wen, Risheng Liu, Nenggan Zheng, Qian Zheng, Zhefeng Gong, Junsong Yuan, Association for the Advancement of Artificial Intelligence (AAAI). 33Jun Wen, Risheng Liu, Nenggan Zheng, Qian Zheng, Zhe- feng Gong, and Junsong Yuan. Exploiting local feature pat- terns for unsupervised domain adaptation. In Association for the Advancement of Artificial Intelligence (AAAI), vol- ume 33, pages 5401-5408, 2019.\n\nGroup normalization. Yuxin Wu, Kaiming He, European Conference on Computer Vision (ECCV). Yuxin Wu and Kaiming He. Group normalization. In Euro- pean Conference on Computer Vision (ECCV), pages 3-19, 2018.\n\nPanoptic segmentation with an end-to-end cell r-cnn for pathology image analysis. Donghao Zhang, Yang Song, Dongnan Liu, Haozhe Jia, Siqi Liu, Yong Xia, Heng Huang, Weidong Cai, International Conference On Medical Image Computing Computer Assisted Intervention (MICCAI). SpringerDonghao Zhang, Yang Song, Dongnan Liu, Haozhe Jia, Siqi Liu, Yong Xia, Heng Huang, and Weidong Cai. Panoptic segmentation with an end-to-end cell r-cnn for pathology im- age analysis. In International Conference On Medical Im- age Computing Computer Assisted Intervention (MICCAI), pages 237-244. Springer, 2018.\n\nTask driven generative modeling for unsupervised domain adaptation: Application to x-ray image segmentation. Yue Zhang, Shun Miao, Tommaso Mansi, Rui Liao, International Conference On Medical Image Computing Computer Assisted Intervention (MICCAI). SpringerYue Zhang, Shun Miao, Tommaso Mansi, and Rui Liao. Task driven generative modeling for unsupervised domain adapta- tion: Application to x-ray image segmentation. In Interna- tional Conference On Medical Image Computing Computer Assisted Intervention (MICCAI), pages 599-607. Springer, 2018.\n\nUnpaired image-to-image translation using cycleconsistent adversarial networks. Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A Efros, International Conference on Computer Vision (ICCV). Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle- consistent adversarial networks. In International Conference on Computer Vision (ICCV), pages 2223-2232, 2017.\n", "annotations": {"author": "[{\"end\":183,\"start\":112},{\"end\":280,\"start\":184},{\"end\":398,\"start\":281},{\"end\":466,\"start\":399},{\"end\":571,\"start\":467},{\"end\":685,\"start\":572},{\"end\":722,\"start\":686},{\"end\":794,\"start\":723}]", "publisher": null, "author_last_name": "[{\"end\":123,\"start\":120},{\"end\":197,\"start\":192},{\"end\":290,\"start\":286},{\"end\":408,\"start\":403},{\"end\":488,\"start\":474},{\"end\":582,\"start\":577},{\"end\":694,\"start\":690},{\"end\":734,\"start\":731}]", "author_first_name": "[{\"end\":119,\"start\":112},{\"end\":191,\"start\":184},{\"end\":285,\"start\":281},{\"end\":402,\"start\":399},{\"end\":473,\"start\":467},{\"end\":576,\"start\":572},{\"end\":689,\"start\":686},{\"end\":730,\"start\":723}]", "author_affiliation": "[{\"end\":182,\"start\":125},{\"end\":279,\"start\":222},{\"end\":397,\"start\":315},{\"end\":465,\"start\":410},{\"end\":570,\"start\":515},{\"end\":684,\"start\":606},{\"end\":721,\"start\":696},{\"end\":793,\"start\":736}]", "title": "[{\"end\":109,\"start\":1},{\"end\":903,\"start\":795}]", "venue": null, "abstract": "[{\"end\":1952,\"start\":905}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2257,\"start\":2254},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2260,\"start\":2257},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2262,\"start\":2260},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2264,\"start\":2262},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2267,\"start\":2264},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2419,\"start\":2415},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2422,\"start\":2419},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2424,\"start\":2422},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2426,\"start\":2424},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2429,\"start\":2426},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":2432,\"start\":2429},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2435,\"start\":2432},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2438,\"start\":2435},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2814,\"start\":2810},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2933,\"start\":2929},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3362,\"start\":3358},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3364,\"start\":3362},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3367,\"start\":3364},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3646,\"start\":3642},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":3649,\"start\":3646},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":3806,\"start\":3802},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3808,\"start\":3806},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3811,\"start\":3808},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4022,\"start\":4018},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4025,\"start\":4022},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":4027,\"start\":4025},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4323,\"start\":4320},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4326,\"start\":4323},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4329,\"start\":4326},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4597,\"start\":4593},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4599,\"start\":4597},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4602,\"start\":4599},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5438,\"start\":5434},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5499,\"start\":5495},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5728,\"start\":5724},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5731,\"start\":5728},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8021,\"start\":8017},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":8256,\"start\":8253},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":8259,\"start\":8256},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8309,\"start\":8305},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":8339,\"start\":8335},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8382,\"start\":8378},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8530,\"start\":8526},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8533,\"start\":8530},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8558,\"start\":8555},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8561,\"start\":8558},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":8564,\"start\":8561},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":8567,\"start\":8564},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8705,\"start\":8701},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8708,\"start\":8705},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":8711,\"start\":8708},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":8738,\"start\":8734},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":8950,\"start\":8946},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9095,\"start\":9092},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9098,\"start\":9095},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":9101,\"start\":9098},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9304,\"start\":9300},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":9307,\"start\":9304},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":9310,\"start\":9307},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":9313,\"start\":9310},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":9316,\"start\":9313},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9319,\"start\":9316},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":9593,\"start\":9589},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":9596,\"start\":9593},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9598,\"start\":9596},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9727,\"start\":9723},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":9730,\"start\":9727},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":9732,\"start\":9730},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9735,\"start\":9732},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":9738,\"start\":9735},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":9744,\"start\":9740},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":9753,\"start\":9749},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9864,\"start\":9861},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":10056,\"start\":10052},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":10069,\"start\":10066},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10284,\"start\":10280},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11685,\"start\":11681},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":11724,\"start\":11720},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11787,\"start\":11784},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":11790,\"start\":11787},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":14048,\"start\":14044},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":16072,\"start\":16068},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":16535,\"start\":16531},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":16579,\"start\":16575},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16970,\"start\":16966},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":16973,\"start\":16970},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":19149,\"start\":19145},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":19564,\"start\":19560},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":19616,\"start\":19612},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":20965,\"start\":20961},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":21154,\"start\":21150},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":21165,\"start\":21161},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":21185,\"start\":21181},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":21745,\"start\":21741},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":21945,\"start\":21941},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22234,\"start\":22230},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":23674,\"start\":23670},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":23677,\"start\":23674},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":24391,\"start\":24387},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24408,\"start\":24405},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24418,\"start\":24415},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":24434,\"start\":24430},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24613,\"start\":24610},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24921,\"start\":24918},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":24953,\"start\":24950},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":25228,\"start\":25224},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25401,\"start\":25397},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25421,\"start\":25418},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":25473,\"start\":25469},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":25739,\"start\":25735},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25851,\"start\":25848},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":25956,\"start\":25953},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":26063,\"start\":26059},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":26175,\"start\":26171},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26879,\"start\":26876},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":27265,\"start\":27261},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":27454,\"start\":27451},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":27945,\"start\":27941},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":28140,\"start\":28136},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":29819,\"start\":29815},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":29925,\"start\":29921},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":30143,\"start\":30139},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":31585,\"start\":31581},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":31756,\"start\":31752},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":31827,\"start\":31823}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":34847,\"start\":34602},{\"attributes\":{\"id\":\"fig_1\"},\"end\":35002,\"start\":34848},{\"attributes\":{\"id\":\"fig_2\"},\"end\":35355,\"start\":35003},{\"attributes\":{\"id\":\"fig_3\"},\"end\":35631,\"start\":35356},{\"attributes\":{\"id\":\"fig_4\"},\"end\":36080,\"start\":35632},{\"attributes\":{\"id\":\"fig_5\"},\"end\":36233,\"start\":36081},{\"attributes\":{\"id\":\"fig_6\"},\"end\":36449,\"start\":36234},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":36590,\"start\":36450}]", "paragraph": "[{\"end\":2077,\"start\":1968},{\"end\":3138,\"start\":2079},{\"end\":3812,\"start\":3140},{\"end\":5136,\"start\":3814},{\"end\":6323,\"start\":5138},{\"end\":6998,\"start\":6325},{\"end\":7212,\"start\":7000},{\"end\":7846,\"start\":7214},{\"end\":9599,\"start\":7902},{\"end\":10738,\"start\":9640},{\"end\":11055,\"start\":10750},{\"end\":11304,\"start\":11105},{\"end\":12678,\"start\":11487},{\"end\":13284,\"start\":12710},{\"end\":13649,\"start\":13286},{\"end\":13947,\"start\":13685},{\"end\":14822,\"start\":13979},{\"end\":15905,\"start\":14859},{\"end\":17305,\"start\":15907},{\"end\":18402,\"start\":17337},{\"end\":19015,\"start\":18457},{\"end\":19319,\"start\":19057},{\"end\":19617,\"start\":19321},{\"end\":19667,\"start\":19619},{\"end\":20237,\"start\":19773},{\"end\":21001,\"start\":20268},{\"end\":22063,\"start\":21063},{\"end\":22428,\"start\":22065},{\"end\":22724,\"start\":22451},{\"end\":22880,\"start\":22726},{\"end\":23576,\"start\":22882},{\"end\":24210,\"start\":23578},{\"end\":25640,\"start\":24276},{\"end\":28420,\"start\":25718},{\"end\":31235,\"start\":28439},{\"end\":32222,\"start\":31280},{\"end\":33362,\"start\":32224},{\"end\":34601,\"start\":33377}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11486,\"start\":11305},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13684,\"start\":13650},{\"attributes\":{\"id\":\"formula_2\"},\"end\":13978,\"start\":13948},{\"attributes\":{\"id\":\"formula_3\"},\"end\":18456,\"start\":18403},{\"attributes\":{\"id\":\"formula_4\"},\"end\":19772,\"start\":19668},{\"attributes\":{\"id\":\"formula_5\"},\"end\":20267,\"start\":20238},{\"attributes\":{\"id\":\"formula_6\"},\"end\":25717,\"start\":25641}]", "table_ref": "[{\"end\":11130,\"start\":11123},{\"end\":12241,\"start\":12234},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":16813,\"start\":16806},{\"end\":26384,\"start\":26377},{\"end\":26579,\"start\":26572},{\"end\":27652,\"start\":27645},{\"end\":28805,\"start\":28798},{\"end\":30247,\"start\":30240},{\"end\":30834,\"start\":30827},{\"end\":31466,\"start\":31459},{\"end\":32243,\"start\":32236}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1966,\"start\":1954},{\"attributes\":{\"n\":\"2.\"},\"end\":7861,\"start\":7849},{\"attributes\":{\"n\":\"2.1.\"},\"end\":7900,\"start\":7864},{\"attributes\":{\"n\":\"2.2.\"},\"end\":9638,\"start\":9602},{\"attributes\":{\"n\":\"3.\"},\"end\":10748,\"start\":10741},{\"attributes\":{\"n\":\"3.1.\"},\"end\":11080,\"start\":11058},{\"end\":11103,\"start\":11083},{\"attributes\":{\"n\":\"3.2.\"},\"end\":12708,\"start\":12681},{\"attributes\":{\"n\":\"3.3.\"},\"end\":14857,\"start\":14825},{\"attributes\":{\"n\":\"3.4.\"},\"end\":17335,\"start\":17308},{\"attributes\":{\"n\":\"3.5.\"},\"end\":19055,\"start\":19018},{\"attributes\":{\"n\":\"4.\"},\"end\":21015,\"start\":21004},{\"attributes\":{\"n\":\"4.1.\"},\"end\":21061,\"start\":21018},{\"attributes\":{\"n\":\"4.2.\"},\"end\":22449,\"start\":22431},{\"attributes\":{\"n\":\"4.3.\"},\"end\":24235,\"start\":24213},{\"attributes\":{\"n\":\"4.3.1\"},\"end\":24274,\"start\":24238},{\"attributes\":{\"n\":\"4.3.2\"},\"end\":28437,\"start\":28423},{\"attributes\":{\"n\":\"4.3.3\"},\"end\":31278,\"start\":31238},{\"attributes\":{\"n\":\"5.\"},\"end\":33375,\"start\":33365},{\"end\":34613,\"start\":34603},{\"end\":34859,\"start\":34849},{\"end\":35014,\"start\":35004},{\"end\":35367,\"start\":35357},{\"end\":36092,\"start\":36082},{\"end\":36460,\"start\":36451}]", "table": null, "figure_caption": "[{\"end\":34847,\"start\":34615},{\"end\":35002,\"start\":34861},{\"end\":35355,\"start\":35016},{\"end\":35631,\"start\":35369},{\"end\":36080,\"start\":35634},{\"end\":36233,\"start\":36094},{\"end\":36449,\"start\":36236},{\"end\":36590,\"start\":36462}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":2669,\"start\":2660},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":6830,\"start\":6821},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11001,\"start\":10995},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":11948,\"start\":11942},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13087,\"start\":13081},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":14168,\"start\":14162},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15925,\"start\":15919},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16801,\"start\":16795},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":18800,\"start\":18794},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":28362,\"start\":28356},{\"end\":28816,\"start\":28810},{\"end\":28889,\"start\":28883},{\"end\":30431,\"start\":30423}]", "bib_author_first_name": "[{\"end\":36763,\"start\":36759},{\"end\":36785,\"start\":36778},{\"end\":36802,\"start\":36795},{\"end\":36816,\"start\":36809},{\"end\":36827,\"start\":36823},{\"end\":36848,\"start\":36841},{\"end\":36863,\"start\":36858},{\"end\":37369,\"start\":37364},{\"end\":37378,\"start\":37376},{\"end\":37387,\"start\":37384},{\"end\":37398,\"start\":37394},{\"end\":37413,\"start\":37404},{\"end\":37854,\"start\":37846},{\"end\":37871,\"start\":37865},{\"end\":37878,\"start\":37876},{\"end\":37887,\"start\":37883},{\"end\":37902,\"start\":37893},{\"end\":38215,\"start\":38210},{\"end\":38225,\"start\":38222},{\"end\":38238,\"start\":38230},{\"end\":38257,\"start\":38250},{\"end\":38266,\"start\":38263},{\"end\":38686,\"start\":38678},{\"end\":39021,\"start\":39019},{\"end\":39032,\"start\":39027},{\"end\":39046,\"start\":39041},{\"end\":39056,\"start\":39053},{\"end\":39072,\"start\":39063},{\"end\":39582,\"start\":39581},{\"end\":39599,\"start\":39596},{\"end\":39601,\"start\":39600},{\"end\":39933,\"start\":39925},{\"end\":39947,\"start\":39941},{\"end\":40268,\"start\":40263},{\"end\":40286,\"start\":40277},{\"end\":40295,\"start\":40291},{\"end\":40297,\"start\":40296},{\"end\":40305,\"start\":40300},{\"end\":40318,\"start\":40312},{\"end\":40328,\"start\":40325},{\"end\":40332,\"start\":40329},{\"end\":40343,\"start\":40340},{\"end\":40347,\"start\":40344},{\"end\":40359,\"start\":40354},{\"end\":40776,\"start\":40772},{\"end\":40803,\"start\":40793},{\"end\":40814,\"start\":40810},{\"end\":40831,\"start\":40824},{\"end\":40844,\"start\":40838},{\"end\":40855,\"start\":40851},{\"end\":40857,\"start\":40856},{\"end\":40872,\"start\":40871},{\"end\":40888,\"start\":40887},{\"end\":40890,\"start\":40889},{\"end\":40899,\"start\":40896},{\"end\":41325,\"start\":41318},{\"end\":41337,\"start\":41330},{\"end\":41353,\"start\":41348},{\"end\":41366,\"start\":41362},{\"end\":41375,\"start\":41367},{\"end\":41651,\"start\":41644},{\"end\":41663,\"start\":41656},{\"end\":41679,\"start\":41671},{\"end\":41689,\"start\":41685},{\"end\":41970,\"start\":41963},{\"end\":41982,\"start\":41975},{\"end\":41998,\"start\":41990},{\"end\":42008,\"start\":42004},{\"end\":42325,\"start\":42318},{\"end\":42333,\"start\":42330},{\"end\":42567,\"start\":42563},{\"end\":42581,\"start\":42577},{\"end\":42596,\"start\":42589},{\"end\":42610,\"start\":42603},{\"end\":42623,\"start\":42616},{\"end\":42635,\"start\":42631},{\"end\":42650,\"start\":42644},{\"end\":42652,\"start\":42651},{\"end\":42666,\"start\":42660},{\"end\":43090,\"start\":43088},{\"end\":43101,\"start\":43096},{\"end\":43119,\"start\":43111},{\"end\":43130,\"start\":43129},{\"end\":43146,\"start\":43145},{\"end\":43160,\"start\":43156},{\"end\":43162,\"start\":43161},{\"end\":43511,\"start\":43508},{\"end\":43526,\"start\":43519},{\"end\":43537,\"start\":43532},{\"end\":43551,\"start\":43548},{\"end\":43922,\"start\":43919},{\"end\":43933,\"start\":43930},{\"end\":43944,\"start\":43941},{\"end\":43957,\"start\":43950},{\"end\":43971,\"start\":43964},{\"end\":43973,\"start\":43972},{\"end\":44403,\"start\":44398},{\"end\":44418,\"start\":44411},{\"end\":44436,\"start\":44427},{\"end\":44455,\"start\":44447},{\"end\":44821,\"start\":44814},{\"end\":44836,\"start\":44829},{\"end\":44849,\"start\":44842},{\"end\":44862,\"start\":44856},{\"end\":44864,\"start\":44863},{\"end\":45237,\"start\":45229},{\"end\":45248,\"start\":45243},{\"end\":45266,\"start\":45256},{\"end\":45279,\"start\":45272},{\"end\":45294,\"start\":45286},{\"end\":45640,\"start\":45631},{\"end\":45655,\"start\":45651},{\"end\":45949,\"start\":45940},{\"end\":45967,\"start\":45960},{\"end\":45976,\"start\":45972},{\"end\":45994,\"start\":45987},{\"end\":46008,\"start\":46003},{\"end\":46345,\"start\":46339},{\"end\":46360,\"start\":46353},{\"end\":46373,\"start\":46368},{\"end\":46389,\"start\":46382},{\"end\":46408,\"start\":46400},{\"end\":46423,\"start\":46419},{\"end\":46934,\"start\":46933},{\"end\":46951,\"start\":46950},{\"end\":46971,\"start\":46970},{\"end\":47390,\"start\":47382},{\"end\":47397,\"start\":47395},{\"end\":47408,\"start\":47404},{\"end\":47770,\"start\":47762},{\"end\":47781,\"start\":47776},{\"end\":47794,\"start\":47790},{\"end\":48167,\"start\":48160},{\"end\":48180,\"start\":48173},{\"end\":48192,\"start\":48188},{\"end\":48203,\"start\":48199},{\"end\":48218,\"start\":48211},{\"end\":48534,\"start\":48527},{\"end\":48547,\"start\":48540},{\"end\":48559,\"start\":48555},{\"end\":48572,\"start\":48566},{\"end\":48583,\"start\":48580},{\"end\":48597,\"start\":48591},{\"end\":48615,\"start\":48608},{\"end\":49039,\"start\":49032},{\"end\":49051,\"start\":49045},{\"end\":49063,\"start\":49060},{\"end\":49382,\"start\":49375},{\"end\":49399,\"start\":49390},{\"end\":49401,\"start\":49400},{\"end\":49418,\"start\":49414},{\"end\":49420,\"start\":49419},{\"end\":49623,\"start\":49614},{\"end\":49633,\"start\":49630},{\"end\":49646,\"start\":49639},{\"end\":49669,\"start\":49653},{\"end\":50064,\"start\":50058},{\"end\":50080,\"start\":50074},{\"end\":50097,\"start\":50090},{\"end\":50105,\"start\":50104},{\"end\":50120,\"start\":50115},{\"end\":50122,\"start\":50121},{\"end\":50139,\"start\":50130},{\"end\":50160,\"start\":50150},{\"end\":50546,\"start\":50541},{\"end\":50560,\"start\":50554},{\"end\":50823,\"start\":50818},{\"end\":50838,\"start\":50832},{\"end\":50850,\"start\":50844},{\"end\":50864,\"start\":50858},{\"end\":51141,\"start\":51136},{\"end\":51440,\"start\":51433},{\"end\":51454,\"start\":51447},{\"end\":51469,\"start\":51460},{\"end\":51483,\"start\":51476},{\"end\":51780,\"start\":51776},{\"end\":51792,\"start\":51789},{\"end\":51807,\"start\":51800},{\"end\":51825,\"start\":51818},{\"end\":51840,\"start\":51834},{\"end\":51854,\"start\":51847},{\"end\":51869,\"start\":51863},{\"end\":51880,\"start\":51875},{\"end\":51896,\"start\":51892},{\"end\":51909,\"start\":51905},{\"end\":52241,\"start\":52237},{\"end\":52252,\"start\":52247},{\"end\":52272,\"start\":52268},{\"end\":52274,\"start\":52273},{\"end\":52284,\"start\":52283},{\"end\":52295,\"start\":52292},{\"end\":52794,\"start\":52787},{\"end\":52813,\"start\":52809},{\"end\":52822,\"start\":52818},{\"end\":53166,\"start\":53160},{\"end\":53173,\"start\":53167},{\"end\":53390,\"start\":53383},{\"end\":53402,\"start\":53396},{\"end\":53413,\"start\":53409},{\"end\":53735,\"start\":53726},{\"end\":53977,\"start\":53969},{\"end\":53992,\"start\":53984},{\"end\":54005,\"start\":53999},{\"end\":54022,\"start\":54016},{\"end\":54039,\"start\":54029},{\"end\":54054,\"start\":54046},{\"end\":54415,\"start\":54411},{\"end\":54427,\"start\":54423},{\"end\":54441,\"start\":54437},{\"end\":54456,\"start\":54450},{\"end\":54792,\"start\":54783},{\"end\":54805,\"start\":54797},{\"end\":54818,\"start\":54812},{\"end\":54835,\"start\":54827},{\"end\":54849,\"start\":54842},{\"end\":55183,\"start\":55180},{\"end\":55198,\"start\":55190},{\"end\":55208,\"start\":55206},{\"end\":55221,\"start\":55215},{\"end\":55509,\"start\":55506},{\"end\":55522,\"start\":55515},{\"end\":55535,\"start\":55528},{\"end\":55547,\"start\":55543},{\"end\":55562,\"start\":55555},{\"end\":55576,\"start\":55569},{\"end\":55940,\"start\":55935},{\"end\":55952,\"start\":55945},{\"end\":56210,\"start\":56203},{\"end\":56222,\"start\":56218},{\"end\":56236,\"start\":56229},{\"end\":56248,\"start\":56242},{\"end\":56258,\"start\":56254},{\"end\":56268,\"start\":56264},{\"end\":56278,\"start\":56274},{\"end\":56293,\"start\":56286},{\"end\":56826,\"start\":56823},{\"end\":56838,\"start\":56834},{\"end\":56852,\"start\":56845},{\"end\":56863,\"start\":56860},{\"end\":57350,\"start\":57343},{\"end\":57363,\"start\":57356},{\"end\":57377,\"start\":57370},{\"end\":57391,\"start\":57385},{\"end\":57393,\"start\":57392}]", "bib_author_last_name": "[{\"end\":36776,\"start\":36764},{\"end\":36793,\"start\":36786},{\"end\":36807,\"start\":36803},{\"end\":36821,\"start\":36817},{\"end\":36839,\"start\":36828},{\"end\":36856,\"start\":36849},{\"end\":36874,\"start\":36864},{\"end\":37374,\"start\":37370},{\"end\":37382,\"start\":37379},{\"end\":37392,\"start\":37388},{\"end\":37402,\"start\":37399},{\"end\":37418,\"start\":37414},{\"end\":37863,\"start\":37855},{\"end\":37874,\"start\":37872},{\"end\":37881,\"start\":37879},{\"end\":37891,\"start\":37888},{\"end\":37906,\"start\":37903},{\"end\":37912,\"start\":37908},{\"end\":38220,\"start\":38216},{\"end\":38228,\"start\":38226},{\"end\":38248,\"start\":38239},{\"end\":38261,\"start\":38258},{\"end\":38275,\"start\":38267},{\"end\":38694,\"start\":38687},{\"end\":39025,\"start\":39022},{\"end\":39039,\"start\":39033},{\"end\":39051,\"start\":39047},{\"end\":39061,\"start\":39057},{\"end\":39077,\"start\":39073},{\"end\":39594,\"start\":39583},{\"end\":39608,\"start\":39602},{\"end\":39615,\"start\":39610},{\"end\":39939,\"start\":39934},{\"end\":39957,\"start\":39948},{\"end\":40275,\"start\":40269},{\"end\":40289,\"start\":40287},{\"end\":40310,\"start\":40306},{\"end\":40323,\"start\":40319},{\"end\":40338,\"start\":40333},{\"end\":40352,\"start\":40348},{\"end\":40367,\"start\":40360},{\"end\":40791,\"start\":40777},{\"end\":40808,\"start\":40804},{\"end\":40822,\"start\":40815},{\"end\":40836,\"start\":40832},{\"end\":40849,\"start\":40845},{\"end\":40862,\"start\":40858},{\"end\":40869,\"start\":40864},{\"end\":40879,\"start\":40873},{\"end\":40885,\"start\":40881},{\"end\":40894,\"start\":40891},{\"end\":40906,\"start\":40900},{\"end\":40912,\"start\":40908},{\"end\":41328,\"start\":41326},{\"end\":41346,\"start\":41338},{\"end\":41360,\"start\":41354},{\"end\":41386,\"start\":41376},{\"end\":41654,\"start\":41652},{\"end\":41669,\"start\":41664},{\"end\":41683,\"start\":41680},{\"end\":41693,\"start\":41690},{\"end\":41973,\"start\":41971},{\"end\":41988,\"start\":41983},{\"end\":42002,\"start\":41999},{\"end\":42012,\"start\":42009},{\"end\":42328,\"start\":42326},{\"end\":42339,\"start\":42334},{\"end\":42575,\"start\":42568},{\"end\":42587,\"start\":42582},{\"end\":42601,\"start\":42597},{\"end\":42614,\"start\":42611},{\"end\":42629,\"start\":42624},{\"end\":42642,\"start\":42636},{\"end\":42658,\"start\":42653},{\"end\":42674,\"start\":42667},{\"end\":43094,\"start\":43091},{\"end\":43109,\"start\":43102},{\"end\":43127,\"start\":43120},{\"end\":43137,\"start\":43131},{\"end\":43143,\"start\":43139},{\"end\":43154,\"start\":43147},{\"end\":43168,\"start\":43163},{\"end\":43175,\"start\":43170},{\"end\":43517,\"start\":43512},{\"end\":43530,\"start\":43527},{\"end\":43546,\"start\":43538},{\"end\":43557,\"start\":43552},{\"end\":43928,\"start\":43923},{\"end\":43939,\"start\":43934},{\"end\":43948,\"start\":43945},{\"end\":43962,\"start\":43958},{\"end\":43979,\"start\":43974},{\"end\":44409,\"start\":44404},{\"end\":44425,\"start\":44419},{\"end\":44445,\"start\":44437},{\"end\":44462,\"start\":44456},{\"end\":44827,\"start\":44822},{\"end\":44840,\"start\":44837},{\"end\":44854,\"start\":44850},{\"end\":44870,\"start\":44865},{\"end\":45241,\"start\":45238},{\"end\":45254,\"start\":45249},{\"end\":45270,\"start\":45267},{\"end\":45284,\"start\":45280},{\"end\":45298,\"start\":45295},{\"end\":45649,\"start\":45641},{\"end\":45664,\"start\":45656},{\"end\":45958,\"start\":45950},{\"end\":45970,\"start\":45968},{\"end\":45985,\"start\":45977},{\"end\":46001,\"start\":45995},{\"end\":46015,\"start\":46009},{\"end\":46351,\"start\":46346},{\"end\":46366,\"start\":46361},{\"end\":46380,\"start\":46374},{\"end\":46398,\"start\":46390},{\"end\":46417,\"start\":46409},{\"end\":46429,\"start\":46424},{\"end\":46931,\"start\":46919},{\"end\":46948,\"start\":46935},{\"end\":46960,\"start\":46952},{\"end\":46968,\"start\":46962},{\"end\":46980,\"start\":46972},{\"end\":46988,\"start\":46982},{\"end\":47393,\"start\":47391},{\"end\":47402,\"start\":47398},{\"end\":47420,\"start\":47409},{\"end\":47774,\"start\":47771},{\"end\":47788,\"start\":47782},{\"end\":47803,\"start\":47795},{\"end\":48171,\"start\":48168},{\"end\":48186,\"start\":48181},{\"end\":48197,\"start\":48193},{\"end\":48209,\"start\":48204},{\"end\":48222,\"start\":48219},{\"end\":48538,\"start\":48535},{\"end\":48553,\"start\":48548},{\"end\":48564,\"start\":48560},{\"end\":48578,\"start\":48573},{\"end\":48589,\"start\":48584},{\"end\":48606,\"start\":48598},{\"end\":48619,\"start\":48616},{\"end\":49043,\"start\":49040},{\"end\":49058,\"start\":49052},{\"end\":49069,\"start\":49064},{\"end\":49388,\"start\":49383},{\"end\":49412,\"start\":49402},{\"end\":49430,\"start\":49421},{\"end\":49628,\"start\":49624},{\"end\":49637,\"start\":49634},{\"end\":49651,\"start\":49647},{\"end\":50072,\"start\":50065},{\"end\":50088,\"start\":50081},{\"end\":50102,\"start\":50098},{\"end\":50113,\"start\":50106},{\"end\":50128,\"start\":50123},{\"end\":50148,\"start\":50140},{\"end\":50166,\"start\":50161},{\"end\":50172,\"start\":50168},{\"end\":50552,\"start\":50547},{\"end\":50565,\"start\":50561},{\"end\":50830,\"start\":50824},{\"end\":50842,\"start\":50839},{\"end\":50856,\"start\":50851},{\"end\":50871,\"start\":50865},{\"end\":51158,\"start\":51142},{\"end\":51164,\"start\":51160},{\"end\":51445,\"start\":51441},{\"end\":51458,\"start\":51455},{\"end\":51474,\"start\":51470},{\"end\":51487,\"start\":51484},{\"end\":51787,\"start\":51781},{\"end\":51798,\"start\":51793},{\"end\":51816,\"start\":51808},{\"end\":51832,\"start\":51826},{\"end\":51845,\"start\":51841},{\"end\":51861,\"start\":51855},{\"end\":51873,\"start\":51870},{\"end\":51890,\"start\":51881},{\"end\":51903,\"start\":51897},{\"end\":51915,\"start\":51910},{\"end\":52245,\"start\":52242},{\"end\":52266,\"start\":52253},{\"end\":52281,\"start\":52275},{\"end\":52290,\"start\":52285},{\"end\":52301,\"start\":52296},{\"end\":52305,\"start\":52303},{\"end\":52807,\"start\":52795},{\"end\":52816,\"start\":52814},{\"end\":52831,\"start\":52823},{\"end\":52836,\"start\":52833},{\"end\":53181,\"start\":53174},{\"end\":53394,\"start\":53391},{\"end\":53407,\"start\":53403},{\"end\":53420,\"start\":53414},{\"end\":53741,\"start\":53736},{\"end\":53982,\"start\":53978},{\"end\":53997,\"start\":53993},{\"end\":54014,\"start\":54006},{\"end\":54027,\"start\":54023},{\"end\":54044,\"start\":54040},{\"end\":54065,\"start\":54055},{\"end\":54421,\"start\":54416},{\"end\":54435,\"start\":54428},{\"end\":54448,\"start\":54442},{\"end\":54464,\"start\":54457},{\"end\":54795,\"start\":54793},{\"end\":54810,\"start\":54806},{\"end\":54825,\"start\":54819},{\"end\":54840,\"start\":54836},{\"end\":54855,\"start\":54850},{\"end\":55188,\"start\":55184},{\"end\":55204,\"start\":55199},{\"end\":55213,\"start\":55209},{\"end\":55226,\"start\":55222},{\"end\":55513,\"start\":55510},{\"end\":55526,\"start\":55523},{\"end\":55541,\"start\":55536},{\"end\":55553,\"start\":55548},{\"end\":55567,\"start\":55563},{\"end\":55581,\"start\":55577},{\"end\":55943,\"start\":55941},{\"end\":55955,\"start\":55953},{\"end\":56216,\"start\":56211},{\"end\":56227,\"start\":56223},{\"end\":56240,\"start\":56237},{\"end\":56252,\"start\":56249},{\"end\":56262,\"start\":56259},{\"end\":56272,\"start\":56269},{\"end\":56284,\"start\":56279},{\"end\":56297,\"start\":56294},{\"end\":56832,\"start\":56827},{\"end\":56843,\"start\":56839},{\"end\":56858,\"start\":56853},{\"end\":56868,\"start\":56864},{\"end\":57354,\"start\":57351},{\"end\":57368,\"start\":57364},{\"end\":57383,\"start\":57378},{\"end\":57399,\"start\":57394}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":2044592},\"end\":37247,\"start\":36592},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":59222756},\"end\":37754,\"start\":37249},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":23873598},\"end\":38145,\"start\":37756},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":3780471},\"end\":38532,\"start\":38147},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":28581791},\"end\":38901,\"start\":38534},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":13741866},\"end\":39421,\"start\":38903},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":17622089},\"end\":39872,\"start\":39423},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":6755881},\"end\":40161,\"start\":39874},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":195583959},\"end\":40651,\"start\":40163},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":3928268},\"end\":41316,\"start\":40653},{\"attributes\":{\"id\":\"b10\"},\"end\":41596,\"start\":41318},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":206594692},\"end\":41916,\"start\":41598},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":6447277},\"end\":42251,\"start\":41918},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":198229858},\"end\":42561,\"start\":42253},{\"attributes\":{\"id\":\"b14\"},\"end\":43020,\"start\":42563},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":119102145},\"end\":43454,\"start\":43022},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":4883312},\"end\":43786,\"start\":43456},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":6137951},\"end\":44309,\"start\":43788},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":4486024},\"end\":44746,\"start\":44311},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":6200260},\"end\":45133,\"start\":44748},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":153312783},\"end\":45594,\"start\":45135},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":57721164},\"end\":45915,\"start\":45596},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":4853375},\"end\":46245,\"start\":45917},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":5162860},\"end\":46720,\"start\":46247},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":32579784},\"end\":47309,\"start\":46722},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":129944996},\"end\":47662,\"start\":47311},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":10716717},\"end\":48069,\"start\":47664},{\"attributes\":{\"doi\":\"arXiv:2002.06345\",\"id\":\"b27\"},\"end\":48449,\"start\":48071},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":199466111},\"end\":48980,\"start\":48451},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":3783306},\"end\":49309,\"start\":48982},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":29890312},\"end\":49612,\"start\":49311},{\"attributes\":{\"id\":\"b31\"},\"end\":49968,\"start\":49614},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":52901563},\"end\":50464,\"start\":49970},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":8453259},\"end\":50728,\"start\":50466},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":59601271},\"end\":51103,\"start\":50730},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":740063},\"end\":51367,\"start\":51105},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":81981856},\"end\":51736,\"start\":51369},{\"attributes\":{\"id\":\"b37\"},\"end\":52139,\"start\":51738},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":46939997},\"end\":52705,\"start\":52141},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":10328909},\"end\":53118,\"start\":52707},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":5747983},\"end\":53333,\"start\":53120},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":16439870},\"end\":53659,\"start\":53335},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":5908881},\"end\":53898,\"start\":53661},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":3556146},\"end\":54363,\"start\":53900},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":4357800},\"end\":54692,\"start\":54365},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":54216961},\"end\":55147,\"start\":54694},{\"attributes\":{\"id\":\"b46\"},\"end\":55434,\"start\":55149},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":53291502},\"end\":55912,\"start\":55436},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":4076251},\"end\":56119,\"start\":55914},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":52270816},\"end\":56712,\"start\":56121},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":49317508},\"end\":57261,\"start\":56714},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":233404466},\"end\":57674,\"start\":57263}]", "bib_title": "[{\"end\":36757,\"start\":36592},{\"end\":37362,\"start\":37249},{\"end\":37844,\"start\":37756},{\"end\":38208,\"start\":38147},{\"end\":38676,\"start\":38534},{\"end\":39017,\"start\":38903},{\"end\":39579,\"start\":39423},{\"end\":39923,\"start\":39874},{\"end\":40261,\"start\":40163},{\"end\":40770,\"start\":40653},{\"end\":41642,\"start\":41598},{\"end\":41961,\"start\":41918},{\"end\":42316,\"start\":42253},{\"end\":43086,\"start\":43022},{\"end\":43506,\"start\":43456},{\"end\":43917,\"start\":43788},{\"end\":44396,\"start\":44311},{\"end\":44812,\"start\":44748},{\"end\":45227,\"start\":45135},{\"end\":45629,\"start\":45596},{\"end\":45938,\"start\":45917},{\"end\":46337,\"start\":46247},{\"end\":46917,\"start\":46722},{\"end\":47380,\"start\":47311},{\"end\":47760,\"start\":47664},{\"end\":48525,\"start\":48451},{\"end\":49030,\"start\":48982},{\"end\":49373,\"start\":49311},{\"end\":50056,\"start\":49970},{\"end\":50539,\"start\":50466},{\"end\":50816,\"start\":50730},{\"end\":51134,\"start\":51105},{\"end\":51431,\"start\":51369},{\"end\":52235,\"start\":52141},{\"end\":52785,\"start\":52707},{\"end\":53158,\"start\":53120},{\"end\":53381,\"start\":53335},{\"end\":53724,\"start\":53661},{\"end\":53967,\"start\":53900},{\"end\":54409,\"start\":54365},{\"end\":54781,\"start\":54694},{\"end\":55178,\"start\":55149},{\"end\":55504,\"start\":55436},{\"end\":55933,\"start\":55914},{\"end\":56201,\"start\":56121},{\"end\":56821,\"start\":56714},{\"end\":57341,\"start\":57263}]", "bib_author": "[{\"end\":36778,\"start\":36759},{\"end\":36795,\"start\":36778},{\"end\":36809,\"start\":36795},{\"end\":36823,\"start\":36809},{\"end\":36841,\"start\":36823},{\"end\":36858,\"start\":36841},{\"end\":36876,\"start\":36858},{\"end\":37376,\"start\":37364},{\"end\":37384,\"start\":37376},{\"end\":37394,\"start\":37384},{\"end\":37404,\"start\":37394},{\"end\":37420,\"start\":37404},{\"end\":37865,\"start\":37846},{\"end\":37876,\"start\":37865},{\"end\":37883,\"start\":37876},{\"end\":37893,\"start\":37883},{\"end\":37908,\"start\":37893},{\"end\":37914,\"start\":37908},{\"end\":38222,\"start\":38210},{\"end\":38230,\"start\":38222},{\"end\":38250,\"start\":38230},{\"end\":38263,\"start\":38250},{\"end\":38277,\"start\":38263},{\"end\":38696,\"start\":38678},{\"end\":39027,\"start\":39019},{\"end\":39041,\"start\":39027},{\"end\":39053,\"start\":39041},{\"end\":39063,\"start\":39053},{\"end\":39079,\"start\":39063},{\"end\":39596,\"start\":39581},{\"end\":39610,\"start\":39596},{\"end\":39617,\"start\":39610},{\"end\":39941,\"start\":39925},{\"end\":39959,\"start\":39941},{\"end\":40277,\"start\":40263},{\"end\":40291,\"start\":40277},{\"end\":40300,\"start\":40291},{\"end\":40312,\"start\":40300},{\"end\":40325,\"start\":40312},{\"end\":40340,\"start\":40325},{\"end\":40354,\"start\":40340},{\"end\":40369,\"start\":40354},{\"end\":40793,\"start\":40772},{\"end\":40810,\"start\":40793},{\"end\":40824,\"start\":40810},{\"end\":40838,\"start\":40824},{\"end\":40851,\"start\":40838},{\"end\":40864,\"start\":40851},{\"end\":40871,\"start\":40864},{\"end\":40881,\"start\":40871},{\"end\":40887,\"start\":40881},{\"end\":40896,\"start\":40887},{\"end\":40908,\"start\":40896},{\"end\":40914,\"start\":40908},{\"end\":41330,\"start\":41318},{\"end\":41348,\"start\":41330},{\"end\":41362,\"start\":41348},{\"end\":41388,\"start\":41362},{\"end\":41656,\"start\":41644},{\"end\":41671,\"start\":41656},{\"end\":41685,\"start\":41671},{\"end\":41695,\"start\":41685},{\"end\":41975,\"start\":41963},{\"end\":41990,\"start\":41975},{\"end\":42004,\"start\":41990},{\"end\":42014,\"start\":42004},{\"end\":42330,\"start\":42318},{\"end\":42341,\"start\":42330},{\"end\":42577,\"start\":42563},{\"end\":42589,\"start\":42577},{\"end\":42603,\"start\":42589},{\"end\":42616,\"start\":42603},{\"end\":42631,\"start\":42616},{\"end\":42644,\"start\":42631},{\"end\":42660,\"start\":42644},{\"end\":42676,\"start\":42660},{\"end\":43096,\"start\":43088},{\"end\":43111,\"start\":43096},{\"end\":43129,\"start\":43111},{\"end\":43139,\"start\":43129},{\"end\":43145,\"start\":43139},{\"end\":43156,\"start\":43145},{\"end\":43170,\"start\":43156},{\"end\":43177,\"start\":43170},{\"end\":43519,\"start\":43508},{\"end\":43532,\"start\":43519},{\"end\":43548,\"start\":43532},{\"end\":43559,\"start\":43548},{\"end\":43930,\"start\":43919},{\"end\":43941,\"start\":43930},{\"end\":43950,\"start\":43941},{\"end\":43964,\"start\":43950},{\"end\":43981,\"start\":43964},{\"end\":44411,\"start\":44398},{\"end\":44427,\"start\":44411},{\"end\":44447,\"start\":44427},{\"end\":44464,\"start\":44447},{\"end\":44829,\"start\":44814},{\"end\":44842,\"start\":44829},{\"end\":44856,\"start\":44842},{\"end\":44872,\"start\":44856},{\"end\":45243,\"start\":45229},{\"end\":45256,\"start\":45243},{\"end\":45272,\"start\":45256},{\"end\":45286,\"start\":45272},{\"end\":45300,\"start\":45286},{\"end\":45651,\"start\":45631},{\"end\":45666,\"start\":45651},{\"end\":45960,\"start\":45940},{\"end\":45972,\"start\":45960},{\"end\":45987,\"start\":45972},{\"end\":46003,\"start\":45987},{\"end\":46017,\"start\":46003},{\"end\":46353,\"start\":46339},{\"end\":46368,\"start\":46353},{\"end\":46382,\"start\":46368},{\"end\":46400,\"start\":46382},{\"end\":46419,\"start\":46400},{\"end\":46431,\"start\":46419},{\"end\":46933,\"start\":46919},{\"end\":46950,\"start\":46933},{\"end\":46962,\"start\":46950},{\"end\":46970,\"start\":46962},{\"end\":46982,\"start\":46970},{\"end\":46990,\"start\":46982},{\"end\":47395,\"start\":47382},{\"end\":47404,\"start\":47395},{\"end\":47422,\"start\":47404},{\"end\":47776,\"start\":47762},{\"end\":47790,\"start\":47776},{\"end\":47805,\"start\":47790},{\"end\":48173,\"start\":48160},{\"end\":48188,\"start\":48173},{\"end\":48199,\"start\":48188},{\"end\":48211,\"start\":48199},{\"end\":48224,\"start\":48211},{\"end\":48540,\"start\":48527},{\"end\":48555,\"start\":48540},{\"end\":48566,\"start\":48555},{\"end\":48580,\"start\":48566},{\"end\":48591,\"start\":48580},{\"end\":48608,\"start\":48591},{\"end\":48621,\"start\":48608},{\"end\":49045,\"start\":49032},{\"end\":49060,\"start\":49045},{\"end\":49071,\"start\":49060},{\"end\":49390,\"start\":49375},{\"end\":49414,\"start\":49390},{\"end\":49432,\"start\":49414},{\"end\":49630,\"start\":49614},{\"end\":49639,\"start\":49630},{\"end\":49653,\"start\":49639},{\"end\":49672,\"start\":49653},{\"end\":50074,\"start\":50058},{\"end\":50090,\"start\":50074},{\"end\":50104,\"start\":50090},{\"end\":50115,\"start\":50104},{\"end\":50130,\"start\":50115},{\"end\":50150,\"start\":50130},{\"end\":50168,\"start\":50150},{\"end\":50174,\"start\":50168},{\"end\":50554,\"start\":50541},{\"end\":50567,\"start\":50554},{\"end\":50832,\"start\":50818},{\"end\":50844,\"start\":50832},{\"end\":50858,\"start\":50844},{\"end\":50873,\"start\":50858},{\"end\":51160,\"start\":51136},{\"end\":51166,\"start\":51160},{\"end\":51447,\"start\":51433},{\"end\":51460,\"start\":51447},{\"end\":51476,\"start\":51460},{\"end\":51489,\"start\":51476},{\"end\":51789,\"start\":51776},{\"end\":51800,\"start\":51789},{\"end\":51818,\"start\":51800},{\"end\":51834,\"start\":51818},{\"end\":51847,\"start\":51834},{\"end\":51863,\"start\":51847},{\"end\":51875,\"start\":51863},{\"end\":51892,\"start\":51875},{\"end\":51905,\"start\":51892},{\"end\":51917,\"start\":51905},{\"end\":52247,\"start\":52237},{\"end\":52268,\"start\":52247},{\"end\":52283,\"start\":52268},{\"end\":52292,\"start\":52283},{\"end\":52303,\"start\":52292},{\"end\":52307,\"start\":52303},{\"end\":52809,\"start\":52787},{\"end\":52818,\"start\":52809},{\"end\":52833,\"start\":52818},{\"end\":52838,\"start\":52833},{\"end\":53183,\"start\":53160},{\"end\":53396,\"start\":53383},{\"end\":53409,\"start\":53396},{\"end\":53422,\"start\":53409},{\"end\":53743,\"start\":53726},{\"end\":53984,\"start\":53969},{\"end\":53999,\"start\":53984},{\"end\":54016,\"start\":53999},{\"end\":54029,\"start\":54016},{\"end\":54046,\"start\":54029},{\"end\":54067,\"start\":54046},{\"end\":54423,\"start\":54411},{\"end\":54437,\"start\":54423},{\"end\":54450,\"start\":54437},{\"end\":54466,\"start\":54450},{\"end\":54797,\"start\":54783},{\"end\":54812,\"start\":54797},{\"end\":54827,\"start\":54812},{\"end\":54842,\"start\":54827},{\"end\":54857,\"start\":54842},{\"end\":55190,\"start\":55180},{\"end\":55206,\"start\":55190},{\"end\":55215,\"start\":55206},{\"end\":55228,\"start\":55215},{\"end\":55515,\"start\":55506},{\"end\":55528,\"start\":55515},{\"end\":55543,\"start\":55528},{\"end\":55555,\"start\":55543},{\"end\":55569,\"start\":55555},{\"end\":55583,\"start\":55569},{\"end\":55945,\"start\":55935},{\"end\":55957,\"start\":55945},{\"end\":56218,\"start\":56203},{\"end\":56229,\"start\":56218},{\"end\":56242,\"start\":56229},{\"end\":56254,\"start\":56242},{\"end\":56264,\"start\":56254},{\"end\":56274,\"start\":56264},{\"end\":56286,\"start\":56274},{\"end\":56299,\"start\":56286},{\"end\":56834,\"start\":56823},{\"end\":56845,\"start\":56834},{\"end\":56860,\"start\":56845},{\"end\":56870,\"start\":56860},{\"end\":57356,\"start\":57343},{\"end\":57370,\"start\":57356},{\"end\":57385,\"start\":57370},{\"end\":57401,\"start\":57385}]", "bib_venue": "[{\"end\":36908,\"start\":36876},{\"end\":37485,\"start\":37420},{\"end\":37936,\"start\":37914},{\"end\":38322,\"start\":38277},{\"end\":38702,\"start\":38696},{\"end\":39145,\"start\":39079},{\"end\":39631,\"start\":39617},{\"end\":40010,\"start\":39959},{\"end\":40391,\"start\":40369},{\"end\":40969,\"start\":40914},{\"end\":41438,\"start\":41388},{\"end\":41740,\"start\":41695},{\"end\":42060,\"start\":42014},{\"end\":42391,\"start\":42341},{\"end\":42783,\"start\":42676},{\"end\":43223,\"start\":43177},{\"end\":43604,\"start\":43559},{\"end\":44030,\"start\":43981},{\"end\":44510,\"start\":44464},{\"end\":44918,\"start\":44872},{\"end\":45346,\"start\":45300},{\"end\":45712,\"start\":45666},{\"end\":46063,\"start\":46017},{\"end\":46467,\"start\":46431},{\"end\":46996,\"start\":46990},{\"end\":47468,\"start\":47422},{\"end\":47851,\"start\":47805},{\"end\":48158,\"start\":48071},{\"end\":48687,\"start\":48621},{\"end\":49130,\"start\":49071},{\"end\":49446,\"start\":49432},{\"end\":49785,\"start\":49672},{\"end\":50210,\"start\":50174},{\"end\":50581,\"start\":50567},{\"end\":50909,\"start\":50873},{\"end\":51217,\"start\":51166},{\"end\":51535,\"start\":51489},{\"end\":51774,\"start\":51738},{\"end\":52398,\"start\":52307},{\"end\":52897,\"start\":52838},{\"end\":53212,\"start\":53183},{\"end\":53487,\"start\":53422},{\"end\":53768,\"start\":53743},{\"end\":54113,\"start\":54067},{\"end\":54512,\"start\":54466},{\"end\":54903,\"start\":54857},{\"end\":55274,\"start\":55228},{\"end\":55648,\"start\":55583},{\"end\":56002,\"start\":55957},{\"end\":56390,\"start\":56299},{\"end\":56961,\"start\":56870},{\"end\":57451,\"start\":57401}]"}}}, "year": 2023, "month": 12, "day": 17}