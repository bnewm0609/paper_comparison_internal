{"id": 13682824, "updated": "2023-09-30 22:50:32.418", "metadata": {"title": "A Reinforcement Learning Approach to Interactive-Predictive Neural Machine Translation", "authors": "[{\"first\":\"Tsz Kin\",\"last\":\"Lam\",\"middle\":[]},{\"first\":\"Julia\",\"last\":\"Kreutzer\",\"middle\":[]},{\"first\":\"Stefan\",\"last\":\"Riezler\",\"middle\":[]}]", "venue": "EAMT", "journal": "Proceedings of the 21st Annual Conference of the European Association for Machine Translation", "publication_date": {"year": 2018, "month": null, "day": null}, "abstract": "We present an approach to interactivepredictive neural machine translation that attempts to reduce human effort from three directions: Firstly, instead of requiring humans to select, correct, or delete segments, we employ the idea of learning from human reinforcements in form of judgments on the quality of partial translations. Secondly, human effort is further reduced by using the entropy of word predictions as uncertainty criterion to trigger feedback requests. Lastly, online updates of the model parameters after every interaction allow the model to adapt quickly. We show in simulation experiments that reward signals on partial translations significantly improve character F-score and BLEU compared to feedback on full translations only, while human effort can be reduced to an average number of 5 feedback requests for every input.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "1805.01553", "mag": "2963542691", "acl": "2018.eamt-main.17", "pubmed": null, "pubmedcentral": null, "dblp": "conf/eamt/LamKR18", "doi": null}}, "content": {"source": {"pdf_hash": "d93d8696db92ff7440752a4598a57f944711991a", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclanthology.org/2018.eamt-main.17.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "2ca57d48fa6946ce048a1b6ebde37873fbd2d836", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d93d8696db92ff7440752a4598a57f944711991a.txt", "contents": "\nA Reinforcement Learning Approach to Interactive-Predictive Neural Machine Translation\n\n\nTsz Kin Lam lam@cl.uni-heidelberg.de \nComputational Linguistics & \u2020 IWR\nHeidelberg University\nGermany\n\nJulia Kreutzer kreutzer@cl.uni-heidelberg.de \nComputational Linguistics & \u2020 IWR\nHeidelberg University\nGermany\n\nStefan Riezler riezler@cl.uni-heidelberg.de \nComputational Linguistics & \u2020 IWR\nHeidelberg University\nGermany\n\nA Reinforcement Learning Approach to Interactive-Predictive Neural Machine Translation\n\nWe present an approach to interactivepredictive neural machine translation that attempts to reduce human effort from three directions: Firstly, instead of requiring humans to select, correct, or delete segments, we employ the idea of learning from human reinforcements in form of judgments on the quality of partial translations. Secondly, human effort is further reduced by using the entropy of word predictions as uncertainty criterion to trigger feedback requests. Lastly, online updates of the model parameters after every interaction allow the model to adapt quickly. We show in simulation experiments that reward signals on partial translations significantly improve character F-score and BLEU compared to feedback on full translations only, while human effort can be reduced to an average number of 5 feedback requests for every input.\n\nIntroduction\n\nInteractive-predictive machine translation aims at obtaining high-quality machine translation by involving humans in a loop of user validations of partial translations suggested by the machine translation system. This interaction protocol can easily be fit to neural machine translation (NMT) (Bahdanau et al., 2015) by conditioning the model's word predictions on the user-validated prefix (Knowles and Koehn, 2016;Wuebker et al., 2016). User studies conducted by Green et c 2018 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CC- BY-ND. al. (2014) for phrase-based machine translation have shown that the interactive-predictive interaction protocol leads to significant reductions in post-editing effort. Other user studies on interactive machine translation based on post-editing have shown that human effort can also be reduced by improving the online adaptation capabilities of the learning system, both for statistical phrase-based (Bentivogli et al., 2016) or NMT systems (Karimova et al., 2017).\n\nThe goal of our work is to further reduce human effort in interactive-predictive NMT by combining the advantages of the interactive-predictive protocol with the advantages of learning from weak feedback. For the latter we rely on techniques from reinforcement learning (Sutton and Barto, 2017), a.k.a. bandit structured prediction (Sokolov et al., 2016;Kreutzer et al., 2017;Nguyen et al., 2017) in the context of sequence-to-sequence learning. Our approach attacks the problem of reducing human effort from three innovative directions.\n\n\u2022 Firstly, instead of requiring humans to correct or delete segments proposed by the machine translation system, we employ the reinforcement learning idea of humans providing reward signals in form of judgments on the quality of the machine translation. Human effort is reduced since each partial translation receives a human reward signal at most once, rendering it a bandit-type feedback signal, and each reward signal itself is easier to obtain than a correction of a translation.\n\n\u2022 In order to reduce the amount of feedback signals even further, we integrate an uncertainty criterion for word predictions to trigger requests for human feedback. Using the comparison of the current average entropy to the entropy of word predictions in the history as a measure for uncertainty, we reduce the amount of feedbacks requested from humans to an average number of 5 requests per input.\n\n\u2022 In contrast to previous approaches to interactive-predictive translation, the parameters of our translation system are updated online after receiving feedback for partial translations. The update is done according to an actor-critic reinforcement learning protocol where each update pushes up the score function of the partial translation sampled by the model (called actor) proportional to a learned reward function (called critic). Furthermore, since the entropy criterion is based on the actor, it is also automatically updated. Frequent updates improve the adaptability of our system, resulting in a further reduction of human effort.\n\nThe rest of this paper is structured as follows. In Section 2, we will situate our approach in the context of interactive machine translation and analyze our contribution related to reinforcement learning for sequence prediction problems. Details of our algorithm are given in Section 3. We evaluate our approach in a simulation study where bandit feedback is computed by evaluating partial translations against references under a character F-score metric (Popovi\u0107, 2015) without revealing the reference translation to the learning system (Section 4). We show that segment-wise reward signals improve translation quality over reinforcement learning with sparse sentence-wise rewards, measured by character F-score and corpusbased BLEU against references. Furthermore, we show that human effort, measured by the number of feedback requests, can be reduced to an average number of 5 requests per input. These implications of our new paradigm are discussed in Section 5.\n\n\nRelated Work\n\nThe interactive-predictive translation paradigm reaches back to early approaches for IBM-type (Foster et al., 1997;?) and phrase-based machine translation (Barrachina et al., 2008;Green et al., 2014). Knowles and Koehn (2016) and Wuebker et al. (2016) presented neural interactive translation prediction -a translation scenario where translators interact with an NMT system by accepting or correcting subsequent target tokens sug-gested by the NMT system in an auto-complete style. NMT is naturally suited for this incremental production of outputs, since it models the probability of target tokens given a history of target tokens sequentially from left to right. In standard supervised training with teacher forcing, this history comes from the ground truth, while in interactivepredictive translation it is provided by the prefix accepted or entered by the user. Both approaches use references to simulate an interaction with a translator and compare their approach to phrasebased prefix-search. They find that NMT is more accurate in word and letter prediction and recovers better from failures. Similar to their work, we will experiment in a simulated environment with references mimicking the translator. However, we do not use the reference directly for teacher forcing, but only to derive weak feedback from it. Furthermore, our approach employs techniques to reduce the number of interactions, and to update the model more frequently than after each sentence.\n\nOur work is also closely related to approaches for interactive pre-post-editing (Marie and Max, 2015;Domingo et al., 2018). The core idea is to ask the translator to mark good segments and use these for a more informed re-decoding. Both studies could show a reduction in human effort for post-editing in simulation experiments. We share the goal of using human feedback more effectively by targeting it towards essential translation segments, however, our approach does adhere to the left-to-right navigation through translation hypotheses. In difference to these approaches, we try to reduce human effort even further by minimizing the number of feedback requests and by frequent model updates.\n\nReinforcing/penalizing a targeted set of actions can also be found in recent approaches to reinforcement learning from human feedback. For example, Judah et al. (2010) presented a scenario where users interactively label freely chosen good and bad parts of a policy's trajectory. The policy is directly trained with this reinforcement signal to play a real-time strategy game. Simulations of NMT systems interacting with human feedback have been presented firstly by Kreutzer et al. (2017), Nguyen (2017), or Bahdanau et al. (2017) who apply different policy gradient algorithms, William's REINFORCE (Williams, 1992) or actorcritic methods (Konda and Tsitsiklis, 2000;Sutton et al., 2000;Mnih et al., 2016), respectively. While Bahdanau et al.'s (2017) approach operates in a fully supervised learning scenario, where rewards are simulated in comparison to references with smoothed and length-rescaled BLEU, Kreutzer et al. (2017) and Nguyen et al. (2017) limit the setup to sentence-level bandit feedback, i.e. only one feedback is obtained for one completed translation per input. In this paper, we use actor-critic update strategies, but we receive simulated bandit feedback on the sub-sentence level.\n\nWe adopt techniques from active learning to reduce the number of feedbacks requested from a user. Gonz\u00e1lez-Rubio et al. (2011;2012) apply active learning for interactive machine translation, where a user interactively finishes the translation of an SMT system. The active learning component decides which sentences to sample for translation (i.e. receive full supervision for) and the SMT system is updated online (Ortiz-Mart\u00ednez et al., 2010). In our algorithm the active learning component decides which prefixes to be rated (i.e. receive weak feedback for) based on their average entropy. Entropy is a popular measure for uncertainty in active learning: the rationale is to feed the learning algorithm with labeled instances where it is least confident about its own predictions. This uncertainty sampling algorithm (Lewis and Gale, 1994) is a popular choice for active learning for NLP tasks with expensive gold labeling, such as text classification (Lewis and Gale, 1994), word-sense disambiguation (Chen et al., 2006) and statistical parsing (Tang et al., 2002). Our method falls into the category of stream-based online active learning (as opposed to pool-based active learning, selecting instances from a large pool of unlabeled data), since the algorithm decides on the fly (online) which translation prefixes of the stream of source tokens to request feedback for. Instead of receiving gold annotations, as in the studies mentioned above, our algorithm receives weaker, bandit feedback -but the motivation of minimizing human labeling effort is the same.\n\n\nReinforcement Learning for Interactive-Predictive Translation\n\nIn the following, we will introduce the key ideas of our approach, formalize them, and present an algorithm for reinforcement learning for interactivepredictive NMT.\n\n\nActor-Critic Reinforcement Learning for NMT\n\nThe objective of reinforcement learning methods is to maximize the expected reward obtainable from interactions of an agent (here: a machine translation system) with an environment (here: a human translator). In our case, the agent/system performs actions by predicting target words y t according to a stochastic policy p \u03b8 parameterized by an RNN encoder-decoder NMT system (Bahdanau et al., 2015) where\np \u03b8 (y|x) = Ty t=1 p \u03b8 (y t |x, y <t ).(1)\nThe environment/human can be formalized as a Markov Decision Process where a state at time t is a tuple s t = x, y <t consisting of the conditioning context of the input x and the current produced history of target tokens y <t . Note that since states s t+1 include the current chosen action y t and can contain long histories y <t , the state distribution is sparse and deterministic. The reward distribution of the environment/critic is estimated by function approximation in actor-critic methods. The reward estimator (called critic) is trained on actual rewards and updated after every interaction, and then used to update the parameters of the policy (called actor) in a direction of function improvement. We use the advantage actor critic framework of Mnih et al. (2016) which estimates the advantage A \u03c6 (y t |s t ) in reward of choosing action y t in a given state s t over the mean reward value for that state. This framework has been applied to reinforcement learning for NMT by Nguyen et al. (2017). The main objective of the actor is then to maximize the expected advantage\nL \u03b8 = E p(x)p \u03b8 (y|x) \uf8ee \uf8f0 Ty t=1 A \u03c6 (y t |s t ) \uf8f9 \uf8fb .(2)\nThe stochastic gradient of this objective for a sampled target word\u0177 t for an input x can be calculated following the policy gradient theorem (Sutton et al., 2000;Konda and Tsitsiklis, 2000) as\n\u2207L \u03b8 (\u0177 t ) = Ty t=1 [\u2207 log p \u03b8 (\u0177 t |s t )A \u03c6 (\u0177 t |s t )] .(3)\nIn standard actor-critic algorithms, the parameters of actor and the critic are updated online at each time step. The actor parameters \u03b8 are updated by sampling\u0177 t from p \u03b8 and performing a step in the opposite direction of the stochastic gradient of L \u03b8 (\u0177 t ); the critic parameters \u03c6 are updated by minimizing L \u03c6 (\u0177 t ), defined as the mean squared error of the reward estimator for sampled target word\u0177 t with respect to actual rewards (for more details see Nguyen et al. (2017)). In our experiments, we simulate user rewards by character F-score (chrF) values of partial translations.\n\n\nTriggering Human Feedback Requests by Actor Entropy\n\nBesides the idea of replacing human post-edits by human rewards, another key feature of our approach is to minimize the number of requests for human feedback. This is achieved by computing the uncertainty of the policy distribution as the average word-level entropyH of an n-word partial translation, defined as\nH(\u0177 1:n ) = 1 n n t=1 \u2212 v\u2208V p \u03b8 (v|s t ) log p \u03b8 (v|s t ) ,(4)\nwhere\u0177 1:n = {\u0177 1 ,\u0177 2 , . . . ,\u0177 n } is a sequence of n predicted tokens starting at the sentence beginning, V is the output vocabulary, and p \u03b8 (v|s t ) is the probability of predicting a word in V at state s t of the RNN decoder.\n\nA request for human feedback is triggered when H(\u0177 1:n ) is higher than a running average \u03b3 by a factor of or when <eos> is generated. Upon receiving a reward from the user, both actor and critic are updated. Hence, our algorithm takes the middle ground between updating at each time step t and performing an update only after a reward signal for the completed translation is received. In our simulation experiments, this process is repeated until the <eos> token is generated, or when a pre-defined maximum length, here T max = 50, is reached.\n\n\nSimulating Human Rewards on Translation Quality\n\nPrevious work on reinforcement learning in machine translation has simulated human bandit feedback by evaluating full-sentence translations against references using per-sentence approximations of BLEU (Sokolov et al., 2016;Kreutzer et al., 2017;Nguyen et al., 2017). We found that when working with partial translations, user feedback on translation quality can successfully be simulated by computing the chrF-score (Popovi\u0107, 2015) of the translation with respect to the reference translation truncated to the same length. If the length of the translation exceeds the length of the reference, no truncation is used. We denote rewards as a function R(\u0177 1:t ) of only the partial translation\u0177 1:t , in order to highlight the fact that rewards are in principle independent of reference translations.\n\n\nSampling versus Forced Decoding via Prefix Buffer \u039e\n\nThe standard approach to estimate the expected reward in policy gradient techniques is to employ Monte-Carlo methods, in specific, multinomial sampling of actions. This guarantees an unbiased estimator and allows sufficient exploration of the action space in learning. In contrast, interactivepredictive machine translation usually avoids exploration in favor of exploitation by decoding the best partial translation under the current model after every interaction. Since in our framework, learning and decoding are interleaved, we have to find the best compromise between exploration and exploitation.\n\nThe general modus operandi of our framework is simultaneous exploration and exploitation by multinomial sampling actions from the current policy. However, in cases where a partial translation receives a high user reward, we store it in a so-called prefix buffer \u039e, and perform forced decoding by feeding the prefix to the decoder for the remaining translation process.\n\n\nAlgorithm for Bandit\n\nInteractive-Predictive NMT Algorithm 1 gives pseudo-code for Bandit-Interactive-Predictive Neural Machine Translation (BIP-NMT). The algorithm receives an input source sequence x i (line 4), and incrementally predicts a sequence of output target tokens up to length T max (line 6). At each step t, a partial translation\u0177 1:t is sampled from the policy distribution p \u03b8 (\u00b7|x i , y <t , \u039e) that implements an RNN encoder-decoder with an additional prefix buffer \u039e for forced decoding (line 7). User feedback is requested in case the average entrop\u0233 H(\u0177 1:t ) of the policy is larger than or equal to a running average by a factor of or when <eos> is generated (line 9). If the reward R(\u0177 1:t ) is larger than or equal to a threshold \u00b5, the prefix is stored in a buffer for forced decoding (lines 11-12). Next, for t = 1 . . . T max do 7:\nSample\u0177 1:t \u223c p \u03b8 t\u22121 (\u00b7|x i , y <t , \u039e) 8:\nComputeH(\u0177 1:t ) using Eq. (4) 9:\n\nifH(\u0177 1:t ) \u2212 \u03b3 t\u22121 \u2265 \u00d7 \u03b3 t\u22121 or <eos> in\u0177 1:t then  (7) Figure 1 visualizes the interaction of the BIP-NMT system with a human for a single translation: Feedback is requested when the model is uncertain or the translation is completed. It is directly used for a model update and, in case it was good, for filling the prefix buffer, before the model moves to generating the next (longer) partial translation.\nUpdate \u03b8 t \u2190 \u03b8 t\u22121 \u2212 \u03b1 A \u2207L \u03b8 t\u22121 (\u0177 t ) (Eq. (3)) 15: Update \u03c6 t \u2190 \u03c6 t\u22121 \u2212 \u03b1 C \u2207L \u03c6 t\u22121 (\u0177 t ) (see Eq.\n\nExperiments\n\nWe simulate a scenario where the learning NMT system requests online bandit feedback for partial translations from a human in the loop. The following experiments will give an initial practical assessment of our proposed interactive learning algorithm. Our analysis of the interactions between actor, critic and simulated human will provide further insights into the learning behavior of BIP-NMT.\n\n\nSetup\n\nData and Preprocessing. We conduct experiments on French-to-English translation on Eu- \n\n\nroparl (EP) and News Commentary (NC) domains.\n\nThe large EP parallel corpus is used to pre-train the actor in a fully-supervised setting with a standard maximum likelihood estimation objective. The critic network is not pre-trained. For interactive training with bandit feedback, we extract 10k sentences from the NC corpus. Validation and test sets are also chosen from the NC domain. Note that in principle more sentences could be used, however, we would like to simulate a realistic scenario where human feedback is costly to obtain. Data sets were tokenized and cleaned using Moses tools (Koehn et al., 2007). Furthermore, sentences longer than 50 tokens were removed from the training data. Each language's vocabulary contains the 50K most frequent tokens extracted from the two training sets. Table 1 summarizes the data statistics.\n\nModel Configuration and Training. Following Nguyen et al. (2017), we employ an architecture of two independent but similar encoder-decoder frameworks for actor and critic, respectively, each using global-attention (Luong et al., 2015) and unidirectional single-layer LSTMs 1 . Both the size of word embedding and LSTM's hidden cells are 500. We used the Adam Optimizer (Kingma and Ba, 2015) with \u03b2 1 = 0.9 and \u03b2 2 = 0.999. During supervised pre-training, we train with minibatches of size 64, and set Adam's \u03b1 = 10 \u22123 . A decay factor of 0.5 is applied to \u03b1, starting from the fifth pass, when perplexity on the validation set increases. During interactive training with bandit feedback, we perform true online updates (i.e. mini-batch size is 1) with Adam's \u03b1 hyperparameter kept constant at 10 \u22125 for both the actor and the critic. In addition, we clip the Euclidean norm of gradients to 5 in all training cases.\n\nBaselines and Evaluation. Our supervised outof-domain baseline consists of the actor NMT system described as above, pre-trained on Europarl, with optimal hyperparameters chosen according to corpus-level BLEU on the validation set. Starting from this pre-trained EP-domain model, we further train a bandit learning baseline by employing Nguyen's (2017) actor-critic model, trained on one epoch of sentence-level simulated feedback. The choice of comparing models after one epoch of training is a realistic simulation of a human-system interaction on a sequence of data where each input is seen only once. The feedback signal is simulated with chrF, using character-n-grams of length 6 and a value of \u03b2 = 2 of the importance factor of recall over precision. While during training exploration through sampling is essential, during inference and for final model evaluation we use greedy decoding. We evaluate the trained models on our test set from the NC-domain using average sentence-level chrF and standard corpus-level BLEU (Papineni et al., 2002) to measure how well they got adapted to the new domain. Table 2 shows the results of an evaluation of a baseline NMT model pre-trained by maximum likelihood on out-of-domain data. This is compared to an actor-critic baseline that trains the model of Nguyen et al. (2017) on sentence-level in-domain bandit feedback for one epoch. This approach can already improve chrF (+0.95) and BLEU (+0.55) significantly by seeing bandit feedback on in-domain data. BIP-NMT, with optimal hyperparameters = 0.75, \u00b5 = 0.8 chosen on the validation set, is trained in a similar way for one epoch, however, with the difference that even weaker sub-sentence level bandit feedback is provided on average 5 times per input. We see that BIP-NMT significantly improves both BLEU (+2.18) and chrF (+2.04) by even larger margins. Table 3 analyzes the impact of the metaparameter of the BIP-NMT algorithm. We run each experiment three times and report mean results and standard deviation. controls the margin by which the average word-level entropy needs to increase with respect to the running average in order to trigger a feedback request. Increasing this margin from 0 to 0.25, 0.5 and 0.75 corresponds to decreasing the number of feedback requests by a factor of 3 from around 16 to around 5. This reduction corresponds to a small increase in chrF (+0.29) and a small decrease in BLEU (-0.47). Figure 2 shows another effect of the metaparameter : It shows the variation of the average wordlevel entropyH over time steps of the algorithm during one epoch of training. This is computed as a cumulative average, i.e., the value ofH is accumulated and averaged over the number of target tokens produced for all inputs seen so far. We see that average cumulative entropy increases in the beginning of the training, but then decreases rapidly, with faster rates for smaller values of , corresponding to more updates per input.\n\n\nResults and Analysis\n\nThe metaparameter \u00b5 controls the threshold of the reward value that triggers a reuse of the prefix for forced decoding. In our experiments, we set this parameter to a value of 0.8 in order to avoid retranslations of already validated prefixes, even if they might sometimes lead to better final full translations. We found the effect of lowering \u00b5 from 1.0 to 0.8 negligible on the number of feedback requests and on translation quality but beneficial for the usability.    Prefixes that receive a feedback \u2265 \u00b5 and are thus stored in the buffer and re-used for later samples are indicated by underlines. Advantage scores < 0 indicate a discouragement of individual tokens and are highlighted in red.\n\n\nExample Protocols\n\nIn the first example, the model makes frequent feedback requests (in 8 of 17 decoding steps) and fills the prefix buffer due to the high quality of the samples. The second example can use the prefix buffer only for the first two tokens since the feedback varies quite a bit for subsequent partial translations. Note how the token-based critic encourages a few phrases of the translations, but discourages others. The final example shows a translation where the model is very certain and hence requests feedback only after the first and last token (minimum number of feedback requests). The critic correctly identifies problematic parts of the translation regarding the choice of prepositions.\n\n\nConclusion\n\nWe presented a novel algorithm, coined BIP-NMT, for bandit interactive-predictive NMT using reinforcement learning techniques. Our algorithm builds on advantage actor-critic learning (Mnih et al., 2016;Nguyen et al., 2017) for an interactive translation process with a human in the loop. The advantage over previously presented algorithms for interactive-predictive NMT is the low human effort for producing feedback (a translation quality judgment instead of a correction of a translatioin), even further reduced by an active learning strategy to request feedback only for situations where the actor is uncertain.\n\nWe showcased the success of BIP-NMT with simulated feedback, with the aim of moving to real human feedback in future work. Before deploying this algorithm in the wild, suitable interfaces for giving real-valued feedback have to be explored to create a pleasant user experience. Furthermore, in order to increase the level of human control, a combination with the standard paradigm that allows user edits might be considered in future work.\n\nFinally, our algorithm is in principle not limited to the application of NMT, but can furthermorethanks to the broad adoption of neural sequenceto-sequence learning in NLP -be extended to other structured prediction or sequence generation tasks.\n\n\nInput: \u03b8 0 , \u03c6 0 , \u03b1 A , \u03b1 C 2: Output: Estimates \u03b8 * , \u03c6 * 3: for i = 1, . . .\n\n\n20: end for updates of the parameters of the policy (line 14), critic (line 15), and average entropy (line 17) are performed. Actor and critic each use a separate learning rate schedule (\u03b1 A and \u03b1 C ).\n\nFigure 2 :\n2Average cumulative entropy during one epoch of BIP-NMT training with \u00b5 = 0.8 and = {0, 0.25, 0.5, 0.75}.\n\n\nFigure 1: Interaction of the NMT system with the human during learning for a single translation.START \n\nPredict \npartial \ntranslation \n\nRequest \nfeedback? \n\nUpdate \nparameters \n\nGood \nprefix? \n\nPrefix \nBuffer \n\nNo \n\nYes \nYes \n\nNMT \n\nSTOP \nEOS? \nYes \n\nNo \n\nDataset \nEP (v.5)n \nNC (WMT07)n \n\nTraining (filt.) 1,346,679 23.5 \n9,216 \n21.9 \nValidation \n2,000 \n29.4 \n1,064 \n24.1 \nTest \n-\n-\n2,007 \n24.8 \n\nTable 1: Number of parallel sentences and average number of \nwords per sentence in target language (en), denoted byn, for \ntraining (filtered to a maximum length of 50), validation and \ntest sets for French-to-English translation for Europarl (EP) \nand News Commentary (NC) domains. \n\n\n\n\nSystem chrF (std) BLEU (std) \u2206 chrF \u2206 BLEU BIP-NMT ( = 0.75, \u00b5 = 0.8) 63.34 (0.12) 26.95 (0.12) +2.04 +2.18Out-of-domain NMT \n61.30 \n24.77 \n0 \n0 \nNguyen et al. (2017) \n62.25 (0.08) 25.32 (0.02) \n+0.95 \n+0.55 \n\n\nTable 2 :\n2Evaluation of pre-trained out-of-domain baseline model, actor-critic learning on one epoch of sentence-level in-domain bandit feedback(Nguyen et al., 2017) and BIP-NMT with settings = 0.75, \u00b5 = 0.8 trained on one epoch of sub-sentence level in-domain bandit feedback. Results are given on the NC test set according to average sentence-level chrF and corpus-level BLEU. Result differences between all pairs of systems are statistically significant according to multeval(Clark et al., 2011). Avg # Requests \u2206 chrF \u2206 BLEU \u2206 Avg # RequestschrF (std) \nBLEU (std) 0 \n61.86 (0.06) 25.54 (0.17) \n15.91 (0.01) \n0 \n0 \n0 \n0.25 62.15 (0.17) 25.84 (0.13) \n11.06 (0.07) \n+0.29 \n+0.3 \n-5 \n0.5 \n61.95 (0.05) 25.46 (0.09) \n7.26 (0.03) \n+0.09 \n-0.08 \n-9 \n0.75 62.15 (0.04) 25.07 (0.12) \n4.94 (0.02) \n+0.29 \n-0.47 \n-11 \n\n\n\nTable 3 :\n3Impact of entropy margin on average sentence-level chrF score, corpus BLEU and average number of feedback requests per sentence on the NC validation set. The feedback quality threshold \u00b5 is set to 0.8 for all models.\n\nTable 4\n4presents user-interaction protocols for three examples encountered during training of BIP-NMT with = 0.75, \u00b5 = 0.8. For illustrative purposes, we chose examples that differ with respect to the number of feedback requests, the use of the prefix buffer, and the feedback values.\nOur code can be accessed via the link https://github. com/heidelkin/BIPNMT.\nAcknowledgments.This work was supported in part by DFG Research Grant RI 2221/4-1. We would like to thank the members of the Statistical NLP Colloquium Heidelberg for fruitful discussions and ideas for improvement of our algorithm.Table 4: Interaction protocol for three translations. These translations were sampled from the model when the algorithm decided to request human feedback (line 10 in Algorithm 1). Tokens that get an overall negative reward (in combination with the critic), are marked in red, the remaining tokens receive a positive reward. When a prefix is good (i.e. \u2265 \u00b5, here \u00b5 = 0.8) it is stored in the buffer and used for forced decoding for later samples (underlined).REF the answer that we as individuals accept is that we are free because we rule ourselves in common , rather than being ruled by some agency that need not take account of us . < /s> Partial sampled translation Feedback the 1 the answer 1 the answer we 0.6964 the answer we , 0.6246 the answer we as individuals allow to 14 are 0.6008 the answer we , as individuals , go down to speak 8 , are being free because we govern ourselves , rather from being based together 0.5155 the answer we , as people , accepts is that we principle are free because we govern ourselves , rather than being led by a organisation which has absolutely no need to take our standards . < /s> 0.5722 SRC lors d' un rallye \"journ\u00e9e j\u00e9rusalem\" tenu\u00e0 l' universit\u00e9 de t\u00e9h\u00e9ran en d\u00e9cembre 2001 , il a prononc\u00e9 l' une des menaces les plus sinistres du r\u00e9gime . REF at a jerusalem day rally at tehran university in december 2001 , he uttered one of the regime 's most sinister threats . < /s> Partial sampled translation Feedback in 0 in a round of jerusalem called a academic university in teheran in december 2001 , he declared one in the most recent hostility to the regime . < /s> 0.5903\nNeural machine translation by jointly learning to align and translate. Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)San Diego, CABahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Ben- gio. 2015. Neural machine translation by jointly learning to align and translate. In Proceedings of the International Conference on Learning Represen- tations (ICLR), San Diego, CA.\n\nAn actor-critic algorithm for sequence prediction. Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, Yoshua Bengio, Proceedings of the 5th International Conference on Learning Representations (ICLR). the 5th International Conference on Learning Representations (ICLR)Toulon, FranceBahdanau, Dzmitry, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. 2017. An actor-critic algorithm for sequence prediction. In Proceedings of the 5th International Conference on Learning Repre- sentations (ICLR), Toulon, France.\n\nStatistical approaches to computer-assisted translation. Sergio Barrachina, Oliver Bender, Francisco Casacuberta, Jorge Civera, Elsa Cubel, Shahram Khadivi, Antonio Lagarda, Hermann Ney, Jes\u00fas Tom\u00e1s, Enrique Vidal, Juan-Miguel Vilar, Computational Linguistics. 351Barrachina, Sergio, Oliver Bender, Francisco Casacu- berta, Jorge Civera, Elsa Cubel, Shahram Khadivi, Antonio Lagarda, Hermann Ney, Jes\u00fas Tom\u00e1s, En- rique Vidal, and Juan-Miguel Vilar. 2008. Statistical approaches to computer-assisted translation. Com- putational Linguistics, 35(1):3-28.\n\nOn the evaluation of adaptive machine translation for human post-editing. Luisa Bentivogli, Nicola Bertoldi, Mauro Cettolo, Marcello Federico, Matteo Negri, Marco Turchi, IEEE Transactions on Audio, Speech and Language Processing (TASLP)). 242Bentivogli, Luisa, Nicola Bertoldi, Mauro Cettolo, Marcello Federico, Matteo Negri, and Marco Turchi. 2016. On the evaluation of adaptive machine trans- lation for human post-editing. IEEE Transactions on Audio, Speech and Language Processing (TASLP)), 24(2):388-399.\n\nAn empirical study of the behavior of active learning for word sense disambiguation. Jinying Chen, Andrew Schein, Lyle Ungar, Martha Palmer, Human Language Technologies: The 2006 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT). New York City, NYChen, Jinying, Andrew Schein, Lyle Ungar, and Martha Palmer. 2006. An empirical study of the behavior of active learning for word sense disambiguation. In Human Language Technologies: The 2006 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL- HLT), New York City, NY.\n\nBetter hypothesis testing for statistical machine translation: Controlling for optimizer instability. Jonathan Clark, Chris Dyer, Alon Lavie, Noah Smith, Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL'11). the 49th Annual Meeting of the Association for Computational Linguistics (ACL'11)Portland, ORClark, Jonathan, Chris Dyer, Alon Lavie, and Noah Smith. 2011. Better hypothesis testing for statis- tical machine translation: Controlling for optimizer instability. In Proceedings of the 49th Annual Meet- ing of the Association for Computational Linguistics (ACL'11), Portland, OR.\n\nSegment-based interactive-predictive machine translation. Machine Translation. Miguel Domingo, \u00c1lvaro Peris, Francisco Casacuberta, Domingo, Miguel,\u00c1lvaro Peris, and Francisco Casacu- berta. 2018. Segment-based interactive-predictive machine translation. Machine Translation.\n\nTarget-text mediated interactive machine translation. George Foster, Pierre Isabelle, Pierre Plamondon, Machine Translation. 121-2Foster, George, Pierre Isabelle, and Pierre Plamon- don. 1997. Target-text mediated interactive machine translation. Machine Translation, 12(1-2):175-194.\n\nAn active learning scenario for interactive machine translation. Jes\u00fas Gonz\u00e1lez-Rubio, Daniel Ortiz-Mart\u00ednez, Francisco Casacuberta, Proceedings of the 13th International Conference on Multimodal Interfaces (ICMI). the 13th International Conference on Multimodal Interfaces (ICMI)Barcelona, SpainGonz\u00e1lez-Rubio, Jes\u00fas, Daniel Ortiz-Mart\u00ednez, and Francisco Casacuberta. 2011. An active learning scenario for interactive machine translation. In Pro- ceedings of the 13th International Conference on Multimodal Interfaces (ICMI), Barcelona, Spain.\n\nActive learning for interactive machine translation. Jes\u00fas Gonz\u00e1lez-Rubio, Daniel Ortiz-Mart\u00ednez, Francisco Casacuberta, Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL). the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL)Avignon, FranceGonz\u00e1lez-Rubio, Jes\u00fas, Daniel Ortiz-Mart\u00ednez, and Francisco Casacuberta. 2012. Active learning for interactive machine translation. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics (EACL), Avignon, France.\n\nHuman effort and machine learnability in computer aided translation. Green, Sida I Spence, Jason Wang, Jeffrey Chuang, Sebastian Heer, Christopher D Schuster, Manning, Proceedings the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language ProcessingDoha, QatarGreen, Spence, Sida I. Wang, Jason Chuang, Jeffrey Heer, Sebastian Schuster, and Christopher D. Man- ning. 2014. Human effort and machine learnability in computer aided translation. In Proceedings the Conference on Empirical Methods in Natural Lan- guage Processing (EMNLP), Doha, Qatar.\n\nReinforcement learning via practice and critique advice. Kshitij Judah, Saikat Roy, Alan Fern, Thomas G Dietterich, Proceedings of the 24th AAAI Conference on Artificial Intelligence. the 24th AAAI Conference on Artificial IntelligenceAtlanta, GAJudah, Kshitij, Saikat Roy, Alan Fern, and Thomas G. Dietterich. 2010. Reinforcement learning via prac- tice and critique advice. In Proceedings of the 24th AAAI Conference on Artificial Intelligence, Atlanta, GA.\n\nA user-study on online adaptation of neural machine translation to human post-edits. Sariya Karimova, Patrick Simianer, Stefan Riezler, abs/1712.04853CoRRKarimova, Sariya, Patrick Simianer, and Stefan Riezler. 2017. A user-study on online adaptation of neu- ral machine translation to human post-edits. CoRR, abs/1712.04853.\n\nAdam: A method for stochastic optimization. Diederik P Kingma, Jimmy Ba, Proceedings of the International Conference on Learning Representations (ICLR). the International Conference on Learning Representations (ICLR)San Diego, CAKingma, Diederik P. and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proceedings of the International Conference on Learning Repre- sentations (ICLR), San Diego, CA.\n\nNeural interactive translation prediction. Rebecca Knowles, Philipp Koehn, Proceedings of the Conference of the Association for Machine Translation in the Americas (AMTA). the Conference of the Association for Machine Translation in the Americas (AMTA)Austin, TXKnowles, Rebecca and Philipp Koehn. 2016. Neu- ral interactive translation prediction. In Proceedings of the Conference of the Association for Machine Translation in the Americas (AMTA), Austin, TX.\n\nMoses: Open source toolkit for statistical machine translation. Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Birch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, Evan Herbst, Proceedings of the ACL 2007 Demo and Poster Sessions. the ACL 2007 Demo and Poster SessionsPrague, Czech RepublicKoehn, Philipp, Hieu Hoang, Alexandra Birch, Chris Callison-Birch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the ACL 2007 Demo and Poster Ses- sions, Prague, Czech Republic.\n\nActorcritic algorithms. Vijay R Konda, John N Tsitsiklis, Advances in Neural Information Processing Systems (NIPS). Vancouver, CanadaKonda, Vijay R. and John N. Tsitsiklis. 2000. Actor- critic algorithms. In Advances in Neural Information Processing Systems (NIPS), Vancouver, Canada.\n\nBandit structured prediction for neural sequence-to-sequence learning. Julia Kreutzer, Artem Sokolov, Stefan Riezler, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL). the 55th Annual Meeting of the Association for Computational Linguistics (ACL)Vancouver, CanadaKreutzer, Julia, Artem Sokolov, and Stefan Riezler. 2017. Bandit structured prediction for neural sequence-to-sequence learning. In Proceedings of the 55th Annual Meeting of the Association for Com- putational Linguistics (ACL), Vancouver, Canada.\n\nA sequential algorithm for training text classifiers. David D Lewis, A William, Gale, Proceedings of the 17th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval (SIGIR). the 17th Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval (SIGIR)Dublin, IrelandLewis, David D and William A Gale. 1994. A se- quential algorithm for training text classifiers. In Proceedings of the 17th Annual International ACM- SIGIR Conference on Research and Development in Information Retrieval (SIGIR), Dublin, Ireland.\n\nEffective approaches to attention-based neural machine translation. Thang Luong, Hieu Pham, Christopher D Manning, Proceedings of the Conference on Empirical Methods in Natural Language Processing. the Conference on Empirical Methods in Natural Language ProcessingLisbon, PortugalLuong, Thang, Hieu Pham, and Christopher D. Man- ning. 2015. Effective approaches to attention-based neural machine translation. In Proceedings of the Conference on Empirical Methods in Natural Lan- guage Processing (EMNLP), Lisbon, Portugal.\n\nTouchbased pre-post-editing of machine translation output. Benjamin Marie, Aur\u00e9lien Max, Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). the Conference on Empirical Methods in Natural Language Processing (EMNLP)Lisbon, PortugalMarie, Benjamin and Aur\u00e9lien Max. 2015. Touch- based pre-post-editing of machine translation out- put. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Lisbon, Portugal.\n\nAsynchronous methods for deep reinforcement learning. Mnih, Adri\u00e0 Volodymyr, Mehdi Puigdom\u00e8nech Badia, Alex Mirza, Timothy P Graves, Tim Lillicrap, David Harley, Koray Silver, Kavukcuoglu, Proceedings of the 33rd International Conference on Machine Learning (ICML). the 33rd International Conference on Machine Learning (ICML)New York, NYMnih, Volodymyr, Adri\u00e0 Puigdom\u00e8nech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. Asynchronous methods for deep reinforce- ment learning. In Proceedings of the 33rd Inter- national Conference on Machine Learning (ICML), New York, NY.\n\nReinforcement learning for bandit neural machine translation with simulated feedback. Khanh Nguyen, Hal Daum\u00e9, Jordan Boyd-Graber, Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP). the Conference on Empirical Methods in Natural Language Processing (EMNLP)Copenhagen, DenmarkNguyen, Khanh, Hal Daum\u00e9, and Jordan Boyd-Graber. 2017. Reinforcement learning for bandit neural ma- chine translation with simulated feedback. In Pro- ceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Copen- hagen, Denmark.\n\nOnline learning for interactive statistical machine translation. Ortiz-Mart\u00ednez, Ismael Daniel, Francisco Garc\u00eda-Varea, Casacuberta, Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (NAACL-HLT). Los Angeles, CAOrtiz-Mart\u00ednez, Daniel, Ismael Garc\u00eda-Varea, and Fran- cisco Casacuberta. 2010. Online learning for in- teractive statistical machine translation. In Human Language Technologies: The 2010 Annual Confer- ence of the North American Chapter of the Associ- ation for Computational Linguistics (NAACL-HLT), Los Angeles, CA.\n\nBleu: a method for automatic evaluation of machine translation. Kishore Papineni, Salim Roukos, Todd Ward, Wei-Jing Zhu, Proceedings of the 40th Annual Meeting on Association for Computational Linguistics (ACL). the 40th Annual Meeting on Association for Computational Linguistics (ACL)Philadelphia, PAPapineni, Kishore, Salim Roukos, Todd Ward, and Wei- Jing Zhu. 2002. Bleu: a method for automatic eval- uation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computa- tional Linguistics (ACL), Philadelphia, PA.\n\nchrF: character n-gram f-score for automatic mt evaluation. Maja Popovi\u0107, Proceedings of the Tenth Workshop on Statistical Machine Translat ion (WMT). the Tenth Workshop on Statistical Machine Translat ion (WMT)Lisbon, PortugalPopovi\u0107, Maja. 2015. chrF: character n-gram f-score for automatic mt evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translat ion (WMT), Lisbon, Portugal.\n\nStochastic structured prediction under bandit feedback. Artem Sokolov, Julia Kreutzer, Christopher Lo, Stefan Riezler, Advances in Neural Information Processing Systems (NIPS). Barcelona, SpainSokolov, Artem, Julia Kreutzer, Christopher Lo, and Stefan Riezler. 2016. Stochastic structured predic- tion under bandit feedback. In Advances in Neural Information Processing Systems (NIPS), Barcelona, Spain.\n\nReinforcement Learning. An Introduction. Richard S Sutton, Andrew G Barto, The MIT Presssecond editionSutton, Richard S. and Andrew G. Barto. 2017. Re- inforcement Learning. An Introduction. The MIT Press, second edition.\n\nPolicy gradient methods for reinforcement learning with function approximation. Richard S Sutton, David Mcallester, Satinder Singh, Yishay Mansour, Advances in Neural Information Processings Systems (NIPS). Vancouver, CanadaSutton, Richard S., David McAllester, Satinder Singh, and Yishay Mansour. 2000. Policy gradient methods for reinforcement learning with function approxima- tion. In Advances in Neural Information Processings Systems (NIPS), Vancouver, Canada.\n\nActive learning for statistical natural language parsing. Min Tang, Xiaoqiang Luo, Salim Roukos, Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL). the 40th Annual Meeting of the Association for Computational Linguistics (ACL)Pennsylvania, PATang, Min, Xiaoqiang Luo, and Salim Roukos. 2002. Active learning for statistical natural language pars- ing. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), Pennsylvania, PA.\n\nSimple statistical gradientfollowing algorithms for connectionist reinforcement learning. Ronald J Williams, Machine Learning. 8Williams, Ronald J. 1992. Simple statistical gradient- following algorithms for connectionist reinforce- ment learning. Machine Learning, 8:229-256.\n\nModels and inference for prefix-constrained machine translation. Joern Wuebker, Spence Green, John Denero, Sasa Hasan, Minh-Thang Luong, Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL). the 54th Annual Meeting of the Association for Computational Linguistics (ACL)Berlin, GermanyWuebker, Joern, Spence Green, John DeNero, Sasa Hasan, and Minh-Thang Luong. 2016. Models and inference for prefix-constrained machine trans- lation. In Proceedings of the 54th Annual Meet- ing of the Association for Computational Linguistics (ACL), Berlin, Germany.\n", "annotations": {"author": "[{\"end\":192,\"start\":90},{\"end\":303,\"start\":193},{\"end\":413,\"start\":304}]", "publisher": null, "author_last_name": "[{\"end\":101,\"start\":98},{\"end\":207,\"start\":199},{\"end\":318,\"start\":311}]", "author_first_name": "[{\"end\":93,\"start\":90},{\"end\":97,\"start\":94},{\"end\":198,\"start\":193},{\"end\":310,\"start\":304}]", "author_affiliation": "[{\"end\":191,\"start\":128},{\"end\":302,\"start\":239},{\"end\":412,\"start\":349}]", "title": "[{\"end\":87,\"start\":1},{\"end\":500,\"start\":414}]", "venue": null, "abstract": "[{\"end\":1344,\"start\":502}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1676,\"start\":1653},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1776,\"start\":1751},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":1797,\"start\":1776},{\"end\":1840,\"start\":1825},{\"end\":1972,\"start\":1955},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2386,\"start\":2361},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2425,\"start\":2402},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2721,\"start\":2697},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2781,\"start\":2759},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2803,\"start\":2781},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2823,\"start\":2803},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4964,\"start\":4949},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5592,\"start\":5571},{\"end\":5594,\"start\":5592},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5657,\"start\":5632},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5676,\"start\":5657},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5702,\"start\":5678},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5728,\"start\":5707},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7048,\"start\":7027},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7069,\"start\":7048},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7811,\"start\":7792},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8133,\"start\":8111},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8148,\"start\":8135},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8175,\"start\":8153},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8260,\"start\":8244},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8312,\"start\":8284},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8332,\"start\":8312},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":8350,\"start\":8332},{\"end\":8396,\"start\":8372},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":8574,\"start\":8552},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8599,\"start\":8579},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8976,\"start\":8948},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8981,\"start\":8976},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":9293,\"start\":9264},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9690,\"start\":9669},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":9826,\"start\":9804},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9873,\"start\":9854},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9917,\"start\":9898},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11091,\"start\":11068},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":11917,\"start\":11899},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12150,\"start\":12130},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12448,\"start\":12427},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":12475,\"start\":12448},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":13027,\"start\":13007},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14618,\"start\":14596},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":14640,\"start\":14618},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":14660,\"start\":14640},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":14826,\"start\":14811},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18794,\"start\":18774},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":19086,\"start\":19066},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":19256,\"start\":19236},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":20289,\"start\":20274},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":20985,\"start\":20962},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":21256,\"start\":21236},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":24539,\"start\":24520},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":24559,\"start\":24539},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":27109,\"start\":27088},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":27442,\"start\":27422}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":25721,\"start\":25640},{\"attributes\":{\"id\":\"fig_2\"},\"end\":25925,\"start\":25722},{\"attributes\":{\"id\":\"fig_3\"},\"end\":26043,\"start\":25926},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":26729,\"start\":26044},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":26941,\"start\":26730},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":27756,\"start\":26942},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":27985,\"start\":27757},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":28272,\"start\":27986}]", "paragraph": "[{\"end\":2426,\"start\":1360},{\"end\":2964,\"start\":2428},{\"end\":3449,\"start\":2966},{\"end\":3849,\"start\":3451},{\"end\":4491,\"start\":3851},{\"end\":5460,\"start\":4493},{\"end\":6945,\"start\":5477},{\"end\":7642,\"start\":6947},{\"end\":8848,\"start\":7644},{\"end\":10414,\"start\":8850},{\"end\":10645,\"start\":10480},{\"end\":11097,\"start\":10693},{\"end\":12226,\"start\":11141},{\"end\":12478,\"start\":12285},{\"end\":13134,\"start\":12544},{\"end\":13501,\"start\":13190},{\"end\":13797,\"start\":13565},{\"end\":14343,\"start\":13799},{\"end\":15191,\"start\":14395},{\"end\":15849,\"start\":15247},{\"end\":16219,\"start\":15851},{\"end\":17079,\"start\":16244},{\"end\":17157,\"start\":17124},{\"end\":17567,\"start\":17159},{\"end\":18082,\"start\":17687},{\"end\":18179,\"start\":18092},{\"end\":19020,\"start\":18229},{\"end\":19936,\"start\":19022},{\"end\":22885,\"start\":19938},{\"end\":23608,\"start\":22910},{\"end\":24322,\"start\":23630},{\"end\":24951,\"start\":24337},{\"end\":25392,\"start\":24953},{\"end\":25639,\"start\":25394}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11140,\"start\":11098},{\"attributes\":{\"id\":\"formula_1\"},\"end\":12284,\"start\":12227},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12543,\"start\":12479},{\"attributes\":{\"id\":\"formula_3\"},\"end\":13564,\"start\":13502},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17123,\"start\":17080},{\"attributes\":{\"id\":\"formula_5\"},\"end\":17672,\"start\":17568}]", "table_ref": "[{\"end\":18988,\"start\":18981},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":21049,\"start\":21042},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":21798,\"start\":21791}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1358,\"start\":1346},{\"attributes\":{\"n\":\"2\"},\"end\":5475,\"start\":5463},{\"attributes\":{\"n\":\"3\"},\"end\":10478,\"start\":10417},{\"attributes\":{\"n\":\"3.1\"},\"end\":10691,\"start\":10648},{\"attributes\":{\"n\":\"3.2\"},\"end\":13188,\"start\":13137},{\"attributes\":{\"n\":\"3.3\"},\"end\":14393,\"start\":14346},{\"attributes\":{\"n\":\"3.4\"},\"end\":15245,\"start\":15194},{\"attributes\":{\"n\":\"3.5\"},\"end\":16242,\"start\":16222},{\"attributes\":{\"n\":\"4\"},\"end\":17685,\"start\":17674},{\"attributes\":{\"n\":\"4.1\"},\"end\":18090,\"start\":18085},{\"end\":18227,\"start\":18182},{\"attributes\":{\"n\":\"4.2\"},\"end\":22908,\"start\":22888},{\"attributes\":{\"n\":\"4.3\"},\"end\":23628,\"start\":23611},{\"attributes\":{\"n\":\"5\"},\"end\":24335,\"start\":24325},{\"end\":25937,\"start\":25927},{\"end\":26952,\"start\":26943},{\"end\":27767,\"start\":27758},{\"end\":27994,\"start\":27987}]", "table": "[{\"end\":26729,\"start\":26142},{\"end\":26941,\"start\":26839},{\"end\":27756,\"start\":27489}]", "figure_caption": "[{\"end\":25721,\"start\":25642},{\"end\":25925,\"start\":25724},{\"end\":26043,\"start\":25939},{\"end\":26142,\"start\":26046},{\"end\":26839,\"start\":26732},{\"end\":27489,\"start\":26954},{\"end\":27985,\"start\":27769},{\"end\":28272,\"start\":27996}]", "figure_ref": "[{\"end\":17224,\"start\":17216},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":22367,\"start\":22359}]", "bib_author_first_name": "[{\"end\":30278,\"start\":30271},{\"end\":30298,\"start\":30289},{\"end\":30310,\"start\":30304},{\"end\":30766,\"start\":30759},{\"end\":30785,\"start\":30777},{\"end\":30800,\"start\":30794},{\"end\":30812,\"start\":30805},{\"end\":30824,\"start\":30820},{\"end\":30837,\"start\":30831},{\"end\":30851,\"start\":30846},{\"end\":30869,\"start\":30863},{\"end\":31393,\"start\":31387},{\"end\":31412,\"start\":31406},{\"end\":31430,\"start\":31421},{\"end\":31449,\"start\":31444},{\"end\":31462,\"start\":31458},{\"end\":31477,\"start\":31470},{\"end\":31494,\"start\":31487},{\"end\":31511,\"start\":31504},{\"end\":31522,\"start\":31517},{\"end\":31537,\"start\":31530},{\"end\":31556,\"start\":31545},{\"end\":31964,\"start\":31959},{\"end\":31983,\"start\":31977},{\"end\":31999,\"start\":31994},{\"end\":32017,\"start\":32009},{\"end\":32034,\"start\":32028},{\"end\":32047,\"start\":32042},{\"end\":32489,\"start\":32482},{\"end\":32502,\"start\":32496},{\"end\":32515,\"start\":32511},{\"end\":32529,\"start\":32523},{\"end\":33137,\"start\":33129},{\"end\":33150,\"start\":33145},{\"end\":33161,\"start\":33157},{\"end\":33173,\"start\":33169},{\"end\":33742,\"start\":33736},{\"end\":33758,\"start\":33752},{\"end\":33775,\"start\":33766},{\"end\":33994,\"start\":33988},{\"end\":34009,\"start\":34003},{\"end\":34026,\"start\":34020},{\"end\":34290,\"start\":34285},{\"end\":34313,\"start\":34307},{\"end\":34339,\"start\":34330},{\"end\":34824,\"start\":34819},{\"end\":34847,\"start\":34841},{\"end\":34873,\"start\":34864},{\"end\":35466,\"start\":35462},{\"end\":35468,\"start\":35467},{\"end\":35482,\"start\":35477},{\"end\":35496,\"start\":35489},{\"end\":35514,\"start\":35505},{\"end\":35532,\"start\":35521},{\"end\":35534,\"start\":35533},{\"end\":36064,\"start\":36057},{\"end\":36078,\"start\":36072},{\"end\":36088,\"start\":36084},{\"end\":36101,\"start\":36095},{\"end\":36103,\"start\":36102},{\"end\":36552,\"start\":36546},{\"end\":36570,\"start\":36563},{\"end\":36587,\"start\":36581},{\"end\":36839,\"start\":36831},{\"end\":36841,\"start\":36840},{\"end\":36855,\"start\":36850},{\"end\":37251,\"start\":37244},{\"end\":37268,\"start\":37261},{\"end\":37734,\"start\":37727},{\"end\":37746,\"start\":37742},{\"end\":37763,\"start\":37754},{\"end\":37776,\"start\":37771},{\"end\":37801,\"start\":37793},{\"end\":37818,\"start\":37812},{\"end\":37835,\"start\":37829},{\"end\":37847,\"start\":37843},{\"end\":37863,\"start\":37854},{\"end\":37878,\"start\":37871},{\"end\":37890,\"start\":37885},{\"end\":37903,\"start\":37897},{\"end\":37920,\"start\":37911},{\"end\":37937,\"start\":37933},{\"end\":38466,\"start\":38461},{\"end\":38468,\"start\":38467},{\"end\":38480,\"start\":38476},{\"end\":38482,\"start\":38481},{\"end\":38799,\"start\":38794},{\"end\":38815,\"start\":38810},{\"end\":38831,\"start\":38825},{\"end\":39339,\"start\":39334},{\"end\":39341,\"start\":39340},{\"end\":39350,\"start\":39349},{\"end\":39940,\"start\":39935},{\"end\":39952,\"start\":39948},{\"end\":39970,\"start\":39959},{\"end\":39972,\"start\":39971},{\"end\":40458,\"start\":40450},{\"end\":40474,\"start\":40466},{\"end\":40942,\"start\":40937},{\"end\":40959,\"start\":40954},{\"end\":40984,\"start\":40980},{\"end\":40999,\"start\":40992},{\"end\":41001,\"start\":41000},{\"end\":41013,\"start\":41010},{\"end\":41030,\"start\":41025},{\"end\":41044,\"start\":41039},{\"end\":41605,\"start\":41600},{\"end\":41617,\"start\":41614},{\"end\":41631,\"start\":41625},{\"end\":42180,\"start\":42174},{\"end\":42198,\"start\":42189},{\"end\":42780,\"start\":42773},{\"end\":42796,\"start\":42791},{\"end\":42809,\"start\":42805},{\"end\":42824,\"start\":42816},{\"end\":43325,\"start\":43321},{\"end\":43729,\"start\":43724},{\"end\":43744,\"start\":43739},{\"end\":43766,\"start\":43755},{\"end\":43777,\"start\":43771},{\"end\":44121,\"start\":44114},{\"end\":44123,\"start\":44122},{\"end\":44138,\"start\":44132},{\"end\":44140,\"start\":44139},{\"end\":44383,\"start\":44376},{\"end\":44385,\"start\":44384},{\"end\":44399,\"start\":44394},{\"end\":44420,\"start\":44412},{\"end\":44434,\"start\":44428},{\"end\":44825,\"start\":44822},{\"end\":44841,\"start\":44832},{\"end\":44852,\"start\":44847},{\"end\":45373,\"start\":45367},{\"end\":45375,\"start\":45374},{\"end\":45625,\"start\":45620},{\"end\":45641,\"start\":45635},{\"end\":45653,\"start\":45649},{\"end\":45666,\"start\":45662},{\"end\":45684,\"start\":45674}]", "bib_author_last_name": "[{\"end\":30287,\"start\":30279},{\"end\":30302,\"start\":30299},{\"end\":30317,\"start\":30311},{\"end\":30775,\"start\":30767},{\"end\":30792,\"start\":30786},{\"end\":30803,\"start\":30801},{\"end\":30818,\"start\":30813},{\"end\":30829,\"start\":30825},{\"end\":30844,\"start\":30838},{\"end\":30861,\"start\":30852},{\"end\":30876,\"start\":30870},{\"end\":31404,\"start\":31394},{\"end\":31419,\"start\":31413},{\"end\":31442,\"start\":31431},{\"end\":31456,\"start\":31450},{\"end\":31468,\"start\":31463},{\"end\":31485,\"start\":31478},{\"end\":31502,\"start\":31495},{\"end\":31515,\"start\":31512},{\"end\":31528,\"start\":31523},{\"end\":31543,\"start\":31538},{\"end\":31562,\"start\":31557},{\"end\":31975,\"start\":31965},{\"end\":31992,\"start\":31984},{\"end\":32007,\"start\":32000},{\"end\":32026,\"start\":32018},{\"end\":32040,\"start\":32035},{\"end\":32054,\"start\":32048},{\"end\":32494,\"start\":32490},{\"end\":32509,\"start\":32503},{\"end\":32521,\"start\":32516},{\"end\":32536,\"start\":32530},{\"end\":33143,\"start\":33138},{\"end\":33155,\"start\":33151},{\"end\":33167,\"start\":33162},{\"end\":33179,\"start\":33174},{\"end\":33750,\"start\":33743},{\"end\":33764,\"start\":33759},{\"end\":33787,\"start\":33776},{\"end\":34001,\"start\":33995},{\"end\":34018,\"start\":34010},{\"end\":34036,\"start\":34027},{\"end\":34305,\"start\":34291},{\"end\":34328,\"start\":34314},{\"end\":34351,\"start\":34340},{\"end\":34839,\"start\":34825},{\"end\":34862,\"start\":34848},{\"end\":34885,\"start\":34874},{\"end\":35460,\"start\":35455},{\"end\":35475,\"start\":35469},{\"end\":35487,\"start\":35483},{\"end\":35503,\"start\":35497},{\"end\":35519,\"start\":35515},{\"end\":35543,\"start\":35535},{\"end\":35552,\"start\":35545},{\"end\":36070,\"start\":36065},{\"end\":36082,\"start\":36079},{\"end\":36093,\"start\":36089},{\"end\":36114,\"start\":36104},{\"end\":36561,\"start\":36553},{\"end\":36579,\"start\":36571},{\"end\":36595,\"start\":36588},{\"end\":36848,\"start\":36842},{\"end\":36858,\"start\":36856},{\"end\":37259,\"start\":37252},{\"end\":37274,\"start\":37269},{\"end\":37740,\"start\":37735},{\"end\":37752,\"start\":37747},{\"end\":37769,\"start\":37764},{\"end\":37791,\"start\":37777},{\"end\":37810,\"start\":37802},{\"end\":37827,\"start\":37819},{\"end\":37841,\"start\":37836},{\"end\":37852,\"start\":37848},{\"end\":37869,\"start\":37864},{\"end\":37883,\"start\":37879},{\"end\":37895,\"start\":37891},{\"end\":37909,\"start\":37904},{\"end\":37931,\"start\":37921},{\"end\":37944,\"start\":37938},{\"end\":38474,\"start\":38469},{\"end\":38493,\"start\":38483},{\"end\":38808,\"start\":38800},{\"end\":38823,\"start\":38816},{\"end\":38839,\"start\":38832},{\"end\":39347,\"start\":39342},{\"end\":39358,\"start\":39351},{\"end\":39364,\"start\":39360},{\"end\":39946,\"start\":39941},{\"end\":39957,\"start\":39953},{\"end\":39980,\"start\":39973},{\"end\":40464,\"start\":40459},{\"end\":40478,\"start\":40475},{\"end\":40935,\"start\":40931},{\"end\":40952,\"start\":40943},{\"end\":40978,\"start\":40960},{\"end\":40990,\"start\":40985},{\"end\":41008,\"start\":41002},{\"end\":41023,\"start\":41014},{\"end\":41037,\"start\":41031},{\"end\":41051,\"start\":41045},{\"end\":41064,\"start\":41053},{\"end\":41612,\"start\":41606},{\"end\":41623,\"start\":41618},{\"end\":41643,\"start\":41632},{\"end\":42172,\"start\":42158},{\"end\":42187,\"start\":42181},{\"end\":42211,\"start\":42199},{\"end\":42224,\"start\":42213},{\"end\":42789,\"start\":42781},{\"end\":42803,\"start\":42797},{\"end\":42814,\"start\":42810},{\"end\":42828,\"start\":42825},{\"end\":43333,\"start\":43326},{\"end\":43737,\"start\":43730},{\"end\":43753,\"start\":43745},{\"end\":43769,\"start\":43767},{\"end\":43785,\"start\":43778},{\"end\":44130,\"start\":44124},{\"end\":44146,\"start\":44141},{\"end\":44392,\"start\":44386},{\"end\":44410,\"start\":44400},{\"end\":44426,\"start\":44421},{\"end\":44442,\"start\":44435},{\"end\":44830,\"start\":44826},{\"end\":44845,\"start\":44842},{\"end\":44859,\"start\":44853},{\"end\":45384,\"start\":45376},{\"end\":45633,\"start\":45626},{\"end\":45647,\"start\":45642},{\"end\":45660,\"start\":45654},{\"end\":45672,\"start\":45667},{\"end\":45690,\"start\":45685}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":11212020},\"end\":30706,\"start\":30200},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":14096841},\"end\":31328,\"start\":30708},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":92327},\"end\":31883,\"start\":31330},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":16418520},\"end\":32395,\"start\":31885},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":403206},\"end\":33025,\"start\":32397},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":512833},\"end\":33655,\"start\":33027},{\"attributes\":{\"id\":\"b6\"},\"end\":33932,\"start\":33657},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":2044034},\"end\":34218,\"start\":33934},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":17431208},\"end\":34764,\"start\":34220},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":393084},\"end\":35384,\"start\":34766},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":12657045},\"end\":35998,\"start\":35386},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":2963545},\"end\":36459,\"start\":36000},{\"attributes\":{\"doi\":\"abs/1712.04853\",\"id\":\"b12\"},\"end\":36785,\"start\":36461},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":6628106},\"end\":37199,\"start\":36787},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":921404},\"end\":37661,\"start\":37201},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":794019},\"end\":38435,\"start\":37663},{\"attributes\":{\"id\":\"b16\"},\"end\":38721,\"start\":38437},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":17355453},\"end\":39278,\"start\":38723},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":915058},\"end\":39865,\"start\":39280},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1998416},\"end\":40389,\"start\":39867},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":11525495},\"end\":40875,\"start\":40391},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6875312},\"end\":41512,\"start\":40877},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":215824512},\"end\":42091,\"start\":41514},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":179424},\"end\":42707,\"start\":42093},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":11080756},\"end\":43259,\"start\":42709},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":15349458},\"end\":43666,\"start\":43261},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":57680},\"end\":44071,\"start\":43668},{\"attributes\":{\"id\":\"b27\"},\"end\":44294,\"start\":44073},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":1211821},\"end\":44762,\"start\":44296},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":5789309},\"end\":45275,\"start\":44764},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":2332513},\"end\":45553,\"start\":45277},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":8014052},\"end\":46146,\"start\":45555}]", "bib_title": "[{\"end\":30269,\"start\":30200},{\"end\":30757,\"start\":30708},{\"end\":31385,\"start\":31330},{\"end\":31957,\"start\":31885},{\"end\":32480,\"start\":32397},{\"end\":33127,\"start\":33027},{\"end\":33986,\"start\":33934},{\"end\":34283,\"start\":34220},{\"end\":34817,\"start\":34766},{\"end\":35453,\"start\":35386},{\"end\":36055,\"start\":36000},{\"end\":36829,\"start\":36787},{\"end\":37242,\"start\":37201},{\"end\":37725,\"start\":37663},{\"end\":38459,\"start\":38437},{\"end\":38792,\"start\":38723},{\"end\":39332,\"start\":39280},{\"end\":39933,\"start\":39867},{\"end\":40448,\"start\":40391},{\"end\":40929,\"start\":40877},{\"end\":41598,\"start\":41514},{\"end\":42156,\"start\":42093},{\"end\":42771,\"start\":42709},{\"end\":43319,\"start\":43261},{\"end\":43722,\"start\":43668},{\"end\":44374,\"start\":44296},{\"end\":44820,\"start\":44764},{\"end\":45365,\"start\":45277},{\"end\":45618,\"start\":45555}]", "bib_author": "[{\"end\":30289,\"start\":30271},{\"end\":30304,\"start\":30289},{\"end\":30319,\"start\":30304},{\"end\":30777,\"start\":30759},{\"end\":30794,\"start\":30777},{\"end\":30805,\"start\":30794},{\"end\":30820,\"start\":30805},{\"end\":30831,\"start\":30820},{\"end\":30846,\"start\":30831},{\"end\":30863,\"start\":30846},{\"end\":30878,\"start\":30863},{\"end\":31406,\"start\":31387},{\"end\":31421,\"start\":31406},{\"end\":31444,\"start\":31421},{\"end\":31458,\"start\":31444},{\"end\":31470,\"start\":31458},{\"end\":31487,\"start\":31470},{\"end\":31504,\"start\":31487},{\"end\":31517,\"start\":31504},{\"end\":31530,\"start\":31517},{\"end\":31545,\"start\":31530},{\"end\":31564,\"start\":31545},{\"end\":31977,\"start\":31959},{\"end\":31994,\"start\":31977},{\"end\":32009,\"start\":31994},{\"end\":32028,\"start\":32009},{\"end\":32042,\"start\":32028},{\"end\":32056,\"start\":32042},{\"end\":32496,\"start\":32482},{\"end\":32511,\"start\":32496},{\"end\":32523,\"start\":32511},{\"end\":32538,\"start\":32523},{\"end\":33145,\"start\":33129},{\"end\":33157,\"start\":33145},{\"end\":33169,\"start\":33157},{\"end\":33181,\"start\":33169},{\"end\":33752,\"start\":33736},{\"end\":33766,\"start\":33752},{\"end\":33789,\"start\":33766},{\"end\":34003,\"start\":33988},{\"end\":34020,\"start\":34003},{\"end\":34038,\"start\":34020},{\"end\":34307,\"start\":34285},{\"end\":34330,\"start\":34307},{\"end\":34353,\"start\":34330},{\"end\":34841,\"start\":34819},{\"end\":34864,\"start\":34841},{\"end\":34887,\"start\":34864},{\"end\":35462,\"start\":35455},{\"end\":35477,\"start\":35462},{\"end\":35489,\"start\":35477},{\"end\":35505,\"start\":35489},{\"end\":35521,\"start\":35505},{\"end\":35545,\"start\":35521},{\"end\":35554,\"start\":35545},{\"end\":36072,\"start\":36057},{\"end\":36084,\"start\":36072},{\"end\":36095,\"start\":36084},{\"end\":36116,\"start\":36095},{\"end\":36563,\"start\":36546},{\"end\":36581,\"start\":36563},{\"end\":36597,\"start\":36581},{\"end\":36850,\"start\":36831},{\"end\":36860,\"start\":36850},{\"end\":37261,\"start\":37244},{\"end\":37276,\"start\":37261},{\"end\":37742,\"start\":37727},{\"end\":37754,\"start\":37742},{\"end\":37771,\"start\":37754},{\"end\":37793,\"start\":37771},{\"end\":37812,\"start\":37793},{\"end\":37829,\"start\":37812},{\"end\":37843,\"start\":37829},{\"end\":37854,\"start\":37843},{\"end\":37871,\"start\":37854},{\"end\":37885,\"start\":37871},{\"end\":37897,\"start\":37885},{\"end\":37911,\"start\":37897},{\"end\":37933,\"start\":37911},{\"end\":37946,\"start\":37933},{\"end\":38476,\"start\":38461},{\"end\":38495,\"start\":38476},{\"end\":38810,\"start\":38794},{\"end\":38825,\"start\":38810},{\"end\":38841,\"start\":38825},{\"end\":39349,\"start\":39334},{\"end\":39360,\"start\":39349},{\"end\":39366,\"start\":39360},{\"end\":39948,\"start\":39935},{\"end\":39959,\"start\":39948},{\"end\":39982,\"start\":39959},{\"end\":40466,\"start\":40450},{\"end\":40480,\"start\":40466},{\"end\":40937,\"start\":40931},{\"end\":40954,\"start\":40937},{\"end\":40980,\"start\":40954},{\"end\":40992,\"start\":40980},{\"end\":41010,\"start\":40992},{\"end\":41025,\"start\":41010},{\"end\":41039,\"start\":41025},{\"end\":41053,\"start\":41039},{\"end\":41066,\"start\":41053},{\"end\":41614,\"start\":41600},{\"end\":41625,\"start\":41614},{\"end\":41645,\"start\":41625},{\"end\":42174,\"start\":42158},{\"end\":42189,\"start\":42174},{\"end\":42213,\"start\":42189},{\"end\":42226,\"start\":42213},{\"end\":42791,\"start\":42773},{\"end\":42805,\"start\":42791},{\"end\":42816,\"start\":42805},{\"end\":42830,\"start\":42816},{\"end\":43335,\"start\":43321},{\"end\":43739,\"start\":43724},{\"end\":43755,\"start\":43739},{\"end\":43771,\"start\":43755},{\"end\":43787,\"start\":43771},{\"end\":44132,\"start\":44114},{\"end\":44148,\"start\":44132},{\"end\":44394,\"start\":44376},{\"end\":44412,\"start\":44394},{\"end\":44428,\"start\":44412},{\"end\":44444,\"start\":44428},{\"end\":44832,\"start\":44822},{\"end\":44847,\"start\":44832},{\"end\":44861,\"start\":44847},{\"end\":45386,\"start\":45367},{\"end\":45635,\"start\":45620},{\"end\":45649,\"start\":45635},{\"end\":45662,\"start\":45649},{\"end\":45674,\"start\":45662},{\"end\":45692,\"start\":45674}]", "bib_venue": "[{\"end\":30397,\"start\":30319},{\"end\":30960,\"start\":30878},{\"end\":31589,\"start\":31564},{\"end\":32123,\"start\":32056},{\"end\":32684,\"start\":32538},{\"end\":33277,\"start\":33181},{\"end\":33734,\"start\":33657},{\"end\":34057,\"start\":34038},{\"end\":34433,\"start\":34353},{\"end\":35001,\"start\":34887},{\"end\":35632,\"start\":35554},{\"end\":36182,\"start\":36116},{\"end\":36544,\"start\":36461},{\"end\":36938,\"start\":36860},{\"end\":37371,\"start\":37276},{\"end\":37998,\"start\":37946},{\"end\":38551,\"start\":38495},{\"end\":38934,\"start\":38841},{\"end\":39492,\"start\":39366},{\"end\":40063,\"start\":39982},{\"end\":40569,\"start\":40480},{\"end\":41141,\"start\":41066},{\"end\":41734,\"start\":41645},{\"end\":42372,\"start\":42226},{\"end\":42919,\"start\":42830},{\"end\":43410,\"start\":43335},{\"end\":43843,\"start\":43787},{\"end\":44112,\"start\":44073},{\"end\":44501,\"start\":44444},{\"end\":44954,\"start\":44861},{\"end\":45402,\"start\":45386},{\"end\":45785,\"start\":45692},{\"end\":30475,\"start\":30399},{\"end\":31043,\"start\":30962},{\"end\":32703,\"start\":32686},{\"end\":33372,\"start\":33279},{\"end\":34516,\"start\":34435},{\"end\":35117,\"start\":35003},{\"end\":35711,\"start\":35634},{\"end\":36246,\"start\":36184},{\"end\":37016,\"start\":36940},{\"end\":37463,\"start\":37373},{\"end\":38059,\"start\":38000},{\"end\":38570,\"start\":38553},{\"end\":39031,\"start\":38936},{\"end\":39620,\"start\":39494},{\"end\":40147,\"start\":40065},{\"end\":40661,\"start\":40571},{\"end\":41215,\"start\":41143},{\"end\":41829,\"start\":41736},{\"end\":42389,\"start\":42374},{\"end\":43011,\"start\":42921},{\"end\":43488,\"start\":43412},{\"end\":43861,\"start\":43845},{\"end\":44520,\"start\":44503},{\"end\":45050,\"start\":44956},{\"end\":45880,\"start\":45787}]"}}}, "year": 2023, "month": 12, "day": 17}