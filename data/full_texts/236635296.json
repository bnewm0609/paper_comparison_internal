{"id": 236635296, "updated": "2023-11-08 03:57:59.241", "metadata": {"title": "Product1M: Towards Weakly Supervised Instance-Level Product Retrieval via Cross-modal Pretraining", "authors": "[{\"first\":\"Xunlin\",\"last\":\"Zhan\",\"middle\":[]},{\"first\":\"Yangxin\",\"last\":\"Wu\",\"middle\":[]},{\"first\":\"Xiao\",\"last\":\"Dong\",\"middle\":[]},{\"first\":\"Yunchao\",\"last\":\"Wei\",\"middle\":[]},{\"first\":\"Minlong\",\"last\":\"Lu\",\"middle\":[]},{\"first\":\"Yichi\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Hang\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Xiaodan\",\"last\":\"Liang\",\"middle\":[]}]", "venue": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "journal": "2021 IEEE/CVF International Conference on Computer Vision (ICCV)", "publication_date": {"year": 2021, "month": 7, "day": 30}, "abstract": "Nowadays, customer's demands for E-commerce are more diversified, which introduces more complications to the product retrieval industry. Previous methods are either subject to single-modal input or perform supervised image-level product retrieval, thus fail to accommodate real-life scenarios where enormous weakly annotated multi-modal data are present. In this paper, we investigate a more realistic setting that aims to perform weakly-supervised multi-modal instance-level product retrieval among fine-grained product categories. To promote the study of this challenging task, we contribute Product1M, one of the largest multi-modal cosmetic datasets for real-world instance-level retrieval. Notably, Product1M contains over 1 million image-caption pairs and consists of two sample types, i.e., single-product and multi-product samples, which encompass a wide variety of cosmetics brands. In addition to the great diversity, Product1M enjoys several appealing characteristics including fine-grained categories, complex combinations, and fuzzy correspondence that well mimic the real-world scenes. Moreover, we propose a novel model named Cross-modal contrAstive Product Transformer for instance-level prodUct REtrieval (CAPTURE), that excels in capturing the potential synergy between multi-modal inputs via a hybrid-stream transformer in a self-supervised manner.CAPTURE generates discriminative instance features via masked multi-modal learning as well as cross-modal contrastive pretraining and it outperforms several SOTA cross-modal baselines. Extensive ablation studies well demonstrate the effectiveness and the generalization capacity of our model. Dataset and codes are available at https: //github.com/zhanxlin/Product1M.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2107.14572", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/iccv/ZhanWDWLZXL21", "doi": "10.1109/iccv48922.2021.01157"}}, "content": {"source": {"pdf_hash": "18982c55e60d3157e07d0fbdaba0bf7df2fab8bd", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2107.14572v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "89cba3b3d32920acc248429c27c4ec4c9a9be912", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/18982c55e60d3157e07d0fbdaba0bf7df2fab8bd.txt", "contents": "\nProduct1M: Towards Weakly Supervised Instance-Level Product Retrieval via Cross-Modal Pretraining\n\n\nXunlin Zhan \nSun Yat-sen University\n\n\n\u2020 \nYangxin Wu \nSun Yat-sen University\n\n\nXiao Dong \nSun Yat-sen University\n\n\nYunchao Wei \nBeijing Jiaotong University\n\n\nMinlong Lu \nAlibaba Group\n\n\nYichi Zhang yichi.zyc@alibaba-inc.com \nAlibaba Group\n\n\nHang Xu \nHuawei Noah's Ark Lab\n\n\nXiaodan Liang xdliang328@gmail.com \nSun Yat-sen University\n\n\nProduct1M: Towards Weakly Supervised Instance-Level Product Retrieval via Cross-Modal Pretraining\n\nNowadays, customer's demands for E-commerce are more diversified, which introduces more complications to the product retrieval industry. Previous methods are either subject to single-modal input or perform supervised imagelevel product retrieval, thus fail to accommodate real-life scenarios where enormous weakly annotated multi-modal data are present. In this paper, we investigate a more realistic setting that aims to perform weakly-supervised multimodal instance-level product retrieval among fine-grained product categories. To promote the study of this challenging task, we contribute Product1M, one of the largest multimodal cosmetic datasets for real-world instance-level retrieval. Notably, Product1M contains over 1 million imagecaption pairs and consists of two sample types, i.e., singleproduct and multi-product samples, which encompass a wide variety of cosmetics brands. In addition to the great diversity, Product1M enjoys several appealing characteristics including fine-grained categories, complex combinations, and fuzzy correspondence that well mimic the realworld scenes. Moreover, we propose a novel model named Cross-modal contrAstive Product Transformer for instancelevel prodUct REtrieval (CAPTURE), that excels in capturing the potential synergy between multi-modal inputs via a hybrid-stream transformer in a self-supervised manner. CAPTURE generates discriminative instance features via masked multi-modal learning as well as cross-modal contrastive pretraining and it outperforms several SOTA crossmodal baselines. Extensive ablation studies well demonstrate the effectiveness and the generalization capacity of our model. Dataset and codes are available at https: //github.com/zhanxlin/Product1M . \u2020 Equal contribution. Corresponding Author.\n\nIntroduction\n\nThe past two decades have witnessed the high enrichment of the commodity types and the diversification of online customer's demand in E-commerce. On the one hand, online merchandise has increasingly diversified categories and a large proportion of them are exhibited as a product portfolio where multiple instances of different products exist in one image. On the other hand, online customers or merchants may want to retrieve the single product in a portfolio for price comparison [42] or online commodity recommendation [34]. Furthermore, with the ever-accelerating accumulation of heterogeneous data generated by multimedia, it remains a problem how an algorithm can handle large-scale and weakly annotated data [45] to perform multi-modal retrieval.\n\nIn this paper, we explore a realistic problem: how to perform instance-level 1 fine-grained product retrieval given the Dataset #samples #categories #instances #obj/img weak supervision multi-modal instance-level retrieval RPC checkout [47] 30,000 200 367,935 12.26 Twitter100k [17] 100,000 ---INRIA-Websearch [22] 71,478 353 --Dress Retrieval [7] 20,200 -\u223c20,200 \u223c1.0 Product1M(Ours) 1,182,083 458 92,200 2.83 Table 1. Comparisons between different datasets. '-' indicates inapplicable. The #instances and #obj/img of Product1M are in italics since there are no instance labels for the train set and we only count the instances in the val and test set. Product1M is one of the largest multi-modal datasets as well as the first dataset specifically tailored for real-world instance-level retrieval scenarios.\n\nlarge-scale weakly annotated multi-modal data? We compare different paradigms of retrieval in Figure 1. As can be seen, image-level retrieval tends to return trivial results since it does not distinguish different instances, while multimodal instance-level retrieval is more favorable for searching for various kinds of products among multi-modal data.\n\nDespite the generality and the practical value of this problem, it is not well studied due to the lack of real-world datasets and a clear problem definition. In the literature of product retrieval, intra-modal [32,1,31,30] and crossmodal retrieval [43,12,48,4,44,8] take as input singlemodal information, e.g., an image or a piece of text, and performs matching search between separate data points. Unfortunately, such retrieval schemes significantly restrict their use in many scenarios where multi-modal information exists in both the queries and targets. More importantly, previous works focus on the relatively simple case, i.e., imagelevel 2 retrieval for single-product images [24,13] and the instance-level nature of retrieval is unexplored. To bridge this gap and advance the related research, we collect a large-scale dataset, named Product1M, proposed for multi-modal instance-level retrieval. Product1M contains over 1 million image-caption pairs and consists of two types of samples, i.e., single-product and multi-product samples. Each single-product sample belongs to a finegrained category and the inter-category difference is subtle. The multi-product samples are of great diversity, resulting in complex combinations and fuzzy correspondence that well mimic the real-world scenarios. To the best of our knowledge, Product1M is one of the largest multi-modal datasets as well as the first dataset specifically tailored for real-world multi-modal instance-level retrieval scenarios.\n\nIn addition to the constructed dataset, we also propose a novel self-supervised training framework that extracts representative instance-level features from large-scale weakly annotated data. Specifically, we first train a multi-product detector from pseudo-labels by incorporating a simple yet effective data augmentation scheme. Then, CAPTURE is proposed to capture the potential synergy of images and texts via several pretext tasks. We showcase that some prevailing cross-modal pretraining methods [27,25,6,38] might be flawed under the multi-instance setting due to the design defects in the network architecture or the inappropriate pretext task. In contrast, CAPTURE utilizes a hybridstream architecture that encodes data of different modalities separately and fuses them in a unified way, which is experimentally shown to be beneficial for our proposed task. Moreover, we introduce the cross-modal contrastive loss to enforce CAPTURE to reach alignment between image and texts, which avoids the mismatch issue incurred by the inappropriate pretext task.\n\nCrucially, CAPTURE surpasses the SOTA cross-modal baselines in terms of all main metrics by a large margin. We further conduct extensive ablation experiments to demonstrate the generalization capacity of CAPTURE and explore several critical factors of our proposed task. We hope the proposed Product1M, CAPTURE, and solid baselines can help advance future research on real-world retrieval.\n\n\nRelated Work\n\nIntra-and Cross-Modal Retrieval. Intra-modal retrieval [32,1] has been extensively studied in the keyword-based web document retrieval [11], content-based image retrieval [29], and product recommendation [19,20]. Cross-modal retrieval [43,12,48,4,44,8] emerges as a promising avenue for efficient indexing and searching among large-scale data with different modalities, and is widely used in search engines [2,14], E-commerce [18,7], to name a few. However, these approaches [30,26,7,47,46] are typically subject to single modal inputs, which makes them hard to apply to many real-world scenarios where multi-modal information exists in both the queries and targets. WSOD: Weakly Supervised Object Detection. WSOD [39,36,50] reduces its excessive reliance on fine-grained labels by learning from cheaper or freely-available data. PCL [39] iteratively generates proposal clusters to facilitate the learning of instance classifiers. Pseudo labels generated from image labels [36] and unstructured textual descriptions like captions [50] are also beneficial for boosting the performance of WSOD. However, WSOD typically relies on a fixed-size collection of predefined classes and is not readily applicable to our proposed task where class labels are not available and categories can be updated dynamically.  Cross-Modal Self-Supervised Learning. Existing Visionlanguage pre-trained models typically use a multi-layer Transformer [41] architecture such as BERT [9] to learn image-text semantic alignment on multi-modal data.\n\nSingle-stream models [25,37,6] encode the combined multi-modal features in a unified architecture while other two-stream models [27,38] instead utilize different encoders for inputs of different modalities. These methods are not tailored for instance-level retrieval and we showcase that they might be flawed due to the design defects in the network architecture and the inappropriate pretext tasks.\n\n\nInstance-Level Retrieval on Product1M\n\n\nTask Definition\n\nA product sample (I, C) is an image-text pair where I is the product image and C is the caption. Given the gallery set of single-product samples\nS = {S i |S i = (I i S , C i S )} and the set of multi-product samples P = {P i |P i = (I i P , C i P )}\n, the task is to retrieve and rank the single-products that appear in the query sample P i , i.e., to predict a list\nRET R i = [id i 1 , id i 2 , \u00b7 \u00b7 \u00b7 , id i k , \u00b7 \u00b7 \u00b7 , id i N ] \u2200P i \u2208 P,\nwhere id i k corresponds to a specific single-product sample in S.\n\n\nDataset Statistics\n\nWe collect a large number of product samples of 49 brands from E-commerce websites. These image-text samples are then manually divided into the single-product and multi-product groups according to the corresponding prod-uct information. Product1M is split into the train, val, test, and gallery set. The train set contains 1,132,830 samples including both the single-product and multi-product samples, while there are only multi-product samples in the val and test set, which contain 2,673 and 6,547 samples respectively. The gallery set has 40,033 single-product samples for 458 categories, 392 of which appear in the val and test set and the remaining ones act as interference items for validating the robustness of a retrieval algorithm. The samples in the gallery, val, and test sets are annotated with class labels for the purpose of evaluation, i.e., they are not involved in the training process, and the samples in the train set are not annotated. The statistics of Product1M are shown in Table 1 and Figure 2. More visualizations of Product1M and comparisons with related datasets can be found in the Supplementary Materials.\n\n\nDataset Characteristics\n\nMulti-product nature and complex combinations: The multi-product images are ubiquitous on E-commerce websites and serve as the query images of instance-level product retrieval. As is shown in Figure 2(1a), products can be organized in abundant forms and layouts and the number of instances can be large. The excessive amount and great diversity of fine-grained single-product samples give rise to the complex combinations in different portfolio images. Weak supervision and fuzzy correspondence: We consider using data of two common modalities, i.e., images and texts, for retrieval. Unlike other datasets with clean class la-copy-paste\n\n\nText transformer\n\nVisual transformer Image embedding layer Text embedding layer Text co-transformer Visual co-transformer\nText-visual transformer \u00d7 \u00d7 \u00d7 (\"#\") (\") ($) (%) (%&') (\") ($) (%)\n\nText transformer Text Transformer Visual transformer Visual Transformer\n\nText co-transformer Text Cross-Transformer Visual co-transformer Visual Cross-Transformer\n\n\nText-visual transformer Text-Visual Co-Transformer\n\n\nMasked Language Modeling Masked Region Prediction\nQ V K Q K V V Q K Q V K Q V K\n\nRPN Head\n\n\nRes50-FPN\n\n\nSingle-product samples\n\n\nForeground masks\nGrab-Cut Box Pseudo-labels RPN Loss (a) (b) (c)\n\nPre-trained RPN\n\n\nCross-Modal Contrastive Loss\n\nHong Kong purchasing Lancome Moisture Edge Comfort Three-piece Set Essence + Water + Gel\n\n\nText Transformer\n\nVisual Transformer\nT \" T $ T % T & \u2026 T '\n\nCross-Modal Contrastive Loss\n\n\nAdvanced [MASK] Two-piece Set Yeux Youth Activating [MASK]\n\nCream 15ML + Essence 50ML bels, the supervision from commodity captions is weak and often uninformative. We show different types of challenging samples in Figure 2(1b). Some samples contain abbreviations, i.e., a shortened form of several products, in their captions. However, the abbreviation like 'eight-piece set' does not contain any specific information about products. The second type of sample carries irrelevant information, where the commodities described in the title may not appear in the image or vice versa. The wide distribution of fuzzy correspondence between images and titles makes it even more challenging for instance-level retrieval. Consistency with real-world scenarios: We show some challenging samples in Figure 2(1c). They can have a complex background with irrelevant objects, amorphous watermarks, or significant clutter covering the product information. Some products of different categories can have almost the same appearance except that the words on the packing are slightly different, e.g., day cream vs night cream. As is shown in Figure 2(2a,2b), the long-tailed distribution of Product1M aligns well with real-world scenarios.\n\n\nRPN\n\n\nPositive Pairs\nI \" I $ I % I & \u2026 I (\n\nMethodology\n\nAs is depicted in Figure 3, our framework consists of an augmentation-based detector and a self-supervised multimodal transformer. In this section, we first elaborate the training process of RPN and the architectural design of CAPTURE in Section 4.1 and Section 4.2. Then we describe two kinds of pretext tasks that enables the selfsupervised learning of CAPTURE in Section 4.3 and Section 4.4. Finally, we illustrate the inference process for instance-level retrieval in Section 4.5.\n\n\nTraining RPN for Multi-Product Detection\n\nRetrieval based simply on the image-level features will lead to an undesirable condition where the retrieval results are overwhelmed by the dominated product in an image. Thus it is crucial to distinguish different products and extract proposal-wise features in a multi-product image. While many pre-trained detectors are available, they are infeasible to directly apply to multi-product detection due to the distribution difference between datasets. Thus we utilize a simple yet effective data augmentation scheme to train a Region Proposal Network (RPN) [35] based solely on the single-product images as shown in Figure 3(a). We first use GrabCut [28] to obtain the foreground masks of single-product images. With real-world background images from Places365 [51], a copy-and-paste augmentation [10] is applied to these foreground masks and background images to generate synthesized images. In this way, we are able to train a well-performing multi-product detector. Given the detected regions of RPN, we utilize RoIAlign [15] to obtain instance-wise features, which are then fed into CAPTURE for further cross-modal learning. More visualizations and details about the synthesized images and the training of RPN can be found in the Supplementary Materials.\n\n\nArchitectural Design of CAPTURE\n\nAfter training the RPN, we can generate high-quality proposals for different products in an image. Different from the prevalent single-stream or two-stream transformer architectures, we propose CAPTURE that combines these two architectures into a unified one by stacking three types of layers for semantic alignment and joint learning of multi-modal inputs. Details are shown in Figure 3(b). To be specific, the Text/Visual Transformer takes as input the embeddings of the texts or image and is responsible for intramodal feature learning. The Text/Visual Cross-Transformer aims to capture and model the inter-modal relations between texts and image by exchanging key-value pairs in the multi-headed attention mechanism. After that, the features of texts and image are concatenated and serve as the query, key, and value inputs to the Co-Transformer for joint learning of multi-modal features. These three types of transformer are stacked L, K, and H times respectively. We verify the effectiveness of our architectural design in Table 4.\n\n\nCAPTURE by Masked Multi-Modal Learning\n\nWe utilize several pretext tasks to enable the selfsupervised learning of CAPTURE. For modality-wise feature learning, we adopt two masked multi-modal modeling tasks, i.e., Masked Language Modeling task (MLM) and Masked Region Prediction task (MRP), following the standard BERT [9] and VisualBERT [25]. Concretely, for MLM and MRP, approximately 15% of texts and proposal inputs are masked out and the remaining inputs are used to reconstruct the masked information. The MLM is handled as in BERT [9]. For the MRP, the model directly regresses the masked features, which is supervised by the features extracted by the pretrained RPN with a MSELoss. As for inter-modal relation modeling, Image-Text Matching task (ITM) is widely adopted in many previous methods [25,6,27,38]. Typically, the model is asked to predict whether the text is the corresponding description of an image, which is formulated as a binary classification task. To generate negative samples, either the image or caption is randomly substituted. We argue that ITM could be problematic to the fine-grained understanding of an image-text sample at the instance-level. We hypothesize the deterioration stems from the unmatched image and caption pairs after substitution, which results in the inconsistency between detected regions and text. We further experimentally validate this claim in Table 3.\n\n\nCAPTURE by Cross-Modal Contrastive Loss\n\nAside from intra-modal feature learning, CAPTURE is expected to generate coherent representations of multimodal inputs and learn the correspondence between them. To this end, we resort to inter-modality contrastive learning [5,33] to reach alignment between image and text. For a minibatch of N image-text samples, there are 2N data points in total. We treat the corresponding image-text pairs as N positive pairs, and the other 2(N \u22121) unmatched pairs are regarded as negative ones. Formally, given an imagetext pair (x i , x j ) and their encoded features (x i ,x j ), the cross-modal contrastive loss for this positive pair is com-puted as:\nL(x i , x j ) = \u2212 log exp (sim (x i ,x j ) /\u03c4 ) 2N k=1 1 [k =i] exp (sim (x i ,x k ) /\u03c4 ) ,\n(1) where sim(u, v) = u v/ u v computes the cosine similarity of (u, v) pairs, \u03c4 denotes the temperature parameter, 1 [k =i] is a binary indicator function that returns 1 iff k = i. This form of contrastive loss encourages the encoded features of positive pairs from different modality to be similar while discriminates those of negative ones. We find it beneficial to inject this supervision at the Text/Visual Transformer and further discussion about the effect of crossmodal contrastive loss can be found in Section 5.3.\n\n\nInference for Instance-Level Retrieval\n\nFor both the single-and multi-product samples, the proposal-wise features extracted via the pre-trained RPN and the captions are used as input to CAPTURE. During inference, the Co-Transformer layer outputs H IM G and H T XT as the overall representations of visual and linguistic inputs, respectively. These two vectors are multiplied together to derive the joint representations of an instance. Furthermore, since Text/Visual Transformer is supervised with cross-modal contrastive loss, we find it beneficial to concatenate the features of this layer for retrieval. The resulting features then serve as the input of our retrieval algorithm. After computing the cosine similarity matrix between an instance and the samples in the gallery set, we retrieve the corresponding single-product samples with the highest similarities for each query.\n\n\nExperiments\n\n\nImplementation Details\n\nWe attach RPN to a ResNet-50 [16] backbone pretrained on ImageNet and follow the training schedule in [35]. We use the BERT [9] to initialize the linguistic transformer of our CAPTURE. The number of the Text/Visual Transformer, Text/Visual Cross-Transformer, and Co-Transformer is set to L = 4, K = 4, and H = 4, respectively, which adds up to 12 transformer layers. We set the hidden state size of CAPTURE and other baselines to 768 for a fair comparison. We separately attach a 512-d fully connected layer after Co-Transformer and Text/Visual Transformer for masked multi-modal learning and crossmodal contrastive learning. The concatenation of the features from these two layers results in a 1024-d feature vector for retrieval, which is also the same for other baselines. The maximum sequence length for the sentence is set to 36. We train CAPTURE with a total batch size of 128 for 10 epochs on 4 RTX 2080 GPUs. We use Adam [21] optimizer with an initial learning rate of 1e-4 and a linear learning rate decay schedule is adopted.  [49,3]. Since exhaustively retrieve every single product is unnecessary and impractical in many scenarios, we report mAP, mAR, and Prec for N = 10, 50, 100. The details of evaluation metrics can be found in the Supplementary Materials.\n\n\nWeakly-Supervised Instance-Level Retrieval\n\nWe compare CAPTURE with several intra-and crossmodal baselines and the results are shown in Table 2. Intra-Modal Schemes. We compare our method to two intra-modal schemes including Image-based and Text-based schemes. For image-based retrieval, we stack the Visual Transformer layer described in Section 4.2 and adopt the same image input and pretext task, i.e., Masked Region Prediction as CAPTURE. For text-based retrieval, we stack the Text Transformer layer and use only the text input and Masked Language Modeling pretext task. We further double the depth of these two models to 24 layers to keep the same amount of parameters as CAPTURE. It turns out that these two schemes are lagging far behind since they are subject to data of single modality, which suggests that the modeling of the relations between multi-modal data is indispensable. We provide more experiment results to validate this point in Section 5.4. Cross-Modal Schemes. We compare CAPTURE to several prevailing self-supervised cross-modal pretraining methods in Table 2, including SOTA single-stream and two-stream Vision-language models as well as a SOTA zero-shot classification model, i.e., CLIP [33]. The CLIP* baseline refers to  Table 3. The impact of different pretext tasks and cross-modal contrastive loss. Evaluation for N = 100. 'Masked' stands for two masked multi-modal pretext tasks, i.e., MLM and MRP. 'CTR' stands for cross-modal contrastive loss.\n\na CLIP-like architecture that uses separate transformers to encode image and text and is trained with a contrastive objective. Notably, CAPTURE outperforms all these baselines in all three metrics for instance-level retrieval. Two-stream models, i.e., ViLBERT [27], LXMERT [38] and CLIP*, are generally worse than single-stream ones, which suggests that the fusion mode of multi-modal features is one of the critical factors. We attribute the superior performance of CAPTURE to its hybrid-stream architecture and we study the impact of different layer types in Section 5.4.\n\n\nImpact of Pretext Tasks and Contrastive Loss\n\nAs shown in Table 3, ITM will hurt the accuracy of instance-level retrieval (#1 vs #3), since it gives rise to mismatch samples, which might be detrimental to the finegrained understanding of a multi-product image. We apply the cross-modal contrastive loss at the Text/Visual Transformer layer to align the representations of image and text, which further benefits the learning of consequent layers. The inclusion of contrastive loss encourages our model to maximizes the feature similarity of positive pairs, which improves all three metrics by 1.2, 0.2, and 0.5, respectively (#1 vs #4), and we find it of little help when added to the deeper layers. Moreover, after concatenating the features from the Text/Visual Transformer with that from the Co-Transformer for retrieval, it further improves all three metrics by 1.3, 1.0, and 0.7, respectively (#4 vs #5). However, we find this concatenation operation will slightly degrade the performance of the model without contrastive loss (#1 vs #2), which suggests that the improvement mainly comes from the contrastive loss instead of the operation itself. \n\n\nImpact of Layer Configuration\n\nWe investigate how the configuration of transformer layers will affect the performance of our model in Table  4. The triplet in the Config column stands for the number of Text/Visual Transformer, Cross-Transformer, and Co-Transformer layer, respectively. We first remove layers of a specific type while keeping the depth of the resulting network the same as CAPTURE's, i.e., 12 layers, for a fair comparison. The 'w/o-Cross', 'w/o-Co', and 'w/o-Txt/Vis' refer to the resulting model after removing the Cross-Transformer, Co-Transformer, and Text/Visual Transformer layers from CAPTURE. As can be seen, the performances of these three models are inferior to that of CAPTURE, which demonstrates the effectiveness of its hybrid-stream architecture. Moreover, in the second group of Table 4 (CAPTURE-A,B,C), we study the combination of three layer types in different proportions. It turns out that the (4,4,4) configuration achieves the best performance. We further explore the performance of a smaller model (CAPTURE-S) and a larger model (CAPTURE-L). As can be seen, CAPTURE with the (4,4,4) configuration achieves a better trade-off between accuracy and parameters.\n\n\nZero-Shot Instance-Level Retrieval\n\nWe argue that a retrieval-based solution generalizes better to real-world scenarios where the category set is updated continuously and large quantities of clean labels are too costly to collect. Unlike detection, our retrieval-based framework does not rely on a fixed-size collection of predefined classes or fine-grained box annotations. To emphasize this, we conduct zero-shot retrieval experiments and report the results in Table 5. We manually remove 5/10/20 brands from the train set and train CAPTURE on the remaining samples so that the removed categories are not disposed to   Table 6. Ablation study of single-product retrieval and the impact of detection performance on retrieval. Note that for single-product retrieval, the metric Prec@N is equivalent to mAR@N since there is only one category in an image.\n\nour model during training. Then we evaluate CAPTURE on the classes of these unseen brands. We further compare our model with a two-stream model LXMERT and a single-stream model UNITER. As can be seen, CAPTURE achieves better performance than LXMERT and UNITER for all three metrics, which well demonstrates its generalization capacity. We also visualize the embeddings generated by CAPTURE and UNITER via t-SNE [40] in Figure  5. It turns out that the features encoded by CAPTURE are more discriminative, which thus benefits the retrieval task.\n\n\nComparisons on Single-Product Retrieval\n\nIt is noteworthy that CAPTURE is applicable to both single-product and multi-product retrieval. Indeed, it excels on these two tasks and achieves better performance than other baselines in single-product retrieval. Specifically, for each single-product sample in the gallery set, we pick it out as a query and perform single-product retrieval among the remaining samples in the gallery set. We compare the performance of three models, i.e., UNITERsingle, LXMERT-single and CAPTURE-single, in Table 6. As can be seen, the performance of single-product retrieval is much higher than that of multi-product retrieval since the difficulty is largely reduced when there is only one instance/entity in the image/text. Furthermore, we notice the  performance of 'CAPTURE-single' is still better than that of two other baselines, which further demonstrates the superiority of CAPTURE.\n\n\nImpact of Detection Performance on Retrieval\n\nWe conduct several experiments to explore how the performance of a detector will influence instance-level retrieval. The results are listed in Table 6. As we claim in Section 4.1, the off-the-shelf pretrained detectors are not readily applicable to our dataset due to the distribution difference between natural images and commodity images. To verify this, we replace the RPN with Faster R-CNN [35] pre-trained on Visual Genome [23] and utilize it to generate instance-wise input features of CAPTURE. The resulting model, named 'CAPTURE-natural', is inferior to CAPTURE in all three metrics. For the 'CAPTURE-1Inst' model, we feed the whole image and an image-level bounding box, which is of the same size as the image, to CAP-TURE for inference. This scheme performs unsatisfactorily due to the failure of instance recognition, which suggests that the detector may become a performance bottleneck. Going a step further, to explore the upper bound of CAP-TURE, we randomly select 1,338 multi-product images and manually label the bounding boxes of these images. For the 'CAPTURE-subset' model, we simply evaluate CAPTURE on this annotated subset. For the 'CAPTURE-gt' model, the ground truth boxes and their corresponding features serve as the input to CAPTURE. As can be seen, the performance gap of these two models suggests that the performance of a detector can play an essential role in instance-level retrieval. Moreover, the mAR gap between them is relatively large, which indicates that the false negatives in detection will hurt the performance of instance-level retrieval.\n\n\nConclusion\n\nIn this paper, we present the first effort on extending canonical intra-/cross-modal retrieval to a more generalized  setting, i.e., weakly-supervised multi-modal instance-level product retrieval, which has wide application potential in the E-commerce industry. We contribute Product1M, which is one of the largest multi-modal retrieval datasets as well as the first one specifically tailored for instance-level retrieval. Aside from that, we propose a novel hybrid-stream transformer, named CAPTURE, that excels in capturing the potential synergy between data of different modalities. Moreover, we overcome the unmatched issue incurred by the inappropriate pretext task by enforcing cross-modal contrastive learning between multi-modal features. Extensive experiments demonstrate that our CAPTURE surpasses the SOTA cross-modal pretraining models in terms of all metrics by a large margin. We hope the proposed Product1M, CAPTURE, and solid baselines will spur further investigation into a more reliable and flexible retrieval system.\n\n\nAcknowledgement\n\nFigure 1 .\n1Our proposed task performs instance-level retrieval among multi-modal data.\n\nFigure 2 .\n2Characteristics and statistics of Product1M: (1a) Complex combinations of single-product; (1b) Weak supervision and fuzzy correspondence; (1c) Difficulties in real-world scenarios; (2) Long-tailed category distribution of Product1M. The line displays the sample number of each category in decreasing order. Product1M contains a wide variety of categories and the long-tailed class distribution aligns well with real-world scenarios.\n\nFigure 3 .\n3An overview of our instance-level retrieval pipeline. (a) Pretrain an RPN based on pseudo-labels generated by a copy-and-paste data augmentation scheme. (b) Utilize CAPTURE to capture the potential synergy across modality via a hybrid-stream architecture and several pretext tasks. (c) Construct positive pairs of matched image-text samples for cross-modal contrastive learning. Best viewed in color.\n\nFigure 4 .\n4Visualize the embeddings generated by CAPTURE and UNITER via t-SNE. Points belonging to the same category are of the same color. Best viewed in color.\n\nFigure 5 .\n5Visualizations of the retrieval results generated by CAPTURE. Multi-product query images are on the left. Correct/Incorrect retrieval images are highlighted in green/red boxes.\n\nTable 5 .\n5Performance comparison of zero-shot retrieval. Organized in the order of LXMERT/UNITER/CAPTURE.Method \nmAP@100 mAR@100 Prec@100 \n\nUNITER-single \n86.56 \n80.82 \n80.82 \nLXMERT-single \n86.05 \n80.59 \n80.59 \nCAPTURE-single \n88.24 \n83.33 \n83.33 \n\nCAPTURE-natural \n70.36 \n26.46 \n66.53 \nCAPTURE-1Inst \n60.03 \n20.43 \n58.42 \nCAPTURE \n74.63 \n30.08 \n73.86 \n\nCAPTURE-subset \n73.36 \n30.44 \n72.41 \nCAPTURE-gt \n77.79 \n37.40 \n77.13 \n\n\nInstance-level product retrieval refers to the retrieval of all single products existed in a product portfolio image.\nImage-level product retrieval refers to recognizing a specific product instance in a single-product image.\n\nOptimization of deep convolutional neural network for large scale image retrieval. Cong Bai, Ling Huang, Xiang Pan, Jianwei Zheng, Shengyong Chen, Neurocomputing. 3032Cong Bai, Ling Huang, Xiang Pan, Jianwei Zheng, and Shengyong Chen. Optimization of deep convolutional neu- ral network for large scale image retrieval. Neurocomputing, 303:60-67, 2018. 2\n\nInformation retrieval: Implementing and evaluating search engines. Stefan B\u00fcttcher, L A Charles, Gordon V Clarke, Cormack, Mit PressStefan B\u00fcttcher, Charles LA Clarke, and Gordon V Cor- mack. Information retrieval: Implementing and evaluating search engines. Mit Press, 2016. 2\n\nA revisit of deep hashings for large-scale content based image retrieval. Deng Cai, Xiuye Gu, Chaoqi Wang, arXiv:1711.06016Deng Cai, Xiuye Gu, and Chaoqi Wang. A revisit of deep hashings for large-scale content based image retrieval. In arXiv:1711.06016, 2017. 6\n\nDeep visual-semantic hashing for cross-modal retrieval. Yue Cao, Mingsheng Long, Jianmin Wang, Qiang Yang, Philip S Yu, Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data MiningYue Cao, Mingsheng Long, Jianmin Wang, Qiang Yang, and Philip S Yu. Deep visual-semantic hashing for cross-modal retrieval. In Proceedings of the 22nd ACM SIGKDD Interna- tional Conference on Knowledge Discovery and Data Min- ing, pages 1445-1454, 2016. 2\n\nA simple framework for contrastive learning of visual representations. Ting Chen, Simon Kornblith, Mohammad Norouzi, Geoffrey Hinton, PMLR, 2020. 5International conference on machine learning. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge- offrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on ma- chine learning, pages 1597-1607. PMLR, 2020. 5\n\nUniter: Universal image-text representation learning. Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu, European Conference on Computer Vision. Springer56Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In European Conference on Computer Vision, pages 104-120. Springer, 2020. 2, 3, 5, 6\n\nLeveraging weakly annotated data for fashion image retrieval and label prediction. Charles Corbiere, Hedi Ben-Younes, Alexandre Ram\u00e9, Charles Ollion, Proceedings of the IEEE International Conference on Computer Vision Workshops. the IEEE International Conference on Computer Vision WorkshopsCharles Corbiere, Hedi Ben-Younes, Alexandre Ram\u00e9, and Charles Ollion. Leveraging weakly annotated data for fash- ion image retrieval and label prediction. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pages 2268-2274, 2017. 2\n\nTriplet-based deep hashing network for crossmodal retrieval. Cheng Deng, Zhaojia Chen, Xianglong Liu, Xinbo Gao, Dacheng Tao, IEEE Transactions on Image Processing. 278Cheng Deng, Zhaojia Chen, Xianglong Liu, Xinbo Gao, and Dacheng Tao. Triplet-based deep hashing network for cross- modal retrieval. IEEE Transactions on Image Processing, 27(8):3893-3903, 2018. 2\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova Bert, arXiv:1810.04805Pre-training of deep bidirectional transformers for language understanding. 35arXiv preprintJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 3, 5\n\nCut, paste and learn: Surprisingly easy synthesis for instance detection. Debidatta Dwibedi, Ishan Misra, Martial Hebert, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionDebidatta Dwibedi, Ishan Misra, and Martial Hebert. Cut, paste and learn: Surprisingly easy synthesis for instance de- tection. In Proceedings of the IEEE International Confer- ence on Computer Vision, pages 1301-1310, 2017. 4\n\nDocument retrieval model through semantic linking. Faezeh Ensan, Ebrahim Bagheri, Proceedings of the tenth ACM international conference on web search and data mining. the tenth ACM international conference on web search and data miningFaezeh Ensan and Ebrahim Bagheri. Document retrieval model through semantic linking. In Proceedings of the tenth ACM international conference on web search and data min- ing, pages 181-190, 2017. 2\n\nCross-modal retrieval with correspondence autoencoder. Fangxiang Feng, Xiaojie Wang, Ruifan Li, Proceedings of the 22nd ACM international conference on Multimedia. the 22nd ACM international conference on MultimediaFangxiang Feng, Xiaojie Wang, and Ruifan Li. Cross-modal retrieval with correspondence autoencoder. In Proceedings of the 22nd ACM international conference on Multimedia, pages 7-16, 2014. 2\n\nMulti-modal and multidomain embedding learning for fashion retrieval and analysis. Xiaoling Gu, Yongkang Wong, Lidan Shou, Pai Peng, Gang Chen, Mohan S Kankanhalli, IEEE Transactions on Multimedia. 216Xiaoling Gu, Yongkang Wong, Lidan Shou, Pai Peng, Gang Chen, and Mohan S Kankanhalli. Multi-modal and multi- domain embedding learning for fashion retrieval and anal- ysis. IEEE Transactions on Multimedia, 21(6):1524-1537, 2018. 2\n\nInformation retrieval: the early years. Foundations and Trends\u00ae in Information Retrieval. Donna Harman, 13Donna Harman et al. Information retrieval: the early years. Foundations and Trends\u00ae in Information Retrieval, 13(5):425-577, 2019. 2\n\nPiotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. Kaiming He, Georgia Gkioxari, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionKaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Gir- shick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961-2969, 2017. 4\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. 5\n\nTwitter100k: A real-world dataset for weakly supervised cross-media retrieval. Yuting Hu, Liang Zheng, Yi Yang, Yongfeng Huang, IEEE Transactions on Multimedia. 204Yuting Hu, Liang Zheng, Yi Yang, and Yongfeng Huang. Twitter100k: A real-world dataset for weakly supervised cross-media retrieval. IEEE Transactions on Multimedia, 20(4):927-938, 2017. 2\n\nCrossdomain image retrieval with attention modeling. Xin Ji, Wei Wang, Meihui Zhang, Yang Yang, Proceedings of the 25th ACM international conference on Multimedia. the 25th ACM international conference on MultimediaXin Ji, Wei Wang, Meihui Zhang, and Yang Yang. Cross- domain image retrieval with attention modeling. In Proceed- ings of the 25th ACM international conference on Multime- dia, pages 1654-1662, 2017. 2\n\nVisually-aware fashion recommendation and design with generative image models. Wang-Cheng Kang, Chen Fang, Zhaowen Wang, Julian Mcauley, 2017 IEEE International Conference on Data Mining (ICDM). Wang-Cheng Kang, Chen Fang, Zhaowen Wang, and Julian McAuley. Visually-aware fashion recommendation and de- sign with generative image models. In 2017 IEEE Interna- tional Conference on Data Mining (ICDM), pages 207-216. IEEE, 2017. 2\n\nComplete the look: Scenebased complementary product recommendation. Wang-Cheng Kang, Eric Kim, Jure Leskovec, Charles Rosenberg, Julian Mcauley, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionWang-Cheng Kang, Eric Kim, Jure Leskovec, Charles Rosenberg, and Julian McAuley. Complete the look: Scene- based complementary product recommendation. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 10532-10541, 2019. 2\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, arXiv:1412.6980arXiv preprintDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 5\n\nImproving web image search results using query-relative classifiers. Josip Krapac, M Allan, J Verbeek, F Jurie, IEEE Computer Society Conference on Computer Vision and Pattern Recognition. Josip Krapac, M. Allan, J. Verbeek, and F. Jurie. Improv- ing web image search results using query-relative classifiers. 2010 IEEE Computer Society Conference on Computer Vi- sion and Pattern Recognition, pages 1094-1101, 2010. 2\n\nVisual genome: Connecting language and vision using crowdsourced dense image annotations. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, International journal of computer vision. 1231Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan- tidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123(1):32-73, 2017. 8\n\nFashion retrieval via graph reasoning networks on a similarity pyramid. Zhanghui Kuang, Yiming Gao, Guanbin Li, Ping Luo, Yimin Chen, Liang Lin, Wayne Zhang, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionZhanghui Kuang, Yiming Gao, Guanbin Li, Ping Luo, Yimin Chen, Liang Lin, and Wayne Zhang. Fashion retrieval via graph reasoning networks on a similarity pyramid. In Pro- ceedings of the IEEE International Conference on Computer Vision, pages 3066-3075, 2019. 2\n\nVisualbert: A simple and performant baseline for vision and language. Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, Kai-Wei Chang, arXiv:1908.0355756arXiv preprintLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple and perfor- mant baseline for vision and language. arXiv preprint arXiv:1908.03557, 2019. 2, 3, 5, 6\n\nRegional maximum activations of convolutions with attention for cross-domain beauty and personal care product retrieval. Zehang Lin, Zhenguo Yang, Feitao Huang, Junhong Chen, Proceedings of the 26th ACM international conference on Multimedia. the 26th ACM international conference on MultimediaZehang Lin, Zhenguo Yang, Feitao Huang, and Junhong Chen. Regional maximum activations of convolutions with attention for cross-domain beauty and personal care product retrieval. In Proceedings of the 26th ACM international con- ference on Multimedia, pages 2073-2077, 2018. 2\n\nVilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee, Advances in Neural Information Processing Systems. 56Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vil- bert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Advances in Neural Infor- mation Processing Systems, pages 13-23, 2019. 2, 3, 5, 6\n\nGrabcut in one cut. Tang Meng, Lena Gorelick, Olga Veksler, Yuri Boykov, IEEE International Conference on Computer Vision. Tang Meng, Lena Gorelick, Olga Veksler, and Yuri Boykov. Grabcut in one cut. In IEEE International Conference on Computer Vision, 2014. 4\n\nLarge-scale image retrieval with attentive deep local features. Hyeonwoo Noh, Andre Araujo, Jack Sim, Tobias Weyand, Bohyung Han, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionHyeonwoo Noh, Andre Araujo, Jack Sim, Tobias Weyand, and Bohyung Han. Large-scale image retrieval with attentive deep local features. In Proceedings of the IEEE international conference on computer vision, pages 3456-3465, 2017. 2\n\nProduct retrieval for grocery stores. Petteri Nurmi, Eemil Lagerspetz, Wray Buntine, Patrik Flor\u00e9en, Joonas Kukkonen, Proceedings of the 31st annual international ACM SIGIR conference on Research and development in information retrieval. the 31st annual international ACM SIGIR conference on Research and development in information retrievalPetteri Nurmi, Eemil Lagerspetz, Wray Buntine, Patrik Flor\u00e9en, and Joonas Kukkonen. Product retrieval for gro- cery stores. In Proceedings of the 31st annual international ACM SIGIR conference on Research and development in in- formation retrieval, pages 781-782, 2008. 2\n\nMedical image retrieval using deep convolutional neural network. Adnan Qayyum, Muhammad Syed, Muhammad Anwar, Muhammad Awais, Majid, Neurocomputing. 2662Adnan Qayyum, Syed Muhammad Anwar, Muhammad Awais, and Muhammad Majid. Medical image retrieval us- ing deep convolutional neural network. Neurocomputing, 266:8-20, 2017. 2\n\nSketch-based image retrieval via siamese convolutional neural network. Yonggang Qi, Yi-Zhe Song, Honggang Zhang, Jun Liu, 2016 IEEE International Conference on Image Processing (ICIP). Yonggang Qi, Yi-Zhe Song, Honggang Zhang, and Jun Liu. Sketch-based image retrieval via siamese convolutional neu- ral network. In 2016 IEEE International Conference on Im- age Processing (ICIP), pages 2460-2464. IEEE, 2016. 2\n\nLearning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, 56Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. 5, 6\n\nA systematic review of recommender system for e-portfolio domain. Puji Rahayu, Dana Indra Sensuse, Betty Purwandari, Indra Budi, N Khalid, Zulkarnaim, Proceedings of the 5th International Conference on Information and Education Technology. the 5th International Conference on Information and Education TechnologyPuji Rahayu, Dana Indra Sensuse, Betty Purwandari, Indra Budi, F Khalid, and N Zulkarnaim. A systematic review of recommender system for e-portfolio domain. In Proceedings of the 5th International Conference on Information and Ed- ucation Technology, pages 21-26, 2017. 1\n\nFaster r-cnn: Towards real-time object detection with region proposal networks. Kaiming Shaoqing Ren, Ross He, Jian Girshick, Sun, Advances in neural information processing systems. 4Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information pro- cessing systems, pages 91-99, 2015. 4, 5, 8\n\nGenerative adversarial learning towards fast weakly supervised detection. Yunhan Shen, Rongrong Ji, Shengchuan Zhang, Wangmeng Zuo, Yan Wang, IEEE/CVF Conference on Computer Vision & Pattern Recognition. Yunhan Shen, Rongrong Ji, Shengchuan Zhang, Wangmeng Zuo, and Yan Wang. Generative adversarial learning towards fast weakly supervised detection. In IEEE/CVF Conference on Computer Vision & Pattern Recognition, 2018. 2\n\nVl-bert: Pre-training of generic visuallinguistic representations. Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, Jifeng Dai, International Conference on Learning Representations. 36Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training of generic visual- linguistic representations. In International Conference on Learning Representations, 2020. 3, 6\n\nLxmert: Learning crossmodality encoder representations from transformers. Hao Tan, Mohit Bansal, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. the 2019 Conference on Empirical Methods in Natural Language Processing56Hao Tan and Mohit Bansal. Lxmert: Learning cross- modality encoder representations from transformers. In Pro- ceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, 2019. 2, 3, 5, 6\n\nPcl: Proposal cluster learning for weakly supervised object detection. Peng Tang, Xinggang Wang, Song Bai, Wei Shen, Xiang Bai, Wenyu Liu, Alan Yuille, IEEE transactions on pattern analysis and machine intelligence. 42Peng Tang, Xinggang Wang, Song Bai, Wei Shen, Xiang Bai, Wenyu Liu, and Alan Yuille. Pcl: Proposal cluster learning for weakly supervised object detection. IEEE transactions on pattern analysis and machine intelligence, 42(1):176- 191, 2018. 2\n\nVisualizing data using t-sne. Laurens Van Der Maaten, Geoffrey Hinton, Journal of machine learning research. 911Laurens Van der Maaten and Geoffrey Hinton. Visualiz- ing data using t-sne. Journal of machine learning research, 9(11), 2008. 7\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017. 3\n\nCancer drugs in 16 european countries, australia, and new zealand: a cross-country price comparison study. Sabine Vogler, Agnes Vitry, The Lancet Oncology. 171Sabine Vogler, Agnes Vitry, et al. Cancer drugs in 16 euro- pean countries, australia, and new zealand: a cross-country price comparison study. The Lancet Oncology, 17(1):39-47, 2016. 1\n\nAdversarial cross-modal retrieval. Bokun Wang, Yang Yang, Xing Xu, Alan Hanjalic, Heng Tao Shen, Proceedings of the 25th ACM international conference on Multimedia. the 25th ACM international conference on MultimediaBokun Wang, Yang Yang, Xing Xu, Alan Hanjalic, and Heng Tao Shen. Adversarial cross-modal retrieval. In Pro- ceedings of the 25th ACM international conference on Mul- timedia, pages 154-162, 2017. 2\n\nJoint feature selection and subspace learning for crossmodal retrieval. Kaiye Wang, Ran He, Liang Wang, Wei Wang, Tieniu Tan, IEEE transactions on pattern analysis and machine intelligence. 38Kaiye Wang, Ran He, Liang Wang, Wei Wang, and Tieniu Tan. Joint feature selection and subspace learning for cross- modal retrieval. IEEE transactions on pattern analysis and machine intelligence, 38(10):2010-2023, 2015. 2\n\nA comprehensive survey on cross-modal retrieval. Kaiye Wang, Qiyue Yin, Wei Wang, Shu Wu, Liang Wang, arXiv:1607.06215arXiv preprintKaiye Wang, Qiyue Yin, Wei Wang, Shu Wu, and Liang Wang. A comprehensive survey on cross-modal retrieval. arXiv preprint arXiv:1607.06215, 2016. 1\n\nEffective deep learning-based multi-modal retrieval. Wei Wang, Xiaoyan Yang, Beng Chin Ooi, Dongxiang Zhang, Yueting Zhuang, The VLDB Journal. 251Wei Wang, Xiaoyan Yang, Beng Chin Ooi, Dongxiang Zhang, and Yueting Zhuang. Effective deep learning-based multi-modal retrieval. The VLDB Journal, 25(1):79-101, 2016. 2\n\nXiu-Shen Wei, Quan Cui, Lei Yang, Peng Wang, Lingqiao Liu, arXiv:1901.07249Rpc: A large-scale retail product checkout dataset. arXiv preprintXiu-Shen Wei, Quan Cui, Lei Yang, Peng Wang, and Lingqiao Liu. Rpc: A large-scale retail product checkout dataset. arXiv preprint arXiv:1901.07249, 2019. 2\n\nCross-modal retrieval with cnn visual features: A new baseline. Yunchao Wei, Yao Zhao, Canyi Lu, Shikui Wei, Luoqi Liu, Zhenfeng Zhu, Shuicheng Yan, IEEE transactions on cybernetics. 472Yunchao Wei, Yao Zhao, Canyi Lu, Shikui Wei, Luoqi Liu, Zhenfeng Zhu, and Shuicheng Yan. Cross-modal retrieval with cnn visual features: A new baseline. IEEE transactions on cybernetics, 47(2):449-460, 2016. 2\n\nGoogle Landmarks Dataset v2 -A Large-Scale Benchmark for Instance-Level Recognition and Retrieval. T Weyand, A Araujo, B Cao, J Sim, Proc. CVPR. CVPRT. Weyand, A. Araujo, B. Cao, and J. Sim. Google Land- marks Dataset v2 -A Large-Scale Benchmark for Instance- Level Recognition and Retrieval. In Proc. CVPR, 2020. 6\n\nCap2det: Learning to amplify weak caption supervision for object detection. Keren Ye, Mingda Zhang, Adriana Kovashka, Wei Li, Danfeng Qin, Jesse Berent, Keren Ye, Mingda Zhang, Adriana Kovashka, Wei Li, Dan- feng Qin, and Jesse Berent. Cap2det: Learning to amplify weak caption supervision for object detection. 2019. 2\n\nPlaces: A 10 million image database for scene recognition. Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, Antonio Torralba, IEEE Transactions on Pattern Analysis and Machine Intelligence. Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE Transactions on Pattern Analy- sis and Machine Intelligence, 2017. 4\n", "annotations": {"author": "[{\"end\":138,\"start\":101},{\"end\":141,\"start\":139},{\"end\":178,\"start\":142},{\"end\":214,\"start\":179},{\"end\":257,\"start\":215},{\"end\":285,\"start\":258},{\"end\":340,\"start\":286},{\"end\":373,\"start\":341},{\"end\":434,\"start\":374}]", "publisher": null, "author_last_name": "[{\"end\":112,\"start\":108},{\"end\":152,\"start\":150},{\"end\":188,\"start\":184},{\"end\":226,\"start\":223},{\"end\":268,\"start\":266},{\"end\":297,\"start\":292},{\"end\":348,\"start\":346},{\"end\":387,\"start\":382}]", "author_first_name": "[{\"end\":107,\"start\":101},{\"end\":140,\"start\":139},{\"end\":149,\"start\":142},{\"end\":183,\"start\":179},{\"end\":222,\"start\":215},{\"end\":265,\"start\":258},{\"end\":291,\"start\":286},{\"end\":345,\"start\":341},{\"end\":381,\"start\":374}]", "author_affiliation": "[{\"end\":137,\"start\":114},{\"end\":177,\"start\":154},{\"end\":213,\"start\":190},{\"end\":256,\"start\":228},{\"end\":284,\"start\":270},{\"end\":339,\"start\":325},{\"end\":372,\"start\":350},{\"end\":433,\"start\":410}]", "title": "[{\"end\":98,\"start\":1},{\"end\":532,\"start\":435}]", "venue": null, "abstract": "[{\"end\":2306,\"start\":534}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2808,\"start\":2804},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":2848,\"start\":2844},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":3041,\"start\":3037},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":3317,\"start\":3313},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3359,\"start\":3355},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3391,\"start\":3387},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3424,\"start\":3421},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4455,\"start\":4451},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4457,\"start\":4455},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":4460,\"start\":4457},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4463,\"start\":4460},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":4493,\"start\":4489},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":4496,\"start\":4493},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":4499,\"start\":4496},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4501,\"start\":4499},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":4504,\"start\":4501},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4506,\"start\":4504},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":4928,\"start\":4924},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4931,\"start\":4928},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6246,\"start\":6242},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6249,\"start\":6246},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":6251,\"start\":6249},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":6254,\"start\":6251},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7268,\"start\":7264},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7270,\"start\":7268},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7348,\"start\":7344},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":7384,\"start\":7380},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7417,\"start\":7413},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":7420,\"start\":7417},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7448,\"start\":7444},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7451,\"start\":7448},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":7454,\"start\":7451},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7456,\"start\":7454},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7459,\"start\":7456},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":7461,\"start\":7459},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":7619,\"start\":7616},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":7622,\"start\":7619},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":7639,\"start\":7635},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7641,\"start\":7639},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7688,\"start\":7684},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7691,\"start\":7688},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":7693,\"start\":7691},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7696,\"start\":7693},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7699,\"start\":7696},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7927,\"start\":7923},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":7930,\"start\":7927},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":7933,\"start\":7930},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8047,\"start\":8043},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8186,\"start\":8182},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":8243,\"start\":8239},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8639,\"start\":8635},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":8669,\"start\":8666},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":8756,\"start\":8752},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8759,\"start\":8756},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":8761,\"start\":8759},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":8863,\"start\":8859},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8866,\"start\":8863},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":14722,\"start\":14718},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":14815,\"start\":14811},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":14926,\"start\":14922},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":14962,\"start\":14958},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":15189,\"start\":15185},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":16817,\"start\":16814},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":16837,\"start\":16833},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":17036,\"start\":17033},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":17301,\"start\":17297},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":17303,\"start\":17301},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":17306,\"start\":17303},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":17309,\"start\":17306},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":18171,\"start\":18168},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":18174,\"start\":18171},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":20161,\"start\":20157},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":20234,\"start\":20230},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":20255,\"start\":20252},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21061,\"start\":21057},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":21169,\"start\":21165},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":21171,\"start\":21169},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":22621,\"start\":22617},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":23147,\"start\":23143},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":23160,\"start\":23156},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25545,\"start\":25542},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25547,\"start\":25545},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":25549,\"start\":25547},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":27081,\"start\":27077},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":28576,\"start\":28572},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":28610,\"start\":28606}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":30917,\"start\":30829},{\"attributes\":{\"id\":\"fig_1\"},\"end\":31363,\"start\":30918},{\"attributes\":{\"id\":\"fig_2\"},\"end\":31777,\"start\":31364},{\"attributes\":{\"id\":\"fig_4\"},\"end\":31941,\"start\":31778},{\"attributes\":{\"id\":\"fig_6\"},\"end\":32131,\"start\":31942},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":32560,\"start\":32132}]", "paragraph": "[{\"end\":3075,\"start\":2322},{\"end\":3885,\"start\":3077},{\"end\":4239,\"start\":3887},{\"end\":5738,\"start\":4241},{\"end\":6801,\"start\":5740},{\"end\":7192,\"start\":6803},{\"end\":8729,\"start\":7209},{\"end\":9130,\"start\":8731},{\"end\":9334,\"start\":9190},{\"end\":9556,\"start\":9440},{\"end\":9696,\"start\":9630},{\"end\":10853,\"start\":9719},{\"end\":11517,\"start\":10881},{\"end\":11641,\"start\":11538},{\"end\":11871,\"start\":11782},{\"end\":12258,\"start\":12170},{\"end\":12297,\"start\":12279},{\"end\":13573,\"start\":12412},{\"end\":14117,\"start\":13633},{\"end\":15419,\"start\":14162},{\"end\":16493,\"start\":15455},{\"end\":17900,\"start\":16536},{\"end\":18587,\"start\":17944},{\"end\":19203,\"start\":18680},{\"end\":20087,\"start\":19246},{\"end\":21400,\"start\":20128},{\"end\":22881,\"start\":21447},{\"end\":23456,\"start\":22883},{\"end\":24610,\"start\":23505},{\"end\":25808,\"start\":24644},{\"end\":26664,\"start\":25847},{\"end\":27210,\"start\":26666},{\"end\":28129,\"start\":27254},{\"end\":29760,\"start\":28178},{\"end\":30810,\"start\":29775}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9439,\"start\":9335},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9629,\"start\":9557},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11707,\"start\":11642},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12006,\"start\":11977},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12120,\"start\":12073},{\"attributes\":{\"id\":\"formula_5\"},\"end\":12319,\"start\":12298},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13618,\"start\":13597},{\"attributes\":{\"id\":\"formula_7\"},\"end\":18679,\"start\":18588}]", "table_ref": "[{\"end\":3495,\"start\":3488},{\"end\":10723,\"start\":10716},{\"end\":16492,\"start\":16485},{\"end\":17899,\"start\":17892},{\"end\":21546,\"start\":21539},{\"end\":22487,\"start\":22480},{\"end\":22660,\"start\":22653},{\"end\":23524,\"start\":23517},{\"end\":24755,\"start\":24747},{\"end\":25430,\"start\":25423},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":26281,\"start\":26274},{\"end\":26439,\"start\":26432},{\"end\":27753,\"start\":27746},{\"end\":28328,\"start\":28321}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2320,\"start\":2308},{\"attributes\":{\"n\":\"2.\"},\"end\":7207,\"start\":7195},{\"attributes\":{\"n\":\"3.\"},\"end\":9170,\"start\":9133},{\"attributes\":{\"n\":\"3.1.\"},\"end\":9188,\"start\":9173},{\"attributes\":{\"n\":\"3.2.\"},\"end\":9717,\"start\":9699},{\"attributes\":{\"n\":\"3.3.\"},\"end\":10879,\"start\":10856},{\"end\":11536,\"start\":11520},{\"end\":11780,\"start\":11709},{\"end\":11924,\"start\":11874},{\"end\":11976,\"start\":11927},{\"end\":12016,\"start\":12008},{\"end\":12028,\"start\":12019},{\"end\":12053,\"start\":12031},{\"end\":12072,\"start\":12056},{\"end\":12137,\"start\":12122},{\"end\":12168,\"start\":12140},{\"end\":12277,\"start\":12261},{\"end\":12349,\"start\":12321},{\"end\":12410,\"start\":12352},{\"end\":13579,\"start\":13576},{\"end\":13596,\"start\":13582},{\"attributes\":{\"n\":\"4.\"},\"end\":13631,\"start\":13620},{\"attributes\":{\"n\":\"4.1.\"},\"end\":14160,\"start\":14120},{\"attributes\":{\"n\":\"4.2.\"},\"end\":15453,\"start\":15422},{\"attributes\":{\"n\":\"4.3.\"},\"end\":16534,\"start\":16496},{\"attributes\":{\"n\":\"4.4.\"},\"end\":17942,\"start\":17903},{\"attributes\":{\"n\":\"4.5.\"},\"end\":19244,\"start\":19206},{\"attributes\":{\"n\":\"5.\"},\"end\":20101,\"start\":20090},{\"attributes\":{\"n\":\"5.1.\"},\"end\":20126,\"start\":20104},{\"attributes\":{\"n\":\"5.2.\"},\"end\":21445,\"start\":21403},{\"attributes\":{\"n\":\"5.3.\"},\"end\":23503,\"start\":23459},{\"attributes\":{\"n\":\"5.4.\"},\"end\":24642,\"start\":24613},{\"attributes\":{\"n\":\"5.5.\"},\"end\":25845,\"start\":25811},{\"attributes\":{\"n\":\"5.6.\"},\"end\":27252,\"start\":27213},{\"attributes\":{\"n\":\"5.7.\"},\"end\":28176,\"start\":28132},{\"attributes\":{\"n\":\"6.\"},\"end\":29773,\"start\":29763},{\"attributes\":{\"n\":\"7.\"},\"end\":30828,\"start\":30813},{\"end\":30840,\"start\":30830},{\"end\":30929,\"start\":30919},{\"end\":31375,\"start\":31365},{\"end\":31789,\"start\":31779},{\"end\":31953,\"start\":31943},{\"end\":32142,\"start\":32133}]", "table": "[{\"end\":32560,\"start\":32239}]", "figure_caption": "[{\"end\":30917,\"start\":30842},{\"end\":31363,\"start\":30931},{\"end\":31777,\"start\":31377},{\"end\":31941,\"start\":31791},{\"end\":32131,\"start\":31955},{\"end\":32239,\"start\":32144}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":3989,\"start\":3981},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10736,\"start\":10728},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":11081,\"start\":11073},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12575,\"start\":12567},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13149,\"start\":13141},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":13491,\"start\":13476},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13659,\"start\":13651},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14785,\"start\":14777},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15842,\"start\":15834},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":27094,\"start\":27085}]", "bib_author_first_name": "[{\"end\":32874,\"start\":32870},{\"end\":32884,\"start\":32880},{\"end\":32897,\"start\":32892},{\"end\":32910,\"start\":32903},{\"end\":32927,\"start\":32918},{\"end\":33216,\"start\":33210},{\"end\":33228,\"start\":33227},{\"end\":33230,\"start\":33229},{\"end\":33246,\"start\":33240},{\"end\":33248,\"start\":33247},{\"end\":33500,\"start\":33496},{\"end\":33511,\"start\":33506},{\"end\":33522,\"start\":33516},{\"end\":33745,\"start\":33742},{\"end\":33760,\"start\":33751},{\"end\":33774,\"start\":33767},{\"end\":33786,\"start\":33781},{\"end\":33801,\"start\":33793},{\"end\":34321,\"start\":34317},{\"end\":34333,\"start\":34328},{\"end\":34353,\"start\":34345},{\"end\":34371,\"start\":34363},{\"end\":34724,\"start\":34716},{\"end\":34737,\"start\":34731},{\"end\":34749,\"start\":34742},{\"end\":34759,\"start\":34754},{\"end\":34762,\"start\":34760},{\"end\":34776,\"start\":34770},{\"end\":34787,\"start\":34784},{\"end\":34795,\"start\":34793},{\"end\":34811,\"start\":34803},{\"end\":35202,\"start\":35195},{\"end\":35217,\"start\":35213},{\"end\":35239,\"start\":35230},{\"end\":35253,\"start\":35246},{\"end\":35733,\"start\":35728},{\"end\":35747,\"start\":35740},{\"end\":35763,\"start\":35754},{\"end\":35774,\"start\":35769},{\"end\":35787,\"start\":35780},{\"end\":36037,\"start\":36032},{\"end\":36054,\"start\":36046},{\"end\":36068,\"start\":36062},{\"end\":36082,\"start\":36074},{\"end\":36092,\"start\":36083},{\"end\":36483,\"start\":36474},{\"end\":36498,\"start\":36493},{\"end\":36513,\"start\":36506},{\"end\":36928,\"start\":36922},{\"end\":36943,\"start\":36936},{\"end\":37369,\"start\":37360},{\"end\":37383,\"start\":37376},{\"end\":37396,\"start\":37390},{\"end\":37803,\"start\":37795},{\"end\":37816,\"start\":37808},{\"end\":37828,\"start\":37823},{\"end\":37838,\"start\":37835},{\"end\":37849,\"start\":37845},{\"end\":37861,\"start\":37856},{\"end\":37863,\"start\":37862},{\"end\":38240,\"start\":38235},{\"end\":38437,\"start\":38430},{\"end\":38449,\"start\":38442},{\"end\":38809,\"start\":38802},{\"end\":38821,\"start\":38814},{\"end\":38837,\"start\":38829},{\"end\":38847,\"start\":38843},{\"end\":39288,\"start\":39282},{\"end\":39298,\"start\":39293},{\"end\":39308,\"start\":39306},{\"end\":39323,\"start\":39315},{\"end\":39612,\"start\":39609},{\"end\":39620,\"start\":39617},{\"end\":39633,\"start\":39627},{\"end\":39645,\"start\":39641},{\"end\":40063,\"start\":40053},{\"end\":40074,\"start\":40070},{\"end\":40088,\"start\":40081},{\"end\":40101,\"start\":40095},{\"end\":40483,\"start\":40473},{\"end\":40494,\"start\":40490},{\"end\":40504,\"start\":40500},{\"end\":40522,\"start\":40515},{\"end\":40540,\"start\":40534},{\"end\":40999,\"start\":40998},{\"end\":41015,\"start\":41010},{\"end\":41248,\"start\":41243},{\"end\":41258,\"start\":41257},{\"end\":41267,\"start\":41266},{\"end\":41278,\"start\":41277},{\"end\":41690,\"start\":41684},{\"end\":41704,\"start\":41700},{\"end\":41716,\"start\":41710},{\"end\":41730,\"start\":41724},{\"end\":41745,\"start\":41740},{\"end\":41758,\"start\":41752},{\"end\":41777,\"start\":41768},{\"end\":41790,\"start\":41784},{\"end\":41809,\"start\":41803},{\"end\":41819,\"start\":41814},{\"end\":41821,\"start\":41820},{\"end\":42266,\"start\":42258},{\"end\":42280,\"start\":42274},{\"end\":42293,\"start\":42286},{\"end\":42302,\"start\":42298},{\"end\":42313,\"start\":42308},{\"end\":42325,\"start\":42320},{\"end\":42336,\"start\":42331},{\"end\":42811,\"start\":42797},{\"end\":42820,\"start\":42816},{\"end\":42832,\"start\":42830},{\"end\":42845,\"start\":42838},{\"end\":42860,\"start\":42853},{\"end\":43225,\"start\":43219},{\"end\":43238,\"start\":43231},{\"end\":43251,\"start\":43245},{\"end\":43266,\"start\":43259},{\"end\":43774,\"start\":43768},{\"end\":43784,\"start\":43779},{\"end\":43796,\"start\":43792},{\"end\":43811,\"start\":43805},{\"end\":44134,\"start\":44130},{\"end\":44145,\"start\":44141},{\"end\":44160,\"start\":44156},{\"end\":44174,\"start\":44170},{\"end\":44444,\"start\":44436},{\"end\":44455,\"start\":44450},{\"end\":44468,\"start\":44464},{\"end\":44480,\"start\":44474},{\"end\":44496,\"start\":44489},{\"end\":44900,\"start\":44893},{\"end\":44913,\"start\":44908},{\"end\":44930,\"start\":44926},{\"end\":44946,\"start\":44940},{\"end\":44962,\"start\":44956},{\"end\":45539,\"start\":45534},{\"end\":45556,\"start\":45548},{\"end\":45571,\"start\":45563},{\"end\":45587,\"start\":45579},{\"end\":45874,\"start\":45866},{\"end\":45885,\"start\":45879},{\"end\":45900,\"start\":45892},{\"end\":45911,\"start\":45908},{\"end\":46283,\"start\":46279},{\"end\":46297,\"start\":46293},{\"end\":46302,\"start\":46298},{\"end\":46313,\"start\":46308},{\"end\":46329,\"start\":46323},{\"end\":46345,\"start\":46338},{\"end\":46359,\"start\":46351},{\"end\":46375,\"start\":46369},{\"end\":46390,\"start\":46384},{\"end\":46405,\"start\":46399},{\"end\":46419,\"start\":46415},{\"end\":46435,\"start\":46427},{\"end\":46449,\"start\":46445},{\"end\":46802,\"start\":46798},{\"end\":46815,\"start\":46811},{\"end\":46821,\"start\":46816},{\"end\":46836,\"start\":46831},{\"end\":46854,\"start\":46849},{\"end\":46862,\"start\":46861},{\"end\":47404,\"start\":47397},{\"end\":47423,\"start\":47419},{\"end\":47432,\"start\":47428},{\"end\":47799,\"start\":47793},{\"end\":47814,\"start\":47806},{\"end\":47829,\"start\":47819},{\"end\":47845,\"start\":47837},{\"end\":47854,\"start\":47851},{\"end\":48216,\"start\":48210},{\"end\":48227,\"start\":48221},{\"end\":48236,\"start\":48233},{\"end\":48245,\"start\":48242},{\"end\":48255,\"start\":48250},{\"end\":48264,\"start\":48260},{\"end\":48276,\"start\":48270},{\"end\":48629,\"start\":48626},{\"end\":48640,\"start\":48635},{\"end\":49098,\"start\":49094},{\"end\":49113,\"start\":49105},{\"end\":49124,\"start\":49120},{\"end\":49133,\"start\":49130},{\"end\":49145,\"start\":49140},{\"end\":49156,\"start\":49151},{\"end\":49166,\"start\":49162},{\"end\":49523,\"start\":49516},{\"end\":49548,\"start\":49540},{\"end\":49761,\"start\":49755},{\"end\":49775,\"start\":49771},{\"end\":49789,\"start\":49785},{\"end\":49803,\"start\":49798},{\"end\":49820,\"start\":49815},{\"end\":49833,\"start\":49828},{\"end\":49835,\"start\":49834},{\"end\":49849,\"start\":49843},{\"end\":49863,\"start\":49858},{\"end\":50274,\"start\":50268},{\"end\":50288,\"start\":50283},{\"end\":50547,\"start\":50542},{\"end\":50558,\"start\":50554},{\"end\":50569,\"start\":50565},{\"end\":50578,\"start\":50574},{\"end\":50597,\"start\":50589},{\"end\":51000,\"start\":50995},{\"end\":51010,\"start\":51007},{\"end\":51020,\"start\":51015},{\"end\":51030,\"start\":51027},{\"end\":51043,\"start\":51037},{\"end\":51392,\"start\":51387},{\"end\":51404,\"start\":51399},{\"end\":51413,\"start\":51410},{\"end\":51423,\"start\":51420},{\"end\":51433,\"start\":51428},{\"end\":51674,\"start\":51671},{\"end\":51688,\"start\":51681},{\"end\":51699,\"start\":51695},{\"end\":51704,\"start\":51700},{\"end\":51719,\"start\":51710},{\"end\":51734,\"start\":51727},{\"end\":51942,\"start\":51934},{\"end\":51952,\"start\":51948},{\"end\":51961,\"start\":51958},{\"end\":51972,\"start\":51968},{\"end\":51987,\"start\":51979},{\"end\":52303,\"start\":52296},{\"end\":52312,\"start\":52309},{\"end\":52324,\"start\":52319},{\"end\":52335,\"start\":52329},{\"end\":52346,\"start\":52341},{\"end\":52360,\"start\":52352},{\"end\":52375,\"start\":52366},{\"end\":52729,\"start\":52728},{\"end\":52739,\"start\":52738},{\"end\":52749,\"start\":52748},{\"end\":52756,\"start\":52755},{\"end\":53027,\"start\":53022},{\"end\":53038,\"start\":53032},{\"end\":53053,\"start\":53046},{\"end\":53067,\"start\":53064},{\"end\":53079,\"start\":53072},{\"end\":53090,\"start\":53085},{\"end\":53331,\"start\":53326},{\"end\":53343,\"start\":53338},{\"end\":53361,\"start\":53355},{\"end\":53374,\"start\":53370},{\"end\":53389,\"start\":53382}]", "bib_author_last_name": "[{\"end\":32878,\"start\":32875},{\"end\":32890,\"start\":32885},{\"end\":32901,\"start\":32898},{\"end\":32916,\"start\":32911},{\"end\":32932,\"start\":32928},{\"end\":33225,\"start\":33217},{\"end\":33238,\"start\":33231},{\"end\":33255,\"start\":33249},{\"end\":33264,\"start\":33257},{\"end\":33504,\"start\":33501},{\"end\":33514,\"start\":33512},{\"end\":33527,\"start\":33523},{\"end\":33749,\"start\":33746},{\"end\":33765,\"start\":33761},{\"end\":33779,\"start\":33775},{\"end\":33791,\"start\":33787},{\"end\":33804,\"start\":33802},{\"end\":34326,\"start\":34322},{\"end\":34343,\"start\":34334},{\"end\":34361,\"start\":34354},{\"end\":34378,\"start\":34372},{\"end\":34729,\"start\":34725},{\"end\":34740,\"start\":34738},{\"end\":34752,\"start\":34750},{\"end\":34768,\"start\":34763},{\"end\":34782,\"start\":34777},{\"end\":34791,\"start\":34788},{\"end\":34801,\"start\":34796},{\"end\":34815,\"start\":34812},{\"end\":35211,\"start\":35203},{\"end\":35228,\"start\":35218},{\"end\":35244,\"start\":35240},{\"end\":35260,\"start\":35254},{\"end\":35738,\"start\":35734},{\"end\":35752,\"start\":35748},{\"end\":35767,\"start\":35764},{\"end\":35778,\"start\":35775},{\"end\":35791,\"start\":35788},{\"end\":36044,\"start\":36038},{\"end\":36060,\"start\":36055},{\"end\":36072,\"start\":36069},{\"end\":36097,\"start\":36093},{\"end\":36491,\"start\":36484},{\"end\":36504,\"start\":36499},{\"end\":36520,\"start\":36514},{\"end\":36934,\"start\":36929},{\"end\":36951,\"start\":36944},{\"end\":37374,\"start\":37370},{\"end\":37388,\"start\":37384},{\"end\":37399,\"start\":37397},{\"end\":37806,\"start\":37804},{\"end\":37821,\"start\":37817},{\"end\":37833,\"start\":37829},{\"end\":37843,\"start\":37839},{\"end\":37854,\"start\":37850},{\"end\":37875,\"start\":37864},{\"end\":38247,\"start\":38241},{\"end\":38440,\"start\":38438},{\"end\":38458,\"start\":38450},{\"end\":38812,\"start\":38810},{\"end\":38827,\"start\":38822},{\"end\":38841,\"start\":38838},{\"end\":38851,\"start\":38848},{\"end\":39291,\"start\":39289},{\"end\":39304,\"start\":39299},{\"end\":39313,\"start\":39309},{\"end\":39329,\"start\":39324},{\"end\":39615,\"start\":39613},{\"end\":39625,\"start\":39621},{\"end\":39639,\"start\":39634},{\"end\":39650,\"start\":39646},{\"end\":40068,\"start\":40064},{\"end\":40079,\"start\":40075},{\"end\":40093,\"start\":40089},{\"end\":40109,\"start\":40102},{\"end\":40488,\"start\":40484},{\"end\":40498,\"start\":40495},{\"end\":40513,\"start\":40505},{\"end\":40532,\"start\":40523},{\"end\":40548,\"start\":40541},{\"end\":41008,\"start\":41000},{\"end\":41022,\"start\":41016},{\"end\":41026,\"start\":41024},{\"end\":41255,\"start\":41249},{\"end\":41264,\"start\":41259},{\"end\":41275,\"start\":41268},{\"end\":41284,\"start\":41279},{\"end\":41698,\"start\":41691},{\"end\":41708,\"start\":41705},{\"end\":41722,\"start\":41717},{\"end\":41738,\"start\":41731},{\"end\":41750,\"start\":41746},{\"end\":41766,\"start\":41759},{\"end\":41782,\"start\":41778},{\"end\":41801,\"start\":41791},{\"end\":41812,\"start\":41810},{\"end\":41828,\"start\":41822},{\"end\":42272,\"start\":42267},{\"end\":42284,\"start\":42281},{\"end\":42296,\"start\":42294},{\"end\":42306,\"start\":42303},{\"end\":42318,\"start\":42314},{\"end\":42329,\"start\":42326},{\"end\":42342,\"start\":42337},{\"end\":42814,\"start\":42812},{\"end\":42828,\"start\":42821},{\"end\":42836,\"start\":42833},{\"end\":42851,\"start\":42846},{\"end\":42866,\"start\":42861},{\"end\":43229,\"start\":43226},{\"end\":43243,\"start\":43239},{\"end\":43257,\"start\":43252},{\"end\":43271,\"start\":43267},{\"end\":43777,\"start\":43775},{\"end\":43790,\"start\":43785},{\"end\":43803,\"start\":43797},{\"end\":43815,\"start\":43812},{\"end\":44139,\"start\":44135},{\"end\":44154,\"start\":44146},{\"end\":44168,\"start\":44161},{\"end\":44181,\"start\":44175},{\"end\":44448,\"start\":44445},{\"end\":44462,\"start\":44456},{\"end\":44472,\"start\":44469},{\"end\":44487,\"start\":44481},{\"end\":44500,\"start\":44497},{\"end\":44906,\"start\":44901},{\"end\":44924,\"start\":44914},{\"end\":44938,\"start\":44931},{\"end\":44954,\"start\":44947},{\"end\":44971,\"start\":44963},{\"end\":45546,\"start\":45540},{\"end\":45561,\"start\":45557},{\"end\":45577,\"start\":45572},{\"end\":45593,\"start\":45588},{\"end\":45600,\"start\":45595},{\"end\":45877,\"start\":45875},{\"end\":45890,\"start\":45886},{\"end\":45906,\"start\":45901},{\"end\":45915,\"start\":45912},{\"end\":46291,\"start\":46284},{\"end\":46306,\"start\":46303},{\"end\":46321,\"start\":46314},{\"end\":46336,\"start\":46330},{\"end\":46349,\"start\":46346},{\"end\":46367,\"start\":46360},{\"end\":46382,\"start\":46376},{\"end\":46397,\"start\":46391},{\"end\":46413,\"start\":46406},{\"end\":46425,\"start\":46420},{\"end\":46443,\"start\":46436},{\"end\":46459,\"start\":46450},{\"end\":46809,\"start\":46803},{\"end\":46829,\"start\":46822},{\"end\":46847,\"start\":46837},{\"end\":46859,\"start\":46855},{\"end\":46869,\"start\":46863},{\"end\":46881,\"start\":46871},{\"end\":47417,\"start\":47405},{\"end\":47426,\"start\":47424},{\"end\":47441,\"start\":47433},{\"end\":47446,\"start\":47443},{\"end\":47804,\"start\":47800},{\"end\":47817,\"start\":47815},{\"end\":47835,\"start\":47830},{\"end\":47849,\"start\":47846},{\"end\":47859,\"start\":47855},{\"end\":48219,\"start\":48217},{\"end\":48231,\"start\":48228},{\"end\":48240,\"start\":48237},{\"end\":48248,\"start\":48246},{\"end\":48258,\"start\":48256},{\"end\":48268,\"start\":48265},{\"end\":48280,\"start\":48277},{\"end\":48633,\"start\":48630},{\"end\":48647,\"start\":48641},{\"end\":49103,\"start\":49099},{\"end\":49118,\"start\":49114},{\"end\":49128,\"start\":49125},{\"end\":49138,\"start\":49134},{\"end\":49149,\"start\":49146},{\"end\":49160,\"start\":49157},{\"end\":49173,\"start\":49167},{\"end\":49538,\"start\":49524},{\"end\":49555,\"start\":49549},{\"end\":49769,\"start\":49762},{\"end\":49783,\"start\":49776},{\"end\":49796,\"start\":49790},{\"end\":49813,\"start\":49804},{\"end\":49826,\"start\":49821},{\"end\":49841,\"start\":49836},{\"end\":49856,\"start\":49850},{\"end\":49874,\"start\":49864},{\"end\":50281,\"start\":50275},{\"end\":50294,\"start\":50289},{\"end\":50552,\"start\":50548},{\"end\":50563,\"start\":50559},{\"end\":50572,\"start\":50570},{\"end\":50587,\"start\":50579},{\"end\":50602,\"start\":50598},{\"end\":51005,\"start\":51001},{\"end\":51013,\"start\":51011},{\"end\":51025,\"start\":51021},{\"end\":51035,\"start\":51031},{\"end\":51047,\"start\":51044},{\"end\":51397,\"start\":51393},{\"end\":51408,\"start\":51405},{\"end\":51418,\"start\":51414},{\"end\":51426,\"start\":51424},{\"end\":51438,\"start\":51434},{\"end\":51679,\"start\":51675},{\"end\":51693,\"start\":51689},{\"end\":51708,\"start\":51705},{\"end\":51725,\"start\":51720},{\"end\":51741,\"start\":51735},{\"end\":51946,\"start\":51943},{\"end\":51956,\"start\":51953},{\"end\":51966,\"start\":51962},{\"end\":51977,\"start\":51973},{\"end\":51991,\"start\":51988},{\"end\":52307,\"start\":52304},{\"end\":52317,\"start\":52313},{\"end\":52327,\"start\":52325},{\"end\":52339,\"start\":52336},{\"end\":52350,\"start\":52347},{\"end\":52364,\"start\":52361},{\"end\":52379,\"start\":52376},{\"end\":52736,\"start\":52730},{\"end\":52746,\"start\":52740},{\"end\":52753,\"start\":52750},{\"end\":52760,\"start\":52757},{\"end\":53030,\"start\":53028},{\"end\":53044,\"start\":53039},{\"end\":53062,\"start\":53054},{\"end\":53070,\"start\":53068},{\"end\":53083,\"start\":53080},{\"end\":53097,\"start\":53091},{\"end\":53336,\"start\":53332},{\"end\":53353,\"start\":53344},{\"end\":53368,\"start\":53362},{\"end\":53380,\"start\":53375},{\"end\":53398,\"start\":53390}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":21689057},\"end\":33141,\"start\":32787},{\"attributes\":{\"id\":\"b1\"},\"end\":33420,\"start\":33143},{\"attributes\":{\"doi\":\"arXiv:1711.06016\",\"id\":\"b2\"},\"end\":33684,\"start\":33422},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":207239085},\"end\":34244,\"start\":33686},{\"attributes\":{\"doi\":\"PMLR, 2020. 5\",\"id\":\"b4\",\"matched_paper_id\":211096730},\"end\":34660,\"start\":34246},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":216080982},\"end\":35110,\"start\":34662},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":4722904},\"end\":35665,\"start\":35112},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":13712645},\"end\":36030,\"start\":35667},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b8\"},\"end\":36398,\"start\":36032},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":2229234},\"end\":36869,\"start\":36400},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":15436601},\"end\":37303,\"start\":36871},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":207216960},\"end\":37710,\"start\":37305},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":70047933},\"end\":38143,\"start\":37712},{\"attributes\":{\"id\":\"b13\"},\"end\":38383,\"start\":38145},{\"attributes\":{\"id\":\"b14\"},\"end\":38754,\"start\":38385},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":206594692},\"end\":39201,\"start\":38756},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":3919117},\"end\":39554,\"start\":39203},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":20770010},\"end\":39972,\"start\":39556},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":23787675},\"end\":40403,\"start\":39974},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":54458988},\"end\":40952,\"start\":40405},{\"attributes\":{\"doi\":\"arXiv:1412.6980\",\"id\":\"b20\"},\"end\":41172,\"start\":40954},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":6628564},\"end\":41592,\"start\":41174},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":4492210},\"end\":42184,\"start\":41594},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":201698118},\"end\":42725,\"start\":42186},{\"attributes\":{\"doi\":\"arXiv:1908.03557\",\"id\":\"b24\"},\"end\":43096,\"start\":42727},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":53036751},\"end\":43668,\"start\":43098},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":199453025},\"end\":44108,\"start\":43670},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":12780604},\"end\":44370,\"start\":44110},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":18376539},\"end\":44853,\"start\":44372},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":621466},\"end\":45467,\"start\":44855},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":13995550},\"end\":45793,\"start\":45469},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":11739471},\"end\":46206,\"start\":45795},{\"attributes\":{\"id\":\"b32\"},\"end\":46730,\"start\":46208},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":13295553},\"end\":47315,\"start\":46732},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":10328909},\"end\":47717,\"start\":47317},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":51996510},\"end\":48141,\"start\":47719},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":201317624},\"end\":48550,\"start\":48143},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":201103729},\"end\":49021,\"start\":48552},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":49664409},\"end\":49484,\"start\":49023},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":5855042},\"end\":49726,\"start\":49486},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":13756489},\"end\":50159,\"start\":49728},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":206141635},\"end\":50505,\"start\":50161},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":23672393},\"end\":50921,\"start\":50507},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":5063278},\"end\":51336,\"start\":50923},{\"attributes\":{\"doi\":\"arXiv:1607.06215\",\"id\":\"b44\"},\"end\":51616,\"start\":51338},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":16741401},\"end\":51932,\"start\":51618},{\"attributes\":{\"doi\":\"arXiv:1901.07249\",\"id\":\"b46\"},\"end\":52230,\"start\":51934},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":13618178},\"end\":52627,\"start\":52232},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":214802288},\"end\":52944,\"start\":52629},{\"attributes\":{\"id\":\"b49\"},\"end\":53265,\"start\":52946},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":2608922},\"end\":53674,\"start\":53267}]", "bib_title": "[{\"end\":32868,\"start\":32787},{\"end\":33740,\"start\":33686},{\"end\":34315,\"start\":34246},{\"end\":34714,\"start\":34662},{\"end\":35193,\"start\":35112},{\"end\":35726,\"start\":35667},{\"end\":36472,\"start\":36400},{\"end\":36920,\"start\":36871},{\"end\":37358,\"start\":37305},{\"end\":37793,\"start\":37712},{\"end\":38428,\"start\":38385},{\"end\":38800,\"start\":38756},{\"end\":39280,\"start\":39203},{\"end\":39607,\"start\":39556},{\"end\":40051,\"start\":39974},{\"end\":40471,\"start\":40405},{\"end\":41241,\"start\":41174},{\"end\":41682,\"start\":41594},{\"end\":42256,\"start\":42186},{\"end\":43217,\"start\":43098},{\"end\":43766,\"start\":43670},{\"end\":44128,\"start\":44110},{\"end\":44434,\"start\":44372},{\"end\":44891,\"start\":44855},{\"end\":45532,\"start\":45469},{\"end\":45864,\"start\":45795},{\"end\":46796,\"start\":46732},{\"end\":47395,\"start\":47317},{\"end\":47791,\"start\":47719},{\"end\":48208,\"start\":48143},{\"end\":48624,\"start\":48552},{\"end\":49092,\"start\":49023},{\"end\":49514,\"start\":49486},{\"end\":49753,\"start\":49728},{\"end\":50266,\"start\":50161},{\"end\":50540,\"start\":50507},{\"end\":50993,\"start\":50923},{\"end\":51669,\"start\":51618},{\"end\":52294,\"start\":52232},{\"end\":52726,\"start\":52629},{\"end\":53324,\"start\":53267}]", "bib_author": "[{\"end\":32880,\"start\":32870},{\"end\":32892,\"start\":32880},{\"end\":32903,\"start\":32892},{\"end\":32918,\"start\":32903},{\"end\":32934,\"start\":32918},{\"end\":33227,\"start\":33210},{\"end\":33240,\"start\":33227},{\"end\":33257,\"start\":33240},{\"end\":33266,\"start\":33257},{\"end\":33506,\"start\":33496},{\"end\":33516,\"start\":33506},{\"end\":33529,\"start\":33516},{\"end\":33751,\"start\":33742},{\"end\":33767,\"start\":33751},{\"end\":33781,\"start\":33767},{\"end\":33793,\"start\":33781},{\"end\":33806,\"start\":33793},{\"end\":34328,\"start\":34317},{\"end\":34345,\"start\":34328},{\"end\":34363,\"start\":34345},{\"end\":34380,\"start\":34363},{\"end\":34731,\"start\":34716},{\"end\":34742,\"start\":34731},{\"end\":34754,\"start\":34742},{\"end\":34770,\"start\":34754},{\"end\":34784,\"start\":34770},{\"end\":34793,\"start\":34784},{\"end\":34803,\"start\":34793},{\"end\":34817,\"start\":34803},{\"end\":35213,\"start\":35195},{\"end\":35230,\"start\":35213},{\"end\":35246,\"start\":35230},{\"end\":35262,\"start\":35246},{\"end\":35740,\"start\":35728},{\"end\":35754,\"start\":35740},{\"end\":35769,\"start\":35754},{\"end\":35780,\"start\":35769},{\"end\":35793,\"start\":35780},{\"end\":36046,\"start\":36032},{\"end\":36062,\"start\":36046},{\"end\":36074,\"start\":36062},{\"end\":36099,\"start\":36074},{\"end\":36493,\"start\":36474},{\"end\":36506,\"start\":36493},{\"end\":36522,\"start\":36506},{\"end\":36936,\"start\":36922},{\"end\":36953,\"start\":36936},{\"end\":37376,\"start\":37360},{\"end\":37390,\"start\":37376},{\"end\":37401,\"start\":37390},{\"end\":37808,\"start\":37795},{\"end\":37823,\"start\":37808},{\"end\":37835,\"start\":37823},{\"end\":37845,\"start\":37835},{\"end\":37856,\"start\":37845},{\"end\":37877,\"start\":37856},{\"end\":38249,\"start\":38235},{\"end\":38442,\"start\":38430},{\"end\":38460,\"start\":38442},{\"end\":38814,\"start\":38802},{\"end\":38829,\"start\":38814},{\"end\":38843,\"start\":38829},{\"end\":38853,\"start\":38843},{\"end\":39293,\"start\":39282},{\"end\":39306,\"start\":39293},{\"end\":39315,\"start\":39306},{\"end\":39331,\"start\":39315},{\"end\":39617,\"start\":39609},{\"end\":39627,\"start\":39617},{\"end\":39641,\"start\":39627},{\"end\":39652,\"start\":39641},{\"end\":40070,\"start\":40053},{\"end\":40081,\"start\":40070},{\"end\":40095,\"start\":40081},{\"end\":40111,\"start\":40095},{\"end\":40490,\"start\":40473},{\"end\":40500,\"start\":40490},{\"end\":40515,\"start\":40500},{\"end\":40534,\"start\":40515},{\"end\":40550,\"start\":40534},{\"end\":41010,\"start\":40998},{\"end\":41024,\"start\":41010},{\"end\":41028,\"start\":41024},{\"end\":41257,\"start\":41243},{\"end\":41266,\"start\":41257},{\"end\":41277,\"start\":41266},{\"end\":41286,\"start\":41277},{\"end\":41700,\"start\":41684},{\"end\":41710,\"start\":41700},{\"end\":41724,\"start\":41710},{\"end\":41740,\"start\":41724},{\"end\":41752,\"start\":41740},{\"end\":41768,\"start\":41752},{\"end\":41784,\"start\":41768},{\"end\":41803,\"start\":41784},{\"end\":41814,\"start\":41803},{\"end\":41830,\"start\":41814},{\"end\":42274,\"start\":42258},{\"end\":42286,\"start\":42274},{\"end\":42298,\"start\":42286},{\"end\":42308,\"start\":42298},{\"end\":42320,\"start\":42308},{\"end\":42331,\"start\":42320},{\"end\":42344,\"start\":42331},{\"end\":42816,\"start\":42797},{\"end\":42830,\"start\":42816},{\"end\":42838,\"start\":42830},{\"end\":42853,\"start\":42838},{\"end\":42868,\"start\":42853},{\"end\":43231,\"start\":43219},{\"end\":43245,\"start\":43231},{\"end\":43259,\"start\":43245},{\"end\":43273,\"start\":43259},{\"end\":43779,\"start\":43768},{\"end\":43792,\"start\":43779},{\"end\":43805,\"start\":43792},{\"end\":43817,\"start\":43805},{\"end\":44141,\"start\":44130},{\"end\":44156,\"start\":44141},{\"end\":44170,\"start\":44156},{\"end\":44183,\"start\":44170},{\"end\":44450,\"start\":44436},{\"end\":44464,\"start\":44450},{\"end\":44474,\"start\":44464},{\"end\":44489,\"start\":44474},{\"end\":44502,\"start\":44489},{\"end\":44908,\"start\":44893},{\"end\":44926,\"start\":44908},{\"end\":44940,\"start\":44926},{\"end\":44956,\"start\":44940},{\"end\":44973,\"start\":44956},{\"end\":45548,\"start\":45534},{\"end\":45563,\"start\":45548},{\"end\":45579,\"start\":45563},{\"end\":45595,\"start\":45579},{\"end\":45602,\"start\":45595},{\"end\":45879,\"start\":45866},{\"end\":45892,\"start\":45879},{\"end\":45908,\"start\":45892},{\"end\":45917,\"start\":45908},{\"end\":46293,\"start\":46279},{\"end\":46308,\"start\":46293},{\"end\":46323,\"start\":46308},{\"end\":46338,\"start\":46323},{\"end\":46351,\"start\":46338},{\"end\":46369,\"start\":46351},{\"end\":46384,\"start\":46369},{\"end\":46399,\"start\":46384},{\"end\":46415,\"start\":46399},{\"end\":46427,\"start\":46415},{\"end\":46445,\"start\":46427},{\"end\":46461,\"start\":46445},{\"end\":46811,\"start\":46798},{\"end\":46831,\"start\":46811},{\"end\":46849,\"start\":46831},{\"end\":46861,\"start\":46849},{\"end\":46871,\"start\":46861},{\"end\":46883,\"start\":46871},{\"end\":47419,\"start\":47397},{\"end\":47428,\"start\":47419},{\"end\":47443,\"start\":47428},{\"end\":47448,\"start\":47443},{\"end\":47806,\"start\":47793},{\"end\":47819,\"start\":47806},{\"end\":47837,\"start\":47819},{\"end\":47851,\"start\":47837},{\"end\":47861,\"start\":47851},{\"end\":48221,\"start\":48210},{\"end\":48233,\"start\":48221},{\"end\":48242,\"start\":48233},{\"end\":48250,\"start\":48242},{\"end\":48260,\"start\":48250},{\"end\":48270,\"start\":48260},{\"end\":48282,\"start\":48270},{\"end\":48635,\"start\":48626},{\"end\":48649,\"start\":48635},{\"end\":49105,\"start\":49094},{\"end\":49120,\"start\":49105},{\"end\":49130,\"start\":49120},{\"end\":49140,\"start\":49130},{\"end\":49151,\"start\":49140},{\"end\":49162,\"start\":49151},{\"end\":49175,\"start\":49162},{\"end\":49540,\"start\":49516},{\"end\":49557,\"start\":49540},{\"end\":49771,\"start\":49755},{\"end\":49785,\"start\":49771},{\"end\":49798,\"start\":49785},{\"end\":49815,\"start\":49798},{\"end\":49828,\"start\":49815},{\"end\":49843,\"start\":49828},{\"end\":49858,\"start\":49843},{\"end\":49876,\"start\":49858},{\"end\":50283,\"start\":50268},{\"end\":50296,\"start\":50283},{\"end\":50554,\"start\":50542},{\"end\":50565,\"start\":50554},{\"end\":50574,\"start\":50565},{\"end\":50589,\"start\":50574},{\"end\":50604,\"start\":50589},{\"end\":51007,\"start\":50995},{\"end\":51015,\"start\":51007},{\"end\":51027,\"start\":51015},{\"end\":51037,\"start\":51027},{\"end\":51049,\"start\":51037},{\"end\":51399,\"start\":51387},{\"end\":51410,\"start\":51399},{\"end\":51420,\"start\":51410},{\"end\":51428,\"start\":51420},{\"end\":51440,\"start\":51428},{\"end\":51681,\"start\":51671},{\"end\":51695,\"start\":51681},{\"end\":51710,\"start\":51695},{\"end\":51727,\"start\":51710},{\"end\":51743,\"start\":51727},{\"end\":51948,\"start\":51934},{\"end\":51958,\"start\":51948},{\"end\":51968,\"start\":51958},{\"end\":51979,\"start\":51968},{\"end\":51993,\"start\":51979},{\"end\":52309,\"start\":52296},{\"end\":52319,\"start\":52309},{\"end\":52329,\"start\":52319},{\"end\":52341,\"start\":52329},{\"end\":52352,\"start\":52341},{\"end\":52366,\"start\":52352},{\"end\":52381,\"start\":52366},{\"end\":52738,\"start\":52728},{\"end\":52748,\"start\":52738},{\"end\":52755,\"start\":52748},{\"end\":52762,\"start\":52755},{\"end\":53032,\"start\":53022},{\"end\":53046,\"start\":53032},{\"end\":53064,\"start\":53046},{\"end\":53072,\"start\":53064},{\"end\":53085,\"start\":53072},{\"end\":53099,\"start\":53085},{\"end\":53338,\"start\":53326},{\"end\":53355,\"start\":53338},{\"end\":53370,\"start\":53355},{\"end\":53382,\"start\":53370},{\"end\":53400,\"start\":53382}]", "bib_venue": "[{\"end\":32948,\"start\":32934},{\"end\":33208,\"start\":33143},{\"end\":33494,\"start\":33422},{\"end\":33904,\"start\":33806},{\"end\":34437,\"start\":34393},{\"end\":34855,\"start\":34817},{\"end\":35339,\"start\":35262},{\"end\":35830,\"start\":35793},{\"end\":36189,\"start\":36115},{\"end\":36589,\"start\":36522},{\"end\":37036,\"start\":36953},{\"end\":37467,\"start\":37401},{\"end\":37908,\"start\":37877},{\"end\":38233,\"start\":38145},{\"end\":38527,\"start\":38460},{\"end\":38930,\"start\":38853},{\"end\":39362,\"start\":39331},{\"end\":39718,\"start\":39652},{\"end\":40167,\"start\":40111},{\"end\":40627,\"start\":40550},{\"end\":40996,\"start\":40954},{\"end\":41361,\"start\":41286},{\"end\":41870,\"start\":41830},{\"end\":42411,\"start\":42344},{\"end\":42795,\"start\":42727},{\"end\":43339,\"start\":43273},{\"end\":43866,\"start\":43817},{\"end\":44231,\"start\":44183},{\"end\":44569,\"start\":44502},{\"end\":45091,\"start\":44973},{\"end\":45616,\"start\":45602},{\"end\":45978,\"start\":45917},{\"end\":46277,\"start\":46208},{\"end\":46970,\"start\":46883},{\"end\":47497,\"start\":47448},{\"end\":47921,\"start\":47861},{\"end\":48334,\"start\":48282},{\"end\":48735,\"start\":48649},{\"end\":49237,\"start\":49175},{\"end\":49593,\"start\":49557},{\"end\":49925,\"start\":49876},{\"end\":50315,\"start\":50296},{\"end\":50670,\"start\":50604},{\"end\":51111,\"start\":51049},{\"end\":51385,\"start\":51338},{\"end\":51759,\"start\":51743},{\"end\":52059,\"start\":52009},{\"end\":52413,\"start\":52381},{\"end\":52772,\"start\":52762},{\"end\":53020,\"start\":52946},{\"end\":53462,\"start\":53400},{\"end\":33989,\"start\":33906},{\"end\":35403,\"start\":35341},{\"end\":36643,\"start\":36591},{\"end\":37106,\"start\":37038},{\"end\":37520,\"start\":37469},{\"end\":38581,\"start\":38529},{\"end\":38994,\"start\":38932},{\"end\":39771,\"start\":39720},{\"end\":40691,\"start\":40629},{\"end\":42465,\"start\":42413},{\"end\":43392,\"start\":43341},{\"end\":44623,\"start\":44571},{\"end\":45196,\"start\":45093},{\"end\":47044,\"start\":46972},{\"end\":48808,\"start\":48737},{\"end\":50723,\"start\":50672},{\"end\":52778,\"start\":52774}]"}}}, "year": 2023, "month": 12, "day": 17}