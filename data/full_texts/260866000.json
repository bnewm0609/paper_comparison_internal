{"id": 260866000, "updated": "2023-10-04 21:18:55.576", "metadata": {"title": "Temporally-Adaptive Models for Efficient Video Understanding", "authors": "[{\"first\":\"Ziyuan\",\"last\":\"Huang\",\"middle\":[]},{\"first\":\"Shiwei\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Liang\",\"last\":\"Pan\",\"middle\":[]},{\"first\":\"Zhiwu\",\"last\":\"Qing\",\"middle\":[]},{\"first\":\"Yingya\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Ziwei\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Marcelo\",\"last\":\"Ang\",\"middle\":[\"H.\"]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "Spatial convolutions are extensively used in numerous deep video models. It fundamentally assumes spatio-temporal invariance, i.e., using shared weights for every location in different frames. This work presents Temporally-Adaptive Convolutions (TAdaConv) for video understanding, which shows that adaptive weight calibration along the temporal dimension is an efficient way to facilitate modeling complex temporal dynamics in videos. Specifically, TAdaConv empowers spatial convolutions with temporal modeling abilities by calibrating the convolution weights for each frame according to its local and global temporal context. Compared to existing operations for temporal modeling, TAdaConv is more efficient as it operates over the convolution kernels instead of the features, whose dimension is an order of magnitude smaller than the spatial resolutions. Further, kernel calibration brings an increased model capacity. Based on this readily plug-in operation TAdaConv as well as its extension, i.e., TAdaConvV2, we construct TAdaBlocks to empower ConvNeXt and Vision Transformer to have strong temporal modeling capabilities. Empirical results show TAdaConvNeXtV2 and TAdaFormer perform competitively against state-of-the-art convolutional and Transformer-based models in various video understanding benchmarks. Our codes and models are released at: https://github.com/alibaba-mmai-research/TAdaConv.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2308.05787", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2308-05787", "doi": "10.48550/arxiv.2308.05787"}}, "content": {"source": {"pdf_hash": "fe779d52ef85c0387ed9a68cd90ca11033689bfa", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2308.05787v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "47c02f1fbd29330ac70041729287d7b092fd6a3a", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/fe779d52ef85c0387ed9a68cd90ca11033689bfa.txt", "contents": "\nTemporally-Adaptive Models for Efficient Video Understanding\nAUGUST 2015 1\n\nJournal Of L A T E X Class \nFiles \nTemporally-Adaptive Models for Efficient Video Understanding\n148AUGUST 2015 1Index Terms-Dynamic NetworksEfficient Video UnderstandingAction RecognitionTemporally-Adaptive ConvolutionsTemporally-Adaptive Transformer \u2726\nSpatial convolutions are extensively used in numerous deep video models. It fundamentally assumes spatio-temporal invariance, i.e., using shared weights for every location in different frames. This work presents Temporally-Adaptive Convolutions (TAdaConv) for video understanding, which shows that adaptive weight calibration along the temporal dimension is an efficient way to facilitate modeling complex temporal dynamics in videos. Specifically, TAdaConv empowers spatial convolutions with temporal modeling abilities by calibrating the convolution weights for each frame according to its local and global temporal context. Compared to existing operations for temporal modeling, TAdaConv is more efficient as it operates over the convolution kernels instead of the features, whose dimension is an order of magnitude smaller than the spatial resolutions. Further, kernel calibration brings an increased model capacity. Based on this readily plug-in operation TAdaConv as well as its extension, i.e., TAdaConvV2, we construct TAdaBlocks to empower ConvNeXt and Vision Transformer to have strong temporal modeling capabilities. Empirical results show TAdaConvNeXtV2 and TAdaFormer perform competitively against state-of-the-art convolutional and Transformer-based models in various video understanding benchmarks. Our codes and models are released at: https://github.com/alibaba-mmai-research/TAdaConv.\n\nINTRODUCTION\n\nConvolutions are an indispensable operation in modern deep vision models [1], [2], [3], [4], whose different variants have driven the state-of-the-art performances of convolutional neural networks (CNNs) in many visual tasks [5], [6], [7], [8], [9] and application scenarios [10], [11]. In the video paradigm, compared to the 3D convolutions [12], the combination of 2D spatial convolutions and 1D temporal convolutions is more widely preferred owing to its efficiency [13], [14]. Nevertheless, 1D temporal convolutions introduce non-negligible computation overhead on top of the spatial convolutions. Therefore, we seek to directly equip spatial convolutions with temporal modeling abilities.\n\nOne essential property of convolutions is the translation invariance [15], [16], resulting from its local connectivity and shared weights. However, recent works in dynamic filtering have shown that strictly shard weights for all pixels may be sub-optimal for modeling various spatial contents [17], [18].\n\nGiven the diverse nature of the temporal dynamics in videos, we hypothesize that temporal modeling could benefit from relaxed invariance along the temporal dimension. This means that convolution weights for different time steps are no longer strictly shared. Existing dynamic filter networks \u2022 * Correspondence to Shiwei Zhang (zhangjin.zsw@alibaba-inc.com) and\n\nMarcelo H. Ang Jr (mpeangh@nus.edu.sg). could achieve this but with two drawbacks. (i) it is difficult for most of them [11], [17] to leverage pre-trained weights, which is critical in video applications since training video models from scratch is highly resource demanding [19], [20] and prone to over-fitting on small datasets. (ii) for most dynamic filters, the weights are generated with respect to its spatial context [17], [21] or the global descriptor [11], [22], which is incapable of capturing the fine-grained temporal variations between frames. Motivated by this, we present Temporally-Adaptive Convolution (TAdaConv) for video understanding, where the convolution weights are no longer fixed across different frames. Specifically, the convolution kernel for the t-th frame W t is factorized to the multiplication of the base weight and a calibration weight: W t = \u03b1 t \u00b7 W b , where the base weight W b is learnable and the calibration weight \u03b1 t is adaptively generated from the input data in the base weight W b . For each frame, we generate the calibration weight based on the frame descriptors of its adjacent time steps as well as the global descriptor, which effectively encodes both local and global temporal dynamics in videos. The difference between TAdaConv and standard convolutions is visualized in Fig. 1.\n\nThe main advantages of this factorization are threefold: (i) TAdaConv can be easily plugged into any existing models to enhance temporal modeling, and their pretrained weights can still be exploited; (ii) the temporal modeling ability can be highly improved with the help of the temporally-adaptive weight; (iii) in comparison with temporal convolutions that often operate on the learned 2D feature maps, TAdaConv is more efficient by directly operating on the convolution kernels.\n\nTAdaConv is proposed as a drop-in replacement for the convolutions in existing models. A preliminary version of this work [23] is published in ICLR 2022, where TAda- Conv has demonstrated a strong capability of temporal modeling, introducing notable performance gains to both image-based models as well as existing video models. In this work, we follow the conceptual idea of TAdaConv and present improvements to the preliminary version on both structural designs as well as model and data scaling. In terms of structural designs, we optimize TAdaConv in the following aspects: (i) At the operation level, the calibration factor generation process of TAdaConv is optimized, where multi-head self-attention [24] is introduced for modeling the global information of the videos. (ii) At the block level, we construct stronger TAdaBlocks by introducing efficient temporal feature aggregation, which we use to construct our convolutional model TAdaConvNeXtV2 and transformer TAdaFormer. Our empirical results show a notable improvement brought by our modifications on both scene-and motion-centric benchmarks. Based on the TAdaConvNeXtV2 and TAdaFormer, we further scale up both the model and data scale, which lead to a competitive performance to existing state-of-the-art approaches.\n\n\nRELATED WORK\n\nConvolutional models for video understanding. Early convolutional models obtain spatio-temporal representations by 3D convolutions [12], [20], [25], [26] or two-stream networks [27]. For efficiency, recent ones build upon 2D networks and design additional operations for temporal modeling [13], [28], [29], [30], [31], [32], [33], [34], [35], where the weights of the 2D convolutions are shared between different timestamps. Our preliminary version [23] find removing this constraint leads to stronger temporal modeling ability. In this work, we modernize the convolutional model according to ConvNeXt [36] and construct a stronger convolutional model for video understanding. Vision Transformers for video understanding. With the great success of Transformers in natural language processing [24], [37], [38], Vision Transformers (ViT) [39] are showing strong performances in various vision tasks [40], [41], [42], [43], [44], [45], [46] including video understanding [47], [48], [49], [50], [51], [52], [53]. The capability of ViTs is further enhanced when it is pre-trained on a large corpus of image [54], [55], video [56], [57], [58] or multi-modal data [59], [60], or when the size of the model is increased [61], [62], or both [63], [64]. Since directly pre-training with video data is both resource-and time-consuming, an alternative is to exploit the models pre-trained on large-scale image data and empower the model thorough additional structures for temporal modeling, such as temporal [50] or 3D windowed self-attention [48], spatio-temporal adapters [65], etc. In our work, we exploit the vanilla Vision Transformer pre-trained on a large corpus of image-text data [59] and equip it with strong temporal modeling ability with our TAdaBlock.\n\nDynamic networks. Dynamic networks refer to networks with content-adaptive weights or modules, such as dynamic filters/convolutions [11], [17], [21], [66], dynamic activations [67], [68], and dynamic routing [69], [70], etc. They have demonstrated exceeding network capacity and performance compared to static ones in various tasks [71], [72], [73], [74] as well as in video understanding [31], [75], [76], [77]. Some recent spatially-adaptive convolutions [78], [79] show relaxing spatial invariance could help modeling diverse visual contents, and our preliminary version [23] shows video understanding can benefit from relaxing the temporal invariance. This work further exploits the idea and enhance the temporal modeling capability of TAdaConv by introducing multi-head self-attention for global temporal modeling.\n\n\nTEMPORALLY-ADAPTIVE CONVOLUTIONS\n\nIn this work, we seek to empower the spatial convolutions with temporal modeling abilities. Inspired by the calibration process of temporal convolutions (Sec. 3.1), TAdaConv dynamically calibrates the convolution weights for each frame (Sec. 3.2) according to its temporal context (Sec. 3.3).\n\n\nRevisiting temporal convolutions\n\nWe first revisit the temporal convolution to show the underlying process and its relation to dynamic filters. We consider depth-wise temporal convolution for simplicity, which is more widely used due to its efficiency [30], [31]. Formally, for a 3\u00d71\u00d71 temporal convolution filter parameterized by \u03b2 = [\u03b2 1 , \u03b2 2 , \u03b2 3 ] and placed (ignoring normalizations) after the 2D convolution parameterized by W, the output featur\u1ebd x t of the t-th frame can be obtained by:\nW t W t 1xKxK 1xKxK W 2D Conv X X C o \u00d7 C \u00d7 K 2 KxKxK W 3D Conv X X C o \u00d7 C \u00d7 K 3 (a) Standard convolutions Dynamic parameters Learnable parameters W 1 Constant (b) TAdaConv 1xKxK X X Temp Pool 1D conv C \u00d7 T \u00d7 H \u00d7 W BN, ReLU 1D conv T \u00d7 C o \u00d7 C \u00d7 K 2 W t Spat Pool C \u00d7 1 C \u00d7 T 1D conv C/r \u00d7 T Global Modelling C \u00d7 T W b C o \u00d7 C \u00d7 K 2 \u03b1 1 (c) TAdaConvV2 X X C \u00d7 T C \u00d7 T \u00d7 H \u00d7 W LN, GELU 1D conv W b C o \u00d7 C \u00d7 K 2 T \u00d7 C o \u00d7 C \u00d7 K 2 \u03b1 Spat Pool 1 1D conv C/r \u00d7 T C \u00d7 T Global Modelling MHSA C/r \u00d7 Tx t = \u03b2 1 \u00b7\u03b4(W * x t\u22121 )+\u03b2 2 \u00b7\u03b4(W * x t )+\u03b2 3 \u00b7\u03b4(W * x t+1 ) , (1)\nwhere the \u00b7 indicates the element-wise multiplication, * denotes the convolution over the spatial dimension and \u03b4 denotes ReLU activation [80]. It can be rewritten as follows:\nx t = W t\u22121 * x t\u22121 + W t * x t + W t+1 * x t+1 ,(2)\nwhere\nW i,j t\u22121 = M i,j t\u22121 \u00b7 \u03b2 1 \u00b7 W, W i,j t = M i,j t \u00b7 \u03b2 2 \u00b7 W and W i,j t+1 = M i,j t+1\n\u00b7 \u03b2 3 \u00b7 W are spatio-temporal location adaptive convolution weights. M t \u2208 R C\u00d7H\u00d7W is a dynamic tensor, with its value dependent on the result of the spatial convolutions (see Appendix for details). Hence, the temporal convolutions in the (2+1)D convolution essentially perform (i) weight calibration on the spatial convolutions and (ii) feature aggregation between adjacent frames. However, if the temporal modeling is achieved by coupling temporal convolutions to spatial convolutions, a non-negligible computation overhead is still introduced (see Table 1).\n\n\nFormulation of TAdaConv and TAdaConvV2\n\nFor efficiency, we set out to directly empower the spatial convolutions with temporal modeling abilities. Inspired by the recent finding that the relaxation of spatial invariance strengthens spatial modeling [17], [78], we hypothesize that temporally adaptive weights can also help temporal modeling. Therefore, the convolution weights in a TAdaConv layer are varied on a frame-by-frame basis. Since we observe that previous dynamic filters can hardly utilize the pretrained weights, we take inspiration from our observation in the temporal convolutions and factorize the weights for the tth frame W t into the multiplication of a base weight W b shared for all frames, and a calibration weight \u03b1 t that are different for each time step:\nx t = W t * x t = (\u03b1 t \u00b7 W b ) * x t .(3)\n\nCalibration weight generation.\n\nTo allow for the TAdaConv to model temporal dynamics, it is crucial that the calibration weight \u03b1 t for the tth frame takes into account not only the current frame, but more importantly, its temporal context, i.e., \u03b1 t = G(..., x t\u22121 , x t , x t+1 , ...). Otherwise, TAdaConv would degenerate to a set of unrelated spatial convolutions with different weights applied on different frames. In practice, the calibration generation function can have various structural designs. In Fig. 2(b) and (c), we show two instantiations of the calibration generation function, which respectively correspond to TAdaConv and TAdaConvV2. TAdaConv. In our design, we aim for efficiency and the ability to capture inter-frame temporal dynamics. For efficiency, we operate on the frame description vectors v \u2208 R T \u00d7C obtained by the global average pooling over the spatial dimension GAP s for each frame, i.e., v t = GAP s (x t ). For temporal modeling, we apply two-layer 1D convolutions F with a dimension reduction ratio of r on the local temporal\ncontext v adj t = {v t\u22121 , v t , v t+1 }: v \u2032adj t = ReLU(BN(f C\u2192C/r (v adj t ))) F(v t ) = f C/r\u2192C (v \u2032adj t ) ,(4)\nwhere we use ReLU [80] and batch normalizations [81] for activation and normalization. f denotes 1-D convolutions.\n\nIn order for a larger inter-frame field of view in complement to the local 1D convolution, we further incorporate global temporal information into the calibration weight generation process. For TAdaConv, we add a global descriptor to the weight generation process F through a linear mapping function FC:\nv \u2032adj t = v adj t + FC(GAP t (v t )) v \u2032\u2032adj t = ReLU(BN(f C\u2192C/r (v \u2032adj t )) F(v t ) = f C/r\u2192C (v \u2032\u2032adj t ) ,(5)\nwhere GAP t (v t ) denotes global average pooling over the temporal dimension on the frame descriptors v t . This is equivalent to global average pooling over all spatiotemporal dimensions on the original input x. Hence, GAP t (v t ) contains the global temporal context in the input videos.\n\nTAdaConvV2. The instantiation of TAdaConvV2 is generally similar to TAdaConv, with two improvements. (i) We alter the combination of ReLU and batch normalizations to GELU and layer normalizations to conform to the structures in ConvNeXt models. (ii) For global temporal context modeling, we take advantage of the powerful global modeling capability of self-attention [24]. Specifically, the calibration weight generation function can be expressed as follows:\nv \u2032adj t = GELU(LN(f C\u2192C/r (v adj t ))) v \u2032\u2032adj t = MHSA(v \u2032adj t ) + v \u2032adj t F(v t ) = f C/r\u2192C (v \u2032\u2032adj t ) ,(6)\nwhere MHSA denotes the multi-head self-attention [24]. Since the 1D convolution before MHSA essentially provides a dynamic positional embedding for the frame descriptors v, we do not add additional positional embeddings before the MHSA operation. Initialization. The TAdaConv is designed to be readily inserted into existing models by simply replacing the 2D convolutions. For effective use of the pre-trained weights, TAdaConv is initialized to behave exactly the same as the standard convolution. This is achieved by zero-initializing the weight of the last convolution in F and adding a constant vector 1 to the formulation:\n\u03b1 t = G(x) = 1 + F(GAP s (x adj t )) .(7)\nIn this way, at initial state, W t = 1 \u00b7 W b = W b , where we load W b with the pre-trained weights. \n\n\n(2+1)D Conv TAdaConv\n\nFLOPs Params. can be calibrated in different dimensions. For standard convolutions, we instantiate the calibration on the C in dimension (\u03b1 t \u2208 R 1\u00d7Cin\u00d71 ), as the weight generation based on the input features yields a more precise estimation for the relation of the input channels than the output channels or spatial structures (empirical analysis in Table 3b). For depthwise convolutions, since the convolution kernel does not have a C in dimension, the calibration is directly applied on the C out dimension of the convolution kernel.\nCo \u00d7 C i \u00d7 K 2 \u00d7 T HW Co \u00d7 C i \u00d7 K 2 \u00d7 T HW +C i \u00d7 (T HW + T ) +Co \u00d7 C i \u00d7 K \u00d7 T HW +C i \u00d7 C i /r \u00d7 (2 \u00d7 K \u00d7 T + 1) +Co \u00d7 C i \u00d7 K 2 \u00d7 T E.G.Co \u00d7 C i \u00d7 K 2 Co \u00d7 C i \u00d7 K 2 +Co \u00d7 C i \u00d7 K +2 \u00d7 C i \u00d7 C i /r \u00d7 K E.G.\nComparison with temporal convolutions. Table 1 compares the TAdaConv with R(2+1)D in parameters and FLOPs, which shows most of our additional computation overhead on top of the spatial convolution is an order of magnitude less than the temporal convolution.\n\nComparison with existing dynamic filters. Table 2 compares TAdaConv with existing dynamic filters. The main difference between different dynamic filtering approaches lies in the way that the dynamic weights are generated. Mixture-of-experts-based dynamic filters [11] generate content-dependent weights to dynamically aggregate learnable convolution weights. Other types of dynamic filters [17], [21], [31] generate dynamic weights entirely based on the input content. Our TAdaConv is different from existing dynamic filters in the following three aspects: (i) Compared to image-based dynamic filters [11], [17], [21], TAdaConv achieves temporal modeling by generating weights based on the local and global context. (ii) Compared to TANet [31] in the video paradigm, TAdaConv could model more complex temporal dynamics because of the temporally adaptive weights. (iii) Most existing dynamic filters are incapable of exploiting existing pre-trained weights, while TAdaConv could be initialized to generate dynamic weights that are identical to pre-trained ones. This reduces the training difficulty in video applications. More detailed comparisons of dynamic filters are included in Appendix.\n\n\nTADABLOCKS\n\nBased on TAdaConv and TAdaConvV2, we can construct a series of TAdaBlocks for various models, both convolutional and Transformer-based ones. In Fig. 3, we construct TAda2D block, TAdaConvNeXt(V2) block, and TAdaFormer block, respectively for ResNet [1], ConvNeXt [36] and ViT [39].\n\nApart from TAdaConv and TAdaConvV2, an important component of our TAdaBlocks is an efficient temporal feature aggregation scheme. This corresponds to the second essential step of temporal convolution. Formally, given the  For convolutional models, we constructed TAda2D block in (a) and TAdaConvNeXt block in (b) in our preliminary version [23]. Inspired by the efficient temporal aggregation in TAda2D, we introduce a similar strided temporal pooling (T-Pool in the figure) and a separate normalization layer to the TAdaConvNeXtV2 block in (c). For transformer-based models, we insert a TAdaBlock before each multi-head self-attention operation in the TAdaFormer block in (d), where the TAdaBlock is composed of two pointwise convolutions, a depth-wise TAdaConvV2, and a temporal feature aggregation module.\n\noutput of TAdaConvx, the aggregated feature can be obtained as follows:\nx aggr = ReLU(Norm 1 (x) + Norm 2 (T-Pool k (x))) ,(8)\nwhere T-Pool k represents the strided temporal pooling operation with a kernel size of k. We use different normalization parameters for the features extracted by TAdaConvx and aggregated by strided average pooling TempAvgPool k (x), as their distributions are essentially different. During initialization, we load pre-trained weights (if any) to Norm 1 , and initialize the parameters of Norm 2 to zero. Coupled with the initialization of TAdaConv, the initial state of the TAdaBlocks is exactly the same as the base model, while the calibration and the aggregation notably increase the model capacity with training (See Appendix). In experiments, we refer to this structure as the shortcut (Sc.) branch and the separate BN (SepBN.) branch.\n\nIn our preliminary version [23], we explored the TAda2D block and TAdaConvNeXt block. Inspired by the improvements brought by the temporal feature aggregation, we present an improved version of the TAdaConvNeXt block in Fig. 3 (c), i.e., TAdaConvNeXtV2 block. To cater to the modernized convolutional block [36], the structure of the aggregation scheme in TAdaConvNeXtV2 block is modified accordingly, where the activation function is removed and the normalization is switched to LayerNorm [82].\n\nFor Transformer-based models, we construct a TAdaFormer block, as in Fig. 3 (d), where a ResNet-like convolutional block is inserted before each self-attention layer. Different from ResNet blocks, we use depth-wise TAdaConvV2 between two point-wise convolutions for efficiency. Inspired by the modernized convolutional block [36], some of the normalization and activation layers are removed, as in Fig. 3 (d). Temporal aggregation is similarly performed using the efficient feature aggregation scheme presented above. Empirically, we found batch normalizations work better in TAdaBlock for TAdaFormer.\n\n\nEVALUATIONS ON VIDEO CLASSIFICATION\n\n\nModel.\n\nWe construct different variants for TAda2D, TAda-ConvNeXtV2, and TAdaFormer, following the structure of the respective base models ResNet [1], ConvNeXt [36], and Vision Transformer [39]. Our model variants are obtained by replacing the residual blocks or the transformer blocks in the original model with our TAdaBlocks. Additionally, for TAdaConvNeXtV2 and TAdaFormer, we follow recent works [48], [49] and use tubelet embedding stem. More details on the model structure is included in Appendix.\n\nDatasets. For video classification, we use Kinetics-400 [83] (K400), Something-Something-V1 and V2 [84] (SSV1 and SSV2), Epic-Kitchens-100 [85] (EK100), and HACS [86]. Further, we employ UCF101 [87] and HMDB51 [88] for multi-modal zero-shot evaluations. K400 is a widely used action classification dataset with 400 categories covered by \u223c300K videos. SSV1 and SSV2 include 108K and 220K videos with challenging spatio-temporal interactions in 174 classes. EK100 includes 90K segments labelled by 97 verb and 300 noun classes with actions defined by the combination of nouns and verbs. HACS contains 504K videos with a taxonomy of 200 action classes. The latter two datasets are used for evaluation on action localization as well.\n\nIn addition, we also construct a large-scale video classification dataset combining Kinetics-400 [83], Kinetics-600 [89], and Kinetics-700 [90] for pre-training our video models, following [64], [91]. This results in a dataset with around 660K videos over 710 action classes, which is referred to as K710 in the following sections.\n\nTraining. We train models initialized with ImageNet pretraining using AdamW [92] for 100/64/50 epochs on K400, SSV1/SSV2, and EK100, respectively. We adopt RandAugment [93] for data augmentation and stochastic depth [94] (a) Benefit of dynamic calibration. *: w/o our init. (d) Calibration weight generation. K: kernel size; Lin./Non-Lin.: linear/non-linear weight generation; G: global information g.\nModel TAdaConv K. G. Top-1 TSN \u22c6 - - - 32.0 Ours Lin. 1 \u2717 37.5 Lin. 3 \u2717 56.5 Non-Lin. (1, 1) \u2717 36.8 Non-Lin. (3, 1) \u2717 57.1 Non-Lin. (1, 3) \u2717 57.3 Non-Lin. (3, 3) \u2717 57.8 Lin. 1 \u2713 53.4 Non-Lin. (1, 1) \u2713 54.4 Non-Lin. (3, 3) \u2713 59.2\n(e) Feature aggregation scheme. FA: feature aggregation; Sc: shortcut for convolution feature; SepBN: separate batch norm. For plug-in evaluations, we plug TAdaConv into existing video recognition models and analyze the performance on both K400 [83] and SSV2 [84]. For ablative experiments on TAdaConv, we mainly investigate its performance on SSV2. and label smoothing [95] for model regularization. We do not use Mixup [96] or Cutmix [97] for both models. Exponential Moving Average (EMA) [98] is used for reducing overfitting during traning. For TAdaFormer with CLIP pretrained weights [59], we shorten the schedule to 30/24/24 epochs respectively. See Appendix for more details.\nTAdaConv FA. Sc. SepBN. Top-1 \u2206 \u2717 - - - 32.0 - \u2713 - - - 59.2 +27.2 \u2717 Avg. \u2717 - 47.9 +15.9 \u2717 Avg. \u2713 \u2717 49.0 +17.0 \u2717 Avg. \u2713 \u2713 57.0 +25.0 \u2713 Avg. \u2717 - 60.1 +28.1 \u2713 Avg. \u2713 \u2717 61.5 +29.5 \u2713 Avg. \u2713 \u2713 63.8 +31.8 \u2713 Max. \u2713 \u2713 63.5 +31.5 \u2713 Mix. \u2713 \u2713 63.7 +31.7\n\nVerification of hypothesis\n\nWe start our experiments by verifying our hypothesis that relaxing the temporal invariance could lead to stronger temporal modeling capabilities of the video models. To this end, we choose several sources for the calibration weights and compare the action classification performance on SSV2, with and without the relaxation of temporal invariance. The results are shown in Table 3a. It can be observed that both learnable and dynamic calibration can bring a notable improvement to the baseline with no calibration (TSN [99]), with dynamic calibration performing stronger than learnable calibration. On top of the calibrated models, making the weights vary along the temporal dimension can further boost classification accuracy, which means the model shows a better capability of temporal modeling when the temporal variance is relaxed.\n\n\nTAdaConv on existing video backbones\n\nTAdaConv is designed as a plug-in substitution for the spatial convolutions in the video models. As in Table 3c, TAda-Conv improves the classification performance with negligible computation overhead on a wide range of video models, including SlowFast [19], R3D [100] and R(2+1)D [13], by an average of 1.3% and 2.8% respectively on K400 and SSV2 at an extra computational cost of less than 0.02 GFlops. Further, not only can TAdaConv improve spatial convolutions, it also notably improve 3D and 1D convolutions. For fair comparison, all models are trained using the same training strategy. Further plug-in evaluations for action classification is presented in Appendix.\n\n\nAblative anslysis on TAdaConv\n\nIn this section, we thoroughly analyze our design choices and the effectiveness of TAdaConv and TAdaConvV2 in modeling temporal dynamics. We begin with TAdaConv, with SSV2 chosen as our main evaluation benchmark because of its more complex spatio-temporal relations. Calibration weight initialization. In Table 3a, we show that our initialization strategy for the calibration weight generation plays a critical role in dynamic weight calibration. As in Table 3a, randomly initializing learnable weights slightly degrades the performance, while randomly initializing dynamic calibration weights (by randomly initializing the last layer of the weight generation function) notably degenerates the performance. It is likely that randomly initialized dynamic calibration weights perturb the pretrained weights more severely than the learnable weights since it is dependent on the input. Further comparisons on the initialization are shown in the Appendix. Calibration weight generation function. Having established that the temporally adaptive dynamic calibration with appropriate initialization can be an ideal strategy for temporal modeling, we further ablate different ways for generating the calibration weight in Table 3d. Linear weight generation function (Lin.) applies a single 1D convolution to generate the calibration weight, while non-linear one (Non-Lin.) uses two stacked 1D convolutions with batch normalizations and ReLU activation in between. When no temporal context is considered (K.=1 or (1,1)), TAdaConv can still improve the baseline but with a limited gap. Enlarging the kernel size to cover the temporal context (K.=3, (1,3), (3,1) or (3,3)) effectively yields a boost of over 20% on the accuracy, with K.=(3,3) having the strongest performance. This shows the importance of the local temporal context during calibration weight generation. Finally, for the scope of temporal context, introducing global context to frame descriptors performs similarly to only generating temporally adaptive calibration weights solely on the global context (in Table 3a). The combination of the global and temporal context yields a better performance for both variants. In Appendix, we also show that this function in our TAdaConv yields a better calibration on the base weight than existing dynamic filters.\n\nFeature aggregation. We ablate the aggregation scheme in TAda2D in Table 3e. The performance is similar for plain aggregation x = Avg(x) and aggregation with a shortcut (Sc.) branch x = x + Avg(x), with Sc. being slightly better. Separating the batchnorm (Eq. 8) for the shortcut and the aggregation branch brings notable improvement. Strided max and mix (avg+max) pooling slightly underperform the average pooling variant. Overall, the combination of TAda-Conv and our feature aggregation scheme has an advantage over the TSN baseline of 31.8%.  Table 3b shows that calibrating the channel dimension more suitable than the spatial dimension, which means that the spatial structure of the original convolution kernel should be retained. Within channels, the calibration works better on C in than C out or both combined. This is probably because the calibration weight generated by the input feature can better adapt to itself.\n\nDifferent stages employing TAdaConv. Fig 4 shows the stage by stage replacement of the spatial convolutions with TAdaConv in a ResNet. A minimum improvement of 17.55% is observed when TAdaConv is used in Res2. Compared to early stages, later stages contribute more to the final performance, as later stages provide more accurate calibration because of its rich semantics. Overall, TAdaConv is used in all stages for the highest accuracy.\n\nDifferent proportion of channels calibrated. Here, we calibrate only a proportion of channels using TAdaConv and leave the other channels uncalibrated. The results are shown in Fig. 4. We find TAdaConv can improve the baseline by a large margin even if only 1/64 channels are calibrated, with larger proportion yielding further larger improvements.\n\n\nModernizing and improving TAdaBlocks\n\nWe modernize our TAdaBlock following [36] and improve it with TAdaConvV2 and temporal aggregation in Table 4. We observe a 5.6% and 9.4% improvement in the classification accuracy on K400 and SSV2, respectively, when we switch the base model from ResNet [1] to ConvNeXt [36]. Substituting the depth-wise convolution for TAdaConv further brings a 0.9% and 17.6% improvement. Following [48], [49], we employ a tubelet embedding stem (T-Down) in our TAdaConvNeXt, instantiated as a 3D convolution with temporal downsampling and an increased number of frames to keep the overall computation unchanged.\n\nOn top of our TAdaConvNeXt model, we improve TAd-aBlock by replacing TAdaConv with TAdaConvV2 and introducing the temporal aggregation scheme (T-Pool). The structural modification further leads to a performance gain of 0.9% and 2.0% on K400 and SSV2, respectively. Finally, with stronger augmentation (m7 to m9 for RandAugment [93]), we achieve an accuracy of 79.6% and 67.2% on the two benchmarks with our tiny model.\n\n\nAblative analysis on TAdaConvV2 and TAdaBlocks\n\nTAdaConvV2 and T-Pool in TAdaBlocks. Table 5 presents the ablative analysis on the TAdaBlock in both TAdaFormer  and TAdaConvNeXtV2, specifically with respect to TAda-ConvV2 and the temporal aggregation strategy. The baseline of TAdaFormer pretrained by CLIP [59] demonstrates a strong spatial modeling capability, achieving an impressive accuracy of 83.6% on K400. However, its ability to model complex dynamics is lacked. Introducing TAdaBlock with simple spatial convolution in between and no temporal aggregation brings negligible effect. On top of this, TAdaConvV2 notably improves the model in terms of temporal modeling, improving the performance on scenerelated benchmark K400 by 0.9% while bringing a 20% performance gain on the temporal-related benchmark SSV2. On top of this, employing temporal aggregation (T-Pool) and tubelet embedding (Temp. Down.) further enhances the model's ability to model complex temporal dynamics.\n\nCompared to TAdaFormer, since TAdaConvNeXtV2 is pre-trained on ImageNet, the baseline performance is slightly lower. All three strategies bring notable improvements to both the scene-and temporal-centric benchmarks.\n\nPre-training. We explore different pre-trained weights as initialization for TAdaConvNeXt and TAdaFormer in Table 6. For TAdaConvNeXtV2, pre-training on K400 benefits SSV2 performance. For TAdaFormer, using pre-trained weights of CLIP [59] outperforms the ImageNet pre-trained ones on both K400 and SSV2. CLIP+K710 initialization further improve the CLIP pre-trained variant by 2.1% on K400, but the effect on SSV2 is less significant (0.1%). For the comparison against the state-of-the-art, we use ImageNet and CLIP as the default pre-training source respectively for TAdaConvNeXtV2 and TAdaFormer.\n\n\nMain results\n\nKinetics-400. Table 7 shows the results on Kinetics-400 without large-scale pre-training. TAdaConvNeXtV2 surpasses most existing approaches with a similar computation budget both when pre-trained on ImageNet-1K and ImageNet-21K. A highlight is observed where our TAdaConvNeXtV2-S with 32 frames outperforms Swin-B by 1.3 using only 57% of the computation. Table 8 presents the comparison for models with largescale pre-training. Compared to existing CLIP pre-trained models, TAdaFormer achieves competitive performance. When post-pre-trained on K710, TAdaFormer outperforms UniFormerV2 by a notable margin under similar computation budgets. We also observe better scalability of TAdaFormer when it is compared with TAdaConvNeXtV2.\n\nSomething-Something-V1 and V2. We show the performance comparison on temporal-related datasets, i.e., SSV1 and SSV2, in Table 9. TAdaConvNeXt and TAdaFormer achieve a favorable performance against existing convolutional and transformer-based models with identical or similar pre-training sources, respectively. Compared to the best convolutional model TDN-R101, TAdaConvNeXt-B outperforms it by 3.9 and 2.9 on SSV1 and SSV2. Compared to CLIP-pre-trained UniFormerV2-L/14, TAdaFormer-L/14 achieves an improvement of 0.8 and 0.5 on the two datasets.\n\nEpic-Kitchens-100. We compare the performance on egocentric action recognition in Table 10. Compared to existing convolutional models, our TAdaConvNeXtV2-S achieves a favourable performance. Notably, we observe a higher accuracy for TAdaConvNeXt models on noun recognition in ego-centric videos. Transformer-based models are generally stronger than convolutional ones on EK100, where our TAdaFormer achieves a competitive performance with existing Transformers for video understanding. Zero-shot classification on UCF101 and HMDB51. To more comprehensively evaluate our TAdaFormer, we include the results on zero-shot classification in Table 11. Here, we initialize the model with CLIP pre-trained weights and train our TAdaFormer with the corresponding language model [59]. We observe a notable improvement of TAdaFormer-B/16 on both datasets compared to the finetuned CLIP ViFi-CLIP [110]. On top of this, we find scaling up the model and pre-training brings a further boost to the zero-shot performance.\n\n\nEVALUATIONS ON ACTION LOCALIZATION\n\nDataset, pipeline, and evaluation. Action localization is an essential task for understanding untrimmed videos, whose current pipeline makes it heavily dependent on the quality of the video representations. We evaluate our TAda-ConvNeXtV2 and TAdaFormer on two large-scale action localization datasets, HACS [86] and Epic-Kitchens-100 [85]. The general pipeline follows [85], [125], [126], which uses Boundary Matching Network (BMN) [122] for generating action boundaries. For evaluation, we use the average mean Average Precision (average mAP) at IoU [0.5:0.05:0.95] for  Main results. We present the results on the two datasets in Table 12 and Table 13. On HACS, we found BMN [122] using TAdaFormer and TAdaConvNeXt features yields a favourable performance compared to some recent meth-  \n\n\nCONCLUSIONS\n\nBased on our preliminary work [23], this work presents TAdaConvV2 in replacement of the convolution operations in existing models for video understanding, and two strong video models, i.e., TAdaConvNeXtV2 and TAdaFormer.\n\nWith large-scale pre-training and post-pre-training, our video models demonstrate competitive performances to the state-of-the-art approaches, both in the task of action recognition and localization. We hope our work can facilitate further research in video understanding. \n\n\nAPPENDIX A OVERVIEW\n\nIn the appendix, we provide detailed analysis on the temporal convolutions (Appendix B), further implementation details (Appendix C) on the action classification and localization, model structures that we used for evaluation (Appendix D), per-category improvement analysis on Something-Something-V2 (Appendix E), further plugin evaluations on Epic-Kitchens classification (Appendix G) plug-in evaluations on the temporal action localization task (Appendix H), the visualization of the training procedure of TSN and TAda2D (Appendix I), as well as detailed comparisons between TAdaConv and existing dynamic filters (Appendix J).\n\n\nAPPENDIX B DETAILED ANALYSIS ON TEMPORAL CONVOLUTIONS\n\nHere, we provide a detailed analysis to showcase the underlying process of temporal modeling by temporal convolutions. As in Sec. 3.1, we use depth-wise temporal convolutions for simplicity and its wide application. We first analyze the case where temporal convolutions are directly placed after spatial convolutions without non-linear activation in between, before activation functions are inserted in the second part of our analysis. Without activation. We first consider a simple case with no non-linear activation functions between the temporal convolution and the spatial convolution. Given a 3\u00d71\u00d71 depth-wise temporal convolution parameterized by \u03b2 = [\u03b2 1 , \u03b2 2 , \u03b2 3 ], where \u03b2 1 , \u03b2 2 , \u03b2 3 \u2208 R Co , a spatial convolution parameterized by W \u2208 R Co\u00d7Ci\u00d7K 2 , the output featurex t of the t-th frame can be obtained by: (9) where \u00b7 denotes element-wise multiplication with broadcasting, and * denotes convolution over the spatial dimension. In this case, \u03b2 could be grouped with the spatial convolution weight W and the combination of temporal and spatial convolution can be rewritten as:\nx t = \u03b2 1 \u00b7 (W * x t\u22121 ) + \u03b2 2 \u00b7 (W * x t ) + \u03b2 3 \u00b7 (W * x t+1 ) ,x t = W t\u22121 * x t\u22121 + W t * x t + W t+1 * x t+1 ,(10)\nwhere\nW t\u22121 = \u03b2 1 \u00b7 W, W t = \u03b2 2 \u00b7 W and W t+1 = \u03b2 3 \u00b7 W.\nThis equation shares the same form with the Eq. 2 in the manuscript. In this case, the combination of temporal convolution with spatial convolution can be certainly viewed as the temporal convolution simply performs calibration on spatial convolutions before aggregation, with different weights assigned to different time steps for the calibration. With activation. Next, we consider a case where activation is in between the temporal convolution and spatial convolution. The output featurex t are now obtained by:\nx t = \u03b2 1 \u00b7\u03b4(W * x t\u22121 )+\u03b2 2 \u00b7\u03b4(W * x t )+\u03b2 3 \u00b7\u03b4(W * x t+1 ) . (11)\nNext, we show that this can be still rewritten in the form of Eq. 2. Here, we consider the case where ReLU [80] is used as the activation function, denoted as \u03b4:\n\u03b4(x) = x x > 0 0 x \u2264 0 .(12)\nHence, the term \u03b4(W * x t ) can be easily expressed as:\n\u03b4(W * x t ) = M t \u00b7 W * x t ,(13)\nwhere M t \u2208 R C\u00d7H\u00d7W is a binary map sharing the same shape as x t , indicating whether the corresponding element in W * x t is greater than 0 or not. That is:\nM (c,i,j) t = 1 if (W * x t ) (c,i,j) > 0 0 if (W * x t ) (c,i,j) \u2264 0 ,(14)\nwhere c, i, j are the location index in the tensor. Hence, with activation, temporal convolution can be expressed as:\nx t = \u03b2 1 \u00b7M t\u22121 \u00b7W * x t\u22121 +\u03b2 2 \u00b7M t \u00b7W * x t +\u03b2 3 \u00b7M t+1 \u00b7W * x t+1 .(15)\nIn this case, we can set W (i,j)\nt\u22121 = \u03b2 1 \u00b7 M (i,j) t\u22121 \u00b7 W, W (i,j) t = \u03b2 2 \u00b7 M (i,j) t \u00b7 W, and W (i,j) t+1 = \u03b2 3 \u00b7 M (i,j)\nt+1 \u00b7 W, where (i, j) indicate the spatial location index. In this case, each filter for a specific time step t is composed of H \u00d7 W filters and Eq. 1 can be rewritten as Eq. 2. Interestingly, it can be observed that with ReLU activation function, the convolution weights are different for all spatio-temporal locations, since the binary map M depends on the results of the spatial convolutions.\n\n\nAPPENDIX C FURTHER IMPLEMENTATION DETAILS\n\nHere, we further describe the implementation details for the action classification and action localization experiments. For fair comparisons, we keep all the training strategies the same for our baseline, the plug-in evaluations as well as our own models.\n\n\nC.1 Action classification with TAdaConvNeXtV2\n\nWe evaluate our approach on action classification using four large-scale benchmarks. We list the training configurations for TAdaConvNeXtV2 and TAdaFormer on action classification benchmarks in Table A1 and Table A2, respectively.\n\n\nC.2 Action Localization\n\nWe evaluate our model on the action localization task using two large-scale datasets. The overall pipeline for our action localization evaluation is divided into finetuning the classification models, obtaining action proposals, and classifying the proposals.\n\nFinetuning. On Epic-Kitchens, we simply use the evaluated action classification model. On HACS, following [126], we initialize the model with Kinetics-400 pre-trained weights and train the model with adamW [92] for 30 epochs (8 warmups) using 32 GPUs. The mini-batch size is 16 videos per GPU. The base learning rate is set to 0.0002, with cosine learning rate decay as in Kinetics. In our case, only the segments with action labels are used for training.\n\nProposal generation. For the action proposals, a boundary matching network (BMN) [122] is trained over the extracted features on the two datasets. On Epic-Kitchens, we extract features with the videos uniformly decoded at 60 FPS. For each clip, we use 8 frames with an interval of 8 to be consistent with finetuning, which means a feature roughly covers a video clip of one seconds. The interval between each clip for feature extraction is 8 frames (i.e., 0.133 sec) as well. The shorter side of the video is resized to 224 and we feed the whole spatial region into the backbone to retain as much information as possible. Following [125], we generate proposals using BMN based on sliding windows. The predictions on the overlapped region of different sliding windows are simply averaged. On HACS, the videos are decoded at 30 FPS, and extend the interval between clips to be 16 (i.e., 0.533 sec) because the actions in HACS last much longer than in Epic-Kitchens. The shorter side is resized to 128 for efficient processing. For the settings in generating proposals, we mainly follow [126], except that the temporal resolution is resized to 100 in our case instead of 200.\n\nClassification. On Epic-Kitchens, we classify the proposals with the fine-tuned model using 6 clips. Spatially, to comply with the feature extraction process, we resize the shorter side to 224 and feed the whole spatial region to the model for classification. On HACS, considering the property of the dataset that only one action category can exist in a video, we obtain the video level classification results by classifying the video level features, following [126].\n\nAction localization with ActionFormer. We follow all the settings in [123], [124] for action localization experiments with ActionFormer.\n\nEvaluation. For evaluation, we follow the standard evaluation protocol used in the respective datasets, i.e., the average mean Average Precision (average mAP) at IoU threshold [0.5:0.05:0.95] for HACS [86] and [0.1:0.1:0.5] for Epic-Kitchens-100 [85].\n\n\nAPPENDIX D MODEL STRUCTURES\n\nThe detailed model structures for R2D, R(2+1)D and R3D is specified in Table A3. We highlight the convolutions that are replaced by TAdaConv by default or optionally. For all of our models, a small modification is made in that we remove the max pooling layer after the first convolution and set the spatial stride of the second stage to be 2, following [32]. Temporal resolution is kept unchanged following recent works [19], [30], [128]. Our R3D is obtained by simply expanding the R2D baseline in the temporal dimension by a factor of three. We initialize with weights reduced by 3 times, which means the original weight is evenly distributed in adjacent time steps. We construct the R(2+1)D by adding a temporal convolution operation after the spatial convolution. The temporal convolution can also be optionally replaced by TAdaConv, as shown in both the manuscript and Table A5. For its initialization, the temporal convolution weights are randomly initialized, while the others are initialized with the pre-trained weights on ImageNet. For SlowFast models, we keep all the model structures identical to the original work [19].\n\nFor TAdaConvNeXt, we keep most of the model architectures as in ConvNeXt [36], except that we use a tubelet embedding similar to [49], with a size of 3\u00d74\u00d74 and stride of 2\u00d74\u00d74. Center initialization is used as in [49]. Based on this, we simply replace the depth-wise convolutions with TAda-Conv to construct TAdaConvNeXt. For TAdaConvNeXtV2, we additionally substitute TAdaConv for TAdaConvV2 and introduce the temporal aggregation scheme.\n\n\nAPPENDIX E PER-CATEGORY IMPROVEMENT ANALYSIS ON SSV2\n\nThis section provides a per-category improvement analysis on the Something-Something-V2 dataset in Fig.A1 and Fig. A2. In terms of overall performance, our TAda2D achieves an improvement of 31.7% over the baseline TSN, while TAdaConvNeXtV2 improves over ConvNeXt by 25.4%. Our per-category analysis shows a mean improvement of 30.35% and 21.36% over all the classes, respectively for TAda2D and TAdaConvNeXtV2. Since both TSN and ConvNeXt have no temporal modeling capabilities, and our approach introduce similar modifications to the base model, the difference pattern in the per-category accuracy is similar. Hence, we take TAda2D as an example for analysis. The largest improvement is observed in class 0 (78.5%, Approaching something with your camera), 32 ( [93] (9, 0.5) label smoothing [95] 0   [93] (9, 0.5) (9, 0.5) (9, 0.5) (9, 0.5) (9, 0.5) label smoothing [95] 0. global spatial context. For class 30, most of its actions last a long time (as it needs to be determined whether the end of something is let down or not). The improvements over the baseline mostly benefit from the global temporal context that is included in the weight generation process.\n\n\nAPPENDIX F FURTHER ABLATION STUDIES\n\nHere we provide further ablation studies on the kernel size in the calibration weight generation. As shown in Table A4a and Table A4b, kernel size does not affect the classification much, as long as the temporal context is considered. Further, Table A4c shows the sensitivity analysis on the reduction ratio, which demonstrate the robustness of our approach against different set of hyper-parameters.\n\n\nAPPENDIX G FURTHER PLUG-IN EVALUATION FOR TADACONV ON\n\n\nCLASSIFICATION\n\nIn complement to the manuscript, we further show in Table A5 the plug-in evaluation on the action classification task on the Epic-Kitchens-100 dataset. As in the plug-in evaluation on Kinetics and Something-Something-V2, we compare performances with and without TAdaConv over three baseline models, SlowFast [19], R(2+1)D [13] and R3D [100] respectively representing three kinds of temporal modeling techniques. The results are in line with our observation in the plug-in evaluation in the manuscript. Over all three kinds of temporal modelling strategies, adding TAdaConv further improves the recognition accuracy of the model.\n\n\nAPPENDIX H PLUG-IN EVALUATION FOR TADACONV ON ACTION LOCALIZATION\n\nHere, we show the plug-in evaluation on the temporal action localization task. Specifically, we use SlowFast as our baseline, as it is shown to be superior in the localization performance in [129] compared to many early backbones. The result is presented in Table A6. With TAdaConv, the average mAP on HACS is improved by 1.4%, and the average mAP on Epic-Kitchens-100 action localization is improved by 1.0%.  \n\n\nAPPENDIX I COMPARISON OF TRAINING PROCEDURE\n\nwe compare the training procedure of TSN and TAda2D on Kinetics-400 and Something-Something-V2 in Fig. A3, and that of ConvNeXt and TAdaConvNeXtV2 in Fig. A4. Although TAda2D and TAdaConvNeXtV2 are initialized to be identical to TSN and ConvNeXt, both TAda2D and TAdaConvNeXtV2 demonstrates a stronger performance on both training and validation sets.\n\n\nAPPENDIX J COMPARISON WITH EXISTING DYNAMIC FILTERS\n\nIn this section, we compare our TAdaConv with previous dynamic filters in two perspectives, respectively the difference in the methodology and in the performance.\n\n\nJ.1 Comparison in terms of methodology\n\nWe compare TAdaConv with several representative dynamic filtering approaches in image and in videos, respec-tively CondConv [11], DynamicFilter [21], DDF [17] and TAM [31].\n\nThe first difference in terms of methodology lies in the source of weights, where previous approaches obtain weights by mixture of experts or generation completely dependent on the input. Mixture of experts denotes W = n \u03b1 n W n , where \u03b1 n is a scalar obtained by a function f , i.e., W = n f (x) n W n . Completely generated means the weights are only dependent on the input, i.e., W = g((x)), where g generates complete kernel for the convolution. In comparison, the weights in TAdaConv are obtained by calibration, i.e,, W = \u03b1W b , where \u03b1 is a vector calibration weight and \u03b1 = h((x)) where h(.) generates the calibration vector for the convolutions. Hence, this fundamental difference in how to obtain the convolution weights makes the previous approaches difficult to exploit pre-trained weights, while TAdaConv can easily load pre-trained weights in W b . This ability is essential for video models to speed up the convergence.\n\nThe second difference lies in the ability to perform Class Index   Accuracy   0   20   40   60   80   0  10  20  30  40  50  60  70  80  90  100  110  120  130  140  150  160  170   ConvNeXt  TAdaConvNeXtV2   Class Index   0  0  10  20  30  40  50  60  70  80  90  100  110  120  130  140  150 160 170 Fig. A2: Per-category performance comparison of TAdaConvNeXtV2 against the baseline ConvNeXt. We achieve an average per-category performance improvement of 21.36%.  only local but also global temporal contexts. Details on the source of the weight generation process is also shown in Table A8. The third difference lies in whether the weights generated are shared for different locations. For CondConv, DynamicFilter and TAM, their generated weights are shared for all locations, while for DDF, the weights are varied according to spatial locations. In comparison, TAdaConv generates temporally adaptive weights.\n\n\nJ.2 Comparison in the performance level\n\nSince TAdaConv is fundamentally different from previous approaches in the generation of calibration weights, it is difficult to directly compare the performance on video modelling, especially for those that are not designed for video modelling. However, since the calibration weight in TAdaConv \u03b1 is completely generated, i.e., \u03b1 = f ((x)), we can use other dynamic filters to generate the calibration weights for TAdaConv. Since MoE-based approaches such as CondConv were essentially designed for applications   with less memory constraint but high computation requirements, it is not suitable for video applications since it would be too memory-heavy for video models. Hence, we apply approaches that generate complete kernel weights to generate calibration weights and compare them with TAdaConv. The performance is listed in Table A8.\n\nIt is worth noting that these approaches originally generate weights that are randomly initialized. However, as is shown in the manuscript, our initialization strategy for the calibration weights are essential for yielding reasonable results, we further apply our initialization on these existing approaches to see whether their generation function is better than the one in TAdaConv. In the following paragraphs, we provide details for applying representative previous dynamic filters in TAdaConv to generate the calibration weight.\n\nFor DynamicFilter [21], the calibration weight \u03b1 is generated using an MLP over the global descriptor that is obtained by performing global average pooling over the whole input GAP st , i.e., \u03b1 = MLP(GAP st (x)). In this case, the calibration weights are shared between different time steps.\n\nFor DDF [17], we only use the channel branch since it is shown in the manuscript that it is better to leave the spatial structure unchanged for the base kernel. Similarly, the weights in DDF are also generated by applying an MLP over the global descriptor, i.e., \u03b1 = MLP(GAP st (x)). The difference between DDF and DynamicFilter is that for different time step, DDF generates a different calibration weight.    The original structure of TAM [31] only generates kernel weights with its global branch and uses the local branch to generate attention maps over different time steps. In our experiments, we modify the TAM a little bit and further make the local branch generate kernel calibration weights as well. Hence, for the only-global version of TAM, the calibration weights are calculated as follows: \u03b1 = G(GAP s (x)), where GAP s denotes global average pooling over the spatial dimension and G denotes the global branch in TAM. In this case, calibration weights are shared for all temporal locations. For local+global version of TAM, the calibration weight are calculated by combining the results of the local L and the global branch G, i.e., \u03b1 = G(GAP s (x))\u00b7L(GAP s (x)), where \u00b7 denotes element-wise multiplication with broadcasting. This means in this case, the calibration weights are temporally adaptive. Note that this is our modified version of TAM. The original TAM droes not have temporally adaptive convolution weights.\n\nThe results in Table A8 show that (a) without our initialization strategy, previous approaches that generate random weights at initialization are not suitable for generating the calibration weights in TAdaConv; (b) our initialization strategy can conveniently change this and make previous approaches yield reasonable performance when they are used for generating calibration weights; and (c) the calibration weight generation function in TAdaConv, which combines the local and global context, outperform all previous approaches for calibration.\n\nFurther, when we compare TAdaConv without global information with TAM (local*+global branch), it can be seen that although both approach generates temporally varying weights from the frame descriptors GAP s (x) with shape C \u00d7 T , our TAdaConv achieves a notably higer performance. Adding the global information enables TAdaConv to achieve a more notable lead in the comparison with previous dynamic filters.\n\n\u2022\nZiyuan Huang and Marcelo H. Ang Jr are with Advanced Robotics Centre, National University of Singapore. \u2022 Shiwei Zhang and Yingya Zhang is with DAMO Academy, Alibaba Group. \u2022 Liang Pan and Ziwei Liu are with S-Lab, National Technological University. \u2022 Zhiwu Qing is with Key Laboratory of Image Processing and Intelligent Control, School of Artificial Intelligence and Automation, Huazhong University of Science.\n\nFig. 1 :\n1Comparisons between TAdaConv and the spatial convolutions in video models. (a) Standard spatial convolutions in videos share the kernel weights between different frames. (b) Our TAdaConv adaptively calibrates the kernel weights for each frame by its temporal context.\n\nFig. 2 :\n2Instantiations of TAdaConv and TAdaConvV2. (a) Standard convolutions used in video models. (b) Our TAdaConv using non-linear weight calibrations with global temporal context. (c) TAdaConvV2 with global temporal context provided by multi-head self-attention (MHSA).\n\nFig. 3 :\n3TAdaBlock designs for both convolutional and transformer-based models.\n\nFig. 4 :\n4The classification accuracy of TAda2D on SSV2 with different channels (C.) and stages (S.) enabled.\n\nFig. A1 :\nA1Per-category performance comparison of TAda2D against the baseline TSN. We achieve an average per-category performance improvement of 30.35%.\n\nFig. A3 :\nA3Training and validation on Kinetics-400 and Something-Something-V2 for TAda2D. On both datasets, TAda2D shows a stronger capability of fitting the data and a better generality to the validation set. Further, TAda2D reduces the overfitting problem in Something-Something-V2.\n\nFig. A4 :\nA4Training and validation on Kinetics-400 and Something-Something-V2 for TAdaConvNeXt.\n\n\nGAPst(x)(C \u00d7 1) 59.2 and GAPs(x)(C \u00d7 T )\n\nTABLE 1 :\n1Comparison of (2+1)D convolution and TAda-\nConv in FLOPs and number of parameters. Example setting \nfor the operation: C o = C i = 64, K = 3, T = 8, \nH = W = 56 and r = 4. Example setting for the net-\nwork: ResNet-50 with input resolution 8 \u00d7 224 2 . Colored \nnumbers denote the extra FLOPs/parameters introduced to \n2D convolutions or ResNet-50. Refer to Appendix for model \nstructures. \n\n\n\nTABLE 2 :\n2Comparison with existing dynamic filters in terms of temporal modeling capability, location adaptiveness and the ability to exploit pre-trained weights in existing models.Temporal Location Pretrained \nOperations \nmodeling adaptive \nweights \nCondConv [11] \n\u2717 \n\u2717 \n\u2717 \nDynamicFilter [21] \n\u2717 \n\u2717 \n\u2717 \nDDF [17] \n\u2717 \n\u2713 \n\u2717 \nTAM [31] \n\u2713 \n\u2717 \n\u2717 \nTAdaConv(V2) \n\u2713 \n\u2713 \n\u2713 \n\n\nTABLE 3 :\n3Verification of hypothesis, plug-in evaluation, and in-depth ablative experiments on TAdaConv.\n\nTABLE 4 :\n4Modernizing and improving TAdaBlocks.Model \nVariant \nK400 SSV2 \n\nResNet2D \nBaseline \n70.4 \n32.0 \n\u2193 \n+ TAdaConv \n73.9 \n59.2 \nTAda2D \n+ T-Pool \n76.7 \n64.0 \n\nConvNeXt \nBaseline \n76.0 \n41.4 \n\u2193 \n+ TAdaConv \n76.9 \n59.0 \nTAdaConvNeXt-T \n+ T-Down \n78.4 \n64.8 \n\n\u2193 \n+ TAdaConvV2 78.9 \n66.0 \n+ T-Pool \n79.3 \n66.8 \nTAdaConvNeXtV2-T + Stronger Aug 79.6 \n67.2 \n\nCalibration dimension. Multiple dimensions can be \ncalibrated in the base weight. \n\nTABLE 5 :\n5Ablation study on the TAdaBlock.TAdaBlock \nTemp. \nModel \nTAdaConvV2 T-Pool Down. K400 SSV2 \n\n\u2022 ViT-B/16 \n\nN/A \nN/A \n\n\u2717 \n83.6 48.1 \n\u2022 TAdaFormer-B/16 \n\u2717 \n\u2717 \n\u2717 \n83.6 48.2 \n\u2022 TAdaFormer-B/16 \n\u2713 \n\u2717 \n\u2717 \n84.5 68.6 \n\u2022 TAdaFormer-B/16 \n\u2713 \n\u2713 \n\u2717 \n84.5 69.2 \n\u2022 TAdaFormer-B/16 \n\u2713 \n\u2713 \n\u2713 \n84.5 70.4 \n\n\u2022 ConvNeXt-T \n\u2717 \n\u2717 \n\u2717 \n77.2 46.2 \n\u2022 TAdaConvNeXtV2-T \n\u2713 \n\u2717 \n\u2717 \n78.0 63.3 \n\u2022 TAdaConvNeXtV2-T \n\u2713 \n\u2713 \n\u2717 \n79.3 66.3 \n\u2022 TAdaConvNeXtV2-T \n\u2713 \n\u2713 \n\u2713 \n79.6 67.2 \n\n\n\nTABLE 6 :\n6Ablation study on different pre-training sources.Model \nPretrain \nK400 SSV2 \n\nTAdaConvNeXtV2-T \nIN1K \n79.6 \n65.2 \nIN1K+K400 \n-\n67.2 \n\nTAdaFormer-B/16 \n\nIN1K \n76.3 \n63.9 \nIN21K \n81.8 \n67.5 \nCLIP \n84.5 \n70.4 \nCLIP+K710 \n86.6 \n70.5 \n\nTimeSformer [50] \nIN21K \n78.7 \n59.5 \n\nUniFormerV2-B/16 [91] \n\nIN21K \n81.6 \n67.5 \nCLIP \n84.4 \n69.5 \nCLIP+K710 \n85.6 \n-\n\n\n\nTABLE 7 :\n7Classification accuracy on Kinetics-400[83].Model \n#frames #param. GFLOPs\u00d7views Top-1 \n\nModels without pretraining \n\n\u2022 SlowFast 8\u00d78 [19] \n8+32 \n34.5M \n66\u00d73\u00d710 77.0 \n\u2022 MViTv2-B [101] \n32 \n51.2M \n225\u00d71\u00d75 82.9 \n\nImageNet-1K pretrained models \n\n\u2022 TSM [28] \n8 \n24.3M \n43\u00d73\u00d710 74.1 \n\u2022 TAda2D [23] \n16 \n27.5M \n86\u00d73\u00d710 77.4 \n\u2022 TAdaConvNeXt-T [23] \n32 \n38.6M \n94\u00d73\u00d74 79.1 \n\u2022 TANet [31] \n16 \n25.6M \n242\u00d73\u00d74 79.3 \n\u2022 TDN-R101 [29] \n8+16 \n-\n258\u00d73\u00d710 79.4 \n\u2022 X3D-XXL [20] \n-\n20.3M \n194\u00d73\u00d710 80.4 \n\u2022 Swin-T [48] \n32 \n28.2M \n88\u00d73\u00d74 78.8 \n\u2022 Swin-S [48] \n32 \n49.8M \n166\u00d73\u00d74 80.6 \n\u2022 Swin-B [48] \n32 \n88.1M \n282\u00d73\u00d74 80.6 \n\u2022 MoViNet-A6 [102] \n120 \n31.4M \n386\u00d71\u00d71 81.5 \n\u2022 TAdaConvNeXtV2-T \n16 \n45.9M \n47\u00d73\u00d74 79.6 \n\u2022 TAdaConvNeXtV2-T \n32 \n45.9M \n94\u00d73\u00d74 80.8 \n\u2022 TAdaConvNeXtV2-S \n16 \n82.2M \n91\u00d73\u00d74 80.8 \n\u2022 TAdaConvNeXtV2-S \n32 \n82.2M \n183\u00d73\u00d74 81.9 \n\u2022 TAdaConvNeXtV2-B \n16 \n145.7M \n162\u00d73\u00d74 81.4 \n\u2022 TAdaConvNeXtV2-B \n32 \n145.7M \n324\u00d73\u00d74 82.3 \n\nImageNet-21K pretrained models \n\n\u2022 X-ViT [103] \n16 \n-\n283\u00d73\u00d71 80.2 \n\u2022 TimeSformer [50] \n96 \n121.4M \n2380\u00d73\u00d71 80.7 \n\u2022 ViViT-L [49] \n16 \n310.8M \n1446\u00d73\u00d74 80.6 \n\u2022 MTV-B\u2191 320 2 [104] \n32 \n310M \n930\u00d73\u00d74 82.4 \n\u2022 Swin-B [48] \n32 \n88.1M \n282\u00d73\u00d74 82.7 \n\u2022 Swin-L [48] \n32 \n197.0M \n604\u00d73\u00d74 83.1 \n\u2022 MViT-v2-L\u2191 312 2 [101] \n40 \n217.6M \n2828\u00d73\u00d75 86.1 \n\u2022 TAdaConvNeXtV2-S \n32 \n82.2M \n183\u00d73\u00d74 82.9 \n\u2022 TAdaConvNeXtV2-B \n32 \n145.7M \n324\u00d73\u00d74 83.7 \n\n\n\nTABLE 8 :\n8Classification accuracy on Kinetics-400[83] with large-scale pre-training and post-pre-training.Model \n#frames #param. GFLOPs\u00d7views Top-1 \nOther large-scale pretrained models \n\n\u2022 MAE-ST [57] \n16 \n632M 1193\u00d73\u00d77 85.1 \n\u2022 MAR [105] \n16 \n311M \n276\u00d73\u00d75 85.3 \n\u2022 MaskFeat [106] \n40 \n218M 3790\u00d73\u00d74 87.0 \n\u2022 CoVeR [107] (JFT-3B) \n16 \n-\n-87.2 \n\u2022 MTV-H(WTS)\u2191 280 2 [104] \n32 \n-6130\u00d73\u00d74 89.9 \n\u2022 VideoMAE V2-g \u2191 266 2 [64] 64 \n-26716\u00d73\u00d72 90.0 \n\nCLIP pretrained models \n\n\u2022 UniFormerV2-B/16 [91] \n8 \n115M \u223c150\u00d73\u00d74 84.4 \n\u2022 ST-Adapter-B/16 [65] \n32 \n93M \n607\u00d73\u00d71 82.0 \n\u2022 EVL ViT-B/16 [108] \n32 \n115M \n592\u00d73\u00d71 84.2 \n\u2022 X-CLIP-B/16 [109] \n16 \n-\n287\u00d73\u00d74 84.7 \n\u2022 ViFi-CLIP [110] \n16 124.7M \n281\u00d74\u00d73 83.9 \n\u2022 TAdaFormer-B/16 \n16 104.1M \n153\u00d73\u00d74 84.5 \n\u2022 ST-Adapter-L/14 [65] \n32 \n347M 2749\u00d73\u00d71 87.2 \n\u2022 EVL ViT-L/14 [108] \n32 \n363M 2696\u00d73\u00d71 87.3 \n\u2022 X-CLIP-L/14 [109] \n8 \n-\n658\u00d73\u00d74 87.1 \n\u2022 TAdaFormer-L/14 \n16 \n364M \n703\u00d73\u00d74 87.6 \n\nCLIP+K710 post-pretrained models \n\n\u2022 UniFormerV2-B/16 [91] \n8 \n115M \u223c150\u00d73\u00d74 85.6 \n\u2022 TAdaConvNeXtV2-S \n32 \n82.2M \n183\u00d73\u00d74 86.1 \n\u2022 TAdaConvNeXtV2-B \n32 145.7M \n324\u00d73\u00d74 86.4 \n\u2022 TAdaFormer-B/16 \n16 104.1M \n153\u00d73\u00d74 86.6 \n\u2022 UniFormerV2-L/14 [91] \n8 \n354M \u223c667\u00d73\u00d74 88.8 \n\u2022 UniFormerV2-L/14 [91] \n16 \n354M \u223c1334\u00d73\u00d74 89.1 \n\u2022 UniFormerV2-L/14 [91] \n32 \n354M \u223c2667\u00d73\u00d74 89.5 \n\u2022 TAdaFormer-L/14 \n16 \n364M \n703\u00d73\u00d74 88.9 \n\u2022 TAdaFormer-L/14 \n32 \n364M 1406\u00d73\u00d74 89.5 \n\u2022 TAdaFormer-L/14 \n64 \n364M 2812\u00d73\u00d74 89.9 \n\n\n\nTABLE 9 :\n9Classification accuracy on SSV1 and SSV2. \u2020 indicates initialization with ImageNet21K+K400 pre-training. \u22c6 indicates initialization with CLIP-400M pre-training.Model \n#frames GFLOPs\u00d7views SSV1 SSV2 \n\n\u2022 TSM [28] \n16 \n86\u00d73\u00d72 47.2 63.4 \n\u2022 MoViNet-A3 [102] \n50 \n24\u00d71\u00d71 \n-64.1 \n\u2022 TANet [31] \n16 \n86\u00d73\u00d72 47.6 64.6 \n\u2022 TEANet [111] \n16 \n86\u00d71\u00d71 48.9 \n-\n\u2022 TEANet [111] \n16 \n86\u00d73\u00d710 \n-65.1 \n\u2022 TAda2D [23] \n16 \n86\u00d73\u00d72 \n-65.6 \n\u2022 TAdaConvNeXt-T [23] \n32 \n94\u00d73\u00d72 \n-67.1 \n\u2022 TDN-R101 [29] \n8+16 \n258\u00d71\u00d71 56.8 68.2 \n\u2022 TAdaConvNeXtV2-T \n16 \n47\u00d73\u00d72 54.1 67.2 \n\u2022 TAdaConvNeXtV2-T \n32 \n94\u00d73\u00d72 56.4 69.8 \n\u2022 TAdaConvNeXtV2-S \n16 \n91\u00d73\u00d72 55.6 68.4 \n\u2022 TAdaConvNeXtV2-S \n32 \n183\u00d73\u00d72 58.5 70.0 \n\u2022 TAdaConvNeXtV2-S  \u2020 \n32 \n183\u00d73\u00d72 59.7 70.6 \n\u2022 TAdaConvNeXtV2-B  \u2020 \n32 \n324\u00d73\u00d72 60.7 71.1 \n\n\u2022 ViViT-L/16x2 FE [49] \n32 \n903\u00d73\u00d74 \n-65.4 \n\u2022 X-ViT [103] \n16 \n283\u00d73\u00d71 \n-67.2 \n\u2022 MTV-B\u2191 320 2 [104] \n32 \n930\u00d73\u00d74 \n-68.5 \n\u2022 Swin-B  \u2020 [48] \n32 \n321\u00d73\u00d71 \n-69.6 \n\u2022 MViTv2-B [101] \n32 \n225\u00d73\u00d71 \n-70.5 \n\u2022 ST-Adapter-B/16 \u22c6 [65] \n32 \n651\u00d73\u00d71 \n-69.5 \n\u2022 ST-Adapter-L/14 \u22c6 [65] \n32 \n2749\u00d73\u00d71 \n-72.3 \n\u2022 UniFormerV2-B/16 \u22c6 [91] \n32 \n\u223c370\u00d73\u00d72 59.5 71.0 \n\u2022 UniFormerV2-L/14 \u22c6 [91] \n32 \n\u223c1716\u00d73\u00d72 62.9 73.1 \n\u2022 MViTv2-L\u2191 312 2 [101] \n40 \n2828\u00d73\u00d71 \n-73.3 \n\u2022 TAdaFormer-B/16 \u22c6 \n16 \n187\u00d73\u00d72 59.2 70.4 \n\u2022 TAdaFormer-B/16 \u22c6 \n32 \n374\u00d73\u00d72 61.2 71.3 \n\u2022 TAdaFormer-L/14 \u22c6 \n16 \n858\u00d73\u00d72 62.0 72.4 \n\u2022 TAdaFormer-L/14 \u22c6 \n32 \n1716\u00d73\u00d72 63.7 73.6 \n\n \n\nTABLE 10 :\n10Classification accuracy on Epic-Kitchens-100[85]. \u2191 indicates the main evaluation metric for the dataset.Model \nAct.\u2191 Verb Noun \n\n\u2022 TSN [99] \n33.2 \n60.2 \n46.0 \n\u2022 TRN [112] \n35.3 \n65.9 \n45.4 \n\u2022 TSM [28] \n38.3 \n67.9 \n49.0 \n\u2022 SlowFast [19] \n38.5 \n65.6 \n50.0 \n\u2022 TAda2D [23] \n41.6 \n65.1 \n52.4 \n\u2022 ir-CSN-152 [113] \n44.5 \n68.4 \n55.9 \n\u2022 MoViNet-A6 [102] \n47.7 \n72.2 \n57.3 \n\u2022 TAdaConvNeXtV2-T (IN1K) \n42.4 \n67.1 \n53.7 \n\u2022 TAdaConvNeXtV2-T (K710) \n47.4 \n70.4 \n58.6 \n\u2022 TAdaConvNeXtV2-S (K710) \n48.9 \n71.0 \n60.2 \n\n\u2022 ViViT-L/16x2 FE [49] \n44.0 \n66.4 \n56.8 \n\u2022 X-ViT [103] \n44.3 \n68.7 \n56.4 \n\u2022 ViViT-B/16x2 FE \u2191 384 [113] \n47.0 \n67.2 \n59.0 \n\u2022 ST-Adapter-B/16 [65] \n-\n67.6 \n55.0 \n\u2022 MeMViT [114] \n48.4 \n71.4 \n60.3 \n\u2022 MTV-B\u2191 320 2 [104] \n48.6 \n68.0 \n63.1 \n\u2022 MTV-B(WTS)\u2191 280 2 [104] \n50.5 \n69.9 \n63.9 \n\u2022 TAdaFormer-B/16 (K710) \n49.1 \n71.0 \n60.5 \n\u2022 TAdaFormer-L/14 (K710) \n51.8 \n71.7 \n64.1 \n\nHACS and [0.1:0.1:0.5] for EK100, following the standard \nprotocol. More details are included in the Appendix. \n\n\nTABLE 11 :\n11Zero-shot classification on UCF101 [87] and \nHMDB51 [88]. \n\nModel \nHMDB-51 UCF-101 \n\n\u2022 MTE [115] \n19.7 \u00b1 1.6 15.8 \u00b1 1.3 \n\u2022 ASR [116] \n21.8 \u00b1 0.9 24.4 \u00b1 1.0 \n\u2022 ER-ZSAR [117] \n35.3 \u00b1 4.6 51.8 \u00b1 2.9 \n\n\u2022 CLIP [59] \n40.8 \u00b1 0.3 63.2 \u00b1 0.2 \n\u2022 ActionCLIP [118] \n40.8 \u00b1 5.4 58.3 \u00b1 3.4 \n\u2022 X-CLIP-B/16 [109] \n44.6 \u00b1 5.2 72.0 \u00b1 2.3 \n\u2022 A5 [119] \n44.3 \u00b1 2.2 69.3 \u00b1 4.2 \n\u2022 ViFi-CLIP [110] \n51.3 \u00b1 0.6 76.8 \u00b1 0.7 \n\u2022 TAdaFormer-B/16 \n52.1 \u00b1 1.4 78.5 \u00b1 1.2 \n\u2022 TAdaFormer-L/14 \n57.2 \u00b1 0.7 81.1 \u00b1 0.9 \n\n\u2022 TAdaFormer-B/16 (K710) 55.9 \u00b1 0.4 79.5 \u00b1 0.7 \n\u2022 TAdaFormer-L/14 (K710) 59.7 \u00b1 0.5 83.0 \u00b1 0.7 \n\n\n\nTABLE 12 :\n12Action localization on HACS[86].HACS \n\nModel \n@0.5 @0.6 @0.7 @0.8 @0.9 Avg.\u2191 \n\nSSN [120] \n28.8 \n-\n-\n-\n-\n19.0 \nG-TAD [121] \n41.1 \n-\n-\n-\n-\n27.5 \nTadTR [51] \n47.1 \n-\n-\n-\n-\n32.1 \n\nBMN [122]+ \n\nTSN [23] \n43.6 37.7 31.9 24.6 15.0 \n28.6 \nTAda2D [23] \n48.7 42.7 36.2 28.1 17.3 \n32.3 \nTAdaFormer-L/14 51.3 44.8 38.0 30.0 18.6 \n34.1 \nTAdaConvNeXt-S 53.3 47.0 40.2 32.0 20.2 \n36.1 \n\nods. On Epic-Kitchens-100, we further employ Action-\nFormer [123] and found TAdaFormer stronger than the \nensemble of ViViT and SlowFast. Overall, we found TAda-\nConvNeXt and TAdaFormer provide strong features for \nlocalzing actions in long videos. \n\n\n\nTABLE 13 :\n13Action localization on Epic-Kitchens-100 [85]. \n\nEpic-Kitchens-100 \n\nModel \nTask @0.1 @0.2 @0.3 @0.4 @0.5 Avg.\u2191 \n\nBMN [122] \n+TSN \n\nVerb 15.98 15.01 14.09 12.25 10.01 13.47 \nNoun 15.11 14.15 12.78 10.94 8.89 12.37 \nAct.\u2191 10.24 9.61 8.94 7.96 6.79 8.71 \n\nBMN [122] \n+TAda2D [23] \n\nVerb 19.70 18.49 17.41 15.50 12.78 16.78 \nNoun 20.54 19.32 17.94 15.77 13.39 17.39 \nAct.\u2191 15.15 14.32 13.59 12.18 10.65 13.18 \n\nBMN [122] \n+TAdaFormer-L/14 \n\nVerb 20.87 20.09 18.99 16.42 13.81 18.03 \nNoun 27.75 26.28 24.51 21.86 17.97 23.67 \nAct.\u2191 20.39 19.35 18.28 16.35 14.51 17.85 \n\nBMN [122] \n+TAdaConvNeXt-S \n\nVerb 17.81 16.94 16.05 14.25 11.89 15.39 \nNoun 21.90 20.92 19.33 17.22 14.68 18.81 \nAct.\u2191 15.61 14.80 13.73 12.35 10.90 13.47 \n\nActionFormer [123] \n+SlowFast \n\nVerb 26.58 25.42 24.15 22.29 19.09 23.51 \nNoun 25.21 24.11 22.66 20.47 16.97 21.88 \nAct.\u2191 18.40 17.71 16.80 15.65 13.52 16.42 \n\nActionFormer [124] \n+SlowFast&ViViT \n\nVerb 26.97 25.90 24.21 21.77 18.47 23.46 \nNoun 28.61 27.14 24.92 22.13 18.69 24.30 \nAct.\u2191 23.90 22.98 21.37 19.57 16.94 20.95 \n\nActionFormer \n+TAdaConvNeXt-S \n\nVerb 29.11 28.37 26.99 24.22 20.64 25.86 \nNoun 29.21 27.94 26.22 23.54 18.73 25.13 \nAct.\u2191 20.78 19.75 18.56 17.07 14.54 18.14 \n\nActionFormer \n+TAdaFormer-L/14 \n\nVerb 32.08 31.09 29.40 26.64 22.71 28.38 \nNoun 35.00 33.42 30.98 27.32 22.36 29.82 \nAct.\u2191 24.92 23.68 22.33 20.61 18.29 21.97 \n\n\nTABLE A1 :\nA1TAdaConvNeXtV2 training settings on K710, K400, SSV1/SSV2, and EK100. \n\ntraining config \nK710 \nK400 (K710) \nK400 (CLIP) \nSSV1/SSV2 \nEK100 \n\noptimizer \nAdamW [92] \nlearning rate schedule \ncosine decay \nweight decay \n0.05 \noptimizer momentum \n\u03b21, \u03b22 = 0.9, 0.999 \ndropout [127] \n0.5 \nclip grading \nNone \nEMA [98] \n0.9996 \n\nBase \nLarge \nBase \nLarge \nBase \nLarge \nBase \nLarge \nBase \nLarge \n\nbase learning rate \n1e-4 \n5e-5 \n1e-5 \n5e-6 \n5e-5 \n2e-5 \n5e-4 \n2.5e-4 \n2.5e-4 \n1e-4 \nbatch size \n512 \n256 \n256 \n128 \n256 \n128 \n256 \n128 \n128 \n64 \ntraining epochs \n30 \n24 \n15 \n10 \n30 \n24 \n24 \n24 \n24 \n15 \nwarmup epochs \n5 \n5 \n2.5 \n2 \n5 \n5 \n5 \n5 \n5 \n2.5 \nlayer-wise lr decay [43] \n0.7 \n0.8 \n0.7 \n0.8 \n0.7 \n0.85 \n0.7 \n0.85 \n0.7 \n0.85 \nrandaugment \n\nTABLE A3 :\nA3Model structure of R3D, R(2+1)D and R2D that we used in our experiments. Brown and green fonts indicate \nrespectively the default convolution operation and optional operation that can be replaced by TAdaConv. (Better viewed \nin color.) \n\nStage \nR3D \nR(2+1)D \nR2D \noutput sizes \nSampling \ninterval 8, 1 2 \ninterval 8, 1 2 \ninterval 8, 1 2 \n8\u00d7224\u00d7224 \n\nconv1 \n3\u00d77 2 , 64 \n1\u00d77 2 , 64 \n1\u00d77 2 , 64 \n8\u00d7112\u00d7112 \nstride 1, 2 2 \nstride 1, 2 2 \nstride 1, 2 2 \n1\u00d71 2 , 512 \n1\u00d73 2 , 512 \n1\u00d71 2 , 2048 \n\n\uf8f9 \n\n\uf8fb \u00d73 \n8\u00d77\u00d77 \n\nglobal average pool, fc \n1\u00d71\u00d71 \n\nClass Index \n\nAccuracy \n\n0 \n\n20 \n\n40 \n\n60 \n\n80 \n\n0 \n10 \n20 \n30 \n40 \n50 \n60 \n70 \n80 \n90 \n100 \n110 \n120 \n130 \n140 \n150 \n160 \n170 \n\nConvNeXt \nTAdaConvNeXtV2 \n\nClass Index \n\nAccuracy \n\n0 \n\n20 \n\n40 \n\n60 \n\n80 \n\n0 \n10 \n20 \n30 \n40 \n50 \n60 \n70 \n80 \n90 \n100 \n110 \n120 \n130 \n140 \n150 \n160 \n170 \n\nTSN \nTAda2D \n\n\n\nTABLE A4 :\nA4Ablation studies.(c) Ablation studies on reduction ratio r for K1 = K2 = 3.(a) Ablation studies on kernel \nsize with linear calibration \nweight generation function. \n\nKernel size \nTop-1 \n1 \n37.5 \n3 \n56.5 \n5 \n57.3 \n7 \n56.5 \n\n(b) Ablation studies on kernel size \nwith non-linear calibration weight \ngeneration function. \n\nK2=1 K2=3 K2=5 K2=7 \nK1=1 36.8 57.1 57.8 57.9 \nK1=3 57.3 57.8 57.9 58.0 \nK1=5 57.6 57.9 58.2 57.9 \nK1=7 57.4 57.6 58.0 57.6 \n\nRatio r Top-1 \n1 \n57.79 \n2 \n57.83 \n4 \n57.78 \n8 \n57.66 \n\n\n\nTABLE A5 :\nA5Classification accuracy on Epic-Kitchens-100[85]. \u2191 indicates the main evaluation metric for the dataset. For fair comparison, we implement all the baseline models using our own training strategies.ModelFrames GFLOPs Params. Act.\u2191 Verb Noun Act.\u2191 Verb Noun temporal modelling. The ability to perform temporal modelling does not only mean the ability to generate weights according to the whole sequence in dynamic filters for videos, but it also requires the model to generate different weights for the same set of frames with different orders. For example, weights generated by the global descriptor obtained by global average pooling over the whole video GAP st does not have the temporal modelling ability, since they can not generate different weights if the order of the frames in the input sequence are reversed or randomized. Hence, most image based approaches based on global descriptor vectors (such as CondConv and DynamicFilter) or based on adjacent spatial contents (DDF) can not achieve temporal modelling. TAM generates convolution weights for temporal convolutions based on temporally local descriptors obtained by the global average pooling over the spatial dimension GAP s , which yields different weights if the sequence changes. Hence, in this sense, TAM has the temporal modelling abilities. In contrast, TAdaConv exploits both temporally local and global descriptors to utilize notTop-1 \nTop-5 \n\nSlowFast 4\u00d716 \n4+32 \n36.10 \n34.5M 38.17 63.54 48.79 58.68 89.75 73.37 \nSlowFast 4\u00d716 + TAdaConv 4+32 \n36.11 \n37.7M 39.14 64.50 49.59 59.21 89.67 73.88 \n\nSlowFast 8\u00d78 \n8+32 \n65.71 \n34.5M 40.08 65.05 50.72 60.10 90.04 74.26 \nSlowFast 8\u00d78 + TAdaConv \n8+32 \n65.73 \n37.7M 41.35 66.36 52.32 61.68 90.59 75.89 \n\nR(2+1)D \n8 \n49.55 \n28.1M 37.45 62.92 48.27 58.02 89.75 73.60 \nR(2+1)D + TAdaConv 2d \n8 \n49.57 \n31.3M 39.72 64.48 50.26 60.22 90.01 75.06 \nR(2+1)D + TAdaConv 2d+1d \n8 \n49.58 \n34.4M 40.10 64.77 50.28 60.45 89.99 75.55 \n\nR3D \n8 \n84.23 \n47.0M 36.67 61.92 47.87 57.47 89.02 73.05 \nR3D + TAdaConv 3d \n8 \n84.24 \n50.1M 39.30 64.03 49.94 59.67 89.84 74.56 \n\n\n\nTABLE A6 :\nA6Plug-in evaluation of TAdaConv on the action localization on HACS and Epic-Kitchens. \u2191 indicates the main evaluation metric for the dataset. 'S.F.' is SlowFast network.Model@0.5 @0.6 @0.7 @0.8 @0.9 Avg.\u2191 Task @0.1 @0.2 @0.3 @0.4 @0.5 Avg.\u2191 Verb 19.93 18.92 17.90 16.08 13.24 17.21 Noun 17.93 16.83 15.53 13.68 11.41 15.07 Act.\u2191 14.00 13.19 12.37 11.18 9.52 12.04 Verb 19.96 18.71 17.65 15.41 13.35 17.01 Noun 20.17 18.90 17.58 15.83 13.18 17.13 Act.\u2191 14.90 14.12 13.32 12.07 10.57 13.00HACS \nEpic-Kitchen-100 \n\nS.F. 8\u00d78 \n50.0 44.1 37.7 29.6 18.4 33.7 \n\nS.F. 8\u00d78 + TAdaConv 51.7 45.7 39.3 31.0 19.5 35.1 \n\nKinetics 400 \nSomething-Something-V2 \n\nTSN (training) \nTSN (validation) \nTAda2D (training) \nTAda2D (validation) \n\nTSN (training) \nTSN (validation) \nTAda2D (training) \nTAda2D (validation) \n\n0 \n10 \n20 \n30 \n40 \n50 \n60 \n\nEpochs \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\n90 \n\n0 \n20 \n40 \n60 \n80 \n100 \n\nEpochs \n\n20 \n\n30 \n\n40 \n\n50 \n\n60 \n\n70 \n\n80 \n\nKinetics 400 \nSomething-Something-V2 \n\nConvNeXt (training) \nConvNeXt (validation) \nTAdaConvNeXtV2 (training) \nTAdaConvNeXtV2 (validation) \n\nConvNeXt (training) \nConvNeXt (validation) \nTAdaConvNeXtV2 (training) \nTAdaConvNeXtV2 (validation) \n\n\n\nTABLE A7 :\nA7Approach comparison between different dynamic filters. The weights column denotes how weights in respective approaches are obtained. The pre-trained weights colmun shows whether the weight generation can exploit pre-trained models such as ResNet[1].CondConvMixture of experts W = n f (x)nWn \u2717 \u2717 \u2717 DynamicFilter Completely generated W = g(x)Temporal Location Pretrained \nOperations \nWeights \nModelling Adaptive weights \n\n\u2717 \n\u2717 \n\u2717 \nDDF \nCompletely generated W = g(x) \n\u2717 \n\u2713 \n\u2717 \nTAM \nCompletely generated W = g(x) \n\u2713 \n\u2717 \n\u2717 \nTAdaConv \nCalibrated from a base weight W = h(x)W b \n\u2713 \n\u2713 \n\u2713 \n\n\n\nTABLE A8 :\nA8Performance comparison with other dynamic filters. Our Init. denotes initializing the calibration weights to ones so that the initial calibrated weights is identical to the pre-trained weights. Temp. Varying is short for temporally varying, which indicates different weights for different temporal locations (frames). * denotes that the branch was originally not designed for generating filter or calibration weights, but we slightly modified the structure so that it can be used for calibration weight generation. (Numbers in brackets) show the performance improvement brought by our initialization scheme for calibration weights.Calibration Generation \nOur Init. Temp. Varying \nGeneration source \nTop-1 \n\nDynamicFilter \n\u2717 \n\u2717 \nGAPst(x)(C \u00d7 1) \n41.7 \nDDF-like \n\u2717 \n\u2713 \nGAPst(x)(C \u00d7 1) \n49.8 \nTAM (global branch) \n\u2717 \n\u2717 \nGAPs(x)(C \u00d7 T ) \n39.7 \nTAM (local*+global branch) \n\u2717 \n\u2713 \nGAPs(x)(C \u00d7 T ) \n41.3 \n\n\nACKNOWLEDGMENTSThis research is supported by the Agency for Science, Technology and Research (A*STAR) under its AME Programmatic Funding Scheme (Project #A18A2b0046), by the RIE2020 Industry Alignment Fund -Industry Collaboration Projects (IAF-ICP) Funding Initiative, as well as cash and in-kind contribution from the industry partner(s), and by Alibaba Group through Alibaba Research Intern Program.\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, CVPR. K. He, X. Zhang, S. Ren, and J. Sun, \"Deep residual learning for image recognition,\" in CVPR, 2016, pp. 770-778.\n\nGoing deeper with convolutions. C Szegedy, W Liu, Y Jia, P Sermanet, S Reed, D Anguelov, D Erhan, V Vanhoucke, A Rabinovich, CVPR. C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, D. Erhan, V. Vanhoucke, and A. Rabinovich, \"Going deeper with convolutions,\" in CVPR, 2015, pp. 1-9.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, NeurIPS. 25A. Krizhevsky, I. Sutskever, and G. E. Hinton, \"Imagenet clas- sification with deep convolutional neural networks,\" NeurIPS, vol. 25, pp. 1097-1105, 2012.\n\nCoatnet: Marrying convolution and attention for all data sizes. Z Dai, H Liu, Q V Le, M Tan, NeurIPS. 34Z. Dai, H. Liu, Q. V. Le, and M. Tan, \"Coatnet: Marrying con- volution and attention for all data sizes,\" NeurIPS, vol. 34, pp. 3965-3977, 2021.\n\nAggregated residual transformations for deep neural networks. S Xie, R Girshick, P Doll\u00e1r, Z Tu, K He, S. Xie, R. Girshick, P. Doll\u00e1r, Z. Tu, and K. He, \"Aggregated residual transformations for deep neural networks,\" in CVPR, 2017, pp. 1492-1500.\n\nDeformable convolutional networks. J Dai, H Qi, Y Xiong, Y Li, G Zhang, H Hu, Y Wei, J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei, \"Deformable convolutional networks,\" in ICCV, 2017, pp. 764- 773.\n\nNeural epitome search for architecture-agnostic network compression. D Zhou, X Jin, Q Hou, K Wang, J Yang, J Feng, arXiv:1907.05642arXiv preprintD. Zhou, X. Jin, Q. Hou, K. Wang, J. Yang, and J. Feng, \"Neural epitome search for architecture-agnostic network compression,\" arXiv preprint arXiv:1907.05642, 2019.\n\nResnest: Split-attention networks. H Zhang, C Wu, Z Zhang, Y Zhu, H Lin, Z Zhang, Y Sun, T He, J Mueller, R Manmatha, CVPRH. Zhang, C. Wu, Z. Zhang, Y. Zhu, H. Lin, Z. Zhang, Y. Sun, T. He, J. Mueller, R. Manmatha et al., \"Resnest: Split-attention networks,\" in CVPR, 2022, pp. 2736-2746.\n\nConditional convolutions for instance segmentation. Z Tian, C Shen, H Chen, ECCV. SpringerZ. Tian, C. Shen, and H. Chen, \"Conditional convolutions for instance segmentation,\" in ECCV. Springer, 2020.\n\nMobilenets: Efficient convolutional neural networks for mobile vision applications. A G Howard, M Zhu, B Chen, D Kalenichenko, W Wang, T Weyand, M Andreetto, H Adam, arXiv:1704.04861arXiv preprintA. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto, and H. Adam, \"Mobilenets: Efficient convolutional neural networks for mobile vision applications,\" arXiv preprint arXiv:1704.04861, 2017.\n\nCondconv: Conditionally parameterized convolutions for efficient inference. B Yang, G Bender, Q V Le, J Ngiam, arXiv:1904.04971arXiv preprintB. Yang, G. Bender, Q. V. Le, and J. Ngiam, \"Condconv: Con- ditionally parameterized convolutions for efficient inference,\" arXiv preprint arXiv:1904.04971, 2019.\n\nLearning spatiotemporal features with 3d convolutional networks. D Tran, L Bourdev, R Fergus, L Torresani, M Paluri, ICCV. D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, \"Learning spatiotemporal features with 3d convolutional net- works,\" in ICCV, 2015, pp. 4489-4497.\n\nA closer look at spatiotemporal convolutions for action recognition. D Tran, H Wang, L Torresani, J Ray, Y Lecun, M Paluri, in CVPR. D. Tran, H. Wang, L. Torresani, J. Ray, Y. LeCun, and M. Paluri, \"A closer look at spatiotemporal convolutions for action recogni- tion,\" in CVPR, 2018, pp. 6450-6459.\n\nLearning spatio-temporal representation with pseudo-3d residual networks. Z Qiu, T Yao, T Mei, Z. Qiu, T. Yao, and T. Mei, \"Learning spatio-temporal represen- tation with pseudo-3d residual networks,\" in ICCV, 2017, pp. 5533-5541.\n\nStatistics of natural images: Scaling in the woods. D L Ruderman, W Bialek, Physical review letters. 736814D. L. Ruderman and W. Bialek, \"Statistics of natural images: Scaling in the woods,\" Physical review letters, vol. 73, no. 6, p. 814, 1994.\n\nNatural image statistics and neural representation. E P Simoncelli, B A Olshausen, Annual review of neuroscience. 241E. P. Simoncelli and B. A. Olshausen, \"Natural image statistics and neural representation,\" Annual review of neuroscience, vol. 24, no. 1, pp. 1193-1216, 2001.\n\nDecoupled dynamic filter networks. J Zhou, V Jampani, Z Pi, Q Liu, M.-H Yang, CVPR. J. Zhou, V. Jampani, Z. Pi, Q. Liu, and M.-H. Yang, \"Decoupled dynamic filter networks,\" in CVPR, 2021, pp. 6647-6656.\n\nDynamic filtering with large sampling field for convnets. J Wu, D Li, Y Yang, C Bajaj, X Ji, in ECCV. J. Wu, D. Li, Y. Yang, C. Bajaj, and X. Ji, \"Dynamic filtering with large sampling field for convnets,\" in ECCV, 2018, pp. 185-200.\n\nSlowfast networks for video recognition. C Feichtenhofer, H Fan, J Malik, K He, ICCV. C. Feichtenhofer, H. Fan, J. Malik, and K. He, \"Slowfast networks for video recognition,\" in ICCV, 2019, pp. 6202-6211.\n\nX3d: Expanding architectures for efficient video recognition. C Feichtenhofer, CVPR. C. Feichtenhofer, \"X3d: Expanding architectures for efficient video recognition,\" in CVPR, 2020, pp. 203-213.\n\nDynamic filter networks. X Jia, B De Brabandere, T Tuytelaars, L V Gool, NeurIPS. 29X. Jia, B. De Brabandere, T. Tuytelaars, and L. V. Gool, \"Dynamic filter networks,\" NeurIPS, vol. 29, pp. 667-675, 2016.\n\nDynamic convolution: Attention over convolution kernels. Y Chen, X Dai, M Liu, D Chen, L Yuan, Z Liu, CVPR. 11Y. Chen, X. Dai, M. Liu, D. Chen, L. Yuan, and Z. Liu, \"Dynamic convolution: Attention over convolution kernels,\" in CVPR, 2020, pp. 11 030-11 039.\n\nTAda! temporally-adaptive convolutions for video understanding. Z Huang, S Zhang, L Pan, Z Qing, M Tang, Z Liu, M H AngJr, ICLR. Z. Huang, S. Zhang, L. Pan, Z. Qing, M. Tang, Z. Liu, and M. H. Ang Jr, \"TAda! temporally-adaptive convolutions for video understanding,\" in ICLR, 2022.\n\nAttention is all you need. A Vaswani, N Shazeer, N Parmar, J Uszkoreit, L Jones, A N Gomez, \u0141 Kaiser, I Polosukhin, NeurIPS. 30A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \"Attention is all you need,\" NeurIPS, vol. 30, 2017.\n\nQuo vadis, action recognition? a new model and the kinetics dataset. J Carreira, A Zisserman, J. Carreira and A. Zisserman, \"Quo vadis, action recognition? a new model and the kinetics dataset,\" in CVPR, 2017, pp. 6299- 6308.\n\nVideo classification with channel-separated convolutional networks. D Tran, H Wang, L Torresani, M Feiszli, ICCV. D. Tran, H. Wang, L. Torresani, and M. Feiszli, \"Video classifica- tion with channel-separated convolutional networks,\" in ICCV, 2019, pp. 5552-5561.\n\nTwo-stream convolutional networks for action recognition in videos. K Simonyan, A Zisserman, arXiv:1406.2199arXiv preprintK. Simonyan and A. Zisserman, \"Two-stream convolutional networks for action recognition in videos,\" arXiv preprint arXiv:1406.2199, 2014.\n\nTsm: Temporal shift module for efficient video understanding. J Lin, C Gan, S Han, ICCV. J. Lin, C. Gan, and S. Han, \"Tsm: Temporal shift module for efficient video understanding,\" in ICCV, 2019, pp. 7083-7093.\n\nTdn: Temporal difference networks for efficient action recognition. L Wang, Z Tong, B Ji, G Wu, CVPR. L. Wang, Z. Tong, B. Ji, and G. Wu, \"Tdn: Temporal difference networks for efficient action recognition,\" in CVPR, 2021, pp. 1895-1904.\n\nStm: Spatiotemporal and motion encoding for action recognition. B Jiang, M Wang, W Gan, W Wu, J Yan, ICCV. B. Jiang, M. Wang, W. Gan, W. Wu, and J. Yan, \"Stm: Spatiotem- poral and motion encoding for action recognition,\" in ICCV, 2019, pp. 2000-2009.\n\nTam: Temporal adaptive module for video recognition. Z Liu, L Wang, W Wu, C Qian, T Lu, 2021Z. Liu, L. Wang, W. Wu, C. Qian, and T. Lu, \"Tam: Temporal adaptive module for video recognition,\" ICCV, 2021.\n\nVideo modeling with correlation networks. H Wang, D Tran, L Torresani, M Feiszli, CVPR. H. Wang, D. Tran, L. Torresani, and M. Feiszli, \"Video modeling with correlation networks,\" in CVPR, 2020, pp. 352-361.\n\nMaximizing spatio-temporal entropy of deep 3d cnns for efficient video recognition. J Wang, Z Sun, Y Qian, D Gong, X Sun, M Lin, M Pagnucco, Y Song, ICLR. J. Wang, Z. Sun, Y. Qian, D. Gong, X. Sun, M. Lin, M. Pagnucco, and Y. Song, \"Maximizing spatio-temporal entropy of deep 3d cnns for efficient video recognition,\" in ICLR, 2023.\n\nSmallbignet: Integrating core and contextual views for video classification. X Li, Y Wang, Z Zhou, Y Qiao, CVPR. X. Li, Y. Wang, Z. Zhou, and Y. Qiao, \"Smallbignet: Integrating core and contextual views for video classification,\" in CVPR, 2020, pp. 1092-1101.\n\nGcm: Efficient video recognition with glance and combine module. Y Zhou, Z Huang, X Yang, M Ang, T K Ng, Pattern Recognition. 133108970Y. Zhou, Z. Huang, X. Yang, M. Ang, and T. K. Ng, \"Gcm: Efficient video recognition with glance and combine module,\" Pattern Recognition, vol. 133, p. 108970, 2023.\n\nZ Liu, H Mao, C.-Y Wu, C Feichtenhofer, T Darrell, S Xie, arXiv:2201.03545A convnet for the 2020s. arXiv preprintZ. Liu, H. Mao, C.-Y. Wu, C. Feichtenhofer, T. Darrell, and S. Xie, \"A convnet for the 2020s,\" arXiv preprint arXiv:2201.03545, 2022.\n\nBert: Pre-training of deep bidirectional transformers for language understanding. J D , M.-W C Kenton, L K Toutanova, Proceedings of NAACL-HLT. NAACL-HLTJ. D. M.-W. C. Kenton and L. K. Toutanova, \"Bert: Pre-training of deep bidirectional transformers for language understanding,\" in Proceedings of NAACL-HLT, 2019, pp. 4171-4186.\n\nImproving language understanding by generative pre-training. A Radford, K Narasimhan, T Salimans, I Sutskever, A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al., \"Improving language understanding by generative pre-training,\" 2018.\n\nAn image is worth 16x16 words: Transformers for image recognition at scale. A Dosovitskiy, L Beyer, A Kolesnikov, D Weissenborn, X Zhai, T Unterthiner, M Dehghani, M Minderer, G Heigold, S Gelly, arXiv:2010.11929arXiv preprintA. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., \"An image is worth 16x16 words: Transformers for image recognition at scale,\" arXiv preprint arXiv:2010.11929, 2020.\n\nMultiscale vision transformers. H Fan, B Xiong, K Mangalam, Y Li, Z Yan, J Malik, C Feichtenhofer, ICCV. H. Fan, B. Xiong, K. Mangalam, Y. Li, Z. Yan, J. Malik, and C. Feichtenhofer, \"Multiscale vision transformers,\" in ICCV, 2021, pp. 6824-6835.\n\nTrackformer: Multi-object tracking with transformers. T Meinhardt, A Kirillov, L Leal-Taixe, C Feichtenhofer, CVPR. T. Meinhardt, A. Kirillov, L. Leal-Taixe, and C. Feichtenhofer, \"Trackformer: Multi-object tracking with transformers,\" in CVPR, 2022, pp. 8844-8854.\n\nTctrack: Temporal contexts for aerial tracking. Z Cao, Z Huang, L Pan, S Zhang, Z Liu, C Fu, CVPR. Z. Cao, Z. Huang, L. Pan, S. Zhang, Z. Liu, and C. Fu, \"Tctrack: Temporal contexts for aerial tracking,\" in CVPR, 2022, pp. 14 798- 14 808.\n\nBeit: Bert pre-training of image transformers. H Bao, L Dong, S Piao, F Wei, arXiv:2106.08254arXiv preprintH. Bao, L. Dong, S. Piao, and F. Wei, \"Beit: Bert pre-training of image transformers,\" arXiv preprint arXiv:2106.08254, 2021.\n\nPttr: Relational 3d point cloud object tracking with transformer. C Zhou, Z Luo, Y Luo, T Liu, L Pan, Z Cai, H Zhao, S Lu, CVPR. C. Zhou, Z. Luo, Y. Luo, T. Liu, L. Pan, Z. Cai, H. Zhao, and S. Lu, \"Pttr: Relational 3d point cloud object tracking with transformer,\" in CVPR, 2022, pp. 8531-8540.\n\nHierarchical text-conditional image generation with clip latents. A Ramesh, P Dhariwal, A Nichol, C Chu, M Chen, arXiv:2204.06125arXiv preprintA. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, \"Hi- erarchical text-conditional image generation with clip latents,\" arXiv preprint arXiv:2204.06125, 2022.\n\nEnd-to-end object detection with transformers. N Carion, F Massa, G Synnaeve, N Usunier, A Kirillov, S Zagoruyko, ECCV. SpringerN. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko, \"End-to-end object detection with transformers,\" in ECCV. Springer, 2020, pp. 213-229.\n\nNon-local neural networks. X Wang, R Girshick, A Gupta, K He, CVPR. X. Wang, R. Girshick, A. Gupta, and K. He, \"Non-local neural networks,\" in CVPR, 2018, pp. 7794-7803.\n\nVideo swin transformer. Z Liu, J Ning, Y Cao, Y Wei, Z Zhang, S Lin, H Hu, arXiv:2106.13230arXiv preprintZ. Liu, J. Ning, Y. Cao, Y. Wei, Z. Zhang, S. Lin, and H. Hu, \"Video swin transformer,\" arXiv preprint arXiv:2106.13230, 2021.\n\nVivit: A video vision transformer. A Arnab, M Dehghani, G Heigold, C Sun, M Lu\u010di\u0107, C Schmid, ICCV. A. Arnab, M. Dehghani, G. Heigold, C. Sun, M. Lu\u010di\u0107, and C. Schmid, \"Vivit: A video vision transformer,\" in ICCV, 2021, pp. 6836-6846.\n\nIs space-time attention all you need for video understanding. G Bertasius, H Wang, L Torresani, arXiv:2102.0509524arXiv preprintG. Bertasius, H. Wang, and L. Torresani, \"Is space-time at- tention all you need for video understanding,\" arXiv preprint arXiv:2102.05095, vol. 2, no. 3, p. 4, 2021.\n\nEnd-to-end temporal action detection with transformer. X Liu, Q Wang, Y Hu, X Tang, S Zhang, S Bai, X Bai, IEEE TIP. 31X. Liu, Q. Wang, Y. Hu, X. Tang, S. Zhang, S. Bai, and X. Bai, \"End-to-end temporal action detection with transformer,\" IEEE TIP, vol. 31, pp. 5427-5441, 2022.\n\nVideollm: Modeling video sequence with large language models. G Chen, Y.-D Zheng, J Wang, J Xu, Y Huang, J Pan, Y Wang, Y Wang, Y Qiao, T Lu, arXiv:2305.13292arXiv preprintG. Chen, Y.-D. Zheng, J. Wang, J. Xu, Y. Huang, J. Pan, Y. Wang, Y. Wang, Y. Qiao, T. Lu et al., \"Videollm: Modeling video sequence with large language models,\" arXiv preprint arXiv:2305.13292, 2023.\n\nKeeping your eye on the ball: Trajectory attention in video transformers. M Patrick, D Campbell, Y Asano, I Misra, F Metze, C Feichtenhofer, A Vedaldi, J F Henriques, Advances in neural information processing systems. 34M. Patrick, D. Campbell, Y. Asano, I. Misra, F. Metze, C. Feicht- enhofer, A. Vedaldi, and J. F. Henriques, \"Keeping your eye on the ball: Trajectory attention in video transformers,\" Advances in neural information processing systems, vol. 34, pp. 12 493-12 506, 2021.\n\nAn empirical study of training selfsupervised vision transformers. X Chen, S Xie, K He, ICCV. X. Chen, S. Xie, and K. He, \"An empirical study of training self- supervised vision transformers,\" in ICCV, 2021, pp. 9640-9649.\n\nMasked autoencoders are scalable vision learners. K He, X Chen, S Xie, Y Li, P Doll\u00e1r, R Girshick, CVPR. 16K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00e1r, and R. Girshick, \"Masked autoencoders are scalable vision learners,\" in CVPR, 2022, pp. 16 000-16 009.\n\nVideomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. Z Tong, Y Song, J Wang, L Wang, arXiv:2203.12602arXiv preprintZ. Tong, Y. Song, J. Wang, and L. Wang, \"Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training,\" arXiv preprint arXiv:2203.12602, 2022.\n\nMasked autoencoders as spatiotemporal learners. C Feichtenhofer, Y Li, K He, NeurIPS. 35C. Feichtenhofer, Y. Li, K. He et al., \"Masked autoencoders as spatiotemporal learners,\" NeurIPS, vol. 35, pp. 35 946-35 958, 2022.\n\nSelf-supervised learning from untrimmed videos via hierarchical consistency. Z Qing, S Zhang, Z Huang, Y Xu, X Wang, C Gao, R Jin, N Sang, PAMIZ. Qing, S. Zhang, Z. Huang, Y. Xu, X. Wang, C. Gao, R. Jin, and N. Sang, \"Self-supervised learning from untrimmed videos via hierarchical consistency,\" PAMI, 2023.\n\nLearning transferable visual models from natural language supervision. A Radford, J W Kim, C Hallacy, A Ramesh, G Goh, S Agarwal, G Sastry, A Askell, P Mishkin, J Clark, in ICML. PMLR, 2021A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agar- wal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., \"Learning transferable visual models from natural language supervision,\" in ICML. PMLR, 2021, pp. 8748-8763.\n\nCoca: Contrastive captioners are image-text foundation models. J Yu, Z Wang, V Vasudevan, L Yeung, M Seyedhosseini, Y Wu, arXiv:2205.01917arXiv preprintJ. Yu, Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini, and Y. Wu, \"Coca: Contrastive captioners are image-text foundation models,\" arXiv preprint arXiv:2205.01917, 2022.\n\nScaling vision transformers. X Zhai, A Kolesnikov, N Houlsby, L Beyer, CVPR. X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer, \"Scaling vision transformers,\" in CVPR, 2022, pp. 12 104-12 113.\n\nScaling vision transformers to 22 billion parameters. M Dehghani, J Djolonga, B Mustafa, P Padlewski, J Heek, J Gilmer, A Steiner, M Caron, R Geirhos, I Alabdulmohsin, arXiv:2302.05442arXiv preprintM. Dehghani, J. Djolonga, B. Mustafa, P. Padlewski, J. Heek, J. Gilmer, A. Steiner, M. Caron, R. Geirhos, I. Alabdulmohsin et al., \"Scaling vision transformers to 22 billion parameters,\" arXiv preprint arXiv:2302.05442, 2023.\n\nImage as a foreign language: Beit pretraining for all vision and vision-language tasks. W Wang, H Bao, L Dong, J Bjorck, Z Peng, Q Liu, K Aggarwal, O K Mohammed, S Singhal, S Som, arXiv:2208.10442arXiv preprintW. Wang, H. Bao, L. Dong, J. Bjorck, Z. Peng, Q. Liu, K. Aggarwal, O. K. Mohammed, S. Singhal, S. Som et al., \"Image as a foreign language: Beit pretraining for all vision and vision-language tasks,\" arXiv preprint arXiv:2208.10442, 2022.\n\nVideomae v2: Scaling video masked autoencoders with dual masking. L Wang, B Huang, Z Zhao, Z Tong, Y He, Y Wang, Y Wang, Y Qiao, arXiv:2303.16727arXiv preprintL. Wang, B. Huang, Z. Zhao, Z. Tong, Y. He, Y. Wang, Y. Wang, and Y. Qiao, \"Videomae v2: Scaling video masked autoencoders with dual masking,\" arXiv preprint arXiv:2303.16727, 2023.\n\nSt-adapter: Parameterefficient image-to-video transfer learning for action recognition. J Pan, Z Lin, X Zhu, J Shao, H Li, arXiv:2206.13559arXiv preprintJ. Pan, Z. Lin, X. Zhu, J. Shao, and H. Li, \"St-adapter: Parameter- efficient image-to-video transfer learning for action recognition,\" arXiv preprint arXiv:2206.13559, 2022.\n\nRevisiting dynamic convolution via matrix decomposition. Y Li, Y Chen, X Dai, D Chen, Y Yu, L Yuan, Z Liu, M Chen, N Vasconcelos, ICLR. Y. Li, Y. Chen, X. Dai, D. Chen, Y. Yu, L. Yuan, Z. Liu, M. Chen, N. Vasconcelos et al., \"Revisiting dynamic convolution via matrix decomposition,\" in ICLR, 2021.\n\nY Li, Y Chen, X Dai, D Chen, M Liu, L Yuan, Z Liu, L Zhang, N Vasconcelos, arXiv:2011.12289Micronet: Towards image recognition with extremely low flops. arXiv preprintY. Li, Y. Chen, X. Dai, D. Chen, M. Liu, L. Yuan, Z. Liu, L. Zhang, and N. Vasconcelos, \"Micronet: Towards image recognition with extremely low flops,\" arXiv preprint arXiv:2011.12289, 2020.\n\nDynamic relu. Y Chen, X Dai, M Liu, D Chen, L Yuan, Z Liu, ECCV. SpringerY. Chen, X. Dai, M. Liu, D. Chen, L. Yuan, and Z. Liu, \"Dynamic relu,\" in ECCV. Springer, 2020, pp. 351-367.\n\nSkipnet: Learning dynamic routing in convolutional networks. X Wang, F Yu, Z.-Y Dou, T Darrell, J E Gonzalez, in ECCV. X. Wang, F. Yu, Z.-Y. Dou, T. Darrell, and J. E. Gonzalez, \"Skipnet: Learning dynamic routing in convolutional networks,\" in ECCV, 2018, pp. 409-424.\n\nLearning dynamic routing for semantic segmentation. Y Li, L Song, Y Chen, Z Li, X Zhang, X Wang, J Sun, CVPR. Y. Li, L. Song, Y. Chen, Z. Li, X. Zhang, X. Wang, and J. Sun, \"Learning dynamic routing for semantic segmentation,\" in CVPR, 2020, pp. 8553-8562.\n\nAudio-driven talking face video generation with dynamic convolution kernels. Z Ye, M Xia, R Yi, J Zhang, Y.-K Lai, X Huang, G Zhang, Y.-J Liu, IEEE Transactions on Multimedia. Z. Ye, M. Xia, R. Yi, J. Zhang, Y.-K. Lai, X. Huang, G. Zhang, and Y.-j. Liu, \"Audio-driven talking face video generation with dynamic convolution kernels,\" IEEE Transactions on Multimedia, 2022.\n\nConvbert: Improving bert with span-based dynamic convolution. Z.-H Jiang, W Yu, D Zhou, Y Chen, J Feng, S Yan, NeurIPS. 33Z.-H. Jiang, W. Yu, D. Zhou, Y. Chen, J. Feng, and S. Yan, \"Con- vbert: Improving bert with span-based dynamic convolution,\" NeurIPS, vol. 33, pp. 12 837-12 848, 2020.\n\nUnified dynamic convolutional network for super-resolution with variational degradations. Y.-S Xu, S.-Y R Tseng, Y Tseng, H.-K Kuo, Y.-M Tsai, CVPR. 505Y.-S. Xu, S.-Y. R. Tseng, Y. Tseng, H.-K. Kuo, and Y.-M. Tsai, \"Unified dynamic convolutional network for super-resolution with variational degradations,\" in CVPR, 2020, pp. 12 496-12 505.\n\nPay less attention with lightweight and dynamic convolutions. F Wu, A Fan, A Baevski, Y N Dauphin, M Auli, arXiv:1901.10430arXiv preprintF. Wu, A. Fan, A. Baevski, Y. N. Dauphin, and M. Auli, \"Pay less attention with lightweight and dynamic convolutions,\" arXiv preprint arXiv:1901.10430, 2019.\n\nAdafuse: Adaptive temporal fusion network for efficient action recognition. Y Meng, R Panda, C.-C Lin, P Sattigeri, L Karlinsky, K Saenko, A Oliva, R Feris, ICLR. Y. Meng, R. Panda, C.-C. Lin, P. Sattigeri, L. Karlinsky, K. Saenko, A. Oliva, and R. Feris, \"Adafuse: Adaptive temporal fusion network for efficient action recognition,\" in ICLR, 2021.\n\nAdaframe: Adaptive frame selection for fast video recognition. Z Wu, C Xiong, C.-Y Ma, R Socher, L S Davis, CVPR. Z. Wu, C. Xiong, C.-Y. Ma, R. Socher, and L. S. Davis, \"Adaframe: Adaptive frame selection for fast video recognition,\" in CVPR, 2019, pp. 1278-1287.\n\nAr-net: Adaptive frame resolution for efficient action recognition. Y Meng, C.-C Lin, R Panda, P Sattigeri, L Karlinsky, A Oliva, K Saenko, R Feris, ECCV. SpringerY. Meng, C.-C. Lin, R. Panda, P. Sattigeri, L. Karlinsky, A. Oliva, K. Saenko, and R. Feris, \"Ar-net: Adaptive frame resolution for efficient action recognition,\" in ECCV. Springer, 2020, pp. 86- 104.\n\nRevisiting spatial invariance with low-rank local connectivity. G Elsayed, P Ramachandran, J Shlens, S Kornblith, ICML. PMLR, 2020. G. Elsayed, P. Ramachandran, J. Shlens, and S. Kornblith, \"Re- visiting spatial invariance with low-rank local connectivity,\" in ICML. PMLR, 2020, pp. 2868-2879.\n\nDynamic regionaware convolution. J Chen, X Wang, Z Guo, X Zhang, J Sun, CVPR. J. Chen, X. Wang, Z. Guo, X. Zhang, and J. Sun, \"Dynamic region- aware convolution,\" in CVPR, 2021, pp. 8064-8073.\n\nRectified linear units improve restricted boltzmann machines. V Nair, G E Hinton, IcmlV. Nair and G. E. Hinton, \"Rectified linear units improve re- stricted boltzmann machines,\" in Icml, 2010.\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, PMLRICML. S. Ioffe and C. Szegedy, \"Batch normalization: Accelerating deep network training by reducing internal covariate shift,\" in ICML. PMLR, 2015, pp. 448-456.\n\nLayer normalization. J L Ba, J R Kiros, G E Hinton, arXiv:1607.06450arXiv preprintJ. L. Ba, J. R. Kiros, and G. E. Hinton, \"Layer normalization,\" arXiv preprint arXiv:1607.06450, 2016.\n\nW Kay, J Carreira, K Simonyan, B Zhang, C Hillier, S Vijayanarasimhan, F Viola, T Green, T Back, P Natsev, arXiv:1705.06950The kinetics human action video dataset. arXiv preprintW. Kay, J. Carreira, K. Simonyan, B. Zhang, C. Hillier, S. Vijaya- narasimhan, F. Viola, T. Green, T. Back, P. Natsev et al., \"The kinet- ics human action video dataset,\" arXiv preprint arXiv:1705.06950, 2017.\n\nThe\" something something\" video database for learning and evaluating visual common sense. R Goyal, S E Kahou, V Michalski, J Materzynska, S Westphal, H Kim, V Haenel, I Fruend, P Yianilos, M Mueller-Freitag, CVPR. 15R. Goyal, S. E. Kahou, V. Michalski, J. Materzynska, S. Westphal, H. Kim, V. Haenel, I. Fruend, P. Yianilos, M. Mueller-Freitag et al., \"The\" something something\" video database for learning and evaluating visual common sense.\" in CVPR, vol. 1, 2017, p. 5.\n\nRescaling egocentric vision. D Damen, H Doughty, G M Farinella, A Furnari, E Kazakos, J Ma, D Moltisanti, J Munro, T Perrett, W Price, arXiv:2006.13256arXiv preprintD. Damen, H. Doughty, G. M. Farinella, A. Furnari, E. Kazakos, J. Ma, D. Moltisanti, J. Munro, T. Perrett, W. Price et al., \"Rescaling egocentric vision,\" arXiv preprint arXiv:2006.13256, 2020.\n\nHacs: Human action clips and segments dataset for recognition and temporal localization. H Zhao, A Torralba, L Torresani, Z Yan, ICCV. H. Zhao, A. Torralba, L. Torresani, and Z. Yan, \"Hacs: Human action clips and segments dataset for recognition and temporal localization,\" in ICCV, 2019, pp. 8668-8678.\n\nUcf101: A dataset of 101 human actions classes from videos in the wild. K Soomro, A R Zamir, M Shah, arXiv:1212.0402arXiv preprintK. Soomro, A. R. Zamir, and M. Shah, \"Ucf101: A dataset of 101 human actions classes from videos in the wild,\" arXiv preprint arXiv:1212.0402, 2012.\n\nHmdb: a large video database for human motion recognition. H Kuehne, H Jhuang, E Garrote, T Poggio, T Serre, ICCV. IEEEH. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre, \"Hmdb: a large video database for human motion recognition,\" in ICCV. IEEE, 2011, pp. 2556-2563.\n\nA short note about kinetics-600. J Carreira, E Noland, A Banki-Horvath, C Hillier, A Zisserman, arXiv:1808.01340arXiv preprintJ. Carreira, E. Noland, A. Banki-Horvath, C. Hillier, and A. Zisserman, \"A short note about kinetics-600,\" arXiv preprint arXiv:1808.01340, 2018.\n\nA short note on the kinetics-700 human action dataset. J Carreira, E Noland, C Hillier, A Zisserman, arXiv:1907.06987arXiv preprintJ. Carreira, E. Noland, C. Hillier, and A. Zisserman, \"A short note on the kinetics-700 human action dataset,\" arXiv preprint arXiv:1907.06987, 2019.\n\nUniformerv2: Spatiotemporal learning by arming image vits with video uniformer. K Li, Y Wang, Y He, Y Li, Y Wang, L Wang, Y Qiao, arXiv:2211.09552arXiv preprintK. Li, Y. Wang, Y. He, Y. Li, Y. Wang, L. Wang, and Y. Qiao, \"Uniformerv2: Spatiotemporal learning by arming image vits with video uniformer,\" arXiv preprint arXiv:2211.09552, 2022.\n\nDecoupled weight decay regularization. I Loshchilov, F Hutter, arXiv:1711.05101arXiv preprintI. Loshchilov and F. Hutter, \"Decoupled weight decay regulariza- tion,\" arXiv preprint arXiv:1711.05101, 2017.\n\nRandaugment: Practical automated data augmentation with a reduced search space. E D Cubuk, B Zoph, J Shlens, Q V Le, CVPR Workshops, 2020. E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le, \"Randaugment: Practical automated data augmentation with a reduced search space,\" in CVPR Workshops, 2020, pp. 702-703.\n\nDeep networks with stochastic depth. G Huang, Y Sun, Z Liu, D Sedra, K Q Weinberger, ECCV. SpringerG. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, \"Deep networks with stochastic depth,\" in ECCV. Springer, 2016, pp. 646-661.\n\nRethinking the inception architecture for computer vision. C Szegedy, V Vanhoucke, S Ioffe, J Shlens, Z Wojna, CVPR. C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna, \"Rethinking the inception architecture for computer vision,\" in CVPR, 2016, pp. 2818-2826.\n\nH Zhang, M Cisse, Y N Dauphin, D Lopez-Paz, arXiv:1710.09412mixup: Beyond empirical risk minimization. arXiv preprintH. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, \"mixup: Beyond empirical risk minimization,\" arXiv preprint arXiv:1710.09412, 2017.\n\nCutmix: Regularization strategy to train strong classifiers with localizable features. S Yun, D Han, S J Oh, S Chun, J Choe, Y Yoo, ICCV. S. Yun, D. Han, S. J. Oh, S. Chun, J. Choe, and Y. Yoo, \"Cutmix: Regularization strategy to train strong classifiers with localizable features,\" in ICCV, 2019, pp. 6023-6032.\n\nAcceleration of stochastic approximation by averaging. B T Polyak, A B Juditsky, SIAM journal on control and optimization. 304B. T. Polyak and A. B. Juditsky, \"Acceleration of stochastic approximation by averaging,\" SIAM journal on control and opti- mization, vol. 30, no. 4, pp. 838-855, 1992.\n\nTemporal segment networks: Towards good practices for deep action recognition. L Wang, Y Xiong, Z Wang, Y Qiao, D Lin, X Tang, L Van Gool, ECCV. SpringerL. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and L. Van Gool, \"Temporal segment networks: Towards good prac- tices for deep action recognition,\" in ECCV. Springer, 2016, pp. 20-36.\n\nCan spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?. K Hara, H Kataoka, Y Satoh, in CVPR. K. Hara, H. Kataoka, and Y. Satoh, \"Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet?\" in CVPR, 2018, pp. 6546-6555.\n\nMvitv2: Improved multiscale vision transformers for classification and detection. Y Li, C.-Y Wu, H Fan, K Mangalam, B Xiong, J Malik, C Feichtenhofer, CVPR. Y. Li, C.-Y. Wu, H. Fan, K. Mangalam, B. Xiong, J. Malik, and C. Feichtenhofer, \"Mvitv2: Improved multiscale vision trans- formers for classification and detection,\" in CVPR, 2022, pp. 4804-4814.\n\nMovinets: Mobile video networks for efficient video recognition. D Kondratyuk, L Yuan, Y Li, L Zhang, M Tan, M Brown, B Gong, CVPR. 1630D. Kondratyuk, L. Yuan, Y. Li, L. Zhang, M. Tan, M. Brown, and B. Gong, \"Movinets: Mobile video networks for efficient video recognition,\" in CVPR, 2021, pp. 16 020-16 030.\n\nSpace-time mixing attention for video transformer. A Bulat, J M Perez Rua, S Sudhakaran, B Martinez, G Tzimiropoulos, NeurIPS. 34A. Bulat, J. M. Perez Rua, S. Sudhakaran, B. Martinez, and G. Tzimiropoulos, \"Space-time mixing attention for video trans- former,\" NeurIPS, vol. 34, pp. 19 594-19 607, 2021.\n\nMultiview transformers for video recognition. S Yan, X Xiong, A Arnab, Z Lu, M Zhang, C Sun, C Schmid, CVPR. S. Yan, X. Xiong, A. Arnab, Z. Lu, M. Zhang, C. Sun, and C. Schmid, \"Multiview transformers for video recognition,\" in CVPR, 2022, pp. 3333-3343.\n\nMar: Masked autoencoders for efficient action recognition. Z Qing, S Zhang, Z Huang, X Wang, Y Wang, Y Lv, C Gao, N Sang, IEEE Transactions on Multimedia. Z. Qing, S. Zhang, Z. Huang, X. Wang, Y. Wang, Y. Lv, C. Gao, and N. Sang, \"Mar: Masked autoencoders for efficient action recognition,\" IEEE Transactions on Multimedia, 2023.\n\nMasked feature prediction for self-supervised visual pretraining. C Wei, H Fan, S Xie, C.-Y Wu, A Yuille, C Feichtenhofer, CVPR. 14C. Wei, H. Fan, S. Xie, C.-Y. Wu, A. Yuille, and C. Feichten- hofer, \"Masked feature prediction for self-supervised visual pre- training,\" in CVPR, 2022, pp. 14 668-14 678.\n\nCo-training transformer with videos and images improves action recognition. B Zhang, J Yu, C Fifty, W Han, A M Dai, R Pang, F Sha, arXiv:2112.07175arXiv preprintB. Zhang, J. Yu, C. Fifty, W. Han, A. M. Dai, R. Pang, and F. Sha, \"Co-training transformer with videos and images improves ac- tion recognition,\" arXiv preprint arXiv:2112.07175, 2021.\n\nFrozen clip models are efficient video learners. Z Lin, S Geng, R Zhang, P Gao, G De Melo, X Wang, J Dai, Y Qiao, H Li, ECCV. SpringerZ. Lin, S. Geng, R. Zhang, P. Gao, G. de Melo, X. Wang, J. Dai, Y. Qiao, and H. Li, \"Frozen clip models are efficient video learners,\" in ECCV. Springer, 2022, pp. 388-404.\n\nExpanding language-image pretrained models for general video recognition. B Ni, H Peng, M Chen, S Zhang, G Meng, J Fu, S Xiang, H Ling, ECCV. SpringerB. Ni, H. Peng, M. Chen, S. Zhang, G. Meng, J. Fu, S. Xiang, and H. Ling, \"Expanding language-image pretrained models for general video recognition,\" in ECCV. Springer, 2022, pp. 1-18.\n\nFine-tuned clip models are efficient video learners. H Rasheed, M U Khattak, M Maaz, S Khan, F S Khan, CVPR. H. Rasheed, M. U. Khattak, M. Maaz, S. Khan, and F. S. Khan, \"Fine-tuned clip models are efficient video learners,\" in CVPR, 2023, pp. 6545-6554.\n\nTea: Temporal excitation and aggregation for action recognition. Y Li, B Ji, X Shi, J Zhang, B Kang, L Wang, CVPR. Y. Li, B. Ji, X. Shi, J. Zhang, B. Kang, and L. Wang, \"Tea: Temporal excitation and aggregation for action recognition,\" in CVPR, 2020, pp. 909-918.\n\nTemporal relational reasoning in videos. B Zhou, A Andonian, A Oliva, A Torralba, in ECCV. B. Zhou, A. Andonian, A. Oliva, and A. Torralba, \"Temporal relational reasoning in videos,\" in ECCV, 2018, pp. 803-818.\n\nTowards training stronger video vision transformers for epic-kitchens-100 action recognition. Z Huang, Z Qing, X Wang, Y Feng, S Zhang, J Jiang, Z Xia, M Tang, N Sang, M H AngJr, arXiv:2106.05058arXiv preprintZ. Huang, Z. Qing, X. Wang, Y. Feng, S. Zhang, J. Jiang, Z. Xia, M. Tang, N. Sang, and M. H. Ang Jr, \"Towards training stronger video vision transformers for epic-kitchens-100 action recogni- tion,\" arXiv preprint arXiv:2106.05058, 2021.\n\nMemvit: Memory-augmented multiscale vision transformer for efficient long-term video recognition. C.-Y Wu, Y Li, K Mangalam, H Fan, B Xiong, J Malik, C Feichtenhofer, CVPR. C.-Y. Wu, Y. Li, K. Mangalam, H. Fan, B. Xiong, J. Malik, and C. Feichtenhofer, \"Memvit: Memory-augmented multiscale vi- sion transformer for efficient long-term video recognition,\" in CVPR, 2022, pp. 13 587-13 597.\n\nMulti-task zero-shot action recognition with prioritised data augmentation. X Xu, T M Hospedales, S Gong, ECCV. SpringerX. Xu, T. M. Hospedales, and S. Gong, \"Multi-task zero-shot action recognition with prioritised data augmentation,\" in ECCV. Springer, 2016, pp. 343-359.\n\nAlternative semantic representations for zero-shot human action recognition. Q Wang, K Chen, Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2017. Skopje, MacedoniaSpringerProceedings, Part I 10Q. Wang and K. Chen, \"Alternative semantic representations for zero-shot human action recognition,\" in Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2017, Skopje, Macedonia, September 18-22, 2017, Proceedings, Part I 10. Springer, 2017, pp. 87-102.\n\nElaborative rehearsal for zero-shot action recognition. S Chen, D Huang, ICCV. S. Chen and D. Huang, \"Elaborative rehearsal for zero-shot action recognition,\" in ICCV, 2021, pp. 13 638-13 647.\n\nActionclip: A new paradigm for video action recognition. M Wang, J Xing, Y Liu, arXiv:2109.08472arXiv preprintM. Wang, J. Xing, and Y. Liu, \"Actionclip: A new paradigm for video action recognition,\" arXiv preprint arXiv:2109.08472, 2021.\n\nPrompting visuallanguage models for efficient video understanding. C Ju, T Han, K Zheng, Y Zhang, W Xie, ECCV. SpringerC. Ju, T. Han, K. Zheng, Y. Zhang, and W. Xie, \"Prompting visual- language models for efficient video understanding,\" in ECCV. Springer, 2022, pp. 105-124.\n\nCuhk & ethz & siat submission to activitynet challenge 2017. Y Zhao, B Zhang, Z Wu, S Yang, L Zhou, S Yan, L Wang, Y Xiong, D Lin, Y Qiao, arXiv:1710.080118arXiv preprintY. Zhao, B. Zhang, Z. Wu, S. Yang, L. Zhou, S. Yan, L. Wang, Y. Xiong, D. Lin, Y. Qiao et al., \"Cuhk & ethz & siat submission to activitynet challenge 2017,\" arXiv preprint arXiv:1710.08011, vol. 8, no. 8, 2017.\n\nG-tad: Sub-graph localization for temporal action detection. M Xu, C Zhao, D S Rojas, A Thabet, B Ghanem, CVPR. M. Xu, C. Zhao, D. S. Rojas, A. Thabet, and B. Ghanem, \"G-tad: Sub-graph localization for temporal action detection,\" in CVPR, 2020, pp. 10 156-10 165.\n\nBmn: Boundarymatching network for temporal action proposal generation. T Lin, X Liu, X Li, E Ding, S Wen, ICCV. T. Lin, X. Liu, X. Li, E. Ding, and S. Wen, \"Bmn: Boundary- matching network for temporal action proposal generation,\" in ICCV, 2019, pp. 3889-3898.\n\nActionformer: Localizing moments of actions with transformers. C.-L Zhang, J Wu, Y Li, ECCV. SpringerC.-L. Zhang, J. Wu, and Y. Li, \"Actionformer: Localizing moments of actions with transformers,\" in ECCV. Springer, 2022, pp. 492- 510.\n\nDetecting egocentric actions with actionformer. C Zhang, L Sui, A Majeedi, V R Gajjala, Y Li, C. Zhang, L. Sui, A. Majeedi, V. R. Gajjala, and Y. Li, \"Detecting egocentric actions with actionformer,\" https://epic-kitchens. github.io/Reports/EPIC-KITCHENS-Challenges-2022-Report. pdf.\n\nA stronger baseline for egocentric action detection. Z Qing, Z Huang, X Wang, Y Feng, S Zhang, J Jiang, M Tang, C Gao, M H AngJr, N Sang, arXiv:2106.06942arXiv preprintZ. Qing, Z. Huang, X. Wang, Y. Feng, S. Zhang, J. Jiang, M. Tang, C. Gao, M. H. Ang Jr, and N. Sang, \"A stronger baseline for ego- centric action detection,\" arXiv preprint arXiv:2106.06942, 2021.\n\nExploring stronger feature for temporal action localization. Z Qing, X Wang, Z Huang, Y Feng, S Zhang, M Tang, C Gao, N Sang, arXiv:2106.13014arXiv preprintZ. Qing, X. Wang, Z. Huang, Y. Feng, S. Zhang, M. Tang, C. Gao, N. Sang et al., \"Exploring stronger feature for temporal action localization,\" arXiv preprint arXiv:2106.13014, 2021.\n\nImproving neural networks by preventing coadaptation of feature detectors. G E Hinton, N Srivastava, A Krizhevsky, I Sutskever, R R Salakhutdinov, arXiv:1207.0580arXiv preprintG. E. Hinton, N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov, \"Improving neural networks by preventing co- adaptation of feature detectors,\" arXiv preprint arXiv:1207.0580, 2012.\n\nTea: Temporal excitation and aggregation for action recognition. Y Li, B Ji, X Shi, J Zhang, B Kang, L Wang, CVPR. Y. Li, B. Ji, X. Shi, J. Zhang, B. Kang, and L. Wang, \"Tea: Temporal excitation and aggregation for action recognition,\" in CVPR, 2020, pp. 909-918.\n\nTemporal context aggregation network for temporal action proposal refinement. Z Qing, H Su, W Gan, D Wang, W Wu, X Wang, Y Qiao, J Yan, C Gao, N Sang, CVPRZ. Qing, H. Su, W. Gan, D. Wang, W. Wu, X. Wang, Y. Qiao, J. Yan, C. Gao, and N. Sang, \"Temporal context aggregation network for temporal action proposal refinement,\" in CVPR, 2021, pp. 485- 494.\n", "annotations": {"author": "[{\"end\":104,\"start\":77},{\"end\":111,\"start\":105}]", "publisher": null, "author_last_name": "[{\"end\":103,\"start\":77},{\"end\":110,\"start\":105}]", "author_first_name": null, "author_affiliation": null, "title": "[{\"end\":61,\"start\":1},{\"end\":172,\"start\":112}]", "venue": null, "abstract": "[{\"end\":1732,\"start\":330}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1824,\"start\":1821},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1829,\"start\":1826},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1834,\"start\":1831},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":1839,\"start\":1836},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1976,\"start\":1973},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1981,\"start\":1978},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1986,\"start\":1983},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1991,\"start\":1988},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1996,\"start\":1993},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2027,\"start\":2023},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2033,\"start\":2029},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2094,\"start\":2090},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2221,\"start\":2217},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2227,\"start\":2223},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2516,\"start\":2512},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2522,\"start\":2518},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2740,\"start\":2736},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2746,\"start\":2742},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3236,\"start\":3232},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3242,\"start\":3238},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3390,\"start\":3386},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3396,\"start\":3392},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3539,\"start\":3535},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3545,\"start\":3541},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3575,\"start\":3571},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3581,\"start\":3577},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5052,\"start\":5048},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":5636,\"start\":5632},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6358,\"start\":6354},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":6364,\"start\":6360},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6370,\"start\":6366},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6376,\"start\":6372},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6404,\"start\":6400},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6516,\"start\":6512},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6522,\"start\":6518},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6528,\"start\":6524},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":6534,\"start\":6530},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6540,\"start\":6536},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6546,\"start\":6542},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6552,\"start\":6548},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":6558,\"start\":6554},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6564,\"start\":6560},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6676,\"start\":6672},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":6829,\"start\":6825},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7019,\"start\":7015},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7025,\"start\":7021},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":7031,\"start\":7027},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":7063,\"start\":7059},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":7124,\"start\":7120},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7130,\"start\":7126},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":7136,\"start\":7132},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":7142,\"start\":7138},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":7148,\"start\":7144},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":7154,\"start\":7150},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":7160,\"start\":7156},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":7195,\"start\":7191},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":7201,\"start\":7197},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":7207,\"start\":7203},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":7213,\"start\":7209},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":7219,\"start\":7215},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":7225,\"start\":7221},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":7231,\"start\":7227},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":7330,\"start\":7326},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":7336,\"start\":7332},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":7348,\"start\":7344},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":7354,\"start\":7350},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":7360,\"start\":7356},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":7385,\"start\":7381},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":7391,\"start\":7387},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":7440,\"start\":7436},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":7446,\"start\":7442},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":7460,\"start\":7456},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":7466,\"start\":7462},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":7724,\"start\":7720},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":7759,\"start\":7755},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":7790,\"start\":7786},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":7905,\"start\":7901},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":8114,\"start\":8110},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8120,\"start\":8116},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":8126,\"start\":8122},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":8132,\"start\":8128},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":8158,\"start\":8154},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":8164,\"start\":8160},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":8190,\"start\":8186},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":8196,\"start\":8192},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":8314,\"start\":8310},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":8320,\"start\":8316},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":8326,\"start\":8322},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":8332,\"start\":8328},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":8371,\"start\":8367},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":8377,\"start\":8373},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":8383,\"start\":8379},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":8389,\"start\":8385},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":8439,\"start\":8435},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":8445,\"start\":8441},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8556,\"start\":8552},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9385,\"start\":9381},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9391,\"start\":9387},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":10330,\"start\":10326},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":11325,\"start\":11321},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":11331,\"start\":11327},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":13096,\"start\":13092},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":13126,\"start\":13122},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":14273,\"start\":14269},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":14529,\"start\":14525},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16547,\"start\":16543},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":16674,\"start\":16670},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16680,\"start\":16676},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":16686,\"start\":16682},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":16885,\"start\":16881},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":16891,\"start\":16887},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":16897,\"start\":16893},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":17023,\"start\":17019},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17738,\"start\":17735},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":17753,\"start\":17749},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":17766,\"start\":17762},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":18113,\"start\":18109},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":19479,\"start\":19475},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":19759,\"start\":19755},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":19942,\"start\":19938},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":20274,\"start\":20270},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20736,\"start\":20733},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":20751,\"start\":20747},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":20780,\"start\":20776},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":20992,\"start\":20988},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":20998,\"start\":20994},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":21153,\"start\":21149},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":21196,\"start\":21192},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":21236,\"start\":21232},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":21259,\"start\":21255},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":21291,\"start\":21287},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":21307,\"start\":21303},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":21925,\"start\":21921},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":21944,\"start\":21940},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":21967,\"start\":21963},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":22017,\"start\":22013},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":22023,\"start\":22019},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":22237,\"start\":22233},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":22329,\"start\":22325},{\"attributes\":{\"ref_id\":\"b93\"},\"end\":22377,\"start\":22373},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":23037,\"start\":23033},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":23051,\"start\":23047},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":23162,\"start\":23158},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":23213,\"start\":23209},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":23228,\"start\":23224},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":23283,\"start\":23279},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":23381,\"start\":23377},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":24265,\"start\":24261},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":24874,\"start\":24870},{\"attributes\":{\"ref_id\":\"b99\"},\"end\":24885,\"start\":24880},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":24902,\"start\":24898},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":29430,\"start\":29426},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":29646,\"start\":29643},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":29663,\"start\":29659},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":29777,\"start\":29773},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":29783,\"start\":29779},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":30319,\"start\":30315},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":30720,\"start\":30716},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":31850,\"start\":31846},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":34281,\"start\":34277},{\"attributes\":{\"ref_id\":\"b109\"},\"end\":34398,\"start\":34393},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":34865,\"start\":34861},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":34892,\"start\":34888},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":34927,\"start\":34923},{\"attributes\":{\"ref_id\":\"b124\"},\"end\":34934,\"start\":34929},{\"attributes\":{\"ref_id\":\"b125\"},\"end\":34941,\"start\":34936},{\"attributes\":{\"ref_id\":\"b121\"},\"end\":34991,\"start\":34986},{\"attributes\":{\"ref_id\":\"b121\"},\"end\":35236,\"start\":35231},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":35393,\"start\":35389},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":37391,\"start\":37388},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":38529,\"start\":38525},{\"attributes\":{\"ref_id\":\"b125\"},\"end\":40630,\"start\":40625},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":40729,\"start\":40725},{\"attributes\":{\"ref_id\":\"b121\"},\"end\":41062,\"start\":41057},{\"attributes\":{\"ref_id\":\"b124\"},\"end\":41613,\"start\":41608},{\"attributes\":{\"ref_id\":\"b125\"},\"end\":42065,\"start\":42060},{\"attributes\":{\"ref_id\":\"b125\"},\"end\":42616,\"start\":42611},{\"attributes\":{\"ref_id\":\"b122\"},\"end\":42693,\"start\":42688},{\"attributes\":{\"ref_id\":\"b123\"},\"end\":42700,\"start\":42695},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":42962,\"start\":42958},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":43007,\"start\":43003},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":43397,\"start\":43393},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":43464,\"start\":43460},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":43470,\"start\":43466},{\"attributes\":{\"ref_id\":\"b127\"},\"end\":43477,\"start\":43472},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":44171,\"start\":44167},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":44251,\"start\":44247},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":44307,\"start\":44303},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":44391,\"start\":44387},{\"end\":45431,\"start\":45430},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":45436,\"start\":45432},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":45466,\"start\":45462},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":45475,\"start\":45471},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":45541,\"start\":45537},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":46660,\"start\":46656},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":46674,\"start\":46670},{\"attributes\":{\"ref_id\":\"b99\"},\"end\":46688,\"start\":46683},{\"attributes\":{\"ref_id\":\"b128\"},\"end\":47242,\"start\":47237},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":48245,\"start\":48241},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":48265,\"start\":48261},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":48275,\"start\":48271},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":48288,\"start\":48284},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":51582,\"start\":51578},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":51865,\"start\":51861},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":52298,\"start\":52294},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":58185,\"start\":58181},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":59546,\"start\":59542},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":62355,\"start\":62351},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":63931,\"start\":63927},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":68087,\"start\":68083},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":71568,\"start\":71565}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":54658,\"start\":54243},{\"attributes\":{\"id\":\"fig_1\"},\"end\":54937,\"start\":54659},{\"attributes\":{\"id\":\"fig_2\"},\"end\":55213,\"start\":54938},{\"attributes\":{\"id\":\"fig_3\"},\"end\":55295,\"start\":55214},{\"attributes\":{\"id\":\"fig_4\"},\"end\":55406,\"start\":55296},{\"attributes\":{\"id\":\"fig_6\"},\"end\":55561,\"start\":55407},{\"attributes\":{\"id\":\"fig_7\"},\"end\":55848,\"start\":55562},{\"attributes\":{\"id\":\"fig_8\"},\"end\":55946,\"start\":55849},{\"attributes\":{\"id\":\"fig_9\"},\"end\":55989,\"start\":55947},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":56392,\"start\":55990},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":56760,\"start\":56393},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":56867,\"start\":56761},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":57310,\"start\":56868},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":57766,\"start\":57311},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":58129,\"start\":57767},{\"attributes\":{\"id\":\"tab_9\",\"type\":\"table\"},\"end\":59490,\"start\":58130},{\"attributes\":{\"id\":\"tab_10\",\"type\":\"table\"},\"end\":60901,\"start\":59491},{\"attributes\":{\"id\":\"tab_11\",\"type\":\"table\"},\"end\":62292,\"start\":60902},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":63290,\"start\":62293},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":63885,\"start\":63291},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":64523,\"start\":63886},{\"attributes\":{\"id\":\"tab_15\",\"type\":\"table\"},\"end\":65907,\"start\":64524},{\"attributes\":{\"id\":\"tab_17\",\"type\":\"table\"},\"end\":66651,\"start\":65908},{\"attributes\":{\"id\":\"tab_19\",\"type\":\"table\"},\"end\":67507,\"start\":66652},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":68024,\"start\":67508},{\"attributes\":{\"id\":\"tab_21\",\"type\":\"table\"},\"end\":70109,\"start\":68025},{\"attributes\":{\"id\":\"tab_22\",\"type\":\"table\"},\"end\":71305,\"start\":70110},{\"attributes\":{\"id\":\"tab_23\",\"type\":\"table\"},\"end\":71902,\"start\":71306},{\"attributes\":{\"id\":\"tab_25\",\"type\":\"table\"},\"end\":72815,\"start\":71903}]", "paragraph": "[{\"end\":2441,\"start\":1748},{\"end\":2747,\"start\":2443},{\"end\":3110,\"start\":2749},{\"end\":4441,\"start\":3112},{\"end\":4924,\"start\":4443},{\"end\":6206,\"start\":4926},{\"end\":7976,\"start\":6223},{\"end\":8797,\"start\":7978},{\"end\":9126,\"start\":8834},{\"end\":9625,\"start\":9163},{\"end\":10363,\"start\":10188},{\"end\":10422,\"start\":10417},{\"end\":11070,\"start\":10510},{\"end\":11850,\"start\":11113},{\"end\":12956,\"start\":11926},{\"end\":13188,\"start\":13074},{\"end\":13493,\"start\":13190},{\"end\":13900,\"start\":13609},{\"end\":14360,\"start\":13902},{\"end\":15103,\"start\":14476},{\"end\":15247,\"start\":15146},{\"end\":15809,\"start\":15272},{\"end\":16278,\"start\":16021},{\"end\":17471,\"start\":16280},{\"end\":17767,\"start\":17486},{\"end\":18577,\"start\":17769},{\"end\":18650,\"start\":18579},{\"end\":19446,\"start\":18706},{\"end\":19943,\"start\":19448},{\"end\":20546,\"start\":19945},{\"end\":21091,\"start\":20595},{\"end\":21822,\"start\":21093},{\"end\":22155,\"start\":21824},{\"end\":22558,\"start\":22157},{\"end\":23470,\"start\":22788},{\"end\":24577,\"start\":23742},{\"end\":25288,\"start\":24618},{\"end\":27631,\"start\":25322},{\"end\":28559,\"start\":27633},{\"end\":28998,\"start\":28561},{\"end\":29348,\"start\":29000},{\"end\":29986,\"start\":29389},{\"end\":30406,\"start\":29988},{\"end\":31392,\"start\":30457},{\"end\":31609,\"start\":31394},{\"end\":32210,\"start\":31611},{\"end\":32957,\"start\":32227},{\"end\":33506,\"start\":32959},{\"end\":34514,\"start\":33508},{\"end\":35343,\"start\":34553},{\"end\":35579,\"start\":35359},{\"end\":35854,\"start\":35581},{\"end\":36505,\"start\":35878},{\"end\":37656,\"start\":36563},{\"end\":37782,\"start\":37777},{\"end\":38349,\"start\":37835},{\"end\":38579,\"start\":38418},{\"end\":38664,\"start\":38609},{\"end\":38857,\"start\":38699},{\"end\":39051,\"start\":38934},{\"end\":39160,\"start\":39128},{\"end\":39650,\"start\":39255},{\"end\":39951,\"start\":39696},{\"end\":40231,\"start\":40001},{\"end\":40517,\"start\":40259},{\"end\":40974,\"start\":40519},{\"end\":42148,\"start\":40976},{\"end\":42617,\"start\":42150},{\"end\":42755,\"start\":42619},{\"end\":43008,\"start\":42757},{\"end\":44172,\"start\":43040},{\"end\":44613,\"start\":44174},{\"end\":45833,\"start\":44670},{\"end\":46273,\"start\":45873},{\"end\":46976,\"start\":46348},{\"end\":47457,\"start\":47046},{\"end\":47856,\"start\":47505},{\"end\":48074,\"start\":47912},{\"end\":48289,\"start\":48117},{\"end\":49226,\"start\":48291},{\"end\":50141,\"start\":49228},{\"end\":51023,\"start\":50185},{\"end\":51558,\"start\":51025},{\"end\":51851,\"start\":51560},{\"end\":53286,\"start\":51853},{\"end\":53833,\"start\":53288},{\"end\":54242,\"start\":53835}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10121,\"start\":9626},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10187,\"start\":10121},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10416,\"start\":10364},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10509,\"start\":10423},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11892,\"start\":11851},{\"attributes\":{\"id\":\"formula_5\"},\"end\":13073,\"start\":12957},{\"attributes\":{\"id\":\"formula_6\"},\"end\":13608,\"start\":13494},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14475,\"start\":14361},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15145,\"start\":15104},{\"attributes\":{\"id\":\"formula_9\"},\"end\":15950,\"start\":15810},{\"attributes\":{\"id\":\"formula_10\"},\"end\":16020,\"start\":15950},{\"attributes\":{\"id\":\"formula_11\"},\"end\":18705,\"start\":18651},{\"attributes\":{\"id\":\"formula_12\"},\"end\":22787,\"start\":22559},{\"attributes\":{\"id\":\"formula_13\"},\"end\":23712,\"start\":23471},{\"attributes\":{\"id\":\"formula_14\"},\"end\":37723,\"start\":37657},{\"attributes\":{\"id\":\"formula_15\"},\"end\":37776,\"start\":37723},{\"attributes\":{\"id\":\"formula_16\"},\"end\":37834,\"start\":37783},{\"attributes\":{\"id\":\"formula_17\"},\"end\":38417,\"start\":38350},{\"attributes\":{\"id\":\"formula_18\"},\"end\":38608,\"start\":38580},{\"attributes\":{\"id\":\"formula_19\"},\"end\":38698,\"start\":38665},{\"attributes\":{\"id\":\"formula_20\"},\"end\":38933,\"start\":38858},{\"attributes\":{\"id\":\"formula_21\"},\"end\":39127,\"start\":39052},{\"attributes\":{\"id\":\"formula_22\"},\"end\":39254,\"start\":39161}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":11068,\"start\":11061},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":15632,\"start\":15624},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":16067,\"start\":16060},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":16329,\"start\":16322},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":24123,\"start\":24115},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":24729,\"start\":24721},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":25635,\"start\":25627},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":25783,\"start\":25775},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":26543,\"start\":26535},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":27392,\"start\":27384},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":27708,\"start\":27700},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":28188,\"start\":28180},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":29497,\"start\":29490},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":30501,\"start\":30494},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":31726,\"start\":31719},{\"attributes\":{\"ref_id\":\"tab_9\"},\"end\":32248,\"start\":32241},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":32590,\"start\":32583},{\"attributes\":{\"ref_id\":\"tab_11\"},\"end\":33086,\"start\":33079},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":33598,\"start\":33590},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":34152,\"start\":34144},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":35207,\"start\":35186},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":40216,\"start\":40195},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":43119,\"start\":43111},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":43922,\"start\":43914},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":45992,\"start\":45983},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":46006,\"start\":45997},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":46126,\"start\":46117},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":47312,\"start\":47304},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":49521,\"start\":49281},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":49821,\"start\":49813},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":51022,\"start\":51014},{\"attributes\":{\"ref_id\":\"tab_10\"},\"end\":53311,\"start\":53303}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1746,\"start\":1734},{\"attributes\":{\"n\":\"2\"},\"end\":6221,\"start\":6209},{\"attributes\":{\"n\":\"3\"},\"end\":8832,\"start\":8800},{\"attributes\":{\"n\":\"3.1\"},\"end\":9161,\"start\":9129},{\"attributes\":{\"n\":\"3.2\"},\"end\":11111,\"start\":11073},{\"attributes\":{\"n\":\"3.3\"},\"end\":11924,\"start\":11894},{\"end\":15270,\"start\":15250},{\"attributes\":{\"n\":\"4\"},\"end\":17484,\"start\":17474},{\"attributes\":{\"n\":\"5\"},\"end\":20584,\"start\":20549},{\"end\":20593,\"start\":20587},{\"attributes\":{\"n\":\"5.1\"},\"end\":23740,\"start\":23714},{\"attributes\":{\"n\":\"5.2\"},\"end\":24616,\"start\":24580},{\"attributes\":{\"n\":\"5.3\"},\"end\":25320,\"start\":25291},{\"attributes\":{\"n\":\"5.4\"},\"end\":29387,\"start\":29351},{\"attributes\":{\"n\":\"5.5\"},\"end\":30455,\"start\":30409},{\"attributes\":{\"n\":\"5.6\"},\"end\":32225,\"start\":32213},{\"attributes\":{\"n\":\"6\"},\"end\":34551,\"start\":34517},{\"attributes\":{\"n\":\"7\"},\"end\":35357,\"start\":35346},{\"end\":35876,\"start\":35857},{\"end\":36561,\"start\":36508},{\"end\":39694,\"start\":39653},{\"end\":39999,\"start\":39954},{\"end\":40257,\"start\":40234},{\"end\":43038,\"start\":43011},{\"end\":44668,\"start\":44616},{\"end\":45871,\"start\":45836},{\"end\":46329,\"start\":46276},{\"end\":46346,\"start\":46332},{\"end\":47044,\"start\":46979},{\"end\":47503,\"start\":47460},{\"end\":47910,\"start\":47859},{\"end\":48115,\"start\":48077},{\"end\":50183,\"start\":50144},{\"end\":54245,\"start\":54244},{\"end\":54668,\"start\":54660},{\"end\":54947,\"start\":54939},{\"end\":55223,\"start\":55215},{\"end\":55305,\"start\":55297},{\"end\":55417,\"start\":55408},{\"end\":55572,\"start\":55563},{\"end\":55859,\"start\":55850},{\"end\":56000,\"start\":55991},{\"end\":56403,\"start\":56394},{\"end\":56771,\"start\":56762},{\"end\":56878,\"start\":56869},{\"end\":57321,\"start\":57312},{\"end\":57777,\"start\":57768},{\"end\":58140,\"start\":58131},{\"end\":59501,\"start\":59492},{\"end\":60912,\"start\":60903},{\"end\":62304,\"start\":62294},{\"end\":63302,\"start\":63292},{\"end\":63897,\"start\":63887},{\"end\":64535,\"start\":64525},{\"end\":65919,\"start\":65909},{\"end\":66663,\"start\":66653},{\"end\":67519,\"start\":67509},{\"end\":68036,\"start\":68026},{\"end\":70121,\"start\":70111},{\"end\":71317,\"start\":71307},{\"end\":71914,\"start\":71904}]", "table": "[{\"end\":56392,\"start\":56002},{\"end\":56760,\"start\":56576},{\"end\":57310,\"start\":56917},{\"end\":57766,\"start\":57355},{\"end\":58129,\"start\":57828},{\"end\":59490,\"start\":58186},{\"end\":60901,\"start\":59599},{\"end\":62292,\"start\":61074},{\"end\":63290,\"start\":62412},{\"end\":63885,\"start\":63305},{\"end\":64523,\"start\":63932},{\"end\":65907,\"start\":64538},{\"end\":66651,\"start\":65922},{\"end\":67507,\"start\":66666},{\"end\":68024,\"start\":67597},{\"end\":70109,\"start\":69440},{\"end\":71305,\"start\":70610},{\"end\":71902,\"start\":71660},{\"end\":72815,\"start\":72548}]", "figure_caption": "[{\"end\":54658,\"start\":54246},{\"end\":54937,\"start\":54670},{\"end\":55213,\"start\":54949},{\"end\":55295,\"start\":55225},{\"end\":55406,\"start\":55307},{\"end\":55561,\"start\":55420},{\"end\":55848,\"start\":55575},{\"end\":55946,\"start\":55862},{\"end\":55989,\"start\":55949},{\"end\":56576,\"start\":56405},{\"end\":56867,\"start\":56773},{\"end\":56917,\"start\":56880},{\"end\":57355,\"start\":57323},{\"end\":57828,\"start\":57779},{\"end\":58186,\"start\":58142},{\"end\":59599,\"start\":59503},{\"end\":61074,\"start\":60914},{\"end\":62412,\"start\":62307},{\"end\":63932,\"start\":63900},{\"end\":67597,\"start\":67522},{\"end\":69440,\"start\":68039},{\"end\":70610,\"start\":70124},{\"end\":71660,\"start\":71320},{\"end\":72548,\"start\":71917}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":4440,\"start\":4434},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":12409,\"start\":12403},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":17636,\"start\":17630},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":19674,\"start\":19668},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20024,\"start\":20014},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":20353,\"start\":20343},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":28603,\"start\":28598},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":29183,\"start\":29177},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":44775,\"start\":44769},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":44787,\"start\":44780},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":47610,\"start\":47603},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":47662,\"start\":47655},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":49537,\"start\":49530}]", "bib_author_first_name": "[{\"end\":73265,\"start\":73264},{\"end\":73271,\"start\":73270},{\"end\":73280,\"start\":73279},{\"end\":73287,\"start\":73286},{\"end\":73446,\"start\":73445},{\"end\":73457,\"start\":73456},{\"end\":73464,\"start\":73463},{\"end\":73471,\"start\":73470},{\"end\":73483,\"start\":73482},{\"end\":73491,\"start\":73490},{\"end\":73503,\"start\":73502},{\"end\":73512,\"start\":73511},{\"end\":73525,\"start\":73524},{\"end\":73775,\"start\":73774},{\"end\":73789,\"start\":73788},{\"end\":73802,\"start\":73801},{\"end\":73804,\"start\":73803},{\"end\":74045,\"start\":74044},{\"end\":74052,\"start\":74051},{\"end\":74059,\"start\":74058},{\"end\":74061,\"start\":74060},{\"end\":74067,\"start\":74066},{\"end\":74293,\"start\":74292},{\"end\":74300,\"start\":74299},{\"end\":74312,\"start\":74311},{\"end\":74322,\"start\":74321},{\"end\":74328,\"start\":74327},{\"end\":74514,\"start\":74513},{\"end\":74521,\"start\":74520},{\"end\":74527,\"start\":74526},{\"end\":74536,\"start\":74535},{\"end\":74542,\"start\":74541},{\"end\":74551,\"start\":74550},{\"end\":74557,\"start\":74556},{\"end\":74761,\"start\":74760},{\"end\":74769,\"start\":74768},{\"end\":74776,\"start\":74775},{\"end\":74783,\"start\":74782},{\"end\":74791,\"start\":74790},{\"end\":74799,\"start\":74798},{\"end\":75039,\"start\":75038},{\"end\":75048,\"start\":75047},{\"end\":75054,\"start\":75053},{\"end\":75063,\"start\":75062},{\"end\":75070,\"start\":75069},{\"end\":75077,\"start\":75076},{\"end\":75086,\"start\":75085},{\"end\":75093,\"start\":75092},{\"end\":75099,\"start\":75098},{\"end\":75110,\"start\":75109},{\"end\":75346,\"start\":75345},{\"end\":75354,\"start\":75353},{\"end\":75362,\"start\":75361},{\"end\":75579,\"start\":75578},{\"end\":75581,\"start\":75580},{\"end\":75591,\"start\":75590},{\"end\":75598,\"start\":75597},{\"end\":75606,\"start\":75605},{\"end\":75622,\"start\":75621},{\"end\":75630,\"start\":75629},{\"end\":75640,\"start\":75639},{\"end\":75653,\"start\":75652},{\"end\":75988,\"start\":75987},{\"end\":75996,\"start\":75995},{\"end\":76006,\"start\":76005},{\"end\":76008,\"start\":76007},{\"end\":76014,\"start\":76013},{\"end\":76282,\"start\":76281},{\"end\":76290,\"start\":76289},{\"end\":76301,\"start\":76300},{\"end\":76311,\"start\":76310},{\"end\":76324,\"start\":76323},{\"end\":76570,\"start\":76569},{\"end\":76578,\"start\":76577},{\"end\":76586,\"start\":76585},{\"end\":76599,\"start\":76598},{\"end\":76606,\"start\":76605},{\"end\":76615,\"start\":76614},{\"end\":76877,\"start\":76876},{\"end\":76884,\"start\":76883},{\"end\":76891,\"start\":76890},{\"end\":77087,\"start\":77086},{\"end\":77089,\"start\":77088},{\"end\":77101,\"start\":77100},{\"end\":77334,\"start\":77333},{\"end\":77336,\"start\":77335},{\"end\":77350,\"start\":77349},{\"end\":77352,\"start\":77351},{\"end\":77595,\"start\":77594},{\"end\":77603,\"start\":77602},{\"end\":77614,\"start\":77613},{\"end\":77620,\"start\":77619},{\"end\":77630,\"start\":77626},{\"end\":77822,\"start\":77821},{\"end\":77828,\"start\":77827},{\"end\":77834,\"start\":77833},{\"end\":77842,\"start\":77841},{\"end\":77851,\"start\":77850},{\"end\":78040,\"start\":78039},{\"end\":78057,\"start\":78056},{\"end\":78064,\"start\":78063},{\"end\":78073,\"start\":78072},{\"end\":78268,\"start\":78267},{\"end\":78427,\"start\":78426},{\"end\":78434,\"start\":78433},{\"end\":78451,\"start\":78450},{\"end\":78465,\"start\":78464},{\"end\":78467,\"start\":78466},{\"end\":78665,\"start\":78664},{\"end\":78673,\"start\":78672},{\"end\":78680,\"start\":78679},{\"end\":78687,\"start\":78686},{\"end\":78695,\"start\":78694},{\"end\":78703,\"start\":78702},{\"end\":78931,\"start\":78930},{\"end\":78940,\"start\":78939},{\"end\":78949,\"start\":78948},{\"end\":78956,\"start\":78955},{\"end\":78964,\"start\":78963},{\"end\":78972,\"start\":78971},{\"end\":78979,\"start\":78978},{\"end\":78981,\"start\":78980},{\"end\":79177,\"start\":79176},{\"end\":79188,\"start\":79187},{\"end\":79199,\"start\":79198},{\"end\":79209,\"start\":79208},{\"end\":79222,\"start\":79221},{\"end\":79231,\"start\":79230},{\"end\":79233,\"start\":79232},{\"end\":79242,\"start\":79241},{\"end\":79252,\"start\":79251},{\"end\":79502,\"start\":79501},{\"end\":79514,\"start\":79513},{\"end\":79728,\"start\":79727},{\"end\":79736,\"start\":79735},{\"end\":79744,\"start\":79743},{\"end\":79757,\"start\":79756},{\"end\":79993,\"start\":79992},{\"end\":80005,\"start\":80004},{\"end\":80248,\"start\":80247},{\"end\":80255,\"start\":80254},{\"end\":80262,\"start\":80261},{\"end\":80466,\"start\":80465},{\"end\":80474,\"start\":80473},{\"end\":80482,\"start\":80481},{\"end\":80488,\"start\":80487},{\"end\":80701,\"start\":80700},{\"end\":80710,\"start\":80709},{\"end\":80718,\"start\":80717},{\"end\":80725,\"start\":80724},{\"end\":80731,\"start\":80730},{\"end\":80942,\"start\":80941},{\"end\":80949,\"start\":80948},{\"end\":80957,\"start\":80956},{\"end\":80963,\"start\":80962},{\"end\":80971,\"start\":80970},{\"end\":81135,\"start\":81134},{\"end\":81143,\"start\":81142},{\"end\":81151,\"start\":81150},{\"end\":81164,\"start\":81163},{\"end\":81386,\"start\":81385},{\"end\":81394,\"start\":81393},{\"end\":81401,\"start\":81400},{\"end\":81409,\"start\":81408},{\"end\":81417,\"start\":81416},{\"end\":81424,\"start\":81423},{\"end\":81431,\"start\":81430},{\"end\":81443,\"start\":81442},{\"end\":81713,\"start\":81712},{\"end\":81719,\"start\":81718},{\"end\":81727,\"start\":81726},{\"end\":81735,\"start\":81734},{\"end\":81962,\"start\":81961},{\"end\":81970,\"start\":81969},{\"end\":81979,\"start\":81978},{\"end\":81987,\"start\":81986},{\"end\":81994,\"start\":81993},{\"end\":81996,\"start\":81995},{\"end\":82198,\"start\":82197},{\"end\":82205,\"start\":82204},{\"end\":82215,\"start\":82211},{\"end\":82221,\"start\":82220},{\"end\":82238,\"start\":82237},{\"end\":82249,\"start\":82248},{\"end\":82528,\"start\":82527},{\"end\":82530,\"start\":82529},{\"end\":82537,\"start\":82533},{\"end\":82539,\"start\":82538},{\"end\":82549,\"start\":82548},{\"end\":82551,\"start\":82550},{\"end\":82838,\"start\":82837},{\"end\":82849,\"start\":82848},{\"end\":82863,\"start\":82862},{\"end\":82875,\"start\":82874},{\"end\":83095,\"start\":83094},{\"end\":83110,\"start\":83109},{\"end\":83119,\"start\":83118},{\"end\":83133,\"start\":83132},{\"end\":83148,\"start\":83147},{\"end\":83156,\"start\":83155},{\"end\":83171,\"start\":83170},{\"end\":83183,\"start\":83182},{\"end\":83195,\"start\":83194},{\"end\":83206,\"start\":83205},{\"end\":83532,\"start\":83531},{\"end\":83539,\"start\":83538},{\"end\":83548,\"start\":83547},{\"end\":83560,\"start\":83559},{\"end\":83566,\"start\":83565},{\"end\":83573,\"start\":83572},{\"end\":83582,\"start\":83581},{\"end\":83802,\"start\":83801},{\"end\":83815,\"start\":83814},{\"end\":83827,\"start\":83826},{\"end\":83841,\"start\":83840},{\"end\":84063,\"start\":84062},{\"end\":84070,\"start\":84069},{\"end\":84079,\"start\":84078},{\"end\":84086,\"start\":84085},{\"end\":84095,\"start\":84094},{\"end\":84102,\"start\":84101},{\"end\":84302,\"start\":84301},{\"end\":84309,\"start\":84308},{\"end\":84317,\"start\":84316},{\"end\":84325,\"start\":84324},{\"end\":84555,\"start\":84554},{\"end\":84563,\"start\":84562},{\"end\":84570,\"start\":84569},{\"end\":84577,\"start\":84576},{\"end\":84584,\"start\":84583},{\"end\":84591,\"start\":84590},{\"end\":84598,\"start\":84597},{\"end\":84606,\"start\":84605},{\"end\":84852,\"start\":84851},{\"end\":84862,\"start\":84861},{\"end\":84874,\"start\":84873},{\"end\":84884,\"start\":84883},{\"end\":84891,\"start\":84890},{\"end\":85142,\"start\":85141},{\"end\":85152,\"start\":85151},{\"end\":85161,\"start\":85160},{\"end\":85173,\"start\":85172},{\"end\":85184,\"start\":85183},{\"end\":85196,\"start\":85195},{\"end\":85415,\"start\":85414},{\"end\":85423,\"start\":85422},{\"end\":85435,\"start\":85434},{\"end\":85444,\"start\":85443},{\"end\":85583,\"start\":85582},{\"end\":85590,\"start\":85589},{\"end\":85598,\"start\":85597},{\"end\":85605,\"start\":85604},{\"end\":85612,\"start\":85611},{\"end\":85621,\"start\":85620},{\"end\":85628,\"start\":85627},{\"end\":85827,\"start\":85826},{\"end\":85836,\"start\":85835},{\"end\":85848,\"start\":85847},{\"end\":85859,\"start\":85858},{\"end\":85866,\"start\":85865},{\"end\":85875,\"start\":85874},{\"end\":86089,\"start\":86088},{\"end\":86102,\"start\":86101},{\"end\":86110,\"start\":86109},{\"end\":86378,\"start\":86377},{\"end\":86385,\"start\":86384},{\"end\":86393,\"start\":86392},{\"end\":86399,\"start\":86398},{\"end\":86407,\"start\":86406},{\"end\":86416,\"start\":86415},{\"end\":86423,\"start\":86422},{\"end\":86665,\"start\":86664},{\"end\":86676,\"start\":86672},{\"end\":86685,\"start\":86684},{\"end\":86693,\"start\":86692},{\"end\":86699,\"start\":86698},{\"end\":86708,\"start\":86707},{\"end\":86715,\"start\":86714},{\"end\":86723,\"start\":86722},{\"end\":86731,\"start\":86730},{\"end\":86739,\"start\":86738},{\"end\":87050,\"start\":87049},{\"end\":87061,\"start\":87060},{\"end\":87073,\"start\":87072},{\"end\":87082,\"start\":87081},{\"end\":87091,\"start\":87090},{\"end\":87100,\"start\":87099},{\"end\":87117,\"start\":87116},{\"end\":87128,\"start\":87127},{\"end\":87130,\"start\":87129},{\"end\":87533,\"start\":87532},{\"end\":87541,\"start\":87540},{\"end\":87548,\"start\":87547},{\"end\":87740,\"start\":87739},{\"end\":87746,\"start\":87745},{\"end\":87754,\"start\":87753},{\"end\":87761,\"start\":87760},{\"end\":87767,\"start\":87766},{\"end\":87777,\"start\":87776},{\"end\":88041,\"start\":88040},{\"end\":88049,\"start\":88048},{\"end\":88057,\"start\":88056},{\"end\":88065,\"start\":88064},{\"end\":88331,\"start\":88330},{\"end\":88348,\"start\":88347},{\"end\":88354,\"start\":88353},{\"end\":88581,\"start\":88580},{\"end\":88589,\"start\":88588},{\"end\":88598,\"start\":88597},{\"end\":88607,\"start\":88606},{\"end\":88613,\"start\":88612},{\"end\":88621,\"start\":88620},{\"end\":88628,\"start\":88627},{\"end\":88635,\"start\":88634},{\"end\":88884,\"start\":88883},{\"end\":88895,\"start\":88894},{\"end\":88897,\"start\":88896},{\"end\":88904,\"start\":88903},{\"end\":88915,\"start\":88914},{\"end\":88925,\"start\":88924},{\"end\":88932,\"start\":88931},{\"end\":88943,\"start\":88942},{\"end\":88953,\"start\":88952},{\"end\":88963,\"start\":88962},{\"end\":88974,\"start\":88973},{\"end\":89294,\"start\":89293},{\"end\":89300,\"start\":89299},{\"end\":89308,\"start\":89307},{\"end\":89321,\"start\":89320},{\"end\":89330,\"start\":89329},{\"end\":89347,\"start\":89346},{\"end\":89586,\"start\":89585},{\"end\":89594,\"start\":89593},{\"end\":89608,\"start\":89607},{\"end\":89619,\"start\":89618},{\"end\":89804,\"start\":89803},{\"end\":89816,\"start\":89815},{\"end\":89828,\"start\":89827},{\"end\":89839,\"start\":89838},{\"end\":89852,\"start\":89851},{\"end\":89860,\"start\":89859},{\"end\":89870,\"start\":89869},{\"end\":89881,\"start\":89880},{\"end\":89890,\"start\":89889},{\"end\":89901,\"start\":89900},{\"end\":90263,\"start\":90262},{\"end\":90271,\"start\":90270},{\"end\":90278,\"start\":90277},{\"end\":90286,\"start\":90285},{\"end\":90296,\"start\":90295},{\"end\":90304,\"start\":90303},{\"end\":90311,\"start\":90310},{\"end\":90323,\"start\":90322},{\"end\":90325,\"start\":90324},{\"end\":90337,\"start\":90336},{\"end\":90348,\"start\":90347},{\"end\":90691,\"start\":90690},{\"end\":90699,\"start\":90698},{\"end\":90708,\"start\":90707},{\"end\":90716,\"start\":90715},{\"end\":90724,\"start\":90723},{\"end\":90730,\"start\":90729},{\"end\":90738,\"start\":90737},{\"end\":90746,\"start\":90745},{\"end\":91055,\"start\":91054},{\"end\":91062,\"start\":91061},{\"end\":91069,\"start\":91068},{\"end\":91076,\"start\":91075},{\"end\":91084,\"start\":91083},{\"end\":91353,\"start\":91352},{\"end\":91359,\"start\":91358},{\"end\":91367,\"start\":91366},{\"end\":91374,\"start\":91373},{\"end\":91382,\"start\":91381},{\"end\":91388,\"start\":91387},{\"end\":91396,\"start\":91395},{\"end\":91403,\"start\":91402},{\"end\":91411,\"start\":91410},{\"end\":91596,\"start\":91595},{\"end\":91602,\"start\":91601},{\"end\":91610,\"start\":91609},{\"end\":91617,\"start\":91616},{\"end\":91625,\"start\":91624},{\"end\":91632,\"start\":91631},{\"end\":91640,\"start\":91639},{\"end\":91647,\"start\":91646},{\"end\":91656,\"start\":91655},{\"end\":91969,\"start\":91968},{\"end\":91977,\"start\":91976},{\"end\":91984,\"start\":91983},{\"end\":91991,\"start\":91990},{\"end\":91999,\"start\":91998},{\"end\":92007,\"start\":92006},{\"end\":92199,\"start\":92198},{\"end\":92207,\"start\":92206},{\"end\":92216,\"start\":92212},{\"end\":92223,\"start\":92222},{\"end\":92234,\"start\":92233},{\"end\":92236,\"start\":92235},{\"end\":92460,\"start\":92459},{\"end\":92466,\"start\":92465},{\"end\":92474,\"start\":92473},{\"end\":92482,\"start\":92481},{\"end\":92488,\"start\":92487},{\"end\":92497,\"start\":92496},{\"end\":92505,\"start\":92504},{\"end\":92743,\"start\":92742},{\"end\":92749,\"start\":92748},{\"end\":92756,\"start\":92755},{\"end\":92762,\"start\":92761},{\"end\":92774,\"start\":92770},{\"end\":92781,\"start\":92780},{\"end\":92790,\"start\":92789},{\"end\":92802,\"start\":92798},{\"end\":93104,\"start\":93100},{\"end\":93113,\"start\":93112},{\"end\":93119,\"start\":93118},{\"end\":93127,\"start\":93126},{\"end\":93135,\"start\":93134},{\"end\":93143,\"start\":93142},{\"end\":93423,\"start\":93419},{\"end\":93432,\"start\":93428},{\"end\":93434,\"start\":93433},{\"end\":93443,\"start\":93442},{\"end\":93455,\"start\":93451},{\"end\":93465,\"start\":93461},{\"end\":93734,\"start\":93733},{\"end\":93740,\"start\":93739},{\"end\":93747,\"start\":93746},{\"end\":93758,\"start\":93757},{\"end\":93760,\"start\":93759},{\"end\":93771,\"start\":93770},{\"end\":94044,\"start\":94043},{\"end\":94052,\"start\":94051},{\"end\":94064,\"start\":94060},{\"end\":94071,\"start\":94070},{\"end\":94084,\"start\":94083},{\"end\":94097,\"start\":94096},{\"end\":94107,\"start\":94106},{\"end\":94116,\"start\":94115},{\"end\":94381,\"start\":94380},{\"end\":94387,\"start\":94386},{\"end\":94399,\"start\":94395},{\"end\":94405,\"start\":94404},{\"end\":94415,\"start\":94414},{\"end\":94417,\"start\":94416},{\"end\":94651,\"start\":94650},{\"end\":94662,\"start\":94658},{\"end\":94669,\"start\":94668},{\"end\":94678,\"start\":94677},{\"end\":94691,\"start\":94690},{\"end\":94704,\"start\":94703},{\"end\":94713,\"start\":94712},{\"end\":94723,\"start\":94722},{\"end\":95012,\"start\":95011},{\"end\":95023,\"start\":95022},{\"end\":95039,\"start\":95038},{\"end\":95049,\"start\":95048},{\"end\":95276,\"start\":95275},{\"end\":95284,\"start\":95283},{\"end\":95292,\"start\":95291},{\"end\":95299,\"start\":95298},{\"end\":95308,\"start\":95307},{\"end\":95499,\"start\":95498},{\"end\":95507,\"start\":95506},{\"end\":95509,\"start\":95508},{\"end\":95725,\"start\":95724},{\"end\":95734,\"start\":95733},{\"end\":95932,\"start\":95931},{\"end\":95934,\"start\":95933},{\"end\":95940,\"start\":95939},{\"end\":95942,\"start\":95941},{\"end\":95951,\"start\":95950},{\"end\":95953,\"start\":95952},{\"end\":96097,\"start\":96096},{\"end\":96104,\"start\":96103},{\"end\":96116,\"start\":96115},{\"end\":96128,\"start\":96127},{\"end\":96137,\"start\":96136},{\"end\":96148,\"start\":96147},{\"end\":96168,\"start\":96167},{\"end\":96177,\"start\":96176},{\"end\":96186,\"start\":96185},{\"end\":96194,\"start\":96193},{\"end\":96576,\"start\":96575},{\"end\":96585,\"start\":96584},{\"end\":96587,\"start\":96586},{\"end\":96596,\"start\":96595},{\"end\":96609,\"start\":96608},{\"end\":96624,\"start\":96623},{\"end\":96636,\"start\":96635},{\"end\":96643,\"start\":96642},{\"end\":96653,\"start\":96652},{\"end\":96663,\"start\":96662},{\"end\":96675,\"start\":96674},{\"end\":96989,\"start\":96988},{\"end\":96998,\"start\":96997},{\"end\":97009,\"start\":97008},{\"end\":97011,\"start\":97010},{\"end\":97024,\"start\":97023},{\"end\":97035,\"start\":97034},{\"end\":97046,\"start\":97045},{\"end\":97052,\"start\":97051},{\"end\":97066,\"start\":97065},{\"end\":97075,\"start\":97074},{\"end\":97086,\"start\":97085},{\"end\":97409,\"start\":97408},{\"end\":97417,\"start\":97416},{\"end\":97429,\"start\":97428},{\"end\":97442,\"start\":97441},{\"end\":97697,\"start\":97696},{\"end\":97707,\"start\":97706},{\"end\":97709,\"start\":97708},{\"end\":97718,\"start\":97717},{\"end\":97964,\"start\":97963},{\"end\":97974,\"start\":97973},{\"end\":97984,\"start\":97983},{\"end\":97995,\"start\":97994},{\"end\":98005,\"start\":98004},{\"end\":98214,\"start\":98213},{\"end\":98226,\"start\":98225},{\"end\":98236,\"start\":98235},{\"end\":98253,\"start\":98252},{\"end\":98264,\"start\":98263},{\"end\":98509,\"start\":98508},{\"end\":98521,\"start\":98520},{\"end\":98531,\"start\":98530},{\"end\":98542,\"start\":98541},{\"end\":98816,\"start\":98815},{\"end\":98822,\"start\":98821},{\"end\":98830,\"start\":98829},{\"end\":98836,\"start\":98835},{\"end\":98842,\"start\":98841},{\"end\":98850,\"start\":98849},{\"end\":98858,\"start\":98857},{\"end\":99118,\"start\":99117},{\"end\":99132,\"start\":99131},{\"end\":99364,\"start\":99363},{\"end\":99366,\"start\":99365},{\"end\":99375,\"start\":99374},{\"end\":99383,\"start\":99382},{\"end\":99393,\"start\":99392},{\"end\":99395,\"start\":99394},{\"end\":99628,\"start\":99627},{\"end\":99637,\"start\":99636},{\"end\":99644,\"start\":99643},{\"end\":99651,\"start\":99650},{\"end\":99660,\"start\":99659},{\"end\":99662,\"start\":99661},{\"end\":99885,\"start\":99884},{\"end\":99896,\"start\":99895},{\"end\":99909,\"start\":99908},{\"end\":99918,\"start\":99917},{\"end\":99928,\"start\":99927},{\"end\":100096,\"start\":100095},{\"end\":100105,\"start\":100104},{\"end\":100114,\"start\":100113},{\"end\":100116,\"start\":100115},{\"end\":100127,\"start\":100126},{\"end\":100438,\"start\":100437},{\"end\":100445,\"start\":100444},{\"end\":100452,\"start\":100451},{\"end\":100454,\"start\":100453},{\"end\":100460,\"start\":100459},{\"end\":100468,\"start\":100467},{\"end\":100476,\"start\":100475},{\"end\":100720,\"start\":100719},{\"end\":100722,\"start\":100721},{\"end\":100732,\"start\":100731},{\"end\":100734,\"start\":100733},{\"end\":101040,\"start\":101039},{\"end\":101048,\"start\":101047},{\"end\":101057,\"start\":101056},{\"end\":101065,\"start\":101064},{\"end\":101073,\"start\":101072},{\"end\":101080,\"start\":101079},{\"end\":101088,\"start\":101087},{\"end\":101378,\"start\":101377},{\"end\":101386,\"start\":101385},{\"end\":101397,\"start\":101396},{\"end\":101637,\"start\":101636},{\"end\":101646,\"start\":101642},{\"end\":101652,\"start\":101651},{\"end\":101659,\"start\":101658},{\"end\":101671,\"start\":101670},{\"end\":101680,\"start\":101679},{\"end\":101689,\"start\":101688},{\"end\":101974,\"start\":101973},{\"end\":101988,\"start\":101987},{\"end\":101996,\"start\":101995},{\"end\":102002,\"start\":102001},{\"end\":102011,\"start\":102010},{\"end\":102018,\"start\":102017},{\"end\":102027,\"start\":102026},{\"end\":102270,\"start\":102269},{\"end\":102279,\"start\":102278},{\"end\":102281,\"start\":102280},{\"end\":102294,\"start\":102293},{\"end\":102308,\"start\":102307},{\"end\":102320,\"start\":102319},{\"end\":102570,\"start\":102569},{\"end\":102577,\"start\":102576},{\"end\":102586,\"start\":102585},{\"end\":102595,\"start\":102594},{\"end\":102601,\"start\":102600},{\"end\":102610,\"start\":102609},{\"end\":102617,\"start\":102616},{\"end\":102839,\"start\":102838},{\"end\":102847,\"start\":102846},{\"end\":102856,\"start\":102855},{\"end\":102865,\"start\":102864},{\"end\":102873,\"start\":102872},{\"end\":102881,\"start\":102880},{\"end\":102887,\"start\":102886},{\"end\":102894,\"start\":102893},{\"end\":103177,\"start\":103176},{\"end\":103184,\"start\":103183},{\"end\":103191,\"start\":103190},{\"end\":103201,\"start\":103197},{\"end\":103207,\"start\":103206},{\"end\":103217,\"start\":103216},{\"end\":103492,\"start\":103491},{\"end\":103501,\"start\":103500},{\"end\":103507,\"start\":103506},{\"end\":103516,\"start\":103515},{\"end\":103523,\"start\":103522},{\"end\":103525,\"start\":103524},{\"end\":103532,\"start\":103531},{\"end\":103540,\"start\":103539},{\"end\":103813,\"start\":103812},{\"end\":103820,\"start\":103819},{\"end\":103828,\"start\":103827},{\"end\":103837,\"start\":103836},{\"end\":103844,\"start\":103843},{\"end\":103855,\"start\":103854},{\"end\":103863,\"start\":103862},{\"end\":103870,\"start\":103869},{\"end\":103878,\"start\":103877},{\"end\":104146,\"start\":104145},{\"end\":104152,\"start\":104151},{\"end\":104160,\"start\":104159},{\"end\":104168,\"start\":104167},{\"end\":104177,\"start\":104176},{\"end\":104185,\"start\":104184},{\"end\":104191,\"start\":104190},{\"end\":104200,\"start\":104199},{\"end\":104461,\"start\":104460},{\"end\":104472,\"start\":104471},{\"end\":104474,\"start\":104473},{\"end\":104485,\"start\":104484},{\"end\":104493,\"start\":104492},{\"end\":104501,\"start\":104500},{\"end\":104503,\"start\":104502},{\"end\":104729,\"start\":104728},{\"end\":104735,\"start\":104734},{\"end\":104741,\"start\":104740},{\"end\":104748,\"start\":104747},{\"end\":104757,\"start\":104756},{\"end\":104765,\"start\":104764},{\"end\":104970,\"start\":104969},{\"end\":104978,\"start\":104977},{\"end\":104990,\"start\":104989},{\"end\":104999,\"start\":104998},{\"end\":105235,\"start\":105234},{\"end\":105244,\"start\":105243},{\"end\":105252,\"start\":105251},{\"end\":105260,\"start\":105259},{\"end\":105268,\"start\":105267},{\"end\":105277,\"start\":105276},{\"end\":105286,\"start\":105285},{\"end\":105293,\"start\":105292},{\"end\":105301,\"start\":105300},{\"end\":105309,\"start\":105308},{\"end\":105311,\"start\":105310},{\"end\":105690,\"start\":105686},{\"end\":105696,\"start\":105695},{\"end\":105702,\"start\":105701},{\"end\":105714,\"start\":105713},{\"end\":105721,\"start\":105720},{\"end\":105730,\"start\":105729},{\"end\":105739,\"start\":105738},{\"end\":106055,\"start\":106054},{\"end\":106061,\"start\":106060},{\"end\":106063,\"start\":106062},{\"end\":106077,\"start\":106076},{\"end\":106331,\"start\":106330},{\"end\":106339,\"start\":106338},{\"end\":106832,\"start\":106831},{\"end\":106840,\"start\":106839},{\"end\":107027,\"start\":107026},{\"end\":107035,\"start\":107034},{\"end\":107043,\"start\":107042},{\"end\":107276,\"start\":107275},{\"end\":107282,\"start\":107281},{\"end\":107289,\"start\":107288},{\"end\":107298,\"start\":107297},{\"end\":107307,\"start\":107306},{\"end\":107546,\"start\":107545},{\"end\":107554,\"start\":107553},{\"end\":107563,\"start\":107562},{\"end\":107569,\"start\":107568},{\"end\":107577,\"start\":107576},{\"end\":107585,\"start\":107584},{\"end\":107592,\"start\":107591},{\"end\":107600,\"start\":107599},{\"end\":107609,\"start\":107608},{\"end\":107616,\"start\":107615},{\"end\":107929,\"start\":107928},{\"end\":107935,\"start\":107934},{\"end\":107943,\"start\":107942},{\"end\":107945,\"start\":107944},{\"end\":107954,\"start\":107953},{\"end\":107964,\"start\":107963},{\"end\":108204,\"start\":108203},{\"end\":108211,\"start\":108210},{\"end\":108218,\"start\":108217},{\"end\":108224,\"start\":108223},{\"end\":108232,\"start\":108231},{\"end\":108461,\"start\":108457},{\"end\":108470,\"start\":108469},{\"end\":108476,\"start\":108475},{\"end\":108680,\"start\":108679},{\"end\":108689,\"start\":108688},{\"end\":108696,\"start\":108695},{\"end\":108707,\"start\":108706},{\"end\":108709,\"start\":108708},{\"end\":108720,\"start\":108719},{\"end\":108970,\"start\":108969},{\"end\":108978,\"start\":108977},{\"end\":108987,\"start\":108986},{\"end\":108995,\"start\":108994},{\"end\":109003,\"start\":109002},{\"end\":109012,\"start\":109011},{\"end\":109021,\"start\":109020},{\"end\":109029,\"start\":109028},{\"end\":109036,\"start\":109035},{\"end\":109038,\"start\":109037},{\"end\":109047,\"start\":109046},{\"end\":109344,\"start\":109343},{\"end\":109352,\"start\":109351},{\"end\":109360,\"start\":109359},{\"end\":109369,\"start\":109368},{\"end\":109377,\"start\":109376},{\"end\":109386,\"start\":109385},{\"end\":109394,\"start\":109393},{\"end\":109401,\"start\":109400},{\"end\":109697,\"start\":109696},{\"end\":109699,\"start\":109698},{\"end\":109709,\"start\":109708},{\"end\":109723,\"start\":109722},{\"end\":109737,\"start\":109736},{\"end\":109750,\"start\":109749},{\"end\":109752,\"start\":109751},{\"end\":110064,\"start\":110063},{\"end\":110070,\"start\":110069},{\"end\":110076,\"start\":110075},{\"end\":110083,\"start\":110082},{\"end\":110092,\"start\":110091},{\"end\":110100,\"start\":110099},{\"end\":110342,\"start\":110341},{\"end\":110350,\"start\":110349},{\"end\":110356,\"start\":110355},{\"end\":110363,\"start\":110362},{\"end\":110371,\"start\":110370},{\"end\":110377,\"start\":110376},{\"end\":110385,\"start\":110384},{\"end\":110393,\"start\":110392},{\"end\":110400,\"start\":110399},{\"end\":110407,\"start\":110406}]", "bib_author_last_name": "[{\"end\":73268,\"start\":73266},{\"end\":73277,\"start\":73272},{\"end\":73284,\"start\":73281},{\"end\":73291,\"start\":73288},{\"end\":73454,\"start\":73447},{\"end\":73461,\"start\":73458},{\"end\":73468,\"start\":73465},{\"end\":73480,\"start\":73472},{\"end\":73488,\"start\":73484},{\"end\":73500,\"start\":73492},{\"end\":73509,\"start\":73504},{\"end\":73522,\"start\":73513},{\"end\":73536,\"start\":73526},{\"end\":73786,\"start\":73776},{\"end\":73799,\"start\":73790},{\"end\":73811,\"start\":73805},{\"end\":74049,\"start\":74046},{\"end\":74056,\"start\":74053},{\"end\":74064,\"start\":74062},{\"end\":74071,\"start\":74068},{\"end\":74297,\"start\":74294},{\"end\":74309,\"start\":74301},{\"end\":74319,\"start\":74313},{\"end\":74325,\"start\":74323},{\"end\":74331,\"start\":74329},{\"end\":74518,\"start\":74515},{\"end\":74524,\"start\":74522},{\"end\":74533,\"start\":74528},{\"end\":74539,\"start\":74537},{\"end\":74548,\"start\":74543},{\"end\":74554,\"start\":74552},{\"end\":74561,\"start\":74558},{\"end\":74766,\"start\":74762},{\"end\":74773,\"start\":74770},{\"end\":74780,\"start\":74777},{\"end\":74788,\"start\":74784},{\"end\":74796,\"start\":74792},{\"end\":74804,\"start\":74800},{\"end\":75045,\"start\":75040},{\"end\":75051,\"start\":75049},{\"end\":75060,\"start\":75055},{\"end\":75067,\"start\":75064},{\"end\":75074,\"start\":75071},{\"end\":75083,\"start\":75078},{\"end\":75090,\"start\":75087},{\"end\":75096,\"start\":75094},{\"end\":75107,\"start\":75100},{\"end\":75119,\"start\":75111},{\"end\":75351,\"start\":75347},{\"end\":75359,\"start\":75355},{\"end\":75367,\"start\":75363},{\"end\":75588,\"start\":75582},{\"end\":75595,\"start\":75592},{\"end\":75603,\"start\":75599},{\"end\":75619,\"start\":75607},{\"end\":75627,\"start\":75623},{\"end\":75637,\"start\":75631},{\"end\":75650,\"start\":75641},{\"end\":75658,\"start\":75654},{\"end\":75993,\"start\":75989},{\"end\":76003,\"start\":75997},{\"end\":76011,\"start\":76009},{\"end\":76020,\"start\":76015},{\"end\":76287,\"start\":76283},{\"end\":76298,\"start\":76291},{\"end\":76308,\"start\":76302},{\"end\":76321,\"start\":76312},{\"end\":76331,\"start\":76325},{\"end\":76575,\"start\":76571},{\"end\":76583,\"start\":76579},{\"end\":76596,\"start\":76587},{\"end\":76603,\"start\":76600},{\"end\":76612,\"start\":76607},{\"end\":76622,\"start\":76616},{\"end\":76881,\"start\":76878},{\"end\":76888,\"start\":76885},{\"end\":76895,\"start\":76892},{\"end\":77098,\"start\":77090},{\"end\":77108,\"start\":77102},{\"end\":77347,\"start\":77337},{\"end\":77362,\"start\":77353},{\"end\":77600,\"start\":77596},{\"end\":77611,\"start\":77604},{\"end\":77617,\"start\":77615},{\"end\":77624,\"start\":77621},{\"end\":77635,\"start\":77631},{\"end\":77825,\"start\":77823},{\"end\":77831,\"start\":77829},{\"end\":77839,\"start\":77835},{\"end\":77848,\"start\":77843},{\"end\":77854,\"start\":77852},{\"end\":78054,\"start\":78041},{\"end\":78061,\"start\":78058},{\"end\":78070,\"start\":78065},{\"end\":78076,\"start\":78074},{\"end\":78282,\"start\":78269},{\"end\":78431,\"start\":78428},{\"end\":78448,\"start\":78435},{\"end\":78462,\"start\":78452},{\"end\":78472,\"start\":78468},{\"end\":78670,\"start\":78666},{\"end\":78677,\"start\":78674},{\"end\":78684,\"start\":78681},{\"end\":78692,\"start\":78688},{\"end\":78700,\"start\":78696},{\"end\":78707,\"start\":78704},{\"end\":78937,\"start\":78932},{\"end\":78946,\"start\":78941},{\"end\":78953,\"start\":78950},{\"end\":78961,\"start\":78957},{\"end\":78969,\"start\":78965},{\"end\":78976,\"start\":78973},{\"end\":78985,\"start\":78982},{\"end\":79185,\"start\":79178},{\"end\":79196,\"start\":79189},{\"end\":79206,\"start\":79200},{\"end\":79219,\"start\":79210},{\"end\":79228,\"start\":79223},{\"end\":79239,\"start\":79234},{\"end\":79249,\"start\":79243},{\"end\":79263,\"start\":79253},{\"end\":79511,\"start\":79503},{\"end\":79524,\"start\":79515},{\"end\":79733,\"start\":79729},{\"end\":79741,\"start\":79737},{\"end\":79754,\"start\":79745},{\"end\":79765,\"start\":79758},{\"end\":80002,\"start\":79994},{\"end\":80015,\"start\":80006},{\"end\":80252,\"start\":80249},{\"end\":80259,\"start\":80256},{\"end\":80266,\"start\":80263},{\"end\":80471,\"start\":80467},{\"end\":80479,\"start\":80475},{\"end\":80485,\"start\":80483},{\"end\":80491,\"start\":80489},{\"end\":80707,\"start\":80702},{\"end\":80715,\"start\":80711},{\"end\":80722,\"start\":80719},{\"end\":80728,\"start\":80726},{\"end\":80735,\"start\":80732},{\"end\":80946,\"start\":80943},{\"end\":80954,\"start\":80950},{\"end\":80960,\"start\":80958},{\"end\":80968,\"start\":80964},{\"end\":80974,\"start\":80972},{\"end\":81140,\"start\":81136},{\"end\":81148,\"start\":81144},{\"end\":81161,\"start\":81152},{\"end\":81172,\"start\":81165},{\"end\":81391,\"start\":81387},{\"end\":81398,\"start\":81395},{\"end\":81406,\"start\":81402},{\"end\":81414,\"start\":81410},{\"end\":81421,\"start\":81418},{\"end\":81428,\"start\":81425},{\"end\":81440,\"start\":81432},{\"end\":81448,\"start\":81444},{\"end\":81716,\"start\":81714},{\"end\":81724,\"start\":81720},{\"end\":81732,\"start\":81728},{\"end\":81740,\"start\":81736},{\"end\":81967,\"start\":81963},{\"end\":81976,\"start\":81971},{\"end\":81984,\"start\":81980},{\"end\":81991,\"start\":81988},{\"end\":81999,\"start\":81997},{\"end\":82202,\"start\":82199},{\"end\":82209,\"start\":82206},{\"end\":82218,\"start\":82216},{\"end\":82235,\"start\":82222},{\"end\":82246,\"start\":82239},{\"end\":82253,\"start\":82250},{\"end\":82546,\"start\":82540},{\"end\":82561,\"start\":82552},{\"end\":82846,\"start\":82839},{\"end\":82860,\"start\":82850},{\"end\":82872,\"start\":82864},{\"end\":82885,\"start\":82876},{\"end\":83107,\"start\":83096},{\"end\":83116,\"start\":83111},{\"end\":83130,\"start\":83120},{\"end\":83145,\"start\":83134},{\"end\":83153,\"start\":83149},{\"end\":83168,\"start\":83157},{\"end\":83180,\"start\":83172},{\"end\":83192,\"start\":83184},{\"end\":83203,\"start\":83196},{\"end\":83212,\"start\":83207},{\"end\":83536,\"start\":83533},{\"end\":83545,\"start\":83540},{\"end\":83557,\"start\":83549},{\"end\":83563,\"start\":83561},{\"end\":83570,\"start\":83567},{\"end\":83579,\"start\":83574},{\"end\":83596,\"start\":83583},{\"end\":83812,\"start\":83803},{\"end\":83824,\"start\":83816},{\"end\":83838,\"start\":83828},{\"end\":83855,\"start\":83842},{\"end\":84067,\"start\":84064},{\"end\":84076,\"start\":84071},{\"end\":84083,\"start\":84080},{\"end\":84092,\"start\":84087},{\"end\":84099,\"start\":84096},{\"end\":84105,\"start\":84103},{\"end\":84306,\"start\":84303},{\"end\":84314,\"start\":84310},{\"end\":84322,\"start\":84318},{\"end\":84329,\"start\":84326},{\"end\":84560,\"start\":84556},{\"end\":84567,\"start\":84564},{\"end\":84574,\"start\":84571},{\"end\":84581,\"start\":84578},{\"end\":84588,\"start\":84585},{\"end\":84595,\"start\":84592},{\"end\":84603,\"start\":84599},{\"end\":84609,\"start\":84607},{\"end\":84859,\"start\":84853},{\"end\":84871,\"start\":84863},{\"end\":84881,\"start\":84875},{\"end\":84888,\"start\":84885},{\"end\":84896,\"start\":84892},{\"end\":85149,\"start\":85143},{\"end\":85158,\"start\":85153},{\"end\":85170,\"start\":85162},{\"end\":85181,\"start\":85174},{\"end\":85193,\"start\":85185},{\"end\":85206,\"start\":85197},{\"end\":85420,\"start\":85416},{\"end\":85432,\"start\":85424},{\"end\":85441,\"start\":85436},{\"end\":85447,\"start\":85445},{\"end\":85587,\"start\":85584},{\"end\":85595,\"start\":85591},{\"end\":85602,\"start\":85599},{\"end\":85609,\"start\":85606},{\"end\":85618,\"start\":85613},{\"end\":85625,\"start\":85622},{\"end\":85631,\"start\":85629},{\"end\":85833,\"start\":85828},{\"end\":85845,\"start\":85837},{\"end\":85856,\"start\":85849},{\"end\":85863,\"start\":85860},{\"end\":85872,\"start\":85867},{\"end\":85882,\"start\":85876},{\"end\":86099,\"start\":86090},{\"end\":86107,\"start\":86103},{\"end\":86120,\"start\":86111},{\"end\":86382,\"start\":86379},{\"end\":86390,\"start\":86386},{\"end\":86396,\"start\":86394},{\"end\":86404,\"start\":86400},{\"end\":86413,\"start\":86408},{\"end\":86420,\"start\":86417},{\"end\":86427,\"start\":86424},{\"end\":86670,\"start\":86666},{\"end\":86682,\"start\":86677},{\"end\":86690,\"start\":86686},{\"end\":86696,\"start\":86694},{\"end\":86705,\"start\":86700},{\"end\":86712,\"start\":86709},{\"end\":86720,\"start\":86716},{\"end\":86728,\"start\":86724},{\"end\":86736,\"start\":86732},{\"end\":86742,\"start\":86740},{\"end\":87058,\"start\":87051},{\"end\":87070,\"start\":87062},{\"end\":87079,\"start\":87074},{\"end\":87088,\"start\":87083},{\"end\":87097,\"start\":87092},{\"end\":87114,\"start\":87101},{\"end\":87125,\"start\":87118},{\"end\":87140,\"start\":87131},{\"end\":87538,\"start\":87534},{\"end\":87545,\"start\":87542},{\"end\":87551,\"start\":87549},{\"end\":87743,\"start\":87741},{\"end\":87751,\"start\":87747},{\"end\":87758,\"start\":87755},{\"end\":87764,\"start\":87762},{\"end\":87774,\"start\":87768},{\"end\":87786,\"start\":87778},{\"end\":88046,\"start\":88042},{\"end\":88054,\"start\":88050},{\"end\":88062,\"start\":88058},{\"end\":88070,\"start\":88066},{\"end\":88345,\"start\":88332},{\"end\":88351,\"start\":88349},{\"end\":88357,\"start\":88355},{\"end\":88586,\"start\":88582},{\"end\":88595,\"start\":88590},{\"end\":88604,\"start\":88599},{\"end\":88610,\"start\":88608},{\"end\":88618,\"start\":88614},{\"end\":88625,\"start\":88622},{\"end\":88632,\"start\":88629},{\"end\":88640,\"start\":88636},{\"end\":88892,\"start\":88885},{\"end\":88901,\"start\":88898},{\"end\":88912,\"start\":88905},{\"end\":88922,\"start\":88916},{\"end\":88929,\"start\":88926},{\"end\":88940,\"start\":88933},{\"end\":88950,\"start\":88944},{\"end\":88960,\"start\":88954},{\"end\":88971,\"start\":88964},{\"end\":88980,\"start\":88975},{\"end\":89297,\"start\":89295},{\"end\":89305,\"start\":89301},{\"end\":89318,\"start\":89309},{\"end\":89327,\"start\":89322},{\"end\":89344,\"start\":89331},{\"end\":89350,\"start\":89348},{\"end\":89591,\"start\":89587},{\"end\":89605,\"start\":89595},{\"end\":89616,\"start\":89609},{\"end\":89625,\"start\":89620},{\"end\":89813,\"start\":89805},{\"end\":89825,\"start\":89817},{\"end\":89836,\"start\":89829},{\"end\":89849,\"start\":89840},{\"end\":89857,\"start\":89853},{\"end\":89867,\"start\":89861},{\"end\":89878,\"start\":89871},{\"end\":89887,\"start\":89882},{\"end\":89898,\"start\":89891},{\"end\":89915,\"start\":89902},{\"end\":90268,\"start\":90264},{\"end\":90275,\"start\":90272},{\"end\":90283,\"start\":90279},{\"end\":90293,\"start\":90287},{\"end\":90301,\"start\":90297},{\"end\":90308,\"start\":90305},{\"end\":90320,\"start\":90312},{\"end\":90334,\"start\":90326},{\"end\":90345,\"start\":90338},{\"end\":90352,\"start\":90349},{\"end\":90696,\"start\":90692},{\"end\":90705,\"start\":90700},{\"end\":90713,\"start\":90709},{\"end\":90721,\"start\":90717},{\"end\":90727,\"start\":90725},{\"end\":90735,\"start\":90731},{\"end\":90743,\"start\":90739},{\"end\":90751,\"start\":90747},{\"end\":91059,\"start\":91056},{\"end\":91066,\"start\":91063},{\"end\":91073,\"start\":91070},{\"end\":91081,\"start\":91077},{\"end\":91087,\"start\":91085},{\"end\":91356,\"start\":91354},{\"end\":91364,\"start\":91360},{\"end\":91371,\"start\":91368},{\"end\":91379,\"start\":91375},{\"end\":91385,\"start\":91383},{\"end\":91393,\"start\":91389},{\"end\":91400,\"start\":91397},{\"end\":91408,\"start\":91404},{\"end\":91423,\"start\":91412},{\"end\":91599,\"start\":91597},{\"end\":91607,\"start\":91603},{\"end\":91614,\"start\":91611},{\"end\":91622,\"start\":91618},{\"end\":91629,\"start\":91626},{\"end\":91637,\"start\":91633},{\"end\":91644,\"start\":91641},{\"end\":91653,\"start\":91648},{\"end\":91668,\"start\":91657},{\"end\":91974,\"start\":91970},{\"end\":91981,\"start\":91978},{\"end\":91988,\"start\":91985},{\"end\":91996,\"start\":91992},{\"end\":92004,\"start\":92000},{\"end\":92011,\"start\":92008},{\"end\":92204,\"start\":92200},{\"end\":92210,\"start\":92208},{\"end\":92220,\"start\":92217},{\"end\":92231,\"start\":92224},{\"end\":92245,\"start\":92237},{\"end\":92463,\"start\":92461},{\"end\":92471,\"start\":92467},{\"end\":92479,\"start\":92475},{\"end\":92485,\"start\":92483},{\"end\":92494,\"start\":92489},{\"end\":92502,\"start\":92498},{\"end\":92509,\"start\":92506},{\"end\":92746,\"start\":92744},{\"end\":92753,\"start\":92750},{\"end\":92759,\"start\":92757},{\"end\":92768,\"start\":92763},{\"end\":92778,\"start\":92775},{\"end\":92787,\"start\":92782},{\"end\":92796,\"start\":92791},{\"end\":92806,\"start\":92803},{\"end\":93110,\"start\":93105},{\"end\":93116,\"start\":93114},{\"end\":93124,\"start\":93120},{\"end\":93132,\"start\":93128},{\"end\":93140,\"start\":93136},{\"end\":93147,\"start\":93144},{\"end\":93426,\"start\":93424},{\"end\":93440,\"start\":93435},{\"end\":93449,\"start\":93444},{\"end\":93459,\"start\":93456},{\"end\":93470,\"start\":93466},{\"end\":93737,\"start\":93735},{\"end\":93744,\"start\":93741},{\"end\":93755,\"start\":93748},{\"end\":93768,\"start\":93761},{\"end\":93776,\"start\":93772},{\"end\":94049,\"start\":94045},{\"end\":94058,\"start\":94053},{\"end\":94068,\"start\":94065},{\"end\":94081,\"start\":94072},{\"end\":94094,\"start\":94085},{\"end\":94104,\"start\":94098},{\"end\":94113,\"start\":94108},{\"end\":94122,\"start\":94117},{\"end\":94384,\"start\":94382},{\"end\":94393,\"start\":94388},{\"end\":94402,\"start\":94400},{\"end\":94412,\"start\":94406},{\"end\":94423,\"start\":94418},{\"end\":94656,\"start\":94652},{\"end\":94666,\"start\":94663},{\"end\":94675,\"start\":94670},{\"end\":94688,\"start\":94679},{\"end\":94701,\"start\":94692},{\"end\":94710,\"start\":94705},{\"end\":94720,\"start\":94714},{\"end\":94729,\"start\":94724},{\"end\":95020,\"start\":95013},{\"end\":95036,\"start\":95024},{\"end\":95046,\"start\":95040},{\"end\":95059,\"start\":95050},{\"end\":95281,\"start\":95277},{\"end\":95289,\"start\":95285},{\"end\":95296,\"start\":95293},{\"end\":95305,\"start\":95300},{\"end\":95312,\"start\":95309},{\"end\":95504,\"start\":95500},{\"end\":95516,\"start\":95510},{\"end\":95731,\"start\":95726},{\"end\":95742,\"start\":95735},{\"end\":95937,\"start\":95935},{\"end\":95948,\"start\":95943},{\"end\":95960,\"start\":95954},{\"end\":96101,\"start\":96098},{\"end\":96113,\"start\":96105},{\"end\":96125,\"start\":96117},{\"end\":96134,\"start\":96129},{\"end\":96145,\"start\":96138},{\"end\":96165,\"start\":96149},{\"end\":96174,\"start\":96169},{\"end\":96183,\"start\":96178},{\"end\":96191,\"start\":96187},{\"end\":96201,\"start\":96195},{\"end\":96582,\"start\":96577},{\"end\":96593,\"start\":96588},{\"end\":96606,\"start\":96597},{\"end\":96621,\"start\":96610},{\"end\":96633,\"start\":96625},{\"end\":96640,\"start\":96637},{\"end\":96650,\"start\":96644},{\"end\":96660,\"start\":96654},{\"end\":96672,\"start\":96664},{\"end\":96691,\"start\":96676},{\"end\":96995,\"start\":96990},{\"end\":97006,\"start\":96999},{\"end\":97021,\"start\":97012},{\"end\":97032,\"start\":97025},{\"end\":97043,\"start\":97036},{\"end\":97049,\"start\":97047},{\"end\":97063,\"start\":97053},{\"end\":97072,\"start\":97067},{\"end\":97083,\"start\":97076},{\"end\":97092,\"start\":97087},{\"end\":97414,\"start\":97410},{\"end\":97426,\"start\":97418},{\"end\":97439,\"start\":97430},{\"end\":97446,\"start\":97443},{\"end\":97704,\"start\":97698},{\"end\":97715,\"start\":97710},{\"end\":97723,\"start\":97719},{\"end\":97971,\"start\":97965},{\"end\":97981,\"start\":97975},{\"end\":97992,\"start\":97985},{\"end\":98002,\"start\":97996},{\"end\":98011,\"start\":98006},{\"end\":98223,\"start\":98215},{\"end\":98233,\"start\":98227},{\"end\":98250,\"start\":98237},{\"end\":98261,\"start\":98254},{\"end\":98274,\"start\":98265},{\"end\":98518,\"start\":98510},{\"end\":98528,\"start\":98522},{\"end\":98539,\"start\":98532},{\"end\":98552,\"start\":98543},{\"end\":98819,\"start\":98817},{\"end\":98827,\"start\":98823},{\"end\":98833,\"start\":98831},{\"end\":98839,\"start\":98837},{\"end\":98847,\"start\":98843},{\"end\":98855,\"start\":98851},{\"end\":98863,\"start\":98859},{\"end\":99129,\"start\":99119},{\"end\":99139,\"start\":99133},{\"end\":99372,\"start\":99367},{\"end\":99380,\"start\":99376},{\"end\":99390,\"start\":99384},{\"end\":99398,\"start\":99396},{\"end\":99634,\"start\":99629},{\"end\":99641,\"start\":99638},{\"end\":99648,\"start\":99645},{\"end\":99657,\"start\":99652},{\"end\":99673,\"start\":99663},{\"end\":99893,\"start\":99886},{\"end\":99906,\"start\":99897},{\"end\":99915,\"start\":99910},{\"end\":99925,\"start\":99919},{\"end\":99934,\"start\":99929},{\"end\":100102,\"start\":100097},{\"end\":100111,\"start\":100106},{\"end\":100124,\"start\":100117},{\"end\":100137,\"start\":100128},{\"end\":100442,\"start\":100439},{\"end\":100449,\"start\":100446},{\"end\":100457,\"start\":100455},{\"end\":100465,\"start\":100461},{\"end\":100473,\"start\":100469},{\"end\":100480,\"start\":100477},{\"end\":100729,\"start\":100723},{\"end\":100743,\"start\":100735},{\"end\":101045,\"start\":101041},{\"end\":101054,\"start\":101049},{\"end\":101062,\"start\":101058},{\"end\":101070,\"start\":101066},{\"end\":101077,\"start\":101074},{\"end\":101085,\"start\":101081},{\"end\":101097,\"start\":101089},{\"end\":101383,\"start\":101379},{\"end\":101394,\"start\":101387},{\"end\":101403,\"start\":101398},{\"end\":101640,\"start\":101638},{\"end\":101649,\"start\":101647},{\"end\":101656,\"start\":101653},{\"end\":101668,\"start\":101660},{\"end\":101677,\"start\":101672},{\"end\":101686,\"start\":101681},{\"end\":101703,\"start\":101690},{\"end\":101985,\"start\":101975},{\"end\":101993,\"start\":101989},{\"end\":101999,\"start\":101997},{\"end\":102008,\"start\":102003},{\"end\":102015,\"start\":102012},{\"end\":102024,\"start\":102019},{\"end\":102032,\"start\":102028},{\"end\":102276,\"start\":102271},{\"end\":102291,\"start\":102282},{\"end\":102305,\"start\":102295},{\"end\":102317,\"start\":102309},{\"end\":102334,\"start\":102321},{\"end\":102574,\"start\":102571},{\"end\":102583,\"start\":102578},{\"end\":102592,\"start\":102587},{\"end\":102598,\"start\":102596},{\"end\":102607,\"start\":102602},{\"end\":102614,\"start\":102611},{\"end\":102624,\"start\":102618},{\"end\":102844,\"start\":102840},{\"end\":102853,\"start\":102848},{\"end\":102862,\"start\":102857},{\"end\":102870,\"start\":102866},{\"end\":102878,\"start\":102874},{\"end\":102884,\"start\":102882},{\"end\":102891,\"start\":102888},{\"end\":102899,\"start\":102895},{\"end\":103181,\"start\":103178},{\"end\":103188,\"start\":103185},{\"end\":103195,\"start\":103192},{\"end\":103204,\"start\":103202},{\"end\":103214,\"start\":103208},{\"end\":103231,\"start\":103218},{\"end\":103498,\"start\":103493},{\"end\":103504,\"start\":103502},{\"end\":103513,\"start\":103508},{\"end\":103520,\"start\":103517},{\"end\":103529,\"start\":103526},{\"end\":103537,\"start\":103533},{\"end\":103544,\"start\":103541},{\"end\":103817,\"start\":103814},{\"end\":103825,\"start\":103821},{\"end\":103834,\"start\":103829},{\"end\":103841,\"start\":103838},{\"end\":103852,\"start\":103845},{\"end\":103860,\"start\":103856},{\"end\":103867,\"start\":103864},{\"end\":103875,\"start\":103871},{\"end\":103881,\"start\":103879},{\"end\":104149,\"start\":104147},{\"end\":104157,\"start\":104153},{\"end\":104165,\"start\":104161},{\"end\":104174,\"start\":104169},{\"end\":104182,\"start\":104178},{\"end\":104188,\"start\":104186},{\"end\":104197,\"start\":104192},{\"end\":104205,\"start\":104201},{\"end\":104469,\"start\":104462},{\"end\":104482,\"start\":104475},{\"end\":104490,\"start\":104486},{\"end\":104498,\"start\":104494},{\"end\":104508,\"start\":104504},{\"end\":104732,\"start\":104730},{\"end\":104738,\"start\":104736},{\"end\":104745,\"start\":104742},{\"end\":104754,\"start\":104749},{\"end\":104762,\"start\":104758},{\"end\":104770,\"start\":104766},{\"end\":104975,\"start\":104971},{\"end\":104987,\"start\":104979},{\"end\":104996,\"start\":104991},{\"end\":105008,\"start\":105000},{\"end\":105241,\"start\":105236},{\"end\":105249,\"start\":105245},{\"end\":105257,\"start\":105253},{\"end\":105265,\"start\":105261},{\"end\":105274,\"start\":105269},{\"end\":105283,\"start\":105278},{\"end\":105290,\"start\":105287},{\"end\":105298,\"start\":105294},{\"end\":105306,\"start\":105302},{\"end\":105315,\"start\":105312},{\"end\":105693,\"start\":105691},{\"end\":105699,\"start\":105697},{\"end\":105711,\"start\":105703},{\"end\":105718,\"start\":105715},{\"end\":105727,\"start\":105722},{\"end\":105736,\"start\":105731},{\"end\":105753,\"start\":105740},{\"end\":106058,\"start\":106056},{\"end\":106074,\"start\":106064},{\"end\":106082,\"start\":106078},{\"end\":106336,\"start\":106332},{\"end\":106344,\"start\":106340},{\"end\":106837,\"start\":106833},{\"end\":106846,\"start\":106841},{\"end\":107032,\"start\":107028},{\"end\":107040,\"start\":107036},{\"end\":107047,\"start\":107044},{\"end\":107279,\"start\":107277},{\"end\":107286,\"start\":107283},{\"end\":107295,\"start\":107290},{\"end\":107304,\"start\":107299},{\"end\":107311,\"start\":107308},{\"end\":107551,\"start\":107547},{\"end\":107560,\"start\":107555},{\"end\":107566,\"start\":107564},{\"end\":107574,\"start\":107570},{\"end\":107582,\"start\":107578},{\"end\":107589,\"start\":107586},{\"end\":107597,\"start\":107593},{\"end\":107606,\"start\":107601},{\"end\":107613,\"start\":107610},{\"end\":107621,\"start\":107617},{\"end\":107932,\"start\":107930},{\"end\":107940,\"start\":107936},{\"end\":107951,\"start\":107946},{\"end\":107961,\"start\":107955},{\"end\":107971,\"start\":107965},{\"end\":108208,\"start\":108205},{\"end\":108215,\"start\":108212},{\"end\":108221,\"start\":108219},{\"end\":108229,\"start\":108225},{\"end\":108236,\"start\":108233},{\"end\":108467,\"start\":108462},{\"end\":108473,\"start\":108471},{\"end\":108479,\"start\":108477},{\"end\":108686,\"start\":108681},{\"end\":108693,\"start\":108690},{\"end\":108704,\"start\":108697},{\"end\":108717,\"start\":108710},{\"end\":108723,\"start\":108721},{\"end\":108975,\"start\":108971},{\"end\":108984,\"start\":108979},{\"end\":108992,\"start\":108988},{\"end\":109000,\"start\":108996},{\"end\":109009,\"start\":109004},{\"end\":109018,\"start\":109013},{\"end\":109026,\"start\":109022},{\"end\":109033,\"start\":109030},{\"end\":109042,\"start\":109039},{\"end\":109052,\"start\":109048},{\"end\":109349,\"start\":109345},{\"end\":109357,\"start\":109353},{\"end\":109366,\"start\":109361},{\"end\":109374,\"start\":109370},{\"end\":109383,\"start\":109378},{\"end\":109391,\"start\":109387},{\"end\":109398,\"start\":109395},{\"end\":109406,\"start\":109402},{\"end\":109706,\"start\":109700},{\"end\":109720,\"start\":109710},{\"end\":109734,\"start\":109724},{\"end\":109747,\"start\":109738},{\"end\":109766,\"start\":109753},{\"end\":110067,\"start\":110065},{\"end\":110073,\"start\":110071},{\"end\":110080,\"start\":110077},{\"end\":110089,\"start\":110084},{\"end\":110097,\"start\":110093},{\"end\":110105,\"start\":110101},{\"end\":110347,\"start\":110343},{\"end\":110353,\"start\":110351},{\"end\":110360,\"start\":110357},{\"end\":110368,\"start\":110364},{\"end\":110374,\"start\":110372},{\"end\":110382,\"start\":110378},{\"end\":110390,\"start\":110386},{\"end\":110397,\"start\":110394},{\"end\":110404,\"start\":110401},{\"end\":110412,\"start\":110408}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":206594692},\"end\":73411,\"start\":73218},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":206592484},\"end\":73707,\"start\":73413},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":195908774},\"end\":73978,\"start\":73709},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":235376986},\"end\":74228,\"start\":73980},{\"attributes\":{\"id\":\"b4\"},\"end\":74476,\"start\":74230},{\"attributes\":{\"id\":\"b5\"},\"end\":74689,\"start\":74478},{\"attributes\":{\"doi\":\"arXiv:1907.05642\",\"id\":\"b6\"},\"end\":75001,\"start\":74691},{\"attributes\":{\"id\":\"b7\"},\"end\":75291,\"start\":75003},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":212675968},\"end\":75492,\"start\":75293},{\"attributes\":{\"doi\":\"arXiv:1704.04861\",\"id\":\"b9\"},\"end\":75909,\"start\":75494},{\"attributes\":{\"doi\":\"arXiv:1904.04971\",\"id\":\"b10\"},\"end\":76214,\"start\":75911},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1122604},\"end\":76498,\"start\":76216},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":206596999},\"end\":76800,\"start\":76500},{\"attributes\":{\"id\":\"b13\"},\"end\":77032,\"start\":76802},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":10210242},\"end\":77279,\"start\":77034},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":147618},\"end\":77557,\"start\":77281},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":233444201},\"end\":77761,\"start\":77559},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":52954392},\"end\":77996,\"start\":77763},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":54463801},\"end\":78203,\"start\":77998},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":215548929},\"end\":78399,\"start\":78205},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":2097418},\"end\":78605,\"start\":78401},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":208910380},\"end\":78864,\"start\":78607},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":238634783},\"end\":79147,\"start\":78866},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":13756489},\"end\":79430,\"start\":79149},{\"attributes\":{\"id\":\"b24\"},\"end\":79657,\"start\":79432},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":102350405},\"end\":79922,\"start\":79659},{\"attributes\":{\"doi\":\"arXiv:1406.2199\",\"id\":\"b26\"},\"end\":80183,\"start\":79924},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":85542740},\"end\":80395,\"start\":80185},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":229331798},\"end\":80634,\"start\":80397},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":199472793},\"end\":80886,\"start\":80636},{\"attributes\":{\"id\":\"b30\"},\"end\":81090,\"start\":80888},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":182952746},\"end\":81299,\"start\":81092},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":257364792},\"end\":81633,\"start\":81301},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":219615680},\"end\":81894,\"start\":81635},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":251533468},\"end\":82195,\"start\":81896},{\"attributes\":{\"doi\":\"arXiv:2201.03545\",\"id\":\"b35\"},\"end\":82443,\"start\":82197},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":52967399},\"end\":82774,\"start\":82445},{\"attributes\":{\"id\":\"b37\"},\"end\":83016,\"start\":82776},{\"attributes\":{\"doi\":\"arXiv:2010.11929\",\"id\":\"b38\"},\"end\":83497,\"start\":83018},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":233346705},\"end\":83745,\"start\":83499},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":230799486},\"end\":84012,\"start\":83747},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":247222985},\"end\":84252,\"start\":84014},{\"attributes\":{\"doi\":\"arXiv:2106.08254\",\"id\":\"b42\"},\"end\":84486,\"start\":84254},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":244908953},\"end\":84783,\"start\":84488},{\"attributes\":{\"doi\":\"arXiv:2204.06125\",\"id\":\"b44\"},\"end\":85092,\"start\":84785},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":218889832},\"end\":85385,\"start\":85094},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":4852647},\"end\":85556,\"start\":85387},{\"attributes\":{\"doi\":\"arXiv:2106.13230\",\"id\":\"b47\"},\"end\":85789,\"start\":85558},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":232417054},\"end\":86024,\"start\":85791},{\"attributes\":{\"doi\":\"arXiv:2102.05095\",\"id\":\"b49\"},\"end\":86320,\"start\":86026},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":235485437},\"end\":86600,\"start\":86322},{\"attributes\":{\"doi\":\"arXiv:2305.13292\",\"id\":\"b51\"},\"end\":86973,\"start\":86602},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":235390605},\"end\":87463,\"start\":86975},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":233024948},\"end\":87687,\"start\":87465},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":243985980},\"end\":87940,\"start\":87689},{\"attributes\":{\"doi\":\"arXiv:2203.12602\",\"id\":\"b55\"},\"end\":88280,\"start\":87942},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":248863181},\"end\":88501,\"start\":88282},{\"attributes\":{\"id\":\"b57\"},\"end\":88810,\"start\":88503},{\"attributes\":{\"id\":\"b58\"},\"end\":89228,\"start\":88812},{\"attributes\":{\"doi\":\"arXiv:2205.01917\",\"id\":\"b59\"},\"end\":89554,\"start\":89230},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":235367962},\"end\":89747,\"start\":89556},{\"attributes\":{\"doi\":\"arXiv:2302.05442\",\"id\":\"b61\"},\"end\":90172,\"start\":89749},{\"attributes\":{\"doi\":\"arXiv:2208.10442\",\"id\":\"b62\"},\"end\":90622,\"start\":90174},{\"attributes\":{\"doi\":\"arXiv:2303.16727\",\"id\":\"b63\"},\"end\":90964,\"start\":90624},{\"attributes\":{\"doi\":\"arXiv:2206.13559\",\"id\":\"b64\"},\"end\":91293,\"start\":90966},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":232240418},\"end\":91593,\"start\":91295},{\"attributes\":{\"doi\":\"arXiv:2011.12289\",\"id\":\"b66\"},\"end\":91952,\"start\":91595},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":214612334},\"end\":92135,\"start\":91954},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":21110409},\"end\":92405,\"start\":92137},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":214611786},\"end\":92663,\"start\":92407},{\"attributes\":{\"id\":\"b70\",\"matched_paper_id\":245949719},\"end\":93036,\"start\":92665},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":221006090},\"end\":93327,\"start\":93038},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":215768718},\"end\":93669,\"start\":93329},{\"attributes\":{\"doi\":\"arXiv:1901.10430\",\"id\":\"b73\"},\"end\":93965,\"start\":93671},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":231879989},\"end\":94315,\"start\":93967},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":54434554},\"end\":94580,\"start\":94317},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":220919641},\"end\":94945,\"start\":94582},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":211066440},\"end\":95240,\"start\":94947},{\"attributes\":{\"id\":\"b78\"},\"end\":95434,\"start\":95242},{\"attributes\":{\"id\":\"b79\"},\"end\":95628,\"start\":95436},{\"attributes\":{\"doi\":\"PMLR\",\"id\":\"b80\",\"matched_paper_id\":5808102},\"end\":95908,\"start\":95630},{\"attributes\":{\"doi\":\"arXiv:1607.06450\",\"id\":\"b81\"},\"end\":96094,\"start\":95910},{\"attributes\":{\"doi\":\"arXiv:1705.06950\",\"id\":\"b82\"},\"end\":96483,\"start\":96096},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":834612},\"end\":96957,\"start\":96485},{\"attributes\":{\"doi\":\"arXiv:2006.13256\",\"id\":\"b84\"},\"end\":97317,\"start\":96959},{\"attributes\":{\"id\":\"b85\",\"matched_paper_id\":68049510},\"end\":97622,\"start\":97319},{\"attributes\":{\"doi\":\"arXiv:1212.0402\",\"id\":\"b86\"},\"end\":97902,\"start\":97624},{\"attributes\":{\"id\":\"b87\",\"matched_paper_id\":206769852},\"end\":98178,\"start\":97904},{\"attributes\":{\"doi\":\"arXiv:1808.01340\",\"id\":\"b88\"},\"end\":98451,\"start\":98180},{\"attributes\":{\"doi\":\"arXiv:1907.06987\",\"id\":\"b89\"},\"end\":98733,\"start\":98453},{\"attributes\":{\"doi\":\"arXiv:2211.09552\",\"id\":\"b90\"},\"end\":99076,\"start\":98735},{\"attributes\":{\"doi\":\"arXiv:1711.05101\",\"id\":\"b91\"},\"end\":99281,\"start\":99078},{\"attributes\":{\"id\":\"b92\",\"matched_paper_id\":208006202},\"end\":99588,\"start\":99283},{\"attributes\":{\"id\":\"b93\",\"matched_paper_id\":6773885},\"end\":99823,\"start\":99590},{\"attributes\":{\"id\":\"b94\",\"matched_paper_id\":206593880},\"end\":100093,\"start\":99825},{\"attributes\":{\"doi\":\"arXiv:1710.09412\",\"id\":\"b95\"},\"end\":100348,\"start\":100095},{\"attributes\":{\"id\":\"b96\",\"matched_paper_id\":152282661},\"end\":100662,\"start\":100350},{\"attributes\":{\"id\":\"b97\",\"matched_paper_id\":3548228},\"end\":100958,\"start\":100664},{\"attributes\":{\"id\":\"b98\",\"matched_paper_id\":5711057},\"end\":101302,\"start\":100960},{\"attributes\":{\"id\":\"b99\",\"matched_paper_id\":4539700},\"end\":101552,\"start\":101304},{\"attributes\":{\"id\":\"b100\",\"matched_paper_id\":244799268},\"end\":101906,\"start\":101554},{\"attributes\":{\"id\":\"b101\",\"matched_paper_id\":232307534},\"end\":102216,\"start\":101908},{\"attributes\":{\"id\":\"b102\",\"matched_paper_id\":235390765},\"end\":102521,\"start\":102218},{\"attributes\":{\"id\":\"b103\",\"matched_paper_id\":245877811},\"end\":102777,\"start\":102523},{\"attributes\":{\"id\":\"b104\",\"matched_paper_id\":251040837},\"end\":103108,\"start\":102779},{\"attributes\":{\"id\":\"b105\",\"matched_paper_id\":245218767},\"end\":103413,\"start\":103110},{\"attributes\":{\"doi\":\"arXiv:2112.07175\",\"id\":\"b106\"},\"end\":103761,\"start\":103415},{\"attributes\":{\"id\":\"b107\",\"matched_paper_id\":251403197},\"end\":104069,\"start\":103763},{\"attributes\":{\"id\":\"b108\",\"matched_paper_id\":251320177},\"end\":104405,\"start\":104071},{\"attributes\":{\"id\":\"b109\",\"matched_paper_id\":254366626},\"end\":104661,\"start\":104407},{\"attributes\":{\"id\":\"b110\",\"matched_paper_id\":214794974},\"end\":104926,\"start\":104663},{\"attributes\":{\"id\":\"b111\",\"matched_paper_id\":22044313},\"end\":105138,\"start\":104928},{\"attributes\":{\"doi\":\"arXiv:2106.05058\",\"id\":\"b112\"},\"end\":105586,\"start\":105140},{\"attributes\":{\"id\":\"b113\",\"matched_paper_id\":246064068},\"end\":105976,\"start\":105588},{\"attributes\":{\"id\":\"b114\",\"matched_paper_id\":18180153},\"end\":106251,\"start\":105978},{\"attributes\":{\"id\":\"b115\",\"matched_paper_id\":9328014},\"end\":106773,\"start\":106253},{\"attributes\":{\"id\":\"b116\",\"matched_paper_id\":236950365},\"end\":106967,\"start\":106775},{\"attributes\":{\"doi\":\"arXiv:2109.08472\",\"id\":\"b117\"},\"end\":107206,\"start\":106969},{\"attributes\":{\"id\":\"b118\",\"matched_paper_id\":244954623},\"end\":107482,\"start\":107208},{\"attributes\":{\"doi\":\"arXiv:1710.08011\",\"id\":\"b119\"},\"end\":107865,\"start\":107484},{\"attributes\":{\"id\":\"b120\",\"matched_paper_id\":208291175},\"end\":108130,\"start\":107867},{\"attributes\":{\"id\":\"b121\",\"matched_paper_id\":198179957},\"end\":108392,\"start\":108132},{\"attributes\":{\"id\":\"b122\",\"matched_paper_id\":246867281},\"end\":108629,\"start\":108394},{\"attributes\":{\"id\":\"b123\"},\"end\":108914,\"start\":108631},{\"attributes\":{\"doi\":\"arXiv:2106.06942\",\"id\":\"b124\"},\"end\":109280,\"start\":108916},{\"attributes\":{\"doi\":\"arXiv:2106.13014\",\"id\":\"b125\"},\"end\":109619,\"start\":109282},{\"attributes\":{\"doi\":\"arXiv:1207.0580\",\"id\":\"b126\"},\"end\":109996,\"start\":109621},{\"attributes\":{\"id\":\"b127\",\"matched_paper_id\":214794974},\"end\":110261,\"start\":109998},{\"attributes\":{\"id\":\"b128\"},\"end\":110613,\"start\":110263}]", "bib_title": "[{\"end\":73262,\"start\":73218},{\"end\":73443,\"start\":73413},{\"end\":73772,\"start\":73709},{\"end\":74042,\"start\":73980},{\"end\":75343,\"start\":75293},{\"end\":76279,\"start\":76216},{\"end\":76567,\"start\":76500},{\"end\":77084,\"start\":77034},{\"end\":77331,\"start\":77281},{\"end\":77592,\"start\":77559},{\"end\":77819,\"start\":77763},{\"end\":78037,\"start\":77998},{\"end\":78265,\"start\":78205},{\"end\":78424,\"start\":78401},{\"end\":78662,\"start\":78607},{\"end\":78928,\"start\":78866},{\"end\":79174,\"start\":79149},{\"end\":79725,\"start\":79659},{\"end\":80245,\"start\":80185},{\"end\":80463,\"start\":80397},{\"end\":80698,\"start\":80636},{\"end\":81132,\"start\":81092},{\"end\":81383,\"start\":81301},{\"end\":81710,\"start\":81635},{\"end\":81959,\"start\":81896},{\"end\":82525,\"start\":82445},{\"end\":83529,\"start\":83499},{\"end\":83799,\"start\":83747},{\"end\":84060,\"start\":84014},{\"end\":84552,\"start\":84488},{\"end\":85139,\"start\":85094},{\"end\":85412,\"start\":85387},{\"end\":85824,\"start\":85791},{\"end\":86375,\"start\":86322},{\"end\":87047,\"start\":86975},{\"end\":87530,\"start\":87465},{\"end\":87737,\"start\":87689},{\"end\":88328,\"start\":88282},{\"end\":89583,\"start\":89556},{\"end\":91350,\"start\":91295},{\"end\":91966,\"start\":91954},{\"end\":92196,\"start\":92137},{\"end\":92457,\"start\":92407},{\"end\":92740,\"start\":92665},{\"end\":93098,\"start\":93038},{\"end\":93417,\"start\":93329},{\"end\":94041,\"start\":93967},{\"end\":94378,\"start\":94317},{\"end\":94648,\"start\":94582},{\"end\":95009,\"start\":94947},{\"end\":95273,\"start\":95242},{\"end\":95722,\"start\":95630},{\"end\":96573,\"start\":96485},{\"end\":97406,\"start\":97319},{\"end\":97961,\"start\":97904},{\"end\":99361,\"start\":99283},{\"end\":99625,\"start\":99590},{\"end\":99882,\"start\":99825},{\"end\":100435,\"start\":100350},{\"end\":100717,\"start\":100664},{\"end\":101037,\"start\":100960},{\"end\":101375,\"start\":101304},{\"end\":101634,\"start\":101554},{\"end\":101971,\"start\":101908},{\"end\":102267,\"start\":102218},{\"end\":102567,\"start\":102523},{\"end\":102836,\"start\":102779},{\"end\":103174,\"start\":103110},{\"end\":103810,\"start\":103763},{\"end\":104143,\"start\":104071},{\"end\":104458,\"start\":104407},{\"end\":104726,\"start\":104663},{\"end\":104967,\"start\":104928},{\"end\":105684,\"start\":105588},{\"end\":106052,\"start\":105978},{\"end\":106328,\"start\":106253},{\"end\":106829,\"start\":106775},{\"end\":107273,\"start\":107208},{\"end\":107926,\"start\":107867},{\"end\":108201,\"start\":108132},{\"end\":108455,\"start\":108394},{\"end\":110061,\"start\":109998}]", "bib_author": "[{\"end\":73270,\"start\":73264},{\"end\":73279,\"start\":73270},{\"end\":73286,\"start\":73279},{\"end\":73293,\"start\":73286},{\"end\":73456,\"start\":73445},{\"end\":73463,\"start\":73456},{\"end\":73470,\"start\":73463},{\"end\":73482,\"start\":73470},{\"end\":73490,\"start\":73482},{\"end\":73502,\"start\":73490},{\"end\":73511,\"start\":73502},{\"end\":73524,\"start\":73511},{\"end\":73538,\"start\":73524},{\"end\":73788,\"start\":73774},{\"end\":73801,\"start\":73788},{\"end\":73813,\"start\":73801},{\"end\":74051,\"start\":74044},{\"end\":74058,\"start\":74051},{\"end\":74066,\"start\":74058},{\"end\":74073,\"start\":74066},{\"end\":74299,\"start\":74292},{\"end\":74311,\"start\":74299},{\"end\":74321,\"start\":74311},{\"end\":74327,\"start\":74321},{\"end\":74333,\"start\":74327},{\"end\":74520,\"start\":74513},{\"end\":74526,\"start\":74520},{\"end\":74535,\"start\":74526},{\"end\":74541,\"start\":74535},{\"end\":74550,\"start\":74541},{\"end\":74556,\"start\":74550},{\"end\":74563,\"start\":74556},{\"end\":74768,\"start\":74760},{\"end\":74775,\"start\":74768},{\"end\":74782,\"start\":74775},{\"end\":74790,\"start\":74782},{\"end\":74798,\"start\":74790},{\"end\":74806,\"start\":74798},{\"end\":75047,\"start\":75038},{\"end\":75053,\"start\":75047},{\"end\":75062,\"start\":75053},{\"end\":75069,\"start\":75062},{\"end\":75076,\"start\":75069},{\"end\":75085,\"start\":75076},{\"end\":75092,\"start\":75085},{\"end\":75098,\"start\":75092},{\"end\":75109,\"start\":75098},{\"end\":75121,\"start\":75109},{\"end\":75353,\"start\":75345},{\"end\":75361,\"start\":75353},{\"end\":75369,\"start\":75361},{\"end\":75590,\"start\":75578},{\"end\":75597,\"start\":75590},{\"end\":75605,\"start\":75597},{\"end\":75621,\"start\":75605},{\"end\":75629,\"start\":75621},{\"end\":75639,\"start\":75629},{\"end\":75652,\"start\":75639},{\"end\":75660,\"start\":75652},{\"end\":75995,\"start\":75987},{\"end\":76005,\"start\":75995},{\"end\":76013,\"start\":76005},{\"end\":76022,\"start\":76013},{\"end\":76289,\"start\":76281},{\"end\":76300,\"start\":76289},{\"end\":76310,\"start\":76300},{\"end\":76323,\"start\":76310},{\"end\":76333,\"start\":76323},{\"end\":76577,\"start\":76569},{\"end\":76585,\"start\":76577},{\"end\":76598,\"start\":76585},{\"end\":76605,\"start\":76598},{\"end\":76614,\"start\":76605},{\"end\":76624,\"start\":76614},{\"end\":76883,\"start\":76876},{\"end\":76890,\"start\":76883},{\"end\":76897,\"start\":76890},{\"end\":77100,\"start\":77086},{\"end\":77110,\"start\":77100},{\"end\":77349,\"start\":77333},{\"end\":77364,\"start\":77349},{\"end\":77602,\"start\":77594},{\"end\":77613,\"start\":77602},{\"end\":77619,\"start\":77613},{\"end\":77626,\"start\":77619},{\"end\":77637,\"start\":77626},{\"end\":77827,\"start\":77821},{\"end\":77833,\"start\":77827},{\"end\":77841,\"start\":77833},{\"end\":77850,\"start\":77841},{\"end\":77856,\"start\":77850},{\"end\":78056,\"start\":78039},{\"end\":78063,\"start\":78056},{\"end\":78072,\"start\":78063},{\"end\":78078,\"start\":78072},{\"end\":78284,\"start\":78267},{\"end\":78433,\"start\":78426},{\"end\":78450,\"start\":78433},{\"end\":78464,\"start\":78450},{\"end\":78474,\"start\":78464},{\"end\":78672,\"start\":78664},{\"end\":78679,\"start\":78672},{\"end\":78686,\"start\":78679},{\"end\":78694,\"start\":78686},{\"end\":78702,\"start\":78694},{\"end\":78709,\"start\":78702},{\"end\":78939,\"start\":78930},{\"end\":78948,\"start\":78939},{\"end\":78955,\"start\":78948},{\"end\":78963,\"start\":78955},{\"end\":78971,\"start\":78963},{\"end\":78978,\"start\":78971},{\"end\":78989,\"start\":78978},{\"end\":79187,\"start\":79176},{\"end\":79198,\"start\":79187},{\"end\":79208,\"start\":79198},{\"end\":79221,\"start\":79208},{\"end\":79230,\"start\":79221},{\"end\":79241,\"start\":79230},{\"end\":79251,\"start\":79241},{\"end\":79265,\"start\":79251},{\"end\":79513,\"start\":79501},{\"end\":79526,\"start\":79513},{\"end\":79735,\"start\":79727},{\"end\":79743,\"start\":79735},{\"end\":79756,\"start\":79743},{\"end\":79767,\"start\":79756},{\"end\":80004,\"start\":79992},{\"end\":80017,\"start\":80004},{\"end\":80254,\"start\":80247},{\"end\":80261,\"start\":80254},{\"end\":80268,\"start\":80261},{\"end\":80473,\"start\":80465},{\"end\":80481,\"start\":80473},{\"end\":80487,\"start\":80481},{\"end\":80493,\"start\":80487},{\"end\":80709,\"start\":80700},{\"end\":80717,\"start\":80709},{\"end\":80724,\"start\":80717},{\"end\":80730,\"start\":80724},{\"end\":80737,\"start\":80730},{\"end\":80948,\"start\":80941},{\"end\":80956,\"start\":80948},{\"end\":80962,\"start\":80956},{\"end\":80970,\"start\":80962},{\"end\":80976,\"start\":80970},{\"end\":81142,\"start\":81134},{\"end\":81150,\"start\":81142},{\"end\":81163,\"start\":81150},{\"end\":81174,\"start\":81163},{\"end\":81393,\"start\":81385},{\"end\":81400,\"start\":81393},{\"end\":81408,\"start\":81400},{\"end\":81416,\"start\":81408},{\"end\":81423,\"start\":81416},{\"end\":81430,\"start\":81423},{\"end\":81442,\"start\":81430},{\"end\":81450,\"start\":81442},{\"end\":81718,\"start\":81712},{\"end\":81726,\"start\":81718},{\"end\":81734,\"start\":81726},{\"end\":81742,\"start\":81734},{\"end\":81969,\"start\":81961},{\"end\":81978,\"start\":81969},{\"end\":81986,\"start\":81978},{\"end\":81993,\"start\":81986},{\"end\":82001,\"start\":81993},{\"end\":82204,\"start\":82197},{\"end\":82211,\"start\":82204},{\"end\":82220,\"start\":82211},{\"end\":82237,\"start\":82220},{\"end\":82248,\"start\":82237},{\"end\":82255,\"start\":82248},{\"end\":82533,\"start\":82527},{\"end\":82548,\"start\":82533},{\"end\":82563,\"start\":82548},{\"end\":82848,\"start\":82837},{\"end\":82862,\"start\":82848},{\"end\":82874,\"start\":82862},{\"end\":82887,\"start\":82874},{\"end\":83109,\"start\":83094},{\"end\":83118,\"start\":83109},{\"end\":83132,\"start\":83118},{\"end\":83147,\"start\":83132},{\"end\":83155,\"start\":83147},{\"end\":83170,\"start\":83155},{\"end\":83182,\"start\":83170},{\"end\":83194,\"start\":83182},{\"end\":83205,\"start\":83194},{\"end\":83214,\"start\":83205},{\"end\":83538,\"start\":83531},{\"end\":83547,\"start\":83538},{\"end\":83559,\"start\":83547},{\"end\":83565,\"start\":83559},{\"end\":83572,\"start\":83565},{\"end\":83581,\"start\":83572},{\"end\":83598,\"start\":83581},{\"end\":83814,\"start\":83801},{\"end\":83826,\"start\":83814},{\"end\":83840,\"start\":83826},{\"end\":83857,\"start\":83840},{\"end\":84069,\"start\":84062},{\"end\":84078,\"start\":84069},{\"end\":84085,\"start\":84078},{\"end\":84094,\"start\":84085},{\"end\":84101,\"start\":84094},{\"end\":84107,\"start\":84101},{\"end\":84308,\"start\":84301},{\"end\":84316,\"start\":84308},{\"end\":84324,\"start\":84316},{\"end\":84331,\"start\":84324},{\"end\":84562,\"start\":84554},{\"end\":84569,\"start\":84562},{\"end\":84576,\"start\":84569},{\"end\":84583,\"start\":84576},{\"end\":84590,\"start\":84583},{\"end\":84597,\"start\":84590},{\"end\":84605,\"start\":84597},{\"end\":84611,\"start\":84605},{\"end\":84861,\"start\":84851},{\"end\":84873,\"start\":84861},{\"end\":84883,\"start\":84873},{\"end\":84890,\"start\":84883},{\"end\":84898,\"start\":84890},{\"end\":85151,\"start\":85141},{\"end\":85160,\"start\":85151},{\"end\":85172,\"start\":85160},{\"end\":85183,\"start\":85172},{\"end\":85195,\"start\":85183},{\"end\":85208,\"start\":85195},{\"end\":85422,\"start\":85414},{\"end\":85434,\"start\":85422},{\"end\":85443,\"start\":85434},{\"end\":85449,\"start\":85443},{\"end\":85589,\"start\":85582},{\"end\":85597,\"start\":85589},{\"end\":85604,\"start\":85597},{\"end\":85611,\"start\":85604},{\"end\":85620,\"start\":85611},{\"end\":85627,\"start\":85620},{\"end\":85633,\"start\":85627},{\"end\":85835,\"start\":85826},{\"end\":85847,\"start\":85835},{\"end\":85858,\"start\":85847},{\"end\":85865,\"start\":85858},{\"end\":85874,\"start\":85865},{\"end\":85884,\"start\":85874},{\"end\":86101,\"start\":86088},{\"end\":86109,\"start\":86101},{\"end\":86122,\"start\":86109},{\"end\":86384,\"start\":86377},{\"end\":86392,\"start\":86384},{\"end\":86398,\"start\":86392},{\"end\":86406,\"start\":86398},{\"end\":86415,\"start\":86406},{\"end\":86422,\"start\":86415},{\"end\":86429,\"start\":86422},{\"end\":86672,\"start\":86664},{\"end\":86684,\"start\":86672},{\"end\":86692,\"start\":86684},{\"end\":86698,\"start\":86692},{\"end\":86707,\"start\":86698},{\"end\":86714,\"start\":86707},{\"end\":86722,\"start\":86714},{\"end\":86730,\"start\":86722},{\"end\":86738,\"start\":86730},{\"end\":86744,\"start\":86738},{\"end\":87060,\"start\":87049},{\"end\":87072,\"start\":87060},{\"end\":87081,\"start\":87072},{\"end\":87090,\"start\":87081},{\"end\":87099,\"start\":87090},{\"end\":87116,\"start\":87099},{\"end\":87127,\"start\":87116},{\"end\":87142,\"start\":87127},{\"end\":87540,\"start\":87532},{\"end\":87547,\"start\":87540},{\"end\":87553,\"start\":87547},{\"end\":87745,\"start\":87739},{\"end\":87753,\"start\":87745},{\"end\":87760,\"start\":87753},{\"end\":87766,\"start\":87760},{\"end\":87776,\"start\":87766},{\"end\":87788,\"start\":87776},{\"end\":88048,\"start\":88040},{\"end\":88056,\"start\":88048},{\"end\":88064,\"start\":88056},{\"end\":88072,\"start\":88064},{\"end\":88347,\"start\":88330},{\"end\":88353,\"start\":88347},{\"end\":88359,\"start\":88353},{\"end\":88588,\"start\":88580},{\"end\":88597,\"start\":88588},{\"end\":88606,\"start\":88597},{\"end\":88612,\"start\":88606},{\"end\":88620,\"start\":88612},{\"end\":88627,\"start\":88620},{\"end\":88634,\"start\":88627},{\"end\":88642,\"start\":88634},{\"end\":88894,\"start\":88883},{\"end\":88903,\"start\":88894},{\"end\":88914,\"start\":88903},{\"end\":88924,\"start\":88914},{\"end\":88931,\"start\":88924},{\"end\":88942,\"start\":88931},{\"end\":88952,\"start\":88942},{\"end\":88962,\"start\":88952},{\"end\":88973,\"start\":88962},{\"end\":88982,\"start\":88973},{\"end\":89299,\"start\":89293},{\"end\":89307,\"start\":89299},{\"end\":89320,\"start\":89307},{\"end\":89329,\"start\":89320},{\"end\":89346,\"start\":89329},{\"end\":89352,\"start\":89346},{\"end\":89593,\"start\":89585},{\"end\":89607,\"start\":89593},{\"end\":89618,\"start\":89607},{\"end\":89627,\"start\":89618},{\"end\":89815,\"start\":89803},{\"end\":89827,\"start\":89815},{\"end\":89838,\"start\":89827},{\"end\":89851,\"start\":89838},{\"end\":89859,\"start\":89851},{\"end\":89869,\"start\":89859},{\"end\":89880,\"start\":89869},{\"end\":89889,\"start\":89880},{\"end\":89900,\"start\":89889},{\"end\":89917,\"start\":89900},{\"end\":90270,\"start\":90262},{\"end\":90277,\"start\":90270},{\"end\":90285,\"start\":90277},{\"end\":90295,\"start\":90285},{\"end\":90303,\"start\":90295},{\"end\":90310,\"start\":90303},{\"end\":90322,\"start\":90310},{\"end\":90336,\"start\":90322},{\"end\":90347,\"start\":90336},{\"end\":90354,\"start\":90347},{\"end\":90698,\"start\":90690},{\"end\":90707,\"start\":90698},{\"end\":90715,\"start\":90707},{\"end\":90723,\"start\":90715},{\"end\":90729,\"start\":90723},{\"end\":90737,\"start\":90729},{\"end\":90745,\"start\":90737},{\"end\":90753,\"start\":90745},{\"end\":91061,\"start\":91054},{\"end\":91068,\"start\":91061},{\"end\":91075,\"start\":91068},{\"end\":91083,\"start\":91075},{\"end\":91089,\"start\":91083},{\"end\":91358,\"start\":91352},{\"end\":91366,\"start\":91358},{\"end\":91373,\"start\":91366},{\"end\":91381,\"start\":91373},{\"end\":91387,\"start\":91381},{\"end\":91395,\"start\":91387},{\"end\":91402,\"start\":91395},{\"end\":91410,\"start\":91402},{\"end\":91425,\"start\":91410},{\"end\":91601,\"start\":91595},{\"end\":91609,\"start\":91601},{\"end\":91616,\"start\":91609},{\"end\":91624,\"start\":91616},{\"end\":91631,\"start\":91624},{\"end\":91639,\"start\":91631},{\"end\":91646,\"start\":91639},{\"end\":91655,\"start\":91646},{\"end\":91670,\"start\":91655},{\"end\":91976,\"start\":91968},{\"end\":91983,\"start\":91976},{\"end\":91990,\"start\":91983},{\"end\":91998,\"start\":91990},{\"end\":92006,\"start\":91998},{\"end\":92013,\"start\":92006},{\"end\":92206,\"start\":92198},{\"end\":92212,\"start\":92206},{\"end\":92222,\"start\":92212},{\"end\":92233,\"start\":92222},{\"end\":92247,\"start\":92233},{\"end\":92465,\"start\":92459},{\"end\":92473,\"start\":92465},{\"end\":92481,\"start\":92473},{\"end\":92487,\"start\":92481},{\"end\":92496,\"start\":92487},{\"end\":92504,\"start\":92496},{\"end\":92511,\"start\":92504},{\"end\":92748,\"start\":92742},{\"end\":92755,\"start\":92748},{\"end\":92761,\"start\":92755},{\"end\":92770,\"start\":92761},{\"end\":92780,\"start\":92770},{\"end\":92789,\"start\":92780},{\"end\":92798,\"start\":92789},{\"end\":92808,\"start\":92798},{\"end\":93112,\"start\":93100},{\"end\":93118,\"start\":93112},{\"end\":93126,\"start\":93118},{\"end\":93134,\"start\":93126},{\"end\":93142,\"start\":93134},{\"end\":93149,\"start\":93142},{\"end\":93428,\"start\":93419},{\"end\":93442,\"start\":93428},{\"end\":93451,\"start\":93442},{\"end\":93461,\"start\":93451},{\"end\":93472,\"start\":93461},{\"end\":93739,\"start\":93733},{\"end\":93746,\"start\":93739},{\"end\":93757,\"start\":93746},{\"end\":93770,\"start\":93757},{\"end\":93778,\"start\":93770},{\"end\":94051,\"start\":94043},{\"end\":94060,\"start\":94051},{\"end\":94070,\"start\":94060},{\"end\":94083,\"start\":94070},{\"end\":94096,\"start\":94083},{\"end\":94106,\"start\":94096},{\"end\":94115,\"start\":94106},{\"end\":94124,\"start\":94115},{\"end\":94386,\"start\":94380},{\"end\":94395,\"start\":94386},{\"end\":94404,\"start\":94395},{\"end\":94414,\"start\":94404},{\"end\":94425,\"start\":94414},{\"end\":94658,\"start\":94650},{\"end\":94668,\"start\":94658},{\"end\":94677,\"start\":94668},{\"end\":94690,\"start\":94677},{\"end\":94703,\"start\":94690},{\"end\":94712,\"start\":94703},{\"end\":94722,\"start\":94712},{\"end\":94731,\"start\":94722},{\"end\":95022,\"start\":95011},{\"end\":95038,\"start\":95022},{\"end\":95048,\"start\":95038},{\"end\":95061,\"start\":95048},{\"end\":95283,\"start\":95275},{\"end\":95291,\"start\":95283},{\"end\":95298,\"start\":95291},{\"end\":95307,\"start\":95298},{\"end\":95314,\"start\":95307},{\"end\":95506,\"start\":95498},{\"end\":95518,\"start\":95506},{\"end\":95733,\"start\":95724},{\"end\":95744,\"start\":95733},{\"end\":95939,\"start\":95931},{\"end\":95950,\"start\":95939},{\"end\":95962,\"start\":95950},{\"end\":96103,\"start\":96096},{\"end\":96115,\"start\":96103},{\"end\":96127,\"start\":96115},{\"end\":96136,\"start\":96127},{\"end\":96147,\"start\":96136},{\"end\":96167,\"start\":96147},{\"end\":96176,\"start\":96167},{\"end\":96185,\"start\":96176},{\"end\":96193,\"start\":96185},{\"end\":96203,\"start\":96193},{\"end\":96584,\"start\":96575},{\"end\":96595,\"start\":96584},{\"end\":96608,\"start\":96595},{\"end\":96623,\"start\":96608},{\"end\":96635,\"start\":96623},{\"end\":96642,\"start\":96635},{\"end\":96652,\"start\":96642},{\"end\":96662,\"start\":96652},{\"end\":96674,\"start\":96662},{\"end\":96693,\"start\":96674},{\"end\":96997,\"start\":96988},{\"end\":97008,\"start\":96997},{\"end\":97023,\"start\":97008},{\"end\":97034,\"start\":97023},{\"end\":97045,\"start\":97034},{\"end\":97051,\"start\":97045},{\"end\":97065,\"start\":97051},{\"end\":97074,\"start\":97065},{\"end\":97085,\"start\":97074},{\"end\":97094,\"start\":97085},{\"end\":97416,\"start\":97408},{\"end\":97428,\"start\":97416},{\"end\":97441,\"start\":97428},{\"end\":97448,\"start\":97441},{\"end\":97706,\"start\":97696},{\"end\":97717,\"start\":97706},{\"end\":97725,\"start\":97717},{\"end\":97973,\"start\":97963},{\"end\":97983,\"start\":97973},{\"end\":97994,\"start\":97983},{\"end\":98004,\"start\":97994},{\"end\":98013,\"start\":98004},{\"end\":98225,\"start\":98213},{\"end\":98235,\"start\":98225},{\"end\":98252,\"start\":98235},{\"end\":98263,\"start\":98252},{\"end\":98276,\"start\":98263},{\"end\":98520,\"start\":98508},{\"end\":98530,\"start\":98520},{\"end\":98541,\"start\":98530},{\"end\":98554,\"start\":98541},{\"end\":98821,\"start\":98815},{\"end\":98829,\"start\":98821},{\"end\":98835,\"start\":98829},{\"end\":98841,\"start\":98835},{\"end\":98849,\"start\":98841},{\"end\":98857,\"start\":98849},{\"end\":98865,\"start\":98857},{\"end\":99131,\"start\":99117},{\"end\":99141,\"start\":99131},{\"end\":99374,\"start\":99363},{\"end\":99382,\"start\":99374},{\"end\":99392,\"start\":99382},{\"end\":99400,\"start\":99392},{\"end\":99636,\"start\":99627},{\"end\":99643,\"start\":99636},{\"end\":99650,\"start\":99643},{\"end\":99659,\"start\":99650},{\"end\":99675,\"start\":99659},{\"end\":99895,\"start\":99884},{\"end\":99908,\"start\":99895},{\"end\":99917,\"start\":99908},{\"end\":99927,\"start\":99917},{\"end\":99936,\"start\":99927},{\"end\":100104,\"start\":100095},{\"end\":100113,\"start\":100104},{\"end\":100126,\"start\":100113},{\"end\":100139,\"start\":100126},{\"end\":100444,\"start\":100437},{\"end\":100451,\"start\":100444},{\"end\":100459,\"start\":100451},{\"end\":100467,\"start\":100459},{\"end\":100475,\"start\":100467},{\"end\":100482,\"start\":100475},{\"end\":100731,\"start\":100719},{\"end\":100745,\"start\":100731},{\"end\":101047,\"start\":101039},{\"end\":101056,\"start\":101047},{\"end\":101064,\"start\":101056},{\"end\":101072,\"start\":101064},{\"end\":101079,\"start\":101072},{\"end\":101087,\"start\":101079},{\"end\":101099,\"start\":101087},{\"end\":101385,\"start\":101377},{\"end\":101396,\"start\":101385},{\"end\":101405,\"start\":101396},{\"end\":101642,\"start\":101636},{\"end\":101651,\"start\":101642},{\"end\":101658,\"start\":101651},{\"end\":101670,\"start\":101658},{\"end\":101679,\"start\":101670},{\"end\":101688,\"start\":101679},{\"end\":101705,\"start\":101688},{\"end\":101987,\"start\":101973},{\"end\":101995,\"start\":101987},{\"end\":102001,\"start\":101995},{\"end\":102010,\"start\":102001},{\"end\":102017,\"start\":102010},{\"end\":102026,\"start\":102017},{\"end\":102034,\"start\":102026},{\"end\":102278,\"start\":102269},{\"end\":102293,\"start\":102278},{\"end\":102307,\"start\":102293},{\"end\":102319,\"start\":102307},{\"end\":102336,\"start\":102319},{\"end\":102576,\"start\":102569},{\"end\":102585,\"start\":102576},{\"end\":102594,\"start\":102585},{\"end\":102600,\"start\":102594},{\"end\":102609,\"start\":102600},{\"end\":102616,\"start\":102609},{\"end\":102626,\"start\":102616},{\"end\":102846,\"start\":102838},{\"end\":102855,\"start\":102846},{\"end\":102864,\"start\":102855},{\"end\":102872,\"start\":102864},{\"end\":102880,\"start\":102872},{\"end\":102886,\"start\":102880},{\"end\":102893,\"start\":102886},{\"end\":102901,\"start\":102893},{\"end\":103183,\"start\":103176},{\"end\":103190,\"start\":103183},{\"end\":103197,\"start\":103190},{\"end\":103206,\"start\":103197},{\"end\":103216,\"start\":103206},{\"end\":103233,\"start\":103216},{\"end\":103500,\"start\":103491},{\"end\":103506,\"start\":103500},{\"end\":103515,\"start\":103506},{\"end\":103522,\"start\":103515},{\"end\":103531,\"start\":103522},{\"end\":103539,\"start\":103531},{\"end\":103546,\"start\":103539},{\"end\":103819,\"start\":103812},{\"end\":103827,\"start\":103819},{\"end\":103836,\"start\":103827},{\"end\":103843,\"start\":103836},{\"end\":103854,\"start\":103843},{\"end\":103862,\"start\":103854},{\"end\":103869,\"start\":103862},{\"end\":103877,\"start\":103869},{\"end\":103883,\"start\":103877},{\"end\":104151,\"start\":104145},{\"end\":104159,\"start\":104151},{\"end\":104167,\"start\":104159},{\"end\":104176,\"start\":104167},{\"end\":104184,\"start\":104176},{\"end\":104190,\"start\":104184},{\"end\":104199,\"start\":104190},{\"end\":104207,\"start\":104199},{\"end\":104471,\"start\":104460},{\"end\":104484,\"start\":104471},{\"end\":104492,\"start\":104484},{\"end\":104500,\"start\":104492},{\"end\":104510,\"start\":104500},{\"end\":104734,\"start\":104728},{\"end\":104740,\"start\":104734},{\"end\":104747,\"start\":104740},{\"end\":104756,\"start\":104747},{\"end\":104764,\"start\":104756},{\"end\":104772,\"start\":104764},{\"end\":104977,\"start\":104969},{\"end\":104989,\"start\":104977},{\"end\":104998,\"start\":104989},{\"end\":105010,\"start\":104998},{\"end\":105243,\"start\":105234},{\"end\":105251,\"start\":105243},{\"end\":105259,\"start\":105251},{\"end\":105267,\"start\":105259},{\"end\":105276,\"start\":105267},{\"end\":105285,\"start\":105276},{\"end\":105292,\"start\":105285},{\"end\":105300,\"start\":105292},{\"end\":105308,\"start\":105300},{\"end\":105319,\"start\":105308},{\"end\":105695,\"start\":105686},{\"end\":105701,\"start\":105695},{\"end\":105713,\"start\":105701},{\"end\":105720,\"start\":105713},{\"end\":105729,\"start\":105720},{\"end\":105738,\"start\":105729},{\"end\":105755,\"start\":105738},{\"end\":106060,\"start\":106054},{\"end\":106076,\"start\":106060},{\"end\":106084,\"start\":106076},{\"end\":106338,\"start\":106330},{\"end\":106346,\"start\":106338},{\"end\":106839,\"start\":106831},{\"end\":106848,\"start\":106839},{\"end\":107034,\"start\":107026},{\"end\":107042,\"start\":107034},{\"end\":107049,\"start\":107042},{\"end\":107281,\"start\":107275},{\"end\":107288,\"start\":107281},{\"end\":107297,\"start\":107288},{\"end\":107306,\"start\":107297},{\"end\":107313,\"start\":107306},{\"end\":107553,\"start\":107545},{\"end\":107562,\"start\":107553},{\"end\":107568,\"start\":107562},{\"end\":107576,\"start\":107568},{\"end\":107584,\"start\":107576},{\"end\":107591,\"start\":107584},{\"end\":107599,\"start\":107591},{\"end\":107608,\"start\":107599},{\"end\":107615,\"start\":107608},{\"end\":107623,\"start\":107615},{\"end\":107934,\"start\":107928},{\"end\":107942,\"start\":107934},{\"end\":107953,\"start\":107942},{\"end\":107963,\"start\":107953},{\"end\":107973,\"start\":107963},{\"end\":108210,\"start\":108203},{\"end\":108217,\"start\":108210},{\"end\":108223,\"start\":108217},{\"end\":108231,\"start\":108223},{\"end\":108238,\"start\":108231},{\"end\":108469,\"start\":108457},{\"end\":108475,\"start\":108469},{\"end\":108481,\"start\":108475},{\"end\":108688,\"start\":108679},{\"end\":108695,\"start\":108688},{\"end\":108706,\"start\":108695},{\"end\":108719,\"start\":108706},{\"end\":108725,\"start\":108719},{\"end\":108977,\"start\":108969},{\"end\":108986,\"start\":108977},{\"end\":108994,\"start\":108986},{\"end\":109002,\"start\":108994},{\"end\":109011,\"start\":109002},{\"end\":109020,\"start\":109011},{\"end\":109028,\"start\":109020},{\"end\":109035,\"start\":109028},{\"end\":109046,\"start\":109035},{\"end\":109054,\"start\":109046},{\"end\":109351,\"start\":109343},{\"end\":109359,\"start\":109351},{\"end\":109368,\"start\":109359},{\"end\":109376,\"start\":109368},{\"end\":109385,\"start\":109376},{\"end\":109393,\"start\":109385},{\"end\":109400,\"start\":109393},{\"end\":109408,\"start\":109400},{\"end\":109708,\"start\":109696},{\"end\":109722,\"start\":109708},{\"end\":109736,\"start\":109722},{\"end\":109749,\"start\":109736},{\"end\":109768,\"start\":109749},{\"end\":110069,\"start\":110063},{\"end\":110075,\"start\":110069},{\"end\":110082,\"start\":110075},{\"end\":110091,\"start\":110082},{\"end\":110099,\"start\":110091},{\"end\":110107,\"start\":110099},{\"end\":110349,\"start\":110341},{\"end\":110355,\"start\":110349},{\"end\":110362,\"start\":110355},{\"end\":110370,\"start\":110362},{\"end\":110376,\"start\":110370},{\"end\":110384,\"start\":110376},{\"end\":110392,\"start\":110384},{\"end\":110399,\"start\":110392},{\"end\":110406,\"start\":110399},{\"end\":110414,\"start\":110406}]", "bib_venue": "[{\"end\":73297,\"start\":73293},{\"end\":73542,\"start\":73538},{\"end\":73820,\"start\":73813},{\"end\":74080,\"start\":74073},{\"end\":74290,\"start\":74230},{\"end\":74511,\"start\":74478},{\"end\":74758,\"start\":74691},{\"end\":75036,\"start\":75003},{\"end\":75373,\"start\":75369},{\"end\":75576,\"start\":75494},{\"end\":75985,\"start\":75911},{\"end\":76337,\"start\":76333},{\"end\":76631,\"start\":76624},{\"end\":76874,\"start\":76802},{\"end\":77133,\"start\":77110},{\"end\":77393,\"start\":77364},{\"end\":77641,\"start\":77637},{\"end\":77863,\"start\":77856},{\"end\":78082,\"start\":78078},{\"end\":78288,\"start\":78284},{\"end\":78481,\"start\":78474},{\"end\":78713,\"start\":78709},{\"end\":78993,\"start\":78989},{\"end\":79272,\"start\":79265},{\"end\":79499,\"start\":79432},{\"end\":79771,\"start\":79767},{\"end\":79990,\"start\":79924},{\"end\":80272,\"start\":80268},{\"end\":80497,\"start\":80493},{\"end\":80741,\"start\":80737},{\"end\":80939,\"start\":80888},{\"end\":81178,\"start\":81174},{\"end\":81454,\"start\":81450},{\"end\":81746,\"start\":81742},{\"end\":82020,\"start\":82001},{\"end\":82294,\"start\":82271},{\"end\":82587,\"start\":82563},{\"end\":82835,\"start\":82776},{\"end\":83092,\"start\":83018},{\"end\":83602,\"start\":83598},{\"end\":83861,\"start\":83857},{\"end\":84111,\"start\":84107},{\"end\":84299,\"start\":84254},{\"end\":84615,\"start\":84611},{\"end\":84849,\"start\":84785},{\"end\":85212,\"start\":85208},{\"end\":85453,\"start\":85449},{\"end\":85580,\"start\":85558},{\"end\":85888,\"start\":85884},{\"end\":86086,\"start\":86026},{\"end\":86437,\"start\":86429},{\"end\":86662,\"start\":86602},{\"end\":87191,\"start\":87142},{\"end\":87557,\"start\":87553},{\"end\":87792,\"start\":87788},{\"end\":88038,\"start\":87942},{\"end\":88366,\"start\":88359},{\"end\":88578,\"start\":88503},{\"end\":88881,\"start\":88812},{\"end\":89291,\"start\":89230},{\"end\":89631,\"start\":89627},{\"end\":89801,\"start\":89749},{\"end\":90260,\"start\":90174},{\"end\":90688,\"start\":90624},{\"end\":91052,\"start\":90966},{\"end\":91429,\"start\":91425},{\"end\":91746,\"start\":91686},{\"end\":92017,\"start\":92013},{\"end\":92254,\"start\":92247},{\"end\":92515,\"start\":92511},{\"end\":92839,\"start\":92808},{\"end\":93156,\"start\":93149},{\"end\":93476,\"start\":93472},{\"end\":93731,\"start\":93671},{\"end\":94128,\"start\":94124},{\"end\":94429,\"start\":94425},{\"end\":94735,\"start\":94731},{\"end\":95077,\"start\":95061},{\"end\":95318,\"start\":95314},{\"end\":95496,\"start\":95436},{\"end\":95752,\"start\":95748},{\"end\":95929,\"start\":95910},{\"end\":96258,\"start\":96219},{\"end\":96697,\"start\":96693},{\"end\":96986,\"start\":96959},{\"end\":97452,\"start\":97448},{\"end\":97694,\"start\":97624},{\"end\":98017,\"start\":98013},{\"end\":98211,\"start\":98180},{\"end\":98506,\"start\":98453},{\"end\":98813,\"start\":98735},{\"end\":99115,\"start\":99078},{\"end\":99420,\"start\":99400},{\"end\":99679,\"start\":99675},{\"end\":99940,\"start\":99936},{\"end\":100196,\"start\":100155},{\"end\":100486,\"start\":100482},{\"end\":100785,\"start\":100745},{\"end\":101103,\"start\":101099},{\"end\":101412,\"start\":101405},{\"end\":101709,\"start\":101705},{\"end\":102038,\"start\":102034},{\"end\":102343,\"start\":102336},{\"end\":102630,\"start\":102626},{\"end\":102932,\"start\":102901},{\"end\":103237,\"start\":103233},{\"end\":103489,\"start\":103415},{\"end\":103887,\"start\":103883},{\"end\":104211,\"start\":104207},{\"end\":104514,\"start\":104510},{\"end\":104776,\"start\":104772},{\"end\":105017,\"start\":105010},{\"end\":105232,\"start\":105140},{\"end\":105759,\"start\":105755},{\"end\":106088,\"start\":106084},{\"end\":106436,\"start\":106346},{\"end\":106852,\"start\":106848},{\"end\":107024,\"start\":106969},{\"end\":107317,\"start\":107313},{\"end\":107543,\"start\":107484},{\"end\":107977,\"start\":107973},{\"end\":108242,\"start\":108238},{\"end\":108485,\"start\":108481},{\"end\":108677,\"start\":108631},{\"end\":108967,\"start\":108916},{\"end\":109341,\"start\":109282},{\"end\":109694,\"start\":109621},{\"end\":110111,\"start\":110107},{\"end\":110339,\"start\":110263},{\"end\":82598,\"start\":82589},{\"end\":106455,\"start\":106438}]"}}}, "year": 2023, "month": 12, "day": 17}