{"id": 16406313, "updated": "2023-09-30 09:04:25.217", "metadata": {"title": "Making Deep Neural Networks Robust to Label Noise: a Loss Correction Approach", "authors": "[{\"first\":\"Giorgio\",\"last\":\"Patrini\",\"middle\":[]},{\"first\":\"Alessandro\",\"last\":\"Rozza\",\"middle\":[]},{\"first\":\"Aditya\",\"last\":\"Menon\",\"middle\":[]},{\"first\":\"Richard\",\"last\":\"Nock\",\"middle\":[]},{\"first\":\"Lizhen\",\"last\":\"Qu\",\"middle\":[]}]", "venue": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "arXiv: Machine Learning", "publication_date": {"year": 2016, "month": 9, "day": 13}, "abstract": "We present a theoretically grounded approach to train deep neural networks, including recurrent networks, subject to class-dependent label noise. We propose two procedures for loss correction that are agnostic to both application domain and network architecture. They simply amount to at most a matrix inversion and multiplication, provided that we know the probability of each class being corrupted into another. We further show how one can estimate these probabilities, adapting a recent technique for noise estimation to the multi-class setting, and thus providing an end-to-end framework. Extensive experiments on MNIST, IMDB, CIFAR-10, CIFAR-100 and a large scale dataset of clothing images employing a diversity of architectures --- stacking dense, convolutional, pooling, dropout, batch normalization, word embedding, LSTM and residual layers --- demonstrate the noise robustness of our proposals. Incidentally, we also prove that, when ReLU is the only non-linearity, the loss curvature is immune to class-dependent label noise.", "fields_of_study": "[\"Mathematics\",\"Computer Science\"]", "external_ids": {"arxiv": "1609.03683", "mag": "2964292098", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/PatriniRMNQ17", "doi": "10.1109/cvpr.2017.240"}}, "content": {"source": {"pdf_hash": "522eab3846f768db31b7323ae164a16062bb7272", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1609.03683v2.pdf\"]", "oa_url_match": true, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/1609.03683", "status": "GREEN"}}, "grobid": {"id": "903c1b531de1024696e16a9cc4bf538cee6a836e", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/522eab3846f768db31b7323ae164a16062bb7272.txt", "contents": "\nMaking Deep Neural Networks Robust to Label Noise: a Loss Correction Approach\n\n\nGiorgio Patrini \nAustralian National University\n2 Data61, 3 Waynaut\n\nAlessandro Rozza alessandro.rozza@waynaut.com \nAditya Krishna Menon \nAustralian National University\n2 Data61, 3 Waynaut\n\nRichard Nock \nAustralian National University\n2 Data61, 3 Waynaut\n\nUniversity of Sydney\n\n\nLizhen Qu \nAustralian National University\n2 Data61, 3 Waynaut\n\nMaking Deep Neural Networks Robust to Label Noise: a Loss Correction Approach\n\nWe present a theoretically grounded approach to train deep neural networks, including recurrent networks, subject to class-dependent label noise. We propose two procedures for loss correction that are agnostic to both application domain and network architecture. They simply amount to at most a matrix inversion and multiplication, provided that we know the probability of each class being corrupted into another. We further show how one can estimate these probabilities, adapting a recent technique for noise estimation to the multi-class setting, and thus providing an end-to-end framework. Extensive experiments on MNIST, IMDB, CIFAR-10, CIFAR-100 and a large scale dataset of clothing images employing a diversity of architectures -stacking dense, convolutional, pooling, dropout, batch normalization, word embedding, LSTM and residual layers -demonstrate the noise robustness of our proposals. Incidentally, we also prove that, when ReLU is the only non-linearity, the loss curvature is immune to class-dependent label noise.\n\nIntroduction\n\nLarge datasets used in training modern machine learning models, such as deep neural networks, are often affected by label noise. The problem is pervasive for a simple reason: manual expert-labelling of each instance at a large scale is not feasible, and so researchers often resort to cheap but imperfect surrogates. Two such popular surrogates are crowdsourcing using non-expert labellers and -especially for images -the use of search engines to query instances by a keyword, assuming the keyword as a valid label [5,35,3,29,17] Both approaches offer the possibility to scale the acquisition of training labels, but invariably result in the introduction of label noise, which may adversely affect model training.\n\nOur goal is to effectively train deep neural networks with modern architectures under label noise. We do so by marrying two different lines of recent research. The first strand is work on ad-hoc deep architectures tailored to the problem, primarily developed in Computer Vision [27,32,39,42]. While some such approaches have shown good experimental performance on specific domains, they lack a solid theoretical framework and often need a large amount of clean labels to obtain acceptable results -in particular, for pre-training or validating hyper-parameters [42,17,32].\n\nThe second strand is recent Machine Learning research on theoretically grounded means of combating label noise.\n\nIn particular, we are interested in the design of corrected losses that are robust to label noise [38,28,30]. Despite their formal guarantees, these methods have not been fully appreciated in practice because, crucially, they require noise rates to be known a priori.\n\nAn estimate of the noise is often available to practitioners by polishing a subset of the training data [42] -which is useful and often necessary for model selection. Yet, interestingly, recent work has provided practical algorithms for estimating the noise rates [36,34,21,26,31]; remarkably, this is achievable with absolutely no knowledge of ground truth labels. To our knowledge, no prior work has combined those estimators with loss correction techniques, nor has either idea been applied to modern deep architectures. Our contributions aim to unify these research streams:\n\n\u2022 We introduce two alternative procedures for loss correction, provided that we know a stochastic matrix T summarizing the probability of one class being flipped into another under noise. The first procedure, a multiclass extension of [28,30] applied to neural networks, is called \"backward\" as it multiplies the loss by T \u22121 . The second, inspired by [39], is named \"forward\" as it multiplies the network predictions by T .\n\n\u2022 We prove that both procedures enjoy formal robustness guarantees w.r.t. the clean data distribution. Since we only operate on the loss function, the approach is both architecture and application domain independent, as well as viable for any chosen loss function.\n\n\u2022 We take a further step and extend the noise estimator of [26] to our multi-class setting, thus formulating an end-to-end solution to the problem.\n\n\u2022 We prove that for ReLU networks the Hessian of the loss is independent from label noise.\n\nWe apply our loss corrections to image recognition on MNIST, CIFAR-10, CIFAR-100 and sentiment analysis on IMDB; we simulate corruption by artificially injecting noise on the training labels. In order to show that no architectural choice is the secret ingredient of our robustification recipe, we experiment with a variety of network modules currently in fashion: convolutions and pooling [20], dropout [37], batch normalization [15], word embedding and residual units [11,12]. Additional tests on LSTM [13] confirm that the procedures can be seamlessly applied to recurrent neural networks as well. Comparisons with non-corrected losses and several known methods confirm robustness of our two procedures, with the forward correction dominating the backward. Unsurprisingly, the noise estimator is the bottleneck in obtaining near-perfect robustness, yet in most experiments our approach is often the best compared to prior work. Finally, we experiment with Clothing1M, the 1M clothing images dataset of [42], and establish the new state of the art.\n\n\nRelated work\n\nOur work leverages recent research in a number of different areas, summarized below.\n\nNoise robustness 1 . Learning with noisy labels has been widely investigated in the literature [7]. From the theoretical standpoint label noise has been studied in two different regimes, with vastly different conclusions. In the case of low-capacity (typically linear) models, even mild symmetric, i.e. class-independent (versus asymmetric, i.e. classdependent), label noise can produce solutions that are akin to random guessing [22]. On the other hand, the Bayes-optimal classifier remains unchanged under symmetric [28,26] and even instance dependent label noise [25] implying that highcapacity models are robust to essentially any level of such noise, given sufficiently many samples.\n\nSurrogate losses. Suppose one wishes to minimize a loss on clean data. When the level of noise is known a priori, [28] provided the general form of a noise corrected loss\u02c6 such that minimization of\u02c6 on noisy data is equivalent to minimization of on clean data. In the idealized case of symmetric label noise, for certain one in fact does not need to know the noise rate: [8] gives a sufficient condition for which is robust, and several examples of such robust nonconvex losses, while [41] shows that the (convex) linear or unhinged loss is its own noise-corrected loss. Another robust non-convex loss is given in [24].\n\nNoise rate estimation. Recent work has provided methods to estimate label flip probabilities directly from noisy samples. Typically, it is required that the generating distribution is such that for each class, there exists some \"perfect\" instance, i.e. one that is classified with probability equal to one. Proposed estimators involve either the use of kernel mean embedding [31], or post-processing the output of a standard class-probability estimator such as logistic regression using order statistics on the range of scores [21,26] or the slope of the induced ROC curve [34].\n\nDeep learning with noisy labels. Several works in Deep Learning have attempted to deal with noisy labels of late, especially in Computer Vision. This is often achieved by formulating noise-aware models. [27] builds a noise model for binary classification of aerial image patches, which can handle omission and wrong location of training labels. [42] constructs a more sophisticated mix of symmetric, asymmetric and instance-dependent noise; two networks are learned by EM as models for classifier and noise type. It is often the case that a small set of clean labels is needed in order either to pre-train or fine-tune the model [42,17,32].\n\nThe work of [39] deserves a particular mention. The method augments the architecture by adding a linear layer on top of the network. Once learned, this layer plays the role of our matrix T . However, learning this architecture appears problematic; heuristics such as trace regularization and a fixed updating schedule for the linear layer are necessary. We sidestep those issues by decoupling the two phases: we first estimate T and then learn with loss correction.\n\nWe are not aware of any other attempt at either applying the noise-corrected loss approach of [28] to neural networks, nor on combining those losses with the above noise rate estimators. Our work sits precisely in this intersection. Note that, even though in principle loss correction should not be necessary for high-capacity models like deep neural networks, owing to aforementioned theoretical results, in practice, such correction may offset the sub-optimality of these models arising from training on finite samples. Specifically, we expect that directly optimizing the (corrected) objective we care about will be beneficial in the finite-sample case.\n\n\nPreliminaries\n\nWe begin by fixing notation. We let [c] . = {1, . . . , c} for any c positive integer. Column vectors are written in bold (e.g. v) and matrices in capitals (e.g. V ). Coordinates of a vector are denoted by a subscript (e.g. v j ), while rows and columns of a matrix are denoted e.g. V j\u00b7 and V \u00b7j respectively. We denote the all-ones vector by 1, with size clear from context, and \u2206 c\u22121 \u2282 [0, 1] c the c-dimensional simplex.\n\nIn supervised c-class classification, one has feature space X \u2286 R d and label space Y = {e i : i \u2208 [c]}, where e i denotes the ith standard canonical vector in R c by , i.e. e i \u2208 {0, 1} c , 1 e i = 1. One observes examples (x, y) drawn from an unknown distribution p(x, y) = p(y|x)p(x) over X \u00d7 Y. We denote expectations over p(x, y) by E x,y . Note that each y only has one non-zero value at the coordinate corresponding to the underlying label.\n\nAn n-layer neural network 2 comprises a transformation h : (1) ) is the composition of a number of intermediate transformationsthe layers -defined by:\nX \u2192 R c , where h = (h (n) \u2022 h (n\u22121) \u2022 \u00b7 \u00b7 \u00b7 \u2022 h(\u2200i \u2208 [n \u2212 1]) h (i) (z) = \u03c3(W (i) z + b (i) ) , h (n) (z) = W (i) z + b (i) . where W (i) \u2208 R d (i) \u00d7d (i\u22121) and b (i) \u2208 R d (i)\nare parameters to be estimated 3 , and \u03c3 is any activation function that acts coordinate-wise, such as the ReLU \u03c3(x) i = max(0, x i ).\n\nObserve that the final layer applies a linear projection, unlike all preceding layers. To simplify notation, we write:\n(\u2200i \u2208 [n]) x (i) . = h (i) (x (i\u22121) ),\nwith the base case x (0) . = x, so that e.g.\n\nx (1) is exactly the representation in the first layer. The coordinates of h(x) represent the relative weights that the model assigns to each class i = 1, . . . , c to be predicted. The predicted label is thus given by arg max i\u2208[c] h i (x). In the training phase, the output of the final layer is contrasted with the true label y via two steps. First, h(\u00b7) passes through the softmax function e hi(x) / c k=1 e h k (x) . The softmax output can be interpreted as a vector approximating the class-conditional probabilities p(y|x); we denote it byp(y|x) \u2208 \u2206 c\u22121 . Next, we measure the discrepancy between label y = e i and network output by a loss function : Y \u00d7 \u2206 c\u22121 \u2192 R, for example by means of cross-entropy:\n(e i ,p(y|x)) = \u2212(e i ) logp(y|x) = \u2212logp(y = e i |x) .(1)\nWith some abuse of notation, we also define a loss in vector form : \u2206 c\u22121 \u2192 R c , computed on every possible label:\n(p(y|x)) = (e 1 ,p(y|x)), . . . , (e c ,p(y|x)) \u2208 R c .(2)\nIn the following, formal results hold under very mild conditions on a generic loss function ; at times we provide examples for the cross-entropy. For simplicity, one could think of cross-entropy every time is mentioned.\n\n\nLabel noise and loss robustness\n\nWe now consider label noise. We assume the asymmetric, i.e. class-conditional noise setting [28], where each label y in the training set is flipped to\u1ef9 \u2208 Y with probability p(\u1ef9|y); feature vectors are untouched. Thus, we observe samples from a distribution p(x,\u1ef9) = y p(\u1ef9|y)p(y|x)p(x). Denote by T \u2208 [0, 1] c\u00d7c the noise transition matrix specifying the probability of one label being flipped to another, so that \u2200i, j T ij = p(\u1ef9 = e j |y = e i ). The matrix is row-stochastic and not necessarily symmetric across the classes. This is an approximation of real-world corruption which can still be useful in certain scenarios. One such case is that of classes representing a fine-grained hierarchy of concepts, for example dog breeds and bird species [17] or narrow categories of clothing [42]. Classes may be too similar between each other for non-expert human labellers to distinguish, regardless of the specific instances. Little is known about learning under the more generic feature dependent noise, with few exceptions [42,8,25].\n\nWe aim to modify a loss so as to make it robust to asymmetric label noise; in fact, this is possible if T is known. Under this assumption -that we relax later on -we introduce two alternative corrections inspired by [28] and [39].\n\n\nThe backward correction procedure\n\nWe can build an unbiased estimator of the loss function, such that under expected label noise the corrected loss equals the original one computed on clean data. This property is stated in the next Theorem, a multi-class generalization of [28,Theorem 1]. The Theorem is also a particular instance of the more abstract [40, Theorem 3.2].\n\nTheorem 1 Suppose that the noise matrix T is non-singular. Given a loss , backward corrected loss is defined as:\n\u2190 (p(y|x)) = T \u22121 (p(y|x)) .\nThen, the loss correction is unbiased, i.e. :\n\u2200x, E\u1ef9 |x \u2190 (y,p(y|x)) = E y|x (y,p(y|x)) ,\nand therefore the minimizers are the same:\nargmin p(y|x) E x,\u1ef9 \u2190 (y,p(y|x)) = argmin p(y|x) E x,y (y,p(y|x)) . Proof. E\u1ef9 |x \u2190 (p(y|x)) = E y|x T \u2190 (p(y|x)) = E y|x T T \u22121 (p(y|x)) = E y|x (p(y|x)) .\nThe corrected loss is effectively a linear combination of the loss values for each observable label, whose coefficients are due to the probability that T \u22121 attributes to each possible true label y, given the observed one\u1ef9. Intuitively, we are \"going one step back\" in the noise process described by the Markov chain T . The corrected loss is differentiablealthough not always non-negative -and can be minimized with any off-the-shelf algorithm for back-propagation. Although in practice T would be invertible almost surely, its condition number may be problematic. A simple solution is to mix T with the identity matrix before inversion; this may be seen as taking a more conservative noise-free prior.\n\n\nThe forward correction procedure\n\nAlternatively, we can correct the model predictions. Following [39], we start by observing that a neural network learned with no loss correction would result in a predictor for noisy labelsp(\u1ef9|x). We can make explicit the dependency on T . For instance, with cross-entropy we have:\n(e i ,p(y|x)) = \u2212logp(\u1ef9 = e i |x) (3) = \u2212log c j=1 p(\u1ef9 = e i |y = e j )p(y = e j |x) (4) = \u2212log c j=1 T jip (y = e j |x) ,(5)\nor in matrix form (p(y|x)) = \u2212log T p(y|x) . This loss compares the noisy label\u1ef9 to averaged noisy prediction corrupted by T . We call this procedure \"forward\" correction. In order to analyze its behavior, we first need to recall definition and properties of a broad family of losses named proper composite [33,Section 4]. Consider a link function \u03c8 : \u2206 c\u22121 \u2192 R c , invertible. Many losses are said to be composite, and denoted by \u03c8 : Y \u00d7 R c \u2192 R, in the sense that they can be expressed by the aid of a link function as\n\u03c8 (y, h(x)) = (y, \u03c8 \u22121 (h(x))) .(6)\nIn the case of cross-entropy, the softmax is the inverse link function. When composite losses are also proper [33], their minimizer assumes the particular shape of the link function applied to the class-conditional probabilities p(y|x):\nargmin h E x,y \u03c8 (y, h(x)) = \u03c8(p(y|x)) .(7)\nCross-entropy and square are examples of proper composite losses. An intriguing robustness property holds for forward correction of proper composite losses.\n\nTheorem 2 Suppose that the noise matrix T is non-singular. Given a proper composite loss \u03c8 , define the forward loss correction as:\n\u2192 \u03c8 (h(x)) = (T \u03c8 \u22121 (h(x))) .\nThen, the minimizer of the corrected loss under the noisy distribution is the same as the minimizer of the original loss under the clean distribution:\nargmin h E x,\u1ef9 \u2192 \u03c8 (y, h(x)) = argmin h E x,y \u03c8 (y, h(x)) .\nProof. First notice that:\n\u2192 \u03c8 (y, h(x)) = (y, T \u03c8 \u22121 (h(x))) = \u03c6 (y, h(x)) (8) where we denote \u03c6 \u22121 = \u03c8 \u22121 \u2022 T . Equivalently, \u03c6 = (T \u22121 )\n\u2022 \u03c8 is invertible by composition of invertible functions, its domain is \u2206 c\u22121 as of \u03c8 and its codomain is R c . The last loss in Equation 8 is therefore proper composite with link \u03c6. Finally, from Equation 7, the loss minimizer over the noisy distribution is\nargmin h E x,\u1ef9 \u03c6 (y, h(x)) = \u03c6(p(\u1ef9|x)) (9) = \u03c8((T \u22121 ) p(\u1ef9|x)) = \u03c8(p(y|x)) ,(10)\nthat proves the Theorem by Equation 7 once again.\n\nRecall thatp(y|x) approximates p(y|x) and thus we can relate to the result by taking any neural network that enough expressive. Although, the property is weaker than unbiasedness of Theorem 1. Robustness applies to the minimizer only, that is, the model learned by forward correction is the minimizer over the clean distribution. Yet, Theorem 2 guarantees noise robustness with no explicit matrix inversion; the \"de-noising\" link function \u03c6 does it behind the scene. This turns out to be an important factor in practice; see below.\n\n\nThe overall algorithm\n\nA limitation of the above procedures is that they require knowing T . In most applications, the matrix T would be unknown and to be estimated. We present here an extension of the recent noise estimator of [21,26] to the multi-class settings. It is derived under two assumptions. in the sense that (\u2203x j \u2208 X ) : p(x j ) > 0 \u2227 p(y = e j |x j ) = 1.\n\n(2) given sufficiently many corrupted samples, h is rich enough to model p(\u1ef9|x) accurately.\n\nIt follows that \u2200i, j \u2208 [c], T ij = p(\u1ef9 = e j |x i ) .\n\nProof. By (2), we can consider p(\u1ef9|x) instead ofp(\u1ef9|x).\n\nFor any j \u2208 [c] and any x \u2208 X , we have that:\np(\u1ef9 = e j |x) = c k=1 p(\u1ef9 = e j |y = e k ) p(y = e k |x) = c k=1\nT kj p(y = e k |x) .\n\nBy (1), when x =x i , p(y = e k |x i ) = 0 for k = i. Rather surprisingly, Theorem 3 tells us that we can estimate each component of matrix T just based on noisy class probability estimates, that is, the output of the softmax of a Algorithm 1 Robust two-stage training Input: the noisy training set S, any loss If T is unknown:\n\nTrain a network h(x) on S with loss Obtain an unlabeled sample X EstimateT by Equations (12)-(13) on X Train the network h(x) on S with loss \u2190 or \u2192 Output: h(\u00b7) network trained with noisy labels. In particular, let X be any set of features vectors. This can be the training set itself, but not necessarily: we do not require this sample to have any label at all and therefore any unlabeled sample from the same distributions can be used as well. We can approximate T with two steps:\nx i = argmax x\u2208X p(\u1ef9 = e i |x)(12)\nT ij =p(\u1ef9 = e j |x i ) .\n\nIn practice, assumption (1) of Theorem 3 might hold true when X is large enough. Assumption (2) of Theorem 3 is more difficult to justify; we require that the network can perfectly model the probability of the noisy labels. Although, in the experiments we can often recover T close to the ground truth and find that small estimation errors have a mild, not catastrophic effect on the quality of the correction.\n\nAlgorithm 1 summarizes the end-to-end approach. If we know T , for example by cleaning manually a subset of training data, we can train with \u2190 or \u2192 . Otherwise, we first have to train the network with on noisy data, and obtain from it estimates of p(\u1ef9|x) for each class via the output of the softmax. After trainingT is computable in O(c 2 \u00b7 |X |). Finally, we re-train with the corrected loss, while potentially utilizing the first network to help initializing the second one.\n\n\nDigression: noise free Hessians via ReLU\n\nWe now present a result of independent interest in the context of label noise. The ReLU activation function appears to be a good fit for an architecture in our noise model, since it brings the particular convenience that the Hessian of the loss does not depend on noise, and hence the local curvature is left unchanged. At the same time, we are assured that backward correction by T -or any arbitrarily bad estimator of the matrix -has no impact on those second order properties of the loss -something that does not hold for the forward correction though. We stress the fact that other activation functions like the sigmoid do not share this guarantee. The proof makes use of the factorization trick due to [30].\n\nTheorem 4 Assume that all activation functions are Re-LUs 4 . Then, the Hessian of does not change under noise. Moreover, the Hessians of \u2190 and are the same for any T . 4 A caveat: must be a linear-odd loss studied in [30]; cross-entropy and loss correction E x,\u1ef9\n\nHessian of E x,\u1ef9 no guarantee unchanged \u2190 T \u22121 \u00b7 unbiased estimator of unchanged \u2192 T \u00b7 same minimizer of no guarantee Table 1: Qualitative comparison of loss corrections.\n\nProof. We give the proof for cross-entropy for simplicity; see [30] for a generalization. When y = e i the loss is:\n\u2212 logp(y = e i |x) i = \u2212log e W (n) i\u00b7 x (n\u22121) +b (n) i c k=1 e W (n) k\u00b7 x (n\u22121) +b (n) k = \u2212W (n) i\u00b7 x (n\u22121) + b (n) i + log c k=1 e W (n) k\u00b7 x (n\u22121) +b (n) k .\nThe only dependence on the true class e i above are the first two terms. The log-partition is independent of the precise class i. Evidently, the noise affects the loss only through W i : those are the only terms in which (y,p(y|x)) and (\u1ef9,p(y|x)) may differ. Therefore we can rewrite the backward corrected loss as:\n\u2190 (e j ,p(y|x)) = T \u22121 (p(y|x)) j (14) = \u2212 T \u22121 W (n) j\u00b7 x (n\u22121) \u2212 T \u22121 b (n) j (15) + log c k=1 e W (n) k\u00b7 x (n\u22121) +b (n) k .(16)\nIn fact, note that T \u22121 does not affect the log-partition function. To see this, let A(x) = log ( c k=1 e W (n) k\u00b7 x (n\u22121) +b (n) k ), with the (vector) log-partition being A(x)1. It follows that its correction is T \u22121 A(x)1 = A(x)1, by left-multiplication of T and because T 1 = 1 since T is row-stochastic. Thus \u2190 (e j , h s (x)) = B(x) + A(x), where B(x) = \u2212(T \u22121 W (n) ) j\u00b7 x (n\u22121) \u2212 (T \u22121 b (n) ) j is a piece-wise linear function of the model parameters, and the log-partition A(x) is non-linear because of the loss and the architecture but does not depend on noise. Since the composition of piece-wise linear function is piece-wise linear, the Hessian of B(x) vanishes, and therefore the Hessian of \u2190 is noise independent for any T . The same holds for (no correction) by taking T = I and hence the Hessians are the same.\n\nTheorem 4 does not provide any assurance on minima: indeed, stationary points may change location due to label noise. What it does guarantee is that the convergence rate of first-order methods is the same: the loss curvature cannot blow up or flat out and instead it is the same point by point in the model space. The Theorem advocates for use of ReLU networks, in line with the recent theoretical breakthrough allowing for deep learning with no local minima [16]. Table  1 summaries the properties of loss correction.\n\nsquare loss are such. At the same time, we could generalize Theorem 4 to any neural network that expresses a piece-wise linear function, including for example max-pooling.\n\n\nExperiments\n\nWe now test the theory on various deep neural networks trained on MNIST [20], IMDB [23], CIFAR-10, CIFAR-100 [18] and Clothing1M [42] so as to stress that our approach is independent on both architecture and data domain.\n\n\nLoss corrections with T known or estimated\n\nWe artificially corrupt labels by a parametric matrix T . The rationale is to mimic some of the structure of real mistakes for similar classes, e.g. CAT \u2192 DOG. Transitions are parameterized by N \u2208 [0, 1] such that ground truth and wrong class have probability respectively of 1 \u2212 N, N . An example of T used for MNIST with N = 0.7 is on the left:  \n\uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0\uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb , \uf8ee \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8ef \uf8f0\uf8f9 \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fa \uf8fb(17)\nCommon to all experiments is what follows. The loss chosen for comparison is cross-entropy. 10% of training data is held out for validation. The loss is evaluated on it during training. With the corrected losses we can validate on noisy data, which is advantageous over other approaches that measure noisy validation accuracy instead. The available standard test sets are used for testing. We use ReLU for all networks and initialize weights prior to ReLUs as in [10], otherwise by uniform sampling in [\u22120.05, 0.05]. The minibatch size is 128. The estimator of T from noisy labels is applied to X being training and validation sets together. In fact, preliminary experiments highlighted that the large size X improve sensibly the approximation of T ; after estimation, we row-normalize the matrix. Following [26], we take a \u03b1-percentile in place of the argmax of Equation 12, and we found \u03b1 = 97% to work well for most experiments; the estimator performs very poorly with CIFAR-100, possibly due the small number of images per class, and we found it is better off computing the argmax instead.\n\nFully connected network on MNIST. In the first set of experiments we consider MNIST. Pixels are normalized in [0, 1]. Noise flips some of the similar digits: 2 \u2192 7, 3 \u2192 8, 5 \u2194 6, 7 \u2192 1; see Equation (17, left). We train an architecture with two dense hidden layers of size 128, with probability 0.5 of dropout. AdaGrad [4] is run for 40 epochs with initial learning rate 0.01 and \u03b4 = 10 \u22126 . We repeat each experiment 5 times to account for noise and weight initialization. It is clear from Figure 1a that, although the model is somewhat robust to mild noise, high level of corruption has a disrupting effect on . Instead, our losses do not witness a drastic drop. WithT estimated performance lays in between, yet it is significantly better than with no correction. An example ofT is in Equation (17, right), with < 10 \u22126 . Word embedding and LSTM on IMDB. We keep only the top 5000 most frequent words in the corpus. Each review is either truncated or padded to be 400-word long. To simulate asymmetric noise in this binary problem, we keep constant noise for the transition 0 \u2192 1 at 5%, while 1 \u2192 0 is parameterized as above; 0/1 are the two review's sentiments. We trained two models inspired by the baselines of [2]. The first maps words into 50-dimensional embeddings, before passing through ReLUs; dropout with probability 0.8 is applied to the embedding output. In the second model the embedding has dimension 256 and it is followed by an LSTM with 512 units and by a last 512-dimensional hidden layer with 0.5 dropout. AdaGrad is run for 50 epochs with the same setup as above; results are averages over 5 runs. Figures 1b-1c display an outcome similar to what previously observed on MNIST, in spite of difference in dataset, number of classes, architecture and structure of T . Noticeably, our approach is effective on recurrent networks as well. Correcting wit\u0125 T is in line with the true T here; we believe this is because estimation is easier on this binary problem.\n\nResidual networks on CIFAR-10 and CIFAR-100. For both datasets we perform per-pixel mean subtraction and data augmentation as in [11], by horizontal random flips and 32\u00d732 random crops after padding with 4 pixels on each side. T for CIFAR-10 is described by: TRUCK \u2192 AUTOMOBILE, BIRD \u2192 AIRPLANE, DEER \u2192 HORSE, CAT \u2194 DOG. In CIFAR-100, the 100 classes are grouped into 20 5-size superclasses, e.g. AQUATIC mammals contain BEAVER, DOLPHIN, OTTER, SEAL and WHALE. Within super-classes, the noise flips each class into the next, circularly.\n\nFor the last experiments we use deep residual networks (ResNet), the CIFAR-10/100 architectures from [11]. In short, residual blocks implements a non-linear operation F (x) in parallel with an identity shortcut: x \u2192 x+F (x). F is as cascade of twice batch normalization \u2192 ReLU \u2192 3 \u00d7 3 convolution, following the \"pre-activation\" recommendation of [12]. Here we experiment with ResNets of depth 14 and 32 (CIFAR-10) and 44 (CIFAR-100). By common practice [14], we run SGD with 0.9 momentum and learning rate 0.01, and divide it by 10 after 40 and 80 epoch (120 in total) for CIFAR-10 and after 80 and 120 (150) for CIFAR-100; weight decay is 10 \u22124 . Training deep ResNets is more time consuming and thus experiments are run only once. Since we use shallower networks than the ones in [11], performance is not comparable with the original work. In figures 1d-1f, forward correction does not suffer any significant loss. Except with the shallowest ResNet, backward correction does not seem to work well in the low noise regime. Finally, noise estimation is particularly difficult on CIFAR-100.\n\n\nComparing with other loss functions\n\nWe now compare with other methods. Data, architectures and artificial noise are the same as above. Additionally, we test the case of symmetric noise where N is the probability of label flip that is spread uniformly among all the other classes. We select methods prescribing changes in the loss function, similarly to ours: unhinged [41], sigmoid [8], Savage [24] and soft and hard bootstrapping [32]; hyper-parameters of the last two methods are set in accordance with their paper.\n\nUnhinged loss is unbounded and cannot be used alone. In the original work L 2 regularization is applied to address the problem, when training non-parametric kernel models. We tried to regularize every layer with little success; learning either does not converge (too little regularization) or converge to very poor solutions (too much). On preliminary experiments sigmoid loss ran into the opposite issue, namely premature saturation; the loss reaches a plateau too quickly, a well-known problem with sigmoidal activation functions [9]. To make those losses usable for comparison, we stack a layer of batch normalization right before the loss function. Essentially, the network outputs are whitened and likely to operate in a bounded, non-saturated area of the loss; note that this is never required for linear or kernel models. Table 2 presents the empirical analysis. We list the key findings: (a) In the absence of artificial noise (first column for each dataset), all losses reach similar accuracies with a spread of 2 points; exceptions are some instances of unhinged, sigmoid and Savage. Additionally, with IMDB there are cases ( \u2020 in Table 2) of loss correction with noise estimation that perform slightly better than assuming no noise. Clearly, the estimator is able to recover the natural noise in the sentiment reviews. (b) With low asymmetric noise (second column) results differ between simple architecture/tasks (datasets on the left) and deep networks/more difficult problems (right); in the former case, the two corrections behave similarly and are not statistically far from the competitors; in the latter case, forward correction with known T is unbeaten, with no clear winner among the remaining ones. (c) With asymmetric noise (last two columns) the two loss corrections with known T are overall the best performing, confirming the practical implications of their formal guarantees; forward is usually the best. (d) If we exclude CIFAR-100, the noise estimation accounts for average accuracy drops between 0 (IMBD with LSTM model) and 27 points (MNIST); nevertheless, our performance is better than every other method in many occasions. (e) In the experiment on CIFAR-100 we obtain essentially perfect noise robustness with the ideal forward correction. The noise estimation works well except in the very last column, yet it guarantees again better accuracy over competing methods. We discuss this issue in Section 6.\n\n\nExperiments on Clothing1M\n\nFinally, we test on Clothing1M [42], consisting of 1M images with noisy labels, with additional 50k, 14k, 10k of clean data respectively for training, validation and testing; we refer to those sets by their size. We aim to classify images within 14 classes, e.g. t-shirt, suit, vest. In the original work two AlexNets [19] are trained together via EM; the networks are pre-trained with ImageNet. Two practical tricks are fundamental: a first learning phase with the clean 50k to help EM (#1 in Table 3) and a second phase with the mix of 50k bootstrapped to 500k and 1M (#3). Data augmentation is also applied, same as in Section 5.1 for CIFAR-10.\n\nWe learn a 50-layer ResNet pre-trained on ImageNetthe bottleneck architecture of [11] -with SGD with learning rate 10 \u22123 and 10 \u22124 for 5 epochs each, 0.9 momentum, and batch size 32. When we train with 50k we use weight decay of 5 \u00b7 10 \u22122 and data augmentation, while with 1M we use only weight decay of 10 \u22123 . The ResNet gives an uplift of about 2.5% by training with 50k only (#7 vs. #1). However, the large amount of noisy images is essential to compete with #3. Instead of estimating the matrix T by (12)-(13), we exploit the curated labels of 50k and their noisy versions in 1M . Forward and backward corrections are confirmed to work better than cross-entropy (#6, #5 vs. #4), yet cannot reach the state of the art without the additional clean data. Thus, we fine tune the networks with 50k, with the same learning parameters as in #7; due to MNIST Table 2: Average accuracy with standard deviation (5 runs, left part) is bold faced when statistically far from the others, by means of passing a Welch's t-test with p-value < 5%; in case the highest accuracy is due to \u2190 or \u2192 with the ground truth T , we denote those by and highlight the next highest accuracy as well. For experiments with no standard deviation (right part), the same rule is applied, but bold face is given to the all accuracies in a range of 0.5 points from the highest. The meaning of N depends on symmetric vs. asymmetric noise and on number of classes (see Section 5.1). On the first columns with no injected noise, \u2020 indicates when the noise estimation recovers some natural noise and beats \"loss correction\" with T = I.  Table 3: Results on the top section are from [42]. In #2, #3 the clean 50k are bootstrapped to 500k. Best result #8 is obtained by fine tuning a net trained with forward correction.\n\nreasons of time we only tune #6. The new state of the art is #8 that outperforms [42] of more than 2 percent, which is achieved without time consuming bootstrapping of the 50k.\n\n\nDiscussion and Conclusion\n\nWe have proposed a framework for training deep neural networks with noisy labels that boils down to two loss corrections. Accuracy is consistently only few percent points away from training cross-entropy on clean data, while corruption can worsen performance of cross-entropy by 40 percent or more. Forward correction often performs better. We believe the reason is not statistical -Theorems 1 and 2 guarantee optimality, in the limit of infinite data. The cause may be either numerical (via matrix inversion) or a drastic change of the loss (in particular its Hessian), which may have a detrimental effect on optimization. Indeed, backward correction is a linear combination of losses for every possible label, with coefficients that can be far by orders of magnitude and thus makes the learning harder. Instead, forward correction projects predictions into a probability distribution in [0, 1].\n\nThe quality of noise estimation is a key factor for obtaining robustness. In practice, it works well in most experiments with a median drop of only 10 points of accuracy with respect to using the true T . The exception is the last column for CIFAR-100, where estimation destroys most of the gain from loss correction. We believe that the mix of high noise and limited number of images per class (500) is detrimental to the estimator. This is confirmed by the sensitivity of \u03b1.\n\nFuture work shall improve the estimation phase by incorporating priors of the noise structure, for example assuming low rank T . Improvements on this direction may also widen the applicability to massively multi-class scenarios. It remains an open question whether instance-dependent noise may be included into our approach [42,25]. Finally, we anticipate the use of our approach as a tool for pre-training models with noisy data from the Web, in the spirit of [17].\n\nTheorem 3\n3Assume p(x, y) is such that: (1) There exist \"perfect examples\" of each of class j \u2208 [c],\n\nFigure 1 :\n1Comparison of cross-entropy with its corrections, with known or estimated T .\n\n\n, fully connected CIFAR-10, 14-layer ResNet NO NOISE SYMM. N = 0.2 ASYMM. N = 0.2 ASYMM. N = 0.6NO NOISE \n\nSYMM. N = 0.2 \nASYMM. N = 0.2 \nASYMM. N = 0.6 \n\ncross-entropy \n97.9 \u00b1 0.0 \n96.9 \u00b1 0.1 \n97.5 \u00b1 0.0 \n53.0 \u00b1 0.6 \n87.8 \n83.7 \n85.0 \n57.6 \nunhinged (BN) \n97.6 \u00b1 0.0 \n96.9 \u00b1 0.1 \n97.0 \u00b1 0.1 \n71.2 \u00b1 1.0 \n86.9 \n84.1 \n83.8 \n52.1 \nsigmoid (BN) \n97.2 \u00b1 0.1 \n93.1 \u00b1 0.2 \n96.7 \u00b1 0.1 \n71.4 \u00b1 1.3 \n76.0 \n66.6 \n71.8 \n57.0 \nSavage \n97.3 \u00b1 0.0 \n96.9 \u00b1 0.0 \n97.0 \u00b1 0.1 \n51.3 \u00b1 0.4 \n80.1 \n77.4 \n76.0 \n50.5 \nbootstrap soft \n97.9 \u00b1 0.0 \n96.9 \u00b1 0.0 \n97.5 \u00b1 0.0 \n53.0 \u00b1 0.4 \n87.7 \n84.3 \n84.6 \n57.8 \nbootstrap hard \n97.9 \u00b1 0.0 \n96.8 \u00b1 0.0 \n97.4 \u00b1 0.0 \n55.0 \u00b1 1.3 \n87.3 \n83.6 \n84.7 \n58.3 \nbackward \n97.9 \u00b1 0.0 \n96.3 \u00b1 0.1 \n96.6 \u00b1 1.1 \n93.0 \u00b1 0.9 \n87.6 \n81.5 \n83.8 \n75.2 \nbackwardT \n97.9 \u00b1 0.0 \n96.9 \u00b1 0.0 \n96.7 \u00b1 0.1 \n67.4 \u00b1 1.5 \n87.7 \n80.4 \n83.8 \n66.7 \nforward \n97.9 \u00b1 0.0 \n97.3 \u00b1 0.0 \n97.7 \u00b1 0.0 \n97.3 \u00b1 0.0 \n87.8 \n85.6 \n86.3 \n84.5 \nforwardT \n97.9 \u00b1 0.0 \n96.9 \u00b1 0.0 \n97.7 \u00b1 0.0 \n64.9 \u00b1 4.4 \n87.4 \n83.4 \n87.0 \n74.8 \n\nIMBD, word embedding \nCIFAR-10, 32-layer ResNet \n\nNO NOISE \n\nSYMM. N = 0.1 \nASYMM. N = 0.1 \nASYMM. N = 0.4 \n\nNO NOISE \n\nSYMM. N = 0.2 \nASYMM. N = 0.2 \nASYMM. N = 0.6 \n\ncross-entropy \n86.7 \u00b1 0.0 \n84.6 \u00b1 0.1 \n85.0 \u00b1 0.2 \n58.1 \u00b1 0.5 \n90.1 \n86.6 \n89.0 \n53.6 \nunhinged (BN) \n83.3 \u00b1 0.0 \n76.9 \u00b1 0.5 \n80.6 \u00b1 0.3 \n72.9 \u00b1 0.4 \n90.2 \n86.5 \n87.1 \n60.0 \nsigmoid (BN) \n84.3 \u00b1 0.0 \n80.2 \u00b1 0.3 \n81.7 \u00b1 0.5 \n72.8 \u00b1 0.6 \n81.6 \n69.6 \n79.1 \n61.8 \nSavage \n86.5 \u00b1 0.0 \n84.3 \u00b1 0.4 \n85.2 \u00b1 0.3 \n58.3 \u00b1 1.0 \n88.3 \n86.2 \n86.3 \n53.5 \nbootstrap soft \n86.7 \u00b1 0.0 \n84.5 \u00b1 0.1 \n85.1 \u00b1 0.1 \n57.8 \u00b1 0.7 \n90.9 \n86.9 \n88.6 \n53.1 \nbootstrap hard \n86.7 \u00b1 0.0 \n84.6 \u00b1 0.3 \n85.1 \u00b1 0.3 \n59.0 \u00b1 0.6 \n90.4 \n86.4 \n88.6 \n54.7 \nbackward \n86.7 \u00b1 0.0 \n85.3 \u00b1 0.3 \n85.7 \u00b1 0.1 \n82.1 \u00b1 0.1 \n90.1 \n83.0 \n84.4 \n74.3 \nbackwardT \n87.0 \u00b1 0.0  \u2020 \n85.1 \u00b1 0.4 \n85.8 \u00b1 0.2 \n77.0 \u00b1 1.4 \n90.8 \n86.9 \n86.4 \n66.7 \nforward \n86.7 \u00b1 0.0 \n85.3 \u00b1 0.2 \n85.9 \u00b1 0.1 \n80.9 \u00b1 1.3 \n91.2 \n87.7 \n89.9 \n87.6 \nforwardT \n87.0 \u00b1 0.0  \u2020 \n85.2 \u00b1 0.3 \n85.9 \u00b1 0.2 \n73.0 \u00b1 1.2 \n90.5 \n87.9 \n90.1 \n77.6 \n\nIMBD, word embedding + LSTM \nCIFAR-100, 44-layer ResNet \n\nNO NOISE \n\nSYMM. N = 0.1 \nASYMM. N = 0.1 \nASYMM. N = 0.4 \n\nNO NOISE \n\nSYMM. N = 0.2 \nASYMM. N = 0.2 \nASYMM. N = 0.6 \n\ncross-entropy \n87.8 \u00b1 0.4 \n85.2 \u00b1 0.5 \n86.8 \u00b1 0.4 \n71.4 \u00b1 1.3 \n68.5 \n57.9 \n63.5 \n17.1 \nunhinged (BN) \n84.3 \u00b1 4.4 \n69.7 \u00b1 15.9 \n85.2 \u00b1 1.2 \n59.4 \u00b1 12.9 \n50.9 \n47.5 \n48.0 \n14.5 \nsigmoid (BN) \n87.7 \u00b1 0.5 \n77.6 \u00b1 13.6 \n86.3 \u00b1 3.1 \n70.0 \u00b1 14.6 \n58.2 \n47.6 \n55.6 \n16.4 \nSavage \n87.4 \u00b1 0.3 \n85.1 \u00b1 0.6 \n87.2 \u00b1 0.3 \n70.4 \u00b1 3.8 \n1.4 \n2.0 \n1.8 \n1.6 \nbootstrap soft \n87.1 \u00b1 0.6 \n83.5 \u00b1 2.5 \n86.1 \u00b1 1.2 \n69.0 \u00b1 5.3 \n67.9 \n57.8 \n63.8 \n16.3 \nbootstrap hard \n86.5 \u00b1 0.5 \n84.3 \u00b1 1.0 \n86.7 \u00b1 0.4 \n71.8 \u00b1 3.3 \n68.5 \n57.3 \n63.9 \n17.0 \nbackward \n87.6 \u00b1 0.2 \n84.3 \u00b1 0.9 \n86.7 \u00b1 0.5 \n83.6 \u00b1 1.4 \n68.5 \n55.1 \n53.8 \n36.8 \nbackwardT \n87.2 \u00b1 0.7 \n82.8 \u00b1 2.7 \n87.3 \u00b1 0.1 \n82.3 \u00b1 1.7 \n68.6 \n51.7 \n63.8 \n18.5 \nforward \n87.5 \u00b1 0.2 \n85.0 \u00b1 0.2 \n87.0 \u00b1 0.4 \n84.7 \u00b1 0.6 \n68.8 \n64.0 \n68.1 \n68.4 \nforwardT \n87.8 \u00b1 1.5  \u2020 \n84.1 \u00b1 1.0 \n87.5 \u00b1 0.2 \n84.2 \u00b1 0.9 \n68.1 \n58.6 \n64.2 \n15.9 \n\n\nWe use the term robustness in its meaning of immunity to noise and not generically as \"adaptivity to various scenarios\", e.g.[6].\nW.l.o.g., we assume all layers to be fully connected, or dense; for example, convolutions can be represented by dense layers with shared sparse weights.3 Here, d (0) = d, the original feature dimensionality, and d (n) = c, the label dimensionality.\nAcknowledgments. We gratefully acknowledge NVIDIA Corporation for the donation of a Tesla K40 GPU. We also thank the developers of Keras[1].\n. F Chollet, F. Chollet. Keras. github.com/fchollet/keras.\n\nSemi-supervised sequence learning. A M Dai, Q V Le, NIPS*29. A. M. Dai and Q. V. Le. Semi-supervised sequence learning. In NIPS*29, 2015.\n\nLearning everything about anything: Webly-supervised visual concept learning. S Divvala, A Farhadi, C Guestrin, 27 th IEEE CVPR. S. Divvala, A. Farhadi, and C. Guestrin. Learning everything about anything: Webly-supervised visual concept learning. In 27 th IEEE CVPR, 2014.\n\nAdaptive subgradient methods for online learning and stochastic optimization. J Duchi, E Hazan, Y Singer, JMLR. 12J. Duchi, E. Hazan, and Y. Singer. Adaptive subgradient meth- ods for online learning and stochastic optimization. JMLR, 12:2121-2159, 2011.\n\nLearning object categories from internet image searches. R Fergus, L Fei-Fei, P Perona, A Zisserman, Proceedings of the IEEE. 988R. Fergus, L. Fei-Fei, P. Perona, and A. Zisserman. Learning object categories from internet image searches. Proceedings of the IEEE, 98(8):1453-1466, 2010.\n\nEfficient and robust automated machine learning. M Feurer, A Klein, K Eggensperger, J Springenberg, M Blum, F Hutter, NIPS*29. M. Feurer, A. Klein, K. Eggensperger, J. Springenberg, M. Blum, and F. Hutter. Efficient and robust automated ma- chine learning. In NIPS*29, 2015.\n\nClassification in the Presence of Label Noise: A Survey. B Fr\u00e9nay, M Verleysen, IEEE Transactions on Neural Networks and Learning Systems. 255B. Fr\u00e9nay and M. Verleysen. Classification in the Presence of Label Noise: A Survey. IEEE Transactions on Neural Networks and Learning Systems, 25(5):845-869, May 2014.\n\nMaking risk minimization tolerant to label noise. A Ghosh, N Manwani, P S Sastry, Neurocomputing. A. Ghosh, N. Manwani, and P. S. Sastry. Making risk mini- mization tolerant to label noise. Neurocomputing, 2015.\n\nUnderstanding the difficulty of training deep feedforward neural networks. X Glorot, Y Bengio, AISTATS. X. Glorot and Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. In AISTATS, 2010.\n\nDelving deep into rectifiers: Surpassing human-level performance on imagenet classification. K He, X Zhang, S Ren, J Sun, ICCV. K. He, X. Zhang, S. Ren, and J. Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In ICCV, 2015.\n\nDeep residual learning for image recognition. K He, X Zhang, S Ren, J Sun, 29 th IEEE CVPR. K. He, X. Zhang., S. Ren, and J. Sun. Deep residual learning for image recognition. In 29 th IEEE CVPR, 2016.\n\nIdentity mappings in deep residual networks. K He, X Zhang, S Ren, J Sun, 14 th ECCV. K. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. In 14 th ECCV, 2016.\n\nLong short-term memory. S Hochreiter, J Schmidhuber, Neural computation. 98S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735-1780, 1997.\n\nDeep networks with stochastic depth. G Huang, Y Sun, Z Liu, D Sedra, K Weinberger, 14 th ECCV. G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Weinberger. Deep networks with stochastic depth. In 14 th ECCV, 2016.\n\nBatch normalization: Accelerating deep network training by reducing internal covariate shift. S Ioffe, C Szegedy, 32 th ICML. S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In 32 th ICML, 2015.\n\nDeep learning without poor local minima. K Kawaguchi, NIPS*30. K. Kawaguchi. Deep learning without poor local minima. In NIPS*30, 2016.\n\nThe unreasonable effectiveness of noisy data for fine-grained recognition. J Krause, B Sapp, A Howard, H Zhou, A Toshev, T Duerig, J Philbin, L Fei-Fei, In 14 th ECCVJ. Krause, B. Sapp, A. Howard, H. Zhou, A. Toshev, T. Duerig, J. Philbin, and L. Fei-Fei. The unreasonable effectiveness of noisy data for fine-grained recognition. In 14 th ECCV, 2016.\n\nLearning multiple layers of features from tiny images. A Krizhevsky, G Hinton, University of TorontoTechnical reportA. Krizhevsky and G. Hinton. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.\n\nImagenet classification with deep convolutional neural networks. A Krizhevsky, I Sutskever, G E Hinton, NIPS*26. A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS*26, 2012.\n\nGradientbased learning applied to document recognition. Y Lecun, L Bottou, Y Bengio, P Haffner, Proceedings of the IEEE. 8611Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient- based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278-2324, 1998.\n\nClassification with noisy labels by importance reweighting. T Liu, D Tao, IEEE Transactions on PAMI. 383T. Liu and D. Tao. Classification with noisy labels by impor- tance reweighting. IEEE Transactions on PAMI, 38(3):447- 461, 2016.\n\nRandom classification noise defeats all convex potential boosters. P M Long, R A Servedio, Machine learning. 783P. M. Long and R. A. Servedio. Random classification noise defeats all convex potential boosters. Machine learning, 78(3):287-304, 2010.\n\nLearning word vectors for sentiment analysis. A L Maas, R E Daly, P T Pham, D Huang, A Y Ng, C Potts, 49 th ACL. A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts. Learning word vectors for sentiment analysis. In 49 th ACL, 2011.\n\nOn the design of loss functions for classification: theory, robustness to outliers, and savageboost. H Masnadi-Shirazi, N Vasconcelos, NIPS*23. H. Masnadi-Shirazi and N. Vasconcelos. On the design of loss functions for classification: theory, robustness to outliers, and savageboost. In NIPS*23, 2009.\n\nLearning from binary labels with instance-dependent corruption. A Menon, B Van Rooyen, N Natarajan, arXiv:1605.00751arXiv preprintA. Menon, B. van Rooyen, and N. Natarajan. Learning from binary labels with instance-dependent corruption. arXiv preprint arXiv:1605.00751, 2016.\n\nLearning from corrupted binary labels via class-probability estimation. A Menon, B Van Rooyen, C S Ong, B Williamson, 32 th ICML. A. Menon, B. van Rooyen, C. S. Ong, and B. Williamson. Learning from corrupted binary labels via class-probability estimation. In 32 th ICML, 2015.\n\nLearning to label aerial images from noisy data. V Mnih, G E Hinton, 29 th ICML. V. Mnih and G. E. Hinton. Learning to label aerial images from noisy data. In 29 th ICML, 2012.\n\nLearning with noisy labels. N Natarajan, I S Dhillon, P K Ravikumar, A Tewari, NIPS*27. N. Natarajan, I. S. Dhillon, P. K. Ravikumar, and A. Tewari. Learning with noisy labels. In NIPS*27, 2013.\n\nVisual recognition by learning from web data: A weakly supervised domain generalization approach. L Niu, W Li, D Xu, 28 th IEEE CVPR. L. Niu, W. Li, and D. Xu. Visual recognition by learning from web data: A weakly supervised domain generalization approach. In 28 th IEEE CVPR, 2015.\n\nLoss factorization, weakly supervised learning and label noise robustness. G Patrini, F Nielsen, R Nock, M Carioni, 33 th ICML. G. Patrini, F. Nielsen, R. Nock, and M. Carioni. Loss factor- ization, weakly supervised learning and label noise robustness. In 33 th ICML, 2016.\n\nMixture proportion estimation via kernel embedding of distributions. H G Ramaswamy, C Scott, A Tewari, 33 th ICML. H. G. Ramaswamy, C. Scott, and A. Tewari. Mixture pro- portion estimation via kernel embedding of distributions. In 33 th ICML, 2016.\n\nS Reed, H Lee, D Anguelov, C Szegedy, D Erhan, A Rabinovich, arXiv:1412.6596Training deep neural networks on noisy labels with bootstrapping. arXiv preprintS. Reed, H. Lee, D. Anguelov, C. Szegedy, D. Erhan, and A. Rabinovich. Training deep neural networks on noisy labels with bootstrapping. arXiv preprint arXiv:1412.6596, 2014.\n\nM D Reid, R C Williamson, Composite binary losses. JMLR. 11M. D. Reid and R. C. Williamson. Composite binary losses. JMLR, 11:2387-2422, 2010.\n\nClass proportion estimation with application to multiclass anomaly rejection. T Sanderson, C C Scott, AISTATS. T. Sanderson and C. C. Scott. Class proportion estimation with application to multiclass anomaly rejection. In AISTATS, 2014.\n\nHarvesting image databases from the web. F Schroff, A Criminisi, A Zisserman, IEEE Transactions on PAMI. 334F. Schroff, A. Criminisi, and A. Zisserman. Harvesting im- age databases from the web. IEEE Transactions on PAMI, 33(4):754-766, 2011.\n\nClassification with asymmetric label noise : Consistency and maximal denoising. C Scott, G Blanchard, G Handy, 26 rd COLT. C. Scott, G. Blanchard, and G. Handy. Classification with asymmetric label noise : Consistency and maximal denoising. In 26 rd COLT, 2013.\n\nDropout: a simple way to prevent neural networks from overfitting. N Srivastava, G E Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov, JMLR. 151N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: a simple way to prevent neural networks from overfitting. JMLR, 15(1):1929-1958, 2014.\n\nLearning SVMs from sloppily labeled data. G Stempfel, L Ralaivola, Artificial Neural Networks (ICANN). SpringerG. Stempfel and L. Ralaivola. Learning SVMs from sloppily labeled data. In Artificial Neural Networks (ICANN), pages 884-893. Springer, 2009.\n\nTraining convolutional networks with noisy labels. S Sukhbaatar, J Bruna, M Paluri, L Bourdev, R Fergus, ICLR Workshops. S. Sukhbaatar, J. Bruna, M. Paluri, L. Bourdev, and R. Fergus. Training convolutional networks with noisy labels. In ICLR Workshops, 2015.\n\nMachine Learning via Transitions. B Van Rooyen, The Australian National UniversityPhD thesisB. van Rooyen. Machine Learning via Transitions. PhD thesis, The Australian National University, 2015.\n\nLearning with symmetric label noise: The importance of being unhinged. B Van Rooyen, A K Menon, R C Williamson, NIPS*29. B. van Rooyen, A. K. Menon, and R. C. Williamson. Learn- ing with symmetric label noise: The importance of being unhinged. In NIPS*29, 2015.\n\nLearning from massive noisy labeled data for image classification. T Xiao, T Xia, T Yang, C Huang, X Wang, 28 th IEEE CVPR. T. Xiao, T. Xia, T. Yang, C. Huang, and X. Wang. Learning from massive noisy labeled data for image classification. In 28 th IEEE CVPR, 2015.\n", "annotations": {"author": "[{\"end\":149,\"start\":81},{\"end\":196,\"start\":150},{\"end\":270,\"start\":197},{\"end\":359,\"start\":271},{\"end\":422,\"start\":360}]", "publisher": null, "author_last_name": "[{\"end\":96,\"start\":89},{\"end\":166,\"start\":161},{\"end\":217,\"start\":212},{\"end\":283,\"start\":279},{\"end\":369,\"start\":367}]", "author_first_name": "[{\"end\":88,\"start\":81},{\"end\":160,\"start\":150},{\"end\":203,\"start\":197},{\"end\":211,\"start\":204},{\"end\":278,\"start\":271},{\"end\":366,\"start\":360}]", "author_affiliation": "[{\"end\":148,\"start\":98},{\"end\":269,\"start\":219},{\"end\":335,\"start\":285},{\"end\":358,\"start\":337},{\"end\":421,\"start\":371}]", "title": "[{\"end\":78,\"start\":1},{\"end\":500,\"start\":423}]", "venue": null, "abstract": "[{\"end\":1532,\"start\":502}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2066,\"start\":2063},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":2069,\"start\":2066},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2071,\"start\":2069},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2074,\"start\":2071},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2077,\"start\":2074},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2545,\"start\":2541},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2548,\"start\":2545},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2551,\"start\":2548},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2554,\"start\":2551},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":2828,\"start\":2824},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2831,\"start\":2828},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2834,\"start\":2831},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":3052,\"start\":3048},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":3055,\"start\":3052},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3058,\"start\":3055},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":3327,\"start\":3323},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3487,\"start\":3483},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3490,\"start\":3487},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3493,\"start\":3490},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3496,\"start\":3493},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3499,\"start\":3496},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":4038,\"start\":4034},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":4041,\"start\":4038},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":4155,\"start\":4151},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4554,\"start\":4550},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5125,\"start\":5121},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5139,\"start\":5135},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5165,\"start\":5161},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5205,\"start\":5201},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5208,\"start\":5205},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5239,\"start\":5235},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":5740,\"start\":5736},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":5982,\"start\":5979},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":6318,\"start\":6314},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6406,\"start\":6402},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6409,\"start\":6406},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6454,\"start\":6450},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6692,\"start\":6688},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":6948,\"start\":6945},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":7063,\"start\":7059},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7192,\"start\":7188},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7574,\"start\":7570},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":7726,\"start\":7722},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":7729,\"start\":7726},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7772,\"start\":7768},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7982,\"start\":7978},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8124,\"start\":8120},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":8408,\"start\":8404},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8411,\"start\":8408},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8414,\"start\":8411},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":8433,\"start\":8429},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":8982,\"start\":8978},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10495,\"start\":10492},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11107,\"start\":11104},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12398,\"start\":12394},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":13055,\"start\":13051},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":13093,\"start\":13089},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":13329,\"start\":13325},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":13331,\"start\":13329},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":13334,\"start\":13331},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13557,\"start\":13553},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":13566,\"start\":13562},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":13847,\"start\":13843},{\"end\":13857,\"start\":13847},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":15180,\"start\":15176},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":15832,\"start\":15828},{\"end\":15842,\"start\":15832},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":16192,\"start\":16188},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":18187,\"start\":18183},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":18190,\"start\":18187},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21183,\"start\":21179},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":21356,\"start\":21355},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21408,\"start\":21404},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":21690,\"start\":21686},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":23641,\"start\":23637},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":23961,\"start\":23957},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":23972,\"start\":23968},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":23998,\"start\":23994},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":24018,\"start\":24014},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":25092,\"start\":25088},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25437,\"start\":25433},{\"end\":25929,\"start\":25919},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":26042,\"start\":26039},{\"end\":26527,\"start\":26516},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26939,\"start\":26936},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":27833,\"start\":27829},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":28343,\"start\":28339},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":28589,\"start\":28585},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":28696,\"start\":28692},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":29025,\"start\":29021},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":29704,\"start\":29700},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":29717,\"start\":29714},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":29730,\"start\":29726},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":29767,\"start\":29763},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":30386,\"start\":30383},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":32352,\"start\":32348},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":32639,\"start\":32635},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":33051,\"start\":33047},{\"end\":33821,\"start\":33816},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":34617,\"start\":34613},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":34836,\"start\":34832},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":36661,\"start\":36657},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":36664,\"start\":36661},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":36798,\"start\":36794},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":40167,\"start\":40164},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":40322,\"start\":40321}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":36901,\"start\":36800},{\"attributes\":{\"id\":\"fig_3\"},\"end\":36992,\"start\":36902},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":40038,\"start\":36993}]", "paragraph": "[{\"end\":2261,\"start\":1548},{\"end\":2835,\"start\":2263},{\"end\":2948,\"start\":2837},{\"end\":3217,\"start\":2950},{\"end\":3797,\"start\":3219},{\"end\":4223,\"start\":3799},{\"end\":4489,\"start\":4225},{\"end\":4638,\"start\":4491},{\"end\":4730,\"start\":4640},{\"end\":5781,\"start\":4732},{\"end\":5882,\"start\":5798},{\"end\":6572,\"start\":5884},{\"end\":7193,\"start\":6574},{\"end\":7773,\"start\":7195},{\"end\":8415,\"start\":7775},{\"end\":8882,\"start\":8417},{\"end\":9540,\"start\":8884},{\"end\":9982,\"start\":9558},{\"end\":10431,\"start\":9984},{\"end\":10583,\"start\":10433},{\"end\":10896,\"start\":10762},{\"end\":11016,\"start\":10898},{\"end\":11100,\"start\":11056},{\"end\":11812,\"start\":11102},{\"end\":11987,\"start\":11872},{\"end\":12266,\"start\":12047},{\"end\":13335,\"start\":12302},{\"end\":13567,\"start\":13337},{\"end\":13940,\"start\":13605},{\"end\":14054,\"start\":13942},{\"end\":14129,\"start\":14084},{\"end\":14216,\"start\":14174},{\"end\":15076,\"start\":14373},{\"end\":15394,\"start\":15113},{\"end\":16041,\"start\":15521},{\"end\":16314,\"start\":16078},{\"end\":16515,\"start\":16359},{\"end\":16648,\"start\":16517},{\"end\":16830,\"start\":16680},{\"end\":16916,\"start\":16891},{\"end\":17288,\"start\":17030},{\"end\":17419,\"start\":17370},{\"end\":17952,\"start\":17421},{\"end\":18324,\"start\":17978},{\"end\":18417,\"start\":18326},{\"end\":18473,\"start\":18419},{\"end\":18530,\"start\":18475},{\"end\":18577,\"start\":18532},{\"end\":18663,\"start\":18643},{\"end\":18992,\"start\":18665},{\"end\":19476,\"start\":18994},{\"end\":19536,\"start\":19512},{\"end\":19948,\"start\":19538},{\"end\":20427,\"start\":19950},{\"end\":21184,\"start\":20472},{\"end\":21449,\"start\":21186},{\"end\":21621,\"start\":21451},{\"end\":21738,\"start\":21623},{\"end\":22216,\"start\":21901},{\"end\":23176,\"start\":22348},{\"end\":23696,\"start\":23178},{\"end\":23869,\"start\":23698},{\"end\":24105,\"start\":23885},{\"end\":24500,\"start\":24152},{\"end\":25718,\"start\":24625},{\"end\":27698,\"start\":25720},{\"end\":28236,\"start\":27700},{\"end\":29328,\"start\":28238},{\"end\":29849,\"start\":29368},{\"end\":32287,\"start\":29851},{\"end\":32964,\"start\":32317},{\"end\":34749,\"start\":32966},{\"end\":34927,\"start\":34751},{\"end\":35853,\"start\":34957},{\"end\":36331,\"start\":35855},{\"end\":36799,\"start\":36333}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10632,\"start\":10584},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10761,\"start\":10632},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11055,\"start\":11017},{\"attributes\":{\"id\":\"formula_3\"},\"end\":11871,\"start\":11813},{\"attributes\":{\"id\":\"formula_4\"},\"end\":12046,\"start\":11988},{\"attributes\":{\"id\":\"formula_5\"},\"end\":14083,\"start\":14055},{\"attributes\":{\"id\":\"formula_6\"},\"end\":14173,\"start\":14130},{\"attributes\":{\"id\":\"formula_7\"},\"end\":14372,\"start\":14217},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15520,\"start\":15395},{\"attributes\":{\"id\":\"formula_9\"},\"end\":16077,\"start\":16042},{\"attributes\":{\"id\":\"formula_10\"},\"end\":16358,\"start\":16315},{\"attributes\":{\"id\":\"formula_11\"},\"end\":16679,\"start\":16649},{\"attributes\":{\"id\":\"formula_12\"},\"end\":16890,\"start\":16831},{\"attributes\":{\"id\":\"formula_13\"},\"end\":17029,\"start\":16917},{\"attributes\":{\"id\":\"formula_14\"},\"end\":17369,\"start\":17289},{\"attributes\":{\"id\":\"formula_15\"},\"end\":18642,\"start\":18578},{\"attributes\":{\"id\":\"formula_17\"},\"end\":19511,\"start\":19477},{\"attributes\":{\"id\":\"formula_19\"},\"end\":21900,\"start\":21739},{\"attributes\":{\"id\":\"formula_20\"},\"end\":22347,\"start\":22217},{\"attributes\":{\"id\":\"formula_21\"},\"end\":24530,\"start\":24501},{\"attributes\":{\"id\":\"formula_22\"},\"end\":24591,\"start\":24530},{\"attributes\":{\"id\":\"formula_23\"},\"end\":24624,\"start\":24591}]", "table_ref": "[{\"end\":21576,\"start\":21569},{\"end\":23651,\"start\":23643},{\"end\":30687,\"start\":30680},{\"end\":30999,\"start\":30992},{\"end\":32818,\"start\":32811},{\"end\":33829,\"start\":33822},{\"end\":34575,\"start\":34568}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1546,\"start\":1534},{\"attributes\":{\"n\":\"2.\"},\"end\":5796,\"start\":5784},{\"attributes\":{\"n\":\"3.\"},\"end\":9556,\"start\":9543},{\"attributes\":{\"n\":\"4.\"},\"end\":12300,\"start\":12269},{\"attributes\":{\"n\":\"4.1.\"},\"end\":13603,\"start\":13570},{\"attributes\":{\"n\":\"4.2.\"},\"end\":15111,\"start\":15079},{\"attributes\":{\"n\":\"4.3.\"},\"end\":17976,\"start\":17955},{\"attributes\":{\"n\":\"4.4.\"},\"end\":20470,\"start\":20430},{\"attributes\":{\"n\":\"5.\"},\"end\":23883,\"start\":23872},{\"attributes\":{\"n\":\"5.1.\"},\"end\":24150,\"start\":24108},{\"attributes\":{\"n\":\"5.2.\"},\"end\":29366,\"start\":29331},{\"attributes\":{\"n\":\"5.3.\"},\"end\":32315,\"start\":32290},{\"attributes\":{\"n\":\"6.\"},\"end\":34955,\"start\":34930},{\"end\":36810,\"start\":36801},{\"end\":36913,\"start\":36903}]", "table": "[{\"end\":40038,\"start\":37091}]", "figure_caption": "[{\"end\":36901,\"start\":36812},{\"end\":36992,\"start\":36915},{\"end\":37091,\"start\":36995}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":26220,\"start\":26211},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":27353,\"start\":27340}]", "bib_author_first_name": "[{\"end\":40562,\"start\":40561},{\"end\":40655,\"start\":40654},{\"end\":40657,\"start\":40656},{\"end\":40664,\"start\":40663},{\"end\":40666,\"start\":40665},{\"end\":40837,\"start\":40836},{\"end\":40848,\"start\":40847},{\"end\":40859,\"start\":40858},{\"end\":41112,\"start\":41111},{\"end\":41121,\"start\":41120},{\"end\":41130,\"start\":41129},{\"end\":41347,\"start\":41346},{\"end\":41357,\"start\":41356},{\"end\":41368,\"start\":41367},{\"end\":41378,\"start\":41377},{\"end\":41626,\"start\":41625},{\"end\":41636,\"start\":41635},{\"end\":41645,\"start\":41644},{\"end\":41661,\"start\":41660},{\"end\":41677,\"start\":41676},{\"end\":41685,\"start\":41684},{\"end\":41910,\"start\":41909},{\"end\":41920,\"start\":41919},{\"end\":42215,\"start\":42214},{\"end\":42224,\"start\":42223},{\"end\":42235,\"start\":42234},{\"end\":42237,\"start\":42236},{\"end\":42453,\"start\":42452},{\"end\":42463,\"start\":42462},{\"end\":42694,\"start\":42693},{\"end\":42700,\"start\":42699},{\"end\":42709,\"start\":42708},{\"end\":42716,\"start\":42715},{\"end\":42921,\"start\":42920},{\"end\":42927,\"start\":42926},{\"end\":42936,\"start\":42935},{\"end\":42943,\"start\":42942},{\"end\":43123,\"start\":43122},{\"end\":43129,\"start\":43128},{\"end\":43138,\"start\":43137},{\"end\":43145,\"start\":43144},{\"end\":43292,\"start\":43291},{\"end\":43306,\"start\":43305},{\"end\":43481,\"start\":43480},{\"end\":43490,\"start\":43489},{\"end\":43497,\"start\":43496},{\"end\":43504,\"start\":43503},{\"end\":43513,\"start\":43512},{\"end\":43747,\"start\":43746},{\"end\":43756,\"start\":43755},{\"end\":43961,\"start\":43960},{\"end\":44132,\"start\":44131},{\"end\":44142,\"start\":44141},{\"end\":44150,\"start\":44149},{\"end\":44160,\"start\":44159},{\"end\":44168,\"start\":44167},{\"end\":44178,\"start\":44177},{\"end\":44188,\"start\":44187},{\"end\":44199,\"start\":44198},{\"end\":44465,\"start\":44464},{\"end\":44479,\"start\":44478},{\"end\":44723,\"start\":44722},{\"end\":44737,\"start\":44736},{\"end\":44750,\"start\":44749},{\"end\":44752,\"start\":44751},{\"end\":44958,\"start\":44957},{\"end\":44967,\"start\":44966},{\"end\":44977,\"start\":44976},{\"end\":44987,\"start\":44986},{\"end\":45243,\"start\":45242},{\"end\":45250,\"start\":45249},{\"end\":45485,\"start\":45484},{\"end\":45487,\"start\":45486},{\"end\":45495,\"start\":45494},{\"end\":45497,\"start\":45496},{\"end\":45714,\"start\":45713},{\"end\":45716,\"start\":45715},{\"end\":45724,\"start\":45723},{\"end\":45726,\"start\":45725},{\"end\":45734,\"start\":45733},{\"end\":45736,\"start\":45735},{\"end\":45744,\"start\":45743},{\"end\":45753,\"start\":45752},{\"end\":45755,\"start\":45754},{\"end\":45761,\"start\":45760},{\"end\":46019,\"start\":46018},{\"end\":46038,\"start\":46037},{\"end\":46285,\"start\":46284},{\"end\":46294,\"start\":46293},{\"end\":46308,\"start\":46307},{\"end\":46570,\"start\":46569},{\"end\":46579,\"start\":46578},{\"end\":46593,\"start\":46592},{\"end\":46595,\"start\":46594},{\"end\":46602,\"start\":46601},{\"end\":46826,\"start\":46825},{\"end\":46834,\"start\":46833},{\"end\":46836,\"start\":46835},{\"end\":46983,\"start\":46982},{\"end\":46996,\"start\":46995},{\"end\":46998,\"start\":46997},{\"end\":47009,\"start\":47008},{\"end\":47011,\"start\":47010},{\"end\":47024,\"start\":47023},{\"end\":47249,\"start\":47248},{\"end\":47256,\"start\":47255},{\"end\":47262,\"start\":47261},{\"end\":47511,\"start\":47510},{\"end\":47522,\"start\":47521},{\"end\":47533,\"start\":47532},{\"end\":47541,\"start\":47540},{\"end\":47781,\"start\":47780},{\"end\":47783,\"start\":47782},{\"end\":47796,\"start\":47795},{\"end\":47805,\"start\":47804},{\"end\":47962,\"start\":47961},{\"end\":47970,\"start\":47969},{\"end\":47977,\"start\":47976},{\"end\":47989,\"start\":47988},{\"end\":48000,\"start\":47999},{\"end\":48009,\"start\":48008},{\"end\":48294,\"start\":48293},{\"end\":48296,\"start\":48295},{\"end\":48304,\"start\":48303},{\"end\":48306,\"start\":48305},{\"end\":48516,\"start\":48515},{\"end\":48529,\"start\":48528},{\"end\":48531,\"start\":48530},{\"end\":48717,\"start\":48716},{\"end\":48728,\"start\":48727},{\"end\":48741,\"start\":48740},{\"end\":49000,\"start\":48999},{\"end\":49009,\"start\":49008},{\"end\":49022,\"start\":49021},{\"end\":49250,\"start\":49249},{\"end\":49264,\"start\":49263},{\"end\":49266,\"start\":49265},{\"end\":49276,\"start\":49275},{\"end\":49290,\"start\":49289},{\"end\":49303,\"start\":49302},{\"end\":49548,\"start\":49547},{\"end\":49560,\"start\":49559},{\"end\":49811,\"start\":49810},{\"end\":49825,\"start\":49824},{\"end\":49834,\"start\":49833},{\"end\":49844,\"start\":49843},{\"end\":49855,\"start\":49854},{\"end\":50055,\"start\":50054},{\"end\":50288,\"start\":50287},{\"end\":50302,\"start\":50301},{\"end\":50304,\"start\":50303},{\"end\":50313,\"start\":50312},{\"end\":50315,\"start\":50314},{\"end\":50547,\"start\":50546},{\"end\":50555,\"start\":50554},{\"end\":50562,\"start\":50561},{\"end\":50570,\"start\":50569},{\"end\":50579,\"start\":50578}]", "bib_author_last_name": "[{\"end\":40570,\"start\":40563},{\"end\":40661,\"start\":40658},{\"end\":40669,\"start\":40667},{\"end\":40845,\"start\":40838},{\"end\":40856,\"start\":40849},{\"end\":40868,\"start\":40860},{\"end\":41118,\"start\":41113},{\"end\":41127,\"start\":41122},{\"end\":41137,\"start\":41131},{\"end\":41354,\"start\":41348},{\"end\":41365,\"start\":41358},{\"end\":41375,\"start\":41369},{\"end\":41388,\"start\":41379},{\"end\":41633,\"start\":41627},{\"end\":41642,\"start\":41637},{\"end\":41658,\"start\":41646},{\"end\":41674,\"start\":41662},{\"end\":41682,\"start\":41678},{\"end\":41692,\"start\":41686},{\"end\":41917,\"start\":41911},{\"end\":41930,\"start\":41921},{\"end\":42221,\"start\":42216},{\"end\":42232,\"start\":42225},{\"end\":42244,\"start\":42238},{\"end\":42460,\"start\":42454},{\"end\":42470,\"start\":42464},{\"end\":42697,\"start\":42695},{\"end\":42706,\"start\":42701},{\"end\":42713,\"start\":42710},{\"end\":42720,\"start\":42717},{\"end\":42924,\"start\":42922},{\"end\":42933,\"start\":42928},{\"end\":42940,\"start\":42937},{\"end\":42947,\"start\":42944},{\"end\":43126,\"start\":43124},{\"end\":43135,\"start\":43130},{\"end\":43142,\"start\":43139},{\"end\":43149,\"start\":43146},{\"end\":43303,\"start\":43293},{\"end\":43318,\"start\":43307},{\"end\":43487,\"start\":43482},{\"end\":43494,\"start\":43491},{\"end\":43501,\"start\":43498},{\"end\":43510,\"start\":43505},{\"end\":43524,\"start\":43514},{\"end\":43753,\"start\":43748},{\"end\":43764,\"start\":43757},{\"end\":43971,\"start\":43962},{\"end\":44139,\"start\":44133},{\"end\":44147,\"start\":44143},{\"end\":44157,\"start\":44151},{\"end\":44165,\"start\":44161},{\"end\":44175,\"start\":44169},{\"end\":44185,\"start\":44179},{\"end\":44196,\"start\":44189},{\"end\":44207,\"start\":44200},{\"end\":44476,\"start\":44466},{\"end\":44486,\"start\":44480},{\"end\":44734,\"start\":44724},{\"end\":44747,\"start\":44738},{\"end\":44759,\"start\":44753},{\"end\":44964,\"start\":44959},{\"end\":44974,\"start\":44968},{\"end\":44984,\"start\":44978},{\"end\":44995,\"start\":44988},{\"end\":45247,\"start\":45244},{\"end\":45254,\"start\":45251},{\"end\":45492,\"start\":45488},{\"end\":45506,\"start\":45498},{\"end\":45721,\"start\":45717},{\"end\":45731,\"start\":45727},{\"end\":45741,\"start\":45737},{\"end\":45750,\"start\":45745},{\"end\":45758,\"start\":45756},{\"end\":45767,\"start\":45762},{\"end\":46035,\"start\":46020},{\"end\":46050,\"start\":46039},{\"end\":46291,\"start\":46286},{\"end\":46305,\"start\":46295},{\"end\":46318,\"start\":46309},{\"end\":46576,\"start\":46571},{\"end\":46590,\"start\":46580},{\"end\":46599,\"start\":46596},{\"end\":46613,\"start\":46603},{\"end\":46831,\"start\":46827},{\"end\":46843,\"start\":46837},{\"end\":46993,\"start\":46984},{\"end\":47006,\"start\":46999},{\"end\":47021,\"start\":47012},{\"end\":47031,\"start\":47025},{\"end\":47253,\"start\":47250},{\"end\":47259,\"start\":47257},{\"end\":47265,\"start\":47263},{\"end\":47519,\"start\":47512},{\"end\":47530,\"start\":47523},{\"end\":47538,\"start\":47534},{\"end\":47549,\"start\":47542},{\"end\":47793,\"start\":47784},{\"end\":47802,\"start\":47797},{\"end\":47812,\"start\":47806},{\"end\":47967,\"start\":47963},{\"end\":47974,\"start\":47971},{\"end\":47986,\"start\":47978},{\"end\":47997,\"start\":47990},{\"end\":48006,\"start\":48001},{\"end\":48020,\"start\":48010},{\"end\":48301,\"start\":48297},{\"end\":48317,\"start\":48307},{\"end\":48526,\"start\":48517},{\"end\":48537,\"start\":48532},{\"end\":48725,\"start\":48718},{\"end\":48738,\"start\":48729},{\"end\":48751,\"start\":48742},{\"end\":49006,\"start\":49001},{\"end\":49019,\"start\":49010},{\"end\":49028,\"start\":49023},{\"end\":49261,\"start\":49251},{\"end\":49273,\"start\":49267},{\"end\":49287,\"start\":49277},{\"end\":49300,\"start\":49291},{\"end\":49317,\"start\":49304},{\"end\":49557,\"start\":49549},{\"end\":49570,\"start\":49561},{\"end\":49822,\"start\":49812},{\"end\":49831,\"start\":49826},{\"end\":49841,\"start\":49835},{\"end\":49852,\"start\":49845},{\"end\":49862,\"start\":49856},{\"end\":50066,\"start\":50056},{\"end\":50299,\"start\":50289},{\"end\":50310,\"start\":50305},{\"end\":50326,\"start\":50316},{\"end\":50552,\"start\":50548},{\"end\":50559,\"start\":50556},{\"end\":50567,\"start\":50563},{\"end\":50576,\"start\":50571},{\"end\":50584,\"start\":50580}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":40617,\"start\":40559},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":7138078},\"end\":40756,\"start\":40619},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":7748515},\"end\":41031,\"start\":40758},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":538820},\"end\":41287,\"start\":41033},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":2199862},\"end\":41574,\"start\":41289},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":856717},\"end\":41850,\"start\":41576},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":6054025},\"end\":42162,\"start\":41852},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":8774197},\"end\":42375,\"start\":42164},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":5575601},\"end\":42598,\"start\":42377},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":13740328},\"end\":42872,\"start\":42600},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":206594692},\"end\":43075,\"start\":42874},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6447277},\"end\":43265,\"start\":43077},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":1915014},\"end\":43441,\"start\":43267},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":6773885},\"end\":43650,\"start\":43443},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":5808102},\"end\":43917,\"start\":43652},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":1605269},\"end\":44054,\"start\":43919},{\"attributes\":{\"id\":\"b16\"},\"end\":44407,\"start\":44056},{\"attributes\":{\"id\":\"b17\"},\"end\":44655,\"start\":44409},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":195908774},\"end\":44899,\"start\":44657},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":64294544},\"end\":45180,\"start\":44901},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":11777930},\"end\":45415,\"start\":45182},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":53861},\"end\":45665,\"start\":45417},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":1428702},\"end\":45915,\"start\":45667},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":11860440},\"end\":46218,\"start\":45917},{\"attributes\":{\"doi\":\"arXiv:1605.00751\",\"id\":\"b24\"},\"end\":46495,\"start\":46220},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":10708717},\"end\":46774,\"start\":46497},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":9448393},\"end\":46952,\"start\":46776},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":423350},\"end\":47148,\"start\":46954},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":366973},\"end\":47433,\"start\":47150},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":865549},\"end\":47709,\"start\":47435},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":7895536},\"end\":47959,\"start\":47711},{\"attributes\":{\"doi\":\"arXiv:1412.6596\",\"id\":\"b31\"},\"end\":48291,\"start\":47961},{\"attributes\":{\"id\":\"b32\"},\"end\":48435,\"start\":48293},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":15111543},\"end\":48673,\"start\":48437},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":9680304},\"end\":48917,\"start\":48675},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":5370766},\"end\":49180,\"start\":48919},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":6844431},\"end\":49503,\"start\":49182},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":18313024},\"end\":49757,\"start\":49505},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":6458072},\"end\":50018,\"start\":49759},{\"attributes\":{\"id\":\"b39\"},\"end\":50214,\"start\":50020},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":6788443},\"end\":50477,\"start\":50216},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":206592873},\"end\":50744,\"start\":50479}]", "bib_title": "[{\"end\":40652,\"start\":40619},{\"end\":40834,\"start\":40758},{\"end\":41109,\"start\":41033},{\"end\":41344,\"start\":41289},{\"end\":41623,\"start\":41576},{\"end\":41907,\"start\":41852},{\"end\":42212,\"start\":42164},{\"end\":42450,\"start\":42377},{\"end\":42691,\"start\":42600},{\"end\":42918,\"start\":42874},{\"end\":43120,\"start\":43077},{\"end\":43289,\"start\":43267},{\"end\":43478,\"start\":43443},{\"end\":43744,\"start\":43652},{\"end\":43958,\"start\":43919},{\"end\":44720,\"start\":44657},{\"end\":44955,\"start\":44901},{\"end\":45240,\"start\":45182},{\"end\":45482,\"start\":45417},{\"end\":45711,\"start\":45667},{\"end\":46016,\"start\":45917},{\"end\":46567,\"start\":46497},{\"end\":46823,\"start\":46776},{\"end\":46980,\"start\":46954},{\"end\":47246,\"start\":47150},{\"end\":47508,\"start\":47435},{\"end\":47778,\"start\":47711},{\"end\":48513,\"start\":48437},{\"end\":48714,\"start\":48675},{\"end\":48997,\"start\":48919},{\"end\":49247,\"start\":49182},{\"end\":49545,\"start\":49505},{\"end\":49808,\"start\":49759},{\"end\":50285,\"start\":50216},{\"end\":50544,\"start\":50479}]", "bib_author": "[{\"end\":40572,\"start\":40561},{\"end\":40663,\"start\":40654},{\"end\":40671,\"start\":40663},{\"end\":40847,\"start\":40836},{\"end\":40858,\"start\":40847},{\"end\":40870,\"start\":40858},{\"end\":41120,\"start\":41111},{\"end\":41129,\"start\":41120},{\"end\":41139,\"start\":41129},{\"end\":41356,\"start\":41346},{\"end\":41367,\"start\":41356},{\"end\":41377,\"start\":41367},{\"end\":41390,\"start\":41377},{\"end\":41635,\"start\":41625},{\"end\":41644,\"start\":41635},{\"end\":41660,\"start\":41644},{\"end\":41676,\"start\":41660},{\"end\":41684,\"start\":41676},{\"end\":41694,\"start\":41684},{\"end\":41919,\"start\":41909},{\"end\":41932,\"start\":41919},{\"end\":42223,\"start\":42214},{\"end\":42234,\"start\":42223},{\"end\":42246,\"start\":42234},{\"end\":42462,\"start\":42452},{\"end\":42472,\"start\":42462},{\"end\":42699,\"start\":42693},{\"end\":42708,\"start\":42699},{\"end\":42715,\"start\":42708},{\"end\":42722,\"start\":42715},{\"end\":42926,\"start\":42920},{\"end\":42935,\"start\":42926},{\"end\":42942,\"start\":42935},{\"end\":42949,\"start\":42942},{\"end\":43128,\"start\":43122},{\"end\":43137,\"start\":43128},{\"end\":43144,\"start\":43137},{\"end\":43151,\"start\":43144},{\"end\":43305,\"start\":43291},{\"end\":43320,\"start\":43305},{\"end\":43489,\"start\":43480},{\"end\":43496,\"start\":43489},{\"end\":43503,\"start\":43496},{\"end\":43512,\"start\":43503},{\"end\":43526,\"start\":43512},{\"end\":43755,\"start\":43746},{\"end\":43766,\"start\":43755},{\"end\":43973,\"start\":43960},{\"end\":44141,\"start\":44131},{\"end\":44149,\"start\":44141},{\"end\":44159,\"start\":44149},{\"end\":44167,\"start\":44159},{\"end\":44177,\"start\":44167},{\"end\":44187,\"start\":44177},{\"end\":44198,\"start\":44187},{\"end\":44209,\"start\":44198},{\"end\":44478,\"start\":44464},{\"end\":44488,\"start\":44478},{\"end\":44736,\"start\":44722},{\"end\":44749,\"start\":44736},{\"end\":44761,\"start\":44749},{\"end\":44966,\"start\":44957},{\"end\":44976,\"start\":44966},{\"end\":44986,\"start\":44976},{\"end\":44997,\"start\":44986},{\"end\":45249,\"start\":45242},{\"end\":45256,\"start\":45249},{\"end\":45494,\"start\":45484},{\"end\":45508,\"start\":45494},{\"end\":45723,\"start\":45713},{\"end\":45733,\"start\":45723},{\"end\":45743,\"start\":45733},{\"end\":45752,\"start\":45743},{\"end\":45760,\"start\":45752},{\"end\":45769,\"start\":45760},{\"end\":46037,\"start\":46018},{\"end\":46052,\"start\":46037},{\"end\":46293,\"start\":46284},{\"end\":46307,\"start\":46293},{\"end\":46320,\"start\":46307},{\"end\":46578,\"start\":46569},{\"end\":46592,\"start\":46578},{\"end\":46601,\"start\":46592},{\"end\":46615,\"start\":46601},{\"end\":46833,\"start\":46825},{\"end\":46845,\"start\":46833},{\"end\":46995,\"start\":46982},{\"end\":47008,\"start\":46995},{\"end\":47023,\"start\":47008},{\"end\":47033,\"start\":47023},{\"end\":47255,\"start\":47248},{\"end\":47261,\"start\":47255},{\"end\":47267,\"start\":47261},{\"end\":47521,\"start\":47510},{\"end\":47532,\"start\":47521},{\"end\":47540,\"start\":47532},{\"end\":47551,\"start\":47540},{\"end\":47795,\"start\":47780},{\"end\":47804,\"start\":47795},{\"end\":47814,\"start\":47804},{\"end\":47969,\"start\":47961},{\"end\":47976,\"start\":47969},{\"end\":47988,\"start\":47976},{\"end\":47999,\"start\":47988},{\"end\":48008,\"start\":47999},{\"end\":48022,\"start\":48008},{\"end\":48303,\"start\":48293},{\"end\":48319,\"start\":48303},{\"end\":48528,\"start\":48515},{\"end\":48539,\"start\":48528},{\"end\":48727,\"start\":48716},{\"end\":48740,\"start\":48727},{\"end\":48753,\"start\":48740},{\"end\":49008,\"start\":48999},{\"end\":49021,\"start\":49008},{\"end\":49030,\"start\":49021},{\"end\":49263,\"start\":49249},{\"end\":49275,\"start\":49263},{\"end\":49289,\"start\":49275},{\"end\":49302,\"start\":49289},{\"end\":49319,\"start\":49302},{\"end\":49559,\"start\":49547},{\"end\":49572,\"start\":49559},{\"end\":49824,\"start\":49810},{\"end\":49833,\"start\":49824},{\"end\":49843,\"start\":49833},{\"end\":49854,\"start\":49843},{\"end\":49864,\"start\":49854},{\"end\":50068,\"start\":50054},{\"end\":50301,\"start\":50287},{\"end\":50312,\"start\":50301},{\"end\":50328,\"start\":50312},{\"end\":50554,\"start\":50546},{\"end\":50561,\"start\":50554},{\"end\":50569,\"start\":50561},{\"end\":50578,\"start\":50569},{\"end\":50586,\"start\":50578}]", "bib_venue": "[{\"end\":40678,\"start\":40671},{\"end\":40885,\"start\":40870},{\"end\":41143,\"start\":41139},{\"end\":41413,\"start\":41390},{\"end\":41701,\"start\":41694},{\"end\":41989,\"start\":41932},{\"end\":42260,\"start\":42246},{\"end\":42479,\"start\":42472},{\"end\":42726,\"start\":42722},{\"end\":42964,\"start\":42949},{\"end\":43161,\"start\":43151},{\"end\":43338,\"start\":43320},{\"end\":43536,\"start\":43526},{\"end\":43776,\"start\":43766},{\"end\":43980,\"start\":43973},{\"end\":44129,\"start\":44056},{\"end\":44462,\"start\":44409},{\"end\":44768,\"start\":44761},{\"end\":45020,\"start\":44997},{\"end\":45281,\"start\":45256},{\"end\":45524,\"start\":45508},{\"end\":45778,\"start\":45769},{\"end\":46059,\"start\":46052},{\"end\":46282,\"start\":46220},{\"end\":46625,\"start\":46615},{\"end\":46855,\"start\":46845},{\"end\":47040,\"start\":47033},{\"end\":47282,\"start\":47267},{\"end\":47561,\"start\":47551},{\"end\":47824,\"start\":47814},{\"end\":48101,\"start\":48037},{\"end\":48348,\"start\":48319},{\"end\":48546,\"start\":48539},{\"end\":48778,\"start\":48753},{\"end\":49040,\"start\":49030},{\"end\":49323,\"start\":49319},{\"end\":49606,\"start\":49572},{\"end\":49878,\"start\":49864},{\"end\":50052,\"start\":50020},{\"end\":50335,\"start\":50328},{\"end\":50601,\"start\":50586}]"}}}, "year": 2023, "month": 12, "day": 17}