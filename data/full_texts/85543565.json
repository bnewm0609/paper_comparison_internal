{"id": 85543565, "updated": "2023-10-01 23:35:02.809", "metadata": {"title": "Distilling Task-Specific Knowledge from BERT into Simple Neural Networks", "authors": "[{\"first\":\"Raphael\",\"last\":\"Tang\",\"middle\":[]},{\"first\":\"Yao\",\"last\":\"Lu\",\"middle\":[]},{\"first\":\"Linqing\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Lili\",\"last\":\"Mou\",\"middle\":[]},{\"first\":\"Olga\",\"last\":\"Vechtomova\",\"middle\":[]},{\"first\":\"Jimmy\",\"last\":\"Lin\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ArXiv", "publication_date": {"year": 2019, "month": 3, "day": 28}, "abstract": "In the natural language processing literature, neural networks are becoming increasingly deeper and complex. The recent poster child of this trend is the deep language representation model, which includes BERT, ELMo, and GPT. These developments have led to the conviction that previous-generation, shallower neural networks for language understanding are obsolete. In this paper, however, we demonstrate that rudimentary, lightweight neural networks can still be made competitive without architecture changes, external training data, or additional input features. We propose to distill knowledge from BERT, a state-of-the-art language representation model, into a single-layer BiLSTM, as well as its siamese counterpart for sentence-pair tasks. Across multiple datasets in paraphrasing, natural language inference, and sentiment classification, we achieve comparable results with ELMo, while using roughly 100 times fewer parameters and 15 times less inference time.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1903.12136", "mag": "2924902521", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-1903-12136", "doi": null}}, "content": {"source": {"pdf_hash": "a08293b2c9c5bcddb023cc7eb3354d4d86bfae89", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1903.12136v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "14da8588ecf9556dadfd8fb8e5414115ace77f00", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/a08293b2c9c5bcddb023cc7eb3354d4d86bfae89.txt", "contents": "\nDistilling Task-Specific Knowledge from BERT into Simple Neural Networks\n\n\nRaphael Tang \nUniversity of Waterloo\n\n\nYao Lu \nUniversity of Waterloo\n\n\nLinqing Liu linqing.liu@uwaterloo.cadoublepower.mou@gmail.com \nUniversity of Waterloo\n\n\nLili Mou \nUniversity of Waterloo\n\n\nOlga Vechtomova \nUniversity of Waterloo\n\n\nJimmy Lin jimmylin@uwaterloo.ca \nUniversity of Waterloo\n\n\nDistilling Task-Specific Knowledge from BERT into Simple Neural Networks\n\nIn the natural language processing literature, neural networks are becoming increasingly deeper and complex. The recent poster child of this trend is the deep language representation model, which includes BERT, ELMo, and GPT. These developments have led to the conviction that previous-generation, shallower neural networks for language understanding are obsolete. In this paper, however, we demonstrate that rudimentary, lightweight neural networks can still be made competitive without architecture changes, external training data, or additional input features. We propose to distill knowledge from BERT, a state-ofthe-art language representation model, into a single-layer BiLSTM, as well as its siamese counterpart for sentence-pair tasks. Across multiple datasets in paraphrasing, natural language inference, and sentiment classification, we achieve comparable results with ELMo, while using roughly 100 times fewer parameters and 15 times less inference time.1 https://goo.gl/Frmwqe\n\nIntroduction\n\nIn the natural language processing (NLP) literature, the march of the neural networks has been an unending yet predictable one, with new architectures constantly surpassing previous ones in not only performance and supposed insight but also complexity and depth. In the midst of all this neural progress, it becomes easy to dismiss earlier, \"first-generation\" neural networks as obsolete. Ostensibly, this appears to be true: Peters et al. (2018) show that using pretrained deep word representations achieves state of the art on a variety of tasks. Recently, Devlin et al. (2018) have pushed this line of work even further with bidirectional encoder representations from transformers (BERT), deeper models that greatly improve * Equal contribution. Ordering decided by coin toss. state of the art on more tasks. More recently, Ope-nAI has described GPT-2, a state-of-the-art, larger transformer model trained on even more data. 1 Such large neural networks are, however, problematic in practice. Due to the large number of parameters, BERT and GPT-2, for example, are undeployable in resource-restricted systems such as mobile devices. They may be inapplicable in realtime systems either, because of low inference-time efficiency. Furthermore, the continued slowdown of Moore's Law and Dennard scaling (Han, 2017) suggests that there exists a point in time when we must compress our models and carefully evaluate our choice of the neural architecture.\n\nIn this paper, we propose a simple yet effective approach that transfers task-specific knowledge from BERT to a shallow neural architecture-in particular, a bidirectional long short-term memory network (BiLSTM). Our motivation is twofold: we question whether a simple architecture actually lacks representation power for text modeling, and we wish to study effective approaches to transfer knowledge from BERT to a BiLSTM. Concretely, we leverage the knowledge distillation approach (Ba and Caruana, 2014;Hinton et al., 2015), where a larger model serves as a teacher and a small model learns to mimic the teacher as a student. This approach is model agnostic, making knowledge transfer possible between BERT and a different neural architecture, such as a single-layer BiLSTM, in our case.\n\nTo facilitate effective knowledge transfer, however, we often require a large, unlabeled dataset. The teacher model provides the probability logits and estimated labels for these unannotated samples, and the student network learns from the teacher's outputs. In computer vision, unlabeled images are usually easy to obtain through augmenting the data using rotation, additive noise, and other distortions. However, obtaining additional, even unlabeled samples for a specific task can be difficult in NLP. Traditional data augmentation in NLP is typically task-specific (Wang and Eisner, 2016;Serban et al., 2016) and difficult to extend to other NLP tasks. To this end, we further propose a novel, rule-based textual data augmentation approach for constructing the knowledge transfer set. Although our augmented samples are not fluent natural language sentences, experimental results show that our approach works surprisingly well for knowledge distillation.\n\nWe evaluate our approach on three tasks in sentence classification and sentence matching. Experiments show that our knowledge distillation procedure significantly outperforms training the original simpler network alone. To our knowledge, we are the first to explore distilling knowledge from BERT. With our approach, a shallow BiLSTMbased model achieves results comparable to Embeddings from Language Models (ELMo; Peters et al., 2018), but uses around 100 times fewer parameters and performs inference 15 times faster. Therefore, our model becomes a state-of-the-art \"small\" model for neural NLP.\n\n\nRelated Work\n\nIn the past, researchers have developed and applied various neural architectures for NLP, including convolutional neural networks (Kalchbrenner et al., 2014;Kim, 2014), recurrent neural networks (Mikolov et al., 2010(Mikolov et al., , 2011Graves, 2013), and recursive neural networks (Socher et al., 2010(Socher et al., , 2011. These generic architectures can be applied to tasks like sentence classification (Zhang et al., 2015;Conneau et al., 2016) and sentence matching (Wan et al., 2016;, but the model is trained only on data of a particular task.\n\nRecently, Peters et al. (2018) introduce Embeddings from Language Models (ELMo), an approach for learning high-quality, deep contextualized representations using bidirectional language models. With ELMo, they achieve large improvements on six different NLP tasks. Devlin et al. (2018) propose Bidirectional Encoder Representations from Transformers (BERT), a new language representation model that obtains state-ofthe-art results on eleven natural language processing tasks. Trained with massive corpora for language modeling, BERT has strong syntactic ability (Goldberg, 2019) and captures generic lan-guage features. A typical downstream use of BERT is to fine-tune it for the NLP task at hand. This improves training efficiency, but for inference efficiency, these models are still considerably slower than traditional neural networks.\n\nModel compression. A prominent line of work is devoted to compressing large neural networks to accelerate inference. Early pioneering works include LeCun et al. (1990), who propose a local error-based method for pruning unimportant weights. Recently, Han et al. (2015) propose a simple compression pipeline, achieving 40 times reduction in model size without hurting accuracy. Unfortunately, these techniques induce irregular weight sparsity, which precludes highly optimized computation routines. Thus, others explore pruning entire filters (Li et al., 2016;, with some even targeting device-centric metrics, such as floating-point operations (Tang et al., 2018) and latency . Still other studies examine quantizing neural networks (Wu et al., 2018); in the extreme, Courbariaux et al. (2016) propose binarized networks with both binary weights and binary activations.\n\nUnlike the aforementioned methods, the knowledge distillation approach (Ba and Caruana, 2014;Hinton et al., 2015) enables the transfer of knowledge from a large model to a smaller, \"student\" network, which is improved in the process. The student network can use a completely different architecture, since distillation works at the output level. This is important in our case, since our research objective is to study the representation power of shallower neural networks for language understanding, while simultaneously compressing models like BERT; thus, we follow this approach in our work. In the NLP literature, it has previously been used in neural machine translation (Kim and Rush, 2016) and language modeling (Yu et al., 2018).\n\n\nOur Approach\n\nFirst, we choose the desired teacher and student models for the knowledge distillation approach. Then, we describe our distillation procedure, which comprises two major components: first, the addition of a logits-regression objective, and second, the construction of a transfer dataset, which augments the training set for more effective knowledge transfer. \n\n\nModel Architecture\n\nFor the teacher network, we use the pretrained, fine-tuned BERT (Devlin et al., 2018) model, a deep, bidirectional transformer encoder that achieves state of the art on a variety of language understanding tasks. From an input sentence (pair), BERT computes a feature vector h \u2208 R d , upon which we build a classifier for the task. For single-sentence classification, we directly build a softmax layer, i.e., the predicted probabilities are y (B) = softmax(W h), where W \u2208 R k\u00d7d is the softmax weight matrix and k is the number of labels. For sentence-pair tasks, we concatenate the BERT features of both sentences and feed them to a softmax layer. During training, we jointly finetune the parameters of BERT and the classifier by maximizing the probability of the correct label, using the cross-entropy loss.\n\nIn contrast, our student model is a single-layer BiLSTM with a non-linear classifier. After feeding the input word embeddings into the BiLSTM, the hidden states of the last step in each direction are concatenated and fed to a fully connected layer with rectified linear units (ReLUs), whose output is then passed to a softmax layer for classification ( Figure 1). For sentence-pair tasks, we share BiLSTM encoder weights in a siamese architecture between the two sentence encoders, producing sentence vectors h s1 and h s2 (Figure 2). We then apply a standard concatenate-compare operation (Wang et al., 2018) between the two sentence vectors:\nf (h s1 , h s2 ) = [h s1 , h s2 , h s1 h s2 , |h s1 \u2212 h s2 |],\nwhere denotes elementwise multiplication. We feed this output to a ReLU- activated classifier.\n\nIt should be emphasized that we restrict the architecture engineering to a minimum to revisit the representation power of BiLSTM itself. We avoid any additional tricks, such as attention and layer normalization.\n\n\nDistillation Objective\n\nThe distillation approach accomplishes knowledge transfer at the output level; that is, the student network learns to mimic a teacher network's behavior given any data point. In particular, Ba and Caruana (2014) posit that, in addition to a one-hot predicted label, the teacher's predicted probability is also important. In binary sentiment classification, for example, some sentences have a strong sentiment polarity, whereas others appear neutral. If we use only the teacher's predicted one-hot label to train the student, we may lose valuable information about the prediction uncertainty.\n\nThe discrete probability output of a neural network is given by\ny i = softmax(z) = exp{w i h} j exp{w j h}(1)\nwhere w i denotes the i th row of softmax weight W , and z is equivalent to w h. The argument of the softmax function is known as logits. Training on logits makes learning easier for the student model since the relationship learned by the teacher model across all of the targets are equally emphasized (Ba and Caruana, 2014). The distillation objective is to penalize the mean-squared-error (MSE) loss between the student network's logits against the teacher's logits:\nL distill = ||z z z (B) \u2212 z z z (S) || 2 2 (2)\nwhere z z z (B) and z z z (S) are the teacher's and student's logits, respectively. Other measures such as cross entropy with soft targets are viable as well (Hinton et al., 2015); however, in our preliminary experiments, we found MSE to perform slightly better.\n\nAt training time, the distilling objective can be used in conjunction with a traditional crossentropy loss against a one-hot label t, given by\nL = \u03b1 \u00b7 L CE + (1 \u2212 \u03b1) \u00b7 L distill (3) = \u2212\u03b1 i t i log y (S) i \u2212 (1 \u2212 \u03b1)||z z z (B) \u2212 z z z (S) || 2 2\nWhen distilling with a labeled dataset, the one-hot target t is simply the ground-truth label. When distilling with an unlabeled dataset, we use the predicted label by the teacher, i.e., t i = 1 if i = argmax y (B) and 0 otherwise.\n\n\nData Augmentation for Distillation\n\nIn the distillation approach, a small dataset may not suffice for the teacher model to fully express its knowledge (Ba and Caruana, 2014). Therefore, we augment the training set with a large, unlabeled dataset, with pseudo-labels provided by the teacher, to aid in effective knowledge distillation. Unfortunately, data augmentation in NLP is usually more difficult than in computer vision. First, there exist a large number of homologous images in computer vision tasks. CIFAR-10, for example, is a subset of the 80 million tiny images dataset (Krizhevsky, 2009). Second, it is possible to synthesize a near-natural image by rotating, adding noise, and other distortions, but if we manually manipulate a natural language sentence, the sentence may not be fluent, and its effect in NLP data augmentation less clear.\n\nIn our work, we propose a set of heuristics for task-agnostic data augmentation: we use the original sentences in the small dataset as blueprints, and then modify them with our heuristics, a process analogous to image distortion. Specifically, we randomly perform the following operations. Masking. With probability p mask , we randomly replace a word with [MASK], which corresponds to an unknown token in our models and the masked word token in BERT. Intuitively, this rule helps to clarify the contribution of each word toward the label, e.g., the teacher network produces less confident logits for \"I [MASK] the comedy\" than for \"I loved the comedy.\"\n\nPOS-guided word replacement. With probability p pos , we replace a word with another of the same POS tag. To preserve the original training distribution, the new word is sampled from the unigram word distribution re-normalized by the partof-speech (POS) tag. This rule perturbs the semantics of each example, e.g., \"What do pigs eat?\" is different from \"How do pigs eat?\" n n n-gram sampling. With probability p ng , we randomly sample an n-gram from the example, where n is randomly selected from {1, 2, . . . , 5}. This rule is conceptually equivalent to dropping out all other words in the example, which is a more aggressive form of masking.\n\nOur data augmentation procedure is as follows: given a training example {w 1 , . . . w n }, we iterate over the words, drawing from the uniform distribution X i \u223c UNIFORM[0, 1] for each w i . If X i < p mask , we apply masking to w i . If p mask \u2264 X i < p mask + p pos , we apply POS-guided word replacement. We treat masking and POS-guided swapping as mutually exclusive: once one rule is applied, the other is disregarded. After iterating through the words, with probability p ng , we apply n-gram sampling to this entire synthetic example. The final synthetic example is appended to the augmented, unlabeled dataset.\n\nWe apply this procedure n iter times per example to generate up to n iter samples from a single example, with any duplicates discarded. For sentencepair datasets, we cycle through augmenting the first sentence only (holding the second fixed), the second sentence only (holding the first fixed), and both sentences.\n\n\nExperimental Setup\n\nFor BERT, we use the large variant BERT LARGE (described below) as the teacher network, starting with the pretrained weights and following the original, task-specific fine-tuning procedure (Devlin et al., 2018). We fine-tune four models using the Adam optimizer with learning rates {2, 3, 4, 5} \u00d7 10 \u22125 , picking the best model on the validation set. We avoid data augmentation during fine-tuning.\n\nFor our models, we feed the original dataset together with the synthesized examples to the taskspecific, fine-tuned BERT model to obtain the predicted logits. We denote our distilled BiL-STM trained on soft logit targets as BiLSTM SOFT , which corresponds to choosing \u03b1 = 0 in Section 3.2. Preliminary experiments suggest that using only the distillation objective works best.\n\n\nDatasets\n\nWe conduct experiments on the General Language Understanding Evaluation (GLUE; Wang et al., 2018) benchmark, a collection of six natural language understanding tasks that are classified into three categories: single-sentence tasks, similarity and paraphrase tasks, and inference tasks. Due to restrictions in time and computational resources, we choose the most widely used dataset from each category, as detailed below.\n\nSST-2. Stanford Sentiment Treebank 2 (SST-2; Socher et al., 2013) comprises single sentences extracted from movie reviews for binary sentiment classification (positive vs. negative). Following GLUE, we consider sentence-level sentiment only, ignoring the sentiment labels of phrases provided by the original dataset.\n\nMNLI. The Multi-genre Natural Language Inference (MNLI; Williams et al., 2017) corpus is a large-scale, crowdsourced entailment classification dataset. The objective is to predict the relationship between a pair of sentences as one of entailment, neutrality, or contradiction. MNLI-m uses development and test sets that contain the same genres from the training set, while MNLI-mm represents development and test sets from the remaining, mismatched genres.\n\nQQP. Quora Question Pairs (QQP; Shankar Iyer and Csernai, 2017) consists of pairs of potentially duplicate questions collected from Quora, a question-and-answer website. The binary label of each question pair indicates redundancy.\n\n\nHyperparameters\n\nWe choose either 150 or 300 hidden units for the BiLSTM, and 200 or 400 units in the ReLUactivated hidden layer, depending on the validation set performance. Following Kim (2014), we use the traditional 300-dimensional word2vec embeddings trained on Google News and multichannel embeddings. For optimization, we use AdaDelta (Zeiler, 2012) with its default learning rate of 1.0 and \u03c1 = 0.95. For SST-2, we use a batch size of 50; for MNLI and QQP, due to their larger size, we choose 256 for the batch size.\n\nFor our dataset augmentation hyperparameters, we fix p mask = p pos = 0.1 and p ng = 0.25 across all datasets. These values have not been tuned at all on the datasets-these are the first values we chose. We choose n iter = 20 for SST-2 and n iter = 10 for both MNLI and QQP, since they are larger.\n\n\nBaseline Models\n\nBERT (Devlin et al., 2018) is a multi-layer, bidirectional transformer encoder that comes in two variants: BERT BASE and the larger BERT LARGE . BERT BASE comprises 12 layers, 768 hidden units, 12 self-attention heads, and 110M parameters. BERT LARGE uses 24 layers, 1024 hidden units, 16 self-attention heads, and 340M parameters.\n\nOpenAI GPT (Radford et al., 2018) is, like BERT, a generative pretrained transformer (GPT) encoder fine-tuned on downstream tasks. Unlike BERT, however, GPT is unidirectional and only makes use of previous context at each time step.\n\n\nGLUE ELMo baselines.\n\nIn the GLUE paper, Wang et al. (2018) provide a BiLSTM-based model baseline trained on top of ELMo and jointly fine-tuned across all tasks. This model contains 4096 units in the ELMo BiLSTM and more than 93 million total parameters. In the BERT paper, Devlin et al. (2018) provide the same model but a result slightly different from Wang et al. (2018). For fair comparison, we report both results.\n\n\nResults and Discussion\n\nWe present the results of our models as well as baselines in Table 1. For QQP, we report both F 1 and accuracy, since the dataset is slightly unbalanced. Following GLUE, we report the average score of each model on the datasets.\n\n\nModel Quality\n\nTo verify the correctness of our implementation, we train the base BiLSTM model on the original labels, without using distillation (row 7). Across all three datasets, we achieve scores comparable with BiLSTMs from previous works (rows 8 and 9), suggesting that our implementation is fair. Note that, on MNLI, the two baselines differ by 4% in accuracy (rows 8 and 9). None of the nondistilled BiLSTM baselines outperform BERT's ELMo baseline (row 4)-our implementation, although attaining a higher accuracy for QQP, falls short in F 1 score. We apply our distillation approach of matching logits using the augmented training dataset, and achieve an absolute improvement of 1.9-4.5 points against our base BiLSTM. On SST-2 and QQP, we outperform the best reported ELMo model (row 4), coming close to GPT. On MNLI, our results trail ELMo's by a few points; however, they still represent a 4.3-point improvement against our BiLSTM, and a 1.8-2.7-point increase over the previous best BiLSTM (row 8). Overall, our distilled model is competitive with two previous implementations of ELMo BiLSTMs (rows 4-5), suggesting that shallow BiLSTMs have greater representation power than previously thought.\n\nWe do not, however, outperform the deep transformer models (rows 1-3), doing 4-7 points worse, on average. Nevertheless, our model has much fewer parameters and better efficiency, as detailed in the following section.\n\n\nInference Efficiency\n\nFor our inference speed and parameter analysis, we use the open-source PyTorch implementations for BERT 2 and ELMo (Gardner et al., 2017). On a single NVIDIA V100 GPU, we perform model inference with a batch size of 512 on all 67350 sentences of the SST-2 training set. As shown in Table 2  faster. At 2.2 million parameters, the variant with 300-dimensional LSTM units is twice as large, though still substantially smaller than ELMo. For sentence-pair tasks, the siamese counterpart uses no pairwise word interactions, unlike previous state of the art ; its runtime thus scales linearly with sentence length.\n\n\nConclusion and Future Work\n\nIn this paper, we explore distilling the knowledge from BERT into a simple BiLSTM-based model. The distilled model achieves comparable results with ELMo, while using much fewer parameters and less inference time. Our results suggest that shallow BiLSTMs are more expressive for natural language tasks than previously thought. One direction of future work is to explore extremely simple architectures in the extreme, such as convolutional neural networks and even support vector machines and logistic regression. Another opposite direction is to explore slightly more complicated architectures using tricks like pairwise word interaction and attention.\n\nFigure 1 :\n1The BiLSTM model for single-sentence classification. The labels are (a) input embeddings, (b) BiLSTM, (c, d) backward and forward hidden states, respectively, (e, g) fully-connected layer; (e) with ReLU, (f) hidden representation, (h) logit outputs, (i) softmax activation, and (j) final probabilities.\n\nFigure 2 :\n2The siamese BiLSTM model for sentence matching, with shared encoder weights for both sentences. The labels are (a) BiLSTM, (b, c) final backward and forward hidden states, respectively, (d) concatenate-compare unit, (e, g) fully connected layer; (e) with ReLU, (f) hidden representation, (h) logit outputs, (i) softmax activation, and (j) final probabilities.\n\n\nTable 1: Test results on different datasets. The BiLSTM results reported by other papers are drawn from Zhou et al. (2016), \u2020 Wang et al. (2017), \u2021 and Williams et al. (2017). * All of our test results are obtained from the GLUE benchmark website.# Model \n\nSST-2 \nQQP \nMNLI-m MNLI-mm \n\nAcc \nF 1 /Acc \nAcc \nAcc \n\n1 BERT LARGE (Devlin et al., 2018) \n94.9 \n72.1/89.3 \n86.7 \n85.9 \n2 BERT BASE (Devlin et al., 2018) \n93.5 \n71.2/89.2 \n84.6 \n83.4 \n3 OpenAI GPT (Radford et al., 2018) \n91.3 \n70.3/88.5 \n82.1 \n81.4 \n4 BERT ELMo baseline (Devlin et al., 2018) 90.4 \n64.8/84.7 \n76.4 \n76.1 \n5 GLUE ELMo baseline (Wang et al., 2018) \n90.4 \n63.1/84.3 \n74.1 \n74.5 \n\n6 Distilled BiLSTM SOFT \n90.7 \n68.2/88.1 \n73.0 \n72.6 \n7 BiLSTM (our implementation) \n86.7 \n63.7/86.2 \n68.7 \n68.3 \n8 BiLSTM (reported by GLUE) \n85.9 \n61.4/81.7 \n70.3 \n70.8 \n9 BiLSTM (reported by other papers) \n87.6  \u2020 \n-/82.6  \u2021 \n66.9 * \n66.9 * \n\n\n\n\n, our single-sentence model uses 98 and 349 times fewer parameters than ELMo and BERT LARGE , respectively, and is 15 and 434 times 2 https://goo.gl/iRPhjP# of Par. \nInference Time \n\nBERT LARGE \n335 (349\u00d7) \n1060 (434\u00d7) \nELMo \n93.6 (98\u00d7) \n36.71 (15\u00d7) \nBiLSTM SOFT \n0.96 (1\u00d7) \n2.44 (1\u00d7) \n\n\n\nTable 2 :\n2Single-sentence model size and inference speed on SST-2. # of Par. denotes number of millions of parameters, and inference time is in seconds.\nAcknowledgementsThis research was enabled in part by resources provided by Compute Ontario and Compute Canada. This research was also supported by the Natural Sciences and Engineering Research Council (NSERC) of Canada.\nDo deep nets really need to be deep?. Jimmy Ba, Rich Caruana, Advances in neural information processing systems. Jimmy Ba and Rich Caruana. 2014. Do deep nets really need to be deep? In Advances in neural information processing systems, pages 2654-2662.\n\nConstraint-aware deep neural network compression. Changan Chen, Frederick Tung, Naveen Vedula, Greg Mori, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)Changan Chen, Frederick Tung, Naveen Vedula, and Greg Mori. 2018. Constraint-aware deep neural network compression. In Proceedings of the Eu- ropean Conference on Computer Vision (ECCV), pages 400-415.\n\nAlexis Conneau, Holger Schwenk, Lo\u00efc Barrault, Yann Lecun, arXiv:1606.01781Very deep convolutional networks for text classification. Alexis Conneau, Holger Schwenk, Lo\u00efc Barrault, and Yann Lecun. 2016. Very deep convolutional net- works for text classification. arXiv:1606.01781.\n\nRan El-Yaniv, and Yoshua Bengio. Matthieu Courbariaux, Itay Hubara, Daniel Soudry, arXiv:1602.02830Binarized neural networks: Training deep neural networks with weights and activations constrained to +1 or -1. Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua Bengio. 2016. Bina- rized neural networks: Training deep neural net- works with weights and activations constrained to +1 or -1. arXiv:1602.02830.\n\nJacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova, arXiv:1810.04805BERT: Pre-training of deep bidirectional transformers for language understanding. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of deep bidirectional transformers for language under- standing. arXiv:1810.04805.\n\nAllenNLP: A deep semantic natural language processing platform. Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F Liu, Matthew Peters, Michael Schmitz, Luke S Zettlemoyer, arXiv:1803.07640Matt Gardner, Joel Grus, Mark Neumann, Oyvind Tafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Peters, Michael Schmitz, and Luke S. Zettlemoyer. 2017. AllenNLP: A deep semantic natural language processing platform. arXiv:1803.07640.\n\nAssessing BERT's syntactic abilities. Yoav Goldberg, arXiv:1901.05287Yoav Goldberg. 2019. Assessing BERT's syntactic abilities. arXiv:1901.05287.\n\nGenerating sequences with recurrent neural networks. Alex Graves, arXiv:1308.0850Alex Graves. 2013. Generating sequences with recur- rent neural networks. arXiv:1308.0850.\n\nEfficient methods and hardware for deep learning. Song Han, Song Han. 2017. Efficient methods and hardware for deep learning.\n\nSong Han, Huizi Mao, William J Dally, arXiv:1510.00149Deep compression: Compressing deep neural networks with pruning, trained quantization and Huffman coding. Song Han, Huizi Mao, and William J Dally. 2015. Deep compression: Compressing deep neural net- works with pruning, trained quantization and Huff- man coding. arXiv:1510.00149.\n\nPairwise word interaction modeling with deep neural networks for semantic similarity measurement. Hua He, Jimmy Lin, Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language TechnologiesHua He and Jimmy Lin. 2016. Pairwise word interac- tion modeling with deep neural networks for seman- tic similarity measurement. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Hu- man Language Technologies, pages 937-948.\n\nUMD-TTIC-UW at SemEval-2016 task 1: Attention-based multi-perspective convolutional neural networks for textual similarity measurement. Hua He, John Wieting, Kevin Gimpel, Jinfeng Rao, Jimmy Lin, Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016). the 10th International Workshop on Semantic Evaluation (SemEval-2016)Hua He, John Wieting, Kevin Gimpel, Jinfeng Rao, and Jimmy Lin. 2016. UMD-TTIC-UW at SemEval- 2016 task 1: Attention-based multi-perspective con- volutional neural networks for textual similarity measurement. In Proceedings of the 10th Interna- tional Workshop on Semantic Evaluation (SemEval- 2016), pages 1103-1108.\n\nGeoffrey Hinton, Oriol Vinyals, Jeff Dean, arXiv:1503.02531Distilling the knowledge in a neural network. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv:1503.02531.\n\nA convolutional neural network for modelling sentences. Nal Kalchbrenner, Edward Grefenstette, Phil Blunsom, arXiv:1404.2188Nal Kalchbrenner, Edward Grefenstette, and Phil Blun- som. 2014. A convolutional neural network for modelling sentences. arXiv:1404.2188.\n\nConvolutional neural networks for sentence classification. Yoon Kim, Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)Yoon Kim. 2014. Convolutional neural networks for sentence classification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Lan- guage Processing (EMNLP), pages 1746-1751.\n\nSequencelevel knowledge distillation. Yoon Kim, Alexander M Rush, Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. the 2016 Conference on Empirical Methods in Natural Language ProcessingYoon Kim and Alexander M. Rush. 2016. Sequence- level knowledge distillation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1317-1327.\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, University of TorontoTechnical reportAlex Krizhevsky. 2009. Learning multiple layers of features from tiny images. Technical report, Univer- sity of Toronto.\n\nOptimal brain damage. Yann Lecun, John S Denker, Sara A Solla, Advances in neural information processing systems. Yann LeCun, John S. Denker, and Sara A. Solla. 1990. Optimal brain damage. In Advances in neural infor- mation processing systems, pages 598-605.\n\nPruning filters for efficient convnets. Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, Hans Peter Graf, arXiv:1608.08710Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. 2016. Pruning filters for effi- cient convnets. arXiv:1608.08710.\n\nLearning efficient convolutional networks through network slimming. Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, Changshui Zhang, Proceedings of the IEEE International Conference on Computer Vision. the IEEE International Conference on Computer VisionZhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. 2017. Learning efficient convolutional networks through network slimming. In Proceedings of the IEEE In- ternational Conference on Computer Vision, pages 2736-2744.\n\nRecurrent neural network based language model. Tom\u00e1\u0161 Mikolov, Martin Karafi\u00e1t, Luk\u00e1\u0161 Burget, Ja\u0148 Cernock\u1ef3, Sanjeev Khudanpur, Eleventh annual conference of the international speech communication association. Tom\u00e1\u0161 Mikolov, Martin Karafi\u00e1t, Luk\u00e1\u0161 Burget, Ja\u0148 Cernock\u1ef3, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Eleventh annual conference of the international speech com- munication association.\n\nExtensions of recurrent neural network language model. Tom\u00e1\u0161 Mikolov, Stefan Kombrink, Luk\u00e1\u0161 Burget, Ja\u0148 Cernock\u1ef3, Sanjeev Khudanpur, 2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Tom\u00e1\u0161 Mikolov, Stefan Kombrink, Luk\u00e1\u0161 Burget, Ja\u0148 Cernock\u1ef3, and Sanjeev Khudanpur. 2011. Exten- sions of recurrent neural network language model. In 2011 IEEE International Conference on Acous- tics, Speech and Signal Processing (ICASSP), pages 5528-5531.\n\nDeep contextualized word representations. Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer, Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies1Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word repre- sentations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), volume 1, pages 2227-2237.\n\nImproving language understanding with unsupervised learning. Alec Radford, Karthik Narasimhan, OpenAITechnical reportTime Salimans, and Ilya SutskeverAlec Radford, Karthik Narasimhan, Time Salimans, and Ilya Sutskever. 2018. Improving language un- derstanding with unsupervised learning. Technical report, Technical report, OpenAI.\n\nIulian Vlad Serban, Alberto Garc\u00eda-Dur\u00e1n, Caglar Gulcehre, Sungjin Ahn, Sarath Chandar, Aaron Courville, Yoshua Bengio, arXiv:1603.06807Generating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus. Iulian Vlad Serban, Alberto Garc\u00eda-Dur\u00e1n, Caglar Gulcehre, Sungjin Ahn, Sarath Chandar, Aaron Courville, and Yoshua Bengio. 2016. Generat- ing factoid questions with recurrent neural net- works: The 30m factoid question-answer corpus. arXiv:1603.06807.\n\nFirst Quora dataset release: Question pairs. Nikhil Dandekar Shankar Iyer and Kornl CsernaiNikhil Dandekar Shankar Iyer and Kornl Csernai. 2017. First Quora dataset release: Question pairs.\n\nRichard Socher, Cliff C Lin, Chris Manning, Andrew Y Ng, Parsing natural scenes and natural language with recursive neural networks. In Proceedings of the 28th international conference on machine learning (ICML-11). Richard Socher, Cliff C. Lin, Chris Manning, and An- drew Y Ng. 2011. Parsing natural scenes and natu- ral language with recursive neural networks. In Pro- ceedings of the 28th international conference on ma- chine learning (ICML-11), pages 129-136.\n\nLearning continuous phrase representations and syntactic parsing with recursive neural networks. Richard Socher, Christopher D Manning, Andrew Y Ng, Proceedings of the NIPS-2010. the NIPS-2010Richard Socher, Christopher D. Manning, and An- drew Y. Ng. 2010. Learning continuous phrase representations and syntactic parsing with recursive neural networks. In Proceedings of the NIPS-2010\n\nDeep Learning and Unsupervised Feature Learning Workshop. 2010Deep Learning and Unsupervised Feature Learning Workshop, volume 2010, pages 1-9.\n\nRecursive deep models for semantic compositionality over a sentiment treebank. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Ng, Christopher Potts, Proceedings of the 2013 conference on empirical methods in natural language processing. the 2013 conference on empirical methods in natural language processingRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment tree- bank. In Proceedings of the 2013 conference on empirical methods in natural language processing, pages 1631-1642.\n\nRaphael Tang, Ashutosh Adhikari, Jimmy Lin, arXiv:1811.03060FLOPs as a direct optimization objective for learning sparse neural networks. Raphael Tang, Ashutosh Adhikari, and Jimmy Lin. 2018. FLOPs as a direct optimization objective for learning sparse neural networks. arXiv:1811.03060.\n\nA deep architecture for semantic matching with multiple positional sentence representations. Yanyan Shengxian Wan, Jiafeng Lan, Jun Guo, Liang Xu, Xueqi Pang, Cheng, Thirtieth AAAI Conference on Artificial Intelligence. Shengxian Wan, Yanyan Lan, Jiafeng Guo, Jun Xu, Liang Pang, and Xueqi Cheng. 2016. A deep ar- chitecture for semantic matching with multiple po- sitional sentence representations. In Thirtieth AAAI Conference on Artificial Intelligence.\n\nAlex Wang, Amapreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, arXiv:1804.07461GLUE: A multi-task benchmark and analysis platform for natural language understanding. Alex Wang, Amapreet Singh, Julian Michael, Fe- lix Hill, Omer Levy, and Samuel R. Bowman. 2018. GLUE: A multi-task benchmark and anal- ysis platform for natural language understanding. arXiv:1804.07461.\n\nThe galactic dependencies treebanks: Getting more data by synthesizing new languages. Dingquan Wang, Jason Eisner, Transactions of the Association for Computational Linguistics. 4Dingquan Wang and Jason Eisner. 2016. The galactic dependencies treebanks: Getting more data by syn- thesizing new languages. Transactions of the Asso- ciation for Computational Linguistics, 4:491-505.\n\nBilateral multi-perspective matching for natural language sentences. Zhiguo Wang, Wael Hamza, Radu Florian, Proceedings of the 26th International Joint Conference on Artificial Intelligence. the 26th International Joint Conference on Artificial IntelligenceZhiguo Wang, Wael Hamza, and Radu Florian. 2017. Bilateral multi-perspective matching for natural lan- guage sentences. In Proceedings of the 26th Inter- national Joint Conference on Artificial Intelligence, pages 4144-4150.\n\nBowman. 2017. A broad-coverage challenge corpus for sentence understanding through inference. Adina Williams, Nikita Nangia, Samuel R , arXiv:1704.05426Adina Williams, Nikita Nangia, and Samuel R. Bow- man. 2017. A broad-coverage challenge cor- pus for sentence understanding through inference. arXiv:1704.05426.\n\nTraining and inference with integers in deep neural networks. Shuang Wu, Guoqi Li, Feng Chen, Luping Shi, International Conference on Learning Representations. Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi. 2018. Training and inference with integers in deep neural networks. In International Conference on Learning Representations.\n\nOn-device neural language model based word prediction. Seunghak Yu, Nilesh Kulkarni, Haejun Lee, Jihie Kim, Proceedings of COLING 2018, the 28th International Conference on Computational Linguistics: Technical Papers. COLING 2018, the 28th International Conference on Computational Linguistics: Technical Papers128Seunghak Yu, Nilesh Kulkarni, Haejun Lee, and Jihie Kim. 2018. On-device neural language model based word prediction. Proceedings of COLING 2018, the 28th International Conference on Computational Linguistics: Technical Papers, page 128.\n\nADADELTA: an adaptive learning rate method. D Matthew, Zeiler, arXiv:1212.5701Matthew D. Zeiler. 2012. ADADELTA: an adaptive learning rate method. arXiv:1212.5701.\n\nCharacter-level convolutional networks for text classification. Xiang Zhang, Junbo Zhao, Yann Lecun, Advances in neural information processing systems. Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text clas- sification. In Advances in neural information pro- cessing systems, pages 649-657.\n\nText classification improved by integrating bidirectional LSTM with two-dimensional max pooling. Peng Zhou, Zhenyu Qi, Suncong Zheng, Jiaming Xu, Hongyun Bao, Bo Xu, Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers. COLING 2016, the 26th International Conference on Computational Linguistics: Technical PapersPeng Zhou, Zhenyu Qi, Suncong Zheng, Jiaming Xu, Hongyun Bao, and Bo Xu. 2016. Text classifi- cation improved by integrating bidirectional LSTM with two-dimensional max pooling. In Proceedings of COLING 2016, the 26th International Confer- ence on Computational Linguistics: Technical Pa- pers, pages 3485-3495.\n", "annotations": {"author": "[{\"end\":114,\"start\":76},{\"end\":147,\"start\":115},{\"end\":235,\"start\":148},{\"end\":270,\"start\":236},{\"end\":312,\"start\":271},{\"end\":370,\"start\":313}]", "publisher": null, "author_last_name": "[{\"end\":88,\"start\":84},{\"end\":121,\"start\":119},{\"end\":159,\"start\":156},{\"end\":244,\"start\":241},{\"end\":286,\"start\":276},{\"end\":322,\"start\":319}]", "author_first_name": "[{\"end\":83,\"start\":76},{\"end\":118,\"start\":115},{\"end\":155,\"start\":148},{\"end\":240,\"start\":236},{\"end\":275,\"start\":271},{\"end\":318,\"start\":313}]", "author_affiliation": "[{\"end\":113,\"start\":90},{\"end\":146,\"start\":123},{\"end\":234,\"start\":211},{\"end\":269,\"start\":246},{\"end\":311,\"start\":288},{\"end\":369,\"start\":346}]", "title": "[{\"end\":73,\"start\":1},{\"end\":443,\"start\":371}]", "venue": null, "abstract": "[{\"end\":1433,\"start\":445}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b22\"},\"end\":1895,\"start\":1875},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2028,\"start\":2008},{\"end\":2378,\"start\":2377},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2762,\"start\":2751},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3407,\"start\":3385},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3427,\"start\":3407},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4285,\"start\":4262},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":4305,\"start\":4285},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5088,\"start\":5068},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5424,\"start\":5397},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5434,\"start\":5424},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5483,\"start\":5462},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5506,\"start\":5483},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5519,\"start\":5506},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5571,\"start\":5551},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5593,\"start\":5571},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5696,\"start\":5676},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5717,\"start\":5696},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":5758,\"start\":5740},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":5851,\"start\":5831},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6105,\"start\":6085},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":6398,\"start\":6382},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":6828,\"start\":6809},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6929,\"start\":6912},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":7220,\"start\":7203},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7324,\"start\":7305},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":7411,\"start\":7394},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":7454,\"start\":7429},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":7625,\"start\":7603},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7645,\"start\":7625},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8226,\"start\":8206},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":8266,\"start\":8249},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":8750,\"start\":8729},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":10084,\"start\":10065},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":10727,\"start\":10706},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":11543,\"start\":11521},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11914,\"start\":11893},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":12651,\"start\":12629},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":13076,\"start\":13058},{\"end\":13693,\"start\":13687},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":15800,\"start\":15779},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":16475,\"start\":16457},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":16865,\"start\":16845},{\"end\":17196,\"start\":17174},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":18004,\"start\":17994},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":18678,\"start\":18657},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":19018,\"start\":18996},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":19279,\"start\":19261},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19514,\"start\":19494},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":19593,\"start\":19575},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":21486,\"start\":21464}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":22956,\"start\":22641},{\"attributes\":{\"id\":\"fig_1\"},\"end\":23329,\"start\":22957},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":24229,\"start\":23330},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":24519,\"start\":24230},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":24674,\"start\":24520}]", "paragraph": "[{\"end\":2900,\"start\":1449},{\"end\":3691,\"start\":2902},{\"end\":4651,\"start\":3693},{\"end\":5250,\"start\":4653},{\"end\":5819,\"start\":5267},{\"end\":6659,\"start\":5821},{\"end\":7530,\"start\":6661},{\"end\":8267,\"start\":7532},{\"end\":8642,\"start\":8284},{\"end\":9473,\"start\":8665},{\"end\":10118,\"start\":9475},{\"end\":10276,\"start\":10182},{\"end\":10489,\"start\":10278},{\"end\":11107,\"start\":10516},{\"end\":11172,\"start\":11109},{\"end\":11687,\"start\":11219},{\"end\":11997,\"start\":11735},{\"end\":12141,\"start\":11999},{\"end\":12475,\"start\":12244},{\"end\":13328,\"start\":12514},{\"end\":13983,\"start\":13330},{\"end\":14630,\"start\":13985},{\"end\":15251,\"start\":14632},{\"end\":15567,\"start\":15253},{\"end\":15987,\"start\":15590},{\"end\":16365,\"start\":15989},{\"end\":16798,\"start\":16378},{\"end\":17116,\"start\":16800},{\"end\":17574,\"start\":17118},{\"end\":17806,\"start\":17576},{\"end\":18333,\"start\":17826},{\"end\":18632,\"start\":18335},{\"end\":18983,\"start\":18652},{\"end\":19217,\"start\":18985},{\"end\":19639,\"start\":19242},{\"end\":19894,\"start\":19666},{\"end\":21105,\"start\":19912},{\"end\":21324,\"start\":21107},{\"end\":21958,\"start\":21349},{\"end\":22640,\"start\":21989}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":10181,\"start\":10119},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11218,\"start\":11173},{\"attributes\":{\"id\":\"formula_2\"},\"end\":11734,\"start\":11688},{\"attributes\":{\"id\":\"formula_3\"},\"end\":12243,\"start\":12142}]", "table_ref": "[{\"end\":19734,\"start\":19727},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":21638,\"start\":21631}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1447,\"start\":1435},{\"attributes\":{\"n\":\"2\"},\"end\":5265,\"start\":5253},{\"attributes\":{\"n\":\"3\"},\"end\":8282,\"start\":8270},{\"attributes\":{\"n\":\"3.1\"},\"end\":8663,\"start\":8645},{\"attributes\":{\"n\":\"3.2\"},\"end\":10514,\"start\":10492},{\"attributes\":{\"n\":\"3.3\"},\"end\":12512,\"start\":12478},{\"attributes\":{\"n\":\"4\"},\"end\":15588,\"start\":15570},{\"attributes\":{\"n\":\"4.1\"},\"end\":16376,\"start\":16368},{\"attributes\":{\"n\":\"4.2\"},\"end\":17824,\"start\":17809},{\"attributes\":{\"n\":\"4.3\"},\"end\":18650,\"start\":18635},{\"end\":19240,\"start\":19220},{\"attributes\":{\"n\":\"5\"},\"end\":19664,\"start\":19642},{\"attributes\":{\"n\":\"5.1\"},\"end\":19910,\"start\":19897},{\"attributes\":{\"n\":\"5.2\"},\"end\":21347,\"start\":21327},{\"attributes\":{\"n\":\"6\"},\"end\":21987,\"start\":21961},{\"end\":22652,\"start\":22642},{\"end\":22968,\"start\":22958},{\"end\":24530,\"start\":24521}]", "table": "[{\"end\":24229,\"start\":23579},{\"end\":24519,\"start\":24387}]", "figure_caption": "[{\"end\":22956,\"start\":22654},{\"end\":23329,\"start\":22970},{\"end\":23579,\"start\":23332},{\"end\":24387,\"start\":24232},{\"end\":24674,\"start\":24532}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":9836,\"start\":9828},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10008,\"start\":9998}]", "bib_author_first_name": "[{\"end\":24938,\"start\":24933},{\"end\":24947,\"start\":24943},{\"end\":25207,\"start\":25200},{\"end\":25223,\"start\":25214},{\"end\":25236,\"start\":25230},{\"end\":25249,\"start\":25245},{\"end\":25580,\"start\":25574},{\"end\":25596,\"start\":25590},{\"end\":25610,\"start\":25606},{\"end\":25625,\"start\":25621},{\"end\":25896,\"start\":25888},{\"end\":25914,\"start\":25910},{\"end\":25929,\"start\":25923},{\"end\":26293,\"start\":26288},{\"end\":26310,\"start\":26302},{\"end\":26324,\"start\":26318},{\"end\":26338,\"start\":26330},{\"end\":26691,\"start\":26687},{\"end\":26705,\"start\":26701},{\"end\":26716,\"start\":26712},{\"end\":26732,\"start\":26726},{\"end\":26749,\"start\":26742},{\"end\":26764,\"start\":26758},{\"end\":26766,\"start\":26765},{\"end\":26779,\"start\":26772},{\"end\":26795,\"start\":26788},{\"end\":26809,\"start\":26805},{\"end\":26811,\"start\":26810},{\"end\":27116,\"start\":27112},{\"end\":27278,\"start\":27274},{\"end\":27448,\"start\":27444},{\"end\":27525,\"start\":27521},{\"end\":27536,\"start\":27531},{\"end\":27549,\"start\":27542},{\"end\":27551,\"start\":27550},{\"end\":27959,\"start\":27956},{\"end\":27969,\"start\":27964},{\"end\":28680,\"start\":28677},{\"end\":28689,\"start\":28685},{\"end\":28704,\"start\":28699},{\"end\":28720,\"start\":28713},{\"end\":28731,\"start\":28726},{\"end\":29219,\"start\":29211},{\"end\":29233,\"start\":29228},{\"end\":29247,\"start\":29243},{\"end\":29493,\"start\":29490},{\"end\":29514,\"start\":29508},{\"end\":29533,\"start\":29529},{\"end\":29760,\"start\":29756},{\"end\":30177,\"start\":30173},{\"end\":30192,\"start\":30183},{\"end\":30194,\"start\":30193},{\"end\":30606,\"start\":30602},{\"end\":30804,\"start\":30800},{\"end\":30816,\"start\":30812},{\"end\":30818,\"start\":30817},{\"end\":30831,\"start\":30827},{\"end\":30833,\"start\":30832},{\"end\":31082,\"start\":31079},{\"end\":31091,\"start\":31087},{\"end\":31103,\"start\":31099},{\"end\":31121,\"start\":31116},{\"end\":31133,\"start\":31129},{\"end\":31139,\"start\":31134},{\"end\":31374,\"start\":31368},{\"end\":31387,\"start\":31380},{\"end\":31400,\"start\":31392},{\"end\":31410,\"start\":31407},{\"end\":31426,\"start\":31418},{\"end\":31441,\"start\":31432},{\"end\":31873,\"start\":31868},{\"end\":31889,\"start\":31883},{\"end\":31905,\"start\":31900},{\"end\":31917,\"start\":31914},{\"end\":31935,\"start\":31928},{\"end\":32313,\"start\":32308},{\"end\":32329,\"start\":32323},{\"end\":32345,\"start\":32340},{\"end\":32357,\"start\":32354},{\"end\":32375,\"start\":32368},{\"end\":32781,\"start\":32774},{\"end\":32794,\"start\":32790},{\"end\":32809,\"start\":32804},{\"end\":32821,\"start\":32817},{\"end\":32842,\"start\":32831},{\"end\":32856,\"start\":32850},{\"end\":32866,\"start\":32862},{\"end\":33576,\"start\":33572},{\"end\":33593,\"start\":33586},{\"end\":33850,\"start\":33844},{\"end\":33871,\"start\":33864},{\"end\":33892,\"start\":33886},{\"end\":33910,\"start\":33903},{\"end\":33922,\"start\":33916},{\"end\":33937,\"start\":33932},{\"end\":33955,\"start\":33949},{\"end\":34533,\"start\":34526},{\"end\":34547,\"start\":34542},{\"end\":34549,\"start\":34548},{\"end\":34560,\"start\":34555},{\"end\":34578,\"start\":34570},{\"end\":35097,\"start\":35090},{\"end\":35117,\"start\":35106},{\"end\":35119,\"start\":35118},{\"end\":35135,\"start\":35129},{\"end\":35137,\"start\":35136},{\"end\":35612,\"start\":35605},{\"end\":35625,\"start\":35621},{\"end\":35641,\"start\":35637},{\"end\":35651,\"start\":35646},{\"end\":35671,\"start\":35660},{\"end\":35673,\"start\":35672},{\"end\":35689,\"start\":35683},{\"end\":35705,\"start\":35694},{\"end\":36188,\"start\":36181},{\"end\":36203,\"start\":36195},{\"end\":36219,\"start\":36214},{\"end\":36569,\"start\":36563},{\"end\":36592,\"start\":36585},{\"end\":36601,\"start\":36598},{\"end\":36612,\"start\":36607},{\"end\":36622,\"start\":36617},{\"end\":36932,\"start\":36928},{\"end\":36947,\"start\":36939},{\"end\":36961,\"start\":36955},{\"end\":36976,\"start\":36971},{\"end\":36987,\"start\":36983},{\"end\":37000,\"start\":36994},{\"end\":37002,\"start\":37001},{\"end\":37412,\"start\":37404},{\"end\":37424,\"start\":37419},{\"end\":37775,\"start\":37769},{\"end\":37786,\"start\":37782},{\"end\":37798,\"start\":37794},{\"end\":38282,\"start\":38277},{\"end\":38299,\"start\":38293},{\"end\":38314,\"start\":38308},{\"end\":38316,\"start\":38315},{\"end\":38565,\"start\":38559},{\"end\":38575,\"start\":38570},{\"end\":38584,\"start\":38580},{\"end\":38597,\"start\":38591},{\"end\":38894,\"start\":38886},{\"end\":38905,\"start\":38899},{\"end\":38922,\"start\":38916},{\"end\":38933,\"start\":38928},{\"end\":39429,\"start\":39428},{\"end\":39618,\"start\":39613},{\"end\":39631,\"start\":39626},{\"end\":39642,\"start\":39638},{\"end\":39987,\"start\":39983},{\"end\":40000,\"start\":39994},{\"end\":40012,\"start\":40005},{\"end\":40027,\"start\":40020},{\"end\":40039,\"start\":40032},{\"end\":40047,\"start\":40045}]", "bib_author_last_name": "[{\"end\":24941,\"start\":24939},{\"end\":24955,\"start\":24948},{\"end\":25212,\"start\":25208},{\"end\":25228,\"start\":25224},{\"end\":25243,\"start\":25237},{\"end\":25254,\"start\":25250},{\"end\":25588,\"start\":25581},{\"end\":25604,\"start\":25597},{\"end\":25619,\"start\":25611},{\"end\":25631,\"start\":25626},{\"end\":25908,\"start\":25897},{\"end\":25921,\"start\":25915},{\"end\":25936,\"start\":25930},{\"end\":26300,\"start\":26294},{\"end\":26316,\"start\":26311},{\"end\":26328,\"start\":26325},{\"end\":26348,\"start\":26339},{\"end\":26699,\"start\":26692},{\"end\":26710,\"start\":26706},{\"end\":26724,\"start\":26717},{\"end\":26740,\"start\":26733},{\"end\":26756,\"start\":26750},{\"end\":26770,\"start\":26767},{\"end\":26786,\"start\":26780},{\"end\":26803,\"start\":26796},{\"end\":26823,\"start\":26812},{\"end\":27125,\"start\":27117},{\"end\":27285,\"start\":27279},{\"end\":27452,\"start\":27449},{\"end\":27529,\"start\":27526},{\"end\":27540,\"start\":27537},{\"end\":27557,\"start\":27552},{\"end\":27962,\"start\":27960},{\"end\":27973,\"start\":27970},{\"end\":28683,\"start\":28681},{\"end\":28697,\"start\":28690},{\"end\":28711,\"start\":28705},{\"end\":28724,\"start\":28721},{\"end\":28735,\"start\":28732},{\"end\":29226,\"start\":29220},{\"end\":29241,\"start\":29234},{\"end\":29252,\"start\":29248},{\"end\":29506,\"start\":29494},{\"end\":29527,\"start\":29515},{\"end\":29541,\"start\":29534},{\"end\":29764,\"start\":29761},{\"end\":30181,\"start\":30178},{\"end\":30199,\"start\":30195},{\"end\":30617,\"start\":30607},{\"end\":30810,\"start\":30805},{\"end\":30825,\"start\":30819},{\"end\":30839,\"start\":30834},{\"end\":31085,\"start\":31083},{\"end\":31097,\"start\":31092},{\"end\":31114,\"start\":31104},{\"end\":31127,\"start\":31122},{\"end\":31144,\"start\":31140},{\"end\":31378,\"start\":31375},{\"end\":31390,\"start\":31388},{\"end\":31405,\"start\":31401},{\"end\":31416,\"start\":31411},{\"end\":31430,\"start\":31427},{\"end\":31447,\"start\":31442},{\"end\":31881,\"start\":31874},{\"end\":31898,\"start\":31890},{\"end\":31912,\"start\":31906},{\"end\":31926,\"start\":31918},{\"end\":31945,\"start\":31936},{\"end\":32321,\"start\":32314},{\"end\":32338,\"start\":32330},{\"end\":32352,\"start\":32346},{\"end\":32366,\"start\":32358},{\"end\":32385,\"start\":32376},{\"end\":32788,\"start\":32782},{\"end\":32802,\"start\":32795},{\"end\":32815,\"start\":32810},{\"end\":32829,\"start\":32822},{\"end\":32848,\"start\":32843},{\"end\":32860,\"start\":32857},{\"end\":32878,\"start\":32867},{\"end\":33584,\"start\":33577},{\"end\":33604,\"start\":33594},{\"end\":33862,\"start\":33851},{\"end\":33884,\"start\":33872},{\"end\":33901,\"start\":33893},{\"end\":33914,\"start\":33911},{\"end\":33930,\"start\":33923},{\"end\":33947,\"start\":33938},{\"end\":33962,\"start\":33956},{\"end\":34540,\"start\":34534},{\"end\":34553,\"start\":34550},{\"end\":34568,\"start\":34561},{\"end\":34581,\"start\":34579},{\"end\":35104,\"start\":35098},{\"end\":35127,\"start\":35120},{\"end\":35140,\"start\":35138},{\"end\":35619,\"start\":35613},{\"end\":35635,\"start\":35626},{\"end\":35644,\"start\":35642},{\"end\":35658,\"start\":35652},{\"end\":35681,\"start\":35674},{\"end\":35692,\"start\":35690},{\"end\":35711,\"start\":35706},{\"end\":36193,\"start\":36189},{\"end\":36212,\"start\":36204},{\"end\":36223,\"start\":36220},{\"end\":36583,\"start\":36570},{\"end\":36596,\"start\":36593},{\"end\":36605,\"start\":36602},{\"end\":36615,\"start\":36613},{\"end\":36627,\"start\":36623},{\"end\":36634,\"start\":36629},{\"end\":36937,\"start\":36933},{\"end\":36953,\"start\":36948},{\"end\":36969,\"start\":36962},{\"end\":36981,\"start\":36977},{\"end\":36992,\"start\":36988},{\"end\":37009,\"start\":37003},{\"end\":37417,\"start\":37413},{\"end\":37431,\"start\":37425},{\"end\":37780,\"start\":37776},{\"end\":37792,\"start\":37787},{\"end\":37806,\"start\":37799},{\"end\":38291,\"start\":38283},{\"end\":38306,\"start\":38300},{\"end\":38568,\"start\":38566},{\"end\":38578,\"start\":38576},{\"end\":38589,\"start\":38585},{\"end\":38601,\"start\":38598},{\"end\":38897,\"start\":38895},{\"end\":38914,\"start\":38906},{\"end\":38926,\"start\":38923},{\"end\":38937,\"start\":38934},{\"end\":39437,\"start\":39430},{\"end\":39445,\"start\":39439},{\"end\":39624,\"start\":39619},{\"end\":39636,\"start\":39632},{\"end\":39648,\"start\":39643},{\"end\":39992,\"start\":39988},{\"end\":40003,\"start\":40001},{\"end\":40018,\"start\":40013},{\"end\":40030,\"start\":40028},{\"end\":40043,\"start\":40040},{\"end\":40050,\"start\":40048}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":11536917},\"end\":25148,\"start\":24895},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":51918897},\"end\":25572,\"start\":25150},{\"attributes\":{\"doi\":\"arXiv:1606.01781\",\"id\":\"b2\"},\"end\":25853,\"start\":25574},{\"attributes\":{\"doi\":\"arXiv:1602.02830\",\"id\":\"b3\"},\"end\":26286,\"start\":25855},{\"attributes\":{\"doi\":\"arXiv:1810.04805\",\"id\":\"b4\"},\"end\":26621,\"start\":26288},{\"attributes\":{\"doi\":\"arXiv:1803.07640\",\"id\":\"b5\"},\"end\":27072,\"start\":26623},{\"attributes\":{\"doi\":\"arXiv:1901.05287\",\"id\":\"b6\"},\"end\":27219,\"start\":27074},{\"attributes\":{\"doi\":\"arXiv:1308.0850\",\"id\":\"b7\"},\"end\":27392,\"start\":27221},{\"attributes\":{\"id\":\"b8\"},\"end\":27519,\"start\":27394},{\"attributes\":{\"doi\":\"arXiv:1510.00149\",\"id\":\"b9\"},\"end\":27856,\"start\":27521},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":16787742},\"end\":28539,\"start\":27858},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":1090122},\"end\":29209,\"start\":28541},{\"attributes\":{\"doi\":\"arXiv:1503.02531\",\"id\":\"b12\"},\"end\":29432,\"start\":29211},{\"attributes\":{\"doi\":\"arXiv:1404.2188\",\"id\":\"b13\"},\"end\":29695,\"start\":29434},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":9672033},\"end\":30133,\"start\":29697},{\"attributes\":{\"id\":\"b15\"},\"end\":30545,\"start\":30135},{\"attributes\":{\"id\":\"b16\"},\"end\":30776,\"start\":30547},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":7785881},\"end\":31037,\"start\":30778},{\"attributes\":{\"doi\":\"arXiv:1608.08710\",\"id\":\"b18\"},\"end\":31298,\"start\":31039},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":5993328},\"end\":31819,\"start\":31300},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":17048224},\"end\":32251,\"start\":31821},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":14850173},\"end\":32730,\"start\":32253},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":3626819},\"end\":33509,\"start\":32732},{\"attributes\":{\"id\":\"b23\"},\"end\":33842,\"start\":33511},{\"attributes\":{\"doi\":\"arXiv:1603.06807\",\"id\":\"b24\"},\"end\":34333,\"start\":33844},{\"attributes\":{\"id\":\"b25\"},\"end\":34524,\"start\":34335},{\"attributes\":{\"id\":\"b26\"},\"end\":34991,\"start\":34526},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":9923502},\"end\":35379,\"start\":34993},{\"attributes\":{\"id\":\"b28\"},\"end\":35524,\"start\":35381},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":990233},\"end\":36179,\"start\":35526},{\"attributes\":{\"doi\":\"arXiv:1811.03060\",\"id\":\"b30\"},\"end\":36468,\"start\":36181},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":3259607},\"end\":36926,\"start\":36470},{\"attributes\":{\"doi\":\"arXiv:1804.07461\",\"id\":\"b32\"},\"end\":37316,\"start\":36928},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":10817864},\"end\":37698,\"start\":37318},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":9395040},\"end\":38181,\"start\":37700},{\"attributes\":{\"doi\":\"arXiv:1704.05426\",\"id\":\"b35\"},\"end\":38495,\"start\":38183},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":3603886},\"end\":38829,\"start\":38497},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":52010945},\"end\":39382,\"start\":38831},{\"attributes\":{\"doi\":\"arXiv:1212.5701\",\"id\":\"b38\"},\"end\":39547,\"start\":39384},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":368182},\"end\":39884,\"start\":39549},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":11270374},\"end\":40566,\"start\":39886}]", "bib_title": "[{\"end\":24931,\"start\":24895},{\"end\":25198,\"start\":25150},{\"end\":25886,\"start\":25855},{\"end\":27954,\"start\":27858},{\"end\":28675,\"start\":28541},{\"end\":29754,\"start\":29697},{\"end\":30171,\"start\":30135},{\"end\":30798,\"start\":30778},{\"end\":31366,\"start\":31300},{\"end\":31866,\"start\":31821},{\"end\":32306,\"start\":32253},{\"end\":32772,\"start\":32732},{\"end\":35088,\"start\":34993},{\"end\":35603,\"start\":35526},{\"end\":36561,\"start\":36470},{\"end\":37402,\"start\":37318},{\"end\":37767,\"start\":37700},{\"end\":38557,\"start\":38497},{\"end\":38884,\"start\":38831},{\"end\":39611,\"start\":39549},{\"end\":39981,\"start\":39886}]", "bib_author": "[{\"end\":24943,\"start\":24933},{\"end\":24957,\"start\":24943},{\"end\":25214,\"start\":25200},{\"end\":25230,\"start\":25214},{\"end\":25245,\"start\":25230},{\"end\":25256,\"start\":25245},{\"end\":25590,\"start\":25574},{\"end\":25606,\"start\":25590},{\"end\":25621,\"start\":25606},{\"end\":25633,\"start\":25621},{\"end\":25910,\"start\":25888},{\"end\":25923,\"start\":25910},{\"end\":25938,\"start\":25923},{\"end\":26302,\"start\":26288},{\"end\":26318,\"start\":26302},{\"end\":26330,\"start\":26318},{\"end\":26350,\"start\":26330},{\"end\":26701,\"start\":26687},{\"end\":26712,\"start\":26701},{\"end\":26726,\"start\":26712},{\"end\":26742,\"start\":26726},{\"end\":26758,\"start\":26742},{\"end\":26772,\"start\":26758},{\"end\":26788,\"start\":26772},{\"end\":26805,\"start\":26788},{\"end\":26825,\"start\":26805},{\"end\":27127,\"start\":27112},{\"end\":27287,\"start\":27274},{\"end\":27454,\"start\":27444},{\"end\":27531,\"start\":27521},{\"end\":27542,\"start\":27531},{\"end\":27559,\"start\":27542},{\"end\":27964,\"start\":27956},{\"end\":27975,\"start\":27964},{\"end\":28685,\"start\":28677},{\"end\":28699,\"start\":28685},{\"end\":28713,\"start\":28699},{\"end\":28726,\"start\":28713},{\"end\":28737,\"start\":28726},{\"end\":29228,\"start\":29211},{\"end\":29243,\"start\":29228},{\"end\":29254,\"start\":29243},{\"end\":29508,\"start\":29490},{\"end\":29529,\"start\":29508},{\"end\":29543,\"start\":29529},{\"end\":29766,\"start\":29756},{\"end\":30183,\"start\":30173},{\"end\":30201,\"start\":30183},{\"end\":30619,\"start\":30602},{\"end\":30812,\"start\":30800},{\"end\":30827,\"start\":30812},{\"end\":30841,\"start\":30827},{\"end\":31087,\"start\":31079},{\"end\":31099,\"start\":31087},{\"end\":31116,\"start\":31099},{\"end\":31129,\"start\":31116},{\"end\":31146,\"start\":31129},{\"end\":31380,\"start\":31368},{\"end\":31392,\"start\":31380},{\"end\":31407,\"start\":31392},{\"end\":31418,\"start\":31407},{\"end\":31432,\"start\":31418},{\"end\":31449,\"start\":31432},{\"end\":31883,\"start\":31868},{\"end\":31900,\"start\":31883},{\"end\":31914,\"start\":31900},{\"end\":31928,\"start\":31914},{\"end\":31947,\"start\":31928},{\"end\":32323,\"start\":32308},{\"end\":32340,\"start\":32323},{\"end\":32354,\"start\":32340},{\"end\":32368,\"start\":32354},{\"end\":32387,\"start\":32368},{\"end\":32790,\"start\":32774},{\"end\":32804,\"start\":32790},{\"end\":32817,\"start\":32804},{\"end\":32831,\"start\":32817},{\"end\":32850,\"start\":32831},{\"end\":32862,\"start\":32850},{\"end\":32880,\"start\":32862},{\"end\":33586,\"start\":33572},{\"end\":33606,\"start\":33586},{\"end\":33864,\"start\":33844},{\"end\":33886,\"start\":33864},{\"end\":33903,\"start\":33886},{\"end\":33916,\"start\":33903},{\"end\":33932,\"start\":33916},{\"end\":33949,\"start\":33932},{\"end\":33964,\"start\":33949},{\"end\":34542,\"start\":34526},{\"end\":34555,\"start\":34542},{\"end\":34570,\"start\":34555},{\"end\":34583,\"start\":34570},{\"end\":35106,\"start\":35090},{\"end\":35129,\"start\":35106},{\"end\":35142,\"start\":35129},{\"end\":35621,\"start\":35605},{\"end\":35637,\"start\":35621},{\"end\":35646,\"start\":35637},{\"end\":35660,\"start\":35646},{\"end\":35683,\"start\":35660},{\"end\":35694,\"start\":35683},{\"end\":35713,\"start\":35694},{\"end\":36195,\"start\":36181},{\"end\":36214,\"start\":36195},{\"end\":36225,\"start\":36214},{\"end\":36585,\"start\":36563},{\"end\":36598,\"start\":36585},{\"end\":36607,\"start\":36598},{\"end\":36617,\"start\":36607},{\"end\":36629,\"start\":36617},{\"end\":36636,\"start\":36629},{\"end\":36939,\"start\":36928},{\"end\":36955,\"start\":36939},{\"end\":36971,\"start\":36955},{\"end\":36983,\"start\":36971},{\"end\":36994,\"start\":36983},{\"end\":37011,\"start\":36994},{\"end\":37419,\"start\":37404},{\"end\":37433,\"start\":37419},{\"end\":37782,\"start\":37769},{\"end\":37794,\"start\":37782},{\"end\":37808,\"start\":37794},{\"end\":38293,\"start\":38277},{\"end\":38308,\"start\":38293},{\"end\":38319,\"start\":38308},{\"end\":38570,\"start\":38559},{\"end\":38580,\"start\":38570},{\"end\":38591,\"start\":38580},{\"end\":38603,\"start\":38591},{\"end\":38899,\"start\":38886},{\"end\":38916,\"start\":38899},{\"end\":38928,\"start\":38916},{\"end\":38939,\"start\":38928},{\"end\":39439,\"start\":39428},{\"end\":39447,\"start\":39439},{\"end\":39626,\"start\":39613},{\"end\":39638,\"start\":39626},{\"end\":39650,\"start\":39638},{\"end\":39994,\"start\":39983},{\"end\":40005,\"start\":39994},{\"end\":40020,\"start\":40005},{\"end\":40032,\"start\":40020},{\"end\":40045,\"start\":40032},{\"end\":40052,\"start\":40045}]", "bib_venue": "[{\"end\":25371,\"start\":25322},{\"end\":28246,\"start\":28119},{\"end\":28892,\"start\":28823},{\"end\":29941,\"start\":29862},{\"end\":30360,\"start\":30289},{\"end\":31570,\"start\":31518},{\"end\":33151,\"start\":33024},{\"end\":35185,\"start\":35172},{\"end\":35872,\"start\":35801},{\"end\":37957,\"start\":37891},{\"end\":39142,\"start\":39049},{\"end\":40255,\"start\":40162},{\"end\":25006,\"start\":24957},{\"end\":25320,\"start\":25256},{\"end\":25705,\"start\":25649},{\"end\":26063,\"start\":25954},{\"end\":26446,\"start\":26366},{\"end\":26685,\"start\":26623},{\"end\":27110,\"start\":27074},{\"end\":27272,\"start\":27221},{\"end\":27442,\"start\":27394},{\"end\":27679,\"start\":27575},{\"end\":28117,\"start\":27975},{\"end\":28821,\"start\":28737},{\"end\":29314,\"start\":29270},{\"end\":29488,\"start\":29434},{\"end\":29860,\"start\":29766},{\"end\":30287,\"start\":30201},{\"end\":30600,\"start\":30547},{\"end\":30890,\"start\":30841},{\"end\":31077,\"start\":31039},{\"end\":31516,\"start\":31449},{\"end\":32027,\"start\":31947},{\"end\":32473,\"start\":32387},{\"end\":33022,\"start\":32880},{\"end\":33570,\"start\":33511},{\"end\":34079,\"start\":33980},{\"end\":34378,\"start\":34335},{\"end\":34740,\"start\":34583},{\"end\":35170,\"start\":35142},{\"end\":35437,\"start\":35381},{\"end\":35799,\"start\":35713},{\"end\":36317,\"start\":36241},{\"end\":36688,\"start\":36636},{\"end\":37112,\"start\":37027},{\"end\":37494,\"start\":37433},{\"end\":37889,\"start\":37808},{\"end\":38275,\"start\":38183},{\"end\":38655,\"start\":38603},{\"end\":39047,\"start\":38939},{\"end\":39426,\"start\":39384},{\"end\":39699,\"start\":39650},{\"end\":40160,\"start\":40052}]"}}}, "year": 2023, "month": 12, "day": 17}