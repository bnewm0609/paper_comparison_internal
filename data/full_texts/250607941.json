{"id": 250607941, "updated": "2023-10-05 12:42:05.512", "metadata": {"title": "Plex: Towards Reliability using Pretrained Large Model Extensions", "authors": "[{\"first\":\"Dustin\",\"last\":\"Tran\",\"middle\":[]},{\"first\":\"Jeremiah\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Michael\",\"last\":\"Dusenberry\",\"middle\":[\"W.\"]},{\"first\":\"Du\",\"last\":\"Phan\",\"middle\":[]},{\"first\":\"Mark\",\"last\":\"Collier\",\"middle\":[]},{\"first\":\"Jie\",\"last\":\"Ren\",\"middle\":[]},{\"first\":\"Kehang\",\"last\":\"Han\",\"middle\":[]},{\"first\":\"Zi\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Zelda\",\"last\":\"Mariet\",\"middle\":[]},{\"first\":\"Huiyi\",\"last\":\"Hu\",\"middle\":[]},{\"first\":\"Neil\",\"last\":\"Band\",\"middle\":[]},{\"first\":\"Tim\",\"last\":\"Rudner\",\"middle\":[\"G.\",\"J.\"]},{\"first\":\"Karan\",\"last\":\"Singhal\",\"middle\":[]},{\"first\":\"Zachary\",\"last\":\"Nado\",\"middle\":[]},{\"first\":\"Joost\",\"last\":\"Amersfoort\",\"middle\":[\"van\"]},{\"first\":\"Andreas\",\"last\":\"Kirsch\",\"middle\":[]},{\"first\":\"Rodolphe\",\"last\":\"Jenatton\",\"middle\":[]},{\"first\":\"Nithum\",\"last\":\"Thain\",\"middle\":[]},{\"first\":\"Honglin\",\"last\":\"Yuan\",\"middle\":[]},{\"first\":\"Kelly\",\"last\":\"Buchanan\",\"middle\":[]},{\"first\":\"Kevin\",\"last\":\"Murphy\",\"middle\":[]},{\"first\":\"D.\",\"last\":\"Sculley\",\"middle\":[]},{\"first\":\"Yarin\",\"last\":\"Gal\",\"middle\":[]},{\"first\":\"Zoubin\",\"last\":\"Ghahramani\",\"middle\":[]},{\"first\":\"Jasper\",\"last\":\"Snoek\",\"middle\":[]},{\"first\":\"Balaji\",\"last\":\"Lakshminarayanan\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "A recent trend in artificial intelligence is the use of pretrained models for language and vision tasks, which have achieved extraordinary performance but also puzzling failures. Probing these models' abilities in diverse ways is therefore critical to the field. In this paper, we explore the reliability of models, where we define a reliable model as one that not only achieves strong predictive performance but also performs well consistently over many decision-making tasks involving uncertainty (e.g., selective prediction, open set recognition), robust generalization (e.g., accuracy and proper scoring rules such as log-likelihood on in- and out-of-distribution datasets), and adaptation (e.g., active learning, few-shot uncertainty). We devise 10 types of tasks over 40 datasets in order to evaluate different aspects of reliability on both vision and language domains. To improve reliability, we developed ViT-Plex and T5-Plex, pretrained large model extensions for vision and language modalities, respectively. Plex greatly improves the state-of-the-art across reliability tasks, and simplifies the traditional protocol as it improves the out-of-the-box performance and does not require designing scores or tuning the model for each task. We demonstrate scaling effects over model sizes up to 1B parameters and pretraining dataset sizes up to 4B examples. We also demonstrate Plex's capabilities on challenging tasks including zero-shot open set recognition, active learning, and uncertainty in conversational language understanding.", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2207.07411", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2207-07411", "doi": "10.48550/arxiv.2207.07411"}}, "content": {"source": {"pdf_hash": "9da634823416e96417530f0a2197b9a4936eee3e", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/2207.07411v1.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "41b0b408eb49afee5256735d43d364e2c37f178b", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9da634823416e96417530f0a2197b9a4936eee3e.txt", "contents": "\nPlex: Towards Reliability Using Pretrained Large Model Extensions\n\n\nDustin Tran \nGoogle\n\nJeremiah Liu \nGoogle\n\nMichael W Dusenberry \nGoogle\n\nDu Phan \nGoogle\n\nMark Collier \nGoogle\n\nJie Ren \nGoogle\n\nKehang Han \nZi Wang \nGoogle\n\nZelda Mariet \nGoogle\n\nHuiyi Hu \nGoogle\n\nNeil Band \nGoogle\n\nUniversity of Oxford\n\n\nTim G J Rudner \nUniversity of Oxford\n\n\nKaran Singhal \nGoogle\n\nZachary Nado \nGoogle\n\nJoost Van Amersfoort \nUniversity of Oxford\n\n\nAndreas Kirsch \nUniversity of Oxford\n\n\nRodolphe Jenatton \nGoogle\n\nNithum Thain \nGoogle\n\nHonglin Yuan \nGoogle\n\nKelly Buchanan \nGoogle\n\nKevin Murphy \nGoogle\n\nD Sculley \nGoogle\n\nYarin Gal \nUniversity of Oxford\n\n\nZoubin Ghahramani \nGoogle\n\nJasper Snoek \nGoogle\n\nBalaji Lakshminarayanan \nGoogle\n\nPlex: Towards Reliability Using Pretrained Large Model Extensions\nreliabilitylarge modelsuncertaintyrobustnessadaptation\nA recent trend in artificial intelligence is the use of pretrained models for language and vision tasks, which have achieved extraordinary performance but also puzzling failures. Probing these models' abilities in diverse ways is therefore critical to the field. In this paper, we explore the reliability of models, where we define a reliable model as one that not only achieves strong predictive performance but also performs well consistently over many decision-making tasks involving uncertainty (e.g., selective prediction, open set recognition), robust generalization (e.g., accuracy and proper scoring rules such as log-likelihood on inand out-of-distribution datasets), and adaptation (e.g., active learning, few-shot uncertainty). We devise 10 types of tasks over 40 datasets in order to evaluate different aspects of reliability on both vision and language domains. To improve reliability, we developed ViT-Plex and T5-Plex, pretrained l arge model ex tensions (plex) for vision and language modalities, respectively. Plex greatly improves the state-of-the-art across reliability tasks, and simplifies the traditional protocol as it improves the out-of-the-box performance and does not require designing scores or tuning the model for each task. We demonstrate scaling effects over model sizes up to 1B parameters and pretraining dataset sizes up to 4B examples. We also demonstrate Plex's capabilities on challenging tasks including zero-shot open set recognition, active learning, and uncertainty in conversational language understanding. 1\n\nReliability as a Goal for Artificial Intelligence\n\nOver the past few years, the deep learning approach to artificial intelligence (AI) has made significant progress on benchmark tasks across domains such as computer vision (Dosovitskiy et al., 2021) and natural language processing (Raffel et al., 2020;Brown et al., 2020). With this progress, there is unfettered excitement about the potential of AI to have a transformative impact on applications ranging from medical diagnoses with a human-in-the loop, to using AI to detect online misinformation and toxicity, and to perhaps a pathway for artificial general intelligence. While hypothesizing about this potential is important, we highlight that the typical tasks where deep learning has been most successful have Figure 1: Desiderata for a Reliable model. We propose to simultaneously stress-test the \"out-of-the-box\" model performance (i.e. the predictive probability distribution p(y|x)) across a suite of uncertainty, robust generalization, and adaptation benchmarks, without any customization for individual tasks.\n\nbeen carefully devised to fit within narrow boundaries-for example, a focus on predictive performance with test inputs close to the data on which the model was trained.\n\nTo go beyond these limitations, we argue that the ability of models to make reliable decisions is critical to the deeper integration of AI in the real world. Here, we define reliability as the ability for a model to work consistently across real-world settings. We borrow the term from reliability engineering (Barlow and Proschan, 1975;O'Connor and Kleyner, 2012), a discipline of engineering involving risk assessment, testability, and fault tolerance. Related nomenclature include robustness (Russell et al., 2015), safety (Amodei et al., 2016;Everitt et al., 2018;Hendrycks et al., 2021b), calibration (Dawid, 1982), credibility (D'Amour et al., 2020), and trustworthiness (Avin et al., 2021), each with their own broad and intersecting scopes.\n\n\nDesiderata for Reliability\n\nIt is common practice in machine learning research to focus on measures of performance based on the accuracy on a test set drawn from the same distribution as the training set, the so-called independent and identically distributed (i.i.d.) assumption. However, this does not capture the real-world deployment of AI systems, where often the testing environment is very different from the training environment, and where the tasks only indirectly involve accuracy measures. The emphasis in our paper is on how reliable an AI system is in broad array of scenarios. We posit three general categories of desiderata for reliable AI systems: they should represent their own uncertainty, they should generalize robustly to new scenarios, and they should be able to efficiently adapt to new data. Importantly, the aim for a reliable model is to do well in all of these areas simultaneously out-of-the-box without requiring any customization for individual tasks (Figure 1):\n\n1. Uncertainty involves imperfect or unknown information where it is impossible to exactly describe an existing state (Ghahramani, 2015). Predictive uncertainty quantification enables practitioners to know when to trust the model's predictions, thereby enabling graceful failures when the model is likely to be wrong. A variety of measurements can be used to quantify the quality of uncertainty such as expected calibration error (Naeini et al., 2015;Nixon et al., 2019), which measures how well the model's confidence aligns with its accuracy. Providing a quantification of uncertainty also enables better decision making (Parmigiani and Inoue, 2009); one popular setting is selective prediction where a model may defer its prediction to human experts when it is not confident. Another popular task is open set recognition 2 , where the model encounters inputs from new classes at test time that were not seen during training, and the goal is to reliably detect that such inputs do not belong to any of the training classes.\n\n2. Robust Generalization involves an estimate or forecast about an unseen event (Abraham and Ledolter, 1983;Tran et al., 2020). Prediction quality is typically measured using accuracy (e.g., top-1 error for classification problems and mean squared error for regression problems) and proper scoring rules such as log likelihood and Brier score (Gneiting and Raftery, 2007). In the real world, we care not only about metrics on new data obtained from the same distribution the model was trained on (i.i.d.), but also about robustness, as measured by metrics on data under out-of-distribution shifts such as covariate or subpopulation shift.\n\n3. Adaptation involves probing the model's abilities over the course of its learning process. Benchmarks typically evaluate on static datasets with pre-defined train-test splits. However, in many applications, we are interested in models that can quickly adapt to new data and efficiently learn with as few labeled examples as possible. Examples include few-shot learning (Li et al., 2006), where the model learns from a small set of examples; active learning (Settles, 2009), where the model not only learns but also participates in acquiring the data to learn from; and lifelong learning (Thrun, 1998), where the model learns over a sequence of tasks and must not forget about relevant information for previous tasks.\n\nWhile there has been initial progress on specific tasks within reliability, there remain critical limitations. First, prior work has typically focused on narrow settings. For example, much focus has been on accuracy and calibration error on ImageNet and its corrupted versions as a benchmark task (Hendrycks and Dietterich, 2019; Ovadia et al., 2019). Similarly, the literature on open set recognition has led to the development of methods that focus solely on improving open set recognition, often at the expense of other downstream tasks. This has led to a fragmentation of research techniques, and it remains unknown how well advances on these individual tasks connect to a broader set of tasks and datasets. The goal of this work is to move towards a notion of general reliability by seeking models that work well across many decision-making tasks. rather than needing to design, train, and tune a model for each individual task. The approach is philosophically inspired by the trend in natural Plex is able to better identify cases where it is likely to be wrong than the baseline, and thus achieves higher Collaborative AUC. (middle) Plex is robust while a baseline latches onto spurious features such as \"destination\" and \"around the world\". (right) Plex enables structured open set recognition. This provides nuanced clarifications, where Plex can distinguish cases where only part of a request is not supported. language processing where evaluating on multiple tasks (cf. GLUE (Wang et al., 2018a)) has encouraged the community to focus on developing general-purpose techniques.\n\n\nApproaches to Reliability\n\nPrior work has investigated a variety of approaches to improve narrower definitions of reliability. From the literature, several overarching dimensions arise (Tran et al., 2020)-such as the importance of model size; model inductive biases (e.g., architecture); and the combination of multiple models (e.g., ensembles and Bayesian neural networks). There is not yet an understanding of how these dimensions interact ( Figure 3: ViT-Plex (left) and T5-Plex (right) evaluated on a highlighted set of reliability tasks. We also display the state-of-the-art for each task. ViT-Plex and T5-Plex significantly improve state-of-the-art across multiple tasks. Importantly, Plex unifies reliability performance under one general model for vision and language respectively as opposed to specific techniques for each downstream task. References are listed in Appendix B.\n\na surprise that combining certain methods is detrimental, e.g., ). We investigate how to combine these dimensions to form an overall model for reliability.\n\nModern AI is trending towards training a single large model on a large and diverse data set, known as pretraining, and then applying the model to a wide variety of related downstream tasks (Brown et al., 2020;Kolesnikov et al., 2020;Radford et al., 2021;Thoppilan et al., 2022). This often improves over task-specific state-of-the-art in predictive performance, with many considering such large scale models to represent a \"paradigm shift\" in ML (Bommasani et al., 2021). Large-scale pretrained models have also significantly improved state-of-the-art on narrower tasks such as accuracy and calibration under covariate shift (Minderer et al., 2021) and open set recognition . Given these initial promising results, we use large pretrained models as a building block for reliability. However, large models are compute-intensive and often operate at a different scale than has been studied in many uncertainty and robustness papers. This warrants revisiting existing recipes in this new context, where we focus on methods that scale efficiently to large models.\n\n\nContributions\n\nFirst, we define and evaluate reliability in a comprehensive fashion. We use 10 types of tasks in order to capture the three reliability areas-uncertainty, robust generalization, and adaptation-and so that the tasks measure a diverse set of desirable properties in each area. Together the tasks comprise 40 downstream datasets across vision and natural language modalities: 14 datasets for finetuning (including few-shot and active learning-based adaptation) and 26 datasets for out-of-distribution evaluation. As part of the tasks, we build two new datasets in order to assess areas that we found missing in the literature:  Figure 4: The model and task pipeline. We experiment with approaches for pretraining; given the pretrained model's checkpoint, we experiment with finetuning and adaptation; finally, given the finetuned model's checkpoint, we evaluate downstream metrics.\n\nImageNet Real-H for large-scale label uncertainty evaluation; and NaLUE for uncertainty in conversational language understanding. To improve reliability, we develop ViT-Plex and T5-Plex, building on large pretrained models for vision (ViT;Dosovitskiy et al. (2021)) and language (T5; Raffel et al. (2020)), respectively. We train Plex over model sizes up to 1 billion parameters and pretraining dataset sizes of up to 4 billion examples. Figure 3 illustrates Plex's performance on selected tasks comparing to existing state-of-the-art, which typically is a model specialized for that task. Plex achieves new state-of-the-art on many of the 40 datasets. Importantly, Plex achieves strong performance across all tasks using the out-of-the-box model output without requiring any custom designing or tuning for each task.\n\n\nEvaluating Reliability\n\nTo experiment with a variety of models and tasks, we set up a pipeline for training and evaluation; see Figure 4. It involves three steps: pretrain on a large diverse dataset, finetune on a given dataset's training split, and evaluate over both in-distribution and out-of-distribution datasets using the appropriate task metrics. We experiment with a variety of methods to improve reliability during both pretraining and finetuning.\n\n\nTasks for Benchmarking Reliability\n\nWe evaluate a model's reliability using 10 types of tasks, which we categorize as follows. More detailed descriptions of metrics in each task and datasets are provided in Appendix C.\n\n\nUncertainty\n\n\u2022 Calibration assesses the quality of a model's predicted confidence over a population (Dawid, 1982). It quantifies how well model confidence (the predictive probability of correctness) aligns with model accuracy (the observed probability of correctness). We compute expected calibration error (Naeini et al., 2015) and calibration AUROC (Kivlichan et al., 2021) on 14 image and 10 text datasets.  Figure 5: Types of distribution shift using an illustration of ImageNet dogs. p ind (\u00b7) refers to in-distribution during training and p ood (\u00b7) is out-of-distribution during testing.\n\n\u2022 Selective prediction jointly assesses a model's predictive performance and quality of uncertainty estimates, by abstaining from making predictions when the model is uncertain (El-Yaniv and Wiener, 2010). There is no popular metric that's the de facto standard in the literature, so we experiment with two: Oracle Collaborative Accuracy measures accuracy where any deferred predictions are sent to an oracle as a proxy for human+AI collaboration (Kivlichan et al., 2021); and selective prediction by rejection rate traces a curve of predictive accuracies over the percentage of examples that can be rejected (Band et al., 2021). We examine selective prediction on 4 image and 10 text datasets.\n\n\u2022 Open set recognition assesses how well a model can detect examples belonging to none of the training classes (Geng et al., 2020). This happens in the case of semantic (class) shift, where the test input and label distributions both change, particularly in a structured manner where the output classes change from train to test. For an overview of distribution shifts, see Figure 5, where we train on a distribution p ind (x, y) and we evaluate on another distribution. For example, the training set may consist only of dog images and the new input is a coyote image. We compute AUROC and use maximum softmax probability as a simple and general detection score.\n\n\u2022 Label uncertainty is a type of uncertainty inherent in the data labels. This is a form of data uncertainty-irreducible output noise-which is distinct from uncertainty arising from the choice of model (Dusenberry et al., 2020b). Label uncertainty can arise when, for example, human raters disagree about the label for an ambiguous input. If human disagreement is encoded as a label distribution, we can directly compare our model's predictive distribution to it. We analyze label uncertainty on two image datasets. 3\n\n\nRobust Generalization\n\n\u2022 In-distribution generalization assesses how well a model can make predictions after finetuning on a downstream dataset. In particular, we examine accuracy, negative loglikelihood, and Brier score on the in-distribution test splits of 5 image and 3 text datasets. For binary classification tasks, we also look at AUROC and AUPRC.\n\n\u2022 Covariate shift refers to scenarios where the distribution of inputs changes while the conditional distribution of outputs is unchanged (Sugiyama and Kawanabe, 2012). For example, the training set may include natural dog images and the new input is a drawing of a dog. We use the same metrics as those used for assessing in-distribution generalization.\n\n\u2022 Subpopulation shift refers to scenarios where the distribution of interest is only part of the full distribution seen during training. A common assumption is that data is sampled from individual subpopulation distributions, which are themselves sampled from a meta population distribution (Santurkar et al., 2020;Yuan et al., 2022). For example, the training set may include natural dog images and the test set only includes terriers. In this setting, we aim to improve predictive performance on unseen or long-tail subpopulations.\n\n\nAdaptation\n\n\u2022 Active learning assesses a model's ability to not only learn over a fixed set of data points, but also participate in knowing which data points to learn from in the first place. This procedure assesses a model's label efficiency, where label annotations may be scarce, and so we would like to maximize performance while minimizing the number of labeled data points used (Settles, 2009). We assess accuracy over the total number of acquired examples and apply margin sampling for multi-class uncertainty sampling.\n\n\u2022 Few-shot learning assesses how well a model can make predictions downstream with only a few training examples (Li et al., 2006). We use 9 datasets and evaluate the settings of 1-shot, 5-shot, 10-shot, and 25-shot (x-shot means x examples per class).\n\n\u2022 With few-shot uncertainty, we examine calibration and open set recognition in the few-shot regime. We use all 9 datasets for few-shot learning in order to evaluate calibration, and we use those with OOD datasets for open set recognition. We also perform zero-shot open set recognition by using Mahalanobis distance scoring to detect whether an input is out-of-distribution based on the model's representation layer (Lee et al., 2018).\n\n\nDatasets\n\nWe selected a broad suite of 40 downstream datasets under the tasks, each ranging from several thousand to a million examples. We outline the datasets for each modality.\n\n\nImages\n\nWe're motivated to capture datasets spanning natural web images, specialized domains that are likely rare or unseen in large pretrained models, and with both small and large sizes. To do so, we use 11 datasets which we describe below.\n\n\u2022 CIFAR-10 is a dataset of web images with a training set of 50,000 examples and a test set of 10,000 examples (Krizhevsky et al., 2009 (Welinder et al., 2010), and Cars196 (Krause et al., 2013).\n\nDistribution shift is a common challenge for image problems, and so we cover multiple types for a total of 19 datasets. Table 1 provides an outline. Most notable, there is little work for evaluating label uncertainty, so we propose a large-scale dataset which we call ImageNet ReaL-H. ImageNet ReaL recollects human ratings for the original ImageNet test set (Beyer et al., 2020), and we use its raw data of individual ratings to construct a label distribution representing rater uncertainty for each image. For ImageNet1K, we use 7 datasets for different covariate shifts: ImageNet-A, ImageNet-C, ImageNet-R, ImageNetV2, ImageNet-Vid-Robust, ObjectNet, and YTBB Robust.\n\n\nText\n\nFor text, we consider real-world decision making tasks that are known to deploy machine learning models: natural language inference, toxic comments detection, and conversational language understanding. Natural language inference and toxic comments are binary classification tasks that map a (pair of) natural language sentences to a binary category: entailment or Covariate shift Semantic shift   Label uncertainty Subpopl. shift   CIFAR-10  CIFAR-10-C  CIFAR-100, SVHN CIFAR-10H  SP-CIFAR-10  CIFAR-100  CIFAR-100-C  CIFAR-10, SVHN  -SP-CIFAR-100  ImageNet1K 7 datasets  Places365  ImageNet ReaL-H -RETINA Country Shift Severity Shift --  no entailment, and toxic or non-toxic, respectively. Conversational language understanding is a task common in chatbot design, where the model maps a natural language query to a multi-token prediction of user intents: for example, \"I want to order dinner using Uber Eats\" \u2192 3-token prediction of (FoodDelivery, Uber, Order).\n\n\u2022 For natural language inference, we use the Multi-Genre Natural Language Inference (MNLI) corpus which consists of 433k sentence pairs from a diverse collection of genres (fiction, government report, news magazine articles, etc.) (Williams et al., 2017).\n\n\u2022 For toxic comments detection, we use the WikipediaTalk corpus (Wulczyn et al., 2017) which is composed of roughly 200k English Wikipedia talk page comments between Wikipedia editors across the world.\n\n\u2022 For conversational language understanding, a large-scale corpus for evaluating uncertainty quantification is lacking. We propose a new dataset Natural Language understanding Uncertainty Evaluation (NaLUE) that is a relabelled and aggregated version of three large NLU corpuses: CLINC150 (Larson et al., 2019), Banks77 (Zhang et al., 2021 and HWU64 . NaLUE contains 50k+ utterances spanning 18 verticals, 77 domains, and roughly 260 intents. For this task, the model needs to map each utterance to a 3-token sequence of (vertical name, domain name, intent name).\n\nIn terms of data distribution, MNLI has a balanced distribution both across the genre and across the label class. NaLUE exhibits a slight skewness toward some popular domains for chatbot development (e.g, banking customer service requests). On the other hand, the toxic comments datasets often exhibit extreme label imbalance. Natural language is diverse, fast evolving, and rich in long-tail linguistic phenomena. Therefore out-of-distribution examples, particularly long-tail subpopulations, are pervasive in the real-world deployment environment. In Table 2, we outline a total of 7 out-of-distribution challenge sets. Most notably, we construct three new out-of-distribution shifts for NaLUE. NaLUE-tail contains utterances from 28 low-frequency intents categories in NaLUE. NaLUE Standard-OOS and NaLUE Near-OOS contain utterances that describe out of the scope services, differing in their closeness in distribution to NaLUE.\n\n\nPlex: Pretrained Large model Ex tensions\n\nPlex is the result of an extensive study of the reliability of large pretrained models and their complementarity with existing reliability methods. ViT-Plex and T5-Plex use several key ingredients:\n\n\u2022 Base Transformer architecture. We adopt the Transformer standard of an alternating sequence of attention and feedforward layers. We build on T5 1.1 (Raffel et al., 2020) for text as a Transformer in an encoder-decoder setup where the raw text is tokenized with SentencePiece, and on Vision Transformer (Dosovitskiy et al., 2021) for images in an encoder-only setup where the raw images are effectively tokenized into patches.\n\n\u2022 Model size. We investigate 3 scales of the model size in ViT-Plex: Small (ViT-Plex S; \u223c22 million parameters) has patch size 32, 384 embedding size, 1536 feedforward size, 12 residual blocks, and 6-headed attention; Base (ViT-Plex B; \u223c87 million parameters) has patch size 32, 768 embedding size, 3072 feedforward size, 12 residual blocks, and 12-headed attention; and Large (ViT-Plex L; \u223c325 million parameters) has patch size 32, 1024 embedding size, 4096 feedforward size, 24 residual blocks, and 16-headed attention.\n\nFor T5-Plex, we consider three model sizes: Small (T5-Plex S; \u223c77 million parameters) has 512 embedding size, 8 encoder / decoder blocks, and 6-headed attention; Base (T5-Plex B; \u223c250 million parameters) has 768 embedding size, 12 encoder / decoder blocks, and 12-headed attention; and Large (T5-Plex L; \u223c880 million parameters) has 1024 embedding size, 24 encoder / decoder blocks, and 16-headed attention.\n\n\u2022 Pretraining dataset size. For vision, we scale pretraining from ImageNet-21K to the JFT web dataset on up to 4B images. This mirrors recent work on scaling vision models (Zhai et al., 2021;Pham et al., 2021). For language, we use the C4 dataset which consists of hundreds of gigabytes of English text scraped from the web (Raffel et al., 2020).\n\n\u2022 Efficient ensembling. Ensembles and Bayesian neural nets have shown to be very effective for uncertainty and robustness (Ovadia et al., 2019;Band et al., 2021). To apply ensembling scalably, we use BatchEnsemble (BE) (Wen et al., 2020b) and experiment with its use on both the attention and feedforward layers or on only the feedforward layer. For faster training, we only apply BatchEnsemble at a select number of later layers, similar to mixture of experts models (Riquelme et al., 2021). In both ViT-Plex and T5-Plex, we apply no dropout.\n\n\u2022 Last layer changes. We experiment with two approaches that modify the model's final layer to improve reliability, given a fixed representation (a.k.a. deterministic uncertainty quantification setting (Van Amersfoort et al., 2020)). First, we use a Gaussian process (GP) last-layer, which improves distance-awareness of the decision surface by increasing uncertainty far away from the training representations. We use the GP layer implementation proposed by . In addition, to model input-dependent label noise in datasets with many output classes we apply the Heteroscedastic (Het) method of Collier et al. (2021). For detailed background, see Appendix D.\n\n\u2022 What to apply in pretraining versus finetuning. We apply efficient ensembling during both pretraining and finetuning. For last-layer methods, we find pretraining benefits can be obtained primarily from only applying the method during finetuning, so we restrict them to that setting (detailed in Section 4.2). In addition, due to compute constraints, we exclusively focus on the finetuning-only setting for T5-Plex. That is, T5-Plex models are initialized from the official pretrained T5 checkpoints, and we apply efficient ensembling and last layer changes during finetuning.\n\n\u2022 Few-shot protocol. As an alternative to logistic regression on the final layer of frozen representations, we experiment with gradient descent over all parameters; on 5-shot and higher, training the full model with gradient descent can lead to significant gains (for example, 2-3% accuracy boost on ImageNet). We also experiment with a GP or Heteroscedastic last layer as an alternative to a linear last layer. Figure 3 displays the largest variants of Plex's performance compared to existing specialized state-of-the-art on a diverse collection of reliability tasks. Plex not only sets new state-ofthe-art on many tasks but Plex also unifies reliability performance under one general model for vision and language respectively. Here, we validate several takeaways as we ablate to understand Plex's ingredients. Scaling model size improves reliability. Figure 6 displays ViT-Plex and T5-Plex over varying model sizes. We compute a reliability score which is a normalized average over all task metrics: 139 for vision and 54 for language (see Appendix B for details). We also display reliability scores for individual reliability areas, which are the averages separately over the uncertainty, robust generalization, and adaptation tasks. Classical machine learning theory would suggest that a larger model translates to more overfitting and might therefore be less reliable as it may be overconfident and less robust. However, we find that scale improves overall performance across tasks.\n\n\nSummary of Results and Scaling Trends\n\nScaling pretraining dataset size improves reliability. ViT-Plex L with JFT performs better than ViT-Plex L with ImageNet-21K ( Figure 6). In Table 3, we also perform an ablation by comparing pretraining on JFT with 300M examples to JFT with 4B examples. We pretrain on up to 8M steps with batch size 4096, which is up to 8X more steps than we typically use for pretraining; each result is a separate run using a tuned learning rate schedule. ImageNet 10-shot accuracy is always better on JFT 4B under the same number of training steps. The models also converge faster with the smaller JFT 300M, reaching a performance limit, whereas JFT 4B keeps improving.    Figure 6: Plex's performance aggregated across (top) 139 vision task metrics and (middle) 54 language task metrics. Subplots (bottom) display average performance for individual reliability areas for vision (first 3) and language (last 2). Compute is the total # of training days for a single TPUv3 core. Dots represent size (Small, Base, Large), pretrained on JFT for images and C4 for text; I21K denotes pretraining on ImageNet-21K.\n\nBatchEnsemble improves pretraining. For vision, we run ablations at the fixed setting of L pretrained with JFT, and we use both B and L sizes for text, which are highly competitive settings. Figure 7 displays the ranking across tasks for each model. Methods are applied either during both pretraining and finetuning, or only during finetuning given a pretrained model checkpoint (\"None\u2192Het\" denotes pretraining with no changes and finetuning with Het on top). All the methods displayed improve over a baseline without ensembling or last layer changes (\"None\"). BatchEnsemble is consistently the best for pretraining.\n\nLast-layer methods improve finetuning. The best ranked models for the vision and language tasks use all of Plex's ingredients: Het on top of a pretrained BE for vision and GP on top of a BE for language. In particular, for T5-Plex ablations, BE+GP and BE tend to have the strongest performance. From more detailed per-dataset analysis in Appendix I, BE+GP and BE perform well on MNLI and NaLUE with BE+GP performing slightly better; notably, they outperform even an expensive deep ensemble baseline which also performs well on MNLI and NaLUE. BE+GP outperforms None on Toxic Comments while a Monte Carlo   Dropout baseline performs best on that task. T5-Plex L also outperforms T5-Plex B, which indicates the benefit of scale not only in Figure 6's normalized average score but in their average ranking. Downstream dataset size has no obvious pattern with reliability. Figure 8 decomposes the performance of ViT-Plex L by analyzing it as a function of the downstream dataset's size of training set. That is, we aggregate reliability performance for each training dataset separately, ranging from a size of 1,880 examples with Describable Textures Dataset (dtd) to 1.2 million examples with ImageNet. There is no clear pattern with respect to size. On the other hand, the datasets with lower reliability tend to be more different from the distribution of natural images in JFT: UC Merced is a remote sensing dataset of map areas, and Colectoral Histology is a histology dataset of human colorectal cancer. Pet images are not uncommon in JFT, but most breeds in Oxford IIT Pets aren't in JFT. This suggests reliability performance is possibly more connected to the distribution shift between pretraining and downstream dataset than the downstream dataset's size.\n\n\nRelationship Between Reliability Tasks\n\nGiven that there are many tasks that make up reliability, here we analyze relationships between the individual tasks. We're motivated by the following question: Is there an inherent ( , Validation NLL) Figure 9: Correlation of pretraining validation loss with each task (evaluation metric), averaging over all datasets for that task. Most of the tasks correlate highly, meaning upstream performance is a key signal for downstream performance.\n\nunderlying evaluation metric that is indicative of reliability? Specifically, can reliability be predicted from pretraining performance, without any downstream finetuning or adaptation?\n\nIn Figure 9, we analyze the correlation of validation loss on the pretraining dataset with each downstream metric. Most of the tasks correlate highly, meaning that the pretraining performance is an important predictor of downstream performance. The one exception are two uncertainty metrics-calibration error and calibration AUROC-which may make sense intuitively given that calibration as a property about models is not tied to predictive performance. Interestingly, AUROC, which refers to the metric for open set detection (an uncertainty task), is strongly coupled with predictive performance (Pearson correlation of 0.99!), more than, say, selective prediction performance (OC-AUC); this suggests that for open set recognition, prediction may be more important than uncertainty. Few-shot accuracy also correlates strongly with pretraining performance, and more examples leads to higher correlation (25 > 10 \u2248 5 > 1-shot). Accuracy and NLL correlate less strongly than few-shot: this is likely because they are measured on both in-and out-of-distribution datasets whereas few-shot is only evaluated on in-distribution test splits. Surprisingly, we can find an even stronger answer to the question: training loss on pretraining data is predictive of reliability. Table 4 shows Pearson correlation of upwards of 0.97 for the reliability score, prediction, and adaptation areas. Uncertainty is the least correlated but still quite strong at 0.76. This suggests that to perform well for reliability, simply fitting the data-that is, having high model capacity and the ability to efficiently train to utilize that capacity-is one of the most essential ingredients.\n\n\nThe Effect of Pretraining vs Finetuning\n\nHow do the effects we find above differ as model ingredients are applied during the pretraining phase vs finetuning phase? Here, we analyze the performance of models depending on choices made separately during the two phases. For example, can the benefits of BatchEnsemble be obtained when only applied during finetuning given a model pretrained without any changes?\n\nTo broadcast the weights of a pretrained model U into those of a downstream model D, we follow the initialization scheme for D and replace any weights common to U and D by those in U . For example, a BatchEnsemble obtained by broadcasting a pretrained vanilla model will inherit its slow weights from the model, but fast weights will be initialized Score Score robustness Score uncertainty Score adaptation  Table 4: Correlation matrix between the training loss on each dataset and the reliability over all vision tasks as well as separately in the three categories. A value of -1.0 means a lower training loss has a perfect linear relationship with a higher score. Training loss on the JFT pretraining dataset has a surprisingly high correlation with reliability. Performance on downstream datasets is predictive of overall robustness but otherwise has less correlation.\n\nrandomly. Consistent with all finetuning experiments, the head layer is also reinitialized rather than inherited from the pretrained model.  Figure 10: Upstream and downstream versus downstream-only use of specific architectures on Imagenet. We compare the extent to which a single pretrained deterministic model can be cast into alternative architectures during finetuning. Figure 10 displays results on ImageNet. BatchEnsemble when applied during both pretraining and finetuning strictly outperforms the finetuning-only variant across the 3 metrics. We also ran a naive deep ensemble baseline and find that this is similarly the case. On the other hand, for both GP and Heteroscedastic, applying the method only during finetuning roughly matches the performance of the method when applied during both pretraining and finetuning. Therefore we apply BatchEnsemble for both pretraining and finetuning, and we restrict using last-layer methods for finetuning. See Appendix H for more analysis of the heteroscedastic last layer for vision tasks.\n\n\nEnsemble Scaling\n\nEnsembles of neural networks (Hansen and Salamon, 1990;Lakshminarayanan et al., 2017), which aggregate the predictions of several instances of the same model class, provide a remarkably simple, yet effective, way to improve the performance of the base model. This holds true not only for predictive performance but also, crucially, for robustness and uncertainty quantification (Ovadia et al., 2019;Gustafsson et al., 2020;Wen et al., 2020b). However, ensembles require an increasing amount of compute as one increases the size of the ensemble. We're motivated to understand this axis of model scaling, ensemble size, as it compares to an alternative, popular axis of model scaling: increasing width and depth.   Figure 11 shows that naive ensembling consistently provides better performance as the number of ensemble members increases. However, this comes at significant computational cost. Scaling up the model size from S to B or from B to L leads to an \u223c4x increase in compute, and larger single models tend to outperform the ensembles of 4 smaller models. This motivates the importance of efficient ensembles in Plex: BatchEnsemble adds minimal extra compute and outperforms the baseline without ensembles consistently in the ranking across tasks (Figure 7).\n\n\nReliability Task Results\n\nIn this section, we examine performance on individual tasks across the areas of uncertainty, robust generalization, and adaptation.\n\n\nUncertainty\n\nTo make an accurate assessment of the quality of a model's predictive uncertainty, we consider a set of tasks, each highlighting a different property of models' predictive uncertainty estimates.\n\n\nCalibration\n\nTL;DR Plex improves calibration over a baseline without any changes, especially on outof-distribution. Scaling model size and pretraining dataset size can improve calibration.\n\nIntuitively, calibration is about the \"accuracy\" of a model's uncertainty estimates. It reflects how well the model's confidence, which quantifies the predictive probability of correctness,    aligns with the model's accuracy, which is the observed probability of correctness (Dawid, 1982). We investigate two different measures of calibration.  is a binning metric that computes the average difference in confidence and accuracy within different confidence bins. We evaluate ECE of vision models on 3 in-distribution datasets-CIFAR-10, CIFAR-100, ImageNet-and 6 out-of-distribution-CIFAR-10H, ImageNet ReaL-H, ImageNet-A, ImageNet-C, ImageNet-R, and ImageNet-V2. We also evaluate ECE on T5-Plex over 2 in-distribution datasets-MNLI-matched and NaLUE-and 3 out-of-distribution datasets-MNLI-mismatched, HANS, and NaLUE-tail 4 . Figure 12 displays the average calibration error of vision models on both in-and outof-distribution evaluation datasets. Calibration errors on in-distribution are relatively low in general (less than 2.5% ECE). Plex improves calibration error over using no changes (None) by roughly a reduction in half on average; on out-of-distribution, it is roughly a 3% improvement whether the models both use JFT or both use ImageNet21K. Regarding model size, we find calibration error improves over S, B, and L. Another significant difference is in pretraining dataset size from ImageNet-21K to JFT, where we find JFT models consistently outperform ImageNet-21K pretrained models.\nPlex L Het L BE L Plex L I21K None L Plex B Plex S None L I21K GP L 0.0% 0.\nA similar trend can be observed in the language domain. As shown in Figure 13, compared to the baseline without changes (None), Plex improves the model's calibration performance by roughly 3% on in-and out-of-distribution. On the other hand, a model's uncertainty performance is impacted more by the type of uncertainty methods than the model size. For example, the Plex B model is on average stronger than the None L model.\n\nCalibration AUROC. Calibration AUROC considers the binary classification problem of predicting from a model's uncertainty on a given input whether its prediction for the same input (i.e., the class associated with the highest class probability) is correct. It is the area under the ROC curve for this binary classification task, and it aggregates the model's predictive performance over all possible confidence thresholds. Comparing to ECE, Calibration AUROC only evaluates the uncertainty score's ranking performance, that is, whether a model consistently assigns higher uncertainty to incorrect predictions rather than correct predictions, and such an uncertainty score can be used as a signal for prediction correctness with good precision-recall and ROC performance (Krishnan and Tickoo, 2020;Kivlichan et al., 2021). Table 5 presents Calibration AUROC on vision and text datasets. We see a similar pattern as ECE where larger models generally perform better, but the pattern is slightly weaker. For example, Plex L performs best on only 4/6 datasets.  Table 6: Oracle Collaborative AUROC with a review fraction of 0.5% of all predictions. Default pretraining dataset is JFT and I21K signifies ImageNet21K.\n\n\nModels\n\n\nSelective Prediction\n\nTL;DR The model extensions in Plex provide a significant improvement for selective prediction, enabling the ability to predict at much lower error rates when deferring just a small fraction of predictions.\n\nDeep learning models are typically evaluated under the lens of average predictive accuracy. However, this doesn't account for the real-world cost of mistakes in deployed models today. Often the risk associated with a mistake can outweigh the benefit of being correct and thus, in expectation, it can be better to not predict at all under some confidence level or defer to a more expensive procedure (e.g., a human expert). This motivates selective prediction, which includes predictive performance as part of a larger decision-making scenario in which the model may abstain from making certain predictions (El-Yaniv and Wiener, 2010). Selective prediction performance can be evaluated using several approaches, and there has not been much standardization in the literature. We investigate two that use model uncertainty, through joint human-model collaboration and through rejection rates.\n\n\nSelective Prediction by Model Uncertainty using Human-Model Collaboration.\n\nMany real world use cases of AI allow for a model to defer a subset of predictions to a human expert. Oracle Collaborative Accuracy and AUROC measure the performance of an oracle-model collaboration system, where the oracle acts a proxy for human experts in order to automate evaluation. The model sends predictions with high predictive uncertainty to an oracle subject to a fixed referral budget (e.g., only 1% of all queries can be referred to the oracle). We compute Oracle Collaborative Accuracy and AUROC over a range of budgets and evaluate on both vision and language modalities.\n\nFor vision, Table 6 displays Oracle Collaborative AUROC with a review budget of 0.5% on three vision datasets over different variants of Plex. Surprisingly, the AUCs are quite high: on ImageNet for example, Plex L attains 0.98, and the models all achieve greater than 0.9 across the datasets. This implies that the ability to defer can enable lower error, which can be useful for higher-risk applications. Plex consistently performs best, even when taking into account different model and dataset sizes. We also examined the metric over review budgets of 1%, 2%, and 5%, and found the results to be unchanged. This suggests that the results on 0.5% are the model's limiting performance with an oracle in the loop.\n\nFor language, Figure 14 reports results across three different model sizes (S, B, L) and for eight methods: a baseline without changes (None), MC Dropout (MCD), Deep Ensemble : Ranking performance in selective prediction (lower is better) across different methods (14a-14c) and model sizes (14d-14f). Each model's box plot is its ranking over multiple datasets: ranking is done by AUROC on MNLI and Toxic Comments, and by Accuracy on NaLUE (AUROC is not well-defined for multi-class classification).\n\n(DE), Gaussian process (GP), Batch Ensemble (BE), Heterostochastic (Het) and two method combinations Gaussian process ensemble (DE+GP) and BE-GP (Plex) (see Tables 19 and 20 in the Appendix for detailed results). We show the rankings of these methods under different types of test data distribution (i.e. in-domain, OOD and tail-population). We first see that across different methods, DE+GP, BE+GP (Plex), BE, and MCD tend to have the strongest performance. In particular, DE+GP almost always dominates the other methods on MNLI and NaLUE, and remains competitive in the case of label imbalance (i.e., Toxic Comments). However, DE+GP is an expensive method that costs 10x more in memory and compute and therefore is not competitive in scale (a more thorough analysis is in Section 4.3). On the other hand, among the more efficient, single-model methods (i.e., Plex, BE and MCD), BE and Plex perform well on MNLI and NaLUE (notably, outperform the most expensive DE), while MCD stands out in the Toxic Comments. The above observations suggest that, when the training examples are drawn from a relatively simple distribution, quantifying output-layer uncertainty alone is sufficient to attain strong performance. However, when there are pathologies in the data distribution (e.g., extreme label imbalance and high label noise), quantifying the uncertainty within the model's intermediate representations (e.g., via some form of perturbation like BE) becomes important.\n\nNext we investigate how a model's performance is impacted by the model size. For model size scaling, we evaluate BE+GP (Plex), MCD, and None. We evaluate the performance of each method under progressively larger architectures, S, B, and L, and observe how the behavior changes across the method and with respect to the architecture size. Figure 14d-14f summarizes the rankings of uncertainty methods organized by the sizes of the architecture. As shown, comparing across architecture sizes, we see a larger architecture almost always Given an input, a model returns a prediction and uncertainty estimate. If the uncertainty estimate exceeds a threshold \u03b3 (indicating a level of uncertainty corresponding to a rejection rate \u03c4 that reflects the capacity for expert reviews), the diagnosis is referred to a human expert; otherwise, it is processed with no further review. The schematic is based on Figure 2 in Band et al. (2021).\n\nleads to stronger performance in collaboration. This trend remains largely consistent even under distributional shifts and in tail groups. On the other hand, a model's uncertainty performance is impacted more by the type of uncertainty methods. That is, the MCD and BE+GP models are on average stronger than None models, regardless of architecture size. Finally, within larger architectures (i.e., T5 B and T5 L), BE+GP generally outperforms MCD, and MCD outperforms None. This trend is broken in two situations, (1) in Toxic Comments, MCD strongly outperforms all other architectures, (2) in small architecture, BE-GP is often the poorer performing model, and None achieves the best performance in multi-token prediction problems (NaLUE). Finally, between the larger architectures (i.e., T5 B v.s. T5 L), Plex (BE+GP) generally outperforms MCD, and MCD outperforms None. This trend is broken in two situations, (1) in Toxic Comments, MCD strongly outperforms all other architectures, (2) among the small architectures, the None baseline achieves the best performance in multi-token prediction problems (i.e., NaLUE).\n\nSelective Prediction by Model Uncertainty using Rejection Rates. Rejection AUC measures a model's performance in the scenario where it is permitted to not predict on some fraction of the data for which the model is most uncertain, e.g., up to 10% of all queries. Unlike the collaboration metric, it does not assume the query is passed to an oracle as rejected inputs may be subject to further review for broader decision making such as to improve data collection. Rejection AUC is computed from predictive uncertainty (e.g., predictive entropy, predictive variance, confidence) and a chosen predictive performance evaluation metric (e.g., accuracy, AUROC, AUPRC) at different rejection rates. For a rejection rate of \u03c4 \u2208 [0, 1], the \u03c4 \u00d7 100% of data points in the evaluation set for which the model is most uncertain are identified. Those inputs are then rejected, and we assess the predictions on the remaining (1 \u2212 \u03c4 ) \u00d7 100% of data points. Accuracy-Rejection AUC is given by computing the area under the Accuracy-Rejection Rate curve. Additional results for AUROC and AUPRC-Rejection AUCs can be found in Appendix F. Table 7 displays overall performance for a selection of finetuned models that were pretrained on ImageNet-21K, and Figure 16 displays the full accuracy-rejection curves for the Country and Severity Shift out-of-distribution tasks. We find that that Plex generally improves performance over using no changes on both datasets on three out of four evaluation tasks and that Het L performs as well or better than Plex L on all four tasks. The Severity Shift task is concerned with detecting levels of severity of diabetic retinopathy more severe than those included in the training data. Accuracy is used as the evaluation metric and model predictive entropy is used as the per-example uncertainty estimate.  Table 7: Accuracy-Rejection AUC on RETINA Country and Severity Shift datasets.\nPlex L None L Het L GP L (I21K) (I21K) (I21K) (I21K)\nExamining the quality of different models' predictive uncertainty estimates for out-ofdistribution evaluation more carefully, we find that on the Country Shift out-of-distribution (covariate shift) task, all models exhibit non-monotonically increasing accuracy-rejection rate curves. Since the rejected examples are selected based on the models' predictive uncertainty estimates, this behavior indicates that beyond certain rejection-rate thresholds, the models tend to be underconfident about correct predictions and overconfident about mistakes. We do not observe this pattern on the Severity Shift out-of-distribution (semantic shift) task or on the Country and Severity Shift in-distribution tasks (see Appendix F).\n\n\nIn-Distribution Out-of-Distribution\n\nFar-OOD Vision CIFAR-10 SVHN CIFAR-100 SVHN Language NaLUE Standard Out-of-Scope Near-OOD Vision CIFAR-10 CIFAR-100 CIFAR-100 CIFAR-10 ImageNet2012\n\nPlaces365 Language NaLUE Near Out-of-Scope Table 8: The open set recognition tasks studied in this section.\n\n\nOpen-set recognition (OSR)\n\nTL;DR A simple scoring technique (maximum softmax probability) works well across open set recognition problems. Over image and text datasets, Plex provides a consistent improvement for new state-of-the-art, and without custom changes per dataset.\n\nOpen-set recognition, also called out-of-distribution detection or anomaly detection, aims to detect samples from new classes that are not included in training. In contrast to OOD generalization where the test example belongs to the same in-distribution training classes (x, y), y \u2208 Y ind , OSR aims to detect test example (x, y), where y \u2208 Y ood .\n\nTo evaluate OSR performance, we design an uncertainty score U \u03b8 (x) that indicates the likelihood of an input x being OOD, and \u03b8 denotes the classification model's parameters. As a default for all OSR problems, we apply the most commonly used uncertainty score: 1 \u2212 MSP, where MSP is the maximum over predicted softmax probabilities (Hendrycks and Gimpel, 2017). A mixture of in-distribution examples and OOD examples are used as the test set. For each test example, the model generates an uncertainty score. Comparing the uncertainty score with its ground truth OOD label (0 indicates the example is from in-distribution data, and 1 indicates the example is from OOD data), we use AUROC to measure how well the uncertainty score separates the two groups. Note that the model is not finetuned using any OOD examples; test OOD examples are only used for evaluation. We discuss results on alternative uncertainty scores in Appendix E.\n\nComparing models' OSR performance across tasks. We evaluate the performance of Plex and multiple baseline models. Based on the similarity between the in-and out-ofdistribution data, we can classify OSR tasks under two categories (Table 8): (a) far-OOD detection and (b) near-OOD detection. In Table 9, we observe that first, larger models have better performance. Second, among all the models, Plex performs the best for most takes. Third, for ImageNet2012 vs Places365, models pretrained with ImageNet-21K (I21K) outperform the same models but pretrained on JFT.\n\nOpen-set intent detection based on large language pretrained models. To study OSR performance of different models in the language domain, we design an intent detection task for detecting natural utterances that are out of the scope (OOS) services. For the in-distribution dataset NaLUE, the model needs to map each utterance input x to a MSP Far OOD Near-OOD C-10 vs SVHN C-100 vs SVHN C-10 vs C-100 C-100 vs C-10 I2012 vs Places  3-token sequence of y = (y 1 , y 2 , y 3 ) = (vertical name, domain name, intent name). To study both far-OOD performance and near-OOD performance, we construct two out-of-the-scope datasets for NaLUE:\n\n\u2022 NaLUE Standard-OOS: completely out-of-domain queries, based on CLINC150-OOS. The standard-OOS queries do not share any of the (vertical name, domain name, intent name) with the in-domain query.\n\n\u2022 NaLUE Near-OOS: in-domain, out-of-scope queries. It is created by sampling 20% intent from each known domain as OOD data. The near-OOS examples can share the same vertical name / domain name (but definitely different intent name) with the in-domain query.\n\nThe model outputs a sequence, and we compute the uncertainty score based on the conditional softmax probability p(y l |y <l , x, \u03b8), using the conditional entropy. The conditional entropy is a standard uncertainty score for sequences (Malinin and Gales, 2021),\nuncertainty(y|x, \u03b8) = \u2212 1 L L l=1 H(y l |y <l , x, \u03b8),(1)\nwhere H(y l |y <l , x, \u03b8) = \u2212 K k=1 p(y l = k|y <l , x, \u03b8) log (p(y l = l|y <l , x, \u03b8)) is the conditional entropy based on the conditional distribution p(y l = k|y <l , x, \u03b8), and k = 1 . . . K is the total number of intent names. The uncertainty score is compared with the ground truth OOD label. An input (x, y 1 , y 2 , y 3 ) is labeled as OOD if any of y l belongs to the OOD labels.\n\nIn Table 10 \n\n\nLabel Uncertainty\n\nTL;DR We propose label uncertainty as an important challenge, with a new large-scale dataset and propose an evaluation metric. The heteroscedastic last-layer particularly helps for label uncertainty.\n\nIt is typical to encounter label noise in non-academic datasets. However, benchmark datasets are often carefully curated to reduce label noise due to unreliable or disagreeing annotators. We use the existing CIFAR-10H dataset, which is a relabelled version of the CIFAR-10 test set with on average >50 crowd-sourced noisy labels per input (Peterson et al., 2019). There is a notable lack of label uncertainty datasets in the literature, and we found limiting performance on CIFAR-10 as our models achieve >99% accuracy on the original CIFAR-10 test set. Therefore we build a larger dataset called ImageNet ReaL-H, which leverages the raw annotations of ImageNet ReaL (Beyer et al., 2020). When raw labels were available for an image we followed the same averaging procedure as for CIFAR-10H to produce a soft label. There are some cases where no raw annotations were available. These cases correspond to an image where a set of ML models all agreed with the original ImageNet label for the image and thus the image was not sent to human annotators for re-labelling. In these cases we took the one-hot ImageNet label as the soft label (equivalent to all human annotators agreeing the original ImageNet label was correct).\n\nMetric To evaluate a model's ability to capture label uncertainty, consider a divergence measure D between probability distributions,\nE x\u223cpdata(x) D p data (y | x) p model (y | x) \u2248 1 N N n=1 D p data (y | x n ) p model (y | x n ) ,(2)\nwhere p data (\u00b7) is the true label distribution for the data point and p model (\u00b7) is the model's distribution. This evaluation metric only reaches 0 when the model captures the true label distribution for every example. We use KL divergence and approximate the label distribution using an empirical distribution over the multiple label samples per input. \n\n\nResults\n\nWe observe in Figure 17 that the heteroscedastic models (Het in finetuning-only and Plex) outperform the None model. Plex does best on ImageNet ReaL-H excluding DE which uses more compute, but Plex performs worse on CIFAR-10H.\n\n\nRobust Generalization\n\nAn important aspect of model reliability is it's ability to make accurate predictions when the test data distribution changes. In this section, we examine model robustness to different forms of data distribution shift. We look at performance both in-distribution and under two out-of-distribution shifts: covariate shift and subpopulation shift. 5\n\n\nIn-Distribution Generalization\n\nTL;DR Ablation experiments show that pretraining does not help for diabetic retinopathy diagnosis on images, but the Plex model extensions do give a benefit. Across language tasks, pretraining, model scale, and the Plex extensions all contribute to better in-domain generalization.\n\nWe look at accuracy and NLL across the in-distribution test splits of each dataset after finetuning on their respective training splits.\n\nVision Tasks For RETINA in Table 11, we compare Plex to the best-performing ResNet-50 baseline results based on a wide array of uncertainty quantification methods (Gal and Ghahramani, 2016;Blundell et al., 2015;Rudner et al., 2021Rudner et al., , 2022Farquhar et al., 2020) reported in Band et al. (2021) and find that pretraining on neither I21K or JFT results in an improvement in predictive performance across all evaluation   metrics compared to the state-of-the-art ResNet-50 trained from scratch. This failure may be due to the fact that the retina scans used for the diagnosis tasks differ significantly in appearance from the images in the pretraining datasets and as such may be too dissimilar to convey meaningful inductive biases into the finetuned neural network. Nevertheless, the Plex extensions do still provide a benefit over the pretrained model. See Figure 18 for CIFAR and ImageNet.\n\nLanguage Tasks In the Appendix, In order to be robust under covariate shift, a model should be able to reliably make correct predictions on noisy, corrupted, and otherwise distribution-shifted inputs. In this section, we evaluate robustness using a variety of datasets that exhibit different types of covariate shift.\n\nRETINA under Country Shift RETINA's Country Shift task is concerned with diagnosing diabetic retinopathy from retina scans obtained using different medical equipment and a different patient population. In Table 12, we compare Plex to the best-performing ResNet-50 baseline results based on a wide array of methods to improve uncertainty quantification (Gal and Ghahramani, 2016;Blundell et al., 2015;Rudner et al., 2021Rudner et al., , 2022Farquhar et al., 2020) reported in Band et al. (2021) and find that pretraining on either I21K or JFT results in an improvement in predictive performance across all evaluation metrics compared to the state-of-the-art ResNet-50 trained from scratch. This result is in contrast with corresponding results on the Country Shift in-distribution task, for which the best-performing models trained from scratch outperform all pretrained models. In this case, pretraining does improve generalization under covariate shift even when  2020)). We find that BE ViT L outperforms None methods, that pretraining on JFT outperforms models trained on other pretraining datasets, and that increasing model scale consistently leads to improved performance under shifts across metrics. Unlike in some sections, for performance under shifts, we consistently find that pretraining on JFT outperforms that on ImageNet-21K.\n\nStability ImageNet-Vid-Robust and YTBB-Robust (Shankar et al., 2021) are benchmark datasets for measuring robustness to natural perturbations in images by using subsequent frames from video sources with human curation. On the robust accuracy (pm-k accuracy) We find that on the main pm-k accuracy, None, Plex L and BE L models perform similarly, although BE L outperforms slightly. On anchor accuracy, None and BE perform similarly. We also see that performance improves with model size.  metric, in Figure 20, None and BE L models perform equally well, and performance improves with model size. Models pretrained on JFT outperform those pretrained on I21K, with BE B JFT even outperforming None L I21K and BE L I21K.\n\n\nLanguage Style and Topical Drift\n\nIn natural language applications, it is common for a trained NLP model to be deployed to an environment that exhibits significant style and topical drift when compared to training data. For example, a NLI model trained on written literature is used to analyze in-person dialogues, or a toxic comment detection model trained on U.S. web forums is deployed to non-U.S. news websites (Williams et al., 2017;Kivlichan et al., 2021). To understand the robustness of large uncertainty models with respect to these common language drifts, we evaluate the models' prediction performance on out-of-distribution splits of the NLI and Toxic Comment tasks (MNLI-mismatched and Civil Comments, respectively). Specifically, MNLI-mismatched represents a low-degree shift scenario where models trained on written and spoken genres (e.g., government report, fiction, phone conversations) are tested on different subtypes of similar genre (e.g., fundraising letters, in-person conversation, 9/11 calls). On the other hand, Civil Comments represents a high-degree shift scenario where models trained on web forum conversations before 2015 were tested on news websites comments from a different period (2015-2017). Figure 21 compares the performance of nine models based on T5 base , and also that of three representative efficient methods (None, MCD and Plex) across model scales (i.e., T5 small , T5 base , T5 large ) (see Appendix Table 34-35 for detailed results). As shown, the models' performance in out-of-distribution generalization correlates well with their in-domain performance, with DE-GP being the most competitive ensemble model, and BE / Plex the most competitive efficient method for MNLI, and MCD the most efficient method for Toxic Comments. Comparing between model scales, larger models in general lead to stronger out-of-distribution performance, with Plex being the most competitive method for moderate-to-large size models (T5 base , T5 large ).\n\n\nSubpopulation shift\n\nTL;DR Plex improves generalization under subpopulation shift relative to models omitting pretraining, efficient ensembling, or last layer changes, for both language and vision datasets.\n\nWe next study performance on shifts where data is composed of subpopulations which may be rare or unseen during training. A common assumption is that subpopulation shift data is drawn from a distribution of distributions: subpopulations are drawn from a population distribution, and each subpopulation has its own data-generating distribution (Santurkar et al., 2020;Yuan et al., 2022). Since data in practice is often generated by individuals or groups who may have different data distributions, and models ideally generalize to unseen individuals and groups, this setting can be used to measure a natural notion of reliability. We study Plex under multiple ways of partitioning data into subpopulations.\n\nSemantic Partitioning. One method for producing data with subpopulation shift is partitioning a standard dataset into subpopulations such that each subpopulation contains semantically similar examples. We leverage the Semantically Partitioned CIFAR10 and CIFAR100 datasets from Yuan et al. (2022) to study how Plex performs on new subpopulations. In particular, we measure classification accuracy with 30 and 100 subpopulations unseen during training, studying how accuracy varies among subpopulations. : Accuracy over subpopulations for CIFAR-10 and CIFAR-100. For each setting, we display the 5th, 25th, 50th, 75th, and 95th percentile accuracy for ViT-Plex L among the subpopulations. See Figure 3 for comparison with no pretraining, which performs much worse.\n\nFrom Figure 3, Plex greatly outperforms the baseline presented in Yuan et al. (2022) for tail subpopulation shift accuracy (we report 25th percentile among subpopulations). The baseline does not leverage large-scale pretraining or other changes Plex introduces. To better understand how Plex's ingredients lead to improvements in accuracy under subpopulation shift, we perform a series of ablations shown in Figure 22. Comparing Plex to training with BatchEnsemble for both pretraining and finetuning ('BE\u2192BE'), the latter produces a slight degradation in performance for both median and tail subpopulations, especially for CIFAR100, where performance is less saturated. This indicates that last layer changes offer some improvement but do not fully explain Plex's improvement over baseline in this setting. Looking at 'None\u2192BE', standard pretraining slightly degrades performance relative to BatchEnsemble pretraining, but using BatchEnsemble for finetuning still offers a significant boost over completely standard training ('None'); ensembling appears to be more important in this setting than last layer changes. Comparing Plex and 'None', we see a significant difference in performance, but the difference between Plex's performance and that of the baseline presented in Figure 3 remains much larger. This indicates that the difference in pretraining is an important driver of Plex's improvements on unseen and long-tail subpopulations, since the baseline does not pretrain on large-scale data.\n\nSpurious Correlations. A well-observed issue in machine learning models is their tendency to learn decision rules that rely on spurious, non-causal patterns that exhibit strong statistical correlation with the outcome in the training data, i.e., shortcut learning (Geirhos et al., 2020). In particular, previous theoretical studies show that a model's tendency to learn spurious patterns is rooted in the overparameterized model's tendency to flexibly  adapt to the (biased) empirical distribution of the data, and cannot be fully addressed by increasing model size (Bommasani et al., 2021). However, recent empirical studies also show that, when the data distribution exhibits suitable diversity such that it contains a small amount of counterexamples where the spurious pattern doesn't hold, large pretrained models can still lead to improved robustness (Tu et al., 2020). Therefore, we investigate if the large pretrained models' robustness indeed improves with model scale, and if the uncertainty methods bring additional gain on top of the None baselines.\n\nFor the language modality, we evaluate the performance of T5 models on two spuriouscorrelation subpopulations: HANS for natural language inference, and CivilComments-Identities for toxic comment detection (McCoy et al., 2019;Borkan et al., 2019). Specifically, HANS evaluates the NLI model's robustness against non-causal heuristic patterns that are empirically associated with sentence entailment (e.g., lexical overlap) (McCoy et al., 2019). On the other hand, CivilComments-Identities contains comments that mention certain gender, sexual orientation, ethnic or religious identities, which empirically exhibits differential distribution of toxicity label with respect to their surface-level textual features. A toxic detection model that relies on these surface-level identity mentions can lead to unintended consequences, unjustly reinforcing existing social stereotypes to disadvantaged identity groups (Borkan et al., 2019). Figure 23 summarizes the performance of different uncertainty methods, and across three different scales (i.e., T5 small , T5 base , T5 large ) (See Appendix Figures 34 and 35 for detailed results). Compared to the None baseline, the uncertainty methods (BE, DE, GP and their combinations) provide significant improvements across all aspects of model performance (i.e., generalization, selective prediction, and uncertainty calibration). For generalization and selective prediction performance, the ensemble of SNGP models (DE-GP) and Plex (BE-GP) perform the best among ensemble and non-ensemble methods, respectively. Furthermore, within each method class, the subpopulation generalization improves as the model scale increases, with Plex (BE+GP) being the most competitive method for moderate-and large-size models. Finally, the trend in uncertainty calibration is less consistent, we see that among all methods, GP and BE are on average the most calibrated ensemble and non-ensemble method, while the model scale does not seem to have a consistent impact to calibration performance.\n\n\nAdaptation\n\nIn this section, we examine the models' ability to adapt to new data. We focus on dataefficient learning (small data generalization), where the goal is to attain high reliability performance with only a small set of examples. Our study focuses on ViT vision models, since adaptation has not yet been studied for T5 text models (Raffel et al., 2020).\n\n\nActive Learning\n\nTL;DR Large pretrained models are good active learners. Plex not only provides a significant boost in initial performance, but it also finds examples in order to adapt and improve at a faster rate than active learning models without pretraining.\n\nThe goal in active learning (AL) (Cohn et al., 1996;Settles, 2009) is to maximize label efficiency for machine learning models where label annotations are scarce. The success of AL has been demonstrated on a range of real-world problems where labels are expensive to acquire, e.g. computer vision (Gal et al., 2017;Citovsky et al., 2021), natural language (Thompson et al., 1999;Siddhant and Lipton, 2018), speech (Hakkani-T\u00fcr et al., 2002;Riccardi and Hakkani-Tur, 2005) and robotics (Martinez-Cantin et al., 2007;Wang et al., 2018b). Here, we investigate Plex for AL on the image domain and focus on four datasets: CIFAR-10 and CIFAR-100 which are standard in active learning research Hu et al., 2019;Song et al., 2019); and we scale active learning to a larger dataset, ImageNet, which is less commonly evaluated (Emam et al., 2021;Beluch et al., 2018). Finally, we evaluate active learning on Places365 (roughly 1.8 million examples in the training pool) which is also less explored.\n\nWe adopt a standard setup of AL for multi-class single-label image datasets (Section 2.2), where we assume an initial model, a training pool of unlabeled images, and a budget of total number of labels to acquire. AL operates in a loop where in each round, labels are acquired for examples with the highest acquisition score from the training pool, the model is subsequently finetuned on the image-label pairs observed so far, and the training pool is updated to remove the newly labeled images. Once we exhaust the budget on label acquisition, the cycle of AL stops and the final model is obtained.\n\nFor the label acquisition strategy, we use margin sampling (Margin) as a representative AL approach (Scheffer et al., 2001;Roth and Small, 2006), and it has been found competitive on AL tasks with deep learning models (Citovsky et al., 2021). Margin uses the difference between the highest and second highest predicted probabilities (Scheffer et al., 2001) to score informativeness: the smaller the difference, the more uncertain the model is about . Plex's large scale pretraining and uncertainty estimates enables active learning on large datasets as well as fast adaptation over the number of labeled images. With Plex models pretrained on JFT, Margin requires roughly 30% less labeled data than Uniform in order to reach the best accuracy of Uniform; and with Plex models pretrained on I21K, Margin is roughly 20% more label efficient than Uniform.\n\nan example and the more informative it is expected to be. As a baseline, we compare to uniformly randomly sampling from the training pool (Uniform).\n\nTo better simulate realistic label acquisition settings with parallel annotators, we adopt the batch active learning setup (Settles, 2009) for both Margin and Uniform. For Margin, the examples with the top-K margin scores are chosen for each acquisition round. For Uniform, we randomly select K examples without replacement. Figure 24 displays the AL results on CIFAR-10, CIFAR-100, ImageNet and Places365. We compare Margin and Uniform with Plex models pretrained on ImageNet21K or JFT, or without any pretraining, i.e. randomly initialized Plex models. We set initial data size to be 2\u00d7 number of classes, max training set size to be 20\u00d7 number of classes, and acquisition batch size to be 0.5\u00d7 number of classes. Figure 24 shows that AL with pretrained models significantly outperform models without pretraining across all tasks. We observed two notable effects. First, pretraining has a significant initial boost in performance, where for example Plex starts at 20% for CIFAR-10 without pretraining and 50-60% with pretraining. Second, pretraining results in faster adaptation in terms of the accuracy gain for each labelled example: for example, it takes roughly 200 examples on CIFAR-10 to go from 20-30% whereas it takes roughly 10 examples for pretrained Plex to go from 50-60%.\n\nPretrained models also enables better performance from better AL acquisition methods. Without pretraining, we observe no gain in performance using Margin comparing to Uniform. However, with pretrained models, Margin almost always outperforms Uniform. Notably, for Plex models pretrained on JFT, Margin requires 37.5%, 31.6%, 35.9% and 10.3% less labeled data than Uniform respectively on CIFAR-10, CIFAR-100, ImageNet and Places365 in order to achieve the best accuracy obtained by Uniform.\n\nFor CIFAR-100 and ImageNet2012, AL with models pretrained on I21K outperforms that on JFT. We hypothesize that CIFAR-100 and ImageNet are more \"in-distribution\" in I21K than JFT (Section 5.3.2 also shows this pattern). However, Places365 is an OOD dataset for both I21K and JFT, where JFT is much larger than I21K. Likely due to better representations pretrained on a larger and more diverse dataset, AL with Plex models pretrained on JFT performs better than I21K. On a related note, Evci et al. (2022) observed how domain shift can impact finetuning and hypothesized that the ability to leverage pretrained representations contributes to the effectiveness of finetuning; and Tamkin et al.\n\n(2022) observed related findings for pretrained models focused on task ambiguity.\n\n\nFew-shot Learning\n\nTL;DR Plex's BatchEnsemble representation improves few-shot and over increasing model and pretraining dataset scales. Full-model training can work better than linear evaluation if the goal is to maximize performance from limited examples.\n\nIn few-shot learning, we examine how well representations learned during pretraining enable fast downstream adaptation. We use a linear evaluation protocol where we extract features from the models' pre-logits layer and train a multinomial logistic regression model using L-BFGS. Unlike linear regression which is also popular (e.g., Dosovitskiy et al. (2021)), logistic regression produces a categorical distribution, enabling one to measure the quality of the few-shot model's uncertainty estimates; we take advantage of this in Section 5.3.3. Figure 25 displays test set accuracy and negative log likelihood (NLL) averaged over 9 datasets: birds, caltech, cars, cifar100, col hist, dtd, imagenet, pets, and uc merced. The choice of model translates to a consistent 1-2% improvement in accuracy depending on the few-shot setting. The BatchEnsemble representation is among the top performing that gives high predictive performance.\n\n\nModel choice and size\n\nThe size of the model and pretraining dataset also has a noticeable effect on fewshot performance. While ImageNet-21K is 10 times smaller than JFT, models pretrained on ImageNet-21K consistently outperform those pretrained on JFT across prediction and uncertainty tasks, with the exception of calibration. We hypothesize this is because ImageNet-21K is closer in distribution to the datasets we evaluate on average. It is also surprising that when finetuning over the full ImageNet dataset (Section 5.2), the conclusion is reversed with JFT models outperforming ImageNet-21K. This suggests a transition point when the pretrained models are trained on a sufficiently high number of examples. Full-model training Intuitively, linear evaluation is about \"representation learning\", assessing how well a pretrained model's fixed representation layer generalizes to many different scenarios. This setup may not be optimal for real-world transfer if the goal is simply to maximize performance from limited examples, without constraints on the approach to doing so. In Figure 26, we adopt the usual finetuning protocol and experiment with training the model's full set of parameters given the pretrained checkpoint. We set up a few-shot version of ImageNet1k for this study. 6 On 1-shot, linear evaluation outperforms full-model training which may not be surprising given the high possibility of overfitting. Starting from 5-shot (and which is likely to hold for even lower shot settings), full-model training surpasses linear evaluation with a consistent 2-3% improvement in accuracy.\n\n\nFew-shot Uncertainty\n\nTL;DR Plex performs well on both few-shot and zero-shot open set recognition, particularly using larger model sizes and on ImageNet21K. Few-shot calibration remains a challenging problem.\n\nFew-shot learning is an important and practical setting, for which the literature almost exclusively considers accuracy to measure performance. Few-shot uncertainty quantification is also a key problem: compared to finetuning, few-shot models are more sensitive to the examples they've seen and by extension, much less accurate. Thus few-shot models should be more likely to be uncertain, and the quality of their uncertainty estimates must be good in order to know when to refrain from making a prediction.\n\nFew-shot uncertainty tasks In Figure 27, we measure uncertainty performance on three metrics: ECE and Calibration AUROC for calibration, averaged over all 9 datasets used Far-OOD\n\nNear-OOD C-10 vs SVHN C-100 vs SVHN C-10 vs C-100 C-100 vs C-10  Zero-shot open set recognition Given that the pretrained model itself provides rich and robust representations, we can use that as a feature extractor and conduct zero-shot open set recognition without any training examples. To do so, we can take advantage of Mahalanobis distance (Maha) (Lee et al., 2018) as a detection score: Maha measures the distance between the test input and the fitted training distribution in the embedding space. It operates on a fixed representation layer and does not require operating on softmax outputs with a newly trained last layer. The training distribution is fitted using a class conditional Gaussian N (\u00b5 k , \u03a3), k = 1, 2, . . . , K to each of the K in-distribution classes based on the embedding z. We estimate the mean vectors and covariance matrix as:\n\u00b5 k = 1 N k i:y i =k z i , for k = 1, . . . , K, and \u03a3 = 1 N K k=1 i:y i =k (z i \u2212 \u00b5 k ) (z i \u2212 \u00b5 k ) T .\nNote that class-conditional means \u00b5 k are independent for each classes, while the covariance matrix \u03a3 is shared by all classes to avoid under-fitting errors. For a test input, Mahalanobis distances from a test input to each of the fitted K in-distribution Gaussian distributions N (\u00b5 k , \u03a3) is computed, and the minimum of the distances over all classes is used as the uncertainty score,\nMD(z) = min k {(z \u2212 \u00b5 k ) T \u03a3 \u22121 (z \u2212 \u00b5 k )}.\nWe also experiment with a Relative Mahalanobis distance variant (RMaha) , a modified version of Mahalanobis distance which corrects for the background confounding effect using another Gaussian distribution fitted using entire training data ignoring class labels. The uncertainty score is defined as RMD k (z) = MD(z) \u2212 MD 0 (z), where MD 0 (z) indicates the Mahalanobis distance to a distribution fitted to the entire training data not considering the class labels:\nN (\u00b5 0 , \u03a3 0 ), where \u00b5 0 = 1 N N i=1 z i and \u03a3 0 = 1 N N i=1 (z i \u2212 \u00b5 0 ) (z i \u2212 \u00b5 0 ) T .\nThis is a good proxy for the background distribution. Zero-shot results are displayed in Table 13. Interestingly, the zero-shot setting achieves performance close to that of the finetuned setting. For example, in the challenging near-OOD task CIFAR-100 vs CIFAR-10, the best zero-shot AUROC achieved by Plex L (I21K) is 0.915, while its performance with finetuning is 0.934, and the best model which uses JFT and finetuning achieves 0.954 (Table 9). For the far-OOD task CIFAR-100 vs SVHN, the best AUROC achieved by zero-shot is 0.933, and the best model which uses JFT and finetuning achievs 0.938.\n\nThe Relative Mahalanobis distance also significantly outperforms the raw Mahalanobis distance, suggesting that there are background features that confound the raw Mahalanobis distance. In fact, the raw Mahalanobis distance does not work well for most of the pretrained models, mostly having AUROC lower than 0.9 and varies depending on the model types.\n\n\nConclusion\n\nWe presented a framework for thinking about reliable deep learning, and provided a number of tasks and datasets for stress-testing the reliability of models through multiple tasks such as the ability to quantify its confidence in predictions, be robust to distribution shifts, and adapt quickly to new distributions. To improve reliability, we also developed Plex, pretrained large model extensions for vision (ViT-Plex) and language (T5-Plex) that significantly improve the reliability in deep learning. Our techniques are broadly applicable for other large models Jian-Guo Zhang, Kazuma Hashimoto, Yao Wan, Ye Liu, Caiming Xiong, and Philip S Yu. Are pretrained transformers robust in intent classification? a missing ingredient in evaluation of out-of-scope intent detection. arXiv preprint arXiv:2106.04564, 2021.\n\n\nAppendix A. Author Contributions\n\n\u2022 Andreas Kirsch: Active Learning only -joint with Joost van Amersfoort. Implemented Active Learning loop, various acquisition functions and datasets, and ran experiments on CIFAR-10/CIFAR-100. Advised on experimental setup for larger datasets, and wrote active learning section.\n\n\u2022 Balaji Lakshminarayanan: Helped with direction, narrative, getting resources, advising folks on experiment setup (open set recognition, few-shot adaptation, last layer), helped with writing (abstract, intro, contributions) and revising other sections.\n\n\u2022 Clara Huiyi Hu: Implemented BE + GP in language domain, executed associated experiments and helped building post-analysis colabs. On the vision side, contributed to sweeping and debugging BE/GP variants.\n\n\u2022 Dustin Tran: Came up with work's vision, lead overall writing and experiment design, coordinated individual leads and contributors. Designed and tuned Plex, various figures and plotting code, and core infra setup.\n\n\u2022 Du Phan: Lead and designed the infrastructure to evaluate uncertainty methods on the language domain. Implemented methods None, DE, MCD, GP, DE-GP, Het for language and performed most corresponding experiments across MNLI, ToxicComments, NaLUE tasks.\n\n\u2022 D. Sculley: Advisor on project, including its direction and the paper narrative.\n\n\u2022 Honglin Yuan: Initial experimental setup for CIFAR subpopulation shift experiments, including pipeline for converting data into format to be consumed by uncertainty baselines codebase.\n\n\u2022 Jasper Snoek: Helped with direction, narrative, getting resources, helped with writing, editing, aggregating results, various plots.\n\n\u2022 Jeremiah Liu: Lead for language domain efforts. Developed all tasks, datasets, evaluation metrics for language. Implemented initial t5x infra and baseline (None) experiments. Work with eng co-lead Du and teammates Clara and Jie in finishing executing experiments and generating visualizations. Wrote the sections on language.\n\nOn the vision side, implemented the ViT-GP model, conducted initial pretraining experiments. Assisted editing the section on SNGP method.\n\n\u2022 Jie Ren: Lead work on open set recognition (vision and text), GPs, and assisted with few-shot. Helped with engineering work including building vision finetuning models, running vision and text experiments. Made the demo figures for vision and text. Helped with Mahalanobis distance-based active learning method.\n\n\u2022 Joost van Amersfoort: See Andreas Kirsch (joint contribution). Co-wrote compute workflow tutorial for collaborators.\n\n\u2022 Karan Singhal: Vision subpopulation shift setup and experiments: integrated code for CIFAR-10/100 subpopulation evaluation into codebase, set up, ran, and tuned experiments with different ingredients (Det, BE, BE+HET, BE+GP). Ran ablations: omitting last-layer changes, omitting ensembling during pretraining, ImageNet-only pretraining, no pretraining. Created ablations figure and added discussion. Made detailed edits to paper.\n\n\u2022 Kehang Han: Responsible for finetuning experiments involving GP and few-shot experiments. Created Imagenet2012Fewshot TFDS (https://www.tensorflow.org/ datasets/catalog/imagenet2012_fewshot). Primary author of Sections 5.3.2 and 5.3.3. Helped with GP model ablations.\n\n\u2022 Kelly Buchanan: Contributed to early formation of project and codebase, particularly around tasks involving structured output evaluation and uncertainty.\n\n\u2022 Kevin Murphy: Advisor on project, including its direction and the paper narrative.\n\n\u2022 \u2022 Yarin Gal: Advisor on project, including its direction and the paper narrative.\n\n\u2022 Zachary Nado: Designed and contributed to core infrastructure that was the major platform to iterate on our models, tasks, and other results.\n\n\u2022 Zelda Mariet: Ran BatchEnsemble experiments, ran ensemble diversity analysis, drew up vision for the ensembling analysis Section 4.3, proposed and ran the upstream vs downstream comparison (Section 4.2). Wrote the code for experimental analysis, hyperparameter tuning, visualization and summarization. Code refactoring for the overall project.\n\n\u2022 Zi Wang: Led the active learning effort including initial exploration as an engagement with internal teams as a close collaboration between Google Reliable Deep Learning and the Oxford OATML group (with Mike, Andreas, Joost, and Dustin). Contributed to refactoring BE and None to facilitate better integration with AL. Contributed to active learning code (debugging, adding new features etc). Extensively experimented with AL on RDL models, conducted analyses and obtained/wrote final results. Added discussions on the relations between prior learning and pretraining.\n\n\u2022 Zoubin Ghahramani: Advisor on project, including its direction and the paper narrative.\n\n\nAppendix B. Details behind Key Figures\n\nFor the task radar plots of Figure 3, we list references used for task-specific state-of-the-art in Table 14.\n\nFor the model scaling plot of Figure 6, we aggregate all task metrics under a single scalar between 0 and 100. In order to do this, we normalize all metrics to be between 0 and 100; we then compute an unweighted average. Most metrics are already bounded between 0 and 100: for example, accuracy, expected calibration error (we do 100 \u2212 ECE so higher is better), Calibration AUROC, and OOD AUROC. The one exception are scoring rules such as log-loss and Brier score. Because the output distributions are discrete, log-loss has a lower bound of 0 and an upper bound given by the highest entropy distribution (uniform). Therefore we rescale scoring rule values based on their lower and upper bounds so that they're now between 0 and 100 and so that higher is better.\n\n\nAppendix C. Additional Details on Tasks & Datasets\n\nWe summarize the list of metrics in Table 15.\n\n\nTask\n\nTask-specific SOTA   Figure 3.\n\nFor out-of-distribution evaluation in vision, we use the following datasets.\n\n1. Covariate shift.\n\n\u2022 CIFAR-10: CIFAR-10-C (Hendrycks and Dietterich, 2019).\n\n\u2022 CIFAR-100: CIFAR-100-C (Hendrycks and Dietterich, 2019).  (Band et al., 2021). We train models on images of retinas obtained from patients in the United States (EyePACS) and evaluate trained models on images of retinas obtained from patients in India using different collection equipment (APTOS).\n\n2. Semantic (class) shift.\n\n\u2022 CIFAR-10: CIFAR-100, SVHN.\n\n\u2022 CIFAR-100: CIFAR-10, SVHN.\n\n\u2022 ImageNet1K: Places365.\n\n\u2022 RETINA: RETINA's Severity Shift dataset (Band et al., 2021). We train models on images of retinas exhibiting no worse than mild diabetic retinopathy, and consider a shifted evaluation dataset with images of moderate diabetic retinopathy or worse. The evaluation data contains features not contained in the training images, such as vitreous hemorrhages. The motivation for this shift is that images of retinas with more severe retinopathy are relatively scarce and that it is likely for a model to be trained only on more widely-available images of retinas exhibiting mild cases of diabetic retinopathy.  4. Subpopulation shift. We use Semantically Partitioned CIFAR-10/100 (Yuan et al., 2022) for vision subpopulation shift. CIFAR-10/100 test data is partitioned into semantically similar subpopulations, where each subpopulation has its own datagenerating distribution sampled from a meta subpopulation distribution. We aim to improve predictive performance on tail subpopulations.\n\nFor covariate and semantic shift in language, we use:\n\n\u2022 the MNLI-mismatched (Williams et al., 2017) data as the OOD set for NLI, which contains sentence pairs from 5 genres that are distinct from those in MNLI training data.\n\n\u2022 the CivilComments corpus (Borkan et al., 2019) as the OOD set for toxic comment prediction, which consists of one million public comments appearing on approximately 50 English-language news sites across the world.\n\nWe also analyze language subpopulation shift where long-tail groups from the indistribution set are a desired area for generalization.\n\n\u2022 HANS (McCoy et al., 2019) eval datasets for NLI, which contains template-generated examples attacking the surface-level heuristics that the neural models are found to rely on when predicting entailment relationships.\n\n\u2022 CivilCommentsIdentity (Borkan et al., 2019) for toxic comments, which is a subset of CivilComments that has explicit mention of social identities (e.g., muslim, LGBTQ, etc) that the model are often found to generate mispredictions.\n\n\u2022 NaLUE-tail dataset for CLU, which is a subset of NaLUE corresponding to utterances from 28 low-frequency intents categories.\n\n\nAppendix D. Details of Plex ingredients\n\nBatchEnsemble (BE) BatchEnsembles (Wen et al., 2020b) approximate deep ensembles (Lakshminarayanan et al., 2017), but reduce their computational and memory costs by sharing weights across the ensemble members. The weight matrix W i of any given ensemble member i is written as the Hadamard product of a shared weight matrix W 0 and a local rank-1 matrix r i s i :\nW i = W 0 \u2022 r i s i .(3)\nThe vectors r and s are commonly referred to as fast weights. Unless otherwise stated, Plex applies BE to all layers in the last 2 residual blocks of the network. This idea follows work for mixture of experts (Riquelme et al., 2021).\n\nSpectral-normalized Neural Gaussian Process (SNGP) Unlike ensemble approaches, SNGP proposed by  focuses on improving the uncertainty quality of a neural network given a fixed representation (a.k.a. deterministic uncertainty quantification setting (Van Amersfoort et al., 2020). When applied to a DNN without pretraining, SNGP enhances the DNN uncertainty property by applying spectral normalization to the hidden weights, and replaces the output layer from a dense layer to a random-feature Gaussian process (GP) layer. That is, given hidden representations h(x), the GP layer enables scalable computation of a GP posterior by applying a random feature approximation \u03c6 to the predictive function and then a Laplace approximation to the predictive variance:\ng(x) \u223c N (logit(x), var(x)) logit(x) = \u03c6(x) \u03b2, where \u03c6(x) = cos(W h(x) + b) var(x) = \u03c6(x) (I + \u03a6 T \u03a6) \u22121 \u03c6(x),\nwhere (W , b) are frozen random weights of the random feature embedding \u03c6(x) = cos(W h(x)+ b), and \u03a6 \u03a6 = i \u03c6(x i )\u03c6(x i ) is the covariance of the random feature embedding estimated using the training data. Liu et al. ( , 2022 show that this combined technique improves the model's awareness of the semantic distance between the test and train examples on the data manifold, leading to improved performance in calibration and out-of-domain detection. When applied to a large pretrained DNN, we find it sufficient to only use the last-layer Gaussian process (i.e., omit the spectral normalization regularization), as the pretrained embedding has already provided a semantic-distance-aware representation of the data.\n\nHeteroscedastic last layer (Het) Heteroscedastic last layers are designed to model input-dependent label noise/label uncertainty (a.k.a. aleatoric uncertainty (Kendall and Gal, 2017)) that is present in the data. We use the Heteroscedastic (Het) last layer introduced by Collier et al. (2020,2021) who place a multivariate Gaussian distribution over the logits in a standard DNN classifier. A low-rank approximation to the K \u00d7 K covariance matrix (K = number of classes/outputs) is made when K is large and (Collier et al., 2021) further develop a parameter efficient version of the method with parameterization inspired by BE to enable scaling to tens of thousands of classes.\n\nAmong the above methods, None, Het, GP, and BE are both memory and compute efficient, since they only require a single forward pass from a single model to compute the output distribution. We also experiment with deep ensembles (Section 4.3), which is the most expensive in both memory and compute, since they require forward passes from multiple trained models, as well as Monte Carlo Dropout (Gal and Ghahramani, 2016  We study the performance of five different OOD detection methods, MSP, Maha, RMaha, Entropy and MaxLogit, across the various tasks and model types. As shown in Table 16, Mahalanobis distance outperforms all other scores, and Relative Mahalanobis distance performs the second best.\n\nDespite the good performance, one drawback of Mahalanobis distance based method is that its computational time is linear to the number of classes. For the ImageNet2012 vs Places365 task, the in-distribution data ImageNet2012 has 1,000 classes where by definition of Mahalanobis distance method, we need to fit 1K Gaussians for each of the classes and compute the distance between the test input to each of the fitted Gaussian. It becomes very time consuming and not scalable. Therefore in that task, we study the lightweight OOD scores, including MSP, Entropy, and MaxLogit. Interestingly, we noticed that MaxLogit is the best among the three methods for single models (None, GP, None\u2192GP) except for Het and None\u2192Het, while Entropy is the best for ensemble models (BE, DE based models).\n\nOverall we suggest using Mahalanobis distance based methods to achieve the best performance, but in case there is a computation budget, MaxLogit is a good choice for single models and Entropy is a good choice for ensemble models.\n\n\nAppendix F. Additional Results for Selective Prediction\n\nWe performed model selection on pretrained models finetuned with different hyperparameters using the area under the in-distribution selective-prediction accuracy curve. We evaluated the predictive performance of the finetuned models on in-distribution and distributionally shifted data. In particular, we computed models' predictive accuracy, negative log-likelihood, expected calibration error, and the area under the selective prediction curve obtained by computing the area under the ROC curve for selective prediction referral rates from 0%  The in-distribution Severity Shift task is concerned with diagnosing diabetic retinopathy amongst images containing at worst moderate cases of diabetic retinopathy. Accuracy is used as the evaluation metric and predictive entropy is used as the per-example uncertainty estimate.\n\nto 99% using different evaluation metrics (accuracy and, where applicable, AUROC and AUPRC).\n\n\nAppendix G. Extensions to Sparse Mixtures of Experts\n\nIn this section, we discuss extensions of some approaches of this paper in the light of recent advances in sparse mixtures of experts models (sparse MoEs).\n\n\nG.1 Background\n\nSparse MoEs constitute a family of models that rely on conditional computation (Bengio et al., 2013(Bengio et al., , 2015 to combine multiple submodels-referred to as experts-in an input-dependent fashion. Sparse MoEs have successfully been applied both in NLP (Shazeer et al., 2017;Lepikhin et al., 2021;Fedus et al., 2022) and more recently in computer vision (Riquelme et al., 2021;Lou et al., 2021). The goal of conditional computation is to grow the number of parameters of the model while maintaining its training and inference costs constant, by only activating a particular subset of experts given an input. This is in contrast with classical neural networks that use all their parameters for each input.\n\nIn the context of this paper, we focus on Allingham et al. (2021) that studied the robustness and uncertainty estimates of several extensions of ViT endowed with conditional computation, namely:\n\n\u2022 V-MoE: The authors of Riquelme et al. (2021) extended ViT to sparse MoEs by placing experts at the level of the MLPs of the transformer architecture. Since ViT operates on image patches, the conditional computation of V-MoEs also operates on image patches. More specifically, the image patches are sparsely routed to K out of E available experts (in practice, E = 32). The experts are selected by computing some gating weights in combination with a top-K strategy, allowing for an end-to-end training procedure. When only a single expert is selected (K = 1), the training and inference costs of V-MoEs almost match those of a standard ViT model (Riquelme et al., 2021), while leading to improved predictive performance at both pretraining and finetuning time.\n\nIn the next section, we will use the acronym MoE to refer to V-MoE. In what follows, we will use the acronym [MoE] 4 to refer to a deep ensemble formed by four MoEs where both the upstream and downstream models were varied. In the situation where only the downstream models were varied for a single fixed pretrained model, we use the notation MoE \u2192 [MoE] 4 . We will use the same convention for the model without any changes, None. While the rest of the paper uses ensembles of size 3, we consider in this section ensembles of size 4 to be faithful to the setting of Allingham et al. (2021).\n\n\u2022 E 3 : The authors of Allingham et al. (2021) further designed an efficient ensemble approach, referred to as E 3 , to overcome the computational burden of the naive ensembling of V-MoEs described above. In a nutshell, E 3 jointly learns an ensemble of smaller sparse MoEs where all layers not equipped with experts (e.g., attention layers) are shared across the ensemble members. Interestingly, E 3 features some of the extensions of Plex due to some of its conceptual similarities with BE. Allingham et al. (2021) observed that E 3 tends to lie either on, or close to, the Pareto frontiers of several metrics-e.g., NLL, Brier score and few-shot classification error-versus the computational cost of the models, as measured by FLOPs.  \n\n\nG.2 Results\n\nWe present the results in Table 18. They extend the evaluation from Allingham et al. (2021) to the additional metrics of this paper related to the computer vision tasks. We summarize the results with the prediction, uncertainty and adaptation scores.\n\nIn agreement with the conclusions reported in Allingham et al. (2021), we can see that [MoE] 4 outperforms 7 the standard deep ensemble [None] 4 while having the same computational cost. This confirms the complementary of static ensembling with the adaptivity of sparse MoEs within the broader evaluation of this paper. More generally, combining ensembling and sparse MoEs seems to be a promising direction to improve the reliability of models.\n\nAs observed in Riquelme et al. (2021), MoE has strong performance in fewshot learning tasks, which is highlighted here by the adaptation scores of  \n\n\nH.2 Impact of heteroscedastic last layer downstream\n\nIn Figure 31, Figure 32 and Figure 33 we look at the core in-distribution performance metrics for downstream vision datasets. Het\u2192Het refers to a model trained with a heteroscedastic output layer upstream on JFT and finetuned on the target dataset with a heteroscedastic head. None\u2192Het is similar but where the cheaper and simpler baseline deterministic model without a heteroscedastic head is used upstream. BE\u2192BE + Het refers to pretraining using a Batch Ensemble model upstream on JFT and finetuning with a BE model with a heteroscedastic head downstream.\n\nOn all downstream datasets None\u2192Het performs similarly or better than Het\u2192Het. The outperformance of a deterministically pre-trained model finetuned with a heteroscedastic last layer, indicates that the gains from a heteroscedastic model downstream can be realized without heteroscedastic upstream pretraining. This is a surprising result given that the heteroscedastic model improves JFT performance upstream. However this result suggests that when the representations are transferred from this upstream model the Deterministic model is sufficient. Given that heteroscedastic model is more expensive than the deterministic model and requires tuning the temperature and the rank for the low-rank approximation hyperparameters this is an encouraging result.\n\nPlex: heteroscedastic last layer helps downstream. None\u2192Het outperforms None on all metrics for ImageNet and CIFAR-100. Performance on CIFAR-10 is saturated and similar. This demonstrates that applying the heteroscedastic head on the downstream model improves downstream performance. Further, see Figure 7 which shows that on a wide variety of reliability metrics beyond in-distribution accuracy, NLL and ECE None\u2192Het outperforms None. We will also see later that the heteroscedastic method models label uncertainty better than baseline models. Heteroscedastic can be combined with BE upstream pretraining for excellent results. The results in the above table and the more comprehensive summary results in Figure 7 show that the heteroscedastic head can be combined with a BE model to give the best performance single model on vision tasks (this is Plex). BE\u2192BE+Het (Plex) combines the epistemic uncertainty model capability of Batch Ensembles with the label uncertainty modeling of the heteroscedastic method. Plex approaches Deep Ensembles in terms of overall reliability, despite being substantially cheaper. Therefore we can recommend this model as our best single model for use by practitioners.\n\n\nAppendix I. Summarization of Language Results\n\nWe first compare the performance across types of uncertainty methods, fixing the architecture size to T5-base. We compare performances in prediction, uncertainty calibration, and humanmodel collaboration, across all datasets (MNLI, NaLUE and Toxic Comments) and all splits (In-domain, OOD, and tail-population). Table 20 reports the full results, and Figure 34 summarizes the rankings of uncertainty methods under each type of population shift (indomain v.s. OOD v.s. tail-population). Among all methods, DE+GP, Plex (i.e., BE+GP), BE, and MC Dropout tend to have the strongest performance. In particular, DE+GP almost always dominates the other methods on MNLI and NaLUE, and remains competitive in the case of label imbalance (i.e., Toxic Comments). However, DE+GP is an expensive method that costs x10 more in memory and compute and therefore is not competitive in scale (a more thorough analysis is in Section 4.3). On the other hand, among the more efficient, single-model methods, BE and Plex perform well on MNLI and NaLUE (notably, outperform the most expensive DE), while MCD stands out in the Toxic Comments. The above observations suggest that, when the training example has a simple distribution, quantifying output-layer uncertainty alone is sufficient to attain strong performance. However, when there are pathologies in the data distribution (e.g., extreme label imbalance), quantifying the uncertainty within the model's intermediate representations (e.g., via some form of perturbation like BE) becomes important.\n\nWe then investigate how a model's uncertainty performance is impacted by the architecture size. For model size scaling, we evaluate Plex, None, and MC Dropout, the three best-performing and efficient methods in the previous study. We evaluate the performance of each method under three progressively larger architectures: T5 S, T5 B, and T5 L, and observe how the behavior changes across the method and with respect to the architecture size. Table 20 reports the full results, and Figure 35 summarizes the rankings of uncertainty methods organized by the sizes of the architecture. As shown, comparing across architecture   sizes, we see a larger architecture almost always leads to stronger performance in collaborative performance. This trend remains largely consistent even when out-of-distribution.   \n\nFigure 2 :\n2Top row: Examples of Plex's capabilities in vision: (left) Label uncertainty in ImageNet ReaL-H, demonstrating the ability to capture the inherent ambiguity of image labels assigned by humans. (middle) Active learning on ImageNet1K, displaying that Plex achieves significantly higher accuracy compared to a non-pretrained baseline and with fewer labels. (right) Zero-shot open set recognition on ImageNet1K vs Places365, showing that Plex can distinguish visually similar images without finetuning. Bottom row: Examples of Plex's capabilities in language: (left) Plex enables human+AI collaboration with selective prediction, where the model is able to defer a fraction of test examples to humans.\n\n\nFor example, \u223c10% of the examples in Wikipedia Talk Corpus examples have positive labels, since most online content is not toxic (Kivlichan et al., 2021).\n\nFigure 7 :\n7(left) Ranking of method ablations over 139 metrics on vision tasks and (right) 54 metrics on language tasks. Each model has a box plot of rankings (lower is better). Plex's combination of efficient ensembling and last-layer changes ranks best on average.\n\nFigure 8 :\n8Reliability performance of ViT-Plex L, aggregated by each downstream dataset.\n\nFigure 11 :\n11Trading off model size with deep ensemble size. Overall, scaling the single model from Small to Large has a greater influence on the overall performance.\n\nFigure 12 :\n12Expected calibration error averaged over three in-distribution vision evaluation datasets (top) and six out-of-distribution vision evaluation datasets (bottom). Model types are shown on the x-axis.\n\nFigure 13 :\n13Expected calibration error averaged over two in-distribution language evaluation datasets (top) and three out-of-distribution language evaluation datasets (bottom). Model types are shown on the x-axis.\n\nFigure 14\n14Figure 14: Ranking performance in selective prediction (lower is better) across different methods (14a-14c) and model sizes (14d-14f). Each model's box plot is its ranking over multiple datasets: ranking is done by AUROC on MNLI and Toxic Comments, and by Accuracy on NaLUE (AUROC is not well-defined for multi-class classification).\n\nFigure 15 :\n15Selective Prediction Pipeline.\n\nFigure 16 :\n16Selective Prediction on RETINA (Out-of-Distribution). (left) The Country Shift task is concerned with diagnosing diabetic retinopathy from retina scans obtained using different medical equipment and a different patient population. (right)\n\n\n, models including None, GP, Het, MCD, BE, BE-GP, Deep Ensemble (DE), DE-GP are evaluated for their detection performance on Standard-OOS and Near-OOS. BE and DE-GP outperform the other models, followed by MCD.\n\nFigure 18 :\n18Predictive performance on downstream datasets, with (top) accuracy and (bottom) negative log-likelihood over CIFAR-10, CIFAR-100, and ImageNet respectively as a function of the model scale (number of model parameters). The blue dotted line is the Pareto frontier. Note, while number of parameters is a useful comparison of model scale (e.g. memory footprint) it doesn't capture all aspects of scale such as computational overhead, training time, etc. SoTA SoTA Plex L Plex L None L None L (Ensemble) (Single Model) (JFT) (I21K) (JFT)\n\nFigure 19 :\n19Predictive performance with, starting from top row, (1) accuracy, (2) NLL, and (3) Brier score across 4 out-of-distribution datasets (columns): ImageNet-C, ImageNet-A, ImageNet-R, and ImageNet-V2 respectively as a function of model scale (number of parameters). Plex L trained on JFT consistently outperforms other methods and pretraining datasets, and increasing model scale consistently leads to improved metric performance under corrupted shifts.training from scratch yields better predictive performance on in-distribution evaluation tasks. The Plex entensions also yield a significant improvement both in accuracy and log-likelihood.ImageNet Covariate ShiftFigure 19displays performance over multiple types of covariate shift as a function of model scale (number of parameters): image corruptions (ImageNet-C (Hendrycks and Dietterich, 2019)), natural adversarial examples (ImageNet-A (Hendrycks et al., 2021c)), artistic renditions (ImageNet-R (Hendrycks et al., 2021a)), and \"natural\" shift obtained by following ImageNet's data collection procedure to obtain a new test set (ImageNet-V2(Recht et al., 2019; Taori et al.,  \n\nFigure 20 :\n20Stability measured with Anchor Accuracy and Accuracy PMK on ImageNet-Vid-Robust and YTBB-Robust.\n\nFigure 21 :\n21T5-Plex model's ranking performance in generalization under domain shift compared to different methods (21a-21c) and across architecture sizes (21d-21f).\n\nFigure 23 :\n23T5-Plex model's ranking performance in subpopulations with spurious patterns compared to different methods (23a-23c) and across architecture sizes (23d-23f).\n\nFigure 24 :\n24Active learning on (a) CIFAR-10, (b) CIFAR-100, (c) ImageNet2012 and (d) Places365\n\nFigure 25 :Figure 26 :\n2526Few-shot performance averaged over 9 image datasets. (top) Comparison of pretrained model choices. (bottom) Comparison of pretrained model and dataset scale, where the default dataset is JFT and I21K signifies ImageNet21K. BatchEnsemble's representation slightly outperforms others. Model and pretraining dataset size have the most significant effect, with larger doing better and ImageNet21K doing better than JFT. Linear evaluation versus full-model training on ImageNet. Full-model training consistently outperforms linear evaluation at 5-shot and higher.\n\nFigure 27 :\n27Few-shot uncertainty performance averaged over 9 image datasets for calibration and selective prediction, and 2 datasets for open set recognition. (top) Comparison of pretrained model choices. (bottom) Comparison of pretrained model and dataset scale, where the default dataset is JFT and I21K signifies ImageNet21K. Plex's BatchEnsemble representation works best on few-shot open set recognition, and with ImageNet21K as the pretraining dataset. Calibration and selective prediction performance are similar across model and scaling choices; they remain challenging problems.\n\n\n(cf. LaMDA (Thoppilan et al., 2022), PaLM (Chowdhery et al., 2022)) and wider suite of tasks, cf. BIG-bench (Srivastava et al., 2022).Zoubin Ghahramani. Probabilistic machine learning and artificial intelligence. Nature, 521 X Zhai,A Kolesnikov, N Houlsby, and L Beyer. Scaling vision transformers. arXiv preprint  arXiv:2106.04560, 2021.    \n\n\u2022\nImageNet1K: ImageNet-A (Hendrycks et al., 2021c), ImageNet-C (Hendrycks and Dietterich, 2019), ImageNetV2 (Recht et al., 2019), ImageNet-Vid-Robust, YTBB Robust (Shankar et al., 2021), ObjectNet (Barbu et al., 2019), and ImageNet-R (Hendrycks et al., 2021a). \u2022 RETINA: RETINA's Country Shift dataset\n\nFigure 28 :\n28Selective Prediction on RETINA (In-Distribution). (left) The indistribution Country Shift task is concerned with diagnosing diabetic retinopathy. (right)\n\n\u2022\nDeep ensembles of V-MoEs: Allingham et al. (2021) explored the combination of the static ensembling of deep ensembles and the conditional computation of V-MoEs.They found that both effects are complementary. In particular, over tasks where either V-MoEs or deep ensembles are known to perform well (e.g., respectively, few-shot classification and OOD detection), the combined approach inherits from the best of both worlds, while matching the cost of standard deep ensembles.\n\nFigure 30 :Figure 31 :Figure 32 :Figure 33 :\n30313233Performance on ImageNet21K for deterministic and heteroscedastic models. Performance on ImageNet1K for deterministic, heteroscedastic (upstream and downstream), deterministic upstream\u2192heteroscedastic downstream, BatchEnsemble L (upstream and downstream) and BatchEnsemble L upstream\u2192BatchEnsemble L + heteroscedastic downstream models. Performance on CIFAR-10 for deterministic, heteroscedastic (upstream and downstream), deterministic upstream\u2192heteroscedastic downstream, BatchEnsemble L (upstream and downstream) and BatchEnsemble L upstream\u2192BatchEnsemble L + heteroscedastic downstream models. Performance on CIFAR-100 for deterministic, heteroscedastic (upstream and downstream), deterministic upstream\u2192heteroscedastic downstream, BatchEnsemble L (upstream and downstream) and BatchEnsemble L upstream\u2192BatchEnsemble L + heteroscedastic downstream models.\n\n\nSelective Pred., ALL.\n\nFigure 34 :\n34T5-Plex model's ranking comparison between different uncertainty methods and across different evaluation datasets. IND: in-domain. OOD: out-of-domain. SUB: subpopulation shift. ALL: aggregated performance across all datasets. Selective Pred., ALL.\n\nFigure 35 :\n35T5-Plex model's ranking comparison between architecture sizes and across different evaluation datasets. IND: in-domain. OOD: out-of-domain. SUB: subpopulation shift. ALL: aggregated performance across all datasets.\n\n\nand within current literature, it can be0.29 \n\n0.38 \n\n0.46 \n\n0.55 \n\n0.85 \n\n0.90 \n\n0.95 \n\n1.00 \n\n0.93 \n\n0.95 \n\n0.97 \n\n1.00 \n\n0.77 \n0.81 \n0.86 \n0.90 \n\n0.87 \n\n0.89 \n\n0.90 \n\n0.92 \n\n0.75 \n\n0.80 \n\n0.85 \n\n0.90 \n\n0.55 \n\n0.70 \n\n0.85 \n\n1.00 \n\n0.47 \n\n0.65 \n\n0.82 \n\n1.00 \n\n0.53 \n0.65 \n0.78 \n0.90 \n\n0.77 \n\n0.85 \n\n0.93 \n\n1.00 \n0.61 \n\n0.72 \n\n0.84 \n\n0.95 \n\n0.29 \n\n0.38 \n\n0.46 \n\n0.55 \n\n0.85 \n\n0.90 \n\n0.95 \n\n1.00 \n\n0.93 \n\n0.95 \n\n0.97 \n\n1.00 \n\n0.77 \n0.81 \n0.86 \n0.90 \n\n0.87 \n\n0.89 \n\n0.90 \n\n0.92 \n\n0.75 \n\n0.80 \n\n0.85 \n\n0.90 \n\n0.55 \n\n0.70 \n\n0.85 \n\n1.00 \n\n0.47 \n\n0.65 \n\n0.82 \n\n1.00 \n\n0.53 \n0.65 \n0.78 \n0.90 \n\n0.77 \n\n0.85 \n\n0.93 \n\n1.00 \n0.61 \n\n0.72 \n\n0.84 \n\n0.95 \n\n0.68 \n\n0.70 \n\n0.72 \n\n0.75 \n\n0.91 \n\n0.93 \n\n0.94 \n\n0.95 \n\n0.91 \n0.93 \n0.94 \n0.95 \n\n0.96 \n\n0.96 \n\n0.97 \n\n0.98 \n\n0.88 \n\n0.90 \n\n0.92 \n\n0.95 \n\n0.88 \n0.90 \n0.92 \n0.95 \n\n0.88 \n\n0.90 \n\n0.92 \n\n0.95 \n\n0.68 \n\n0.70 \n\n0.72 \n\n0.75 \n\n0.91 \n\n0.93 \n\n0.94 \n\n0.95 \n\n0.91 \n0.93 \n0.94 \n0.95 \n\n0.96 \n\n0.96 \n\n0.97 \n\n0.98 \n\n0.88 \n\n0.90 \n\n0.92 \n\n0.95 \n\n0.88 \n0.90 \n0.92 \n0.95 \n\n0.88 \n\n0.90 \n\n0.92 \n\n0.95 \n\n0.68 \n\n0.70 \n\n0.72 \n\n0.75 \n\n0.91 \n\n0.93 \n\n0.94 \n\n0.95 \n\n0.91 \n0.93 \n0.94 \n0.95 \n\n0.96 \n\n0.96 \n\n0.97 \n\n0.98 \n\n0.88 \n\n0.90 \n\n0.92 \n\n0.95 \n\n0.88 \n0.90 \n0.92 \n0.95 \n\n0.88 \n\n0.90 \n\n0.92 \n\n0.95 \n\nAccuracy \nMNLI (Subpop shift) \n\nAccuracy \nMNLI \n\nAccuracy \nMNLI \n(OOD) \n\nAUROC \nWikipediaTalk \n\nAUROC \nWikipediaTalk (OOD) \n\nAUROC \nWikipediaTalk \n(Subpop shift) \n\nAUROC \nNaLUE \n\nPlex L (ours) \nSOTA (specialized) \nPlex B (ours) \n\n\n\n\n\u2022 RETINA is a set of benchmarking datasets containing retina scans exhibiting varying degrees of diabetic retinopathy, a medical condition that can result in a loss of eyesight(Band et al., 2021). We chose RETINA as an example of difficult transfer, since retina images are quite different from natural web images used for pretraining. RETINA includes two types of splits: a \"Country Shift\" split with an in-distribution test set and a test set exhibiting covariate shift; and a \"Severity Shift\" split with an in-distribution test set and a test set containing labels not included in the training data, representing more severe types of diabetic retinopathy.). Following Dosovitskiy et al. (2021), we use \n99% of the training set for training and 1% for validation. \n\n\u2022 CIFAR-100 is a dataset of web images with a training set of 50,000 examples and a test \nset of 10,000 examples. Following Dosovitskiy et al. (2021), we use 99% of the training set \nfor training and 1% for validation. \n\n\u2022 ImageNet1K is an image dataset organized according to the WordNet hierarchy, with a \ntraining set of roughly 1.2 million examples and a test set of 50,000 examples (Deng et al., \n2009). Following Dosovitskiy et al. (2021), we use 98% of the training set for training and \n2% for validation. \n\n\u2022 We use 7 datasets with a range from 1,880 to 8,144 training examples: Describable \nTextures Dataset (Cimpoi et al., 2014), UC Merced (Yang and Newsam, 2010), Caltech \n101 (Fei-Fei et al., 2004), Oxford-IIIT Pets (Parkhi et al., 2012), Colorectal Histology \n(Kather et al., 2016), Caltech-UCSD Birds 200 \n\nTable 1 :\n1Vision datasets for evaluation on distribution shift.Covariate shift \nSubpopulation shift \nSemantic (class) shift \n\nMNLI \nMNLI-mismatched HANS \n-\nWikipediaTalk CivilComments \nCivilCommentsIdentity -\nNaLUE \n-\nNaLUE-tail \nStandard-OOS, Near-OOS \n\n\n\nTable 2 :\n2Language datasets for evaluation on distribution shift.\n\n\nPretraining Dataset 1M steps 2M steps 4M steps 8M stepsJFT 300M \n72.1% \n72.9% \n73.0% \n73.0% \nJFT 4B \n72.7% \n74.4% \n74.6% \n-\n\n\n\nTable 3 :\n3ImageNet 10-shot accuracy consistently improves with a larger pretraining dataset.10 2 \n\n\nTable 5 :\n5Calibration AUROC, measured on the in-distribution test split of each vision \ndataset (left) and text dataset (right). \n\nExpected Calibration Error Expected calibration error (ECE) (Naeini et al., 2015) \n\n\nof-Distribution 96.9% 94.4% 96.9% 93.6%Country Shift \nIn-Distribution \n96.3% 96.2% 96.3% 95.1% \nOut-of-Distribution 90.4% \n89.5% 91.2% 89.4% \n\nSeverity Shift \nIn-Distribution \n95.4% 96.3% 96.0% 96.0% \nOut-\n\nTable 9 :\n9The MSP based OOD AUROC of all types of models across the five tasks.\n\nTable 10 :\n10OOS intent detection performance with AUROC and AUPRC for different models.\n\n\nFigure 17: KL divergence on ImageNet ReaL-H (left) and CIFAR-10H (right) for None, None upstream\u2192heteroscedastic downstream, BatchEnsemble upstream\u2192BatchEnsemble L + heteroscedastic downstream, and Deep Ensemble models.None \nNone->Het \nPLEX \nDE \n\n0.70 \n\n0.72 \n\n0.74 \n\n0.76 \n\n0.78 \n\nKL Divergence \n\nImageNet ReaL-H \n\nNone \nNone->Het \nPLEX \nDE \n0.42 \n\n0.43 \n\n0.44 \n\n0.45 \n\n0.46 \n\n0.47 \n\n0.48 \n\n0.49 \n\nKL Divergence \n\nCIFAR-10H \n\n\n\nTable 11 :\n11Results on the RETINA Country Shift task (for in-distribution evaluation). The SoTA (Ensemble) outperforms Plex L pretrained on both ImageNet-21K or JFT on all evaluation metrics but ECE, where Plex L pretrained on ImageNet-21K performs best.\n\nTable 19\n19performance of GP, MCD and BE varies across the datasets(Table 19). Comparing across model scale, the methods' generalization performance generally improves as the model scale increases, with Plex (BE+GP) the method benefiting the most strongly from scaling, delivering on average the strongest performance at T5 large .summarizes the performance of different \n\n\nTable 13 :\n13Zero-shot open set recognition with Plex, varying scale and detection method.for few-shot learning, and OOD AUROC for open set recognition. Open set recognition \nrequires an OOD dataset in addition to a test split, so we only evaluate it for two datasets: \nCIFAR-100 with an OOD dataset of CIFAR-10; and ImageNet1K with an OOD dataset of \nPlaces365. Calibration performance is roughly comparable across methods. BatchEnsemble \nhas a noticeable improvement on open set recognition. The scale of the pretraining dataset \nhas no significant effect on calibration, where the trend is weaker than what's seen on \ncalibration of finetuned models (Section 5.1.1). The choice of pretraining dataset does have \na noticeable effect on open set recognition, where like in Section 5.3.2, Plex on ImageNet21K \nsurprisingly does better than JFT models. \n\n\n\nTable 14 :\n14References for the task-specific state-of-the-art used in\n\nTable 15 :\n15Metrics used for each task. \u2191 / \u2193 mean higher / lower is better.3. Label uncertainty. We use CIFAR-10H (Peterson et al., 2019) which captures human \nuncertainty over labels for CIFAR-10 dataset. We also construct a larger-scale variant, \nwhich we call ImageNet ReaL-H. Individual human ratings were recollected for the \noriginal ImageNet test set, available as raw data from ImageNet ReaL (Beyer et al., \n2020), and we use them to newly construct soft label targets. \n\n\n\n\n).C-100 vs C-10 \nImageNet vs Places \nMSP Entropy MaxLogits Maha RMaha MSP Entropy MaxLogits \n\nNone \n0.929 \n0.944 \n0.953 \n0.982 \n0.965 \n0.828 \n0.855 \n0.880 \nNone I21K \n0.924 \n0.934 \n0.940 \n0.964 \n0.943 \n0.838 \n0.861 \n0.887 \nNone\u2192GP \n0.927 \n0.940 \n0.948 \n0.975 \n0.956 \n0.824 \n0.851 \n0.877 \nNone\u2192Het \n0.948 \n0.962 \n0.948 \n0.985 \n0.970 \n0.827 \n0.854 \n0.827 \nNone\u2192DE L 0.940 \n0.953 \n0.940 \n0.982 \n0.965 \n0.831 \n0.860 \n0.831 \nNone\u2192BE \n0.924 \n0.939 \n0.924 \n0.980 \n0.962 \n0.822 \n0.850 \n0.822 \nGP \n0.927 \n0.936 \n0.945 \n0.971 \n0.945 \n0.828 \n0.854 \n0.873 \nHet \n0.948 \n0.964 \n0.948 \n0.981 \n0.979 \n0.828 \n0.856 \n0.828 \nDE L \n0.946 \n0.961 \n0.946 \n0.987 \n0.976 \n0.834 \n0.863 \n0.834 \nBE L \n0.940 \n0.954 \n0.940 \n0.985 \n0.971 \n0.827 \n0.855 \n0.827 \nBE L (I21K) 0.934 \n0.943 \n0.934 \n0.975 \n0.959 \n0.842 \n0.864 \n0.842 \nBE\u2192BE+Het 0.954 \n0.969 \n0.954 \n0.984 \n0.980 \n0.831 \n0.857 \n0.831 \nBE B \n0.906 \n0.927 \n0.906 \n0.966 \n0.959 \n0.801 \n0.831 \n0.801 \nBE S \n0.864 \n0.889 \n0.864 \n0.923 \n0.916 \n0.773 \n0.803 \n0.773 \n\n\n\nTable 16 :\n16The AUROC based on the five OOD measures on the most challenging near-OOD tasks CIFAR-100 vs CIFAR-10 and ImageNet2012 vs Places365.\n\nTable 17 :\n17Selective prediction performance, using AUROC as the evaluation metric, on the RETINA Country Shift task (for in and out-of-distribution evaluation). For in-distribution, the SoTA (Ensemble) outperforms Plex L pretrained on both ImageNet-21K or JFT. For out-of-distribution, Plex L pretrained on either ImageNet-21K or JFT outperforms state-ofthe-art.SoTA \n\nSoTA \nPlex L Plex L None L None L \n(Ensemble) (Single Model) (JFT) (I21K) (JFT) \n(I21K) \n\nIn-Distribution \n95.0% \n94.4% \n92.7% 92.3% \n94.3% \n94.1% \nOut-of-Distribution \n77.9% \n78.3% \n84.8% 78.9% \n79.5% \n78.3% \n\nPlex L (I21K) \nNone L (I21K) \nHet L (I21K) \nGP L (I21K) \n\n0.0 \n0.2 \n0.4 \n0.6 \n\nProportion of Cases Referred to Expert \n\n0.88 \n\n0.90 \n\n0.92 \n\n0.94 \n\n0.96 \n\n0.98 \n\nAccuracy \n\n(a) Country Shift: In-Distribution \n\n0.0 \n0.2 \n0.4 \n0.6 \n\nProportion of Cases Referred to Expert \n\n0.88 \n\n0.90 \n\n0.92 \n\n0.94 \n\n0.96 \n\n0.98 \n\nAccuracy \n\n(b) Severity Shift: In-Distribution \n\n\n\n\nScore Score prediction Score uncertainty Score adaptationNone \n85.99 \n91.13 \n92.71 \n84.65 \nMoE \n86.76 \n90.52 \n91.97 \n85.75 \nE 3 \n\u2212 \n91.44 \n92.77 \n\u2212 \n\nNone \u2192 [None] 4 \n\u2212 \n91.41 \n92.83 \n\u2212 \nMoE \u2192 [MoE] 4 \n\u2212 \n91.07 \n92.82 \n\u2212 \n\n[None] 4 \n88.54 \n91.99 \n93.44 \n87.59 \n[MoE] 4 \n88.66 \n91.53 \n93.51 \n87.77 \n\n\n\nTable 18 :\n18Results of sparse MoE variants developed in Allingham et al. (2021). E 3 , None \u2192 [None] 4 and MoE \u2192 [MoE] 4 do not have adaptation scores since those approaches are applied only at finetuning time.\n\n\nMoE and [MoE] 4 . Moreover, Allingham et al. (2021) further observed that MoE did not perform well in terms of calibration, as illustrated by its lower uncertainty score compared with None. The efficient ensemble E 3 has performance comparable to deep ensembles formed from a single pretrained model (i.e., None \u2192 [None] 4 and MoE \u2192 [MoE] 4 ), while being substantially cheaper. This conclusion echoes the take-home messages from Allingham et al. (2021). Performance on JFT for deterministic and heteroscedastic models.None \nHet \n\n45.75 \n\n46.00 \n\n46.25 \n\n46.50 \n\n46.75 \n\n47.00 \n\n47.25 \n\n47.50 \n\nprec@1 \n\nJFT \n\n(a) Accuracy \n\nNone \nHet \n\n7.60 \n\n7.65 \n\n7.70 \n\n7.75 \n\n7.80 \n\nNLL \n\nJFT \n\n(b) NLL \n\nFigure 29: None \nHet \n\n46.2 \n\n46.4 \n\n46.6 \n\n46.8 \n\n47.0 \n\n47.2 \n\n47.4 \n\nprec@1 \n\nImageNet21k \n\n(a) Accuracy \n\nNone \nHet \n\n5.600 \n\n5.625 \n\n5.650 \n\n5.675 \n\n5.700 \n\n5.725 \n\n5.750 \n\n5.775 \n\n5.800 \n\n\n\nTable 19 :\n19Comparison of method performance between uncertainty methods. Black: Best. Dark Grey: Second. Light Grey: Third.None S \nMCD S \nPlex S \nNone B \nMCD B \nPlex B \nNone L \nMCD L \nPlex L \nTask \nSplit \nScore \n\nMNLI \n\nIn-domain \n\ncalibration \n0.399 \n0.406 \n0.364 \n0.381 \n0.416 \n0.388 \n0.39 \n0.404 \n0.394 \ngeneralization \n0.924 \n0.927 \n0.913 \n0.938 \n0.946 \n0.948 \n0.963 \n0.964 \n0.965 \nselect. pred. \n0.953 \n0.959 \n0.942 \n0.961 \n0.973 \n0.973 \n0.982 \n0.985 \n0.985 \n\nOOD \n\ncalibration \n0.398 \n0.396 \n0.367 \n0.391 \n0.41 \n0.394 \n0.418 \n0.411 \n0.406 \ngeneralization \n0.924 \n0.93 \n0.911 \n0.937 \n0.946 \n0.948 \n0.963 \n0.965 \n0.967 \nselect. pred. \n0.953 \n0.96 \n0.94 \n0.959 \n0.972 \n0.973 \n0.983 \n0.986 \n0.987 \n\nSubpopulation \n\ncalibration \n0.555 \n0.56 \n0.514 \n0.451 \n0.474 \n0.401 \n0.417 \n0.45 \n0.447 \ngeneralization \n0.648 \n0.619 \n0.59 \n0.749 \n0.739 \n0.788 \n0.817 \n0.807 \n0.803 \nselect. pred. \n0.747 \n0.717 \n0.677 \n0.811 \n0.831 \n0.871 \n0.875 \n0.896 \n0.876 \n\nNaLUE \n\nIn-domain \n\ncalibration \n0.507 \n0.48 \n0.498 \n0.498 \n0.471 \n0.486 \n0.486 \n0.453 \n0.496 \ngeneralization \n0.942 \n0.932 \n0.937 \n0.939 \n0.938 \n0.94 \n0.931 \n0.928 \n0.944 \nselect. pred. \n0.937 \n0.926 \n0.929 \n0.936 \n0.932 \n0.938 \n0.93 \n0.922 \n0.935 \nOOS, Near \ndetection \n0.71 \n0.756 \n0.689 \n0.706 \n0.766 \n0.716 \n0.692 \n0.733 \n0.781 \nOOS, Standard \ndetection \n0.968 \n0.992 \n0.999 \n0.964 \n0.994 \n0.991 \n0.956 \n0.991 \n0.991 \n\nSubpopulation \n\ncalibration \n0.528 \n0.462 \n0.499 \n0.518 \n0.466 \n0.519 \n0.518 \n0.466 \n0.492 \ngeneralization \n0.878 \n0.854 \n0.851 \n0.866 \n0.858 \n0.873 \n0.843 \n0.83 \n0.871 \nselect. pred. \n0.864 \n0.836 \n0.84 \n0.862 \n0.829 \n0.856 \n0.835 \n0.801 \n0.835 \n\nToxic \nComments \n\nIn-domain \n\ncalibration \n0.455 \n0.445 \n0.478 \n0.459 \n0.442 \n0.471 \n0.448 \n0.451 \n0.436 \ngeneralization \n0.879 \n0.898 \n0.863 \n0.888 \n0.904 \n0.895 \n0.886 \n0.906 \n0.89 \nselect. pred. \n0.932 \n0.938 \n0.919 \n0.936 \n0.941 \n0.94 \n0.936 \n0.944 \n0.942 \n\nOOD \n\ncalibration \n0.423 \n0.412 \n0.425 \n0.425 \n0.413 \n0.447 \n0.432 \n0.417 \n0.459 \ngeneralization \n0.81 \n0.823 \n0.807 \n0.82 \n0.831 \n0.816 \n0.823 \n0.837 \n0.816 \nselect. pred. \n0.85 \n0.851 \n0.838 \n0.86 \n0.862 \n0.855 \n0.865 \n0.869 \n0.863 \n\nSubpopulation \n\ncalibration \n0.409 \n0.404 \n0.412 \n0.415 \n0.405 \n0.428 \n0.426 \n0.403 \n0.46 \ngeneralization \n0.795 \n0.806 \n0.786 \n0.806 \n0.814 \n0.801 \n0.809 \n0.822 \n0.805 \nselect. pred. \n0.82 \n0.824 \n0.803 \n0.831 \n0.835 \n0.826 \n0.837 \n0.842 \n0.838 \n\n\n\nTable 20 :\n20Comparison of method performance between architecture sizes. Black: Best. Dark Grey: Second. Light Grey: Third.\nEarlier work sometimes referred to this setting as out-of-distribution detection. However, in recent work, the term \"out-of-distribution\" is used as a unifying term for \"non I.I.D\" test data which includes any type of shift, e.g., covariate or subpopulation shift. We use the term open set recognition to denote detection for shifts where test inputs belong to semantic classes not encountered in the training data.\nWe focus on label uncertainty in an OOD evaluation-only context, as collecting label distributions can be expensive and therefore the datasets are small relative to single-label training sets. It is not exclusive to the distribution shift setting as training sets may also carry a label distribution per input.\nWe excluded toxicity detection datasets which exhibits severe label imbalance. For these datasets, ECE as a population-average metric is not a suitable measure. For example, a naive model that always predict the majority label will trivially achieve high accuracy and low ECE.\nThere are 2 distribution shift types which we do not cover here: semantic (class) shift and label uncertainty. Semantic (class) shift is not possible to measure predictive performance under without an open vocabulary model such as recent image-text models(Radford et al., 2021). Therefore we restrict studies of class shift to the open set recognition task (Section 5.1.3). Label uncertainty is covered in Section 5.1.4.\nPlex: Towards Reliability using Pretrained Large Model Extensions\nhttps://www.tensorflow.org/datasets/catalog/imagenet2012_fewshot\nA closer inspection at the results shows that the prediction score of [MoE]4 is slightly worse than that of [None]4 because of isolated, slightly worse performance on CIFAR-10.\nAcknowledgementsWe thank Ben Adlam, Dilip Krishnan, Ed Chi, Neil Houlsby, Rif A. Saurous, and Sharat Chikkerur for helpful discussions and feedback on earlier drafts of the paper. We also thank Tom Small and Ajay Nainani for assisting with visualizations.Appendix E. Additional Results for Open Set RecognitionComparing OOD detection methods across all the tasks and model types. While we use MSP as the basic uncertainty score in the main text, we also would like to discuss other alternative uncertainty scores, and how is their performance improved by the Plex models. We study the following 4 more uncertainty scores,\u2022 Mahalanobis distance (Maha)(Lee et al., 2018)which measures the distance between the test input and the fitted training distribution in the embedding space. The training distribution is fitted using a class conditional Gaussian. The uncertainty score is the distance.\u2022 Relative Mahalanobis distance ) is a modified version of Mahalanobis distance which corrects for the background confounding effect using another Gaussian distribution fitted using entire training data ignoring class labels.\u2022 Entropy of the softmax probability. High entropy suggests high uncertainty. So the uncertainty score is entropy.\u2022 Maximum over the logits (MaxLogit) (Hendrycks et al., 2019) which uses the maximum of the un-normalized logits as the confidence score. Then the uncertainty score is then the negative of the MaxLogit.Appendix H. Analysis of Heteroscedastic Last LayerWe add a heteroscedastic output layer to the base deterministic model and the BE base model. We assess whether enabling the network to model input-dependent (heteroscedastic) label noise results in better performance on datasets known to have noisy labels.H.1 Heteroscedastic improves pretraining performance on JFT.InFigure 29andFigure 30the heteroscedastic model, when applied on top of the base deterministic model, provides performance gains on the JFT dataset while performance is neutral on ImageNet-21K.\nEfficient and scalable Bayesian neural nets with rank-1 factors. Michael Dusenberry, Ghassen Jerfel, Yeming Wen, Yian Ma, Jasper Snoek, Katherine Heller, Balaji Lakshminarayanan, Dustin Tran, International Conference on Machine Learning. Michael Dusenberry, Ghassen Jerfel, Yeming Wen, Yian Ma, Jasper Snoek, Katherine Heller, Balaji Lakshminarayanan, and Dustin Tran. Efficient and scalable Bayesian neural nets with rank-1 factors. In International Conference on Machine Learning, 2020a.\n\nAnalyzing the role of model uncertainty for electronic health records. Dustin Michael W Dusenberry, Edward Tran, Jonas Choi, Jeremy Kemp, Ghassen Nixon, Katherine Jerfel, Andrew M Heller, Dai, ACM Conference on Health, Inference, and Learning. Michael W Dusenberry, Dustin Tran, Edward Choi, Jonas Kemp, Jeremy Nixon, Ghassen Jerfel, Katherine Heller, and Andrew M Dai. Analyzing the role of model uncertainty for electronic health records. In ACM Conference on Health, Inference, and Learning, 2020b.\n\nOn the foundations of noise-free selective classification. Ran El-Yaniv, Yair Wiener, Journal of Machine Learning Research. 115Ran El-Yaniv and Yair Wiener. On the foundations of noise-free selective classification. Journal of Machine Learning Research, 11(5), 2010.\n\nActive learning at the imagenet scale. Zeyad Ali Sami Emam, Hong-Min Chu, Ping-Yeh Chiang, Wojciech Czaja, Richard Leapman, Micah Goldblum, Tom Goldstein, arXiv:2111.12880arXiv preprintZeyad Ali Sami Emam, Hong-Min Chu, Ping-Yeh Chiang, Wojciech Czaja, Richard Leapman, Micah Goldblum, and Tom Goldstein. Active learning at the imagenet scale. arXiv preprint arXiv:2111.12880, 2021.\n\nUtku Evci, Vincent Dumoulin, Hugo Larochelle, Michael C Mozer, arXiv:2201.03529Head2toe: Utilizing intermediate representations for better transfer learning. arXiv preprintUtku Evci, Vincent Dumoulin, Hugo Larochelle, and Michael C Mozer. Head2toe: Utilizing intermediate representations for better transfer learning. arXiv preprint arXiv:2201.03529, 2022.\n\nTom Everitt, Gary Lea, Marcus Hutter, arXiv:1805.01109AGI safety literature review. arXiv preprintTom Everitt, Gary Lea, and Marcus Hutter. AGI safety literature review. arXiv preprint arXiv:1805.01109, 2018.\n\nRadial Bayesian Neural Networks: Beyond Discrete Support In Large-Scale Bayesian Deep Learning. Sebastian Farquhar, Michael A Osborne, Yarin Gal, International Conference on Artificial Intelligence and Statistics. Sebastian Farquhar, Michael A. Osborne, and Yarin Gal. Radial Bayesian Neural Networks: Beyond Discrete Support In Large-Scale Bayesian Deep Learning. In International Conference on Artificial Intelligence and Statistics, 2020.\n\nSwitch transformers: Scaling to trillion parameter models with simple and efficient sparsity. William Fedus, Barret Zoph, Noam Shazeer, Journal of Machine Learning Research. 231202022William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120), 2022.\n\nLearning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories. Computer Vision and Pattern Recognition Workshop. Li Fei-Fei, Rob Fergus, Pietro Perona, Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories. Computer Vision and Pattern Recognition Workshop, 2004.\n\nExploring the limits of out-ofdistribution detection. Stanislav Fort, Jie Ren, Balaji Lakshminarayanan, Advances in Neural Information Processing Systems. Stanislav Fort, Jie Ren, and Balaji Lakshminarayanan. Exploring the limits of out-of- distribution detection. In Advances in Neural Information Processing Systems, 2021.\n\nDropout As a Bayesian Approximation: Representing Model Uncertainty in Deep Learning. Yarin Gal, Zoubin Ghahramani, International Conference on Machine Learning. Yarin Gal and Zoubin Ghahramani. Dropout As a Bayesian Approximation: Representing Model Uncertainty in Deep Learning. In International Conference on Machine Learning, 2016.\n\nDeep Bayesian active learning with image data. Yarin Gal, Riashat Islam, Zoubin Ghahramani, International Conference on Machine Learning. Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep Bayesian active learning with image data. In International Conference on Machine Learning, 2017.\n\nShortcut learning in deep neural networks. Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, Felix A Wichmann, Nature Machine Intelligence. 211Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665-673, 2020.\n\nRecent advances in open set recognition: A survey. Chuanxing Geng, Sheng-Jun, Songcan Huang, Chen, IEEE transactions on pattern analysis and machine intelligence. 43Chuanxing Geng, Sheng-jun Huang, and Songcan Chen. Recent advances in open set recognition: A survey. IEEE transactions on pattern analysis and machine intelligence, 43(10):3614-3631, 2020.\n\nMeasuring and improving modelmoderator collaboration using uncertainty estimation. Zi Ian D Kivlichan, Jeremiah Lin, Lucy Liu, Vasserman, arXiv:2107.04212arXiv preprintIan D Kivlichan, Zi Lin, Jeremiah Liu, and Lucy Vasserman. Measuring and improving model- moderator collaboration using uncertainty estimation. arXiv preprint arXiv:2107.04212, 2021.\n\nBig transfer (bit): General visual representation learning. Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, Neil Houlsby, ECCV. Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. Big transfer (bit): General visual representation learning. In ECCV, 2020.\n\n3d object representations for fine-grained categorization. Jonathan Krause, Michael Stark, Jia Deng, Li Fei-Fei, IEEE Workshop on 3D Representation and Recognition. Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In IEEE Workshop on 3D Representation and Recognition, 2013.\n\nImproving model calibration with accuracy versus uncertainty optimization. Ranganath Krishnan, Omesh Tickoo, Advances in Neural Information Processing Systems. 33Ranganath Krishnan and Omesh Tickoo. Improving model calibration with accuracy versus uncertainty optimization. In Advances in Neural Information Processing Systems, volume 33, 2020.\n\nLearning multiple layers of features from tiny images. Alex Krizhevsky, Geoffrey Hinton, arXiv preprintAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. arXiv preprint, 2009.\n\nSimple and scalable predictive uncertainty estimation using deep ensembles. Balaji Lakshminarayanan, Alexander Pritzel, Charles Blundell, Advances in Neural Information Processing Systems. Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems, 2017.\n\nAn evaluation dataset for intent classification and out-of-scope prediction. Stefan Larson, Anish Mahendran, J Joseph, Christopher Peper, Andrew Clarke, Parker Lee, Jonathan K Hill, Kevin Kummerfeld, Leach, A Michael, Lingjia Laurenzano, Tang, arXiv:1909.02027arXiv preprintStefan Larson, Anish Mahendran, Joseph J Peper, Christopher Clarke, Andrew Lee, Parker Hill, Jonathan K Kummerfeld, Kevin Leach, Michael A Laurenzano, Lingjia Tang, et al. An evaluation dataset for intent classification and out-of-scope prediction. arXiv preprint arXiv:1909.02027, 2019.\n\nA simple unified framework for detecting out-of-distribution samples and adversarial attacks. Kimin Lee, Kibok Lee, Honglak Lee, Jinwoo Shin, Advances in Neural Information Processing Systems. Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. In Advances in Neural Information Processing Systems, 2018.\n\nGShard: Scaling giant models with conditional computation and automatic sharding. Dmitry Lepikhin, Hyoukjoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, Zhifeng Chen, International Conference on Learning Representations. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. GShard: Scaling giant models with conditional computation and automatic sharding. In International Conference on Learning Representations, 2021.\n\nOne-shot learning of object categories. Fei-Fei Li, Rob Fergus, Pietro Perona, IEEE transactions on pattern analysis and machine intelligence. 28Fei-Fei Li, Rob Fergus, and Pietro Perona. One-shot learning of object categories. IEEE transactions on pattern analysis and machine intelligence, 28(4):594-611, 2006.\n\nSimple and principled uncertainty estimation with deterministic deep learning via distance awareness. Jeremiah Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax Weiss, Balaji Lakshminarayanan, Advances in Neural Information Processing Systems. Jeremiah Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax Weiss, and Balaji Lakshmi- narayanan. Simple and principled uncertainty estimation with deterministic deep learning via distance awareness. In Advances in Neural Information Processing Systems, 2020.\n\nA simple approach to improve single-model deep uncertainty via distance-awareness. Jeremiah Zhe Liu, Shreyas Padhy, Jie Ren, Zi Lin, Yeming Wen, Ghassen Jerfel, Zack Nado, Jasper Snoek, Dustin Tran, Balaji Lakshminarayanan, arXiv:2205.00403arXiv preprintJeremiah Zhe Liu, Shreyas Padhy, Jie Ren, Zi Lin, Yeming Wen, Ghassen Jerfel, Zack Nado, Jasper Snoek, Dustin Tran, and Balaji Lakshminarayanan. A simple approach to improve single-model deep uncertainty via distance-awareness. arXiv preprint arXiv:2205.00403, 2022.\n\nBenchmarking natural language understanding services for building conversational agents. Xingkun Liu, Arash Eshghi, Pawel Swietojanski, Verena Rieser, Increasing Naturalness and Flexibility in Spoken Dialogue Interaction. SpringerXingkun Liu, Arash Eshghi, Pawel Swietojanski, and Verena Rieser. Benchmarking nat- ural language understanding services for building conversational agents. In Increasing Naturalness and Flexibility in Spoken Dialogue Interaction, pages 165-183. Springer, 2021.\n\nCross-token modeling with conditional computation. Yuxuan Lou, Fuzhao Xue, Zangwei Zheng, Yang You, Yuxuan Lou, Fuzhao Xue, Zangwei Zheng, and Yang You. Cross-token modeling with conditional computation, 2021. URL https://arxiv.org/abs/2109.02008.\n\nUncertainty estimation in autoregressive structured prediction. Andrey Malinin, Mark Gales, International Conference on Learning Representations. Andrey Malinin and Mark Gales. Uncertainty estimation in autoregressive structured prediction. In International Conference on Learning Representations, 2021.\n\nActive policy learning for robot planning and exploration under uncertainty. Ruben Martinez-Cantin, Arnaud Nando De Freitas, Jos\u00e9 A Doucet, Castellanos, Robotics: Science and Systems. 3Ruben Martinez-Cantin, Nando de Freitas, Arnaud Doucet, and Jos\u00e9 A Castellanos. Active policy learning for robot planning and exploration under uncertainty. In Robotics: Science and Systems, volume 3, pages 321-328, 2007.\n\nRight for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. Tom Mccoy, Ellie Pavlick, Tal Linzen, Annual Meeting of the Association for Computational Linguistics. Tom McCoy, Ellie Pavlick, and Tal Linzen. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Annual Meeting of the Association for Computational Linguistics, 2019.\n\nRevisiting the calibration of modern neural networks. Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis, Xiaohua Zhai, Neil Houlsby, Dustin Tran, Mario Lucic, Advances in Neural Information Processing Systems. Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis, Xiaohua Zhai, Neil Houlsby, Dustin Tran, and Mario Lucic. Revisiting the calibration of modern neural networks. In Advances in Neural Information Processing Systems, 2021.\n\nZachary Nado, Neil Band, Mark Collier, Josip Djolonga, W Michael, Sebastian Dusenberry, Qixuan Farquhar, Feng, arXiv:2106.04015Angelos Filos, Marton Havasi, Rodolphe Jenatton, et al. Uncertainty baselines: Benchmarks for uncertainty & robustness in deep learning. arXiv preprintZachary Nado, Neil Band, Mark Collier, Josip Djolonga, Michael W Dusenberry, Sebastian Farquhar, Qixuan Feng, Angelos Filos, Marton Havasi, Rodolphe Jenatton, et al. Un- certainty baselines: Benchmarks for uncertainty & robustness in deep learning. arXiv preprint arXiv:2106.04015, 2021.\n\nObtaining well calibrated probabilities using Bayesian binning. Gregory Mahdi Pakdaman Naeini, Milos Cooper, Hauskrecht, AAAI. Mahdi Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht. Obtaining well calibrated probabilities using Bayesian binning. In AAAI, 2015.\n\nMeasuring calibration in deep learning. Jeremy Nixon, W Michael, Linchuan Dusenberry, Ghassen Zhang, Dustin Jerfel, Tran, CVPR Workshops. Jeremy Nixon, Michael W Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin Tran. Measuring calibration in deep learning. In CVPR Workshops, 2019.\n\nPractical reliability engineering. O&apos; Patrick, Andre Connor, Kleyner, John Wiley & SonsPatrick O'Connor and Andre Kleyner. Practical reliability engineering. John Wiley & Sons, 2012.\n\nCan you trust your model's uncertainty? Evaluating predictive uncertainty under dataset shift. Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, Jasper Snoek, Advances in Neural Information Processing Systems. Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, D. Sculley, Sebastian Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek. Can you trust your model's uncer- tainty? Evaluating predictive uncertainty under dataset shift. In Advances in Neural Information Processing Systems, 2019.\n\nCats and dogs. O M Parkhi, A Vedaldi, A Zisserman, C V Jawahar, Computer Vision and Pattern Recognition. O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. V. Jawahar. Cats and dogs. In Computer Vision and Pattern Recognition, 2012.\n\nDecision theory: principles and approaches. Wiley series in probability and statistics. G Parmigiani, Lurdes Inoue, John Wiley & SonsG. Parmigiani and Lurdes Inoue. Decision theory: principles and approaches. Wiley series in probability and statistics. John Wiley & Sons, 2009.\n\nHuman uncertainty makes classification more robust. Joshua C Peterson, M Ruairidh, Battleday, L Thomas, Olga Griffiths, Russakovsky, International Conference on Computer Vision. Joshua C Peterson, Ruairidh M Battleday, Thomas L Griffiths, and Olga Russakovsky. Human uncertainty makes classification more robust. In International Conference on Computer Vision, 2019.\n\nCombined scaling for zero-shot transfer learning. Hieu Pham, Zihang Dai, Golnaz Ghiasi, Hanxiao Liu, Adams Wei Yu, Minh-Thang Luong, Mingxing Tan, Quoc V Le, arXiv:2111.10050arXiv preprintHieu Pham, Zihang Dai, Golnaz Ghiasi, Hanxiao Liu, Adams Wei Yu, Minh-Thang Luong, Mingxing Tan, and Quoc V Le. Combined scaling for zero-shot transfer learning. arXiv preprint arXiv:2111.10050, 2021.\n\nLearning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever, International Conference on Machine Learning. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 2021.\n\nExploring the limits of transfer learning with a unified text-to-text transformer. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, Journal of Machine Learning Research. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 2020.\n\nDo ImageNet classifiers generalize to ImageNet. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, Vaishaal Shankar, International Conference on Machine Learning. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classifiers generalize to ImageNet? In International Conference on Machine Learning, 2019.\n\nA simple fix to mahalanobis distance for improving near-ood detection. Jie Ren, Stanislav Fort, Jeremiah Liu, Abhijit Guha Roy, Shreyas Padhy, Balaji Lakshminarayanan, arXiv:2106.09022arXiv preprintJie Ren, Stanislav Fort, Jeremiah Liu, Abhijit Guha Roy, Shreyas Padhy, and Balaji Lakshminarayanan. A simple fix to mahalanobis distance for improving near-ood detection. arXiv preprint arXiv:2106.09022, 2021.\n\nActive learning: Theory and applications to automatic speech recognition. Giuseppe Riccardi, Dilek Hakkani-Tur, IEEE Transactions on Speech and Audio Processing. 134Giuseppe Riccardi and Dilek Hakkani-Tur. Active learning: Theory and applications to automatic speech recognition. IEEE Transactions on Speech and Audio Processing, 13(4): 504-511, 2005.\n\nScaling vision with sparse mixture of experts. Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr\u00e9 Susano Pinto, Daniel Keysers, Neil Houlsby, Advances in Neural Information Processing Systems. Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr\u00e9 Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. In Advances in Neural Information Processing Systems, 2021.\n\nMargin-based active learning for structured output spaces. Dan Roth, Kevin Small, European Conference on Machine Learning. SpringerDan Roth and Kevin Small. Margin-based active learning for structured output spaces. In European Conference on Machine Learning, pages 413-424. Springer, 2006.\n\nTractable Function-Space Variational Inference in Bayesian Neural Networks. G J Tim, Zonghao Rudner, Yee Whye Chen, Yarin Teh, Gal, ICML Workshop on Uncertainty & Robustness in Deep Learning. Tim G. J. Rudner, Zonghao Chen, Yee Whye Teh, and Yarin Gal. Tractable Function-Space Variational Inference in Bayesian Neural Networks. In ICML Workshop on Uncertainty & Robustness in Deep Learning, 2021.\n\nContinual Learning via Sequential Function-Space Variational Inference. G J Tim, Freddie Bickford Rudner, Qixuan Smith, Yee Whye Feng, Yarin Teh, Gal, International Conference on Machine Learning. Tim G. J. Rudner, Freddie Bickford Smith, Qixuan Feng, Yee Whye Teh, and Yarin Gal. Continual Learning via Sequential Function-Space Variational Inference. In International Conference on Machine Learning, 2022.\n\nResearch priorities for robust and beneficial artificial intelligence. Stuart Russell, Daniel Dewey, Max Tegmark, 36Ai MagazineStuart Russell, Daniel Dewey, and Max Tegmark. Research priorities for robust and beneficial artificial intelligence. Ai Magazine, 36(4):105-114, 2015.\n\nBreeds: Benchmarks for subpopulation shift. Shibani Santurkar, Dimitris Tsipras, Aleksander Madry, arXiv:2008.04859arXiv preprintShibani Santurkar, Dimitris Tsipras, and Aleksander Madry. Breeds: Benchmarks for subpopulation shift. arXiv preprint arXiv:2008.04859, 2020.\n\nActive hidden markov models for information extraction. Tobias Scheffer, Christian Decomain, Stefan Wrobel, International Symposium on Intelligent Data Analysis. SpringerTobias Scheffer, Christian Decomain, and Stefan Wrobel. Active hidden markov models for information extraction. In International Symposium on Intelligent Data Analysis, pages 309-318. Springer, 2001.\n\nBayesian generative active deep learning. Toan Tran, Thanh-Toan Do, Ian Reid, Gustavo Carneiro, International Conference on Machine Learning. PMLRToan Tran, Thanh-Toan Do, Ian Reid, and Gustavo Carneiro. Bayesian generative active deep learning. In International Conference on Machine Learning, pages 6295-6304. PMLR, 2019.\n\nAn empirical study on robustness to spurious correlations using pre-trained language models. Lifu Tu, Garima Lalwani, Spandana Gella, He He, Transactions of the Association for Computational Linguistics. 8Lifu Tu, Garima Lalwani, Spandana Gella, and He He. An empirical study on robustness to spurious correlations using pre-trained language models. Transactions of the Association for Computational Linguistics, 8:621-633, 2020.\n\nUncertainty estimation using a single deep deterministic neural network. Joost Van Amersfoort, Lewis Smith, Yee Whye Teh, Yarin Gal, International Conference on Machine Learning. Joost Van Amersfoort, Lewis Smith, Yee Whye Teh, and Yarin Gal. Uncertainty estimation using a single deep deterministic neural network. In International Conference on Machine Learning, 2020.\n\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R Bowman, arXiv:1804.07461Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprintAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018a.\n\nActive model learning and diverse action sampling for task and motion planning. Zi Wang, Caelan Reed Garrett, Leslie Pack Kaelbling, Tom\u00e1s Lozano-P\u00e9rez, Zi Wang, Caelan Reed Garrett, Leslie Pack Kaelbling, and Tom\u00e1s Lozano-P\u00e9rez. Active model learning and diverse action sampling for task and motion planning. In 2018\n\nIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS). IEEEIEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 4107-4114. IEEE, 2018b.\n\nCaltech-ucsd birds 200. Peter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, Pietro Perona, preprintPeter Welinder, Steve Branson, Takeshi Mita, Catherine Wah, Florian Schroff, Serge Belongie, and Pietro Perona. Caltech-ucsd birds 200. preprint, 2010.\n\nCombining ensembles and data augmentation can harm your calibration. Yeming Wen, Ghassen Jerfel, Rafael Muller, W Michael, Jasper Dusenberry, Balaji Snoek, Dustin Lakshminarayanan, Tran, arXiv:2010.09875arXiv preprintYeming Wen, Ghassen Jerfel, Rafael Muller, Michael W Dusenberry, Jasper Snoek, Balaji Lakshminarayanan, and Dustin Tran. Combining ensembles and data augmentation can harm your calibration. arXiv preprint arXiv:2010.09875, 2020a.\n\nBatchEnsemble: an Alternative Approach to Efficient Ensemble and Lifelong Learning. Yeming Wen, Dustin Tran, Jimmy Ba, International Conference on Learning Representations. Yeming Wen, Dustin Tran, and Jimmy Ba. BatchEnsemble: an Alternative Approach to Efficient Ensemble and Lifelong Learning. In International Conference on Learning Representations, 2020b.\n\nA broad-coverage challenge corpus for sentence understanding through inference. Adina Williams, Nikita Nangia, Samuel R Bowman, arXiv:1704.05426arXiv preprintAdina Williams, Nikita Nangia, and Samuel R Bowman. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426, 2017.\n\nEx machina: Personal attacks seen at scale. Ellery Wulczyn, Nithum Thain, Lucas Dixon, WWW. Ellery Wulczyn, Nithum Thain, and Lucas Dixon. Ex machina: Personal attacks seen at scale. In WWW, 2017.\n\nBag-of-visual-words and spatial extensions for land-use classification. Yi Yang, Shawn Newsam, International Conference on Advances in Geographic Information Systems. Yi Yang and Shawn Newsam. Bag-of-visual-words and spatial extensions for land-use classification. In International Conference on Advances in Geographic Information Systems, 2010.\n\nUsing self-supervised pretext tasks for active learning. John Seon Keun Yi, Minseok Seo, Jongchan Park, Dong-Geol Choi, John Seon Keun Yi, Minseok Seo, Jongchan Park, and Dong-Geol Choi. Using self-supervised pretext tasks for active learning, 2022. URL https://arxiv.org/abs/2201.07459.\n\nWhat do we mean by generalization in federated learning?. Honglin Yuan, Warren Morningstar, Lin Ning, Karan Singhal, International Conference on Learning Representations. Honglin Yuan, Warren Morningstar, Lin Ning, and Karan Singhal. What do we mean by gen- eralization in federated learning? International Conference on Learning Representations, 2022.\n", "annotations": {"author": "[{\"end\":89,\"start\":69},{\"end\":111,\"start\":90},{\"end\":141,\"start\":112},{\"end\":158,\"start\":142},{\"end\":180,\"start\":159},{\"end\":197,\"start\":181},{\"end\":209,\"start\":198},{\"end\":226,\"start\":210},{\"end\":248,\"start\":227},{\"end\":266,\"start\":249},{\"end\":308,\"start\":267},{\"end\":347,\"start\":309},{\"end\":370,\"start\":348},{\"end\":392,\"start\":371},{\"end\":437,\"start\":393},{\"end\":476,\"start\":438},{\"end\":503,\"start\":477},{\"end\":525,\"start\":504},{\"end\":547,\"start\":526},{\"end\":571,\"start\":548},{\"end\":593,\"start\":572},{\"end\":612,\"start\":594},{\"end\":646,\"start\":613},{\"end\":673,\"start\":647},{\"end\":695,\"start\":674},{\"end\":728,\"start\":696}]", "publisher": null, "author_last_name": "[{\"end\":80,\"start\":76},{\"end\":102,\"start\":99},{\"end\":132,\"start\":122},{\"end\":149,\"start\":145},{\"end\":171,\"start\":164},{\"end\":188,\"start\":185},{\"end\":208,\"start\":205},{\"end\":217,\"start\":213},{\"end\":239,\"start\":233},{\"end\":257,\"start\":255},{\"end\":276,\"start\":272},{\"end\":323,\"start\":317},{\"end\":361,\"start\":354},{\"end\":383,\"start\":379},{\"end\":413,\"start\":399},{\"end\":452,\"start\":446},{\"end\":494,\"start\":486},{\"end\":516,\"start\":511},{\"end\":538,\"start\":534},{\"end\":562,\"start\":554},{\"end\":584,\"start\":578},{\"end\":603,\"start\":596},{\"end\":622,\"start\":619},{\"end\":664,\"start\":654},{\"end\":686,\"start\":681},{\"end\":719,\"start\":703}]", "author_first_name": "[{\"end\":75,\"start\":69},{\"end\":98,\"start\":90},{\"end\":119,\"start\":112},{\"end\":121,\"start\":120},{\"end\":144,\"start\":142},{\"end\":163,\"start\":159},{\"end\":184,\"start\":181},{\"end\":204,\"start\":198},{\"end\":212,\"start\":210},{\"end\":232,\"start\":227},{\"end\":254,\"start\":249},{\"end\":271,\"start\":267},{\"end\":312,\"start\":309},{\"end\":316,\"start\":313},{\"end\":353,\"start\":348},{\"end\":378,\"start\":371},{\"end\":398,\"start\":393},{\"end\":445,\"start\":438},{\"end\":485,\"start\":477},{\"end\":510,\"start\":504},{\"end\":533,\"start\":526},{\"end\":553,\"start\":548},{\"end\":577,\"start\":572},{\"end\":595,\"start\":594},{\"end\":618,\"start\":613},{\"end\":653,\"start\":647},{\"end\":680,\"start\":674},{\"end\":702,\"start\":696}]", "author_affiliation": "[{\"end\":88,\"start\":82},{\"end\":110,\"start\":104},{\"end\":140,\"start\":134},{\"end\":157,\"start\":151},{\"end\":179,\"start\":173},{\"end\":196,\"start\":190},{\"end\":225,\"start\":219},{\"end\":247,\"start\":241},{\"end\":265,\"start\":259},{\"end\":284,\"start\":278},{\"end\":307,\"start\":286},{\"end\":346,\"start\":325},{\"end\":369,\"start\":363},{\"end\":391,\"start\":385},{\"end\":436,\"start\":415},{\"end\":475,\"start\":454},{\"end\":502,\"start\":496},{\"end\":524,\"start\":518},{\"end\":546,\"start\":540},{\"end\":570,\"start\":564},{\"end\":592,\"start\":586},{\"end\":611,\"start\":605},{\"end\":645,\"start\":624},{\"end\":672,\"start\":666},{\"end\":694,\"start\":688},{\"end\":727,\"start\":721}]", "title": "[{\"end\":66,\"start\":1},{\"end\":794,\"start\":729}]", "venue": null, "abstract": "[{\"end\":2401,\"start\":850}]", "bib_ref": "[{\"end\":2652,\"start\":2626},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":2706,\"start\":2685},{\"end\":2725,\"start\":2706},{\"end\":3984,\"start\":3957},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4011,\"start\":3984},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":4164,\"start\":4142},{\"end\":4194,\"start\":4173},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4215,\"start\":4194},{\"end\":4239,\"start\":4215},{\"end\":4266,\"start\":4253},{\"end\":4302,\"start\":4280},{\"end\":4343,\"start\":4324},{\"end\":5528,\"start\":5510},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5843,\"start\":5822},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5862,\"start\":5843},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6043,\"start\":6015},{\"end\":6527,\"start\":6499},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6545,\"start\":6527},{\"end\":6790,\"start\":6762},{\"end\":6923,\"start\":6915},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":7448,\"start\":7431},{\"end\":7533,\"start\":7519},{\"end\":7662,\"start\":7649},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":8130,\"start\":8110},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":9286,\"start\":9266},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":9573,\"start\":9555},{\"end\":9813,\"start\":9812},{\"end\":10623,\"start\":10603},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":10647,\"start\":10623},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":10668,\"start\":10647},{\"end\":10691,\"start\":10668},{\"end\":10884,\"start\":10860},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11062,\"start\":11039},{\"end\":12611,\"start\":12606},{\"end\":12636,\"start\":12611},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":12676,\"start\":12656},{\"end\":13985,\"start\":13972},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":14200,\"start\":14179},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":14671,\"start\":14644},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":14938,\"start\":14914},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":15095,\"start\":15076},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":15293,\"start\":15274},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":16055,\"start\":16029},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":17373,\"start\":17349},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":17391,\"start\":17373},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":18251,\"start\":18234},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":18810,\"start\":18792},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":19375,\"start\":19351},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":19399,\"start\":19376},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":19434,\"start\":19413},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19816,\"start\":19796},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":21336,\"start\":21313},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":21425,\"start\":21403},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":21851,\"start\":21831},{\"end\":21881,\"start\":21851},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":23453,\"start\":23432},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":24835,\"start\":24816},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":24853,\"start\":24835},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":24989,\"start\":24968},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":25135,\"start\":25114},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":25153,\"start\":25135},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":25230,\"start\":25211},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":25483,\"start\":25460},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":26151,\"start\":26130},{\"end\":36516,\"start\":36490},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":36546,\"start\":36516},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":36860,\"start\":36839},{\"end\":36884,\"start\":36860},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":36902,\"start\":36884},{\"end\":38576,\"start\":38563},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":41085,\"start\":41058},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":41108,\"start\":41085},{\"end\":51991,\"start\":51963},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":54478,\"start\":54453},{\"end\":55850,\"start\":55825},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":58233,\"start\":58207},{\"end\":58255,\"start\":58233},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":58274,\"start\":58255},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":58295,\"start\":58274},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":58317,\"start\":58295},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":58348,\"start\":58330},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":59644,\"start\":59618},{\"end\":59666,\"start\":59644},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":59685,\"start\":59666},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":59706,\"start\":59685},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":59728,\"start\":59706},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":59759,\"start\":59741},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":61766,\"start\":61743},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":61789,\"start\":61766},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":63888,\"start\":63864},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":63906,\"start\":63888},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":64524,\"start\":64506},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":65077,\"start\":65059},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":66780,\"start\":66758},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":67367,\"start\":67350},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":67781,\"start\":67761},{\"end\":67801,\"start\":67781},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":67998,\"start\":67978},{\"end\":68485,\"start\":68464},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":69936,\"start\":69915},{\"end\":70256,\"start\":70237},{\"end\":70269,\"start\":70256},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":70519,\"start\":70501},{\"end\":70541,\"start\":70519},{\"end\":70583,\"start\":70560},{\"end\":70609,\"start\":70583},{\"end\":70644,\"start\":70618},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":70675,\"start\":70644},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":70719,\"start\":70689},{\"end\":70738,\"start\":70719},{\"end\":70907,\"start\":70891},{\"end\":70925,\"start\":70907},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":71039,\"start\":71020},{\"end\":71059,\"start\":71039},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":71916,\"start\":71893},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":71937,\"start\":71916},{\"end\":72034,\"start\":72011},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":72149,\"start\":72126},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":75080,\"start\":75062},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":79421,\"start\":79403},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":88833,\"start\":88814},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":89229,\"start\":89210},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":89862,\"start\":89843},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":90254,\"start\":90231},{\"end\":90429,\"start\":90408},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":91412,\"start\":91393},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":91471,\"start\":91440},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":91980,\"start\":91957},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":92260,\"start\":92231},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":93078,\"start\":93059},{\"end\":93751,\"start\":93728},{\"end\":93861,\"start\":93855},{\"end\":93866,\"start\":93861},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":94098,\"start\":94076},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":94666,\"start\":94641},{\"end\":97275,\"start\":97255},{\"end\":97297,\"start\":97275},{\"end\":97459,\"start\":97437},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":97481,\"start\":97459},{\"end\":97500,\"start\":97481},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":97561,\"start\":97538},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":97578,\"start\":97561},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":98132,\"start\":98110},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":98756,\"start\":98733},{\"end\":99958,\"start\":99935},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":100930,\"start\":100908},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":110368,\"start\":110348},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":116483,\"start\":116464},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":128771,\"start\":128749}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":106713,\"start\":106003},{\"attributes\":{\"id\":\"fig_1\"},\"end\":106870,\"start\":106714},{\"attributes\":{\"id\":\"fig_3\"},\"end\":107139,\"start\":106871},{\"attributes\":{\"id\":\"fig_5\"},\"end\":107230,\"start\":107140},{\"attributes\":{\"id\":\"fig_7\"},\"end\":107399,\"start\":107231},{\"attributes\":{\"id\":\"fig_8\"},\"end\":107612,\"start\":107400},{\"attributes\":{\"id\":\"fig_9\"},\"end\":107829,\"start\":107613},{\"attributes\":{\"id\":\"fig_10\"},\"end\":108176,\"start\":107830},{\"attributes\":{\"id\":\"fig_11\"},\"end\":108222,\"start\":108177},{\"attributes\":{\"id\":\"fig_12\"},\"end\":108476,\"start\":108223},{\"attributes\":{\"id\":\"fig_13\"},\"end\":108689,\"start\":108477},{\"attributes\":{\"id\":\"fig_14\"},\"end\":109238,\"start\":108690},{\"attributes\":{\"id\":\"fig_15\"},\"end\":110384,\"start\":109239},{\"attributes\":{\"id\":\"fig_16\"},\"end\":110496,\"start\":110385},{\"attributes\":{\"id\":\"fig_18\"},\"end\":110665,\"start\":110497},{\"attributes\":{\"id\":\"fig_20\"},\"end\":110838,\"start\":110666},{\"attributes\":{\"id\":\"fig_21\"},\"end\":110936,\"start\":110839},{\"attributes\":{\"id\":\"fig_22\"},\"end\":111523,\"start\":110937},{\"attributes\":{\"id\":\"fig_23\"},\"end\":112114,\"start\":111524},{\"attributes\":{\"id\":\"fig_24\"},\"end\":112459,\"start\":112115},{\"attributes\":{\"id\":\"fig_25\"},\"end\":112762,\"start\":112460},{\"attributes\":{\"id\":\"fig_26\"},\"end\":112931,\"start\":112763},{\"attributes\":{\"id\":\"fig_27\"},\"end\":113410,\"start\":112932},{\"attributes\":{\"id\":\"fig_28\"},\"end\":114323,\"start\":113411},{\"attributes\":{\"id\":\"fig_29\"},\"end\":114347,\"start\":114324},{\"attributes\":{\"id\":\"fig_30\"},\"end\":114610,\"start\":114348},{\"attributes\":{\"id\":\"fig_31\"},\"end\":114840,\"start\":114611},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":116285,\"start\":114841},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":117877,\"start\":116286},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":118135,\"start\":117878},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":118203,\"start\":118136},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":118331,\"start\":118204},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":118432,\"start\":118332},{\"attributes\":{\"id\":\"tab_14\",\"type\":\"table\"},\"end\":118648,\"start\":118433},{\"attributes\":{\"id\":\"tab_16\",\"type\":\"table\"},\"end\":118856,\"start\":118649},{\"attributes\":{\"id\":\"tab_18\",\"type\":\"table\"},\"end\":118938,\"start\":118857},{\"attributes\":{\"id\":\"tab_20\",\"type\":\"table\"},\"end\":119028,\"start\":118939},{\"attributes\":{\"id\":\"tab_21\",\"type\":\"table\"},\"end\":119458,\"start\":119029},{\"attributes\":{\"id\":\"tab_23\",\"type\":\"table\"},\"end\":119715,\"start\":119459},{\"attributes\":{\"id\":\"tab_24\",\"type\":\"table\"},\"end\":120089,\"start\":119716},{\"attributes\":{\"id\":\"tab_27\",\"type\":\"table\"},\"end\":120945,\"start\":120090},{\"attributes\":{\"id\":\"tab_30\",\"type\":\"table\"},\"end\":121017,\"start\":120946},{\"attributes\":{\"id\":\"tab_32\",\"type\":\"table\"},\"end\":121501,\"start\":121018},{\"attributes\":{\"id\":\"tab_33\",\"type\":\"table\"},\"end\":122493,\"start\":121502},{\"attributes\":{\"id\":\"tab_34\",\"type\":\"table\"},\"end\":122640,\"start\":122494},{\"attributes\":{\"id\":\"tab_35\",\"type\":\"table\"},\"end\":123587,\"start\":122641},{\"attributes\":{\"id\":\"tab_36\",\"type\":\"table\"},\"end\":123889,\"start\":123588},{\"attributes\":{\"id\":\"tab_37\",\"type\":\"table\"},\"end\":124102,\"start\":123890},{\"attributes\":{\"id\":\"tab_38\",\"type\":\"table\"},\"end\":124993,\"start\":124103},{\"attributes\":{\"id\":\"tab_40\",\"type\":\"table\"},\"end\":127363,\"start\":124994},{\"attributes\":{\"id\":\"tab_41\",\"type\":\"table\"},\"end\":127489,\"start\":127364}]", "paragraph": "[{\"end\":3475,\"start\":2454},{\"end\":3645,\"start\":3477},{\"end\":4395,\"start\":3647},{\"end\":5390,\"start\":4426},{\"end\":6417,\"start\":5392},{\"end\":7057,\"start\":6419},{\"end\":7778,\"start\":7059},{\"end\":9367,\"start\":7780},{\"end\":10255,\"start\":9397},{\"end\":10412,\"start\":10257},{\"end\":11473,\"start\":10414},{\"end\":12370,\"start\":11491},{\"end\":13189,\"start\":12372},{\"end\":13648,\"start\":13216},{\"end\":13869,\"start\":13687},{\"end\":14465,\"start\":13885},{\"end\":15161,\"start\":14467},{\"end\":15825,\"start\":15163},{\"end\":16344,\"start\":15827},{\"end\":16700,\"start\":16370},{\"end\":17056,\"start\":16702},{\"end\":17591,\"start\":17058},{\"end\":18120,\"start\":17606},{\"end\":18373,\"start\":18122},{\"end\":18811,\"start\":18375},{\"end\":18993,\"start\":18824},{\"end\":19238,\"start\":19004},{\"end\":19435,\"start\":19240},{\"end\":20107,\"start\":19437},{\"end\":21080,\"start\":20116},{\"end\":21337,\"start\":21082},{\"end\":21540,\"start\":21339},{\"end\":22105,\"start\":21542},{\"end\":23038,\"start\":22107},{\"end\":23280,\"start\":23083},{\"end\":23709,\"start\":23282},{\"end\":24233,\"start\":23711},{\"end\":24642,\"start\":24235},{\"end\":24990,\"start\":24644},{\"end\":25535,\"start\":24992},{\"end\":26193,\"start\":25537},{\"end\":26772,\"start\":26195},{\"end\":28262,\"start\":26774},{\"end\":29397,\"start\":28304},{\"end\":30015,\"start\":29399},{\"end\":31777,\"start\":30017},{\"end\":32262,\"start\":31820},{\"end\":32449,\"start\":32264},{\"end\":34113,\"start\":32451},{\"end\":34523,\"start\":34157},{\"end\":35396,\"start\":34525},{\"end\":36440,\"start\":35398},{\"end\":37724,\"start\":36461},{\"end\":37884,\"start\":37753},{\"end\":38094,\"start\":37900},{\"end\":38285,\"start\":38110},{\"end\":39785,\"start\":38287},{\"end\":40286,\"start\":39862},{\"end\":41498,\"start\":40288},{\"end\":41737,\"start\":41532},{\"end\":42628,\"start\":41739},{\"end\":43293,\"start\":42707},{\"end\":44008,\"start\":43295},{\"end\":44509,\"start\":44010},{\"end\":45978,\"start\":44511},{\"end\":46907,\"start\":45980},{\"end\":48026,\"start\":46909},{\"end\":49932,\"start\":48028},{\"end\":50705,\"start\":49986},{\"end\":50892,\"start\":50745},{\"end\":51001,\"start\":50894},{\"end\":51278,\"start\":51032},{\"end\":51628,\"start\":51280},{\"end\":52562,\"start\":51630},{\"end\":53127,\"start\":52564},{\"end\":53761,\"start\":53129},{\"end\":53958,\"start\":53763},{\"end\":54217,\"start\":53960},{\"end\":54479,\"start\":54219},{\"end\":54926,\"start\":54538},{\"end\":54940,\"start\":54928},{\"end\":55161,\"start\":54962},{\"end\":56383,\"start\":55163},{\"end\":56518,\"start\":56385},{\"end\":56977,\"start\":56621},{\"end\":57215,\"start\":56989},{\"end\":57588,\"start\":57241},{\"end\":57904,\"start\":57623},{\"end\":58042,\"start\":57906},{\"end\":58945,\"start\":58044},{\"end\":59264,\"start\":58947},{\"end\":60606,\"start\":59266},{\"end\":61325,\"start\":60608},{\"end\":63310,\"start\":61362},{\"end\":63519,\"start\":63334},{\"end\":64226,\"start\":63521},{\"end\":64991,\"start\":64228},{\"end\":66492,\"start\":64993},{\"end\":67554,\"start\":66494},{\"end\":69573,\"start\":67556},{\"end\":69937,\"start\":69588},{\"end\":70202,\"start\":69957},{\"end\":71191,\"start\":70204},{\"end\":71791,\"start\":71193},{\"end\":72645,\"start\":71793},{\"end\":72795,\"start\":72647},{\"end\":74083,\"start\":72797},{\"end\":74575,\"start\":74085},{\"end\":75267,\"start\":74577},{\"end\":75350,\"start\":75269},{\"end\":75610,\"start\":75372},{\"end\":76544,\"start\":75612},{\"end\":78147,\"start\":76570},{\"end\":78359,\"start\":78172},{\"end\":78868,\"start\":78361},{\"end\":79048,\"start\":78870},{\"end\":79907,\"start\":79050},{\"end\":80401,\"start\":80014},{\"end\":80913,\"start\":80448},{\"end\":81606,\"start\":81006},{\"end\":81960,\"start\":81608},{\"end\":82792,\"start\":81975},{\"end\":83108,\"start\":82829},{\"end\":83363,\"start\":83110},{\"end\":83570,\"start\":83365},{\"end\":83787,\"start\":83572},{\"end\":84041,\"start\":83789},{\"end\":84125,\"start\":84043},{\"end\":84313,\"start\":84127},{\"end\":84449,\"start\":84315},{\"end\":84778,\"start\":84451},{\"end\":84917,\"start\":84780},{\"end\":85232,\"start\":84919},{\"end\":85352,\"start\":85234},{\"end\":85785,\"start\":85354},{\"end\":86056,\"start\":85787},{\"end\":86213,\"start\":86058},{\"end\":86299,\"start\":86215},{\"end\":86384,\"start\":86301},{\"end\":86529,\"start\":86386},{\"end\":86876,\"start\":86531},{\"end\":87448,\"start\":86878},{\"end\":87539,\"start\":87450},{\"end\":87691,\"start\":87582},{\"end\":88456,\"start\":87693},{\"end\":88556,\"start\":88511},{\"end\":88595,\"start\":88565},{\"end\":88673,\"start\":88597},{\"end\":88694,\"start\":88675},{\"end\":88752,\"start\":88696},{\"end\":89052,\"start\":88754},{\"end\":89080,\"start\":89054},{\"end\":89110,\"start\":89082},{\"end\":89140,\"start\":89112},{\"end\":89166,\"start\":89142},{\"end\":90152,\"start\":89168},{\"end\":90207,\"start\":90154},{\"end\":90379,\"start\":90209},{\"end\":90596,\"start\":90381},{\"end\":90732,\"start\":90598},{\"end\":90952,\"start\":90734},{\"end\":91187,\"start\":90954},{\"end\":91315,\"start\":91189},{\"end\":91722,\"start\":91359},{\"end\":91981,\"start\":91748},{\"end\":92740,\"start\":91983},{\"end\":93567,\"start\":92852},{\"end\":94246,\"start\":93569},{\"end\":94948,\"start\":94248},{\"end\":95736,\"start\":94950},{\"end\":95967,\"start\":95738},{\"end\":96851,\"start\":96027},{\"end\":96945,\"start\":96853},{\"end\":97157,\"start\":97002},{\"end\":97888,\"start\":97176},{\"end\":98084,\"start\":97890},{\"end\":98847,\"start\":98086},{\"end\":99440,\"start\":98849},{\"end\":100179,\"start\":99442},{\"end\":100445,\"start\":100195},{\"end\":100891,\"start\":100447},{\"end\":101041,\"start\":100893},{\"end\":101655,\"start\":101097},{\"end\":102413,\"start\":101657},{\"end\":103615,\"start\":102415},{\"end\":105195,\"start\":103665},{\"end\":106002,\"start\":105197}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":39861,\"start\":39786},{\"attributes\":{\"id\":\"formula_1\"},\"end\":49985,\"start\":49933},{\"attributes\":{\"id\":\"formula_2\"},\"end\":54537,\"start\":54480},{\"attributes\":{\"id\":\"formula_3\"},\"end\":56620,\"start\":56519},{\"attributes\":{\"id\":\"formula_5\"},\"end\":80013,\"start\":79908},{\"attributes\":{\"id\":\"formula_6\"},\"end\":80447,\"start\":80402},{\"attributes\":{\"id\":\"formula_7\"},\"end\":81005,\"start\":80914},{\"attributes\":{\"id\":\"formula_8\"},\"end\":91747,\"start\":91723},{\"attributes\":{\"id\":\"formula_9\"},\"end\":92851,\"start\":92741}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":19564,\"start\":19557},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":20722,\"start\":20480},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":22667,\"start\":22660},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":28452,\"start\":28445},{\"end\":33723,\"start\":33716},{\"end\":34940,\"start\":34933},{\"attributes\":{\"ref_id\":\"tab_14\"},\"end\":41117,\"start\":41110},{\"end\":41352,\"start\":41345},{\"end\":43314,\"start\":43307},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":44684,\"start\":44668},{\"end\":49156,\"start\":49149},{\"end\":49861,\"start\":49854},{\"end\":50944,\"start\":50937},{\"end\":52802,\"start\":52793},{\"attributes\":{\"ref_id\":\"tab_18\"},\"end\":52864,\"start\":52857},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":54939,\"start\":54931},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":58079,\"start\":58071},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":59479,\"start\":59471},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":62784,\"start\":62767},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":81103,\"start\":81095},{\"attributes\":{\"ref_id\":\"tab_18\"},\"end\":81453,\"start\":81445},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":87690,\"start\":87682},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":88555,\"start\":88547},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":94836,\"start\":94828},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":100229,\"start\":100221},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":103985,\"start\":103977},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":105647,\"start\":105639}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":2452,\"start\":2403},{\"attributes\":{\"n\":\"1.1\"},\"end\":4424,\"start\":4398},{\"attributes\":{\"n\":\"1.2\"},\"end\":9395,\"start\":9370},{\"attributes\":{\"n\":\"1.3\"},\"end\":11489,\"start\":11476},{\"attributes\":{\"n\":\"2.\"},\"end\":13214,\"start\":13192},{\"attributes\":{\"n\":\"2.1\"},\"end\":13685,\"start\":13651},{\"attributes\":{\"n\":\"2.1.1\"},\"end\":13883,\"start\":13872},{\"attributes\":{\"n\":\"2.1.2\"},\"end\":16368,\"start\":16347},{\"attributes\":{\"n\":\"2.1.3\"},\"end\":17604,\"start\":17594},{\"attributes\":{\"n\":\"2.2\"},\"end\":18822,\"start\":18814},{\"attributes\":{\"n\":\"2.2.1\"},\"end\":19002,\"start\":18996},{\"attributes\":{\"n\":\"2.2.2\"},\"end\":20114,\"start\":20110},{\"attributes\":{\"n\":\"3.\"},\"end\":23081,\"start\":23041},{\"attributes\":{\"n\":\"4.\"},\"end\":28302,\"start\":28265},{\"attributes\":{\"n\":\"4.1\"},\"end\":31818,\"start\":31780},{\"attributes\":{\"n\":\"4.2\"},\"end\":34155,\"start\":34116},{\"attributes\":{\"n\":\"4.3\"},\"end\":36459,\"start\":36443},{\"attributes\":{\"n\":\"5.\"},\"end\":37751,\"start\":37727},{\"attributes\":{\"n\":\"5.1\"},\"end\":37898,\"start\":37887},{\"attributes\":{\"n\":\"5.1.1\"},\"end\":38108,\"start\":38097},{\"end\":41507,\"start\":41501},{\"attributes\":{\"n\":\"5.1.2\"},\"end\":41530,\"start\":41510},{\"end\":42705,\"start\":42631},{\"end\":50743,\"start\":50708},{\"attributes\":{\"n\":\"5.1.3\"},\"end\":51030,\"start\":51004},{\"attributes\":{\"n\":\"5.1.4\"},\"end\":54960,\"start\":54943},{\"end\":56987,\"start\":56980},{\"attributes\":{\"n\":\"5.2\"},\"end\":57239,\"start\":57218},{\"attributes\":{\"n\":\"5.2.1\"},\"end\":57621,\"start\":57591},{\"end\":61360,\"start\":61328},{\"attributes\":{\"n\":\"5.2.3\"},\"end\":63332,\"start\":63313},{\"attributes\":{\"n\":\"5.3\"},\"end\":69586,\"start\":69576},{\"attributes\":{\"n\":\"5.3.1\"},\"end\":69955,\"start\":69940},{\"attributes\":{\"n\":\"5.3.2\"},\"end\":75370,\"start\":75353},{\"end\":76568,\"start\":76547},{\"attributes\":{\"n\":\"5.3.3\"},\"end\":78170,\"start\":78150},{\"attributes\":{\"n\":\"6.\"},\"end\":81973,\"start\":81963},{\"end\":82827,\"start\":82795},{\"end\":87580,\"start\":87542},{\"end\":88509,\"start\":88459},{\"end\":88563,\"start\":88559},{\"end\":91357,\"start\":91318},{\"end\":96025,\"start\":95970},{\"end\":97000,\"start\":96948},{\"end\":97174,\"start\":97160},{\"end\":100193,\"start\":100182},{\"end\":101095,\"start\":101044},{\"end\":103663,\"start\":103618},{\"end\":106014,\"start\":106004},{\"end\":106882,\"start\":106872},{\"end\":107151,\"start\":107141},{\"end\":107243,\"start\":107232},{\"end\":107412,\"start\":107401},{\"end\":107625,\"start\":107614},{\"end\":107840,\"start\":107831},{\"end\":108189,\"start\":108178},{\"end\":108235,\"start\":108224},{\"end\":108702,\"start\":108691},{\"end\":109251,\"start\":109240},{\"end\":110397,\"start\":110386},{\"end\":110509,\"start\":110498},{\"end\":110678,\"start\":110667},{\"end\":110851,\"start\":110840},{\"end\":110960,\"start\":110938},{\"end\":111536,\"start\":111525},{\"end\":112462,\"start\":112461},{\"end\":112775,\"start\":112764},{\"end\":112934,\"start\":112933},{\"end\":113456,\"start\":113412},{\"end\":114360,\"start\":114349},{\"end\":114623,\"start\":114612},{\"end\":117888,\"start\":117879},{\"end\":118146,\"start\":118137},{\"end\":118342,\"start\":118333},{\"end\":118443,\"start\":118434},{\"end\":118867,\"start\":118858},{\"end\":118950,\"start\":118940},{\"end\":119470,\"start\":119460},{\"end\":119725,\"start\":119717},{\"end\":120101,\"start\":120091},{\"end\":120957,\"start\":120947},{\"end\":121029,\"start\":121019},{\"end\":122505,\"start\":122495},{\"end\":122652,\"start\":122642},{\"end\":123901,\"start\":123891},{\"end\":125005,\"start\":124995},{\"end\":127375,\"start\":127365}]", "table": "[{\"end\":116285,\"start\":114883},{\"end\":117877,\"start\":116946},{\"end\":118135,\"start\":117943},{\"end\":118331,\"start\":118261},{\"end\":118432,\"start\":118426},{\"end\":118648,\"start\":118445},{\"end\":118856,\"start\":118690},{\"end\":119458,\"start\":119250},{\"end\":120089,\"start\":120048},{\"end\":120945,\"start\":120181},{\"end\":121501,\"start\":121096},{\"end\":122493,\"start\":121506},{\"end\":123587,\"start\":123006},{\"end\":123889,\"start\":123647},{\"end\":124993,\"start\":124624},{\"end\":127363,\"start\":125120}]", "figure_caption": "[{\"end\":106713,\"start\":106016},{\"end\":106870,\"start\":106716},{\"end\":107139,\"start\":106884},{\"end\":107230,\"start\":107153},{\"end\":107399,\"start\":107246},{\"end\":107612,\"start\":107415},{\"end\":107829,\"start\":107628},{\"end\":108176,\"start\":107843},{\"end\":108222,\"start\":108192},{\"end\":108476,\"start\":108238},{\"end\":108689,\"start\":108479},{\"end\":109238,\"start\":108705},{\"end\":110384,\"start\":109254},{\"end\":110496,\"start\":110400},{\"end\":110665,\"start\":110512},{\"end\":110838,\"start\":110681},{\"end\":110936,\"start\":110854},{\"end\":111523,\"start\":110965},{\"end\":112114,\"start\":111539},{\"end\":112459,\"start\":112117},{\"end\":112762,\"start\":112463},{\"end\":112931,\"start\":112778},{\"end\":113410,\"start\":112935},{\"end\":114323,\"start\":113465},{\"end\":114347,\"start\":114326},{\"end\":114610,\"start\":114363},{\"end\":114840,\"start\":114626},{\"end\":114883,\"start\":114843},{\"end\":116946,\"start\":116288},{\"end\":117943,\"start\":117890},{\"end\":118203,\"start\":118148},{\"end\":118261,\"start\":118206},{\"end\":118426,\"start\":118344},{\"end\":118690,\"start\":118651},{\"end\":118938,\"start\":118869},{\"end\":119028,\"start\":118953},{\"end\":119250,\"start\":119031},{\"end\":119715,\"start\":119473},{\"end\":120048,\"start\":119728},{\"end\":120181,\"start\":120104},{\"end\":121017,\"start\":120960},{\"end\":121096,\"start\":121032},{\"end\":121506,\"start\":121504},{\"end\":122640,\"start\":122508},{\"end\":123006,\"start\":122655},{\"end\":123647,\"start\":123590},{\"end\":124102,\"start\":123904},{\"end\":124624,\"start\":124105},{\"end\":125120,\"start\":125008},{\"end\":127489,\"start\":127378}]", "figure_ref": "[{\"end\":3178,\"start\":3170},{\"end\":5388,\"start\":5379},{\"end\":9822,\"start\":9814},{\"end\":12125,\"start\":12117},{\"end\":12818,\"start\":12810},{\"end\":13328,\"start\":13320},{\"end\":14291,\"start\":14283},{\"end\":15545,\"start\":15537},{\"end\":27194,\"start\":27186},{\"end\":27636,\"start\":27628},{\"end\":28439,\"start\":28431},{\"end\":28972,\"start\":28964},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":29598,\"start\":29590},{\"end\":30763,\"start\":30755},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":30894,\"start\":30886},{\"end\":32030,\"start\":32022},{\"end\":32462,\"start\":32454},{\"end\":35548,\"start\":35539},{\"end\":35782,\"start\":35773},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":37183,\"start\":37174},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":37722,\"start\":37713},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":39124,\"start\":39115},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":39939,\"start\":39930},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":44033,\"start\":44024},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":46328,\"start\":46318},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":46884,\"start\":46876},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":49273,\"start\":49264},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":57012,\"start\":57003},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":58921,\"start\":58912},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":61117,\"start\":61108},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":62566,\"start\":62557},{\"end\":64928,\"start\":64920},{\"end\":65006,\"start\":64998},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":65410,\"start\":65401},{\"end\":66277,\"start\":66269},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":68496,\"start\":68487},{\"attributes\":{\"ref_id\":\"fig_30\"},\"end\":68662,\"start\":68645},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":73131,\"start\":73122},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":73522,\"start\":73513},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":76167,\"start\":76158},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":77640,\"start\":77631},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":78909,\"start\":78900},{\"end\":87618,\"start\":87610},{\"end\":87731,\"start\":87723},{\"end\":88594,\"start\":88586},{\"end\":101109,\"start\":101100},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":101120,\"start\":101111},{\"end\":101134,\"start\":101125},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":102720,\"start\":102712},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":103129,\"start\":103121},{\"attributes\":{\"ref_id\":\"fig_30\"},\"end\":104025,\"start\":104016},{\"attributes\":{\"ref_id\":\"fig_31\"},\"end\":105687,\"start\":105678}]", "bib_author_first_name": "[{\"end\":131287,\"start\":131280},{\"end\":131307,\"start\":131300},{\"end\":131322,\"start\":131316},{\"end\":131332,\"start\":131328},{\"end\":131343,\"start\":131337},{\"end\":131360,\"start\":131351},{\"end\":131375,\"start\":131369},{\"end\":131400,\"start\":131394},{\"end\":131783,\"start\":131777},{\"end\":131812,\"start\":131806},{\"end\":131824,\"start\":131819},{\"end\":131837,\"start\":131831},{\"end\":131851,\"start\":131844},{\"end\":131868,\"start\":131859},{\"end\":131885,\"start\":131877},{\"end\":132271,\"start\":132268},{\"end\":132286,\"start\":132282},{\"end\":132530,\"start\":132516},{\"end\":132545,\"start\":132537},{\"end\":132559,\"start\":132551},{\"end\":132576,\"start\":132568},{\"end\":132591,\"start\":132584},{\"end\":132606,\"start\":132601},{\"end\":132620,\"start\":132617},{\"end\":132865,\"start\":132861},{\"end\":132879,\"start\":132872},{\"end\":132894,\"start\":132890},{\"end\":132914,\"start\":132907},{\"end\":132916,\"start\":132915},{\"end\":133222,\"start\":133219},{\"end\":133236,\"start\":133232},{\"end\":133248,\"start\":133242},{\"end\":133534,\"start\":133525},{\"end\":133552,\"start\":133545},{\"end\":133554,\"start\":133553},{\"end\":133569,\"start\":133564},{\"end\":133973,\"start\":133966},{\"end\":133987,\"start\":133981},{\"end\":133998,\"start\":133994},{\"end\":134429,\"start\":134427},{\"end\":134442,\"start\":134439},{\"end\":134457,\"start\":134451},{\"end\":134757,\"start\":134748},{\"end\":134767,\"start\":134764},{\"end\":134779,\"start\":134773},{\"end\":135111,\"start\":135106},{\"end\":135123,\"start\":135117},{\"end\":135409,\"start\":135404},{\"end\":135422,\"start\":135415},{\"end\":135436,\"start\":135430},{\"end\":135696,\"start\":135690},{\"end\":135717,\"start\":135706},{\"end\":135735,\"start\":135728},{\"end\":135754,\"start\":135747},{\"end\":135769,\"start\":135762},{\"end\":135787,\"start\":135779},{\"end\":135801,\"start\":135796},{\"end\":135803,\"start\":135802},{\"end\":136128,\"start\":136119},{\"end\":136153,\"start\":136146},{\"end\":136509,\"start\":136507},{\"end\":136535,\"start\":136527},{\"end\":136545,\"start\":136541},{\"end\":136845,\"start\":136836},{\"end\":136863,\"start\":136858},{\"end\":136878,\"start\":136871},{\"end\":136889,\"start\":136885},{\"end\":136909,\"start\":136902},{\"end\":136923,\"start\":136916},{\"end\":136935,\"start\":136931},{\"end\":137207,\"start\":137199},{\"end\":137223,\"start\":137216},{\"end\":137234,\"start\":137231},{\"end\":137243,\"start\":137241},{\"end\":137568,\"start\":137559},{\"end\":137584,\"start\":137579},{\"end\":137889,\"start\":137885},{\"end\":137910,\"start\":137902},{\"end\":138134,\"start\":138128},{\"end\":138162,\"start\":138153},{\"end\":138179,\"start\":138172},{\"end\":138527,\"start\":138521},{\"end\":138541,\"start\":138536},{\"end\":138554,\"start\":138553},{\"end\":138574,\"start\":138563},{\"end\":138588,\"start\":138582},{\"end\":138603,\"start\":138597},{\"end\":138617,\"start\":138609},{\"end\":138619,\"start\":138618},{\"end\":138631,\"start\":138626},{\"end\":138652,\"start\":138651},{\"end\":138669,\"start\":138662},{\"end\":139106,\"start\":139101},{\"end\":139117,\"start\":139112},{\"end\":139130,\"start\":139123},{\"end\":139142,\"start\":139136},{\"end\":139495,\"start\":139489},{\"end\":139516,\"start\":139506},{\"end\":139531,\"start\":139522},{\"end\":139541,\"start\":139536},{\"end\":139553,\"start\":139548},{\"end\":139568,\"start\":139561},{\"end\":139581,\"start\":139576},{\"end\":139594,\"start\":139590},{\"end\":139611,\"start\":139604},{\"end\":139998,\"start\":139991},{\"end\":140006,\"start\":140003},{\"end\":140021,\"start\":140015},{\"end\":140375,\"start\":140367},{\"end\":140383,\"start\":140381},{\"end\":140396,\"start\":140389},{\"end\":140410,\"start\":140404},{\"end\":140422,\"start\":140417},{\"end\":140429,\"start\":140423},{\"end\":140443,\"start\":140437},{\"end\":140868,\"start\":140860},{\"end\":140872,\"start\":140869},{\"end\":140885,\"start\":140878},{\"end\":140896,\"start\":140893},{\"end\":140904,\"start\":140902},{\"end\":140916,\"start\":140910},{\"end\":140929,\"start\":140922},{\"end\":140942,\"start\":140938},{\"end\":140955,\"start\":140949},{\"end\":140969,\"start\":140963},{\"end\":140982,\"start\":140976},{\"end\":141395,\"start\":141388},{\"end\":141406,\"start\":141401},{\"end\":141420,\"start\":141415},{\"end\":141441,\"start\":141435},{\"end\":141849,\"start\":141843},{\"end\":141861,\"start\":141855},{\"end\":141874,\"start\":141867},{\"end\":141886,\"start\":141882},{\"end\":142111,\"start\":142105},{\"end\":142125,\"start\":142121},{\"end\":142428,\"start\":142423},{\"end\":142452,\"start\":142446},{\"end\":142477,\"start\":142471},{\"end\":142849,\"start\":142846},{\"end\":142862,\"start\":142857},{\"end\":142875,\"start\":142872},{\"end\":143220,\"start\":143212},{\"end\":143236,\"start\":143231},{\"end\":143250,\"start\":143247},{\"end\":143270,\"start\":143263},{\"end\":143285,\"start\":143278},{\"end\":143296,\"start\":143292},{\"end\":143312,\"start\":143306},{\"end\":143324,\"start\":143319},{\"end\":143629,\"start\":143622},{\"end\":143640,\"start\":143636},{\"end\":143651,\"start\":143647},{\"end\":143666,\"start\":143661},{\"end\":143678,\"start\":143677},{\"end\":143697,\"start\":143688},{\"end\":143716,\"start\":143710},{\"end\":144260,\"start\":144253},{\"end\":144289,\"start\":144284},{\"end\":144503,\"start\":144497},{\"end\":144512,\"start\":144511},{\"end\":144530,\"start\":144522},{\"end\":144550,\"start\":144543},{\"end\":144564,\"start\":144558},{\"end\":144788,\"start\":144781},{\"end\":144803,\"start\":144798},{\"end\":145035,\"start\":145030},{\"end\":145049,\"start\":145044},{\"end\":145061,\"start\":145058},{\"end\":145074,\"start\":145067},{\"end\":145082,\"start\":145081},{\"end\":145101,\"start\":145092},{\"end\":145117,\"start\":145111},{\"end\":145132,\"start\":145126},{\"end\":145157,\"start\":145151},{\"end\":145530,\"start\":145529},{\"end\":145532,\"start\":145531},{\"end\":145542,\"start\":145541},{\"end\":145553,\"start\":145552},{\"end\":145566,\"start\":145565},{\"end\":145568,\"start\":145567},{\"end\":145833,\"start\":145832},{\"end\":145852,\"start\":145846},{\"end\":146095,\"start\":146094},{\"end\":146118,\"start\":146117},{\"end\":146131,\"start\":146127},{\"end\":146445,\"start\":146441},{\"end\":146458,\"start\":146452},{\"end\":146470,\"start\":146464},{\"end\":146486,\"start\":146479},{\"end\":146497,\"start\":146492},{\"end\":146501,\"start\":146498},{\"end\":146516,\"start\":146506},{\"end\":146532,\"start\":146524},{\"end\":146544,\"start\":146538},{\"end\":146856,\"start\":146852},{\"end\":146870,\"start\":146866},{\"end\":146875,\"start\":146871},{\"end\":146886,\"start\":146881},{\"end\":146902,\"start\":146896},{\"end\":146918,\"start\":146911},{\"end\":146932,\"start\":146924},{\"end\":146948,\"start\":146942},{\"end\":146963,\"start\":146957},{\"end\":146978,\"start\":146972},{\"end\":146992,\"start\":146988},{\"end\":147008,\"start\":147000},{\"end\":147022,\"start\":147018},{\"end\":147481,\"start\":147476},{\"end\":147494,\"start\":147490},{\"end\":147508,\"start\":147504},{\"end\":147527,\"start\":147518},{\"end\":147539,\"start\":147533},{\"end\":147555,\"start\":147548},{\"end\":147569,\"start\":147564},{\"end\":147579,\"start\":147576},{\"end\":147591,\"start\":147584},{\"end\":147944,\"start\":147936},{\"end\":147959,\"start\":147952},{\"end\":147975,\"start\":147969},{\"end\":147993,\"start\":147985},{\"end\":148298,\"start\":148295},{\"end\":148313,\"start\":148304},{\"end\":148328,\"start\":148320},{\"end\":148346,\"start\":148334},{\"end\":148359,\"start\":148352},{\"end\":148373,\"start\":148367},{\"end\":148716,\"start\":148708},{\"end\":148732,\"start\":148727},{\"end\":149040,\"start\":149034},{\"end\":149055,\"start\":149051},{\"end\":149073,\"start\":149068},{\"end\":149088,\"start\":149083},{\"end\":149106,\"start\":149098},{\"end\":149129,\"start\":149117},{\"end\":149143,\"start\":149137},{\"end\":149157,\"start\":149153},{\"end\":149525,\"start\":149522},{\"end\":149537,\"start\":149532},{\"end\":149832,\"start\":149831},{\"end\":149834,\"start\":149833},{\"end\":149847,\"start\":149840},{\"end\":149859,\"start\":149856},{\"end\":149864,\"start\":149860},{\"end\":149876,\"start\":149871},{\"end\":150227,\"start\":150226},{\"end\":150229,\"start\":150228},{\"end\":150242,\"start\":150235},{\"end\":150251,\"start\":150243},{\"end\":150266,\"start\":150260},{\"end\":150277,\"start\":150274},{\"end\":150282,\"start\":150278},{\"end\":150294,\"start\":150289},{\"end\":150640,\"start\":150634},{\"end\":150656,\"start\":150650},{\"end\":150667,\"start\":150664},{\"end\":150894,\"start\":150887},{\"end\":150914,\"start\":150906},{\"end\":150934,\"start\":150924},{\"end\":151177,\"start\":151171},{\"end\":151197,\"start\":151188},{\"end\":151214,\"start\":151208},{\"end\":151532,\"start\":151528},{\"end\":151549,\"start\":151539},{\"end\":151557,\"start\":151554},{\"end\":151571,\"start\":151564},{\"end\":151908,\"start\":151904},{\"end\":151919,\"start\":151913},{\"end\":151937,\"start\":151929},{\"end\":151947,\"start\":151945},{\"end\":152320,\"start\":152315},{\"end\":152342,\"start\":152337},{\"end\":152353,\"start\":152350},{\"end\":152358,\"start\":152354},{\"end\":152369,\"start\":152364},{\"end\":152618,\"start\":152614},{\"end\":152634,\"start\":152625},{\"end\":152648,\"start\":152642},{\"end\":152663,\"start\":152658},{\"end\":152674,\"start\":152670},{\"end\":152689,\"start\":152681},{\"end\":153113,\"start\":153111},{\"end\":153126,\"start\":153120},{\"end\":153131,\"start\":153127},{\"end\":153147,\"start\":153141},{\"end\":153152,\"start\":153148},{\"end\":153169,\"start\":153164},{\"end\":153566,\"start\":153561},{\"end\":153582,\"start\":153577},{\"end\":153599,\"start\":153592},{\"end\":153615,\"start\":153606},{\"end\":153628,\"start\":153621},{\"end\":153643,\"start\":153638},{\"end\":153660,\"start\":153654},{\"end\":153905,\"start\":153899},{\"end\":153918,\"start\":153911},{\"end\":153933,\"start\":153927},{\"end\":153943,\"start\":153942},{\"end\":153959,\"start\":153953},{\"end\":153978,\"start\":153972},{\"end\":153992,\"start\":153986},{\"end\":154368,\"start\":154362},{\"end\":154380,\"start\":154374},{\"end\":154392,\"start\":154387},{\"end\":154724,\"start\":154719},{\"end\":154741,\"start\":154735},{\"end\":154758,\"start\":154750},{\"end\":155019,\"start\":155013},{\"end\":155035,\"start\":155029},{\"end\":155048,\"start\":155043},{\"end\":155241,\"start\":155239},{\"end\":155253,\"start\":155248},{\"end\":155575,\"start\":155571},{\"end\":155597,\"start\":155590},{\"end\":155611,\"start\":155603},{\"end\":155627,\"start\":155618},{\"end\":155868,\"start\":155861},{\"end\":155881,\"start\":155875},{\"end\":155898,\"start\":155895},{\"end\":155910,\"start\":155905}]", "bib_author_last_name": "[{\"end\":131298,\"start\":131288},{\"end\":131314,\"start\":131308},{\"end\":131326,\"start\":131323},{\"end\":131335,\"start\":131333},{\"end\":131349,\"start\":131344},{\"end\":131367,\"start\":131361},{\"end\":131392,\"start\":131376},{\"end\":131405,\"start\":131401},{\"end\":131804,\"start\":131784},{\"end\":131817,\"start\":131813},{\"end\":131829,\"start\":131825},{\"end\":131842,\"start\":131838},{\"end\":131857,\"start\":131852},{\"end\":131875,\"start\":131869},{\"end\":131892,\"start\":131886},{\"end\":131897,\"start\":131894},{\"end\":132280,\"start\":132272},{\"end\":132293,\"start\":132287},{\"end\":132535,\"start\":132531},{\"end\":132549,\"start\":132546},{\"end\":132566,\"start\":132560},{\"end\":132582,\"start\":132577},{\"end\":132599,\"start\":132592},{\"end\":132615,\"start\":132607},{\"end\":132630,\"start\":132621},{\"end\":132870,\"start\":132866},{\"end\":132888,\"start\":132880},{\"end\":132905,\"start\":132895},{\"end\":132922,\"start\":132917},{\"end\":133230,\"start\":133223},{\"end\":133240,\"start\":133237},{\"end\":133255,\"start\":133249},{\"end\":133543,\"start\":133535},{\"end\":133562,\"start\":133555},{\"end\":133573,\"start\":133570},{\"end\":133979,\"start\":133974},{\"end\":133992,\"start\":133988},{\"end\":134006,\"start\":133999},{\"end\":134437,\"start\":134430},{\"end\":134449,\"start\":134443},{\"end\":134464,\"start\":134458},{\"end\":134762,\"start\":134758},{\"end\":134771,\"start\":134768},{\"end\":134796,\"start\":134780},{\"end\":135115,\"start\":135112},{\"end\":135134,\"start\":135124},{\"end\":135413,\"start\":135410},{\"end\":135428,\"start\":135423},{\"end\":135447,\"start\":135437},{\"end\":135704,\"start\":135697},{\"end\":135726,\"start\":135718},{\"end\":135745,\"start\":135736},{\"end\":135760,\"start\":135755},{\"end\":135777,\"start\":135770},{\"end\":135794,\"start\":135788},{\"end\":135812,\"start\":135804},{\"end\":136133,\"start\":136129},{\"end\":136144,\"start\":136135},{\"end\":136159,\"start\":136154},{\"end\":136165,\"start\":136161},{\"end\":136525,\"start\":136510},{\"end\":136539,\"start\":136536},{\"end\":136549,\"start\":136546},{\"end\":136560,\"start\":136551},{\"end\":136856,\"start\":136846},{\"end\":136869,\"start\":136864},{\"end\":136883,\"start\":136879},{\"end\":136900,\"start\":136890},{\"end\":136914,\"start\":136910},{\"end\":136929,\"start\":136924},{\"end\":136943,\"start\":136936},{\"end\":137214,\"start\":137208},{\"end\":137229,\"start\":137224},{\"end\":137239,\"start\":137235},{\"end\":137251,\"start\":137244},{\"end\":137577,\"start\":137569},{\"end\":137591,\"start\":137585},{\"end\":137900,\"start\":137890},{\"end\":137917,\"start\":137911},{\"end\":138151,\"start\":138135},{\"end\":138170,\"start\":138163},{\"end\":138188,\"start\":138180},{\"end\":138534,\"start\":138528},{\"end\":138551,\"start\":138542},{\"end\":138561,\"start\":138555},{\"end\":138580,\"start\":138575},{\"end\":138595,\"start\":138589},{\"end\":138607,\"start\":138604},{\"end\":138624,\"start\":138620},{\"end\":138642,\"start\":138632},{\"end\":138649,\"start\":138644},{\"end\":138660,\"start\":138653},{\"end\":138680,\"start\":138670},{\"end\":138686,\"start\":138682},{\"end\":139110,\"start\":139107},{\"end\":139121,\"start\":139118},{\"end\":139134,\"start\":139131},{\"end\":139147,\"start\":139143},{\"end\":139504,\"start\":139496},{\"end\":139520,\"start\":139517},{\"end\":139534,\"start\":139532},{\"end\":139546,\"start\":139542},{\"end\":139559,\"start\":139554},{\"end\":139574,\"start\":139569},{\"end\":139588,\"start\":139582},{\"end\":139602,\"start\":139595},{\"end\":139616,\"start\":139612},{\"end\":140001,\"start\":139999},{\"end\":140013,\"start\":140007},{\"end\":140028,\"start\":140022},{\"end\":140379,\"start\":140376},{\"end\":140387,\"start\":140384},{\"end\":140402,\"start\":140397},{\"end\":140415,\"start\":140411},{\"end\":140435,\"start\":140430},{\"end\":140460,\"start\":140444},{\"end\":140876,\"start\":140873},{\"end\":140891,\"start\":140886},{\"end\":140900,\"start\":140897},{\"end\":140908,\"start\":140905},{\"end\":140920,\"start\":140917},{\"end\":140936,\"start\":140930},{\"end\":140947,\"start\":140943},{\"end\":140961,\"start\":140956},{\"end\":140974,\"start\":140970},{\"end\":140999,\"start\":140983},{\"end\":141399,\"start\":141396},{\"end\":141413,\"start\":141407},{\"end\":141433,\"start\":141421},{\"end\":141448,\"start\":141442},{\"end\":141853,\"start\":141850},{\"end\":141865,\"start\":141862},{\"end\":141880,\"start\":141875},{\"end\":141890,\"start\":141887},{\"end\":142119,\"start\":142112},{\"end\":142131,\"start\":142126},{\"end\":142444,\"start\":142429},{\"end\":142469,\"start\":142453},{\"end\":142484,\"start\":142478},{\"end\":142497,\"start\":142486},{\"end\":142855,\"start\":142850},{\"end\":142870,\"start\":142863},{\"end\":142882,\"start\":142876},{\"end\":143229,\"start\":143221},{\"end\":143245,\"start\":143237},{\"end\":143261,\"start\":143251},{\"end\":143276,\"start\":143271},{\"end\":143290,\"start\":143286},{\"end\":143304,\"start\":143297},{\"end\":143317,\"start\":143313},{\"end\":143330,\"start\":143325},{\"end\":143634,\"start\":143630},{\"end\":143645,\"start\":143641},{\"end\":143659,\"start\":143652},{\"end\":143675,\"start\":143667},{\"end\":143686,\"start\":143679},{\"end\":143708,\"start\":143698},{\"end\":143725,\"start\":143717},{\"end\":143731,\"start\":143727},{\"end\":144282,\"start\":144261},{\"end\":144296,\"start\":144290},{\"end\":144308,\"start\":144298},{\"end\":144509,\"start\":144504},{\"end\":144520,\"start\":144513},{\"end\":144541,\"start\":144531},{\"end\":144556,\"start\":144551},{\"end\":144571,\"start\":144565},{\"end\":144577,\"start\":144573},{\"end\":144796,\"start\":144789},{\"end\":144810,\"start\":144804},{\"end\":144819,\"start\":144812},{\"end\":145042,\"start\":145036},{\"end\":145056,\"start\":145050},{\"end\":145065,\"start\":145062},{\"end\":145079,\"start\":145075},{\"end\":145090,\"start\":145083},{\"end\":145109,\"start\":145102},{\"end\":145124,\"start\":145118},{\"end\":145149,\"start\":145133},{\"end\":145163,\"start\":145158},{\"end\":145539,\"start\":145533},{\"end\":145550,\"start\":145543},{\"end\":145563,\"start\":145554},{\"end\":145576,\"start\":145569},{\"end\":145844,\"start\":145834},{\"end\":145858,\"start\":145853},{\"end\":146092,\"start\":146075},{\"end\":146104,\"start\":146096},{\"end\":146115,\"start\":146106},{\"end\":146125,\"start\":146119},{\"end\":146141,\"start\":146132},{\"end\":146154,\"start\":146143},{\"end\":146450,\"start\":146446},{\"end\":146462,\"start\":146459},{\"end\":146477,\"start\":146471},{\"end\":146490,\"start\":146487},{\"end\":146504,\"start\":146502},{\"end\":146522,\"start\":146517},{\"end\":146536,\"start\":146533},{\"end\":146547,\"start\":146545},{\"end\":146864,\"start\":146857},{\"end\":146879,\"start\":146876},{\"end\":146894,\"start\":146887},{\"end\":146909,\"start\":146903},{\"end\":146922,\"start\":146919},{\"end\":146940,\"start\":146933},{\"end\":146955,\"start\":146949},{\"end\":146970,\"start\":146964},{\"end\":146986,\"start\":146979},{\"end\":146998,\"start\":146993},{\"end\":147016,\"start\":147009},{\"end\":147032,\"start\":147023},{\"end\":147488,\"start\":147482},{\"end\":147502,\"start\":147495},{\"end\":147516,\"start\":147509},{\"end\":147531,\"start\":147528},{\"end\":147546,\"start\":147540},{\"end\":147562,\"start\":147556},{\"end\":147574,\"start\":147570},{\"end\":147582,\"start\":147580},{\"end\":147595,\"start\":147592},{\"end\":147950,\"start\":147945},{\"end\":147967,\"start\":147960},{\"end\":147983,\"start\":147976},{\"end\":148001,\"start\":147994},{\"end\":148302,\"start\":148299},{\"end\":148318,\"start\":148314},{\"end\":148332,\"start\":148329},{\"end\":148350,\"start\":148347},{\"end\":148365,\"start\":148360},{\"end\":148390,\"start\":148374},{\"end\":148725,\"start\":148717},{\"end\":148744,\"start\":148733},{\"end\":149049,\"start\":149041},{\"end\":149066,\"start\":149056},{\"end\":149081,\"start\":149074},{\"end\":149096,\"start\":149089},{\"end\":149115,\"start\":149107},{\"end\":149135,\"start\":149130},{\"end\":149151,\"start\":149144},{\"end\":149165,\"start\":149158},{\"end\":149530,\"start\":149526},{\"end\":149543,\"start\":149538},{\"end\":149838,\"start\":149835},{\"end\":149854,\"start\":149848},{\"end\":149869,\"start\":149865},{\"end\":149880,\"start\":149877},{\"end\":149885,\"start\":149882},{\"end\":150233,\"start\":150230},{\"end\":150258,\"start\":150252},{\"end\":150272,\"start\":150267},{\"end\":150287,\"start\":150283},{\"end\":150298,\"start\":150295},{\"end\":150303,\"start\":150300},{\"end\":150648,\"start\":150641},{\"end\":150662,\"start\":150657},{\"end\":150675,\"start\":150668},{\"end\":150904,\"start\":150895},{\"end\":150922,\"start\":150915},{\"end\":150940,\"start\":150935},{\"end\":151186,\"start\":151178},{\"end\":151206,\"start\":151198},{\"end\":151221,\"start\":151215},{\"end\":151537,\"start\":151533},{\"end\":151552,\"start\":151550},{\"end\":151562,\"start\":151558},{\"end\":151580,\"start\":151572},{\"end\":151911,\"start\":151909},{\"end\":151927,\"start\":151920},{\"end\":151943,\"start\":151938},{\"end\":151950,\"start\":151948},{\"end\":152335,\"start\":152321},{\"end\":152348,\"start\":152343},{\"end\":152362,\"start\":152359},{\"end\":152373,\"start\":152370},{\"end\":152623,\"start\":152619},{\"end\":152640,\"start\":152635},{\"end\":152656,\"start\":152649},{\"end\":152668,\"start\":152664},{\"end\":152679,\"start\":152675},{\"end\":152696,\"start\":152690},{\"end\":153118,\"start\":153114},{\"end\":153139,\"start\":153132},{\"end\":153162,\"start\":153153},{\"end\":153182,\"start\":153170},{\"end\":153575,\"start\":153567},{\"end\":153590,\"start\":153583},{\"end\":153604,\"start\":153600},{\"end\":153619,\"start\":153616},{\"end\":153636,\"start\":153629},{\"end\":153652,\"start\":153644},{\"end\":153667,\"start\":153661},{\"end\":153909,\"start\":153906},{\"end\":153925,\"start\":153919},{\"end\":153940,\"start\":153934},{\"end\":153951,\"start\":153944},{\"end\":153970,\"start\":153960},{\"end\":153984,\"start\":153979},{\"end\":154009,\"start\":153993},{\"end\":154015,\"start\":154011},{\"end\":154372,\"start\":154369},{\"end\":154385,\"start\":154381},{\"end\":154395,\"start\":154393},{\"end\":154733,\"start\":154725},{\"end\":154748,\"start\":154742},{\"end\":154765,\"start\":154759},{\"end\":155027,\"start\":155020},{\"end\":155041,\"start\":155036},{\"end\":155054,\"start\":155049},{\"end\":155246,\"start\":155242},{\"end\":155260,\"start\":155254},{\"end\":155588,\"start\":155576},{\"end\":155601,\"start\":155598},{\"end\":155616,\"start\":155612},{\"end\":155632,\"start\":155628},{\"end\":155873,\"start\":155869},{\"end\":155893,\"start\":155882},{\"end\":155903,\"start\":155899},{\"end\":155918,\"start\":155911}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":218628552},\"end\":131704,\"start\":131215},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":182952949},\"end\":132207,\"start\":131706},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":10773394},\"end\":132475,\"start\":132209},{\"attributes\":{\"doi\":\"arXiv:2111.12880\",\"id\":\"b3\"},\"end\":132859,\"start\":132477},{\"attributes\":{\"doi\":\"arXiv:2201.03529\",\"id\":\"b4\"},\"end\":133217,\"start\":132861},{\"attributes\":{\"doi\":\"arXiv:1805.01109\",\"id\":\"b5\"},\"end\":133427,\"start\":133219},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":209947029},\"end\":133870,\"start\":133429},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":231573431},\"end\":134247,\"start\":133872},{\"attributes\":{\"id\":\"b8\"},\"end\":134692,\"start\":134249},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":235358891},\"end\":135018,\"start\":134694},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":160705},\"end\":135355,\"start\":135020},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":6318455},\"end\":135645,\"start\":135357},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":215786368},\"end\":136066,\"start\":135647},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":53734567},\"end\":136422,\"start\":136068},{\"attributes\":{\"doi\":\"arXiv:2107.04212\",\"id\":\"b14\"},\"end\":136774,\"start\":136424},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":214728308},\"end\":137138,\"start\":136776},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":14342571},\"end\":137482,\"start\":137140},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":227275177},\"end\":137828,\"start\":137484},{\"attributes\":{\"id\":\"b18\"},\"end\":138050,\"start\":137830},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":6294674},\"end\":138442,\"start\":138052},{\"attributes\":{\"doi\":\"arXiv:1909.02027\",\"id\":\"b20\"},\"end\":139005,\"start\":138444},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":49667948},\"end\":139405,\"start\":139007},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":220265858},\"end\":139949,\"start\":139407},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":6953475},\"end\":140263,\"start\":139951},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":219792902},\"end\":140775,\"start\":140265},{\"attributes\":{\"doi\":\"arXiv:2205.00403\",\"id\":\"b25\"},\"end\":141297,\"start\":140777},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":76660838},\"end\":141790,\"start\":141299},{\"attributes\":{\"id\":\"b27\"},\"end\":142039,\"start\":141792},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":231895728},\"end\":142344,\"start\":142041},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":2077953},\"end\":142752,\"start\":142346},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":59599752},\"end\":143156,\"start\":142754},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":235435823},\"end\":143620,\"start\":143158},{\"attributes\":{\"doi\":\"arXiv:2106.04015\",\"id\":\"b32\"},\"end\":144187,\"start\":143622},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":6292807},\"end\":144455,\"start\":144189},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":102486060},\"end\":144744,\"start\":144457},{\"attributes\":{\"id\":\"b35\"},\"end\":144933,\"start\":144746},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":174803437},\"end\":145512,\"start\":144935},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":383200},\"end\":145742,\"start\":145514},{\"attributes\":{\"id\":\"b38\"},\"end\":146021,\"start\":145744},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":201103726},\"end\":146389,\"start\":146023},{\"attributes\":{\"doi\":\"arXiv:2111.10050\",\"id\":\"b40\"},\"end\":146779,\"start\":146391},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":231591445},\"end\":147391,\"start\":146781},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":204838007},\"end\":147886,\"start\":147393},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":67855879},\"end\":148222,\"start\":147888},{\"attributes\":{\"doi\":\"arXiv:2106.09022\",\"id\":\"b44\"},\"end\":148632,\"start\":148224},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":1635495},\"end\":148985,\"start\":148634},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":235417196},\"end\":149461,\"start\":148987},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":18166136},\"end\":149753,\"start\":149463},{\"attributes\":{\"id\":\"b48\"},\"end\":150152,\"start\":149755},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":250340773},\"end\":150561,\"start\":150154},{\"attributes\":{\"id\":\"b50\"},\"end\":150841,\"start\":150563},{\"attributes\":{\"doi\":\"arXiv:2008.04859\",\"id\":\"b51\"},\"end\":151113,\"start\":150843},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":38833768},\"end\":151484,\"start\":151115},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":135466220},\"end\":151809,\"start\":151486},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":220514568},\"end\":152240,\"start\":151811},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":237386425},\"end\":152612,\"start\":152242},{\"attributes\":{\"doi\":\"arXiv:1804.07461\",\"id\":\"b56\"},\"end\":153029,\"start\":152614},{\"attributes\":{\"id\":\"b57\"},\"end\":153348,\"start\":153031},{\"attributes\":{\"id\":\"b58\"},\"end\":153535,\"start\":153350},{\"attributes\":{\"id\":\"b59\"},\"end\":153828,\"start\":153537},{\"attributes\":{\"doi\":\"arXiv:2010.09875\",\"id\":\"b60\"},\"end\":154276,\"start\":153830},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":211132990},\"end\":154637,\"start\":154278},{\"attributes\":{\"doi\":\"arXiv:1704.05426\",\"id\":\"b62\"},\"end\":154967,\"start\":154639},{\"attributes\":{\"id\":\"b63\",\"matched_paper_id\":6060248},\"end\":155165,\"start\":154969},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":993769},\"end\":155512,\"start\":155167},{\"attributes\":{\"id\":\"b65\"},\"end\":155801,\"start\":155514},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":239998253},\"end\":156155,\"start\":155803}]", "bib_title": "[{\"end\":131278,\"start\":131215},{\"end\":131775,\"start\":131706},{\"end\":132266,\"start\":132209},{\"end\":133523,\"start\":133429},{\"end\":133964,\"start\":133872},{\"end\":134746,\"start\":134694},{\"end\":135104,\"start\":135020},{\"end\":135402,\"start\":135357},{\"end\":135688,\"start\":135647},{\"end\":136117,\"start\":136068},{\"end\":136834,\"start\":136776},{\"end\":137197,\"start\":137140},{\"end\":137557,\"start\":137484},{\"end\":138126,\"start\":138052},{\"end\":139099,\"start\":139007},{\"end\":139487,\"start\":139407},{\"end\":139989,\"start\":139951},{\"end\":140365,\"start\":140265},{\"end\":141386,\"start\":141299},{\"end\":142103,\"start\":142041},{\"end\":142421,\"start\":142346},{\"end\":142844,\"start\":142754},{\"end\":143210,\"start\":143158},{\"end\":144251,\"start\":144189},{\"end\":144495,\"start\":144457},{\"end\":145028,\"start\":144935},{\"end\":145527,\"start\":145514},{\"end\":146073,\"start\":146023},{\"end\":146850,\"start\":146781},{\"end\":147474,\"start\":147393},{\"end\":147934,\"start\":147888},{\"end\":148706,\"start\":148634},{\"end\":149032,\"start\":148987},{\"end\":149520,\"start\":149463},{\"end\":149829,\"start\":149755},{\"end\":150224,\"start\":150154},{\"end\":151169,\"start\":151115},{\"end\":151526,\"start\":151486},{\"end\":151902,\"start\":151811},{\"end\":152313,\"start\":152242},{\"end\":154360,\"start\":154278},{\"end\":155011,\"start\":154969},{\"end\":155237,\"start\":155167},{\"end\":155859,\"start\":155803}]", "bib_author": "[{\"end\":131300,\"start\":131280},{\"end\":131316,\"start\":131300},{\"end\":131328,\"start\":131316},{\"end\":131337,\"start\":131328},{\"end\":131351,\"start\":131337},{\"end\":131369,\"start\":131351},{\"end\":131394,\"start\":131369},{\"end\":131407,\"start\":131394},{\"end\":131806,\"start\":131777},{\"end\":131819,\"start\":131806},{\"end\":131831,\"start\":131819},{\"end\":131844,\"start\":131831},{\"end\":131859,\"start\":131844},{\"end\":131877,\"start\":131859},{\"end\":131894,\"start\":131877},{\"end\":131899,\"start\":131894},{\"end\":132282,\"start\":132268},{\"end\":132295,\"start\":132282},{\"end\":132537,\"start\":132516},{\"end\":132551,\"start\":132537},{\"end\":132568,\"start\":132551},{\"end\":132584,\"start\":132568},{\"end\":132601,\"start\":132584},{\"end\":132617,\"start\":132601},{\"end\":132632,\"start\":132617},{\"end\":132872,\"start\":132861},{\"end\":132890,\"start\":132872},{\"end\":132907,\"start\":132890},{\"end\":132924,\"start\":132907},{\"end\":133232,\"start\":133219},{\"end\":133242,\"start\":133232},{\"end\":133257,\"start\":133242},{\"end\":133545,\"start\":133525},{\"end\":133564,\"start\":133545},{\"end\":133575,\"start\":133564},{\"end\":133981,\"start\":133966},{\"end\":133994,\"start\":133981},{\"end\":134008,\"start\":133994},{\"end\":134439,\"start\":134427},{\"end\":134451,\"start\":134439},{\"end\":134466,\"start\":134451},{\"end\":134764,\"start\":134748},{\"end\":134773,\"start\":134764},{\"end\":134798,\"start\":134773},{\"end\":135117,\"start\":135106},{\"end\":135136,\"start\":135117},{\"end\":135415,\"start\":135404},{\"end\":135430,\"start\":135415},{\"end\":135449,\"start\":135430},{\"end\":135706,\"start\":135690},{\"end\":135728,\"start\":135706},{\"end\":135747,\"start\":135728},{\"end\":135762,\"start\":135747},{\"end\":135779,\"start\":135762},{\"end\":135796,\"start\":135779},{\"end\":135814,\"start\":135796},{\"end\":136135,\"start\":136119},{\"end\":136146,\"start\":136135},{\"end\":136161,\"start\":136146},{\"end\":136167,\"start\":136161},{\"end\":136527,\"start\":136507},{\"end\":136541,\"start\":136527},{\"end\":136551,\"start\":136541},{\"end\":136562,\"start\":136551},{\"end\":136858,\"start\":136836},{\"end\":136871,\"start\":136858},{\"end\":136885,\"start\":136871},{\"end\":136902,\"start\":136885},{\"end\":136916,\"start\":136902},{\"end\":136931,\"start\":136916},{\"end\":136945,\"start\":136931},{\"end\":137216,\"start\":137199},{\"end\":137231,\"start\":137216},{\"end\":137241,\"start\":137231},{\"end\":137253,\"start\":137241},{\"end\":137579,\"start\":137559},{\"end\":137593,\"start\":137579},{\"end\":137902,\"start\":137885},{\"end\":137919,\"start\":137902},{\"end\":138153,\"start\":138128},{\"end\":138172,\"start\":138153},{\"end\":138190,\"start\":138172},{\"end\":138536,\"start\":138521},{\"end\":138553,\"start\":138536},{\"end\":138563,\"start\":138553},{\"end\":138582,\"start\":138563},{\"end\":138597,\"start\":138582},{\"end\":138609,\"start\":138597},{\"end\":138626,\"start\":138609},{\"end\":138644,\"start\":138626},{\"end\":138651,\"start\":138644},{\"end\":138662,\"start\":138651},{\"end\":138682,\"start\":138662},{\"end\":138688,\"start\":138682},{\"end\":139112,\"start\":139101},{\"end\":139123,\"start\":139112},{\"end\":139136,\"start\":139123},{\"end\":139149,\"start\":139136},{\"end\":139506,\"start\":139489},{\"end\":139522,\"start\":139506},{\"end\":139536,\"start\":139522},{\"end\":139548,\"start\":139536},{\"end\":139561,\"start\":139548},{\"end\":139576,\"start\":139561},{\"end\":139590,\"start\":139576},{\"end\":139604,\"start\":139590},{\"end\":139618,\"start\":139604},{\"end\":140003,\"start\":139991},{\"end\":140015,\"start\":140003},{\"end\":140030,\"start\":140015},{\"end\":140381,\"start\":140367},{\"end\":140389,\"start\":140381},{\"end\":140404,\"start\":140389},{\"end\":140417,\"start\":140404},{\"end\":140437,\"start\":140417},{\"end\":140462,\"start\":140437},{\"end\":140878,\"start\":140860},{\"end\":140893,\"start\":140878},{\"end\":140902,\"start\":140893},{\"end\":140910,\"start\":140902},{\"end\":140922,\"start\":140910},{\"end\":140938,\"start\":140922},{\"end\":140949,\"start\":140938},{\"end\":140963,\"start\":140949},{\"end\":140976,\"start\":140963},{\"end\":141001,\"start\":140976},{\"end\":141401,\"start\":141388},{\"end\":141415,\"start\":141401},{\"end\":141435,\"start\":141415},{\"end\":141450,\"start\":141435},{\"end\":141855,\"start\":141843},{\"end\":141867,\"start\":141855},{\"end\":141882,\"start\":141867},{\"end\":141892,\"start\":141882},{\"end\":142121,\"start\":142105},{\"end\":142133,\"start\":142121},{\"end\":142446,\"start\":142423},{\"end\":142471,\"start\":142446},{\"end\":142486,\"start\":142471},{\"end\":142499,\"start\":142486},{\"end\":142857,\"start\":142846},{\"end\":142872,\"start\":142857},{\"end\":142884,\"start\":142872},{\"end\":143231,\"start\":143212},{\"end\":143247,\"start\":143231},{\"end\":143263,\"start\":143247},{\"end\":143278,\"start\":143263},{\"end\":143292,\"start\":143278},{\"end\":143306,\"start\":143292},{\"end\":143319,\"start\":143306},{\"end\":143332,\"start\":143319},{\"end\":143636,\"start\":143622},{\"end\":143647,\"start\":143636},{\"end\":143661,\"start\":143647},{\"end\":143677,\"start\":143661},{\"end\":143688,\"start\":143677},{\"end\":143710,\"start\":143688},{\"end\":143727,\"start\":143710},{\"end\":143733,\"start\":143727},{\"end\":144284,\"start\":144253},{\"end\":144298,\"start\":144284},{\"end\":144310,\"start\":144298},{\"end\":144511,\"start\":144497},{\"end\":144522,\"start\":144511},{\"end\":144543,\"start\":144522},{\"end\":144558,\"start\":144543},{\"end\":144573,\"start\":144558},{\"end\":144579,\"start\":144573},{\"end\":144798,\"start\":144781},{\"end\":144812,\"start\":144798},{\"end\":144821,\"start\":144812},{\"end\":145044,\"start\":145030},{\"end\":145058,\"start\":145044},{\"end\":145067,\"start\":145058},{\"end\":145081,\"start\":145067},{\"end\":145092,\"start\":145081},{\"end\":145111,\"start\":145092},{\"end\":145126,\"start\":145111},{\"end\":145151,\"start\":145126},{\"end\":145165,\"start\":145151},{\"end\":145541,\"start\":145529},{\"end\":145552,\"start\":145541},{\"end\":145565,\"start\":145552},{\"end\":145578,\"start\":145565},{\"end\":145846,\"start\":145832},{\"end\":145860,\"start\":145846},{\"end\":146094,\"start\":146075},{\"end\":146106,\"start\":146094},{\"end\":146117,\"start\":146106},{\"end\":146127,\"start\":146117},{\"end\":146143,\"start\":146127},{\"end\":146156,\"start\":146143},{\"end\":146452,\"start\":146441},{\"end\":146464,\"start\":146452},{\"end\":146479,\"start\":146464},{\"end\":146492,\"start\":146479},{\"end\":146506,\"start\":146492},{\"end\":146524,\"start\":146506},{\"end\":146538,\"start\":146524},{\"end\":146549,\"start\":146538},{\"end\":146866,\"start\":146852},{\"end\":146881,\"start\":146866},{\"end\":146896,\"start\":146881},{\"end\":146911,\"start\":146896},{\"end\":146924,\"start\":146911},{\"end\":146942,\"start\":146924},{\"end\":146957,\"start\":146942},{\"end\":146972,\"start\":146957},{\"end\":146988,\"start\":146972},{\"end\":147000,\"start\":146988},{\"end\":147018,\"start\":147000},{\"end\":147034,\"start\":147018},{\"end\":147490,\"start\":147476},{\"end\":147504,\"start\":147490},{\"end\":147518,\"start\":147504},{\"end\":147533,\"start\":147518},{\"end\":147548,\"start\":147533},{\"end\":147564,\"start\":147548},{\"end\":147576,\"start\":147564},{\"end\":147584,\"start\":147576},{\"end\":147597,\"start\":147584},{\"end\":147952,\"start\":147936},{\"end\":147969,\"start\":147952},{\"end\":147985,\"start\":147969},{\"end\":148003,\"start\":147985},{\"end\":148304,\"start\":148295},{\"end\":148320,\"start\":148304},{\"end\":148334,\"start\":148320},{\"end\":148352,\"start\":148334},{\"end\":148367,\"start\":148352},{\"end\":148392,\"start\":148367},{\"end\":148727,\"start\":148708},{\"end\":148746,\"start\":148727},{\"end\":149051,\"start\":149034},{\"end\":149068,\"start\":149051},{\"end\":149083,\"start\":149068},{\"end\":149098,\"start\":149083},{\"end\":149117,\"start\":149098},{\"end\":149137,\"start\":149117},{\"end\":149153,\"start\":149137},{\"end\":149167,\"start\":149153},{\"end\":149532,\"start\":149522},{\"end\":149545,\"start\":149532},{\"end\":149840,\"start\":149831},{\"end\":149856,\"start\":149840},{\"end\":149871,\"start\":149856},{\"end\":149882,\"start\":149871},{\"end\":149887,\"start\":149882},{\"end\":150235,\"start\":150226},{\"end\":150260,\"start\":150235},{\"end\":150274,\"start\":150260},{\"end\":150289,\"start\":150274},{\"end\":150300,\"start\":150289},{\"end\":150305,\"start\":150300},{\"end\":150650,\"start\":150634},{\"end\":150664,\"start\":150650},{\"end\":150677,\"start\":150664},{\"end\":150906,\"start\":150887},{\"end\":150924,\"start\":150906},{\"end\":150942,\"start\":150924},{\"end\":151188,\"start\":151171},{\"end\":151208,\"start\":151188},{\"end\":151223,\"start\":151208},{\"end\":151539,\"start\":151528},{\"end\":151554,\"start\":151539},{\"end\":151564,\"start\":151554},{\"end\":151582,\"start\":151564},{\"end\":151913,\"start\":151904},{\"end\":151929,\"start\":151913},{\"end\":151945,\"start\":151929},{\"end\":151952,\"start\":151945},{\"end\":152337,\"start\":152315},{\"end\":152350,\"start\":152337},{\"end\":152364,\"start\":152350},{\"end\":152375,\"start\":152364},{\"end\":152625,\"start\":152614},{\"end\":152642,\"start\":152625},{\"end\":152658,\"start\":152642},{\"end\":152670,\"start\":152658},{\"end\":152681,\"start\":152670},{\"end\":152698,\"start\":152681},{\"end\":153120,\"start\":153111},{\"end\":153141,\"start\":153120},{\"end\":153164,\"start\":153141},{\"end\":153184,\"start\":153164},{\"end\":153577,\"start\":153561},{\"end\":153592,\"start\":153577},{\"end\":153606,\"start\":153592},{\"end\":153621,\"start\":153606},{\"end\":153638,\"start\":153621},{\"end\":153654,\"start\":153638},{\"end\":153669,\"start\":153654},{\"end\":153911,\"start\":153899},{\"end\":153927,\"start\":153911},{\"end\":153942,\"start\":153927},{\"end\":153953,\"start\":153942},{\"end\":153972,\"start\":153953},{\"end\":153986,\"start\":153972},{\"end\":154011,\"start\":153986},{\"end\":154017,\"start\":154011},{\"end\":154374,\"start\":154362},{\"end\":154387,\"start\":154374},{\"end\":154397,\"start\":154387},{\"end\":154735,\"start\":154719},{\"end\":154750,\"start\":154735},{\"end\":154767,\"start\":154750},{\"end\":155029,\"start\":155013},{\"end\":155043,\"start\":155029},{\"end\":155056,\"start\":155043},{\"end\":155248,\"start\":155239},{\"end\":155262,\"start\":155248},{\"end\":155590,\"start\":155571},{\"end\":155603,\"start\":155590},{\"end\":155618,\"start\":155603},{\"end\":155634,\"start\":155618},{\"end\":155875,\"start\":155861},{\"end\":155895,\"start\":155875},{\"end\":155905,\"start\":155895},{\"end\":155920,\"start\":155905}]", "bib_venue": "[{\"end\":131451,\"start\":131407},{\"end\":131948,\"start\":131899},{\"end\":132331,\"start\":132295},{\"end\":132514,\"start\":132477},{\"end\":133017,\"start\":132940},{\"end\":133301,\"start\":133273},{\"end\":133641,\"start\":133575},{\"end\":134044,\"start\":134008},{\"end\":134425,\"start\":134249},{\"end\":134847,\"start\":134798},{\"end\":135180,\"start\":135136},{\"end\":135493,\"start\":135449},{\"end\":135841,\"start\":135814},{\"end\":136229,\"start\":136167},{\"end\":136505,\"start\":136424},{\"end\":136949,\"start\":136945},{\"end\":137303,\"start\":137253},{\"end\":137642,\"start\":137593},{\"end\":137883,\"start\":137830},{\"end\":138239,\"start\":138190},{\"end\":138519,\"start\":138444},{\"end\":139198,\"start\":139149},{\"end\":139670,\"start\":139618},{\"end\":140092,\"start\":140030},{\"end\":140511,\"start\":140462},{\"end\":140858,\"start\":140777},{\"end\":141519,\"start\":141450},{\"end\":141841,\"start\":141792},{\"end\":142185,\"start\":142133},{\"end\":142528,\"start\":142499},{\"end\":142947,\"start\":142884},{\"end\":143381,\"start\":143332},{\"end\":143884,\"start\":143749},{\"end\":144314,\"start\":144310},{\"end\":144593,\"start\":144579},{\"end\":144779,\"start\":144746},{\"end\":145214,\"start\":145165},{\"end\":145617,\"start\":145578},{\"end\":145830,\"start\":145744},{\"end\":146199,\"start\":146156},{\"end\":146439,\"start\":146391},{\"end\":147078,\"start\":147034},{\"end\":147633,\"start\":147597},{\"end\":148047,\"start\":148003},{\"end\":148293,\"start\":148224},{\"end\":148794,\"start\":148746},{\"end\":149216,\"start\":149167},{\"end\":149584,\"start\":149545},{\"end\":149945,\"start\":149887},{\"end\":150349,\"start\":150305},{\"end\":150632,\"start\":150563},{\"end\":150885,\"start\":150843},{\"end\":151275,\"start\":151223},{\"end\":151626,\"start\":151582},{\"end\":152013,\"start\":151952},{\"end\":152419,\"start\":152375},{\"end\":152799,\"start\":152714},{\"end\":153109,\"start\":153031},{\"end\":153424,\"start\":153350},{\"end\":153559,\"start\":153537},{\"end\":153897,\"start\":153830},{\"end\":154449,\"start\":154397},{\"end\":154717,\"start\":154639},{\"end\":155059,\"start\":155056},{\"end\":155332,\"start\":155262},{\"end\":155569,\"start\":155514},{\"end\":155972,\"start\":155920}]"}}}, "year": 2023, "month": 12, "day": 17}