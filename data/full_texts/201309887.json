{"id": 201309887, "updated": "2023-10-06 23:51:42.75", "metadata": {"title": "BRIDGE: Byzantine-resilient Decentralized Gradient Descent", "authors": "[{\"first\":\"Cheng\",\"last\":\"Fang\",\"middle\":[]},{\"first\":\"Zhixiong\",\"last\":\"Yang\",\"middle\":[]},{\"first\":\"Waheed\",\"last\":\"Bajwa\",\"middle\":[\"U.\"]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Machine learning has begun to play a central role in many applications. A multitude of these applications typically also involve datasets that are distributed across multiple computing devices/machines due to either design constraints (e.g., multiagent systems) or computational/privacy reasons (e.g., learning on smartphone data). Such applications often require the learning tasks to be carried out in a decentralized fashion, in which there is no central server that is directly connected to all nodes. In real-world decentralized settings, nodes are prone to undetected failures due to malfunctioning equipment, cyberattacks, etc., which are likely to crash non-robust learning algorithms. The focus of this paper is on robustification of decentralized learning in the presence of nodes that have undergone Byzantine failures. The Byzantine failure model allows faulty nodes to arbitrarily deviate from their intended behaviors, thereby ensuring designs of the most robust of algorithms. But the study of Byzantine resilience within decentralized learning, in contrast to distributed learning, is still in its infancy. In particular, existing Byzantine-resilient decentralized learning methods either do not scale well to large-scale machine learning models, or they lack statistical convergence guarantees that help characterize their generalization errors. In this paper, a scalable, Byzantine-resilient decentralized machine learning framework termed Byzantine-resilient decentralized gradient descent (BRIDGE) is introduced. Algorithmic and statistical convergence guarantees for one variant of BRIDGE are also provided in the paper for both strongly convex problems and a class of nonconvex problems. In addition, large-scale decentralized learning experiments are used to establish that the BRIDGE framework is scalable and it delivers competitive results for Byzantine-resilient convex and nonconvex learning.", "fields_of_study": "[\"Mathematics\",\"Computer Science\",\"Engineering\"]", "external_ids": {"arxiv": "1908.08098", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/tsipn/FangYB22", "doi": "10.1109/tsipn.2022.3188456"}}, "content": {"source": {"pdf_hash": "27104c5212b1a2216fe37dfb2a52384e16bb45e5", "pdf_src": "Arxiv", "pdf_uri": "[\"https://arxiv.org/pdf/1908.08098v3.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "49b842cc206e12eb7f4cc7a7c2907b887cd6a94d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/27104c5212b1a2216fe37dfb2a52384e16bb45e5.txt", "contents": "\nBRIDGE: Byzantine-resilient Decentralized Gradient Descent\n\n\nCheng Fang \nZhixiong Yang \nWaheed U Bajwa \nBRIDGE: Byzantine-resilient Decentralized Gradient Descent\n1\nMachine learning has begun to play a central role in many applications. A multitude of these applications typically also involve datasets that are distributed across multiple computing devices/machines due to either design constraints (e.g., multiagent and Internet-of-Things systems) or computational/privacy reasons (e.g., large-scale machine learning on smartphone data). Such applications often require the learning tasks to be carried out in a decentralized fashion, in which there is no central server that is directly connected to all nodes. In real-world decentralized settings, nodes are prone to undetected failures due to malfunctioning equipment, cyberattacks, etc., which are likely to crash non-robust learning algorithms. The focus of this paper is on robustification of decentralized learning in the presence of nodes that have undergone Byzantine failures. The Byzantine failure model allows faulty nodes to arbitrarily deviate from their intended behaviors, thereby ensuring designs of the most robust of algorithms. But the study of Byzantine resilience within decentralized learning, in contrast to distributed learning, is still in its infancy. In particular, existing Byzantine-resilient decentralized learning methods either do not scale well to large-scale machine learning models, or they lack statistical convergence guarantees that help characterize their generalization errors. In this paper, a scalable, Byzantine-resilient decentralized machine learning framework termed Byzantine-resilient decentralized gradient descent (BRIDGE) is introduced. Algorithmic and statistical convergence guarantees for one variant of BRIDGE are also provided in the paper for both strongly convex problems and a class of nonconvex problems. In addition, large-scale decentralized learning experiments are used to establish that the BRIDGE framework is scalable and it delivers competitive results for Byzantine-resilient convex and nonconvex learning.\n\nI. INTRODUCTION\n\nOne of the fundamental tasks of machine learning (ML) is to learn a model using training data that minimizes the statistical risk [1]. A typical technique that accomplishes this task is empirical risk minimization (ERM) of a loss function [2]- [6]. Under the ERM framework, an ML model is learned by an optimization algorithm that tries to minimize the average loss with respect to the training data that are assumed available at a single location. In many recent applications of ML, however, training data tend to be geographically distributed; examples include the multi-agent and Internet-of-Things systems, smart grids, sensor networks, etc. In several other recent applications C of ML, the training data cannot be gathered at a single machine due to either the massive scale of data and/or privacy concerns; examples in this case include the social network data, smartphone data, healthcare data, etc. The applications in both such cases require that the ML model be learned using training data that are distributed over a network. When the ML/optimization algorithm in such applications requires a central coordinating server connected to all the nodes in the network, the resulting framework is often referred to as distributed learning [7]. Practical constraints many times also require an application to accomplish the learning tasks without a central server [8], in which case the resulting framework is referred to as decentralized learning.\n\nThe focus of this paper is on decentralized learning, with a particular emphasis on characterizing the sample complexity of the decentralized learning algorithm-i.e., the rate, as a function of the number of training data samples, at which the ERM solution approaches the Bayes optimal solution in a decentralized setting [5], [8]. While decentralized learning has a rich history, a significant fraction of that work has focused on the faultless setting [9]- [13]. But real-world decentralized systems are bound to undergo failures because of malfunctioning equipment, cyberattacks, and so on [14]. And when the failures happen and go undetected, the learning algorithms designed for the faultless networks break down [7], [15]. Among the different types of failures in the network, the so-called Byzantine failure [16] is considered the most general, as it allows the faulty/compromised nodes to arbitrarily deviate from the agreed-upon protocol [14]. Byzantine failures are the hardest to safeguard against and can easily jeopardize the ability of the network to reach consensus [17], [18]. Moreover, it has been shown in [15] that a single Byzantine node with a simple strategy can lead to the failure of a decentralized learning algorithm. The overarching goal of this paper is to develop and (algorithmically and statistically) analyze an efficient decentralized learning algorithm that is provably resilient against Byzantine failures in decentralized settings with respect to both convex and nonconvex loss functions.\n\n\nA. Relationship to prior works\n\nAlthough the model of Byzantine failure was brought up decades ago, it has attracted the attention of ML researchers only very recently. Motivated by applications in large-scale machine learning [8], much of that work has focused solely on the distributed learning setup such as the parameter-server setting [19] and the federated learning setting [20]. A necessarily incomplete list of these works, most of which have developed and analyzed Byzantine-resilient distributed learning approaches from the perspective of stochastic gradient descent, include [21]- [42]. Nonetheless, translating the algorithmic and analytical insights from the distributed learning setups to the decentralized ones, which lack central coordinating servers, is a nontrivial endeavor. As such, despite the plethora of work on Byzantine-resilient distributed learning, the problem of Byzantine-resilient decentralized learning-with the exception of a handful of works discussed in the following-largely remains unexplored in the literature.\n\nIn terms of decentralized learning in general, there are three broad classes of iterative algorithms that can be utilized for decentralized training purposes. The first one of these classes of algorithms corresponds to first-order methods such as the distributed gradient descent (DGD) and its (stochastic) variants [43]- [46]. The iterative methods in this class have low (local) computational complexity, which makes them particularly well suited for large-scale problems. The second class of algorithms involves the use of augmented Lagrangianbased methods [47]- [49], which require each node in the network to locally solve an optimization subproblem. The third class of algorithms includes second-order methods [50], [51], which typically have high computational and/or communications cost. Although the decentralized learning methods within these three classes of algorithms have their own sets of strengths and weaknesses, all of these traditional works assume faultless operations within the decentralized network.\n\nWithin the context of Byzantine failures in decentralized systems, some of the first works focused on the problem of Byzantine-resilient averaging consensus [52], [53]. These works were then leveraged to develop theory and algorithms for Byzantine-resilient decentralized learning for the case of scalar-valued models [15], [54]. But neither of these works are applicable to the general vector-valued ML framework being considered in this paper. In parallel, some researchers have also developed Byzantine-resilient decentralized learning methods for some specific vector-valued problems that include the decentralized support vector machine [55] and decentralized estimation [56]- [58].\n\nSimilar to the classical ML framework, however, there is a need to develop and algorithmically/statistically analyze Byzantine-resilient decentralized learning methods for vector-valued models for general-rather than specializedloss functions, which can be broadly divided into two classes of convex and nonconvex loss functions. The first work in the literature that tackled this problem is [59], which developed a decentralized coordinate-descent-based learning algorithm termed ByRDiE and established its resilience to Byzantine failures in the case of a loss function that is given by the sum of a convex differentiable function and a strictly convex and smooth regularizer. The analysis in [59] also provided rates for algorithmic convergence as well as statistical convergence (i.e., sample complexity) of ByRDiE. One of the limitations of [59] is its exclusive focus on convex loss functions for the purposes of analysis. More importantly, however, the coordinate-descent nature of ByRDiE makes it slow and inefficient for learning of large-scale models. Let d denote the number of parameters in the ML model being trained (e.g., the number of weights in a deep neural network). One iteration of ByRDiE then requires updating the d coordinates of the model in d network-wide collaborative steps, each one of which requires a computation of the local d-dimensional gradient at each node in the network. In the case of large-scale models such as the deep neural networks with tens or hundreds of thousands of parameters, the local computation costs as well as the network-wide coordination and communications overhead of such an approach can be prohibitive for many applications. By contrast, since the algorithmic developments in this paper are based on the gradient-descent method, the resulting computational framework is highly efficient and scalable in a decentralized setting. And while the algorithmic and statistical convergence results derived in here match those for ByRDiE in the case of convex loss functions, the proposed framework is fundamentally different from ByRDiE and therefore necessitates its own theoretical analysis.\n\nWe conclude by noting that some additional works [60]- [63] relevant to the topic of Byzantine-resilient decentralized learning have appeared during the course of revising this paper, which are being discussed here for the sake of completeness. It is worth reminding the reader, however, that the work in this paper predates these recent efforts. Equally important, none of these works provide statistical convergence rates for the proposed methods. Additionally, the work in [60] only focuses on convex loss functions and it does not provide any convergence rates. Further, the ability of the proposed algorithm to defend against a large number of Byzantine nodes severely diminishes with an increase in the problem dimension. In contrast, the authors in [61] focus on Byzantine-resilient decentralized learning in the presence of non-uniformly distributed data and time-varying networks. The focus in this work is also only on convex loss functions and the performance of the proposed algorithm is worse than that of the approach advocated in this work for static networks and uniformly distributed data. Next, an algorithm termed MOZI is proposed in [62], with the focus once again being on convex loss functions. The resilience of MOZI, however, requires an aggressive two-step 'filtering' operation, which limits the maximum number of Byzantine nodes that can be handled by the algorithm. The analysis in [62] also makes the unrealistic assumption that the faulty nodes always send messages that are 'outliers' relative to those of the regular nodes. Finally, the only paper in the literature that has investigated Byzantineresilient decentralized learning for nonconvex loss functions is [63]. The authors in this work have introduced three methods, among which the so-called ICwTM method is effectively a variant of our approach. The ICwTM algorithm, however, has at least twice the communications overhead of our approach, since it requires the neighbors to exchange both their local models and local gradients. In addition, [63] requires the nodes to have the same initialization and it does not bring out the dependence of the network topology on the learning problem.\n\nRemark 1. While this paper was in review, a related work [68] appeared on a preprint server; this recent work on Byzantineresilient decentralized learning, in contrast to our paper, studies a more general class of nonconvex loss functions and also allows the distribution of training data at the regular nodes to  be heterogeneous. However, in addition to the fact that [68] significantly postdates our work, the main result in [68] relies on clairvoyant knowledge of several network-wide parameters, including the subset of Byzantine nodes within the network, and also requires the maximum 'cumulative mixing weight' associated with the Byzantine nodes to be impractically small (e.g., even in the case of a fully connected network, the cumulative weight must be no greater than 9.76 \u00d7 10 \u22125 ).\n\n\nB. Our contributions\n\nOne of the main contributions of this paper is the introduction of an efficient and scalable algorithmic framework for Byzantine-resilient decentralized learning. The proposed framework, termed Byzantine-resilient decentralized gradient descent (BRIDGE), overcomes the computational and communications overhead associated with the one-coordinate-ata-time update pattern of ByRDiE through its use of the gradient descent-style updates. Specifically, the network nodes locally compute the d-dimensional gradient (and exchange the local d-dimensional model) only once in each iteration of the BRIDGE framework, as opposed to the d computations of the d-dimensional gradient in each iteration of ByRDiE. The BRIDGE framework therefore has significantly less local computational cost due to fewer gradient computations, and it also has smaller network-wide coordination and communications overhead due to fewer exchange of node-to-node messages. Note that BRIDGE is being referred to as a framework since it allows for multiple variants of a single algorithm depending on the choice of the screening method used within the algorithm for resilience purposes; see Section III for further details.\n\nAnother main contribution of this paper is analysis of one of the variants of BRIDGE, termed BRIDGE-T, for resilience against Byzantine failures in the network. The analysis enables us to provide both algorithmic convergence rates and statistical convergence rates for BRIDGE-T for certain classes of convex and nonconvex loss functions, with the rates derived for the convex setting matching those for ByRDiE [59]. The final main contribution of this paper is reporting of large-scale numerical results on the MNIST [69] and CIFAR-10 [70] datasets for both convex and nonconvex decentralized learning problems in the presence of Byzantine failures. The reported results, which include both independent and identically distributed (i.i.d.) and non-i.i.d. datasets within the network, highlight the benefits of the BRIDGE framework and validate our theoretical findings.\n\nIn summary, and to the best of our knowledge, BRIDGE is the first Byzantine-resilient decentralized learning algorithm that is scalable, has results for a class of nonconvex learning problems, and provides rates for both algorithmic and statistical convergence. We also refer the reader to Table I, which compares BRIDGE with recent works in both faultless and faulty vector-valued decentralized optimization/learning settings. Additional relevant works not appearing in this table include [15], [54], since they limit themselves to scalar-valued problems, and [68], since it substantially postdates our work. Further, [15], [54] neither study nonconvex loss functions nor derive the statistical convergence rates, while the main result in [68]-despite the generality of its problem setup-is significantly restrictive, as discussed in Remark 1.\n\n\nC. Notation and organization\n\nThe following notation is used throughout the rest of the paper. We denote scalars with regular-faced lowercase and uppercase letters (e.g., a and A), vectors with bold-faced lowercase letters (e.g., a), and matrices with bold-faced uppercase letters (e.g., A). All vectors are taken to be column vectors, while [a] k and [A] ij denote the k-th element of vector a and the (i, j)-th element of matrix A, respectively. We use a to denote the 2 -norm of a, 1 to denote the vector of all ones, and I to denote the identity matrix, while (\u00b7) T denotes the transpose operation. Given two matrices A and B, the notation A B signifies that A \u2212 B is a positive semidefinite matrix. We also use a 1 , a 2 to denote the inner product between two vectors. For a given vector a and nonnegative constant \u03b3, we denote the 2 -ball of radius \u03b3 centered around a as B(a, \u03b3) := {a : a \u2212 a \u2264 \u03b3}. Finally, given a set, | \u00b7 | denotes its cardinality, while we use the notation G(J , E) to denote a graph with the set of nodes J and edges E.\n\nThe rest of this paper is organized as follows. Section II provides a mathematical formulation of the Byzantine-resilient decentralized learning problem, along with a formal definition of a Byzantine node and various assumptions on the loss function. Section III introduces the BRIDGE framework and discusses different variants of the BRIDGE algorithm. Section IV provides theoretical guarantees for the BRIDGE-T algorithm for certain classes of convex and nonconvex loss functions, which include guarantees for network-wide consensus among the nonfaulty nodes and statistical convergence. Section V reports results corresponding to numerical experiments on the MNIST and CIFAR-10 datasets for both convex and nonconvex learning problems, establishing the usefulness of the BRIDGE framework for Byzantine-resilient decentralized learning. We conclude the paper in Section VI, while the appendices contain the proofs of the main lemmas and theorems.\n\n\nII. PROBLEM FORMULATION\n\n\nA. Preliminaries\n\nLet (w, z) \u2192 f (w, z) be a non-negative-valued (and possibly regularized) loss function that maps the tuple of a model w and a data sample z to the corresponding loss f (w, z). Without loss of much generality, we assume the model w in this paper to be a parametric one, i.e., w \u2208 R d (e.g., d could be the number of parameters in a deep neural network). The data sample z, on the other hand, corresponds to a random variable on some probability space (\u2126, F, P), i.e., z is F-measurable and has been drawn from the sample space \u2126 according to the probability law P. The holy grail in machine learning (ML) is to obtain an optimal model w * that minimizes the expected loss, termed the statistical risk [5], [6], i.e.,\nw * \u2208 arg min w\u2208R d E P [f (w, z)].\n(1)\n\nA model w * that satisfies (1) is termed a statistical risk minimizer (also known as a Bayes optimal model). In the real world, however, one seldom has access to the distribution of z, which precludes the use of the statistical risk E P [f (w, z)] in any computations. Instead, a common approach utilized in ML is to leverage a collection Z := {z n } N n=1 of data samples that have been drawn according to the law P and solve an empirical variant of (1) as follows [5], [6]:\nw * ERM \u2208 arg min w\u2208R d 1 N N n=1 f (w, z n ).(2)\nThis approach, which is termed as the empirical risk minimization (ERM), typically relies on an optimization algorithm to solve for w * ERM . The resulting solution w, from the perspective of an ML practitioner, must satisfy two criteria: (i) it should have fast algorithmic convergence, measured in terms of the algorithmic convergence rate, to a fixed point (often taken to be a stationary point of (2) in centralized settings); and (ii) it should have fast statistical convergence, often specified in terms of the sample complexity (number of samples), to a statistical risk minimizer. Our focus in this paper, in contrast to several related prior works (cf. Table I), is on both the algorithmic and the statistical convergence of the ERM solution. The final set of results in this case rely on a number of assumptions on the loss function f (w, z), stated below.\n\nAssumption 1 (Bounded and Lipschitz gradients). The loss function f (w, z) is differentiable in the first argument Palmost surely (a.s.) and the gradient of f (w, z) with respect to the first argument, denoted as \u2207f (w, z), is bounded and L -Lipschitz a.s., i.e., 1 \u2200w \u2208 R d , \u2207f (w, z) \u2264 L a.s. and \u2200w 1 , w 2 \u2208 R d , \u2207f (w 1 , z)\u2212\u2207f (w 2 , z) \u2264 L w 1 \u2212w 2 a.s. 1 Unless specified otherwise, all almost sure statements in the paper are to be understood with respect to the probability law P.\n\nIn the literature, functions with L -Lipschitz gradients are also referred to as L -smooth functions. Assumption 1 implies the loss function is itself a.s. L-Lipschitz continuous [71], i.e.,\n\u2200w 1 , w 2 \u2208 R d , |f (w 1 , z) \u2212 f (w 2 , z)| \u2264 L w 1 \u2212 w 2 a.s.\nAssumption 2 (Bounded training loss). The loss function is a.s. bounded over the training samples, i.e., there exists a constant C such that sup w\u2208R d ,z\u2208Z f (w, z) \u2264 C < \u221e a.s.\n\nThe analysis carried out in this paper considers two different classes of loss functions, namely, convex functions and nonconvex functions. In the case of analysis for the convex loss functions, we make the following assumption.\n\nAssumption 3 (Strong convexity). The loss function f (w, z) is a.s. \u03bb-strongly convex in the first argument, i.e.,\n\u2200w 1 , w 2 \u2208 R d , f (w 1 , z) \u2265 f (w 2 , z)+ \u2207f (w 2 , z), w 1 \u2212w 2 + \u03bb 2 w 1 \u2212 w 2 2 a.s.\nNote that the Lipschitz gradients assumption can be relaxed to Lipschitz subgradients in the case of strongly convex loss functions. Some examples of loss functions that satisfy Assumptions 1 and 3 arise in ridge regression, elastic net regression, 2 -regularized logistic regression, and 2 -regularized training of support vector machines when the optimization variable w is constrained to belong to a bounded set in R d .\n\nOur discussion in the sequel shows that w indeed remains bounded for the algorithms in consideration, justifying the usage of Assumptions 1 and 3. And while Assumption 2, as currently stated, would not be satisfied for the aforementioned regularized problems, the analysis in the paper only requires boundedness of the data-dependent term(s) of the loss function over the finite set of training data. For the sake of compactness of notation, however, we refrain from expressing the loss function as the sum of two terms, with the implicit understanding that Assumption 2 is concerned only with the data-dependent component of f (\u00b7, \u00b7). Finally, in contrast to Assumption 3, we make the following assumption in relation to analysis of the class of nonconvex loss functions.\n\nAssumption 3 (Local strong convexity). The loss function f (w, z) is nonconvex and a.s. twice differentiable in the first argument. Next, let \u2207 2 F (w) denote the Hessian of the statistical risk F (w) := E P [f (w, z)] and let W * s denote the set of all first-order stationary points of F (w), i.e., W * s := {w \u2208 R d : \u2207F (w) = 0}. Then, for any w * s \u2208 W * s , the statistical risk is locally \u03bb-strongly convex in a sufficiently large neighborhood of w * s , i.e., there exist positive constants \u03bb and \u03b2 such that \u2200w \u2208 B(w * s , \u03b2), \u2207 2 F (w) \u03bbI. It is straightforward to see that the local strong convexity of the statistical risk does not imply the global strong convexity of either the statistical risk or the loss function. Assumptions similar to Assumption 3 are nowadays routinely used for analysis of nonconvex optimization problems in machine learning; see, e.g., [72]- [75]. In particular, Assumption 3 along with the proof techniques utilized in this work allow the theoretical results to be applicable to a broader class of functions in which the strong convexity is preserved only locally.\n\nRemark 2. The convergence guarantees for nonconvex loss functions in this paper are local in the sense that they hold as long as the BRIDGE iterates are initialized within a sufficiently small neighborhood of a stationary point (cf. Section IV). Such local convergence guarantees are typical of many results in nonconvex optimization (see, e.g., [76]- [78]), but they do not imply that an iterative algorithm requires knowledge of the stationary point.\n\n\nB. System model for decentralized learning\n\nConsider a network of M nodes (devices, machines, etc.), expressed as a directed, static, and connected graph G(J , E) in which the set J := {1, . . . , M } represents nodes in the network and the set of edges E represents communication links between the nodes. Specifically, (j, i) \u2208 E if and only if node i can directly receive messages from node j and vice versa. We also define the neighborhood N j of node j as the set of nodes that can send messages to it, i.e., N j := {i \u2208 J : (i, j) \u2208 E}. We assume each node j has access to a local training dataset\nZ j := {z jn } |Zj | n=1\n. For simplicity of exposition, we assume the cardinalities of the local training sets to be the same, as the generalization of our results to the case of Z j 's not being same sized is trivial. Collectively, therefore, the network has a total of M N samples that could be utilized for learning purposes.\n\nIn order to obtain an estimate of the statistical risk minimizer w * (cf. (1)) in this decentralized setting, one would ideally like to solve the following ERM problem:\nmin w\u2208R d 1 M N M j=1 N n=1 f (w, z jn ) = min w\u2208R d 1 M M j=1 f j (w),(3)\nwhere we have used f j (w) := 1 N N n=1 f (w, z jn ) to denote the local empirical risk associated with the j-th node. In particular, it is well known that the minimizer of (3) will statistically converge with high probability to w * in the case of a strictly convex loss function [1]. The problem in (3), however, necessitates bringing together of the data at a single location; as such, it cannot be practically solved in its current form in the decentralized setting. Instead, we assume each node j maintains a local version w j of the desired global model and collaborate among themselves to solve the following decentralized ERM problem:\nmin {w1,...,w M } 1 M M j=1 f j (w j ) subject to \u2200i, j, w i = w j . (4)\nTraditional decentralized learning algorithms proceed iteratively to solve this decentralized ERM problem [9]- [13], [47], [79]. This is typically accomplished through each node j engaging in two tasks during each iteration: update the local variable w j according to some (local and data-dependent) rule g j (\u00b7), and broadcast some summary of its local information to the nodes that have node j in their respective neighborhoods.\n\n\nC. Byzantine-resilient decentralized learning\n\nWhile decentralized learning is well understood in the case of faultless networks, the main assumption in this paper is that some of the network nodes can arbitrarily deviate from their intended behavior during the iterative process. Such deviations could be caused by malfunctioning equipment, cyberattacks, etc. We model the deviations of the faulty nodes as a Byzantine failure, which is formally defined as follows [14], [16].\n\nDefinition 1 (Byzantine node). A node j \u2208 J is said to have undergone a Byzantine failure if, during any iteration of decentralized learning, it either updates its local variable w j using an update rule g j (\u00b7) = g j (\u00b7) or it broadcasts some information other than the intended summary of its local information to the nodes in its vicinity.\n\nThroughout the remainder of this paper, we use R \u2286 J and B \u2282 J to denote the sets of nonfaulty and Byzantine nodes in the network, respectively. In addition, we use r to denote the cardinality of the set R and assume that the number of Byzantine nodes is upper bounded by an integer b. Thus, we have 0 \u2264 |B| \u2264 b and r \u2265 M \u2212 b. In addition, without loss of generality, we label the nonfaulty nodes from 1 to r within our analysis, i.e., R := {1, . . . , r}.\n\nUnder this assumption of Byzantine failures in the network, it is straightforward to see that the decentralized ERM problem as stated in (4) cannot be solved. Rather, the best one could hope for is to solve an ERM problem that is restricted to the set of nonfaulty nodes, i.e.,\nmin {wj :j\u2208R} 1 r j\u2208R f j (w j ) subject to \u2200i, j \u2208 R, w i = w j , (5)\nexcept that the set R is unknown to an algorithm and therefore traditional decentralized learning algorithms cannot be utilized for this purpose. Consequently, the main goal in this paper is threefold: (i) Develop a decentralized learning algorithm that can provably solve some variant of the decentralized ERM problem (4); (ii) Establish that the resulting solution statistically converges to the statistical risk minimizer (Assumption 3) or a stationary point of the statistical risk (Assumption 3 ); and (iii) Characterize the sample complexity of the solution, i.e., the statistical rate of convergence as a function of the number of samples, rN , associated with the nonfaulty nodes.\n\nIn order to accomplish the stated goal of this paper, we need to make one additional assumption concerning the topology of the network. This assumption, which is common in the literature on Byzantine resilience within decentralized networks [15], [59], requires definitions of the notions of a source component of a graph and a reduced graph, G red (b), of G.\n\nDefinition 2 (Source component). A source component of a graph is any subset of graph nodes such that each node in the subset has a directed path to every other node in the graph.\n\nDefinition 3 (Reduced graph). A subgraph G red (b) of G is called a reduced graph with parameter b if it is generated from G by (i) removing all Byzantine nodes along with all their incoming and outgoing edges from G, and (ii) additionally removing b incoming edges from each nonfaulty node.\n\nAssumption 4 (Sufficient network connectivity). The decentralized network is assumed to be sufficiently connected in the sense that all reduced graphs G red (b) of the underlying graph G(J , E) contain at least one source component of cardinality greater than or equal to (b + 1).\n\nWe conclude by expanding further on Assumption 4, which concerns the redundancy of information flow within the network. In words, this assumption ensures that each nonfaulty node can continue to receive information from a few other nonfaulty nodes even after a certain number of edges have been removed from every nonfaulty node. And while efficient certification of this assumption remains an open problem, there is an understanding of the generation of graphs that satisfy this assumption [52]. In addition, we have empirically observed that Assumption 4 is often satisfied in Erd\u0151s-R\u00e9nyi graphs as long as the degree of the least connected node is larger than 2b. This is also the approach we take while generating graphs for our numerical experiments. Remark 3. In the finite sample regime, in which each (nonfaulty) node has only a finite number of training data samples, the local empirical risk f j (w) at every node will be different due to the randomness of the data samples, regardless of whether the training data across the nodes are i.i.d. or non-i.i.d. While this makes the formulation in (5) similar to the one in [54] and [60] for scalar-valued and vector-valued Byzantineresilient decentralized optimization, respectively, the fundamental difference between the statistical learning framework of this work and the optimization-only framework in [54], [60] is the intrinsic focus on sample complexity in statistical learning. In particular, the sample complexity results in Section IV-B also help characterize the gap between local-only learning, in which every regular node learns its own model using its own training data, and decentralized learning, in which the regular nodes collaborate to learn a common model.\n\n\nIII. BYZANTINE-RESILIENT DECENTRALIZED\n\nGRADIENT DESCENT In the faultless case, the decentralized ERM problem (4) can be solved, perhaps to one of its stationary points, using any one of the distributed/decentralized optimization methods in the literature [43]- [46], [64]- [67]. The prototypical distributed gradient descent (DGD) method [43] with decreasing step size, for instance, accomplishes this for (strongly) convex loss functions by letting each node in iteration (t + 1) update its local variable w j (t) as\nw j (t + 1) = i\u2208Nj \u222a{j} a ji w i (t) \u2212 \u03c1(t)\u2207f j (w j (t)),(6)\nwhere 0 \u2264 a ji \u2264 1 is the weighting that node j applies to the local variable w i (t) that it receives from node i, and {\u03c1(t)} denotes a positive sequence of step sizes that satisfies\n\u03c1(t+1) \u2264 \u03c1(t), \u03c1(t) t \u2192 0, \u221e t=0 \u03c1(t) = \u221e, and \u221e t=0 \u03c1 2 (t) < \u221e.\nOne choice for such a sequence is \u03c1(t) = 1 \u03bb(t0+t) for some t 0 , which ensures that a network-wide consensus is reached among all nodes, i.e., \u2200i, j, w i (t) t \u2192 w j (t), and all local variables converge to the decentralized (and thus the centralized) ERM solution.\n\nTraditional distributed/decentralized optimization methods, however, fail to reach a stationary point of the decentralized ERM problem (4) (or its restricted variant (5)) in the\n\n\nAlgorithm 1 The BRIDGE Framework\n\nInput: Local datasets Z j , maximum number of Byzantine nodes b, step size sequence {\u03c1(t)} \u221e t=0 , and maximum number of iterations t max 1: Initialize: t \u2190 0 and \u2200j \u2208 R, w j (0) 2: for t = 0, 1, . . . , t max \u2212 1 do 3:\nBroadcast w j (t), \u2200j \u2208 R 4: Receive w i (t) at each node j \u2208 R from every i \u2208 N j \u2282 (R \u222a B) 5: y j (t) \u2190 screen({w i (t)} i\u2208Nj \u222a{j} ), \u2200j \u2208 R 6: w j (t + 1) \u2190 y j (t) \u2212 \u03c1(t) f j (w j (t)), \u2200j \u2208 R 7: end for Output: w j (t max ), \u2200j \u2208 R\npresence of a single Byzantine failure in the network [27]- [29], [54], [80]- [82]. To overcome this shortcoming of the traditional approaches as well as improve on the limitations of existing works on Bynzatine-resilient decentralized learning (cf. Sections I-A and I-B), we introduce an algorithmic framework termed Byzantine-resilient decentralized gradient descent (BRIDGE).\n\nThe BRIDGE framework, which is listed in Algorithm 1, is a gradient descent-based approach whose main update step (Step 6 in Algorithm 1) is similar to the DGD update (6). The main difference between the BRIDGE framework and DGD is that each node j \u2208 R in BRIDGE screens the incoming messages from its neighboring nodes for potentially malicious content (Step 5 in Algorithm 1) before updating its local variable w j (t). Note, however, that BRIDGE does not permanently label any nodes as malicious, which also allows it to manage any transitory Byzantine failures in a graceful manner. While this makes BRIDGE similar to the ByRDiE algorithm [59], the fundamental advantage of BRIDGE over ByRDiE is its scalability that comes from the fact that it eschews the one-coordinate-at-a-time update of the local variables in ByRDiE in favor of one update of the entire vector w j (t) in each iteration. In terms of other details, the BRIDGE framework is input with the maximum number of Byzantine nodes b that need to be tolerated, a decreasing step size sequence {\u03c1(t)}, and the maximum number of gradient descent iterations t max . Next, the local variable at each nonfaulty node j in the network is initialized at w j (0). Afterward, within every iteration (t + 1) of the framework, each node j \u2208 R broadcasts w j (t) as well as receives w i (t) from every node i \u2208 N j . This is followed by every node j \u2208 R screening the received w i (t)'s for any malicious information and then updating the local variable w j (t).\n\nIn the following, we discuss four different variants of the screening rule (Step 5 in Algorithm 1), each one of which in turn gives rise to a different realization of the BRIDGE framework. The motivation for these screening rules comes from the literature on robust statistics [83], with all these rules appearing in some form within the literature on robust averaging consensus [52], [53] and robust distributed learning [21], [27]- [29]. The challenge here of course, as discussed in Section I, is that these prior works on robust averaging con-  The BRIDGE-T variant of the BRIDGE framework uses the coordinate-wise trimmed mean as the screening rule. A similar screening principle has been utilized within distributed frameworks [28] and decentralized frameworks [15], [54], [59]. The coordinate-wise trimmed-mean screening within BRIDGE-T filters the b largest and the b smallest values in each coordinate of the local variables w i (t) received from the neighborhood of node j and uses an average of the remaining values for the update of w j (t). Specifically, for any iteration index t, BRIDGE-T finds the following three sets for each coordinate k \u2208 {1, . . . , d} in parallel:\nN k j (t) := arg min X :X \u2282Nj ,|X |=b i\u2208X [w i (t)] k ,(7)\nN k j (t) := arg max\nX :X \u2282Nj ,|X |=b i\u2208X [w i (t)] k , and(8)C k j (t) := N j \\ N k j (t) N k j (t) .(9)\nAfterward, the screening routine outputs a combined and filtered vector y j (t) whose k-th element is given by\n[y j (t)] k = 1 |N j | \u2212 2b + 1 i\u2208C k j (t)\u222a{j} [w i (t)] k .(10)\nNotice that BRIDGE-T requires each node to have at least 2b + 1 neighbors. Also, note that the elements from different neighbors may survive the screening at different coordinates. Therefore, the average within BRIDGE-T is not taken over vectors; rather, the calculation of y j (t) has to be carried out in a coordinate-wise manner.\n\nThe BRIDGE-M variant uses the coordinate-wise median as the screening rule, with a similar screening idea having been utilized within distributed frameworks [28]. Similar to BRIDGE-T, BRIDGE-M is also a coordinate-wise screening procedure in which the k-th element of the combined and filtered output y j (t) takes the form\n[y j (t)] k = median {[w i (t)] k } i\u2208Nj \u222a{j} .(11)\nNotice that, unlike BRIDGE-T, the coordinate-wise median screening within BRIDGE-M neither requires an explicit knowledge of b nor does it impose an explicit constraint on the minimum number of neighbors of each node. The BRIDGE-K variant uses the Krum function as the screening rule, which is similar to the screening principle that has been employed within distributed frameworks [21]. In terms of specifics, the Krum screening for the decentralized framework can be described as follows.\nGiven i, h \u2208 N j \u222a{j}, write h \u223c i if w h (t) is one of the |N j | \u2212 b \u2212 2 vectors with the smallest Euclidean distance, expressed as w h (t)\u2212w i (t) , from w i (t).\nThe Krum-based screening at node j then finds the neighbor index i * j (t) as i * j (t) = arg min i\u2208Nj h\u2208Nj \u222a{j}:h\u223ci\nw h (t) \u2212 w i (t) ,(12)\nand outputs the (combined and filtered) vector y j (t) as y j (t) = w i * j (t). Unlike BRIDGE-T and BRIDGE-M, BRIDGE-K utilizes vector-valued operations for screening, resulting in the surviving vector y j (t) to be entirely from one neighbor of each node. The Krum screening rule within BRIDGE-K requires the neighborhood of every node to be larger than b + 2. Note that since the Krum function requires the pairwise distances of all nodes within the neighborhood of every node, BRIDGE-K has high computational complexity in comparison to BRIDGE-T and BRIDGE-M.\n\nLast but not least, the BRIDGE-B variant-inspired by a similar screening procedure within distributed frameworks [27]-uses a combination of Krum and coordinate-wise trimmed mean as the screening rule. Specifically, the screening within BRIDGE-B involves first selecting |N j | \u2212 2b neighbors of the j-th node by recursively finding an index i * j (t) \u2208 N j using (12), removing the selected node from the neighborhood, finding a new index from N j \\ {i * j (t)} using (12) again, and repeating this Krum-based process |N j | \u2212 2b times. Next, coordinate-wise trimmed mean-based screening, as described within BRIDGE-T, is applied to the received w i (t)'s of the |N j |\u22122b neighbors of node j that survive the first-stage Krumbased screening. Intuitively, the Krum-based vector screening first guarantees the surviving neighbors have the closest w i (t)'s in terms of the Euclidean distance, while coordinatewise trimmed-mean screening afterward guarantees that each coordinate of the combined and filtered vector y j (t) only includes the \"inlier\" values. The cost of this two-step screening procedure includes high computational complexity due to the use of the Krum function and the stricter requirement that the neighborhood of each node be larger than max(4b, 3b + 2).\n\nWe conclude by providing a comparison in Table II between the four different variants of the BRIDGE framework. Note that both BRIDGE-T and BRIDGE-B reduce to DGD in the case of b = 0; this, however, is not the case for BRIDGE-M and BRIDGE-K. It is also worth noting that additional variants of BRIDGE can be obtained through further combinations of the different screening rules and/or incorporation of additional ideas from the literature on robust statistics. Nonetheless, each variant of the BRIDGE framework requires its own theoretical analysis for convergence guarantees. In this paper, we limit ourselves to BRIDGE-T for this purpose and provide the corresponding guarantees in the next section.\n\n\nIV. BRIDGE-T: CONVERGENCE GUARANTEES\n\nIn this section, we derive the algorithmic and statistical convergence guarantees for BRIDGE-T for both convex and nonconvex loss functions. While these results match those for ByRDiE for convex loss functions, they cannot be obtained directly from [59] since the filtered vector y j (t) in BRIDGE-T corresponds to an iteration-dependent amalgamation of the iterates {w i (t)} in the neighborhood of node j (cf. (10)). Our statistical convergence guarantees require the training data to be independent and identically distributed (i.i.d.) among all the nodes. Additionally, let w * denote the unique global minimizer of the statistical risk in the case of the strongly convex loss function, while it denotes one of the first-order stationary points of the statistical risk in the case of the nonconvex loss function. We assume there exists a positive constant \u0393 such that for all j \u2208 R, t \u2208 R d , w j (t) \u2212 w * \u2264 \u0393. Note that this \u0393 can be arbitrary large and so the above assumption, which is again needed for derivation of the statistical rate of convergence, is a mild one (also, see Lemma 4 and its accompanying discussion). Broadly speaking, the analysis for our algorithmic and statistical convergence guarantees proceeds as follows.\n\nFirst, we define a \"consensus\" vector v(t) and establish in Section IV-A that \u2200j \u2208 R, w j (t) \u2192 v(t) as t \u2192 \u221e for an appropriate choice of the step-size sequence \u03c1(t) that satisfies\n\u03c1(t + 1) \u2264 \u03c1(t), \u03c1(t) t \u2192 0, \u221e t=0 \u03c1(t) = \u221e, and \u221e t=0 \u03c1 2 (t) < \u221e.\nIn particular, we work with the step size\n\u03c1(t) = 1 \u03bb(t0+t) , t 0 \u2265 L \u03bb ,\nwhere L is the Lipschitz constant of the loss function; see, e.g., Assumption 1. This consensus analysis relies on the condition that the w j (0)'s for all j \u2208 R are initialized such that v(0) \u2208 B(w * , \u0393). One such choice of initialization is initializing all w j (0)'s to be within B(w * , \u0393). (Note that in terms of our analysis for the nonconvex loss function, which is provided in Section IV-B2, we will impose additional constraints on the initialization.). Afterward, we define two vectors u(t + 1) and x(t + 1) in Section IV-B that correspond to gradient descent updates of the consensus vector v(t) using, respectively, a convex combination of gradients of the local loss functions evaluated at v(t) and the gradient of the statistical risk evaluated at v(t). Finally, we define the following collection of distances: a 1 (t) := x(t+1)\u2212w * , a 2 (t) := u(t + 1) \u2212 x(t + 1) , a 3 (t) := v(t + 1) \u2212 u(t + 1) , and a 4 (t + 1) := max j\u2208R w j (t + 1) \u2212 v(t + 1) . It is straightforward to see that v(t+1)\u2212w * \u2264 a 1 (t)+a 2 (t)+a 3 (t), and we establish in Section IV-B that a 1 (t) + a 2 (t) + a 3 (t) t \u2192 0 with high probability as well as a 4 (t + 1) t \u2192 0 using Assumptions 1, 2, 3 and 4 in the convex setting and Assumptions 1, 2, 3 and 4 in the nonconvex setting, thereby completing our proof of optimality for both convex and nonconvex loss functions.\n\n\nA. Consensus guarantees\n\nLet us pick an arbitrary index k \u2208 {1, . . . , d} and define a vector \u2126(t) \u2208 R r whose respective elements correspond to the k-th element of the iterate w j (t) of nonfaulty nodes, i.e., \u2200j \u2208 R, [\u2126(t)] j = [w j (t)] k . Note that \u2126(t) as well as most of the variables in our discussion in this section depend on the index k; however, since k is arbitrary, we drop this explicit dependence on k in many instances for simplicity of notation.\n\nWe first show that the BRIDGE-T update at the nonfaulty nodes in the k-th coordinate can be expressed in a form that only involves the nonfaulty nodes. Specifically, we write\n\u2126(t + 1) = Y(t)\u2126(t) \u2212 \u03c1(t)g(t),(13)\nwhere the vector g(t) is defined as [g(t)] j = [\u2207f j (w j (t))] k , j \u2208 R. The formulation of the matrix Y(t) in this expression is as follows. Let N r j denote the nonfaulty nodes in the neighborhood of node j, i.e., N r j := R \u2229 N j . The set of Byzantine neighbors of node j can then be defined as N b j := N j \\ N r j . To make the rest of the expressions clearer, we drop the iteration index t for the remainder of this discussion, even though the variables are still t-dependent. Let us now define the notation b * := |B| as the actual (unknown) number of Byzantine nodes in the network, b k j as the number of Byzantine nodes remaining in the filtered set C k j , and q k j := b \u2212 b * + b k j . Since b \u2212 b * \u2265 0 by assumption and b k j \u2265 0 by definition, notice that only one of two cases can happen during each iteration for every coordinate k: (i) q k j > 0 or (ii) q k j = 0. For case (i), we either have b \u2212 b * > 0 or b k j > 0 or both. These conditions correspond to the scenario where the node j filters out more than b regular nodes from its neighborhood. Thus, we know that N\nk j \u2229 N r j = \u2205. Likewise, it follows that N k j \u2229 N r j = \u2205. Then \u2203m j \u2208 N k j \u2229 N r j and m j \u2208 N k j \u2229 N r j satisfying [w m j ] k \u2264 [w i ] k \u2264 [w m j ] k for any i \u2208 C k j . Thus, for every i \u2208 C k j \u2229 N b j , \u2203\u03b8 i \u2208 (0, 1) satisfying [w i ] k = \u03b8 i [w m j ] k + (1 \u2212 \u03b8 i )[w m j ] k .\nConsequently, the elements of the matrix Y can be written as\n[Y] ji = \uf8f1 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f2 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f4 \uf8f3 1 2(|Nj |\u22122b+1) , i \u2208 N r j \u2229 C k j , 1 |Nj |\u22122b+1 , i = j, i \u2208N b j \u2229C k j \u03b8 i q k j (|Nj |\u22122b+1) + i \u2208N r j \u2229C k j \u03b8 i q k j (|Nj |\u22122b+1) , i \u2208 N k j \u2229 N r j , i \u2208N b j \u2229C k j 1\u2212\u03b8 i q k j (|Nj |\u22122b+1) + i \u2208N r j \u2229C k j 1\u2212\u03b8 i q k j (|Nj |\u22122b+1) , i \u2208 N k j \u2229 N r j , 0, otherwise.(14)\nFor case (ii), we must have that b \u2212 b * = 0 and b k j = 0. Thus, all the filtered nodes in C k j would be regular nodes in this case. Therefore, we can describe Y in this case as\n[Y] ji = 1 |Nj |\u22122b+1 , i \u2208 {j} \u222a C k j , 0, otherwise.(15)\nCombining the expressions of Y in the two cases above allows us to express the update in (13) exclusively in terms of information from the nonfaulty nodes. Next, we define \u03c8 to be the total number of reduced graphs that can be generated from G, the parameter \u03bd as \u03bd := \u03c8r, and the maximum neighborhood size of the nonfaulty nodes as N max := max j\u2208R |N j |. Further, we define a transition matrix \u03a6(t, t 0 ) from some index t 0 \u2264 t to t, i.e.,\n\u03a6(t, t 0 ) := Y(t)Y(t \u2212 1) \u00b7 \u00b7 \u00b7 Y(t 0 ).(16)\nThen it follows from [84,Lemma 4] that if Assumption 4 is satisfied then\nlim t\u2192\u221e \u03a6(t, t 0 ) = 1\u03b1 T (t 0 ),(17)\nwhere the vector \u03b1(t 0 ) \u2208 R r satisfies [\u03b1(t 0 )] j \u2265 0 and \n|[\u03a6(t, t 0 )] ji \u2212 [\u03b1(t 0 )] i | \u2264 \u00b5 ( t\u2212t 0 +1 \u03bd ) ,(18)\nwhere \u00b5 \u2208 (0, 1) is defined as \u00b5 := 1 \u2212 1 (2Nmax\u22122b+1) \u03bd . Next, it follows from (13) and the definition of \u03a6(t, t 0 ) that\n\u2126(t) = Y(t \u2212 1)\u2126(t \u2212 1) \u2212 \u03c1(t \u2212 1)g(t \u2212 1) = Y(t \u2212 1)Y(t \u2212 2) \u00b7 \u00b7 \u00b7 Y(0)\u2126(0) \u2212 t\u22121 \u03c4 =0 Y(t \u2212 1)Y(t \u2212 2) \u00b7 \u00b7 \u00b7 Y(\u03c4 + 1)\u03c1(\u03c4 )g(\u03c4 ) = \u03a6(t \u2212 1, 0)\u2126(0) \u2212 t\u22121 \u03c4 =0 \u03a6(t \u2212 1, \u03c4 + 1)\u03c1(\u03c4 )g(\u03c4 ).(19)\nNow, similar to [84, Convergence Analysis of Algorithm 1], suppose all nodes stop computing their local gradients after iteration t so that g(\u03c4 ) = 0 when \u03c4 > t. Note that this is without loss of generality when we let t approach infinity, as we recover BRIDGE-T in that case. Further, let T \u2265 0 be an integer and define a vectorv(t) as follows:\nv(t) = lim T \u2192\u221e \u2126(t + T + 1) = lim T \u2192\u221e \u03a6(t + T, 0)\u2126(0) \u2212 lim T \u2192\u221e t+T \u03c4 =0 \u03a6(t + T, \u03c4 + 1)\u03c1(\u03c4 )g(\u03c4 ) = 1\u03b1 T (0)\u2126(0) \u2212 t\u22121 \u03c4 =0 1\u03b1 T (\u03c4 + 1)\u03c1(\u03c4 )g(\u03c4 ).(20)\nNotice thatv(t) is a constant vector and we define a scalarvalued sequence v(t) to be any one of its elements. We now\nshow that [w j (t)] k t \u2192 v(t). Indeed, we have from (20) that v(t) = r i=1 [\u03b1(0)] i [w i (0)] k \u2212 t\u22121 \u03c4 =0 \u03c1(\u03c4 ) r i=1 [\u03b1(\u03c4 + 1)] i [\u2207f i (w i (\u03c4 ))] k . (21)\nAlso recall from the update of [w j (t)] k that\n[w j (t)] k = r i=1 [\u03a6(t \u2212 1, 0)] ji [w i (0)] k \u2212 t\u22121 \u03c4 =0 \u03c1(\u03c4 ) r i=1 [\u03a6(t \u2212 1, \u03c4 + 1)] ji [\u2207f i (w i (\u03c4 ))] k .\nFrom Assumption 1 and the initialization of w j (t)'s, there exist two scalars C w and L such that \u2200j \u2208 R, |[w j (0)] k | \u2264 C w and |[\u2207f j (w j )] k | \u2264 L. Therefore, we have\n|[w j (t)] k \u2212 v(t)| \u2264 | r i=1 ([\u03a6(t \u2212 1, 0)] ji \u2212 [\u03b1(0)] i )[w i (0)] k | + | t\u22121 \u03c4 =0 \u03c1(\u03c4 ) r i=1 ([\u03a6(t \u2212 1, \u03c4 + 1)] ji \u2212 [\u03b1(\u03c4 + 1)] i )[\u2207f i (w i (\u03c4 ))] k | \u2264 rC w \u00b5 t \u03bd + rL t \u03c4 =0 \u03c1(\u03c4 )\u00b5 t\u2212\u03c4 +1 \u03bd t \u2192 0.(22)\nHere, the fact that the second term in the second inequality of (22) converges to zero follows from our assumptions on the decreasing step size sequence along with [84,Lemma 6]. Finally, recall that the vector-valued iterate updates in BRIDGE-T can be thought of as individual updates of the d coordinates in parallel. Therefore, since the coordinate k was arbitrarily picked, we have proven that BRIDGE-T achieves consensus among the nonfaulty nodes for both convex and nonconvex loss functions, as summarized in the following. Theorem 1. Define a vector v(t) \u2208 R d as one whose kth entry [v(t)] k is given by the right-hand-side of (21). If Assumptions 1 and 4 are satisfied, then the gap between w j (t), \u2200j \u2208 R, and v(t) goes to 0 as t \u2192 \u221e, i.e.,\nlim t\u2192\u221e a 4 (t) = lim t\u2192\u221e max j\u2208R w j (t) \u2212 v(t) \u2264 lim t\u2192\u221e \u221a drC w \u00b5 t \u03bd + \u221a drL t \u03c4 =0 \u03c1(\u03c4 )\u00b5 t\u2212\u03c4 +1 \u03bd = 0. (23)\nWe conclude our discussion with a couple of remarks. First, note that Theorem 1 has been obtained without needing Assumptions 2 and 3 / 3 . Thus, BRIDGE-T guarantees consensus among the nonfaulty nodes for both convex and nonconvex loss functions under a general set of assumptions. Second, notice that the second term in (23) is a sum of t terms and it converges to 0 at a slower rate than the first term. Among all the sub-terms in the sum of the second term, the last term \u03c1(t)\u00b5 1 \u03bd is the one that converge to zero at the slowest rate. Thus, the rate at which BRIDGE-T achieves consensus is determined by this sub-term and is given by O( \u221a d\u03c1(t)). In particular, if we choose \u03c1(t) to be O(1/t), the rate of consensus for BRIDGE-T is O(\n\u221a d/t).\n\nB. Statistical optimality guarantees\n\nWhile Theorem 1 guarantees consensus among the nonfaulty nodes by providing an upper bound on the distance a 4 (t), this result alone cannot be used to characterize the gap between the iterates {w j (t)} j\u2208R and the global minimizer (resp., first-order stationary point) w * of the statistical risk for the convex loss function (resp., nonconvex loss function). We accomplish the goal of establishing the statistical optimality by providing bounds on the remaining three distances a 1 (t), a 2 (t), and a 3 (t) described in the beginning of the section.\n\nWe start with a bound on a 3 (t). To this end, let v(t) be as defined in Theorem 1 and notice from (20) that\nv(t + 1) = v(t) \u2212 \u03c1(t)g 1 (t),(24)\nwhere g 1 (t) has the k-th entry defined in terms of gradients of the local loss functions evaluated at the local iterates as\n[g 1 (t)] k = r j=1 [\u03b1 k (t)] j [\u2207f j (w j (t))] k .\nHere, \u03b1 k (t) is an element of the probability simplex associated with the k-th coordinate of the consensus vector v(t), as described earlier. Next, define another vector g 2 (t) whose k-th entry is defined in terms of gradients of the local loss functions evaluated at the consensus vector as\n[g 2 (t)] k = r j=1 [\u03b1 k (t)] j [\u2207f j (v(t))] k .\nFurther, define a new sequence u(t + 1) as\nu(t + 1) = v(t) \u2212 \u03c1(t)g 2 (t).(25)\nIt then follows from (22), Assumption 1, and some algebraic manipulations that\na 3 (t) = v(t + 1) \u2212 u(t + 1) = \u03c1(t) g 2 (t) \u2212 g 1 (t) \u2264 \u03c1(t)L max j\u2208R v(t) \u2212 w j (t) = \u03c1(t)L a 4 (t). (26)\nWe next turn our attention to a bound on a 2 (t), which is necessarily going to be probabilistic in nature because of its dependence on the training samples, and define a new sequence x(t) as\nx(t + 1) = v(t) \u2212 \u03c1(t)\u2207E P [f (v(t), z)].(27)\nTrivially, we have\na 2 (t) = u(t + 1) \u2212 x(t + 1) = \u03c1(t) g 2 (t) \u2212 E P [f (v(t), z)] .(28)\nThe following lemma, whose proof is provided in Appendix A, now bounds a 2 (t) by establishing that g 2 (t) converges in probability to E P [f (v(t), z)].\n\nLemma 1. Suppose Assumptions 1 and 2 are satisfied and the training data are i.i.d. Then, fixing any \u03b4 \u2208 (0, 1), we have with probability at least 1 \u2212 \u03b4 that\na 2 (t) \u2264 \u03c1(t) sup t g 2 (t) \u2212 E P [f (v(t), z)] = O d \u03b1 m 2 log 2 \u03b4 N \u03c1(t),(29)\nwhere the vector \u03b1 m \u2208 R r is a problem-dependent (unknown) vector defined in Appendix A and satisfies [\u03b1 m ] j \u2265 0 and r j=1 [\u03b1 m ] j = 1.\n\nNotice that while the bounds for a 2 (t), a 3 (t), and a 4 (t) have been obtained for both the convex and nonconvex loss functions in the same manner, the bound for a 1 (t) = x(t + 1) \u2212 w * in the nonconvex setting does require the aid of Assumption 3 , as opposed to Assumption 3 in the convex setting. This leads to separate proofs of the final statistical optimality results under Assumption 3 and Assumption 3 .\n\n1) Statistical optimality for the convex case: Notice that x(t + 1) is obtained from v(t) by taking a regular gradient descent step, with step size \u03c1(t), with respect to the gradient E P [f (v(t), z)] of the statistical risk. Under Assumptions 1 and 3, therefore, it follows from our understanding of the behavior of gradient descent iterations that [85, Chapter 2.1.5]\na 1 (t) = x(t + 1) \u2212 w * \u2264 (1 \u2212 L \u03c1(t)) v(t) \u2212 w * \u2264 (1 \u2212 \u03bb\u03c1(t)) v(t) \u2212 w * ,(30)\nwhere the last inequality holds because of the fact that \u03bb \u2264 L . We then have the following bound:\nv(t + 1) \u2212 w * \u2264 (1 \u2212 \u03bb\u03c1(t)) v(t) \u2212 w * + a 2 (t) + a 3 (t).(31)\nNotice that (31) only provides the relationship between steps t and t + 1. In order to bound the distance v(t + 1) \u2212 w * in terms of the initial distance v(0) \u2212 w * , we can recursively make use of (31) to arrive at the following lemma.  \u2208 (0, 1), an upper bound on v(t + 1) \u2212 w * can be derived with probability at least 1 \u2212 \u03b4 as\nv(t + 1) \u2212 w * \u2264 t 0 t + t 0 C 1 + C 2 (N ) \u03bb + C 3 t + t 0 + C 4 1 t + t 0 1 t 0 + 1 1 + t 0 + 1 2 + t 0 + \u00b7 \u00b7 \u00b7 + 1 t + t 0 ,(32)\nwhere\nC 1 = v(0) \u2212 w * , C 2 (N ) = O d \u03b1m 2 log 2 \u03b4 N , C 3 = \u221a dL rCw \u03bb 1\u2212\u00b5 1 \u03bd and C 4 = \u221a dLL r\u00b5 1 \u03bd t0\u03bb 2 1\u2212\u00b5 1 \u03bd .\nProof of Lemma 2 is provided in Appendix B. Lemma 2 establishes that v(t + 1) \u2212 w * can be upper bounded by a sum of terms that can be made arbitrarily small for sufficiently large t and N . Since max j\u2208R w j (t) \u2212 v(t) can also be made arbitrarily small when t is sufficiently large, we can therefore bound w j (t + 1) \u2212 w * , j \u2208 R, using the bounds on v(t + 1) \u2212 w * and max j\u2208R w j (t + 1) \u2212 v(t + 1) to arrive at the following lemma. \u03bb > 0, we can always find a t 1 such that for all t \u2265 t 1 and j \u2208 R, with probability at least 1 \u2212 \u03b4, w j (t + 1) \u2212 w * \u2264 .\n\nProof of Lemma 3 is provided in Appendix C. Notice that given a sufficiently large N , C 2 (N ) is arbitrarily small, which means that the iterates of non-Byzantine nodes can be made arbitrarily close to w * . We are now ready to state the main result concerning the statistical convergence of BRIDGE-T at the nonfaulty nodes to the global statistical risk minimizer w * . Theorem 2. Suppose Assumptions 1, 2, 3, and 4 are satisfied and the training data are i.i.d. Then the iterates of BRIDGE-T converge sublinearly in t to the minimum of the global statistical risk at each nonfaulty node. In particular, given any > \u03bb > 0, \u2200j \u2208 R, with probability at least 1 \u2212 \u03b4 and for large enough t,\nw j (t + 1) \u2212 w * \u2264 ,(33)\nwhere \u03b4 = 2 exp \u2212 4rN 2\n16L 2 rd \u03b1m 2 + 2 + r log 12L \u221a rd + d log 12L \u03b2 \u221a d and = C 2 (N ) = O d \u03b1m 2 log 2 \u03b4 N .\nProof of Theorem 2 is provided in Appendix D. Note that when N \u2192 \u221e and when \u03c1(t) is chosen as a O(1/t) sequence, (33) leads to a sublinear convergence rate, as shown in Appendix D. Thus, both the algorithmic and statistical convergence rates derived for BRIDGE-T match the existing Byzantine-resilient rates in the decentralized setting [59]. In terms of the statistical guarantees, recall that when there is no failure in the network, the non-resilient gradient descent algorithms such as DGD usually have the statistical learning rate as O 1/M N , while if each node runs centralized algorithm with the given N samples, the learning rate is given as O 1/N . BRIDGE-T achieves the statistical learning rate of O \u03b1 m 2 /N , which lies between the rate of centralized learning and DGD. In particular, compared to local learning, BRIDGE-T reduces the sample complexity by a factor of \u03b1 m 2 for each node by cooperating over a network, but it cannot approach the fault-free rate. This shows the trade-off between sample complexity and robustness.\n\n2) Statistical optimality for the nonconvex case: A general challenge for optimization methods in the nonconvex setting is the presence of multiple stationary points within the landscape of the loss function. Distributed frameworks in general and potential Byzantine failures within the network in particular make this an even more challenging problem. We overcome this challenge by making use of Assumption 3 (local strong convexity) and aiming for local convergence guarantees.\n\nIn terms of specifics, recall the positive constant \u03b2 from Assumption 3 that describes the region of local strong convexity around a stationary point w * , let \u03b2 1 \u2264 \u03b2 be another positive constant that will be defined shortly, and pick any \u03b2 0 \u2208 (0, \u03b2 \u2212 \u03b2 1 ]. Then our local convergence guarantees are based on the assumption that \u2200j \u2208 R, w j (0)'s are initialized such that v(0) \u2208 B(w * , \u03b2 0 ), with one such choice of initialization being that the w j (0)'s at the nonfaulty nodes are initialized within B(w * , \u03b2 0 ). In particular, assuming \u03b2 \u2265 \u0393 for \u0393 defined in the beginning of Section IV, we obtain the following lemma that establishes the boundedness of the iterates w j (t) for any j \u2208 R and t \u2208 R, characterizes the relationship between \u03b2, \u03b2 0 , and \u03b2 1 , and helps us understand how large \u03b2 need be as a function of the different parameters. Lemma 4. Suppose Assumptions 1, 2, 3 , and 4 are satisfied and the training data are i.i.d. Then with the initialization described above for \u03b2 \u2265 max{\u0393, \u03b2 1 } and \u03b2 1 defined as \u03b2 1 :=\nC2(N ) \u03bb + C3 t0 + C4 t 2 0\n+ C 5 , the w j (t)'s will never escape from B(w * , \u03b2) for all j \u2208 R, t \u2208 R. Here, the constants C 2 , C 3 , and C 4 are as defined in Lemma 2, while the constant\nC 5 := \u221a drC w \u00b5 1 \u03bd + \u221a drL 1 1\u2212\u00b5 1 \u03bd 1 \u03bbt0 \u00b5 1 \u03bd + 1 \u03bb(t0+1) .\nLemma 4 is proved in Appendix E. In terms of the implications of this lemma, it first and foremost helps justify the assumption of bounded iterates of the nonfaulty nodes stated in the beginning of Section IV. More importantly, however, notice that the constraint \u03b2 \u2265 \u0393 and the assumption that \u2200j \u2208 R, t \u2208 R d , w j (t) \u2212 w * \u2264 \u0393 have the potential to make Assumption 3 meaningless for large-enough \u0393. But Lemma 4 effectively helps us characterize the extent of \u0393, i.e., choosing \u0393 to be C 1 + C2(N ) \u03bb + C3 t0 + C4 t 2 0 + C 5 is sufficient to guarantee that all iterates of the nonfaulty nodes remain within B(w * , \u0393) \u2286 B(w * , \u03b2).\n\nWe conclude with our main result for the nonconvex case, which mirrors that for the convex loss functions.\n\nTheorem 3. Suppose Assumptions 1, 2, 3 and 4 are satisfied and the training data are i.i.d. Then with the earlier described initialization within B(w * , \u03b2 0 ), the iterates of BRIDGE-T converge sublinearly in t to the stationary point w * of the statistical risk at each nonfaulty node. In particular, given any > \u03bb > 0, \u2200j \u2208 R, with probability at least 1 \u2212 \u03b4 and for large enough t,\nw j (t + 1) \u2212 w * \u2264 ,(34)where \u03b4 = 2 exp \u2212 4rN 2 16L 2 rd \u03b1m 2 + 2 + r log 12L \u221a rd + d log 12L \u03b2 \u221a d and = C 2 (N ) = O d \u03b1m 2 log 2 \u03b4 N .\nProof of Theorem 3 is provided in Appendix F. It can be seen from the appendix that the proof in the convex setting maps to the nonconvex one without much additional work. The reason for this is the new proof technique being used in the convex case, instead of the one utilized in [86], which guarantees BRIDGE-T converges to a local stationary point for the nonconvex functions described in Assumption 3 . Remark 4. The convergence rates derived for BRIDGE-T are a function of the dimension d. Such dimension dependence is typical of many results in (centralized) statistical learning theory [87], [88]. While there is an ongoing effort to obtain dimension-independent rates in statistical learning [88], [89], we leave an investigation of this within the context of Byzantine-resilient decentralized learning for future work.\n\n\nV. NUMERICAL RESULTS\n\nThe numerical experiments are separated into three parts. In the first part, we run experiments on the MNIST and CIFAR-10 datasets using a linear classifier with squared hinge loss, which is a case that fully satisfies all our assumptions for the theoretical guarantees in the convex setting. In the second part, we run experiments on the MNIST and CIFAR-10 datasets using a convolutional neural network. By showing that BRIDGE works in this general nonconvex loss function setting, we establish that BRIDGE indeed works for loss functions that satisfy Assumption 3 . In the third part, we run experiments on the MNIST dataset with non-i.i.d. distributions of data across the agents. The purpose of all the experiments is to provide numerical validation of our theoretical results and address the usefulness of our Byzantine-resilient technique on a broad scope (convex, nonconvex, non-i.i.d. data) of machine learning problems.\n\n\nA. Linear classifier on MNIST and CIFAR-10\n\nThe first set of experiments is performed to demonstrate two facts: BRIDGE can maintain good performance under Byzantine attacks while classic decentralized learning methods fail; and compared to an existing Byzantine-resilient method, ByRDiE [59], BRIDGE is more efficient in terms of communications cost. We choose one of the most well-understood machine learning tools, the linear classifier with squared hinge loss, to learn the model for this purpose. Note that by showing BRIDGE works in this strictly convex and Lispchitz loss function setting means BRIDGE also works for strongly convex loss functions with bounded Lipschitz gradients.\n\nThe MNIST dataset is a set of 60,000 training images and 10,000 test images of handwritten digits from '0' to '9'. Each image is converted to a 784-dimensional vector and we distribute 60,000 images equally among 50 nodes. The CIFAR-10 dataset is a set of 50,000 training images and 10,000 test images of 10 different classes. Each image is converted to a 3072-dimensional vector and we distribute 50,000 images equally among 50 nodes. Then, unless stated otherwise, we connect each pair of nodes with probability p = 0.5. Some of the nodes are randomly picked to be Byzantine nodes, which broadcast random vectors to all their neighbors during each iteration. The parameter b for BRIDGE is set to be b = 1 in the faultless setting, while it is set to be equal to |B| in the faulty setting. Once a random network is generated and the Byzantine nodes are randomly placed, we check for each variant of BRIDGE whether the minimum neighborhood-size condition listed in Table II for its execution is satisfied before running that variant. The classifiers are trained using the \"one vs all\" strategy. We run five sets of experiments, with the first four on the MNIST dataset and the last one on the CIFAR-10 dataset: Byzantine nodes. The performance is evaluated by two metrics: classification accuracy on the 10,000 test images and whether consensus is achieved. When comparing ByRDiE and BRIDGE, we compare the accuracy with respect to the number of communication iterations, which is defined as the number of scalarvalued information exchanged among the neighboring nodes.\n\nAs we can see from Figure 1, despite using b = 1, the performances of all BRIDGE methods except BRIDGE-B are as good as DGD in the faultless setting with \u223c 88% average accuracy, while BRIDGE-B performs slightly worse Note that these accuracy figures match the state-of-the-art results for the MNIST dataset using a linear classifier [69]. We also attribute the superiority of BRIDGE-T over -M, -K, and -B to its ability to retain information from a wider set of neighbors after the screening in each iteration. Next, we conclude from Figure 2 that DGD fails in the faulty setting when |B| = 2 and produces an even worse accuracy in the faulty setting when |B| = 4. However, BRIDGE-T, -M, -K and -B with b = |B| are able to learn relatively good models in these faulty settings.\n\nThe next set of results in Figure 3 highlights the robustness of the BRIDGE framework to a larger number of Byzantine nodes for varying levels of network connectivity that range from p = 0.5 to p = 1. We exclude BRIDGE-B in this figure since it fails to run for a majority of the randomly generated networks because of the stringent minimum neighborhood size condition (cf. Table II). The results in this figure reaffirm our findings from Figure 2 that the BRIDGE framework is extremely resilient to Byzantine attacks in the network. In particular, we see that BRIDGE-T (when it satisfies the minimum neighborhood size condition) and -M perform very similarly in the face of a large number of Byzantine nodes in   run when b \u2265 18 (resp., b = 24) and p = 0.5 (resp., p = 0.75), while BRIDGE-K could not be run when b = 24 and p = 0.5.\n\nWe next compare BRIDGE-T and ByRDiE in Figure 4. Both BRIDGE-T and ByRDiE are resilient to two Byzantine nodes but since ByRDiE is based on a coordinate-wise screening method, the time it takes to reach the final optimal solution is thousands-fold more than BRIDGE-T. Indeed, since nodes within the BRIDGE framework compute the local gradients and communicate with their neighbors only once per iteration, this leads to massive savings in computation and communications costs in comparison with ByRDiE. This difference in terms of computation and communications costs is even more pronounced in higher-dimensional tasks; thus we will not compare with ByRDiE for the rest of our experiments.\n\nLast but not the least, Figure 5 highlights the robustness of the BRIDGE framework on the higher-dimensional CIFAR-10 dataset for the case of a linear classifier. It can be seen from this figure that the earlier conclusions concerning the different variants of BRIDGE still hold, with BRIDGE-T approaching the state-of-the-art accuracy of \u223c 39% [90] for a linear classifier. We conclude by noting that BRIDGE-B is excluded here and in the following for the experiments on CIFAR-10 dataset because of its relatively high computational complexity on larger-dimensional data.\n\n\nB. Convolutional neural network on MNIST and CIFAR-10\n\nIn the theoretical analysis, we gave local convergence guarantees of BRIDGE-T in the nonconvex setting. In this set of experiments, we numerically show that BRIDGE indeed performs well in the nonconvex case. We train a convolutional neural network (CNN) on MNIST and CIFAR-10 datasets for this purpose, with the model including two convolutional layers followed by two fully connected layers. Each convolutional layer is followed by a max pooling and ReLU activation while the output layer uses softmax activation. We construct a network with 50 nodes and each pair of nodes has probability of 0.5 to be directly connected. Each node has access to 1200 samples randomly picked from the training set. We randomly choose two or four of the nodes to be Byzantine nodes, which broadcast random vectors to all their neighbors during each iteration for all screening methods. We again use the averaged classification accuracy on the MNIST and CIFAR-10 test sets over all nonfaulty nodes as the metric for performance. In this experiment, we cannot compare with ByRDiE due to the fact that the learning model is of high dimension, which makes ByRDiE unfeasible in this setting.\n\nAs we can see from Figure 6, the performances of all BRIDGE methods are as good as DGD in the faultless setting with 92% to 95% percent average accuracy. In Figure 7, we see that DGD fails in the faulty setting when b = 2 and b = 4. But BRIDGE-T, -M, -K, and -B are able to learn a relatively good model in these cases. The final set of results for the CIFAR-10 dataset obtained using BRIDGE-T, -M, and -K for the case of zero, two, and four Byzantine nodes is presented in Figure 8. The top-left quadrant in this figure is reserved for the accuracy of the centralized solution for the chosen CNN architecture. 2 It can be seen that both BRIDGE-T and -M remain resilient to Byzantine attacks and achieve accuracy similar to the centralized solution. However, BRIDGE-K gets stuck in a suboptimal critical point of the loss landscape for the case of two and four Byzantine nodes.\n\n\nC. Non-i.i.d. data distribution on MNIST\n\nIn the theoretical analysis section, we gave convergence gurantees for BRIDGE-T in both convex and nonconvex settings. However, the main results are based on identical and independent distribution (i.i.d.) of the dataset. In this section, we compare our method to the one proposed in [61], which we term \"Byzantine-robust decentralized stochastic optimization\" (BRDSO) in the following discussion based on the terminology used in [61]. We compare BRIDGE-T with BRDSO [61] in the following non-i.i.d. settings.\n\nExtreme non-i.i.d. setting: We group the dataset corresponding to labels and distribute all the samples labelled \"0\" to 5 agents, distribute all the samples labelled \"1\" to another 5 agents, and so on. We can see from Figure 9 that when the number of Byzantine nodes is equal to 0 or 2, the accuracies of both the algorithms are as good as in the i.i.d. case, while in the case when the number of Byzantine nodes is equal to 4, there is about 9 percent of accuracy drop due to the non-i.i.d. distribution of data for both algorithms. In this extreme non-i.i.d. setting when four of the agents are chosen to be Byzantine, the worst-case scenario happens when all the Byzantine nodes are assigned all samples with the same label. This means 80 percent of the samples of one label is not being used towards the training process, which causes both algorithms to underperform compared to the i.i.d. setting.\n\nModerate non-i.i.d. setting: We group the dataset corresponding to labels and distribute the samples associated with each label evenly to 10 agents. Every agent receives two sets of differently labelled data evenly. As we can see from Figure  10, both algorithms perform as well as in the i.i.d. setting in the presence of two or four Byzantine nodes. We conclude that, with distribution closer to i.i.d., the impact of Byzantine nodes in the non-i.i.d. setting will be less.\n\n\nVI. CONCLUSION\n\nThis paper introduced a new decentralized machine learning framework called Byzantine resilient decentralized gradient descent (BRIDGE). This framework was designed to solve machine learning problems when the training set is distributed over a decentralized network in the presence of Byzantine failures. Both theoretical results and experimental results were used to show that the framework could perform well while tolerating Byzantine attacks. One variant of the framework was shown to converge sublinearly to the global minimum in the convex setting and first-order stationary point in the convex setting. In addition, statistical convergence rates were also provided for this variant.\n\nFuture work aims to improve the framework to tolerate more Byzantine agents within the network with faster convergence rates and to deal with more general nonconvex objective  First, we drop P and z in notation of the statistical risk for convenience and observe that for any dimension k we have\nE[g 2 (t)] k = E r j=1 [\u03b1 k (t)] j [\u2207f j (v(t))] k = E[\u2207f (v(t))] k .\nSince k is arbitrary, it then follows that\nE[g 2 (t)] = E[\u2207f (v(t))].(35)\nIn the definition of g 2 (t), notice that v(t) depends on t and \u03b1 k (t) depends on both t and k. We therefore need to show that the convergence of g 2 (t) to E[\u2207f (v(t))] is uniformly over all v(t) and \u03b1 k (t). We fix an arbitrary coordinate k and drop the index k for the rest of this section for simplicity. We next define a vector h(t) as h(t) := [[\u2207f j (v(t))] : j \u2208 R]\n\nand note that g 2 (t) = \u03b1(t), h(t) . Since the training data are i.i.d., h(t) has identically distributed elements. We therefore have from Hoeffding's inequality [91] that for any 0 \u2208 (0, 1):\nP | \u03b1(t), h(t) \u2212 E[\u2207f (v(t))]| \u2265 0 \u2264 2 exp \u2212 2N 2 0 L 2 \u03b1(t) 2 . (36)\nFurther, since the r-dimensional vector \u03b1(t) is an arbitrary element of the standard simplex, defined as \u2206 := {q \u2208 R r : r j=1\n\n[q] j = 1 and \u2200j, [q] j \u2265 0},\n\nthe probability bound in (36) also holds for any q \u2208 \u2206, i.e.,\nP | q, h(t) \u2212 E[\u2207f (v(t))]| \u2265 0 \u2264 2 exp \u2212 2N 0 2 L 2 q 2 .(38)\nWe now define the set S \u03b1 := {\u03b1 k (t)} \u221e,d t,k=1 . Our next goal is to leverage (38) and derive a probability bound similar to (36) that uniformly holds for all q \u2208 S \u03b1 . To this end, let \n\nwhere (a) is due to the triangle and Cauchy-Schwarz inequalities. Trivially, sup q\u2208S\u03b1,c\u2208C \u03be q \u2212 c \u2264 \u03be from the definition of C \u03be , while h(t) \u2264 \u221a rL from the definition of h(t) and Assumption 1. Combining (40) and (41), we get\nP sup q\u2208S\u03b1 | q, h(t) \u2212 E[\u2207f (v(t))]| \u2265 0 + \u221a r\u03beL \u2264 2d \u03be exp \u2212 2N 0 2 L 2 c 2 .(42)\nWe now define \u03b1 m := arg max q\u2208S\u03b1 q . It can then be shown from the definitions of C \u03be andc that\nc 2 \u2264 2( \u03b1 m 2 + \u03be 2 ).(43)\nTherefore, fixing 0 \u2208 (0, 1), and defining := 2 0 and \u03be := /(2L \u221a r), we have from (42) and (43)  Note that (44) is derived for a fixed but arbitrary k. Extending this to the entire vector gives us for any v(t)\nP g 2 (t) \u2212 E[\u2207f (v(t))] \u2265 \u221a d \u2264 2d \u03be exp \u2212 4rN 2 4L 2 r \u03b1 m 2 + 2 . (45)\nTo obtain the desired uniform bound, we next need to remove the dependence on v(t) in (45). Here we drop t from v(t) for simplicity of notation and write g 2 (t) as g 2 (v) to show the dependence of g 2 on v. Notice from our discussion in the beginning of Section IV and the analysis in Section IV-A that v(t) \u2208 V := {v : v \u2264 \u0393 0 } for some \u0393 0 and all t. We then define E \u03b6 := {e 1 , . . . , e m \u03b6 } \u2282 V to be a \u03b6-covering of V in terms of the 2 norm. It then follows from (45) that P sup \n\nFurther, Assumption 1 and definition of the set E \u03b6 imply sup e\u2208E \u03b6 ,v\u2208V g 2 (v) \u2212 g 2 (e) \u2264 L \u03b6.\n\nWe now define := 2 \u221a d and \u03b6 := /4L . We then obtain the following from (45) Since v(t) \u2208 V for all t, we then have\nP sup t g 2 (v(t)) \u2212 E[\u2207f (v(t))] \u2265 \u2264 2d \u03be m \u03b6 exp \u2212 4rN 2 16L 2 rd \u03b1 m 2 + 2 .(50)\nThe proof now follows from (50) and the following facts about the covering numbers of the sets S \u03b1 and V: (1) Since S \u03b1 is a subset of \u2206, which can be circumscribed by a sphere in R r\u22121 of radius r \u2212 1/r < 1, we can upper bound d \u03be by . Then for any \u2208 (0, 1), we have\nsup t g 2 (v(t)) \u2212 E[\u2207f (v(t))] <(51)\nwith probability exceeding\n1 \u2212 2 exp \u2212 4rN 2 16L 2 rd \u03b1 m 2 + 2 + r log 12L \u221a rd + d log 12L \u0393 0 \u221a d .(52)\nEquivalently, we have with probability at least 1 \u2212 \u03b4 that\nsup t g 2 (v(t)) \u2212 E[\u2207f (v(t))] < O 4L 2 d \u03b1 m 2 log 2 \u03b4 N , where \u03b4 = 2 exp \u2212 4rN 2 16L 2 rd \u03b1m 2 + 2 + r log 12L \u221a rd + d log 12L \u03930 \u221a d .\n\nB. Proof of Lemma 2\n\nAll statements in this proof are probabilistic, with probability at least 1 \u2212 \u03b4 and \u03b4 being given in Appendix A. In particular, the discussion should be assumed to have been implicitly conditioned on this high probability event. From (31), we iteratively add up from v(0) to v(t + 1) and get v(t + 1) \u2212 w * \u2264 1 \u2212 1 t + t 0 v(t) \u2212 w * + a 2 (t) + a 3 (t)\n\u2264 1 \u2212 1 t + t 0 1 \u2212 1 t \u2212 1 + t 0 v(t \u2212 1) \u2212 w * + a 2 (t \u2212 1, N ) + a 3 (t \u2212 1) + a 2 (t) + a 3 (t) \u2264 t \u03c4 =0 1 \u2212 1 \u03c4 + t 0 v(0) \u2212 w * + a 2 (t) + a 3 (t) + t\u22121 \u03c4 =0 [a 2 (\u03c4 ) + a 3 (\u03c4 )] t z=\u03c4 +1 1 \u2212 1 z + t 0 .(53)\nThe first term in (53) can be simplified as\nt \u03c4 =0 1 \u2212 1 \u03c4 + t 0 v(0) \u2212 w * = 1 \u2212 1 0 + t 0 1 \u2212 1 1 + t 0 \u00b7 \u00b7 \u00b7 1 \u2212 1 t + t 0 v(0) \u2212 w * = t 0 \u2212 1 t + t 0 C 1 ,(54)\nwhere\nC 1 = v(0) \u2212 w * .\nThe remaining terms in (53) can be simplified as\na 2 (t) + a 3 (t) + t\u22121 \u03c4 =0 [a 2 (\u03c4 ) + a 3 (\u03c4 )] t z=\u03c4 +1 1 \u2212 1 z + t 0 \u2264 1 \u2212 1 t + t 0 a 2 (t \u2212 1) + 1 \u2212 1 t + t 0 1 \u2212 1 t \u2212 1 + t 0 a 2 (t \u2212 2)\n+ a 2 (t) + \u00b7 \u00b7 \u00b7 + 1 \u2212 1 t + t 0 \u00b7 \u00b7 \u00b7 1 \u2212 1 1 + t 0 a 2 (0)\n+ L \u03bb(t + t 0 ) a 4 (t) + 1 \u2212 1 t + t 0 L \u03bb(t \u2212 1 + t 0 )\na 4 (t \u2212 1)\n+ \u00b7 \u00b7 \u00b7 + 1 \u2212 1 t + t 0 \u00b7 \u00b7 \u00b7 1 \u2212 1 1 + t 0 L \u03bb(t 0 ) a 4 (0) = C 2 (N )\u03c1(t) + C 2 (N )\u03c1(t \u2212 1) t + t 0 \u2212 1 t + t 0 + \u00b7 \u00b7 \u00b7 + C 2 (N )\u03c1(0) t 0 t + t 0 + L \u03bb(t + t 0 ) a 4 (t) + t + t 0 \u2212 1 \u03bb(t + t 0 ) 1 t \u2212 1 + t 0 L a 4 (t \u2212 1) + \u00b7 \u00b7 \u00b7 + t 0 \u03bb(t + t 0 ) 1 t 0 L a 4 (0) = t \u03bb(t + t 0 ) C 2 (N ) + L \u03bb 1 t 0 + t\n[a 4 (0) + \u00b7 \u00b7 \u00b7 + a 4 (t)], (55) where\nC 2 (N ) = O d \u03b1m 2 log 2 \u03b4 N .\nThe term a 4 (0)+\u00b7 \u00b7 \u00b7+a 4 (t) in (55) can be further simplified using (22) as: a 4 (0) + \u00b7 \u00b7 \u00b7 + a 4 (t) \u2264 \u221a d rC w + rL\n1 \u03bbt 0 \u00b5 1 \u03bd + \u221a d rC w \u00b5 1 \u03bd + rL 1 \u03bbt 0 \u00b5 2 \u03bd + rL 1 \u03bb(1 + t 0 ) \u00b5 1 \u03bd + \u00b7 \u00b7 \u00b7 + \u221a d rC w \u00b5 t \u03bd + rL 1 \u03bbt 0 \u00b5 t+1 \u03bd + rL 1 \u03bb(1 + t 0 ) \u00b5 t \u03bd + rL 1 \u03bb(2 + t 0 ) \u00b5 t\u22121 \u03bd + \u00b7 \u00b7 \u00b7 + rL 1 \u03bb(t + t 0 ) \u00b5 1 \u03bd \u2264 \u221a d rC w + rC w \u00b5 1 \u03bd + rC w \u00b5 2 \u03bd + \u00b7 \u00b7 \u00b7 + rC w \u00b5 t \u03bd + \u221a drL \u03bb \u00b5 1 \u03bd 1 t 0 + 1 t 0 + 1 + \u00b7 \u00b7 \u00b7 1 t 0 + t + \u221a drL \u03bb \u00b5 2 \u03bd 1 t 0 + 1 t 0 + 1 + \u00b7 \u00b7 \u00b7 + 1 t 0 + t + \u00b7 \u00b7 \u00b7 + + \u221a drL \u03bb \u00b5 t \u03bd 1 t 0 + 1 t 0 + 1 + \u00b7 \u00b7 \u00b7 + 1 t 0 + t = \u221a drC w (1 \u2212 \u00b5 t \u03bd ) 1 \u2212 \u00b5 1 \u03bd + \u221a drL \u03bb \u00b5 1 \u03bd (1 \u2212 \u00b5 t+1 \u03bd ) 1 \u2212 \u00b5 1 \u03bd 1 t 0 + 1 t 0 + 1 + \u00b7 \u00b7 \u00b7 + 1 t 0 + t .(56)\nIV and Lemma 4, and the constraint \u03b2 \u2265 \u0393 imply that the proof of this theorem is a straightforward replication of the proof of Theorem 2 for the convex case.\n\nt 0 )] j = 1 .\n01In particular, we have[84, Theorem 3]    \n\nLemma 2 .\n2Suppose Assumptions 1, 2, 3, and 4 are satisfied and the training data are i.i.d. Then fixing any \u03b4\n\nLemma 3 .\n3Suppose Assumptions 1, 2, 3, and 4 are satisfied and the training data are i.i.d. Then fixing any \u03b4 \u2208 (0, 1) and any > C2(N )\n\nFig. 1 :\n1Comparison between DGD and BRIDGE-T, -M, -K, -B in the faultless setting for a convex loss function and the MNIST dataset, where b is set to be b = 1 for BRIDGE.\n\n\n(i) classic distributed gradient descent (DGD) and BRIDGE-T, -M, -K, -B with no Byzantine nodes; (ii) classic DGD and BRIDGE-T, -M, -K, -B with 2 and 4 Byzantine nodes; (iii) BRIDGE-T, -M, and -K with 6, 12, 18, and 24 Byzantine nodes and varying probabilities of connection (p = 0.5, 0.75, and 1); (iv) ByRDiE and BRIDGE-T with 2 Byzantine nodes; and (v) BRIDGE-T, -M, and -K with 0, 2, 4, and 6\n\nFig. 2 :\n2Comparison between DGD and BRIDGE-T, M, K, B with two and four Byzantine nodes for a convex loss function with the MNIST dataset. at 83% average accuracy (final accuracy: DGD = 87.8%, BRIDGE-T = 87.6%, -M = 87.3%; -K = 86.9%, and -B = 83.1%).\n\nFig. 3 :\n3Comparison between BRIDGE-T, -M, and -K for different numbers of Byzantine nodes and varying levels of network connectivity (convex loss and MNIST dataset). the network, while BRIDGE-K is a close third in performance. Another observation from this figure is the robustness of BRIDGE-M for loosely connected Erd\u0151s-R\u00e9nyi networks and a large number of Byzantine attacks; indeed, BRIDGE-M is the only variant that can be run in the case of b = 24 and p = 0.5 since it always satisfies the minimum neighborhood size condition even when close to 50% of the nodes in the network are Byzantine. In contrast, BRIDGE-T could not be\n\nFig. 4 :\n4Comparing BRIDGE-T with ByRDiE in the presence of two Byzantine nodes (convex loss and MNIST dataset).\n\nFig. 5 :\n5Performance of BRIDGE-T, -M, and -K with zero, two, four, and six Byzantine nodes for a convex loss function with the CIFAR-10 dataset.\n\nFig. 6 :\n6Comparison between DGD and BRIDGE-T, -M, -K, and -B in the faultless setting for a nonconvex loss function with the MNIST dataset.\n\nFig. 7 :\n7Comparison between DGD and BRIDGE-T, -M, -K, and -B with two and four Byzantine nodes for nonconvex loss with the MNIST dataset.\n\nFig. 8 :\n8Performance comparison of BRIDGE-T, -M, and -K with zero, two, and four Byzantine nodes for nonconvex loss with the CIFAR-10 dataset.\n\nFig. 9 :\n9Comparing BRIDGE-T with BRDSO[61] in the extreme non-i.i.d. setting (convex loss and MNIST dataset).\n\nFig. 10 :\n10Comparing BRIDGE-T with BRDSO[61] in the moderate non-i.i.d. setting (convex loss and MNIST dataset) functions with either i.i.d. or non-i.i.d. distribution of the dataset. In addition, an exploration of the number of Byzantine nodes that this framework can tolerate under different kinds of non-i.i.d. settings will also be undertaken in future work.APPENDIXA. Proof of Lemma 1\n\n0 \u2264\n0C \u03be := {c 1 , . . . , c d \u03be } \u2282 \u2206 s.t. S \u03b1 \u2286 d \u03be q=1 B(c q , \u03be) (39)denote a \u03be-covering of S \u03b1 in terms of the 2 norm and defin\u0113 c := arg max c\u2208C \u03be c . Then from(38) and the union boundP sup c\u2208C \u03be | c, h(t) \u2212 E[\u2207f (v(t))]| \u2265 2d , h(t) \u2212 E[\u2207f (v(t))]| ,\n\n\u2264\n2d \u03be m \u03b6 exp \u2212 4rN 2 4L 2 r \u03b1 m 2 + 2 . (46)Similar to(41), we can also writesup v\u2208V g 2 (v) \u2212 E[\u2207f (v) | \u2264 sup e\u2208E \u03b6 g 2 (e) \u2212 E[\u2207f (e)] + sup e\u2208E \u03b6 ,v\u2208V g 2 (v) \u2212 g 2 (e) + E[\u2207f (e)] \u2212 E[\u2207f (v)] .\n\n\n(v) \u2212 E[\u2207f (v)] \u2265 \u2264 2d \u03be m \u03b6 exp \u2212 4rN 2 16L 2 rd \u03b1 m 2 + 2 . (49)\n\n\nwaheed.bajwa@rutgers.edu) are with the Department of Electrical and Computer Engineering, Rutgers University-New Brunswick, NJ 08854. Z. Yang (zhixiong.yang@bluedanube.com) completed this work as part of his PhD dissertation at Rutgers University; he is now a Machine Learning Systems Engineer at Blue Danube Systems. This work is supported in part by the National Science Foundation under awards CCF-1453073, CCF-1907658, and OAC-1940074, and by the Army Research Office under award W911NF2110301.. \nFang \n(cf446@soe.rutgers.edu) \nand \nW.U. \nBajwa \n(\n\nTABLE I :\nIComparison of BRIDGE with different vector-valued decentralized learning/optimization methods in the literature.\n\nTABLE II :\nIIComparison between the four different variants of the BRIDGE framework.sensus and distributed learning do not translate into equivalent \nresults for Byzantine-resilient decentralized learning. \n\n\nthat P sup\nPq\u2208S\u03b1 | q, h(t) \u2212 E[\u2207f (v(t))]| \u2265 \u2264 2d \u03be exp \u2212 4rN 4L 2 r \u03b1 m 2 + 2 . (44)2 \n\n\n\n\n[92]; and (2) Since V \u2282 R d can be circumscribed by a sphere in R d of radius \u0393 0 \u221a d, we can upper bound m \u03b612L \n\u221a \nrd \nr \n\nby 12L \u03930 \n\n\u221a \nd \nd \n\n\nNote that a better CIFAR-10 accuracy can be obtained through a fine tuning of the CNN architecture and the step size sequence.\nPlugging(56)into(55)we finish the simplification of the remaining terms as:Finally we express v(t + 1) \u2212 w * using(54)and(57)as:whereC. Proof of Lemma 3Similar to the proof of Lemma 2, our statements in this appendix are also conditioned on the high probability event described in Appendix A. From Theorem 1 and (58) in the proof of Lemma 2, we conclude for all j \u2208 R thatNext, we get an upper bound on a 4 (t + 1) as following when we choose t as an even number:When t is an odd number, we have:and the remaining steps are similar to (60), so we omit them. Plugging either(60)or(61)into(59), we have for all j \u2208 RFrom(62)we see that all the terms except C2(N ) \u03bb are monotonically decreasing with increasing t. Thus, given any > C2(N ) \u03bb > 0, we can find a t 1 such that for all t \u2265 t 1 , with probability at least 1 \u2212 \u03b4, w j (t + 1) \u2212 w * \u2264 .D. Proof of Theorem 2Lemma 3 establishes the convergence of w j (t+1) to w * for all j \u2208 R with probability at least 1 \u2212 \u03b4. Next, we derive the rate of convergence. From (59) since a 4 (t) has a convergence rate of O(1/t), as mentioned in Theorem 1, the term that converges the slowest to 0 among all the terms in w j (t+1)\u2212w * is C4 t+t0 1 t0 + 1 1+t0 +\u00b7 \u00b7 \u00b7+ 1 t+t0 . Therefore, using the harmonic series approximation, we conclude that the convergence rate is O log t t . The above statement shows BRIDGE-T converges to the minimum of the global statistical risk at a sublinear rate, thus completing the proof of Theorem 2.E. Proof of Lemma 4Under the stated assumptions of the lemma as well as the initialization for the nonconvex loss function, it is straightforward to see that (62) also holds for the nonconvex setting. In particular, the upper bound in (62) on w j (t+1)\u2212w * for all j \u2208 R monotonically decreases with t. Thus, w j (t+1)\u2212w * for all j \u2208 R, t \u2208 R can be upper bounded by the bound onThe proof now follows from the fact that C 1 := v(0)\u2212w * \u2264 \u03b2 0 by virtue of the initialization.F. Proof of Theorem 3The local strong convexity of the loss function implies that f (w, z) can be treated as \u03bb-strongly convex when restricted to the ball B(w * , \u03b2). Therefore, Assumption 3 , the \u0393boundedness of the iterates stated in the beginning of Section\nV Vapnik, The Nature of Statistical Learning Theory. New York, NYSpringer-Verlag2nd edV. Vapnik, The Nature of Statistical Learning Theory, 2nd ed. New York, NY: Springer-Verlag, 1999.\n\nMachine learning in automated text categorization. F Sebastiani, ACM Computing Surveys. 341F. Sebastiani, \"Machine learning in automated text categorization,\" ACM Computing Surveys, vol. 34, no. 1, pp. 1-47, 2002.\n\nSupervised machine learning: A review of classification techniques. S B Kotsiantis, I Zaharakis, P Pintelas, Emerging Artificial Intell. Applicat. Comput. Eng. 160S. B. Kotsiantis, I. Zaharakis, and P. Pintelas, \"Supervised machine learning: A review of classification techniques,\" Emerging Artificial Intell. Applicat. Comput. Eng., vol. 160, pp. 3-24, 2007.\n\nLearning deep architectures for AI. Y Bengio, Found. and Trends Mach. Learning. 21Y. Bengio, \"Learning deep architectures for AI,\" Found. and Trends Mach. Learning, vol. 2, no. 1, pp. 1-127, 2009.\n\nM Mohri, A Rostamizadeh, A Talwalkar, Foundations of Machine Learning. Cambridge, MAMIT Press2nd edM. Mohri, A. Rostamizadeh, and A. Talwalkar, Foundations of Machine Learning, 2nd ed. Cambridge, MA: MIT Press, 2018.\n\nStatistical Machine Learning: A Unified Framework. R M Golden, Chapman and Hall/CRC2020Boca Raton, FLR. M. Golden, Statistical Machine Learning: A Unified Framework. Boca Raton, FL: Chapman and Hall/CRC, 2020.\n\nAdversary-resilient distributed and decentralized statistical inference and machine learning: An overview of recent advances under the Byzantine threat model. Z Yang, A Gang, W U Bajwa, IEEE Signal Process. Mag. 373Z. Yang, A. Gang, and W. U. Bajwa, \"Adversary-resilient distributed and decentralized statistical inference and machine learning: An overview of recent advances under the Byzantine threat model,\" IEEE Signal Process. Mag., vol. 37, no. 3, pp. 146-159, May 2020.\n\nScaling-up distributed processing of data streams for machine learning. M Nokleby, H Raja, W U Bajwa, Proceedings of the IEEE. 10811M. Nokleby, H. Raja, and W. U. Bajwa, \"Scaling-up distributed process- ing of data streams for machine learning,\" Proceedings of the IEEE, vol. 108, no. 11, pp. 1984-2012, 2020.\n\nDistributed learning in wireless sensor networks. J B Predd, S B Kulkarni, H V Poor, IEEE Signal Process. Mag. 234J. B. Predd, S. B. Kulkarni, and H. V. Poor, \"Distributed learning in wireless sensor networks,\" IEEE Signal Process. Mag., vol. 23, no. 4, pp. 56-69, 2006.\n\nDistributed optimization and statistical learning via the alternating direction method of multipliers. S Boyd, N Parikh, E Chu, B Peleato, J Eckstein, Found. and Trends Mach. Learning. 31S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, \"Distributed optimization and statistical learning via the alternating direction method of multipliers,\" Found. and Trends Mach. Learning, vol. 3, no. 1, pp. 1-122, 2011.\n\nAdaptation, learning, and optimization over networks. A H Sayed, Found. and Trends Mach. Learning. 74-5A. H. Sayed, \"Adaptation, learning, and optimization over networks,\" Found. and Trends Mach. Learning, vol. 7, no. 4-5, pp. 311-801, 2014.\n\nNetwork topology and communication-computation tradeoffs in decentralized optimization. A Nedi\u0107, A Olshevsky, M G Rabbat, Proceedings of the IEEE. 1065A. Nedi\u0107, A. Olshevsky, and M. G. Rabbat, \"Network topology and communication-computation tradeoffs in decentralized optimization,\" Proceedings of the IEEE, vol. 106, no. 5, pp. 953-976, 2018.\n\nDecentralized federated averaging. T Sun, D Li, B Wang, arXiv:2104.11375arXiv preprintT. Sun, D. Li, and B. Wang, \"Decentralized federated averaging,\" arXiv preprint arXiv:2104.11375, 2021.\n\nByzantine fault tolerance, from theory to reality. K Driscoll, B Hall, H Sivencrona, P Zumsteq, Proc. Int. Conf. Computer Safety, Reliability, and Security (SAFECOMP'03). Int. Conf. Computer Safety, Reliability, and Security (SAFECOMP'03)K. Driscoll, B. Hall, H. Sivencrona, and P. Zumsteq, \"Byzantine fault tolerance, from theory to reality,\" in Proc. Int. Conf. Computer Safety, Reliability, and Security (SAFECOMP'03), 2003, pp. 235-248.\n\nFault-tolerant multi-agent optimization: Optimal iterative distributed algorithms. L Su, N H Vaidya, Proc. ACM Symp. Principles of Distributed Computing. ACM Symp. Principles of Distributed ComputingL. Su and N. H. Vaidya, \"Fault-tolerant multi-agent optimization: Optimal iterative distributed algorithms,\" in Proc. ACM Symp. Principles of Distributed Computing, 2016, pp. 425-434.\n\nThe Byzantine generals problem. L Lamport, R Shostak, M Pease, ACM Trans. Programming Languages and Syst. 43L. Lamport, R. Shostak, and M. Pease, \"The Byzantine generals prob- lem,\" ACM Trans. Programming Languages and Syst., vol. 4, no. 3, pp. 382-401, 1982.\n\nBest-case complexity of asynchronous Byzantine consensus. P Dutta, R Guerraoui, M Vukolic, Tech. Rep. P. Dutta, R. Guerraoui, and M. Vukolic, \"Best-case complexity of asyn- chronous Byzantine consensus,\" EPFL/IC/200499, Tech. Rep., 2005.\n\nFrom Byzantine consensus to BFT state machine replication: A latency-optimal transformation. J Sousa, A Bessani, Proc. 9th. 9thJ. Sousa and A. Bessani, \"From Byzantine consensus to BFT state machine replication: A latency-optimal transformation,\" in Proc. 9th\n\nEuro. Dependable Computing Conf.(EDCC'12). Euro. Dependable Computing Conf.(EDCC'12), 2012, pp. 37-48.\n\nScaling distributed machine learning with the parameter server. M Li, D G Andersen, J W Park, A J Smola, A Ahmed, V Josifovski, J Long, E J Shekita, B.-Y. Su, Proc. 11th USENIX Symp. Operating Systems Design and Implementation (OSDI'14). 11th USENIX Symp. Operating Systems Design and Implementation (OSDI'14)Broomfield, COM. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed, V. Josifovski, J. Long, E. J. Shekita, and B.-Y. Su, \"Scaling distributed machine learning with the parameter server,\" in Proc. 11th USENIX Symp. Operating Systems Design and Implementation (OSDI'14), Broomfield, CO, Oct. 2014, pp. 583-598.\n\nFederated learning: Strategies for improving communication efficiency. J Kone\u010dn\u00fd, H B Mcmahan, F X Yu, P Richtarik, A T Suresh, D Bacon, Proc. NeurIPS Workshop on Private Multi-Party Machine Learning. NeurIPS Workshop on Private Multi-Party Machine LearningJ. Kone\u010dn\u00fd, H. B. McMahan, F. X. Yu, P. Richtarik, A. T. Suresh, and D. Bacon, \"Federated learning: Strategies for improving communication efficiency,\" in Proc. NeurIPS Workshop on Private Multi-Party Machine Learning, 2016.\n\nMachine learning with adversaries: Byzantine tolerant gradient descent. P Blanchard, R Guerraoui, J Stainer, Proc. Advances in Neural Inf. Process. Syst. Advances in Neural Inf. ess. SystP. Blanchard, R. Guerraoui, and J. Stainer, \"Machine learning with adversaries: Byzantine tolerant gradient descent,\" in Proc. Advances in Neural Inf. Process. Syst., 2017, pp. 118-128.\n\nDRACO: Byzantine-resilient distributed training via redundant gradients. L Chen, H Wang, Z Charles, D Papailiopoulos, Proc. 35th Intl. Conf. Machine Learning (ICML). 35th Intl. Conf. Machine Learning (ICML)L. Chen, H. Wang, Z. Charles, and D. Papailiopoulos, \"DRACO: Byzantine-resilient distributed training via redundant gradients,\" in Proc. 35th Intl. Conf. Machine Learning (ICML), 2018, pp. 903-912.\n\nRobust distributed gradient descent with arbitrary number of Byzantine attackers. X Cao, L Lai, Proc. IEEE Int. Conf. Acoust. Speech and Signal Process. (ICASSP'19). IEEE Int. Conf. Acoust. Speech and Signal ess. (ICASSP'19)X. Cao and L. Lai, \"Robust distributed gradient descent with arbitrary number of Byzantine attackers,\" in Proc. IEEE Int. Conf. Acoust. Speech and Signal Process. (ICASSP'19), 2018, pp. 6373-6377.\n\nSecuring distributed machine learning in high dimensions. L Su, J Xu, arXiv:1804.10140arXiv preprintL. Su and J. Xu, \"Securing distributed machine learning in high dimensions,\" arXiv preprint arXiv:1804.10140, 2018.\n\nDefending against saddle point attack in Byzantine-robust distributed learning. D Yin, Y Chen, R Kannan, P Bartlett, Proc. 36th Intl. Conf. Machine Learning. 36th Intl. Conf. Machine LearningD. Yin, Y. Chen, R. Kannan, and P. Bartlett, \"Defending against saddle point attack in Byzantine-robust distributed learning,\" in Proc. 36th Intl. Conf. Machine Learning, Jun. 2019, pp. 7074-7084.\n\nAsynchronous Byzantine machine learning (the case of SGD). G Damaskinos, E E Mhamdi, R Guerraoui, R Patra, M Taziki, Proc. 35th Int. Conf. Machine Learning. 35th Int. Conf. Machine LearningG. Damaskinos, E. E. Mhamdi, R. Guerraoui, R. Patra, and M. Taziki, \"Asynchronous Byzantine machine learning (the case of SGD),\" in Proc. 35th Int. Conf. Machine Learning, 2018, pp. 1145-1154.\n\nThe hidden vulnerability of distributed learning in Byzantium. E E Mhamdi, R Guerraoui, S Rouault, Proc. 35th Int. Conf. Machine Learning. 35th Int. Conf. Machine LearningE. E. Mhamdi, R. Guerraoui, and S. Rouault, \"The hidden vulnerability of distributed learning in Byzantium,\" in Proc. 35th Int. Conf. Machine Learning, 2018, pp. 3521-3530.\n\nByzantine-robust distributed learning: Towards optimal statistical rates. D Yin, Y Chen, K Ramchandran, P Bartlett, Proc. 35th Intl. Conf. Machine Learning. 35th Intl. Conf. Machine LearningD. Yin, Y. Chen, K. Ramchandran, and P. Bartlett, \"Byzantine-robust distributed learning: Towards optimal statistical rates,\" in Proc. 35th Intl. Conf. Machine Learning, Jul. 2018, pp. 5650-5659.\n\nByzantine stochastic gradient descent. D Alistarh, Z Allen-Zhu, J Li, Proc. Advances in Neural Information Processing Systems. Advances in Neural Information essing SystemsD. Alistarh, Z. Allen-Zhu, and J. Li, \"Byzantine stochastic gradient descent,\" in Proc. Advances in Neural Information Processing Systems, 2018, pp. 4618-4628.\n\nZeno: Byzantine-suspicious stochastic gradient descent. C Xie, O Koyejo, I Gupta, arXiv:1805.10032arXiv preprintC. Xie, O. Koyejo, and I. Gupta, \"Zeno: Byzantine-suspicious stochastic gradient descent,\" arXiv preprint arXiv:1805.10032, 2018.\n\narXiv:1802.10116Generalized Byzantine-tolerant SGD. arXiv preprint--, \"Generalized Byzantine-tolerant SGD,\" arXiv preprint arXiv:1802.10116, 2018.\n\nPhocas: Dimensional Byzantine-resilient stochastic gradient descent. arXiv:1805.09682arXiv preprint--, \"Phocas: Dimensional Byzantine-resilient stochastic gradient de- scent,\" arXiv preprint arXiv:1805.09682, 2018.\n\nDistributed training with heterogeneous data: Bridging median-and mean-based algorithms. X Chen, T Chen, H Sun, S Wu, M Hong, Proc. Advances in Neural Information Processing Systems. Advances in Neural Information essing Systems21X. Chen, T. Chen, H. Sun, S. Wu, and M. Hong, \"Distributed training with heterogeneous data: Bridging median-and mean-based algorithms,\" in Proc. Advances in Neural Information Processing Systems, 2020, pp. 21 616-21 626.\n\nDETOX: A redundancy-based framework for faster and more robust gradient aggregation. S Rajput, H Wang, Z Charles, D Papailiopoulos, Proc. Advances in Neural Information Processing Systems. Advances in Neural Information essing SystemsS. Rajput, H. Wang, Z. Charles, and D. Papailiopoulos, \"DETOX: A redundancy-based framework for faster and more robust gradient aggre- gation,\" in Proc. Advances in Neural Information Processing Systems, 2019.\n\nRSA: Byzantinerobust stochastic aggregation methods for distributed learning from heterogeneous datasets. L Li, W Xu, T Chen, G Giannakis, Q Ling, Proc. AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence33L. Li, W. Xu, T. Chen, G. Giannakis, and Q. Ling, \"RSA: Byzantine- robust stochastic aggregation methods for distributed learning from heterogeneous datasets,\" in Proc. AAAI Conference on Artificial Intelli- gence, vol. 33, 2019, pp. 1544-1551.\n\nDistributed Byzantine tolerant stochastic gradient descent in the era of big data. R Jin, X He, H Dai, Proc. IEEE Intl. Conf. Communications (ICC). IEEE Intl. Conf. Communications (ICC)R. Jin, X. He, and H. Dai, \"Distributed Byzantine tolerant stochastic gradient descent in the era of big data,\" in Proc. IEEE Intl. Conf. Communications (ICC), 2019, pp. 1-6.\n\nByzantine-resilient distributed largescale matrix completion. F Lin, Q Ling, Z Xiong, Proc. IEEE Int. Conf. Acoust. Speech and Signal Process. (ICASSP'19). IEEE Int. Conf. Acoust. Speech and Signal ess. (ICASSP'19)F. Lin, Q. Ling, and Z. Xiong, \"Byzantine-resilient distributed large- scale matrix completion,\" in Proc. IEEE Int. Conf. Acoust. Speech and Signal Process. (ICASSP'19), 2019, pp. 8167-8171.\n\nRobust federated learning in a heterogeneous environment. A Ghosh, J Hong, D Yin, K Ramchandran, arXiv:1906.06629arXiv preprintA. Ghosh, J. Hong, D. Yin, and K. Ramchandran, \"Robust fed- erated learning in a heterogeneous environment,\" arXiv preprint arXiv:1906.06629, 2019.\n\nData encoding for Byzantineresilient distributed optimization. D Data, L Song, S N Diggavi, IEEE Transactions on Information Theory. 672D. Data, L. Song, and S. N. Diggavi, \"Data encoding for Byzantine- resilient distributed optimization,\" IEEE Transactions on Information Theory, vol. 67, no. 2, pp. 1117-1140, 2021.\n\nSGD: Decentralized Byzantine resilience. E M E Mhamdi, R Guerraoui, A Guirguis, S Rouault, arXiv:1905.03853arXiv preprintE. M. E. Mhamdi, R. Guerraoui, A. Guirguis, and S. Rouault, \"SGD: Decentralized Byzantine resilience,\" arXiv preprint arXiv:1905.03853, 2019.\n\nZeno++: Robust fully asynchronous SGD. C Xie, S Koyejo, I Gupta, Proc. 37th Intl. Conf. Machine Learning. 37th Intl. Conf. Machine Learning10C. Xie, S. Koyejo, and I. Gupta, \"Zeno++: Robust fully asynchronous SGD,\" in Proc. 37th Intl. Conf. Machine Learning, Jul. 2020, pp. 10 495- 10 503.\n\nFast and robust distributed learning in high dimension. E.-M El-Mhamdi, R Guerraoui, S Rouault, arXiv:1905.04374arXiv preprintE.-M. El-Mhamdi, R. Guerraoui, and S. Rouault, \"Fast and robust dis- tributed learning in high dimension,\" arXiv preprint arXiv:1905.04374, 2019.\n\nDistributed subgradient methods for multiagent optimization. A Nedic, A Ozdaglar, IEEE Trans. Autom. Control. 541A. Nedic and A. Ozdaglar, \"Distributed subgradient methods for multi- agent optimization,\" IEEE Trans. Autom. Control, vol. 54, no. 1, pp. 48-61, 2009.\n\nDistributed stochastic subgradient projection algorithms for convex optimization. S S Ram, A Nedi\u0107, V Veeravalli, J. Optim. Theory and Appl. 1473S. S. Ram, A. Nedi\u0107, and V. Veeravalli, \"Distributed stochastic subgra- dient projection algorithms for convex optimization,\" J. Optim. Theory and Appl., vol. 147, no. 3, pp. 516-545, 2010.\n\nDistributed optimization over time-varying directed graphs. A Nedi\u0107, A Olshevsky, IEEE Trans. Autom. Control. 603A. Nedi\u0107 and A. Olshevsky, \"Distributed optimization over time-varying directed graphs,\" IEEE Trans. Autom. Control, vol. 60, no. 3, pp. 601- 615, 2015.\n\nDistributed stochastic gradient tracking methods. S Pu, A Nedi\u0107, Mathematical Programming. 187S. Pu and A. Nedi\u0107, \"Distributed stochastic gradient tracking methods,\" Mathematical Programming, vol. 187, pp. 409-457, 2021.\n\nConsensus-based distributed support vector machines. P A Forero, A Cano, G B Giannakis, J. Mach. Learning Research. 11P. A. Forero, A. Cano, and G. B. Giannakis, \"Consensus-based dis- tributed support vector machines,\" J. Mach. Learning Research, vol. 11, pp. 1663-1707, 2010.\n\nD-ADMM: A communication-efficient distributed algorithm for separable optimization. J F Mota, J M Xavier, P M Aquiar, M Puschel, IEEE Trans. Signal Process. 6110J. F. Mota, J. M. Xavier, P. M. Aquiar, and M. Puschel, \"D-ADMM: A communication-efficient distributed algorithm for separable optimiza- tion,\" IEEE Trans. Signal Process., vol. 61, no. 10, pp. 2718-2723, 2013.\n\nOn the linear convergence of the ADMM in decentralized consensus optimization. W Shi, Q Ling, K Yuan, G Wu, W Yin, IEEE Trans. Signal Process. 627W. Shi, Q. Ling, K. Yuan, G. Wu, and W. Yin, \"On the linear convergence of the ADMM in decentralized consensus optimization,\" IEEE Trans. Signal Process., vol. 62, no. 7, pp. 1750-1761, 2014.\n\nA decentralized second-order method with exact linear convergence rate for consensus optimization. A Mokhtari, W Shi, Q Ling, A Ribeiro, IEEE Trans. Signal Inf. Process. Netw. 24A. Mokhtari, W. Shi, Q. Ling, and A. Ribeiro, \"A decentralized second-order method with exact linear convergence rate for consensus optimization,\" IEEE Trans. Signal Inf. Process. Netw., vol. 2, no. 4, pp. 507-522, 2016.\n\nNetwork Newton distributed optimization methods. A Mokhtari, Q Ling, A Ribeiro, IEEE Trans. Signal Process. 651A. Mokhtari, Q. Ling, and A. Ribeiro, \"Network Newton distributed optimization methods,\" IEEE Trans. Signal Process., vol. 65, no. 1, pp. 146-161, 2017.\n\nResilient asymptotic consensus in robust networks. H J Leblanc, H Zhang, X Koutsoukos, S Sundaram, IEEE J. Sel. Areas in Commun. 314H. J. LeBlanc, H. Zhang, X. Koutsoukos, and S. Sundaram, \"Resilient asymptotic consensus in robust networks,\" IEEE J. Sel. Areas in Com- mun., vol. 31, no. 4, pp. 766-781, 2013.\n\nIterative Byzantine vector consensus in incomplete graphs. N H Vaidya, L Tseng, G Liang, Proc. 15th Int. Conf. Distributed Computing and Networking. 15th Int. Conf. Distributed Computing and NetworkingN. H. Vaidya, L. Tseng, and G. Liang, \"Iterative Byzantine vector consensus in incomplete graphs,\" in Proc. 15th Int. Conf. Distributed Computing and Networking, 2014, pp. 14-28.\n\nDistributed optimization under adversarial nodes. S Sundaram, B Gharesifard, IEEE Trans. Autom. Control. 643S. Sundaram and B. Gharesifard, \"Distributed optimization under ad- versarial nodes,\" IEEE Trans. Autom. Control, vol. 64, no. 3, pp. 1063- 1076, 2019.\n\nRD-SVM: A resilient distributed support vector machine. Z Yang, W U Bajwa, Proc. IEEE Int. Conf. Acoust. Speech and Signal Process. (ICASSP'16). IEEE Int. Conf. Acoust. Speech and Signal ess. (ICASSP'16)Z. Yang and W. U. Bajwa, \"RD-SVM: A resilient distributed support vector machine,\" in Proc. IEEE Int. Conf. Acoust. Speech and Signal Process. (ICASSP'16), 2016, pp. 2444-2448.\n\nRobust decentralized dynamic optimization at presence of malfunctioning agents. W Xu, Z Li, Q Ling, Signal Process. 153W. Xu, Z. Li, and Q. Ling, \"Robust decentralized dynamic optimization at presence of malfunctioning agents,\" Signal Process., vol. 153, pp. 24-33, 2018.\n\nResilient distributed state estimation with mobile agents: Overcoming Byzantine adversaries, communication losses, and intermittent measurements. A Mitra, J Richards, S Bagchi, S Sundaram, Autonomous Robots. 433A. Mitra, J. Richards, S. Bagchi, and S. Sundaram, \"Resilient distributed state estimation with mobile agents: Overcoming Byzantine adversaries, communication losses, and intermittent measurements,\" Autonomous Robots, vol. 43, no. 3, pp. 743-768, 2019.\n\nFinite-time guarantees for Byzantineresilient distributed state estimation with noisy measurements. L Su, S Shahrampour, IEEE Transactions on Automatic Control. 659L. Su and S. Shahrampour, \"Finite-time guarantees for Byzantine- resilient distributed state estimation with noisy measurements,\" IEEE Transactions on Automatic Control, vol. 65, no. 9, pp. 3758-3771, 2020.\n\nByRDiE: Byzantine-resilient distributed coordinate descent for decentralized learning. Z Yang, W U Bajwa, IEEE Trans. Signal Inf. Process. Netw. 54Z. Yang and W. U. Bajwa, \"ByRDiE: Byzantine-resilient distributed coordinate descent for decentralized learning,\" IEEE Trans. Signal Inf. Process. Netw., vol. 5, no. 4, pp. 611-627, Dec. 2019.\n\nByzantine-resilient distributed optimization of multi-dimensional functions. K Kuwaranancharoen, L Xin, S Sundaram, Proc. American Control Conference (ACC). American Control Conference (ACC)K. Kuwaranancharoen, L. Xin, and S. Sundaram, \"Byzantine-resilient distributed optimization of multi-dimensional functions,\" in Proc. Amer- ican Control Conference (ACC), 2020, pp. 4399-4404.\n\nByzantine-robust decentralized stochastic optimization over static and time-varying networks. J Peng, W Li, Q Ling, Signal Processing. 183108020J. Peng, W. Li, and Q. Ling, \"Byzantine-robust decentralized stochastic optimization over static and time-varying networks,\" Signal Processing, vol. 183, p. 108020, 2021.\n\nTowards Byzantine-resilient learning in decentralized systems. S Guo, T Zhang, X Xie, L Ma, T Xiang, Y Liu, arXiv:2002.08569arXiv preprintS. Guo, T. Zhang, X. Xie, L. Ma, T. Xiang, and Y. Liu, \"Towards Byzantine-resilient learning in decentralized systems,\" arXiv preprint arXiv:2002.08569, 2020.\n\nCollaborative learning as an agreement problem. E.-M El-Mhamdi, R Guerraoui, A Guirguis, L Hoang, S Rouault, arXiv:2008.00742v3arXiv preprintE.-M. El-Mhamdi, R. Guerraoui, A. Guirguis, L. Hoang, and S. Rouault, \"Collaborative learning as an agreement problem,\" arXiv preprint arXiv:2008.00742v3, 2020.\n\nNext: In-network nonconvex optimization. P D Lorenzo, G Scutari, IEEE Transactions on Signal and Information Processing over Networks. 2P. D. Lorenzo and G. Scutari, \"Next: In-network nonconvex optimiza- tion,\" IEEE Transactions on Signal and Information Processing over Networks, vol. 2, pp. 120-136, 2016.\n\nOn nonconvex decentralized gradient descent. J Zeng, W Yin, IEEE Transactions on Signal Processing. 66J. Zeng and W. Yin, \"On nonconvex decentralized gradient descent,\" IEEE Transactions on Signal Processing, vol. 66, pp. 2834-2848, 2018.\n\nImproving the sample and communication complexity for decentralized non-convex optimization: Joint gradient estimation and tracking. H Sun, S Lu, M Hong, Proc. 37th Intl. Conf. Machine Learning. 37th Intl. Conf. Machine LearningH. Sun, S. Lu, and M. Hong, \"Improving the sample and communication complexity for decentralized non-convex optimization: Joint gradient estimation and tracking,\" in Proc. 37th Intl. Conf. Machine Learning, Jul. 2020, pp. 9217-9228.\n\nFast decentralized non-convex finitesum optimization with recursive variance reduction. R Xin, U A Khan, S Kar, arXiv:2008.07428arXiv preprintR. Xin, U. A. Khan, and S. Kar, \"Fast decentralized non-convex finite- sum optimization with recursive variance reduction,\" arXiv preprint arXiv:2008.07428, 2020.\n\nByzantine-robust decentralized learning via self-centered clipping. L He, S P Karimireddy, M Jaggi, arXiv:2202.01545arXiv preprintL. He, S. P. Karimireddy, and M. Jaggi, \"Byzantine-robust decentralized learning via self-centered clipping,\" arXiv preprint arXiv:2202.01545, 2022.\n\nGradient-based learning applied to document recognition. Y Lecun, L Bottou, Y Bengio, P Haffner, Proceedings of the IEEE. 8611Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner, \"Gradient-based learning applied to document recognition,\" Proceedings of the IEEE, vol. 86, no. 11, pp. 2278-2324, 1998.\n\nLearning multiple layers of features from tiny images. A Krizhevsky, G Hinton, A. Krizhevsky and G. Hinton, \"Learning multiple layers of features from tiny images,\" 2009.\n\nH H Sohrab, Basic Real Analysis. New York, NYSpringer2nd edH. H. Sohrab, Basic Real Analysis, 2nd ed. New York, NY: Springer, 2003.\n\nWhen are nonconvex problems not scary. J Sun, Q Qu, J Wright, arXiv:1510.06096arXiv preprintJ. Sun, Q. Qu, and J. Wright, \"When are nonconvex problems not scary?\" arXiv preprint arXiv:1510.06096, 2015.\n\nNon-convex optimization for machine learning. P Jain, P Kar, Foundations and Trends in Machine Learning. 10P. Jain and P. Kar, \"Non-convex optimization for machine learning,\" Foundations and Trends in Machine Learning, vol. 10, no. 3-4, p. 142-336, 2017.\n\nA deterministic theory for exact non-convex phase retrieval. B Yonel, B Yazici, IEEE Transactions on Signal Processing. 68B. Yonel and B. Yazici, \"A deterministic theory for exact non-convex phase retrieval,\" IEEE Transactions on Signal Processing, vol. 68, pp. 4612-4626, 2020.\n\nGeometrical properties and accelerated gradient solvers of non-convex phase retrieval. Y Zhou, H Zhang, Y Liang, Proc. 54th. 54thY. Zhou, H. Zhang, and Y. Liang, \"Geometrical properties and accel- erated gradient solvers of non-convex phase retrieval,\" in Proc. 54th\n\nAnnu, Allerton, Conf. Communication, Control, and Computing. Annu. Allerton Conf. Communication, Control, and Computing, 2016, pp. 331-335.\n\nLocal convergence of inexact Newton methods. T Ypma, SIAM Journal on Numerical Analysis. 213T. Ypma, \"Local convergence of inexact Newton methods,\" SIAM Journal on Numerical Analysis, vol. 21, no. 3, pp. 583-590, 1984.\n\nLocal convergence of the heavy-ball method and iPiano for non-convex optimization. P Ochs, Journal of Optimization Theory and Applications. 1771P. Ochs, \"Local convergence of the heavy-ball method and iPiano for non-convex optimization,\" Journal of Optimization Theory and Applications, vol. 177, no. 1, pp. 153-180, 2018.\n\nA proof of local convergence for the Adam optimizer. S Bock, M Wei\u00df, Proc. International Joint Conference on Neural Networks (IJCNN). International Joint Conference on Neural Networks (IJCNN)IEEES. Bock and M. Wei\u00df, \"A proof of local convergence for the Adam optimizer,\" in Proc. International Joint Conference on Neural Networks (IJCNN). IEEE, 2019, pp. 1-8.\n\nDual averaging for distributed optimization: Convergence analysis and network scaling. J C Duchi, A Agarwal, M J Wainwright, IEEE Trans. Autom. control. 573J. C. Duchi, A. Agarwal, and M. J. Wainwright, \"Dual averaging for distributed optimization: Convergence analysis and network scaling,\" IEEE Trans. Autom. control, vol. 57, no. 3, pp. 592-606, 2012.\n\nMulti-agent optimization in the presence of Byzantine adversaries: Fundamental limits. L Su, N Vaidya, Proc. American Control Conference (ACC). American Control Conference (ACC)L. Su and N. Vaidya, \"Multi-agent optimization in the presence of Byzantine adversaries: Fundamental limits,\" in Proc. American Control Conference (ACC), 2016, pp. 7183-7188.\n\nByzantine-resilient stochastic gradient descent for distributed learning: A Lipschitz-inspired coordinate-wise median approach. H Yang, X Zhang, M Fang, J Liu, Proc. IEEE Conference on Decision and Control (CDC). IEEE Conference on Decision and Control (CDC)H. Yang, X. zhong Zhang, M. Fang, and J. Liu, \"Byzantine-resilient stochastic gradient descent for distributed learning: A Lipschitz-inspired coordinate-wise median approach,\" Proc. IEEE Conference on Decision and Control (CDC), pp. 5832-5837, 2019.\n\nByzantine-resilient high-dimensional SGD with local iterations on heterogeneous data. D Data, S Diggavi, Proc. 38th Intl. Conf. Machine Learning. 38th Intl. Conf. Machine LearningD. Data and S. Diggavi, \"Byzantine-resilient high-dimensional SGD with local iterations on heterogeneous data,\" in Proc. 38th Intl. Conf. Machine Learning, Jul. 2021, pp. 2478-2488.\n\nP J Huber, Robust Statistics. Berlin, HeidelbergSpringerP. J. Huber, Robust Statistics. Berlin, Heidelberg: Springer, 2011.\n\nFault-tolerant distributed optimization (part IV): Constrained optimization with arbitrary directed networks. L Su, N H Vaidya, arXiv:1511.01821arXiv preprintL. Su and N. H. Vaidya, \"Fault-tolerant distributed optimization (part IV): Constrained optimization with arbitrary directed networks,\" arXiv preprint arXiv:1511.01821, 2015.\n\nIntroductory Lectures on Convex Optimization, ser. Y Nesterov, Springer US87Applied optimization; vY. Nesterov, Introductory Lectures on Convex Optimization, ser. Applied optimization; v. 87. Springer US, 2004.\n\nBRIDGE: Byzantine-resilient decentralized gradient descent. Z Yang, W U Bajwa, arXiv:1908.08098v1arXiv preprintZ. Yang and W. U. Bajwa, \"BRIDGE: Byzantine-resilient decentralized gradient descent,\" arXiv preprint arXiv:1908.08098v1, Aug. 2019. [Online]. Available: https://arxiv.org/abs/1908.08098v1\n\nThe landscape of empirical risk for nonconvex losses. S Mei, Y Bai, A Montanari, The Annals of Statistics. 466AS. Mei, Y. Bai, and A. Montanari, \"The landscape of empirical risk for nonconvex losses,\" The Annals of Statistics, vol. 46, no. 6A, pp. 2747 -2774, 2018.\n\nGraphical convergence of subgradients in nonconvex optimization and learning. D Davis, D Drusvyatskiy, Mathematics of Operations Research. 471D. Davis and D. Drusvyatskiy, \"Graphical convergence of subgradients in nonconvex optimization and learning,\" Mathematics of Operations Research, vol. 47, no. 1, pp. 209-231, 2021.\n\nUniform convergence of gradients for non-convex learning and optimization. D J Foster, A Sekhari, K Sridharan, Proc. Advances in Neural Information Processing Systems. Advances in Neural Information essing SystemsCurran Associates, Inc31D. J. Foster, A. Sekhari, and K. Sridharan, \"Uniform convergence of gradients for non-convex learning and optimization,\" in Proc. Advances in Neural Information Processing Systems, vol. 31. Curran Associates, Inc., 2018.\n\nClassifying CIFAR-10 images using unsupervised feature & ensemble learning. T V J Le, N Gopee, T. V. J. Le and N. Gopee, \"Classifying CIFAR-10 images using unsupervised feature & ensemble learning,\" Dec 2016. [Online].\n\nProbability inequalities for sums of bounded random variables. W Hoeffding, J. American Stat. Assoc. 58301W. Hoeffding, \"Probability inequalities for sums of bounded random variables,\" J. American Stat. Assoc., vol. 58, no. 301, pp. 13-30, 1963.\n\nCovering a ball with smaller equal balls in R n. J Verger-Gaugry, Discrete & Computational Geometry. 331J. Verger-Gaugry, \"Covering a ball with smaller equal balls in R n ,\" Discrete & Computational Geometry, vol. 33, no. 1, pp. 143-155, 2005.\n", "annotations": {"author": "[{\"end\":73,\"start\":62},{\"end\":88,\"start\":74},{\"end\":104,\"start\":89}]", "publisher": null, "author_last_name": "[{\"end\":72,\"start\":68},{\"end\":87,\"start\":83},{\"end\":103,\"start\":98}]", "author_first_name": "[{\"end\":67,\"start\":62},{\"end\":82,\"start\":74},{\"end\":95,\"start\":89},{\"end\":97,\"start\":96}]", "author_affiliation": null, "title": "[{\"end\":59,\"start\":1},{\"end\":163,\"start\":105}]", "venue": null, "abstract": "[{\"end\":2129,\"start\":166}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2281,\"start\":2278},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2390,\"start\":2387},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2395,\"start\":2392},{\"end\":2832,\"start\":2831},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3396,\"start\":3393},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3520,\"start\":3517},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3928,\"start\":3925},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3933,\"start\":3930},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4060,\"start\":4057},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":4066,\"start\":4062},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4200,\"start\":4196},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4324,\"start\":4321},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4330,\"start\":4326},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":4422,\"start\":4418},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4554,\"start\":4550},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4688,\"start\":4684},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4694,\"start\":4690},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":4731,\"start\":4727},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":5360,\"start\":5357},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5474,\"start\":5470},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":5514,\"start\":5510},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":5721,\"start\":5717},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":5727,\"start\":5723},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":6501,\"start\":6497},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":6507,\"start\":6503},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":6745,\"start\":6741},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":6751,\"start\":6747},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":6901,\"start\":6897},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":6907,\"start\":6903},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":7366,\"start\":7362},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":7372,\"start\":7368},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":7527,\"start\":7523},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":7533,\"start\":7529},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":7851,\"start\":7847},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":7885,\"start\":7881},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":7891,\"start\":7887},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":8290,\"start\":8286},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":8593,\"start\":8589},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":8744,\"start\":8740},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":10094,\"start\":10090},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":10100,\"start\":10096},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":10521,\"start\":10517},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":10801,\"start\":10797},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":11198,\"start\":11194},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":11455,\"start\":11451},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":11739,\"start\":11735},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":12078,\"start\":12074},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":12282,\"start\":12278},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":12595,\"start\":12591},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":12653,\"start\":12649},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":14646,\"start\":14642},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":14753,\"start\":14749},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":14771,\"start\":14767},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":15597,\"start\":15593},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":15603,\"start\":15599},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":15668,\"start\":15664},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":15726,\"start\":15722},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":15732,\"start\":15728},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":15847,\"start\":15843},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":18700,\"start\":18697},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":18705,\"start\":18702},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19223,\"start\":19220},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":19228,\"start\":19225},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":20512,\"start\":20511},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":20825,\"start\":20821},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":23593,\"start\":23589},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":23599,\"start\":23595},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":24170,\"start\":24166},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":24176,\"start\":24172},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":25737,\"start\":25734},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":26278,\"start\":26275},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":26284,\"start\":26280},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":26290,\"start\":26286},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":26296,\"start\":26292},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":27072,\"start\":27068},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":27078,\"start\":27074},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":29167,\"start\":29163},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":29173,\"start\":29169},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":30534,\"start\":30530},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":31172,\"start\":31168},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":31181,\"start\":31177},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":31405,\"start\":31401},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":31411,\"start\":31407},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":32034,\"start\":32030},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":32040,\"start\":32036},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":32046,\"start\":32042},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":32052,\"start\":32048},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":32117,\"start\":32113},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":33602,\"start\":33598},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":33608,\"start\":33604},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":33614,\"start\":33610},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":33620,\"start\":33616},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":33626,\"start\":33622},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":34094,\"start\":34091},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":34571,\"start\":34567},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":35721,\"start\":35717},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":35823,\"start\":35819},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":35829,\"start\":35825},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":35866,\"start\":35862},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":35872,\"start\":35868},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":35878,\"start\":35874},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":36177,\"start\":36173},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":36211,\"start\":36207},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":36217,\"start\":36213},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":36223,\"start\":36219},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":37464,\"start\":37460},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":38065,\"start\":38061},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":39159,\"start\":39155},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":41313,\"start\":41309},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":47238,\"start\":47234},{\"end\":47246,\"start\":47238},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":47529,\"start\":47525},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":49256,\"start\":49252},{\"end\":49264,\"start\":49256},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":51398,\"start\":51394},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":54140,\"start\":54136},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":54326,\"start\":54322},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":56444,\"start\":56440},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":60481,\"start\":60477},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":60793,\"start\":60789},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":60799,\"start\":60795},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":60900,\"start\":60896},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":60906,\"start\":60902},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":62270,\"start\":62266},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":64576,\"start\":64572},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":66894,\"start\":66890},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":68959,\"start\":68958},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":69557,\"start\":69553},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":69703,\"start\":69699},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":69740,\"start\":69736},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":72850,\"start\":72846},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":73314,\"start\":73310},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":73629,\"start\":73625},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":74230,\"start\":74226},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":74962,\"start\":74958},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":76159,\"start\":76155},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":76354,\"start\":76350},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":77001,\"start\":76997},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":77115,\"start\":77111},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":77913,\"start\":77909},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":80371,\"start\":80367},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":80485,\"start\":80481},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":81002,\"start\":80998},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":81151,\"start\":81147},{\"attributes\":{\"ref_id\":\"b93\"},\"end\":82346,\"start\":82342}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":77928,\"start\":77869},{\"attributes\":{\"id\":\"fig_1\"},\"end\":78040,\"start\":77929},{\"attributes\":{\"id\":\"fig_2\"},\"end\":78178,\"start\":78041},{\"attributes\":{\"id\":\"fig_3\"},\"end\":78351,\"start\":78179},{\"attributes\":{\"id\":\"fig_4\"},\"end\":78750,\"start\":78352},{\"attributes\":{\"id\":\"fig_5\"},\"end\":79004,\"start\":78751},{\"attributes\":{\"id\":\"fig_6\"},\"end\":79638,\"start\":79005},{\"attributes\":{\"id\":\"fig_7\"},\"end\":79752,\"start\":79639},{\"attributes\":{\"id\":\"fig_8\"},\"end\":79899,\"start\":79753},{\"attributes\":{\"id\":\"fig_9\"},\"end\":80041,\"start\":79900},{\"attributes\":{\"id\":\"fig_10\"},\"end\":80181,\"start\":80042},{\"attributes\":{\"id\":\"fig_11\"},\"end\":80326,\"start\":80182},{\"attributes\":{\"id\":\"fig_12\"},\"end\":80438,\"start\":80327},{\"attributes\":{\"id\":\"fig_13\"},\"end\":80830,\"start\":80439},{\"attributes\":{\"id\":\"fig_14\"},\"end\":81089,\"start\":80831},{\"attributes\":{\"id\":\"fig_15\"},\"end\":81291,\"start\":81090},{\"attributes\":{\"id\":\"fig_16\"},\"end\":81360,\"start\":81292},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":81914,\"start\":81361},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":82039,\"start\":81915},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":82248,\"start\":82040},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":82339,\"start\":82249},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":82489,\"start\":82340}]", "paragraph": "[{\"end\":3601,\"start\":2148},{\"end\":5127,\"start\":3603},{\"end\":6179,\"start\":5162},{\"end\":7203,\"start\":6181},{\"end\":7892,\"start\":7205},{\"end\":10039,\"start\":7894},{\"end\":12219,\"start\":10041},{\"end\":13016,\"start\":12221},{\"end\":14230,\"start\":13041},{\"end\":15101,\"start\":14232},{\"end\":15947,\"start\":15103},{\"end\":16999,\"start\":15980},{\"end\":17949,\"start\":17001},{\"end\":18712,\"start\":17996},{\"end\":18752,\"start\":18749},{\"end\":19229,\"start\":18754},{\"end\":20146,\"start\":19280},{\"end\":20640,\"start\":20148},{\"end\":20832,\"start\":20642},{\"end\":21076,\"start\":20899},{\"end\":21306,\"start\":21078},{\"end\":21422,\"start\":21308},{\"end\":21938,\"start\":21515},{\"end\":22712,\"start\":21940},{\"end\":23818,\"start\":22714},{\"end\":24272,\"start\":23820},{\"end\":24877,\"start\":24319},{\"end\":25207,\"start\":24903},{\"end\":25377,\"start\":25209},{\"end\":26095,\"start\":25453},{\"end\":26599,\"start\":26169},{\"end\":27079,\"start\":26649},{\"end\":27423,\"start\":27081},{\"end\":27881,\"start\":27425},{\"end\":28160,\"start\":27883},{\"end\":28920,\"start\":28232},{\"end\":29281,\"start\":28922},{\"end\":29462,\"start\":29283},{\"end\":29755,\"start\":29464},{\"end\":30037,\"start\":29757},{\"end\":31771,\"start\":30039},{\"end\":32292,\"start\":31814},{\"end\":32538,\"start\":32355},{\"end\":32871,\"start\":32605},{\"end\":33050,\"start\":32873},{\"end\":33306,\"start\":33087},{\"end\":33922,\"start\":33544},{\"end\":35438,\"start\":33924},{\"end\":36626,\"start\":35440},{\"end\":36706,\"start\":36686},{\"end\":36902,\"start\":36792},{\"end\":37301,\"start\":36969},{\"end\":37626,\"start\":37303},{\"end\":38169,\"start\":37679},{\"end\":38452,\"start\":38336},{\"end\":39040,\"start\":38477},{\"end\":40315,\"start\":39042},{\"end\":41019,\"start\":40317},{\"end\":42299,\"start\":41060},{\"end\":42482,\"start\":42301},{\"end\":42592,\"start\":42551},{\"end\":43986,\"start\":42624},{\"end\":44453,\"start\":44014},{\"end\":44629,\"start\":44455},{\"end\":45758,\"start\":44666},{\"end\":46109,\"start\":46049},{\"end\":46662,\"start\":46483},{\"end\":47166,\"start\":46723},{\"end\":47285,\"start\":47213},{\"end\":47385,\"start\":47324},{\"end\":47567,\"start\":47444},{\"end\":48103,\"start\":47758},{\"end\":48377,\"start\":48260},{\"end\":48585,\"start\":48538},{\"end\":48875,\"start\":48701},{\"end\":49838,\"start\":49088},{\"end\":50692,\"start\":49953},{\"end\":51293,\"start\":50740},{\"end\":51403,\"start\":51295},{\"end\":51564,\"start\":51439},{\"end\":51911,\"start\":51618},{\"end\":52004,\"start\":51962},{\"end\":52118,\"start\":52040},{\"end\":52418,\"start\":52227},{\"end\":52483,\"start\":52465},{\"end\":52709,\"start\":52555},{\"end\":52868,\"start\":52711},{\"end\":53089,\"start\":52950},{\"end\":53506,\"start\":53091},{\"end\":53877,\"start\":53508},{\"end\":54058,\"start\":53960},{\"end\":54454,\"start\":54124},{\"end\":54592,\"start\":54587},{\"end\":55270,\"start\":54708},{\"end\":55961,\"start\":55272},{\"end\":56011,\"start\":55988},{\"end\":57146,\"start\":56103},{\"end\":57627,\"start\":57148},{\"end\":58668,\"start\":57629},{\"end\":58860,\"start\":58697},{\"end\":59560,\"start\":58926},{\"end\":59668,\"start\":59562},{\"end\":60055,\"start\":59670},{\"end\":61023,\"start\":60196},{\"end\":61976,\"start\":61048},{\"end\":62666,\"start\":62023},{\"end\":64237,\"start\":62668},{\"end\":65016,\"start\":64239},{\"end\":65851,\"start\":65018},{\"end\":66543,\"start\":65853},{\"end\":67117,\"start\":66545},{\"end\":68345,\"start\":67175},{\"end\":69224,\"start\":68347},{\"end\":69778,\"start\":69269},{\"end\":70682,\"start\":69780},{\"end\":71159,\"start\":70684},{\"end\":71867,\"start\":71178},{\"end\":72164,\"start\":71869},{\"end\":72277,\"start\":72235},{\"end\":72682,\"start\":72309},{\"end\":72875,\"start\":72684},{\"end\":73072,\"start\":72946},{\"end\":73103,\"start\":73074},{\"end\":73166,\"start\":73105},{\"end\":73418,\"start\":73230},{\"end\":73646,\"start\":73420},{\"end\":73826,\"start\":73730},{\"end\":74065,\"start\":73855},{\"end\":74630,\"start\":74140},{\"end\":74729,\"start\":74632},{\"end\":74846,\"start\":74731},{\"end\":75198,\"start\":74931},{\"end\":75263,\"start\":75237},{\"end\":75402,\"start\":75344},{\"end\":75919,\"start\":75566},{\"end\":76180,\"start\":76137},{\"end\":76307,\"start\":76302},{\"end\":76375,\"start\":76327},{\"end\":76585,\"start\":76524},{\"end\":76655,\"start\":76644},{\"end\":77007,\"start\":76968},{\"end\":77161,\"start\":77040},{\"end\":77868,\"start\":77711}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":18748,\"start\":18713},{\"attributes\":{\"id\":\"formula_1\"},\"end\":19279,\"start\":19230},{\"attributes\":{\"id\":\"formula_2\"},\"end\":20898,\"start\":20833},{\"attributes\":{\"id\":\"formula_3\"},\"end\":21514,\"start\":21423},{\"attributes\":{\"id\":\"formula_4\"},\"end\":24902,\"start\":24878},{\"attributes\":{\"id\":\"formula_5\"},\"end\":25452,\"start\":25378},{\"attributes\":{\"id\":\"formula_6\"},\"end\":26168,\"start\":26096},{\"attributes\":{\"id\":\"formula_7\"},\"end\":28231,\"start\":28161},{\"attributes\":{\"id\":\"formula_8\"},\"end\":32354,\"start\":32293},{\"attributes\":{\"id\":\"formula_9\"},\"end\":32604,\"start\":32539},{\"attributes\":{\"id\":\"formula_10\"},\"end\":33543,\"start\":33307},{\"attributes\":{\"id\":\"formula_11\"},\"end\":36685,\"start\":36627},{\"attributes\":{\"id\":\"formula_12\"},\"end\":36748,\"start\":36707},{\"attributes\":{\"id\":\"formula_13\"},\"end\":36791,\"start\":36748},{\"attributes\":{\"id\":\"formula_14\"},\"end\":36968,\"start\":36903},{\"attributes\":{\"id\":\"formula_15\"},\"end\":37678,\"start\":37627},{\"attributes\":{\"id\":\"formula_16\"},\"end\":38335,\"start\":38170},{\"attributes\":{\"id\":\"formula_17\"},\"end\":38476,\"start\":38453},{\"attributes\":{\"id\":\"formula_18\"},\"end\":42550,\"start\":42483},{\"attributes\":{\"id\":\"formula_19\"},\"end\":42623,\"start\":42593},{\"attributes\":{\"id\":\"formula_20\"},\"end\":44665,\"start\":44630},{\"attributes\":{\"id\":\"formula_21\"},\"end\":46048,\"start\":45759},{\"attributes\":{\"id\":\"formula_22\"},\"end\":46482,\"start\":46110},{\"attributes\":{\"id\":\"formula_23\"},\"end\":46722,\"start\":46663},{\"attributes\":{\"id\":\"formula_24\"},\"end\":47212,\"start\":47167},{\"attributes\":{\"id\":\"formula_25\"},\"end\":47323,\"start\":47286},{\"attributes\":{\"id\":\"formula_26\"},\"end\":47443,\"start\":47386},{\"attributes\":{\"id\":\"formula_27\"},\"end\":47757,\"start\":47568},{\"attributes\":{\"id\":\"formula_28\"},\"end\":48259,\"start\":48104},{\"attributes\":{\"id\":\"formula_29\"},\"end\":48537,\"start\":48378},{\"attributes\":{\"id\":\"formula_30\"},\"end\":48700,\"start\":48586},{\"attributes\":{\"id\":\"formula_31\"},\"end\":49087,\"start\":48876},{\"attributes\":{\"id\":\"formula_32\"},\"end\":49952,\"start\":49839},{\"attributes\":{\"id\":\"formula_33\"},\"end\":50700,\"start\":50693},{\"attributes\":{\"id\":\"formula_34\"},\"end\":51438,\"start\":51404},{\"attributes\":{\"id\":\"formula_35\"},\"end\":51617,\"start\":51565},{\"attributes\":{\"id\":\"formula_36\"},\"end\":51961,\"start\":51912},{\"attributes\":{\"id\":\"formula_37\"},\"end\":52039,\"start\":52005},{\"attributes\":{\"id\":\"formula_38\"},\"end\":52226,\"start\":52119},{\"attributes\":{\"id\":\"formula_39\"},\"end\":52464,\"start\":52419},{\"attributes\":{\"id\":\"formula_40\"},\"end\":52554,\"start\":52484},{\"attributes\":{\"id\":\"formula_41\"},\"end\":52949,\"start\":52869},{\"attributes\":{\"id\":\"formula_42\"},\"end\":53959,\"start\":53878},{\"attributes\":{\"id\":\"formula_43\"},\"end\":54123,\"start\":54059},{\"attributes\":{\"id\":\"formula_44\"},\"end\":54586,\"start\":54455},{\"attributes\":{\"id\":\"formula_45\"},\"end\":54707,\"start\":54593},{\"attributes\":{\"id\":\"formula_46\"},\"end\":55987,\"start\":55962},{\"attributes\":{\"id\":\"formula_47\"},\"end\":56102,\"start\":56012},{\"attributes\":{\"id\":\"formula_48\"},\"end\":58696,\"start\":58669},{\"attributes\":{\"id\":\"formula_49\"},\"end\":58925,\"start\":58861},{\"attributes\":{\"id\":\"formula_50\"},\"end\":60081,\"start\":60056},{\"attributes\":{\"id\":\"formula_51\"},\"end\":60195,\"start\":60081},{\"attributes\":{\"id\":\"formula_52\"},\"end\":72234,\"start\":72165},{\"attributes\":{\"id\":\"formula_53\"},\"end\":72308,\"start\":72278},{\"attributes\":{\"id\":\"formula_54\"},\"end\":72945,\"start\":72876},{\"attributes\":{\"id\":\"formula_56\"},\"end\":73229,\"start\":73167},{\"attributes\":{\"id\":\"formula_58\"},\"end\":73729,\"start\":73647},{\"attributes\":{\"id\":\"formula_59\"},\"end\":73854,\"start\":73827},{\"attributes\":{\"id\":\"formula_60\"},\"end\":74139,\"start\":74066},{\"attributes\":{\"id\":\"formula_63\"},\"end\":74930,\"start\":74847},{\"attributes\":{\"id\":\"formula_64\"},\"end\":75236,\"start\":75199},{\"attributes\":{\"id\":\"formula_65\"},\"end\":75343,\"start\":75264},{\"attributes\":{\"id\":\"formula_66\"},\"end\":75543,\"start\":75403},{\"attributes\":{\"id\":\"formula_67\"},\"end\":76136,\"start\":75920},{\"attributes\":{\"id\":\"formula_68\"},\"end\":76301,\"start\":76181},{\"attributes\":{\"id\":\"formula_69\"},\"end\":76326,\"start\":76308},{\"attributes\":{\"id\":\"formula_70\"},\"end\":76523,\"start\":76376},{\"attributes\":{\"id\":\"formula_71\"},\"end\":76643,\"start\":76586},{\"attributes\":{\"id\":\"formula_72\"},\"end\":76967,\"start\":76656},{\"attributes\":{\"id\":\"formula_73\"},\"end\":77039,\"start\":77008},{\"attributes\":{\"id\":\"formula_74\"},\"end\":77710,\"start\":77162}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":15400,\"start\":15393},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":19950,\"start\":19942},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":40366,\"start\":40358},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":63641,\"start\":63633},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":65400,\"start\":65392}]", "section_header": "[{\"end\":2146,\"start\":2131},{\"end\":5160,\"start\":5130},{\"end\":13039,\"start\":13019},{\"end\":15978,\"start\":15950},{\"end\":17975,\"start\":17952},{\"end\":17994,\"start\":17978},{\"end\":24317,\"start\":24275},{\"end\":26647,\"start\":26602},{\"end\":31812,\"start\":31774},{\"end\":33085,\"start\":33053},{\"end\":41058,\"start\":41022},{\"end\":44012,\"start\":43989},{\"end\":50738,\"start\":50702},{\"end\":61046,\"start\":61026},{\"end\":62021,\"start\":61979},{\"end\":67173,\"start\":67120},{\"end\":69267,\"start\":69227},{\"end\":71176,\"start\":71162},{\"end\":75564,\"start\":75545},{\"end\":77884,\"start\":77870},{\"end\":77939,\"start\":77930},{\"end\":78051,\"start\":78042},{\"end\":78188,\"start\":78180},{\"end\":78760,\"start\":78752},{\"end\":79014,\"start\":79006},{\"end\":79648,\"start\":79640},{\"end\":79762,\"start\":79754},{\"end\":79909,\"start\":79901},{\"end\":80051,\"start\":80043},{\"end\":80191,\"start\":80183},{\"end\":80336,\"start\":80328},{\"end\":80449,\"start\":80440},{\"end\":80835,\"start\":80832},{\"end\":81092,\"start\":81091},{\"end\":81925,\"start\":81916},{\"end\":82051,\"start\":82041},{\"end\":82260,\"start\":82250}]", "table": "[{\"end\":81914,\"start\":81861},{\"end\":82248,\"start\":82125},{\"end\":82339,\"start\":82335},{\"end\":82489,\"start\":82451}]", "figure_caption": "[{\"end\":77928,\"start\":77887},{\"end\":78040,\"start\":77941},{\"end\":78178,\"start\":78053},{\"end\":78351,\"start\":78190},{\"end\":78750,\"start\":78354},{\"end\":79004,\"start\":78762},{\"end\":79638,\"start\":79016},{\"end\":79752,\"start\":79650},{\"end\":79899,\"start\":79764},{\"end\":80041,\"start\":79911},{\"end\":80181,\"start\":80053},{\"end\":80326,\"start\":80193},{\"end\":80438,\"start\":80338},{\"end\":80830,\"start\":80452},{\"end\":81089,\"start\":80837},{\"end\":81291,\"start\":81093},{\"end\":81360,\"start\":81294},{\"end\":81861,\"start\":81363},{\"end\":82039,\"start\":81927},{\"end\":82125,\"start\":82054},{\"end\":82335,\"start\":82262},{\"end\":82451,\"start\":82342}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":54370,\"start\":54362},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":64266,\"start\":64258},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":64781,\"start\":64773},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":65053,\"start\":65045},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":65465,\"start\":65457},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":65900,\"start\":65892},{\"attributes\":{\"ref_id\":\"fig_8\"},\"end\":66577,\"start\":66569},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":68374,\"start\":68366},{\"attributes\":{\"ref_id\":\"fig_10\"},\"end\":68512,\"start\":68504},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":68829,\"start\":68821},{\"attributes\":{\"ref_id\":\"fig_12\"},\"end\":70006,\"start\":69998},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":70929,\"start\":70919}]", "bib_author_first_name": "[{\"end\":84824,\"start\":84823},{\"end\":85061,\"start\":85060},{\"end\":85293,\"start\":85292},{\"end\":85295,\"start\":85294},{\"end\":85309,\"start\":85308},{\"end\":85322,\"start\":85321},{\"end\":85622,\"start\":85621},{\"end\":85784,\"start\":85783},{\"end\":85793,\"start\":85792},{\"end\":85809,\"start\":85808},{\"end\":86053,\"start\":86052},{\"end\":86055,\"start\":86054},{\"end\":86372,\"start\":86371},{\"end\":86380,\"start\":86379},{\"end\":86388,\"start\":86387},{\"end\":86390,\"start\":86389},{\"end\":86763,\"start\":86762},{\"end\":86774,\"start\":86773},{\"end\":86782,\"start\":86781},{\"end\":86784,\"start\":86783},{\"end\":87052,\"start\":87051},{\"end\":87054,\"start\":87053},{\"end\":87063,\"start\":87062},{\"end\":87065,\"start\":87064},{\"end\":87077,\"start\":87076},{\"end\":87079,\"start\":87078},{\"end\":87377,\"start\":87376},{\"end\":87385,\"start\":87384},{\"end\":87395,\"start\":87394},{\"end\":87402,\"start\":87401},{\"end\":87413,\"start\":87412},{\"end\":87744,\"start\":87743},{\"end\":87746,\"start\":87745},{\"end\":88021,\"start\":88020},{\"end\":88030,\"start\":88029},{\"end\":88043,\"start\":88042},{\"end\":88045,\"start\":88044},{\"end\":88313,\"start\":88312},{\"end\":88320,\"start\":88319},{\"end\":88326,\"start\":88325},{\"end\":88520,\"start\":88519},{\"end\":88532,\"start\":88531},{\"end\":88540,\"start\":88539},{\"end\":88554,\"start\":88553},{\"end\":88994,\"start\":88993},{\"end\":89000,\"start\":88999},{\"end\":89002,\"start\":89001},{\"end\":89327,\"start\":89326},{\"end\":89338,\"start\":89337},{\"end\":89349,\"start\":89348},{\"end\":89614,\"start\":89613},{\"end\":89623,\"start\":89622},{\"end\":89636,\"start\":89635},{\"end\":89888,\"start\":89887},{\"end\":89897,\"start\":89896},{\"end\":90224,\"start\":90223},{\"end\":90230,\"start\":90229},{\"end\":90232,\"start\":90231},{\"end\":90244,\"start\":90243},{\"end\":90246,\"start\":90245},{\"end\":90254,\"start\":90253},{\"end\":90256,\"start\":90255},{\"end\":90265,\"start\":90264},{\"end\":90274,\"start\":90273},{\"end\":90288,\"start\":90287},{\"end\":90296,\"start\":90295},{\"end\":90298,\"start\":90297},{\"end\":90313,\"start\":90308},{\"end\":90854,\"start\":90853},{\"end\":90865,\"start\":90864},{\"end\":90867,\"start\":90866},{\"end\":90878,\"start\":90877},{\"end\":90880,\"start\":90879},{\"end\":90886,\"start\":90885},{\"end\":90899,\"start\":90898},{\"end\":90901,\"start\":90900},{\"end\":90911,\"start\":90910},{\"end\":91338,\"start\":91337},{\"end\":91351,\"start\":91350},{\"end\":91364,\"start\":91363},{\"end\":91713,\"start\":91712},{\"end\":91721,\"start\":91720},{\"end\":91729,\"start\":91728},{\"end\":91740,\"start\":91739},{\"end\":92127,\"start\":92126},{\"end\":92134,\"start\":92133},{\"end\":92525,\"start\":92524},{\"end\":92531,\"start\":92530},{\"end\":92764,\"start\":92763},{\"end\":92771,\"start\":92770},{\"end\":92779,\"start\":92778},{\"end\":92789,\"start\":92788},{\"end\":93132,\"start\":93131},{\"end\":93146,\"start\":93145},{\"end\":93148,\"start\":93147},{\"end\":93158,\"start\":93157},{\"end\":93171,\"start\":93170},{\"end\":93180,\"start\":93179},{\"end\":93519,\"start\":93518},{\"end\":93521,\"start\":93520},{\"end\":93531,\"start\":93530},{\"end\":93544,\"start\":93543},{\"end\":93875,\"start\":93874},{\"end\":93882,\"start\":93881},{\"end\":93890,\"start\":93889},{\"end\":93905,\"start\":93904},{\"end\":94227,\"start\":94226},{\"end\":94239,\"start\":94238},{\"end\":94252,\"start\":94251},{\"end\":94577,\"start\":94576},{\"end\":94584,\"start\":94583},{\"end\":94594,\"start\":94593},{\"end\":95217,\"start\":95216},{\"end\":95225,\"start\":95224},{\"end\":95233,\"start\":95232},{\"end\":95240,\"start\":95239},{\"end\":95246,\"start\":95245},{\"end\":95666,\"start\":95665},{\"end\":95676,\"start\":95675},{\"end\":95684,\"start\":95683},{\"end\":95695,\"start\":95694},{\"end\":96132,\"start\":96131},{\"end\":96138,\"start\":96137},{\"end\":96144,\"start\":96143},{\"end\":96152,\"start\":96151},{\"end\":96165,\"start\":96164},{\"end\":96596,\"start\":96595},{\"end\":96603,\"start\":96602},{\"end\":96609,\"start\":96608},{\"end\":96936,\"start\":96935},{\"end\":96943,\"start\":96942},{\"end\":96951,\"start\":96950},{\"end\":97338,\"start\":97337},{\"end\":97347,\"start\":97346},{\"end\":97355,\"start\":97354},{\"end\":97362,\"start\":97361},{\"end\":97619,\"start\":97618},{\"end\":97627,\"start\":97626},{\"end\":97635,\"start\":97634},{\"end\":97637,\"start\":97636},{\"end\":97916,\"start\":97915},{\"end\":97920,\"start\":97917},{\"end\":97930,\"start\":97929},{\"end\":97943,\"start\":97942},{\"end\":97955,\"start\":97954},{\"end\":98178,\"start\":98177},{\"end\":98185,\"start\":98184},{\"end\":98195,\"start\":98194},{\"end\":98489,\"start\":98485},{\"end\":98502,\"start\":98501},{\"end\":98515,\"start\":98514},{\"end\":98764,\"start\":98763},{\"end\":98773,\"start\":98772},{\"end\":99051,\"start\":99050},{\"end\":99053,\"start\":99052},{\"end\":99060,\"start\":99059},{\"end\":99069,\"start\":99068},{\"end\":99365,\"start\":99364},{\"end\":99374,\"start\":99373},{\"end\":99622,\"start\":99621},{\"end\":99628,\"start\":99627},{\"end\":99847,\"start\":99846},{\"end\":99849,\"start\":99848},{\"end\":99859,\"start\":99858},{\"end\":99867,\"start\":99866},{\"end\":99869,\"start\":99868},{\"end\":100156,\"start\":100155},{\"end\":100158,\"start\":100157},{\"end\":100166,\"start\":100165},{\"end\":100168,\"start\":100167},{\"end\":100178,\"start\":100177},{\"end\":100180,\"start\":100179},{\"end\":100190,\"start\":100189},{\"end\":100524,\"start\":100523},{\"end\":100531,\"start\":100530},{\"end\":100539,\"start\":100538},{\"end\":100547,\"start\":100546},{\"end\":100553,\"start\":100552},{\"end\":100883,\"start\":100882},{\"end\":100895,\"start\":100894},{\"end\":100902,\"start\":100901},{\"end\":100910,\"start\":100909},{\"end\":101233,\"start\":101232},{\"end\":101245,\"start\":101244},{\"end\":101253,\"start\":101252},{\"end\":101500,\"start\":101499},{\"end\":101502,\"start\":101501},{\"end\":101513,\"start\":101512},{\"end\":101522,\"start\":101521},{\"end\":101536,\"start\":101535},{\"end\":101819,\"start\":101818},{\"end\":101821,\"start\":101820},{\"end\":101831,\"start\":101830},{\"end\":101840,\"start\":101839},{\"end\":102191,\"start\":102190},{\"end\":102203,\"start\":102202},{\"end\":102458,\"start\":102457},{\"end\":102466,\"start\":102465},{\"end\":102468,\"start\":102467},{\"end\":102863,\"start\":102862},{\"end\":102869,\"start\":102868},{\"end\":102875,\"start\":102874},{\"end\":103202,\"start\":103201},{\"end\":103211,\"start\":103210},{\"end\":103223,\"start\":103222},{\"end\":103233,\"start\":103232},{\"end\":103621,\"start\":103620},{\"end\":103627,\"start\":103626},{\"end\":103980,\"start\":103979},{\"end\":103988,\"start\":103987},{\"end\":103990,\"start\":103989},{\"end\":104311,\"start\":104310},{\"end\":104331,\"start\":104330},{\"end\":104338,\"start\":104337},{\"end\":104711,\"start\":104710},{\"end\":104719,\"start\":104718},{\"end\":104725,\"start\":104724},{\"end\":104996,\"start\":104995},{\"end\":105003,\"start\":105002},{\"end\":105012,\"start\":105011},{\"end\":105019,\"start\":105018},{\"end\":105025,\"start\":105024},{\"end\":105034,\"start\":105033},{\"end\":105282,\"start\":105278},{\"end\":105295,\"start\":105294},{\"end\":105308,\"start\":105307},{\"end\":105320,\"start\":105319},{\"end\":105329,\"start\":105328},{\"end\":105575,\"start\":105574},{\"end\":105577,\"start\":105576},{\"end\":105588,\"start\":105587},{\"end\":105888,\"start\":105887},{\"end\":105896,\"start\":105895},{\"end\":106216,\"start\":106215},{\"end\":106223,\"start\":106222},{\"end\":106229,\"start\":106228},{\"end\":106633,\"start\":106632},{\"end\":106640,\"start\":106639},{\"end\":106642,\"start\":106641},{\"end\":106650,\"start\":106649},{\"end\":106919,\"start\":106918},{\"end\":106925,\"start\":106924},{\"end\":106927,\"start\":106926},{\"end\":106942,\"start\":106941},{\"end\":107188,\"start\":107187},{\"end\":107197,\"start\":107196},{\"end\":107207,\"start\":107206},{\"end\":107217,\"start\":107216},{\"end\":107483,\"start\":107482},{\"end\":107497,\"start\":107496},{\"end\":107600,\"start\":107599},{\"end\":107602,\"start\":107601},{\"end\":107772,\"start\":107771},{\"end\":107779,\"start\":107778},{\"end\":107785,\"start\":107784},{\"end\":107982,\"start\":107981},{\"end\":107990,\"start\":107989},{\"end\":108253,\"start\":108252},{\"end\":108262,\"start\":108261},{\"end\":108559,\"start\":108558},{\"end\":108567,\"start\":108566},{\"end\":108576,\"start\":108575},{\"end\":108926,\"start\":108925},{\"end\":109184,\"start\":109183},{\"end\":109478,\"start\":109477},{\"end\":109486,\"start\":109485},{\"end\":109873,\"start\":109872},{\"end\":109875,\"start\":109874},{\"end\":109884,\"start\":109883},{\"end\":109895,\"start\":109894},{\"end\":109897,\"start\":109896},{\"end\":110229,\"start\":110228},{\"end\":110235,\"start\":110234},{\"end\":110623,\"start\":110622},{\"end\":110631,\"start\":110630},{\"end\":110640,\"start\":110639},{\"end\":110648,\"start\":110647},{\"end\":111090,\"start\":111089},{\"end\":111098,\"start\":111097},{\"end\":111366,\"start\":111365},{\"end\":111368,\"start\":111367},{\"end\":111601,\"start\":111600},{\"end\":111607,\"start\":111606},{\"end\":111609,\"start\":111608},{\"end\":111876,\"start\":111875},{\"end\":112097,\"start\":112096},{\"end\":112105,\"start\":112104},{\"end\":112107,\"start\":112106},{\"end\":112392,\"start\":112391},{\"end\":112399,\"start\":112398},{\"end\":112406,\"start\":112405},{\"end\":112683,\"start\":112682},{\"end\":112692,\"start\":112691},{\"end\":113004,\"start\":113003},{\"end\":113006,\"start\":113005},{\"end\":113016,\"start\":113015},{\"end\":113027,\"start\":113026},{\"end\":113464,\"start\":113463},{\"end\":113468,\"start\":113465},{\"end\":113474,\"start\":113473},{\"end\":113671,\"start\":113670},{\"end\":113904,\"start\":113903}]", "bib_author_last_name": "[{\"end\":84831,\"start\":84825},{\"end\":85072,\"start\":85062},{\"end\":85306,\"start\":85296},{\"end\":85319,\"start\":85310},{\"end\":85331,\"start\":85323},{\"end\":85629,\"start\":85623},{\"end\":85790,\"start\":85785},{\"end\":85806,\"start\":85794},{\"end\":85819,\"start\":85810},{\"end\":86062,\"start\":86056},{\"end\":86377,\"start\":86373},{\"end\":86385,\"start\":86381},{\"end\":86396,\"start\":86391},{\"end\":86771,\"start\":86764},{\"end\":86779,\"start\":86775},{\"end\":86790,\"start\":86785},{\"end\":87060,\"start\":87055},{\"end\":87074,\"start\":87066},{\"end\":87084,\"start\":87080},{\"end\":87382,\"start\":87378},{\"end\":87392,\"start\":87386},{\"end\":87399,\"start\":87396},{\"end\":87410,\"start\":87403},{\"end\":87422,\"start\":87414},{\"end\":87752,\"start\":87747},{\"end\":88027,\"start\":88022},{\"end\":88040,\"start\":88031},{\"end\":88052,\"start\":88046},{\"end\":88317,\"start\":88314},{\"end\":88323,\"start\":88321},{\"end\":88331,\"start\":88327},{\"end\":88529,\"start\":88521},{\"end\":88537,\"start\":88533},{\"end\":88551,\"start\":88541},{\"end\":88562,\"start\":88555},{\"end\":88997,\"start\":88995},{\"end\":89009,\"start\":89003},{\"end\":89335,\"start\":89328},{\"end\":89346,\"start\":89339},{\"end\":89355,\"start\":89350},{\"end\":89620,\"start\":89615},{\"end\":89633,\"start\":89624},{\"end\":89644,\"start\":89637},{\"end\":89894,\"start\":89889},{\"end\":89905,\"start\":89898},{\"end\":90227,\"start\":90225},{\"end\":90241,\"start\":90233},{\"end\":90251,\"start\":90247},{\"end\":90262,\"start\":90257},{\"end\":90271,\"start\":90266},{\"end\":90285,\"start\":90275},{\"end\":90293,\"start\":90289},{\"end\":90306,\"start\":90299},{\"end\":90316,\"start\":90314},{\"end\":90862,\"start\":90855},{\"end\":90875,\"start\":90868},{\"end\":90883,\"start\":90881},{\"end\":90896,\"start\":90887},{\"end\":90908,\"start\":90902},{\"end\":90917,\"start\":90912},{\"end\":91348,\"start\":91339},{\"end\":91361,\"start\":91352},{\"end\":91372,\"start\":91365},{\"end\":91718,\"start\":91714},{\"end\":91726,\"start\":91722},{\"end\":91737,\"start\":91730},{\"end\":91755,\"start\":91741},{\"end\":92131,\"start\":92128},{\"end\":92138,\"start\":92135},{\"end\":92528,\"start\":92526},{\"end\":92534,\"start\":92532},{\"end\":92768,\"start\":92765},{\"end\":92776,\"start\":92772},{\"end\":92786,\"start\":92780},{\"end\":92798,\"start\":92790},{\"end\":93143,\"start\":93133},{\"end\":93155,\"start\":93149},{\"end\":93168,\"start\":93159},{\"end\":93177,\"start\":93172},{\"end\":93187,\"start\":93181},{\"end\":93528,\"start\":93522},{\"end\":93541,\"start\":93532},{\"end\":93552,\"start\":93545},{\"end\":93879,\"start\":93876},{\"end\":93887,\"start\":93883},{\"end\":93902,\"start\":93891},{\"end\":93914,\"start\":93906},{\"end\":94236,\"start\":94228},{\"end\":94249,\"start\":94240},{\"end\":94255,\"start\":94253},{\"end\":94581,\"start\":94578},{\"end\":94591,\"start\":94585},{\"end\":94600,\"start\":94595},{\"end\":95222,\"start\":95218},{\"end\":95230,\"start\":95226},{\"end\":95237,\"start\":95234},{\"end\":95243,\"start\":95241},{\"end\":95251,\"start\":95247},{\"end\":95673,\"start\":95667},{\"end\":95681,\"start\":95677},{\"end\":95692,\"start\":95685},{\"end\":95710,\"start\":95696},{\"end\":96135,\"start\":96133},{\"end\":96141,\"start\":96139},{\"end\":96149,\"start\":96145},{\"end\":96162,\"start\":96153},{\"end\":96170,\"start\":96166},{\"end\":96600,\"start\":96597},{\"end\":96606,\"start\":96604},{\"end\":96613,\"start\":96610},{\"end\":96940,\"start\":96937},{\"end\":96948,\"start\":96944},{\"end\":96957,\"start\":96952},{\"end\":97344,\"start\":97339},{\"end\":97352,\"start\":97348},{\"end\":97359,\"start\":97356},{\"end\":97374,\"start\":97363},{\"end\":97624,\"start\":97620},{\"end\":97632,\"start\":97628},{\"end\":97645,\"start\":97638},{\"end\":97927,\"start\":97921},{\"end\":97940,\"start\":97931},{\"end\":97952,\"start\":97944},{\"end\":97963,\"start\":97956},{\"end\":98182,\"start\":98179},{\"end\":98192,\"start\":98186},{\"end\":98201,\"start\":98196},{\"end\":98499,\"start\":98490},{\"end\":98512,\"start\":98503},{\"end\":98523,\"start\":98516},{\"end\":98770,\"start\":98765},{\"end\":98782,\"start\":98774},{\"end\":99057,\"start\":99054},{\"end\":99066,\"start\":99061},{\"end\":99080,\"start\":99070},{\"end\":99371,\"start\":99366},{\"end\":99384,\"start\":99375},{\"end\":99625,\"start\":99623},{\"end\":99634,\"start\":99629},{\"end\":99856,\"start\":99850},{\"end\":99864,\"start\":99860},{\"end\":99879,\"start\":99870},{\"end\":100163,\"start\":100159},{\"end\":100175,\"start\":100169},{\"end\":100187,\"start\":100181},{\"end\":100198,\"start\":100191},{\"end\":100528,\"start\":100525},{\"end\":100536,\"start\":100532},{\"end\":100544,\"start\":100540},{\"end\":100550,\"start\":100548},{\"end\":100557,\"start\":100554},{\"end\":100892,\"start\":100884},{\"end\":100899,\"start\":100896},{\"end\":100907,\"start\":100903},{\"end\":100918,\"start\":100911},{\"end\":101242,\"start\":101234},{\"end\":101250,\"start\":101246},{\"end\":101261,\"start\":101254},{\"end\":101510,\"start\":101503},{\"end\":101519,\"start\":101514},{\"end\":101533,\"start\":101523},{\"end\":101545,\"start\":101537},{\"end\":101828,\"start\":101822},{\"end\":101837,\"start\":101832},{\"end\":101846,\"start\":101841},{\"end\":102200,\"start\":102192},{\"end\":102215,\"start\":102204},{\"end\":102463,\"start\":102459},{\"end\":102474,\"start\":102469},{\"end\":102866,\"start\":102864},{\"end\":102872,\"start\":102870},{\"end\":102880,\"start\":102876},{\"end\":103208,\"start\":103203},{\"end\":103220,\"start\":103212},{\"end\":103230,\"start\":103224},{\"end\":103242,\"start\":103234},{\"end\":103624,\"start\":103622},{\"end\":103639,\"start\":103628},{\"end\":103985,\"start\":103981},{\"end\":103996,\"start\":103991},{\"end\":104328,\"start\":104312},{\"end\":104335,\"start\":104332},{\"end\":104347,\"start\":104339},{\"end\":104716,\"start\":104712},{\"end\":104722,\"start\":104720},{\"end\":104730,\"start\":104726},{\"end\":105000,\"start\":104997},{\"end\":105009,\"start\":105004},{\"end\":105016,\"start\":105013},{\"end\":105022,\"start\":105020},{\"end\":105031,\"start\":105026},{\"end\":105038,\"start\":105035},{\"end\":105292,\"start\":105283},{\"end\":105305,\"start\":105296},{\"end\":105317,\"start\":105309},{\"end\":105326,\"start\":105321},{\"end\":105337,\"start\":105330},{\"end\":105585,\"start\":105578},{\"end\":105596,\"start\":105589},{\"end\":105893,\"start\":105889},{\"end\":105900,\"start\":105897},{\"end\":106220,\"start\":106217},{\"end\":106226,\"start\":106224},{\"end\":106234,\"start\":106230},{\"end\":106637,\"start\":106634},{\"end\":106647,\"start\":106643},{\"end\":106654,\"start\":106651},{\"end\":106922,\"start\":106920},{\"end\":106939,\"start\":106928},{\"end\":106948,\"start\":106943},{\"end\":107194,\"start\":107189},{\"end\":107204,\"start\":107198},{\"end\":107214,\"start\":107208},{\"end\":107225,\"start\":107218},{\"end\":107494,\"start\":107484},{\"end\":107504,\"start\":107498},{\"end\":107609,\"start\":107603},{\"end\":107776,\"start\":107773},{\"end\":107782,\"start\":107780},{\"end\":107792,\"start\":107786},{\"end\":107987,\"start\":107983},{\"end\":107994,\"start\":107991},{\"end\":108259,\"start\":108254},{\"end\":108269,\"start\":108263},{\"end\":108564,\"start\":108560},{\"end\":108573,\"start\":108568},{\"end\":108582,\"start\":108577},{\"end\":108743,\"start\":108739},{\"end\":108753,\"start\":108745},{\"end\":108931,\"start\":108927},{\"end\":109189,\"start\":109185},{\"end\":109483,\"start\":109479},{\"end\":109491,\"start\":109487},{\"end\":109881,\"start\":109876},{\"end\":109892,\"start\":109885},{\"end\":109908,\"start\":109898},{\"end\":110232,\"start\":110230},{\"end\":110242,\"start\":110236},{\"end\":110628,\"start\":110624},{\"end\":110637,\"start\":110632},{\"end\":110645,\"start\":110641},{\"end\":110652,\"start\":110649},{\"end\":111095,\"start\":111091},{\"end\":111106,\"start\":111099},{\"end\":111374,\"start\":111369},{\"end\":111604,\"start\":111602},{\"end\":111616,\"start\":111610},{\"end\":111885,\"start\":111877},{\"end\":112102,\"start\":112098},{\"end\":112113,\"start\":112108},{\"end\":112396,\"start\":112393},{\"end\":112403,\"start\":112400},{\"end\":112416,\"start\":112407},{\"end\":112689,\"start\":112684},{\"end\":112705,\"start\":112693},{\"end\":113013,\"start\":113007},{\"end\":113024,\"start\":113017},{\"end\":113037,\"start\":113028},{\"end\":113471,\"start\":113469},{\"end\":113480,\"start\":113475},{\"end\":113681,\"start\":113672},{\"end\":113918,\"start\":113905}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":85007,\"start\":84823},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":3091},\"end\":85222,\"start\":85009},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":47128183},\"end\":85583,\"start\":85224},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":207178999},\"end\":85781,\"start\":85585},{\"attributes\":{\"id\":\"b4\"},\"end\":85999,\"start\":85783},{\"attributes\":{\"id\":\"b5\"},\"end\":86210,\"start\":86001},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":211069157},\"end\":86688,\"start\":86212},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":218674507},\"end\":86999,\"start\":86690},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":3205159},\"end\":87271,\"start\":87001},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":51789432},\"end\":87687,\"start\":87273},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":207179167},\"end\":87930,\"start\":87689},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":9297955},\"end\":88275,\"start\":87932},{\"attributes\":{\"doi\":\"arXiv:2104.11375\",\"id\":\"b12\"},\"end\":88466,\"start\":88277},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":12690337},\"end\":88908,\"start\":88468},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":15981366},\"end\":89292,\"start\":88910},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":55899582},\"end\":89553,\"start\":89294},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":17487113},\"end\":89792,\"start\":89555},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":13009312},\"end\":90053,\"start\":89794},{\"attributes\":{\"id\":\"b18\"},\"end\":90157,\"start\":90055},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":4614646},\"end\":90780,\"start\":90159},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":14999259},\"end\":91263,\"start\":90782},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":28527385},\"end\":91637,\"start\":91265},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":49397275},\"end\":92042,\"start\":91639},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":52287318},\"end\":92464,\"start\":92044},{\"attributes\":{\"doi\":\"arXiv:1804.10140\",\"id\":\"b24\"},\"end\":92681,\"start\":92466},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":49208440},\"end\":93070,\"start\":92683},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":49657908},\"end\":93453,\"start\":93072},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":3473997},\"end\":93798,\"start\":93455},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":3708326},\"end\":94185,\"start\":93800},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":4312831},\"end\":94518,\"start\":94187},{\"attributes\":{\"doi\":\"arXiv:1805.10032\",\"id\":\"b30\"},\"end\":94761,\"start\":94520},{\"attributes\":{\"doi\":\"arXiv:1802.10116\",\"id\":\"b31\"},\"end\":94909,\"start\":94763},{\"attributes\":{\"doi\":\"arXiv:1805.09682\",\"id\":\"b32\"},\"end\":95125,\"start\":94911},{\"attributes\":{\"id\":\"b33\",\"matched_paper_id\":174799264},\"end\":95578,\"start\":95127},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":198968245},\"end\":96023,\"start\":95580},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":53250530},\"end\":96510,\"start\":96025},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":67856703},\"end\":96871,\"start\":96512},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":145836044},\"end\":97277,\"start\":96873},{\"attributes\":{\"doi\":\"arXiv:1906.06629\",\"id\":\"b38\"},\"end\":97553,\"start\":97279},{\"attributes\":{\"id\":\"b39\"},\"end\":97872,\"start\":97555},{\"attributes\":{\"doi\":\"arXiv:1905.03853\",\"id\":\"b40\"},\"end\":98136,\"start\":97874},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":203591932},\"end\":98427,\"start\":98138},{\"attributes\":{\"doi\":\"arXiv:1905.04374\",\"id\":\"b42\"},\"end\":98700,\"start\":98429},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":6489200},\"end\":98966,\"start\":98702},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":11542801},\"end\":99302,\"start\":98968},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":8361755},\"end\":99569,\"start\":99304},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":44061760},\"end\":99791,\"start\":99571},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":16514400},\"end\":100069,\"start\":99793},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":10667549},\"end\":100442,\"start\":100071},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":5642927},\"end\":100781,\"start\":100444},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":1659799},\"end\":101181,\"start\":100783},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":17060613},\"end\":101446,\"start\":101183},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":11287513},\"end\":101757,\"start\":101448},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":17529835},\"end\":102138,\"start\":101759},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":14712012},\"end\":102399,\"start\":102140},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":10307078},\"end\":102780,\"start\":102401},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":49244108},\"end\":103053,\"start\":102782},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":53407867},\"end\":103518,\"start\":103055},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":53086787},\"end\":103890,\"start\":103520},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":30921068},\"end\":104231,\"start\":103892},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":214605808},\"end\":104614,\"start\":104233},{\"attributes\":{\"id\":\"b61\",\"matched_paper_id\":218613799},\"end\":104930,\"start\":104616},{\"attributes\":{\"doi\":\"arXiv:2002.08569\",\"id\":\"b62\"},\"end\":105228,\"start\":104932},{\"attributes\":{\"doi\":\"arXiv:2008.00742v3\",\"id\":\"b63\"},\"end\":105531,\"start\":105230},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":3261082},\"end\":105840,\"start\":105533},{\"attributes\":{\"id\":\"b65\",\"matched_paper_id\":2772640},\"end\":106080,\"start\":105842},{\"attributes\":{\"id\":\"b66\",\"matched_paper_id\":221089684},\"end\":106542,\"start\":106082},{\"attributes\":{\"doi\":\"arXiv:2008.07428\",\"id\":\"b67\"},\"end\":106848,\"start\":106544},{\"attributes\":{\"doi\":\"arXiv:2202.01545\",\"id\":\"b68\"},\"end\":107128,\"start\":106850},{\"attributes\":{\"id\":\"b69\",\"matched_paper_id\":14542261},\"end\":107425,\"start\":107130},{\"attributes\":{\"id\":\"b70\"},\"end\":107597,\"start\":107427},{\"attributes\":{\"id\":\"b71\"},\"end\":107730,\"start\":107599},{\"attributes\":{\"doi\":\"arXiv:1510.06096\",\"id\":\"b72\"},\"end\":107933,\"start\":107732},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":51984738},\"end\":108189,\"start\":107935},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":219769943},\"end\":108469,\"start\":108191},{\"attributes\":{\"id\":\"b75\",\"matched_paper_id\":15219943},\"end\":108737,\"start\":108471},{\"attributes\":{\"id\":\"b76\"},\"end\":108878,\"start\":108739},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":122142994},\"end\":109098,\"start\":108880},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":4949829},\"end\":109422,\"start\":109100},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":203605988},\"end\":109783,\"start\":109424},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":1617976},\"end\":110139,\"start\":109785},{\"attributes\":{\"id\":\"b81\",\"matched_paper_id\":29552283},\"end\":110492,\"start\":110141},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":202542725},\"end\":111001,\"start\":110494},{\"attributes\":{\"id\":\"b83\",\"matched_paper_id\":219981321},\"end\":111363,\"start\":111003},{\"attributes\":{\"id\":\"b84\"},\"end\":111488,\"start\":111365},{\"attributes\":{\"doi\":\"arXiv:1511.01821\",\"id\":\"b85\"},\"end\":111822,\"start\":111490},{\"attributes\":{\"id\":\"b86\"},\"end\":112034,\"start\":111824},{\"attributes\":{\"doi\":\"arXiv:1908.08098v1\",\"id\":\"b87\"},\"end\":112335,\"start\":112036},{\"attributes\":{\"id\":\"b88\",\"matched_paper_id\":13691873},\"end\":112602,\"start\":112337},{\"attributes\":{\"id\":\"b89\",\"matched_paper_id\":52987994},\"end\":112926,\"start\":112604},{\"attributes\":{\"id\":\"b90\",\"matched_paper_id\":53081067},\"end\":113385,\"start\":112928},{\"attributes\":{\"id\":\"b91\"},\"end\":113605,\"start\":113387},{\"attributes\":{\"id\":\"b92\",\"matched_paper_id\":121341745},\"end\":113852,\"start\":113607},{\"attributes\":{\"id\":\"b93\",\"matched_paper_id\":14625507},\"end\":114097,\"start\":113854}]", "bib_title": "[{\"end\":85058,\"start\":85009},{\"end\":85290,\"start\":85224},{\"end\":85619,\"start\":85585},{\"end\":86369,\"start\":86212},{\"end\":86760,\"start\":86690},{\"end\":87049,\"start\":87001},{\"end\":87374,\"start\":87273},{\"end\":87741,\"start\":87689},{\"end\":88018,\"start\":87932},{\"end\":88517,\"start\":88468},{\"end\":88991,\"start\":88910},{\"end\":89324,\"start\":89294},{\"end\":89611,\"start\":89555},{\"end\":89885,\"start\":89794},{\"end\":90221,\"start\":90159},{\"end\":90851,\"start\":90782},{\"end\":91335,\"start\":91265},{\"end\":91710,\"start\":91639},{\"end\":92124,\"start\":92044},{\"end\":92761,\"start\":92683},{\"end\":93129,\"start\":93072},{\"end\":93516,\"start\":93455},{\"end\":93872,\"start\":93800},{\"end\":94224,\"start\":94187},{\"end\":95214,\"start\":95127},{\"end\":95663,\"start\":95580},{\"end\":96129,\"start\":96025},{\"end\":96593,\"start\":96512},{\"end\":96933,\"start\":96873},{\"end\":97616,\"start\":97555},{\"end\":98175,\"start\":98138},{\"end\":98761,\"start\":98702},{\"end\":99048,\"start\":98968},{\"end\":99362,\"start\":99304},{\"end\":99619,\"start\":99571},{\"end\":99844,\"start\":99793},{\"end\":100153,\"start\":100071},{\"end\":100521,\"start\":100444},{\"end\":100880,\"start\":100783},{\"end\":101230,\"start\":101183},{\"end\":101497,\"start\":101448},{\"end\":101816,\"start\":101759},{\"end\":102188,\"start\":102140},{\"end\":102455,\"start\":102401},{\"end\":102860,\"start\":102782},{\"end\":103199,\"start\":103055},{\"end\":103618,\"start\":103520},{\"end\":103977,\"start\":103892},{\"end\":104308,\"start\":104233},{\"end\":104708,\"start\":104616},{\"end\":105572,\"start\":105533},{\"end\":105885,\"start\":105842},{\"end\":106213,\"start\":106082},{\"end\":107185,\"start\":107130},{\"end\":107979,\"start\":107935},{\"end\":108250,\"start\":108191},{\"end\":108556,\"start\":108471},{\"end\":108923,\"start\":108880},{\"end\":109181,\"start\":109100},{\"end\":109475,\"start\":109424},{\"end\":109870,\"start\":109785},{\"end\":110226,\"start\":110141},{\"end\":110620,\"start\":110494},{\"end\":111087,\"start\":111003},{\"end\":112389,\"start\":112337},{\"end\":112680,\"start\":112604},{\"end\":113001,\"start\":112928},{\"end\":113668,\"start\":113607},{\"end\":113901,\"start\":113854}]", "bib_author": "[{\"end\":84833,\"start\":84823},{\"end\":85074,\"start\":85060},{\"end\":85308,\"start\":85292},{\"end\":85321,\"start\":85308},{\"end\":85333,\"start\":85321},{\"end\":85631,\"start\":85621},{\"end\":85792,\"start\":85783},{\"end\":85808,\"start\":85792},{\"end\":85821,\"start\":85808},{\"end\":86064,\"start\":86052},{\"end\":86379,\"start\":86371},{\"end\":86387,\"start\":86379},{\"end\":86398,\"start\":86387},{\"end\":86773,\"start\":86762},{\"end\":86781,\"start\":86773},{\"end\":86792,\"start\":86781},{\"end\":87062,\"start\":87051},{\"end\":87076,\"start\":87062},{\"end\":87086,\"start\":87076},{\"end\":87384,\"start\":87376},{\"end\":87394,\"start\":87384},{\"end\":87401,\"start\":87394},{\"end\":87412,\"start\":87401},{\"end\":87424,\"start\":87412},{\"end\":87754,\"start\":87743},{\"end\":88029,\"start\":88020},{\"end\":88042,\"start\":88029},{\"end\":88054,\"start\":88042},{\"end\":88319,\"start\":88312},{\"end\":88325,\"start\":88319},{\"end\":88333,\"start\":88325},{\"end\":88531,\"start\":88519},{\"end\":88539,\"start\":88531},{\"end\":88553,\"start\":88539},{\"end\":88564,\"start\":88553},{\"end\":88999,\"start\":88993},{\"end\":89011,\"start\":88999},{\"end\":89337,\"start\":89326},{\"end\":89348,\"start\":89337},{\"end\":89357,\"start\":89348},{\"end\":89622,\"start\":89613},{\"end\":89635,\"start\":89622},{\"end\":89646,\"start\":89635},{\"end\":89896,\"start\":89887},{\"end\":89907,\"start\":89896},{\"end\":90229,\"start\":90223},{\"end\":90243,\"start\":90229},{\"end\":90253,\"start\":90243},{\"end\":90264,\"start\":90253},{\"end\":90273,\"start\":90264},{\"end\":90287,\"start\":90273},{\"end\":90295,\"start\":90287},{\"end\":90308,\"start\":90295},{\"end\":90318,\"start\":90308},{\"end\":90864,\"start\":90853},{\"end\":90877,\"start\":90864},{\"end\":90885,\"start\":90877},{\"end\":90898,\"start\":90885},{\"end\":90910,\"start\":90898},{\"end\":90919,\"start\":90910},{\"end\":91350,\"start\":91337},{\"end\":91363,\"start\":91350},{\"end\":91374,\"start\":91363},{\"end\":91720,\"start\":91712},{\"end\":91728,\"start\":91720},{\"end\":91739,\"start\":91728},{\"end\":91757,\"start\":91739},{\"end\":92133,\"start\":92126},{\"end\":92140,\"start\":92133},{\"end\":92530,\"start\":92524},{\"end\":92536,\"start\":92530},{\"end\":92770,\"start\":92763},{\"end\":92778,\"start\":92770},{\"end\":92788,\"start\":92778},{\"end\":92800,\"start\":92788},{\"end\":93145,\"start\":93131},{\"end\":93157,\"start\":93145},{\"end\":93170,\"start\":93157},{\"end\":93179,\"start\":93170},{\"end\":93189,\"start\":93179},{\"end\":93530,\"start\":93518},{\"end\":93543,\"start\":93530},{\"end\":93554,\"start\":93543},{\"end\":93881,\"start\":93874},{\"end\":93889,\"start\":93881},{\"end\":93904,\"start\":93889},{\"end\":93916,\"start\":93904},{\"end\":94238,\"start\":94226},{\"end\":94251,\"start\":94238},{\"end\":94257,\"start\":94251},{\"end\":94583,\"start\":94576},{\"end\":94593,\"start\":94583},{\"end\":94602,\"start\":94593},{\"end\":95224,\"start\":95216},{\"end\":95232,\"start\":95224},{\"end\":95239,\"start\":95232},{\"end\":95245,\"start\":95239},{\"end\":95253,\"start\":95245},{\"end\":95675,\"start\":95665},{\"end\":95683,\"start\":95675},{\"end\":95694,\"start\":95683},{\"end\":95712,\"start\":95694},{\"end\":96137,\"start\":96131},{\"end\":96143,\"start\":96137},{\"end\":96151,\"start\":96143},{\"end\":96164,\"start\":96151},{\"end\":96172,\"start\":96164},{\"end\":96602,\"start\":96595},{\"end\":96608,\"start\":96602},{\"end\":96615,\"start\":96608},{\"end\":96942,\"start\":96935},{\"end\":96950,\"start\":96942},{\"end\":96959,\"start\":96950},{\"end\":97346,\"start\":97337},{\"end\":97354,\"start\":97346},{\"end\":97361,\"start\":97354},{\"end\":97376,\"start\":97361},{\"end\":97626,\"start\":97618},{\"end\":97634,\"start\":97626},{\"end\":97647,\"start\":97634},{\"end\":97929,\"start\":97915},{\"end\":97942,\"start\":97929},{\"end\":97954,\"start\":97942},{\"end\":97965,\"start\":97954},{\"end\":98184,\"start\":98177},{\"end\":98194,\"start\":98184},{\"end\":98203,\"start\":98194},{\"end\":98501,\"start\":98485},{\"end\":98514,\"start\":98501},{\"end\":98525,\"start\":98514},{\"end\":98772,\"start\":98763},{\"end\":98784,\"start\":98772},{\"end\":99059,\"start\":99050},{\"end\":99068,\"start\":99059},{\"end\":99082,\"start\":99068},{\"end\":99373,\"start\":99364},{\"end\":99386,\"start\":99373},{\"end\":99627,\"start\":99621},{\"end\":99636,\"start\":99627},{\"end\":99858,\"start\":99846},{\"end\":99866,\"start\":99858},{\"end\":99881,\"start\":99866},{\"end\":100165,\"start\":100155},{\"end\":100177,\"start\":100165},{\"end\":100189,\"start\":100177},{\"end\":100200,\"start\":100189},{\"end\":100530,\"start\":100523},{\"end\":100538,\"start\":100530},{\"end\":100546,\"start\":100538},{\"end\":100552,\"start\":100546},{\"end\":100559,\"start\":100552},{\"end\":100894,\"start\":100882},{\"end\":100901,\"start\":100894},{\"end\":100909,\"start\":100901},{\"end\":100920,\"start\":100909},{\"end\":101244,\"start\":101232},{\"end\":101252,\"start\":101244},{\"end\":101263,\"start\":101252},{\"end\":101512,\"start\":101499},{\"end\":101521,\"start\":101512},{\"end\":101535,\"start\":101521},{\"end\":101547,\"start\":101535},{\"end\":101830,\"start\":101818},{\"end\":101839,\"start\":101830},{\"end\":101848,\"start\":101839},{\"end\":102202,\"start\":102190},{\"end\":102217,\"start\":102202},{\"end\":102465,\"start\":102457},{\"end\":102476,\"start\":102465},{\"end\":102868,\"start\":102862},{\"end\":102874,\"start\":102868},{\"end\":102882,\"start\":102874},{\"end\":103210,\"start\":103201},{\"end\":103222,\"start\":103210},{\"end\":103232,\"start\":103222},{\"end\":103244,\"start\":103232},{\"end\":103626,\"start\":103620},{\"end\":103641,\"start\":103626},{\"end\":103987,\"start\":103979},{\"end\":103998,\"start\":103987},{\"end\":104330,\"start\":104310},{\"end\":104337,\"start\":104330},{\"end\":104349,\"start\":104337},{\"end\":104718,\"start\":104710},{\"end\":104724,\"start\":104718},{\"end\":104732,\"start\":104724},{\"end\":105002,\"start\":104995},{\"end\":105011,\"start\":105002},{\"end\":105018,\"start\":105011},{\"end\":105024,\"start\":105018},{\"end\":105033,\"start\":105024},{\"end\":105040,\"start\":105033},{\"end\":105294,\"start\":105278},{\"end\":105307,\"start\":105294},{\"end\":105319,\"start\":105307},{\"end\":105328,\"start\":105319},{\"end\":105339,\"start\":105328},{\"end\":105587,\"start\":105574},{\"end\":105598,\"start\":105587},{\"end\":105895,\"start\":105887},{\"end\":105902,\"start\":105895},{\"end\":106222,\"start\":106215},{\"end\":106228,\"start\":106222},{\"end\":106236,\"start\":106228},{\"end\":106639,\"start\":106632},{\"end\":106649,\"start\":106639},{\"end\":106656,\"start\":106649},{\"end\":106924,\"start\":106918},{\"end\":106941,\"start\":106924},{\"end\":106950,\"start\":106941},{\"end\":107196,\"start\":107187},{\"end\":107206,\"start\":107196},{\"end\":107216,\"start\":107206},{\"end\":107227,\"start\":107216},{\"end\":107496,\"start\":107482},{\"end\":107506,\"start\":107496},{\"end\":107611,\"start\":107599},{\"end\":107778,\"start\":107771},{\"end\":107784,\"start\":107778},{\"end\":107794,\"start\":107784},{\"end\":107989,\"start\":107981},{\"end\":107996,\"start\":107989},{\"end\":108261,\"start\":108252},{\"end\":108271,\"start\":108261},{\"end\":108566,\"start\":108558},{\"end\":108575,\"start\":108566},{\"end\":108584,\"start\":108575},{\"end\":108745,\"start\":108739},{\"end\":108755,\"start\":108745},{\"end\":108933,\"start\":108925},{\"end\":109191,\"start\":109183},{\"end\":109485,\"start\":109477},{\"end\":109493,\"start\":109485},{\"end\":109883,\"start\":109872},{\"end\":109894,\"start\":109883},{\"end\":109910,\"start\":109894},{\"end\":110234,\"start\":110228},{\"end\":110244,\"start\":110234},{\"end\":110630,\"start\":110622},{\"end\":110639,\"start\":110630},{\"end\":110647,\"start\":110639},{\"end\":110654,\"start\":110647},{\"end\":111097,\"start\":111089},{\"end\":111108,\"start\":111097},{\"end\":111376,\"start\":111365},{\"end\":111606,\"start\":111600},{\"end\":111618,\"start\":111606},{\"end\":111887,\"start\":111875},{\"end\":112104,\"start\":112096},{\"end\":112115,\"start\":112104},{\"end\":112398,\"start\":112391},{\"end\":112405,\"start\":112398},{\"end\":112418,\"start\":112405},{\"end\":112691,\"start\":112682},{\"end\":112707,\"start\":112691},{\"end\":113015,\"start\":113003},{\"end\":113026,\"start\":113015},{\"end\":113039,\"start\":113026},{\"end\":113473,\"start\":113463},{\"end\":113482,\"start\":113473},{\"end\":113683,\"start\":113670},{\"end\":113920,\"start\":113903}]", "bib_venue": "[{\"end\":84888,\"start\":84876},{\"end\":85867,\"start\":85854},{\"end\":88706,\"start\":88639},{\"end\":89109,\"start\":89064},{\"end\":89921,\"start\":89918},{\"end\":90482,\"start\":90397},{\"end\":91039,\"start\":90983},{\"end\":91452,\"start\":91419},{\"end\":91845,\"start\":91805},{\"end\":92268,\"start\":92210},{\"end\":92874,\"start\":92841},{\"end\":93261,\"start\":93229},{\"end\":93626,\"start\":93594},{\"end\":93990,\"start\":93957},{\"end\":94359,\"start\":94314},{\"end\":95355,\"start\":95310},{\"end\":95814,\"start\":95769},{\"end\":96264,\"start\":96222},{\"end\":96697,\"start\":96660},{\"end\":97087,\"start\":97029},{\"end\":98277,\"start\":98244},{\"end\":101960,\"start\":101908},{\"end\":102604,\"start\":102546},{\"end\":104423,\"start\":104390},{\"end\":106310,\"start\":106277},{\"end\":107644,\"start\":107632},{\"end\":108600,\"start\":108596},{\"end\":109615,\"start\":109558},{\"end\":110318,\"start\":110285},{\"end\":110752,\"start\":110707},{\"end\":111182,\"start\":111149},{\"end\":111413,\"start\":111395},{\"end\":113141,\"start\":113096},{\"end\":84874,\"start\":84833},{\"end\":85095,\"start\":85074},{\"end\":85382,\"start\":85333},{\"end\":85663,\"start\":85631},{\"end\":85852,\"start\":85821},{\"end\":86050,\"start\":86001},{\"end\":86422,\"start\":86398},{\"end\":86815,\"start\":86792},{\"end\":87110,\"start\":87086},{\"end\":87456,\"start\":87424},{\"end\":87786,\"start\":87754},{\"end\":88077,\"start\":88054},{\"end\":88310,\"start\":88277},{\"end\":88637,\"start\":88564},{\"end\":89062,\"start\":89011},{\"end\":89398,\"start\":89357},{\"end\":89655,\"start\":89646},{\"end\":89916,\"start\":89907},{\"end\":90096,\"start\":90055},{\"end\":90395,\"start\":90318},{\"end\":90981,\"start\":90919},{\"end\":91417,\"start\":91374},{\"end\":91803,\"start\":91757},{\"end\":92208,\"start\":92140},{\"end\":92522,\"start\":92466},{\"end\":92839,\"start\":92800},{\"end\":93227,\"start\":93189},{\"end\":93592,\"start\":93554},{\"end\":93955,\"start\":93916},{\"end\":94312,\"start\":94257},{\"end\":94574,\"start\":94520},{\"end\":94813,\"start\":94779},{\"end\":94978,\"start\":94911},{\"end\":95308,\"start\":95253},{\"end\":95767,\"start\":95712},{\"end\":96220,\"start\":96172},{\"end\":96658,\"start\":96615},{\"end\":97027,\"start\":96959},{\"end\":97335,\"start\":97279},{\"end\":97686,\"start\":97647},{\"end\":97913,\"start\":97874},{\"end\":98242,\"start\":98203},{\"end\":98483,\"start\":98429},{\"end\":98810,\"start\":98784},{\"end\":99107,\"start\":99082},{\"end\":99412,\"start\":99386},{\"end\":99660,\"start\":99636},{\"end\":99907,\"start\":99881},{\"end\":100226,\"start\":100200},{\"end\":100585,\"start\":100559},{\"end\":100957,\"start\":100920},{\"end\":101289,\"start\":101263},{\"end\":101575,\"start\":101547},{\"end\":101906,\"start\":101848},{\"end\":102243,\"start\":102217},{\"end\":102544,\"start\":102476},{\"end\":102896,\"start\":102882},{\"end\":103261,\"start\":103244},{\"end\":103679,\"start\":103641},{\"end\":104035,\"start\":103998},{\"end\":104388,\"start\":104349},{\"end\":104749,\"start\":104732},{\"end\":104993,\"start\":104932},{\"end\":105276,\"start\":105230},{\"end\":105666,\"start\":105598},{\"end\":105940,\"start\":105902},{\"end\":106275,\"start\":106236},{\"end\":106630,\"start\":106544},{\"end\":106916,\"start\":106850},{\"end\":107250,\"start\":107227},{\"end\":107480,\"start\":107427},{\"end\":107630,\"start\":107611},{\"end\":107769,\"start\":107732},{\"end\":108038,\"start\":107996},{\"end\":108309,\"start\":108271},{\"end\":108594,\"start\":108584},{\"end\":108798,\"start\":108755},{\"end\":108967,\"start\":108933},{\"end\":109238,\"start\":109191},{\"end\":109556,\"start\":109493},{\"end\":109936,\"start\":109910},{\"end\":110283,\"start\":110244},{\"end\":110705,\"start\":110654},{\"end\":111147,\"start\":111108},{\"end\":111393,\"start\":111376},{\"end\":111598,\"start\":111490},{\"end\":111873,\"start\":111824},{\"end\":112094,\"start\":112036},{\"end\":112442,\"start\":112418},{\"end\":112741,\"start\":112707},{\"end\":113094,\"start\":113039},{\"end\":113461,\"start\":113387},{\"end\":113706,\"start\":113683},{\"end\":113953,\"start\":113920}]"}}}, "year": 2023, "month": 12, "day": 17}