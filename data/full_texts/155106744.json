{"id": 155106744, "updated": "2023-10-27 22:59:35.907", "metadata": {"title": "HDCluster: An Accurate Clustering Using Brain-Inspired High-Dimensional Computing", "authors": "[{\"first\":\"Mohsen\",\"last\":\"Imani\",\"middle\":[]},{\"first\":\"Yeseong\",\"last\":\"Kim\",\"middle\":[]},{\"first\":\"Thomas\",\"last\":\"Worley\",\"middle\":[]},{\"first\":\"Saransh\",\"last\":\"Gupta\",\"middle\":[]},{\"first\":\"Tajana\",\"last\":\"Rosing\",\"middle\":[]}]", "venue": "2019 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "journal": "2019 Design, Automation & Test in Europe Conference & Exhibition (DATE)", "publication_date": {"year": 2019, "month": null, "day": null}, "abstract": "Internet of things has increased the rate of data generation. Clustering is one of the most important tasks in this domain to find the latent correlation between data. However, performing today\u2019s clustering tasks is often inefficient due to the data movement cost between cores and memory. We propose HDCluster, a brain-inspired unsupervised learning algorithm which clusters input data in a high-dimensional space by fully mapping and processing in memory. Instead of clustering input data in either fixed-point or floating-point representation, HDCluster maps data to vectors with dimension in thousands, called hypervectors, to cluster them. Our evaluation shows that HDCluster provides better clustering quality for the tasks that involve a large amount of data while providing a potential for accelerating in a memory-centric architecture.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2946432794", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/date/ImaniKWGR19", "doi": "10.23919/date.2019.8715147"}}, "content": {"source": {"pdf_hash": "7f3f9f8be78eb5d124ca9fbd8a944ec884cb185b", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "f8a5697e63f702c682c5fc31c5b1326c450baacb", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/7f3f9f8be78eb5d124ca9fbd8a944ec884cb185b.txt", "contents": "\nHDCluster: An Accurate Clustering Using Brain-Inspired High-Dimensional Computing\n\n\nMohsen Imani moimani@ucsd.edu \nComputer Science and Engineering Department\nUC San Diego\nLa Jolla92093CAUSA\n\nYeseong Kim \nComputer Science and Engineering Department\nUC San Diego\nLa Jolla92093CAUSA\n\nThomas Worley tworley@ucsd.edu \nComputer Science and Engineering Department\nUC San Diego\nLa Jolla92093CAUSA\n\nSaransh Gupta sgupta@ucsd.edu \nComputer Science and Engineering Department\nUC San Diego\nLa Jolla92093CAUSA\n\nTajana Rosing tajana@ucsd.edu \nComputer Science and Engineering Department\nUC San Diego\nLa Jolla92093CAUSA\n\nHDCluster: An Accurate Clustering Using Brain-Inspired High-Dimensional Computing\nIndex Terms-Hyperdimension computingClusteringBrain- inspired computing\nInternet of things has increased the rate of data generation. Clustering is one of the most important tasks in this domain to find the latent correlation between data. However, performing today's clustering tasks is often inefficient due to the data movement cost between cores and memory. We propose HDCluster, a brain-inspired unsupervised learning algorithm which clusters input data in a high-dimensional space by fully mapping and processing in memory. Instead of clustering input data in either fixed-point or floating-point representation, HDCluster maps data to vectors with dimension in thousands, called hypervectors, to cluster them. Our evaluation shows that HDCluster provides better clustering quality for the tasks that involve a large amount of data while providing a potential for accelerating in a memory-centric architecture.\n\nI. INTRODUCTION\n\nInternet of things (IoT) significantly increases the number of devices around the world. Recent studies report that more than 25 billion connected smart devices exist in 2015. [1] This number is expected to be doubled by 2020. [2] As the large network of connected devices generates a huge amount of data, machine learning gains popularity as an autonomous solution that extracts the useful information and learns patterns from the collected data [3]- [5]. However, the large amount of data dominates the processing capability of the current computing systems [6]- [8]. This inefficiency is mainly due to significant data movement costs between processing cores and memory. For example, to perform clustering tasks which are one of the most important unsupervised learning, [9], [10] the processors expensively compute the similarity between data points by fetching every point from the memory. A traditional solution is to migrate this issue by running the tasks on a cloud, but transferring large amount of data incurs significant congestion on the network. In addition, this may inherently lead to security and privacy issues in many applications. Thus, going toward IoT, it is crucial to have a light-weight clustering technique with adequate architectural supports which can efficiently run even on end-node devices.\n\nBrain-inspired Hyperdimensional (HD) computing is based on understanding the fact that brains compute with patterns of neural activity which are not readily associated with numbers [11]. However, due to the very large size of the brain's circuits, such neural activity patterns can only be modeled with points of high-dimensional space (e.g., D=10,000). Operations on hypervectors can be combined into interesting computational behavior with unique features that make them robust and efficient. HD computing builds upon a well-defined set of operations with random HD vectors, is extremely robust in the presence of failures, and offers a complete computational paradigm that is easily applied to learning problems [11], [12]. Its main differentiation from other paradigms is that data are represented as approximate patterns, which can favorably scale for many learning applications.\n\nIn this paper, we present a new clustering algorithm which maps a large amount of original data to a hardware-friendly high-dimension space and performs the clustering tasks by using Hyperdimensional (HD) computing. HD computing is an alternative computational model which emulates cognition tasks by computing with vectors in high-dimensional space, called hypervectors. A hypervector has the dimensionality in thousands (e.g., D=10,000) to mimic neural activity [11]. Since the elements of hypervectors are independent in processing, we can design a fully-parallelized hardware architecture that handles the hypervectors. In this work, we show how the proposed HDCluster encodes the original data to the hypervectors without losing the necessary information and performs the clustering tasks with well-defined linear algebra for the data types in the high-dimension space. To the best of our knowledge, HDCluster is the first design which processes the clustering tasks with the hypervectors. Our evaluation shows that HDCluster provides better clustering quality for the tasks that involve a large amount of data while providing a potential for accelerating in a memory-centric architecture.\n\n\nII. HDCLUSTER ALGORITHM\n\n\nA. HDCluster Overview\n\nWe propose HDCluster, a brain-inspired clustering algorithm which clusters input data into a high-dimensional space. Instead of clustering input data in either fixed-point or floating-point representations, HDCluster maps data to vectors with thousands of dimensions, called hypervectors, and then clusters them using concrete linear algebra. The well-defined set of HD operations is known to be extremely robust in the presence of failures, and offers a complete computational paradigm that is easily applied to learning problems such as analogy-based reasoning, sequence memory, language recognition, biosignal processing, speech recognition, and prediction from multimodal sensor fusion [13]- [20]. Figure 1 illustrates an overview of HDCluster. As the first step, HDCluster encodes data into high-dimensional space and then applies clustering tasks to the encoded data ( \u2022 1 ).\n\nThe clustering procedure starts with the initial centers of each cluster. For each iteration, HDCluster identifies which center is the most similar to each data point. The identified centers are stored as tags ( \u2022 2 ). Then, HDCluster updates the centers by calculating the average of the data points whose tags are the same ( \u2022 3 ). The iterations are repeated to obtain the converged cluster centers ( \u2022 4 ). In the next section, we show the details of the clustering procedure.\n\n\nB. Encoding into HD Space\n\nThe first step of HDCluster is to encode input data into the hypervectors, where an original data point has n features, i.e., v = v 1 ,...v n [21]. Table I summarizes all notations used in this paper. An encoded hypervector that corresponds to one data point has D dimensions (e.g. D = 10, 000). We need to keep all information of a data point in the original space, i.e., the feature values and their indexes. As Figure 2a shows, we use two sets of pre-computed hypervectors: level and ID hypervectors. To create level hypervectors, we compute the minimum and maximum feature values among all data points, v min and v max , and then quantize the range of [v min , v max ] linearly into Q levels, L = {L 1 , \u00b7\u00b7\u00b7 , L Q }. Each level hypervector, L i , is unique and has D binarized dimensions, i.e., L i \u2208 {0, 1} D . The level hypervectors need to have the spectrum of similarities, such that the neighbor levels get more similar hypervectors. We create the first level hypervector, L 1 , by randomly selecting each element of a hypervector to be either 0 or 1 value. The second level hypervector, L 2 , is created by flipping D/Q random dimensions of the L 1 . This continues until creating the L Q hypervector by flipping D/Q random dimensions of L Q\u22121 . Since we select and flip the dimensions randomly, there is a high probability that the L 1 and L Q will have D/2 dimension difference. As a result, the level hypervectors have similar values if the corresponding original data are closer, while L 1 and L Q will be nearly orthogonal.\n\nThe hypervector also needs to contain all the information that the original features have. To differentiate the impact of each feature index, we devise ID hypervectors, {ID 1 , \u00b7\u00b7\u00b7 , ID n }.\n\nAn ID hypervector has the binarized dimensions, i.e., ID i \u2208 {0, 1} D . We create IDs with random binary values so that the ID hypervectors of different feature indexes are nearly orthogonal:\n\u03b4 (ID i , ID j ) D/2 (i = j & 0 < i, j n)\nwhere the similarity metric, \u03b4 (ID i , ID j ), is the Hamming distance between the two ID hypervectors. The orthogonality of ID hypervectors is ensured as long as the hypervector dimension, D, is large enough compared to the number of features (D >> n) in the original data point. Figure 2b shows how we map each data point, v, to the high-dimensional space using the precomputed hypervectors. For each feature, we perform an element-wise XOR operation for the ID and level hypervector corresponding to the feature value. The different features are combined by adding each element. For example, when a feature value of an original data point, v i , is quantized to L i \u2208 L, the following equation represents the calculated hypervector, h:\nh = ID 1 \u2295 L 1 + ID 2 \u2295 L 2 + ... + ID n \u2295 L n .\nNote that the element-wise addition can make a hypervector that has integer elements, i.e., H \u2208 N D . To perform all the other clustering procedures with binarized hypervectors, we apply a majority function for the calculated hypervector. For a given hypervector, h = h 1 , \u00b7\u00b7\u00b7 , h D , the majority function is defined as follows:\nMAJ(h, \u03c4) = h 1 , \u00b7\u00b7\u00b7 , h D where h i = 0, if h i < \u03c4 1, otherwise.\nUsing the majority function, the final hypervector for each data point is encoded by e = MAJ(h, n/2), and e \u2208 {0, 1} D .\n\n\nC. Clustering in HD Space\n\nHDCluster procedure identifies the cluster indexes (tags) through an iterative process as shown in Figure 2c and d, by using the encoded hypervectors. The proposed HD clustering algorithm is inspired by k-means. [22] In a similar way to the standard k-means algorithm, we initially choose K random hypervectors as cluster centers. We denote each hypervector for the centers as shown in Figure 2d. Randomness and high dimensionality of hypervectors ensure that the centers of clusters are orthogonal at the initial iteration. With the initial cluster centers, we perform an iterative process to find the best clusters. There are two steps in each iteration, computing distance similarity and updating cluster centers. Computing distance similarity: In this step, HDCluster assigns a cluster center for each encoded hypervector among all the center candidates. HDCluster measures the Hamming distances between the hypervector of a data point and each k center and identifies the cluster center which has the highest similarity. For an encoded hypervector, e, the index of the cluster center, called tag, k, is chosen as follows:\nargmin k \u03b4 (C k , e)\nUpdating cluster centers: After identifying tags of all data points, HDCluster updates the cluster centers using the data points which belong to each cluster. For a set of the data points in the k th cluster, E k = {e k i }, we perform element-wise additions to produce the hypervector sum, s k = \u2211 i e k i . Then, HDCluster binarizes the hypervector sum so that it is mapped into the {0, 1} D space. The following equation illustrates this procedure:\nC k = MAJ(s k , |E k |/2)\nTermination of HDCluster: The algorithm converges when there is no significant change in the cluster centers. This iterative procedure continues until (i) the hypervectors representing the center of clusters has minor change during two consecutive iterations or (ii) the number of iterations exceeds a pre-defined parameter.\n\n\nIII. EXPERIMENTAL RESULTS\n\n\nA. Experimental Setup\n\nWe implemented full HDCluster functionality using C++ implementation. We test the clustering quality of the proposed HDCluster, with diverse datasets including: MNIST handwritten digit [23], DIM dataset [24], Glass Identification [25], Iris [26], Voice dataset (ISOLET) [27] and medical-related datasets such as Gene Expression Cancer RNA Sequence (RNA-Seq) [28], Breast Cancer dataset [29], Primary Tumor dataset [30], and Parkinsons dataset [31]. We evaluate the clustering quality for each dataset by using the provided ground truth. Table II show the quality of clustering for HDCluster and k-means algorithms over different applications. The quality of clustering has been measured by comparing the result of clustering with the provided ground truth. For all the reported results, we use Q = 16 and D = 10K. To better understand when HDCluster provides higher quality and efficiency, we perform the evaluation for DIM which has multiple datasets with different feature sizes. Table III summarizes the evaluation results. As compared to the k-means++, HDCluster shows higher robustness with the increase in the dimension of the original data. For example, k-means++ and HDCluster exhibit the similar quality for DIM 32, while for DIM 1024, the HDCluster provides 10% better clustering quality. In addition, since the computation cost of HDCluster is independent of dimensions of a dataset after encoding the dataset, we achieve higher efficiency for larger dimension than k-means++ which does not scale.\n\n\nB. Clustering Quality\n\n\nC. Parameter Exploration\n\nThe precision of the encoding procedure is defined by how accurate it can map original data points to the high-dimensional space. For example, quantizing each feature to a small level (Q) may not keep all the required information of the original data points. With the relatively large number of clusters (K), the encoded hypervector may need to maintain more finegrained information. Figure 3 shows the sufficient quantization   levels, Q, when the number of clusters changes from 5 to 30, where the quality is evaluated in mean square error. HDCluster with a large number of cluster centers requires to quantize the input data into a higher number of levels. For instance, for ISOLET, HDCluster achieves the best quality with Q = 8 when clustering with 10 centers, while the quantization level of Q = 32 is required in the case of clustering with 30 centers.\n\n\nD. Scalability with Hypervector Dimension\n\nSince the hypervectors map the original information into a high-dimension space, the quality of clustering is directly related to the vector size. In the accelerator design, it also determines the system efficiency. Figure 4a shows the quality loss of clustering when HDCluster dimensionality changes from 1k to 10k. The results show that, for most of the applications, HDCluster can perform the clustering tasks with a relatively small dimensionality, while providing similar quality of clustering. When reducing the dimensionality of hypervectors to 4,000 and 2,000, HDCluster on average has 1.7% and 3.1% lower quality respectively as compared to HDCluster with the full dimension, i.e., D = 10K. Figure 4b also shows the normalized energy and execution time of HDCluster running. All results are normalized to HDCluster running with 10K hypervector. Our results shows that decreasing dimensionality from 10K to 4K can result in 2.1\u00d7 speedup and 2.4\u00d7 energy efficiency.\n\n\nIV. CONCLUSION\n\nWe propose a novel clustering algorithm, HDCluster, that maps data points to the high-dimensional space and clusters them using concrete linear algebra for hypervectors. HDCluster provides high clustering quality for diverse and practical applications that involve a large number of samples and high complexity in feature domains. Our future work is to design an accelerator which processes the entire HDCluster operations in a memory-centric architecture.\n\n(\nID1, \u2026, IDn (L1, \u2026, LQ Fig. 1. The functionality of the proposed HDCluster algorithm.\n\nFig. 2 .\n2{C 1 , C 2 , \u00b7\u00b7\u00b7 ,C K }, where C k \u2208 {0,1} D , as The functionality of HDCluster algorithm using encoding, data and search blocks.\n\nFig. 3 .\n3Impact of number of clusters on the minimum quantization levels.\n\nFig. 4 .\n4(a) quality loss of clustering and (b) normalized Energy consumption and execution time of HDCluster using different dimensions.\n\nTABLE I TABLE\nIOF NOTATIONSSymbol \nDefinition \nSymbol \nDefinition \nv \nFeature vector in original domain \nL \nLevel hypervectors \u2208 {0, 1} D \nn \n# of features in original domain \nQ \n# of quanitized levels \nD \nDimension of encoded data \nIDi \nAn ID hypervector \u2208 {0, 1} D \nK \n# of clusters \nCk \nA cluster center hypervector \u2208 {0, 1} D \nh \nA non-binary hypervector \u2208 N D \nN \n# of data points \nE k \nEncoded data points of a cluster \nI \n# of executed iterations \n\n\n\n\nDesign, Automation And Test in Europe (DATE 2019)\n\nTABLE II THE\nIIQUALITY OF CLUSTERING OF HDCLUSTER AS COMPARE TO K-MEANS (Q = 16).TABLE III THE QUALITY OF CLUSTERING OF HDCLUSTER ON DIM DATASET WITH DIFFERENT DIMENSIONS.Datasets \nMNIST \nISOLET \nIRIS \nGlass \nUnbalance \nRNA-seq \nCancer \nEcoli \nParkinsons \n\n# of Data Samples (N) \n10,000 \n7,797 \n150 \n214 \n6,500 \n801 \n569 \n339 \n6,590 \n# of Features (n) \n784 \n617 \n4 \n10 \n2 \n20,531 \n32 \n17 \n1 \n# of Clusters (K) \n10 \n26 \n3 \n7 \n8 \n5 \n2 \n22 \n2 \nk-means \n48.8% \n28.4% \n88.7% \n51.8% \n93.8% \n37.5% \n94.1% \n74.3% \n75.3% \nHDCluster \n58.6% \n33.1% \n89.9% \n67.5% \n92.3% \n37.5% \n96.2% \n78.5% \n75.6% \n\nDatasets \nDIM 32 DIM 128 DIM 256 DIM 512 DIM 1024 \n# of Features (n) \n32 \n128 \n256 \n512 \n1024 \nk-means \n87.5% \n81.2% \n76.4% \n68.6% \n62.5% \nHDCluster \n87.5% \n85.3% \n81.2% \n76.1% \n72.6% \n\n\nAuthorized licensed use limited to: Univ of Calif San Diego. Downloaded on May 11,2020 at 08:38:31 UTC from IEEE Xplore. Restrictions apply.\nACKNOWLEDGEMENTSThis work was partially supported by CRISP, one of six centers in JUMP, an SRC program sponsored by DARPA, and also NSF grants #1730158 and #1527034.\nCognitive machine-to-machine communications for internet-of-things: A protocol stack perspective. A Aijaz, IEEE IoT-J. 22A. Aijaz et al., \"Cognitive machine-to-machine communications for internet-of-things: A protocol stack perspective,\" IEEE IoT-J, vol. 2, no. 2, pp. 103-112, 2015.\n\nThe global footprint of mobile communications: The ecological and economic perspective. A Fehske, IEEE Communications Magazine. 498A. Fehske et al., \"The global footprint of mobile communications: The ecological and economic perspective,\" IEEE Communications Magazine, vol. 49, no. 8, 2011.\n\nData Mining: Practical machine learning tools and techniques. I H Witten, Morgan KaufmannI. H. Witten et al., Data Mining: Practical machine learning tools and techniques. Morgan Kaufmann, 2016.\n\nA survey of open source tools for machine learning with big data in the hadoop ecosystem. S Landset, Journal of Big Data. 2124S. Landset et al., \"A survey of open source tools for machine learning with big data in the hadoop ecosystem,\" Journal of Big Data, vol. 2, no. 1, p. 24, 2015.\n\nData mining with big data. X Wu, IEEE TKDE. 261X. Wu et al., \"Data mining with big data,\" IEEE TKDE, vol. 26, no. 1, pp. 97-107, 2014.\n\nOn the integration of cloud computing and internet of things. A Botta, IEEE FiCloud. IEEEA. Botta et al., \"On the integration of cloud computing and internet of things,\" in IEEE FiCloud, pp. 23-30, IEEE, 2014.\n\nEvolution of wireless sensor networks towards the internet of things: A survey. L Mainetti, IEEE SoftCOM. IEEEL. Mainetti et al., \"Evolution of wireless sensor networks towards the internet of things: A survey,\" in IEEE SoftCOM, pp. 1-6, IEEE, 2011.\n\nBayesian control of large MDPs with unknown dynamics in data-poor environments. M Imani, NIPS. M. Imani et al., \"Bayesian control of large MDPs with unknown dynamics in data-poor environments,\" in NIPS, 2018.\n\nA survey of clustering algorithms for big data: Taxonomy and empirical analysis. A Fahad, IEEE TETC. 23A. Fahad et al., \"A survey of clustering algorithms for big data: Taxonomy and empirical analysis,\" IEEE TETC, vol. 2, no. 3, pp. 267-279, 2014.\n\nBuilding high-level features using large scale unsupervised learning. Q V Le, IEEE ICASSP. IEEEQ. V. Le, \"Building high-level features using large scale unsupervised learning,\" in IEEE ICASSP, pp. 8595-8598, IEEE, 2013.\n\nHyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors. P Kanerva, Cognitive Computation. 12P. Kanerva, \"Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors,\" Cognitive Computation, vol. 1, no. 2, pp. 139-159, 2009.\n\nFelix: fast and energy-efficient logic in memory. S Gupta, ICCAD. ACM55S. Gupta et al., \"Felix: fast and energy-efficient logic in memory,\" in ICCAD, p. 55, ACM, 2018.\n\nFach: Fpga-based acceleration of hyperdimensional computing by reducing computational complexity. M Imani, ASP-DAC. IEEE55M. Imani et al., \"Fach: Fpga-based acceleration of hyperdimensional computing by reducing computational complexity,\" in ASP-DAC, p. 55, IEEE, 2019.\n\nSequence prediction with sparse distributed hyperdimensional coding applied to the analysis of mobile phone use patterns. O Rasanen, J Saarinen, IEEE Trans. Neural Netw. Learn. Syst. 99O. Rasanen and J. Saarinen, \"Sequence prediction with sparse distributed hyperdimensional coding applied to the analysis of mobile phone use patterns,\" IEEE Trans. Neural Netw. Learn. Syst, vol. PP, no. 99, pp. 1-12, 2015.\n\nVoicehd: Hyperdimensional computing for efficient speech recognition. M Imani, IEEE ICRC. IEEEM. Imani et al., \"Voicehd: Hyperdimensional computing for efficient speech recognition,\" in IEEE ICRC, pp. 1-6, IEEE, 2017.\n\nF5-hd: Fast flexible fpga-based framework for refreshing hyperdimensional computing. S Salamat, FPGA. ACMS. Salamat et al., \"F5-hd: Fast flexible fpga-based framework for refreshing hyperdimensional computing,\" in FPGA, ACM, 2019.\n\nHdna: Energy-efficient dna sequencing using hyperdimensional computing. M Imani, BHI. IEEEM. Imani et al., \"Hdna: Energy-efficient dna sequencing using hyperdimensional computing,\" in BHI, pp. 271-274, IEEE, 2018.\n\nEfficient human activity recognition using hyperdimensional computing. Y Kim, IoTACM38Y. Kim et al., \"Efficient human activity recognition using hyperdimensional computing,\" in IoT, p. 38, ACM, 2018.\n\nLow-power sparse hyperdimensional encoder for language recognition. M Imani, IEEE Design & Test. 346M. Imani et al., \"Low-power sparse hyperdimensional encoder for language recognition,\" IEEE Design & Test, vol. 34, no. 6, pp. 94-101, 2017.\n\nExploring hyperdimensional associative memory. M Imani, IEEE HPCA. IEEEM. Imani et al., \"Exploring hyperdimensional associative memory,\" in IEEE HPCA, pp. 445- 456, IEEE, 2017.\n\nHierarchical hyperdimensional computing for energy efficient classification. M Imani, DAC. IEEEM. Imani et al., \"Hierarchical hyperdimensional computing for energy efficient classification,\" in DAC, pp. 1-6, IEEE, 2018.\n\nk-means++: The advantages of careful seeding. D Arthur, ACM-SIAM. D. Arthur et al., \"k-means++: The advantages of careful seeding,\" in ACM-SIAM, pp. 1027- 1035, Society for Industrial and Applied Mathematics, 2007.\n\nMnist handwritten digit database. Y Lecun, C Cortes, C J Burges, AT&T Labs. Y. LeCun, C. Cortes, and C. J. Burges, \"Mnist handwritten digit database,\" AT&T Labs [Online]. Available: http://yann. lecun. com/exdb/mnist, 2010.\n\nFast agglomerative clustering using a k-nearest neighbor graph. P Fr\u00e4nti, O Virmajoki, V Hautam\u00e4ki, TPAMI. 2811Uci machine learning repositoryP. Fr\u00e4nti, O. Virmajoki, and V. Hautam\u00e4ki, \"Fast agglomerative clustering using a k-nearest neighbor graph,\" TPAMI, vol. 28, no. 11, pp. 1875-1881, 2006. [25] \"Uci machine learning repository.\" https://archive.ics.uci.edu/ml/datasets/glass+ identification.\n\nUci machine learning repository. \"Uci machine learning repository.\" https://archive.ics.uci.edu/ml/datasets/iris.\n\nUci machine learning repository. \"Uci machine learning repository.\" http://archive.ics.uci.edu/ml/datasets/ISOLET.\n\nThe cancer genome atlas pan-cancer analysis project. J N Weinstein, Nature genetics. 4510J. N. Weinstein et al., \"The cancer genome atlas pan-cancer analysis project,\" Nature genetics, vol. 45, no. 10, pp. 1113-1120, 2013.\n\nThe multi-purpose incremental learning system aq15 and its testing application to three medical domains. R S Michalski, Proc. AAAI. AAAIR. S. Michalski et al., \"The multi-purpose incremental learning system aq15 and its testing application to three medical domains,\" Proc. AAAI 1986, pp. 1-041, 1986.\n\nAssistant 86: A knowledge-elicitation tool for sophisticated users. B Cestnik, EWSL. Sigma PressB. Cestnik et al., \"Assistant 86: A knowledge-elicitation tool for sophisticated users,\" in EWSL, pp. 31-45, Sigma Press, 1987.\n\nExploiting nonlinear recurrence and fractal scaling properties for voice disorder detection. M A Little, BioMedical Engineering OnLine. 6123M. A. Little et al., \"Exploiting nonlinear recurrence and fractal scaling properties for voice disorder detection,\" BioMedical Engineering OnLine, vol. 6, no. 1, p. 23, 2007.\n", "annotations": {"author": "[{\"end\":192,\"start\":85},{\"end\":282,\"start\":193},{\"end\":391,\"start\":283},{\"end\":499,\"start\":392},{\"end\":607,\"start\":500}]", "publisher": null, "author_last_name": "[{\"end\":97,\"start\":92},{\"end\":204,\"start\":201},{\"end\":296,\"start\":290},{\"end\":405,\"start\":400},{\"end\":513,\"start\":507}]", "author_first_name": "[{\"end\":91,\"start\":85},{\"end\":200,\"start\":193},{\"end\":289,\"start\":283},{\"end\":399,\"start\":392},{\"end\":506,\"start\":500}]", "author_affiliation": "[{\"end\":191,\"start\":116},{\"end\":281,\"start\":206},{\"end\":390,\"start\":315},{\"end\":498,\"start\":423},{\"end\":606,\"start\":531}]", "title": "[{\"end\":82,\"start\":1},{\"end\":689,\"start\":608}]", "venue": null, "abstract": "[{\"end\":1606,\"start\":762}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1804,\"start\":1801},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1855,\"start\":1852},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2075,\"start\":2072},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2080,\"start\":2077},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2188,\"start\":2185},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2193,\"start\":2190},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2402,\"start\":2399},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2408,\"start\":2404},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3133,\"start\":3129},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3667,\"start\":3663},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3673,\"start\":3669},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4302,\"start\":4298},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5774,\"start\":5770},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":5780,\"start\":5776},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6619,\"start\":6615},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":9992,\"start\":9988},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":11969,\"start\":11965},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11987,\"start\":11983},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":12025,\"start\":12021},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":12054,\"start\":12050},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":12142,\"start\":12138},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":12170,\"start\":12166},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":12198,\"start\":12194},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":12227,\"start\":12223}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":15782,\"start\":15694},{\"attributes\":{\"id\":\"fig_1\"},\"end\":15924,\"start\":15783},{\"attributes\":{\"id\":\"fig_2\"},\"end\":16000,\"start\":15925},{\"attributes\":{\"id\":\"fig_3\"},\"end\":16140,\"start\":16001},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":16598,\"start\":16141},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":16650,\"start\":16599},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":17426,\"start\":16651}]", "paragraph": "[{\"end\":2946,\"start\":1625},{\"end\":3832,\"start\":2948},{\"end\":5028,\"start\":3834},{\"end\":5961,\"start\":5080},{\"end\":6443,\"start\":5963},{\"end\":8011,\"start\":6473},{\"end\":8203,\"start\":8013},{\"end\":8396,\"start\":8205},{\"end\":9177,\"start\":8439},{\"end\":9557,\"start\":9227},{\"end\":9746,\"start\":9626},{\"end\":10902,\"start\":9776},{\"end\":11375,\"start\":10924},{\"end\":11726,\"start\":11402},{\"end\":13288,\"start\":11780},{\"end\":14200,\"start\":13341},{\"end\":15218,\"start\":14246},{\"end\":15693,\"start\":15237}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":8438,\"start\":8397},{\"attributes\":{\"id\":\"formula_1\"},\"end\":9226,\"start\":9178},{\"attributes\":{\"id\":\"formula_2\"},\"end\":9625,\"start\":9558},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10923,\"start\":10903},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11401,\"start\":11376}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":6628,\"start\":6621},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":12325,\"start\":12317},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":12771,\"start\":12762}]", "section_header": "[{\"end\":1623,\"start\":1608},{\"end\":5054,\"start\":5031},{\"end\":5078,\"start\":5057},{\"end\":6471,\"start\":6446},{\"end\":9774,\"start\":9749},{\"end\":11754,\"start\":11729},{\"end\":11778,\"start\":11757},{\"end\":13312,\"start\":13291},{\"end\":13339,\"start\":13315},{\"end\":14244,\"start\":14203},{\"end\":15235,\"start\":15221},{\"end\":15696,\"start\":15695},{\"end\":15792,\"start\":15784},{\"end\":15934,\"start\":15926},{\"end\":16010,\"start\":16002},{\"end\":16155,\"start\":16142},{\"end\":16664,\"start\":16652}]", "table": "[{\"end\":16598,\"start\":16169},{\"end\":17426,\"start\":16823}]", "figure_caption": "[{\"end\":15782,\"start\":15697},{\"end\":15924,\"start\":15794},{\"end\":16000,\"start\":15936},{\"end\":16140,\"start\":16012},{\"end\":16169,\"start\":16157},{\"end\":16650,\"start\":16601},{\"end\":16823,\"start\":16667}]", "figure_ref": "[{\"end\":5790,\"start\":5782},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":6896,\"start\":6887},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":8729,\"start\":8720},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9884,\"start\":9875},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":10171,\"start\":10162},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13733,\"start\":13725},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":14471,\"start\":14462},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":14955,\"start\":14946}]", "bib_author_first_name": "[{\"end\":17833,\"start\":17832},{\"end\":18108,\"start\":18107},{\"end\":18374,\"start\":18373},{\"end\":18376,\"start\":18375},{\"end\":18598,\"start\":18597},{\"end\":18822,\"start\":18821},{\"end\":18993,\"start\":18992},{\"end\":19222,\"start\":19221},{\"end\":19473,\"start\":19472},{\"end\":19684,\"start\":19683},{\"end\":19922,\"start\":19921},{\"end\":19924,\"start\":19923},{\"end\":20198,\"start\":20197},{\"end\":20481,\"start\":20480},{\"end\":20698,\"start\":20697},{\"end\":20993,\"start\":20992},{\"end\":21004,\"start\":21003},{\"end\":21350,\"start\":21349},{\"end\":21584,\"start\":21583},{\"end\":21803,\"start\":21802},{\"end\":22017,\"start\":22016},{\"end\":22215,\"start\":22214},{\"end\":22436,\"start\":22435},{\"end\":22644,\"start\":22643},{\"end\":22834,\"start\":22833},{\"end\":23038,\"start\":23037},{\"end\":23047,\"start\":23046},{\"end\":23057,\"start\":23056},{\"end\":23059,\"start\":23058},{\"end\":23293,\"start\":23292},{\"end\":23303,\"start\":23302},{\"end\":23316,\"start\":23315},{\"end\":23913,\"start\":23912},{\"end\":23915,\"start\":23914},{\"end\":24189,\"start\":24188},{\"end\":24191,\"start\":24190},{\"end\":24454,\"start\":24453},{\"end\":24704,\"start\":24703},{\"end\":24706,\"start\":24705}]", "bib_author_last_name": "[{\"end\":17839,\"start\":17834},{\"end\":18115,\"start\":18109},{\"end\":18383,\"start\":18377},{\"end\":18606,\"start\":18599},{\"end\":18825,\"start\":18823},{\"end\":18999,\"start\":18994},{\"end\":19231,\"start\":19223},{\"end\":19479,\"start\":19474},{\"end\":19690,\"start\":19685},{\"end\":19927,\"start\":19925},{\"end\":20206,\"start\":20199},{\"end\":20487,\"start\":20482},{\"end\":20704,\"start\":20699},{\"end\":21001,\"start\":20994},{\"end\":21013,\"start\":21005},{\"end\":21356,\"start\":21351},{\"end\":21592,\"start\":21585},{\"end\":21809,\"start\":21804},{\"end\":22021,\"start\":22018},{\"end\":22221,\"start\":22216},{\"end\":22442,\"start\":22437},{\"end\":22650,\"start\":22645},{\"end\":22841,\"start\":22835},{\"end\":23044,\"start\":23039},{\"end\":23054,\"start\":23048},{\"end\":23066,\"start\":23060},{\"end\":23300,\"start\":23294},{\"end\":23313,\"start\":23304},{\"end\":23326,\"start\":23317},{\"end\":23925,\"start\":23916},{\"end\":24201,\"start\":24192},{\"end\":24462,\"start\":24455},{\"end\":24713,\"start\":24707}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":246616},\"end\":18017,\"start\":17734},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":14545141},\"end\":18309,\"start\":18019},{\"attributes\":{\"id\":\"b2\"},\"end\":18505,\"start\":18311},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":10213552},\"end\":18792,\"start\":18507},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":18886861},\"end\":18928,\"start\":18794},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":11431216},\"end\":19139,\"start\":18930},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":18243649},\"end\":19390,\"start\":19141},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":53965616},\"end\":19600,\"start\":19392},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":16494521},\"end\":19849,\"start\":19602},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":206741597},\"end\":20070,\"start\":19851},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":733980},\"end\":20428,\"start\":20072},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":53235957},\"end\":20597,\"start\":20430},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":58027670},\"end\":20868,\"start\":20599},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":15258913},\"end\":21277,\"start\":20870},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":21351739},\"end\":21496,\"start\":21279},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":67872077},\"end\":21728,\"start\":21498},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":4708051},\"end\":21943,\"start\":21730},{\"attributes\":{\"id\":\"b17\"},\"end\":22144,\"start\":21945},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":8038292},\"end\":22386,\"start\":22146},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1677864},\"end\":22564,\"start\":22388},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":49301394},\"end\":22785,\"start\":22566},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":1782131},\"end\":23001,\"start\":22787},{\"attributes\":{\"id\":\"b22\"},\"end\":23226,\"start\":23003},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":11753837},\"end\":23626,\"start\":23228},{\"attributes\":{\"id\":\"b24\"},\"end\":23741,\"start\":23628},{\"attributes\":{\"id\":\"b25\"},\"end\":23857,\"start\":23743},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":9652087},\"end\":24081,\"start\":23859},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":18018701},\"end\":24383,\"start\":24083},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":39569406},\"end\":24608,\"start\":24385},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":3138759},\"end\":24924,\"start\":24610}]", "bib_title": "[{\"end\":17830,\"start\":17734},{\"end\":18105,\"start\":18019},{\"end\":18595,\"start\":18507},{\"end\":18819,\"start\":18794},{\"end\":18990,\"start\":18930},{\"end\":19219,\"start\":19141},{\"end\":19470,\"start\":19392},{\"end\":19681,\"start\":19602},{\"end\":19919,\"start\":19851},{\"end\":20195,\"start\":20072},{\"end\":20478,\"start\":20430},{\"end\":20695,\"start\":20599},{\"end\":20990,\"start\":20870},{\"end\":21347,\"start\":21279},{\"end\":21581,\"start\":21498},{\"end\":21800,\"start\":21730},{\"end\":22212,\"start\":22146},{\"end\":22433,\"start\":22388},{\"end\":22641,\"start\":22566},{\"end\":22831,\"start\":22787},{\"end\":23035,\"start\":23003},{\"end\":23290,\"start\":23228},{\"end\":23910,\"start\":23859},{\"end\":24186,\"start\":24083},{\"end\":24451,\"start\":24385},{\"end\":24701,\"start\":24610}]", "bib_author": "[{\"end\":17841,\"start\":17832},{\"end\":18117,\"start\":18107},{\"end\":18385,\"start\":18373},{\"end\":18608,\"start\":18597},{\"end\":18827,\"start\":18821},{\"end\":19001,\"start\":18992},{\"end\":19233,\"start\":19221},{\"end\":19481,\"start\":19472},{\"end\":19692,\"start\":19683},{\"end\":19929,\"start\":19921},{\"end\":20208,\"start\":20197},{\"end\":20489,\"start\":20480},{\"end\":20706,\"start\":20697},{\"end\":21003,\"start\":20992},{\"end\":21015,\"start\":21003},{\"end\":21358,\"start\":21349},{\"end\":21594,\"start\":21583},{\"end\":21811,\"start\":21802},{\"end\":22023,\"start\":22016},{\"end\":22223,\"start\":22214},{\"end\":22444,\"start\":22435},{\"end\":22652,\"start\":22643},{\"end\":22843,\"start\":22833},{\"end\":23046,\"start\":23037},{\"end\":23056,\"start\":23046},{\"end\":23068,\"start\":23056},{\"end\":23302,\"start\":23292},{\"end\":23315,\"start\":23302},{\"end\":23328,\"start\":23315},{\"end\":23927,\"start\":23912},{\"end\":24203,\"start\":24188},{\"end\":24464,\"start\":24453},{\"end\":24715,\"start\":24703}]", "bib_venue": "[{\"end\":24219,\"start\":24215},{\"end\":17851,\"start\":17841},{\"end\":18145,\"start\":18117},{\"end\":18371,\"start\":18311},{\"end\":18627,\"start\":18608},{\"end\":18836,\"start\":18827},{\"end\":19013,\"start\":19001},{\"end\":19245,\"start\":19233},{\"end\":19485,\"start\":19481},{\"end\":19701,\"start\":19692},{\"end\":19940,\"start\":19929},{\"end\":20229,\"start\":20208},{\"end\":20494,\"start\":20489},{\"end\":20713,\"start\":20706},{\"end\":21051,\"start\":21015},{\"end\":21367,\"start\":21358},{\"end\":21598,\"start\":21594},{\"end\":21814,\"start\":21811},{\"end\":22014,\"start\":21945},{\"end\":22241,\"start\":22223},{\"end\":22453,\"start\":22444},{\"end\":22655,\"start\":22652},{\"end\":22851,\"start\":22843},{\"end\":23077,\"start\":23068},{\"end\":23333,\"start\":23328},{\"end\":23659,\"start\":23628},{\"end\":23774,\"start\":23743},{\"end\":23942,\"start\":23927},{\"end\":24213,\"start\":24203},{\"end\":24468,\"start\":24464},{\"end\":24744,\"start\":24715}]"}}}, "year": 2023, "month": 12, "day": 17}