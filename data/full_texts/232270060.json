{"id": 232270060, "updated": "2023-10-06 05:27:45.546", "metadata": {"title": "Faster quantum-inspired algorithms for solving linear systems", "authors": "[{\"first\":\"Changpeng\",\"last\":\"Shao\",\"middle\":[]},{\"first\":\"Ashley\",\"last\":\"Montanaro\",\"middle\":[]}]", "venue": "ArXiv", "journal": "ACM Transactions on Quantum Computing", "publication_date": {"year": 2021, "month": null, "day": null}, "abstract": "We establish an improved classical algorithm for solving linear systems in a model analogous to the QRAM that is used by quantum linear solvers. Precisely, for the linear system $A\\x = \\b$, we show that there is a classical algorithm that outputs a data structure for $\\x$ allowing sampling and querying to the entries, where $\\x$ is such that $\\|\\x - A^{+}\\b\\|\\leq \\epsilon \\|A^{+}\\b\\|$. This output can be viewed as a classical analogue to the output of quantum linear solvers. The complexity of our algorithm is $\\widetilde{O}(\\kappa_F^4 \\kappa^2/\\epsilon^2 )$, where $\\kappa_F = \\|A\\|_F\\|A^{+}\\|$ and $\\kappa = \\|A\\|\\|A^{+}\\|$. This improves the previous best algorithm [Gily{\\'e}n, Song and Tang, arXiv:2009.07268] of complexity $\\widetilde{O}(\\kappa_F^6 \\kappa^6/\\epsilon^4)$. Our algorithm is based on the randomized Kaczmarz method, which is a particular case of stochastic gradient descent. We also find that when $A$ is row sparse, this method already returns an approximate solution $\\x$ in time $\\widetilde{O}(\\kappa_F^2)$, while the best quantum algorithm known returns $\\ket{\\x}$ in time $\\widetilde{O}(\\kappa_F)$ when $A$ is stored in the QRAM data structure. As a result, assuming access to QRAM and if $A$ is row sparse, the speedup based on current quantum algorithms is quadratic.", "fields_of_study": "[\"Physics\",\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2103.10309", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2103-10309", "doi": "10.1145/3520141"}}, "content": {"source": {"pdf_hash": "baa7b58d0ea6424a27960570ea68e92c070d50c2", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2103.10309v2.pdf\"]", "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "http://arxiv.org/pdf/2103.10309", "status": "GREEN"}}, "grobid": {"id": "0dcfe59ab1c1c0f2f6dc4393af37189421c62712", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/baa7b58d0ea6424a27960570ea68e92c070d50c2.txt", "contents": "\nFaster quantum-inspired algorithms for solving linear systems\nApril 18, 2023\n\nChangpeng Shao \nSchool of Mathematics\nUniversity of Bristol\nUK\n\nAshley Montanaro \nSchool of Mathematics\nUniversity of Bristol\nUK\n\nPhasecraft Ltd\nUK\n\nFaster quantum-inspired algorithms for solving linear systems\nApril 18, 2023\nWe establish an improved classical algorithm for solving linear systems in a model analogous to the QRAM that is used by quantum linear solvers. Precisely, for the linear system Ax = b, we show that there is a classical algorithm that outputs a data structure for x allowing sampling and querying to the entries, where x is such thatThis output can be viewed as a classical analogue to the output of quantum linear solvers. The complexity of our algorithm is O(\u03ba 4 F \u03ba 2 / 2 ), where \u03ba F = A F A + and \u03ba = A A + . This improves the previous best algorithm [Gily\u00e9n, Song and Tang, arXiv:2009.07268] of complexity O(\u03ba 6 F \u03ba 6 / 4 ). Our algorithm is based on the randomized Kaczmarz method, which is a particular case of stochastic gradient descent. We also find that when A is row sparse, this method already returns an approximate solution x in time O(\u03ba 2 F ), while the best quantum algorithm known returns |x in time O(\u03ba F ) when A is stored in the QRAM data structure. As a result, assuming access to QRAM and if A is row sparse, the speedup based on current quantum algorithms is quadratic.\n\nIntroduction\n\nSince the discovery of the Harrow-Hassidim-Lloyd algorithm [17] for solving linear systems, many quantum algorithms for solving machine learning problems were proposed, e.g. [2, 20-22, 25, 32]. Most of them claimed that quantum computers could achieve exponential speedups over classical algorithms. However, as shown by Tang [37] and some subsequent works [5][6][7][8]11,12,18,36] (currently known as quantum-inspired algorithms), for many previously discovered quantum machine learning algorithms, the quantum computer actually only achieves a polynomial speedup in the dimension. Tang's work makes us reconsider the advantages of quantum computers in machine learning. For instance, one may wonder what is the best possible separation between quantum and classical computers for solving a given machine learning problem?\n\nHere we focus on the solving of linear systems, which is a fundamental problem in machine learning. For a linear system Ax = b, as far as we know, the currently best quantum algorithm seems to be the one building on the technique of block-encoding. For simplicity, assume that the singular values of A lie in [1/\u03ba, 1], where \u03ba is the condition number of A. Let \u03b1 \u2208 R + , if A/\u03b1 is efficiently encoded as a block of a unitary, the complexity to obtain |A + b up to error is O(\u03b1\u03ba) [4], where the O notation hides logarithmic factors in all parameters (including \u03ba, and the size of A). For example, when we have the QRAM data structure for A, we can take \u03b1 = A F . 1 When A is s-sparse and given in the sparse-access input model, we can take \u03b1 = s A max . In the classical case, the quantum-inspired algorithm given in [6] costs O(\u03ba 6 F \u03ba 22 / 6 ) to output a classical analogue to |A + b , where \u03ba F = A F A + , 2 and is the accuracy. This result was recently improved to O(\u03ba 6 F \u03ba 6 / 4 ) [12]. Although the quantum-inspired algorithms are polylog in the dimension, the heavy dependence on \u03ba F , \u03ba, may make them hard to use in practice [1]. So it is necessary to improve the efficiency of quantum-inspired algorithms. Meanwhile, this can help us better understand the quantum advantages in certain problems.\n\nQRAM is a widely used data structure in many quantum machine learning algorithms (e.g. [20][21][22]). It allows us efficiently to prepare a quantum state corresponding to the data and to perform quantum linear algebraic operations. The quantum-inspired algorithms usually assume a data structure which is a classical analogue to the QRAM. For a vector, this data structure allows us to do the sampling and query operations to the entries. The sampling operation can be viewed as measurements of the quantum state.\n\nAs for the problem of solving linear systems, in the QRAM model, the input of quantum algorithms is the QRAM data structures for A and b, and the output is |x such that |x \u2212 |A + b \u2264 . In the classical setting, the input of quantum-inspired algorithms is the classical analogue of QRAM data structures for A and b, and the output is a data structure for x such that x \u2212 A + b \u2264 A + b . This makes the comparison of quantum/quantum-inspired algorithms reasonable. There are also some other quantum models, such as the sparse-access input model. This allows efficient quantum algorithms for solving sparse linear systems [9]. However, because of the similarity between QRAM and the model used in quantum-inspired algorithms, in this paper we will mainly focus on the QRAM model for quantum linear solvers.\n\nRandomized iterative methods have received a lot of attention recently in solving linear systems because of their connection to the stochastic gradient descent method. The randomized Kaczmarz method [35] is a typical example. It was first proposed by Kaczmarz in 1937 [19], and rediscovered by Gordon, Bender and Herman in 1970 [15]. After that, there appear many generalizations [16,27,28,33]. Kaczmarz method is a particular case of the stochastic gradient descent algorithm for solving linear systems. In its simplest version, at each step of iteration, it randomly chooses a row of the matrix with probability corresponding to the norm of that row, then finds the closest solution that satisfies this linear constraint. Geometrically, the next point is the orthogonal projection of the previous one onto the hyperplane defined by the chosen linear constraint. The simple iterative structure and clear geometric meaning greatly simplifies the analysis of this method and makes it popular in applications like computed tomography [3]. Usually, the randomized iterative methods need to do some importance sampling according to the norms of the rows. Sampling access is made available by the data structure for quantum-inspired algorithms and the QRAM for quantum linear solvers. So in this paper, for the fairness of comparison we also assume a similar data structure for the randomized iterative methods.\n\n\nMain results\n\nIn this paper, we propose two quantum-inspired algorithms for solving linear systems. One is based on the randomized Kaczmarz method [35], and the other one is based on the randomized coordinate descent method [23]. The second algorithm only works for symmetric positive-definite (SPD) matrices. Our results are summarized in Table 1 \nO(\u03ba 4 F \u03ba 2 / 2 ) Theorem 13\nO(Tr(A) 2 A + 2 \u03ba/ 2 ) Theorem 14 SPD Table 1: Comparison of different algorithms for solving the linear system Ax = b, where \u03ba F = A F A + , \u03ba = A A + and s is the row sparsity. SPD = symmetric positive definite.\n\nIn the table, the quantum algorithm is based on the QRAM model. The randomized classical and quantum-inspired algorithms assume a data structure that is a classical analogue to the QRAM (see Section 3). So all those algorithms are based on similar data structures, which makes the comparison in the table fair enough.\n\n\u2022 When A is dense, the complexity of our algorithm is O(\u03ba 4 F \u03ba 2 / 2 ). If A is additionally SPD, the complexity becomes O(Tr(A) 2 A + 2 \u03ba/ 2 ). This reduces the complexity of the previous quantum-inspired algorithm given by Gily\u00e9n et al. [12] by at least a 4th power in terms of the dependence on \u03ba, improves the dependence on \u03ba F from 6th to 4th, and the dependence on1/ from 4th to 2nd. One result not shown in the table is about a description of the solution. In [12], the authors define an s-sparse description of a vector x as an s-sparse vector y such that x = A \u2020 y. The sparsity of y determines the runtime of querying one entry of x. In [12], they obtain an O(\u03ba 2 F \u03ba 2 / 2 )-sparse description of the approximate solution in cost O(\u03ba 6 F \u03ba 2 / 4 ). In comparison, we obtain an O(\u03ba 2 F )-sparse description in cost O(\u03ba 4 F \u03ba 2 / 2 ) by Theorem 13.\n\n\u2022 When A is row sparse, i.e., each row is sparse, the randomized Kaczmarz method can find an approximate sparse solution in time O(s\u03ba 2 F ), where s is the row sparsity. Moreover, the output is a vector, which is stronger than the outputs of quantum and quantum-inspired algorithms. Assuming s = O(1), this result means that the quantum speedup of solving row sparse linear systems is quadratic if using the QRAM data structure. It also means that to explore large quantum speedups in solving sparse linear systems, we may need to move our attentions to other quantum models. For example, it is known that when A is row and column sparse, the quantum algorithm has complexity O(s A max A + ) in the sparse access input model [4], which is better than the quantum result we show in the table.\n\n\u2022 When A is sparse and SPD, the randomized coordinate descent method can find an approximate sparse solution (also a sparse vector) in time O(sTr(A) A + ). In this special case, it may happen that quantum computers do not achieve a speedup in solving linear systems. For example, suppose A is a density matrix with A = \u0398(1), s = O(1), then the randomized coordinate descent method costs O(\u03ba), which is also the cost of the quantum linear solver. As shown in [31], to solve a sparse SPD linear system in a quantum computer, the dependence on \u03ba is at least linear. However, for some special SPD linear systems, quantum algorithms can achieve quadratic speedups in terms of \u03ba. 3 However, the special cases mentioned in [31] seem not directly related to the case we mentioned above.\n\n\u2022 In Table 1, the complexity of the quantum algorithm is the cost to obtain the quantum state corresponding to the solution. For quantum-inspired algorithms, the table shows the complexity to obtain the data structure for the solution that allows sampling and querying. This output can be viewed as a classical analogue to the output of the quantum algorithm. If we want to estimate the norm of the solution, then there is an extra factor 1/ for the quantum algorithm [4] and an extra factor 1/ 2 for the quantum-inspired algorithms [6] (also see Lemma 7). However, for the randomized classical algorithms, the complexity does not change because their outputs are sparse vectors whose sparsity has the same order as the time complexity. So to estimate the norm of the solution, the quantum and quantum-inspired algorithms are worse than the randomized classical algorithms in terms of the precision in the row sparse case.\n\n\u2022 For solving a row sparse linear system, the table shows that the complexities of quantum and classical randomized algorithms are logarithmic in the precision . However, if we write down the complexity more precisely, the classical randomized algorithms may have better dependence on , at least compared with the currently best quantum algorithm. Indeed, for the classical randomized algorithms, the dependence on is O(log(1/ )), while it is O(log 2 (1/ )) for the quantum algorithm [4]. If the linear system is also column sparse, the dependence on can be reduced to O(log(1/ )) in the quantum case [24].\n\n\nSummary of the techniques\n\nOur main algorithm is based on a generalization of Kaczmarz method. As briefly introduced above (also see Section 2 for more details), the Kaczmarz method aims to solve one linear constraint at each step of the iteration, which can be viewed as a particular case of the stochastic gradient method. To design an efficient quantum-inspired algorithm, we concentrate on the dual form of the Kaczmarz method. Namely, introducing y such that x = A \u2020 y, then focusing on the iteration on y. The linear system Ax = b can be reformulated as the least squares problem (LSP) x = arg min x Ax \u2212 b . It turns out that y converges to the optimal solution of the dual problem of the LSP. Our idea is to apply the iterative method to find a sparse approximate solution y of the dual problem. The sparsity is guaranteed by the iterative method. Now x = A \u2020 y gives an approximate solution of the LSP. Since y is sparse, it is easy to query an entry of x. To sample an entry of x, we can use the rejection sampling idea.\n\nThe technical part of this paper is the complexity analysis of the rejection sampling method. However, it turns out that the main cost of the above algorithm comes from the computation of y, whose complexity analysis is straightforward. It is time-consuming if we directly use the iterative method because it relates to the calculation of inner products. To alleviate this, we modify the iteration by using the sampling idea. We prove that this modified method has the same convergence rate as the original one by choosing some parameters appropriately.\n\nNotation. For a matrix A, we use A \u2020 to denote its conjugate transpose, A + to denote its Moore-Penrose inverse. The scaled condition number is \u03ba F = A F A + , and the condition number is \u03ba = A A + , where \u00b7 F is the Frobenius norm and \u00b7 is the spectral norm. The row sparsity of A is defined as the maximum of the number of nonzero zero entries of any row. For a unit vector v, we sometimes denote it as |v , and denote v \u2020 as v|. For two vectors a, b, we use a|b or a \u00b7 b to denote their inner product. If m is an integer, we define [m] := {1, 2, . . . , m}. For matrix A, we denote its i-th row as A i * , its j-th column as A * j and the (i, j)-th entry as A i,j . For a vector v, we denote the i-th component as v i . We use {e 1 , . . . , e m } to denote the standard basis of C m .\n\n\nRandomized Kaczmarz algorithm\n\nLet A be an m\u00d7n matrix, b be an m\u00d71 vector. Solving the linear system Ax = b can be described as a quadratic optimization problem, i.e., the so-called least squares problem (LSP) arg min\nx\u2208C n Ax \u2212 b 2 .(1)\nClassically, there are many algorithms for solving the LSP [14]. The best known classical algorithm might be the conjugate-gradient (CG) method. Compared to CG, the Kaczmarz algorithm has received much attention recently due to its simplicity. For instance, at each step of iteration, CG uses the whole information of the matrix A and O(mn) operations, while Kaczmarz method only uses one row of A and O(n) operations. So Kaczmarz method becomes effective when the entire matrix (data set) cannot be loaded into memory.\n\nDefinition 1 (Randomized Kaczmarz algorithm [35]). Let x 0 be an arbitrary initial approximation to the solution of (1). For k = 0, 1, . . ., compute\nx k+1 = x k + b r k \u2212 A r k * |x k A r k * 2 A r k * ,(2)\nwhere A r k * is the r k -th row of A, and r k is chosen from [m] randomly with probability proportional to A r k * 2 .\n\nThe Kaczmarz algorithm has a clear geometric meaning, see Figure 1. Geometrically, x k+1 is the orthogonal projection of x k onto the hyperplane A r k * \u00b7 x = b r k . So at each step of iteration, this method finds the closest vector that satisfies the chosen linear constraint.\nA 1 * \u00b7 x = b 1 A 2 * \u00b7 x = b 2 x 0 x 1 x 2 x 3\nx 4\n\nx 5\n\nx 6\n\nx * From the definition, we can simplify the notation by defining\nb = diag( A i * \u22121 : i \u2208 [m])b, A = diag( A i * \u22121 : i \u2208 [m])A,(3)so thatb i = b i / A i * and A i * = A i * / A i * .\nThen we can rewrite (2) as\nx k+1 = x k + b r k \u2212 A r k * |x k A r k * = I \u2212 | A r k * A r k * | x k +b r k A r k * .(4)\nIn [34], the Kaczmarz algorithm has been quantized based on the description (4). As an orthogonal projector, it is not hard to extend I \u2212 | A i * A i * | to a block of a unitary operator. For example, a unitary of the form\nI \u2297 (I \u2212 | A i * A i * |) + X \u2297 | A i * A i * |, where X is Pauli-X.\nThis unitary is closely related to the Grover diffusion operator when we apply it to |\u2212 |x .\n\nLemma 2 (Theorem 2 of [35]). Assume that Ax = b is consistent. Let x * = A + b be a solution of (1). Then the randomized Kaczmarz algorithm converges to x * in expectation, with the average error\nE[ x T \u2212 x * 2 ] \u2264 (1 \u2212 \u03ba \u22122 F ) T x 0 \u2212 x * 2 ,(5)\nwhere\n\u03ba F = A F A + .\nThe above result is stated for consistent linear systems, while the Kaczmarz algorithm also works for inconsistent linear systems, for example, see [27]. The convergence rate does not change, the error bound is affected by an extra term depending on min Ax \u2212 b . Our results (especially the main Theorem 13) stated below are indeed for general cases -consistent and inconsistent.\n\nThe above result holds for all x 0 , so throughout this paper, we shall assume x 0 = 0 for simplicity. In the above lemma, by Markov's inequality, with high probability 0.99, we have\nx T \u2212 x * 2 \u2264 100(1 \u2212 \u03ba \u22122 F ) T x * 2 .\nTo make sure the error is bounded by 2 x * 2 with high probability, it suffices to choose\nT = O(\u03ba 2 F log(1/ )).(6)\nFrom (2), we know that there exist y k,0 , . . . , y k,k\u22121 such that x k = k\u22121 j=0 y k,j A r j * , that is x k = A \u2020 y k for some vector y k . One obvious fact is that y k is at most k-sparse. In [12], y k is called the sparse description of x k . So this description comes easily in the Kaczmarz method. Actually, y k converges to the optimal solution of the dual problem of the LSP (1). More precisely, an alternative formulation of LSP is to find the least-norm solution of the linear system min x\u2208C n\n1 2 x 2 , s.t. Ax = b.\nThe dual problem takes the form:\nmin y\u2208C m g(y) := 1 2 A \u2020 y 2 \u2212 b \u00b7 y.(7)\nThe randomized Kaczmarz algorithm (2) is equivalent to one step of the stochastic gradient descent method [29] applied to 1 2 \n(A r k * \u00b7 x \u2212 b r k ) 2 with stepsize 1/ A r k * 2 .\nIt is also equivalent to one step of the randomized coordinate descent method [30] applied to the dual problem (7). Namely, when we apply this method to the r k -th component, the gradient is \u2207 r k g = A r k * |A \u2020 |y k \u2212 b r k . Setting the stepsize as 1/ A r k * 2 leads to the updating\ny k+1 = y k + b r k \u2212 A r k * |A \u2020 |y k A r k * 2 e r k ,(8)\nwhere {e 1 , . . . , e m } is the standard basis of C m . We can recover the original Kaczmarz iteration (2) by multiplying A \u2020 on both sides of (8).\n\nThe Kaczmarz method can be generalized to use multiple rows at each step of iteration [16]. Instead of orthogonally projecting to a hyperplane determined by one linear constraint, we can orthogonally project to a vector space defined by several linear constraints. This idea is similar to the minibatch stochastic gradient descent. However, this generalization needs to compute the pseudoinverse of certain matrices. A simple pseudoinverse-free variant is as follows [27]:\nx k+1 = x k + \u03b1 2q i\u2208T k b i \u2212 A i * |x k A i * 2 A i * ,(9)\nwhere T k is a random set of q row indices sampled with replacement, \u03b1 is known as the relaxation parameter. Each index i \u2208 [m] is put into T k with probability proportional to A i * 2 . For instance, let \u03b1 = 2, then (9) can be rewritten in a parallel form\nx k+1 = 1 q i\u2208T k x k + b i \u2212 A i * |x k A i * 2 A i * .\nIt was proved in [27,Corollary 3] \nthat if \u03b1 = q = A 2 F / A 2 (also see a related proof in Appendix B), E[ x T \u2212 x * 2 ] \u2264 1 \u2212 \u03ba \u22122 T x 0 \u2212 x * 2 .(10)\nSo the number of iterations required to achieve error via (9) is O(\u03ba 2 log(1/ )).\n\n\nPreliminaries of quantum-inspired algorithms\n\nThis section briefly recalls some definitions and results about quantum-inspired algorithms that will be used in this paper. The main reference is [6].\n\n\nDefinition 3 (Query access). For a vector\nv \u2208 C n , we have Q(v), query access to v if for all i \u2208 [n], we can query v i . Likewise, for a matrix A \u2208 C m\u00d7n , we have Q(A) if for all (i, j) \u2208 [m] \u00d7 [n]\n, we can query A ij . Let q(v) (or q(A)) denote the (time) cost of such a query.\n\nDefinition 4 (Sampling and query access to a vector). For a vector v \u2208 C n , we have SQ(v), sampling and query access to v, if we can: The sampling and query access defined above can be viewed as a classical analogue to the QRAM data structure, which is widely used in many quantum machine learning (QML) algorithms. In QML, this data structure can be used to efficiently prepare the quantum states of vectors, which are usually the inputs of the QML algorithms. For matrices, it allows us to find an efficient blockencoding so that many quantum linear algebraic techniques become effective. However, the QRAM (and Definition 5) is a quite strong data structure because it contains a lot of information (e.g. A i * and A F ) about A, which is usually not easy to obtain. For instance, it usually needs \u0398( A 0 ) operations to build this data structure, which A 0 is the number of nonzero entries of A. For quantum-inspired algorithms, we need to use certain sampling techniques to make sure that the complexity has a polylog dependence on the dimension, so it seems quite natural to use the above data structure. In comparison, there are also some other efficient quantum models, e.g., sparse-access model, in which the QML algorithms are still efficient. For v \u2208 C n and \u03c6 \u2265 1, we have SQ \u03c6 (v), \u03c6oversampling and query access to v, if we have Q(v) and SQ(\u1e7d) for a vector\u1e7d \u2208 C n satisfying\n1. Sample: obtain independent samples i \u2208 [n] following the distribution D v \u2208 R n , where D v (i) = |v i | 2 / v 2 .\u1e7d 2 = \u03c6 v 2 and |\u1e7d i | 2 \u2265 |v i | 2 for all i \u2208 [n]. We denote s \u03c6 (v) = s(\u1e7d), q \u03c6 (v) = q(\u1e7d), n \u03c6 (v) = n(\u1e7d), and sq \u03c6 (v) = s \u03c6 (v) + q \u03c6 (v) + q(v) + n \u03c6 (v).\nFor example, if we view v as the vector (v, 0, . . . , 0) and\u1e7d as the vector (v, v cot(\u03b8)w) so that they have the same dimension, then it is easy to see that \u1e7d 2 = v 2 / sin 2 (\u03b8) and \u03c6 = 1/ sin 2 (\u03b8). The condition |\u1e7d i | 2 \u2265 |v i | 2 is also satisfied. But the above definition is stronger than this intuition. Lemma 7 (Lemma 2.9 of [6]). Given SQ \u03c6 (v) and \u03b4 \u2208 (0, 1] we can sample from D v with success probability at least 1 \u2212 \u03b4 and cost O(\u03c6sq \u03c6 (v) log(1/\u03b4)). We can also estimate v up to relative error in time O( \u22122 \u03c6sq \u03c6 (v) log(1/\u03b4)).\nLemma 8 (Lemma 2.10 of [6]). Given SQ \u03d5 i (v i ) and \u03bb i \u2208 C for i \u2208 [k], denote v = i \u03bb i v i , then we have SQ \u03c6 (v) for \u03c6 = k i \u03d5 i \u03bb i v i 2 / v 2 and sq \u03c6 (v) = max i s \u03d5 i (v i ) + i q(v i ).\nThe first lemma tells us how to construct the original sampling access from an over-sampled one. By Definition 6, given SQ \u03c6 (v), we have Q(v). Lemma 7 tells us how to obtain S(v) from SQ \u03c6 (v). So this lemma actually tells us how to obtain SQ(v) from SQ \u03c6 (v). The second lemma is about how to construct a linear combination of the data structures. Equivalently, it is about constructing the data structure for M c when we have the data structures of M and c, where M is a matrix and c is a vector.\n\nIn our algorithms below, v i will be a row of A, k will refer to the sparsity of a certain known vector and v will be the approximate solution to the linear system. So in Lemma 8, sq \u03c6 (v) = O(ksq(A)). Combing the above two lemmas, given SQ(v i ) and \u03bb i , the cost to sample and query v is O(k\u03c6sq(A)), and the cost to estimate its norm is O(k\u03c6sq(A)/ 2 ). So a main difficulty in the the complexity analysis is the estimation of \u03c6. 4 The algorithm for solving linear systems 4\n\n\n.1 Solving general linear systems\n\nIn this section, based on the Kaczmarz method, we give a quantum-inspired classical algorithm for solving linear systems. The basic idea is similar to [12]. However, our result has a lower complexity and the analysis is much simpler. The main idea is as follows: we first use the iteration (8) to compute y T , then output SQ(x T ) with x T = A \u2020 y T by Lemmas 7 and 8.\n\nWhen A is dense, the calculation of A r k * |A \u2020 |y k in (8) is expensive. Note that A r k * |A \u2020 |y k = n j=1 A r k ,j A * j |y k . As an inner product, we can use the sampling idea (e.g. Monte Carlo) to approximate it. Now let j 1 , . . . , j d be drawn randomly from [n] under the distribution that Pr[j] = A * j 2 / A 2 F . This distribution is designed to put more weight on the most important rows [10,35]. Let S = {j 1 , . . . , j d } be the sequence of the chosen indices. We modify the updating of y to:\ny k+1 = y k + 1 A r k * \uf8eb \uf8edb r k \u2212 1 d j\u2208S\u00c3 r k ,j A * j |y k A 2 F A * j 2 \uf8f6 \uf8f8 e r k .(11)\nFor conciseness, define the matrix D as\nD = j\u2208S A 2 F d A * j 2 |j j|.(12)\nThen\ny k+1 = y k +b r k \u2212 \u00c3 r k * |DA \u2020 |y k A r k * e r k(13)= y k +b r k \u2212 \u00c3 r k * |A \u2020 |y k A r k * e r k + \u00c3 r k * |(I \u2212 D)A \u2020 |y k A r k * e r k .(14)\nCompared to the original iteration (8), the procedure (14) contains a perturbation term. To analyze its convergence rate, we need to determine d. Before that, we first analyze the properties of the following random variable:\n\u00b5 k := \u00c3 r k * |(I \u2212 D)A \u2020 |y k = \u00c3 r k * |I \u2212 D|x k = \u00c3 r k * |x k \u2212 1 d j\u2208S\u00c3 r k ,j x k,j A 2 F A * j 2 .(15)\nLemma 9. The mean and the variance of\n\u00b5 k satisfy E D [\u00b5 k ] = 0, Var D [\u00b5 k ] \u2264 1 d n j=1\u00c3 2 r k ,j x 2 k,j A 2 F A * j 2 .\nProof. The results follow from the standard calculation.\nE D [\u00b5 k ] = \u00c3 r k * |x k \u2212 E D \uf8ee \uf8f0 1 d j\u2208S\u00c3 r k ,j x k,j A 2 F A * j 2 \uf8f9 \uf8fb = \u00c3 r k * |x k \u2212 E D \u00c3 r k ,j x k,j A 2 F A * j 2 = \u00c3 r k * |x k \u2212 n j=1\u00c3 r k ,j x k,j A 2 F A * j 2 A * j 2 A 2 F = 0. Var D [\u00b5 k ] = Var D \uf8ee \uf8f0 1 d j\u2208S\u00c3 r k ,j x k,j A 2 F A * j 2 \uf8f9 \uf8fb = j\u2208S Var D 1 d\u00c3 r k ,j x k,j A 2 F A * j 2 \u2264 1 d E D \u00c3 r k ,j x k,j A 2 F A * j 2 2 = 1 d n j=1 \u00c3 r k ,j x k,j A 2 F A * j 2 2 A * j 2 A 2 F = 1 d n j=1\u00c3 2 r k ,j x 2 k,j A 2 F A * j 2 .\nThis completes the proof.\n\n\nRemark 10. In an early version, we further bounded Var\nD [\u00b5 k ] by 1 d A 2 F min j\u2208[n] A * j 2 x k 2 . This leads to d = O(\u03ba 4 F / 2 ).\nIn comparison, the current bound leads to d = O(\u03ba 2 F / 2 ). So, the overall complexity of the algorithms presented below can be reduced by a factor of \u03ba 2 F .\n\nFrom (14), the updating of x k = A \u2020 y k then becomes\nx k+1 = x k + (b r k \u2212 \u00c3 r k * |x k )\u00c3 r k * + \u00c3 r k * |I \u2212 D|x k \u00c3 r k * .(16)\nNext, we prove a similar result to Lemma 2. The idea of our proof is similar to that of Lemma 2 from [35], which nicely utilizes the geometrical structure of the Kaczmarz method.\n\nProposition 11. Choose d = 4(\u03ba 2 F / 2 )(log 2/ 2 ), then after T = \u03ba 2 F (log 2/ 2 ) steps of iteration of the procedure (16)\n, we have E[ x T \u2212 x * 2 ] \u2264 2 x * 2 .\nProof. For any vector v such that Av = 0 we have v 2 \u2264 A + 2 Av 2 . This implies\nv 2 \u03ba 2 F = v 2 A 2 F A + 2 \u2264 m i=1 \u00c3 i * |v 2 A i * 2 A 2 F = E[ \u00c3 i * |v 2 ].(17)\nHere the random variable is defined by\nPr[i] = A i * 2 A 2 F .\nFor simplicity, we set the initial vector x 0 = 0. In procedure (16), we denotex k+1 = x k + (b r k \u2212 \u00c3 r k * |x k )\u00c3 r k * , it is the orthogonal projection of x k onto the hyperplane\u00c3 r k * \u00b7 x =b r k . Thus we can rewrite the iteration as\n\nx k+1 =x k+1 + \u00b5 k\u00c3r k * .\n\nThe orthogonality implies (see Figure 2)\nx k+1 \u2212 x * 2 = x k+1 \u2212 x * 2 + \u00b5 2 k x * \u00c3 r k * \u00b7 x =b r k x k x k+1\nx k+1 \u00b5 k Figure 2: Illustration of the orthogonality in the proof of Proposition 11.\n= x k \u2212 x * 2 \u2212 x k \u2212x k+1 2 + \u00b5 2 k = x k \u2212 x * 2 \u2212 x k \u2212 x * |\u00c3 r k * 2 + \u00b5 2 k = 1 \u2212 x k \u2212 x * x k \u2212 x * |\u00c3 r k * 2 x k \u2212 x * 2 + \u00b5 2 k .\nTaking the expectation yields\nE[ x k+1 \u2212 x * 2 ] = 1 \u2212 E x k \u2212 x * x k \u2212 x * |\u00c3 r k * 2 E[ x k \u2212 x * 2 ] + E[\u00b5 2 k ].\nIn the above, the expectation is taken first on D by fixing k, r k , then on r k by fixing k, and finally on k. Also if A(x k \u2212 x * ) = 0, then x k is already the optimal solution, so we assume that this does not happen before convergence.\n\nBy equation (17),\n1 \u2212 E x k \u2212 x * x k \u2212 x * |\u00c3 r k * 2 \u2264 1 \u2212 \u03ba \u22122 F .\nBy Lemma 9,\nE r k E D [\u00b5 2 k ] \u2264 E r k 1 d n j=1\u00c3 2 r k ,j x 2 k,j A 2 F A * j 2 = 1 d m r k =1 n j=1\u00c3 2 r k ,j x 2 k,j A 2 F A * j 2 A r k * 2 A 2 F = x k 2 d .\nHence,\nE[ x k+1 \u2212 x * 2 ] \u2264 1 \u2212 \u03ba \u22122 F E[ x k \u2212 x * 2 ] + 1 d E[ x k 2 ] \u2264 1 \u2212 \u03ba \u22122 F E[ x k \u2212 x * 2 ] + 2 d ( x * 2 + E[ x k \u2212 x * 2 ]) = (1 \u2212 \u03ba \u22122 F + 2 d )E[ x k \u2212 x * 2 ] + 2 d x * 2 . Now we choose d = 4\u03ba 2 F (log 2/ 2 ) 2\n, then the above estimation leads to\nE[ x k+1 \u2212 x * 2 ] \u2264 1 \u2212 \u03ba \u22122 F + 1 2 \u03ba \u22122 F 2 (log 2/ 2 ) \u22121 E[ x k \u2212 x * 2 ] + 2 2\u03ba 2 F (log 2/ 2 ) x * 2 .\nSetting \u03c1 := 1 \u2212 \u03ba \u22122 F + 1 2 \u03ba \u22122 F 2 (log 2/ 2 ) \u22121 which is less than 1, we finally obtain\nE[ x T \u2212 x * 2 ] \u2264 \u03c1 T E[ x 0 \u2212 x * 2 ] + 2 2\u03ba 2 F (log 2/ 2 ) x * 2 T \u22121 i=0 \u03c1 i \u2264 \u03c1 T x * 2 + T 2 2\u03ba 2 F (log 2/ 2 ) x * 2 .\nLet T = \u03ba 2 F (log 2/ 2 ), then\n\u03c1 T = exp(\u03ba 2 F (log 2/ 2 ) log(1 \u2212 \u03ba \u22122 F + \u03ba \u22122 F 2 (log 1/ ) \u22121 )) \u2248 exp(\u2212(log 2/ 2 )(1 \u2212 2 (log 1/ ) \u22121 ))) \u2248 2 /2. Therefore, E[ x T \u2212 x * 2 ] \u2264 2 x * 2 as claimed. Theorem 12. Assume that x * = A + b. Given SQ(A), Q(b)\n, there is an algorithm that returns SQ(x) such that x \u2212 x * \u2264 x * with probability at least 0.99 and\nsq(x) = O(( \u03ba 6 F 2 + \u03b1\u03ba 4 F \u03ba 4 )(sq(A) + q(b))),(18)\nwhere \u03b1 = \u03b7/(1 \u2212 \u03b7 2 ) and \u03b7 = Ax\n* \u2212 b / b .\nProof. Let d = O(\u03ba 2 F / 2 ). By the updating formula (11), we know that y k is at most k-sparse. So in the k-th step of the iteration, the calculation of the summation in (11) costs O(kd). At each step of the iteration, we sample a row of A and query the corresponding entry of b. So the overall cost of k-th step is O(kd(sq(A) + q(b))). After T = O(\u03ba 2 F log(1/ )) steps, it converges and the cost is O(T 2 d(sq(A) + q(b))) = O((\u03ba 6 F /\u03b5 2 )(sq(A) + q(b))). Note that x T = A \u2020 y T and y T is T -sparse. By Lemma 8 (taking v i to be rows of A), we can\nobtain SQ \u03c6 (x T ) in time O(T sq(A)), where \u03c6 = T i y 2 T,i A i * 2 x T 2 .\nIn Appendix A, we shall show that \u03c6 = O(\u03ba 4 F + \u03b1\u03ba 2 F \u03ba 4 ). By Lemma 7, we can obtain SQ(\nx T ) in time O(\u03c6T sq(A)) = O((\u03ba 6 F + \u03b1\u03ba 4 F \u03ba 4 )sq(A)).\nIn the above, the parameter \u03b7 describes the overlap of b in the column space of A. If b almost lies in the column space of A, then \u03b7 is close to 0. In this case, the complexity is basically dominated by the first term.\n\nWe next use the Kaczmarz method with averaging (9) to reduce the dependence on \u03ba F in the above theorem. Similar to (16), we change the updating rule into\nx k+1 = x k + 1 2 i\u2208T k (b i \u2212 \u00c3 i * |x k )\u00c3 i * + 1 2 i\u2208T k \u00c3 i * |I \u2212 D i |x k \u00c3 i * ,(19)\nThe definition of D i is similar to (12), which depends on i now. In Appendix B, we shall prove\nthat if d = O(\u03ba 2 F / 2 ), then after T = O(\u03ba 2 log(1/ )) steps of iteration, we have E[ x T \u2212 x * 2 ] \u2264 2 x 0 \u2212 x * 2 .\nTogether with the estimation of \u03c6 in Appendix C, we have the following improved result.\n\nTheorem 13. Assume that x * = A + b. Given SQ(A), Q(b), there is an algorithm that returns SQ(x) such that x \u2212 x * \u2264 x * with probability at least 0.99 and\nsq(x) = O ( \u03ba 4 F \u03ba 2 2 + \u03b1\u03ba 2 F \u03ba 6 )(sq(A) + q(b)) ,(20)\nwhere \u03b1 = \u03b7/(1 \u2212 \u03b7 2 ) and \u03b7 = Ax\n* \u2212 b / b .\nProof. From the updating formula (19), the updating of y k is\ny k+1 = y k + 1 2 i\u2208T kb i \u2212 \u00c3 i * |D i A \u2020 |y k A i * e i . So y k is at most (k\u03ba 2 F /\u03ba 2 )-sparse. Thus the cost in step k is O(kd\u03ba 2 F (sq(A) + q(b))/\u03ba 2 ) = O(k\u03ba 4 F (sq(A) + q(b))/\u03ba 2 2 ). After convergence, the total cost is O(T 2 \u03ba 4 F (sq(A) + q(b))/\u03ba 2 2 ) = O(\u03ba 4 F \u03ba 2 (sq(A) + q(b))/ 2 ). By the estimation in Appendix C, we have \u03c6 = O(\u03ba 2 F \u03ba 2 + \u03b1\u03ba 6 ), so the cost to obtain SQ(x T ) is O(\u03c6T \u03ba 2 F sq(A)/\u03ba 2 ) = O((\u03ba 4 F \u03ba 2 + \u03b1\u03ba 2 F \u03ba 6 )sq(A)).\n\nSolving symmetric positive definite linear systems\n\nNow suppose A is a symmetric positive definite matrix. Then a simple approach to solve Ax = b is the randomized coordinate descent iteration [16] x\nk+1 = x k \u2212 A r k * |x k \u2212 b r k A r k ,r k e r k ,(21)\nwhere the probability of choosing r k is A r k ,r k /Tr(A). If we apply the above iteration to the normal equation A \u2020 Ax = A \u2020 b, we will obtain the Kaczmarz method. As for the iteration (21), it is not hard to show that\nE[ x k \u2212 x * 2 ] \u2264 1 \u2212 1 A + Tr(A) k x 0 \u2212 x * 2 .\nIn fact, assume Ax * = b, then (21) implies that\nx k+1 \u2212 x * = I \u2212 |r k r k |A A r k ,r k (x k \u2212 x * ). Thus E[ x k+1 \u2212 x * 2 ] \u2264 E I \u2212 |r k r k |A A r k ,r k E[ x k \u2212 x * 2 ] = I \u2212 A Tr(A) E[ x k \u2212 x * 2 ] = 1 \u2212 1 A + Tr(A) E[ x k \u2212 x * 2 ].\nSimilar to the idea introduced above to handle Kaczmarz method, we can use the randomized coordinate descent method to solve dense linear systems Ax = b such that A is symmetric positive definite. The only difference is that A 2 F becomes Tr(A), and A + 2 becomes A + . Moreover, the randomized coordinate descent (21) can also be parallelized into\nx k+1 = x k \u2212 \u03b1 2q i\u2208T k A i * |x k \u2212 b i A i,i e i .(22)\nIf we take \u03b1 = q = Tr(A)/ A , then it converges after O(\u03ba log(1/ )) iterations. The proof is the same as that of (9). Therefore, by a similar argument to the proof of Theorem 13, we have Theorem 14. Let A be symmetric positive definite. Assume that x * = arg min Ax \u2212 b . Given SQ(A), Q(b), there is an algorithm that returns SQ(x) such that x \u2212 x * \u2264 x * with probability at least 0.99 and\nsq(x) = O Tr(A) 2 A + 2 \u03ba(sq(A) + q(b)) 2 .(23)\nThe above result is actually more general than Theorem 13. If we consider the normal equation A \u2020 Ax = A \u2020 b, Theorem 14 implies Theorem 13.\n\n\nDiscussion\n\n\nSolving row sparse linear systems\n\nWhen A is row sparse, the inner product in the Kaczmarz method (2) is not expensive to calculate. As a result, we can directly apply the Kaczmarz method to solve the linear system Ax = b. The result is summarized below, and the proof is straightforward. \n\nProof. In the Kaczmarz iteration (2), since A has row sparsity s, it follows that the calculation of the inner product A r k * |x k costs O(s) operations. This is also the cost of the k-th step of iteration. After convergence, the total cost is O(T s), where T = O(\u03ba 2 F log(1/ )). As for the sparsity of x k , we can choose the initial approximation x 0 = 0, then x k is at most sk-sparse.\n\nSimilarly, based on the randomized coordinate descent method (21), we have \n\nDifferently from quantum and quantum-inspired algorithms, the outputs of the above two randomized algorithms are vectors. It is known that if A has row and column sparsity s and given in the sparse access input model, the currently best known quantum algorithm for matrix inversion costs O(s A max A + log 2 (1/ )), where A max refers to the maximal entry in the sense of absolute value [4]. The comparison between this quantum algorithm and the randomized algorithm stated in Theorem 15 may be not fair enough because they are building on different models -the quantum algorithm uses the sparse access model, while the later one assumes access to SQ(A). However, if we only concentrate on the final complexity, the quantum linear solver achieves large speedups if A F A max . This can often happen, especially when the rank of A is large (e.g. A is a sparse unitary). However, when the rank of A is polylog in the dimension, then A F = O(s A max ) if A is row and column sparse. In this case, the quantum computer only achieves a quadratic speedup. For a row sparse but column dense matrix A, if it is stored in the QRAM data structure, the complexity to solve Ax = b using a quantum computer is O(\u03ba F log 2 (1/ )) [4]. This only gives a quadratic speedup in terms of \u03ba F over Theorem 15.\n\nBy Theorem 16, when A is symmetric positive definite, the randomized coordinate descent method finds an approximate solution in time O(Tr(A) A + log(1/ )). For simplicity, let us consider the special case that A is a sparse density operator, then Tr(A) = 1. Under certain conditions [13], we can efficiently find a unitary such that A is a sub-block. This means that the complexity of the quantum linear solver for this particular linear system is O( A A + log 2 (1/ )), while the algorithm based on Theorem 16 costs O( A + log(1/ )). So if A = \u0398(1), it is possible that there is no quantum speedup based on the current quantum linear solvers.\n\nIn conclusion, quantum linear solvers can build on many different models, e.g. sparse-access input model, QRAM model, etc. Whenever we have an efficient block-encoding, quantum computers can solve the corresponding linear system efficiently. In comparison, so far there is only one model for quantum-inspired algorithms. The above two theorems indicate that to explore large quantum speedups for sparse problems, it is better to focus on other models rather than the QRAM model. This also leads to the problem of exploring other models for quantum-inspired algorithms.\n\n\nReducing the dependence on A F\n\nThe Frobenius norm can be large for high-rank matrices. For example, when we use the discretization method (e.g. Euler method) to solve a linear system of ordinary differential equations, the obtained linear system contains the identity as a sub-matrix. So the Frobenius norm can be as large as the dimension of the problem. The performance of quantum-inspired algorithms is highly affected by the Frobenius norm, so currently they are mainly effective for low-rank systems. To enlarge the application scope, it is necessary to consider the problem of how to reduce or even remove the the dependence on the Frobenius norm in the complexity.\n\nTo the best of our knowledge, there are no generalizations of the Kaczmarz method such that the total cost is independent of A F . However, it can be generalized such that E[x k \u2212 x * ] \u2264 (1 \u2212 \u03ba \u2212r ) k x 0 \u2212 x * and each step of iteration is not expensive, where r \u2208 {1, 2}. For example, a simple generalization is\nx k+1 = x k + A 2 F A 2 (b r k \u2212 A r k * |x k ) A r k * .\nGenerally, the relaxation parameter A 2 F / A 2 2, so this iterative scheme is not convergent, 4 that is we usually do not have E[ x k \u2212 x * 2 ] \u2264 \u03c1 k x 0 \u2212 x * 2 for some \u03c1 < 1. However, we do have\nE[x k \u2212 x * ] \u2264 (1 \u2212 \u03ba \u22122 ) k x 0 \u2212 x * . The proof is straightforward. If we set T = O(\u03ba 2 log(1/ )), then E[x T \u2212 x * ] \u2264 x 0 \u2212 x * .\nThere are also some other generalizations [26,28,33] such that T = O(\u03ba log(1/ )), which is optimal for this kind of iterative schemes. One idea to apply this result is to generate L samples x T 1 , . . . ,\nx T L such that E[x T ] \u2212 1 L L l=1 x T l \u2264 E[x T ] .\nThe matrix Bernstein inequality might be helpful to determine L. However, to apply this method, we have to find tight bounds for x T \u2212 x * and E[ x T \u2212 x * 2 ], which seems not that obvious.\n\n\nOutlook\n\nIn this paper, for solving linear systems, we reduced the separation between quantum and quantuminspired algorithms from \u03ba F : \u03ba 6 F \u03ba 6 / 4 to \u03ba F : \u03ba 4 F \u03ba 2 / 2 . It is interesting to consider whether the parameters of our algorithm can be improved further. For the class of iterative schemes including the Kaczmarz iteration, the optimal convergence rate equals the condition number \u03ba (as opposed to \u03ba 2 F as in the Kaczmarz method), which is achieved by the conjugate gradient method. However, there is currently no such result for the Kaczmarz iterative schemes themselves. As discussed in Section 5.2, in [26,28,33], it was proved that there are generalizations of the Kaczmarz method\nso that E[x T \u2212 x * ] \u2264 (1 \u2212 \u03ba \u22121 ) T x 0 \u2212 x * .\nThis convergence result is a little weak but may be helpful to reduce the separation in terms of \u03ba F further.\nz k =b r k \u2212 \u00c3 r k * |A \u2020 |y k A r k * e r k , z k = \u00b5 k A r k * e r k . Denote Z = Ax * \u2212 b = min x Ax \u2212 b , \u039b = diag( A i * 2 : i \u2208 [m]).(27)\nIn the following, for any two vectors a, b, we define a|b \u039b = a|\u039b|b . To bound \u03c6, it suffices to bound y T 2 \u039b . From (26), it is plausible that y k+1\n2 \u039b \u2248 y k 2 \u039b + z k 2 \u039b + z k 2 \u039b .\nIn the following, we shall prove that in fact this holds up to a constant factor on average. We shall choose the initial vector as 0 for simplicity, i.e., y 0 = 0.\n\nFirst, we consider the case Z = 0, that is b = Ax * . In the following, we first fix k, r k and compute the mean value over the random variable D, then we compute the mean value over r k by fixing k. Finally, we calculate the mean value over the random variable k.\n\nFrom now on, we assume that d = O((\u03ba 2 F / 2 )(log 1/ )) and T = O(\u03ba 2 F log(1/ )). By Lemma 9,\nE D [ z k 2 \u039b ] \u2264 A 2 F x k 2 d min j\u2208[n] A * j 2 = 2 x k 2 4T \u2264 2 ( x k \u2212 x * 2 + x * 2 )\n2T ,\nE D [ z k |z k \u039b ] = b r k \u2212 \u00c3 r k * |x k E D [\u00b5 k ] = 0, E D [ y k |z k \u039b ] = A r k * y k |e r k E D [\u00b5 k ] = 0.\nAbout the norm of z k , we have\nE[ z k 2 \u039b ] = m r k =1 A r k * 2 A 2 F (b r k \u2212 \u00c3 r k * |A \u2020 |y k ) 2 A r k * 2 A r k * 2 = 1 A 2 F m r k =1 (b r k \u2212 A r k |x k ) 2 \u2264 2 b 2 + A 2 F x k 2 A 2 F \u2264 2 b 2 A 2 F + 4 x * 2 + 4 x * \u2212 x k 2 .\nAs for the inner product between y k and z k , we have the following estimate\nE[ y k |z k \u039b ] = m r k =1 A r k * 2 A 2 F (b r k \u2212 \u00c3 r k * |A \u2020 |y k ) A r k * y k |e r k A r k * 2 = m r k =1 A r k * 2 A 2 F (b r k \u2212 A r k * |A \u2020 |y k ) y k |e r k = b|y k \u039b \u2212 x k 2 \u039b A 2 F = x * |A \u2020 |y k \u039b \u2212 x k 2 \u039b A 2 F = x * |x k \u039b \u2212 x k 2 \u039b A 2 F \u2264 x * x k + x k 2 \u2264 3 x * 2 + x * x k \u2212 x * + 2 x k \u2212 x * 2 \u2264 7 2 x * 2 + 5 2 x k \u2212 x * 2 .\nIn the above, we used the fact that for any two vectors a, b we have | a|b \u039b | \u2264 \u039b 2 | a|b |, and\n\u039b 2 \u2264 A 2 F . Hence, we have E[ y k+1 2 \u039b ] = E[ y k 2 \u039b ] + E[ z k 2 \u039b ] + E[ z k 2 \u039b ] + 2E[ y k |z k \u039b ] \u2264 E[ y k 2 \u039b ] + 2 b 2 A 2 F + (9 + 2 2T )E[ x k \u2212 x * 2 ] + (11 + 2 2T ) x * 2 \u2264 E[ y k 2 \u039b ] + 2 b 2 A 2 F + (20 + 2 T ) x * 2 ,\nwhere we use that E[ x k \u2212 x * 2 ] \u2264 x * 2 by Lemma 2. Therefore,\nE[ y T 2 \u039b ] \u2264 T 2 b 2 A 2 F + (20 + 2 T ) x * 2 .\nThis means that, with high probability,\n\u03c6 = T y T 2 \u039b x T 2 \u2264 T 2 2 b 2 A 2 F x T 2 + (20 + 2 T ) x * 2 x T 2 = O(T 2 ).\nWhen Z = 0, then b = Ax * + c for some vector c of norm Z that is not in the column space of A. This can happen when A is not full rank. Since c is independent of A, we cannot bound it in terms of A. In this case, the only change is E[ y k |z k \u039b ], which is now bounded by\nE[ y k |z k \u039b ] \u2264 c|E[y k ] \u039b A 2 F + 7 2 x * 2 + 5 2 x k \u2212 x * 2 \u2264 A 2 A 2 F c E[y k ] + 7 2 x * 2 + 5 2 x k \u2212 x * 2 = \u03ba 2 Z \u03ba 2 F E[y k ] + 7 2 x * 2 + 5 2 x k \u2212 x * 2 .\nAt the end, we obtain\nE[ y k+1 2 \u039b ] \u2264 E[ y k 2 \u039b ] + 2 b 2 A 2 F + (20 + 2 T ) x * 2 + 2\u03ba 2 Z \u03ba 2 F E[y k ] .\nFrom (26), we know that\nE[y k+1 ] = I \u2212 AA \u2020 A 2 F E[y k ] + b A 2 F .\nThis means\nE[y k ] = k\u22121 i=0 I \u2212 AA \u2020 A 2 F i b A 2 F \u2264 k\u22121 i=0 (1 \u2212 \u03ba \u22122 F ) i b A 2 F \u2264 \u03ba 2 F b A 2 F .\nTherefore,\nE[ y T 2 \u039b ] \u2264 T 2 b 2 A 2 F + (20 + 2 T ) x * 2 + 2\u03ba 2 b Z A 2 F = T 2 Ax * 2 A 2 F + (20 + 2 T ) x * 2 + 2\u03ba 2 b Z + 2Z 2 A 2 F .\nFinally, by Markov's inequality, with a high probability we have\n\u03c6 = T y T 2 \u039b x T 2 = O T 2 + T 2 \u03ba 2 b Z + Z 2 A 2 F x * 2 ,\nwhere we used the fact that x T \u2248 x * and Ax\n* 2 / A 2 F \u2264 x * 2 . Let Z = \u03b7 b , then x * 2 \u2265 Ax * 2 / A 2 = (1 \u2212 \u03b7 2 ) b 2 / A 2 . So T 2 \u03ba 2 b Z A 2 F x * 2 \u2264 \u03b7 1 \u2212 \u03b7 2 \u03ba 2 F \u03ba 4 .\nThis means\n\u03c6 = O \u03ba 4 F + \u03b7 1 \u2212 \u03b7 2 \u03ba 2 F \u03ba 4 .\n\nAppendix B Estimation of the convergence rate\n\nFor simplicity, we assume that Ax * = b. Following the analysis of [27], the convergence rate does not change too much if the LSP is not consistent. By the updating formula (19), we have\nx k+1 \u2212 x * = x k \u2212 x * + 1 2 i\u2208T k (b i \u2212 \u00c3 i * |x k )|\u00c3 i * + 1 2 i\u2208T k \u00c3 i * |I \u2212 D i |x k |\u00c3 i * = \uf8eb \uf8ed I \u2212 1 2 i\u2208T k |\u00c3 i * \u00c3 i * | \uf8f6 \uf8f8 (x k \u2212 x * ) + 1 2 i\u2208T k \u00c3 i * |I \u2212 D i |x k |\u00c3 i * .\nBelow, we try to bound E[ x k+1 \u2212 x * 2 ]. We will follow the notation of (15). But here to avoid any confusion, we denote \u00b5 ik := \u00c3 i * |I \u2212 D i |x k .\n\nIn the following, the result of Lemma 9 will be used, and |T k | = A 2 F / A 2 . First, we have\nE D \uf8ee \uf8f0 i\u2208T k \u00c3 i * |I \u2212 D i |x k |\u00c3 i * 2 \uf8f9 \uf8fb = i,j\u2208T k \u00c3 i * |\u00c3 j * E D i ,D j [\u00b5 ik \u00b5 jk ] = i\u2208T k E D i [\u00b5 2 ik ] + i =j \u00c3 i * |\u00c3 j * E D i [\u00b5 ik ] E D j [\u00b5 jk ] \u2264 1 d i\u2208T k n j=1\u00c3 2 i,j x 2 k,j A 2 F A * j 2 .\nBy Lemma 9, we have\nE D \uf8ee \uf8f0 \uf8eb \uf8ed I \u2212 1 2 i\u2208T k |\u00c3 i * \u00c3 i * | \uf8f6 \uf8f8 (x k \u2212 x * ) i\u2208T k \u00b5 ik |\u00c3 i * \uf8f9 \uf8fb = \uf8eb \uf8ed I \u2212 1 2 i\u2208T k |\u00c3 i * \u00c3 i * | \uf8f6 \uf8f8 (x k \u2212 x * ) i\u2208T k E D [\u00b5 ik ]|\u00c3 i * = 0.\nHence, after computing the mean value over D, we have\nx k+1 \u2212 x * 2 \u2264 \uf8eb \uf8ed I \u2212 1 2 i\u2208T k |\u00c3 i * \u00c3 i * | \uf8f6 \uf8f8 (x k \u2212 x * ) 2 + 1 4d i\u2208T k n j=1\u00c3 2 i,j x 2 k,j A 2 F A * j 2 .\nFor the first term, we now compute its mean value over the random variable T k\nE \uf8ee \uf8f0 \uf8eb \uf8ed I \u2212 1 2 i\u2208T k |\u00c3 i * \u00c3 i * | \uf8f6 \uf8f8 (x k \u2212 x * ) 2 \uf8f9 \uf8fb = x k \u2212 x * |E \uf8ee \uf8f0 \uf8eb \uf8ed I \u2212 1 2 i\u2208T k |\u00c3 i * \u00c3 i * | \uf8f6 \uf8f8 2 \uf8f9 \uf8fb |x k \u2212 x * .\nAnd it can be shown that\nE \uf8ee \uf8f0 \uf8eb \uf8ed I \u2212 1 2 i\u2208T k |\u00c3 i * \u00c3 i * | \uf8f6 \uf8f8 2 \uf8f9 \uf8fb = E \uf8ee \uf8f0 I \u2212 3 4 i\u2208T k |\u00c3 i * \u00c3 i * | + 1 4 i,j\u2208T k ,i =j |\u00c3 i * \u00c3 i * |\u00c3 j * \u00c3 j * | \uf8f9 \uf8fb = I \u2212 3q 4 A \u2020 A A 2 F + 1 4 (q 2 \u2212 q) A \u2020 A A 2 F 2 1 \u2212 3q 4 \u03c3 min (A \u2020 A) A 2 F + 1 4 (q 2 \u2212 q) \u03c3 min (A \u2020 A) A 2 F 2 I 1 \u2212 1 2\u03ba 2 I.\nIn the last step, we used the result q = A 2 F / A 2 so that the second term is 3/4\u03ba 2 and the third term is less than 1/4\u03ba 4 \u2264 1/4\u03ba 2 .\n\nTherefore,\nE[ x k+1 \u2212 x * 2 ] \u2264 (1 \u2212 1 2\u03ba 2 )E[ x k \u2212 x * 2 ] + 1 4d E \uf8ee \uf8f0 i\u2208T k n j=1\u00c3 2 i,j x 2 k,j A 2 F A * j 2 \uf8f9 \uf8fb = (1 \u2212 1 2\u03ba 2 )E[ x k \u2212 x * 2 ] + A 2 F 4d A 2 E[ x k 2 ].\nNow set T = O(\u03ba 2 log(2/ 2 )), and\nd = T A 2 F A 2 2 = \u03ba 2 F log(2/ 2 ) 2 .(28)\nThen\nE[ x k+1 \u2212 x * 2 ] \u2264 (1 \u2212 1 2\u03ba 2 )E[ x k \u2212 x * 2 ] + 2 4T E[ x k 2 ] \u2264 (1 \u2212 1 2\u03ba 2 + 2 2T )E[ x k \u2212 x * 2 ] + 2 2T x * 2 .\nFinally, we obtain\nE[ x T \u2212 x * 2 ] (1 \u2212 1 2\u03ba 2 + 2 2T ) T x 0 \u2212 x * 2 + 2 2 x * 2 \u2264 2 x * 2 .\nwhere\nw k := i\u2208T k b i \u2212 A i * |x k A i * 2 e i * , w k := i\u2208T k \u00b5 ik A i * e i * .\nIn this section, d is given in the formula (28). From a similar estimation in Appendix A, we have E D [ w k |w k \u039b ] = E D [ y k |w k \u039b ] = 0 and\nE r k E D [ w k 2 \u039b ] \u2264 A 2 F A 2 1 d n j=1 E r k [\u00c3 2 r k ,j ]x 2 k,j A 2 F A * j 2 = 2 x k 2 4T \u2264 2 ( x k \u2212 x * 2 + x * 2 )\n2T .\n\nAs for w k 2 \u039b , we still have\nE[ w k 2 \u039b ] = E \uf8ee \uf8f0 i,j\u2208T k b i \u2212 A i * |x k A i * 2 b j \u2212 A j * |x k A j * 2 e i * |e j * A i * 2 \uf8f9 \uf8fb = A 2 F A 2 E (b i \u2212 A i * |x k ) 2 A i * 2 = b \u2212 Ax k 2 A 2 \u2264 2 b 2 A 2 + 2 x k 2 .\nBy the estimation of E[ y k |z k \u039b ] in Appendix A and noting that \u039b \u2264 A , we also have\nE[ y k |w k \u039b ] = A 2 F A 2 E[ y k | b i \u2212 A i * |x k A i * 2 e i * ] \u2264 7 2 x * 2 + 5 2 x k \u2212 x * 2 .\nAll the estimations above do not change. The constant 1/2 in the decomposition of y k+1 does not affect the upper bound, so when Z = 0 we have\nE[ y T 2 \u039b ] \u2264 T 2 b 2 A 2 F + (20 + 2 T ) x * 2 ,\nwhere T = O(\u03ba 2 ). Therefore\n\u03c6 = T \u03ba 2 F \u03ba 2 y T 2 \u039b x T 2 \u2264 T 2 \u03ba 2 F \u03ba 2 2 b 2 A 2 F x T 2 + (20 + 2 T ) x * 2 x T 2 = O(\u03ba 2 F \u03ba 2 ).\nWhen Z = 0, we similarly have\n\u03c6 = O \u03ba 2 F \u03ba 2 + \u03b7 1 \u2212 \u03b7 2 \u03ba 6 .\nFigure 1 :\n1Illustration of Kaczmarz algorithm when m = 2, where x * is the optimal solution.\n\n2 .\n2Query: query entries of v as in Q(v); 3. Norm: query v . Let q(v), s(v), and n(v) denote the cost of querying entries, sampling indices, and querying the norm respectively. Further define sq(v) := q(v) + s(v) + n(v). Definition 5 (Sampling and query access to a matrix). For any matrix A \u2208 C m\u00d7n , we have SQ(A) if we have SQ(A i * ) for all i \u2208 [m] and SQ(a), where a is the vector of row norms. We define sq(A) as the cost to obtain SQ(A).\n\n\nThe sampling operation on v can be viewed as a classical analogue of the measurement of the quantum state |v = v \u22121 i\u2208[n] v i |i in the computational basis. For some problems, we do not have the state |v exactly. Instead, what we have is a state of the form sin(\u03b8)|v |0 + cos(\u03b8)|w |1 . Then the probability of obtaining |i, 0 is v 2 i v 2 sin 2 (\u03b8). This leads to the definition of oversampling for quantum-inspired algorithms.Definition 6 (Oversampling and query access).\n\nTheorem 15 .\n15Assume that A has row sparsity s. Let x * = arg min Ax\u2212b . Given SQ(A), SQ(b), then there is an algorithm that returns an O(s\u03ba 2 F log(1/ ))-sparse vector x such that x \u2212 x * \u2264 x * with probability at least 0.99 in time O(s\u03ba 2 F log(1/ )).\n\nTheorem 16 .\n16Suppose that A is symmetric positive definite and s-sparse. Let x * = arg min Ax \u2212 b . Given SQ(A), SQ(b), then there is an algorithm that returns an O(Tr(A) A + log(1/ ))-sparse vector x such that x \u2212 x * \u2264 x * with probability at least 0.99 in time O(sTr(A) A + log(1/ )).\n\n\n.Complexity \nReference \nAssumptions \n\nQuantum algorithm \nO(\u03ba F ) \n[4], 2018 \n\nRandomized classical algorithm \nO(s\u03ba 2 \nF ) \n[35], 2009 \nrow sparse \n\nO(sTr(A) A + ) \n[23], 2010 \nsparse, SPD \n\nQuantum-inspired algorithm \n\nO(\u03ba 6 \nF \u03ba 6 / 4 ) \n[12], 2020 \n\n\nIn fact, \u03b1 = \u00b5(A) for some \u00b5(A) \u2264 A F (see[13]). The definition of \u00b5(A) is quite complicated. In this paper, for convenience of the comparison we just use A F .2 In the literature, \u03baF does not have a standard name. It is named the scaled condition number in[16,35].\nIn[31], the authors mentioned two special cases. Case 1: Access to a block-encoding of I \u2212\u03b7A and A + b \u2208 O(\u03ba), where \u03b7 \u2208 (0, 1]. Case 2: A is a sum of SPD local Hamiltonians, b is sparse and a parameter \u03b3 \u2208 O(1) that quantifies the overlap of b with the subspace where A is non-singular.\nTo make sure that it is convergent, the relaxation parameter should be chosen from (0, 2). Usually, the optimal choice is 1.\nAcknowledgementWe would like to thank Ryan Mann for useful discussions. This paper was supported by the Quan-tERA ERA-NET Cofund in Quantum Technologies implemented within the European Union's Horizon 2020 Programme (QuantAlgo project), and EPSRC grants EP/L021005/1 and EP/R043957/1. This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No. 817581). No new data were created during this study.Appendix A Estimation of \u03c6 Using the notation (15), we can decompose the updating rule(14)of y as followswhereAppendix C Estimation of \u03c6 for Kaczmarz method with averagingThe calculation here is similar to that in Appendix A. For simplicity, denote\nQuantuminspired algorithms in practice. Juan Miguel Arrazola, Alain Delgado, Seth Bhaskar Roy Bardhan, Lloyd, arXiv:1905.10415307Juan Miguel Arrazola, Alain Delgado, Bhaskar Roy Bardhan, and Seth Lloyd. Quantum- inspired algorithms in practice. Quantum, 4:307, 2020. arXiv:1905.10415.\n\nQuantum machine learning. Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, Seth Lloyd, Nature. 5497671Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth Lloyd. Quantum machine learning. Nature, 549(7671):195-202, 2017.\n\nParallel optimization: Theory, algorithms, and applications. Yair Censor, Stavros Andrea Zenios, Oxford University Press on DemandYair Censor, Stavros Andrea Zenios, et al. Parallel optimization: Theory, algorithms, and applications. Oxford University Press on Demand, 1997.\n\nThe Power of Block-Encoded Matrix Powers: Improved Regression Techniques via Faster Hamiltonian Simulation. Shantanav Chakraborty, Andr\u00e1s Gily\u00e9n, Stacey Jeffery, arXiv:1804.0197346th International Colloquium on Automata, Languages, and Programming (ICALP 2019). 33Shantanav Chakraborty, Andr\u00e1s Gily\u00e9n, and Stacey Jeffery. The Power of Block-Encoded Matrix Powers: Improved Regression Techniques via Faster Hamiltonian Simulation. In 46th International Colloquium on Automata, Languages, and Programming (ICALP 2019), pages 33:1-33:14, 2019. arXiv:1804.01973.\n\nNadiia Chepurko, L Kenneth, Lior Clarkson, David P Horesh, Woodruff, arXiv:2011.04125Quantum-inspired algorithms from randomized numerical linear algebra. arXiv preprintNadiia Chepurko, Kenneth L Clarkson, Lior Horesh, and David P Woodruff. Quantum-inspired algorithms from randomized numerical linear algebra. arXiv preprint arXiv:2011.04125, 2020.\n\nSampling-based sublinear low-rank matrix arithmetic framework for dequantizing quantum machine learning. Nai-Hui Chia, Andr\u00e1s Gily\u00e9n, Tongyang Li, Han-Hsuan Lin, Ewin Tang, Chunhao Wang, arXiv:1910.06151Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing (STOC 2020). the 52nd Annual ACM SIGACT Symposium on Theory of Computing (STOC 2020)Nai-Hui Chia, Andr\u00e1s Gily\u00e9n, Tongyang Li, Han-Hsuan Lin, Ewin Tang, and Chunhao Wang. Sampling-based sublinear low-rank matrix arithmetic framework for dequantizing quantum machine learning. In Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing (STOC 2020), pages 387-400, 2020. arXiv:1910.06151.\n\nQuantum-inspired classical algorithms for singular value transformation. Nai-Hui Chia, Tongyang Li, Han-Hsuan Lin, Chunhao Wang, arXiv:1901.0325445th International Symposium on Mathematical Foundations of Computer Science (MFCS 2020). 232020:15. Schloss Dagstuhl-Leibniz-Zentrum f\u00fcr InformatikNai-Hui Chia, Tongyang Li, Han-Hsuan Lin, and Chunhao Wang. Quantum-inspired classical algorithms for singular value transformation. In 45th International Symposium on Mathe- matical Foundations of Computer Science (MFCS 2020), pages 23:1-23:15. Schloss Dagstuhl- Leibniz-Zentrum f\u00fcr Informatik, 2020. arXiv:1901.03254.\n\nQuantum-inspired sublinear classical algorithms for solving low-rank linear systems. Nai-Hui Chia, Han-Hsuan Lin, Chunhao Wang, arXiv:1811.04852arXiv preprintNai-Hui Chia, Han-Hsuan Lin, and Chunhao Wang. Quantum-inspired sublinear classical algorithms for solving low-rank linear systems. arXiv preprint arXiv:1811.04852, 2018.\n\nQuantum algorithm for systems of linear equations with exponentially improved dependence on precision. M Andrew, Robin Childs, Rolando D Kothari, Somma, SIAM Journal on Computing. 466Andrew M Childs, Robin Kothari, and Rolando D Somma. Quantum algorithm for systems of linear equations with exponentially improved dependence on precision. SIAM Journal on Computing, 46(6):1920-1950, 2017.\n\nFast Monte Carlo algorithms for matrices II: Computing a low-rank approximation to a matrix. Petros Drineas, Ravi Kannan, Michael W Mahoney, SIAM Journal on computing. 361Petros Drineas, Ravi Kannan, and Michael W Mahoney. Fast Monte Carlo algorithms for matrices II: Computing a low-rank approximation to a matrix. SIAM Journal on computing, 36(1):158-183, 2006.\n\nQuantum-inspired low-rank stochastic regression with logarithmic dependence on the dimension. Andr\u00e1s Gily\u00e9n, Seth Lloyd, Ewin Tang, arXiv:1811.04909Andr\u00e1s Gily\u00e9n, Seth Lloyd, and Ewin Tang. Quantum-inspired low-rank stochastic regression with logarithmic dependence on the dimension. 2018. arXiv:1811.04909.\n\nAn improved quantum-inspired algorithm for linear regression. Andr\u00e1s Gily\u00e9n, Zhao Song, Ewin Tang, arXiv:2009.07268Andr\u00e1s Gily\u00e9n, Zhao Song, and Ewin Tang. An improved quantum-inspired algorithm for linear regression. 2020. arXiv:2009.07268.\n\nQuantum singular value transformation and beyond: exponential improvements for quantum matrix arithmetics. Andr\u00e1s Gily\u00e9n, Yuan Su, Guang Hao Low, Nathan Wiebe, Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing. the 51st Annual ACM SIGACT Symposium on Theory of ComputingAndr\u00e1s Gily\u00e9n, Yuan Su, Guang Hao Low, and Nathan Wiebe. Quantum singular value transformation and beyond: exponential improvements for quantum matrix arithmetics. In Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing, pages 193-204, 2019.\n\nMatrix Computations. Gene H Golub, Charles F Van Loan, The Johns Hopkins University PressBaltimorefourth editionGene H. Golub and Charles F. van Loan. Matrix Computations. The Johns Hopkins University Press, Baltimore, fourth edition, 2013.\n\nAlgebraic reconstruction techniques (art) for three-dimensional electron microscopy and x-ray photography. Richard Gordon, Robert Bender, Gabor T Herman, Journal of theoretical Biology. 293Richard Gordon, Robert Bender, and Gabor T Herman. Algebraic reconstruction techniques (art) for three-dimensional electron microscopy and x-ray photography. Journal of theoretical Biology, 29(3):471-481, 1970.\n\nRandomized iterative methods for linear systems. M Robert, Peter Gower, Richt\u00e1rik, SIAM Journal on Matrix Analysis and Applications. 364Robert M Gower and Peter Richt\u00e1rik. Randomized iterative methods for linear systems. SIAM Journal on Matrix Analysis and Applications, 36(4):1660-1690, 2015.\n\nQuantum algorithm for linear systems of equations. W Aram, Avinatan Harrow, Seth Hassidim, Lloyd, arXiv:0811.3171Phys Rev Lett. 10315150502Aram W Harrow, Avinatan Hassidim, and Seth Lloyd. Quantum algorithm for linear systems of equations. Phys Rev Lett, 103(15):150502, 2009. arXiv:0811.3171.\n\nQuantum-inspired classical algorithms for singular value transformation. Dhawal Jethwani, Franccois Le Gall, K Sanjay, Singh, arXiv:1910.0569945th International Symposium on Mathematical Foundations of Computer Science (MFCS 2020). 202014. Schloss Dagstuhl-Leibniz-Zentrum f\u00fcr InformatikDhawal Jethwani, Franccois Le Gall, and Sanjay K Singh. Quantum-inspired classical algo- rithms for singular value transformation. In 45th International Symposium on Mathematical Foundations of Computer Science (MFCS 2020), page 53:1-53:14. Schloss Dagstuhl-Leibniz- Zentrum f\u00fcr Informatik, 2020. arXiv:1910.05699.\n\nAngenaherte auflosung von systemen linearer glei-chungen. S Karczmarz, Bull. Int. Acad. Pol. Sic. Let., Cl. Sci. Math. Nat. S Karczmarz. Angenaherte auflosung von systemen linearer glei-chungen. Bull. Int. Acad. Pol. Sic. Let., Cl. Sci. Math. Nat., pages 355-357, 1937.\n\nq-means: A quantum algorithm for unsupervised machine learning. Iordanis Kerenidis, Jonas Landman, Alessandro Luongo, Anupam Prakash, Advances in Neural Information Processing Systems. Iordanis Kerenidis, Jonas Landman, Alessandro Luongo, and Anupam Prakash. q-means: A quantum algorithm for unsupervised machine learning. In Advances in Neural Information Processing Systems, pages 4134-4144, 2019.\n\nQuantum Recommendation Systems. Iordanis Kerenidis, Anupam Prakash, arXiv:1603.086758th Innovations in Theoretical Computer Science Conference (ITCS 2017). 49Iordanis Kerenidis and Anupam Prakash. Quantum Recommendation Systems. In 8th In- novations in Theoretical Computer Science Conference (ITCS 2017), pages 49:1-49:21, 2017. arXiv:1603.08675.\n\nA quantum interior point method for LPs and SDPs. Iordanis Kerenidis, Anupam Prakash, ACM Transactions on Quantum Computing. 11Iordanis Kerenidis and Anupam Prakash. A quantum interior point method for LPs and SDPs. ACM Transactions on Quantum Computing, 1(1):1-32, 2020.\n\nRandomized methods for linear constraints: convergence rates and conditioning. Dennis Leventhal, Adrian S Lewis, Mathematics of Operations Research. 353Dennis Leventhal and Adrian S Lewis. Randomized methods for linear constraints: convergence rates and conditioning. Mathematics of Operations Research, 35(3):641-654, 2010.\n\nOptimal polynomial based quantum eigenstate filtering with application to solving quantum linear systems. Lin Lin, Yu Tong, 361Lin Lin and Yu Tong. Optimal polynomial based quantum eigenstate filtering with application to solving quantum linear systems. Quantum, 4:361, 2020.\n\nQuantum principal component analysis. Seth Lloyd, Masoud Mohseni, Patrick Rebentrost, arXiv:1307.0401Nature Physics. 109Seth Lloyd, Masoud Mohseni, and Patrick Rebentrost. Quantum principal component analysis. Nature Physics, 10(9):631-633, 2014. arXiv:1307.0401.\n\nMomentum and stochastic momentum for stochastic gradient, newton, proximal point and subspace descent methods. Nicolas Loizou, Peter Richt\u00e1rik, Computational Optimization and Applications. 773Nicolas Loizou and Peter Richt\u00e1rik. Momentum and stochastic momentum for stochastic gradient, newton, proximal point and subspace descent methods. Computational Optimization and Applications, 77(3):653-710, 2020.\n\nRandomized Kaczmarz with averaging. Jacob D Moorman, K Thomas, Denali Tu, Deanna Molitor, Needell, arXiv:2002.04126Jacob D Moorman, Thomas K Tu, Denali Molitor, and Deanna Needell. Randomized Kaczmarz with averaging. 2020. arXiv:2002.04126.\n\nFaster randomized block Kaczmarz algorithms. Ion Necoara, arXiv:1902.09946SIAM J Matrix Anal Appl. 404Ion Necoara. Faster randomized block Kaczmarz algorithms. SIAM J Matrix Anal Appl, 40(4):1425-1452, 2019. arXiv:1902.09946.\n\nRobust stochastic approximation approach to stochastic programming. Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, Alexander Shapiro, SIAM J Optim. 194Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro. Robust stochas- tic approximation approach to stochastic programming. SIAM J Optim, 19(4):1574-1609, 2009.\n\nEfficiency of coordinate descent methods on huge-scale optimization problems. Yu Nesterov, SIAM J Optim. 222Yu Nesterov. Efficiency of coordinate descent methods on huge-scale optimization problems. SIAM J Optim, 22(2):341-362, 2012.\n\nOn solving classes of positive-definite quantum linear systems with quadratically improved runtime in the condition number. Davide Orsucci, Vedran Dunjko, arXiv:2101.11868arXiv preprintDavide Orsucci and Vedran Dunjko. On solving classes of positive-definite quantum lin- ear systems with quadratically improved runtime in the condition number. arXiv preprint arXiv:2101.11868, 2021.\n\nQuantum support vector machine for big data classification. Patrick Rebentrost, Masoud Mohseni, Seth Lloyd, arXiv:1307.0471Phys Rev Lett. 11313130503Patrick Rebentrost, Masoud Mohseni, and Seth Lloyd. Quantum support vector machine for big data classification. Phys Rev Lett, 113(13):130503, 2014. arXiv:1307.0471.\n\nStochastic reformulations of linear systems: algorithms and convergence theory. Peter Richt\u00e1rik, Martin Tak\u00e1c, SIAM Journal on Matrix Analysis and Applications. 412Peter Richt\u00e1rik and Martin Tak\u00e1c. Stochastic reformulations of linear systems: algorithms and convergence theory. SIAM Journal on Matrix Analysis and Applications, 41(2):487-524, 2020.\n\nRow and column iteration methods to solve linear systems on a quantum computer. Changpeng Shao, Hua Xiang, arXiv:1905.11686Phys Rev A. 101222322Changpeng Shao and Hua Xiang. Row and column iteration methods to solve linear systems on a quantum computer. Phys Rev A, 101(2):022322, 2020. arXiv:1905.11686.\n\nA randomized kaczmarz algorithm with exponential convergence. Thomas Strohmer, Roman Vershynin, arXiv:math/0702226J Fourier Anal Appl. 152262Thomas Strohmer and Roman Vershynin. A randomized kaczmarz algorithm with exponential convergence. J Fourier Anal Appl, 15(2):262, 2009. arXiv:math/0702226.\n\nQuantum-inspired classical algorithms for principal component analysis and supervised clustering. Ewin Tang, arXiv:1811.00414Ewin Tang. Quantum-inspired classical algorithms for principal component analysis and su- pervised clustering. 2018. arXiv:1811.00414.\n\nA quantum-inspired classical algorithm for recommendation systems. Ewin Tang, arXiv:1807.04271Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing (STOC 2019). the 51st Annual ACM SIGACT Symposium on Theory of Computing (STOC 2019)Ewin Tang. A quantum-inspired classical algorithm for recommendation systems. In Proceed- ings of the 51st Annual ACM SIGACT Symposium on Theory of Computing (STOC 2019), pages 217-228, 2019. arXiv:1807.04271.\n", "annotations": {"author": "[{\"end\":142,\"start\":79},{\"end\":227,\"start\":143}]", "publisher": null, "author_last_name": "[{\"end\":93,\"start\":89},{\"end\":159,\"start\":150}]", "author_first_name": "[{\"end\":88,\"start\":79},{\"end\":149,\"start\":143}]", "author_affiliation": "[{\"end\":141,\"start\":95},{\"end\":207,\"start\":161},{\"end\":226,\"start\":209}]", "title": "[{\"end\":62,\"start\":1},{\"end\":289,\"start\":228}]", "venue": null, "abstract": "[{\"end\":1399,\"start\":305}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b16\"},\"end\":1478,\"start\":1474},{\"end\":1607,\"start\":1589},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":1745,\"start\":1741},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":1775,\"start\":1772},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":1778,\"start\":1775},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":1781,\"start\":1778},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":1784,\"start\":1781},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":1787,\"start\":1784},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":1790,\"start\":1787},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":1793,\"start\":1790},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":1796,\"start\":1793},{\"end\":2557,\"start\":2549},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2722,\"start\":2719},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":2903,\"start\":2902},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3059,\"start\":3056},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3071,\"start\":3070},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3232,\"start\":3228},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3379,\"start\":3376},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":3640,\"start\":3636},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":3644,\"start\":3640},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":3648,\"start\":3644},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4686,\"start\":4683},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5072,\"start\":5068},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":5141,\"start\":5137},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":5201,\"start\":5197},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":5253,\"start\":5249},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":5256,\"start\":5253},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":5259,\"start\":5256},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":5262,\"start\":5259},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5904,\"start\":5901},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6429,\"start\":6425},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":6506,\"start\":6502},{\"end\":7434,\"start\":7416},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7662,\"start\":7658},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7842,\"start\":7838},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":8778,\"start\":8775},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9305,\"start\":9301},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":9518,\"start\":9517},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":9563,\"start\":9559},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":10094,\"start\":10091},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":10159,\"start\":10156},{\"end\":10178,\"start\":10170},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":11034,\"start\":11031},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":11152,\"start\":11148},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":13835,\"start\":13831},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":14341,\"start\":14337},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":15276,\"start\":15272},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":15681,\"start\":15677},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":16077,\"start\":16073},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":16846,\"start\":16842},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":17359,\"start\":17355},{\"end\":17374,\"start\":17371},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":17512,\"start\":17508},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":17544,\"start\":17541},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":18021,\"start\":18017},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18402,\"start\":18398},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":18800,\"start\":18796},{\"end\":18812,\"start\":18800},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":19212,\"start\":19209},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":21506,\"start\":21503},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22845,\"start\":22844},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":22888,\"start\":22887},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":23081,\"start\":23077},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":23705,\"start\":23701},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":23708,\"start\":23705},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":25435,\"start\":25431},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":25665,\"start\":25661},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":28567,\"start\":28563},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":29443,\"start\":29440},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":29513,\"start\":29509},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":29681,\"start\":29677},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":30245,\"start\":30241},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":30931,\"start\":30927},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":31298,\"start\":31294},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":31824,\"start\":31820},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":33658,\"start\":33655},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":34487,\"start\":34484},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":34846,\"start\":34842},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":37203,\"start\":37199},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":37206,\"start\":37203},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":37209,\"start\":37206},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":38235,\"start\":38231},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":38238,\"start\":38235},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":38241,\"start\":38238},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":38737,\"start\":38733},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":42129,\"start\":42125},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":42235,\"start\":42231},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":42517,\"start\":42513},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":44523,\"start\":44519},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":47423,\"start\":47419},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":47538,\"start\":47537},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":47638,\"start\":47634},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":47641,\"start\":47638},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":47649,\"start\":47645}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":45651,\"start\":45557},{\"attributes\":{\"id\":\"fig_1\"},\"end\":46099,\"start\":45652},{\"attributes\":{\"id\":\"fig_2\"},\"end\":46574,\"start\":46100},{\"attributes\":{\"id\":\"fig_3\"},\"end\":46830,\"start\":46575},{\"attributes\":{\"id\":\"fig_4\"},\"end\":47121,\"start\":46831},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":47376,\"start\":47122}]", "paragraph": "[{\"end\":2238,\"start\":1415},{\"end\":3547,\"start\":2240},{\"end\":4062,\"start\":3549},{\"end\":4867,\"start\":4064},{\"end\":6275,\"start\":4869},{\"end\":6626,\"start\":6292},{\"end\":6869,\"start\":6656},{\"end\":7188,\"start\":6871},{\"end\":8048,\"start\":7190},{\"end\":8841,\"start\":8050},{\"end\":9621,\"start\":8843},{\"end\":10545,\"start\":9623},{\"end\":11153,\"start\":10547},{\"end\":12186,\"start\":11183},{\"end\":12741,\"start\":12188},{\"end\":13531,\"start\":12743},{\"end\":13751,\"start\":13565},{\"end\":14291,\"start\":13772},{\"end\":14442,\"start\":14293},{\"end\":14620,\"start\":14501},{\"end\":14900,\"start\":14622},{\"end\":14952,\"start\":14949},{\"end\":14957,\"start\":14954},{\"end\":14962,\"start\":14959},{\"end\":15029,\"start\":14964},{\"end\":15175,\"start\":15149},{\"end\":15491,\"start\":15269},{\"end\":15653,\"start\":15561},{\"end\":15850,\"start\":15655},{\"end\":15908,\"start\":15903},{\"end\":16304,\"start\":15925},{\"end\":16488,\"start\":16306},{\"end\":16619,\"start\":16530},{\"end\":17150,\"start\":16646},{\"end\":17206,\"start\":17174},{\"end\":17375,\"start\":17249},{\"end\":17718,\"start\":17430},{\"end\":17929,\"start\":17780},{\"end\":18403,\"start\":17931},{\"end\":18721,\"start\":18465},{\"end\":18813,\"start\":18779},{\"end\":19013,\"start\":18932},{\"end\":19213,\"start\":19062},{\"end\":19497,\"start\":19417},{\"end\":20888,\"start\":19499},{\"end\":21712,\"start\":21168},{\"end\":22410,\"start\":21911},{\"end\":22888,\"start\":22412},{\"end\":23295,\"start\":22926},{\"end\":23809,\"start\":23297},{\"end\":23941,\"start\":23902},{\"end\":23981,\"start\":23977},{\"end\":24357,\"start\":24133},{\"end\":24507,\"start\":24470},{\"end\":24651,\"start\":24595},{\"end\":25126,\"start\":25101},{\"end\":25424,\"start\":25265},{\"end\":25479,\"start\":25426},{\"end\":25738,\"start\":25560},{\"end\":25866,\"start\":25740},{\"end\":25986,\"start\":25906},{\"end\":26109,\"start\":26071},{\"end\":26375,\"start\":26134},{\"end\":26403,\"start\":26377},{\"end\":26445,\"start\":26405},{\"end\":26602,\"start\":26517},{\"end\":26773,\"start\":26744},{\"end\":27101,\"start\":26862},{\"end\":27120,\"start\":27103},{\"end\":27184,\"start\":27173},{\"end\":27341,\"start\":27335},{\"end\":27599,\"start\":27563},{\"end\":27803,\"start\":27710},{\"end\":27962,\"start\":27931},{\"end\":28289,\"start\":28188},{\"end\":28378,\"start\":28345},{\"end\":28944,\"start\":28391},{\"end\":29113,\"start\":29022},{\"end\":29391,\"start\":29173},{\"end\":29547,\"start\":29393},{\"end\":29736,\"start\":29641},{\"end\":29945,\"start\":29858},{\"end\":30102,\"start\":29947},{\"end\":30195,\"start\":30162},{\"end\":30269,\"start\":30208},{\"end\":30933,\"start\":30786},{\"end\":31211,\"start\":30990},{\"end\":31311,\"start\":31263},{\"end\":31854,\"start\":31506},{\"end\":32303,\"start\":31913},{\"end\":32492,\"start\":32352},{\"end\":32797,\"start\":32543},{\"end\":33189,\"start\":32799},{\"end\":33266,\"start\":33191},{\"end\":34557,\"start\":33268},{\"end\":35202,\"start\":34559},{\"end\":35772,\"start\":35204},{\"end\":36447,\"start\":35807},{\"end\":36763,\"start\":36449},{\"end\":37020,\"start\":36822},{\"end\":37362,\"start\":37157},{\"end\":37607,\"start\":37417},{\"end\":38310,\"start\":37619},{\"end\":38470,\"start\":38361},{\"end\":38765,\"start\":38615},{\"end\":38965,\"start\":38802},{\"end\":39231,\"start\":38967},{\"end\":39328,\"start\":39233},{\"end\":39424,\"start\":39420},{\"end\":39570,\"start\":39539},{\"end\":39852,\"start\":39775},{\"end\":40299,\"start\":40202},{\"end\":40604,\"start\":40539},{\"end\":40695,\"start\":40656},{\"end\":41050,\"start\":40777},{\"end\":41244,\"start\":41223},{\"end\":41357,\"start\":41334},{\"end\":41415,\"start\":41405},{\"end\":41521,\"start\":41511},{\"end\":41717,\"start\":41653},{\"end\":41824,\"start\":41780},{\"end\":41973,\"start\":41963},{\"end\":42244,\"start\":42058},{\"end\":42591,\"start\":42439},{\"end\":42688,\"start\":42593},{\"end\":42923,\"start\":42904},{\"end\":43138,\"start\":43085},{\"end\":43335,\"start\":43257},{\"end\":43497,\"start\":43473},{\"end\":43908,\"start\":43772},{\"end\":43920,\"start\":43910},{\"end\":44123,\"start\":44089},{\"end\":44173,\"start\":44169},{\"end\":44315,\"start\":44297},{\"end\":44397,\"start\":44392},{\"end\":44621,\"start\":44476},{\"end\":44752,\"start\":44748},{\"end\":44784,\"start\":44754},{\"end\":45061,\"start\":44974},{\"end\":45306,\"start\":45164},{\"end\":45386,\"start\":45358},{\"end\":45523,\"start\":45494}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":6655,\"start\":6627},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13771,\"start\":13752},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14500,\"start\":14443},{\"attributes\":{\"id\":\"formula_3\"},\"end\":14948,\"start\":14901},{\"attributes\":{\"id\":\"formula_4\"},\"end\":15096,\"start\":15030},{\"attributes\":{\"id\":\"formula_5\"},\"end\":15148,\"start\":15096},{\"attributes\":{\"id\":\"formula_6\"},\"end\":15268,\"start\":15176},{\"attributes\":{\"id\":\"formula_7\"},\"end\":15560,\"start\":15492},{\"attributes\":{\"id\":\"formula_8\"},\"end\":15902,\"start\":15851},{\"attributes\":{\"id\":\"formula_9\"},\"end\":15924,\"start\":15909},{\"attributes\":{\"id\":\"formula_10\"},\"end\":16529,\"start\":16489},{\"attributes\":{\"id\":\"formula_11\"},\"end\":16645,\"start\":16620},{\"attributes\":{\"id\":\"formula_12\"},\"end\":17173,\"start\":17151},{\"attributes\":{\"id\":\"formula_13\"},\"end\":17248,\"start\":17207},{\"attributes\":{\"id\":\"formula_14\"},\"end\":17429,\"start\":17376},{\"attributes\":{\"id\":\"formula_15\"},\"end\":17779,\"start\":17719},{\"attributes\":{\"id\":\"formula_16\"},\"end\":18464,\"start\":18404},{\"attributes\":{\"id\":\"formula_17\"},\"end\":18778,\"start\":18722},{\"attributes\":{\"id\":\"formula_18\"},\"end\":18931,\"start\":18814},{\"attributes\":{\"id\":\"formula_19\"},\"end\":19416,\"start\":19258},{\"attributes\":{\"id\":\"formula_20\"},\"end\":21006,\"start\":20889},{\"attributes\":{\"id\":\"formula_21\"},\"end\":21167,\"start\":21006},{\"attributes\":{\"id\":\"formula_22\"},\"end\":21910,\"start\":21713},{\"attributes\":{\"id\":\"formula_23\"},\"end\":23901,\"start\":23810},{\"attributes\":{\"id\":\"formula_24\"},\"end\":23976,\"start\":23942},{\"attributes\":{\"id\":\"formula_25\"},\"end\":24039,\"start\":23982},{\"attributes\":{\"id\":\"formula_26\"},\"end\":24132,\"start\":24039},{\"attributes\":{\"id\":\"formula_27\"},\"end\":24469,\"start\":24358},{\"attributes\":{\"id\":\"formula_28\"},\"end\":24594,\"start\":24508},{\"attributes\":{\"id\":\"formula_29\"},\"end\":25100,\"start\":24652},{\"attributes\":{\"id\":\"formula_30\"},\"end\":25264,\"start\":25184},{\"attributes\":{\"id\":\"formula_31\"},\"end\":25559,\"start\":25480},{\"attributes\":{\"id\":\"formula_32\"},\"end\":25905,\"start\":25867},{\"attributes\":{\"id\":\"formula_33\"},\"end\":26070,\"start\":25987},{\"attributes\":{\"id\":\"formula_34\"},\"end\":26133,\"start\":26110},{\"attributes\":{\"id\":\"formula_35\"},\"end\":26516,\"start\":26446},{\"attributes\":{\"id\":\"formula_36\"},\"end\":26743,\"start\":26603},{\"attributes\":{\"id\":\"formula_37\"},\"end\":26861,\"start\":26774},{\"attributes\":{\"id\":\"formula_38\"},\"end\":27172,\"start\":27121},{\"attributes\":{\"id\":\"formula_39\"},\"end\":27334,\"start\":27185},{\"attributes\":{\"id\":\"formula_40\"},\"end\":27562,\"start\":27342},{\"attributes\":{\"id\":\"formula_41\"},\"end\":27709,\"start\":27600},{\"attributes\":{\"id\":\"formula_42\"},\"end\":27930,\"start\":27804},{\"attributes\":{\"id\":\"formula_43\"},\"end\":28187,\"start\":27963},{\"attributes\":{\"id\":\"formula_44\"},\"end\":28344,\"start\":28290},{\"attributes\":{\"id\":\"formula_45\"},\"end\":28390,\"start\":28379},{\"attributes\":{\"id\":\"formula_46\"},\"end\":29021,\"start\":28945},{\"attributes\":{\"id\":\"formula_47\"},\"end\":29172,\"start\":29114},{\"attributes\":{\"id\":\"formula_48\"},\"end\":29640,\"start\":29548},{\"attributes\":{\"id\":\"formula_49\"},\"end\":29857,\"start\":29737},{\"attributes\":{\"id\":\"formula_50\"},\"end\":30161,\"start\":30103},{\"attributes\":{\"id\":\"formula_51\"},\"end\":30207,\"start\":30196},{\"attributes\":{\"id\":\"formula_52\"},\"end\":30732,\"start\":30270},{\"attributes\":{\"id\":\"formula_53\"},\"end\":30989,\"start\":30934},{\"attributes\":{\"id\":\"formula_54\"},\"end\":31262,\"start\":31212},{\"attributes\":{\"id\":\"formula_55\"},\"end\":31505,\"start\":31312},{\"attributes\":{\"id\":\"formula_56\"},\"end\":31912,\"start\":31855},{\"attributes\":{\"id\":\"formula_57\"},\"end\":32351,\"start\":32304},{\"attributes\":{\"id\":\"formula_60\"},\"end\":36821,\"start\":36764},{\"attributes\":{\"id\":\"formula_61\"},\"end\":37156,\"start\":37021},{\"attributes\":{\"id\":\"formula_62\"},\"end\":37416,\"start\":37363},{\"attributes\":{\"id\":\"formula_63\"},\"end\":38360,\"start\":38311},{\"attributes\":{\"id\":\"formula_64\"},\"end\":38614,\"start\":38471},{\"attributes\":{\"id\":\"formula_65\"},\"end\":38801,\"start\":38766},{\"attributes\":{\"id\":\"formula_66\"},\"end\":39419,\"start\":39329},{\"attributes\":{\"id\":\"formula_67\"},\"end\":39538,\"start\":39425},{\"attributes\":{\"id\":\"formula_68\"},\"end\":39774,\"start\":39571},{\"attributes\":{\"id\":\"formula_69\"},\"end\":40201,\"start\":39853},{\"attributes\":{\"id\":\"formula_70\"},\"end\":40538,\"start\":40300},{\"attributes\":{\"id\":\"formula_71\"},\"end\":40655,\"start\":40605},{\"attributes\":{\"id\":\"formula_72\"},\"end\":40776,\"start\":40696},{\"attributes\":{\"id\":\"formula_73\"},\"end\":41222,\"start\":41051},{\"attributes\":{\"id\":\"formula_74\"},\"end\":41333,\"start\":41245},{\"attributes\":{\"id\":\"formula_75\"},\"end\":41404,\"start\":41358},{\"attributes\":{\"id\":\"formula_76\"},\"end\":41510,\"start\":41416},{\"attributes\":{\"id\":\"formula_77\"},\"end\":41652,\"start\":41522},{\"attributes\":{\"id\":\"formula_78\"},\"end\":41779,\"start\":41718},{\"attributes\":{\"id\":\"formula_79\"},\"end\":41962,\"start\":41825},{\"attributes\":{\"id\":\"formula_80\"},\"end\":42009,\"start\":41974},{\"attributes\":{\"id\":\"formula_81\"},\"end\":42438,\"start\":42245},{\"attributes\":{\"id\":\"formula_82\"},\"end\":42903,\"start\":42689},{\"attributes\":{\"id\":\"formula_83\"},\"end\":43084,\"start\":42924},{\"attributes\":{\"id\":\"formula_84\"},\"end\":43256,\"start\":43139},{\"attributes\":{\"id\":\"formula_85\"},\"end\":43472,\"start\":43336},{\"attributes\":{\"id\":\"formula_86\"},\"end\":43771,\"start\":43498},{\"attributes\":{\"id\":\"formula_87\"},\"end\":44088,\"start\":43921},{\"attributes\":{\"id\":\"formula_88\"},\"end\":44168,\"start\":44124},{\"attributes\":{\"id\":\"formula_89\"},\"end\":44296,\"start\":44174},{\"attributes\":{\"id\":\"formula_90\"},\"end\":44391,\"start\":44316},{\"attributes\":{\"id\":\"formula_91\"},\"end\":44475,\"start\":44398},{\"attributes\":{\"id\":\"formula_92\"},\"end\":44747,\"start\":44622},{\"attributes\":{\"id\":\"formula_93\"},\"end\":44973,\"start\":44785},{\"attributes\":{\"id\":\"formula_94\"},\"end\":45163,\"start\":45062},{\"attributes\":{\"id\":\"formula_95\"},\"end\":45357,\"start\":45307},{\"attributes\":{\"id\":\"formula_96\"},\"end\":45493,\"start\":45387},{\"attributes\":{\"id\":\"formula_97\"},\"end\":45557,\"start\":45524}]", "table_ref": "[{\"end\":6625,\"start\":6618},{\"end\":6701,\"start\":6694},{\"end\":9635,\"start\":9628}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":1413,\"start\":1401},{\"attributes\":{\"n\":\"1.1\"},\"end\":6290,\"start\":6278},{\"attributes\":{\"n\":\"1.2\"},\"end\":11181,\"start\":11156},{\"attributes\":{\"n\":\"2\"},\"end\":13563,\"start\":13534},{\"attributes\":{\"n\":\"3\"},\"end\":19060,\"start\":19016},{\"end\":19257,\"start\":19216},{\"end\":22924,\"start\":22891},{\"end\":25183,\"start\":25129},{\"attributes\":{\"n\":\"4.2\"},\"end\":30784,\"start\":30734},{\"attributes\":{\"n\":\"5\"},\"end\":32505,\"start\":32495},{\"attributes\":{\"n\":\"5.1\"},\"end\":32541,\"start\":32508},{\"attributes\":{\"n\":\"5.2\"},\"end\":35805,\"start\":35775},{\"attributes\":{\"n\":\"6\"},\"end\":37617,\"start\":37610},{\"end\":42056,\"start\":42011},{\"end\":45568,\"start\":45558},{\"end\":45656,\"start\":45653},{\"end\":46588,\"start\":46576},{\"end\":46844,\"start\":46832}]", "table": "[{\"end\":47376,\"start\":47125}]", "figure_caption": "[{\"end\":45651,\"start\":45570},{\"end\":46099,\"start\":45658},{\"end\":46574,\"start\":46102},{\"end\":46830,\"start\":46591},{\"end\":47121,\"start\":46847},{\"end\":47125,\"start\":47124}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":14688,\"start\":14680},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":26444,\"start\":26436},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":26535,\"start\":26527}]", "bib_author_first_name": "[{\"end\":48856,\"start\":48852},{\"end\":48879,\"start\":48874},{\"end\":48893,\"start\":48889},{\"end\":49129,\"start\":49124},{\"end\":49145,\"start\":49140},{\"end\":49160,\"start\":49154},{\"end\":49178,\"start\":49171},{\"end\":49197,\"start\":49191},{\"end\":49209,\"start\":49205},{\"end\":49454,\"start\":49450},{\"end\":49470,\"start\":49463},{\"end\":49477,\"start\":49471},{\"end\":49782,\"start\":49773},{\"end\":49802,\"start\":49796},{\"end\":49817,\"start\":49811},{\"end\":50231,\"start\":50225},{\"end\":50243,\"start\":50242},{\"end\":50257,\"start\":50253},{\"end\":50275,\"start\":50268},{\"end\":50688,\"start\":50681},{\"end\":50701,\"start\":50695},{\"end\":50718,\"start\":50710},{\"end\":50732,\"start\":50723},{\"end\":50742,\"start\":50738},{\"end\":50756,\"start\":50749},{\"end\":51340,\"start\":51333},{\"end\":51355,\"start\":51347},{\"end\":51369,\"start\":51360},{\"end\":51382,\"start\":51375},{\"end\":51966,\"start\":51959},{\"end\":51982,\"start\":51973},{\"end\":51995,\"start\":51988},{\"end\":52308,\"start\":52307},{\"end\":52322,\"start\":52317},{\"end\":52338,\"start\":52331},{\"end\":52340,\"start\":52339},{\"end\":52693,\"start\":52687},{\"end\":52707,\"start\":52703},{\"end\":52723,\"start\":52716},{\"end\":52725,\"start\":52724},{\"end\":53059,\"start\":53053},{\"end\":53072,\"start\":53068},{\"end\":53084,\"start\":53080},{\"end\":53336,\"start\":53330},{\"end\":53349,\"start\":53345},{\"end\":53360,\"start\":53356},{\"end\":53624,\"start\":53618},{\"end\":53637,\"start\":53633},{\"end\":53647,\"start\":53642},{\"end\":53663,\"start\":53657},{\"end\":54096,\"start\":54092},{\"end\":54098,\"start\":54097},{\"end\":54113,\"start\":54106},{\"end\":54115,\"start\":54114},{\"end\":54427,\"start\":54420},{\"end\":54442,\"start\":54436},{\"end\":54458,\"start\":54451},{\"end\":54764,\"start\":54763},{\"end\":54778,\"start\":54773},{\"end\":55061,\"start\":55060},{\"end\":55076,\"start\":55068},{\"end\":55089,\"start\":55085},{\"end\":55383,\"start\":55377},{\"end\":55403,\"start\":55394},{\"end\":55406,\"start\":55404},{\"end\":55414,\"start\":55413},{\"end\":55966,\"start\":55965},{\"end\":56250,\"start\":56242},{\"end\":56267,\"start\":56262},{\"end\":56287,\"start\":56277},{\"end\":56302,\"start\":56296},{\"end\":56619,\"start\":56611},{\"end\":56637,\"start\":56631},{\"end\":56986,\"start\":56978},{\"end\":57004,\"start\":56998},{\"end\":57286,\"start\":57280},{\"end\":57304,\"start\":57298},{\"end\":57306,\"start\":57305},{\"end\":57636,\"start\":57633},{\"end\":57644,\"start\":57642},{\"end\":57846,\"start\":57842},{\"end\":57860,\"start\":57854},{\"end\":57877,\"start\":57870},{\"end\":58187,\"start\":58180},{\"end\":58201,\"start\":58196},{\"end\":58529,\"start\":58528},{\"end\":58544,\"start\":58538},{\"end\":58555,\"start\":58549},{\"end\":58765,\"start\":58762},{\"end\":59018,\"start\":59012},{\"end\":59038,\"start\":59031},{\"end\":59057,\"start\":59049},{\"end\":59072,\"start\":59063},{\"end\":59361,\"start\":59359},{\"end\":59646,\"start\":59640},{\"end\":59662,\"start\":59656},{\"end\":59968,\"start\":59961},{\"end\":59987,\"start\":59981},{\"end\":60001,\"start\":59997},{\"end\":60302,\"start\":60297},{\"end\":60320,\"start\":60314},{\"end\":60656,\"start\":60647},{\"end\":60666,\"start\":60663},{\"end\":60941,\"start\":60935},{\"end\":60957,\"start\":60952},{\"end\":61274,\"start\":61270},{\"end\":61504,\"start\":61500}]", "bib_author_last_name": "[{\"end\":48872,\"start\":48857},{\"end\":48887,\"start\":48880},{\"end\":48913,\"start\":48894},{\"end\":48920,\"start\":48915},{\"end\":49138,\"start\":49130},{\"end\":49152,\"start\":49146},{\"end\":49169,\"start\":49161},{\"end\":49189,\"start\":49179},{\"end\":49203,\"start\":49198},{\"end\":49215,\"start\":49210},{\"end\":49461,\"start\":49455},{\"end\":49484,\"start\":49478},{\"end\":49794,\"start\":49783},{\"end\":49809,\"start\":49803},{\"end\":49825,\"start\":49818},{\"end\":50240,\"start\":50232},{\"end\":50251,\"start\":50244},{\"end\":50266,\"start\":50258},{\"end\":50282,\"start\":50276},{\"end\":50292,\"start\":50284},{\"end\":50693,\"start\":50689},{\"end\":50708,\"start\":50702},{\"end\":50721,\"start\":50719},{\"end\":50736,\"start\":50733},{\"end\":50747,\"start\":50743},{\"end\":50761,\"start\":50757},{\"end\":51345,\"start\":51341},{\"end\":51358,\"start\":51356},{\"end\":51373,\"start\":51370},{\"end\":51387,\"start\":51383},{\"end\":51971,\"start\":51967},{\"end\":51986,\"start\":51983},{\"end\":52000,\"start\":51996},{\"end\":52315,\"start\":52309},{\"end\":52329,\"start\":52323},{\"end\":52348,\"start\":52341},{\"end\":52355,\"start\":52350},{\"end\":52701,\"start\":52694},{\"end\":52714,\"start\":52708},{\"end\":52733,\"start\":52726},{\"end\":53066,\"start\":53060},{\"end\":53078,\"start\":53073},{\"end\":53089,\"start\":53085},{\"end\":53343,\"start\":53337},{\"end\":53354,\"start\":53350},{\"end\":53365,\"start\":53361},{\"end\":53631,\"start\":53625},{\"end\":53640,\"start\":53638},{\"end\":53655,\"start\":53648},{\"end\":53669,\"start\":53664},{\"end\":54104,\"start\":54099},{\"end\":54124,\"start\":54116},{\"end\":54434,\"start\":54428},{\"end\":54449,\"start\":54443},{\"end\":54465,\"start\":54459},{\"end\":54771,\"start\":54765},{\"end\":54784,\"start\":54779},{\"end\":54795,\"start\":54786},{\"end\":55066,\"start\":55062},{\"end\":55083,\"start\":55077},{\"end\":55098,\"start\":55090},{\"end\":55105,\"start\":55100},{\"end\":55392,\"start\":55384},{\"end\":55411,\"start\":55407},{\"end\":55421,\"start\":55415},{\"end\":55428,\"start\":55423},{\"end\":55976,\"start\":55967},{\"end\":56260,\"start\":56251},{\"end\":56275,\"start\":56268},{\"end\":56294,\"start\":56288},{\"end\":56310,\"start\":56303},{\"end\":56629,\"start\":56620},{\"end\":56645,\"start\":56638},{\"end\":56996,\"start\":56987},{\"end\":57012,\"start\":57005},{\"end\":57296,\"start\":57287},{\"end\":57312,\"start\":57307},{\"end\":57640,\"start\":57637},{\"end\":57649,\"start\":57645},{\"end\":57852,\"start\":57847},{\"end\":57868,\"start\":57861},{\"end\":57888,\"start\":57878},{\"end\":58194,\"start\":58188},{\"end\":58211,\"start\":58202},{\"end\":58526,\"start\":58511},{\"end\":58536,\"start\":58530},{\"end\":58547,\"start\":58545},{\"end\":58563,\"start\":58556},{\"end\":58572,\"start\":58565},{\"end\":58773,\"start\":58766},{\"end\":59029,\"start\":59019},{\"end\":59047,\"start\":59039},{\"end\":59061,\"start\":59058},{\"end\":59080,\"start\":59073},{\"end\":59370,\"start\":59362},{\"end\":59654,\"start\":59647},{\"end\":59669,\"start\":59663},{\"end\":59979,\"start\":59969},{\"end\":59995,\"start\":59988},{\"end\":60007,\"start\":60002},{\"end\":60312,\"start\":60303},{\"end\":60326,\"start\":60321},{\"end\":60661,\"start\":60657},{\"end\":60672,\"start\":60667},{\"end\":60950,\"start\":60942},{\"end\":60967,\"start\":60958},{\"end\":61279,\"start\":61275},{\"end\":61509,\"start\":61505}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:1905.10415\",\"id\":\"b0\"},\"end\":49096,\"start\":48812},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":25062002},\"end\":49387,\"start\":49098},{\"attributes\":{\"id\":\"b2\"},\"end\":49663,\"start\":49389},{\"attributes\":{\"doi\":\"arXiv:1804.01973\",\"id\":\"b3\",\"matched_paper_id\":4614529},\"end\":50223,\"start\":49665},{\"attributes\":{\"doi\":\"arXiv:2011.04125\",\"id\":\"b4\"},\"end\":50574,\"start\":50225},{\"attributes\":{\"doi\":\"arXiv:1910.06151\",\"id\":\"b5\",\"matched_paper_id\":204509632},\"end\":51258,\"start\":50576},{\"attributes\":{\"doi\":\"arXiv:1901.03254\",\"id\":\"b6\",\"matched_paper_id\":204509006},\"end\":51872,\"start\":51260},{\"attributes\":{\"doi\":\"arXiv:1811.04852\",\"id\":\"b7\"},\"end\":52202,\"start\":51874},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":3834959},\"end\":52592,\"start\":52204},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":5453786},\"end\":52957,\"start\":52594},{\"attributes\":{\"doi\":\"arXiv:1811.04909\",\"id\":\"b10\"},\"end\":53266,\"start\":52959},{\"attributes\":{\"doi\":\"arXiv:2009.07268\",\"id\":\"b11\"},\"end\":53509,\"start\":53268},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":46941335},\"end\":54069,\"start\":53511},{\"attributes\":{\"id\":\"b13\"},\"end\":54311,\"start\":54071},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":12331729},\"end\":54712,\"start\":54313},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":8215294},\"end\":55007,\"start\":54714},{\"attributes\":{\"doi\":\"arXiv:0811.3171\",\"id\":\"b16\",\"matched_paper_id\":5187993},\"end\":55302,\"start\":55009},{\"attributes\":{\"doi\":\"arXiv:1910.05699\",\"id\":\"b17\",\"matched_paper_id\":204509006},\"end\":55905,\"start\":55304},{\"attributes\":{\"id\":\"b18\"},\"end\":56176,\"start\":55907},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":54465030},\"end\":56577,\"start\":56178},{\"attributes\":{\"doi\":\"arXiv:1603.08675\",\"id\":\"b20\",\"matched_paper_id\":579463},\"end\":56926,\"start\":56579},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":52112641},\"end\":57199,\"start\":56928},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":10414859},\"end\":57525,\"start\":57201},{\"attributes\":{\"id\":\"b23\"},\"end\":57802,\"start\":57527},{\"attributes\":{\"doi\":\"arXiv:1307.0401\",\"id\":\"b24\",\"matched_paper_id\":5589954},\"end\":58067,\"start\":57804},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":4442618},\"end\":58473,\"start\":58069},{\"attributes\":{\"doi\":\"arXiv:2002.04126\",\"id\":\"b26\"},\"end\":58715,\"start\":58475},{\"attributes\":{\"doi\":\"arXiv:1902.09946\",\"id\":\"b27\",\"matched_paper_id\":119328624},\"end\":58942,\"start\":58717},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":1767867},\"end\":59279,\"start\":58944},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":1424102},\"end\":59514,\"start\":59281},{\"attributes\":{\"doi\":\"arXiv:2101.11868\",\"id\":\"b30\"},\"end\":59899,\"start\":59516},{\"attributes\":{\"doi\":\"arXiv:1307.0471\",\"id\":\"b31\"},\"end\":60215,\"start\":59901},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":34268587},\"end\":60565,\"start\":60217},{\"attributes\":{\"doi\":\"arXiv:1905.11686\",\"id\":\"b33\",\"matched_paper_id\":213350106},\"end\":60871,\"start\":60567},{\"attributes\":{\"doi\":\"arXiv:math/0702226\",\"id\":\"b34\",\"matched_paper_id\":1903919},\"end\":61170,\"start\":60873},{\"attributes\":{\"doi\":\"arXiv:1811.00414\",\"id\":\"b35\"},\"end\":61431,\"start\":61172},{\"attributes\":{\"doi\":\"arXiv:1807.04271\",\"id\":\"b36\",\"matched_paper_id\":44036160},\"end\":61895,\"start\":61433}]", "bib_title": "[{\"end\":49122,\"start\":49098},{\"end\":49771,\"start\":49665},{\"end\":50679,\"start\":50576},{\"end\":51331,\"start\":51260},{\"end\":52305,\"start\":52204},{\"end\":52685,\"start\":52594},{\"end\":53616,\"start\":53511},{\"end\":54418,\"start\":54313},{\"end\":54761,\"start\":54714},{\"end\":55058,\"start\":55009},{\"end\":55375,\"start\":55304},{\"end\":55963,\"start\":55907},{\"end\":56240,\"start\":56178},{\"end\":56609,\"start\":56579},{\"end\":56976,\"start\":56928},{\"end\":57278,\"start\":57201},{\"end\":57840,\"start\":57804},{\"end\":58178,\"start\":58069},{\"end\":58760,\"start\":58717},{\"end\":59010,\"start\":58944},{\"end\":59357,\"start\":59281},{\"end\":59959,\"start\":59901},{\"end\":60295,\"start\":60217},{\"end\":60645,\"start\":60567},{\"end\":60933,\"start\":60873},{\"end\":61498,\"start\":61433}]", "bib_author": "[{\"end\":48874,\"start\":48852},{\"end\":48889,\"start\":48874},{\"end\":48915,\"start\":48889},{\"end\":48922,\"start\":48915},{\"end\":49140,\"start\":49124},{\"end\":49154,\"start\":49140},{\"end\":49171,\"start\":49154},{\"end\":49191,\"start\":49171},{\"end\":49205,\"start\":49191},{\"end\":49217,\"start\":49205},{\"end\":49463,\"start\":49450},{\"end\":49486,\"start\":49463},{\"end\":49796,\"start\":49773},{\"end\":49811,\"start\":49796},{\"end\":49827,\"start\":49811},{\"end\":50242,\"start\":50225},{\"end\":50253,\"start\":50242},{\"end\":50268,\"start\":50253},{\"end\":50284,\"start\":50268},{\"end\":50294,\"start\":50284},{\"end\":50695,\"start\":50681},{\"end\":50710,\"start\":50695},{\"end\":50723,\"start\":50710},{\"end\":50738,\"start\":50723},{\"end\":50749,\"start\":50738},{\"end\":50763,\"start\":50749},{\"end\":51347,\"start\":51333},{\"end\":51360,\"start\":51347},{\"end\":51375,\"start\":51360},{\"end\":51389,\"start\":51375},{\"end\":51973,\"start\":51959},{\"end\":51988,\"start\":51973},{\"end\":52002,\"start\":51988},{\"end\":52317,\"start\":52307},{\"end\":52331,\"start\":52317},{\"end\":52350,\"start\":52331},{\"end\":52357,\"start\":52350},{\"end\":52703,\"start\":52687},{\"end\":52716,\"start\":52703},{\"end\":52735,\"start\":52716},{\"end\":53068,\"start\":53053},{\"end\":53080,\"start\":53068},{\"end\":53091,\"start\":53080},{\"end\":53345,\"start\":53330},{\"end\":53356,\"start\":53345},{\"end\":53367,\"start\":53356},{\"end\":53633,\"start\":53618},{\"end\":53642,\"start\":53633},{\"end\":53657,\"start\":53642},{\"end\":53671,\"start\":53657},{\"end\":54106,\"start\":54092},{\"end\":54126,\"start\":54106},{\"end\":54436,\"start\":54420},{\"end\":54451,\"start\":54436},{\"end\":54467,\"start\":54451},{\"end\":54773,\"start\":54763},{\"end\":54786,\"start\":54773},{\"end\":54797,\"start\":54786},{\"end\":55068,\"start\":55060},{\"end\":55085,\"start\":55068},{\"end\":55100,\"start\":55085},{\"end\":55107,\"start\":55100},{\"end\":55394,\"start\":55377},{\"end\":55413,\"start\":55394},{\"end\":55423,\"start\":55413},{\"end\":55430,\"start\":55423},{\"end\":55978,\"start\":55965},{\"end\":56262,\"start\":56242},{\"end\":56277,\"start\":56262},{\"end\":56296,\"start\":56277},{\"end\":56312,\"start\":56296},{\"end\":56631,\"start\":56611},{\"end\":56647,\"start\":56631},{\"end\":56998,\"start\":56978},{\"end\":57014,\"start\":56998},{\"end\":57298,\"start\":57280},{\"end\":57314,\"start\":57298},{\"end\":57642,\"start\":57633},{\"end\":57651,\"start\":57642},{\"end\":57854,\"start\":57842},{\"end\":57870,\"start\":57854},{\"end\":57890,\"start\":57870},{\"end\":58196,\"start\":58180},{\"end\":58213,\"start\":58196},{\"end\":58528,\"start\":58511},{\"end\":58538,\"start\":58528},{\"end\":58549,\"start\":58538},{\"end\":58565,\"start\":58549},{\"end\":58574,\"start\":58565},{\"end\":58775,\"start\":58762},{\"end\":59031,\"start\":59012},{\"end\":59049,\"start\":59031},{\"end\":59063,\"start\":59049},{\"end\":59082,\"start\":59063},{\"end\":59372,\"start\":59359},{\"end\":59656,\"start\":59640},{\"end\":59671,\"start\":59656},{\"end\":59981,\"start\":59961},{\"end\":59997,\"start\":59981},{\"end\":60009,\"start\":59997},{\"end\":60314,\"start\":60297},{\"end\":60328,\"start\":60314},{\"end\":60663,\"start\":60647},{\"end\":60674,\"start\":60663},{\"end\":60952,\"start\":60935},{\"end\":60969,\"start\":60952},{\"end\":61281,\"start\":61270},{\"end\":61511,\"start\":61500}]", "bib_venue": "[{\"end\":48850,\"start\":48812},{\"end\":49223,\"start\":49217},{\"end\":49448,\"start\":49389},{\"end\":49925,\"start\":49843},{\"end\":50378,\"start\":50310},{\"end\":50865,\"start\":50779},{\"end\":51493,\"start\":51405},{\"end\":51957,\"start\":51874},{\"end\":52382,\"start\":52357},{\"end\":52760,\"start\":52735},{\"end\":53051,\"start\":52959},{\"end\":53328,\"start\":53268},{\"end\":53745,\"start\":53671},{\"end\":54090,\"start\":54071},{\"end\":54497,\"start\":54467},{\"end\":54845,\"start\":54797},{\"end\":55135,\"start\":55122},{\"end\":55534,\"start\":55446},{\"end\":56029,\"start\":55978},{\"end\":56361,\"start\":56312},{\"end\":56733,\"start\":56663},{\"end\":57051,\"start\":57014},{\"end\":57348,\"start\":57314},{\"end\":57631,\"start\":57527},{\"end\":57919,\"start\":57905},{\"end\":58256,\"start\":58213},{\"end\":58509,\"start\":58475},{\"end\":58814,\"start\":58791},{\"end\":59094,\"start\":59082},{\"end\":59384,\"start\":59372},{\"end\":59638,\"start\":59516},{\"end\":60037,\"start\":60024},{\"end\":60376,\"start\":60328},{\"end\":60700,\"start\":60690},{\"end\":61006,\"start\":60987},{\"end\":61268,\"start\":61172},{\"end\":61613,\"start\":61527},{\"end\":50938,\"start\":50867},{\"end\":53806,\"start\":53747},{\"end\":61686,\"start\":61615}]"}}}, "year": 2023, "month": 12, "day": 17}