{"id": 250042546, "updated": "2022-10-06 13:49:27.032", "metadata": {"title": "End-to-End Multi-Person Pose Estimation with Transformers", "authors": "[{\"first\":\"Dahu\",\"last\":\"Shi\",\"middle\":[]},{\"first\":\"Xing\",\"last\":\"Wei\",\"middle\":[]},{\"first\":\"Liangqi\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Ye\",\"last\":\"Ren\",\"middle\":[]},{\"first\":\"Wenming\",\"last\":\"Tan\",\"middle\":[]}]", "venue": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "journal": "2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)", "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Current methods of multi-person pose estimation typically treat the localization and association of body joints separately. In this paper, we propose the first fully end-to-end multi-person Pose Estimation framework with TRansformers, termed PETR. Our method views pose estimation as a hierarchical set prediction problem and effectively removes the need for many hand-crafted modules like RoI cropping, NMS and grouping post-processing. In PETR, multiple pose queries are learned to directly reason a set of full-body poses. Then a joint decoder is utilized to further refine the poses by exploring the kinematic relations between body joints. With the attention mechanism, the proposed method is able to adaptively attend to the features most relevant to target keypoints, which largely overcomes the feature misalignment difficulty in pose estimation and improves the performance considerably. Extensive experiments on the MS COCO and CrowdPose benchmarks show that PETR plays favorably against state-of-the-art approaches in terms of both accuracy and efficiency. The code and models are available at https://github.com/hikvision-research/opera.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/cvpr/ShiWLRT22", "doi": "10.1109/cvpr52688.2022.01079"}}, "content": {"source": {"pdf_hash": "9d4744f83cd9e613e429f0da0deb7105f116c32b", "pdf_src": "IEEE", "pdf_uri": null, "oa_url_match": false, "oa_info": null}, "grobid": {"id": "57811979aed30bf4ea490ed202c7e854aabdb2d3", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/9d4744f83cd9e613e429f0da0deb7105f116c32b.txt", "contents": "\nEnd-to-End Multi-Person Pose Estimation with Transformers\n\n\nDahu Shi \nHikvision Research Institute\nHangzhouChina\n\nXing Wei weixing@mail.xjtu.edu.cn \nSchool of Software Engineering\nXi'an Jiaotong University\n\n\nLiangqi Li \nHikvision Research Institute\nHangzhouChina\n\nYe Ren \nHikvision Research Institute\nHangzhouChina\n\nWenming Tan tanwenming@hikvision.com \nHikvision Research Institute\nHangzhouChina\n\nEnd-to-End Multi-Person Pose Estimation with Transformers\n10.1109/CVPR52688.2022.01079\nCurrent methods of multi-person pose estimation typically treat the localization and association of body joints separately. In this paper, we propose the first fully end-toend multi-person Pose Estimation framework with TRansformers, termed PETR. Our method views pose estimation as a hierarchical set prediction problem and effectively removes the need for many hand-crafted modules like RoI cropping, NMS and grouping post-processing. In PETR, multiple pose queries are learned to directly reason a set of full-body poses. Then a joint decoder is utilized to further refine the poses by exploring the kinematic relations between body joints. With the attention mechanism, the proposed method is able to adaptively attend to the features most relevant to target keypoints, which largely overcomes the feature misalignment difficulty in pose estimation and improves the performance considerably. Extensive experiments on the MS COCO and CrowdPose benchmarks show that PETR plays favorably against state-of-the-art approaches in terms of both accuracy and efficiency. The code and models are available at https://github.com/ hikvision-research/opera.\n\nIntroduction\n\nMulti-person pose estimation (aka, keypoint detection) aims to detect all the instances and identify the kinematic joints of each person simultaneously. It is one of the fundamental computer vision tasks and has a wide range of applications such as action recognition [9], humancomputer interaction [15], pedestrian tracking [1,31] and re-identification [22], etc.\n\nExisting mainstream methods solve this challenging task with two-stage frameworks, including top-down and bottom-up approaches. Top-down methods [5,10,12,33,39], as illustrated in Figure 1a, first detect each individual by a person detector and then transfer the task to a sim-* Co-first authors. \u2020 Corresponding author.    pler single-person pose estimation problem. The top-down pipeline comes with the following drawbacks: 1) the pose estimation accuracy heavily relies on the performance of person detector, incurring inferior performance in complex scenarios [7]; 2) the computational cost is expensive due to the use of the isolated detector [26,32] and the running time depends on the number of instances in the image. On the other hand, bottom-up methods [3,17,27,30] (shown in Figure 1b) first detect all potential keypoints in the image in an instance-agnostic manner, and then perform a grouping post-processing to get instance-aware full-body poses. The grouping process is usually heuristic, hand-crafted and Framework RoI-free Grouping-free NMS-free   Two-stage  Top-down  Bottom-up Single-stage Non end-to-end Fully end-to-end Table 1. Comparison of pose estimation frameworks.\n\ncumbersome [27], involving several hyper-parameters and tricks. These kinds of methods split the pose estimation problem into two steps and are often not optimized in a fully end-to-end fashion.\n\nRecently, there has been a great interest to directly estimate multi-person poses from the input image in a single stage [26,29,32,34,36,41]. SPM [29] propose a structured pose representation that unifies person instance and body joint position representations and simplifies the multi-person pose estimation pipeline. FCPose [26] and In-sPose [32] propose a fully convolutional multi-person pose estimation framework using dynamic instance-aware convolutions, which is compact and efficient. These methods eliminate the need for RoI (Region of Interest) cropping and keypoint grouping post-processing and achieve a good trade-off between accuracy and efficiency. However, they still rely on \"taking-peak\" on the heatmap [29,41] or score map [26,32] and hand-crafted NMS (Non-Maximum Suppression) post-processing [26,32,36], which are still not end-to-end optimized.\n\nInspired by the paradigm emerged in object detection [4,42], we present a fully end-to-end multi-person pose estimation framework (Sec. 3.1) with transformers, termed PETR. The proposed method unifies person instance and fine-grained body joint localization by formulating pose estimation as a hierarchical set prediction problem. Given multiple randomly initialized pose queries, a pose decoder (Sec. 3.3) learns to reason about the relations of objects [14] and estimate a set of instance-aware poses under the global image context. Then, a joint decoder (Sec. 3.4) is designed to explore the structured relations between different joints and further optimize the full-body poses at a finer level. Compared with existing single-stage methods, PETR could hierarchically attend to the features most relevant to target keypoints, largely overcomes the feature misalignment issue [11,34] and improves the performance considerably. Our end-to-end query-based framework is learned via the bipartite matching strategy that avoids the heuristic label assignment and eliminates the need for NMS post-processing.\n\nWe illustrate and compare the mainstream pose estimation frameworks in Figure 1 and Table 1. The main contributions of this work are summarized as follows.\n\n\u2022 We propose the first fully end-to-end learning framework for multi-person pose estimation. The proposed PETR method directly predicts instance-aware fullbody poses and eliminates the need for RoI cropping, grouping, and NMS post-processings.\n\n\u2022 We design hierarchical decoders to deal with the feature misalignment issue, and capture both relations between person instances and kinematic joints by the attention mechanism.\n\n\u2022 PETR surpasses all single-stage and bottom-up methods and is comparable to top-down methods on COCO dataset. Besides, PETR performs well in crowded scenes and establishes a new state of the art on Crowd-Pose dataset.\n\n\nRelated Work\n\n\nMulti-Person Pose Estimation\n\nThe existing multi-person pose estimation approaches can be summarized into three categories: top-down methods, bottom-up methods and recent single-stage methods.\n\nTop-down methods. The top-down methods first employ an object detector to obtain the bounding box of each person instance in an image. Then the instance is cropped from the bounding box for single-person pose estimation. Representative works include Hourglass [28], RMPE [10], CPN [5], SimpleBaseline [39], HRNet [33] and so on. In general, top-down methods have a slow inference speed. They break the multi-person pose estimation task into two steps: person detection and single-person pose estimation. Instead of cropping RoIs from the original image, Mask R-CNN [12] utilizes RoIAlign operation to extract features of RoIs from the feature maps of the detector, significantly speeding up the inference. Moreover, top-down methods are highly dependent on the performance of the detector.\n\nBottom-up methods. The bottom-up methods detect all keypoints in an instance-agnostic fashion, and then group them into individuals. Most existing bottom-up methods mainly focus on how to associate the detected keypoints that belong to the same person. OpenPose [3] utilizes part affinity fields to establish connections between keypoints of the same instance. Associative embedding [27] produces a detection heatmap and a tagging map for each body joint, and then groups keypoints with similar tags into an individual. PersonLab [30] groups keypoints by directly learning a 2D offset field for each pair of keypoints. PifPaf [17] learns a Part Assiciation Field (PAF) to connect the keypoints into full-body poses. Compared to top-down methods, bottom-up methods are usually more efficient because of their simpler pipeline of sharing convolutional computation. However, the grouping post-process is heuristic and involves many tricks which often makes its performance inferior to top-down methods.  Single-stage methods. To avoid the aforementioned limitations in both top-down and bottom-up methods, the single-stage methods [26,29,32,34,36,41] are proposed to densely regress a set of pose candidates over spatial locations, where each candidate consists of the keypoint positions that are from the same person. SPM [29] proposes a structured pose representation to unify position information of person instances and body joints. Due to the weak regression results, CenterNet [41] proposes to match the regressed keypoint positions to the closest keypoints detected from the keypoint heatmaps. Point-set anchors [36] adopt deformable-like convolutions to refine the predefined pose anchors, mitigating the difficulties of feature misalignment. FCPose [26] and InsPose [32] utilize dynamic instance-aware convolutions to solve the multi-person pose estimation problem, achieving better accuracy/efficiency trade-off than other single-stage methods. Although these approaches obtain competitive performance, they are not fully end-to-end optimized and still need heuristic postprocessing like NMS or keypoint location correction [41].\n\n\nTransformer in Vision\n\nTransformer [35] has been widely applied in natural language processing. Recently, many works attempted to involve transformer architecture in computer vision tasks and showed promising performances [4,6,8,37,42]. ViT [8] apply the transformer to encode a sequence of image patches for image classification. DETR [4] and Deformable DETR [42] adopt transformer architecture together with bipartite matching to perform object detection in an end-to-end fash-ion. MaskFormer [6] and SOIT [40] employ transformer decoders to predict a set of binary masks directly, and effectively remove the need for many hand-crafted components. SAANet [37] proposes a scene-adaptive transformer network for crowd counting, achieving highest accuracy on several benchmarks. PRTR [20] and TFPose [25] formulate the pose estimation task as a regression problem by transformers. However, they still follow the top-down framework and need the hand-crafted RoI cropping operation. In this paper, we use transformer to build a fully end-to-end framework for multi-person pose estimation.\n\n\nMethodology\n\n\nOverall Architecture\n\nAs depicted in Figure 2, the proposed framework consists of three key modules: visual feature encoder, pose decoder and joint decoder, where (1) the visual feature encoder is applied to refine the multi-scale feature maps extracted from the backbone network, (2) the pose decoder is employed to predict multiple full-body poses, and (3) the joint decoder is designed to further refine the full-body poses at a joint level.\n\nGiven an image I \u2208 R H\u00d7W \u00d73 , we extract multi-scale feature maps C 3 , C 4 and C 5 from the last three stages of the backbone (e.g., ResNet [13]), whose strides are 8, 16 and 32, respectively. The multi-scale feature maps are projected to the ones with 256 channels by a spatial-wise fullyconnected (FC) layer and then flattened into feature tokens C 3 , C 4 and C 5 . Specifically, the shape of C i is L i \u00d7 256,  where\nL i = H 2 i \u00d7 W 2 i .\nNext, using the concatenated feature tokens [C 3 , C 4 , C 5 ] as input, the visual feature encoder outputs the refined multi-scale feature tokens F \u2208 R L\u00d7256 , where L = L 3 + L 4 + L 5 is the total number of feature tokens. After that, N randomly initialized pose queries are utilized to directly reason N full-body poses (and their corresponding confidence score) under the global image context. Finally, we scatter each full-body pose into a sequence of body joints and adopt a joint decoder to further refine them.\n\n\nVisual Feature Encoder\n\nHigh-resolution and multi-scale feature maps are important for the pose estimation task [7,33]. Since the multihead self-attention module [4,8] has quadratic computation complexity to input size, we employ the deformable attention module [42] to implement our feature encoder.\n\nDue to the low computational complexity of the deformable attention layer, our encoder can merge and refine the multi-scale feature maps. Concretely, each encoder layer comprises a multi-scale deformable attention module and a feed-forward network (FFN). In order to identify which feature level each feature token lies in, we add a scale-level embedding, in addition to the positional embedding. There are six deformable encoder layers stacked in sequence in our visual feature encoder. After that, we can obtain the refined multi-scale visual feature memory F .\n\n\nPose Decoder\n\nIn the pose decoder, we aim to reason a set of full-body poses under the global image context (i.e., feature memory F ). Similar to the visual feature encoder, we use the deformable attention module to build our pose decoder due to its efficiency. Specifically, given N randomly initialized pose queries Q pose \u2208 R N \u00d7D , the pose decoder out-\nputs N full-body poses {P i } N i=1 \u2208 R N \u00d72K , where P i = {(x j i , y j i )} K j=1\ndenotes the coordinates of K joints for the i th person and D indicates the dimension of the query embedding.\n\nThe detailed structure of the pose decoder is illustrated in Figure 3. First, the query embeddings are fed into the selfattention module for interacting with each other (i.e., poseto-pose attention). Then each query extracts features from the multi-scale feature memory F via the deformable crossattention module (i.e., feature-to-pose attention). There are K reference points, serving as the initial locations of a full-body pose in our deformable cross-attention module, in contrast to [42]. Subsequently, the instance-aware query features are fed into the multi-task prediction heads. The classification head predicts the confidence score for each object by a linear projection layer (FC). The pose regression head predicts the relative offsets w.r.t. the K reference points using a multi-layer perceptron (MLP) with a hidden size of 256. There are three decoder layers applied sequentially in our pose decoder.\n\nInstead of only using the final decoder layer to predict the pose coordinates, inspired by [42], we leverage all the decoder layers to estimate the pose coordinates progressively. Specifically, each layer refines the poses based on the predictions from the previous layer. Formally, given a normalized pose P d\u22121 predicted by the (d\u22121) th decoder layer, the d th decoder layer refines the pose as\nP d = \u03c3(\u03c3 \u22121 (P d\u22121 ) + \u0394P d ),(1)\nwhere \u0394P d are predicted offsets at the d th layer, \u03c3 and \u03c3 \u22121 denote the sigmoid and inverse sigmoid function, respectively. In this way, P d\u22121 serves as the new reference point of cross-attention module in the d th decoder layer. The initial reference point P 0 is a randomly-initialized matrix and jointly updated with the model parameters during training. As a result, the progressive deformable crossattention module can attend to the visual features most relevant to the target keypoints, which overcoming the feature misalignment issue naturally.\n\n\nJoint Decoder\n\nAs shown in Figure 4, the joint decoder is proposed to explore the structured relations between articulated joints and further refine full-body poses at a joint level. We employ deformable attention module to build our joint decoder   as in the pose decoder. Concretely, given K randomly initialized joint queries Q joint \u2208 R K\u00d7D , the joint decoder takes the joint locations of each full-body pose predicted by preceding pose decoder as their initial reference points and then further refine the joint locations. Note that all the poses can be processed in parallel since they are independent of each other in the joint decoder. The detailed structure of the joint decoder is illustrated in Figure 4. The joint queries firstly interact with each other via a self-attention module (i.e., joint-to-joint attention), and then extract visual features in a deformable crossattention module (i.e., feature-to-joint attention). Subsequently, a joint regression head predicts the 2-D joint displacement \u0394J = (\u0394x, \u0394y) by applying an MLP. Similar to the pose decoder, the joint coordinates are progressively refined. Formally, let J d\u22121 be the normalized joint coordinates predicted by the (d \u2212 1) th decoder layer, the predictions of the d th decoder layer are\nJ d = \u03c3(\u03c3 \u22121 (J d\u22121 ) + \u0394J d ),\nwhere J 0 is joint locations of the pose predicted by the proceeding pose decoder.\n\n\nLoss Functions\n\nFollowing [4], we use a set-based Hungarian loss that forces a unique prediction for each ground-truth pose. The same classification loss function (denoted as L cls ) as in [42] is used for classification head in our pose decoder. Besides, we adopt both L 1 loss (denoted as L reg ) and OKS loss (denoted as L oks ) for pose regression head and joint regression head in our pose decoder and joint decoder, respectively. OKS loss. The most commonly-used L 1 loss have different scales for small and large poses even if their relative errors are similar. To mitigate this issue, we propose to use the Object Keypoint Similarity (OKS) loss additionally, which can be formulated as,\nL oks (P, P * ) = K i exp(\u2212 P i \u2212 P * i /2s 2 k 2 i )\u03b4(v i > 0) K i \u03b4(v i > 0) ,(2)\nwhere P i \u2212 P * i is the Euclidian distance between the i th predicted keypoint and ground-truth one, v i is the visibility flag of the ground truth, s is the object scale, and k i is a perkeypoint constant that controls falloff. As shown above, the OKS Loss is normalized by the scale of the person instance with the importance of keypoints equalized.\n\nHeatmap loss. Similar to [26,32], we use the auxiliary heatmap regression training for fast convergence. We gather the feature tokens from C 3 outputs of visual feature encoder and reshape the tokens into the original spatial shape. The result is denoted by F C3 \u2208 R (H/8)\u00d7(W/8)\u00d7D . We apply a deformable transformer encoder to generate the heatmap prediction. Then, we compute a variant of focal loss [18] between the predicted and ground-truth heatmaps (denoted as L hm ). Note that the heatmap branch is only used for aided training and is discarded in inference.\n\nOverall loss. Formally, the overall loss function of our model can be formulated as:\nL = L cls + \u03bb 1 L reg + \u03bb 2 L oks + \u03bb 3 L hm(3)\nwhere \u03bb 1 , \u03bb 2 and \u03bb 3 are the loss weights, respectively.\n\n\nExperiments\n\n\nCOCO Keypoint Detection\n\nWe evaluate the performance on the COCO dataset [21], which contains over 200K images and 250K person instances labeled with 17 keypoints. All the models are trained on the train2017 set (57K images). We use the val2017 set (5K images) as validation for our ablation experiments and compare with other state-of-the-art methods on the test-dev set (20K images).\n\nEvaluation metrics. The standard evaluation metric is based on Object keypoint Similarity (OKS). We report standard average precision and recall scores 1 : AP 50 (AP at OKS = 0.50), AP 75 , AP (mean of AP scores from OKS = 0.50 to OKS = 0.95 with the increment as 0.05), AP M for persons of medium sizes and AP L for persons of large sizes.\n\nTraining details. Following the setting of [26,32], we augment the input image by random crop, random flip, and random resize (the shorter sides in [480,800] [38], which are better than the original results reported in the Mask R-CNN paper [12]. We measure the inference time of other methods on the same hardware if possible and all the times are counted with single-scale test. Note that some top-down methods need extra inference time of person detector which is not contained in this table.\n\nsides less or equal to 1333). The models are trained with Adam optimizer [16] with base learning rate of 2 \u00d7 10 \u22124 , momentum of 0.9 and weight decay of 1 \u00d7 10 \u22124 . Specifically, we train the model for 50 epochs with a total batch size of 32 and the initial learning rate is decayed at 40 th epoch by a factor of 0.1 in ablation experiments. For the main results on test-dev set, the model is trained for 100 epochs and the initial learning rate is decayed at 80 th epoch by a factor of 0.1.\n\nTesting details. The input images are resized to have their shorter sides being 800 and their longer sides less or equal to 1333. For the multi-scale test, we resize the original images with their short sides being 800, 1000, and 1200 respectively. All reported numbers have been obtained with single model without model ensemble. The inference time is measured using a single NVIDIA Tesla V100 GPU.\n\n\nResults on COCO test-dev\n\nWe firstly make comparisons with the state-of-the-art methods, as shown in Table 2. When using the same backbone network as the feature extractor, our PETR outperforms all existing bottom-up methods as well as the singlestage methods with or without multi-scale test. Without any bells and whistles, the proposed method achieves 67.6 and 68.5 AP scores, with ResNet-50 and ResNet-101 as the backbone, respectively. Our best model with Swin-L [23] achieves 71.2 AP score on COCO test-dev2017.\n\nComparison with single-stage methods. Our method significantly outperforms existing single-stage methods, such as DirectPose [34], CenterNet [41], Point-Set Anchors [36] and InsPose [32]. The performance of our method is 2.2 points higher compared with InsPose [32] with both ResNet-50 and ResNet-101 as the backbone. Our PETR with ResNet-101 even outperforms Points-Set Anchors with HRNet-w48, which has a much larger size than ResNet-101, recording 70.0 vs. 68.7 in AP score. Note that our approach is NMS-free which make it more efficient compared with these single-stage methods.\n\nComparison with two-stage methods. With a more compact pipeline, we even outperforms the state-of-theart bottom-up methods, such as CMU-Pose [2], AE [27], PifPaf [17], HigherHRNet [7], DEKR [11] and SWAHR [24]. With single-scale test, PETR achieves significantly improvement over HigherHRNet [7], 68.5 vs. 64.7 in AP score, in which our PETR using a smaller backbone ResNet-101 than HRNet-w32 used in HigherHRNet [7]. Our method also outperforms the latest proposed SWAHR [24], 68.5 vs. 67.9, with a smaller backbone. Moreover, PETR outperforms previous strong baseline Mask R-CNN [12] with backbone ResNet-101 (68.5 vs. 64.3 in AP), while maintain a competitive inference speed.\n\nComparison of inference time. We measure the inference time of our models with different backbones and other methods on the same hardware if possible. As shown in Table 2, PETR with ResNet-50 could achieve competitive inference speed to the typical top-down method, Mask R-CNN [12], and the single-stage method InsPose [32], i.e. 89ms vs. 89ms vs. 80ms. We also show the speed-accuracy trad-off between our PETR and state-of-the-art methods in Figure 6, and PETR surpasses all those bottom-up methods in both speed and accuracy field. Although it seems a little slower than some of the other methods (FCPose [26]), we should note that current computational devices like GPU are not specifically optimized for the transformer-based architecture. \n\n\nAblation Study\n\nWe perform a number of ablation experiments to analyze effectiveness of the proposed pose/joint decoders and OKS loss on the COCO val2017 dataset.\n\nPose and joint decoders. PETR use hierarchical decoders (i.e., the pose decoder and joint decoder) to regress keypoint locations progressively. The pose decoder alone already estimates full-body poses, which could be refined by the joint decoder further. As shown in Table 3, the joint decoder improves the AP by 1.0 points. Note that the improvement of AP 75 is more significant (1.3 points), indicating a finer prediction offered by the joint decoder. Moreover, we conduct another experiment where both the pose  Table 3. Ablation experiments: ablation of the proposed pose decoder and joint decoder on COCO val2017. The first row means that the refined multi-scale feature tokens are directly utilized to regress full-body poses, which incurs severe feature misalignment as mentioned in [34]. OKS   decoder and joint decoder are disabled. In this case, we only utilize the multi-scale feature tokens refined by the visual feature encoder to regress full-body poses directly.\n\nThe performance (1st row in Table 3) drops remarkably due to the misalignment between features and target joints, as mentioned in [11,34]. OKS loss and OKS matching cost. Following DETR [4], we use a bipartite matching mechanism to indicate the relationship between the training samples and ground truths, and then compute several types of loss to supervise the model. OKS is a commonly-used evaluation metric in pose estimation benchmarks. However, most methods use L 1 loss for training, therefore leave a gap between optimizing the loss and maximizing the OKS metric. To our knowledge, this is the first work to adopt OKS as the loss function in the pose estimation field. We conduct experiments to study the impact of OKS loss and its matching cost, respectively. As shown in Table 4, the OKS loss brings 1.4 AP score improvement and using OKS for matching cost gains 2.7 AP score. When combining both two components, the performance is significantly improved from 64.2 to 67.4.\n\n\nCrowdPose\n\nWe further evaluate our approach on the CrowdPose [19] dataset that is more challenging and includes many crowded scenes. It consists of 20K images, containing about 80,000 persons. Each person is labeled with 14 body joints. The train, val and test datasets contain about 10K, 2K and 8K images, respectively. We train our models on the train and val sets and report the results on the test set as done in [7].\n\nEvaluation metrics. The standard average precision based on OKS which is the same as COCO is adopted as the evaluation metrics. The CrowdPose dataset is split into three crowding levels: easy, medium and hard. We report the following metrics: AP, AP 50 , AP 75 , as well as AP E , AP M and AP H for easy, mudium and hard images.\n\nTest set results. The results of our approach and other state-of-the-art methods on the test set are shown in Table 5. Different from the top-down methods which have lost their superiority in crowded scenes, our approach shows its robustness and achieves 72.0 AP score, which surpasses the latest bottom-up method SWAHR [24], especially on AP H item. Our PETR does not depend on detection results like top-down methods, and does not need NMS to suppress redundant results like bottom-up and other single-stage methods, which makes it more flexible and suitable to estimate human pose under the crowded scenes.\n\n\nConclusion\n\nThis paper presents the first fully end-to-end multiperson pose estimation framework, termed PETR. It reformulates multi-person pose estimation as a hierarchical set prediction problem, which effectively removes the need for many hand-crafted components like RoI cropping, grouping, and NMS post-processings. PETR is simple and direct, offering a better trade-off between accuracy and efficiency than other methods.\n\n\nend-to-end framework.\n\nFigure 1 .\n1Comparison of mainstream pose estimation frameworks. SPPE in (a) indicates single-person pose estimation. We proposed a fully end-to-end framework as show in (c).\n\nFigure 2 .\n2The overall architecture of PETR. C3 to C5 are multi-scale feature maps extracted from the backbone network (e.g., ResNet-50). The visual feature encoder takes the flattened image features as inputs and refines them. Given N pose queries and the refined multi-scale feature tokens, pose decoder predicts N full-body poses in parallel. After that, an additional joint decoder takes each scattered pose (i.e., kinematic joints of each pose) as its reference points and outputs the refined pose as final results. K is the number of keypoints for each instance (e.g., K = 17 in COCO[21] dataset).\n\nFigure 3 .\n3Detailed structure of the pose decoder. Given N pose queries, the pose decoder outputs N instance-aware full-body poses. The progressive deformable cross-attention module can attend to the visual features most relevant to the target keypoints.\n\nFigure 4 .\n4Detailed structure of the joint decoder. Each one of the K joint queries takes a keypoint location of the full-body pose predicted by the pose decoder as its reference point for further refinement.\n\nFigure 5 .\n5Visualization results of PETR. The first row and the second row show the visualization results on COCO val2017 and CrowdPose test set, respectively. PETR performs well on a wide range of poses, containing viewpoint change, occlusion, motion blur and crowded scene. Best viewed in color.\n\nFigure 6 .\n6The speed-accuracy trade-off comparison. PETR-R50-600 indicates a variant of PETR with ResNet-50 backbone where the short side of the input image is 600 pixels.\n\n\nand the longerMethod \n\nBackbone \nAP \nAP 50 \nAP 75 \nAP M \nAP L \nTime [ms] \n\nTwo-stage methods \n\nTop-down \n\nMask R-CNN [12] \nResNet-50 \n62.7 \n87.0 \n68.4 \n57.4 \n71.1 \n89 \nMask R-CNN  *  \nResNet-50 \n63.9 \n87.7 \n69.9 \n59.7 \n71.5 \n89 \nMask R-CNN  *  \nResNet-101 \n64.3 \n88.2 \n70.6 \n60.1 \n71.9 \n108 \nCPN [5] \nResNet-Inception \n72.1 \n91.4 \n80.0 \n68.7 \n77.2 \n>472 \nSimpleBaseline  \u2020 [39] \nResNet-152 \n73.7 \n91.9 \n81.1 \n70.3 \n80.0 \n>784 \nPRTR [20] \nHRNet-w32 \n72.1 \n90.4 \n79.6 \n68.1 \n79.0 \n-\nHRNet  \u2020 [33] \nHRNet-w32 \n74.9 \n92.5 \n82.8 \n71.3 \n80.9 \n>632 \nHRNet  \u2020 [33] \nHRNet-w48 \n75.5 \n92.5 \n83.3 \n71.9 \n81.5 \n>857 \n\nBottom-up \n\nCMU-Pose  \u2021 [3] \n3CM-3PAF \n61.8 \n84.9 \n67.5 \n57.1 \n68.2 \n-\nCMU-Pose [2] \nVGG-19 \n64.2 \n86.2 \n70.1 \n61.0 \n68.8 \n74 \nAE  \u2020 [27] \nHourglass-4 stacked \n62.8 \n84.6 \n69.2 \n57.5 \n70.6 \n139 \nPifPaf [17] \nResNet-152 \n66.7 \n-\n-\n62.4 \n72.9 \n260 \nHrHRNet  \u2020 [7] \nHRNet-w32 \n66.4 \n87.5 \n72.8 \n61.2 \n74.2 \n400 \nDEKR  \u2020 [11] \nHRNet-w32 \n67.3 \n87.9 \n74.1 \n61.5 \n76.1 \n411 \nSWAHR  \u2020 [24] \nHRNet-w32 \n67.9 \n88.9 \n74.5 \n62.4 \n75.5 \n406 \n\nSingle-stage methods \n\nNon end-to-end \n\nDirectPose [34] \nResNet-50 \n62.2 \n86.4 \n68.2 \n56.7 \n69.8 \n74 \nFCPose [26] \nResNet-50 \n64.3 \n87.3 \n71.0 \n61.6 \n70.5 \n68 \nInsPose [32] \nResNet-50 \n65.4 \n88.9 \n71.7 \n60.2 \n72.7 \n80 \nDirectPose [34] \nResNet-101 \n63.3 \n86.7 \n69.4 \n57.8 \n71.2 \n-\nFCPose [26] \nResNet-101 \n65.6 \n87.9 \n72.6 \n62.1 \n72.3 \n93 \nInsPose [32] \nResNet-101 \n66.3 \n89.2 \n73.0 \n61.2 \n73.9 \n100 \nCenterNet [41] \nHourglass-104 \n63.0 \n86.8 \n69.6 \n58.9 \n70.4 \n160 \nPoint-Set Anchors  \u2020 \u2021 [36] \nHRNet-w48 \n68.7 \n89.9 \n76.3 \n64.8 \n75.3 \n-\n\nFully end-to-end \n\nPETR (Ours) \nResNet-50 \n67.6 \n89.8 \n75.3 \n61.6 \n76.0 \n89 \nPETR  \u2021 (Ours) \nResNet-50 \n69.2 \n90.5 \n77.1 \n64.2 \n76.4 \n-\nPETR (Ours) \nResNet-101 \n68.5 \n90.3 \n76.5 \n62.5 \n77.0 \n95 \nPETR  \u2021 (Ours) \nResNet-101 \n70.0 \n90.9 \n78.2 \n65.3 \n77.1 \n-\nPETR (Ours) \nSwin-L \n70.5 \n91.5 \n78.7 \n65.2 \n78.0 \n133 \nPETR  \u2021 (Ours) \nSwin-L \n71.2 \n91.4 \n79.6 \n66.9 \n78.0 \n-\n\nTable 2. Comparisons with state-of-the-art methods on COCO test-dev dataset.  \u2020 and  \u2021 denote flipping and multi-scale test, \nrespectively. Mask R-CNN  *  are the results from Detectron2 \n\n\nPose decoder Joint decoder AP AP 50 AP 75 AP M AP L AR 49.4 75.7 55.1 46.4 54.3 60.4 66.4 87.2 73.6 60.6 74.8 73.7 67.4 87.0 74.9 61.7 75.9 74.8\n\n\nloss OKS matching AP AP 50 AP 75 AP M AP L AR64.2 86.7 \n70.1 \n58.4 73.5 73.9 \n65.6 87.7 \n72.1 \n60.3 73.9 74.8 \n66.9 86.6 \n74.4 \n60.9 75.8 74.5 \n67.4 87.0 \n74.9 \n61.7 75.9 74.8 \n\n\n\nTable 4 .\n4Ablation experiments: the effect of the OKS loss and its matching cost on COCO val2017.\n\nTable 5 .\n5MethodAP AP 50 AP 75 AP E AP M AP H Comparisons with state-of-the-art methods on Crowd-Pose test dataset. Superscripts E, M, H of AP stand for easy, medium and hard images, respectively. \u2020 denotes flipping test.Top-down methods \n\nMask R-CNN [12] \n57.2 83.5 \n60.3 \n69.4 \n57.9 \n45.8 \nAlphaPose [10] \n61.0 81.3 \n66.0 \n71.2 \n61.4 \n51.1 \nSimpleBaseline [39] 60.8 81.4 \n65.7 \n71.4 \n61.2 \n51.2 \nSPPE [19] \n66.0 84.2 \n71.5 \n75.5 \n66.3 \n57.4 \n\nBottom-up methods \n\nOpenPose [3] \n-\n-\n-\n62.7 \n48.7 \n32.3 \nHrHRNet  \u2020 [7] \n65.9 86.4 \n70.6 \n73.3 \n66.5 \n57.9 \nDEKR  \u2020 [11] \n67.3 86.4 \n72.2 \n74.6 \n68.1 \n58.7 \nSWAHR  \u2020 [24] \n71.6 88.5 \n77.6 \n78.9 \n72.4 \n63.0 \n\nFully end-to-end methods \n\nPETR (Ours) \n71.6 90.4 \n78.3 \n77.3 \n72.0 \n65.8 \nPETR  \u2020 (Ours) \n72.0 90.9 \n78.8 \n78.0 \n72.5 \n65.4 \n\n\nhttp://cocodataset.org/#keypoints-eval\nAcknowledgments\nPosetrack: A benchmark for human pose estimation and tracking. Mykhaylo Andriluka, Umar Iqbal, Eldar Insafutdinov, Leonid Pishchulin, Anton Milan, Juergen Gall, Bernt Schiele, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionMykhaylo Andriluka, Umar Iqbal, Eldar Insafutdinov, Leonid Pishchulin, Anton Milan, Juergen Gall, and Bernt Schiele. Posetrack: A benchmark for human pose estima- tion and tracking. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5167-5176, 2018. 1\n\nOpenpose: realtime multi-person 2d pose estimation using part affinity fields. Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, Yaser Sheikh, IEEE transactions on pattern analysis and machine intelligence. 437Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Openpose: realtime multi-person 2d pose estimation using part affinity fields. IEEE transactions on pattern analysis and machine intelligence, 43(1):172-186, 2019. 6, 7\n\nRealtime multi-person 2d pose estimation using part affinity fields. Zhe Cao, Tomas Simon, Shih-En Wei, Yaser Sheikh, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionZhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Realtime multi-person 2d pose estimation using part affinity fields. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7291-7299, 2017. 1, 2, 6, 8\n\nEnd-toend object detection with transformers. Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, Sergey Zagoruyko, European Conference on Computer Vision. Springer5Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to- end object detection with transformers. In European Confer- ence on Computer Vision, pages 213-229. Springer, 2020. 2, 3, 4, 5, 8\n\nCascaded pyramid network for multi-person pose estimation. Yilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang Zhang, Gang Yu, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognition6Yilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang Zhang, Gang Yu, and Jian Sun. Cascaded pyramid net- work for multi-person pose estimation. In Proceedings of the IEEE conference on computer vision and pattern recog- nition, pages 7103-7112, 2018. 1, 2, 6\n\nPer-pixel classification is not all you need for semantic segmentation. Bowen Cheng, Alexander G Schwing, Alexander Kirillov, arXiv:2107.06278arXiv preprintBowen Cheng, Alexander G Schwing, and Alexander Kir- illov. Per-pixel classification is not all you need for semantic segmentation. arXiv preprint arXiv:2107.06278, 2021. 3\n\nHigherhrnet: Scale-aware representation learning for bottom-up human pose estimation. Bowen Cheng, Bin Xiao, Jingdong Wang, Honghui Shi, S Thomas, Lei Huang, Zhang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition7Bowen Cheng, Bin Xiao, Jingdong Wang, Honghui Shi, Thomas S Huang, and Lei Zhang. Higherhrnet: Scale-aware representation learning for bottom-up human pose estima- tion. In Proceedings of the IEEE/CVF Conference on Com- puter Vision and Pattern Recognition, pages 5386-5395, 2020. 1, 4, 6, 7, 8\n\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, arXiv:2010.11929Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. 34arXiv preprintAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, et al. An image is worth 16x16 words: Trans- formers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 3, 4\n\nHierarchical recurrent neural network for skeleton based action recognition. Yong Du, Wei Wang, Liang Wang, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionYong Du, Wei Wang, and Liang Wang. Hierarchical recur- rent neural network for skeleton based action recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1110-1118, 2015. 1\n\nRmpe: Regional multi-person pose estimation. Shuqin Hao-Shu Fang, Yu-Wing Xie, Cewu Tai, Lu, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer visionHao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu. Rmpe: Regional multi-person pose estimation. In Proceed- ings of the IEEE international conference on computer vi- sion, pages 2334-2343, 2017. 1, 2, 8\n\nBottom-up human pose estimation via disentangled keypoint regression. Zigang Geng, Ke Sun, Bin Xiao, Zhaoxiang Zhang, Jingdong Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition7Zigang Geng, Ke Sun, Bin Xiao, Zhaoxiang Zhang, and Jing- dong Wang. Bottom-up human pose estimation via disentan- gled keypoint regression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14676-14686, 2021. 2, 6, 7, 8\n\nMask r-cnn. Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, Ross Girshick, Proceedings of the IEEE international conference on computer vision. the IEEE international conference on computer vision7Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Gir- shick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961-2969, 2017. 1, 2, 6, 7, 8\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. 3\n\nRelation networks for object detection. Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, Yichen Wei, Proceedings of the IEEE conference on computer vision and pattern recognition. the IEEE conference on computer vision and pattern recognitionHan Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object detection. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 3588-3597, 2018. 2\n\nReal-time upper-body human pose estimation using a depth camera. Anbumani Himanshu Prakash Jain, Sukhendu Subramanian, Anurag Das, Mittal, International Conference on Computer Vision/Computer Graphics Collaboration Techniques and Applications. SpringerHimanshu Prakash Jain, Anbumani Subramanian, Sukhendu Das, and Anurag Mittal. Real-time upper-body human pose estimation using a depth camera. In International Con- ference on Computer Vision/Computer Graphics Collabora- tion Techniques and Applications, pages 227-238. Springer, 2011. 1\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, ICLR. 96Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. ICLR, 9, 2015. 6\n\nPifpaf: Composite fields for human pose estimation. Sven Kreiss, Lorenzo Bertoni, Alexandre Alahi, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition7Sven Kreiss, Lorenzo Bertoni, and Alexandre Alahi. Pifpaf: Composite fields for human pose estimation. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11977-11986, 2019. 1, 2, 6, 7\n\nCornernet: Detecting objects as paired keypoints. Hei Law, Jia Deng, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)Hei Law and Jia Deng. Cornernet: Detecting objects as paired keypoints. In Proceedings of the European confer- ence on computer vision (ECCV), pages 734-750, 2018. 5\n\nCrowdpose: Efficient crowded scenes pose estimation and a new benchmark. Jiefeng Li, Can Wang, Hao Zhu, Yihuan Mao, Hao-Shu Fang, Cewu Lu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionJiefeng Li, Can Wang, Hao Zhu, Yihuan Mao, Hao-Shu Fang, and Cewu Lu. Crowdpose: Efficient crowded scenes pose estimation and a new benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10863-10872, 2019. 8\n\nPose recognition with cascade transformers. Ke Li, Shijie Wang, Xiang Zhang, Yifan Xu, Weijian Xu, Zhuowen Tu, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition36Ke Li, Shijie Wang, Xiang Zhang, Yifan Xu, Weijian Xu, and Zhuowen Tu. Pose recognition with cascade transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1944-1953, 2021. 3, 6\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, European conference on computer vision. Springer35Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740-755. Springer, 2014. 3, 5\n\nPose transferrable person reidentification. Jinxian Liu, Bingbing Ni, Yichao Yan, Peng Zhou, Shuo Cheng, Jianguo Hu, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. the IEEE Conference on Computer Vision and Pattern RecognitionJinxian Liu, Bingbing Ni, Yichao Yan, Peng Zhou, Shuo Cheng, and Jianguo Hu. Pose transferrable person re- identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4099- 4108, 2018. 1\n\nSwin transformer: Hierarchical vision transformer using shifted windows. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, Baining Guo, Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). the IEEE/CVF International Conference on Computer Vision (ICCV)Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin trans- former: Hierarchical vision transformer using shifted win- dows. In Proceedings of the IEEE/CVF International Con- ference on Computer Vision (ICCV), pages 10012-10022, October 2021. 6\n\nRethinking the heatmap regression for bottom-up human pose estimation. Zhengxiong Luo, Zhicheng Wang, Yan Huang, Liang Wang, Tieniu Tan, Erjin Zhou, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition7Zhengxiong Luo, Zhicheng Wang, Yan Huang, Liang Wang, Tieniu Tan, and Erjin Zhou. Rethinking the heatmap regres- sion for bottom-up human pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, pages 13264-13273, 2021. 6, 7, 8\n\nTfpose: Direct human pose estimation with transformers. Weian Mao, Yongtao Ge, Chunhua Shen, Zhi Tian, Xinlong Wang, Zhibin Wang, arXiv:2103.15320arXiv preprintWeian Mao, Yongtao Ge, Chunhua Shen, Zhi Tian, Xinlong Wang, and Zhibin Wang. Tfpose: Direct human pose esti- mation with transformers. arXiv preprint arXiv:2103.15320, 2021. 3\n\nFcpose: Fully convolutional multi-person pose estimation with dynamic instance-aware convolutions. Weian Mao, Zhi Tian, Xinlong Wang, Chunhua Shen, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition67Weian Mao, Zhi Tian, Xinlong Wang, and Chunhua Shen. Fcpose: Fully convolutional multi-person pose estimation with dynamic instance-aware convolutions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, pages 9034-9043, 2021. 1, 2, 3, 5, 6, 7\n\nAssociative embedding: End-to-end learning for joint detection and grouping. Alejandro Newell, Zhiao Huang, Jia Deng, Advances in Neural Information Processing Systems. I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. GarnettCurran Associates, Inc307Alejandro Newell, Zhiao Huang, and Jia Deng. Associa- tive embedding: End-to-end learning for joint detection and grouping. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wal- lach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, vol- ume 30. Curran Associates, Inc., 2017. 1, 2, 6, 7\n\nStacked hourglass networks for human pose estimation. Alejandro Newell, Kaiyu Yang, Jia Deng, European conference on computer vision. SpringerAlejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hour- glass networks for human pose estimation. In European con- ference on computer vision, pages 483-499. Springer, 2016. 2\n\nSingle-stage multi-person pose machines. Xuecheng Nie, Jiashi Feng, Jianfeng Zhang, Shuicheng Yan, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer Vision23Xuecheng Nie, Jiashi Feng, Jianfeng Zhang, and Shuicheng Yan. Single-stage multi-person pose machines. In Proceed- ings of the IEEE/CVF International Conference on Com- puter Vision, pages 6951-6960, 2019. 2, 3\n\nPersonlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model. George Papandreou, Tyler Zhu, Liang-Chieh Chen, Spyros Gidaris, Jonathan Tompson, Kevin Murphy, Proceedings of the European Conference on Computer Vision (ECCV). the European Conference on Computer Vision (ECCV)1George Papandreou, Tyler Zhu, Liang-Chieh Chen, Spyros Gidaris, Jonathan Tompson, and Kevin Murphy. Person- lab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model. In Pro- ceedings of the European Conference on Computer Vision (ECCV), pages 269-286, 2018. 1, 2\n\nStrike a pose: Tracking people by finding stylized poses. Deva Ramanan, A David, Andrew Forsyth, Zisserman, IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05). IEEE1Deva Ramanan, David A Forsyth, and Andrew Zisserman. Strike a pose: Tracking people by finding stylized poses. In 2005 IEEE Computer Society Conference on Computer Vi- sion and Pattern Recognition (CVPR'05), volume 1, pages 271-278. IEEE, 2005. 1\n\nInspose: Instance-aware networks for single-stage multi-person pose estimation. Dahu Shi, Xing Wei, Xiaodong Yu, Wenming Tan, Ye Ren, Shiliang Pu, Proceedings of the 29th ACM International Conference on Multimedia. the 29th ACM International Conference on Multimedia67Dahu Shi, Xing Wei, Xiaodong Yu, Wenming Tan, Ye Ren, and Shiliang Pu. Inspose: Instance-aware networks for single-stage multi-person pose estimation. In Proceedings of the 29th ACM International Conference on Multimedia, pages 3079-3087, 2021. 1, 2, 3, 5, 6, 7\n\nDeep high-resolution representation learning for human pose estimation. Ke Sun, Bin Xiao, Dong Liu, Jingdong Wang, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition6Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for human pose es- timation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5693- 5703, 2019. 1, 2, 4, 6\n\nDirectpose: Direct end-to-end multi-person pose estimation. Zhi Tian, Hao Chen, Chunhua Shen, arXiv:1911.074516arXiv preprintZhi Tian, Hao Chen, and Chunhua Shen. Directpose: Di- rect end-to-end multi-person pose estimation. arXiv preprint arXiv:1911.07451, 2019. 2, 3, 6, 8\n\nAttention is all you need. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, Illia Polosukhin, Advances in neural information processing systems. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017. 3\n\nPoint-set anchors for object detection, instance segmentation and pose estimation. Fangyun Wei, Xiao Sun, Hongyang Li, Jingdong Wang, Stephen Lin, European Conference on Computer Vision. Springer26Fangyun Wei, Xiao Sun, Hongyang Li, Jingdong Wang, and Stephen Lin. Point-set anchors for object detection, instance segmentation and pose estimation. In European Conference on Computer Vision, pages 527-544. Springer, 2020. 2, 3, 6\n\nWenming Tan, and Yihong Gong. Scene-adaptive attention network for crowd counting. Xing Wei, Yuanrui Kang, Jihao Yang, Yunfeng Qiu, Dahu Shi, arXiv:2112.15509arXiv preprintXing Wei, Yuanrui Kang, Jihao Yang, Yunfeng Qiu, Dahu Shi, Wenming Tan, and Yihong Gong. Scene-adaptive attention network for crowd counting. arXiv preprint arXiv:2112.15509, 2021. 3\n\n. Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, Ross Girshick, Detectron2, Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. https://github. com/facebookresearch/detectron2, 2019. 6\n\nSimple baselines for human pose estimation and tracking. Bin Xiao, Haiping Wu, Yichen Wei, Proceedings of the European conference on computer vision (ECCV). the European conference on computer vision (ECCV)Bin Xiao, Haiping Wu, and Yichen Wei. Simple baselines for human pose estimation and tracking. In Proceedings of the European conference on computer vision (ECCV), pages 466-481, 2018. 1, 2, 6, 8\n\nSoit: Segmenting objects with instanceaware transformers. Xiaodong Yu, Dahu Shi, Xing Wei, Ye Ren, Tingqun Ye, Wenming Tan, Proceedings of the AAAI Conference on Artificial Intelligence. the AAAI Conference on Artificial Intelligence2022Xiaodong Yu, Dahu Shi, Xing Wei, Ye Ren, Tingqun Ye, and Wenming Tan. Soit: Segmenting objects with instance- aware transformers. In Proceedings of the AAAI Conference on Artificial Intelligence, 2022. 3\n\n. Xingyi Zhou, Dequan Wang, Philipp Kr\u00e4henb\u00fchl, arXiv:1904.0785026Objects as points. arXiv preprintXingyi Zhou, Dequan Wang, and Philipp Kr\u00e4henb\u00fchl. Ob- jects as points. arXiv preprint arXiv:1904.07850, 2019. 2, 3, 6\n\nDeformable detr: Deformable transformers for end-to-end object detection. Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, Jifeng Dai, ICLR 2021: The Ninth International Conference on Learning Representations. Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. In ICLR 2021: The Ninth In- ternational Conference on Learning Representations, 2021. 2, 3, 4, 5\n", "annotations": {"author": "[{\"end\":114,\"start\":61},{\"end\":208,\"start\":115},{\"end\":264,\"start\":209},{\"end\":316,\"start\":265},{\"end\":398,\"start\":317}]", "publisher": null, "author_last_name": "[{\"end\":69,\"start\":66},{\"end\":123,\"start\":120},{\"end\":219,\"start\":217},{\"end\":271,\"start\":268},{\"end\":328,\"start\":325}]", "author_first_name": "[{\"end\":65,\"start\":61},{\"end\":119,\"start\":115},{\"end\":216,\"start\":209},{\"end\":267,\"start\":265},{\"end\":324,\"start\":317}]", "author_affiliation": "[{\"end\":113,\"start\":71},{\"end\":207,\"start\":150},{\"end\":263,\"start\":221},{\"end\":315,\"start\":273},{\"end\":397,\"start\":355}]", "title": "[{\"end\":58,\"start\":1},{\"end\":456,\"start\":399}]", "venue": null, "abstract": "[{\"end\":1635,\"start\":486}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b8\"},\"end\":1922,\"start\":1919},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":1954,\"start\":1950},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1979,\"start\":1976},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":1982,\"start\":1979},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2009,\"start\":2005},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2165,\"start\":2162},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2168,\"start\":2165},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2171,\"start\":2168},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":2174,\"start\":2171},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":2177,\"start\":2174},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2584,\"start\":2581},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2669,\"start\":2665},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":2672,\"start\":2669},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2783,\"start\":2780},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2786,\"start\":2783},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2789,\"start\":2786},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2792,\"start\":2789},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":3226,\"start\":3222},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3532,\"start\":3528},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3535,\"start\":3532},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3538,\"start\":3535},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":3541,\"start\":3538},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3544,\"start\":3541},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":3547,\"start\":3544},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3557,\"start\":3553},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3737,\"start\":3733},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3755,\"start\":3751},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":4132,\"start\":4128},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":4135,\"start\":4132},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4153,\"start\":4149},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4156,\"start\":4153},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":4224,\"start\":4220},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":4227,\"start\":4224},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":4230,\"start\":4227},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":4331,\"start\":4328},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":4334,\"start\":4331},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":4734,\"start\":4730},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":5157,\"start\":5153},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":5160,\"start\":5157},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6658,\"start\":6654},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6669,\"start\":6665},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6678,\"start\":6675},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":6699,\"start\":6695},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6711,\"start\":6707},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6963,\"start\":6959},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7450,\"start\":7447},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":7572,\"start\":7568},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":7719,\"start\":7715},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":7815,\"start\":7811},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8317,\"start\":8313},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8320,\"start\":8317},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8323,\"start\":8320},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":8326,\"start\":8323},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8329,\"start\":8326},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8332,\"start\":8329},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":8509,\"start\":8505},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":8669,\"start\":8665},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":8805,\"start\":8801},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":8944,\"start\":8940},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":8961,\"start\":8957},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":9320,\"start\":9316},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":9363,\"start\":9359},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9549,\"start\":9546},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9551,\"start\":9549},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9553,\"start\":9551},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9556,\"start\":9553},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9559,\"start\":9556},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":9568,\"start\":9565},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":9663,\"start\":9660},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":9688,\"start\":9684},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":9822,\"start\":9819},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9836,\"start\":9832},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":9985,\"start\":9981},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":10111,\"start\":10107},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":10127,\"start\":10123},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11017,\"start\":11013},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":11953,\"start\":11950},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11956,\"start\":11953},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":12003,\"start\":12000},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":12005,\"start\":12003},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":12104,\"start\":12100},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":13752,\"start\":13748},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":14271,\"start\":14267},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":16578,\"start\":16575},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":16742,\"start\":16738},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":17711,\"start\":17707},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":17714,\"start\":17711},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":18088,\"start\":18084},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":18536,\"start\":18532},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":19235,\"start\":19231},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":19238,\"start\":19235},{\"end\":19341,\"start\":19336},{\"end\":19345,\"start\":19341},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":19350,\"start\":19346},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":19432,\"start\":19428},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19761,\"start\":19757},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":21051,\"start\":21047},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":21227,\"start\":21223},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":21243,\"start\":21239},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":21267,\"start\":21263},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":21284,\"start\":21280},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":21363,\"start\":21359},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":21827,\"start\":21824},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":21836,\"start\":21832},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":21849,\"start\":21845},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21866,\"start\":21863},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":21877,\"start\":21873},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":21892,\"start\":21888},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":21978,\"start\":21975},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":22099,\"start\":22096},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":22159,\"start\":22155},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22268,\"start\":22264},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":22645,\"start\":22641},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":22687,\"start\":22683},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":22976,\"start\":22972},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24070,\"start\":24066},{\"end\":24075,\"start\":24072},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":24389,\"start\":24385},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":24392,\"start\":24389},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":24444,\"start\":24441},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":25305,\"start\":25301},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":25660,\"start\":25657},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":26317,\"start\":26313},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":27828,\"start\":27824}]", "figure": "[{\"attributes\":{\"id\":\"fig_2\"},\"end\":27056,\"start\":27033},{\"attributes\":{\"id\":\"fig_3\"},\"end\":27232,\"start\":27057},{\"attributes\":{\"id\":\"fig_5\"},\"end\":27838,\"start\":27233},{\"attributes\":{\"id\":\"fig_6\"},\"end\":28095,\"start\":27839},{\"attributes\":{\"id\":\"fig_9\"},\"end\":28306,\"start\":28096},{\"attributes\":{\"id\":\"fig_10\"},\"end\":28606,\"start\":28307},{\"attributes\":{\"id\":\"fig_11\"},\"end\":28780,\"start\":28607},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":30914,\"start\":28781},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":31061,\"start\":30915},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":31242,\"start\":31062},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":31342,\"start\":31243},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":32126,\"start\":31343}]", "paragraph": "[{\"end\":2015,\"start\":1651},{\"end\":3209,\"start\":2017},{\"end\":3405,\"start\":3211},{\"end\":4273,\"start\":3407},{\"end\":5379,\"start\":4275},{\"end\":5536,\"start\":5381},{\"end\":5781,\"start\":5538},{\"end\":5962,\"start\":5783},{\"end\":6182,\"start\":5964},{\"end\":6392,\"start\":6230},{\"end\":7183,\"start\":6394},{\"end\":9321,\"start\":7185},{\"end\":10409,\"start\":9347},{\"end\":10870,\"start\":10448},{\"end\":11293,\"start\":10872},{\"end\":11835,\"start\":11316},{\"end\":12138,\"start\":11862},{\"end\":12703,\"start\":12140},{\"end\":13063,\"start\":12720},{\"end\":13258,\"start\":13149},{\"end\":14174,\"start\":13260},{\"end\":14572,\"start\":14176},{\"end\":15161,\"start\":14608},{\"end\":16431,\"start\":15179},{\"end\":16546,\"start\":16464},{\"end\":17243,\"start\":16565},{\"end\":17680,\"start\":17328},{\"end\":18248,\"start\":17682},{\"end\":18334,\"start\":18250},{\"end\":18442,\"start\":18383},{\"end\":18844,\"start\":18484},{\"end\":19186,\"start\":18846},{\"end\":19682,\"start\":19188},{\"end\":20175,\"start\":19684},{\"end\":20576,\"start\":20177},{\"end\":21096,\"start\":20605},{\"end\":21681,\"start\":21098},{\"end\":22362,\"start\":21683},{\"end\":23109,\"start\":22364},{\"end\":23274,\"start\":23128},{\"end\":24253,\"start\":23276},{\"end\":25237,\"start\":24255},{\"end\":25661,\"start\":25251},{\"end\":25991,\"start\":25663},{\"end\":26602,\"start\":25993},{\"end\":27032,\"start\":26617}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":11315,\"start\":11294},{\"attributes\":{\"id\":\"formula_1\"},\"end\":13148,\"start\":13064},{\"attributes\":{\"id\":\"formula_2\"},\"end\":14607,\"start\":14573},{\"attributes\":{\"id\":\"formula_3\"},\"end\":16463,\"start\":16432},{\"attributes\":{\"id\":\"formula_4\"},\"end\":17327,\"start\":17244},{\"attributes\":{\"id\":\"formula_5\"},\"end\":18382,\"start\":18335}]", "table_ref": "[{\"end\":3113,\"start\":3058},{\"end\":3166,\"start\":3159},{\"end\":5472,\"start\":5465},{\"end\":20687,\"start\":20680},{\"end\":22534,\"start\":22527},{\"end\":23550,\"start\":23543},{\"end\":23798,\"start\":23791},{\"end\":24290,\"start\":24283},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":25042,\"start\":25035},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":26110,\"start\":26103}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1649,\"start\":1637},{\"attributes\":{\"n\":\"2.\"},\"end\":6197,\"start\":6185},{\"attributes\":{\"n\":\"2.1.\"},\"end\":6228,\"start\":6200},{\"attributes\":{\"n\":\"2.2.\"},\"end\":9345,\"start\":9324},{\"attributes\":{\"n\":\"3.\"},\"end\":10423,\"start\":10412},{\"attributes\":{\"n\":\"3.1.\"},\"end\":10446,\"start\":10426},{\"attributes\":{\"n\":\"3.2.\"},\"end\":11860,\"start\":11838},{\"attributes\":{\"n\":\"3.3.\"},\"end\":12718,\"start\":12706},{\"attributes\":{\"n\":\"3.4.\"},\"end\":15177,\"start\":15164},{\"attributes\":{\"n\":\"3.5.\"},\"end\":16563,\"start\":16549},{\"attributes\":{\"n\":\"4.\"},\"end\":18456,\"start\":18445},{\"attributes\":{\"n\":\"4.1.\"},\"end\":18482,\"start\":18459},{\"attributes\":{\"n\":\"4.2.\"},\"end\":20603,\"start\":20579},{\"attributes\":{\"n\":\"4.3.\"},\"end\":23126,\"start\":23112},{\"attributes\":{\"n\":\"4.4.\"},\"end\":25249,\"start\":25240},{\"attributes\":{\"n\":\"5.\"},\"end\":26615,\"start\":26605},{\"end\":27068,\"start\":27058},{\"end\":27244,\"start\":27234},{\"end\":27850,\"start\":27840},{\"end\":28107,\"start\":28097},{\"end\":28318,\"start\":28308},{\"end\":28618,\"start\":28608},{\"end\":31253,\"start\":31244},{\"end\":31353,\"start\":31344}]", "table": "[{\"end\":30914,\"start\":28797},{\"end\":31242,\"start\":31109},{\"end\":32126,\"start\":31566}]", "figure_caption": "[{\"end\":27056,\"start\":27035},{\"end\":27232,\"start\":27070},{\"end\":27838,\"start\":27246},{\"end\":28095,\"start\":27852},{\"end\":28306,\"start\":28109},{\"end\":28606,\"start\":28320},{\"end\":28780,\"start\":28620},{\"end\":28797,\"start\":28783},{\"end\":31061,\"start\":30917},{\"end\":31109,\"start\":31064},{\"end\":31342,\"start\":31255},{\"end\":31566,\"start\":31355}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":2206,\"start\":2197},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":2812,\"start\":2803},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":5460,\"start\":5452},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":10471,\"start\":10463},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":13329,\"start\":13321},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":15199,\"start\":15191},{\"attributes\":{\"ref_id\":\"fig_9\"},\"end\":15879,\"start\":15871},{\"attributes\":{\"ref_id\":\"fig_11\"},\"end\":22816,\"start\":22808},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":23668,\"start\":23656}]", "bib_author_first_name": "[{\"end\":32253,\"start\":32245},{\"end\":32269,\"start\":32265},{\"end\":32282,\"start\":32277},{\"end\":32303,\"start\":32297},{\"end\":32321,\"start\":32316},{\"end\":32336,\"start\":32329},{\"end\":32348,\"start\":32343},{\"end\":32871,\"start\":32868},{\"end\":32882,\"start\":32877},{\"end\":32897,\"start\":32892},{\"end\":32912,\"start\":32905},{\"end\":32923,\"start\":32918},{\"end\":33309,\"start\":33306},{\"end\":33320,\"start\":33315},{\"end\":33335,\"start\":33328},{\"end\":33346,\"start\":33341},{\"end\":33788,\"start\":33781},{\"end\":33806,\"start\":33797},{\"end\":33821,\"start\":33814},{\"end\":33839,\"start\":33832},{\"end\":33858,\"start\":33849},{\"end\":33875,\"start\":33869},{\"end\":34249,\"start\":34244},{\"end\":34264,\"start\":34256},{\"end\":34278,\"start\":34271},{\"end\":34293,\"start\":34285},{\"end\":34305,\"start\":34301},{\"end\":34314,\"start\":34310},{\"end\":34796,\"start\":34791},{\"end\":34813,\"start\":34804},{\"end\":34815,\"start\":34814},{\"end\":34834,\"start\":34825},{\"end\":35140,\"start\":35135},{\"end\":35151,\"start\":35148},{\"end\":35166,\"start\":35158},{\"end\":35180,\"start\":35173},{\"end\":35187,\"start\":35186},{\"end\":35199,\"start\":35196},{\"end\":35666,\"start\":35660},{\"end\":35685,\"start\":35680},{\"end\":35702,\"start\":35693},{\"end\":35719,\"start\":35715},{\"end\":35740,\"start\":35733},{\"end\":35753,\"start\":35747},{\"end\":35774,\"start\":35767},{\"end\":35793,\"start\":35785},{\"end\":35809,\"start\":35804},{\"end\":36336,\"start\":36332},{\"end\":36344,\"start\":36341},{\"end\":36356,\"start\":36351},{\"end\":36777,\"start\":36771},{\"end\":36799,\"start\":36792},{\"end\":36809,\"start\":36805},{\"end\":37221,\"start\":37215},{\"end\":37230,\"start\":37228},{\"end\":37239,\"start\":37236},{\"end\":37255,\"start\":37246},{\"end\":37271,\"start\":37263},{\"end\":37711,\"start\":37704},{\"end\":37723,\"start\":37716},{\"end\":37739,\"start\":37734},{\"end\":37752,\"start\":37748},{\"end\":38125,\"start\":38118},{\"end\":38137,\"start\":38130},{\"end\":38153,\"start\":38145},{\"end\":38163,\"start\":38159},{\"end\":38562,\"start\":38559},{\"end\":38574,\"start\":38567},{\"end\":38584,\"start\":38579},{\"end\":38598,\"start\":38592},{\"end\":38610,\"start\":38604},{\"end\":39041,\"start\":39033},{\"end\":39073,\"start\":39065},{\"end\":39093,\"start\":39087},{\"end\":39554,\"start\":39553},{\"end\":39570,\"start\":39565},{\"end\":39741,\"start\":39737},{\"end\":39757,\"start\":39750},{\"end\":39776,\"start\":39767},{\"end\":40215,\"start\":40212},{\"end\":40224,\"start\":40221},{\"end\":40593,\"start\":40586},{\"end\":40601,\"start\":40598},{\"end\":40611,\"start\":40608},{\"end\":40623,\"start\":40617},{\"end\":40636,\"start\":40629},{\"end\":40647,\"start\":40643},{\"end\":41104,\"start\":41102},{\"end\":41115,\"start\":41109},{\"end\":41127,\"start\":41122},{\"end\":41140,\"start\":41135},{\"end\":41152,\"start\":41145},{\"end\":41164,\"start\":41157},{\"end\":41601,\"start\":41593},{\"end\":41614,\"start\":41607},{\"end\":41627,\"start\":41622},{\"end\":41643,\"start\":41638},{\"end\":41656,\"start\":41650},{\"end\":41669,\"start\":41665},{\"end\":41684,\"start\":41679},{\"end\":41703,\"start\":41693},{\"end\":42061,\"start\":42054},{\"end\":42075,\"start\":42067},{\"end\":42086,\"start\":42080},{\"end\":42096,\"start\":42092},{\"end\":42107,\"start\":42103},{\"end\":42122,\"start\":42115},{\"end\":42575,\"start\":42573},{\"end\":42587,\"start\":42581},{\"end\":42596,\"start\":42593},{\"end\":42605,\"start\":42602},{\"end\":42616,\"start\":42610},{\"end\":42627,\"start\":42622},{\"end\":42642,\"start\":42635},{\"end\":42655,\"start\":42648},{\"end\":43175,\"start\":43165},{\"end\":43189,\"start\":43181},{\"end\":43199,\"start\":43196},{\"end\":43212,\"start\":43207},{\"end\":43225,\"start\":43219},{\"end\":43236,\"start\":43231},{\"end\":43731,\"start\":43726},{\"end\":43744,\"start\":43737},{\"end\":43756,\"start\":43749},{\"end\":43766,\"start\":43763},{\"end\":43780,\"start\":43773},{\"end\":43793,\"start\":43787},{\"end\":44112,\"start\":44107},{\"end\":44121,\"start\":44118},{\"end\":44135,\"start\":44128},{\"end\":44149,\"start\":44142},{\"end\":44674,\"start\":44665},{\"end\":44688,\"start\":44683},{\"end\":44699,\"start\":44696},{\"end\":45272,\"start\":45263},{\"end\":45286,\"start\":45281},{\"end\":45296,\"start\":45293},{\"end\":45579,\"start\":45571},{\"end\":45591,\"start\":45585},{\"end\":45606,\"start\":45598},{\"end\":45623,\"start\":45614},{\"end\":46095,\"start\":46089},{\"end\":46113,\"start\":46108},{\"end\":46130,\"start\":46119},{\"end\":46143,\"start\":46137},{\"end\":46161,\"start\":46153},{\"end\":46176,\"start\":46171},{\"end\":46680,\"start\":46676},{\"end\":46691,\"start\":46690},{\"end\":46705,\"start\":46699},{\"end\":47150,\"start\":47146},{\"end\":47160,\"start\":47156},{\"end\":47174,\"start\":47166},{\"end\":47186,\"start\":47179},{\"end\":47194,\"start\":47192},{\"end\":47208,\"start\":47200},{\"end\":47671,\"start\":47669},{\"end\":47680,\"start\":47677},{\"end\":47691,\"start\":47687},{\"end\":47705,\"start\":47697},{\"end\":48168,\"start\":48165},{\"end\":48178,\"start\":48175},{\"end\":48192,\"start\":48185},{\"end\":48414,\"start\":48408},{\"end\":48428,\"start\":48424},{\"end\":48442,\"start\":48438},{\"end\":48456,\"start\":48451},{\"end\":48473,\"start\":48468},{\"end\":48486,\"start\":48481},{\"end\":48488,\"start\":48487},{\"end\":48502,\"start\":48496},{\"end\":48516,\"start\":48511},{\"end\":48904,\"start\":48897},{\"end\":48914,\"start\":48910},{\"end\":48928,\"start\":48920},{\"end\":48941,\"start\":48933},{\"end\":48955,\"start\":48948},{\"end\":49332,\"start\":49328},{\"end\":49345,\"start\":49338},{\"end\":49357,\"start\":49352},{\"end\":49371,\"start\":49364},{\"end\":49381,\"start\":49377},{\"end\":49608,\"start\":49603},{\"end\":49622,\"start\":49613},{\"end\":49642,\"start\":49633},{\"end\":49657,\"start\":49650},{\"end\":49666,\"start\":49662},{\"end\":49897,\"start\":49894},{\"end\":49911,\"start\":49904},{\"end\":49922,\"start\":49916},{\"end\":50306,\"start\":50298},{\"end\":50315,\"start\":50311},{\"end\":50325,\"start\":50321},{\"end\":50333,\"start\":50331},{\"end\":50346,\"start\":50339},{\"end\":50358,\"start\":50351},{\"end\":50690,\"start\":50684},{\"end\":50703,\"start\":50697},{\"end\":50717,\"start\":50710},{\"end\":50980,\"start\":50974},{\"end\":50992,\"start\":50986},{\"end\":51002,\"start\":50997},{\"end\":51010,\"start\":51007},{\"end\":51023,\"start\":51015},{\"end\":51036,\"start\":51030}]", "bib_author_last_name": "[{\"end\":32263,\"start\":32254},{\"end\":32275,\"start\":32270},{\"end\":32295,\"start\":32283},{\"end\":32314,\"start\":32304},{\"end\":32327,\"start\":32322},{\"end\":32341,\"start\":32337},{\"end\":32356,\"start\":32349},{\"end\":32875,\"start\":32872},{\"end\":32890,\"start\":32883},{\"end\":32903,\"start\":32898},{\"end\":32916,\"start\":32913},{\"end\":32930,\"start\":32924},{\"end\":33313,\"start\":33310},{\"end\":33326,\"start\":33321},{\"end\":33339,\"start\":33336},{\"end\":33353,\"start\":33347},{\"end\":33795,\"start\":33789},{\"end\":33812,\"start\":33807},{\"end\":33830,\"start\":33822},{\"end\":33847,\"start\":33840},{\"end\":33867,\"start\":33859},{\"end\":33885,\"start\":33876},{\"end\":34254,\"start\":34250},{\"end\":34269,\"start\":34265},{\"end\":34283,\"start\":34279},{\"end\":34299,\"start\":34294},{\"end\":34308,\"start\":34306},{\"end\":34318,\"start\":34315},{\"end\":34802,\"start\":34797},{\"end\":34823,\"start\":34816},{\"end\":34843,\"start\":34835},{\"end\":35146,\"start\":35141},{\"end\":35156,\"start\":35152},{\"end\":35171,\"start\":35167},{\"end\":35184,\"start\":35181},{\"end\":35194,\"start\":35188},{\"end\":35205,\"start\":35200},{\"end\":35212,\"start\":35207},{\"end\":35678,\"start\":35667},{\"end\":35691,\"start\":35686},{\"end\":35713,\"start\":35703},{\"end\":35731,\"start\":35720},{\"end\":35745,\"start\":35741},{\"end\":35765,\"start\":35754},{\"end\":35783,\"start\":35775},{\"end\":35802,\"start\":35794},{\"end\":35817,\"start\":35810},{\"end\":36339,\"start\":36337},{\"end\":36349,\"start\":36345},{\"end\":36361,\"start\":36357},{\"end\":36790,\"start\":36778},{\"end\":36803,\"start\":36800},{\"end\":36813,\"start\":36810},{\"end\":36817,\"start\":36815},{\"end\":37226,\"start\":37222},{\"end\":37234,\"start\":37231},{\"end\":37244,\"start\":37240},{\"end\":37261,\"start\":37256},{\"end\":37276,\"start\":37272},{\"end\":37714,\"start\":37712},{\"end\":37732,\"start\":37724},{\"end\":37746,\"start\":37740},{\"end\":37761,\"start\":37753},{\"end\":38128,\"start\":38126},{\"end\":38143,\"start\":38138},{\"end\":38157,\"start\":38154},{\"end\":38167,\"start\":38164},{\"end\":38565,\"start\":38563},{\"end\":38577,\"start\":38575},{\"end\":38590,\"start\":38585},{\"end\":38602,\"start\":38599},{\"end\":38614,\"start\":38611},{\"end\":39063,\"start\":39042},{\"end\":39085,\"start\":39074},{\"end\":39097,\"start\":39094},{\"end\":39105,\"start\":39099},{\"end\":39563,\"start\":39555},{\"end\":39577,\"start\":39571},{\"end\":39581,\"start\":39579},{\"end\":39748,\"start\":39742},{\"end\":39765,\"start\":39758},{\"end\":39782,\"start\":39777},{\"end\":40219,\"start\":40216},{\"end\":40229,\"start\":40225},{\"end\":40596,\"start\":40594},{\"end\":40606,\"start\":40602},{\"end\":40615,\"start\":40612},{\"end\":40627,\"start\":40624},{\"end\":40641,\"start\":40637},{\"end\":40650,\"start\":40648},{\"end\":41107,\"start\":41105},{\"end\":41120,\"start\":41116},{\"end\":41133,\"start\":41128},{\"end\":41143,\"start\":41141},{\"end\":41155,\"start\":41153},{\"end\":41167,\"start\":41165},{\"end\":41605,\"start\":41602},{\"end\":41620,\"start\":41615},{\"end\":41636,\"start\":41628},{\"end\":41648,\"start\":41644},{\"end\":41663,\"start\":41657},{\"end\":41677,\"start\":41670},{\"end\":41691,\"start\":41685},{\"end\":41711,\"start\":41704},{\"end\":42065,\"start\":42062},{\"end\":42078,\"start\":42076},{\"end\":42090,\"start\":42087},{\"end\":42101,\"start\":42097},{\"end\":42113,\"start\":42108},{\"end\":42125,\"start\":42123},{\"end\":42579,\"start\":42576},{\"end\":42591,\"start\":42588},{\"end\":42600,\"start\":42597},{\"end\":42608,\"start\":42606},{\"end\":42620,\"start\":42617},{\"end\":42633,\"start\":42628},{\"end\":42646,\"start\":42643},{\"end\":42659,\"start\":42656},{\"end\":43179,\"start\":43176},{\"end\":43194,\"start\":43190},{\"end\":43205,\"start\":43200},{\"end\":43217,\"start\":43213},{\"end\":43229,\"start\":43226},{\"end\":43241,\"start\":43237},{\"end\":43735,\"start\":43732},{\"end\":43747,\"start\":43745},{\"end\":43761,\"start\":43757},{\"end\":43771,\"start\":43767},{\"end\":43785,\"start\":43781},{\"end\":43798,\"start\":43794},{\"end\":44116,\"start\":44113},{\"end\":44126,\"start\":44122},{\"end\":44140,\"start\":44136},{\"end\":44154,\"start\":44150},{\"end\":44681,\"start\":44675},{\"end\":44694,\"start\":44689},{\"end\":44704,\"start\":44700},{\"end\":45279,\"start\":45273},{\"end\":45291,\"start\":45287},{\"end\":45301,\"start\":45297},{\"end\":45583,\"start\":45580},{\"end\":45596,\"start\":45592},{\"end\":45612,\"start\":45607},{\"end\":45627,\"start\":45624},{\"end\":46106,\"start\":46096},{\"end\":46117,\"start\":46114},{\"end\":46135,\"start\":46131},{\"end\":46151,\"start\":46144},{\"end\":46169,\"start\":46162},{\"end\":46183,\"start\":46177},{\"end\":46688,\"start\":46681},{\"end\":46697,\"start\":46692},{\"end\":46713,\"start\":46706},{\"end\":46724,\"start\":46715},{\"end\":47154,\"start\":47151},{\"end\":47164,\"start\":47161},{\"end\":47177,\"start\":47175},{\"end\":47190,\"start\":47187},{\"end\":47198,\"start\":47195},{\"end\":47211,\"start\":47209},{\"end\":47675,\"start\":47672},{\"end\":47685,\"start\":47681},{\"end\":47695,\"start\":47692},{\"end\":47710,\"start\":47706},{\"end\":48173,\"start\":48169},{\"end\":48183,\"start\":48179},{\"end\":48197,\"start\":48193},{\"end\":48422,\"start\":48415},{\"end\":48436,\"start\":48429},{\"end\":48449,\"start\":48443},{\"end\":48466,\"start\":48457},{\"end\":48479,\"start\":48474},{\"end\":48494,\"start\":48489},{\"end\":48509,\"start\":48503},{\"end\":48527,\"start\":48517},{\"end\":48908,\"start\":48905},{\"end\":48918,\"start\":48915},{\"end\":48931,\"start\":48929},{\"end\":48946,\"start\":48942},{\"end\":48959,\"start\":48956},{\"end\":49336,\"start\":49333},{\"end\":49350,\"start\":49346},{\"end\":49362,\"start\":49358},{\"end\":49375,\"start\":49372},{\"end\":49385,\"start\":49382},{\"end\":49611,\"start\":49609},{\"end\":49631,\"start\":49623},{\"end\":49648,\"start\":49643},{\"end\":49660,\"start\":49658},{\"end\":49675,\"start\":49667},{\"end\":49687,\"start\":49677},{\"end\":49902,\"start\":49898},{\"end\":49914,\"start\":49912},{\"end\":49926,\"start\":49923},{\"end\":50309,\"start\":50307},{\"end\":50319,\"start\":50316},{\"end\":50329,\"start\":50326},{\"end\":50337,\"start\":50334},{\"end\":50349,\"start\":50347},{\"end\":50362,\"start\":50359},{\"end\":50695,\"start\":50691},{\"end\":50708,\"start\":50704},{\"end\":50728,\"start\":50718},{\"end\":50984,\"start\":50981},{\"end\":50995,\"start\":50993},{\"end\":51005,\"start\":51003},{\"end\":51013,\"start\":51011},{\"end\":51028,\"start\":51024},{\"end\":51040,\"start\":51037}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":4768761},\"end\":32787,\"start\":32182},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":198169848},\"end\":33235,\"start\":32789},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":16224674},\"end\":33733,\"start\":33237},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":218889832},\"end\":34183,\"start\":33735},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":4703058},\"end\":34717,\"start\":34185},{\"attributes\":{\"doi\":\"arXiv:2107.06278\",\"id\":\"b5\"},\"end\":35047,\"start\":34719},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":212675120},\"end\":35658,\"start\":35049},{\"attributes\":{\"doi\":\"arXiv:2010.11929\",\"id\":\"b7\"},\"end\":36253,\"start\":35660},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":8040013},\"end\":36724,\"start\":36255},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":6529517},\"end\":37143,\"start\":36726},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":233033921},\"end\":37690,\"start\":37145},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":54465873},\"end\":38070,\"start\":37692},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":206594692},\"end\":38517,\"start\":38072},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":37158713},\"end\":38966,\"start\":38519},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":17631501},\"end\":39507,\"start\":38968},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":6628106},\"end\":39683,\"start\":39509},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":80628362},\"end\":40160,\"start\":39685},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":51923817},\"end\":40511,\"start\":40162},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":54443684},\"end\":41056,\"start\":40513},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":233231207},\"end\":41548,\"start\":41058},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":14113767},\"end\":42008,\"start\":41550},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":51748458},\"end\":42498,\"start\":42010},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":232352874},\"end\":43092,\"start\":42500},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":229923779},\"end\":43668,\"start\":43094},{\"attributes\":{\"doi\":\"arXiv:2103.15320\",\"id\":\"b24\"},\"end\":44006,\"start\":43670},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":235254668},\"end\":44586,\"start\":44008},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":340420},\"end\":45207,\"start\":44588},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":13613792},\"end\":45528,\"start\":45209},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":201668824},\"end\":45970,\"start\":45530},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":4031629},\"end\":46616,\"start\":45972},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":5574410},\"end\":47064,\"start\":46618},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":236087411},\"end\":47595,\"start\":47066},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":67856425},\"end\":48103,\"start\":47597},{\"attributes\":{\"doi\":\"arXiv:1911.07451\",\"id\":\"b33\"},\"end\":48379,\"start\":48105},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":13756489},\"end\":48812,\"start\":48381},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":220364346},\"end\":49243,\"start\":48814},{\"attributes\":{\"doi\":\"arXiv:2112.15509\",\"id\":\"b36\"},\"end\":49599,\"start\":49245},{\"attributes\":{\"id\":\"b37\"},\"end\":49835,\"start\":49601},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":4934594},\"end\":50238,\"start\":49837},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":245353546},\"end\":50680,\"start\":50240},{\"attributes\":{\"doi\":\"arXiv:1904.07850\",\"id\":\"b40\"},\"end\":50898,\"start\":50682},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":222208633},\"end\":51359,\"start\":50900}]", "bib_title": "[{\"end\":32243,\"start\":32182},{\"end\":32866,\"start\":32789},{\"end\":33304,\"start\":33237},{\"end\":33779,\"start\":33735},{\"end\":34242,\"start\":34185},{\"end\":35133,\"start\":35049},{\"end\":36330,\"start\":36255},{\"end\":36769,\"start\":36726},{\"end\":37213,\"start\":37145},{\"end\":37702,\"start\":37692},{\"end\":38116,\"start\":38072},{\"end\":38557,\"start\":38519},{\"end\":39031,\"start\":38968},{\"end\":39551,\"start\":39509},{\"end\":39735,\"start\":39685},{\"end\":40210,\"start\":40162},{\"end\":40584,\"start\":40513},{\"end\":41100,\"start\":41058},{\"end\":41591,\"start\":41550},{\"end\":42052,\"start\":42010},{\"end\":42571,\"start\":42500},{\"end\":43163,\"start\":43094},{\"end\":44105,\"start\":44008},{\"end\":44663,\"start\":44588},{\"end\":45261,\"start\":45209},{\"end\":45569,\"start\":45530},{\"end\":46087,\"start\":45972},{\"end\":46674,\"start\":46618},{\"end\":47144,\"start\":47066},{\"end\":47667,\"start\":47597},{\"end\":48406,\"start\":48381},{\"end\":48895,\"start\":48814},{\"end\":49892,\"start\":49837},{\"end\":50296,\"start\":50240},{\"end\":50972,\"start\":50900}]", "bib_author": "[{\"end\":32265,\"start\":32245},{\"end\":32277,\"start\":32265},{\"end\":32297,\"start\":32277},{\"end\":32316,\"start\":32297},{\"end\":32329,\"start\":32316},{\"end\":32343,\"start\":32329},{\"end\":32358,\"start\":32343},{\"end\":32877,\"start\":32868},{\"end\":32892,\"start\":32877},{\"end\":32905,\"start\":32892},{\"end\":32918,\"start\":32905},{\"end\":32932,\"start\":32918},{\"end\":33315,\"start\":33306},{\"end\":33328,\"start\":33315},{\"end\":33341,\"start\":33328},{\"end\":33355,\"start\":33341},{\"end\":33797,\"start\":33781},{\"end\":33814,\"start\":33797},{\"end\":33832,\"start\":33814},{\"end\":33849,\"start\":33832},{\"end\":33869,\"start\":33849},{\"end\":33887,\"start\":33869},{\"end\":34256,\"start\":34244},{\"end\":34271,\"start\":34256},{\"end\":34285,\"start\":34271},{\"end\":34301,\"start\":34285},{\"end\":34310,\"start\":34301},{\"end\":34320,\"start\":34310},{\"end\":34804,\"start\":34791},{\"end\":34825,\"start\":34804},{\"end\":34845,\"start\":34825},{\"end\":35148,\"start\":35135},{\"end\":35158,\"start\":35148},{\"end\":35173,\"start\":35158},{\"end\":35186,\"start\":35173},{\"end\":35196,\"start\":35186},{\"end\":35207,\"start\":35196},{\"end\":35214,\"start\":35207},{\"end\":35680,\"start\":35660},{\"end\":35693,\"start\":35680},{\"end\":35715,\"start\":35693},{\"end\":35733,\"start\":35715},{\"end\":35747,\"start\":35733},{\"end\":35767,\"start\":35747},{\"end\":35785,\"start\":35767},{\"end\":35804,\"start\":35785},{\"end\":35819,\"start\":35804},{\"end\":36341,\"start\":36332},{\"end\":36351,\"start\":36341},{\"end\":36363,\"start\":36351},{\"end\":36792,\"start\":36771},{\"end\":36805,\"start\":36792},{\"end\":36815,\"start\":36805},{\"end\":36819,\"start\":36815},{\"end\":37228,\"start\":37215},{\"end\":37236,\"start\":37228},{\"end\":37246,\"start\":37236},{\"end\":37263,\"start\":37246},{\"end\":37278,\"start\":37263},{\"end\":37716,\"start\":37704},{\"end\":37734,\"start\":37716},{\"end\":37748,\"start\":37734},{\"end\":37763,\"start\":37748},{\"end\":38130,\"start\":38118},{\"end\":38145,\"start\":38130},{\"end\":38159,\"start\":38145},{\"end\":38169,\"start\":38159},{\"end\":38567,\"start\":38559},{\"end\":38579,\"start\":38567},{\"end\":38592,\"start\":38579},{\"end\":38604,\"start\":38592},{\"end\":38616,\"start\":38604},{\"end\":39065,\"start\":39033},{\"end\":39087,\"start\":39065},{\"end\":39099,\"start\":39087},{\"end\":39107,\"start\":39099},{\"end\":39565,\"start\":39553},{\"end\":39579,\"start\":39565},{\"end\":39583,\"start\":39579},{\"end\":39750,\"start\":39737},{\"end\":39767,\"start\":39750},{\"end\":39784,\"start\":39767},{\"end\":40221,\"start\":40212},{\"end\":40231,\"start\":40221},{\"end\":40598,\"start\":40586},{\"end\":40608,\"start\":40598},{\"end\":40617,\"start\":40608},{\"end\":40629,\"start\":40617},{\"end\":40643,\"start\":40629},{\"end\":40652,\"start\":40643},{\"end\":41109,\"start\":41102},{\"end\":41122,\"start\":41109},{\"end\":41135,\"start\":41122},{\"end\":41145,\"start\":41135},{\"end\":41157,\"start\":41145},{\"end\":41169,\"start\":41157},{\"end\":41607,\"start\":41593},{\"end\":41622,\"start\":41607},{\"end\":41638,\"start\":41622},{\"end\":41650,\"start\":41638},{\"end\":41665,\"start\":41650},{\"end\":41679,\"start\":41665},{\"end\":41693,\"start\":41679},{\"end\":41713,\"start\":41693},{\"end\":42067,\"start\":42054},{\"end\":42080,\"start\":42067},{\"end\":42092,\"start\":42080},{\"end\":42103,\"start\":42092},{\"end\":42115,\"start\":42103},{\"end\":42127,\"start\":42115},{\"end\":42581,\"start\":42573},{\"end\":42593,\"start\":42581},{\"end\":42602,\"start\":42593},{\"end\":42610,\"start\":42602},{\"end\":42622,\"start\":42610},{\"end\":42635,\"start\":42622},{\"end\":42648,\"start\":42635},{\"end\":42661,\"start\":42648},{\"end\":43181,\"start\":43165},{\"end\":43196,\"start\":43181},{\"end\":43207,\"start\":43196},{\"end\":43219,\"start\":43207},{\"end\":43231,\"start\":43219},{\"end\":43243,\"start\":43231},{\"end\":43737,\"start\":43726},{\"end\":43749,\"start\":43737},{\"end\":43763,\"start\":43749},{\"end\":43773,\"start\":43763},{\"end\":43787,\"start\":43773},{\"end\":43800,\"start\":43787},{\"end\":44118,\"start\":44107},{\"end\":44128,\"start\":44118},{\"end\":44142,\"start\":44128},{\"end\":44156,\"start\":44142},{\"end\":44683,\"start\":44665},{\"end\":44696,\"start\":44683},{\"end\":44706,\"start\":44696},{\"end\":45281,\"start\":45263},{\"end\":45293,\"start\":45281},{\"end\":45303,\"start\":45293},{\"end\":45585,\"start\":45571},{\"end\":45598,\"start\":45585},{\"end\":45614,\"start\":45598},{\"end\":45629,\"start\":45614},{\"end\":46108,\"start\":46089},{\"end\":46119,\"start\":46108},{\"end\":46137,\"start\":46119},{\"end\":46153,\"start\":46137},{\"end\":46171,\"start\":46153},{\"end\":46185,\"start\":46171},{\"end\":46690,\"start\":46676},{\"end\":46699,\"start\":46690},{\"end\":46715,\"start\":46699},{\"end\":46726,\"start\":46715},{\"end\":47156,\"start\":47146},{\"end\":47166,\"start\":47156},{\"end\":47179,\"start\":47166},{\"end\":47192,\"start\":47179},{\"end\":47200,\"start\":47192},{\"end\":47213,\"start\":47200},{\"end\":47677,\"start\":47669},{\"end\":47687,\"start\":47677},{\"end\":47697,\"start\":47687},{\"end\":47712,\"start\":47697},{\"end\":48175,\"start\":48165},{\"end\":48185,\"start\":48175},{\"end\":48199,\"start\":48185},{\"end\":48424,\"start\":48408},{\"end\":48438,\"start\":48424},{\"end\":48451,\"start\":48438},{\"end\":48468,\"start\":48451},{\"end\":48481,\"start\":48468},{\"end\":48496,\"start\":48481},{\"end\":48511,\"start\":48496},{\"end\":48529,\"start\":48511},{\"end\":48910,\"start\":48897},{\"end\":48920,\"start\":48910},{\"end\":48933,\"start\":48920},{\"end\":48948,\"start\":48933},{\"end\":48961,\"start\":48948},{\"end\":49338,\"start\":49328},{\"end\":49352,\"start\":49338},{\"end\":49364,\"start\":49352},{\"end\":49377,\"start\":49364},{\"end\":49387,\"start\":49377},{\"end\":49613,\"start\":49603},{\"end\":49633,\"start\":49613},{\"end\":49650,\"start\":49633},{\"end\":49662,\"start\":49650},{\"end\":49677,\"start\":49662},{\"end\":49689,\"start\":49677},{\"end\":49904,\"start\":49894},{\"end\":49916,\"start\":49904},{\"end\":49928,\"start\":49916},{\"end\":50311,\"start\":50298},{\"end\":50321,\"start\":50311},{\"end\":50331,\"start\":50321},{\"end\":50339,\"start\":50331},{\"end\":50351,\"start\":50339},{\"end\":50364,\"start\":50351},{\"end\":50697,\"start\":50684},{\"end\":50710,\"start\":50697},{\"end\":50730,\"start\":50710},{\"end\":50986,\"start\":50974},{\"end\":50997,\"start\":50986},{\"end\":51007,\"start\":50997},{\"end\":51015,\"start\":51007},{\"end\":51030,\"start\":51015},{\"end\":51042,\"start\":51030}]", "bib_venue": "[{\"end\":32435,\"start\":32358},{\"end\":32994,\"start\":32932},{\"end\":33432,\"start\":33355},{\"end\":33925,\"start\":33887},{\"end\":34397,\"start\":34320},{\"end\":34789,\"start\":34719},{\"end\":35295,\"start\":35214},{\"end\":35931,\"start\":35835},{\"end\":36440,\"start\":36363},{\"end\":36886,\"start\":36819},{\"end\":37359,\"start\":37278},{\"end\":37830,\"start\":37763},{\"end\":38246,\"start\":38169},{\"end\":38693,\"start\":38616},{\"end\":39210,\"start\":39107},{\"end\":39587,\"start\":39583},{\"end\":39865,\"start\":39784},{\"end\":40295,\"start\":40231},{\"end\":40733,\"start\":40652},{\"end\":41250,\"start\":41169},{\"end\":41751,\"start\":41713},{\"end\":42204,\"start\":42127},{\"end\":42739,\"start\":42661},{\"end\":43324,\"start\":43243},{\"end\":43724,\"start\":43670},{\"end\":44237,\"start\":44156},{\"end\":44755,\"start\":44706},{\"end\":45341,\"start\":45303},{\"end\":45700,\"start\":45629},{\"end\":46249,\"start\":46185},{\"end\":46811,\"start\":46726},{\"end\":47279,\"start\":47213},{\"end\":47793,\"start\":47712},{\"end\":48163,\"start\":48105},{\"end\":48578,\"start\":48529},{\"end\":48999,\"start\":48961},{\"end\":49326,\"start\":49245},{\"end\":49992,\"start\":49928},{\"end\":50425,\"start\":50364},{\"end\":51115,\"start\":51042},{\"end\":32499,\"start\":32437},{\"end\":33496,\"start\":33434},{\"end\":34461,\"start\":34399},{\"end\":35363,\"start\":35297},{\"end\":36504,\"start\":36442},{\"end\":36940,\"start\":36888},{\"end\":37427,\"start\":37361},{\"end\":37884,\"start\":37832},{\"end\":38310,\"start\":38248},{\"end\":38757,\"start\":38695},{\"end\":39933,\"start\":39867},{\"end\":40346,\"start\":40297},{\"end\":40801,\"start\":40735},{\"end\":41318,\"start\":41252},{\"end\":42268,\"start\":42206},{\"end\":42804,\"start\":42741},{\"end\":43392,\"start\":43326},{\"end\":44305,\"start\":44239},{\"end\":45758,\"start\":45702},{\"end\":46300,\"start\":46251},{\"end\":47332,\"start\":47281},{\"end\":47861,\"start\":47795},{\"end\":50043,\"start\":49994},{\"end\":50473,\"start\":50427}]"}}}, "year": 2023, "month": 12, "day": 17}