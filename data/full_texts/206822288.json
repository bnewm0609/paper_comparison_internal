{"id": 206822288, "updated": "2023-03-25 14:06:36.924", "metadata": {"title": "Recipe recognition with large multimodal food dataset", "authors": "[{\"first\":\"Xin\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Devinder\",\"last\":\"Kumar\",\"middle\":[]},{\"first\":\"Nicolas\",\"last\":\"Thome\",\"middle\":[]},{\"first\":\"Matthieu\",\"last\":\"Cord\",\"middle\":[]},{\"first\":\"Frederic\",\"last\":\"Precioso\",\"middle\":[]}]", "venue": "2015 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)", "journal": "2015 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)", "publication_date": {"year": 2015, "month": null, "day": null}, "abstract": "This paper deals with automatic systems for image recipe recognition. For this purpose, we compare and evaluate leading vision-based and text-based technologies on a new very large multimodal dataset (UPMC Food-101) containing about 100,000 recipes for a total of 101 food categories. Each item in this dataset is represented by one image plus textual information. We present deep experiments of recipe recognition on our dataset using visual, textual information and fusion. Additionally, we present experiments with text-based embedding technology to represent any food word in a semantical continuous space. We also compare our dataset features with a twin dataset provided by ETHZ university: we revisit their data collection protocols and carry out transfer learning schemes to highlight similarities and differences between both datasets. Finally, we propose a real application for daily users to identify recipes. This application is a web search engine that allows any mobile device to send a query image and retrieve the most relevant recipes in our dataset.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": null, "mag": "2296448531", "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/icmcs/WangKTCP15", "doi": "10.1109/icmew.2015.7169757"}}, "content": {"source": {"pdf_hash": "770e5096a33556df0af35911f36add9ea926e253", "pdf_src": "MergedPDFExtraction", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://hal.archives-ouvertes.fr/hal-01196959/file/CEA_ICME2015.pdf", "status": "GREEN"}}, "grobid": {"id": "5b07451295d9192158185adb209298d5e7ae08fe", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/770e5096a33556df0af35911f36add9ea926e253.txt", "contents": "\nRECIPE RECOGNITION WITH LARGE MULTIMODAL FOOD DATASET\n\n\nXin Wang \nUMR 7606\nSorbonne Universit\u00e9s\nUPMC Univ Paris 06\nLIP6, F-75005ParisFrance (\n\n) Devinder Kumar \nVision and Image Processing (VIP) Lab\nUniversity of Waterloo\nOntarioCanada (\n\nNicolas Thome \nUMR 7606\nSorbonne Universit\u00e9s\nUPMC Univ Paris 06\nLIP6, F-75005ParisFrance (\n\nMatthieu Cord \nUMR 7606\nSorbonne Universit\u00e9s\nUPMC Univ Paris 06\nLIP6, F-75005ParisFrance (\n\nFr\u00e9d\u00e9ric Precioso \nUMR 7271\nUniversit\u00e9s Nice Sophia Antipolis\nI3S, F-06900Sophia AntipolisFrance\n\nRECIPE RECOGNITION WITH LARGE MULTIMODAL FOOD DATASET\n\nThis paper deals with automatic systems for image recipe recognition. For this purpose, we compare and evaluate leading vision-based and text-based technologies on a new very large multimodal dataset (UPMC Food-101) containing about 100,000 recipes for a total of 101 food categories. Each item in this dataset is represented by one image plus textual information. We present deep experiments of recipe recognition on our dataset using visual, textual information and fusion. Additionally, we present experiments with text-based embedding technology to represent any food word in a semantical continuous space. We also compare our dataset features with a twin dataset provided by ETHZ university: we revisit their data collection protocols and carry out transfer learning schemes to highlight similarities and differences between both datasets.Finally, we propose a real application for daily users to identify recipes. This application is a web search engine that allows any mobile device to send a query image and retrieve the most relevant recipes in our dataset.\n\nINTRODUCTION\n\nFood category classification is a key technology for many food-related applications such as monitoring healthy diet, computational cooking, food recommendation system, etc. In [1], a novel smart phone application to record daily meal activities by image retrieval technique is developed. Based on this personal dietary data log system, they were able to conduct further usage preference experiments [2] and food nutrition balance estimation [3].\n\nOpen Food System 2 aims at inventing new smart cooking appliances, with the ability to monitor cooking settings automatically for optimal results and preserve the nutritional value and organoleptic qualities of cooked foods. The Technology Assisted Dietary Assessment (TADA) project of Purdue University [4] aims at developing a mobile food recorder which can translate dietary information to an accurate account of daily food and nutrient intake. Food category classification is an indispensable ingredient in all these applications.\n\nIn this paper, we focus on building automatic systems for image recipe recognition. For this purpose, we propose a new very large multimodal dataset (UPMC Food-101) containing about 100,000 recipes for a total of 101 food categories collected from the web. Each item in this dataset is represented by one image and the HTML information including metadata, content etc. of the seed page from which the image originated. We detail our initiative to build our dataset in sections 2 and 3 explaining the specificities and the originality of our dataset. We perform experiments at a large scale to evaluate visual and textual features along with their fusion in section 4. We propose in section 5, further statistics to highlight dataset characteristics and comparison with another recent large scale dataset (ETHZ Food-101 [5]). Finally, in section 6, we demonstrate the interest of these recognition technologies coupled with web-based dataset in a mobile search application, which can receive food image as a query and return the most relevant classes and corresponding recipes.\n\n\nRELATED WORKS ON FOOD DATASETS\n\nThere is an increasing demand of food category data in various food-related applications like dietary assessment, computational cooking, recipe retrieval, etc. However, specific public massive dataset for food research community is still insufficient. One of them is the Pittsburgh Food Image Dataset (PFID) [6] dataset of 4556 fast food images. Another one is UNICT-FD889 dataset [7] that has 889 distinct plates of food. The authors use this database for Near Duplicate Image retrieval (NDIR) by using three different state-of-the-art image descriptors. There are a couple of databases from the Max-Planck Institute for Informatics that contain images of cooking activities which focus on detecting fine grained activities while cooking [8]. UEC-Food100 [9] contains 100 categories of food images, each category contains about 100 images, mainly Japanese food categories. [10] performs a late fusion of deep convolutional features and conventional handcrafted image features upon dataset UEC-Food100 [9], which outperforms the best classification accuracy on this dataset. Most of the datasets are either collected in a controlled environment or contain too few samples for each food category to build a generic food recognizer or classifier.\n\nIn this paper, we propose a new very large multimodal dataset henceforth named as UPMC Food-101, which is collected in uncontrolled environment with a huge diversity among the instances. UPMC Food-101 contains about 100,000 images and textual descriptions for 101 food categories. This dataset aims at stimulating the research of food category recognition in the domain of multimedia and it will be released freely to the research community. In addition to the large number of images, an extra property of our dataset is that it shares the same food categories with one of the largest public food image dataset ETHZ Food-101 [5]. Such a \"twins dataset pair\" can thus enable many interesting research hot spots such as transfer learning. To the best of our knowledge, UPMC Food-101 and ETHZ Food-101 are the first \"twins dataset pair\" in food area. We explain the similarities and differences between both datasets in detail in the following sections.\n\n\nUPMC FOOD-101 DATASET\n\n\nData Collection Protocol\n\nTo create a real world and challenging dataset (with both visual and textual data) which can truly represent the existing large intra-class variations among food categories, we decide to use Google Image search. Unlike controlled sources, using long ranking (one thousand results) of Web search engine allows to explore recipes that are potentially deeply buried in the world wide web. Similarly, Y. Kawano and K. Yanai explore the Web resources in [11], to extend their initial UEC Food-100 dataset [9]. It is also interesting to note that the past approaches [12] using Google search engine to obtain images for classification tasks have reported around 30 percent of precision level on some of collected images (in 2006). We observe that the results returned by Google Image search in 2014 for textual queries related to food images are more relevant with very low level of noise. This is explained by the large improvement in the field of searching and page ranking algorithms since 2006. Based on these preliminary findings, we decide to create our database by querying Google image search with 101 labels taken from the ETHZ Food-101 dataset [5] along with an added word \"recipes\". We added the word \"recipes\" to each label before passing the query to Google for two reasons:\n\n\u2022 As we are interested in recipe recognition, adding \"recipes\" word after the labels, for example, \"hamburger recipe\", returns more focused information about \"how to make hamburgers\" rather than other topics like \"where to eat hamburgers\" or \"Hamburger is junk food\" in the textual form.\n\n\u2022 We observed that adding \"recipes\" to our queries helps decreasing the noise level a little further in the returned images. For example, a simple \"hamburger\" in search engine could return some thing like \"hamburger menu icon\" or \"hamburger-like evening dress\" which are far from our expectations.\n\nWe then collect the first 1, 000 images returned for each query and remove any image with a size smaller than 120 pixels. In total, UPMC Food-101 contains 101 food categories and 90, 840 images, with a size range between 790 and 956 images for different classes. Figure 1 shows representative instances of all 100 categories. Due to no human intervention in grasping these data, we estimate that each category may contain about 5% irrelevant images for each category. 3 examples of \"hamburger\" class are shown in Figure 2. We notice that adding the keyword \"recipes\" results in taking into account ingredient or intermediate food images. Determining whether these images should be considered as noise or not, directly depends on the specific application. Additionally, we save 93, 533 raw HTML source pages which embed images. The reason that we don't have 101, 000 HTML pages is that some pages are not available. The number of the images that have text is 86, 574.    Figure 2c, as well as hamburger ingredient like Figure 2b, which depends on the specific application to judge whether it is noise or not. \n\n\nComparison with ETHZ Food-101\n\nThe food dataset ETHZ Food-101 [5] has been recently introduced. 101,000 images for 101 food categories have been collected from a specific website (e.g. www.foodspotting.com). The labels of food categories were chosen from the top 101 most popular dishes on the mentioned website. We have used the same class labels as ETHZ Food-101 for our dataset. In Table 1, general statistics on both sets are reported. The main difference comes from the data collection protocols. Since our data is collected directly from a search engine with automatic annotations, whereas ETHZ Food-101 dataset images were collected from a specific website which contains manual annotated images uploaded by humans, leading to less number of false positive/noise in ETHZ Food-101 than in UPMC Food-101. As the three examples of \"hamburger\" class show in Figure 3, ETHZ Food-101 ensures images irrelevant with food categories are mostly excluded from this dataset. Moreover, there was no textual data provided with images in ETHZ Food-101. However, to classify between two variants of the same food categories, text can help a lot. We explore visual and text classification in the next section.\n\n\nCLASSIFICATION RESULTS\n\nIn the following subsections we run several classification algorithms by using visual information, textual information and the fusion, to make quantitative descriptions of our dataset. The results are shown in Table 2. A unified training and test protocol is applied for both visual and textual tests, in order to evaluate and compare the performances with minimal extra factors. The protocol is as follows: we split out the examples which have both image and text, then randomly select 600 training examples for each category to train a one-vs-rest linear SVM [13] with C = 100, the remaining examples are for test. We evaluate our results by averaging accuracy over 10 tests, where accuracy is defined as #(true positives) #(test examples) .\n\n\nVisual Features Experiments\n\n\nBag-of-Words Histogram (BoW) + SIFT\n\nWe represent images as Bag-of-Words histogram with a spatial pyramid as our first baseline. In detail, we first proportionally resize images which has a size larger than 300 pixels, then extract mono-scale SIFT with window size 4 and step size 8, 1024 word visual dictionary, soft coding and max pooling with 3 level spatial information. This baseline obtains an average accuracy 23.96%.\n\n\nBossanova Image Pooling Representation\n\nBossanova [14] reinforces the pooling stage of BoW by considering distance between a word and a given center of a cluster. As Bossanova only modifies the pooling stage, we can reuse the same coding setting as BoW. In our experiment, 2 bins are used in the quantization step to encode the distances from sifts to clusters, BoW is concatenated with vector histogram with no scaling factor, we set range of distances per cluster to [0.4, 2.0], for each word we consider 10 neighbors. This method results in an average accuracy of 28.59%, which constitutes an improvement of 19.37% over the BoW model.\n\n\nConvolutional Neural Networks (CNN) Deep Features\n\nCNN deep feature is the state of the art in many image recognition challenges. Deep feature contains color information, nevertheless, its dimensionality is often much lower than the traditional SIFT descriptor but with much better performance. In our experiment, we first adopt the \"fast network\" pretrained model of OverFeat 3 as the feature extractor. The output of the 7th stage of this model, a 4096 dimension vector, is used as the feature description of a given image. We get an average accuracy of 33.91%, which gains an relative improvement of 18.6% with respect to the Bossanova. This result is very interesting because the OverFeat CNN was trained on 1,000 class dataset ImageNet, which contains very few images of food categories (French fries, few images of waffles etc). Even after having been trained on few food images, the OverFeat CNN produces very good deep features  which outperform the standard Bossanova baseline in the context of classification. [15] pushes CNN network to 16\u221219 weight layers, which is about twice deeper than the previous work. In our experiment, we use the pre-trained model \"imagenet-vgg-verydeep-19\" 4 to extract features. This model is also trained on Ima-geNet so it is comparable with the result in the last paragraph. We take the output of the last layer before the classification layer as image features. Each feature is a 4096 dimensions vector. We finally achieve an accuracy of 40.21% over our dataset with very deep features.\n\n\nTextual Features Experiment\n\nSince our raw textual data is in html format, we need some preprocessing in order to remove numerous noisy elements such as html tags, code, punctuations. Our foremost preprocessing is parsing content out from HTML pages by Python package html2text 5 .\n\n\nTF-IDF\n\nTF-IDF (Term Frequency-Inverse Document Frequency) value measures the importance of a word w in a document D with respect to the whole corpus, where TF evaluates the importance of word in a document, and IDF evaluates the importance of a word in the corpus.\n\nTo represent a document with TF-IDF, we generate the dictionary by preprocessing words as follows: 1/ Stemming all words. For example, words like \"dogs\" and \"sleeping\" are respectively stemmed to \"dog\" and \"work\", 2/ Removing words with high frequency of occurrence (stop words) such as \"the\",\"is\",\"in\", 3/ Removing words occurred less than in 11 docs, 4/ Keeping stems with length between 6 and 18. After the pre-processing, 46972 words are left. We then form a dictionary Dict t using these words.\n\nWe calculate TF-IDF value for every word in document by formula tf idf w,D = tf w,D \u00d7 idf w , with tf w,D = n w,D k n k,D , where n i,j is the frequency of word i appearing in document j, and idf w = log |N | |{j:w\u2208Dj }| , where N is the total number of documents in the corpus, and |{j : w \u2208 D j }| is the number of documents where the term w appears. TF-IDF value favors the words less occurred in corpus and more occurred in a given document D, and suppress the word in reverse case. A document can be represented by the TF-IDF value of all its words belonging to the dictionary Dict t . We obtain 82.06% classification average accuracy on our dataset. Such a high score is partly due to the bias introduced by our data crawling protocol.\n\n\nLate Fusion of Image+Text\n\nWe merge very deep features and TF-IDF classification scores by late fusion. The fusion score s f is a linear combination of the scores provided by both image and text classification systems, as s f = \u03b1s i +(1\u2212\u03b1)s t , where \u03b1 is the fusion parameter in the range [0, 1], s i is the score from the image classifier and s t is the score from the text. We select \u03b1 by cross-validation over different splits of data and the final classification score is 85.1%, which improves 3.6% with respect to textual information alone and 109.8% with respect to visual information alone. Note that the classification scores were not calibrated prior to late fusion so that \u03b1 does not depend on the relative accuracy of each source of scores.\n\n\nQUANTITATIVE ANALYSIS OF UPMC FOOD-101\n\nIn this section, we report further analysis of UPMC Food-101. We investigate the word vector representations [16] for its strong semantic expressiveness. Transfer learning between UPMC Food-101 and ETHZ Food-101 is also analyzed.\n\n\nWord Vector Representation\n\nWe first introduce how to extract word vectors, then explore some interesting features of this representation.\n\nAfter parsing out the content of web pages, we concatenate all of them together to build a corpus for training a dictionary Dict v with word2vec [16], which is a tool to efficiently compute vector representations of words. Words with an occurrence frequency less than 5 in the corpus are removed from Dict v . This condition results in 137092 words, in which each word is described by a 200 dimensional feature vector. Dict v contains stop words and other noisy words, so we intersect Dict t and Dict v , which creates a new dictionary Dict containing 46773 words.\n\nOn the other hand, each document is first preprocessed by the tool html2text, then represented by the element-wise average of its valid word vectors, where \"valid\" means that the word is in Dict. A linear SVM is trained and we obtain an average accuracy of 67.21% on our dataset. Although this classification result is worse than TF-IDF (82.06%), it can be enhanced by more advanced pooling strategies, rather than TF-IDF word2vec TF-IDF+word2vec 82.06% 67.21% 84.19% Table 3: Late fusion of TF-IDF and average word2vec representations.\n\na simple average vector over all words, as reported in [17]. Additionally, recall that our data source is the Google search results according to a category name: this step can also reinforce the superiority for word frequency based methods like TF-IDF. On the other hand, since the word vector tries to learn a semantic representation of words with much less dimension, the simple word frequency statistical information will surely lose a lot. However, by late fusion with TF-IDF, we get the score of 84.19%, improving by 2% the single TF-IDF performance, as shown in Table 3. TF-IDF and word2vect encode complementary information in textual data. The embedded word vector space allows to explore semantic relationships. To investigate this aspect, we report in Table 4 the closest words by using the cosine distance metric for -ravioli, -sushi, -pho in the embedded vector space (using the Dict v dataset). The five most closest words are strongly semantically related to the given query. Additionally, calculating a simple average of the words in a phrase also results in a reasonable semantic. In Table 5, we show the closest words of -rice, -japan and -rice japan. As we can see, -koshihikari, which is a popular variety of rice cultivated in Japan, is closest to -rice japan, meanwhile for either -rice or -japan, -koshihikari-is out of their first five candidates, which means word vector has well expressed the semantic of the short phrase -rice+japan. Moreover, -koshihikari is not among the 101 food category, its meaning and relation with other words are all learned from the corpus in a purely unsupervised manner. Such a powerful semantic understanding property could help search engine understand user-level needs with natural language as input. It is a promising tool for filling the semantic gap.   There are some interesting points that can be inferred from the results. The first one is that even though both datasets contain images for same food categories, they are very different from each other. This can be derived from the fact that there is a considerable difference of around 50% average accuracy when training on one dataset and testing on both datasets (first 2 rows in Table 6).\n\nSecond point that can be observed from the Table 6   Note that our ETHZ deep results are not comparable with the CNN results in [5] because they train deep features as we use a pre-trained CNN on ImageNet.\n\n\nMOBILE RECIPE RETRIEVAL\n\nProviding an efficient way to automatically recognize the food/dish or its recipes on our plates will not only satisfy our curiosity but can have a wider impact on daily life in both the real and virtual worlds. \"What is the name of this dish?\", \"How to cook this?\". We all have asked these questions to a chef or friends. As a proof of concept, we have created a web search engine 5 that allows any mobile device to send a query image and to get answers to our questions. For any query image, the result is a ranking of the 3 best categories automatically found with a matching score that may at least indicate if the match is correct (positive) or not (negative score). For each selected category, images related to the query are displayed with the hyperlink to the recipe webpage available. Figure 4 presents the answer to a query image (representing a pizza) displayed at the top of the page. The categories predicted with the highest scores are returned on the next three lines, followed by seven clickable images (by category) linked to the original recipe webpages. In this case, only the correct result -pizza gets a positive score (top ranking). \n\n\nCONCLUSION\n\nIn this paper, we introduce a large multimedia dataset with 101 food categories. We present an extended evaluation of BoVW, Bossanova and deep features for food image recognition, as well as TF-IDF for document classification. Our experiments suggest that for visual recognition, CNN deep feature is the best step forward. Due to the manner of collecting data, a strong bias makes bag-of-textual-words perform better than any other single method. Nevertheless, the fusion of visual and textual information achieves better average precision 85.1%. Additionally, we find that word vector shows powerful ability in representing any word in a semantical food continuous space. We also run complementary experiments to highlight differences and complementarity of our UPMC Food-101 dataset with the recently published ETHZ Food-101 dataset. Based on our dataset, we have proposed a re- 5 Available at http://visiir.lip6.fr/.\n\ntrieval system that we plan to improve using machine learning techniques [18,19,20] for user interaction.\n\nFig. 1 :\n1Category examples of our UPMC Food-101 dataset.\n\nFig. 3 :\n3Example images within class \"hamburger\" of ETHZ Food-101. All these images have strong selfie style as they are uploaded by consumers. Although some background noise (human faces, hands) are introduced in images, it ensures images out of food categories are excluded from this dataset.\n\nFig. 4 :\n4Results for a pizza image\n\nTable 1 :\n1UPMC Food-101 and ETHZ Food-101 statistics(a) Correct \n\n(b) Ingredient \n(c) Noise \n\nFig. 2: Example images within class \"hamburger\" of UPMC \nFood-101. Note that we have images completely irrelevant \nwith hamburger like \n\nTable 2 :\n2Classification results (Ave. Precision %) on UPMC Food-101 for Visual, Textual and Combined features.\n\nTable 4 :\n45 most similar words of -ravioli, -sushi and -pho. \nEach group is indeed semantically relevant, except for some \nwords with low scores like -delallocom and -itemtitlea. \n\n5.2. Transfer Learning \n\nAs another set of experiments, we perform knowledge trans-\nferring experiments over both datasets (ETHZ Food-101 and \n\n\n\nTable 5 :\n5Short In this experiment, we use very deep features. The results of the transfer learning experiments are shown in Table 6. The first two rows show the results of classification when training with the same number of examples (e.g. 600 examples for each class) of one dataset and testing on the rest of this dataset or on the whole of the other dataset, while the last two rows show the results of classification when training with all examples on one dataset and testing on the other dataset.phrase -rice japan, represented as the average \nof -rice and -japan, is closest to -koshihikari. \n\nUPMC Food-101), namely learning the classifier model on \none dataset and testing it on the other one. This experiment \naims at showing the different performances of UPMC Food-\n101 and ETHZ Food-101 when performing visual classifica-\ntion. \n\n\nis that training on part of UPMC Food-101 outperforms training on the whole UPMC Food-101 when testing on ETHZ Food-101 by a margin of 1.57%, while on the contrary, only a negligible difference (0.36%) for training on ETHZ Food-101 and testing on UPMC Food-101 is observed. This perhaps can be an indication of comparative noise levels in both datasets, UPMC Food-101 being the noisier dataset.train / test \nUPMC ETHZ \nUPMC (600 examples) 40.56 \n25.63 \nETHZ (600 examples) \n25.28 \n42.54 \nUPMC (all examples) \n-\n24.06 \nETHZ (all examples) \n24.92 \n-\n\n\n\nTable 6 :\n6Results of transfer learning between UPMC Food-101 and ETHZ Food-101.\nhttp://www.futur-en-seine.fr/fens2014/en/ projet/open-food-system-2/\nhttp://cilvr.nyu.edu/doku.php?id=software: overfeat:start\nhttp://www.vlfeat.org/matconvnet/pretrained/ 5 https://pypi.python.org/pypi/html2text\n\nMultimedia foodlog: Diverse applications from self-monitoring to social contributions. K Aizawa, ITE Transactions on Media Technology and Applications. K. Aizawa, \"Multimedia foodlog: Diverse applications from self-monitoring to social contributions,\" ITE Transactions on Media Technology and Applications, 2013.\n\nComparative Study of the Routine Daily Usability of FoodLog: A Smartphone-based Food Recording Tool Assisted by Image Retrieval. K Aizawa, Journal of diabetes science and technology. K. Aizawa and et al., \"Comparative Study of the Routine Daily Usability of FoodLog: A Smartphone-based Food Recording Tool Assisted by Image Retrieval,\" Journal of diabetes science and technology, 2014.\n\nFood balance estimation by using personal dietary tendencies in a multimedia food log. K Aizawa, Y Maruyama, H Li, C Morikawa, IEEE Transactions on Multimedia. K. Aizawa, Y. Maruyama, H. Li, and C. Morikawa, \"Food bal- ance estimation by using personal dietary tendencies in a mul- timedia food log.,\" IEEE Transactions on Multimedia, 2013.\n\nAn overview of the technology assisted dietary assessment project at purdue university.,\" in ISM. N Khanna, N. Khanna and et al., \"An overview of the technology assisted dietary assessment project at purdue university.,\" in ISM, 2010.\n\nFood-101 Mining Discriminative Components with Random Forests. L Bossard, ECCV. L. Bossard and et al., \"Food-101 Mining Discriminative Com- ponents with Random Forests,\" in ECCV, 2014.\n\nPFID: Pittsburgh fast-food image dataset. M Chen, ICIP. M Chen and et al., \"PFID: Pittsburgh fast-food image dataset,\" in ICIP, 2009.\n\nA Benchmark Dataset to Study Representation of Food Images. G M Farinella, ACVRGM Farinella and et all, \"A Benchmark Dataset to Study Rep- resentation of Food Images,\" in ACVR, 2014.\n\nA database for fine grained activity detection of cooking activities. Marcus Rohrbach, Amin, CVPR. Marcus Rohrbach and S Amin, \"A database for fine grained activity detection of cooking activities,\" in CVPR, 2012.\n\nFoodCam: A Real-Time Mobile Food Recognition System Employing Fisher Vector. Y Kawano, K Yanai, MMM. Y. Kawano and K. Yanai, \"FoodCam: A Real-Time Mobile Food Recognition System Employing Fisher Vector,\" in MMM, 2014.\n\nFood image recognition with deep convolutional features. Yoshiyuki Kawano, Keiji Yanai, ACM UbiComp. Yoshiyuki Kawano and Keiji Yanai, \"Food image recognition with deep convolutional features,\" in ACM UbiComp, 2014.\n\nAutomatic expansion of a food image dataset leveraging existing categories with domain adaptation. Y Kawano, K Yanai, Proc. of ECCV Workshop on TASK-CV. of ECCV Workshop on TASK-CVY. Kawano and K. Yanai, \"Automatic expansion of a food im- age dataset leveraging existing categories with domain adapta- tion,\" in Proc. of ECCV Workshop on TASK-CV, 2014.\n\nHarvesting image databases from the web. A Schroff, F Criminisi, A Zisserman, PAMIA. Schroff, F. and Criminisi, A. and Zisserman, \"Harvesting image databases from the web,\" PAMI, 2011.\n\nLIBLINEAR: A library for large linear classification. R Fan, JMLR. R. Fan and et al., \"LIBLINEAR: A library for large linear classification,\" JMLR, 2008.\n\nPooling in image representation: The visual codeword point of view. S Avila, CVIU. S. Avila and et al., \"Pooling in image representation: The vi- sual codeword point of view,\" CVIU, 2013.\n\nVery deep convolutional networks for large-scale image recognition. K Simonyan, Zisserman, CoRRK Simonyan and A Zisserman, \"Very deep convolutional net- works for large-scale image recognition,\" CoRR, 2014.\n\nDistributed representations of words and phrases and their compositionality. T Mikolov, NIPS. T. Mikolov and et all, \"Distributed representations of words and phrases and their compositionality,\" in NIPS, 2013.\n\nDistributed Representations of Sentences and Documents. Quoc Le, Tomas Mikolov, ICML. Quoc Le and Tomas Mikolov, \"Distributed Representations of Sentences and Documents,\" in ICML, 2014.\n\nSalsas: Sub-linear active learning strategy with approximate k-nn search. D Gorisse, M Cord, F Precioso, Pattern Recognition. 4410D. Gorisse, M. Cord, and F. Precioso, \"Salsas: Sub-linear ac- tive learning strategy with approximate k-nn search,\" Pattern Recognition, vol. 44, no. 10, pp. 2343-2357, 2011.\n\nActive learning methods for interactive image retrieval. P H Gosselin, M Cord, IEEE Transactions on. 177Image ProcessingPH Gosselin and M Cord, \"Active learning methods for inter- active image retrieval,\" Image Processing, IEEE Transactions on, vol. 17, no. 7, pp. 1200-1211, 2008.\n\nImage retrieval over networks: Active learning using ant algorithm. D Picard, M Cord, A Revel, IEEE Transactions on. 107MultimediaD. Picard, M. Cord, and A. Revel, \"Image retrieval over net- works: Active learning using ant algorithm,\" Multimedia, IEEE Transactions on, vol. 10, no. 7, pp. 1356-1365, 2008.\n", "annotations": {"author": "[{\"end\":143,\"start\":57},{\"end\":239,\"start\":144},{\"end\":331,\"start\":240},{\"end\":423,\"start\":332},{\"end\":521,\"start\":424}]", "publisher": null, "author_last_name": "[{\"end\":65,\"start\":61},{\"end\":160,\"start\":146},{\"end\":253,\"start\":248},{\"end\":345,\"start\":341},{\"end\":441,\"start\":433}]", "author_first_name": "[{\"end\":60,\"start\":57},{\"end\":145,\"start\":144},{\"end\":247,\"start\":240},{\"end\":340,\"start\":332},{\"end\":432,\"start\":424}]", "author_affiliation": "[{\"end\":142,\"start\":67},{\"end\":238,\"start\":162},{\"end\":330,\"start\":255},{\"end\":422,\"start\":347},{\"end\":520,\"start\":443}]", "title": "[{\"end\":54,\"start\":1},{\"end\":575,\"start\":522}]", "venue": null, "abstract": "[{\"end\":1643,\"start\":577}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1838,\"start\":1835},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":2061,\"start\":2058},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":2103,\"start\":2100},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2413,\"start\":2410},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3464,\"start\":3461},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4064,\"start\":4061},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":4137,\"start\":4134},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":4495,\"start\":4492},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4512,\"start\":4509},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":4631,\"start\":4627},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":4758,\"start\":4755},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":5627,\"start\":5624},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6455,\"start\":6451},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6505,\"start\":6502},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6567,\"start\":6563},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":7153,\"start\":7150},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":9049,\"start\":9046},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":10776,\"start\":10772},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11468,\"start\":11464},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":13078,\"start\":13074},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16290,\"start\":16286},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":16698,\"start\":16694},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":17712,\"start\":17708},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":19992,\"start\":19989},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":22146,\"start\":22145},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":22262,\"start\":22258},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":22265,\"start\":22262},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":22268,\"start\":22265}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":22349,\"start\":22291},{\"attributes\":{\"id\":\"fig_1\"},\"end\":22646,\"start\":22350},{\"attributes\":{\"id\":\"fig_2\"},\"end\":22683,\"start\":22647},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":22915,\"start\":22684},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":23029,\"start\":22916},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":23357,\"start\":23030},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":24200,\"start\":23358},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":24752,\"start\":24201},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":24834,\"start\":24753}]", "paragraph": "[{\"end\":2104,\"start\":1659},{\"end\":2640,\"start\":2106},{\"end\":3718,\"start\":2642},{\"end\":4997,\"start\":3753},{\"end\":5949,\"start\":4999},{\"end\":7283,\"start\":6002},{\"end\":7572,\"start\":7285},{\"end\":7871,\"start\":7574},{\"end\":8981,\"start\":7873},{\"end\":10184,\"start\":9015},{\"end\":10954,\"start\":10211},{\"end\":11411,\"start\":11024},{\"end\":12051,\"start\":11454},{\"end\":13583,\"start\":12105},{\"end\":13867,\"start\":13615},{\"end\":14135,\"start\":13878},{\"end\":14636,\"start\":14137},{\"end\":15379,\"start\":14638},{\"end\":16134,\"start\":15409},{\"end\":16406,\"start\":16177},{\"end\":16547,\"start\":16437},{\"end\":17113,\"start\":16549},{\"end\":17651,\"start\":17115},{\"end\":19859,\"start\":17653},{\"end\":20066,\"start\":19861},{\"end\":21249,\"start\":20094},{\"end\":22183,\"start\":21264},{\"end\":22290,\"start\":22185}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":9376,\"start\":9369},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":10428,\"start\":10421},{\"end\":17590,\"start\":17583},{\"end\":18228,\"start\":18221},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":18422,\"start\":18415},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":18760,\"start\":18753},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":19857,\"start\":19850},{\"attributes\":{\"ref_id\":\"tab_8\"},\"end\":19911,\"start\":19904}]", "section_header": "[{\"attributes\":{\"n\":\"1.\"},\"end\":1657,\"start\":1645},{\"attributes\":{\"n\":\"2.\"},\"end\":3751,\"start\":3721},{\"attributes\":{\"n\":\"3.\"},\"end\":5973,\"start\":5952},{\"attributes\":{\"n\":\"3.1.\"},\"end\":6000,\"start\":5976},{\"attributes\":{\"n\":\"3.2.\"},\"end\":9013,\"start\":8984},{\"attributes\":{\"n\":\"4.\"},\"end\":10209,\"start\":10187},{\"attributes\":{\"n\":\"4.1.\"},\"end\":10984,\"start\":10957},{\"attributes\":{\"n\":\"4.1.1.\"},\"end\":11022,\"start\":10987},{\"attributes\":{\"n\":\"4.1.2.\"},\"end\":11452,\"start\":11414},{\"attributes\":{\"n\":\"4.1.3.\"},\"end\":12103,\"start\":12054},{\"attributes\":{\"n\":\"4.2.\"},\"end\":13613,\"start\":13586},{\"attributes\":{\"n\":\"4.2.1.\"},\"end\":13876,\"start\":13870},{\"attributes\":{\"n\":\"4.3.\"},\"end\":15407,\"start\":15382},{\"attributes\":{\"n\":\"5.\"},\"end\":16175,\"start\":16137},{\"attributes\":{\"n\":\"5.1.\"},\"end\":16435,\"start\":16409},{\"attributes\":{\"n\":\"6.\"},\"end\":20092,\"start\":20069},{\"attributes\":{\"n\":\"7.\"},\"end\":21262,\"start\":21252},{\"end\":22300,\"start\":22292},{\"end\":22359,\"start\":22351},{\"end\":22656,\"start\":22648},{\"end\":22694,\"start\":22685},{\"end\":22926,\"start\":22917},{\"end\":23040,\"start\":23031},{\"end\":23368,\"start\":23359},{\"end\":24763,\"start\":24754}]", "table": "[{\"end\":22915,\"start\":22738},{\"end\":23357,\"start\":23042},{\"end\":24200,\"start\":23862},{\"end\":24752,\"start\":24597}]", "figure_caption": "[{\"end\":22349,\"start\":22302},{\"end\":22646,\"start\":22361},{\"end\":22683,\"start\":22658},{\"end\":22738,\"start\":22696},{\"end\":23029,\"start\":22928},{\"end\":23862,\"start\":23370},{\"end\":24597,\"start\":24203},{\"end\":24834,\"start\":24765}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":8144,\"start\":8136},{\"end\":8394,\"start\":8386},{\"end\":8852,\"start\":8843},{\"end\":8900,\"start\":8891},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":9853,\"start\":9845},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":20896,\"start\":20888}]", "bib_author_first_name": "[{\"end\":25137,\"start\":25136},{\"end\":25493,\"start\":25492},{\"end\":25838,\"start\":25837},{\"end\":25848,\"start\":25847},{\"end\":25860,\"start\":25859},{\"end\":25866,\"start\":25865},{\"end\":26191,\"start\":26190},{\"end\":26392,\"start\":26391},{\"end\":26557,\"start\":26556},{\"end\":26710,\"start\":26709},{\"end\":26712,\"start\":26711},{\"end\":26909,\"start\":26903},{\"end\":27126,\"start\":27125},{\"end\":27136,\"start\":27135},{\"end\":27333,\"start\":27324},{\"end\":27347,\"start\":27342},{\"end\":27584,\"start\":27583},{\"end\":27594,\"start\":27593},{\"end\":27880,\"start\":27879},{\"end\":27891,\"start\":27890},{\"end\":27904,\"start\":27903},{\"end\":28079,\"start\":28078},{\"end\":28248,\"start\":28247},{\"end\":28437,\"start\":28436},{\"end\":28654,\"start\":28653},{\"end\":28848,\"start\":28844},{\"end\":28858,\"start\":28853},{\"end\":29050,\"start\":29049},{\"end\":29061,\"start\":29060},{\"end\":29069,\"start\":29068},{\"end\":29339,\"start\":29338},{\"end\":29341,\"start\":29340},{\"end\":29353,\"start\":29352},{\"end\":29633,\"start\":29632},{\"end\":29643,\"start\":29642},{\"end\":29651,\"start\":29650}]", "bib_author_last_name": "[{\"end\":25144,\"start\":25138},{\"end\":25500,\"start\":25494},{\"end\":25845,\"start\":25839},{\"end\":25857,\"start\":25849},{\"end\":25863,\"start\":25861},{\"end\":25875,\"start\":25867},{\"end\":26198,\"start\":26192},{\"end\":26400,\"start\":26393},{\"end\":26562,\"start\":26558},{\"end\":26722,\"start\":26713},{\"end\":26918,\"start\":26910},{\"end\":26924,\"start\":26920},{\"end\":27133,\"start\":27127},{\"end\":27142,\"start\":27137},{\"end\":27340,\"start\":27334},{\"end\":27353,\"start\":27348},{\"end\":27591,\"start\":27585},{\"end\":27600,\"start\":27595},{\"end\":27888,\"start\":27881},{\"end\":27901,\"start\":27892},{\"end\":27914,\"start\":27905},{\"end\":28083,\"start\":28080},{\"end\":28254,\"start\":28249},{\"end\":28446,\"start\":28438},{\"end\":28457,\"start\":28448},{\"end\":28662,\"start\":28655},{\"end\":28851,\"start\":28849},{\"end\":28866,\"start\":28859},{\"end\":29058,\"start\":29051},{\"end\":29066,\"start\":29062},{\"end\":29078,\"start\":29070},{\"end\":29350,\"start\":29342},{\"end\":29358,\"start\":29354},{\"end\":29640,\"start\":29634},{\"end\":29648,\"start\":29644},{\"end\":29657,\"start\":29652}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":62218717},\"end\":25361,\"start\":25049},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":18984747},\"end\":25748,\"start\":25363},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":14337063},\"end\":26090,\"start\":25750},{\"attributes\":{\"id\":\"b3\"},\"end\":26326,\"start\":26092},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":12726540},\"end\":26512,\"start\":26328},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":1548631},\"end\":26647,\"start\":26514},{\"attributes\":{\"id\":\"b6\"},\"end\":26831,\"start\":26649},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":9349950},\"end\":27046,\"start\":26833},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":12478923},\"end\":27265,\"start\":27048},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":15628580},\"end\":27482,\"start\":27267},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":14915460},\"end\":27836,\"start\":27484},{\"attributes\":{\"id\":\"b11\"},\"end\":28022,\"start\":27838},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":3116168},\"end\":28177,\"start\":28024},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":16048921},\"end\":28366,\"start\":28179},{\"attributes\":{\"id\":\"b14\"},\"end\":28574,\"start\":28368},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":16447573},\"end\":28786,\"start\":28576},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":2407601},\"end\":28973,\"start\":28788},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":20213108},\"end\":29279,\"start\":28975},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":114844},\"end\":29562,\"start\":29281},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":1608129},\"end\":29870,\"start\":29564}]", "bib_title": "[{\"end\":25134,\"start\":25049},{\"end\":25490,\"start\":25363},{\"end\":25835,\"start\":25750},{\"end\":26389,\"start\":26328},{\"end\":26554,\"start\":26514},{\"end\":26901,\"start\":26833},{\"end\":27123,\"start\":27048},{\"end\":27322,\"start\":27267},{\"end\":27581,\"start\":27484},{\"end\":28076,\"start\":28024},{\"end\":28245,\"start\":28179},{\"end\":28651,\"start\":28576},{\"end\":28842,\"start\":28788},{\"end\":29047,\"start\":28975},{\"end\":29336,\"start\":29281},{\"end\":29630,\"start\":29564}]", "bib_author": "[{\"end\":25146,\"start\":25136},{\"end\":25502,\"start\":25492},{\"end\":25847,\"start\":25837},{\"end\":25859,\"start\":25847},{\"end\":25865,\"start\":25859},{\"end\":25877,\"start\":25865},{\"end\":26200,\"start\":26190},{\"end\":26402,\"start\":26391},{\"end\":26564,\"start\":26556},{\"end\":26724,\"start\":26709},{\"end\":26920,\"start\":26903},{\"end\":26926,\"start\":26920},{\"end\":27135,\"start\":27125},{\"end\":27144,\"start\":27135},{\"end\":27342,\"start\":27324},{\"end\":27355,\"start\":27342},{\"end\":27593,\"start\":27583},{\"end\":27602,\"start\":27593},{\"end\":27890,\"start\":27879},{\"end\":27903,\"start\":27890},{\"end\":27916,\"start\":27903},{\"end\":28085,\"start\":28078},{\"end\":28256,\"start\":28247},{\"end\":28448,\"start\":28436},{\"end\":28459,\"start\":28448},{\"end\":28664,\"start\":28653},{\"end\":28853,\"start\":28844},{\"end\":28868,\"start\":28853},{\"end\":29060,\"start\":29049},{\"end\":29068,\"start\":29060},{\"end\":29080,\"start\":29068},{\"end\":29352,\"start\":29338},{\"end\":29360,\"start\":29352},{\"end\":29642,\"start\":29632},{\"end\":29650,\"start\":29642},{\"end\":29659,\"start\":29650}]", "bib_venue": "[{\"end\":27664,\"start\":27637},{\"end\":25199,\"start\":25146},{\"end\":25544,\"start\":25502},{\"end\":25908,\"start\":25877},{\"end\":26188,\"start\":26092},{\"end\":26406,\"start\":26402},{\"end\":26568,\"start\":26564},{\"end\":26707,\"start\":26649},{\"end\":26930,\"start\":26926},{\"end\":27147,\"start\":27144},{\"end\":27366,\"start\":27355},{\"end\":27635,\"start\":27602},{\"end\":27877,\"start\":27838},{\"end\":28089,\"start\":28085},{\"end\":28260,\"start\":28256},{\"end\":28434,\"start\":28368},{\"end\":28668,\"start\":28664},{\"end\":28872,\"start\":28868},{\"end\":29099,\"start\":29080},{\"end\":29380,\"start\":29360},{\"end\":29679,\"start\":29659}]"}}}, "year": 2023, "month": 12, "day": 17}