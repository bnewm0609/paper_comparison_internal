{"id": 54457125, "updated": "2023-10-05 10:13:42.28", "metadata": {"title": "A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play", "authors": "[{\"first\":\"David\",\"last\":\"Silver\",\"middle\":[]},{\"first\":\"Thomas\",\"last\":\"Hubert\",\"middle\":[]},{\"first\":\"Julian\",\"last\":\"Schrittwieser\",\"middle\":[]},{\"first\":\"Ioannis\",\"last\":\"Antonoglou\",\"middle\":[]},{\"first\":\"Matthew\",\"last\":\"Lai\",\"middle\":[]},{\"first\":\"Arthur\",\"last\":\"Guez\",\"middle\":[]},{\"first\":\"Marc\",\"last\":\"Lanctot\",\"middle\":[]},{\"first\":\"Laurent\",\"last\":\"Sifre\",\"middle\":[]},{\"first\":\"Dharshan\",\"last\":\"Kumaran\",\"middle\":[]},{\"first\":\"Thore\",\"last\":\"Graepel\",\"middle\":[]},{\"first\":\"Timothy\",\"last\":\"Lillicrap\",\"middle\":[]},{\"first\":\"Karen\",\"last\":\"Simonyan\",\"middle\":[]},{\"first\":\"Demis\",\"last\":\"Hassabis\",\"middle\":[]}]", "venue": "Science", "journal": "Science", "publication_date": {"year": 2018, "month": 12, "day": 7}, "abstract": "One program to rule them all Computers can beat humans at increasingly complex games, including chess and Go. However, these programs are typically constructed for a particular game, exploiting its properties, such as the symmetries of the board on which it is played. Silver et al. developed a program called AlphaZero, which taught itself to play Go, chess, and shogi (a Japanese version of chess) (see the Editorial, and the Perspective by Campbell). AlphaZero managed to beat state-of-the-art programs specializing in these three games. The ability of AlphaZero to adapt to various game rules is a notable step toward achieving a general game-playing system. Science, this issue p. 1140; see also pp. 1087 and 1118 AlphaZero teaches itself to play three different board games and beats state-of-the-art programs in each. The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.", "fields_of_study": "[\"Medicine\"]", "external_ids": {"arxiv": null, "mag": "2902907165", "acl": null, "pubmed": "30523106", "pubmedcentral": null, "dblp": null, "doi": "10.1126/science.aar6404"}}, "content": {"source": {"pdf_hash": "259be4b0b9afb213e30ec75f7fbe4cd651dc7972", "pdf_src": "Highwire", "pdf_uri": null, "oa_url_match": false, "oa_info": {"license": null, "open_access_url": "https://discovery.ucl.ac.uk/id/eprint/10069050/1/alphazero_preprint.pdf", "status": "GREEN"}}, "grobid": {"id": "3e6c3f784d1913410832a750c849df21829f6b1d", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/259be4b0b9afb213e30ec75f7fbe4cd651dc7972.txt", "contents": "\nA general reinforcement learning algorithm that masters chess, shogi, and Go through self-play\n\n\nDavid Silver \nDeepMind\n6 Pancras SquareN1C 4AGLondonUK\n\nUniversity College London\nGower StreetWC1E 6BTLondonUK\n\nThomas Hubert \nDeepMind\n6 Pancras SquareN1C 4AGLondonUK\n\nJulian Schrittwieser \nDeepMind\n6 Pancras SquareN1C 4AGLondonUK\n\nIoannis Antonoglou \nDeepMind\n6 Pancras SquareN1C 4AGLondonUK\n\nMatthew Lai \nDeepMind\n6 Pancras SquareN1C 4AGLondonUK\n\nArthur Guez \nDeepMind\n6 Pancras SquareN1C 4AGLondonUK\n\nMarc Lanctot \nDeepMind\n6 Pancras SquareN1C 4AGLondonUK\n\nLaurent Sifre \nDeepMind\n6 Pancras SquareN1C 4AGLondonUK\n\nDharshan Kumaran \nDeepMind\n6 Pancras SquareN1C 4AGLondonUK\n\nThore Graepel \nDeepMind\n6 Pancras SquareN1C 4AGLondonUK\n\nTimothy Lillicrap \nDeepMind\n6 Pancras SquareN1C 4AGLondonUK\n\nKaren Simonyan \nDeepMind\n6 Pancras SquareN1C 4AGLondonUK\n\nDemis Hassabis \nDeepMind\n6 Pancras SquareN1C 4AGLondonUK\n\nA general reinforcement learning algorithm that masters chess, shogi, and Go through self-play\nCOMPUTER SCIENCE\nThe game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.\n\nThe game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from self-play. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess), as well as Go.\n\nT he study of computer chess is as old as computer science itself. Charles Babbage, Alan Turing, Claude Shannon, and John von Neumann devised hardware, algorithms, and theory to analyze and play the game of chess. Chess subsequently became a grand challenge task for a generation of artificial intelligence researchers, culminating in highperformance computer chess programs that play at a superhuman level (1,2). However, these systems are highly tuned to their domain and cannot be generalized to other games without substantial human effort, whereas general gameplaying systems (3,4) remain comparatively weak.\n\nA long-standing ambition of artificial intelligence has been to create programs that can instead learn for themselves from first principles (5,6). Recently, the AlphaGo Zero algorithm achieved superhuman performance in the game of Go by representing Go knowledge with the use of deep convolutional neural networks (7,8), trained solely by reinforcement learning from games of self-play (9). In this paper, we introduce AlphaZero, a more generic version of the AlphaGo Zero algorithm that accommodates, without special casing, a broader class of game rules. We apply AlphaZero to the games of chess and shogi, as well as Go, by using the same algorithm and network architecture for all three games. Our results demonstrate that a general-purpose reinforcement learning algorithm can learn, tabula rasa-without domain-specific human knowledge or data, as evidenced by the same algorithm succeeding in multiple domainssuperhuman performance across multiple challenging games.\n\nA landmark for artificial intelligence was achieved in 1997 when Deep Blue defeated the human world chess champion (1). Computer chess programs continued to progress steadily beyond human level in the following two decades. These programs evaluate positions by using handcrafted features and carefully tuned weights, constructed by strong human players and programmers, combined with a high-performance alpha-beta search that expands a vast search tree by using a large number of clever heuristics and domain-specific adaptations. In (10) we describe these augmentations, focusing on the 2016 Top Chess Engine Championship (TCEC) season 9 world champion Stockfish (11); other strong chess programs, including Deep Blue, use very similar architectures (1,12).\n\nIn terms of game tree complexity, shogi is a substantially harder game than chess (13,14): It is played on a larger board with a wider variety of pieces; any captured opponent piece switches sides and may subsequently be dropped anywhere on the board. The strongest shogi programs, such as the 2017 Computer Shogi Association (CSA) world champion Elmo, have only recently defeated human champions (15). These programs use an algorithm similar to those used by computer chess programs, again based on a highly optimized alpha-beta search engine with many domain-specific adaptations.\n\nAlphaZero replaces the handcrafted knowledge and domain-specific augmentations used in traditional game-playing programs with deep neural networks, a general-purpose reinforcement learning algorithm, and a general-purpose tree search algorithm.\n\nInstead of a handcrafted evaluation function and move-ordering heuristics, AlphaZero uses a deep neural network (p, v) = f q (s) with parameters q. This neural network f q (s) takes the board position s as an input and outputs a vector of move probabilities p with components p a = Pr(a|s) for each action a and a scalar value v estimating the expected outcome z of the game from position s, v\u2248E\u00bdzjs. AlphaZero learns these move probabilities and value estimates entirely from self-play; these are then used to guide its search in future games.\n\nInstead of an alpha-beta search with domainspecific enhancements, AlphaZero uses a generalpurpose Monte Carlo tree search (MCTS) algorithm. Each search consists of a series of simulated games of self-play that traverse a tree from root state s root until a leaf state is reached. Each simulation proceeds by selecting in each state s a move a with low visit count (not previously frequently explored), high move probability, and high value (averaged over the leaf states of simulations that selected a from s) according to the current neural network f q . The search returns a vector p representing a probability distribution over moves, p a = Pr(a|s root ).\n\nThe parameters q of the deep neural network in AlphaZero are trained by reinforcement learning from self-play games, starting from randomly initialized parameters q. Each game is played by running an MCTS from the current position s root = s t at turn t and then selecting a move, a t~pt , either proportionally (for exploration) or greedily (for exploitation) with respect to the visit counts at the root state. At the end of the game, the terminal position s T is scored according to the rules of the game to compute the game outcome z: \u22121 for a loss, 0 for a draw, and +1 for a win. The neural network parameters q are updated to minimize the error between the predicted outcome v t and the game outcome z and to maximize the similarity of the policy vector p t to the search probabilities p t . Specifically, the parameters q are adjusted by gradient descent on a loss function l that sums over mean-squared error and cross-entropy losses\n\u00f0p; v\u00de \u00bc f q \u00f0s\u00de; l \u00bc \u00f0z \u00c0 v\u00de 2 \u00c0 p \u03a4 log p \u00fe c\u2016q\u2016 2 ;\u00f01\u00de\nwhere c is a parameter controlling the level of L 2 weight regularization. The updated parameters are used in subsequent games of self-play.\n\nThe AlphaZero algorithm described in this paper [see (10) for the pseudocode] differs from the original AlphaGo Zero algorithm in several respects.\n\nAlphaGo Zero estimated and optimized the probability of winning, exploiting the fact that Go games have a binary win or loss outcome. However, both chess and shogi may end in drawn outcomes; it is believed that the optimal solution to chess is a draw (16)(17)(18). AlphaZero instead estimates and optimizes the expected outcome.\n\nThe rules of Go are invariant to rotation and reflection. This fact was exploited in AlphaGo and AlphaGo Zero in two ways. First, training data were augmented by generating eight symmetries for each position. Second, during MCTS, board positions were transformed by using a randomly selected rotation or reflection before being evaluated by the neural network, so that the Monte Carlo evaluation was averaged over different biases. To accommodate a broader class of games, AlphaZero does not assume symmetry; the rules of chess and shogi are asymmetric (e.g., pawns only move forward, and castling is different on kingside and queenside). AlphaZero does not augment the training data and does not transform the board position during MCTS.\n\nIn AlphaGo Zero, self-play games were generated by the best player from all previous iterations. After each iteration of training, the performance of the new player was measured against the best player; if the new player won by a margin of 55%, then it replaced the best player. By contrast, AlphaZero simply maintains a single neural network that is updated continually rather than waiting for an iteration to complete. Self-play games are always generated by using the latest parameters for this neural network.\n\nAs in AlphaGo Zero, the board state is encoded by spatial planes based only on the basic rules for each game. The actions are encoded by either spatial planes or a flat vector, again based only on the basic rules for each game (10).\n\nAlphaGo Zero used a convolutional neural network architecture that is particularly wellsuited to Go: The rules of the game are translationally invariant (matching the weight-sharing structure of convolutional networks) and are defined in terms of liberties corresponding to the adjacencies between points on the board (matching the local structure of convolutional networks). By contrast, the rules of chess and shogi are position dependent (e.g., pawns may move two steps forward from the second rank and promote on the eighth rank) and include longrange interactions (e.g., the queen may traverse the board in one move). Despite these differences, AlphaZero uses the same convolutional network architecture as AlphaGo Zero for chess, shogi, and Go. Silver   The hyperparameters of AlphaGo Zero were tuned by Bayesian optimization. In AlphaZero, we reuse the same hyperparameters, algorithm settings, and network architecture for all games without game-specific tuning. The only exceptions are the exploration noise and the learning rate schedule [see (10) for further details].\n\nWe trained separate instances of AlphaZero for chess, shogi, and Go. Training proceeded for 700,000 steps (in mini-batches of 4096 training positions) starting from randomly initialized parameters. During training only, 5000 firstgeneration tensor processing units (TPUs) (19) were used to generate self-play games, and 16 second-generation TPUs were used to train the neural networks. Training lasted for approximately 9 hours in chess, 12 hours in shogi, and 13 days in Go (see table S3) (20). Further details of the training procedure are provided in (10). Figure 1 shows the performance of AlphaZero during self-play reinforcement learning, as a Silver  AlphaZero's perspective: win (green), draw (gray), or loss (red). The percentage frequency of self-play training games in which this opening was selected by AlphaZero is plotted against the duration of training, in hours.\n\nfunction of training steps, on an Elo (21) scale (22). In chess, AlphaZero first outperformed Stockfish after just 4 hours (300,000 steps); in shogi, AlphaZero first outperformed Elmo after 2 hours (110,000 steps); and in Go, AlphaZero first outperformed AlphaGo Lee (9) after 30 hours (74,000 steps). The training algorithm achieved similar performance in all independent runs (see fig. S3), suggesting that the high performance of AlphaZero's training algorithm is repeatable. We evaluated the fully trained instances of AlphaZero against Stockfish, Elmo, and the previous version of AlphaGo Zero in chess, shogi, and Go, respectively. Each program was run on the hardware for which it was designed (23): Stockfish and Elmo used 44 central processing unit (CPU) cores (as in the TCEC world championship), whereas AlphaZero and AlphaGo Zero used a single machine with four first-generation TPUs and 44 CPU cores (24). The chess match was played against the 2016 TCEC (season 9) world champion Stockfish [see (10) for details]. The shogi match was played against the 2017 CSA world champion version of Elmo (10). The Go match was played against the previously published version of AlphaGo Zero [also trained for 700,000 steps (25)]. All matches were played by using time controls of 3 hours per game, plus an additional 15 s for each move.\n\nIn Go, AlphaZero defeated AlphaGo Zero (9), winning 61% of games. This demonstrates that a general approach can recover the performance of an algorithm that exploited board symmetries to generate eight times as much data (see fig. S1).\n\nIn chess, AlphaZero defeated Stockfish, winning 155 games and losing 6 games out of 1000 (Fig. 2). To verify the robustness of AlphaZero, we played additional matches that started from common human openings (Fig. 3). AlphaZero defeated Stockfish in each opening, suggesting that AlphaZero has mastered a wide spectrum of chess play. The frequency plots in Fig. 3 and the time line in fig. S2 show that common human openings were independently discovered and played frequently by AlphaZero during self-play training. We also played a match that started from the set of opening positions used in the 2016 TCEC world championship; AlphaZero won convincingly in this match, too (26) (fig. S4). We played additional matches against the most recent development version of Stockfish (27) and a variant of Stockfish that uses a strong opening book (28). AlphaZero won all matches by a large margin (Fig. 2). Table S6 shows 20 chess games played by AlphaZero in its matches against Stockfish. In several games, AlphaZero sacrificed pieces for long-term strategic advantage, suggesting that it has a more fluid, context-dependent positional evaluation than the rule-based evaluations used by previous chess programs.\n\nIn shogi, AlphaZero defeated Elmo, winning 98.2% of games when playing black and 91.2% overall. We also played a match under the faster time controls used in the 2017 CSA world championship and against another state-of-the-art shogi program (29); AlphaZero again won both matches by a wide margin (Fig. 2). Table S7 shows 10 shogi games played by AlphaZero in its matches against Elmo. The frequency plots in Fig. 3 and the time line in fig. S2 show that AlphaZero frequently plays one of the two most common human openings but rarely plays the second, deviating on the very first move.\n\nAlphaZero searches just 60,000 positions per second in chess and shogi, compared with 60 million for Stockfish and 25 million for Elmo (table S4). AlphaZero may compensate for the lower number of evaluations by using its deep neural network to focus much more selectively on the most promising variations (Fig. 4 provides an example from the match against Stockfish)arguably a more humanlike approach to searching, as originally proposed by Shannon (30). AlphaZero also defeated Stockfish when given 1 10 = as much thinking time as its opponent (i.e., searching \u223c1 = 10;000 as many positions) and won 46% of games against Elmo when given 1 100 = as much time (i.e., searching \u223c1 = 40;000 as many positions) (Fig. 2) of AlphaZero with the use of MCTS calls into question the widely held belief (31, 32) that alpha-beta search is inherently superior in these domains. The game of chess represented the pinnacle of artificial intelligence research over several decades. State-of-the-art programs are based on powerful engines that search many millions of positions, leveraging handcrafted domain expertise and sophisticated domain adaptations. AlphaZero is a generic reinforcement learning and search algorithm-originally devised for the game of Go-that achieved superior results within a few hours, searching 1 1000 = as many positions, given no domain knowledge except the rules of chess. Furthermore, the same algorithm was applied without modification to the more challenging game of shogi, again outperforming state-of-the-art programs within a few hours. These results bring us a step closer to fulfilling a longstanding ambition of artificial intelligence (3): a general game-playing system that can learn to master any game.\n\nFig. 1 .\n1Training AlphaZero for 700,000 steps. Elo ratings were computed from games between different players where each player was given 1 s per move. (A) Performance of AlphaZero in chess compared with the 2016 TCEC world champion program Stockfish. (B) Performance of AlphaZero in shogi compared with the 2017 CSA world champion program Elmo. (C) Performance of AlphaZero in Go compared with AlphaGo Lee and AlphaGo Zero (20 blocks over 3 days).\n\nFig. 2 .\n2Comparison with specialized programs. (A) Tournament evaluation of AlphaZero in chess, shogi, and Go in matches against, respectively, Stockfish, Elmo, and the previously published version of AlphaGo Zero (AG0) that was trained for 3 days. In the top bar, AlphaZero plays white; in the bottom bar, AlphaZero plays black. Each bar shows the results from AlphaZero's perspective: win (W; green), draw (D; gray), or loss (L; red). (B) Scalability of AlphaZero with thinking time compared with Stockfish and Elmo. Stockfish and Elmo always receive full time (3 hours per game plus 15 s per move); time for AlphaZero is scaled down as indicated. (C) Extra evaluations of AlphaZero in chess against the most recent version of Stockfish at the time of writing (27) and against Stockfish with a strong opening book (28). Extra evaluations of AlphaZero in shogi were carried out against another strong shogi program, Aperyqhapaq (29), at full time controls and against Elmo under 2017 CSA world championship time controls (10 min per game and 10 s per move). (D) Average result of chess matches starting from different opening positions, either common human positions (see also Fig. 3) or the 2016 TCEC world championship opening positions (see also fig. S4), and average result of shogi matches starting from common human positions (see also Fig. 3). CSA world championship games start from the initial board position. Match conditions are summarized in tables S8 and S9.\n\nFig. 3 .\n3Matches starting from the most popular human openings. AlphaZero plays against (A) Stockfish in chess and (B) Elmo in shogi. In the left bar, AlphaZero plays white, starting from the given position; in the right bar, AlphaZero plays black. Each bar shows the results from\n\n\n. The high performanceFig. 4. AlphaZero's search procedure. The search is illustrated for a position (inset) from game 1 (table S6) between AlphaZero (white) and Stockfish (black) after 29. ... Qf8. The internal state of AlphaZero's MCTS is summarized after 10 2 , ..., 10 6 simulations. Each summary shows the 10 most visited states. The estimated value is shown in each state, from white's perspective, scaled to the range [0, 100]. The visit count of each state, relative to the root state of that tree, is proportional to the thickness of the border circle. AlphaZero considers 30. c6 but eventually plays 30. d5.Silver et al., Science 362, 1140-1144 (2018) \n7 December 2018 \n\n4 of 5 \n\n\nof 5 RESEARCH | REPORT\nACKNOWLEDGMENTSWe thank M. Sadler for analyzing chess games; Y. Habu for analyzing shogi games; L. Bennett for organizational assistance; B. Konrad, E. Lockhart, and G. Ostrovski for reviewing the paper; and the rest of the DeepMind team for their support.\n. M Campbell, A J HoaneJr, F Hsu, Artif. Intell. 134M. Campbell, A. J. Hoane Jr., F. Hsu, Artif. Intell. 134, 57-83 (2002).\n\nBehind Deep Blue: Building the Computer That Defeated the World Chess Champion. F.-H Hsu, Princeton Univ.F.-H. Hsu, Behind Deep Blue: Building the Computer That Defeated the World Chess Champion (Princeton Univ., 2002).\n\n. B Pell, Comput. Intell. 12B. Pell, Comput. Intell. 12, 177-198 (1996).\n\n. M R Genesereth, N Love, B Pell, A I Mag, 26M. R. Genesereth, N. Love, B. Pell, AI Mag. 26, 62-72 (2005).\n\n. A L Samuel, IBM J. Res. Dev. 11A. L. Samuel, IBM J. Res. Dev. 11, 601-617 (1967).\n\n. G Tesauro, Neural Comput. 6G. Tesauro, Neural Comput. 6, 215-219 (1994).\n\nC J Maddison, A Huang, I Sutskever, D Silver, the International Conference on Learning Representations. San Diego, CAC. J. Maddison, A. Huang, I. Sutskever, D. Silver, paper presented at the International Conference on Learning Representations 2015, San Diego, CA, 7 to 9 May 2015.\n\n. D Silver, Nature. 529D. Silver et al., Nature 529, 484-489 (2016).\n\n. D Silver, Nature. 550D. Silver et al., Nature 550, 354-359 (2017).\n\nSee the supplementary materials for additional information. 1129Stockfish: Strong open source chess engineSee the supplementary materials for additional information. 11. Stockfish: Strong open source chess engine; https:// stockfishchess.org/ [accessed 29 November 2017].\n\nD N L Levy, M Newborn, How Computers Play Chess. Ishi PressD. N. L. Levy, M. Newborn, How Computers Play Chess (Ishi Press, 2009).\n\nSearching for solutions in games and artificial intelligence. V , Allis , Maastricht, NetherlandsTransnational University LimburgPh.D. thesisV. Allis, \"Searching for solutions in games and artificial intelligence,\" Ph.D. thesis, Transnational University Limburg, Maastricht, Netherlands (1994).\n\n. H Iida, M Sakuta, J Rollason, Artif. Intell. 134H. Iida, M. Sakuta, J. Rollason, Artif. Intell. 134, 121-144 (2002).\n\nResults of the 27th world computer shogi championship; www2.computer-shogi.org/ wcsc27/index_e.html. 29Computer Shogi AssociationComputer Shogi Association, Results of the 27th world computer shogi championship; www2.computer-shogi.org/ wcsc27/index_e.html [accessed 29 November 2017].\n\nThe Modern Chess Instructor (Edition Olms. W Steinitz, W. Steinitz, The Modern Chess Instructor (Edition Olms, 1990).\n\nE Lasker, Common Sense in Chess. Dover PublicationsE. Lasker, Common Sense in Chess (Dover Publications, 1965).\n\nEssential Chess Quotations (iUniverse. J Knudsen, J. Knudsen, Essential Chess Quotations (iUniverse, 2000).\n\nN P Jouppi, Proceedings of the 44th Annual International Symposium on Computer Architecture. the 44th Annual International Symposium on Computer ArchitectureToronto, CanadaAssociation for Computing MachineryN. P. Jouppi et al., in Proceedings of the 44th Annual International Symposium on Computer Architecture, Toronto, Canada, 24 to 28 June 2017 (Association for Computing Machinery, 2017), pp. 1-12.\n\nNote that the original AlphaGo Zero study used graphics processing units (GPUs) to train the neural networks. Note that the original AlphaGo Zero study used graphics processing units (GPUs) to train the neural networks.\n\nR Coulom, Proceedings of the Sixth International Conference on Computers and Games. the Sixth International Conference on Computers and GamesBeijing, ChinaSpringerR. Coulom, in Proceedings of the Sixth International Conference on Computers and Games, Beijing, China, 29 September to 1 October 2008 (Springer, 2008), pp. 113-124.\n\nThe prevalence of draws in high-level chess tends to compress the Elo scale, compared with that for shogi or Go. The prevalence of draws in high-level chess tends to compress the Elo scale, compared with that for shogi or Go.\n\nStockfish is designed to exploit CPU hardware and cannot make use of GPUs or TPUs, whereas AlphaZero is designed to exploit GPU-TPU hardware rather than CPU hardware. Stockfish is designed to exploit CPU hardware and cannot make use of GPUs or TPUs, whereas AlphaZero is designed to exploit GPU-TPU hardware rather than CPU hardware.\n\nA first generation TPU is roughly similar in inference speed to a Titan V GPU. although the architectures are not directly comparableA first generation TPU is roughly similar in inference speed to a Titan V GPU, although the architectures are not directly comparable.\n\nAlphaGo Zero was ultimately trained for 3.1 million steps over 40 days. AlphaGo Zero was ultimately trained for 3.1 million steps over 40 days.\n\nMany TCEC opening positions are unbalanced according to both AlphaZero and Stockfish, resulting in more losses for both players. Many TCEC opening positions are unbalanced according to both AlphaZero and Stockfish, resulting in more losses for both players.\n\nNewest available version of Stockfish as of. 13resolved_base b508f9561cc2302c129efe8d60f201ff03ee72c8Newest available version of Stockfish as of 13 January 2018 (resolved_base b508f9561cc2302c129efe8d60f201ff03ee72c8), from https://github.com/official-stockfish/Stockfish/commit/.\n\nAlphaZero did not use an opening book. To ensure diversity against a deterministic opening book, AlphaZero used a small amount of randomization in its opening moves (10). this avoided duplicate games but also resulted in more lossesThe Stockfish variant used the Cerebellum opening book downloaded from https://zipproth.de/#Brainfish. AlphaZero did not use an opening book. To ensure diversity against a deterministic opening book, AlphaZero used a small amount of randomization in its opening moves (10); this avoided duplicate games but also resulted in more losses.\n\nAperyqhapaq's evaluation files are. Aperyqhapaq's evaluation files are available at https://github. com/qhapaq-49/qhapaq-bin/releases/tag/eloqhappa.\n\n. C E Shannon, London Edinburgh Dublin Philos. Mag. J. Sci. 41C. E. Shannon, London Edinburgh Dublin Philos. Mag. J. Sci. 41, 256-275 (1950).\n\nMonte Carlo chess. O Arenz, Technische Universit\u00e4t Darmstadtmaster's thesisO. Arenz, \"Monte Carlo chess,\" master's thesis, Technische Universit\u00e4t Darmstadt (2012).\n\nO E David, N S Netanyahu, L Wolf, Artificial Neural Networks and Machine Learning-ICANN 2016. Barcelona, SpainSpringerO. E. David, N. S. Netanyahu, L. Wolf, in Artificial Neural Networks and Machine Learning-ICANN 2016, Part II, Barcelona, Spain, 6 to 9 September 2016 (Springer, 2016), pp. 88-96.\n", "annotations": {"author": "[{\"end\":209,\"start\":98},{\"end\":266,\"start\":210},{\"end\":330,\"start\":267},{\"end\":392,\"start\":331},{\"end\":447,\"start\":393},{\"end\":502,\"start\":448},{\"end\":558,\"start\":503},{\"end\":615,\"start\":559},{\"end\":675,\"start\":616},{\"end\":732,\"start\":676},{\"end\":793,\"start\":733},{\"end\":851,\"start\":794},{\"end\":909,\"start\":852}]", "publisher": null, "author_last_name": "[{\"end\":110,\"start\":104},{\"end\":223,\"start\":217},{\"end\":287,\"start\":274},{\"end\":349,\"start\":339},{\"end\":404,\"start\":401},{\"end\":459,\"start\":455},{\"end\":515,\"start\":508},{\"end\":572,\"start\":567},{\"end\":632,\"start\":625},{\"end\":689,\"start\":682},{\"end\":750,\"start\":741},{\"end\":808,\"start\":800},{\"end\":866,\"start\":858}]", "author_first_name": "[{\"end\":103,\"start\":98},{\"end\":216,\"start\":210},{\"end\":273,\"start\":267},{\"end\":338,\"start\":331},{\"end\":400,\"start\":393},{\"end\":454,\"start\":448},{\"end\":507,\"start\":503},{\"end\":566,\"start\":559},{\"end\":624,\"start\":616},{\"end\":681,\"start\":676},{\"end\":740,\"start\":733},{\"end\":799,\"start\":794},{\"end\":857,\"start\":852}]", "author_affiliation": "[{\"end\":152,\"start\":112},{\"end\":208,\"start\":154},{\"end\":265,\"start\":225},{\"end\":329,\"start\":289},{\"end\":391,\"start\":351},{\"end\":446,\"start\":406},{\"end\":501,\"start\":461},{\"end\":557,\"start\":517},{\"end\":614,\"start\":574},{\"end\":674,\"start\":634},{\"end\":731,\"start\":691},{\"end\":792,\"start\":752},{\"end\":850,\"start\":810},{\"end\":908,\"start\":868}]", "title": "[{\"end\":95,\"start\":1},{\"end\":1004,\"start\":910}]", "venue": null, "abstract": "[{\"end\":1810,\"start\":1022}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":3012,\"start\":3009},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":3014,\"start\":3012},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":3186,\"start\":3183},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":3188,\"start\":3186},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3360,\"start\":3357},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":3362,\"start\":3360},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":3534,\"start\":3531},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3536,\"start\":3534},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3606,\"start\":3603},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4945,\"start\":4942},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":4948,\"start\":4945},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":5037,\"start\":5033},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":5040,\"start\":5037},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5352,\"start\":5348},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":8187,\"start\":8183},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":8534,\"start\":8530},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":8538,\"start\":8534},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":8542,\"start\":8538},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":10095,\"start\":10091},{\"end\":10855,\"start\":10849},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11155,\"start\":11151},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":11455,\"start\":11451},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":11673,\"start\":11669},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":11737,\"start\":11733},{\"end\":11835,\"start\":11829},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":12113,\"start\":12109},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":12977,\"start\":12973},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13073,\"start\":13069},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":13290,\"start\":13286},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":14316,\"start\":14312},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":14418,\"start\":14414},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":14482,\"start\":14478},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":15091,\"start\":15087},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":15887,\"start\":15883},{\"end\":15938,\"start\":15934}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":17614,\"start\":17164},{\"attributes\":{\"id\":\"fig_2\"},\"end\":19089,\"start\":17615},{\"attributes\":{\"id\":\"fig_3\"},\"end\":19372,\"start\":19090},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":20065,\"start\":19373}]", "paragraph": "[{\"end\":2600,\"start\":1812},{\"end\":3215,\"start\":2602},{\"end\":4189,\"start\":3217},{\"end\":4949,\"start\":4191},{\"end\":5533,\"start\":4951},{\"end\":5779,\"start\":5535},{\"end\":6325,\"start\":5781},{\"end\":6985,\"start\":6327},{\"end\":7929,\"start\":6987},{\"end\":8128,\"start\":7988},{\"end\":8277,\"start\":8130},{\"end\":8607,\"start\":8279},{\"end\":9347,\"start\":8609},{\"end\":9862,\"start\":9349},{\"end\":10096,\"start\":9864},{\"end\":11177,\"start\":10098},{\"end\":12058,\"start\":11179},{\"end\":13399,\"start\":12060},{\"end\":13636,\"start\":13401},{\"end\":14844,\"start\":13638},{\"end\":15432,\"start\":14846},{\"end\":17163,\"start\":15434}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":7987,\"start\":7930}]", "table_ref": "[{\"end\":11667,\"start\":11651},{\"end\":14546,\"start\":14538},{\"end\":15161,\"start\":15153},{\"end\":15579,\"start\":15569}]", "section_header": "[{\"end\":17173,\"start\":17165},{\"end\":17624,\"start\":17616},{\"end\":19099,\"start\":19091}]", "table": "[{\"end\":20065,\"start\":19992}]", "figure_caption": "[{\"end\":17614,\"start\":17175},{\"end\":19089,\"start\":17626},{\"end\":19372,\"start\":19101},{\"end\":19992,\"start\":19375}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":11747,\"start\":11739},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":12450,\"start\":12443},{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":13634,\"start\":13627},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":13735,\"start\":13727},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":13853,\"start\":13845},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":14000,\"start\":13994},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14029,\"start\":14022},{\"end\":14325,\"start\":14317},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":14536,\"start\":14528},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15151,\"start\":15143},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":15261,\"start\":15255},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":15290,\"start\":15283},{\"end\":15746,\"start\":15739},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16149,\"start\":16141}]", "bib_author_first_name": "[{\"end\":20349,\"start\":20348},{\"end\":20361,\"start\":20360},{\"end\":20363,\"start\":20362},{\"end\":20374,\"start\":20373},{\"end\":20555,\"start\":20551},{\"end\":20695,\"start\":20694},{\"end\":20769,\"start\":20768},{\"end\":20771,\"start\":20770},{\"end\":20785,\"start\":20784},{\"end\":20793,\"start\":20792},{\"end\":20801,\"start\":20800},{\"end\":20803,\"start\":20802},{\"end\":20877,\"start\":20876},{\"end\":20879,\"start\":20878},{\"end\":20962,\"start\":20961},{\"end\":21036,\"start\":21035},{\"end\":21038,\"start\":21037},{\"end\":21050,\"start\":21049},{\"end\":21059,\"start\":21058},{\"end\":21072,\"start\":21071},{\"end\":21321,\"start\":21320},{\"end\":21391,\"start\":21390},{\"end\":21732,\"start\":21731},{\"end\":21736,\"start\":21733},{\"end\":21744,\"start\":21743},{\"end\":21926,\"start\":21925},{\"end\":21934,\"start\":21929},{\"end\":22162,\"start\":22161},{\"end\":22170,\"start\":22169},{\"end\":22180,\"start\":22179},{\"end\":22610,\"start\":22609},{\"end\":22686,\"start\":22685},{\"end\":22838,\"start\":22837},{\"end\":22908,\"start\":22907},{\"end\":22910,\"start\":22909},{\"end\":23533,\"start\":23532},{\"end\":26102,\"start\":26101},{\"end\":26104,\"start\":26103},{\"end\":26262,\"start\":26261},{\"end\":26408,\"start\":26407},{\"end\":26410,\"start\":26409},{\"end\":26419,\"start\":26418},{\"end\":26421,\"start\":26420},{\"end\":26434,\"start\":26433}]", "bib_author_last_name": "[{\"end\":20358,\"start\":20350},{\"end\":20369,\"start\":20364},{\"end\":20378,\"start\":20375},{\"end\":20559,\"start\":20556},{\"end\":20700,\"start\":20696},{\"end\":20782,\"start\":20772},{\"end\":20790,\"start\":20786},{\"end\":20798,\"start\":20794},{\"end\":20807,\"start\":20804},{\"end\":20886,\"start\":20880},{\"end\":20970,\"start\":20963},{\"end\":21047,\"start\":21039},{\"end\":21056,\"start\":21051},{\"end\":21069,\"start\":21060},{\"end\":21079,\"start\":21073},{\"end\":21328,\"start\":21322},{\"end\":21398,\"start\":21392},{\"end\":21741,\"start\":21737},{\"end\":21752,\"start\":21745},{\"end\":22167,\"start\":22163},{\"end\":22177,\"start\":22171},{\"end\":22189,\"start\":22181},{\"end\":22619,\"start\":22611},{\"end\":22693,\"start\":22687},{\"end\":22846,\"start\":22839},{\"end\":22917,\"start\":22911},{\"end\":23540,\"start\":23534},{\"end\":26112,\"start\":26105},{\"end\":26268,\"start\":26263},{\"end\":26416,\"start\":26411},{\"end\":26431,\"start\":26422},{\"end\":26439,\"start\":26435}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\"},\"end\":20469,\"start\":20346},{\"attributes\":{\"id\":\"b1\"},\"end\":20690,\"start\":20471},{\"attributes\":{\"id\":\"b2\"},\"end\":20764,\"start\":20692},{\"attributes\":{\"id\":\"b3\"},\"end\":20872,\"start\":20766},{\"attributes\":{\"id\":\"b4\"},\"end\":20957,\"start\":20874},{\"attributes\":{\"id\":\"b5\"},\"end\":21033,\"start\":20959},{\"attributes\":{\"id\":\"b6\"},\"end\":21316,\"start\":21035},{\"attributes\":{\"id\":\"b7\"},\"end\":21386,\"start\":21318},{\"attributes\":{\"id\":\"b8\"},\"end\":21456,\"start\":21388},{\"attributes\":{\"id\":\"b9\"},\"end\":21729,\"start\":21458},{\"attributes\":{\"id\":\"b10\"},\"end\":21861,\"start\":21731},{\"attributes\":{\"id\":\"b11\"},\"end\":22157,\"start\":21863},{\"attributes\":{\"id\":\"b12\"},\"end\":22277,\"start\":22159},{\"attributes\":{\"id\":\"b13\"},\"end\":22564,\"start\":22279},{\"attributes\":{\"id\":\"b14\"},\"end\":22683,\"start\":22566},{\"attributes\":{\"id\":\"b15\"},\"end\":22796,\"start\":22685},{\"attributes\":{\"id\":\"b16\"},\"end\":22905,\"start\":22798},{\"attributes\":{\"id\":\"b17\"},\"end\":23309,\"start\":22907},{\"attributes\":{\"id\":\"b18\"},\"end\":23530,\"start\":23311},{\"attributes\":{\"id\":\"b19\"},\"end\":23860,\"start\":23532},{\"attributes\":{\"id\":\"b20\"},\"end\":24087,\"start\":23862},{\"attributes\":{\"id\":\"b21\"},\"end\":24422,\"start\":24089},{\"attributes\":{\"id\":\"b22\"},\"end\":24691,\"start\":24424},{\"attributes\":{\"id\":\"b23\"},\"end\":24836,\"start\":24693},{\"attributes\":{\"id\":\"b24\"},\"end\":25095,\"start\":24838},{\"attributes\":{\"id\":\"b25\"},\"end\":25377,\"start\":25097},{\"attributes\":{\"id\":\"b26\"},\"end\":25947,\"start\":25379},{\"attributes\":{\"id\":\"b27\"},\"end\":26097,\"start\":25949},{\"attributes\":{\"id\":\"b28\"},\"end\":26240,\"start\":26099},{\"attributes\":{\"id\":\"b29\"},\"end\":26405,\"start\":26242},{\"attributes\":{\"id\":\"b30\"},\"end\":26704,\"start\":26407}]", "bib_title": null, "bib_author": "[{\"end\":20360,\"start\":20348},{\"end\":20373,\"start\":20360},{\"end\":20380,\"start\":20373},{\"end\":20561,\"start\":20551},{\"end\":20702,\"start\":20694},{\"end\":20784,\"start\":20768},{\"end\":20792,\"start\":20784},{\"end\":20800,\"start\":20792},{\"end\":20809,\"start\":20800},{\"end\":20888,\"start\":20876},{\"end\":20972,\"start\":20961},{\"end\":21049,\"start\":21035},{\"end\":21058,\"start\":21049},{\"end\":21071,\"start\":21058},{\"end\":21081,\"start\":21071},{\"end\":21330,\"start\":21320},{\"end\":21400,\"start\":21390},{\"end\":21743,\"start\":21731},{\"end\":21754,\"start\":21743},{\"end\":21929,\"start\":21925},{\"end\":21937,\"start\":21929},{\"end\":22169,\"start\":22161},{\"end\":22179,\"start\":22169},{\"end\":22191,\"start\":22179},{\"end\":22621,\"start\":22609},{\"end\":22695,\"start\":22685},{\"end\":22848,\"start\":22837},{\"end\":22919,\"start\":22907},{\"end\":23542,\"start\":23532},{\"end\":26114,\"start\":26101},{\"end\":26270,\"start\":26261},{\"end\":26418,\"start\":26407},{\"end\":26433,\"start\":26418},{\"end\":26441,\"start\":26433}]", "bib_venue": "[{\"end\":20393,\"start\":20380},{\"end\":20549,\"start\":20471},{\"end\":20716,\"start\":20702},{\"end\":20903,\"start\":20888},{\"end\":20985,\"start\":20972},{\"end\":21137,\"start\":21081},{\"end\":21336,\"start\":21330},{\"end\":21406,\"start\":21400},{\"end\":21516,\"start\":21458},{\"end\":21778,\"start\":21754},{\"end\":21923,\"start\":21863},{\"end\":22204,\"start\":22191},{\"end\":22378,\"start\":22279},{\"end\":22607,\"start\":22566},{\"end\":22716,\"start\":22695},{\"end\":22835,\"start\":22798},{\"end\":22998,\"start\":22919},{\"end\":23419,\"start\":23311},{\"end\":23614,\"start\":23542},{\"end\":23973,\"start\":23862},{\"end\":24254,\"start\":24089},{\"end\":24501,\"start\":24424},{\"end\":24763,\"start\":24693},{\"end\":24965,\"start\":24838},{\"end\":25140,\"start\":25097},{\"end\":25548,\"start\":25379},{\"end\":25983,\"start\":25949},{\"end\":26157,\"start\":26114},{\"end\":26259,\"start\":26242},{\"end\":26499,\"start\":26441},{\"end\":21152,\"start\":21139},{\"end\":23079,\"start\":23000},{\"end\":23687,\"start\":23616},{\"end\":26517,\"start\":26501}]"}}}, "year": 2023, "month": 12, "day": 17}