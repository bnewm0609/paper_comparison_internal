{"id": 251493118, "updated": "2023-10-05 11:34:54.018", "metadata": {"title": "General Cutting Planes for Bound-Propagation-Based Neural Network Verification", "authors": "[{\"first\":\"Huan\",\"last\":\"Zhang\",\"middle\":[]},{\"first\":\"Shiqi\",\"last\":\"Wang\",\"middle\":[]},{\"first\":\"Kaidi\",\"last\":\"Xu\",\"middle\":[]},{\"first\":\"Linyi\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Bo\",\"last\":\"Li\",\"middle\":[]},{\"first\":\"Suman\",\"last\":\"Jana\",\"middle\":[]},{\"first\":\"Cho-Jui\",\"last\":\"Hsieh\",\"middle\":[]},{\"first\":\"J.\",\"last\":\"Kolter\",\"middle\":[\"Zico\"]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2022, "month": null, "day": null}, "abstract": "Bound propagation methods, when combined with branch and bound, are among the most effective methods to formally verify properties of deep neural networks such as correctness, robustness, and safety. However, existing works cannot handle the general form of cutting plane constraints widely accepted in traditional solvers, which are crucial for strengthening verifiers with tightened convex relaxations. In this paper, we generalize the bound propagation procedure to allow the addition of arbitrary cutting plane constraints, including those involving relaxed integer variables that do not appear in existing bound propagation formulations. Our generalized bound propagation method, GCP-CROWN, opens up the opportunity to apply general cutting plane methods for neural network verification while benefiting from the efficiency and GPU acceleration of bound propagation methods. As a case study, we investigate the use of cutting planes generated by off-the-shelf mixed integer programming (MIP) solver. We find that MIP solvers can generate high-quality cutting planes for strengthening bound-propagation-based verifiers using our new formulation. Since the branching-focused bound propagation procedure and the cutting-plane-focused MIP solver can run in parallel utilizing different types of hardware (GPUs and CPUs), their combination can quickly explore a large number of branches with strong cutting planes, leading to strong verification performance. Experiments demonstrate that our method is the first verifier that can completely solve the oval20 benchmark and verify twice as many instances on the oval21 benchmark compared to the best tool in VNN-COMP 2021, and also noticeably outperforms state-of-the-art verifiers on a wide range of benchmarks. GCP-CROWN is part of the $\\alpha,\\!\\beta$-CROWN verifier, the VNN-COMP 2022 winner. Code is available at http://PaperCode.cc/GCP-CROWN", "fields_of_study": "[\"Computer Science\",\"Mathematics\"]", "external_ids": {"arxiv": "2208.05740", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "conf/nips/ZhangWXLLJHK22", "doi": "10.48550/arxiv.2208.05740"}}, "content": {"source": {"pdf_hash": "d77ecb65db24407c6557f83cd517e07fcfee7ddd", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2208.05740v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "ab6fe53f0de632fa398c0d1bc6c0333c4739de89", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/d77ecb65db24407c6557f83cd517e07fcfee7ddd.txt", "contents": "\nGeneral Cutting Planes for Bound-Propagation-Based Neural Network Verification\n\n\nHuan Zhang \nCMU\n\n\nEqual Contribution\n\n\nShiqi Wang \nColumbia University\n3 Drexel University 4 UIUC 5 UCLA 6 Bosch Center for AI\n\nEqual Contribution\n\n\nKaidi Xu \nEqual Contribution\n\n\nLinyi Li linyi2@illinois.edu \nBo Li \nSuman Jana \nColumbia University\n3 Drexel University 4 UIUC 5 UCLA 6 Bosch Center for AI\n\nCho-Jui Hsieh chohsieh@cs.ucla.edu \nJ Zico Kolter \nCMU\n\n\nGeneral Cutting Planes for Bound-Propagation-Based Neural Network Verification\nCode is available at http://PaperCode.cc/GCP-CROWN.\nBound propagation methods, when combined with branch and bound, are among the most effective methods to formally verify properties of deep neural networks such as correctness, robustness, and safety. However, existing works cannot handle the general form of cutting plane constraints widely accepted in traditional solvers, which are crucial for strengthening verifiers with tightened convex relaxations. In this paper, we generalize the bound propagation procedure to allow the addition of arbitrary cutting plane constraints, including those involving relaxed integer variables that do not appear in existing bound propagation formulations. Our generalized bound propagation method, GCP-CROWN, opens up the opportunity to apply general cutting plane methods for neural network verification while benefiting from the efficiency and GPU acceleration of bound propagation methods. As a case study, we investigate the use of cutting planes generated by off-the-shelf mixed integer programming (MIP) solver. We find that MIP solvers can generate high-quality cutting planes for strengthening bound-propagation-based verifiers using our new formulation. Since the branching-focused bound propagation procedure and the cutting-plane-focused MIP solver can run in parallel utilizing different types of hardware (GPUs and CPUs), their combination can quickly explore a large number of branches with strong cutting planes, leading to strong verification performance. Experiments demonstrate that our method is the first verifier that can completely solve the oval20 benchmark and verify twice as many instances on the oval21 benchmark compared to the best tool in VNN-COMP 2021, and also noticeably outperforms state-of-the-art verifiers on a wide range of benchmarks. GCP-CROWN is part of the \u03b1,\u03b2-CROWN verifier, the VNN-COMP 2022 winner. Code is available at http://PaperCode.cc/GCP-CROWN.\n\nIntroduction\n\nNeural network (NN) verification aims to formally prove or disprove certain properties (e.g., correctness and safety properties) of a NN under a certain set of inputs. These methods can provide worst-case performance guarantees of a NN, and have been applied to mission-critical applications that involve neural networks, such as automatic aircraft control [31,4], learning-enabled cyber-physical systems [54], and NN based algorithms in an operating system [51].\n\nThe NN verification problem is generally NP-complete [30]. For piece-wise linear networks, it can be encoded as a mixed integer programming (MIP) [53] problem with the non-linear ReLU neurons 36th Conference on Neural Information Processing Systems (NeurIPS 2022). described by binary variables. Thus, fundamentally, the NN verification problem can be solved using the branch and bound (BaB) [10] method similar to generic MIP solvers, by branching some binary variables and relaxing the rest into a convex problem such as linear programming (LP) to obtain bounds on the objective. Although early neural network verifiers relied on off-the-shelf CPU-based LP solvers [36,9] for bounding in BaB, LP solvers do not scale well to large NNs. Thus, many recent verifiers are instead based on efficient and GPU-accelerated algorithms customized to NN verification, such as bound propagation methods [60,57], Lagrangian decomposition methods [8,17] and others [16,11]. Bound propagation methods, presented in a few different formulations [58,18,56,61,50,25], empower state-of-the-art NN verifiers such as \u03b1,\u03b2-CROWN [61,60,57] and VeriNet [3], and can achieve two to three orders of magnitudes speedup compared to solving the NN verification problem using an off-the-shelf solver directly [57], especially on large networks.\n\nDespite the success of existing NN verifiers, we experimentally find that state-of-the-art NN verifiers may timeout on certain hard instances which a generic MIP solver can solve relatively quickly, sometimes even without branching. Compared to an MIP solver, a crucial factor missing in most scalable NN verifiers is the ability to efficiently generate and solve general cutting planes (or \"cuts\"). In generic MIP solvers, cutting planes are essential to strengthen the convex relaxation, so that much less branching is required. Advanced cutting planes are among the most important factors in modern MIP solvers [6]; they can strengthen the convex relaxation without removing any valid integer solution from the MIP formulation. In the setting of NN verification, cutting planes reflects complex intra-layer and inter-layer dependencies between multiple neurons, which cannot be easily captured by existing bound propagation methods with single neuron relaxations [45]. This motivates us to seek the combination of efficient bound propagation method with effective cutting planes to further increase the power of NN verifiers.\n\nA few key factors make the inclusion of general cutting planes in NN verifiers quite challenging. First, existing efficient bound propagation frameworks such as CROWN [61] and \u03b2-CROWN [57] cannot solve general cutting plane constraints that may involve variables across different layers in the MIP formulation. Particularly, these frameworks do not explicitly include the integer variables in the MIP formulation that are crucial when encoding many classical strong cutting planes, such as Gomory cuts and mixed integer rounding (MIR) cuts. Furthermore, although some existing works [47,52,40] enhanced the basic convex relaxation used in NN verification (such as the Planet relaxation [19]), these enhanced relaxations involve only one or a few neurons in a single layer or two adjacent layers, and are not general enough. In addition, an LP solver is often required to handle these additional cutting plane constraints [40], for which the efficient and GPU-accelerated bound propagation cannot be used, so the use of these tighter relaxations may not always bring improvements.\n\nIn this paper, we achieve major progress in using general cutting planes in bound propagation based NN verifiers. To mitigate the challenge of efficiently solving general cuts, our first contribution is to generalize existing bound propagation methods to their most general form, enabling constraints involving variables from neurons of any layer as well as integer variables that encode the status of a ReLU neuron. This allows us to consider any cuts during bound propagation without relying on a slow LP solver, and opens up the opportunity for using advanced cutting plane techniques efficiently for the NN verification problem. Our second contribution involves combining a cutting-plane-focused, off-the-shelf MIP solver with our GPU-accelerated, branching-focused bound propagation method capable of handling general cuts. We entirely disable branching in the MIP solver and use it only for generating high quality cutting planes not restricting to neurons within adjacent layers. Although an MIP solver often cannot verify large neural networks, we find that they can generate high quality cutting planes within a short time, significantly helping bound propagation to achieve better bounds.\n\nOur experiments show that general cutting planes can bring significant improvements to NN verifiers: we are the first verifier that completely solves all instances in the oval20 benchmark, with an average time of less than 5 seconds per instance; on the even harder oval21 benchmark in VNN-COMP 2021 [3], we can verify twice as many instances compared to the competition winner. We also outperform existing state-of-the-art bound-propagation-based methods including those using multi-neuron relaxations [20] (a limited form of cutting planes).\n\n\nBackground\n\nThe NN verification problem We consider the verification problem for an L-layer ReLU-based Neural Network (NN) with inputsx (0) := x \u2208 R d0 , weights W (i) \u2208 R di\u00d7di\u22121 , and biases b (i) \u2208 R di (i \u2208 {1, \u00b7 \u00b7 \u00b7 , L}). We can get the NN outputs f (x) = x (L) \u2208 R d L by sequentially propagating the input x through affine layers with x (i) = W (i)x(i\u22121) + b (i) and ReLU layer wit\u0125 x (i) = ReLU(x (i) ). We also let scalarsx (i) j and x (i) j denote the post-activation and pre-activation, respectively, of j-th ReLU neuron in i-th layer. Throughout the paper, we use bold symbols to denote vectors (e.g., x (i) ) and regular symbols to denote scalars (e.g., x (i) j is the j-th element of x (i) ). We use the shorthand [N ] to denote {1, \u00b7 \u00b7 \u00b7 , N }, and W (i) :,j is the j-th column of W (i) . Commonly, the input x is bounded within a perturbation set C (such as an p norm ball) and the verification specification defines a property of the output f (x) that should hold for any x \u2208 C, e.g., whether the true label's logit f y (x) will be always larger than another label's logit f j (x), (i.e., checking if f y (x)\u2212f j (x) is always positive). Since we can append the verification specification (such as a linear function on neural network output) as an additional layer of the network, canonically, the NN verification problem requires one to solve the following one-dimensional (d L = 1) optimization objective on f (x):\nf * = min x f (x), \u2200x \u2208 C(1)\nwith the relevant property defined to be proven if the optimal solution f * \u2265 0. Throughout this work, we consider the \u221e norm ball C := {x :\n\nx \u2212 x 0 \u221e \u2264 } where x 0 is a predefined constant (e.g., a clean input image), although it is possible to extend to other norms or specifications [43,59].\n\nThe MIP and LP formulation for NN verification The mixed integer programming (MIP) formulation is the root of many NN verification algorithms. This formulation uses binary variables z to encode the non-linear ReLU neurons to make the non-convex optimization problem (1) tractable. Additionally, we assume that we know sound pre-activation bounds l (i) \u2264 x (i) \u2264 u (i) for x \u2208 C which can be obtained via cheap bound propagation methods such as IBP [23] or CROWN [61]. Then ReLU neurons for each layer i can be classified into three classes [58], namely \"active\" (I +(i) ), \"inactive\" (I \u2212(i) ) and \"unstable\" (I (i) ) neurons, respectively:\nI +(i) := {j : l (i) j \u2265 0}; I \u2212(i) := {j : u (i) j \u2264 0}; I (i) := {j : l (i) j \u2264 0, u (i) j \u2265 0}\nBased on the definition of ReLU, activate and inactive neurons are linear functions, so only unstable neurons require binary encoding. The MIP formulation of (1) is:\nf * = min x,x,z f (x) s.t. f (x) = x (L) ;x (0) = x; x \u2208 C;\n(2)\nx (i) = W (i)x(i\u22121) + b (i) ; i \u2208 [L],(3)\nx (i)\nj \u2265 0; j \u2208 I (i) , i \u2208 [L\u22121] (4) x (i) j \u2265 x (i) j ; j \u2208 I (i) , i \u2208 [L\u22121] (5) x (i) j \u2264 u (i) j z (i) j ; j \u2208 I (i) , i \u2208 [L\u22121] (6) x (i) j \u2264 x (i) j \u2212 l (i) j (1 \u2212 z (i) j ); j \u2208 I (i) , i \u2208 [L\u22121] (7) z (i) j \u2208 {0, 1}; j \u2208 I \u2212(i) , i \u2208 [L\u22121] (8) x (i) j = x (i) j ; j \u2208 I +(i) , i \u2208 [L\u22121](9)\nx (i)\nj = 0; j \u2208 I \u2212(i) , i \u2208 [L\u22121](10)\nSince a MIP problem is slow or intractable to solve, it is commonly relaxed as a When the integer variables are relaxed to continuous ones, we obtain the LP relaxation of NN verification problem:\nf * LP = min x,x,z f (x) s.t.\n(3), (2), (4), (5), (6), (7), (9),(10), 0 \u2264 z\n(i) j \u2264 1; j \u2208 I (i) , i \u2208 [L\u22121](11)\nThe ReLU constraints involving z is often projected out, leading to the well-known Planet relaxation used in many NN verifiers, replacing (6), (7) and (8) with a single constraint to get an equivalent LP: \nf * LP = min x,x,z f (x)(i) j \u2264 u (i) j u (i) j \u2212 l (i) j (x (i) j \u2212 l (i) j ); j \u2208 I (i) , i \u2208 [L\u22121](12)\nDue to the relaxations, the objective of the LP formulation is always a lower bound of the MIP formulation: f * LP \u2264 f * . A verifier using this formulation is incomplete: if f * LP \u2265 0, then f * \u2265 0 and the property is verified; otherwise, we cannot conclude the sign of f * so the verifier must return \"unknown\". Branch and bound can be used to improve the lower bound and achieve completeness [10,57]. However, in this paper, we work on an orthogonal direction of strengthening the LP formulation by adding cutting planes to obtain larger bounds.\n\nBound propagation methods Instead of solving the LP formulation directly using a LP solver, bound propagation methods aims to quickly give a lower bound for f * LP . For example, CROWN [61] and \u03b2-CROWN [57] propagate a sound linear lower bound backwards for f L (x) with respect to each intermediate layer. For example, suppose we know\nmin x\u2208C f L (x) \u2265 min x\u2208C a (i) x (i) + c (i)(13)\nWith i = L \u2212 1 the above is trivially hold with a (L\u22121) = W (L) , c (L\u22121) = b (L) . In bound propagation methods, a propagation rule propagates an inequality (13) through a previous layer\nx (i) := ReLU(W (i)x(i\u22121) + b (i) )\nto obtain a sound inequality with respect tox (i\u22121) :\nmin x\u2208C f L (x) \u2265 min x\u2208C a (i\u22121) x (i\u22121) + c (i\u22121)\nHere a (i\u22121) , c (i\u22121) can be calculated in close-form via a (i) , c (i) , W (i) , b (i) , l (i) and u (i) such that the bound still holds (see Lemma 1 in [57]). Applying the procedure repeatedly will eventually reach the input layer: min\nx\u2208C f L (x) \u2265 min x\u2208C a (0) x + c (0)(14)\nThe minimization on linear layer can be solved easily when C is a p norm ball to obtain a valid lower bound of f * . Since the bounds propagate layer-by-layer, this process can be implemented efficiently on GPUs [59] without relying on a slow LP solver, which greatly improves the scalability and solving time. Additionally, it is often used to obtain intermediate layer bounds l (i) and and u (i) required for the MIP formulation (6)(7), by treating each x (i) j as the output neuron. The bound propagation rule can either be derived in primal space [61], dual space [58] or abstract interpretations [50]. In Sec.3.1, we will discuss the our bound propagation procedure with general cutting plane constraints.\n\n\nNeural Network Verification with General Cutting Planes\n\n\nGCP-CROWN: General Cutting Planes in Bound Propagation\n\nIn this section, we generalize existing bound propagation method to handle general cutting plane constraints. Our goal is to derive a bound propagation rule similar to CROWN and \u03b2-CROWN discussed in Section 2, however considering additional constraints among any variables within the LP relaxation. To achieve this, we first derive the dual problem of the LP; inspired by the dual formulation, we derive the bound prorogation rule in a layer by layer manner that takes all cutting plane constraints into consideration. The derivation process is inspired by [58,45] and [57].\n\nLP relaxation with cutting planes. In this section, we derive the bound propagation procedure under the presence of general cutting plane constraints. A cutting plane is a constraint involving any variables x (i) (pre-activation),x (i) (post-activation), z (i) (ReLU indicators) from any layer i:\nL\u22121 i=1 h (i) x (i) + g (i) x (i) + q (i) z (i) \u2264 d\nHere h (i) , g (i) and q (i) are coefficients for this cut constraint. The difference between a valid cutting plane and an arbitrary constraint is that a valid cutting plane should not remove any valid integer solution from the MIP formulation. Our new bound propagation procedure can work for any constraints, although in this work we focus on studying the impacts of cutting planes. When there are N cutting planes, we write them in a matrix form:\nL\u22121 i=1 H (i) x (i) + G (i)x(i) + Q (i) z (i) \u2264 d(15)\nwhere H (i) , G (i) , Q (i) \u2208 R N \u00d7di . The LP relaxation with all cutting planes is:\nf * LP-cut = min x,x,z f (x) s.t.\n(3), (2), (4), (5), (6), (7),(9), (10), 0 \u2264 z\n(i) j \u2264 1; j \u2208 I (i) , i \u2208 [L\u22121] L\u22121 i=1 H (i) x (i) + G (i)x(i) + Q (i) z (i) \u2264 d(16)\nSince an additional constraint is added, f * LP-cut \u2265 f * LP and we get closer to f * . Unlike the original LP where each constraint only contains variables from two consecutive layers, our general cutting plane constraint may involve any variable from any layer in a single constraint.\n\nThe dual problem with cutting planes We first show the dual problem for the above LP. The dual problem we consider here is different from existing works in two ways: first, we have constraints with integer variables to support potential cutting planes on z. Additionally, we have cutting planes constraints that may involve variables in any layer, so the dual in previous works such as [58] cannot be directly reused. Our dual problem is given below (derivation details in Appendix A):\nf * LP-cut = max \u03bd,\u00b5\u22650,\u03c4 \u22650 \u03b3\u22650,\u03c0\u22650,\u03b2\u22650 \u2212 \u03bd (1) W (1) x 0 1 \u2212 \u03b2 d \u2212 L i=1 \u03bd (i) b (i) + L\u22121 i=1 j\u2208I (i) \u03c0 (i) j l (i) j \u2212 ReLU(u (i) j \u03b3 (i) j + l (i) j \u03c0 (i) j \u2212 \u03b2 Q (i) :,j ) s.t. \u03bd (L) = \u22121; and for each i \u2208 [L \u2212 1] : \u03bd (i) j = \u03bd (i+1) W (i+1) :,j \u2212 \u03b2 (H (i) :,j + G (i) :,j ); j \u2208 I +(i) \u03bd (i) j = \u2212\u03b2 H (i)\n:,j ; j \u2208 I \u2212(i) and for each j \u2208 I (i) the two equalities below hold:\n\u03bd (i) j = \u03c0 (i) j \u2212 \u03c4 (i) j \u2212 \u03b2 H (i) :,j ; \u03c0 (i) j + \u03b3 (i) j \u2212 \u00b5 (i) j + \u03c4 (i) j = \u03bd (i+1) W (i+1) :,j \u2212 \u03b2 G (i) :,j\nInstead of solving the dual problem exactly, we use it to obtain a lower bound of f * LP-cut . Intuitively, due to the definition of due problem, any valid setting of dual variables leads to a lower bound of f * LP-cut . Informally, starting from \u03bd (L) = \u22121, by applying the constraints in this dual formulation, we can compute \u03bd (L\u22121) , \u03bd (L\u22122) , \u00b7 \u00b7 \u00b7 until \u03bd (1) . The final objective is a function of \u03bd (i) , i \u2208 [N ] and other dual variables. Precisely, our GCP-CROWN bound propagation procedure with general cutting plane constraint is presented in the theorem below (proof in Appendix A): Theorem 3.1 (Bound propagation with general cutting planes). Given any optimizable parameters 0 \u2264 \u03b1 (i) j \u2264 1 and \u03b2 \u2265 0, f * LP-cut is lower bounded by the following objective function, \u03c0\n(i) j * is a function of Q (i) :,j : g(\u03b1, \u03b2) = \u2212 \u03bd (1) W (1) x 0 1 \u2212 L i=1 \u03bd (i) b (i) \u2212 \u03b2 d + L\u22121 i=1 j\u2208I (i) h (i) j (\u03b2) where variables \u03bd (i) are obtained by propagating \u03bd (L) = \u22121 throughout all i \u2208 [L\u22121]: \u03bd (i) j = \u03bd (i+1) W (i+1) :,j \u2212 \u03b2 (H (i) :,j + G (i) :,j ), j \u2208 I +(i) \u03bd (i) j = \u2212\u03b2 H (i) :,j , j \u2208 I \u2212(i) \u03bd (i) j = \u03c0 (i) j * \u2212 \u03b1 (i) j [\u03bd (i) j ] \u2212 \u2212 \u03b2 H (i) :,j , j \u2208 I (i) Here\u03bd (i) j , \u03c0 (i) j * and h (i) j (\u03b2) are defined for each unstable neuron j \u2208 I (i) . \u03bd (i) j := \u03bd (i+1) W (i+1) :,j \u2212 \u03b2 G (i) :,j \u03c0 (i) j * = max min u (i) j [\u03bd (i) j ] + + \u03b2 Q (i) :,j u (i) j \u2212 l (i) j , [\u03bd (i) j ] + , 0 h (i) j (\u03b2) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 l (i) j \u03c0 (i) j * if l (i) j [\u03bd (i) j ] + \u2264 \u03b2 Q (i) :,j \u2264 u (i) j [\u03bd (i) j ] + 0 if \u03b2 Q (i) :,j \u2265 u (i) j [\u03bd (i) j ] + \u03b2 Q (i) :,j if \u03b2 Q (i) :,j \u2264 l (i) j [\u03bd (i) j ] + \u03c0 (i) j * isaf unctionof Q (i) :,j\nBased on Theorem 3.1, to obtain an lower bound of f * LP-cut , we start with any valid setting of 0 \u2264 \u03b1 \u2264 1 and \u03b2 \u2265 0 and \u03bd (L) = \u22121. According to the bound propagation rule, we can compute each \u03bd (i) , i \u2208 [L \u2212 1], in a layer by layer manner. Then objective g(\u03b1, \u03b2) can be evaluated based on all \u03bd (i) to give an lower bound of f * LP-cut . Since any valid setting of \u03b1 and \u03b2 lead to a valid lower bound, we can optimize \u03b1 and \u03b2 using gradient ascent in a similar manner as in [60,57] to tighten this lower bound. The entire procedure can also run on GPU for great acceleration.\n\nConnection to Convex Outer Adversarial Polytope In convex outer adversarial polytope [58], a bound propagation rule was developed in a similar manner in the dual space without considering cutting plane constraints, and is a special case of ours. We denote their bound propagation objective function as g WK which also contains optimizable parameters \u03b1 WK . Proposition 3.2. Given the same input x, perturbation set C, network weights, and N cutting plane constraints, max\n\u03b1,\u03b2 g(\u03b1, \u03b2) \u2265 max \u03b1WK g WK (\u03b1 WK )\nProof. In Theorem 3.1, when all \u03b2 are set to 0, then \u03c0\n(i) j * = u (i) j [\u03bd (i) j ]+ u (i) j \u2212l (i) j and h (i) j (\u03b2) = \u03c0 (i) j * l (i) j ,\nwe recover exactly the same bound propagation equations as in [58]. However, since we allow the addition of cutting plane methods and we can maximize over the additional parameter \u03b2, the objective given by our bound propagation is always at least as good as g WK .\n\nConnection to CROWN-like bound propagation methods CROWN [61] and \u03b1-CROWN [60] use the same bound propagation rule as [58] so Proposition 3.2 also applies, although they were derived from primal space without explicitly formulating the problem as a LP. Salman et al. [45] showed that many other bound propagation methods [50,55] are equivalent to or weaker than [58]. Recently, Wang et al. [57] extends CROWN to \u03b2-CROWN to handle split constraints (e.g., x (i) j \u2265 0). It can be seen a special case as GCP-CROWN where all H, G and Q matrices are zeros except:\nH (i) j,j = 1,j \u2208 I \u2212(i) for x (i) j \u2264 0 split; H (i) j,j = \u22121,j \u2208 I +(i) for x (i) j \u2265 0 split\nIn addition, [20] encodes multi-neuron relaxations using sparse H (i) and G (i) and each cut contains a small number of neurons involving x (i) andx (i) for the same layer i. Wang et al. [57] derived bound propagation rules from both the dual LP and the primal space with a Lagrangian without LP. However, in our case, it is not intuitive to derive bound propagation without LP due to the potential cutting planes on relaxed integer variables z, which do not appear without the explicit LP formulation. Furthermore, although we derived cutting planes for bound propagation methods, it is technically also possible to derive them using other bounding frameworks such as Lagrangian decomposition [8].\n\n\nBranch-and-bound with GCP-CROWN and MIP Solver Generated Cuts\n\nTo build a complete NN verifier, we follow the popular branch-and-bound (BaB) procedure [10,9] in state-of-the-art NN verifiers with GPU accelerated bound propagation method [8,60,16,57], and our GCP-CROWN is used as the bounding procedure in BaB. We refer the readers to Appendix B for a more detailed background on branch-and-bound. Having the efficient bound propagation procedure with general cutting plane constraints, we now need to find a good set of general cutting planes H (i) , G (i) , Q (i) to accelerate NN verification. Since GCP-CROWN can adopt any cutting planes, to fully exploit its power, we propose to use off-the-shelf MIP solvers to generate cutting planes and create an NN verifier combining GPU-accelerated bound propagation with strong cuts generated by a MIP solver. We make the bound propagation on GPU and the MIP solver on CPU run in parallel with cuts added on-the-fly, so the original strong performance of bound-propagation-based NN verifier will never be affected by a potentially slow MIP solver. This allows us to make full use of available computing resource (GPU + CPU). The architecture of our verifier is shown in Fig 1. \n\n\nMIP Solver Coordination Process\n\nGeneric NN Parser  Figure 1: Overview of our cutting-planeenhanced, fully-parallelized NN verifier.\n\nMIP solvers for cutting plane generation Generic MIP solvers such as cplex [28] and gurobi [24] also apply a branch-and-bound strategy, conceptually similar to state-of-the-art NN verifiers. They often tend to be slower than specialized NN verifiers because MIP solvers rely on slower bounding procedures (e.g., Simplex or barrier method) and cannot apply an GPU-accelerated method such as bound propagation or Lagrangian decomposition. However, we still find that MIP solvers are a strong baseline when combined with tight intermediate layer bounds. For example, in the oval21 benchmark in Table 2, \u03b1-CROWN+MIP (MIP solver combined with tight intermediate layer bound computed by \u03b1-CROWN) is able to solve 4 more instances compared to all other tools in the competition. Our investigation found that \u03b1-CROWN+MIP explores much less branches than other state-of-the-art branch-and-bound based NN verifiers, however before branching starts, the MIP solver produces very effective cutting planes that can often verify an instance with little branching. MIP solvers is able to discover sophisticated cutting planes involving several NN layers reflecting complex correlations among neurons, while existing NN verifiers only exploit specific forms of constraints within same layer or between adjacent layers [52,47,40]. This motivates us to combine the strong cutting planes generated by MIP solvers with GPU-accelerated bound-propagation-based BaB.\n\nIn this work, we use cplex as the MIP solver for cutting plane generation since gurobi cannot export cuts. We entirely disable all branching features in cplex and make it focus on finding cutting planes only. These cutting planes are generated only for the root node of the BaB search tree so they are sound for any subdomains with neuron splits in BaB. We conduct branching using our generalized bound propagation procedure in Section (3.1) with the cutting planes generated by cplex.\n\nFully-parallelized NN verifier design We design our verifier as shown in Figure 1. After parsing the NN under verification, we launch a separate process to encode the NN verification problem as MIP problem and start multiple MIP solvers, one for each target label to be verified. At the same time, the main NN verifier process executes branch-and-bound without waiting for the MIP solver process. In each iteration of branch and bound, we query the MIP solving processes and fetch any newly generated cutting planes. If any cutting planes are produced, they are added as H (i) , G (i) , Q (i) in GCP-CROWN and tighten the bounds for subsequent branching and bounding. If no cutting planes are produced, GCP-CROWN reduces to \u03b2-CROWN [57]. Since our verifier is based on strengthening the bounds in \u03b2-CROWN with sound cutting planes, it is also sound and complete.\n\nAdjustments to existing branch and bound method. We implement GCP-CROWN into the \u03b1,\u03b2-CROWN verifier [61,59,57], the winning verifier in VNN-COMP 2021 [3], as the backbone for BaB with bound propagation. To better exploit the power of cutting planes under a fully parallel and asynchronous design, we made a key changes to the BaB procedure. When the number of BaB subdomains are greater than batch size, we rank the subdomains by their lower bounds and choose the easiest domains with largest lower bounds first to verify with GCP-CROWN, unlike most existing verifiers which solve the worst domains first. We use such a order because the MIP solver generates cutting planes incrementally. Solving these easier subdomains tend to require no or fewer cutting planes, so we solve them at earlier stages where cutting planes have not been generated or are relatively weak. On the other hand, if we split worst subdomains first, the number of subdomains will grow quickly, and it can take a long time to verify these domains when stronger cuts become available later. Under a similar rationale, when verifying a multi-class model and BaB needs to verify each  target class label one by one, we start BaB from the easiest label first (\u03b1-CROWN bound closest to 0), allowing the MIP solver to run longer for harder labels, generating stronger cuts for harder labels.\n\n\nExperiments\n\nWe now evaluate our verifier, GCP-CROWN with MIP cuts, on a few popular verification benchmarks. Since our verifier uses a MIP solver in our pipeline, we also include a new baseline, \u03b1-CROWN+MIP, which uses gurobi as the MIP solver with the tightest possible intermediate layer bounds from \u03b1-CROWN [59]. We use the same branch and bound algorithm as in \u03b2-CROWN and we use filtered smart branching (FSB) [16] as the branching heuristic in all experiments. Without cutting planes, GCP-CROWN becomes vanilla \u03b2-CROWN as we share the same code base as \u03b2-CROWN. We include model information and detailed setup for our experiments in Appendix C. GCP-CROWN has been integrated into the \u03b1,\u03b2-CROWN (alpha-beta-CROWN) verifier, and the instructions to reproduce results in this paper are available at http://PaperCode.cc/GCP-CROWN.\n\nResults on the oval20 benchmark in VNN-COMP 2020. oval20 is a popular benchmark consistently used in huge amount of NN verifiers and it perfectly reflects the progress of NN verifiers. We include literature results for many baselines in Table 1. We are the only verifier that can completely solve all three models without any timeout. Our average runtime is significantly lower compared to the time of baselines because we have no timeout (counted as 3600s), and our slowest instance only takes about a few minutes while easy ones only take a few seconds, as shown in Figure 2. Additionally, we often use less number of branches compared to the state-of-the-art verifier, \u03b2-CROWN, since our strong cutting planes help us to eliminate many hard to solve subdomains in BaB. Furthermore, we highlight that \u03b1-CROWN+MIP also achieves a low timeout rate, although it is much slower than our bound propagation based approach combined with cuts from a MIP solver.\n\nResults on VNN-COMP 2021 benchmarks. Among the eight scoring benchmarks in VNN-COMP 2021 [3], only two (oval21 and cifar10-resnet) are most suitable for the evaluation of this work. Among other benchmarks, acasxu and nn4sys have low input dimensionality and require input space branching rather than ReLU branching; verivital, mnistfc, and eran bench-  marks consist of small MLP networks that can be solved directly by MIP solvers; marabou contains mostly adversarial examples, making it a good benchmark for falsifiers rather than verifiers. We present our results in Table 2. Besides results from 6 VNN-COMP 2021 participants, we also include two additional baselines, \u03b1-CROWN+MIP (same as in Table 1), and MN-BaB [20], a recently proposed branch and bound framework with multi-neuron relaxations [39], which can be viewed as a restricted form of cutting planes. On the oval21 benchmark, OVAL, VeriNet and \u03b1,\u03b2-CROWN are the best performing tools, verified 11 out of 27 instances, while we can verify twice more instances (22 out of 27) on this benchmark. On the cifar10-resnet benchmark, our verifier also solves the most number of instances and achieves the lowest average time. In fact, \u03b1-CROWN+MIP is also a strong baseline, solving 4 more instances than all competition participants, showing the importance of strong cutting planes. Our GCP-CROWN with MIP cuts combines the benefits of fast bound propagation on GPU with the strong cutting planes generated by a MIP solver and achieves the best performance. We present a more detailed analysis on the cutting planes used in this benchmark in Appendix C.2.\n\nThe oval21 benchmark was also included as part of VNN-COMP 2022, concluded in July 2022, with a different sample of 30 instances. GCP-CROWN is part of the winning tool in VNN-COMP 2022, \u03b1,\u03b2-CROWN, which verified 25 out of 30 instances in this benchmark, outperforming the second place tool (MN-BaB [20] with multi-neuron relaxations) with 19 verified instances by a large margin. More results on VNN-COMP 2022 can be found in these slides 1 .\n\nResults on SDP-FO benchmarks. We further evaluate our method on the SDP-FO benchmarks in [15,57]. This benchmark contains 7 mostly adversarially trained MNIST and CIFAR models with 200 instances each, which are hard for many existing verifiers. Beyond the baselines reported in [57], we also include two additional baselines, \u03b1-CROWN+MIP (same as in Table 1) and MN-BaB [20] (same as in Table 2). Table 3 shows that our method improves the percentage of verified images (\"verified accuracy\") on all models compared to state-of-the-art verifiers, further closing the gap between verified accuracy and empirical robust accuracy obtained by PGD attack (reported as \"upper bound\" in Table 3).\n\n\nRelated Work\n\nCutting plane method is a classic technique to strengthen the convex relaxation of an integer programming problem. Generic cutting planes such as Gomory's cut [21,22], Chv\u00e1tal-Gomory cut [12], implied bound cut [27], lift-and-project [35], reformulation-linearization techniques [46] and mixed integer rounding cuts [41,37] can be applied to almost any LP relaxed problems, and problem specific cutting planes such as Knapsack cut [14], Flow-cover cut [42] and Clique cut [29] require specific problem structures. Modern MIP solvers typically uses a branch-and-cut strategy, which tends to generate a large number of cuts before starting the next iteration of branching, and solve the LP relaxation of the MIP problem with cutting planes with an exact method such as the Simplex method. Our GCP-CROWN is a specialized solver for the NN verification problem, which can quickly obtain a lower bound of the LP relaxation with cutting planes specially for the NN verification problem.\n\nThe verification of piece-wise linear NNs can be formulated as a MIP problem, so early works [30,31,53] solve an integer or combinatorial formulation directly. For efficiency reasons, most recent works use a convex relaxation such as linear relaxation [19,58] or semidefinite relaxation [44,15]. Salman et al. [45] discussed the limitation of many convex relaxation based NN verification algorithms and coined the term \"convex relaxation barrier\", specifically for the popular single-neuron \"Planet\" relaxation [19]. Several works developed novel techniques to break this barrier. [47] added constraints that depends on the aggregation of multiple neurons, and these constraints were passed to a LP solver. [40] enhanced the multi-neuron formulation of [47] to obtain tighter relaxations. [1] studied stronger convex relaxations for a ReLU neuron after an affine layer, and [52] constructed efficient algorithms based on this relaxation for incomplete verification. [16] extended the formulation in [1] to a dual form and combined it with branch and bound to achieve completeness. [7] proposed specialized cuts by considering neuron dependencies and solve them using a MIP solver. [20] combined the multi-neuron relaxation [40] with branch and bound. Although these works can be seen as a special form of cutting planes, they mostly focused on enhancing the relaxation for several neurons within a single layer or two adjacent layers. GCP-CROWN can efficiently handle general cutting plane constraints with neurons from any layers in a bound propagation manner, and the cutting planes we find from a MIP solver can be seen as tighter convex relaxations encoding multi-neuron and multi-layer correlations.\n\n\nConclusion\n\nIn this paper, we propose GCP-CROWN, an efficient and GPU-accelerated bound propagation method for NN verification capable of handling any cutting plane constraints. We combine GCP-CROWN with branch and bound and high quality cutting planes generated by a MIP solver to tighten the convex relaxation for NN verification. The combination of fast bound propagation and strong cutting planes lead to state-of-the-art verification performance on multiple benchmarks. Our work opens up a great opportunity for studying more efficient and powerful cutting planes for NN verification.\n\nLimitations of this work Our work generalizes existing bound propagation methods that can handle only simple constraints (such as neuron split constraints in \u03b2-CROWN [57]) to general constraints, and we share a few common limitations as in previous works [10,60,16]: the branchand-bound procces and bound propagation procedure are developed on ReLU networks, and it can be non-trivial to extend it to neural networks with non-piecewise-linear operations. In addition, we currently directly use cutting planes generated by a generic MIP solver, and there might exist stronger and faster cutting plane methods that can exploit the structure of the neural network verification problem. We hope these limitations can be addressed in future works.\n\nPotential Negative Societal Impact Our work focuses on formally proving desired properties of a neural network under investigation such as safety and robustness, which is an important direction of trustworthy machine learning and has overall positive societal impact. Since our verifier is a complete verifier, it might be possible to use it to find weakness of a neural network and guide adversarial attacks. However, we believe that formally characterizing a model's behavior and potenti al weakness is important for building robust models and preventing real-world malicious attack. In Section A we derive the bound propagation procedure of GCP-CROWN and give a proof for Theorem 3.1 (GCP-CROWN bound propagation). In Section B we give additional background on branch and bound. In section C we give more details of experiments, more results as well as a case study for cutting planes used in GCP-CROWN with MIP cuts.\n\nA The dual problem with cutting planes\n\nIn this section we derive the dual formulation for neural network verification problem with general cutting planes (or arbitrarily added linear constraints across any layers). Our derivation is based on the linearly relaxed MIP formulation with original binary variables z intact, to allow us to add cuts also to the integer variable (the mostly commonly used triangle relaxation does not have z). We first write the full LP relaxation with arbitrary cutting planes, as well as their corresponding dual variables:\nf * LP-cut = min x,x,z f (x) s.t. f (x) = x (L) ; x 0 \u2212 \u2264 x \u2264 x 0 + ; (2) x (i) = W (i)x(i\u22121) + b (i) ; i \u2208 [L], \u21d2 \u03bd (i) \u2208 R di (3) x (i) j \u2265 0; j \u2208 I (i) \u21d2 \u00b5 (i) j \u2208 R (4) x (i) j \u2265 x (i) j ; j \u2208 I (i) \u21d2 \u03c4 (i) j \u2208 R (5) x (i) j \u2264 u (i) j z (i) j ; j \u2208 I (i) \u21d2 \u03b3 (i) j \u2208 R (6) x (i) j \u2264 x (i) j \u2212 l (i) j (1 \u2212 z (i) j ); j \u2208 I (i) \u21d2 \u03c0 (i) j \u2208 R(7)\nx (i)\nj = x (i) j ; j \u2208 I +(i)(9)\nx (i)\nj = 0; j \u2208 I \u2212(i) (10) 0 \u2264 z (i) j \u2264 1; j \u2208 I (i) , i \u2208 [L](11)L\u22121 i=1 H (i) x (i) + G (i)x(i) + Q (i) z (i) \u2264 d \u21d2 \u03b2 \u2208 R N(15)\nThe Lagrangian function can be constructed as:\nf * LP-cut = min x,x,z max \u03bd,\u00b5,\u03c4 ,\u03b3,\u03c0,\u03b2 x (L) + L i=1 \u03bd (i) x (i) \u2212 W (i)x(i\u22121) + b (i) + L\u22121 i=1 j\u2208I (i) \u00b5 (i) j (\u2212x (i) j ) + \u03c4 (i) j (x (i) j \u2212x (i) j ) + \u03b3 (i) j (x (i) j \u2212 u (i) j z (i) j ) + \u03c0 (i) j (x (i) j \u2212 x (i) j + l (i) j \u2212 l (i) j z (i) j ) + \u03b2 L\u22121 i=1 H (i) x (i) + G (i)x(i) + Q (i) z (i) \u2212 d s.t.x (i) j = 0, j \u2208 I \u2212(i) ;x (i) j = x (i) j , j \u2208 I +(i) ; x 0 \u2212 \u2264 x \u2264 x 0 + ; 0 \u2264 z (i) j \u2264 1, j \u2208 I (i) \u00b5 \u2265 0; \u03c4 \u2265 0; \u03b3 \u2265 0; \u03c0 \u2265 0; \u03b2 \u2265 0\nHere \u00b5, \u03c4 , \u03b3, \u03c0 are shorthands for all dual variables in each layer and each neuron. Note that for some constraints, their dual variables are not created because they are trivial to handle in the step steps. Rearrange the equation and swap the min and max (strong duality) gives us:\nf * LP-cut = max \u03bd,\u00b5,\u03c4 ,\u03b3,\u03c0,\u03b2 min x,x,z (\u03bd (L) + 1)x (L) \u2212 \u03bd (1) W (1)x(0) + L\u22121 i=1 j\u2208I +(i) \u03bd (i) j + \u03b2 H (i) :,j \u2212 \u03bd (i+1) W (i+1) :,j + \u03b2 G (i) :,j x (i) j + L\u22121 i=1 j\u2208I \u2212(i) \u03bd (i) j + \u03b2 H (i) :,j x (i) j + L\u22121 i=1 j\u2208I (i) \u03bd (i) j + \u03b2 H (i) :,j + \u03c4 (i) j \u2212 \u03c0 (i) j x (i) j + \u2212\u03bd (i) W (i) :,j \u2212 \u00b5 (i) j \u2212 \u03c4 (i) j + \u03b3 (i) j + \u03c0 (i) j + \u03b2 G (i) :,j x (i) j + \u2212u (i) j \u03b3 (i) j \u2212 l (i) j \u03c0 (i) j + \u03b2 Q (i) :,j z (i) j \u2212 L i=1 \u03bd (i) b (i) + L\u22121 i=1 j\u2208I (i) \u03c0 (i) j l (i) j \u2212 \u03b2 d s.t. x 0 \u2212 \u2264 x \u2264 x 0 + ; 0 \u2264 z (i) j \u2264 1, j \u2208 I (i) \u00b5 \u2265 0; \u03c4 \u2265 0; \u03b3 \u2265 0; \u03c0 \u2265 0; \u03b2 \u2265 0\nHere W (i+1) :,j denotes the j-th column of W (i+1) . Note that for the term involving j \u2208 I +(i) we have replacedx (i) with x (i) to obtain the above equation. For the the term x (i) j , j \u2208 I +(i) it is always 0 so does not appear.\n\nSolving the inner minimization gives us the dual formulation:\nf * LP-cut = max \u03bd,\u00b5,\u03c4 ,\u03b3,\u03c0,\u03b2 \u2212 \u03bd (1) W (1) x 0 1 \u2212 L i=1 \u03bd (i) b (i) \u2212 \u03b2 d + L\u22121 i=1 j\u2208I (i) \u03c0 (i) j l (i) j \u2212 ReLU(u (i) j \u03b3 (i) j + l (i) j \u03c0 (i) j \u2212 \u03b2 Q (i) :,j )(17)s.t. \u03bd (L) = \u22121 (18) \u03bd (i) j = \u03bd (i+1) W (i+1) :,j \u2212 \u03b2 (H (i) :,j + G (i) :,j ), for j \u2208 I +(i) , i \u2208 [L \u2212 1](19)\u03bd (i) j = \u2212\u03b2 H (i) :,j , for j \u2208 I \u2212(i) , i \u2208 [L \u2212 1](20)\u03bd (i) j = \u03c0 (i) j \u2212 \u03c4 (i) j \u2212 \u03b2 H (i) :,j for j \u2208 I (i) , i \u2208 [L \u2212 1](21)\u03c0 (i) j + \u03b3 (i) j \u2212 \u00b5 (i) j + \u03c4 (i) j = \u03bd (i+1) W (i+1) :,j \u2212 \u03b2 G (i) :,j , for j \u2208 I (i) , i \u2208 [L \u2212 1](22)s.t. \u00b5 \u2265 0; \u03c4 \u2265 0; \u03b3 \u2265 0; \u03c0 \u2265 0; \u03b2 \u2265 0\nThe \u00b7 1 comes from minimizing over x 0 with the constraint x 0 \u2212 \u2264 x \u2264 x 0 + , and the ReLU(\u00b7) term comes from minimizing over z (i) j with the constraint 0 \u2264 z (i) j \u2264 1. Before we give the bound propagation procedure, we first give a technical lemma:\n\nLemma A.1. Given u \u2265 0, l \u2264 0, \u03c0 \u2265 0, \u03b3 \u2265 0, and \u03c0 + \u03b3 = C, and define the function: g(\u03c0, \u03b3) = \u2212ReLU(u\u03b3 + l\u03c0 + q) + l\u03c0 Then max \u03c0\u22650,\u03b3\u22650\ng(\u03c0, \u03b3) = \uf8f1 \uf8f2 \uf8f3 l\u03c0 * , if \u2212uC \u2264 q \u2264 \u2212lC 0, if q < \u2212uC \u2212q, if q > \u2212lC\nwhere the optimal values for \u03c0 and \u03b3 are:\n\u03c0 * = max min uC + q u \u2212 l , C , 0 , \u03b3 * = max min \u2212lC \u2212 q u \u2212 l , C , 0\nProof. Case 1: when u\u03b3 + l\u03c0 + q \u2265 0, the objective becomes:\nmax \u03c0\u22650,\u03b3\u22650 g(\u03c0, \u03b3) = \u2212u\u03b3 \u2212 q s.t. \u03c0 + \u03b3 = C u\u03b3 + l\u03c0 + q \u2265 0\nSince q is a constant and the object only involves a non-negative variable \u03b3 with non-positive coefficient \u2212u, \u03b3 needs to be as smaller as possible as long as the constraint is satisfied. Substitute \u03c0 = C \u2212 \u03b3 into the constraint u\u03b3 + l\u03c0 + q \u2265 0,\nu\u03b3 + l(C \u2212 \u03b3) + q \u2265 0 \u21d2 \u03b3 \u2265 \u2212lC \u2212 q u \u2212 l\nConsidering \u03b3 is within [0, C], the optimal \u03b3 and \u03c0 are:\n\u03b3 * = max min \u2212lC \u2212 q u \u2212 l , C , \u03c0 * = max min uC + q u \u2212 l , C\nCase 2: when u\u03b3 + l\u03c0 + q \u2264 0, the objective becomes:\nmax \u03c0\u22650,\u03b3\u22650 g(\u03c0, \u03b3) = l\u03c0 s.t. \u03c0 + \u03b3 = C u\u03b3 + l\u03c0 + q \u2264 0\nSince q is a constant and the object only involves a non-negative variable \u03c0 with non-positive coefficient l, \u03c0 needs to be as smaller as possible as long as the constraint is satisfied. Substitute \u03b3 = C \u2212 \u03c0 into the constraint u\u03b3 + l\u03c0 + q \u2264 0,\nu(C \u2212 \u03c0) + l\u03c0 + q \u2264 0 \u21d2 \u03c0 \u2265 uC + q u \u2212 l\nThe optimal \u03c0 * and \u03b3 * are the same as in case 1. The optimal objective can be obtained by substitute \u03c0 * and \u03b3 * into g(\u03c0, \u03b3). The objective depends on q because when q is too large (q \u2265 \u2212lC) or too small (q \u2264 \u2212uC), \u03c0 * and \u03b3 * are fixed at either 0 or C.\n\nWith this lemma, we are now ready to prove Theorem 3.1, the bound propagation rule with general cutting planes (GCP-CROWN): Theorem 3.1 (Bound propagation with general cutting planes). Given the following bound propagation rule on \u03bd with optimizable parameter 0 \u2264 \u03b1 (i) j \u2264 1 and \u03b2 \u2265 0:\n\u03bd (L) = \u22121 \u03bd (i) j = \u03bd (i+1) W (i+1) :,j \u2212 \u03b2 (H (i) :,j + G (i) :,j ), j \u2208 I +(i) , i \u2208 [L \u2212 1] \u03bd (i) j = \u2212\u03b2 H (i) :,j , j \u2208 I \u2212(i) , i \u2208 [L \u2212 1] \u03bd (i) j := \u03bd (i+1) W (i+1) :,j \u2212 \u03b2 G (i) :,j j \u2208 I (i) , i \u2208 [L \u2212 1] \u03bd (i) j := max min u (i) j [\u03bd (i) j ] + + \u03b2 Q (i) :,j u (i) j \u2212 l (i) j , [\u03bd (i) j ] + , 0 \u2212 \u03b1 (i) j [\u03bd (i) j ] \u2212 \u2212 \u03b2 H (i) :,j j \u2208 I (i) , i \u2208 [L \u2212 1] = 0, \u03b3 (i) j = 0, \u03c0 (i) j = \u00b5 (i) j = 0 or \u03c0 (i) j = 0, \u00b5 (i) j = 0, \u03c4 (i) j = \u03b3 (i) j = 0.\nThis situation may happen when u j are loose bounds so this situation will not occur. The same argument is also applicable to [58]. A unstable ReLU neuron (e.g., x 1 may receive either negative or positive inputs for some input x in perturbation set C. On the other hand, a stable ReLU neuron is always positive or negative for all x \u2208 C, so it is a linear operation and branching is not needed.\n\nHere 0 \u2264 \u03b1 (i) j \u2264 1 is an optimizable parameter that can be updated via its gradient during bound propagation, and any 0 \u2264 \u03b1 (i) j \u2264 1 produces valid lower bounds. All above substitutions replace each dual variable \u03c0, \u03b3 and \u00b5 to a valid setting in the dual formulation, so the final objective g(\u03b1, \u03b2) is always a sound lower bound of f * LP-cut . This theorem establishes the soundness of GCP-CROWN.\n\nNote that an optimal setting of \u03b1 and \u03b2 do not necessarily lead to the optimal primal value f * LP-cut . The main reason is that (25) gives a closed-form solution for \u03c0 and \u03b3 to eliminate these variables without considering other dual variables like \u03bd to simplify the bound propagation process. To achieve theoretical optimality, \u03c0 and \u03b3 can also be optimized.\n\n\nB More technical details and background of GCP-CROWN with MIP cuts\n\nBranch-and-bound Our work is based on branch-and-bound (BaB), a powerful framework for neural network verification [10] which many state-of-the-art verifiers are based on [57,16,26,20]. Here we give a brief introduction for the concept of branch-and-bound for readers who are not familiar with this field.\n\nWe illustrate the branch-and-bound process in Figure 3. Neural network verification seeks to minimize the objective (1): f * = min x f (x), \u2200x \u2208 C, where f (x) is a ReLU network under our setting. A positive f * indicates that the network can be verified. However, since ReLU neurons are non-linear, this optimization problem is non-convex, and it usually requires us to relax ReLU neurons with linear constraints (e.g., the LP relaxation in (11)) to obtain a lower bound for objective f * . This lower bound (\u22123.0 at the root node in Figure 3) might become negative, even if f * (the unknown ground-truth) is positive. Branch-and-bound is a systematic method to tighten this lower bound. First, we split an unstable ReLU neuron, such as x 3 , into two cases: x 3 \u2265 0 and x 3 \u2264 0 (the branching step), producing two subproblems each with an additional constraint x 3 \u2265 0 or x 3 \u2264 0. In each subproblem, neuron x 3 does not need to be relaxed anymore, so the lower bound of f * becomes tighter in each of the two subdomains in the subsequent bounding step (\u22123.0 becomes \u22122.0 and 0.5) in Figure 3. Any subproblem with a positive lower bound is successfully verified and no further split is needed. Then, we repeat the branching and bounding steps on subproblems that still have a negative lower bound. We terminate when all unstable neurons are split or all subproblems are verified. In Figure 3, all leaf subproblems have positive lower bounds so the network is verified. On the other hand, if we have split all unstable neurons and there still exist domains with negative bounds, a counter-example can be constructed.\n\nThe contribution of this work is to improve the bounding step using cutting planes. As one can observe in Figure 3, if we can make the bounds tighter, they approach to zero faster and less branching is needed. Since the number of the subproblems may be exponential to the number of unstable neurons in the worst case, it is crucial to improve the bounds quickly. Informally, cutting planes are additional valid constraints for the relaxed subproblems, and adding these additional constraints as in (16) makes the lower bounds tighter. We refer the readers to integer programming literature [5,13] for more details on cutting plane methods. Existing neural network verifiers mostly use bound propagation method [61,50,60,57,16] for the bounding step thanks to their efficiency and scalability, but none of them can handle general cutting planes. Our GCP-CROWN is the first bound propagation method that supports general cutting planes, and we demonstrated its effectiveness in Section 4 when combined with cutting planes generated by a MIP solver.\n\nSoundness and Completeness GCP-CROWN is sound because Theorem 3.1 guarantees that we always obtain a sound lower bound of the verification problem as long as valid cutting planes (generated by a MIP solver in our case) are added. Our method, when combined with branch-andbound, is also complete because we provide a strict improvement in the bounding step over existing methods such as [57], and the arguments for completeness in existing verifiers using branch-andbound on splitting ReLU neurons such as [9,57] are still valid for this work.\n\n\nC More details on experiments C.1 Experimental Setup\n\nOur experiments are conducted on a desktop with an AMD Ryzen 9 5950X CPU, one NVIDIA RTX 3090 GPU (24GB GPU memory), and 64GB CPU memory. Our implementation is based on the open-source \u03b1,\u03b2-CROWN verifier 3 with cutting plane related code added. Both \u03b1-CROWN+MIP and GCP-CROWN with MIP cuts use all 16 CPU cores and 1 GPU. gurobi is used as the MIP solver in \u03b1-CROWN+MIP. Although gurobi usually outperforms other MIP solvers for NN verification problems, it cannot export cutting planes, so our cutting planes are acquired by the cplex [28] solver (version 22.1.0.0). We use the Adam optimizer [32] to solve both \u03b1 and \u03b2 with 20 iterations. The learning rates are set as 0.1 and 0.02 (0.01 for oval20) for optimizing \u03b1 and \u03b2 respectively. We decay the learning rates with a factor of 0.9 (0.8 for oval20) per iteration. Timeout for properties in oval20 are set as 1 hour follow the original benchmark. Timeout for oval21 and cifar10-resnet are set as 720s and 300s respectively, the same as in VNN-COMP 2021. Timeout for SDP-FO models are set as 600s for \u03b1-CROWN+MIP and MN-BaB, and a shorter 200s timeout is used for GCP-CROWN with MIP cuts. We summarize the model structures and batch size used in our experiments in Table 4. The CIFAR-10 Base, Wide and Deep models are used in oval20 and oval21 benchmarks. Table 4: Model structures used in our experiments. The notation Conv(a, b, c) stands for a conventional layer with a input channel, b output channels and a kernel size of c \u00d7 c. Linear(a, b) stands for a fully connected layer with a input features and b output features. ResBlock(a, b) stands for a residual block that has a input channels and b output channels. We have ReLU activation functions between two consecutive linear or convolutional layers. tighten the lower bounds and strengthen verification performance. We use the oval21 as a sample case study because GCP-CROWN with MIP cuts is very effective on this benchmark.\n\nThe oval21 benchmark has 30 instances, each with 9 target labels (properties) to verify. Among the total of 270 properties, we filter out the easy cases where fast incomplete verifiers like \u03b1-CROWN can verify directly, and 39 hard properties remain which must be solved using branch and bound and/or cutting planes. Cutting planes are generated on these hard properties and they greatly help GCP-CROWN.\n\nNumber of cuts used to verify each property. The maximal number of cuts applied per property is 4,162 and the minimal number is 318. On average, we have 1,683 cuts applied to our GCP-CROWN to solve these hard properties.\n\nImprovements on lower bounds. In branch-and-bound, we lower bound the objective of Eq. 1 with ReLU split constraints [57]. A tighter lower bound can reduce the number of branches used and usually leads to stronger verification. We measure how well the cuts generated by off-the-shelf solvers in improving the tightness of the lower bound for verification. Without branching and cutting planes, the average \u03b1-CROWN lower bound is -2.54. With generated cutting planes, we can improve the lower bound by 0.51 on average and can directly verify 4 out of 39 hard properties without branching. The lower bound without branching can be maximally improved by 1.54 and minimally improved by 0.04.\n\nStructure of Generated Cuts. Finally, we investigate the variables involved in each generated cutting plane. In total, the MIP solver generates 65,647 cutting planes in total for the 39 hard properties. 65,301 of the cuts involve variables across multiple layers. It indicates that single layer cutting planes (e.g., constraints involving pre-and post-activation variables of a single ReLU layer only) commonly used in previous works [47,40] are not optimal in general. Additionally, all of 65,647 cuts have at least one ReLU integer variable z (i) used, which was not supported in existing bound propagation methods. 65,197 of all cuts involve at least one variable of input neurons. 23,600 of all cuts have at least one pre-ReLU variables and 51,617 of them have at least one post-ReLU variables.\n\n\ns.t.(3), (2), (4), (5), (9), (10),x\n\nFigure 2 :\n2Percentage of solved properties on the oval20 benchmark vs. running time (timeout 1 hour).\n\nj\nis exactly tight and the solution x(i) j is at the intersection of one lower and upper linear equatlities. Practically, this can be avoided by adding a small \u03b4 to u (i) j or l (i) j , and in most scenarios u (i) j are l (i)\n\nFigure 3 :\n3Branch and Bound (BaB) for neural network verification.\n\nTable 1 :\n1Average runtime and average number of branches on oval20 benchmarks with 100 properties per model. Timeout is set to 3,600 seconds (consistent with other literature results). GCP-CROWN is the only method that can completely solve all instances (0% timeout) and the average time per-instance is less than 5 seconds on all three networks.CIFAR-10 Base \nCIFAR-10 Wide \nCIFAR-10 Deep \n\nMethod \ntime(s) \nbranches \n%timeout time(s) \nbranches \n%timeout time(s) \nbranches %timeout \n\nMIPplanet [19] \n2849.69 \n-\n68.00 \n2417.53 \n-\n46.00 \n2302.25 \n-\n40.00 \nBaBSR [9] \n2367.78 \n1020.55 \n36.00 \n2871.14 \n812.65 \n49.00 \n2750.75 \n401.28 \n39.00 \nGNN-online [36] \n1794.85 \n565.13 \n33.00 \n1367.38 \n372.74 \n15.00 \n1055.33 \n131.85 \n4.00 \nBDD+ BaBSR [8] \n807.91 195480.14 \n20.00 \n505.65 \n74203.11 \n10.00 \n266.28 12722.74 \n4.00 \nFast-and-Complete [60] \n695.01 119522.65 \n17.00 \n495.88 \n80519.85 \n9.00 \n105.64 \n2455.11 \n1.00 \nOVAL (BDD+ GNN)  *  [8, 36] 662.17 \n67938.38 \n16.00 \n280.38 \n17895.94 \n6.00 \n94.69 \n1990.34 \n1.00 \nA.set BaBSR [16] \n381.78 \n12004.60 \n7.00 \n165.91 \n2233.10 \n3.00 \n190.28 \n2491.55 \n2.00 \nBigM+A.set BaBSR [16] \n390.44 \n11938.75 \n7.00 \n172.65 \n4050.59 \n3.00 \n177.22 \n3275.25 \n2.00 \nBaDNB (BDD+ FSB)[17] \n309.29 \n38239.04 \n7.00 \n165.53 \n11214.44 \n4.00 \n10.50 \n368.16 \n0.00 \nERAN  *  [47, 48, 50, 49] \n805.94 \n-\n5.00 \n632.20 \n-\n9.00 \n545.72 \n-\n0.00 \n\u03b2-CROWN [57] \n118.23 208018.21 \n3.00 \n78.32 \n116912.57 \n2.00 \n5.69 \n41.12 \n0.00 \n\u03b1-CROWN+MIP \u2020 \n335.50 \n8523.37 \n3.00 \n203.87 \n2029.60 \n0.00 \n76.90 \n1364.24 \n0.00 \nGCP-CROWN with MIP cuts \n4.07 \n2580.53 \n0.00 \n3.02 \n2095.18 \n0.00 \n3.87 \n110.92 \n0.00 \n\n* Results from VNN-COMP 2020 report [34].  \u2020 A new baseline proposed and evaluated in this work, not presented in previous papers. \n\n\n\nTable 2 :\n2VNN-COMP 2021 benchmarks: oval21 and cifar10-resnet. Results marked with VNN-COMP are from publicly available benchmark data on VNN-COMP 2021 Github. \"-\" indicates unsupported model.oval21 (30 properties; PGD upper bound 27) \ncifar10-resnet (72 properties) \n\nMethod \ntime(s) \n# verified \n%timeout \ntime(s) \n# verified \n%timeout \n\nnnenum  *  [2, 4] \n630.06 \n2 \n86.66 \n-\n-\n-\nMarabou  *  [31] \n429.13 \n5 \n73.33 \n157.70 \n39 \n45.83 \nERAN  *  [40, 38] \n233.84 \n6 \n70.00 \n129.48 \n43 \n40.28 \nOVAL  *  [17, 16] \n393.14 \n11 \n53.33 \n-\n-\n-\nVeriNet  *  [25, 26] \n414.61 \n11 \n53.33 \n105.91 \n48 \n33.33 \n\u03b1,\u03b2-CROWN  *  [61, 60, 57] \n395.32 \n11 \n53.33 \n99.87 \n58 \n19.44 \nMN-BaB [20] \n435.46 \n10 \n56.66 \n-\n-\n-\nVenus2 \u2020 [7, 33] \n386.71 \n17 \n33.33 \n-\n-\n-\n\u03b1-CROWN+MIP \n301.23 \n15 \n40.00 \n125.48 \n46 \n36.11 \nGCP-CROWN with MIP cuts \n145.26 \n23 \n13.33 \n53.49 \n63 \n12.5 \n\n* Results from VNN-COMP 2021 report [3].  \u2020 We use the latest code of Venus2 in the vnncomp branch, committed on \nJul 18, 2022. Older versions cannot run these convolutional networks. \n\n\n\nTable 3 :\n3Verified accuracy (%) and avg. per-example verification time (s) on 7 models from SDP-FO[15].We run \u03b1-CROWN+MIP and MN-BaB with 600s timeout threshold for all models. \"-\" indicates that we could not run a model due to unsupported model structure or other errors. We run our GCP-CROWN with MIP cuts with a shorter 200s timeout for all models and it achieves better verified accuracy than all other baselines. Other results are reported from[57].Dataset \nModel \nSDP-FO [15]  *  \nPRIMA [40] \n\u03b2-CROWN [57] \nMN-BaB [20] \nVenus2 [7, 33] \n\u03b1-CROWN+MIP \nGCP-CROWN \nUpper \n= 0.3 and = 2/255 \nVerified% Time (s) Ver.% Time(s) Ver.% Time(s) Ver.% Time(s) Ver.% Time(s) Ver.% Time(s) Ver.% Time(s) bound \n\nMNIST \nCNN-A-Adv \n43.4 \n>20h \n44.5 \n135.9 \n70.5 \n21.1 \n-\n-\n35.5 \n148.4 \n56.5 \n224.3 \n72.0 \n19.9 \n76.5 \n\nCIFAR \n\nCNN-B-Adv \n32.8 \n>25h \n38.0 \n343.6 \n46.5 \n32.2 \n-\n-\n-\n-\n27.0 \n360.6 \n48.5 \n57.8 \n65.0 \nCNN-B-Adv-4 \n46.0 \n>25h \n53.5 \n43.8 \n54.0 \n11.6 \n-\n-\n-\n-\n52.5 \n129.5 \n59.0 \n21.5 \n63.5 \nCNN-A-Adv \n39.6 \n>25h \n41.5 \n4.8 \n44.0 \n5.8 \n42.5 \n68.3 \n47.5 \n26.0 \n46.0 \n63.1 \n48.5 \n9.8 \n50.0 \nCNN-A-Adv-4 \n40.0 \n>25h \n45.0 \n4.9 \n46.0 \n5.6 \n46.0 \n37.7 \n47.5 \n13.1 \n48.5 \n16.4 \n48.5 \n5.7 \n49.5 \nCNN-A-Mix \n39.6 \n>25h \n37.5 \n34.3 \n41.5 \n49.6 \n35.0 \n140.3 \n33.5 \n72.4 \n32.5 \n231.3 \n47.5 \n29.2 \n53.0 \nCNN-A-Mix-4 \n47.8 \n>25h \n48.5 \n7.0 \n50.5 \n5.9 \n49.0 \n70.9 \n49.0 \n37.3 \n52.5 \n77.7 \n55.5 \n12.4 \n57.5 \n\n* \n\n\nA formal competition report is under preparation by VNN-COMP organizers; scores were presented in FLoC 2022: https://drive.google.com/file/d/1nnRWSq3plsPvOT3V-drAF5D8zWGu02VF/view.\nIn theorey, it is possible that \u03c4 (i) j\nhttps://github.com/huanzhang12/alpha-beta-CROWN\nAppendixThen f * LP-cut is lower bounded by the following objective with any valid 0 \u2264 \u03b1 \u2264 1 and \u03b2 \u2265 0:Proof. Given(22), for j \u2208 I (i) , observing that the upper and lower bounds of ReLU relaxations for a single neuron cannot be tight simultaneously[58], \u03c0To avoid clutter, we define\u03bd:,j , and we define [ \u00b7 ] + := max(0, \u00b7) and [ \u00b7 ] \u2212 := min(0, \u00b7).To derived the proposed bound propagation rule, in(17), we must eliminate variable \u03b3Ignoring terms related to \u03bd, we observe that we can optimize the term \u03c0:,j ) for each j for I (i) individually. We seek the optimal solution for the follow optimization problem on function h:Note that we optimize over \u03c0 (i) j and \u03b3 (i) j here and treat \u03b2 as a constant. Applying Lemma A.1 with \u03c0 = \u03c0 (i)j ] + we obtain the optimal objective for(25):Substitute this solution of \u03c0 (i) j into(21), and also observe that due to(24), \u03c4\nStrong convex relaxations and mixed-integer programming formulations for trained neural networks. Ross Anderson, Joey Huchette, Christian Tjandraatmadja, Juan Pablo Vielma, Mathematical Programming. Ross Anderson, Joey Huchette, Christian Tjandraatmadja, and Juan Pablo Vielma. Strong convex relaxations and mixed-integer programming formulations for trained neural networks. In Mathematical Programming, 2020.\n\nnnenum: Verification of relu neural networks with optimized abstraction refinement. Stanley Bak, NASA Formal Methods Symposium. SpringerStanley Bak. nnenum: Verification of relu neural networks with optimized abstraction refinement. In NASA Formal Methods Symposium, pages 19-36. Springer, 2021.\n\nThe second international verification of neural networks competition. Stanley Bak, Changliu Liu, Taylor Johnson, arXiv:2109.00498Summary and results. arXiv preprintStanley Bak, Changliu Liu, and Taylor Johnson. The second international verification of neural networks competition (vnn-comp 2021): Summary and results. arXiv preprint arXiv:2109.00498, 2021.\n\nImproved geometric path enumeration for verifying relu neural networks. Stanley Bak, Hoang-Dung Tran, Kerianne Hobbs, Taylor T Johnson, Proceedings of the 32nd International Conference on Computer Aided Verification. the 32nd International Conference on Computer Aided VerificationSpringerStanley Bak, Hoang-Dung Tran, Kerianne Hobbs, and Taylor T. Johnson. Improved geometric path enumeration for verifying relu neural networks. In Proceedings of the 32nd International Conference on Computer Aided Verification. Springer, 2020.\n\nOptimization over integers. Dimitris Bertsimas, Robert Weismantel, Athena Scientific. Dimitris Bertsimas and Robert Weismantel. Optimization over integers. Athena Scientific, 2005.\n\nProgress in computational mixed integer programming-a look back from the other side of the tipping point. Robert Bixby, Edward Rothberg, Annals of Operations Research. 1491Robert Bixby and Edward Rothberg. Progress in computational mixed integer programming-a look back from the other side of the tipping point. Annals of Operations Research, 149(1):37-41, 2007.\n\nEfficient verification of relu-based neural networks via dependency analysis. Elena Botoeva, Panagiotis Kouvaros, Jan Kronqvist, Alessio Lomuscio, Ruth Misener, AAAI Conference on Artificial Intelligence (AAAI). 2020Elena Botoeva, Panagiotis Kouvaros, Jan Kronqvist, Alessio Lomuscio, and Ruth Misener. Efficient verification of relu-based neural networks via dependency analysis. In AAAI Conference on Artificial Intelligence (AAAI), 2020.\n\nLagrangian decomposition for neural network verification. Rudy Bunel, Alessandro De Palma, Alban Desmaison, Krishnamurthy Dvijotham, Pushmeet Kohli, H S Philip, M. Pawan Torr, Kumar, Conference on Uncertainty in Artificial Intelligence (UAI). 2020Rudy Bunel, Alessandro De Palma, Alban Desmaison, Krishnamurthy Dvijotham, Pushmeet Kohli, Philip H. S. Torr, and M. Pawan Kumar. Lagrangian decomposition for neural network verification. Conference on Uncertainty in Artificial Intelligence (UAI), 2020.\n\nBranch and bound for piecewise linear neural network verification. Rudy Bunel, Jingyue Lu, Ilker Turkaslan, P Kohli, P Torr, Mudigonda, Journal of Machine Learning Research. 2020JMLRRudy Bunel, Jingyue Lu, Ilker Turkaslan, P Kohli, P Torr, and P Mudigonda. Branch and bound for piecewise linear neural network verification. Journal of Machine Learning Research (JMLR), 2020.\n\nA unified view of piecewise linear neural network verification. Ilker Rudy R Bunel, Philip Turkaslan, Pushmeet Torr, Pawan K Kohli, Mudigonda, Advances in Neural Information Processing Systems (NeurIPS). Rudy R Bunel, Ilker Turkaslan, Philip Torr, Pushmeet Kohli, and Pawan K Mudigonda. A unified view of piecewise linear neural network verification. In Advances in Neural Information Processing Systems (NeurIPS), 2018.\n\nDeepsplit: Scalable verification of deep neural networks via operator splitting. Shaoru Chen, Eric Wong, Zico Kolter, Mahyar Fazlyab, arXiv:2106.09117arXiv preprintShaoru Chen, Eric Wong, J Zico Kolter, and Mahyar Fazlyab. Deepsplit: Scalable verification of deep neural networks via operator splitting. arXiv preprint arXiv:2106.09117, 2021.\n\nEdmonds polytopes and a hierarchy of combinatorial problems. Vasek Chv\u00e1tal, Discrete mathematics. 44Vasek Chv\u00e1tal. Edmonds polytopes and a hierarchy of combinatorial problems. Discrete mathematics, 4(4):305-337, 1973.\n\nInteger programming / Michele Conforti, G\u00e9rard Cornu\u00e9jols, Giacomo Zambelli. Graduate texts in mathematics. Michele Conforti, G\u00e9rard Cornu\u00e9jols, Giacomo Zambelli, SpringerChamMichele Conforti, G\u00e9rard Cornu\u00e9jols, and Giacomo Zambelli. Integer programming / Michele Conforti, G\u00e9rard Cornu\u00e9jols, Giacomo Zambelli. Graduate texts in mathematics, . 271. Springer, Cham, 2014.\n\nSolving large-scale zero-one linear programming problems. Harlan Crowder, L Ellis, Manfred Johnson, Padberg, Operations Research. 315Harlan Crowder, Ellis L Johnson, and Manfred Padberg. Solving large-scale zero-one linear programming problems. Operations Research, 31(5):803-834, 1983.\n\nEnabling certification of verification-agnostic networks via memory-efficient semidefinite programming. Sumanth Dathathri, Krishnamurthy Dvijotham, Alexey Kurakin, Aditi Raghunathan, Jonathan Uesato, Rudy R Bunel, Shreya Shankar, Jacob Steinhardt, Ian Goodfellow, Percy S Liang, Advances in Neural Information Processing Systems (NeurIPS). 2020Sumanth Dathathri, Krishnamurthy Dvijotham, Alexey Kurakin, Aditi Raghunathan, Jonathan Uesato, Rudy R Bunel, Shreya Shankar, Jacob Steinhardt, Ian Goodfellow, Percy S Liang, et al. Enabling certification of verification-agnostic networks via memory-efficient semidefinite programming. Advances in Neural Information Processing Systems (NeurIPS), 2020.\n\nScaling the convex barrier with active sets. Alessandro De Palma, Harkirat Singh Behl, Rudy Bunel, H S Philip, M. Pawan Torr, Kumar, International Conference on Learning Representations (ICLR. 2021Alessandro De Palma, Harkirat Singh Behl, Rudy Bunel, Philip H. S. Torr, and M. Pawan Kumar. Scaling the convex barrier with active sets. International Conference on Learning Representations (ICLR), 2021.\n\nImproved branch and bound for neural network verification via lagrangian decomposition. Alessandro De Palma, Rudy Bunel, Alban Desmaison, Krishnamurthy Dvijotham, Pushmeet Kohli, H S Philip, M Pawan Torr, Kumar, arXiv:2104.06718arXiv preprintAlessandro De Palma, Rudy Bunel, Alban Desmaison, Krishnamurthy Dvijotham, Pushmeet Kohli, Philip HS Torr, and M Pawan Kumar. Improved branch and bound for neural network verification via lagrangian decomposition. arXiv preprint arXiv:2104.06718, 2021.\n\nA dual approach to scalable verification of deep networks. Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy Mann, Pushmeet Kohli, Conference on Uncertainty in Artificial Intelligence (UAI). Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy Mann, and Pushmeet Kohli. A dual approach to scalable verification of deep networks. Conference on Uncertainty in Artificial Intelligence (UAI), 2018.\n\nFormal verification of piece-wise linear feed-forward neural networks. Ruediger Ehlers, International Symposium on Automated Technology for Verification and Analysis. ATVARuediger Ehlers. Formal verification of piece-wise linear feed-forward neural networks. In International Symposium on Automated Technology for Verification and Analysis (ATVA), 2017.\n\nComplete verification via multi-neuron relaxation guided branch-and-bound. Claudio Ferrari, Mark Niklas Muller, Nikola Jovanovic, Martin Vechev, arXiv:2205.00263arXiv preprintClaudio Ferrari, Mark Niklas Muller, Nikola Jovanovic, and Martin Vechev. Complete verifica- tion via multi-neuron relaxation guided branch-and-bound. arXiv preprint arXiv:2205.00263, 2022.\n\nA linear programming approach to the cutting-stock problem. C Paul, Ralph E Gilmore, Gomory, Operations research. 96Paul C Gilmore and Ralph E Gomory. A linear programming approach to the cutting-stock problem. Operations research, 9(6):849-859, 1961.\n\nA linear programming approach to the cutting stock problem-part ii. C Paul, Ralph E Gilmore, Gomory, Operations research. 116Paul C Gilmore and Ralph E Gomory. A linear programming approach to the cutting stock problem-part ii. Operations research, 11(6):863-888, 1963.\n\nOn the effectiveness of interval bound propagation for training verifiably robust models. Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Timothy Mann, Pushmeet Kohli, Proceedings of the IEEE International Conference on Computer Vision (ICCV). the IEEE International Conference on Computer Vision (ICCV)Sven Gowal, Krishnamurthy Dvijotham, Robert Stanforth, Rudy Bunel, Chongli Qin, Jonathan Uesato, Timothy Mann, and Pushmeet Kohli. On the effectiveness of interval bound propagation for training verifiably robust models. Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2019.\n\n. LLC Gurobi Optimization. Gurobi optimizer -gurobi. LLC Gurobi Optimization. Gurobi optimizer -gurobi, 2022.\n\nEfficient neural network verification via adaptive refinement and adversarial search. Patrick Henriksen, Alessio Lomuscio, ECAI 2020. IOS PressPatrick Henriksen and Alessio Lomuscio. Efficient neural network verification via adaptive refinement and adversarial search. In ECAI 2020, pages 2513-2520. IOS Press, 2020.\n\nDeepsplit: An efficient splitting method for neural network verification via indirect effect analysis. Patrick Henriksen, Alessio Lomuscio, Proceedings of the 30th international joint conference on artificial intelligence (IJCAI21). the 30th international joint conference on artificial intelligence (IJCAI21)Patrick Henriksen and Alessio Lomuscio. Deepsplit: An efficient splitting method for neural network verification via indirect effect analysis. In Proceedings of the 30th international joint conference on artificial intelligence (IJCAI21), pages 2549-2555, 2021.\n\nSolving airline crew scheduling problems by branchand-cut. L Karla, Manfred Hoffman, Padberg, Management science. 396Karla L Hoffman and Manfred Padberg. Solving airline crew scheduling problems by branch- and-cut. Management science, 39(6):657-682, 1993.\n\nIBM. Ibm ilog cplex optimizer. IBM. Ibm ilog cplex optimizer, 2022.\n\nDegree-two inequalities, clique facets, and biperfect graphs. L Ellis, Manfred W Johnson, Padberg, North-Holland Mathematics Studies. Elsevier66Ellis L Johnson and Manfred W Padberg. Degree-two inequalities, clique facets, and biperfect graphs. In North-Holland Mathematics Studies, volume 66, pages 169-187. Elsevier, 1982.\n\nReluplex: An efficient smt solver for verifying deep neural networks. Guy Katz, Clark Barrett, L David, Kyle Dill, Julian, Kochenderfer, International Conference on Computer Aided Verification (CAV. Guy Katz, Clark Barrett, David L Dill, Kyle Julian, and Mykel J Kochenderfer. Reluplex: An efficient smt solver for verifying deep neural networks. In International Conference on Computer Aided Verification (CAV), 2017.\n\nThe marabou framework for verification and analysis of deep neural networks. Guy Katz, Derek A Huang, Duligur Ibeling, Kyle Julian, Christopher Lazarus, Rachel Lim, Parth Shah, Shantanu Thakoor, Haoze Wu, Aleksandar Zelji\u0107, International Conference on Computer Aided Verification (CAV). Guy Katz, Derek A Huang, Duligur Ibeling, Kyle Julian, Christopher Lazarus, Rachel Lim, Parth Shah, Shantanu Thakoor, Haoze Wu, Aleksandar Zelji\u0107, et al. The marabou framework for verification and analysis of deep neural networks. In International Conference on Computer Aided Verification (CAV), 2019.\n\nAdam: A method for stochastic optimization. P Diederik, Jimmy Kingma, Ba, International Conference on Learning Representations (ICLR). Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations (ICLR), 2015.\n\nTowards scalable complete verification of relu neural networks via dependency-based branching. Panagiotis Kouvaros, Alessio Lomuscio, IJCAI. Panagiotis Kouvaros and Alessio Lomuscio. Towards scalable complete verification of relu neural networks via dependency-based branching. In IJCAI, pages 2643-2650, 2021.\n\n. Changliu Liu, Taylor Johnson, Vnn. 2020Changliu Liu and Taylor Johnson. Vnn comp 2020.\n\nCones of matrices and set-functions and 0-1 optimization. L\u00e1szl\u00f3 Lov\u00e1sz, Alexander Schrijver, SIAM journal on optimization. 12L\u00e1szl\u00f3 Lov\u00e1sz and Alexander Schrijver. Cones of matrices and set-functions and 0-1 optimiza- tion. SIAM journal on optimization, 1(2):166-190, 1991.\n\nJingyue Lu, M Pawan Kumar, Neural network branching for neural network verification. International Conference on Learning Representation (ICLR. 2020Jingyue Lu and M Pawan Kumar. Neural network branching for neural network verification. International Conference on Learning Representation (ICLR), 2020.\n\nAggregation and mixed integer rounding to solve mips. Hugues Marchand, A Laurence, Wolsey, Operations research. 493Hugues Marchand and Laurence A Wolsey. Aggregation and mixed integer rounding to solve mips. Operations research, 49(3):363-371, 2001.\n\nScaling polyhedral neural network verification on gpus. Christoph M\u00fcller, Francois Serre, Gagandeep Singh, Markus P\u00fcschel, Martin Vechev, Proceedings of Machine Learning and Systems. Machine Learning and Systems3Christoph M\u00fcller, Francois Serre, Gagandeep Singh, Markus P\u00fcschel, and Martin Vechev. Scaling polyhedral neural network verification on gpus. Proceedings of Machine Learning and Systems, 3:733-746, 2021.\n\nChristoph M\u00fcller, Gagandeep Singh, Markus P\u00fcschel, Martin Vechev, arXiv:2007.10868Neural network robustness verification on gpus. arXiv preprintChristoph M\u00fcller, Gagandeep Singh, Markus P\u00fcschel, and Martin Vechev. Neural network robustness verification on gpus. arXiv preprint arXiv:2007.10868, 2020.\n\nPrecise multi-neuron abstractions for neural network certification. Gleb Mark Niklas M\u00fcller, Gagandeep Makarchuk, Markus Singh, Martin P\u00fcschel, Vechev, arXiv:2103.03638arXiv preprintMark Niklas M\u00fcller, Gleb Makarchuk, Gagandeep Singh, Markus P\u00fcschel, and Martin Vechev. Precise multi-neuron abstractions for neural network certification. arXiv preprint arXiv:2103.03638, 2021.\n\nA recursive procedure to generate all cuts for 0-1 mixed integer programs. L George, Laurence A Nemhauser, Wolsey, Mathematical Programming. 461George L Nemhauser and Laurence A Wolsey. A recursive procedure to generate all cuts for 0-1 mixed integer programs. Mathematical Programming, 46(1):379-390, 1990.\n\nValid linear inequalities for fixed charge problems. Tony J Manfred W Padberg, Laurence A Van Roy, Wolsey, Operations Research. 334Manfred W Padberg, Tony J Van Roy, and Laurence A Wolsey. Valid linear inequalities for fixed charge problems. Operations Research, 33(4):842-861, 1985.\n\nVerification of non-linear specifications for neural networks. Chongli Qin, O&apos; Brendan, Rudy Donoghue, Robert Bunel, Sven Stanforth, Jonathan Gowal, Grzegorz Uesato, Pushmeet Swirszcz, Kohli, arXiv:1902.09592arXiv preprintChongli Qin, Brendan O'Donoghue, Rudy Bunel, Robert Stanforth, Sven Gowal, Jonathan Uesato, Grzegorz Swirszcz, Pushmeet Kohli, et al. Verification of non-linear specifications for neural networks. arXiv preprint arXiv:1902.09592, 2019.\n\nSemidefinite relaxations for certifying robustness to adversarial examples. Aditi Raghunathan, Jacob Steinhardt, Percy S Liang, Advances in Neural Information Processing Systems (NeurIPS). Aditi Raghunathan, Jacob Steinhardt, and Percy S Liang. Semidefinite relaxations for certifying robustness to adversarial examples. In Advances in Neural Information Processing Systems (NeurIPS), 2018.\n\nA convex relaxation barrier to tight robustness verification of neural networks. Greg Hadi Salman, Huan Yang, Cho-Jui Zhang, Pengchuan Hsieh, Zhang, Advances in Neural Information Processing Systems (NeurIPS). Hadi Salman, Greg Yang, Huan Zhang, Cho-Jui Hsieh, and Pengchuan Zhang. A convex relaxation barrier to tight robustness verification of neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2019.\n\nA reformulation-linearization technique for solving discrete and continuous nonconvex problems. D Hanif, Warren P Sherali, Adams, Springer Science & Business Media31Hanif D Sherali and Warren P Adams. A reformulation-linearization technique for solving discrete and continuous nonconvex problems, volume 31. Springer Science & Business Media, 2013.\n\nBeyond the single neuron convex barrier for neural network certification. Gagandeep Singh, Rupanshu Ganvir, Markus P\u00fcschel, Martin Vechev, Advances in Neural Information Processing Systems (NeurIPS). Gagandeep Singh, Rupanshu Ganvir, Markus P\u00fcschel, and Martin Vechev. Beyond the single neuron convex barrier for neural network certification. In Advances in Neural Information Processing Systems (NeurIPS), 2019.\n\nFast and effective robustness certification. Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus P\u00fcschel, Martin Vechev, Advances in Neural Information Processing Systems (NeurIPS). Gagandeep Singh, Timon Gehr, Matthew Mirman, Markus P\u00fcschel, and Martin Vechev. Fast and effective robustness certification. In Advances in Neural Information Processing Systems (NeurIPS), 2018.\n\nBoosting robustness certification of neural networks. Gagandeep Singh, Timon Gehr, Markus P\u00fcschel, Martin Vechev, International Conference on Learning Representations. Gagandeep Singh, Timon Gehr, Markus P\u00fcschel, and Martin Vechev. Boosting robustness certification of neural networks. In International Conference on Learning Representations, 2018.\n\nAn abstract domain for certifying neural networks. Gagandeep Singh, Timon Gehr, Markus P\u00fcschel, Martin Vechev, Proceedings of the ACM on Programming Languages. Gagandeep Singh, Timon Gehr, Markus P\u00fcschel, and Martin Vechev. An abstract domain for certifying neural networks. Proceedings of the ACM on Programming Languages (POPL), 2019.\n\nBuilding verified neural networks with specifications for systems. Cheng Tan, Yibo Zhu, Chuanxiong Guo, Proceedings of the 12th ACM SIGOPS Asia-Pacific Workshop on Systems. the 12th ACM SIGOPS Asia-Pacific Workshop on SystemsCheng Tan, Yibo Zhu, and Chuanxiong Guo. Building verified neural networks with specifica- tions for systems. In Proceedings of the 12th ACM SIGOPS Asia-Pacific Workshop on Systems, pages 42-47, 2021.\n\nThe convex relaxation barrier, revisited: Tightened single-neuron relaxations for neural network verification. Christian Tjandraatmadja, Ross Anderson, Joey Huchette, Will Ma, Krunal Patel, Juan Pablo Vielma, Advances in Neural Information Processing Systems (NeurIPS). 2020Christian Tjandraatmadja, Ross Anderson, Joey Huchette, Will Ma, Krunal Patel, and Juan Pablo Vielma. The convex relaxation barrier, revisited: Tightened single-neuron relaxations for neural network verification. Advances in Neural Information Processing Systems (NeurIPS), 2020.\n\nEvaluating robustness of neural networks with mixed integer programming. Vincent Tjeng, Kai Xiao, Russ Tedrake, International Conference on Learning Representations (ICLR). Vincent Tjeng, Kai Xiao, and Russ Tedrake. Evaluating robustness of neural networks with mixed integer programming. International Conference on Learning Representations (ICLR), 2019.\n\nNnv: The neural network verification tool for deep neural networks and learning-enabled cyber-physical systems. Hoang-Dung Tran, Xiaodong Yang, Diego Manzanas Lopez, Patrick Musau, Luan Viet Nguyen, Weiming Xiang, Stanley Bak, Taylor T Johnson , International Conference on Computer Aided Verification. SpringerHoang-Dung Tran, Xiaodong Yang, Diego Manzanas Lopez, Patrick Musau, Luan Viet Nguyen, Weiming Xiang, Stanley Bak, and Taylor T Johnson. Nnv: The neural network verification tool for deep neural networks and learning-enabled cyber-physical systems. In International Conference on Computer Aided Verification, pages 3-17. Springer, 2020.\n\nEfficient formal safety analysis of neural networks. Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, Suman Jana, Advances in Neural Information Processing Systems (NeurIPS). Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. Efficient formal safety analysis of neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2018.\n\nFormal security analysis of neural networks using symbolic intervals. Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, Suman Jana, USENIX Security Symposium. Shiqi Wang, Kexin Pei, Justin Whitehouse, Junfeng Yang, and Suman Jana. Formal security analysis of neural networks using symbolic intervals. In USENIX Security Symposium, 2018.\n\nBeta-crown: Efficient bound propagation with per-neuron split constraints for neural network robustness verification. Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, J Zico Kolter, Advances in Neural Information Processing Systems. 342021Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh, and J Zico Kolter. Beta-crown: Efficient bound propagation with per-neuron split constraints for neural network robustness verification. Advances in Neural Information Processing Systems, 34, 2021.\n\nProvable defenses against adversarial examples via the convex outer adversarial polytope. Eric Wong, Zico Kolter, International Conference on Machine Learning (ICML). Eric Wong and Zico Kolter. Provable defenses against adversarial examples via the convex outer adversarial polytope. In International Conference on Machine Learning (ICML), 2018.\n\nAutomatic perturbation analysis for scalable certified robustness and beyond. Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya Kailkhura, Xue Lin, Cho-Jui Hsieh, Advances in Neural Information Processing Systems (NeurIPS). 2020Kaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya Kailkhura, Xue Lin, and Cho-Jui Hsieh. Automatic perturbation analysis for scalable certified robustness and beyond. Advances in Neural Information Processing Systems (NeurIPS), 2020.\n\nFast and complete: Enabling complete neural network verification with rapid and massively parallel incomplete verifiers. Kaidi Xu, Huan Zhang, Shiqi Wang, Yihan Wang, Suman Jana, Xue Lin, Cho-Jui Hsieh, International Conference on Learning Representations (ICLR. 2021Kaidi Xu, Huan Zhang, Shiqi Wang, Yihan Wang, Suman Jana, Xue Lin, and Cho-Jui Hsieh. Fast and complete: Enabling complete neural network verification with rapid and massively parallel incomplete verifiers. International Conference on Learning Representations (ICLR), 2021.\n\nEfficient neural network robustness certification with general activation functions. Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, Luca Daniel, Advances in Neural Information Processing Systems (NeurIPS). Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, and Luca Daniel. Efficient neural network robustness certification with general activation functions. In Advances in Neural Information Processing Systems (NeurIPS), 2018.\n\nBase (CIFAR-10). Base (CIFAR-10)\n\nWide (CIFAR-10). Wide (CIFAR-10)\n\n. Deep, CIFAR-10Deep (CIFAR-10)\n\nResBlock(8, 16) -ResBlock(16, 16)) -Linear(1024, 100) -Linear(100, 10). Conv. 83Conv(3, 8, 3) -ResBlock(8, 16) -ResBlock(16, 16)) -Linear(1024, 100) -Linear(100, 10) 2048 cifar10-resnet4b\n\nResBlock(16, 32) -ResBlock(32, 32)) -Linear(512, 100) -Linear. Conv. 1632048Conv(3, 16, 3) -ResBlock(16, 32) -ResBlock(32, 32)) -Linear(512, 100) -Linear(100, 10) 2048\n\nCnn-A-Adv, MNIST) Conv(1, 16, 4) -Conv. 100409616, 32, 4) -Linear(1568, 100) -LinearCNN-A-Adv (MNIST) Conv(1, 16, 4) -Conv(16, 32, 4) -Linear(1568, 100) -Linear(100, 10) 4096\n\n. Cnn-A-Adv, CIFAR-10CNN-A-Adv/-4 (CIFAR-10)\n\n. Cnn-B-Adv, CIFAR-10CNN-B-Adv/-4 (CIFAR-10)\n\n. Cnn-A-Mix, CIFAR-10CNN-A-Mix/-4 (CIFAR-10)\n\nThe effectiveness of GCP-CROWN with MIP cuts motivates us to take a more careful look at the cutting planes generated by off-the-shelf MIP solvers, and understand how well it can contribute to. The effectiveness of GCP-CROWN with MIP cuts motivates us to take a more careful look at the cutting planes generated by off-the-shelf MIP solvers, and understand how well it can contribute to\n", "annotations": {"author": "[{\"end\":120,\"start\":82},{\"end\":230,\"start\":121},{\"end\":261,\"start\":231},{\"end\":291,\"start\":262},{\"end\":298,\"start\":292},{\"end\":387,\"start\":299},{\"end\":423,\"start\":388},{\"end\":444,\"start\":424}]", "publisher": null, "author_last_name": "[{\"end\":92,\"start\":87},{\"end\":131,\"start\":127},{\"end\":239,\"start\":237},{\"end\":270,\"start\":268},{\"end\":297,\"start\":295},{\"end\":309,\"start\":305},{\"end\":401,\"start\":396},{\"end\":437,\"start\":426}]", "author_first_name": "[{\"end\":86,\"start\":82},{\"end\":126,\"start\":121},{\"end\":236,\"start\":231},{\"end\":267,\"start\":262},{\"end\":294,\"start\":292},{\"end\":304,\"start\":299},{\"end\":395,\"start\":388},{\"end\":425,\"start\":424}]", "author_affiliation": "[{\"end\":98,\"start\":94},{\"end\":119,\"start\":100},{\"end\":208,\"start\":133},{\"end\":229,\"start\":210},{\"end\":260,\"start\":241},{\"end\":386,\"start\":311},{\"end\":443,\"start\":439}]", "title": "[{\"end\":79,\"start\":1},{\"end\":523,\"start\":445}]", "venue": null, "abstract": "[{\"end\":2459,\"start\":576}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b30\"},\"end\":2836,\"start\":2832},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2838,\"start\":2836},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":2884,\"start\":2880},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":2937,\"start\":2933},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":2997,\"start\":2993},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":3090,\"start\":3086},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":3336,\"start\":3332},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":3611,\"start\":3607},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":3613,\"start\":3611},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":3837,\"start\":3833},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":3840,\"start\":3837},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":3878,\"start\":3875},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":3881,\"start\":3878},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3897,\"start\":3893},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3900,\"start\":3897},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":3975,\"start\":3971},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":3978,\"start\":3975},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":3981,\"start\":3978},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":3984,\"start\":3981},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":3987,\"start\":3984},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":3990,\"start\":3987},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":4052,\"start\":4048},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":4055,\"start\":4052},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":4058,\"start\":4055},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":4074,\"start\":4071},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":4225,\"start\":4221},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":4875,\"start\":4872},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":5228,\"start\":5224},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":5559,\"start\":5555},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":5576,\"start\":5572},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":5975,\"start\":5971},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":5978,\"start\":5975},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":5981,\"start\":5978},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6078,\"start\":6074},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6313,\"start\":6309},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7972,\"start\":7969},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":8176,\"start\":8172},{\"end\":8652,\"start\":8649},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":9970,\"start\":9966},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":9973,\"start\":9970},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":10428,\"start\":10424},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":10442,\"start\":10438},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":10520,\"start\":10516},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":12348,\"start\":12344},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":12351,\"start\":12348},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":12688,\"start\":12684},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":12705,\"start\":12701},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":13374,\"start\":13370},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":13712,\"start\":13708},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":14051,\"start\":14047},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":14068,\"start\":14064},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":14101,\"start\":14097},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":14884,\"start\":14880},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":14887,\"start\":14884},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":14896,\"start\":14892},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":16683,\"start\":16679},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":17644,\"start\":17641},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":19385,\"start\":19381},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":19388,\"start\":19385},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":19573,\"start\":19569},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":20197,\"start\":20193},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":20458,\"start\":20454},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":20475,\"start\":20471},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":20519,\"start\":20515},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":20668,\"start\":20664},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":20722,\"start\":20718},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":20725,\"start\":20722},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":20763,\"start\":20759},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":20791,\"start\":20787},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":21070,\"start\":21066},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":21244,\"start\":21240},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21750,\"start\":21747},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":21909,\"start\":21905},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":21911,\"start\":21909},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":21994,\"start\":21991},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":21997,\"start\":21994},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":22000,\"start\":21997},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":22003,\"start\":22000},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":23193,\"start\":23189},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":23209,\"start\":23205},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":24420,\"start\":24416},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":24423,\"start\":24420},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":24426,\"start\":24423},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":25782,\"start\":25778},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":26014,\"start\":26010},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":26017,\"start\":26014},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":26020,\"start\":26017},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":26063,\"start\":26060},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":27586,\"start\":27582},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":27691,\"start\":27687},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":29155,\"start\":29152},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":29784,\"start\":29780},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":29867,\"start\":29863},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":30979,\"start\":30975},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":31214,\"start\":31210},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":31217,\"start\":31214},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":31403,\"start\":31399},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":31495,\"start\":31491},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":31989,\"start\":31985},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":31992,\"start\":31989},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":32017,\"start\":32013},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":32041,\"start\":32037},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":32064,\"start\":32060},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":32109,\"start\":32105},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":32146,\"start\":32142},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":32149,\"start\":32146},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":32261,\"start\":32257},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":32282,\"start\":32278},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":32302,\"start\":32298},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":32905,\"start\":32901},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":32908,\"start\":32905},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":32911,\"start\":32908},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":33064,\"start\":33060},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":33067,\"start\":33064},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":33099,\"start\":33095},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":33102,\"start\":33099},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":33122,\"start\":33118},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":33323,\"start\":33319},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":33393,\"start\":33389},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":33519,\"start\":33515},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":33565,\"start\":33561},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":33600,\"start\":33597},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":33686,\"start\":33682},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":33778,\"start\":33774},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":33810,\"start\":33807},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":33892,\"start\":33889},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":33993,\"start\":33989},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":34035,\"start\":34031},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":35276,\"start\":35272},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":35365,\"start\":35361},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":35368,\"start\":35365},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":35371,\"start\":35368},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":42677,\"start\":42673},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":43479,\"start\":43475},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":43896,\"start\":43892},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":43952,\"start\":43948},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":43955,\"start\":43952},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":43958,\"start\":43955},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":43961,\"start\":43958},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":46296,\"start\":46293},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":46299,\"start\":46296},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":46417,\"start\":46413},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":46420,\"start\":46417},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":46423,\"start\":46420},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":46426,\"start\":46423},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":46429,\"start\":46426},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":47141,\"start\":47137},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":47259,\"start\":47256},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":47262,\"start\":47259},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":47948,\"start\":47944},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":50037,\"start\":50033},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":51043,\"start\":51039},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":51046,\"start\":51043},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":54738,\"start\":54734},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":55089,\"start\":55085}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":51441,\"start\":51404},{\"attributes\":{\"id\":\"fig_1\"},\"end\":51545,\"start\":51442},{\"attributes\":{\"id\":\"fig_2\"},\"end\":51772,\"start\":51546},{\"attributes\":{\"id\":\"fig_3\"},\"end\":51841,\"start\":51773},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":53587,\"start\":51842},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":54633,\"start\":53588},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":56033,\"start\":54634}]", "paragraph": "[{\"end\":2938,\"start\":2475},{\"end\":4256,\"start\":2940},{\"end\":5386,\"start\":4258},{\"end\":6467,\"start\":5388},{\"end\":7667,\"start\":6469},{\"end\":8212,\"start\":7669},{\"end\":9649,\"start\":8227},{\"end\":9819,\"start\":9679},{\"end\":9974,\"start\":9821},{\"end\":10616,\"start\":9976},{\"end\":10880,\"start\":10715},{\"end\":10944,\"start\":10941},{\"end\":10992,\"start\":10987},{\"end\":11292,\"start\":11287},{\"end\":11522,\"start\":11327},{\"end\":11598,\"start\":11553},{\"end\":11841,\"start\":11636},{\"end\":12497,\"start\":11948},{\"end\":12834,\"start\":12499},{\"end\":13072,\"start\":12885},{\"end\":13162,\"start\":13109},{\"end\":13453,\"start\":13215},{\"end\":14206,\"start\":13496},{\"end\":14897,\"start\":14323},{\"end\":15195,\"start\":14899},{\"end\":15697,\"start\":15248},{\"end\":15837,\"start\":15752},{\"end\":15917,\"start\":15872},{\"end\":16291,\"start\":16005},{\"end\":16778,\"start\":16293},{\"end\":17160,\"start\":17090},{\"end\":18062,\"start\":17279},{\"end\":19482,\"start\":18903},{\"end\":19955,\"start\":19484},{\"end\":20045,\"start\":19991},{\"end\":20395,\"start\":20131},{\"end\":20956,\"start\":20397},{\"end\":21751,\"start\":21053},{\"end\":22977,\"start\":21817},{\"end\":23112,\"start\":23013},{\"end\":24557,\"start\":23114},{\"end\":25044,\"start\":24559},{\"end\":25908,\"start\":25046},{\"end\":27268,\"start\":25910},{\"end\":28104,\"start\":27284},{\"end\":29061,\"start\":28106},{\"end\":30675,\"start\":29063},{\"end\":31119,\"start\":30677},{\"end\":31809,\"start\":31121},{\"end\":32806,\"start\":31826},{\"end\":34512,\"start\":32808},{\"end\":35104,\"start\":34527},{\"end\":35848,\"start\":35106},{\"end\":36770,\"start\":35850},{\"end\":36810,\"start\":36772},{\"end\":37325,\"start\":36812},{\"end\":37679,\"start\":37674},{\"end\":37713,\"start\":37708},{\"end\":37887,\"start\":37841},{\"end\":38622,\"start\":38339},{\"end\":39419,\"start\":39186},{\"end\":39482,\"start\":39421},{\"end\":40294,\"start\":40042},{\"end\":40431,\"start\":40296},{\"end\":40542,\"start\":40501},{\"end\":40675,\"start\":40616},{\"end\":40982,\"start\":40737},{\"end\":41081,\"start\":41025},{\"end\":41199,\"start\":41147},{\"end\":41500,\"start\":41256},{\"end\":41799,\"start\":41542},{\"end\":42087,\"start\":41801},{\"end\":42942,\"start\":42547},{\"end\":43344,\"start\":42944},{\"end\":43706,\"start\":43346},{\"end\":44082,\"start\":43777},{\"end\":45701,\"start\":44084},{\"end\":46749,\"start\":45703},{\"end\":47293,\"start\":46751},{\"end\":49288,\"start\":47350},{\"end\":49692,\"start\":49290},{\"end\":49914,\"start\":49694},{\"end\":50603,\"start\":49916},{\"end\":51403,\"start\":50605}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9678,\"start\":9650},{\"attributes\":{\"id\":\"formula_1\"},\"end\":10714,\"start\":10617},{\"attributes\":{\"id\":\"formula_2\"},\"end\":10940,\"start\":10881},{\"attributes\":{\"id\":\"formula_3\"},\"end\":10986,\"start\":10945},{\"attributes\":{\"id\":\"formula_4\"},\"end\":11286,\"start\":10993},{\"attributes\":{\"id\":\"formula_5\"},\"end\":11326,\"start\":11293},{\"attributes\":{\"id\":\"formula_6\"},\"end\":11552,\"start\":11523},{\"attributes\":{\"id\":\"formula_7\"},\"end\":11635,\"start\":11599},{\"attributes\":{\"id\":\"formula_8\"},\"end\":11866,\"start\":11842},{\"attributes\":{\"id\":\"formula_9\"},\"end\":11947,\"start\":11866},{\"attributes\":{\"id\":\"formula_10\"},\"end\":12884,\"start\":12835},{\"attributes\":{\"id\":\"formula_11\"},\"end\":13108,\"start\":13073},{\"attributes\":{\"id\":\"formula_12\"},\"end\":13214,\"start\":13163},{\"attributes\":{\"id\":\"formula_13\"},\"end\":13495,\"start\":13454},{\"attributes\":{\"id\":\"formula_14\"},\"end\":15247,\"start\":15196},{\"attributes\":{\"id\":\"formula_15\"},\"end\":15751,\"start\":15698},{\"attributes\":{\"id\":\"formula_16\"},\"end\":15871,\"start\":15838},{\"attributes\":{\"id\":\"formula_17\"},\"end\":16004,\"start\":15918},{\"attributes\":{\"id\":\"formula_18\"},\"end\":17089,\"start\":16779},{\"attributes\":{\"id\":\"formula_19\"},\"end\":17278,\"start\":17161},{\"attributes\":{\"id\":\"formula_20\"},\"end\":18902,\"start\":18063},{\"attributes\":{\"id\":\"formula_21\"},\"end\":19990,\"start\":19956},{\"attributes\":{\"id\":\"formula_22\"},\"end\":20130,\"start\":20046},{\"attributes\":{\"id\":\"formula_23\"},\"end\":21052,\"start\":20957},{\"attributes\":{\"id\":\"formula_24\"},\"end\":37673,\"start\":37326},{\"attributes\":{\"id\":\"formula_25\"},\"end\":37707,\"start\":37680},{\"attributes\":{\"id\":\"formula_26\"},\"end\":37777,\"start\":37714},{\"attributes\":{\"id\":\"formula_27\"},\"end\":37840,\"start\":37777},{\"attributes\":{\"id\":\"formula_28\"},\"end\":38338,\"start\":37888},{\"attributes\":{\"id\":\"formula_29\"},\"end\":39185,\"start\":38623},{\"attributes\":{\"id\":\"formula_30\"},\"end\":39653,\"start\":39483},{\"attributes\":{\"id\":\"formula_31\"},\"end\":39766,\"start\":39653},{\"attributes\":{\"id\":\"formula_32\"},\"end\":39823,\"start\":39766},{\"attributes\":{\"id\":\"formula_33\"},\"end\":39896,\"start\":39823},{\"attributes\":{\"id\":\"formula_34\"},\"end\":40003,\"start\":39896},{\"attributes\":{\"id\":\"formula_35\"},\"end\":40041,\"start\":40003},{\"attributes\":{\"id\":\"formula_36\"},\"end\":40500,\"start\":40432},{\"attributes\":{\"id\":\"formula_37\"},\"end\":40615,\"start\":40543},{\"attributes\":{\"id\":\"formula_38\"},\"end\":40736,\"start\":40676},{\"attributes\":{\"id\":\"formula_39\"},\"end\":41024,\"start\":40983},{\"attributes\":{\"id\":\"formula_40\"},\"end\":41146,\"start\":41082},{\"attributes\":{\"id\":\"formula_41\"},\"end\":41255,\"start\":41200},{\"attributes\":{\"id\":\"formula_42\"},\"end\":41541,\"start\":41501},{\"attributes\":{\"id\":\"formula_43\"},\"end\":42546,\"start\":42088}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":23712,\"start\":23705},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":28350,\"start\":28343},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":29640,\"start\":29633},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":29766,\"start\":29759},{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":31478,\"start\":31471},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":31515,\"start\":31508},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":31525,\"start\":31518},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":31807,\"start\":31800},{\"end\":48576,\"start\":48569},{\"end\":48667,\"start\":48660}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2473,\"start\":2461},{\"attributes\":{\"n\":\"2\"},\"end\":8225,\"start\":8215},{\"attributes\":{\"n\":\"3\"},\"end\":14264,\"start\":14209},{\"attributes\":{\"n\":\"3.1\"},\"end\":14321,\"start\":14267},{\"attributes\":{\"n\":\"3.2\"},\"end\":21815,\"start\":21754},{\"end\":23011,\"start\":22980},{\"attributes\":{\"n\":\"4\"},\"end\":27282,\"start\":27271},{\"attributes\":{\"n\":\"5\"},\"end\":31824,\"start\":31812},{\"attributes\":{\"n\":\"6\"},\"end\":34525,\"start\":34515},{\"end\":43775,\"start\":43709},{\"end\":47348,\"start\":47296},{\"end\":51453,\"start\":51443},{\"end\":51548,\"start\":51547},{\"end\":51784,\"start\":51774},{\"end\":51852,\"start\":51843},{\"end\":53598,\"start\":53589},{\"end\":54644,\"start\":54635}]", "table": "[{\"end\":53587,\"start\":52190},{\"end\":54633,\"start\":53782},{\"end\":56033,\"start\":55090}]", "figure_caption": "[{\"end\":51441,\"start\":51406},{\"end\":51545,\"start\":51455},{\"end\":51772,\"start\":51549},{\"end\":51841,\"start\":51786},{\"end\":52190,\"start\":51854},{\"end\":53782,\"start\":53600},{\"end\":55090,\"start\":54646}]", "figure_ref": "[{\"end\":22976,\"start\":22970},{\"end\":23040,\"start\":23032},{\"end\":25127,\"start\":25119},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":28682,\"start\":28674},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":44138,\"start\":44130},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":44627,\"start\":44619},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":45178,\"start\":45170},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":45477,\"start\":45469},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":45817,\"start\":45809}]", "bib_author_first_name": "[{\"end\":57270,\"start\":57266},{\"end\":57285,\"start\":57281},{\"end\":57305,\"start\":57296},{\"end\":57326,\"start\":57322},{\"end\":57332,\"start\":57327},{\"end\":57671,\"start\":57664},{\"end\":57954,\"start\":57947},{\"end\":57968,\"start\":57960},{\"end\":57980,\"start\":57974},{\"end\":58314,\"start\":58307},{\"end\":58330,\"start\":58320},{\"end\":58345,\"start\":58337},{\"end\":58359,\"start\":58353},{\"end\":58361,\"start\":58360},{\"end\":58802,\"start\":58794},{\"end\":58820,\"start\":58814},{\"end\":59060,\"start\":59054},{\"end\":59074,\"start\":59068},{\"end\":59395,\"start\":59390},{\"end\":59415,\"start\":59405},{\"end\":59429,\"start\":59426},{\"end\":59448,\"start\":59441},{\"end\":59463,\"start\":59459},{\"end\":59816,\"start\":59812},{\"end\":59834,\"start\":59824},{\"end\":59837,\"start\":59835},{\"end\":59850,\"start\":59845},{\"end\":59875,\"start\":59862},{\"end\":59895,\"start\":59887},{\"end\":59904,\"start\":59903},{\"end\":59906,\"start\":59905},{\"end\":59923,\"start\":59915},{\"end\":60327,\"start\":60323},{\"end\":60342,\"start\":60335},{\"end\":60352,\"start\":60347},{\"end\":60365,\"start\":60364},{\"end\":60374,\"start\":60373},{\"end\":60701,\"start\":60696},{\"end\":60722,\"start\":60716},{\"end\":60742,\"start\":60734},{\"end\":60756,\"start\":60749},{\"end\":61141,\"start\":61135},{\"end\":61152,\"start\":61148},{\"end\":61163,\"start\":61159},{\"end\":61178,\"start\":61172},{\"end\":61464,\"start\":61459},{\"end\":61732,\"start\":61725},{\"end\":61749,\"start\":61743},{\"end\":61769,\"start\":61762},{\"end\":62053,\"start\":62047},{\"end\":62064,\"start\":62063},{\"end\":62079,\"start\":62072},{\"end\":62388,\"start\":62381},{\"end\":62413,\"start\":62400},{\"end\":62431,\"start\":62425},{\"end\":62446,\"start\":62441},{\"end\":62468,\"start\":62460},{\"end\":62481,\"start\":62477},{\"end\":62483,\"start\":62482},{\"end\":62497,\"start\":62491},{\"end\":62512,\"start\":62507},{\"end\":62528,\"start\":62525},{\"end\":62546,\"start\":62541},{\"end\":62548,\"start\":62547},{\"end\":63033,\"start\":63020},{\"end\":63049,\"start\":63041},{\"end\":63066,\"start\":63062},{\"end\":63075,\"start\":63074},{\"end\":63077,\"start\":63076},{\"end\":63094,\"start\":63086},{\"end\":63479,\"start\":63466},{\"end\":63491,\"start\":63487},{\"end\":63504,\"start\":63499},{\"end\":63529,\"start\":63516},{\"end\":63549,\"start\":63541},{\"end\":63558,\"start\":63557},{\"end\":63560,\"start\":63559},{\"end\":63576,\"start\":63569},{\"end\":63946,\"start\":63933},{\"end\":63964,\"start\":63958},{\"end\":63980,\"start\":63976},{\"end\":63995,\"start\":63988},{\"end\":64010,\"start\":64002},{\"end\":64372,\"start\":64364},{\"end\":64730,\"start\":64723},{\"end\":64744,\"start\":64740},{\"end\":64751,\"start\":64745},{\"end\":64766,\"start\":64760},{\"end\":64784,\"start\":64778},{\"end\":65075,\"start\":65074},{\"end\":65087,\"start\":65082},{\"end\":65089,\"start\":65088},{\"end\":65336,\"start\":65335},{\"end\":65348,\"start\":65343},{\"end\":65350,\"start\":65349},{\"end\":65632,\"start\":65628},{\"end\":65653,\"start\":65640},{\"end\":65671,\"start\":65665},{\"end\":65687,\"start\":65683},{\"end\":65702,\"start\":65695},{\"end\":65716,\"start\":65708},{\"end\":65732,\"start\":65725},{\"end\":65747,\"start\":65739},{\"end\":66398,\"start\":66391},{\"end\":66417,\"start\":66410},{\"end\":66733,\"start\":66726},{\"end\":66752,\"start\":66745},{\"end\":67255,\"start\":67254},{\"end\":67270,\"start\":67263},{\"end\":67584,\"start\":67583},{\"end\":67599,\"start\":67592},{\"end\":67601,\"start\":67600},{\"end\":67920,\"start\":67917},{\"end\":67932,\"start\":67927},{\"end\":67943,\"start\":67942},{\"end\":67955,\"start\":67951},{\"end\":68347,\"start\":68344},{\"end\":68359,\"start\":68354},{\"end\":68361,\"start\":68360},{\"end\":68376,\"start\":68369},{\"end\":68390,\"start\":68386},{\"end\":68410,\"start\":68399},{\"end\":68426,\"start\":68420},{\"end\":68437,\"start\":68432},{\"end\":68452,\"start\":68444},{\"end\":68467,\"start\":68462},{\"end\":68482,\"start\":68472},{\"end\":68903,\"start\":68902},{\"end\":68919,\"start\":68914},{\"end\":69242,\"start\":69232},{\"end\":69260,\"start\":69253},{\"end\":69459,\"start\":69451},{\"end\":69471,\"start\":69465},{\"end\":69603,\"start\":69597},{\"end\":69621,\"start\":69612},{\"end\":69822,\"start\":69815},{\"end\":69834,\"start\":69827},{\"end\":70178,\"start\":70172},{\"end\":70190,\"start\":70189},{\"end\":70434,\"start\":70425},{\"end\":70451,\"start\":70443},{\"end\":70468,\"start\":70459},{\"end\":70482,\"start\":70476},{\"end\":70498,\"start\":70492},{\"end\":70795,\"start\":70786},{\"end\":70813,\"start\":70804},{\"end\":70827,\"start\":70821},{\"end\":70843,\"start\":70837},{\"end\":71160,\"start\":71156},{\"end\":71190,\"start\":71181},{\"end\":71208,\"start\":71202},{\"end\":71222,\"start\":71216},{\"end\":71542,\"start\":71541},{\"end\":71559,\"start\":71551},{\"end\":71561,\"start\":71560},{\"end\":71832,\"start\":71828},{\"end\":71834,\"start\":71833},{\"end\":71862,\"start\":71854},{\"end\":71864,\"start\":71863},{\"end\":72130,\"start\":72123},{\"end\":72143,\"start\":72136},{\"end\":72157,\"start\":72153},{\"end\":72174,\"start\":72168},{\"end\":72186,\"start\":72182},{\"end\":72206,\"start\":72198},{\"end\":72222,\"start\":72214},{\"end\":72239,\"start\":72231},{\"end\":72605,\"start\":72600},{\"end\":72624,\"start\":72619},{\"end\":72644,\"start\":72637},{\"end\":73001,\"start\":72997},{\"end\":73019,\"start\":73015},{\"end\":73033,\"start\":73026},{\"end\":73050,\"start\":73041},{\"end\":73447,\"start\":73446},{\"end\":73463,\"start\":73455},{\"end\":73783,\"start\":73774},{\"end\":73799,\"start\":73791},{\"end\":73814,\"start\":73808},{\"end\":73830,\"start\":73824},{\"end\":74168,\"start\":74159},{\"end\":74181,\"start\":74176},{\"end\":74195,\"start\":74188},{\"end\":74210,\"start\":74204},{\"end\":74226,\"start\":74220},{\"end\":74555,\"start\":74546},{\"end\":74568,\"start\":74563},{\"end\":74581,\"start\":74575},{\"end\":74597,\"start\":74591},{\"end\":74902,\"start\":74893},{\"end\":74915,\"start\":74910},{\"end\":74928,\"start\":74922},{\"end\":74944,\"start\":74938},{\"end\":75252,\"start\":75247},{\"end\":75262,\"start\":75258},{\"end\":75278,\"start\":75268},{\"end\":75727,\"start\":75718},{\"end\":75748,\"start\":75744},{\"end\":75763,\"start\":75759},{\"end\":75778,\"start\":75774},{\"end\":75789,\"start\":75783},{\"end\":75801,\"start\":75797},{\"end\":75807,\"start\":75802},{\"end\":76242,\"start\":76235},{\"end\":76253,\"start\":76250},{\"end\":76264,\"start\":76260},{\"end\":76641,\"start\":76631},{\"end\":76656,\"start\":76648},{\"end\":76668,\"start\":76663},{\"end\":76677,\"start\":76669},{\"end\":76692,\"start\":76685},{\"end\":76704,\"start\":76700},{\"end\":76725,\"start\":76718},{\"end\":76740,\"start\":76733},{\"end\":76762,\"start\":76746},{\"end\":77226,\"start\":77221},{\"end\":77238,\"start\":77233},{\"end\":77250,\"start\":77244},{\"end\":77270,\"start\":77263},{\"end\":77282,\"start\":77277},{\"end\":77621,\"start\":77616},{\"end\":77633,\"start\":77628},{\"end\":77645,\"start\":77639},{\"end\":77665,\"start\":77658},{\"end\":77677,\"start\":77672},{\"end\":78013,\"start\":78008},{\"end\":78024,\"start\":78020},{\"end\":78037,\"start\":78032},{\"end\":78045,\"start\":78042},{\"end\":78056,\"start\":78051},{\"end\":78070,\"start\":78063},{\"end\":78084,\"start\":78078},{\"end\":78513,\"start\":78509},{\"end\":78524,\"start\":78520},{\"end\":78849,\"start\":78844},{\"end\":78862,\"start\":78854},{\"end\":78872,\"start\":78868},{\"end\":78885,\"start\":78880},{\"end\":78899,\"start\":78892},{\"end\":78913,\"start\":78907},{\"end\":78927,\"start\":78921},{\"end\":78942,\"start\":78939},{\"end\":78955,\"start\":78948},{\"end\":79423,\"start\":79418},{\"end\":79432,\"start\":79428},{\"end\":79445,\"start\":79440},{\"end\":79457,\"start\":79452},{\"end\":79469,\"start\":79464},{\"end\":79479,\"start\":79476},{\"end\":79492,\"start\":79485},{\"end\":79928,\"start\":79924},{\"end\":79944,\"start\":79936},{\"end\":79957,\"start\":79951},{\"end\":79971,\"start\":79964},{\"end\":79983,\"start\":79979}]", "bib_author_last_name": "[{\"end\":57279,\"start\":57271},{\"end\":57294,\"start\":57286},{\"end\":57320,\"start\":57306},{\"end\":57339,\"start\":57333},{\"end\":57675,\"start\":57672},{\"end\":57958,\"start\":57955},{\"end\":57972,\"start\":57969},{\"end\":57988,\"start\":57981},{\"end\":58318,\"start\":58315},{\"end\":58335,\"start\":58331},{\"end\":58351,\"start\":58346},{\"end\":58369,\"start\":58362},{\"end\":58812,\"start\":58803},{\"end\":58831,\"start\":58821},{\"end\":59066,\"start\":59061},{\"end\":59083,\"start\":59075},{\"end\":59403,\"start\":59396},{\"end\":59424,\"start\":59416},{\"end\":59439,\"start\":59430},{\"end\":59457,\"start\":59449},{\"end\":59471,\"start\":59464},{\"end\":59822,\"start\":59817},{\"end\":59843,\"start\":59838},{\"end\":59860,\"start\":59851},{\"end\":59885,\"start\":59876},{\"end\":59901,\"start\":59896},{\"end\":59913,\"start\":59907},{\"end\":59928,\"start\":59924},{\"end\":59935,\"start\":59930},{\"end\":60333,\"start\":60328},{\"end\":60345,\"start\":60343},{\"end\":60362,\"start\":60353},{\"end\":60371,\"start\":60366},{\"end\":60379,\"start\":60375},{\"end\":60390,\"start\":60381},{\"end\":60714,\"start\":60702},{\"end\":60732,\"start\":60723},{\"end\":60747,\"start\":60743},{\"end\":60762,\"start\":60757},{\"end\":60773,\"start\":60764},{\"end\":61146,\"start\":61142},{\"end\":61157,\"start\":61153},{\"end\":61170,\"start\":61164},{\"end\":61186,\"start\":61179},{\"end\":61472,\"start\":61465},{\"end\":61741,\"start\":61733},{\"end\":61760,\"start\":61750},{\"end\":61778,\"start\":61770},{\"end\":62061,\"start\":62054},{\"end\":62070,\"start\":62065},{\"end\":62087,\"start\":62080},{\"end\":62096,\"start\":62089},{\"end\":62398,\"start\":62389},{\"end\":62423,\"start\":62414},{\"end\":62439,\"start\":62432},{\"end\":62458,\"start\":62447},{\"end\":62475,\"start\":62469},{\"end\":62489,\"start\":62484},{\"end\":62505,\"start\":62498},{\"end\":62523,\"start\":62513},{\"end\":62539,\"start\":62529},{\"end\":62554,\"start\":62549},{\"end\":63039,\"start\":63034},{\"end\":63060,\"start\":63050},{\"end\":63072,\"start\":63067},{\"end\":63084,\"start\":63078},{\"end\":63099,\"start\":63095},{\"end\":63106,\"start\":63101},{\"end\":63485,\"start\":63480},{\"end\":63497,\"start\":63492},{\"end\":63514,\"start\":63505},{\"end\":63539,\"start\":63530},{\"end\":63555,\"start\":63550},{\"end\":63567,\"start\":63561},{\"end\":63581,\"start\":63577},{\"end\":63588,\"start\":63583},{\"end\":63956,\"start\":63947},{\"end\":63974,\"start\":63965},{\"end\":63986,\"start\":63981},{\"end\":64000,\"start\":63996},{\"end\":64016,\"start\":64011},{\"end\":64379,\"start\":64373},{\"end\":64738,\"start\":64731},{\"end\":64758,\"start\":64752},{\"end\":64776,\"start\":64767},{\"end\":64791,\"start\":64785},{\"end\":65080,\"start\":65076},{\"end\":65097,\"start\":65090},{\"end\":65105,\"start\":65099},{\"end\":65341,\"start\":65337},{\"end\":65358,\"start\":65351},{\"end\":65366,\"start\":65360},{\"end\":65638,\"start\":65633},{\"end\":65663,\"start\":65654},{\"end\":65681,\"start\":65672},{\"end\":65693,\"start\":65688},{\"end\":65706,\"start\":65703},{\"end\":65723,\"start\":65717},{\"end\":65737,\"start\":65733},{\"end\":65753,\"start\":65748},{\"end\":66408,\"start\":66399},{\"end\":66426,\"start\":66418},{\"end\":66743,\"start\":66734},{\"end\":66761,\"start\":66753},{\"end\":67261,\"start\":67256},{\"end\":67278,\"start\":67271},{\"end\":67287,\"start\":67280},{\"end\":67590,\"start\":67585},{\"end\":67609,\"start\":67602},{\"end\":67618,\"start\":67611},{\"end\":67925,\"start\":67921},{\"end\":67940,\"start\":67933},{\"end\":67949,\"start\":67944},{\"end\":67960,\"start\":67956},{\"end\":67968,\"start\":67962},{\"end\":67982,\"start\":67970},{\"end\":68352,\"start\":68348},{\"end\":68367,\"start\":68362},{\"end\":68384,\"start\":68377},{\"end\":68397,\"start\":68391},{\"end\":68418,\"start\":68411},{\"end\":68430,\"start\":68427},{\"end\":68442,\"start\":68438},{\"end\":68460,\"start\":68453},{\"end\":68470,\"start\":68468},{\"end\":68489,\"start\":68483},{\"end\":68912,\"start\":68904},{\"end\":68926,\"start\":68920},{\"end\":68930,\"start\":68928},{\"end\":69251,\"start\":69243},{\"end\":69269,\"start\":69261},{\"end\":69463,\"start\":69460},{\"end\":69479,\"start\":69472},{\"end\":69610,\"start\":69604},{\"end\":69631,\"start\":69622},{\"end\":69825,\"start\":69823},{\"end\":69840,\"start\":69835},{\"end\":70187,\"start\":70179},{\"end\":70199,\"start\":70191},{\"end\":70207,\"start\":70201},{\"end\":70441,\"start\":70435},{\"end\":70457,\"start\":70452},{\"end\":70474,\"start\":70469},{\"end\":70490,\"start\":70483},{\"end\":70505,\"start\":70499},{\"end\":70802,\"start\":70796},{\"end\":70819,\"start\":70814},{\"end\":70835,\"start\":70828},{\"end\":70850,\"start\":70844},{\"end\":71179,\"start\":71161},{\"end\":71200,\"start\":71191},{\"end\":71214,\"start\":71209},{\"end\":71230,\"start\":71223},{\"end\":71238,\"start\":71232},{\"end\":71549,\"start\":71543},{\"end\":71571,\"start\":71562},{\"end\":71579,\"start\":71573},{\"end\":71852,\"start\":71835},{\"end\":71872,\"start\":71865},{\"end\":71880,\"start\":71874},{\"end\":72134,\"start\":72131},{\"end\":72151,\"start\":72144},{\"end\":72166,\"start\":72158},{\"end\":72180,\"start\":72175},{\"end\":72196,\"start\":72187},{\"end\":72212,\"start\":72207},{\"end\":72229,\"start\":72223},{\"end\":72248,\"start\":72240},{\"end\":72255,\"start\":72250},{\"end\":72617,\"start\":72606},{\"end\":72635,\"start\":72625},{\"end\":72650,\"start\":72645},{\"end\":73013,\"start\":73002},{\"end\":73024,\"start\":73020},{\"end\":73039,\"start\":73034},{\"end\":73056,\"start\":73051},{\"end\":73063,\"start\":73058},{\"end\":73453,\"start\":73448},{\"end\":73471,\"start\":73464},{\"end\":73478,\"start\":73473},{\"end\":73789,\"start\":73784},{\"end\":73806,\"start\":73800},{\"end\":73822,\"start\":73815},{\"end\":73837,\"start\":73831},{\"end\":74174,\"start\":74169},{\"end\":74186,\"start\":74182},{\"end\":74202,\"start\":74196},{\"end\":74218,\"start\":74211},{\"end\":74233,\"start\":74227},{\"end\":74561,\"start\":74556},{\"end\":74573,\"start\":74569},{\"end\":74589,\"start\":74582},{\"end\":74604,\"start\":74598},{\"end\":74908,\"start\":74903},{\"end\":74920,\"start\":74916},{\"end\":74936,\"start\":74929},{\"end\":74951,\"start\":74945},{\"end\":75256,\"start\":75253},{\"end\":75266,\"start\":75263},{\"end\":75282,\"start\":75279},{\"end\":75742,\"start\":75728},{\"end\":75757,\"start\":75749},{\"end\":75772,\"start\":75764},{\"end\":75781,\"start\":75779},{\"end\":75795,\"start\":75790},{\"end\":75814,\"start\":75808},{\"end\":76248,\"start\":76243},{\"end\":76258,\"start\":76254},{\"end\":76272,\"start\":76265},{\"end\":76646,\"start\":76642},{\"end\":76661,\"start\":76657},{\"end\":76683,\"start\":76678},{\"end\":76698,\"start\":76693},{\"end\":76716,\"start\":76705},{\"end\":76731,\"start\":76726},{\"end\":76744,\"start\":76741},{\"end\":77231,\"start\":77227},{\"end\":77242,\"start\":77239},{\"end\":77261,\"start\":77251},{\"end\":77275,\"start\":77271},{\"end\":77287,\"start\":77283},{\"end\":77626,\"start\":77622},{\"end\":77637,\"start\":77634},{\"end\":77656,\"start\":77646},{\"end\":77670,\"start\":77666},{\"end\":77682,\"start\":77678},{\"end\":78018,\"start\":78014},{\"end\":78030,\"start\":78025},{\"end\":78040,\"start\":78038},{\"end\":78049,\"start\":78046},{\"end\":78061,\"start\":78057},{\"end\":78076,\"start\":78071},{\"end\":78091,\"start\":78085},{\"end\":78518,\"start\":78514},{\"end\":78531,\"start\":78525},{\"end\":78852,\"start\":78850},{\"end\":78866,\"start\":78863},{\"end\":78878,\"start\":78873},{\"end\":78890,\"start\":78886},{\"end\":78905,\"start\":78900},{\"end\":78919,\"start\":78914},{\"end\":78937,\"start\":78928},{\"end\":78946,\"start\":78943},{\"end\":78961,\"start\":78956},{\"end\":79426,\"start\":79424},{\"end\":79438,\"start\":79433},{\"end\":79450,\"start\":79446},{\"end\":79462,\"start\":79458},{\"end\":79474,\"start\":79470},{\"end\":79483,\"start\":79480},{\"end\":79498,\"start\":79493},{\"end\":79934,\"start\":79929},{\"end\":79949,\"start\":79945},{\"end\":79962,\"start\":79958},{\"end\":79977,\"start\":79972},{\"end\":79990,\"start\":79984},{\"end\":80355,\"start\":80351},{\"end\":80749,\"start\":80740},{\"end\":80927,\"start\":80918},{\"end\":80973,\"start\":80964},{\"end\":81019,\"start\":81010}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":209880690},\"end\":57578,\"start\":57168},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":234785792},\"end\":57875,\"start\":57580},{\"attributes\":{\"doi\":\"arXiv:2109.00498\",\"id\":\"b2\",\"matched_paper_id\":237372248},\"end\":58233,\"start\":57877},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":220548064},\"end\":58764,\"start\":58235},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":1880299},\"end\":58946,\"start\":58766},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":32449118},\"end\":59310,\"start\":58948},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":213299187},\"end\":59752,\"start\":59312},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":211259417},\"end\":60254,\"start\":59754},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":202577669},\"end\":60630,\"start\":60256},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":41612217},\"end\":61052,\"start\":60632},{\"attributes\":{\"doi\":\"arXiv:2106.09117\",\"id\":\"b10\"},\"end\":61396,\"start\":61054},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":18699365},\"end\":61615,\"start\":61398},{\"attributes\":{\"id\":\"b12\"},\"end\":61987,\"start\":61617},{\"attributes\":{\"id\":\"b13\",\"matched_paper_id\":2819341},\"end\":62275,\"start\":61989},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":225039902},\"end\":62973,\"start\":62277},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":235613337},\"end\":63376,\"start\":62975},{\"attributes\":{\"doi\":\"arXiv:2104.06718\",\"id\":\"b16\"},\"end\":63872,\"start\":63378},{\"attributes\":{\"id\":\"b17\",\"matched_paper_id\":3972365},\"end\":64291,\"start\":63874},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":1931807},\"end\":64646,\"start\":64293},{\"attributes\":{\"doi\":\"arXiv:2205.00263\",\"id\":\"b19\"},\"end\":65012,\"start\":64648},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":8079477},\"end\":65265,\"start\":65014},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":122381815},\"end\":65536,\"start\":65267},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":53112003},\"end\":66192,\"start\":65538},{\"attributes\":{\"id\":\"b23\"},\"end\":66303,\"start\":66194},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":208232329},\"end\":66621,\"start\":66305},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":237101098},\"end\":67193,\"start\":66623},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":124069420},\"end\":67450,\"start\":67195},{\"attributes\":{\"id\":\"b27\"},\"end\":67519,\"start\":67452},{\"attributes\":{\"id\":\"b28\",\"matched_paper_id\":117902072},\"end\":67845,\"start\":67521},{\"attributes\":{\"id\":\"b29\",\"matched_paper_id\":516928},\"end\":68265,\"start\":67847},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":196610895},\"end\":68856,\"start\":68267},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":6628106},\"end\":69135,\"start\":68858},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":237101205},\"end\":69447,\"start\":69137},{\"attributes\":{\"id\":\"b33\"},\"end\":69537,\"start\":69449},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":17998142},\"end\":69813,\"start\":69539},{\"attributes\":{\"id\":\"b35\"},\"end\":70116,\"start\":69815},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":14668082},\"end\":70367,\"start\":70118},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":233253654},\"end\":70784,\"start\":70369},{\"attributes\":{\"doi\":\"arXiv:2007.10868\",\"id\":\"b38\"},\"end\":71086,\"start\":70786},{\"attributes\":{\"doi\":\"arXiv:2103.03638\",\"id\":\"b39\"},\"end\":71464,\"start\":71088},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":206799928},\"end\":71773,\"start\":71466},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":29308357},\"end\":72058,\"start\":71775},{\"attributes\":{\"doi\":\"arXiv:1902.09592\",\"id\":\"b42\"},\"end\":72522,\"start\":72060},{\"attributes\":{\"id\":\"b43\",\"matched_paper_id\":53215541},\"end\":72914,\"start\":72524},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":67855530},\"end\":73348,\"start\":72916},{\"attributes\":{\"id\":\"b45\"},\"end\":73698,\"start\":73350},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":207796611},\"end\":74112,\"start\":73700},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":53960414},\"end\":74490,\"start\":74114},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":196059499},\"end\":74840,\"start\":74492},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":57757287},\"end\":75178,\"start\":74842},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":237206155},\"end\":75605,\"start\":75180},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":220055765},\"end\":76160,\"start\":75607},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":47016770},\"end\":76517,\"start\":76162},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":215744954},\"end\":77166,\"start\":76519},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":52347370},\"end\":77544,\"start\":77168},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":13745458},\"end\":77888,\"start\":77546},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":244114085},\"end\":78417,\"start\":77890},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":3659467},\"end\":78764,\"start\":78419},{\"attributes\":{\"id\":\"b58\",\"matched_paper_id\":225086082},\"end\":79295,\"start\":78766},{\"attributes\":{\"id\":\"b59\",\"matched_paper_id\":227210816},\"end\":79837,\"start\":79297},{\"attributes\":{\"id\":\"b60\",\"matched_paper_id\":53297058},\"end\":80279,\"start\":79839},{\"attributes\":{\"id\":\"b61\"},\"end\":80313,\"start\":80281},{\"attributes\":{\"id\":\"b62\"},\"end\":80347,\"start\":80315},{\"attributes\":{\"doi\":\"CIFAR-10\",\"id\":\"b63\"},\"end\":80380,\"start\":80349},{\"attributes\":{\"id\":\"b64\"},\"end\":80569,\"start\":80382},{\"attributes\":{\"id\":\"b65\"},\"end\":80738,\"start\":80571},{\"attributes\":{\"id\":\"b66\"},\"end\":80914,\"start\":80740},{\"attributes\":{\"doi\":\"CIFAR-10\",\"id\":\"b67\"},\"end\":80960,\"start\":80916},{\"attributes\":{\"doi\":\"CIFAR-10\",\"id\":\"b68\"},\"end\":81006,\"start\":80962},{\"attributes\":{\"doi\":\"CIFAR-10\",\"id\":\"b69\"},\"end\":81052,\"start\":81008},{\"attributes\":{\"id\":\"b70\"},\"end\":81440,\"start\":81054}]", "bib_title": "[{\"end\":57264,\"start\":57168},{\"end\":57662,\"start\":57580},{\"end\":57945,\"start\":57877},{\"end\":58305,\"start\":58235},{\"end\":58792,\"start\":58766},{\"end\":59052,\"start\":58948},{\"end\":59388,\"start\":59312},{\"end\":59810,\"start\":59754},{\"end\":60321,\"start\":60256},{\"end\":60694,\"start\":60632},{\"end\":61457,\"start\":61398},{\"end\":62045,\"start\":61989},{\"end\":62379,\"start\":62277},{\"end\":63018,\"start\":62975},{\"end\":63931,\"start\":63874},{\"end\":64362,\"start\":64293},{\"end\":65072,\"start\":65014},{\"end\":65333,\"start\":65267},{\"end\":65626,\"start\":65538},{\"end\":66389,\"start\":66305},{\"end\":66724,\"start\":66623},{\"end\":67252,\"start\":67195},{\"end\":67581,\"start\":67521},{\"end\":67915,\"start\":67847},{\"end\":68342,\"start\":68267},{\"end\":68900,\"start\":68858},{\"end\":69230,\"start\":69137},{\"end\":69595,\"start\":69539},{\"end\":70170,\"start\":70118},{\"end\":70423,\"start\":70369},{\"end\":71539,\"start\":71466},{\"end\":71826,\"start\":71775},{\"end\":72598,\"start\":72524},{\"end\":72995,\"start\":72916},{\"end\":73772,\"start\":73700},{\"end\":74157,\"start\":74114},{\"end\":74544,\"start\":74492},{\"end\":74891,\"start\":74842},{\"end\":75245,\"start\":75180},{\"end\":75716,\"start\":75607},{\"end\":76233,\"start\":76162},{\"end\":76629,\"start\":76519},{\"end\":77219,\"start\":77168},{\"end\":77614,\"start\":77546},{\"end\":78006,\"start\":77890},{\"end\":78507,\"start\":78419},{\"end\":78842,\"start\":78766},{\"end\":79416,\"start\":79297},{\"end\":79922,\"start\":79839},{\"end\":80452,\"start\":80382},{\"end\":80632,\"start\":80571}]", "bib_author": "[{\"end\":57281,\"start\":57266},{\"end\":57296,\"start\":57281},{\"end\":57322,\"start\":57296},{\"end\":57341,\"start\":57322},{\"end\":57677,\"start\":57664},{\"end\":57960,\"start\":57947},{\"end\":57974,\"start\":57960},{\"end\":57990,\"start\":57974},{\"end\":58320,\"start\":58307},{\"end\":58337,\"start\":58320},{\"end\":58353,\"start\":58337},{\"end\":58371,\"start\":58353},{\"end\":58814,\"start\":58794},{\"end\":58833,\"start\":58814},{\"end\":59068,\"start\":59054},{\"end\":59085,\"start\":59068},{\"end\":59405,\"start\":59390},{\"end\":59426,\"start\":59405},{\"end\":59441,\"start\":59426},{\"end\":59459,\"start\":59441},{\"end\":59473,\"start\":59459},{\"end\":59824,\"start\":59812},{\"end\":59845,\"start\":59824},{\"end\":59862,\"start\":59845},{\"end\":59887,\"start\":59862},{\"end\":59903,\"start\":59887},{\"end\":59915,\"start\":59903},{\"end\":59930,\"start\":59915},{\"end\":59937,\"start\":59930},{\"end\":60335,\"start\":60323},{\"end\":60347,\"start\":60335},{\"end\":60364,\"start\":60347},{\"end\":60373,\"start\":60364},{\"end\":60381,\"start\":60373},{\"end\":60392,\"start\":60381},{\"end\":60716,\"start\":60696},{\"end\":60734,\"start\":60716},{\"end\":60749,\"start\":60734},{\"end\":60764,\"start\":60749},{\"end\":60775,\"start\":60764},{\"end\":61148,\"start\":61135},{\"end\":61159,\"start\":61148},{\"end\":61172,\"start\":61159},{\"end\":61188,\"start\":61172},{\"end\":61474,\"start\":61459},{\"end\":61743,\"start\":61725},{\"end\":61762,\"start\":61743},{\"end\":61780,\"start\":61762},{\"end\":62063,\"start\":62047},{\"end\":62072,\"start\":62063},{\"end\":62089,\"start\":62072},{\"end\":62098,\"start\":62089},{\"end\":62400,\"start\":62381},{\"end\":62425,\"start\":62400},{\"end\":62441,\"start\":62425},{\"end\":62460,\"start\":62441},{\"end\":62477,\"start\":62460},{\"end\":62491,\"start\":62477},{\"end\":62507,\"start\":62491},{\"end\":62525,\"start\":62507},{\"end\":62541,\"start\":62525},{\"end\":62556,\"start\":62541},{\"end\":63041,\"start\":63020},{\"end\":63062,\"start\":63041},{\"end\":63074,\"start\":63062},{\"end\":63086,\"start\":63074},{\"end\":63101,\"start\":63086},{\"end\":63108,\"start\":63101},{\"end\":63487,\"start\":63466},{\"end\":63499,\"start\":63487},{\"end\":63516,\"start\":63499},{\"end\":63541,\"start\":63516},{\"end\":63557,\"start\":63541},{\"end\":63569,\"start\":63557},{\"end\":63583,\"start\":63569},{\"end\":63590,\"start\":63583},{\"end\":63958,\"start\":63933},{\"end\":63976,\"start\":63958},{\"end\":63988,\"start\":63976},{\"end\":64002,\"start\":63988},{\"end\":64018,\"start\":64002},{\"end\":64381,\"start\":64364},{\"end\":64740,\"start\":64723},{\"end\":64760,\"start\":64740},{\"end\":64778,\"start\":64760},{\"end\":64793,\"start\":64778},{\"end\":65082,\"start\":65074},{\"end\":65099,\"start\":65082},{\"end\":65107,\"start\":65099},{\"end\":65343,\"start\":65335},{\"end\":65360,\"start\":65343},{\"end\":65368,\"start\":65360},{\"end\":65640,\"start\":65628},{\"end\":65665,\"start\":65640},{\"end\":65683,\"start\":65665},{\"end\":65695,\"start\":65683},{\"end\":65708,\"start\":65695},{\"end\":65725,\"start\":65708},{\"end\":65739,\"start\":65725},{\"end\":65755,\"start\":65739},{\"end\":66410,\"start\":66391},{\"end\":66428,\"start\":66410},{\"end\":66745,\"start\":66726},{\"end\":66763,\"start\":66745},{\"end\":67263,\"start\":67254},{\"end\":67280,\"start\":67263},{\"end\":67289,\"start\":67280},{\"end\":67592,\"start\":67583},{\"end\":67611,\"start\":67592},{\"end\":67620,\"start\":67611},{\"end\":67927,\"start\":67917},{\"end\":67942,\"start\":67927},{\"end\":67951,\"start\":67942},{\"end\":67962,\"start\":67951},{\"end\":67970,\"start\":67962},{\"end\":67984,\"start\":67970},{\"end\":68354,\"start\":68344},{\"end\":68369,\"start\":68354},{\"end\":68386,\"start\":68369},{\"end\":68399,\"start\":68386},{\"end\":68420,\"start\":68399},{\"end\":68432,\"start\":68420},{\"end\":68444,\"start\":68432},{\"end\":68462,\"start\":68444},{\"end\":68472,\"start\":68462},{\"end\":68491,\"start\":68472},{\"end\":68914,\"start\":68902},{\"end\":68928,\"start\":68914},{\"end\":68932,\"start\":68928},{\"end\":69253,\"start\":69232},{\"end\":69271,\"start\":69253},{\"end\":69465,\"start\":69451},{\"end\":69481,\"start\":69465},{\"end\":69612,\"start\":69597},{\"end\":69633,\"start\":69612},{\"end\":69827,\"start\":69815},{\"end\":69842,\"start\":69827},{\"end\":70189,\"start\":70172},{\"end\":70201,\"start\":70189},{\"end\":70209,\"start\":70201},{\"end\":70443,\"start\":70425},{\"end\":70459,\"start\":70443},{\"end\":70476,\"start\":70459},{\"end\":70492,\"start\":70476},{\"end\":70507,\"start\":70492},{\"end\":70804,\"start\":70786},{\"end\":70821,\"start\":70804},{\"end\":70837,\"start\":70821},{\"end\":70852,\"start\":70837},{\"end\":71181,\"start\":71156},{\"end\":71202,\"start\":71181},{\"end\":71216,\"start\":71202},{\"end\":71232,\"start\":71216},{\"end\":71240,\"start\":71232},{\"end\":71551,\"start\":71541},{\"end\":71573,\"start\":71551},{\"end\":71581,\"start\":71573},{\"end\":71854,\"start\":71828},{\"end\":71874,\"start\":71854},{\"end\":71882,\"start\":71874},{\"end\":72136,\"start\":72123},{\"end\":72153,\"start\":72136},{\"end\":72168,\"start\":72153},{\"end\":72182,\"start\":72168},{\"end\":72198,\"start\":72182},{\"end\":72214,\"start\":72198},{\"end\":72231,\"start\":72214},{\"end\":72250,\"start\":72231},{\"end\":72257,\"start\":72250},{\"end\":72619,\"start\":72600},{\"end\":72637,\"start\":72619},{\"end\":72652,\"start\":72637},{\"end\":73015,\"start\":72997},{\"end\":73026,\"start\":73015},{\"end\":73041,\"start\":73026},{\"end\":73058,\"start\":73041},{\"end\":73065,\"start\":73058},{\"end\":73455,\"start\":73446},{\"end\":73473,\"start\":73455},{\"end\":73480,\"start\":73473},{\"end\":73791,\"start\":73774},{\"end\":73808,\"start\":73791},{\"end\":73824,\"start\":73808},{\"end\":73839,\"start\":73824},{\"end\":74176,\"start\":74159},{\"end\":74188,\"start\":74176},{\"end\":74204,\"start\":74188},{\"end\":74220,\"start\":74204},{\"end\":74235,\"start\":74220},{\"end\":74563,\"start\":74546},{\"end\":74575,\"start\":74563},{\"end\":74591,\"start\":74575},{\"end\":74606,\"start\":74591},{\"end\":74910,\"start\":74893},{\"end\":74922,\"start\":74910},{\"end\":74938,\"start\":74922},{\"end\":74953,\"start\":74938},{\"end\":75258,\"start\":75247},{\"end\":75268,\"start\":75258},{\"end\":75284,\"start\":75268},{\"end\":75744,\"start\":75718},{\"end\":75759,\"start\":75744},{\"end\":75774,\"start\":75759},{\"end\":75783,\"start\":75774},{\"end\":75797,\"start\":75783},{\"end\":75816,\"start\":75797},{\"end\":76250,\"start\":76235},{\"end\":76260,\"start\":76250},{\"end\":76274,\"start\":76260},{\"end\":76648,\"start\":76631},{\"end\":76663,\"start\":76648},{\"end\":76685,\"start\":76663},{\"end\":76700,\"start\":76685},{\"end\":76718,\"start\":76700},{\"end\":76733,\"start\":76718},{\"end\":76746,\"start\":76733},{\"end\":76765,\"start\":76746},{\"end\":77233,\"start\":77221},{\"end\":77244,\"start\":77233},{\"end\":77263,\"start\":77244},{\"end\":77277,\"start\":77263},{\"end\":77289,\"start\":77277},{\"end\":77628,\"start\":77616},{\"end\":77639,\"start\":77628},{\"end\":77658,\"start\":77639},{\"end\":77672,\"start\":77658},{\"end\":77684,\"start\":77672},{\"end\":78020,\"start\":78008},{\"end\":78032,\"start\":78020},{\"end\":78042,\"start\":78032},{\"end\":78051,\"start\":78042},{\"end\":78063,\"start\":78051},{\"end\":78078,\"start\":78063},{\"end\":78093,\"start\":78078},{\"end\":78520,\"start\":78509},{\"end\":78533,\"start\":78520},{\"end\":78854,\"start\":78844},{\"end\":78868,\"start\":78854},{\"end\":78880,\"start\":78868},{\"end\":78892,\"start\":78880},{\"end\":78907,\"start\":78892},{\"end\":78921,\"start\":78907},{\"end\":78939,\"start\":78921},{\"end\":78948,\"start\":78939},{\"end\":78963,\"start\":78948},{\"end\":79428,\"start\":79418},{\"end\":79440,\"start\":79428},{\"end\":79452,\"start\":79440},{\"end\":79464,\"start\":79452},{\"end\":79476,\"start\":79464},{\"end\":79485,\"start\":79476},{\"end\":79500,\"start\":79485},{\"end\":79936,\"start\":79924},{\"end\":79951,\"start\":79936},{\"end\":79964,\"start\":79951},{\"end\":79979,\"start\":79964},{\"end\":79992,\"start\":79979},{\"end\":80357,\"start\":80351},{\"end\":80751,\"start\":80740},{\"end\":80929,\"start\":80918},{\"end\":80975,\"start\":80964},{\"end\":81021,\"start\":81010}]", "bib_venue": "[{\"end\":57365,\"start\":57341},{\"end\":57706,\"start\":57677},{\"end\":58025,\"start\":58006},{\"end\":58450,\"start\":58371},{\"end\":58850,\"start\":58833},{\"end\":59114,\"start\":59085},{\"end\":59522,\"start\":59473},{\"end\":59995,\"start\":59937},{\"end\":60428,\"start\":60392},{\"end\":60834,\"start\":60775},{\"end\":61133,\"start\":61054},{\"end\":61494,\"start\":61474},{\"end\":61723,\"start\":61617},{\"end\":62117,\"start\":62098},{\"end\":62615,\"start\":62556},{\"end\":63166,\"start\":63108},{\"end\":63464,\"start\":63378},{\"end\":64076,\"start\":64018},{\"end\":64458,\"start\":64381},{\"end\":64721,\"start\":64648},{\"end\":65126,\"start\":65107},{\"end\":65387,\"start\":65368},{\"end\":65829,\"start\":65755},{\"end\":66245,\"start\":66196},{\"end\":66437,\"start\":66428},{\"end\":66854,\"start\":66763},{\"end\":67307,\"start\":67289},{\"end\":67481,\"start\":67452},{\"end\":67653,\"start\":67620},{\"end\":68044,\"start\":67984},{\"end\":68552,\"start\":68491},{\"end\":68991,\"start\":68932},{\"end\":69276,\"start\":69271},{\"end\":69484,\"start\":69481},{\"end\":69661,\"start\":69633},{\"end\":69957,\"start\":69842},{\"end\":70228,\"start\":70209},{\"end\":70550,\"start\":70507},{\"end\":70914,\"start\":70868},{\"end\":71154,\"start\":71088},{\"end\":71605,\"start\":71581},{\"end\":71901,\"start\":71882},{\"end\":72121,\"start\":72060},{\"end\":72711,\"start\":72652},{\"end\":73124,\"start\":73065},{\"end\":73444,\"start\":73350},{\"end\":73898,\"start\":73839},{\"end\":74294,\"start\":74235},{\"end\":74658,\"start\":74606},{\"end\":75000,\"start\":74953},{\"end\":75351,\"start\":75284},{\"end\":75875,\"start\":75816},{\"end\":76333,\"start\":76274},{\"end\":76820,\"start\":76765},{\"end\":77348,\"start\":77289},{\"end\":77709,\"start\":77684},{\"end\":78142,\"start\":78093},{\"end\":78584,\"start\":78533},{\"end\":79022,\"start\":78963},{\"end\":79558,\"start\":79500},{\"end\":80051,\"start\":79992},{\"end\":80296,\"start\":80281},{\"end\":80330,\"start\":80315},{\"end\":80458,\"start\":80454},{\"end\":80638,\"start\":80634},{\"end\":80778,\"start\":80751},{\"end\":81246,\"start\":81054},{\"end\":58516,\"start\":58452},{\"end\":65890,\"start\":65831},{\"end\":66932,\"start\":66856},{\"end\":70580,\"start\":70552},{\"end\":75405,\"start\":75353}]"}}}, "year": 2023, "month": 12, "day": 17}