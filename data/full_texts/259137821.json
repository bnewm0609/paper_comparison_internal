{"id": 259137821, "updated": "2023-10-04 23:46:16.447", "metadata": {"title": "Scalable 3D Captioning with Pretrained Models", "authors": "[{\"first\":\"Tiange\",\"last\":\"Luo\",\"middle\":[]},{\"first\":\"Chris\",\"last\":\"Rockwell\",\"middle\":[]},{\"first\":\"Honglak\",\"last\":\"Lee\",\"middle\":[]},{\"first\":\"Justin\",\"last\":\"Johnson\",\"middle\":[]}]", "venue": "ArXiv", "journal": null, "publication_date": {"year": 2023, "month": null, "day": null}, "abstract": "We introduce Cap3D, an automatic approach for generating descriptive text for 3D objects. This approach utilizes pretrained models from image captioning, image-text alignment, and LLM to consolidate captions from multiple views of a 3D asset, completely side-stepping the time-consuming and costly process of manual annotation. We apply Cap3D to the recently introduced large-scale 3D dataset, Objaverse, resulting in 660k 3D-text pairs. Our evaluation, conducted using 41k human annotations from the same dataset, demonstrates that Cap3D surpasses human-authored descriptions in terms of quality, cost, and speed. Through effective prompt engineering, Cap3D rivals human performance in generating geometric descriptions on 17k collected annotations from the ABO dataset. Finally, we finetune Text-to-3D models on Cap3D and human captions, and show Cap3D outperforms; and benchmark the SOTA including Point-E, Shape-E, and DreamFusion.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "2306.07279", "mag": null, "acl": null, "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/abs-2306-07279", "doi": "10.48550/arxiv.2306.07279"}}, "content": {"source": {"pdf_hash": "4279a38a098d1d359881b73c6a88a112fe93443a", "pdf_src": "Arxiv", "pdf_uri": "[\"https://export.arxiv.org/pdf/2306.07279v2.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "08d57e7078cbfa28bd2d5768e22f69aae09cb445", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/4279a38a098d1d359881b73c6a88a112fe93443a.txt", "contents": "\nScalable 3D Captioning with Pretrained Models\n16 Jun 2023\n\nTiange Luo \nUniversity of Michigan\n\n\nChris Rockwell \nUniversity of Michigan\n\n\nHonglak Lee \nUniversity of Michigan\n\n\nLG AI Research\n\n\nJustin Johnson \nUniversity of Michigan\n\n\nScalable 3D Captioning with Pretrained Models\n16 Jun 2023Cap3D uses eight. Additional examples are available in Appendix B. * joint first authorship; \u2020 equal advising Method A/B Human Testing Cost per Annotation Win % (Tie %) 1k Objects Speed Human 37.8% \u00b1 0.5% (9.5%) $87.18 1.4k / day Cap3D 52.3% \u00b1 0.5% (9.5%) $8.35 65k / day 1k Objects Cost Breakdown BLIP2 $3.79 CLIP $0.38 GPT4 $4.18 Cap3D Total Cost $8.35\nWe introduce Cap3D, an automatic approach for generating descriptive text for 3D objects. This approach utilizes pretrained models from image captioning, image-text alignment, and LLM to consolidate captions from multiple views of a 3D asset, completely side-stepping the time-consuming and costly process of manual annotation. We apply Cap3D to the recently introduced large-scale 3D dataset, Objaverse, resulting in 660k 3D-text pairs. Our evaluation, conducted using 41k human annotations from the same dataset, demonstrates that Cap3D surpasses human-authored descriptions in terms of quality, cost, and speed. Through effective prompt engineering, Cap3D rivals human performance in generating geometric descriptions on 17k collected annotations from the ABO dataset. Finally, we finetune text-to-3D models on Cap3D and human captions, and show Cap3D outperforms; and benchmark the SOTA including Point\u00b7E, Shap\u00b7E, and DreamFusion. * joint first authorship; \u2020 equal advising Preprint.\n\n3D model of a sakura soft drink can with purple and yellow gradient, Japanese writing, and purple flowers.\n\nA 3D model of a blue grand piano with spikes and sharp teeth resembling a shark mouth.\n\nA 3D model of a metal cube featuring a skull, pizza, and various stickers.    1 Introduction\n\nText-conditioned 3D synthesis [1][2][3] could revolutionize the creation process of 3D assets, impacting various sectors, including 3D design, virtual reality [4], film [5], robotics [6,7], and autonomous driving [8]. However, challenges persist, namely the high cost of 3D asset creation and the scarcity of high-quality captions for 3D assets. Objaverse [9] takes a step towards this as the first public largescale 3D object dataset. Unfortunately, while objects contain paired metadata, these do not serve as informative captions, as shown in Table 3. In contrast with 3D, a plethora of high-quality text-image paired data is publicly available [10][11][12][13][14]. This data has led to incredible recent progress in image-text learning [15][16][17][18], text-conditioned image synthesis [19][20][21][22][23][24], and image captioning [25][26][27][28][29].\n\nIn this work, we present Cap3D, a method to automate 3D object annotation. Our key insight is to leverage the abundance of knowledge in pretrained image-text models to remedy the lack of existing 3D-text data. The core of our data collection process is to apply an image captioning model (BLIP2 [29]) to a set of 3D asset renders, use an image-text alignment model (CLIP [16]) to filter captions, and apply a language model (GPT4 [30]) to fuse the filtered captions across views. Critically, the models we apply are pretrained on varied and large-scale text-image [11][12][13][31][32][33], and text [34], data; and approach complementary problems. As a result, each model adds additional value to the framework, as we show in Table 3.\n\nCap3D is agnostic to 3D asset sources and can be effectively scaled to larger extents with increased 3D assets and computational resources. In this paper, we apply it primarily to Objaverse, gathering a dataset of 660k 3D-text pairs. Through object rendering and captioning, we enable ethical filtering of 3D objects via both image and text, as detailed in \u00a7 3.2. We publicly release all of our collected data including automated and human-annotated captions, along with associated Point Clouds and Rendered Images, at huggingface.co/datasets/tiange/Cap3D. The dataset is released under ODC-By 1.0 license. We will release trained models and code for replicating the benchmark table.\n\nWe validate our collection approach by collecting over 50k crowdsourced captions on over 40k objects. We conduct human evaluations and show on Objaverse that our automated captions are superior to crowdsourced captions in quality, cost, and speed (Table 1, details in Appendix A). Specifically, it is preferred 35% more often by humans, costs more than 10 times less, and is over 40 times faster, assuming only 8A40 GPUs. We also test the limits of automated captioning. We consider a separate task of captioning geometry (as shown in Figure 1 bottom-right) using ABO, a dataset of 3D models with complex geometries [35]. Shown in Table 4, our automated captioning underperforms humans. However, by formulating description as a question answering task (detailed in \u00a7 3.1), we show stronger performance compared to crowdsourced workers. This result shows the ability of our method to adapt beyond traditional captioning and still be highly competitive.\n\nFinally, our high-quality gathered 3D-text dataset enables us to train and validate large-scale textto-3D models. In \u00a75.3, we evaluate several state-of-the-art methods on Objaverse out-of-the box, including Point\u00b7E, Shap\u00b7E, DreamFields, and DreamFusion. Finetuning on our data typically shows meaningful improvements, demonstrating the value of the collected dataset. In addition, we show our automatically collected captions yield better finetuning performance than human captions -even at the same scale. At full scale, finetuning is further boosted.\n\n\nRelated Work\n\nObtaining 3D-text pairs at scale is challenging, and we take inspiration from image-text datasets and methods when approaching this task.\n\nImage-Text Data and Modeling. Early image captioning [36][37][38] and text-image representation learning methods [39,40] were built using CNNs [41][42][43] and LSTMs [44,45], leveraging humanannotated datasets [31][32][33]46]. Text-to-image methods used similar datasets, and relied on GANs [47,48] and VQVAEs [19,[49][50][51]. The advent of semi-automated image-text collection has enabled successful scaling of datasets [10][11][12][13][14] and models [25][26][27][28]. Transformer-based architectures [16,52,53] and diffusion models [54][55][56][57][58][59] have scaled best to large data; we employ transformer-based methods through our captioning process and adopt diffusion models for text-to-3D experiments.\n\nTraining models upon large datasets and using the corresponding trained models to filter larger data has led to datasets of rapidly increasing size [13,14]. In addition to filtering, trained models have been used to annotate new data with high-quality [60]. We take this approach, captioning rendered views with BLIP2 [29], refining with CLIP [16,61], and summarizing with GPT4 [62]; all of which are trained on large datasets, including [11][12][13][31][32][33]. Concurrent works [63][64][65] use automated captioning on 2D images using an older system [66] or based upon metadata [65,67].\n\n3D-Text Data and Modeling. Until recently, 3D data was of relatively small scale (\u223c 50k objects) [68][69][70][71]. Labeled 3D-text data was scarce, relying on human annotation, and typically limited to ShapeNet [68] chairs [72] or tables and chairs [73,74], and ScanNet [75,76]. This enabled prior work to undertake the task of 3D captioning [77][78][79] or text-to-3D [73,77,[80][81][82][83] at small scale. Methods that approached text-to-3D would sometimes avoid 3D supervision entirely [3,[84][85][86], leading to slow generation due to many optimization steps. We annotate a small-scale dataset containing 3D furniture, ABO [35], to evaluate the ability of Cap3D to specify fine-grained geometry.\n\nObjaverse [9] introduced a diverse set of objects over 10 times the size of the prior largest public 3D dataset [68]. This data is our primary captioning focus, and we associate a single caption with each object in Objaverse after filtering. Concurrent works [65,87] gather text associated with Objaverse, but do not fuse captions across views [87] or rely upon metadata [65], and do not approach text-to-3D.\n\nThe concurrent studies 3DGen [2] learns text and image to 3D on Objaverse; Point\u00b7E [88] and Shap\u00b7E [89] learn text-to-3D models on a large-scale 3D dataset, but none have fully disclosed their code or data. Point\u00b7E involves two variants and released a text-to-3D model and a text-to-image-to-3D model by finetuning GLIDE [23] and training an image-to-point cloud diffusion model [90]. Other recent works [91,92] also focus on scaled image-3D generation. We show finetuning on our captions improves Point\u00b7E performance despite having already been trained on large amounts of Internet data.\n\n\nMethod\n\n\nCaptioning Process\n\nOur task is to produce a single descriptive caption given a 3D asset. Our proposed method, Cap3D, employs a four-step process. First, we render a set of 2D views for each 3D object. Next, we apply image captioning to achieve preliminary descriptions. As these captions may contain inaccuracies, an image-text alignment model, CLIP, is introduced in the third step to rectify errors. Finally, an LLM is employed to unify captions from various perspectives, creating a comprehensive caption. This process is shown in Figure 2 and detailed below.\n\nObject Rendering: We render using Blender at 512\u00d7512 from M = 8 high-information camera angles rotating horizontally around the object, with two slightly below and the rest slightly above the object, to cover all the object details. The reason we prefer multiple views is a forward-facing view may miss self-occluded object details (e.g. Figure 1 row 1) or face strange appearance and/or lighting. In contrast, multiple views will see much of the object from different viewpoints, increasing the number of chances for a captioning model to predict objects in detail. For instance, in Figure 2, the back view 1 identifies the \"yellow handle\", which is barely visible in forward view M .\n\nImage Captioning: We use BLIP2 [29] for captioning, selecting the largest pretrained model adapting ViT-G [53,93] image encoder and FlanT5XXL [94] text encoder. We generate N = 5 captions per rendered image using nucleus sampling [95]. By generating multiple captions, we increase the likelihood of generating correct details (e.g. \"black and yellow toy bomb\" in Figure 2 view M caption 1). Incorrect captions, such as \"scissors\" in Figure 2   generates one answer to a prompt asking what object is pictured. The answered object is passed into a second prompt, which asks its structure and geometry, and generates 5 answers.\n\nCaption Selection: While BLIP2 often generates high-quality captions, it is not uncommon for samples to contain mistakes, particularly in non-forward facing views such as \"yellow cup\", in Figure  2 view 1, caption N . To reduce the frequency of mistakes, we compute CLIP [16] ViT-B/32 [53] encodings from each of 5 captions and the associated image, and select the caption maximizing cosine similarity. CLIP tends to select good captions for each view, e.g. Figure 2: view 1, BLIP2 caption 1 and view M , caption 1. CLIP is complementary to BLIP2 as not only does it have different training details and architecture, but it trains on different data. While BLIP2 is trained upon COCO [31], Visual Genome [32], CC3M [11], CC12M [12], SBU [33] and LAION400M [13]; CLIP is trained upon a dataset of 400M images based on frequent text occurrence in Wikipedia.\n\nCaption Consolidation: Accumulating information across viewpoints to form a complete picture of 3D objects is challenging, but crucial. We find prompting of GPT4 [62] to summarize the M captions results in good parsing of the details across captions. By applying GPT4 as the final summary step, it can both include significant details and remove unlikely ones. For example, the final caption in Figure 2 filters the incorrect information, from view 2, \"toy ball\", while keeping key details, including \"handle\" and \"straw\". The alternative order of GPT4 followed by CLIP would result in (1) GPT4 having to make sense of more incorrect input details and (2) CLIP simply selecting between aggregate captions instead of being able to error-correct small mistakes. The effectiveness of introducing GPT4 is verified in ablations (Table 3).\n\n\nEthical Filtering\n\nCaptions generated and images rendered by Cap3D enhance the identification and mitigation of legal and ethical issues associated with large-scale 3D object datasets, including identifiable information and NSFW content.\n\nWe manage two datasets: Objaverse and ABO. In Objaverse, our main responsibility involves dealing with artist-created assets. These can include identifiable elements such as human face scans and NSFW objects. Objaverse contains approximately 800k objects, which makes the manual verification of each asset impractical. The ABO dataset, on the other hand, is smaller and mostly consists of furniture. We manually ensure the ethical integrity of this dataset.\n\nWe begin by filtering Objaverse to include only those objects that can be rendered and shared. Objects with CC BY-NC-SA and CC BY-NC licenses are removed, while we retain those with CC BY, CC BY-SA, and CC0 licenses, thereby facilitating commercial usage of our data. This process reduces the dataset size from 798k to 723.7k objects. Furthermore, we exclude objects that lack sufficient camera information for rendering, leaving us with 680k objects.\n\nWe next follow prior work [10] and use a face detector [96] and NSFW classifier [97,98] on forwardfacing object renders and filter detected objects with score >= 0.9. The face detector filters out 18.6k\n\nA plant with white flowers Flower.\n\n\nMetadata Human\n\nCap3D 3D model of a branch with white flowers and green leaves. objects, and the NSFW classifier filters out 217 objects. Text is also carefully processed. Our final captions are the output of GPT4, which has been trained to filter out inappropriate or harmful content [62]. We run a standard blocklist [99] on its output, removing any object-caption pairs including blocked words. This filters out 226 objects. After all the filtering, we are left with 661k objects in the Objaverse dataset. We manually estimate detection precision and recall in Table 2. To summarize, our process detects over 19k objects, of which a nontrivial amount is accurately removed. We estimate roughly 1k face and less than 1k NSFW are missed, using a conservative standard (e.g. missed faces are typically sports cards).\n\n\nDataset\n\nWe collect captions in two distinct settings: Objaverse, a large and varied dataset of artist-created 3D assets; and ABO, a small dataset of real products, typically furniture.\n\n\nObjaverse Captions\n\nObjaverse [9] features roughly 800k 3D object assets across 21k classes designed by over 100k artists. It is of significantly larger scale than prior work; the paper shows this size enables more diversity by generative 3D models trained upon it. It is released under the ODC-By 1.0 license, permitting subsequent researchers to curate new data from it. Metadata is paired with many assets, however as seen in Figure 3 (right), metadata caption length is frequently short or empty. We collect two caption datasets on Objaverse. First, an automated set of one caption for each of 660k objects using Cap3D (a total of 660k captions). Second, a crowdsourced set of 41.4k captions spanning 39.7k objects for evaluating generated captions. Captions are collected using thehive.ai, a crowdsourced platform similar to AMT. Workers are given instructions with gold-standard sample captions, see the same 8 views as models during captioning, and are routinely monitored. Poor captioning performance results in a ban and deletion of the worker's captions. Crowdsourced captions are also filtered using the blocklist in \u00a7 3.2. Figure 3 (left) shows human captions provide more detail than metadata, but automated captions tend to be most descriptive.\n\n\nABO Geometry Captions\n\nABO [35] is a collection of 3D models of Amazon products and is primarily furniture. ABO serves as an important contrast to Objaverse as it consists of a small number of classes varying primarily in geometry. Captioning, therefore, needs to focus more on structure as opposed to semantic category. To emphasize this focus, we consider the task of captioning the geometric structure of objects without color or texture (seen in the bottom right of Figure 1). Like Objaverse, ABO contains metadata that is typically quite short (Table 4), resulting in limited detail. We collect three sets of captions on the 6.4k ABO splits of [77]: crowdsourced (a total of 17.2k captions), captions generated by Cap3D\n\nBed.\n\n\nCap3D\n\n\nMetadata Human\n\n3D rendering of a couch.\n\nCap3D (QA) Three-seater sofa with a slender, curved backrest and armrests.\n\nA three seater sofa with low backrest to the height of the armrests. (a total of 6.4k captions), and captions generated by Cap3D (QA) which uses the two-stage prompt captioning (a total of 6.4k captions). Crowdsourced captions follow similar detail to Objaverse with the exception instructions and examples are focused on geometric structure. We compare alternatives in Figure 4. In contrast to Objaverse, human geometric descriptions on ABO are more detailed than captioning. With prompting (QA), the Cap3D pipeline can rival human descriptions.\n\n\nExperiments\n\nIn this section, we first validate the quality of Cap3D captions against metadata and human-authored captions on both Objaverse and ABO. To verify Cap3D captions are helpful in practice, we next compare text-to-3D models finetuned on both human-authored captions and Cap3D (using the same >30k set as crowdsourced captions). Finally, we evaluate state-of-the-art text-to-3D models on our captions at scale to measure if finetuning on our captions can improve performance.\n\n\n3D Captioning on Objaverse\n\nDataset. We evaluate caption quality on three subsets of Objaverse: (1) a random set of 22k objects containing a human caption, (2) a random split of 5k objects containing a human caption, and (3) a random 5k split across the entire dataset.\n\nBaselines. In data splits (1) and (2), we compare the caption generated by Cap3D with humanauthored annotations, Human, and existing Objaverse metadata, Metadata, described in \u00a7 4.1. Split (1) is used for A/B testing of Cap3D vs. Human, as shown in Table 1, at scale. Collecting A/B comparison is expensive, so we compute more extensive experiments on the smaller set (2) in Table 3.\n\nIn data split (3), we ablate the main components of Cap3D into BLIP2 and +GPT4. BLIP2 uses only the image captioning component of our method, taking a front-view rendering and producing a single output caption. +GPT4 uses the same image captioning process of our method, producing 5 captions for each of 8 views. However, instead of using CLIP to filter 5 captions from each view, it directly summarizes all 40 captions into a final caption.\n\nMetrics. Our primary metric is human judgment A/B tests, where we ask workers to select between two captions on a scale of 1-5, where 3 is a tie. Workers are carefully monitored and each comparison has at least 10k observations across 5k objects.We report mean score, along with the percent each method is preferred (i.e. scores a 4 or 5). We use automated metrics CLIPScore [16,61], the cosine similarity of CLIP encodings with input images; and ViLT Image and Text Retrieval, which ranks likely image-text pairs, from which one computes precision.\n\nWe emphasize CLIPScore is not our primary metric since our captioning model utilizes CLIP. BLIP2 utilizes ViT-L/14 and ViT-g/14, while our filtering uses ViT-B/32, so following previous work [84] we compute CLIP score using a different model to reduce bias (ViT-B/16). However, we report it as it has shown a higher correlation with human judgments than other automated metrics [61]. ViLT [100] is trained on different data and is a different architecture than CLIP, providing an orthogonal metric.  Results. We report large scale A/B testing (1) against Human in Table 1, which shows Cap3D is better across metrics, with high confidence. The top three rows of Table 3 use the smaller humancaptioned split (2), and demonstrate Cap3D's superior performance over Objaverse metadata and human-authored captions across A/B studies and automated metrics. The bottom three rows of Table 3, studied across a random split of the full dataset (3), reveal that while BLIP2 is effective, incorporating multiple views with +GPT4 enhances performance. As shown in Figure 5, GPT4 adds detail by consolidating view-specific information. Filtering using +CLIP (Cap3D) mitigates false details by purging subpar captions from GPT input. In addition to reducing errors, utilizing CLIP also reduces GPT input captions from 40 to 8, effectively decreasing token numbers and facilitating a cost reduction from $15.33 to $4.18.\n\n\nGeometry 3D Captioning on ABO\n\nDataset. We evaluate geometric captioning on a 6.4k object split from ABO [35,77], comparing Cap3D captions for each object against a maximum of two human-authored ones. To emphasize geometric focus, images used for model input and human assessment are texture-free and colorless.\n\nBaselines and Metrics. We use two automated variants from \u00a73.1: Cap3D and Cap3D (QA), which uses a two-stage prompt captioning to ask more about the input 3D geometry; and compare to crowdsourced human descriptions, Human, detailed in \u00a74.1, and ABO metadata, Meta.\n\nOur primary metric of comparison is similar human A/B testing to \u00a75.1, since automated metrics such as CLIPScore do not accurately represent the distance between fine-grained captions and images as shown in [77].\n\nResults. In stark contrast to Objaverse, Human captions beat automated (Cap3D) in Table 4. Automated captions alone contain little geometric detail (e.g., Figure 4), making Cap3D unsuited for this setting. However, by using the two-stage prompt engineering, Cap3D (QA) is preferred to Human. Shown in Figure 4, Cap3D (QA) produces significant fine-grained geometric detail as well as longer captions in general. In contrast, Metadata is clearly the weakest baseline.  Metrics. We use standard metrics from prior work [3,84,88,89] to evaluate. Primarily, these are CLIP Score and CLIP R-Precision. CLIP R-Precision ranks a rendered image against all text pairs in the test set by CLIP cosine similarity, and computes precision upon true text-image correspondence.\n\nSince we have ground truth images, we calculate the FID [107] of 3D rendered images against ground truth images, as well as assess CLIP Score on these reference images. We also use ViLT Retrieval R-Precision, used in 5.1, which has the same evaluation procedure as CLIP R-Precision with a different model.\n\nResults. Table 5 lists the results of finetuning using human-authored and Cap3D captions. Point\u00b7E improves after finetuning upon human captions. However, performance is further improved using our captions on the same dataset; and improved most by training upon the full dataset. This result strongly defends Cap3D captioning at scale. Shap\u00b7E does not improve on CLIP metrics after finetuning in any dataset, but performs the least bad on the full dataset using our captions; and FID improves most. Table 6 presents results from several state-of-the-art pretrained and finetuned models using Cap3Dgenerated captions. The models finetuned on our captions generally outperform pretrained models under the FID metric. For CLIP-related metrics, the finetuned models of Point\u00b7E (Text-to-3D) and StableDiffusion + Point\u00b7E (Im-to-3D) also beat their pretrained counterparts. Point\u00b7E and Stable Diffusion have been trained on massive datasets, so improvement from finetuning is strong evidence Cap3D captions are effective. The observed downturns in Shap\u00b7E could be attributed to at least two factors. First, our replication of their privately-available train code is unstable, often resulting in NaN loss during finetuning. We restart from earlier checkpoints upon crashing, but the result alone is concerning. Second, we exclusively finetune the diffusion model in Shap\u00b7E's two-stage approach.\n\nQualitative results in Figure 6 validate quantitative findings. Point\u00b7E and Stable Diffusion baselines show large improvements from finetuning, while Shap\u00b7E can better fit the Objaverse data distribution (corresponding to improved FID).   Optimization baselines, shown in Table 7, perform very well upon CLIP-based metrics, consistent with prior work [89]. In fact, DreamField outperforms ground truth images in CLIP metrics. This demonstrates DreamField overfits to the CLIP metric, which is the standard protocol for text-to-3D evaluation. We propose to also consider ViLT precision (see \u00a75.1). This helps mitigate the bias of CLIP, though DreamField performance on this metric is still strong.\n\n\nConclusion\n\nIn this work, we collect (1) 3D object captions at scale, creating the largest publicly available highquality 3D-text by an order of magnitude. To do so we propose Cap3D, an automated pipeline leveraging several models pretrained on large datasets, and show design choices are important to performance. In addition, we collect (2) a dataset of geometric captions upon fine-grained 3D objects. This helps analyze shortcomings of automated captioning and study the potential of question answering, while yielding geometric descriptions for 3D assets of real objects paired with real images. These datasets serve as benchmarks for text-to-3D tasks (1) at scale and (2) in geometric detail.\n\n\nAppendix A Price Breakdown Details\n\nThis section provides our details computation for We compute our GPT4 cost by averaging input token numbers, as OpenAI GPT4 API (8k context) costs 0.03/1k tokens, Our input prompt is: \"Given a set of descriptions about the same 3D object, distill these descriptions into one concise caption. The descriptions are as follows: 'captions'. Avoid describing background, surface, and posture. The caption should be:\", which consists of (1) text prompt and (2)  The average cost per 1k objects for human-authored annotation is computed as the average expenditure on the crowdsourcing platform, Hive. The human annotation speed is computed by averaging the annotation progress across our whole annotation process.\n\nWe do not report the average cost of Cap3D (QA) in the main paper, as we only use it on ABO. For completeness, we report it here. The one distinction is BLIP2 is run twice instead of once for the two-stage question answering (QA). The cost of BLIP2 thus doubles, from $3.79 to $7.58; and total cost increases from $8.35 to $12.14 per 1k objects.\n\n\nAppendix B Additional 3D Captioning Results\n\n3D low poly model of a soldier/warrior with a sword and shield.\n\n3D model of a red and white molecule.\n\nA 3D model of a helmet with a red, yellow, and black gas respirator mask and a red light.\n\nA 3D model of a red and white mushroom.\n\nA 3D rendering of a spiral staircase with a railing in a white room with an open door.\n\nA red and blue leather suitcase with a cross, resembling an old medical bag or first aid box. z A 3D model of a Rubik's Cube featuring blue, orange, red, green, and yellow squares.\n\nA 3D model of a robotic horse with wings and spikes.\n\n3D model of a green and yellow metal truss with two holes and cross beam.\n\nA 3D model of a house with a red roof, fence, and carousel.\n\nA white 3D printed figurine of Santa Claus with reindeer antlers, holding a stick and standing on a rock.\n\nL-shaped sectional sofa with a chaise, U-shaped backrest, curved armrests, and a footstool on one side. A 3D model of a white cylindrical object with features resembling a radiator, vase, and light bulb.\n\n3D illustration of a small yellow flower with leaves.\n\n\n3D model of Transformers\n\nOptimus Prime blue and red truck Figure 9: Random 3D captioning examples generated by Cap3D. Two views of 3D objects (Objaverse [9]) are shown here, Cap3D uses eight.\n\nA 3D rendering of a house with a roof structure featuring pink lines.\n\nA white doily with a floral pattern and a circular ceiling light with a flower design.\n\n3D rendering of a white square object 3D model of a wooden park bench with leaves on it.\n\nA 3D model of a modern blue leather sofa with metal legs and a gray metal shelf with two hooks.\n\n3D model of a green pea pod and propeller with two peas.\n\nz 3D model of a marble nativity scene figurine.\n\nA 3D white skull model with red eyes, resembling a combination of animal, squid, and starfish features.\n\n\n3D model of a cow\n\nWhite Nintendo Wii console with a power outlet and USB port.\n\nA yellow gold ring with a pink sapphire stone.\n\nA 3D scene featuring a destroyed house, building, plane, and car, with a flying bird.  A 3D model of a red shipping container with a blue and white logo and white label on it.\n\n3D model of a gravestone/tombstone.\n\nBrown bag with coins and a green string, featuring a coin falling out. Figure 11: Random 3D captioning examples generated by Cap3D. Two views of 3D objects (Objaverse [9]) are shown here, Cap3D uses eight.\n\nA 3D model of a colorful, multi-colored boat with a flag, oars, and paddles.\n\nA purple and yellow toy lantern with a handle.\n\nA 3D model of a wooden door with a design, featuring a wooden sign, curved wall, and clock.\n\nA 3D model featuring a bird, twigs, fishing poles, a dragonfly, and skis with poles.\n\nA 3D blue box with a hole and sand on it.\n\n3D model of a small white and blue toy robot teddy bear.\n\nz 3D model of a cricket bat, royalty-free vector illustration.\n\nA 3D rendering of a row of yellow poles.\n\nWhite 3D spiral staircase model 3D model of a white axe Wooden coat rack in a 3D model.\n\nA 3D model of a green plant with leaves and an eye. A knife and sliced bread on a wooden cutting board.\n\nA 3D model of a two-story house with a roof structure.\n\nA 3D rendering of a fish next to a Rubik's cube.\n\nA 3D model of a house on a grassy field with a road in front.\n\n3D model of a flower in a black box with a purple container and blue corner shelf. Figure 13: Random 3D captioning examples generated by Cap3D. Two views of 3D objects (Objaverse [9]) are shown here, Cap3D uses eight. \n\n\nCap3D\n\nHuman Metadata Figure 14: Comparative Analysis: Cap3D Generated Caption vs Human-Annotated Caption vs Objaverse Metadata [9]. Two views of 3D objects are shown here, Cap3D and human use eight.\n\n3D model of a jar with a green lid.\n\n3D rendering of grey Champion sweatpants with red and black logo.\n\nA 3D model of a rusty, old train engine.\n\nThis is a backup of a Poly Asset named Jar of jam. Saved from Poly by Google. Preview may be without textures, they are still in the Download ZIP with a preview thumbnail.\n\na three layer structure with green oval top and white middle part and also having a brown base.  \n\n\n3D Fuse Reference\n\nA 3D model of a green teapot with horns. Figure 16: Text-to-3D results. The top text prompt and \"Reference\" are from our test set. We fine-tune the left 5-column methods on Cap3D-generated captions. The detailed setting and methods are described in \u00a75.3.   \n\n\nDream Fusion Dream Field\n\nA 3D model of an axe featuring a dragon head, a sword, and a long, colorful handle. Figure 18: Text-to-3D results. The top text prompt and \"Reference\" are from our test set. We fine-tune the left 5-column methods on Cap3D-generated captions. The detailed setting and methods are described in \u00a75.3.  Figure 19: Text-to-3D results. The top text prompt and \"Reference\" are from our test set. We fine-tune the left 5-column methods on Cap3D-generated captions. The detailed setting and methods are described in \u00a75.3. \n\n\nReference\n\n\nReference\n\n\nDream Fusion Dream Field\n\nA 3D model of a mountain range with green grass and mountainous terrain. Figure 20: Text-to-3D results. The top text prompt and \"Reference\" are from our test set. We fine-tune the left 5-column methods on Cap3D-generated captions. The detailed setting and methods are described in \u00a75.3.  \n\n\nReference\n\n\nAppendix D Limitations and Failure Cases\n\nAs described in \u00a73, Cap3D consists of four steps: (1) 3D objects rendering; (2) captioning via BLIP2;\n\n(3) filtering captions via CLIP; (4) consolidate multiview information via GPT4. To effectively capture comprehensive information through 2D renderings, our cameras are positioned above or below the object. However, this sometimes leads to unusual 2D views, which cause the BLIP2 to produce inaccurate information that CLIP cannot filter. Consequently, GPT4 struggles to consolidate disparate information from multiple views, leading to ambiguous, verbose, and imprecise descriptions. One example is shown in Figure 22. Moreover, our system struggles to accurately process certain indoor 3D scans due to their inherent complexity (as shown in Figure 23), making them challenging to distinguish, sometimes even for humans.\n\nNote that, none of a caption from a single view can well describe the complete details from the given 3D object.  Figure 23: An failed case. The caption under each rendered image are generated by BLIP2 + filtered by CLIP. The inaccurate content are highlighted with colors. The various views contain inaccurate information. The associated details, roughly described, fail to accurately depict the indoor scene.\n\n\nOutput Caption\n\nIn \u00a75.2, we report human A/B judgments on ABO. We do not report automated metrics, which are poor measures of performance for at least two reasons. First, ABO contains a large number of objects that are very similar, meaning it would be challenging for captions to distinguish their differences. Thus, retrieval metrics such as ViLT Image or Text Retrieval will show very poor scores across metrics. Second, we show automated captioning performs poorly at describing geometry well, meaning it is likely automated image-caption alignment will not align based on geometry well. For completeness, we report automated metrics in Table 8. As expected, all retrieval scores are very low. Automated captioning scores best across automated metrics, however we caution against drawing conclusions from this result. Human studies in Table 4 suggest the opposite, and qualitative results agree with this finding, e.g. Figure 4. In contrast with A/B tests, which take place on the full 6.4k objects of ABO, this table is computed on a random 5k object subset of ABO to follow standard retrieval benchmarks (performance drops considerably as dataset size increases. Using 5k instead of the full 6.4k makes it much easier to contextualize retrieval numbers). A/B performance on this 5k subset is very close to the full 6.4k dataset, meaning the sample is highly representative, and one can compare the results from this table in combination with Table 4 in the main paper.\n\n\nAppendix F Additional Details\n\n\nF.1 Prompt used in Cap3D\n\nThe two prompts used for BLIP2 used in Cap3D (QA) are (1) \"Question: what object is in this image? Answer:\" and (2) \"Question: what is the structure and geometry of this <object>?\" where <object> is replaced with the response to prompt (1).\n\nFor the prompt used in GPT4, we used \"Given a set of descriptions about the same 3D object, distill these descriptions into one concise caption. The descriptions are as follows: 'captions'. Avoid describing background, surface, and posture. The caption should be:\". We did several prompt engineering and considered prompt with more context, like \"Below you will find a set of descriptions, each one is originating from various renderings of an identical 3D object. The level of accuracy in these descriptions ranges significantly: some might not correspond to the 3D object at all, others could be entirely accurate, while a few may only partially represent the object. Your task involves scrutinizing these descriptions and distilling them into a single, holistic depiction. The descriptions are as follows: 'captions'. Note: Please avoid using the phrases 'grey background', 'gray background', and 'gray surface' in your consolidated depiction. The synthesized description of the 3D object should be:\". However, with those longer prompt with more context, we noticed GPT4 sometimes would generate its reasoning process which led to confusing output captions. Also, for the sake of cost, we hope to make our prompt as short as possible.\n\n\nF.2 Rendering Details\n\nWe use Blender to render 3D objects in Objaverse [9] and ABO [35]. For each object, we first normalize them into a unit cube and recenter to origin. Then, we place 8 different cameras surrounding    \n\nFigure 1 :\n1Cap3D provides detailed descriptions of 3D objects by leveraging pretrained models in captioning, alignment, and LLM to consolidate multi-view information. Two views of 3D objects are shown here, Cap3D uses eight. Additional examples are available in Appendix B.\n\nFigure 2 :\n2Overview of Cap3D. Left to Right: (1) Render 3D objects from M = 8 camera angles to capture object details (2) Generate N = 5 image captions per rendered image using BLIP2; (3) Select one caption for each image based on its similarity to the image encoding using CLIP; (4) Use GPT4 to consolidate all selected captions into a final, summary of the object.\n\nFigure 3 :\n3Objaverse Caption Comparison. Human captions and Internet metadata frequently contain limited detail. Cap3D captions typically have longer length and more detail.\n\nFigure 4 :\n4ABO Automated Geometric Description. Left: Human descriptions provide more detailed geometry than automated captions. With careful prompting, Cap3D (QA) can match humanlevel detail. Right: The high peak of Metadata is cropped, which otherwise obscures other curves.\n\nFigure 5 :\n5Objaverse Caption Ablations. GPT produces longer and more detailed captions than BLIP2; CLIP tends to prune incorrect details and reduces length slightly.\n\nFigure 6 :\n6-to-3D) S.Diff(LoRA) + Point\u00b7E (Im-to-3D) S.Diff(CNet) + Point\u00b7E (Im-to-3D) Text-to-3D results. Finetuning on Cap3D captions can significantly improve results.\n\nFigure 7 :Figure 8 :\n78Random 3D captioning examples generated by Cap3D. Two views of 3D objects (Objaverse[9]) are shown here, Cap3D uses eight.UFC card featuring a shirtless man in red and white design. Random 3D captioning examples generated by Cap3D. Two views of 3D objects (Objaverse [9]) are shown here, Cap3D uses eight. A 3D model of a white robot. A 3D model of a green, armored lizard wearing a crown and holding a sword. A 3D model of a large white rock, possibly marble or granite, with a flag on it. A 3D model of a statue of a man with a hole in the ground. A 3D rendering of a row of vending machines and various colored boxes. A 3D model of a small flying robot-spaceship hybrid with extended arms, featuring an alien and a man on it. z 3D wooden bear statue model 3D printed kookaburra model sitting on a branch. A 3D yellow table with a cup of coffee on top.\n\nFigure 10 :\n10Random 3D captioning examples generated by Cap3D. Two views of 3D objects (Objaverse[9]) are shown here, Cap3D uses eight.\n\nFigure 12 :\n12Random 3D captioning examples generated by Cap3D. Two views of 3D objects (Objaverse [9]) are shown here, Cap3D uses eight. A 3D model of an armored hand wearing gloves with a leather strap and metal cuff. 3D model of a white cloud. White plastic ring 3D model of a small white building with stairs, featuring a cube and ceiling light fixture. 3D model of a wooden power pole with wires. 3D model of a white plastic bottle with a lid. z A 3D model of a small boat and house on a yellow platform.\n\n\nJogger pants made in the likeness of a pair of Joggers made by Champion. I started in Zbrush with the high res and exported to Maya for quad drawing the game res.. The model was also UV mapped and textured by myself. The texture was baked and finalized in Substance Painter. The poly count is just over 5200 tri's to fit the topology of the high res that was decimated from approximately 2,400,000 to around 415,000 quads.\n\nFigure 15 :\n15Comparative Analysis: Cap3D Generated Caption vs Human-Annotated Caption vs Objaverse Metadata[9]. Two views of 3D objects are shown here, Cap3D and human use eight.\n\n\nand black table lamp with a black shade.\n\nFigure 17 :\n17Text-to-3D results. The top text prompt and \"Reference\" are from our test set. We fine-tune the left 5-column methods on Cap3D-generated captions. The detailed setting and methods are described in \u00a75.3.\n\n\nof a Five Nights at Freddy's fox character with outstretched arms, wearing a hat and holding a gun.\n\n\nof witch hats and lanterns hanging from a chain.\n\nFigure 21 :\n21Text-to-3D results. The top text prompt and \"Reference\" are from our test set. We fine-tune the left 5-column methods on Cap3D-generated captions. The detailed setting and methods are described in \u00a75.3.\n\nFigure 25 :\n25ABO Caption Instructions.\n\nFigure 24 :\n24Objaverse Caption Instructions.\n\nFigure 26 :\n26A/B Instructions: Objaverse Captions.\n\nFigure 27 :\n27A/B Instructions: ABO Captions.\n\nTable 1 :\n1Cap3D is better, cheaper, and faster than crowdsourced annotation. Use 36k responses across 22k objects for A/B testing; 8A40s on a cloud platform for speed and cost computations.Method \nA/B Human Testing \nCost per \nAnnotation \nWin % (Tie %) \n1k Objects \nSpeed \n\nHuman \n37.8% \u00b1 0.5% (9.5%) \n$87.18 \n1.4k / day \nCap3D 52.3% \u00b1 0.5% (9.5%) \n$8.35 \n65k / day \n\n1k Objects Cost Breakdown \n\nBLIP2 \n$3.79 \nCLIP \n$0.38 \nGPT4 \n$4.18 \n\nCap3D Total Cost $8.35 \n\n\n\nTable 2 :\n2Ethical String match filtering is deterministic.Filtering Analysis. We \nmanually detect faces and NSFW content \nto validate automated filtering. 16 of 17 \nmissed face detections were sports cards. \n\nDetected Precision Missed dets. \n\n(Filtered) 5k (%) 10k 680k \n\nFaces \n18.6k \n790 16% 17 \u22481k \nNSFW \n217 \n102 47% 12 <1k \nLanguage  \u2020 226 \n-\n-\n-\n-\n \u2020: \n\nTable 3 :\n3Objaverse Captions Evaluations. Cap3D outperforms human and Metadata; BLIP2, GPT4, and CLIP are all important to performance. We report 95% confidence interval and use 5k objects.Method \nUser A/B Study vs. Cap3D \nCLIP ViLT Img Retr. ViLT Text Retr. \nScore (1-5) \nWin % \nLose % \nScore R@5 R@10 R@5 R@10 \n\nMetadata \n1.74\u00b10.026 10.7 \u00b1 0.7 83.8 \u00b1 0.8 66.8 \n4.3 \n6.3 \n6.1 \n8.5 \nHuman \n2.86\u00b10.026 \n37.0\u00b11.0 \n46.1\u00b11.0 \n72.5 \n21.2 \n29.0 \n18.5 \n24.9 \nCap3D \n-\n-\n-\n88.4 \n35.7 \n46.3 \n34.7 \n44.2 \n\nBLIP2 \n2.87\u00b1 0.019 41.0\u00b1 0.7 50.6\u00b1 0.7 \n83.1 \n24.7 \n32.3 \n21.9 \n29.3 \n+ GPT4 \n2.94\u00b1 0.015 35.2\u00b1 0.6 40.8\u00b1 0.6 \n86.3 \n31.9 \n39.9 \n30.2 \n38.4 \n+ CLIP (Cap3D) \n-\n-\n-\n86.9 \n31.1 \n40.2 \n30.3 \n38.6 \n\nA 3D model of a house \nwith a garage, roof, \ngrass and trees. \n\nA 3D model of a house with a \nroof, garage, grass, trees, and \na green field, featuring a knife \nand a pair of scissors. \n\nA 3D model \nof a house \non a gray \nbackground. \n\n\n\nTable 4 :\n4ABO Fine-Grained Geometry Captions. Cap3D (QA) performs best; crowdsourced beats captioning alone.Method \nA/B \nA/B \nA/B \nScore (1-5) \nWin % \nLose % \n\nHuman v. Cap3D \n3.09\u00b10.02 47.3\u00b11% 41.4\u00b11% \nCap3D(QA) v. Human 3.08\u00b10.02 50.2\u00b11% 44.0\u00b11% \nCap3D(QA) v. Cap3D 3.27\u00b10.02 56.0\u00b11% 37.4\u00b11% \nCap3D(QA) v. Meta \n4.27\u00b10.02 88.2\u00b11% 10.0\u00b11% \n\n\n\nTable 5 :\n5Text-to-3D: Human Captions. Cap3D captions are better than human on the 30k set. Finetuning on Cap3D full set performs best. for optimization-based baselines, which typically take >30 mins per object to optimize. Pretrained and Finetuned models are evaluated on 8 views across a held-out test set of 2k objects.Methods. We consider several recent SOTA methods in three general categories: text-to-3D diffusion, cascaded text-to-image then image-to-3D diffusion, and optimization-based. We use the direct text-to-3D variant of Point\u00b7E[88], as well as two variants of Shap\u00b7E[89]: STF[101] and NeRF[102]. We use Stable Diffusion cascaded with Point\u00b7E (Im-to-3D), adapting ControlNet[63] and LoRA[103] Finetune \nFID\u2193 \nCLIP CLIP R-Precision (2k) \nDataset \nScore R@1 R@5 R@10 \n\nPoint\u00b7E \nPretrained \n36.1 \n72.4 \n6.0 \n16.2 \n22.4 \n30k (Human) \n34.6 \n74.4 \n8.2 \n21.3 \n29.1 \n30k (Cap3D) \n33.7 \n75.0 \n10.4 \n24.3 \n32.1 \n350k (Cap3D) 32.8 \n75.6 \n12.4 \n28.1 \n36.9 \n\nShap\u00b7E \nPretrained \n37.2 \n80.4 \n20.3 \n39.7 \n48.7 \n30k (Human) \n36.0 \n79.6 \n18.6 \n36.3 \n45.3 \n30k (Cap3D) \n37.2 \n79.4 \n19.1 \n37.5 \n46.1 \n350k (Cap3D) 35.5 \n79.1 \n20.0 \n38.8 \n47.3 \n\n\n\nTable 6 :\n6Text-to-3D on Objaverse. Finetuning improves FID over pretrained performance across models. CLIP metrics of Stable Diffusion increase; CLIP metrics of Point\u00b7E increase significantly.Pretrained \nFinetuned on Cap3D \n\nFID\u2193 \nCLIP CLIP R-Precision (2k) FID\u2193 \nCLIP CLIP R-Precision (2k) \nScore R@1 R@5 R@10 \nScore R@1 R@5 R@10 \n\nGround Truth Images \n-\n81.6 \n32.7 \n55.1 \n64.3 \n-\n81.6 \n32.7 \n55.1 \n64.3 \n\nPoint\u00b7E (Text-to-3D) [88] \n36.1 \n72.4 \n6.0 \n16.2 \n22.4 \n32.8 \n75.6 \n12.4 \n28.1 \n36.9 \nS. Diff. [22] (CNet) [63]+ [88](Im-to-3D) \n54.7 \n73.6 \n11.0 \n23.4 \n30.0 \n53.3 \n74.6 \n12.4 \n26.2 \n33.8 \nS. Diff. [22] (LoRA) [103]+ [88](Im-to-3D) 54.7 \n73.6 \n11.0 \n23.4 \n30.0 \n53.7 \n74.4 \n11.6 \n24.6 \n31.4 \nShap\u00b7E [89] (STF) [101] \n37.2 \n80.4 \n20.3 \n39.7 \n48.7 \n35.5 \n79.1 \n20.0 \n38.8 \n47.3 \nShap\u00b7E [89] (NeRF) [102] \n48.7 \n79.4 \n19.0 \n37.7 \n46.8 \n48.2 \n78.1 \n18.3 \n35.1 \n43.5 \n\nA 3D white skateboard ramp model. \nA 3D model of a green teapot with horns. \n\n\n\nTable 7 :\n7Text-to-3D: Optimization Baselines. Overfitting via CLIP leads to higher CLIP-based scores than ground truth; ViLT score is more fair.FID\u2193 \nCLIP \nViLT \nScore R@1 R@5 R@1 R@5 \n\nTrue Images \n-\n83.2 \n53.2 \n77.8 \n41.3 \n69.0 \n\nD. Field [84] \n106.1 83.7 \n61.8 \n83.6 \n32.3 \n56.0 \nD. Fusion [3] 127.8 72.4 \n28.4 \n46.1 \n23.7 \n45.3 \n3DFuse [105] 93.4 \n75.8 \n38.8 \n59.5 \n24.7 \n51.0 \n\n\n\nTable 1 .\n1Using a single A40 GPU, BLIP2 runs at \u223c 2700 iterations per hour, enabling it to process around \u223c 337.5 objects hourly given the eight-run requirement for generating captions for 8 rendering views. This translates to about 2.96 hours to process 1k objects, costing 2.96 \u00d7 $1.28 = $3.79 with the rate $1.28/hr on the cloud platform, CoreWeave. On the same A40 GPU, CLIP operates at \u223c 27000 iterations per hour, incurring a cost of $0.38. Importantly, utilizing eight A40s costs the same as using one, due to the parallel processing capacity across multiple GPUs for multiple rendering views.\n\n\nA 3D model of a blue dragon on a rock formation with surrounding elements like a butterfly, girl, flowers, and water.Figure 22: An failed case. The caption under each rendered image are generated by BLIP2 + filtered by CLIP. The inaccurate content are highlighted with colors. GPT4 + CLIP cannot fix the error generated by BLIP2 and result in a fuzzy description.A 3D model featuring a house with a hole and window, a boat in a field, a mud house, a dump truck, a wooden boat, and a rusted car.a 3d model of a \ndragon on a rock \n\na 3d model of a \nbutterfly on a rock \n\na 3d model of a \nrock formation \nwith a dragon on it \n\na 3d model of a \nblue dragon on a \nrock \n\na 3d model of a rock \nwith a blue and \nyellow blob on it \n\na 3d model of a \ndragon with rocks \nand water \n\na 3d model of a rock \nwith a girl sitting on \nit \n\na 3d rendering of a \nrock with flowers \non it \n\nGPT4 \n\nPrompt: Given a set of \ndescriptions about the same \n3D object \u2026 distill these \ndescriptions into one concise \ncaption: \n\nOutput Caption \n\na 3d model of a \nhouse with a hole \nin it \n\na model of a dump \ntruck on a gray \nbackground \n\na 3d model of a \nhouse with a \nwindow \n\na 3d model of a \nboat in the \nmiddle of a field, \n\na 3d model of a mud \nhouse \n\na 3d model of a \nwooden boat \n\na 3d model of a \nrusted car on a gray \nbackground \n\na 3d model of a torn \npiece of wood \n\nGPT4 \n\nPrompt: Given a set of \ndescriptions about the same \n3D object \u2026 distill these \ndescriptions into one concise \ncaption: \n\n\n\nTable 8 :\n8ABO Automated Caption Evaluations. Automated captions are a poor measure of performance on ABO as (1) many objects are similar, making retrieval difficult; (2) automated captioning does not describe geometry well, so we should not expect automated image-caption alignment to describe geometrically correct captions well.Method \nCLIP ViLT Img Retr. ViLT Text Retr. \nScore R@5 R@10 R@5 R@10 \n\nMeta \n61.9 \n0.8 \n1.7 \n0.8 \n1.7 \nHuman \n75.2 \n2.6 \n4.4 \n2.3 \n4.2 \nCap3D \n89.9 \n4.2 \n7.2 \n3.2 \n5.6 \nCap3D(QA) 82.7 \n2.9 \n5.3 \n2.4 \n4.3 \n\n\nAcknowledgments and Disclosure of FundingThis work is supported by two grants from LG AI Research and Grant #1453651 from NSF. We greatly thank Kaiyi Li for his technical support. We thank Mohamed EI Banani, Karan Desai, and Ang Cao for their helpful discussions. Thanks Matt Deitke for helping with Objaverse-related questions.Appendix C Additional Text-to-3D ResultsIn this section, we provide several text-to-3D results for all of our compared methods. We include Shap\u00b7E and Point\u00b7E pretrained models and the models finetuned on our data, as well as optimization baselines, including DreamFusion, DreamField, and 3D Fuse.Appendix E ABO Captioning: Automated MetricsAppendix G Crowdsourced Captioning DetailsWe use Hive for crowdsourced captioning. Workers are given instructions for the task including gold-standard examples. Captioning instructions are shared below for Objaverse inFigure 24and ABO inFigure 25. Workers are persistently monitored. If a worker produces bad captions they are promptly banned from captioning, and their previous captions are discarded. Workers are paid approximately $50 per 1k tasks. We do not have access to their captioning rates; assuming a rate of 3 objects per minute, this would result in $9 per hour. Across Objaverse and ABO we spend a total of $7k on captioning.Appendix H Crowdsourced A/B Testing Details\nShap-e: Generating conditional 3d implicit functions. Heewoo Jun, Alex Nichol, arXiv:2305.02463arXiv preprintHeewoo Jun and Alex Nichol. Shap-e: Generating conditional 3d implicit functions. arXiv preprint arXiv:2305.02463, 2023.\n\nAnchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, Barlas Oguz, Triplane latent diffusion for textured mesh generation. arXiv. 3Anchit Gupta, Wenhan Xiong, Yixin Nie, Ian Jones, and Barlas Oguz. 3dgen: Triplane latent diffusion for textured mesh generation. arXiv, 2023.\n\nDreamfusion: Text-to-3d using 2d diffusion. Ben Poole, Ajay Jain, Jonathan T Barron, Ben Mildenhall, arXivBen Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv, 2022.\n\n3d modeling and computer graphics in virtual reality. Yuk Ming Tang, Ho Lun, Ho, Mixed Reality and Three-Dimensional Computer Graphics. IntechOpen. Yuk Ming Tang and Ho Lun Ho. 3d modeling and computer graphics in virtual reality. In Mixed Reality and Three-Dimensional Computer Graphics. IntechOpen, 2020.\n\nComputer animation: algorithms and techniques. Rick Parent, NewnesRick Parent. Computer animation: algorithms and techniques. Newnes, 2012.\n\nAfsoon Afzal, S Deborah, Claire Le Katz, Christopher S Goues, Timperley, arXiv:2004.07368A study on the challenges of using robotics simulators for testing. arXiv preprintAfsoon Afzal, Deborah S Katz, Claire Le Goues, and Christopher S Timperley. A study on the challenges of using robotics simulators for testing. arXiv preprint arXiv:2004.07368, 2020.\n\nBehavior-1k: A benchmark for embodied ai with 1,000 everyday activities and realistic simulation. Chengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Mart\u00edn-Mart\u00edn, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai Sun, Conference on Robot Learning. PMLRChengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Mart\u00edn-Mart\u00edn, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai Sun, et al. Behavior-1k: A benchmark for embodied ai with 1,000 everyday activities and realistic simulation. In Conference on Robot Learning, pages 80-93. PMLR, 2023.\n\nCarla: An open urban driving simulator. Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, Vladlen Koltun, Conference on robot learning. PMLRAlexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An open urban driving simulator. In Conference on robot learning, pages 1-16. PMLR, 2017.\n\nObjaverse: A universe of annotated 3d objects. Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli Vanderbilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, Ali Farhadi, Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated 3d objects. 2023.\n\nRedcaps: Web-curated image-text data created by the people, for the people. Karan Desai, Gaurav Kaul, Zubin Aysola, Justin Johnson, NeurIPS. Karan Desai, Gaurav Kaul, Zubin Aysola, and Justin Johnson. Redcaps: Web-curated image-text data created by the people, for the people. NeurIPS, 2021.\n\nConceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. Piyush Sharma, Nan Ding, Sebastian Goodman, Radu Soricut, ACL. Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL, 2018.\n\nConceptual 12m: Pushing webscale image-text pre-training to recognize long-tail visual concepts. Soravit Changpinyo, Piyush Sharma, Nan Ding, Radu Soricut, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionSoravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web- scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3558-3568, 2021.\n\nLaion-400m: Open dataset of clip-filtered 400 million image-text pairs. Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, Aran Komatsuzaki, arXivChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv, 2021.\n\nLaion-5b: An open large-scale dataset for training next generation image-text models. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. 2022.\n\nLearning Visual Representations via Language-Guided Sampling. Mohamed El Banani, Karan Desai, Justin Johnson, CVPR. Mohamed El Banani, Karan Desai, and Justin Johnson. Learning Visual Representations via Language- Guided Sampling. In CVPR, 2023.\n\nLearning transferable visual models from natural language supervision. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, ICML. 2021Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021.\n\nSlip: Self-supervision meets languageimage pre-training. Norman Mu, Alexander Kirillov, David Wagner, Saining Xie, ECCV. 2022Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. Slip: Self-supervision meets language- image pre-training. In ECCV, 2022.\n\nFlamingo: a visual language model for few-shot learning. Jeff Jean-Baptiste Alayrac, Pauline Donahue, Antoine Luc, Iain Miech, Yana Barr, Karel Hasson, Arthur Lenc, Katherine Mensch, Malcolm Millican, Reynolds, NeurIPSJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. NeurIPS, 2022.\n\nZero-shot text-to-image generation. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, Ilya Sutskever, ICML. 2021Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021.\n\nMake-a-scene: Scene-based text-to-image generation with human priors. Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, Yaniv Taigman, ECCV. 2022Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scene-based text-to-image generation with human priors. In ECCV, 2022.\n\nTim Salimans, et al. Photorealistic text-toimage diffusion models with deep language understanding. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to- image diffusion models with deep language understanding. 2022.\n\nHigh-resolution image synthesis with latent diffusion models. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Bj\u00f6rn Ommer, CVPR. 2022Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022.\n\nGlide: Towards photorealistic image generation and editing with text-guided diffusion models. Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, Mark Chen, CoRRAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. CoRR, 2021.\n\nHierarchical text-conditional image generation with clip latents. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, Mark Chen, arXivAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv, 2022.\n\nSimvlm: Simple visual language model pretraining with weak supervision. Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, Yuan Cao, 2022Zirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. ICLR, 2022.\n\nObject-semantics aligned pre-training for vision-language tasks. Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, ECCV. Xiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-semantics aligned pre-training for vision-language tasks. In ECCV, 2020.\n\nVinvl: Revisiting visual representations in vision-language models. Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, Jianfeng Gao, CVPR. 2021Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. In CVPR, 2021.\n\nShapecaptioner: Generative caption network for 3d shapes by learning a mapping from parts detected in multiple views to sentences. Zhizhong Han, Chao Chen, Yu-Shen Liu, Matthias Zwicker, ACM MM. Zhizhong Han, Chao Chen, Yu-Shen Liu, and Matthias Zwicker. Shapecaptioner: Generative caption network for 3d shapes by learning a mapping from parts detected in multiple views to sentences. In ACM MM, 2020.\n\nBlip-2: Bootstrapping language-image pretraining with frozen image encoders and large language models. Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi, arXivJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre- training with frozen image encoders and large language models. arXiv, 2023.\n\n. Openai, Gpt-4 technical reportOpenAI. Gpt-4 technical report, 2023.\n\nMicrosoft coco: Common objects in context. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, C Lawrence Zitnick, ECCV. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.\n\nVisual genome: Connecting language and vision using crowdsourced dense image annotations. Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV, 2017.\n\nIm2text: Describing images using 1 million captioned photographs. NeurIPS. Vicente Ordonez, Girish Kulkarni, Tamara Berg, Vicente Ordonez, Girish Kulkarni, and Tamara Berg. Im2text: Describing images using 1 million captioned photographs. NeurIPS, 2011.\n\nAbo: Dataset and benchmarks for real-world 3d object understanding. Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas F Yago Vicente, Thomas Dideriksen, Himanshu Arora, Matthieu Guillaumin, Jitendra Malik, CVPRJasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas F Yago Vicente, Thomas Dideriksen, Himanshu Arora, Matthieu Guillaumin, and Jitendra Malik. Abo: Dataset and benchmarks for real-world 3d object understanding. CVPR, 2022.\n\nBottom-up and top-down attention for image captioning and visual question answering. Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, Lei Zhang, CVPR. Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In CVPR, 2018.\n\nSelf-critical sequence training for image captioning. J Steven, Etienne Rennie, Youssef Marcheret, Jerret Mroueh, Vaibhava Ross, Goel, CVPR. Steven J Rennie, Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. Self-critical sequence training for image captioning. In CVPR, 2017.\n\nNeural baby talk. Jiasen Lu, Jianwei Yang, Dhruv Batra, Devi Parikh, CVPR. Jiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Neural baby talk. In CVPR, 2018.\n\nMattnet: Modular attention network for referring expression comprehension. Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, Tamara L Berg, CVPR. Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L Berg. Mattnet: Modular attention network for referring expression comprehension. In CVPR, 2018.\n\nGang Hua, Houdong Hu, and Xiaodong He. Stacked cross attention for image-text matching. Kuang-Huei Lee, Xi Chen, In ECCV. Kuang-Huei Lee, Xi Chen, Gang Hua, Houdong Hu, and Xiaodong He. Stacked cross attention for image-text matching. In ECCV, 2018.\n\nDeep residual learning for image recognition. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.\n\nImagenet classification with deep convolutional neural networks. Alex Krizhevsky, Ilya Sutskever, Geoffrey E Hinton, Advances in Neural Information Processing Systems. Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems, 2012.\n\nU-net: Convolutional networks for biomedical image segmentation. Olaf Ronneberger, Philipp Fischer, Thomas Brox, MICCAI 2015. Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI 2015, 2015.\n\nLong short-term memory. Sepp Hochreiter, J\u00fcrgen Schmidhuber, Neural computation. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 1997.\n\nMike Schuster, K Kuldip, Paliwal, Bidirectional recurrent neural networks. transactions on Signal Processing. Mike Schuster and Kuldip K Paliwal. Bidirectional recurrent neural networks. transactions on Signal Processing, 1997.\n\nNocaps: Novel object captioning at scale. Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, Peter Anderson, ICCV. Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, Stefan Lee, and Peter Anderson. Nocaps: Novel object captioning at scale. In ICCV, 2019.\n\nGenerative adversarial nets. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio, Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. 2014.\n\nAnalyzing and improving the image quality of stylegan. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila, CVPR. Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In CVPR, 2020.\n\nNeural discrete representation learning. Aaron Van Den, Oriol Oord, Vinyals, NeurIPS. Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. NeurIPS, 2017.\n\nTaming transformers for high-resolution image synthesis. Patrick Esser, Robin Rombach, Bjorn Ommer, CVPR. 2021Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, 2021.\n\nMastering text-to-image generation via transformers. Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, NeurIPSMing Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. NeurIPS, 2021.\n\nVirtex: Learning visual representations from textual annotations. Karan Desai, Justin Johnson, CVPR. 2021Karan Desai and Justin Johnson. Virtex: Learning visual representations from textual annotations. In CVPR, 2021.\n\nAn image is worth 16x16 words: Transformers for image recognition at scale. ICLR. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021.\n\nDiffusion models beat gans on image synthesis. Prafulla Dhariwal, Alexander Nichol, NeurIPS. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. NeurIPS, 2021.\n\nDenoising diffusion probabilistic models. Jonathan Ho, Ajay Jain, Pieter Abbeel, NeurIPS. 33Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33, 2020.\n\nElucidating the design space of diffusion-based generative models. Tero Karras, Miika Aittala, Timo Aila, Samuli Laine, NeurIPS. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. NeurIPS, 2022.\n\nImproved denoising diffusion probabilistic models. Alexander Quinn, Nichol , Prafulla Dhariwal, ICML. 2021Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In ICML, 2021.\n\nGenerative modeling by estimating gradients of the data distribution. Yang Song, Stefano Ermon, NeurIPS. Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. NeurIPS, 2019.\n\nDeep unsupervised learning using nonequilibrium thermodynamics. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, Surya Ganguli, ICML. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML, 2015.\n\n. Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Wan-Yen LoTete Xiao, Spencer Whitehead, Alexander C Berget al. Segment anything. arXivAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv, 2023.\n\nClipscore: A reference-free evaluation metric for image captioning. Jack Hessel, Ari Holtzman, Maxwell Forbes, Yejin Ronan Le Bras, Choi, arXivJack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. arXiv, 2021.\n\n. Openai, arXivOpenAI. Gpt-4 technical report. arXiv, 2023.\n\nAdding conditional control to text-to-image diffusion models. Lvmin Zhang, Maneesh Agrawala, arXivLvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. arXiv, 2023.\n\nN M Justin, Pinkney, Pokemon blip captions. Justin N. M. Pinkney. Pokemon blip captions. https://huggingface.co/datasets/lambdalabs/ pokemon-blip-captions/, 2022.\n\nOpenshape: Scaling up 3d shape representation towards open-world understanding. Minghua Liu, Ruoxi Shi, Kaiming Kuang, Yinhao Zhu, Xuanlin Li, Shizhong Han, Hong Cai, Fatih Porikli, Hao Su, arXivMinghua Liu, Ruoxi Shi, Kaiming Kuang, Yinhao Zhu, Xuanlin Li, Shizhong Han, Hong Cai, Fatih Porikli, and Hao Su. Openshape: Scaling up 3d shape representation towards open-world understanding. arXiv, 2023.\n\nBlip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi, ICML. 2022Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML, 2022.\n\nUlip: Learning unified representation of language, image and point cloud for 3d understanding. Le Xue, Mingfei Gao, Chen Xing, Roberto Mart\u00edn-Mart\u00edn, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, Silvio Savarese, CVPRLe Xue, Mingfei Gao, Chen Xing, Roberto Mart\u00edn-Mart\u00edn, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Car- los Niebles, and Silvio Savarese. Ulip: Learning unified representation of language, image and point cloud for 3d understanding. CVPR, 2023.\n\nX Angel, Thomas Chang, Leonidas Funkhouser, Pat Guibas, Qixing Hanrahan, Zimo Huang, Silvio Li, Manolis Savarese, Shuran Savva, Hao Song, Su, An information-rich 3d model repository. arXivAngel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv, 2015.\n\nPix3d: Dataset and methods for single-image 3d shape modeling. Xingyuan Sun, Jiajun Wu, Xiuming Zhang, Zhoutong Zhang, Chengkai Zhang, Tianfan Xue, Joshua B Tenenbaum, William T Freeman, CVPR. Xingyuan Sun, Jiajun Wu, Xiuming Zhang, Zhoutong Zhang, Chengkai Zhang, Tianfan Xue, Joshua B Tenenbaum, and William T Freeman. Pix3d: Dataset and methods for single-image 3d shape modeling. In CVPR, 2018.\n\nParsing ikea objects: Fine pose estimation. J Joseph, Hamed Lim, Antonio Pirsiavash, Torralba, ICCV. Joseph J Lim, Hamed Pirsiavash, and Antonio Torralba. Parsing ikea objects: Fine pose estimation. In ICCV, 2013.\n\n. Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang Zhao, Steve Maybank, and Dacheng Tao3d-future: 3d furniture shape with texture. IJCV, 2021Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang Zhao, Steve Maybank, and Dacheng Tao. 3d-future: 3d furniture shape with texture. IJCV, 2021.\n\nShape-Glot: Learning language for shape differentiation. Panos Achlioptas, Judy Fan, X D Hawkins, D Noah Goodman, J Leonidas Guibas, CoRRPanos Achlioptas, Judy Fan, X.D. Robert Hawkins, D. Noah Goodman, and J. Leonidas Guibas. Shape- Glot: Learning language for shape differentiation. CoRR, 2019.\n\nText2shape: Generating shapes from natural language by learning joint embeddings. Kevin Chen, B Christopher, Manolis Choy, Savva, X Angel, Thomas Chang, Silvio Funkhouser, Savarese, ACCV. Kevin Chen, Christopher B Choy, Manolis Savva, Angel X Chang, Thomas Funkhouser, and Silvio Savarese. Text2shape: Generating shapes from natural language by learning joint embeddings. In ACCV, 2019.\n\nShapecrafter: A recursive text-conditioned 3d shape generation model. Rao Fu, Xiao Zhan, Yiwen Chen, Daniel Ritchie, Srinath Sridhar, Advances in Neural Information Processing Systems. Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun ChoRao Fu, Xiao Zhan, Yiwen Chen, Daniel Ritchie, and Srinath Sridhar. Shapecrafter: A recursive text-conditioned 3d shape generation model. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https: //openreview.net/forum?id=KUOKpojFr_.\n\nScannet: Richly-annotated 3d reconstructions of indoor scenes. Angela Dai, X Angel, Manolis Chang, Maciej Savva, Thomas Halber, Matthias Funkhouser, Nie\u00dfner, CVPR. Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie\u00dfner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In CVPR, 2017.\n\nScanrefer: 3d object localization in rgb-d scans using natural language. Dave Zhenyu Chen, X Angel, Matthias Chang, Nie\u00dfner, ECCV. Dave Zhenyu Chen, Angel X Chang, and Matthias Nie\u00dfner. Scanrefer: 3d object localization in rgb-d scans using natural language. In ECCV, 2020.\n\nNeural shape compiler: A unified framework for transforming between text, point cloud, and program. Tiange Luo, Honglak Lee, Justin Johnson, 2835-8856Transactions on Machine Learning Research. Tiange Luo, Honglak Lee, and Justin Johnson. Neural shape compiler: A unified framework for transforming between text, point cloud, and program. Transactions on Machine Learning Research, 2023. ISSN 2835-8856. URL https://openreview.net/forum?id=gR9UVgH8PZ.\n\nShow and tell: A neural image caption generator. Oriol Vinyals, Alexander Toshev, Samy Bengio, Dumitru Erhan, ICML. Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: A neural image caption generator. In ICML, 2015.\n\nScan2cap: Context-aware dense captioning in rgb-d scans. Zhenyu Chen, Ali Gholami, Matthias Nie\u00dfner, Angel X Chang, CVPR. 2021Zhenyu Chen, Ali Gholami, Matthias Nie\u00dfner, and Angel X. Chang. Scan2cap: Context-aware dense captioning in rgb-d scans. In CVPR, 2021.\n\nClip-forge: Towards zero-shot text-to-shape generation. Aditya Sanghi, Hang Chu, Ye Joseph G Lambourne, Chin-Yi Wang, Marco Cheng, Kamal Rahimi Fumero, Malekshan, CVPR. 2022Aditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang, Chin-Yi Cheng, Marco Fumero, and Ka- mal Rahimi Malekshan. Clip-forge: Towards zero-shot text-to-shape generation. In CVPR, 2022.\n\nAutosdf: Shape priors for 3d completion, reconstruction and generation. Paritosh Mittal, Yen-Chi Cheng, Maneesh Singh, Shubham Tulsiani, CVPR. 2022Paritosh Mittal, Yen-Chi Cheng, Maneesh Singh, and Shubham Tulsiani. Autosdf: Shape priors for 3d completion, reconstruction and generation. In CVPR, 2022.\n\nText-guided 3d textured shape generation from pseudo supervision. Jiacheng Wei, Hao Wang, Jiashi Feng, Guosheng Lin, Kim-Hui Yap, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern Recognition3Jiacheng Wei, Hao Wang, Jiashi Feng, Guosheng Lin, and Kim-Hui Yap. Taps3d: Text-guided 3d textured shape generation from pseudo supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16805-16815, 2023.\n\nBiao Zhang, Jiapeng Tang, Matthias Niessner, Peter Wonka, arXiv:2301.11445A 3d shape representation for neural fields and generative diffusion models. 3arXiv preprintBiao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: A 3d shape representa- tion for neural fields and generative diffusion models. arXiv preprint arXiv:2301.11445, 2023.\n\nZero-shot text-guided object generation with dream fields. Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, Ben Poole, CVPR. 2022Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter Abbeel, and Ben Poole. Zero-shot text-guided object generation with dream fields. In CVPR, 2022.\n\nTextmesh: Generation of realistic 3d meshes from text prompts. Christina Tsalicoglou, Fabian Manhardt, Alessio Tonioni, Michael Niemeyer, Federico Tombari, arXivChristina Tsalicoglou, Fabian Manhardt, Alessio Tonioni, Michael Niemeyer, and Federico Tombari. Textmesh: Generation of realistic 3d meshes from text prompts. arXiv, 2023.\n\nMagic3d: High-resolution text-to-3d content creation. Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, Tsung-Yi Lin, Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. the IEEE/CVF Conference on Computer Vision and Pattern RecognitionChen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 300-309, 2023.\n\nUlip-2: Towards scalable multimodal pre-training for 3d understanding. Le Xue, Ning Yu, Shu Zhang, Junnan Li, Roberto Mart\u00edn-Mart\u00edn, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, Silvio Savarese, arXivLe Xue, Ning Yu, Shu Zhang, Junnan Li, Roberto Mart\u00edn-Mart\u00edn, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, and Silvio Savarese. Ulip-2: Towards scalable multimodal pre-training for 3d understanding. arXiv, 2023.\n\nPoint-e: A system for generating 3d point clouds from complex prompts. Alex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, Mark Chen, arXivAlex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: A system for generating 3d point clouds from complex prompts. arXiv, 2022.\n\nAlex Nichol, Heewoo Jun, Shap-e: Generating conditional 3d implicit functions. arXiv. Alex Nichol and Heewoo Jun. Shap-e: Generating conditional 3d implicit functions. arXiv, 2023.\n\n3d shape generation and completion through point-voxel diffusion. Linqi Zhou, Yilun Du, Jiajun Wu, Proceedings of the IEEE/CVF International Conference on Computer Vision. the IEEE/CVF International Conference on Computer VisionLinqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation and completion through point-voxel diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5826-5835, 2021.\n\nChao-Yuan, Justin Wu, Jitendra Johnson, Christoph Malik, Georgia Feichtenhofer, Gkioxari, Multiview compressive coding for 3d reconstruction. arXiv. Chao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph Feichtenhofer, and Georgia Gkioxari. Multiview compressive coding for 3d reconstruction. arXiv, 2023.\n\nZero-1-to-3: Zero-shot one image to 3d object. Ruoshi Liu, Rundi Wu, Pavel Basile Van Hoorick, Sergey Tokmakov, Carl Zakharov, Vondrick, arXivRuoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. arXiv, 2023.\n\nExploring the limits of masked visual representation learning at scale. Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, Yue Cao, Eva, CVPRYuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. CVPR, 2023.\n\nHyung Won, Le Chung, Shayne Hou, Barret Longpre, Yi Zoph, William Tay, Eric Fedus, Xuezhi Li, Mostafa Wang, Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXivHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models. arXiv, 2022.\n\nThe curious case of neural text degeneration. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, Yejin Choi, ICLR. Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text degeneration. In ICLR, 2020.\n\nRetinaface: Singleshot multi-level face localisation in the wild. Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia, Stefanos Zafeiriou, CVPR. Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia, and Stefanos Zafeiriou. Retinaface: Single- shot multi-level face localisation in the wild. In CVPR, 2020.\n\nRethinking the inception architecture for computer vision. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, Zbigniew Wojna, CVPR. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In CVPR, 2016.\n\nDeep nn for nsfw detection. Gant Laborde, Gant Laborde. Deep nn for nsfw detection. https://github.com/GantMan/nsfw_model. [Online; accessed 7-May-2023].\n\nVilt: Vision-and-language transformer without convolution or region supervision. Wonjae Kim, Bokyung Son, Ildoo Kim, ICML. 2021Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In ICML, 2021.\n\nOr Litany, Zan Gojcic, and Sanja Fidler. Get3d: A generative model of high quality 3d textured shapes learned from images. Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, NeurIPSJun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja Fidler. Get3d: A generative model of high quality 3d textured shapes learned from images. NeurIPS, 2022.\n\nNerf: Representing scenes as neural radiance fields for view synthesis. Ben Mildenhall, P Pratul, Matthew Srinivasan, Jonathan T Tancik, Ravi Barron, Ren Ramamoorthi, Ng, ECCV. Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020.\n\nLora: Low-rank adaptation of large language models. Edward Hu, Yelong Shen, Phil Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Lu Wang, Weizhu Chen, Edward Hu, Yelong Shen, Phil Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models, 2021.\n\nStable-dreamfusion: Text-to-3d with stable-diffusion. Jiaxiang Tang, Jiaxiang Tang. Stable-dreamfusion: Text-to-3d with stable-diffusion, 2022. https://github.com/ashawkey/stable-dreamfusion.\n\nLet 2d diffusion model know 3d-consistency for robust text-to-3d generation. Junyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon Ko, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee, Seungryong Kim, arXivJunyoung Seo, Wooseok Jang, Min-Seop Kwak, Jaehoon Ko, Hyeonsu Kim, Junho Kim, Jin-Hwa Kim, Jiyoung Lee, and Seungryong Kim. Let 2d diffusion model know 3d-consistency for robust text-to-3d generation. arXiv, 2023.\n\nKarlo-v1.0.alpha on coyo-100m and cc15m. Donghoon Lee, Jiseob Kim, Jisu Choi, Jongmin Kim, Minwoo Byeon, Woonhyuk Baek, Saehoon Kim, Donghoon Lee, Jiseob Kim, Jisu Choi, Jongmin Kim, Minwoo Byeon, Woonhyuk Baek, and Saehoon Kim. Karlo-v1.0.alpha on coyo-100m and cc15m. https://github.com/kakaobrain/karlo, 2022.\n\nGans trained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS, 2017. the object with 2 cameras slightly below the object to capture the bottom of the object. Three area lights are placed and function as key light, fill light, and rim light, respectively. The detailed parameters are listed in our rendering script. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, Sepp Hochreiter, provided in our GithubMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS, 2017. the object with 2 cameras slightly below the object to capture the bottom of the object. Three area lights are placed and function as key light, fill light, and rim light, respectively. The detailed parameters are listed in our rendering script, provided in our Github.\n\nThis resulting a final subset of 6.4k objects for rendering and captioning. decipherable objects. These objects are likely to be the most sensible for captioning and A/B testing. For instance. According to \u00a7 3.2, we filter objects in Objaverse based on commerical-license, rendering information, and ethicial standards. and results a subset of 660k objects for rendering and captioning. some Objaverse objects are essentially a simple rock with little textureAccording to \u00a7 3.2, we filter objects in Objaverse based on commerical-license, rendering information, and ethicial standards, and results a subset of 660k objects for rendering and captioning. In ABO, we exclude categories with simple geometry to concentrate on geometrical captioning, including \"BLANKET\", \"RUG\", \"WALL_ART\", \"PLACEMAT\", \"CURTAIN\", \"MOUSE_PAD\". This resulting a final subset of 6.4k objects for rendering and captioning. decipherable objects. These objects are likely to be the most sensible for captioning and A/B testing. For instance, some Objaverse objects are essentially a simple rock with little texture;\n\nThese excluded objects are generally not effective samples to use for human A/B testing, as the correct caption may not be clear or may be trivial. We also exclude furniture, which is suitable for captioning, but we measure this with more focus on ABO. others it can be difficult for a human to describe an object. abstract art, no clear object visible, or 3D scans with hard-to-distinguish details). Human captions on ABO follow the split of [77in others it can be difficult for a human to describe an object (e.g. abstract art, no clear object visible, or 3D scans with hard-to-distinguish details). These excluded objects are generally not effective samples to use for human A/B testing, as the correct caption may not be clear or may be trivial. We also exclude furniture, which is suitable for captioning, but we measure this with more focus on ABO. Human captions on ABO follow the split of [77].\n\nWe use Hive for crowdsourced A/B testing. Specifically, workers are given an image and two captions, and select which is better on a scale from 1 to 5, where 3 is a tie. So 1 would be \"left much better\", and 2 would be \"left better\". Workers are given instructions for the task along with gold standard examples. Workers are informed to prioritize accuracy, then informative detail, then brevity. Left/right order between methods was randomized for each instance. A/B Testing instructions are shared below for. Objaverse in Figure 27 and ABO in Figure 26We use Hive for crowdsourced A/B testing. Specifically, workers are given an image and two captions, and select which is better on a scale from 1 to 5, where 3 is a tie. So 1 would be \"left much better\", and 2 would be \"left better\". Workers are given instructions for the task along with gold standard examples. Workers are informed to prioritize accuracy, then informative detail, then brevity. Left/right order between methods was randomized for each instance. A/B Testing instructions are shared below for Objaverse in Figure 27 and ABO in Figure 26.\n\nHowever, we found some workers would successfully pass the handful of gold-standard examples while scamming on the rest of the examples. The most common scam cases were always picking the same number, or always picking the shorter or longer caption. We thus manually search through all workers and ban workers who meet these scamming criteria and discard their judgments. Unfortunately, discarding judgments leads to uneven numbers of observations for each individual experiment. Nevertheless, in all cases. Workers are automatically banned by the platform if they miss too many gold-standard examples. enough observations are available to draw conclusive findings. The size of each experiment's data after discarded judgments is belowWorkers are automatically banned by the platform if they miss too many gold-standard examples. However, we found some workers would successfully pass the handful of gold-standard examples while scamming on the rest of the examples. The most common scam cases were always picking the same number, or always picking the shorter or longer caption. We thus manually search through all workers and ban workers who meet these scamming criteria and discard their judgments. Unfortu- nately, discarding judgments leads to uneven numbers of observations for each individual experiment. Nevertheless, in all cases, enough observations are available to draw conclusive findings. The size of each experiment's data after discarded judgments is below.\n\n) takes place on a random set upon which human captions are available. Cap3D vs. Human has 36k observations across 22k objects. \u2022 Objaverse, Split, \u2022 Objaverse Split (1) takes place on a random set upon which human captions are available. Cap3D vs. Human has 36k observations across 22k objects.\n\n) takes place on a random object set upon which human captions are available. Cap3D vs. Human has 10k observations across 4.7k objects. Cap3D vs. \u2022 Objaverse, Split, Metadata has 7k observations across 4.7k objects (less than the target 10k), though given the extremely poor rating of Metadata, results are conclusive\u2022 Objaverse Split (2) takes place on a random object set upon which human captions are available. Cap3D vs. Human has 10k observations across 4.7k objects. Cap3D vs. Metadata has 7k observations across 4.7k objects (less than the target 10k), though given the extremely poor rating of Metadata, results are conclusive.\n\n) takes place on a random object set upon the entire Objaverse dataset. Cap3D vs. BLIP2 has 20k observations across 5.0k objects and Cap3D vs. \u2022 Objaverse, Split, +GPT4 has 29k observations across 5.0k objects\u2022 Objaverse Split (3) takes place on a random object set upon the entire Objaverse dataset. Cap3D vs. BLIP2 has 20k observations across 5.0k objects and Cap3D vs. +GPT4 has 29k observations across 5.0k objects.\n\nCap3D has 21k observations across 6.4k objects, Cap3D (QA) vs. Human has 17k observations across 6.4k objects, Cap3D (QA) vs. Cap3D has 13k observations across 6.4k objects, and Cap3D (QA) vs. \u2022 ABO takes place on the full ABO object set. Human vs. Meta has 12k observations across 6.4k objects\u2022 ABO takes place on the full ABO object set. Human vs. Cap3D has 21k observations across 6.4k objects, Cap3D (QA) vs. Human has 17k observations across 6.4k objects, Cap3D (QA) vs. Cap3D has 13k observations across 6.4k objects, and Cap3D (QA) vs. Meta has 12k observations across 6.4k objects.\n\nAppendix I Additional Experimental Details Captioning: we perform one full-scale evaluation run for all captioning experiments. 95% confidence interval for mean is presented. Metrics are overviewed in \u00a75.1Appendix I Additional Experimental Details Captioning: we perform one full-scale evaluation run for all captioning experiments; 95% confidence interval for mean is presented. Metrics are overviewed in \u00a75.1;\n\nA/B testing is detailed further in \u00a7H. CLIP Score takes about 5 minutes, while ViLT R-Precision takes about 8 hours using an A40 for test set of 5k object-caption pairs. Crowdsourced A/B testing takes about 12 hours for 10k responses across 5k objects. A/B testing is detailed further in \u00a7H. CLIP Score takes about 5 minutes, while ViLT R-Precision takes about 8 hours using an A40 for test set of 5k object-caption pairs. Crowdsourced A/B testing takes about 12 hours for 10k responses across 5k objects.\n\nTraining took about 3 days on the full set and 1 day on the small (human) set. We used AdamW optimizer and CosineAnnealingLR scheduler with initial learning rate 1e \u2212 5 for finetuning both Point\u00b7E and Shap\u00b7E. We adopted batch size 64 and 256 for Shap\u00b7E and Point\u00b7E, respectively. However, for Shap\u00b7E, we found it usually outputs NaN and needed to re-start from saved checkpoints, which could be one of the reaons why our finetune did not bring improvements. For LoRA, we use AdamW optimizer and CosineAnnealingLR scheduler with initial learning rate 1e \u2212 4 and batch size of 3. For ControlNet, we use AdamW optimizer and constant learning rate of 1e \u2212 5 and batch size of 8. Experiments use 4 A40s to train except LoRA, which fails upon multi-gpu training due to a HuggingFace internal DDP error. Notably single-gpu training still yields improvement. Evaluation takes the following time (in seconds) per iteration. 3D, finetuning: for finetuning experiments, we used one train and evaluation run using a learning rate validated on a small overfitting experiment on the train set. which includes rendering: \u2022 PointE (text-to-3D): 37sec = 28sec (text-to-3D) + 9sec (renderText-to-3D, finetuning: for finetuning experiments, we used one train and evaluation run using a learning rate validated on a small overfitting experiment on the train set. Training took about 3 days on the full set and 1 day on the small (human) set. We used AdamW optimizer and CosineAnnealingLR scheduler with initial learning rate 1e \u2212 5 for finetuning both Point\u00b7E and Shap\u00b7E. We adopted batch size 64 and 256 for Shap\u00b7E and Point\u00b7E, respectively. However, for Shap\u00b7E, we found it usually outputs NaN and needed to re-start from saved checkpoints, which could be one of the reaons why our finetune did not bring improvements. For LoRA, we use AdamW optimizer and CosineAnnealingLR scheduler with initial learning rate 1e \u2212 4 and batch size of 3. For ControlNet, we use AdamW optimizer and constant learning rate of 1e \u2212 5 and batch size of 8. Experiments use 4 A40s to train except LoRA, which fails upon multi-gpu training due to a HuggingFace internal DDP error. Notably single-gpu training still yields improvement. Evaluation takes the following time (in seconds) per iteration, which includes rendering: \u2022 PointE (text-to-3D): 37sec = 28sec (text-to-3D) + 9sec (render)\n\n\u2022 Lora, + Pointe, 114sec = 5sec + 100sec (im-to-3D) + 9sec (render) \u2022 ControlNet + PointE(im-to-3D): 124sec = 15sec + 100sec (im-to-3D) + 9sec (render) \u2022 ShapE (NeRF): 193sec (text-to-3D + render) \u2022 ShapE (stf): 16sec. text-to-3D + render\u2022 LoRA + PointE(im-to-3D): 114sec = 5sec + 100sec (im-to-3D) + 9sec (render) \u2022 ControlNet + PointE(im-to-3D): 124sec = 15sec + 100sec (im-to-3D) + 9sec (render) \u2022 ShapE (NeRF): 193sec (text-to-3D + render) \u2022 ShapE (stf): 16sec (text-to-3D + render)\n\nNote publicly available PointE (im-to-3D) is 1B param, making it slower than the largest publicly available PointE (text-to-3D) of 40M. Evaluation metrics are detailed in \u00a75.3Note publicly available PointE (im-to-3D) is 1B param, making it slower than the largest publicly available PointE (text-to-3D) of 40M. Evaluation metrics are detailed in \u00a75.3.\n\nText-to-3D, optimization: For one object, optimization plus final rendering takes 40 minutes for 3DFuse, 95 minutes for Stable DreamFusion, and 35 minutes for DreamField; using 1 A40 GPU. We use default parameters for all methods and run them once. Text-to-3D, optimization: For one object, optimization plus final rendering takes 40 minutes for 3DFuse, 95 minutes for Stable DreamFusion, and 35 minutes for DreamField; using 1 A40 GPU. We use default parameters for all methods and run them once.\n", "annotations": {"author": "[{\"end\":96,\"start\":60},{\"end\":137,\"start\":97},{\"end\":192,\"start\":138},{\"end\":233,\"start\":193}]", "publisher": null, "author_last_name": "[{\"end\":70,\"start\":67},{\"end\":111,\"start\":103},{\"end\":149,\"start\":146},{\"end\":207,\"start\":200}]", "author_first_name": "[{\"end\":66,\"start\":60},{\"end\":102,\"start\":97},{\"end\":145,\"start\":138},{\"end\":199,\"start\":193}]", "author_affiliation": "[{\"end\":95,\"start\":72},{\"end\":136,\"start\":113},{\"end\":174,\"start\":151},{\"end\":191,\"start\":176},{\"end\":232,\"start\":209}]", "title": "[{\"end\":46,\"start\":1},{\"end\":279,\"start\":234}]", "venue": null, "abstract": "[{\"end\":1633,\"start\":646}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b0\"},\"end\":1958,\"start\":1955},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":1961,\"start\":1958},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":1964,\"start\":1961},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":2087,\"start\":2084},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":2097,\"start\":2094},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":2111,\"start\":2108},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":2113,\"start\":2111},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":2141,\"start\":2138},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":2284,\"start\":2281},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":2577,\"start\":2573},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":2581,\"start\":2577},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":2585,\"start\":2581},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":2589,\"start\":2585},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":2593,\"start\":2589},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":2670,\"start\":2666},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":2674,\"start\":2670},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":2678,\"start\":2674},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":2682,\"start\":2678},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":2721,\"start\":2717},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":2725,\"start\":2721},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":2729,\"start\":2725},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":2733,\"start\":2729},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":2737,\"start\":2733},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2741,\"start\":2737},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":2768,\"start\":2764},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":2772,\"start\":2768},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":2776,\"start\":2772},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":2780,\"start\":2776},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":2784,\"start\":2780},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":3086,\"start\":3082},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":3162,\"start\":3158},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":3221,\"start\":3217},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":3355,\"start\":3351},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":3359,\"start\":3355},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3363,\"start\":3359},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":3367,\"start\":3363},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3371,\"start\":3367},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":3375,\"start\":3371},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4828,\"start\":4824},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":5926,\"start\":5922},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":5930,\"start\":5926},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":5934,\"start\":5930},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":5986,\"start\":5982},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":5989,\"start\":5986},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":6016,\"start\":6012},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":6020,\"start\":6016},{\"attributes\":{\"ref_id\":\"b41\"},\"end\":6024,\"start\":6020},{\"attributes\":{\"ref_id\":\"b42\"},\"end\":6039,\"start\":6035},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":6042,\"start\":6039},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":6083,\"start\":6079},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":6087,\"start\":6083},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":6091,\"start\":6087},{\"attributes\":{\"ref_id\":\"b44\"},\"end\":6094,\"start\":6091},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":6164,\"start\":6160},{\"attributes\":{\"ref_id\":\"b46\"},\"end\":6167,\"start\":6164},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":6183,\"start\":6179},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":6187,\"start\":6183},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":6191,\"start\":6187},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":6195,\"start\":6191},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":6295,\"start\":6291},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":6299,\"start\":6295},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":6303,\"start\":6299},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6307,\"start\":6303},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6311,\"start\":6307},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":6327,\"start\":6323},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":6331,\"start\":6327},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":6335,\"start\":6331},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":6339,\"start\":6335},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6377,\"start\":6373},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":6380,\"start\":6377},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":6383,\"start\":6380},{\"attributes\":{\"ref_id\":\"b52\"},\"end\":6409,\"start\":6405},{\"attributes\":{\"ref_id\":\"b53\"},\"end\":6413,\"start\":6409},{\"attributes\":{\"ref_id\":\"b54\"},\"end\":6417,\"start\":6413},{\"attributes\":{\"ref_id\":\"b55\"},\"end\":6421,\"start\":6417},{\"attributes\":{\"ref_id\":\"b56\"},\"end\":6425,\"start\":6421},{\"attributes\":{\"ref_id\":\"b57\"},\"end\":6429,\"start\":6425},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":6737,\"start\":6733},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6740,\"start\":6737},{\"attributes\":{\"ref_id\":\"b58\"},\"end\":6841,\"start\":6837},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":6907,\"start\":6903},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":6932,\"start\":6928},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":6935,\"start\":6932},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":6967,\"start\":6963},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":7027,\"start\":7023},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":7031,\"start\":7027},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":7035,\"start\":7031},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":7039,\"start\":7035},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":7043,\"start\":7039},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":7047,\"start\":7043},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":7070,\"start\":7066},{\"attributes\":{\"ref_id\":\"b62\"},\"end\":7074,\"start\":7070},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":7078,\"start\":7074},{\"attributes\":{\"ref_id\":\"b64\"},\"end\":7143,\"start\":7139},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":7171,\"start\":7167},{\"attributes\":{\"ref_id\":\"b65\"},\"end\":7174,\"start\":7171},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":7278,\"start\":7274},{\"attributes\":{\"ref_id\":\"b67\"},\"end\":7282,\"start\":7278},{\"attributes\":{\"ref_id\":\"b68\"},\"end\":7286,\"start\":7282},{\"attributes\":{\"ref_id\":\"b69\"},\"end\":7290,\"start\":7286},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":7392,\"start\":7388},{\"attributes\":{\"ref_id\":\"b70\"},\"end\":7404,\"start\":7400},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":7430,\"start\":7426},{\"attributes\":{\"ref_id\":\"b72\"},\"end\":7433,\"start\":7430},{\"attributes\":{\"ref_id\":\"b73\"},\"end\":7451,\"start\":7447},{\"attributes\":{\"ref_id\":\"b74\"},\"end\":7454,\"start\":7451},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":7523,\"start\":7519},{\"attributes\":{\"ref_id\":\"b76\"},\"end\":7527,\"start\":7523},{\"attributes\":{\"ref_id\":\"b77\"},\"end\":7531,\"start\":7527},{\"attributes\":{\"ref_id\":\"b71\"},\"end\":7550,\"start\":7546},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":7553,\"start\":7550},{\"attributes\":{\"ref_id\":\"b78\"},\"end\":7557,\"start\":7553},{\"attributes\":{\"ref_id\":\"b79\"},\"end\":7561,\"start\":7557},{\"attributes\":{\"ref_id\":\"b80\"},\"end\":7565,\"start\":7561},{\"attributes\":{\"ref_id\":\"b81\"},\"end\":7569,\"start\":7565},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":7670,\"start\":7667},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":7674,\"start\":7670},{\"attributes\":{\"ref_id\":\"b83\"},\"end\":7678,\"start\":7674},{\"attributes\":{\"ref_id\":\"b84\"},\"end\":7682,\"start\":7678},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":7810,\"start\":7806},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":7893,\"start\":7890},{\"attributes\":{\"ref_id\":\"b66\"},\"end\":7996,\"start\":7992},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":8143,\"start\":8139},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":8146,\"start\":8143},{\"attributes\":{\"ref_id\":\"b85\"},\"end\":8228,\"start\":8224},{\"attributes\":{\"ref_id\":\"b63\"},\"end\":8255,\"start\":8251},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":8322,\"start\":8319},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":8377,\"start\":8373},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":8393,\"start\":8389},{\"attributes\":{\"ref_id\":\"b22\"},\"end\":8615,\"start\":8611},{\"attributes\":{\"ref_id\":\"b88\"},\"end\":8673,\"start\":8669},{\"attributes\":{\"ref_id\":\"b89\"},\"end\":8698,\"start\":8694},{\"attributes\":{\"ref_id\":\"b90\"},\"end\":8701,\"start\":8698},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10177,\"start\":10173},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":10252,\"start\":10248},{\"attributes\":{\"ref_id\":\"b91\"},\"end\":10255,\"start\":10252},{\"attributes\":{\"ref_id\":\"b92\"},\"end\":10288,\"start\":10284},{\"attributes\":{\"ref_id\":\"b93\"},\"end\":10376,\"start\":10372},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":11043,\"start\":11039},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":11057,\"start\":11053},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":11455,\"start\":11451},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":11475,\"start\":11471},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":11486,\"start\":11482},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":11498,\"start\":11494},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":11508,\"start\":11504},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":11527,\"start\":11523},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":11790,\"start\":11786},{\"attributes\":{\"ref_id\":\"b9\"},\"end\":13641,\"start\":13637},{\"attributes\":{\"ref_id\":\"b94\"},\"end\":13670,\"start\":13666},{\"attributes\":{\"ref_id\":\"b95\"},\"end\":13695,\"start\":13691},{\"attributes\":{\"ref_id\":\"b96\"},\"end\":13698,\"start\":13695},{\"attributes\":{\"ref_id\":\"b60\"},\"end\":14141,\"start\":14137},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":14892,\"start\":14889},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":16151,\"start\":16147},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":16773,\"start\":16769},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":18657,\"start\":18654},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":19493,\"start\":19489},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":19496,\"start\":19493},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":19860,\"start\":19856},{\"attributes\":{\"ref_id\":\"b59\"},\"end\":20047,\"start\":20043},{\"attributes\":{\"ref_id\":\"b97\"},\"end\":20059,\"start\":20054},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":20374,\"start\":20371},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":21181,\"start\":21177},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":21184,\"start\":21181},{\"attributes\":{\"ref_id\":\"b75\"},\"end\":21862,\"start\":21858},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":22385,\"start\":22382},{\"attributes\":{\"ref_id\":\"b82\"},\"end\":22388,\"start\":22385},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":22391,\"start\":22388},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":22394,\"start\":22391},{\"attributes\":{\"ref_id\":\"b104\"},\"end\":22690,\"start\":22685},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":24679,\"start\":24675},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":28082,\"start\":28079},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":29191,\"start\":29188},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":30285,\"start\":30282},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":30455,\"start\":30452},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":36491,\"start\":36488},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":36504,\"start\":36500},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":38191,\"start\":38188},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":39061,\"start\":39058},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":40145,\"start\":40142},{\"attributes\":{\"ref_id\":\"b86\"},\"end\":43682,\"start\":43678},{\"attributes\":{\"ref_id\":\"b87\"},\"end\":43721,\"start\":43717},{\"attributes\":{\"ref_id\":\"b98\"},\"end\":43731,\"start\":43726},{\"attributes\":{\"ref_id\":\"b99\"},\"end\":43745,\"start\":43740},{\"attributes\":{\"ref_id\":\"b61\"},\"end\":43828,\"start\":43824},{\"attributes\":{\"ref_id\":\"b100\"},\"end\":43842,\"start\":43837}]", "figure": "[{\"attributes\":{\"id\":\"fig_2\"},\"end\":36914,\"start\":36639},{\"attributes\":{\"id\":\"fig_3\"},\"end\":37283,\"start\":36915},{\"attributes\":{\"id\":\"fig_4\"},\"end\":37459,\"start\":37284},{\"attributes\":{\"id\":\"fig_5\"},\"end\":37738,\"start\":37460},{\"attributes\":{\"id\":\"fig_6\"},\"end\":37906,\"start\":37739},{\"attributes\":{\"id\":\"fig_7\"},\"end\":38079,\"start\":37907},{\"attributes\":{\"id\":\"fig_8\"},\"end\":38958,\"start\":38080},{\"attributes\":{\"id\":\"fig_9\"},\"end\":39096,\"start\":38959},{\"attributes\":{\"id\":\"fig_10\"},\"end\":39607,\"start\":39097},{\"attributes\":{\"id\":\"fig_11\"},\"end\":40032,\"start\":39608},{\"attributes\":{\"id\":\"fig_12\"},\"end\":40213,\"start\":40033},{\"attributes\":{\"id\":\"fig_13\"},\"end\":40256,\"start\":40214},{\"attributes\":{\"id\":\"fig_14\"},\"end\":40474,\"start\":40257},{\"attributes\":{\"id\":\"fig_16\"},\"end\":40576,\"start\":40475},{\"attributes\":{\"id\":\"fig_18\"},\"end\":40627,\"start\":40577},{\"attributes\":{\"id\":\"fig_19\"},\"end\":40845,\"start\":40628},{\"attributes\":{\"id\":\"fig_20\"},\"end\":40886,\"start\":40846},{\"attributes\":{\"id\":\"fig_21\"},\"end\":40933,\"start\":40887},{\"attributes\":{\"id\":\"fig_22\"},\"end\":40986,\"start\":40934},{\"attributes\":{\"id\":\"fig_23\"},\"end\":41033,\"start\":40987},{\"attributes\":{\"id\":\"tab_0\",\"type\":\"table\"},\"end\":41497,\"start\":41034},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":41858,\"start\":41498},{\"attributes\":{\"id\":\"tab_3\",\"type\":\"table\"},\"end\":42787,\"start\":41859},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":43132,\"start\":42788},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":44276,\"start\":43133},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":45228,\"start\":44277},{\"attributes\":{\"id\":\"tab_7\",\"type\":\"table\"},\"end\":45614,\"start\":45229},{\"attributes\":{\"id\":\"tab_8\",\"type\":\"table\"},\"end\":46217,\"start\":45615},{\"attributes\":{\"id\":\"tab_12\",\"type\":\"table\"},\"end\":47701,\"start\":46218},{\"attributes\":{\"id\":\"tab_13\",\"type\":\"table\"},\"end\":48240,\"start\":47702}]", "paragraph": "[{\"end\":1741,\"start\":1635},{\"end\":1829,\"start\":1743},{\"end\":1923,\"start\":1831},{\"end\":2785,\"start\":1925},{\"end\":3521,\"start\":2787},{\"end\":4206,\"start\":3523},{\"end\":5159,\"start\":4208},{\"end\":5713,\"start\":5161},{\"end\":5867,\"start\":5730},{\"end\":6583,\"start\":5869},{\"end\":7175,\"start\":6585},{\"end\":7878,\"start\":7177},{\"end\":8288,\"start\":7880},{\"end\":8878,\"start\":8290},{\"end\":9453,\"start\":8910},{\"end\":10140,\"start\":9455},{\"end\":10766,\"start\":10142},{\"end\":11622,\"start\":10768},{\"end\":12457,\"start\":11624},{\"end\":12697,\"start\":12479},{\"end\":13156,\"start\":12699},{\"end\":13609,\"start\":13158},{\"end\":13813,\"start\":13611},{\"end\":13849,\"start\":13815},{\"end\":14668,\"start\":13868},{\"end\":14856,\"start\":14680},{\"end\":16117,\"start\":14879},{\"end\":16844,\"start\":16143},{\"end\":16850,\"start\":16846},{\"end\":16901,\"start\":16877},{\"end\":16977,\"start\":16903},{\"end\":17525,\"start\":16979},{\"end\":18012,\"start\":17541},{\"end\":18284,\"start\":18043},{\"end\":18669,\"start\":18286},{\"end\":19112,\"start\":18671},{\"end\":19663,\"start\":19114},{\"end\":21069,\"start\":19665},{\"end\":21383,\"start\":21103},{\"end\":21649,\"start\":21385},{\"end\":21863,\"start\":21651},{\"end\":22627,\"start\":21865},{\"end\":22934,\"start\":22629},{\"end\":24322,\"start\":22936},{\"end\":25020,\"start\":24324},{\"end\":25721,\"start\":25035},{\"end\":26466,\"start\":25760},{\"end\":26813,\"start\":26468},{\"end\":26924,\"start\":26861},{\"end\":26963,\"start\":26926},{\"end\":27054,\"start\":26965},{\"end\":27095,\"start\":27056},{\"end\":27183,\"start\":27097},{\"end\":27365,\"start\":27185},{\"end\":27419,\"start\":27367},{\"end\":27494,\"start\":27421},{\"end\":27555,\"start\":27496},{\"end\":27662,\"start\":27557},{\"end\":27867,\"start\":27664},{\"end\":27922,\"start\":27869},{\"end\":28117,\"start\":27951},{\"end\":28188,\"start\":28119},{\"end\":28276,\"start\":28190},{\"end\":28366,\"start\":28278},{\"end\":28463,\"start\":28368},{\"end\":28521,\"start\":28465},{\"end\":28570,\"start\":28523},{\"end\":28675,\"start\":28572},{\"end\":28757,\"start\":28697},{\"end\":28805,\"start\":28759},{\"end\":28982,\"start\":28807},{\"end\":29019,\"start\":28984},{\"end\":29226,\"start\":29021},{\"end\":29304,\"start\":29228},{\"end\":29352,\"start\":29306},{\"end\":29445,\"start\":29354},{\"end\":29531,\"start\":29447},{\"end\":29574,\"start\":29533},{\"end\":29632,\"start\":29576},{\"end\":29696,\"start\":29634},{\"end\":29738,\"start\":29698},{\"end\":29827,\"start\":29740},{\"end\":29932,\"start\":29829},{\"end\":29988,\"start\":29934},{\"end\":30038,\"start\":29990},{\"end\":30101,\"start\":30040},{\"end\":30321,\"start\":30103},{\"end\":30523,\"start\":30331},{\"end\":30560,\"start\":30525},{\"end\":30627,\"start\":30562},{\"end\":30669,\"start\":30629},{\"end\":30842,\"start\":30671},{\"end\":30941,\"start\":30844},{\"end\":31220,\"start\":30963},{\"end\":31762,\"start\":31249},{\"end\":32103,\"start\":31815},{\"end\":32261,\"start\":32160},{\"end\":32984,\"start\":32263},{\"end\":33396,\"start\":32986},{\"end\":34873,\"start\":33415},{\"end\":35174,\"start\":34934},{\"end\":36413,\"start\":35176},{\"end\":36638,\"start\":36439}]", "formula": null, "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":2478,\"start\":2471},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":3520,\"start\":3513},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":4463,\"start\":4455},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":4846,\"start\":4839},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":12456,\"start\":12447},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":16678,\"start\":16669},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":18542,\"start\":18535},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":18668,\"start\":18661},{\"attributes\":{\"ref_id\":\"tab_0\"},\"end\":20236,\"start\":20229},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":20333,\"start\":20326},{\"attributes\":{\"ref_id\":\"tab_3\"},\"end\":20547,\"start\":20540},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":21954,\"start\":21947},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":22952,\"start\":22945},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":23441,\"start\":23434},{\"attributes\":{\"ref_id\":\"tab_7\"},\"end\":24603,\"start\":24596},{\"attributes\":{\"ref_id\":\"tab_13\"},\"end\":34047,\"start\":34040},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":34245,\"start\":34238},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":34854,\"start\":34847}]", "section_header": "[{\"attributes\":{\"n\":\"2\"},\"end\":5728,\"start\":5716},{\"attributes\":{\"n\":\"3\"},\"end\":8887,\"start\":8881},{\"attributes\":{\"n\":\"3.1\"},\"end\":8908,\"start\":8890},{\"attributes\":{\"n\":\"3.2\"},\"end\":12477,\"start\":12460},{\"end\":13866,\"start\":13852},{\"attributes\":{\"n\":\"4\"},\"end\":14678,\"start\":14671},{\"attributes\":{\"n\":\"4.1\"},\"end\":14877,\"start\":14859},{\"attributes\":{\"n\":\"4.2\"},\"end\":16141,\"start\":16120},{\"end\":16858,\"start\":16853},{\"end\":16875,\"start\":16861},{\"attributes\":{\"n\":\"5\"},\"end\":17539,\"start\":17528},{\"attributes\":{\"n\":\"5.1\"},\"end\":18041,\"start\":18015},{\"attributes\":{\"n\":\"5.2\"},\"end\":21101,\"start\":21072},{\"attributes\":{\"n\":\"6\"},\"end\":25033,\"start\":25023},{\"end\":25758,\"start\":25724},{\"end\":26859,\"start\":26816},{\"end\":27949,\"start\":27925},{\"end\":28695,\"start\":28678},{\"end\":30329,\"start\":30324},{\"end\":30961,\"start\":30944},{\"end\":31247,\"start\":31223},{\"end\":31774,\"start\":31765},{\"end\":31786,\"start\":31777},{\"end\":31813,\"start\":31789},{\"end\":32115,\"start\":32106},{\"end\":32158,\"start\":32118},{\"end\":33413,\"start\":33399},{\"end\":34905,\"start\":34876},{\"end\":34932,\"start\":34908},{\"end\":36437,\"start\":36416},{\"end\":36650,\"start\":36640},{\"end\":36926,\"start\":36916},{\"end\":37295,\"start\":37285},{\"end\":37471,\"start\":37461},{\"end\":37750,\"start\":37740},{\"end\":37918,\"start\":37908},{\"end\":38101,\"start\":38081},{\"end\":38971,\"start\":38960},{\"end\":39109,\"start\":39098},{\"end\":40045,\"start\":40034},{\"end\":40269,\"start\":40258},{\"end\":40640,\"start\":40629},{\"end\":40858,\"start\":40847},{\"end\":40899,\"start\":40888},{\"end\":40946,\"start\":40935},{\"end\":40999,\"start\":40988},{\"end\":41044,\"start\":41035},{\"end\":41508,\"start\":41499},{\"end\":41869,\"start\":41860},{\"end\":42798,\"start\":42789},{\"end\":43143,\"start\":43134},{\"end\":44287,\"start\":44278},{\"end\":45239,\"start\":45230},{\"end\":45625,\"start\":45616},{\"end\":47712,\"start\":47703}]", "table": "[{\"end\":41497,\"start\":41225},{\"end\":41858,\"start\":41558},{\"end\":42787,\"start\":42050},{\"end\":43132,\"start\":42898},{\"end\":44276,\"start\":43843},{\"end\":45228,\"start\":44471},{\"end\":45614,\"start\":45375},{\"end\":47701,\"start\":46714},{\"end\":48240,\"start\":48034}]", "figure_caption": "[{\"end\":36914,\"start\":36652},{\"end\":37283,\"start\":36928},{\"end\":37459,\"start\":37297},{\"end\":37738,\"start\":37473},{\"end\":37906,\"start\":37752},{\"end\":38079,\"start\":37920},{\"end\":38958,\"start\":38104},{\"end\":39096,\"start\":38974},{\"end\":39607,\"start\":39112},{\"end\":40032,\"start\":39610},{\"end\":40213,\"start\":40048},{\"end\":40256,\"start\":40216},{\"end\":40474,\"start\":40272},{\"end\":40576,\"start\":40477},{\"end\":40627,\"start\":40579},{\"end\":40845,\"start\":40643},{\"end\":40886,\"start\":40861},{\"end\":40933,\"start\":40902},{\"end\":40986,\"start\":40949},{\"end\":41033,\"start\":41002},{\"end\":41225,\"start\":41046},{\"end\":41558,\"start\":41510},{\"end\":42050,\"start\":41871},{\"end\":42898,\"start\":42800},{\"end\":43843,\"start\":43145},{\"end\":44471,\"start\":44289},{\"end\":45375,\"start\":45241},{\"end\":46217,\"start\":45627},{\"end\":46714,\"start\":46220},{\"end\":48034,\"start\":47714}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":4751,\"start\":4743},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":9433,\"start\":9425},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":9801,\"start\":9793},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":10047,\"start\":10039},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":10513,\"start\":10505},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":10583,\"start\":10575},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":10965,\"start\":10956},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":11234,\"start\":11226},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":12027,\"start\":12019},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":15296,\"start\":15288},{\"attributes\":{\"ref_id\":\"fig_4\"},\"end\":16002,\"start\":15994},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":16598,\"start\":16590},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":17357,\"start\":17349},{\"attributes\":{\"ref_id\":\"fig_6\"},\"end\":20724,\"start\":20716},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22028,\"start\":22020},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":22174,\"start\":22166},{\"attributes\":{\"ref_id\":\"fig_7\"},\"end\":24355,\"start\":24347},{\"end\":27992,\"start\":27984},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":29101,\"start\":29092},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":30195,\"start\":30186},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":30355,\"start\":30346},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":31013,\"start\":31004},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":31342,\"start\":31333},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":31557,\"start\":31548},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":31897,\"start\":31888},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":32781,\"start\":32772},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":32915,\"start\":32906},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":33109,\"start\":33100},{\"attributes\":{\"ref_id\":\"fig_5\"},\"end\":34330,\"start\":34322}]", "bib_author_first_name": "[{\"end\":49652,\"start\":49646},{\"end\":49662,\"start\":49658},{\"end\":49829,\"start\":49823},{\"end\":49843,\"start\":49837},{\"end\":49856,\"start\":49851},{\"end\":49865,\"start\":49862},{\"end\":49879,\"start\":49873},{\"end\":50141,\"start\":50138},{\"end\":50153,\"start\":50149},{\"end\":50168,\"start\":50160},{\"end\":50170,\"start\":50169},{\"end\":50182,\"start\":50179},{\"end\":50376,\"start\":50373},{\"end\":50381,\"start\":50377},{\"end\":50678,\"start\":50674},{\"end\":50774,\"start\":50768},{\"end\":50783,\"start\":50782},{\"end\":50799,\"start\":50793},{\"end\":50802,\"start\":50800},{\"end\":50820,\"start\":50809},{\"end\":50822,\"start\":50821},{\"end\":51229,\"start\":51221},{\"end\":51240,\"start\":51234},{\"end\":51254,\"start\":51248},{\"end\":51264,\"start\":51261},{\"end\":51280,\"start\":51273},{\"end\":51300,\"start\":51293},{\"end\":51320,\"start\":51316},{\"end\":51334,\"start\":51327},{\"end\":51350,\"start\":51343},{\"end\":51370,\"start\":51363},{\"end\":51775,\"start\":51769},{\"end\":51795,\"start\":51789},{\"end\":51807,\"start\":51801},{\"end\":51826,\"start\":51819},{\"end\":51841,\"start\":51834},{\"end\":52118,\"start\":52114},{\"end\":52133,\"start\":52127},{\"end\":52148,\"start\":52143},{\"end\":52163,\"start\":52159},{\"end\":52176,\"start\":52171},{\"end\":52188,\"start\":52185},{\"end\":52207,\"start\":52201},{\"end\":52222,\"start\":52217},{\"end\":52240,\"start\":52231},{\"end\":52254,\"start\":52251},{\"end\":52553,\"start\":52548},{\"end\":52567,\"start\":52561},{\"end\":52579,\"start\":52574},{\"end\":52594,\"start\":52588},{\"end\":52870,\"start\":52864},{\"end\":52882,\"start\":52879},{\"end\":52898,\"start\":52889},{\"end\":52912,\"start\":52908},{\"end\":53207,\"start\":53200},{\"end\":53226,\"start\":53220},{\"end\":53238,\"start\":53235},{\"end\":53249,\"start\":53245},{\"end\":53761,\"start\":53752},{\"end\":53780,\"start\":53773},{\"end\":53794,\"start\":53788},{\"end\":53811,\"start\":53805},{\"end\":53832,\"start\":53825},{\"end\":53847,\"start\":53841},{\"end\":53859,\"start\":53855},{\"end\":53874,\"start\":53869},{\"end\":53887,\"start\":53883},{\"end\":54240,\"start\":54231},{\"end\":54258,\"start\":54252},{\"end\":54276,\"start\":54269},{\"end\":54288,\"start\":54284},{\"end\":54301,\"start\":54297},{\"end\":54317,\"start\":54312},{\"end\":54330,\"start\":54326},{\"end\":54346,\"start\":54340},{\"end\":54361,\"start\":54354},{\"end\":54378,\"start\":54370},{\"end\":54716,\"start\":54709},{\"end\":54719,\"start\":54717},{\"end\":54733,\"start\":54728},{\"end\":54747,\"start\":54741},{\"end\":54969,\"start\":54965},{\"end\":54983,\"start\":54979},{\"end\":54988,\"start\":54984},{\"end\":54999,\"start\":54994},{\"end\":55015,\"start\":55009},{\"end\":55031,\"start\":55024},{\"end\":55045,\"start\":55037},{\"end\":55061,\"start\":55055},{\"end\":55076,\"start\":55070},{\"end\":55091,\"start\":55085},{\"end\":55105,\"start\":55101},{\"end\":55428,\"start\":55422},{\"end\":55442,\"start\":55433},{\"end\":55458,\"start\":55453},{\"end\":55474,\"start\":55467},{\"end\":55688,\"start\":55684},{\"end\":55719,\"start\":55712},{\"end\":55736,\"start\":55729},{\"end\":55746,\"start\":55742},{\"end\":55758,\"start\":55754},{\"end\":55770,\"start\":55765},{\"end\":55785,\"start\":55779},{\"end\":55801,\"start\":55792},{\"end\":55817,\"start\":55810},{\"end\":56121,\"start\":56115},{\"end\":56137,\"start\":56130},{\"end\":56153,\"start\":56146},{\"end\":56164,\"start\":56159},{\"end\":56178,\"start\":56171},{\"end\":56189,\"start\":56185},{\"end\":56203,\"start\":56199},{\"end\":56214,\"start\":56210},{\"end\":56477,\"start\":56473},{\"end\":56489,\"start\":56485},{\"end\":56502,\"start\":56498},{\"end\":56517,\"start\":56511},{\"end\":56531,\"start\":56527},{\"end\":56545,\"start\":56540},{\"end\":56844,\"start\":56837},{\"end\":56861,\"start\":56854},{\"end\":56875,\"start\":56868},{\"end\":56888,\"start\":56884},{\"end\":56896,\"start\":56893},{\"end\":56909,\"start\":56904},{\"end\":56911,\"start\":56910},{\"end\":56926,\"start\":56920},{\"end\":56947,\"start\":56940},{\"end\":56955,\"start\":56948},{\"end\":57305,\"start\":57300},{\"end\":57322,\"start\":57315},{\"end\":57341,\"start\":57334},{\"end\":57357,\"start\":57350},{\"end\":57370,\"start\":57365},{\"end\":57646,\"start\":57642},{\"end\":57663,\"start\":57655},{\"end\":57680,\"start\":57674},{\"end\":57695,\"start\":57689},{\"end\":57709,\"start\":57703},{\"end\":57722,\"start\":57719},{\"end\":57735,\"start\":57731},{\"end\":57751,\"start\":57747},{\"end\":58061,\"start\":58055},{\"end\":58078,\"start\":58070},{\"end\":58093,\"start\":58089},{\"end\":58107,\"start\":58102},{\"end\":58117,\"start\":58113},{\"end\":58359,\"start\":58354},{\"end\":58372,\"start\":58366},{\"end\":58382,\"start\":58377},{\"end\":58386,\"start\":58383},{\"end\":58397,\"start\":58391},{\"end\":58408,\"start\":58403},{\"end\":58423,\"start\":58419},{\"end\":58668,\"start\":58662},{\"end\":58675,\"start\":58673},{\"end\":58689,\"start\":58681},{\"end\":58703,\"start\":58694},{\"end\":58718,\"start\":58711},{\"end\":58726,\"start\":58723},{\"end\":58740,\"start\":58734},{\"end\":58754,\"start\":58747},{\"end\":58761,\"start\":58759},{\"end\":58772,\"start\":58768},{\"end\":59072,\"start\":59063},{\"end\":59086,\"start\":59080},{\"end\":59098,\"start\":59091},{\"end\":59110,\"start\":59103},{\"end\":59120,\"start\":59117},{\"end\":59134,\"start\":59128},{\"end\":59146,\"start\":59141},{\"end\":59161,\"start\":59153},{\"end\":59508,\"start\":59500},{\"end\":59518,\"start\":59514},{\"end\":59532,\"start\":59525},{\"end\":59546,\"start\":59538},{\"end\":59882,\"start\":59876},{\"end\":59893,\"start\":59887},{\"end\":59904,\"start\":59898},{\"end\":59921,\"start\":59915},{\"end\":60228,\"start\":60220},{\"end\":60241,\"start\":60234},{\"end\":60254,\"start\":60249},{\"end\":60270,\"start\":60265},{\"end\":60283,\"start\":60277},{\"end\":60296,\"start\":60292},{\"end\":60311,\"start\":60306},{\"end\":60330,\"start\":60320},{\"end\":60625,\"start\":60619},{\"end\":60639,\"start\":60635},{\"end\":60651,\"start\":60645},{\"end\":60665,\"start\":60659},{\"end\":60680,\"start\":60675},{\"end\":60693,\"start\":60687},{\"end\":60712,\"start\":60703},{\"end\":60725,\"start\":60719},{\"end\":60744,\"start\":60738},{\"end\":60754,\"start\":60749},{\"end\":60756,\"start\":60755},{\"end\":61103,\"start\":61096},{\"end\":61119,\"start\":61113},{\"end\":61136,\"start\":61130},{\"end\":61351,\"start\":61344},{\"end\":61368,\"start\":61361},{\"end\":61380,\"start\":61375},{\"end\":61397,\"start\":61387},{\"end\":61410,\"start\":61406},{\"end\":61420,\"start\":61415},{\"end\":61433,\"start\":61431},{\"end\":61453,\"start\":61441},{\"end\":61469,\"start\":61463},{\"end\":61490,\"start\":61482},{\"end\":61506,\"start\":61498},{\"end\":61527,\"start\":61519},{\"end\":61905,\"start\":61900},{\"end\":61924,\"start\":61916},{\"end\":61934,\"start\":61929},{\"end\":61950,\"start\":61944},{\"end\":61962,\"start\":61958},{\"end\":61979,\"start\":61972},{\"end\":61990,\"start\":61987},{\"end\":62262,\"start\":62261},{\"end\":62278,\"start\":62271},{\"end\":62294,\"start\":62287},{\"end\":62312,\"start\":62306},{\"end\":62329,\"start\":62321},{\"end\":62526,\"start\":62520},{\"end\":62538,\"start\":62531},{\"end\":62550,\"start\":62545},{\"end\":62562,\"start\":62558},{\"end\":62748,\"start\":62741},{\"end\":62756,\"start\":62753},{\"end\":62769,\"start\":62762},{\"end\":62781,\"start\":62776},{\"end\":62791,\"start\":62788},{\"end\":62801,\"start\":62796},{\"end\":62816,\"start\":62810},{\"end\":62818,\"start\":62817},{\"end\":63108,\"start\":63098},{\"end\":63116,\"start\":63114},{\"end\":63314,\"start\":63307},{\"end\":63326,\"start\":63319},{\"end\":63342,\"start\":63334},{\"end\":63352,\"start\":63348},{\"end\":63550,\"start\":63546},{\"end\":63567,\"start\":63563},{\"end\":63587,\"start\":63579},{\"end\":63589,\"start\":63588},{\"end\":63900,\"start\":63896},{\"end\":63921,\"start\":63914},{\"end\":63937,\"start\":63931},{\"end\":64125,\"start\":64121},{\"end\":64144,\"start\":64138},{\"end\":64273,\"start\":64269},{\"end\":64285,\"start\":64284},{\"end\":64545,\"start\":64540},{\"end\":64560,\"start\":64555},{\"end\":64573,\"start\":64568},{\"end\":64586,\"start\":64580},{\"end\":64600,\"start\":64593},{\"end\":64611,\"start\":64607},{\"end\":64626,\"start\":64621},{\"end\":64638,\"start\":64634},{\"end\":64653,\"start\":64647},{\"end\":64664,\"start\":64659},{\"end\":64910,\"start\":64907},{\"end\":64927,\"start\":64923},{\"end\":64948,\"start\":64943},{\"end\":64960,\"start\":64956},{\"end\":64970,\"start\":64965},{\"end\":64992,\"start\":64985},{\"end\":65005,\"start\":65000},{\"end\":65023,\"start\":65017},{\"end\":65256,\"start\":65252},{\"end\":65271,\"start\":65265},{\"end\":65284,\"start\":65279},{\"end\":65299,\"start\":65294},{\"end\":65316,\"start\":65310},{\"end\":65331,\"start\":65327},{\"end\":65551,\"start\":65546},{\"end\":65566,\"start\":65561},{\"end\":65754,\"start\":65747},{\"end\":65767,\"start\":65762},{\"end\":65782,\"start\":65777},{\"end\":65977,\"start\":65973},{\"end\":65990,\"start\":65984},{\"end\":66002,\"start\":65997},{\"end\":66014,\"start\":66009},{\"end\":66027,\"start\":66022},{\"end\":66036,\"start\":66034},{\"end\":66049,\"start\":66042},{\"end\":66057,\"start\":66055},{\"end\":66067,\"start\":66063},{\"end\":66081,\"start\":66074},{\"end\":66366,\"start\":66361},{\"end\":66380,\"start\":66374},{\"end\":66602,\"start\":66596},{\"end\":66621,\"start\":66616},{\"end\":66638,\"start\":66629},{\"end\":66655,\"start\":66651},{\"end\":66676,\"start\":66669},{\"end\":66689,\"start\":66683},{\"end\":66710,\"start\":66703},{\"end\":66729,\"start\":66721},{\"end\":66745,\"start\":66740},{\"end\":66762,\"start\":66755},{\"end\":67095,\"start\":67087},{\"end\":67115,\"start\":67106},{\"end\":67286,\"start\":67278},{\"end\":67295,\"start\":67291},{\"end\":67308,\"start\":67302},{\"end\":67504,\"start\":67500},{\"end\":67518,\"start\":67513},{\"end\":67532,\"start\":67528},{\"end\":67545,\"start\":67539},{\"end\":67762,\"start\":67753},{\"end\":67776,\"start\":67770},{\"end\":67787,\"start\":67779},{\"end\":67995,\"start\":67991},{\"end\":68009,\"start\":68002},{\"end\":68211,\"start\":68205},{\"end\":68232,\"start\":68228},{\"end\":68244,\"start\":68240},{\"end\":68267,\"start\":68262},{\"end\":68450,\"start\":68441},{\"end\":68465,\"start\":68461},{\"end\":68481,\"start\":68474},{\"end\":68493,\"start\":68488},{\"end\":68504,\"start\":68499},{\"end\":68519,\"start\":68514},{\"end\":68878,\"start\":68874},{\"end\":68890,\"start\":68887},{\"end\":68908,\"start\":68901},{\"end\":68922,\"start\":68917},{\"end\":69233,\"start\":69228},{\"end\":69248,\"start\":69241},{\"end\":69375,\"start\":69374},{\"end\":69377,\"start\":69376},{\"end\":69625,\"start\":69618},{\"end\":69636,\"start\":69631},{\"end\":69649,\"start\":69642},{\"end\":69663,\"start\":69657},{\"end\":69676,\"start\":69669},{\"end\":69689,\"start\":69681},{\"end\":69699,\"start\":69695},{\"end\":69710,\"start\":69705},{\"end\":69723,\"start\":69720},{\"end\":70053,\"start\":70047},{\"end\":70064,\"start\":70058},{\"end\":70076,\"start\":70069},{\"end\":70090,\"start\":70084},{\"end\":70378,\"start\":70376},{\"end\":70391,\"start\":70384},{\"end\":70401,\"start\":70397},{\"end\":70415,\"start\":70408},{\"end\":70437,\"start\":70431},{\"end\":70449,\"start\":70442},{\"end\":70460,\"start\":70457},{\"end\":70469,\"start\":70465},{\"end\":70476,\"start\":70470},{\"end\":70492,\"start\":70486},{\"end\":70749,\"start\":70748},{\"end\":70763,\"start\":70757},{\"end\":70779,\"start\":70771},{\"end\":70795,\"start\":70792},{\"end\":70810,\"start\":70804},{\"end\":70825,\"start\":70821},{\"end\":70839,\"start\":70833},{\"end\":70851,\"start\":70844},{\"end\":70868,\"start\":70862},{\"end\":70879,\"start\":70876},{\"end\":71220,\"start\":71212},{\"end\":71232,\"start\":71226},{\"end\":71244,\"start\":71237},{\"end\":71260,\"start\":71252},{\"end\":71276,\"start\":71268},{\"end\":71291,\"start\":71284},{\"end\":71303,\"start\":71297},{\"end\":71305,\"start\":71304},{\"end\":71326,\"start\":71317},{\"end\":71594,\"start\":71593},{\"end\":71608,\"start\":71603},{\"end\":71621,\"start\":71614},{\"end\":71770,\"start\":71766},{\"end\":71782,\"start\":71775},{\"end\":71791,\"start\":71788},{\"end\":71805,\"start\":71797},{\"end\":71820,\"start\":71812},{\"end\":72123,\"start\":72118},{\"end\":72140,\"start\":72136},{\"end\":72147,\"start\":72146},{\"end\":72149,\"start\":72148},{\"end\":72160,\"start\":72159},{\"end\":72165,\"start\":72161},{\"end\":72176,\"start\":72175},{\"end\":72185,\"start\":72177},{\"end\":72446,\"start\":72441},{\"end\":72454,\"start\":72453},{\"end\":72475,\"start\":72468},{\"end\":72490,\"start\":72489},{\"end\":72504,\"start\":72498},{\"end\":72518,\"start\":72512},{\"end\":72820,\"start\":72817},{\"end\":72829,\"start\":72825},{\"end\":72841,\"start\":72836},{\"end\":72854,\"start\":72848},{\"end\":72871,\"start\":72864},{\"end\":73388,\"start\":73382},{\"end\":73395,\"start\":73394},{\"end\":73410,\"start\":73403},{\"end\":73424,\"start\":73418},{\"end\":73438,\"start\":73432},{\"end\":73455,\"start\":73447},{\"end\":73744,\"start\":73733},{\"end\":73752,\"start\":73751},{\"end\":73768,\"start\":73760},{\"end\":74041,\"start\":74035},{\"end\":74054,\"start\":74047},{\"end\":74066,\"start\":74060},{\"end\":74441,\"start\":74436},{\"end\":74460,\"start\":74451},{\"end\":74473,\"start\":74469},{\"end\":74489,\"start\":74482},{\"end\":74696,\"start\":74690},{\"end\":74706,\"start\":74703},{\"end\":74724,\"start\":74716},{\"end\":74739,\"start\":74734},{\"end\":74741,\"start\":74740},{\"end\":74958,\"start\":74952},{\"end\":74971,\"start\":74967},{\"end\":74979,\"start\":74977},{\"end\":75007,\"start\":75000},{\"end\":75019,\"start\":75014},{\"end\":75039,\"start\":75027},{\"end\":75334,\"start\":75326},{\"end\":75350,\"start\":75343},{\"end\":75365,\"start\":75358},{\"end\":75380,\"start\":75373},{\"end\":75632,\"start\":75624},{\"end\":75641,\"start\":75638},{\"end\":75654,\"start\":75648},{\"end\":75669,\"start\":75661},{\"end\":75682,\"start\":75675},{\"end\":76096,\"start\":76092},{\"end\":76111,\"start\":76104},{\"end\":76126,\"start\":76118},{\"end\":76142,\"start\":76137},{\"end\":76518,\"start\":76514},{\"end\":76528,\"start\":76525},{\"end\":76549,\"start\":76541},{\"end\":76551,\"start\":76550},{\"end\":76566,\"start\":76560},{\"end\":76578,\"start\":76575},{\"end\":76819,\"start\":76810},{\"end\":76839,\"start\":76833},{\"end\":76857,\"start\":76850},{\"end\":76874,\"start\":76867},{\"end\":76893,\"start\":76885},{\"end\":77146,\"start\":77136},{\"end\":77155,\"start\":77152},{\"end\":77167,\"start\":77161},{\"end\":77180,\"start\":77174},{\"end\":77198,\"start\":77191},{\"end\":77208,\"start\":77205},{\"end\":77223,\"start\":77216},{\"end\":77236,\"start\":77231},{\"end\":77252,\"start\":77245},{\"end\":77266,\"start\":77258},{\"end\":77796,\"start\":77794},{\"end\":77806,\"start\":77802},{\"end\":77814,\"start\":77811},{\"end\":77828,\"start\":77822},{\"end\":77840,\"start\":77833},{\"end\":77862,\"start\":77856},{\"end\":77874,\"start\":77867},{\"end\":77885,\"start\":77882},{\"end\":77894,\"start\":77890},{\"end\":77901,\"start\":77895},{\"end\":77917,\"start\":77911},{\"end\":78231,\"start\":78227},{\"end\":78246,\"start\":78240},{\"end\":78260,\"start\":78252},{\"end\":78277,\"start\":78271},{\"end\":78291,\"start\":78287},{\"end\":78467,\"start\":78463},{\"end\":78482,\"start\":78476},{\"end\":78716,\"start\":78711},{\"end\":78728,\"start\":78723},{\"end\":78739,\"start\":78733},{\"end\":79093,\"start\":79087},{\"end\":79106,\"start\":79098},{\"end\":79125,\"start\":79116},{\"end\":79140,\"start\":79133},{\"end\":79437,\"start\":79431},{\"end\":79448,\"start\":79443},{\"end\":79458,\"start\":79453},{\"end\":79485,\"start\":79479},{\"end\":79500,\"start\":79496},{\"end\":79758,\"start\":79753},{\"end\":79768,\"start\":79765},{\"end\":79781,\"start\":79775},{\"end\":79791,\"start\":79787},{\"end\":79803,\"start\":79797},{\"end\":79816,\"start\":79808},{\"end\":79829,\"start\":79823},{\"end\":79844,\"start\":79837},{\"end\":79854,\"start\":79851},{\"end\":80083,\"start\":80081},{\"end\":80097,\"start\":80091},{\"end\":80109,\"start\":80103},{\"end\":80121,\"start\":80119},{\"end\":80135,\"start\":80128},{\"end\":80145,\"start\":80141},{\"end\":80159,\"start\":80153},{\"end\":80171,\"start\":80164},{\"end\":80519,\"start\":80516},{\"end\":80533,\"start\":80530},{\"end\":80542,\"start\":80540},{\"end\":80554,\"start\":80547},{\"end\":80568,\"start\":80563},{\"end\":80780,\"start\":80772},{\"end\":80790,\"start\":80787},{\"end\":80805,\"start\":80796},{\"end\":80821,\"start\":80816},{\"end\":80838,\"start\":80830},{\"end\":81090,\"start\":81081},{\"end\":81107,\"start\":81100},{\"end\":81125,\"start\":81119},{\"end\":81136,\"start\":81133},{\"end\":81153,\"start\":81145},{\"end\":81568,\"start\":81562},{\"end\":81581,\"start\":81574},{\"end\":81592,\"start\":81587},{\"end\":81871,\"start\":81868},{\"end\":81886,\"start\":81877},{\"end\":81897,\"start\":81893},{\"end\":81912,\"start\":81904},{\"end\":81926,\"start\":81919},{\"end\":81939,\"start\":81932},{\"end\":82241,\"start\":82238},{\"end\":82255,\"start\":82254},{\"end\":82271,\"start\":82264},{\"end\":82292,\"start\":82284},{\"end\":82294,\"start\":82293},{\"end\":82307,\"start\":82303},{\"end\":82319,\"start\":82316},{\"end\":82593,\"start\":82587},{\"end\":82604,\"start\":82598},{\"end\":82615,\"start\":82611},{\"end\":82630,\"start\":82624},{\"end\":82649,\"start\":82642},{\"end\":82656,\"start\":82654},{\"end\":82669,\"start\":82663},{\"end\":82890,\"start\":82882},{\"end\":83106,\"start\":83098},{\"end\":83119,\"start\":83112},{\"end\":83134,\"start\":83126},{\"end\":83148,\"start\":83141},{\"end\":83160,\"start\":83153},{\"end\":83171,\"start\":83166},{\"end\":83184,\"start\":83177},{\"end\":83197,\"start\":83190},{\"end\":83213,\"start\":83203},{\"end\":83489,\"start\":83481},{\"end\":83501,\"start\":83495},{\"end\":83511,\"start\":83507},{\"end\":83525,\"start\":83518},{\"end\":83537,\"start\":83531},{\"end\":83553,\"start\":83545},{\"end\":83567,\"start\":83560},{\"end\":84104,\"start\":84098},{\"end\":84119,\"start\":84113},{\"end\":84136,\"start\":84130},{\"end\":84158,\"start\":84150},{\"end\":84172,\"start\":84168},{\"end\":94464,\"start\":94463},{\"end\":94472,\"start\":94471}]", "bib_author_last_name": "[{\"end\":49656,\"start\":49653},{\"end\":49669,\"start\":49663},{\"end\":49835,\"start\":49830},{\"end\":49849,\"start\":49844},{\"end\":49860,\"start\":49857},{\"end\":49871,\"start\":49866},{\"end\":49884,\"start\":49880},{\"end\":50147,\"start\":50142},{\"end\":50158,\"start\":50154},{\"end\":50177,\"start\":50171},{\"end\":50193,\"start\":50183},{\"end\":50386,\"start\":50382},{\"end\":50394,\"start\":50388},{\"end\":50398,\"start\":50396},{\"end\":50685,\"start\":50679},{\"end\":50780,\"start\":50775},{\"end\":50791,\"start\":50784},{\"end\":50807,\"start\":50803},{\"end\":50828,\"start\":50823},{\"end\":50839,\"start\":50830},{\"end\":51232,\"start\":51230},{\"end\":51246,\"start\":51241},{\"end\":51259,\"start\":51255},{\"end\":51271,\"start\":51265},{\"end\":51291,\"start\":51281},{\"end\":51314,\"start\":51301},{\"end\":51325,\"start\":51321},{\"end\":51341,\"start\":51335},{\"end\":51361,\"start\":51351},{\"end\":51374,\"start\":51371},{\"end\":51787,\"start\":51776},{\"end\":51799,\"start\":51796},{\"end\":51817,\"start\":51808},{\"end\":51832,\"start\":51827},{\"end\":51848,\"start\":51842},{\"end\":52125,\"start\":52119},{\"end\":52141,\"start\":52134},{\"end\":52157,\"start\":52149},{\"end\":52169,\"start\":52164},{\"end\":52183,\"start\":52177},{\"end\":52199,\"start\":52189},{\"end\":52215,\"start\":52208},{\"end\":52229,\"start\":52223},{\"end\":52249,\"start\":52241},{\"end\":52262,\"start\":52255},{\"end\":52559,\"start\":52554},{\"end\":52572,\"start\":52568},{\"end\":52586,\"start\":52580},{\"end\":52602,\"start\":52595},{\"end\":52877,\"start\":52871},{\"end\":52887,\"start\":52883},{\"end\":52906,\"start\":52899},{\"end\":52920,\"start\":52913},{\"end\":53218,\"start\":53208},{\"end\":53233,\"start\":53227},{\"end\":53243,\"start\":53239},{\"end\":53257,\"start\":53250},{\"end\":53771,\"start\":53762},{\"end\":53786,\"start\":53781},{\"end\":53803,\"start\":53795},{\"end\":53823,\"start\":53812},{\"end\":53839,\"start\":53833},{\"end\":53853,\"start\":53848},{\"end\":53867,\"start\":53860},{\"end\":53881,\"start\":53875},{\"end\":53899,\"start\":53888},{\"end\":54250,\"start\":54241},{\"end\":54267,\"start\":54259},{\"end\":54282,\"start\":54277},{\"end\":54295,\"start\":54289},{\"end\":54310,\"start\":54302},{\"end\":54324,\"start\":54318},{\"end\":54338,\"start\":54331},{\"end\":54352,\"start\":54347},{\"end\":54368,\"start\":54362},{\"end\":54387,\"start\":54379},{\"end\":54726,\"start\":54720},{\"end\":54739,\"start\":54734},{\"end\":54755,\"start\":54748},{\"end\":54977,\"start\":54970},{\"end\":54992,\"start\":54989},{\"end\":55007,\"start\":55000},{\"end\":55022,\"start\":55016},{\"end\":55035,\"start\":55032},{\"end\":55053,\"start\":55046},{\"end\":55068,\"start\":55062},{\"end\":55083,\"start\":55077},{\"end\":55099,\"start\":55092},{\"end\":55111,\"start\":55106},{\"end\":55431,\"start\":55429},{\"end\":55451,\"start\":55443},{\"end\":55465,\"start\":55459},{\"end\":55478,\"start\":55475},{\"end\":55710,\"start\":55689},{\"end\":55727,\"start\":55720},{\"end\":55740,\"start\":55737},{\"end\":55752,\"start\":55747},{\"end\":55763,\"start\":55759},{\"end\":55777,\"start\":55771},{\"end\":55790,\"start\":55786},{\"end\":55808,\"start\":55802},{\"end\":55826,\"start\":55818},{\"end\":55836,\"start\":55828},{\"end\":56128,\"start\":56122},{\"end\":56144,\"start\":56138},{\"end\":56157,\"start\":56154},{\"end\":56169,\"start\":56165},{\"end\":56183,\"start\":56179},{\"end\":56197,\"start\":56190},{\"end\":56208,\"start\":56204},{\"end\":56224,\"start\":56215},{\"end\":56483,\"start\":56478},{\"end\":56496,\"start\":56490},{\"end\":56509,\"start\":56503},{\"end\":56525,\"start\":56518},{\"end\":56538,\"start\":56532},{\"end\":56553,\"start\":56546},{\"end\":56852,\"start\":56845},{\"end\":56866,\"start\":56862},{\"end\":56882,\"start\":56876},{\"end\":56891,\"start\":56889},{\"end\":56902,\"start\":56897},{\"end\":56918,\"start\":56912},{\"end\":56938,\"start\":56927},{\"end\":56961,\"start\":56956},{\"end\":57313,\"start\":57306},{\"end\":57332,\"start\":57323},{\"end\":57348,\"start\":57342},{\"end\":57363,\"start\":57358},{\"end\":57376,\"start\":57371},{\"end\":57653,\"start\":57647},{\"end\":57672,\"start\":57664},{\"end\":57687,\"start\":57681},{\"end\":57701,\"start\":57696},{\"end\":57717,\"start\":57710},{\"end\":57729,\"start\":57723},{\"end\":57745,\"start\":57736},{\"end\":57756,\"start\":57752},{\"end\":58068,\"start\":58062},{\"end\":58087,\"start\":58079},{\"end\":58100,\"start\":58094},{\"end\":58111,\"start\":58108},{\"end\":58122,\"start\":58118},{\"end\":58364,\"start\":58360},{\"end\":58375,\"start\":58373},{\"end\":58389,\"start\":58387},{\"end\":58401,\"start\":58398},{\"end\":58417,\"start\":58409},{\"end\":58427,\"start\":58424},{\"end\":58671,\"start\":58669},{\"end\":58679,\"start\":58676},{\"end\":58692,\"start\":58690},{\"end\":58709,\"start\":58704},{\"end\":58721,\"start\":58719},{\"end\":58732,\"start\":58727},{\"end\":58745,\"start\":58741},{\"end\":58757,\"start\":58755},{\"end\":58766,\"start\":58762},{\"end\":58776,\"start\":58773},{\"end\":59078,\"start\":59073},{\"end\":59089,\"start\":59087},{\"end\":59101,\"start\":59099},{\"end\":59115,\"start\":59111},{\"end\":59126,\"start\":59121},{\"end\":59139,\"start\":59135},{\"end\":59151,\"start\":59147},{\"end\":59165,\"start\":59162},{\"end\":59512,\"start\":59509},{\"end\":59523,\"start\":59519},{\"end\":59536,\"start\":59533},{\"end\":59554,\"start\":59547},{\"end\":59885,\"start\":59883},{\"end\":59896,\"start\":59894},{\"end\":59913,\"start\":59905},{\"end\":59925,\"start\":59922},{\"end\":60114,\"start\":60108},{\"end\":60232,\"start\":60229},{\"end\":60247,\"start\":60242},{\"end\":60263,\"start\":60255},{\"end\":60275,\"start\":60271},{\"end\":60290,\"start\":60284},{\"end\":60304,\"start\":60297},{\"end\":60318,\"start\":60312},{\"end\":60338,\"start\":60331},{\"end\":60633,\"start\":60626},{\"end\":60643,\"start\":60640},{\"end\":60657,\"start\":60652},{\"end\":60673,\"start\":60666},{\"end\":60685,\"start\":60681},{\"end\":60701,\"start\":60694},{\"end\":60717,\"start\":60713},{\"end\":60736,\"start\":60726},{\"end\":60747,\"start\":60745},{\"end\":60763,\"start\":60757},{\"end\":61111,\"start\":61104},{\"end\":61128,\"start\":61120},{\"end\":61141,\"start\":61137},{\"end\":61359,\"start\":61352},{\"end\":61373,\"start\":61369},{\"end\":61385,\"start\":61381},{\"end\":61404,\"start\":61398},{\"end\":61413,\"start\":61411},{\"end\":61429,\"start\":61421},{\"end\":61439,\"start\":61434},{\"end\":61461,\"start\":61454},{\"end\":61480,\"start\":61470},{\"end\":61496,\"start\":61491},{\"end\":61517,\"start\":61507},{\"end\":61533,\"start\":61528},{\"end\":61914,\"start\":61906},{\"end\":61927,\"start\":61925},{\"end\":61942,\"start\":61935},{\"end\":61956,\"start\":61951},{\"end\":61970,\"start\":61963},{\"end\":61985,\"start\":61980},{\"end\":61996,\"start\":61991},{\"end\":62269,\"start\":62263},{\"end\":62285,\"start\":62279},{\"end\":62304,\"start\":62295},{\"end\":62319,\"start\":62313},{\"end\":62334,\"start\":62330},{\"end\":62340,\"start\":62336},{\"end\":62529,\"start\":62527},{\"end\":62543,\"start\":62539},{\"end\":62556,\"start\":62551},{\"end\":62569,\"start\":62563},{\"end\":62751,\"start\":62749},{\"end\":62760,\"start\":62757},{\"end\":62774,\"start\":62770},{\"end\":62786,\"start\":62782},{\"end\":62794,\"start\":62792},{\"end\":62808,\"start\":62802},{\"end\":62823,\"start\":62819},{\"end\":63112,\"start\":63109},{\"end\":63121,\"start\":63117},{\"end\":63317,\"start\":63315},{\"end\":63332,\"start\":63327},{\"end\":63346,\"start\":63343},{\"end\":63356,\"start\":63353},{\"end\":63561,\"start\":63551},{\"end\":63577,\"start\":63568},{\"end\":63596,\"start\":63590},{\"end\":63912,\"start\":63901},{\"end\":63929,\"start\":63922},{\"end\":63942,\"start\":63938},{\"end\":64136,\"start\":64126},{\"end\":64156,\"start\":64145},{\"end\":64282,\"start\":64274},{\"end\":64292,\"start\":64286},{\"end\":64301,\"start\":64294},{\"end\":64553,\"start\":64546},{\"end\":64566,\"start\":64561},{\"end\":64578,\"start\":64574},{\"end\":64591,\"start\":64587},{\"end\":64605,\"start\":64601},{\"end\":64619,\"start\":64612},{\"end\":64632,\"start\":64627},{\"end\":64645,\"start\":64639},{\"end\":64657,\"start\":64654},{\"end\":64673,\"start\":64665},{\"end\":64921,\"start\":64911},{\"end\":64941,\"start\":64928},{\"end\":64954,\"start\":64949},{\"end\":64963,\"start\":64961},{\"end\":64983,\"start\":64971},{\"end\":64998,\"start\":64993},{\"end\":65015,\"start\":65006},{\"end\":65030,\"start\":65024},{\"end\":65263,\"start\":65257},{\"end\":65277,\"start\":65272},{\"end\":65292,\"start\":65285},{\"end\":65308,\"start\":65300},{\"end\":65325,\"start\":65317},{\"end\":65336,\"start\":65332},{\"end\":65559,\"start\":65552},{\"end\":65571,\"start\":65567},{\"end\":65580,\"start\":65573},{\"end\":65760,\"start\":65755},{\"end\":65775,\"start\":65768},{\"end\":65788,\"start\":65783},{\"end\":65982,\"start\":65978},{\"end\":65995,\"start\":65991},{\"end\":66007,\"start\":66003},{\"end\":66020,\"start\":66015},{\"end\":66032,\"start\":66028},{\"end\":66040,\"start\":66037},{\"end\":66053,\"start\":66050},{\"end\":66061,\"start\":66058},{\"end\":66072,\"start\":66068},{\"end\":66086,\"start\":66082},{\"end\":66372,\"start\":66367},{\"end\":66388,\"start\":66381},{\"end\":66614,\"start\":66603},{\"end\":66627,\"start\":66622},{\"end\":66649,\"start\":66639},{\"end\":66667,\"start\":66656},{\"end\":66681,\"start\":66677},{\"end\":66701,\"start\":66690},{\"end\":66719,\"start\":66711},{\"end\":66738,\"start\":66730},{\"end\":66753,\"start\":66746},{\"end\":66768,\"start\":66763},{\"end\":67104,\"start\":67096},{\"end\":67122,\"start\":67116},{\"end\":67289,\"start\":67287},{\"end\":67300,\"start\":67296},{\"end\":67315,\"start\":67309},{\"end\":67511,\"start\":67505},{\"end\":67526,\"start\":67519},{\"end\":67537,\"start\":67533},{\"end\":67551,\"start\":67546},{\"end\":67768,\"start\":67763},{\"end\":67796,\"start\":67788},{\"end\":68000,\"start\":67996},{\"end\":68015,\"start\":68010},{\"end\":68226,\"start\":68212},{\"end\":68238,\"start\":68233},{\"end\":68260,\"start\":68245},{\"end\":68275,\"start\":68268},{\"end\":68459,\"start\":68451},{\"end\":68472,\"start\":68466},{\"end\":68486,\"start\":68482},{\"end\":68497,\"start\":68494},{\"end\":68512,\"start\":68505},{\"end\":68529,\"start\":68520},{\"end\":68885,\"start\":68879},{\"end\":68899,\"start\":68891},{\"end\":68915,\"start\":68909},{\"end\":68936,\"start\":68923},{\"end\":68942,\"start\":68938},{\"end\":69113,\"start\":69107},{\"end\":69239,\"start\":69234},{\"end\":69257,\"start\":69249},{\"end\":69384,\"start\":69378},{\"end\":69393,\"start\":69386},{\"end\":69629,\"start\":69626},{\"end\":69640,\"start\":69637},{\"end\":69655,\"start\":69650},{\"end\":69667,\"start\":69664},{\"end\":69679,\"start\":69677},{\"end\":69693,\"start\":69690},{\"end\":69703,\"start\":69700},{\"end\":69718,\"start\":69711},{\"end\":69726,\"start\":69724},{\"end\":70056,\"start\":70054},{\"end\":70067,\"start\":70065},{\"end\":70082,\"start\":70077},{\"end\":70094,\"start\":70091},{\"end\":70382,\"start\":70379},{\"end\":70395,\"start\":70392},{\"end\":70406,\"start\":70402},{\"end\":70429,\"start\":70416},{\"end\":70440,\"start\":70438},{\"end\":70455,\"start\":70450},{\"end\":70463,\"start\":70461},{\"end\":70484,\"start\":70477},{\"end\":70501,\"start\":70493},{\"end\":70755,\"start\":70750},{\"end\":70769,\"start\":70764},{\"end\":70790,\"start\":70780},{\"end\":70802,\"start\":70796},{\"end\":70819,\"start\":70811},{\"end\":70831,\"start\":70826},{\"end\":70842,\"start\":70840},{\"end\":70860,\"start\":70852},{\"end\":70874,\"start\":70869},{\"end\":70884,\"start\":70880},{\"end\":70888,\"start\":70886},{\"end\":71224,\"start\":71221},{\"end\":71235,\"start\":71233},{\"end\":71250,\"start\":71245},{\"end\":71266,\"start\":71261},{\"end\":71282,\"start\":71277},{\"end\":71295,\"start\":71292},{\"end\":71315,\"start\":71306},{\"end\":71334,\"start\":71327},{\"end\":71601,\"start\":71595},{\"end\":71612,\"start\":71609},{\"end\":71632,\"start\":71622},{\"end\":71642,\"start\":71634},{\"end\":71773,\"start\":71771},{\"end\":71786,\"start\":71783},{\"end\":71795,\"start\":71792},{\"end\":71810,\"start\":71806},{\"end\":71825,\"start\":71821},{\"end\":72134,\"start\":72124},{\"end\":72144,\"start\":72141},{\"end\":72157,\"start\":72150},{\"end\":72173,\"start\":72166},{\"end\":72192,\"start\":72186},{\"end\":72451,\"start\":72447},{\"end\":72466,\"start\":72455},{\"end\":72480,\"start\":72476},{\"end\":72487,\"start\":72482},{\"end\":72496,\"start\":72491},{\"end\":72510,\"start\":72505},{\"end\":72529,\"start\":72519},{\"end\":72539,\"start\":72531},{\"end\":72823,\"start\":72821},{\"end\":72834,\"start\":72830},{\"end\":72846,\"start\":72842},{\"end\":72862,\"start\":72855},{\"end\":72879,\"start\":72872},{\"end\":73392,\"start\":73389},{\"end\":73401,\"start\":73396},{\"end\":73416,\"start\":73411},{\"end\":73430,\"start\":73425},{\"end\":73445,\"start\":73439},{\"end\":73466,\"start\":73456},{\"end\":73475,\"start\":73468},{\"end\":73749,\"start\":73745},{\"end\":73758,\"start\":73753},{\"end\":73774,\"start\":73769},{\"end\":73783,\"start\":73776},{\"end\":74045,\"start\":74042},{\"end\":74058,\"start\":74055},{\"end\":74074,\"start\":74067},{\"end\":74449,\"start\":74442},{\"end\":74467,\"start\":74461},{\"end\":74480,\"start\":74474},{\"end\":74495,\"start\":74490},{\"end\":74701,\"start\":74697},{\"end\":74714,\"start\":74707},{\"end\":74732,\"start\":74725},{\"end\":74747,\"start\":74742},{\"end\":74965,\"start\":74959},{\"end\":74975,\"start\":74972},{\"end\":74998,\"start\":74980},{\"end\":75012,\"start\":75008},{\"end\":75025,\"start\":75020},{\"end\":75046,\"start\":75040},{\"end\":75057,\"start\":75048},{\"end\":75341,\"start\":75335},{\"end\":75356,\"start\":75351},{\"end\":75371,\"start\":75366},{\"end\":75389,\"start\":75381},{\"end\":75636,\"start\":75633},{\"end\":75646,\"start\":75642},{\"end\":75659,\"start\":75655},{\"end\":75673,\"start\":75670},{\"end\":75686,\"start\":75683},{\"end\":76102,\"start\":76097},{\"end\":76116,\"start\":76112},{\"end\":76135,\"start\":76127},{\"end\":76148,\"start\":76143},{\"end\":76523,\"start\":76519},{\"end\":76539,\"start\":76529},{\"end\":76558,\"start\":76552},{\"end\":76573,\"start\":76567},{\"end\":76584,\"start\":76579},{\"end\":76831,\"start\":76820},{\"end\":76848,\"start\":76840},{\"end\":76865,\"start\":76858},{\"end\":76883,\"start\":76875},{\"end\":76901,\"start\":76894},{\"end\":77150,\"start\":77147},{\"end\":77159,\"start\":77156},{\"end\":77172,\"start\":77168},{\"end\":77189,\"start\":77181},{\"end\":77203,\"start\":77199},{\"end\":77214,\"start\":77209},{\"end\":77229,\"start\":77224},{\"end\":77243,\"start\":77237},{\"end\":77256,\"start\":77253},{\"end\":77270,\"start\":77267},{\"end\":77800,\"start\":77797},{\"end\":77809,\"start\":77807},{\"end\":77820,\"start\":77815},{\"end\":77831,\"start\":77829},{\"end\":77854,\"start\":77841},{\"end\":77865,\"start\":77863},{\"end\":77880,\"start\":77875},{\"end\":77888,\"start\":77886},{\"end\":77909,\"start\":77902},{\"end\":77926,\"start\":77918},{\"end\":78238,\"start\":78232},{\"end\":78250,\"start\":78247},{\"end\":78269,\"start\":78261},{\"end\":78285,\"start\":78278},{\"end\":78296,\"start\":78292},{\"end\":78474,\"start\":78468},{\"end\":78486,\"start\":78483},{\"end\":78721,\"start\":78717},{\"end\":78731,\"start\":78729},{\"end\":78742,\"start\":78740},{\"end\":79085,\"start\":79076},{\"end\":79096,\"start\":79094},{\"end\":79114,\"start\":79107},{\"end\":79131,\"start\":79126},{\"end\":79154,\"start\":79141},{\"end\":79164,\"start\":79156},{\"end\":79441,\"start\":79438},{\"end\":79451,\"start\":79449},{\"end\":79477,\"start\":79459},{\"end\":79494,\"start\":79486},{\"end\":79509,\"start\":79501},{\"end\":79519,\"start\":79511},{\"end\":79763,\"start\":79759},{\"end\":79773,\"start\":79769},{\"end\":79785,\"start\":79782},{\"end\":79795,\"start\":79792},{\"end\":79806,\"start\":79804},{\"end\":79821,\"start\":79817},{\"end\":79835,\"start\":79830},{\"end\":79849,\"start\":79845},{\"end\":79858,\"start\":79855},{\"end\":79863,\"start\":79860},{\"end\":80079,\"start\":80070},{\"end\":80089,\"start\":80084},{\"end\":80101,\"start\":80098},{\"end\":80117,\"start\":80110},{\"end\":80126,\"start\":80122},{\"end\":80139,\"start\":80136},{\"end\":80151,\"start\":80146},{\"end\":80162,\"start\":80160},{\"end\":80176,\"start\":80172},{\"end\":80186,\"start\":80178},{\"end\":80528,\"start\":80520},{\"end\":80538,\"start\":80534},{\"end\":80545,\"start\":80543},{\"end\":80561,\"start\":80555},{\"end\":80573,\"start\":80569},{\"end\":80785,\"start\":80781},{\"end\":80794,\"start\":80791},{\"end\":80814,\"start\":80806},{\"end\":80828,\"start\":80822},{\"end\":80848,\"start\":80839},{\"end\":81098,\"start\":81091},{\"end\":81117,\"start\":81108},{\"end\":81131,\"start\":81126},{\"end\":81143,\"start\":81137},{\"end\":81159,\"start\":81154},{\"end\":81366,\"start\":81354},{\"end\":81572,\"start\":81569},{\"end\":81585,\"start\":81582},{\"end\":81596,\"start\":81593},{\"end\":81875,\"start\":81872},{\"end\":81891,\"start\":81887},{\"end\":81902,\"start\":81898},{\"end\":81917,\"start\":81913},{\"end\":81930,\"start\":81927},{\"end\":81942,\"start\":81940},{\"end\":82252,\"start\":82242},{\"end\":82262,\"start\":82256},{\"end\":82282,\"start\":82272},{\"end\":82301,\"start\":82295},{\"end\":82314,\"start\":82308},{\"end\":82331,\"start\":82320},{\"end\":82335,\"start\":82333},{\"end\":82596,\"start\":82594},{\"end\":82609,\"start\":82605},{\"end\":82622,\"start\":82616},{\"end\":82640,\"start\":82631},{\"end\":82652,\"start\":82650},{\"end\":82661,\"start\":82657},{\"end\":82674,\"start\":82670},{\"end\":82895,\"start\":82891},{\"end\":83110,\"start\":83107},{\"end\":83124,\"start\":83120},{\"end\":83139,\"start\":83135},{\"end\":83151,\"start\":83149},{\"end\":83164,\"start\":83161},{\"end\":83175,\"start\":83172},{\"end\":83188,\"start\":83185},{\"end\":83201,\"start\":83198},{\"end\":83217,\"start\":83214},{\"end\":83493,\"start\":83490},{\"end\":83505,\"start\":83502},{\"end\":83516,\"start\":83512},{\"end\":83529,\"start\":83526},{\"end\":83543,\"start\":83538},{\"end\":83558,\"start\":83554},{\"end\":83571,\"start\":83568},{\"end\":84111,\"start\":84105},{\"end\":84128,\"start\":84120},{\"end\":84148,\"start\":84137},{\"end\":84166,\"start\":84159},{\"end\":84183,\"start\":84173},{\"end\":89385,\"start\":89374},{\"end\":89392,\"start\":89387},{\"end\":89700,\"start\":89689},{\"end\":89707,\"start\":89702},{\"end\":90334,\"start\":90323},{\"end\":90341,\"start\":90336},{\"end\":94469,\"start\":94465},{\"end\":94479,\"start\":94473}]", "bib_entry": "[{\"attributes\":{\"doi\":\"arXiv:2305.02463\",\"id\":\"b0\"},\"end\":49821,\"start\":49592},{\"attributes\":{\"id\":\"b1\"},\"end\":50092,\"start\":49823},{\"attributes\":{\"id\":\"b2\"},\"end\":50317,\"start\":50094},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":216284316},\"end\":50625,\"start\":50319},{\"attributes\":{\"id\":\"b4\"},\"end\":50766,\"start\":50627},{\"attributes\":{\"doi\":\"arXiv:2004.07368\",\"id\":\"b5\"},\"end\":51121,\"start\":50768},{\"attributes\":{\"id\":\"b6\",\"matched_paper_id\":255198985},\"end\":51727,\"start\":51123},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":5550767},\"end\":52065,\"start\":51729},{\"attributes\":{\"id\":\"b8\"},\"end\":52470,\"start\":52067},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":237262876},\"end\":52763,\"start\":52472},{\"attributes\":{\"id\":\"b10\",\"matched_paper_id\":51876975},\"end\":53101,\"start\":52765},{\"attributes\":{\"id\":\"b11\",\"matched_paper_id\":231951742},\"end\":53678,\"start\":53103},{\"attributes\":{\"id\":\"b12\"},\"end\":54143,\"start\":53680},{\"attributes\":{\"id\":\"b13\"},\"end\":54645,\"start\":54145},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":257102924},\"end\":54892,\"start\":54647},{\"attributes\":{\"id\":\"b15\",\"matched_paper_id\":231591445},\"end\":55363,\"start\":54894},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":245424883},\"end\":55625,\"start\":55365},{\"attributes\":{\"id\":\"b17\"},\"end\":56077,\"start\":55627},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":232035663},\"end\":56401,\"start\":56079},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":247628171},\"end\":56735,\"start\":56403},{\"attributes\":{\"id\":\"b20\"},\"end\":57236,\"start\":56737},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":245335280},\"end\":57546,\"start\":57238},{\"attributes\":{\"id\":\"b22\"},\"end\":57987,\"start\":57548},{\"attributes\":{\"id\":\"b23\"},\"end\":58280,\"start\":57989},{\"attributes\":{\"id\":\"b24\"},\"end\":58595,\"start\":58282},{\"attributes\":{\"id\":\"b25\",\"matched_paper_id\":215754208},\"end\":58993,\"start\":58597},{\"attributes\":{\"id\":\"b26\",\"matched_paper_id\":235692795},\"end\":59367,\"start\":58995},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":199064226},\"end\":59771,\"start\":59369},{\"attributes\":{\"id\":\"b28\"},\"end\":60104,\"start\":59773},{\"attributes\":{\"id\":\"b29\"},\"end\":60175,\"start\":60106},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":14113767},\"end\":60527,\"start\":60177},{\"attributes\":{\"id\":\"b31\"},\"end\":61019,\"start\":60529},{\"attributes\":{\"id\":\"b32\"},\"end\":61274,\"start\":61021},{\"attributes\":{\"id\":\"b33\"},\"end\":61813,\"start\":61276},{\"attributes\":{\"id\":\"b34\",\"matched_paper_id\":3753452},\"end\":62205,\"start\":61815},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":206594923},\"end\":62500,\"start\":62207},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":4406645},\"end\":62664,\"start\":62502},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":3441497},\"end\":63008,\"start\":62666},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":3994012},\"end\":63259,\"start\":63010},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":206594692},\"end\":63479,\"start\":63261},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":195908774},\"end\":63829,\"start\":63481},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":3719281},\"end\":64095,\"start\":63831},{\"attributes\":{\"id\":\"b42\",\"matched_paper_id\":1915014},\"end\":64267,\"start\":64097},{\"attributes\":{\"id\":\"b43\"},\"end\":64496,\"start\":64269},{\"attributes\":{\"id\":\"b44\",\"matched_paper_id\":56517630},\"end\":64876,\"start\":64498},{\"attributes\":{\"id\":\"b45\"},\"end\":65195,\"start\":64878},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":209202273},\"end\":65503,\"start\":65197},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":20282961},\"end\":65688,\"start\":65505},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":229297973},\"end\":65918,\"start\":65690},{\"attributes\":{\"id\":\"b49\"},\"end\":66293,\"start\":65920},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":219573658},\"end\":66512,\"start\":66295},{\"attributes\":{\"id\":\"b51\"},\"end\":67038,\"start\":66514},{\"attributes\":{\"id\":\"b52\",\"matched_paper_id\":234357997},\"end\":67234,\"start\":67040},{\"attributes\":{\"id\":\"b53\",\"matched_paper_id\":219955663},\"end\":67431,\"start\":67236},{\"attributes\":{\"id\":\"b54\",\"matched_paper_id\":249240415},\"end\":67700,\"start\":67433},{\"attributes\":{\"id\":\"b55\",\"matched_paper_id\":231979499},\"end\":67919,\"start\":67702},{\"attributes\":{\"id\":\"b56\",\"matched_paper_id\":196470871},\"end\":68139,\"start\":67921},{\"attributes\":{\"id\":\"b57\",\"matched_paper_id\":14888175},\"end\":68437,\"start\":68141},{\"attributes\":{\"id\":\"b58\"},\"end\":68804,\"start\":68439},{\"attributes\":{\"id\":\"b59\"},\"end\":69103,\"start\":68806},{\"attributes\":{\"id\":\"b60\"},\"end\":69164,\"start\":69105},{\"attributes\":{\"id\":\"b61\"},\"end\":69372,\"start\":69166},{\"attributes\":{\"id\":\"b62\"},\"end\":69536,\"start\":69374},{\"attributes\":{\"id\":\"b63\"},\"end\":69939,\"start\":69538},{\"attributes\":{\"id\":\"b64\",\"matched_paper_id\":246411402},\"end\":70279,\"start\":69941},{\"attributes\":{\"id\":\"b65\"},\"end\":70746,\"start\":70281},{\"attributes\":{\"id\":\"b66\"},\"end\":71147,\"start\":70748},{\"attributes\":{\"id\":\"b67\",\"matched_paper_id\":4794860},\"end\":71547,\"start\":71149},{\"attributes\":{\"id\":\"b68\",\"matched_paper_id\":152322},\"end\":71762,\"start\":71549},{\"attributes\":{\"id\":\"b69\"},\"end\":72059,\"start\":71764},{\"attributes\":{\"id\":\"b70\"},\"end\":72357,\"start\":72061},{\"attributes\":{\"id\":\"b71\",\"matched_paper_id\":4707877},\"end\":72745,\"start\":72359},{\"attributes\":{\"id\":\"b72\",\"matched_paper_id\":250643915},\"end\":73317,\"start\":72747},{\"attributes\":{\"id\":\"b73\",\"matched_paper_id\":7684883},\"end\":73658,\"start\":73319},{\"attributes\":{\"id\":\"b74\",\"matched_paper_id\":209414687},\"end\":73933,\"start\":73660},{\"attributes\":{\"doi\":\"2835-8856\",\"id\":\"b75\",\"matched_paper_id\":255125289},\"end\":74385,\"start\":73935},{\"attributes\":{\"id\":\"b76\",\"matched_paper_id\":1169492},\"end\":74631,\"start\":74387},{\"attributes\":{\"id\":\"b77\",\"matched_paper_id\":227305513},\"end\":74894,\"start\":74633},{\"attributes\":{\"id\":\"b78\",\"matched_paper_id\":238408362},\"end\":75252,\"start\":74896},{\"attributes\":{\"id\":\"b79\",\"matched_paper_id\":247518895},\"end\":75556,\"start\":75254},{\"attributes\":{\"id\":\"b80\",\"matched_paper_id\":257687804},\"end\":76090,\"start\":75558},{\"attributes\":{\"doi\":\"arXiv:2301.11445\",\"id\":\"b81\"},\"end\":76453,\"start\":76092},{\"attributes\":{\"id\":\"b82\",\"matched_paper_id\":244799255},\"end\":76745,\"start\":76455},{\"attributes\":{\"id\":\"b83\"},\"end\":77080,\"start\":76747},{\"attributes\":{\"id\":\"b84\",\"matched_paper_id\":253708074},\"end\":77721,\"start\":77082},{\"attributes\":{\"id\":\"b85\"},\"end\":78154,\"start\":77723},{\"attributes\":{\"id\":\"b86\"},\"end\":78461,\"start\":78156},{\"attributes\":{\"id\":\"b87\"},\"end\":78643,\"start\":78463},{\"attributes\":{\"id\":\"b88\",\"matched_paper_id\":233182041},\"end\":79074,\"start\":78645},{\"attributes\":{\"id\":\"b89\"},\"end\":79382,\"start\":79076},{\"attributes\":{\"id\":\"b90\"},\"end\":79679,\"start\":79384},{\"attributes\":{\"id\":\"b91\"},\"end\":80068,\"start\":79681},{\"attributes\":{\"id\":\"b92\"},\"end\":80468,\"start\":80070},{\"attributes\":{\"id\":\"b93\",\"matched_paper_id\":127986954},\"end\":80704,\"start\":80470},{\"attributes\":{\"id\":\"b94\",\"matched_paper_id\":219964874},\"end\":81020,\"start\":80706},{\"attributes\":{\"id\":\"b95\",\"matched_paper_id\":206593880},\"end\":81324,\"start\":81022},{\"attributes\":{\"id\":\"b96\"},\"end\":81479,\"start\":81326},{\"attributes\":{\"id\":\"b97\",\"matched_paper_id\":231839613},\"end\":81743,\"start\":81481},{\"attributes\":{\"id\":\"b98\"},\"end\":82164,\"start\":81745},{\"attributes\":{\"id\":\"b99\",\"matched_paper_id\":213175590},\"end\":82533,\"start\":82166},{\"attributes\":{\"id\":\"b100\"},\"end\":82826,\"start\":82535},{\"attributes\":{\"id\":\"b101\"},\"end\":83019,\"start\":82828},{\"attributes\":{\"id\":\"b102\"},\"end\":83438,\"start\":83021},{\"attributes\":{\"id\":\"b103\"},\"end\":83752,\"start\":83440},{\"attributes\":{\"id\":\"b104\"},\"end\":84665,\"start\":83754},{\"attributes\":{\"id\":\"b105\"},\"end\":85755,\"start\":84667},{\"attributes\":{\"id\":\"b106\"},\"end\":86659,\"start\":85757},{\"attributes\":{\"id\":\"b107\"},\"end\":87769,\"start\":86661},{\"attributes\":{\"id\":\"b108\"},\"end\":89244,\"start\":87771},{\"attributes\":{\"id\":\"b109\"},\"end\":89541,\"start\":89246},{\"attributes\":{\"id\":\"b110\"},\"end\":90178,\"start\":89543},{\"attributes\":{\"id\":\"b111\"},\"end\":90599,\"start\":90180},{\"attributes\":{\"id\":\"b112\"},\"end\":91190,\"start\":90601},{\"attributes\":{\"id\":\"b113\"},\"end\":91603,\"start\":91192},{\"attributes\":{\"id\":\"b114\"},\"end\":92110,\"start\":91605},{\"attributes\":{\"id\":\"b115\"},\"end\":94461,\"start\":92112},{\"attributes\":{\"id\":\"b116\"},\"end\":94949,\"start\":94463},{\"attributes\":{\"id\":\"b117\"},\"end\":95302,\"start\":94951},{\"attributes\":{\"id\":\"b118\"},\"end\":95801,\"start\":95304}]", "bib_title": "[{\"end\":50371,\"start\":50319},{\"end\":51219,\"start\":51123},{\"end\":51767,\"start\":51729},{\"end\":52546,\"start\":52472},{\"end\":52862,\"start\":52765},{\"end\":53198,\"start\":53103},{\"end\":54707,\"start\":54647},{\"end\":54963,\"start\":54894},{\"end\":55420,\"start\":55365},{\"end\":56113,\"start\":56079},{\"end\":56471,\"start\":56403},{\"end\":56835,\"start\":56737},{\"end\":57298,\"start\":57238},{\"end\":58660,\"start\":58597},{\"end\":59061,\"start\":58995},{\"end\":59498,\"start\":59369},{\"end\":60218,\"start\":60177},{\"end\":61898,\"start\":61815},{\"end\":62259,\"start\":62207},{\"end\":62518,\"start\":62502},{\"end\":62739,\"start\":62666},{\"end\":63096,\"start\":63010},{\"end\":63305,\"start\":63261},{\"end\":63544,\"start\":63481},{\"end\":63894,\"start\":63831},{\"end\":64119,\"start\":64097},{\"end\":64538,\"start\":64498},{\"end\":65250,\"start\":65197},{\"end\":65544,\"start\":65505},{\"end\":65745,\"start\":65690},{\"end\":66359,\"start\":66295},{\"end\":67085,\"start\":67040},{\"end\":67276,\"start\":67236},{\"end\":67498,\"start\":67433},{\"end\":67751,\"start\":67702},{\"end\":67989,\"start\":67921},{\"end\":68203,\"start\":68141},{\"end\":70045,\"start\":69941},{\"end\":71210,\"start\":71149},{\"end\":71591,\"start\":71549},{\"end\":72439,\"start\":72359},{\"end\":72815,\"start\":72747},{\"end\":73380,\"start\":73319},{\"end\":73731,\"start\":73660},{\"end\":74033,\"start\":73935},{\"end\":74434,\"start\":74387},{\"end\":74688,\"start\":74633},{\"end\":74950,\"start\":74896},{\"end\":75324,\"start\":75254},{\"end\":75622,\"start\":75558},{\"end\":76512,\"start\":76455},{\"end\":77134,\"start\":77082},{\"end\":78709,\"start\":78645},{\"end\":80514,\"start\":80470},{\"end\":80770,\"start\":80706},{\"end\":81079,\"start\":81022},{\"end\":81560,\"start\":81481},{\"end\":82236,\"start\":82166},{\"end\":84858,\"start\":84667},{\"end\":86008,\"start\":85757},{\"end\":88277,\"start\":87771},{\"end\":90792,\"start\":90601},{\"end\":93025,\"start\":92112}]", "bib_author": "[{\"end\":49658,\"start\":49646},{\"end\":49671,\"start\":49658},{\"end\":49837,\"start\":49823},{\"end\":49851,\"start\":49837},{\"end\":49862,\"start\":49851},{\"end\":49873,\"start\":49862},{\"end\":49886,\"start\":49873},{\"end\":50149,\"start\":50138},{\"end\":50160,\"start\":50149},{\"end\":50179,\"start\":50160},{\"end\":50195,\"start\":50179},{\"end\":50388,\"start\":50373},{\"end\":50396,\"start\":50388},{\"end\":50400,\"start\":50396},{\"end\":50687,\"start\":50674},{\"end\":50782,\"start\":50768},{\"end\":50793,\"start\":50782},{\"end\":50809,\"start\":50793},{\"end\":50830,\"start\":50809},{\"end\":50841,\"start\":50830},{\"end\":51234,\"start\":51221},{\"end\":51248,\"start\":51234},{\"end\":51261,\"start\":51248},{\"end\":51273,\"start\":51261},{\"end\":51293,\"start\":51273},{\"end\":51316,\"start\":51293},{\"end\":51327,\"start\":51316},{\"end\":51343,\"start\":51327},{\"end\":51363,\"start\":51343},{\"end\":51376,\"start\":51363},{\"end\":51789,\"start\":51769},{\"end\":51801,\"start\":51789},{\"end\":51819,\"start\":51801},{\"end\":51834,\"start\":51819},{\"end\":51850,\"start\":51834},{\"end\":52127,\"start\":52114},{\"end\":52143,\"start\":52127},{\"end\":52159,\"start\":52143},{\"end\":52171,\"start\":52159},{\"end\":52185,\"start\":52171},{\"end\":52201,\"start\":52185},{\"end\":52217,\"start\":52201},{\"end\":52231,\"start\":52217},{\"end\":52251,\"start\":52231},{\"end\":52264,\"start\":52251},{\"end\":52561,\"start\":52548},{\"end\":52574,\"start\":52561},{\"end\":52588,\"start\":52574},{\"end\":52604,\"start\":52588},{\"end\":52879,\"start\":52864},{\"end\":52889,\"start\":52879},{\"end\":52908,\"start\":52889},{\"end\":52922,\"start\":52908},{\"end\":53220,\"start\":53200},{\"end\":53235,\"start\":53220},{\"end\":53245,\"start\":53235},{\"end\":53259,\"start\":53245},{\"end\":53773,\"start\":53752},{\"end\":53788,\"start\":53773},{\"end\":53805,\"start\":53788},{\"end\":53825,\"start\":53805},{\"end\":53841,\"start\":53825},{\"end\":53855,\"start\":53841},{\"end\":53869,\"start\":53855},{\"end\":53883,\"start\":53869},{\"end\":53901,\"start\":53883},{\"end\":54252,\"start\":54231},{\"end\":54269,\"start\":54252},{\"end\":54284,\"start\":54269},{\"end\":54297,\"start\":54284},{\"end\":54312,\"start\":54297},{\"end\":54326,\"start\":54312},{\"end\":54340,\"start\":54326},{\"end\":54354,\"start\":54340},{\"end\":54370,\"start\":54354},{\"end\":54389,\"start\":54370},{\"end\":54728,\"start\":54709},{\"end\":54741,\"start\":54728},{\"end\":54757,\"start\":54741},{\"end\":54979,\"start\":54965},{\"end\":54994,\"start\":54979},{\"end\":55009,\"start\":54994},{\"end\":55024,\"start\":55009},{\"end\":55037,\"start\":55024},{\"end\":55055,\"start\":55037},{\"end\":55070,\"start\":55055},{\"end\":55085,\"start\":55070},{\"end\":55101,\"start\":55085},{\"end\":55113,\"start\":55101},{\"end\":55433,\"start\":55422},{\"end\":55453,\"start\":55433},{\"end\":55467,\"start\":55453},{\"end\":55480,\"start\":55467},{\"end\":55712,\"start\":55684},{\"end\":55729,\"start\":55712},{\"end\":55742,\"start\":55729},{\"end\":55754,\"start\":55742},{\"end\":55765,\"start\":55754},{\"end\":55779,\"start\":55765},{\"end\":55792,\"start\":55779},{\"end\":55810,\"start\":55792},{\"end\":55828,\"start\":55810},{\"end\":55838,\"start\":55828},{\"end\":56130,\"start\":56115},{\"end\":56146,\"start\":56130},{\"end\":56159,\"start\":56146},{\"end\":56171,\"start\":56159},{\"end\":56185,\"start\":56171},{\"end\":56199,\"start\":56185},{\"end\":56210,\"start\":56199},{\"end\":56226,\"start\":56210},{\"end\":56485,\"start\":56473},{\"end\":56498,\"start\":56485},{\"end\":56511,\"start\":56498},{\"end\":56527,\"start\":56511},{\"end\":56540,\"start\":56527},{\"end\":56555,\"start\":56540},{\"end\":56854,\"start\":56837},{\"end\":56868,\"start\":56854},{\"end\":56884,\"start\":56868},{\"end\":56893,\"start\":56884},{\"end\":56904,\"start\":56893},{\"end\":56920,\"start\":56904},{\"end\":56940,\"start\":56920},{\"end\":56963,\"start\":56940},{\"end\":57315,\"start\":57300},{\"end\":57334,\"start\":57315},{\"end\":57350,\"start\":57334},{\"end\":57365,\"start\":57350},{\"end\":57378,\"start\":57365},{\"end\":57655,\"start\":57642},{\"end\":57674,\"start\":57655},{\"end\":57689,\"start\":57674},{\"end\":57703,\"start\":57689},{\"end\":57719,\"start\":57703},{\"end\":57731,\"start\":57719},{\"end\":57747,\"start\":57731},{\"end\":57758,\"start\":57747},{\"end\":58070,\"start\":58055},{\"end\":58089,\"start\":58070},{\"end\":58102,\"start\":58089},{\"end\":58113,\"start\":58102},{\"end\":58124,\"start\":58113},{\"end\":58366,\"start\":58354},{\"end\":58377,\"start\":58366},{\"end\":58391,\"start\":58377},{\"end\":58403,\"start\":58391},{\"end\":58419,\"start\":58403},{\"end\":58429,\"start\":58419},{\"end\":58673,\"start\":58662},{\"end\":58681,\"start\":58673},{\"end\":58694,\"start\":58681},{\"end\":58711,\"start\":58694},{\"end\":58723,\"start\":58711},{\"end\":58734,\"start\":58723},{\"end\":58747,\"start\":58734},{\"end\":58759,\"start\":58747},{\"end\":58768,\"start\":58759},{\"end\":58778,\"start\":58768},{\"end\":59080,\"start\":59063},{\"end\":59091,\"start\":59080},{\"end\":59103,\"start\":59091},{\"end\":59117,\"start\":59103},{\"end\":59128,\"start\":59117},{\"end\":59141,\"start\":59128},{\"end\":59153,\"start\":59141},{\"end\":59167,\"start\":59153},{\"end\":59514,\"start\":59500},{\"end\":59525,\"start\":59514},{\"end\":59538,\"start\":59525},{\"end\":59556,\"start\":59538},{\"end\":59887,\"start\":59876},{\"end\":59898,\"start\":59887},{\"end\":59915,\"start\":59898},{\"end\":59927,\"start\":59915},{\"end\":60116,\"start\":60108},{\"end\":60234,\"start\":60220},{\"end\":60249,\"start\":60234},{\"end\":60265,\"start\":60249},{\"end\":60277,\"start\":60265},{\"end\":60292,\"start\":60277},{\"end\":60306,\"start\":60292},{\"end\":60320,\"start\":60306},{\"end\":60340,\"start\":60320},{\"end\":60635,\"start\":60619},{\"end\":60645,\"start\":60635},{\"end\":60659,\"start\":60645},{\"end\":60675,\"start\":60659},{\"end\":60687,\"start\":60675},{\"end\":60703,\"start\":60687},{\"end\":60719,\"start\":60703},{\"end\":60738,\"start\":60719},{\"end\":60749,\"start\":60738},{\"end\":60765,\"start\":60749},{\"end\":61113,\"start\":61096},{\"end\":61130,\"start\":61113},{\"end\":61143,\"start\":61130},{\"end\":61361,\"start\":61344},{\"end\":61375,\"start\":61361},{\"end\":61387,\"start\":61375},{\"end\":61406,\"start\":61387},{\"end\":61415,\"start\":61406},{\"end\":61431,\"start\":61415},{\"end\":61441,\"start\":61431},{\"end\":61463,\"start\":61441},{\"end\":61482,\"start\":61463},{\"end\":61498,\"start\":61482},{\"end\":61519,\"start\":61498},{\"end\":61535,\"start\":61519},{\"end\":61916,\"start\":61900},{\"end\":61929,\"start\":61916},{\"end\":61944,\"start\":61929},{\"end\":61958,\"start\":61944},{\"end\":61972,\"start\":61958},{\"end\":61987,\"start\":61972},{\"end\":61998,\"start\":61987},{\"end\":62271,\"start\":62261},{\"end\":62287,\"start\":62271},{\"end\":62306,\"start\":62287},{\"end\":62321,\"start\":62306},{\"end\":62336,\"start\":62321},{\"end\":62342,\"start\":62336},{\"end\":62531,\"start\":62520},{\"end\":62545,\"start\":62531},{\"end\":62558,\"start\":62545},{\"end\":62571,\"start\":62558},{\"end\":62753,\"start\":62741},{\"end\":62762,\"start\":62753},{\"end\":62776,\"start\":62762},{\"end\":62788,\"start\":62776},{\"end\":62796,\"start\":62788},{\"end\":62810,\"start\":62796},{\"end\":62825,\"start\":62810},{\"end\":63114,\"start\":63098},{\"end\":63123,\"start\":63114},{\"end\":63319,\"start\":63307},{\"end\":63334,\"start\":63319},{\"end\":63348,\"start\":63334},{\"end\":63358,\"start\":63348},{\"end\":63563,\"start\":63546},{\"end\":63579,\"start\":63563},{\"end\":63598,\"start\":63579},{\"end\":63914,\"start\":63896},{\"end\":63931,\"start\":63914},{\"end\":63944,\"start\":63931},{\"end\":64138,\"start\":64121},{\"end\":64158,\"start\":64138},{\"end\":64284,\"start\":64269},{\"end\":64294,\"start\":64284},{\"end\":64303,\"start\":64294},{\"end\":64555,\"start\":64540},{\"end\":64568,\"start\":64555},{\"end\":64580,\"start\":64568},{\"end\":64593,\"start\":64580},{\"end\":64607,\"start\":64593},{\"end\":64621,\"start\":64607},{\"end\":64634,\"start\":64621},{\"end\":64647,\"start\":64634},{\"end\":64659,\"start\":64647},{\"end\":64675,\"start\":64659},{\"end\":64923,\"start\":64907},{\"end\":64943,\"start\":64923},{\"end\":64956,\"start\":64943},{\"end\":64965,\"start\":64956},{\"end\":64985,\"start\":64965},{\"end\":65000,\"start\":64985},{\"end\":65017,\"start\":65000},{\"end\":65032,\"start\":65017},{\"end\":65265,\"start\":65252},{\"end\":65279,\"start\":65265},{\"end\":65294,\"start\":65279},{\"end\":65310,\"start\":65294},{\"end\":65327,\"start\":65310},{\"end\":65338,\"start\":65327},{\"end\":65561,\"start\":65546},{\"end\":65573,\"start\":65561},{\"end\":65582,\"start\":65573},{\"end\":65762,\"start\":65747},{\"end\":65777,\"start\":65762},{\"end\":65790,\"start\":65777},{\"end\":65984,\"start\":65973},{\"end\":65997,\"start\":65984},{\"end\":66009,\"start\":65997},{\"end\":66022,\"start\":66009},{\"end\":66034,\"start\":66022},{\"end\":66042,\"start\":66034},{\"end\":66055,\"start\":66042},{\"end\":66063,\"start\":66055},{\"end\":66074,\"start\":66063},{\"end\":66088,\"start\":66074},{\"end\":66374,\"start\":66361},{\"end\":66390,\"start\":66374},{\"end\":66616,\"start\":66596},{\"end\":66629,\"start\":66616},{\"end\":66651,\"start\":66629},{\"end\":66669,\"start\":66651},{\"end\":66683,\"start\":66669},{\"end\":66703,\"start\":66683},{\"end\":66721,\"start\":66703},{\"end\":66740,\"start\":66721},{\"end\":66755,\"start\":66740},{\"end\":66770,\"start\":66755},{\"end\":67106,\"start\":67087},{\"end\":67124,\"start\":67106},{\"end\":67291,\"start\":67278},{\"end\":67302,\"start\":67291},{\"end\":67317,\"start\":67302},{\"end\":67513,\"start\":67500},{\"end\":67528,\"start\":67513},{\"end\":67539,\"start\":67528},{\"end\":67553,\"start\":67539},{\"end\":67770,\"start\":67753},{\"end\":67779,\"start\":67770},{\"end\":67798,\"start\":67779},{\"end\":68002,\"start\":67991},{\"end\":68017,\"start\":68002},{\"end\":68228,\"start\":68205},{\"end\":68240,\"start\":68228},{\"end\":68262,\"start\":68240},{\"end\":68277,\"start\":68262},{\"end\":68461,\"start\":68441},{\"end\":68474,\"start\":68461},{\"end\":68488,\"start\":68474},{\"end\":68499,\"start\":68488},{\"end\":68514,\"start\":68499},{\"end\":68531,\"start\":68514},{\"end\":68887,\"start\":68874},{\"end\":68901,\"start\":68887},{\"end\":68917,\"start\":68901},{\"end\":68938,\"start\":68917},{\"end\":68944,\"start\":68938},{\"end\":69115,\"start\":69107},{\"end\":69241,\"start\":69228},{\"end\":69259,\"start\":69241},{\"end\":69386,\"start\":69374},{\"end\":69395,\"start\":69386},{\"end\":69631,\"start\":69618},{\"end\":69642,\"start\":69631},{\"end\":69657,\"start\":69642},{\"end\":69669,\"start\":69657},{\"end\":69681,\"start\":69669},{\"end\":69695,\"start\":69681},{\"end\":69705,\"start\":69695},{\"end\":69720,\"start\":69705},{\"end\":69728,\"start\":69720},{\"end\":70058,\"start\":70047},{\"end\":70069,\"start\":70058},{\"end\":70084,\"start\":70069},{\"end\":70096,\"start\":70084},{\"end\":70384,\"start\":70376},{\"end\":70397,\"start\":70384},{\"end\":70408,\"start\":70397},{\"end\":70431,\"start\":70408},{\"end\":70442,\"start\":70431},{\"end\":70457,\"start\":70442},{\"end\":70465,\"start\":70457},{\"end\":70486,\"start\":70465},{\"end\":70503,\"start\":70486},{\"end\":70757,\"start\":70748},{\"end\":70771,\"start\":70757},{\"end\":70792,\"start\":70771},{\"end\":70804,\"start\":70792},{\"end\":70821,\"start\":70804},{\"end\":70833,\"start\":70821},{\"end\":70844,\"start\":70833},{\"end\":70862,\"start\":70844},{\"end\":70876,\"start\":70862},{\"end\":70886,\"start\":70876},{\"end\":70890,\"start\":70886},{\"end\":71226,\"start\":71212},{\"end\":71237,\"start\":71226},{\"end\":71252,\"start\":71237},{\"end\":71268,\"start\":71252},{\"end\":71284,\"start\":71268},{\"end\":71297,\"start\":71284},{\"end\":71317,\"start\":71297},{\"end\":71336,\"start\":71317},{\"end\":71603,\"start\":71593},{\"end\":71614,\"start\":71603},{\"end\":71634,\"start\":71614},{\"end\":71644,\"start\":71634},{\"end\":71775,\"start\":71766},{\"end\":71788,\"start\":71775},{\"end\":71797,\"start\":71788},{\"end\":71812,\"start\":71797},{\"end\":71827,\"start\":71812},{\"end\":72136,\"start\":72118},{\"end\":72146,\"start\":72136},{\"end\":72159,\"start\":72146},{\"end\":72175,\"start\":72159},{\"end\":72194,\"start\":72175},{\"end\":72453,\"start\":72441},{\"end\":72468,\"start\":72453},{\"end\":72482,\"start\":72468},{\"end\":72489,\"start\":72482},{\"end\":72498,\"start\":72489},{\"end\":72512,\"start\":72498},{\"end\":72531,\"start\":72512},{\"end\":72541,\"start\":72531},{\"end\":72825,\"start\":72817},{\"end\":72836,\"start\":72825},{\"end\":72848,\"start\":72836},{\"end\":72864,\"start\":72848},{\"end\":72881,\"start\":72864},{\"end\":73394,\"start\":73382},{\"end\":73403,\"start\":73394},{\"end\":73418,\"start\":73403},{\"end\":73432,\"start\":73418},{\"end\":73447,\"start\":73432},{\"end\":73468,\"start\":73447},{\"end\":73477,\"start\":73468},{\"end\":73751,\"start\":73733},{\"end\":73760,\"start\":73751},{\"end\":73776,\"start\":73760},{\"end\":73785,\"start\":73776},{\"end\":74047,\"start\":74035},{\"end\":74060,\"start\":74047},{\"end\":74076,\"start\":74060},{\"end\":74451,\"start\":74436},{\"end\":74469,\"start\":74451},{\"end\":74482,\"start\":74469},{\"end\":74497,\"start\":74482},{\"end\":74703,\"start\":74690},{\"end\":74716,\"start\":74703},{\"end\":74734,\"start\":74716},{\"end\":74749,\"start\":74734},{\"end\":74967,\"start\":74952},{\"end\":74977,\"start\":74967},{\"end\":75000,\"start\":74977},{\"end\":75014,\"start\":75000},{\"end\":75027,\"start\":75014},{\"end\":75048,\"start\":75027},{\"end\":75059,\"start\":75048},{\"end\":75343,\"start\":75326},{\"end\":75358,\"start\":75343},{\"end\":75373,\"start\":75358},{\"end\":75391,\"start\":75373},{\"end\":75638,\"start\":75624},{\"end\":75648,\"start\":75638},{\"end\":75661,\"start\":75648},{\"end\":75675,\"start\":75661},{\"end\":75688,\"start\":75675},{\"end\":76104,\"start\":76092},{\"end\":76118,\"start\":76104},{\"end\":76137,\"start\":76118},{\"end\":76150,\"start\":76137},{\"end\":76525,\"start\":76514},{\"end\":76541,\"start\":76525},{\"end\":76560,\"start\":76541},{\"end\":76575,\"start\":76560},{\"end\":76586,\"start\":76575},{\"end\":76833,\"start\":76810},{\"end\":76850,\"start\":76833},{\"end\":76867,\"start\":76850},{\"end\":76885,\"start\":76867},{\"end\":76903,\"start\":76885},{\"end\":77152,\"start\":77136},{\"end\":77161,\"start\":77152},{\"end\":77174,\"start\":77161},{\"end\":77191,\"start\":77174},{\"end\":77205,\"start\":77191},{\"end\":77216,\"start\":77205},{\"end\":77231,\"start\":77216},{\"end\":77245,\"start\":77231},{\"end\":77258,\"start\":77245},{\"end\":77272,\"start\":77258},{\"end\":77802,\"start\":77794},{\"end\":77811,\"start\":77802},{\"end\":77822,\"start\":77811},{\"end\":77833,\"start\":77822},{\"end\":77856,\"start\":77833},{\"end\":77867,\"start\":77856},{\"end\":77882,\"start\":77867},{\"end\":77890,\"start\":77882},{\"end\":77911,\"start\":77890},{\"end\":77928,\"start\":77911},{\"end\":78240,\"start\":78227},{\"end\":78252,\"start\":78240},{\"end\":78271,\"start\":78252},{\"end\":78287,\"start\":78271},{\"end\":78298,\"start\":78287},{\"end\":78476,\"start\":78463},{\"end\":78488,\"start\":78476},{\"end\":78723,\"start\":78711},{\"end\":78733,\"start\":78723},{\"end\":78744,\"start\":78733},{\"end\":79087,\"start\":79076},{\"end\":79098,\"start\":79087},{\"end\":79116,\"start\":79098},{\"end\":79133,\"start\":79116},{\"end\":79156,\"start\":79133},{\"end\":79166,\"start\":79156},{\"end\":79443,\"start\":79431},{\"end\":79453,\"start\":79443},{\"end\":79479,\"start\":79453},{\"end\":79496,\"start\":79479},{\"end\":79511,\"start\":79496},{\"end\":79521,\"start\":79511},{\"end\":79765,\"start\":79753},{\"end\":79775,\"start\":79765},{\"end\":79787,\"start\":79775},{\"end\":79797,\"start\":79787},{\"end\":79808,\"start\":79797},{\"end\":79823,\"start\":79808},{\"end\":79837,\"start\":79823},{\"end\":79851,\"start\":79837},{\"end\":79860,\"start\":79851},{\"end\":79865,\"start\":79860},{\"end\":80081,\"start\":80070},{\"end\":80091,\"start\":80081},{\"end\":80103,\"start\":80091},{\"end\":80119,\"start\":80103},{\"end\":80128,\"start\":80119},{\"end\":80141,\"start\":80128},{\"end\":80153,\"start\":80141},{\"end\":80164,\"start\":80153},{\"end\":80178,\"start\":80164},{\"end\":80188,\"start\":80178},{\"end\":80530,\"start\":80516},{\"end\":80540,\"start\":80530},{\"end\":80547,\"start\":80540},{\"end\":80563,\"start\":80547},{\"end\":80575,\"start\":80563},{\"end\":80787,\"start\":80772},{\"end\":80796,\"start\":80787},{\"end\":80816,\"start\":80796},{\"end\":80830,\"start\":80816},{\"end\":80850,\"start\":80830},{\"end\":81100,\"start\":81081},{\"end\":81119,\"start\":81100},{\"end\":81133,\"start\":81119},{\"end\":81145,\"start\":81133},{\"end\":81161,\"start\":81145},{\"end\":81368,\"start\":81354},{\"end\":81574,\"start\":81562},{\"end\":81587,\"start\":81574},{\"end\":81598,\"start\":81587},{\"end\":81877,\"start\":81868},{\"end\":81893,\"start\":81877},{\"end\":81904,\"start\":81893},{\"end\":81919,\"start\":81904},{\"end\":81932,\"start\":81919},{\"end\":81944,\"start\":81932},{\"end\":82254,\"start\":82238},{\"end\":82264,\"start\":82254},{\"end\":82284,\"start\":82264},{\"end\":82303,\"start\":82284},{\"end\":82316,\"start\":82303},{\"end\":82333,\"start\":82316},{\"end\":82337,\"start\":82333},{\"end\":82598,\"start\":82587},{\"end\":82611,\"start\":82598},{\"end\":82624,\"start\":82611},{\"end\":82642,\"start\":82624},{\"end\":82654,\"start\":82642},{\"end\":82663,\"start\":82654},{\"end\":82676,\"start\":82663},{\"end\":82897,\"start\":82882},{\"end\":83112,\"start\":83098},{\"end\":83126,\"start\":83112},{\"end\":83141,\"start\":83126},{\"end\":83153,\"start\":83141},{\"end\":83166,\"start\":83153},{\"end\":83177,\"start\":83166},{\"end\":83190,\"start\":83177},{\"end\":83203,\"start\":83190},{\"end\":83219,\"start\":83203},{\"end\":83495,\"start\":83481},{\"end\":83507,\"start\":83495},{\"end\":83518,\"start\":83507},{\"end\":83531,\"start\":83518},{\"end\":83545,\"start\":83531},{\"end\":83560,\"start\":83545},{\"end\":83573,\"start\":83560},{\"end\":84113,\"start\":84098},{\"end\":84130,\"start\":84113},{\"end\":84150,\"start\":84130},{\"end\":84168,\"start\":84150},{\"end\":84185,\"start\":84168},{\"end\":89387,\"start\":89374},{\"end\":89394,\"start\":89387},{\"end\":89702,\"start\":89689},{\"end\":89709,\"start\":89702},{\"end\":90336,\"start\":90323},{\"end\":90343,\"start\":90336},{\"end\":94471,\"start\":94463},{\"end\":94481,\"start\":94471}]", "bib_venue": "[{\"end\":53408,\"start\":53342},{\"end\":75837,\"start\":75771},{\"end\":77421,\"start\":77355},{\"end\":78873,\"start\":78817},{\"end\":49644,\"start\":49592},{\"end\":49947,\"start\":49886},{\"end\":50136,\"start\":50094},{\"end\":50465,\"start\":50400},{\"end\":50672,\"start\":50627},{\"end\":50923,\"start\":50857},{\"end\":51404,\"start\":51376},{\"end\":51878,\"start\":51850},{\"end\":52112,\"start\":52067},{\"end\":52611,\"start\":52604},{\"end\":52925,\"start\":52922},{\"end\":53340,\"start\":53259},{\"end\":53750,\"start\":53680},{\"end\":54229,\"start\":54145},{\"end\":54761,\"start\":54757},{\"end\":55117,\"start\":55113},{\"end\":55484,\"start\":55480},{\"end\":55682,\"start\":55627},{\"end\":56230,\"start\":56226},{\"end\":56559,\"start\":56555},{\"end\":56981,\"start\":56963},{\"end\":57382,\"start\":57378},{\"end\":57640,\"start\":57548},{\"end\":58053,\"start\":57989},{\"end\":58352,\"start\":58282},{\"end\":58782,\"start\":58778},{\"end\":59171,\"start\":59167},{\"end\":59562,\"start\":59556},{\"end\":59874,\"start\":59773},{\"end\":60344,\"start\":60340},{\"end\":60617,\"start\":60529},{\"end\":61094,\"start\":61021},{\"end\":61342,\"start\":61276},{\"end\":62002,\"start\":61998},{\"end\":62346,\"start\":62342},{\"end\":62575,\"start\":62571},{\"end\":62829,\"start\":62825},{\"end\":63130,\"start\":63123},{\"end\":63362,\"start\":63358},{\"end\":63647,\"start\":63598},{\"end\":63955,\"start\":63944},{\"end\":64176,\"start\":64158},{\"end\":64377,\"start\":64303},{\"end\":64679,\"start\":64675},{\"end\":64905,\"start\":64878},{\"end\":65342,\"start\":65338},{\"end\":65589,\"start\":65582},{\"end\":65794,\"start\":65790},{\"end\":65971,\"start\":65920},{\"end\":66394,\"start\":66390},{\"end\":66594,\"start\":66514},{\"end\":67131,\"start\":67124},{\"end\":67324,\"start\":67317},{\"end\":67560,\"start\":67553},{\"end\":67802,\"start\":67798},{\"end\":68024,\"start\":68017},{\"end\":68281,\"start\":68277},{\"end\":68872,\"start\":68806},{\"end\":69226,\"start\":69166},{\"end\":69416,\"start\":69395},{\"end\":69616,\"start\":69538},{\"end\":70100,\"start\":70096},{\"end\":70374,\"start\":70281},{\"end\":70929,\"start\":70890},{\"end\":71340,\"start\":71336},{\"end\":71648,\"start\":71644},{\"end\":72116,\"start\":72061},{\"end\":72545,\"start\":72541},{\"end\":72930,\"start\":72881},{\"end\":73481,\"start\":73477},{\"end\":73789,\"start\":73785},{\"end\":74126,\"start\":74085},{\"end\":74501,\"start\":74497},{\"end\":74753,\"start\":74749},{\"end\":75063,\"start\":75059},{\"end\":75395,\"start\":75391},{\"end\":75769,\"start\":75688},{\"end\":76241,\"start\":76166},{\"end\":76590,\"start\":76586},{\"end\":76808,\"start\":76747},{\"end\":77353,\"start\":77272},{\"end\":77792,\"start\":77723},{\"end\":78225,\"start\":78156},{\"end\":78547,\"start\":78488},{\"end\":78815,\"start\":78744},{\"end\":79223,\"start\":79166},{\"end\":79429,\"start\":79384},{\"end\":79751,\"start\":79681},{\"end\":80259,\"start\":80188},{\"end\":80579,\"start\":80575},{\"end\":80854,\"start\":80850},{\"end\":81165,\"start\":81161},{\"end\":81352,\"start\":81326},{\"end\":81602,\"start\":81598},{\"end\":81866,\"start\":81745},{\"end\":82341,\"start\":82337},{\"end\":82585,\"start\":82535},{\"end\":82880,\"start\":82828},{\"end\":83096,\"start\":83021},{\"end\":83479,\"start\":83440},{\"end\":84096,\"start\":83754},{\"end\":84985,\"start\":84860},{\"end\":86070,\"start\":86010},{\"end\":87170,\"start\":86661},{\"end\":88372,\"start\":88279},{\"end\":89372,\"start\":89246},{\"end\":89687,\"start\":89543},{\"end\":90321,\"start\":90180},{\"end\":90848,\"start\":90794},{\"end\":91318,\"start\":91192},{\"end\":91856,\"start\":91605},{\"end\":93190,\"start\":93027},{\"end\":94680,\"start\":94481},{\"end\":95085,\"start\":94951},{\"end\":95551,\"start\":95304}]"}}}, "year": 2023, "month": 12, "day": 17}