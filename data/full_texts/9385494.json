{"id": 9385494, "updated": "2023-10-01 08:44:34.898", "metadata": {"title": "Neural Paraphrase Generation with Stacked Residual LSTM Networks", "authors": "[{\"first\":\"Aaditya\",\"last\":\"Prakash\",\"middle\":[]},{\"first\":\"Sadid A.\",\"last\":\"Hasan\",\"middle\":[]},{\"first\":\"Kathy\",\"last\":\"Lee\",\"middle\":[]},{\"first\":\"Vivek\",\"last\":\"Datla\",\"middle\":[]},{\"first\":\"Ashequl\",\"last\":\"Qadir\",\"middle\":[]},{\"first\":\"Joey\",\"last\":\"Liu\",\"middle\":[]},{\"first\":\"Oladimeji\",\"last\":\"Farri\",\"middle\":[]}]", "venue": "COLING", "journal": "Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers", "publication_date": {"year": 2016, "month": null, "day": null}, "abstract": "In this paper, we propose a novel neural approach for paraphrase generation. Conventional paraphrase generation methods either leverage hand-written rules and thesauri-based alignments, or use statistical machine learning principles. To the best of our knowledge, this work is the first to explore deep learning models for paraphrase generation. Our primary contribution is a stacked residual LSTM network, where we add residual connections between LSTM layers. This allows for efficient training of deep LSTMs. We evaluate our model and other state-of-the-art deep learning models on three different datasets: PPDB, WikiAnswers, and MSCOCO. Evaluation results demonstrate that our model outperforms sequence to sequence, attention-based, and bi-directional LSTM models on BLEU, METEOR, TER, and an embedding-based sentence similarity metric.", "fields_of_study": "[\"Computer Science\"]", "external_ids": {"arxiv": "1610.03098", "mag": "2952463445", "acl": "C16-1275", "pubmed": null, "pubmedcentral": null, "dblp": "journals/corr/PrakashHLDQLF16", "doi": null}}, "content": {"source": {"pdf_hash": "db64a9c76ece5cd754fe22afca672be40686d096", "pdf_src": "ACL", "pdf_uri": "[\"https://www.aclweb.org/anthology/C16-1275.pdf\"]", "oa_url_match": false, "oa_info": null}, "grobid": {"id": "e9154c0e777c24ed7b31e5a351d2d1fa9cd90f76", "type": "plain-text", "url": "s3://ai2-s2-pdf-extraction-prod/parse-results/s2orc_worker/db64a9c76ece5cd754fe22afca672be40686d096.txt", "contents": "\nNeural Paraphrase Generation with Stacked Residual LSTM Networks\nDecember 11-17 2016\n\nAaditya Prakash aprakash@philips.com \nBrandeis University\nWalthamMAUSA\n\nArtificial Intelligence Laboratory\nPhilips Research North America\nCambridgeMAUSA\n\nSadid A Hasan sadid.hasan@philips.com \nArtificial Intelligence Laboratory\nPhilips Research North America\nCambridgeMAUSA\n\nKathy Lee kathy.lee1@philips.com \nArtificial Intelligence Laboratory\nPhilips Research North America\nCambridgeMAUSA\n\nVivek Datla vivek.datla@philips.com \nArtificial Intelligence Laboratory\nPhilips Research North America\nCambridgeMAUSA\n\nAshequl Qadir ashequl.qadir@philips.com \nArtificial Intelligence Laboratory\nPhilips Research North America\nCambridgeMAUSA\n\nJoey Liu joey.liu@philips.com \nArtificial Intelligence Laboratory\nPhilips Research North America\nCambridgeMAUSA\n\nOladimeji Farri dimeji.farri@philips.com \nArtificial Intelligence Laboratory\nPhilips Research North America\nCambridgeMAUSA\n\nNeural Paraphrase Generation with Stacked Residual LSTM Networks\n\nProceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers\nCOLING 2016, the 26th International Conference on Computational Linguistics: Technical PapersOsaka, JapanDecember 11-17 2016\nIn this paper, we propose a novel neural approach for paraphrase generation. Conventional paraphrase generation methods either leverage hand-written rules and thesauri-based alignments, or use statistical machine learning principles. To the best of our knowledge, this work is the first to explore deep learning models for paraphrase generation. Our primary contribution is a stacked residual LSTM network, where we add residual connections between LSTM layers. This allows for efficient training of deep LSTMs. We evaluate our model and other state-of-the-art deep learning models on three different datasets: PPDB, WikiAnswers, and MSCOCO. Evaluation results demonstrate that our model outperforms sequence to sequence, attention-based, and bidirectional LSTM models on BLEU, METEOR, TER, and an embedding-based sentence similarity metric.\n\nIntroduction\n\nParaphrasing, the act to express the same meaning in different possible ways, is an important subtask in various Natural Language Processing (NLP) applications such as question answering, information extraction, information retrieval, summarization and natural language generation. Research on paraphrasing methods typically aims at solving three related problems: (1) recognition (i.e. to identify if two textual units are paraphrases of each other), (2) extraction (i.e. to extract paraphrase instances from a thesaurus or a corpus), and (3) generation (i.e. to generate a reference paraphrase given a source text) (Madnani and Dorr, 2010). In this paper, we focus on the paraphrase generation problem.\n\nParaphrase generation has been used to gain performance improvements in several NLP applications, for example, by generating query variants or pattern alternatives for information retrieval, information extraction or question answering systems, by creating reference paraphrases for automatic evaluation of machine translation and document summarization systems, and by generating concise or simplified information for sentence compression or sentence simplification systems (Madnani and Dorr, 2010). Traditional paraphrase generation methods exploit hand-crafted rules (McKeown, 1983) or automatically learned complex paraphrase patterns (Zhao et al., 2009), use thesaurus-based (Hassan et al., 2007) or semantic analysis driven natural language generation approaches (Kozlowski et al., 2003), or leverage statistical machine learning theory (Quirk et al., 2004;Wubben et al., 2010). In this paper, we propose to use deep learning principles to address the paraphrase generation problem.\n\nRecently, techniques like sequence to sequence learning  have been applied to various NLP tasks with promising results, for example, in the areas of machine translation (Cho et al., 2014;Bahdanau et al., 2015), speech recognition (Li and Wu, 2015), language modeling (Vinyals et al., 2015), and dialogue systems (Serban et al., 2016). Although paraphrase generation can be formulated as a sequence to sequence learning task, not much work has been done in this area with regard to applications of state-of-the-art deep neural networks. There are several works on paraphrase recognition (Socher et al., 2011;Yin and Sch\u00fctze, 2015;Kiros et al., 2015), but those employ classification techniques and do not attempt to generate paraphrases. More recently, attention-based Long Short-Term Memory (LSTM) networks have been used for textual entailment generation (Kolesnyk et al., 2016); however, paraphrase generation is a type of bi-directional textual entailment generation and no prior work has proposed a deep learning-based formulation of this task.\n\nTo address this gap in the literature, we explore various types of sequence to sequence models for paraphrase generation. We test these models on three different datasets and evaluate them using well recognized metrics. Along with the application of various existing sequence to sequence models for the paraphrase generation task, in this paper we also propose a new model that allows for training multiple stacked LSTM networks by introducing a residual connection between the layers. This is inspired by the recent success of such connections in a deep Convolutional Neural Network (CNN) for the image recognition task (He et al., 2015). Our experiments demonstrate that the proposed model can outperform other techniques we have explored.\n\nMost of the deep learning models for NLP use Recurrent Neural Networks (RNNs). RNNs differ from normal perceptrons as they allow gradient propagation in time to model sequential data with variable-length input and output (Sutskever et al., 2011). In practice, RNNs often suffer from the vanishing/exploding gradient problems while learning long-range dependencies (Bengio et al., 1994). LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Cho et al., 2014) are known to be successful remedies to these problems.\n\nIt has been observed that increasing the depth of a deep neural network can improve the performance of the model (Simonyan and Zisserman, 2014;He et al., 2015) as deeper networks learn better representations of features (Farabet et al., 2013). In the vision-related tasks where CNNs are more widely used, adding many layers of neurons is a common practice. For tasks like speech recognition (Li and Wu, 2015) and also in machine translation, it is useful to stack layers of LSTM or other variants of RNN. So far this has been limited to only a few layers due to the difficulty in training deep RNN networks. We propose to add residual connections between multiple stacked LSTM networks and show that this allows us to stack more layers of LSTM successfully.\n\nThe rest of the paper is organized as follows: Section 2 presents a brief overview of the sequence to sequence models followed by a description of our proposed residual deep LSTM model, Section 3 describes the datasets used in this work, Section 4 explains the experimental setup, Section 5 presents the evaluation results and analyses, Section 6 discusses the related work, and in Section 7 we conclude and discuss future work.\n\n\nModel Description\n\n\nEncoder-Decoder Model\n\nA neural approach to sequence to sequence modeling proposed by  is a twocomponent model, where a source sequence is first encoded into some low dimensional representation ( Figure 1) that is later used to reproduce the sequence back to a high dimensional target sequence (i.e. decoding). In machine translation, an encoder operates on a sentence written in the source language and encodes its meaning to a vector representation before the decoder can take that vector (which represents the meaning) and generate a sentence in the target language. These encoder-decoder blocks can be either a vanilla RNN or its variants. While producing the target sequence, the generation of each new word depends on the model and the preceding generated word. Generation of the first word in the target sequence depends on the special 'EOS' (end-of-sentence) token appended to the source sequence.\n\nThe training objective is to maximize the log probability of the target sequence given the source sequence. Therefore, the best possible decoded target is the one that has the maximum score over the length of the sequence. To find this, a small set of hypotheses (candidate set) called beam size is used and the total score for all these hypotheses are computed. In the original work by , they observe that although a beam size of 1 achieves good results, a higher beam size is always better. This is because for some of the hypotheses, the first word may not always have the highest score. Figure 2) is a variant of RNN, which computes the hidden state h t using a different approach by adding an internal memory cell c t \u2208 R n at every time step t. In particular, an LSTM unit considers the input state x t at time step t, the hidden state h t\u22121 , and the internal memory state c t\u22121 at time step  Figure 2: LSTM cell (Paszke, 2015).\n\n\nDeep LSTM\n\n\nLSTM (\n\n\nGates\ni t = \u03c3(W xi x t + W hi h t\u22121 + b i ) f t = \u03c3(W xf x t + W hf h t\u22121 + b f ) o t = \u03c3(W xo x t + W ho h t\u22121 + b o ) 2. Input transform c in t = tanh(W xc x t + W hc h t\u22121 + b c in ) 3. State Update c t = f t c t\u22121 + i t c in t h t = o t tanh(c t )\nt \u2212 1 to produce the hidden state h t and the internal memory state c t at time step t. The memory cell is controlled via three learned gates: input i, forget f , and output o. These memory cells use the addition of gradient with respect to time and thus minimize the gradient explosion. In most NLP tasks, LSTM outperforms vanilla RNN (Sundermeyer et al., 2012). Therefore, for our model we only explore LSTM as a basic unit in the encoder and decoder. Here, we describe the basic computations in an LSTM unit, which will provide the grounding to understand the residual connections between stacked LSTM layers later. In the equations above, W x , W h are the learned parameters for x and h respectively. \u03c3(.) and tanh(.) denote element-wise sigmoid and hyperbolic tangent functions respectively. is the element-wise multiplication operator and b denotes the added bias. Graves (2013) explored the advantages of deep LSTMs for handwriting recognition and text generation. There are multiple ways of combining one layer of LSTM with another. For example, Pascanu et al. (2013) explored multiple ways of combining them and discussed various difficulties in training deep LSTMs. In this work, we employ vertical stacking where only the output of the previous layer of LSTM is fed to the input, as compared to the stacking technique used by , where hidden states of all LSTM layers are fully connected. In our model, all but the first layer input at time step t is passed from the hidden state of the previous layer h l t , where l denotes the layer. This is similar to stacked RNN proposed by Bengio et al. (1994) but with LSTM units. Thus, for a layer l the activation is described by:\nh (l) t = f l h (h (l\u22121) t , h (l) t\u22121 )\nwhere hidden states h are recursively computed and h \n\n\nStacked Residual LSTM\n\nWe take inspiration from a very successful deep learning network ResNet (He et al., 2015) with regard to adding residue for the purpose of learning. With theoretical and empirical reasoning, He et al. (2015) have shown that the explicit addition of the residue x to the function being learned allows for deeper network training without overfitting the data.\n\nWhen stacking multiple layers of neurons, the network often suffers through a degradation problem (He et al., 2015). The degradation problem arises due to the low convergence rate of training error and is different from the vanishing gradient problem. Residual connections can help overcome this issue. We experimented with four-layers of stacked LSTM for each of the model. Residual connections are added at layer two as the pointwise addition (see Figure 3), and thus it requires the input to be in the same dimension as the output of h t . Principally because of this reason, we use a simple last hidden unit stacking of LSTM instead of a more intricate way as shown by . This allowed us to clip the h t to match the dimension of x t\u22122 where they were not the same. Similar results could be achieved by padding x to match the dimension instead. The function\u0125 that is being learned for the layer with residual connection is therefore:\nh (l) t = f l h (h (l\u22121) t , h (l) t\u22121 ) + x l\u2212n\nwhere\u0125 for layer l is updated with residual value x l\u2212n and x i represents the input to layer i + 1. Residual connection is added after every n layers. However, for stacked LSTM, n > 3 is very expensive in terms of computation. In this paper we experimented with n = 2. Note that, when n = 1, the resulting function learned is a standard LSTM with bias that depends on the input x. That is why, it is not necessary to add the residual connection after every stacked layer of LSTM. The addition of residual connection does not add any learnable parameters. Therefore, this does not increase the complexity of the model unlike bi-directional models which double the number of LSTM units.\n\n\nDatasets\n\nWe present the performance of our model on three datasets, which are significantly different in their characteristics. So, evaluating our paraphrase generation approach on these datasets demonstrates the versatility and robustness of our model. PPDB (Pavlick et al., 2015) is a well known paraphrase dataset used for various NLP tasks. It comes in different sizes and the precision of the paraphrases degrades with the size of the dataset. We use the size L dataset from PPDB 2.0, which comes with over 18M paraphrases including lexical, phrasal and syntactic types. We have omitted the syntactic paraphrases and the instances which contain numbers, as they increase the vocabulary size significantly without giving any advantage of a larger dataset. This dataset contains relatively short paraphrases (86% of the data is less than four words), which makes it suitable for synonym generation and phrase substitution to address lexical and phrasal paraphrasing (Madnani and Dorr, 2010). For some phrases, PPDB has one-to-many paraphrases. We collect all such phrases to make a set of paraphrases and sampling without replacement was used to obtain the source and reference phrases.\n\nWikiAnswers (Fader et al., 2013) is a large question paraphrase corpus created by crawling the WikiAnswers website 1 , where users can post questions and answers about any topic. The paraphrases are different questions, which were tagged by the users as similar questions. The dataset contains approximately 18M word-aligned question pairs. Sometimes, there occurs a loss of specialization between a given source question and its corresponding reference question when a paraphrase is tagged as similar to a reference question. For example, \"prepare a three month cash budget\" is tagged to \"how to prepare a cash budget\". This happens because general questions are typically more popular and get answered. So, specific questions are redirected to the general ones due to a comparative lack of interest in the very specific questions. It should be noted that this dataset comes preprocessed and lemmatized. We refer the reader to the original paper for more details.\n\nMSCOCO (Lin et al., 2014) dataset contains human annotated captions of over 120K images. Each image contains five captions from five different annotators. While there is no guarantee that the human annotations are paraphrases, the nature of the images (which tends to focus on only a few objects and in most cases one prominent object or action) allows most annotators describe the most obvious things in an image. In fact, this is the main reason why neural networks for generating captions obtain better BLEU scores , which confirms the suitability of using this dataset for the paraphrase generation task.\n\n\nExperimental Settings\n\n\nData Selection\n\nFor PPDB we remove the phrases that contain numbers including all syntactic phrases. This gives us a total of 5.3M paraphrases from which we randomly select 90% instances for training. For testing, we randomly select 20K pairs of paraphrases from the remaining 10% data. Although WikiAnswers comes with over 29M instances, we randomly select 4.8M for training to keep the training size similar to PPDB (see Table 1). 20K instances were randomly selected from the remaining data for testing. Note that, for the WikiAnswers dataset, we clip the vocabulary size 2 to 50K and use the special UNK symbol for the words outside the vocabulary. MSCOCO dataset has five captions for every image. This dataset comes with separate subsets for training and validation: Train 2014 contains over 82K images and Val 2014 contains over 40K images. From the five captions accompanying each image, we randomly omit one caption and use the other four as training instances (by creating two source-reference pairs). Thus, we obtain a collection of over 330K instances for training and 20K instances for testing. Because of the free form nature of the caption generation task , some captions were very long. We reduced those captions to the size of 15 words (by removing the words beyond the first 15) in order to reduce the training complexity of the models.   \n\n\nModels\n\nWe experimented with four different models (see Table 2). For each model, we experimented with twoand four-layers of stacked LSTMs. This was motivated by the state-of-the-art speech recognition systems that also use three to four layers of stacked LSTMs (Li and Wu, 2015). In encoder-decoder models, the size of the beam search used during inference is very important. Larger beam size always gives higher accuracy but is associated with a computational cost. We experimented with beam sizes of 5 and 10 to compare the models, as these are the most common beam sizes used in the literature . The bi-directional model used half of the number of layers shown for other models. This was done to ensure similar parameter sizes across the models.\n\n\nTraining\n\nWe used a one-hot vector approach to represent the words in all models. Models were trained with a stochastic gradient descent (SGD) algorithm. The learning rate began at 1.0, and was halved after every third training epoch. Each network was trained for ten epochs. In order to allow exploration of a wide variety of models, training was restricted to a limited number of epochs, and no hyper-parameter search was performed. A standard dropout (Srivastava et al., 2014) of 50% was applied after every LSTM layer. The number of LSTM units in each layer was fixed to 512 across all models. Training time ranged from 36 hours for WikiAnswers and PPDB to 14 hours for MSCOCO on a Titan X with CuDNN 5 using Theano version 0.9.0dev1 (Theano Development Team, 2016). A beam search algorithm was used to generate optimal paraphrases by exploiting the trained models in the testing phase . We used perplexity as the loss function during training. Perplexity measures the uncertainty of the language model, corresponding to how many bits on average would be needed to encode each word given the language model. A lower perplexity indicates a better score. While WikiAnswers and MSCOCO had a very good correlation between training and validation perplexity, overfitting was observed with PPDB that yielded a worse validation perplexity (see Figure 4).\n\n\nEvaluation\n\n\nMetrics\n\nTo quantitatively evaluate the performance of our paraphrase generation models, we use the well-known automatic evaluation metrics 3 for comparing parallel corpora: BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), and Translation Error Rate (TER) (Snover et al., 2006). Even though these metrics were designed for machine translation, previous works have shown that they can perform well for the paraphrase recognition task (Madnani et al., 2012) and correlate well with human judgments in evaluating generated paraphrases (Wubben et al., 2010).\n\nAlthough there exists a few automatic evaluation metrics that are specifically designed for paraphrase generation, such as PEM (Paraphrase Evaluation Metric)  and PINC (Paraphrase In Ngram Changes) (Chen and Dolan, 2011), they have certain limitations. PEM relies on large in-domain bilingual parallel corpora along with sample human ratings for training while it can only model paraphrasing up to the phrase-level granularity. PINC attempts to solve these limitations by proposing a method that is essentially the inverse of BLEU, as it calculates the n-gram difference between the source and the reference sentences. Although PINC correlates well with human judgments in lexical dissimilarity assessment, BLEU has been shown to correlate better for semantic equivalence agreements at the sentence-level when a sufficiently large number of reference sentences are available for each source sentence (Chen and Dolan, 2011).\n\nBLEU considers exact matching between reference paraphrases and system generated paraphrases by considering n-gram overlaps while METEOR improves upon this measure via stemming and synonymy using WordNet. TER measures the number of edits required to change a system generated paraphrase into one of the reference paraphrases. As suggested in Clark et al. (2011), we used a stratified approximate randomization (AR) test. AR calculates the probability of a metric score providing the same reference sentence by chance. We report our p-values at 95% Confidence Intervals (CI).\n\nThe major limitation of these evaluation metrics is that they do not consider the meaning of the paraphrases, and hence, are not able to capture paraphrases of entities. For example, these metrics do not reward the paraphrasing of \"London\" to \"Capital of UK\". Therefore, we also evaluate our models on a sentence similarity metric 4 proposed by Rus et al. (2012). This metric uses word embeddings to compare the phrases. In our experiments, we used Word2Vec embeddings pre-trained on the Google News Corpus (Mikolov et al., 2014). This is referred to as 'Emb Greedy' in our results table. Table 3 presents the results from various models across different datasets. \u2191 denotes that higher scores represent better models while \u2193 means that a lower score yields a better model. Although our focus is on stacked residual LSTM, which is applicable only when there are more than two layers, we still present the scores from two-layer LSTM as a baseline. This provides a good comparison against deeper models. The results demonstrate that our proposed model outperforms other models on BLEU and TER for all datasets. On Emb Greedy, our model outperforms other models in all datasets except the Attention model when beam size is 10. On METEOR, our model outperforms other models on MSCOCO and WikiAnswers; however, for PPDB, the simple sequence to sequence model performs better. Note that these results were obtained by using single models and no ensemble of the models was used.\n\n\nResults\n\nTo calculate BLEU and METEOR, four references were used for MSCOCO, and five for PPDB and WikiAnswers. In some instances, WikiAnswers did not have up to five reference paraphrases for every source, hence, those were calculated on reduced references. In Table 4, we present the variance due to the test set selection. This is calculated using bootstrap re-sampling for each optimizer run (Clark et al., 2011). Variance due to optimizer instability was less than 0.1 in all cases. p-value of these tests are less than 0.05 in all cases. Thus, comparison between two models is significant at 95% CI if the difference in their score is more than the variance due to test set selection (Table 4).\n\n\nAnalysis\n\nScores on various metrics vary a lot across the datasets, which is understandable due to their inherent differences. PPDB contains very small phrases and thus does not score well with metrics like BLEU and METEOR which penalize shorter phrases. As shown in Figure 5, more than 50% of PPDB contains one or two words. This leads to a substantial difference between training and validation errors, as shown in Figure 4. The results demonstrate that deeper LSTMs consistently improve performance over shallow models. For beam size of 5 our model outperforms other models in all datasets. For beam size of 10, the attention-based model has a marginally better Emb Greedy score than our model. When we look at the qualitative results, we notice that the bias in the dataset is exploited by the system which is a side effect of any form of learning on a limited dataset. We can see this effect in Table 5. For example, an OBJECT is mostly paraphrased with an OBJECT (e.g. bowl, motorcycle). Shorter sentences mostly generate shorter paraphrases and the same is true for longer sequences. Based on our results, the embedding-based metric correlates well with statistical metrics. Figure 4 and the results from Table 5 suggest that perplexity is a good loss function for training paraphrase generation models. However, a more ideal metric to fully encode the fundamental objective of paraphrasing should also reward novelty and penalize redundancy during paraphrase generation, which is a notable limitation of the existing paraphrase evaluation metrics.    \n\n\nRelated Work\n\nPrior approaches to paraphrase generation have applied relatively different methodologies, typically using knowledge-driven approaches or statistical machine translation (SMT) principles. Knowledge-driven methods for paraphrase generation (Madnani and Dorr, 2010) utilize hand-crafted rules (McKeown, 1983) or automatically learned complex paraphrase patterns (Zhao et al., 2009). Other paraphrase generation methods use thesaurus-based (Hassan et al., 2007) or semantic analysis-driven natural language generation approaches (Kozlowski et al., 2003) to generate paraphrases. In contrast, Quirk et al., (2004)   show the effectiveness of SMT techniques for paraphrase generation given adequate monolingual parallel corpus extracted from comparable news articles. Wubben et al., (2010) propose a phrase-based SMT framework for sentential paraphrase generation by using a large aligned monolingual corpus of news headlines. Zhao et al., (2008) propose a combination of multiple resources to learn phrase-based paraphrase tables and corresponding feature functions to devise a log-linear SMT model. Other models generate application-specific paraphrases (Zhao et al., 2009), leverage bilingual parallel corpora (Bannard and Callison-Burch, 2005) or apply a multi-pivot approach to output candidate paraphrases (Zhao et al., 2010). Applications of deep learning for paraphrase generation tasks have not been rigorously explored. We utilized several sources as potential large datasets. Recently, Weiting et al. (2015) took the PPDB dataset (size XL) and annotated phrases based on their paraphrasability. This dataset is called Annotated-PPDB and contains 3000 pairs in total. They also introduced another dataset called ML-Paraphrase for the purpose of evaluating bigram paraphrases. This dataset contains 327 instances. Microsoft Research Paraphrase Corpus (MSRP) (Dolan et al., 2005) is another widely used dataset for paraphrase detection. MSRP contains 5800 pairs of sentences (obtained from various news sources) accompanied with human annotations. These datasets are too small and therefore, we did not use them for training our deep learning models.\n\nTo the best of our knowledge, this is the first work on using residual connections with recurrent neural networks. Very recently, we found that Toderici et al. (2016) used residual GRU to show an improvement in image compression rates for a given quality over JPEG. Another variant of residual network called DenseNet (Huang et al., 2016), which uses dense connections over every layer, has been shown to be effective for image recognition tasks achieving state-of-the-art results in CIFAR and SVHN datasets. Such works further validate the efficacy of adding residual connections for training deep networks.\n\n\nConclusion and Future Work\n\nIn this paper, we described a novel technique to train stacked LSTM networks for paraphrase generation. This is an extension to sequence to sequence learning, which has been shown to be effective for various NLP tasks. Our model outperforms state-of-the-art models for sequence to sequence learning. We have shown that stacking of residual LSTM layers is useful for paraphrase generation, but it may not perform equally well for machine translation because not every word in a source sequence needs to be substituted for paraphrasing. Residual connections help retain important words in the generated paraphrases.\n\nWe experimented on three different large scale datasets and reported results using various automatic evaluation metrics. We showed the use of the well-known MSCOCO dataset for paraphrase generation and demonstrated that the models can be trained effectively without leveraging the images. The presented experiments should set strong baselines for neural paraphrase generation on these datasets, enabling future researchers to easily compare and evaluate subsequent works in paraphrase generation.\n\nRecent advances in neural networks with regard to learnable memory (Sukhbaatar et al., 2015;Graves et al., 2014) have enabled models to get one step closer to learning comprehension. It may be helpful to explore such networks for the paraphrase generation task. Also, it remains to be explored how unsupervised deep learning could be harnessed for paraphrase generation. It would be interesting to see if researchers working on image-captioning can employ neural paraphrase generation to augment their dataset.\n\nFigure 1 :\n1Encoder-Decoder framework for sequence to sequence learning.\n\ntFigure 3 :\n3at t = 0 and l = 0 is given by the LSTM equation of h t . A unit of stacked residual LSTM.\n\nFigure 4 :\n4Perplexity during training (T ) and validation (V ) for various models [shared legend]. A lower perplexity represents a better model.\n\nFigure 5 :\n5Distribution of sequence length (in number of words) across datasets.\n\nTable 1 :\n1Dataset details.Models \nReference \n\nSequence to Sequence (Sutskever et al., 2014) \nWith Attention \n(Bahdanau et al., 2015) \nBi-directional LSTM \n(Graves et al., 2013) \nResidual LSTM \nOur proposed model \n\n\n\nTable 2 :\n2Models.\n\nTable 3 :\n3Evaluation results on PPDB, WikiAnswers, and MSCOCO (Best results are in bold).Dataset \n\u03c3 2 [BLEU] \u03c3 2 [METEOR] \u03c3 2 [TER] \u03c3 2 [Emb Greedy] \n\nPPDB \n2.8 \n0.2 \n0.4 \n0.000100 \nWikiAnswers 0.3 \n0.1 \n0.1 \n0.000017 \nMSCOCO \n0.2 \n0.1 \n0.1 \n0.000013 \n\n\n\nTable 4 :\n4Variance due to test set selection. galaxy be there in you known universe two people sitting on dock looking at the ocean Generated arranged about how many galaxy do the universe contain a couple standing on top of a sandy beach Source counselling what do the ph of acid range to a little baby is sitting on a huge motorcycle Reference be kept informed a acid have ph range of what a little boy sitting alone on a motorcycle Generated consultations how do acid affect ph a baby sitting on top of a motorcyclePPDB \nWikiAnswers \nMSCOCO \nSource \nsouth eastern \nwhat be the symbol of magnesium sulphate \na small kitten is sitting in a bowl \nReference \nthe eastern part \nchemical formulum for magnesium sulphate \na cat is curled up in a bowl \nGenerated \nsouth east \ndo magnesium sulphate have a formulum \na cat that is sitting on a bowl \nSource \norganized \nwhat be the bigggest galaxy know to man \nan old couple at the beach during the day \nReference \nmanaged \nhow many \n\nTable 5 :\n5Example paraphrases generated using the 4-layer Residual LSTM with beam size 5.\nThis work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/\nhttp://wiki.answers.com 2 WikiAnswers dataset had many spelling errors yielding a very large vocabulary size (approximately 250K). Hence, we selected the most frequent 50K words in the vocabulary to reduce the computational complexity.\nWe used the software available at https://github.com/jhclark/multeval\nWe used the software available at https://github.com/julianser/hed-dlg-truncated/\nAcknowledgmentThe authors would like to thank the anonymous reviewers for their valuable comments and feedback. The first author is especially grateful to Prof. James Storer, Brandeis University, for his guidance and Nick Moran, Brandeis University, for helpful discussions.\nNeural Machine Translation by Jointly Learning to Align and Translate. D Bahdanau, K Cho, Y Bengio, Proceedings of ICLR. ICLRD. Bahdanau, K. Cho, and Y. Bengio. 2015. Neural Machine Translation by Jointly Learning to Align and Translate. In Proceedings of ICLR, pages 1-15.\n\nParaphrasing with Bilingual Parallel Corpora. C Bannard, C Callison-Burch, Proceedings of ACL. ACLC. Bannard and C. Callison-Burch. 2005. Paraphrasing with Bilingual Parallel Corpora. In Proceedings of ACL, pages 597-604.\n\nLearning Long-Term Dependencies with Gradient Descent is Difficult. Y Bengio, P Simard, P Frasconi, IEEE Transactions on Neural Networks. 52Y. Bengio, P. Simard, and P. Frasconi. 1994. Learning Long-Term Dependencies with Gradient Descent is Diffi- cult. IEEE Transactions on Neural Networks, 5(2):157-166.\n\nCollecting Highly Parallel Data for Paraphrase Evaluation. D Chen, W B Dolan, Proceedings of ACL-HLT. ACL-HLTD. Chen and W. B. Dolan. 2011. Collecting Highly Parallel Data for Paraphrase Evaluation. In Proceedings of ACL-HLT, pages 190-200.\n\nLearning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. K Cho, B Van Merrienboer, C Gulcehre, D Bahdanau, F Bougares, H Schwenk, Y Bengio, Proceedings of EMNLP. EMNLPK. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. 2014. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. In Proceedings of EMNLP, pages 1724-1734.\n\nBetter Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability. J H Clark, C Dyer, A Lavie, N A Smith, Proceedings of ACL-HLT. ACL-HLTJ. H. Clark, C. Dyer, A. Lavie, and N. A. Smith. 2011. Better Hypothesis Testing for Statistical Machine Transla- tion: Controlling for Optimizer Instability. In Proceedings of ACL-HLT, pages 176-181.\n\nMicrosoft Research Paraphrase Corpus. B Dolan, C Brockett, C Quirk, 29B. Dolan, C. Brockett, and C. Quirk. 2005. Microsoft Research Paraphrase Corpus. Retrieved March, 29:2008.\n\nParaphrase-Driven Learning for Open Question Answering. A Fader, L Zettlemoyer, O Etzioni, ACL. ACLA. Fader, L. S Zettlemoyer, and O. Etzioni. 2013. Paraphrase-Driven Learning for Open Question Answering. In ACL, pages 1608-1618. ACL.\n\nLearning Hierarchical Features for Scene Labeling. C Farabet, C Couprie, L Najman, Y Lecun, IEEE Transactions on Pattern Analysis and Machine Intelligence. 358C. Farabet, C. Couprie, L. Najman, and Y. LeCun. 2013. Learning Hierarchical Features for Scene Labeling. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1915-1929.\n\nHybrid Speech Recognition with Deep Bidirectional LSTM. A Graves, N Jaitly, A Mohamed, Automatic Speech Recognition and Understanding (ASRU). IEEEA. Graves, N. Jaitly, and A. Mohamed. 2013. Hybrid Speech Recognition with Deep Bidirectional LSTM. In Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on, pages 273-278. IEEE.\n\nA Graves, G Wayne, I Danihelka, arXiv:1410.5401Neural Turing Machines. A. Graves, G. Wayne, and I. Danihelka. 2014. Neural Turing Machines. In arXiv:1410.5401.\n\nA Graves, arXiv:1308.0850Generating Sequences with Recurrent Neural Networks. arXiv preprintA. Graves. 2013. Generating Sequences with Recurrent Neural Networks. arXiv preprint arXiv:1308.0850.\n\nUNT: SubFinder: Combining Knowledge Sources for Automatic Lexical Substitution. S Hassan, A Csomai, C Banea, R Sinha, R Mihalcea, Proceedings of SemEval. SemEvalS. Hassan, A. Csomai, C. Banea, R. Sinha, and R. Mihalcea. 2007. UNT: SubFinder: Combining Knowledge Sources for Automatic Lexical Substitution. In Proceedings of SemEval, pages 410-413.\n\nK He, X Zhang, S Ren, J Sun, arXiv:1512.03385Deep Residual Learning for Image Recognition. arXiv preprintK. He, X. Zhang, S. Ren, and J. Sun. 2015. Deep Residual Learning for Image Recognition. arXiv preprint arXiv:1512.03385.\n\nLong Short-Term Memory. S Hochreiter, J Schmidhuber, Neural Computation. 98S. Hochreiter and J. Schmidhuber. 1997. Long Short-Term Memory. Neural Computation, 9(8):1735-1780.\n\nG Huang, Z Liu, K Q Weinberger, arXiv:1608.06993Densely Connected Convolutional Networks. arXiv preprintG. Huang, Z. Liu, and K. Q. Weinberger. 2016. Densely Connected Convolutional Networks. arXiv preprint arXiv:1608.06993.\n\nSkip-Thought Vectors. R Kiros, Y Zhu, R R Salakhutdinov, R Zemel, R Urtasun, A Torralba, S Fidler, Advances in Neural Information Processing Systems. R. Kiros, Y. Zhu, R. R. Salakhutdinov, R. Zemel, R. Urtasun, A. Torralba, and S. Fidler. 2015. Skip-Thought Vectors. In Advances in Neural Information Processing Systems, pages 3294-3302.\n\nGenerating Natural Language Inference Chains. CoRR. V Kolesnyk, T Rockt\u00e4schel, S Riedel, abs/1606.01404V. Kolesnyk, T. Rockt\u00e4schel, and S. Riedel. 2016. Generating Natural Language Inference Chains. CoRR, abs/1606.01404.\n\nGeneration of Single-sentence Paraphrases from Predicate/Argument Structure Using Lexico-grammatical Resources. R Kozlowski, K F Mccoy, K Vijay-Shanker, Proceedings of the 2nd International Workshop on Paraphrasing. the 2nd International Workshop on ParaphrasingR. Kozlowski, K. F. McCoy, and K. Vijay-Shanker. 2003. Generation of Single-sentence Paraphrases from Predi- cate/Argument Structure Using Lexico-grammatical Resources. In Proceedings of the 2nd International Work- shop on Paraphrasing, pages 1-8.\n\nMETEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments. A Lavie, A Agarwal, Proceedings of the Second Workshop on Statistical Machine Translation. the Second Workshop on Statistical Machine TranslationA. Lavie and A. Agarwal. 2007. METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments. In Proceedings of the Second Workshop on Statistical Machine Translation, pages 228-231.\n\nConstructing Long Short-term Memory based Deep Recurrent Neural Networks for Large Vocabulary Speech Recognition. X Li, X Wu, 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEEX. Li and X. Wu. 2015. Constructing Long Short-term Memory based Deep Recurrent Neural Networks for Large Vocabulary Speech Recognition. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4520-4524. IEEE.\n\nMicrosoft COCO: Common Objects in Context. T Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Doll\u00e1r, C L Zitnick, European Conference on Computer Vision. SpringerT. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r, and C. L. Zitnick. 2014. Microsoft COCO: Common Objects in Context. In European Conference on Computer Vision, pages 740-755. Springer.\n\nPEM: A Paraphrase Evaluation Metric Exploiting Parallel Texts. C Liu, D Dahlmeier, H T Ng, Proceedings of EMNLP. EMNLPC. Liu, D. Dahlmeier, and H. T. Ng. 2010. PEM: A Paraphrase Evaluation Metric Exploiting Parallel Texts. In Proceedings of EMNLP, pages 923-932.\n\nGenerating Phrasal and Sentential Paraphrases: A Survey of Data-driven. N Madnani, B J Dorr, Methods. Computational Linguistics. 363N. Madnani and B. J. Dorr. 2010. Generating Phrasal and Sentential Paraphrases: A Survey of Data-driven Methods. Computational Linguistics, 36(3):341-387.\n\nRe-examining Machine Translation Metrics for Paraphrase Identification. N Madnani, J Tetreault, M Chodorow, Proceedings of NAACL-HLT. NAACL-HLTN. Madnani, J. Tetreault, and M. Chodorow. 2012. Re-examining Machine Translation Metrics for Paraphrase Identification. In Proceedings of NAACL-HLT, pages 182-190.\n\nK R Mckeown, Paraphrasing Questions Using Given and New Information. Computational Linguistics. 9K. R. McKeown. 1983. Paraphrasing Questions Using Given and New Information. Computational Linguistics, 9(1):1-10.\n\n. T Mikolov, K Chen, G Corrado, J Dean, T. Mikolov, K. Chen, G. Corrado, and J. Dean. 2014. Word2Vec. https://code.google.com/p/word2vec. Online; accessed 2014-04-15.\n\nBLEU: A Method for Automatic Evaluation of Machine Translation. K Papineni, S Roukos, T Ward, W Zhu, Proceedings of ACL. ACLK. Papineni, S. Roukos, T. Ward, and W. Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In Proceedings of ACL, pages 311-318.\n\nR Pascanu, C Gulcehre, K Cho, Y Bengio, arXiv:1312.6026How to Construct Deep Recurrent Neural Networks. arXiv preprintR. Pascanu, C. Gulcehre, K. Cho, and Y. Bengio. 2013. How to Construct Deep Recurrent Neural Networks. arXiv preprint arXiv:1312.6026.\n\nLSTM Implementation Explained: apaszke.github.io/lstm-explained.html. A Paszke, A. Paszke. 2015. LSTM Implementation Explained: apaszke.github.io/lstm-explained.html , 2016-07-15.\n\nPPDB 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification. E Pavlick, P Rastogi, J Ganitkevitch, B Van Durme, C Callison-Burch, Proceedings of ACL-IJCNLP). ACL-IJCNLP)E. Pavlick, P. Rastogi, J. Ganitkevitch, B. Van Durme, and C. Callison-Burch. 2015. PPDB 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification. In Proceedings of ACL- IJCNLP), pages 425-430.\n\nMonolingual Machine Translation for Paraphrase Generation. C Quirk, C Brockett, W Dolan, Proceedings of EMNLP. EMNLPC. Quirk, C. Brockett, and W. Dolan. 2004. Monolingual Machine Translation for Paraphrase Generation. In Proceedings of EMNLP, pages 142-149.\n\nA Comparison of Greedy and Optimal Assessment of Natural Language Student Input using Word-to-Word Similarity Metrics. V Rus, M Lintean, Proceedings of the Seventh Workshop on Building Educational Applications Using NLP. the Seventh Workshop on Building Educational Applications Using NLPACLV. Rus and M. Lintean. 2012. A Comparison of Greedy and Optimal Assessment of Natural Language Student Input using Word-to-Word Similarity Metrics. In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages 157-162. ACL.\n\nMultiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation. I V Serban, T Klinger, G Tesauro, K Talamadupula, B Zhou, Y Bengio, A Courville, arXiv:1606.00776arXiv preprintI. V. Serban, T. Klinger, G. Tesauro, K. Talamadupula, B. Zhou, Y. Bengio, and A. Courville. 2016. Mul- tiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation. arXiv preprint arXiv:1606.00776.\n\nK Simonyan, A Zisserman, arXiv:1409.1556Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv preprintK. Simonyan and A. Zisserman. 2014. Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv preprint arXiv:1409.1556.\n\nA Study of Translation Edit Rate with Targeted Human Annotation. M Snover, B Dorr, R Schwartz, L Micciulla, J Makhoul, Proceedings of Association for Machine Translation in the Americas. Association for Machine Translation in the AmericasM. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proceedings of Association for Machine Translation in the Americas, pages 223-231.\n\nDynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection. R Socher, E H Huang, J Pennington, A Y Ng, C D Manning, Advances in Neural Information Processing Systems. R. Socher, E. H. Huang, J. Pennington, A. Y. Ng, and C. D. Manning. 2011. Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection. In Advances in Neural Information Processing Systems, pages 1-9.\n\nDropout: A Simple Way to Prevent Neural Networks from Overfitting. N Srivastava, G Hinton, A Krizhevsky, I Sutskever, R Salakhutdinov, Journal of Machine Learning Research. 151N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. 2014. Dropout: A Simple Way to Prevent Neural Networks from Overfitting. Journal of Machine Learning Research, 15(1):1929-1958.\n\nEnd-to-End Memory Networks. S Sukhbaatar, J Weston, R Fergus, Advances in neural information processing systems. S. Sukhbaatar, J. Weston, R. Fergus, et al. 2015. End-to-End Memory Networks. In Advances in neural informa- tion processing systems, pages 2440-2448.\n\nLSTM Neural Networks for Language Modeling. M Sundermeyer, R Schl\u00fcter, H Ney, Interspeech. M. Sundermeyer, R. Schl\u00fcter, and H. Ney. 2012. LSTM Neural Networks for Language Modeling. In Interspeech, pages 194-197.\n\nGenerating Text with Recurrent Neural Networks. I Sutskever, J Martens, G E Hinton, Proceedings of ICML. ICMLI. Sutskever, J. Martens, and G. E. Hinton. 2011. Generating Text with Recurrent Neural Networks. In Proceed- ings of ICML, pages 1017-1024.\n\nSequence to Sequence Learning with Neural Networks. I Sutskever, O Vinyals, Q V Le, Annual Conference on Neural Information Processing Systems. I. Sutskever, O. Vinyals, and Q. V. Le. 2014. Sequence to Sequence Learning with Neural Networks. In Annual Conference on Neural Information Processing Systems, pages 3104-3112.\n\nTheano: A Python Framework for Fast Computation of Mathematical Expressions. abs/1605.02688Theano Development Team. arXiv e-printsTheano Development Team. 2016. Theano: A Python Framework for Fast Computation of Mathematical Expres- sions. arXiv e-prints, abs/1605.02688, May.\n\nG Toderici, D Vincent, N Johnston, S J Hwang, D Minnen, J Shor, M Covell, arXiv:1608.05148Full Resolution Image Compression with Recurrent Neural Networks. arXiv preprintG. Toderici, D. Vincent, N. Johnston, S. J. Hwang, D. Minnen, J. Shor, and M. Covell. 2016. Full Resolution Image Compression with Recurrent Neural Networks. arXiv preprint arXiv:1608.05148.\n\nShow and tell: A neural image caption generator. O Vinyals, A Toshev, S Bengio, D Erhan, abs/1411.4555CoRRO. Vinyals, A. Toshev, S. Bengio, and D. Erhan. 2014. Show and tell: A neural image caption generator. CoRR, abs/1411.4555.\n\nGrammar as a Foreign Language. O Vinyals, L Kaiser, T Koo, S Petrov, I Sutskever, G Hinton, Advances in Neural Information Processing Systems. O. Vinyals, L. Kaiser, T. Koo, S. Petrov, I. Sutskever, and G. Hinton. 2015. Grammar as a Foreign Language. In Advances in Neural Information Processing Systems, pages 2773-2781.\n\nFrom Paraphrase Database to Compositional Paraphrase Model and Back. J Wieting, M Bansal, K Gimpel, K Livescu, Transactions of the ACL (TACL). J. Wieting, M. Bansal, K. Gimpel, and K. Livescu. 2015. From Paraphrase Database to Compositional Paraphrase Model and Back. Transactions of the ACL (TACL).\n\nParaphrase Generation As Monolingual Translation: Data and Evaluation. S Wubben, A Van Den, E Bosch, Krahmer, Proceedings of INLG. INLGS. Wubben, A. van den Bosch, and E. Krahmer. 2010. Paraphrase Generation As Monolingual Translation: Data and Evaluation. In Proceedings of INLG, pages 203-207.\n\nConvolutional Neural Network for Paraphrase Identification. W Yin, H Sch\u00fctze, Proceedings of NAACL-HLT. NAACL-HLTW. Yin and H. Sch\u00fctze. 2015. Convolutional Neural Network for Paraphrase Identification. In Proceedings of NAACL-HLT, pages 901-911.\n\nCombining Multiple Resources to Improve SMT-based Paraphrasing Model. S Zhao, C Niu, M Zhou, T Liu, S Li, Proceedings of ACL-HLT. ACL-HLTS. Zhao, C. Niu, M. Zhou, T. Liu, and S. Li. 2008. Combining Multiple Resources to Improve SMT-based Paraphrasing Model. In Proceedings of ACL-HLT, pages 1021-1029.\n\nApplication-driven Statistical Paraphrase Generation. S Zhao, X Lan, T Liu, S Li, Proceedings of ACL-IJCNLP. ACL-IJCNLPS. Zhao, X. Lan, T. Liu, and S. Li. 2009. Application-driven Statistical Paraphrase Generation. In Proceedings of ACL-IJCNLP, pages 834-842.\n\nLeveraging Multiple MT Engines for Paraphrase Generation. S Zhao, H Wang, X Lan, T Liu, Proceedings of COLING. COLINGS. Zhao, H. Wang, X. Lan, and T. Liu. 2010. Leveraging Multiple MT Engines for Paraphrase Generation. In Proceedings of COLING, pages 1326-1334.\n", "annotations": {"author": "[{\"end\":240,\"start\":87},{\"end\":361,\"start\":241},{\"end\":477,\"start\":362},{\"end\":596,\"start\":478},{\"end\":719,\"start\":597},{\"end\":832,\"start\":720},{\"end\":956,\"start\":833}]", "publisher": null, "author_last_name": "[{\"end\":102,\"start\":95},{\"end\":254,\"start\":249},{\"end\":371,\"start\":368},{\"end\":489,\"start\":484},{\"end\":610,\"start\":605},{\"end\":728,\"start\":725},{\"end\":848,\"start\":843}]", "author_first_name": "[{\"end\":94,\"start\":87},{\"end\":246,\"start\":241},{\"end\":248,\"start\":247},{\"end\":367,\"start\":362},{\"end\":483,\"start\":478},{\"end\":604,\"start\":597},{\"end\":724,\"start\":720},{\"end\":842,\"start\":833}]", "author_affiliation": "[{\"end\":157,\"start\":125},{\"end\":239,\"start\":159},{\"end\":360,\"start\":280},{\"end\":476,\"start\":396},{\"end\":595,\"start\":515},{\"end\":718,\"start\":638},{\"end\":831,\"start\":751},{\"end\":955,\"start\":875}]", "title": "[{\"end\":65,\"start\":1},{\"end\":1021,\"start\":957}]", "venue": "[{\"end\":1131,\"start\":1023}]", "abstract": "[{\"end\":2098,\"start\":1257}]", "bib_ref": "[{\"attributes\":{\"ref_id\":\"b23\"},\"end\":2755,\"start\":2731},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":3319,\"start\":3295},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":3405,\"start\":3390},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":3478,\"start\":3459},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":3521,\"start\":3500},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":3613,\"start\":3589},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":3683,\"start\":3663},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":3703,\"start\":3683},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":3997,\"start\":3979},{\"attributes\":{\"ref_id\":\"b0\"},\"end\":4019,\"start\":3997},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":4057,\"start\":4040},{\"attributes\":{\"ref_id\":\"b45\"},\"end\":4099,\"start\":4077},{\"attributes\":{\"ref_id\":\"b33\"},\"end\":4143,\"start\":4122},{\"attributes\":{\"ref_id\":\"b36\"},\"end\":4417,\"start\":4396},{\"attributes\":{\"ref_id\":\"b48\"},\"end\":4439,\"start\":4417},{\"attributes\":{\"ref_id\":\"b16\"},\"end\":4458,\"start\":4439},{\"attributes\":{\"ref_id\":\"b17\"},\"end\":4689,\"start\":4666},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":5498,\"start\":5481},{\"attributes\":{\"ref_id\":\"b40\"},\"end\":5848,\"start\":5824},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":5988,\"start\":5967},{\"attributes\":{\"ref_id\":\"b14\"},\"end\":6029,\"start\":5995},{\"attributes\":{\"ref_id\":\"b4\"},\"end\":6056,\"start\":6038},{\"attributes\":{\"ref_id\":\"b34\"},\"end\":6256,\"start\":6226},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":6272,\"start\":6256},{\"attributes\":{\"ref_id\":\"b8\"},\"end\":6355,\"start\":6333},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":6521,\"start\":6504},{\"attributes\":{\"ref_id\":\"b29\"},\"end\":9164,\"start\":9150},{\"attributes\":{\"ref_id\":\"b39\"},\"end\":9803,\"start\":9777},{\"attributes\":{\"ref_id\":\"b11\"},\"end\":10326,\"start\":10313},{\"attributes\":{\"ref_id\":\"b28\"},\"end\":10517,\"start\":10496},{\"attributes\":{\"ref_id\":\"b2\"},\"end\":11052,\"start\":11032},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11335,\"start\":11318},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11453,\"start\":11437},{\"attributes\":{\"ref_id\":\"b13\"},\"end\":11720,\"start\":11703},{\"attributes\":{\"ref_id\":\"b30\"},\"end\":13561,\"start\":13539},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":14273,\"start\":14249},{\"attributes\":{\"ref_id\":\"b7\"},\"end\":14502,\"start\":14483},{\"attributes\":{\"ref_id\":\"b21\"},\"end\":15462,\"start\":15444},{\"attributes\":{\"ref_id\":\"b20\"},\"end\":17711,\"start\":17694},{\"attributes\":{\"ref_id\":\"b37\"},\"end\":18663,\"start\":18638},{\"attributes\":{\"ref_id\":\"b27\"},\"end\":19753,\"start\":19730},{\"attributes\":{\"ref_id\":\"b19\"},\"end\":19787,\"start\":19762},{\"attributes\":{\"ref_id\":\"b35\"},\"end\":19843,\"start\":19822},{\"attributes\":{\"ref_id\":\"b24\"},\"end\":20021,\"start\":19999},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":20119,\"start\":20098},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":20342,\"start\":20320},{\"attributes\":{\"ref_id\":\"b3\"},\"end\":21044,\"start\":21022},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":21408,\"start\":21389},{\"attributes\":{\"ref_id\":\"b32\"},\"end\":21985,\"start\":21968},{\"attributes\":{\"ref_id\":\"b26\"},\"end\":22152,\"start\":22130},{\"attributes\":{\"ref_id\":\"b5\"},\"end\":23513,\"start\":23493},{\"attributes\":{\"ref_id\":\"b23\"},\"end\":25639,\"start\":25615},{\"attributes\":{\"ref_id\":\"b25\"},\"end\":25682,\"start\":25667},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":25755,\"start\":25736},{\"attributes\":{\"ref_id\":\"b12\"},\"end\":25834,\"start\":25813},{\"attributes\":{\"ref_id\":\"b18\"},\"end\":25926,\"start\":25902},{\"attributes\":{\"ref_id\":\"b31\"},\"end\":25985,\"start\":25965},{\"attributes\":{\"ref_id\":\"b47\"},\"end\":26160,\"start\":26139},{\"attributes\":{\"ref_id\":\"b49\"},\"end\":26317,\"start\":26298},{\"attributes\":{\"ref_id\":\"b50\"},\"end\":26546,\"start\":26527},{\"attributes\":{\"ref_id\":\"b1\"},\"end\":26618,\"start\":26584},{\"attributes\":{\"ref_id\":\"b51\"},\"end\":26702,\"start\":26683},{\"end\":26889,\"start\":26858},{\"attributes\":{\"ref_id\":\"b6\"},\"end\":27258,\"start\":27238},{\"attributes\":{\"ref_id\":\"b43\"},\"end\":27697,\"start\":27675},{\"attributes\":{\"ref_id\":\"b15\"},\"end\":27869,\"start\":27849},{\"attributes\":{\"ref_id\":\"b38\"},\"end\":29375,\"start\":29350},{\"attributes\":{\"ref_id\":\"b10\"},\"end\":29395,\"start\":29375}]", "figure": "[{\"attributes\":{\"id\":\"fig_0\"},\"end\":29867,\"start\":29794},{\"attributes\":{\"id\":\"fig_1\"},\"end\":29972,\"start\":29868},{\"attributes\":{\"id\":\"fig_2\"},\"end\":30119,\"start\":29973},{\"attributes\":{\"id\":\"fig_3\"},\"end\":30202,\"start\":30120},{\"attributes\":{\"id\":\"tab_1\",\"type\":\"table\"},\"end\":30419,\"start\":30203},{\"attributes\":{\"id\":\"tab_2\",\"type\":\"table\"},\"end\":30439,\"start\":30420},{\"attributes\":{\"id\":\"tab_4\",\"type\":\"table\"},\"end\":30695,\"start\":30440},{\"attributes\":{\"id\":\"tab_5\",\"type\":\"table\"},\"end\":31673,\"start\":30696},{\"attributes\":{\"id\":\"tab_6\",\"type\":\"table\"},\"end\":31765,\"start\":31674}]", "paragraph": "[{\"end\":2818,\"start\":2114},{\"end\":3808,\"start\":2820},{\"end\":4858,\"start\":3810},{\"end\":5601,\"start\":4860},{\"end\":6111,\"start\":5603},{\"end\":6870,\"start\":6113},{\"end\":7300,\"start\":6872},{\"end\":8228,\"start\":7346},{\"end\":9165,\"start\":8230},{\"end\":11125,\"start\":9441},{\"end\":11220,\"start\":11167},{\"end\":11603,\"start\":11246},{\"end\":12541,\"start\":11605},{\"end\":13276,\"start\":12591},{\"end\":14469,\"start\":13289},{\"end\":15435,\"start\":14471},{\"end\":16045,\"start\":15437},{\"end\":17429,\"start\":16088},{\"end\":18181,\"start\":17440},{\"end\":19535,\"start\":18194},{\"end\":20120,\"start\":19560},{\"end\":21045,\"start\":20122},{\"end\":21621,\"start\":21047},{\"end\":23094,\"start\":21623},{\"end\":23797,\"start\":23106},{\"end\":25359,\"start\":23810},{\"end\":27529,\"start\":25376},{\"end\":28139,\"start\":27531},{\"end\":28783,\"start\":28170},{\"end\":29281,\"start\":28785},{\"end\":29793,\"start\":29283}]", "formula": "[{\"attributes\":{\"id\":\"formula_0\"},\"end\":9440,\"start\":9195},{\"attributes\":{\"id\":\"formula_1\"},\"end\":11166,\"start\":11126},{\"attributes\":{\"id\":\"formula_2\"},\"end\":12590,\"start\":12542}]", "table_ref": "[{\"attributes\":{\"ref_id\":\"tab_1\"},\"end\":16502,\"start\":16495},{\"attributes\":{\"ref_id\":\"tab_2\"},\"end\":17495,\"start\":17488},{\"attributes\":{\"ref_id\":\"tab_4\"},\"end\":22219,\"start\":22212},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":23366,\"start\":23359},{\"attributes\":{\"ref_id\":\"tab_5\"},\"end\":23796,\"start\":23787},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":24707,\"start\":24700},{\"attributes\":{\"ref_id\":\"tab_6\"},\"end\":25019,\"start\":25012}]", "section_header": "[{\"attributes\":{\"n\":\"1\"},\"end\":2112,\"start\":2100},{\"attributes\":{\"n\":\"2\"},\"end\":7320,\"start\":7303},{\"attributes\":{\"n\":\"2.1\"},\"end\":7344,\"start\":7323},{\"attributes\":{\"n\":\"2.2\"},\"end\":9177,\"start\":9168},{\"end\":9186,\"start\":9180},{\"attributes\":{\"n\":\"1.\"},\"end\":9194,\"start\":9189},{\"attributes\":{\"n\":\"2.3\"},\"end\":11244,\"start\":11223},{\"attributes\":{\"n\":\"3\"},\"end\":13287,\"start\":13279},{\"attributes\":{\"n\":\"4\"},\"end\":16069,\"start\":16048},{\"attributes\":{\"n\":\"4.1\"},\"end\":16086,\"start\":16072},{\"attributes\":{\"n\":\"4.2\"},\"end\":17438,\"start\":17432},{\"attributes\":{\"n\":\"4.3\"},\"end\":18192,\"start\":18184},{\"attributes\":{\"n\":\"5\"},\"end\":19548,\"start\":19538},{\"attributes\":{\"n\":\"5.1\"},\"end\":19558,\"start\":19551},{\"attributes\":{\"n\":\"5.2\"},\"end\":23104,\"start\":23097},{\"attributes\":{\"n\":\"5.3\"},\"end\":23808,\"start\":23800},{\"attributes\":{\"n\":\"6\"},\"end\":25374,\"start\":25362},{\"attributes\":{\"n\":\"7\"},\"end\":28168,\"start\":28142},{\"end\":29805,\"start\":29795},{\"end\":29880,\"start\":29869},{\"end\":29984,\"start\":29974},{\"end\":30131,\"start\":30121},{\"end\":30213,\"start\":30204},{\"end\":30430,\"start\":30421},{\"end\":30450,\"start\":30441},{\"end\":30706,\"start\":30697},{\"end\":31684,\"start\":31675}]", "table": "[{\"end\":30419,\"start\":30231},{\"end\":30695,\"start\":30531},{\"end\":31673,\"start\":31216}]", "figure_caption": "[{\"end\":29867,\"start\":29807},{\"end\":29972,\"start\":29882},{\"end\":30119,\"start\":29986},{\"end\":30202,\"start\":30133},{\"end\":30231,\"start\":30215},{\"end\":30439,\"start\":30432},{\"end\":30531,\"start\":30452},{\"end\":31216,\"start\":30708},{\"end\":31765,\"start\":31686}]", "figure_ref": "[{\"attributes\":{\"ref_id\":\"fig_0\"},\"end\":7527,\"start\":7519},{\"end\":8829,\"start\":8821},{\"end\":9138,\"start\":9130},{\"attributes\":{\"ref_id\":\"fig_1\"},\"end\":12063,\"start\":12055},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":19533,\"start\":19525},{\"attributes\":{\"ref_id\":\"fig_3\"},\"end\":24075,\"start\":24067},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24225,\"start\":24217},{\"attributes\":{\"ref_id\":\"fig_2\"},\"end\":24990,\"start\":24982}]", "bib_author_first_name": "[{\"end\":32648,\"start\":32647},{\"end\":32660,\"start\":32659},{\"end\":32667,\"start\":32666},{\"end\":32898,\"start\":32897},{\"end\":32909,\"start\":32908},{\"end\":33143,\"start\":33142},{\"end\":33153,\"start\":33152},{\"end\":33163,\"start\":33162},{\"end\":33442,\"start\":33441},{\"end\":33450,\"start\":33449},{\"end\":33452,\"start\":33451},{\"end\":33720,\"start\":33719},{\"end\":33727,\"start\":33726},{\"end\":33746,\"start\":33745},{\"end\":33758,\"start\":33757},{\"end\":33770,\"start\":33769},{\"end\":33782,\"start\":33781},{\"end\":33793,\"start\":33792},{\"end\":34170,\"start\":34169},{\"end\":34172,\"start\":34171},{\"end\":34181,\"start\":34180},{\"end\":34189,\"start\":34188},{\"end\":34198,\"start\":34197},{\"end\":34200,\"start\":34199},{\"end\":34480,\"start\":34479},{\"end\":34489,\"start\":34488},{\"end\":34501,\"start\":34500},{\"end\":34676,\"start\":34675},{\"end\":34685,\"start\":34684},{\"end\":34700,\"start\":34699},{\"end\":34907,\"start\":34906},{\"end\":34918,\"start\":34917},{\"end\":34929,\"start\":34928},{\"end\":34939,\"start\":34938},{\"end\":35259,\"start\":35258},{\"end\":35269,\"start\":35268},{\"end\":35279,\"start\":35278},{\"end\":35552,\"start\":35551},{\"end\":35562,\"start\":35561},{\"end\":35571,\"start\":35570},{\"end\":35713,\"start\":35712},{\"end\":35988,\"start\":35987},{\"end\":35998,\"start\":35997},{\"end\":36008,\"start\":36007},{\"end\":36017,\"start\":36016},{\"end\":36026,\"start\":36025},{\"end\":36257,\"start\":36256},{\"end\":36263,\"start\":36262},{\"end\":36272,\"start\":36271},{\"end\":36279,\"start\":36278},{\"end\":36509,\"start\":36508},{\"end\":36523,\"start\":36522},{\"end\":36661,\"start\":36660},{\"end\":36670,\"start\":36669},{\"end\":36677,\"start\":36676},{\"end\":36679,\"start\":36678},{\"end\":36909,\"start\":36908},{\"end\":36918,\"start\":36917},{\"end\":36925,\"start\":36924},{\"end\":36927,\"start\":36926},{\"end\":36944,\"start\":36943},{\"end\":36953,\"start\":36952},{\"end\":36964,\"start\":36963},{\"end\":36976,\"start\":36975},{\"end\":37278,\"start\":37277},{\"end\":37290,\"start\":37289},{\"end\":37305,\"start\":37304},{\"end\":37560,\"start\":37559},{\"end\":37573,\"start\":37572},{\"end\":37575,\"start\":37574},{\"end\":37584,\"start\":37583},{\"end\":38059,\"start\":38058},{\"end\":38068,\"start\":38067},{\"end\":38539,\"start\":38538},{\"end\":38545,\"start\":38544},{\"end\":38938,\"start\":38937},{\"end\":38945,\"start\":38944},{\"end\":38954,\"start\":38953},{\"end\":38966,\"start\":38965},{\"end\":38974,\"start\":38973},{\"end\":38984,\"start\":38983},{\"end\":38995,\"start\":38994},{\"end\":39005,\"start\":39004},{\"end\":39007,\"start\":39006},{\"end\":39340,\"start\":39339},{\"end\":39347,\"start\":39346},{\"end\":39360,\"start\":39359},{\"end\":39362,\"start\":39361},{\"end\":39613,\"start\":39612},{\"end\":39624,\"start\":39623},{\"end\":39626,\"start\":39625},{\"end\":39901,\"start\":39900},{\"end\":39912,\"start\":39911},{\"end\":39925,\"start\":39924},{\"end\":40138,\"start\":40137},{\"end\":40140,\"start\":40139},{\"end\":40353,\"start\":40352},{\"end\":40364,\"start\":40363},{\"end\":40372,\"start\":40371},{\"end\":40383,\"start\":40382},{\"end\":40583,\"start\":40582},{\"end\":40595,\"start\":40594},{\"end\":40605,\"start\":40604},{\"end\":40613,\"start\":40612},{\"end\":40797,\"start\":40796},{\"end\":40808,\"start\":40807},{\"end\":40820,\"start\":40819},{\"end\":40827,\"start\":40826},{\"end\":41121,\"start\":41120},{\"end\":41347,\"start\":41346},{\"end\":41358,\"start\":41357},{\"end\":41369,\"start\":41368},{\"end\":41385,\"start\":41384},{\"end\":41398,\"start\":41397},{\"end\":41761,\"start\":41760},{\"end\":41770,\"start\":41769},{\"end\":41782,\"start\":41781},{\"end\":42080,\"start\":42079},{\"end\":42087,\"start\":42086},{\"end\":42599,\"start\":42598},{\"end\":42601,\"start\":42600},{\"end\":42611,\"start\":42610},{\"end\":42622,\"start\":42621},{\"end\":42633,\"start\":42632},{\"end\":42649,\"start\":42648},{\"end\":42657,\"start\":42656},{\"end\":42667,\"start\":42666},{\"end\":42936,\"start\":42935},{\"end\":42948,\"start\":42947},{\"end\":43260,\"start\":43259},{\"end\":43270,\"start\":43269},{\"end\":43278,\"start\":43277},{\"end\":43290,\"start\":43289},{\"end\":43303,\"start\":43302},{\"end\":43733,\"start\":43732},{\"end\":43743,\"start\":43742},{\"end\":43745,\"start\":43744},{\"end\":43754,\"start\":43753},{\"end\":43768,\"start\":43767},{\"end\":43770,\"start\":43769},{\"end\":43776,\"start\":43775},{\"end\":43778,\"start\":43777},{\"end\":44126,\"start\":44125},{\"end\":44140,\"start\":44139},{\"end\":44150,\"start\":44149},{\"end\":44164,\"start\":44163},{\"end\":44177,\"start\":44176},{\"end\":44469,\"start\":44468},{\"end\":44483,\"start\":44482},{\"end\":44493,\"start\":44492},{\"end\":44750,\"start\":44749},{\"end\":44765,\"start\":44764},{\"end\":44777,\"start\":44776},{\"end\":44968,\"start\":44967},{\"end\":44981,\"start\":44980},{\"end\":44992,\"start\":44991},{\"end\":44994,\"start\":44993},{\"end\":45223,\"start\":45222},{\"end\":45236,\"start\":45235},{\"end\":45247,\"start\":45246},{\"end\":45249,\"start\":45248},{\"end\":45772,\"start\":45771},{\"end\":45784,\"start\":45783},{\"end\":45795,\"start\":45794},{\"end\":45807,\"start\":45806},{\"end\":45809,\"start\":45808},{\"end\":45818,\"start\":45817},{\"end\":45828,\"start\":45827},{\"end\":45836,\"start\":45835},{\"end\":46183,\"start\":46182},{\"end\":46194,\"start\":46193},{\"end\":46204,\"start\":46203},{\"end\":46214,\"start\":46213},{\"end\":46396,\"start\":46395},{\"end\":46407,\"start\":46406},{\"end\":46417,\"start\":46416},{\"end\":46424,\"start\":46423},{\"end\":46434,\"start\":46433},{\"end\":46447,\"start\":46446},{\"end\":46757,\"start\":46756},{\"end\":46768,\"start\":46767},{\"end\":46778,\"start\":46777},{\"end\":46788,\"start\":46787},{\"end\":47060,\"start\":47059},{\"end\":47070,\"start\":47069},{\"end\":47081,\"start\":47080},{\"end\":47346,\"start\":47345},{\"end\":47353,\"start\":47352},{\"end\":47603,\"start\":47602},{\"end\":47611,\"start\":47610},{\"end\":47618,\"start\":47617},{\"end\":47626,\"start\":47625},{\"end\":47633,\"start\":47632},{\"end\":47890,\"start\":47889},{\"end\":47898,\"start\":47897},{\"end\":47905,\"start\":47904},{\"end\":47912,\"start\":47911},{\"end\":48155,\"start\":48154},{\"end\":48163,\"start\":48162},{\"end\":48171,\"start\":48170},{\"end\":48178,\"start\":48177}]", "bib_author_last_name": "[{\"end\":32657,\"start\":32649},{\"end\":32664,\"start\":32661},{\"end\":32674,\"start\":32668},{\"end\":32906,\"start\":32899},{\"end\":32924,\"start\":32910},{\"end\":33150,\"start\":33144},{\"end\":33160,\"start\":33154},{\"end\":33172,\"start\":33164},{\"end\":33447,\"start\":33443},{\"end\":33458,\"start\":33453},{\"end\":33724,\"start\":33721},{\"end\":33743,\"start\":33728},{\"end\":33755,\"start\":33747},{\"end\":33767,\"start\":33759},{\"end\":33779,\"start\":33771},{\"end\":33790,\"start\":33783},{\"end\":33800,\"start\":33794},{\"end\":34178,\"start\":34173},{\"end\":34186,\"start\":34182},{\"end\":34195,\"start\":34190},{\"end\":34206,\"start\":34201},{\"end\":34486,\"start\":34481},{\"end\":34498,\"start\":34490},{\"end\":34507,\"start\":34502},{\"end\":34682,\"start\":34677},{\"end\":34697,\"start\":34686},{\"end\":34708,\"start\":34701},{\"end\":34915,\"start\":34908},{\"end\":34926,\"start\":34919},{\"end\":34936,\"start\":34930},{\"end\":34945,\"start\":34940},{\"end\":35266,\"start\":35260},{\"end\":35276,\"start\":35270},{\"end\":35287,\"start\":35280},{\"end\":35559,\"start\":35553},{\"end\":35568,\"start\":35563},{\"end\":35581,\"start\":35572},{\"end\":35720,\"start\":35714},{\"end\":35995,\"start\":35989},{\"end\":36005,\"start\":35999},{\"end\":36014,\"start\":36009},{\"end\":36023,\"start\":36018},{\"end\":36035,\"start\":36027},{\"end\":36260,\"start\":36258},{\"end\":36269,\"start\":36264},{\"end\":36276,\"start\":36273},{\"end\":36283,\"start\":36280},{\"end\":36520,\"start\":36510},{\"end\":36535,\"start\":36524},{\"end\":36667,\"start\":36662},{\"end\":36674,\"start\":36671},{\"end\":36690,\"start\":36680},{\"end\":36915,\"start\":36910},{\"end\":36922,\"start\":36919},{\"end\":36941,\"start\":36928},{\"end\":36950,\"start\":36945},{\"end\":36961,\"start\":36954},{\"end\":36973,\"start\":36965},{\"end\":36983,\"start\":36977},{\"end\":37287,\"start\":37279},{\"end\":37302,\"start\":37291},{\"end\":37312,\"start\":37306},{\"end\":37570,\"start\":37561},{\"end\":37581,\"start\":37576},{\"end\":37598,\"start\":37585},{\"end\":38065,\"start\":38060},{\"end\":38076,\"start\":38069},{\"end\":38542,\"start\":38540},{\"end\":38548,\"start\":38546},{\"end\":38942,\"start\":38939},{\"end\":38951,\"start\":38946},{\"end\":38963,\"start\":38955},{\"end\":38971,\"start\":38967},{\"end\":38981,\"start\":38975},{\"end\":38992,\"start\":38985},{\"end\":39002,\"start\":38996},{\"end\":39015,\"start\":39008},{\"end\":39344,\"start\":39341},{\"end\":39357,\"start\":39348},{\"end\":39365,\"start\":39363},{\"end\":39621,\"start\":39614},{\"end\":39631,\"start\":39627},{\"end\":39909,\"start\":39902},{\"end\":39922,\"start\":39913},{\"end\":39934,\"start\":39926},{\"end\":40148,\"start\":40141},{\"end\":40361,\"start\":40354},{\"end\":40369,\"start\":40365},{\"end\":40380,\"start\":40373},{\"end\":40388,\"start\":40384},{\"end\":40592,\"start\":40584},{\"end\":40602,\"start\":40596},{\"end\":40610,\"start\":40606},{\"end\":40617,\"start\":40614},{\"end\":40805,\"start\":40798},{\"end\":40817,\"start\":40809},{\"end\":40824,\"start\":40821},{\"end\":40834,\"start\":40828},{\"end\":41128,\"start\":41122},{\"end\":41355,\"start\":41348},{\"end\":41366,\"start\":41359},{\"end\":41382,\"start\":41370},{\"end\":41395,\"start\":41386},{\"end\":41413,\"start\":41399},{\"end\":41767,\"start\":41762},{\"end\":41779,\"start\":41771},{\"end\":41788,\"start\":41783},{\"end\":42084,\"start\":42081},{\"end\":42095,\"start\":42088},{\"end\":42608,\"start\":42602},{\"end\":42619,\"start\":42612},{\"end\":42630,\"start\":42623},{\"end\":42646,\"start\":42634},{\"end\":42654,\"start\":42650},{\"end\":42664,\"start\":42658},{\"end\":42677,\"start\":42668},{\"end\":42945,\"start\":42937},{\"end\":42958,\"start\":42949},{\"end\":43267,\"start\":43261},{\"end\":43275,\"start\":43271},{\"end\":43287,\"start\":43279},{\"end\":43300,\"start\":43291},{\"end\":43311,\"start\":43304},{\"end\":43740,\"start\":43734},{\"end\":43751,\"start\":43746},{\"end\":43765,\"start\":43755},{\"end\":43773,\"start\":43771},{\"end\":43786,\"start\":43779},{\"end\":44137,\"start\":44127},{\"end\":44147,\"start\":44141},{\"end\":44161,\"start\":44151},{\"end\":44174,\"start\":44165},{\"end\":44191,\"start\":44178},{\"end\":44480,\"start\":44470},{\"end\":44490,\"start\":44484},{\"end\":44500,\"start\":44494},{\"end\":44762,\"start\":44751},{\"end\":44774,\"start\":44766},{\"end\":44781,\"start\":44778},{\"end\":44978,\"start\":44969},{\"end\":44989,\"start\":44982},{\"end\":45001,\"start\":44995},{\"end\":45233,\"start\":45224},{\"end\":45244,\"start\":45237},{\"end\":45252,\"start\":45250},{\"end\":45781,\"start\":45773},{\"end\":45792,\"start\":45785},{\"end\":45804,\"start\":45796},{\"end\":45815,\"start\":45810},{\"end\":45825,\"start\":45819},{\"end\":45833,\"start\":45829},{\"end\":45843,\"start\":45837},{\"end\":46191,\"start\":46184},{\"end\":46201,\"start\":46195},{\"end\":46211,\"start\":46205},{\"end\":46220,\"start\":46215},{\"end\":46404,\"start\":46397},{\"end\":46414,\"start\":46408},{\"end\":46421,\"start\":46418},{\"end\":46431,\"start\":46425},{\"end\":46444,\"start\":46435},{\"end\":46454,\"start\":46448},{\"end\":46765,\"start\":46758},{\"end\":46775,\"start\":46769},{\"end\":46785,\"start\":46779},{\"end\":46796,\"start\":46789},{\"end\":47067,\"start\":47061},{\"end\":47078,\"start\":47071},{\"end\":47087,\"start\":47082},{\"end\":47096,\"start\":47089},{\"end\":47350,\"start\":47347},{\"end\":47361,\"start\":47354},{\"end\":47608,\"start\":47604},{\"end\":47615,\"start\":47612},{\"end\":47623,\"start\":47619},{\"end\":47630,\"start\":47627},{\"end\":47636,\"start\":47634},{\"end\":47895,\"start\":47891},{\"end\":47902,\"start\":47899},{\"end\":47909,\"start\":47906},{\"end\":47915,\"start\":47913},{\"end\":48160,\"start\":48156},{\"end\":48168,\"start\":48164},{\"end\":48175,\"start\":48172},{\"end\":48182,\"start\":48179}]", "bib_entry": "[{\"attributes\":{\"id\":\"b0\",\"matched_paper_id\":11212020},\"end\":32849,\"start\":32576},{\"attributes\":{\"id\":\"b1\",\"matched_paper_id\":15728911},\"end\":33072,\"start\":32851},{\"attributes\":{\"id\":\"b2\",\"matched_paper_id\":206457500},\"end\":33380,\"start\":33074},{\"attributes\":{\"id\":\"b3\",\"matched_paper_id\":215717103},\"end\":33622,\"start\":33382},{\"attributes\":{\"id\":\"b4\",\"matched_paper_id\":5590763},\"end\":34065,\"start\":33624},{\"attributes\":{\"id\":\"b5\",\"matched_paper_id\":512833},\"end\":34439,\"start\":34067},{\"attributes\":{\"id\":\"b6\"},\"end\":34617,\"start\":34441},{\"attributes\":{\"id\":\"b7\",\"matched_paper_id\":8893912},\"end\":34853,\"start\":34619},{\"attributes\":{\"id\":\"b8\",\"matched_paper_id\":206765110},\"end\":35200,\"start\":34855},{\"attributes\":{\"id\":\"b9\",\"matched_paper_id\":3338763},\"end\":35549,\"start\":35202},{\"attributes\":{\"doi\":\"arXiv:1410.5401\",\"id\":\"b10\"},\"end\":35710,\"start\":35551},{\"attributes\":{\"doi\":\"arXiv:1308.0850\",\"id\":\"b11\"},\"end\":35905,\"start\":35712},{\"attributes\":{\"id\":\"b12\",\"matched_paper_id\":10296658},\"end\":36254,\"start\":35907},{\"attributes\":{\"doi\":\"arXiv:1512.03385\",\"id\":\"b13\"},\"end\":36482,\"start\":36256},{\"attributes\":{\"id\":\"b14\",\"matched_paper_id\":1915014},\"end\":36658,\"start\":36484},{\"attributes\":{\"doi\":\"arXiv:1608.06993\",\"id\":\"b15\"},\"end\":36884,\"start\":36660},{\"attributes\":{\"id\":\"b16\",\"matched_paper_id\":9126867},\"end\":37223,\"start\":36886},{\"attributes\":{\"doi\":\"abs/1606.01404\",\"id\":\"b17\"},\"end\":37445,\"start\":37225},{\"attributes\":{\"id\":\"b18\",\"matched_paper_id\":3099330},\"end\":37956,\"start\":37447},{\"attributes\":{\"id\":\"b19\",\"matched_paper_id\":16289845},\"end\":38422,\"start\":37958},{\"attributes\":{\"id\":\"b20\",\"matched_paper_id\":13930403},\"end\":38892,\"start\":38424},{\"attributes\":{\"id\":\"b21\",\"matched_paper_id\":14113767},\"end\":39274,\"start\":38894},{\"attributes\":{\"id\":\"b22\",\"matched_paper_id\":11888861},\"end\":39538,\"start\":39276},{\"attributes\":{\"id\":\"b23\",\"matched_paper_id\":17652653},\"end\":39826,\"start\":39540},{\"attributes\":{\"id\":\"b24\",\"matched_paper_id\":2503536},\"end\":40135,\"start\":39828},{\"attributes\":{\"id\":\"b25\"},\"end\":40348,\"start\":40137},{\"attributes\":{\"id\":\"b26\"},\"end\":40516,\"start\":40350},{\"attributes\":{\"id\":\"b27\",\"matched_paper_id\":11080756},\"end\":40794,\"start\":40518},{\"attributes\":{\"doi\":\"arXiv:1312.6026\",\"id\":\"b28\"},\"end\":41048,\"start\":40796},{\"attributes\":{\"id\":\"b29\"},\"end\":41229,\"start\":41050},{\"attributes\":{\"id\":\"b30\",\"matched_paper_id\":9711750},\"end\":41699,\"start\":41231},{\"attributes\":{\"id\":\"b31\",\"matched_paper_id\":13043395},\"end\":41958,\"start\":41701},{\"attributes\":{\"id\":\"b32\",\"matched_paper_id\":15813737},\"end\":42505,\"start\":41960},{\"attributes\":{\"doi\":\"arXiv:1606.00776\",\"id\":\"b33\"},\"end\":42933,\"start\":42507},{\"attributes\":{\"doi\":\"arXiv:1409.1556\",\"id\":\"b34\"},\"end\":43192,\"start\":42935},{\"attributes\":{\"id\":\"b35\",\"matched_paper_id\":8938789},\"end\":43651,\"start\":43194},{\"attributes\":{\"id\":\"b36\",\"matched_paper_id\":6979578},\"end\":44056,\"start\":43653},{\"attributes\":{\"id\":\"b37\",\"matched_paper_id\":6844431},\"end\":44438,\"start\":44058},{\"attributes\":{\"id\":\"b38\",\"matched_paper_id\":1399322},\"end\":44703,\"start\":44440},{\"attributes\":{\"id\":\"b39\",\"matched_paper_id\":18939716},\"end\":44917,\"start\":44705},{\"attributes\":{\"id\":\"b40\",\"matched_paper_id\":8843166},\"end\":45168,\"start\":44919},{\"attributes\":{\"id\":\"b41\",\"matched_paper_id\":7961699},\"end\":45491,\"start\":45170},{\"attributes\":{\"doi\":\"abs/1605.02688\",\"id\":\"b42\"},\"end\":45769,\"start\":45493},{\"attributes\":{\"doi\":\"arXiv:1608.05148\",\"id\":\"b43\"},\"end\":46131,\"start\":45771},{\"attributes\":{\"doi\":\"abs/1411.4555\",\"id\":\"b44\"},\"end\":46362,\"start\":46133},{\"attributes\":{\"id\":\"b45\",\"matched_paper_id\":14223},\"end\":46685,\"start\":46364},{\"attributes\":{\"id\":\"b46\",\"matched_paper_id\":57564106},\"end\":46986,\"start\":46687},{\"attributes\":{\"id\":\"b47\",\"matched_paper_id\":11507867},\"end\":47283,\"start\":46988},{\"attributes\":{\"id\":\"b48\",\"matched_paper_id\":17578970},\"end\":47530,\"start\":47285},{\"attributes\":{\"id\":\"b49\",\"matched_paper_id\":12003953},\"end\":47833,\"start\":47532},{\"attributes\":{\"id\":\"b50\",\"matched_paper_id\":14891575},\"end\":48094,\"start\":47835},{\"attributes\":{\"id\":\"b51\",\"matched_paper_id\":6982885},\"end\":48357,\"start\":48096}]", "bib_title": "[{\"end\":32645,\"start\":32576},{\"end\":32895,\"start\":32851},{\"end\":33140,\"start\":33074},{\"end\":33439,\"start\":33382},{\"end\":33717,\"start\":33624},{\"end\":34167,\"start\":34067},{\"end\":34673,\"start\":34619},{\"end\":34904,\"start\":34855},{\"end\":35256,\"start\":35202},{\"end\":35985,\"start\":35907},{\"end\":36506,\"start\":36484},{\"end\":36906,\"start\":36886},{\"end\":37557,\"start\":37447},{\"end\":38056,\"start\":37958},{\"end\":38536,\"start\":38424},{\"end\":38935,\"start\":38894},{\"end\":39337,\"start\":39276},{\"end\":39610,\"start\":39540},{\"end\":39898,\"start\":39828},{\"end\":40580,\"start\":40518},{\"end\":41344,\"start\":41231},{\"end\":41758,\"start\":41701},{\"end\":42077,\"start\":41960},{\"end\":43257,\"start\":43194},{\"end\":43730,\"start\":43653},{\"end\":44123,\"start\":44058},{\"end\":44466,\"start\":44440},{\"end\":44747,\"start\":44705},{\"end\":44965,\"start\":44919},{\"end\":45220,\"start\":45170},{\"end\":46393,\"start\":46364},{\"end\":46754,\"start\":46687},{\"end\":47057,\"start\":46988},{\"end\":47343,\"start\":47285},{\"end\":47600,\"start\":47532},{\"end\":47887,\"start\":47835},{\"end\":48152,\"start\":48096}]", "bib_author": "[{\"end\":32659,\"start\":32647},{\"end\":32666,\"start\":32659},{\"end\":32676,\"start\":32666},{\"end\":32908,\"start\":32897},{\"end\":32926,\"start\":32908},{\"end\":33152,\"start\":33142},{\"end\":33162,\"start\":33152},{\"end\":33174,\"start\":33162},{\"end\":33449,\"start\":33441},{\"end\":33460,\"start\":33449},{\"end\":33726,\"start\":33719},{\"end\":33745,\"start\":33726},{\"end\":33757,\"start\":33745},{\"end\":33769,\"start\":33757},{\"end\":33781,\"start\":33769},{\"end\":33792,\"start\":33781},{\"end\":33802,\"start\":33792},{\"end\":34180,\"start\":34169},{\"end\":34188,\"start\":34180},{\"end\":34197,\"start\":34188},{\"end\":34208,\"start\":34197},{\"end\":34488,\"start\":34479},{\"end\":34500,\"start\":34488},{\"end\":34509,\"start\":34500},{\"end\":34684,\"start\":34675},{\"end\":34699,\"start\":34684},{\"end\":34710,\"start\":34699},{\"end\":34917,\"start\":34906},{\"end\":34928,\"start\":34917},{\"end\":34938,\"start\":34928},{\"end\":34947,\"start\":34938},{\"end\":35268,\"start\":35258},{\"end\":35278,\"start\":35268},{\"end\":35289,\"start\":35278},{\"end\":35561,\"start\":35551},{\"end\":35570,\"start\":35561},{\"end\":35583,\"start\":35570},{\"end\":35722,\"start\":35712},{\"end\":35997,\"start\":35987},{\"end\":36007,\"start\":35997},{\"end\":36016,\"start\":36007},{\"end\":36025,\"start\":36016},{\"end\":36037,\"start\":36025},{\"end\":36262,\"start\":36256},{\"end\":36271,\"start\":36262},{\"end\":36278,\"start\":36271},{\"end\":36285,\"start\":36278},{\"end\":36522,\"start\":36508},{\"end\":36537,\"start\":36522},{\"end\":36669,\"start\":36660},{\"end\":36676,\"start\":36669},{\"end\":36692,\"start\":36676},{\"end\":36917,\"start\":36908},{\"end\":36924,\"start\":36917},{\"end\":36943,\"start\":36924},{\"end\":36952,\"start\":36943},{\"end\":36963,\"start\":36952},{\"end\":36975,\"start\":36963},{\"end\":36985,\"start\":36975},{\"end\":37289,\"start\":37277},{\"end\":37304,\"start\":37289},{\"end\":37314,\"start\":37304},{\"end\":37572,\"start\":37559},{\"end\":37583,\"start\":37572},{\"end\":37600,\"start\":37583},{\"end\":38067,\"start\":38058},{\"end\":38078,\"start\":38067},{\"end\":38544,\"start\":38538},{\"end\":38550,\"start\":38544},{\"end\":38944,\"start\":38937},{\"end\":38953,\"start\":38944},{\"end\":38965,\"start\":38953},{\"end\":38973,\"start\":38965},{\"end\":38983,\"start\":38973},{\"end\":38994,\"start\":38983},{\"end\":39004,\"start\":38994},{\"end\":39017,\"start\":39004},{\"end\":39346,\"start\":39339},{\"end\":39359,\"start\":39346},{\"end\":39367,\"start\":39359},{\"end\":39623,\"start\":39612},{\"end\":39633,\"start\":39623},{\"end\":39911,\"start\":39900},{\"end\":39924,\"start\":39911},{\"end\":39936,\"start\":39924},{\"end\":40150,\"start\":40137},{\"end\":40363,\"start\":40352},{\"end\":40371,\"start\":40363},{\"end\":40382,\"start\":40371},{\"end\":40390,\"start\":40382},{\"end\":40594,\"start\":40582},{\"end\":40604,\"start\":40594},{\"end\":40612,\"start\":40604},{\"end\":40619,\"start\":40612},{\"end\":40807,\"start\":40796},{\"end\":40819,\"start\":40807},{\"end\":40826,\"start\":40819},{\"end\":40836,\"start\":40826},{\"end\":41130,\"start\":41120},{\"end\":41357,\"start\":41346},{\"end\":41368,\"start\":41357},{\"end\":41384,\"start\":41368},{\"end\":41397,\"start\":41384},{\"end\":41415,\"start\":41397},{\"end\":41769,\"start\":41760},{\"end\":41781,\"start\":41769},{\"end\":41790,\"start\":41781},{\"end\":42086,\"start\":42079},{\"end\":42097,\"start\":42086},{\"end\":42610,\"start\":42598},{\"end\":42621,\"start\":42610},{\"end\":42632,\"start\":42621},{\"end\":42648,\"start\":42632},{\"end\":42656,\"start\":42648},{\"end\":42666,\"start\":42656},{\"end\":42679,\"start\":42666},{\"end\":42947,\"start\":42935},{\"end\":42960,\"start\":42947},{\"end\":43269,\"start\":43259},{\"end\":43277,\"start\":43269},{\"end\":43289,\"start\":43277},{\"end\":43302,\"start\":43289},{\"end\":43313,\"start\":43302},{\"end\":43742,\"start\":43732},{\"end\":43753,\"start\":43742},{\"end\":43767,\"start\":43753},{\"end\":43775,\"start\":43767},{\"end\":43788,\"start\":43775},{\"end\":44139,\"start\":44125},{\"end\":44149,\"start\":44139},{\"end\":44163,\"start\":44149},{\"end\":44176,\"start\":44163},{\"end\":44193,\"start\":44176},{\"end\":44482,\"start\":44468},{\"end\":44492,\"start\":44482},{\"end\":44502,\"start\":44492},{\"end\":44764,\"start\":44749},{\"end\":44776,\"start\":44764},{\"end\":44783,\"start\":44776},{\"end\":44980,\"start\":44967},{\"end\":44991,\"start\":44980},{\"end\":45003,\"start\":44991},{\"end\":45235,\"start\":45222},{\"end\":45246,\"start\":45235},{\"end\":45254,\"start\":45246},{\"end\":45783,\"start\":45771},{\"end\":45794,\"start\":45783},{\"end\":45806,\"start\":45794},{\"end\":45817,\"start\":45806},{\"end\":45827,\"start\":45817},{\"end\":45835,\"start\":45827},{\"end\":45845,\"start\":45835},{\"end\":46193,\"start\":46182},{\"end\":46203,\"start\":46193},{\"end\":46213,\"start\":46203},{\"end\":46222,\"start\":46213},{\"end\":46406,\"start\":46395},{\"end\":46416,\"start\":46406},{\"end\":46423,\"start\":46416},{\"end\":46433,\"start\":46423},{\"end\":46446,\"start\":46433},{\"end\":46456,\"start\":46446},{\"end\":46767,\"start\":46756},{\"end\":46777,\"start\":46767},{\"end\":46787,\"start\":46777},{\"end\":46798,\"start\":46787},{\"end\":47069,\"start\":47059},{\"end\":47080,\"start\":47069},{\"end\":47089,\"start\":47080},{\"end\":47098,\"start\":47089},{\"end\":47352,\"start\":47345},{\"end\":47363,\"start\":47352},{\"end\":47610,\"start\":47602},{\"end\":47617,\"start\":47610},{\"end\":47625,\"start\":47617},{\"end\":47632,\"start\":47625},{\"end\":47638,\"start\":47632},{\"end\":47897,\"start\":47889},{\"end\":47904,\"start\":47897},{\"end\":47911,\"start\":47904},{\"end\":47917,\"start\":47911},{\"end\":48162,\"start\":48154},{\"end\":48170,\"start\":48162},{\"end\":48177,\"start\":48170},{\"end\":48184,\"start\":48177}]", "bib_venue": "[{\"end\":32701,\"start\":32697},{\"end\":32949,\"start\":32946},{\"end\":33491,\"start\":33484},{\"end\":33829,\"start\":33824},{\"end\":34239,\"start\":34232},{\"end\":36068,\"start\":36061},{\"end\":37709,\"start\":37663},{\"end\":38203,\"start\":38149},{\"end\":39394,\"start\":39389},{\"end\":39971,\"start\":39962},{\"end\":40642,\"start\":40639},{\"end\":41454,\"start\":41443},{\"end\":41817,\"start\":41812},{\"end\":42248,\"start\":42181},{\"end\":43432,\"start\":43381},{\"end\":45028,\"start\":45024},{\"end\":47123,\"start\":47119},{\"end\":47398,\"start\":47389},{\"end\":47669,\"start\":47662},{\"end\":47954,\"start\":47944},{\"end\":48213,\"start\":48207},{\"end\":32695,\"start\":32676},{\"end\":32944,\"start\":32926},{\"end\":33210,\"start\":33174},{\"end\":33482,\"start\":33460},{\"end\":33822,\"start\":33802},{\"end\":34230,\"start\":34208},{\"end\":34477,\"start\":34441},{\"end\":34713,\"start\":34710},{\"end\":35009,\"start\":34947},{\"end\":35342,\"start\":35289},{\"end\":35620,\"start\":35598},{\"end\":35788,\"start\":35737},{\"end\":36059,\"start\":36037},{\"end\":36345,\"start\":36301},{\"end\":36555,\"start\":36537},{\"end\":36748,\"start\":36708},{\"end\":37034,\"start\":36985},{\"end\":37275,\"start\":37225},{\"end\":37661,\"start\":37600},{\"end\":38147,\"start\":38078},{\"end\":38636,\"start\":38550},{\"end\":39055,\"start\":39017},{\"end\":39387,\"start\":39367},{\"end\":39667,\"start\":39633},{\"end\":39960,\"start\":39936},{\"end\":40231,\"start\":40150},{\"end\":40637,\"start\":40619},{\"end\":40898,\"start\":40851},{\"end\":41118,\"start\":41050},{\"end\":41441,\"start\":41415},{\"end\":41810,\"start\":41790},{\"end\":42179,\"start\":42097},{\"end\":42596,\"start\":42507},{\"end\":43041,\"start\":42975},{\"end\":43379,\"start\":43313},{\"end\":43837,\"start\":43788},{\"end\":44229,\"start\":44193},{\"end\":44551,\"start\":44502},{\"end\":44794,\"start\":44783},{\"end\":45022,\"start\":45003},{\"end\":45312,\"start\":45254},{\"end\":45568,\"start\":45493},{\"end\":45925,\"start\":45861},{\"end\":46180,\"start\":46133},{\"end\":46505,\"start\":46456},{\"end\":46828,\"start\":46798},{\"end\":47117,\"start\":47098},{\"end\":47387,\"start\":47363},{\"end\":47660,\"start\":47638},{\"end\":47942,\"start\":47917},{\"end\":48205,\"start\":48184}]"}}}, "year": 2023, "month": 12, "day": 17}